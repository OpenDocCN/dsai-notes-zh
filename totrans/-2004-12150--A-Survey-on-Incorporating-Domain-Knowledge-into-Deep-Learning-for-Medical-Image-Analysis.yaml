- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:01:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.12150] A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.12150] 关于将领域知识融入深度学习以进行医学图像分析的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12150](https://ar5iv.labs.arxiv.org/html/2004.12150)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12150](https://ar5iv.labs.arxiv.org/html/2004.12150)
- en: A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于将领域知识融入深度学习以进行医学图像分析的调查
- en: 'Xiaozheng Xie, Jianwei Niu, , Xuefeng Liu, Zhengsu Chen, Shaojie Tang,  and
    Shui Yu X. Xie, J. Niu, X. Liu and Z. Chen are with the State Key Laboratory of
    Virtual Reality Technology and Systems, School of Computer Science and Engineering,
    Beihang University, Beijing 100191, China. E-mails: {xiexzheng,niujianwei,liu_xuefeng,danczs}@buaa.edu.cn
    J. Niu is also with the Beijing Advanced Innovation Center for Big Data and Brain
    Computing (BDBC) and Hangzhou Innovation Institute of Beihang University. S. Tang
    is in Jindal School of Management, The University of Texas at Dallas. E-mails:
    tangshaojie@gmail.com S. Yu is in School of Computer Science, University of Technology
    Sydney, Australia. E-mails: Shui.Yu@uts.edu.au'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xie Xiaozheng，Niu Jianwei，Liu Xuefeng，Chen Zhengsu，Tang Shaojie 和 Shui Yu X.
    Xie、J. Niu、X. Liu 和 Z. Chen 均来自北京航空航天大学计算机科学与工程学院虚拟现实技术与系统国家重点实验室，中国北京 100191。电子邮件：{xiexzheng,
    niujianwei, liu_xuefeng, danczs}@buaa.edu.cn J. Niu 还与北京大数据与脑计算高级创新中心（BDBC）及北京航空航天大学杭州创新研究院有关。S.
    Tang 在德克萨斯大学达拉斯分校金达尔管理学院任职。电子邮件：tangshaojie@gmail.com S. Yu 在澳大利亚悉尼科技大学计算机科学学院任职。电子邮件：Shui.Yu@uts.edu.au
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Although deep learning models like CNNs have achieved great success in medical
    image analysis, the small size of medical datasets remains a major bottleneck
    in this area. To address this problem, researchers have started looking for external
    information beyond current available medical datasets. Traditional approaches
    generally leverage the information from natural images via transfer learning.
    More recent works utilize the domain knowledge from medical doctors, to create
    networks that resemble how medical doctors are trained, mimic their diagnostic
    patterns, or focus on the features or areas they pay particular attention to.
    In this survey, we summarize the current progress on integrating medical domain
    knowledge into deep learning models for various tasks, such as disease diagnosis,
    lesion, organ and abnormality detection, lesion and organ segmentation. For each
    task, we systematically categorize different kinds of medical domain knowledge
    that have been utilized and their corresponding integrating methods. We also provide
    current challenges and directions for future research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像CNN这样的深度学习模型在医学图像分析中取得了巨大成功，但医学数据集的规模小仍然是该领域的主要瓶颈。为了解决这个问题，研究人员已经开始寻找超出当前可用医学数据集的外部信息。传统方法通常通过迁移学习利用自然图像中的信息。最近的工作则利用来自医学医生的领域知识，创建出类似于医学医生训练方式的网络，模拟他们的诊断模式，或关注他们特别关注的特征或区域。在本调查中，我们总结了将医学领域知识整合到深度学习模型中的当前进展，涉及疾病诊断、病灶、器官和异常检测、病灶和器官分割等各种任务。对于每个任务，我们系统地分类了已经利用的不同类型的医学领域知识及其相应的整合方法。我们还提供了当前的挑战和未来研究的方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: medical image analysis, medical domain knowledge, deep neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分析，医学领域知识，深度神经网络。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent years have witnessed a tremendous progress in computer-aided detection/diagnosis
    (CAD) in medical imaging and diagnostic radiology, primarily thanks to the advancement
    of deep learning techniques. Having achieved great success in computer vision
    tasks, various deep learning models, mainly convolutional neural networks (CNNs),
    soon be applied to CAD. Among the applications are the early detection and diagnosis
    of breast cancer, lung cancer, glaucoma, and skin cancer [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机辅助检测/诊断（CAD）在医学成像和诊断放射学中取得了巨大进展，这主要得益于深度学习技术的进步。各种深度学习模型，主要是卷积神经网络（CNNs），在计算机视觉任务中取得了巨大成功，并很快被应用于CAD。应用包括乳腺癌、肺癌、青光眼和皮肤癌的早期检测和诊断[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]。
- en: However, the small size of medical datasets continues to be an issue in obtaining
    satisfactory deep learning model for CAD; in general, bigger datasets result in
    better deep learning models [[5](#bib.bib5)]. In traditional computer vision tasks,
    there are many large-scale and well-annotated datasets, such as ImageNet ¹¹1http://www.image-net.org/
    (over 14M labeled images from 20k categories) and COCO ²²2http://mscoco.org/ (with
    more than 200k annotated images across 80 categories). In contrast, some popular
    publicly available medical datasets are much smaller (see Table [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis")). For example, among the datasets for different tasks,
    only ChestX-ray14 and DeepLesion, contain more than 100k labeled medical images,
    while most datasets only have a few thousands or even hundreds of medical images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，医学数据集的规模较小仍然是获得令人满意的 CAD 深度学习模型的问题；一般来说，数据集越大，深度学习模型的效果越好 [[5](#bib.bib5)]。在传统计算机视觉任务中，有许多大规模且标注良好的数据集，例如
    ImageNet ¹¹1http://www.image-net.org/（超过 1400 万张标注图像，涵盖 2 万个类别）和 COCO ²²2http://mscoco.org/（超过
    20 万张标注图像，涵盖 80 个类别）。相比之下，一些流行的公开医学数据集要小得多（参见表 [I](#S1.T1 "TABLE I ‣ 1 Introduction
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis)）。例如，在不同任务的数据集中，只有 ChestX-ray14 和 DeepLesion 包含超过 10 万张标注医学图像，而大多数数据集仅有几千张甚至几百张医学图像。
- en: 'TABLE I: Examples of popular datasets in the medical domain'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 医学领域中流行数据集的示例'
- en: '| Name | Purpose | Type | Imaging | Number of Images |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 目的 | 类型 | 成像 | 图像数量 |'
- en: '|'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ADNI [[6](#bib.bib6)] &#124;'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ADNI [[6](#bib.bib6)] &#124;'
- en: '| Classification | Brain | Multiple | 1921 patients |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 大脑 | 多种 | 1921 位患者 |'
- en: '| ABIDE [[7](#bib.bib7)] | Classification | Brain | MRI | 539 patients and
    573 controls |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ABIDE [[7](#bib.bib7)] | 分类 | 大脑 | MRI | 539 位患者和 573 位对照 |'
- en: '|'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ACDC [[8](#bib.bib8)] &#124;'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACDC [[8](#bib.bib8)] &#124;'
- en: '| Classification | Cardiac | MRI | 150 patients |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 心脏 | MRI | 150 位患者 |'
- en: '| ChestX-ray14 [[9](#bib.bib9)] | Detection | Chest | X-ray | 112,120 images
    from 30,805 patients |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ChestX-ray14 [[9](#bib.bib9)] | 检测 | 胸部 | X-ray | 112,120 张图像来自 30,805 位患者
    |'
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LIDC-IDRI [[10](#bib.bib10)] &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LIDC-IDRI [[10](#bib.bib10)] &#124;'
- en: '| Detection | Lung | CT, X-ray | 1,018 patients |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 检测 | 肺部 | CT, X-ray | 1,018 位患者 |'
- en: '|'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LUNA16 [[11](#bib.bib11)] &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LUNA16 [[11](#bib.bib11)] &#124;'
- en: '| Detection | Lung | CT | 888 images |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 检测 | 肺部 | CT | 888 张图像 |'
- en: '| MURA [[12](#bib.bib12)] | Detection | Musculo-skeletal | X-ray |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| MURA [[12](#bib.bib12)] | 检测 | 骨骼肌肉系统 | X-ray |'
- en: '&#124; 40,895 images from 14,982 patients &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 40,895 张图像来自 14,982 位患者 &#124;'
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BraTS2018 [[13](#bib.bib13)] &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BraTS2018 [[13](#bib.bib13)] &#124;'
- en: '| Segmentation | Brain | MRI | 542 images |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 分割 | 大脑 | MRI | 542 张图像 |'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; STARE [[14](#bib.bib14)] &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; STARE [[14](#bib.bib14)] &#124;'
- en: '| Segmentation | Eye | SLO | 400 images |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 分割 | 眼睛 | SLO | 400 张图像 |'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DDSM [[15](#bib.bib15)] &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDSM [[15](#bib.bib15)] &#124;'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '| Breast | Mammography | 2,500 patients |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺 | 乳腺X光 | 2,500 位患者 |'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DeepLesion [[16](#bib.bib16)] &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepLesion [[16](#bib.bib16)] &#124;'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '| Multiple | CT | 32,735 images from 4,427 patients |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 多种 | CT | 32,735 张图像来自 4,427 位患者 |'
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cardiac MRI [[17](#bib.bib17)] &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 心脏 MRI [[17](#bib.bib17)] &#124;'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| Cardiac | MRI | 7,980 images from 33 cases |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 心脏 | MRI | 7,980 张图像来自 33 个案例 |'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISIC 2018 [[18](#bib.bib18)] &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISIC 2018 [[18](#bib.bib18)] &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| Skin | Dermoscopic | 13,000 images |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 皮肤 | 皮肤镜 | 13,000 张图像 |'
- en: The lack of medical datasets is represented in three aspects. First, the number
    of medical images in datasets is usually small. This problem is mainly due to
    the high cost associated with the data collection. Medical images are collected
    from computerized tomography (CT), Ultrasonic imaging (US), magnetic resonance
    imaging (MRI) scans, positron emission tomography (PET), all of which are expensive
    and labor-intensive. Second, only a small portion of medical images are annotated.
    These annotations including classification labels (e.g., benign or malignant),
    the segmentation annotations of lesion areas, etc., require efforts from experienced
    doctors. Third, it is difficult to collect enough positive cases for some rare
    diseases to obtain the balanced datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 医学数据集的不足表现为三个方面。首先，数据集中的医学图像数量通常较少。这一问题主要由于数据收集的高成本。医学图像来源于计算机断层扫描（CT）、超声成像（US）、磁共振成像（MRI）扫描、正电子发射断层扫描（PET），这些都是昂贵且劳动密集的。其次，只有少量医学图像经过注释。这些注释包括分类标签（例如，良性或恶性）、病灶区域的分割注释等，需要经验丰富的医生的努力。第三，对于一些罕见疾病，收集足够的阳性病例以获得平衡的数据集也是困难的。
- en: One direct consequence of the lack of well annotated medical data is that the
    trained deep learning models can easily suffer from the overfitting problem [[19](#bib.bib19)].
    As a result, the models perform very well on training datasets, but fail when
    dealing with new data from the problem domain. Correspondingly, many existing
    studies on medical image analysis adopt techniques from computer vision to address
    overfitting, such as reducing the complexity of the network [[20](#bib.bib20),
    [21](#bib.bib21)], adopting some regularization techniques [[22](#bib.bib22)],
    or using data augmentation strategies [[23](#bib.bib23)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏良好注释的医学数据的一个直接后果是，训练出的深度学习模型很容易遭遇过拟合问题[[19](#bib.bib19)]。因此，模型在训练数据集上表现非常好，但在处理来自问题领域的新数据时却表现不佳。相应地，许多现有的医学图像分析研究采用计算机视觉技术来解决过拟合问题，例如减少网络复杂性[[20](#bib.bib20),
    [21](#bib.bib21)]、采用一些正则化技术[[22](#bib.bib22)]，或使用数据增强策略[[23](#bib.bib23)]。
- en: However, in essence, both decreasing model complexity and leveraging data augmentation
    techniques only focus on the target task on the given datasets, but *do not introduce
    any new information into deep learning models*. Nowdays, introducing more information,
    beyond the given medical datasets has become a more promising approach to address
    the problem of small-sized medical datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从本质上讲，降低模型复杂性和利用数据增强技术只是关注给定数据集上的目标任务，但*并没有向深度学习模型引入任何新信息*。如今，除了给定的医学数据集，引入更多信息已经成为解决小规模医学数据集问题的更有前景的方法。
- en: The idea of introducing external information to improve the performance of deep
    learning models for CAD is not new. For example, it is common practice to first
    train a deep learning model on some natural image datasets like ImageNet, and
    then fine tune them on target medical datasets [[24](#bib.bib24)]. This process,
    called transfer learning [[25](#bib.bib25)], implicitly introduces information
    from natural images. Besides natural images, multi-modal medical datasets or medical
    images from different but related diseases can also be used to improve the performance
    of deep learning models [[26](#bib.bib26), [27](#bib.bib27)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 引入外部信息以提高计算机辅助诊断（CAD）深度学习模型性能的想法并不新颖。例如，通常的做法是先在一些自然图像数据集（如ImageNet）上训练深度学习模型，然后在目标医学数据集上进行微调[[24](#bib.bib24)]。这个过程称为迁移学习[[25](#bib.bib25)]，它隐式地引入了自然图像中的信息。除了自然图像，多模态医学数据集或来自不同但相关疾病的医学图像也可以用来提高深度学习模型的性能[[26](#bib.bib26),
    [27](#bib.bib27)]。
- en: Moreover, as experienced medical doctors (e.g., radiologists, ophthalmologists,
    and dermatologists) can generally give fairly accurate results, it is not surprising
    that their knowledge may help deep learning models to better accomplish the designated
    tasks. The domain knowledge of medical doctors includes the way they browse images,
    the particular areas they usually focus on, the features they give special attention
    to, and the anatomical prior knowledge they used. These types of knowledge are
    accumulated, summarized, and validated by a large number of practitioners over
    many years based on a huge amount of cases. Note that in this survey any network
    that incorporate one of these types of knowledge in their training or designing
    process should be regarded as the one incorporated medical domain knowledge.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于经验丰富的医疗医生（如放射科医生、眼科医生和皮肤科医生）通常能够提供相当准确的结果，他们的知识可能帮助深度学习模型更好地完成指定任务也就不足为奇了。医疗医生的领域知识包括他们浏览图像的方式、他们通常关注的特定区域、他们特别注意的特征以及他们使用的解剖学先验知识。这些知识是通过大量案例由众多从业者在多年中积累、总结和验证的。请注意，在本调查中，任何在其训练或设计过程中融入这些类型知识的网络都应被视为融入了医学领域知识的网络。
- en: '![Refer to caption](img/35ea96f042890d766a6a045567db868d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35ea96f042890d766a6a045567db868d.png)'
- en: 'Figure 1: Methods of information categorization and incorporating methods in
    disease diagnosis; lesion, organ, and abnormality detection; lesion and organ
    segmentation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：疾病诊断中的信息分类和方法融入方法；病灶、器官和异常检测；病灶和器官分割。
- en: 'In this survey, we focus on the three main tasks of medical image analysis:
    (1) disease diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion
    and organ segmentation. We also include other related tasks such as the image
    reconstruction, image retrieval and report generation. This survey demonstrates
    that, for almost all tasks, identifying and carefully integrating one or more
    types of domain knowledge related to the designated task will improve the performance
    of deep learning models. We organize existing works according to the following
    three aspects: the types of tasks, the types of domain knowledge that are introduced,
    and the ways of introducing the domain knowledge.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们关注医学图像分析的三个主要任务：（1）疾病诊断，（2）病灶、器官和异常检测，以及（3）病灶和器官分割。我们还包括其他相关任务，如图像重建、图像检索和报告生成。该调查表明，几乎所有任务中，识别和仔细整合一种或多种与指定任务相关的领域知识将提高深度学习模型的性能。我们根据以下三个方面组织现有工作：任务类型、引入的领域知识类型以及引入领域知识的方法。
- en: More specifically, in terms of the types of domain knowledge, some of them are
    of high-level such as training pattern [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]
    and diagnostic pattern. Some domain knowledge are low-level, such as particular
    features and special areas where medical doctors pay more attention to [[31](#bib.bib31)].
    In particular, in disease diagnosis tasks, high-level domain knowledge is widely
    utilized. For an object detection task, the low-level domain knowledge, such as
    detection patterns and specific features where medical doctors give special attention
    is more commonly adopted. For lesion or organ segmentation tasks, anatomical priors
    and the knowledge from different modalities seem to be more useful [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在领域知识的类型方面，有些是高级的，例如训练模式[[28](#bib.bib28)、[29](#bib.bib29)、[30](#bib.bib30)]和诊断模式。一些领域知识则是低级的，例如特定特征和医疗医生更关注的特殊区域[[31](#bib.bib31)]。特别是在疾病诊断任务中，高级领域知识被广泛利用。对于目标检测任务，低级领域知识，如检测模式和医疗医生特别关注的具体特征，更为常见。对于病灶或器官分割任务，解剖学先验和来自不同模态的知识似乎更为有用[[32](#bib.bib32)、[33](#bib.bib33)、[34](#bib.bib34)]。
- en: '![Refer to caption](img/6cc738d2ce8301a87d9258d1f3c41477.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6cc738d2ce8301a87d9258d1f3c41477.png)'
- en: 'Figure 2: (a) Number of papers arranged chronically (2016-2020). (b) The distribution
    of selected papers in different applications of medical image analysis.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2： (a) 按时间顺序排列的论文数量（2016-2020）。 (b) 选择的论文在不同医学图像分析应用中的分布。
- en: In terms of the integrating methods, various approaches have been designed to
    incorporate different types of domain knowledge into networks [[35](#bib.bib35)].
    For example, a simple approach is to concatenate hand-crafted features with the
    ones extracted from deep learning models [[36](#bib.bib36)]. In some works, network
    architectures are revised to simulate the pattern of radiologists when they read
    images [[37](#bib.bib37)]. Attention mechanism, which allows a network to pay
    more attention to a certain region of an image, is a powerful technique to incorporate
    domain knowledge of radiologists [[38](#bib.bib38)]. In addition, multi-task learning
    and meta learning are also widely used to introduce medical domain knowledge into
    deep learning models [[39](#bib.bib39), [40](#bib.bib40)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在整合方法方面，设计了各种方法将不同类型的领域知识融入网络中[[35](#bib.bib35)]。例如，一种简单的方法是将手工制作的特征与从深度学习模型中提取的特征进行连接[[36](#bib.bib36)]。在一些工作中，网络架构被修改以模拟放射科医生在阅读图像时的模式[[37](#bib.bib37)]。注意机制，允许网络对图像的特定区域给予更多关注，是一种有效的技术，用于融入放射科医生的领域知识[[38](#bib.bib38)]。此外，多任务学习和元学习也被广泛用于将医学领域知识引入深度学习模型中[[39](#bib.bib39),
    [40](#bib.bib40)]。
- en: Although there are a number of reviews on deep learning for medical image analysis,
    including [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)],
    they all describe the existing works from the application point of view, i.e.,
    how deep learning techniques are applied to various medical applications. To the
    best of our knowledge, there is no review that gives systematic introduction on
    *how medical domain knowledge can help deep learning models*. This aspect, we
    believe, is the unique feature that distinguishes deep learning models for CAD
    from those for general computer vision tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有许多关于深度学习在医学图像分析中的综述，包括[[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]，但它们都从应用角度描述现有工作，即深度学习技术如何应用于各种医学应用。根据我们所知，目前没有综述系统性地介绍*医学领域知识如何帮助深度学习模型*。我们认为，这一方面是深度学习模型用于计算机辅助诊断（CAD）与用于一般计算机视觉任务的独特之处。
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") gives the overview on
    how we organize the related researches. At the top level, existing studies are
    classified into three main categories according to their purposes: (1) disease
    diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion and organ
    segmentation. In each category, we organize them into several groups based on
    the types of extra knowledge have been incorporated. At the bottom level, they
    are further categorized according to the different integrating approaches of the
    domain knowledge.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")概述了我们如何组织相关研究。在顶层，现有研究根据其目的被分为三个主要类别：（1）疾病诊断，（2）病灶、器官和异常检测，以及（3）病灶和器官分割。在每个类别中，我们根据融入的额外知识类型将其组织为几个组。在底层，根据领域知识的不同整合方法进一步分类。
- en: This survey contains more than 200 papers (163 are with domain knowledge), most
    of which are published recently (2016-2020), on a wide variety of applications
    of deep learning techniques for medical image analysis. In addition, most of the
    corresponding works are from the conference proceedings for MICCAI, EMBC, ISBI
    and some journals such as TMI, Medical Image Analysis, JBHI and so on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查包含了200多篇论文（其中163篇涉及领域知识），大多数是最近（2016-2020年）发表的，涉及深度学习技术在医学图像分析中的广泛应用。此外，大多数相关工作来自于MICCAI、EMBC、ISBI等会议论文集，以及TMI、Medical
    Image Analysis、JBHI等期刊。
- en: '![Refer to caption](img/afbdc1af2cfaab97cd40ba6f5c79756a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/afbdc1af2cfaab97cd40ba6f5c79756a.png)'
- en: 'Figure 3: The organizational structure of this survey.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：本调查的组织结构。
- en: 'The distribution of these papers are shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")(a). It can be seen that the number of papers increases
    rapidly from 2016 to 2020\. With respect to the applications, most of them are
    related to disease diagnosis and lesion/organ segmentation (shown in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")(b)). To sum up, with this survey we aim
    to:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文的分布如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")(a) 所示。从2016年到2020年，论文数量急剧增加。就应用而言，大多数论文与疾病诊断和病灶/器官分割相关（见图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")(b)）。总之，通过本次调查，我们的目标是：
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: summarize and classify different types of domain knowledge in medical areas
    that are utilized to improve the performance of deep learning models in various
    applications;
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结和分类在医学领域中用于提高深度学习模型在各种应用中的性能的不同类型的领域知识；
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: summarize and classify different ways of introducing medical domain knowledge
    into deep learning models;
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结和分类将医学领域知识引入深度学习模型的不同方式；
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: give the outlook of challenges and future directions in integrating medical
    domain knowledge into deep learning models.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍将医学领域知识整合到深度学习模型中的挑战和未来方向。
- en: The rest of the survey is organized as follows. Sections [2](#S2 "2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis"), [3](#S3 "3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and [4](#S4 "4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") introduce the
    existing works for the major three tasks in medical image analysis. Besides these
    three major tasks, other tasks in medical image analysis are described in Section
    [5](#S5 "5 Other Medical Applications ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"). In each section, we first introduce
    the general architectures of deep learning models for a task, and then categorize
    related works according to the types of the domain knowledge to be integrated.
    Various incorporating methods for each type of domain knowledge are then described.
    Section [6](#S6 "6 Research Challenges and Future Directions ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") discusses research
    challenges, and gives the outlook of future directions. Lastly, Section [7](#S7
    "7 Conclusion ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis") concludes this survey. The structure of this survey
    is shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的调查内容组织如下：第 [2](#S2 "2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis")、[3](#S3 "3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis") 和 [4](#S4 "4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") 节介绍了医学图像分析中三个主要任务的现有工作。除了这三个主要任务，医学图像分析中的其他任务在第 [5](#S5 "5 Other Medical
    Applications ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") 节中描述。在每一节中，我们首先介绍该任务的深度学习模型的通用架构，然后根据待整合的领域知识类型对相关工作进行分类。随后描述每种领域知识的各种整合方法。第
    [6](#S6 "6 Research Challenges and Future Directions ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") 节讨论了研究挑战，并展望未来方向。最后，第
    [7](#S7 "7 Conclusion ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis") 节总结了本次调查。调查的结构如图 [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") 所示。
- en: 2 Disease Diagnosis
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 疾病诊断
- en: Disease diagnosis refers to the task of determining the type and condition of
    possible diseases based on the medical images provided. In this section, we give
    an overview of the deep learning models that generally used for disease diagnosis.
    Concretely, subsection [2.1](#S2.SS1 "2.1 General Structures of Deep Learning
    Models Used for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") outlines the
    general structures of deep learning models used for disease diagnosis. Subsection
    [2.2](#S2.SS2 "2.2 Incorporating Knowledge from Natural Datasets or Other Medical
    Datasets ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into
    Deep Learning for Medical Image Analysis") introduces the works that utilize knowledge
    from natural images or other medical datasets. Deep learning models that leverage
    knowledge from medical doctors are introduced in Subsection [2.3](#S2.SS3 "2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")
    in detail. Lastly, Subsection [2.4](#S2.SS4 "2.4 Summary ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") summarizes the research of disease diagnosis.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 疾病诊断是指基于提供的医学图像确定可能的疾病类型和状态的任务。在本节中，我们概述了通常用于疾病诊断的深度学习模型。具体而言，[2.1](#S2.SS1
    "2.1 深度学习模型在疾病诊断中的一般结构 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节概述了用于疾病诊断的深度学习模型的一般结构。[2.2](#S2.SS2
    "2.2 从自然数据集或其他医学数据集中融入知识 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节介绍了利用自然图像或其他医学数据集中的知识的研究。利用医学专家知识的深度学习模型在
    [2.3](#S2.SS3 "2.3 从医学专家中融入知识 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节中详细介绍。最后，[2.4](#S2.SS4
    "2.4 总结 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节总结了疾病诊断的研究。
- en: 2.1 General Structures of Deep Learning Models Used for Disease Diagnosis
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度学习模型在疾病诊断中的一般结构
- en: In the last decades, deep learning techniques, especially CNNs, have achieved
    a great success in disease diagnosis.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，深度学习技术，特别是 CNNs，在疾病诊断中取得了巨大成功。
- en: Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 General Structures of Deep Learning Models Used
    for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") shows the structure
    of a typical CNN that used for disease diagnosis in chest X-ray image. The CNN
    employs alternating convolutional and pooling layers, and contains trainable filter
    banks per layer. Each individual filter in a filter bank is able to generate a
    feature map. This process is alternated and the CNN can learn increasingly more
    and more abstract features that will later be used by the fully connected layers
    to accomplish the classification task.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S2.F4 "图 4 ‣ 2.1 深度学习模型在疾病诊断中的一般结构 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查")
    显示了用于胸部 X 光图像疾病诊断的典型 CNN 结构。该 CNN 采用交替的卷积层和池化层，并且每层包含可训练的滤波器组。每个滤波器组中的单个滤波器能够生成特征图。这个过程是交替进行的，CNN
    可以学习越来越抽象的特征，这些特征将被全连接层用于完成分类任务。
- en: '![Refer to caption](img/13d7097710f05c622a19c53a7a5401b0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13d7097710f05c622a19c53a7a5401b0.png)'
- en: 'Figure 4: A typical CNN architecture for medical disease diagnosis.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于医学疾病诊断的典型 CNN 结构。
- en: Different types of CNN architectures, from AlexNet [[45](#bib.bib45)], GoogLeNet
    [[46](#bib.bib46)], VGGNet [[47](#bib.bib47)], ResNet [[48](#bib.bib48)] to DenseNet
    [[49](#bib.bib49)], have achieved a great success in the diagnosis of various
    diseases. For example, GoogLeNet, ResNet, and VGGNet models are used in the diagnosis
    of canine ulcerative keratitis [[50](#bib.bib50)], and most of them achieve accuracies
    of over 90% when classifying superficial and deep corneal ulcers. DenseNet is
    adopted to diagnose lung nodules on chest X-ray radiograph [[51](#bib.bib51)],
    and experimental results show that more than 99% of lung nodules can be detected.
    In addition, it is found that VGGNet and ResNet are more effective than other
    network structures for many medical diagnostic tasks [[37](#bib.bib37), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从AlexNet [[45](#bib.bib45)]、GoogLeNet [[46](#bib.bib46)]、VGGNet [[47](#bib.bib47)]、ResNet
    [[48](#bib.bib48)] 到 DenseNet [[49](#bib.bib49)] 等不同类型的卷积神经网络（CNN）架构，在各种疾病的诊断中取得了巨大的成功。例如，GoogLeNet、ResNet
    和 VGGNet 模型被用于犬类溃疡性角膜炎的诊断 [[50](#bib.bib50)]，其中大多数在分类浅层和深层角膜溃疡时的准确率超过 90%。DenseNet
    被用于胸部 X 射线影像的肺结节诊断 [[51](#bib.bib51)]，实验结果显示超过 99% 的肺结节能够被检测到。此外，研究发现，VGGNet 和
    ResNet 在许多医学诊断任务中比其他网络结构更有效 [[37](#bib.bib37), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]。
- en: However, the above works generally directly apply CNNs to medical image analysis
    or slightly modified CNNs (e.g., by modifying the number of kernals, the number
    of channels or the size of filters), and no medical knowledge is incorporated.
    In addition, these methods generally require large medical datasets to achieve
    a satisfactory performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述工作通常直接将 CNN 应用于医学图像分析，或对 CNN 进行轻微修改（例如，修改卷积核数量、通道数量或滤波器大小），而未融入医学知识。此外，这些方法通常需要大量的医学数据集才能达到令人满意的性能。
- en: In the following subsections, we systematically review on the research that
    utilizes medical domain knowledge for the disease diagnosis. The types of knowledge
    and the incorporating methods are summarized in Table [II](#S2.T2 "TABLE II ‣
    2.1 General Structures of Deep Learning Models Used for Disease Diagnosis ‣ 2
    Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们系统地回顾了利用医学领域知识进行疾病诊断的研究。知识类型及其结合方法在表 [II](#S2.T2 "TABLE II ‣ 2.1
    General Structures of Deep Learning Models Used for Disease Diagnosis ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") 中进行了总结。
- en: 'TABLE II: A compilation of the knowledge and corresponding incorporating methods
    used in disease diagnosis'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 疾病诊断中使用的知识及其相应结合方法的汇编'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating Method &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合方法 &#124;'
- en: '| References |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[55](#bib.bib55)][[1](#bib.bib1)]  [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)]  [[59](#bib.bib59)]  [[60](#bib.bib60)][[61](#bib.bib61)]
    &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[55](#bib.bib55)][[1](#bib.bib1)]  [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)]  [[59](#bib.bib59)]  [[60](#bib.bib60)][[61](#bib.bib61)]
    &#124;'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical datasets |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 医学数据集 |'
- en: '&#124; Multi-modal images &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态图像 &#124;'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[26](#bib.bib26)][[62](#bib.bib62)]  [[63](#bib.bib63)][[64](#bib.bib64)][[65](#bib.bib65)][[66](#bib.bib66)]
    &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[26](#bib.bib26)][[62](#bib.bib62)]  [[63](#bib.bib63)][[64](#bib.bib64)][[65](#bib.bib65)][[66](#bib.bib66)]
    &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Datasets from other diseases &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自其他疾病的数据集 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[67](#bib.bib67)][[68](#bib.bib68)] &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[67](#bib.bib67)][[68](#bib.bib68)] &#124;'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical doctors |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 医学专家 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[28](#bib.bib28)][[29](#bib.bib29)]  [[69](#bib.bib69)][[70](#bib.bib70)][[71](#bib.bib71)]  [[72](#bib.bib72)][[73](#bib.bib73)][[74](#bib.bib74)]
    &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[28](#bib.bib28)][[29](#bib.bib29)]  [[69](#bib.bib69)][[70](#bib.bib70)][[71](#bib.bib71)]  [[72](#bib.bib72)][[73](#bib.bib73)][[74](#bib.bib74)]
    &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Diagnostic patterns &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 诊断模式 &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Network design &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络设计 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[37](#bib.bib37)][[75](#bib.bib75)]  [[76](#bib.bib76)][[77](#bib.bib77)]  [[78](#bib.bib78)][[79](#bib.bib79)]
    &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[37](#bib.bib37)][[75](#bib.bib75)]  [[76](#bib.bib76)][[77](#bib.bib77)]  [[78](#bib.bib78)][[79](#bib.bib79)]
    &#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Areas doctors focus on &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医生关注的领域 &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attention mechanism &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制 &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[38](#bib.bib38)]  [[53](#bib.bib53)]  [[80](#bib.bib80)][[81](#bib.bib81)]  [[82](#bib.bib82)][[83](#bib.bib83)]
    &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[38](#bib.bib38)]  [[53](#bib.bib53)]  [[80](#bib.bib80)][[81](#bib.bib81)]  [[82](#bib.bib82)][[83](#bib.bib83)]
    &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Features doctors focus on |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 医生关注的特征 |'
- en: '&#124; Decision level fusion &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 决策层融合 &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[61](#bib.bib61)]  [[84](#bib.bib84)]  [[85](#bib.bib85)][[86](#bib.bib86)]  [[87](#bib.bib87)][[88](#bib.bib88)]
    &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[61](#bib.bib61)]  [[84](#bib.bib84)]  [[85](#bib.bib85)][[86](#bib.bib86)]  [[87](#bib.bib87)][[88](#bib.bib88)]
    &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征层融合 &#124;'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[36](#bib.bib36)][[89](#bib.bib89)]  [[57](#bib.bib57)]  [[90](#bib.bib90)]  [[91](#bib.bib91)][[92](#bib.bib92)]
    &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[36](#bib.bib36)][[89](#bib.bib89)]  [[57](#bib.bib57)]  [[90](#bib.bib90)]  [[91](#bib.bib91)][[92](#bib.bib92)]
    &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input level fusion &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入层融合 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[93](#bib.bib93)]  [[54](#bib.bib54)]  [[94](#bib.bib94)][[95](#bib.bib95)]  [[96](#bib.bib96)]
    &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[93](#bib.bib93)]  [[54](#bib.bib54)]  [[94](#bib.bib94)][[95](#bib.bib95)]  [[96](#bib.bib96)]
    &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; As labels of CNNs &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN标签 &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[39](#bib.bib39)][[59](#bib.bib59)]  [[97](#bib.bib97)] &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[39](#bib.bib39)][[59](#bib.bib59)]  [[97](#bib.bib97)] &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other related information &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他相关信息 &#124;'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '&#124; /network design &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; /网络设计 &#124;'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[52](#bib.bib52)]  [[98](#bib.bib98)]  [[99](#bib.bib99)][[100](#bib.bib100)]  [[101](#bib.bib101)]
    &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[52](#bib.bib52)]  [[98](#bib.bib98)]  [[99](#bib.bib99)][[100](#bib.bib100)]  [[101](#bib.bib101)]
    &#124;'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 2.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 从自然数据集或其他医疗数据集中融入知识
- en: Despite the disparity between natural and medical images, it has been demonstrated
    that CNNs comprehensively trained on the large-scale well-annotated natural image
    datasets can still be helpful for disease diagnosis tasks [[56](#bib.bib56)].
    Intrinsically speaking, this transfer learning process introduces knowledge from
    natural images into the network for medical image diagnosis.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自然图像和医疗图像之间存在差异，但已经证明在大规模、标注良好的自然图像数据集上全面训练的CNN仍对疾病诊断任务有帮助[[56](#bib.bib56)]。从本质上讲，这种迁移学习过程将自然图像中的知识引入网络，用于医疗图像诊断。
- en: 'According to [[42](#bib.bib42)], the networks pre-trained on natural images
    can be leveraged via two different ways: by utilizing them as fixed feature extractors,
    and as an initialization which will then be fine-tuned on target medical datasets.
    These two strategies are illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating
    Knowledge from Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")(a) and Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating Knowledge from
    Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis ‣ A Survey on
    Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")(b),
    respectively.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[42](#bib.bib42)]，可以通过两种不同方式利用在自然图像上预训练的网络：作为固定特征提取器，或作为初始化，然后在目标医疗数据集上进行微调。这两种策略分别在图
    [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating Knowledge from Natural Datasets or Other
    Medical Datasets ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")(a) 和图 [5](#S2.F5 "Figure 5 ‣ 2.2
    Incorporating Knowledge from Natural Datasets or Other Medical Datasets ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")(b) 中展示。
- en: '![Refer to caption](img/74fb2fd2d81cd8d79d45403b8cddcb93.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74fb2fd2d81cd8d79d45403b8cddcb93.png)'
- en: 'Figure 5: Two strategies to utilize the pre-trained network on natural images:
    (a) as a feature extractor and (b) as an initialization which will be fine-tuned
    on the target dataset.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 利用在自然图像上预训练的网络的两种策略：(a) 作为特征提取器，(b) 作为初始化，在目标数据集上进行微调。'
- en: The first strategy takes a pre-trained network, removes its last fully-connected
    layer, and treats the rest of the network as a fixed feature extractor. Extracted
    features are then fed into a linear classifier (e.g., support vector machine (SVM)),
    which is trained on the target medical datasets. Applications in this category
    include mammography mass lesion classification [[61](#bib.bib61)] and chest pathology
    identification [[55](#bib.bib55)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个策略是取一个预训练的网络，移除其最后一个全连接层，并将其余部分作为固定特征提取器。提取的特征然后输入到线性分类器（如支持向量机（SVM））中，该分类器在目标医疗数据集上进行训练。此类应用包括乳腺X光检查肿块分类[[61](#bib.bib61)]和胸部病理识别[[55](#bib.bib55)]。
- en: The success of leveraging information from natural images for disease diagnosis
    can be attributed to the fact that a network pre-trained on natural images, especially
    in the earlier layers, contain more generic features (e.g., edge detectors and
    color blob detectors) [[102](#bib.bib102)].
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 利用自然图像信息进行疾病诊断的成功可以归因于网络在自然图像上预训练的事实，特别是在早期层中，包含了更多的通用特征（例如，边缘检测器和颜色斑块检测器）[[102](#bib.bib102)]。
- en: In the second strategy, the weights of the pre-trained network are fine-tuned
    based on the medical datasets. It is possible to fine-tune the weights of all
    layers in the network, or to keep some of the earlier layers fixed and only fine-tune
    some higher-level portion of the network. This can be applied to the classification
    of skin cancer [[1](#bib.bib1)], breast cancer [[57](#bib.bib57)], thorax diseases
    [[58](#bib.bib58)], prostate cancer [[60](#bib.bib60)] and interstitial lung diseases
    [[59](#bib.bib59)] .
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种策略中，预训练网络的权重根据医疗数据集进行微调。可以微调网络中所有层的权重，或者保持一些早期层不变，只微调网络中某些高级部分。这可以应用于皮肤癌[[1](#bib.bib1)]、乳腺癌[[57](#bib.bib57)]、胸部疾病[[58](#bib.bib58)]、前列腺癌[[60](#bib.bib60)]和间质性肺疾病[[59](#bib.bib59)]的分类。
- en: Besides the information from natural images, using images from other medical
    datasets is also quite popular.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自然图像的信息，使用来自其他医疗数据集的图像也相当流行。
- en: Medical datasets containing images of the same or similar modality as target
    images have similar distribution and therefore can be helpful. For example, to
    classify malignant and benign breast masses in digitized screen-film mammograms
    (SFMs), a multi-task transfer learning DCNN is proposed to incorporate the information
    from digital mammograms (DMs) [[63](#bib.bib63)]. It is found to have significantly
    higher performance compared to the single-task transfer learning DCNN which only
    utilizes SFMs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 含有与目标图像相同或相似模态的图像的医疗数据集具有相似的分布，因此可以提供帮助。例如，为了在数字化的乳腺X光筛查图像（SFM）中分类恶性和良性乳腺肿块，提出了一种多任务迁移学习的DCNN，该模型结合了数字乳腺图像（DM）的信息[[63](#bib.bib63)]。与仅使用SFM的单任务迁移学习DCNN相比，发现其性能显著提高。
- en: In addition, even medical images with different modalities can provide complementary
    information. For example, [[26](#bib.bib26)] uses a model pre-trained on a mammography
    dataset to show that it could obtain better results than models trained solely
    on the target dataset comprising digital breast tomosynthesis (DBT) images. Another
    example is in prostate cancer classification, where the radiofrequency ultrasound
    images are first used to train the DCNN, then the model is fine-tuned on B-mode
    ultrasound images [[64](#bib.bib64)]. Other examples of using the images from
    different modalities can be found in [[62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使是不同模态的医疗图像也可以提供互补信息。例如，[[26](#bib.bib26)]使用在乳腺X光检查数据集上预训练的模型，展示了该模型能够获得比仅在包含数字乳腺断层摄影（DBT）图像的目标数据集上训练的模型更好的结果。另一个例子是在前列腺癌分类中，首先使用射频超声图像训练深度卷积神经网络（DCNN），然后在B模式超声图像上微调模型[[64](#bib.bib64)]。使用不同模态图像的其他例子可以在[[62](#bib.bib62)、[65](#bib.bib65)、[66](#bib.bib66)]中找到。
- en: Furthermore, as datasets of different classes can help each other in classification
    tasks [[103](#bib.bib103)], medical datasets featuring images of a variety of
    diseases can also have similar morphological structures or distribution, which
    may be beneficial for other tasks. For example, a multi-task deep learning (MTDL)
    method is proposed in [[68](#bib.bib68)]. MTDL can simultaneously utilize multiple
    cancer datasets so that hidden representations among these datasets can provide
    more information to small-scale cancer datasets, and enhance the classification
    performance. Another example is a cross-disease attention network (CANet) proposed
    in [[67](#bib.bib67)]. CANet characterizes and leverages the relationship between
    diabetic retinopathy (DR) and diabetic macular edema (DME) in fundus images using
    a special designed disease-dependent attention module. Experimental results on
    two public datasets show that CANet outperforms other methods on diagnosing both
    of the two diseases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于不同类别的数据集可以在分类任务中互相帮助[[103](#bib.bib103)]，具有多种疾病图像的医学数据集也可能具有类似的形态结构或分布，这对其他任务可能是有益的。例如，[[68](#bib.bib68)]中提出了一种多任务深度学习（MTDL）方法。MTDL可以同时利用多个癌症数据集，以便这些数据集之间的隐藏表示可以为小规模癌症数据集提供更多信息，从而增强分类性能。另一个例子是[[67](#bib.bib67)]中提出的跨疾病注意网络（CANet）。CANet利用特殊设计的疾病相关注意模块来表征和利用糖尿病视网膜病变（DR）和糖尿病黄斑水肿（DME）在眼底图像中的关系。对两个公开数据集的实验结果表明，CANet在诊断这两种疾病方面优于其他方法。
- en: 2.3 Incorporating Knowledge from Medical Doctors
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 融入医生知识
- en: Experienced medical doctors can give fairly accurate conclusion on the given
    medical images, mainly thanks to the training they have received and the expertise
    they have accumulated over many years. In general, they often follow some certain
    patterns or take some procedures when reading medical images. Incorporating these
    types of knowledge can improve the diagnostic performance of deep learning models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 经验丰富的医生可以对给定的医学图像给出相当准确的结论，这主要得益于他们所接受的培训和多年来积累的专业知识。一般来说，他们在阅读医学图像时常常遵循某些特定的模式或采取某些程序。将这些类型的知识融入深度学习模型中，可以提高诊断性能。
- en: 'The types of medical domain knowledge utilized in deep learning models for
    disease diagnosis can be summarized into the following five categories:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 用于疾病诊断的深度学习模型中利用的医学领域知识可以总结为以下五类：
- en: '1.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 培训模式，
- en: '2.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general diagnostic patterns they view images,
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像的一般诊断模式，
- en: '3.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the areas on which they usually focus,
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们通常关注的领域，
- en: '4.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: the features (e.g., characteristics, structures, shapes) they give special attention
    to, and
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们特别关注的特征（例如，特性、结构、形状），以及
- en: '5.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: other related information for diagnosis.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他与诊断相关的信息。
- en: The research works for each category will be described in the following sections.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每一类的研究工作将在以下部分中描述。
- en: 2.3.1 Training Pattern of Medical Doctors
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 医生的培训模式
- en: 'The training process of medical students has a character: they are trained
    by tasks with increasing difficulty. For example, students begin with some easier
    tasks, such as deciding whether an image contains lesions, later are required
    to accomplish more challenging tasks, such as determining whether the lesions
    are benign or malignant. Over time, they will advance to more difficult tasks,
    such as determining the subtypes of lesions in images.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 医学生的培训过程具有一个特点：他们通过逐渐增加难度的任务进行训练。例如，学生们从一些较简单的任务开始，比如判断图像中是否有病变，然后逐步要求完成更具挑战性的任务，如判断病变是良性还是恶性。随着时间的推移，他们将会逐渐进入更难的任务，如判断图像中病变的亚型。
- en: This training pattern can be introduced in the training process of deep neural
    networks via curriculum learning [[104](#bib.bib104)]. Curriculum determines a
    sequence of training samples ranked in ascending order of learning difficulty.
    Curriculum learning has been an active research topic in computer vision and has
    been recently utilized for medical image diagnosis.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这种培训模式可以通过课程学习[[104](#bib.bib104)]引入深度神经网络的训练过程中。课程学习决定了一系列按学习难度递增排序的训练样本。课程学习已成为计算机视觉领域的一个活跃研究课题，并且最近被用于医学图像诊断。
- en: For example, a teacher-student curriculum learning strategy is proposed for
    breast screening classification from DCE-MRI [[28](#bib.bib28)]. The deep learning
    model is trained on simpler tasks before introducing the hard problem of malignancy
    detection. This strategy shows the better performance when compared with the other
    methods.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，针对DCE-MRI的乳腺筛查分类，提出了一种教师-学生课程学习策略 [[28](#bib.bib28)]。在引入恶性肿瘤检测的困难问题之前，深度学习模型在较简单的任务上进行训练。这一策略与其他方法相比，表现出了更好的性能。
- en: Similarly, [[29](#bib.bib29)] presents a CNN based attention-guided curriculum
    learning framework by leveraging the severity-level attributes mined from radiology
    reports. Images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN to boost the learning process gradually.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，[[29](#bib.bib29)] 通过利用从放射学报告中挖掘的严重程度属性，提出了一种基于CNN的注意力引导课程学习框架。按照难度（按不同的严重程度分组）排序的图像被输入到CNN中，以逐步提升学习过程。
- en: In [[69](#bib.bib69)], the curriculum learning is adopted to support the classification
    of proximal femur fracture from X-ray images. The approach assigns a degree of
    difficulty to each training sample. By first learning ‘easy’ examples and then
    ‘hard’ ones, the model can reach a better performance even with fewer data. Other
    examples of using curriculum learning for disease diagnosis can be found in [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)].
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[69](#bib.bib69)] 中，课程学习被用于支持从X光图像中分类近端股骨骨折。这种方法为每个训练样本分配了难度等级。通过首先学习“简单”示例，然后学习“困难”示例，即使数据量较少，模型也能达到更好的性能。其他使用课程学习进行疾病诊断的例子可以在
    [[70](#bib.bib70)、[71](#bib.bib71)、[72](#bib.bib72)、[73](#bib.bib73)、[74](#bib.bib74)]
    中找到。
- en: 2.3.2 General Diagnostic Patterns of Medical Doctors
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 医生的一般诊断模式
- en: Experienced medical doctors generally follow some patterns when they read medical
    images. These patterns can be integrated into deep learning models with appropriately
    modified architectures.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的医生在阅读医学图像时通常遵循一些模式。这些模式可以通过适当修改架构将其集成到深度学习模型中。
- en: 'For example, radiologists generally follow a three-staged approach when they
    read chest X-ray images: first browsing the whole image, then concentrating on
    the local lesion areas, and finally combining the global and local information
    to make decisions. This pattern is incorporated in the architecture design of
    the network for thorax disease classification [[37](#bib.bib37)] (see Fig. [6](#S2.F6
    "Figure 6 ‣ 2.3.2 General Diagnostic Patterns of Medical Doctors ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")). The proposed
    network has three branches, one is used to view the whole image, the second for
    viewing the local areas, and the third one for combining the global and local
    information together. The network yields state-of-the-art accuracy on the ChestX-ray14
    dataset. In addition, besides the information from the whole image and local lesion
    area, the information from lung area is also leveraged in [[76](#bib.bib76)].
    In particular, a segmentation subnetwork is first used to locate the lung area
    from the whole image, and then lesion areas are generated by using an attention
    heatmap. Finally, the most discriminative features are fused for final disease
    prediction. Another example is a Dual-Ray Net proposed to deal with the front
    and lateral chest radiography simultaneously [[77](#bib.bib77)], which also mimics
    the reading pattern of radiologists.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，放射科医生在阅读胸部X光图像时通常遵循三阶段的方法：首先浏览整个图像，然后集中注意局部病变区域，最后将全局和局部信息结合起来做出决策。这种模式被纳入了用于胸部疾病分类的网络架构设计中
    [[37](#bib.bib37)]（参见图 [6](#S2.F6 "图 6 ‣ 2.3.2 医生的一般诊断模式 ‣ 2.3 融入医生知识 ‣ 2 疾病诊断
    ‣ 深度学习在医学图像分析中的领域知识融入综述")）。所提出的网络有三个分支，一个用于查看整个图像，第二个用于查看局部区域，第三个用于将全局和局部信息结合起来。该网络在ChestX-ray14数据集上达到了最先进的准确率。此外，除了全图和局部病变区域的信息外，[[76](#bib.bib76)]
    还利用了肺部区域的信息。特别地，首先使用分割子网络从整个图像中定位肺部区域，然后通过使用注意力热图生成病变区域。最后，融合最具区分性的特征以进行最终的疾病预测。另一个例子是为同时处理前面和侧面胸部X光片而提出的Dual-Ray
    Net [[77](#bib.bib77)]，它也模拟了放射科医生的阅读模式。
- en: '![Refer to caption](img/63db055ed18834f9dc2592a9c8e11944.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/63db055ed18834f9dc2592a9c8e11944.png)'
- en: 'Figure 6: The example of leveraging the diagnostic pattern of radiologists
    for thorax disease diagnosis [[37](#bib.bib37)], where three branches are used
    to extract and combine the global and local features.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：利用放射科医生的诊断模式进行胸部疾病诊断的示例[[37](#bib.bib37)]，其中使用三个分支提取和结合全局与局部特征。
- en: 'In the diagnosis of skin lesions, experienced dermatologists generally first
    locate lesions, then identify dermoscopic features from the lesion areas, and
    finally make diagnosis based on the features. This pattern is mimicked in the
    design of the network for the diagnosis of skin lesions [[75](#bib.bib75)]. The
    proposed network, DermaKNet, comprised several subnetworks with dedicated tasks:
    lesion-skin segmentation, detection of dermoscopic features, and global lesion
    diagnosis. The DermaKNet achieves higher performance compared to the traditional
    CNN models.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在皮肤病变诊断中，经验丰富的皮肤科医生通常首先定位病变，然后从病变区域识别皮肤镜特征，最后根据这些特征进行诊断。这种模式在皮肤病变诊断的网络设计中得到了模拟[[75](#bib.bib75)]。所提出的网络DermaKNet包括几个具有专门任务的子网络：病变-皮肤分割、皮肤镜特征检测和全局病变诊断。DermaKNet相比传统的卷积神经网络模型表现出更高的性能。
- en: In addition, in mass identification in mammogram, radiologists generally analyze
    the bilateral and ipsilateral views simultaneously. To emulate this reading practice,
    [[78](#bib.bib78)] proposes MommiNet to perform end-to-end bilateral and ipsilateral
    analysis of mammogram images. In addition, symmetry and geometry constraints are
    also aggregated from these views. Experiments show the state-of-the-art mass identification
    accuracy on DDSM. Another example of leveraging this diagnostic pattern of medical
    doctors can be found in skin lesion diagnosis and thorax disease classification
    [[79](#bib.bib79)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在乳腺X光图像的质量识别中，放射科医生通常会同时分析双侧和同侧视图。为了模拟这种阅读习惯，[[78](#bib.bib78)]提出了MommiNet来进行乳腺X光图像的端到端双侧和同侧分析。此外，还从这些视图中聚合了对称性和几何约束。实验表明在DDSM上达到了最先进的肿块识别准确率。另一个利用这种医疗医生诊断模式的示例可以在皮肤病变诊断和胸部疾病分类中找到[[79](#bib.bib79)]。
- en: 2.3.3 The Areas Medical Doctors Usually Focus on
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 医疗医生通常关注的区域
- en: When experienced medical doctors read images, they generally focus on a few
    specific areas, as these areas are more informative than other places for the
    purpose of disease diagnosis. Therefore, the information about where medical doctors
    focus may help deep learning models yield better results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当经验丰富的医生阅读图像时，他们通常会集中注意于几个特定区域，因为这些区域在疾病诊断中比其他地方更具信息性。因此，关于医生关注区域的信息可能有助于深度学习模型取得更好的结果。
- en: The knowledge above is generally represented as ‘attention maps’, which are
    annotations given by medical doctors indicating the areas they focus on when reading
    images. For example, a CNN named AG-CNN explicitly incorporates the ‘attention
    maps’ for glaucoma diagnosis [[38](#bib.bib38)]. The attention maps of ophthalmologists
    are collected through a simulated eye-tracking experiment, which are used to indicate
    where they focus when reading images. An example of capturing the attention maps
    of an ophthalmologist in glaucoma diagnosis is shown in Fig. [7](#S2.F7 "Figure
    7 ‣ 2.3.3 The Areas Medical Doctors Usually Focus on ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). To incorporate the
    attention maps, an attention prediction subnet in AG-CNN is designed, and the
    attention prediction loss measuring the difference between the generated and ground
    truth attention maps (provided by ophthalmologists) is utilized to supervise the
    training process. Experimental results show that AG-CNN significantly outperforms
    the state-of-the-art glaucoma detection methods.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述知识通常被表示为“注意力图”，这是医疗医生在阅读图像时指出他们关注区域的注释。例如，一个名为AG-CNN的卷积神经网络明确地融入了用于青光眼诊断的“注意力图”[[38](#bib.bib38)]。眼科医生的注意力图通过模拟眼动实验收集，用于指示他们在阅读图像时关注的地方。青光眼诊断中捕捉眼科医生注意力图的一个示例见图[7](#S2.F7
    "Figure 7 ‣ 2.3.3 The Areas Medical Doctors Usually Focus on ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")。为了融入注意力图，AG-CNN中设计了一个注意力预测子网，并利用测量生成的和真实注意力图（由眼科医生提供）之间差异的注意力预测损失来监督训练过程。实验结果表明，AG-CNN显著超越了最先进的青光眼检测方法。
- en: '![Refer to caption](img/63abb68645f11e9014fbdd75d5005420.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/63abb68645f11e9014fbdd75d5005420.png)'
- en: 'Figure 7: Example of capturing the attention maps of an ophthalmologist in
    glaucoma diagnosis [[38](#bib.bib38)]. I, II, III and IV are the original blurred
    fundus image, the fixations of ophthalmologists with cleared regions, the order
    of clearing the blurred regions, and the generated attention map based on the
    captured fixations, respectively. V and VII represent the original fundus images.
    VI and VIII are the corresponding attention maps of V and VII generated by using
    the method in I-IV.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：眼科医生在青光眼诊断中捕捉注意力图的示例[[38](#bib.bib38)]。I、II、III和IV分别为原始模糊视网膜图像、眼科医生的注视区域、清晰区域的顺序以及基于捕获的注视生成的注意力图。V和VII表示原始视网膜图像。VI和VIII是使用I-IV方法生成的V和VII的相应注意力图。
- en: Another example in this category is the lesion-aware CNN (LACNN) for the classification
    of retinal optical coherence tomography (OCT) images [[80](#bib.bib80)]. The LACNN
    simulates the pattern of ophthalmologists’ diagnosis by focusing on local lesion-related
    regions. Concretely, the ‘attention maps’ are firstly represented as the annotated
    OCT images delineating the lesion regions using bounding polygons. To incorporate
    the information, the LACNN proposes a lesion-attention module to enhance the features
    from local lesion-related regions while still preserving the meaningful structures
    in global OCT images. Experimental results on two clinically acquired OCT datasets
    demonstrate the effectiveness of introducing attention maps for retinal OCT image
    classification, with 8.3% performance gain when compared with the baseline method.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类别中的另一个例子是用于视网膜光学相干断层扫描（OCT）图像分类的病灶感知卷积神经网络（LACNN）[[80](#bib.bib80)]。LACNN通过关注局部病灶相关区域，模拟眼科医生的诊断模式。具体来说，‘注意力图’首先表示为标注了病灶区域的OCT图像，使用了边界多边形。为了融入这些信息，LACNN提出了一个病灶注意力模块，以增强来自局部病灶相关区域的特征，同时保持全局OCT图像中的有意义结构。对两个临床获得的OCT数据集的实验结果表明，引入注意力图在视网膜OCT图像分类中具有有效性，与基线方法相比，性能提升了8.3%。
- en: Furthermore, [[53](#bib.bib53)] proposes an Attention Branch Network (ABN) to
    incorporate the knowledge given by the radiologists in diabetic retinopathy. ABN
    introduces a branch structure which generates attention maps that highlight the
    attention regions of the network. During the training process, ABN allows the
    attention maps to be modified with semantic segmentation labels of disease regions.
    The semantic labels are also annotated by radiologists as the ground truth attention
    maps. Experimental results on the disease grade recognition of retina images show
    that ABN achieves 93.73% classification accuracy and its interpretability is clearer
    than conventional approaches.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[53](#bib.bib53)]提出了一种注意力分支网络（ABN），以融入放射科医生在糖尿病视网膜病变中的知识。ABN引入了一个分支结构，生成突出网络注意区域的注意力图。在训练过程中，ABN允许将注意力图与疾病区域的语义分割标签进行修改。语义标签也由放射科医生注释为真实的注意力图。对视网膜图像疾病等级识别的实验结果表明，ABN实现了93.73%的分类准确率，其可解释性比传统方法更清晰。
- en: Other examples of incorporating attention maps of medical doctors can be found
    in the diagnosis of radiotherapy-related esophageal fistula [[81](#bib.bib81)],
    breast cancer diagnosis [[82](#bib.bib82)], and short-term lesion change detection
    in melanoma screening [[83](#bib.bib83)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其他将医学专家的注意力图纳入诊断的例子包括放射治疗相关食管瘘的诊断[[81](#bib.bib81)]、乳腺癌诊断[[82](#bib.bib82)]以及黑色素瘤筛查中的短期病灶变化检测[[83](#bib.bib83)]。
- en: 2.3.4 Features That Medical Doctors Give Special Attention to
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 医学专家特别关注的特征
- en: In the last decades, many guidelines and rules have gradually developed in various
    medical fields to point out some important features for diagnosis. These features
    are called *‘hand-crafted features’* as they are designated by medical doctors.
    For example, the popular ABCD rule [[105](#bib.bib105)] is widely used by dermatologists
    to classify melanocytic tumors. The ABCD rule points out four distinguishing features,
    namely asymmetry, border, color and differential structures, to determine whether
    a melanocytic skin lesion under the investigation is benign or malignant.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，各种医学领域逐渐制定了许多指南和规则，以指出一些重要的诊断特征。这些特征被称为*‘手工特征’*，因为它们是由医学专家指定的。例如，广泛使用的ABCD规则[[105](#bib.bib105)]被皮肤科医生用来分类黑色素瘤。ABCD规则指出了四个区分特征，即不对称性、边界、颜色和差异结构，以确定待检查的黑色素瘤皮肤病变是良性还是恶性。
- en: 'TABLE III: Features in the BI-RADS guideline to classify benign and malignant
    breast tumors in ultrasound images [[106](#bib.bib106)]'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：BI-RADS指南中用于分类乳腺超声图像中良性和恶性肿瘤的特征[[106](#bib.bib106)]
- en: '|  | Benign | Malignant |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 良性 | 恶性 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Margin | smooth, thin, regular | irregular, thick |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 边缘 | 光滑、细、规则 | 不规则、粗 |'
- en: '| Shape | round or oval | irregular |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 形状 | 圆形或椭圆形 | 不规则 |'
- en: '| Microcalcification | no | yes |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 微钙化 | 无 | 有 |'
- en: '| Echo Pattern | clear | unclear |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 回声模式 | 清晰 | 不清晰 |'
- en: '| Acoustic Attenuation | not obvious | obvious |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 声学衰减 | 不明显 | 明显 |'
- en: '| Side Acoustic Shadow | obvious | not obvious |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 侧声学阴影 | 明显 | 不明显 |'
- en: Another example is in the field of breast cancer diagnosis. Radiologists use
    the BI-RADS (Breast Imaging Reporting and Data System) score [[106](#bib.bib106)]
    to place abnormal findings into different categories, with a score of 1 indicating
    negative findings and a score of 6 indicating breast cancer. More importantly,
    for each imaging modality, BI-RADS indicates some features closely related to
    its scores, including margin, shape, micro-calcification, and echo pattern. For
    example, for breast ultrasound images, tumors with smooth, thin and regular margins
    are more likely to be benign, while tumors with irregular and thick margins are
    highly suspected to be malignant. Other features that can help to classify benign
    and malignant breast tumors are shown in Table [III](#S2.T3 "TABLE III ‣ 2.3.4
    Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). Similarly, for the
    benign-malignant risk assessment of lung nodules in [[59](#bib.bib59)], six high-level
    nodule features, including calcification, sphericity, margin, lobulation, spiculation
    and texture, have shown a tightly connection with malignancy scores (see Fig.
    [8](#S2.F8 "Figure 8 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是乳腺癌诊断领域。放射科医生使用BI-RADS（乳腺影像报告和数据系统）评分[[106](#bib.bib106)]将异常发现分为不同类别，其中评分1表示阴性发现，评分6表示乳腺癌。更重要的是，对于每种成像方式，BI-RADS指示与其评分密切相关的一些特征，包括边缘、形状、微钙化和回声模式。例如，对于乳腺超声图像，具有光滑、细且规则边缘的肿瘤更可能是良性的，而具有不规则和粗边缘的肿瘤则高度怀疑为恶性。其他可以帮助分类良性和恶性乳腺肿瘤的特征见表[III](#S2.T3
    "TABLE III ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")。类似地，对于[[59](#bib.bib59)]中肺结节的良恶性风险评估，六种高级结节特征，包括钙化、球形度、边缘、分叶、刺状和质地，显示出与恶性评分的紧密关联（见图[8](#S2.F8
    "Figure 8 ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")）。
- en: '![Refer to caption](img/26456a4a8677eac2c4f9c3d2f1c2d605.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/26456a4a8677eac2c4f9c3d2f1c2d605.png)'
- en: 'Figure 8: Lung nodule attributes with different malignancy scores [[59](#bib.bib59)].(a)
    From top to the bottom, six different nodule features attribute from missing to
    highest prominence. (b) The number of nodules with different malignancy scores.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同恶性评分的肺结节特征[[59](#bib.bib59)]。(a) 从上到下，六种不同的结节特征从缺失到最高突出性。(b) 不同恶性评分的结节数量。
- en: These different kinds of hand-crafted features have been widely used in many
    traditional CAD systems. These systems generally first extract these features
    from medical images, and then feed them into some classifiers like SVM or Random
    Forest [[107](#bib.bib107), [108](#bib.bib108)]. For example, for the lung nodule
    classification on CT images, many CAD systems utilize features including the size,
    shape, morphology, and texture from the suspected lesion areas [[109](#bib.bib109),
    [54](#bib.bib54)]. Similarly, in the CAD systems for the diagnosis of breast ultrasound
    images, features such as intensity, texture and shape are used [[31](#bib.bib31)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同类型的手工特征已被广泛应用于许多传统的CAD系统。这些系统通常首先从医学图像中提取这些特征，然后将其输入一些分类器，如SVM或随机森林[[107](#bib.bib107),
    [108](#bib.bib108)]。例如，对于CT图像上的肺结节分类，许多CAD系统利用包括大小、形状、形态和质地等特征来处理可疑病变区域[[109](#bib.bib109),
    [54](#bib.bib54)]。类似地，在乳腺超声图像的CAD系统中，使用了强度、质地和形状等特征[[31](#bib.bib31)]。
- en: When using deep learning models like CNNs, which have the ability to automatically
    extract representative features, there are four approaches to combining ‘hand-crafted
    features’ with features extracted from CNNs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用具有自动提取代表性特征能力的深度学习模型（如CNN）时，有四种方法将“手工设计的特征”与CNN提取的特征结合起来。
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Decision-level fusion: the two types of features are fed into two classifiers
    separately, then the decisions from the two classifiers are combined.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策级融合：这两种特征分别输入到两个分类器中，然后将两个分类器的决策进行结合。
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature-level fusion: the two types of features are directly combined via techniques
    such as concatenation.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征级融合：这两种特征通过连接等技术直接组合。
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input-level fusion: the hand-crafted features are represented as image patches,
    which are then taken as inputs to the CNNs.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入级融合：手工设计的特征被表示为图像补丁，然后作为输入提供给CNN。
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Usage of features as labels of CNNs: the hand-crafted features are first annotated
    and then utilized as labels for CNNs during the training process.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征作为CNN标签的使用：手工设计的特征首先被标注，然后在训练过程中作为CNN的标签使用。
- en: 'Decision-level fusion: The structure of this approach is illustrated in Fig.
    [9](#S2.F9 "Figure 9 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"). In this approach, the hand-crafted features and the features extracted
    from CNNs are separately fed into two classifiers. Then, the classification results
    from both classifiers are combined using some decision fusion techniques to produce
    final classification results.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 决策级融合：这种方法的结构如图[9](#S2.F9 "Figure 9 ‣ 2.3.4 Features That Medical Doctors Give
    Special Attention to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")所示。在这种方法中，手工设计的特征和从CNN提取的特征分别输入到两个分类器中。然后，使用一些决策融合技术将两个分类器的分类结果结合起来，以生成最终的分类结果。
- en: '![Refer to caption](img/4551553557c5bd87c2bdb9ec55a257d3.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/4551553557c5bd87c2bdb9ec55a257d3.png)'
- en: 'Figure 9: Decision-level fusion: the decisions from two classifiers, one based
    on hand-crafted features, and the other on the CNNs, are combined.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：决策级融合：来自两个分类器的决策，一个基于手工设计的特征，另一个基于CNN，进行组合。
- en: For example, a skin lesion classification model proposed in [[85](#bib.bib85)]
    combines the results from two SVM classifiers. The first one uses hand-crafted
    features (i.e., RSurf features and local binary patterns (LBP)) as input and the
    second employs features derived from a CNN. Both of the classifiers predict the
    category for each tested image with a classification score. These scores are subsequently
    used to determine the final classification result.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[85](#bib.bib85)]中提出的皮肤病变分类模型结合了两个SVM分类器的结果。第一个使用手工设计的特征（即RSurf特征和局部二值模式（LBP））作为输入，第二个使用从CNN提取的特征。这两个分类器都为每个测试图像预测类别并给出分类得分。这些得分随后用于确定最终分类结果。
- en: Similarly, a mammographic tumor classification method also combines features
    in decision-level fusion [[61](#bib.bib61)]. After individually performing classification
    with CNN features and analytically extracted features (e.g., contrast, texture,
    and margin spiculation), the method adopts the soft voting to combine the outputs
    from both individual classifiers. Experimental results show that the performance
    of the ensemble classifier was significantly better than the individual ones.
    Other examples that utilize this approach include lung nodule diagnosis [[86](#bib.bib86)],
    breast cancer diagnosis [[87](#bib.bib87)], the classification of cardiac slices
    [[84](#bib.bib84)] and the prediction of the invasiveness risk of stage-I lung
    adenocarcinomas [[88](#bib.bib88)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，乳腺肿瘤分类方法也在决策级融合中结合特征[[61](#bib.bib61)]。在分别使用CNN特征和分析提取的特征（如对比度、纹理和边缘刺状）进行分类后，该方法采用软投票来结合两个独立分类器的输出。实验结果表明，集成分类器的性能显著优于单个分类器。其他采用这种方法的例子包括肺结节诊断[[86](#bib.bib86)]、乳腺癌诊断[[87](#bib.bib87)]、心脏切片分类[[84](#bib.bib84)]以及I期肺腺癌侵袭风险预测[[88](#bib.bib88)]。
- en: 'Feature-level fusion: In this approach, hand-crafted features and features
    extracted from CNNs are concatenated, and the combined features are fed into a
    classifier for diagnosis. The structure of this approach is illustrated in Fig.
    [10](#S2.F10 "Figure 10 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 特征级融合：在这种方法中，将手工设计的特征与从CNN中提取的特征进行拼接，然后将组合特征输入到分类器中进行诊断。该方法的结构如图 [10](#S2.F10
    "图 10 ‣ 2.3.4 医学医生特别关注的特征 ‣ 2.3 融合医学医生的知识 ‣ 2 疾病诊断 ‣ 融合领域知识到深度学习中的调查") 所示。
- en: '![Refer to caption](img/ee039724171545db44e50f3ba608fd97.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee039724171545db44e50f3ba608fd97.png)'
- en: 'Figure 10: Feature-level fusion: the hand-crafted features are combined with
    the features extracted from CNNs as the new feature vectors.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：特征级融合：手工设计的特征与从CNN中提取的特征结合，形成新的特征向量。
- en: For example, a combined-feature based classification approach called CFBC is
    proposed for lung nodule classification by [[36](#bib.bib36)]. In CFBC, the hand-crafted
    features (including texture and shape descriptors) and the features learned by
    a nine-layer CNN are combined and fed into a back-propagation neural network.
    Experimental results derived from a publicly available dataset show that compared
    with a purely CNN model, incorporating hand-crafted features improves the accuracy,
    sensitivity, and specificity by 3.87%, 6.41%, and 3.21%, respectively.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过[[36](#bib.bib36)]提出了一种基于组合特征的分类方法，称为CFBC，用于肺结节分类。在CFBC中，将手工设计的特征（包括纹理和形状描述符）与九层CNN学习到的特征结合，并输入到反向传播神经网络中。来自公开数据集的实验结果表明，与纯CNN模型相比，融合手工设计的特征可以分别提高准确率、敏感性和特异性3.87%、6.41%和3.21%。
- en: Another example in this category is the breast cancer classification in histology
    images [[57](#bib.bib57)]. More specifically, two hand-crafted features, namely
    the parameter-free threshold adjacency statistics and the gray-level co-occurrence
    matrix, are fused with the five groups of deep learning features extracted from
    five different deep models. The results show that after incorporating hand-crafted
    features, the accuracy of the deep learning model can be significantly improved.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的另一个例子是组织学图像中的乳腺癌分类 [[57](#bib.bib57)]。更具体地说，将两种手工设计的特征，即无参数阈值邻接统计量和灰度共生矩阵，与从五个不同深度模型中提取的五组深度学习特征融合在一起。结果表明，经过融合手工设计的特征后，深度学习模型的准确率可以显著提高。
- en: Other examples of employing feature-level fusion can be found in glaucoma diagnosis
    [[90](#bib.bib90)], skin lesion classification [[89](#bib.bib89)], lung nodule
    classification [[91](#bib.bib91)] and brain tumor diagnosis [[92](#bib.bib92)].
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其他应用特征级融合的例子包括青光眼诊断 [[90](#bib.bib90)]、皮肤病变分类 [[89](#bib.bib89)]、肺结节分类 [[91](#bib.bib91)]
    和脑肿瘤诊断 [[92](#bib.bib92)]。
- en: 'Input-level fusion: In this approach, hand-crafted features are first represented
    as patches highlighting the corresponding features. Then, these patches are taken
    as input to CNNs to make the final conclusion. Using this approach, the CNNs are
    expected to rely more on the hand-crafted features. It should be noted that generally
    speaking, one CNN is required for each type of hand-crafted feature, and information
    obtained from these CNNs need to be combined in some manner to make a final decision.
    The structure of this approach is illustrated in Fig. [11](#S2.F11 "Figure 11
    ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输入级融合：在这种方法中，手工设计的特征首先表示为突出对应特征的图像块。然后，这些图像块作为输入提供给CNN，以得出最终结论。使用这种方法，CNN预期会更多依赖于手工设计的特征。需要注意的是，一般来说，每种手工设计的特征需要一个CNN，并且从这些CNN获得的信息需要以某种方式结合起来，以做出最终决策。该方法的结构如图
    [11](#S2.F11 "图 11 ‣ 2.3.4 医学医生特别关注的特征 ‣ 2.3 融合医学医生的知识 ‣ 2 疾病诊断 ‣ 融合领域知识到深度学习中的调查")
    所示。
- en: '![Refer to caption](img/d7e1908a544551c52978c7518c8f5148.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7e1908a544551c52978c7518c8f5148.png)'
- en: 'Figure 11: Input-level fusion: the hand-crafted features are represented as
    image patches that are taken as inputs to the CNNs.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：输入级融合：手工设计的特征被表示为图像块，作为CNN的输入。
- en: For example, in [[94](#bib.bib94)], three types of hand-crafted features, namely
    the contrast information of the initial nodule candidates (INCs) and the outer
    environments, the histogram of oriented gradients (HOG) feature, and the LBP feature,
    are transformed into the corresponding patches and are taken as inputs of multiple
    CNNs. The results show that this approach outperforms both conventional CNN-based
    approaches and traditional machine-learning approaches based on hand-crafted features.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[94](#bib.bib94)]中，三种手工特征，即初步结节候选者（INCs）与外部环境的对比信息、方向梯度直方图（HOG）特征以及LBP特征，被转换为相应的补丁，并作为多个CNN的输入。结果表明，这种方法优于传统的基于CNN的方法以及基于手工特征的传统机器学习方法。
- en: 'Another example using input-level fusion approach is the MV-KBC (multi-view
    knowledge-based collaborative) algorithm proposed for lung nodule classification
    [[54](#bib.bib54)]. The MV-KBC first extracts patches corresponding to three features:
    the overall appearance (OA), nodule’s heterogeneity in voxel values (HVV) and
    heterogeneity in shapes (HS). Each type of patch is fed into a ResNet. Then, the
    outputs of these ResNets are combined to generate the final classification results.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用输入级融合方法的例子是用于肺结节分类的MV-KBC（多视角知识基础协作）算法[[54](#bib.bib54)]。MV-KBC首先提取与三种特征对应的补丁：整体外观（OA）、结节的体素值异质性（HVV）和形状异质性（HS）。每种类型的补丁被输入到ResNet中。然后，这些ResNet的输出被结合起来生成最终的分类结果。
- en: Moreover, [[93](#bib.bib93)] proposes the dual-path semi-supervised conditional
    generative adversarial networks (DScGAN) for thyroid classification. More specifically,
    the features from the ultrasound B-mode images and the ultrasound elastography
    images are first extracted as the OB patches (indicating the region of interest
    (ROI) in B-mode images), OS patches (characterizing the shape of the nodules),
    and OE patches (indicating the elasticity ROI according to the B-mode ROI position).
    Then, these patches are used as the input of the DScGAN. Using the information
    from these patches is demonstrated to be effective to improve the classification
    performance. Other examples employ input-level fusion can be found in thyroid
    nodule diagnosis [[95](#bib.bib95)] and breast cancer diagnosis on multi-sequence
    MRI [[96](#bib.bib96)].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[93](#bib.bib93)]提出了用于甲状腺分类的双路径半监督条件生成对抗网络（DScGAN）。具体来说，首先从超声B模式图像和超声弹性成像图像中提取特征，作为OB补丁（表示B模式图像中的感兴趣区域（ROI））、OS补丁（描述结节的形状）和OE补丁（根据B模式ROI位置指示弹性ROI）。然后，这些补丁作为DScGAN的输入。利用这些补丁中的信息被证明有效地提高了分类性能。其他使用输入级融合的方法可以在甲状腺结节诊断[[95](#bib.bib95)]和多序列MRI中的乳腺癌诊断[[96](#bib.bib96)]中找到。
- en: 'Usage of features as labels of CNNs: In this approach, besides the original
    classification labels of images, medical doctors also provide labels for some
    hand-crafted features. This extra information is generally incorporated into deep
    learning models via the multi-task learning structure.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 特征作为CNN标签的使用：在这种方法中，除了图像的原始分类标签外，医学专家还提供了一些手工特征的标签。这些额外的信息通常通过多任务学习结构纳入深度学习模型中。
- en: For example, in [[39](#bib.bib39)], the nodules in lung images were quantitatively
    rated by radiologists with regard to nine hand-crafted features (e.g., spiculation,
    texture, and margin). The multi-task learning framework is used to incorporate
    the above information into the main task of lung nodule classification.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[39](#bib.bib39)]中，放射科医生根据九种手工特征（如刺突、纹理和边缘）对肺图像中的结节进行了定量评级。多任务学习框架用于将上述信息纳入肺结节分类的主要任务中。
- en: In addition, for the benign-malignant risk assessment of lung nodules in low-dose
    CT scans [[59](#bib.bib59)], the binary labels about the presence of six high-level
    nodule attributes, namely the calcification, sphericity, margin, lobulation, spiculation
    and texture, are utilized. Six CNNs are designed and each one aims at learning
    one attribute. The discriminative features automatically learned by CNNs for these
    attributes are fused in a multi-task learning framework to obtain the final risk
    assessment scores.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于低剂量CT扫描中的肺结节良恶性风险评估[[59](#bib.bib59)]，利用了关于六种高级结节特征的二元标签，即钙化、球形度、边缘、分叶、刺突和纹理。设计了六个CNN，每个CNN旨在学习一个特征。CNN自动学习到的这些特征在多任务学习框架中融合，以获得最终的风险评估分数。
- en: Similarly, in [[97](#bib.bib97)], each glioma nuclear image is exclusively labeled
    in terms of the shapes and attributes for the centermost nuclei of the image.
    These features are then learned by a multi-task CNN. Experiments demonstrate that
    the proposed method outperforms the baseline CNN.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在[[97](#bib.bib97)]中，每个胶质瘤细胞核图像都以图像中心最核的形状和属性作为独家标记。然后这些特征通过多任务CNN进行学习。实验证明，所提出的方法优于基线CNN。
- en: 2.3.5 Other Types of Information Related to Diagnosis
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 与诊断相关的其他类型信息
- en: In this section, we summarize other types of information that can help deep
    learning models to improve their diagnostic performance. These types of information
    include extra category labels and clinical diagnostic reports.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了其他类型的信息，可以帮助深度学习模型提高其诊断性能。这些信息类型包括额外的类别标签和临床诊断报告。
- en: Extra category labels
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的类别标签
- en: For medical images, besides a classification label (i.e., normal, malignant
    or benign), radiologists may give some extra category labels. For example, in
    the ultrasonic diagnosis of breast cancer, an image usually has a BI-RADS label
    that classifies the image into 0$\sim$6 [[106](#bib.bib106)]—category 0 suggests
    re-examination, categories 1 and 2 indicate that it is prone to be a benign lesion,
    category 3 suggests probably benign findings, categories 4 and 5 are suspected
    to be malignant, category 6 definitely suggests malignant). These labels also
    contain information about the condition of lesions. In [[52](#bib.bib52)], the
    BI-RADS label for each medical image is incorporated in a multi-task learning
    structure as the label of an auxiliary task. Experimental results show that incorporating
    these BI-RADS labels can improve the diagnostic performance of major classification
    task. Another example of using the information from BI-RADS labels can be found
    in [[100](#bib.bib100)].
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学图像，除了分类标签（即正常、恶性或良性），放射科医师可能会给出一些额外的类别标签。例如，在乳腺癌超声诊断中，一张图像通常具有BI-RADS标签，将图像分类为0$\sim$6
    [[106](#bib.bib106)] ——0类建议重新检查，1和2类表示可能是良性病变，3类表示可能是良性发现，4和5类被怀疑恶性，6类绝对被认为是恶性）。这些标签还包含有关病变状态的信息。在[[52](#bib.bib52)]中，每幅医学图像的BI-RADS标签被纳入多任务学习结构作为辅助任务的标签。实验结果表明，纳入这些BI-RADS标签可以提高主要分类任务的诊断性能。另一个利用BI-RADS标签信息的例子可以在[[100](#bib.bib100)]中找到。
- en: In addition, during the process of image annotation, consensus or disagreement
    among experts regarding images reflects the gradeability and difficulty levels
    of the image, which is also a representation of medical domain knowledge. To incorporate
    this information,[[101](#bib.bib101)] proposes a multi-branch model structure
    to predict the most sensitive, most specifical and a balanced fused result for
    glaucoma images. Meanwhile, the consensus loss is also used to encourage the sensitivity
    and specificity branch to generate consistent and contradictory predictions for
    images with consensus and disagreement labels, respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在图像注释过程中，专家之间关于图像的一致性或不一致性反映出图像的可分级性和难度级别，这也是医学领域知识的一种表现。为了整合这些信息，[[101](#bib.bib101)]提出了一种多分支模型结构，用于预测青光眼图像最敏感、最特异和平衡融合的结果。与此同时，一致性损失也被用来鼓励敏感性和特异性分支对具有一致性和不一致性标签的图像生成一致和矛盾的预测。
- en: Extra clinical diagnostic reports
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 附加临床诊断报告
- en: A clinical report is a summary of all the clinical findings and impressions
    determined during examination of a radiography study. It usually contains rich
    information and reflects the findings and descriptions of medical doctors. Incorporating
    clinical reports into CNNs designed for disease diagnosis is typically beneficial.
    As medical reports are generally handled by recurrent neural networks (RNNs),
    to incorporate information from medical reports, generally hybrid networks containing
    both CNNs and RNNs are needed.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 临床报告是对放射学研究检查期间确定的所有临床发现和印象的总结。它通常包含丰富的信息，反映了医生的发现和描述。将临床报告纳入设计用于疾病诊断的CNNs通常是有益的。由于医疗报告通常由递归神经网络（RNNs）处理，因此通常需要包含既包含CNNs又包含RNNs的混合网络来整合来自医疗报告的信息。
- en: For example, the Text-Image embedding network (TieNet) is designed to classify
    the common thorax disease in chest X-rays [[98](#bib.bib98)]. TieNet has an end-to-end
    CNN-RNN architecture enabling it to integrate information of radiological reports.
    The classification results are significantly improved (with about a 6% increase
    on average in AUCs) compared with the baseline CNN purely based on medical images.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文本-图像嵌入网络 (TieNet) 旨在分类胸部 X 光片中的常见胸部疾病 [[98](#bib.bib98)]。TieNet 具有端到端的 CNN-RNN
    架构，使其能够整合放射报告的信息。与完全基于医学图像的基线 CNN 相比，分类结果显著提高（AUC 平均提高约 6%）。
- en: In addition, using semantic information from diagnostic reports is explored
    in [[99](#bib.bib99)] for understanding pathological bladder cancer images. A
    dual-attention model is designed to facilitate the high-level interaction of semantic
    information and visual information. Experiments demonstrate that incorporating
    information from diagnostic reports significantly improves the cancer diagnostic
    performance over the baseline method.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 [[99](#bib.bib99)] 中探讨了使用诊断报告中的语义信息来理解病理性膀胱癌图像。设计了一个双重注意力模型，以促进语义信息和视觉信息的高级交互。实验表明，整合诊断报告中的信息显著提高了癌症诊断性能，相较于基线方法。
- en: 2.4 Summary
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 总结
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for disease
    diagnosis. Generally, almost all types of domain knowledge are proven to be effective
    in boosting the diagnostic performance, especially using the metrics of accuracy
    and AUC, some examples and their quantitative improvements are listed in Table
    [IV](#S2.T4 "TABLE IV ‣ 2.4 Summary ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了不同类型的领域知识及其相应的整合方法，以应用于疾病诊断的深度学习模型中。一般而言，几乎所有类型的领域知识都被证明能有效提升诊断性能，特别是使用准确率和
    AUC 指标的情况下，表 [IV](#S2.T4 "TABLE IV ‣ 2.4 Summary ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")
    列出了部分示例及其定量改进。
- en: 'TABLE IV: The comparison of the quantitative metrics for some disease diagnosis
    methods after integrating domain knowledge'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 整合领域知识后的某些疾病诊断方法的定量指标比较'
- en: '| Reference |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/带领域知识 &#124;'
- en: '| Accuracy | AUC |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | AUC |'
- en: '| [[37](#bib.bib37)] |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bib37)] |'
- en: '&#124; AG-CNN only with &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅 AG-CNN &#124;'
- en: '&#124; global branch/AG-CNN &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全局分支/AG-CNN &#124;'
- en: '| –/– | 0.840/0.871 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.840/0.871 |'
- en: '| [[75](#bib.bib75)] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] |'
- en: '&#124; ResNet-50/DermaKNet &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50/DermaKNet &#124;'
- en: '| –/– | 0.874/0.908 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.874/0.908 |'
- en: '| [[62](#bib.bib62)] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] |'
- en: '&#124; Fine-tuned VGG-Net/ &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调的 VGG-Net/ &#124;'
- en: '&#124; Fine-tuned MG-Net &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调的 MG-Net &#124;'
- en: '| 0.900/0.930 | 0.950/0.970 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 0.900/0.930 | 0.950/0.970 |'
- en: '| [[89](#bib.bib89)] |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] |'
- en: '&#124; ResNet-50/ResNet-50 &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50/ResNet-50 &#124;'
- en: '&#124; with handcrafted features &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带手工特征 &#124;'
- en: '| –/– | 0.830/0.940 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.830/0.940 |'
- en: '| [[38](#bib.bib38)] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] |'
- en: '&#124; CNN without using &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未使用的 CNN &#124;'
- en: '&#124; attention map/AG-CNN &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力图/AG-CNN &#124;'
- en: '| 0.908/0.953 | 0.966/0.975 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 0.908/0.953 | 0.966/0.975 |'
- en: '| [[67](#bib.bib67)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] |'
- en: '&#124; ResNet50/CANet &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet50/CANet &#124;'
- en: '| 0.828/0.851 | –/– |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 0.828/0.851 | –/– |'
- en: '| [[52](#bib.bib52)] |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] |'
- en: '&#124; VGG16/Multi-task VGG16 &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VGG16/多任务 VGG16 &#124;'
- en: '| 0.829 /0.833 | –/– |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 0.829 /0.833 | –/– |'
- en: '| [[28](#bib.bib28)] | DenseNet/BMSL | –/– | 0.850/0.900 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | DenseNet/BMSL | –/– | 0.850/0.900 |'
- en: '| [[26](#bib.bib26)] |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] |'
- en: '&#124; CNN with single-stage/ &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单阶段 CNN/ &#124;'
- en: '&#124; multi-stage transfer learning &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段迁移学习 &#124;'
- en: '| –/– | 0.850/0.910 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.850/0.910 |'
- en: '| [[29](#bib.bib29)] |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] |'
- en: '&#124; CNN/AGCL &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/AGCL &#124;'
- en: '| –/– | 0.771/0.803 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.771/0.803 |'
- en: '| [[93](#bib.bib93)] |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] |'
- en: '&#124; DScGAN without/with &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DScGAN 无/有 &#124;'
- en: '&#124; domain knowledge &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 领域知识 &#124;'
- en: '| 0.816/0.905 | 0.812/0.914 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 0.816/0.905 | 0.812/0.914 |'
- en: With respect to type of domain knowledge for disease diagnosis, the knowledge
    from natural images is widely incorporated in deep learning models. However, considering
    the gap between natural images and medical ones, using information from external
    medical datasets of the same diseases with similar modalities (e.g., SFM and DM)
    [[63](#bib.bib63)], with different modalities (DBT and MM, Ultrasound) [[26](#bib.bib26)],
    and even with different diseases [[68](#bib.bib68)] may be more effective. To
    incorporate the above information is relatively easy, and both transfer learning
    and multi-task learning have been widely adopted. However, there are still no
    comparative studies on how different extra datasets can improve the performance
    of deep learning models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用于疾病诊断的领域知识，自然图像中的知识被广泛地整合到深度学习模型中。然而，考虑到自然图像与医学图像之间的差距，使用来自相同疾病、相似模态（例如，SFM和DM）[[63](#bib.bib63)]、不同模态（DBT和MM，超声）[[26](#bib.bib26)]，甚至不同疾病[[68](#bib.bib68)]的外部医学数据集中的信息可能更为有效。整合上述信息相对容易，转移学习和多任务学习已被广泛采用。然而，目前尚无比较研究来评估不同额外数据集如何提高深度学习模型的性能。
- en: For the domain knowledge of medical doctors, the high-level domain knowledge
    (e.g., the training pattern and the diagnostic pattern) is generally common for
    different diseases. On the other hand, the low-level domain knowledge, such as
    the areas in images and features of diseases that medical doctors usually pay
    attention to, is generally different for different diseases. There is generally
    a trade-off between the versatility and the utility of domain knowledge. To diagnose
    a certain disease, the benefit of incorporating a versatile type of domain knowledge
    suitable for many diseases may not be as significant as using the one that is
    specific for the disease. However, identifying such specific domain knowledge
    may not be easy, and generally requires more efforts from medical doctors (e.g.,
    to identify hand-crafted features or attention maps).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学医生的领域知识，高层领域知识（例如，训练模式和诊断模式）通常对不同疾病是通用的。另一方面，低层领域知识，例如医学医生通常关注的图像区域和疾病特征，对于不同的疾病通常是不同的。领域知识的通用性与实用性之间通常存在权衡。为了诊断某种特定疾病，整合适用于多种疾病的通用领域知识的好处可能不如使用特定于该疾病的领域知识显著。然而，识别这种特定领域知识可能并不容易，并且通常需要医学医生付出更多的努力（例如，识别手工制作的特征或注意力图）。
- en: We believe that more kinds of medical domain knowledge can be explored and utilized
    for disease diagnosis. In addition, comparative studies on some benchmark datasets
    should be carried out with respect to different types of domain knowledge and
    different incorporating methods for disease diagnosis. The results can provide
    further insights about the utility of medical domain knowledge for deep learning
    models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，可以探索和利用更多种类的医学领域知识来进行疾病诊断。此外，还应该对不同类型的领域知识及其在疾病诊断中的不同整合方法在一些基准数据集上的比较研究。这些结果可以进一步洞察医学领域知识对深度学习模型的实用性。
- en: 3 Lesion, Organ, and Abnormality Detection
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 病变、器官和异常检测
- en: Detecting lesions in medical images is an important task and a key part for
    disease diagnosis in many conditions. Similarly, organ detection is an essential
    preprocessing step for image registration, organ segmentation, and lesion detection.
    Detecting abnormalities in medical images, such as cerebral microbleeds in brain
    MRI images and hard exudates in retinal images, is also required in many applications.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 检测医学影像中的病变是一个重要任务，是许多疾病诊断的关键部分。类似地，器官检测是图像配准、器官分割和病变检测的必要预处理步骤。在许多应用中，检测医学影像中的异常，如脑MRI影像中的脑微出血和视网膜图像中的硬性渗出物，也是必需的。
- en: In this section, the deep learning models widely used for object detection in
    medical images are first described in Subsection [3.1](#S3.SS1 "3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis"). Then, the existing works on utilizing domain
    knowledge from natural and medical datasets, and from medical doctors are introduced
    in detail in Subsection [3.2](#S3.SS2 "3.2 Incorporating Knowledge from Natural
    Datasets or Other Medical Datasets ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and Subsection [3.3](#S3.SS3 "3.3 Incorporating Knowledge from Medical
    Doctors ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"), respectively.
    Lastly, we summarize and discuss these different types of domain knowledge and
    the associated incorporating methods in Subsection [3.4](#S3.SS4 "3.4 Summary
    ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先在小节 [3.1](#S3.SS1 "3.1 医学图像中目标检测深度学习模型的一般结构 ‣ 3 病灶、器官和异常检测 ‣ 领域知识融入医学图像分析的深度学习研究")
    中描述了广泛用于医学图像目标检测的深度学习模型。然后，在小节 [3.2](#S3.SS2 "3.2 从自然数据集或其他医学数据集中融入知识 ‣ 3 病灶、器官和异常检测
    ‣ 领域知识融入医学图像分析的深度学习研究") 和小节 [3.3](#S3.SS3 "3.3 从医学医生处融入知识 ‣ 3 病灶、器官和异常检测 ‣ 领域知识融入医学图像分析的深度学习研究")
    中详细介绍了利用自然和医学数据集以及医学医生知识的现有工作。最后，我们在小节 [3.4](#S3.SS4 "3.4 总结 ‣ 3 病灶、器官和异常检测 ‣
    领域知识融入医学图像分析的深度学习研究") 中总结和讨论了这些不同类型的领域知识及其融入方法。
- en: 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 医学图像中目标检测深度学习模型的一般结构
- en: Deep learning models designed for object detection in natural images have been
    directly applied to detect objects in medical images. These applications include
    pulmonary nodule detection in CT images [[110](#bib.bib110)], retinal diseases
    detection in retinal fundus [[111](#bib.bib111)] and so on.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于自然图像中目标检测的深度学习模型已被直接应用于医学图像中的目标检测。这些应用包括 CT 图像中的肺结节检测 [[110](#bib.bib110)]，视网膜眼底中的视网膜疾病检测
    [[111](#bib.bib111)] 等。
- en: '![Refer to caption](img/24ae3b01e5ddfcecfd137e07a5d79694.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24ae3b01e5ddfcecfd137e07a5d79694.png)'
- en: 'Figure 12: The workflow of colitis detection method by using the Faster R-CNN
    structure [[112](#bib.bib112)].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：使用 Faster R-CNN 结构进行结肠炎检测的方法工作流程 [[112](#bib.bib112)]。
- en: One popular type of model is the two-stage detectors like the Faster R-CNN [[113](#bib.bib113)]
    and the Mask R-CNN [[114](#bib.bib114)]. They generally consist of a region proposal
    network (RPN) that hypothesizes candidate object locations and a detection network
    that refines region proposals. Applications in this category include colitis detection
    in CT images [[112](#bib.bib112)], intervertebral disc detection in X-ray images
    [[115](#bib.bib115)] and the detection of architectural distortions in mammograms
    [[116](#bib.bib116)]. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1 General Structures of
    Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis") shows an example of using Faster R-CNN structure
    for colitis detection [[112](#bib.bib112)].
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的模型类型是两阶段检测器，例如 Faster R-CNN [[113](#bib.bib113)] 和 Mask R-CNN [[114](#bib.bib114)]。这些模型通常包括一个区域建议网络（RPN），用于假设候选对象的位置，以及一个检测网络，用于细化区域建议。此类别的应用包括
    CT 图像中的结肠炎检测 [[112](#bib.bib112)]，X 光图像中的椎间盘检测 [[115](#bib.bib115)]，以及乳腺X光检查中的结构扭曲检测
    [[116](#bib.bib116)]。图 [12](#S3.F12 "图 12 ‣ 3.1 医学图像中目标检测深度学习模型的一般结构 ‣ 3 病灶、器官和异常检测
    ‣ 领域知识融入医学图像分析的深度学习研究") 显示了使用 Faster R-CNN 结构进行结肠炎检测的示例 [[112](#bib.bib112)]。
- en: Another category is one-stage models like YOLO (You Only Look Once) [[117](#bib.bib117)],
    SSD (Single Shot MultiBox Detector) [[118](#bib.bib118)] and RetinaNet [[119](#bib.bib119)].
    These networks skip the region proposal stage and run detection directly by considering
    the probability that the object appears at each point in image. Compared with
    the two-stage models, models in this approach are generally faster and simpler.
    Examples in this category can be found in breast tumor detection in mammograms
    [[120](#bib.bib120)], pulmonary lung nodule detection in CT [[121](#bib.bib121)],
    and the detection of different lesions (e.g., liver lesion, lung lesion, bone
    lesion, abdomen lesion) in CT images [[122](#bib.bib122)]. An example of using
    one-stage structure for lesion detection is shown in Fig. [13](#S3.F13 "Figure
    13 ‣ 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类是像YOLO（You Only Look Once）[[117](#bib.bib117)]、SSD（Single Shot MultiBox Detector）[[118](#bib.bib118)]
    和 RetinaNet [[119](#bib.bib119)]这样的单阶段模型。这些网络跳过了区域提议阶段，直接通过考虑物体在图像中每个点出现的概率来进行检测。与双阶段模型相比，这种方法的模型通常更快、更简单。这类模型的示例包括乳腺X光检查中的乳腺肿瘤检测[[120](#bib.bib120)]、CT中的肺结节检测[[121](#bib.bib121)]，以及CT图像中不同病变（例如肝病变、肺病变、骨病变、腹部病变）的检测[[122](#bib.bib122)]。使用单阶段结构进行病变检测的示例如图
    [13](#S3.F13 "图 13 ‣ 3.1 医学图像中物体检测的深度学习模型的通用结构 ‣ 3 病变、器官和异常检测 ‣ 将领域知识融入深度学习的综述")所示。
- en: '![Refer to caption](img/956a5c6359b36ecfe818810ac33e1386.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/956a5c6359b36ecfe818810ac33e1386.png)'
- en: 'Figure 13: An example of using one-stage structure for lesion detection in
    CT images [[122](#bib.bib122)].'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：使用单阶段结构进行CT图像病变检测的示例[[122](#bib.bib122)]。
- en: In the following subsections, we will introduce related works that incorporate
    external knowledge into deep learning models for object detection in medical images.
    A summary of these works is shown in Table [V](#S3.T5 "TABLE V ‣ 3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis").
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子节中，我们将介绍将外部知识融入深度学习模型以进行医学图像中的物体检测的相关工作。这些工作的总结见表 [V](#S3.T5 "表 V ‣ 3.1
    医学图像中物体检测的深度学习模型的通用结构 ‣ 3 病变、器官和异常检测 ‣ 将领域知识融入深度学习的综述")。
- en: 'TABLE V: List of studies on lesion, organ, and abnormality detection and the
    knowledge they incorporated'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：病变、器官和异常检测及其融合知识的研究列表
- en: '|'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating Method &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融合方法 &#124;'
- en: '| References |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[21](#bib.bib21)][[123](#bib.bib123)]  [[124](#bib.bib124)][[125](#bib.bib125)]  [[126](#bib.bib126)]
    &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[21](#bib.bib21)][[123](#bib.bib123)]  [[124](#bib.bib124)][[125](#bib.bib125)]  [[126](#bib.bib126)]
    &#124;'
- en: '|'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Medical datasets &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学数据集 &#124;'
- en: '|'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-modal images &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态图像 &#124;'
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[127](#bib.bib127)][[128](#bib.bib128)][[129](#bib.bib129)] &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[127](#bib.bib127)][[128](#bib.bib128)][[129](#bib.bib129)] &#124;'
- en: '|'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical doctors |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 医学医生 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[29](#bib.bib29)][[130](#bib.bib130)]  [[131](#bib.bib131)] &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[29](#bib.bib29)][[130](#bib.bib130)]  [[131](#bib.bib131)] &#124;'
- en: '|'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Detection patterns |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 检测模式 |'
- en: '&#124; Using images collected &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用收集的图像 &#124;'
- en: '&#124; under different settings &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在不同设置下 &#124;'
- en: '| [[132](#bib.bib132)][[133](#bib.bib133)] |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)][[133](#bib.bib133)] |'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Comparing bilateral or &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比较双侧或 &#124;'
- en: '&#124; cross-view images &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨视图图像 &#124;'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[134](#bib.bib134)][[135](#bib.bib135)]  [[136](#bib.bib136), [137](#bib.bib137)]  [[138](#bib.bib138)]
    &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[134](#bib.bib134)][[135](#bib.bib135)]  [[136](#bib.bib136), [137](#bib.bib137)]  [[138](#bib.bib138)]
    &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Analyzing adjacent slices &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分析相邻切片 &#124;'
- en: '| [[121](#bib.bib121)] |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] |'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Features doctors focus on &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医生关注的特征 &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征级融合 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[139](#bib.bib139)][[140](#bib.bib140)]  [[141](#bib.bib141)][[95](#bib.bib95)]  [[142](#bib.bib142)][[143](#bib.bib143)]
    &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139)][[140](#bib.bib140)]  [[141](#bib.bib141)][[95](#bib.bib95)]  [[142](#bib.bib142)][[143](#bib.bib143)]
    &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other related information &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他相关信息 &#124;'
- en: '|'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '&#124; /training pattern design &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; /训练模式设计 &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[29](#bib.bib29)][[144](#bib.bib144)]  [[145](#bib.bib145)][[146](#bib.bib146)]
    &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[29](#bib.bib29)][[144](#bib.bib144)]  [[145](#bib.bib145)][[146](#bib.bib146)]
    &#124;'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 从自然数据集或其他医疗数据集中引入知识
- en: Similar to disease diagnosis, it is quite popular to pre-train a large natural
    image dataset (generally ImageNet) to introduce information for object detection
    in medical domain. Examples can be found in lymph node detection [[21](#bib.bib21)],
    polyp and pulmonary embolism detection [[126](#bib.bib126)], breast tumor detection
    [[123](#bib.bib123)], colorectal polyps detection [[124](#bib.bib124), [125](#bib.bib125)]
    and so on.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，预训练一个大型自然图像数据集（通常是 ImageNet）以引入医疗领域的对象检测信息也很流行。可以在淋巴结检测 [[21](#bib.bib21)]、息肉和肺栓塞检测
    [[126](#bib.bib126)]、乳腺肿瘤检测 [[123](#bib.bib123)]、结直肠息肉检测 [[124](#bib.bib124),
    [125](#bib.bib125)] 等方面找到相关例子。
- en: In addition, utilizing other medical datasets (i.e., multi-modal images) is
    also quite common. For example, PET images are used to help the lesion detection
    in CT scans of livers [[128](#bib.bib128)]. Specifically, PET images are first
    generated from CT scans using a combined structure of FCN and GAN, then the synthesized
    PET images are used in a false-positive reduction layer for detecting malignant
    lesions. Quantitative results show a 28% reduction in the average false positive
    per case. Besides, [[127](#bib.bib127)] develops a strategy to detect breast masses
    from digital tomosynthesis by fine-tuning the model pre-trained on mammography
    datasets. Another example of using multi-modal medical images can be found in
    liver tumor detection [[129](#bib.bib129)].
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用其他医疗数据集（即多模态图像）也相当常见。例如，PET 图像被用来帮助肝脏 CT 扫描中的病灶检测 [[128](#bib.bib128)]。具体来说，PET
    图像首先通过 FCN 和 GAN 的组合结构从 CT 扫描生成，然后将合成的 PET 图像用于假阳性减少层以检测恶性病变。定量结果显示，每个病例的平均假阳性减少了
    28%。此外，[[127](#bib.bib127)] 开发了一种策略，通过微调在乳腺 X 光数据集上预训练的模型来检测数字断层摄影中的乳腺肿块。另一个使用多模态医疗图像的例子可以在肝肿瘤检测中找到
    [[129](#bib.bib129)]。
- en: 3.3 Incorporating Knowledge from Medical Doctors
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 从医疗医生那里引入知识
- en: 'In this subsection, we summarize the existing works on incorporating the knowledge
    of medical doctors to more effectively detect objects in medical images. The types
    of domain knowledge from medical doctors can be grouped into the following four
    categories:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们总结了现有的将医疗医生知识融入医疗图像对象检测的方法。来自医疗医生的领域知识可以分为以下四类：
- en: '1.'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练模式，
- en: '2.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general detection patterns they view images,
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像的常规检测模式，
- en: '3.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the features (e.g., locations, structures, shapes) they give special attention
    to, and
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们特别关注的特征（例如位置、结构、形状），以及
- en: '4.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: other related information regarding detection.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他与检测相关的信息。
- en: 3.3.1 Training Pattern of Medical Doctors
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 医疗医生的训练模式
- en: The training pattern of medical doctors, which is generally characterized as
    being given tasks with increasing difficulty, is also used for object detection
    in medical images. Similar to the disease diagnosis, most works utilize the curriculum
    learning to introduce this pattern. For example, an attention-guided curriculum
    learning (AGCL) framework is presented to locate the lesion in chest X-rays [[29](#bib.bib29)].
    During this process, images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN gradually, and the disease heatmaps generated from the CNN
    are used to locate the lesion areas.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗医生的训练模式，通常以任务难度逐渐增加为特征，也被用于医疗图像中的对象检测。与疾病诊断类似，大多数工作利用课程学习来引入这种模式。例如，提出了一种注意力引导的课程学习（AGCL）框架，用于定位胸部
    X 光中的病灶 [[29](#bib.bib29)]。在这个过程中，按照难度顺序（按不同严重程度分组）的图像逐渐输入到 CNN 中，CNN 生成的疾病热图用于定位病灶区域。
- en: Another work is called as CASED proposed for lung nodule detection in chest
    CT [[130](#bib.bib130)]. CASED adopts a curriculum adaptive sampling technique
    to address the problem of extreme data imbalance. In particular, CASED lets the
    network to first learn how to distinguish nodules from their immediate surroundings,
    and then it adds a greater proportion of difficult-to-classify global context,
    until uniformly samples from the empirical data distribution. In this way, CASED
    tops the LUNA16 challenge leader-board with an average sensitivity score of 88.35%.
    Another example of using curriculum learning can be found in cardiac landmark
    detection [[131](#bib.bib131)].
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工作是称为CASED的肺结节检测方法[[130](#bib.bib130)]。CASED采用课程自适应采样技术来解决极端数据不平衡的问题。特别是，CASED让网络首先学习如何区分结节与其周围环境，然后逐渐增加难以分类的全局上下文，直到从经验数据分布中均匀采样。通过这种方式，CASED以88.35%的平均敏感度得分在LUNA16挑战赛中名列前茅。另一个使用课程学习的例子可以在心脏标志物检测中找到[[131](#bib.bib131)]。
- en: 3.3.2 General Detection Patterns of Medical Doctors
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 医生的一般检测模式
- en: 'When experienced medical doctors are locating possible lesions in medical images,
    they also display particular patterns, and these patterns can be incorporated
    into deep learning models for object detection. Experienced medical doctors generally
    have the following patterns: (1) they usually take into account images collected
    under different settings (e.g., brightness and contrast), (2) they often compare
    bilateral images or use cross-view images, and (3) they generally read adjacent
    slices.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 当经验丰富的医生在医学图像中定位可能的病变时，他们也会展示特定的模式，这些模式可以融入深度学习模型中以进行目标检测。经验丰富的医生通常有以下模式：（1）他们通常考虑在不同设置（如亮度和对比度）下收集的图像，（2）他们经常比较双侧图像或使用交叉视图图像，（3）他们通常阅读相邻的切片。
- en: For example, to locate possible lesions during visual inspection of the CT images,
    radiologists would combine images collected under different settings (e.g., brightness
    and contrast). To imitate the above process, a multi-view feature pyramid network
    (FPN) is proposed in [[132](#bib.bib132)], where multi-view features are extracted
    from images rendered with varied brightness and contrast. The multi-view information
    is then combined using a position-aware attention module. Experiments show that
    the proposed model achieves an absolute gain of 5.65% over the previous state-of-the-art
    method on the NIH DeepLesion dataset. Another example of using images under different
    settings can be found in the COVID-19 pneumonia-based lung lesion detection [[133](#bib.bib133)].
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了在CT图像的视觉检查中定位可能的病变，放射科医生会结合在不同设置（如亮度和对比度）下收集的图像。为了模拟上述过程，[[132](#bib.bib132)]中提出了一种多视角特征金字塔网络（FPN），其中多视角特征从具有不同亮度和对比度的图像中提取。然后，使用位置感知注意力模块结合多视角信息。实验表明，所提出的模型在NIH
    DeepLesion数据集上相较于之前的最先进方法取得了绝对提升5.65%。另一个使用不同设置下的图像的例子可以在基于COVID-19肺炎的肺部病变检测中找到[[133](#bib.bib133)]。
- en: In addition, the bilateral information is widely used by radiologists. For example,
    in standard mammographic screening, images are captured from both two breasts,
    and experienced radiologists generally compare bilateral mammogram images to find
    masses. To incorporate this pattern, a contrasted bilateral network (CBN) is proposed
    in [[134](#bib.bib134)], in which the bilateral images are first coarsely aligned
    and then fed into a pair of networks to extract features for the following detection
    steps (shown in Fig. [14](#S3.F14 "Figure 14 ‣ 3.3.2 General Detection Patterns
    of Medical Doctors ‣ 3.3 Incorporating Knowledge from Medical Doctors ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")). Experimental results demonstrate
    the effectiveness of incorporating the bilateral information.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，双侧信息被放射科医生广泛使用。例如，在标准的乳腺X光筛查中，会捕获双侧乳腺的图像，经验丰富的放射科医生通常比较双侧乳腺图像以寻找肿块。为了融合这一模式，[[134](#bib.bib134)]中提出了一种对比双侧网络（CBN），其中双侧图像首先粗略对齐，然后输入一对网络以提取特征用于后续的检测步骤（如图[14](#S3.F14
    "Figure 14 ‣ 3.3.2 General Detection Patterns of Medical Doctors ‣ 3.3 Incorporating
    Knowledge from Medical Doctors ‣ 3 Lesion, Organ, and Abnormality Detection ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")所示）。实验结果证明了融合双侧信息的有效性。
- en: '![Refer to caption](img/31af6a214a801ccde849eb1be904a1d3.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/31af6a214a801ccde849eb1be904a1d3.png)'
- en: 'Figure 14: The workflow of mammogram mass detection by integrating the bilateral
    information [[134](#bib.bib134)], where the aligned images are fed into two networks
    seperately to extract features for further detection.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：通过整合双侧信息[[134](#bib.bib134)]进行乳腺肿块检测的工作流程，其中对齐的图像分别输入到两个网络中以提取特征用于进一步检测。
- en: Similarly, to detect acute stroke signs in non-contrast CT images, experienced
    neuroradiologists routinely compare the appearance and Hounsfield Unit intensities
    of the left and right hemispheres, and then find the regions most commonly affected
    in stroke episodes. This pattern is mimicked by [[137](#bib.bib137)] for the detection
    of dense vessels and ischaemia. The experimental results show that introducing
    the pattern greatly improves the performance for detecting ischaemia. Other examples
    of integrating the bilateral feature comparison or the symmetry constrains can
    be found in thrombus detection [[136](#bib.bib136)] and hemorrhagic lesion detection
    [[138](#bib.bib138)] in brain CT images.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为了在无对比剂CT图像中检测急性中风的迹象，经验丰富的神经放射科医师通常会比较左右半球的外观和Hounsfield单位强度，然后找到中风发作中最常受影响的区域。[[137](#bib.bib137)]通过模拟这种模式来检测密集的血管和缺血。实验结果显示，引入该模式大大提高了检测缺血的性能。其他集成双侧特征比较或对称约束的例子可以在脑CT图像中的血栓检测[[136](#bib.bib136)]和出血性病变检测[[138](#bib.bib138)]中找到。
- en: Besides the bilateral images, the information from cross views (i.e., mediolateral-oblique
    and cranio-caudal) is highly related and complementary, and hence is also used
    for mammogram mass detection. In [[135](#bib.bib135)], a bipartite graph convolutional
    network is introduced to endow the existing methods with cross-view reasoning
    ability of radiologists. Concretely, the bipartite node sets are constructed to
    represent the relatively consistent regions, and the bipartite edge are used to
    model both inherent cross-view geometric constraints and appearance similarities
    between correspondences. This process can enables spatial visual features equipped
    with cross-view reasoning ability. Experimental results on DDSM dataset achieve
    the state-of-the-art performance (with a recall of 79.5 at 0.5 false positives
    per image).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 除了双侧图像，来自交叉视图（即，侧面斜视和头尾视图）的信息高度相关且互补，因此也用于乳腺X线图像中的肿块检测。在[[135](#bib.bib135)]中，引入了一种双分图卷积网络，以赋予现有方法放射科医师的交叉视图推理能力。具体而言，构建了双分节点集来表示相对一致的区域，并使用双分边来建模固有的交叉视图几何约束和对应点之间的外观相似性。这个过程能够使空间视觉特征具备交叉视图推理能力。DDSM数据集上的实验结果达到最先进的性能（在每张图像0.5个假阳性下的召回率为79.5）。
- en: When looking for small nodules in CT images , radiologists often observe each
    slice together with adjacent slices, similar to detecting an object in a video.
    This workflow is imitated in [[121](#bib.bib121)] to detect pulmonary lung nodule
    in CT images, where the state-of-the-art object detector SSD is applied in this
    process. This method obtains state-of-the-art result with the FROC score of 0.892
    in LUNA16 dataset.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在CT图像中寻找小结节时，放射科医师通常会观察每一切片及其相邻切片，类似于在视频中检测物体。这种工作流程在[[121](#bib.bib121)]中被模拟用于检测CT图像中的肺结节，其中应用了最先进的目标检测器SSD。这种方法在LUNA16数据集中取得了最先进的FROC得分0.892。
- en: 3.3.3 Features That Medical Doctors Give Special Attention to
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 医生特别关注的特征
- en: Similar to disease diagnosis, medical doctors also use many ‘hand-crafted features’
    to help them to find target objects (e.g., nodules or lesions) in medical images.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，医生也使用许多“手工特征”来帮助他们在医学图像中找到目标物体（例如，结节或病变）。
- en: For example, in [[140](#bib.bib140)], to detect mammographic lesions, different
    types of hand-crafted features (e.g., contrast features, geometrical features,
    and location features) are first extracted, and then concatenated with those learned
    by a CNN. The results show that these hand-crafted features, particularly the
    location and context features , can complement the network generating a higher
    specificity over the CNN alone.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[140](#bib.bib140)]中，为了检测乳腺X线图像中的病变，首先提取不同类型的手工特征（例如，对比度特征、几何特征和位置特征），然后与CNN学习到的特征进行连接。结果显示，这些手工特征，特别是位置和上下文特征，可以补充网络，从而比单独使用CNN生成更高的特异性。
- en: Similarly, [[141](#bib.bib141)] presents a deep learning model based on Faster
    R-CNN to detect abnormalities in the esophagus from endoscopic images. In particular,
    to enhance texture details, the proposed detection system incorporates the Gabor
    handcrafted features with the CNN features through concatenation in the detection
    stage. The experimental results on two datasets (Kvasir and MICCAI 2015) show
    that the model is able to surpass the state-of-the-art performance.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[141](#bib.bib141)]提出了一种基于Faster R-CNN的深度学习模型，用于从内窥镜图像中检测食管异常。特别是，为了增强纹理细节，所提出的检测系统通过在检测阶段的串联中将Gabor手工特征与CNN特征相结合。在两个数据集（Kvasir和MICCAI
    2015）上的实验结果表明，该模型能够超越最先进的性能。
- en: Another example can be found in [[139](#bib.bib139)] for the detection of lung
    nodules, where 88 hand-crafted features, including intensity, shape, texture are
    extracted and combined with features extracted by a CNN and then fed into a classifier.
    Experimental results demonstrate the effectiveness of the combination of handcrafted
    features and CNN features.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可以在[[139](#bib.bib139)]中找到，用于检测肺结节，其中提取了88个手工特征，包括强度，形状，纹理，并与CNN提取的特征相结合，然后馈入分类器。实验结果证明了手工特征和CNN特征的组合的有效性。
- en: In the automated detection of thyroid nodules, the size and shape attribute
    of nodules are considered in [[95](#bib.bib95)]. To incorporate the information
    above, the generating process of region proposals is constrained, and the detection
    results on two different datasets show high accuracy.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在甲状腺结节的自动检测中，考虑了结节的大小和形状属性[[95](#bib.bib95)]。为了结合以上信息，对区域提议的生成过程加以限制，并且在两个不同数据集上的检测结果显示出高准确性。
- en: Furthermore, in lymph node gross tumor volume detection (GTV[LN]) in oncology
    imaging, some attributes of lymph nodes (LNs) are also utilized [[142](#bib.bib142)].
    Motivated by the prior clinical knowledge that LNs from a connected lymphatic
    system, and the spread of cancer cells among LNs often follows certain pathways,
    a LN appearance and inter-LN relationship learning framework is proposed for GTV[LN]
    detection. More specifically, the instance-wise appearance features are first
    extracted by a 3D CNN, then a graph neural network (GNN) is used to model the
    inter-LN relationships, and the global LN-tumor spatial priors are included in
    this process. This method significantly improves over state-of-the-art method.
    Another example of combining handcrafted features and deep features can be found
    in lung lesion detection [[143](#bib.bib143)].
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在肿瘤学成像中的淋巴结肿瘤体积（GTV [LN]）检测中，还利用了淋巴结（LNs）的一些属性[[142](#bib.bib142)]。受到先前的临床知识的启发，即来自连通淋巴系统的LN，并且癌细胞在LNs之间的传播通常遵循某些路径，为GTV[
    LN]检测提出了LN外观和LN之间关系学习框架。更具体地说，首先通过3D CNN提取实例化外观特征，然后使用图神经网络（GNN）来建模LN之间的关系，并在此过程中包含全局LN-肿瘤空间先验。该方法显著改进了最先进的方法。在肺部病变检测方面，还可以找到另一个将手工特征和深度特征结合的例子[[143](#bib.bib143)]。
- en: 3.3.4 Other Types of Information Related to Detection
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 与检测相关的其他类型信息
- en: Similar with that in disease diagnosis, there are other information (e.g., radiological
    reports, extra labels) can also be integrated into the lesion detection process.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 与疾病诊断类似，还有其他信息（例如，放射学报告，额外标签）也可以整合到病变检测过程中。
- en: For example in [[29](#bib.bib29)], to locate thoracic diseases on chest radiographs,
    the difficulty of each sample, represented as the severity level of the thoracic
    disease, is first extracted from radiology reports. Then, the curriculum learning
    technique is adopted, in which the training samples are presented to the network
    in order of increasing difficulty. Experiments on the ChestXray14 database validate
    the effectiveness on significant performance improvement over baseline methods.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[29](#bib.bib29)]中，为了定位胸部X射线片上的胸部疾病，首先从放射学报告中提取每个样本的难度，表示为胸部疾病的严重程度。然后采用课程学习技术，即按难度递增的顺序将训练样本呈现给网络。对ChestXray14数据库上的实验证实了相对基线方法的显着性能提升的有效性。
- en: Example of using extra labels can be found in [[144](#bib.bib144)]. In this
    method, the information of the classification labels is incorporated to help the
    lesion localization in chest X-rays and mammograms. In particular, a framework
    named as self-transfer learning (STL) is proposed, which jointly optimizes both
    classification and localization networks to help the localization network focus
    on correct lesions. Experimental results show that STL can achieve significantly
    better localization performance compared to previous weakly supervised localization
    approaches. More examples of using extra labels can be found in detection in mammograms
    [[145](#bib.bib145), [146](#bib.bib146)].
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 使用额外标签的例子可以在[[144](#bib.bib144)]中找到。在这种方法中，将分类标签的信息纳入，以帮助胸部X光片和乳腺X光片中的病灶定位。特别地，提出了一个名为自我迁移学习（STL）的框架，该框架联合优化分类和定位网络，以帮助定位网络关注正确的病灶。实验结果表明，与之前的弱监督定位方法相比，STL能够显著提高定位性能。更多使用额外标签的例子可以在乳腺X光片的检测中找到[[145](#bib.bib145),
    [146](#bib.bib146)]。
- en: 3.4 Summary
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 摘要
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for object
    detection in medical images. Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis") illustrates the quantitative improvements,
    in terms of sensitivity and recall, of some typical work over the baseline methods
    for object detection in medical images. From the results we can see that in general,
    integrating domain knowledge can be beneficial for detection tasks.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了不同类型的领域知识以及将其整合到深度学习模型中的方法，以用于医学图像的目标检测。表[VI](#S3.T6 "TABLE VI
    ‣ 3.4 Summary ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")展示了与基线方法相比，一些典型工作在医学图像目标检测中的灵敏度和召回率方面的定量改进。结果表明，通常情况下，整合领域知识对检测任务是有利的。
- en: Similar to disease diagnosis, the high-level training pattern of medical doctors
    is generic and can be utilized for detecting different diseases or organs. In
    contrast, the low-level domain knowledge, like the detection patterns that medical
    doctors adopt and some hand-crafted features they give more attention when searching
    lesions, are generally different for different diseases. For example, the pattern
    of comparing bilateral images can only be utilized for detecting organs with symmetrical
    structures [[137](#bib.bib137), [134](#bib.bib134)]. In addition, we can see from
    Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") that leveraging pattern of medical doctors on average shows better
    performance when compared with integrating hand-crafted features. This may indicate
    that there is still large room to explore more effective features for object detection
    in medical images.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，医疗医生的高级培训模式是通用的，可以用于检测不同的疾病或器官。相对而言，低级别的领域知识，例如医疗医生在搜索病灶时采用的检测模式以及他们更加关注的一些手工特征，通常对于不同的疾病是不同的。例如，比较双侧图像的模式只能用于检测具有对称结构的器官[[137](#bib.bib137),
    [134](#bib.bib134)]。此外，从表[VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")中可以看出，利用医疗医生的模式在平均表现上优于集成手工特征。这可能表明在医学图像的目标检测中，仍有很大的空间可以探索更有效的特征。
- en: 'TABLE VI: The comparison of the quantitative metrics for some medical object
    detection methods after incorporating domain knowledge'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 结合领域知识后，一些医学目标检测方法的定量指标比较'
- en: '| Reference |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline model/With domain knowledge &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/与领域知识 &#124;'
- en: '| Sensitivity | Recall |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 灵敏度 | 召回率 |'
- en: '| [[139](#bib.bib139)] |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] |'
- en: '&#124; CNN/CNN with hand-crafted features &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/CNN与手工特征 &#124;'
- en: '| 0.890/0.909 | –/– |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 0.890/0.909 | –/– |'
- en: '| [[141](#bib.bib141)] |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] |'
- en: '&#124; ResNet/CNN with handcrafted features &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet/CNN与手工特征 &#124;'
- en: '| –/– | 0.940/0.950 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.940/0.950 |'
- en: '| [[121](#bib.bib121)] | SSD/MSSD | 0.927/0.976 | –/– |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | SSD/MSSD | 0.927/0.976 | –/– |'
- en: '| [[134](#bib.bib134)] | Mask R-CNN/CBD | –/– | 0.869/0.890 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] | Mask R-CNN/CBD | –/– | 0.869/0.890 |'
- en: '| [[135](#bib.bib135)] | Mask R-CNN/BG-RCNN | –/– | 0.918/0.945 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| [[135](#bib.bib135)] | Mask R-CNN/BG-RCNN | –/– | 0.918/0.945 |'
- en: '| [[29](#bib.bib29)] |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] |'
- en: '&#124; CNN/AGCL &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/AGCL &#124;'
- en: '| –/– | 0.660/0.730 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.660/0.730 |'
- en: '1'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: The performance is evaluated at 4 false positives per image in [[139](#bib.bib139),
    [134](#bib.bib134), [135](#bib.bib135), [26](#bib.bib26)].
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能在[[139](#bib.bib139)、[134](#bib.bib134)、[135](#bib.bib135)、[26](#bib.bib26)]中以每张图像4个假阳性进行评估。
- en: 4 Lesion and Organ Segmentation
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 病灶和器官分割
- en: Medical image segmentation devotes to identifying pixels of lesions or organs
    from the background, and is generally regarded as a prerequisite step for the
    lesion assessment and disease diagnosis. Segmentation methods based on deep learning
    models have become the dominant technique in recent years and have been widely
    used for the segmentation of lesions such as brain tumors [[147](#bib.bib147)],
    breast tumors [[148](#bib.bib148)], and organs such as livers [[149](#bib.bib149)]
    and pancreas [[150](#bib.bib150)].
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割致力于从背景中识别病灶或器官的像素，通常被视为病灶评估和疾病诊断的先决步骤。近年来，基于深度学习模型的分割方法已成为主流技术，并被广泛应用于脑肿瘤[[147](#bib.bib147)]、乳腺肿瘤[[148](#bib.bib148)]以及肝脏[[149](#bib.bib149)]和胰腺[[150](#bib.bib150)]等器官的分割。
- en: In Subsection [4.1](#S4.SS1 "4.1 General Structures of Deep Learning Models
    for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), we describe the models that are generally used for object segmentation
    in the medical domain. Then in Subsection [4.2](#S4.SS2 "4.2 Incorporating Knowledge
    from Natural Datasets or Other Medical Datasets ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), the works of utilizing domain knowledge from natural and other medical
    datasets are introduced. Then, the models utilizing domain knowledge from medical
    doctors are introduced in Subsection [4.3](#S4.SS3 "4.3 Incorporating Knowledge
    from Medical Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"). A summary of
    this section is provided in Subsection [4.4](#S4.SS4 "4.4 Summary ‣ 4 Lesion and
    Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在子节[4.1](#S4.SS1 "4.1 医学图像中对象分割的深度学习模型的通用结构 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中，我们描述了医学领域中一般用于对象分割的模型。然后在子节[4.2](#S4.SS2
    "4.2 从自然数据集或其他医学数据集中纳入知识 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中，介绍了利用自然和其他医学数据集领域知识的研究。接着，在子节[4.3](#S4.SS3
    "4.3 从医学医生那里纳入知识 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中介绍了利用医学医生领域知识的模型。本节的总结在子节[4.4](#S4.SS4
    "4.4 总结 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中提供。
- en: 4.1 General Structures of Deep Learning Models for Object Segmentation in Medical
    Images
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 医学图像中对象分割的深度学习模型的通用结构
- en: 'The deep learning models utilized for medical image segmentation are generally
    divided into three categories: the FCN (fully convolutional network) [[151](#bib.bib151)]
    based models, the U-Net [[152](#bib.bib152)] based models, and the GAN [[153](#bib.bib153)]
    based models.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 用于医学图像分割的深度学习模型通常分为三类：基于FCN（全卷积网络）[[151](#bib.bib151)]的模型、基于U-Net [[152](#bib.bib152)]的模型，以及基于GAN
    [[153](#bib.bib153)]的模型。
- en: '![Refer to caption](img/1ce4537d0995a0260396a4aa27213369.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ce4537d0995a0260396a4aa27213369.png)'
- en: 'Figure 15: The schematic diagram of using FCN structure for cardiac segmentation
    [[154](#bib.bib154)].'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：使用FCN结构进行心脏分割的示意图[[154](#bib.bib154)]。
- en: In particular, the FCN has been proven to perform well in various medical image
    segmentation tasks [[155](#bib.bib155), [156](#bib.bib156)]. Some variants of
    FCN, such as cascaded FCN [[157](#bib.bib157)], parallel FCN [[158](#bib.bib158)]
    and recurrent FCN [[159](#bib.bib159)] are also widely used for segmentation tasks
    in medical images. Fig. [15](#S4.F15 "Figure 15 ‣ 4.1 General Structures of Deep
    Learning Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ
    Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") illustrates an example of using FCN based model for cardiac
    segmentation.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，FCN在各种医学图像分割任务中表现出色[[155](#bib.bib155), [156](#bib.bib156)]。FCN的一些变体，如级联FCN[[157](#bib.bib157)]、并行FCN[[158](#bib.bib158)]和递归FCN[[159](#bib.bib159)]，也广泛用于医学图像中的分割任务。图[15](#S4.F15
    "图 15 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构 ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")展示了使用FCN模型进行心脏分割的一个示例。
- en: '![Refer to caption](img/be8d2746eeb8929d9d868d196e4abaf5.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be8d2746eeb8929d9d868d196e4abaf5.png)'
- en: 'Figure 16: The network structure of U-Net [[152](#bib.bib152)].'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：U-Net的网络结构[[152](#bib.bib152)]。
- en: In addition, the U-Net [[152](#bib.bib152)] (shown in Fig. [16](#S4.F16 "Figure
    16 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation in
    Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis")) and its variants are
    also widely utilized for medical image segmentation. U-Net builds upon FCN structure,
    mainly consists of a series of convolutional and deconvolutional layers, and with
    the short connections between the layers of equal resolution. U-Net and its variants
    like UNet++[[160](#bib.bib160)] and recurrent U-Net [[161](#bib.bib161)] perform
    well in many medical image segmentation tasks[[162](#bib.bib162)].
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，U-Net[[152](#bib.bib152)]（见图[16](#S4.F16 "图 16 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构
    ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")）及其变体也被广泛用于医学图像分割。U-Net建立在FCN结构的基础上，主要由一系列卷积层和反卷积层组成，并在相同分辨率的层之间有短连接。U-Net及其变体，如UNet++[[160](#bib.bib160)]和递归U-Net[[161](#bib.bib161)]，在许多医学图像分割任务中表现良好[[162](#bib.bib162)]。
- en: In the GAN-based models [[163](#bib.bib163), [164](#bib.bib164)], the generator
    is used to predict the mask of the target based on some encoder-decoder structures
    (such as FCN or U-Net). The discriminator serves as a shape regulator that helps
    the generator to obtain satisfactory segmentation results. Applications of using
    GAN-based models in medical image segmentation include brain segmentation [[165](#bib.bib165)],
    skin lesion segmentation [[166](#bib.bib166)], vessel segmentation [[167](#bib.bib167)]
    and anomaly segmentation in retinal fundus images [[168](#bib.bib168)]. Fig. [17](#S4.F17
    "Figure 17 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation
    in Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") is an example
    of using GAN-based model for vessel segmentation in miscroscopy images.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于GAN的模型[[163](#bib.bib163), [164](#bib.bib164)]中，生成器用于根据某些编码器-解码器结构（如FCN或U-Net）预测目标的掩模。鉴别器则作为形状调节器，帮助生成器获得令人满意的分割结果。GAN-based模型在医学图像分割中的应用包括脑部分割[[165](#bib.bib165)]、皮肤病变分割[[166](#bib.bib166)]、血管分割[[167](#bib.bib167)]和视网膜眼底图像中的异常分割[[168](#bib.bib168)]。图[17](#S4.F17
    "图 17 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构 ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")展示了使用GAN-based模型进行显微图像中的血管分割的一个示例。
- en: '![Refer to caption](img/a23a66ce2424bc3820148a5a0c2e348f.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a23a66ce2424bc3820148a5a0c2e348f.png)'
- en: 'Figure 17: An example of using a GAN-based model for vessel segmentation [[167](#bib.bib167)].'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：使用GAN-based模型进行血管分割的一个示例[[167](#bib.bib167)]。
- en: 'TABLE VII: The list of researches of lesion, organ segmentation and the knowledge
    they incorporated'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表VII：病变、器官分割研究及其融入的知识列表
- en: '|'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入 &#124;'
- en: '&#124; Method &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '| References |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[155](#bib.bib155)][[169](#bib.bib169)][[170](#bib.bib170)][[171](#bib.bib171)]  [[126](#bib.bib126)]
    &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[155](#bib.bib155)][[169](#bib.bib169)][[170](#bib.bib170)][[171](#bib.bib171)]  [[126](#bib.bib126)]
    &#124;'
- en: '|'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical datasets | Multi-modal images |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 医学数据集 | 多模态图像 |'
- en: '&#124; Transfer learning &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[172](#bib.bib172)][[34](#bib.bib34)] &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)][[34](#bib.bib34)] &#124;'
- en: '|'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task/-modal learning &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务/多模态学习 &#124;'
- en: '|'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[173](#bib.bib173)][[174](#bib.bib174)] &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[173](#bib.bib173)][[174](#bib.bib174)] &#124;'
- en: '|'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Modality transformation /synthesis &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态转换/合成 &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[175](#bib.bib175)][[176](#bib.bib176)]  [[177](#bib.bib177)]  [[165](#bib.bib165)]  [[178](#bib.bib178)]
    &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[175](#bib.bib175)][[176](#bib.bib176)]  [[177](#bib.bib177)]  [[165](#bib.bib165)]  [[178](#bib.bib178)]
    &#124;'
- en: '&#124; [[179](#bib.bib179)][[180](#bib.bib180)][[181](#bib.bib181)][[182](#bib.bib182)]  [[183](#bib.bib183)][[184](#bib.bib184)]
    &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[179](#bib.bib179)][[180](#bib.bib180)][[181](#bib.bib181)][[182](#bib.bib182)]  [[183](#bib.bib183)][[184](#bib.bib184)]
    &#124;'
- en: '|'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Datasets of other diseases | Transfer learning | [[185](#bib.bib185)] |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 其他疾病的数据集 | 迁移学习 | [[185](#bib.bib185)] |'
- en: '|'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Disease domain transformation &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 疾病领域转换 &#124;'
- en: '| [[27](#bib.bib27)] |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] |'
- en: '| Medical doctors |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 医学专家 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[186](#bib.bib186)]  [[30](#bib.bib30)]  [[187](#bib.bib187)][[188](#bib.bib188)]  [[189](#bib.bib189)][[190](#bib.bib190)]
    &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[186](#bib.bib186)]  [[30](#bib.bib30)]  [[187](#bib.bib187)][[188](#bib.bib188)]  [[189](#bib.bib189)][[190](#bib.bib190)]
    &#124;'
- en: '|'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Diagnostic patterns |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 诊断模式 |'
- en: '&#124; Using different views as input &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用不同视角作为输入 &#124;'
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[191](#bib.bib191)][[32](#bib.bib32)] &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[191](#bib.bib191)][[32](#bib.bib32)] &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attention mechanism &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制 &#124;'
- en: '| [[192](#bib.bib192)] |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| [[192](#bib.bib192)] |'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Network design &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络设计 &#124;'
- en: '| [[193](#bib.bib193)][[194](#bib.bib194)] |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| [[193](#bib.bib193)][[194](#bib.bib194)] |'
- en: '| Anatomical priors |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学先验 |'
- en: '&#124; Incorporated in post-processing &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入后处理 &#124;'
- en: '|'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[195](#bib.bib195)]  [[196](#bib.bib196), [197](#bib.bib197)]  [[33](#bib.bib33)]
    &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[195](#bib.bib195)]  [[196](#bib.bib196), [197](#bib.bib197)]  [[33](#bib.bib33)]
    &#124;'
- en: '|'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporated in loss function &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入损失函数 &#124;'
- en: '|'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[198](#bib.bib198)][[199](#bib.bib199)]  [[200](#bib.bib200)][[201](#bib.bib201)]  [[40](#bib.bib40)]
    &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[198](#bib.bib198)][[199](#bib.bib199)]  [[200](#bib.bib200)][[201](#bib.bib201)]  [[40](#bib.bib40)]
    &#124;'
- en: '&#124; [[202](#bib.bib202)]  [[198](#bib.bib198)]  [[203](#bib.bib203)] &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[202](#bib.bib202)]  [[198](#bib.bib198)]  [[203](#bib.bib203)] &#124;'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generative models &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成模型 &#124;'
- en: '|'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[32](#bib.bib32)][[204](#bib.bib204)]  [[205](#bib.bib205)][[206](#bib.bib206)]  [[207](#bib.bib207)]
    &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[32](#bib.bib32)][[204](#bib.bib204)]  [[205](#bib.bib205)][[206](#bib.bib206)]  [[207](#bib.bib207)]
    &#124;'
- en: '&#124; [[208](#bib.bib208)]  [[209](#bib.bib209)][[210](#bib.bib210)]  [[211](#bib.bib211)]
    &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[208](#bib.bib208)]  [[209](#bib.bib209)][[210](#bib.bib210)]  [[211](#bib.bib211)]
    &#124;'
- en: '|'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Hand-crafted features |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 手工特征 |'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征级融合 &#124;'
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[212](#bib.bib212)][[213](#bib.bib213)] &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[212](#bib.bib212)][[213](#bib.bib213)] &#124;'
- en: '|'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input level fusion &#124;'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入级融合 &#124;'
- en: '|'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[214](#bib.bib214)][[215](#bib.bib215)] &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[214](#bib.bib214)][[215](#bib.bib215)] &#124;'
- en: '|'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In the following sections, we will introduce research studies that incorporate
    domain knowledge into deep segmentation models. The summary of these works is
    shown in Table [VII](#S4.T7 "TABLE VII ‣ 4.1 General Structures of Deep Learning
    Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍将领域知识融入深度分割模型的研究。这些工作的总结见表 [VII](#S4.T7 "TABLE VII ‣ 4.1 General
    Structures of Deep Learning Models for Object Segmentation in Medical Images ‣
    4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into
    Deep Learning for Medical Image Analysis")。
- en: 4.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 融合自然数据集或其他医学数据集的知识
- en: It is also quite common that deep learning segmentation models are firstly trained
    on a large-scale natural image dataset (e.g., ImageNet) and then fine-tuned on
    the target datasets. Using the above transfer learning strategy to introduce knowledge
    from natural images has demonstrated to achieve a better performance in medical
    image segmentation. Examples can be found in intima-media boundary segmentation
    [[126](#bib.bib126)] and prenatal ultrasound image segmentation[[170](#bib.bib170)].
    Besides ImageNet, [[155](#bib.bib155)] adopts the off-the-shelf DeepLab model
    trained on the PASCAL VOC dataset for anatomical structure segmentation in ultrasound
    images. This pre-trained model is also used in the deep contour-aware network
    (DCAN), which is designed for the gland segmentation in histopathological images
    [[169](#bib.bib169)].
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习分割模型首先在大规模自然图像数据集（如ImageNet）上训练，然后在目标数据集上微调，这种做法也相当常见。利用上述迁移学习策略引入自然图像的知识已被证明能在医疗图像分割中取得更好的性能。可以在内膜-中膜边界分割[[126](#bib.bib126)]和产前超声图像分割[[170](#bib.bib170)]中找到相关示例。除了ImageNet，[[155](#bib.bib155)]还采用了在PASCAL
    VOC数据集上训练的现成DeepLab模型，用于超声图像中的解剖结构分割。这个预训练模型也被用于深度轮廓感知网络（DCAN），该网络旨在用于组织病理图像中的腺体分割[[169](#bib.bib169)]。
- en: Besides using models pre-trained on ‘static’ datasets like ImageNet and PASCAL
    VOC, many deep neural networks, especially those designed for the segmentation
    of 3D medial images, leverage models pre-trained on large-scale video datasets.
    For example, in the automatic segmentation of proximal femur in 3D MRI, the C3D
    pre-trained model is adopted as the encoder of the proposed 3D U-Net [[171](#bib.bib171)].
    Notably, the C3D model is trained on the Sports-1M dataset, which is the largest
    video classification benchmark with 1.1 million sports videos in 487 categories
    [[216](#bib.bib216)].
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用在‘静态’数据集（如ImageNet和PASCAL VOC）上预训练的模型，许多深度神经网络，特别是那些设计用于3D医疗图像分割的模型，还利用在大规模视频数据集上预训练的模型。例如，在3D
    MRI中自动分割近端股骨时，采用了在Sports-1M数据集上预训练的C3D模型作为提出的3D U-Net [[171](#bib.bib171)]的编码器。值得注意的是，C3D模型是在Sports-1M数据集上训练的，该数据集是具有487个类别的110万体育视频的最大视频分类基准[[216](#bib.bib216)]。
- en: In addition to natural images, using knowledge from external medical datasets
    with different modalities and with different diseases is also quite popular.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自然图像，利用来自外部医疗数据集的不同模态和不同疾病的知识也相当流行。
- en: For example, [[172](#bib.bib172)] investigates the transferability of the acquired
    knowledge of a CNN model initially trained for WM hyper-intensity segmentation
    on legacy low-resolution data to new data from the same scanner but with higher
    image resolution. Likewise, the images with different MRI scanners and protocols
    are used in [[34](#bib.bib34)] to help the multi sclerosis segmentation process
    via transfer learning.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[172](#bib.bib172)]研究了最初在旧版低分辨率数据上训练的CNN模型在同一扫描仪但具有更高图像分辨率的新数据上的知识转移能力。同样，在[[34](#bib.bib34)]中使用了不同MRI扫描仪和协议的图像，通过迁移学习帮助多发性硬化症分割过程。
- en: In [[173](#bib.bib173)], the multi-task learning is adopted, where the data
    of brain MRI, breast MRI and cardiac CT angiography (CTA) are used simultaneously
    as multiple tasks. On the other hand, [[174](#bib.bib174)] adopts a multi-modal
    learning structure for organ segmentation. A dual-stream encoder-decoder architecture
    is proposed to learn modality-independent, and thus, generalisable and robust
    features shared among medical datasets with different modalities (MRI and CT images).
    Experimental results prove the effectiveness of this multi-modal learning structure.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[173](#bib.bib173)]中，采用了多任务学习方法，其中同时使用脑部MRI、乳腺MRI和心脏CT血管造影（CTA）的数据作为多个任务。另一方面，[[174](#bib.bib174)]采用了多模态学习结构用于器官分割。提出了一种双流编码器-解码器架构，用于学习模态无关的特征，从而在具有不同模态（MRI和CT图像）的医疗数据集中共享一般化和鲁棒的特征。实验结果证明了这种多模态学习结构的有效性。
- en: Moreover, many works adopt GAN-based models to achieve the domain transformation
    among datasets with different modalities. For example, a model named SeUDA (unsupervised
    domain adaptation) is proposed for the left/right lung segmentation process [[175](#bib.bib175)].
    It leverages the semantic-aware GAN to transfer the knowledge from one chest dataset
    to another. In particular, target images are first mapped towards the source data
    space via the constraint of a semantic-aware GAN loss. Then the segmentation results
    are obtained from the segmentation DNN learned on the source domain. Experimental
    results show that the segmentation performance of SeUDA is highly competitive.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多工作采用基于 GAN 的模型实现不同模态数据集之间的领域转换。例如，提出了一种名为 SeUDA（无监督领域适应）的模型用于左右肺分割过程 [[175](#bib.bib175)]。它利用语义感知
    GAN 将知识从一个胸部数据集转移到另一个数据集。具体而言，目标图像首先通过语义感知 GAN 损失的约束映射到源数据空间。然后，从在源领域上学习的分割 DNN
    中获得分割结果。实验结果表明，SeUDA 的分割性能非常具有竞争力。
- en: More examples of using the knowledge from images with other modalities can be
    found in brain MRI segmentation [[165](#bib.bib165), [182](#bib.bib182)], cardiac
    segmentation [[178](#bib.bib178), [183](#bib.bib183), [181](#bib.bib181), [180](#bib.bib180),
    [184](#bib.bib184)], liver segmentation [[179](#bib.bib179)], lung tumor segmentation
    [[177](#bib.bib177)], cardiac substructure and abdominal multi-organ segmentation
    [[176](#bib.bib176)].
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他模态图像知识的更多例子可以在脑部 MRI 分割 [[165](#bib.bib165), [182](#bib.bib182)]、心脏分割 [[178](#bib.bib178),
    [183](#bib.bib183), [181](#bib.bib181), [180](#bib.bib180), [184](#bib.bib184)]、肝脏分割
    [[179](#bib.bib179)]、肺肿瘤分割 [[177](#bib.bib177)]、心脏子结构和腹部多脏器分割 [[176](#bib.bib176)]
    中找到。
- en: There are also a few works utilize the datasets of other diseases. For instance,
    [[185](#bib.bib185)] first builds a union dataset (3DSeg-8) by aggregating eight
    different 3D medical segmentation datasets, and designs the Med3D network to co-train
    based on 3DSeg-8\. Then the pre-trained models obtained from Med3D are transferred
    into lung and liver segmentation tasks. Experiments show that this method not
    only improves the accuracy, but also accelerates the training convergence speed.
    In addition, the annotated retinal images are used to help the cardiac vessel
    segmentation without annotations [[27](#bib.bib27)]. In particular, a shape-consistent
    generative adversarial network (SC-GAN) is used to generate the synthetic images
    and the corresponding labels. Then the synthetic images are used to train the
    segmentor. Experiments demonstrate the supreme accuracy of coronary artery segmentation.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些工作利用了其他疾病的数据集。例如，[[185](#bib.bib185)] 首先通过整合八个不同的三维医学分割数据集构建了一个联合数据集（3DSeg-8），并设计了
    Med3D 网络，以便基于 3DSeg-8 进行联合训练。然后，将从 Med3D 获得的预训练模型转移到肺部和肝脏分割任务中。实验表明，这种方法不仅提高了准确性，还加速了训练收敛速度。此外，带注释的视网膜图像被用于帮助进行心脏血管分割，而无需注释
    [[27](#bib.bib27)]。特别地，使用了形状一致的生成对抗网络（SC-GAN）来生成合成图像及其对应的标签。然后，使用合成图像来训练分割器。实验表明，冠状动脉分割的准确性非常高。
- en: 4.3 Incorporating Knowledge from Medical Doctors
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 融入医学医生的知识
- en: 'The domain knowledge of medical doctors is also widely adopted when designing
    deep learning models for segmentation tasks in medical images. The types of domain
    knowledge from medical doctors utilized in deep segmentation models can be divided
    into four categories:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计医学图像分割任务的深度学习模型时，医生的领域知识也被广泛采用。医疗医生在深度分割模型中利用的领域知识可以分为四类：
- en: '1.'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练模式，
- en: '2.'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general diagnostic patterns they view images,
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像时的一般诊断模式，
- en: '3.'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the anatomical priors (e.g., shape, location, topology) of lesions or organs,
    and
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 病变或器官的解剖先验信息（例如，形状、位置、拓扑）以及
- en: '4.'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: other hand-crafted features they give special attention to.
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，他们特别关注的其他手工特征。
- en: 4.3.1 Training Pattern of Medical Doctors
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 医生的训练模式
- en: Similar to disease diagnosis and lesion/organ detection, many research works
    for the object segmentation in medical images also mimic the training pattern
    of medical doctors, which involves assigning tasks that increase in difficulty
    over time. In this process, the curriculum learning technique or its derivative
    methods like self-paced learning (SPL) are also utilized [[217](#bib.bib217)].
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断和病变/器官检测，许多医学图像中的目标分割研究也模仿了医学医生的训练模式，其中包括分配逐渐增加难度的任务。在这个过程中，还利用了课程学习技术或其衍生方法，如自适应学习（SPL）[[217](#bib.bib217)]。
- en: For example, for the segmentation of multi-organ CT images [[186](#bib.bib186)],
    each annotated image is divided into small patches. During the training process,
    patches producing large error by the network are selected with a high probability.
    In this manner, the network can focus sampling on difficult regions, resulting
    in improved performance. In addition, [[30](#bib.bib30)] combines the SPL with
    the active learning for the pulmonary segmentation in 3D images. This system achieves
    the state-of-the-art segmentation results.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于多器官CT图像的分割[[186](#bib.bib186)]，每张标注的图像被分成小块。在训练过程中，网络产生较大误差的块会被高概率选择。这样，网络可以将采样重点放在困难区域，从而提高性能。此外，[[30](#bib.bib30)]将SPL与主动学习相结合，用于3D图像中的肺部分割。该系统实现了最先进的分割结果。
- en: Moreover, a three-stage curriculum learning approach is proposed for liver tumor
    segmentation [[187](#bib.bib187)]. The first stage is performed on the whole input
    volume to initialize the network, then the second stage of learning focuses on
    tumor-specific features by training the network on the tumor patches, and finally
    the network is retrained on the whole input in the third stage. This approach
    exhibits significant improvement when compared with the commonly used cascade
    counterpart in MICCAI 2017 liver tumor segmentation (LiTS) challenge dataset.
    More examples can also be found in left ventricle segmentation [[188](#bib.bib188)],
    finger bones segmentation [[189](#bib.bib189)] and vessel segmentation [[190](#bib.bib190)].
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提出了一种三阶段课程学习方法用于肝肿瘤分割[[187](#bib.bib187)]。第一阶段在整个输入体积上进行以初始化网络，第二阶段的学习集中在肿瘤特定特征上，通过对肿瘤块的训练来完成，最后在第三阶段对整个输入进行重新训练。与MICCAI
    2017肝肿瘤分割（LiTS）挑战数据集中常用的级联方法相比，这种方法表现出显著的改进。更多例子还可以在左心室分割[[188](#bib.bib188)]、手指骨分割[[189](#bib.bib189)]和血管分割[[190](#bib.bib190)]中找到。
- en: 4.3.2 General Diagnostic Patterns of Medical Doctors
  id: totrans-578
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 医学医生的一般诊断模式
- en: In the lesion or organ segmentation tasks, some specific patterns that medical
    doctors adopted are also incorporated into the network.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在病变或器官分割任务中，一些医学医生采用的特定模式也被融入到网络中。
- en: For example, during the visual inspection of CT images, radiologists often change
    window widths and window centers to help to make decision on uncertain nodules.
    This pattern is mimicked in [[191](#bib.bib191)]. In particular, image patches
    of different window widths and window centers are stacked together as the input
    of the deep learning model to gain rich information. The evaluation implemented
    on the public LIDC-IDRI dataset indicates that the proposed method achieves promising
    performance on lung nodule segmentation compared with the state-of-the-art methods.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在CT图像的视觉检查过程中，放射科医生通常会改变窗口宽度和窗口中心，以帮助决策不确定的结节。这种模式在[[191](#bib.bib191)]中被模仿。特别地，不同窗口宽度和窗口中心的图像块被叠加在一起作为深度学习模型的输入，以获取丰富的信息。对公开的LIDC-IDRI数据集的评估表明，该方法在肺结节分割方面的表现优于最先进的方法。
- en: In addition, experienced clinicians generally assess the cardiac morphology
    and function from multiple standard views, using both long-axis (LA) and short-axis
    (SA) images to form an understanding of the cardiac anatomy. Inspired by the above
    observation, a cardiac MR segmentation method is proposed which takes three LA
    and one SA views as the input [[32](#bib.bib32)]. In particular, the features
    are firstly extracted using a multi-view autoencoder (MAE) structure, and then
    are fed in a segmentation network. Experimental results show that this method
    has a superior segmentation accuracy over state-of-the-art methods.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经验丰富的临床医生通常通过多种标准视图来评估心脏的形态和功能，使用长轴（LA）和短轴（SA）图像来形成对心脏解剖学的理解。受上述观察的启发，提出了一种心脏MR分割方法，该方法以三视LA和一视SA作为输入[[32](#bib.bib32)]。特别地，首先使用多视图自编码器（MAE）结构提取特征，然后将这些特征输入到分割网络中。实验结果表明，该方法在分割准确性上优于最先进的方法。
- en: Furthermore, expert manual segmentation usually relies on the boundaries of
    anatomical structures of interest. For instance, radiologists segmenting a liver
    from CT images would usually trace liver edges first, and then deduce the internal
    segmentation mask. Correspondingly, boundary-aware CNNs are proposed in [[192](#bib.bib192)]
    for medical image segmentation. The networks are designed to account for organ
    boundary information, both by providing a special network edge branch and edge-aware
    loss terms. The effectiveness of these boundary aware segmentation networks are
    tested on BraTS 2018 dataset for the task of brain tumor segmentation.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，专家手动分割通常依赖于感兴趣的解剖结构的边界。例如，放射科医生在从CT图像中分割肝脏时，通常会先描绘肝脏的边缘，然后推导出内部分割掩模。相应地，[[192](#bib.bib192)]中提出了边界感知CNN用于医学图像分割。这些网络设计用于考虑器官边界信息，既提供了一个特殊的网络边缘分支，也提供了边缘感知的损失项。这些边界感知分割网络在BraTS
    2018数据集上的脑肿瘤分割任务中进行了有效性测试。
- en: Recently, the diagnostic pattern named as ‘divide-and-conquer manner’ is mimicked
    in the GTV[LN] detection and segmentation method [[193](#bib.bib193)]. Concretely,
    the GTV[LN] is first divided into two subgroups of ‘tumor-proximal’ and ‘tumor-distal’,
    by means of binary of soft distance gating. Then a multi-branch detection-by-segmentation
    network is trained with each branch specializing on learning one GTV[LN] category
    features. After fusing the outs from multi-branch, the method shows significant
    improvements on the mean recall from 72.5% to 78.2%. Another example of using
    the diagnostic pattern of medical doctors can be found in gross tumor and clinical
    target volume segmentation [[194](#bib.bib194)].
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在GTV[LN]检测和分割方法中模仿了名为“分而治之”的诊断模式[[193](#bib.bib193)]。具体来说，GTV[LN]首先通过软距离门控的二值化被分为“肿瘤近端”和“肿瘤远端”两个子组。然后，训练一个多分支的检测-分割网络，每个分支专注于学习一个GTV[LN]类别的特征。经过多分支融合，方法在平均召回率上从72.5%显著提高到78.2%。另一个使用医学医生诊断模式的例子可以在肿瘤总体和临床靶体积分割中找到[[194](#bib.bib194)]。
- en: 4.3.3 Anatomical Priors of Lesions or Organs
  id: totrans-584
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 病变或器官的解剖学先验
- en: 'In comparison to non-medical images, medical images have many anatomical priors
    such as the shape, position and topology of organs or lesions. Experienced medical
    doctors greatly rely on these anatomical priors when they are doing segmentation
    tasks on these images. Incorporating the knowledge of anatomical priors into deep
    learning models has been demonstrated to be an effective way for accurate medical
    image segmentation. Generally speaking, there are three different approaches to
    incorporate these anatomical priors into deep learning models: (1) incorporating
    anatomical priors in the post-processing stage, (2) incorporating anatomical priors
    as regularization terms in the loss function and (3) learning anatomical priors
    via generative models.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 与非医学图像相比，医学图像具有许多解剖学先验，例如器官或病变的形状、位置和拓扑结构。经验丰富的医生在对这些图像进行分割任务时非常依赖这些解剖学先验。将解剖学先验的知识融入深度学习模型已被证明是准确医学图像分割的有效方法。一般来说，将这些解剖学先验融入深度学习模型有三种不同的方法：（1）在后处理阶段融入解剖学先验，（2）将解剖学先验作为损失函数中的正则化项，以及（3）通过生成模型学习解剖学先验。
- en: Incorporating anatomical priors in post-processing stage
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理阶段融入解剖学先验
- en: The first approach is to incorporate the anatomical priors in the post processing
    stage. The result of a segmentation network is often blurry and post-processing
    is generally needed to refine the segmentation result.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法是在后处理阶段中融入解剖学先验。分割网络的结果通常是模糊的，后处理通常是必要的，以细化分割结果。
- en: For example, according to the pathology, most of breast tumors begin in glandular
    tissues and are located inside the mammary layer [[218](#bib.bib218)]. This position
    feature is utilized by [[195](#bib.bib195)] in its post-processing stage where
    a fully connected conditional random field (CRF) model is employed. In particular,
    the position of tumors and their relative locations with mammary layer are added
    as a new term in CRF energy function to obtain better segmentation results.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，根据病理学，大多数乳腺肿瘤始于腺体组织，并位于乳腺层内[[218](#bib.bib218)]。[[195](#bib.bib195)]在其后处理阶段利用了这一位置特征，其中采用了全连接条件随机场（CRF）模型。具体而言，将肿瘤的位置及其与乳腺层的相对位置作为CRF能量函数中的新项，以获得更好的分割结果。
- en: Besides, some research first learn the anatomical priors, and then incorporate
    them into the post-processing stage to help produce anatomically plausible segmentation
    results [[33](#bib.bib33), [196](#bib.bib196), [197](#bib.bib197)]. For instance,
    the latent representation of anatomically correct cardiac shape is first learned
    by using adversarial variational autoencoder (aVAE), then be used to convert erroneous
    segmentation maps into anatomically plausible ones [[196](#bib.bib196)]. Experiments
    manifest that aVAE is able to accommodate any segmentation method, and convert
    its anatomically implausible results to plausible ones without affecting its overall
    geometric and clinical metrics.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究首先学习解剖学先验，然后将其融入后处理阶段，以帮助生成解剖学上合理的分割结果[[33](#bib.bib33), [196](#bib.bib196),
    [197](#bib.bib197)]。例如，通过使用对抗变分自编码器（aVAE）首先学习解剖学上正确的心脏形状，然后用于将错误的分割图转换为解剖学上合理的图[[196](#bib.bib196)]。实验表明，aVAE能够适应任何分割方法，并将其解剖学上不合理的结果转换为合理的结果，而不影响整体的几何和临床指标。
- en: Another example in [[33](#bib.bib33)] introduces the post-processing step based
    on denoising autoencoders (DAE) for lung segmentation. In particular, the DAE
    is trained using only segmentation masks, then the learned representations of
    anatomical shape and topological constraints are imposed on the original segmentation
    results (as shown in Fig. [18](#S4.F18 "Figure 18 ‣ 4.3.3 Anatomical Priors of
    Lesions or Organs ‣ 4.3 Incorporating Knowledge from Medical Doctors ‣ 4 Lesion
    and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")). By applying the Post-DAE on the resulting
    masks from arbitrary segmentation methods, the lung anatomical segmentation of
    X-ray images shows plausible results.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[33](#bib.bib33)]中的另一个例子介绍了基于去噪自编码器（DAE）的后处理步骤，用于肺部分割。具体而言，DAE仅使用分割掩膜进行训练，然后将学习到的解剖形状和拓扑约束应用于原始分割结果（如图[18](#S4.F18
    "Figure 18 ‣ 4.3.3 Anatomical Priors of Lesions or Organs ‣ 4.3 Incorporating
    Knowledge from Medical Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on
    Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")所示）。通过在任意分割方法得到的掩膜上应用Post-DAE，X光图像的肺部解剖分割显示出合理的结果。
- en: '![Refer to caption](img/2960236e9b94ca1410afe2987350d46f.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/2960236e9b94ca1410afe2987350d46f.png)'
- en: 'Figure 18: The example of integrating the shape prior in the post-process stage
    [[33](#bib.bib33)].'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：在后处理阶段集成形状先验的例子[[33](#bib.bib33)]。
- en: Incorporating anatomical priors as regularization terms in the loss function
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 将解剖学先验作为正则化项纳入损失函数中。
- en: 'The second approach is incorporating anatomical priors as regularization terms
    in the objective function of segmentation networks. For example, for the segmentation
    of cardiac MR images, a network called as SRSCN is proposed [[40](#bib.bib40)].
    SRSCN comprises a shape reconstruction neural network (SRNN) and a spatial constraint
    network (SCN). SRNN aims to maintain a realistic shape of the resulting segmentation
    and the SCN is adopted to incorporate the spatial information of the 2D slices.
    The loss of the SRSCN comes from three parts: the segmentation loss, the shape
    reconstruction (SR) loss for shape regularization, and the spatial constraint
    (SC) loss to assist segmentation. The results using images from 45 patients demonstrate
    the effectiveness of the SR and SC regularization terms, and show the superiority
    of segmentation performance of the SRSCN over the conventional schemes.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是将解剖先验作为分割网络目标函数中的正则化项进行整合。例如，在心脏MR图像的分割中，提出了一种名为SRSCN的网络[[40](#bib.bib40)]。SRSCN包括一个形状重建神经网络（SRNN）和一个空间约束网络（SCN）。SRNN旨在保持分割结果的真实形状，而SCN则用于整合2D切片的空间信息。SRSCN的损失来自三个部分：分割损失、用于形状正则化的形状重建（SR）损失，以及辅助分割的空间约束（SC）损失。使用45名患者的图像的结果展示了SR和SC正则化项的有效性，并显示了SRSCN在分割性能上相较于传统方案的优越性。
- en: Another example in this category is the one designed for skin lesion segmentation
    [[201](#bib.bib201)]. In this work, the star shape prior is encoded as a new loss
    term in a FCN to improve its segmentation of skin lesions from their surrounding
    healthy skin. In this manner, the non-star shape segments in FCN prediction maps
    are penalized to guarantee a global structure in segmentation results. The experimental
    results on the ISBI 2017 skin segmentation challenge dataset demonstrate the advantage
    of regularizing FCN parameters by the star shape prior.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别中的另一个例子是针对皮肤病变分割设计的[[201](#bib.bib201)]。在这项工作中，星形状先验被编码为FCN中的一个新损失项，以改善其从周围健康皮肤中分割皮肤病变的能力。通过这种方式，FCN预测图中的非星形状分割会受到惩罚，以确保分割结果的全球结构。ISBI
    2017皮肤分割挑战数据集上的实验结果展示了通过星形状先验正则化FCN参数的优势。
- en: More examples in this category can be found in gland segmentation [[200](#bib.bib200)],
    kidney segmentation [[199](#bib.bib199)], liver segmentation [[202](#bib.bib202)]
    and cardiac segmentation [[198](#bib.bib198), [203](#bib.bib203)].
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别中的更多例子可以在腺体分割[[200](#bib.bib200)]、肾脏分割[[199](#bib.bib199)]、肝脏分割[[202](#bib.bib202)]和心脏分割[[198](#bib.bib198),
    [203](#bib.bib203)]中找到。
- en: Learning anatomical priors via generative models
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成模型学习解剖先验
- en: In the third approach, the anatomical priors (especially the shape prior) are
    learned by some generative models first and then incorporated into segmentation
    networks.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三种方法中，解剖先验（尤其是形状先验）首先由一些生成模型学习，然后被整合到分割网络中。
- en: For example, in the cardiac MR segmentation process, a shape multi-view autoencoder
    (MAE) is proposed to learn shape priors from MR images of multiple standard views
    [[32](#bib.bib32)]. The information encoded in the latent space of the trained
    shape MAE is incorporated into multi-view U-Net (MV U-Net) in the fuse block to
    guide the segmentation process.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在心脏MR分割过程中，提出了一种形状多视角自编码器（MAE）以从多个标准视角的MR图像中学习形状先验[[32](#bib.bib32)]。训练好的形状MAE在潜在空间中编码的信息被整合到多视角U-Net（MV
    U-Net）的融合块中，以指导分割过程。
- en: Another example is shown in [[206](#bib.bib206)], where the shape constrained
    network (SCN) is proposed to incorporate the shape prior into the eye segmentation
    network. More specifically, the prior information is first learned by a VAE-GAN,
    and then the pre-trained encoder and discriminator are leveraged to regularize
    the training process.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子见于[[206](#bib.bib206)]，其中提出了形状约束网络（SCN），以将形状先验整合到眼部分割网络中。更具体地说，先验信息首先由VAE-GAN学习，然后利用预训练的编码器和判别器来正则化训练过程。
- en: More examples can be found in brain geometry segmentation in MRI [[204](#bib.bib204)],
    3D fine renal artery segmentation [[205](#bib.bib205)], overlapping cervical cytoplasms
    segmentation [[207](#bib.bib207)], scapula segmentation [[208](#bib.bib208)],
    liver segmentation [[209](#bib.bib209)], carotid segmentation [[210](#bib.bib210)],
    and head and neck segmentation [[211](#bib.bib211)].
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的例子可以在MRI脑部几何分割[[204](#bib.bib204)]、3D精细肾动脉分割[[205](#bib.bib205)]、重叠的颈部细胞质分割[[207](#bib.bib207)]、肩胛骨分割[[208](#bib.bib208)]、肝脏分割[[209](#bib.bib209)]、颈动脉分割[[210](#bib.bib210)]和头颈部分割[[211](#bib.bib211)]中找到。
- en: 4.3.4 Other Hand-crafted Features
  id: totrans-602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 其他手工特征
- en: 'Besides anatomical priors, some hand-crafted features are also utilized for
    segmentation tasks. Generally speaking, there are two ways to incorporate the
    hand-crafted features into deep learning models: the feature-level fusion and
    the input-level fusion.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解剖先验外，还利用了一些手工特征用于分割任务。一般来说，将手工特征融入深度学习模型有两种方式：特征级融合和输入级融合。
- en: In the feature-level fusion, the hand-crafted features and the features learned
    by the deep models are concatenated. For example, for the gland segmentation in
    histopathology images [[213](#bib.bib213)], two handcrafted features, namely invariant
    LBP features as well as $H\&amp;E$ components, are firstly calculated from images.
    Then these features are concatenated with the features generated from the last
    convolutional layer of the network for predicting the segmentation results. Similarly,
    in the brain structure segmentation [[212](#bib.bib212)], the spatial atlas prior
    is first represented as a vector and then concatenated with the deep features.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征级融合中，手工特征与深度模型学习到的特征进行连接。例如，在组织病理图像中的腺体分割 [[213](#bib.bib213)]，首先从图像中计算两个手工特征，即不变的
    LBP 特征以及 $H\&amp;E$ 组分。然后，这些特征与网络最后一个卷积层生成的特征连接以预测分割结果。同样，在脑部结构分割 [[212](#bib.bib212)]
    中，空间图谱先表示为向量，然后与深度特征连接。
- en: For the input-level fusion, the hand-crafted features are transformed into the
    input patches. Then the original image patches and the feature-transformed patches
    are fed into a deep segmentation network. For example in [[214](#bib.bib214)],
    for automatic brain tumor segmentation in MRI images, three handcrafted features
    (i.e., mean intensity, LBP and HOG) are firstly extracted. Based on these features,
    a SVM is employed to generate confidence surface modality (CSM) patches. Then
    the CSM patches and the original patches from MRI images are fed into a segmentation
    network. This method achieves good performance on BRATS2015 dataset.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入级融合，手工特征被转换为输入补丁。然后，将原始图像补丁和特征转换的补丁输入到深度分割网络中。例如，在 [[214](#bib.bib214)]
    中，对于 MRI 图像中的自动脑肿瘤分割，首先提取三个手工特征（即均值强度、LBP 和 HOG）。基于这些特征，使用 SVM 生成置信度表面模态 (CSM)
    补丁。然后，将 CSM 补丁和 MRI 图像中的原始补丁输入到分割网络中。这种方法在 BRATS2015 数据集上取得了良好的性能。
- en: 'TABLE VIII: The comparison of the quantitative metrics for some medical object
    segmentation methods after incorporating domain knowledge'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：在融合领域知识后，一些医学目标分割方法的定量指标比较
- en: '| Reference |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/带领域知识 &#124;'
- en: '| Dice score |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| Dice 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[32](#bib.bib32)] | 3D U-Net/MV U-Net | 0.923/0.926 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| [[32](#bib.bib32)] | 3D U-Net/MV U-Net | 0.923/0.926 |'
- en: '| [[205](#bib.bib205)] | V-Net/DPA-DenseBiasNet | 0.787/0.861 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | V-Net/DPA-DenseBiasNet | 0.787/0.861 |'
- en: '| [[177](#bib.bib177)] |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| [[177](#bib.bib177)] |'
- en: '&#124; Masked cycle-GAN/Tumore &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 掩码循环-GAN/肿瘤 &#124;'
- en: '&#124; aware semi-unsupervised &#124;'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知半监督 &#124;'
- en: '| 0.630/0.800 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 0.630/0.800 |'
- en: '| [[174](#bib.bib174)] |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| [[174](#bib.bib174)] |'
- en: '&#124; modality specific method/dual-stream &#124;'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态特定方法/双流 &#124;'
- en: '&#124; encoder-decoder multi-model method &#124;'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器多模型方法 &#124;'
- en: '| 0.838/0.860 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 0.838/0.860 |'
- en: '| [[40](#bib.bib40)] | U-Net/SRSCN | 0.737/0.830 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bib40)] | U-Net/SRSCN | 0.737/0.830 |'
- en: '| [[27](#bib.bib27)] |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] |'
- en: '&#124; U-Net/SC-GAN &#124;'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net/SC-GAN &#124;'
- en: '| 0.742/0.824 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 0.742/0.824 |'
- en: '| [[183](#bib.bib183)] |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] |'
- en: '&#124; cycle- and shape-consistency GAN &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环和形状一致性 GAN &#124;'
- en: '&#124; trained without/with synthetic data &#124;'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无/有合成数据训练 &#124;'
- en: '| 0.678/0.744 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| 0.678/0.744 |'
- en: In addition, using handcrafted features by input-level fusion is also adopted
    in cell nuclei segmentation [[215](#bib.bib215)]. In particular, as nuclei are
    expected to have an approximately round shape, a map of gradient convergence is
    computed and be used by CNN as an extra channel besides the fluorescence microscopy
    image. Experimental results show higher F1-score when compared with other methods.
    Another example in this category can be found in brain tumor segmentation [[214](#bib.bib214)].
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输入级融合中使用手工特征也应用于细胞核分割 [[215](#bib.bib215)]。特别是，由于细胞核预计具有近似圆形，因此计算梯度收敛图，并由
    CNN 作为额外通道使用，除了荧光显微镜图像。实验结果表明，与其他方法相比，F1 分数更高。另一个此类别的例子可以在脑肿瘤分割 [[214](#bib.bib214)]
    中找到。
- en: 4.4 Summary
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 总结
- en: The aforementioned sections described researches of incorporating domain knowledge
    for object (lesion or organ) segmentation in medical images. The segmentation
    performance of some methods is shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3.4
    Other Hand-crafted Features ‣ 4.3 Incorporating Knowledge from Medical Doctors
    ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"), where the Dice score is used
    with a higher score indicating a better performance.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 上述章节描述了在医学图像中将领域知识应用于对象（病灶或器官）分割的研究。一些方法的分割性能显示在表格 [VIII](#S4.T8 "TABLE VIII
    ‣ 4.3.4 Other Hand-crafted Features ‣ 4.3 Incorporating Knowledge from Medical
    Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis") 中，其中Dice评分用于评估，得分越高表示性能越好。
- en: We can see that similar to disease diagnosis, using information from natural
    images like ImageNet is quite popular for lesion and organ segmentation tasks.
    The reason behind it may be that segmentation can be seen as a specific classification
    problem. Meanwhile, besides the ImageNet, some video datasets can also be utilized
    for segmenting 3D medical images (e.g., [[171](#bib.bib171)]). Using extra medical
    datasets with different modalities has also been proven to be helpful, although
    most applications are limited in using MRI to help segmentation tasks in CT images
    [[174](#bib.bib174)]. Leveraging domain knowledge from medical doctors is also
    widely used in segmentation tasks. In particular, the anatomical priors of organs
    are widely adopted. However, anatomical priors are only suitable for the segmentation
    of organs with fixed shapes like hearts [[32](#bib.bib32)] or lungs [[32](#bib.bib32)].
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与疾病诊断类似，使用自然图像（如ImageNet）的信息在病灶和器官分割任务中非常流行。这背后的原因可能是分割可以被视为一个特定的分类问题。同时，除了ImageNet，一些视频数据集也可以用于分割3D医学图像（例如，[[171](#bib.bib171)]）。利用具有不同模态的额外医学数据集也被证明是有帮助的，尽管大多数应用仍限于使用MRI来辅助CT图像的分割任务[[174](#bib.bib174)]。利用医学医生的领域知识在分割任务中也被广泛使用。特别是，器官的解剖先验被广泛采用。然而，解剖先验仅适用于具有固定形状的器官，如心脏[[32](#bib.bib32)]或肺部[[32](#bib.bib32)]。
- en: 5 Other Medical Applications
  id: totrans-633
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 其他医学应用
- en: In this section, we briefly introduce the works on incorporating domain knowledge
    in other medical images analysis applications, like medical image reconstruction,
    medical image retrieval and medical report generation.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了将领域知识融入其他医学图像分析应用的研究，如医学图像重建、医学图像检索和医学报告生成。
- en: 5.1 Medical Image Reconstruction
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 医学图像重建
- en: The objective of medical image reconstruction is reconstructing a diagnostic
    image from a number of measurements (e.g., X-ray projections in CT or the spatial
    frequency information in MRI). Deep learning based methods have been widely applied
    in this field [[219](#bib.bib219), [220](#bib.bib220)]. It is also quite common
    that external information is incorporated into deep learning models for medical
    image reconstruction.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像重建的目标是从多个测量值（例如，CT中的X射线投影或MRI中的空间频率信息）中重建诊断图像。基于深度学习的方法已在这一领域得到广泛应用[[219](#bib.bib219)、[220](#bib.bib220)]。将外部信息融入深度学习模型进行医学图像重建也是相当常见的。
- en: Some methods incorporate hand-crafted features in the medical image reconstruction
    process. For example, a network model called as DAGAN is proposed for the reconstruction
    of compressed sensing magnetic resonance imaging (CS-MRI) [[221](#bib.bib221)].
    In the DAGAN, to better preserve texture and edges in the reconstruction process,
    the adversarial loss is coupled with a content loss. In addition, the frequency-domain
    information is incorporated to enforce similarity in both the image and frequency
    domains. Experimental results show that the DAGAN method provides superior reconstruction
    with preserved perceptual image details.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法在医学图像重建过程中融合了手工特征。例如，提出了一种名为DAGAN的网络模型，用于压缩感知磁共振成像（CS-MRI）的重建[[221](#bib.bib221)]。在DAGAN中，为了更好地保留重建过程中的纹理和边缘，敌对损失与内容损失相结合。此外，还结合了频域信息，以强制图像和频域之间的相似性。实验结果表明，DAGAN方法在保留感知图像细节方面提供了优越的重建效果。
- en: In [[222](#bib.bib222)], a new image reconstruction method is proposed to solve
    the limited-angle and limited sources breast cancer diffuse optical tomography
    (DOT) image reconstruction problem in a strong scattering medium. By adaptively
    focusing on important features and filtering irrelevant and noisy ones using the
    Fuzzy Jaccard loss, the network is able to reduce false positive reconstructed
    pixels and reconstruct more accurate images.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[222](#bib.bib222)]中，提出了一种新的图像重建方法，用于解决在强散射介质中的有限角度和有限来源乳腺癌扩散光学断层成像（DOT）图像重建问题。通过自适应地关注重要特征并使用模糊Jaccard损失过滤不相关和噪声特征，网络能够减少假阳性重建像素，并重建更准确的图像。
- en: Similarly, a GAN-based method is proposed to recover MRI images of the target
    contrast [[223](#bib.bib223)]. The method simultaneously leverages the relatively
    low-spatial-frequency information available in the collected evidence for the
    target contrast and the relatively high-spatial frequency information available
    in the source contrast. Demonstrations on brain MRI datasets indicate the proposed
    method outperforms state-of-the-art reconstruction methods, with enhanced recovery
    of high-frequency tissue structure, and improved reliability against feature leakage
    or loss.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，提出了一种基于GAN的方法来恢复目标对比度的MRI图像[[223](#bib.bib223)]。该方法同时利用了在收集到的证据中相对较低空间频率的信息和在源对比度中相对较高空间频率的信息。对脑MRI数据集的演示表明，该方法优于最新的重建方法，增强了高频组织结构的恢复，并提高了对特征泄漏或丢失的可靠性。
- en: 5.2 Medical Image Retrieval
  id: totrans-640
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 医学图像检索
- en: The hospitals are producing large amount of imaging data and the development
    of medical image retrieval, especially the content based image retrieval (CBIR)
    systems can be of great help to aid clinicians in browsing these large datasets.
    Deep learning methods have been applied to CBIR and have achieved high performance
    due to their superior capability for extracting features automatically.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 医院正在生成大量的影像数据，医学图像检索的发展，特别是基于内容的图像检索（CBIR）系统，可以极大地帮助临床医生浏览这些大型数据集。深度学习方法已被应用于CBIR，并由于其自动提取特征的优越能力，取得了高性能。
- en: It is also quite common that these deep learning models for CBIR utilize external
    information beyond the given medical datasets. Some methods adopt the transfer
    learning to utilize the knowledge from natural images or external medical datasets
    [[224](#bib.bib224), [225](#bib.bib225), [226](#bib.bib226)]. For example, the
    VGG model pre-trained based on ImageNet is used in brain tumor retrieval process
    [[226](#bib.bib226)], where a block-wise fine-tuning strategy is proposed to enhance
    the retrieval performance on the T1-weighted CE-MRI dataset. Another example can
    be found in x-ray image retrieval process [[225](#bib.bib225)], where a model
    pre-trained on the large augmented dataset is fine-tuned on the target dataset
    to extract general features.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些用于CBIR的深度学习模型也常常利用超出给定医学数据集的外部信息。一些方法采用迁移学习来利用自然图像或外部医学数据集的知识[[224](#bib.bib224),
    [225](#bib.bib225), [226](#bib.bib226)]。例如，基于ImageNet预训练的VGG模型在脑肿瘤检索过程中得到应用[[226](#bib.bib226)]，其中提出了一种块级微调策略，以提升T1加权CE-MRI数据集上的检索性能。另一个例子是x射线图像检索过程[[225](#bib.bib225)]，在该过程中，预训练于大规模增强数据集的模型被微调到目标数据集上以提取通用特征。
- en: Besides, as features play an important role in the similarly analysis in CBIR,
    some methods fuse prior features with deep features. In particular, in the chest
    radiograph image retrieval process, the decision values of binary features and
    texture features are combined with the deep features in the form of decision-level
    fusion [[227](#bib.bib227)]. Similarly, the metadata such as patients’ age and
    gender is combined with the image-based features extracted from deep CNN for X-ray
    chest pathology image retrieval [[228](#bib.bib228)]. Furthermore, the features
    extracted from saliency areas can also be injected into the features extracted
    from the whole image for the high retrieval accuracy [[224](#bib.bib224)].
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，特征在CBIR的相似性分析中扮演着重要角色，一些方法将先验特征与深度特征融合。特别是在胸部X光图像检索过程中，二进制特征和纹理特征的决策值与深度特征通过决策级融合的形式结合[[227](#bib.bib227)]。类似地，患者的年龄和性别等元数据与从深度CNN中提取的基于图像的特征结合，用于X光胸部病理图像检索[[228](#bib.bib228)]。此外，从显著区域提取的特征也可以注入到从整个图像提取的特征中，以实现更高的检索准确性[[224](#bib.bib224)]。
- en: 5.3 Medical Report Generation
  id: totrans-644
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 医学报告生成
- en: Recently, deep learning models for image captioning have been successfully applied
    for automatic generation of medical reports [[229](#bib.bib229), [230](#bib.bib230)].
    It is also found that incorporating external knowledge can help deep learning
    models to generate better medical reports.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习模型在图像描述生成中的成功应用已被用于医学报告的自动生成[[229](#bib.bib229), [230](#bib.bib230)]。研究还发现，融入外部知识可以帮助深度学习模型生成更好的医学报告。
- en: For example, some methods try to incorporate specific or general patterns that
    doctors adopt when writing reports. For example, radiologists generally write
    reports using certain templates. Therefore, some templates are used during the
    sentence generation process [[231](#bib.bib231), [232](#bib.bib232)]. Furthermore,
    as the explanation given by doctors is fairly simple and phrase changing does
    not change their meaning, a model-agnostic method is presented to learn the short
    text description to explain this decision process [[233](#bib.bib233)].
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些方法尝试融入医生在撰写报告时采用的特定或通用模式。例如，放射科医师通常使用特定的模板撰写报告。因此，在句子生成过程中使用了某些模板[[231](#bib.bib231),
    [232](#bib.bib232)]。此外，由于医生给出的解释相对简单且短语的变化不会改变其含义，提出了一种模型无关的方法来学习短文本描述以解释这一决策过程[[233](#bib.bib233)]。
- en: 'In addition, radiologists follow some procedures when writing reports: they
    generally first check a patient’s images for abnormal findings, then write reports
    by following certain templates, and adjust statements in the templates for each
    individual case when necessary [[234](#bib.bib234)]. This process is mimicked
    in [[232](#bib.bib232)], which first transfers the visual features of medical
    images into an abnormality graph, then retrieves text templates based on the abnormalities
    and their attributes for chest X-ray images.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，放射科医师在撰写报告时遵循一些程序：他们通常首先检查患者的影像是否有异常发现，然后按照特定的模板撰写报告，并根据每个个案的具体情况调整模板中的陈述[[234](#bib.bib234)]。这一过程在[[232](#bib.bib232)]中得到了模拟，该研究首先将医学影像的视觉特征转化为异常图谱，然后根据异常及其属性为胸部X光图像检索文本模板。
- en: In [[235](#bib.bib235)], a pre-constructed graph embedding module (modeled with
    a graph CNN) on multiple disease findings is utilized to assist the generation
    of reports. The incorporation of knowledge graph allows for dedicated feature
    learning for each disease finding and the relationship modeling between them.
    Experiments on the publicly accessible dataset (IU-RR) demonstrate the superior
    performance of the method integrated with the proposed graph module.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[235](#bib.bib235)]中，利用了一个基于图卷积神经网络（graph CNN）建模的预构建图嵌入模块来辅助生成报告。知识图谱的引入允许对每种疾病发现进行专门的特征学习，并建模它们之间的关系。在公开数据集（IU-RR）上的实验展示了与所提出的图模块集成的方法的卓越性能。
- en: 6 Research Challenges and Future Directions
  id: totrans-649
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 研究挑战与未来方向
- en: The aforementioned sections reviewed research studies on deep learning models
    that incorporate medical domain knowledge for various tasks. Although using medical
    domain knowledge in deep learning models is quite popular, there are still many
    difficulties about the selection, representation and incorporating method of medical
    domain knowledge. In the following sections, we summarize challenges and future
    directions in this area.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 上述部分回顾了将医学领域知识融入深度学习模型进行各种任务的研究。尽管在深度学习模型中使用医学领域知识非常流行，但在医学领域知识的选择、表示和融入方法上仍存在许多困难。在接下来的部分中，我们总结了该领域的挑战和未来方向。
- en: 6.1 The Challenges Related to the Identification and Selection of Medical Domain
    Knowledge
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 医学领域知识识别与选择相关的挑战
- en: Identifying medical domain knowledge is not an easy task. Firstly, the experiences
    of medical doctors are generally subjective and fuzzy. Not all medical doctors
    can give accurate and objective descriptions on what kinds of experiences they
    have leveraged to finish a given task. In addition, experiences of medical doctors
    can vary significantly or even contradictory to each other. Furthermore, medical
    doctors generally utilize many types of domain knowledge simultaneously. Finally,
    currently the medical domain knowledge is identified manually, and there is no
    existing work on the automatically and comprehensively identifying medical domain
    knowledge for a given area.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 识别医疗领域知识并不是一件容易的事。首先，医疗医生的经验通常是主观和模糊的，并非所有医生都能准确客观地描述他们为完成特定任务而借用的经验。此外，医疗医生的经验可能会有显著差异，甚至相互矛盾。此外，医疗医生通常同时利用多种领域知识。最后，目前医疗领域知识是通过手动识别的，尚无关于自动和全面识别特定领域医疗知识的现有工作。
- en: One solution to the automatic identifying medical knowledge is through text
    mining techniques on the guidelines, books, and medical reports related to different
    medical areas. Guidelines or books are more robust than individual experiences.
    Medical reports generally contain specific terms (usually adjectives) that describe
    the characteristics of tumors. These terms, containing important information to
    help doctors to make diagnosis, can potentially be beneficial for deep learning
    models.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 识别医疗知识的一个解决方案是通过对涉及不同医疗领域的指南、书籍和医疗报告进行文本挖掘技术。指南或书籍比个人经验更为可靠。医疗报告通常包含描述肿瘤特征的特定术语（通常是形容词）。这些术语包含对医生诊断有帮助的重要信息，可能对深度学习模型有利。
- en: Besides the identification of medical domain knowledge, how to select appropriate
    knowledge to help image analysis tasks is also challenging. It should be noted
    that a common practice of medical doctors may not be able to help deep learning
    models because *the domain knowledge might be learned by the deep learning model
    from training data.* We believe that the knowledge that is not easily learned
    by a deep learning model can greatly help the model to improve its performance.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 除了医疗领域知识的识别，如何选择合适的知识来帮助图像分析任务也是一项挑战。需要注意的是，医疗医生的常见做法可能无法帮助深度学习模型，因为*领域知识可能通过训练数据被深度学习模型学习到。*
    我们认为，那些不易被深度学习模型学习到的知识可以极大地帮助模型提高性能。
- en: 6.2 The Challenges Related to the Representation of Medical Domain Knowledge
  id: totrans-655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 医疗领域知识表示相关的挑战
- en: The original domain knowledge of medical doctors is generally in the form of
    descriptive sentences like ‘we will focus more on the margin areas of a tumor
    to determine whether it is benign or malignant’, or ‘we often compare bilateral
    images to make decision’. How to transform the knowledge into appropriate representations
    and incorporate it into deep learning models need a careful design.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗医生的原始领域知识通常以描述性句子的形式存在，例如‘我们将更加关注肿瘤的边缘区域，以确定其是良性还是恶性’，或‘我们经常比较双侧图像以做出决策’。如何将这些知识转化为适当的表示并将其纳入深度学习模型中需要仔细设计。
- en: There are generally four ways to represent a certain type of medical domain
    knowledge. One is to represent knowledge as patches or highlighted images (as
    in [[94](#bib.bib94)]). This is generally used when doctors pay more attention
    to specific areas. The second approach is to represent knowledge as feature vectors
    [[85](#bib.bib85)]. This way is suitable when the selected domain knowledge can
    be described as certain features. The third approach is to represent domain knowledge
    as extra labels [[59](#bib.bib59), [52](#bib.bib52)], which is suitable for the
    knowledge in clinical reports or extra feature attributes of diseases. The last
    approach is to embed medical domain knowledge in network structure design, which
    is suitable to represent high-level domain knowledge like the training pattern
    and diagnostic pattern of medical doctors[[69](#bib.bib69), [134](#bib.bib134),
    [135](#bib.bib135)].
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 表示某种医学领域知识通常有四种方式。一种是将知识表示为补丁或高亮图像（如[[94](#bib.bib94)]所示）。当医生更关注特定区域时，通常使用这种方法。第二种方法是将知识表示为特征向量[[85](#bib.bib85)]。当选定的领域知识可以描述为某些特征时，这种方式适用。第三种方法是将领域知识表示为额外的标签[[59](#bib.bib59),
    [52](#bib.bib52)]，适用于临床报告中的知识或疾病的额外特征属性。最后一种方法是将医学领域知识嵌入网络结构设计中，适用于表示高层次的领域知识，如医学医生的训练模式和诊断模式[[69](#bib.bib69),
    [134](#bib.bib134), [135](#bib.bib135)]。
- en: 6.3 The Challenges Related to the Incorporating Methods of Medical Domain Knowledge
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 医学领域知识整合方法相关的挑战
- en: Currently, there are four ways to incorporate medical domain knowledge. The
    first is to transform the knowledge into certain patches or highlighted images
    and put them as extra inputs [[54](#bib.bib54)]. The second approach is via concatenation
    [[213](#bib.bib213)]. The domain knowledge are generally transformed into feature
    vectors and concatenated with those extracted by deep learning models. The third
    way is the attention mechanism [[38](#bib.bib38)]. The approach is applicable
    when doctors focus on certain areas of medical images or focus on certain time
    slots on medical videos. The last one is to learn the domain knowledge by using
    some specific network structures like generative models [[32](#bib.bib32), [206](#bib.bib206)].
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，医学领域知识的融入方式有四种。第一种是将知识转化为某些补丁或高亮图像，作为额外输入[[54](#bib.bib54)]。第二种方法是通过拼接[[213](#bib.bib213)]。领域知识通常转化为特征向量，并与深度学习模型提取的特征向量拼接。第三种方法是注意力机制[[38](#bib.bib38)]。当医生关注医学图像的特定区域或医学视频的特定时间段时，这种方法适用。最后一种方法是通过使用一些特定的网络结构，如生成模型[[32](#bib.bib32),
    [206](#bib.bib206)]，来学习领域知识。
- en: However, most of the existing works only incorporate a single type of medical
    domain knowledge, or a few types of medical domain knowledge of the same modality
    (e.g., a number of hand-crafted features). In practice, experienced medical doctors
    usually combine different experience in different stages.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数现有工作仅融入单一类型的医学领域知识，或少数相同模态的医学领域知识（例如，一些手工制作的特征）。实际上，有经验的医学医生通常在不同阶段结合不同的经验。
- en: There are some researches that simultaneously introduce high-level domain knowledge
    (e.g., diagnostic pattern, training pattern) and the low-level one (e.g., hand-crafted
    features, anatomical priors). In particular, the high-level domain knowledge is
    incorporated as input images, and low-level one is learned by using specific network
    structures [[32](#bib.bib32)]. In addition, besides incorporating into network
    directly, the information from low-level domain knowledge can also be used to
    design the training orders when combined with the easy-to-hard training pattern
    [[29](#bib.bib29)]. We believe that simultaneously incorporating multiple kinds
    of medical domain knowledge can better help deep learning models in various medical
    applications.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究同时引入高层次领域知识（例如，诊断模式、训练模式）和低层次领域知识（例如，手工制作的特征、解剖先验）。特别是，高层次领域知识作为输入图像引入，低层次知识则通过特定的网络结构进行学习[[32](#bib.bib32)]。此外，除了直接融入网络中外，低层次领域知识的信息还可以结合易到难的训练模式用于设计训练顺序[[29](#bib.bib29)]。我们认为，同时融入多种医学领域知识可以更好地帮助深度学习模型在各种医学应用中。
- en: 6.4 Future Research Directions
  id: totrans-662
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 未来的研究方向
- en: Besides the above challenges, there are several directions that we feel need
    further investigation in the future.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述挑战外，还有几个方向需要在未来进一步探讨。
- en: Domain adaptation
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应
- en: Domain adaptation is developed to transfer the information from a source domain
    to a target one. Via techniques like adversarial learning [[153](#bib.bib153)],
    domain adaptation is able to narrow the domain shift between the source domain
    and the target one in input space [[236](#bib.bib236)], feature space [[237](#bib.bib237),
    [238](#bib.bib238)] and output space [[239](#bib.bib239), [240](#bib.bib240)].
    It can be naturally adopted to transfer knowledge of one medical dataset to another
    [[181](#bib.bib181)], even when they have different imaging modes or belong to
    different diseases [[172](#bib.bib172), [177](#bib.bib177)].
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应的目的是将信息从源领域转移到目标领域。通过诸如对抗学习[[153](#bib.bib153)]等技术，领域适应能够缩小源领域和目标领域之间的领域偏移，涵盖输入空间[[236](#bib.bib236)]、特征空间[[237](#bib.bib237),
    [238](#bib.bib238)]和输出空间[[239](#bib.bib239), [240](#bib.bib240)]。它可以自然地用于将一个医学数据集的知识转移到另一个数据集[[181](#bib.bib181)]，即使它们具有不同的成像模式或属于不同的疾病[[172](#bib.bib172),
    [177](#bib.bib177)]。
- en: In addition, unsupervised domain adaptation (UDA) is a promising avenue to enhance
    the performance of deep neural networks on the target domain, using labels only
    from the source domain. This is especially useful for medical field, as annotating
    the medical images is quite labor-intensive and the lack of annotations is quite
    common in medical datasets. Some examples have demonstrated the effectiveness
    of UDA in disease diagnosis and organ segmentation [[175](#bib.bib175), [93](#bib.bib93),
    [241](#bib.bib241), [242](#bib.bib242)], but further depth study needs to be implemented
    in the future.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**无监督领域适应（UDA）**是一种有前景的途径，可以在目标领域提高深度神经网络的性能，使用的标签仅来自源领域。这对于医学领域尤其有用，因为标注医学图像非常费力，医学数据集中缺乏标注的情况也很常见。一些实例已经证明了UDA在疾病诊断和器官分割中的有效性[[175](#bib.bib175),
    [93](#bib.bib93), [241](#bib.bib241), [242](#bib.bib242)]，但未来需要进行更深入的研究。
- en: The knowledge graph
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱
- en: We believe that the knowledge graph [[243](#bib.bib243)], with the character
    of embedding different types of knowledge, is a generic and flexible approach
    to incorporate multi-modal medical domain knowledge. Although rarely used at present,
    it also shows advantage in medical image analysis tasks, especially in medical
    report generation [[232](#bib.bib232)]. We believe that more types of knowledge
    graph can be used to represent and learn domain knowledge in medical image analysis
    tasks.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，具有嵌入不同类型知识特性的知识图谱[[243](#bib.bib243)]，是一种通用且灵活的方法，可以整合多模态医学领域知识。尽管目前应用较少，但它在医学图像分析任务中也显示出优势，特别是在医学报告生成[[232](#bib.bib232)]方面。我们相信，更多类型的知识图谱可以用于表示和学习医学图像分析任务中的领域知识。
- en: According to different relationships in graphs, there are three possible types
    of knowledge graphs can be established. The first knowledge graph reflects the
    relationship among different kinds of medical domain knowledge with respect to
    a certain disease. This knowledge graph can help us identify a few key types of
    knowledge that may help to improve the performance deep learning models. The second
    type of knowledge graph may reflects the relationship among different diseases.
    This knowledge graph can help us to find out the potential domain knowledge from
    other related diseases. The third type one can describe the relationship among
    medical datasets. These datasets can belong to different diseases and in different
    imaging modes (e.g., CT, MRI, ultrasound). This type of knowledge graph will help
    to identify the external datasets that may help to improve the performance of
    the current deep learning model.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图中的不同关系，可以建立三种可能类型的知识图谱。第一种知识图谱反映了不同种类的医学领域知识与某种疾病之间的关系。这种知识图谱可以帮助我们识别一些关键类型的知识，可能有助于提高深度学习模型的性能。第二种知识图谱可能反映了不同疾病之间的关系。这种知识图谱可以帮助我们发现其他相关疾病中的潜在领域知识。第三种知识图谱可以描述医学数据集之间的关系。这些数据集可以属于不同的疾病并具有不同的成像模式（例如CT、MRI、超声）。这种类型的知识图谱将有助于识别可能有助于提高当前深度学习模型性能的外部数据集。
- en: The generative models
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型
- en: The generative models, like GAN and AE, have shown great promise to be applied
    to incorporate medical domain knowledge into deep learning models, especially
    for segmentation tasks. GAN has shown its capability to leverage information from
    extra datasets with different imaging modes (e.g., using a MRI dataset to help
    segmenting CT images [[175](#bib.bib175), [177](#bib.bib177)]). In addition, GAN
    is able to learn important features contained in medical images in a weakly or
    fully unsupervised manner and therefore is quite suitable for medical image analysis.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型，如GAN和AE，已经显示出将医学领域知识融入深度学习模型的巨大潜力，特别是在分割任务中。GAN展示了其利用来自不同成像模式的额外数据集的信息的能力（例如，使用MRI数据集帮助分割CT图像[[175](#bib.bib175),
    [177](#bib.bib177)]）。此外，GAN能够以弱监督或完全无监督的方式学习医学图像中的重要特征，因此非常适合医学图像分析。
- en: AE-based models have already achieved a great success in extracting features,
    especially the shape priors in objects like organs or lesions in a fully unsupervised
    manner [[32](#bib.bib32), [206](#bib.bib206)]. The features learning by AE can
    also be easily integrated into the training process of networks.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 基于AE的模型在特征提取方面已经取得了很大成功，特别是在以完全无监督方式处理器官或病变等对象的形状先验方面[[32](#bib.bib32), [206](#bib.bib206)]。AE学习到的特征也可以轻松地集成到网络的训练过程中。
- en: Network architecture search (NAS)
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构搜索（NAS）
- en: At last, we mentioned in the previous section that one challenge is to find
    appropriate network architectures to incorporate medical domain knowledge. We
    believe one approach to address this problem is the technique of network architecture
    search (NAS). NAS has demonstrated its capability to automatically find a good
    network architecture in many computer vision tasks [[244](#bib.bib244)] and has
    a great promise in the medical domain [[245](#bib.bib245)]. For instance, when
    some hand-crafted features are used as the domain knowledge, with the help of
    NAS, a network structure can be identified with the special connections between
    domain knowledge features and deep features. In addition, instead of designing
    the feature fusion method (feature-level fusion, decision-level fusion or input-level
    fusion) for these two kinds of features, the integrating phase and integrating
    intensity of these two kinds of features can also be determined during the searching
    process.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在前一节提到，一个挑战是找到合适的网络架构以融入医学领域知识。我们相信，解决这个问题的一种方法是网络架构搜索（NAS）技术。NAS已证明其在许多计算机视觉任务中自动找到良好网络架构的能力[[244](#bib.bib244)]，并在医学领域具有极大的潜力[[245](#bib.bib245)]。例如，当使用一些手工特征作为领域知识时，在NAS的帮助下，可以识别出网络结构，其中包含领域知识特征和深度特征之间的特殊连接。此外，不仅可以为这两种特征设计特征融合方法（特征级融合、决策级融合或输入级融合），还可以在搜索过程中确定这两种特征的集成阶段和集成强度。
- en: 7 Conclusion
  id: totrans-675
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we give a comprehensive survey on incorporating medical domain
    knowledge into deep learning models for various medical image analysis tasks ranging
    from disease diagnosis, lesion, organ and abnormality detection to lesion and
    organ segmentation. In addition, some other tasks such as medical image reconstruction,
    medical image retrieval and medical report generation are also included. For each
    task, we first introduce different types of medical domain knowledge, and then
    review some works of introducing domain knowledge into target tasks by using different
    incorporating methods. From this survey, we can see that with appropriate integrating
    methods, different kinds of domain knowledge can help deep learning models to
    better accomplish corresponding tasks.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对将医学领域知识融入深度学习模型以处理各种医学图像分析任务进行了全面的综述，包括从疾病诊断、病变、器官和异常检测到病变和器官分割等任务。此外，还包括一些其他任务，如医学图像重建、医学图像检索和医学报告生成。对于每个任务，我们首先介绍不同类型的医学领域知识，然后回顾一些将领域知识引入目标任务的不同融入方法。从这项综述中，我们可以看出，通过适当的集成方法，不同类型的领域知识可以帮助深度学习模型更好地完成相应的任务。
- en: Besides reviewing current works on incorporating domain knowledge into deep
    learning models, we also summarize challenges of using medical domain knowledge,
    and introduce the identification, selection, representation and incorporating
    method of medical domain knowledge. Finally, we give some future directions of
    incorporating domain knowledge for medical image analysis tasks.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 除了回顾将领域知识融入深度学习模型的现有工作外，我们还总结了使用医学领域知识的挑战，并介绍了医学领域知识的识别、选择、表示和融入方法。最后，我们给出了一些将领域知识融入医学图像分析任务的未来方向。
- en: Acknowledgments
  id: totrans-678
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Natural Science Foundation of China
    [grant numbers 61976012, 61772060]; the National Key R&D Program of China [grant
    number 2017YFB1301100]; and the CERNET Innovation Project [grant number NGII20170315].
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了中国国家自然科学基金资助[资助编号61976012, 61772060]；中国国家重点研发计划资助[资助编号2017YFB1301100]；以及CERNET创新项目资助[资助编号NGII20170315]。
- en: References
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Esteva, B. Kuprel, R. A. Novoa, J. M. Ko, S. M. Swetter, H. M. Blau,
    and S. Thrun, “Dermatologist-level classification of skin cancer with deep neural
    networks,” *Nature*, vol. 542, no. 7639, pp. 115–118, 2017.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Esteva, B. Kuprel, R. A. Novoa, J. M. Ko, S. M. Swetter, H. M. Blau,
    和 S. Thrun，“利用深度神经网络进行皮肤癌的皮肤科医生级分类，” *Nature*，第542卷，第7639期，页码115–118，2017年。'
- en: '[2] S. Y. Shin, S. Lee, I. D. Yun, S. M. Kim, and K. M. Lee, “Joint weakly
    and semi-supervised deep learning for localization and classification of masses
    in breast ultrasound images,” *IEEE TMI*, vol. 38, no. 3, pp. 762–774, 2019.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Y. Shin, S. Lee, I. D. Yun, S. M. Kim, 和 K. M. Lee，“联合弱监督和半监督深度学习用于乳腺超声图像中肿块的定位和分类，”
    *IEEE TMI*，第38卷，第3期，页码762–774，2019年。'
- en: '[3] Z. Zhou, J. Y. Shin, L. Zhang, S. R. Gurudu, M. B. Gotway, and J. Liang,
    “Fine-tuning convolutional neural networks for biomedical image analysis: Actively
    and incrementally,” *CVPR2017*, pp. 4761–4772, 2017.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Z. Zhou, J. Y. Shin, L. Zhang, S. R. Gurudu, M. B. Gotway, 和 J. Liang，“微调卷积神经网络用于生物医学图像分析：主动和逐步，”
    *CVPR2017*，页码4761–4772，2017年。'
- en: '[4] W. Zhu, C. Liu, W. Fan, and X. Xie, “Deeplung: Deep 3d dual path nets for
    automated pulmonary nodule detection and classification,” *workshop on applications
    of computer vision*, pp. 673–681, 2018.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] W. Zhu, C. Liu, W. Fan, 和 X. Xie，“Deeplung：用于自动化肺结节检测和分类的深度3D双路径网络，” *计算机视觉应用研讨会*，页码673–681，2018年。'
- en: '[5] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness of
    data,” *IEEE Intelligent Systems*, vol. 24, no. 2, pp. 8–12, 2009.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Halevy, P. Norvig, 和 F. Pereira，“数据的非凡有效性，” *IEEE Intelligent Systems*，第24卷，第2期，页码8–12，2009年。'
- en: '[6] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns, R. C.
    Green, D. Harvey, C. R. Jack, W. Jagust, J. C. Morris, R. C. Petersen, J. Salazar,
    A. J. Saykin, L. M. Shaw, A. W. Toga, and J. Q. Trojanowski, “The alzheimer’s
    disease neuroimaging initiative 3: Continued innovation for clinical trial improvement,”
    *Alzheimer’s and Dementia*, vol. 13, no. 5, pp. 561 – 571, 2017\. [Online]. Available:
    http://www.sciencedirect.com/science/article/pii/S1552526016330722'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns, R.
    C. Green, D. Harvey, C. R. Jack, W. Jagust, J. C. Morris, R. C. Petersen, J. Salazar,
    A. J. Saykin, L. M. Shaw, A. W. Toga, 和 J. Q. Trojanowski，“阿尔茨海默病神经影像学倡议3：临床试验改进的持续创新，”
    *Alzheimer’s and Dementia*，第13卷，第5期，页码561 – 571，2017年。[在线]。可用网址：http://www.sciencedirect.com/science/article/pii/S1552526016330722'
- en: '[7] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,
    J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto *et al.*, “The autism
    brain imaging data exchange: towards a large-scale evaluation of the intrinsic
    brain architecture in autism,” *Molecular psychiatry*, vol. 19, no. 6, pp. 659–667,
    2014.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,
    J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto *等*，“自闭症脑成像数据交换：朝着对自闭症中固有脑结构的大规模评估迈进，”
    *Molecular Psychiatry*，第19卷，第6期，页码659–667，2014年。'
- en: '[8] C. MICCAI, “Automated cardiac diagnosis challenge (acdc),” 2017\. [Online].
    Available: https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. MICCAI，“自动化心脏诊断挑战（acdc），” 2017年。[在线]。可用网址：https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html'
- en: '[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. Summers, “Chestx-ray14:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” 09 2017.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, 和 R. Summers，“Chestx-ray14：医院规模的胸部X光数据库及常见胸部疾病的弱监督分类和定位基准，”
    2017年9月。'
- en: '[10] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *et al.*, “The
    lung image database consortium (lidc) and image database resource initiative (idri):
    a completed reference database of lung nodules on ct scans,” *Medical physics*,
    vol. 38, no. 2, pp. 915–931, 2011.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *等*, “肺部图像数据库联盟（LIDC）和图像数据库资源倡议（IDRI）：完成的
    CT 扫描肺结节参考数据库，” *医学物理学*, 卷 38，第 2 期，第 915–931 页, 2011。'
- en: '[11] A. T. B. v. G. Colin Jacobs, Arnaud Arindra Adiyoso Setio, “Lung nodule
    analysis 2016,” 2016\. [Online]. Available: https://luna16.grand-challenge.org/'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. T. B. v. G. Colin Jacobs, Arnaud Arindra Adiyoso Setio, “肺结节分析 2016,”
    2016\. [在线]. 可用链接: https://luna16.grand-challenge.org/'
- en: '[12] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang,
    K. Zhu, D. Laird, R. L. Ball *et al.*, “Mura: Large dataset for abnormality detection
    in musculoskeletal radiographs,” *arXiv preprint arXiv:1712.06957*, 2017.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang,
    K. Zhu, D. Laird, R. L. Ball *等*, “Mura：用于肌肉骨骼放射图像异常检测的大型数据集，” *arXiv 预印本 arXiv:1712.06957*,
    2017。'
- en: '[13] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara,
    C. Berger, S. M. Ha, M. Rozycki *et al.*, “Identifying the best machine learning
    algorithms for brain tumor segmentation, progression assessment, and overall survival
    prediction in the brats challenge,” *arXiv preprint arXiv:1811.02629*, 2018.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara,
    C. Berger, S. M. Ha, M. Rozycki *等*, “在 BRATS 挑战赛中识别用于脑肿瘤分割、进展评估和总体生存预测的最佳机器学习算法，”
    *arXiv 预印本 arXiv:1811.02629*, 2018。'
- en: '[14] A. Hoover, V. Kouznetsova, and M. Goldbaum, “Locating blood vessels in
    retinal images by piecewise threshold probing of a matched filter response,” *IEEE
    Transactions on Medical imaging*, vol. 19, no. 3, pp. 203–210, 2000.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Hoover, V. Kouznetsova, 和 M. Goldbaum, “通过逐段阈值探测匹配滤波响应来定位视网膜图像中的血管，”
    *IEEE 医学成像学报*, 卷 19，第 3 期，第 203–210 页, 2000。'
- en: '[15] M. Heath, K. Bowyer, D. Kopans, R. Moore, and P. Kegelmeyer, “The digital
    database for screening mammography,” *Proceedings of the Fourth International
    Workshop on Digital Mammography*, 01 2000.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Heath, K. Bowyer, D. Kopans, R. Moore, 和 P. Kegelmeyer, “用于筛查乳腺X光照片的数字数据库，”
    *第四届数字乳腺摄影国际研讨会论文集*, 01 2000。'
- en: '[16] K. Yan, X. Wang, L. Lu, and R. M. Summers, “Deeplesion: automated mining
    of large-scale lesion annotations and universal lesion detection with deep learning,”
    *Journal of Medical Imaging*, vol. 5, no. 3, p. 036501, 2018.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Yan, X. Wang, L. Lu, 和 R. M. Summers, “Deeplesion：利用深度学习自动挖掘大规模病变标注和通用病变检测，”
    *医学成像杂志*, 卷 5，第 3 期，第 036501 页, 2018。'
- en: '[17] J. K. T. Alexander A., “Efficient and generalizable statistical models
    of shape and appearance for analysis of cardiac mri,” *Medical Image Analysis*,
    vol. 12, no. 3, pp. 335 – 357, 2008\. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1361841508000029'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. K. T. Alexander A., “用于心脏 MRI 分析的高效且通用的形状和外观统计模型，” *医学图像分析*, 卷 12，第
    3 期，第 335–357 页, 2008\. [在线]. 可用链接: http://www.sciencedirect.com/science/article/pii/S1361841508000029'
- en: '[18] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
    B. Helba, A. Kalloo, K. Liopyris, M. Marchetti *et al.*, “Skin lesion analysis
    toward melanoma detection 2018: A challenge hosted by the international skin imaging
    collaboration (isic),” *arXiv preprint arXiv:1902.03368*, 2019.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
    B. Helba, A. Kalloo, K. Liopyris, M. Marchetti *等*, “皮肤病变分析以检测黑色素瘤 2018：由国际皮肤成像合作组织（ISIC）主办的挑战赛，”
    *arXiv 预印本 arXiv:1902.03368*, 2019。'
- en: '[19] H. Chen, X. J. Qi, J. Z. Cheng, and P. A. Heng, “Deep contextual networks
    for neuronal structure segmentation,” in *Thirtieth AAAI conference on artificial
    intelligence*, 2016.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Chen, X. J. Qi, J. Z. Cheng, 和 P. A. Heng, “用于神经结构分割的深度上下文网络，” 见于 *第三十届
    AAAI 人工智能会议*, 2016。'
- en: '[20] F. Ciompi, B. de Hoop, S. J. van Riel, K. Chung, E. T. Scholten, M. Oudkerk,
    P. A. de Jong, M. Prokop, and B. van Ginneken, “Automatic classification of pulmonary
    peri-fissural nodules in computed tomography using an ensemble of 2d views and
    a convolutional neural network out-of-the-box,” *Medical Image Analysis*, vol. 26,
    no. 1, pp. 195–202, 2015.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] F. Ciompi, B. de Hoop, S. J. van Riel, K. Chung, E. T. Scholten, M. Oudkerk,
    P. A. de Jong, M. Prokop, 和 B. van Ginneken, “使用 2D 视图和卷积神经网络的组合进行计算机断层扫描中肺部裂缝结节的自动分类，”
    *医学图像分析*, 卷 26，第 1 期，第 195–202 页, 2015。'
- en: '[21] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    and R. M. Summers, “Deep convolutional neural networks for computer-aided detection:
    Cnn architectures, dataset characteristics and transfer learning,” *IEEE TMI*,
    vol. 35, no. 5, pp. 1285–1298, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    和 R. M. Summers, “用于计算机辅助检测的深度卷积神经网络：CNN架构、数据集特征和迁移学习，”*IEEE TMI*，第35卷，第5期，第1285–1298页，2016。'
- en: '[22] B. J. Erickson, P. Korfiatis, Z. Akkus, and T. L. Kline, “Machine learning
    for medical imaging,” *Radiographics*, vol. 37, no. 2, pp. 505–515, 2017.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] B. J. Erickson, P. Korfiatis, Z. Akkus, 和 T. L. Kline, “医学影像中的机器学习，”*Radiographics*，第37卷，第2期，第505–515页，2017。'
- en: '[23] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV 2017*, 2017,
    pp. 2223–2232.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros, “使用循环一致对抗网络的无配对图像到图像翻译，”在*ICCV
    2017*中，2017，第2223–2232页。'
- en: '[24] B. Huynh, K. Drukker, and M. Giger, “Mo-de-207b-06: Computer-aided diagnosis
    of breast ultrasound images using transfer learning from deep convolutional neural
    networks,” *Medical physics*, vol. 43, no. 6Part30, pp. 3705–3705, 2016.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] B. Huynh, K. Drukker, 和 M. Giger, “Mo-de-207b-06: 使用深度卷积神经网络的迁移学习进行乳腺超声图像的计算机辅助诊断，”*Medical
    physics*，第43卷，第6期第30页，第3705–3705页，2016。'
- en: '[25] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. J. Pan 和 Q. Yang, “迁移学习综述，”*IEEE Transactions on knowledge and data
    engineering*，第22卷，第10期，第1345–1359页，2009。'
- en: '[26] R. K. Samala, H.-P. Chan, L. Hadjiiski, M. A. Helvie, C. D. Richter, and
    K. H. Cha, “Breast cancer diagnosis in digital breast tomosynthesis: effects of
    training sample size on multi-stage transfer learning using deep neural nets,”
    *IEEE TMI*, vol. 38, no. 3, pp. 686–696, 2018.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. K. Samala, H.-P. Chan, L. Hadjiiski, M. A. Helvie, C. D. Richter, 和
    K. H. Cha, “在数字乳腺断层扫描中的乳腺癌诊断：训练样本大小对使用深度神经网络的多阶段迁移学习的影响，”*IEEE TMI*，第38卷，第3期，第686–696页，2018。'
- en: '[27] F. Yu, J. Zhao, Y. Gong, Z. Wang, Y. Li, F. Yang, B. Dong, Q. Li, and
    L. Zhang, “Annotation-free cardiac vessel segmentation via knowledge transfer
    from retinal images,” in *MICCAI2019*, 2019, pp. 714–722.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F. Yu, J. Zhao, Y. Gong, Z. Wang, Y. Li, F. Yang, B. Dong, Q. Li, 和 L.
    Zhang, “通过从视网膜图像知识转移的无注释心脏血管分割，”在*MICCAI2019*中，2019，第714–722页。'
- en: '[28] G. Maicas, A. P. Bradley, J. C. Nascimento, I. Reid, and G. Carneiro,
    “Training medical image analysis systems like radiologists,” in *MICCAI2018*,
    2018, pp. 546–554.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] G. Maicas, A. P. Bradley, J. C. Nascimento, I. Reid, 和 G. Carneiro, “像放射科医生一样训练医学图像分析系统，”在*MICCAI2018*中，2018，第546–554页。'
- en: '[29] Y. Tang, X. Wang, A. P. Harrison, L. Lu, J. Xiao, and R. M. Summers, “Attention-guided
    curriculum learning for weakly supervised classification and localization of thoracic
    diseases on chest radiographs,” in *International Workshop on Machine Learning
    in Medical Imaging*.   Springer, 2018, pp. 249–258.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Tang, X. Wang, A. P. Harrison, L. Lu, J. Xiao, 和 R. M. Summers, “用于胸部X光片上胸部疾病的弱监督分类和定位的注意力引导课程学习，”在*International
    Workshop on Machine Learning in Medical Imaging*中，Springer，2018，第249–258页。'
- en: '[30] W. Wang, Y. Lu, B. Wu, T. Chen, D. Z. Chen, and J. Wu, “Deep active self-paced
    learning for accurate pulmonary nodule segmentation,” in *MICCAI2018*.   Springer,
    2018, pp. 723–731.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Wang, Y. Lu, B. Wu, T. Chen, D. Z. Chen, 和 J. Wu, “用于肺结节分割的深度主动自适应学习，”在*MICCAI2018*中，Springer，2018，第723–731页。'
- en: '[31] S.-M. Hsu, W.-H. Kuo, F.-C. Kuo, and Y.-Y. Liao, “Breast tumor classification
    using different features of quantitative ultrasound parametric images,” *International
    journal of computer assisted radiology and surgery*, vol. 14, no. 4, pp. 623–633,
    2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S.-M. Hsu, W.-H. Kuo, F.-C. Kuo, 和 Y.-Y. Liao, “使用定量超声参数图像的不同特征进行乳腺肿瘤分类，”*International
    journal of computer assisted radiology and surgery*，第14卷，第4期，第623–633页，2019。'
- en: '[32] C. Chen, C. Biffi, G. Tarroni, S. E. Petersen, W. Bai, and D. Rueckert,
    “Learning shape priors for robust cardiac mr segmentation from multi-view images.”
    in *Medical Image Computing and Computer-Assisted Intervention*, 2019, pp. 523–531.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C. Chen, C. Biffi, G. Tarroni, S. E. Petersen, W. Bai, 和 D. Rueckert,
    “从多视角图像中学习形状先验以进行稳健的心脏MR分割。”在*Medical Image Computing and Computer-Assisted Intervention*中，2019，第523–531页。'
- en: '[33] A. J. Larrazabal, C. E. Martinez, and E. Ferrante, “Anatomical priors
    for image segmentation via post-processing with denoising autoencoders,” in *MICCAI2019*,
    2019, pp. 585–593.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. J. Larrazabal, C. E. Martinez, 和 E. Ferrante, “通过后处理与去噪自编码器结合的解剖先验用于图像分割，”在*MICCAI2019*中，2019，第585–593页。'
- en: '[34] S. Valverde, M. Salem, M. Cabezas, D. Pareto, J. C. Vilanova, L. Ramiotorrenta,
    A. Rovira, J. Salvi, A. Oliver, and X. Llado, “One-shot domain adaptation in multiple
    sclerosis lesion segmentation using convolutional neural networks,” *NeuroImage:
    Clinical*, vol. 21, p. 101638, 2019.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Valverde, M. Salem, M. Cabezas, D. Pareto, J. C. Vilanova, L. Ramiotorrenta,
    A. Rovira, J. Salvi, A. Oliver, 和 X. Llado，“使用卷积神经网络进行多发性硬化症病灶分割的单次领域适应，” *NeuroImage:
    Clinical*，第 21 卷，第 101638 页，2019 年。'
- en: '[35] C. Ji, S. Basodi, X. Xiao, and Y. Pan, “Infant sound classification on
    multi-stage cnns with hybrid features and prior knowledge,” in *International
    Conference on AI and Mobile Services*.   Springer, 2020, pp. 3–16.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] C. Ji, S. Basodi, X. Xiao, 和 Y. Pan，“基于多阶段卷积神经网络的婴儿声音分类，结合混合特征和先验知识，”在
    *International Conference on AI and Mobile Services* 中。 Springer，2020 年，第 3–16
    页。'
- en: '[36] Y. Xie, J. Zhang, S. Liu, W. Cai, and Y. Xia, “Lung nodule classification
    by jointly using visual descriptors and deep features,” in *Medical Computer Vision
    and Bayesian and Graphical Models for Biomedical Imaging*.   Springer, 2016, pp.
    116–125.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Xie, J. Zhang, S. Liu, W. Cai, 和 Y. Xia，“通过联合使用视觉描述符和深度特征进行肺结节分类，”在
    *Medical Computer Vision and Bayesian and Graphical Models for Biomedical Imaging*
    中。 Springer，2016 年，第 116–125 页。'
- en: '[37] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang, “Diagnose
    like a radiologist: Attention guided convolutional neural network for thorax disease
    classification,” *arXiv preprint arXiv:1801.09927*, 2018.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, 和 Y. Yang，“像放射科医生一样诊断：用于胸部疾病分类的注意力引导卷积神经网络，”
    *arXiv preprint arXiv:1801.09927*，2018 年。'
- en: '[38] L. Li, M. Xu, X. Wang, L. Jiang, and H. Liu, “Attention based glaucoma
    detection: A large-scale database and cnn model,” in *CVPR2019*, 2019, pp. 10 571–10 580.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] L. Li, M. Xu, X. Wang, L. Jiang, 和 H. Liu，“基于注意力的青光眼检测：一个大规模数据库和 CNN 模型，”在
    *CVPR2019* 中，2019 年，第 10 571–10 580 页。'
- en: '[39] S. Chen, J. Qin, X. Ji, B. Lei, T. Wang, D. Ni, and J.-Z. Cheng, “Automatic
    scoring of multiple semantic attributes with multi-task feature leverage: A study
    on pulmonary nodules in ct images,” *IEEE TMI*, vol. 36, no. 3, pp. 802–814, 2016.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Chen, J. Qin, X. Ji, B. Lei, T. Wang, D. Ni, 和 J.-Z. Cheng，“利用多任务特征利用的自动评分：关于
    CT 图像中肺结节的研究，” *IEEE TMI*，第 36 卷，第 3 期，第 802–814 页，2016 年。'
- en: '[40] Q. Yue, X. Luo, Q. Ye, L. Xu, and X. Zhuang, “Cardiac segmentation from
    lge mri using deep neural network incorporating shape and spatial priors,” in
    *MICCAI2019*.   Springer, 2019, pp. 559–567.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Q. Yue, X. Luo, Q. Ye, L. Xu, 和 X. Zhuang，“利用形状和空间先验的深度神经网络进行心脏分割，”在 *MICCAI2019*
    中。 Springer，2019 年，第 559–567 页。'
- en: '[41] T. G. Debelee, F. Schwenker, A. Ibenthal, and D. Yohannes, “Survey of
    deep learning in breast cancer image analysis,” *Evolving Systems*, pp. 1–21,
    2019.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] T. G. Debelee, F. Schwenker, A. Ibenthal, 和 D. Yohannes，“乳腺癌图像分析中的深度学习调查，”
    *Evolving Systems*，第 1–21 页，2019 年。'
- en: '[42] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical Image Analysis*, vol. 42, pp. 60–88, 2017.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, 和 C. I. Sánchez，“深度学习在医学图像分析中的调查，” *Medical
    Image Analysis*，第 42 卷，第 60–88 页，2017 年。'
- en: '[43] D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,”
    *Annual review of biomedical engineering*, vol. 19, pp. 221–248, 2017.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] D. Shen, G. Wu, 和 H.-I. Suk，“医学图像分析中的深度学习，” *Annual review of biomedical
    engineering*，第 19 卷，第 221–248 页，2017 年。'
- en: '[44] K. Suzuki, “Survey of deep learning applications to medical image analysis,”
    *Medical Imaging Technology*, vol. 35, no. 4, pp. 212–226, 2017.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] K. Suzuki，“深度学习在医学图像分析中的应用调查，” *Medical Imaging Technology*，第 35 卷，第 4
    期，第 212–226 页，2017 年。'
- en: '[45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *neural information processing systems*,
    vol. 141, no. 5, pp. 1097–1105, 2012.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络进行 Imagenet 分类，”
    *neural information processing systems*，第 141 卷，第 5 期，第 1097–1105 页，2012 年。'
- en: '[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” *CVPR2015*,
    pp. 1–9, 2015.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich，“更深入的卷积网络，” *CVPR2015*，第 1–9 页，2015 年。'
- en: '[47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *ICLR*, 2015.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深度卷积网络，” *ICLR*，2015 年。'
- en: '[48] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *CVPR2016*, pp. 770–778, 2016.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，” *CVPR2016*，第 770–778
    页，2016 年。'
- en: '[49] G. Huang, Z. Liu, L. V. Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” *CVPR2017*, pp. 2261–2269, 2017.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] G. Huang, Z. Liu, L. V. Der Maaten, 和 K. Q. Weinberger, “密集连接卷积网络，” *CVPR2017*，第2261–2269页，2017年。'
- en: '[50] J. Y. Kim, H. E. Lee, Y. H. Choi, S. J. Lee, and J. S. Jeon, “Cnn-based
    diagnosis models for canine ulcerative keratitis,” *Scientific reports*, vol. 9,
    no. 1, pp. 1–7, 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Y. Kim, H. E. Lee, Y. H. Choi, S. J. Lee, 和 J. S. Jeon, “基于CNN的犬只溃疡性角膜炎诊断模型，”
    *科学报告*，第9卷，第1期，第1–7页，2019年。'
- en: '[51] X. Li, L. Shen, X. Xie, S. Huang, Z. Xie, X. Hong, and J. Yu, “Multi-resolution
    convolutional networks for chest x-ray radiograph based lung nodule detection,”
    *Artificial intelligence in medicine*, p. 101744, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Li, L. Shen, X. Xie, S. Huang, Z. Xie, X. Hong, 和 J. Yu, “用于胸部X光片肺结节检测的多分辨率卷积网络，”
    *医学中的人工智能*，第101744页，2019年。'
- en: '[52] J. Liu, W. Li, N. Zhao, K. Cao, Y. Yin, Q. Song, H. Chen, and X. Gong,
    “Integrate domain knowledge in training cnn for ultrasonography breast cancer
    diagnosis,” in *MICCAI2018*, 2018, pp. 868–875.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Liu, W. Li, N. Zhao, K. Cao, Y. Yin, Q. Song, H. Chen, 和 X. Gong, “在训练CNN中整合领域知识用于超声乳腺癌诊断，”
    *MICCAI2018*，2018年，第868–875页。'
- en: '[53] M. Mitsuhara, H. Fukui, Y. Sakashita, T. Ogata, T. Hirakawa, T. Yamashita,
    and H. Fujiyoshi, “Embedding human knowledge in deep neural network via attention
    map,” *arXiv preprint arXiv:1905.03540*, 2019.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Mitsuhara, H. Fukui, Y. Sakashita, T. Ogata, T. Hirakawa, T. Yamashita,
    和 H. Fujiyoshi, “通过注意力图在深度神经网络中嵌入人类知识，” *arXiv预印本 arXiv:1905.03540*，2019年。'
- en: '[54] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. J. Fulham, and W. Cai,
    “Knowledge-based collaborative deep learning for benign-malignant lung nodule
    classification on chest ct,” *IEEE TMI*, vol. 38, no. 4, pp. 991–1004, 2019.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. J. Fulham, 和 W. Cai, “基于知识的协作深度学习用于胸部CT的良性-恶性肺结节分类，”
    *IEEE TMI*，第38卷，第4期，第991–1004页，2019年。'
- en: '[55] Y. Bar, I. Diamant, L. Wolf, and H. Greenspan, “Deep learning with non-medical
    training used for chest pathology identification,” in *Medical Imaging 2015: Computer-Aided
    Diagnosis*, vol. 9414.   International Society for Optics and Photonics, 2015,
    p. 94140V.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Bar, I. Diamant, L. Wolf, 和 H. Greenspan, “使用非医学训练的深度学习进行胸部病理识别，” *医学影像2015：计算机辅助诊断*，第9414卷。 国际光学与光子学学会，2015年，第94140V页。'
- en: '[56] G. Wimmer, S. Hegenbart, A. Vécsei, and A. Uhl, “Convolutional neural
    network architectures for the automated diagnosis of celiac disease,” in *International
    Workshop on Computer-Assisted and Robotic Endoscopy*.   Springer, 2016, pp. 104–113.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. Wimmer, S. Hegenbart, A. Vécsei, 和 A. Uhl, “用于乳糜泻自动诊断的卷积神经网络架构，” *计算机辅助与机器人内镜国际研讨会*。 Springer，2016年，第104–113页。'
- en: '[57] H. Cao, S. Bernard, L. Heutte, and R. Sabourin, “Improve the performance
    of transfer learning without fine-tuning using dissimilarity-based multi-view
    learning for breast cancer histology images,” in *International conference image
    analysis and recognition*, 2018, pp. 779–787.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Cao, S. Bernard, L. Heutte, 和 R. Sabourin, “通过不相似性基的多视图学习改进迁移学习性能，无需微调，应用于乳腺癌组织学图像，”
    *国际图像分析与识别会议*，2018年，第779–787页。'
- en: '[58] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” in *CVPR2017*, 2017, pp. 2097–2106.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, 和 R. M. Summers, “Chestx-ray8:
    医院级胸部X光数据库及常见胸部疾病的弱监督分类和定位基准，” *CVPR2017*，2017年，第2097–2106页。'
- en: '[59] S. Hussein, K. Cao, Q. Song, and U. Bagci, “Risk stratification of lung
    nodules using 3d cnn-based multi-task learning,” *international conference information
    processing*, pp. 249–260, 2017.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] S. Hussein, K. Cao, Q. Song, 和 U. Bagci, “使用基于3D CNN的多任务学习进行肺结节风险分层，”
    *国际信息处理会议*，第249–260页，2017年。'
- en: '[60] W. Li, J. Li, K. V. Sarma, K. C. Ho, S. Shen, B. S. Knudsen, A. Gertych,
    and C. W. Arnold, “Path r-cnn for prostate cancer diagnosis and gleason grading
    of histological images,” *IEEE TMI*, vol. 38, no. 4, pp. 945–954, 2018.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] W. Li, J. Li, K. V. Sarma, K. C. Ho, S. Shen, B. S. Knudsen, A. Gertych,
    和 C. W. Arnold, “用于前列腺癌诊断和组织学图像Gleason分级的Path R-CNN，” *IEEE TMI*，第38卷，第4期，第945–954页，2018年。'
- en: '[61] B. Q. Huynh, H. Li, and M. L. Giger, “Digital mammographic tumor classification
    using transfer learning from deep convolutional neural networks,” *Journal of
    Medical Imaging*, vol. 3, no. 3, p. 034501, 2016.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] B. Q. Huynh, H. Li, 和 M. L. Giger, “使用深度卷积神经网络的迁移学习进行数字乳腺X光肿瘤分类，” *医学影像学杂志*，第3卷，第3期，第034501页，2016年。'
- en: '[62] O. Hadad, R. Bakalo, R. Ben-Ari, S. Hashoul, and G. Amit, “Classification
    of breast lesions using cross-modal deep learning,” in *ISBI 2017*.   IEEE, 2017,
    pp. 109–112.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] O. Hadad, R. Bakalo, R. Ben-Ari, S. Hashoul, 和 G. Amit，“使用跨模态深度学习进行乳腺病变分类，”
    在 *ISBI 2017* 中。 IEEE，2017 年，第 109–112 页。'
- en: '[63] R. K. Samala, H.-P. Chan, L. M. Hadjiiski, M. A. Helvie, K. H. Cha, and
    C. D. Richter, “Multi-task transfer learning deep convolutional neural network:
    application to computer-aided diagnosis of breast cancer on mammograms,” *Physics
    in Medicine & Biology*, vol. 62, no. 23, p. 8894, 2017.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. K. Samala, H.-P. Chan, L. M. Hadjiiski, M. A. Helvie, K. H. Cha, 和
    C. D. Richter，“多任务迁移学习深度卷积神经网络：应用于计算机辅助诊断乳腺癌的乳腺X光片，” *Physics in Medicine & Biology*，第
    62 卷，第 23 期，第 8894 页，2017 年。'
- en: '[64] S. Azizi, P. Mousavi, P. Yan, A. Tahmasebi, J. T. Kwak, S. Xu, B. Turkbey,
    P. Choyke, P. Pinto, B. Wood *et al.*, “Transfer learning from rf to b-mode temporal
    enhanced ultrasound features for prostate cancer detection,” *International journal
    of computer assisted radiology and surgery*, vol. 12, no. 7, pp. 1111–1121, 2017.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. Azizi, P. Mousavi, P. Yan, A. Tahmasebi, J. T. Kwak, S. Xu, B. Turkbey,
    P. Choyke, P. Pinto, B. Wood *等*，“从射频到B模式时效增强超声特征的迁移学习用于前列腺癌检测，” *International
    journal of computer assisted radiology and surgery*，第 12 卷，第 7 期，第 1111–1121 页，2017
    年。'
- en: '[65] X. Li, G. Qin, Q. He, L. Sun, H. Zeng, Z. He, W. Chen, X. Zhen, and L. Zhou,
    “Digital breast tomosynthesis versus digital mammography: integration of image
    modalities enhances deep learning-based breast mass classification,” *European
    radiology*, vol. 30, no. 2, pp. 778–788, 2020.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Li, G. Qin, Q. He, L. Sun, H. Zeng, Z. He, W. Chen, X. Zhen, 和 L. Zhou，“数字乳腺断层摄影与数字乳腺X光摄影：图像模态的整合增强了基于深度学习的乳腺肿块分类，”
    *European radiology*，第 30 卷，第 2 期，第 778–788 页，2020 年。'
- en: '[66] X. Han, J. Wang, W. Zhou, C. Chang, S. Ying, and J. Shi, “Deep doubly
    supervised transfer network for diagnosis of breast cancer with imbalanced ultrasound
    imaging modalities,” in *International Conference on Medical Image Computing and
    Computer-Assisted Intervention*.   Springer, 2020, pp. 141–149.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Han, J. Wang, W. Zhou, C. Chang, S. Ying, 和 J. Shi，“深度双重监督迁移网络用于不平衡超声成像模态的乳腺癌诊断，”
    在 *国际医学图像计算与计算机辅助干预会议* 中。 Springer，2020 年，第 141–149 页。'
- en: '[67] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, and P.-A. Heng, “Canet: Cross-disease
    attention network for joint diabetic retinopathy and diabetic macular edema grading,”
    *IEEE TMI*, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, 和 P.-A. Heng，“Canet: 跨疾病注意力网络用于糖尿病视网膜病变和糖尿病性黄斑水肿的分级，”
    *IEEE TMI*，2019 年。'
- en: '[68] Q. Liao, Y. Ding, Z. L. Jiang, X. Wang, C. Zhang, and Q. Zhang, “Multi-task
    deep convolutional neural network for cancer diagnosis,” *Neurocomputing*, vol.
    348, pp. 66–73, 2019.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Q. Liao, Y. Ding, Z. L. Jiang, X. Wang, C. Zhang, 和 Q. Zhang，“用于癌症诊断的多任务深度卷积神经网络，”
    *Neurocomputing*，第 348 卷，第 66–73 页，2019 年。'
- en: '[69] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Medical-based deep curriculum learning
    for improved fracture classification,” in *MICCAI2019*.   Springer, 2019, pp.
    694–702.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, 和 G. Piella，“用于改进骨折分类的基于医学的深度课程学习，” 在 *MICCAI2019*
    中。 Springer，2019 年，第 694–702 页。'
- en: '[70] C. Haarburger, M. Baumgartner, D. Truhn, M. Broeckmann, H. Schneider,
    S. Schrading, C. Kuhl, and D. Merhof, “Multi scale curriculum cnn for context-aware
    breast mri malignancy classification,” in *MICCAI2019*.   Springer, 2019, pp.
    495–503.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Haarburger, M. Baumgartner, D. Truhn, M. Broeckmann, H. Schneider,
    S. Schrading, C. Kuhl, 和 D. Merhof，“多尺度课程CNN用于上下文感知的乳腺MRI恶性分类，” 在 *MICCAI2019*
    中。 Springer，2019 年，第 495–503 页。'
- en: '[71] R. Zhao, X. Chen, Z. Chen, and S. Li, “Egdcl: An adaptive curriculum learning
    framework for unbiased glaucoma diagnosis,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 190–205.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] R. Zhao, X. Chen, Z. Chen, 和 S. Li，“Egdcl：用于无偏青光眼诊断的自适应课程学习框架，” 在 *欧洲计算机视觉会议*
    中。 Springer，2020 年，第 190–205 页。'
- en: '[72] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Curriculum learning for annotation-efficient
    medical image analysis: scheduling data with prior knowledge and uncertainty,”
    *arXiv preprint arXiv:2007.16102*, 2020.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, 和 G. Piella，“用于注释高效医学图像分析的课程学习：通过先验知识和不确定性调度数据，”
    *arXiv 预印本 arXiv:2007.16102*，2020 年。'
- en: '[73] J. Wei, A. Suriawinata, B. Ren, X. Liu, M. Lisovsky, L. Vaickus, C. Brown,
    M. Baker, M. Nasir-Moin, N. Tomita *et al.*, “Learn like a pathologist: Curriculum
    learning by annotator agreement for histopathology image classification,” *arXiv
    preprint arXiv:2009.13698*, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Wei, A. Suriawinata, B. Ren, X. Liu, M. Lisovsky, L. Vaickus, C. Brown,
    M. Baker, M. Nasir-Moin, N. Tomita *等*，“像病理学家一样学习：通过标注者一致性的课程学习用于组织病理学图像分类，” *arXiv预印本
    arXiv:2009.13698*，2020年。'
- en: '[74] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, and Y. Yu,
    “Curriculum feature alignment domain adaptation for epithelium-stroma classification
    in histopathological images,” *IEEE Journal of Biomedical and Health Informatics*,
    2020.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, 和 Y. Yu，“用于表皮-基质分类的课程特征对齐领域适应方法，”
    *IEEE Journal of Biomedical and Health Informatics*，2020年。'
- en: '[75] I. González-Díaz, “Dermaknet: Incorporating the knowledge of dermatologists
    to convolutional neural networks for skin lesion diagnosis,” *IEEE journal of
    biomedical and health informatics*, vol. 23, no. 2, pp. 547–559, 2018.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] I. González-Díaz，“Dermaknet：将皮肤科医生的知识融入卷积神经网络以进行皮肤病变诊断，” *IEEE Journal
    of Biomedical and Health Informatics*，第23卷，第2期，第547–559页，2018年。'
- en: '[76] K. Wang, X. Zhang, S. Huang, F. Chen, X. Zhang, and L. Huangfu, “Learning
    to recognize thoracic disease in chest x-rays with knowledge-guided deep zoom
    neural networks,” *IEEE Access*, vol. 8, pp. 159 790–159 805, 2020.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. Wang, X. Zhang, S. Huang, F. Chen, X. Zhang, 和 L. Huangfu，“利用知识引导的深度缩放神经网络学习识别胸部X光中的胸部疾病，”
    *IEEE Access*，第8卷，第159 790–159 805页，2020年。'
- en: '[77] X. Huang, Y. Fang, M. Lu, F. Yan, J. Yang, and Y. Xu, “Dual-ray net: Automatic
    diagnosis of thoracic diseases using frontal and lateral chest x-rays,” *Journal
    of Medical Imaging and Health Informatics*, vol. 10, no. 2, pp. 348–355, 2020.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Huang, Y. Fang, M. Lu, F. Yan, J. Yang, 和 Y. Xu，“双射线网：利用前视和侧视胸部X光自动诊断胸部疾病，”
    *Journal of Medical Imaging and Health Informatics*，第10卷，第2期，第348–355页，2020年。'
- en: '[78] Z. Yang, Z. Cao, Y. Zhang, M. Han, J. Xiao, L. Huang, S. Wu, J. Ma, and
    P. Chang, “Momminet: Mammographic multi-view mass identification networks,” in
    *International Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 200–210.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. Yang, Z. Cao, Y. Zhang, M. Han, J. Xiao, L. Huang, S. Wu, J. Ma, 和
    P. Chang，“Momminet：乳腺X光多视角肿块识别网络，” 在 *国际医学图像计算与计算机辅助干预会议*。  Springer，2020年，第200–210页。'
- en: '[79] Q. Liu, L. Yu, L. Luo, Q. Dou, and P. A. Heng, “Semi-supervised medical
    image classification with relation-driven self-ensembling model,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Q. Liu, L. Yu, L. Luo, Q. Dou, 和 P. A. Heng，“基于关系驱动自我集成模型的半监督医学图像分类，”
    *IEEE Transactions on Medical Imaging*，2020年。'
- en: '[80] L. Fang, C. Wang, S. Li, H. Rabbani, X. Chen, and Z. Liu, “Attention to
    lesion: Lesion-aware convolutional neural network for retinal optical coherence
    tomography image classification,” *IEEE TMI*, vol. 38, no. 8, pp. 1959–1970, Aug
    2019.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] L. Fang, C. Wang, S. Li, H. Rabbani, X. Chen, 和 Z. Liu，“关注病变：用于视网膜光学相干断层扫描图像分类的病变感知卷积神经网络，”
    *IEEE TMI*，第38卷，第8期，第1959–1970页，2019年8月。'
- en: '[81] H. Cui, Y. Xu, W. Li, L. Wang, and H. Duh, “Collaborative learning of
    cross-channel clinical attention for radiotherapy-related esophageal fistula prediction
    from ct,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 212–220.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Cui, Y. Xu, W. Li, L. Wang, 和 H. Duh，“用于放射治疗相关食管瘘预测的跨通道临床注意力的协作学习，”
    在 *国际医学图像计算与计算机辅助干预会议*。  Springer，2020年，第212–220页。'
- en: '[82] X. Xie, J. Niu, X. Liu, Q. Li, Y. Wang, J. Han, and S. Tang, “Dg-cnn:
    Introducing margin information into cnn for breast cancer diagnosis in ultrasound
    images,” *Journal of Computer Science and Engineering*, 2020.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] X. Xie, J. Niu, X. Liu, Q. Li, Y. Wang, J. Han, 和 S. Tang，“Dg-cnn：将边距信息引入卷积神经网络用于超声图像中的乳腺癌诊断，”
    *Journal of Computer Science and Engineering*，2020年。'
- en: '[83] B. Zhang, Z. Wang, J. Gao, C. Rutjes, K. Nufer, D. Tao, D. D. Feng, and
    S. W. Menzies, “Short-term lesion change detection for melanoma screening with
    novel siamese neural network.” *IEEE transactions on medical imaging*, 2020.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] B. Zhang, Z. Wang, J. Gao, C. Rutjes, K. Nufer, D. Tao, D. D. Feng, 和
    S. W. Menzies，“用于黑色素瘤筛查的短期病变变化检测与新型孪生神经网络。” *IEEE Transactions on Medical Imaging*，2020年。'
- en: '[84] M. Moradi, Y. Gur, H. Wang, P. Prasanna, and T. Syeda-Mahmood, “A hybrid
    learning approach for semantic labeling of cardiac ct slices and recognition of
    body position,” in *ISBI 2016*.   IEEE, 2016, pp. 1418–1421.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Moradi, Y. Gur, H. Wang, P. Prasanna, 和 T. Syeda-Mahmood，“一种混合学习方法用于心脏CT切片的语义标注和体位识别，”
    在 *ISBI 2016*。  IEEE，2016年，第1418–1421页。'
- en: '[85] T. Majtner, S. Yildirim-Yayilgan, and J. Y. Hardeberg, “Combining deep
    learning and hand-crafted features for skin lesion classification,” in *2016 Sixth
    International Conference on Image Processing Theory, Tools and Applications (IPTA)*.   IEEE,
    2016, pp. 1–6.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] T. Majtner, S. Yildirim-Yayilgan, 和 J. Y. Hardeberg, “将深度学习和手工特征结合用于皮肤病变分类，”发表于
    *2016年第六届国际图像处理理论、工具与应用大会（IPTA）*。IEEE，2016年，第1–6页。'
- en: '[86] Y. Xie, J. Zhang, Y. Xia, M. Fulham, and Y. Zhang, “Fusing texture, shape
    and deep model-learned information at decision level for automated classification
    of lung nodules on chest ct,” *Information Fusion*, vol. 42, pp. 102–110, 2018.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Xie, J. Zhang, Y. Xia, M. Fulham, 和 Y. Zhang, “在决策层融合纹理、形状和深度模型学习的信息，用于自动化分类胸部CT中的肺结节，”
    *信息融合*，第42卷，第102–110页，2018年。'
- en: '[87] N. Antropova, B. Q. Huynh, and M. L. Giger, “A deep feature fusion methodology
    for breast cancer diagnosis demonstrated on three imaging modality datasets,”
    *Medical physics*, vol. 44, no. 10, pp. 5162–5171, 2017.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] N. Antropova, B. Q. Huynh, 和 M. L. Giger, “一种用于乳腺癌诊断的深度特征融合方法，展示在三种成像模式数据集上，”
    *医学物理学*，第44卷，第10期，第5162–5171页，2017年。'
- en: '[88] X. Xia, J. Gong, W. Hao, T. Yang, Y. Lin, S. Wang, and W. Peng, “Comparison
    and fusion of deep learning and radiomics features of ground-glass nodules to
    predict the invasiveness risk of stage-i lung adenocarcinomas in ct scan,” *Frontiers
    in Oncology*, vol. 10, 2020.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] X. Xia, J. Gong, W. Hao, T. Yang, Y. Lin, S. Wang, 和 W. Peng, “比较和融合深度学习与放射组学特征，以预测CT扫描中I期肺腺癌的侵袭风险，”
    *肿瘤学前沿*，第10卷，2020年。'
- en: '[89] J. Hagerty, J. Stanley *et al.*, “Deep learning and handcrafted method
    fusion: Higher diagnostic accuracy for melanoma dermoscopy images,” *IEEE journal
    of biomedical and health informatics*, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Hagerty, J. Stanley *等*，“深度学习与手工方法融合：提高黑色素瘤皮肤镜图像的诊断准确性，” *IEEE生物医学与健康信息学期刊*，2019年。'
- en: '[90] Y. Chai, H. Liu, and J. Xu, “Glaucoma diagnosis based on both hidden features
    and domain knowledge through deep learning models,” *Knowledge-Based Systems*,
    vol. 161, pp. 147–156, 2018.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Chai, H. Liu, 和 J. Xu, “基于深度学习模型隐含特征和领域知识的青光眼诊断，” *基于知识的系统*，第161卷，第147–156页，2018年。'
- en: '[91] M. Buty, Z. Xu, M. Gao, U. Bagci, A. Wu, and D. J. Mollura, “Characterization
    of lung nodule malignancy using hybrid shape and appearance features,” in *MICCAI2016*,
    2016, pp. 662–670.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Buty, Z. Xu, M. Gao, U. Bagci, A. Wu, 和 D. J. Mollura, “使用混合形状和外观特征对肺结节恶性程度进行表征，”发表于
    *MICCAI2016*，2016年，第662–670页。'
- en: '[92] T. Saba, A. S. Mohamed, M. El-Affendi, J. Amin, and M. Sharif, “Brain
    tumor detection using fusion of hand crafted and deep learning features,” *Cognitive
    Systems Research*, vol. 59, pp. 221–230, 2020.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Saba, A. S. Mohamed, M. El-Affendi, J. Amin, 和 M. Sharif, “使用手工特征与深度学习特征融合的脑肿瘤检测，”
    *认知系统研究*，第59卷，第221–230页，2020年。'
- en: '[93] W. Yang, J. Zhao, Y. Qiang, X. Yang, Y. Dong, Q. Du, G. Shi, and M. B.
    Zia, “Dscgans: Integrate domain knowledge in training dual-path semi-supervised
    conditional generative adversarial networks and s3vm for ultrasonography thyroid
    nodules classification,” in *MICCAI2019*, 2019, pp. 558–566.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] W. Yang, J. Zhao, Y. Qiang, X. Yang, Y. Dong, Q. Du, G. Shi, 和 M. B. Zia,
    “Dscgans：在训练双路径半监督条件生成对抗网络和s3vm中融入领域知识，用于超声甲状腺结节分类，”发表于 *MICCAI2019*，2019年，第558–566页。'
- en: '[94] J. Tan, Y. Huo, Z. Liang, and L. Li, “Expert knowledge-infused deep learning
    for automatic lung nodule detection,” *Journal of X-ray science and technology*,
    no. Preprint, pp. 1–20, 2019.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Tan, Y. Huo, Z. Liang, 和 L. Li, “注入专家知识的深度学习用于自动肺结节检测，” *X射线科学与技术杂志*，预印本，第1–20页，2019年。'
- en: '[95] T. Liu, Q. Guo, C. Lian, X. Ren, S. Liang, J. Yu, L. Niu, W. Sun, and
    D. Shen, “Automated detection and classification of thyroid nodules in ultrasound
    images using clinical-knowledge-guided convolutional neural networks,” *Medical
    image analysis*, vol. 58, p. 101555, 2019.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Liu, Q. Guo, C. Lian, X. Ren, S. Liang, J. Yu, L. Niu, W. Sun, 和 D.
    Shen, “使用临床知识指导的卷积神经网络自动检测和分类甲状腺结节，” *医学影像分析*，第58卷，第101555页，2019年。'
- en: '[96] H. Feng, J. Cao, H. Wang, Y. Xie, D. Yang, J. Feng, and B. Chen, “A knowledge-driven
    feature learning and integration method for breast cancer diagnosis on multi-sequence
    mri,” *Magnetic Resonance Imaging*, 2020.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] H. Feng, J. Cao, H. Wang, Y. Xie, D. Yang, J. Feng, 和 B. Chen, “一种知识驱动的特征学习和集成方法，用于多序列MRI中的乳腺癌诊断，”
    *磁共振成像*，2020年。'
- en: '[97] V. Murthy, L. Hou, D. Samaras, T. M. Kurc, and J. H. Saltz, “Center-focusing
    multi-task cnn with injected features for classification of glioma nuclear images,”
    in *2017 IEEE Winter Conference on Applications of Computer Vision (WACV)*.   IEEE,
    2017, pp. 834–841.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] V. Murthy, L. Hou, D. Samaras, T. M. Kurc, 和 J. H. Saltz， “具有注入特征的中心聚焦多任务CNN用于分类神经胶质瘤核图像”，见于
    *2017 IEEE Winter Conference on Applications of Computer Vision (WACV)*。 IEEE,
    2017, 第834–841页。'
- en: '[98] X. Wang, Y. Peng, L. Lu, Z. Lu, and R. M. Summers, “Tienet: Text-image
    embedding network for common thorax disease classification and reporting in chest
    x-rays,” in *CVPR2018*, 2018, pp. 9049–9058.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Wang, Y. Peng, L. Lu, Z. Lu, 和 R. M. Summers， “Tienet：用于胸部X光片中常见胸部疾病分类和报告的文本-图像嵌入网络”，见于
    *CVPR2018*，2018，第9049–9058页。'
- en: '[99] Z. Zhang, P. Chen, M. Sapkota, and L. Yang, “Tandemnet: Distilling knowledge
    from medical images using diagnostic reports as optional semantic references,”
    in *MICCAI2017*.   Springer, 2017, pp. 320–328.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Z. Zhang, P. Chen, M. Sapkota, 和 L. Yang， “Tandemnet：使用诊断报告作为可选语义参考从医学图像中提取知识”，见于
    *MICCAI2017*。 Springer, 2017, 第320–328页。'
- en: '[100] N. Wu, J. Phang, J. Park, Y. Shen, Z. Huang, M. Zorin, S. Jastrzebski,
    T. Févry, J. Katsnelson, E. Kim *et al.*, “Deep neural networks improve radiologists
    performance in breast cancer screening,” *IEEE transactions on medical imaging*,
    vol. 39, no. 4, pp. 1184–1194, 2019.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] N. Wu, J. Phang, J. Park, Y. Shen, Z. Huang, M. Zorin, S. Jastrzebski,
    T. Févry, J. Katsnelson, E. Kim *等*， “深度神经网络提高了放射科医生在乳腺癌筛查中的表现”， *IEEE transactions
    on medical imaging*，第39卷，第4期， 第1184–1194页，2019。'
- en: '[101] S. Yu, H.-Y. Zhou, K. Ma, C. Bian, C. Chu, H. Liu, and Y. Zheng, “Difficulty-aware
    glaucoma classification with multi-rater consensus modeling,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 741–750.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Yu, H.-Y. Zhou, K. Ma, C. Bian, C. Chu, H. Liu, 和 Y. Zheng， “难度感知青光眼分类与多评估者共识建模”，见于
    *International Conference on Medical Image Computing and Computer-Assisted Intervention*。
    Springer, 2020, 第741–750页。'
- en: '[102] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *Advances in neural information processing
    systems*, 2014, pp. 3320–3328.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Yosinski, J. Clune, Y. Bengio, 和 H. Lipson， “深度神经网络中的特征可转移性如何？”见于
    *Advances in neural information processing systems*，2014，第3320–3328页。'
- en: '[103] X. Xiao, C. Ji, T. B. Mudiyanselage, and Y. Pan, “Pk-gcn: Prior knowledge
    assisted image classification using graph convolution networks,” *arXiv preprint
    arXiv:2009.11892*, 2020.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] X. Xiao, C. Ji, T. B. Mudiyanselage, 和 Y. Pan， “PK-GCN：使用图卷积网络的先验知识辅助图像分类”，
    *arXiv preprint arXiv:2009.11892*，2020。'
- en: '[104] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proceedings of the 26th annual international conference on machine learning*.   ACM,
    2009, pp. 41–48.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Bengio, J. Louradour, R. Collobert, 和 J. Weston， “课程学习”，见于 *Proceedings
    of the 26th annual international conference on machine learning*。 ACM, 2009, 第41–48页。'
- en: '[105] F. Nachbar, W. Stolz, T. Merkle, A. B. Cognetta, T. Vogt, M. Landthaler,
    P. Bilek, O. Braun-Falco, and G. Plewig, “The abcd rule of dermatoscopy: high
    prospective value in the diagnosis of doubtful melanocytic skin lesions,” *Journal
    of the American Academy of Dermatology*, vol. 30, no. 4, pp. 551–559, 1994.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] F. Nachbar, W. Stolz, T. Merkle, A. B. Cognetta, T. Vogt, M. Landthaler,
    P. Bilek, O. Braun-Falco, 和 G. Plewig， “皮肤镜检查的ABCD规则：在诊断可疑黑色素皮肤病变中的高前瞻性价值”， *Journal
    of the American Academy of Dermatology*，第30卷，第4期， 第551–559页，1994。'
- en: '[106] B. W. e. a. Mendelson EB, Bohm-V lez M, “Acr bi-rads ultrasound. in:
    Acr bi-rads atlas, breast imaging reporting and data system.” *Reston, VA, American
    College of Radiology*, 2013.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] B. W. 等， Mendelson EB, Bohm-V lez M， “ACR BI-RADS 超声。见于：ACR BI-RADS Atlas,
    Breast Imaging Reporting and Data System。” *Reston, VA, American College of Radiology*，2013。'
- en: '[107] L. Breiman, “Random forests,” *Machine learning*, vol. 45, no. 1, pp.
    5–32, 2001.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] L. Breiman， “随机森林”， *Machine learning*，第45卷，第1期， 第5–32页，2001。'
- en: '[108] C. Cortes and V. Vapnik, “Support-vector networks,” *Machine learning*,
    vol. 20, no. 3, pp. 273–297, 1995.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] C. Cortes 和 V. Vapnik， “支持向量网络”， *Machine learning*，第20卷，第3期， 第273–297页，1995。'
- en: '[109] M. Alilou, M. Orooji, and A. Madabhushi, “Intra-perinodular textural
    transition (ipris): A 3d descriptor for nodule diagnosis on lung ct,” in *MICCAI2017*.   Springer,
    2017, pp. 647–655.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Alilou, M. Orooji, 和 A. Madabhushi， “ intra-perinodular textural transition
    (ipris)：用于肺CT结节诊断的3D描述符”，见于 *MICCAI2017*。 Springer, 2017, 第647–655页。'
- en: '[110] A. A. A. Setio, F. Ciompi, G. Litjens, P. Gerke, C. Jacobs, S. J. Van Riel,
    M. M. W. Wille, M. Naqibullah, C. I. Sánchez, and B. van Ginneken, “Pulmonary
    nodule detection in ct images: false positive reduction using multi-view convolutional
    networks,” *IEEE TMI*, vol. 35, no. 5, pp. 1160–1169, 2016.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. A. A. Setio, F. Ciompi, G. Litjens, P. Gerke, C. Jacobs, S. J. Van
    Riel, M. M. W. Wille, M. Naqibullah, C. I. Sánchez, 和 B. van Ginneken， “CT图像中的肺结节检测：使用多视角卷积网络减少假阳性”，*IEEE
    TMI*，第35卷，第5期，页码1160–1169，2016年。'
- en: '[111] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros *et al.*, “Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs,” *JAMA*, vol. 316, no. 22, pp. 2402–2410, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros *等*， “开发和验证用于检测视网膜眼底照片中糖尿病视网膜病变的深度学习算法”，*JAMA*，第316卷，第22期，页码2402–2410，2016年。'
- en: '[112] J. Liu, D. Wang, L. Lu, Z. Wei, L. Kim, E. B. Turkbey, B. Sahiner, N. A.
    Petrick, and R. M. Summers, “Detection and diagnosis of colitis on computed tomography
    using deep convolutional neural networks,” *Medical physics*, vol. 44, no. 9,
    pp. 4630–4642, 2017.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Liu, D. Wang, L. Lu, Z. Wei, L. Kim, E. B. Turkbey, B. Sahiner, N.
    A. Petrick, 和 R. M. Summers， “使用深度卷积神经网络在计算机断层扫描中检测和诊断结肠炎”，*医学物理*，第44卷，第9期，页码4630–4642，2017年。'
- en: '[113] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time
    object detection with region proposal networks,” vol. 2015, pp. 91–99, 2015.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] S. Ren, K. He, R. Girshick, 和 J. Sun， “Faster R-CNN：朝着实时对象检测与区域提议网络的目标前进”，第2015卷，页码91–99，2015年。'
- en: '[114] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, “Mask r-cnn,” *international
    conference on computer vision*, pp. 2980–2988, 2017.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. He, G. Gkioxari, P. Dollar, 和 R. B. Girshick， “Mask R-CNN”，*国际计算机视觉会议*，页码2980–2988，2017年。'
- en: '[115] R. Sa, W. Owens, R. Wiegand, M. Studin, D. Capoferri, K. Barooha, A. Greaux,
    R. Rattray, A. Hutton, J. Cintineo *et al.*, “Intervertebral disc detection in
    x-ray images using faster r-cnn,” in *2017 39th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE, 2017,
    pp. 564–567.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] R. Sa, W. Owens, R. Wiegand, M. Studin, D. Capoferri, K. Barooha, A.
    Greaux, R. Rattray, A. Hutton, J. Cintineo *等*， “使用Faster R-CNN在X射线图像中检测椎间盘”，收录于*2017年第39届IEEE医学与生物学工程年会（EMBC）*。   IEEE，2017年，页码564–567。'
- en: '[116] R. Ben-Ari, A. Akselrod-Ballin, L. Karlinsky, and S. Hashoul, “Domain
    specific convolutional neural nets for detection of architectural distortion in
    mammograms,” in *ISBI 2017*.   IEEE, 2017, pp. 552–556.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] R. Ben-Ari, A. Akselrod-Ballin, L. Karlinsky, 和 S. Hashoul， “用于检测乳腺X线照片中结构扭曲的领域特定卷积神经网络”，收录于*ISBI
    2017*。   IEEE，2017年，页码552–556。'
- en: '[117] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *CVPR2016*, 2016, pp. 779–788.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Redmon, S. Divvala, R. Girshick, 和 A. Farhadi， “你只看一次：统一的实时目标检测”，收录于*CVPR2016*，2016年，页码779–788。'
- en: '[118] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, 和 A. C.
    Berg， “SSD：单次多框检测器”，收录于*欧洲计算机视觉会议*。   Springer，2016年，页码21–37。'
- en: '[119] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T.-Y. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár， “密集目标检测的焦点损失”，收录于*IEEE国际计算机视觉会议论文集*，2017年，页码2980–2988。'
- en: '[120] R. Platania, S. Shams, S. Yang, J. Zhang, K. Lee, and S.-J. Park, “Automated
    breast cancer diagnosis using deep learning and region of interest detection (bc-droid),”
    in *Proceedings of the 8th ACM International Conference on Bioinformatics, Computational
    Biology, and Health Informatics*.   ACM, 2017, pp. 536–543.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] R. Platania, S. Shams, S. Yang, J. Zhang, K. Lee, 和 S.-J. Park， “使用深度学习和感兴趣区域检测的自动化乳腺癌诊断（BC-Droid）”，收录于*第八届ACM国际生物信息学、计算生物学与健康信息学会议论文集*。   ACM，2017年，页码536–543。'
- en: '[121] N. Li, H. Liu, B. Qiu, W. Guo, S. Zhao, K. Li, and J. He, “Detection
    and attention: Diagnosing pulmonary lung cancer from ct by imitating physicians,”
    *arXiv preprint arXiv:1712.05114*, 2017.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] N. Li, H. Liu, B. Qiu, W. Guo, S. Zhao, K. Li, 和 J. He， “检测与关注：通过模仿医生从CT中诊断肺癌”，*arXiv预印本arXiv:1712.05114*，2017年。'
- en: '[122] G. Cai, J. Chen, Z. Wu, H. Tang, Y. Liu, S. Wang, and S. Su, “One stage
    lesion detection based on 3d context convolutional neural networks,” *Computers
    and Electrical Engineering*, vol. 79, p. 106449, 2019.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] G. Cai, J. Chen, Z. Wu, H. Tang, Y. Liu, S. Wang, 和 S. Su， “基于3D上下文卷积神经网络的一阶段病变检测”，*计算机与电气工程*，第79卷，页码106449，2019年。'
- en: '[123] M. H. Yap, G. Pons, J. Marti, S. Ganau, M. Sentis, R. Zwiggelaar, A. K.
    Davison, and R. Marti, “Automated breast ultrasound lesions detection using convolutional
    neural networks,” *IEEE Journal of Biomedical and Health Informatics*, vol. 22,
    no. 4, pp. 1218–1226, 2018.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. H. Yap, G. Pons, J. Marti, S. Ganau, M. Sentis, R. Zwiggelaar, A.
    K. Davison 和 R. Marti，“使用卷积神经网络自动检测乳腺超声病变”，*IEEE生物医学与健康信息学杂志*，第22卷，第4期，第1218–1226页，2018年。'
- en: '[124] J. J. Näppi, T. Hironaka, D. Regge, and H. Yoshida, “Deep transfer learning
    of virtual endoluminal views for the detection of polyps in ct colonography,”
    in *Medical Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International
    Society for Optics and Photonics, 2016, p. 97852B.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. J. Näppi, T. Hironaka, D. Regge 和 H. Yoshida，“虚拟内腔视图的深度迁移学习用于CT结肠造影中息肉的检测”，在*医学影像2016:
    计算机辅助诊断*中，第9785卷。 国际光学和光子学学会，2016年，第97852B页。'
- en: '[125] R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau, and C. C.
    Poon, “Automatic detection and classification of colorectal polyps by transferring
    low-level cnn features from nonmedical domain,” *IEEE journal of biomedical and
    health informatics*, vol. 21, no. 1, pp. 41–47, 2016.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau 和 C. C.
    Poon，“通过从非医学领域迁移低级CNN特征来自动检测和分类结直肠息肉”，*IEEE生物医学与健康信息学杂志*，第21卷，第1期，第41–47页，2016年。'
- en: '[126] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
    Gotway, and J. Liang, “Convolutional neural networks for medical image analysis:
    Full training or fine tuning?” *IEEE TMI*, vol. 35, no. 5, pp. 1299–1312, 2016.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M.
    B. Gotway 和 J. Liang，“用于医学图像分析的卷积神经网络：全量训练还是微调？”，*IEEE TMI*，第35卷，第5期，第1299–1312页，2016年。'
- en: '[127] J. Zhang, E. H. Cain, A. Saha, Z. Zhu, and M. A. Mazurowski, “Breast
    mass detection in mammography and tomosynthesis via fully convolutional network-based
    heatmap regression,” in *Medical Imaging 2018: Computer-Aided Diagnosis*, vol.
    10575.   International Society for Optics and Photonics, 2018, p. 1057525.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Zhang, E. H. Cain, A. Saha, Z. Zhu 和 M. A. Mazurowski，“通过基于全卷积网络的热图回归进行乳腺肿块检测”，在*医学影像2018:
    计算机辅助诊断*中，第10575卷。 国际光学和光子学学会，2018年，第1057525页。'
- en: '[128] A. Ben-Cohen, E. Klang, S. P. Raskin, S. Soffer, S. Ben-Haim, E. Konen,
    M. M. Amitai, and H. Greenspan, “Cross-modality synthesis from ct to pet using
    fcn and gan networks for improved automated lesion detection,” *Engineering Applications
    of Artificial Intelligence*, vol. 78, pp. 186–194, 2019.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Ben-Cohen, E. Klang, S. P. Raskin, S. Soffer, S. Ben-Haim, E. Konen,
    M. M. Amitai 和 H. Greenspan，“使用FCN和GAN网络从CT到PET的跨模态合成，以改进自动病变检测”，*人工智能工程应用*，第78卷，第186–194页，2019年。'
- en: '[129] J. Zhao, D. Li, Z. Kassam, J. Howey, J. Chong, B. Chen, and S. Li, “Tripartite-gan:
    Synthesizing liver contrast-enhanced mri to improve tumor detection,” *Medical
    Image Analysis*, p. 101667, 2020.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Zhao, D. Li, Z. Kassam, J. Howey, J. Chong, B. Chen 和 S. Li，“Tripartite-gan:
    合成肝脏对比增强MRI以改善肿瘤检测”，*医学影像分析*，第101667页，2020年。'
- en: '[130] A. Jesson, N. Guizard, S. H. Ghalehjegh, D. Goblot, F. Soudan, and N. Chapados,
    “Cased: curriculum adaptive sampling for extreme data imbalance,” in *MICCAI2017*,
    2017, pp. 639–646.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Jesson, N. Guizard, S. H. Ghalehjegh, D. Goblot, F. Soudan 和 N. Chapados，“Cased:
    适应极端数据不平衡的课程采样”，在*MICCAI2017*中，2017年，第639–646页。'
- en: '[131] P. Astudillo, P. Mortier, M. De Beule *et al.*, “Curriculum deep reinforcement
    learning with different exploration strategies: A feasibility study on cardiac
    landmark detection,” in *BIOIMAGING 2020: 7th International Conference on Bioimaging*,
    2020.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] P. Astudillo, P. Mortier, M. De Beule *等*，“具有不同探索策略的课程深度强化学习：心脏标志检测的可行性研究”，在*BIOIMAGING
    2020: 第七届国际生物影像会议*中，2020年。'
- en: '[132] Z. Li, S. Zhang, J. Zhang, K. Huang, Y. Wang, and Y. Yu, “Mvp-net: Multi-view
    fpn with position-aware attention for deep universal lesion detection,” in *Medical
    Image Computing and Computer-Assisted Intervention*, 2019, pp. 13–21.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Z. Li, S. Zhang, J. Zhang, K. Huang, Y. Wang 和 Y. Yu，“Mvp-net: 带有位置感知注意力的多视角FPN用于深度通用病变检测”，在*医学影像计算与计算机辅助干预*中，2019年，第13–21页。'
- en: '[133] Q. Ni, Z. Y. Sun, L. Qi, W. Chen, Y. Yang, L. Wang, X. Zhang, L. Yang,
    Y. Fang, Z. Xing *et al.*, “A deep learning approach to characterize 2019 coronavirus
    disease (covid-19) pneumonia in chest ct images,” *European radiology*, vol. 30,
    no. 12, pp. 6517–6527, 2020.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Q. Ni, Z. Y. Sun, L. Qi, W. Chen, Y. Yang, L. Wang, X. Zhang, L. Yang,
    Y. Fang, Z. Xing *等*，“一种深度学习方法用于表征2019冠状病毒病（COVID-19）肺炎在胸部CT图像中的特征”，*欧洲放射学*，第30卷，第12期，第6517–6527页，2020年。'
- en: '[134] Y. Liu, Z. Zhou, S. Zhang, L. Luo, Q. Zhang, F. Zhang, X. Li, Y. Wang,
    and Y. Yu, “From unilateral to bilateral learning: Detecting mammogram masses
    with contrasted bilateral network,” in *Medical Image Computing and Computer-Assisted
    Intervention*, 2019, pp. 477–485.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y. Liu, Z. Zhou, S. Zhang, L. Luo, Q. Zhang, F. Zhang, X. Li, Y. Wang,
    和 Y. Yu, “从单侧到双侧学习：使用对比双侧网络检测乳腺X光检查中的肿块，” 见于 *Medical Image Computing and Computer-Assisted
    Intervention*，2019年，第477–485页。'
- en: '[135] Y. Liu, F. Zhang, Q. Zhang, S. Wang, and Y. Yu, “Cross-view correspondence
    reasoning based on bipartite graph convolutional network for mammogram mass detection,”
    in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Liu, F. Zhang, Q. Zhang, S. Wang, 和 Y. Yu, “基于二分图卷积网络的交叉视图对应推理用于乳腺X光检查肿块检测，”
    见于 *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*，2020年。'
- en: '[136] A. Lisowska, E. Beveridge, K. Muir, and I. Poole, “Thrombus detection
    in ct brain scans using a convolutional neural network.” in *BIOIMAGING*, 2017,
    pp. 24–33.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Lisowska, E. Beveridge, K. Muir, 和 I. Poole, “使用卷积神经网络在CT脑扫描中检测血栓。”
    见于 *BIOIMAGING*，2017年，第24–33页。'
- en: '[137] A. Lisowska, A. O Neil, V. Dilys, M. Daykin, E. Beveridge, K. Muir, S. Mclaughlin,
    and I. Poole, “Context-aware convolutional neural networks for stroke sign detection
    in non-contrast ct scans,” in *Annual Conference on Medical Image Understanding
    and Analysis*.   Springer, 2017, pp. 494–505.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Lisowska, A. O Neil, V. Dilys, M. Daykin, E. Beveridge, K. Muir, S.
    Mclaughlin, 和 I. Poole, “基于上下文感知的卷积神经网络用于中风症状检测（非对比CT扫描），” 见于 *Annual Conference
    on Medical Image Understanding and Analysis*。 Springer，2017年，第494–505页。'
- en: '[138] L. Li, M. Wei, B. Liu, K. Atchaneeyasakul, F. Zhou, Z. Pan, S. Kumar,
    J. Zhang, Y. Pu, D. S. Liebeskind *et al.*, “Deep learning for hemorrhagic lesion
    detection and segmentation on brain ct images,” *IEEE Journal of Biomedical and
    Health Informatics*, 2020.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] L. Li, M. Wei, B. Liu, K. Atchaneeyasakul, F. Zhou, Z. Pan, S. Kumar,
    J. Zhang, Y. Pu, D. S. Liebeskind *等*，“深度学习用于脑CT图像中的出血性病变检测和分割，” *IEEE Journal
    of Biomedical and Health Informatics*，2020年。'
- en: '[139] L. Fu, J. Ma, Y. Ren, Y. S. Han, and J. Zhao, “Automatic detection of
    lung nodules: false positive reduction using convolution neural networks and handcrafted
    features,” in *Medical Imaging 2017: Computer-Aided Diagnosis*, vol. 10134.   International
    Society for Optics and Photonics, 2017, p. 101340A.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] L. Fu, J. Ma, Y. Ren, Y. S. Han, 和 J. Zhao, “肺结节的自动检测：使用卷积神经网络和手工特征减少假阳性，”
    见于 *Medical Imaging 2017: Computer-Aided Diagnosis*，第10134卷。 国际光学与光子学学会，2017年，第101340A页。'
- en: '[140] T. Kooi, G. Litjens, B. Van Ginneken, A. Gubern-Mérida, C. I. Sánchez,
    R. Mann, A. den Heeten, and N. Karssemeijer, “Large scale deep learning for computer
    aided detection of mammographic lesions,” *Medical Image Analysis*, vol. 35, pp.
    303–312, 2017.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] T. Kooi, G. Litjens, B. Van Ginneken, A. Gubern-Mérida, C. I. Sánchez,
    R. Mann, A. den Heeten, 和 N. Karssemeijer, “大规模深度学习用于计算机辅助检测乳腺X光病变，” *Medical
    Image Analysis*，第35卷，第303–312页，2017年。'
- en: '[141] N. Ghatwary, X. Ye, and M. Zolgharni, “Esophageal abnormality detection
    using densenet based faster r-cnn with gabor features,” *IEEE Access*, vol. 7,
    pp. 84 374–84 385, 2019.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] N. Ghatwary, X. Ye, 和 M. Zolgharni, “利用基于densenet的faster r-cnn和gabor特征检测食道异常，”
    *IEEE Access*，第7卷，第84,374–84,385页，2019年。'
- en: '[142] C.-H. Chao, Z. Zhu, D. Guo, K. Yan, T.-Y. Ho, J. Cai, A. P. Harrison,
    X. Ye, J. Xiao, A. Yuille *et al.*, “Lymph node gross tumor volume detection in
    oncology imaging via relationship learning using graph neural network,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 772–782.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C.-H. Chao, Z. Zhu, D. Guo, K. Yan, T.-Y. Ho, J. Cai, A. P. Harrison,
    X. Ye, J. Xiao, A. Yuille *等*，“通过图神经网络的关系学习在肿瘤学影像中检测淋巴结肿瘤体积，” 见于 *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*。 Springer，2020年，第772–782页。'
- en: '[143] A. Sóñora-Mengan Sr, P. Gonidakis, B. Jansen, J. Garcı?a-Naranjo, and
    J. Vandemeulebroucke, “Evaluating several ways to combine handcrafted features-based
    system with a deep learning system using the luna16 challenge framework,” in *Medical
    Imaging 2020: Computer-Aided Diagnosis*, vol. 11314.   International Society for
    Optics and Photonics, 2020, p. 113143T.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Sóñora-Mengan Sr, P. Gonidakis, B. Jansen, J. Garcı?a-Naranjo, 和 J.
    Vandemeulebroucke, “评估将基于手工特征的系统与深度学习系统结合的几种方式，使用luna16挑战框架，” 见于 *Medical Imaging
    2020: Computer-Aided Diagnosis*，第11314卷。 国际光学与光子学学会，2020年，第113143T页。'
- en: '[144] S. Hwang and H.-E. Kim, “Self-transfer learning for weakly supervised
    lesion localization,” in *MICCAI2016*.   Springer, 2016, pp. 239–246.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] S. Hwang 和 H.-E. Kim, “弱监督病变定位的自我迁移学习，” 见于 *MICCAI2016*。 Springer，2016年，第239–246页。'
- en: '[145] R. Bakalo, R. Ben-Ari, and J. Goldberger, “Classification and detection
    in mammograms with weak supervision via dual branch deep neural net,” in *2019
    IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)*.   IEEE,
    2019, pp. 1905–1909.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] R. Bakalo, R. Ben-Ari 和 J. Goldberger，“通过双分支深度神经网络在乳腺 X 光图像中进行分类和检测的弱监督，”
    见 *2019 IEEE 第 16 届生物医学成像国际研讨会（ISBI 2019）*。   IEEE，2019 年，第 1905–1909 页。'
- en: '[146] G. Liang, X. Wang, Y. Zhang, and N. Jacobs, “Weakly-supervised self-training
    for breast cancer localization,” in *2020 42nd Annual International Conference
    of the IEEE Engineering in Medicine & Biology Society (EMBC)*.   IEEE, 2020, pp.
    1124–1127.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] G. Liang, X. Wang, Y. Zhang 和 N. Jacobs，“用于乳腺癌定位的弱监督自我训练，” 见 *2020 年第
    42 届 IEEE 医学与生物学学会国际年会（EMBC）*。   IEEE，2020 年，第 1124–1127 页。'
- en: '[147] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, and H. Larochelle, “Brain tumor segmentation with deep neural
    networks,” *Medical Image Analysis*, vol. 35, pp. 18–31, 2017.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin 和 H. Larochelle，“使用深度神经网络进行脑肿瘤分割，” *医学图像分析*，第 35 卷，第 18–31
    页，2017 年。'
- en: '[148] J. Zhang, A. Saha, Z. Zhu, and M. A. Mazurowski, “Hierarchical convolutional
    neural networks for segmentation of breast tumors in mri with application to radiogenomics,”
    *IEEE TMI*, vol. 38, no. 2, pp. 435–447, 2018.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Zhang, A. Saha, Z. Zhu 和 M. A. Mazurowski，“用于 MRI 中乳腺肿瘤分割的层次卷积神经网络及其在放射基因组学中的应用，”
    *IEEE TMI*，第 38 卷，第 2 期，第 435–447 页，2018 年。'
- en: '[149] P. F. Christ, F. Ettlinger, F. Grün *et al.*, “Automatic liver and tumor
    segmentation of ct and mri volumes using cascaded fully convolutional neural networks,”
    *arXiv preprint arXiv:1702.05970*, 2017.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] P. F. Christ, F. Ettlinger, F. Grün *等*，“使用级联全卷积神经网络对 CT 和 MRI 体积进行自动肝脏和肿瘤分割，”
    *arXiv 预印本 arXiv:1702.05970*，2017 年。'
- en: '[150] H. R. Roth, A. Farag, L. Lu, E. B. Turkbey, and R. M. Summers, “Deep
    convolutional networks for pancreas segmentation in ct imaging,” in *Medical Imaging
    2015: Image Processing*, vol. 9413.   International Society for Optics and Photonics,
    2015, p. 94131G.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] H. R. Roth, A. Farag, L. Lu, E. B. Turkbey 和 R. M. Summers，“用于 CT 成像中的胰腺分割的深度卷积网络，”
    见 *医学成像 2015：图像处理*，第 9413 卷。   国际光学和光子学学会，2015 年，第 94131G 页。'
- en: '[151] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” *CVPR2015*, pp. 3431–3440, 2015.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Long, E. Shelhamer 和 T. Darrell，“用于语义分割的全卷积网络，” *CVPR2015*，第 3431–3440
    页，2015 年。'
- en: '[152] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI2015*.   Springer, 2015, pp. 234–241.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] O. Ronneberger, P. Fischer 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络，” 见 *MICCAI2015*。   Springer，2015
    年，第 234–241 页。'
- en: '[153] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio，“生成对抗网络，” 见 *神经信息处理系统进展*，2014 年，第 2672–2680 页。'
- en: '[154] P. V. Tran, “A fully convolutional neural network for cardiac segmentation
    in short-axis mri,” *arXiv preprint arXiv:1604.00494*, 2016.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. V. Tran, “一种用于短轴 MRI 心脏分割的全卷积神经网络，” *arXiv 预印本 arXiv:1604.00494*，2016
    年。'
- en: '[155] H. Chen, Y. Zheng, J.-H. Park, P.-A. Heng, and S. K. Zhou, “Iterative
    multi-domain regularized deep learning for anatomical structure detection and
    segmentation from ultrasound images,” in *MICCAI2016*.   Springer, 2016, pp. 487–495.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. Chen, Y. Zheng, J.-H. Park, P.-A. Heng 和 S. K. Zhou，“用于超声图像的解剖结构检测和分割的迭代多领域正则化深度学习，”
    见 *MICCAI2016*。   Springer，2016 年，第 487–495 页。'
- en: '[156] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R.
    Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Towards image-guided
    pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal
    ct with dense dilated networks,” in *MICCAI2017*, 2017, pp. 728–736.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B.
    R. Davidson, S. P. Pereira, M. J. Clarkson 和 D. C. Barratt，“朝着图像引导的胰腺和胆道内窥镜检查：在腹部
    CT 上使用密集扩张网络进行自动多器官分割，” 见 *MICCAI2017*，2017 年，第 728–736 页。'
- en: '[157] P. F. Christ, M. E. A. Elshaer, *et al.*, “Automatic liver and lesion
    segmentation in ct using cascaded fully convolutional neural networks and 3d conditional
    random fields,” in *MICCAI2016*.   Springer, 2016, pp. 415–423.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] P. F. Christ, M. E. A. Elshaer, *等*，“使用级联全卷积神经网络和 3D 条件随机场进行 CT 中的自动肝脏和病变分割，”
    见 *MICCAI2016*。   Springer，2016 年，第 415–423 页。'
- en: '[158] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane, D. K.
    Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3d cnn with fully connected
    crf for accurate brain lesion segmentation,” *Medical Image Analysis*, vol. 36,
    pp. 61–78, 2017.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane, D.
    K. Menon, D. Rueckert, 和 B. Glocker, “用于准确脑损伤分割的高效多尺度 3D CNN 和全连接 CRF,” *医学图像分析*,
    vol. 36, 页码 61–78, 2017年。'
- en: '[159] X. Yang, L. Yu, L. Wu, Y. Wang, D. Ni, J. Qin, and P.-A. Heng, “Fine-grained
    recurrent neural networks for automatic prostate segmentation in ultrasound images,”
    in *Thirty-First AAAI Conference on Artificial Intelligence*, 2017.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] X. Yang, L. Yu, L. Wu, Y. Wang, D. Ni, J. Qin, 和 P.-A. Heng, “细粒度递归神经网络用于自动前列腺分割在超声图像中,”
    在 *第三十一届 AAAI 人工智能会议* 中, 2017年。'
- en: '[160] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested
    u-net architecture for medical image segmentation,” in *Deep Learning in Medical
    Image Analysis and Multimodal Learning for Clinical Decision Support*.   Springer,
    2018, pp. 3–11.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, 和 J. Liang, “Unet++: 一种用于医学图像分割的嵌套
    U-Net 架构,” 在 *医学图像分析中的深度学习及多模态学习用于临床决策支持* 中。Springer, 2018年, 页码 3–11。'
- en: '[161] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, “Recurrent
    residual convolutional neural network based on u-net (r2u-net) for medical image
    segmentation,” *arXiv preprint arXiv:1802.06955*, 2018.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, 和 V. K. Asari, “基于 U-Net
    的递归残差卷积神经网络（R2U-Net）用于医学图像分割,” *arXiv 预印本 arXiv:1802.06955*, 2018年。'
- en: '[162] Y. Gordienko, P. Gang, J. Hui, W. Zeng, Y. Kochura, O. Alienin, O. Rokovyi,
    and S. Stirenko, “Deep learning with lung segmentation and bone shadow exclusion
    techniques for chest x-ray analysis of lung cancer,” in *International Conference
    on Computer Science, Engineering and Education Applications*.   Springer, 2018,
    pp. 638–647.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Gordienko, P. Gang, J. Hui, W. Zeng, Y. Kochura, O. Alienin, O. Rokovyi,
    和 S. Stirenko, “结合肺部分割和骨影排除技术的深度学习用于胸部 X 光分析肺癌,” 在 *计算机科学、工程与教育应用国际会议* 中。Springer,
    2018年, 页码 638–647。'
- en: '[163] D. Yang, D. Xu, S. K. Zhou, B. Georgescu, M. Chen, S. Grbic, D. Metaxas,
    and D. Comaniciu, “Automatic liver segmentation using an adversarial image-to-image
    network,” in *MICCAI2017*.   Springer, 2017, pp. 507–515.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] D. Yang, D. Xu, S. K. Zhou, B. Georgescu, M. Chen, S. Grbic, D. Metaxas,
    和 D. Comaniciu, “利用对抗图像到图像网络进行自动肝脏分割,” 在 *MICCAI 2017* 中。Springer, 2017年, 页码 507–515。'
- en: '[164] M. Zhao, L. Wang, J. Chen, D. Nie, Y. Cong, S. Ahmad, A. Ho, P. Yuan,
    S. H. Fung, H. H. Deng *et al.*, “Craniomaxillofacial bony structures segmentation
    from mri with deep-supervision adversarial learning,” in *MICCAI2018*, 2018, pp.
    720–727.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. Zhao, L. Wang, J. Chen, D. Nie, Y. Cong, S. Ahmad, A. Ho, P. Yuan,
    S. H. Fung, H. H. Deng *等*, “利用深度监督对抗学习从 MRI 中分割颅面骨结构,” 在 *MICCAI 2018* 中, 2018年,
    页码 720–727。'
- en: '[165] K. Kamnitsas, C. Baumgartner *et al.*, “Unsupervised domain adaptation
    in brain lesion segmentation with adversarial networks,” in *International conference
    on information processing in medical imaging*.   Springer, 2017, pp. 597–609.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] K. Kamnitsas, C. Baumgartner *等*, “在脑损伤分割中利用对抗网络进行无监督领域适配,” 在 *医学图像信息处理国际会议*
    中。Springer, 2017年, 页码 597–609。'
- en: '[166] S. Izadi, Z. Mirikharaji, J. Kawahara, and G. Hamarneh, “Generative adversarial
    networks to segment skin lesions,” in *ISBI 2018*.   IEEE, 2018, pp. 881–884.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] S. Izadi, Z. Mirikharaji, J. Kawahara, 和 G. Hamarneh, “利用生成对抗网络分割皮肤病变,”
    在 *ISBI 2018* 中。IEEE, 2018年, 页码 881–884。'
- en: '[167] A. Lahiri, K. Ayush, P. Kumar Biswas, and P. Mitra, “Generative adversarial
    learning for reducing manual annotation in semantic segmentation on large scale
    miscroscopy images: Automated vessel segmentation in retinal fundus image as test
    case,” in *CVPR Workshops*, 2017, pp. 42–48.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Lahiri, K. Ayush, P. Kumar Biswas, 和 P. Mitra, “生成对抗学习用于减少大规模显微图像语义分割中的手动注释：以视网膜眼底图像中的自动血管分割为测试案例,”
    在 *CVPR 研讨会* 中, 2017年, 页码 42–48。'
- en: '[168] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs,
    “Unsupervised anomaly detection with generative adversarial networks to guide
    marker discovery,” in *International Conference on Information Processing in Medical
    Imaging*.   Springer, 2017, pp. 146–157.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, 和 G. Langs,
    “利用生成对抗网络进行无监督异常检测以指导标记发现,” 在 *医学图像信息处理国际会议* 中。Springer, 2017年, 页码 146–157。'
- en: '[169] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, “Dcan: Deep contour-aware
    networks for object instance segmentation from histology images,” *Medical Image
    Analysis*, vol. 36, pp. 135–146, 2017.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, 和 P.-A. Heng, “Dcan: 基于深度轮廓感知网络的组织图像目标实例分割,”
    *医学图像分析*, vol. 36, 页码 135–146, 2017年。'
- en: '[170] L. Wu, Y. Xin, S. Li, T. Wang, P.-A. Heng, and D. Ni, “Cascaded fully
    convolutional networks for automatic prenatal ultrasound image segmentation,”
    in *ISBI 2017*.   IEEE, 2017, pp. 663–666.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] L. Wu, Y. Xin, S. Li, T. Wang, P.-A. Heng, 和 D. Ni, “级联全卷积网络用于自动产前超声图像分割，”在
    *ISBI 2017* 中。 IEEE, 2017, 页码 663–666。'
- en: '[171] G. Zeng, X. Yang, J. Li, L. Yu, P.-A. Heng, and G. Zheng, “3d u-net with
    multi-level deep supervision: fully automatic segmentation of proximal femur in
    3d mr images,” in *International workshop on machine learning in medical imaging*.   Springer,
    2017, pp. 274–282.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] G. Zeng, X. Yang, J. Li, L. Yu, P.-A. Heng, 和 G. Zheng, “带有多级深度监督的 3D
    U-Net：完全自动分割 3D MR 图像中的近端股骨，”在 *国际医学影像机器学习研讨会* 中。 Springer, 2017, 页码 274–282。'
- en: '[172] M. Ghafoorian, A. Mehrtash, T. Kapur, N. Karssemeijer, E. Marchiori,
    M. Pesteie, C. R. Guttmann, F.-E. de Leeuw, C. M. Tempany, B. van Ginneken *et al.*,
    “Transfer learning for domain adaptation in mri: Application in brain lesion segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 516–524.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Ghafoorian, A. Mehrtash, T. Kapur, N. Karssemeijer, E. Marchiori,
    M. Pesteie, C. R. Guttmann, F.-E. de Leeuw, C. M. Tempany, B. van Ginneken *等*，“迁移学习在
    MRI 中的领域适应：用于脑病灶分割的应用，”在 *MICCAI2017* 中。 Springer, 2017, 页码 516–524。'
- en: '[173] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” in *MICCAI2016*.   Springer, 2016, pp. 478–486.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T.
    Leiner, M. A. Viergever, 和 I. Išgum, “深度学习用于多任务医学图像分割的多模态应用，”在 *MICCAI2016* 中。
    Springer, 2016, 页码 478–486。'
- en: '[174] V. V. Valindria, N. Pawlowski *et al.*, “Multi-modal learning from unpaired
    images: Application to multi-organ segmentation in ct and mri,” in *2018 IEEE
    Winter Conference on Applications of Computer Vision (WACV)*.   IEEE, 2018, pp.
    547–556.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] V. V. Valindria, N. Pawlowski *等*，“从未配对图像中进行多模态学习：在 CT 和 MRI 中的多脏器分割应用，”在
    *2018 年 IEEE 冬季计算机视觉应用会议 (WACV)* 中。 IEEE, 2018, 页码 547–556。'
- en: '[175] C. Chen, Q. Dou, H. Chen, and P.-A. Heng, “Semantic-aware generative
    adversarial nets for unsupervised domain adaptation in chest x-ray segmentation,”
    in *International Workshop on Machine Learning in Medical Imaging*.   Springer,
    2018, pp. 143–151.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] C. Chen, Q. Dou, H. Chen, 和 P.-A. Heng, “语义感知生成对抗网络用于胸部 X 光图像分割中的无监督领域适应，”在
    *国际医学影像机器学习研讨会* 中。 Springer, 2018, 页码 143–151。'
- en: '[176] C. Chen, Q. Dou, H. Chen, J. Qin, and P. A. Heng, “Unsupervised bidirectional
    cross-modality adaptation via deeply synergistic image and feature alignment for
    medical image segmentation,” *IEEE TMI*, 2020.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] C. Chen, Q. Dou, H. Chen, J. Qin, 和 P. A. Heng, “通过深度协同图像和特征对齐的无监督双向跨模态适应用于医学图像分割，”
    *IEEE TMI*，2020。'
- en: '[177] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J. O.
    Deasy, and H. Veeraraghavan, “Tumor-aware, adversarial domain adaptation from
    ct to mri for lung cancer segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    777–785.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J.
    O. Deasy, 和 H. Veeraraghavan, “肿瘤感知的对抗领域适应从 CT 到 MRI 进行肺癌分割，”在 *MICCAI2018* 中。
    Springer, 2018, 页码 777–785。'
- en: '[178] W. Yan, Y. Wang, S. Gu, L. Huang, F. Yan, L. Xia, and Q. Tao, “The domain
    shift problem of medical image segmentation and vendor-adaptation by unet-gan,”
    in *MICCAI2019*.   Springer, 2019, pp. 623–631.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] W. Yan, Y. Wang, S. Gu, L. Huang, F. Yan, L. Xia, 和 Q. Tao, “医学图像分割中的领域转移问题和通过
    U-Net-GAN 的厂商适应，”在 *MICCAI2019* 中。 Springer, 2019, 页码 623–631。'
- en: '[179] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, and J. S. Duncan,
    “Unsupervised domain adaptation via disentangled representations: Application
    to cross-modality liver segmentation,” in *MICCAI2019*.   Springer, 2019, pp.
    255–263.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, 和 J. S. Duncan,
    “通过解缠表示的无监督领域适应：应用于跨模态肝脏分割，”在 *MICCAI2019* 中。 Springer, 2019, 页码 255–263。'
- en: '[180] K. Li, L. Yu, S. Wang, and P.-A. Heng, “Towards cross-modality medical
    image segmentation with online mutual knowledge distillation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 01, 2020, pp.
    775–783.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] K. Li, L. Yu, S. Wang, 和 P.-A. Heng, “通过在线互知识蒸馏实现跨模态医学图像分割，”在 *AAAI 人工智能会议论文集*
    中，卷 34，第 01 期，2020，页码 775–783。'
- en: '[181] K. Li, S. Wang, L. Yu, and P.-A. Heng, “Dual-teacher: Integrating intra-domain
    and inter-domain teachers for annotation-efficient cardiac segmentation,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 418–427.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] K. Li, S. Wang, L. Yu, 和 P.-A. Heng, “双教师：整合内领域和跨领域教师以实现高效标注的心脏分割，”在
    *国际医学图像计算与计算机辅助干预会议* 中。 Springer, 2020, 页码 418–427。'
- en: '[182] M. Hu, M. Maillard, Y. Zhang, T. Ciceri, G. La Barbera, I. Bloch, and
    P. Gori, “Knowledge distillation from multi-modal to mono-modal segmentation networks,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 772–781.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] M. Hu, M. Maillard, Y. Zhang, T. Ciceri, G. La Barbera, I. Bloch, 和 P.
    Gori，“从多模态到单模态分割网络的知识蒸馏，” 在 *国际医学图像计算与计算机辅助手术会议*. Springer, 2020, 页 772–781。'
- en: '[183] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting multimodal
    medical volumes with cycle-and shape-consistency generative adversarial network,”
    in *CVPR2018*, 2018, pp. 9242–9251.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Z. Zhang, L. Yang, 和 Y. Zheng，“使用循环和形状一致性生成对抗网络翻译和分割多模态医学体积，” 在 *CVPR2018*,
    2018, 页 9242–9251。'
- en: '[184] A. Chartsias, G. Papanastasiou, C. Wang, S. Semple, D. Newby, R. Dharmakumar,
    and S. Tsaftaris, “Disentangle, align and fuse for multimodal and semi-supervised
    image segmentation.” *IEEE transactions on medical imaging*, 2020.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] A. Chartsias, G. Papanastasiou, C. Wang, S. Semple, D. Newby, R. Dharmakumar,
    和 S. Tsaftaris，“为多模态和半监督图像分割解开、对齐和融合。” *IEEE 医学成像交易*, 2020。'
- en: '[185] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis.” *arXiv preprint arXiv:1904.00625*, 2019.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] S. Chen, K. Ma, 和 Y. Zheng，“Med3d: 用于 3d 医学图像分析的迁移学习。” *arXiv 预印本 arXiv:1904.00625*,
    2019。'
- en: '[186] L. Berger, H. Eoin, M. J. Cardoso, and S. Ourselin, “An adaptive sampling
    scheme to efficiently train fully convolutional networks for semantic segmentation,”
    in *Annual Conference on Medical Image Understanding and Analysis*.   Springer,
    2018, pp. 277–286.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] L. Berger, H. Eoin, M. J. Cardoso, 和 S. Ourselin，“一种自适应采样方案以高效训练用于语义分割的全卷积网络，”
    在 *医学图像理解与分析年会*. Springer, 2018, 页 277–286。'
- en: '[187] H. Li, X. Liu, S. Boumaraf, W. Liu, X. Gong, and X. Ma, “A new three-stage
    curriculum learning approach for deep network based liver tumor segmentation,”
    in *2020 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020,
    pp. 1–6.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Li, X. Liu, S. Boumaraf, W. Liu, X. Gong, 和 X. Ma，“一种新的三阶段课程学习方法用于基于深度网络的肝肿瘤分割，”
    在 *2020 国际联合神经网络会议 (IJCNN)*. IEEE, 2020, 页 1–6。'
- en: '[188] H. Kervadec, J. Dolz, É. Granger, and I. B. Ayed, “Curriculum semi-supervised
    segmentation,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2019, pp. 568–576.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. Kervadec, J. Dolz, É. Granger, 和 I. B. Ayed，“课程半监督分割，” 在 *国际医学图像计算与计算机辅助手术会议*.
    Springer, 2019, 页 568–576。'
- en: '[189] Z. Zhao, X. Zhang, C. Chen, W. Li, S. Peng, J. Wang, X. Yang, L. Zhang,
    and Z. Zeng, “Semi-supervised self-taught deep learning for finger bones segmentation,”
    in *2019 IEEE EMBS International Conference on Biomedical & Health Informatics
    (BHI)*.   IEEE, 2019, pp. 1–4.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Z. Zhao, X. Zhang, C. Chen, W. Li, S. Peng, J. Wang, X. Yang, L. Zhang,
    和 Z. Zeng，“用于手指骨分割的半监督自学深度学习，” 在 *2019 IEEE EMBS 国际生物医学与健康信息学会议 (BHI)*. IEEE,
    2019, 页 1–4。'
- en: '[190] J. Zhang, G. Wang, H. Xie, S. Zhang, N. Huang, S. Zhang, and L. Gu, “Weakly
    supervised vessel segmentation in x-ray angiograms by self-paced learning from
    noisy labels with suggestive annotation,” *Neurocomputing*, vol. 417, pp. 114–127,
    2020.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] J. Zhang, G. Wang, H. Xie, S. Zhang, N. Huang, S. Zhang, 和 L. Gu，“通过自适应学习从噪声标签和建议注释中进行弱监督血管分割，”
    *Neurocomputing*, 卷 417, 页 114–127, 2020。'
- en: '[191] B. Wu, Z. Zhou, J. Wang, and Y. Wang, “Joint learning for pulmonary nodule
    segmentation, attributes and malignancy prediction,” in *ISBI 2018*.   IEEE, 2018,
    pp. 1109–1113.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] B. Wu, Z. Zhou, J. Wang, 和 Y. Wang，“用于肺结节分割、属性和恶性预测的联合学习，” 在 *ISBI 2018*.
    IEEE, 2018, 页 1109–1113。'
- en: '[192] A. Hatamizadeh, D. Terzopoulos, and A. Myronenko, “End-to-end boundary
    aware networks for medical image segmentation,” in *International Workshop on
    Machine Learning in Medical Imaging*.   Springer, 2019, pp. 187–194.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] A. Hatamizadeh, D. Terzopoulos, 和 A. Myronenko，“用于医学图像分割的端到端边界感知网络，”
    在 *国际医学影像机器学习研讨会*. Springer, 2019, 页 187–194。'
- en: '[193] Z. Zhu, D. Jin, K. Yan, T.-Y. Ho, X. Ye, D. Guo, C.-H. Chao, J. Xiao,
    A. Yuille, and L. Lu, “Lymph node gross tumor volume detection and segmentation
    via distance-based gating using 3d ct/pet imaging in radiotherapy,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 753–762.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Z. Zhu, D. Jin, K. Yan, T.-Y. Ho, X. Ye, D. Guo, C.-H. Chao, J. Xiao,
    A. Yuille, 和 L. Lu，“通过基于距离的门控使用 3d ct/pet 成像在放射治疗中检测和分割淋巴结总肿瘤体积，” 在 *国际医学图像计算与计算机辅助手术会议*.
    Springer, 2020, 页 753–762。'
- en: '[194] D. Jin, D. Guo, T.-Y. Ho, A. P. Harrison, J. Xiao, C.-k. Tseng, and L. Lu,
    “Deeptarget: Gross tumor and clinical target volume segmentation in esophageal
    cancer radiotherapy,” *Medical Image Analysis*, p. 101909, 2020.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] D. Jin、D. Guo、T.-Y. Ho、A. P. Harrison、J. Xiao、C.-k. Tseng 和 L. Lu， “Deeptarget：食管癌放疗中的肿瘤和临床靶体积分割，”
    *Medical Image Analysis*，第101909页，2020年。'
- en: '[195] K. Huang, H.-D. Cheng, Y. Zhang, B. Zhang, P. Xing, and C. Ning, “Medical
    knowledge constrained semantic breast ultrasound image segmentation,” in *2018
    24th International Conference on Pattern Recognition (ICPR)*.   IEEE, 2018, pp.
    1193–1198.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] K. Huang、H.-D. Cheng、Y. Zhang、B. Zhang、P. Xing 和 C. Ning， “医学知识约束的语义乳腺超声图像分割，”
    收录于 *2018年第24届国际模式识别大会（ICPR）*。 IEEE，2018，第1193–1198页。'
- en: '[196] N. Painchaud, Y. Skandarani, T. Judge, O. Bernard, A. Lalande, and P.-M.
    Jodoin, “Cardiac mri segmentation with strong anatomical guarantees,” in *MICCAI2019*.   Springer,
    2019, pp. 632–640.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] N. Painchaud、Y. Skandarani、T. Judge、O. Bernard、A. Lalande 和 P.-M. Jodoin，
    “具有强解剖学保证的心脏MRI分割，” 收录于 *MICCAI2019*。 Springer，2019，第632–640页。'
- en: '[197] ——, “Cardiac segmentation with strong anatomical guarantees,” *IEEE Transactions
    on Medical Imaging*, vol. 39, no. 11, pp. 3703–3713, 2020.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] ——， “具有强解剖学保证的心脏分割，” *IEEE Transactions on Medical Imaging*，第39卷，第11期，第3703–3713页，2020年。'
- en: '[198] O. Oktay, E. Ferrante *et al.*, “Anatomically constrained neural networks
    (acnns): application to cardiac image enhancement and segmentation,” *IEEE TMI*,
    vol. 37, no. 2, pp. 384–395, 2017.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] O. Oktay、E. Ferrante *et al.*， “解剖学约束神经网络（ACNNs）：应用于心脏图像增强和分割，” *IEEE
    TMI*，第37卷，第2期，第384–395页，2017年。'
- en: '[199] H. Ravishankar, R. Venkataramani, S. Thiruvenkadam, P. Sudhakar, and
    V. Vaidya, “Learning and incorporating shape models for semantic segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 203–211.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] H. Ravishankar、R. Venkataramani、S. Thiruvenkadam、P. Sudhakar 和 V. Vaidya，
    “学习和融入形状模型用于语义分割，” 收录于 *MICCAI2017*。 Springer，2017，第203–211页。'
- en: '[200] A. BenTaieb and G. Hamarneh, “Topology aware fully convolutional networks
    for histology gland segmentation,” in *MICCAI2018*.   Springer, 2016, pp. 460–468.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. BenTaieb 和 G. Hamarneh， “用于组织腺体分割的拓扑感知全卷积网络，” 收录于 *MICCAI2018*。 Springer，2016，第460–468页。'
- en: '[201] Z. Mirikharaji and G. Hamarneh, “Star shape prior in fully convolutional
    networks for skin lesion segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    737–745.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Z. Mirikharaji 和 G. Hamarneh， “用于皮肤病变分割的星形先验全卷积网络，” 收录于 *MICCAI2018*。
    Springer，2018，第737–745页。'
- en: '[202] H. Zheng, L. Lin, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y. Chen,
    R. Tong, and J. Wu, “Semi-supervised segmentation of liver using adversarial learning
    with deep atlas prior,” in *Medical Image Computing and Computer-Assisted Intervention*,
    2019, pp. 148–156.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] H. Zheng、L. Lin、H. Hu、Q. Zhang、Q. Chen、Y. Iwamoto、X. Han、Y. Chen、R. Tong
    和 J. Wu， “使用对抗学习和深度图谱先验的半监督肝脏分割，” 收录于 *Medical Image Computing and Computer-Assisted
    Intervention*，2019，第148–156页。'
- en: '[203] C. Zotti, Z. Luo, A. Lalande, and P. Jodoin, “Convolutional neural network
    with shape prior applied to cardiac mri segmentation,” *IEEE Journal of Biomedical
    and Health Informatics*, vol. 23, no. 3, pp. 1119–1128, 2019.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] C. Zotti、Z. Luo、A. Lalande 和 P. Jodoin， “应用形状先验的卷积神经网络用于心脏MRI分割，” *IEEE
    Biomedical and Health Informatics*，第23卷，第3期，第1119–1128页，2019年。'
- en: '[204] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in convolutional
    networks for unsupervised biomedical segmentation,” in *CVPR2018*, 2018, pp. 9290–9299.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] A. V. Dalca、J. Guttag 和 M. R. Sabuncu， “卷积网络中的解剖学先验用于无监督生物医学分割，” 收录于
    *CVPR2018*，2018，第9290–9299页。'
- en: '[205] Y. He, G. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J. Dillenseger,
    P. Shao, S. Zhang *et al.*, “Dpa-densebiasnet: Semi-supervised 3d fine renal artery
    segmentation with dense biased network and deep priori anatomy.” in *Medical Image
    Computing and Computer-Assisted Intervention*, 2019, pp. 139–147.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Y. He、G. Yang、Y. Chen、Y. Kong、J. Wu、L. Tang、X. Zhu、J. Dillenseger、P.
    Shao 和 S. Zhang *et al.*， “Dpa-densebiasnet：使用密集偏置网络和深度先验解剖学的半监督3D细分肾动脉分割。” 收录于
    *Medical Image Computing and Computer-Assisted Intervention*，2019，第139–147页。'
- en: '[206] B. Luo, J. Shen, S. Cheng, Y. Wang, and M. Pantic, “Shape constrained
    network for eye segmentation in the wild,” in *The IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 1952–1960.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] B. Luo、J. Shen、S. Cheng、Y. Wang 和 M. Pantic， “用于野外眼部分割的形状约束网络，” 收录于 *The
    IEEE Winter Conference on Applications of Computer Vision*，2020，第1952–1960页。'
- en: '[207] Y. Song, L. Zhu, B. Lei, B. Sheng, Q. Dou, J. Qin, and K.-S. Choi, “Shape
    mask generator: Learning to refine shape priors for segmenting overlapping cervical
    cytoplasms,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 639–649.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Song, L. Zhu, B. Lei, B. Sheng, Q. Dou, J. Qin, 和 K.-S. Choi，“形状掩码生成器：学习优化形状先验以分割重叠的宫颈细胞质，”
    在 *国际医学图像计算与计算机辅助干预会议*。   Springer，2020年，第639–649页。'
- en: '[208] A. Boutillon, B. Borotikar, V. Burdin, and P.-H. Conze, “Combining shape
    priors with conditional adversarial networks for improved scapula segmentation
    in mr images,” in *2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI)*.   IEEE, 2020, pp. 1164–1167.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] A. Boutillon, B. Borotikar, V. Burdin, 和 P.-H. Conze，“结合形状先验与条件对抗网络以改进MR图像中肩胛骨分割，”
    在 *2020 IEEE第17届国际生物医学成像研讨会（ISBI）*。   IEEE，2020年，第1164–1167页。'
- en: '[209] D. Pham, G. Dovletov, and J. Pauli, “Liver segmentation in ct with mri
    data: Zero-shot domain adaptation by contour extraction and shape priors,” in
    *2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)*.   IEEE,
    2020, pp. 1538–1542.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] D. Pham, G. Dovletov, 和 J. Pauli，“结合MRI数据的CT肝脏分割：通过轮廓提取和形状先验进行零样本领域适应，”
    在 *2020 IEEE第17届国际生物医学成像研讨会（ISBI）*。   IEEE，2020年，第1538–1542页。'
- en: '[210] M. Engin, R. Lange, A. Nemes, S. Monajemi, M. Mohammadzadeh, C. K. Goh,
    T. M. Tu, B. Y. Tan, P. Paliwal, L. L. Yeo *et al.*, “Agan: An anatomy corrector
    conditional generative adversarial network,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*.   Springer, 2020, pp. 708–717.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] M. Engin, R. Lange, A. Nemes, S. Monajemi, M. Mohammadzadeh, C. K. Goh,
    T. M. Tu, B. Y. Tan, P. Paliwal, L. L. Yeo *等*，“Agan：一种解剖校正条件生成对抗网络，” 在 *国际医学图像计算与计算机辅助干预会议*。   Springer，2020年，第708–717页。'
- en: '[211] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N.
    Metaxas, H. Li, and M. Chen, “Focusnetv2: Imbalanced large and small organ segmentation
    with adversarial shape constraint for head and neck ct images,” *Medical Image
    Analysis*, vol. 67, p. 101831, 2020.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N.
    Metaxas, H. Li, 和 M. Chen，“Focusnetv2：具有对抗形状约束的大型和小型器官分割的不平衡头颈CT图像，” *医学影像分析*，第67卷，第101831页，2020年。'
- en: '[212] K. Kushibar, S. Valverde, S. González-Villà, J. Bernal, M. Cabezas, A. Oliver,
    and X. Lladó, “Automated sub-cortical brain structure segmentation combining spatial
    and deep convolutional features,” *Medical Image Analysis*, vol. 48, pp. 177–186,
    2018.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] K. Kushibar, S. Valverde, S. González-Villà, J. Bernal, M. Cabezas, A. Oliver,
    和 X. Lladó，“结合空间和深度卷积特征的自动亚皮质脑结构分割，” *医学影像分析*，第48卷，第177–186页，2018年。'
- en: '[213] S. Rezaei, A. Emami *et al.*, “Gland segmentation in histopathology images
    using deep networks and handcrafted features,” in *2019 41st Annual International
    Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE,
    2019, pp. 1031–1034.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] S. Rezaei, A. Emami *等*，“在组织病理学图像中使用深度网络和手工特征进行腺体分割，” 在 *2019年第41届IEEE医学与生物学工程学会年会（EMBC）*。   IEEE，2019年，第1031–1034页。'
- en: '[214] H. Khan, P. M. Shah, M. A. Shah, S. ul Islam, and J. J. Rodrigues, “Cascading
    handcrafted features and convolutional neural network for iot-enabled brain tumor
    segmentation,” *Computer Communications*, 2020.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] H. Khan, P. M. Shah, M. A. Shah, S. ul Islam, 和 J. J. Rodrigues，“级联手工特征与卷积神经网络用于物联网支持的脑肿瘤分割，”
    *计算机通信*，2020年。'
- en: '[215] H. Narotamo, J. M. Sanches, and M. Silveira, “Combining deep learning
    with handcrafted features for cell nuclei segmentation,” in *2020 42nd Annual
    International Conference of the IEEE Engineering in Medicine & Biology Society
    (EMBC)*.   IEEE, 2020, pp. 1428–1431.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] H. Narotamo, J. M. Sanches, 和 M. Silveira，“结合深度学习和手工特征进行细胞核分割，” 在 *2020年第42届IEEE医学与生物学工程学会年会（EMBC）*。   IEEE，2020年，第1428–1431页。'
- en: '[216] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 4489–4497.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] D. Tran, L. Bourdev, R. Fergus, L. Torresani, 和 M. Paluri，“利用3D卷积网络学习时空特征，”
    在 *IEEE国际计算机视觉会议论文集*，2015年，第4489–4497页。'
- en: '[217] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for latent
    variable models,” in *Advances in Neural Information Processing Systems*, 2010,
    pp. 1189–1197.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] M. P. Kumar, B. Packer, 和 D. Koller，“自适应学习用于潜变量模型，” 在 *神经信息处理系统进展*，2010年，第1189–1197页。'
- en: '[218] G. N. Sharma, R. Dave, J. Sanadya, P. Sharma, and K. Sharma, “Various
    types and management of breast cancer: an overview,” *Journal of advanced pharmaceutical
    technology &amp; research*, vol. 1, no. 2, p. 109, 2010.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] G. N. Sharma, R. Dave, J. Sanadya, P. Sharma 和 K. Sharma，“乳腺癌的各种类型及其管理：概述”，*Journal
    of advanced pharmaceutical technology & research*，第 1 卷，第 2 期，页码 109，2010 年。'
- en: '[219] C. Qin, J. Schlemper, J. Caballero, A. N. Price, J. V. Hajnal, and D. Rueckert,
    “Convolutional recurrent neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 38, no. 1, pp. 280–290, 2018.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] C. Qin, J. Schlemper, J. Caballero, A. N. Price, J. V. Hajnal 和 D. Rueckert，“用于动态
    MR 图像重建的卷积递归神经网络”，*IEEE TMI*，第 38 卷，第 1 期，页码 280–290，2018 年。'
- en: '[220] J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price, and D. Rueckert,
    “A deep cascade of convolutional neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 37, no. 2, pp. 491–503, 2017.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price 和 D. Rueckert，“用于动态
    MR 图像重建的深度卷积神经网络级联”，*IEEE TMI*，第 37 卷，第 2 期，页码 491–503，2017 年。'
- en: '[221] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,
    S. Arridge, J. Keegan, Y. Guo *et al.*, “Dagan: deep de-aliasing generative adversarial
    networks for fast compressed sensing mri reconstruction,” *IEEE TMI*, vol. 37,
    no. 6, pp. 1310–1321, 2017.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,
    S. Arridge, J. Keegan, Y. Guo *等*，“Dagan: 深度去混叠生成对抗网络用于快速压缩感知 MRI 重建”，*IEEE TMI*，第
    37 卷，第 6 期，页码 1310–1321，2017 年。'
- en: '[222] H. B. Yedder, M. Shokoufi, B. Cardoen, F. Golnaraghi, and G. Hamarneh,
    “Limited-angle diffuse optical tomography image reconstruction using deep learning,”
    in *MICCAI2019*.   Springer, 2019, pp. 66–74.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] H. B. Yedder, M. Shokoufi, B. Cardoen, F. Golnaraghi 和 G. Hamarneh，“使用深度学习进行有限角度漫射光学断层图像重建”，见
    *MICCAI2019*。Springer，2019 年，页码 66–74。'
- en: '[223] S. U. H. Dar, M. Yurt, M. Shahdloo, M. E. Ildız, and T. Çukur, “Synergistic
    reconstruction and synthesis via generative adversarial networks for accelerated
    multi-contrast mri,” *arXiv preprint arXiv:1805.10704*, 2018.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] S. U. H. Dar, M. Yurt, M. Shahdloo, M. E. Ildız 和 T. Çukur，“通过生成对抗网络进行协同重建和合成以加速多对比
    MRI”，*arXiv 预印本 arXiv:1805.10704*，2018 年。'
- en: '[224] J. Ahmad, M. Sajjad, I. Mehmood, and S. W. Baik, “Sinc: Saliency-injected
    neural codes for representation and efficient retrieval of medical radiographs,”
    *PloS one*, vol. 12, no. 8, 2017.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] J. Ahmad, M. Sajjad, I. Mehmood 和 S. W. Baik，“Sinc: 用于医学放射图像表示和高效检索的显著性注入神经代码”，*PloS
    one*，第 12 卷，第 8 期，2017 年。'
- en: '[225] A. Khatami, M. Babaie, H. R. Tizhoosh, A. Khosravi, T. Nguyen, and S. Nahavandi,
    “A sequential search-space shrinking using cnn transfer learning and a radon projection
    pool for medical image retrieval,” *Expert Systems with Applications*, vol. 100,
    pp. 224–233, 2018.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. Khatami, M. Babaie, H. R. Tizhoosh, A. Khosravi, T. Nguyen 和 S. Nahavandi，“利用
    CNN 迁移学习和 Radon 投影池进行医学图像检索的顺序搜索空间缩小”，*Expert Systems with Applications*，第 100
    卷，页码 224–233，2018 年。'
- en: '[226] Z. N. K. Swati, Q. Zhao, M. Kabir, F. Ali, Z. Ali, S. Ahmed, and J. Lu,
    “Content-based brain tumor retrieval for mr images using transfer learning,” *IEEE
    Access*, vol. 7, pp. 17 809–17 822, 2019.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Z. N. K. Swati, Q. Zhao, M. Kabir, F. Ali, Z. Ali, S. Ahmed 和 J. Lu，“基于内容的脑肿瘤
    MR 图像检索使用迁移学习”，*IEEE Access*，第 7 卷，页码 17,809–17,822，2019 年。'
- en: '[227] Y. Anavi, I. Kogan, E. Gelbart, O. Geva, and H. Greenspan, “A comparative
    study for chest radiograph image retrieval using binary texture and deep learning
    classification,” in *EMBC 2015*.   IEEE, 2015, pp. 2940–2943.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Y. Anavi, I. Kogan, E. Gelbart, O. Geva 和 H. Greenspan，“使用二值纹理和深度学习分类进行胸部放射图像检索的比较研究”，见
    *EMBC 2015*。IEEE，2015 年，页码 2940–2943。'
- en: '[228] Y. Anavi, I. Kogan *et al.*, “Visualizing and enhancing a deep learning
    framework using patients age and gender for chest x-ray image retrieval,” in *Medical
    Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International Society for
    Optics and Photonics, 2016, p. 978510.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Y. Anavi, I. Kogan *等*，“利用患者年龄和性别可视化和增强深度学习框架用于胸部 X 光图像检索”，见 *Medical
    Imaging 2016: Computer-Aided Diagnosis*，第 9785 卷。国际光学和光子学学会，2016 年，页码 978510。'
- en: '[229] B. Jing, P. Xie, and E. Xing, “On the automatic generation of medical
    imaging reports,” *arXiv preprint arXiv:1711.08195*, 2017.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] B. Jing, P. Xie 和 E. Xing，“医学影像报告的自动生成”，*arXiv 预印本 arXiv:1711.08195*，2017
    年。'
- en: '[230] G. Liu, T. H. Hsu, M. B. A. Mcdermott, W. Boag, W. Weng, P. Szolovits,
    and M. Ghassemi, “Clinically accurate chest x-ray report generation.” *arXiv preprint
    arXiv: 1904.02633*, 2019.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] G. Liu, T. H. Hsu, M. B. A. Mcdermott, W. Boag, W. Weng, P. Szolovits
    和 M. Ghassemi，“临床准确的胸部 X 光报告生成。” *arXiv 预印本 arXiv: 1904.02633*，2019 年。'
- en: '[231] Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Hybrid retrieval-generation
    reinforced agent for medical image report generation,” in *Advances in neural
    information processing systems*, 2018, pp. 1530–1540.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Y. Li, X. Liang, Z. Hu, 和 E. P. Xing，“用于医学图像报告生成的混合检索-生成增强代理”，发表于*神经信息处理系统进展*，2018年，第1530–1540页。'
- en: '[232] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Knowledge-driven encode,
    retrieve, paraphrase for medical image report generation,” in *Proceedings of
    the AAAI Conference on Artificial Intelligence*, vol. 33, 2019, pp. 6666–6673.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] C. Y. Li, X. Liang, Z. Hu, 和 E. P. Xing，“基于知识的编码、检索、改述用于医学图像报告生成”，发表于*AAAI人工智能会议论文集*，第33卷，2019年，第6666–6673页。'
- en: '[233] W. Gale, L. Oakden-Rayner, G. Carneiro, A. P. Bradley, and L. J. Palmer,
    “Producing radiologist-quality reports for interpretable artificial intelligence,”
    *arXiv preprint arXiv:1806.00340*, 2018.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] W. Gale, L. Oakden-Rayner, G. Carneiro, A. P. Bradley, 和 L. J. Palmer，“为可解释的人工智能生成放射科医师质量的报告”，*arXiv
    预印本 arXiv:1806.00340*，2018年。'
- en: '[234] Y. Hong and C. E. Kahn, “Content analysis of reporting templates and
    free-text radiology reports,” *Journal of digital imaging*, vol. 26, no. 5, pp.
    843–849, 2013.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Y. Hong 和 C. E. Kahn，“报告模板和自由文本放射学报告的内容分析”，*数字成像杂志*，第26卷，第5期，第843–849页，2013年。'
- en: '[235] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, “When radiology
    report generation meets knowledge graph,” *arXiv preprint arXiv:2002.08277*, 2020.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, 和 D. Xu，“当放射学报告生成遇上知识图谱”，*arXiv
    预印本 arXiv:2002.08277*，2020年。'
- en: '[236] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in *International
    conference on machine learning*.   PMLR, 2018, pp. 1989–1998.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    和 T. Darrell，“Cycada：循环一致的对抗领域适应”，发表于*国际机器学习会议*。PMLR，2018年，第1989–1998页。'
- en: '[237] M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Unsupervised domain adaptation
    with residual transfer networks,” in *Advances in Neural Information Processing
    Systems*, 2016, pp. 136–144.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] M. Long, H. Zhu, J. Wang, 和 M. I. Jordan，“使用残差转移网络的无监督领域适应”，发表于*神经信息处理系统进展*，2016年，第136–144页。'
- en: '[238] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *CVPR2017*, 2017, pp. 7167–7176.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell，“对抗性辨别领域适应”，发表于*CVPR2017*，2017年，第7167–7176页。'
- en: '[239] Y. Luo, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Taking a closer look
    at domain shift: Category-level adversaries for semantics consistent domain adaptation,”
    in *CVPR2019*, 2019, pp. 2507–2516.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Y. Luo, L. Zheng, T. Guan, J. Yu, 和 Y. Yang，“深入探讨领域偏移：用于语义一致领域适应的类别级对抗者”，发表于*CVPR2019*，2019年，第2507–2516页。'
- en: '[240] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker,
    “Learning to adapt structured output space for semantic segmentation,” in *CVPR2018*,
    2018, pp. 7472–7481.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, 和 M. Chandraker，“学习适应结构化输出空间用于语义分割”，发表于*CVPR2018*，2018年，第7472–7481页。'
- en: '[241] Y. Zhang, Y. Wei, Q. Wu, P. Zhao, S. Niu, J. Huang, and M. Tan, “Collaborative
    unsupervised domain adaptation for medical image diagnosis,” *IEEE Transactions
    on Image Processing*, vol. 29, pp. 7834–7844, 2020.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Y. Zhang, Y. Wei, Q. Wu, P. Zhao, S. Niu, J. Huang, 和 M. Tan，“用于医学图像诊断的协作无监督领域适应”，*IEEE图像处理学报*，第29卷，第7834–7844页，2020年。'
- en: '[242] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O Donnell, H. Huang, M. Chen,
    and W. Cai, “Pdam: A panoptic-level feature alignment framework for unsupervised
    domain adaptive instance segmentation in microscopy images,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O Donnell, H. Huang, M. Chen,
    和 W. Cai，“Pdam：用于显微图像中无监督领域自适应实例分割的全景级特征对齐框架”，*IEEE医学成像学报*，2020年。'
- en: '[243] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge graph and text jointly
    embedding,” pp. 1591–1601, 2014.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Z. Wang, J. Zhang, J. Feng, 和 Z. Chen，“知识图谱与文本的联合嵌入”，第1591–1601页，2014年。'
- en: '[244] M. Wistuba, A. Rawat, and T. Pedapati, “A survey on neural architecture
    search.” *arXiv preprint arXiv: 1905.01392*, 2019.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] M. Wistuba, A. Rawat, 和 T. Pedapati，“神经架构搜索综述”，*arXiv 预印本 arXiv: 1905.01392*，2019年。'
- en: '[245] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    and L. Lu, “Organ at risk segmentation for head and neck cancer using stratified
    learning and neural architecture search,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 4223–4232.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    和 L. Lu，“使用分层学习和神经架构搜索的头颈癌器官风险分割”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第4223–4232页。'
