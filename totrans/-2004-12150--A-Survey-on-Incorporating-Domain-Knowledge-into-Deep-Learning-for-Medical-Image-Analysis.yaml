- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:01:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.12150] A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.12150] 关于将领域知识融入深度学习以进行医学图像分析的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12150](https://ar5iv.labs.arxiv.org/html/2004.12150)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12150](https://ar5iv.labs.arxiv.org/html/2004.12150)
- en: A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于将领域知识融入深度学习以进行医学图像分析的调查
- en: 'Xiaozheng Xie, Jianwei Niu, , Xuefeng Liu, Zhengsu Chen, Shaojie Tang,  and
    Shui Yu X. Xie, J. Niu, X. Liu and Z. Chen are with the State Key Laboratory of
    Virtual Reality Technology and Systems, School of Computer Science and Engineering,
    Beihang University, Beijing 100191, China. E-mails: {xiexzheng,niujianwei,liu_xuefeng,danczs}@buaa.edu.cn
    J. Niu is also with the Beijing Advanced Innovation Center for Big Data and Brain
    Computing (BDBC) and Hangzhou Innovation Institute of Beihang University. S. Tang
    is in Jindal School of Management, The University of Texas at Dallas. E-mails:
    tangshaojie@gmail.com S. Yu is in School of Computer Science, University of Technology
    Sydney, Australia. E-mails: Shui.Yu@uts.edu.au'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xie Xiaozheng，Niu Jianwei，Liu Xuefeng，Chen Zhengsu，Tang Shaojie 和 Shui Yu X.
    Xie、J. Niu、X. Liu 和 Z. Chen 均来自北京航空航天大学计算机科学与工程学院虚拟现实技术与系统国家重点实验室，中国北京 100191。电子邮件：{xiexzheng,
    niujianwei, liu_xuefeng, danczs}@buaa.edu.cn J. Niu 还与北京大数据与脑计算高级创新中心（BDBC）及北京航空航天大学杭州创新研究院有关。S.
    Tang 在德克萨斯大学达拉斯分校金达尔管理学院任职。电子邮件：tangshaojie@gmail.com S. Yu 在澳大利亚悉尼科技大学计算机科学学院任职。电子邮件：Shui.Yu@uts.edu.au
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Although deep learning models like CNNs have achieved great success in medical
    image analysis, the small size of medical datasets remains a major bottleneck
    in this area. To address this problem, researchers have started looking for external
    information beyond current available medical datasets. Traditional approaches
    generally leverage the information from natural images via transfer learning.
    More recent works utilize the domain knowledge from medical doctors, to create
    networks that resemble how medical doctors are trained, mimic their diagnostic
    patterns, or focus on the features or areas they pay particular attention to.
    In this survey, we summarize the current progress on integrating medical domain
    knowledge into deep learning models for various tasks, such as disease diagnosis,
    lesion, organ and abnormality detection, lesion and organ segmentation. For each
    task, we systematically categorize different kinds of medical domain knowledge
    that have been utilized and their corresponding integrating methods. We also provide
    current challenges and directions for future research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像CNN这样的深度学习模型在医学图像分析中取得了巨大成功，但医学数据集的规模小仍然是该领域的主要瓶颈。为了解决这个问题，研究人员已经开始寻找超出当前可用医学数据集的外部信息。传统方法通常通过迁移学习利用自然图像中的信息。最近的工作则利用来自医学医生的领域知识，创建出类似于医学医生训练方式的网络，模拟他们的诊断模式，或关注他们特别关注的特征或区域。在本调查中，我们总结了将医学领域知识整合到深度学习模型中的当前进展，涉及疾病诊断、病灶、器官和异常检测、病灶和器官分割等各种任务。对于每个任务，我们系统地分类了已经利用的不同类型的医学领域知识及其相应的整合方法。我们还提供了当前的挑战和未来研究的方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: medical image analysis, medical domain knowledge, deep neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分析，医学领域知识，深度神经网络。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent years have witnessed a tremendous progress in computer-aided detection/diagnosis
    (CAD) in medical imaging and diagnostic radiology, primarily thanks to the advancement
    of deep learning techniques. Having achieved great success in computer vision
    tasks, various deep learning models, mainly convolutional neural networks (CNNs),
    soon be applied to CAD. Among the applications are the early detection and diagnosis
    of breast cancer, lung cancer, glaucoma, and skin cancer [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机辅助检测/诊断（CAD）在医学成像和诊断放射学中取得了巨大进展，这主要得益于深度学习技术的进步。各种深度学习模型，主要是卷积神经网络（CNNs），在计算机视觉任务中取得了巨大成功，并很快被应用于CAD。应用包括乳腺癌、肺癌、青光眼和皮肤癌的早期检测和诊断[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]。
- en: However, the small size of medical datasets continues to be an issue in obtaining
    satisfactory deep learning model for CAD; in general, bigger datasets result in
    better deep learning models [[5](#bib.bib5)]. In traditional computer vision tasks,
    there are many large-scale and well-annotated datasets, such as ImageNet ¹¹1http://www.image-net.org/
    (over 14M labeled images from 20k categories) and COCO ²²2http://mscoco.org/ (with
    more than 200k annotated images across 80 categories). In contrast, some popular
    publicly available medical datasets are much smaller (see Table [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis")). For example, among the datasets for different tasks,
    only ChestX-ray14 and DeepLesion, contain more than 100k labeled medical images,
    while most datasets only have a few thousands or even hundreds of medical images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，医学数据集的规模较小仍然是获得令人满意的 CAD 深度学习模型的问题；一般来说，数据集越大，深度学习模型的效果越好 [[5](#bib.bib5)]。在传统计算机视觉任务中，有许多大规模且标注良好的数据集，例如
    ImageNet ¹¹1http://www.image-net.org/（超过 1400 万张标注图像，涵盖 2 万个类别）和 COCO ²²2http://mscoco.org/（超过
    20 万张标注图像，涵盖 80 个类别）。相比之下，一些流行的公开医学数据集要小得多（参见表 [I](#S1.T1 "TABLE I ‣ 1 Introduction
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis)）。例如，在不同任务的数据集中，只有 ChestX-ray14 和 DeepLesion 包含超过 10 万张标注医学图像，而大多数数据集仅有几千张甚至几百张医学图像。
- en: 'TABLE I: Examples of popular datasets in the medical domain'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 医学领域中流行数据集的示例'
- en: '| Name | Purpose | Type | Imaging | Number of Images |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 目的 | 类型 | 成像 | 图像数量 |'
- en: '|'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ADNI [[6](#bib.bib6)] &#124;'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ADNI [[6](#bib.bib6)] &#124;'
- en: '| Classification | Brain | Multiple | 1921 patients |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 大脑 | 多种 | 1921 位患者 |'
- en: '| ABIDE [[7](#bib.bib7)] | Classification | Brain | MRI | 539 patients and
    573 controls |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ABIDE [[7](#bib.bib7)] | 分类 | 大脑 | MRI | 539 位患者和 573 位对照 |'
- en: '|'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ACDC [[8](#bib.bib8)] &#124;'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACDC [[8](#bib.bib8)] &#124;'
- en: '| Classification | Cardiac | MRI | 150 patients |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 心脏 | MRI | 150 位患者 |'
- en: '| ChestX-ray14 [[9](#bib.bib9)] | Detection | Chest | X-ray | 112,120 images
    from 30,805 patients |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ChestX-ray14 [[9](#bib.bib9)] | 检测 | 胸部 | X-ray | 112,120 张图像来自 30,805 位患者
    |'
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LIDC-IDRI [[10](#bib.bib10)] &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LIDC-IDRI [[10](#bib.bib10)] &#124;'
- en: '| Detection | Lung | CT, X-ray | 1,018 patients |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 检测 | 肺部 | CT, X-ray | 1,018 位患者 |'
- en: '|'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LUNA16 [[11](#bib.bib11)] &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LUNA16 [[11](#bib.bib11)] &#124;'
- en: '| Detection | Lung | CT | 888 images |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 检测 | 肺部 | CT | 888 张图像 |'
- en: '| MURA [[12](#bib.bib12)] | Detection | Musculo-skeletal | X-ray |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| MURA [[12](#bib.bib12)] | 检测 | 骨骼肌肉系统 | X-ray |'
- en: '&#124; 40,895 images from 14,982 patients &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 40,895 张图像来自 14,982 位患者 &#124;'
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BraTS2018 [[13](#bib.bib13)] &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BraTS2018 [[13](#bib.bib13)] &#124;'
- en: '| Segmentation | Brain | MRI | 542 images |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 分割 | 大脑 | MRI | 542 张图像 |'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; STARE [[14](#bib.bib14)] &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; STARE [[14](#bib.bib14)] &#124;'
- en: '| Segmentation | Eye | SLO | 400 images |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 分割 | 眼睛 | SLO | 400 张图像 |'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DDSM [[15](#bib.bib15)] &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDSM [[15](#bib.bib15)] &#124;'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '| Breast | Mammography | 2,500 patients |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺 | 乳腺X光 | 2,500 位患者 |'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DeepLesion [[16](#bib.bib16)] &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepLesion [[16](#bib.bib16)] &#124;'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '| Multiple | CT | 32,735 images from 4,427 patients |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 多种 | CT | 32,735 张图像来自 4,427 位患者 |'
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cardiac MRI [[17](#bib.bib17)] &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 心脏 MRI [[17](#bib.bib17)] &#124;'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| Cardiac | MRI | 7,980 images from 33 cases |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 心脏 | MRI | 7,980 张图像来自 33 个案例 |'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ISIC 2018 [[18](#bib.bib18)] &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ISIC 2018 [[18](#bib.bib18)] &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; Detection &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| Skin | Dermoscopic | 13,000 images |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 皮肤 | 皮肤镜 | 13,000 张图像 |'
- en: The lack of medical datasets is represented in three aspects. First, the number
    of medical images in datasets is usually small. This problem is mainly due to
    the high cost associated with the data collection. Medical images are collected
    from computerized tomography (CT), Ultrasonic imaging (US), magnetic resonance
    imaging (MRI) scans, positron emission tomography (PET), all of which are expensive
    and labor-intensive. Second, only a small portion of medical images are annotated.
    These annotations including classification labels (e.g., benign or malignant),
    the segmentation annotations of lesion areas, etc., require efforts from experienced
    doctors. Third, it is difficult to collect enough positive cases for some rare
    diseases to obtain the balanced datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 医学数据集的不足表现为三个方面。首先，数据集中的医学图像数量通常较少。这一问题主要由于数据收集的高成本。医学图像来源于计算机断层扫描（CT）、超声成像（US）、磁共振成像（MRI）扫描、正电子发射断层扫描（PET），这些都是昂贵且劳动密集的。其次，只有少量医学图像经过注释。这些注释包括分类标签（例如，良性或恶性）、病灶区域的分割注释等，需要经验丰富的医生的努力。第三，对于一些罕见疾病，收集足够的阳性病例以获得平衡的数据集也是困难的。
- en: One direct consequence of the lack of well annotated medical data is that the
    trained deep learning models can easily suffer from the overfitting problem [[19](#bib.bib19)].
    As a result, the models perform very well on training datasets, but fail when
    dealing with new data from the problem domain. Correspondingly, many existing
    studies on medical image analysis adopt techniques from computer vision to address
    overfitting, such as reducing the complexity of the network [[20](#bib.bib20),
    [21](#bib.bib21)], adopting some regularization techniques [[22](#bib.bib22)],
    or using data augmentation strategies [[23](#bib.bib23)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏良好注释的医学数据的一个直接后果是，训练出的深度学习模型很容易遭遇过拟合问题[[19](#bib.bib19)]。因此，模型在训练数据集上表现非常好，但在处理来自问题领域的新数据时却表现不佳。相应地，许多现有的医学图像分析研究采用计算机视觉技术来解决过拟合问题，例如减少网络复杂性[[20](#bib.bib20),
    [21](#bib.bib21)]、采用一些正则化技术[[22](#bib.bib22)]，或使用数据增强策略[[23](#bib.bib23)]。
- en: However, in essence, both decreasing model complexity and leveraging data augmentation
    techniques only focus on the target task on the given datasets, but *do not introduce
    any new information into deep learning models*. Nowdays, introducing more information,
    beyond the given medical datasets has become a more promising approach to address
    the problem of small-sized medical datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从本质上讲，降低模型复杂性和利用数据增强技术只是关注给定数据集上的目标任务，但*并没有向深度学习模型引入任何新信息*。如今，除了给定的医学数据集，引入更多信息已经成为解决小规模医学数据集问题的更有前景的方法。
- en: The idea of introducing external information to improve the performance of deep
    learning models for CAD is not new. For example, it is common practice to first
    train a deep learning model on some natural image datasets like ImageNet, and
    then fine tune them on target medical datasets [[24](#bib.bib24)]. This process,
    called transfer learning [[25](#bib.bib25)], implicitly introduces information
    from natural images. Besides natural images, multi-modal medical datasets or medical
    images from different but related diseases can also be used to improve the performance
    of deep learning models [[26](#bib.bib26), [27](#bib.bib27)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 引入外部信息以提高计算机辅助诊断（CAD）深度学习模型性能的想法并不新颖。例如，通常的做法是先在一些自然图像数据集（如ImageNet）上训练深度学习模型，然后在目标医学数据集上进行微调[[24](#bib.bib24)]。这个过程称为迁移学习[[25](#bib.bib25)]，它隐式地引入了自然图像中的信息。除了自然图像，多模态医学数据集或来自不同但相关疾病的医学图像也可以用来提高深度学习模型的性能[[26](#bib.bib26),
    [27](#bib.bib27)]。
- en: Moreover, as experienced medical doctors (e.g., radiologists, ophthalmologists,
    and dermatologists) can generally give fairly accurate results, it is not surprising
    that their knowledge may help deep learning models to better accomplish the designated
    tasks. The domain knowledge of medical doctors includes the way they browse images,
    the particular areas they usually focus on, the features they give special attention
    to, and the anatomical prior knowledge they used. These types of knowledge are
    accumulated, summarized, and validated by a large number of practitioners over
    many years based on a huge amount of cases. Note that in this survey any network
    that incorporate one of these types of knowledge in their training or designing
    process should be regarded as the one incorporated medical domain knowledge.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于经验丰富的医疗医生（如放射科医生、眼科医生和皮肤科医生）通常能够提供相当准确的结果，他们的知识可能帮助深度学习模型更好地完成指定任务也就不足为奇了。医疗医生的领域知识包括他们浏览图像的方式、他们通常关注的特定区域、他们特别注意的特征以及他们使用的解剖学先验知识。这些知识是通过大量案例由众多从业者在多年中积累、总结和验证的。请注意，在本调查中，任何在其训练或设计过程中融入这些类型知识的网络都应被视为融入了医学领域知识的网络。
- en: '![Refer to caption](img/35ea96f042890d766a6a045567db868d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35ea96f042890d766a6a045567db868d.png)'
- en: 'Figure 1: Methods of information categorization and incorporating methods in
    disease diagnosis; lesion, organ, and abnormality detection; lesion and organ
    segmentation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：疾病诊断中的信息分类和方法融入方法；病灶、器官和异常检测；病灶和器官分割。
- en: 'In this survey, we focus on the three main tasks of medical image analysis:
    (1) disease diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion
    and organ segmentation. We also include other related tasks such as the image
    reconstruction, image retrieval and report generation. This survey demonstrates
    that, for almost all tasks, identifying and carefully integrating one or more
    types of domain knowledge related to the designated task will improve the performance
    of deep learning models. We organize existing works according to the following
    three aspects: the types of tasks, the types of domain knowledge that are introduced,
    and the ways of introducing the domain knowledge.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们关注医学图像分析的三个主要任务：（1）疾病诊断，（2）病灶、器官和异常检测，以及（3）病灶和器官分割。我们还包括其他相关任务，如图像重建、图像检索和报告生成。该调查表明，几乎所有任务中，识别和仔细整合一种或多种与指定任务相关的领域知识将提高深度学习模型的性能。我们根据以下三个方面组织现有工作：任务类型、引入的领域知识类型以及引入领域知识的方法。
- en: More specifically, in terms of the types of domain knowledge, some of them are
    of high-level such as training pattern [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]
    and diagnostic pattern. Some domain knowledge are low-level, such as particular
    features and special areas where medical doctors pay more attention to [[31](#bib.bib31)].
    In particular, in disease diagnosis tasks, high-level domain knowledge is widely
    utilized. For an object detection task, the low-level domain knowledge, such as
    detection patterns and specific features where medical doctors give special attention
    is more commonly adopted. For lesion or organ segmentation tasks, anatomical priors
    and the knowledge from different modalities seem to be more useful [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在领域知识的类型方面，有些是高级的，例如训练模式[[28](#bib.bib28)、[29](#bib.bib29)、[30](#bib.bib30)]和诊断模式。一些领域知识则是低级的，例如特定特征和医疗医生更关注的特殊区域[[31](#bib.bib31)]。特别是在疾病诊断任务中，高级领域知识被广泛利用。对于目标检测任务，低级领域知识，如检测模式和医疗医生特别关注的具体特征，更为常见。对于病灶或器官分割任务，解剖学先验和来自不同模态的知识似乎更为有用[[32](#bib.bib32)、[33](#bib.bib33)、[34](#bib.bib34)]。
- en: '![Refer to caption](img/6cc738d2ce8301a87d9258d1f3c41477.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6cc738d2ce8301a87d9258d1f3c41477.png)'
- en: 'Figure 2: (a) Number of papers arranged chronically (2016-2020). (b) The distribution
    of selected papers in different applications of medical image analysis.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2： (a) 按时间顺序排列的论文数量（2016-2020）。 (b) 选择的论文在不同医学图像分析应用中的分布。
- en: In terms of the integrating methods, various approaches have been designed to
    incorporate different types of domain knowledge into networks [[35](#bib.bib35)].
    For example, a simple approach is to concatenate hand-crafted features with the
    ones extracted from deep learning models [[36](#bib.bib36)]. In some works, network
    architectures are revised to simulate the pattern of radiologists when they read
    images [[37](#bib.bib37)]. Attention mechanism, which allows a network to pay
    more attention to a certain region of an image, is a powerful technique to incorporate
    domain knowledge of radiologists [[38](#bib.bib38)]. In addition, multi-task learning
    and meta learning are also widely used to introduce medical domain knowledge into
    deep learning models [[39](#bib.bib39), [40](#bib.bib40)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在整合方法方面，设计了各种方法将不同类型的领域知识融入网络中[[35](#bib.bib35)]。例如，一种简单的方法是将手工制作的特征与从深度学习模型中提取的特征进行连接[[36](#bib.bib36)]。在一些工作中，网络架构被修改以模拟放射科医生在阅读图像时的模式[[37](#bib.bib37)]。注意机制，允许网络对图像的特定区域给予更多关注，是一种有效的技术，用于融入放射科医生的领域知识[[38](#bib.bib38)]。此外，多任务学习和元学习也被广泛用于将医学领域知识引入深度学习模型中[[39](#bib.bib39),
    [40](#bib.bib40)]。
- en: Although there are a number of reviews on deep learning for medical image analysis,
    including [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)],
    they all describe the existing works from the application point of view, i.e.,
    how deep learning techniques are applied to various medical applications. To the
    best of our knowledge, there is no review that gives systematic introduction on
    *how medical domain knowledge can help deep learning models*. This aspect, we
    believe, is the unique feature that distinguishes deep learning models for CAD
    from those for general computer vision tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有许多关于深度学习在医学图像分析中的综述，包括[[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]，但它们都从应用角度描述现有工作，即深度学习技术如何应用于各种医学应用。根据我们所知，目前没有综述系统性地介绍*医学领域知识如何帮助深度学习模型*。我们认为，这一方面是深度学习模型用于计算机辅助诊断（CAD）与用于一般计算机视觉任务的独特之处。
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") gives the overview on
    how we organize the related researches. At the top level, existing studies are
    classified into three main categories according to their purposes: (1) disease
    diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion and organ
    segmentation. In each category, we organize them into several groups based on
    the types of extra knowledge have been incorporated. At the bottom level, they
    are further categorized according to the different integrating approaches of the
    domain knowledge.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")概述了我们如何组织相关研究。在顶层，现有研究根据其目的被分为三个主要类别：（1）疾病诊断，（2）病灶、器官和异常检测，以及（3）病灶和器官分割。在每个类别中，我们根据融入的额外知识类型将其组织为几个组。在底层，根据领域知识的不同整合方法进一步分类。
- en: This survey contains more than 200 papers (163 are with domain knowledge), most
    of which are published recently (2016-2020), on a wide variety of applications
    of deep learning techniques for medical image analysis. In addition, most of the
    corresponding works are from the conference proceedings for MICCAI, EMBC, ISBI
    and some journals such as TMI, Medical Image Analysis, JBHI and so on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查包含了200多篇论文（其中163篇涉及领域知识），大多数是最近（2016-2020年）发表的，涉及深度学习技术在医学图像分析中的广泛应用。此外，大多数相关工作来自于MICCAI、EMBC、ISBI等会议论文集，以及TMI、Medical
    Image Analysis、JBHI等期刊。
- en: '![Refer to caption](img/afbdc1af2cfaab97cd40ba6f5c79756a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/afbdc1af2cfaab97cd40ba6f5c79756a.png)'
- en: 'Figure 3: The organizational structure of this survey.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：本调查的组织结构。
- en: 'The distribution of these papers are shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")(a). It can be seen that the number of papers increases
    rapidly from 2016 to 2020\. With respect to the applications, most of them are
    related to disease diagnosis and lesion/organ segmentation (shown in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")(b)). To sum up, with this survey we aim
    to:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文的分布如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")(a) 所示。从2016年到2020年，论文数量急剧增加。就应用而言，大多数论文与疾病诊断和病灶/器官分割相关（见图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")(b)）。总之，通过本次调查，我们的目标是：
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: summarize and classify different types of domain knowledge in medical areas
    that are utilized to improve the performance of deep learning models in various
    applications;
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结和分类在医学领域中用于提高深度学习模型在各种应用中的性能的不同类型的领域知识；
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: summarize and classify different ways of introducing medical domain knowledge
    into deep learning models;
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结和分类将医学领域知识引入深度学习模型的不同方式；
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: give the outlook of challenges and future directions in integrating medical
    domain knowledge into deep learning models.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍将医学领域知识整合到深度学习模型中的挑战和未来方向。
- en: The rest of the survey is organized as follows. Sections [2](#S2 "2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis"), [3](#S3 "3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and [4](#S4 "4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") introduce the
    existing works for the major three tasks in medical image analysis. Besides these
    three major tasks, other tasks in medical image analysis are described in Section
    [5](#S5 "5 Other Medical Applications ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"). In each section, we first introduce
    the general architectures of deep learning models for a task, and then categorize
    related works according to the types of the domain knowledge to be integrated.
    Various incorporating methods for each type of domain knowledge are then described.
    Section [6](#S6 "6 Research Challenges and Future Directions ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") discusses research
    challenges, and gives the outlook of future directions. Lastly, Section [7](#S7
    "7 Conclusion ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis") concludes this survey. The structure of this survey
    is shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的调查内容组织如下：第 [2](#S2 "2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis")、[3](#S3 "3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis") 和 [4](#S4 "4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") 节介绍了医学图像分析中三个主要任务的现有工作。除了这三个主要任务，医学图像分析中的其他任务在第 [5](#S5 "5 Other Medical
    Applications ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") 节中描述。在每一节中，我们首先介绍该任务的深度学习模型的通用架构，然后根据待整合的领域知识类型对相关工作进行分类。随后描述每种领域知识的各种整合方法。第
    [6](#S6 "6 Research Challenges and Future Directions ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") 节讨论了研究挑战，并展望未来方向。最后，第
    [7](#S7 "7 Conclusion ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis") 节总结了本次调查。调查的结构如图 [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") 所示。
- en: 2 Disease Diagnosis
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 疾病诊断
- en: Disease diagnosis refers to the task of determining the type and condition of
    possible diseases based on the medical images provided. In this section, we give
    an overview of the deep learning models that generally used for disease diagnosis.
    Concretely, subsection [2.1](#S2.SS1 "2.1 General Structures of Deep Learning
    Models Used for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") outlines the
    general structures of deep learning models used for disease diagnosis. Subsection
    [2.2](#S2.SS2 "2.2 Incorporating Knowledge from Natural Datasets or Other Medical
    Datasets ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into
    Deep Learning for Medical Image Analysis") introduces the works that utilize knowledge
    from natural images or other medical datasets. Deep learning models that leverage
    knowledge from medical doctors are introduced in Subsection [2.3](#S2.SS3 "2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")
    in detail. Lastly, Subsection [2.4](#S2.SS4 "2.4 Summary ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") summarizes the research of disease diagnosis.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 疾病诊断是指基于提供的医学图像确定可能的疾病类型和状态的任务。在本节中，我们概述了通常用于疾病诊断的深度学习模型。具体而言，[2.1](#S2.SS1
    "2.1 深度学习模型在疾病诊断中的一般结构 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节概述了用于疾病诊断的深度学习模型的一般结构。[2.2](#S2.SS2
    "2.2 从自然数据集或其他医学数据集中融入知识 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节介绍了利用自然图像或其他医学数据集中的知识的研究。利用医学专家知识的深度学习模型在
    [2.3](#S2.SS3 "2.3 从医学专家中融入知识 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节中详细介绍。最后，[2.4](#S2.SS4
    "2.4 总结 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查") 小节总结了疾病诊断的研究。
- en: 2.1 General Structures of Deep Learning Models Used for Disease Diagnosis
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度学习模型在疾病诊断中的一般结构
- en: In the last decades, deep learning techniques, especially CNNs, have achieved
    a great success in disease diagnosis.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，深度学习技术，特别是 CNNs，在疾病诊断中取得了巨大成功。
- en: Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 General Structures of Deep Learning Models Used
    for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") shows the structure
    of a typical CNN that used for disease diagnosis in chest X-ray image. The CNN
    employs alternating convolutional and pooling layers, and contains trainable filter
    banks per layer. Each individual filter in a filter bank is able to generate a
    feature map. This process is alternated and the CNN can learn increasingly more
    and more abstract features that will later be used by the fully connected layers
    to accomplish the classification task.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S2.F4 "图 4 ‣ 2.1 深度学习模型在疾病诊断中的一般结构 ‣ 2 疾病诊断 ‣ 关于将领域知识融入深度学习进行医学图像分析的调查")
    显示了用于胸部 X 光图像疾病诊断的典型 CNN 结构。该 CNN 采用交替的卷积层和池化层，并且每层包含可训练的滤波器组。每个滤波器组中的单个滤波器能够生成特征图。这个过程是交替进行的，CNN
    可以学习越来越抽象的特征，这些特征将被全连接层用于完成分类任务。
- en: '![Refer to caption](img/13d7097710f05c622a19c53a7a5401b0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13d7097710f05c622a19c53a7a5401b0.png)'
- en: 'Figure 4: A typical CNN architecture for medical disease diagnosis.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于医学疾病诊断的典型 CNN 结构。
- en: Different types of CNN architectures, from AlexNet [[45](#bib.bib45)], GoogLeNet
    [[46](#bib.bib46)], VGGNet [[47](#bib.bib47)], ResNet [[48](#bib.bib48)] to DenseNet
    [[49](#bib.bib49)], have achieved a great success in the diagnosis of various
    diseases. For example, GoogLeNet, ResNet, and VGGNet models are used in the diagnosis
    of canine ulcerative keratitis [[50](#bib.bib50)], and most of them achieve accuracies
    of over 90% when classifying superficial and deep corneal ulcers. DenseNet is
    adopted to diagnose lung nodules on chest X-ray radiograph [[51](#bib.bib51)],
    and experimental results show that more than 99% of lung nodules can be detected.
    In addition, it is found that VGGNet and ResNet are more effective than other
    network structures for many medical diagnostic tasks [[37](#bib.bib37), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从AlexNet [[45](#bib.bib45)]、GoogLeNet [[46](#bib.bib46)]、VGGNet [[47](#bib.bib47)]、ResNet
    [[48](#bib.bib48)] 到 DenseNet [[49](#bib.bib49)] 等不同类型的卷积神经网络（CNN）架构，在各种疾病的诊断中取得了巨大的成功。例如，GoogLeNet、ResNet
    和 VGGNet 模型被用于犬类溃疡性角膜炎的诊断 [[50](#bib.bib50)]，其中大多数在分类浅层和深层角膜溃疡时的准确率超过 90%。DenseNet
    被用于胸部 X 射线影像的肺结节诊断 [[51](#bib.bib51)]，实验结果显示超过 99% 的肺结节能够被检测到。此外，研究发现，VGGNet 和
    ResNet 在许多医学诊断任务中比其他网络结构更有效 [[37](#bib.bib37), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]。
- en: However, the above works generally directly apply CNNs to medical image analysis
    or slightly modified CNNs (e.g., by modifying the number of kernals, the number
    of channels or the size of filters), and no medical knowledge is incorporated.
    In addition, these methods generally require large medical datasets to achieve
    a satisfactory performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述工作通常直接将 CNN 应用于医学图像分析，或对 CNN 进行轻微修改（例如，修改卷积核数量、通道数量或滤波器大小），而未融入医学知识。此外，这些方法通常需要大量的医学数据集才能达到令人满意的性能。
- en: In the following subsections, we systematically review on the research that
    utilizes medical domain knowledge for the disease diagnosis. The types of knowledge
    and the incorporating methods are summarized in Table [II](#S2.T2 "TABLE II ‣
    2.1 General Structures of Deep Learning Models Used for Disease Diagnosis ‣ 2
    Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们系统地回顾了利用医学领域知识进行疾病诊断的研究。知识类型及其结合方法在表 [II](#S2.T2 "TABLE II ‣ 2.1
    General Structures of Deep Learning Models Used for Disease Diagnosis ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") 中进行了总结。
- en: 'TABLE II: A compilation of the knowledge and corresponding incorporating methods
    used in disease diagnosis'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 疾病诊断中使用的知识及其相应结合方法的汇编'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating Method &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合方法 &#124;'
- en: '| References |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[55](#bib.bib55)][[1](#bib.bib1)]  [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)]  [[59](#bib.bib59)]  [[60](#bib.bib60)][[61](#bib.bib61)]
    &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[55](#bib.bib55)][[1](#bib.bib1)]  [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)]  [[59](#bib.bib59)]  [[60](#bib.bib60)][[61](#bib.bib61)]
    &#124;'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical datasets |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 医学数据集 |'
- en: '&#124; Multi-modal images &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态图像 &#124;'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[26](#bib.bib26)][[62](#bib.bib62)]  [[63](#bib.bib63)][[64](#bib.bib64)][[65](#bib.bib65)][[66](#bib.bib66)]
    &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[26](#bib.bib26)][[62](#bib.bib62)]  [[63](#bib.bib63)][[64](#bib.bib64)][[65](#bib.bib65)][[66](#bib.bib66)]
    &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Datasets from other diseases &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自其他疾病的数据集 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[67](#bib.bib67)][[68](#bib.bib68)] &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[67](#bib.bib67)][[68](#bib.bib68)] &#124;'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical doctors |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 医学专家 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[28](#bib.bib28)][[29](#bib.bib29)]  [[69](#bib.bib69)][[70](#bib.bib70)][[71](#bib.bib71)]  [[72](#bib.bib72)][[73](#bib.bib73)][[74](#bib.bib74)]
    &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[28](#bib.bib28)][[29](#bib.bib29)]  [[69](#bib.bib69)][[70](#bib.bib70)][[71](#bib.bib71)]  [[72](#bib.bib72)][[73](#bib.bib73)][[74](#bib.bib74)]
    &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Diagnostic patterns &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 诊断模式 &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Network design &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络设计 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[37](#bib.bib37)][[75](#bib.bib75)]  [[76](#bib.bib76)][[77](#bib.bib77)]  [[78](#bib.bib78)][[79](#bib.bib79)]
    &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[37](#bib.bib37)][[75](#bib.bib75)]  [[76](#bib.bib76)][[77](#bib.bib77)]  [[78](#bib.bib78)][[79](#bib.bib79)]
    &#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Areas doctors focus on &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医生关注的领域 &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attention mechanism &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制 &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[38](#bib.bib38)]  [[53](#bib.bib53)]  [[80](#bib.bib80)][[81](#bib.bib81)]  [[82](#bib.bib82)][[83](#bib.bib83)]
    &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[38](#bib.bib38)]  [[53](#bib.bib53)]  [[80](#bib.bib80)][[81](#bib.bib81)]  [[82](#bib.bib82)][[83](#bib.bib83)]
    &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Features doctors focus on |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 医生关注的特征 |'
- en: '&#124; Decision level fusion &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 决策层融合 &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[61](#bib.bib61)]  [[84](#bib.bib84)]  [[85](#bib.bib85)][[86](#bib.bib86)]  [[87](#bib.bib87)][[88](#bib.bib88)]
    &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[61](#bib.bib61)]  [[84](#bib.bib84)]  [[85](#bib.bib85)][[86](#bib.bib86)]  [[87](#bib.bib87)][[88](#bib.bib88)]
    &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征层融合 &#124;'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[36](#bib.bib36)][[89](#bib.bib89)]  [[57](#bib.bib57)]  [[90](#bib.bib90)]  [[91](#bib.bib91)][[92](#bib.bib92)]
    &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[36](#bib.bib36)][[89](#bib.bib89)]  [[57](#bib.bib57)]  [[90](#bib.bib90)]  [[91](#bib.bib91)][[92](#bib.bib92)]
    &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input level fusion &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入层融合 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[93](#bib.bib93)]  [[54](#bib.bib54)]  [[94](#bib.bib94)][[95](#bib.bib95)]  [[96](#bib.bib96)]
    &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[93](#bib.bib93)]  [[54](#bib.bib54)]  [[94](#bib.bib94)][[95](#bib.bib95)]  [[96](#bib.bib96)]
    &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; As labels of CNNs &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN标签 &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[39](#bib.bib39)][[59](#bib.bib59)]  [[97](#bib.bib97)] &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[39](#bib.bib39)][[59](#bib.bib59)]  [[97](#bib.bib97)] &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other related information &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他相关信息 &#124;'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '&#124; /network design &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; /网络设计 &#124;'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[52](#bib.bib52)]  [[98](#bib.bib98)]  [[99](#bib.bib99)][[100](#bib.bib100)]  [[101](#bib.bib101)]
    &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[52](#bib.bib52)]  [[98](#bib.bib98)]  [[99](#bib.bib99)][[100](#bib.bib100)]  [[101](#bib.bib101)]
    &#124;'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 2.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 从自然数据集或其他医疗数据集中融入知识
- en: Despite the disparity between natural and medical images, it has been demonstrated
    that CNNs comprehensively trained on the large-scale well-annotated natural image
    datasets can still be helpful for disease diagnosis tasks [[56](#bib.bib56)].
    Intrinsically speaking, this transfer learning process introduces knowledge from
    natural images into the network for medical image diagnosis.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自然图像和医疗图像之间存在差异，但已经证明在大规模、标注良好的自然图像数据集上全面训练的CNN仍对疾病诊断任务有帮助[[56](#bib.bib56)]。从本质上讲，这种迁移学习过程将自然图像中的知识引入网络，用于医疗图像诊断。
- en: 'According to [[42](#bib.bib42)], the networks pre-trained on natural images
    can be leveraged via two different ways: by utilizing them as fixed feature extractors,
    and as an initialization which will then be fine-tuned on target medical datasets.
    These two strategies are illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating
    Knowledge from Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")(a) and Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating Knowledge from
    Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis ‣ A Survey on
    Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")(b),
    respectively.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[42](#bib.bib42)]，可以通过两种不同方式利用在自然图像上预训练的网络：作为固定特征提取器，或作为初始化，然后在目标医疗数据集上进行微调。这两种策略分别在图
    [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating Knowledge from Natural Datasets or Other
    Medical Datasets ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")(a) 和图 [5](#S2.F5 "Figure 5 ‣ 2.2
    Incorporating Knowledge from Natural Datasets or Other Medical Datasets ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")(b) 中展示。
- en: '![Refer to caption](img/74fb2fd2d81cd8d79d45403b8cddcb93.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74fb2fd2d81cd8d79d45403b8cddcb93.png)'
- en: 'Figure 5: Two strategies to utilize the pre-trained network on natural images:
    (a) as a feature extractor and (b) as an initialization which will be fine-tuned
    on the target dataset.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 利用在自然图像上预训练的网络的两种策略：(a) 作为特征提取器，(b) 作为初始化，在目标数据集上进行微调。'
- en: The first strategy takes a pre-trained network, removes its last fully-connected
    layer, and treats the rest of the network as a fixed feature extractor. Extracted
    features are then fed into a linear classifier (e.g., support vector machine (SVM)),
    which is trained on the target medical datasets. Applications in this category
    include mammography mass lesion classification [[61](#bib.bib61)] and chest pathology
    identification [[55](#bib.bib55)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个策略是取一个预训练的网络，移除其最后一个全连接层，并将其余部分作为固定特征提取器。提取的特征然后输入到线性分类器（如支持向量机（SVM））中，该分类器在目标医疗数据集上进行训练。此类应用包括乳腺X光检查肿块分类[[61](#bib.bib61)]和胸部病理识别[[55](#bib.bib55)]。
- en: The success of leveraging information from natural images for disease diagnosis
    can be attributed to the fact that a network pre-trained on natural images, especially
    in the earlier layers, contain more generic features (e.g., edge detectors and
    color blob detectors) [[102](#bib.bib102)].
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 利用自然图像信息进行疾病诊断的成功可以归因于网络在自然图像上预训练的事实，特别是在早期层中，包含了更多的通用特征（例如，边缘检测器和颜色斑块检测器）[[102](#bib.bib102)]。
- en: In the second strategy, the weights of the pre-trained network are fine-tuned
    based on the medical datasets. It is possible to fine-tune the weights of all
    layers in the network, or to keep some of the earlier layers fixed and only fine-tune
    some higher-level portion of the network. This can be applied to the classification
    of skin cancer [[1](#bib.bib1)], breast cancer [[57](#bib.bib57)], thorax diseases
    [[58](#bib.bib58)], prostate cancer [[60](#bib.bib60)] and interstitial lung diseases
    [[59](#bib.bib59)] .
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种策略中，预训练网络的权重根据医疗数据集进行微调。可以微调网络中所有层的权重，或者保持一些早期层不变，只微调网络中某些高级部分。这可以应用于皮肤癌[[1](#bib.bib1)]、乳腺癌[[57](#bib.bib57)]、胸部疾病[[58](#bib.bib58)]、前列腺癌[[60](#bib.bib60)]和间质性肺疾病[[59](#bib.bib59)]的分类。
- en: Besides the information from natural images, using images from other medical
    datasets is also quite popular.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自然图像的信息，使用来自其他医疗数据集的图像也相当流行。
- en: Medical datasets containing images of the same or similar modality as target
    images have similar distribution and therefore can be helpful. For example, to
    classify malignant and benign breast masses in digitized screen-film mammograms
    (SFMs), a multi-task transfer learning DCNN is proposed to incorporate the information
    from digital mammograms (DMs) [[63](#bib.bib63)]. It is found to have significantly
    higher performance compared to the single-task transfer learning DCNN which only
    utilizes SFMs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 含有与目标图像相同或相似模态的图像的医疗数据集具有相似的分布，因此可以提供帮助。例如，为了在数字化的乳腺X光筛查图像（SFM）中分类恶性和良性乳腺肿块，提出了一种多任务迁移学习的DCNN，该模型结合了数字乳腺图像（DM）的信息[[63](#bib.bib63)]。与仅使用SFM的单任务迁移学习DCNN相比，发现其性能显著提高。
- en: In addition, even medical images with different modalities can provide complementary
    information. For example, [[26](#bib.bib26)] uses a model pre-trained on a mammography
    dataset to show that it could obtain better results than models trained solely
    on the target dataset comprising digital breast tomosynthesis (DBT) images. Another
    example is in prostate cancer classification, where the radiofrequency ultrasound
    images are first used to train the DCNN, then the model is fine-tuned on B-mode
    ultrasound images [[64](#bib.bib64)]. Other examples of using the images from
    different modalities can be found in [[62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使是不同模态的医疗图像也可以提供互补信息。例如，[[26](#bib.bib26)]使用在乳腺X光检查数据集上预训练的模型，展示了该模型能够获得比仅在包含数字乳腺断层摄影（DBT）图像的目标数据集上训练的模型更好的结果。另一个例子是在前列腺癌分类中，首先使用射频超声图像训练深度卷积神经网络（DCNN），然后在B模式超声图像上微调模型[[64](#bib.bib64)]。使用不同模态图像的其他例子可以在[[62](#bib.bib62)、[65](#bib.bib65)、[66](#bib.bib66)]中找到。
- en: Furthermore, as datasets of different classes can help each other in classification
    tasks [[103](#bib.bib103)], medical datasets featuring images of a variety of
    diseases can also have similar morphological structures or distribution, which
    may be beneficial for other tasks. For example, a multi-task deep learning (MTDL)
    method is proposed in [[68](#bib.bib68)]. MTDL can simultaneously utilize multiple
    cancer datasets so that hidden representations among these datasets can provide
    more information to small-scale cancer datasets, and enhance the classification
    performance. Another example is a cross-disease attention network (CANet) proposed
    in [[67](#bib.bib67)]. CANet characterizes and leverages the relationship between
    diabetic retinopathy (DR) and diabetic macular edema (DME) in fundus images using
    a special designed disease-dependent attention module. Experimental results on
    two public datasets show that CANet outperforms other methods on diagnosing both
    of the two diseases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于不同类别的数据集可以在分类任务中互相帮助[[103](#bib.bib103)]，具有多种疾病图像的医学数据集也可能具有类似的形态结构或分布，这对其他任务可能是有益的。例如，[[68](#bib.bib68)]中提出了一种多任务深度学习（MTDL）方法。MTDL可以同时利用多个癌症数据集，以便这些数据集之间的隐藏表示可以为小规模癌症数据集提供更多信息，从而增强分类性能。另一个例子是[[67](#bib.bib67)]中提出的跨疾病注意网络（CANet）。CANet利用特殊设计的疾病相关注意模块来表征和利用糖尿病视网膜病变（DR）和糖尿病黄斑水肿（DME）在眼底图像中的关系。对两个公开数据集的实验结果表明，CANet在诊断这两种疾病方面优于其他方法。
- en: 2.3 Incorporating Knowledge from Medical Doctors
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 融入医生知识
- en: Experienced medical doctors can give fairly accurate conclusion on the given
    medical images, mainly thanks to the training they have received and the expertise
    they have accumulated over many years. In general, they often follow some certain
    patterns or take some procedures when reading medical images. Incorporating these
    types of knowledge can improve the diagnostic performance of deep learning models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 经验丰富的医生可以对给定的医学图像给出相当准确的结论，这主要得益于他们所接受的培训和多年来积累的专业知识。一般来说，他们在阅读医学图像时常常遵循某些特定的模式或采取某些程序。将这些类型的知识融入深度学习模型中，可以提高诊断性能。
- en: 'The types of medical domain knowledge utilized in deep learning models for
    disease diagnosis can be summarized into the following five categories:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 用于疾病诊断的深度学习模型中利用的医学领域知识可以总结为以下五类：
- en: '1.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 培训模式，
- en: '2.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general diagnostic patterns they view images,
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像的一般诊断模式，
- en: '3.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the areas on which they usually focus,
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们通常关注的领域，
- en: '4.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: the features (e.g., characteristics, structures, shapes) they give special attention
    to, and
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们特别关注的特征（例如，特性、结构、形状），以及
- en: '5.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: other related information for diagnosis.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他与诊断相关的信息。
- en: The research works for each category will be described in the following sections.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每一类的研究工作将在以下部分中描述。
- en: 2.3.1 Training Pattern of Medical Doctors
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 医生的培训模式
- en: 'The training process of medical students has a character: they are trained
    by tasks with increasing difficulty. For example, students begin with some easier
    tasks, such as deciding whether an image contains lesions, later are required
    to accomplish more challenging tasks, such as determining whether the lesions
    are benign or malignant. Over time, they will advance to more difficult tasks,
    such as determining the subtypes of lesions in images.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 医学生的培训过程具有一个特点：他们通过逐渐增加难度的任务进行训练。例如，学生们从一些较简单的任务开始，比如判断图像中是否有病变，然后逐步要求完成更具挑战性的任务，如判断病变是良性还是恶性。随着时间的推移，他们将会逐渐进入更难的任务，如判断图像中病变的亚型。
- en: This training pattern can be introduced in the training process of deep neural
    networks via curriculum learning [[104](#bib.bib104)]. Curriculum determines a
    sequence of training samples ranked in ascending order of learning difficulty.
    Curriculum learning has been an active research topic in computer vision and has
    been recently utilized for medical image diagnosis.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这种培训模式可以通过课程学习[[104](#bib.bib104)]引入深度神经网络的训练过程中。课程学习决定了一系列按学习难度递增排序的训练样本。课程学习已成为计算机视觉领域的一个活跃研究课题，并且最近被用于医学图像诊断。
- en: For example, a teacher-student curriculum learning strategy is proposed for
    breast screening classification from DCE-MRI [[28](#bib.bib28)]. The deep learning
    model is trained on simpler tasks before introducing the hard problem of malignancy
    detection. This strategy shows the better performance when compared with the other
    methods.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，针对DCE-MRI的乳腺筛查分类，提出了一种教师-学生课程学习策略 [[28](#bib.bib28)]。在引入恶性肿瘤检测的困难问题之前，深度学习模型在较简单的任务上进行训练。这一策略与其他方法相比，表现出了更好的性能。
- en: Similarly, [[29](#bib.bib29)] presents a CNN based attention-guided curriculum
    learning framework by leveraging the severity-level attributes mined from radiology
    reports. Images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN to boost the learning process gradually.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，[[29](#bib.bib29)] 通过利用从放射学报告中挖掘的严重程度属性，提出了一种基于CNN的注意力引导课程学习框架。按照难度（按不同的严重程度分组）排序的图像被输入到CNN中，以逐步提升学习过程。
- en: In [[69](#bib.bib69)], the curriculum learning is adopted to support the classification
    of proximal femur fracture from X-ray images. The approach assigns a degree of
    difficulty to each training sample. By first learning ‘easy’ examples and then
    ‘hard’ ones, the model can reach a better performance even with fewer data. Other
    examples of using curriculum learning for disease diagnosis can be found in [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)].
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[69](#bib.bib69)] 中，课程学习被用于支持从X光图像中分类近端股骨骨折。这种方法为每个训练样本分配了难度等级。通过首先学习“简单”示例，然后学习“困难”示例，即使数据量较少，模型也能达到更好的性能。其他使用课程学习进行疾病诊断的例子可以在
    [[70](#bib.bib70)、[71](#bib.bib71)、[72](#bib.bib72)、[73](#bib.bib73)、[74](#bib.bib74)]
    中找到。
- en: 2.3.2 General Diagnostic Patterns of Medical Doctors
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 医生的一般诊断模式
- en: Experienced medical doctors generally follow some patterns when they read medical
    images. These patterns can be integrated into deep learning models with appropriately
    modified architectures.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的医生在阅读医学图像时通常遵循一些模式。这些模式可以通过适当修改架构将其集成到深度学习模型中。
- en: 'For example, radiologists generally follow a three-staged approach when they
    read chest X-ray images: first browsing the whole image, then concentrating on
    the local lesion areas, and finally combining the global and local information
    to make decisions. This pattern is incorporated in the architecture design of
    the network for thorax disease classification [[37](#bib.bib37)] (see Fig. [6](#S2.F6
    "Figure 6 ‣ 2.3.2 General Diagnostic Patterns of Medical Doctors ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")). The proposed
    network has three branches, one is used to view the whole image, the second for
    viewing the local areas, and the third one for combining the global and local
    information together. The network yields state-of-the-art accuracy on the ChestX-ray14
    dataset. In addition, besides the information from the whole image and local lesion
    area, the information from lung area is also leveraged in [[76](#bib.bib76)].
    In particular, a segmentation subnetwork is first used to locate the lung area
    from the whole image, and then lesion areas are generated by using an attention
    heatmap. Finally, the most discriminative features are fused for final disease
    prediction. Another example is a Dual-Ray Net proposed to deal with the front
    and lateral chest radiography simultaneously [[77](#bib.bib77)], which also mimics
    the reading pattern of radiologists.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，放射科医生在阅读胸部X光图像时通常遵循三阶段的方法：首先浏览整个图像，然后集中注意局部病变区域，最后将全局和局部信息结合起来做出决策。这种模式被纳入了用于胸部疾病分类的网络架构设计中
    [[37](#bib.bib37)]（参见图 [6](#S2.F6 "图 6 ‣ 2.3.2 医生的一般诊断模式 ‣ 2.3 融入医生知识 ‣ 2 疾病诊断
    ‣ 深度学习在医学图像分析中的领域知识融入综述")）。所提出的网络有三个分支，一个用于查看整个图像，第二个用于查看局部区域，第三个用于将全局和局部信息结合起来。该网络在ChestX-ray14数据集上达到了最先进的准确率。此外，除了全图和局部病变区域的信息外，[[76](#bib.bib76)]
    还利用了肺部区域的信息。特别地，首先使用分割子网络从整个图像中定位肺部区域，然后通过使用注意力热图生成病变区域。最后，融合最具区分性的特征以进行最终的疾病预测。另一个例子是为同时处理前面和侧面胸部X光片而提出的Dual-Ray
    Net [[77](#bib.bib77)]，它也模拟了放射科医生的阅读模式。
- en: '![Refer to caption](img/63db055ed18834f9dc2592a9c8e11944.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/63db055ed18834f9dc2592a9c8e11944.png)'
- en: 'Figure 6: The example of leveraging the diagnostic pattern of radiologists
    for thorax disease diagnosis [[37](#bib.bib37)], where three branches are used
    to extract and combine the global and local features.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：利用放射科医生的诊断模式进行胸部疾病诊断的示例[[37](#bib.bib37)]，其中使用三个分支提取和结合全局与局部特征。
- en: 'In the diagnosis of skin lesions, experienced dermatologists generally first
    locate lesions, then identify dermoscopic features from the lesion areas, and
    finally make diagnosis based on the features. This pattern is mimicked in the
    design of the network for the diagnosis of skin lesions [[75](#bib.bib75)]. The
    proposed network, DermaKNet, comprised several subnetworks with dedicated tasks:
    lesion-skin segmentation, detection of dermoscopic features, and global lesion
    diagnosis. The DermaKNet achieves higher performance compared to the traditional
    CNN models.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在皮肤病变诊断中，经验丰富的皮肤科医生通常首先定位病变，然后从病变区域识别皮肤镜特征，最后根据这些特征进行诊断。这种模式在皮肤病变诊断的网络设计中得到了模拟[[75](#bib.bib75)]。所提出的网络DermaKNet包括几个具有专门任务的子网络：病变-皮肤分割、皮肤镜特征检测和全局病变诊断。DermaKNet相比传统的卷积神经网络模型表现出更高的性能。
- en: In addition, in mass identification in mammogram, radiologists generally analyze
    the bilateral and ipsilateral views simultaneously. To emulate this reading practice,
    [[78](#bib.bib78)] proposes MommiNet to perform end-to-end bilateral and ipsilateral
    analysis of mammogram images. In addition, symmetry and geometry constraints are
    also aggregated from these views. Experiments show the state-of-the-art mass identification
    accuracy on DDSM. Another example of leveraging this diagnostic pattern of medical
    doctors can be found in skin lesion diagnosis and thorax disease classification
    [[79](#bib.bib79)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在乳腺X光图像的质量识别中，放射科医生通常会同时分析双侧和同侧视图。为了模拟这种阅读习惯，[[78](#bib.bib78)]提出了MommiNet来进行乳腺X光图像的端到端双侧和同侧分析。此外，还从这些视图中聚合了对称性和几何约束。实验表明在DDSM上达到了最先进的肿块识别准确率。另一个利用这种医疗医生诊断模式的示例可以在皮肤病变诊断和胸部疾病分类中找到[[79](#bib.bib79)]。
- en: 2.3.3 The Areas Medical Doctors Usually Focus on
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 医疗医生通常关注的区域
- en: When experienced medical doctors read images, they generally focus on a few
    specific areas, as these areas are more informative than other places for the
    purpose of disease diagnosis. Therefore, the information about where medical doctors
    focus may help deep learning models yield better results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当经验丰富的医生阅读图像时，他们通常会集中注意于几个特定区域，因为这些区域在疾病诊断中比其他地方更具信息性。因此，关于医生关注区域的信息可能有助于深度学习模型取得更好的结果。
- en: The knowledge above is generally represented as ‘attention maps’, which are
    annotations given by medical doctors indicating the areas they focus on when reading
    images. For example, a CNN named AG-CNN explicitly incorporates the ‘attention
    maps’ for glaucoma diagnosis [[38](#bib.bib38)]. The attention maps of ophthalmologists
    are collected through a simulated eye-tracking experiment, which are used to indicate
    where they focus when reading images. An example of capturing the attention maps
    of an ophthalmologist in glaucoma diagnosis is shown in Fig. [7](#S2.F7 "Figure
    7 ‣ 2.3.3 The Areas Medical Doctors Usually Focus on ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). To incorporate the
    attention maps, an attention prediction subnet in AG-CNN is designed, and the
    attention prediction loss measuring the difference between the generated and ground
    truth attention maps (provided by ophthalmologists) is utilized to supervise the
    training process. Experimental results show that AG-CNN significantly outperforms
    the state-of-the-art glaucoma detection methods.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述知识通常被表示为“注意力图”，这是医疗医生在阅读图像时指出他们关注区域的注释。例如，一个名为AG-CNN的卷积神经网络明确地融入了用于青光眼诊断的“注意力图”[[38](#bib.bib38)]。眼科医生的注意力图通过模拟眼动实验收集，用于指示他们在阅读图像时关注的地方。青光眼诊断中捕捉眼科医生注意力图的一个示例见图[7](#S2.F7
    "Figure 7 ‣ 2.3.3 The Areas Medical Doctors Usually Focus on ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")。为了融入注意力图，AG-CNN中设计了一个注意力预测子网，并利用测量生成的和真实注意力图（由眼科医生提供）之间差异的注意力预测损失来监督训练过程。实验结果表明，AG-CNN显著超越了最先进的青光眼检测方法。
- en: '![Refer to caption](img/63abb68645f11e9014fbdd75d5005420.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/63abb68645f11e9014fbdd75d5005420.png)'
- en: 'Figure 7: Example of capturing the attention maps of an ophthalmologist in
    glaucoma diagnosis [[38](#bib.bib38)]. I, II, III and IV are the original blurred
    fundus image, the fixations of ophthalmologists with cleared regions, the order
    of clearing the blurred regions, and the generated attention map based on the
    captured fixations, respectively. V and VII represent the original fundus images.
    VI and VIII are the corresponding attention maps of V and VII generated by using
    the method in I-IV.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：眼科医生在青光眼诊断中捕捉注意力图的示例[[38](#bib.bib38)]。I、II、III和IV分别为原始模糊视网膜图像、眼科医生的注视区域、清晰区域的顺序以及基于捕获的注视生成的注意力图。V和VII表示原始视网膜图像。VI和VIII是使用I-IV方法生成的V和VII的相应注意力图。
- en: Another example in this category is the lesion-aware CNN (LACNN) for the classification
    of retinal optical coherence tomography (OCT) images [[80](#bib.bib80)]. The LACNN
    simulates the pattern of ophthalmologists’ diagnosis by focusing on local lesion-related
    regions. Concretely, the ‘attention maps’ are firstly represented as the annotated
    OCT images delineating the lesion regions using bounding polygons. To incorporate
    the information, the LACNN proposes a lesion-attention module to enhance the features
    from local lesion-related regions while still preserving the meaningful structures
    in global OCT images. Experimental results on two clinically acquired OCT datasets
    demonstrate the effectiveness of introducing attention maps for retinal OCT image
    classification, with 8.3% performance gain when compared with the baseline method.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类别中的另一个例子是用于视网膜光学相干断层扫描（OCT）图像分类的病灶感知卷积神经网络（LACNN）[[80](#bib.bib80)]。LACNN通过关注局部病灶相关区域，模拟眼科医生的诊断模式。具体来说，‘注意力图’首先表示为标注了病灶区域的OCT图像，使用了边界多边形。为了融入这些信息，LACNN提出了一个病灶注意力模块，以增强来自局部病灶相关区域的特征，同时保持全局OCT图像中的有意义结构。对两个临床获得的OCT数据集的实验结果表明，引入注意力图在视网膜OCT图像分类中具有有效性，与基线方法相比，性能提升了8.3%。
- en: Furthermore, [[53](#bib.bib53)] proposes an Attention Branch Network (ABN) to
    incorporate the knowledge given by the radiologists in diabetic retinopathy. ABN
    introduces a branch structure which generates attention maps that highlight the
    attention regions of the network. During the training process, ABN allows the
    attention maps to be modified with semantic segmentation labels of disease regions.
    The semantic labels are also annotated by radiologists as the ground truth attention
    maps. Experimental results on the disease grade recognition of retina images show
    that ABN achieves 93.73% classification accuracy and its interpretability is clearer
    than conventional approaches.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[53](#bib.bib53)]提出了一种注意力分支网络（ABN），以融入放射科医生在糖尿病视网膜病变中的知识。ABN引入了一个分支结构，生成突出网络注意区域的注意力图。在训练过程中，ABN允许将注意力图与疾病区域的语义分割标签进行修改。语义标签也由放射科医生注释为真实的注意力图。对视网膜图像疾病等级识别的实验结果表明，ABN实现了93.73%的分类准确率，其可解释性比传统方法更清晰。
- en: Other examples of incorporating attention maps of medical doctors can be found
    in the diagnosis of radiotherapy-related esophageal fistula [[81](#bib.bib81)],
    breast cancer diagnosis [[82](#bib.bib82)], and short-term lesion change detection
    in melanoma screening [[83](#bib.bib83)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其他将医学专家的注意力图纳入诊断的例子包括放射治疗相关食管瘘的诊断[[81](#bib.bib81)]、乳腺癌诊断[[82](#bib.bib82)]以及黑色素瘤筛查中的短期病灶变化检测[[83](#bib.bib83)]。
- en: 2.3.4 Features That Medical Doctors Give Special Attention to
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 医学专家特别关注的特征
- en: In the last decades, many guidelines and rules have gradually developed in various
    medical fields to point out some important features for diagnosis. These features
    are called *‘hand-crafted features’* as they are designated by medical doctors.
    For example, the popular ABCD rule [[105](#bib.bib105)] is widely used by dermatologists
    to classify melanocytic tumors. The ABCD rule points out four distinguishing features,
    namely asymmetry, border, color and differential structures, to determine whether
    a melanocytic skin lesion under the investigation is benign or malignant.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，各种医学领域逐渐制定了许多指南和规则，以指出一些重要的诊断特征。这些特征被称为*‘手工特征’*，因为它们是由医学专家指定的。例如，广泛使用的ABCD规则[[105](#bib.bib105)]被皮肤科医生用来分类黑色素瘤。ABCD规则指出了四个区分特征，即不对称性、边界、颜色和差异结构，以确定待检查的黑色素瘤皮肤病变是良性还是恶性。
- en: 'TABLE III: Features in the BI-RADS guideline to classify benign and malignant
    breast tumors in ultrasound images [[106](#bib.bib106)]'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：BI-RADS指南中用于分类乳腺超声图像中良性和恶性肿瘤的特征[[106](#bib.bib106)]
- en: '|  | Benign | Malignant |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 良性 | 恶性 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Margin | smooth, thin, regular | irregular, thick |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 边缘 | 光滑、细、规则 | 不规则、粗 |'
- en: '| Shape | round or oval | irregular |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 形状 | 圆形或椭圆形 | 不规则 |'
- en: '| Microcalcification | no | yes |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 微钙化 | 无 | 有 |'
- en: '| Echo Pattern | clear | unclear |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 回声模式 | 清晰 | 不清晰 |'
- en: '| Acoustic Attenuation | not obvious | obvious |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 声学衰减 | 不明显 | 明显 |'
- en: '| Side Acoustic Shadow | obvious | not obvious |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 侧声学阴影 | 明显 | 不明显 |'
- en: Another example is in the field of breast cancer diagnosis. Radiologists use
    the BI-RADS (Breast Imaging Reporting and Data System) score [[106](#bib.bib106)]
    to place abnormal findings into different categories, with a score of 1 indicating
    negative findings and a score of 6 indicating breast cancer. More importantly,
    for each imaging modality, BI-RADS indicates some features closely related to
    its scores, including margin, shape, micro-calcification, and echo pattern. For
    example, for breast ultrasound images, tumors with smooth, thin and regular margins
    are more likely to be benign, while tumors with irregular and thick margins are
    highly suspected to be malignant. Other features that can help to classify benign
    and malignant breast tumors are shown in Table [III](#S2.T3 "TABLE III ‣ 2.3.4
    Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). Similarly, for the
    benign-malignant risk assessment of lung nodules in [[59](#bib.bib59)], six high-level
    nodule features, including calcification, sphericity, margin, lobulation, spiculation
    and texture, have shown a tightly connection with malignancy scores (see Fig.
    [8](#S2.F8 "Figure 8 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是乳腺癌诊断领域。放射科医生使用BI-RADS（乳腺影像报告和数据系统）评分[[106](#bib.bib106)]将异常发现分为不同类别，其中评分1表示阴性发现，评分6表示乳腺癌。更重要的是，对于每种成像方式，BI-RADS指示与其评分密切相关的一些特征，包括边缘、形状、微钙化和回声模式。例如，对于乳腺超声图像，具有光滑、细且规则边缘的肿瘤更可能是良性的，而具有不规则和粗边缘的肿瘤则高度怀疑为恶性。其他可以帮助分类良性和恶性乳腺肿瘤的特征见表[III](#S2.T3
    "TABLE III ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")。类似地，对于[[59](#bib.bib59)]中肺结节的良恶性风险评估，六种高级结节特征，包括钙化、球形度、边缘、分叶、刺状和质地，显示出与恶性评分的紧密关联（见图[8](#S2.F8
    "Figure 8 ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")）。
- en: '![Refer to caption](img/26456a4a8677eac2c4f9c3d2f1c2d605.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/26456a4a8677eac2c4f9c3d2f1c2d605.png)'
- en: 'Figure 8: Lung nodule attributes with different malignancy scores [[59](#bib.bib59)].(a)
    From top to the bottom, six different nodule features attribute from missing to
    highest prominence. (b) The number of nodules with different malignancy scores.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同恶性评分的肺结节特征[[59](#bib.bib59)]。(a) 从上到下，六种不同的结节特征从缺失到最高突出性。(b) 不同恶性评分的结节数量。
- en: These different kinds of hand-crafted features have been widely used in many
    traditional CAD systems. These systems generally first extract these features
    from medical images, and then feed them into some classifiers like SVM or Random
    Forest [[107](#bib.bib107), [108](#bib.bib108)]. For example, for the lung nodule
    classification on CT images, many CAD systems utilize features including the size,
    shape, morphology, and texture from the suspected lesion areas [[109](#bib.bib109),
    [54](#bib.bib54)]. Similarly, in the CAD systems for the diagnosis of breast ultrasound
    images, features such as intensity, texture and shape are used [[31](#bib.bib31)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同类型的手工特征已被广泛应用于许多传统的CAD系统。这些系统通常首先从医学图像中提取这些特征，然后将其输入一些分类器，如SVM或随机森林[[107](#bib.bib107),
    [108](#bib.bib108)]。例如，对于CT图像上的肺结节分类，许多CAD系统利用包括大小、形状、形态和质地等特征来处理可疑病变区域[[109](#bib.bib109),
    [54](#bib.bib54)]。类似地，在乳腺超声图像的CAD系统中，使用了强度、质地和形状等特征[[31](#bib.bib31)]。
- en: When using deep learning models like CNNs, which have the ability to automatically
    extract representative features, there are four approaches to combining ‘hand-crafted
    features’ with features extracted from CNNs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用具有自动提取代表性特征能力的深度学习模型（如CNN）时，有四种方法将“手工设计的特征”与CNN提取的特征结合起来。
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Decision-level fusion: the two types of features are fed into two classifiers
    separately, then the decisions from the two classifiers are combined.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策级融合：这两种特征分别输入到两个分类器中，然后将两个分类器的决策进行结合。
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature-level fusion: the two types of features are directly combined via techniques
    such as concatenation.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征级融合：这两种特征通过连接等技术直接组合。
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input-level fusion: the hand-crafted features are represented as image patches,
    which are then taken as inputs to the CNNs.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入级融合：手工设计的特征被表示为图像补丁，然后作为输入提供给CNN。
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Usage of features as labels of CNNs: the hand-crafted features are first annotated
    and then utilized as labels for CNNs during the training process.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征作为CNN标签的使用：手工设计的特征首先被标注，然后在训练过程中作为CNN的标签使用。
- en: 'Decision-level fusion: The structure of this approach is illustrated in Fig.
    [9](#S2.F9 "Figure 9 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"). In this approach, the hand-crafted features and the features extracted
    from CNNs are separately fed into two classifiers. Then, the classification results
    from both classifiers are combined using some decision fusion techniques to produce
    final classification results.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 决策级融合：这种方法的结构如图[9](#S2.F9 "Figure 9 ‣ 2.3.4 Features That Medical Doctors Give
    Special Attention to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")所示。在这种方法中，手工设计的特征和从CNN提取的特征分别输入到两个分类器中。然后，使用一些决策融合技术将两个分类器的分类结果结合起来，以生成最终的分类结果。
- en: '![Refer to caption](img/4551553557c5bd87c2bdb9ec55a257d3.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/4551553557c5bd87c2bdb9ec55a257d3.png)'
- en: 'Figure 9: Decision-level fusion: the decisions from two classifiers, one based
    on hand-crafted features, and the other on the CNNs, are combined.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：决策级融合：来自两个分类器的决策，一个基于手工设计的特征，另一个基于CNN，进行组合。
- en: For example, a skin lesion classification model proposed in [[85](#bib.bib85)]
    combines the results from two SVM classifiers. The first one uses hand-crafted
    features (i.e., RSurf features and local binary patterns (LBP)) as input and the
    second employs features derived from a CNN. Both of the classifiers predict the
    category for each tested image with a classification score. These scores are subsequently
    used to determine the final classification result.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[85](#bib.bib85)]中提出的皮肤病变分类模型结合了两个SVM分类器的结果。第一个使用手工设计的特征（即RSurf特征和局部二值模式（LBP））作为输入，第二个使用从CNN提取的特征。这两个分类器都为每个测试图像预测类别并给出分类得分。这些得分随后用于确定最终分类结果。
- en: Similarly, a mammographic tumor classification method also combines features
    in decision-level fusion [[61](#bib.bib61)]. After individually performing classification
    with CNN features and analytically extracted features (e.g., contrast, texture,
    and margin spiculation), the method adopts the soft voting to combine the outputs
    from both individual classifiers. Experimental results show that the performance
    of the ensemble classifier was significantly better than the individual ones.
    Other examples that utilize this approach include lung nodule diagnosis [[86](#bib.bib86)],
    breast cancer diagnosis [[87](#bib.bib87)], the classification of cardiac slices
    [[84](#bib.bib84)] and the prediction of the invasiveness risk of stage-I lung
    adenocarcinomas [[88](#bib.bib88)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，乳腺肿瘤分类方法也在决策级融合中结合特征[[61](#bib.bib61)]。在分别使用CNN特征和分析提取的特征（如对比度、纹理和边缘刺状）进行分类后，该方法采用软投票来结合两个独立分类器的输出。实验结果表明，集成分类器的性能显著优于单个分类器。其他采用这种方法的例子包括肺结节诊断[[86](#bib.bib86)]、乳腺癌诊断[[87](#bib.bib87)]、心脏切片分类[[84](#bib.bib84)]以及I期肺腺癌侵袭风险预测[[88](#bib.bib88)]。
- en: 'Feature-level fusion: In this approach, hand-crafted features and features
    extracted from CNNs are concatenated, and the combined features are fed into a
    classifier for diagnosis. The structure of this approach is illustrated in Fig.
    [10](#S2.F10 "Figure 10 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 特征级融合：在这种方法中，将手工设计的特征与从CNN中提取的特征进行拼接，然后将组合特征输入到分类器中进行诊断。该方法的结构如图 [10](#S2.F10
    "图 10 ‣ 2.3.4 医学医生特别关注的特征 ‣ 2.3 融合医学医生的知识 ‣ 2 疾病诊断 ‣ 融合领域知识到深度学习中的调查") 所示。
- en: '![Refer to caption](img/ee039724171545db44e50f3ba608fd97.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee039724171545db44e50f3ba608fd97.png)'
- en: 'Figure 10: Feature-level fusion: the hand-crafted features are combined with
    the features extracted from CNNs as the new feature vectors.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：特征级融合：手工设计的特征与从CNN中提取的特征结合，形成新的特征向量。
- en: For example, a combined-feature based classification approach called CFBC is
    proposed for lung nodule classification by [[36](#bib.bib36)]. In CFBC, the hand-crafted
    features (including texture and shape descriptors) and the features learned by
    a nine-layer CNN are combined and fed into a back-propagation neural network.
    Experimental results derived from a publicly available dataset show that compared
    with a purely CNN model, incorporating hand-crafted features improves the accuracy,
    sensitivity, and specificity by 3.87%, 6.41%, and 3.21%, respectively.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过[[36](#bib.bib36)]提出了一种基于组合特征的分类方法，称为CFBC，用于肺结节分类。在CFBC中，将手工设计的特征（包括纹理和形状描述符）与九层CNN学习到的特征结合，并输入到反向传播神经网络中。来自公开数据集的实验结果表明，与纯CNN模型相比，融合手工设计的特征可以分别提高准确率、敏感性和特异性3.87%、6.41%和3.21%。
- en: Another example in this category is the breast cancer classification in histology
    images [[57](#bib.bib57)]. More specifically, two hand-crafted features, namely
    the parameter-free threshold adjacency statistics and the gray-level co-occurrence
    matrix, are fused with the five groups of deep learning features extracted from
    five different deep models. The results show that after incorporating hand-crafted
    features, the accuracy of the deep learning model can be significantly improved.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的另一个例子是组织学图像中的乳腺癌分类 [[57](#bib.bib57)]。更具体地说，将两种手工设计的特征，即无参数阈值邻接统计量和灰度共生矩阵，与从五个不同深度模型中提取的五组深度学习特征融合在一起。结果表明，经过融合手工设计的特征后，深度学习模型的准确率可以显著提高。
- en: Other examples of employing feature-level fusion can be found in glaucoma diagnosis
    [[90](#bib.bib90)], skin lesion classification [[89](#bib.bib89)], lung nodule
    classification [[91](#bib.bib91)] and brain tumor diagnosis [[92](#bib.bib92)].
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其他应用特征级融合的例子包括青光眼诊断 [[90](#bib.bib90)]、皮肤病变分类 [[89](#bib.bib89)]、肺结节分类 [[91](#bib.bib91)]
    和脑肿瘤诊断 [[92](#bib.bib92)]。
- en: 'Input-level fusion: In this approach, hand-crafted features are first represented
    as patches highlighting the corresponding features. Then, these patches are taken
    as input to CNNs to make the final conclusion. Using this approach, the CNNs are
    expected to rely more on the hand-crafted features. It should be noted that generally
    speaking, one CNN is required for each type of hand-crafted feature, and information
    obtained from these CNNs need to be combined in some manner to make a final decision.
    The structure of this approach is illustrated in Fig. [11](#S2.F11 "Figure 11
    ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输入级融合：在这种方法中，手工设计的特征首先表示为突出对应特征的图像块。然后，这些图像块作为输入提供给CNN，以得出最终结论。使用这种方法，CNN预期会更多依赖于手工设计的特征。需要注意的是，一般来说，每种手工设计的特征需要一个CNN，并且从这些CNN获得的信息需要以某种方式结合起来，以做出最终决策。该方法的结构如图
    [11](#S2.F11 "图 11 ‣ 2.3.4 医学医生特别关注的特征 ‣ 2.3 融合医学医生的知识 ‣ 2 疾病诊断 ‣ 融合领域知识到深度学习中的调查")
    所示。
- en: '![Refer to caption](img/d7e1908a544551c52978c7518c8f5148.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7e1908a544551c52978c7518c8f5148.png)'
- en: 'Figure 11: Input-level fusion: the hand-crafted features are represented as
    image patches that are taken as inputs to the CNNs.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：输入级融合：手工设计的特征被表示为图像块，作为CNN的输入。
- en: For example, in [[94](#bib.bib94)], three types of hand-crafted features, namely
    the contrast information of the initial nodule candidates (INCs) and the outer
    environments, the histogram of oriented gradients (HOG) feature, and the LBP feature,
    are transformed into the corresponding patches and are taken as inputs of multiple
    CNNs. The results show that this approach outperforms both conventional CNN-based
    approaches and traditional machine-learning approaches based on hand-crafted features.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[94](#bib.bib94)]中，三种手工特征，即初步结节候选者（INCs）与外部环境的对比信息、方向梯度直方图（HOG）特征以及LBP特征，被转换为相应的补丁，并作为多个CNN的输入。结果表明，这种方法优于传统的基于CNN的方法以及基于手工特征的传统机器学习方法。
- en: 'Another example using input-level fusion approach is the MV-KBC (multi-view
    knowledge-based collaborative) algorithm proposed for lung nodule classification
    [[54](#bib.bib54)]. The MV-KBC first extracts patches corresponding to three features:
    the overall appearance (OA), nodule’s heterogeneity in voxel values (HVV) and
    heterogeneity in shapes (HS). Each type of patch is fed into a ResNet. Then, the
    outputs of these ResNets are combined to generate the final classification results.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用输入级融合方法的例子是用于肺结节分类的MV-KBC（多视角知识基础协作）算法[[54](#bib.bib54)]。MV-KBC首先提取与三种特征对应的补丁：整体外观（OA）、结节的体素值异质性（HVV）和形状异质性（HS）。每种类型的补丁被输入到ResNet中。然后，这些ResNet的输出被结合起来生成最终的分类结果。
- en: Moreover, [[93](#bib.bib93)] proposes the dual-path semi-supervised conditional
    generative adversarial networks (DScGAN) for thyroid classification. More specifically,
    the features from the ultrasound B-mode images and the ultrasound elastography
    images are first extracted as the OB patches (indicating the region of interest
    (ROI) in B-mode images), OS patches (characterizing the shape of the nodules),
    and OE patches (indicating the elasticity ROI according to the B-mode ROI position).
    Then, these patches are used as the input of the DScGAN. Using the information
    from these patches is demonstrated to be effective to improve the classification
    performance. Other examples employ input-level fusion can be found in thyroid
    nodule diagnosis [[95](#bib.bib95)] and breast cancer diagnosis on multi-sequence
    MRI [[96](#bib.bib96)].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[93](#bib.bib93)]提出了用于甲状腺分类的双路径半监督条件生成对抗网络（DScGAN）。具体来说，首先从超声B模式图像和超声弹性成像图像中提取特征，作为OB补丁（表示B模式图像中的感兴趣区域（ROI））、OS补丁（描述结节的形状）和OE补丁（根据B模式ROI位置指示弹性ROI）。然后，这些补丁作为DScGAN的输入。利用这些补丁中的信息被证明有效地提高了分类性能。其他使用输入级融合的方法可以在甲状腺结节诊断[[95](#bib.bib95)]和多序列MRI中的乳腺癌诊断[[96](#bib.bib96)]中找到。
- en: 'Usage of features as labels of CNNs: In this approach, besides the original
    classification labels of images, medical doctors also provide labels for some
    hand-crafted features. This extra information is generally incorporated into deep
    learning models via the multi-task learning structure.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 特征作为CNN标签的使用：在这种方法中，除了图像的原始分类标签外，医学专家还提供了一些手工特征的标签。这些额外的信息通常通过多任务学习结构纳入深度学习模型中。
- en: For example, in [[39](#bib.bib39)], the nodules in lung images were quantitatively
    rated by radiologists with regard to nine hand-crafted features (e.g., spiculation,
    texture, and margin). The multi-task learning framework is used to incorporate
    the above information into the main task of lung nodule classification.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[39](#bib.bib39)]中，放射科医生根据九种手工特征（如刺突、纹理和边缘）对肺图像中的结节进行了定量评级。多任务学习框架用于将上述信息纳入肺结节分类的主要任务中。
- en: In addition, for the benign-malignant risk assessment of lung nodules in low-dose
    CT scans [[59](#bib.bib59)], the binary labels about the presence of six high-level
    nodule attributes, namely the calcification, sphericity, margin, lobulation, spiculation
    and texture, are utilized. Six CNNs are designed and each one aims at learning
    one attribute. The discriminative features automatically learned by CNNs for these
    attributes are fused in a multi-task learning framework to obtain the final risk
    assessment scores.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于低剂量CT扫描中的肺结节良恶性风险评估[[59](#bib.bib59)]，利用了关于六种高级结节特征的二元标签，即钙化、球形度、边缘、分叶、刺突和纹理。设计了六个CNN，每个CNN旨在学习一个特征。CNN自动学习到的这些特征在多任务学习框架中融合，以获得最终的风险评估分数。
- en: Similarly, in [[97](#bib.bib97)], each glioma nuclear image is exclusively labeled
    in terms of the shapes and attributes for the centermost nuclei of the image.
    These features are then learned by a multi-task CNN. Experiments demonstrate that
    the proposed method outperforms the baseline CNN.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在[[97](#bib.bib97)]中，每个胶质瘤细胞核图像都以图像中心最核的形状和属性作为独家标记。然后这些特征通过多任务CNN进行学习。实验证明，所提出的方法优于基线CNN。
- en: 2.3.5 Other Types of Information Related to Diagnosis
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 与诊断相关的其他类型信息
- en: In this section, we summarize other types of information that can help deep
    learning models to improve their diagnostic performance. These types of information
    include extra category labels and clinical diagnostic reports.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了其他类型的信息，可以帮助深度学习模型提高其诊断性能。这些信息类型包括额外的类别标签和临床诊断报告。
- en: Extra category labels
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的类别标签
- en: For medical images, besides a classification label (i.e., normal, malignant
    or benign), radiologists may give some extra category labels. For example, in
    the ultrasonic diagnosis of breast cancer, an image usually has a BI-RADS label
    that classifies the image into 0$\sim$6 [[106](#bib.bib106)]—category 0 suggests
    re-examination, categories 1 and 2 indicate that it is prone to be a benign lesion,
    category 3 suggests probably benign findings, categories 4 and 5 are suspected
    to be malignant, category 6 definitely suggests malignant). These labels also
    contain information about the condition of lesions. In [[52](#bib.bib52)], the
    BI-RADS label for each medical image is incorporated in a multi-task learning
    structure as the label of an auxiliary task. Experimental results show that incorporating
    these BI-RADS labels can improve the diagnostic performance of major classification
    task. Another example of using the information from BI-RADS labels can be found
    in [[100](#bib.bib100)].
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学图像，除了分类标签（即正常、恶性或良性），放射科医师可能会给出一些额外的类别标签。例如，在乳腺癌超声诊断中，一张图像通常具有BI-RADS标签，将图像分类为0$\sim$6
    [[106](#bib.bib106)] ——0类建议重新检查，1和2类表示可能是良性病变，3类表示可能是良性发现，4和5类被怀疑恶性，6类绝对被认为是恶性）。这些标签还包含有关病变状态的信息。在[[52](#bib.bib52)]中，每幅医学图像的BI-RADS标签被纳入多任务学习结构作为辅助任务的标签。实验结果表明，纳入这些BI-RADS标签可以提高主要分类任务的诊断性能。另一个利用BI-RADS标签信息的例子可以在[[100](#bib.bib100)]中找到。
- en: In addition, during the process of image annotation, consensus or disagreement
    among experts regarding images reflects the gradeability and difficulty levels
    of the image, which is also a representation of medical domain knowledge. To incorporate
    this information,[[101](#bib.bib101)] proposes a multi-branch model structure
    to predict the most sensitive, most specifical and a balanced fused result for
    glaucoma images. Meanwhile, the consensus loss is also used to encourage the sensitivity
    and specificity branch to generate consistent and contradictory predictions for
    images with consensus and disagreement labels, respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在图像注释过程中，专家之间关于图像的一致性或不一致性反映出图像的可分级性和难度级别，这也是医学领域知识的一种表现。为了整合这些信息，[[101](#bib.bib101)]提出了一种多分支模型结构，用于预测青光眼图像最敏感、最特异和平衡融合的结果。与此同时，一致性损失也被用来鼓励敏感性和特异性分支对具有一致性和不一致性标签的图像生成一致和矛盾的预测。
- en: Extra clinical diagnostic reports
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 附加临床诊断报告
- en: A clinical report is a summary of all the clinical findings and impressions
    determined during examination of a radiography study. It usually contains rich
    information and reflects the findings and descriptions of medical doctors. Incorporating
    clinical reports into CNNs designed for disease diagnosis is typically beneficial.
    As medical reports are generally handled by recurrent neural networks (RNNs),
    to incorporate information from medical reports, generally hybrid networks containing
    both CNNs and RNNs are needed.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 临床报告是对放射学研究检查期间确定的所有临床发现和印象的总结。它通常包含丰富的信息，反映了医生的发现和描述。将临床报告纳入设计用于疾病诊断的CNNs通常是有益的。由于医疗报告通常由递归神经网络（RNNs）处理，因此通常需要包含既包含CNNs又包含RNNs的混合网络来整合来自医疗报告的信息。
- en: For example, the Text-Image embedding network (TieNet) is designed to classify
    the common thorax disease in chest X-rays [[98](#bib.bib98)]. TieNet has an end-to-end
    CNN-RNN architecture enabling it to integrate information of radiological reports.
    The classification results are significantly improved (with about a 6% increase
    on average in AUCs) compared with the baseline CNN purely based on medical images.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文本-图像嵌入网络 (TieNet) 旨在分类胸部 X 光片中的常见胸部疾病 [[98](#bib.bib98)]。TieNet 具有端到端的 CNN-RNN
    架构，使其能够整合放射报告的信息。与完全基于医学图像的基线 CNN 相比，分类结果显著提高（AUC 平均提高约 6%）。
- en: In addition, using semantic information from diagnostic reports is explored
    in [[99](#bib.bib99)] for understanding pathological bladder cancer images. A
    dual-attention model is designed to facilitate the high-level interaction of semantic
    information and visual information. Experiments demonstrate that incorporating
    information from diagnostic reports significantly improves the cancer diagnostic
    performance over the baseline method.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 [[99](#bib.bib99)] 中探讨了使用诊断报告中的语义信息来理解病理性膀胱癌图像。设计了一个双重注意力模型，以促进语义信息和视觉信息的高级交互。实验表明，整合诊断报告中的信息显著提高了癌症诊断性能，相较于基线方法。
- en: 2.4 Summary
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 总结
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for disease
    diagnosis. Generally, almost all types of domain knowledge are proven to be effective
    in boosting the diagnostic performance, especially using the metrics of accuracy
    and AUC, some examples and their quantitative improvements are listed in Table
    [IV](#S2.T4 "TABLE IV ‣ 2.4 Summary ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了不同类型的领域知识及其相应的整合方法，以应用于疾病诊断的深度学习模型中。一般而言，几乎所有类型的领域知识都被证明能有效提升诊断性能，特别是使用准确率和
    AUC 指标的情况下，表 [IV](#S2.T4 "TABLE IV ‣ 2.4 Summary ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")
    列出了部分示例及其定量改进。
- en: 'TABLE IV: The comparison of the quantitative metrics for some disease diagnosis
    methods after integrating domain knowledge'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 整合领域知识后的某些疾病诊断方法的定量指标比较'
- en: '| Reference |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/带领域知识 &#124;'
- en: '| Accuracy | AUC |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | AUC |'
- en: '| [[37](#bib.bib37)] |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bib37)] |'
- en: '&#124; AG-CNN only with &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅 AG-CNN &#124;'
- en: '&#124; global branch/AG-CNN &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全局分支/AG-CNN &#124;'
- en: '| –/– | 0.840/0.871 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.840/0.871 |'
- en: '| [[75](#bib.bib75)] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] |'
- en: '&#124; ResNet-50/DermaKNet &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50/DermaKNet &#124;'
- en: '| –/– | 0.874/0.908 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.874/0.908 |'
- en: '| [[62](#bib.bib62)] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] |'
- en: '&#124; Fine-tuned VGG-Net/ &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调的 VGG-Net/ &#124;'
- en: '&#124; Fine-tuned MG-Net &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调的 MG-Net &#124;'
- en: '| 0.900/0.930 | 0.950/0.970 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 0.900/0.930 | 0.950/0.970 |'
- en: '| [[89](#bib.bib89)] |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] |'
- en: '&#124; ResNet-50/ResNet-50 &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50/ResNet-50 &#124;'
- en: '&#124; with handcrafted features &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带手工特征 &#124;'
- en: '| –/– | 0.830/0.940 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.830/0.940 |'
- en: '| [[38](#bib.bib38)] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] |'
- en: '&#124; CNN without using &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未使用的 CNN &#124;'
- en: '&#124; attention map/AG-CNN &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力图/AG-CNN &#124;'
- en: '| 0.908/0.953 | 0.966/0.975 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 0.908/0.953 | 0.966/0.975 |'
- en: '| [[67](#bib.bib67)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] |'
- en: '&#124; ResNet50/CANet &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet50/CANet &#124;'
- en: '| 0.828/0.851 | –/– |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 0.828/0.851 | –/– |'
- en: '| [[52](#bib.bib52)] |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] |'
- en: '&#124; VGG16/Multi-task VGG16 &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VGG16/多任务 VGG16 &#124;'
- en: '| 0.829 /0.833 | –/– |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 0.829 /0.833 | –/– |'
- en: '| [[28](#bib.bib28)] | DenseNet/BMSL | –/– | 0.850/0.900 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | DenseNet/BMSL | –/– | 0.850/0.900 |'
- en: '| [[26](#bib.bib26)] |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] |'
- en: '&#124; CNN with single-stage/ &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单阶段 CNN/ &#124;'
- en: '&#124; multi-stage transfer learning &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段迁移学习 &#124;'
- en: '| –/– | 0.850/0.910 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.850/0.910 |'
- en: '| [[29](#bib.bib29)] |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] |'
- en: '&#124; CNN/AGCL &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/AGCL &#124;'
- en: '| –/– | 0.771/0.803 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.771/0.803 |'
- en: '| [[93](#bib.bib93)] |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] |'
- en: '&#124; DScGAN without/with &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DScGAN 无/有 &#124;'
- en: '&#124; domain knowledge &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 领域知识 &#124;'
- en: '| 0.816/0.905 | 0.812/0.914 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 0.816/0.905 | 0.812/0.914 |'
- en: With respect to type of domain knowledge for disease diagnosis, the knowledge
    from natural images is widely incorporated in deep learning models. However, considering
    the gap between natural images and medical ones, using information from external
    medical datasets of the same diseases with similar modalities (e.g., SFM and DM)
    [[63](#bib.bib63)], with different modalities (DBT and MM, Ultrasound) [[26](#bib.bib26)],
    and even with different diseases [[68](#bib.bib68)] may be more effective. To
    incorporate the above information is relatively easy, and both transfer learning
    and multi-task learning have been widely adopted. However, there are still no
    comparative studies on how different extra datasets can improve the performance
    of deep learning models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用于疾病诊断的领域知识，自然图像中的知识被广泛地整合到深度学习模型中。然而，考虑到自然图像与医学图像之间的差距，使用来自相同疾病、相似模态（例如，SFM和DM）[[63](#bib.bib63)]、不同模态（DBT和MM，超声）[[26](#bib.bib26)]，甚至不同疾病[[68](#bib.bib68)]的外部医学数据集中的信息可能更为有效。整合上述信息相对容易，转移学习和多任务学习已被广泛采用。然而，目前尚无比较研究来评估不同额外数据集如何提高深度学习模型的性能。
- en: For the domain knowledge of medical doctors, the high-level domain knowledge
    (e.g., the training pattern and the diagnostic pattern) is generally common for
    different diseases. On the other hand, the low-level domain knowledge, such as
    the areas in images and features of diseases that medical doctors usually pay
    attention to, is generally different for different diseases. There is generally
    a trade-off between the versatility and the utility of domain knowledge. To diagnose
    a certain disease, the benefit of incorporating a versatile type of domain knowledge
    suitable for many diseases may not be as significant as using the one that is
    specific for the disease. However, identifying such specific domain knowledge
    may not be easy, and generally requires more efforts from medical doctors (e.g.,
    to identify hand-crafted features or attention maps).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学医生的领域知识，高层领域知识（例如，训练模式和诊断模式）通常对不同疾病是通用的。另一方面，低层领域知识，例如医学医生通常关注的图像区域和疾病特征，对于不同的疾病通常是不同的。领域知识的通用性与实用性之间通常存在权衡。为了诊断某种特定疾病，整合适用于多种疾病的通用领域知识的好处可能不如使用特定于该疾病的领域知识显著。然而，识别这种特定领域知识可能并不容易，并且通常需要医学医生付出更多的努力（例如，识别手工制作的特征或注意力图）。
- en: We believe that more kinds of medical domain knowledge can be explored and utilized
    for disease diagnosis. In addition, comparative studies on some benchmark datasets
    should be carried out with respect to different types of domain knowledge and
    different incorporating methods for disease diagnosis. The results can provide
    further insights about the utility of medical domain knowledge for deep learning
    models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，可以探索和利用更多种类的医学领域知识来进行疾病诊断。此外，还应该对不同类型的领域知识及其在疾病诊断中的不同整合方法在一些基准数据集上的比较研究。这些结果可以进一步洞察医学领域知识对深度学习模型的实用性。
- en: 3 Lesion, Organ, and Abnormality Detection
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 病变、器官和异常检测
- en: Detecting lesions in medical images is an important task and a key part for
    disease diagnosis in many conditions. Similarly, organ detection is an essential
    preprocessing step for image registration, organ segmentation, and lesion detection.
    Detecting abnormalities in medical images, such as cerebral microbleeds in brain
    MRI images and hard exudates in retinal images, is also required in many applications.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 检测医学影像中的病变是一个重要任务，是许多疾病诊断的关键部分。类似地，器官检测是图像配准、器官分割和病变检测的必要预处理步骤。在许多应用中，检测医学影像中的异常，如脑MRI影像中的脑微出血和视网膜图像中的硬性渗出物，也是必需的。
- en: In this section, the deep learning models widely used for object detection in
    medical images are first described in Subsection [3.1](#S3.SS1 "3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis"). Then, the existing works on utilizing domain
    knowledge from natural and medical datasets, and from medical doctors are introduced
    in detail in Subsection [3.2](#S3.SS2 "3.2 Incorporating Knowledge from Natural
    Datasets or Other Medical Datasets ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and Subsection [3.3](#S3.SS3 "3.3 Incorporating Knowledge from Medical
    Doctors ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"), respectively.
    Lastly, we summarize and discuss these different types of domain knowledge and
    the associated incorporating methods in Subsection [3.4](#S3.SS4 "3.4 Summary
    ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先在小节 [3.1](#S3.SS1 "3.1 医学图像中目标检测深度学习模型的一般结构 ‣ 3 病灶、器官和异常检测 ‣ 领域知识融入医学图像分析的深度学习研究")
    中描述了广泛用于医学图像目标检测的深度学习模型。然后，在小节 [3.2](#S3.SS2 "3.2 从自然数据集或其他医学数据集中融入知识 ‣ 3 病灶、器官和异常检测
    ‣ 领域知识融入医学图像分析的深度学习研究") 和小节 [3.3](#S3.SS3 "3.3 从医学医生处融入知识 ‣ 3 病灶、器官和异常检测 ‣ 领域知识融入医学图像分析的深度学习研究")
    中详细介绍了利用自然和医学数据集以及医学医生知识的现有工作。最后，我们在小节 [3.4](#S3.SS4 "3.4 总结 ‣ 3 病灶、器官和异常检测 ‣
    领域知识融入医学图像分析的深度学习研究") 中总结和讨论了这些不同类型的领域知识及其融入方法。
- en: 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 医学图像中目标检测深度学习模型的一般结构
- en: Deep learning models designed for object detection in natural images have been
    directly applied to detect objects in medical images. These applications include
    pulmonary nodule detection in CT images [[110](#bib.bib110)], retinal diseases
    detection in retinal fundus [[111](#bib.bib111)] and so on.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于自然图像中目标检测的深度学习模型已被直接应用于医学图像中的目标检测。这些应用包括 CT 图像中的肺结节检测 [[110](#bib.bib110)]，视网膜眼底中的视网膜疾病检测
    [[111](#bib.bib111)] 等。
- en: '![Refer to caption](img/24ae3b01e5ddfcecfd137e07a5d79694.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24ae3b01e5ddfcecfd137e07a5d79694.png)'
- en: 'Figure 12: The workflow of colitis detection method by using the Faster R-CNN
    structure [[112](#bib.bib112)].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：使用 Faster R-CNN 结构进行结肠炎检测的方法工作流程 [[112](#bib.bib112)]。
- en: One popular type of model is the two-stage detectors like the Faster R-CNN [[113](#bib.bib113)]
    and the Mask R-CNN [[114](#bib.bib114)]. They generally consist of a region proposal
    network (RPN) that hypothesizes candidate object locations and a detection network
    that refines region proposals. Applications in this category include colitis detection
    in CT images [[112](#bib.bib112)], intervertebral disc detection in X-ray images
    [[115](#bib.bib115)] and the detection of architectural distortions in mammograms
    [[116](#bib.bib116)]. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1 General Structures of
    Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis") shows an example of using Faster R-CNN structure
    for colitis detection [[112](#bib.bib112)].
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的模型类型是两阶段检测器，例如 Faster R-CNN [[113](#bib.bib113)] 和 Mask R-CNN [[114](#bib.bib114)]。这些模型通常包括一个区域建议网络（RPN），用于假设候选对象的位置，以及一个检测网络，用于细化区域建议。此类别的应用包括
    CT 图像中的结肠炎检测 [[112](#bib.bib112)]，X 光图像中的椎间盘检测 [[115](#bib.bib115)]，以及乳腺X光检查中的结构扭曲检测
    [[116](#bib.bib116)]。图 [12](#S3.F12 "图 12 ‣ 3.1 医学图像中目标检测深度学习模型的一般结构 ‣ 3 病灶、器官和异常检测
    ‣ 领域知识融入医学图像分析的深度学习研究") 显示了使用 Faster R-CNN 结构进行结肠炎检测的示例 [[112](#bib.bib112)]。
- en: Another category is one-stage models like YOLO (You Only Look Once) [[117](#bib.bib117)],
    SSD (Single Shot MultiBox Detector) [[118](#bib.bib118)] and RetinaNet [[119](#bib.bib119)].
    These networks skip the region proposal stage and run detection directly by considering
    the probability that the object appears at each point in image. Compared with
    the two-stage models, models in this approach are generally faster and simpler.
    Examples in this category can be found in breast tumor detection in mammograms
    [[120](#bib.bib120)], pulmonary lung nodule detection in CT [[121](#bib.bib121)],
    and the detection of different lesions (e.g., liver lesion, lung lesion, bone
    lesion, abdomen lesion) in CT images [[122](#bib.bib122)]. An example of using
    one-stage structure for lesion detection is shown in Fig. [13](#S3.F13 "Figure
    13 ‣ 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类是像YOLO（You Only Look Once）[[117](#bib.bib117)]、SSD（Single Shot MultiBox Detector）[[118](#bib.bib118)]
    和 RetinaNet [[119](#bib.bib119)]这样的单阶段模型。这些网络跳过了区域提议阶段，直接通过考虑物体在图像中每个点出现的概率来进行检测。与双阶段模型相比，这种方法的模型通常更快、更简单。这类模型的示例包括乳腺X光检查中的乳腺肿瘤检测[[120](#bib.bib120)]、CT中的肺结节检测[[121](#bib.bib121)]，以及CT图像中不同病变（例如肝病变、肺病变、骨病变、腹部病变）的检测[[122](#bib.bib122)]。使用单阶段结构进行病变检测的示例如图
    [13](#S3.F13 "图 13 ‣ 3.1 医学图像中物体检测的深度学习模型的通用结构 ‣ 3 病变、器官和异常检测 ‣ 将领域知识融入深度学习的综述")所示。
- en: '![Refer to caption](img/956a5c6359b36ecfe818810ac33e1386.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/956a5c6359b36ecfe818810ac33e1386.png)'
- en: 'Figure 13: An example of using one-stage structure for lesion detection in
    CT images [[122](#bib.bib122)].'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：使用单阶段结构进行CT图像病变检测的示例[[122](#bib.bib122)]。
- en: In the following subsections, we will introduce related works that incorporate
    external knowledge into deep learning models for object detection in medical images.
    A summary of these works is shown in Table [V](#S3.T5 "TABLE V ‣ 3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis").
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子节中，我们将介绍将外部知识融入深度学习模型以进行医学图像中的物体检测的相关工作。这些工作的总结见表 [V](#S3.T5 "表 V ‣ 3.1
    医学图像中物体检测的深度学习模型的通用结构 ‣ 3 病变、器官和异常检测 ‣ 将领域知识融入深度学习的综述")。
- en: 'TABLE V: List of studies on lesion, organ, and abnormality detection and the
    knowledge they incorporated'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：病变、器官和异常检测及其融合知识的研究列表
- en: '|'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating Method &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融合方法 &#124;'
- en: '| References |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[21](#bib.bib21)][[123](#bib.bib123)]  [[124](#bib.bib124)][[125](#bib.bib125)]  [[126](#bib.bib126)]
    &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[21](#bib.bib21)][[123](#bib.bib123)]  [[124](#bib.bib124)][[125](#bib.bib125)]  [[126](#bib.bib126)]
    &#124;'
- en: '|'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Medical datasets &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学数据集 &#124;'
- en: '|'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-modal images &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态图像 &#124;'
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[127](#bib.bib127)][[128](#bib.bib128)][[129](#bib.bib129)] &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[127](#bib.bib127)][[128](#bib.bib128)][[129](#bib.bib129)] &#124;'
- en: '|'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical doctors |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 医学医生 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[29](#bib.bib29)][[130](#bib.bib130)]  [[131](#bib.bib131)] &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[29](#bib.bib29)][[130](#bib.bib130)]  [[131](#bib.bib131)] &#124;'
- en: '|'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Detection patterns |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 检测模式 |'
- en: '&#124; Using images collected &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用收集的图像 &#124;'
- en: '&#124; under different settings &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在不同设置下 &#124;'
- en: '| [[132](#bib.bib132)][[133](#bib.bib133)] |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)][[133](#bib.bib133)] |'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Comparing bilateral or &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比较双侧或 &#124;'
- en: '&#124; cross-view images &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨视图图像 &#124;'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[134](#bib.bib134)][[135](#bib.bib135)]  [[136](#bib.bib136), [137](#bib.bib137)]  [[138](#bib.bib138)]
    &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[134](#bib.bib134)][[135](#bib.bib135)]  [[136](#bib.bib136), [137](#bib.bib137)]  [[138](#bib.bib138)]
    &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Analyzing adjacent slices &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分析相邻切片 &#124;'
- en: '| [[121](#bib.bib121)] |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] |'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Features doctors focus on &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医生关注的特征 &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征级融合 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[139](#bib.bib139)][[140](#bib.bib140)]  [[141](#bib.bib141)][[95](#bib.bib95)]  [[142](#bib.bib142)][[143](#bib.bib143)]
    &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139)][[140](#bib.bib140)]  [[141](#bib.bib141)][[95](#bib.bib95)]  [[142](#bib.bib142)][[143](#bib.bib143)]
    &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other related information &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他相关信息 &#124;'
- en: '|'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task learning &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务学习 &#124;'
- en: '&#124; /training pattern design &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; /训练模式设计 &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[29](#bib.bib29)][[144](#bib.bib144)]  [[145](#bib.bib145)][[146](#bib.bib146)]
    &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[29](#bib.bib29)][[144](#bib.bib144)]  [[145](#bib.bib145)][[146](#bib.bib146)]
    &#124;'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 从自然数据集或其他医疗数据集中引入知识
- en: Similar to disease diagnosis, it is quite popular to pre-train a large natural
    image dataset (generally ImageNet) to introduce information for object detection
    in medical domain. Examples can be found in lymph node detection [[21](#bib.bib21)],
    polyp and pulmonary embolism detection [[126](#bib.bib126)], breast tumor detection
    [[123](#bib.bib123)], colorectal polyps detection [[124](#bib.bib124), [125](#bib.bib125)]
    and so on.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，预训练一个大型自然图像数据集（通常是 ImageNet）以引入医疗领域的对象检测信息也很流行。可以在淋巴结检测 [[21](#bib.bib21)]、息肉和肺栓塞检测
    [[126](#bib.bib126)]、乳腺肿瘤检测 [[123](#bib.bib123)]、结直肠息肉检测 [[124](#bib.bib124),
    [125](#bib.bib125)] 等方面找到相关例子。
- en: In addition, utilizing other medical datasets (i.e., multi-modal images) is
    also quite common. For example, PET images are used to help the lesion detection
    in CT scans of livers [[128](#bib.bib128)]. Specifically, PET images are first
    generated from CT scans using a combined structure of FCN and GAN, then the synthesized
    PET images are used in a false-positive reduction layer for detecting malignant
    lesions. Quantitative results show a 28% reduction in the average false positive
    per case. Besides, [[127](#bib.bib127)] develops a strategy to detect breast masses
    from digital tomosynthesis by fine-tuning the model pre-trained on mammography
    datasets. Another example of using multi-modal medical images can be found in
    liver tumor detection [[129](#bib.bib129)].
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用其他医疗数据集（即多模态图像）也相当常见。例如，PET 图像被用来帮助肝脏 CT 扫描中的病灶检测 [[128](#bib.bib128)]。具体来说，PET
    图像首先通过 FCN 和 GAN 的组合结构从 CT 扫描生成，然后将合成的 PET 图像用于假阳性减少层以检测恶性病变。定量结果显示，每个病例的平均假阳性减少了
    28%。此外，[[127](#bib.bib127)] 开发了一种策略，通过微调在乳腺 X 光数据集上预训练的模型来检测数字断层摄影中的乳腺肿块。另一个使用多模态医疗图像的例子可以在肝肿瘤检测中找到
    [[129](#bib.bib129)]。
- en: 3.3 Incorporating Knowledge from Medical Doctors
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 从医疗医生那里引入知识
- en: 'In this subsection, we summarize the existing works on incorporating the knowledge
    of medical doctors to more effectively detect objects in medical images. The types
    of domain knowledge from medical doctors can be grouped into the following four
    categories:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们总结了现有的将医疗医生知识融入医疗图像对象检测的方法。来自医疗医生的领域知识可以分为以下四类：
- en: '1.'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练模式，
- en: '2.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general detection patterns they view images,
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像的常规检测模式，
- en: '3.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the features (e.g., locations, structures, shapes) they give special attention
    to, and
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们特别关注的特征（例如位置、结构、形状），以及
- en: '4.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: other related information regarding detection.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他与检测相关的信息。
- en: 3.3.1 Training Pattern of Medical Doctors
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 医疗医生的训练模式
- en: The training pattern of medical doctors, which is generally characterized as
    being given tasks with increasing difficulty, is also used for object detection
    in medical images. Similar to the disease diagnosis, most works utilize the curriculum
    learning to introduce this pattern. For example, an attention-guided curriculum
    learning (AGCL) framework is presented to locate the lesion in chest X-rays [[29](#bib.bib29)].
    During this process, images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN gradually, and the disease heatmaps generated from the CNN
    are used to locate the lesion areas.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗医生的训练模式，通常以任务难度逐渐增加为特征，也被用于医疗图像中的对象检测。与疾病诊断类似，大多数工作利用课程学习来引入这种模式。例如，提出了一种注意力引导的课程学习（AGCL）框架，用于定位胸部
    X 光中的病灶 [[29](#bib.bib29)]。在这个过程中，按照难度顺序（按不同严重程度分组）的图像逐渐输入到 CNN 中，CNN 生成的疾病热图用于定位病灶区域。
- en: Another work is called as CASED proposed for lung nodule detection in chest
    CT [[130](#bib.bib130)]. CASED adopts a curriculum adaptive sampling technique
    to address the problem of extreme data imbalance. In particular, CASED lets the
    network to first learn how to distinguish nodules from their immediate surroundings,
    and then it adds a greater proportion of difficult-to-classify global context,
    until uniformly samples from the empirical data distribution. In this way, CASED
    tops the LUNA16 challenge leader-board with an average sensitivity score of 88.35%.
    Another example of using curriculum learning can be found in cardiac landmark
    detection [[131](#bib.bib131)].
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工作是称为CASED的肺结节检测方法[[130](#bib.bib130)]。CASED采用课程自适应采样技术来解决极端数据不平衡的问题。特别是，CASED让网络首先学习如何区分结节与其周围环境，然后逐渐增加难以分类的全局上下文，直到从经验数据分布中均匀采样。通过这种方式，CASED以88.35%的平均敏感度得分在LUNA16挑战赛中名列前茅。另一个使用课程学习的例子可以在心脏标志物检测中找到[[131](#bib.bib131)]。
- en: 3.3.2 General Detection Patterns of Medical Doctors
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 医生的一般检测模式
- en: 'When experienced medical doctors are locating possible lesions in medical images,
    they also display particular patterns, and these patterns can be incorporated
    into deep learning models for object detection. Experienced medical doctors generally
    have the following patterns: (1) they usually take into account images collected
    under different settings (e.g., brightness and contrast), (2) they often compare
    bilateral images or use cross-view images, and (3) they generally read adjacent
    slices.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 当经验丰富的医生在医学图像中定位可能的病变时，他们也会展示特定的模式，这些模式可以融入深度学习模型中以进行目标检测。经验丰富的医生通常有以下模式：（1）他们通常考虑在不同设置（如亮度和对比度）下收集的图像，（2）他们经常比较双侧图像或使用交叉视图图像，（3）他们通常阅读相邻的切片。
- en: For example, to locate possible lesions during visual inspection of the CT images,
    radiologists would combine images collected under different settings (e.g., brightness
    and contrast). To imitate the above process, a multi-view feature pyramid network
    (FPN) is proposed in [[132](#bib.bib132)], where multi-view features are extracted
    from images rendered with varied brightness and contrast. The multi-view information
    is then combined using a position-aware attention module. Experiments show that
    the proposed model achieves an absolute gain of 5.65% over the previous state-of-the-art
    method on the NIH DeepLesion dataset. Another example of using images under different
    settings can be found in the COVID-19 pneumonia-based lung lesion detection [[133](#bib.bib133)].
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了在CT图像的视觉检查中定位可能的病变，放射科医生会结合在不同设置（如亮度和对比度）下收集的图像。为了模拟上述过程，[[132](#bib.bib132)]中提出了一种多视角特征金字塔网络（FPN），其中多视角特征从具有不同亮度和对比度的图像中提取。然后，使用位置感知注意力模块结合多视角信息。实验表明，所提出的模型在NIH
    DeepLesion数据集上相较于之前的最先进方法取得了绝对提升5.65%。另一个使用不同设置下的图像的例子可以在基于COVID-19肺炎的肺部病变检测中找到[[133](#bib.bib133)]。
- en: In addition, the bilateral information is widely used by radiologists. For example,
    in standard mammographic screening, images are captured from both two breasts,
    and experienced radiologists generally compare bilateral mammogram images to find
    masses. To incorporate this pattern, a contrasted bilateral network (CBN) is proposed
    in [[134](#bib.bib134)], in which the bilateral images are first coarsely aligned
    and then fed into a pair of networks to extract features for the following detection
    steps (shown in Fig. [14](#S3.F14 "Figure 14 ‣ 3.3.2 General Detection Patterns
    of Medical Doctors ‣ 3.3 Incorporating Knowledge from Medical Doctors ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")). Experimental results demonstrate
    the effectiveness of incorporating the bilateral information.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，双侧信息被放射科医生广泛使用。例如，在标准的乳腺X光筛查中，会捕获双侧乳腺的图像，经验丰富的放射科医生通常比较双侧乳腺图像以寻找肿块。为了融合这一模式，[[134](#bib.bib134)]中提出了一种对比双侧网络（CBN），其中双侧图像首先粗略对齐，然后输入一对网络以提取特征用于后续的检测步骤（如图[14](#S3.F14
    "Figure 14 ‣ 3.3.2 General Detection Patterns of Medical Doctors ‣ 3.3 Incorporating
    Knowledge from Medical Doctors ‣ 3 Lesion, Organ, and Abnormality Detection ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")所示）。实验结果证明了融合双侧信息的有效性。
- en: '![Refer to caption](img/31af6a214a801ccde849eb1be904a1d3.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/31af6a214a801ccde849eb1be904a1d3.png)'
- en: 'Figure 14: The workflow of mammogram mass detection by integrating the bilateral
    information [[134](#bib.bib134)], where the aligned images are fed into two networks
    seperately to extract features for further detection.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：通过整合双侧信息[[134](#bib.bib134)]进行乳腺肿块检测的工作流程，其中对齐的图像分别输入到两个网络中以提取特征用于进一步检测。
- en: Similarly, to detect acute stroke signs in non-contrast CT images, experienced
    neuroradiologists routinely compare the appearance and Hounsfield Unit intensities
    of the left and right hemispheres, and then find the regions most commonly affected
    in stroke episodes. This pattern is mimicked by [[137](#bib.bib137)] for the detection
    of dense vessels and ischaemia. The experimental results show that introducing
    the pattern greatly improves the performance for detecting ischaemia. Other examples
    of integrating the bilateral feature comparison or the symmetry constrains can
    be found in thrombus detection [[136](#bib.bib136)] and hemorrhagic lesion detection
    [[138](#bib.bib138)] in brain CT images.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为了在无对比剂CT图像中检测急性中风的迹象，经验丰富的神经放射科医师通常会比较左右半球的外观和Hounsfield单位强度，然后找到中风发作中最常受影响的区域。[[137](#bib.bib137)]通过模拟这种模式来检测密集的血管和缺血。实验结果显示，引入该模式大大提高了检测缺血的性能。其他集成双侧特征比较或对称约束的例子可以在脑CT图像中的血栓检测[[136](#bib.bib136)]和出血性病变检测[[138](#bib.bib138)]中找到。
- en: Besides the bilateral images, the information from cross views (i.e., mediolateral-oblique
    and cranio-caudal) is highly related and complementary, and hence is also used
    for mammogram mass detection. In [[135](#bib.bib135)], a bipartite graph convolutional
    network is introduced to endow the existing methods with cross-view reasoning
    ability of radiologists. Concretely, the bipartite node sets are constructed to
    represent the relatively consistent regions, and the bipartite edge are used to
    model both inherent cross-view geometric constraints and appearance similarities
    between correspondences. This process can enables spatial visual features equipped
    with cross-view reasoning ability. Experimental results on DDSM dataset achieve
    the state-of-the-art performance (with a recall of 79.5 at 0.5 false positives
    per image).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 除了双侧图像，来自交叉视图（即，侧面斜视和头尾视图）的信息高度相关且互补，因此也用于乳腺X线图像中的肿块检测。在[[135](#bib.bib135)]中，引入了一种双分图卷积网络，以赋予现有方法放射科医师的交叉视图推理能力。具体而言，构建了双分节点集来表示相对一致的区域，并使用双分边来建模固有的交叉视图几何约束和对应点之间的外观相似性。这个过程能够使空间视觉特征具备交叉视图推理能力。DDSM数据集上的实验结果达到最先进的性能（在每张图像0.5个假阳性下的召回率为79.5）。
- en: When looking for small nodules in CT images , radiologists often observe each
    slice together with adjacent slices, similar to detecting an object in a video.
    This workflow is imitated in [[121](#bib.bib121)] to detect pulmonary lung nodule
    in CT images, where the state-of-the-art object detector SSD is applied in this
    process. This method obtains state-of-the-art result with the FROC score of 0.892
    in LUNA16 dataset.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在CT图像中寻找小结节时，放射科医师通常会观察每一切片及其相邻切片，类似于在视频中检测物体。这种工作流程在[[121](#bib.bib121)]中被模拟用于检测CT图像中的肺结节，其中应用了最先进的目标检测器SSD。这种方法在LUNA16数据集中取得了最先进的FROC得分0.892。
- en: 3.3.3 Features That Medical Doctors Give Special Attention to
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 医生特别关注的特征
- en: Similar to disease diagnosis, medical doctors also use many ‘hand-crafted features’
    to help them to find target objects (e.g., nodules or lesions) in medical images.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，医生也使用许多“手工特征”来帮助他们在医学图像中找到目标物体（例如，结节或病变）。
- en: For example, in [[140](#bib.bib140)], to detect mammographic lesions, different
    types of hand-crafted features (e.g., contrast features, geometrical features,
    and location features) are first extracted, and then concatenated with those learned
    by a CNN. The results show that these hand-crafted features, particularly the
    location and context features , can complement the network generating a higher
    specificity over the CNN alone.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[140](#bib.bib140)]中，为了检测乳腺X线图像中的病变，首先提取不同类型的手工特征（例如，对比度特征、几何特征和位置特征），然后与CNN学习到的特征进行连接。结果显示，这些手工特征，特别是位置和上下文特征，可以补充网络，从而比单独使用CNN生成更高的特异性。
- en: Similarly, [[141](#bib.bib141)] presents a deep learning model based on Faster
    R-CNN to detect abnormalities in the esophagus from endoscopic images. In particular,
    to enhance texture details, the proposed detection system incorporates the Gabor
    handcrafted features with the CNN features through concatenation in the detection
    stage. The experimental results on two datasets (Kvasir and MICCAI 2015) show
    that the model is able to surpass the state-of-the-art performance.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[141](#bib.bib141)]提出了一种基于Faster R-CNN的深度学习模型，用于从内窥镜图像中检测食管异常。特别是，为了增强纹理细节，所提出的检测系统通过在检测阶段的串联中将Gabor手工特征与CNN特征相结合。在两个数据集（Kvasir和MICCAI
    2015）上的实验结果表明，该模型能够超越最先进的性能。
- en: Another example can be found in [[139](#bib.bib139)] for the detection of lung
    nodules, where 88 hand-crafted features, including intensity, shape, texture are
    extracted and combined with features extracted by a CNN and then fed into a classifier.
    Experimental results demonstrate the effectiveness of the combination of handcrafted
    features and CNN features.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可以在[[139](#bib.bib139)]中找到，用于检测肺结节，其中提取了88个手工特征，包括强度，形状，纹理，并与CNN提取的特征相结合，然后馈入分类器。实验结果证明了手工特征和CNN特征的组合的有效性。
- en: In the automated detection of thyroid nodules, the size and shape attribute
    of nodules are considered in [[95](#bib.bib95)]. To incorporate the information
    above, the generating process of region proposals is constrained, and the detection
    results on two different datasets show high accuracy.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在甲状腺结节的自动检测中，考虑了结节的大小和形状属性[[95](#bib.bib95)]。为了结合以上信息，对区域提议的生成过程加以限制，并且在两个不同数据集上的检测结果显示出高准确性。
- en: Furthermore, in lymph node gross tumor volume detection (GTV[LN]) in oncology
    imaging, some attributes of lymph nodes (LNs) are also utilized [[142](#bib.bib142)].
    Motivated by the prior clinical knowledge that LNs from a connected lymphatic
    system, and the spread of cancer cells among LNs often follows certain pathways,
    a LN appearance and inter-LN relationship learning framework is proposed for GTV[LN]
    detection. More specifically, the instance-wise appearance features are first
    extracted by a 3D CNN, then a graph neural network (GNN) is used to model the
    inter-LN relationships, and the global LN-tumor spatial priors are included in
    this process. This method significantly improves over state-of-the-art method.
    Another example of combining handcrafted features and deep features can be found
    in lung lesion detection [[143](#bib.bib143)].
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在肿瘤学成像中的淋巴结肿瘤体积（GTV [LN]）检测中，还利用了淋巴结（LNs）的一些属性[[142](#bib.bib142)]。受到先前的临床知识的启发，即来自连通淋巴系统的LN，并且癌细胞在LNs之间的传播通常遵循某些路径，为GTV[
    LN]检测提出了LN外观和LN之间关系学习框架。更具体地说，首先通过3D CNN提取实例化外观特征，然后使用图神经网络（GNN）来建模LN之间的关系，并在此过程中包含全局LN-肿瘤空间先验。该方法显著改进了最先进的方法。在肺部病变检测方面，还可以找到另一个将手工特征和深度特征结合的例子[[143](#bib.bib143)]。
- en: 3.3.4 Other Types of Information Related to Detection
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 与检测相关的其他类型信息
- en: Similar with that in disease diagnosis, there are other information (e.g., radiological
    reports, extra labels) can also be integrated into the lesion detection process.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 与疾病诊断类似，还有其他信息（例如，放射学报告，额外标签）也可以整合到病变检测过程中。
- en: For example in [[29](#bib.bib29)], to locate thoracic diseases on chest radiographs,
    the difficulty of each sample, represented as the severity level of the thoracic
    disease, is first extracted from radiology reports. Then, the curriculum learning
    technique is adopted, in which the training samples are presented to the network
    in order of increasing difficulty. Experiments on the ChestXray14 database validate
    the effectiveness on significant performance improvement over baseline methods.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[[29](#bib.bib29)]中，为了定位胸部X射线片上的胸部疾病，首先从放射学报告中提取每个样本的难度，表示为胸部疾病的严重程度。然后采用课程学习技术，即按难度递增的顺序将训练样本呈现给网络。对ChestXray14数据库上的实验证实了相对基线方法的显着性能提升的有效性。
- en: Example of using extra labels can be found in [[144](#bib.bib144)]. In this
    method, the information of the classification labels is incorporated to help the
    lesion localization in chest X-rays and mammograms. In particular, a framework
    named as self-transfer learning (STL) is proposed, which jointly optimizes both
    classification and localization networks to help the localization network focus
    on correct lesions. Experimental results show that STL can achieve significantly
    better localization performance compared to previous weakly supervised localization
    approaches. More examples of using extra labels can be found in detection in mammograms
    [[145](#bib.bib145), [146](#bib.bib146)].
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 使用额外标签的例子可以在[[144](#bib.bib144)]中找到。在这种方法中，将分类标签的信息纳入，以帮助胸部X光片和乳腺X光片中的病灶定位。特别地，提出了一个名为自我迁移学习（STL）的框架，该框架联合优化分类和定位网络，以帮助定位网络关注正确的病灶。实验结果表明，与之前的弱监督定位方法相比，STL能够显著提高定位性能。更多使用额外标签的例子可以在乳腺X光片的检测中找到[[145](#bib.bib145),
    [146](#bib.bib146)]。
- en: 3.4 Summary
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 摘要
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for object
    detection in medical images. Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis") illustrates the quantitative improvements,
    in terms of sensitivity and recall, of some typical work over the baseline methods
    for object detection in medical images. From the results we can see that in general,
    integrating domain knowledge can be beneficial for detection tasks.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了不同类型的领域知识以及将其整合到深度学习模型中的方法，以用于医学图像的目标检测。表[VI](#S3.T6 "TABLE VI
    ‣ 3.4 Summary ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")展示了与基线方法相比，一些典型工作在医学图像目标检测中的灵敏度和召回率方面的定量改进。结果表明，通常情况下，整合领域知识对检测任务是有利的。
- en: Similar to disease diagnosis, the high-level training pattern of medical doctors
    is generic and can be utilized for detecting different diseases or organs. In
    contrast, the low-level domain knowledge, like the detection patterns that medical
    doctors adopt and some hand-crafted features they give more attention when searching
    lesions, are generally different for different diseases. For example, the pattern
    of comparing bilateral images can only be utilized for detecting organs with symmetrical
    structures [[137](#bib.bib137), [134](#bib.bib134)]. In addition, we can see from
    Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") that leveraging pattern of medical doctors on average shows better
    performance when compared with integrating hand-crafted features. This may indicate
    that there is still large room to explore more effective features for object detection
    in medical images.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断，医疗医生的高级培训模式是通用的，可以用于检测不同的疾病或器官。相对而言，低级别的领域知识，例如医疗医生在搜索病灶时采用的检测模式以及他们更加关注的一些手工特征，通常对于不同的疾病是不同的。例如，比较双侧图像的模式只能用于检测具有对称结构的器官[[137](#bib.bib137),
    [134](#bib.bib134)]。此外，从表[VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")中可以看出，利用医疗医生的模式在平均表现上优于集成手工特征。这可能表明在医学图像的目标检测中，仍有很大的空间可以探索更有效的特征。
- en: 'TABLE VI: The comparison of the quantitative metrics for some medical object
    detection methods after incorporating domain knowledge'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 结合领域知识后，一些医学目标检测方法的定量指标比较'
- en: '| Reference |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline model/With domain knowledge &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/与领域知识 &#124;'
- en: '| Sensitivity | Recall |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 灵敏度 | 召回率 |'
- en: '| [[139](#bib.bib139)] |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] |'
- en: '&#124; CNN/CNN with hand-crafted features &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/CNN与手工特征 &#124;'
- en: '| 0.890/0.909 | –/– |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 0.890/0.909 | –/– |'
- en: '| [[141](#bib.bib141)] |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] |'
- en: '&#124; ResNet/CNN with handcrafted features &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet/CNN与手工特征 &#124;'
- en: '| –/– | 0.940/0.950 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.940/0.950 |'
- en: '| [[121](#bib.bib121)] | SSD/MSSD | 0.927/0.976 | –/– |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | SSD/MSSD | 0.927/0.976 | –/– |'
- en: '| [[134](#bib.bib134)] | Mask R-CNN/CBD | –/– | 0.869/0.890 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] | Mask R-CNN/CBD | –/– | 0.869/0.890 |'
- en: '| [[135](#bib.bib135)] | Mask R-CNN/BG-RCNN | –/– | 0.918/0.945 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| [[135](#bib.bib135)] | Mask R-CNN/BG-RCNN | –/– | 0.918/0.945 |'
- en: '| [[29](#bib.bib29)] |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] |'
- en: '&#124; CNN/AGCL &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/AGCL &#124;'
- en: '| –/– | 0.660/0.730 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| –/– | 0.660/0.730 |'
- en: '1'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: The performance is evaluated at 4 false positives per image in [[139](#bib.bib139),
    [134](#bib.bib134), [135](#bib.bib135), [26](#bib.bib26)].
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能在[[139](#bib.bib139)、[134](#bib.bib134)、[135](#bib.bib135)、[26](#bib.bib26)]中以每张图像4个假阳性进行评估。
- en: 4 Lesion and Organ Segmentation
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 病灶和器官分割
- en: Medical image segmentation devotes to identifying pixels of lesions or organs
    from the background, and is generally regarded as a prerequisite step for the
    lesion assessment and disease diagnosis. Segmentation methods based on deep learning
    models have become the dominant technique in recent years and have been widely
    used for the segmentation of lesions such as brain tumors [[147](#bib.bib147)],
    breast tumors [[148](#bib.bib148)], and organs such as livers [[149](#bib.bib149)]
    and pancreas [[150](#bib.bib150)].
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割致力于从背景中识别病灶或器官的像素，通常被视为病灶评估和疾病诊断的先决步骤。近年来，基于深度学习模型的分割方法已成为主流技术，并被广泛应用于脑肿瘤[[147](#bib.bib147)]、乳腺肿瘤[[148](#bib.bib148)]以及肝脏[[149](#bib.bib149)]和胰腺[[150](#bib.bib150)]等器官的分割。
- en: In Subsection [4.1](#S4.SS1 "4.1 General Structures of Deep Learning Models
    for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), we describe the models that are generally used for object segmentation
    in the medical domain. Then in Subsection [4.2](#S4.SS2 "4.2 Incorporating Knowledge
    from Natural Datasets or Other Medical Datasets ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), the works of utilizing domain knowledge from natural and other medical
    datasets are introduced. Then, the models utilizing domain knowledge from medical
    doctors are introduced in Subsection [4.3](#S4.SS3 "4.3 Incorporating Knowledge
    from Medical Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"). A summary of
    this section is provided in Subsection [4.4](#S4.SS4 "4.4 Summary ‣ 4 Lesion and
    Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在子节[4.1](#S4.SS1 "4.1 医学图像中对象分割的深度学习模型的通用结构 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中，我们描述了医学领域中一般用于对象分割的模型。然后在子节[4.2](#S4.SS2
    "4.2 从自然数据集或其他医学数据集中纳入知识 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中，介绍了利用自然和其他医学数据集领域知识的研究。接着，在子节[4.3](#S4.SS3
    "4.3 从医学医生那里纳入知识 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中介绍了利用医学医生领域知识的模型。本节的总结在子节[4.4](#S4.SS4
    "4.4 总结 ‣ 4 病灶和器官分割 ‣ 关于将领域知识纳入深度学习的调查")中提供。
- en: 4.1 General Structures of Deep Learning Models for Object Segmentation in Medical
    Images
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 医学图像中对象分割的深度学习模型的通用结构
- en: 'The deep learning models utilized for medical image segmentation are generally
    divided into three categories: the FCN (fully convolutional network) [[151](#bib.bib151)]
    based models, the U-Net [[152](#bib.bib152)] based models, and the GAN [[153](#bib.bib153)]
    based models.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 用于医学图像分割的深度学习模型通常分为三类：基于FCN（全卷积网络）[[151](#bib.bib151)]的模型、基于U-Net [[152](#bib.bib152)]的模型，以及基于GAN
    [[153](#bib.bib153)]的模型。
- en: '![Refer to caption](img/1ce4537d0995a0260396a4aa27213369.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ce4537d0995a0260396a4aa27213369.png)'
- en: 'Figure 15: The schematic diagram of using FCN structure for cardiac segmentation
    [[154](#bib.bib154)].'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：使用FCN结构进行心脏分割的示意图[[154](#bib.bib154)]。
- en: In particular, the FCN has been proven to perform well in various medical image
    segmentation tasks [[155](#bib.bib155), [156](#bib.bib156)]. Some variants of
    FCN, such as cascaded FCN [[157](#bib.bib157)], parallel FCN [[158](#bib.bib158)]
    and recurrent FCN [[159](#bib.bib159)] are also widely used for segmentation tasks
    in medical images. Fig. [15](#S4.F15 "Figure 15 ‣ 4.1 General Structures of Deep
    Learning Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ
    Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") illustrates an example of using FCN based model for cardiac
    segmentation.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，FCN在各种医学图像分割任务中表现出色[[155](#bib.bib155), [156](#bib.bib156)]。FCN的一些变体，如级联FCN[[157](#bib.bib157)]、并行FCN[[158](#bib.bib158)]和递归FCN[[159](#bib.bib159)]，也广泛用于医学图像中的分割任务。图[15](#S4.F15
    "图 15 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构 ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")展示了使用FCN模型进行心脏分割的一个示例。
- en: '![Refer to caption](img/be8d2746eeb8929d9d868d196e4abaf5.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be8d2746eeb8929d9d868d196e4abaf5.png)'
- en: 'Figure 16: The network structure of U-Net [[152](#bib.bib152)].'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：U-Net的网络结构[[152](#bib.bib152)]。
- en: In addition, the U-Net [[152](#bib.bib152)] (shown in Fig. [16](#S4.F16 "Figure
    16 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation in
    Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis")) and its variants are
    also widely utilized for medical image segmentation. U-Net builds upon FCN structure,
    mainly consists of a series of convolutional and deconvolutional layers, and with
    the short connections between the layers of equal resolution. U-Net and its variants
    like UNet++[[160](#bib.bib160)] and recurrent U-Net [[161](#bib.bib161)] perform
    well in many medical image segmentation tasks[[162](#bib.bib162)].
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，U-Net[[152](#bib.bib152)]（见图[16](#S4.F16 "图 16 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构
    ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")）及其变体也被广泛用于医学图像分割。U-Net建立在FCN结构的基础上，主要由一系列卷积层和反卷积层组成，并在相同分辨率的层之间有短连接。U-Net及其变体，如UNet++[[160](#bib.bib160)]和递归U-Net[[161](#bib.bib161)]，在许多医学图像分割任务中表现良好[[162](#bib.bib162)]。
- en: In the GAN-based models [[163](#bib.bib163), [164](#bib.bib164)], the generator
    is used to predict the mask of the target based on some encoder-decoder structures
    (such as FCN or U-Net). The discriminator serves as a shape regulator that helps
    the generator to obtain satisfactory segmentation results. Applications of using
    GAN-based models in medical image segmentation include brain segmentation [[165](#bib.bib165)],
    skin lesion segmentation [[166](#bib.bib166)], vessel segmentation [[167](#bib.bib167)]
    and anomaly segmentation in retinal fundus images [[168](#bib.bib168)]. Fig. [17](#S4.F17
    "Figure 17 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation
    in Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") is an example
    of using GAN-based model for vessel segmentation in miscroscopy images.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于GAN的模型[[163](#bib.bib163), [164](#bib.bib164)]中，生成器用于根据某些编码器-解码器结构（如FCN或U-Net）预测目标的掩模。鉴别器则作为形状调节器，帮助生成器获得令人满意的分割结果。GAN-based模型在医学图像分割中的应用包括脑部分割[[165](#bib.bib165)]、皮肤病变分割[[166](#bib.bib166)]、血管分割[[167](#bib.bib167)]和视网膜眼底图像中的异常分割[[168](#bib.bib168)]。图[17](#S4.F17
    "图 17 ‣ 4.1 医学图像目标分割的深度学习模型的一般结构 ‣ 4 病变和器官分割 ‣ 关于将领域知识融入医学图像分析深度学习的调查")展示了使用GAN-based模型进行显微图像中的血管分割的一个示例。
- en: '![Refer to caption](img/a23a66ce2424bc3820148a5a0c2e348f.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a23a66ce2424bc3820148a5a0c2e348f.png)'
- en: 'Figure 17: An example of using a GAN-based model for vessel segmentation [[167](#bib.bib167)].'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：使用GAN-based模型进行血管分割的一个示例[[167](#bib.bib167)]。
- en: 'TABLE VII: The list of researches of lesion, organ segmentation and the knowledge
    they incorporated'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表VII：病变、器官分割研究及其融入的知识列表
- en: '|'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Source &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识来源 &#124;'
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Knowledge Type &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识类型 &#124;'
- en: '|'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporating &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入 &#124;'
- en: '&#124; Method &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '| References |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural datasets &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然数据集 &#124;'
- en: '|'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural images &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Transfer learning &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[155](#bib.bib155)][[169](#bib.bib169)][[170](#bib.bib170)][[171](#bib.bib171)]  [[126](#bib.bib126)]
    &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[155](#bib.bib155)][[169](#bib.bib169)][[170](#bib.bib170)][[171](#bib.bib171)]  [[126](#bib.bib126)]
    &#124;'
- en: '|'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Medical datasets | Multi-modal images |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 医学数据集 | 多模态图像 |'
- en: '&#124; Transfer learning &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 &#124;'
- en: '|'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[172](#bib.bib172)][[34](#bib.bib34)] &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)][[34](#bib.bib34)] &#124;'
- en: '|'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-task/-modal learning &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多任务/多模态学习 &#124;'
- en: '|'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[173](#bib.bib173)][[174](#bib.bib174)] &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[173](#bib.bib173)][[174](#bib.bib174)] &#124;'
- en: '|'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Modality transformation /synthesis &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态转换/合成 &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[175](#bib.bib175)][[176](#bib.bib176)]  [[177](#bib.bib177)]  [[165](#bib.bib165)]  [[178](#bib.bib178)]
    &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[175](#bib.bib175)][[176](#bib.bib176)]  [[177](#bib.bib177)]  [[165](#bib.bib165)]  [[178](#bib.bib178)]
    &#124;'
- en: '&#124; [[179](#bib.bib179)][[180](#bib.bib180)][[181](#bib.bib181)][[182](#bib.bib182)]  [[183](#bib.bib183)][[184](#bib.bib184)]
    &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[179](#bib.bib179)][[180](#bib.bib180)][[181](#bib.bib181)][[182](#bib.bib182)]  [[183](#bib.bib183)][[184](#bib.bib184)]
    &#124;'
- en: '|'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Datasets of other diseases | Transfer learning | [[185](#bib.bib185)] |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 其他疾病的数据集 | 迁移学习 | [[185](#bib.bib185)] |'
- en: '|'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Disease domain transformation &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 疾病领域转换 &#124;'
- en: '| [[27](#bib.bib27)] |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] |'
- en: '| Medical doctors |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 医学专家 |'
- en: '&#124; Training pattern &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练模式 &#124;'
- en: '|'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Curriculum learning &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 课程学习 &#124;'
- en: '|'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[186](#bib.bib186)]  [[30](#bib.bib30)]  [[187](#bib.bib187)][[188](#bib.bib188)]  [[189](#bib.bib189)][[190](#bib.bib190)]
    &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[186](#bib.bib186)]  [[30](#bib.bib30)]  [[187](#bib.bib187)][[188](#bib.bib188)]  [[189](#bib.bib189)][[190](#bib.bib190)]
    &#124;'
- en: '|'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Diagnostic patterns |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 诊断模式 |'
- en: '&#124; Using different views as input &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用不同视角作为输入 &#124;'
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[191](#bib.bib191)][[32](#bib.bib32)] &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[191](#bib.bib191)][[32](#bib.bib32)] &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attention mechanism &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制 &#124;'
- en: '| [[192](#bib.bib192)] |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| [[192](#bib.bib192)] |'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Network design &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络设计 &#124;'
- en: '| [[193](#bib.bib193)][[194](#bib.bib194)] |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| [[193](#bib.bib193)][[194](#bib.bib194)] |'
- en: '| Anatomical priors |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学先验 |'
- en: '&#124; Incorporated in post-processing &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入后处理 &#124;'
- en: '|'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[195](#bib.bib195)]  [[196](#bib.bib196), [197](#bib.bib197)]  [[33](#bib.bib33)]
    &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[195](#bib.bib195)]  [[196](#bib.bib196), [197](#bib.bib197)]  [[33](#bib.bib33)]
    &#124;'
- en: '|'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorporated in loss function &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融入损失函数 &#124;'
- en: '|'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[198](#bib.bib198)][[199](#bib.bib199)]  [[200](#bib.bib200)][[201](#bib.bib201)]  [[40](#bib.bib40)]
    &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[198](#bib.bib198)][[199](#bib.bib199)]  [[200](#bib.bib200)][[201](#bib.bib201)]  [[40](#bib.bib40)]
    &#124;'
- en: '&#124; [[202](#bib.bib202)]  [[198](#bib.bib198)]  [[203](#bib.bib203)] &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[202](#bib.bib202)]  [[198](#bib.bib198)]  [[203](#bib.bib203)] &#124;'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generative models &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成模型 &#124;'
- en: '|'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[32](#bib.bib32)][[204](#bib.bib204)]  [[205](#bib.bib205)][[206](#bib.bib206)]  [[207](#bib.bib207)]
    &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[32](#bib.bib32)][[204](#bib.bib204)]  [[205](#bib.bib205)][[206](#bib.bib206)]  [[207](#bib.bib207)]
    &#124;'
- en: '&#124; [[208](#bib.bib208)]  [[209](#bib.bib209)][[210](#bib.bib210)]  [[211](#bib.bib211)]
    &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[208](#bib.bib208)]  [[209](#bib.bib209)][[210](#bib.bib210)]  [[211](#bib.bib211)]
    &#124;'
- en: '|'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Hand-crafted features |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 手工特征 |'
- en: '&#124; Feature level fusion &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征级融合 &#124;'
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[212](#bib.bib212)][[213](#bib.bib213)] &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[212](#bib.bib212)][[213](#bib.bib213)] &#124;'
- en: '|'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input level fusion &#124;'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入级融合 &#124;'
- en: '|'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[214](#bib.bib214)][[215](#bib.bib215)] &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[214](#bib.bib214)][[215](#bib.bib215)] &#124;'
- en: '|'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In the following sections, we will introduce research studies that incorporate
    domain knowledge into deep segmentation models. The summary of these works is
    shown in Table [VII](#S4.T7 "TABLE VII ‣ 4.1 General Structures of Deep Learning
    Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍将领域知识融入深度分割模型的研究。这些工作的总结见表 [VII](#S4.T7 "TABLE VII ‣ 4.1 General
    Structures of Deep Learning Models for Object Segmentation in Medical Images ‣
    4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into
    Deep Learning for Medical Image Analysis")。
- en: 4.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  id: totrans-555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 融合自然数据集或其他医学数据集的知识
- en: It is also quite common that deep learning segmentation models are firstly trained
    on a large-scale natural image dataset (e.g., ImageNet) and then fine-tuned on
    the target datasets. Using the above transfer learning strategy to introduce knowledge
    from natural images has demonstrated to achieve a better performance in medical
    image segmentation. Examples can be found in intima-media boundary segmentation
    [[126](#bib.bib126)] and prenatal ultrasound image segmentation[[170](#bib.bib170)].
    Besides ImageNet, [[155](#bib.bib155)] adopts the off-the-shelf DeepLab model
    trained on the PASCAL VOC dataset for anatomical structure segmentation in ultrasound
    images. This pre-trained model is also used in the deep contour-aware network
    (DCAN), which is designed for the gland segmentation in histopathological images
    [[169](#bib.bib169)].
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习分割模型首先在大规模自然图像数据集（如ImageNet）上训练，然后在目标数据集上微调，这种做法也相当常见。利用上述迁移学习策略引入自然图像的知识已被证明能在医疗图像分割中取得更好的性能。可以在内膜-中膜边界分割[[126](#bib.bib126)]和产前超声图像分割[[170](#bib.bib170)]中找到相关示例。除了ImageNet，[[155](#bib.bib155)]还采用了在PASCAL
    VOC数据集上训练的现成DeepLab模型，用于超声图像中的解剖结构分割。这个预训练模型也被用于深度轮廓感知网络（DCAN），该网络旨在用于组织病理图像中的腺体分割[[169](#bib.bib169)]。
- en: Besides using models pre-trained on ‘static’ datasets like ImageNet and PASCAL
    VOC, many deep neural networks, especially those designed for the segmentation
    of 3D medial images, leverage models pre-trained on large-scale video datasets.
    For example, in the automatic segmentation of proximal femur in 3D MRI, the C3D
    pre-trained model is adopted as the encoder of the proposed 3D U-Net [[171](#bib.bib171)].
    Notably, the C3D model is trained on the Sports-1M dataset, which is the largest
    video classification benchmark with 1.1 million sports videos in 487 categories
    [[216](#bib.bib216)].
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用在‘静态’数据集（如ImageNet和PASCAL VOC）上预训练的模型，许多深度神经网络，特别是那些设计用于3D医疗图像分割的模型，还利用在大规模视频数据集上预训练的模型。例如，在3D
    MRI中自动分割近端股骨时，采用了在Sports-1M数据集上预训练的C3D模型作为提出的3D U-Net [[171](#bib.bib171)]的编码器。值得注意的是，C3D模型是在Sports-1M数据集上训练的，该数据集是具有487个类别的110万体育视频的最大视频分类基准[[216](#bib.bib216)]。
- en: In addition to natural images, using knowledge from external medical datasets
    with different modalities and with different diseases is also quite popular.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自然图像，利用来自外部医疗数据集的不同模态和不同疾病的知识也相当流行。
- en: For example, [[172](#bib.bib172)] investigates the transferability of the acquired
    knowledge of a CNN model initially trained for WM hyper-intensity segmentation
    on legacy low-resolution data to new data from the same scanner but with higher
    image resolution. Likewise, the images with different MRI scanners and protocols
    are used in [[34](#bib.bib34)] to help the multi sclerosis segmentation process
    via transfer learning.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[172](#bib.bib172)]研究了最初在旧版低分辨率数据上训练的CNN模型在同一扫描仪但具有更高图像分辨率的新数据上的知识转移能力。同样，在[[34](#bib.bib34)]中使用了不同MRI扫描仪和协议的图像，通过迁移学习帮助多发性硬化症分割过程。
- en: In [[173](#bib.bib173)], the multi-task learning is adopted, where the data
    of brain MRI, breast MRI and cardiac CT angiography (CTA) are used simultaneously
    as multiple tasks. On the other hand, [[174](#bib.bib174)] adopts a multi-modal
    learning structure for organ segmentation. A dual-stream encoder-decoder architecture
    is proposed to learn modality-independent, and thus, generalisable and robust
    features shared among medical datasets with different modalities (MRI and CT images).
    Experimental results prove the effectiveness of this multi-modal learning structure.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[173](#bib.bib173)]中，采用了多任务学习方法，其中同时使用脑部MRI、乳腺MRI和心脏CT血管造影（CTA）的数据作为多个任务。另一方面，[[174](#bib.bib174)]采用了多模态学习结构用于器官分割。提出了一种双流编码器-解码器架构，用于学习模态无关的特征，从而在具有不同模态（MRI和CT图像）的医疗数据集中共享一般化和鲁棒的特征。实验结果证明了这种多模态学习结构的有效性。
- en: Moreover, many works adopt GAN-based models to achieve the domain transformation
    among datasets with different modalities. For example, a model named SeUDA (unsupervised
    domain adaptation) is proposed for the left/right lung segmentation process [[175](#bib.bib175)].
    It leverages the semantic-aware GAN to transfer the knowledge from one chest dataset
    to another. In particular, target images are first mapped towards the source data
    space via the constraint of a semantic-aware GAN loss. Then the segmentation results
    are obtained from the segmentation DNN learned on the source domain. Experimental
    results show that the segmentation performance of SeUDA is highly competitive.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多工作采用基于 GAN 的模型实现不同模态数据集之间的领域转换。例如，提出了一种名为 SeUDA（无监督领域适应）的模型用于左右肺分割过程 [[175](#bib.bib175)]。它利用语义感知
    GAN 将知识从一个胸部数据集转移到另一个数据集。具体而言，目标图像首先通过语义感知 GAN 损失的约束映射到源数据空间。然后，从在源领域上学习的分割 DNN
    中获得分割结果。实验结果表明，SeUDA 的分割性能非常具有竞争力。
- en: More examples of using the knowledge from images with other modalities can be
    found in brain MRI segmentation [[165](#bib.bib165), [182](#bib.bib182)], cardiac
    segmentation [[178](#bib.bib178), [183](#bib.bib183), [181](#bib.bib181), [180](#bib.bib180),
    [184](#bib.bib184)], liver segmentation [[179](#bib.bib179)], lung tumor segmentation
    [[177](#bib.bib177)], cardiac substructure and abdominal multi-organ segmentation
    [[176](#bib.bib176)].
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他模态图像知识的更多例子可以在脑部 MRI 分割 [[165](#bib.bib165), [182](#bib.bib182)]、心脏分割 [[178](#bib.bib178),
    [183](#bib.bib183), [181](#bib.bib181), [180](#bib.bib180), [184](#bib.bib184)]、肝脏分割
    [[179](#bib.bib179)]、肺肿瘤分割 [[177](#bib.bib177)]、心脏子结构和腹部多脏器分割 [[176](#bib.bib176)]
    中找到。
- en: There are also a few works utilize the datasets of other diseases. For instance,
    [[185](#bib.bib185)] first builds a union dataset (3DSeg-8) by aggregating eight
    different 3D medical segmentation datasets, and designs the Med3D network to co-train
    based on 3DSeg-8\. Then the pre-trained models obtained from Med3D are transferred
    into lung and liver segmentation tasks. Experiments show that this method not
    only improves the accuracy, but also accelerates the training convergence speed.
    In addition, the annotated retinal images are used to help the cardiac vessel
    segmentation without annotations [[27](#bib.bib27)]. In particular, a shape-consistent
    generative adversarial network (SC-GAN) is used to generate the synthetic images
    and the corresponding labels. Then the synthetic images are used to train the
    segmentor. Experiments demonstrate the supreme accuracy of coronary artery segmentation.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些工作利用了其他疾病的数据集。例如，[[185](#bib.bib185)] 首先通过整合八个不同的三维医学分割数据集构建了一个联合数据集（3DSeg-8），并设计了
    Med3D 网络，以便基于 3DSeg-8 进行联合训练。然后，将从 Med3D 获得的预训练模型转移到肺部和肝脏分割任务中。实验表明，这种方法不仅提高了准确性，还加速了训练收敛速度。此外，带注释的视网膜图像被用于帮助进行心脏血管分割，而无需注释
    [[27](#bib.bib27)]。特别地，使用了形状一致的生成对抗网络（SC-GAN）来生成合成图像及其对应的标签。然后，使用合成图像来训练分割器。实验表明，冠状动脉分割的准确性非常高。
- en: 4.3 Incorporating Knowledge from Medical Doctors
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 融入医学医生的知识
- en: 'The domain knowledge of medical doctors is also widely adopted when designing
    deep learning models for segmentation tasks in medical images. The types of domain
    knowledge from medical doctors utilized in deep segmentation models can be divided
    into four categories:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计医学图像分割任务的深度学习模型时，医生的领域知识也被广泛采用。医疗医生在深度分割模型中利用的领域知识可以分为四类：
- en: '1.'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: the training pattern,
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练模式，
- en: '2.'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: the general diagnostic patterns they view images,
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们查看图像时的一般诊断模式，
- en: '3.'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: the anatomical priors (e.g., shape, location, topology) of lesions or organs,
    and
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 病变或器官的解剖先验信息（例如，形状、位置、拓扑）以及
- en: '4.'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: other hand-crafted features they give special attention to.
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，他们特别关注的其他手工特征。
- en: 4.3.1 Training Pattern of Medical Doctors
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 医生的训练模式
- en: Similar to disease diagnosis and lesion/organ detection, many research works
    for the object segmentation in medical images also mimic the training pattern
    of medical doctors, which involves assigning tasks that increase in difficulty
    over time. In this process, the curriculum learning technique or its derivative
    methods like self-paced learning (SPL) are also utilized [[217](#bib.bib217)].
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于疾病诊断和病变/器官检测，许多医学图像中的目标分割研究也模仿了医学医生的训练模式，其中包括分配逐渐增加难度的任务。在这个过程中，还利用了课程学习技术或其衍生方法，如自适应学习（SPL）[[217](#bib.bib217)]。
- en: For example, for the segmentation of multi-organ CT images [[186](#bib.bib186)],
    each annotated image is divided into small patches. During the training process,
    patches producing large error by the network are selected with a high probability.
    In this manner, the network can focus sampling on difficult regions, resulting
    in improved performance. In addition, [[30](#bib.bib30)] combines the SPL with
    the active learning for the pulmonary segmentation in 3D images. This system achieves
    the state-of-the-art segmentation results.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于多器官CT图像的分割[[186](#bib.bib186)]，每张标注的图像被分成小块。在训练过程中，网络产生较大误差的块会被高概率选择。这样，网络可以将采样重点放在困难区域，从而提高性能。此外，[[30](#bib.bib30)]将SPL与主动学习相结合，用于3D图像中的肺部分割。该系统实现了最先进的分割结果。
- en: Moreover, a three-stage curriculum learning approach is proposed for liver tumor
    segmentation [[187](#bib.bib187)]. The first stage is performed on the whole input
    volume to initialize the network, then the second stage of learning focuses on
    tumor-specific features by training the network on the tumor patches, and finally
    the network is retrained on the whole input in the third stage. This approach
    exhibits significant improvement when compared with the commonly used cascade
    counterpart in MICCAI 2017 liver tumor segmentation (LiTS) challenge dataset.
    More examples can also be found in left ventricle segmentation [[188](#bib.bib188)],
    finger bones segmentation [[189](#bib.bib189)] and vessel segmentation [[190](#bib.bib190)].
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提出了一种三阶段课程学习方法用于肝肿瘤分割[[187](#bib.bib187)]。第一阶段在整个输入体积上进行以初始化网络，第二阶段的学习集中在肿瘤特定特征上，通过对肿瘤块的训练来完成，最后在第三阶段对整个输入进行重新训练。与MICCAI
    2017肝肿瘤分割（LiTS）挑战数据集中常用的级联方法相比，这种方法表现出显著的改进。更多例子还可以在左心室分割[[188](#bib.bib188)]、手指骨分割[[189](#bib.bib189)]和血管分割[[190](#bib.bib190)]中找到。
- en: 4.3.2 General Diagnostic Patterns of Medical Doctors
  id: totrans-578
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 医学医生的一般诊断模式
- en: In the lesion or organ segmentation tasks, some specific patterns that medical
    doctors adopted are also incorporated into the network.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在病变或器官分割任务中，一些医学医生采用的特定模式也被融入到网络中。
- en: For example, during the visual inspection of CT images, radiologists often change
    window widths and window centers to help to make decision on uncertain nodules.
    This pattern is mimicked in [[191](#bib.bib191)]. In particular, image patches
    of different window widths and window centers are stacked together as the input
    of the deep learning model to gain rich information. The evaluation implemented
    on the public LIDC-IDRI dataset indicates that the proposed method achieves promising
    performance on lung nodule segmentation compared with the state-of-the-art methods.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在CT图像的视觉检查过程中，放射科医生通常会改变窗口宽度和窗口中心，以帮助决策不确定的结节。这种模式在[[191](#bib.bib191)]中被模仿。特别地，不同窗口宽度和窗口中心的图像块被叠加在一起作为深度学习模型的输入，以获取丰富的信息。对公开的LIDC-IDRI数据集的评估表明，该方法在肺结节分割方面的表现优于最先进的方法。
- en: In addition, experienced clinicians generally assess the cardiac morphology
    and function from multiple standard views, using both long-axis (LA) and short-axis
    (SA) images to form an understanding of the cardiac anatomy. Inspired by the above
    observation, a cardiac MR segmentation method is proposed which takes three LA
    and one SA views as the input [[32](#bib.bib32)]. In particular, the features
    are firstly extracted using a multi-view autoencoder (MAE) structure, and then
    are fed in a segmentation network. Experimental results show that this method
    has a superior segmentation accuracy over state-of-the-art methods.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经验丰富的临床医生通常通过多种标准视图来评估心脏的形态和功能，使用长轴（LA）和短轴（SA）图像来形成对心脏解剖学的理解。受上述观察的启发，提出了一种心脏MR分割方法，该方法以三视LA和一视SA作为输入[[32](#bib.bib32)]。特别地，首先使用多视图自编码器（MAE）结构提取特征，然后将这些特征输入到分割网络中。实验结果表明，该方法在分割准确性上优于最先进的方法。
- en: Furthermore, expert manual segmentation usually relies on the boundaries of
    anatomical structures of interest. For instance, radiologists segmenting a liver
    from CT images would usually trace liver edges first, and then deduce the internal
    segmentation mask. Correspondingly, boundary-aware CNNs are proposed in [[192](#bib.bib192)]
    for medical image segmentation. The networks are designed to account for organ
    boundary information, both by providing a special network edge branch and edge-aware
    loss terms. The effectiveness of these boundary aware segmentation networks are
    tested on BraTS 2018 dataset for the task of brain tumor segmentation.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，专家手动分割通常依赖于感兴趣的解剖结构的边界。例如，放射科医生在从CT图像中分割肝脏时，通常会先描绘肝脏的边缘，然后推导出内部分割掩模。相应地，[[192](#bib.bib192)]中提出了边界感知CNN用于医学图像分割。这些网络设计用于考虑器官边界信息，既提供了一个特殊的网络边缘分支，也提供了边缘感知的损失项。这些边界感知分割网络在BraTS
    2018数据集上的脑肿瘤分割任务中进行了有效性测试。
- en: Recently, the diagnostic pattern named as ‘divide-and-conquer manner’ is mimicked
    in the GTV[LN] detection and segmentation method [[193](#bib.bib193)]. Concretely,
    the GTV[LN] is first divided into two subgroups of ‘tumor-proximal’ and ‘tumor-distal’,
    by means of binary of soft distance gating. Then a multi-branch detection-by-segmentation
    network is trained with each branch specializing on learning one GTV[LN] category
    features. After fusing the outs from multi-branch, the method shows significant
    improvements on the mean recall from 72.5% to 78.2%. Another example of using
    the diagnostic pattern of medical doctors can be found in gross tumor and clinical
    target volume segmentation [[194](#bib.bib194)].
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在GTV[LN]检测和分割方法中模仿了名为“分而治之”的诊断模式[[193](#bib.bib193)]。具体来说，GTV[LN]首先通过软距离门控的二值化被分为“肿瘤近端”和“肿瘤远端”两个子组。然后，训练一个多分支的检测-分割网络，每个分支专注于学习一个GTV[LN]类别的特征。经过多分支融合，方法在平均召回率上从72.5%显著提高到78.2%。另一个使用医学医生诊断模式的例子可以在肿瘤总体和临床靶体积分割中找到[[194](#bib.bib194)]。
- en: 4.3.3 Anatomical Priors of Lesions or Organs
  id: totrans-584
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 病变或器官的解剖学先验
- en: 'In comparison to non-medical images, medical images have many anatomical priors
    such as the shape, position and topology of organs or lesions. Experienced medical
    doctors greatly rely on these anatomical priors when they are doing segmentation
    tasks on these images. Incorporating the knowledge of anatomical priors into deep
    learning models has been demonstrated to be an effective way for accurate medical
    image segmentation. Generally speaking, there are three different approaches to
    incorporate these anatomical priors into deep learning models: (1) incorporating
    anatomical priors in the post-processing stage, (2) incorporating anatomical priors
    as regularization terms in the loss function and (3) learning anatomical priors
    via generative models.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 与非医学图像相比，医学图像具有许多解剖学先验，例如器官或病变的形状、位置和拓扑结构。经验丰富的医生在对这些图像进行分割任务时非常依赖这些解剖学先验。将解剖学先验的知识融入深度学习模型已被证明是准确医学图像分割的有效方法。一般来说，将这些解剖学先验融入深度学习模型有三种不同的方法：（1）在后处理阶段融入解剖学先验，（2）将解剖学先验作为损失函数中的正则化项，以及（3）通过生成模型学习解剖学先验。
- en: Incorporating anatomical priors in post-processing stage
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理阶段融入解剖学先验
- en: The first approach is to incorporate the anatomical priors in the post processing
    stage. The result of a segmentation network is often blurry and post-processing
    is generally needed to refine the segmentation result.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法是在后处理阶段中融入解剖学先验。分割网络的结果通常是模糊的，后处理通常是必要的，以细化分割结果。
- en: For example, according to the pathology, most of breast tumors begin in glandular
    tissues and are located inside the mammary layer [[218](#bib.bib218)]. This position
    feature is utilized by [[195](#bib.bib195)] in its post-processing stage where
    a fully connected conditional random field (CRF) model is employed. In particular,
    the position of tumors and their relative locations with mammary layer are added
    as a new term in CRF energy function to obtain better segmentation results.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，根据病理学，大多数乳腺肿瘤始于腺体组织，并位于乳腺层内[[218](#bib.bib218)]。[[195](#bib.bib195)]在其后处理阶段利用了这一位置特征，其中采用了全连接条件随机场（CRF）模型。具体而言，将肿瘤的位置及其与乳腺层的相对位置作为CRF能量函数中的新项，以获得更好的分割结果。
- en: Besides, some research first learn the anatomical priors, and then incorporate
    them into the post-processing stage to help produce anatomically plausible segmentation
    results [[33](#bib.bib33), [196](#bib.bib196), [197](#bib.bib197)]. For instance,
    the latent representation of anatomically correct cardiac shape is first learned
    by using adversarial variational autoencoder (aVAE), then be used to convert erroneous
    segmentation maps into anatomically plausible ones [[196](#bib.bib196)]. Experiments
    manifest that aVAE is able to accommodate any segmentation method, and convert
    its anatomically implausible results to plausible ones without affecting its overall
    geometric and clinical metrics.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究首先学习解剖学先验，然后将其融入后处理阶段，以帮助生成解剖学上合理的分割结果[[33](#bib.bib33), [196](#bib.bib196),
    [197](#bib.bib197)]。例如，通过使用对抗变分自编码器（aVAE）首先学习解剖学上正确的心脏形状，然后用于将错误的分割图转换为解剖学上合理的图[[196](#bib.bib196)]。实验表明，aVAE能够适应任何分割方法，并将其解剖学上不合理的结果转换为合理的结果，而不影响整体的几何和临床指标。
- en: Another example in [[33](#bib.bib33)] introduces the post-processing step based
    on denoising autoencoders (DAE) for lung segmentation. In particular, the DAE
    is trained using only segmentation masks, then the learned representations of
    anatomical shape and topological constraints are imposed on the original segmentation
    results (as shown in Fig. [18](#S4.F18 "Figure 18 ‣ 4.3.3 Anatomical Priors of
    Lesions or Organs ‣ 4.3 Incorporating Knowledge from Medical Doctors ‣ 4 Lesion
    and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")). By applying the Post-DAE on the resulting
    masks from arbitrary segmentation methods, the lung anatomical segmentation of
    X-ray images shows plausible results.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[33](#bib.bib33)]中的另一个例子介绍了基于去噪自编码器（DAE）的后处理步骤，用于肺部分割。具体而言，DAE仅使用分割掩膜进行训练，然后将学习到的解剖形状和拓扑约束应用于原始分割结果（如图[18](#S4.F18
    "Figure 18 ‣ 4.3.3 Anatomical Priors of Lesions or Organs ‣ 4.3 Incorporating
    Knowledge from Medical Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on
    Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")所示）。通过在任意分割方法得到的掩膜上应用Post-DAE，X光图像的肺部解剖分割显示出合理的结果。
- en: '![Refer to caption](img/2960236e9b94ca1410afe2987350d46f.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/2960236e9b94ca1410afe2987350d46f.png)'
- en: 'Figure 18: The example of integrating the shape prior in the post-process stage
    [[33](#bib.bib33)].'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：在后处理阶段集成形状先验的例子[[33](#bib.bib33)]。
- en: Incorporating anatomical priors as regularization terms in the loss function
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 将解剖学先验作为正则化项纳入损失函数中。
- en: 'The second approach is incorporating anatomical priors as regularization terms
    in the objective function of segmentation networks. For example, for the segmentation
    of cardiac MR images, a network called as SRSCN is proposed [[40](#bib.bib40)].
    SRSCN comprises a shape reconstruction neural network (SRNN) and a spatial constraint
    network (SCN). SRNN aims to maintain a realistic shape of the resulting segmentation
    and the SCN is adopted to incorporate the spatial information of the 2D slices.
    The loss of the SRSCN comes from three parts: the segmentation loss, the shape
    reconstruction (SR) loss for shape regularization, and the spatial constraint
    (SC) loss to assist segmentation. The results using images from 45 patients demonstrate
    the effectiveness of the SR and SC regularization terms, and show the superiority
    of segmentation performance of the SRSCN over the conventional schemes.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是将解剖先验作为分割网络目标函数中的正则化项进行整合。例如，在心脏MR图像的分割中，提出了一种名为SRSCN的网络[[40](#bib.bib40)]。SRSCN包括一个形状重建神经网络（SRNN）和一个空间约束网络（SCN）。SRNN旨在保持分割结果的真实形状，而SCN则用于整合2D切片的空间信息。SRSCN的损失来自三个部分：分割损失、用于形状正则化的形状重建（SR）损失，以及辅助分割的空间约束（SC）损失。使用45名患者的图像的结果展示了SR和SC正则化项的有效性，并显示了SRSCN在分割性能上相较于传统方案的优越性。
- en: Another example in this category is the one designed for skin lesion segmentation
    [[201](#bib.bib201)]. In this work, the star shape prior is encoded as a new loss
    term in a FCN to improve its segmentation of skin lesions from their surrounding
    healthy skin. In this manner, the non-star shape segments in FCN prediction maps
    are penalized to guarantee a global structure in segmentation results. The experimental
    results on the ISBI 2017 skin segmentation challenge dataset demonstrate the advantage
    of regularizing FCN parameters by the star shape prior.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别中的另一个例子是针对皮肤病变分割设计的[[201](#bib.bib201)]。在这项工作中，星形状先验被编码为FCN中的一个新损失项，以改善其从周围健康皮肤中分割皮肤病变的能力。通过这种方式，FCN预测图中的非星形状分割会受到惩罚，以确保分割结果的全球结构。ISBI
    2017皮肤分割挑战数据集上的实验结果展示了通过星形状先验正则化FCN参数的优势。
- en: More examples in this category can be found in gland segmentation [[200](#bib.bib200)],
    kidney segmentation [[199](#bib.bib199)], liver segmentation [[202](#bib.bib202)]
    and cardiac segmentation [[198](#bib.bib198), [203](#bib.bib203)].
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别中的更多例子可以在腺体分割[[200](#bib.bib200)]、肾脏分割[[199](#bib.bib199)]、肝脏分割[[202](#bib.bib202)]和心脏分割[[198](#bib.bib198),
    [203](#bib.bib203)]中找到。
- en: Learning anatomical priors via generative models
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成模型学习解剖先验
- en: In the third approach, the anatomical priors (especially the shape prior) are
    learned by some generative models first and then incorporated into segmentation
    networks.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三种方法中，解剖先验（尤其是形状先验）首先由一些生成模型学习，然后被整合到分割网络中。
- en: For example, in the cardiac MR segmentation process, a shape multi-view autoencoder
    (MAE) is proposed to learn shape priors from MR images of multiple standard views
    [[32](#bib.bib32)]. The information encoded in the latent space of the trained
    shape MAE is incorporated into multi-view U-Net (MV U-Net) in the fuse block to
    guide the segmentation process.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在心脏MR分割过程中，提出了一种形状多视角自编码器（MAE）以从多个标准视角的MR图像中学习形状先验[[32](#bib.bib32)]。训练好的形状MAE在潜在空间中编码的信息被整合到多视角U-Net（MV
    U-Net）的融合块中，以指导分割过程。
- en: Another example is shown in [[206](#bib.bib206)], where the shape constrained
    network (SCN) is proposed to incorporate the shape prior into the eye segmentation
    network. More specifically, the prior information is first learned by a VAE-GAN,
    and then the pre-trained encoder and discriminator are leveraged to regularize
    the training process.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子见于[[206](#bib.bib206)]，其中提出了形状约束网络（SCN），以将形状先验整合到眼部分割网络中。更具体地说，先验信息首先由VAE-GAN学习，然后利用预训练的编码器和判别器来正则化训练过程。
- en: More examples can be found in brain geometry segmentation in MRI [[204](#bib.bib204)],
    3D fine renal artery segmentation [[205](#bib.bib205)], overlapping cervical cytoplasms
    segmentation [[207](#bib.bib207)], scapula segmentation [[208](#bib.bib208)],
    liver segmentation [[209](#bib.bib209)], carotid segmentation [[210](#bib.bib210)],
    and head and neck segmentation [[211](#bib.bib211)].
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的例子可以在MRI脑部几何分割[[204](#bib.bib204)]、3D精细肾动脉分割[[205](#bib.bib205)]、重叠的颈部细胞质分割[[207](#bib.bib207)]、肩胛骨分割[[208](#bib.bib208)]、肝脏分割[[209](#bib.bib209)]、颈动脉分割[[210](#bib.bib210)]和头颈部分割[[211](#bib.bib211)]中找到。
- en: 4.3.4 Other Hand-crafted Features
  id: totrans-602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 其他手工特征
- en: 'Besides anatomical priors, some hand-crafted features are also utilized for
    segmentation tasks. Generally speaking, there are two ways to incorporate the
    hand-crafted features into deep learning models: the feature-level fusion and
    the input-level fusion.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解剖先验外，还利用了一些手工特征用于分割任务。一般来说，将手工特征融入深度学习模型有两种方式：特征级融合和输入级融合。
- en: In the feature-level fusion, the hand-crafted features and the features learned
    by the deep models are concatenated. For example, for the gland segmentation in
    histopathology images [[213](#bib.bib213)], two handcrafted features, namely invariant
    LBP features as well as $H\&amp;E$ components, are firstly calculated from images.
    Then these features are concatenated with the features generated from the last
    convolutional layer of the network for predicting the segmentation results. Similarly,
    in the brain structure segmentation [[212](#bib.bib212)], the spatial atlas prior
    is first represented as a vector and then concatenated with the deep features.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征级融合中，手工特征与深度模型学习到的特征进行连接。例如，在组织病理图像中的腺体分割 [[213](#bib.bib213)]，首先从图像中计算两个手工特征，即不变的
    LBP 特征以及 $H\&amp;E$ 组分。然后，这些特征与网络最后一个卷积层生成的特征连接以预测分割结果。同样，在脑部结构分割 [[212](#bib.bib212)]
    中，空间图谱先表示为向量，然后与深度特征连接。
- en: For the input-level fusion, the hand-crafted features are transformed into the
    input patches. Then the original image patches and the feature-transformed patches
    are fed into a deep segmentation network. For example in [[214](#bib.bib214)],
    for automatic brain tumor segmentation in MRI images, three handcrafted features
    (i.e., mean intensity, LBP and HOG) are firstly extracted. Based on these features,
    a SVM is employed to generate confidence surface modality (CSM) patches. Then
    the CSM patches and the original patches from MRI images are fed into a segmentation
    network. This method achieves good performance on BRATS2015 dataset.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入级融合，手工特征被转换为输入补丁。然后，将原始图像补丁和特征转换的补丁输入到深度分割网络中。例如，在 [[214](#bib.bib214)]
    中，对于 MRI 图像中的自动脑肿瘤分割，首先提取三个手工特征（即均值强度、LBP 和 HOG）。基于这些特征，使用 SVM 生成置信度表面模态 (CSM)
    补丁。然后，将 CSM 补丁和 MRI 图像中的原始补丁输入到分割网络中。这种方法在 BRATS2015 数据集上取得了良好的性能。
- en: 'TABLE VIII: The comparison of the quantitative metrics for some medical object
    segmentation methods after incorporating domain knowledge'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：在融合领域知识后，一些医学目标分割方法的定量指标比较
- en: '| Reference |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基线模型/带领域知识 &#124;'
- en: '| Dice score |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| Dice 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[32](#bib.bib32)] | 3D U-Net/MV U-Net | 0.923/0.926 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| [[32](#bib.bib32)] | 3D U-Net/MV U-Net | 0.923/0.926 |'
- en: '| [[205](#bib.bib205)] | V-Net/DPA-DenseBiasNet | 0.787/0.861 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | V-Net/DPA-DenseBiasNet | 0.787/0.861 |'
- en: '| [[177](#bib.bib177)] |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| [[177](#bib.bib177)] |'
- en: '&#124; Masked cycle-GAN/Tumore &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 掩码循环-GAN/肿瘤 &#124;'
- en: '&#124; aware semi-unsupervised &#124;'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知半监督 &#124;'
- en: '| 0.630/0.800 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 0.630/0.800 |'
- en: '| [[174](#bib.bib174)] |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| [[174](#bib.bib174)] |'
- en: '&#124; modality specific method/dual-stream &#124;'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态特定方法/双流 &#124;'
- en: '&#124; encoder-decoder multi-model method &#124;'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器-解码器多模型方法 &#124;'
- en: '| 0.838/0.860 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 0.838/0.860 |'
- en: '| [[40](#bib.bib40)] | U-Net/SRSCN | 0.737/0.830 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bib40)] | U-Net/SRSCN | 0.737/0.830 |'
- en: '| [[27](#bib.bib27)] |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] |'
- en: '&#124; U-Net/SC-GAN &#124;'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net/SC-GAN &#124;'
- en: '| 0.742/0.824 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 0.742/0.824 |'
- en: '| [[183](#bib.bib183)] |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] |'
- en: '&#124; cycle- and shape-consistency GAN &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环和形状一致性 GAN &#124;'
- en: '&#124; trained without/with synthetic data &#124;'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无/有合成数据训练 &#124;'
- en: '| 0.678/0.744 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| 0.678/0.744 |'
- en: In addition, using handcrafted features by input-level fusion is also adopted
    in cell nuclei segmentation [[215](#bib.bib215)]. In particular, as nuclei are
    expected to have an approximately round shape, a map of gradient convergence is
    computed and be used by CNN as an extra channel besides the fluorescence microscopy
    image. Experimental results show higher F1-score when compared with other methods.
    Another example in this category can be found in brain tumor segmentation [[214](#bib.bib214)].
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输入级融合中使用手工特征也应用于细胞核分割 [[215](#bib.bib215)]。特别是，由于细胞核预计具有近似圆形，因此计算梯度收敛图，并由
    CNN 作为额外通道使用，除了荧光显微镜图像。实验结果表明，与其他方法相比，F1 分数更高。另一个此类别的例子可以在脑肿瘤分割 [[214](#bib.bib214)]
    中找到。
- en: 4.4 Summary
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 总结
- en: The aforementioned sections described researches of incorporating domain knowledge
    for object (lesion or organ) segmentation in medical images. The segmentation
    performance of some methods is shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3.4
    Other Hand-crafted Features ‣ 4.3 Incorporating Knowledge from Medical Doctors
    ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"), where the Dice score is used
    with a higher score indicating a better performance.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 上述章节描述了在医学图像中将领域知识应用于对象（病灶或器官）分割的研究。一些方法的分割性能显示在表格 [VIII](#S4.T8 "TABLE VIII
    ‣ 4.3.4 Other Hand-crafted Features ‣ 4.3 Incorporating Knowledge from Medical
    Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis") 中，其中Dice评分用于评估，得分越高表示性能越好。
- en: We can see that similar to disease diagnosis, using information from natural
    images like ImageNet is quite popular for lesion and organ segmentation tasks.
    The reason behind it may be that segmentation can be seen as a specific classification
    problem. Meanwhile, besides the ImageNet, some video datasets can also be utilized
    for segmenting 3D medical images (e.g., [[171](#bib.bib171)]). Using extra medical
    datasets with different modalities has also been proven to be helpful, although
    most applications are limited in using MRI to help segmentation tasks in CT images
    [[174](#bib.bib174)]. Leveraging domain knowledge from medical doctors is also
    widely used in segmentation tasks. In particular, the anatomical priors of organs
    are widely adopted. However, anatomical priors are only suitable for the segmentation
    of organs with fixed shapes like hearts [[32](#bib.bib32)] or lungs [[32](#bib.bib32)].
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与疾病诊断类似，使用自然图像（如ImageNet）的信息在病灶和器官分割任务中非常流行。这背后的原因可能是分割可以被视为一个特定的分类问题。同时，除了ImageNet，一些视频数据集也可以用于分割3D医学图像（例如，[[171](#bib.bib171)]）。利用具有不同模态的额外医学数据集也被证明是有帮助的，尽管大多数应用仍限于使用MRI来辅助CT图像的分割任务[[174](#bib.bib174)]。利用医学医生的领域知识在分割任务中也被广泛使用。特别是，器官的解剖先验被广泛采用。然而，解剖先验仅适用于具有固定形状的器官，如心脏[[32](#bib.bib32)]或肺部[[32](#bib.bib32)]。
- en: 5 Other Medical Applications
  id: totrans-633
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 其他医学应用
- en: In this section, we briefly introduce the works on incorporating domain knowledge
    in other medical images analysis applications, like medical image reconstruction,
    medical image retrieval and medical report generation.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了将领域知识融入其他医学图像分析应用的研究，如医学图像重建、医学图像检索和医学报告生成。
- en: 5.1 Medical Image Reconstruction
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 医学图像重建
- en: The objective of medical image reconstruction is reconstructing a diagnostic
    image from a number of measurements (e.g., X-ray projections in CT or the spatial
    frequency information in MRI). Deep learning based methods have been widely applied
    in this field [[219](#bib.bib219), [220](#bib.bib220)]. It is also quite common
    that external information is incorporated into deep learning models for medical
    image reconstruction.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像重建的目标是从多个测量值（例如，CT中的X射线投影或MRI中的空间频率信息）中重建诊断图像。基于深度学习的方法已在这一领域得到广泛应用[[219](#bib.bib219)、[220](#bib.bib220)]。将外部信息融入深度学习模型进行医学图像重建也是相当常见的。
- en: Some methods incorporate hand-crafted features in the medical image reconstruction
    process. For example, a network model called as DAGAN is proposed for the reconstruction
    of compressed sensing magnetic resonance imaging (CS-MRI) [[221](#bib.bib221)].
    In the DAGAN, to better preserve texture and edges in the reconstruction process,
    the adversarial loss is coupled with a content loss. In addition, the frequency-domain
    information is incorporated to enforce similarity in both the image and frequency
    domains. Experimental results show that the DAGAN method provides superior reconstruction
    with preserved perceptual image details.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法在医学图像重建过程中融合了手工特征。例如，提出了一种名为DAGAN的网络模型，用于压缩感知磁共振成像（CS-MRI）的重建[[221](#bib.bib221)]。在DAGAN中，为了更好地保留重建过程中的纹理和边缘，敌对损失与内容损失相结合。此外，还结合了频域信息，以强制图像和频域之间的相似性。实验结果表明，DAGAN方法在保留感知图像细节方面提供了优越的重建效果。
- en: In [[222](#bib.bib222)], a new image reconstruction method is proposed to solve
    the limited-angle and limited sources breast cancer diffuse optical tomography
    (DOT) image reconstruction problem in a strong scattering medium. By adaptively
    focusing on important features and filtering irrelevant and noisy ones using the
    Fuzzy Jaccard loss, the network is able to reduce false positive reconstructed
    pixels and reconstruct more accurate images.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a GAN-based method is proposed to recover MRI images of the target
    contrast [[223](#bib.bib223)]. The method simultaneously leverages the relatively
    low-spatial-frequency information available in the collected evidence for the
    target contrast and the relatively high-spatial frequency information available
    in the source contrast. Demonstrations on brain MRI datasets indicate the proposed
    method outperforms state-of-the-art reconstruction methods, with enhanced recovery
    of high-frequency tissue structure, and improved reliability against feature leakage
    or loss.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Medical Image Retrieval
  id: totrans-640
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hospitals are producing large amount of imaging data and the development
    of medical image retrieval, especially the content based image retrieval (CBIR)
    systems can be of great help to aid clinicians in browsing these large datasets.
    Deep learning methods have been applied to CBIR and have achieved high performance
    due to their superior capability for extracting features automatically.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: It is also quite common that these deep learning models for CBIR utilize external
    information beyond the given medical datasets. Some methods adopt the transfer
    learning to utilize the knowledge from natural images or external medical datasets
    [[224](#bib.bib224), [225](#bib.bib225), [226](#bib.bib226)]. For example, the
    VGG model pre-trained based on ImageNet is used in brain tumor retrieval process
    [[226](#bib.bib226)], where a block-wise fine-tuning strategy is proposed to enhance
    the retrieval performance on the T1-weighted CE-MRI dataset. Another example can
    be found in x-ray image retrieval process [[225](#bib.bib225)], where a model
    pre-trained on the large augmented dataset is fine-tuned on the target dataset
    to extract general features.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: Besides, as features play an important role in the similarly analysis in CBIR,
    some methods fuse prior features with deep features. In particular, in the chest
    radiograph image retrieval process, the decision values of binary features and
    texture features are combined with the deep features in the form of decision-level
    fusion [[227](#bib.bib227)]. Similarly, the metadata such as patients’ age and
    gender is combined with the image-based features extracted from deep CNN for X-ray
    chest pathology image retrieval [[228](#bib.bib228)]. Furthermore, the features
    extracted from saliency areas can also be injected into the features extracted
    from the whole image for the high retrieval accuracy [[224](#bib.bib224)].
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Medical Report Generation
  id: totrans-644
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, deep learning models for image captioning have been successfully applied
    for automatic generation of medical reports [[229](#bib.bib229), [230](#bib.bib230)].
    It is also found that incorporating external knowledge can help deep learning
    models to generate better medical reports.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: For example, some methods try to incorporate specific or general patterns that
    doctors adopt when writing reports. For example, radiologists generally write
    reports using certain templates. Therefore, some templates are used during the
    sentence generation process [[231](#bib.bib231), [232](#bib.bib232)]. Furthermore,
    as the explanation given by doctors is fairly simple and phrase changing does
    not change their meaning, a model-agnostic method is presented to learn the short
    text description to explain this decision process [[233](#bib.bib233)].
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, radiologists follow some procedures when writing reports: they
    generally first check a patient’s images for abnormal findings, then write reports
    by following certain templates, and adjust statements in the templates for each
    individual case when necessary [[234](#bib.bib234)]. This process is mimicked
    in [[232](#bib.bib232)], which first transfers the visual features of medical
    images into an abnormality graph, then retrieves text templates based on the abnormalities
    and their attributes for chest X-ray images.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: In [[235](#bib.bib235)], a pre-constructed graph embedding module (modeled with
    a graph CNN) on multiple disease findings is utilized to assist the generation
    of reports. The incorporation of knowledge graph allows for dedicated feature
    learning for each disease finding and the relationship modeling between them.
    Experiments on the publicly accessible dataset (IU-RR) demonstrate the superior
    performance of the method integrated with the proposed graph module.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: 6 Research Challenges and Future Directions
  id: totrans-649
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aforementioned sections reviewed research studies on deep learning models
    that incorporate medical domain knowledge for various tasks. Although using medical
    domain knowledge in deep learning models is quite popular, there are still many
    difficulties about the selection, representation and incorporating method of medical
    domain knowledge. In the following sections, we summarize challenges and future
    directions in this area.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 The Challenges Related to the Identification and Selection of Medical Domain
    Knowledge
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying medical domain knowledge is not an easy task. Firstly, the experiences
    of medical doctors are generally subjective and fuzzy. Not all medical doctors
    can give accurate and objective descriptions on what kinds of experiences they
    have leveraged to finish a given task. In addition, experiences of medical doctors
    can vary significantly or even contradictory to each other. Furthermore, medical
    doctors generally utilize many types of domain knowledge simultaneously. Finally,
    currently the medical domain knowledge is identified manually, and there is no
    existing work on the automatically and comprehensively identifying medical domain
    knowledge for a given area.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: One solution to the automatic identifying medical knowledge is through text
    mining techniques on the guidelines, books, and medical reports related to different
    medical areas. Guidelines or books are more robust than individual experiences.
    Medical reports generally contain specific terms (usually adjectives) that describe
    the characteristics of tumors. These terms, containing important information to
    help doctors to make diagnosis, can potentially be beneficial for deep learning
    models.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: Besides the identification of medical domain knowledge, how to select appropriate
    knowledge to help image analysis tasks is also challenging. It should be noted
    that a common practice of medical doctors may not be able to help deep learning
    models because *the domain knowledge might be learned by the deep learning model
    from training data.* We believe that the knowledge that is not easily learned
    by a deep learning model can greatly help the model to improve its performance.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 The Challenges Related to the Representation of Medical Domain Knowledge
  id: totrans-655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original domain knowledge of medical doctors is generally in the form of
    descriptive sentences like ‘we will focus more on the margin areas of a tumor
    to determine whether it is benign or malignant’, or ‘we often compare bilateral
    images to make decision’. How to transform the knowledge into appropriate representations
    and incorporate it into deep learning models need a careful design.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: There are generally four ways to represent a certain type of medical domain
    knowledge. One is to represent knowledge as patches or highlighted images (as
    in [[94](#bib.bib94)]). This is generally used when doctors pay more attention
    to specific areas. The second approach is to represent knowledge as feature vectors
    [[85](#bib.bib85)]. This way is suitable when the selected domain knowledge can
    be described as certain features. The third approach is to represent domain knowledge
    as extra labels [[59](#bib.bib59), [52](#bib.bib52)], which is suitable for the
    knowledge in clinical reports or extra feature attributes of diseases. The last
    approach is to embed medical domain knowledge in network structure design, which
    is suitable to represent high-level domain knowledge like the training pattern
    and diagnostic pattern of medical doctors[[69](#bib.bib69), [134](#bib.bib134),
    [135](#bib.bib135)].
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 The Challenges Related to the Incorporating Methods of Medical Domain Knowledge
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, there are four ways to incorporate medical domain knowledge. The
    first is to transform the knowledge into certain patches or highlighted images
    and put them as extra inputs [[54](#bib.bib54)]. The second approach is via concatenation
    [[213](#bib.bib213)]. The domain knowledge are generally transformed into feature
    vectors and concatenated with those extracted by deep learning models. The third
    way is the attention mechanism [[38](#bib.bib38)]. The approach is applicable
    when doctors focus on certain areas of medical images or focus on certain time
    slots on medical videos. The last one is to learn the domain knowledge by using
    some specific network structures like generative models [[32](#bib.bib32), [206](#bib.bib206)].
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the existing works only incorporate a single type of medical
    domain knowledge, or a few types of medical domain knowledge of the same modality
    (e.g., a number of hand-crafted features). In practice, experienced medical doctors
    usually combine different experience in different stages.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: There are some researches that simultaneously introduce high-level domain knowledge
    (e.g., diagnostic pattern, training pattern) and the low-level one (e.g., hand-crafted
    features, anatomical priors). In particular, the high-level domain knowledge is
    incorporated as input images, and low-level one is learned by using specific network
    structures [[32](#bib.bib32)]. In addition, besides incorporating into network
    directly, the information from low-level domain knowledge can also be used to
    design the training orders when combined with the easy-to-hard training pattern
    [[29](#bib.bib29)]. We believe that simultaneously incorporating multiple kinds
    of medical domain knowledge can better help deep learning models in various medical
    applications.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Future Research Directions
  id: totrans-662
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the above challenges, there are several directions that we feel need
    further investigation in the future.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation is developed to transfer the information from a source domain
    to a target one. Via techniques like adversarial learning [[153](#bib.bib153)],
    domain adaptation is able to narrow the domain shift between the source domain
    and the target one in input space [[236](#bib.bib236)], feature space [[237](#bib.bib237),
    [238](#bib.bib238)] and output space [[239](#bib.bib239), [240](#bib.bib240)].
    It can be naturally adopted to transfer knowledge of one medical dataset to another
    [[181](#bib.bib181)], even when they have different imaging modes or belong to
    different diseases [[172](#bib.bib172), [177](#bib.bib177)].
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: In addition, unsupervised domain adaptation (UDA) is a promising avenue to enhance
    the performance of deep neural networks on the target domain, using labels only
    from the source domain. This is especially useful for medical field, as annotating
    the medical images is quite labor-intensive and the lack of annotations is quite
    common in medical datasets. Some examples have demonstrated the effectiveness
    of UDA in disease diagnosis and organ segmentation [[175](#bib.bib175), [93](#bib.bib93),
    [241](#bib.bib241), [242](#bib.bib242)], but further depth study needs to be implemented
    in the future.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge graph
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: We believe that the knowledge graph [[243](#bib.bib243)], with the character
    of embedding different types of knowledge, is a generic and flexible approach
    to incorporate multi-modal medical domain knowledge. Although rarely used at present,
    it also shows advantage in medical image analysis tasks, especially in medical
    report generation [[232](#bib.bib232)]. We believe that more types of knowledge
    graph can be used to represent and learn domain knowledge in medical image analysis
    tasks.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: According to different relationships in graphs, there are three possible types
    of knowledge graphs can be established. The first knowledge graph reflects the
    relationship among different kinds of medical domain knowledge with respect to
    a certain disease. This knowledge graph can help us identify a few key types of
    knowledge that may help to improve the performance deep learning models. The second
    type of knowledge graph may reflects the relationship among different diseases.
    This knowledge graph can help us to find out the potential domain knowledge from
    other related diseases. The third type one can describe the relationship among
    medical datasets. These datasets can belong to different diseases and in different
    imaging modes (e.g., CT, MRI, ultrasound). This type of knowledge graph will help
    to identify the external datasets that may help to improve the performance of
    the current deep learning model.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: The generative models
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: The generative models, like GAN and AE, have shown great promise to be applied
    to incorporate medical domain knowledge into deep learning models, especially
    for segmentation tasks. GAN has shown its capability to leverage information from
    extra datasets with different imaging modes (e.g., using a MRI dataset to help
    segmenting CT images [[175](#bib.bib175), [177](#bib.bib177)]). In addition, GAN
    is able to learn important features contained in medical images in a weakly or
    fully unsupervised manner and therefore is quite suitable for medical image analysis.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: AE-based models have already achieved a great success in extracting features,
    especially the shape priors in objects like organs or lesions in a fully unsupervised
    manner [[32](#bib.bib32), [206](#bib.bib206)]. The features learning by AE can
    also be easily integrated into the training process of networks.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture search (NAS)
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: At last, we mentioned in the previous section that one challenge is to find
    appropriate network architectures to incorporate medical domain knowledge. We
    believe one approach to address this problem is the technique of network architecture
    search (NAS). NAS has demonstrated its capability to automatically find a good
    network architecture in many computer vision tasks [[244](#bib.bib244)] and has
    a great promise in the medical domain [[245](#bib.bib245)]. For instance, when
    some hand-crafted features are used as the domain knowledge, with the help of
    NAS, a network structure can be identified with the special connections between
    domain knowledge features and deep features. In addition, instead of designing
    the feature fusion method (feature-level fusion, decision-level fusion or input-level
    fusion) for these two kinds of features, the integrating phase and integrating
    intensity of these two kinds of features can also be determined during the searching
    process.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-675
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we give a comprehensive survey on incorporating medical domain
    knowledge into deep learning models for various medical image analysis tasks ranging
    from disease diagnosis, lesion, organ and abnormality detection to lesion and
    organ segmentation. In addition, some other tasks such as medical image reconstruction,
    medical image retrieval and medical report generation are also included. For each
    task, we first introduce different types of medical domain knowledge, and then
    review some works of introducing domain knowledge into target tasks by using different
    incorporating methods. From this survey, we can see that with appropriate integrating
    methods, different kinds of domain knowledge can help deep learning models to
    better accomplish corresponding tasks.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: Besides reviewing current works on incorporating domain knowledge into deep
    learning models, we also summarize challenges of using medical domain knowledge,
    and introduce the identification, selection, representation and incorporating
    method of medical domain knowledge. Finally, we give some future directions of
    incorporating domain knowledge for medical image analysis tasks.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-678
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    [grant numbers 61976012, 61772060]; the National Key R&D Program of China [grant
    number 2017YFB1301100]; and the CERNET Innovation Project [grant number NGII20170315].
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Esteva, B. Kuprel, R. A. Novoa, J. M. Ko, S. M. Swetter, H. M. Blau,
    and S. Thrun, “Dermatologist-level classification of skin cancer with deep neural
    networks,” *Nature*, vol. 542, no. 7639, pp. 115–118, 2017.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Y. Shin, S. Lee, I. D. Yun, S. M. Kim, and K. M. Lee, “Joint weakly
    and semi-supervised deep learning for localization and classification of masses
    in breast ultrasound images,” *IEEE TMI*, vol. 38, no. 3, pp. 762–774, 2019.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z. Zhou, J. Y. Shin, L. Zhang, S. R. Gurudu, M. B. Gotway, and J. Liang,
    “Fine-tuning convolutional neural networks for biomedical image analysis: Actively
    and incrementally,” *CVPR2017*, pp. 4761–4772, 2017.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] W. Zhu, C. Liu, W. Fan, and X. Xie, “Deeplung: Deep 3d dual path nets for
    automated pulmonary nodule detection and classification,” *workshop on applications
    of computer vision*, pp. 673–681, 2018.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness of
    data,” *IEEE Intelligent Systems*, vol. 24, no. 2, pp. 8–12, 2009.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns, R. C.
    Green, D. Harvey, C. R. Jack, W. Jagust, J. C. Morris, R. C. Petersen, J. Salazar,
    A. J. Saykin, L. M. Shaw, A. W. Toga, and J. Q. Trojanowski, “The alzheimer’s
    disease neuroimaging initiative 3: Continued innovation for clinical trial improvement,”
    *Alzheimer’s and Dementia*, vol. 13, no. 5, pp. 561 – 571, 2017\. [Online]. Available:
    http://www.sciencedirect.com/science/article/pii/S1552526016330722'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,
    J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto *et al.*, “The autism
    brain imaging data exchange: towards a large-scale evaluation of the intrinsic
    brain architecture in autism,” *Molecular psychiatry*, vol. 19, no. 6, pp. 659–667,
    2014.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. MICCAI, “Automated cardiac diagnosis challenge (acdc),” 2017\. [Online].
    Available: https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. Summers, “Chestx-ray14:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” 09 2017.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *et al.*, “The
    lung image database consortium (lidc) and image database resource initiative (idri):
    a completed reference database of lung nodules on ct scans,” *Medical physics*,
    vol. 38, no. 2, pp. 915–931, 2011.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. T. B. v. G. Colin Jacobs, Arnaud Arindra Adiyoso Setio, “Lung nodule
    analysis 2016,” 2016\. [Online]. Available: https://luna16.grand-challenge.org/'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang,
    K. Zhu, D. Laird, R. L. Ball *et al.*, “Mura: Large dataset for abnormality detection
    in musculoskeletal radiographs,” *arXiv preprint arXiv:1712.06957*, 2017.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara,
    C. Berger, S. M. Ha, M. Rozycki *et al.*, “Identifying the best machine learning
    algorithms for brain tumor segmentation, progression assessment, and overall survival
    prediction in the brats challenge,” *arXiv preprint arXiv:1811.02629*, 2018.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Hoover, V. Kouznetsova, and M. Goldbaum, “Locating blood vessels in
    retinal images by piecewise threshold probing of a matched filter response,” *IEEE
    Transactions on Medical imaging*, vol. 19, no. 3, pp. 203–210, 2000.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Heath, K. Bowyer, D. Kopans, R. Moore, and P. Kegelmeyer, “The digital
    database for screening mammography,” *Proceedings of the Fourth International
    Workshop on Digital Mammography*, 01 2000.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. Yan, X. Wang, L. Lu, and R. M. Summers, “Deeplesion: automated mining
    of large-scale lesion annotations and universal lesion detection with deep learning,”
    *Journal of Medical Imaging*, vol. 5, no. 3, p. 036501, 2018.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. K. T. Alexander A., “Efficient and generalizable statistical models
    of shape and appearance for analysis of cardiac mri,” *Medical Image Analysis*,
    vol. 12, no. 3, pp. 335 – 357, 2008\. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1361841508000029'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
    B. Helba, A. Kalloo, K. Liopyris, M. Marchetti *et al.*, “Skin lesion analysis
    toward melanoma detection 2018: A challenge hosted by the international skin imaging
    collaboration (isic),” *arXiv preprint arXiv:1902.03368*, 2019.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Chen, X. J. Qi, J. Z. Cheng, and P. A. Heng, “Deep contextual networks
    for neuronal structure segmentation,” in *Thirtieth AAAI conference on artificial
    intelligence*, 2016.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] F. Ciompi, B. de Hoop, S. J. van Riel, K. Chung, E. T. Scholten, M. Oudkerk,
    P. A. de Jong, M. Prokop, and B. van Ginneken, “Automatic classification of pulmonary
    peri-fissural nodules in computed tomography using an ensemble of 2d views and
    a convolutional neural network out-of-the-box,” *Medical Image Analysis*, vol. 26,
    no. 1, pp. 195–202, 2015.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    and R. M. Summers, “Deep convolutional neural networks for computer-aided detection:
    Cnn architectures, dataset characteristics and transfer learning,” *IEEE TMI*,
    vol. 35, no. 5, pp. 1285–1298, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] B. J. Erickson, P. Korfiatis, Z. Akkus, and T. L. Kline, “Machine learning
    for medical imaging,” *Radiographics*, vol. 37, no. 2, pp. 505–515, 2017.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV 2017*, 2017,
    pp. 2223–2232.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] B. Huynh, K. Drukker, and M. Giger, “Mo-de-207b-06: Computer-aided diagnosis
    of breast ultrasound images using transfer learning from deep convolutional neural
    networks,” *Medical physics*, vol. 43, no. 6Part30, pp. 3705–3705, 2016.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. K. Samala, H.-P. Chan, L. Hadjiiski, M. A. Helvie, C. D. Richter, and
    K. H. Cha, “Breast cancer diagnosis in digital breast tomosynthesis: effects of
    training sample size on multi-stage transfer learning using deep neural nets,”
    *IEEE TMI*, vol. 38, no. 3, pp. 686–696, 2018.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] F. Yu, J. Zhao, Y. Gong, Z. Wang, Y. Li, F. Yang, B. Dong, Q. Li, and
    L. Zhang, “Annotation-free cardiac vessel segmentation via knowledge transfer
    from retinal images,” in *MICCAI2019*, 2019, pp. 714–722.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] G. Maicas, A. P. Bradley, J. C. Nascimento, I. Reid, and G. Carneiro,
    “Training medical image analysis systems like radiologists,” in *MICCAI2018*,
    2018, pp. 546–554.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Tang, X. Wang, A. P. Harrison, L. Lu, J. Xiao, and R. M. Summers, “Attention-guided
    curriculum learning for weakly supervised classification and localization of thoracic
    diseases on chest radiographs,” in *International Workshop on Machine Learning
    in Medical Imaging*.   Springer, 2018, pp. 249–258.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Wang, Y. Lu, B. Wu, T. Chen, D. Z. Chen, and J. Wu, “Deep active self-paced
    learning for accurate pulmonary nodule segmentation,” in *MICCAI2018*.   Springer,
    2018, pp. 723–731.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S.-M. Hsu, W.-H. Kuo, F.-C. Kuo, and Y.-Y. Liao, “Breast tumor classification
    using different features of quantitative ultrasound parametric images,” *International
    journal of computer assisted radiology and surgery*, vol. 14, no. 4, pp. 623–633,
    2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C. Chen, C. Biffi, G. Tarroni, S. E. Petersen, W. Bai, and D. Rueckert,
    “Learning shape priors for robust cardiac mr segmentation from multi-view images.”
    in *Medical Image Computing and Computer-Assisted Intervention*, 2019, pp. 523–531.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. J. Larrazabal, C. E. Martinez, and E. Ferrante, “Anatomical priors
    for image segmentation via post-processing with denoising autoencoders,” in *MICCAI2019*,
    2019, pp. 585–593.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Valverde, M. Salem, M. Cabezas, D. Pareto, J. C. Vilanova, L. Ramiotorrenta,
    A. Rovira, J. Salvi, A. Oliver, and X. Llado, “One-shot domain adaptation in multiple
    sclerosis lesion segmentation using convolutional neural networks,” *NeuroImage:
    Clinical*, vol. 21, p. 101638, 2019.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. Ji, S. Basodi, X. Xiao, and Y. Pan, “Infant sound classification on
    multi-stage cnns with hybrid features and prior knowledge,” in *International
    Conference on AI and Mobile Services*.   Springer, 2020, pp. 3–16.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Xie, J. Zhang, S. Liu, W. Cai, and Y. Xia, “Lung nodule classification
    by jointly using visual descriptors and deep features,” in *Medical Computer Vision
    and Bayesian and Graphical Models for Biomedical Imaging*.   Springer, 2016, pp.
    116–125.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang, “Diagnose
    like a radiologist: Attention guided convolutional neural network for thorax disease
    classification,” *arXiv preprint arXiv:1801.09927*, 2018.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] L. Li, M. Xu, X. Wang, L. Jiang, and H. Liu, “Attention based glaucoma
    detection: A large-scale database and cnn model,” in *CVPR2019*, 2019, pp. 10 571–10 580.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Chen, J. Qin, X. Ji, B. Lei, T. Wang, D. Ni, and J.-Z. Cheng, “Automatic
    scoring of multiple semantic attributes with multi-task feature leverage: A study
    on pulmonary nodules in ct images,” *IEEE TMI*, vol. 36, no. 3, pp. 802–814, 2016.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Q. Yue, X. Luo, Q. Ye, L. Xu, and X. Zhuang, “Cardiac segmentation from
    lge mri using deep neural network incorporating shape and spatial priors,” in
    *MICCAI2019*.   Springer, 2019, pp. 559–567.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. G. Debelee, F. Schwenker, A. Ibenthal, and D. Yohannes, “Survey of
    deep learning in breast cancer image analysis,” *Evolving Systems*, pp. 1–21,
    2019.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical Image Analysis*, vol. 42, pp. 60–88, 2017.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,”
    *Annual review of biomedical engineering*, vol. 19, pp. 221–248, 2017.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] K. Suzuki, “Survey of deep learning applications to medical image analysis,”
    *Medical Imaging Technology*, vol. 35, no. 4, pp. 212–226, 2017.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *neural information processing systems*,
    vol. 141, no. 5, pp. 1097–1105, 2012.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” *CVPR2015*,
    pp. 1–9, 2015.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *ICLR*, 2015.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *CVPR2016*, pp. 770–778, 2016.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] G. Huang, Z. Liu, L. V. Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” *CVPR2017*, pp. 2261–2269, 2017.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Y. Kim, H. E. Lee, Y. H. Choi, S. J. Lee, and J. S. Jeon, “Cnn-based
    diagnosis models for canine ulcerative keratitis,” *Scientific reports*, vol. 9,
    no. 1, pp. 1–7, 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Li, L. Shen, X. Xie, S. Huang, Z. Xie, X. Hong, and J. Yu, “Multi-resolution
    convolutional networks for chest x-ray radiograph based lung nodule detection,”
    *Artificial intelligence in medicine*, p. 101744, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Liu, W. Li, N. Zhao, K. Cao, Y. Yin, Q. Song, H. Chen, and X. Gong,
    “Integrate domain knowledge in training cnn for ultrasonography breast cancer
    diagnosis,” in *MICCAI2018*, 2018, pp. 868–875.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Mitsuhara, H. Fukui, Y. Sakashita, T. Ogata, T. Hirakawa, T. Yamashita,
    and H. Fujiyoshi, “Embedding human knowledge in deep neural network via attention
    map,” *arXiv preprint arXiv:1905.03540*, 2019.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. J. Fulham, and W. Cai,
    “Knowledge-based collaborative deep learning for benign-malignant lung nodule
    classification on chest ct,” *IEEE TMI*, vol. 38, no. 4, pp. 991–1004, 2019.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Bar, I. Diamant, L. Wolf, and H. Greenspan, “Deep learning with non-medical
    training used for chest pathology identification,” in *Medical Imaging 2015: Computer-Aided
    Diagnosis*, vol. 9414.   International Society for Optics and Photonics, 2015,
    p. 94140V.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. Wimmer, S. Hegenbart, A. Vécsei, and A. Uhl, “Convolutional neural
    network architectures for the automated diagnosis of celiac disease,” in *International
    Workshop on Computer-Assisted and Robotic Endoscopy*.   Springer, 2016, pp. 104–113.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Cao, S. Bernard, L. Heutte, and R. Sabourin, “Improve the performance
    of transfer learning without fine-tuning using dissimilarity-based multi-view
    learning for breast cancer histology images,” in *International conference image
    analysis and recognition*, 2018, pp. 779–787.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” in *CVPR2017*, 2017, pp. 2097–2106.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S. Hussein, K. Cao, Q. Song, and U. Bagci, “Risk stratification of lung
    nodules using 3d cnn-based multi-task learning,” *international conference information
    processing*, pp. 249–260, 2017.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] W. Li, J. Li, K. V. Sarma, K. C. Ho, S. Shen, B. S. Knudsen, A. Gertych,
    and C. W. Arnold, “Path r-cnn for prostate cancer diagnosis and gleason grading
    of histological images,” *IEEE TMI*, vol. 38, no. 4, pp. 945–954, 2018.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B. Q. Huynh, H. Li, and M. L. Giger, “Digital mammographic tumor classification
    using transfer learning from deep convolutional neural networks,” *Journal of
    Medical Imaging*, vol. 3, no. 3, p. 034501, 2016.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] O. Hadad, R. Bakalo, R. Ben-Ari, S. Hashoul, and G. Amit, “Classification
    of breast lesions using cross-modal deep learning,” in *ISBI 2017*.   IEEE, 2017,
    pp. 109–112.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. K. Samala, H.-P. Chan, L. M. Hadjiiski, M. A. Helvie, K. H. Cha, and
    C. D. Richter, “Multi-task transfer learning deep convolutional neural network:
    application to computer-aided diagnosis of breast cancer on mammograms,” *Physics
    in Medicine & Biology*, vol. 62, no. 23, p. 8894, 2017.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Azizi, P. Mousavi, P. Yan, A. Tahmasebi, J. T. Kwak, S. Xu, B. Turkbey,
    P. Choyke, P. Pinto, B. Wood *et al.*, “Transfer learning from rf to b-mode temporal
    enhanced ultrasound features for prostate cancer detection,” *International journal
    of computer assisted radiology and surgery*, vol. 12, no. 7, pp. 1111–1121, 2017.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Li, G. Qin, Q. He, L. Sun, H. Zeng, Z. He, W. Chen, X. Zhen, and L. Zhou,
    “Digital breast tomosynthesis versus digital mammography: integration of image
    modalities enhances deep learning-based breast mass classification,” *European
    radiology*, vol. 30, no. 2, pp. 778–788, 2020.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Han, J. Wang, W. Zhou, C. Chang, S. Ying, and J. Shi, “Deep doubly
    supervised transfer network for diagnosis of breast cancer with imbalanced ultrasound
    imaging modalities,” in *International Conference on Medical Image Computing and
    Computer-Assisted Intervention*.   Springer, 2020, pp. 141–149.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, and P.-A. Heng, “Canet: Cross-disease
    attention network for joint diabetic retinopathy and diabetic macular edema grading,”
    *IEEE TMI*, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Liao, Y. Ding, Z. L. Jiang, X. Wang, C. Zhang, and Q. Zhang, “Multi-task
    deep convolutional neural network for cancer diagnosis,” *Neurocomputing*, vol.
    348, pp. 66–73, 2019.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Medical-based deep curriculum learning
    for improved fracture classification,” in *MICCAI2019*.   Springer, 2019, pp.
    694–702.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Haarburger, M. Baumgartner, D. Truhn, M. Broeckmann, H. Schneider,
    S. Schrading, C. Kuhl, and D. Merhof, “Multi scale curriculum cnn for context-aware
    breast mri malignancy classification,” in *MICCAI2019*.   Springer, 2019, pp.
    495–503.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] R. Zhao, X. Chen, Z. Chen, and S. Li, “Egdcl: An adaptive curriculum learning
    framework for unbiased glaucoma diagnosis,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 190–205.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Curriculum learning for annotation-efficient
    medical image analysis: scheduling data with prior knowledge and uncertainty,”
    *arXiv preprint arXiv:2007.16102*, 2020.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Wei, A. Suriawinata, B. Ren, X. Liu, M. Lisovsky, L. Vaickus, C. Brown,
    M. Baker, M. Nasir-Moin, N. Tomita *et al.*, “Learn like a pathologist: Curriculum
    learning by annotator agreement for histopathology image classification,” *arXiv
    preprint arXiv:2009.13698*, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, and Y. Yu,
    “Curriculum feature alignment domain adaptation for epithelium-stroma classification
    in histopathological images,” *IEEE Journal of Biomedical and Health Informatics*,
    2020.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] I. González-Díaz, “Dermaknet: Incorporating the knowledge of dermatologists
    to convolutional neural networks for skin lesion diagnosis,” *IEEE journal of
    biomedical and health informatics*, vol. 23, no. 2, pp. 547–559, 2018.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] K. Wang, X. Zhang, S. Huang, F. Chen, X. Zhang, and L. Huangfu, “Learning
    to recognize thoracic disease in chest x-rays with knowledge-guided deep zoom
    neural networks,” *IEEE Access*, vol. 8, pp. 159 790–159 805, 2020.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Huang, Y. Fang, M. Lu, F. Yan, J. Yang, and Y. Xu, “Dual-ray net: Automatic
    diagnosis of thoracic diseases using frontal and lateral chest x-rays,” *Journal
    of Medical Imaging and Health Informatics*, vol. 10, no. 2, pp. 348–355, 2020.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Yang, Z. Cao, Y. Zhang, M. Han, J. Xiao, L. Huang, S. Wu, J. Ma, and
    P. Chang, “Momminet: Mammographic multi-view mass identification networks,” in
    *International Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 200–210.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Q. Liu, L. Yu, L. Luo, Q. Dou, and P. A. Heng, “Semi-supervised medical
    image classification with relation-driven self-ensembling model,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Fang, C. Wang, S. Li, H. Rabbani, X. Chen, and Z. Liu, “Attention to
    lesion: Lesion-aware convolutional neural network for retinal optical coherence
    tomography image classification,” *IEEE TMI*, vol. 38, no. 8, pp. 1959–1970, Aug
    2019.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Cui, Y. Xu, W. Li, L. Wang, and H. Duh, “Collaborative learning of
    cross-channel clinical attention for radiotherapy-related esophageal fistula prediction
    from ct,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 212–220.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] X. Xie, J. Niu, X. Liu, Q. Li, Y. Wang, J. Han, and S. Tang, “Dg-cnn:
    Introducing margin information into cnn for breast cancer diagnosis in ultrasound
    images,” *Journal of Computer Science and Engineering*, 2020.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] B. Zhang, Z. Wang, J. Gao, C. Rutjes, K. Nufer, D. Tao, D. D. Feng, and
    S. W. Menzies, “Short-term lesion change detection for melanoma screening with
    novel siamese neural network.” *IEEE transactions on medical imaging*, 2020.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M. Moradi, Y. Gur, H. Wang, P. Prasanna, and T. Syeda-Mahmood, “A hybrid
    learning approach for semantic labeling of cardiac ct slices and recognition of
    body position,” in *ISBI 2016*.   IEEE, 2016, pp. 1418–1421.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Majtner, S. Yildirim-Yayilgan, and J. Y. Hardeberg, “Combining deep
    learning and hand-crafted features for skin lesion classification,” in *2016 Sixth
    International Conference on Image Processing Theory, Tools and Applications (IPTA)*.   IEEE,
    2016, pp. 1–6.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Xie, J. Zhang, Y. Xia, M. Fulham, and Y. Zhang, “Fusing texture, shape
    and deep model-learned information at decision level for automated classification
    of lung nodules on chest ct,” *Information Fusion*, vol. 42, pp. 102–110, 2018.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] N. Antropova, B. Q. Huynh, and M. L. Giger, “A deep feature fusion methodology
    for breast cancer diagnosis demonstrated on three imaging modality datasets,”
    *Medical physics*, vol. 44, no. 10, pp. 5162–5171, 2017.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] X. Xia, J. Gong, W. Hao, T. Yang, Y. Lin, S. Wang, and W. Peng, “Comparison
    and fusion of deep learning and radiomics features of ground-glass nodules to
    predict the invasiveness risk of stage-i lung adenocarcinomas in ct scan,” *Frontiers
    in Oncology*, vol. 10, 2020.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Hagerty, J. Stanley *et al.*, “Deep learning and handcrafted method
    fusion: Higher diagnostic accuracy for melanoma dermoscopy images,” *IEEE journal
    of biomedical and health informatics*, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Y. Chai, H. Liu, and J. Xu, “Glaucoma diagnosis based on both hidden features
    and domain knowledge through deep learning models,” *Knowledge-Based Systems*,
    vol. 161, pp. 147–156, 2018.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Buty, Z. Xu, M. Gao, U. Bagci, A. Wu, and D. J. Mollura, “Characterization
    of lung nodule malignancy using hybrid shape and appearance features,” in *MICCAI2016*,
    2016, pp. 662–670.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Saba, A. S. Mohamed, M. El-Affendi, J. Amin, and M. Sharif, “Brain
    tumor detection using fusion of hand crafted and deep learning features,” *Cognitive
    Systems Research*, vol. 59, pp. 221–230, 2020.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] W. Yang, J. Zhao, Y. Qiang, X. Yang, Y. Dong, Q. Du, G. Shi, and M. B.
    Zia, “Dscgans: Integrate domain knowledge in training dual-path semi-supervised
    conditional generative adversarial networks and s3vm for ultrasonography thyroid
    nodules classification,” in *MICCAI2019*, 2019, pp. 558–566.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Tan, Y. Huo, Z. Liang, and L. Li, “Expert knowledge-infused deep learning
    for automatic lung nodule detection,” *Journal of X-ray science and technology*,
    no. Preprint, pp. 1–20, 2019.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Liu, Q. Guo, C. Lian, X. Ren, S. Liang, J. Yu, L. Niu, W. Sun, and
    D. Shen, “Automated detection and classification of thyroid nodules in ultrasound
    images using clinical-knowledge-guided convolutional neural networks,” *Medical
    image analysis*, vol. 58, p. 101555, 2019.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. Feng, J. Cao, H. Wang, Y. Xie, D. Yang, J. Feng, and B. Chen, “A knowledge-driven
    feature learning and integration method for breast cancer diagnosis on multi-sequence
    mri,” *Magnetic Resonance Imaging*, 2020.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] V. Murthy, L. Hou, D. Samaras, T. M. Kurc, and J. H. Saltz, “Center-focusing
    multi-task cnn with injected features for classification of glioma nuclear images,”
    in *2017 IEEE Winter Conference on Applications of Computer Vision (WACV)*.   IEEE,
    2017, pp. 834–841.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Wang, Y. Peng, L. Lu, Z. Lu, and R. M. Summers, “Tienet: Text-image
    embedding network for common thorax disease classification and reporting in chest
    x-rays,” in *CVPR2018*, 2018, pp. 9049–9058.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Zhang, P. Chen, M. Sapkota, and L. Yang, “Tandemnet: Distilling knowledge
    from medical images using diagnostic reports as optional semantic references,”
    in *MICCAI2017*.   Springer, 2017, pp. 320–328.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Wu, J. Phang, J. Park, Y. Shen, Z. Huang, M. Zorin, S. Jastrzebski,
    T. Févry, J. Katsnelson, E. Kim *et al.*, “Deep neural networks improve radiologists
    performance in breast cancer screening,” *IEEE transactions on medical imaging*,
    vol. 39, no. 4, pp. 1184–1194, 2019.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. Yu, H.-Y. Zhou, K. Ma, C. Bian, C. Chu, H. Liu, and Y. Zheng, “Difficulty-aware
    glaucoma classification with multi-rater consensus modeling,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 741–750.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *Advances in neural information processing
    systems*, 2014, pp. 3320–3328.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] X. Xiao, C. Ji, T. B. Mudiyanselage, and Y. Pan, “Pk-gcn: Prior knowledge
    assisted image classification using graph convolution networks,” *arXiv preprint
    arXiv:2009.11892*, 2020.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proceedings of the 26th annual international conference on machine learning*.   ACM,
    2009, pp. 41–48.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] F. Nachbar, W. Stolz, T. Merkle, A. B. Cognetta, T. Vogt, M. Landthaler,
    P. Bilek, O. Braun-Falco, and G. Plewig, “The abcd rule of dermatoscopy: high
    prospective value in the diagnosis of doubtful melanocytic skin lesions,” *Journal
    of the American Academy of Dermatology*, vol. 30, no. 4, pp. 551–559, 1994.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B. W. e. a. Mendelson EB, Bohm-V lez M, “Acr bi-rads ultrasound. in:
    Acr bi-rads atlas, breast imaging reporting and data system.” *Reston, VA, American
    College of Radiology*, 2013.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] L. Breiman, “Random forests,” *Machine learning*, vol. 45, no. 1, pp.
    5–32, 2001.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Cortes and V. Vapnik, “Support-vector networks,” *Machine learning*,
    vol. 20, no. 3, pp. 273–297, 1995.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Alilou, M. Orooji, and A. Madabhushi, “Intra-perinodular textural
    transition (ipris): A 3d descriptor for nodule diagnosis on lung ct,” in *MICCAI2017*.   Springer,
    2017, pp. 647–655.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. A. A. Setio, F. Ciompi, G. Litjens, P. Gerke, C. Jacobs, S. J. Van Riel,
    M. M. W. Wille, M. Naqibullah, C. I. Sánchez, and B. van Ginneken, “Pulmonary
    nodule detection in ct images: false positive reduction using multi-view convolutional
    networks,” *IEEE TMI*, vol. 35, no. 5, pp. 1160–1169, 2016.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros *et al.*, “Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs,” *JAMA*, vol. 316, no. 22, pp. 2402–2410, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Liu, D. Wang, L. Lu, Z. Wei, L. Kim, E. B. Turkbey, B. Sahiner, N. A.
    Petrick, and R. M. Summers, “Detection and diagnosis of colitis on computed tomography
    using deep convolutional neural networks,” *Medical physics*, vol. 44, no. 9,
    pp. 4630–4642, 2017.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time
    object detection with region proposal networks,” vol. 2015, pp. 91–99, 2015.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, “Mask r-cnn,” *international
    conference on computer vision*, pp. 2980–2988, 2017.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] R. Sa, W. Owens, R. Wiegand, M. Studin, D. Capoferri, K. Barooha, A. Greaux,
    R. Rattray, A. Hutton, J. Cintineo *et al.*, “Intervertebral disc detection in
    x-ray images using faster r-cnn,” in *2017 39th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE, 2017,
    pp. 564–567.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Ben-Ari, A. Akselrod-Ballin, L. Karlinsky, and S. Hashoul, “Domain
    specific convolutional neural nets for detection of architectural distortion in
    mammograms,” in *ISBI 2017*.   IEEE, 2017, pp. 552–556.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *CVPR2016*, 2016, pp. 779–788.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. Platania, S. Shams, S. Yang, J. Zhang, K. Lee, and S.-J. Park, “Automated
    breast cancer diagnosis using deep learning and region of interest detection (bc-droid),”
    in *Proceedings of the 8th ACM International Conference on Bioinformatics, Computational
    Biology, and Health Informatics*.   ACM, 2017, pp. 536–543.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] N. Li, H. Liu, B. Qiu, W. Guo, S. Zhao, K. Li, and J. He, “Detection
    and attention: Diagnosing pulmonary lung cancer from ct by imitating physicians,”
    *arXiv preprint arXiv:1712.05114*, 2017.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] G. Cai, J. Chen, Z. Wu, H. Tang, Y. Liu, S. Wang, and S. Su, “One stage
    lesion detection based on 3d context convolutional neural networks,” *Computers
    and Electrical Engineering*, vol. 79, p. 106449, 2019.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. H. Yap, G. Pons, J. Marti, S. Ganau, M. Sentis, R. Zwiggelaar, A. K.
    Davison, and R. Marti, “Automated breast ultrasound lesions detection using convolutional
    neural networks,” *IEEE Journal of Biomedical and Health Informatics*, vol. 22,
    no. 4, pp. 1218–1226, 2018.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. J. Näppi, T. Hironaka, D. Regge, and H. Yoshida, “Deep transfer learning
    of virtual endoluminal views for the detection of polyps in ct colonography,”
    in *Medical Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International
    Society for Optics and Photonics, 2016, p. 97852B.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau, and C. C.
    Poon, “Automatic detection and classification of colorectal polyps by transferring
    low-level cnn features from nonmedical domain,” *IEEE journal of biomedical and
    health informatics*, vol. 21, no. 1, pp. 41–47, 2016.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
    Gotway, and J. Liang, “Convolutional neural networks for medical image analysis:
    Full training or fine tuning?” *IEEE TMI*, vol. 35, no. 5, pp. 1299–1312, 2016.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Zhang, E. H. Cain, A. Saha, Z. Zhu, and M. A. Mazurowski, “Breast
    mass detection in mammography and tomosynthesis via fully convolutional network-based
    heatmap regression,” in *Medical Imaging 2018: Computer-Aided Diagnosis*, vol.
    10575.   International Society for Optics and Photonics, 2018, p. 1057525.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Ben-Cohen, E. Klang, S. P. Raskin, S. Soffer, S. Ben-Haim, E. Konen,
    M. M. Amitai, and H. Greenspan, “Cross-modality synthesis from ct to pet using
    fcn and gan networks for improved automated lesion detection,” *Engineering Applications
    of Artificial Intelligence*, vol. 78, pp. 186–194, 2019.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Zhao, D. Li, Z. Kassam, J. Howey, J. Chong, B. Chen, and S. Li, “Tripartite-gan:
    Synthesizing liver contrast-enhanced mri to improve tumor detection,” *Medical
    Image Analysis*, p. 101667, 2020.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Jesson, N. Guizard, S. H. Ghalehjegh, D. Goblot, F. Soudan, and N. Chapados,
    “Cased: curriculum adaptive sampling for extreme data imbalance,” in *MICCAI2017*,
    2017, pp. 639–646.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] P. Astudillo, P. Mortier, M. De Beule *et al.*, “Curriculum deep reinforcement
    learning with different exploration strategies: A feasibility study on cardiac
    landmark detection,” in *BIOIMAGING 2020: 7th International Conference on Bioimaging*,
    2020.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Z. Li, S. Zhang, J. Zhang, K. Huang, Y. Wang, and Y. Yu, “Mvp-net: Multi-view
    fpn with position-aware attention for deep universal lesion detection,” in *Medical
    Image Computing and Computer-Assisted Intervention*, 2019, pp. 13–21.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Q. Ni, Z. Y. Sun, L. Qi, W. Chen, Y. Yang, L. Wang, X. Zhang, L. Yang,
    Y. Fang, Z. Xing *et al.*, “A deep learning approach to characterize 2019 coronavirus
    disease (covid-19) pneumonia in chest ct images,” *European radiology*, vol. 30,
    no. 12, pp. 6517–6527, 2020.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Liu, Z. Zhou, S. Zhang, L. Luo, Q. Zhang, F. Zhang, X. Li, Y. Wang,
    and Y. Yu, “From unilateral to bilateral learning: Detecting mammogram masses
    with contrasted bilateral network,” in *Medical Image Computing and Computer-Assisted
    Intervention*, 2019, pp. 477–485.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Liu, F. Zhang, Q. Zhang, S. Wang, and Y. Yu, “Cross-view correspondence
    reasoning based on bipartite graph convolutional network for mammogram mass detection,”
    in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Lisowska, E. Beveridge, K. Muir, and I. Poole, “Thrombus detection
    in ct brain scans using a convolutional neural network.” in *BIOIMAGING*, 2017,
    pp. 24–33.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Lisowska, A. O Neil, V. Dilys, M. Daykin, E. Beveridge, K. Muir, S. Mclaughlin,
    and I. Poole, “Context-aware convolutional neural networks for stroke sign detection
    in non-contrast ct scans,” in *Annual Conference on Medical Image Understanding
    and Analysis*.   Springer, 2017, pp. 494–505.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] L. Li, M. Wei, B. Liu, K. Atchaneeyasakul, F. Zhou, Z. Pan, S. Kumar,
    J. Zhang, Y. Pu, D. S. Liebeskind *et al.*, “Deep learning for hemorrhagic lesion
    detection and segmentation on brain ct images,” *IEEE Journal of Biomedical and
    Health Informatics*, 2020.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] L. Fu, J. Ma, Y. Ren, Y. S. Han, and J. Zhao, “Automatic detection of
    lung nodules: false positive reduction using convolution neural networks and handcrafted
    features,” in *Medical Imaging 2017: Computer-Aided Diagnosis*, vol. 10134.   International
    Society for Optics and Photonics, 2017, p. 101340A.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. Kooi, G. Litjens, B. Van Ginneken, A. Gubern-Mérida, C. I. Sánchez,
    R. Mann, A. den Heeten, and N. Karssemeijer, “Large scale deep learning for computer
    aided detection of mammographic lesions,” *Medical Image Analysis*, vol. 35, pp.
    303–312, 2017.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] N. Ghatwary, X. Ye, and M. Zolgharni, “Esophageal abnormality detection
    using densenet based faster r-cnn with gabor features,” *IEEE Access*, vol. 7,
    pp. 84 374–84 385, 2019.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C.-H. Chao, Z. Zhu, D. Guo, K. Yan, T.-Y. Ho, J. Cai, A. P. Harrison,
    X. Ye, J. Xiao, A. Yuille *et al.*, “Lymph node gross tumor volume detection in
    oncology imaging via relationship learning using graph neural network,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 772–782.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Sóñora-Mengan Sr, P. Gonidakis, B. Jansen, J. Garcı?a-Naranjo, and
    J. Vandemeulebroucke, “Evaluating several ways to combine handcrafted features-based
    system with a deep learning system using the luna16 challenge framework,” in *Medical
    Imaging 2020: Computer-Aided Diagnosis*, vol. 11314.   International Society for
    Optics and Photonics, 2020, p. 113143T.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Hwang and H.-E. Kim, “Self-transfer learning for weakly supervised
    lesion localization,” in *MICCAI2016*.   Springer, 2016, pp. 239–246.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] R. Bakalo, R. Ben-Ari, and J. Goldberger, “Classification and detection
    in mammograms with weak supervision via dual branch deep neural net,” in *2019
    IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)*.   IEEE,
    2019, pp. 1905–1909.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] G. Liang, X. Wang, Y. Zhang, and N. Jacobs, “Weakly-supervised self-training
    for breast cancer localization,” in *2020 42nd Annual International Conference
    of the IEEE Engineering in Medicine & Biology Society (EMBC)*.   IEEE, 2020, pp.
    1124–1127.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, and H. Larochelle, “Brain tumor segmentation with deep neural
    networks,” *Medical Image Analysis*, vol. 35, pp. 18–31, 2017.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Zhang, A. Saha, Z. Zhu, and M. A. Mazurowski, “Hierarchical convolutional
    neural networks for segmentation of breast tumors in mri with application to radiogenomics,”
    *IEEE TMI*, vol. 38, no. 2, pp. 435–447, 2018.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] P. F. Christ, F. Ettlinger, F. Grün *et al.*, “Automatic liver and tumor
    segmentation of ct and mri volumes using cascaded fully convolutional neural networks,”
    *arXiv preprint arXiv:1702.05970*, 2017.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] H. R. Roth, A. Farag, L. Lu, E. B. Turkbey, and R. M. Summers, “Deep
    convolutional networks for pancreas segmentation in ct imaging,” in *Medical Imaging
    2015: Image Processing*, vol. 9413.   International Society for Optics and Photonics,
    2015, p. 94131G.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” *CVPR2015*, pp. 3431–3440, 2015.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI2015*.   Springer, 2015, pp. 234–241.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. V. Tran, “A fully convolutional neural network for cardiac segmentation
    in short-axis mri,” *arXiv preprint arXiv:1604.00494*, 2016.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Chen, Y. Zheng, J.-H. Park, P.-A. Heng, and S. K. Zhou, “Iterative
    multi-domain regularized deep learning for anatomical structure detection and
    segmentation from ultrasound images,” in *MICCAI2016*.   Springer, 2016, pp. 487–495.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R.
    Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Towards image-guided
    pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal
    ct with dense dilated networks,” in *MICCAI2017*, 2017, pp. 728–736.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] P. F. Christ, M. E. A. Elshaer, *et al.*, “Automatic liver and lesion
    segmentation in ct using cascaded fully convolutional neural networks and 3d conditional
    random fields,” in *MICCAI2016*.   Springer, 2016, pp. 415–423.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane, D. K.
    Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3d cnn with fully connected
    crf for accurate brain lesion segmentation,” *Medical Image Analysis*, vol. 36,
    pp. 61–78, 2017.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] X. Yang, L. Yu, L. Wu, Y. Wang, D. Ni, J. Qin, and P.-A. Heng, “Fine-grained
    recurrent neural networks for automatic prostate segmentation in ultrasound images,”
    in *Thirty-First AAAI Conference on Artificial Intelligence*, 2017.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested
    u-net architecture for medical image segmentation,” in *Deep Learning in Medical
    Image Analysis and Multimodal Learning for Clinical Decision Support*.   Springer,
    2018, pp. 3–11.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, “Recurrent
    residual convolutional neural network based on u-net (r2u-net) for medical image
    segmentation,” *arXiv preprint arXiv:1802.06955*, 2018.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Gordienko, P. Gang, J. Hui, W. Zeng, Y. Kochura, O. Alienin, O. Rokovyi,
    and S. Stirenko, “Deep learning with lung segmentation and bone shadow exclusion
    techniques for chest x-ray analysis of lung cancer,” in *International Conference
    on Computer Science, Engineering and Education Applications*.   Springer, 2018,
    pp. 638–647.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] D. Yang, D. Xu, S. K. Zhou, B. Georgescu, M. Chen, S. Grbic, D. Metaxas,
    and D. Comaniciu, “Automatic liver segmentation using an adversarial image-to-image
    network,” in *MICCAI2017*.   Springer, 2017, pp. 507–515.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Zhao, L. Wang, J. Chen, D. Nie, Y. Cong, S. Ahmad, A. Ho, P. Yuan,
    S. H. Fung, H. H. Deng *et al.*, “Craniomaxillofacial bony structures segmentation
    from mri with deep-supervision adversarial learning,” in *MICCAI2018*, 2018, pp.
    720–727.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] K. Kamnitsas, C. Baumgartner *et al.*, “Unsupervised domain adaptation
    in brain lesion segmentation with adversarial networks,” in *International conference
    on information processing in medical imaging*.   Springer, 2017, pp. 597–609.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] S. Izadi, Z. Mirikharaji, J. Kawahara, and G. Hamarneh, “Generative adversarial
    networks to segment skin lesions,” in *ISBI 2018*.   IEEE, 2018, pp. 881–884.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Lahiri, K. Ayush, P. Kumar Biswas, and P. Mitra, “Generative adversarial
    learning for reducing manual annotation in semantic segmentation on large scale
    miscroscopy images: Automated vessel segmentation in retinal fundus image as test
    case,” in *CVPR Workshops*, 2017, pp. 42–48.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs,
    “Unsupervised anomaly detection with generative adversarial networks to guide
    marker discovery,” in *International Conference on Information Processing in Medical
    Imaging*.   Springer, 2017, pp. 146–157.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, “Dcan: Deep contour-aware
    networks for object instance segmentation from histology images,” *Medical Image
    Analysis*, vol. 36, pp. 135–146, 2017.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] L. Wu, Y. Xin, S. Li, T. Wang, P.-A. Heng, and D. Ni, “Cascaded fully
    convolutional networks for automatic prenatal ultrasound image segmentation,”
    in *ISBI 2017*.   IEEE, 2017, pp. 663–666.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Zeng, X. Yang, J. Li, L. Yu, P.-A. Heng, and G. Zheng, “3d u-net with
    multi-level deep supervision: fully automatic segmentation of proximal femur in
    3d mr images,” in *International workshop on machine learning in medical imaging*.   Springer,
    2017, pp. 274–282.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Ghafoorian, A. Mehrtash, T. Kapur, N. Karssemeijer, E. Marchiori,
    M. Pesteie, C. R. Guttmann, F.-E. de Leeuw, C. M. Tempany, B. van Ginneken *et al.*,
    “Transfer learning for domain adaptation in mri: Application in brain lesion segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 516–524.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” in *MICCAI2016*.   Springer, 2016, pp. 478–486.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] V. V. Valindria, N. Pawlowski *et al.*, “Multi-modal learning from unpaired
    images: Application to multi-organ segmentation in ct and mri,” in *2018 IEEE
    Winter Conference on Applications of Computer Vision (WACV)*.   IEEE, 2018, pp.
    547–556.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] C. Chen, Q. Dou, H. Chen, and P.-A. Heng, “Semantic-aware generative
    adversarial nets for unsupervised domain adaptation in chest x-ray segmentation,”
    in *International Workshop on Machine Learning in Medical Imaging*.   Springer,
    2018, pp. 143–151.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] C. Chen, Q. Dou, H. Chen, J. Qin, and P. A. Heng, “Unsupervised bidirectional
    cross-modality adaptation via deeply synergistic image and feature alignment for
    medical image segmentation,” *IEEE TMI*, 2020.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J. O.
    Deasy, and H. Veeraraghavan, “Tumor-aware, adversarial domain adaptation from
    ct to mri for lung cancer segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    777–785.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] W. Yan, Y. Wang, S. Gu, L. Huang, F. Yan, L. Xia, and Q. Tao, “The domain
    shift problem of medical image segmentation and vendor-adaptation by unet-gan,”
    in *MICCAI2019*.   Springer, 2019, pp. 623–631.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, and J. S. Duncan,
    “Unsupervised domain adaptation via disentangled representations: Application
    to cross-modality liver segmentation,” in *MICCAI2019*.   Springer, 2019, pp.
    255–263.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] K. Li, L. Yu, S. Wang, and P.-A. Heng, “Towards cross-modality medical
    image segmentation with online mutual knowledge distillation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 01, 2020, pp.
    775–783.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] K. Li, S. Wang, L. Yu, and P.-A. Heng, “Dual-teacher: Integrating intra-domain
    and inter-domain teachers for annotation-efficient cardiac segmentation,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 418–427.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] M. Hu, M. Maillard, Y. Zhang, T. Ciceri, G. La Barbera, I. Bloch, and
    P. Gori, “Knowledge distillation from multi-modal to mono-modal segmentation networks,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 772–781.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting multimodal
    medical volumes with cycle-and shape-consistency generative adversarial network,”
    in *CVPR2018*, 2018, pp. 9242–9251.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] A. Chartsias, G. Papanastasiou, C. Wang, S. Semple, D. Newby, R. Dharmakumar,
    and S. Tsaftaris, “Disentangle, align and fuse for multimodal and semi-supervised
    image segmentation.” *IEEE transactions on medical imaging*, 2020.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis.” *arXiv preprint arXiv:1904.00625*, 2019.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] L. Berger, H. Eoin, M. J. Cardoso, and S. Ourselin, “An adaptive sampling
    scheme to efficiently train fully convolutional networks for semantic segmentation,”
    in *Annual Conference on Medical Image Understanding and Analysis*.   Springer,
    2018, pp. 277–286.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Li, X. Liu, S. Boumaraf, W. Liu, X. Gong, and X. Ma, “A new three-stage
    curriculum learning approach for deep network based liver tumor segmentation,”
    in *2020 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020,
    pp. 1–6.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Kervadec, J. Dolz, É. Granger, and I. B. Ayed, “Curriculum semi-supervised
    segmentation,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2019, pp. 568–576.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Z. Zhao, X. Zhang, C. Chen, W. Li, S. Peng, J. Wang, X. Yang, L. Zhang,
    and Z. Zeng, “Semi-supervised self-taught deep learning for finger bones segmentation,”
    in *2019 IEEE EMBS International Conference on Biomedical & Health Informatics
    (BHI)*.   IEEE, 2019, pp. 1–4.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] J. Zhang, G. Wang, H. Xie, S. Zhang, N. Huang, S. Zhang, and L. Gu, “Weakly
    supervised vessel segmentation in x-ray angiograms by self-paced learning from
    noisy labels with suggestive annotation,” *Neurocomputing*, vol. 417, pp. 114–127,
    2020.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] B. Wu, Z. Zhou, J. Wang, and Y. Wang, “Joint learning for pulmonary nodule
    segmentation, attributes and malignancy prediction,” in *ISBI 2018*.   IEEE, 2018,
    pp. 1109–1113.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] A. Hatamizadeh, D. Terzopoulos, and A. Myronenko, “End-to-end boundary
    aware networks for medical image segmentation,” in *International Workshop on
    Machine Learning in Medical Imaging*.   Springer, 2019, pp. 187–194.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Z. Zhu, D. Jin, K. Yan, T.-Y. Ho, X. Ye, D. Guo, C.-H. Chao, J. Xiao,
    A. Yuille, and L. Lu, “Lymph node gross tumor volume detection and segmentation
    via distance-based gating using 3d ct/pet imaging in radiotherapy,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 753–762.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] D. Jin, D. Guo, T.-Y. Ho, A. P. Harrison, J. Xiao, C.-k. Tseng, and L. Lu,
    “Deeptarget: Gross tumor and clinical target volume segmentation in esophageal
    cancer radiotherapy,” *Medical Image Analysis*, p. 101909, 2020.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] K. Huang, H.-D. Cheng, Y. Zhang, B. Zhang, P. Xing, and C. Ning, “Medical
    knowledge constrained semantic breast ultrasound image segmentation,” in *2018
    24th International Conference on Pattern Recognition (ICPR)*.   IEEE, 2018, pp.
    1193–1198.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] N. Painchaud, Y. Skandarani, T. Judge, O. Bernard, A. Lalande, and P.-M.
    Jodoin, “Cardiac mri segmentation with strong anatomical guarantees,” in *MICCAI2019*.   Springer,
    2019, pp. 632–640.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] ——, “Cardiac segmentation with strong anatomical guarantees,” *IEEE Transactions
    on Medical Imaging*, vol. 39, no. 11, pp. 3703–3713, 2020.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] O. Oktay, E. Ferrante *et al.*, “Anatomically constrained neural networks
    (acnns): application to cardiac image enhancement and segmentation,” *IEEE TMI*,
    vol. 37, no. 2, pp. 384–395, 2017.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] H. Ravishankar, R. Venkataramani, S. Thiruvenkadam, P. Sudhakar, and
    V. Vaidya, “Learning and incorporating shape models for semantic segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 203–211.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. BenTaieb and G. Hamarneh, “Topology aware fully convolutional networks
    for histology gland segmentation,” in *MICCAI2018*.   Springer, 2016, pp. 460–468.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Z. Mirikharaji and G. Hamarneh, “Star shape prior in fully convolutional
    networks for skin lesion segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    737–745.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] H. Zheng, L. Lin, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y. Chen,
    R. Tong, and J. Wu, “Semi-supervised segmentation of liver using adversarial learning
    with deep atlas prior,” in *Medical Image Computing and Computer-Assisted Intervention*,
    2019, pp. 148–156.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] C. Zotti, Z. Luo, A. Lalande, and P. Jodoin, “Convolutional neural network
    with shape prior applied to cardiac mri segmentation,” *IEEE Journal of Biomedical
    and Health Informatics*, vol. 23, no. 3, pp. 1119–1128, 2019.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in convolutional
    networks for unsupervised biomedical segmentation,” in *CVPR2018*, 2018, pp. 9290–9299.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Y. He, G. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J. Dillenseger,
    P. Shao, S. Zhang *et al.*, “Dpa-densebiasnet: Semi-supervised 3d fine renal artery
    segmentation with dense biased network and deep priori anatomy.” in *Medical Image
    Computing and Computer-Assisted Intervention*, 2019, pp. 139–147.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] B. Luo, J. Shen, S. Cheng, Y. Wang, and M. Pantic, “Shape constrained
    network for eye segmentation in the wild,” in *The IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 1952–1960.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Y. Song, L. Zhu, B. Lei, B. Sheng, Q. Dou, J. Qin, and K.-S. Choi, “Shape
    mask generator: Learning to refine shape priors for segmenting overlapping cervical
    cytoplasms,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 639–649.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] A. Boutillon, B. Borotikar, V. Burdin, and P.-H. Conze, “Combining shape
    priors with conditional adversarial networks for improved scapula segmentation
    in mr images,” in *2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI)*.   IEEE, 2020, pp. 1164–1167.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] D. Pham, G. Dovletov, and J. Pauli, “Liver segmentation in ct with mri
    data: Zero-shot domain adaptation by contour extraction and shape priors,” in
    *2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)*.   IEEE,
    2020, pp. 1538–1542.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] M. Engin, R. Lange, A. Nemes, S. Monajemi, M. Mohammadzadeh, C. K. Goh,
    T. M. Tu, B. Y. Tan, P. Paliwal, L. L. Yeo *et al.*, “Agan: An anatomy corrector
    conditional generative adversarial network,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*.   Springer, 2020, pp. 708–717.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N.
    Metaxas, H. Li, and M. Chen, “Focusnetv2: Imbalanced large and small organ segmentation
    with adversarial shape constraint for head and neck ct images,” *Medical Image
    Analysis*, vol. 67, p. 101831, 2020.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] K. Kushibar, S. Valverde, S. González-Villà, J. Bernal, M. Cabezas, A. Oliver,
    and X. Lladó, “Automated sub-cortical brain structure segmentation combining spatial
    and deep convolutional features,” *Medical Image Analysis*, vol. 48, pp. 177–186,
    2018.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Rezaei, A. Emami *et al.*, “Gland segmentation in histopathology images
    using deep networks and handcrafted features,” in *2019 41st Annual International
    Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE,
    2019, pp. 1031–1034.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] H. Khan, P. M. Shah, M. A. Shah, S. ul Islam, and J. J. Rodrigues, “Cascading
    handcrafted features and convolutional neural network for iot-enabled brain tumor
    segmentation,” *Computer Communications*, 2020.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] H. Narotamo, J. M. Sanches, and M. Silveira, “Combining deep learning
    with handcrafted features for cell nuclei segmentation,” in *2020 42nd Annual
    International Conference of the IEEE Engineering in Medicine & Biology Society
    (EMBC)*.   IEEE, 2020, pp. 1428–1431.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 4489–4497.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for latent
    variable models,” in *Advances in Neural Information Processing Systems*, 2010,
    pp. 1189–1197.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] G. N. Sharma, R. Dave, J. Sanadya, P. Sharma, and K. Sharma, “Various
    types and management of breast cancer: an overview,” *Journal of advanced pharmaceutical
    technology &amp; research*, vol. 1, no. 2, p. 109, 2010.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] C. Qin, J. Schlemper, J. Caballero, A. N. Price, J. V. Hajnal, and D. Rueckert,
    “Convolutional recurrent neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 38, no. 1, pp. 280–290, 2018.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price, and D. Rueckert,
    “A deep cascade of convolutional neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 37, no. 2, pp. 491–503, 2017.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,
    S. Arridge, J. Keegan, Y. Guo *et al.*, “Dagan: deep de-aliasing generative adversarial
    networks for fast compressed sensing mri reconstruction,” *IEEE TMI*, vol. 37,
    no. 6, pp. 1310–1321, 2017.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] H. B. Yedder, M. Shokoufi, B. Cardoen, F. Golnaraghi, and G. Hamarneh,
    “Limited-angle diffuse optical tomography image reconstruction using deep learning,”
    in *MICCAI2019*.   Springer, 2019, pp. 66–74.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] S. U. H. Dar, M. Yurt, M. Shahdloo, M. E. Ildız, and T. Çukur, “Synergistic
    reconstruction and synthesis via generative adversarial networks for accelerated
    multi-contrast mri,” *arXiv preprint arXiv:1805.10704*, 2018.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. Ahmad, M. Sajjad, I. Mehmood, and S. W. Baik, “Sinc: Saliency-injected
    neural codes for representation and efficient retrieval of medical radiographs,”
    *PloS one*, vol. 12, no. 8, 2017.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] A. Khatami, M. Babaie, H. R. Tizhoosh, A. Khosravi, T. Nguyen, and S. Nahavandi,
    “A sequential search-space shrinking using cnn transfer learning and a radon projection
    pool for medical image retrieval,” *Expert Systems with Applications*, vol. 100,
    pp. 224–233, 2018.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Z. N. K. Swati, Q. Zhao, M. Kabir, F. Ali, Z. Ali, S. Ahmed, and J. Lu,
    “Content-based brain tumor retrieval for mr images using transfer learning,” *IEEE
    Access*, vol. 7, pp. 17 809–17 822, 2019.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Y. Anavi, I. Kogan, E. Gelbart, O. Geva, and H. Greenspan, “A comparative
    study for chest radiograph image retrieval using binary texture and deep learning
    classification,” in *EMBC 2015*.   IEEE, 2015, pp. 2940–2943.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Y. Anavi, I. Kogan *et al.*, “Visualizing and enhancing a deep learning
    framework using patients age and gender for chest x-ray image retrieval,” in *Medical
    Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International Society for
    Optics and Photonics, 2016, p. 978510.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] B. Jing, P. Xie, and E. Xing, “On the automatic generation of medical
    imaging reports,” *arXiv preprint arXiv:1711.08195*, 2017.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] G. Liu, T. H. Hsu, M. B. A. Mcdermott, W. Boag, W. Weng, P. Szolovits,
    and M. Ghassemi, “Clinically accurate chest x-ray report generation.” *arXiv preprint
    arXiv: 1904.02633*, 2019.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Hybrid retrieval-generation
    reinforced agent for medical image report generation,” in *Advances in neural
    information processing systems*, 2018, pp. 1530–1540.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Knowledge-driven encode,
    retrieve, paraphrase for medical image report generation,” in *Proceedings of
    the AAAI Conference on Artificial Intelligence*, vol. 33, 2019, pp. 6666–6673.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] W. Gale, L. Oakden-Rayner, G. Carneiro, A. P. Bradley, and L. J. Palmer,
    “Producing radiologist-quality reports for interpretable artificial intelligence,”
    *arXiv preprint arXiv:1806.00340*, 2018.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Y. Hong and C. E. Kahn, “Content analysis of reporting templates and
    free-text radiology reports,” *Journal of digital imaging*, vol. 26, no. 5, pp.
    843–849, 2013.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, “When radiology
    report generation meets knowledge graph,” *arXiv preprint arXiv:2002.08277*, 2020.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in *International
    conference on machine learning*.   PMLR, 2018, pp. 1989–1998.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Unsupervised domain adaptation
    with residual transfer networks,” in *Advances in Neural Information Processing
    Systems*, 2016, pp. 136–144.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *CVPR2017*, 2017, pp. 7167–7176.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Y. Luo, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Taking a closer look
    at domain shift: Category-level adversaries for semantics consistent domain adaptation,”
    in *CVPR2019*, 2019, pp. 2507–2516.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker,
    “Learning to adapt structured output space for semantic segmentation,” in *CVPR2018*,
    2018, pp. 7472–7481.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Y. Zhang, Y. Wei, Q. Wu, P. Zhao, S. Niu, J. Huang, and M. Tan, “Collaborative
    unsupervised domain adaptation for medical image diagnosis,” *IEEE Transactions
    on Image Processing*, vol. 29, pp. 7834–7844, 2020.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O Donnell, H. Huang, M. Chen,
    and W. Cai, “Pdam: A panoptic-level feature alignment framework for unsupervised
    domain adaptive instance segmentation in microscopy images,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge graph and text jointly
    embedding,” pp. 1591–1601, 2014.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] M. Wistuba, A. Rawat, and T. Pedapati, “A survey on neural architecture
    search.” *arXiv preprint arXiv: 1905.01392*, 2019.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    and L. Lu, “Organ at risk segmentation for head and neck cancer using stratified
    learning and neural architecture search,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 4223–4232.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
