- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:01:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2004.12150] A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12150](https://ar5iv.labs.arxiv.org/html/2004.12150)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xiaozheng Xie, Jianwei Niu, , Xuefeng Liu, Zhengsu Chen, Shaojie Tang,  and
    Shui Yu X. Xie, J. Niu, X. Liu and Z. Chen are with the State Key Laboratory of
    Virtual Reality Technology and Systems, School of Computer Science and Engineering,
    Beihang University, Beijing 100191, China. E-mails: {xiexzheng,niujianwei,liu_xuefeng,danczs}@buaa.edu.cn
    J. Niu is also with the Beijing Advanced Innovation Center for Big Data and Brain
    Computing (BDBC) and Hangzhou Innovation Institute of Beihang University. S. Tang
    is in Jindal School of Management, The University of Texas at Dallas. E-mails:
    tangshaojie@gmail.com S. Yu is in School of Computer Science, University of Technology
    Sydney, Australia. E-mails: Shui.Yu@uts.edu.au'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although deep learning models like CNNs have achieved great success in medical
    image analysis, the small size of medical datasets remains a major bottleneck
    in this area. To address this problem, researchers have started looking for external
    information beyond current available medical datasets. Traditional approaches
    generally leverage the information from natural images via transfer learning.
    More recent works utilize the domain knowledge from medical doctors, to create
    networks that resemble how medical doctors are trained, mimic their diagnostic
    patterns, or focus on the features or areas they pay particular attention to.
    In this survey, we summarize the current progress on integrating medical domain
    knowledge into deep learning models for various tasks, such as disease diagnosis,
    lesion, organ and abnormality detection, lesion and organ segmentation. For each
    task, we systematically categorize different kinds of medical domain knowledge
    that have been utilized and their corresponding integrating methods. We also provide
    current challenges and directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: medical image analysis, medical domain knowledge, deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent years have witnessed a tremendous progress in computer-aided detection/diagnosis
    (CAD) in medical imaging and diagnostic radiology, primarily thanks to the advancement
    of deep learning techniques. Having achieved great success in computer vision
    tasks, various deep learning models, mainly convolutional neural networks (CNNs),
    soon be applied to CAD. Among the applications are the early detection and diagnosis
    of breast cancer, lung cancer, glaucoma, and skin cancer [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: However, the small size of medical datasets continues to be an issue in obtaining
    satisfactory deep learning model for CAD; in general, bigger datasets result in
    better deep learning models [[5](#bib.bib5)]. In traditional computer vision tasks,
    there are many large-scale and well-annotated datasets, such as ImageNet ¹¹1http://www.image-net.org/
    (over 14M labeled images from 20k categories) and COCO ²²2http://mscoco.org/ (with
    more than 200k annotated images across 80 categories). In contrast, some popular
    publicly available medical datasets are much smaller (see Table [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis")). For example, among the datasets for different tasks,
    only ChestX-ray14 and DeepLesion, contain more than 100k labeled medical images,
    while most datasets only have a few thousands or even hundreds of medical images.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Examples of popular datasets in the medical domain'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Purpose | Type | Imaging | Number of Images |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ADNI [[6](#bib.bib6)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification | Brain | Multiple | 1921 patients |'
  prefs: []
  type: TYPE_TB
- en: '| ABIDE [[7](#bib.bib7)] | Classification | Brain | MRI | 539 patients and
    573 controls |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ACDC [[8](#bib.bib8)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification | Cardiac | MRI | 150 patients |'
  prefs: []
  type: TYPE_TB
- en: '| ChestX-ray14 [[9](#bib.bib9)] | Detection | Chest | X-ray | 112,120 images
    from 30,805 patients |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LIDC-IDRI [[10](#bib.bib10)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Detection | Lung | CT, X-ray | 1,018 patients |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LUNA16 [[11](#bib.bib11)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Detection | Lung | CT | 888 images |'
  prefs: []
  type: TYPE_TB
- en: '| MURA [[12](#bib.bib12)] | Detection | Musculo-skeletal | X-ray |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 40,895 images from 14,982 patients &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BraTS2018 [[13](#bib.bib13)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Segmentation | Brain | MRI | 542 images |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; STARE [[14](#bib.bib14)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Segmentation | Eye | SLO | 400 images |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DDSM [[15](#bib.bib15)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Breast | Mammography | 2,500 patients |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DeepLesion [[16](#bib.bib16)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multiple | CT | 32,735 images from 4,427 patients |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cardiac MRI [[17](#bib.bib17)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cardiac | MRI | 7,980 images from 33 cases |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISIC 2018 [[18](#bib.bib18)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Skin | Dermoscopic | 13,000 images |'
  prefs: []
  type: TYPE_TB
- en: The lack of medical datasets is represented in three aspects. First, the number
    of medical images in datasets is usually small. This problem is mainly due to
    the high cost associated with the data collection. Medical images are collected
    from computerized tomography (CT), Ultrasonic imaging (US), magnetic resonance
    imaging (MRI) scans, positron emission tomography (PET), all of which are expensive
    and labor-intensive. Second, only a small portion of medical images are annotated.
    These annotations including classification labels (e.g., benign or malignant),
    the segmentation annotations of lesion areas, etc., require efforts from experienced
    doctors. Third, it is difficult to collect enough positive cases for some rare
    diseases to obtain the balanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: One direct consequence of the lack of well annotated medical data is that the
    trained deep learning models can easily suffer from the overfitting problem [[19](#bib.bib19)].
    As a result, the models perform very well on training datasets, but fail when
    dealing with new data from the problem domain. Correspondingly, many existing
    studies on medical image analysis adopt techniques from computer vision to address
    overfitting, such as reducing the complexity of the network [[20](#bib.bib20),
    [21](#bib.bib21)], adopting some regularization techniques [[22](#bib.bib22)],
    or using data augmentation strategies [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: However, in essence, both decreasing model complexity and leveraging data augmentation
    techniques only focus on the target task on the given datasets, but *do not introduce
    any new information into deep learning models*. Nowdays, introducing more information,
    beyond the given medical datasets has become a more promising approach to address
    the problem of small-sized medical datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of introducing external information to improve the performance of deep
    learning models for CAD is not new. For example, it is common practice to first
    train a deep learning model on some natural image datasets like ImageNet, and
    then fine tune them on target medical datasets [[24](#bib.bib24)]. This process,
    called transfer learning [[25](#bib.bib25)], implicitly introduces information
    from natural images. Besides natural images, multi-modal medical datasets or medical
    images from different but related diseases can also be used to improve the performance
    of deep learning models [[26](#bib.bib26), [27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as experienced medical doctors (e.g., radiologists, ophthalmologists,
    and dermatologists) can generally give fairly accurate results, it is not surprising
    that their knowledge may help deep learning models to better accomplish the designated
    tasks. The domain knowledge of medical doctors includes the way they browse images,
    the particular areas they usually focus on, the features they give special attention
    to, and the anatomical prior knowledge they used. These types of knowledge are
    accumulated, summarized, and validated by a large number of practitioners over
    many years based on a huge amount of cases. Note that in this survey any network
    that incorporate one of these types of knowledge in their training or designing
    process should be regarded as the one incorporated medical domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35ea96f042890d766a6a045567db868d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Methods of information categorization and incorporating methods in
    disease diagnosis; lesion, organ, and abnormality detection; lesion and organ
    segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we focus on the three main tasks of medical image analysis:
    (1) disease diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion
    and organ segmentation. We also include other related tasks such as the image
    reconstruction, image retrieval and report generation. This survey demonstrates
    that, for almost all tasks, identifying and carefully integrating one or more
    types of domain knowledge related to the designated task will improve the performance
    of deep learning models. We organize existing works according to the following
    three aspects: the types of tasks, the types of domain knowledge that are introduced,
    and the ways of introducing the domain knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, in terms of the types of domain knowledge, some of them are
    of high-level such as training pattern [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]
    and diagnostic pattern. Some domain knowledge are low-level, such as particular
    features and special areas where medical doctors pay more attention to [[31](#bib.bib31)].
    In particular, in disease diagnosis tasks, high-level domain knowledge is widely
    utilized. For an object detection task, the low-level domain knowledge, such as
    detection patterns and specific features where medical doctors give special attention
    is more commonly adopted. For lesion or organ segmentation tasks, anatomical priors
    and the knowledge from different modalities seem to be more useful [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6cc738d2ce8301a87d9258d1f3c41477.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: (a) Number of papers arranged chronically (2016-2020). (b) The distribution
    of selected papers in different applications of medical image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the integrating methods, various approaches have been designed to
    incorporate different types of domain knowledge into networks [[35](#bib.bib35)].
    For example, a simple approach is to concatenate hand-crafted features with the
    ones extracted from deep learning models [[36](#bib.bib36)]. In some works, network
    architectures are revised to simulate the pattern of radiologists when they read
    images [[37](#bib.bib37)]. Attention mechanism, which allows a network to pay
    more attention to a certain region of an image, is a powerful technique to incorporate
    domain knowledge of radiologists [[38](#bib.bib38)]. In addition, multi-task learning
    and meta learning are also widely used to introduce medical domain knowledge into
    deep learning models [[39](#bib.bib39), [40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: Although there are a number of reviews on deep learning for medical image analysis,
    including [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)],
    they all describe the existing works from the application point of view, i.e.,
    how deep learning techniques are applied to various medical applications. To the
    best of our knowledge, there is no review that gives systematic introduction on
    *how medical domain knowledge can help deep learning models*. This aspect, we
    believe, is the unique feature that distinguishes deep learning models for CAD
    from those for general computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") gives the overview on
    how we organize the related researches. At the top level, existing studies are
    classified into three main categories according to their purposes: (1) disease
    diagnosis, (2) lesion, organ and abnormality detection, and (3) lesion and organ
    segmentation. In each category, we organize them into several groups based on
    the types of extra knowledge have been incorporated. At the bottom level, they
    are further categorized according to the different integrating approaches of the
    domain knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: This survey contains more than 200 papers (163 are with domain knowledge), most
    of which are published recently (2016-2020), on a wide variety of applications
    of deep learning techniques for medical image analysis. In addition, most of the
    corresponding works are from the conference proceedings for MICCAI, EMBC, ISBI
    and some journals such as TMI, Medical Image Analysis, JBHI and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afbdc1af2cfaab97cd40ba6f5c79756a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The organizational structure of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of these papers are shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis")(a). It can be seen that the number of papers increases
    rapidly from 2016 to 2020\. With respect to the applications, most of them are
    related to disease diagnosis and lesion/organ segmentation (shown in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")(b)). To sum up, with this survey we aim
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarize and classify different types of domain knowledge in medical areas
    that are utilized to improve the performance of deep learning models in various
    applications;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarize and classify different ways of introducing medical domain knowledge
    into deep learning models;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: give the outlook of challenges and future directions in integrating medical
    domain knowledge into deep learning models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of the survey is organized as follows. Sections [2](#S2 "2 Disease
    Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis"), [3](#S3 "3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and [4](#S4 "4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") introduce the
    existing works for the major three tasks in medical image analysis. Besides these
    three major tasks, other tasks in medical image analysis are described in Section
    [5](#S5 "5 Other Medical Applications ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"). In each section, we first introduce
    the general architectures of deep learning models for a task, and then categorize
    related works according to the types of the domain knowledge to be integrated.
    Various incorporating methods for each type of domain knowledge are then described.
    Section [6](#S6 "6 Research Challenges and Future Directions ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") discusses research
    challenges, and gives the outlook of future directions. Lastly, Section [7](#S7
    "7 Conclusion ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis") concludes this survey. The structure of this survey
    is shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Disease Diagnosis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Disease diagnosis refers to the task of determining the type and condition of
    possible diseases based on the medical images provided. In this section, we give
    an overview of the deep learning models that generally used for disease diagnosis.
    Concretely, subsection [2.1](#S2.SS1 "2.1 General Structures of Deep Learning
    Models Used for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") outlines the
    general structures of deep learning models used for disease diagnosis. Subsection
    [2.2](#S2.SS2 "2.2 Incorporating Knowledge from Natural Datasets or Other Medical
    Datasets ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into
    Deep Learning for Medical Image Analysis") introduces the works that utilize knowledge
    from natural images or other medical datasets. Deep learning models that leverage
    knowledge from medical doctors are introduced in Subsection [2.3](#S2.SS3 "2.3
    Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey
    on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")
    in detail. Lastly, Subsection [2.4](#S2.SS4 "2.4 Summary ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") summarizes the research of disease diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 General Structures of Deep Learning Models Used for Disease Diagnosis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last decades, deep learning techniques, especially CNNs, have achieved
    a great success in disease diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 General Structures of Deep Learning Models Used
    for Disease Diagnosis ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis") shows the structure
    of a typical CNN that used for disease diagnosis in chest X-ray image. The CNN
    employs alternating convolutional and pooling layers, and contains trainable filter
    banks per layer. Each individual filter in a filter bank is able to generate a
    feature map. This process is alternated and the CNN can learn increasingly more
    and more abstract features that will later be used by the fully connected layers
    to accomplish the classification task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13d7097710f05c622a19c53a7a5401b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A typical CNN architecture for medical disease diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: Different types of CNN architectures, from AlexNet [[45](#bib.bib45)], GoogLeNet
    [[46](#bib.bib46)], VGGNet [[47](#bib.bib47)], ResNet [[48](#bib.bib48)] to DenseNet
    [[49](#bib.bib49)], have achieved a great success in the diagnosis of various
    diseases. For example, GoogLeNet, ResNet, and VGGNet models are used in the diagnosis
    of canine ulcerative keratitis [[50](#bib.bib50)], and most of them achieve accuracies
    of over 90% when classifying superficial and deep corneal ulcers. DenseNet is
    adopted to diagnose lung nodules on chest X-ray radiograph [[51](#bib.bib51)],
    and experimental results show that more than 99% of lung nodules can be detected.
    In addition, it is found that VGGNet and ResNet are more effective than other
    network structures for many medical diagnostic tasks [[37](#bib.bib37), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)].
  prefs: []
  type: TYPE_NORMAL
- en: However, the above works generally directly apply CNNs to medical image analysis
    or slightly modified CNNs (e.g., by modifying the number of kernals, the number
    of channels or the size of filters), and no medical knowledge is incorporated.
    In addition, these methods generally require large medical datasets to achieve
    a satisfactory performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we systematically review on the research that
    utilizes medical domain knowledge for the disease diagnosis. The types of knowledge
    and the incorporating methods are summarized in Table [II](#S2.T2 "TABLE II ‣
    2.1 General Structures of Deep Learning Models Used for Disease Diagnosis ‣ 2
    Disease Diagnosis ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: A compilation of the knowledge and corresponding incorporating methods
    used in disease diagnosis'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Source &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Type &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorporating Method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| References |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural datasets &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[55](#bib.bib55)][[1](#bib.bib1)]  [[56](#bib.bib56)][[57](#bib.bib57)][[58](#bib.bib58)]  [[59](#bib.bib59)]  [[60](#bib.bib60)][[61](#bib.bib61)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Medical datasets |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Multi-modal images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[26](#bib.bib26)][[62](#bib.bib62)]  [[63](#bib.bib63)][[64](#bib.bib64)][[65](#bib.bib65)][[66](#bib.bib66)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Datasets from other diseases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-task learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[67](#bib.bib67)][[68](#bib.bib68)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Medical doctors |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Training pattern &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[28](#bib.bib28)][[29](#bib.bib29)]  [[69](#bib.bib69)][[70](#bib.bib70)][[71](#bib.bib71)]  [[72](#bib.bib72)][[73](#bib.bib73)][[74](#bib.bib74)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Diagnostic patterns &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Network design &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[37](#bib.bib37)][[75](#bib.bib75)]  [[76](#bib.bib76)][[77](#bib.bib77)]  [[78](#bib.bib78)][[79](#bib.bib79)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Areas doctors focus on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attention mechanism &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[38](#bib.bib38)]  [[53](#bib.bib53)]  [[80](#bib.bib80)][[81](#bib.bib81)]  [[82](#bib.bib82)][[83](#bib.bib83)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Features doctors focus on |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Decision level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[61](#bib.bib61)]  [[84](#bib.bib84)]  [[85](#bib.bib85)][[86](#bib.bib86)]  [[87](#bib.bib87)][[88](#bib.bib88)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[36](#bib.bib36)][[89](#bib.bib89)]  [[57](#bib.bib57)]  [[90](#bib.bib90)]  [[91](#bib.bib91)][[92](#bib.bib92)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[93](#bib.bib93)]  [[54](#bib.bib54)]  [[94](#bib.bib94)][[95](#bib.bib95)]  [[96](#bib.bib96)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; As labels of CNNs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[39](#bib.bib39)][[59](#bib.bib59)]  [[97](#bib.bib97)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Other related information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-task learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; /network design &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[52](#bib.bib52)]  [[98](#bib.bib98)]  [[99](#bib.bib99)][[100](#bib.bib100)]  [[101](#bib.bib101)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the disparity between natural and medical images, it has been demonstrated
    that CNNs comprehensively trained on the large-scale well-annotated natural image
    datasets can still be helpful for disease diagnosis tasks [[56](#bib.bib56)].
    Intrinsically speaking, this transfer learning process introduces knowledge from
    natural images into the network for medical image diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [[42](#bib.bib42)], the networks pre-trained on natural images
    can be leveraged via two different ways: by utilizing them as fixed feature extractors,
    and as an initialization which will then be fine-tuned on target medical datasets.
    These two strategies are illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating
    Knowledge from Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")(a) and Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Incorporating Knowledge from
    Natural Datasets or Other Medical Datasets ‣ 2 Disease Diagnosis ‣ A Survey on
    Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis")(b),
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74fb2fd2d81cd8d79d45403b8cddcb93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Two strategies to utilize the pre-trained network on natural images:
    (a) as a feature extractor and (b) as an initialization which will be fine-tuned
    on the target dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The first strategy takes a pre-trained network, removes its last fully-connected
    layer, and treats the rest of the network as a fixed feature extractor. Extracted
    features are then fed into a linear classifier (e.g., support vector machine (SVM)),
    which is trained on the target medical datasets. Applications in this category
    include mammography mass lesion classification [[61](#bib.bib61)] and chest pathology
    identification [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: The success of leveraging information from natural images for disease diagnosis
    can be attributed to the fact that a network pre-trained on natural images, especially
    in the earlier layers, contain more generic features (e.g., edge detectors and
    color blob detectors) [[102](#bib.bib102)].
  prefs: []
  type: TYPE_NORMAL
- en: In the second strategy, the weights of the pre-trained network are fine-tuned
    based on the medical datasets. It is possible to fine-tune the weights of all
    layers in the network, or to keep some of the earlier layers fixed and only fine-tune
    some higher-level portion of the network. This can be applied to the classification
    of skin cancer [[1](#bib.bib1)], breast cancer [[57](#bib.bib57)], thorax diseases
    [[58](#bib.bib58)], prostate cancer [[60](#bib.bib60)] and interstitial lung diseases
    [[59](#bib.bib59)] .
  prefs: []
  type: TYPE_NORMAL
- en: Besides the information from natural images, using images from other medical
    datasets is also quite popular.
  prefs: []
  type: TYPE_NORMAL
- en: Medical datasets containing images of the same or similar modality as target
    images have similar distribution and therefore can be helpful. For example, to
    classify malignant and benign breast masses in digitized screen-film mammograms
    (SFMs), a multi-task transfer learning DCNN is proposed to incorporate the information
    from digital mammograms (DMs) [[63](#bib.bib63)]. It is found to have significantly
    higher performance compared to the single-task transfer learning DCNN which only
    utilizes SFMs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, even medical images with different modalities can provide complementary
    information. For example, [[26](#bib.bib26)] uses a model pre-trained on a mammography
    dataset to show that it could obtain better results than models trained solely
    on the target dataset comprising digital breast tomosynthesis (DBT) images. Another
    example is in prostate cancer classification, where the radiofrequency ultrasound
    images are first used to train the DCNN, then the model is fine-tuned on B-mode
    ultrasound images [[64](#bib.bib64)]. Other examples of using the images from
    different modalities can be found in [[62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as datasets of different classes can help each other in classification
    tasks [[103](#bib.bib103)], medical datasets featuring images of a variety of
    diseases can also have similar morphological structures or distribution, which
    may be beneficial for other tasks. For example, a multi-task deep learning (MTDL)
    method is proposed in [[68](#bib.bib68)]. MTDL can simultaneously utilize multiple
    cancer datasets so that hidden representations among these datasets can provide
    more information to small-scale cancer datasets, and enhance the classification
    performance. Another example is a cross-disease attention network (CANet) proposed
    in [[67](#bib.bib67)]. CANet characterizes and leverages the relationship between
    diabetic retinopathy (DR) and diabetic macular edema (DME) in fundus images using
    a special designed disease-dependent attention module. Experimental results on
    two public datasets show that CANet outperforms other methods on diagnosing both
    of the two diseases.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Incorporating Knowledge from Medical Doctors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experienced medical doctors can give fairly accurate conclusion on the given
    medical images, mainly thanks to the training they have received and the expertise
    they have accumulated over many years. In general, they often follow some certain
    patterns or take some procedures when reading medical images. Incorporating these
    types of knowledge can improve the diagnostic performance of deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The types of medical domain knowledge utilized in deep learning models for
    disease diagnosis can be summarized into the following five categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the training pattern,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the general diagnostic patterns they view images,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the areas on which they usually focus,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the features (e.g., characteristics, structures, shapes) they give special attention
    to, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: other related information for diagnosis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The research works for each category will be described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Training Pattern of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training process of medical students has a character: they are trained
    by tasks with increasing difficulty. For example, students begin with some easier
    tasks, such as deciding whether an image contains lesions, later are required
    to accomplish more challenging tasks, such as determining whether the lesions
    are benign or malignant. Over time, they will advance to more difficult tasks,
    such as determining the subtypes of lesions in images.'
  prefs: []
  type: TYPE_NORMAL
- en: This training pattern can be introduced in the training process of deep neural
    networks via curriculum learning [[104](#bib.bib104)]. Curriculum determines a
    sequence of training samples ranked in ascending order of learning difficulty.
    Curriculum learning has been an active research topic in computer vision and has
    been recently utilized for medical image diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a teacher-student curriculum learning strategy is proposed for
    breast screening classification from DCE-MRI [[28](#bib.bib28)]. The deep learning
    model is trained on simpler tasks before introducing the hard problem of malignancy
    detection. This strategy shows the better performance when compared with the other
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, [[29](#bib.bib29)] presents a CNN based attention-guided curriculum
    learning framework by leveraging the severity-level attributes mined from radiology
    reports. Images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN to boost the learning process gradually.
  prefs: []
  type: TYPE_NORMAL
- en: In [[69](#bib.bib69)], the curriculum learning is adopted to support the classification
    of proximal femur fracture from X-ray images. The approach assigns a degree of
    difficulty to each training sample. By first learning ‘easy’ examples and then
    ‘hard’ ones, the model can reach a better performance even with fewer data. Other
    examples of using curriculum learning for disease diagnosis can be found in [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 General Diagnostic Patterns of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Experienced medical doctors generally follow some patterns when they read medical
    images. These patterns can be integrated into deep learning models with appropriately
    modified architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, radiologists generally follow a three-staged approach when they
    read chest X-ray images: first browsing the whole image, then concentrating on
    the local lesion areas, and finally combining the global and local information
    to make decisions. This pattern is incorporated in the architecture design of
    the network for thorax disease classification [[37](#bib.bib37)] (see Fig. [6](#S2.F6
    "Figure 6 ‣ 2.3.2 General Diagnostic Patterns of Medical Doctors ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis")). The proposed
    network has three branches, one is used to view the whole image, the second for
    viewing the local areas, and the third one for combining the global and local
    information together. The network yields state-of-the-art accuracy on the ChestX-ray14
    dataset. In addition, besides the information from the whole image and local lesion
    area, the information from lung area is also leveraged in [[76](#bib.bib76)].
    In particular, a segmentation subnetwork is first used to locate the lung area
    from the whole image, and then lesion areas are generated by using an attention
    heatmap. Finally, the most discriminative features are fused for final disease
    prediction. Another example is a Dual-Ray Net proposed to deal with the front
    and lateral chest radiography simultaneously [[77](#bib.bib77)], which also mimics
    the reading pattern of radiologists.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63db055ed18834f9dc2592a9c8e11944.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The example of leveraging the diagnostic pattern of radiologists
    for thorax disease diagnosis [[37](#bib.bib37)], where three branches are used
    to extract and combine the global and local features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the diagnosis of skin lesions, experienced dermatologists generally first
    locate lesions, then identify dermoscopic features from the lesion areas, and
    finally make diagnosis based on the features. This pattern is mimicked in the
    design of the network for the diagnosis of skin lesions [[75](#bib.bib75)]. The
    proposed network, DermaKNet, comprised several subnetworks with dedicated tasks:
    lesion-skin segmentation, detection of dermoscopic features, and global lesion
    diagnosis. The DermaKNet achieves higher performance compared to the traditional
    CNN models.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in mass identification in mammogram, radiologists generally analyze
    the bilateral and ipsilateral views simultaneously. To emulate this reading practice,
    [[78](#bib.bib78)] proposes MommiNet to perform end-to-end bilateral and ipsilateral
    analysis of mammogram images. In addition, symmetry and geometry constraints are
    also aggregated from these views. Experiments show the state-of-the-art mass identification
    accuracy on DDSM. Another example of leveraging this diagnostic pattern of medical
    doctors can be found in skin lesion diagnosis and thorax disease classification
    [[79](#bib.bib79)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 The Areas Medical Doctors Usually Focus on
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When experienced medical doctors read images, they generally focus on a few
    specific areas, as these areas are more informative than other places for the
    purpose of disease diagnosis. Therefore, the information about where medical doctors
    focus may help deep learning models yield better results.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge above is generally represented as ‘attention maps’, which are
    annotations given by medical doctors indicating the areas they focus on when reading
    images. For example, a CNN named AG-CNN explicitly incorporates the ‘attention
    maps’ for glaucoma diagnosis [[38](#bib.bib38)]. The attention maps of ophthalmologists
    are collected through a simulated eye-tracking experiment, which are used to indicate
    where they focus when reading images. An example of capturing the attention maps
    of an ophthalmologist in glaucoma diagnosis is shown in Fig. [7](#S2.F7 "Figure
    7 ‣ 2.3.3 The Areas Medical Doctors Usually Focus on ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). To incorporate the
    attention maps, an attention prediction subnet in AG-CNN is designed, and the
    attention prediction loss measuring the difference between the generated and ground
    truth attention maps (provided by ophthalmologists) is utilized to supervise the
    training process. Experimental results show that AG-CNN significantly outperforms
    the state-of-the-art glaucoma detection methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63abb68645f11e9014fbdd75d5005420.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Example of capturing the attention maps of an ophthalmologist in
    glaucoma diagnosis [[38](#bib.bib38)]. I, II, III and IV are the original blurred
    fundus image, the fixations of ophthalmologists with cleared regions, the order
    of clearing the blurred regions, and the generated attention map based on the
    captured fixations, respectively. V and VII represent the original fundus images.
    VI and VIII are the corresponding attention maps of V and VII generated by using
    the method in I-IV.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example in this category is the lesion-aware CNN (LACNN) for the classification
    of retinal optical coherence tomography (OCT) images [[80](#bib.bib80)]. The LACNN
    simulates the pattern of ophthalmologists’ diagnosis by focusing on local lesion-related
    regions. Concretely, the ‘attention maps’ are firstly represented as the annotated
    OCT images delineating the lesion regions using bounding polygons. To incorporate
    the information, the LACNN proposes a lesion-attention module to enhance the features
    from local lesion-related regions while still preserving the meaningful structures
    in global OCT images. Experimental results on two clinically acquired OCT datasets
    demonstrate the effectiveness of introducing attention maps for retinal OCT image
    classification, with 8.3% performance gain when compared with the baseline method.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, [[53](#bib.bib53)] proposes an Attention Branch Network (ABN) to
    incorporate the knowledge given by the radiologists in diabetic retinopathy. ABN
    introduces a branch structure which generates attention maps that highlight the
    attention regions of the network. During the training process, ABN allows the
    attention maps to be modified with semantic segmentation labels of disease regions.
    The semantic labels are also annotated by radiologists as the ground truth attention
    maps. Experimental results on the disease grade recognition of retina images show
    that ABN achieves 93.73% classification accuracy and its interpretability is clearer
    than conventional approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples of incorporating attention maps of medical doctors can be found
    in the diagnosis of radiotherapy-related esophageal fistula [[81](#bib.bib81)],
    breast cancer diagnosis [[82](#bib.bib82)], and short-term lesion change detection
    in melanoma screening [[83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Features That Medical Doctors Give Special Attention to
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the last decades, many guidelines and rules have gradually developed in various
    medical fields to point out some important features for diagnosis. These features
    are called *‘hand-crafted features’* as they are designated by medical doctors.
    For example, the popular ABCD rule [[105](#bib.bib105)] is widely used by dermatologists
    to classify melanocytic tumors. The ABCD rule points out four distinguishing features,
    namely asymmetry, border, color and differential structures, to determine whether
    a melanocytic skin lesion under the investigation is benign or malignant.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Features in the BI-RADS guideline to classify benign and malignant
    breast tumors in ultrasound images [[106](#bib.bib106)]'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Benign | Malignant |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | smooth, thin, regular | irregular, thick |'
  prefs: []
  type: TYPE_TB
- en: '| Shape | round or oval | irregular |'
  prefs: []
  type: TYPE_TB
- en: '| Microcalcification | no | yes |'
  prefs: []
  type: TYPE_TB
- en: '| Echo Pattern | clear | unclear |'
  prefs: []
  type: TYPE_TB
- en: '| Acoustic Attenuation | not obvious | obvious |'
  prefs: []
  type: TYPE_TB
- en: '| Side Acoustic Shadow | obvious | not obvious |'
  prefs: []
  type: TYPE_TB
- en: Another example is in the field of breast cancer diagnosis. Radiologists use
    the BI-RADS (Breast Imaging Reporting and Data System) score [[106](#bib.bib106)]
    to place abnormal findings into different categories, with a score of 1 indicating
    negative findings and a score of 6 indicating breast cancer. More importantly,
    for each imaging modality, BI-RADS indicates some features closely related to
    its scores, including margin, shape, micro-calcification, and echo pattern. For
    example, for breast ultrasound images, tumors with smooth, thin and regular margins
    are more likely to be benign, while tumors with irregular and thick margins are
    highly suspected to be malignant. Other features that can help to classify benign
    and malignant breast tumors are shown in Table [III](#S2.T3 "TABLE III ‣ 2.3.4
    Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating Knowledge
    from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis"). Similarly, for the
    benign-malignant risk assessment of lung nodules in [[59](#bib.bib59)], six high-level
    nodule features, including calcification, sphericity, margin, lobulation, spiculation
    and texture, have shown a tightly connection with malignancy scores (see Fig.
    [8](#S2.F8 "Figure 8 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis")).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26456a4a8677eac2c4f9c3d2f1c2d605.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Lung nodule attributes with different malignancy scores [[59](#bib.bib59)].(a)
    From top to the bottom, six different nodule features attribute from missing to
    highest prominence. (b) The number of nodules with different malignancy scores.'
  prefs: []
  type: TYPE_NORMAL
- en: These different kinds of hand-crafted features have been widely used in many
    traditional CAD systems. These systems generally first extract these features
    from medical images, and then feed them into some classifiers like SVM or Random
    Forest [[107](#bib.bib107), [108](#bib.bib108)]. For example, for the lung nodule
    classification on CT images, many CAD systems utilize features including the size,
    shape, morphology, and texture from the suspected lesion areas [[109](#bib.bib109),
    [54](#bib.bib54)]. Similarly, in the CAD systems for the diagnosis of breast ultrasound
    images, features such as intensity, texture and shape are used [[31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: When using deep learning models like CNNs, which have the ability to automatically
    extract representative features, there are four approaches to combining ‘hand-crafted
    features’ with features extracted from CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision-level fusion: the two types of features are fed into two classifiers
    separately, then the decisions from the two classifiers are combined.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature-level fusion: the two types of features are directly combined via techniques
    such as concatenation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input-level fusion: the hand-crafted features are represented as image patches,
    which are then taken as inputs to the CNNs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage of features as labels of CNNs: the hand-crafted features are first annotated
    and then utilized as labels for CNNs during the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Decision-level fusion: The structure of this approach is illustrated in Fig.
    [9](#S2.F9 "Figure 9 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"). In this approach, the hand-crafted features and the features extracted
    from CNNs are separately fed into two classifiers. Then, the classification results
    from both classifiers are combined using some decision fusion techniques to produce
    final classification results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4551553557c5bd87c2bdb9ec55a257d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Decision-level fusion: the decisions from two classifiers, one based
    on hand-crafted features, and the other on the CNNs, are combined.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, a skin lesion classification model proposed in [[85](#bib.bib85)]
    combines the results from two SVM classifiers. The first one uses hand-crafted
    features (i.e., RSurf features and local binary patterns (LBP)) as input and the
    second employs features derived from a CNN. Both of the classifiers predict the
    category for each tested image with a classification score. These scores are subsequently
    used to determine the final classification result.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a mammographic tumor classification method also combines features
    in decision-level fusion [[61](#bib.bib61)]. After individually performing classification
    with CNN features and analytically extracted features (e.g., contrast, texture,
    and margin spiculation), the method adopts the soft voting to combine the outputs
    from both individual classifiers. Experimental results show that the performance
    of the ensemble classifier was significantly better than the individual ones.
    Other examples that utilize this approach include lung nodule diagnosis [[86](#bib.bib86)],
    breast cancer diagnosis [[87](#bib.bib87)], the classification of cardiac slices
    [[84](#bib.bib84)] and the prediction of the invasiveness risk of stage-I lung
    adenocarcinomas [[88](#bib.bib88)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature-level fusion: In this approach, hand-crafted features and features
    extracted from CNNs are concatenated, and the combined features are fed into a
    classifier for diagnosis. The structure of this approach is illustrated in Fig.
    [10](#S2.F10 "Figure 10 ‣ 2.3.4 Features That Medical Doctors Give Special Attention
    to ‣ 2.3 Incorporating Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee039724171545db44e50f3ba608fd97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Feature-level fusion: the hand-crafted features are combined with
    the features extracted from CNNs as the new feature vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, a combined-feature based classification approach called CFBC is
    proposed for lung nodule classification by [[36](#bib.bib36)]. In CFBC, the hand-crafted
    features (including texture and shape descriptors) and the features learned by
    a nine-layer CNN are combined and fed into a back-propagation neural network.
    Experimental results derived from a publicly available dataset show that compared
    with a purely CNN model, incorporating hand-crafted features improves the accuracy,
    sensitivity, and specificity by 3.87%, 6.41%, and 3.21%, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Another example in this category is the breast cancer classification in histology
    images [[57](#bib.bib57)]. More specifically, two hand-crafted features, namely
    the parameter-free threshold adjacency statistics and the gray-level co-occurrence
    matrix, are fused with the five groups of deep learning features extracted from
    five different deep models. The results show that after incorporating hand-crafted
    features, the accuracy of the deep learning model can be significantly improved.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples of employing feature-level fusion can be found in glaucoma diagnosis
    [[90](#bib.bib90)], skin lesion classification [[89](#bib.bib89)], lung nodule
    classification [[91](#bib.bib91)] and brain tumor diagnosis [[92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Input-level fusion: In this approach, hand-crafted features are first represented
    as patches highlighting the corresponding features. Then, these patches are taken
    as input to CNNs to make the final conclusion. Using this approach, the CNNs are
    expected to rely more on the hand-crafted features. It should be noted that generally
    speaking, one CNN is required for each type of hand-crafted feature, and information
    obtained from these CNNs need to be combined in some manner to make a final decision.
    The structure of this approach is illustrated in Fig. [11](#S2.F11 "Figure 11
    ‣ 2.3.4 Features That Medical Doctors Give Special Attention to ‣ 2.3 Incorporating
    Knowledge from Medical Doctors ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d7e1908a544551c52978c7518c8f5148.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Input-level fusion: the hand-crafted features are represented as
    image patches that are taken as inputs to the CNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [[94](#bib.bib94)], three types of hand-crafted features, namely
    the contrast information of the initial nodule candidates (INCs) and the outer
    environments, the histogram of oriented gradients (HOG) feature, and the LBP feature,
    are transformed into the corresponding patches and are taken as inputs of multiple
    CNNs. The results show that this approach outperforms both conventional CNN-based
    approaches and traditional machine-learning approaches based on hand-crafted features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example using input-level fusion approach is the MV-KBC (multi-view
    knowledge-based collaborative) algorithm proposed for lung nodule classification
    [[54](#bib.bib54)]. The MV-KBC first extracts patches corresponding to three features:
    the overall appearance (OA), nodule’s heterogeneity in voxel values (HVV) and
    heterogeneity in shapes (HS). Each type of patch is fed into a ResNet. Then, the
    outputs of these ResNets are combined to generate the final classification results.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, [[93](#bib.bib93)] proposes the dual-path semi-supervised conditional
    generative adversarial networks (DScGAN) for thyroid classification. More specifically,
    the features from the ultrasound B-mode images and the ultrasound elastography
    images are first extracted as the OB patches (indicating the region of interest
    (ROI) in B-mode images), OS patches (characterizing the shape of the nodules),
    and OE patches (indicating the elasticity ROI according to the B-mode ROI position).
    Then, these patches are used as the input of the DScGAN. Using the information
    from these patches is demonstrated to be effective to improve the classification
    performance. Other examples employ input-level fusion can be found in thyroid
    nodule diagnosis [[95](#bib.bib95)] and breast cancer diagnosis on multi-sequence
    MRI [[96](#bib.bib96)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage of features as labels of CNNs: In this approach, besides the original
    classification labels of images, medical doctors also provide labels for some
    hand-crafted features. This extra information is generally incorporated into deep
    learning models via the multi-task learning structure.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [[39](#bib.bib39)], the nodules in lung images were quantitatively
    rated by radiologists with regard to nine hand-crafted features (e.g., spiculation,
    texture, and margin). The multi-task learning framework is used to incorporate
    the above information into the main task of lung nodule classification.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, for the benign-malignant risk assessment of lung nodules in low-dose
    CT scans [[59](#bib.bib59)], the binary labels about the presence of six high-level
    nodule attributes, namely the calcification, sphericity, margin, lobulation, spiculation
    and texture, are utilized. Six CNNs are designed and each one aims at learning
    one attribute. The discriminative features automatically learned by CNNs for these
    attributes are fused in a multi-task learning framework to obtain the final risk
    assessment scores.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in [[97](#bib.bib97)], each glioma nuclear image is exclusively labeled
    in terms of the shapes and attributes for the centermost nuclei of the image.
    These features are then learned by a multi-task CNN. Experiments demonstrate that
    the proposed method outperforms the baseline CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 Other Types of Information Related to Diagnosis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we summarize other types of information that can help deep
    learning models to improve their diagnostic performance. These types of information
    include extra category labels and clinical diagnostic reports.
  prefs: []
  type: TYPE_NORMAL
- en: Extra category labels
  prefs: []
  type: TYPE_NORMAL
- en: For medical images, besides a classification label (i.e., normal, malignant
    or benign), radiologists may give some extra category labels. For example, in
    the ultrasonic diagnosis of breast cancer, an image usually has a BI-RADS label
    that classifies the image into 0$\sim$6 [[106](#bib.bib106)]—category 0 suggests
    re-examination, categories 1 and 2 indicate that it is prone to be a benign lesion,
    category 3 suggests probably benign findings, categories 4 and 5 are suspected
    to be malignant, category 6 definitely suggests malignant). These labels also
    contain information about the condition of lesions. In [[52](#bib.bib52)], the
    BI-RADS label for each medical image is incorporated in a multi-task learning
    structure as the label of an auxiliary task. Experimental results show that incorporating
    these BI-RADS labels can improve the diagnostic performance of major classification
    task. Another example of using the information from BI-RADS labels can be found
    in [[100](#bib.bib100)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, during the process of image annotation, consensus or disagreement
    among experts regarding images reflects the gradeability and difficulty levels
    of the image, which is also a representation of medical domain knowledge. To incorporate
    this information,[[101](#bib.bib101)] proposes a multi-branch model structure
    to predict the most sensitive, most specifical and a balanced fused result for
    glaucoma images. Meanwhile, the consensus loss is also used to encourage the sensitivity
    and specificity branch to generate consistent and contradictory predictions for
    images with consensus and disagreement labels, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Extra clinical diagnostic reports
  prefs: []
  type: TYPE_NORMAL
- en: A clinical report is a summary of all the clinical findings and impressions
    determined during examination of a radiography study. It usually contains rich
    information and reflects the findings and descriptions of medical doctors. Incorporating
    clinical reports into CNNs designed for disease diagnosis is typically beneficial.
    As medical reports are generally handled by recurrent neural networks (RNNs),
    to incorporate information from medical reports, generally hybrid networks containing
    both CNNs and RNNs are needed.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Text-Image embedding network (TieNet) is designed to classify
    the common thorax disease in chest X-rays [[98](#bib.bib98)]. TieNet has an end-to-end
    CNN-RNN architecture enabling it to integrate information of radiological reports.
    The classification results are significantly improved (with about a 6% increase
    on average in AUCs) compared with the baseline CNN purely based on medical images.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, using semantic information from diagnostic reports is explored
    in [[99](#bib.bib99)] for understanding pathological bladder cancer images. A
    dual-attention model is designed to facilitate the high-level interaction of semantic
    information and visual information. Experiments demonstrate that incorporating
    information from diagnostic reports significantly improves the cancer diagnostic
    performance over the baseline method.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for disease
    diagnosis. Generally, almost all types of domain knowledge are proven to be effective
    in boosting the diagnostic performance, especially using the metrics of accuracy
    and AUC, some examples and their quantitative improvements are listed in Table
    [IV](#S2.T4 "TABLE IV ‣ 2.4 Summary ‣ 2 Disease Diagnosis ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: The comparison of the quantitative metrics for some disease diagnosis
    methods after integrating domain knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accuracy | AUC |'
  prefs: []
  type: TYPE_TB
- en: '| [[37](#bib.bib37)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AG-CNN only with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; global branch/AG-CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.840/0.871 |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-50/DermaKNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.874/0.908 |'
  prefs: []
  type: TYPE_TB
- en: '| [[62](#bib.bib62)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fine-tuned VGG-Net/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fine-tuned MG-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.900/0.930 | 0.950/0.970 |'
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-50/ResNet-50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with handcrafted features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.830/0.940 |'
  prefs: []
  type: TYPE_TB
- en: '| [[38](#bib.bib38)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN without using &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; attention map/AG-CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.908/0.953 | 0.966/0.975 |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet50/CANet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.828/0.851 | –/– |'
  prefs: []
  type: TYPE_TB
- en: '| [[52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VGG16/Multi-task VGG16 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.829 /0.833 | –/– |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | DenseNet/BMSL | –/– | 0.850/0.900 |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN with single-stage/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-stage transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.850/0.910 |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN/AGCL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.771/0.803 |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DScGAN without/with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.816/0.905 | 0.812/0.914 |'
  prefs: []
  type: TYPE_TB
- en: With respect to type of domain knowledge for disease diagnosis, the knowledge
    from natural images is widely incorporated in deep learning models. However, considering
    the gap between natural images and medical ones, using information from external
    medical datasets of the same diseases with similar modalities (e.g., SFM and DM)
    [[63](#bib.bib63)], with different modalities (DBT and MM, Ultrasound) [[26](#bib.bib26)],
    and even with different diseases [[68](#bib.bib68)] may be more effective. To
    incorporate the above information is relatively easy, and both transfer learning
    and multi-task learning have been widely adopted. However, there are still no
    comparative studies on how different extra datasets can improve the performance
    of deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: For the domain knowledge of medical doctors, the high-level domain knowledge
    (e.g., the training pattern and the diagnostic pattern) is generally common for
    different diseases. On the other hand, the low-level domain knowledge, such as
    the areas in images and features of diseases that medical doctors usually pay
    attention to, is generally different for different diseases. There is generally
    a trade-off between the versatility and the utility of domain knowledge. To diagnose
    a certain disease, the benefit of incorporating a versatile type of domain knowledge
    suitable for many diseases may not be as significant as using the one that is
    specific for the disease. However, identifying such specific domain knowledge
    may not be easy, and generally requires more efforts from medical doctors (e.g.,
    to identify hand-crafted features or attention maps).
  prefs: []
  type: TYPE_NORMAL
- en: We believe that more kinds of medical domain knowledge can be explored and utilized
    for disease diagnosis. In addition, comparative studies on some benchmark datasets
    should be carried out with respect to different types of domain knowledge and
    different incorporating methods for disease diagnosis. The results can provide
    further insights about the utility of medical domain knowledge for deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Lesion, Organ, and Abnormality Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Detecting lesions in medical images is an important task and a key part for
    disease diagnosis in many conditions. Similarly, organ detection is an essential
    preprocessing step for image registration, organ segmentation, and lesion detection.
    Detecting abnormalities in medical images, such as cerebral microbleeds in brain
    MRI images and hard exudates in retinal images, is also required in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the deep learning models widely used for object detection in
    medical images are first described in Subsection [3.1](#S3.SS1 "3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis"). Then, the existing works on utilizing domain
    knowledge from natural and medical datasets, and from medical doctors are introduced
    in detail in Subsection [3.2](#S3.SS2 "3.2 Incorporating Knowledge from Natural
    Datasets or Other Medical Datasets ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") and Subsection [3.3](#S3.SS3 "3.3 Incorporating Knowledge from Medical
    Doctors ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"), respectively.
    Lastly, we summarize and discuss these different types of domain knowledge and
    the associated incorporating methods in Subsection [3.4](#S3.SS4 "3.4 Summary
    ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning models designed for object detection in natural images have been
    directly applied to detect objects in medical images. These applications include
    pulmonary nodule detection in CT images [[110](#bib.bib110)], retinal diseases
    detection in retinal fundus [[111](#bib.bib111)] and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24ae3b01e5ddfcecfd137e07a5d79694.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The workflow of colitis detection method by using the Faster R-CNN
    structure [[112](#bib.bib112)].'
  prefs: []
  type: TYPE_NORMAL
- en: One popular type of model is the two-stage detectors like the Faster R-CNN [[113](#bib.bib113)]
    and the Mask R-CNN [[114](#bib.bib114)]. They generally consist of a region proposal
    network (RPN) that hypothesizes candidate object locations and a detection network
    that refines region proposals. Applications in this category include colitis detection
    in CT images [[112](#bib.bib112)], intervertebral disc detection in X-ray images
    [[115](#bib.bib115)] and the detection of architectural distortions in mammograms
    [[116](#bib.bib116)]. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1 General Structures of
    Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis") shows an example of using Faster R-CNN structure
    for colitis detection [[112](#bib.bib112)].
  prefs: []
  type: TYPE_NORMAL
- en: Another category is one-stage models like YOLO (You Only Look Once) [[117](#bib.bib117)],
    SSD (Single Shot MultiBox Detector) [[118](#bib.bib118)] and RetinaNet [[119](#bib.bib119)].
    These networks skip the region proposal stage and run detection directly by considering
    the probability that the object appears at each point in image. Compared with
    the two-stage models, models in this approach are generally faster and simpler.
    Examples in this category can be found in breast tumor detection in mammograms
    [[120](#bib.bib120)], pulmonary lung nodule detection in CT [[121](#bib.bib121)],
    and the detection of different lesions (e.g., liver lesion, lung lesion, bone
    lesion, abdomen lesion) in CT images [[122](#bib.bib122)]. An example of using
    one-stage structure for lesion detection is shown in Fig. [13](#S3.F13 "Figure
    13 ‣ 3.1 General Structures of Deep Learning Models for Object Detection in Medical
    Images ‣ 3 Lesion, Organ, and Abnormality Detection ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/956a5c6359b36ecfe818810ac33e1386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: An example of using one-stage structure for lesion detection in
    CT images [[122](#bib.bib122)].'
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will introduce related works that incorporate
    external knowledge into deep learning models for object detection in medical images.
    A summary of these works is shown in Table [V](#S3.T5 "TABLE V ‣ 3.1 General Structures
    of Deep Learning Models for Object Detection in Medical Images ‣ 3 Lesion, Organ,
    and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: List of studies on lesion, organ, and abnormality detection and the
    knowledge they incorporated'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Source &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Type &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorporating Method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| References |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural datasets &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[21](#bib.bib21)][[123](#bib.bib123)]  [[124](#bib.bib124)][[125](#bib.bib125)]  [[126](#bib.bib126)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Medical datasets &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-modal images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[127](#bib.bib127)][[128](#bib.bib128)][[129](#bib.bib129)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Medical doctors |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Training pattern &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[29](#bib.bib29)][[130](#bib.bib130)]  [[131](#bib.bib131)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Detection patterns |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Using images collected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; under different settings &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[132](#bib.bib132)][[133](#bib.bib133)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Comparing bilateral or &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross-view images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[134](#bib.bib134)][[135](#bib.bib135)]  [[136](#bib.bib136), [137](#bib.bib137)]  [[138](#bib.bib138)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Analyzing adjacent slices &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[121](#bib.bib121)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Features doctors focus on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[139](#bib.bib139)][[140](#bib.bib140)]  [[141](#bib.bib141)][[95](#bib.bib95)]  [[142](#bib.bib142)][[143](#bib.bib143)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Other related information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-task learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; /training pattern design &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[29](#bib.bib29)][[144](#bib.bib144)]  [[145](#bib.bib145)][[146](#bib.bib146)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to disease diagnosis, it is quite popular to pre-train a large natural
    image dataset (generally ImageNet) to introduce information for object detection
    in medical domain. Examples can be found in lymph node detection [[21](#bib.bib21)],
    polyp and pulmonary embolism detection [[126](#bib.bib126)], breast tumor detection
    [[123](#bib.bib123)], colorectal polyps detection [[124](#bib.bib124), [125](#bib.bib125)]
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, utilizing other medical datasets (i.e., multi-modal images) is
    also quite common. For example, PET images are used to help the lesion detection
    in CT scans of livers [[128](#bib.bib128)]. Specifically, PET images are first
    generated from CT scans using a combined structure of FCN and GAN, then the synthesized
    PET images are used in a false-positive reduction layer for detecting malignant
    lesions. Quantitative results show a 28% reduction in the average false positive
    per case. Besides, [[127](#bib.bib127)] develops a strategy to detect breast masses
    from digital tomosynthesis by fine-tuning the model pre-trained on mammography
    datasets. Another example of using multi-modal medical images can be found in
    liver tumor detection [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Incorporating Knowledge from Medical Doctors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we summarize the existing works on incorporating the knowledge
    of medical doctors to more effectively detect objects in medical images. The types
    of domain knowledge from medical doctors can be grouped into the following four
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the training pattern,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the general detection patterns they view images,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the features (e.g., locations, structures, shapes) they give special attention
    to, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: other related information regarding detection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3.1 Training Pattern of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training pattern of medical doctors, which is generally characterized as
    being given tasks with increasing difficulty, is also used for object detection
    in medical images. Similar to the disease diagnosis, most works utilize the curriculum
    learning to introduce this pattern. For example, an attention-guided curriculum
    learning (AGCL) framework is presented to locate the lesion in chest X-rays [[29](#bib.bib29)].
    During this process, images in order of difficulty (grouped by different severity-levels)
    are fed into the CNN gradually, and the disease heatmaps generated from the CNN
    are used to locate the lesion areas.
  prefs: []
  type: TYPE_NORMAL
- en: Another work is called as CASED proposed for lung nodule detection in chest
    CT [[130](#bib.bib130)]. CASED adopts a curriculum adaptive sampling technique
    to address the problem of extreme data imbalance. In particular, CASED lets the
    network to first learn how to distinguish nodules from their immediate surroundings,
    and then it adds a greater proportion of difficult-to-classify global context,
    until uniformly samples from the empirical data distribution. In this way, CASED
    tops the LUNA16 challenge leader-board with an average sensitivity score of 88.35%.
    Another example of using curriculum learning can be found in cardiac landmark
    detection [[131](#bib.bib131)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 General Detection Patterns of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When experienced medical doctors are locating possible lesions in medical images,
    they also display particular patterns, and these patterns can be incorporated
    into deep learning models for object detection. Experienced medical doctors generally
    have the following patterns: (1) they usually take into account images collected
    under different settings (e.g., brightness and contrast), (2) they often compare
    bilateral images or use cross-view images, and (3) they generally read adjacent
    slices.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, to locate possible lesions during visual inspection of the CT images,
    radiologists would combine images collected under different settings (e.g., brightness
    and contrast). To imitate the above process, a multi-view feature pyramid network
    (FPN) is proposed in [[132](#bib.bib132)], where multi-view features are extracted
    from images rendered with varied brightness and contrast. The multi-view information
    is then combined using a position-aware attention module. Experiments show that
    the proposed model achieves an absolute gain of 5.65% over the previous state-of-the-art
    method on the NIH DeepLesion dataset. Another example of using images under different
    settings can be found in the COVID-19 pneumonia-based lung lesion detection [[133](#bib.bib133)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the bilateral information is widely used by radiologists. For example,
    in standard mammographic screening, images are captured from both two breasts,
    and experienced radiologists generally compare bilateral mammogram images to find
    masses. To incorporate this pattern, a contrasted bilateral network (CBN) is proposed
    in [[134](#bib.bib134)], in which the bilateral images are first coarsely aligned
    and then fed into a pair of networks to extract features for the following detection
    steps (shown in Fig. [14](#S3.F14 "Figure 14 ‣ 3.3.2 General Detection Patterns
    of Medical Doctors ‣ 3.3 Incorporating Knowledge from Medical Doctors ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis")). Experimental results demonstrate
    the effectiveness of incorporating the bilateral information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31af6a214a801ccde849eb1be904a1d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The workflow of mammogram mass detection by integrating the bilateral
    information [[134](#bib.bib134)], where the aligned images are fed into two networks
    seperately to extract features for further detection.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, to detect acute stroke signs in non-contrast CT images, experienced
    neuroradiologists routinely compare the appearance and Hounsfield Unit intensities
    of the left and right hemispheres, and then find the regions most commonly affected
    in stroke episodes. This pattern is mimicked by [[137](#bib.bib137)] for the detection
    of dense vessels and ischaemia. The experimental results show that introducing
    the pattern greatly improves the performance for detecting ischaemia. Other examples
    of integrating the bilateral feature comparison or the symmetry constrains can
    be found in thrombus detection [[136](#bib.bib136)] and hemorrhagic lesion detection
    [[138](#bib.bib138)] in brain CT images.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the bilateral images, the information from cross views (i.e., mediolateral-oblique
    and cranio-caudal) is highly related and complementary, and hence is also used
    for mammogram mass detection. In [[135](#bib.bib135)], a bipartite graph convolutional
    network is introduced to endow the existing methods with cross-view reasoning
    ability of radiologists. Concretely, the bipartite node sets are constructed to
    represent the relatively consistent regions, and the bipartite edge are used to
    model both inherent cross-view geometric constraints and appearance similarities
    between correspondences. This process can enables spatial visual features equipped
    with cross-view reasoning ability. Experimental results on DDSM dataset achieve
    the state-of-the-art performance (with a recall of 79.5 at 0.5 false positives
    per image).
  prefs: []
  type: TYPE_NORMAL
- en: When looking for small nodules in CT images , radiologists often observe each
    slice together with adjacent slices, similar to detecting an object in a video.
    This workflow is imitated in [[121](#bib.bib121)] to detect pulmonary lung nodule
    in CT images, where the state-of-the-art object detector SSD is applied in this
    process. This method obtains state-of-the-art result with the FROC score of 0.892
    in LUNA16 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Features That Medical Doctors Give Special Attention to
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to disease diagnosis, medical doctors also use many ‘hand-crafted features’
    to help them to find target objects (e.g., nodules or lesions) in medical images.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [[140](#bib.bib140)], to detect mammographic lesions, different
    types of hand-crafted features (e.g., contrast features, geometrical features,
    and location features) are first extracted, and then concatenated with those learned
    by a CNN. The results show that these hand-crafted features, particularly the
    location and context features , can complement the network generating a higher
    specificity over the CNN alone.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, [[141](#bib.bib141)] presents a deep learning model based on Faster
    R-CNN to detect abnormalities in the esophagus from endoscopic images. In particular,
    to enhance texture details, the proposed detection system incorporates the Gabor
    handcrafted features with the CNN features through concatenation in the detection
    stage. The experimental results on two datasets (Kvasir and MICCAI 2015) show
    that the model is able to surpass the state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another example can be found in [[139](#bib.bib139)] for the detection of lung
    nodules, where 88 hand-crafted features, including intensity, shape, texture are
    extracted and combined with features extracted by a CNN and then fed into a classifier.
    Experimental results demonstrate the effectiveness of the combination of handcrafted
    features and CNN features.
  prefs: []
  type: TYPE_NORMAL
- en: In the automated detection of thyroid nodules, the size and shape attribute
    of nodules are considered in [[95](#bib.bib95)]. To incorporate the information
    above, the generating process of region proposals is constrained, and the detection
    results on two different datasets show high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in lymph node gross tumor volume detection (GTV[LN]) in oncology
    imaging, some attributes of lymph nodes (LNs) are also utilized [[142](#bib.bib142)].
    Motivated by the prior clinical knowledge that LNs from a connected lymphatic
    system, and the spread of cancer cells among LNs often follows certain pathways,
    a LN appearance and inter-LN relationship learning framework is proposed for GTV[LN]
    detection. More specifically, the instance-wise appearance features are first
    extracted by a 3D CNN, then a graph neural network (GNN) is used to model the
    inter-LN relationships, and the global LN-tumor spatial priors are included in
    this process. This method significantly improves over state-of-the-art method.
    Another example of combining handcrafted features and deep features can be found
    in lung lesion detection [[143](#bib.bib143)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4 Other Types of Information Related to Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar with that in disease diagnosis, there are other information (e.g., radiological
    reports, extra labels) can also be integrated into the lesion detection process.
  prefs: []
  type: TYPE_NORMAL
- en: For example in [[29](#bib.bib29)], to locate thoracic diseases on chest radiographs,
    the difficulty of each sample, represented as the severity level of the thoracic
    disease, is first extracted from radiology reports. Then, the curriculum learning
    technique is adopted, in which the training samples are presented to the network
    in order of increasing difficulty. Experiments on the ChestXray14 database validate
    the effectiveness on significant performance improvement over baseline methods.
  prefs: []
  type: TYPE_NORMAL
- en: Example of using extra labels can be found in [[144](#bib.bib144)]. In this
    method, the information of the classification labels is incorporated to help the
    lesion localization in chest X-rays and mammograms. In particular, a framework
    named as self-transfer learning (STL) is proposed, which jointly optimizes both
    classification and localization networks to help the localization network focus
    on correct lesions. Experimental results show that STL can achieve significantly
    better localization performance compared to previous weakly supervised localization
    approaches. More examples of using extra labels can be found in detection in mammograms
    [[145](#bib.bib145), [146](#bib.bib146)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we introduced different kinds of domain knowledge
    and the corresponding integrating methods into the deep learning models for object
    detection in medical images. Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion,
    Organ, and Abnormality Detection ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis") illustrates the quantitative improvements,
    in terms of sensitivity and recall, of some typical work over the baseline methods
    for object detection in medical images. From the results we can see that in general,
    integrating domain knowledge can be beneficial for detection tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to disease diagnosis, the high-level training pattern of medical doctors
    is generic and can be utilized for detecting different diseases or organs. In
    contrast, the low-level domain knowledge, like the detection patterns that medical
    doctors adopt and some hand-crafted features they give more attention when searching
    lesions, are generally different for different diseases. For example, the pattern
    of comparing bilateral images can only be utilized for detecting organs with symmetrical
    structures [[137](#bib.bib137), [134](#bib.bib134)]. In addition, we can see from
    Table [VI](#S3.T6 "TABLE VI ‣ 3.4 Summary ‣ 3 Lesion, Organ, and Abnormality Detection
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis") that leveraging pattern of medical doctors on average shows better
    performance when compared with integrating hand-crafted features. This may indicate
    that there is still large room to explore more effective features for object detection
    in medical images.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: The comparison of the quantitative metrics for some medical object
    detection methods after incorporating domain knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Baseline model/With domain knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sensitivity | Recall |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN/CNN with hand-crafted features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.890/0.909 | –/– |'
  prefs: []
  type: TYPE_TB
- en: '| [[141](#bib.bib141)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet/CNN with handcrafted features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.940/0.950 |'
  prefs: []
  type: TYPE_TB
- en: '| [[121](#bib.bib121)] | SSD/MSSD | 0.927/0.976 | –/– |'
  prefs: []
  type: TYPE_TB
- en: '| [[134](#bib.bib134)] | Mask R-CNN/CBD | –/– | 0.869/0.890 |'
  prefs: []
  type: TYPE_TB
- en: '| [[135](#bib.bib135)] | Mask R-CNN/BG-RCNN | –/– | 0.918/0.945 |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN/AGCL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| –/– | 0.660/0.730 |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance is evaluated at 4 false positives per image in [[139](#bib.bib139),
    [134](#bib.bib134), [135](#bib.bib135), [26](#bib.bib26)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Lesion and Organ Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical image segmentation devotes to identifying pixels of lesions or organs
    from the background, and is generally regarded as a prerequisite step for the
    lesion assessment and disease diagnosis. Segmentation methods based on deep learning
    models have become the dominant technique in recent years and have been widely
    used for the segmentation of lesions such as brain tumors [[147](#bib.bib147)],
    breast tumors [[148](#bib.bib148)], and organs such as livers [[149](#bib.bib149)]
    and pancreas [[150](#bib.bib150)].
  prefs: []
  type: TYPE_NORMAL
- en: In Subsection [4.1](#S4.SS1 "4.1 General Structures of Deep Learning Models
    for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation ‣
    A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), we describe the models that are generally used for object segmentation
    in the medical domain. Then in Subsection [4.2](#S4.SS2 "4.2 Incorporating Knowledge
    from Natural Datasets or Other Medical Datasets ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis"), the works of utilizing domain knowledge from natural and other medical
    datasets are introduced. Then, the models utilizing domain knowledge from medical
    doctors are introduced in Subsection [4.3](#S4.SS3 "4.3 Incorporating Knowledge
    from Medical Doctors ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis"). A summary of
    this section is provided in Subsection [4.4](#S4.SS4 "4.4 Summary ‣ 4 Lesion and
    Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning
    for Medical Image Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 General Structures of Deep Learning Models for Object Segmentation in Medical
    Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deep learning models utilized for medical image segmentation are generally
    divided into three categories: the FCN (fully convolutional network) [[151](#bib.bib151)]
    based models, the U-Net [[152](#bib.bib152)] based models, and the GAN [[153](#bib.bib153)]
    based models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ce4537d0995a0260396a4aa27213369.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The schematic diagram of using FCN structure for cardiac segmentation
    [[154](#bib.bib154)].'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the FCN has been proven to perform well in various medical image
    segmentation tasks [[155](#bib.bib155), [156](#bib.bib156)]. Some variants of
    FCN, such as cascaded FCN [[157](#bib.bib157)], parallel FCN [[158](#bib.bib158)]
    and recurrent FCN [[159](#bib.bib159)] are also widely used for segmentation tasks
    in medical images. Fig. [15](#S4.F15 "Figure 15 ‣ 4.1 General Structures of Deep
    Learning Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ
    Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for
    Medical Image Analysis") illustrates an example of using FCN based model for cardiac
    segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be8d2746eeb8929d9d868d196e4abaf5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The network structure of U-Net [[152](#bib.bib152)].'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the U-Net [[152](#bib.bib152)] (shown in Fig. [16](#S4.F16 "Figure
    16 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation in
    Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain
    Knowledge into Deep Learning for Medical Image Analysis")) and its variants are
    also widely utilized for medical image segmentation. U-Net builds upon FCN structure,
    mainly consists of a series of convolutional and deconvolutional layers, and with
    the short connections between the layers of equal resolution. U-Net and its variants
    like UNet++[[160](#bib.bib160)] and recurrent U-Net [[161](#bib.bib161)] perform
    well in many medical image segmentation tasks[[162](#bib.bib162)].
  prefs: []
  type: TYPE_NORMAL
- en: In the GAN-based models [[163](#bib.bib163), [164](#bib.bib164)], the generator
    is used to predict the mask of the target based on some encoder-decoder structures
    (such as FCN or U-Net). The discriminator serves as a shape regulator that helps
    the generator to obtain satisfactory segmentation results. Applications of using
    GAN-based models in medical image segmentation include brain segmentation [[165](#bib.bib165)],
    skin lesion segmentation [[166](#bib.bib166)], vessel segmentation [[167](#bib.bib167)]
    and anomaly segmentation in retinal fundus images [[168](#bib.bib168)]. Fig. [17](#S4.F17
    "Figure 17 ‣ 4.1 General Structures of Deep Learning Models for Object Segmentation
    in Medical Images ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating
    Domain Knowledge into Deep Learning for Medical Image Analysis") is an example
    of using GAN-based model for vessel segmentation in miscroscopy images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a23a66ce2424bc3820148a5a0c2e348f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: An example of using a GAN-based model for vessel segmentation [[167](#bib.bib167)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: The list of researches of lesion, organ segmentation and the knowledge
    they incorporated'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Source &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge Type &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorporating &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| References |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural datasets &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[155](#bib.bib155)][[169](#bib.bib169)][[170](#bib.bib170)][[171](#bib.bib171)]  [[126](#bib.bib126)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Medical datasets | Multi-modal images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Transfer learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[172](#bib.bib172)][[34](#bib.bib34)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-task/-modal learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[173](#bib.bib173)][[174](#bib.bib174)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Modality transformation /synthesis &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[175](#bib.bib175)][[176](#bib.bib176)]  [[177](#bib.bib177)]  [[165](#bib.bib165)]  [[178](#bib.bib178)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[179](#bib.bib179)][[180](#bib.bib180)][[181](#bib.bib181)][[182](#bib.bib182)]  [[183](#bib.bib183)][[184](#bib.bib184)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets of other diseases | Transfer learning | [[185](#bib.bib185)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Disease domain transformation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '| Medical doctors |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Training pattern &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Curriculum learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[186](#bib.bib186)]  [[30](#bib.bib30)]  [[187](#bib.bib187)][[188](#bib.bib188)]  [[189](#bib.bib189)][[190](#bib.bib190)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Diagnostic patterns |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Using different views as input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[191](#bib.bib191)][[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attention mechanism &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[192](#bib.bib192)] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Network design &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[193](#bib.bib193)][[194](#bib.bib194)] |'
  prefs: []
  type: TYPE_TB
- en: '| Anatomical priors |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Incorporated in post-processing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[195](#bib.bib195)]  [[196](#bib.bib196), [197](#bib.bib197)]  [[33](#bib.bib33)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Incorporated in loss function &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[198](#bib.bib198)][[199](#bib.bib199)]  [[200](#bib.bib200)][[201](#bib.bib201)]  [[40](#bib.bib40)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[202](#bib.bib202)]  [[198](#bib.bib198)]  [[203](#bib.bib203)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generative models &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[32](#bib.bib32)][[204](#bib.bib204)]  [[205](#bib.bib205)][[206](#bib.bib206)]  [[207](#bib.bib207)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[208](#bib.bib208)]  [[209](#bib.bib209)][[210](#bib.bib210)]  [[211](#bib.bib211)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hand-crafted features |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Feature level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[212](#bib.bib212)][[213](#bib.bib213)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input level fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[214](#bib.bib214)][[215](#bib.bib215)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will introduce research studies that incorporate
    domain knowledge into deep segmentation models. The summary of these works is
    shown in Table [VII](#S4.T7 "TABLE VII ‣ 4.1 General Structures of Deep Learning
    Models for Object Segmentation in Medical Images ‣ 4 Lesion and Organ Segmentation
    ‣ A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image
    Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Incorporating Knowledge from Natural Datasets or Other Medical Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is also quite common that deep learning segmentation models are firstly trained
    on a large-scale natural image dataset (e.g., ImageNet) and then fine-tuned on
    the target datasets. Using the above transfer learning strategy to introduce knowledge
    from natural images has demonstrated to achieve a better performance in medical
    image segmentation. Examples can be found in intima-media boundary segmentation
    [[126](#bib.bib126)] and prenatal ultrasound image segmentation[[170](#bib.bib170)].
    Besides ImageNet, [[155](#bib.bib155)] adopts the off-the-shelf DeepLab model
    trained on the PASCAL VOC dataset for anatomical structure segmentation in ultrasound
    images. This pre-trained model is also used in the deep contour-aware network
    (DCAN), which is designed for the gland segmentation in histopathological images
    [[169](#bib.bib169)].
  prefs: []
  type: TYPE_NORMAL
- en: Besides using models pre-trained on ‘static’ datasets like ImageNet and PASCAL
    VOC, many deep neural networks, especially those designed for the segmentation
    of 3D medial images, leverage models pre-trained on large-scale video datasets.
    For example, in the automatic segmentation of proximal femur in 3D MRI, the C3D
    pre-trained model is adopted as the encoder of the proposed 3D U-Net [[171](#bib.bib171)].
    Notably, the C3D model is trained on the Sports-1M dataset, which is the largest
    video classification benchmark with 1.1 million sports videos in 487 categories
    [[216](#bib.bib216)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition to natural images, using knowledge from external medical datasets
    with different modalities and with different diseases is also quite popular.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [[172](#bib.bib172)] investigates the transferability of the acquired
    knowledge of a CNN model initially trained for WM hyper-intensity segmentation
    on legacy low-resolution data to new data from the same scanner but with higher
    image resolution. Likewise, the images with different MRI scanners and protocols
    are used in [[34](#bib.bib34)] to help the multi sclerosis segmentation process
    via transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: In [[173](#bib.bib173)], the multi-task learning is adopted, where the data
    of brain MRI, breast MRI and cardiac CT angiography (CTA) are used simultaneously
    as multiple tasks. On the other hand, [[174](#bib.bib174)] adopts a multi-modal
    learning structure for organ segmentation. A dual-stream encoder-decoder architecture
    is proposed to learn modality-independent, and thus, generalisable and robust
    features shared among medical datasets with different modalities (MRI and CT images).
    Experimental results prove the effectiveness of this multi-modal learning structure.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, many works adopt GAN-based models to achieve the domain transformation
    among datasets with different modalities. For example, a model named SeUDA (unsupervised
    domain adaptation) is proposed for the left/right lung segmentation process [[175](#bib.bib175)].
    It leverages the semantic-aware GAN to transfer the knowledge from one chest dataset
    to another. In particular, target images are first mapped towards the source data
    space via the constraint of a semantic-aware GAN loss. Then the segmentation results
    are obtained from the segmentation DNN learned on the source domain. Experimental
    results show that the segmentation performance of SeUDA is highly competitive.
  prefs: []
  type: TYPE_NORMAL
- en: More examples of using the knowledge from images with other modalities can be
    found in brain MRI segmentation [[165](#bib.bib165), [182](#bib.bib182)], cardiac
    segmentation [[178](#bib.bib178), [183](#bib.bib183), [181](#bib.bib181), [180](#bib.bib180),
    [184](#bib.bib184)], liver segmentation [[179](#bib.bib179)], lung tumor segmentation
    [[177](#bib.bib177)], cardiac substructure and abdominal multi-organ segmentation
    [[176](#bib.bib176)].
  prefs: []
  type: TYPE_NORMAL
- en: There are also a few works utilize the datasets of other diseases. For instance,
    [[185](#bib.bib185)] first builds a union dataset (3DSeg-8) by aggregating eight
    different 3D medical segmentation datasets, and designs the Med3D network to co-train
    based on 3DSeg-8\. Then the pre-trained models obtained from Med3D are transferred
    into lung and liver segmentation tasks. Experiments show that this method not
    only improves the accuracy, but also accelerates the training convergence speed.
    In addition, the annotated retinal images are used to help the cardiac vessel
    segmentation without annotations [[27](#bib.bib27)]. In particular, a shape-consistent
    generative adversarial network (SC-GAN) is used to generate the synthetic images
    and the corresponding labels. Then the synthetic images are used to train the
    segmentor. Experiments demonstrate the supreme accuracy of coronary artery segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Incorporating Knowledge from Medical Doctors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The domain knowledge of medical doctors is also widely adopted when designing
    deep learning models for segmentation tasks in medical images. The types of domain
    knowledge from medical doctors utilized in deep segmentation models can be divided
    into four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the training pattern,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the general diagnostic patterns they view images,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the anatomical priors (e.g., shape, location, topology) of lesions or organs,
    and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: other hand-crafted features they give special attention to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3.1 Training Pattern of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to disease diagnosis and lesion/organ detection, many research works
    for the object segmentation in medical images also mimic the training pattern
    of medical doctors, which involves assigning tasks that increase in difficulty
    over time. In this process, the curriculum learning technique or its derivative
    methods like self-paced learning (SPL) are also utilized [[217](#bib.bib217)].
  prefs: []
  type: TYPE_NORMAL
- en: For example, for the segmentation of multi-organ CT images [[186](#bib.bib186)],
    each annotated image is divided into small patches. During the training process,
    patches producing large error by the network are selected with a high probability.
    In this manner, the network can focus sampling on difficult regions, resulting
    in improved performance. In addition, [[30](#bib.bib30)] combines the SPL with
    the active learning for the pulmonary segmentation in 3D images. This system achieves
    the state-of-the-art segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a three-stage curriculum learning approach is proposed for liver tumor
    segmentation [[187](#bib.bib187)]. The first stage is performed on the whole input
    volume to initialize the network, then the second stage of learning focuses on
    tumor-specific features by training the network on the tumor patches, and finally
    the network is retrained on the whole input in the third stage. This approach
    exhibits significant improvement when compared with the commonly used cascade
    counterpart in MICCAI 2017 liver tumor segmentation (LiTS) challenge dataset.
    More examples can also be found in left ventricle segmentation [[188](#bib.bib188)],
    finger bones segmentation [[189](#bib.bib189)] and vessel segmentation [[190](#bib.bib190)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 General Diagnostic Patterns of Medical Doctors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the lesion or organ segmentation tasks, some specific patterns that medical
    doctors adopted are also incorporated into the network.
  prefs: []
  type: TYPE_NORMAL
- en: For example, during the visual inspection of CT images, radiologists often change
    window widths and window centers to help to make decision on uncertain nodules.
    This pattern is mimicked in [[191](#bib.bib191)]. In particular, image patches
    of different window widths and window centers are stacked together as the input
    of the deep learning model to gain rich information. The evaluation implemented
    on the public LIDC-IDRI dataset indicates that the proposed method achieves promising
    performance on lung nodule segmentation compared with the state-of-the-art methods.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, experienced clinicians generally assess the cardiac morphology
    and function from multiple standard views, using both long-axis (LA) and short-axis
    (SA) images to form an understanding of the cardiac anatomy. Inspired by the above
    observation, a cardiac MR segmentation method is proposed which takes three LA
    and one SA views as the input [[32](#bib.bib32)]. In particular, the features
    are firstly extracted using a multi-view autoencoder (MAE) structure, and then
    are fed in a segmentation network. Experimental results show that this method
    has a superior segmentation accuracy over state-of-the-art methods.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, expert manual segmentation usually relies on the boundaries of
    anatomical structures of interest. For instance, radiologists segmenting a liver
    from CT images would usually trace liver edges first, and then deduce the internal
    segmentation mask. Correspondingly, boundary-aware CNNs are proposed in [[192](#bib.bib192)]
    for medical image segmentation. The networks are designed to account for organ
    boundary information, both by providing a special network edge branch and edge-aware
    loss terms. The effectiveness of these boundary aware segmentation networks are
    tested on BraTS 2018 dataset for the task of brain tumor segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the diagnostic pattern named as ‘divide-and-conquer manner’ is mimicked
    in the GTV[LN] detection and segmentation method [[193](#bib.bib193)]. Concretely,
    the GTV[LN] is first divided into two subgroups of ‘tumor-proximal’ and ‘tumor-distal’,
    by means of binary of soft distance gating. Then a multi-branch detection-by-segmentation
    network is trained with each branch specializing on learning one GTV[LN] category
    features. After fusing the outs from multi-branch, the method shows significant
    improvements on the mean recall from 72.5% to 78.2%. Another example of using
    the diagnostic pattern of medical doctors can be found in gross tumor and clinical
    target volume segmentation [[194](#bib.bib194)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Anatomical Priors of Lesions or Organs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In comparison to non-medical images, medical images have many anatomical priors
    such as the shape, position and topology of organs or lesions. Experienced medical
    doctors greatly rely on these anatomical priors when they are doing segmentation
    tasks on these images. Incorporating the knowledge of anatomical priors into deep
    learning models has been demonstrated to be an effective way for accurate medical
    image segmentation. Generally speaking, there are three different approaches to
    incorporate these anatomical priors into deep learning models: (1) incorporating
    anatomical priors in the post-processing stage, (2) incorporating anatomical priors
    as regularization terms in the loss function and (3) learning anatomical priors
    via generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating anatomical priors in post-processing stage
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to incorporate the anatomical priors in the post processing
    stage. The result of a segmentation network is often blurry and post-processing
    is generally needed to refine the segmentation result.
  prefs: []
  type: TYPE_NORMAL
- en: For example, according to the pathology, most of breast tumors begin in glandular
    tissues and are located inside the mammary layer [[218](#bib.bib218)]. This position
    feature is utilized by [[195](#bib.bib195)] in its post-processing stage where
    a fully connected conditional random field (CRF) model is employed. In particular,
    the position of tumors and their relative locations with mammary layer are added
    as a new term in CRF energy function to obtain better segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, some research first learn the anatomical priors, and then incorporate
    them into the post-processing stage to help produce anatomically plausible segmentation
    results [[33](#bib.bib33), [196](#bib.bib196), [197](#bib.bib197)]. For instance,
    the latent representation of anatomically correct cardiac shape is first learned
    by using adversarial variational autoencoder (aVAE), then be used to convert erroneous
    segmentation maps into anatomically plausible ones [[196](#bib.bib196)]. Experiments
    manifest that aVAE is able to accommodate any segmentation method, and convert
    its anatomically implausible results to plausible ones without affecting its overall
    geometric and clinical metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Another example in [[33](#bib.bib33)] introduces the post-processing step based
    on denoising autoencoders (DAE) for lung segmentation. In particular, the DAE
    is trained using only segmentation masks, then the learned representations of
    anatomical shape and topological constraints are imposed on the original segmentation
    results (as shown in Fig. [18](#S4.F18 "Figure 18 ‣ 4.3.3 Anatomical Priors of
    Lesions or Organs ‣ 4.3 Incorporating Knowledge from Medical Doctors ‣ 4 Lesion
    and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge into Deep
    Learning for Medical Image Analysis")). By applying the Post-DAE on the resulting
    masks from arbitrary segmentation methods, the lung anatomical segmentation of
    X-ray images shows plausible results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2960236e9b94ca1410afe2987350d46f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The example of integrating the shape prior in the post-process stage
    [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating anatomical priors as regularization terms in the loss function
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach is incorporating anatomical priors as regularization terms
    in the objective function of segmentation networks. For example, for the segmentation
    of cardiac MR images, a network called as SRSCN is proposed [[40](#bib.bib40)].
    SRSCN comprises a shape reconstruction neural network (SRNN) and a spatial constraint
    network (SCN). SRNN aims to maintain a realistic shape of the resulting segmentation
    and the SCN is adopted to incorporate the spatial information of the 2D slices.
    The loss of the SRSCN comes from three parts: the segmentation loss, the shape
    reconstruction (SR) loss for shape regularization, and the spatial constraint
    (SC) loss to assist segmentation. The results using images from 45 patients demonstrate
    the effectiveness of the SR and SC regularization terms, and show the superiority
    of segmentation performance of the SRSCN over the conventional schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example in this category is the one designed for skin lesion segmentation
    [[201](#bib.bib201)]. In this work, the star shape prior is encoded as a new loss
    term in a FCN to improve its segmentation of skin lesions from their surrounding
    healthy skin. In this manner, the non-star shape segments in FCN prediction maps
    are penalized to guarantee a global structure in segmentation results. The experimental
    results on the ISBI 2017 skin segmentation challenge dataset demonstrate the advantage
    of regularizing FCN parameters by the star shape prior.
  prefs: []
  type: TYPE_NORMAL
- en: More examples in this category can be found in gland segmentation [[200](#bib.bib200)],
    kidney segmentation [[199](#bib.bib199)], liver segmentation [[202](#bib.bib202)]
    and cardiac segmentation [[198](#bib.bib198), [203](#bib.bib203)].
  prefs: []
  type: TYPE_NORMAL
- en: Learning anatomical priors via generative models
  prefs: []
  type: TYPE_NORMAL
- en: In the third approach, the anatomical priors (especially the shape prior) are
    learned by some generative models first and then incorporated into segmentation
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the cardiac MR segmentation process, a shape multi-view autoencoder
    (MAE) is proposed to learn shape priors from MR images of multiple standard views
    [[32](#bib.bib32)]. The information encoded in the latent space of the trained
    shape MAE is incorporated into multi-view U-Net (MV U-Net) in the fuse block to
    guide the segmentation process.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is shown in [[206](#bib.bib206)], where the shape constrained
    network (SCN) is proposed to incorporate the shape prior into the eye segmentation
    network. More specifically, the prior information is first learned by a VAE-GAN,
    and then the pre-trained encoder and discriminator are leveraged to regularize
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: More examples can be found in brain geometry segmentation in MRI [[204](#bib.bib204)],
    3D fine renal artery segmentation [[205](#bib.bib205)], overlapping cervical cytoplasms
    segmentation [[207](#bib.bib207)], scapula segmentation [[208](#bib.bib208)],
    liver segmentation [[209](#bib.bib209)], carotid segmentation [[210](#bib.bib210)],
    and head and neck segmentation [[211](#bib.bib211)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Other Hand-crafted Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides anatomical priors, some hand-crafted features are also utilized for
    segmentation tasks. Generally speaking, there are two ways to incorporate the
    hand-crafted features into deep learning models: the feature-level fusion and
    the input-level fusion.'
  prefs: []
  type: TYPE_NORMAL
- en: In the feature-level fusion, the hand-crafted features and the features learned
    by the deep models are concatenated. For example, for the gland segmentation in
    histopathology images [[213](#bib.bib213)], two handcrafted features, namely invariant
    LBP features as well as $H\&amp;E$ components, are firstly calculated from images.
    Then these features are concatenated with the features generated from the last
    convolutional layer of the network for predicting the segmentation results. Similarly,
    in the brain structure segmentation [[212](#bib.bib212)], the spatial atlas prior
    is first represented as a vector and then concatenated with the deep features.
  prefs: []
  type: TYPE_NORMAL
- en: For the input-level fusion, the hand-crafted features are transformed into the
    input patches. Then the original image patches and the feature-transformed patches
    are fed into a deep segmentation network. For example in [[214](#bib.bib214)],
    for automatic brain tumor segmentation in MRI images, three handcrafted features
    (i.e., mean intensity, LBP and HOG) are firstly extracted. Based on these features,
    a SVM is employed to generate confidence surface modality (CSM) patches. Then
    the CSM patches and the original patches from MRI images are fed into a segmentation
    network. This method achieves good performance on BRATS2015 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: The comparison of the quantitative metrics for some medical object
    segmentation methods after incorporating domain knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Baseline Model/With Domain Knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dice score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[32](#bib.bib32)] | 3D U-Net/MV U-Net | 0.923/0.926 |'
  prefs: []
  type: TYPE_TB
- en: '| [[205](#bib.bib205)] | V-Net/DPA-DenseBiasNet | 0.787/0.861 |'
  prefs: []
  type: TYPE_TB
- en: '| [[177](#bib.bib177)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Masked cycle-GAN/Tumore &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; aware semi-unsupervised &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.630/0.800 |'
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; modality specific method/dual-stream &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; encoder-decoder multi-model method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.838/0.860 |'
  prefs: []
  type: TYPE_TB
- en: '| [[40](#bib.bib40)] | U-Net/SRSCN | 0.737/0.830 |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net/SC-GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.742/0.824 |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; cycle- and shape-consistency GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; trained without/with synthetic data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.678/0.744 |'
  prefs: []
  type: TYPE_TB
- en: In addition, using handcrafted features by input-level fusion is also adopted
    in cell nuclei segmentation [[215](#bib.bib215)]. In particular, as nuclei are
    expected to have an approximately round shape, a map of gradient convergence is
    computed and be used by CNN as an extra channel besides the fluorescence microscopy
    image. Experimental results show higher F1-score when compared with other methods.
    Another example in this category can be found in brain tumor segmentation [[214](#bib.bib214)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aforementioned sections described researches of incorporating domain knowledge
    for object (lesion or organ) segmentation in medical images. The segmentation
    performance of some methods is shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3.4
    Other Hand-crafted Features ‣ 4.3 Incorporating Knowledge from Medical Doctors
    ‣ 4 Lesion and Organ Segmentation ‣ A Survey on Incorporating Domain Knowledge
    into Deep Learning for Medical Image Analysis"), where the Dice score is used
    with a higher score indicating a better performance.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that similar to disease diagnosis, using information from natural
    images like ImageNet is quite popular for lesion and organ segmentation tasks.
    The reason behind it may be that segmentation can be seen as a specific classification
    problem. Meanwhile, besides the ImageNet, some video datasets can also be utilized
    for segmenting 3D medical images (e.g., [[171](#bib.bib171)]). Using extra medical
    datasets with different modalities has also been proven to be helpful, although
    most applications are limited in using MRI to help segmentation tasks in CT images
    [[174](#bib.bib174)]. Leveraging domain knowledge from medical doctors is also
    widely used in segmentation tasks. In particular, the anatomical priors of organs
    are widely adopted. However, anatomical priors are only suitable for the segmentation
    of organs with fixed shapes like hearts [[32](#bib.bib32)] or lungs [[32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: 5 Other Medical Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly introduce the works on incorporating domain knowledge
    in other medical images analysis applications, like medical image reconstruction,
    medical image retrieval and medical report generation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Medical Image Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The objective of medical image reconstruction is reconstructing a diagnostic
    image from a number of measurements (e.g., X-ray projections in CT or the spatial
    frequency information in MRI). Deep learning based methods have been widely applied
    in this field [[219](#bib.bib219), [220](#bib.bib220)]. It is also quite common
    that external information is incorporated into deep learning models for medical
    image reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods incorporate hand-crafted features in the medical image reconstruction
    process. For example, a network model called as DAGAN is proposed for the reconstruction
    of compressed sensing magnetic resonance imaging (CS-MRI) [[221](#bib.bib221)].
    In the DAGAN, to better preserve texture and edges in the reconstruction process,
    the adversarial loss is coupled with a content loss. In addition, the frequency-domain
    information is incorporated to enforce similarity in both the image and frequency
    domains. Experimental results show that the DAGAN method provides superior reconstruction
    with preserved perceptual image details.
  prefs: []
  type: TYPE_NORMAL
- en: In [[222](#bib.bib222)], a new image reconstruction method is proposed to solve
    the limited-angle and limited sources breast cancer diffuse optical tomography
    (DOT) image reconstruction problem in a strong scattering medium. By adaptively
    focusing on important features and filtering irrelevant and noisy ones using the
    Fuzzy Jaccard loss, the network is able to reduce false positive reconstructed
    pixels and reconstruct more accurate images.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a GAN-based method is proposed to recover MRI images of the target
    contrast [[223](#bib.bib223)]. The method simultaneously leverages the relatively
    low-spatial-frequency information available in the collected evidence for the
    target contrast and the relatively high-spatial frequency information available
    in the source contrast. Demonstrations on brain MRI datasets indicate the proposed
    method outperforms state-of-the-art reconstruction methods, with enhanced recovery
    of high-frequency tissue structure, and improved reliability against feature leakage
    or loss.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Medical Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hospitals are producing large amount of imaging data and the development
    of medical image retrieval, especially the content based image retrieval (CBIR)
    systems can be of great help to aid clinicians in browsing these large datasets.
    Deep learning methods have been applied to CBIR and have achieved high performance
    due to their superior capability for extracting features automatically.
  prefs: []
  type: TYPE_NORMAL
- en: It is also quite common that these deep learning models for CBIR utilize external
    information beyond the given medical datasets. Some methods adopt the transfer
    learning to utilize the knowledge from natural images or external medical datasets
    [[224](#bib.bib224), [225](#bib.bib225), [226](#bib.bib226)]. For example, the
    VGG model pre-trained based on ImageNet is used in brain tumor retrieval process
    [[226](#bib.bib226)], where a block-wise fine-tuning strategy is proposed to enhance
    the retrieval performance on the T1-weighted CE-MRI dataset. Another example can
    be found in x-ray image retrieval process [[225](#bib.bib225)], where a model
    pre-trained on the large augmented dataset is fine-tuned on the target dataset
    to extract general features.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, as features play an important role in the similarly analysis in CBIR,
    some methods fuse prior features with deep features. In particular, in the chest
    radiograph image retrieval process, the decision values of binary features and
    texture features are combined with the deep features in the form of decision-level
    fusion [[227](#bib.bib227)]. Similarly, the metadata such as patients’ age and
    gender is combined with the image-based features extracted from deep CNN for X-ray
    chest pathology image retrieval [[228](#bib.bib228)]. Furthermore, the features
    extracted from saliency areas can also be injected into the features extracted
    from the whole image for the high retrieval accuracy [[224](#bib.bib224)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Medical Report Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, deep learning models for image captioning have been successfully applied
    for automatic generation of medical reports [[229](#bib.bib229), [230](#bib.bib230)].
    It is also found that incorporating external knowledge can help deep learning
    models to generate better medical reports.
  prefs: []
  type: TYPE_NORMAL
- en: For example, some methods try to incorporate specific or general patterns that
    doctors adopt when writing reports. For example, radiologists generally write
    reports using certain templates. Therefore, some templates are used during the
    sentence generation process [[231](#bib.bib231), [232](#bib.bib232)]. Furthermore,
    as the explanation given by doctors is fairly simple and phrase changing does
    not change their meaning, a model-agnostic method is presented to learn the short
    text description to explain this decision process [[233](#bib.bib233)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, radiologists follow some procedures when writing reports: they
    generally first check a patient’s images for abnormal findings, then write reports
    by following certain templates, and adjust statements in the templates for each
    individual case when necessary [[234](#bib.bib234)]. This process is mimicked
    in [[232](#bib.bib232)], which first transfers the visual features of medical
    images into an abnormality graph, then retrieves text templates based on the abnormalities
    and their attributes for chest X-ray images.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[235](#bib.bib235)], a pre-constructed graph embedding module (modeled with
    a graph CNN) on multiple disease findings is utilized to assist the generation
    of reports. The incorporation of knowledge graph allows for dedicated feature
    learning for each disease finding and the relationship modeling between them.
    Experiments on the publicly accessible dataset (IU-RR) demonstrate the superior
    performance of the method integrated with the proposed graph module.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Research Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aforementioned sections reviewed research studies on deep learning models
    that incorporate medical domain knowledge for various tasks. Although using medical
    domain knowledge in deep learning models is quite popular, there are still many
    difficulties about the selection, representation and incorporating method of medical
    domain knowledge. In the following sections, we summarize challenges and future
    directions in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 The Challenges Related to the Identification and Selection of Medical Domain
    Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying medical domain knowledge is not an easy task. Firstly, the experiences
    of medical doctors are generally subjective and fuzzy. Not all medical doctors
    can give accurate and objective descriptions on what kinds of experiences they
    have leveraged to finish a given task. In addition, experiences of medical doctors
    can vary significantly or even contradictory to each other. Furthermore, medical
    doctors generally utilize many types of domain knowledge simultaneously. Finally,
    currently the medical domain knowledge is identified manually, and there is no
    existing work on the automatically and comprehensively identifying medical domain
    knowledge for a given area.
  prefs: []
  type: TYPE_NORMAL
- en: One solution to the automatic identifying medical knowledge is through text
    mining techniques on the guidelines, books, and medical reports related to different
    medical areas. Guidelines or books are more robust than individual experiences.
    Medical reports generally contain specific terms (usually adjectives) that describe
    the characteristics of tumors. These terms, containing important information to
    help doctors to make diagnosis, can potentially be beneficial for deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the identification of medical domain knowledge, how to select appropriate
    knowledge to help image analysis tasks is also challenging. It should be noted
    that a common practice of medical doctors may not be able to help deep learning
    models because *the domain knowledge might be learned by the deep learning model
    from training data.* We believe that the knowledge that is not easily learned
    by a deep learning model can greatly help the model to improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 The Challenges Related to the Representation of Medical Domain Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original domain knowledge of medical doctors is generally in the form of
    descriptive sentences like ‘we will focus more on the margin areas of a tumor
    to determine whether it is benign or malignant’, or ‘we often compare bilateral
    images to make decision’. How to transform the knowledge into appropriate representations
    and incorporate it into deep learning models need a careful design.
  prefs: []
  type: TYPE_NORMAL
- en: There are generally four ways to represent a certain type of medical domain
    knowledge. One is to represent knowledge as patches or highlighted images (as
    in [[94](#bib.bib94)]). This is generally used when doctors pay more attention
    to specific areas. The second approach is to represent knowledge as feature vectors
    [[85](#bib.bib85)]. This way is suitable when the selected domain knowledge can
    be described as certain features. The third approach is to represent domain knowledge
    as extra labels [[59](#bib.bib59), [52](#bib.bib52)], which is suitable for the
    knowledge in clinical reports or extra feature attributes of diseases. The last
    approach is to embed medical domain knowledge in network structure design, which
    is suitable to represent high-level domain knowledge like the training pattern
    and diagnostic pattern of medical doctors[[69](#bib.bib69), [134](#bib.bib134),
    [135](#bib.bib135)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 The Challenges Related to the Incorporating Methods of Medical Domain Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, there are four ways to incorporate medical domain knowledge. The
    first is to transform the knowledge into certain patches or highlighted images
    and put them as extra inputs [[54](#bib.bib54)]. The second approach is via concatenation
    [[213](#bib.bib213)]. The domain knowledge are generally transformed into feature
    vectors and concatenated with those extracted by deep learning models. The third
    way is the attention mechanism [[38](#bib.bib38)]. The approach is applicable
    when doctors focus on certain areas of medical images or focus on certain time
    slots on medical videos. The last one is to learn the domain knowledge by using
    some specific network structures like generative models [[32](#bib.bib32), [206](#bib.bib206)].
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the existing works only incorporate a single type of medical
    domain knowledge, or a few types of medical domain knowledge of the same modality
    (e.g., a number of hand-crafted features). In practice, experienced medical doctors
    usually combine different experience in different stages.
  prefs: []
  type: TYPE_NORMAL
- en: There are some researches that simultaneously introduce high-level domain knowledge
    (e.g., diagnostic pattern, training pattern) and the low-level one (e.g., hand-crafted
    features, anatomical priors). In particular, the high-level domain knowledge is
    incorporated as input images, and low-level one is learned by using specific network
    structures [[32](#bib.bib32)]. In addition, besides incorporating into network
    directly, the information from low-level domain knowledge can also be used to
    design the training orders when combined with the easy-to-hard training pattern
    [[29](#bib.bib29)]. We believe that simultaneously incorporating multiple kinds
    of medical domain knowledge can better help deep learning models in various medical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Future Research Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the above challenges, there are several directions that we feel need
    further investigation in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation is developed to transfer the information from a source domain
    to a target one. Via techniques like adversarial learning [[153](#bib.bib153)],
    domain adaptation is able to narrow the domain shift between the source domain
    and the target one in input space [[236](#bib.bib236)], feature space [[237](#bib.bib237),
    [238](#bib.bib238)] and output space [[239](#bib.bib239), [240](#bib.bib240)].
    It can be naturally adopted to transfer knowledge of one medical dataset to another
    [[181](#bib.bib181)], even when they have different imaging modes or belong to
    different diseases [[172](#bib.bib172), [177](#bib.bib177)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, unsupervised domain adaptation (UDA) is a promising avenue to enhance
    the performance of deep neural networks on the target domain, using labels only
    from the source domain. This is especially useful for medical field, as annotating
    the medical images is quite labor-intensive and the lack of annotations is quite
    common in medical datasets. Some examples have demonstrated the effectiveness
    of UDA in disease diagnosis and organ segmentation [[175](#bib.bib175), [93](#bib.bib93),
    [241](#bib.bib241), [242](#bib.bib242)], but further depth study needs to be implemented
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge graph
  prefs: []
  type: TYPE_NORMAL
- en: We believe that the knowledge graph [[243](#bib.bib243)], with the character
    of embedding different types of knowledge, is a generic and flexible approach
    to incorporate multi-modal medical domain knowledge. Although rarely used at present,
    it also shows advantage in medical image analysis tasks, especially in medical
    report generation [[232](#bib.bib232)]. We believe that more types of knowledge
    graph can be used to represent and learn domain knowledge in medical image analysis
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: According to different relationships in graphs, there are three possible types
    of knowledge graphs can be established. The first knowledge graph reflects the
    relationship among different kinds of medical domain knowledge with respect to
    a certain disease. This knowledge graph can help us identify a few key types of
    knowledge that may help to improve the performance deep learning models. The second
    type of knowledge graph may reflects the relationship among different diseases.
    This knowledge graph can help us to find out the potential domain knowledge from
    other related diseases. The third type one can describe the relationship among
    medical datasets. These datasets can belong to different diseases and in different
    imaging modes (e.g., CT, MRI, ultrasound). This type of knowledge graph will help
    to identify the external datasets that may help to improve the performance of
    the current deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: The generative models
  prefs: []
  type: TYPE_NORMAL
- en: The generative models, like GAN and AE, have shown great promise to be applied
    to incorporate medical domain knowledge into deep learning models, especially
    for segmentation tasks. GAN has shown its capability to leverage information from
    extra datasets with different imaging modes (e.g., using a MRI dataset to help
    segmenting CT images [[175](#bib.bib175), [177](#bib.bib177)]). In addition, GAN
    is able to learn important features contained in medical images in a weakly or
    fully unsupervised manner and therefore is quite suitable for medical image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: AE-based models have already achieved a great success in extracting features,
    especially the shape priors in objects like organs or lesions in a fully unsupervised
    manner [[32](#bib.bib32), [206](#bib.bib206)]. The features learning by AE can
    also be easily integrated into the training process of networks.
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture search (NAS)
  prefs: []
  type: TYPE_NORMAL
- en: At last, we mentioned in the previous section that one challenge is to find
    appropriate network architectures to incorporate medical domain knowledge. We
    believe one approach to address this problem is the technique of network architecture
    search (NAS). NAS has demonstrated its capability to automatically find a good
    network architecture in many computer vision tasks [[244](#bib.bib244)] and has
    a great promise in the medical domain [[245](#bib.bib245)]. For instance, when
    some hand-crafted features are used as the domain knowledge, with the help of
    NAS, a network structure can be identified with the special connections between
    domain knowledge features and deep features. In addition, instead of designing
    the feature fusion method (feature-level fusion, decision-level fusion or input-level
    fusion) for these two kinds of features, the integrating phase and integrating
    intensity of these two kinds of features can also be determined during the searching
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we give a comprehensive survey on incorporating medical domain
    knowledge into deep learning models for various medical image analysis tasks ranging
    from disease diagnosis, lesion, organ and abnormality detection to lesion and
    organ segmentation. In addition, some other tasks such as medical image reconstruction,
    medical image retrieval and medical report generation are also included. For each
    task, we first introduce different types of medical domain knowledge, and then
    review some works of introducing domain knowledge into target tasks by using different
    incorporating methods. From this survey, we can see that with appropriate integrating
    methods, different kinds of domain knowledge can help deep learning models to
    better accomplish corresponding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Besides reviewing current works on incorporating domain knowledge into deep
    learning models, we also summarize challenges of using medical domain knowledge,
    and introduce the identification, selection, representation and incorporating
    method of medical domain knowledge. Finally, we give some future directions of
    incorporating domain knowledge for medical image analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    [grant numbers 61976012, 61772060]; the National Key R&D Program of China [grant
    number 2017YFB1301100]; and the CERNET Innovation Project [grant number NGII20170315].
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Esteva, B. Kuprel, R. A. Novoa, J. M. Ko, S. M. Swetter, H. M. Blau,
    and S. Thrun, “Dermatologist-level classification of skin cancer with deep neural
    networks,” *Nature*, vol. 542, no. 7639, pp. 115–118, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Y. Shin, S. Lee, I. D. Yun, S. M. Kim, and K. M. Lee, “Joint weakly
    and semi-supervised deep learning for localization and classification of masses
    in breast ultrasound images,” *IEEE TMI*, vol. 38, no. 3, pp. 762–774, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z. Zhou, J. Y. Shin, L. Zhang, S. R. Gurudu, M. B. Gotway, and J. Liang,
    “Fine-tuning convolutional neural networks for biomedical image analysis: Actively
    and incrementally,” *CVPR2017*, pp. 4761–4772, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] W. Zhu, C. Liu, W. Fan, and X. Xie, “Deeplung: Deep 3d dual path nets for
    automated pulmonary nodule detection and classification,” *workshop on applications
    of computer vision*, pp. 673–681, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness of
    data,” *IEEE Intelligent Systems*, vol. 24, no. 2, pp. 8–12, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J. Cairns, R. C.
    Green, D. Harvey, C. R. Jack, W. Jagust, J. C. Morris, R. C. Petersen, J. Salazar,
    A. J. Saykin, L. M. Shaw, A. W. Toga, and J. Q. Trojanowski, “The alzheimer’s
    disease neuroimaging initiative 3: Continued innovation for clinical trial improvement,”
    *Alzheimer’s and Dementia*, vol. 13, no. 5, pp. 561 – 571, 2017\. [Online]. Available:
    http://www.sciencedirect.com/science/article/pii/S1552526016330722'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts,
    J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto *et al.*, “The autism
    brain imaging data exchange: towards a large-scale evaluation of the intrinsic
    brain architecture in autism,” *Molecular psychiatry*, vol. 19, no. 6, pp. 659–667,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. MICCAI, “Automated cardiac diagnosis challenge (acdc),” 2017\. [Online].
    Available: https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. Summers, “Chestx-ray14:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” 09 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *et al.*, “The
    lung image database consortium (lidc) and image database resource initiative (idri):
    a completed reference database of lung nodules on ct scans,” *Medical physics*,
    vol. 38, no. 2, pp. 915–931, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. T. B. v. G. Colin Jacobs, Arnaud Arindra Adiyoso Setio, “Lung nodule
    analysis 2016,” 2016\. [Online]. Available: https://luna16.grand-challenge.org/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang,
    K. Zhu, D. Laird, R. L. Ball *et al.*, “Mura: Large dataset for abnormality detection
    in musculoskeletal radiographs,” *arXiv preprint arXiv:1712.06957*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara,
    C. Berger, S. M. Ha, M. Rozycki *et al.*, “Identifying the best machine learning
    algorithms for brain tumor segmentation, progression assessment, and overall survival
    prediction in the brats challenge,” *arXiv preprint arXiv:1811.02629*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Hoover, V. Kouznetsova, and M. Goldbaum, “Locating blood vessels in
    retinal images by piecewise threshold probing of a matched filter response,” *IEEE
    Transactions on Medical imaging*, vol. 19, no. 3, pp. 203–210, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Heath, K. Bowyer, D. Kopans, R. Moore, and P. Kegelmeyer, “The digital
    database for screening mammography,” *Proceedings of the Fourth International
    Workshop on Digital Mammography*, 01 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. Yan, X. Wang, L. Lu, and R. M. Summers, “Deeplesion: automated mining
    of large-scale lesion annotations and universal lesion detection with deep learning,”
    *Journal of Medical Imaging*, vol. 5, no. 3, p. 036501, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. K. T. Alexander A., “Efficient and generalizable statistical models
    of shape and appearance for analysis of cardiac mri,” *Medical Image Analysis*,
    vol. 12, no. 3, pp. 335 – 357, 2008\. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1361841508000029'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
    B. Helba, A. Kalloo, K. Liopyris, M. Marchetti *et al.*, “Skin lesion analysis
    toward melanoma detection 2018: A challenge hosted by the international skin imaging
    collaboration (isic),” *arXiv preprint arXiv:1902.03368*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Chen, X. J. Qi, J. Z. Cheng, and P. A. Heng, “Deep contextual networks
    for neuronal structure segmentation,” in *Thirtieth AAAI conference on artificial
    intelligence*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] F. Ciompi, B. de Hoop, S. J. van Riel, K. Chung, E. T. Scholten, M. Oudkerk,
    P. A. de Jong, M. Prokop, and B. van Ginneken, “Automatic classification of pulmonary
    peri-fissural nodules in computed tomography using an ensemble of 2d views and
    a convolutional neural network out-of-the-box,” *Medical Image Analysis*, vol. 26,
    no. 1, pp. 195–202, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura,
    and R. M. Summers, “Deep convolutional neural networks for computer-aided detection:
    Cnn architectures, dataset characteristics and transfer learning,” *IEEE TMI*,
    vol. 35, no. 5, pp. 1285–1298, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] B. J. Erickson, P. Korfiatis, Z. Akkus, and T. L. Kline, “Machine learning
    for medical imaging,” *Radiographics*, vol. 37, no. 2, pp. 505–515, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV 2017*, 2017,
    pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] B. Huynh, K. Drukker, and M. Giger, “Mo-de-207b-06: Computer-aided diagnosis
    of breast ultrasound images using transfer learning from deep convolutional neural
    networks,” *Medical physics*, vol. 43, no. 6Part30, pp. 3705–3705, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. K. Samala, H.-P. Chan, L. Hadjiiski, M. A. Helvie, C. D. Richter, and
    K. H. Cha, “Breast cancer diagnosis in digital breast tomosynthesis: effects of
    training sample size on multi-stage transfer learning using deep neural nets,”
    *IEEE TMI*, vol. 38, no. 3, pp. 686–696, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] F. Yu, J. Zhao, Y. Gong, Z. Wang, Y. Li, F. Yang, B. Dong, Q. Li, and
    L. Zhang, “Annotation-free cardiac vessel segmentation via knowledge transfer
    from retinal images,” in *MICCAI2019*, 2019, pp. 714–722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] G. Maicas, A. P. Bradley, J. C. Nascimento, I. Reid, and G. Carneiro,
    “Training medical image analysis systems like radiologists,” in *MICCAI2018*,
    2018, pp. 546–554.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Tang, X. Wang, A. P. Harrison, L. Lu, J. Xiao, and R. M. Summers, “Attention-guided
    curriculum learning for weakly supervised classification and localization of thoracic
    diseases on chest radiographs,” in *International Workshop on Machine Learning
    in Medical Imaging*.   Springer, 2018, pp. 249–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Wang, Y. Lu, B. Wu, T. Chen, D. Z. Chen, and J. Wu, “Deep active self-paced
    learning for accurate pulmonary nodule segmentation,” in *MICCAI2018*.   Springer,
    2018, pp. 723–731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S.-M. Hsu, W.-H. Kuo, F.-C. Kuo, and Y.-Y. Liao, “Breast tumor classification
    using different features of quantitative ultrasound parametric images,” *International
    journal of computer assisted radiology and surgery*, vol. 14, no. 4, pp. 623–633,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C. Chen, C. Biffi, G. Tarroni, S. E. Petersen, W. Bai, and D. Rueckert,
    “Learning shape priors for robust cardiac mr segmentation from multi-view images.”
    in *Medical Image Computing and Computer-Assisted Intervention*, 2019, pp. 523–531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. J. Larrazabal, C. E. Martinez, and E. Ferrante, “Anatomical priors
    for image segmentation via post-processing with denoising autoencoders,” in *MICCAI2019*,
    2019, pp. 585–593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Valverde, M. Salem, M. Cabezas, D. Pareto, J. C. Vilanova, L. Ramiotorrenta,
    A. Rovira, J. Salvi, A. Oliver, and X. Llado, “One-shot domain adaptation in multiple
    sclerosis lesion segmentation using convolutional neural networks,” *NeuroImage:
    Clinical*, vol. 21, p. 101638, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. Ji, S. Basodi, X. Xiao, and Y. Pan, “Infant sound classification on
    multi-stage cnns with hybrid features and prior knowledge,” in *International
    Conference on AI and Mobile Services*.   Springer, 2020, pp. 3–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Xie, J. Zhang, S. Liu, W. Cai, and Y. Xia, “Lung nodule classification
    by jointly using visual descriptors and deep features,” in *Medical Computer Vision
    and Bayesian and Graphical Models for Biomedical Imaging*.   Springer, 2016, pp.
    116–125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang, “Diagnose
    like a radiologist: Attention guided convolutional neural network for thorax disease
    classification,” *arXiv preprint arXiv:1801.09927*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] L. Li, M. Xu, X. Wang, L. Jiang, and H. Liu, “Attention based glaucoma
    detection: A large-scale database and cnn model,” in *CVPR2019*, 2019, pp. 10 571–10 580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Chen, J. Qin, X. Ji, B. Lei, T. Wang, D. Ni, and J.-Z. Cheng, “Automatic
    scoring of multiple semantic attributes with multi-task feature leverage: A study
    on pulmonary nodules in ct images,” *IEEE TMI*, vol. 36, no. 3, pp. 802–814, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Q. Yue, X. Luo, Q. Ye, L. Xu, and X. Zhuang, “Cardiac segmentation from
    lge mri using deep neural network incorporating shape and spatial priors,” in
    *MICCAI2019*.   Springer, 2019, pp. 559–567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. G. Debelee, F. Schwenker, A. Ibenthal, and D. Yohannes, “Survey of
    deep learning in breast cancer image analysis,” *Evolving Systems*, pp. 1–21,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical Image Analysis*, vol. 42, pp. 60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,”
    *Annual review of biomedical engineering*, vol. 19, pp. 221–248, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] K. Suzuki, “Survey of deep learning applications to medical image analysis,”
    *Medical Imaging Technology*, vol. 35, no. 4, pp. 212–226, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *neural information processing systems*,
    vol. 141, no. 5, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” *CVPR2015*,
    pp. 1–9, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *CVPR2016*, pp. 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] G. Huang, Z. Liu, L. V. Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” *CVPR2017*, pp. 2261–2269, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Y. Kim, H. E. Lee, Y. H. Choi, S. J. Lee, and J. S. Jeon, “Cnn-based
    diagnosis models for canine ulcerative keratitis,” *Scientific reports*, vol. 9,
    no. 1, pp. 1–7, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Li, L. Shen, X. Xie, S. Huang, Z. Xie, X. Hong, and J. Yu, “Multi-resolution
    convolutional networks for chest x-ray radiograph based lung nodule detection,”
    *Artificial intelligence in medicine*, p. 101744, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Liu, W. Li, N. Zhao, K. Cao, Y. Yin, Q. Song, H. Chen, and X. Gong,
    “Integrate domain knowledge in training cnn for ultrasonography breast cancer
    diagnosis,” in *MICCAI2018*, 2018, pp. 868–875.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Mitsuhara, H. Fukui, Y. Sakashita, T. Ogata, T. Hirakawa, T. Yamashita,
    and H. Fujiyoshi, “Embedding human knowledge in deep neural network via attention
    map,” *arXiv preprint arXiv:1905.03540*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. J. Fulham, and W. Cai,
    “Knowledge-based collaborative deep learning for benign-malignant lung nodule
    classification on chest ct,” *IEEE TMI*, vol. 38, no. 4, pp. 991–1004, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Bar, I. Diamant, L. Wolf, and H. Greenspan, “Deep learning with non-medical
    training used for chest pathology identification,” in *Medical Imaging 2015: Computer-Aided
    Diagnosis*, vol. 9414.   International Society for Optics and Photonics, 2015,
    p. 94140V.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. Wimmer, S. Hegenbart, A. Vécsei, and A. Uhl, “Convolutional neural
    network architectures for the automated diagnosis of celiac disease,” in *International
    Workshop on Computer-Assisted and Robotic Endoscopy*.   Springer, 2016, pp. 104–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Cao, S. Bernard, L. Heutte, and R. Sabourin, “Improve the performance
    of transfer learning without fine-tuning using dissimilarity-based multi-view
    learning for breast cancer histology images,” in *International conference image
    analysis and recognition*, 2018, pp. 779–787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases,” in *CVPR2017*, 2017, pp. 2097–2106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S. Hussein, K. Cao, Q. Song, and U. Bagci, “Risk stratification of lung
    nodules using 3d cnn-based multi-task learning,” *international conference information
    processing*, pp. 249–260, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] W. Li, J. Li, K. V. Sarma, K. C. Ho, S. Shen, B. S. Knudsen, A. Gertych,
    and C. W. Arnold, “Path r-cnn for prostate cancer diagnosis and gleason grading
    of histological images,” *IEEE TMI*, vol. 38, no. 4, pp. 945–954, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B. Q. Huynh, H. Li, and M. L. Giger, “Digital mammographic tumor classification
    using transfer learning from deep convolutional neural networks,” *Journal of
    Medical Imaging*, vol. 3, no. 3, p. 034501, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] O. Hadad, R. Bakalo, R. Ben-Ari, S. Hashoul, and G. Amit, “Classification
    of breast lesions using cross-modal deep learning,” in *ISBI 2017*.   IEEE, 2017,
    pp. 109–112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. K. Samala, H.-P. Chan, L. M. Hadjiiski, M. A. Helvie, K. H. Cha, and
    C. D. Richter, “Multi-task transfer learning deep convolutional neural network:
    application to computer-aided diagnosis of breast cancer on mammograms,” *Physics
    in Medicine & Biology*, vol. 62, no. 23, p. 8894, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Azizi, P. Mousavi, P. Yan, A. Tahmasebi, J. T. Kwak, S. Xu, B. Turkbey,
    P. Choyke, P. Pinto, B. Wood *et al.*, “Transfer learning from rf to b-mode temporal
    enhanced ultrasound features for prostate cancer detection,” *International journal
    of computer assisted radiology and surgery*, vol. 12, no. 7, pp. 1111–1121, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Li, G. Qin, Q. He, L. Sun, H. Zeng, Z. He, W. Chen, X. Zhen, and L. Zhou,
    “Digital breast tomosynthesis versus digital mammography: integration of image
    modalities enhances deep learning-based breast mass classification,” *European
    radiology*, vol. 30, no. 2, pp. 778–788, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Han, J. Wang, W. Zhou, C. Chang, S. Ying, and J. Shi, “Deep doubly
    supervised transfer network for diagnosis of breast cancer with imbalanced ultrasound
    imaging modalities,” in *International Conference on Medical Image Computing and
    Computer-Assisted Intervention*.   Springer, 2020, pp. 141–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, and P.-A. Heng, “Canet: Cross-disease
    attention network for joint diabetic retinopathy and diabetic macular edema grading,”
    *IEEE TMI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Liao, Y. Ding, Z. L. Jiang, X. Wang, C. Zhang, and Q. Zhang, “Multi-task
    deep convolutional neural network for cancer diagnosis,” *Neurocomputing*, vol.
    348, pp. 66–73, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Medical-based deep curriculum learning
    for improved fracture classification,” in *MICCAI2019*.   Springer, 2019, pp.
    694–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Haarburger, M. Baumgartner, D. Truhn, M. Broeckmann, H. Schneider,
    S. Schrading, C. Kuhl, and D. Merhof, “Multi scale curriculum cnn for context-aware
    breast mri malignancy classification,” in *MICCAI2019*.   Springer, 2019, pp.
    495–503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] R. Zhao, X. Chen, Z. Chen, and S. Li, “Egdcl: An adaptive curriculum learning
    framework for unbiased glaucoma diagnosis,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 190–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Jiménez-Sánchez, D. Mateus, S. Kirchhoff, C. Kirchhoff, P. Biberthaler,
    N. Navab, M. A. G. Ballester, and G. Piella, “Curriculum learning for annotation-efficient
    medical image analysis: scheduling data with prior knowledge and uncertainty,”
    *arXiv preprint arXiv:2007.16102*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Wei, A. Suriawinata, B. Ren, X. Liu, M. Lisovsky, L. Vaickus, C. Brown,
    M. Baker, M. Nasir-Moin, N. Tomita *et al.*, “Learn like a pathologist: Curriculum
    learning by annotator agreement for histopathology image classification,” *arXiv
    preprint arXiv:2009.13698*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu, and Y. Yu,
    “Curriculum feature alignment domain adaptation for epithelium-stroma classification
    in histopathological images,” *IEEE Journal of Biomedical and Health Informatics*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] I. González-Díaz, “Dermaknet: Incorporating the knowledge of dermatologists
    to convolutional neural networks for skin lesion diagnosis,” *IEEE journal of
    biomedical and health informatics*, vol. 23, no. 2, pp. 547–559, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] K. Wang, X. Zhang, S. Huang, F. Chen, X. Zhang, and L. Huangfu, “Learning
    to recognize thoracic disease in chest x-rays with knowledge-guided deep zoom
    neural networks,” *IEEE Access*, vol. 8, pp. 159 790–159 805, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Huang, Y. Fang, M. Lu, F. Yan, J. Yang, and Y. Xu, “Dual-ray net: Automatic
    diagnosis of thoracic diseases using frontal and lateral chest x-rays,” *Journal
    of Medical Imaging and Health Informatics*, vol. 10, no. 2, pp. 348–355, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Yang, Z. Cao, Y. Zhang, M. Han, J. Xiao, L. Huang, S. Wu, J. Ma, and
    P. Chang, “Momminet: Mammographic multi-view mass identification networks,” in
    *International Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 200–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Q. Liu, L. Yu, L. Luo, Q. Dou, and P. A. Heng, “Semi-supervised medical
    image classification with relation-driven self-ensembling model,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Fang, C. Wang, S. Li, H. Rabbani, X. Chen, and Z. Liu, “Attention to
    lesion: Lesion-aware convolutional neural network for retinal optical coherence
    tomography image classification,” *IEEE TMI*, vol. 38, no. 8, pp. 1959–1970, Aug
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H. Cui, Y. Xu, W. Li, L. Wang, and H. Duh, “Collaborative learning of
    cross-channel clinical attention for radiotherapy-related esophageal fistula prediction
    from ct,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 212–220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] X. Xie, J. Niu, X. Liu, Q. Li, Y. Wang, J. Han, and S. Tang, “Dg-cnn:
    Introducing margin information into cnn for breast cancer diagnosis in ultrasound
    images,” *Journal of Computer Science and Engineering*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] B. Zhang, Z. Wang, J. Gao, C. Rutjes, K. Nufer, D. Tao, D. D. Feng, and
    S. W. Menzies, “Short-term lesion change detection for melanoma screening with
    novel siamese neural network.” *IEEE transactions on medical imaging*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M. Moradi, Y. Gur, H. Wang, P. Prasanna, and T. Syeda-Mahmood, “A hybrid
    learning approach for semantic labeling of cardiac ct slices and recognition of
    body position,” in *ISBI 2016*.   IEEE, 2016, pp. 1418–1421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Majtner, S. Yildirim-Yayilgan, and J. Y. Hardeberg, “Combining deep
    learning and hand-crafted features for skin lesion classification,” in *2016 Sixth
    International Conference on Image Processing Theory, Tools and Applications (IPTA)*.   IEEE,
    2016, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Xie, J. Zhang, Y. Xia, M. Fulham, and Y. Zhang, “Fusing texture, shape
    and deep model-learned information at decision level for automated classification
    of lung nodules on chest ct,” *Information Fusion*, vol. 42, pp. 102–110, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] N. Antropova, B. Q. Huynh, and M. L. Giger, “A deep feature fusion methodology
    for breast cancer diagnosis demonstrated on three imaging modality datasets,”
    *Medical physics*, vol. 44, no. 10, pp. 5162–5171, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] X. Xia, J. Gong, W. Hao, T. Yang, Y. Lin, S. Wang, and W. Peng, “Comparison
    and fusion of deep learning and radiomics features of ground-glass nodules to
    predict the invasiveness risk of stage-i lung adenocarcinomas in ct scan,” *Frontiers
    in Oncology*, vol. 10, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Hagerty, J. Stanley *et al.*, “Deep learning and handcrafted method
    fusion: Higher diagnostic accuracy for melanoma dermoscopy images,” *IEEE journal
    of biomedical and health informatics*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Y. Chai, H. Liu, and J. Xu, “Glaucoma diagnosis based on both hidden features
    and domain knowledge through deep learning models,” *Knowledge-Based Systems*,
    vol. 161, pp. 147–156, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Buty, Z. Xu, M. Gao, U. Bagci, A. Wu, and D. J. Mollura, “Characterization
    of lung nodule malignancy using hybrid shape and appearance features,” in *MICCAI2016*,
    2016, pp. 662–670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Saba, A. S. Mohamed, M. El-Affendi, J. Amin, and M. Sharif, “Brain
    tumor detection using fusion of hand crafted and deep learning features,” *Cognitive
    Systems Research*, vol. 59, pp. 221–230, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] W. Yang, J. Zhao, Y. Qiang, X. Yang, Y. Dong, Q. Du, G. Shi, and M. B.
    Zia, “Dscgans: Integrate domain knowledge in training dual-path semi-supervised
    conditional generative adversarial networks and s3vm for ultrasonography thyroid
    nodules classification,” in *MICCAI2019*, 2019, pp. 558–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Tan, Y. Huo, Z. Liang, and L. Li, “Expert knowledge-infused deep learning
    for automatic lung nodule detection,” *Journal of X-ray science and technology*,
    no. Preprint, pp. 1–20, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Liu, Q. Guo, C. Lian, X. Ren, S. Liang, J. Yu, L. Niu, W. Sun, and
    D. Shen, “Automated detection and classification of thyroid nodules in ultrasound
    images using clinical-knowledge-guided convolutional neural networks,” *Medical
    image analysis*, vol. 58, p. 101555, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. Feng, J. Cao, H. Wang, Y. Xie, D. Yang, J. Feng, and B. Chen, “A knowledge-driven
    feature learning and integration method for breast cancer diagnosis on multi-sequence
    mri,” *Magnetic Resonance Imaging*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] V. Murthy, L. Hou, D. Samaras, T. M. Kurc, and J. H. Saltz, “Center-focusing
    multi-task cnn with injected features for classification of glioma nuclear images,”
    in *2017 IEEE Winter Conference on Applications of Computer Vision (WACV)*.   IEEE,
    2017, pp. 834–841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Wang, Y. Peng, L. Lu, Z. Lu, and R. M. Summers, “Tienet: Text-image
    embedding network for common thorax disease classification and reporting in chest
    x-rays,” in *CVPR2018*, 2018, pp. 9049–9058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Zhang, P. Chen, M. Sapkota, and L. Yang, “Tandemnet: Distilling knowledge
    from medical images using diagnostic reports as optional semantic references,”
    in *MICCAI2017*.   Springer, 2017, pp. 320–328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Wu, J. Phang, J. Park, Y. Shen, Z. Huang, M. Zorin, S. Jastrzebski,
    T. Févry, J. Katsnelson, E. Kim *et al.*, “Deep neural networks improve radiologists
    performance in breast cancer screening,” *IEEE transactions on medical imaging*,
    vol. 39, no. 4, pp. 1184–1194, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. Yu, H.-Y. Zhou, K. Ma, C. Bian, C. Chu, H. Liu, and Y. Zheng, “Difficulty-aware
    glaucoma classification with multi-rater consensus modeling,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 741–750.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *Advances in neural information processing
    systems*, 2014, pp. 3320–3328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] X. Xiao, C. Ji, T. B. Mudiyanselage, and Y. Pan, “Pk-gcn: Prior knowledge
    assisted image classification using graph convolution networks,” *arXiv preprint
    arXiv:2009.11892*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proceedings of the 26th annual international conference on machine learning*.   ACM,
    2009, pp. 41–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] F. Nachbar, W. Stolz, T. Merkle, A. B. Cognetta, T. Vogt, M. Landthaler,
    P. Bilek, O. Braun-Falco, and G. Plewig, “The abcd rule of dermatoscopy: high
    prospective value in the diagnosis of doubtful melanocytic skin lesions,” *Journal
    of the American Academy of Dermatology*, vol. 30, no. 4, pp. 551–559, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B. W. e. a. Mendelson EB, Bohm-V lez M, “Acr bi-rads ultrasound. in:
    Acr bi-rads atlas, breast imaging reporting and data system.” *Reston, VA, American
    College of Radiology*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] L. Breiman, “Random forests,” *Machine learning*, vol. 45, no. 1, pp.
    5–32, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Cortes and V. Vapnik, “Support-vector networks,” *Machine learning*,
    vol. 20, no. 3, pp. 273–297, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Alilou, M. Orooji, and A. Madabhushi, “Intra-perinodular textural
    transition (ipris): A 3d descriptor for nodule diagnosis on lung ct,” in *MICCAI2017*.   Springer,
    2017, pp. 647–655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. A. A. Setio, F. Ciompi, G. Litjens, P. Gerke, C. Jacobs, S. J. Van Riel,
    M. M. W. Wille, M. Naqibullah, C. I. Sánchez, and B. van Ginneken, “Pulmonary
    nodule detection in ct images: false positive reduction using multi-view convolutional
    networks,” *IEEE TMI*, vol. 35, no. 5, pp. 1160–1169, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros *et al.*, “Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs,” *JAMA*, vol. 316, no. 22, pp. 2402–2410, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Liu, D. Wang, L. Lu, Z. Wei, L. Kim, E. B. Turkbey, B. Sahiner, N. A.
    Petrick, and R. M. Summers, “Detection and diagnosis of colitis on computed tomography
    using deep convolutional neural networks,” *Medical physics*, vol. 44, no. 9,
    pp. 4630–4642, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time
    object detection with region proposal networks,” vol. 2015, pp. 91–99, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, “Mask r-cnn,” *international
    conference on computer vision*, pp. 2980–2988, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] R. Sa, W. Owens, R. Wiegand, M. Studin, D. Capoferri, K. Barooha, A. Greaux,
    R. Rattray, A. Hutton, J. Cintineo *et al.*, “Intervertebral disc detection in
    x-ray images using faster r-cnn,” in *2017 39th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE, 2017,
    pp. 564–567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Ben-Ari, A. Akselrod-Ballin, L. Karlinsky, and S. Hashoul, “Domain
    specific convolutional neural nets for detection of architectural distortion in
    mammograms,” in *ISBI 2017*.   IEEE, 2017, pp. 552–556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *CVPR2016*, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. Platania, S. Shams, S. Yang, J. Zhang, K. Lee, and S.-J. Park, “Automated
    breast cancer diagnosis using deep learning and region of interest detection (bc-droid),”
    in *Proceedings of the 8th ACM International Conference on Bioinformatics, Computational
    Biology, and Health Informatics*.   ACM, 2017, pp. 536–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] N. Li, H. Liu, B. Qiu, W. Guo, S. Zhao, K. Li, and J. He, “Detection
    and attention: Diagnosing pulmonary lung cancer from ct by imitating physicians,”
    *arXiv preprint arXiv:1712.05114*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] G. Cai, J. Chen, Z. Wu, H. Tang, Y. Liu, S. Wang, and S. Su, “One stage
    lesion detection based on 3d context convolutional neural networks,” *Computers
    and Electrical Engineering*, vol. 79, p. 106449, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. H. Yap, G. Pons, J. Marti, S. Ganau, M. Sentis, R. Zwiggelaar, A. K.
    Davison, and R. Marti, “Automated breast ultrasound lesions detection using convolutional
    neural networks,” *IEEE Journal of Biomedical and Health Informatics*, vol. 22,
    no. 4, pp. 1218–1226, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. J. Näppi, T. Hironaka, D. Regge, and H. Yoshida, “Deep transfer learning
    of virtual endoluminal views for the detection of polyps in ct colonography,”
    in *Medical Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International
    Society for Optics and Photonics, 2016, p. 97852B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau, and C. C.
    Poon, “Automatic detection and classification of colorectal polyps by transferring
    low-level cnn features from nonmedical domain,” *IEEE journal of biomedical and
    health informatics*, vol. 21, no. 1, pp. 41–47, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.
    Gotway, and J. Liang, “Convolutional neural networks for medical image analysis:
    Full training or fine tuning?” *IEEE TMI*, vol. 35, no. 5, pp. 1299–1312, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Zhang, E. H. Cain, A. Saha, Z. Zhu, and M. A. Mazurowski, “Breast
    mass detection in mammography and tomosynthesis via fully convolutional network-based
    heatmap regression,” in *Medical Imaging 2018: Computer-Aided Diagnosis*, vol.
    10575.   International Society for Optics and Photonics, 2018, p. 1057525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Ben-Cohen, E. Klang, S. P. Raskin, S. Soffer, S. Ben-Haim, E. Konen,
    M. M. Amitai, and H. Greenspan, “Cross-modality synthesis from ct to pet using
    fcn and gan networks for improved automated lesion detection,” *Engineering Applications
    of Artificial Intelligence*, vol. 78, pp. 186–194, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Zhao, D. Li, Z. Kassam, J. Howey, J. Chong, B. Chen, and S. Li, “Tripartite-gan:
    Synthesizing liver contrast-enhanced mri to improve tumor detection,” *Medical
    Image Analysis*, p. 101667, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Jesson, N. Guizard, S. H. Ghalehjegh, D. Goblot, F. Soudan, and N. Chapados,
    “Cased: curriculum adaptive sampling for extreme data imbalance,” in *MICCAI2017*,
    2017, pp. 639–646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] P. Astudillo, P. Mortier, M. De Beule *et al.*, “Curriculum deep reinforcement
    learning with different exploration strategies: A feasibility study on cardiac
    landmark detection,” in *BIOIMAGING 2020: 7th International Conference on Bioimaging*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Z. Li, S. Zhang, J. Zhang, K. Huang, Y. Wang, and Y. Yu, “Mvp-net: Multi-view
    fpn with position-aware attention for deep universal lesion detection,” in *Medical
    Image Computing and Computer-Assisted Intervention*, 2019, pp. 13–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Q. Ni, Z. Y. Sun, L. Qi, W. Chen, Y. Yang, L. Wang, X. Zhang, L. Yang,
    Y. Fang, Z. Xing *et al.*, “A deep learning approach to characterize 2019 coronavirus
    disease (covid-19) pneumonia in chest ct images,” *European radiology*, vol. 30,
    no. 12, pp. 6517–6527, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Liu, Z. Zhou, S. Zhang, L. Luo, Q. Zhang, F. Zhang, X. Li, Y. Wang,
    and Y. Yu, “From unilateral to bilateral learning: Detecting mammogram masses
    with contrasted bilateral network,” in *Medical Image Computing and Computer-Assisted
    Intervention*, 2019, pp. 477–485.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Liu, F. Zhang, Q. Zhang, S. Wang, and Y. Yu, “Cross-view correspondence
    reasoning based on bipartite graph convolutional network for mammogram mass detection,”
    in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Lisowska, E. Beveridge, K. Muir, and I. Poole, “Thrombus detection
    in ct brain scans using a convolutional neural network.” in *BIOIMAGING*, 2017,
    pp. 24–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Lisowska, A. O Neil, V. Dilys, M. Daykin, E. Beveridge, K. Muir, S. Mclaughlin,
    and I. Poole, “Context-aware convolutional neural networks for stroke sign detection
    in non-contrast ct scans,” in *Annual Conference on Medical Image Understanding
    and Analysis*.   Springer, 2017, pp. 494–505.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] L. Li, M. Wei, B. Liu, K. Atchaneeyasakul, F. Zhou, Z. Pan, S. Kumar,
    J. Zhang, Y. Pu, D. S. Liebeskind *et al.*, “Deep learning for hemorrhagic lesion
    detection and segmentation on brain ct images,” *IEEE Journal of Biomedical and
    Health Informatics*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] L. Fu, J. Ma, Y. Ren, Y. S. Han, and J. Zhao, “Automatic detection of
    lung nodules: false positive reduction using convolution neural networks and handcrafted
    features,” in *Medical Imaging 2017: Computer-Aided Diagnosis*, vol. 10134.   International
    Society for Optics and Photonics, 2017, p. 101340A.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. Kooi, G. Litjens, B. Van Ginneken, A. Gubern-Mérida, C. I. Sánchez,
    R. Mann, A. den Heeten, and N. Karssemeijer, “Large scale deep learning for computer
    aided detection of mammographic lesions,” *Medical Image Analysis*, vol. 35, pp.
    303–312, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] N. Ghatwary, X. Ye, and M. Zolgharni, “Esophageal abnormality detection
    using densenet based faster r-cnn with gabor features,” *IEEE Access*, vol. 7,
    pp. 84 374–84 385, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C.-H. Chao, Z. Zhu, D. Guo, K. Yan, T.-Y. Ho, J. Cai, A. P. Harrison,
    X. Ye, J. Xiao, A. Yuille *et al.*, “Lymph node gross tumor volume detection in
    oncology imaging via relationship learning using graph neural network,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 772–782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Sóñora-Mengan Sr, P. Gonidakis, B. Jansen, J. Garcı?a-Naranjo, and
    J. Vandemeulebroucke, “Evaluating several ways to combine handcrafted features-based
    system with a deep learning system using the luna16 challenge framework,” in *Medical
    Imaging 2020: Computer-Aided Diagnosis*, vol. 11314.   International Society for
    Optics and Photonics, 2020, p. 113143T.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Hwang and H.-E. Kim, “Self-transfer learning for weakly supervised
    lesion localization,” in *MICCAI2016*.   Springer, 2016, pp. 239–246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] R. Bakalo, R. Ben-Ari, and J. Goldberger, “Classification and detection
    in mammograms with weak supervision via dual branch deep neural net,” in *2019
    IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)*.   IEEE,
    2019, pp. 1905–1909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] G. Liang, X. Wang, Y. Zhang, and N. Jacobs, “Weakly-supervised self-training
    for breast cancer localization,” in *2020 42nd Annual International Conference
    of the IEEE Engineering in Medicine & Biology Society (EMBC)*.   IEEE, 2020, pp.
    1124–1127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, and H. Larochelle, “Brain tumor segmentation with deep neural
    networks,” *Medical Image Analysis*, vol. 35, pp. 18–31, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Zhang, A. Saha, Z. Zhu, and M. A. Mazurowski, “Hierarchical convolutional
    neural networks for segmentation of breast tumors in mri with application to radiogenomics,”
    *IEEE TMI*, vol. 38, no. 2, pp. 435–447, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] P. F. Christ, F. Ettlinger, F. Grün *et al.*, “Automatic liver and tumor
    segmentation of ct and mri volumes using cascaded fully convolutional neural networks,”
    *arXiv preprint arXiv:1702.05970*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] H. R. Roth, A. Farag, L. Lu, E. B. Turkbey, and R. M. Summers, “Deep
    convolutional networks for pancreas segmentation in ct imaging,” in *Medical Imaging
    2015: Image Processing*, vol. 9413.   International Society for Optics and Photonics,
    2015, p. 94131G.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” *CVPR2015*, pp. 3431–3440, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI2015*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. V. Tran, “A fully convolutional neural network for cardiac segmentation
    in short-axis mri,” *arXiv preprint arXiv:1604.00494*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Chen, Y. Zheng, J.-H. Park, P.-A. Heng, and S. K. Zhou, “Iterative
    multi-domain regularized deep learning for anatomical structure detection and
    segmentation from ultrasound images,” in *MICCAI2016*.   Springer, 2016, pp. 487–495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R.
    Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, “Towards image-guided
    pancreas and biliary endoscopy: automatic multi-organ segmentation on abdominal
    ct with dense dilated networks,” in *MICCAI2017*, 2017, pp. 728–736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] P. F. Christ, M. E. A. Elshaer, *et al.*, “Automatic liver and lesion
    segmentation in ct using cascaded fully convolutional neural networks and 3d conditional
    random fields,” in *MICCAI2016*.   Springer, 2016, pp. 415–423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane, D. K.
    Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3d cnn with fully connected
    crf for accurate brain lesion segmentation,” *Medical Image Analysis*, vol. 36,
    pp. 61–78, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] X. Yang, L. Yu, L. Wu, Y. Wang, D. Ni, J. Qin, and P.-A. Heng, “Fine-grained
    recurrent neural networks for automatic prostate segmentation in ultrasound images,”
    in *Thirty-First AAAI Conference on Artificial Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested
    u-net architecture for medical image segmentation,” in *Deep Learning in Medical
    Image Analysis and Multimodal Learning for Clinical Decision Support*.   Springer,
    2018, pp. 3–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, “Recurrent
    residual convolutional neural network based on u-net (r2u-net) for medical image
    segmentation,” *arXiv preprint arXiv:1802.06955*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Gordienko, P. Gang, J. Hui, W. Zeng, Y. Kochura, O. Alienin, O. Rokovyi,
    and S. Stirenko, “Deep learning with lung segmentation and bone shadow exclusion
    techniques for chest x-ray analysis of lung cancer,” in *International Conference
    on Computer Science, Engineering and Education Applications*.   Springer, 2018,
    pp. 638–647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] D. Yang, D. Xu, S. K. Zhou, B. Georgescu, M. Chen, S. Grbic, D. Metaxas,
    and D. Comaniciu, “Automatic liver segmentation using an adversarial image-to-image
    network,” in *MICCAI2017*.   Springer, 2017, pp. 507–515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Zhao, L. Wang, J. Chen, D. Nie, Y. Cong, S. Ahmad, A. Ho, P. Yuan,
    S. H. Fung, H. H. Deng *et al.*, “Craniomaxillofacial bony structures segmentation
    from mri with deep-supervision adversarial learning,” in *MICCAI2018*, 2018, pp.
    720–727.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] K. Kamnitsas, C. Baumgartner *et al.*, “Unsupervised domain adaptation
    in brain lesion segmentation with adversarial networks,” in *International conference
    on information processing in medical imaging*.   Springer, 2017, pp. 597–609.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] S. Izadi, Z. Mirikharaji, J. Kawahara, and G. Hamarneh, “Generative adversarial
    networks to segment skin lesions,” in *ISBI 2018*.   IEEE, 2018, pp. 881–884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Lahiri, K. Ayush, P. Kumar Biswas, and P. Mitra, “Generative adversarial
    learning for reducing manual annotation in semantic segmentation on large scale
    miscroscopy images: Automated vessel segmentation in retinal fundus image as test
    case,” in *CVPR Workshops*, 2017, pp. 42–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs,
    “Unsupervised anomaly detection with generative adversarial networks to guide
    marker discovery,” in *International Conference on Information Processing in Medical
    Imaging*.   Springer, 2017, pp. 146–157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, “Dcan: Deep contour-aware
    networks for object instance segmentation from histology images,” *Medical Image
    Analysis*, vol. 36, pp. 135–146, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] L. Wu, Y. Xin, S. Li, T. Wang, P.-A. Heng, and D. Ni, “Cascaded fully
    convolutional networks for automatic prenatal ultrasound image segmentation,”
    in *ISBI 2017*.   IEEE, 2017, pp. 663–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Zeng, X. Yang, J. Li, L. Yu, P.-A. Heng, and G. Zheng, “3d u-net with
    multi-level deep supervision: fully automatic segmentation of proximal femur in
    3d mr images,” in *International workshop on machine learning in medical imaging*.   Springer,
    2017, pp. 274–282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Ghafoorian, A. Mehrtash, T. Kapur, N. Karssemeijer, E. Marchiori,
    M. Pesteie, C. R. Guttmann, F.-E. de Leeuw, C. M. Tempany, B. van Ginneken *et al.*,
    “Transfer learning for domain adaptation in mri: Application in brain lesion segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 516–524.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” in *MICCAI2016*.   Springer, 2016, pp. 478–486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] V. V. Valindria, N. Pawlowski *et al.*, “Multi-modal learning from unpaired
    images: Application to multi-organ segmentation in ct and mri,” in *2018 IEEE
    Winter Conference on Applications of Computer Vision (WACV)*.   IEEE, 2018, pp.
    547–556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] C. Chen, Q. Dou, H. Chen, and P.-A. Heng, “Semantic-aware generative
    adversarial nets for unsupervised domain adaptation in chest x-ray segmentation,”
    in *International Workshop on Machine Learning in Medical Imaging*.   Springer,
    2018, pp. 143–151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] C. Chen, Q. Dou, H. Chen, J. Qin, and P. A. Heng, “Unsupervised bidirectional
    cross-modality adaptation via deeply synergistic image and feature alignment for
    medical image segmentation,” *IEEE TMI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S. Mageras, J. O.
    Deasy, and H. Veeraraghavan, “Tumor-aware, adversarial domain adaptation from
    ct to mri for lung cancer segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    777–785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] W. Yan, Y. Wang, S. Gu, L. Huang, F. Yan, L. Xia, and Q. Tao, “The domain
    shift problem of medical image segmentation and vendor-adaptation by unet-gan,”
    in *MICCAI2019*.   Springer, 2019, pp. 623–631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, and J. S. Duncan,
    “Unsupervised domain adaptation via disentangled representations: Application
    to cross-modality liver segmentation,” in *MICCAI2019*.   Springer, 2019, pp.
    255–263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] K. Li, L. Yu, S. Wang, and P.-A. Heng, “Towards cross-modality medical
    image segmentation with online mutual knowledge distillation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 01, 2020, pp.
    775–783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] K. Li, S. Wang, L. Yu, and P.-A. Heng, “Dual-teacher: Integrating intra-domain
    and inter-domain teachers for annotation-efficient cardiac segmentation,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 418–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] M. Hu, M. Maillard, Y. Zhang, T. Ciceri, G. La Barbera, I. Bloch, and
    P. Gori, “Knowledge distillation from multi-modal to mono-modal segmentation networks,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 772–781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting multimodal
    medical volumes with cycle-and shape-consistency generative adversarial network,”
    in *CVPR2018*, 2018, pp. 9242–9251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] A. Chartsias, G. Papanastasiou, C. Wang, S. Semple, D. Newby, R. Dharmakumar,
    and S. Tsaftaris, “Disentangle, align and fuse for multimodal and semi-supervised
    image segmentation.” *IEEE transactions on medical imaging*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis.” *arXiv preprint arXiv:1904.00625*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] L. Berger, H. Eoin, M. J. Cardoso, and S. Ourselin, “An adaptive sampling
    scheme to efficiently train fully convolutional networks for semantic segmentation,”
    in *Annual Conference on Medical Image Understanding and Analysis*.   Springer,
    2018, pp. 277–286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Li, X. Liu, S. Boumaraf, W. Liu, X. Gong, and X. Ma, “A new three-stage
    curriculum learning approach for deep network based liver tumor segmentation,”
    in *2020 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Kervadec, J. Dolz, É. Granger, and I. B. Ayed, “Curriculum semi-supervised
    segmentation,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2019, pp. 568–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Z. Zhao, X. Zhang, C. Chen, W. Li, S. Peng, J. Wang, X. Yang, L. Zhang,
    and Z. Zeng, “Semi-supervised self-taught deep learning for finger bones segmentation,”
    in *2019 IEEE EMBS International Conference on Biomedical & Health Informatics
    (BHI)*.   IEEE, 2019, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] J. Zhang, G. Wang, H. Xie, S. Zhang, N. Huang, S. Zhang, and L. Gu, “Weakly
    supervised vessel segmentation in x-ray angiograms by self-paced learning from
    noisy labels with suggestive annotation,” *Neurocomputing*, vol. 417, pp. 114–127,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] B. Wu, Z. Zhou, J. Wang, and Y. Wang, “Joint learning for pulmonary nodule
    segmentation, attributes and malignancy prediction,” in *ISBI 2018*.   IEEE, 2018,
    pp. 1109–1113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] A. Hatamizadeh, D. Terzopoulos, and A. Myronenko, “End-to-end boundary
    aware networks for medical image segmentation,” in *International Workshop on
    Machine Learning in Medical Imaging*.   Springer, 2019, pp. 187–194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Z. Zhu, D. Jin, K. Yan, T.-Y. Ho, X. Ye, D. Guo, C.-H. Chao, J. Xiao,
    A. Yuille, and L. Lu, “Lymph node gross tumor volume detection and segmentation
    via distance-based gating using 3d ct/pet imaging in radiotherapy,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 753–762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] D. Jin, D. Guo, T.-Y. Ho, A. P. Harrison, J. Xiao, C.-k. Tseng, and L. Lu,
    “Deeptarget: Gross tumor and clinical target volume segmentation in esophageal
    cancer radiotherapy,” *Medical Image Analysis*, p. 101909, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] K. Huang, H.-D. Cheng, Y. Zhang, B. Zhang, P. Xing, and C. Ning, “Medical
    knowledge constrained semantic breast ultrasound image segmentation,” in *2018
    24th International Conference on Pattern Recognition (ICPR)*.   IEEE, 2018, pp.
    1193–1198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] N. Painchaud, Y. Skandarani, T. Judge, O. Bernard, A. Lalande, and P.-M.
    Jodoin, “Cardiac mri segmentation with strong anatomical guarantees,” in *MICCAI2019*.   Springer,
    2019, pp. 632–640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] ——, “Cardiac segmentation with strong anatomical guarantees,” *IEEE Transactions
    on Medical Imaging*, vol. 39, no. 11, pp. 3703–3713, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] O. Oktay, E. Ferrante *et al.*, “Anatomically constrained neural networks
    (acnns): application to cardiac image enhancement and segmentation,” *IEEE TMI*,
    vol. 37, no. 2, pp. 384–395, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] H. Ravishankar, R. Venkataramani, S. Thiruvenkadam, P. Sudhakar, and
    V. Vaidya, “Learning and incorporating shape models for semantic segmentation,”
    in *MICCAI2017*.   Springer, 2017, pp. 203–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. BenTaieb and G. Hamarneh, “Topology aware fully convolutional networks
    for histology gland segmentation,” in *MICCAI2018*.   Springer, 2016, pp. 460–468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Z. Mirikharaji and G. Hamarneh, “Star shape prior in fully convolutional
    networks for skin lesion segmentation,” in *MICCAI2018*.   Springer, 2018, pp.
    737–745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] H. Zheng, L. Lin, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y. Chen,
    R. Tong, and J. Wu, “Semi-supervised segmentation of liver using adversarial learning
    with deep atlas prior,” in *Medical Image Computing and Computer-Assisted Intervention*,
    2019, pp. 148–156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] C. Zotti, Z. Luo, A. Lalande, and P. Jodoin, “Convolutional neural network
    with shape prior applied to cardiac mri segmentation,” *IEEE Journal of Biomedical
    and Health Informatics*, vol. 23, no. 3, pp. 1119–1128, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in convolutional
    networks for unsupervised biomedical segmentation,” in *CVPR2018*, 2018, pp. 9290–9299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Y. He, G. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J. Dillenseger,
    P. Shao, S. Zhang *et al.*, “Dpa-densebiasnet: Semi-supervised 3d fine renal artery
    segmentation with dense biased network and deep priori anatomy.” in *Medical Image
    Computing and Computer-Assisted Intervention*, 2019, pp. 139–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] B. Luo, J. Shen, S. Cheng, Y. Wang, and M. Pantic, “Shape constrained
    network for eye segmentation in the wild,” in *The IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 1952–1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Y. Song, L. Zhu, B. Lei, B. Sheng, Q. Dou, J. Qin, and K.-S. Choi, “Shape
    mask generator: Learning to refine shape priors for segmenting overlapping cervical
    cytoplasms,” in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*.   Springer, 2020, pp. 639–649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] A. Boutillon, B. Borotikar, V. Burdin, and P.-H. Conze, “Combining shape
    priors with conditional adversarial networks for improved scapula segmentation
    in mr images,” in *2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI)*.   IEEE, 2020, pp. 1164–1167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] D. Pham, G. Dovletov, and J. Pauli, “Liver segmentation in ct with mri
    data: Zero-shot domain adaptation by contour extraction and shape priors,” in
    *2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)*.   IEEE,
    2020, pp. 1538–1542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] M. Engin, R. Lange, A. Nemes, S. Monajemi, M. Mohammadzadeh, C. K. Goh,
    T. M. Tu, B. Y. Tan, P. Paliwal, L. L. Yeo *et al.*, “Agan: An anatomy corrector
    conditional generative adversarial network,” in *International Conference on Medical
    Image Computing and Computer-Assisted Intervention*.   Springer, 2020, pp. 708–717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N.
    Metaxas, H. Li, and M. Chen, “Focusnetv2: Imbalanced large and small organ segmentation
    with adversarial shape constraint for head and neck ct images,” *Medical Image
    Analysis*, vol. 67, p. 101831, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] K. Kushibar, S. Valverde, S. González-Villà, J. Bernal, M. Cabezas, A. Oliver,
    and X. Lladó, “Automated sub-cortical brain structure segmentation combining spatial
    and deep convolutional features,” *Medical Image Analysis*, vol. 48, pp. 177–186,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Rezaei, A. Emami *et al.*, “Gland segmentation in histopathology images
    using deep networks and handcrafted features,” in *2019 41st Annual International
    Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)*.   IEEE,
    2019, pp. 1031–1034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] H. Khan, P. M. Shah, M. A. Shah, S. ul Islam, and J. J. Rodrigues, “Cascading
    handcrafted features and convolutional neural network for iot-enabled brain tumor
    segmentation,” *Computer Communications*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] H. Narotamo, J. M. Sanches, and M. Silveira, “Combining deep learning
    with handcrafted features for cell nuclei segmentation,” in *2020 42nd Annual
    International Conference of the IEEE Engineering in Medicine & Biology Society
    (EMBC)*.   IEEE, 2020, pp. 1428–1431.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 4489–4497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for latent
    variable models,” in *Advances in Neural Information Processing Systems*, 2010,
    pp. 1189–1197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] G. N. Sharma, R. Dave, J. Sanadya, P. Sharma, and K. Sharma, “Various
    types and management of breast cancer: an overview,” *Journal of advanced pharmaceutical
    technology &amp; research*, vol. 1, no. 2, p. 109, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] C. Qin, J. Schlemper, J. Caballero, A. N. Price, J. V. Hajnal, and D. Rueckert,
    “Convolutional recurrent neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 38, no. 1, pp. 280–290, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price, and D. Rueckert,
    “A deep cascade of convolutional neural networks for dynamic mr image reconstruction,”
    *IEEE TMI*, vol. 37, no. 2, pp. 491–503, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,
    S. Arridge, J. Keegan, Y. Guo *et al.*, “Dagan: deep de-aliasing generative adversarial
    networks for fast compressed sensing mri reconstruction,” *IEEE TMI*, vol. 37,
    no. 6, pp. 1310–1321, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] H. B. Yedder, M. Shokoufi, B. Cardoen, F. Golnaraghi, and G. Hamarneh,
    “Limited-angle diffuse optical tomography image reconstruction using deep learning,”
    in *MICCAI2019*.   Springer, 2019, pp. 66–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] S. U. H. Dar, M. Yurt, M. Shahdloo, M. E. Ildız, and T. Çukur, “Synergistic
    reconstruction and synthesis via generative adversarial networks for accelerated
    multi-contrast mri,” *arXiv preprint arXiv:1805.10704*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. Ahmad, M. Sajjad, I. Mehmood, and S. W. Baik, “Sinc: Saliency-injected
    neural codes for representation and efficient retrieval of medical radiographs,”
    *PloS one*, vol. 12, no. 8, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] A. Khatami, M. Babaie, H. R. Tizhoosh, A. Khosravi, T. Nguyen, and S. Nahavandi,
    “A sequential search-space shrinking using cnn transfer learning and a radon projection
    pool for medical image retrieval,” *Expert Systems with Applications*, vol. 100,
    pp. 224–233, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Z. N. K. Swati, Q. Zhao, M. Kabir, F. Ali, Z. Ali, S. Ahmed, and J. Lu,
    “Content-based brain tumor retrieval for mr images using transfer learning,” *IEEE
    Access*, vol. 7, pp. 17 809–17 822, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Y. Anavi, I. Kogan, E. Gelbart, O. Geva, and H. Greenspan, “A comparative
    study for chest radiograph image retrieval using binary texture and deep learning
    classification,” in *EMBC 2015*.   IEEE, 2015, pp. 2940–2943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Y. Anavi, I. Kogan *et al.*, “Visualizing and enhancing a deep learning
    framework using patients age and gender for chest x-ray image retrieval,” in *Medical
    Imaging 2016: Computer-Aided Diagnosis*, vol. 9785.   International Society for
    Optics and Photonics, 2016, p. 978510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] B. Jing, P. Xie, and E. Xing, “On the automatic generation of medical
    imaging reports,” *arXiv preprint arXiv:1711.08195*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] G. Liu, T. H. Hsu, M. B. A. Mcdermott, W. Boag, W. Weng, P. Szolovits,
    and M. Ghassemi, “Clinically accurate chest x-ray report generation.” *arXiv preprint
    arXiv: 1904.02633*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Hybrid retrieval-generation
    reinforced agent for medical image report generation,” in *Advances in neural
    information processing systems*, 2018, pp. 1530–1540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, “Knowledge-driven encode,
    retrieve, paraphrase for medical image report generation,” in *Proceedings of
    the AAAI Conference on Artificial Intelligence*, vol. 33, 2019, pp. 6666–6673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] W. Gale, L. Oakden-Rayner, G. Carneiro, A. P. Bradley, and L. J. Palmer,
    “Producing radiologist-quality reports for interpretable artificial intelligence,”
    *arXiv preprint arXiv:1806.00340*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Y. Hong and C. E. Kahn, “Content analysis of reporting templates and
    free-text radiology reports,” *Journal of digital imaging*, vol. 26, no. 5, pp.
    843–849, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, “When radiology
    report generation meets knowledge graph,” *arXiv preprint arXiv:2002.08277*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in *International
    conference on machine learning*.   PMLR, 2018, pp. 1989–1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Unsupervised domain adaptation
    with residual transfer networks,” in *Advances in Neural Information Processing
    Systems*, 2016, pp. 136–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *CVPR2017*, 2017, pp. 7167–7176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Y. Luo, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Taking a closer look
    at domain shift: Category-level adversaries for semantics consistent domain adaptation,”
    in *CVPR2019*, 2019, pp. 2507–2516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker,
    “Learning to adapt structured output space for semantic segmentation,” in *CVPR2018*,
    2018, pp. 7472–7481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Y. Zhang, Y. Wei, Q. Wu, P. Zhao, S. Niu, J. Huang, and M. Tan, “Collaborative
    unsupervised domain adaptation for medical image diagnosis,” *IEEE Transactions
    on Image Processing*, vol. 29, pp. 7834–7844, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] D. Liu, D. Zhang, Y. Song, F. Zhang, L. O Donnell, H. Huang, M. Chen,
    and W. Cai, “Pdam: A panoptic-level feature alignment framework for unsupervised
    domain adaptive instance segmentation in microscopy images,” *IEEE Transactions
    on Medical Imaging*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge graph and text jointly
    embedding,” pp. 1591–1601, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] M. Wistuba, A. Rawat, and T. Pedapati, “A survey on neural architecture
    search.” *arXiv preprint arXiv: 1905.01392*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] D. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao,
    and L. Lu, “Organ at risk segmentation for head and neck cancer using stratified
    learning and neural architecture search,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 4223–4232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
