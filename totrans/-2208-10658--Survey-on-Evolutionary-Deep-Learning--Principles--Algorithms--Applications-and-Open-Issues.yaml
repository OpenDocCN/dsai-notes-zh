- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2208.10658] Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10658](https://ar5iv.labs.arxiv.org/html/2208.10658)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \UseRawInputEncoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nan Li Northeastern UniversityNo.195, Chuangxin RoadShenyangLiaoning ProvinceChina
    [2010500@stu.neu.edu.cn](mailto:2010500@stu.neu.edu.cn) ,  Lianbo Ma Northeastern
    UniversityNo.195, Chuangxin RoadShenyang CityLiaoning ProvinceChina [malb@swc.neu.edu.cn](mailto:malb@swc.neu.edu.cn)
    ,  Guo Yu East China University of Science and TechnologyMeilong Road 130ShanghaiChina
    [guoyu@ecust.edu.cn](mailto:guoyu@ecust.edu.cn) ,  Bing Xue Victoria University
    of WellingtonWellingtonNew Zealand [bing.xue@ecs.vuw.ac.nz](mailto:bing.xue@ecs.vuw.ac.nz)
    ,  Mengjie Zhang Victoria University of WellingtonWellingtonNew Zealand [mengjie.zhang@ecs.vuw.ac.nz](mailto:mengjie.zhang@ecs.vuw.ac.nz)
     and  Yaochu Jin Bielefeld UniversityBielefeldGermany [yaochu.jin@uni-bielefeld.de](mailto:yaochu.jin@uni-bielefeld.de)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Over recent years, there has been a rapid development of deep learning (DL)
    in both industry and academia fields. However, finding the optimal hyperparameters
    of a DL model often needs high computational cost and human expertise. To mitigate
    the above issue, evolutionary computation (EC) as a powerful heuristic search
    approach has shown significant merits in the automated design of DL models, so-called
    evolutionary deep learning (EDL). This paper aims to analyze EDL from the perspective
    of automated machine learning (AutoML). Specifically, we firstly illuminate EDL
    from machine learning and EC and regard EDL as an optimization problem. According
    to the DL pipeline, we systematically introduce EDL methods ranging from feature
    engineering, model generation, to model deployment with a new taxonomy (i.e.,
    what and how to evolve/optimize), and focus on the discussions of solution representation
    and search paradigm in handling the optimization problem by EC. Finally, key applications,
    open issues and potentially promising lines of future research are suggested.
    This survey has reviewed recent developments of EDL and offers insightful guidelines
    for the development of EDL.
  prefs: []
  type: TYPE_NORMAL
- en: 'deep learning, evolutionary computation, feature engineering, model generation,
    model deployment.^†^†ccs: General and reference Surveys and overviews^†^†ccs:
    Computing methodologies Machine learning algorithms^†^†ccs: Theory of computation Evolutionary
    algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning (DL) as a promising technology has been widely used in a variety
    of challenging tasks, such as image analysis (Krizhevsky et al., [2012](#bib.bib103))
    and pattern recognition (LeCun et al., [2015](#bib.bib105)). However, the practitioners
    of DL struggle to manually design deep models and find appropriate configurations
    by trial and error. An example is given in Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"), where domain knowledge is fed to DL in different stages like feature
    engineering (FE) (Xue et al., [2015](#bib.bib226)), model generation (Zhou et al.,
    [2021a](#bib.bib258)) and model deployment (Choudhary et al., [2020](#bib.bib32);
    Cheng et al., [2017](#bib.bib30)). Unfortunately, the difficulty in the acquisition
    of expert knowledge makes DL undergo a great challenge in its development.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the automatic design of deep neural networks (DNNs) tends to be
    prevalent in recent decades (He et al., [2021](#bib.bib72); Zhou et al., [2021a](#bib.bib258)).
    The main reason lies in the flexibility and computation efficiency of automated
    machine learning (AutoML) in FE (Xue et al., [2015](#bib.bib226)), parameter optimization
    (PO) (Zhang and Gouza, [2018](#bib.bib243)), hyperparameter optimization (HPO)
    (Stanley and Miikkulainen, [2002](#bib.bib186)), neural architecture search (NAS)
    (Yao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72); Zhou et al.,
    [2021a](#bib.bib258)), and model compression (MC) (Hu et al., [2021b](#bib.bib79)).
    In this way, AutoML without manual intervention has attracted great attention
    and much progress has been made.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/157981cf4645442887259fefbd7ba762.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. An overview of DL, driven by domain knowledge or evolutionary computation,
    where the life of DL gets through problem, data collection, feature engineering,
    model generation and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evolutionary computation (EC) has been widely applied to automatic DL, owing
    to its flexibility and automatically evolving mechanism. In EC, a population of
    individuals are driven by the environmental selection to evolve towards the optimal
    solutions or front (Jin, [2006](#bib.bib89)). Nowadays, there are many automatic
    DL methods driven by EC, termed as evolutionary deep learning (EDL) (Telikani
    et al., [2021](#bib.bib197); Evans et al., [2018a](#bib.bib53); Zhang and Cagnoni,
    [2020](#bib.bib248); Zhang, [2018](#bib.bib247)). For example, a number of studies
    on EC have been carried out to the feature engineering (Xue et al., [2015](#bib.bib226)),
    model generation (Yao et al., [2018](#bib.bib231); Zhou et al., [2021a](#bib.bib258)),
    and model deployments (Choudhary et al., [2020](#bib.bib32)), as shown in Fig.
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues"). Therefore, the integration
    of EC and DL has become a hot research topic in both academic and industrial communities.
    Moreover, in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), the number
    of publications and citations referring to EC & DL by years from Web of Science
    gradually increases until around 2012, whereas it sharply rises in the following
    decade. Hence, more and more researchers work on the area of EDL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e00f51b178cb0629a960e5ee4e773716.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Total publications and citations referring to EC & DL by years from
    Web of Science until July 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. Comparison between existing surveys and our work, where FE, PO, HPO,
    NAS, and MC indicate feature engineering, parameter optimization, hyperparameter
    optimization, neural architecture search, and model compression, respectively.
    “✓” and “-” indicate the content is included or not in the paper, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Type | FE | PO | HPO | NAS | MC \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| (Yao et al., [2018](#bib.bib231)) | AutoML | ✓ | - | ✓ | ✓ | - \bigstrut[t]
    |'
  prefs: []
  type: TYPE_TB
- en: '| (He et al., [2021](#bib.bib72)) | AutoML | ✓ | - | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Yao, [1993](#bib.bib232)) | NAS | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ren et al., [2021](#bib.bib165)) | NAS | - | - | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Jaâfra et al., [2019](#bib.bib86)) | NAS | - | - | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Santra et al., [2021](#bib.bib172)) | NAS | - | - | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Telikani et al., [2021](#bib.bib197)) | EDL | - | - | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2021b](#bib.bib117)) | EDL | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhou et al., [2021a](#bib.bib258)) | EDL | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Xue et al., [2015](#bib.bib226)) | EDL | ✓ | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Al-Sahaf et al., [2019](#bib.bib6)) | EDL | ✓ | ✓ | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2011](#bib.bib244)) | EDL | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Alexandropoulos and Aridas, [2019](#bib.bib8)) | EDL | ✓ | - | ✓ | ✓ | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Mirjalili et al., [2019](#bib.bib138)) | EDL | ✓ | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Darwish et al., [2020](#bib.bib41)) | EDL | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Freitas, [2003](#bib.bib61)) | EDL | - | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | EDL | ✓ | ✓ | ✓ | ✓ | ✓\bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: 'In Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues"), we have listed
    recent surveys on automatic DL. A large number of surveies concentrate on the
    optimization of DL models (He et al., [2021](#bib.bib72); Telikani et al., [2021](#bib.bib197);
    Zhou et al., [2021a](#bib.bib258); Xue et al., [2015](#bib.bib226)), or NAS (Liu
    et al., [2021b](#bib.bib117); Yao, [1993](#bib.bib232)). Many others focus on
    specific optimization paradigms such as reinforcement learning (RL) (Jaâfra et al.,
    [2019](#bib.bib86)), EC (Sun et al., [2020](#bib.bib192)) and gradient (Santra
    et al., [2021](#bib.bib172)). However, very few of them have systematically analysed
    EDL and runs the gamut of FE, PO, HPO, NAS, and MC. To fill the gap, we aim to
    give a comprehensive review of EDL in detail. The main contributions of this work
    are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing work on EDL is reviewed from the perspective of DL and EC to facilitate
    the understanding of readers from the communities of both ML and EC, and we also
    formulated EDL into an optimization problem from the perspective of EC.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The survey describes and discusses on EDL in terms of feature engineering, model
    generation, and model deployment from a novel taxonomy, where the solution representation
    and the search paradigms are emphasized and systematically discussed. To the best
    of our knowledge, few survey has investigated the evolutionary model deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the basis of the comprehensive review of EDL approaches, a number of applications,
    open issues and trends of EDL are discussed, which will guide the development
    of EDL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows. Section [2](#S2 "2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") presents an overview of EDL. In Section
    [3](#S3 "3\. Feature Engineering ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues"), EC-driven feature engineering is presented.
    EC-driven model generation is discussed in Section [4](#S4 "4\. Model Generation
    ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"). Section [5](#S5 "5\. Model Deployment ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues") reviews
    EC-driven model compressions. After that, relevant applications, open issues and
    the trends of EDL are discussed in Section [6](#S6 "6\. Applications, Open Issues,
    and Trends ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"). Finally, a conclusion of the paper is drawn in Section [7](#S7
    "7\. Conclusions ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. An Overview of Evolutionary Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DL can be described as a triplet $M$ = ($D$, $T$, $P$) (Yao et al., [2018](#bib.bib231)),
    where $D$ is the dataset used for the training of a deep model ($M$), and $T$
    is the targeted task. $P$ indicates the performance of $M$. The aim of DL is to
    boost its performance over specific task $T$, which is measured by $P$ on dataset
    $D$. In Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we can
    see there are three fundamental processes of DL, i.e., feature engineering, model
    generation and model deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature engineering: It aims to find a high-quality $D$ to improve the performance
    ($P$) of the deep model ($M$) on specific tasks ($T$). In practice, the feature
    space of $D$ may include redundant and noisy information, which harms the performance
    ($P$) of the model ($M$). On Prostate dataset, the size of feature subset (65)
    selected in (Tran et al., [2018](#bib.bib200)) is only 1% of the total size of
    features (10509).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model generation: It targets at optimizing/generating a model ($M$) with desirable
    performance ($P$) for specific task ($T$) on the given datasets ($D$) (He et al.,
    [2021](#bib.bib72)). Model generation can be further divided into parameter optimization,
    model architecture optimization, and joint optimization (Zhou et al., [2021a](#bib.bib258)).
    Parameter optimization is to search the best parameters (e.g., weights) for a
    predefined model. Architecture optimization is dedicated to finding the optimal
    network topology (e.g., number of layers and types of operations) of a deep model
    ($M$) (Luo et al., [2018](#bib.bib127)). Joint optimization involves in the above
    two optimization issues by automatically searching for a powerful model ($M$)
    on the datasets ($D$) (Miikkulainen et al., [2019](#bib.bib137)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model deployment: This process aims to deploy a deep model ($M$) to solve a
    deployment task $T$ with acceptable performance ($P$) on input data ($D$) within
    limited computational budgets. The key issue of model deployment is how to reduce
    the latency, storage, and energy consumption when the number of parameters of
    a deep model is large, e.g., Transformer-XL Large has 257M parameters (Dowdell
    and Zhang, [2020](#bib.bib48)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Evolutionary Computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EC is a collection of stochastic population-based search methods inspired by
    evolution mechanisms such as natural selection and genetics, which does not need
    gradient information and is able to handle a black-box optimization problem without
    explicit mathematical formulations (Ma et al., [2022](#bib.bib129); Vargas-Hákim
    et al., [2022](#bib.bib204)). Owing to the above characteristics, EC has been
    widely employed to the automatic design of DL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43778ec24fd8eb2d2ec7491b539a713d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. A general framework of EC.
  prefs: []
  type: TYPE_NORMAL
- en: 'In principle, we can broadly divide EC methods into two categories: evolutionary
    algorithms (EA) and swarm intelligence (SI) (Zhou et al., [2021a](#bib.bib258)).
    Our work doesn’t make an explicit distinction between EAs and SI since they comply
    with a general framework, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary
    Computation ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), which consists
    of three main components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: is performed to generate a population of individuals which are encoded according
    to the decision space (or search space and variable space) of the optimization
    problem, such as the feature set, model parameters and topological structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: aims to calculate the fitness of individuals. In fact, the evaluation of the
    individuals in EDL is a computationally expensive task (Miahi et al., [2022](#bib.bib136)).
    For example, the work (Real et al., [2017](#bib.bib163)) used 3000 GPU days to
    find a desirable architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating:'
  prefs: []
  type: TYPE_NORMAL
- en: aims to generates a number of offspring solutions through various reproduction
    operations. For example, a new soultion is generated via velocity and position
    formula in particle swarm optimization (PSO) (Tran et al., [2018](#bib.bib200)).
    In terms of genetic algorithm (GA), some reproduction operators (e.g., crossover
    and mutation) are used to generate new individuals (Vafaie and De Jong, [1998](#bib.bib203)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Evolutionary Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.3.1\. EDL from two perspectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to traditional DL which heavily relies on expert or domain knowledge
    to build deep model, EDL is to automatically design the deep model through an
    evolutionary process (Sun et al., [2020](#bib.bib192); Yao, [1993](#bib.bib232);
    Ren et al., [2021](#bib.bib165); Zhang, [2018](#bib.bib247)).
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of DL: Traditional DL needs a lot of expert knowledge
    in inventing and analysing a learning tool to a specific dataset or task. In contrast,
    EDL can be seen as a human-friendly learning tool that can automatically find
    appropriate deep models on given datasets or tasks (Yao et al., [2018](#bib.bib231)).
    In other words, EDL concentrates on how easy a learning tool can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of EC: The configurations of a model is represented as
    an individual, and the performance as the objective to be optimized. EC plays
    an important role in the optimization driven by evolutionary mechanisms. Namely,
    EDL can be seen as an evolutionary optimization process to find the optimal configurations
    of the deep model with high performance.'
  prefs: []
  type: TYPE_NORMAL
- en: From the above analysis, EDL not only aims to increase the adaptability of a
    deep model towards learning tasks via the automatic construction approach (from
    the perspective of DL), but also tries to achieve the optimal model under the
    designed objectives or constraints (from the perspective of EC).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. Definition and Framework of EDL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to the above discussion in Subsection [2.3.1](#S2.SS3.SSS1 "2.3.1\.
    EDL from two perspectives ‣ 2.3\. Evolutionary Deep Learning ‣ 2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") and following (Yao et al., [2018](#bib.bib231)),
    we can define EDL as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | <math id="S2.E1.m1.5" class="ltx_Math" alttext="\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&amp;{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&amp;{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}"
    display="block"><semantics id="S2.E1.m1.5a"><mtable displaystyle="true" rowspacing="0pt"
    id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mtr id="S2.E1.m1.5.5a" xref="S2.E1.m1.5.5.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S2.E1.m1.5.5b" xref="S2.E1.m1.5.5.cmml"><mtable
    columnspacing="5pt" displaystyle="true" id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mtr
    id="S2.E1.m1.1.1.1.1.1a" xref="S2.E1.m1.1.1.1.1.1.cmml"><mtd id="S2.E1.m1.1.1.1.1.1b"
    xref="S2.E1.m1.1.1.1.1.1.cmml"><munder id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo
    movablelimits="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">Max</mo><mrow
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mrow
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">c</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">o</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml">n</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.cmml">f</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1c" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.6.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1d" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.7" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.7.cmml">g</mi></mrow><mo
    lspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">.</mo></mrow></munder></mtd><mtd
    id="S2.E1.m1.1.1.1.1.1c" xref="S2.E1.m1.1.1.1.1.1.cmml"><mtext id="S2.E1.m1.1.1.1.1.1.1.1.2.1"
    xref="S2.E1.m1.1.1.1.1.1.1.1.2.1a.cmml">Learning tools’ performance,</mtext></mtd></mtr></mtable></mtd></mtr><mtr
    id="S2.E1.m1.5.5c" xref="S2.E1.m1.5.5.cmml"><mtd class="ltx_align_left" columnalign="left"
    id="S2.E1.m1.5.5d" xref="S2.E1.m1.5.5.cmml"><mtable columnspacing="5pt" displaystyle="true"
    id="S2.E1.m1.5.5.5.4.4" xref="S2.E1.m1.5.5.5.4.4.cmml"><mtr id="S2.E1.m1.5.5.5.4.4a"
    xref="S2.E1.m1.5.5.5.4.4.cmml"><mtd id="S2.E1.m1.5.5.5.4.4b" xref="S2.E1.m1.5.5.5.4.4.cmml"><mrow
    id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3" xref="S2.E1.m1.5.5.5.4.4.cmml"><mrow id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.2"
    xref="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1"
    xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.cmml">s</mi><mo lspace="0em" rspace="0.167em"
    id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.2.1" xref="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.1a.cmml">.</mo><mi
    id="S2.E1.m1.3.3.3.2.2.2.2.2.2.2" xref="S2.E1.m1.3.3.3.2.2.2.2.2.2.2.cmml">t</mi></mrow><mo
    lspace="0em" id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.2" xref="S2.E1.m1.5.5.5.4.4.cmml">.</mo></mrow></mtd><mtd
    id="S2.E1.m1.5.5.5.4.4c" xref="S2.E1.m1.5.5.5.4.4.cmml"><mrow id="S2.E1.m1.5.5.5.4.4.4.4.4.1.3"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.2.cmml"><mo id="S2.E1.m1.5.5.5.4.4.4.4.4.1.3.1"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.2.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml"><mtr
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1a" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml"><mtd
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1b" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml"><mtext
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1a.cmml">No
    assistance from humans</mtext></mtd></mtr><mtr id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1v"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml"><mtd id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1w"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml"><mtext id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1a.cmml">Limited computational budgets.</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S2.E1.m1.5b"><matrix id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><matrixrow
    id="S2.E1.m1.5.5a.cmml" xref="S2.E1.m1.5.5"><matrix id="S2.E1.m1.1.1.1.1.1.cmml"
    xref="S2.E1.m1.1.1.1.1.1"><matrixrow id="S2.E1.m1.1.1.1.1.1a.cmml" xref="S2.E1.m1.1.1.1.1.1"><apply
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3">Max</ci><list
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><apply
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑐</ci><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑜</ci><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.4">𝑛</ci><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.5">𝑓</ci><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.6">𝑖</ci><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.7.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.7">𝑔</ci></apply></list></apply><ci
    id="S2.E1.m1.1.1.1.1.1.1.1.2.1a.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1"><mtext
    id="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1">Learning
    tools’ performance,</mtext></ci><cerror id="S2.E1.m1.1.1.1.1.1b.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.1.1.1.1.1c.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1d.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1e.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1f.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1g.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1h.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1i.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1j.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1k.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1l.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1m.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1n.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1o.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1p.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1q.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1r.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1s.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1t.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1u.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1v.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1w.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1x.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1y.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1z.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1aa.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1ab.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1ac.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1ad.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1ae.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1af.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1ag.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1ah.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1ai.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.1.1.1.1.1aj.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S2.E1.m1.1.1.1.1.1ak.cmml" xref="S2.E1.m1.1.1.1.1.1">missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow><matrixrow
    id="S2.E1.m1.5.5b.cmml" xref="S2.E1.m1.5.5"><matrix id="S2.E1.m1.5.5.5.4.4.cmml"
    xref="S2.E1.m1.5.5.5.4.4"><matrixrow id="S2.E1.m1.5.5.5.4.4a.cmml" xref="S2.E1.m1.5.5.5.4.4"><apply
    id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.1.cmml" xref="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.2"><csymbol
    cd="ambiguous" id="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.1a.cmml" xref="S2.E1.m1.4.4.4.3.3.3.3.3.3.3.1.2.1">formulae-sequence</csymbol><ci
    id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1">𝑠</ci><ci
    id="S2.E1.m1.3.3.3.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.3.3.3.2.2.2.2.2.2.2">𝑡</ci></apply><apply
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.2.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.3"><csymbol
    cd="latexml" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.2.1.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.3.1">cases</csymbol><matrix
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><matrixrow
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1a.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><ci
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1a.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1"><mtext
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.1.1.1">No
    assistance from humans</mtext></ci><cerror id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1b.cmml"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1c.cmml"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1d.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1e.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1f.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1g.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1h.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1i.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1j.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1k.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1l.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1m.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1n.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1o.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1p.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1q.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1r.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1s.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1t.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1u.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1v.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1w.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1x.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1y.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1z.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1aa.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ab.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ac.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ad.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ae.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1af.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ag.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ah.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ai.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1aj.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ak.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1al.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1am.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1an.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><ci
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1a.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1"><mtext
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1.2.1.1">Limited
    computational budgets.</mtext></ci><cerror id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ao.cmml"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ap.cmml"
    xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1aq.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ar.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1as.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1at.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1au.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1av.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1aw.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ax.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ay.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1az.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1ba.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bb.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bc.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bd.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1be.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bf.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bg.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bh.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bi.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bj.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bk.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bl.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bm.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bn.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bo.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bp.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bq.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1br.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bs.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bt.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bu.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bv.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bw.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bx.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1by.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.5.5.5.4.4.4.4.4.1.1bz.cmml" xref="S2.E1.m1.5.5.5.4.4.4.4.4.1.1">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    id="S2.E1.m1.5.5.5.4.4b.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4c.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4d.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4e.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4f.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4g.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4h.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4i.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4j.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4k.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4l.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4m.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4n.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4o.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4p.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4q.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4r.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4s.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4t.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4u.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4v.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4w.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4x.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4y.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4z.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4aa.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4ab.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4ac.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4ad.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4ae.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4af.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4ag.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4ah.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4ai.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror><cerror
    id="S2.E1.m1.5.5.5.4.4aj.cmml" xref="S2.E1.m1.5.5.5.4.4"><csymbol cd="ambiguous"
    id="S2.E1.m1.5.5.5.4.4ak.cmml" xref="S2.E1.m1.5.5.5.4.4">missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E1.m1.5c">\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\ {{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}</annotation></semantics></math>
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: where $config.$ indicates the configurations which form the decision space of
    an optimization problem. The problem is to maximize the objective (i.e., learning
    tools’ performance $P$) of tasks $T$ on datasets $D$ under the constraints of
    no assistance from humans and limited computational resources. Accordingly, three
    aspects are taken into account in the design of EDL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Desirable generalization performance::'
  prefs: []
  type: TYPE_NORMAL
- en: EDL should have desirable generalization performance across given datasets and
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'High search efficiency::'
  prefs: []
  type: TYPE_NORMAL
- en: EDL is able to find optimal or desirable configuration within a limited computational
    budges (e.g., hardware, latency, energy consumption) under different designed
    objectives (e.g., high accuracy, small model size).
  prefs: []
  type: TYPE_NORMAL
- en: 'Without human assistance::'
  prefs: []
  type: TYPE_NORMAL
- en: EDL is able to automatically configure without human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the EC framework described in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary
    Computation ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we present
    a general framework of EDL as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1 Initialization::'
  prefs: []
  type: TYPE_NORMAL
- en: A population of individuals are initialized according to the designed encoding
    scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2 Evaluation::'
  prefs: []
  type: TYPE_NORMAL
- en: Each individual is evaluated according to the objectives (e.g., high accuracy,
    small model size) or constraints (e.g., energy consumption).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3 Updating::'
  prefs: []
  type: TYPE_NORMAL
- en: A required number of new solutions are generated from previous generation via
    various updating operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4 Termination condition::'
  prefs: []
  type: TYPE_NORMAL
- en: Go to Step 2 if the predefined termination condition is unsatisfied; Otherwise,
    go to Step 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5 Output::'
  prefs: []
  type: TYPE_NORMAL
- en: Output the solution with the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. Taxonomy of EDL Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, a novel taxonomy of EDL approaches is proposed according to
    “what to evolve/optimize” and “how to evolve/optimize”, as shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.3.3\. Taxonomy of EDL Approaches ‣ 2.3\. Evolutionary Deep Learning
    ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc42ccf964acca317096d7d4d08a8c05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. A taxonomy of EDL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '“What to evolve/optimize”: We may be concerned about “what EDL can do” or “what
    kinds of problems EDL can tackle”. In feature engineering, there are three key
    issues to be resolved, including the feature selection, feature construction and
    feature extraction (Yao et al., [2018](#bib.bib231)). In model generation, parameter
    optimization, architecture optimization, and joint optimization become the critical
    issues (Zhou et al., [2021a](#bib.bib258)), while model deployment is involved
    with the issues of model pruning and other compression technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: '“How to evolve/optimize”: The answer to the question is designing appropriate
    solution representation and search paradigm for EC, and acceleration strategies
    for NAS. The representation schemes are designed for the encoding of individuals,
    search paradigms for the achievement of optimal configurations, acceleration strategies
    for the reduction of time or resources consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the above taxonomy, we will elaborately introduce EDL in feature
    engineering, model generation and model deployment in Sections [3](#S3 "3\. Feature
    Engineering ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"), [4](#S4 "4\. Model Generation ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues") and [5](#S5 "5\.
    Model Deployment ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature engineering is adopted to pre-process given raw data by filtering out
    the irrelevant features of the data or creating the new features based on original
    features (Xue et al., [2015](#bib.bib226)). Various EC-based techniques have been
    proposed to reduce data dimensionality, speed up learning process, or improve
    model performance (Xue et al., [2015](#bib.bib226)). The common techniques can
    be categorized into feature selection (Nguyen et al., [2014](#bib.bib147)), feature
    construction (Bhanu and Krawiec, [2002](#bib.bib17)) and feature extraction (Peng
    et al., [2021](#bib.bib153)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Feature Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Feature selection aims to automatically select a representative subset of features
    where there are no irrelevant or redundant features. However, the search space
    grows exponentially with the increase of features. If a dataset has $n$ features,
    then there are $2^{n}$ solutions in the search space. In addition, the interactions
    between features may seriously impact the feature selection performance (Xue et al.,
    [2015](#bib.bib226)). In the followings, we will review existing work on solution
    representations and search paradigms in EC for feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, there are three different categories of solution representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear encoding: This encoding uses vectors or strings to store feature information.
    For example, in (Estévez and Caballero, [1998](#bib.bib52)), a fixed-length binary
    vector was used to express whether a feature is selected or not, where “1” indicates
    a corresponding feature is selected, and “0” is the opposite. In (Hong and Cho,
    [2006](#bib.bib75)), a binary index was used to indicate the corresponding feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-based encoding: In canonical genetic programming (GP), all leaf nodes/terminal
    nodes represent the selected features and non-terminal nodes represent functions
    (e.g., arithmetic or logic operators) (Krawiec, [2002](#bib.bib101)). For automatic
    classification on high-dimensional data, Krawiec et al. (Krawiec, [2002](#bib.bib101))
    proposed a tree-based encoding to select a subset of highly discriminative features,
    where each feature consisted of sibling leaf nodes and their paternal function
    node. On the basis of the tree-based encoding, Muni et al. (Muni et al., [2006](#bib.bib141))
    proposed a multi-tree GP mothed for online feature selection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph-based encoding: In (O’Boyle and Palmer, [2008](#bib.bib148)), the feature
    space of the high-dimensional data is represented by a graph and each node of
    the graph represents a feature. A feature subset is composed of visited nodes
    of the graph, i.e., the path of node composition or subgraph. Yu et al. (Yu et al.,
    [2009](#bib.bib236)) converted feature selection to the optimal path problem in
    a directed graph, where the value of the node was “1” or “0” to indicate whether
    the feature was selected or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In feature selection, representative types of search paradigms are introduced
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: In feature selection, typical evolutionary search
    methods have been widely used, such as GA (Da Silva and Neto, [2011](#bib.bib37);
    Estévez and Caballero, [1998](#bib.bib52)), GP (Krawiec, [2002](#bib.bib101);
    Nekrasov et al., [2020](#bib.bib143)), PSO (Vieira et al., [2013](#bib.bib205);
    Nguyen et al., [2014](#bib.bib147)), ant colony optimization (ACO) (Khushaba et al.,
    [2008](#bib.bib96); Ma et al., [2021c](#bib.bib131)), and artificial bee colony
    (ABC) (Wang et al., [2020d](#bib.bib211)). Besides, some other studies (Khushaba
    et al., [2008](#bib.bib96)) combined ACO with DE to seek optimal feature subsets,
    where the solutions searched by the ACO were fed into the DE to further explore
    the optimal solution. In (Da Silva and Neto, [2011](#bib.bib37)), a family of
    feature selection methods based on different variants of GA were developed to
    improve the accuracy of content-based image retrieval systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-evolution search paradigm: In co-evolution search paradigm for feature selection,
    at least two populations are simultaneously evolved and interacted toward the
    optimal subset of features (Wen and Xu, [2011](#bib.bib214); Rashid et al., [2020](#bib.bib160)).
    For example, a divide-and-conquer strategy was developed in (Wen and Xu, [2011](#bib.bib214))
    to manage two subpopulations. One subpopulation was to conduct an evolution process
    of classifier design, while the other one was to search for an optimal subset
    of features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective search paradigm: This type of search paradigms are driven by
    two or more conflicting objectives (Xue et al., [2012](#bib.bib225); Hancer et al.,
    [2015](#bib.bib71); Cheng et al., [2021](#bib.bib29)), such as the maximization
    of the accuracy of a classifier and minimization of the size of a feature subset.
    On the basis of the above two conflicting objectives, Xue et al. (Xue et al.,
    [2012](#bib.bib225)) designed a multi-objective PSO algorithm for feature selection
    and obtained a set of Pareto non-dominated candidate solutions for feature selection
    after the multi-objective search.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GA and GP are widely applied to feature selection. GA early serves for low-dimensional
    (i.e., $\leq$1000) datasets (Xue et al., [2015](#bib.bib226); Hosni et al., [2020](#bib.bib77)).
    Recently, many GA-based approaches have been proposed to solve high-dimensional
    feature selection (Cheng et al., [2021](#bib.bib29)). Nevertheless, GP is commonly
    applied to large-scale/high-dimensional feature selection since it is flexible
    in feature representation (Xue et al., [2015](#bib.bib226)). Especially, GP outperforms
    GA on some small but high-dimensional datasets, e.g., Brain Tumor-2 (Chen et al.,
    [2022](#bib.bib24)) with 10367 features but only 50 samples. In addition, PSO
    has been proved with faster convergence rate to an optimal feature subset than
    GAs and GP (Zhang et al., [2017](#bib.bib251)). The graph representation of ACO
    outperforms GA and GP on flexibility, but the challenge of ACO is how to design
    appropriate graph encoding for large-scale scenarios (Telikani et al., [2021](#bib.bib197);
    Xue et al., [2015](#bib.bib226)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Feature Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Feature construction is to create new high-level features from the original
    features (Tran et al., [2016](#bib.bib202)) via appropriate function operators
    (e.g., conjunction and average) (Xue et al., [2013](#bib.bib227); Neshatian et al.,
    [2012](#bib.bib145)), so that the high-level features are more easily discriminative
    than the original ones. Feature construction is a complicated combinatorial optimization
    problem, where search space increases exponentially along with the total number
    of original features and the function operators. In the following subsections,
    we will describe the EC-based feature construction methods in terms of both solution
    representations and search paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing EC-based approaches for feature construction can be categorized into
    three groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear encoding: The study (Xue et al., [2013](#bib.bib227)) used $n$-bit ($n$
    is the total number of original features) binary vector to represent each particle,
    where “0” indicated the corresponding feature not applied to build the new high-level
    feature while “1” was in the opposite. On the basis of the encoding, a local search
    was performed to select candidate operators from a predefined function set to
    construct a new high-level feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-based encoding: Tree-based encoding is natural for feature construction,
    where leaf nodes represent the feature information and internal nodes represent
    operators. Many studies (Tran et al., [2016](#bib.bib202); Bhanu and Krawiec,
    [2002](#bib.bib17)) have demonstrated the effectiveness of tree encoding in feature
    construction. For example, Bhanu et al. (Bhanu and Krawiec, [2002](#bib.bib17))
    designed a GP-based coevolutionary feature construction procedure to improve the
    discriminative ability of classifiers. In (Tran et al., [2016](#bib.bib202)),
    an individual in EC was represented by a multi-tree encoding with multiple high-level
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph-based encoding: In this encoding, the nodes and edges represent features
    and operators (e.g., “+”, “-”, “*”, “/”), respectively. Teller et al. (Teller
    and Veloso, [1996](#bib.bib198)) applied an arbitrary directed graph to represent
    all features and operators, where each possible high-level feature can be represented
    as a subgraph of this directed graph. For linear GP, features and operations form
    a many-to-many directed acyclic graph, in which each feature is loaded into predefined
    registers and register’s value can be used in multiple operators (Fogelberg and
    Zhang, [2005](#bib.bib58)). However, graph encoding becomes inefficient on high-dimensional
    feature sets since the complexity of graph traversal exacerbates the difficulty
    of feature construction.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are four categories of search paradigms for feature construction in existing
    work .
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: Existing studies include but are not limited to GA
    (Vafaie and De Jong, [1998](#bib.bib203)), and GP (Tran et al., [2016](#bib.bib202);
    Neshatian et al., [2007](#bib.bib146); Tariq et al., [2018](#bib.bib196)). For
    example, the work (Neshatian et al., [2007](#bib.bib146)) designed GP-based feature
    construction to reduce the feature (input) dimensions of a classifier. Especially,
    GP has been also widely used to construct new features, where each individual
    following the form of a GP tree usually represents a constructed high-level feature
    (García et al., [2011](#bib.bib63)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-evolution search paradigm: It can be decomposed to feature construction
    subproblem and classifier design subproblem, and each subproblem is solved with
    a standalone subpopulation by an EC-based method (Bhanu and Krawiec, [2002](#bib.bib17);
    Roberts and Claridge, [2005](#bib.bib166)). For example, the study (Roberts and
    Claridge, [2005](#bib.bib166)) decomposed feature construction into two subproblems
    (i.e., feature construction, and object detection), where the feature construction
    was solved by evolving a population of pixel (i.e., feature) and the object detection
    was optimized using object detection algorithm (ODA) (Roberts and Claridge, [2005](#bib.bib166)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-features construction search paradigm: Unlike early methods (Tran et al.,
    [2016](#bib.bib202); Vafaie and De Jong, [1998](#bib.bib203); Shafti and Pérez,
    [2008](#bib.bib176); Tran et al., [2019](#bib.bib201)) only constructing one high-level
    feature in a single search process, this sort of paradigms are able to create
    multiple high-level features. For example, Ahmed et al. (Ahmed et al., [2014](#bib.bib4))
    employed Fisher criterion together with $p$-value measure as the discriminant
    information between classes, based on which multiple features were constructed
    through multiple GP trees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective evolutionary search paradigm: In this search paradigm, the
    number of features and classification accuracy are commonly taken into account
    as the objective functions for multi-objective evolutionary optimization (Hammami
    et al., [2018](#bib.bib69); Castelli et al., [2011](#bib.bib20)). Especially,
    Hammami et al. (Hammami et al., [2018](#bib.bib69)) constructed a set of high-level
    features by optimizing a multi-objective optimization problem (MOP) with three
    objectives (i.e., the number of features, the mutual information, and classification
    accuracy) with Pareto dominance relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GP-based approaches are popular in feature construction due to the flexible
    representation of features and operations. In addition, the hybrid of evolutionary
    algorithms also attracts much attention for feature construction. However, there
    is still plenty of room for the improvement of efficiency in constructing features
    in high-dimensional or large-scale scenarios, where a large number of computational
    resources are needed (Tariq et al., [2018](#bib.bib196); Tran et al., [2019](#bib.bib201)).
    Notably, feature construction often requires more computational overhead than
    feature selection, since feature construction commonly performs after the feature
    selection and the quality of the selected features may influence the performance
    of feature construction.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Feature extraction is to reduce the feature dimensions by altering the original
    features/data via some transformation functions (He et al., [2021](#bib.bib72)).
    Traditional extractors include principal component analysis (PCA) (Abdi and Williams,
    [2010](#bib.bib2)) and linear discriminant analysis (LDA) (Izenman, [2013](#bib.bib85)).
    However, they cannot keep somewhat important information after the transformation
    (Abdi and Williams, [2010](#bib.bib2)) and it is tedious to tune their hyperparameters
    (e.g., number of retained features) to find the best extraction. Thus, automatically
    finding high-quality map functions by EC-based approaches to achieve informative
    feature set tends to be popular.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two typical ways for solution representation in EC-driven feature
    extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear encoding: In this encoding, map functions (Refahi et al., [2020](#bib.bib164);
    Albukhanajer et al., [2015](#bib.bib7)) or function parameters (Zhao et al., [2006](#bib.bib255))
    are encoded as a linear format. For example, Wissam et al. (Albukhanajer et al.,
    [2015](#bib.bib7)) predefined three sets of track functions (i.e., trace functions,
    diametric functions, and circus functions) for feature extraction, and the optimal
    combination between the functions were obtained by an EC-based method. In (Zhao
    et al., [2006](#bib.bib255)), the hyperparameters of map functions were encoded
    by some linear vectors which were constructed by a number of optimal projection
    basis vectors obtained via EC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-based encoding: In tree-based encoding, leaf nodes represent original
    features or constants, while the non-leaf nodes are some operators for feature
    extraction including common arithmetic, logical operators (i.e., “+”, “/”, “$\cup$”)
    or other transformation operators (e.g., uLBP, and SobelY). In EC-driven feature
    extraction, an individual represents a feature extractor or map function (Peng
    et al., [2021](#bib.bib153); Zhang and Rockett, [2011](#bib.bib253)). Especially,
    an EC-based framework was developed in (Zhang and Rockett, [2011](#bib.bib253))
    to search for features and sequences of operations by use of tree-based encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, some common search paradigms for feature extraction are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: EC has been successfully utilized in various feature
    extraction tasks (Z.-Flores et al., [2020](#bib.bib237); Bi et al., [2018](#bib.bib18)).
    For example, Zhao et al. (Zhao et al., [2007](#bib.bib257)) introduced bagging
    concept to an evolutionary algorithm for feature extraction. The work in (Zhao
    et al., [2009](#bib.bib256)) developed an evolutionary discriminant feature extraction
    (EDFE) algorithm by combining GA with subspace analysis, which can reduce the
    complexity of the search space and improve the classification performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-evolution search paradigm: In feature extraction, finding the optimal extractor
    is an optimization problem, which can be decomposed into a series of subproblems
    (Kotani and Kato, [2004](#bib.bib98); Hajati et al., [2010](#bib.bib68)). For
    example, Hajati et al. (Hajati et al., [2010](#bib.bib68)) proposed a co-evolutionary
    method for feature extraction. Specifically, a subpopulation was evolved to optimize
    the classifier-related subproblem (i.e., classifier construction), and the other
    subpopulation made use of genetic information from the first population for the
    optimization of a feature-related subproblem (i.e., feature extraction).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective search paradigm: In multi-objective feature extraction, the
    model accuracy, computational time, complexity, and robustness are often taken
    into account as the objectives (Zhang and Rockett, [2011](#bib.bib253); Cano et al.,
    [2017](#bib.bib19)). Cano et al. (Cano et al., [2017](#bib.bib19)) proposed a
    Pareto-based multi-objective GP algorithm for feature extraction and data visualization,
    where the objectives were to minimize the complexity of data transformation (i.e.,
    tree size) and maximize the recognition performance (i.e., accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In existing studies, many efficient searching and balancing strategies, driven
    by EC approaches to achieve satisfactory solutions at significantly-reduced computation
    overheads, have been developed in recent years (Zhang and Rockett, [2011](#bib.bib253);
    Cano et al., [2017](#bib.bib19); Mauceri et al., [2021](#bib.bib133); Shakya et al.,
    [2021](#bib.bib177)). However, the performance of extractors may be limited with
    existing encoding methods and predefined operation sets. Therefore, it is essential
    to develop efficient algorithms, operation control strategies and representation
    for high-dimensional feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Model Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model generation is to search for optimal models with desirable learning capability
    on given tasks (Yao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72)).
    In this section, we introduce corresponding evolutionary parameter optimization,
    architecture optimization, and joint optimization from solution representation
    to search paradigms. Readers interested in other model generation approaches (e.g.,
    RL-based and gradient-based approaches) can refer to the reviews (Ren et al.,
    [2021](#bib.bib165); Jaâfra et al., [2019](#bib.bib86)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Model Parameter Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Model parameter optimization targets at searching for the best parameter set
    (i.e., weights $W^{*}$) for a predefined architecture ($A$). The loss function
    $L$ (e.g., the cross-entropy loss function) measures the performance of the model
    with optimized parameters (i.e., $W$ in Eq. [2](#S4.E2 "In 4.1.1\. Problem Formulation
    ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model Generation ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")) on given
    datasets. The general model parameter optimization can be formulated as'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\begin{matrix}{{W}^{*}}$=$\underset{W}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $W$ is usually large-scale (millions of model parameters) and highly non-convex.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two typical EC-based representation schemes for model parameter optimization,
    including direct encoding and indirect encoding (He et al., [2021](#bib.bib72)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct encoding: The model parameters are directly represented via a vector
    or matrix, in which each element represents a specific parameter (Ijjina and Chalavadi,
    [2016](#bib.bib83); Karegowda and Manjunath, [2011](#bib.bib94)). For example,
    a chromosome with 64 real numbers was used to directly represent the network corresponding
    weights, where the first 63 real numbers were used to encode three convolution
    masks of size 1 $\times$ 21\. The last real number was the random seed of a generator
    for the initialization of a fully connected network (Ijjina and Chalavadi, [2016](#bib.bib83)).
    This encoding approach may require a huge computational overhead to represent
    and optimize the large-scale weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indirect encoding: This encoding approach represents only a subset of the model
    parameters via a deterministic transformation (Li et al., [2019](#bib.bib110);
    Koutník et al., [2010](#bib.bib99)). In (Koutník et al., [2010](#bib.bib99)),
    the weight information was encoded as a set of Fourier coefficients in the frequency
    domain to reduce dimensionality of representation by ignoring high-frequency coefficients.
    Although this method is able to speed up the search process, the loss of parameter
    the information may occur due to the incomplete information representation, which
    maydegrades the model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EC-based methods for model parameter optimization can be divided into two categories
    according to whether or not method combines with the gradient approach, i.e.,
    pure EC and gradient-based EC.
  prefs: []
  type: TYPE_NORMAL
- en: Pure EC paradigms optimize model parameters only via evolutionary search, including
    the basic EC search paradigm and co-evolution search paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: In addition to GA (Montana and Davis, [1989](#bib.bib140);
    Karegowda and Manjunath, [2011](#bib.bib94)), some heuristic algorithms like PSO
    (Al-kazemi and Mohan, [2002](#bib.bib5)), ABC (Karaboga et al., [2007](#bib.bib93))
    and ACO (Socha and Blum, [2007](#bib.bib183)) are also commonly utilized for model
    parameter optimization. For example, Karaboga et al. (Karaboga et al., [2007](#bib.bib93))
    adopted ABC to find a set of weights for a feed-forward neural network (FNN) on
    targeted tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Co-evolution search paradigm: Co-evolution search is conducted on the subproblems
    of the original optimization problem (e.g., synapse-based and neuron-based problems
    (Chandra and Zhang, [2012](#bib.bib23); Chandra, [2015](#bib.bib22))). For example,
    Chandra et al. (Chandra and Zhang, [2012](#bib.bib23)) regarded a single hidden
    layer as a subcomponent in the initialization phase, which will be merged with
    the individuals with the best fitness from different sub-populations to constitute
    new neural networks during the co-evolution optimization process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6825f4f47b67248cf5a997f1ecfa83e1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/38e7f4b1d3767aae05ade0ace0f876a1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2410148fead42dccca01069d6689080e.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5. Three hybrid ways of gradient-based ECs.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based EC combine basic EC with the gradient-based method to enhance
    the exploitation ability in optimizing model parameters. According to the execution
    order, there are three hybrid ways.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first hybridization approach is shown in Fig. ([5(a)](#S4.F5.sf1 "In Figure
    5 ‣ 4.1.3\. Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model
    Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the EC is used to identify the optimal parameters for
    model, then the parameters are further optimized using gradient-based method to
    find the final optimal solution (Zhang and Smart, [2004](#bib.bib250); Chen et al.,
    [2015](#bib.bib25)). For example, a genetic adaptive momentum estimation algorithm
    (GADAM) was proposed in (Zhang and Gouza, [2018](#bib.bib243)) by incorporating
    Adam and GA into a unified learning scheme, where Adam was an adaptive moment
    estimation method with first-order gradient.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second hybridization approach is given in Fig. ([5(b)](#S4.F5.sf2 "In Figure
    5 ‣ 4.1.3\. Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model
    Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the gradient-based method is used to produce a set of
    parameters for the initialization of the population used in EC (Wu et al., [2021b](#bib.bib217)).
    For example, the study (Khadka and Tumer, [2018](#bib.bib95)) firstly trained
    a RL agent through a gradient-based method, then the parameters of the RL were
    used as the initial population to feed the EC. As a result, the parameters will
    be further optimized by the EC.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third approach is presented in Fig. ([5(c)](#S4.F5.sf3 "In Figure 5 ‣ 4.1.3\.
    Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model Generation ‣
    Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues")), which iteratively applies EC and gradient-based method during
    the optimization to find the optimal parameters. Following this framework, when
    the method is used and which method is chosen are varying in different studies
    (Yang et al., [2021](#bib.bib229); Cui et al., [2018](#bib.bib36)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In model parameter optimization, direct encoding is straightforward and able
    to keep more information than the indirect encoding. Compared to gradient-based
    methods easily trapped into local optima, EC shows more powerful ability in global
    search. Here, several scenarios are introduced as follows, where EC is applied
    to model parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Small-scale scenario: (Montana and Davis, [1989](#bib.bib140)) shows that pure
    EC approaches outperform gradient-based methods in search effectiveness on some
    small-scale problems, where the models are with small numbers of parameters or
    simple architectures (e.g., FNN).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large-scale scenario: The performance of pure EC approaches might not be promising
    in large-scale learning models, while a better way is to utilize the hybridization
    of EC and gradient-based methods. Such hybrid methods can alleviate the issue
    of getting trapped in local optima and increase the effectiveness of subsequent
    exploitation (Yang et al., [2021](#bib.bib229)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to above scenarios, EC-based method can be used to train the DNN,
    when the exact gradient information of the loss function is difficult to be acquired
    (Peng et al., [2018](#bib.bib154)). For example, the rewards of policy network
    are sparse or deceptive in deep reinforcement learning (DRL) so that the gradient
    information is unattainable. The work (Conti et al., [2018](#bib.bib35)) introduced
    the novelty search (NS) and the quality diversity (QD) to the evolution strategies
    (ES) for the policy network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Model Architecture Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Model architecture optimization, also termed as NAS, is to search promising
    network architectures with good performance such as model accuracy on given tasks.
    The model architecture optimization can be formulated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | <math id="S4.E3.m1.1" class="ltx_math_unparsed" alttext="\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&amp;A\in\mathcal{A}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{matrix}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{matrix}\right." display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1b"><mo
    id="S4.E3.m1.1.2">{</mo><mtable displaystyle="true" rowspacing="0pt" id="S4.E3.m1.1.1.1.1"><mtr
    id="S4.E3.m1.1.1.1.1a"><mtd id="S4.E3.m1.1.1.1.1b"><mrow id="S4.E3.m1.1.1.1.1.5.5.4.4"><msup
    id="S4.E3.m1.1.1.1.1.5.5.4.4.6"><mi id="S4.E3.m1.1.1.1.1.5.5.4.4.6.2">A</mi><mo
    id="S4.E3.m1.1.1.1.1.5.5.4.4.6.3">∗</mo></msup><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.5.5.4.4.5">​</mo><mtext
    id="S4.E3.m1.1.1.1.1.5.5.4.4.7">=</mtext><mo lspace="0.167em" rspace="0em" id="S4.E3.m1.1.1.1.1.5.5.4.4.5a">​</mo><munder
    accentunder="true" id="S4.E3.m1.1.1.1.1.3.3.2.2.2"><mrow id="S4.E3.m1.1.1.1.1.3.3.2.2.2.3"><mi
    id="S4.E3.m1.1.1.1.1.3.3.2.2.2.3.1">arg</mi><mo lspace="0.167em" id="S4.E3.m1.1.1.1.1.3.3.2.2.2.3a">⁡</mo><mi
    id="S4.E3.m1.1.1.1.1.3.3.2.2.2.3.2">min</mi></mrow><mrow id="S4.E3.m1.1.1.1.1.3.3.2.2.2.2.4"><mi
    id="S4.E3.m1.1.1.1.1.2.2.1.1.1.1.1">W</mi><mo id="S4.E3.m1.1.1.1.1.3.3.2.2.2.2.4.1">,</mo><mi
    id="S4.E3.m1.1.1.1.1.3.3.2.2.2.2.2">A</mi></mrow></munder><mo lspace="0.167em"
    rspace="0em" id="S4.E3.m1.1.1.1.1.5.5.4.4.5b">​</mo><mi id="S4.E3.m1.1.1.1.1.5.5.4.4.8">L</mi><mo
    lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.5.5.4.4.5c">​</mo><mrow id="S4.E3.m1.1.1.1.1.5.5.4.4.9.2"><mo
    id="S4.E3.m1.1.1.1.1.5.5.4.4.9.2.1">(</mo><mi id="S4.E3.m1.1.1.1.1.4.4.3.3.3">W</mi><mo
    id="S4.E3.m1.1.1.1.1.5.5.4.4.9.2.2">,</mo><mi id="S4.E3.m1.1.1.1.1.5.5.4.4.4">A</mi><mo
    id="S4.E3.m1.1.1.1.1.5.5.4.4.9.2.3">)</mo></mrow></mrow></mtd></mtr><mtr id="S4.E3.m1.1.1.1.1c"><mtd
    id="S4.E3.m1.1.1.1.1d"><mtable columnspacing="5pt" displaystyle="true" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1"><mtr
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1a"><mtd id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1b"><mrow
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.3"><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.3.1.2"><mi
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">s</mi><mo lspace="0em" rspace="0.167em"
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.3.1.2.1">.</mo><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2">t</mi></mrow><mo
    lspace="0em" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.3.2">.</mo></mrow></mtd><mtd
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1c"><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.4.1"><mi
    id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.4.1.2">A</mi><mo id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.4.1.1">∈</mo><mi
    class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.4.1.3">𝒜</mi></mrow></mtd></mtr></mtable></mtd></mtr></mtable></mrow><annotation
    encoding="application/x-tex" id="S4.E3.m1.1c">\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&A\in\mathcal{A}\\ \end{matrix}\\ \end{matrix}\right.</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $A^{*}$ indicates the architecture from the search space ($\mathcal{A}$)
    with the best performance under the parameters $W$, and $L$ is used to measure
    the performance of architectures on given tasks. Thereby, this optimization is
    a bi-level optimization problem (Zhou et al., [2021a](#bib.bib258); Liu et al.,
    [2021b](#bib.bib117)), where the model architecture optimization is subject to
    the model parameter optimization (Lu et al., [2021](#bib.bib125)). Since the current
    NAS works are mainly focused on CNN, we will discuss the solution representations,
    the search paradigms, and acceleration strategies of CNN. Due to the page limit,
    the design of the search space of NAS are not introduced here, but interested
    readers can check these surveys (Liu et al., [2021b](#bib.bib117); Ren et al.,
    [2021](#bib.bib165)) which have details about search space design.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to varying lengths of encodings, we can classify the encoding strategies
    into fixed-length and variable-length encodings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixed-length encoding: The length of each individual is fixed during the evolution.
    For example, a fixed-length vector is designed to represent the model architecture
    of CNN (Xie and Yuille, [2017](#bib.bib222)), where a subset of elements in the
    vector represents an architectural units (e.g., convolutional, pooling or fully-connected
    layer) of a CNN. Such encoding may be easily adapted to evolutionary operations
    (e.g., crossover and mutation) of EC (Xie and Yuille, [2017](#bib.bib222)), but
    it has to specify an appropriate maximal length, which is usually unknown in advance
    and needs to predefine based on domain expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable-length encoding: Different from the fixed-length approach, the variable-length
    encoding strategy does not require a prior knowledge about the optimal depth of
    model architecture and actually could be a way to reduce the complexity of the
    search space. The flexible design of this encoding may encode more detailed information
    about the architecture into a solution vector, and the optimal length of the solution
    is automatically found during the search process (Liu et al., [2021b](#bib.bib117)).
    In (Chen et al., [2020](#bib.bib27)), the entire variational autoencoder (VAE)
    was divided into four blocks, including h-block, $\mu$-block, $\sigma$-block and
    t-block, while the variable-length chromosomes consisted of different quantities
    and types of layers. Notably, variable-length encoding it is not straightforward
    to apply standard genetic operators (e.g., crossover).'
  prefs: []
  type: TYPE_NORMAL
- en: Since the neural network architectures are composed of basic units and connections
    between them, so that both of them are to be encoded, as suggested in (Liu et al.,
    [2021b](#bib.bib117)).
  prefs: []
  type: TYPE_NORMAL
- en: 1) Encoding hyperparameters of basic units. In CNNs, there are many hyperparameters
    to be specified for each unit (e.g., layer, block or cell), such as feature map
    size, type of convolution layer, and filter size (Sun et al., [2020](#bib.bib192)).
    In (Sun et al., [2019b](#bib.bib189)), DenseBlock only had to set two hyperparameters
    (e.g., block type and specific parameter of internal unit) to configure the block
    can be seen as a microcosm of a complete CNN model. The parameterization of a
    cell is more flexible than that of a block since it can be configured via a combination
    of different primitive layers (Sun et al., [2019c](#bib.bib190)).
  prefs: []
  type: TYPE_NORMAL
- en: '2) Encoding connections between units. In general, there are two kinds of model
    architectures according to the connection patterns of basic units: linear topological
    architectures and non-linear topological architectures (Yang et al., [2020](#bib.bib230)).
    The linear pattern of architecture consists of sequential basic units, and the
    non-linear pattern allows for skip or loop connections in the architecture (Liu
    et al., [2021b](#bib.bib117)).'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear topological architecture: The linear topology widely appears in the
    construction of layer-wise and block-wise search spaces. Due to the simplicity
    of linear topology, basic units can be stacked one by one by a linear piecing
    method. In this way, the skeleton of an architecture can be built up effectively
    (Sun et al., [2019b](#bib.bib189); Chen et al., [2020](#bib.bib27)) regardless
    of the complexity of the internal of basic units.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Non-linear topological architecture: Compared to the linear architecture, the
    non-linear topological architecture receives much more attention due to its flexibility
    to construct well-performing architectures (Wang et al., [2021c](#bib.bib209),
    [2020b](#bib.bib207); Xie and Yuille, [2017](#bib.bib222)), such as macro structures
    composed of basic units, and micro structures within basic units. There are two
    typical encoding approaches for non-linear topological architectures. The one
    is to use adjacent matrix to represent the connections in non-linear architectures,
    where “1” of the matrix denotes the existence of the connection between two units
    and “0” goes the opposite. In (Lorenzo and Nalepa, [2018](#bib.bib123)), skip
    connections are represented by a matrix where constraints can be set in place
    to guarantee valid encoding and avoid recurrent edges while performing skip connections.
    Note that adjacent matrix has a limitation that the number of basic units needs
    to be fixed in advance (Kitano, [1990](#bib.bib97)). Another one is to utilize
    an ordered pair to represent a directed acyclic graph, and then encode the connections
    between unites. The ordered pair can be formulated as $G$ = ($V$, $E$) where $V$
    is a set of vertices and $E$ is a directed edge in the acyclic graph, and it has
    been applied in (Irwin-Harris et al., [2019](#bib.bib84)) to encode the connections.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, the commonly used EC-based search paradigms for NAS are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: Many basic EC algorithms have been widely applied
    in existing NAS methods, such as GA (Kitano, [1990](#bib.bib97)) and PSO (Sun
    et al., [2019d](#bib.bib191)). A general framework of EC is presented in Fig.
    [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary Computation ‣ 2\. An Overview of Evolutionary
    Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Incremental search paradigm: A model architecture can be built in an incremental
    way where model elements (e.g., layers and connections) are gradually added to
    the model during the evolutionary process (Liu et al., [2018c](#bib.bib111); Wang
    et al., [2020c](#bib.bib208); Shen et al., [2019](#bib.bib179)). This way allows
    to find parts of architecture at different optimization stages, which reduces
    the computational burden on acquiring a complete model at once (Liu et al., [2018c](#bib.bib111)).
    For example, Wang et al. (Wang et al., [2020c](#bib.bib208)) used an incremental
    approach to stack blocks for building architectures, which improved the capacity
    of the final architecture via a progressive process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-evolution search paradigm: An architecture optimization problem is decomposed
    into the optimizations of a blueprint and its components (O’Neill et al., [2018](#bib.bib150);
    Zhang et al., [2022b](#bib.bib241)). Specifically, the blueprint plays a role
    in specifying the topological connection patterns of its components, and an optimal
    architecture is acquired by cooperatively optimizing the blueprint and its components.
    For example, O’Neill et al. (O’Neill et al., [2018](#bib.bib150)) proposed a co-evolution
    search paradigm for NAS, where the candidate blueprints and components were sampled
    from two populations, and then combined to form new architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective search paradigm: This paradigm targets at searching for a set
    of Pareto optimal architectures based on multiple criteria, and finding the final
    solutions according to some practical considerations, such as computational environment
    (Neshat et al., [2020](#bib.bib144); Lu et al., [2019](#bib.bib126)). This paradigm
    becomes popular in practical applications, since many objectives are required
    to be considered such as the accuracy, inference time, model size, and energy
    consumption. In (Neshat et al., [2020](#bib.bib144)), NSGA-II and RL were used
    to explore model architectures with respect to the model accuracy, and model complexity
    (e.g., the number of model parameters and multiply-adds operators).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4\. Acceleration Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'NAS is a high computational overhead task, mainly due to the large search space
    and highly time-consuming evaluation (Yao et al., [2018](#bib.bib231)). To overcome
    this challenge, various acceleration strategies (Sun et al., [2019d](#bib.bib191);
    Assunção et al., [2019b](#bib.bib13)) have been developed to accelerate the optimization.
    In this section, we summarize the speed-up strategies from the aspects of algorithm
    design to the hardware implementation, as shown in Fig. [6](#S4.F6 "Figure 6 ‣
    4.2.4\. Acceleration Strategies ‣ 4.2\. Model Architecture Optimization ‣ 4\.
    Model Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f173dbb8c1a13e886eadc7aa6aba2e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Overview of acceleration strategies.
  prefs: []
  type: TYPE_NORMAL
- en: From the algorithm design point of view, we summarized a number of acceleration
    strategies from population initialization to evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initialization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reduced population: The simplest way of acceleration during the initialization
    stage is to set the population with a small size. In other words, less evaluations
    are required with a smaller size of population since the evaluation of a candidate
    architecture is time-consuming (Yao et al., [2018](#bib.bib231)). As a result,
    some studies (Assunção et al., [2019b](#bib.bib13), [2018](#bib.bib12)) use small
    population with fixed size to speed up their evolution, like CARS (size = 32)
    (Yang et al., [2020](#bib.bib230)). In contrast, some other studies use dynamic
    sizes of populations during the optimization. In (Fan et al., [2020](#bib.bib56)),
    the population size is dynamically changed to reach a balance between algorithmic
    efficiency and population diversity.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficient search space: Another way is to design efficient search space to
    speed up the search process. For example, an architecture constructed on the basis
    of cell-wise search space (Liu et al., [2021b](#bib.bib117)) is composed of many
    similar structures of cells and only representative cells need to be optimized,
    which contributes to significant computational speed-up.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Early stopping policy: A relatively small number of training epochs are used
    to reduce the training cost (i.e., early stopping policy) since the training time
    is reduced (Ahmed et al., [2019](#bib.bib3); Tian et al., [2019](#bib.bib199);
    Sun et al., [2019d](#bib.bib191)).'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduced training set: Some methods are designed to reduce the size of the training
    set to improve training efficiency at the expense of a little accuracy (Sapra
    and Pimentel, [2020](#bib.bib173); Liu et al., [2019a](#bib.bib114)). Besides,
    low-resolution data (e.g., ImageNet 32) (Chrabaszcz et al., [2017](#bib.bib33))
    is also commonly used as the training set to accelerate the search process for
    the optimal architecture.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Weight inheritance ¹¹1In (Rumelhart et al., [1985](#bib.bib168), [1986](#bib.bib169);
    LeCun et al., [1989](#bib.bib106)), Rumelhart/Hinton/LeCun used the term ”weight
    sharing” to mean that different network connections/links share the same set of
    weights, and pointed out that ”weight sharing” is the core of shared weight NNs/CNNs.
    More recent (Lu et al., [2021](#bib.bib125); Xie et al., [2022a](#bib.bib221))
    use of this term refers to ”weight/parameter replications” or ”weight inheritance”.:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supernet-based inheritance: uses an over-parameterized and pre-trained supernet
    to encode all candidate architectures (i.e., subnets). In other words, the subnets
    share weights of the identical structures from the supernet, and they are directly
    evaluated on the validation dataset to obtain their model accuracy (Rapaport et al.,
    [2019](#bib.bib159); Dahal and Zhan, [2020](#bib.bib38); Elsken et al., [2019](#bib.bib50);
    Sapra and Pimentel, [2020](#bib.bib173)).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parent-based inheritance: inherits weights from previously-trained networks
    (i.e., parental networks) instead of a supernet, since offspring individuals retain
    some identical parts of their parental architectures (Real et al., [2017](#bib.bib163);
    Elsken et al., [2017](#bib.bib49); Kwasigroch et al., [2019](#bib.bib104); Zhu
    et al., [2019](#bib.bib262)) (Real et al., [2017](#bib.bib163)). As a result,
    offspring architectures can inherit the weights of the identical parts and no
    longer need to be trained from scratch.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Surrogate model: Since the evaluation of an architecture is time-consuming
    (Yao et al., [2018](#bib.bib231); Liu et al., [2021b](#bib.bib117)), cheap surrogate
    models have been introduced in NAS as performance predictors to reduce the computational
    time (Lu et al., [2021](#bib.bib125)).'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Online performance predictors: They are trained online on the datasets sampled
    from past several epochs (Liu et al., [2021b](#bib.bib117)), including a sequence
    of data pairs with different training epochs and their corresponding performance
    of these epochs (Lu et al., [2021](#bib.bib125)). After that, they will be used
    for the performance prediction on new architectures. To reduce the true evaluations
    of architectures, some performance predictors directly predict whether a candidate
    architecture can be survived into next iteration through a trained ranking or
    classification method, such as classification-wise NAS (Ma et al., [2021a](#bib.bib130)).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Offline performance predictors: They are essentially a sort of regression models
    mapping the architectures to specific performance. End-to-end predictors can be
    trained in an offline manner, so that they are able to predict the performance
    of architectures during the entire search process. Consequently, they can significantly
    reduce the computational burden (Sun et al., [2019a](#bib.bib188); Liu et al.,
    [2018c](#bib.bib111); Sun et al., [2021](#bib.bib187)).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Population memory: Population memory is used to store elite individuals from
    different generations during the optimization (Sun et al., [2020](#bib.bib192);
    Fujino et al., [2017](#bib.bib62)). When a new individual is generated, it does
    not need to be evaluated again if it is the same as an individual in the memory.
    In other words, the performance of individuals sharing the same architectures
    are the same and can be acquired via the population memory instead of training
    from scratch. This mechanism relies on the fact that similar or same individuals
    may repeatedly appear in different generations.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: According to the above introduction, we can conclude that many of them improve
    the search efficiency at the expense of sub-optimiality. For example, a small
    population cannot well cover a multi-objective optimal front. Parameter sharing
    may lead to the biased search due to much similarities among the individuals.
    Highly accurate surrogates need a large number of training data, which are commonly
    time-consuming. Population memory heavily relies on the random emergence of similar
    or same individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hardware implementation: Importantly, a powerful hardware platform can significantly
    speed up the search process under the reasonable utilization of computing resources
    (e.g., cloud computing (Chiba et al., [2019](#bib.bib31)) and volunteer computers
    (Atre et al., [2021](#bib.bib14))). Parallel computation is a powerful tool to
    decompose large search problems into small sub-problems, which can be simultaneously
    optimized by several cheaper hardware(Jin et al., [2019](#bib.bib88), [2018](#bib.bib87)).
    For example, Lorenzo et al. (Lorenzo et al., [2017](#bib.bib124)) proposed a parallel
    PSO algorithm to search for optimal architecture of CNN. The security of the computing
    device also becomes an important consideration. For this reason, an emerging decentralized
    privacy-preserving framework is applied to NAS, which unites multiple local clients
    to collaboratively learn a shared global model trained on the parameters or gradients
    of the local models, instead of the raw data. For example, Zhu et al. (Zhu and
    Jin, [2022](#bib.bib263)) firstly proposed a real-time federated NAS that can
    not only optimize the model architecture but also reduce the computational payload.
    Specifically, the decentralized system is able to accelerate the algorithm efficiency
    of federated NAS. Besides, data encryption is employed on the transmitted data
    (parameters or gradients of the local models) between the clients and the server
    to ensure the privacy even though all of the training are performed in local.
    Accordingly, federated NAS is highly efficient and secure, which may become a
    new hot research topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. Different acceleration strategies
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm design | Initialization | Reduced population | (Assunção et al.,
    [2019b](#bib.bib13)),(Fan et al., [2020](#bib.bib56)),(Liu et al., [2019a](#bib.bib114)),(Yang
    et al., [2020](#bib.bib230))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation | Early stopping policy | (Sun et al., [2019d](#bib.bib191)),(Ahmed
    et al., [2019](#bib.bib3)),(Ortego et al., [2020](#bib.bib152)),(Frachon et al.,
    [2019](#bib.bib60)),(Assunção et al., [2019a](#bib.bib11)),(Assunção et al., [2019b](#bib.bib13)),(Mo
    et al., [2021](#bib.bib139))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced training set | (Sapra and Pimentel, [2020](#bib.bib173)),(Liu et al.,
    [2019a](#bib.bib114)),(Wang et al., [2020c](#bib.bib208))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Weight inheritance | Supernet-based sharing | (Sapra and Pimentel, [2020](#bib.bib173)),(Rapaport
    et al., [2019](#bib.bib159)),(Dahal and Zhan, [2020](#bib.bib38)),(Elsken et al.,
    [2019](#bib.bib50))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Parent-based sharing | (Elsken et al., [2017](#bib.bib49)),(Kwasigroch et al.,
    [2019](#bib.bib104)),(Zhu et al., [2019](#bib.bib262)),(Ahmed et al., [2019](#bib.bib3)),(Frachon
    et al., [2019](#bib.bib60)),(Schorn et al., [2020](#bib.bib174)),(Chen et al.,
    [2019b](#bib.bib28))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Surrogate model | Online performance predictors | (Lu et al., [2021](#bib.bib125)),(Liu
    et al., [2018c](#bib.bib111)), (Ma et al., [2021a](#bib.bib130)), (Lomurno et al.,
    [2021](#bib.bib119))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Offline performance predictors | (Sun et al., [2019a](#bib.bib188)),(Liu
    et al., [2021a](#bib.bib112)), (Rawal and Miikkulainen, [2018](#bib.bib161))\bigstrut
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Population memory | (Sun et al., [2020](#bib.bib192)),(Miahi et al., [2022](#bib.bib136)),(Li
    et al., [2022](#bib.bib109)),(Chu et al., [2020](#bib.bib34))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Hardware implementation | (Chiba et al., [2019](#bib.bib31)),(Jin et al.,
    [2019](#bib.bib88)),(Jin et al., [2018](#bib.bib87)),(Zhu and Jin, [2022](#bib.bib263))  \bigstrut
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2.4\. Acceleration Strategies ‣ 4.2\. Model Architecture
    Optimization ‣ 4\. Model Generation ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") lists the common acceleration strategies
    to improve the algorithm efficiency. It is noted that multiple strategies can
    be utilized together to improve computational speed-up. For example, Lu et al.
    (Lu et al., [2021](#bib.bib125)) employed supernet and learning curve performance
    predictor in NAS, while Liu et al. (Liu et al., [2019a](#bib.bib114)) leveraged
    a small populations size and a small dataset to reduce the time overhead of evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most NAS methods are based on basic EC search paradigms on an entire-structured
    search space, which are introduced above. However, there are also some other automatic
    search techniques such as RL-based (Jaâfra et al., [2019](#bib.bib86)), Bayesian-based
    (White et al., [2021](#bib.bib215)), and gradient-based (Liu et al., [2018b](#bib.bib113))
    methods, for architecture search.
  prefs: []
  type: TYPE_NORMAL
- en: RL-based methods can be regarded as an incremental search, where a policy function
    is learned by using a reward-prediction error to drive the generation of incremental
    architecture. Due to the large-scale of state space and action space, RL-based
    methods require immense computational resources. In addition, there are a large
    number of hyper-parameters (e.g., discount factor) in RL-based NAS. Besides, they
    transform a multi-objective optimization problem into a single-objective problem
    via a priori or expert knowledge, so they are unable to find a Pareto optimal
    set to the target tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian-based methods are a common tool for hyperparametric optimization problems
    with low dimensions. In comparison to EC-based methods, they are much more efficient
    on the condition that a proper distance function has to be designed to evaluate
    the similarities between two subnets. However, the computational cost of Gaussian
    process grows exponentially and its accuracy decreases, when the dimensionality
    of the problem increases.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based methods, taking a NAS problem as a continuous differentiable
    problem instead of a discrete one, are able to efficiently search architectures
    with proper weight parameters. Unfortunately, their GPU costs are usually very
    high due to a large number of parameters to be updated in gradient-based algorithms
    (Liu et al., [2018b](#bib.bib113)).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, EC-based methods benefit from less hyperparameters to be optimized
    and no distance functions to be designed. In addition, EC-based methods can be
    applied to NAS with multiple objectives and constraints. Although there many acceleration
    strategies in EC-based methods, they still suffer from high computational overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Joint Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.3.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The independent optimization of architecture or parameters is difficult to
    achieve the optimal model on give tasks. Hence, joint optimization methods have
    been developed to search for the optimal configuration of architecture ($A^{*}$),
    and parameters ($W^{*}$, associated weights). The optimization problem can be
    defined in Eq. [4](#S4.E4 "In 4.3.1\. Problem Formulation ‣ 4.3\. Joint Optimization
    ‣ 4\. Model Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\begin{matrix}\left({{W}^{*}},{{A}^{*}}\right)$=$\underset{W,A}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $L$ is the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In the followings, we will introduce the joint optimization regarding the solution
    representations and search paradigms, and then discuss the pros and cons of EC-based
    methods in comparison to others.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are three typical classes of encoding schemes used for joint optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear encoding: This is a simple but effective encoding strategy, which has
    been widely used in many studies to build architecture with high performance (Aljarah
    et al., [2018](#bib.bib9); Maniezzo, [1994](#bib.bib132)). In (Maniezzo, [1994](#bib.bib132)),
    a variable-length binary vector was used to represent weights and structure of
    neural networks, where the weights utilize direct encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree-based encoding: In this encoding, the topology and weights of an architecture
    can be represented by a tree structure with a number of nodes and edges (Zhang
    and Mühlenbein, [1995](#bib.bib240); Golubski and Feuring, [1999](#bib.bib65)).
    In (Zhang et al., [2020b](#bib.bib242)), the mechanism of Reverse Encoding Tree
    (RET) was developed to ensure the robustness of a deep model, where the topological
    information of an architecture was represented by a combination of nodes and the
    weight information was recorded on the edges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph-based encoding: In this encoding, the nodes of a graph represent neurons
    or other network units, and the edges are used to record the weight information
    (Han and Cho, [2006](#bib.bib70); Chai et al., [2022](#bib.bib21)). For example,
    a graph incidence matrix was developed in (Oong and Isa, [2011](#bib.bib151))
    to encode a neural network. The size of the matrix was set to ($N_{i}$ + $N_{h}$
    + $N_{o}$) $\times$ ($N_{i}$ + $N_{h}$ + $N_{o}$), where $N_{i}$, $N_{h}$ and
    $N_{o}$ indicate the numbers of input, hidden, and output nodes, respectively.
    In the graph incidence matrix, real numbers represented the weight and biases,
    and “0” meant that there was no connection between two nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are a number of effective search paradigms for joint optimization, and
    the EC-based search paradigms are in the spotlight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: Some basic EC search methods have been employed to
    handle joint optimization problems (Fahrudin et al., [2016](#bib.bib55); Oong
    and Isa, [2011](#bib.bib151); Stanley and Miikkulainen, [2002](#bib.bib186); Barman
    and Kwon, [2020](#bib.bib15); Behjat and Chidambaran, [2019](#bib.bib16)). In
    (Oong and Isa, [2011](#bib.bib151)), an architecture and its corresponding weights
    were simultaneously optimized by an EC-based method using linear and graph encodings.
    In neuro-evolution of augmenting topologies (NEAT) (Stanley and Miikkulainen,
    [2002](#bib.bib186)), the architecture of a small network is evolved by an incremental
    mechanism, while the weights are optimized by an EC-based method. NEAT is able
    to ensure the lowest dimensional search space over all generations. Some representative
    studies on NEAT are presented in (Barman and Kwon, [2020](#bib.bib15); Behjat
    and Chidambaran, [2019](#bib.bib16)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective search paradigm: Multi-objective optimization on model design
    has been developed in many studies (e.g., artificial neural network (Rostami and
    Neri, [2016](#bib.bib167)) and recurrent neural network (Smith and Jin, [2014](#bib.bib182))).
    For example, Smith et al. (Smith and Jin, [2014](#bib.bib182)) built a bi-objective
    optimization (i.e., the minimizations of the mean squared error (MSE) on a training
    dataset and the number of connections in the network) to search for optimal weights
    and connections of network architectures. The chromosome of an individual was
    composed of two parts, where the one with Boolean type represented the structure
    of a network, and the other with real values represented the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Direct encoding is used to be prevalent in the joint optimization of small-scale
    neural networks (Oong and Isa, [2011](#bib.bib151); Yao and Liu, [1996](#bib.bib233)).
    However, with the increase of the scale of neural networks, direct coding of high-dimensional
    vector or matrix of weights is not realistic. Therefore, recent studies are more
    on indirect encoding. For example, a complex mapping with acceptable accuracy
    loss is designed in (Koutník et al., [2014](#bib.bib100); D’Ambrosio and Stanley,
    [2007](#bib.bib40)) to construct weight vectors with arbitrary size.
  prefs: []
  type: TYPE_NORMAL
- en: EC-based approaches with capability of searching the optimal solution have been
    developed to configure a DL model for the specific task. However, they often encounter
    a prohibitive computational cost, which is even higher than that of model architecture
    optimization. Hence, designing efficient EC-based approaches for architecture
    and parameter search deserves much investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Model Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The large-scale DNNs are not straightforward to be deployed into devices (e.g.,
    smartphones) with limited computation and storage resources (e.g., battery capacity
    and memory size). To solve this issue, various model compression approaches have
    been proposed to reduce the model size and inference time, such as pruning, model
    distillation, and quantization (Choudhary et al., [2020](#bib.bib32)). However,
    they need much expert knowledge and a lot of efforts on the manual compression
    of neural network models. In contrast, EC-based approaches are automation approaches
    and has been recently introduced to achieve automated model compression. We have
    observed that most of them concentrate on the area of model pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Model Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1\. Problem Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DNN is commonly an over-parameterized model, which has redundant and non-informative
    components (e.g., weights, channels and filters). To address this issue, researchers
    have designed various pruning approaches (e.g., channel pruning (He et al., [2017](#bib.bib73)))
    to obtain a lightweight deep network model with high accuracy. Model pruning can
    be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  |  | $\displaystyle Los{{s}_{A_{s}^{*}}}\approx Los{{s}_{A}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\begin{matrix}\text{s}\text{.t}\text{.}&amp;{A}_{{s}}^{*}$=$\underset{{C}}{\mathop{\text{pruning}}}\,\left({A,C}\right)\\
    \end{matrix}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $C$ represents redundant and non-informative units, $A$ and $A{{}_{s}}^{*}$
    represent original model and lightweight model, respectively, $Los{{s}_{A_{s}^{*}}}$
    and $Los{{s}_{A}}$ represent loss of $A{{}_{s}}^{*}$ and $A$.
  prefs: []
  type: TYPE_NORMAL
- en: This study aims to introduce EC-based methods for model pruning, and readers
    interested in traditional pruning methods such as weight-based pruning, neuron-based
    pruning, filter-based pruning, layer-based pruning, and channel-based pruning
    may refer to the surveys (Choudhary et al., [2020](#bib.bib32)) to get more details.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Solution Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For model pruning, binary encoding is one of the most popular approaches among
    these solution representations, where each element corresponds to the network
    component (e.g., channel). In (Wang et al., [2018](#bib.bib212)), the network
    pruning task was formulated as a binary programming problem, where a binary variable
    was directly associated with each convolution filter to determine whether or not
    the filter took effect. Although binary representation is straightforward, the
    length of the representation becomes large when the model complexity (i.e., the
    number of units) improves, and the overhead of exploration will also increase.
  prefs: []
  type: TYPE_NORMAL
- en: To address the above issue, some efficient solution representations (i.e., indirect
    encoding) have been developed. For example, Liu et al. (Liu and Guo, [2021](#bib.bib115))
    used $N$ digits to record the number of compressed layers. The first digit represented
    the number of compressed layers, and following digits recorded the selected compression
    operator index of each layer. This way can significantly improve the search efficiency.
    In (Liu et al., [2019b](#bib.bib118)), encoding vectors are used to represent
    the number of channels in each layer for original networks. Then a meta-network
    is constructed to generate the weights according to network encoding vectors.
    By stochastically fed with different structure encoding, the meta-network gradually
    learns to generate weights for various pruned structures.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Search Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The search paradigms in model pruning studies can be categorized into two main
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic EC search paradigm: A number of studies introduce single-objective EC
    search paradigm for model pruning (Wu et al., [2021a](#bib.bib218); Tang et al.,
    [2019](#bib.bib195)). For example, Wu et al. (Wu et al., [2021a](#bib.bib218))
    first analysed the pruning sensitivity on weights via differential evolution (DE),
    and then the model was compressed by iteratively performing the weight pruning
    process according to the weight sensitivity. In addition, this method adopted
    a recovery strategy to increase the pruned model performance during the fine-tuning
    phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-objective search paradigm: Recently, this sort of search paradigm has
    been adopted to model pruning, which is able to provide users with a set of Pareto
    lightweight models. For example, Zhou et al. (Zhou et al., [2020](#bib.bib259))
    considered two objectives (i.e., minimizing convolutional filters and maximizing
    the model performance) for biomedical image segmentation. During the model pruning,
    a classical multi-objective optimization algorithm (NSGA-II (Deb et al., [2002](#bib.bib42)))
    was used find the optimal set of non-dominated solutions, where the optimization
    was based on a binary string encoding (each bit represents a filter).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.1.3\. Search Paradigms ‣ 5.1\. Model Pruning
    ‣ 5\. Model Deployment ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), we have summarized these two categories of search
    paradigms as well as their corresponding ways of encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3. Different search paradigms and solutions representations for model
    pruning
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Direct encoding | Indirect encoding \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Basic EC search paradigm | (Samala et al., [2018](#bib.bib171)),(Junior and
    Yen, [2021a](#bib.bib91)),(Tang et al., [2019](#bib.bib195)),(Wu et al., [2021a](#bib.bib218)),(Junior
    and Yen, [2021b](#bib.bib92)),(Zhou et al., [2021b](#bib.bib260)),(Gerum et al.,
    [2020](#bib.bib64)),(Zemouri et al., [2019](#bib.bib238)),(Chen et al., [2019a](#bib.bib26)),(Poyatos
    et al., [2022](#bib.bib157)),(Fernandes and Yen, [2021](#bib.bib57)),(Li et al.,
    [2020](#bib.bib107)),(Shang et al., [2022](#bib.bib178)) | (Liu et al., [2019b](#bib.bib118))  \bigstrut
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-objective search paradigm | (Zhou et al., [2020](#bib.bib259)),(Zhou
    et al., [2021c](#bib.bib261)),(Wu et al., [2019](#bib.bib219)),(Hong et al., [2020](#bib.bib76)),(Xu
    et al., [2021](#bib.bib224)),(Yang et al., [2019](#bib.bib228)),(Zhang et al.,
    [2021a](#bib.bib245)) | (Wang et al., [2021a](#bib.bib213)),(Zhang et al., [2021b](#bib.bib254)),(Loni
    et al., [2020](#bib.bib122))  \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: 5.2\. Other EC-based Model Deployment Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from model pruning, there are several other EC-based model compression
    methods for model deployment. In the followings, some typical methods are introduced,
    including knowledge distillation, low-rank factorization, and EC for hybrid techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Knowledge Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Knowledge distillation (KD) (Gou et al., [2021](#bib.bib66)) aims to get a small
    light network but with good generalization capability. The basic idea is to transfer
    the knowledge learned from a big cumbersome network (or teacher network) with
    good generalization ability to a small but light network (or student network).
  prefs: []
  type: TYPE_NORMAL
- en: However, knowledge distillation may be seriously influenced when there is a
    big gap in the learning capability between the teacher and student networks. In
    other words, if the difference is large, the student network may not be able to
    learn knowledge from the teacher network. Recently, several EC-based approaches
    have been proposed to mitigate the above issue of knowledge distillation. For
    example, Wu et al. (Wu et al., [2020](#bib.bib220)) proposed an evolutionary embedding
    learning (EEL) paradigm to learn a fast accurate student network via massive knowledge
    distillation. Their experimental results show that the EEL is able to narrow the
    performance between the teacher and student networks on given tasks. Zhang et
    al. (Zhang et al., [2022a](#bib.bib246)) developed an evolutionary knowledge distillation
    method to improve the effectiveness of knowledge transfer. In this method, an
    evolutionary teacher was learned online and consistently transfers intermediate
    knowledge to the student network to narrow the gap of the learning capability
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Low-rank Factorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DNNs often involve in a huge number of weights, which may impact the inference
    speed and seriously increase the storage overhead of the DNN. The weights can
    be viewed as a matrix $W$ with $m$ $\times$ $n$ dimensions. The low-rank approach
    is commonly applied to the weight matrix ($W$) after the DNN is fully trained.
    For example, singular value decomposition (Fortuna and Frasca, [2021](#bib.bib59))
    is a typical low-rank factorization method, where $W$ is decomposed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle\begin{matrix}W$=$USV^{T}\end{matrix}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $U$ $\in$ $R^{m\times m}$, $V^{T}$ $\in$ $R^{n\times n}$ are orthogonal
    matrices and $S$ $\in$ $R^{m\times n}$ is a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, most of the existing low-rank factorization methods rely on domain
    expertise and experience for the selection of hyperparameters (e.g., the rank
    and sparsity of weight matrix) to get an appropriate compression results without
    serious performance degradation (Swaminathan et al., [2020](#bib.bib193); Winata
    et al., [2019](#bib.bib216); Hsu et al., [2021](#bib.bib78)).
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, EC-based methods have been introduced to solve the above challenge
    (Wang et al., [2018](#bib.bib212); Huang et al., [2020](#bib.bib82)). For example,
    Huang et al. (Huang et al., [2020](#bib.bib82)) presented a multi-objective evolution
    approach to automatically optimize rank and sparsity for weight matrix without
    human intervention, where two objectives were taken into account including the
    minimization of the model classification error rate and maximization of the model
    compression rate. They therefore generated a set of approximately compressed models
    with different compression rates to mitigate the expensive training process.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. EC for Jointly Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many compression techniques (e.g., quantization) can be easily applied on top
    of other techniques (e.g., pruning and low-rank factorization). For example, pruning
    first and then quantification can obtain a lightweight model with faster inference.
    Similarly, EC can optimize more than one model compression method at the same
    time. In the followings, we will briefly review such works.
  prefs: []
  type: TYPE_NORMAL
- en: Phan et al. (Phan et al., [2020](#bib.bib155)) designed an efficient 1-Bit CNNs,
    which combined quantization with a compact model. Specifically, they firstly created
    a number of strong baseline binary networks (BNNs), which had abundant random
    group combinations at each convolutional layer. Then, they adopted evolutionary
    search to seek an optimal group convolution combination with accuracy above threshold.
    Finally, the obtained binary models werr trained from scratch to achieve the final
    lightweight network. Different from (Phan et al., [2020](#bib.bib155)), Polino
    et al. (Polino et al., [2018](#bib.bib156)) jointly utilized weight quantization
    and distillation to compress large networks (i.e., teacher network) into small
    networks (i.e., student network), where the latency and model error were regarded
    as the objectives during the optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Zhou et al. (Zhou et al., [2021b](#bib.bib260)) developed an evolutionary
    algorithm-based method for shallowing DNNs at block levels (ESNB). In ESNB, a
    prior knowledge was extracted from the original model to guide the population
    initialization. Then, an evolutionary multi-objective optimization mothed was
    performed to minimize the number of blocks and the accuracy drop (i.e., loss).
    After that, knowledge distillation was employed to compensate for the performance
    degradation via matching output of the pruned model with the softened and hardened
    output of the original model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is still a big room for the improvement on addressing the huge computational
    overhead of evolutionary model deployment. Acceleration strategies may be able
    to alleviate the issue. Besides, there is a high coupling between model deployment
    and model generation since the performance of the compressed network is strongly
    dependent on the performance of the original network. The black-box nature of
    model also hampers deployment in security-critical tasks (e.g., medicine and finance).Consequently,
    it is promising and challenging to take the model compression, NAS, and interpretability
    as a single optimization problem and handle it with acceptable time consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Applications, Open Issues, and Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EDL algorithms have been widely used in various real-world applications. In
    practical, great development has been achieved in computer vision (CV), natural
    language processing (NLP) and other practical applications (e.g., crisis prediction
    and disease prediction).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1\. Computer Vision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CV is an important domain of computer science, playing an important role in
    identifying useful information (e.g., objects and classifications) for specific
    tasks (e.g., image segmentation (Zhou et al., [2020](#bib.bib259)) and object
    detection (Zhang and Rockett, [2005](#bib.bib252))) on images or videos. In the
    early days, manually designed models for computer vision achieved good performance
    on public datasets at the expense of extensive time and labour. With the development
    of EDL, many new structures have been developed by computer programming and they
    show better performance than these manually designed models, especially on the
    widely used benchmark datasets for image classification, such as CIFAR-10, CIFAR-100
    (Krizhevsky et al., [2009](#bib.bib102)), and ImageNet (Deng et al., [2009](#bib.bib44)).
    For example, the state-of-the-art NAT-M4 (Lu et al., [2021](#bib.bib125)) with
    a small model size achieves Top-1 accuracy of 80.5% on ImageNet. Image-to-image
    processing (Liu et al., [2021b](#bib.bib117)) (e.g., super-resolution, image inpainting,
    and image restoration) also received extensive attention from researchers (Rundo
    et al., [2019](#bib.bib170); Song et al., [2020](#bib.bib184); Zhan et al., [2021](#bib.bib239)).
    Ho et al. (Ho et al., [2021](#bib.bib74)) employed NAS techniques to improve image
    denoising, inpainting, and super-resolution on the foundation of deep image prior
    (Ho et al., [2021](#bib.bib74)). In addition to the above applications, EDL also
    has great potential in other areas of CV, such as object detection (Demirkir and
    Sankur, [2006](#bib.bib43)), video/picture understanding (Maniezzo, [1994](#bib.bib132)),
    and image segmentation (Fan et al., [2020](#bib.bib56)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. Natural Language Processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Natural language processing (NLP) driven by computer science and computational
    linguistics, aims to understand, analyze, and extract knowledge on text and speech
    recoginition (Young et al., [2018](#bib.bib235)). Many effective NLP models (e.g.,
    GPT-2 (Radford et al., [2018](#bib.bib158)) and BERT (Devlin et al., [2018](#bib.bib45)))
    narrow the chasm between human communication and computer understanding using
    sophisticated mechanisms. Recently, EC-inspired NLP models have been proposed
    such as language model (Murray and Chiang, [2015](#bib.bib142)), entity recognition
    (Sikdar et al., [2012](#bib.bib181)), text classification (Tanaka et al., [2016](#bib.bib194);
    Andersen et al., [2021](#bib.bib10); Londt et al., [2021](#bib.bib120), [2020](#bib.bib121)),
    and keyword spotting (Mazzawi et al., [2019](#bib.bib134)). Satapathy et al. (Song,
    [2021](#bib.bib185)) introduced evolutionary multi-objective (i.e., inference
    time and accuracy) optimization in an English translation system. Sikdar et al.
    (Sikdar et al., [2012](#bib.bib181)) employed DE in feature selection for named
    entity recognition (NER).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3\. Other Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to CV and NLP, EDL also shows strong ability on handling other practical
    applications, such as medical analysis (Liu et al., [2018a](#bib.bib116); Zhu
    et al., [2007](#bib.bib264)), financial prediction (Tang et al., [2019](#bib.bib195)),
    signal processing (Erguzel et al., [2014](#bib.bib51); Huang et al., [2012](#bib.bib81)),
    and industrial prediction (Mei et al., [2017](#bib.bib135); Ma et al., [2021b](#bib.bib128)).
    In particular, Zhu et al. (Zhu et al., [2007](#bib.bib264)) presented a Markov
    blanket-embedded genetic algorithm for feature selection to improve gene selection.
    In (Tang et al., [2019](#bib.bib195)), financial bankruptcy analysis was handled
    by an evolutionary pruning neural network. The work in (Erguzel et al., [2014](#bib.bib51))
    designed a feature selection method based on ACO to classify electromyography
    signals. For remote sensing imagery, a suitable model was found by multi-objective
    neural evolution architecture search (Ma et al., [2021b](#bib.bib128)), where
    architecture complexity and performance error of searched network were two conflicting
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Open Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EDL is a hot research topic in both fields of machine learning and evolutionary
    computation. There are a large number of publications on various top conferences
    and journals, such as ICCV, CVPR, GECCO, TPAMI, TEVC, TCYB, and TNNLS (see the
    reference list). Yet some challenges remain to be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Acceleration strategies: Many EDL approaches suffer from low efficiency due
    to the expensive evaluations. So various acceleration strategies, such as surrogate
    model (Sun et al., [2019a](#bib.bib188)), supernet (Guo et al., [2020](#bib.bib67)),
    and early stop (Sun et al., [2019d](#bib.bib191)) have been designed. However,
    the improvements of the accuracy are at the expense of sacrifice a bit of model
    accuracy. Taking the supernet-based inheritance as an example (Xie et al., [2022a](#bib.bib221)),
    we cannot guarantee that every subnet receives a reliable evaluation due to the
    catastrophic forgetting (Zhang et al., [2020a](#bib.bib249)) and weight coupling
    (Hu et al., [2021a](#bib.bib80)). Therefore, how to balance the efficiency and
    accuracy needs further investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effectiveness: There is a debate on whether EDL has many advantages over other
    search paradigms (e.g., random search and RL). Some studies argue that many popular
    search paradigms (e.g., EC-based methods and RL-based methods) have no big difference
    from the random search methods in their performance, and some random search methods
    even outperform EC-based methods in some scenarios (Sciuto et al., [2020](#bib.bib175)).
    On the contrary, EC-based approaches have also been proved to be more effective
    than random search methods in many studies (Real et al., [2019](#bib.bib162);
    Guo et al., [2020](#bib.bib67); Jones et al., [2019](#bib.bib90)). Thus, a unified
    platform is essential to measure the effectiveness of different search models,
    under the consistent search space and hyperparameters configuration (Xie et al.,
    [2022b](#bib.bib223)). In addition, elaborate experiments are required to justify
    the effects of different genetic operators (e.g., crossover operator) to the evolutionary
    process of EDL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large-scale datasets: There is an issue for the studies of EDL on large-scale
    datasets. It is noted that many studies of EDL are tested on small- and medium-scale
    datasets such as CIFAR-10 and CIFAR-100 (including 60000 32$\times$32 images),
    and especially the accuracy on CIFAR-10 reaches up to 98% (Lu et al., [2021](#bib.bib125)).
    Although large-scale datasets are ubiquitous and essential in various domains
    like gene analysis (Yu et al., [2009](#bib.bib236)) and ImageNet (Deng et al.,
    [2009](#bib.bib44)), computational costs are unaffordable for many researchers
    as pointed in some statistical reports (He et al., [2021](#bib.bib72); Liu et al.,
    [2021b](#bib.bib117)). Therefore, the sensitivity of the EDL methods to different
    scales of datasets is necessary (Zhou et al., [2021a](#bib.bib258)) and how to
    economically and efficiently verify EDL methods on large-scale datasets also deserves
    much investigations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'End-to-end EDL: Originally, AutoML aims to simultaneously optimize feature
    engineering, model generation and model deployment as a whole. However, there
    is a strong correlation between them where the performance of next phase heavily
    relies on the results of the previous phase (Liu and Guo, [2021](#bib.bib115)).
    As a result, most studies only focus on parts of the EDL pipeline (Fig. [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues")). For instance, TPOT (Olson and Moore,
    [2016](#bib.bib149)) is designed on top of Pytorch for building classification
    tasks, which however only supports a multi-layer perception machine (i.e., model
    generation). There are many partially accomplished end-to-end for EDL, such as
    ModelArts (model generation), Google’s Cloud (NAS), and Feature Labs (feature
    engineering) (Yao et al., [2018](#bib.bib231)), to name but a few. The main reason
    is that the optimization of the whole EDL pipeline may need huge computational
    cost not only on the exploration of the large-scale search space but also on handling
    highly-coupled relation between different parts of EDL. Consequently, finding
    an optimal solution of the complete EDL pipeline is essential but challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Challenges and Future Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although remarkable progress has been made in EDL, there are still many promising
    lines of research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fair comparisons: Unfair comparisons of different EDL methods are easily encountered
    with the following reasons. Firstly, uniform benchmarks are essential. In feature
    engineering, no uniform benchmark is for the fair comparison of different algorithms
    due to different downstream prediction models and feature sets. Secondly, there
    is no uniform criterion for different methods in handling NAS and model compression
    by using different tricks (e.g., cutout (DeVries and Taylor, [2017](#bib.bib46))
    and ScheduledDropPath (Zoph et al., [2018](#bib.bib265))), which may influence
    the performance of the final architecture. Thirdly, a fair platform for EDL is
    essential. There are some fair benchmarks but only for specific tasks, such as
    BenchENAS (Xie et al., [2022b](#bib.bib223)), NAS-Bench-101 (Ying et al., [2019](#bib.bib234)),
    NAS-Bench-201 (Dong and Yang, [2020](#bib.bib47)), NAS-Bench-301 (Siems et al.,
    [2020](#bib.bib180)), and HW-NAS-Bench (Li et al., [2021](#bib.bib108)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretability: EDL is known as a black-box optimization, and there is a
    lack of theoretical analysis to explain its superiority (Wang et al., [2021b](#bib.bib206)).
    For example, it is difficult to explain why EC-based method tends to select features
    contribute to the performance of the classification model in feature engineering.
    As a result, the development of EDL in some sensitive domains such as financial
    and medical fields is slow. To overcome this issue, Evans et al. (Evans et al.,
    [2018b](#bib.bib54)) used visualization to expound how the evolved convolution
    filter served and indirectly explained the search process of the model. Nevertheless,
    some studies argue that the explanation for these occurrences is usually post-hoc
    and lacks trustworthy mathematical deduction (Liu et al., [2021b](#bib.bib117);
    Al-Sahaf et al., [2019](#bib.bib6)). Thus, the interpretability of EDL is an interesting
    and promising research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring more scenarios: There is still plenty of room for the improvement
    of the performance of EDL on both benchmarks and real-world applications. Although
    EDL methods outperform manually designed models on various image benchmarks (CIFAR-10
    and ImageNet), the state-of-the-art EDL methods (Wang et al., [2020a](#bib.bib210))
    lost their advantages on NLP in comparison with human-designed models (e.g., GPT-2
    (Radford et al., [2018](#bib.bib158)), Transformer-XL (Dai et al., [2019](#bib.bib39))).
    In comparison with the benchmarks, it is more difficult to handle real-world tasks,
    which inevitably contain noise (e.g., mislabeling and inadequate or imbalance
    data) or may have small-scale datasets (leading to overfitting). Hence, some techniques
    such as unsupervised and self-supervised learning may be incorporated into EDL
    to mitigate these types of issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of machine leaning and evolutionary computation, many EDL
    approaches have been proposed to automatically optimize the parameters or architectures
    of deep models following the EC optimization framework. EDL approaches show competitive
    performance in robust and search capability, in comparison with the manually designed
    approaches. Therefore, EDL has become a hot research topic.
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we first introduced EDL from the perspective of DL and EC to
    facilitate the understanding of readers from the communities of ML and EC. Then
    we formulated EDL as a complex optimization problem, and provided a comprehensive
    survey of EC techniques in solving EDL optimization problems in terms of feature
    engineering, model generation to model deployment to form a new taxonomy (i.e.,
    what, where and how to evolve/optimize in EDL). Specifiically, we discussed the
    solution representations and search paradigms of EDL at different stages of its
    pipeline in detail. Then the pros and cons of EC-based approaches in comparison
    to non-EC based ones are discussed. Subsequently, various applications are summarized
    to show the potential ability of EDL in handling real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Although EDL approaches have achieved great progress in AutoML, there are still
    a number of challenging issues to be resolved. For example, effective acceleration
    strategies are essential to reduce the expensive optimization process. Another
    issue is to handle large-scale datasets and how to perform fair comparisons between
    different EDL approaches or non-EC based methods. More investigations are required
    to theoretically analyse or interpret the search ability of EDL. In addition,
    a lot of efforts are required on the improving the performance of EDL on both
    benchmarks (e.g., large-scale and small-scale data) and real-world applications.
    Lastly, the development of end-to-end EDL is challenging but deserves much efforts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdi and Williams (2010) Hervé Abdi and Lynne J Williams. 2010. Principal Component
    Analysis. *Comput. Stat.* 2, 4 (2010), 433–459.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed et al. (2019) Amr Ahmed, Saad Mohamed Darwish, and Mohamed M. El-Sherbiny.
    2019. A Novel Automatic CNN Architecture Design Approach Based on Genetic Algorithm.
    In *Int. Conf. Adv. Intell. Syst. Inform.* 473–482.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed et al. (2014) Soha Ahmed, Mengjie Zhang, Lifeng Peng, and Bing Xue. 2014.
    Multiple Feature Construction for Effective Biomarker Identification and Classification
    Using Genetic Programming. In *Proc. Genetic Evol. Comput. Conf.* 249–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-kazemi and Mohan (2002) Buthainah Al-kazemi and Chilukuri Krishna Mohan.
    2002. Training Feedforward Neural Networks using Nulti-phase Particle Swarm Optimization.
    In *Proc. Int. Conf. Neural Inf. Process.*, Vol. 5. 2615–2619.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Sahaf et al. (2019) Harith Al-Sahaf, Ying Bi, Qi Chen, Andrew Lensen, Yi
    Mei, Yanan Sun, Binh Tran, Bing Xue, and Mengjie Zhang. 2019. A Survey on Evolutionary
    Machine Learning. *J. R. Soc. N. Z.* 49, 2 (2019), 205–228.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Albukhanajer et al. (2015) Wissam A. Albukhanajer, Johann A. Briffa, and Yaochu
    Jin. 2015. Evolutionary Multiobjective Image Feature Extraction in the Presence
    of Noise. *IEEE Trans. Cybern.* 45, 9 (2015), 1757–1768.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alexandropoulos and Aridas (2019) Stamatios-Aggelos N Alexandropoulos and Christos K
    Aridas. 2019. Multi-objective Evolutionary Optimization Algorithms for Machine
    Learning: A Recent Survey. *Approximation and Optimization* (2019), 35–55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aljarah et al. (2018) Ibrahim Aljarah, Hossam Faris, and Seyed Mohammad Mirjalili.
    2018. Optimizing Connection Weights in Neural Networks Using the Whale Optimization
    Algorithm. *Soft Comput.* 22, 1 (2018), 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andersen et al. (2021) Hayden Andersen, Sean Stevenson, Tuan Ha, Xiaoying Gao,
    and Bing Xue. 2021. Evolving Neural Networks for Text Classification Using Genetic
    Algorithm-based Approaches. In *Proc. IEEE Congr. Evol. Comput.* 1241–1248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assunção et al. (2019a) Filipe Assunção, Joao Correia, and Rúben Conceição.
    2019a. Automatic Design of Artificial Neural Networks for Gamma-Ray Detection.
    *IEEE Access* 7 (2019), 110531–110540.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assunção et al. (2018) Filipe Assunção, Nuno Lourenço, P. Machado, and Bernardete
    Ribeiro. 2018. Evolving the Topology of Large Scale Deep Neural Networks. In *Proc.
    Eur. Conf. Genetic Program*. 19–34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assunção et al. (2019b) Filipe Assunção, Nuno Lourenço, Penousal Machado, and
    Bernardete Ribeiro. 2019b. Fast denser: Efficient Deep Neuroevolution. In *Proc.
    Eur. Conf. Genetic Program*. 197–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atre et al. (2021) Medha Atre, Birendra Jha, and Ashwini Rao. 2021. Distributed
    Deep Learning Using Volunteer Computing-Like Paradigm. In *Proc. Int. Parallel
    and Distrib. Process. Symp.* 933–942.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barman and Kwon (2020) Shohag Barman and Yung-Keun Kwon. 2020. A Neuro-Evolution
    Approach to Infer A Boolean Network From Time-Series Gene Expressions. *Bioinformatics*
    36, 2 (2020), i762–i769.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behjat and Chidambaran (2019) Amir Behjat and Sharat Chidambaran. 2019. Adaptive
    Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping
    in Autonomous Agents. In *Proc. Int. Conf. Robot. Autom.* 9638–9644.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhanu and Krawiec (2002) Bir Bhanu and Krzysztof Krawiec. 2002. Coevolutionary
    Construction of Features for Transformation of Representation in Machine Learning.
    In *Proc. Genetic Evol. Comput. Conf.* 249–254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2018) Ying Bi, Bing Xue, and Mengjie Zhang. 2018. An Automatic Feature
    Extraction Approach to Image Classification Using Genetic Programming. In *Proc.
    Int. Conf. Appl. Evol. Comput.* 421–438.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cano et al. (2017) Alberto Cano, Sebastián Ventura, and Krzysztof J. Cios. 2017.
    Multi-Objective Genetic Programming for Feature Extraction and Data Visualization.
    *Soft Comput.* 21, 8 (2017), 2069–2089.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castelli et al. (2011) Mauro Castelli, Luca Manzoni, and Leonardo Vanneschi.
    2011. Multi Objective Genetic Programming for Feature Construction in Classification
    Problems. In *Proc. Int. Conf. Learn. Intell. Optim.* 503–506.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chai et al. (2022) Zheng-Yi Chai, ChuanHua Yang, and Ya-Lun Li. 2022. Communication
    Efficiency Optimization in Federated Learning Based on Multi-Objective Evolutionary
    Algorithm. *Evol. Intell.* (2022), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chandra (2015) Rohitash Chandra. 2015. Competition and Collaboration in Cooperative
    Coevolution of Elman Recurrent Neural Networks for Time-Series Prediction. *IEEE
    Trans. Neural Netw. Learn. Syst.* 26, 12 (2015), 3123–3136.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chandra and Zhang (2012) Rohitash Chandra and Mengjie Zhang. 2012. Cooperative
    Coevolution of Elman Recurrent Neural Networks for Chaotic Time Series Prediction.
    *Neurocomputing* 86 (2012), 116–123.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022) Ke Chen, Bing Xue, Mengjie Zhang, and Fengyu Zhou. 2022.
    Evolutionary Multitasking for Feature Selection in High-Dimensional Classification
    via Particle Swarm Optimization. *IEEE Trans. Evol. Comput.* 26, 3 (2022), 446–460.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2015) Qi Chen, Bing Xue, and Mengjie Zhang. 2015. Generalisation
    and Domain Adaptation in GP with Gradient Descent for Symbolic Regression. In
    *Proc. IEEE Congr. Evol. Comput.* 1137–1144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Shuxin Chen, Lin Lin, Zixun Zhang, and Mitsuo Gen. 2019a.
    Evolutionary NetArchitecture Search for Deep Neural Networks Pruning. In *Proc.
    Aust. Conf. Artif. Intell.* 189–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Xiangru Chen, Yanan Sun, Mengjie Zhang, and Dezhong Peng.
    2020. Evolving Deep Convolutional Variational Autoencoders for Image Classification.
    *IEEE Trans. Evol. Comput.* 25, 5 (2020), 815–829.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, and
    Chang Huang. 2019b. RENAS: Reinforced Evolutionary Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 4787–4796.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2021) Fan Cheng, Feixiang Chu, Yi Xu, and Lei Zhang. 2021. A Steering-Matrix-Based
    Multiobjective Evolutionary Algorithm for High-Dimensional Feature Selection.
    *IEEE Trans. Cybern.* 52, 9 (2021), 9695–9708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A Survey
    of Model Compression and Acceleration for Deep Neural networks. *arXiv preprint
    arXiv:1710.09282* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiba et al. (2019) Zouhair Chiba, Noreddine Abghour, Khalid Moussaid, Amina El
    Omri, and Mohamed Rida. 2019. Intelligent Approach to Build a Deep Neural Network
    Based IDS for Cloud Environment Using Combination of Machine Learning Algorithms.
    *Comput. & Sec.* 86 (2019), 291–317.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and
    Jagannathan Sarangapani. 2020. A Comprehensive Survey on Model Compression and
    Acceleration. *Artif. Intell. Rev.* 53, 7 (2020), 5113–5155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chrabaszcz et al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
    2017. A Downsampled variant of imageNet as an alternative to the Cifar datasets.
    *arXiv preprint arXiv:1707.08819* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Hailong Ma. 2020.
    Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search. In
    *Proc. Eur. Conf. Comput. Vis.* 99–113.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conti et al. (2018) Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such,
    Joel Lehman, Kenneth O. Stanley, and Jeff Clune. 2018. Improving Exploration in
    Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking
    Agents. In *Proc. Adv. Neural Inf. Process. Syst.*, Vol. 31\. 5032–5043.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. (2018) Xiaodong Cui, Wei Zhang, Zoltán Tüske, and Michael Picheny.
    2018. Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural
    Networks. *Proc. Adv. Neural Inf. Process. Syst.* 31 (2018), 6051–6061.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Da Silva and Neto (2011) Sérgio Francisco Da Silva and João do ES Batista Neto.
    2011. Improving The Ranking Quality of Medical Image Retrieval Using A Genetic
    Feature Selection Method. *Decis. Support. Syst.* 51, 4 (2011), 810–820.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dahal and Zhan (2020) Binay Dahal and Justin Zhijun Zhan. 2020. Effective Mutation
    and Recombination for Evolving Convolutional Networks. In *Proc. Adv. Neural Inf.
    Process. Syst.* 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,
    Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models
    beyond a Fixed-Length Context. In *Proc. Assoc. Comput. Linguist.* 2978–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D’Ambrosio and Stanley (2007) David B. D’Ambrosio and Kenneth O. Stanley. 2007.
    A Novel Generative Encoding for Exploiting Neural Network Sensor and Output Geometry.
    In *Proc. Genetic Evol. Comput. Conf.* 974–981.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Darwish et al. (2020) Ashraf Darwish, Aboul Ella Hassanien, and Swagatam Das.
    2020. A Survey of Swarm And Evolutionary Computing Approaches for Deep Learning.
    *Artif. Intell. Rev.* 53, 3 (2020), 1767–1812.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deb et al. (2002) Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan.
    2002. A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II. *IEEE Trans.
    Evol. Comput.* 6, 2 (2002), 182–197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demirkir and Sankur (2006) Cem Demirkir and Bülent Sankur. 2006. Object Detection
    Using Haar Feature Selection Optimization. In *Proc. IEEE Signal Process. Commun.
    Appl.* 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and
    Li Fei-Fei. 2009. ImageNet: A Large-scale Hierarchical Image Database. *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* (2009), 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeVries and Taylor (2017) Terrance DeVries and Graham W Taylor. 2017. Improved
    Regularization of Convolutional Neural Networks with Cutout. *arXiv preprint arXiv:1708.04552*
    (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong and Yang (2020) Xuanyi Dong and Yi Yang. 2020. NAS-Bench-201: Extending
    the Scope of Reproducible Neural Architecture Search. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2001.00326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dowdell and Zhang (2020) Thomas Dowdell and Hongyu Zhang. 2020. Language Modelling
    for Source Code with Transformer-XL. *arXiv preprint arXiv:2007.15813* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsken et al. (2017) Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. 2017.
    Simple and Efficient Architecture Search for Convolutional Neural Networks. *arXiv
    preprint arXiv:1711.04528* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019.
    Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1804.09081.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erguzel et al. (2014) Turker Tekin Erguzel, Serhat Ozekes, Selahattin Gultekin,
    and Nevzat Tarhan. 2014. Ant Colony optimization Based Feature Selection Method
    for QEEG data classification. *Psychiatry Investig.* 11, 3 (2014), 243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estévez and Caballero (1998) Pablo A Estévez and Rodrigo E Caballero. 1998.
    A Niching Genetic Algorithm for Selecting Features for Neural Network Classifiers.
    In *Proc. Int. Conf. Artif. Neural Netw.* 311–316.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evans et al. (2018a) Benjamin Evans, Harith Al-Sahaf, Bing Xue, and Mengjie
    Zhang. 2018a. Evolutionary Deep Learning: A Genetic Programming Approach to Image
    Classification. In *Proc. IEEE Congr. Evol. Comput.* 1538–1545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evans et al. (2018b) Benjamin Patrick Evans, Harith Al-Sahaf, Bing Xue, and
    Mengjie Zhang. 2018b. Evolutionary Deep Learning: A Genetic Programming Approach
    to Image Classification. In *Proc. IEEE Congr. Evol. Comput.* 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fahrudin et al. (2016) Tresna Maulana Fahrudin, Iwan Syarif, and Ali Ridho Barakbah.
    2016. Ant Colony Algorithm for Feature Selection on Microarray Datasets. In *International
    Electronics Symposium*. 351–356.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, and Wenji Li.
    2020. Evolutionary Neural Architecture Search for Retinal Vessel Segmentation.
    *arXiv preprint arXiv:2001.06678* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandes and Yen (2021) Francisco Erivaldo Fernandes and Gary G. Yen. 2021.
    Automatic Searching and Pruning of Deep Neural Networks for Medical Imaging Diagnostic.
    *IEEE Trans. Neural Netw. Learn. Syst.* 32, 12 (2021), 5664–5674.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fogelberg and Zhang (2005) Christopher Fogelberg and Mengjie Zhang. 2005. Linear
    Genetic Programming for Multi-class Object Classification. In *Proc. Aust. Joint
    Conf. Artif.l Intell.* 369–379.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortuna and Frasca (2021) Luigi Fortuna and Mattia Frasca. 2021. Singular Value
    Decomposition. *Optim. Rob. Control* (2021), 51–58.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frachon et al. (2019) Luc Frachon, Wei Pang, and George M Coghill. 2019. Immunecs:
    Neural Committee Search by an Artificial Immune System. *arXiv preprint arXiv:1911.07729*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freitas (2003) Alex A Freitas. 2003. A Survey of Evolutionary Algorithms for
    Data Mining and Knowledge Discovery. In *Adv. Evol. Comput.* Springer, 819–845.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujino et al. (2017) Saya Fujino, Naoki Mori, and Keinosuke Matsumoto. 2017.
    Deep Convolutional Networks for Human Sketches By Means of The Evolutionary Deep
    Learning. In *Proc. Int. Conf. Soft Comput. Intell. Syst.* 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: García et al. (2011) David García, Antonio González Muñoz, and Raúl Pérez. 2011.
    A Two-Step Approach of Feature Construction for A Genetic Learning Algorithm.
    *Proc. Int. Conf. Fuzzy Syst.* (2011), 1255–1262.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerum et al. (2020) Richard C Gerum, André Erpenbeck, Patrick Krauss, and Achim
    Schilling. 2020. Sparsity Through Evolutionary Pruning Prevents Neuronal Networks
    From Overfitting. *Neural Netw.* 128 (2020), 305–312.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Golubski and Feuring (1999) Wolfgang Golubski and Thomas Feuring. 1999. Evolving
    Neural Network Structures by Means of Genetic Programming. In *Proc. Eur. Conf.
    Genetic Program*. 211–220.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. 2021. Knowledge Distillation: A Survey. *Int. J. Comput. Vis.* 129, 6 (2021),
    1789–1819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, and Jian Sun. 2020. Single Path One-Shot Neural Architecture Search
    with Uniform Sampling. In *Proc. Eur. Conf. Comput. Vis.* 544–560.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hajati et al. (2010) Farshid Hajati, Caro Lucas, and Yongsheng Gao. 2010. Face
    Localization Using an Effective Co-evolutionary Genetic Algorithm. *Proc. Int.
    Conf. Digit. Image Comput.: Tech. and Appl.* (2010), 522–527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hammami et al. (2018) Marwa Hammami, Slim Bechikh, and Chih-Cheng Hung. 2018.
    A Multi-Objective Hybrid Filter-Wrapper Evolutionary Approach for Feature Construction
    on High-Dimensional Data. In *Proc. IEEE Congr. Evol. Comput.* 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han and Cho (2006) Sang-Jun Han and Sung-Bae Cho. 2006. Evolutionary Neural
    Networks for Anomaly Detection Based on the Behavior of a Program. *IEEE Trans.
    Syst. Man Cybern.* 36, 3 (2006), 559–570.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hancer et al. (2015) Emrah Hancer, Bing Xue, Mengjie Zhang, and Dervis Karaboga.
    2015. A Multi-objective Artificial Bee Colony Approach to Feature Selection Using
    Fuzzy Mutual Information. In *Proc. IEEE Congr. Evol. Comput.* 2420–2427.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey
    of the State-of-the-Art. *Knowl-Based Syst* 212 (2021), 106622.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel Pruning
    for Accelerating Very Deep Neural Networks. In *Proc. IEEE Int. Conf. Comput.
    Vis.* 1398–1406.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2021) Kary Ho, Andrew Gilbert, Hailin Jin, and John P. Collomosse.
    2021. Neural Architecture Search for Deep Image Prior. *Comput. & Graph.* 98 (2021),
    188–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong and Cho (2006) Jin-Hyuk Hong and Sung-Bae Cho. 2006. Efficient Huge-Scale
    Feature Selection With Speciated Genetic Algorithm. *Pattern Recognit. Lett.*
    27, 2 (2006), 143–150.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Wenjing Hong, Peng Yang, Yiwen Wang, and Ke Tang. 2020. Multi-objective
    Magnitude-Based Pruning for Latency-Aware Deep Neural Network Compression. In
    *Proc. Int. Conf. on Parallel Probl. Solving Nat.* 470–483.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosni et al. (2020) Mohamed Hosni, Ginés García-Mateos, and Juan Carrillo-de
    Gea. 2020. A Mapping Study of Ensemble Classification Methods in Lung Cancer Decision
    Support Systems. *Med. & Biol. Eng. & Comput.* 58, 10 (2020), 2177–2193.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsu et al. (2021) Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    and Hongxia Jin. 2021. Language Model Compression with Weighted Low-rank Factorization.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/2207.00112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021b) Bin Hu, Tianming Zhao, Yucheng Xie, Yan Wang, and Xiaonan
    Guo. 2021b. MIXP: Efficient Deep Neural Networks Pruning for Further FLOPs Compression
    via Neuron Bond. In *Proc. Int. Joint Conf. Neural Netw.* 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2021a) Yiming Hu, Xingang Wang, Lujun Li, and Qingyi Gu. 2021a. Improving
    One-Shot NAS with Shrinking-and-Expanding Supernet. *Pattern Recognit.* 118 (2021),
    108025.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2012) Hu Huang, Hong-Bo Xie, Jing-Yi Guo, and Hui-Juan Chen. 2012.
    Ant Colony Optimization-based Feature Selection Method for Surface Electromyography
    Signals Classification. *Comput. Biol. Med.* 42, 1 (2012), 30–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020) Junhao Huang, Weize Sun, and Lei Huang. 2020. Deep Neural
    Networks Compression Learning Based on Multiobjective Evolutionary Algorithms.
    *Neurocomputing* 378 (2020), 260–269.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ijjina and Chalavadi (2016) Earnest Paul Ijjina and Krishna Mohan Chalavadi.
    2016. Human Action Recognition Using Genetic Algorithms and Convolutional Neural
    Networks. *Pattern Recognit.* 59 (2016), 199–212.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irwin-Harris et al. (2019) William Irwin-Harris, Yanan Sun, Bing Xue, and Mengjie
    Zhang. 2019. A Graph-Based Encoding for Evolutionary Convolutional Neural Network
    Architecture Design. In *Proc. IEEE Congr. Evol. Comput.* 546–553.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izenman (2013) Alan Julian Izenman. 2013. Linear Discriminant Analysis. In *Modern
    Multivariate Statistical Techniques*. Springer, 237–280.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaâfra et al. (2019) Yesmina Jaâfra, Jean Luc Laurent, Aline Deruyver, and
    Mohamed Saber Naceur. 2019. Reinforcement Learning for Neural Architecture Search:
    A Review. *Image Vis. Comput.* 89 (2019), 57–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2018) Haifeng Jin, Qingquan Song, and Xia Hu. 2018. Auto-keras:
    Efficient Neural Architecture Search with Network Morphism. *arXiv preprint arXiv:1806.10282*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2019) Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras:
    An Efficient Neural Architecture Search System. In *Proc. ACM SIGKDD Int. Conf.
    on Knowl. Discov. & Data Min.* 1946–1956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin (2006) Yaochu Jin. 2006. *Multi-Objective Machine Learning*. Springer Science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jones et al. (2019) David T Jones, Anja Schroeder, and Geoff S. Nitschke. 2019.
    Evolutionary Deep Learning to Identify Galaxies in the Zone of Avoidance. *arXiv
    preprint arXiv:1903.07461* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Junior and Yen (2021a) Francisco Erivaldo Fernandes Junior and Gary G. Yen.
    2021a. Pruning Deep Convolutional Neural Networks Architectures with Evolution
    Strategy. *Inf. Sci.* 552 (2021), 29–47.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Junior and Yen (2021b) Francisco Erivaldo Fernandes Junior and Gary G. Yen.
    2021b. Pruning of Generative Adversarial Neural Networks for Medical Imaging Diagnostics
    with Evolution Strategy. *Inf. Sci.* 558 (2021), 91–102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karaboga et al. (2007) Dervis Karaboga, Bahriye Akay, and Celal Öztürk. 2007.
    Artificial Bee Colony (ABC) Optimization Algorithm for Training Feed-Forward Neural
    Networks. In *Proc. Int. Conf. Modeling Decis. Artif. Intell.* 318–329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karegowda and Manjunath (2011) Asha Gowda Karegowda and A. S. Manjunath. 2011.
    Application of Genetic Algorithm Optimized Neural Network Connection Weights for
    Medical Diagnosis of PIMA Indians Diabetes. *Int. J. Soft Comput.* 2, 2 (2011),
    15–23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khadka and Tumer (2018) Shauharda Khadka and Kagan Tumer. 2018. Evolution-Guided
    Policy Gradient in Reinforcement Learning. In *Proc. Adv. Neural Inf. Process.
    Syst.*, Vol. 31\. 1196–1208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khushaba et al. (2008) Rami N Khushaba, Ahmed Al-Ani, Akram AlSukker, and Adel
    Al-Jumaily. 2008. A Combined Ant Colony and Differential Evolution Feature Selection
    Algorithm. In *Proc. Int. Conf. Ant Colony Optim. Swarm Intell.* 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kitano (1990) Hiroaki Kitano. 1990. Designing Neural Networks Using Genetic
    Algorithms with Graph Generation System. *Complex Syst.* 4, 4 (1990), 225–238.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kotani and Kato (2004) Manabu Kotani and Daisuke Kato. 2004. Feature Extraction
    Using Coevolutionary Genetic Programming. In *Proc. IEEE Congr. Evol. Comput.*
    614–619.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koutník et al. (2010) Jan Koutník, Faustino J. Gomez, and Jürgen Schmidhuber.
    2010. Evolving Neural Networks in Compressed Weight Space. In *Proc. Genetic Evol.
    Comput. Conf.* 619–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koutník et al. (2014) Jan Koutník, Jürgen Schmidhuber, and Faustino J. Gomez.
    2014. Evolving Deep Unsupervised Convolutional Networks for Vision-Based Reinforcement
    Learning. In *Proc. Genetic Evol. Comput. Conf.* 541–548.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krawiec (2002) Krzysztof Krawiec. 2002. Genetic Programming-Based Construction
    of Features for Machine Learning and Knowledge Discovery Tasks. *Genet. Program
    Evolvable Mach.* 3, 4 (2002), 329–343.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    Multiple Layers of Features From Tiny Images. (2009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Proc.
    Adv. Neural Inf. Process. Syst.*, Vol. 25\. 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwasigroch et al. (2019) Arkadiusz Kwasigroch, Michał Grochowski, and Mateusz
    Mikolajczyk. 2019. Deep Neural Network Architecture Search using Network Morphism.
    In *Proc. Int. Conf. Methods and Models in Autom. and Robot.* 30–35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    Learning. *Nature* 521, 7553 (2015), 436–444.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, and Wayne Hubbard. 1989. Backpropagation Applied to Handwritten
    Zip Code Recognition. *Neural computation* 1, 4 (1989), 541–551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, and Liang Lin.
    2020. EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning.
    In *Proc. Eur. Conf. Comput. Vis.* 639–654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao,
    Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. HW-NAS-Bench:
    Hardware-Aware Neural Architecture Search Benchmark. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2103.10584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Qing Li, Wei Zhang, Lin Zhao, Xia Wu, and Tianming Liu. 2022.
    Evolutional Neural Architecture Search for Optimization of Spatiotemporal Brain
    Network Decomposition. *IEEE. Trans. Biomed. Eng.* 69, 2 (2022), 624–634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao.
    2019. EA-LSTM: Evolutionary Attention-Based LSTM for Time Series Prediction. *Knowl.-Based
    Syst.* 181 (2019), 104785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive Neural Architecture Search. In *Proc. Eur. Conf. Comput. Vis.*
    19–34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
    Chiang, and Kai-Chiang Wu. 2021a. FOX-NAS: Fast, On-device and Explainable Neural
    Architecture Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 789–797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018b. DARTS:
    Differentiable Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1806.09055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Peng Liu, Mohammad D. El Basha, Yangjunyi Li, Yao Xiao, Pina C.
    Sanelli, and Ruogu Fang. 2019a. Deep Evolutionary Networks with Expedited Genetic
    Algorithms for Medical Image Denoising. *Med. Image Anal.* 54 (2019), 306–315.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Guo (2021) Sicong Liu and Bin Guo. 2021. AdaSpring: Context-adaptive
    and Runtime-evolutionary Deep Model Compression for Mobile Applications. In *Proc.
    ACM Interact., Mobile, Wearable Ubiquitous Tech.*, Vol. 5\. ACM, 1–22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, and Han-Shuo
    Ye. 2018a. A Hybrid Genetic Algorithm With Wrapper-Embedded Approaches for Feature
    Selection. *IEEE Access* 6 (2018), 22863–22874.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen,
    and Kay Chen Tan. 2021b. A Survey on Evolutionary Neural Architecture Search.
    *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3100554](https://doi.org/10.1109/TNNLS.2021.3100554)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    K. Cheng, and Jian Sun. 2019b. MetaPruning: Meta Learning for Automatic Neural
    Network Channel Pruning. In *Proc. IEEE Int. Conf. Comput. Vis.* 3295–3304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lomurno et al. (2021) Eugenio Lomurno, Stefano Samele, Matteo Matteucci, and
    Danilo Ardagna. 2021. Pareto-optimal Progressive Neural Architecture Search. In
    *Proc. Genetic Evol. Comput. Conf.* 1726–1734.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Londt et al. (2021) Trevor Londt, Xiaoying Gao, and Peter Andreae. 2021. Evolving
    Character-level DenseNet Architectures Using Genetic Programming. In *Proc. Int.
    Conf. Appl. Evol. Comput.* 665–680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Londt et al. (2020) Trevor Londt, Xiaoying Gao, Bing Xue, and Peter Andreae.
    2020. Evolving Character-level Convolutional Neural Networks for Text Classification.
    *arXiv preprint arXiv:2012.02223* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loni et al. (2020) Mohammad Loni, Sima Sinaei, and Ali Zoljodi. 2020. DeepMaker:
    A Multi-Objective Optimization Framework for Deep Neural Networks in Embedded
    Systems. *Microprocess. Microsyst.* 73 (2020), 102989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lorenzo and Nalepa (2018) Pablo Ribalta Lorenzo and Jakub Nalepa. 2018. Memetic
    Evolution of Deep Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 505–512.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lorenzo et al. (2017) Pablo Ribalta Lorenzo, Jakub Nalepa, Luciano Sánchez Ramos,
    and José Ranilla. 2017. Hyper-parameter Selection in Deep Neural Networks Using
    Parallel Particle Swarm Optimization. In *Proc. Genetic Evol. Comput. Conf.* 1864–1871.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2021) Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf,
    Kalyanmoy Deb, and Vishnu Naresh Boddeti. 2021. Neural Architecture Transfer.
    *IEEE IEEE Trans. Pattern Anal. Mach. Intell.* 43, 9 (2021), 2971–2989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019) Zhichao Lu, Ian Whalen, Vishnu Naresh Boddeti, Yashesh D.
    Dhebar, and Kalyanmoy Deb. 2019. NSGA-Net: Neural Architecture Search using Multi-objective
    Genetic Algorithm. In *Proc. Genetic Evol. Comput. Conf.* 419–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2018) Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
    2018. Neural architecture optimization. In *Proc. Adv. Neural Inf. Process. Syst.*,
    Vol. 31\. 7827–7838.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2021b) Ailong Ma, Yuting Wan, Yanfei Zhong, Junjue Wang, and Liang
    pei Zhang. 2021b. SceneNet: Remote Sensing Scene Classification Deep Learning
    Network Using Multi-Objective Neural Evolution Architecture Search. *ISPRS J.
    Photogramm. Remote Sens.* 172 (2021), 171–188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2022) Lianbo Ma, Min Huang, Shengxiang Yang, Rui Wang, and Xingwei
    Wang. 2022. An Adaptive Localized Decision Variable Analysis Approach to Large-Scale
    Multiobjective and Many-Objective Optimization. *IEEE Trans. Cybern.* 52, 7 (2022),
    6684–6696.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2021a) Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang, and Xingwei
    Wang. 2021a. How to Simplify Search: Classification-wise Pareto Evolution for
    One-shot Neural Architecture Search. *arXiv preprint arXiv:2109.07582* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2021c) Wenping Ma, Xiaobo Zhou, Hao Zhu, Longwei Li, and Licheng
    Jiao. 2021c. A Two-stage Hybrid Ant Colony Optimization for High-dimensional Feature
    Selection. *Pattern Recognit.* 116 (2021), 107933.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maniezzo (1994) V. Maniezzo. 1994. Genetic Evolution of the Topology and Weight
    Distribution of Neural Networks. *IEEE Trans. Neural. Netw.* 5, 1 (1994), 39–53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mauceri et al. (2021) Stefano Mauceri, James Sweeney, Miguel Nicolau, and James
    McDermott. 2021. Feature Extraction by Grammatical Evolution for One-class Time
    Series Classification. *Genet. Program. Evolvable Mach.* 22, 3 (2021), 267–295.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mazzawi et al. (2019) Hanna Mazzawi, Xavi Gonzalvo, Aleksandar Kracun, and Prashant
    Sridhar. 2019. Improving Keyword Spotting and Language Identification via Neural
    Architecture Search at Scale. In *INTERSPEECH*. 1278–1282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. (2017) Yi Mei, Su Nguyen, Bing Xue, and Mengjie Zhang. 2017. An Efficient
    Feature Selection Algorithm for Evolving Job Shop Scheduling Rules With Genetic
    Programming. *IEEE Trans. Emerg. Topics Comput. Intell.* 1, 5 (2017), 339–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miahi et al. (2022) Erfan Miahi, Seyed Abolghasem Mirroshandel, and Alexis Nasr.
    2022. Genetic Neural Architecture Search for Automatic Assessment of Human Sperm
    Images. *Expert Syst. Appl.* 188 (2022), 115937.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miikkulainen et al. (2019) Risto Miikkulainen, Jason Liang, Elliot Meyerson,
    Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak
    Navruzyan, Nigel Duffy, et al. 2019. Evolving Deep Neural Networks. In *Artificial
    Intelligence in the Age Of Neural Networks and Brain Computing*. Elsevier, 293–312.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirjalili et al. (2019) Seyedali Mirjalili, Hossam Faris, and Ibrahim Aljarah.
    2019. *Evolutionary Machine Learning Techniques*. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo et al. (2021) Hyunho Mo, Leonardo Lucio Custode, and Giovanni Iacca. 2021.
    Evolutionary Neural Architecture Search for Remaining Useful Life Prediction.
    *Appl. Soft Comput.* 108 (2021), 107474.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montana and Davis (1989) David J Montana and Lawrence Davis. 1989. Training
    Feedforward Neural Networks Using Genetic Algorithms. In *Proc. of the Int. Joint
    Conf. Artif. Intell.*, Vol. 4\. 762–767.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muni et al. (2006) Durga Prasad Muni, Nikhil R Pal, and Jyotirmay Das. 2006.
    Genetic Programming for Simultaneous Feature Selection and Classifier Design.
    *IEEE Trans. Syst. Man. Cybern.* 36, 1 (2006), 106–117.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murray and Chiang (2015) Kenton Murray and David Chiang. 2015. Auto-Sizing
    Neural Networks: With Applications to N-Gram Language Models. In *Proc. Conf.
    Empir. Methods Nat. Lang. Proc.* 908–916.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nekrasov et al. (2020) Vladimir Nekrasov, Chunhua Shen, and Ian Reid. 2020.
    Template-Based Automatic Search of Compact Semantic Segmentation Architectures.
    In *Proc. Winter Conf. Appl. Comput. Vis.* 1980–1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neshat et al. (2020) Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad,
    Lina Bertling Tjernberg, Davide Astiaso Garcia, Bradley Alexander, and Markus
    Wagner. 2020. An Evolutionary Deep Learning Method for Short-term Wind Speed Prediction:
    A Case Study of the Lillgrund Offshore Wind Farm. *arXiv preprint arXiv:abs/2002.09106*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neshatian et al. (2012) Kourosh Neshatian, Mengjie Zhang, and Peter Andreae.
    2012. A Filter Approach to Multiple Feature Construction for Symbolic Learning
    Classifiers Using Genetic Programming. *IEEE Trans. Evol. Comput.* 16, 5 (2012),
    645–661.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neshatian et al. (2007) Kourosh Neshatian, Mengjie Zhang, and Mark Johnston.
    2007. Feature Construction and Dimension Reduction Using Genetic Programming.
    In *Proc. Aust. Conf. Artif. Intell.* 242–253.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2014) Hoai Bach Nguyen, Bing Xue, Ivy Liu, and Mengjie Zhang.
    2014. PSO and Statistical Clustering for Feature Selection: A New Representation.
    In *Proc. Asia-Pacific Conf. Simulated Evol. Learn.* 569–581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O’Boyle and Palmer (2008) Noel M O’Boyle and David S Palmer. 2008. Simultaneous
    Feature Selection and Parameter Optimisation Using An Artificial Ant Colony: Case
    Study of Melting Point Prediction. *Chem. Cent. J.* 2, 1 (2008), 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Olson and Moore (2016) Randal S. Olson and Jason H. Moore. 2016. TPOT: A Tree-based
    Pipeline Optimization Tool for Automating Machine Learning. In *Proc. Int. Conf.
    Mach. Learn.* 151–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O’Neill et al. (2018) Damien O’Neill, Bing Xue, and Mengjie Zhang. 2018. Co-evolution
    of Novel Tree-Like ANNs and Activation Functions: An Observational Study. In *Proc.
    Aust. Conf. Artif. Intell.* 616–629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oong and Isa (2011) Tatt Hee Oong and Nor Ashidi Mat Isa. 2011. Adaptive Evolutionary
    Artificial Neural Networks for Pattern Classification. *IEEE Trans. Neural Netw.*
    22, 11 (2011), 1823–1836.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ortego et al. (2020) Patxi Ortego, Alberto Diez-Olivan, Javier Del Ser, and
    Fernando Veiga. 2020. Evolutionary LSTM-FCN Networks for Pattern Classification
    in Industrial Processes. *Swarm Evol. Comput.* 54 (2020), 100650.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2021) Bo Peng, Shuting Wan, Ying Bi, Bing Xue, and Mengjie Zhang.
    2021. Automatic Feature Extraction and Construction Using Genetic Programming
    for Rotating Machinery Fault Diagnosis. *IEEE Trans. Cybern.* 51, 10 (2021), 4909–4923.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2018) Yiming Peng, Gang Chen, Harman Singh, and Mengjie Zhang.
    2018. NEAT for Large-scale Reinforcement Learning Through Evolutionary Feature
    Learning and Policy Gradient Search. In *Proc. Genetic Evol. Comput. Conf.* 490–497.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phan et al. (2020) Hai T. Phan, Zechun Liu, Dang The Huynh, Marios Savvides,
    Kwang-Ting Cheng, and Zhiqiang Shen. 2020. Binarizing MobileNet via Evolution-Based
    Searching. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 13417–13426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model Compression via Distillation and Quantization. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/1802.05668.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poyatos et al. (2022) Javier Poyatos, Daniel Molina, Aritz Martinez, et al.
    2022. EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning based
    Deep Neural Networks. *arXiv preprint arXiv:2202.03844* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving Language Understanding by Generative Pre-training. (2018),
    https://www.cs.ubc.ca/ amuham01/LING530/papers/radford2018improving.pdf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rapaport et al. (2019) Elad Rapaport, Oren Shriki, and Rami Puzis. 2019. EEGNAS:
    Neural Architecture Search for Electroencephalography Data Analysis and Decoding.
    In *Proc. Int. Joint Conf. Artif. Intell.* 3–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rashid et al. (2020) ANM Bazlur Rashid, Mohiuddin Ahmed, Leslie F Sikos, and
    Paul Haskell-Dowland. 2020. Cooperative Co-Evolution for Feature Selection in
    Big Data With Random Feature Grouping. *J. Big Data* 7, 1 (2020), 1–42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rawal and Miikkulainen (2018) Aditya Rawal and Risto Miikkulainen. 2018. From
    Nodes to Networks: Evolving Recurrent Neural Networks. *arXiv preprint arXiv:1803.04439*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    2019. Regularized Evolution for Image Classifier Architecture Search. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol. 33\. 4780–4789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. 2017. Large-Scale
    Evolution of Image Classifiers. In *Proc. Int. Conf. Mach. Learn.* 2902–2911.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refahi et al. (2020) Mohammad Saleh Refahi, A Mir, and Jalal A Nasiri. 2020.
    A Novel Fusion Based on the Evolutionary Features for Protein Fold Recognition
    Using Support Vector Machines. *Sci. Rep.* 10, 1 (2020), 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, and
    Zhihui Li. 2021. A Comprehensive Survey of Neural Architecture Search: Challenges
    and Solutions. *ACM Comput. Surv.* 54, 4 (2021), 1–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roberts and Claridge (2005) Mark E. Roberts and Ela Claridge. 2005. A Multistage
    Approach to Cooperatively Coevolving Feature Construction and Object Detection.
    In *Proc. Appl. Evol. Comput.* 396–406.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rostami and Neri (2016) Shahin Rostami and Ferrante Neri. 2016. Covariance Matrix
    Adaptation Pareto Archived Evolution Strategy With Hypervolume-Sorted Adaptive
    Grid Algorithm. *Integr. Comput. Aided. Eng.* 23, 4 (2016), 313–329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1985) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1985. *Learning Internal Representations by Error Propagation*. Technical Report.
    California Univ San Diego La Jolla Inst for Cognitive Science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning Representations by Back-propagating Errors. *Nature* 323, 6088
    (1986), 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rundo et al. (2019) Leonardo Rundo, Andrea Tangherloni, Marco S Nobile, Carmelo
    Militello, Daniela Besozzi, Giancarlo Mauri, and Paolo Cazzaniga. 2019. MedGA:
    A Novel Evolutionary Method for Image Enhancement in Medical Imaging Systems.
    *Expert Syst. Appl.* 119 (2019), 387–399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samala et al. (2018) Ravi K. Samala, Heang-Ping Chan, Lubomir M. Hadjiiski,
    Mark A. Helvie, Caleb D. Richter, and Kenny H. Cha. 2018. Evolutionary Pruning
    of Transfer Learned Deep Convolutional Neural Network For Breast Cancer Diagnosis
    In Digital Breast Tomosynthesis. *Phys. Med. Biol.* 63, 9 (2018), 095005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santra et al. (2021) Santanu Santra, Jun-Wei Hsieh, and Chi-Fang Lin. 2021.
    Gradient Descent Effects on Differential Neural Architecture Search: A Survey.
    *IEEE Access* 9 (2021), 89602–89618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sapra and Pimentel (2020) Dolly Sapra and Andy D Pimentel. 2020. Constrained
    Evolutionary Piecemeal Training to Design Convolutional Neural Networks. In *Proc.
    Int. Conf. Industr., Eng. and Other Appl. of App. Intell. Syst.* 709–721.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schorn et al. (2020) Christoph Schorn, Thomas Elsken, Sebastian Vogel, and Armin
    Runge. 2020. Automated Design Of Error-Resilient and Hardware-Efficient Deep Neural
    Networks. *Neural. Comput. Appl.* 32, 24 (2020), 18327–18345.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sciuto et al. (2020) Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Cristian
    Musat, and Mathieu Salzmann. 2020. Evaluating the Search Phase of Neural Architecture
    Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.08142.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shafti and Pérez (2008) Leila Shila Shafti and E. Islas Pérez. 2008. Data Reduction
    by Genetic Algorithms and Non-Algebraic Feature Construction: A Case Study. *Proc.
    Int. Conf. Hybri. Intell. Syst.* (2008), 573–578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shakya et al. (2021) Pratistha Shakya, Eamonn Kennedy, Christopher Rose, and
    Jacob K. Rotein. 2021. High-Dimensional Time Series Feature Extraction for Low-Cost
    Machine Olfaction. *IEEE Sens. J.* 21, 3 (2021), 2495–2504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2022) Haopu Shang, Jia-Liang Wu, Wenjing Hong, and Chao Qian.
    2022. Neural Network Pruning by Cooperative Coevolution. *arXiv preprint arXiv:2204.05639*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2019) Mingzhu Shen, Kai Han, Chunjing Xu, and Yunhe Wang. 2019.
    Searching for Accurate Binary Neural Architectures. In *Proc. IEEE Int. Conf.
    Comput. Vis.* 2041–2044.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siems et al. (2020) Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
    Margret Keuper, and Frank Hutter. 2020. NAS-bench-301 and The Case for Surrogate
    Benchmarks for Neural Architecture Search. *arXiv preprint arXiv:2008.09777* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sikdar et al. (2012) Utpal Kumar Sikdar, Asif Ekbal, and Sriparna Saha. 2012.
    Differential Evolution Based Feature Selection and Classifier Ensemble for Named
    Entity Recognition. In *Proc. COLING*. 2475–2490.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith and Jin (2014) Christopher Smith and Yaochu Jin. 2014. Evolutionary Multi-objective
    Generation of Recurrent Neural Network Ensembles for Time Series Prediction. *Neurocomputing*
    143 (2014), 302–311.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Socha and Blum (2007) Krzysztof Socha and Christian Blum. 2007. An Ant Colony
    Optimization Algorithm for Continuous Optimization: Application to Feed-forward
    Neural Network Training. *Neural. Comput. Appl.* 16, 3 (2007), 235–247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, and
    Yunhe Wang. 2020. Efficient Residual Dense Block Search for Image Super-Resolution.
    In *Proc. AAAI Conf. Artif. Intell.*, Vol. 34\. 12007–12014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song (2021) Xin Song. 2021. Intelligent English Translation System Based on
    Evolutionary Multi-Objective Optimization Algorithm. *J. Intell. Fuzzy Syst.*
    40 (2021), 6327–6337.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanley and Miikkulainen (2002) Kenneth O Stanley and Risto Miikkulainen. 2002.
    Evolving Neural Networks Through Augmenting Topologies. *Evol. Comput.* 10, 2
    (2002), 99–127.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Yanan Sun, Xian Sun, Yuhan Fang, Gary G. Yen, and Yuqiao Liu.
    2021. A Novel Training Protocol for Performance Predictors of Evolutionary Neural
    Architecture Search Algorithms. *IEEE Trans. Evol. Comput.* 25, 3 (2021), 524–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019a) Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen,
    and Mengjie Zhang. 2019a. Surrogate-Assisted Evolutionary Deep Learning Using
    an End-To-End Random Forest-Based Performance Predictor. *IEEE Trans. Evol. Comput.*
    24, 2 (2019), 350–364.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019b) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen. 2019b.
    Completely Automated CNN Architecture Design Based on Blocks. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 4 (2019), 1242–1254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019c) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen. 2019c.
    Evolving Deep Convolutional Neural Networks for Image Classification. *IEEE Trans.
    Evol. Comput.* 24, 2 (2019), 394–407.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019d) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G. Yen. 2019d.
    A Particle Swarm Optimization-Based Flexible Convolutional Autoencoder for Image
    Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 30, 8 (2019), 2295–2309.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Jiancheng
    Lv. 2020. Automatically Designing CNN Architectures Using the Genetic Algorithm
    for Image Classification. *IEEE Trans. Cybern.* 50, 9 (2020), 3840–3854.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swaminathan et al. (2020) Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan,
    and Frédéric Andrès. 2020. Sparse Low Rank Factorization for Deep Neural Network
    compression. *Neurocomputing* 398 (2020), 185–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanaka et al. (2016) Tomohiro Tanaka, Takafumi Moriya, and Takahiro Shinozaki.
    2016. Evolutionary Optimization of Long Short-Term Memory Neural Network Language
    Model. *J. Acoust. Soc. Am.* 140, 4 (2016), 3062–3062.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Yajiao Tang, Junkai Ji, Yulin Zhu, Shangce Gao, Zheng Tang,
    and Yuki Todo. 2019. A Differential Evolution-Oriented Pruning Neural Network
    Model for Bankruptcy Prediction. In *Complexity*, Vol. 2019. 8682124:1–8682124:21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tariq et al. (2018) Hassan Tariq, Elf Eldridge, and Ian Welch. 2018. An Efficient
    Approach for Feature Construction of High-Dimensional Microarray Data By Random
    Projections. *PLoS ONE* 13, 4 (2018), e0196385.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Telikani et al. (2021) Akbar Telikani, Amirhessam Tahmassebi, Wolfgang Banzhaf,
    and Amir H Gandomi. 2021. Evolutionary Machine Learning: A Survey. *ACM Comput.
    Surv.* 54, 8 (2021), 1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teller and Veloso (1996) Astro Teller and Manuela Veloso. 1996. PADO: A New
    Learning Architecture for Object Recognition. *Symbolic visual learn.* (1996),
    81–116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2019) Haiman Tian, ShuChing Chen, MeiLing Shyu, and Stuart Harvey
    Rubin. 2019. Automated Neural Network Construction with Similarity Sensitive Evolutionary
    Algorithms. In *Proc. IEEE Int. Conf. Inf. Reuse Integr. Data Sci.* 283–290.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2018) Binh Tran, Bing Xue, and Mengjie Zhang. 2018. A New Representation
    in PSO for Discretization-Based Feature Selection. *IEEE Trans. Cybern.* 48, 6
    (2018), 1733–1746.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2019) Binh Tran, Bing Xue, and Mengjie Zhang. 2019. Genetic Programming
    for Multiple-Feature Construction on High-Dimensional Classification. *Pattern
    Recognit.* 93 (2019), 404–417.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2016) Binh Tran, Mengjie Zhang, and Bing Xue. 2016. Multiple Feature
    Construction in Classification on High-Dimensional Data Using GP. In *IEEE Symposium
    Series on Computational Intelligence*. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vafaie and De Jong (1998) Haleh Vafaie and Kenneth De Jong. 1998. Feature Space
    Transformation Using Genetic Algorithms. *IEEE Intell. Syst. Appli.* 13, 2 (1998),
    57–65.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vargas-Hákim et al. (2022) Gustavo A Vargas-Hákim, Efrén Mezura-Montes, and
    Héctor-Gabriel Acosta-Mesa. 2022. A Review on Convolutional Neural Networks Encodings
    for Neuroevolution. *IEEE Trans. Evol. Comput.* 26, 1 (2022), 12–27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vieira et al. (2013) Susana M Vieira, Luís F Mendonça, Goncalo J Farinha, and
    João MC Sousa. 2013. Modified Binary PSO for Feature Selection Using SVM Applied
    to Mortality Prediction of Septic Patients. *Appl. Soft Comput.* 13, 8 (2013),
    3494–3504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Bin Wang, Wenbin Pei, Bing Xue, and Mengjie Zhang. 2021b.
    Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks
    in Image Classification. In *Proc. Genetic Evol. Comput. Conf.* 173–174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Bin Wang, Bing Xue, and Mengjie Zhang. 2020b. Particle
    Swarm Optimization for Evolving Deep Convolutional Neural Networks for Image Classification:
    Single-and Multi-objective Approaches. In *Deep Neural Evolution*. Springer, 155–184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Bin Wang, Bing Xue, and Mengjie Zhang. 2020c. Particle Swarm
    Optimization for Evolving Deep Neural Networks for Image Classification By Evolving
    and Stacking Transferable Blocks. In *Proc. IEEE Congr. Evol. Comput.* 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021c) Bin Wang, Bing Xue, and Mengjie Zhang. 2021c. Surrogate-Assisted
    Particle Swarm Optimization for Evolving Variable-Length Transferable Blocks for
    Image Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 33, 8 (2021), 3727–3740.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng
    Zhu, Chuang Gan, and Song Han. 2020a. HAT: Hardware-Aware Transformers for Efficient
    Natural Language Processing. In *Proc. Assoc. Comput. Linguist.* 7675–7688.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020d) Xiao-han Wang, Yong Zhang, and Xiao-yan Sun. 2020d. Multi-Objective
    Feature Selection Based on Artificial Bee Colony: An Acceleration Approach With
    Variable Sample Size. *Appl. Soft Comput.* 88 (2020), 106041.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, and Dacheng Tao.
    2018. Towards Evolutionary Compression. In *Proc. ACM SIGKDD Int. Conf. Knowl.
    Discov. & Data Min.* 2476–2485.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Zhehui Wang, Tao Luo, Miqing Li, Joey Tianyi Zhou, Rick
    Siow Mong Goh, and Liangli Zhen. 2021a. Evolutionary Multi-Objective Model Compression
    for Deep Neural Networks. *IEEE Comput. Intell. Mag.* 16, 3 (2021), 10–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen and Xu (2011) Yun Wen and Hua Xu. 2011. A Cooperative Coevolution-Based
    Pittsburgh Learning Classifier System Embedded With Memetic Feature Selection.
    In *Proc. IEEE Congr. Evol. Comput.* 2415–2422.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'White et al. (2021) Colin White, Willie Neiswanger, and Yash Savani. 2021.
    BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture
    Search. In *Proc. AAAI Conf. Artif. Intell.*, Vol. 35\. 10293–10301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J
    Barezi, and Pascale Fung. 2019. On the Effectiveness of Low-rank Matrix Factorization
    for LSTM Model Compression. *arXiv preprint arXiv:1908.09982* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021b) Min Wu, Wanjuan Su, Luefeng Chen, and Zhentao Liu. 2021b.
    Weight-Adapted Convolution Neural Network for Facial Expression Recognition in
    Human-Robot Interaction. *IEEE Trans. Syst. Man Cybern.* 51, 3 (2021), 1473–1484.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021a) Tao Wu, Xiaoyang Li, Deyun Zhou, Na Li, and Jiao Shi. 2021a.
    Differential Evolution Based Layer-Wise Weight Pruning for Compressing Deep Neural
    Networks. *Sens.* 21, 3 (2021), 880.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Tao Wu, Jiao Shi, Deyun Zhou, Yu Lei, and Maoguo Gong. 2019.
    A Multi-objective Particle Swarm Optimization for Neural Networks Pruning. In
    *Proc. IEEE Congr. Evol. Comput.* 570–577.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Xiang Wu, Ran He, Yibo Hu, and Zhenan Sun. 2020. Learning an
    Evolutionary Embedding via Massive Knowledge Distillation. *Int. J. Comput. Vis.*
    128, 8 (2020), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022a) Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu,
    Zhengsu Chen, Lanfei Wang, Anxiang Xiao, Jianlong Chang, Xiaopeng Zhang, and Qi
    Tian. 2022a. Weight-Sharing Neural Architecture Search: A Battle to Shrink the
    Optimization Gap. *ACM Comput. Surv.* 54, 9 (2022), 1–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Yuille (2017) Lingxi Xie and Alan Yuille. 2017. Genetic CNN. In *Proc.
    IEEE Int. Conf. Comput. Vis.* 1379–1388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022b) Xiangning Xie, Yuqiao Liu, Yanan Sun, Gary G. Yen, Bing
    Xue, and Mengjie Zhang. 2022b. BenchENAS: A Benchmarking Platform for Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* (2022). [https://doi.org/10.1109/TEVC.2022.3147526](https://doi.org/10.1109/TEVC.2022.3147526)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Ke Xu, Dezheng Zhang, Jianjing An, Li Liu, Lingzhi Liu, and
    Dong Wang. 2021. GenExp: Multi-objective Pruning for Deep Neural Network based
    on Genetic Algorithm. *Neurocomputing* 451 (2021), 81–94.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2012) Bing Xue, Mengjie Zhang, and Will N Browne. 2012. Multi-Objective
    Particle Swarm Optimization (PSO) for Feature Selection. In *Proc. Genetic Evol.
    Comput. Conf.* 81–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2015) Bing Xue, Mengjie Zhang, Will N Browne, and Xin Yao. 2015.
    A Survey on Evolutionary Computation Approaches to Feature Selection. *IEEE Trans.
    Evol. Comput.* 20, 4 (2015), 606–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2013) Bing Xue, Mengjie Zhang, Yan Dai, and Will N Browne. 2013.
    PSO for Feature Construction and Binary Classification. In *Proc. Genetic Evol.
    Comput. Conf.* 137–144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Chuanguang Yang, Zhulin An, Chao Li, Boyu Diao, and Yongjun
    Xu. 2019. Multi-objective Pruning for CNNs Using Genetic Algorithm. In *Proc.
    Int. Conf. Artif. Neural Netw.* 299–305.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Shangshang Yang, Ye Tian, Cheng He, Xingyi Zhang, Kay Chen
    Tan, and Yaochu Jin. 2021. A Gradient-Guided Evolutionary Approach to Training
    Deep Neural Networks. *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3061630](https://doi.org/10.1109/TNNLS.2021.3061630)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, and Chao
    Xu. 2020. CARS: Continuous Evolution for Efficient Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 1826–1835.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, and
    Yu-Feng Li. 2018. Taking Human out of Learning Applications: A Survey on Automated
    Machine Learning. *arXiv preprint arXiv:1810.13306* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao (1993) Xin Yao. 1993. A Review of Evolutionary Artificial Neural Networks.
    *Int. J. Intell. Syst.* 8, 4 (1993), 539–567.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao and Liu (1996) Xin Yao and Yong Liu. 1996. Ensemble Structure of Evolutionary
    Artificial Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 659–664.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying et al. (2019) Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen,
    Kevin P. Murphy, and Frank Hutter. 2019. NAS-Bench-101: Towards Reproducible Neural
    Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.09635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing
    [Review Article]. *IEEE Comput. Intell. Mag.* 13, 3 (2018), 55–75.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2009) Hualong Yu, Guochang Gu, Haibo Liu, Jing Shen, and Jing Zhao.
    2009. A modified Ant Colony Optimization Algorithm for Tumor Marker Gene Selection.
    *Genomics, Proteomics & Bioinformatics* 7, 4 (2009), 200–208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z.-Flores et al. (2020) Emigdio Z.-Flores, Leonardo Trujillo, Pierrick Legrand,
    and Frédérique Faïta-Aïnseba. 2020. EEG Feature Extraction Using Genetic Programming
    for the Classification of Mental States. *Algorithms* 13, 9 (2020), 221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zemouri et al. (2019) Ryad A. Zemouri, N. Omri, Farhat Fnaiech, Noureddine Zerhouni,
    and Nader Fnaiech. 2019. A New Growing Pruning Deep Learning Neural Network Algorithm
    (GP-DLNN). *Neural. Comput. Appl.* 32 (2019), 1–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et al. (2021) Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu
    Wu, Tianyun Zhang, Malith Jayaweera, David R. Kaeli, Bin Ren, Xue Lin, and Yanzhi
    Wang. 2021. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture
    and Pruning Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 4801–4811.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Mühlenbein (1995) Byoung-Tak Zhang and Heinz Mühlenbein. 1995. Balancing
    Accuracy and Parsimony in Genetic Programming. *Evol. Comput.* 3, 1 (1995), 17–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Di Zhang, Yichen Zhou, Jiaqi Zhao, and Yong Zhou. 2022b.
    Co-evolution-based Parameter Learning for Remote Sensing Scene Classification.
    *Int. J. Wavelets Multiresolut. Inf. Process.* 20, 2 (2022), 2150046.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Haoling Zhang, Chao-Han Huck Yang, Hector Zenil, Narsis Aftab
    Kiani, Yue Shen, and Jesper N. Tegner. 2020b. Evolving Neural Networks through
    a Reverse Encoding Tree. In *Proc. IEEE Congr. Evol. Comput.* 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Gouza (2018) Jiawei Zhang and Fisher B Gouza. 2018. GADAM: Genetic-evolutionary
    ADAM for Deep Neural Network optimization. *arXiv preprint arXiv:1805.07500* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong,
    Jing-hui Zhong, Henry SH Chung, Yun Li, and Yu-hui Shi. 2011. Evolutionary Computation
    Meets Machine Learning: A Survey. *IEEE Comput. Intell. Mag.* 6, 4 (2011), 68–75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Kaiyu Zhang, Jinglong Chen, Shuilong He, Enyong Xu, Fudong
    Li, and Zitong Zhou. 2021a. Differentiable Neural Architecture Search Augmented
    with Pruning and Multi-objective Optimization for Time-efficient Intelligent Fault
    Diagnosis of Machinery. *Mech. Syst. Signal Process.* 158 (2021), 107773.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Kangkai Zhang, Chunhui Zhang, Shikun Li, Dan Zeng, and
    Shiming Ge. 2022a. Student Network Learning via Evolutionary Knowledge Distillation.
    *IEEE Trans. Circuits. Syst. Video Technol.* 32, 4 (2022), 2251–2263.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang (2018) Mengjie Zhang. 2018. Evolutionary Deep Learning for Image Analysis.
    (2018), https://ieeetv.ieee.org/mengjie–zhang–evolutionary–deep–learning–for–image–analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Cagnoni (2020) Mengjie Zhang and Stefano Cagnoni. 2020. Evolutionary
    Computation and Evolutionary Deep Learning for Image Analysis, Signal Processing
    and Pattern Recognition. In *Proc. Genetic Evol. Comput. Conf.* 1221–1257.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and Steven W.
    Su. 2020a. Overcoming Multi-Model Forgetting in One-Shot NAS With Diversity Maximization.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 7806–7815.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Smart (2004) Mengjie Zhang and Will Smart. 2004. Genetic Programming
    with Gradient Descent Search for Multiclass Object Classification. In *Proc. Eur.
    Conf. Genetic Program*. 399–408.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Yong Zhang, Dun-wei Gong, Xiao-yan Sun, and Yi-nan Guo.
    2017. A PSO-Based Multi-Objective Multi-Label Feature Selection Method in Classification.
    *Sci. Rep.* 7, 1 (2017), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Rockett (2005) Yang Zhang and Peter I. Rockett. 2005. Evolving Optimal
    Feature Extraction Using Multi-objective Genetic Programming: A Methodology and
    Preliminary Study on Edge Detection. In *Proc. Genetic Evol. Comput. Conf.* 795–802.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Rockett (2011) Yang Zhang and Peter I. Rockett. 2011. A Generic Optimising
    Feature Extraction Method Using Multiobjective Genetic Programming. *Appl. Soft
    Comput.* 11, 1 (2011), 1087–1097.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Yidan Zhang, Youheng Zhen, Zhenan He, and Gray G. Yen.
    2021b. Improvement of Efficiency in Evolutionary Pruning. In *Proc. Int. Joint
    Conf. Neural Netw.* 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2006) Qijun Zhao, David Zhang, and Hongtao Lu. 2006. A Direct Evolutionary
    Feature Extraction Algorithm for Classifying High Dimensional Data. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol. 1. 561–566.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2009) Qijun Zhao, David Dian Zhang, Lei Zhang, and Hongtao Lu.
    2009. Evolutionary Discriminant Feature Extraction with Application to Face Recognition.
    *EURASIP J. Adv. Signal. Process.* 2009 (2009), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2007) Tianwen Zhao, Qijun Zhao, Hongtao Lu, and David Dian Zhang.
    2007. Bagging Evolutionary Feature Extraction Algorithm for Classification. In
    *Proc. Int. Conf. Neural Comput.*, Vol. 3\. 540–545.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021a) Xun Zhou, A. K. Qin, Maoguo Gong, and Kay Chen Tan. 2021a.
    A Survey on Evolutionary Construction of Deep Neural Networks. *IEEE Trans. Evol.
    Comput.* 25, 5 (2021), 894–912.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Yao Zhou, Gary G. Yen, and Zhang Yi. 2020. Evolutionary Compression
    of Deep Neural Networks for Biomedical Image Segmentation. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 8 (2020), 2916–2929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021b) Yao Zhou, Gary G. Yen, and Zhang Yi. 2021b. Evolutionary
    Shallowing Deep Neural Networks at Block Levels. *IEEE Trans. Neural Netw. Learn.
    Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3059529](https://doi.org/10.1109/TNNLS.2021.3059529)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021c) Yao Zhou, Gary G. Yen, and Zhang Yi. 2021c. A Knee-Guided
    Evolutionary Algorithm for Compressing Deep Neural Networks. *IEEE Trans. Cybern.*
    51, 3 (2021), 1626–1638.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, and Yongjun
    Xu. 2019. EENA: Efficient Evolution of Neural Architecture. In *Proc. IEEE Int.
    Conf. Comput. Vis.* 1891–1899.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Jin (2022) Hangyu Zhu and Yaochu Jin. 2022. Real-Time Federated Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* 26, 2 (2022), 364–378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2007) Zexuan Zhu, Y. Ong, and Manoranjan Dash. 2007. Markov Blanket-Embedded
    Genetic Algorithm for Gene Selection. *Pattern Recognit.* 40, 11 (2007), 3236–3248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 8697–8710.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
