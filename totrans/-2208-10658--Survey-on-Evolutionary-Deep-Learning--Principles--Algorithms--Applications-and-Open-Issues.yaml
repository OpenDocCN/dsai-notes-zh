- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:44:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.10658] Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.10658] 关于进化深度学习的调查：原理、算法、应用及开放问题'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10658](https://ar5iv.labs.arxiv.org/html/2208.10658)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10658](https://ar5iv.labs.arxiv.org/html/2208.10658)
- en: \UseRawInputEncoding
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \UseRawInputEncoding
- en: 'Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于进化深度学习的调查：原理、算法、应用及开放问题
- en: Nan Li Northeastern UniversityNo.195, Chuangxin RoadShenyangLiaoning ProvinceChina
    [2010500@stu.neu.edu.cn](mailto:2010500@stu.neu.edu.cn) ,  Lianbo Ma Northeastern
    UniversityNo.195, Chuangxin RoadShenyang CityLiaoning ProvinceChina [malb@swc.neu.edu.cn](mailto:malb@swc.neu.edu.cn)
    ,  Guo Yu East China University of Science and TechnologyMeilong Road 130ShanghaiChina
    [guoyu@ecust.edu.cn](mailto:guoyu@ecust.edu.cn) ,  Bing Xue Victoria University
    of WellingtonWellingtonNew Zealand [bing.xue@ecs.vuw.ac.nz](mailto:bing.xue@ecs.vuw.ac.nz)
    ,  Mengjie Zhang Victoria University of WellingtonWellingtonNew Zealand [mengjie.zhang@ecs.vuw.ac.nz](mailto:mengjie.zhang@ecs.vuw.ac.nz)
     and  Yaochu Jin Bielefeld UniversityBielefeldGermany [yaochu.jin@uni-bielefeld.de](mailto:yaochu.jin@uni-bielefeld.de)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Nan Li Northeastern UniversityNo.195, Chuangxin RoadShenyangLiaoning ProvinceChina
    [2010500@stu.neu.edu.cn](mailto:2010500@stu.neu.edu.cn) ,  Lianbo Ma Northeastern
    UniversityNo.195, Chuangxin RoadShenyang CityLiaoning ProvinceChina [malb@swc.neu.edu.cn](mailto:malb@swc.neu.edu.cn)
    ,  Guo Yu East China University of Science and TechnologyMeilong Road 130ShanghaiChina
    [guoyu@ecust.edu.cn](mailto:guoyu@ecust.edu.cn) ,  Bing Xue Victoria University
    of WellingtonWellingtonNew Zealand [bing.xue@ecs.vuw.ac.nz](mailto:bing.xue@ecs.vuw.ac.nz)
    ,  Mengjie Zhang Victoria University of WellingtonWellingtonNew Zealand [mengjie.zhang@ecs.vuw.ac.nz](mailto:mengjie.zhang@ecs.vuw.ac.nz)  and  Yaochu
    Jin Bielefeld UniversityBielefeldGermany [yaochu.jin@uni-bielefeld.de](mailto:yaochu.jin@uni-bielefeld.de)
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Over recent years, there has been a rapid development of deep learning (DL)
    in both industry and academia fields. However, finding the optimal hyperparameters
    of a DL model often needs high computational cost and human expertise. To mitigate
    the above issue, evolutionary computation (EC) as a powerful heuristic search
    approach has shown significant merits in the automated design of DL models, so-called
    evolutionary deep learning (EDL). This paper aims to analyze EDL from the perspective
    of automated machine learning (AutoML). Specifically, we firstly illuminate EDL
    from machine learning and EC and regard EDL as an optimization problem. According
    to the DL pipeline, we systematically introduce EDL methods ranging from feature
    engineering, model generation, to model deployment with a new taxonomy (i.e.,
    what and how to evolve/optimize), and focus on the discussions of solution representation
    and search paradigm in handling the optimization problem by EC. Finally, key applications,
    open issues and potentially promising lines of future research are suggested.
    This survey has reviewed recent developments of EDL and offers insightful guidelines
    for the development of EDL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习（DL）在工业和学术界都取得了快速发展。然而，找到深度学习模型的最佳超参数通常需要高计算成本和人类专业知识。为了缓解上述问题，进化计算（EC）作为一种强大的启发式搜索方法，在深度学习模型的自动设计中表现出了显著的优势，这被称为进化深度学习（EDL）。本文旨在从自动化机器学习（AutoML）的角度分析EDL。具体而言，我们首先从机器学习和EC的角度阐明EDL，并将EDL视为一个优化问题。根据DL管道，我们系统地介绍了从特征工程、模型生成到模型部署的EDL方法，并提出了一种新的分类方法（即，如何演化/优化），重点讨论了在处理优化问题时EC的解表示和搜索范式。最后，提出了关键应用、开放问题以及潜在的未来研究方向。本文回顾了EDL的最新进展，并为EDL的发展提供了有价值的指导。
- en: 'deep learning, evolutionary computation, feature engineering, model generation,
    model deployment.^†^†ccs: General and reference Surveys and overviews^†^†ccs:
    Computing methodologies Machine learning algorithms^†^†ccs: Theory of computation Evolutionary
    algorithms'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习，进化计算，特征工程，模型生成，模型部署。^†^†ccs: 一般和参考 调查与概述^†^†ccs: 计算方法 机器学习算法^†^†ccs: 计算理论
    进化算法'
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Deep learning (DL) as a promising technology has been widely used in a variety
    of challenging tasks, such as image analysis (Krizhevsky et al., [2012](#bib.bib103))
    and pattern recognition (LeCun et al., [2015](#bib.bib105)). However, the practitioners
    of DL struggle to manually design deep models and find appropriate configurations
    by trial and error. An example is given in Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"), where domain knowledge is fed to DL in different stages like feature
    engineering (FE) (Xue et al., [2015](#bib.bib226)), model generation (Zhou et al.,
    [2021a](#bib.bib258)) and model deployment (Choudhary et al., [2020](#bib.bib32);
    Cheng et al., [2017](#bib.bib30)). Unfortunately, the difficulty in the acquisition
    of expert knowledge makes DL undergo a great challenge in its development.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习（DL）作为一种前景广阔的技术，已经广泛应用于各种具有挑战性的任务，如图像分析（Krizhevsky et al., [2012](#bib.bib103)）和模式识别（LeCun
    et al., [2015](#bib.bib105)）。然而，DL 的从业者在手动设计深度模型和通过试错找到合适的配置时面临困难。图中 [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") 展示了一个例子，其中领域知识在不同阶段被输入到 DL 中，例如特征工程（FE）（Xue
    et al., [2015](#bib.bib226)）、模型生成（Zhou et al., [2021a](#bib.bib258)）和模型部署（Choudhary
    et al., [2020](#bib.bib32); Cheng et al., [2017](#bib.bib30)）。不幸的是，专家知识获取的困难使得
    DL 在发展过程中面临巨大挑战。'
- en: In contrast, the automatic design of deep neural networks (DNNs) tends to be
    prevalent in recent decades (He et al., [2021](#bib.bib72); Zhou et al., [2021a](#bib.bib258)).
    The main reason lies in the flexibility and computation efficiency of automated
    machine learning (AutoML) in FE (Xue et al., [2015](#bib.bib226)), parameter optimization
    (PO) (Zhang and Gouza, [2018](#bib.bib243)), hyperparameter optimization (HPO)
    (Stanley and Miikkulainen, [2002](#bib.bib186)), neural architecture search (NAS)
    (Yao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72); Zhou et al.,
    [2021a](#bib.bib258)), and model compression (MC) (Hu et al., [2021b](#bib.bib79)).
    In this way, AutoML without manual intervention has attracted great attention
    and much progress has been made.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，深度神经网络（DNNs）的自动设计在近几十年内趋于流行（He et al., [2021](#bib.bib72); Zhou et al.,
    [2021a](#bib.bib258)）。主要原因在于自动化机器学习（AutoML）在特征工程（FE）（Xue et al., [2015](#bib.bib226)）、参数优化（PO）（Zhang
    和 Gouza, [2018](#bib.bib243)）、超参数优化（HPO）（Stanley 和 Miikkulainen, [2002](#bib.bib186)）、神经架构搜索（NAS）（Yao
    et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72); Zhou et al., [2021a](#bib.bib258)）和模型压缩（MC）（Hu
    et al., [2021b](#bib.bib79)）中的灵活性和计算效率。因此，AutoML 在没有人工干预的情况下引起了广泛关注，并取得了很大进展。
- en: '![Refer to caption](img/157981cf4645442887259fefbd7ba762.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/157981cf4645442887259fefbd7ba762.png)'
- en: Figure 1. An overview of DL, driven by domain knowledge or evolutionary computation,
    where the life of DL gets through problem, data collection, feature engineering,
    model generation and model deployment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. DL 的概述，由领域知识或进化计算驱动，其中 DL 的生命周期经历了问题、数据收集、特征工程、模型生成和模型部署。
- en: 'Evolutionary computation (EC) has been widely applied to automatic DL, owing
    to its flexibility and automatically evolving mechanism. In EC, a population of
    individuals are driven by the environmental selection to evolve towards the optimal
    solutions or front (Jin, [2006](#bib.bib89)). Nowadays, there are many automatic
    DL methods driven by EC, termed as evolutionary deep learning (EDL) (Telikani
    et al., [2021](#bib.bib197); Evans et al., [2018a](#bib.bib53); Zhang and Cagnoni,
    [2020](#bib.bib248); Zhang, [2018](#bib.bib247)). For example, a number of studies
    on EC have been carried out to the feature engineering (Xue et al., [2015](#bib.bib226)),
    model generation (Yao et al., [2018](#bib.bib231); Zhou et al., [2021a](#bib.bib258)),
    and model deployments (Choudhary et al., [2020](#bib.bib32)), as shown in Fig.
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues"). Therefore, the integration
    of EC and DL has become a hot research topic in both academic and industrial communities.
    Moreover, in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), the number
    of publications and citations referring to EC & DL by years from Web of Science
    gradually increases until around 2012, whereas it sharply rises in the following
    decade. Hence, more and more researchers work on the area of EDL.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 进化计算（EC）由于其灵活性和自动演化机制，已广泛应用于自动深度学习（DL）。在 EC 中，一组个体通过环境选择驱动向最优解或前沿演化（Jin，[2006](#bib.bib89)）。目前，许多自动
    DL 方法是由 EC 驱动的，称为进化深度学习（EDL）（Telikani 等，[2021](#bib.bib197)；Evans 等，[2018a](#bib.bib53)；Zhang
    和 Cagnoni，[2020](#bib.bib248)；Zhang，[2018](#bib.bib247)）。例如，已经有许多关于 EC 的研究涉及特征工程（Xue
    等，[2015](#bib.bib226)）、模型生成（姚等，[2018](#bib.bib231)；周等，[2021a](#bib.bib258)）和模型部署（Choudhary
    等，[2020](#bib.bib32)），如图 [1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣ 进化深度学习调查：原理、算法、应用和开放问题") 所示。因此，EC
    和 DL 的结合已成为学术界和工业界的热门研究主题。此外，在图 [2](#S1.F2 "图 2 ‣ 1\. 介绍 ‣ 进化深度学习调查：原理、算法、应用和开放问题")
    中，涉及 EC 和 DL 的出版物和引用数量从 Web of Science 起逐年增加，直到大约 2012 年，而在接下来的十年中急剧上升。因此，越来越多的研究人员开始从事
    EDL 领域的研究。
- en: '![Refer to caption](img/e00f51b178cb0629a960e5ee4e773716.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e00f51b178cb0629a960e5ee4e773716.png)'
- en: Figure 2. Total publications and citations referring to EC & DL by years from
    Web of Science until July 2022.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 从 Web of Science 到 2022 年 7 月涉及 EC 和 DL 的总出版物和引用数量。
- en: Table 1. Comparison between existing surveys and our work, where FE, PO, HPO,
    NAS, and MC indicate feature engineering, parameter optimization, hyperparameter
    optimization, neural architecture search, and model compression, respectively.
    “✓” and “-” indicate the content is included or not in the paper, respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 现有调查与我们工作的比较，其中 FE、PO、HPO、NAS 和 MC 分别表示特征工程、参数优化、超参数优化、神经架构搜索和模型压缩。 “✓”
    和 “-” 分别表示论文中是否包含该内容。
- en: '| Survey | Type | FE | PO | HPO | NAS | MC \bigstrut |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 类型 | FE | PO | HPO | NAS | MC \bigstrut |'
- en: '| (Yao et al., [2018](#bib.bib231)) | AutoML | ✓ | - | ✓ | ✓ | - \bigstrut[t]
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| (姚等，[2018](#bib.bib231)) | AutoML | ✓ | - | ✓ | ✓ | - \bigstrut[t] |'
- en: '| (He et al., [2021](#bib.bib72)) | AutoML | ✓ | - | ✓ | ✓ | - |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| (He 等，[2021](#bib.bib72)) | AutoML | ✓ | - | ✓ | ✓ | - |'
- en: '| (Yao, [1993](#bib.bib232)) | NAS | - | ✓ | ✓ | ✓ | - |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| (姚，[1993](#bib.bib232)) | NAS | - | ✓ | ✓ | ✓ | - |'
- en: '| (Ren et al., [2021](#bib.bib165)) | NAS | - | - | ✓ | ✓ | - |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| (Ren 等，[2021](#bib.bib165)) | NAS | - | - | ✓ | ✓ | - |'
- en: '| (Jaâfra et al., [2019](#bib.bib86)) | NAS | - | - | ✓ | ✓ | - |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| (Jaâfra 等，[2019](#bib.bib86)) | NAS | - | - | ✓ | ✓ | - |'
- en: '| (Santra et al., [2021](#bib.bib172)) | NAS | - | - | ✓ | ✓ | - |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| (Santra 等，[2021](#bib.bib172)) | NAS | - | - | ✓ | ✓ | - |'
- en: '| (Telikani et al., [2021](#bib.bib197)) | EDL | - | - | - | ✓ | - |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| (Telikani 等，[2021](#bib.bib197)) | EDL | - | - | - | ✓ | - |'
- en: '| (Liu et al., [2021b](#bib.bib117)) | EDL | - | ✓ | ✓ | ✓ | - |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (刘等，[2021b](#bib.bib117)) | EDL | - | ✓ | ✓ | ✓ | - |'
- en: '| (Zhou et al., [2021a](#bib.bib258)) | EDL | - | ✓ | ✓ | ✓ | - |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| (周等，[2021a](#bib.bib258)) | EDL | - | ✓ | ✓ | - |'
- en: '| (Xue et al., [2015](#bib.bib226)) | EDL | ✓ | - | - | - | - |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (Xue 等，[2015](#bib.bib226)) | EDL | ✓ | - | - | - | - |'
- en: '| (Al-Sahaf et al., [2019](#bib.bib6)) | EDL | ✓ | ✓ | - | - | - |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| (Al-Sahaf 等，[2019](#bib.bib6)) | EDL | ✓ | ✓ | - | - | - |'
- en: '| (Zhang et al., [2011](#bib.bib244)) | EDL | - | ✓ | ✓ | ✓ | - |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang 等，[2011](#bib.bib244)) | EDL | - | ✓ | ✓ | ✓ | - |'
- en: '| (Alexandropoulos and Aridas, [2019](#bib.bib8)) | EDL | ✓ | - | ✓ | ✓ | -
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| (Alexandropoulos 和 Aridas，[2019](#bib.bib8)) | EDL | ✓ | - | ✓ | ✓ | - |'
- en: '| (Mirjalili et al., [2019](#bib.bib138)) | EDL | ✓ | ✓ | ✓ | ✓ | - |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (Mirjalili 等，[2019](#bib.bib138)) | EDL | ✓ | ✓ | ✓ | ✓ | - |'
- en: '| (Darwish et al., [2020](#bib.bib41)) | EDL | - | ✓ | ✓ | ✓ | - |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| (Darwish 等， [2020](#bib.bib41)) | EDL | - | ✓ | ✓ | ✓ | - |'
- en: '| (Freitas, [2003](#bib.bib61)) | EDL | - | ✓ | ✓ | ✓ | - |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| (Freitas, [2003](#bib.bib61)) | EDL | - | ✓ | ✓ | ✓ | - |'
- en: '| Ours | EDL | ✓ | ✓ | ✓ | ✓ | ✓\bigstrut[b] |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | EDL | ✓ | ✓ | ✓ | ✓ | ✓\bigstrut[b] |'
- en: 'In Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues"), we have listed
    recent surveys on automatic DL. A large number of surveies concentrate on the
    optimization of DL models (He et al., [2021](#bib.bib72); Telikani et al., [2021](#bib.bib197);
    Zhou et al., [2021a](#bib.bib258); Xue et al., [2015](#bib.bib226)), or NAS (Liu
    et al., [2021b](#bib.bib117); Yao, [1993](#bib.bib232)). Many others focus on
    specific optimization paradigms such as reinforcement learning (RL) (Jaâfra et al.,
    [2019](#bib.bib86)), EC (Sun et al., [2020](#bib.bib192)) and gradient (Santra
    et al., [2021](#bib.bib172)). However, very few of them have systematically analysed
    EDL and runs the gamut of FE, PO, HPO, NAS, and MC. To fill the gap, we aim to
    give a comprehensive review of EDL in detail. The main contributions of this work
    are as follows.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues") 中，我们列出了关于自动深度学习的近期调查。大量的调查集中在深度学习模型的优化（He
    等，[2021](#bib.bib72)；Telikani 等，[2021](#bib.bib197)；Zhou 等，[2021a](#bib.bib258)；Xue
    等，[2015](#bib.bib226)），或神经架构搜索（Liu 等，[2021b](#bib.bib117)；Yao，[1993](#bib.bib232)）。许多其他研究关注特定的优化范式，如强化学习（RL）（Jaâfra
    等，[2019](#bib.bib86)），进化计算（EC）（Sun 等，[2020](#bib.bib192)）和梯度（Santra 等，[2021](#bib.bib172)）。然而，很少有研究系统地分析了进化深度学习（EDL）并涵盖了特征工程（FE）、超参数优化（HPO）、神经架构搜索（NAS）和模型选择（MC）。为填补这一空白，我们旨在详细综述
    EDL。这项工作的主要贡献如下。'
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Existing work on EDL is reviewed from the perspective of DL and EC to facilitate
    the understanding of readers from the communities of both ML and EC, and we also
    formulated EDL into an optimization problem from the perspective of EC.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从深度学习和进化计算的角度回顾现有的 EDL 工作，以促进机器学习和进化计算社区读者的理解，我们还从进化计算的角度将 EDL 形式化为一个优化问题。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The survey describes and discusses on EDL in terms of feature engineering, model
    generation, and model deployment from a novel taxonomy, where the solution representation
    and the search paradigms are emphasized and systematically discussed. To the best
    of our knowledge, few survey has investigated the evolutionary model deployment.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该调查从一种新的分类法角度描述和讨论了 EDL，重点强调并系统地讨论了解决方案表示和搜索范式。据我们所知，少有调查研究了进化模型部署。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: On the basis of the comprehensive review of EDL approaches, a number of applications,
    open issues and trends of EDL are discussed, which will guide the development
    of EDL.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于对 EDL 方法的全面综述，讨论了多个应用、开放问题和 EDL 的趋势，这将指导 EDL 的发展。
- en: 'The rest of this paper is organized as follows. Section [2](#S2 "2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") presents an overview of EDL. In Section
    [3](#S3 "3\. Feature Engineering ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues"), EC-driven feature engineering is presented.
    EC-driven model generation is discussed in Section [4](#S4 "4\. Model Generation
    ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues"). Section [5](#S5 "5\. Model Deployment ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues") reviews
    EC-driven model compressions. After that, relevant applications, open issues and
    the trends of EDL are discussed in Section [6](#S6 "6\. Applications, Open Issues,
    and Trends ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"). Finally, a conclusion of the paper is drawn in Section [7](#S7
    "7\. Conclusions ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '本文其余部分的组织结构如下。第[2](#S2 "2\. An Overview of Evolutionary Deep Learning ‣ Survey
    on Evolutionary Deep Learning: Principles, Algorithms, Applications and Open Issues")节介绍了进化深度学习的概述。在第[3](#S3
    "3\. Feature Engineering ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")节中介绍了EC驱动的特征工程。第[4](#S4 "4\. Model Generation ‣
    Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues")节讨论了EC驱动的模型生成。第[5](#S5 "5\. Model Deployment ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")节回顾了EC驱动的模型压缩。之后，在第[6](#S6
    "6\. Applications, Open Issues, and Trends ‣ Survey on Evolutionary Deep Learning:
    Principles, Algorithms, Applications and Open Issues")节中讨论了相关应用、开放问题和进化深度学习的趋势。最后，在第[7](#S7
    "7\. Conclusions ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")节中对本文进行了总结。'
- en: 2\. An Overview of Evolutionary Deep Learning
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 进化深度学习概述
- en: 2.1\. Deep Learning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 深度学习
- en: 'DL can be described as a triplet $M$ = ($D$, $T$, $P$) (Yao et al., [2018](#bib.bib231)),
    where $D$ is the dataset used for the training of a deep model ($M$), and $T$
    is the targeted task. $P$ indicates the performance of $M$. The aim of DL is to
    boost its performance over specific task $T$, which is measured by $P$ on dataset
    $D$. In Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we can
    see there are three fundamental processes of DL, i.e., feature engineering, model
    generation and model deployment.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习可以被描述为三元组$M$ = ($D$, $T$, $P$) (Yao et al., [2018](#bib.bib231))，其中$D$是用于训练深度模型($M$)的数据集，$T$是目标任务。$P$表示$M$的性能。深度学习的目标是提升其在特定任务$T$上的性能，这通过在数据集$D$上测量$P$来实现。在图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues")中，我们可以看到深度学习的三个基本过程，即特征工程、模型生成和模型部署。'
- en: 'Feature engineering: It aims to find a high-quality $D$ to improve the performance
    ($P$) of the deep model ($M$) on specific tasks ($T$). In practice, the feature
    space of $D$ may include redundant and noisy information, which harms the performance
    ($P$) of the model ($M$). On Prostate dataset, the size of feature subset (65)
    selected in (Tran et al., [2018](#bib.bib200)) is only 1% of the total size of
    features (10509).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程：它旨在寻找一个高质量的$D$来提升深度模型($M$)在特定任务($T$)上的性能($P$)。在实际应用中，$D$的特征空间可能包含冗余和噪声信息，这会损害模型($M$)的性能($P$)。在前列腺数据集上，(Tran
    et al., [2018](#bib.bib200))中选择的特征子集的大小（65）仅占特征总大小（10509）的1%。
- en: 'Model generation: It targets at optimizing/generating a model ($M$) with desirable
    performance ($P$) for specific task ($T$) on the given datasets ($D$) (He et al.,
    [2021](#bib.bib72)). Model generation can be further divided into parameter optimization,
    model architecture optimization, and joint optimization (Zhou et al., [2021a](#bib.bib258)).
    Parameter optimization is to search the best parameters (e.g., weights) for a
    predefined model. Architecture optimization is dedicated to finding the optimal
    network topology (e.g., number of layers and types of operations) of a deep model
    ($M$) (Luo et al., [2018](#bib.bib127)). Joint optimization involves in the above
    two optimization issues by automatically searching for a powerful model ($M$)
    on the datasets ($D$) (Miikkulainen et al., [2019](#bib.bib137)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成：旨在优化/生成具有理想性能（$P$）的模型（$M$），以执行特定任务（$T$）在给定的数据集（$D$）上（He 等， [2021](#bib.bib72)）。模型生成可以进一步分为参数优化、模型架构优化和联合优化（Zhou
    等， [2021a](#bib.bib258)）。参数优化是为预定义的模型搜索最佳参数（例如权重）。架构优化致力于寻找深度模型（$M$）的最佳网络拓扑（例如层数和操作类型）（Luo
    等， [2018](#bib.bib127)）。联合优化通过在数据集（$D$）上自动搜索强大的模型（$M$）来涉及上述两个优化问题（Miikkulainen
    等， [2019](#bib.bib137)）。
- en: 'Model deployment: This process aims to deploy a deep model ($M$) to solve a
    deployment task $T$ with acceptable performance ($P$) on input data ($D$) within
    limited computational budgets. The key issue of model deployment is how to reduce
    the latency, storage, and energy consumption when the number of parameters of
    a deep model is large, e.g., Transformer-XL Large has 257M parameters (Dowdell
    and Zhang, [2020](#bib.bib48)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署：此过程旨在将深度模型（$M$）部署以在有限的计算预算内解决一个部署任务（$T$），并在输入数据（$D$）上达到可接受的性能（$P$）。模型部署的关键问题是当深度模型的参数数量很大时如何减少延迟、存储和能耗，例如
    Transformer-XL Large 有 257M 参数（Dowdell 和 Zhang， [2020](#bib.bib48)）。
- en: 2.2\. Evolutionary Computation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 进化计算
- en: EC is a collection of stochastic population-based search methods inspired by
    evolution mechanisms such as natural selection and genetics, which does not need
    gradient information and is able to handle a black-box optimization problem without
    explicit mathematical formulations (Ma et al., [2022](#bib.bib129); Vargas-Hákim
    et al., [2022](#bib.bib204)). Owing to the above characteristics, EC has been
    widely employed to the automatic design of DL.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: EC 是一种基于随机种群的搜索方法，灵感来源于自然选择和遗传学等进化机制，它不需要梯度信息，能够处理没有明确数学公式的黑箱优化问题（Ma 等， [2022](#bib.bib129);
    Vargas-Hákim 等， [2022](#bib.bib204)）。由于上述特点，EC 已被广泛应用于深度学习的自动设计。
- en: '![Refer to caption](img/43778ec24fd8eb2d2ec7491b539a713d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/43778ec24fd8eb2d2ec7491b539a713d.png)'
- en: Figure 3. A general framework of EC.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. EC 的通用框架。
- en: 'In principle, we can broadly divide EC methods into two categories: evolutionary
    algorithms (EA) and swarm intelligence (SI) (Zhou et al., [2021a](#bib.bib258)).
    Our work doesn’t make an explicit distinction between EAs and SI since they comply
    with a general framework, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary
    Computation ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), which consists
    of three main components.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '原则上，我们可以将 EC 方法大致分为两类：进化算法（EA）和群体智能（SI）（Zhou 等， [2021a](#bib.bib258)）。我们的工作没有明确区分
    EA 和 SI，因为它们符合一个通用框架，如图 [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary Computation
    ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues") 所示，该框架由三个主要组件组成。'
- en: 'Initialization:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: is performed to generate a population of individuals which are encoded according
    to the decision space (or search space and variable space) of the optimization
    problem, such as the feature set, model parameters and topological structure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 进行以生成一群个体，这些个体根据优化问题的决策空间（或搜索空间和变量空间）进行编码，如特征集、模型参数和拓扑结构。
- en: 'Evaluation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：
- en: aims to calculate the fitness of individuals. In fact, the evaluation of the
    individuals in EDL is a computationally expensive task (Miahi et al., [2022](#bib.bib136)).
    For example, the work (Real et al., [2017](#bib.bib163)) used 3000 GPU days to
    find a desirable architecture.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在计算个体的适应度。实际上，在 EDL 中评估个体是一个计算密集的任务（Miahi 等， [2022](#bib.bib136)）。例如，工作（Real
    等， [2017](#bib.bib163)）使用了 3000 个 GPU 天来寻找理想的架构。
- en: 'Updating:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更新：
- en: aims to generates a number of offspring solutions through various reproduction
    operations. For example, a new soultion is generated via velocity and position
    formula in particle swarm optimization (PSO) (Tran et al., [2018](#bib.bib200)).
    In terms of genetic algorithm (GA), some reproduction operators (e.g., crossover
    and mutation) are used to generate new individuals (Vafaie and De Jong, [1998](#bib.bib203)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在通过各种繁殖运算生成许多后代解决方案。例如，通过粒子群优化（PSO）中的速度和位置公式生成功能解（Tran等人，[2018](#bib.bib200)）。对于遗传算法（GA），使用一些繁殖算子（例如交叉和变异）来生成新的个体（Vafaie和De Jong，[1998](#bib.bib203)）。
- en: 2.3\. Evolutionary Deep Learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 进化深度学习
- en: 2.3.1\. EDL from two perspectives
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. EDL的两个视角
- en: In contrast to traditional DL which heavily relies on expert or domain knowledge
    to build deep model, EDL is to automatically design the deep model through an
    evolutionary process (Sun et al., [2020](#bib.bib192); Yao, [1993](#bib.bib232);
    Ren et al., [2021](#bib.bib165); Zhang, [2018](#bib.bib247)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的依赖专家或领域知识构建深度模型的DL相比，EDL是通过进化过程自动设计深度模型的。（Sun等人，[2020](#bib.bib192); Yao，[1993](#bib.bib232);
    Ren等人，[2021](#bib.bib165); Zhang，[2018](#bib.bib247)）。
- en: 'From the perspective of DL: Traditional DL needs a lot of expert knowledge
    in inventing and analysing a learning tool to a specific dataset or task. In contrast,
    EDL can be seen as a human-friendly learning tool that can automatically find
    appropriate deep models on given datasets or tasks (Yao et al., [2018](#bib.bib231)).
    In other words, EDL concentrates on how easy a learning tool can be used.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从DL的角度来看：传统的DL需要很多专家知识来发明和分析特定数据集或任务的学习工具。相反，EDL可以被看作是一个人性化的学习工具，可以在给定的数据集或任务上自动找到合适的深度模型（Yao等人，[2018](#bib.bib231)）。换句话说，EDL关注的是学习工具的易用性。
- en: 'From the perspective of EC: The configurations of a model is represented as
    an individual, and the performance as the objective to be optimized. EC plays
    an important role in the optimization driven by evolutionary mechanisms. Namely,
    EDL can be seen as an evolutionary optimization process to find the optimal configurations
    of the deep model with high performance.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从EC的角度：模型的配置表示为一个个体，性能作为要优化的目标。 EC在以进化机制驱动的优化中起着重要作用。也就是说，EDL可以被看作是一个进化优化过程，以找到具有高性能的深度模型的最优配置。
- en: From the above analysis, EDL not only aims to increase the adaptability of a
    deep model towards learning tasks via the automatic construction approach (from
    the perspective of DL), but also tries to achieve the optimal model under the
    designed objectives or constraints (from the perspective of EC).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述分析可知，EDL不仅通过自动构建方法（从DL的角度）旨在增加深度模型对学习任务的适应性，而且还试图在设计的目标或约束条件下实现最优模型（从EC的角度）。
- en: 2.3.2\. Definition and Framework of EDL
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. EDL的定义和框架
- en: 'According to the above discussion in Subsection [2.3.1](#S2.SS3.SSS1 "2.3.1\.
    EDL from two perspectives ‣ 2.3\. Evolutionary Deep Learning ‣ 2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") and following (Yao et al., [2018](#bib.bib231)),
    we can define EDL as follows.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[2.3.1](#S2.SS3.SSS1 "2.3.1\. EDL from two perspectives ‣ 2.3\. Evolutionary
    Deep Learning ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")小节中的讨论和随后的（Yao等人，[2018](#bib.bib231)）我们可以如下定义EDL。'
- en: '| (1) |  | <math   alttext="\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&amp;{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&amp;{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '| (1) |  | <math   alttext="\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&amp;{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&amp;{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\'
- en: '{{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd
    columnalign="left"  ><mtable columnspacing="5pt" displaystyle="true"  ><mtr ><mtd
    ><munder  ><mo movablelimits="false"  >Max</mo><mrow ><mrow ><mi >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >f</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi></mrow><mo lspace="0em"  >.</mo></mrow></munder></mtd><mtd
    ><mtext >Learning tools’ performance,</mtext></mtd></mtr></mtable></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mtable columnspacing="5pt" displaystyle="true" ><mtr
    ><mtd  ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em" >.</mo><mi
    >t</mi></mrow><mo lspace="0em"  >.</mo></mrow></mtd><mtd ><mrow ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><mtext
    >No assistance from humans</mtext></mtd></mtr><mtr ><mtd ><mtext >Limited computational
    budgets.</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><matrix ><matrixrow  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >Max</ci><list ><apply ><ci
    >𝑐</ci><ci >𝑜</ci><ci >𝑛</ci><ci >𝑓</ci><ci >𝑖</ci><ci >𝑔</ci></apply></list></apply><ci
    ><mtext >Learning tools’ performance,</mtext></ci><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow><matrixrow
    ><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><ci
    >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix
    ><matrixrow ><ci ><mtext >No assistance from humans</mtext></ci><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><ci ><mtext >Limited computational budgets.</mtext></ci><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&{{\textrm{Learning
    tools'' performance,}}}\end{array}\\ \begin{array}[]{*{20}{c}}{s.t.}&{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{No
    assistance from humans}}}\\ {{\textrm{Limited computational budgets.}}}\end{array}}\right.}\end{array}\end{array}</annotation></semantics></math>
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{array}[]{l}\begin{array}[]{*{20}{c}}{\mathop{{\rm{Max}}}\limits_{config.}}&{{\textrm{学习工具的性能，}}}\end{array}\\
    \begin{array}[]{*{20}{c}}{s.t.}&{\left\{{\begin{array}[]{*{20}{c}}{{\textrm{没有人工帮助}}}\\
    {{\textrm{有限的计算预算。}}}\end{array}}\right.}\end{array}\end{array}
- en: where $config.$ indicates the configurations which form the decision space of
    an optimization problem. The problem is to maximize the objective (i.e., learning
    tools’ performance $P$) of tasks $T$ on datasets $D$ under the constraints of
    no assistance from humans and limited computational resources. Accordingly, three
    aspects are taken into account in the design of EDL.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $config.$ 表示形成优化问题决策空间的配置。问题是最大化任务 $T$ 在数据集 $D$ 上的目标（即学习工具的性能 $P$），在没有人类帮助和有限计算资源的约束下。因此，EDL
    的设计考虑了三个方面。
- en: 'Desirable generalization performance::'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '良好的泛化性能::'
- en: EDL should have desirable generalization performance across given datasets and
    tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: EDL 应该在给定的数据集和任务上具有良好的泛化性能。
- en: 'High search efficiency::'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '高搜索效率::'
- en: EDL is able to find optimal or desirable configuration within a limited computational
    budges (e.g., hardware, latency, energy consumption) under different designed
    objectives (e.g., high accuracy, small model size).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: EDL 能够在不同设计目标（例如，高准确率、小模型尺寸）的限制下，在有限的计算预算（例如，硬件、延迟、能耗）内找到最佳或理想的配置。
- en: 'Without human assistance::'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '无需人工干预::'
- en: EDL is able to automatically configure without human intervention.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: EDL 能够在没有人工干预的情况下自动配置。
- en: 'Following the EC framework described in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary
    Computation ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues"), we present
    a general framework of EDL as follows.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '按照图 [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary Computation ‣ 2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") 中描述的 EC 框架，我们提出了如下的 EDL 通用框架。'
- en: 'Step 1 Initialization::'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 1 初始化::'
- en: A population of individuals are initialized according to the designed encoding
    scheme.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据设计的编码方案初始化一个个体群体。
- en: 'Step 2 Evaluation::'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 2 评估::'
- en: Each individual is evaluated according to the objectives (e.g., high accuracy,
    small model size) or constraints (e.g., energy consumption).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目标（例如，高准确率、小模型尺寸）或约束（例如，能耗）评估每个个体。
- en: 'Step 3 Updating::'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 3 更新::'
- en: A required number of new solutions are generated from previous generation via
    various updating operations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一代生成所需数量的新解决方案，通过各种更新操作。
- en: 'Step 4 Termination condition::'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 4 终止条件::'
- en: Go to Step 2 if the predefined termination condition is unsatisfied; Otherwise,
    go to Step 5.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未满足预定义的终止条件，则进入步骤 2；否则，进入步骤 5。
- en: 'Step 5 Output::'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤 5 输出::'
- en: Output the solution with the best performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出性能最佳的解决方案。
- en: 2.3.3\. Taxonomy of EDL Approaches
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. EDL 方法的分类
- en: 'In this section, a novel taxonomy of EDL approaches is proposed according to
    “what to evolve/optimize” and “how to evolve/optimize”, as shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.3.3\. Taxonomy of EDL Approaches ‣ 2.3\. Evolutionary Deep Learning
    ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '本节根据“要进化/优化的内容”和“如何进化/优化”提出了一种新的 EDL 方法分类，如图 [4](#S2.F4 "Figure 4 ‣ 2.3.3\.
    Taxonomy of EDL Approaches ‣ 2.3\. Evolutionary Deep Learning ‣ 2\. An Overview
    of Evolutionary Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") 所示。'
- en: '![Refer to caption](img/dc42ccf964acca317096d7d4d08a8c05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc42ccf964acca317096d7d4d08a8c05.png)'
- en: Figure 4. A taxonomy of EDL approaches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. EDL 方法的分类。
- en: '“What to evolve/optimize”: We may be concerned about “what EDL can do” or “what
    kinds of problems EDL can tackle”. In feature engineering, there are three key
    issues to be resolved, including the feature selection, feature construction and
    feature extraction (Yao et al., [2018](#bib.bib231)). In model generation, parameter
    optimization, architecture optimization, and joint optimization become the critical
    issues (Zhou et al., [2021a](#bib.bib258)), while model deployment is involved
    with the issues of model pruning and other compression technologies.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “要进化/优化的内容”：我们可能关注“EDL 能做什么”或“EDL 可以解决哪些问题”。在特征工程中，有三个关键问题需要解决，包括特征选择、特征构造和特征提取
    (Yao et al., [2018](#bib.bib231))。在模型生成中，参数优化、架构优化和联合优化成为关键问题 (Zhou et al., [2021a](#bib.bib258))，而模型部署涉及模型剪枝和其他压缩技术的问题。
- en: '“How to evolve/optimize”: The answer to the question is designing appropriate
    solution representation and search paradigm for EC, and acceleration strategies
    for NAS. The representation schemes are designed for the encoding of individuals,
    search paradigms for the achievement of optimal configurations, acceleration strategies
    for the reduction of time or resources consumption.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: “如何进化/优化”：这个问题的答案是为EC设计适当的解决方案表示和搜索范式，以及NAS的加速策略。表示方案用于个体的编码，搜索范式用于实现最佳配置，加速策略用于减少时间或资源消耗。
- en: 'According to the above taxonomy, we will elaborately introduce EDL in feature
    engineering, model generation and model deployment in Sections [3](#S3 "3\. Feature
    Engineering ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues"), [4](#S4 "4\. Model Generation ‣ Survey on Evolutionary Deep
    Learning: Principles, Algorithms, Applications and Open Issues") and [5](#S5 "5\.
    Model Deployment ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), respectively.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述分类，我们将在第[3](#S3 "3\. 特征工程 ‣ 进化深度学习综述：原理、算法、应用及开放问题")、[4](#S4 "4\. 模型生成 ‣
    进化深度学习综述：原理、算法、应用及开放问题")和[5](#S5 "5\. 模型部署 ‣ 进化深度学习综述：原理、算法、应用及开放问题")节中详细介绍特征工程中的EDL、模型生成和模型部署。
- en: 3\. Feature Engineering
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 特征工程
- en: Feature engineering is adopted to pre-process given raw data by filtering out
    the irrelevant features of the data or creating the new features based on original
    features (Xue et al., [2015](#bib.bib226)). Various EC-based techniques have been
    proposed to reduce data dimensionality, speed up learning process, or improve
    model performance (Xue et al., [2015](#bib.bib226)). The common techniques can
    be categorized into feature selection (Nguyen et al., [2014](#bib.bib147)), feature
    construction (Bhanu and Krawiec, [2002](#bib.bib17)) and feature extraction (Peng
    et al., [2021](#bib.bib153)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程用于通过筛选数据中的无关特征或基于原始特征创建新特征来预处理给定的原始数据（Xue 等，[2015](#bib.bib226)）。已经提出了各种基于进化计算（EC）的技术，以减少数据维度，加速学习过程或提高模型性能（Xue
    等，[2015](#bib.bib226)）。常见的技术可以分为特征选择（Nguyen 等，[2014](#bib.bib147)）、特征构造（Bhanu
    和 Krawiec，[2002](#bib.bib17)）和特征提取（Peng 等，[2021](#bib.bib153)）。
- en: 3.1\. Feature Selection
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 特征选择
- en: 3.1.1\. Problem Formulation
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 问题表述
- en: Feature selection aims to automatically select a representative subset of features
    where there are no irrelevant or redundant features. However, the search space
    grows exponentially with the increase of features. If a dataset has $n$ features,
    then there are $2^{n}$ solutions in the search space. In addition, the interactions
    between features may seriously impact the feature selection performance (Xue et al.,
    [2015](#bib.bib226)). In the followings, we will review existing work on solution
    representations and search paradigms in EC for feature selection.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择旨在自动选择一个具有代表性的特征子集，其中没有无关或冗余的特征。然而，随着特征数量的增加，搜索空间呈指数增长。如果一个数据集有 $n$ 个特征，那么搜索空间中有
    $2^{n}$ 个解。此外，特征之间的相互作用可能会严重影响特征选择性能（Xue 等，[2015](#bib.bib226)）。接下来，我们将回顾在EC中用于特征选择的解决方案表示和搜索范式的现有工作。
- en: 3.1.2\. Solution Representations
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 解决方案表示
- en: Generally, there are three different categories of solution representations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有三种不同的解决方案表示类别。
- en: 'Linear encoding: This encoding uses vectors or strings to store feature information.
    For example, in (Estévez and Caballero, [1998](#bib.bib52)), a fixed-length binary
    vector was used to express whether a feature is selected or not, where “1” indicates
    a corresponding feature is selected, and “0” is the opposite. In (Hong and Cho,
    [2006](#bib.bib75)), a binary index was used to indicate the corresponding feature.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 线性编码：这种编码使用向量或字符串来存储特征信息。例如，在（Estévez 和 Caballero，[1998](#bib.bib52)）中，使用固定长度的二进制向量来表示特征是否被选择，其中“1”表示对应的特征被选择，“0”则表示相反。在（Hong
    和 Cho，[2006](#bib.bib75)）中，使用二进制索引来表示对应的特征。
- en: 'Tree-based encoding: In canonical genetic programming (GP), all leaf nodes/terminal
    nodes represent the selected features and non-terminal nodes represent functions
    (e.g., arithmetic or logic operators) (Krawiec, [2002](#bib.bib101)). For automatic
    classification on high-dimensional data, Krawiec et al. (Krawiec, [2002](#bib.bib101))
    proposed a tree-based encoding to select a subset of highly discriminative features,
    where each feature consisted of sibling leaf nodes and their paternal function
    node. On the basis of the tree-based encoding, Muni et al. (Muni et al., [2006](#bib.bib141))
    proposed a multi-tree GP mothed for online feature selection.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于树的编码**：在经典遗传编程（GP）中，所有叶节点/终端节点代表所选特征，而非终端节点则代表函数（例如，算术或逻辑运算符）（Krawiec，[2002](#bib.bib101)）。针对高维数据的自动分类，Krawiec
    等人（Krawiec，[2002](#bib.bib101)）提出了一种基于树的编码方法，用于选择具有高度区分性的特征子集，其中每个特征由兄弟叶节点及其父功能节点组成。在基于树的编码的基础上，Muni
    等人（Muni 等，[2006](#bib.bib141)）提出了一种用于在线特征选择的多树 GP 方法。'
- en: 'Graph-based encoding: In (O’Boyle and Palmer, [2008](#bib.bib148)), the feature
    space of the high-dimensional data is represented by a graph and each node of
    the graph represents a feature. A feature subset is composed of visited nodes
    of the graph, i.e., the path of node composition or subgraph. Yu et al. (Yu et al.,
    [2009](#bib.bib236)) converted feature selection to the optimal path problem in
    a directed graph, where the value of the node was “1” or “0” to indicate whether
    the feature was selected or not.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于图的编码**：在（O’Boyle 和 Palmer，[2008](#bib.bib148)）中，高维数据的特征空间由一个图表示，图中的每个节点代表一个特征。特征子集由图中的访问节点组成，即节点组合的路径或子图。Yu
    等人（Yu 等，[2009](#bib.bib236)）将特征选择转化为有向图中的最优路径问题，其中节点的值为“1”或“0”以表示特征是否被选择。'
- en: 3.1.3\. Search Paradigms
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**3.1.3 搜索范式**'
- en: In feature selection, representative types of search paradigms are introduced
    as follows.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择中，介绍了以下几种代表性的搜索范式。
- en: 'Basic EC search paradigm: In feature selection, typical evolutionary search
    methods have been widely used, such as GA (Da Silva and Neto, [2011](#bib.bib37);
    Estévez and Caballero, [1998](#bib.bib52)), GP (Krawiec, [2002](#bib.bib101);
    Nekrasov et al., [2020](#bib.bib143)), PSO (Vieira et al., [2013](#bib.bib205);
    Nguyen et al., [2014](#bib.bib147)), ant colony optimization (ACO) (Khushaba et al.,
    [2008](#bib.bib96); Ma et al., [2021c](#bib.bib131)), and artificial bee colony
    (ABC) (Wang et al., [2020d](#bib.bib211)). Besides, some other studies (Khushaba
    et al., [2008](#bib.bib96)) combined ACO with DE to seek optimal feature subsets,
    where the solutions searched by the ACO were fed into the DE to further explore
    the optimal solution. In (Da Silva and Neto, [2011](#bib.bib37)), a family of
    feature selection methods based on different variants of GA were developed to
    improve the accuracy of content-based image retrieval systems.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**基本进化计算搜索范式**：在特征选择中，已经广泛使用了典型的进化搜索方法，如 GA（Da Silva 和 Neto，[2011](#bib.bib37)；Estévez
    和 Caballero，[1998](#bib.bib52)），GP（Krawiec，[2002](#bib.bib101)；Nekrasov 等，[2020](#bib.bib143)），PSO（Vieira
    等，[2013](#bib.bib205)；Nguyen 等，[2014](#bib.bib147)），蚁群优化（ACO）（Khushaba 等，[2008](#bib.bib96)；Ma
    等，[2021c](#bib.bib131)），以及人工蜂群（ABC）（Wang 等，[2020d](#bib.bib211)）。此外，一些其他研究（Khushaba
    等，[2008](#bib.bib96)）将 ACO 与 DE 结合使用以寻找最优特征子集，其中 ACO 搜索到的解被输入到 DE 中以进一步探索最优解。在（Da
    Silva 和 Neto，[2011](#bib.bib37)）中，开发了一系列基于不同变种的 GA 的特征选择方法，以提高基于内容的图像检索系统的准确性。'
- en: 'Co-evolution search paradigm: In co-evolution search paradigm for feature selection,
    at least two populations are simultaneously evolved and interacted toward the
    optimal subset of features (Wen and Xu, [2011](#bib.bib214); Rashid et al., [2020](#bib.bib160)).
    For example, a divide-and-conquer strategy was developed in (Wen and Xu, [2011](#bib.bib214))
    to manage two subpopulations. One subpopulation was to conduct an evolution process
    of classifier design, while the other one was to search for an optimal subset
    of features.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**协同进化搜索范式**：在特征选择的协同进化搜索范式中，至少有两个种群同时进行进化并相互作用，以找到最优的特征子集（Wen 和 Xu，[2011](#bib.bib214)；Rashid
    等，[2020](#bib.bib160)）。例如，（Wen 和 Xu，[2011](#bib.bib214)）中开发了一种分而治之的策略来管理两个子种群。一个子种群负责进行分类器设计的进化过程，而另一个子种群则负责搜索最优的特征子集。'
- en: 'Multi-objective search paradigm: This type of search paradigms are driven by
    two or more conflicting objectives (Xue et al., [2012](#bib.bib225); Hancer et al.,
    [2015](#bib.bib71); Cheng et al., [2021](#bib.bib29)), such as the maximization
    of the accuracy of a classifier and minimization of the size of a feature subset.
    On the basis of the above two conflicting objectives, Xue et al. (Xue et al.,
    [2012](#bib.bib225)) designed a multi-objective PSO algorithm for feature selection
    and obtained a set of Pareto non-dominated candidate solutions for feature selection
    after the multi-objective search.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标搜索范式：这类搜索范式由两个或更多冲突的目标驱动（Xue et al., [2012](#bib.bib225); Hancer et al.,
    [2015](#bib.bib71); Cheng et al., [2021](#bib.bib29)），例如分类器准确度的最大化和特征子集大小的最小化。在这两个冲突目标的基础上，Xue
    et al.（Xue et al., [2012](#bib.bib225)）设计了一个多目标PSO算法用于特征选择，并在多目标搜索后获得了一组帕累托非支配的候选解用于特征选择。
- en: 3.1.4\. Summary
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 总结
- en: GA and GP are widely applied to feature selection. GA early serves for low-dimensional
    (i.e., $\leq$1000) datasets (Xue et al., [2015](#bib.bib226); Hosni et al., [2020](#bib.bib77)).
    Recently, many GA-based approaches have been proposed to solve high-dimensional
    feature selection (Cheng et al., [2021](#bib.bib29)). Nevertheless, GP is commonly
    applied to large-scale/high-dimensional feature selection since it is flexible
    in feature representation (Xue et al., [2015](#bib.bib226)). Especially, GP outperforms
    GA on some small but high-dimensional datasets, e.g., Brain Tumor-2 (Chen et al.,
    [2022](#bib.bib24)) with 10367 features but only 50 samples. In addition, PSO
    has been proved with faster convergence rate to an optimal feature subset than
    GAs and GP (Zhang et al., [2017](#bib.bib251)). The graph representation of ACO
    outperforms GA and GP on flexibility, but the challenge of ACO is how to design
    appropriate graph encoding for large-scale scenarios (Telikani et al., [2021](#bib.bib197);
    Xue et al., [2015](#bib.bib226)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: GA和GP广泛应用于特征选择。GA早期用于低维（即，$\leq$1000）数据集（Xue et al., [2015](#bib.bib226); Hosni
    et al., [2020](#bib.bib77)）。最近，许多基于GA的方法被提出用于解决高维特征选择（Cheng et al., [2021](#bib.bib29)）。然而，GP通常应用于大规模/高维特征选择，因为它在特征表示上具有灵活性（Xue
    et al., [2015](#bib.bib226)）。特别是，GP在一些小但高维的数据集上优于GA，例如，具有10367个特征但仅50个样本的Brain
    Tumor-2（Chen et al., [2022](#bib.bib24)）。此外，PSO已被证明比GA和GP具有更快的收敛速度（Zhang et al.,
    [2017](#bib.bib251)）。ACO的图表示在灵活性上优于GA和GP，但ACO的挑战在于如何为大规模场景设计合适的图编码（Telikani et
    al., [2021](#bib.bib197); Xue et al., [2015](#bib.bib226)）。
- en: 3.2\. Feature Construction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 特征构建
- en: 3.2.1\. Problem Formulation
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 问题表述
- en: Feature construction is to create new high-level features from the original
    features (Tran et al., [2016](#bib.bib202)) via appropriate function operators
    (e.g., conjunction and average) (Xue et al., [2013](#bib.bib227); Neshatian et al.,
    [2012](#bib.bib145)), so that the high-level features are more easily discriminative
    than the original ones. Feature construction is a complicated combinatorial optimization
    problem, where search space increases exponentially along with the total number
    of original features and the function operators. In the following subsections,
    we will describe the EC-based feature construction methods in terms of both solution
    representations and search paradigms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 特征构建是通过适当的函数操作符（例如，连接和平均）（Xue et al., [2013](#bib.bib227); Neshatian et al.,
    [2012](#bib.bib145)）从原始特征（Tran et al., [2016](#bib.bib202)）创建新的高级特征，使得高级特征比原始特征更容易区分。特征构建是一个复杂的组合优化问题，其中搜索空间随着原始特征和函数操作符的总数呈指数增长。在以下小节中，我们将描述基于EC的特征构建方法，包括解的表示和搜索范式。
- en: 3.2.2\. Solution Representations
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 解的表示
- en: Existing EC-based approaches for feature construction can be categorized into
    three groups.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的基于EC的方法可以分为三类。
- en: 'Linear encoding: The study (Xue et al., [2013](#bib.bib227)) used $n$-bit ($n$
    is the total number of original features) binary vector to represent each particle,
    where “0” indicated the corresponding feature not applied to build the new high-level
    feature while “1” was in the opposite. On the basis of the encoding, a local search
    was performed to select candidate operators from a predefined function set to
    construct a new high-level feature.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 线性编码：研究（Xue et al., [2013](#bib.bib227)）使用$n$-bit（$n$是原始特征的总数）二进制向量来表示每个粒子，其中“0”表示相应的特征未被用于构建新的高级特征，而“1”则相反。在此编码的基础上，进行了局部搜索，从预定义的函数集中选择候选操作符以构建新的高级特征。
- en: 'Tree-based encoding: Tree-based encoding is natural for feature construction,
    where leaf nodes represent the feature information and internal nodes represent
    operators. Many studies (Tran et al., [2016](#bib.bib202); Bhanu and Krawiec,
    [2002](#bib.bib17)) have demonstrated the effectiveness of tree encoding in feature
    construction. For example, Bhanu et al. (Bhanu and Krawiec, [2002](#bib.bib17))
    designed a GP-based coevolutionary feature construction procedure to improve the
    discriminative ability of classifiers. In (Tran et al., [2016](#bib.bib202)),
    an individual in EC was represented by a multi-tree encoding with multiple high-level
    features.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的编码：基于树的编码对特征构建来说是自然的，其中叶节点表示特征信息，内部节点表示操作符。许多研究（Tran等，[2016](#bib.bib202)；Bhanu和Krawiec，[2002](#bib.bib17)）已经证明了树编码在特征构建中的有效性。例如，Bhanu等（Bhanu和Krawiec，[2002](#bib.bib17)）设计了一种基于GP的协同进化特征构建程序，以提高分类器的区分能力。在（Tran等，[2016](#bib.bib202)）中，EC中的一个个体由多棵树编码表示，具有多个高级特征。
- en: 'Graph-based encoding: In this encoding, the nodes and edges represent features
    and operators (e.g., “+”, “-”, “*”, “/”), respectively. Teller et al. (Teller
    and Veloso, [1996](#bib.bib198)) applied an arbitrary directed graph to represent
    all features and operators, where each possible high-level feature can be represented
    as a subgraph of this directed graph. For linear GP, features and operations form
    a many-to-many directed acyclic graph, in which each feature is loaded into predefined
    registers and register’s value can be used in multiple operators (Fogelberg and
    Zhang, [2005](#bib.bib58)). However, graph encoding becomes inefficient on high-dimensional
    feature sets since the complexity of graph traversal exacerbates the difficulty
    of feature construction.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的编码：在这种编码中，节点和边分别表示特征和操作符（例如，“+”，“ -”，“*”，“/”）。Teller等（Teller和Veloso，[1996](#bib.bib198)）应用任意有向图表示所有特征和操作符，其中每个可能的高级特征可以表示为这个有向图的一个子图。对于线性GP，特征和操作形成一个多对多的有向无环图，其中每个特征被加载到预定义的寄存器中，寄存器的值可以在多个操作符中使用（Fogelberg和Zhang，[2005](#bib.bib58)）。然而，图编码在高维特征集上变得低效，因为图遍历的复杂性加剧了特征构建的难度。
- en: 3.2.3\. Search Paradigms
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 搜索范式
- en: There are four categories of search paradigms for feature construction in existing
    work .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现有研究中的特征构建有四种搜索范式。
- en: 'Basic EC search paradigm: Existing studies include but are not limited to GA
    (Vafaie and De Jong, [1998](#bib.bib203)), and GP (Tran et al., [2016](#bib.bib202);
    Neshatian et al., [2007](#bib.bib146); Tariq et al., [2018](#bib.bib196)). For
    example, the work (Neshatian et al., [2007](#bib.bib146)) designed GP-based feature
    construction to reduce the feature (input) dimensions of a classifier. Especially,
    GP has been also widely used to construct new features, where each individual
    following the form of a GP tree usually represents a constructed high-level feature
    (García et al., [2011](#bib.bib63)).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基础EC搜索范式：现有研究包括但不限于GA（Vafaie和De Jong，[1998](#bib.bib203)）和GP（Tran等，[2016](#bib.bib202)；Neshatian等，[2007](#bib.bib146)；Tariq等，[2018](#bib.bib196)）。例如，工作（Neshatian等，[2007](#bib.bib146)）设计了基于GP的特征构建以减少分类器的特征（输入）维度。特别是，GP也被广泛用于构建新特征，其中每个遵循GP树形式的个体通常代表一个构建的高级特征（García等，[2011](#bib.bib63)）。
- en: 'Co-evolution search paradigm: It can be decomposed to feature construction
    subproblem and classifier design subproblem, and each subproblem is solved with
    a standalone subpopulation by an EC-based method (Bhanu and Krawiec, [2002](#bib.bib17);
    Roberts and Claridge, [2005](#bib.bib166)). For example, the study (Roberts and
    Claridge, [2005](#bib.bib166)) decomposed feature construction into two subproblems
    (i.e., feature construction, and object detection), where the feature construction
    was solved by evolving a population of pixel (i.e., feature) and the object detection
    was optimized using object detection algorithm (ODA) (Roberts and Claridge, [2005](#bib.bib166)).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 协同进化搜索范式：它可以被分解为特征构建子问题和分类器设计子问题，每个子问题由EC方法的独立子种群解决（Bhanu和Krawiec，[2002](#bib.bib17)；Roberts和Claridge，[2005](#bib.bib166)）。例如，研究（Roberts和Claridge，[2005](#bib.bib166)）将特征构建分解为两个子问题（即特征构建和目标检测），其中特征构建通过进化像素（即特征）种群来解决，而目标检测则通过目标检测算法（ODA）（Roberts和Claridge，[2005](#bib.bib166)）进行优化。
- en: 'Multi-features construction search paradigm: Unlike early methods (Tran et al.,
    [2016](#bib.bib202); Vafaie and De Jong, [1998](#bib.bib203); Shafti and Pérez,
    [2008](#bib.bib176); Tran et al., [2019](#bib.bib201)) only constructing one high-level
    feature in a single search process, this sort of paradigms are able to create
    multiple high-level features. For example, Ahmed et al. (Ahmed et al., [2014](#bib.bib4))
    employed Fisher criterion together with $p$-value measure as the discriminant
    information between classes, based on which multiple features were constructed
    through multiple GP trees.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 多特征构建搜索范式：与早期方法（Tran et al., [2016](#bib.bib202); Vafaie 和 De Jong, [1998](#bib.bib203);
    Shafti 和 Pérez, [2008](#bib.bib176); Tran et al., [2019](#bib.bib201)）仅在单次搜索过程中构建一个高级特征不同，这类范式能够创建多个高级特征。例如，Ahmed
    et al.（Ahmed et al., [2014](#bib.bib4)）利用Fisher准则和$p$-值度量作为类别间的判别信息，基于此通过多个GP树构建了多个特征。
- en: 'Multi-objective evolutionary search paradigm: In this search paradigm, the
    number of features and classification accuracy are commonly taken into account
    as the objective functions for multi-objective evolutionary optimization (Hammami
    et al., [2018](#bib.bib69); Castelli et al., [2011](#bib.bib20)). Especially,
    Hammami et al. (Hammami et al., [2018](#bib.bib69)) constructed a set of high-level
    features by optimizing a multi-objective optimization problem (MOP) with three
    objectives (i.e., the number of features, the mutual information, and classification
    accuracy) with Pareto dominance relationship.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标进化搜索范式：在这一搜索范式中，特征数量和分类准确率通常作为多目标进化优化的目标函数（Hammami et al., [2018](#bib.bib69);
    Castelli et al., [2011](#bib.bib20)）。特别是，Hammami et al.（Hammami et al., [2018](#bib.bib69)）通过优化一个具有三个目标（即特征数量、互信息和分类准确率）的多目标优化问题（MOP）构建了一组高级特征，并使用了帕累托主导关系。
- en: 3.2.4\. Summary
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 总结
- en: GP-based approaches are popular in feature construction due to the flexible
    representation of features and operations. In addition, the hybrid of evolutionary
    algorithms also attracts much attention for feature construction. However, there
    is still plenty of room for the improvement of efficiency in constructing features
    in high-dimensional or large-scale scenarios, where a large number of computational
    resources are needed (Tariq et al., [2018](#bib.bib196); Tran et al., [2019](#bib.bib201)).
    Notably, feature construction often requires more computational overhead than
    feature selection, since feature construction commonly performs after the feature
    selection and the quality of the selected features may influence the performance
    of feature construction.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GP的方法在特征构建中很受欢迎，因为它们对特征和操作的表示灵活。此外，进化算法的混合也受到特征构建的广泛关注。然而，在高维或大规模场景下构建特征的效率仍有很大的提升空间，因为这些场景需要大量的计算资源（Tariq
    et al., [2018](#bib.bib196); Tran et al., [2019](#bib.bib201)）。值得注意的是，特征构建通常需要比特征选择更多的计算开销，因为特征构建通常在特征选择之后进行，所选特征的质量可能会影响特征构建的性能。
- en: 3.3\. Feature Extraction
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 特征提取
- en: 3.3.1\. Problem Formulation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 问题定义
- en: Feature extraction is to reduce the feature dimensions by altering the original
    features/data via some transformation functions (He et al., [2021](#bib.bib72)).
    Traditional extractors include principal component analysis (PCA) (Abdi and Williams,
    [2010](#bib.bib2)) and linear discriminant analysis (LDA) (Izenman, [2013](#bib.bib85)).
    However, they cannot keep somewhat important information after the transformation
    (Abdi and Williams, [2010](#bib.bib2)) and it is tedious to tune their hyperparameters
    (e.g., number of retained features) to find the best extraction. Thus, automatically
    finding high-quality map functions by EC-based approaches to achieve informative
    feature set tends to be popular.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是通过一些变换函数（He et al., [2021](#bib.bib72)）改变原始特征/数据以减少特征维度。传统的提取方法包括主成分分析（PCA）（Abdi
    和 Williams, [2010](#bib.bib2)）和线性判别分析（LDA）（Izenman, [2013](#bib.bib85)）。然而，它们在变换后无法保留一些重要的信息（Abdi
    和 Williams, [2010](#bib.bib2)），并且调整它们的超参数（例如，保留特征的数量）以找到最佳提取方法非常繁琐。因此，通过基于EC的方法自动寻找高质量的映射函数以获得信息丰富的特征集趋向于更受欢迎。
- en: 3.3.2\. Solution Representations
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 解表示
- en: There are two typical ways for solution representation in EC-driven feature
    extraction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在EC驱动的特征提取中，有两种典型的解表示方式。
- en: 'Linear encoding: In this encoding, map functions (Refahi et al., [2020](#bib.bib164);
    Albukhanajer et al., [2015](#bib.bib7)) or function parameters (Zhao et al., [2006](#bib.bib255))
    are encoded as a linear format. For example, Wissam et al. (Albukhanajer et al.,
    [2015](#bib.bib7)) predefined three sets of track functions (i.e., trace functions,
    diametric functions, and circus functions) for feature extraction, and the optimal
    combination between the functions were obtained by an EC-based method. In (Zhao
    et al., [2006](#bib.bib255)), the hyperparameters of map functions were encoded
    by some linear vectors which were constructed by a number of optimal projection
    basis vectors obtained via EC.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 线性编码：在这种编码中，映射函数（Refahi 等，[2020](#bib.bib164)；Albukhanajer 等，[2015](#bib.bib7)）或函数参数（Zhao
    等，[2006](#bib.bib255)）被编码为线性格式。例如，Wissam 等（Albukhanajer 等，[2015](#bib.bib7)）为特征提取预定义了三组轨迹函数（即，追踪函数、直径函数和圆形函数），并通过基于进化计算的方法获得了函数之间的最优组合。在
    (Zhao 等，[2006](#bib.bib255)) 中，映射函数的超参数被编码为一些线性向量，这些向量由通过进化计算获得的若干最优投影基向量构建而成。
- en: 'Tree-based encoding: In tree-based encoding, leaf nodes represent original
    features or constants, while the non-leaf nodes are some operators for feature
    extraction including common arithmetic, logical operators (i.e., “+”, “/”, “$\cup$”)
    or other transformation operators (e.g., uLBP, and SobelY). In EC-driven feature
    extraction, an individual represents a feature extractor or map function (Peng
    et al., [2021](#bib.bib153); Zhang and Rockett, [2011](#bib.bib253)). Especially,
    an EC-based framework was developed in (Zhang and Rockett, [2011](#bib.bib253))
    to search for features and sequences of operations by use of tree-based encoding.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的编码：在基于树的编码中，叶子节点代表原始特征或常量，而非叶子节点则是一些用于特征提取的运算符，包括常见的算术运算符、逻辑运算符（例如，“+”、
    “/”、 “$\cup$”）或其他变换运算符（如 uLBP 和 SobelY）。在基于进化计算的特征提取中，一个个体代表一个特征提取器或映射函数（Peng
    等，[2021](#bib.bib153)；Zhang 和 Rockett，[2011](#bib.bib253)）。特别是，基于进化计算的框架在 (Zhang
    和 Rockett，[2011](#bib.bib253)) 中被开发出来，用于通过树形编码来搜索特征和操作序列。
- en: 3.3.3\. Search Paradigms
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 搜索范式
- en: In this section, some common search paradigms for feature extraction are introduced.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一些用于特征提取的常见搜索范式。
- en: 'Basic EC search paradigm: EC has been successfully utilized in various feature
    extraction tasks (Z.-Flores et al., [2020](#bib.bib237); Bi et al., [2018](#bib.bib18)).
    For example, Zhao et al. (Zhao et al., [2007](#bib.bib257)) introduced bagging
    concept to an evolutionary algorithm for feature extraction. The work in (Zhao
    et al., [2009](#bib.bib256)) developed an evolutionary discriminant feature extraction
    (EDFE) algorithm by combining GA with subspace analysis, which can reduce the
    complexity of the search space and improve the classification performance.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的进化计算搜索范式：进化计算已经成功应用于各种特征提取任务（Z.-Flores 等，[2020](#bib.bib237)；Bi 等，[2018](#bib.bib18)）。例如，Zhao
    等（Zhao 等，[2007](#bib.bib257)）将袋装概念引入了特征提取的进化算法中。工作 (Zhao 等，[2009](#bib.bib256))
    开发了一种进化判别特征提取（EDFE）算法，通过将遗传算法与子空间分析结合，减少了搜索空间的复杂性，并提高了分类性能。
- en: 'Co-evolution search paradigm: In feature extraction, finding the optimal extractor
    is an optimization problem, which can be decomposed into a series of subproblems
    (Kotani and Kato, [2004](#bib.bib98); Hajati et al., [2010](#bib.bib68)). For
    example, Hajati et al. (Hajati et al., [2010](#bib.bib68)) proposed a co-evolutionary
    method for feature extraction. Specifically, a subpopulation was evolved to optimize
    the classifier-related subproblem (i.e., classifier construction), and the other
    subpopulation made use of genetic information from the first population for the
    optimization of a feature-related subproblem (i.e., feature extraction).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 共进化搜索范式：在特征提取中，找到最优的提取器是一个优化问题，这可以被分解成一系列子问题（Kotani 和 Kato，[2004](#bib.bib98)；Hajati
    等，[2010](#bib.bib68)）。例如，Hajati 等（Hajati 等，[2010](#bib.bib68)）提出了一种用于特征提取的共进化方法。具体来说，一个子群体被进化以优化与分类器相关的子问题（即，分类器构建），另一个子群体利用第一个群体的遗传信息来优化与特征相关的子问题（即，特征提取）。
- en: 'Multi-objective search paradigm: In multi-objective feature extraction, the
    model accuracy, computational time, complexity, and robustness are often taken
    into account as the objectives (Zhang and Rockett, [2011](#bib.bib253); Cano et al.,
    [2017](#bib.bib19)). Cano et al. (Cano et al., [2017](#bib.bib19)) proposed a
    Pareto-based multi-objective GP algorithm for feature extraction and data visualization,
    where the objectives were to minimize the complexity of data transformation (i.e.,
    tree size) and maximize the recognition performance (i.e., accuracy).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标搜索范式：在多目标特征提取中，模型准确性、计算时间、复杂性和鲁棒性通常作为目标（Zhang and Rockett, [2011](#bib.bib253);
    Cano et al., [2017](#bib.bib19)）。Cano et al.（Cano et al., [2017](#bib.bib19)）提出了一种基于帕累托的多目标GP算法用于特征提取和数据可视化，其中目标是最小化数据转换的复杂性（即树的大小）并最大化识别性能（即准确性）。
- en: 3.3.4\. Summary
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 总结
- en: In existing studies, many efficient searching and balancing strategies, driven
    by EC approaches to achieve satisfactory solutions at significantly-reduced computation
    overheads, have been developed in recent years (Zhang and Rockett, [2011](#bib.bib253);
    Cano et al., [2017](#bib.bib19); Mauceri et al., [2021](#bib.bib133); Shakya et al.,
    [2021](#bib.bib177)). However, the performance of extractors may be limited with
    existing encoding methods and predefined operation sets. Therefore, it is essential
    to develop efficient algorithms, operation control strategies and representation
    for high-dimensional feature extraction.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有研究中，近年来已经开发了许多高效的搜索和平衡策略，这些策略由EC方法驱动，以在显著降低计算开销的情况下实现满意的解决方案（Zhang and Rockett,
    [2011](#bib.bib253); Cano et al., [2017](#bib.bib19); Mauceri et al., [2021](#bib.bib133);
    Shakya et al., [2021](#bib.bib177)）。然而，现有编码方法和预定义操作集可能限制提取器的性能。因此，开发高效的算法、操作控制策略和高维特征提取的表示是至关重要的。
- en: 4\. Model Generation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 模型生成
- en: Model generation is to search for optimal models with desirable learning capability
    on given tasks (Yao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72)).
    In this section, we introduce corresponding evolutionary parameter optimization,
    architecture optimization, and joint optimization from solution representation
    to search paradigms. Readers interested in other model generation approaches (e.g.,
    RL-based and gradient-based approaches) can refer to the reviews (Ren et al.,
    [2021](#bib.bib165); Jaâfra et al., [2019](#bib.bib86)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成是指在给定任务上搜索具有理想学习能力的最佳模型（Yao et al., [2018](#bib.bib231); He et al., [2021](#bib.bib72)）。在这一部分，我们介绍了从解的表示到搜索范式的对应演化参数优化、架构优化和联合优化。对其他模型生成方法（例如，基于强化学习和基于梯度的方法）感兴趣的读者可以参考相关综述（Ren
    et al., [2021](#bib.bib165); Jaâfra et al., [2019](#bib.bib86)）。
- en: 4.1\. Model Parameter Optimization
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 模型参数优化
- en: 4.1.1\. Problem Formulation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 问题定义
- en: 'Model parameter optimization targets at searching for the best parameter set
    (i.e., weights $W^{*}$) for a predefined architecture ($A$). The loss function
    $L$ (e.g., the cross-entropy loss function) measures the performance of the model
    with optimized parameters (i.e., $W$ in Eq. [2](#S4.E2 "In 4.1.1\. Problem Formulation
    ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model Generation ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")) on given
    datasets. The general model parameter optimization can be formulated as'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数优化的目标是搜索一个预定义架构（$A$）的最佳参数集（即权重 $W^{*}$）。损失函数 $L$（例如，交叉熵损失函数）衡量在给定数据集上优化参数（即
    $W$ 在 Eq. [2](#S4.E2 "在 4.1.1\. 问题定义 ‣ 4.1\. 模型参数优化 ‣ 4\. 模型生成 ‣ 演化深度学习综述：原理、算法、应用及开放问题")）的模型性能。一般的模型参数优化可以表示为
- en: '| (2) |  | $\displaystyle\begin{matrix}{{W}^{*}}$=$\underset{W}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\begin{matrix}{{W}^{*}}$=$\underset{W}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
- en: where $W$ is usually large-scale (millions of model parameters) and highly non-convex.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 通常是大规模的（数百万个模型参数）且高度非凸。
- en: 4.1.2\. Solution Representations
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 解的表示
- en: There are two typical EC-based representation schemes for model parameter optimization,
    including direct encoding and indirect encoding (He et al., [2021](#bib.bib72)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基于演化计算（EC）的模型参数优化有两种典型的表示方案，包括直接编码和间接编码（He et al., [2021](#bib.bib72)）。
- en: 'Direct encoding: The model parameters are directly represented via a vector
    or matrix, in which each element represents a specific parameter (Ijjina and Chalavadi,
    [2016](#bib.bib83); Karegowda and Manjunath, [2011](#bib.bib94)). For example,
    a chromosome with 64 real numbers was used to directly represent the network corresponding
    weights, where the first 63 real numbers were used to encode three convolution
    masks of size 1 $\times$ 21\. The last real number was the random seed of a generator
    for the initialization of a fully connected network (Ijjina and Chalavadi, [2016](#bib.bib83)).
    This encoding approach may require a huge computational overhead to represent
    and optimize the large-scale weights.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 直接编码：模型参数通过向量或矩阵直接表示，其中每个元素表示一个特定的参数（Ijjina 和 Chalavadi，[2016](#bib.bib83)；Karegowda
    和 Manjunath，[2011](#bib.bib94)）。例如，使用包含64个实数的染色体直接表示网络对应的权重，其中前63个实数用于编码大小为 1
    $\times$ 21 的三个卷积掩模。最后一个实数是一个生成器的随机种子，用于初始化一个全连接网络（Ijjina 和 Chalavadi，[2016](#bib.bib83)）。这种编码方法可能需要巨大的计算开销来表示和优化大规模权重。
- en: 'Indirect encoding: This encoding approach represents only a subset of the model
    parameters via a deterministic transformation (Li et al., [2019](#bib.bib110);
    Koutník et al., [2010](#bib.bib99)). In (Koutník et al., [2010](#bib.bib99)),
    the weight information was encoded as a set of Fourier coefficients in the frequency
    domain to reduce dimensionality of representation by ignoring high-frequency coefficients.
    Although this method is able to speed up the search process, the loss of parameter
    the information may occur due to the incomplete information representation, which
    maydegrades the model performance.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 间接编码：这种编码方法通过确定性变换仅表示模型参数的一个子集（Li 等，[2019](#bib.bib110)；Koutník 等，[2010](#bib.bib99)）。在
    (Koutník 等，[2010](#bib.bib99)) 中，权重信息被编码为频域中的一组傅里叶系数，以通过忽略高频系数来减少表示的维度。虽然这种方法能够加速搜索过程，但由于信息表示不完整，可能会丢失参数信息，从而降低模型性能。
- en: 4.1.3\. Search Paradigms
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 搜索范式
- en: EC-based methods for model parameter optimization can be divided into two categories
    according to whether or not method combines with the gradient approach, i.e.,
    pure EC and gradient-based EC.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于EC的方法可以根据是否结合梯度方法分为两类，即纯EC和基于梯度的EC。
- en: Pure EC paradigms optimize model parameters only via evolutionary search, including
    the basic EC search paradigm and co-evolution search paradigm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 纯EC范式仅通过进化搜索优化模型参数，包括基本EC搜索范式和共同进化搜索范式。
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Basic EC search paradigm: In addition to GA (Montana and Davis, [1989](#bib.bib140);
    Karegowda and Manjunath, [2011](#bib.bib94)), some heuristic algorithms like PSO
    (Al-kazemi and Mohan, [2002](#bib.bib5)), ABC (Karaboga et al., [2007](#bib.bib93))
    and ACO (Socha and Blum, [2007](#bib.bib183)) are also commonly utilized for model
    parameter optimization. For example, Karaboga et al. (Karaboga et al., [2007](#bib.bib93))
    adopted ABC to find a set of weights for a feed-forward neural network (FNN) on
    targeted tasks.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本EC搜索范式：除了GA（Montana 和 Davis，[1989](#bib.bib140)；Karegowda 和 Manjunath，[2011](#bib.bib94)），一些启发式算法如PSO（Al-kazemi
    和 Mohan，[2002](#bib.bib5)），ABC（Karaboga 等，[2007](#bib.bib93)）和ACO（Socha 和 Blum，[2007](#bib.bib183)）也常用于模型参数优化。例如，Karaboga
    等（Karaboga 等，[2007](#bib.bib93)）采用ABC来为目标任务上的前馈神经网络（FNN）找到一组权重。
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Co-evolution search paradigm: Co-evolution search is conducted on the subproblems
    of the original optimization problem (e.g., synapse-based and neuron-based problems
    (Chandra and Zhang, [2012](#bib.bib23); Chandra, [2015](#bib.bib22))). For example,
    Chandra et al. (Chandra and Zhang, [2012](#bib.bib23)) regarded a single hidden
    layer as a subcomponent in the initialization phase, which will be merged with
    the individuals with the best fitness from different sub-populations to constitute
    new neural networks during the co-evolution optimization process.'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 共同进化搜索范式：共同进化搜索是在原始优化问题的子问题上进行的（例如，基于突触和基于神经元的问题（Chandra 和 Zhang，[2012](#bib.bib23)；Chandra，[2015](#bib.bib22)））。例如，Chandra
    等（Chandra 和 Zhang，[2012](#bib.bib23)）在初始化阶段将单个隐藏层视为一个子组件，该子组件将在共同进化优化过程中与来自不同子种群的最佳适应个体合并，以构成新的神经网络。
- en: '![Refer to caption](img/6825f4f47b67248cf5a997f1ecfa83e1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6825f4f47b67248cf5a997f1ecfa83e1.png)'
- en: (a)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/38e7f4b1d3767aae05ade0ace0f876a1.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38e7f4b1d3767aae05ade0ace0f876a1.png)'
- en: (b)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/2410148fead42dccca01069d6689080e.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2410148fead42dccca01069d6689080e.png)'
- en: (c)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: Figure 5. Three hybrid ways of gradient-based ECs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 基于梯度的EC的三种混合方式。
- en: Gradient-based EC combine basic EC with the gradient-based method to enhance
    the exploitation ability in optimizing model parameters. According to the execution
    order, there are three hybrid ways.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的EC将基础EC与基于梯度的方法结合起来，以增强在优化模型参数时的开发能力。根据执行顺序，有三种混合方式。
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The first hybridization approach is shown in Fig. ([5(a)](#S4.F5.sf1 "In Figure
    5 ‣ 4.1.3\. Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model
    Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the EC is used to identify the optimal parameters for
    model, then the parameters are further optimized using gradient-based method to
    find the final optimal solution (Zhang and Smart, [2004](#bib.bib250); Chen et al.,
    [2015](#bib.bib25)). For example, a genetic adaptive momentum estimation algorithm
    (GADAM) was proposed in (Zhang and Gouza, [2018](#bib.bib243)) by incorporating
    Adam and GA into a unified learning scheme, where Adam was an adaptive moment
    estimation method with first-order gradient.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个混合方法如图所示 ([5(a)](#S4.F5.sf1 "图 5 ‣ 4.1.3\. 搜索范式 ‣ 4.1\. 模型参数优化 ‣ 4\. 模型生成
    ‣ 演化深度学习调查：原理、算法、应用及开放问题"))，其中EC用于识别模型的最佳参数，然后使用基于梯度的方法进一步优化这些参数，以找到最终的最优解 (Zhang
    和 Smart, [2004](#bib.bib250); Chen 等, [2015](#bib.bib25))。例如，(Zhang 和 Gouza, [2018](#bib.bib243))
    提出的遗传自适应动量估计算法（GADAM）通过将Adam和GA融入统一学习方案中，其中Adam是一个具有一阶梯度的自适应动量估计方法。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The second hybridization approach is given in Fig. ([5(b)](#S4.F5.sf2 "In Figure
    5 ‣ 4.1.3\. Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model
    Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications
    and Open Issues")), where the gradient-based method is used to produce a set of
    parameters for the initialization of the population used in EC (Wu et al., [2021b](#bib.bib217)).
    For example, the study (Khadka and Tumer, [2018](#bib.bib95)) firstly trained
    a RL agent through a gradient-based method, then the parameters of the RL were
    used as the initial population to feed the EC. As a result, the parameters will
    be further optimized by the EC.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二种混合方法如图所示 ([5(b)](#S4.F5.sf2 "图 5 ‣ 4.1.3\. 搜索范式 ‣ 4.1\. 模型参数优化 ‣ 4\. 模型生成
    ‣ 演化深度学习调查：原理、算法、应用及开放问题"))，其中基于梯度的方法用于生成一组参数以初始化用于EC的人群 (Wu 等, [2021b](#bib.bib217))。例如，研究
    (Khadka 和 Tumer, [2018](#bib.bib95)) 首先通过基于梯度的方法训练一个RL代理，然后将RL的参数作为初始种群输入EC。因此，参数将由EC进一步优化。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The third approach is presented in Fig. ([5(c)](#S4.F5.sf3 "In Figure 5 ‣ 4.1.3\.
    Search Paradigms ‣ 4.1\. Model Parameter Optimization ‣ 4\. Model Generation ‣
    Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and
    Open Issues")), which iteratively applies EC and gradient-based method during
    the optimization to find the optimal parameters. Following this framework, when
    the method is used and which method is chosen are varying in different studies
    (Yang et al., [2021](#bib.bib229); Cui et al., [2018](#bib.bib36)).'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三种方法在图中展示 ([5(c)](#S4.F5.sf3 "图 5 ‣ 4.1.3\. 搜索范式 ‣ 4.1\. 模型参数优化 ‣ 4\. 模型生成
    ‣ 演化深度学习调查：原理、算法、应用及开放问题"))，它在优化过程中迭代地应用EC和基于梯度的方法以找到最优参数。根据这一框架，何时使用该方法以及选择哪种方法在不同的研究中有所不同
    (Yang 等, [2021](#bib.bib229); Cui 等, [2018](#bib.bib36))。
- en: 4.1.4\. Summary
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 总结
- en: In model parameter optimization, direct encoding is straightforward and able
    to keep more information than the indirect encoding. Compared to gradient-based
    methods easily trapped into local optima, EC shows more powerful ability in global
    search. Here, several scenarios are introduced as follows, where EC is applied
    to model parameter optimization.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型参数优化中，直接编码直观且能够保留比间接编码更多的信息。与容易陷入局部最优的基于梯度的方法相比，EC在全局搜索中展现出更强的能力。这里介绍了几个场景，其中EC应用于模型参数优化。
- en: 'Small-scale scenario: (Montana and Davis, [1989](#bib.bib140)) shows that pure
    EC approaches outperform gradient-based methods in search effectiveness on some
    small-scale problems, where the models are with small numbers of parameters or
    simple architectures (e.g., FNN).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 小规模场景： (Montana 和 Davis, [1989](#bib.bib140)) 显示纯EC方法在一些小规模问题的搜索有效性上优于基于梯度的方法，这些问题中的模型参数较少或结构简单（例如，FNN）。
- en: 'Large-scale scenario: The performance of pure EC approaches might not be promising
    in large-scale learning models, while a better way is to utilize the hybridization
    of EC and gradient-based methods. Such hybrid methods can alleviate the issue
    of getting trapped in local optima and increase the effectiveness of subsequent
    exploitation (Yang et al., [2021](#bib.bib229)).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模场景：在大规模学习模型中，纯 EC 方法的表现可能不尽如人意，而更好的方法是利用 EC 和基于梯度的方法的混合。这种混合方法可以缓解陷入局部最优解的问题，并提高后续开发的有效性（Yang
    et al., [2021](#bib.bib229)）。
- en: In addition to above scenarios, EC-based method can be used to train the DNN,
    when the exact gradient information of the loss function is difficult to be acquired
    (Peng et al., [2018](#bib.bib154)). For example, the rewards of policy network
    are sparse or deceptive in deep reinforcement learning (DRL) so that the gradient
    information is unattainable. The work (Conti et al., [2018](#bib.bib35)) introduced
    the novelty search (NS) and the quality diversity (QD) to the evolution strategies
    (ES) for the policy network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述场景，当难以获得损失函数的确切梯度信息时，可以使用基于 EC 的方法来训练 DNN（Peng et al., [2018](#bib.bib154)）。例如，在深度强化学习（DRL）中，策略网络的奖励是稀疏或具有误导性的，从而使得梯度信息无法获得。工作（Conti
    et al., [2018](#bib.bib35)）将新颖性搜索（NS）和质量多样性（QD）引入了策略网络的进化策略（ES）中。
- en: 4.2\. Model Architecture Optimization
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 模型架构优化
- en: 4.2.1\. Problem Formulation
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 问题表述
- en: Model architecture optimization, also termed as NAS, is to search promising
    network architectures with good performance such as model accuracy on given tasks.
    The model architecture optimization can be formulated as follows.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构优化，也称为 NAS，是指搜索具有良好性能的网络架构，例如在给定任务上的模型准确性。模型架构优化可以表述如下。
- en: '| (3) |  | <math   alttext="\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&amp;A\in\mathcal{A}\\'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '| (3) |  | <math alttext="\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&amp;A\in\mathcal{A}\\'
- en: \end{matrix}\\
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \end{matrix}\\
- en: \end{matrix}\right." display="block"><semantics ><mrow ><mo >{</mo><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msup ><mi >A</mi><mo >∗</mo></msup><mo lspace="0em"
    rspace="0em" >​</mo><mtext >=</mtext><mo lspace="0.167em" rspace="0em" >​</mo><munder
    accentunder="true" ><mrow ><mi >arg</mi><mo lspace="0.167em" >⁡</mo><mi >min</mi></mrow><mrow
    ><mi >W</mi><mo >,</mo><mi >A</mi></mrow></munder><mo lspace="0.167em" rspace="0em"
    >​</mo><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo >(</mo><mi >W</mi><mo
    >,</mo><mi >A</mi><mo >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em"
    >.</mo><mi >t</mi></mrow><mo lspace="0em" >.</mo></mrow></mtd><mtd ><mrow ><mi
    >A</mi><mo >∈</mo><mi >𝒜</mi></mrow></mtd></mtr></mtable></mtd></mtr></mtable></mrow><annotation
    encoding="application/x-tex" >\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&A\in\mathcal{A}\\ \end{matrix}\\ \end{matrix}\right.</annotation></semantics></math>
    |  |
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \end{matrix}\right." display="block"><semantics ><mrow ><mo >{</mo><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msup ><mi >A</mi><mo >∗</mo></msup><mo lspace="0em"
    rspace="0em" >​</mo><mtext >=</mtext><mo lspace="0.167em" rspace="0em" >​</mo><munder
    accentunder="true" ><mrow ><mi >arg</mi><mo lspace="0.167em" >⁡</mo><mi >min</mi></mrow><mrow
    ><mi >W</mi><mo >,</mo><mi >A</mi></mrow></munder><mo lspace="0.167em" rspace="0em"
    >​</mo><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo >(</mo><mi >W</mi><mo
    >,</mo><mi >A</mi><mo >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mrow ><mrow ><mi >s</mi><mo lspace="0em" rspace="0.167em"
    >.</mo><mi >t</mi></mrow><mo lspace="0em" >.</mo></mrow></mtd><mtd ><mrow ><mi
    >A</mi><mo >∈</mo><mi >𝒜</mi></mrow></mtd></mtr></mtable></mtd></mtr></mtable></mrow><annotation
    encoding="application/x-tex" >\left\{\begin{matrix}{{A}^{*}}$=$\underset{W,A}{\mathop{\arg\min}}\,L\left(W,A\right)\\
    \begin{matrix}s.t.&A\in\mathcal{A}\\ \end{matrix}\\ \end{matrix}\right.</annotation></semantics></math>
    |  |
- en: where $A^{*}$ indicates the architecture from the search space ($\mathcal{A}$)
    with the best performance under the parameters $W$, and $L$ is used to measure
    the performance of architectures on given tasks. Thereby, this optimization is
    a bi-level optimization problem (Zhou et al., [2021a](#bib.bib258); Liu et al.,
    [2021b](#bib.bib117)), where the model architecture optimization is subject to
    the model parameter optimization (Lu et al., [2021](#bib.bib125)). Since the current
    NAS works are mainly focused on CNN, we will discuss the solution representations,
    the search paradigms, and acceleration strategies of CNN. Due to the page limit,
    the design of the search space of NAS are not introduced here, but interested
    readers can check these surveys (Liu et al., [2021b](#bib.bib117); Ren et al.,
    [2021](#bib.bib165)) which have details about search space design.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A^{*}$ 表示在参数 $W$ 下，具有最佳性能的搜索空间（$\mathcal{A}$）中的架构，而 $L$ 用于衡量架构在给定任务上的性能。因此，这种优化是一个双层次优化问题（Zhou
    et al., [2021a](#bib.bib258); Liu et al., [2021b](#bib.bib117)），其中模型架构优化受限于模型参数优化（Lu
    et al., [2021](#bib.bib125)）。由于目前的NAS工作主要集中在CNN上，我们将讨论CNN的解决方案表示、搜索范式和加速策略。由于页面限制，这里没有介绍NAS的搜索空间设计，但感兴趣的读者可以查阅这些综述（Liu
    et al., [2021b](#bib.bib117); Ren et al., [2021](#bib.bib165)），其中包含有关搜索空间设计的详细信息。
- en: 4.2.2\. Solution Representations
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 解决方案表示
- en: According to varying lengths of encodings, we can classify the encoding strategies
    into fixed-length and variable-length encodings.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 根据编码长度的不同，我们可以将编码策略分为固定长度编码和变长编码。
- en: 'Fixed-length encoding: The length of each individual is fixed during the evolution.
    For example, a fixed-length vector is designed to represent the model architecture
    of CNN (Xie and Yuille, [2017](#bib.bib222)), where a subset of elements in the
    vector represents an architectural units (e.g., convolutional, pooling or fully-connected
    layer) of a CNN. Such encoding may be easily adapted to evolutionary operations
    (e.g., crossover and mutation) of EC (Xie and Yuille, [2017](#bib.bib222)), but
    it has to specify an appropriate maximal length, which is usually unknown in advance
    and needs to predefine based on domain expertise.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 固定长度编码：在进化过程中，每个个体的长度是固定的。例如，设计一个固定长度的向量来表示CNN的模型架构（Xie and Yuille, [2017](#bib.bib222)），其中向量中的一个子集表示CNN的架构单元（例如卷积层、池化层或全连接层）。这种编码可能容易适应进化计算（EC）的操作（例如交叉和突变）（Xie
    and Yuille, [2017](#bib.bib222)），但它必须指定一个合适的最大长度，这通常是未知的，需要基于领域专业知识预先定义。
- en: 'Variable-length encoding: Different from the fixed-length approach, the variable-length
    encoding strategy does not require a prior knowledge about the optimal depth of
    model architecture and actually could be a way to reduce the complexity of the
    search space. The flexible design of this encoding may encode more detailed information
    about the architecture into a solution vector, and the optimal length of the solution
    is automatically found during the search process (Liu et al., [2021b](#bib.bib117)).
    In (Chen et al., [2020](#bib.bib27)), the entire variational autoencoder (VAE)
    was divided into four blocks, including h-block, $\mu$-block, $\sigma$-block and
    t-block, while the variable-length chromosomes consisted of different quantities
    and types of layers. Notably, variable-length encoding it is not straightforward
    to apply standard genetic operators (e.g., crossover).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 变长编码：与固定长度方法不同，变长编码策略不需要事先了解模型架构的最佳深度，实际上可能是减少搜索空间复杂性的一种方式。这种编码的灵活设计可能将有关架构的更多详细信息编码到解决方案向量中，并且解决方案的最佳长度在搜索过程中自动找到（Liu
    et al., [2021b](#bib.bib117)）。在（Chen et al., [2020](#bib.bib27)）中，整个变分自编码器（VAE）被分为四个块，包括h块、$\mu$块、$\sigma$块和t块，而变长染色体则由不同数量和类型的层组成。值得注意的是，变长编码不容易直接应用标准遗传操作（例如交叉）。
- en: Since the neural network architectures are composed of basic units and connections
    between them, so that both of them are to be encoded, as suggested in (Liu et al.,
    [2021b](#bib.bib117)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络架构由基本单元及其之间的连接组成，因此这两者都需要编码，如（Liu et al., [2021b](#bib.bib117)）所建议的。
- en: 1) Encoding hyperparameters of basic units. In CNNs, there are many hyperparameters
    to be specified for each unit (e.g., layer, block or cell), such as feature map
    size, type of convolution layer, and filter size (Sun et al., [2020](#bib.bib192)).
    In (Sun et al., [2019b](#bib.bib189)), DenseBlock only had to set two hyperparameters
    (e.g., block type and specific parameter of internal unit) to configure the block
    can be seen as a microcosm of a complete CNN model. The parameterization of a
    cell is more flexible than that of a block since it can be configured via a combination
    of different primitive layers (Sun et al., [2019c](#bib.bib190)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基本单元的超参数编码。在卷积神经网络（CNN）中，每个单元（例如，层、块或细胞）都有许多需要指定的超参数，如特征图大小、卷积层类型和滤波器大小（Sun
    et al., [2020](#bib.bib192)）。在（Sun et al., [2019b](#bib.bib189)）中，DenseBlock 只需设置两个超参数（例如，块类型和内部单元的具体参数）即可配置该块，这可以看作是完整
    CNN 模型的缩影。与块的参数化相比，细胞的参数化更加灵活，因为它可以通过不同原始层的组合来配置（Sun et al., [2019c](#bib.bib190)）。
- en: '2) Encoding connections between units. In general, there are two kinds of model
    architectures according to the connection patterns of basic units: linear topological
    architectures and non-linear topological architectures (Yang et al., [2020](#bib.bib230)).
    The linear pattern of architecture consists of sequential basic units, and the
    non-linear pattern allows for skip or loop connections in the architecture (Liu
    et al., [2021b](#bib.bib117)).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 单元之间连接的编码。一般来说，根据基本单元的连接模式，有两种类型的模型架构：线性拓扑架构和非线性拓扑架构（Yang et al., [2020](#bib.bib230)）。线性架构的模式由连续的基本单元组成，而非线性模式允许在架构中进行跳跃或循环连接（Liu
    et al., [2021b](#bib.bib117)）。
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Linear topological architecture: The linear topology widely appears in the
    construction of layer-wise and block-wise search spaces. Due to the simplicity
    of linear topology, basic units can be stacked one by one by a linear piecing
    method. In this way, the skeleton of an architecture can be built up effectively
    (Sun et al., [2019b](#bib.bib189); Chen et al., [2020](#bib.bib27)) regardless
    of the complexity of the internal of basic units.'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性拓扑结构：线性拓扑广泛出现在层次化和模块化的搜索空间构建中。由于线性拓扑的简单性，基本单元可以通过线性拼接方法一个接一个地堆叠起来。通过这种方式，架构的骨架可以有效地建立起来（Sun
    et al., [2019b](#bib.bib189); Chen et al., [2020](#bib.bib27)），而不考虑基本单元内部的复杂性。
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Non-linear topological architecture: Compared to the linear architecture, the
    non-linear topological architecture receives much more attention due to its flexibility
    to construct well-performing architectures (Wang et al., [2021c](#bib.bib209),
    [2020b](#bib.bib207); Xie and Yuille, [2017](#bib.bib222)), such as macro structures
    composed of basic units, and micro structures within basic units. There are two
    typical encoding approaches for non-linear topological architectures. The one
    is to use adjacent matrix to represent the connections in non-linear architectures,
    where “1” of the matrix denotes the existence of the connection between two units
    and “0” goes the opposite. In (Lorenzo and Nalepa, [2018](#bib.bib123)), skip
    connections are represented by a matrix where constraints can be set in place
    to guarantee valid encoding and avoid recurrent edges while performing skip connections.
    Note that adjacent matrix has a limitation that the number of basic units needs
    to be fixed in advance (Kitano, [1990](#bib.bib97)). Another one is to utilize
    an ordered pair to represent a directed acyclic graph, and then encode the connections
    between unites. The ordered pair can be formulated as $G$ = ($V$, $E$) where $V$
    is a set of vertices and $E$ is a directed edge in the acyclic graph, and it has
    been applied in (Irwin-Harris et al., [2019](#bib.bib84)) to encode the connections.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非线性拓扑结构：与线性结构相比，非线性拓扑结构因其在构建高性能架构方面的灵活性而受到更多关注（Wang et al., [2021c](#bib.bib209),
    [2020b](#bib.bib207); Xie and Yuille, [2017](#bib.bib222)），例如由基本单元组成的宏观结构和基本单元内部的微观结构。非线性拓扑结构有两种典型的编码方法。一种是使用邻接矩阵表示非线性架构中的连接，其中矩阵中的“1”表示两个单元之间的连接存在，而“0”表示连接不存在。在（Lorenzo
    and Nalepa, [2018](#bib.bib123)）中，跳跃连接由一个矩阵表示，其中可以设置约束以保证有效编码，并避免在执行跳跃连接时出现重复边。需要注意的是，邻接矩阵有一个限制，即基本单元的数量需要预先固定（Kitano,
    [1990](#bib.bib97)）。另一种是利用有序对表示有向无环图，然后编码单元之间的连接。这个有序对可以表示为 $G$ = ($V$, $E$)，其中
    $V$ 是顶点的集合，$E$ 是无环图中的有向边，并已在（Irwin-Harris et al., [2019](#bib.bib84)）中应用于连接的编码。
- en: 4.2.3\. Search Paradigms
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 搜索范式
- en: In this section, the commonly used EC-based search paradigms for NAS are introduced.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了用于NAS的常用EC基础搜索范式。
- en: 'Basic EC search paradigm: Many basic EC algorithms have been widely applied
    in existing NAS methods, such as GA (Kitano, [1990](#bib.bib97)) and PSO (Sun
    et al., [2019d](#bib.bib191)). A general framework of EC is presented in Fig.
    [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary Computation ‣ 2\. An Overview of Evolutionary
    Deep Learning ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '基本EC搜索范式：许多基本的EC算法已广泛应用于现有的NAS方法中，如GA (Kitano, [1990](#bib.bib97)) 和PSO (Sun
    et al., [2019d](#bib.bib191))。EC的一般框架见图 [3](#S2.F3 "Figure 3 ‣ 2.2\. Evolutionary
    Computation ‣ 2\. An Overview of Evolutionary Deep Learning ‣ Survey on Evolutionary
    Deep Learning: Principles, Algorithms, Applications and Open Issues")。'
- en: 'Incremental search paradigm: A model architecture can be built in an incremental
    way where model elements (e.g., layers and connections) are gradually added to
    the model during the evolutionary process (Liu et al., [2018c](#bib.bib111); Wang
    et al., [2020c](#bib.bib208); Shen et al., [2019](#bib.bib179)). This way allows
    to find parts of architecture at different optimization stages, which reduces
    the computational burden on acquiring a complete model at once (Liu et al., [2018c](#bib.bib111)).
    For example, Wang et al. (Wang et al., [2020c](#bib.bib208)) used an incremental
    approach to stack blocks for building architectures, which improved the capacity
    of the final architecture via a progressive process.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 增量搜索范式：模型架构可以以增量的方式构建，其中模型元素（例如，层和连接）在进化过程中逐步添加到模型中（Liu et al., [2018c](#bib.bib111);
    Wang et al., [2020c](#bib.bib208); Shen et al., [2019](#bib.bib179)）。这种方式允许在不同优化阶段找到架构的一部分，从而减少一次性获取完整模型的计算负担（Liu
    et al., [2018c](#bib.bib111)）。例如，Wang et al. (Wang et al., [2020c](#bib.bib208))
    使用增量方法堆叠块来构建架构，通过渐进过程提高了最终架构的能力。
- en: 'Co-evolution search paradigm: An architecture optimization problem is decomposed
    into the optimizations of a blueprint and its components (O’Neill et al., [2018](#bib.bib150);
    Zhang et al., [2022b](#bib.bib241)). Specifically, the blueprint plays a role
    in specifying the topological connection patterns of its components, and an optimal
    architecture is acquired by cooperatively optimizing the blueprint and its components.
    For example, O’Neill et al. (O’Neill et al., [2018](#bib.bib150)) proposed a co-evolution
    search paradigm for NAS, where the candidate blueprints and components were sampled
    from two populations, and then combined to form new architectures.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 协同进化搜索范式：一个架构优化问题被分解为蓝图及其组件的优化（O’Neill et al., [2018](#bib.bib150); Zhang et
    al., [2022b](#bib.bib241)）。具体来说，蓝图在指定其组件的拓扑连接模式方面发挥作用，通过协同优化蓝图及其组件来获得最佳架构。例如，O’Neill
    et al. (O’Neill et al., [2018](#bib.bib150)) 提出了用于NAS的协同进化搜索范式，其中候选蓝图和组件从两个种群中采样，然后组合形成新的架构。
- en: 'Multi-objective search paradigm: This paradigm targets at searching for a set
    of Pareto optimal architectures based on multiple criteria, and finding the final
    solutions according to some practical considerations, such as computational environment
    (Neshat et al., [2020](#bib.bib144); Lu et al., [2019](#bib.bib126)). This paradigm
    becomes popular in practical applications, since many objectives are required
    to be considered such as the accuracy, inference time, model size, and energy
    consumption. In (Neshat et al., [2020](#bib.bib144)), NSGA-II and RL were used
    to explore model architectures with respect to the model accuracy, and model complexity
    (e.g., the number of model parameters and multiply-adds operators).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标搜索范式：该范式旨在基于多个标准搜索一组帕累托最优架构，并根据一些实际考虑因素（如计算环境）寻找最终解决方案（Neshat et al., [2020](#bib.bib144);
    Lu et al., [2019](#bib.bib126)）。该范式在实际应用中变得流行，因为需要考虑许多目标，如准确性、推理时间、模型大小和能耗。在 (Neshat
    et al., [2020](#bib.bib144)) 中，使用了NSGA-II和RL来探索与模型准确性和模型复杂性（例如，模型参数数量和乘加操作数）相关的模型架构。
- en: 4.2.4\. Acceleration Strategies
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 加速策略
- en: 'NAS is a high computational overhead task, mainly due to the large search space
    and highly time-consuming evaluation (Yao et al., [2018](#bib.bib231)). To overcome
    this challenge, various acceleration strategies (Sun et al., [2019d](#bib.bib191);
    Assunção et al., [2019b](#bib.bib13)) have been developed to accelerate the optimization.
    In this section, we summarize the speed-up strategies from the aspects of algorithm
    design to the hardware implementation, as shown in Fig. [6](#S4.F6 "Figure 6 ‣
    4.2.4\. Acceleration Strategies ‣ 4.2\. Model Architecture Optimization ‣ 4\.
    Model Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'NAS 是一个高计算开销的任务，这主要是由于大的搜索空间和高度耗时的评估 (Yao et al., [2018](#bib.bib231))。为克服这一挑战，已经开发了各种加速策略
    (Sun et al., [2019d](#bib.bib191); Assunção et al., [2019b](#bib.bib13)) 以加速优化。在这一部分，我们从算法设计到硬件实现的各个方面总结了加速策略，如图
    [6](#S4.F6 "Figure 6 ‣ 4.2.4\. Acceleration Strategies ‣ 4.2\. Model Architecture
    Optimization ‣ 4\. Model Generation ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") 所示。'
- en: '![Refer to caption](img/f173dbb8c1a13e886eadc7aa6aba2e6a.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f173dbb8c1a13e886eadc7aa6aba2e6a.png)'
- en: Figure 6. Overview of acceleration strategies.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. 加速策略概览。
- en: From the algorithm design point of view, we summarized a number of acceleration
    strategies from population initialization to evaluations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法设计的角度来看，我们总结了从种群初始化到评估的多种加速策略。
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Initialization:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化：
- en: –
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Reduced population: The simplest way of acceleration during the initialization
    stage is to set the population with a small size. In other words, less evaluations
    are required with a smaller size of population since the evaluation of a candidate
    architecture is time-consuming (Yao et al., [2018](#bib.bib231)). As a result,
    some studies (Assunção et al., [2019b](#bib.bib13), [2018](#bib.bib12)) use small
    population with fixed size to speed up their evolution, like CARS (size = 32)
    (Yang et al., [2020](#bib.bib230)). In contrast, some other studies use dynamic
    sizes of populations during the optimization. In (Fan et al., [2020](#bib.bib56)),
    the population size is dynamically changed to reach a balance between algorithmic
    efficiency and population diversity.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少种群：在初始化阶段，加速的最简单方法是设定一个较小的种群。换句话说，较小的种群需要更少的评估，因为候选架构的评估是耗时的 (Yao et al.,
    [2018](#bib.bib231))。因此，一些研究 (Assunção et al., [2019b](#bib.bib13), [2018](#bib.bib12))
    使用固定大小的小种群来加速其演化，比如 CARS (大小 = 32) (Yang et al., [2020](#bib.bib230))。相比之下，其他一些研究在优化过程中使用动态大小的种群。在
    (Fan et al., [2020](#bib.bib56)) 中，种群大小动态变化，以在算法效率和种群多样性之间达到平衡。
- en: –
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Efficient search space: Another way is to design efficient search space to
    speed up the search process. For example, an architecture constructed on the basis
    of cell-wise search space (Liu et al., [2021b](#bib.bib117)) is composed of many
    similar structures of cells and only representative cells need to be optimized,
    which contributes to significant computational speed-up.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的搜索空间：另一种方法是设计高效的搜索空间以加速搜索过程。例如，基于细胞搜索空间 (Liu et al., [2021b](#bib.bib117))
    构建的架构由许多相似的细胞结构组成，仅需要优化代表性细胞，这有助于显著加快计算速度。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluations:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估：
- en: (1)
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Early stopping policy: A relatively small number of training epochs are used
    to reduce the training cost (i.e., early stopping policy) since the training time
    is reduced (Ahmed et al., [2019](#bib.bib3); Tian et al., [2019](#bib.bib199);
    Sun et al., [2019d](#bib.bib191)).'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 早期停止策略：使用相对较少的训练周期来减少训练成本（即早期停止策略），因为训练时间减少 (Ahmed et al., [2019](#bib.bib3);
    Tian et al., [2019](#bib.bib199); Sun et al., [2019d](#bib.bib191))。
- en: (2)
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Reduced training set: Some methods are designed to reduce the size of the training
    set to improve training efficiency at the expense of a little accuracy (Sapra
    and Pimentel, [2020](#bib.bib173); Liu et al., [2019a](#bib.bib114)). Besides,
    low-resolution data (e.g., ImageNet 32) (Chrabaszcz et al., [2017](#bib.bib33))
    is also commonly used as the training set to accelerate the search process for
    the optimal architecture.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少训练集：一些方法旨在减少训练集的大小，以提高训练效率，但会牺牲一些准确性 (Sapra and Pimentel, [2020](#bib.bib173);
    Liu et al., [2019a](#bib.bib114))。此外，低分辨率数据 (例如，ImageNet 32) (Chrabaszcz et al.,
    [2017](#bib.bib33)) 也常用于训练集，以加速搜索最佳架构的过程。
- en: (3)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Weight inheritance ¹¹1In (Rumelhart et al., [1985](#bib.bib168), [1986](#bib.bib169);
    LeCun et al., [1989](#bib.bib106)), Rumelhart/Hinton/LeCun used the term ”weight
    sharing” to mean that different network connections/links share the same set of
    weights, and pointed out that ”weight sharing” is the core of shared weight NNs/CNNs.
    More recent (Lu et al., [2021](#bib.bib125); Xie et al., [2022a](#bib.bib221))
    use of this term refers to ”weight/parameter replications” or ”weight inheritance”.:'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重继承 ¹¹1In（Rumelhart et al., [1985](#bib.bib168), [1986](#bib.bib169); LeCun
    et al., [1989](#bib.bib106)），Rumelhart/Hinton/LeCun 使用“权重共享”一词来表示不同的网络连接/链接共享相同的权重集，并指出“权重共享”是共享权重
    NNs/CNNs 的核心。最近（Lu et al., [2021](#bib.bib125); Xie et al., [2022a](#bib.bib221)）对这个术语的使用指的是“权重/参数复制”或“权重继承”。
- en: –
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Supernet-based inheritance: uses an over-parameterized and pre-trained supernet
    to encode all candidate architectures (i.e., subnets). In other words, the subnets
    share weights of the identical structures from the supernet, and they are directly
    evaluated on the validation dataset to obtain their model accuracy (Rapaport et al.,
    [2019](#bib.bib159); Dahal and Zhan, [2020](#bib.bib38); Elsken et al., [2019](#bib.bib50);
    Sapra and Pimentel, [2020](#bib.bib173)).'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于超网络的继承：使用一个过度参数化且预训练的超网络来编码所有候选架构（即子网络）。换句话说，子网络从超网络中共享相同结构的权重，并在验证数据集上直接评估以获得其模型准确性（Rapaport
    et al., [2019](#bib.bib159); Dahal and Zhan, [2020](#bib.bib38); Elsken et al.,
    [2019](#bib.bib50); Sapra and Pimentel, [2020](#bib.bib173)）。
- en: –
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Parent-based inheritance: inherits weights from previously-trained networks
    (i.e., parental networks) instead of a supernet, since offspring individuals retain
    some identical parts of their parental architectures (Real et al., [2017](#bib.bib163);
    Elsken et al., [2017](#bib.bib49); Kwasigroch et al., [2019](#bib.bib104); Zhu
    et al., [2019](#bib.bib262)) (Real et al., [2017](#bib.bib163)). As a result,
    offspring architectures can inherit the weights of the identical parts and no
    longer need to be trained from scratch.'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于父体的继承：从先前训练过的网络（即父网络）继承权重，而不是从超网络，因为后代个体保留了父母架构的一些相同部分（Real et al., [2017](#bib.bib163);
    Elsken et al., [2017](#bib.bib49); Kwasigroch et al., [2019](#bib.bib104); Zhu
    et al., [2019](#bib.bib262))（Real et al., [2017](#bib.bib163)）。因此，后代架构可以继承相同部分的权重，而不需要从头开始训练。
- en: (4)
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Surrogate model: Since the evaluation of an architecture is time-consuming
    (Yao et al., [2018](#bib.bib231); Liu et al., [2021b](#bib.bib117)), cheap surrogate
    models have been introduced in NAS as performance predictors to reduce the computational
    time (Lu et al., [2021](#bib.bib125)).'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 替代模型：由于架构评估是耗时的（Yao et al., [2018](#bib.bib231); Liu et al., [2021b](#bib.bib117)），在
    NAS 中引入了廉价的替代模型作为性能预测器，以减少计算时间（Lu et al., [2021](#bib.bib125)）。
- en: –
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Online performance predictors: They are trained online on the datasets sampled
    from past several epochs (Liu et al., [2021b](#bib.bib117)), including a sequence
    of data pairs with different training epochs and their corresponding performance
    of these epochs (Lu et al., [2021](#bib.bib125)). After that, they will be used
    for the performance prediction on new architectures. To reduce the true evaluations
    of architectures, some performance predictors directly predict whether a candidate
    architecture can be survived into next iteration through a trained ranking or
    classification method, such as classification-wise NAS (Ma et al., [2021a](#bib.bib130)).'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线性能预测器：它们在线训练，使用来自过去几个训练周期的数据集（Liu et al., [2021b](#bib.bib117)），包括具有不同训练周期的数据对及这些周期对应的性能（Lu
    et al., [2021](#bib.bib125)）。之后，它们将用于对新架构进行性能预测。为了减少对架构的真实评估，一些性能预测器直接通过训练的排序或分类方法预测候选架构是否能进入下一次迭代，例如分类导向的
    NAS（Ma et al., [2021a](#bib.bib130)）。
- en: –
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Offline performance predictors: They are essentially a sort of regression models
    mapping the architectures to specific performance. End-to-end predictors can be
    trained in an offline manner, so that they are able to predict the performance
    of architectures during the entire search process. Consequently, they can significantly
    reduce the computational burden (Sun et al., [2019a](#bib.bib188); Liu et al.,
    [2018c](#bib.bib111); Sun et al., [2021](#bib.bib187)).'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 离线性能预测器：它们本质上是一种回归模型，将架构映射到特定性能。端到端预测器可以以离线方式进行训练，以便在整个搜索过程中预测架构的性能。因此，它们可以显著减少计算负担（Sun
    et al., [2019a](#bib.bib188); Liu et al., [2018c](#bib.bib111); Sun et al., [2021](#bib.bib187)）。
- en: (5)
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: 'Population memory: Population memory is used to store elite individuals from
    different generations during the optimization (Sun et al., [2020](#bib.bib192);
    Fujino et al., [2017](#bib.bib62)). When a new individual is generated, it does
    not need to be evaluated again if it is the same as an individual in the memory.
    In other words, the performance of individuals sharing the same architectures
    are the same and can be acquired via the population memory instead of training
    from scratch. This mechanism relies on the fact that similar or same individuals
    may repeatedly appear in different generations.'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人口记忆：人口记忆用于在优化过程中存储来自不同世代的精英个体（Sun et al., [2020](#bib.bib192); Fujino et al.,
    [2017](#bib.bib62)）。当生成新的个体时，如果它与记忆中的个体相同，则无需再次评估。换句话说，具有相同架构的个体的表现是相同的，可以通过人口记忆获取，而无需从头开始训练。该机制依赖于相似或相同的个体可能在不同世代中重复出现这一事实。
- en: According to the above introduction, we can conclude that many of them improve
    the search efficiency at the expense of sub-optimiality. For example, a small
    population cannot well cover a multi-objective optimal front. Parameter sharing
    may lead to the biased search due to much similarities among the individuals.
    Highly accurate surrogates need a large number of training data, which are commonly
    time-consuming. Population memory heavily relies on the random emergence of similar
    or same individuals.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述介绍，我们可以得出结论，许多方法通过牺牲次优性来提高搜索效率。例如，小规模种群无法很好地覆盖多目标最优前沿。参数共享可能由于个体之间的相似性过多而导致偏倚搜索。高度准确的代理需要大量的训练数据，这通常是时间消耗大的。人口记忆在很大程度上依赖于相似或相同个体的随机出现。
- en: 'Hardware implementation: Importantly, a powerful hardware platform can significantly
    speed up the search process under the reasonable utilization of computing resources
    (e.g., cloud computing (Chiba et al., [2019](#bib.bib31)) and volunteer computers
    (Atre et al., [2021](#bib.bib14))). Parallel computation is a powerful tool to
    decompose large search problems into small sub-problems, which can be simultaneously
    optimized by several cheaper hardware(Jin et al., [2019](#bib.bib88), [2018](#bib.bib87)).
    For example, Lorenzo et al. (Lorenzo et al., [2017](#bib.bib124)) proposed a parallel
    PSO algorithm to search for optimal architecture of CNN. The security of the computing
    device also becomes an important consideration. For this reason, an emerging decentralized
    privacy-preserving framework is applied to NAS, which unites multiple local clients
    to collaboratively learn a shared global model trained on the parameters or gradients
    of the local models, instead of the raw data. For example, Zhu et al. (Zhu and
    Jin, [2022](#bib.bib263)) firstly proposed a real-time federated NAS that can
    not only optimize the model architecture but also reduce the computational payload.
    Specifically, the decentralized system is able to accelerate the algorithm efficiency
    of federated NAS. Besides, data encryption is employed on the transmitted data
    (parameters or gradients of the local models) between the clients and the server
    to ensure the privacy even though all of the training are performed in local.
    Accordingly, federated NAS is highly efficient and secure, which may become a
    new hot research topic.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件实现：重要的是，强大的硬件平台可以在合理利用计算资源（例如，云计算（Chiba et al., [2019](#bib.bib31)）和志愿计算机（Atre
    et al., [2021](#bib.bib14)））的情况下显著加快搜索过程。并行计算是将大规模搜索问题分解为小型子问题的有力工具，这些子问题可以由几台便宜的硬件同时优化（Jin
    et al., [2019](#bib.bib88), [2018](#bib.bib87)）。例如，Lorenzo et al.（Lorenzo et al.,
    [2017](#bib.bib124)）提出了一种并行PSO算法来搜索CNN的最优架构。计算设备的安全性也成为一个重要的考虑因素。因此，应用于NAS的一个新兴的去中心化隐私保护框架将多个本地客户端联合起来，共同学习一个基于本地模型参数或梯度训练的共享全局模型，而不是原始数据。例如，Zhu
    et al.（Zhu and Jin, [2022](#bib.bib263)）首次提出了一种实时联邦NAS，不仅可以优化模型架构，还能减少计算负担。具体来说，去中心化系统能够加速联邦NAS的算法效率。此外，对客户端与服务器之间传输的数据（本地模型的参数或梯度）进行数据加密，以确保隐私，即使所有训练都在本地进行。因此，联邦NAS高效且安全，可能成为新的热门研究话题。
- en: Table 2. Different acceleration strategies
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 不同的加速策略
- en: '| Algorithm design | Initialization | Reduced population | (Assunção et al.,
    [2019b](#bib.bib13)),(Fan et al., [2020](#bib.bib56)),(Liu et al., [2019a](#bib.bib114)),(Yang
    et al., [2020](#bib.bib230))  \bigstrut |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 算法设计 | 初始化 | 减少的种群 | (Assunção et al., [2019b](#bib.bib13)),(Fan et al.,
    [2020](#bib.bib56)),(Liu et al., [2019a](#bib.bib114)),(Yang et al., [2020](#bib.bib230))
    \bigstrut |'
- en: '| Evaluation | Early stopping policy | (Sun et al., [2019d](#bib.bib191)),(Ahmed
    et al., [2019](#bib.bib3)),(Ortego et al., [2020](#bib.bib152)),(Frachon et al.,
    [2019](#bib.bib60)),(Assunção et al., [2019a](#bib.bib11)),(Assunção et al., [2019b](#bib.bib13)),(Mo
    et al., [2021](#bib.bib139))  \bigstrut |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 早停策略 | (Sun et al., [2019d](#bib.bib191)),(Ahmed et al., [2019](#bib.bib3)),(Ortego
    et al., [2020](#bib.bib152)),(Frachon et al., [2019](#bib.bib60)),(Assunção et al.,
    [2019a](#bib.bib11)),(Assunção et al., [2019b](#bib.bib13)),(Mo et al., [2021](#bib.bib139))
    \bigstrut |'
- en: '| Reduced training set | (Sapra and Pimentel, [2020](#bib.bib173)),(Liu et al.,
    [2019a](#bib.bib114)),(Wang et al., [2020c](#bib.bib208))  \bigstrut |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 减少的训练集 | (Sapra and Pimentel, [2020](#bib.bib173)),(Liu et al., [2019a](#bib.bib114)),(Wang
    et al., [2020c](#bib.bib208)) \bigstrut |'
- en: '| Weight inheritance | Supernet-based sharing | (Sapra and Pimentel, [2020](#bib.bib173)),(Rapaport
    et al., [2019](#bib.bib159)),(Dahal and Zhan, [2020](#bib.bib38)),(Elsken et al.,
    [2019](#bib.bib50))  \bigstrut |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 权重继承 | 基于超网络的共享 | (Sapra and Pimentel, [2020](#bib.bib173)),(Rapaport et al.,
    [2019](#bib.bib159)),(Dahal and Zhan, [2020](#bib.bib38)),(Elsken et al., [2019](#bib.bib50))
    \bigstrut |'
- en: '| Parent-based sharing | (Elsken et al., [2017](#bib.bib49)),(Kwasigroch et al.,
    [2019](#bib.bib104)),(Zhu et al., [2019](#bib.bib262)),(Ahmed et al., [2019](#bib.bib3)),(Frachon
    et al., [2019](#bib.bib60)),(Schorn et al., [2020](#bib.bib174)),(Chen et al.,
    [2019b](#bib.bib28))  \bigstrut |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 基于父级的共享 | (Elsken et al., [2017](#bib.bib49)),(Kwasigroch et al., [2019](#bib.bib104)),(Zhu
    et al., [2019](#bib.bib262)),(Ahmed et al., [2019](#bib.bib3)),(Frachon et al.,
    [2019](#bib.bib60)),(Schorn et al., [2020](#bib.bib174)),(Chen et al., [2019b](#bib.bib28))
    \bigstrut |'
- en: '| Surrogate model | Online performance predictors | (Lu et al., [2021](#bib.bib125)),(Liu
    et al., [2018c](#bib.bib111)), (Ma et al., [2021a](#bib.bib130)), (Lomurno et al.,
    [2021](#bib.bib119))  \bigstrut |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 替代模型 | 在线性能预测器 | (Lu et al., [2021](#bib.bib125)),(Liu et al., [2018c](#bib.bib111)),
    (Ma et al., [2021a](#bib.bib130)), (Lomurno et al., [2021](#bib.bib119)) \bigstrut
    |'
- en: '| Offline performance predictors | (Sun et al., [2019a](#bib.bib188)),(Liu
    et al., [2021a](#bib.bib112)), (Rawal and Miikkulainen, [2018](#bib.bib161))\bigstrut
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 离线性能预测器 | (Sun et al., [2019a](#bib.bib188)),(Liu et al., [2021a](#bib.bib112)),
    (Rawal and Miikkulainen, [2018](#bib.bib161))\bigstrut |'
- en: '|  | Population memory | (Sun et al., [2020](#bib.bib192)),(Miahi et al., [2022](#bib.bib136)),(Li
    et al., [2022](#bib.bib109)),(Chu et al., [2020](#bib.bib34))  \bigstrut |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 人口记忆 | (Sun et al., [2020](#bib.bib192)),(Miahi et al., [2022](#bib.bib136)),(Li
    et al., [2022](#bib.bib109)),(Chu et al., [2020](#bib.bib34)) \bigstrut |'
- en: '| Hardware implementation | (Chiba et al., [2019](#bib.bib31)),(Jin et al.,
    [2019](#bib.bib88)),(Jin et al., [2018](#bib.bib87)),(Zhu and Jin, [2022](#bib.bib263))  \bigstrut
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 硬件实现 | (Chiba et al., [2019](#bib.bib31)),(Jin et al., [2019](#bib.bib88)),(Jin
    et al., [2018](#bib.bib87)),(Zhu and Jin, [2022](#bib.bib263)) \bigstrut |'
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2.4\. Acceleration Strategies ‣ 4.2\. Model Architecture
    Optimization ‣ 4\. Model Generation ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues") lists the common acceleration strategies
    to improve the algorithm efficiency. It is noted that multiple strategies can
    be utilized together to improve computational speed-up. For example, Lu et al.
    (Lu et al., [2021](#bib.bib125)) employed supernet and learning curve performance
    predictor in NAS, while Liu et al. (Liu et al., [2019a](#bib.bib114)) leveraged
    a small populations size and a small dataset to reduce the time overhead of evaluation.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "Table 2 ‣ 4.2.4\. 加速策略 ‣ 4.2\. 模型架构优化 ‣ 4\. 模型生成 ‣ 进化深度学习调查：原理、算法、应用与开放问题")
    列出了常见的加速策略以提高算法效率。值得注意的是，可以结合多种策略以提高计算速度。例如，Lu et al. (Lu et al., [2021](#bib.bib125))
    在 NAS 中使用了超网络和学习曲线性能预测器，而 Liu et al. (Liu et al., [2019a](#bib.bib114)) 利用较小的种群规模和数据集来减少评估的时间开销。
- en: 4.2.5\. Summary
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 摘要
- en: Most NAS methods are based on basic EC search paradigms on an entire-structured
    search space, which are introduced above. However, there are also some other automatic
    search techniques such as RL-based (Jaâfra et al., [2019](#bib.bib86)), Bayesian-based
    (White et al., [2021](#bib.bib215)), and gradient-based (Liu et al., [2018b](#bib.bib113))
    methods, for architecture search.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 NAS 方法基于整个结构化搜索空间上的基本 EC 搜索范式，如上所述。然而，还有一些其他自动搜索技术，例如基于 RL 的 (Jaâfra et al.,
    [2019](#bib.bib86))、基于贝叶斯的 (White et al., [2021](#bib.bib215)) 和基于梯度的 (Liu et al.,
    [2018b](#bib.bib113)) 方法，用于架构搜索。
- en: RL-based methods can be regarded as an incremental search, where a policy function
    is learned by using a reward-prediction error to drive the generation of incremental
    architecture. Due to the large-scale of state space and action space, RL-based
    methods require immense computational resources. In addition, there are a large
    number of hyper-parameters (e.g., discount factor) in RL-based NAS. Besides, they
    transform a multi-objective optimization problem into a single-objective problem
    via a priori or expert knowledge, so they are unable to find a Pareto optimal
    set to the target tasks.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RL 的方法可以被视为一种增量搜索，其中通过使用奖励预测误差来驱动增量架构的生成，从而学习一个策略函数。由于状态空间和动作空间的规模庞大，基于 RL
    的方法需要巨大的计算资源。此外，基于 RL 的 NAS 中有大量的超参数（例如，折扣因子）。此外，它们通过先验或专家知识将多目标优化问题转化为单目标问题，因此无法为目标任务找到一个
    Pareto 最优集。
- en: Bayesian-based methods are a common tool for hyperparametric optimization problems
    with low dimensions. In comparison to EC-based methods, they are much more efficient
    on the condition that a proper distance function has to be designed to evaluate
    the similarities between two subnets. However, the computational cost of Gaussian
    process grows exponentially and its accuracy decreases, when the dimensionality
    of the problem increases.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯的方法是用于低维超参数优化问题的常用工具。与基于 EC 的方法相比，在设计合适的距离函数以评估两个子网之间的相似性时，它们的效率更高。然而，当问题的维度增加时，高斯过程的计算成本呈指数增长，其准确性也会下降。
- en: Gradient-based methods, taking a NAS problem as a continuous differentiable
    problem instead of a discrete one, are able to efficiently search architectures
    with proper weight parameters. Unfortunately, their GPU costs are usually very
    high due to a large number of parameters to be updated in gradient-based algorithms
    (Liu et al., [2018b](#bib.bib113)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的方法将 NAS 问题视为连续可微的问题，而不是离散问题，能够有效地搜索具有适当权重参数的架构。不幸的是，由于梯度方法中需要更新的参数数量庞大，这些方法的
    GPU 成本通常非常高 (Liu et al., [2018b](#bib.bib113))。
- en: In contrast, EC-based methods benefit from less hyperparameters to be optimized
    and no distance functions to be designed. In addition, EC-based methods can be
    applied to NAS with multiple objectives and constraints. Although there many acceleration
    strategies in EC-based methods, they still suffer from high computational overheads.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于 EC 的方法受益于需要优化的超参数较少且无需设计距离函数。此外，基于 EC 的方法可以应用于具有多个目标和约束的 NAS。尽管基于 EC
    的方法有许多加速策略，但它们仍然面临高计算开销的问题。
- en: 4.3\. Joint Optimization
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 联合优化
- en: 4.3.1\. Problem Formulation
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 问题表述
- en: 'The independent optimization of architecture or parameters is difficult to
    achieve the optimal model on give tasks. Hence, joint optimization methods have
    been developed to search for the optimal configuration of architecture ($A^{*}$),
    and parameters ($W^{*}$, associated weights). The optimization problem can be
    defined in Eq. [4](#S4.E4 "In 4.3.1\. Problem Formulation ‣ 4.3\. Joint Optimization
    ‣ 4\. Model Generation ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 架构或参数的独立优化难以在给定任务上实现**最佳模型**。因此，已经开发了联合优化方法来搜索**架构**($A^{*}$)和**参数**($W^{*}$，相关权重)的最佳配置。优化问题可以在
    Eq. [4](#S4.E4 "在 4.3.1\. 问题表述 ‣ 4.3\. 联合优化 ‣ 4\. 模型生成 ‣ 进化深度学习的调查：原则、算法、应用及开放问题")中定义。
- en: '| (4) |  | $\displaystyle\begin{matrix}\left({{W}^{*}},{{A}^{*}}\right)$=$\underset{W,A}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\begin{matrix}\left({{W}^{*}},{{A}^{*}}\right)$=$\underset{W,A}{\arg\min}L\left(W,A\right)\end{matrix}$
    |  |'
- en: where $L$ is the loss function.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是损失函数。
- en: In the followings, we will introduce the joint optimization regarding the solution
    representations and search paradigms, and then discuss the pros and cons of EC-based
    methods in comparison to others.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍有关解决方案表示和搜索范式的联合优化，然后讨论与其他方法相比，基于 EC 的方法的优缺点。
- en: 4.3.2\. Solution Representations
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 解决方案表示
- en: There are three typical classes of encoding schemes used for joint optimization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种典型的编码方案用于联合优化。
- en: 'Linear encoding: This is a simple but effective encoding strategy, which has
    been widely used in many studies to build architecture with high performance (Aljarah
    et al., [2018](#bib.bib9); Maniezzo, [1994](#bib.bib132)). In (Maniezzo, [1994](#bib.bib132)),
    a variable-length binary vector was used to represent weights and structure of
    neural networks, where the weights utilize direct encoding.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 线性编码：这是一种简单但有效的编码策略，在许多研究中被广泛使用，以构建高性能的架构（Aljarah 等，[2018](#bib.bib9)；Maniezzo，[1994](#bib.bib132)）。在
    (Maniezzo，[1994](#bib.bib132)) 中，使用了一个变长的二进制向量来表示神经网络的权重和结构，其中权重利用直接编码。
- en: 'Tree-based encoding: In this encoding, the topology and weights of an architecture
    can be represented by a tree structure with a number of nodes and edges (Zhang
    and Mühlenbein, [1995](#bib.bib240); Golubski and Feuring, [1999](#bib.bib65)).
    In (Zhang et al., [2020b](#bib.bib242)), the mechanism of Reverse Encoding Tree
    (RET) was developed to ensure the robustness of a deep model, where the topological
    information of an architecture was represented by a combination of nodes and the
    weight information was recorded on the edges.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的编码：在这种编码中，架构的拓扑和权重可以通过一个具有多个节点和边的树结构来表示（Zhang 和 Mühlenbein，[1995](#bib.bib240)；Golubski
    和 Feuring，[1999](#bib.bib65)）。在 (Zhang 等，[2020b](#bib.bib242)) 中，开发了反向编码树（RET）机制，以确保深度模型的鲁棒性，其中架构的拓扑信息由节点组合表示，权重信息记录在边缘上。
- en: 'Graph-based encoding: In this encoding, the nodes of a graph represent neurons
    or other network units, and the edges are used to record the weight information
    (Han and Cho, [2006](#bib.bib70); Chai et al., [2022](#bib.bib21)). For example,
    a graph incidence matrix was developed in (Oong and Isa, [2011](#bib.bib151))
    to encode a neural network. The size of the matrix was set to ($N_{i}$ + $N_{h}$
    + $N_{o}$) $\times$ ($N_{i}$ + $N_{h}$ + $N_{o}$), where $N_{i}$, $N_{h}$ and
    $N_{o}$ indicate the numbers of input, hidden, and output nodes, respectively.
    In the graph incidence matrix, real numbers represented the weight and biases,
    and “0” meant that there was no connection between two nodes.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的编码：在这种编码中，图的节点代表神经元或其他网络单元，边缘用于记录权重信息（Han 和 Cho，[2006](#bib.bib70)；Chai
    等，[2022](#bib.bib21)）。例如，在 (Oong 和 Isa，[2011](#bib.bib151)) 中，开发了一个图的发生矩阵来编码神经网络。矩阵的大小设置为
    ($N_{i}$ + $N_{h}$ + $N_{o}$) $\times$ ($N_{i}$ + $N_{h}$ + $N_{o}$)，其中 $N_{i}$、$N_{h}$
    和 $N_{o}$ 分别表示输入、隐藏和输出节点的数量。在图的发生矩阵中，实数表示权重和偏置，“0”表示两个节点之间没有连接。
- en: 4.3.3\. Search Paradigms
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 搜索范式
- en: There are a number of effective search paradigms for joint optimization, and
    the EC-based search paradigms are in the spotlight.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有效的搜索范式用于联合优化，其中基于 EC 的搜索范式备受关注。
- en: 'Basic EC search paradigm: Some basic EC search methods have been employed to
    handle joint optimization problems (Fahrudin et al., [2016](#bib.bib55); Oong
    and Isa, [2011](#bib.bib151); Stanley and Miikkulainen, [2002](#bib.bib186); Barman
    and Kwon, [2020](#bib.bib15); Behjat and Chidambaran, [2019](#bib.bib16)). In
    (Oong and Isa, [2011](#bib.bib151)), an architecture and its corresponding weights
    were simultaneously optimized by an EC-based method using linear and graph encodings.
    In neuro-evolution of augmenting topologies (NEAT) (Stanley and Miikkulainen,
    [2002](#bib.bib186)), the architecture of a small network is evolved by an incremental
    mechanism, while the weights are optimized by an EC-based method. NEAT is able
    to ensure the lowest dimensional search space over all generations. Some representative
    studies on NEAT are presented in (Barman and Kwon, [2020](#bib.bib15); Behjat
    and Chidambaran, [2019](#bib.bib16)).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 EC 搜索范式：一些基本的 EC 搜索方法已经被用于处理联合优化问题（Fahrudin 等，[2016](#bib.bib55)；Oong 和
    Isa，[2011](#bib.bib151)；Stanley 和 Miikkulainen，[2002](#bib.bib186)；Barman 和 Kwon，[2020](#bib.bib15)；Behjat
    和 Chidambaran，[2019](#bib.bib16)）。在 (Oong 和 Isa，[2011](#bib.bib151)) 中，通过使用线性和图编码的基于
    EC 的方法，同时优化了一个架构及其对应的权重。在增强拓扑的神经进化（NEAT）（Stanley 和 Miikkulainen，[2002](#bib.bib186)）中，小型网络的架构通过增量机制进行进化，而权重由基于
    EC 的方法进行优化。NEAT 能够在所有代中确保最低维度的搜索空间。在 (Barman 和 Kwon，[2020](#bib.bib15)；Behjat
    和 Chidambaran，[2019](#bib.bib16)) 中介绍了一些 NEAT 的代表性研究。
- en: 'Multi-objective search paradigm: Multi-objective optimization on model design
    has been developed in many studies (e.g., artificial neural network (Rostami and
    Neri, [2016](#bib.bib167)) and recurrent neural network (Smith and Jin, [2014](#bib.bib182))).
    For example, Smith et al. (Smith and Jin, [2014](#bib.bib182)) built a bi-objective
    optimization (i.e., the minimizations of the mean squared error (MSE) on a training
    dataset and the number of connections in the network) to search for optimal weights
    and connections of network architectures. The chromosome of an individual was
    composed of two parts, where the one with Boolean type represented the structure
    of a network, and the other with real values represented the weights.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标搜索范式：在模型设计上的多目标优化已经在许多研究中得到发展（例如，人工神经网络（Rostami 和 Neri, [2016](#bib.bib167)）和递归神经网络（Smith
    和 Jin, [2014](#bib.bib182)））。例如，Smith 等人（Smith 和 Jin, [2014](#bib.bib182)）建立了一个双目标优化（即，最小化训练数据集上的均方误差（MSE）和网络中的连接数）来搜索网络架构的最优权重和连接。个体的染色体由两部分组成，其中
    Boolean 类型的部分代表网络的结构，另一个具有实数值的部分代表权重。
- en: 4.3.4\. Summary
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 总结
- en: Direct encoding is used to be prevalent in the joint optimization of small-scale
    neural networks (Oong and Isa, [2011](#bib.bib151); Yao and Liu, [1996](#bib.bib233)).
    However, with the increase of the scale of neural networks, direct coding of high-dimensional
    vector or matrix of weights is not realistic. Therefore, recent studies are more
    on indirect encoding. For example, a complex mapping with acceptable accuracy
    loss is designed in (Koutník et al., [2014](#bib.bib100); D’Ambrosio and Stanley,
    [2007](#bib.bib40)) to construct weight vectors with arbitrary size.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 直接编码在小规模神经网络的联合优化中曾经很流行（Oong 和 Isa, [2011](#bib.bib151)；Yao 和 Liu, [1996](#bib.bib233)）。然而，随着神经网络规模的增加，直接编码高维向量或矩阵的权重已不再现实。因此，近期的研究更多集中在间接编码上。例如，在（Koutník
    等, [2014](#bib.bib100)；D’Ambrosio 和 Stanley, [2007](#bib.bib40)）中设计了一个复杂的映射，具有可接受的准确度损失，以构造任意大小的权重向量。
- en: EC-based approaches with capability of searching the optimal solution have been
    developed to configure a DL model for the specific task. However, they often encounter
    a prohibitive computational cost, which is even higher than that of model architecture
    optimization. Hence, designing efficient EC-based approaches for architecture
    and parameter search deserves much investigation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 具有搜索最优解能力的基于进化计算的方法已被开发出来，以配置用于特定任务的深度学习模型。然而，它们通常会遇到难以承受的计算成本，这甚至高于模型架构优化的成本。因此，设计高效的基于进化计算的架构和参数搜索方法值得深入研究。
- en: 5\. Model Deployment
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 模型部署
- en: The large-scale DNNs are not straightforward to be deployed into devices (e.g.,
    smartphones) with limited computation and storage resources (e.g., battery capacity
    and memory size). To solve this issue, various model compression approaches have
    been proposed to reduce the model size and inference time, such as pruning, model
    distillation, and quantization (Choudhary et al., [2020](#bib.bib32)). However,
    they need much expert knowledge and a lot of efforts on the manual compression
    of neural network models. In contrast, EC-based approaches are automation approaches
    and has been recently introduced to achieve automated model compression. We have
    observed that most of them concentrate on the area of model pruning.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模深度神经网络在部署到计算和存储资源有限的设备（例如，智能手机）上并不简单（例如，电池容量和内存大小）。为了解决这个问题，已经提出了各种模型压缩方法来减少模型大小和推理时间，如剪枝、模型蒸馏和量化（Choudhary
    等, [2020](#bib.bib32)）。然而，这些方法需要大量的专家知识和对神经网络模型的手动压缩付出很多努力。相比之下，基于进化计算的方法是自动化的方法，并且最近被引入以实现自动化模型压缩。我们观察到，它们中的大多数集中在模型剪枝领域。
- en: 5.1\. Model Pruning
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 模型剪枝
- en: 5.1.1\. Problem Formulation
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 问题描述
- en: DNN is commonly an over-parameterized model, which has redundant and non-informative
    components (e.g., weights, channels and filters). To address this issue, researchers
    have designed various pruning approaches (e.g., channel pruning (He et al., [2017](#bib.bib73)))
    to obtain a lightweight deep network model with high accuracy. Model pruning can
    be formulated as
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通常是一个过度参数化的模型，具有冗余和非信息性的组件（例如，权重、通道和滤波器）。为了解决这个问题，研究人员设计了各种剪枝方法（例如，通道剪枝（He
    等, [2017](#bib.bib73)））来获得一个具有高准确度的轻量级深度网络模型。模型剪枝可以被表述为
- en: '| (5) |  |  | $\displaystyle Los{{s}_{A_{s}^{*}}}\approx Los{{s}_{A}}$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  |  | $\displaystyle Los{{s}_{A_{s}^{*}}}\approx Los{{s}_{A}}$ |  |'
- en: '|  |  | $\displaystyle\begin{matrix}\text{s}\text{.t}\text{.}&amp;{A}_{{s}}^{*}$=$\underset{{C}}{\mathop{\text{pruning}}}\,\left({A,C}\right)\\
    \end{matrix}$ |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\begin{matrix}\text{s}\text{.t}\text{.}&amp;{A}_{{s}}^{*}$=$\underset{{C}}{\mathop{\text{pruning}}}\,\left({A,C}\right)\\
    \end{matrix}$ |  |'
- en: where $C$ represents redundant and non-informative units, $A$ and $A{{}_{s}}^{*}$
    represent original model and lightweight model, respectively, $Los{{s}_{A_{s}^{*}}}$
    and $Los{{s}_{A}}$ represent loss of $A{{}_{s}}^{*}$ and $A$.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C$ 表示冗余和无信息的单元，$A$ 和 $A{{}_{s}}^{*}$ 分别表示原始模型和轻量级模型，$Los{{s}_{A_{s}^{*}}}$
    和 $Los{{s}_{A}}$ 分别表示 $A{{}_{s}}^{*}$ 和 $A$ 的损失。
- en: This study aims to introduce EC-based methods for model pruning, and readers
    interested in traditional pruning methods such as weight-based pruning, neuron-based
    pruning, filter-based pruning, layer-based pruning, and channel-based pruning
    may refer to the surveys (Choudhary et al., [2020](#bib.bib32)) to get more details.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究旨在引入基于 EC 的模型剪枝方法，感兴趣于传统剪枝方法如基于权重的剪枝、基于神经元的剪枝、基于滤波器的剪枝、基于层的剪枝和基于通道的剪枝的读者可以参考调查（Choudhary
    et al., [2020](#bib.bib32)）以获取更多细节。
- en: 5.1.2\. Solution Representations
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 解决方案表示
- en: For model pruning, binary encoding is one of the most popular approaches among
    these solution representations, where each element corresponds to the network
    component (e.g., channel). In (Wang et al., [2018](#bib.bib212)), the network
    pruning task was formulated as a binary programming problem, where a binary variable
    was directly associated with each convolution filter to determine whether or not
    the filter took effect. Although binary representation is straightforward, the
    length of the representation becomes large when the model complexity (i.e., the
    number of units) improves, and the overhead of exploration will also increase.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型剪枝，二进制编码是这些解决方案表示方法中最流行的方式之一，其中每个元素对应于网络组件（例如，通道）。在（Wang et al., [2018](#bib.bib212)）中，网络剪枝任务被表述为一个二进制编程问题，其中一个二进制变量直接与每个卷积滤波器相关联，以确定该滤波器是否有效。尽管二进制表示方式简单明了，但当模型复杂度（即单位数量）增加时，表示的长度也会变得很大，探索的开销也会增加。
- en: To address the above issue, some efficient solution representations (i.e., indirect
    encoding) have been developed. For example, Liu et al. (Liu and Guo, [2021](#bib.bib115))
    used $N$ digits to record the number of compressed layers. The first digit represented
    the number of compressed layers, and following digits recorded the selected compression
    operator index of each layer. This way can significantly improve the search efficiency.
    In (Liu et al., [2019b](#bib.bib118)), encoding vectors are used to represent
    the number of channels in each layer for original networks. Then a meta-network
    is constructed to generate the weights according to network encoding vectors.
    By stochastically fed with different structure encoding, the meta-network gradually
    learns to generate weights for various pruned structures.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述问题，一些高效的解决方案表示（即间接编码）已经被开发出来。例如，Liu et al.（Liu and Guo, [2021](#bib.bib115)）使用
    $N$ 位来记录压缩层的数量。第一位表示压缩层的数量，后续位记录每层选择的压缩操作符索引。这种方式可以显著提高搜索效率。在（Liu et al., [2019b](#bib.bib118)）中，编码向量用于表示原始网络中每层的通道数量。然后构建一个元网络，根据网络编码向量生成权重。通过以不同的结构编码随机喂入，元网络逐渐学习生成各种剪枝结构的权重。
- en: 5.1.3\. Search Paradigms
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 搜索范式
- en: The search paradigms in model pruning studies can be categorized into two main
    groups.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 模型剪枝研究中的搜索范式可以分为两大类。
- en: 'Basic EC search paradigm: A number of studies introduce single-objective EC
    search paradigm for model pruning (Wu et al., [2021a](#bib.bib218); Tang et al.,
    [2019](#bib.bib195)). For example, Wu et al. (Wu et al., [2021a](#bib.bib218))
    first analysed the pruning sensitivity on weights via differential evolution (DE),
    and then the model was compressed by iteratively performing the weight pruning
    process according to the weight sensitivity. In addition, this method adopted
    a recovery strategy to increase the pruned model performance during the fine-tuning
    phase.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 EC 搜索范式：一些研究介绍了单目标 EC 搜索范式用于模型剪枝（Wu et al., [2021a](#bib.bib218); Tang et
    al., [2019](#bib.bib195)）。例如，Wu et al.（Wu et al., [2021a](#bib.bib218)）首先通过差分进化（DE）分析了权重的剪枝敏感性，然后通过根据权重敏感性迭代执行权重剪枝过程来压缩模型。此外，该方法在微调阶段采用了恢复策略，以提高剪枝模型的性能。
- en: 'Multi-objective search paradigm: Recently, this sort of search paradigm has
    been adopted to model pruning, which is able to provide users with a set of Pareto
    lightweight models. For example, Zhou et al. (Zhou et al., [2020](#bib.bib259))
    considered two objectives (i.e., minimizing convolutional filters and maximizing
    the model performance) for biomedical image segmentation. During the model pruning,
    a classical multi-objective optimization algorithm (NSGA-II (Deb et al., [2002](#bib.bib42)))
    was used find the optimal set of non-dominated solutions, where the optimization
    was based on a binary string encoding (each bit represents a filter).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标搜索范式：最近，这种搜索范式已被采用到模型修剪中，能够为用户提供一组帕累托轻量级模型。例如，周等人（周等，[2020](#bib.bib259)）考虑了两个目标（即最小化卷积滤波器和最大化模型性能）用于生物医学图像分割。在模型修剪过程中，使用了一个经典的多目标优化算法（NSGA-II
    (Deb等，[2002](#bib.bib42)）），找到了一组非支配解的最优集，其中优化基于二进制字符串编码（每个比特代表一个滤波器）。
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.1.3\. Search Paradigms ‣ 5.1\. Model Pruning
    ‣ 5\. Model Deployment ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues"), we have summarized these two categories of search
    paradigms as well as their corresponding ways of encoding.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[3](#S5.T3 "Table 3 ‣ 5.1.3\. Search Paradigms ‣ 5.1\. Model Pruning ‣ 5\.
    Model Deployment ‣ Survey on Evolutionary Deep Learning: Principles, Algorithms,
    Applications and Open Issues")中，我们总结了这两类搜索范式以及它们对应的编码方式。'
- en: Table 3. Different search paradigms and solutions representations for model
    pruning
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 模型修剪的不同搜索范式和解决方案表示
- en: '|  | Direct encoding | Indirect encoding \bigstrut |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | 直接编码 | 间接编码 \bigstrut |'
- en: '| Basic EC search paradigm | (Samala et al., [2018](#bib.bib171)),(Junior and
    Yen, [2021a](#bib.bib91)),(Tang et al., [2019](#bib.bib195)),(Wu et al., [2021a](#bib.bib218)),(Junior
    and Yen, [2021b](#bib.bib92)),(Zhou et al., [2021b](#bib.bib260)),(Gerum et al.,
    [2020](#bib.bib64)),(Zemouri et al., [2019](#bib.bib238)),(Chen et al., [2019a](#bib.bib26)),(Poyatos
    et al., [2022](#bib.bib157)),(Fernandes and Yen, [2021](#bib.bib57)),(Li et al.,
    [2020](#bib.bib107)),(Shang et al., [2022](#bib.bib178)) | (Liu et al., [2019b](#bib.bib118))  \bigstrut
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 基本 EC 搜索范式 | (Samala等，[2018](#bib.bib171)),(Junior和Yen，[2021a](#bib.bib91)),(唐等，[2019](#bib.bib195)),(吴等，[2021a](#bib.bib218)),(Junior和Yen，[2021b](#bib.bib92)),(周等，[2021b](#bib.bib260)),(Gerum等，[2020](#bib.bib64)),(Zemouri等，[2019](#bib.bib238)),(陈等，[2019a](#bib.bib26)),(Poyatos等，[2022](#bib.bib157)),(Fernandes和Yen，[2021](#bib.bib57)),(李等，[2020](#bib.bib107)),(尚等，[2022](#bib.bib178))
    | (刘等，[2019b](#bib.bib118))  \bigstrut |'
- en: '| Multi-objective search paradigm | (Zhou et al., [2020](#bib.bib259)),(Zhou
    et al., [2021c](#bib.bib261)),(Wu et al., [2019](#bib.bib219)),(Hong et al., [2020](#bib.bib76)),(Xu
    et al., [2021](#bib.bib224)),(Yang et al., [2019](#bib.bib228)),(Zhang et al.,
    [2021a](#bib.bib245)) | (Wang et al., [2021a](#bib.bib213)),(Zhang et al., [2021b](#bib.bib254)),(Loni
    et al., [2020](#bib.bib122))  \bigstrut |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 多目标搜索范式 | (周等，[2020](#bib.bib259)),(周等，[2021c](#bib.bib261)),(吴等，[2019](#bib.bib219)),(洪等，[2020](#bib.bib76)),(徐等，[2021](#bib.bib224)),(杨等，[2019](#bib.bib228)),(张等，[2021a](#bib.bib245))
    | (王等，[2021a](#bib.bib213)),(张等，[2021b](#bib.bib254)),(Loni等，[2020](#bib.bib122))  \bigstrut
    |'
- en: 5.2\. Other EC-based Model Deployment Methods
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 其他基于 EC 的模型部署方法
- en: Different from model pruning, there are several other EC-based model compression
    methods for model deployment. In the followings, some typical methods are introduced,
    including knowledge distillation, low-rank factorization, and EC for hybrid techniques.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型修剪不同，还有一些其他基于 EC 的模型压缩方法用于模型部署。接下来，介绍了一些典型的方法，包括知识蒸馏、低秩因子分解和混合技术的 EC。
- en: 5.2.1\. Knowledge Distillation
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 知识蒸馏
- en: Knowledge distillation (KD) (Gou et al., [2021](#bib.bib66)) aims to get a small
    light network but with good generalization capability. The basic idea is to transfer
    the knowledge learned from a big cumbersome network (or teacher network) with
    good generalization ability to a small but light network (or student network).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）（高等，[2021](#bib.bib66)）旨在获得一个小型轻量级网络，但具有良好的泛化能力。其基本思想是将从一个大型繁琐网络（或教师网络）学到的知识，转移到一个小而轻的网络（或学生网络）。
- en: However, knowledge distillation may be seriously influenced when there is a
    big gap in the learning capability between the teacher and student networks. In
    other words, if the difference is large, the student network may not be able to
    learn knowledge from the teacher network. Recently, several EC-based approaches
    have been proposed to mitigate the above issue of knowledge distillation. For
    example, Wu et al. (Wu et al., [2020](#bib.bib220)) proposed an evolutionary embedding
    learning (EEL) paradigm to learn a fast accurate student network via massive knowledge
    distillation. Their experimental results show that the EEL is able to narrow the
    performance between the teacher and student networks on given tasks. Zhang et
    al. (Zhang et al., [2022a](#bib.bib246)) developed an evolutionary knowledge distillation
    method to improve the effectiveness of knowledge transfer. In this method, an
    evolutionary teacher was learned online and consistently transfers intermediate
    knowledge to the student network to narrow the gap of the learning capability
    between them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当教师网络和学生网络之间的学习能力差距很大时，知识蒸馏可能会受到严重影响。换句话说，如果差距很大，学生网络可能无法从教师网络中学习知识。最近，提出了几种基于EC的方法来缓解知识蒸馏中的上述问题。例如，Wu
    et al.（Wu et al., [2020](#bib.bib220)）提出了一种进化嵌入学习（EEL）范式，通过大量知识蒸馏来学习一个快速准确的学生网络。他们的实验结果表明，EEL能够缩小教师和学生网络在特定任务上的性能差距。Zhang
    et al.（Zhang et al., [2022a](#bib.bib246)）开发了一种进化知识蒸馏方法，以提高知识转移的有效性。在此方法中，进化教师在线学习并持续将中间知识传递给学生网络，以缩小两者之间的学习能力差距。
- en: 5.2.2\. Low-rank Factorization
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 低秩分解
- en: DNNs often involve in a huge number of weights, which may impact the inference
    speed and seriously increase the storage overhead of the DNN. The weights can
    be viewed as a matrix $W$ with $m$ $\times$ $n$ dimensions. The low-rank approach
    is commonly applied to the weight matrix ($W$) after the DNN is fully trained.
    For example, singular value decomposition (Fortuna and Frasca, [2021](#bib.bib59))
    is a typical low-rank factorization method, where $W$ is decomposed as follows.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）通常涉及大量的权重，这可能会影响推理速度，并严重增加DNN的存储开销。权重可以视为一个$W$的矩阵，具有$m$ $\times$
    $n$维度。低秩方法通常应用于DNN完全训练后的权重矩阵（$W$）。例如，奇异值分解（Fortuna 和 Frasca，[2021](#bib.bib59)）是一种典型的低秩分解方法，其中$W$被分解如下。
- en: '| (6) |  | $\displaystyle\begin{matrix}W$=$USV^{T}\end{matrix}$ |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\begin{matrix}W$=$USV^{T}\end{matrix}$ |  |'
- en: where $U$ $\in$ $R^{m\times m}$, $V^{T}$ $\in$ $R^{n\times n}$ are orthogonal
    matrices and $S$ $\in$ $R^{m\times n}$ is a diagonal matrix.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$U$ $\in$ $R^{m\times m}$，$V^{T}$ $\in$ $R^{n\times n}$是正交矩阵，$S$ $\in$ $R^{m\times
    n}$是对角矩阵。
- en: Notably, most of the existing low-rank factorization methods rely on domain
    expertise and experience for the selection of hyperparameters (e.g., the rank
    and sparsity of weight matrix) to get an appropriate compression results without
    serious performance degradation (Swaminathan et al., [2020](#bib.bib193); Winata
    et al., [2019](#bib.bib216); Hsu et al., [2021](#bib.bib78)).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大多数现有的低秩分解方法依赖于领域专业知识和经验来选择超参数（例如，权重矩阵的秩和稀疏性），以获得合适的压缩结果，而不会出现严重的性能下降（Swaminathan
    et al., [2020](#bib.bib193)；Winata et al., [2019](#bib.bib216)；Hsu et al., [2021](#bib.bib78)）。
- en: Accordingly, EC-based methods have been introduced to solve the above challenge
    (Wang et al., [2018](#bib.bib212); Huang et al., [2020](#bib.bib82)). For example,
    Huang et al. (Huang et al., [2020](#bib.bib82)) presented a multi-objective evolution
    approach to automatically optimize rank and sparsity for weight matrix without
    human intervention, where two objectives were taken into account including the
    minimization of the model classification error rate and maximization of the model
    compression rate. They therefore generated a set of approximately compressed models
    with different compression rates to mitigate the expensive training process.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，基于EC的方法已被引入以解决上述挑战（Wang et al., [2018](#bib.bib212)；Huang et al., [2020](#bib.bib82)）。例如，Huang
    et al.（Huang et al., [2020](#bib.bib82)）提出了一种多目标进化方法，以自动优化权重矩阵的秩和稀疏性，无需人工干预，其中考虑了两个目标，包括最小化模型分类错误率和最大化模型压缩率。因此，他们生成了一组具有不同压缩率的近似压缩模型，以缓解昂贵的训练过程。
- en: 5.2.3\. EC for Jointly Optimization
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 联合优化中的EC
- en: Many compression techniques (e.g., quantization) can be easily applied on top
    of other techniques (e.g., pruning and low-rank factorization). For example, pruning
    first and then quantification can obtain a lightweight model with faster inference.
    Similarly, EC can optimize more than one model compression method at the same
    time. In the followings, we will briefly review such works.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 许多压缩技术（例如，量化）可以轻松地应用于其他技术（例如，剪枝和低秩分解）。例如，先剪枝再量化可以获得一个具有更快推理速度的轻量级模型。同样，EC可以同时优化多个模型压缩方法。接下来，我们将简要回顾这些工作。
- en: Phan et al. (Phan et al., [2020](#bib.bib155)) designed an efficient 1-Bit CNNs,
    which combined quantization with a compact model. Specifically, they firstly created
    a number of strong baseline binary networks (BNNs), which had abundant random
    group combinations at each convolutional layer. Then, they adopted evolutionary
    search to seek an optimal group convolution combination with accuracy above threshold.
    Finally, the obtained binary models werr trained from scratch to achieve the final
    lightweight network. Different from (Phan et al., [2020](#bib.bib155)), Polino
    et al. (Polino et al., [2018](#bib.bib156)) jointly utilized weight quantization
    and distillation to compress large networks (i.e., teacher network) into small
    networks (i.e., student network), where the latency and model error were regarded
    as the objectives during the optimization.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Phan等人（Phan et al., [2020](#bib.bib155)）设计了一种高效的1-Bit CNNs，将量化与紧凑模型相结合。具体而言，他们首先创建了一些强大的基线二值网络（BNNs），这些网络在每个卷积层有丰富的随机组组合。然后，他们采用进化搜索来寻求一个准确度高于阈值的最佳组卷积组合。最后，获得的二值模型从头开始训练，以实现最终的轻量级网络。与（Phan
    et al., [2020](#bib.bib155)）不同，Polino等人（Polino et al., [2018](#bib.bib156)）联合利用权重量化和蒸馏将大型网络（即教师网络）压缩成小型网络（即学生网络），在优化过程中，将延迟和模型误差作为目标。
- en: Recently, Zhou et al. (Zhou et al., [2021b](#bib.bib260)) developed an evolutionary
    algorithm-based method for shallowing DNNs at block levels (ESNB). In ESNB, a
    prior knowledge was extracted from the original model to guide the population
    initialization. Then, an evolutionary multi-objective optimization mothed was
    performed to minimize the number of blocks and the accuracy drop (i.e., loss).
    After that, knowledge distillation was employed to compensate for the performance
    degradation via matching output of the pruned model with the softened and hardened
    output of the original model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Zhou等人（Zhou et al., [2021b](#bib.bib260)）开发了一种基于进化算法的方法，用于在块级别浅化DNN（ESNB）。在ESNB中，从原始模型中提取了先验知识以指导种群初始化。然后，执行了一种进化多目标优化方法，以最小化块的数量和准确度下降（即损失）。之后，采用知识蒸馏来弥补通过将剪枝模型的输出与原始模型的软化和硬化输出匹配所带来的性能退化。
- en: 5.2.4\. Summary
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 摘要
- en: There is still a big room for the improvement on addressing the huge computational
    overhead of evolutionary model deployment. Acceleration strategies may be able
    to alleviate the issue. Besides, there is a high coupling between model deployment
    and model generation since the performance of the compressed network is strongly
    dependent on the performance of the original network. The black-box nature of
    model also hampers deployment in security-critical tasks (e.g., medicine and finance).Consequently,
    it is promising and challenging to take the model compression, NAS, and interpretability
    as a single optimization problem and handle it with acceptable time consumption.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决进化模型部署的巨大计算开销方面仍有很大的改进空间。加速策略可能有助于缓解这个问题。此外，由于压缩网络的性能强烈依赖于原始网络的性能，模型部署与模型生成之间存在较高的耦合性。模型的黑箱性质也阻碍了在安全关键任务（例如，医学和金融）中的部署。因此，将模型压缩、NAS和可解释性作为一个单一优化问题来处理，并在可接受的时间消耗内完成这一任务是充满前景但又具有挑战性的。
- en: 6\. Applications, Open Issues, and Trends
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 应用、开放问题与趋势
- en: 6.1\. Applications
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 应用
- en: EDL algorithms have been widely used in various real-world applications. In
    practical, great development has been achieved in computer vision (CV), natural
    language processing (NLP) and other practical applications (e.g., crisis prediction
    and disease prediction).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: EDL算法已广泛应用于各种现实世界的应用中。在实践中，计算机视觉（CV）、自然语言处理（NLP）及其他实际应用（例如，危机预测和疾病预测）取得了显著进展。
- en: 6.1.1\. Computer Vision
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 计算机视觉
- en: CV is an important domain of computer science, playing an important role in
    identifying useful information (e.g., objects and classifications) for specific
    tasks (e.g., image segmentation (Zhou et al., [2020](#bib.bib259)) and object
    detection (Zhang and Rockett, [2005](#bib.bib252))) on images or videos. In the
    early days, manually designed models for computer vision achieved good performance
    on public datasets at the expense of extensive time and labour. With the development
    of EDL, many new structures have been developed by computer programming and they
    show better performance than these manually designed models, especially on the
    widely used benchmark datasets for image classification, such as CIFAR-10, CIFAR-100
    (Krizhevsky et al., [2009](#bib.bib102)), and ImageNet (Deng et al., [2009](#bib.bib44)).
    For example, the state-of-the-art NAT-M4 (Lu et al., [2021](#bib.bib125)) with
    a small model size achieves Top-1 accuracy of 80.5% on ImageNet. Image-to-image
    processing (Liu et al., [2021b](#bib.bib117)) (e.g., super-resolution, image inpainting,
    and image restoration) also received extensive attention from researchers (Rundo
    et al., [2019](#bib.bib170); Song et al., [2020](#bib.bib184); Zhan et al., [2021](#bib.bib239)).
    Ho et al. (Ho et al., [2021](#bib.bib74)) employed NAS techniques to improve image
    denoising, inpainting, and super-resolution on the foundation of deep image prior
    (Ho et al., [2021](#bib.bib74)). In addition to the above applications, EDL also
    has great potential in other areas of CV, such as object detection (Demirkir and
    Sankur, [2006](#bib.bib43)), video/picture understanding (Maniezzo, [1994](#bib.bib132)),
    and image segmentation (Fan et al., [2020](#bib.bib56)).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是计算机科学中的一个重要领域，在识别有用信息（例如，对象和分类）方面发挥了重要作用（例如，图像分割（Zhou et al., [2020](#bib.bib259)）和对象检测（Zhang
    and Rockett, [2005](#bib.bib252)））。早期，手动设计的计算机视觉模型在公共数据集上表现良好，但付出了大量时间和劳力。随着EDL的发展，许多新结构由计算机编程开发，并且它们在图像分类等广泛使用的基准数据集上表现优于这些手动设计的模型，如CIFAR-10、CIFAR-100（Krizhevsky
    et al., [2009](#bib.bib102)）和ImageNet（Deng et al., [2009](#bib.bib44)）。例如，最先进的NAT-M4（Lu
    et al., [2021](#bib.bib125)）在ImageNet上以较小的模型规模实现了80.5%的Top-1准确率。图像到图像处理（Liu et
    al., [2021b](#bib.bib117)）（例如，超分辨率、图像修补和图像恢复）也受到了研究人员的广泛关注（Rundo et al., [2019](#bib.bib170);
    Song et al., [2020](#bib.bib184); Zhan et al., [2021](#bib.bib239)）。Ho et al.（Ho
    et al., [2021](#bib.bib74)）在深度图像先验的基础上使用了NAS技术来改进图像去噪、修补和超分辨率。除了上述应用，EDL在CV的其他领域也具有巨大潜力，例如对象检测（Demirkir
    and Sankur, [2006](#bib.bib43)）、视频/图片理解（Maniezzo, [1994](#bib.bib132)）和图像分割（Fan
    et al., [2020](#bib.bib56)）。
- en: 6.1.2\. Natural Language Processing
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 自然语言处理
- en: Natural language processing (NLP) driven by computer science and computational
    linguistics, aims to understand, analyze, and extract knowledge on text and speech
    recoginition (Young et al., [2018](#bib.bib235)). Many effective NLP models (e.g.,
    GPT-2 (Radford et al., [2018](#bib.bib158)) and BERT (Devlin et al., [2018](#bib.bib45)))
    narrow the chasm between human communication and computer understanding using
    sophisticated mechanisms. Recently, EC-inspired NLP models have been proposed
    such as language model (Murray and Chiang, [2015](#bib.bib142)), entity recognition
    (Sikdar et al., [2012](#bib.bib181)), text classification (Tanaka et al., [2016](#bib.bib194);
    Andersen et al., [2021](#bib.bib10); Londt et al., [2021](#bib.bib120), [2020](#bib.bib121)),
    and keyword spotting (Mazzawi et al., [2019](#bib.bib134)). Satapathy et al. (Song,
    [2021](#bib.bib185)) introduced evolutionary multi-objective (i.e., inference
    time and accuracy) optimization in an English translation system. Sikdar et al.
    (Sikdar et al., [2012](#bib.bib181)) employed DE in feature selection for named
    entity recognition (NER).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）由计算机科学和计算语言学推动，旨在理解、分析和提取文本和语音识别的知识（Young et al., [2018](#bib.bib235)）。许多有效的NLP模型（例如，GPT-2（Radford
    et al., [2018](#bib.bib158)）和BERT（Devlin et al., [2018](#bib.bib45)））利用复杂机制缩短了人类交流与计算机理解之间的鸿沟。最近，提出了受进化计算启发的NLP模型，如语言模型（Murray
    and Chiang, [2015](#bib.bib142)）、实体识别（Sikdar et al., [2012](#bib.bib181)）、文本分类（Tanaka
    et al., [2016](#bib.bib194); Andersen et al., [2021](#bib.bib10); Londt et al.,
    [2021](#bib.bib120), [2020](#bib.bib121)）和关键词识别（Mazzawi et al., [2019](#bib.bib134)）。Satapathy
    et al.（Song, [2021](#bib.bib185)）在英语翻译系统中引入了进化多目标（即推理时间和准确性）优化。Sikdar et al.（Sikdar
    et al., [2012](#bib.bib181)）在命名实体识别（NER）的特征选择中应用了差分进化（DE）。
- en: 6.1.3\. Other Applications
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 其他应用
- en: In addition to CV and NLP, EDL also shows strong ability on handling other practical
    applications, such as medical analysis (Liu et al., [2018a](#bib.bib116); Zhu
    et al., [2007](#bib.bib264)), financial prediction (Tang et al., [2019](#bib.bib195)),
    signal processing (Erguzel et al., [2014](#bib.bib51); Huang et al., [2012](#bib.bib81)),
    and industrial prediction (Mei et al., [2017](#bib.bib135); Ma et al., [2021b](#bib.bib128)).
    In particular, Zhu et al. (Zhu et al., [2007](#bib.bib264)) presented a Markov
    blanket-embedded genetic algorithm for feature selection to improve gene selection.
    In (Tang et al., [2019](#bib.bib195)), financial bankruptcy analysis was handled
    by an evolutionary pruning neural network. The work in (Erguzel et al., [2014](#bib.bib51))
    designed a feature selection method based on ACO to classify electromyography
    signals. For remote sensing imagery, a suitable model was found by multi-objective
    neural evolution architecture search (Ma et al., [2021b](#bib.bib128)), where
    architecture complexity and performance error of searched network were two conflicting
    objectives.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 CV 和 NLP，EDL 在处理其他实际应用方面也展现了强大的能力，例如医学分析（Liu 等，[2018a](#bib.bib116)；Zhu 等，[2007](#bib.bib264)）、金融预测（Tang
    等，[2019](#bib.bib195)）、信号处理（Erguzel 等，[2014](#bib.bib51)；Huang 等，[2012](#bib.bib81)）和工业预测（Mei
    等，[2017](#bib.bib135)；Ma 等，[2021b](#bib.bib128)）。特别是，Zhu 等（Zhu 等，[2007](#bib.bib264)）提出了一种嵌入马尔可夫毯的遗传算法，用于特征选择以改善基因选择。在（Tang
    等，[2019](#bib.bib195)）中，金融破产分析通过进化修剪神经网络来处理。工作（Erguzel 等，[2014](#bib.bib51)）设计了一种基于
    ACO 的特征选择方法，用于分类肌电图信号。对于遥感图像，通过多目标神经进化架构搜索（Ma 等，[2021b](#bib.bib128)）找到了适合的模型，其中架构复杂性和搜索网络的性能误差是两个相互冲突的目标。
- en: 6.2\. Open Issues
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 未解决的问题
- en: EDL is a hot research topic in both fields of machine learning and evolutionary
    computation. There are a large number of publications on various top conferences
    and journals, such as ICCV, CVPR, GECCO, TPAMI, TEVC, TCYB, and TNNLS (see the
    reference list). Yet some challenges remain to be resolved.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: EDL 是机器学习和进化计算领域的一个热门研究话题。关于这一主题，有大量的论文发表在各大顶级会议和期刊上，如 ICCV、CVPR、GECCO、TPAMI、TEVC、TCYB
    和 TNNLS（请参见参考文献列表）。然而，仍然存在一些挑战需要解决。
- en: 'Acceleration strategies: Many EDL approaches suffer from low efficiency due
    to the expensive evaluations. So various acceleration strategies, such as surrogate
    model (Sun et al., [2019a](#bib.bib188)), supernet (Guo et al., [2020](#bib.bib67)),
    and early stop (Sun et al., [2019d](#bib.bib191)) have been designed. However,
    the improvements of the accuracy are at the expense of sacrifice a bit of model
    accuracy. Taking the supernet-based inheritance as an example (Xie et al., [2022a](#bib.bib221)),
    we cannot guarantee that every subnet receives a reliable evaluation due to the
    catastrophic forgetting (Zhang et al., [2020a](#bib.bib249)) and weight coupling
    (Hu et al., [2021a](#bib.bib80)). Therefore, how to balance the efficiency and
    accuracy needs further investigation.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 加速策略：许多 EDL 方法由于评估成本高而效率低下。因此，设计了各种加速策略，如代理模型（Sun 等，[2019a](#bib.bib188)）、超网络（Guo
    等，[2020](#bib.bib67)）和早停（Sun 等，[2019d](#bib.bib191)）。然而，这些策略在提高准确性的同时，也牺牲了部分模型精度。以基于超网络的继承为例（Xie
    等，[2022a](#bib.bib221)），由于灾难性遗忘（Zhang 等，[2020a](#bib.bib249)）和权重耦合（Hu 等，[2021a](#bib.bib80)），我们无法保证每个子网络都能得到可靠的评估。因此，如何平衡效率和准确性需要进一步研究。
- en: 'Effectiveness: There is a debate on whether EDL has many advantages over other
    search paradigms (e.g., random search and RL). Some studies argue that many popular
    search paradigms (e.g., EC-based methods and RL-based methods) have no big difference
    from the random search methods in their performance, and some random search methods
    even outperform EC-based methods in some scenarios (Sciuto et al., [2020](#bib.bib175)).
    On the contrary, EC-based approaches have also been proved to be more effective
    than random search methods in many studies (Real et al., [2019](#bib.bib162);
    Guo et al., [2020](#bib.bib67); Jones et al., [2019](#bib.bib90)). Thus, a unified
    platform is essential to measure the effectiveness of different search models,
    under the consistent search space and hyperparameters configuration (Xie et al.,
    [2022b](#bib.bib223)). In addition, elaborate experiments are required to justify
    the effects of different genetic operators (e.g., crossover operator) to the evolutionary
    process of EDL.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 效果性：关于EDL是否相较于其他搜索范式（如随机搜索和RL）具有更多优势存在争论。一些研究认为，许多流行的搜索范式（如基于EC的方法和基于RL的方法）在性能上与随机搜索方法没有太大区别，甚至在某些场景下，一些随机搜索方法的表现优于基于EC的方法（Sciuto
    et al., [2020](#bib.bib175)）。相反，基于EC的方法也在许多研究中被证明比随机搜索方法更有效（Real et al., [2019](#bib.bib162)；Guo
    et al., [2020](#bib.bib67)；Jones et al., [2019](#bib.bib90)）。因此，在一致的搜索空间和超参数配置下，建立一个统一的平台来衡量不同搜索模型的效果是至关重要的（Xie
    et al., [2022b](#bib.bib223)）。此外，需要详细的实验来证明不同遗传算子（如交叉算子）对EDL进化过程的影响。
- en: 'Large-scale datasets: There is an issue for the studies of EDL on large-scale
    datasets. It is noted that many studies of EDL are tested on small- and medium-scale
    datasets such as CIFAR-10 and CIFAR-100 (including 60000 32$\times$32 images),
    and especially the accuracy on CIFAR-10 reaches up to 98% (Lu et al., [2021](#bib.bib125)).
    Although large-scale datasets are ubiquitous and essential in various domains
    like gene analysis (Yu et al., [2009](#bib.bib236)) and ImageNet (Deng et al.,
    [2009](#bib.bib44)), computational costs are unaffordable for many researchers
    as pointed in some statistical reports (He et al., [2021](#bib.bib72); Liu et al.,
    [2021b](#bib.bib117)). Therefore, the sensitivity of the EDL methods to different
    scales of datasets is necessary (Zhou et al., [2021a](#bib.bib258)) and how to
    economically and efficiently verify EDL methods on large-scale datasets also deserves
    much investigations.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据集：在大规模数据集上研究EDL存在问题。注意到许多EDL的研究是在小型和中型数据集上测试的，如CIFAR-10和CIFAR-100（包括60000张32$\times$32的图像），尤其是CIFAR-10上的准确率达到98%（Lu
    et al., [2021](#bib.bib125)）。尽管大规模数据集在基因分析（Yu et al., [2009](#bib.bib236)）和ImageNet（Deng
    et al., [2009](#bib.bib44)）等各种领域中随处可见且至关重要，但如一些统计报告所指出，计算成本对许多研究人员来说是难以承受的（He
    et al., [2021](#bib.bib72)；Liu et al., [2021b](#bib.bib117)）。因此，EDL方法对不同规模数据集的敏感性是必要的（Zhou
    et al., [2021a](#bib.bib258)），如何经济高效地在大规模数据集上验证EDL方法也值得深入研究。
- en: 'End-to-end EDL: Originally, AutoML aims to simultaneously optimize feature
    engineering, model generation and model deployment as a whole. However, there
    is a strong correlation between them where the performance of next phase heavily
    relies on the results of the previous phase (Liu and Guo, [2021](#bib.bib115)).
    As a result, most studies only focus on parts of the EDL pipeline (Fig. [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Survey on Evolutionary Deep Learning: Principles,
    Algorithms, Applications and Open Issues")). For instance, TPOT (Olson and Moore,
    [2016](#bib.bib149)) is designed on top of Pytorch for building classification
    tasks, which however only supports a multi-layer perception machine (i.e., model
    generation). There are many partially accomplished end-to-end for EDL, such as
    ModelArts (model generation), Google’s Cloud (NAS), and Feature Labs (feature
    engineering) (Yao et al., [2018](#bib.bib231)), to name but a few. The main reason
    is that the optimization of the whole EDL pipeline may need huge computational
    cost not only on the exploration of the large-scale search space but also on handling
    highly-coupled relation between different parts of EDL. Consequently, finding
    an optimal solution of the complete EDL pipeline is essential but challenging.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的EDL：最初，AutoML旨在整体上同时优化特征工程、模型生成和模型部署。然而，它们之间有很强的关联，下一阶段的表现严重依赖于前一阶段的结果（Liu
    和 Guo, [2021](#bib.bib115)）。因此，大多数研究仅关注EDL管道的部分内容（图 [1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣
    进化深度学习：原理、算法、应用及开放问题的调查")）。例如，TPOT（Olson 和 Moore, [2016](#bib.bib149)）是基于Pytorch构建分类任务的，但只支持多层感知机（即，模型生成）。目前有许多部分完成的端到端EDL，例如ModelArts（模型生成）、Google的Cloud（NAS）和Feature
    Labs（特征工程）（Yao 等, [2018](#bib.bib231)），仅举几例。主要原因是优化整个EDL管道可能需要巨大的计算成本，不仅在探索大规模搜索空间方面，还在处理EDL不同部分之间的高度耦合关系方面。因此，找到完整EDL管道的最优解决方案至关重要但具有挑战性。
- en: 6.3\. Challenges and Future Trends
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 挑战与未来趋势
- en: Although remarkable progress has been made in EDL, there are still many promising
    lines of research.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管EDL取得了显著进展，但仍有许多有前景的研究方向。
- en: 'Fair comparisons: Unfair comparisons of different EDL methods are easily encountered
    with the following reasons. Firstly, uniform benchmarks are essential. In feature
    engineering, no uniform benchmark is for the fair comparison of different algorithms
    due to different downstream prediction models and feature sets. Secondly, there
    is no uniform criterion for different methods in handling NAS and model compression
    by using different tricks (e.g., cutout (DeVries and Taylor, [2017](#bib.bib46))
    and ScheduledDropPath (Zoph et al., [2018](#bib.bib265))), which may influence
    the performance of the final architecture. Thirdly, a fair platform for EDL is
    essential. There are some fair benchmarks but only for specific tasks, such as
    BenchENAS (Xie et al., [2022b](#bib.bib223)), NAS-Bench-101 (Ying et al., [2019](#bib.bib234)),
    NAS-Bench-201 (Dong and Yang, [2020](#bib.bib47)), NAS-Bench-301 (Siems et al.,
    [2020](#bib.bib180)), and HW-NAS-Bench (Li et al., [2021](#bib.bib108)).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 公平比较：不同EDL方法的非公平比较常因以下原因出现。首先，统一基准测试是必需的。在特征工程中，由于不同的下游预测模型和特征集，没有统一的基准测试来公平比较不同算法。其次，处理NAS和模型压缩的不同方法没有统一标准（例如，cutout（DeVries
    和 Taylor, [2017](#bib.bib46)）和ScheduledDropPath（Zoph 等, [2018](#bib.bib265)）），这可能影响最终架构的性能。第三，公平的EDL平台是必需的。有一些公平的基准测试，但仅限于特定任务，例如BenchENAS（Xie
    等, [2022b](#bib.bib223)）、NAS-Bench-101（Ying 等, [2019](#bib.bib234)）、NAS-Bench-201（Dong
    和 Yang, [2020](#bib.bib47)）、NAS-Bench-301（Siems 等, [2020](#bib.bib180)）和HW-NAS-Bench（Li
    等, [2021](#bib.bib108)）。
- en: 'Interpretability: EDL is known as a black-box optimization, and there is a
    lack of theoretical analysis to explain its superiority (Wang et al., [2021b](#bib.bib206)).
    For example, it is difficult to explain why EC-based method tends to select features
    contribute to the performance of the classification model in feature engineering.
    As a result, the development of EDL in some sensitive domains such as financial
    and medical fields is slow. To overcome this issue, Evans et al. (Evans et al.,
    [2018b](#bib.bib54)) used visualization to expound how the evolved convolution
    filter served and indirectly explained the search process of the model. Nevertheless,
    some studies argue that the explanation for these occurrences is usually post-hoc
    and lacks trustworthy mathematical deduction (Liu et al., [2021b](#bib.bib117);
    Al-Sahaf et al., [2019](#bib.bib6)). Thus, the interpretability of EDL is an interesting
    and promising research direction.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性：EDL被称为黑箱优化，目前缺乏理论分析来解释其优越性（Wang et al., [2021b](#bib.bib206)）。例如，很难解释为什么基于EC的方法倾向于选择有助于分类模型性能的特征。在特征工程中。这导致EDL在一些敏感领域，如金融和医疗领域的发展缓慢。为了解决这个问题，Evans
    et al. (Evans et al., [2018b](#bib.bib54)) 使用可视化来阐明进化卷积滤波器的作用，并间接解释模型的搜索过程。然而，一些研究认为，这些现象的解释通常是事后解释的，缺乏可靠的数学推导（Liu
    et al., [2021b](#bib.bib117)；Al-Sahaf et al., [2019](#bib.bib6)）。因此，EDL的可解释性是一个有趣且充满前景的研究方向。
- en: 'Exploring more scenarios: There is still plenty of room for the improvement
    of the performance of EDL on both benchmarks and real-world applications. Although
    EDL methods outperform manually designed models on various image benchmarks (CIFAR-10
    and ImageNet), the state-of-the-art EDL methods (Wang et al., [2020a](#bib.bib210))
    lost their advantages on NLP in comparison with human-designed models (e.g., GPT-2
    (Radford et al., [2018](#bib.bib158)), Transformer-XL (Dai et al., [2019](#bib.bib39))).
    In comparison with the benchmarks, it is more difficult to handle real-world tasks,
    which inevitably contain noise (e.g., mislabeling and inadequate or imbalance
    data) or may have small-scale datasets (leading to overfitting). Hence, some techniques
    such as unsupervised and self-supervised learning may be incorporated into EDL
    to mitigate these types of issues.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 探索更多场景：在基准测试和实际应用中，EDL的性能仍有很大的提升空间。虽然EDL方法在各种图像基准测试（如CIFAR-10和ImageNet）中优于手动设计的模型，但最先进的EDL方法（Wang
    et al., [2020a](#bib.bib210)）在NLP领域相比于人类设计的模型（如GPT-2 (Radford et al., [2018](#bib.bib158))，Transformer-XL
    (Dai et al., [2019](#bib.bib39))）失去了其优势。与基准测试相比，处理实际任务更为困难，因为实际任务不可避免地包含噪声（如错误标记和数据不足或不平衡）或可能具有小规模数据集（导致过拟合）。因此，可以将一些技术，如无监督和自监督学习，纳入EDL中，以缓解这些问题。
- en: 7\. Conclusions
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: With the development of machine leaning and evolutionary computation, many EDL
    approaches have been proposed to automatically optimize the parameters or architectures
    of deep models following the EC optimization framework. EDL approaches show competitive
    performance in robust and search capability, in comparison with the manually designed
    approaches. Therefore, EDL has become a hot research topic.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习和进化计算的发展，许多EDL方法被提出，用于自动优化深度模型的参数或架构，遵循EC优化框架。与手动设计的方法相比，EDL方法在鲁棒性和搜索能力方面表现出竞争力。因此，EDL已成为一个热门研究课题。
- en: In this survey, we first introduced EDL from the perspective of DL and EC to
    facilitate the understanding of readers from the communities of ML and EC. Then
    we formulated EDL as a complex optimization problem, and provided a comprehensive
    survey of EC techniques in solving EDL optimization problems in terms of feature
    engineering, model generation to model deployment to form a new taxonomy (i.e.,
    what, where and how to evolve/optimize in EDL). Specifiically, we discussed the
    solution representations and search paradigms of EDL at different stages of its
    pipeline in detail. Then the pros and cons of EC-based approaches in comparison
    to non-EC based ones are discussed. Subsequently, various applications are summarized
    to show the potential ability of EDL in handling real-world problems.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们首先从DL和EC的角度介绍了EDL，以便ML和EC领域的读者能够更好地理解。接着，我们将EDL定义为一个复杂的优化问题，并提供了关于EC技术在解决EDL优化问题中的全面调查，包括特征工程、模型生成到模型部署，从而形成了一个新的分类法（即，在EDL中什么、在哪里以及如何进化/优化）。具体而言，我们详细讨论了EDL在其管道不同阶段的解决方案表示和搜索范式。然后，讨论了基于EC的方法与非EC方法的优缺点。随后，总结了各种应用，以展示EDL在处理现实世界问题中的潜在能力。
- en: Although EDL approaches have achieved great progress in AutoML, there are still
    a number of challenging issues to be resolved. For example, effective acceleration
    strategies are essential to reduce the expensive optimization process. Another
    issue is to handle large-scale datasets and how to perform fair comparisons between
    different EDL approaches or non-EC based methods. More investigations are required
    to theoretically analyse or interpret the search ability of EDL. In addition,
    a lot of efforts are required on the improving the performance of EDL on both
    benchmarks (e.g., large-scale and small-scale data) and real-world applications.
    Lastly, the development of end-to-end EDL is challenging but deserves much efforts.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管EDL方法在AutoML中取得了巨大进展，但仍然存在许多待解决的挑战性问题。例如，有效的加速策略对于减少昂贵的优化过程至关重要。另一个问题是处理大规模数据集以及如何在不同的EDL方法或非EC方法之间进行公平比较。需要更多研究来理论分析或解释EDL的搜索能力。此外，还需要大量努力来提高EDL在基准（例如，大规模和小规模数据）和现实应用中的表现。最后，端到端的EDL开发具有挑战性，但值得付出大量努力。
- en: References
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abdi and Williams (2010) Hervé Abdi and Lynne J Williams. 2010. Principal Component
    Analysis. *Comput. Stat.* 2, 4 (2010), 433–459.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdi和Williams（2010）Hervé Abdi和Lynne J Williams。2010年。主成分分析。*Comput. Stat.* 2,
    4 (2010), 433–459。
- en: Ahmed et al. (2019) Amr Ahmed, Saad Mohamed Darwish, and Mohamed M. El-Sherbiny.
    2019. A Novel Automatic CNN Architecture Design Approach Based on Genetic Algorithm.
    In *Int. Conf. Adv. Intell. Syst. Inform.* 473–482.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed等（2019）Amr Ahmed、Saad Mohamed Darwish和Mohamed M. El-Sherbiny。2019年。基于遗传算法的自动CNN架构设计新方法。在*Int.
    Conf. Adv. Intell. Syst. Inform.* 473–482。
- en: Ahmed et al. (2014) Soha Ahmed, Mengjie Zhang, Lifeng Peng, and Bing Xue. 2014.
    Multiple Feature Construction for Effective Biomarker Identification and Classification
    Using Genetic Programming. In *Proc. Genetic Evol. Comput. Conf.* 249–256.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed等（2014）Soha Ahmed、Mengjie Zhang、Lifeng Peng和Bing Xue。2014年。使用遗传编程进行有效生物标志物识别和分类的多特征构造。在*Proc.
    Genetic Evol. Comput. Conf.* 249–256。
- en: Al-kazemi and Mohan (2002) Buthainah Al-kazemi and Chilukuri Krishna Mohan.
    2002. Training Feedforward Neural Networks using Nulti-phase Particle Swarm Optimization.
    In *Proc. Int. Conf. Neural Inf. Process.*, Vol. 5. 2615–2619.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-kazemi和Mohan（2002）Buthainah Al-kazemi和Chilukuri Krishna Mohan。2002年。使用多相粒子群优化训练前馈神经网络。在*Proc.
    Int. Conf. Neural Inf. Process.*, Vol. 5. 2615–2619。
- en: Al-Sahaf et al. (2019) Harith Al-Sahaf, Ying Bi, Qi Chen, Andrew Lensen, Yi
    Mei, Yanan Sun, Binh Tran, Bing Xue, and Mengjie Zhang. 2019. A Survey on Evolutionary
    Machine Learning. *J. R. Soc. N. Z.* 49, 2 (2019), 205–228.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Sahaf等（2019）Harith Al-Sahaf、Ying Bi、Qi Chen、Andrew Lensen、Yi Mei、Yanan Sun、Binh
    Tran、Bing Xue和Mengjie Zhang。2019年。关于进化机器学习的调查。*J. R. Soc. N. Z.* 49, 2 (2019),
    205–228。
- en: Albukhanajer et al. (2015) Wissam A. Albukhanajer, Johann A. Briffa, and Yaochu
    Jin. 2015. Evolutionary Multiobjective Image Feature Extraction in the Presence
    of Noise. *IEEE Trans. Cybern.* 45, 9 (2015), 1757–1768.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albukhanajer等（2015）Wissam A. Albukhanajer、Johann A. Briffa和Yaochu Jin。2015年。在噪声存在下的进化多目标图像特征提取。*IEEE
    Trans. Cybern.* 45, 9 (2015), 1757–1768。
- en: 'Alexandropoulos and Aridas (2019) Stamatios-Aggelos N Alexandropoulos and Christos K
    Aridas. 2019. Multi-objective Evolutionary Optimization Algorithms for Machine
    Learning: A Recent Survey. *Approximation and Optimization* (2019), 35–55.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandropoulos and Aridas (2019) Stamatios-Aggelos N Alexandropoulos 和 Christos
    K Aridas. 2019. 机器学习的多目标进化优化算法：最新综述。*Approximation and Optimization* (2019), 35–55.
- en: Aljarah et al. (2018) Ibrahim Aljarah, Hossam Faris, and Seyed Mohammad Mirjalili.
    2018. Optimizing Connection Weights in Neural Networks Using the Whale Optimization
    Algorithm. *Soft Comput.* 22, 1 (2018), 1–15.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aljarah et al. (2018) Ibrahim Aljarah, Hossam Faris, 和 Seyed Mohammad Mirjalili.
    2018. 使用鲸鱼优化算法优化神经网络中的连接权重。*Soft Comput.* 22, 1 (2018), 1–15.
- en: Andersen et al. (2021) Hayden Andersen, Sean Stevenson, Tuan Ha, Xiaoying Gao,
    and Bing Xue. 2021. Evolving Neural Networks for Text Classification Using Genetic
    Algorithm-based Approaches. In *Proc. IEEE Congr. Evol. Comput.* 1241–1248.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andersen et al. (2021) Hayden Andersen, Sean Stevenson, Tuan Ha, Xiaoying Gao,
    和 Bing Xue. 2021. 利用基于遗传算法的方法进化神经网络进行文本分类。见于 *Proc. IEEE Congr. Evol. Comput.*
    1241–1248.
- en: Assunção et al. (2019a) Filipe Assunção, Joao Correia, and Rúben Conceição.
    2019a. Automatic Design of Artificial Neural Networks for Gamma-Ray Detection.
    *IEEE Access* 7 (2019), 110531–110540.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Assunção et al. (2019a) Filipe Assunção, Joao Correia, 和 Rúben Conceição. 2019a.
    用于伽马射线探测的人工神经网络自动设计。*IEEE Access* 7 (2019), 110531–110540.
- en: Assunção et al. (2018) Filipe Assunção, Nuno Lourenço, P. Machado, and Bernardete
    Ribeiro. 2018. Evolving the Topology of Large Scale Deep Neural Networks. In *Proc.
    Eur. Conf. Genetic Program*. 19–34.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Assunção et al. (2018) Filipe Assunção, Nuno Lourenço, P. Machado, 和 Bernardete
    Ribeiro. 2018. 大规模深度神经网络的拓扑进化。见于 *Proc. Eur. Conf. Genetic Program*. 19–34.
- en: 'Assunção et al. (2019b) Filipe Assunção, Nuno Lourenço, Penousal Machado, and
    Bernardete Ribeiro. 2019b. Fast denser: Efficient Deep Neuroevolution. In *Proc.
    Eur. Conf. Genetic Program*. 197–212.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Assunção et al. (2019b) Filipe Assunção, Nuno Lourenço, Penousal Machado, 和
    Bernardete Ribeiro. 2019b. Fast denser: Efficient Deep Neuroevolution. 见于 *Proc.
    Eur. Conf. Genetic Program*. 197–212.'
- en: Atre et al. (2021) Medha Atre, Birendra Jha, and Ashwini Rao. 2021. Distributed
    Deep Learning Using Volunteer Computing-Like Paradigm. In *Proc. Int. Parallel
    and Distrib. Process. Symp.* 933–942.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atre et al. (2021) Medha Atre, Birendra Jha, 和 Ashwini Rao. 2021. 使用志愿计算类似范式的分布式深度学习。见于
    *Proc. Int. Parallel and Distrib. Process. Symp.* 933–942.
- en: Barman and Kwon (2020) Shohag Barman and Yung-Keun Kwon. 2020. A Neuro-Evolution
    Approach to Infer A Boolean Network From Time-Series Gene Expressions. *Bioinformatics*
    36, 2 (2020), i762–i769.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barman and Kwon (2020) Shohag Barman 和 Yung-Keun Kwon. 2020. 一种神经进化方法来推断时间序列基因表达的布尔网络。*Bioinformatics*
    36, 2 (2020), i762–i769.
- en: Behjat and Chidambaran (2019) Amir Behjat and Sharat Chidambaran. 2019. Adaptive
    Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping
    in Autonomous Agents. In *Proc. Int. Conf. Robot. Autom.* 9638–9644.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Behjat and Chidambaran (2019) Amir Behjat 和 Sharat Chidambaran. 2019. 自适应基因组进化神经网络拓扑（AGENT）用于自主体的状态到动作映射。见于
    *Proc. Int. Conf. Robot. Autom.* 9638–9644.
- en: Bhanu and Krawiec (2002) Bir Bhanu and Krzysztof Krawiec. 2002. Coevolutionary
    Construction of Features for Transformation of Representation in Machine Learning.
    In *Proc. Genetic Evol. Comput. Conf.* 249–254.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhanu and Krawiec (2002) Bir Bhanu 和 Krzysztof Krawiec. 2002. 用于机器学习中表示转化的特征的共进化构造。见于
    *Proc. Genetic Evol. Comput. Conf.* 249–254.
- en: Bi et al. (2018) Ying Bi, Bing Xue, and Mengjie Zhang. 2018. An Automatic Feature
    Extraction Approach to Image Classification Using Genetic Programming. In *Proc.
    Int. Conf. Appl. Evol. Comput.* 421–438.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi et al. (2018) Ying Bi, Bing Xue, 和 Mengjie Zhang. 2018. 一种基于遗传编程的自动特征提取方法用于图像分类。见于
    *Proc. Int. Conf. Appl. Evol. Comput.* 421–438.
- en: Cano et al. (2017) Alberto Cano, Sebastián Ventura, and Krzysztof J. Cios. 2017.
    Multi-Objective Genetic Programming for Feature Extraction and Data Visualization.
    *Soft Comput.* 21, 8 (2017), 2069–2089.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cano et al. (2017) Alberto Cano, Sebastián Ventura, 和 Krzysztof J. Cios. 2017.
    多目标遗传编程用于特征提取和数据可视化。*Soft Comput.* 21, 8 (2017), 2069–2089.
- en: Castelli et al. (2011) Mauro Castelli, Luca Manzoni, and Leonardo Vanneschi.
    2011. Multi Objective Genetic Programming for Feature Construction in Classification
    Problems. In *Proc. Int. Conf. Learn. Intell. Optim.* 503–506.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castelli et al. (2011) Mauro Castelli, Luca Manzoni, 和 Leonardo Vanneschi. 2011.
    多目标遗传编程用于分类问题中的特征构造。见于 *Proc. Int. Conf. Learn. Intell. Optim.* 503–506.
- en: Chai et al. (2022) Zheng-Yi Chai, ChuanHua Yang, and Ya-Lun Li. 2022. Communication
    Efficiency Optimization in Federated Learning Based on Multi-Objective Evolutionary
    Algorithm. *Evol. Intell.* (2022), 1–12.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai et al. (2022) Zheng-Yi Chai, ChuanHua Yang, 和 Ya-Lun Li. 2022. 基于多目标进化算法的联邦学习通信效率优化。*Evol.
    Intell.* (2022), 1–12.
- en: Chandra (2015) Rohitash Chandra. 2015. Competition and Collaboration in Cooperative
    Coevolution of Elman Recurrent Neural Networks for Time-Series Prediction. *IEEE
    Trans. Neural Netw. Learn. Syst.* 26, 12 (2015), 3123–3136.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandra (2015) Rohitash Chandra. 2015. 合作进化中的竞争与合作：用于时间序列预测的 Elman 递归神经网络。*IEEE
    Trans. Neural Netw. Learn. Syst.* 26, 12 (2015), 3123–3136。
- en: Chandra and Zhang (2012) Rohitash Chandra and Mengjie Zhang. 2012. Cooperative
    Coevolution of Elman Recurrent Neural Networks for Chaotic Time Series Prediction.
    *Neurocomputing* 86 (2012), 116–123.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandra and Zhang (2012) Rohitash Chandra 和 Mengjie Zhang. 2012. 用于混沌时间序列预测的
    Elman 递归神经网络的合作进化。*Neurocomputing* 86 (2012), 116–123。
- en: Chen et al. (2022) Ke Chen, Bing Xue, Mengjie Zhang, and Fengyu Zhou. 2022.
    Evolutionary Multitasking for Feature Selection in High-Dimensional Classification
    via Particle Swarm Optimization. *IEEE Trans. Evol. Comput.* 26, 3 (2022), 446–460.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022) Ke Chen, Bing Xue, Mengjie Zhang, 和 Fengyu Zhou. 2022. 基于粒子群优化的高维分类特征选择的进化多任务处理。*IEEE
    Trans. Evol. Comput.* 26, 3 (2022), 446–460。
- en: Chen et al. (2015) Qi Chen, Bing Xue, and Mengjie Zhang. 2015. Generalisation
    and Domain Adaptation in GP with Gradient Descent for Symbolic Regression. In
    *Proc. IEEE Congr. Evol. Comput.* 1137–1144.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2015) Qi Chen, Bing Xue, 和 Mengjie Zhang. 2015. 使用梯度下降的 GP 中的泛化和领域适应。发表于
    *Proc. IEEE Congr. Evol. Comput.* 1137–1144。
- en: Chen et al. (2019a) Shuxin Chen, Lin Lin, Zixun Zhang, and Mitsuo Gen. 2019a.
    Evolutionary NetArchitecture Search for Deep Neural Networks Pruning. In *Proc.
    Aust. Conf. Artif. Intell.* 189–196.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019a) Shuxin Chen, Lin Lin, Zixun Zhang, 和 Mitsuo Gen. 2019a.
    用于深度神经网络剪枝的进化网络架构搜索。发表于 *Proc. Aust. Conf. Artif. Intell.* 189–196。
- en: Chen et al. (2020) Xiangru Chen, Yanan Sun, Mengjie Zhang, and Dezhong Peng.
    2020. Evolving Deep Convolutional Variational Autoencoders for Image Classification.
    *IEEE Trans. Evol. Comput.* 25, 5 (2020), 815–829.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Xiangru Chen, Yanan Sun, Mengjie Zhang, 和 Dezhong Peng. 2020.
    为图像分类进化深度卷积变分自编码器。*IEEE Trans. Evol. Comput.* 25, 5 (2020), 815–829。
- en: 'Chen et al. (2019b) Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, and
    Chang Huang. 2019b. RENAS: Reinforced Evolutionary Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 4787–4796.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019b) Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, 和
    Chang Huang. 2019b. RENAS：强化进化神经架构搜索。发表于 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.* 4787–4796。
- en: Cheng et al. (2021) Fan Cheng, Feixiang Chu, Yi Xu, and Lei Zhang. 2021. A Steering-Matrix-Based
    Multiobjective Evolutionary Algorithm for High-Dimensional Feature Selection.
    *IEEE Trans. Cybern.* 52, 9 (2021), 9695–9708.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2021) Fan Cheng, Feixiang Chu, Yi Xu, 和 Lei Zhang. 2021. 基于引导矩阵的多目标进化算法用于高维特征选择。*IEEE
    Trans. Cybern.* 52, 9 (2021), 9695–9708。
- en: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A Survey
    of Model Compression and Acceleration for Deep Neural networks. *arXiv preprint
    arXiv:1710.09282* (2017).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, 和 Tao Zhang. 2017. 深度神经网络的模型压缩和加速调查。*arXiv
    preprint arXiv:1710.09282* (2017)。
- en: Chiba et al. (2019) Zouhair Chiba, Noreddine Abghour, Khalid Moussaid, Amina El
    Omri, and Mohamed Rida. 2019. Intelligent Approach to Build a Deep Neural Network
    Based IDS for Cloud Environment Using Combination of Machine Learning Algorithms.
    *Comput. & Sec.* 86 (2019), 291–317.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiba et al. (2019) Zouhair Chiba, Noreddine Abghour, Khalid Moussaid, Amina
    El Omri, 和 Mohamed Rida. 2019. 基于深度神经网络的云环境入侵检测系统的智能构建方法，结合了机器学习算法。*Comput. &
    Sec.* 86 (2019), 291–317。
- en: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and
    Jagannathan Sarangapani. 2020. A Comprehensive Survey on Model Compression and
    Acceleration. *Artif. Intell. Rev.* 53, 7 (2020), 5113–5155.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, 和 Jagannathan
    Sarangapani. 2020. 关于模型压缩和加速的全面调查。*Artif. Intell. Rev.* 53, 7 (2020), 5113–5155。
- en: Chrabaszcz et al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
    2017. A Downsampled variant of imageNet as an alternative to the Cifar datasets.
    *arXiv preprint arXiv:1707.08819* (2017).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chrabaszcz et al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, 和 Frank Hutter.
    2017. 作为 Cifar 数据集替代方案的下采样版 imageNet。*arXiv preprint arXiv:1707.08819* (2017)。
- en: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Hailong Ma. 2020.
    Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search. In
    *Proc. Eur. Conf. Comput. Vis.* 99–113.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, Ruijun Xu, 和 Hailong Ma. 2020. 移动神经架构搜索中的多目标强化进化。发表于
    *Proc. Eur. Conf. Comput. Vis.* 99–113。
- en: Conti et al. (2018) Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such,
    Joel Lehman, Kenneth O. Stanley, and Jeff Clune. 2018. Improving Exploration in
    Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking
    Agents. In *Proc. Adv. Neural Inf. Process. Syst.*, Vol. 31\. 5032–5043.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conti 等（2018）Edoardo Conti、Vashisht Madhavan、Felipe Petroski Such、Joel Lehman、Kenneth
    O. Stanley 和 Jeff Clune。2018。通过一群寻求新奇的代理改进深度强化学习的进化策略探索。在 *Proc. Adv. Neural Inf.
    Process. Syst.*，第 31 卷，5032–5043。
- en: Cui et al. (2018) Xiaodong Cui, Wei Zhang, Zoltán Tüske, and Michael Picheny.
    2018. Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural
    Networks. *Proc. Adv. Neural Inf. Process. Syst.* 31 (2018), 6051–6061.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等（2018）Xiaodong Cui、Wei Zhang、Zoltán Tüske 和 Michael Picheny。2018。用于优化深度神经网络的进化随机梯度下降。*Proc.
    Adv. Neural Inf. Process. Syst.* 31（2018），6051–6061。
- en: Da Silva and Neto (2011) Sérgio Francisco Da Silva and João do ES Batista Neto.
    2011. Improving The Ranking Quality of Medical Image Retrieval Using A Genetic
    Feature Selection Method. *Decis. Support. Syst.* 51, 4 (2011), 810–820.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Da Silva 和 Neto（2011）Sérgio Francisco Da Silva 和 João do ES Batista Neto。2011。使用遗传特征选择方法提高医学图像检索的排名质量。*Decis.
    Support. Syst.* 51, 4（2011），810–820。
- en: Dahal and Zhan (2020) Binay Dahal and Justin Zhijun Zhan. 2020. Effective Mutation
    and Recombination for Evolving Convolutional Networks. In *Proc. Adv. Neural Inf.
    Process. Syst.* 1–6.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahal 和 Zhan（2020）Binay Dahal 和 Justin Zhijun Zhan。2020。有效的突变和重组以演化卷积网络。在 *Proc.
    Adv. Neural Inf. Process. Syst.* 1–6。
- en: 'Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,
    Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models
    beyond a Fixed-Length Context. In *Proc. Assoc. Comput. Linguist.* 2978–2988.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2019）Zihang Dai、Zhilin Yang、Yiming Yang、Jaime G Carbonell、Quoc Le 和 Ruslan
    Salakhutdinov。2019。Transformer-XL：超越固定长度上下文的注意力语言模型。在 *Proc. Assoc. Comput. Linguist.*
    2978–2988。
- en: D’Ambrosio and Stanley (2007) David B. D’Ambrosio and Kenneth O. Stanley. 2007.
    A Novel Generative Encoding for Exploiting Neural Network Sensor and Output Geometry.
    In *Proc. Genetic Evol. Comput. Conf.* 974–981.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Ambrosio 和 Stanley（2007）David B. D’Ambrosio 和 Kenneth O. Stanley。2007。用于利用神经网络传感器和输出几何的新型生成编码。在
    *Proc. Genetic Evol. Comput. Conf.* 974–981。
- en: Darwish et al. (2020) Ashraf Darwish, Aboul Ella Hassanien, and Swagatam Das.
    2020. A Survey of Swarm And Evolutionary Computing Approaches for Deep Learning.
    *Artif. Intell. Rev.* 53, 3 (2020), 1767–1812.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darwish 等（2020）Ashraf Darwish、Aboul Ella Hassanien 和 Swagatam Das。2020。深度学习的群体和进化计算方法调查。*Artif.
    Intell. Rev.* 53, 3（2020），1767–1812。
- en: 'Deb et al. (2002) Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan.
    2002. A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II. *IEEE Trans.
    Evol. Comput.* 6, 2 (2002), 182–197.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deb 等（2002）Kalyanmoy Deb、Samir Agrawal、Amrit Pratap 和 T. Meyarivan。2002。一个快速且精英的多目标遗传算法：NSGA-II。*IEEE
    Trans. Evol. Comput.* 6, 2（2002），182–197。
- en: Demirkir and Sankur (2006) Cem Demirkir and Bülent Sankur. 2006. Object Detection
    Using Haar Feature Selection Optimization. In *Proc. IEEE Signal Process. Commun.
    Appl.* 1–4.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demirkir 和 Sankur（2006）Cem Demirkir 和 Bülent Sankur。2006。使用 Haar 特征选择优化进行目标检测。在
    *Proc. IEEE Signal Process. Commun. Appl.* 1–4。
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and
    Li Fei-Fei. 2009. ImageNet: A Large-scale Hierarchical Image Database. *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* (2009), 248–255.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2009）Jia Deng、Wei Dong、Richard Socher、Li-Jia Li、K. Li 和 Li Fei-Fei。2009。ImageNet：一个大规模层次化图像数据库。*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*（2009），248–255。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2018。Bert：用于语言理解的深度双向
    Transformer 预训练。*arXiv preprint arXiv:1810.04805*（2018）。
- en: DeVries and Taylor (2017) Terrance DeVries and Graham W Taylor. 2017. Improved
    Regularization of Convolutional Neural Networks with Cutout. *arXiv preprint arXiv:1708.04552*
    (2017).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries 和 Taylor（2017）Terrance DeVries 和 Graham W Taylor。2017。通过 Cutout 改进卷积神经网络的正则化。*arXiv
    preprint arXiv:1708.04552*（2017）。
- en: 'Dong and Yang (2020) Xuanyi Dong and Yi Yang. 2020. NAS-Bench-201: Extending
    the Scope of Reproducible Neural Architecture Search. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2001.00326.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 和 Yang（2020）Xuanyi Dong 和 Yi Yang。2020。NAS-Bench-201：扩展可重复神经架构搜索的范围。在 *Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/2001.00326。
- en: Dowdell and Zhang (2020) Thomas Dowdell and Hongyu Zhang. 2020. Language Modelling
    for Source Code with Transformer-XL. *arXiv preprint arXiv:2007.15813* (2020).
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dowdell 和 Zhang（2020）Thomas Dowdell 和 Hongyu Zhang。2020。使用 Transformer-XL 的源代码语言建模。*arXiv
    preprint arXiv:2007.15813*（2020）。
- en: Elsken et al. (2017) Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. 2017.
    Simple and Efficient Architecture Search for Convolutional Neural Networks. *arXiv
    preprint arXiv:1711.04528* (2017).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Elsken et al. (2017)](https://arxiv.org/abs/1711.04528) Thomas Elsken, Jan-Hendrik
    Metzen, and Frank Hutter. 2017. 卷积神经网络的简单高效架构搜索。*arXiv 预印本 arXiv:1711.04528* (2017)。'
- en: Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019.
    Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1804.09081.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Elsken et al. (2019)](https://arxiv.org/abs/1804.09081) Thomas Elsken, Jan Hendrik
    Metzen, 和Frank Hutter. 2019. 通过拉马克进化实现的高效多目标神经架构搜索。在*学习代表国际会议*。https://arxiv.org/abs/1804.09081。'
- en: Erguzel et al. (2014) Turker Tekin Erguzel, Serhat Ozekes, Selahattin Gultekin,
    and Nevzat Tarhan. 2014. Ant Colony optimization Based Feature Selection Method
    for QEEG data classification. *Psychiatry Investig.* 11, 3 (2014), 243.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Erguzel et al. (2014)](https://pubmed.ncbi.nlm.nih.gov/25071325/) Turker Tekin
    Erguzel, Serhat Ozekes, Selahattin Gultekin, 和Nevzat Tarhan. 2014. 用于 QEEG 数据分类的蚁群优化的特征选择方法。*Psychiatry
    Investig.* 11, 3 (2014), 243。'
- en: Estévez and Caballero (1998) Pablo A Estévez and Rodrigo E Caballero. 1998.
    A Niching Genetic Algorithm for Selecting Features for Neural Network Classifiers.
    In *Proc. Int. Conf. Artif. Neural Netw.* 311–316.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Estévez and Caballero (1998)](https://link.springer.com/chapter/10.1007/3-540-49435-4_19)
    Pablo A Estévez and Rodrigo E Caballero. 1998. 用于神经网络分类器特征选择的分级遗传算法。在*国际人工神经网络会议*。311–316。'
- en: 'Evans et al. (2018a) Benjamin Evans, Harith Al-Sahaf, Bing Xue, and Mengjie
    Zhang. 2018a. Evolutionary Deep Learning: A Genetic Programming Approach to Image
    Classification. In *Proc. IEEE Congr. Evol. Comput.* 1538–1545.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Evans et al. (2018a)](https://ieeexplore.ieee.org/document/8477923) Benjamin
    Evans, Harith Al-Sahaf, Bing Xue, 和Mengjie Zhang. 2018a. 进化深度学习：一种基于遗传编程的图像分类方法。在*IEEE
    Evol. Comput. 学会议*。1538–1545。'
- en: 'Evans et al. (2018b) Benjamin Patrick Evans, Harith Al-Sahaf, Bing Xue, and
    Mengjie Zhang. 2018b. Evolutionary Deep Learning: A Genetic Programming Approach
    to Image Classification. In *Proc. IEEE Congr. Evol. Comput.* 1–6.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Evans et al. (2018b)](https://ieeexplore.ieee.org/document/8479282) Benjamin Patrick
    Evans, Harith Al-Sahaf, Bing Xue, and Mengjie Zhang. 2018b. 进化深度学习：一种基于遗传编程的图像分类方法。在*IEEE
    Evol. Comput. 学会议*。1–6。'
- en: Fahrudin et al. (2016) Tresna Maulana Fahrudin, Iwan Syarif, and Ali Ridho Barakbah.
    2016. Ant Colony Algorithm for Feature Selection on Microarray Datasets. In *International
    Electronics Symposium*. 351–356.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fahrudin et al. (2016)](https://doi.org/10.1109/IESYSD.2016.7852844) Tresna Maulana
    Fahrudin, Iwan Syarif, and Ali Ridho Barakbah. 2016. 特征选择的蚁群算法在微阵列数据集上的应用。在*国际电子学研讨会*。351–356。'
- en: Fan et al. (2020) Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, and Wenji Li.
    2020. Evolutionary Neural Architecture Search for Retinal Vessel Segmentation.
    *arXiv preprint arXiv:2001.06678* (2020).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fan et al. (2020)](https://arxiv.org/abs/2001.06678) Zhun Fan, Jiahong Wei,
    Guijie Zhu, Jiajie Mo, and Wenji Li. 2020. 用于视网膜血管分割的进化神经架构搜索。*arXiv 预印本 arXiv:2001.06678*
    (2020)。'
- en: Fernandes and Yen (2021) Francisco Erivaldo Fernandes and Gary G. Yen. 2021.
    Automatic Searching and Pruning of Deep Neural Networks for Medical Imaging Diagnostic.
    *IEEE Trans. Neural Netw. Learn. Syst.* 32, 12 (2021), 5664–5674.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fernandes and Yen (2021)](https://doi.org/10.1109/TNNLS.2021.3090053) Francisco Erivaldo
    Fernandes and Gary G. Yen. 2021. 医学影像诊断的深度神经网络的自动搜索和修剪。*IEEE Trans. Neural Netw.
    Learn. Syst.* 32, 12 (2021), 5664–5674。'
- en: Fogelberg and Zhang (2005) Christopher Fogelberg and Mengjie Zhang. 2005. Linear
    Genetic Programming for Multi-class Object Classification. In *Proc. Aust. Joint
    Conf. Artif.l Intell.* 369–379.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fogelberg and Zhang (2005)](https://doi.org/10.1007/11552569_36) Christopher
    Fogelberg and Mengjie Zhang. 2005. 多类物体分类的线性遗传编程。在*澳大利亚人工智能联合会议*。369–379。'
- en: Fortuna and Frasca (2021) Luigi Fortuna and Mattia Frasca. 2021. Singular Value
    Decomposition. *Optim. Rob. Control* (2021), 51–58.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fortuna and Frasca (2021)](https://doi.org/10.1080/01430708.2021.1902983)
    Luigi Fortuna and Mattia Frasca. 2021. 奇异值分解。*Optim. Rob. 控制* (2021), 51–58。'
- en: 'Frachon et al. (2019) Luc Frachon, Wei Pang, and George M Coghill. 2019. Immunecs:
    Neural Committee Search by an Artificial Immune System. *arXiv preprint arXiv:1911.07729*
    (2019).'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Frachon et al. (2019)](https://arxiv.org/abs/1911.07729) Luc Frachon, Wei
    Pang, and George M Coghill. 2019. Immunecs: 神经委员会搜索的人工免疫系统。*arXiv 预印本 arXiv:1911.07729*
    (2019)。'
- en: Freitas (2003) Alex A Freitas. 2003. A Survey of Evolutionary Algorithms for
    Data Mining and Knowledge Discovery. In *Adv. Evol. Comput.* Springer, 819–845.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Freitas (2003)](https://doi.org/10.1007/978-3-540-39695-5_31) Alex A Freitas.
    2003. 用于数据挖掘和知识发现的进化算法综述。在*Adv. Evol. Comput.* Springer, 819–845。'
- en: Fujino et al. (2017) Saya Fujino, Naoki Mori, and Keinosuke Matsumoto. 2017.
    Deep Convolutional Networks for Human Sketches By Means of The Evolutionary Deep
    Learning. In *Proc. Int. Conf. Soft Comput. Intell. Syst.* 1–5.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fujino et al. (2017)](https://doi.org/10.1109/SCIS-ISIS.2017.108) Saya Fujino,
    Naoki Mori, and Keinosuke Matsumoto. 2017. 通过进化深度学习实现人类素描的深度卷积网络。在*国际软计算智能系统会议*。1–5。'
- en: García et al. (2011) David García, Antonio González Muñoz, and Raúl Pérez. 2011.
    A Two-Step Approach of Feature Construction for A Genetic Learning Algorithm.
    *Proc. Int. Conf. Fuzzy Syst.* (2011), 1255–1262.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: García 等 (2011) David García, Antonio González Muñoz, 和 Raúl Pérez. 2011. 一种用于遗传学习算法的特征构造的两步法。见
    *Proc. Int. Conf. Fuzzy Syst.* (2011), 1255–1262。
- en: Gerum et al. (2020) Richard C Gerum, André Erpenbeck, Patrick Krauss, and Achim
    Schilling. 2020. Sparsity Through Evolutionary Pruning Prevents Neuronal Networks
    From Overfitting. *Neural Netw.* 128 (2020), 305–312.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerum 等 (2020) Richard C Gerum, André Erpenbeck, Patrick Krauss, 和 Achim Schilling.
    2020. 通过进化剪枝实现的稀疏性防止神经网络过拟合。*Neural Netw.* 128 (2020), 305–312。
- en: Golubski and Feuring (1999) Wolfgang Golubski and Thomas Feuring. 1999. Evolving
    Neural Network Structures by Means of Genetic Programming. In *Proc. Eur. Conf.
    Genetic Program*. 211–220.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golubski 和 Feuring (1999) Wolfgang Golubski 和 Thomas Feuring. 1999. 通过遗传编程演化神经网络结构。见
    *Proc. Eur. Conf. Genetic Program*. 211–220。
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. 2021. Knowledge Distillation: A Survey. *Int. J. Comput. Vis.* 129, 6 (2021),
    1789–1819.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等 (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, 和 Dacheng Tao. 2021.
    知识蒸馏：综述。*Int. J. Comput. Vis.* 129, 6 (2021), 1789–1819。
- en: Guo et al. (2020) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, and Jian Sun. 2020. Single Path One-Shot Neural Architecture Search
    with Uniform Sampling. In *Proc. Eur. Conf. Comput. Vis.* 544–560.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2020) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen
    Wei, 和 Jian Sun. 2020. 单路径一次性神经架构搜索与均匀采样。见 *Proc. Eur. Conf. Comput. Vis.* 544–560。
- en: 'Hajati et al. (2010) Farshid Hajati, Caro Lucas, and Yongsheng Gao. 2010. Face
    Localization Using an Effective Co-evolutionary Genetic Algorithm. *Proc. Int.
    Conf. Digit. Image Comput.: Tech. and Appl.* (2010), 522–527.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hajati 等 (2010) Farshid Hajati, Caro Lucas, 和 Yongsheng Gao. 2010. 使用有效的协进化遗传算法进行人脸定位。见
    *Proc. Int. Conf. Digit. Image Comput.: Tech. and Appl.* (2010), 522–527。'
- en: Hammami et al. (2018) Marwa Hammami, Slim Bechikh, and Chih-Cheng Hung. 2018.
    A Multi-Objective Hybrid Filter-Wrapper Evolutionary Approach for Feature Construction
    on High-Dimensional Data. In *Proc. IEEE Congr. Evol. Comput.* 1–8.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hammami 等 (2018) Marwa Hammami, Slim Bechikh, 和 Chih-Cheng Hung. 2018. 面向高维数据的多目标混合过滤-包裹进化方法用于特征构造。见
    *Proc. IEEE Congr. Evol. Comput.* 1–8。
- en: Han and Cho (2006) Sang-Jun Han and Sung-Bae Cho. 2006. Evolutionary Neural
    Networks for Anomaly Detection Based on the Behavior of a Program. *IEEE Trans.
    Syst. Man Cybern.* 36, 3 (2006), 559–570.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 和 Cho (2006) Sang-Jun Han 和 Sung-Bae Cho. 2006. 基于程序行为的异常检测的进化神经网络。*IEEE
    Trans. Syst. Man Cybern.* 36, 3 (2006), 559–570。
- en: Hancer et al. (2015) Emrah Hancer, Bing Xue, Mengjie Zhang, and Dervis Karaboga.
    2015. A Multi-objective Artificial Bee Colony Approach to Feature Selection Using
    Fuzzy Mutual Information. In *Proc. IEEE Congr. Evol. Comput.* 2420–2427.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hancer 等 (2015) Emrah Hancer, Bing Xue, Mengjie Zhang, 和 Dervis Karaboga. 2015.
    使用模糊互信息的多目标人工蜂群算法进行特征选择。见 *Proc. IEEE Congr. Evol. Comput.* 2420–2427。
- en: 'He et al. (2021) Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey
    of the State-of-the-Art. *Knowl-Based Syst* 212 (2021), 106622.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 (2021) Xin He, Kaiyong Zhao, 和 Xiaowen Chu. 2021. AutoML: 前沿技术调查。*Knowl-Based
    Syst* 212 (2021), 106622。'
- en: He et al. (2017) Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel Pruning
    for Accelerating Very Deep Neural Networks. In *Proc. IEEE Int. Conf. Comput.
    Vis.* 1398–1406.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2017) Yihui He, Xiangyu Zhang, 和 Jian Sun. 2017. 加速非常深层神经网络的通道剪枝。见 *Proc.
    IEEE Int. Conf. Comput. Vis.* 1398–1406。
- en: Ho et al. (2021) Kary Ho, Andrew Gilbert, Hailin Jin, and John P. Collomosse.
    2021. Neural Architecture Search for Deep Image Prior. *Comput. & Graph.* 98 (2021),
    188–196.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等 (2021) Kary Ho, Andrew Gilbert, Hailin Jin, 和 John P. Collomosse. 2021.
    深度图像先验的神经架构搜索。*Comput. & Graph.* 98 (2021), 188–196。
- en: Hong and Cho (2006) Jin-Hyuk Hong and Sung-Bae Cho. 2006. Efficient Huge-Scale
    Feature Selection With Speciated Genetic Algorithm. *Pattern Recognit. Lett.*
    27, 2 (2006), 143–150.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 和 Cho (2006) Jin-Hyuk Hong 和 Sung-Bae Cho. 2006. 使用专化遗传算法进行高效的大规模特征选择。*Pattern
    Recognit. Lett.* 27, 2 (2006), 143–150。
- en: Hong et al. (2020) Wenjing Hong, Peng Yang, Yiwen Wang, and Ke Tang. 2020. Multi-objective
    Magnitude-Based Pruning for Latency-Aware Deep Neural Network Compression. In
    *Proc. Int. Conf. on Parallel Probl. Solving Nat.* 470–483.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等 (2020) Wenjing Hong, Peng Yang, Yiwen Wang, 和 Ke Tang. 2020. 面向延迟感知的多目标基于幅度的剪枝用于深度神经网络压缩。见
    *Proc. Int. Conf. on Parallel Probl. Solving Nat.* 470–483。
- en: Hosni et al. (2020) Mohamed Hosni, Ginés García-Mateos, and Juan Carrillo-de
    Gea. 2020. A Mapping Study of Ensemble Classification Methods in Lung Cancer Decision
    Support Systems. *Med. & Biol. Eng. & Comput.* 58, 10 (2020), 2177–2193.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosni 等人 (2020) Mohamed Hosni、Ginés García-Mateos 和 Juan Carrillo-de Gea。2020年。肺癌决策支持系统中集成分类方法的映射研究。*医学与生物工程与计算*
    58, 10 (2020), 2177–2193。
- en: Hsu et al. (2021) Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    and Hongxia Jin. 2021. Language Model Compression with Weighted Low-rank Factorization.
    In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/2207.00112.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu 等人 (2021) Yen-Chang Hsu、Ting Hua、Sungen Chang、Qian Lou、Yilin Shen 和 Hongxia
    Jin。2021年。基于加权低秩分解的语言模型压缩。在 *国际学习表征会议论文集* https://arxiv.org/abs/2207.00112。
- en: 'Hu et al. (2021b) Bin Hu, Tianming Zhao, Yucheng Xie, Yan Wang, and Xiaonan
    Guo. 2021b. MIXP: Efficient Deep Neural Networks Pruning for Further FLOPs Compression
    via Neuron Bond. In *Proc. Int. Joint Conf. Neural Netw.* 1–8.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2021b) Bin Hu、Tianming Zhao、Yucheng Xie、Yan Wang 和 Xiaonan Guo。2021b。MIXP：通过神经元绑定进一步压缩
    FLOPs 的高效深度神经网络剪枝。在 *国际联合神经网络会议论文集* 1–8。
- en: Hu et al. (2021a) Yiming Hu, Xingang Wang, Lujun Li, and Qingyi Gu. 2021a. Improving
    One-Shot NAS with Shrinking-and-Expanding Supernet. *Pattern Recognit.* 118 (2021),
    108025.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2021a) Yiming Hu、Xingang Wang、Lujun Li 和 Qingyi Gu。2021a。通过收缩与扩展超级网络改进单次神经架构搜索。*模式识别*
    118 (2021), 108025。
- en: Huang et al. (2012) Hu Huang, Hong-Bo Xie, Jing-Yi Guo, and Hui-Juan Chen. 2012.
    Ant Colony Optimization-based Feature Selection Method for Surface Electromyography
    Signals Classification. *Comput. Biol. Med.* 42, 1 (2012), 30–38.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2012) Hu Huang、Hong-Bo Xie、Jing-Yi Guo 和 Hui-Juan Chen。2012年。基于蚁群优化的表面肌电信号分类特征选择方法。*计算生物医学*
    42, 1 (2012), 30–38。
- en: Huang et al. (2020) Junhao Huang, Weize Sun, and Lei Huang. 2020. Deep Neural
    Networks Compression Learning Based on Multiobjective Evolutionary Algorithms.
    *Neurocomputing* 378 (2020), 260–269.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2020) Junhao Huang、Weize Sun 和 Lei Huang。2020年。基于多目标进化算法的深度神经网络压缩学习。*神经计算*
    378 (2020), 260–269。
- en: Ijjina and Chalavadi (2016) Earnest Paul Ijjina and Krishna Mohan Chalavadi.
    2016. Human Action Recognition Using Genetic Algorithms and Convolutional Neural
    Networks. *Pattern Recognit.* 59 (2016), 199–212.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ijjina 和 Chalavadi (2016) Earnest Paul Ijjina 和 Krishna Mohan Chalavadi。2016年。使用遗传算法和卷积神经网络的人体动作识别。*模式识别*
    59 (2016), 199–212。
- en: Irwin-Harris et al. (2019) William Irwin-Harris, Yanan Sun, Bing Xue, and Mengjie
    Zhang. 2019. A Graph-Based Encoding for Evolutionary Convolutional Neural Network
    Architecture Design. In *Proc. IEEE Congr. Evol. Comput.* 546–553.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irwin-Harris 等人 (2019) William Irwin-Harris、Yanan Sun、Bing Xue 和 Mengjie Zhang。2019年。用于演化卷积神经网络架构设计的图编码。
    在 *IEEE 进化计算大会论文集* 546–553。
- en: Izenman (2013) Alan Julian Izenman. 2013. Linear Discriminant Analysis. In *Modern
    Multivariate Statistical Techniques*. Springer, 237–280.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izenman (2013) Alan Julian Izenman。2013年。线性判别分析。在 *现代多变量统计技术*。Springer，237–280。
- en: 'Jaâfra et al. (2019) Yesmina Jaâfra, Jean Luc Laurent, Aline Deruyver, and
    Mohamed Saber Naceur. 2019. Reinforcement Learning for Neural Architecture Search:
    A Review. *Image Vis. Comput.* 89 (2019), 57–66.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaâfra 等人 (2019) Yesmina Jaâfra、Jean Luc Laurent、Aline Deruyver 和 Mohamed Saber
    Naceur。2019年。神经架构搜索的强化学习：综述。*图像视觉计算* 89 (2019), 57–66。
- en: 'Jin et al. (2018) Haifeng Jin, Qingquan Song, and Xia Hu. 2018. Auto-keras:
    Efficient Neural Architecture Search with Network Morphism. *arXiv preprint arXiv:1806.10282*
    (2018).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2018) Haifeng Jin、Qingquan Song 和 Xia Hu。2018年。Auto-keras：具有网络形态学的高效神经架构搜索。*arXiv
    预印本 arXiv:1806.10282* (2018)。
- en: 'Jin et al. (2019) Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras:
    An Efficient Neural Architecture Search System. In *Proc. ACM SIGKDD Int. Conf.
    on Knowl. Discov. & Data Min.* 1946–1956.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2019) Haifeng Jin、Qingquan Song 和 Xia Hu。2019年。Auto-Keras：高效的神经架构搜索系统。在
    *ACM SIGKDD 国际会议论文集* 1946–1956。
- en: Jin (2006) Yaochu Jin. 2006. *Multi-Objective Machine Learning*. Springer Science.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin (2006) Yaochu Jin。2006年。*多目标机器学习*。Springer Science。
- en: Jones et al. (2019) David T Jones, Anja Schroeder, and Geoff S. Nitschke. 2019.
    Evolutionary Deep Learning to Identify Galaxies in the Zone of Avoidance. *arXiv
    preprint arXiv:1903.07461* (2019).
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones 等人 (2019) David T Jones、Anja Schroeder 和 Geoff S. Nitschke。2019年。演化深度学习用于识别回避区的星系。*arXiv
    预印本 arXiv:1903.07461* (2019)。
- en: Junior and Yen (2021a) Francisco Erivaldo Fernandes Junior and Gary G. Yen.
    2021a. Pruning Deep Convolutional Neural Networks Architectures with Evolution
    Strategy. *Inf. Sci.* 552 (2021), 29–47.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Junior 和 Yen (2021a) Francisco Erivaldo Fernandes Junior 和 Gary G. Yen。2021a。使用进化策略修剪深度卷积神经网络架构。*信息科学*
    552 (2021), 29–47。
- en: Junior and Yen (2021b) Francisco Erivaldo Fernandes Junior and Gary G. Yen.
    2021b. Pruning of Generative Adversarial Neural Networks for Medical Imaging Diagnostics
    with Evolution Strategy. *Inf. Sci.* 558 (2021), 91–102.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Junior 和 Yen（2021b）Francisco Erivaldo Fernandes Junior 和 Gary G. Yen。2021b年。通过进化策略对生成对抗神经网络进行剪枝，用于医疗成像诊断。*Inf.
    Sci.* 558（2021），91–102。
- en: Karaboga et al. (2007) Dervis Karaboga, Bahriye Akay, and Celal Öztürk. 2007.
    Artificial Bee Colony (ABC) Optimization Algorithm for Training Feed-Forward Neural
    Networks. In *Proc. Int. Conf. Modeling Decis. Artif. Intell.* 318–329.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karaboga 等（2007）Dervis Karaboga，Bahriye Akay 和 Celal Öztürk。2007年。用于训练前馈神经网络的人工蜂群（ABC）优化算法。发表于*Proc.
    Int. Conf. Modeling Decis. Artif. Intell.* 318–329。
- en: Karegowda and Manjunath (2011) Asha Gowda Karegowda and A. S. Manjunath. 2011.
    Application of Genetic Algorithm Optimized Neural Network Connection Weights for
    Medical Diagnosis of PIMA Indians Diabetes. *Int. J. Soft Comput.* 2, 2 (2011),
    15–23.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karegowda 和 Manjunath（2011）Asha Gowda Karegowda 和 A. S. Manjunath。2011年。基于遗传算法优化的神经网络连接权重在PIMA印第安人糖尿病医疗诊断中的应用。*Int.
    J. Soft Comput.* 2, 2（2011），15–23。
- en: Khadka and Tumer (2018) Shauharda Khadka and Kagan Tumer. 2018. Evolution-Guided
    Policy Gradient in Reinforcement Learning. In *Proc. Adv. Neural Inf. Process.
    Syst.*, Vol. 31\. 1196–1208.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadka 和 Tumer（2018）Shauharda Khadka 和 Kagan Tumer。2018年。强化学习中的进化引导策略梯度。发表于*Proc.
    Adv. Neural Inf. Process. Syst.*，第31卷。1196–1208。
- en: Khushaba et al. (2008) Rami N Khushaba, Ahmed Al-Ani, Akram AlSukker, and Adel
    Al-Jumaily. 2008. A Combined Ant Colony and Differential Evolution Feature Selection
    Algorithm. In *Proc. Int. Conf. Ant Colony Optim. Swarm Intell.* 1–12.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khushaba 等（2008）Rami N Khushaba，Ahmed Al-Ani，Akram AlSukker 和 Adel Al-Jumaily。2008年。结合蚁群算法和差分进化特征选择算法。发表于*Proc.
    Int. Conf. Ant Colony Optim. Swarm Intell.* 1–12。
- en: Kitano (1990) Hiroaki Kitano. 1990. Designing Neural Networks Using Genetic
    Algorithms with Graph Generation System. *Complex Syst.* 4, 4 (1990), 225–238.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitano（1990）Hiroaki Kitano。1990年。使用图生成系统设计神经网络的遗传算法。*Complex Syst.* 4, 4（1990），225–238。
- en: Kotani and Kato (2004) Manabu Kotani and Daisuke Kato. 2004. Feature Extraction
    Using Coevolutionary Genetic Programming. In *Proc. IEEE Congr. Evol. Comput.*
    614–619.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotani 和 Kato（2004）Manabu Kotani 和 Daisuke Kato。2004年。使用共进化遗传编程进行特征提取。发表于*Proc.
    IEEE Congr. Evol. Comput.* 614–619。
- en: Koutník et al. (2010) Jan Koutník, Faustino J. Gomez, and Jürgen Schmidhuber.
    2010. Evolving Neural Networks in Compressed Weight Space. In *Proc. Genetic Evol.
    Comput. Conf.* 619–626.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koutník 等（2010）Jan Koutník，Faustino J. Gomez 和 Jürgen Schmidhuber。2010年。在压缩权重空间中进化神经网络。发表于*Proc.
    Genetic Evol. Comput. Conf.* 619–626。
- en: Koutník et al. (2014) Jan Koutník, Jürgen Schmidhuber, and Faustino J. Gomez.
    2014. Evolving Deep Unsupervised Convolutional Networks for Vision-Based Reinforcement
    Learning. In *Proc. Genetic Evol. Comput. Conf.* 541–548.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koutník 等（2014）Jan Koutník，Jürgen Schmidhuber 和 Faustino J. Gomez。2014年。为基于视觉的强化学习进化深度无监督卷积网络。发表于*Proc.
    Genetic Evol. Comput. Conf.* 541–548。
- en: Krawiec (2002) Krzysztof Krawiec. 2002. Genetic Programming-Based Construction
    of Features for Machine Learning and Knowledge Discovery Tasks. *Genet. Program
    Evolvable Mach.* 3, 4 (2002), 329–343.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krawiec（2002）Krzysztof Krawiec。2002年。基于遗传编程的特征构造用于机器学习和知识发现任务。*Genet. Program
    Evolvable Mach.* 3, 4（2002），329–343。
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    Multiple Layers of Features From Tiny Images. (2009).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2009）Alex Krizhevsky，Geoffrey Hinton 等。2009年。从微小图像中学习多个层次的特征。（2009年）。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Proc.
    Adv. Neural Inf. Process. Syst.*, Vol. 25\. 1097–1105.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）Alex Krizhevsky，Ilya Sutskever 和 Geoffrey E Hinton。2012年。使用深度卷积神经网络的ImageNet分类。发表于*Proc.
    Adv. Neural Inf. Process. Syst.*，第25卷。1097–1105。
- en: Kwasigroch et al. (2019) Arkadiusz Kwasigroch, Michał Grochowski, and Mateusz
    Mikolajczyk. 2019. Deep Neural Network Architecture Search using Network Morphism.
    In *Proc. Int. Conf. Methods and Models in Autom. and Robot.* 30–35.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwasigroch 等（2019）Arkadiusz Kwasigroch，Michał Grochowski 和 Mateusz Mikolajczyk。2019年。使用网络形态学的深度神经网络架构搜索。发表于*Proc.
    Int. Conf. Methods and Models in Autom. and Robot.* 30–35。
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    Learning. *Nature* 521, 7553 (2015), 436–444.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（2015）Yann LeCun，Yoshua Bengio 和 Geoffrey Hinton。2015年。深度学习。*Nature*
    521, 7553（2015），436–444。
- en: LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, and Wayne Hubbard. 1989. Backpropagation Applied to Handwritten
    Zip Code Recognition. *Neural computation* 1, 4 (1989), 541–551.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1989）Yann LeCun，Bernhard Boser，John S Denker，Donnie Henderson，Richard
    E Howard 和 Wayne Hubbard。1989年。反向传播应用于手写邮政编码识别。*Neural computation* 1, 4（1989），541–551。
- en: 'Li et al. (2020) Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, and Liang Lin.
    2020. EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning.
    In *Proc. Eur. Conf. Comput. Vis.* 639–654.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, and Liang Lin.
    2020. EagleEye：用于高效神经网络剪枝的快速子网络评估。在*Proc. Eur. Conf. Comput. Vis.* 639–654.
- en: 'Li et al. (2021) Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao,
    Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. HW-NAS-Bench:
    Hardware-Aware Neural Architecture Search Benchmark. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/2103.10584.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao,
    Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. 2021. HW-NAS-Bench：硬件感知神经架构搜索基准。在*Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/2103.10584.
- en: Li et al. (2022) Qing Li, Wei Zhang, Lin Zhao, Xia Wu, and Tianming Liu. 2022.
    Evolutional Neural Architecture Search for Optimization of Spatiotemporal Brain
    Network Decomposition. *IEEE. Trans. Biomed. Eng.* 69, 2 (2022), 624–634.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Qing Li, Wei Zhang, Lin Zhao, Xia Wu, and Tianming Liu. 2022.
    用于优化时空脑网络分解的进化神经架构搜索。*IEEE. Trans. Biomed. Eng.* 69, 2 (2022), 624–634.
- en: 'Li et al. (2019) Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao.
    2019. EA-LSTM: Evolutionary Attention-Based LSTM for Time Series Prediction. *Knowl.-Based
    Syst.* 181 (2019), 104785.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, and Yao Zhao.
    2019. EA-LSTM：基于进化注意力的LSTM用于时间序列预测。*Knowl.-Based Syst.* 181 (2019), 104785.
- en: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive Neural Architecture Search. In *Proc. Eur. Conf. Comput. Vis.*
    19–34.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. 渐进神经架构搜索。在*Proc. Eur. Conf. Comput. Vis.* 19–34.
- en: 'Liu et al. (2021a) Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
    Chiang, and Kai-Chiang Wu. 2021a. FOX-NAS: Fast, On-device and Explainable Neural
    Architecture Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 789–797.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021a) Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
    Chiang, and Kai-Chiang Wu. 2021a. FOX-NAS：快速、设备端和可解释的神经架构搜索。在*Proc. IEEE Int.
    Conf. Comput. Vis.* 789–797.
- en: 'Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018b. DARTS:
    Differentiable Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1806.09055.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018b. DARTS：可微分架构搜索。在*Proc.
    Int. Conf. Learn. Represent.* https://arxiv.org/abs/1806.09055.
- en: Liu et al. (2019a) Peng Liu, Mohammad D. El Basha, Yangjunyi Li, Yao Xiao, Pina C.
    Sanelli, and Ruogu Fang. 2019a. Deep Evolutionary Networks with Expedited Genetic
    Algorithms for Medical Image Denoising. *Med. Image Anal.* 54 (2019), 306–315.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019a) Peng Liu, Mohammad D. El Basha, Yangjunyi Li, Yao Xiao, Pina
    C. Sanelli, and Ruogu Fang. 2019a. 使用加速遗传算法的深度进化网络用于医学图像去噪。*Med. Image Anal.*
    54 (2019), 306–315.
- en: 'Liu and Guo (2021) Sicong Liu and Bin Guo. 2021. AdaSpring: Context-adaptive
    and Runtime-evolutionary Deep Model Compression for Mobile Applications. In *Proc.
    ACM Interact., Mobile, Wearable Ubiquitous Tech.*, Vol. 5\. ACM, 1–22.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Guo (2021) Sicong Liu and Bin Guo. 2021. AdaSpring：适应上下文和运行时演化的深度模型压缩用于移动应用。在*Proc.
    ACM Interact., Mobile, Wearable Ubiquitous Tech.*, Vol. 5. ACM, 1–22.
- en: Liu et al. (2018a) Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, and Han-Shuo
    Ye. 2018a. A Hybrid Genetic Algorithm With Wrapper-Embedded Approaches for Feature
    Selection. *IEEE Access* 6 (2018), 22863–22874.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018a) Xiao-Ying Liu, Yong Liang, Sai Wang, Zi-Yi Yang, and Han-Shuo
    Ye. 2018a. 一种混合遗传算法与包装嵌入方法的特征选择。*IEEE Access* 6 (2018), 22863–22874.
- en: Liu et al. (2021b) Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen,
    and Kay Chen Tan. 2021b. A Survey on Evolutionary Neural Architecture Search.
    *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3100554](https://doi.org/10.1109/TNNLS.2021.3100554)
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G. Yen,
    and Kay Chen Tan. 2021b. 关于进化神经架构搜索的综述。*IEEE Trans. Neural Netw. Learn. Syst.*
    (2021). [https://doi.org/10.1109/TNNLS.2021.3100554](https://doi.org/10.1109/TNNLS.2021.3100554)
- en: 'Liu et al. (2019b) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    K. Cheng, and Jian Sun. 2019b. MetaPruning: Meta Learning for Automatic Neural
    Network Channel Pruning. In *Proc. IEEE Int. Conf. Comput. Vis.* 3295–3304.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019b) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    K. Cheng, and Jian Sun. 2019b. MetaPruning：用于自动神经网络通道剪枝的元学习。在*Proc. IEEE Int.
    Conf. Comput. Vis.* 3295–3304.
- en: Lomurno et al. (2021) Eugenio Lomurno, Stefano Samele, Matteo Matteucci, and
    Danilo Ardagna. 2021. Pareto-optimal Progressive Neural Architecture Search. In
    *Proc. Genetic Evol. Comput. Conf.* 1726–1734.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lomurno 等（2021）Eugenio Lomurno, Stefano Samele, Matteo Matteucci 和 Danilo Ardagna。2021年。Pareto
    最优渐进神经架构搜索。发表于 *Proc. Genetic Evol. Comput. Conf.* 1726–1734。
- en: Londt et al. (2021) Trevor Londt, Xiaoying Gao, and Peter Andreae. 2021. Evolving
    Character-level DenseNet Architectures Using Genetic Programming. In *Proc. Int.
    Conf. Appl. Evol. Comput.* 665–680.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Londt 等（2021）Trevor Londt, Xiaoying Gao 和 Peter Andreae。2021年。使用遗传编程进化字符级 DenseNet
    架构。发表于 *Proc. Int. Conf. Appl. Evol. Comput.* 665–680。
- en: Londt et al. (2020) Trevor Londt, Xiaoying Gao, Bing Xue, and Peter Andreae.
    2020. Evolving Character-level Convolutional Neural Networks for Text Classification.
    *arXiv preprint arXiv:2012.02223* (2020).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Londt 等（2020）Trevor Londt, Xiaoying Gao, Bing Xue 和 Peter Andreae。2020年。针对文本分类的字符级卷积神经网络进化。*arXiv
    preprint arXiv:2012.02223*（2020）。
- en: 'Loni et al. (2020) Mohammad Loni, Sima Sinaei, and Ali Zoljodi. 2020. DeepMaker:
    A Multi-Objective Optimization Framework for Deep Neural Networks in Embedded
    Systems. *Microprocess. Microsyst.* 73 (2020), 102989.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loni 等（2020）Mohammad Loni, Sima Sinaei 和 Ali Zoljodi。2020年。DeepMaker：用于嵌入式系统中深度神经网络的多目标优化框架。*Microprocess.
    Microsyst.* 73（2020），102989。
- en: Lorenzo and Nalepa (2018) Pablo Ribalta Lorenzo and Jakub Nalepa. 2018. Memetic
    Evolution of Deep Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 505–512.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorenzo 和 Nalepa（2018）Pablo Ribalta Lorenzo 和 Jakub Nalepa。2018年。深度神经网络的记忆进化。发表于
    *Proc. Genetic Evol. Comput. Conf.* 505–512。
- en: Lorenzo et al. (2017) Pablo Ribalta Lorenzo, Jakub Nalepa, Luciano Sánchez Ramos,
    and José Ranilla. 2017. Hyper-parameter Selection in Deep Neural Networks Using
    Parallel Particle Swarm Optimization. In *Proc. Genetic Evol. Comput. Conf.* 1864–1871.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorenzo 等（2017）Pablo Ribalta Lorenzo, Jakub Nalepa, Luciano Sánchez Ramos 和
    José Ranilla。2017年。使用并行粒子群优化进行深度神经网络的超参数选择。发表于 *Proc. Genetic Evol. Comput. Conf.*
    1864–1871。
- en: Lu et al. (2021) Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf,
    Kalyanmoy Deb, and Vishnu Naresh Boddeti. 2021. Neural Architecture Transfer.
    *IEEE IEEE Trans. Pattern Anal. Mach. Intell.* 43, 9 (2021), 2971–2989.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2021）Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf, Kalyanmoy
    Deb 和 Vishnu Naresh Boddeti。2021年。神经架构迁移。*IEEE IEEE Trans. Pattern Anal. Mach.
    Intell.* 43, 9（2021），2971–2989。
- en: 'Lu et al. (2019) Zhichao Lu, Ian Whalen, Vishnu Naresh Boddeti, Yashesh D.
    Dhebar, and Kalyanmoy Deb. 2019. NSGA-Net: Neural Architecture Search using Multi-objective
    Genetic Algorithm. In *Proc. Genetic Evol. Comput. Conf.* 419–427.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2019）Zhichao Lu, Ian Whalen, Vishnu Naresh Boddeti, Yashesh D. Dhebar 和
    Kalyanmoy Deb。2019年。NSGA-Net：使用多目标遗传算法的神经架构搜索。发表于 *Proc. Genetic Evol. Comput.
    Conf.* 419–427。
- en: Luo et al. (2018) Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
    2018. Neural architecture optimization. In *Proc. Adv. Neural Inf. Process. Syst.*,
    Vol. 31\. 7827–7838.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2018）Renqian Luo, Fei Tian, Tao Qin, Enhong Chen 和 Tie-Yan Liu。2018年。神经架构优化。发表于
    *Proc. Adv. Neural Inf. Process. Syst.*, Vol. 31\. 7827–7838。
- en: 'Ma et al. (2021b) Ailong Ma, Yuting Wan, Yanfei Zhong, Junjue Wang, and Liang
    pei Zhang. 2021b. SceneNet: Remote Sensing Scene Classification Deep Learning
    Network Using Multi-Objective Neural Evolution Architecture Search. *ISPRS J.
    Photogramm. Remote Sens.* 172 (2021), 171–188.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2021b）Ailong Ma, Yuting Wan, Yanfei Zhong, Junjue Wang 和 Liang pei Zhang。2021b年。SceneNet：使用多目标神经进化架构搜索的遥感场景分类深度学习网络。*ISPRS
    J. Photogramm. Remote Sens.* 172（2021），171–188。
- en: Ma et al. (2022) Lianbo Ma, Min Huang, Shengxiang Yang, Rui Wang, and Xingwei
    Wang. 2022. An Adaptive Localized Decision Variable Analysis Approach to Large-Scale
    Multiobjective and Many-Objective Optimization. *IEEE Trans. Cybern.* 52, 7 (2022),
    6684–6696.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2022）Lianbo Ma, Min Huang, Shengxiang Yang, Rui Wang 和 Xingwei Wang。2022年。一种自适应局部决策变量分析方法用于大规模多目标和多目标优化。*IEEE
    Trans. Cybern.* 52, 7（2022），6684–6696。
- en: 'Ma et al. (2021a) Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang, and Xingwei
    Wang. 2021a. How to Simplify Search: Classification-wise Pareto Evolution for
    One-shot Neural Architecture Search. *arXiv preprint arXiv:2109.07582* (2021).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2021a）Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang 和 Xingwei Wang。2021a年。如何简化搜索：面向分类的
    Pareto 进化用于一阶段神经架构搜索。*arXiv preprint arXiv:2109.07582*（2021）。
- en: Ma et al. (2021c) Wenping Ma, Xiaobo Zhou, Hao Zhu, Longwei Li, and Licheng
    Jiao. 2021c. A Two-stage Hybrid Ant Colony Optimization for High-dimensional Feature
    Selection. *Pattern Recognit.* 116 (2021), 107933.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2021c）Wenping Ma, Xiaobo Zhou, Hao Zhu, Longwei Li 和 Licheng Jiao。2021c年。一种两阶段混合蚁群优化用于高维特征选择。*Pattern
    Recognit.* 116（2021），107933。
- en: Maniezzo (1994) V. Maniezzo. 1994. Genetic Evolution of the Topology and Weight
    Distribution of Neural Networks. *IEEE Trans. Neural. Netw.* 5, 1 (1994), 39–53.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maniezzo (1994) V. Maniezzo。1994年。神经网络拓扑和权重分布的遗传进化。*IEEE Trans. Neural. Netw.*
    5, 1 (1994), 39–53。
- en: Mauceri et al. (2021) Stefano Mauceri, James Sweeney, Miguel Nicolau, and James
    McDermott. 2021. Feature Extraction by Grammatical Evolution for One-class Time
    Series Classification. *Genet. Program. Evolvable Mach.* 22, 3 (2021), 267–295.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mauceri 等人 (2021) Stefano Mauceri, James Sweeney, Miguel Nicolau 和 James McDermott。2021年。通过语法进化进行特征提取用于单类时间序列分类。*Genet.
    Program. Evolvable Mach.* 22, 3 (2021), 267–295。
- en: Mazzawi et al. (2019) Hanna Mazzawi, Xavi Gonzalvo, Aleksandar Kracun, and Prashant
    Sridhar. 2019. Improving Keyword Spotting and Language Identification via Neural
    Architecture Search at Scale. In *INTERSPEECH*. 1278–1282.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mazzawi 等人 (2019) Hanna Mazzawi, Xavi Gonzalvo, Aleksandar Kracun 和 Prashant
    Sridhar。2019年。通过大规模神经架构搜索改进关键词检测和语言识别。在 *INTERSPEECH*。1278–1282。
- en: Mei et al. (2017) Yi Mei, Su Nguyen, Bing Xue, and Mengjie Zhang. 2017. An Efficient
    Feature Selection Algorithm for Evolving Job Shop Scheduling Rules With Genetic
    Programming. *IEEE Trans. Emerg. Topics Comput. Intell.* 1, 5 (2017), 339–353.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei 等人 (2017) Yi Mei, Su Nguyen, Bing Xue 和 Mengjie Zhang。2017年。用于进化作业车间调度规则的高效特征选择算法。*IEEE
    Trans. Emerg. Topics Comput. Intell.* 1, 5 (2017), 339–353。
- en: Miahi et al. (2022) Erfan Miahi, Seyed Abolghasem Mirroshandel, and Alexis Nasr.
    2022. Genetic Neural Architecture Search for Automatic Assessment of Human Sperm
    Images. *Expert Syst. Appl.* 188 (2022), 115937.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miahi 等人 (2022) Erfan Miahi, Seyed Abolghasem Mirroshandel 和 Alexis Nasr。2022年。用于自动评估人类精子图像的遗传神经架构搜索。*Expert
    Syst. Appl.* 188 (2022), 115937。
- en: Miikkulainen et al. (2019) Risto Miikkulainen, Jason Liang, Elliot Meyerson,
    Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak
    Navruzyan, Nigel Duffy, et al. 2019. Evolving Deep Neural Networks. In *Artificial
    Intelligence in the Age Of Neural Networks and Brain Computing*. Elsevier, 293–312.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miikkulainen 等人 (2019) Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya
    Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan,
    Nigel Duffy 等。2019年。进化深度神经网络。在 *Artificial Intelligence in the Age Of Neural Networks
    and Brain Computing*。Elsevier，293–312。
- en: Mirjalili et al. (2019) Seyedali Mirjalili, Hossam Faris, and Ibrahim Aljarah.
    2019. *Evolutionary Machine Learning Techniques*. Springer.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirjalili 等人 (2019) Seyedali Mirjalili, Hossam Faris 和 Ibrahim Aljarah。2019年。*进化机器学习技术*。Springer。
- en: Mo et al. (2021) Hyunho Mo, Leonardo Lucio Custode, and Giovanni Iacca. 2021.
    Evolutionary Neural Architecture Search for Remaining Useful Life Prediction.
    *Appl. Soft Comput.* 108 (2021), 107474.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo 等人 (2021) Hyunho Mo, Leonardo Lucio Custode 和 Giovanni Iacca。2021年。用于剩余有效寿命预测的进化神经架构搜索。*Appl.
    Soft Comput.* 108 (2021), 107474。
- en: Montana and Davis (1989) David J Montana and Lawrence Davis. 1989. Training
    Feedforward Neural Networks Using Genetic Algorithms. In *Proc. of the Int. Joint
    Conf. Artif. Intell.*, Vol. 4\. 762–767.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montana 和 Davis (1989) David J Montana 和 Lawrence Davis。1989年。使用遗传算法训练前馈神经网络。在
    *Proc. of the Int. Joint Conf. Artif. Intell.*, 第 4 卷。762–767。
- en: Muni et al. (2006) Durga Prasad Muni, Nikhil R Pal, and Jyotirmay Das. 2006.
    Genetic Programming for Simultaneous Feature Selection and Classifier Design.
    *IEEE Trans. Syst. Man. Cybern.* 36, 1 (2006), 106–117.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muni 等人 (2006) Durga Prasad Muni, Nikhil R Pal 和 Jyotirmay Das。2006年。用于特征选择和分类器设计的遗传编程。*IEEE
    Trans. Syst. Man. Cybern.* 36, 1 (2006), 106–117。
- en: 'Murray and Chiang (2015) Kenton Murray and David Chiang. 2015. Auto-Sizing
    Neural Networks: With Applications to N-Gram Language Models. In *Proc. Conf.
    Empir. Methods Nat. Lang. Proc.* 908–916.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murray 和 Chiang (2015) Kenton Murray 和 David Chiang。2015年。自动调整神经网络：应用于 N-Gram
    语言模型。在 *Proc. Conf. Empir. Methods Nat. Lang. Proc.* 908–916。
- en: Nekrasov et al. (2020) Vladimir Nekrasov, Chunhua Shen, and Ian Reid. 2020.
    Template-Based Automatic Search of Compact Semantic Segmentation Architectures.
    In *Proc. Winter Conf. Appl. Comput. Vis.* 1980–1989.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nekrasov 等人 (2020) Vladimir Nekrasov, Chunhua Shen 和 Ian Reid。2020年。基于模板的紧凑语义分割架构自动搜索。在
    *Proc. Winter Conf. Appl. Comput. Vis.* 1980–1989。
- en: 'Neshat et al. (2020) Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad,
    Lina Bertling Tjernberg, Davide Astiaso Garcia, Bradley Alexander, and Markus
    Wagner. 2020. An Evolutionary Deep Learning Method for Short-term Wind Speed Prediction:
    A Case Study of the Lillgrund Offshore Wind Farm. *arXiv preprint arXiv:abs/2002.09106*
    (2020).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshat 等人 (2020) Mehdi Neshat, Meysam Majidi Nezhad, Ehsan Abbasnejad, Lina
    Bertling Tjernberg, Davide Astiaso Garcia, Bradley Alexander 和 Markus Wagner。2020年。用于短期风速预测的进化深度学习方法：以
    Lillgrund 海上风电场为例。*arXiv 预印本 arXiv:abs/2002.09106* (2020)。
- en: Neshatian et al. (2012) Kourosh Neshatian, Mengjie Zhang, and Peter Andreae.
    2012. A Filter Approach to Multiple Feature Construction for Symbolic Learning
    Classifiers Using Genetic Programming. *IEEE Trans. Evol. Comput.* 16, 5 (2012),
    645–661.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshatian 等（2012）库罗什·内沙蒂安、张梦洁 和 彼得·安德烈。2012年。使用遗传编程的多特征构建的过滤方法。*IEEE Trans.
    Evol. Comput.* 16, 5 (2012), 645–661。
- en: Neshatian et al. (2007) Kourosh Neshatian, Mengjie Zhang, and Mark Johnston.
    2007. Feature Construction and Dimension Reduction Using Genetic Programming.
    In *Proc. Aust. Conf. Artif. Intell.* 242–253.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neshatian 等（2007）库罗什·内沙蒂安、张梦洁 和 马克·约翰斯顿。2007年。使用遗传编程的特征构建与维度约简。见于 *Proc. Aust.
    Conf. Artif. Intell.* 242–253。
- en: 'Nguyen et al. (2014) Hoai Bach Nguyen, Bing Xue, Ivy Liu, and Mengjie Zhang.
    2014. PSO and Statistical Clustering for Feature Selection: A New Representation.
    In *Proc. Asia-Pacific Conf. Simulated Evol. Learn.* 569–581.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阮等（2014）阮怀博、薛冰、刘艾维 和 张梦洁。2014年。用于特征选择的 PSO 和统计聚类：一种新表示。见于 *Proc. Asia-Pacific
    Conf. Simulated Evol. Learn.* 569–581。
- en: 'O’Boyle and Palmer (2008) Noel M O’Boyle and David S Palmer. 2008. Simultaneous
    Feature Selection and Parameter Optimisation Using An Artificial Ant Colony: Case
    Study of Melting Point Prediction. *Chem. Cent. J.* 2, 1 (2008), 1–15.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Boyle 和 Palmer（2008）诺埃尔·M·奥博伊尔 和 大卫·S·帕尔默。2008年。使用人工蚁群进行特征选择与参数优化的同时进行：熔点预测案例研究。*Chem.
    Cent. J.* 2, 1 (2008), 1–15。
- en: 'Olson and Moore (2016) Randal S. Olson and Jason H. Moore. 2016. TPOT: A Tree-based
    Pipeline Optimization Tool for Automating Machine Learning. In *Proc. Int. Conf.
    Mach. Learn.* 151–160.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olson 和 Moore（2016）兰道尔·S·奥尔森 和 詹森·H·摩尔。2016年。TPOT：一种基于树的管道优化工具，用于自动化机器学习。见于
    *Proc. Int. Conf. Mach. Learn.* 151–160。
- en: 'O’Neill et al. (2018) Damien O’Neill, Bing Xue, and Mengjie Zhang. 2018. Co-evolution
    of Novel Tree-Like ANNs and Activation Functions: An Observational Study. In *Proc.
    Aust. Conf. Artif. Intell.* 616–629.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Neill 等（2018）达米安·奥尼尔、薛冰 和 张梦洁。2018年。新型树状 ANN 和激活函数的共进化：一种观察性研究。见于 *Proc. Aust.
    Conf. Artif. Intell.* 616–629。
- en: Oong and Isa (2011) Tatt Hee Oong and Nor Ashidi Mat Isa. 2011. Adaptive Evolutionary
    Artificial Neural Networks for Pattern Classification. *IEEE Trans. Neural Netw.*
    22, 11 (2011), 1823–1836.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oong 和 Isa（2011）黄达禧 和 马特·伊萨。2011年。用于模式分类的自适应进化人工神经网络。*IEEE Trans. Neural Netw.*
    22, 11 (2011), 1823–1836。
- en: Ortego et al. (2020) Patxi Ortego, Alberto Diez-Olivan, Javier Del Ser, and
    Fernando Veiga. 2020. Evolutionary LSTM-FCN Networks for Pattern Classification
    in Industrial Processes. *Swarm Evol. Comput.* 54 (2020), 100650.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ortego 等（2020）帕特西·奥尔特戈、阿尔贝托·迪耶斯-奥利万、哈维尔·德尔·塞尔 和 费尔南多·维加。2020年。用于工业过程模式分类的进化
    LSTM-FCN 网络。*Swarm Evol. Comput.* 54 (2020), 100650。
- en: Peng et al. (2021) Bo Peng, Shuting Wan, Ying Bi, Bing Xue, and Mengjie Zhang.
    2021. Automatic Feature Extraction and Construction Using Genetic Programming
    for Rotating Machinery Fault Diagnosis. *IEEE Trans. Cybern.* 51, 10 (2021), 4909–4923.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2021）彭博、万书婷、毕莹、薛冰和张梦洁。2021年。使用遗传编程进行旋转机械故障诊断的自动特征提取与构建。*IEEE Trans. Cybern.*
    51, 10 (2021), 4909–4923。
- en: Peng et al. (2018) Yiming Peng, Gang Chen, Harman Singh, and Mengjie Zhang.
    2018. NEAT for Large-scale Reinforcement Learning Through Evolutionary Feature
    Learning and Policy Gradient Search. In *Proc. Genetic Evol. Comput. Conf.* 490–497.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2018）彭逸鸣、陈刚、哈曼·辛格 和 张梦洁。2018年。通过进化特征学习和策略梯度搜索的大规模强化学习的 NEAT。见于 *Proc. Genetic
    Evol. Comput. Conf.* 490–497。
- en: Phan et al. (2020) Hai T. Phan, Zechun Liu, Dang The Huynh, Marios Savvides,
    Kwang-Ting Cheng, and Zhiqiang Shen. 2020. Binarizing MobileNet via Evolution-Based
    Searching. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 13417–13426.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phan 等（2020）潘海涛、刘泽春、胡淡和 马里奥斯·萨夫维德斯、郑光廷、沈智强。2020年。通过基于进化的搜索二值化 MobileNet。见于 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* 13417–13426。
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model Compression via Distillation and Quantization. In *Proc. Int. Conf. Learn.
    Represent.* https://arxiv.org/abs/1802.05668.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polino 等（2018）安东尼奥·波利诺、拉兹万·帕斯卡努 和 丹·阿利斯塔赫。2018年。通过蒸馏和量化进行模型压缩。见于 *Proc. Int.
    Conf. Learn. Represent.* https://arxiv.org/abs/1802.05668。
- en: 'Poyatos et al. (2022) Javier Poyatos, Daniel Molina, Aritz Martinez, et al.
    2022. EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning based
    Deep Neural Networks. *arXiv preprint arXiv:2202.03844* (2022).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poyatos 等（2022）哈维尔·波亚托斯、丹尼尔·莫利纳、阿里茨·马丁内斯 等。2022年。EvoPruneDeepTL：一种用于迁移学习的进化剪枝模型，基于深度神经网络。*arXiv
    preprint arXiv:2202.03844* (2022)。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving Language Understanding by Generative Pre-training. (2018),
    https://www.cs.ubc.ca/ amuham01/LING530/papers/radford2018improving.pdf.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    等. 2018. 通过生成预训练提高语言理解。 (2018), https://www.cs.ubc.ca/amuham01/LING530/papers/radford2018improving.pdf.
- en: 'Rapaport et al. (2019) Elad Rapaport, Oren Shriki, and Rami Puzis. 2019. EEGNAS:
    Neural Architecture Search for Electroencephalography Data Analysis and Decoding.
    In *Proc. Int. Joint Conf. Artif. Intell.* 3–20.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rapaport 等 (2019) Elad Rapaport, Oren Shriki, 和 Rami Puzis. 2019. EEGNAS: 用于脑电图数据分析和解码的神经架构搜索。见
    *国际人工智能联合会议论文集* 3–20.'
- en: Rashid et al. (2020) ANM Bazlur Rashid, Mohiuddin Ahmed, Leslie F Sikos, and
    Paul Haskell-Dowland. 2020. Cooperative Co-Evolution for Feature Selection in
    Big Data With Random Feature Grouping. *J. Big Data* 7, 1 (2020), 1–42.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid 等 (2020) ANM Bazlur Rashid, Mohiuddin Ahmed, Leslie F Sikos, 和 Paul Haskell-Dowland.
    2020. 在大数据中使用随机特征分组的特征选择合作共进。*大数据杂志* 7, 1 (2020), 1–42.
- en: 'Rawal and Miikkulainen (2018) Aditya Rawal and Risto Miikkulainen. 2018. From
    Nodes to Networks: Evolving Recurrent Neural Networks. *arXiv preprint arXiv:1803.04439*
    (2018).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawal 和 Miikkulainen (2018) Aditya Rawal 和 Risto Miikkulainen. 2018. 从节点到网络：进化递归神经网络。*arXiv
    预印本 arXiv:1803.04439* (2018).
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    2019. Regularized Evolution for Image Classifier Architecture Search. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol. 33\. 4780–4789.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real 等 (2019) Esteban Real, Alok Aggarwal, Yanping Huang, 和 Quoc V Le. 2019.
    用于图像分类器架构搜索的正则化进化。见 *AAAI人工智能会议论文集* 第33卷 4780–4789.
- en: Real et al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. 2017. Large-Scale
    Evolution of Image Classifiers. In *Proc. Int. Conf. Mach. Learn.* 2902–2911.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real 等 (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka
    Leon Suematsu, Jie Tan, Quoc V. Le, 和 Alexey Kurakin. 2017. 大规模图像分类器进化。见 *国际机器学习会议论文集*
    2902–2911.
- en: Refahi et al. (2020) Mohammad Saleh Refahi, A Mir, and Jalal A Nasiri. 2020.
    A Novel Fusion Based on the Evolutionary Features for Protein Fold Recognition
    Using Support Vector Machines. *Sci. Rep.* 10, 1 (2020), 1–13.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refahi 等 (2020) Mohammad Saleh Refahi, A Mir, 和 Jalal A Nasiri. 2020. 基于进化特征的新型融合方法用于蛋白质折叠识别，使用支持向量机。*科学报告*
    10, 1 (2020), 1–13.
- en: 'Ren et al. (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, and
    Zhihui Li. 2021. A Comprehensive Survey of Neural Architecture Search: Challenges
    and Solutions. *ACM Comput. Surv.* 54, 4 (2021), 1–34.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, 和 Zhihui Li.
    2021. 神经架构搜索的综合调查：挑战与解决方案。*ACM计算机调查* 54, 4 (2021), 1–34.
- en: Roberts and Claridge (2005) Mark E. Roberts and Ela Claridge. 2005. A Multistage
    Approach to Cooperatively Coevolving Feature Construction and Object Detection.
    In *Proc. Appl. Evol. Comput.* 396–406.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts 和 Claridge (2005) Mark E. Roberts 和 Ela Claridge. 2005. 合作共进的特征构建与物体检测的多阶段方法。见
    *应用进化计算会议论文集* 396–406.
- en: Rostami and Neri (2016) Shahin Rostami and Ferrante Neri. 2016. Covariance Matrix
    Adaptation Pareto Archived Evolution Strategy With Hypervolume-Sorted Adaptive
    Grid Algorithm. *Integr. Comput. Aided. Eng.* 23, 4 (2016), 313–329.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rostami 和 Neri (2016) Shahin Rostami 和 Ferrante Neri. 2016. 协方差矩阵适应帕累托归档进化策略与超体积排序自适应网格算法。*集成计算机辅助工程*
    23, 4 (2016), 313–329.
- en: Rumelhart et al. (1985) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1985. *Learning Internal Representations by Error Propagation*. Technical Report.
    California Univ San Diego La Jolla Inst for Cognitive Science.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等 (1985) David E Rumelhart, Geoffrey E Hinton, 和 Ronald J Williams.
    1985. *通过误差传播学习内部表示*。技术报告。加州大学圣地亚哥分校认知科学研究所.
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning Representations by Back-propagating Errors. *Nature* 323, 6088
    (1986), 533–536.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等 (1986) David E Rumelhart, Geoffrey E Hinton, 和 Ronald J Williams.
    1986. 通过误差反向传播学习表示。*自然* 323, 6088 (1986), 533–536.
- en: 'Rundo et al. (2019) Leonardo Rundo, Andrea Tangherloni, Marco S Nobile, Carmelo
    Militello, Daniela Besozzi, Giancarlo Mauri, and Paolo Cazzaniga. 2019. MedGA:
    A Novel Evolutionary Method for Image Enhancement in Medical Imaging Systems.
    *Expert Syst. Appl.* 119 (2019), 387–399.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rundo 等 (2019) Leonardo Rundo, Andrea Tangherloni, Marco S Nobile, Carmelo
    Militello, Daniela Besozzi, Giancarlo Mauri, 和 Paolo Cazzaniga. 2019. MedGA: 一种用于医学成像系统中图像增强的新型进化方法。*专家系统应用*
    119 (2019), 387–399.'
- en: Samala et al. (2018) Ravi K. Samala, Heang-Ping Chan, Lubomir M. Hadjiiski,
    Mark A. Helvie, Caleb D. Richter, and Kenny H. Cha. 2018. Evolutionary Pruning
    of Transfer Learned Deep Convolutional Neural Network For Breast Cancer Diagnosis
    In Digital Breast Tomosynthesis. *Phys. Med. Biol.* 63, 9 (2018), 095005.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samala et al. (2018) Ravi K. Samala, Heang-Ping Chan, Lubomir M. Hadjiiski,
    Mark A. Helvie, Caleb D. Richter, 和 Kenny H. Cha. 2018. 用于数字乳腺断层扫描的迁移学习深度卷积神经网络的进化剪枝。*Phys.
    Med. Biol.* 63, 9 (2018), 095005。
- en: 'Santra et al. (2021) Santanu Santra, Jun-Wei Hsieh, and Chi-Fang Lin. 2021.
    Gradient Descent Effects on Differential Neural Architecture Search: A Survey.
    *IEEE Access* 9 (2021), 89602–89618.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santra et al. (2021) Santanu Santra, Jun-Wei Hsieh, 和 Chi-Fang Lin. 2021. 梯度下降对差分神经架构搜索的影响：一项调查。*IEEE
    Access* 9 (2021), 89602–89618。
- en: Sapra and Pimentel (2020) Dolly Sapra and Andy D Pimentel. 2020. Constrained
    Evolutionary Piecemeal Training to Design Convolutional Neural Networks. In *Proc.
    Int. Conf. Industr., Eng. and Other Appl. of App. Intell. Syst.* 709–721.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sapra and Pimentel (2020) Dolly Sapra 和 Andy D Pimentel. 2020. 受限进化分段训练设计卷积神经网络。在
    *Proc. Int. Conf. Industr., Eng. and Other Appl. of App. Intell. Syst.* 中。709–721。
- en: Schorn et al. (2020) Christoph Schorn, Thomas Elsken, Sebastian Vogel, and Armin
    Runge. 2020. Automated Design Of Error-Resilient and Hardware-Efficient Deep Neural
    Networks. *Neural. Comput. Appl.* 32, 24 (2020), 18327–18345.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schorn et al. (2020) Christoph Schorn, Thomas Elsken, Sebastian Vogel, 和 Armin
    Runge. 2020. 自动设计错误弹性和硬件高效的深度神经网络。*Neural. Comput. Appl.* 32, 24 (2020), 18327–18345。
- en: Sciuto et al. (2020) Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Cristian
    Musat, and Mathieu Salzmann. 2020. Evaluating the Search Phase of Neural Architecture
    Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.08142.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sciuto et al. (2020) Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Cristian
    Musat, 和 Mathieu Salzmann. 2020. 评估神经架构搜索的搜索阶段。在 *Proc. Int. Conf. Learn. Represent.*
    中。https://arxiv.org/abs/1902.08142。
- en: 'Shafti and Pérez (2008) Leila Shila Shafti and E. Islas Pérez. 2008. Data Reduction
    by Genetic Algorithms and Non-Algebraic Feature Construction: A Case Study. *Proc.
    Int. Conf. Hybri. Intell. Syst.* (2008), 573–578.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafti and Pérez (2008) Leila Shila Shafti 和 E. Islas Pérez. 2008. 通过遗传算法和非代数特征构造的数据减少：案例研究。在
    *Proc. Int. Conf. Hybri. Intell. Syst.* 中。(2008), 573–578。
- en: Shakya et al. (2021) Pratistha Shakya, Eamonn Kennedy, Christopher Rose, and
    Jacob K. Rotein. 2021. High-Dimensional Time Series Feature Extraction for Low-Cost
    Machine Olfaction. *IEEE Sens. J.* 21, 3 (2021), 2495–2504.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shakya et al. (2021) Pratistha Shakya, Eamonn Kennedy, Christopher Rose, 和 Jacob
    K. Rotein. 2021. 低成本机器嗅觉的高维时间序列特征提取。*IEEE Sens. J.* 21, 3 (2021), 2495–2504。
- en: Shang et al. (2022) Haopu Shang, Jia-Liang Wu, Wenjing Hong, and Chao Qian.
    2022. Neural Network Pruning by Cooperative Coevolution. *arXiv preprint arXiv:2204.05639*
    (2022).
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang et al. (2022) Haopu Shang, Jia-Liang Wu, Wenjing Hong, 和 Chao Qian. 2022.
    通过合作共进化进行神经网络剪枝。*arXiv preprint arXiv:2204.05639* (2022)。
- en: Shen et al. (2019) Mingzhu Shen, Kai Han, Chunjing Xu, and Yunhe Wang. 2019.
    Searching for Accurate Binary Neural Architectures. In *Proc. IEEE Int. Conf.
    Comput. Vis.* 2041–2044.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2019) Mingzhu Shen, Kai Han, Chunjing Xu, 和 Yunhe Wang. 2019. 寻找准确的二进制神经架构。在
    *Proc. IEEE Int. Conf. Comput. Vis.* 中。2041–2044。
- en: Siems et al. (2020) Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
    Margret Keuper, and Frank Hutter. 2020. NAS-bench-301 and The Case for Surrogate
    Benchmarks for Neural Architecture Search. *arXiv preprint arXiv:2008.09777* (2020).
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siems et al. (2020) Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,
    Margret Keuper, 和 Frank Hutter. 2020. NAS-bench-301 和神经架构搜索的替代基准的案例。*arXiv preprint
    arXiv:2008.09777* (2020)。
- en: Sikdar et al. (2012) Utpal Kumar Sikdar, Asif Ekbal, and Sriparna Saha. 2012.
    Differential Evolution Based Feature Selection and Classifier Ensemble for Named
    Entity Recognition. In *Proc. COLING*. 2475–2490.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sikdar et al. (2012) Utpal Kumar Sikdar, Asif Ekbal, 和 Sriparna Saha. 2012.
    基于差分进化的特征选择和分类器集成用于命名实体识别。在 *Proc. COLING* 中。2475–2490。
- en: Smith and Jin (2014) Christopher Smith and Yaochu Jin. 2014. Evolutionary Multi-objective
    Generation of Recurrent Neural Network Ensembles for Time Series Prediction. *Neurocomputing*
    143 (2014), 302–311.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith and Jin (2014) Christopher Smith 和 Yaochu Jin. 2014. 用于时间序列预测的进化多目标生成递归神经网络集成。*Neurocomputing*
    143 (2014), 302–311。
- en: 'Socha and Blum (2007) Krzysztof Socha and Christian Blum. 2007. An Ant Colony
    Optimization Algorithm for Continuous Optimization: Application to Feed-forward
    Neural Network Training. *Neural. Comput. Appl.* 16, 3 (2007), 235–247.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socha and Blum (2007) Krzysztof Socha 和 Christian Blum. 2007. 一种用于连续优化的蚁群优化算法：应用于前馈神经网络训练。*Neural.
    Comput. Appl.* 16, 3 (2007), 235–247。
- en: Song et al. (2020) Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, and
    Yunhe Wang. 2020. Efficient Residual Dense Block Search for Image Super-Resolution.
    In *Proc. AAAI Conf. Artif. Intell.*, Vol. 34\. 12007–12014.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2020）Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu 和 Yunhe Wang.
    2020. 高效残差密集块搜索用于图像超分辨率。见 *Proc. AAAI Conf. Artif. Intell.*, 第 34 卷. 12007–12014.
- en: Song (2021) Xin Song. 2021. Intelligent English Translation System Based on
    Evolutionary Multi-Objective Optimization Algorithm. *J. Intell. Fuzzy Syst.*
    40 (2021), 6327–6337.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song（2021）Xin Song. 2021. 基于进化多目标优化算法的智能英语翻译系统。*J. Intell. Fuzzy Syst.* 40 (2021),
    6327–6337.
- en: Stanley and Miikkulainen (2002) Kenneth O Stanley and Risto Miikkulainen. 2002.
    Evolving Neural Networks Through Augmenting Topologies. *Evol. Comput.* 10, 2
    (2002), 99–127.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanley 和 Miikkulainen（2002）Kenneth O Stanley 和 Risto Miikkulainen. 2002. 通过扩展拓扑演化神经网络。*Evol.
    Comput.* 10, 2 (2002), 99–127.
- en: Sun et al. (2021) Yanan Sun, Xian Sun, Yuhan Fang, Gary G. Yen, and Yuqiao Liu.
    2021. A Novel Training Protocol for Performance Predictors of Evolutionary Neural
    Architecture Search Algorithms. *IEEE Trans. Evol. Comput.* 25, 3 (2021), 524–536.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2021）Yanan Sun, Xian Sun, Yuhan Fang, Gary G. Yen 和 Yuqiao Liu. 2021.
    用于进化神经网络架构搜索算法性能预测器的新训练协议。*IEEE Trans. Evol. Comput.* 25, 3 (2021), 524–536.
- en: Sun et al. (2019a) Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen,
    and Mengjie Zhang. 2019a. Surrogate-Assisted Evolutionary Deep Learning Using
    an End-To-End Random Forest-Based Performance Predictor. *IEEE Trans. Evol. Comput.*
    24, 2 (2019), 350–364.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019a）Yanan Sun, Handing Wang, Bing Xue, Yaochu Jin, Gary G Yen 和 Mengjie
    Zhang. 2019a. 使用端到端随机森林性能预测器的代理辅助进化深度学习。*IEEE Trans. Evol. Comput.* 24, 2 (2019),
    350–364.
- en: Sun et al. (2019b) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen. 2019b.
    Completely Automated CNN Architecture Design Based on Blocks. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 4 (2019), 1242–1254.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019b）Yanan Sun, Bing Xue, Mengjie Zhang 和 Gary G Yen. 2019b. 基于块的完全自动化
    CNN 架构设计。*IEEE Trans. Neural Netw. Learn. Syst.* 31, 4 (2019), 1242–1254.
- en: Sun et al. (2019c) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen. 2019c.
    Evolving Deep Convolutional Neural Networks for Image Classification. *IEEE Trans.
    Evol. Comput.* 24, 2 (2019), 394–407.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019c）Yanan Sun, Bing Xue, Mengjie Zhang 和 Gary G Yen. 2019c. 为图像分类演化深度卷积神经网络。*IEEE
    Trans. Evol. Comput.* 24, 2 (2019), 394–407.
- en: Sun et al. (2019d) Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G. Yen. 2019d.
    A Particle Swarm Optimization-Based Flexible Convolutional Autoencoder for Image
    Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 30, 8 (2019), 2295–2309.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019d）Yanan Sun, Bing Xue, Mengjie Zhang 和 Gary G. Yen. 2019d. 基于粒子群优化的灵活卷积自编码器用于图像分类。*IEEE
    Trans. Neural Netw. Learn. Syst.* 30, 8 (2019), 2295–2309.
- en: Sun et al. (2020) Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Jiancheng
    Lv. 2020. Automatically Designing CNN Architectures Using the Genetic Algorithm
    for Image Classification. *IEEE Trans. Cybern.* 50, 9 (2020), 3840–3854.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2020）Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen 和 Jiancheng Lv. 2020.
    使用遗传算法自动设计 CNN 架构用于图像分类。*IEEE Trans. Cybern.* 50, 9 (2020), 3840–3854.
- en: Swaminathan et al. (2020) Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan,
    and Frédéric Andrès. 2020. Sparse Low Rank Factorization for Deep Neural Network
    compression. *Neurocomputing* 398 (2020), 185–196.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swaminathan 等（2020）Sridhar Swaminathan, Deepak Garg, Rajkumar Kannan 和 Frédéric
    Andrès. 2020. 用于深度神经网络压缩的稀疏低秩分解。*Neurocomputing* 398 (2020), 185–196.
- en: Tanaka et al. (2016) Tomohiro Tanaka, Takafumi Moriya, and Takahiro Shinozaki.
    2016. Evolutionary Optimization of Long Short-Term Memory Neural Network Language
    Model. *J. Acoust. Soc. Am.* 140, 4 (2016), 3062–3062.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka 等（2016）Tomohiro Tanaka, Takafumi Moriya 和 Takahiro Shinozaki. 2016. 长短期记忆神经网络语言模型的进化优化。*J.
    Acoust. Soc. Am.* 140, 4 (2016), 3062–3062.
- en: Tang et al. (2019) Yajiao Tang, Junkai Ji, Yulin Zhu, Shangce Gao, Zheng Tang,
    and Yuki Todo. 2019. A Differential Evolution-Oriented Pruning Neural Network
    Model for Bankruptcy Prediction. In *Complexity*, Vol. 2019. 8682124:1–8682124:21.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等（2019）Yajiao Tang, Junkai Ji, Yulin Zhu, Shangce Gao, Zheng Tang 和 Yuki
    Todo. 2019. 面向差分进化的破产预测神经网络模型。在 *Complexity*, 第 2019 卷. 8682124:1–8682124:21.
- en: Tariq et al. (2018) Hassan Tariq, Elf Eldridge, and Ian Welch. 2018. An Efficient
    Approach for Feature Construction of High-Dimensional Microarray Data By Random
    Projections. *PLoS ONE* 13, 4 (2018), e0196385.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tariq 等（2018）Hassan Tariq, Elf Eldridge 和 Ian Welch. 2018. 通过随机投影对高维微阵列数据进行特征构建的高效方法。*PLoS
    ONE* 13, 4 (2018), e0196385.
- en: 'Telikani et al. (2021) Akbar Telikani, Amirhessam Tahmassebi, Wolfgang Banzhaf,
    and Amir H Gandomi. 2021. Evolutionary Machine Learning: A Survey. *ACM Comput.
    Surv.* 54, 8 (2021), 1–35.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telikani 等 (2021) Akbar Telikani、Amirhessam Tahmassebi、Wolfgang Banzhaf 和 Amir
    H Gandomi。2021年。进化机器学习：综述。*ACM Comput. Surv.* 54, 8 (2021), 1–35。
- en: 'Teller and Veloso (1996) Astro Teller and Manuela Veloso. 1996. PADO: A New
    Learning Architecture for Object Recognition. *Symbolic visual learn.* (1996),
    81–116.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teller 和 Veloso (1996) Astro Teller 和 Manuela Veloso。1996年。PADO：一种用于物体识别的新学习架构。*Symbolic
    visual learn.* (1996), 81–116。
- en: Tian et al. (2019) Haiman Tian, ShuChing Chen, MeiLing Shyu, and Stuart Harvey
    Rubin. 2019. Automated Neural Network Construction with Similarity Sensitive Evolutionary
    Algorithms. In *Proc. IEEE Int. Conf. Inf. Reuse Integr. Data Sci.* 283–290.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2019) Haiman Tian、ShuChing Chen、MeiLing Shyu 和 Stuart Harvey Rubin。2019年。具有相似性敏感进化算法的自动神经网络构建。在
    *Proc. IEEE Int. Conf. Inf. Reuse Integr. Data Sci.* 283–290。
- en: Tran et al. (2018) Binh Tran, Bing Xue, and Mengjie Zhang. 2018. A New Representation
    in PSO for Discretization-Based Feature Selection. *IEEE Trans. Cybern.* 48, 6
    (2018), 1733–1746.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 (2018) Binh Tran、Bing Xue 和 Mengjie Zhang。2018年。在 PSO 中的新表示方法用于基于离散化的特征选择。*IEEE
    Trans. Cybern.* 48, 6 (2018), 1733–1746。
- en: Tran et al. (2019) Binh Tran, Bing Xue, and Mengjie Zhang. 2019. Genetic Programming
    for Multiple-Feature Construction on High-Dimensional Classification. *Pattern
    Recognit.* 93 (2019), 404–417.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 (2019) Binh Tran、Bing Xue 和 Mengjie Zhang。2019年。用于高维分类的多特征构建的遗传编程。*Pattern
    Recognit.* 93 (2019), 404–417。
- en: Tran et al. (2016) Binh Tran, Mengjie Zhang, and Bing Xue. 2016. Multiple Feature
    Construction in Classification on High-Dimensional Data Using GP. In *IEEE Symposium
    Series on Computational Intelligence*. 1–8.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 (2016) Binh Tran、Mengjie Zhang 和 Bing Xue。2016年。在高维数据分类中使用 GP 进行多特征构建。在
    *IEEE Symposium Series on Computational Intelligence*。1–8。
- en: Vafaie and De Jong (1998) Haleh Vafaie and Kenneth De Jong. 1998. Feature Space
    Transformation Using Genetic Algorithms. *IEEE Intell. Syst. Appli.* 13, 2 (1998),
    57–65.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vafaie 和 De Jong (1998) Haleh Vafaie 和 Kenneth De Jong。1998年。利用遗传算法进行特征空间转换。*IEEE
    Intell. Syst. Appli.* 13, 2 (1998), 57–65。
- en: Vargas-Hákim et al. (2022) Gustavo A Vargas-Hákim, Efrén Mezura-Montes, and
    Héctor-Gabriel Acosta-Mesa. 2022. A Review on Convolutional Neural Networks Encodings
    for Neuroevolution. *IEEE Trans. Evol. Comput.* 26, 1 (2022), 12–27.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vargas-Hákim 等 (2022) Gustavo A Vargas-Hákim、Efrén Mezura-Montes 和 Héctor-Gabriel
    Acosta-Mesa。2022年。关于用于神经进化的卷积神经网络编码的综述。*IEEE Trans. Evol. Comput.* 26, 1 (2022),
    12–27。
- en: Vieira et al. (2013) Susana M Vieira, Luís F Mendonça, Goncalo J Farinha, and
    João MC Sousa. 2013. Modified Binary PSO for Feature Selection Using SVM Applied
    to Mortality Prediction of Septic Patients. *Appl. Soft Comput.* 13, 8 (2013),
    3494–3504.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieira 等 (2013) Susana M Vieira、Luís F Mendonça、Goncalo J Farinha 和 João MC
    Sousa。2013年。使用 SVM 的改进二进制 PSO 特征选择应用于脓毒症患者的死亡预测。*Appl. Soft Comput.* 13, 8 (2013),
    3494–3504。
- en: Wang et al. (2021b) Bin Wang, Wenbin Pei, Bing Xue, and Mengjie Zhang. 2021b.
    Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks
    in Image Classification. In *Proc. Genetic Evol. Comput. Conf.* 173–174.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021b) Bin Wang、Wenbin Pei、Bing Xue 和 Mengjie Zhang。2021b年。为图像分类中的深度神经网络演化局部可解释模型无关解释。在
    *Proc. Genetic Evol. Comput. Conf.* 173–174。
- en: 'Wang et al. (2020b) Bin Wang, Bing Xue, and Mengjie Zhang. 2020b. Particle
    Swarm Optimization for Evolving Deep Convolutional Neural Networks for Image Classification:
    Single-and Multi-objective Approaches. In *Deep Neural Evolution*. Springer, 155–184.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020b) Bin Wang、Bing Xue 和 Mengjie Zhang。2020b年。用于图像分类的深度卷积神经网络的粒子群优化：单目标和多目标方法。在
    *Deep Neural Evolution*。Springer，155–184。
- en: Wang et al. (2020c) Bin Wang, Bing Xue, and Mengjie Zhang. 2020c. Particle Swarm
    Optimization for Evolving Deep Neural Networks for Image Classification By Evolving
    and Stacking Transferable Blocks. In *Proc. IEEE Congr. Evol. Comput.* 1–8.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020c) Bin Wang、Bing Xue 和 Mengjie Zhang。2020c年。通过演化和堆叠可迁移块进行图像分类的深度神经网络的粒子群优化。在
    *Proc. IEEE Congr. Evol. Comput.* 1–8。
- en: Wang et al. (2021c) Bin Wang, Bing Xue, and Mengjie Zhang. 2021c. Surrogate-Assisted
    Particle Swarm Optimization for Evolving Variable-Length Transferable Blocks for
    Image Classification. *IEEE Trans. Neural Netw. Learn. Syst.* 33, 8 (2021), 3727–3740.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021c) Bin Wang、Bing Xue 和 Mengjie Zhang。2021c年。用于图像分类的可变长度可迁移块的代理辅助粒子群优化。*IEEE
    Trans. Neural Netw. Learn. Syst.* 33, 8 (2021), 3727–3740。
- en: 'Wang et al. (2020a) Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng
    Zhu, Chuang Gan, and Song Han. 2020a. HAT: Hardware-Aware Transformers for Efficient
    Natural Language Processing. In *Proc. Assoc. Comput. Linguist.* 7675–7688.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu,
    Chuang Gan, 和 Song Han. 2020a. HAT：针对高效自然语言处理的硬件感知变换器。在*Proc. Assoc. Comput. Linguist.*
    7675–7688。
- en: 'Wang et al. (2020d) Xiao-han Wang, Yong Zhang, and Xiao-yan Sun. 2020d. Multi-Objective
    Feature Selection Based on Artificial Bee Colony: An Acceleration Approach With
    Variable Sample Size. *Appl. Soft Comput.* 88 (2020), 106041.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020d) Xiao-han Wang, Yong Zhang, 和 Xiao-yan Sun. 2020d. 基于人工蜂群的多目标特征选择：一种具有可变样本量的加速方法。*Appl.
    Soft Comput.* 88 (2020), 106041。
- en: Wang et al. (2018) Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, and Dacheng Tao.
    2018. Towards Evolutionary Compression. In *Proc. ACM SIGKDD Int. Conf. Knowl.
    Discov. & Data Min.* 2476–2485.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, 和 Dacheng Tao.
    2018. 朝着进化压缩的方向。在*Proc. ACM SIGKDD Int. Conf. Knowl. Discov. & Data Min.* 2476–2485。
- en: Wang et al. (2021a) Zhehui Wang, Tao Luo, Miqing Li, Joey Tianyi Zhou, Rick
    Siow Mong Goh, and Liangli Zhen. 2021a. Evolutionary Multi-Objective Model Compression
    for Deep Neural Networks. *IEEE Comput. Intell. Mag.* 16, 3 (2021), 10–21.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Zhehui Wang, Tao Luo, Miqing Li, Joey Tianyi Zhou, Rick
    Siow Mong Goh, 和 Liangli Zhen. 2021a. 用于深度神经网络的进化多目标模型压缩。*IEEE Comput. Intell.
    Mag.* 16, 3 (2021), 10–21。
- en: Wen and Xu (2011) Yun Wen and Hua Xu. 2011. A Cooperative Coevolution-Based
    Pittsburgh Learning Classifier System Embedded With Memetic Feature Selection.
    In *Proc. IEEE Congr. Evol. Comput.* 2415–2422.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 和 Xu (2011) Yun Wen 和 Hua Xu. 2011. 一种基于协作共进化的匹兹堡学习分类器系统，嵌入了启发式特征选择。在*Proc.
    IEEE Congr. Evol. Comput.* 2415–2422。
- en: 'White et al. (2021) Colin White, Willie Neiswanger, and Yash Savani. 2021.
    BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture
    Search. In *Proc. AAAI Conf. Artif. Intell.*, Vol. 35\. 10293–10301.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. (2021) Colin White, Willie Neiswanger, 和 Yash Savani. 2021. BANANAS：用于神经架构搜索的贝叶斯优化与神经架构。在*Proc.
    AAAI Conf. Artif. Intell.*, 第35卷. 10293–10301。
- en: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J
    Barezi, and Pascale Fung. 2019. On the Effectiveness of Low-rank Matrix Factorization
    for LSTM Model Compression. *arXiv preprint arXiv:1908.09982* (2019).
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J
    Barezi, 和 Pascale Fung. 2019. 低秩矩阵分解在LSTM模型压缩中的有效性。*arXiv preprint arXiv:1908.09982*
    (2019)。
- en: Wu et al. (2021b) Min Wu, Wanjuan Su, Luefeng Chen, and Zhentao Liu. 2021b.
    Weight-Adapted Convolution Neural Network for Facial Expression Recognition in
    Human-Robot Interaction. *IEEE Trans. Syst. Man Cybern.* 51, 3 (2021), 1473–1484.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021b) Min Wu, Wanjuan Su, Luefeng Chen, 和 Zhentao Liu. 2021b. 面向人机交互的面部表情识别的权重自适应卷积神经网络。*IEEE
    Trans. Syst. Man Cybern.* 51, 3 (2021), 1473–1484。
- en: Wu et al. (2021a) Tao Wu, Xiaoyang Li, Deyun Zhou, Na Li, and Jiao Shi. 2021a.
    Differential Evolution Based Layer-Wise Weight Pruning for Compressing Deep Neural
    Networks. *Sens.* 21, 3 (2021), 880.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021a) Tao Wu, Xiaoyang Li, Deyun Zhou, Na Li, 和 Jiao Shi. 2021a.
    基于差分进化的逐层权重剪枝用于压缩深度神经网络。*Sens.* 21, 3 (2021), 880。
- en: Wu et al. (2019) Tao Wu, Jiao Shi, Deyun Zhou, Yu Lei, and Maoguo Gong. 2019.
    A Multi-objective Particle Swarm Optimization for Neural Networks Pruning. In
    *Proc. IEEE Congr. Evol. Comput.* 570–577.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Tao Wu, Jiao Shi, Deyun Zhou, Yu Lei, 和 Maoguo Gong. 2019.
    用于神经网络剪枝的多目标粒子群优化。在*Proc. IEEE Congr. Evol. Comput.* 570–577。
- en: Wu et al. (2020) Xiang Wu, Ran He, Yibo Hu, and Zhenan Sun. 2020. Learning an
    Evolutionary Embedding via Massive Knowledge Distillation. *Int. J. Comput. Vis.*
    128, 8 (2020), 1–18.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) Xiang Wu, Ran He, Yibo Hu, 和 Zhenan Sun. 2020. 通过大规模知识蒸馏学习进化嵌入。*Int.
    J. Comput. Vis.* 128, 8 (2020), 1–18。
- en: 'Xie et al. (2022a) Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu,
    Zhengsu Chen, Lanfei Wang, Anxiang Xiao, Jianlong Chang, Xiaopeng Zhang, and Qi
    Tian. 2022a. Weight-Sharing Neural Architecture Search: A Battle to Shrink the
    Optimization Gap. *ACM Comput. Surv.* 54, 9 (2022), 1–37.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022a) Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu,
    Zhengsu Chen, Lanfei Wang, Anxiang Xiao, Jianlong Chang, Xiaopeng Zhang, 和 Qi
    Tian. 2022a. 权重共享神经架构搜索：缩小优化差距的战斗。*ACM Comput. Surv.* 54, 9 (2022), 1–37。
- en: Xie and Yuille (2017) Lingxi Xie and Alan Yuille. 2017. Genetic CNN. In *Proc.
    IEEE Int. Conf. Comput. Vis.* 1379–1388.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 和 Yuille (2017) Lingxi Xie 和 Alan Yuille. 2017. 遗传卷积神经网络。在*Proc. IEEE Int.
    Conf. Comput. Vis.* 1379–1388。
- en: 'Xie et al. (2022b) Xiangning Xie, Yuqiao Liu, Yanan Sun, Gary G. Yen, Bing
    Xue, and Mengjie Zhang. 2022b. BenchENAS: A Benchmarking Platform for Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* (2022). [https://doi.org/10.1109/TEVC.2022.3147526](https://doi.org/10.1109/TEVC.2022.3147526)'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022b) Xiangning Xie, Yuqiao Liu, Yanan Sun, Gary G. Yen, Bing Xue,
    和 Mengjie Zhang. 2022b. BenchENAS：用于进化神经架构搜索的基准平台。*IEEE Trans. Evol. Comput.*
    (2022). [https://doi.org/10.1109/TEVC.2022.3147526](https://doi.org/10.1109/TEVC.2022.3147526)
- en: 'Xu et al. (2021) Ke Xu, Dezheng Zhang, Jianjing An, Li Liu, Lingzhi Liu, and
    Dong Wang. 2021. GenExp: Multi-objective Pruning for Deep Neural Network based
    on Genetic Algorithm. *Neurocomputing* 451 (2021), 81–94.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) Ke Xu, Dezheng Zhang, Jianjing An, Li Liu, Lingzhi Liu, 和 Dong
    Wang. 2021. GenExp：基于遗传算法的深度神经网络多目标剪枝。*Neurocomputing* 451 (2021), 81–94。
- en: Xue et al. (2012) Bing Xue, Mengjie Zhang, and Will N Browne. 2012. Multi-Objective
    Particle Swarm Optimization (PSO) for Feature Selection. In *Proc. Genetic Evol.
    Comput. Conf.* 81–88.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2012) Bing Xue, Mengjie Zhang, 和 Will N Browne. 2012. 用于特征选择的多目标粒子群优化（PSO）。见于
    *Proc. Genetic Evol. Comput. Conf.* 81–88。
- en: Xue et al. (2015) Bing Xue, Mengjie Zhang, Will N Browne, and Xin Yao. 2015.
    A Survey on Evolutionary Computation Approaches to Feature Selection. *IEEE Trans.
    Evol. Comput.* 20, 4 (2015), 606–626.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2015) Bing Xue, Mengjie Zhang, Will N Browne, 和 Xin Yao. 2015. 关于进化计算方法在特征选择中的应用的综述。*IEEE
    Trans. Evol. Comput.* 20, 4 (2015), 606–626。
- en: Xue et al. (2013) Bing Xue, Mengjie Zhang, Yan Dai, and Will N Browne. 2013.
    PSO for Feature Construction and Binary Classification. In *Proc. Genetic Evol.
    Comput. Conf.* 137–144.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue et al. (2013) Bing Xue, Mengjie Zhang, Yan Dai, 和 Will N Browne. 2013. 用于特征构建和二分类的PSO。见于
    *Proc. Genetic Evol. Comput. Conf.* 137–144。
- en: Yang et al. (2019) Chuanguang Yang, Zhulin An, Chao Li, Boyu Diao, and Yongjun
    Xu. 2019. Multi-objective Pruning for CNNs Using Genetic Algorithm. In *Proc.
    Int. Conf. Artif. Neural Netw.* 299–305.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Chuanguang Yang, Zhulin An, Chao Li, Boyu Diao, 和 Yongjun
    Xu. 2019. 使用遗传算法的CNN多目标剪枝。见于 *Proc. Int. Conf. Artif. Neural Netw.* 299–305。
- en: Yang et al. (2021) Shangshang Yang, Ye Tian, Cheng He, Xingyi Zhang, Kay Chen
    Tan, and Yaochu Jin. 2021. A Gradient-Guided Evolutionary Approach to Training
    Deep Neural Networks. *IEEE Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3061630](https://doi.org/10.1109/TNNLS.2021.3061630)
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021) Shangshang Yang, Ye Tian, Cheng He, Xingyi Zhang, Kay Chen
    Tan, 和 Yaochu Jin. 2021. 一种梯度引导的进化方法用于训练深度神经网络。*IEEE Trans. Neural Netw. Learn.
    Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3061630](https://doi.org/10.1109/TNNLS.2021.3061630)
- en: 'Yang et al. (2020) Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, and Chao
    Xu. 2020. CARS: Continuous Evolution for Efficient Neural Architecture Search.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 1826–1835.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020) Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, 和 Chao
    Xu. 2020. CARS：用于高效神经架构搜索的连续进化。见于 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*
    1826–1835。
- en: 'Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, and
    Yu-Feng Li. 2018. Taking Human out of Learning Applications: A Survey on Automated
    Machine Learning. *arXiv preprint arXiv:1810.13306* (2018).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, 和
    Yu-Feng Li. 2018. 从学习应用中去除人为因素：自动化机器学习的综述。*arXiv preprint arXiv:1810.13306* (2018)。
- en: Yao (1993) Xin Yao. 1993. A Review of Evolutionary Artificial Neural Networks.
    *Int. J. Intell. Syst.* 8, 4 (1993), 539–567.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao (1993) Xin Yao. 1993. 进化人工神经网络的综述。*Int. J. Intell. Syst.* 8, 4 (1993), 539–567。
- en: Yao and Liu (1996) Xin Yao and Yong Liu. 1996. Ensemble Structure of Evolutionary
    Artificial Neural Networks. In *Proc. Genetic Evol. Comput. Conf.* 659–664.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao and Liu (1996) Xin Yao 和 Yong Liu. 1996. 进化人工神经网络的集成结构。见于 *Proc. Genetic
    Evol. Comput. Conf.* 659–664。
- en: 'Ying et al. (2019) Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen,
    Kevin P. Murphy, and Frank Hutter. 2019. NAS-Bench-101: Towards Reproducible Neural
    Architecture Search. In *Proc. Int. Conf. Learn. Represent.* https://arxiv.org/abs/1902.09635.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying et al. (2019) Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen,
    Kevin P. Murphy, 和 Frank Hutter. 2019. NAS-Bench-101：迈向可重复的神经架构搜索。见于 *Proc. Int.
    Conf. Learn. Represent.* https://arxiv.org/abs/1902.09635。
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing
    [Review Article]. *IEEE Comput. Intell. Mag.* 13, 3 (2018), 55–75.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, 和 Erik Cambria.
    2018. 基于深度学习的自然语言处理的最新趋势 [综述文章]。*IEEE Comput. Intell. Mag.* 13, 3 (2018), 55–75。
- en: Yu et al. (2009) Hualong Yu, Guochang Gu, Haibo Liu, Jing Shen, and Jing Zhao.
    2009. A modified Ant Colony Optimization Algorithm for Tumor Marker Gene Selection.
    *Genomics, Proteomics & Bioinformatics* 7, 4 (2009), 200–208.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2009) Hualong Yu, Guochang Gu, Haibo Liu, Jing Shen, 和 Jing Zhao. 2009.
    一种改进的蚁群优化算法用于肿瘤标志基因选择。*基因组学、蛋白质组学与生物信息学* 7, 4 (2009), 200–208。
- en: Z.-Flores et al. (2020) Emigdio Z.-Flores, Leonardo Trujillo, Pierrick Legrand,
    and Frédérique Faïta-Aïnseba. 2020. EEG Feature Extraction Using Genetic Programming
    for the Classification of Mental States. *Algorithms* 13, 9 (2020), 221.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z.-Flores 等 (2020) Emigdio Z.-Flores, Leonardo Trujillo, Pierrick Legrand, 和
    Frédérique Faïta-Aïnseba. 2020. 使用遗传编程的脑电图特征提取用于心理状态分类。*算法* 13, 9 (2020), 221。
- en: Zemouri et al. (2019) Ryad A. Zemouri, N. Omri, Farhat Fnaiech, Noureddine Zerhouni,
    and Nader Fnaiech. 2019. A New Growing Pruning Deep Learning Neural Network Algorithm
    (GP-DLNN). *Neural. Comput. Appl.* 32 (2019), 1–17.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zemouri 等 (2019) Ryad A. Zemouri, N. Omri, Farhat Fnaiech, Noureddine Zerhouni,
    和 Nader Fnaiech. 2019. 一种新的生长剪枝深度学习神经网络算法 (GP-DLNN)。*神经计算应用* 32 (2019), 1–17。
- en: Zhan et al. (2021) Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu
    Wu, Tianyun Zhang, Malith Jayaweera, David R. Kaeli, Bin Ren, Xue Lin, and Yanzhi
    Wang. 2021. Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture
    and Pruning Search. In *Proc. IEEE Int. Conf. Comput. Vis.* 4801–4811.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan 等 (2021) Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu,
    Tianyun Zhang, Malith Jayaweera, David R. Kaeli, Bin Ren, Xue Lin, 和 Yanzhi Wang.
    2021. 基于移动设备的实时超分辨率实现，结合神经架构与剪枝搜索。在 *IEEE 国际计算机视觉会议论文集* 4801–4811。
- en: Zhang and Mühlenbein (1995) Byoung-Tak Zhang and Heinz Mühlenbein. 1995. Balancing
    Accuracy and Parsimony in Genetic Programming. *Evol. Comput.* 3, 1 (1995), 17–38.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Mühlenbein (1995) Byoung-Tak Zhang 和 Heinz Mühlenbein. 1995. 在遗传编程中平衡准确性和简约性。*进化计算*
    3, 1 (1995), 17–38。
- en: Zhang et al. (2022b) Di Zhang, Yichen Zhou, Jiaqi Zhao, and Yong Zhou. 2022b.
    Co-evolution-based Parameter Learning for Remote Sensing Scene Classification.
    *Int. J. Wavelets Multiresolut. Inf. Process.* 20, 2 (2022), 2150046.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022b) Di Zhang, Yichen Zhou, Jiaqi Zhao, 和 Yong Zhou. 2022b. 基于共进化的遥感场景分类参数学习。*国际小波多分辨率信息处理杂志*
    20, 2 (2022), 2150046。
- en: Zhang et al. (2020b) Haoling Zhang, Chao-Han Huck Yang, Hector Zenil, Narsis Aftab
    Kiani, Yue Shen, and Jesper N. Tegner. 2020b. Evolving Neural Networks through
    a Reverse Encoding Tree. In *Proc. IEEE Congr. Evol. Comput.* 1–10.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020b) Haoling Zhang, Chao-Han Huck Yang, Hector Zenil, Narsis Aftab
    Kiani, Yue Shen, 和 Jesper N. Tegner. 2020b. 通过反向编码树进化神经网络。在 *IEEE 进化计算大会论文集* 1–10。
- en: 'Zhang and Gouza (2018) Jiawei Zhang and Fisher B Gouza. 2018. GADAM: Genetic-evolutionary
    ADAM for Deep Neural Network optimization. *arXiv preprint arXiv:1805.07500* (2018).'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Gouza (2018) Jiawei Zhang 和 Fisher B Gouza. 2018. GADAM：用于深度神经网络优化的遗传进化ADAM。*arXiv
    预印本 arXiv:1805.07500* (2018)。
- en: 'Zhang et al. (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong,
    Jing-hui Zhong, Henry SH Chung, Yun Li, and Yu-hui Shi. 2011. Evolutionary Computation
    Meets Machine Learning: A Survey. *IEEE Comput. Intell. Mag.* 6, 4 (2011), 68–75.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong, Jing-hui
    Zhong, Henry SH Chung, Yun Li, 和 Yu-hui Shi. 2011. 进化计算与机器学习的交汇：一项调查。*IEEE 计算智能杂志*
    6, 4 (2011), 68–75。
- en: Zhang et al. (2021a) Kaiyu Zhang, Jinglong Chen, Shuilong He, Enyong Xu, Fudong
    Li, and Zitong Zhou. 2021a. Differentiable Neural Architecture Search Augmented
    with Pruning and Multi-objective Optimization for Time-efficient Intelligent Fault
    Diagnosis of Machinery. *Mech. Syst. Signal Process.* 158 (2021), 107773.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2021a) Kaiyu Zhang, Jinglong Chen, Shuilong He, Enyong Xu, Fudong Li,
    和 Zitong Zhou. 2021a. 增强的可微神经架构搜索与剪枝和多目标优化，用于机械设备的时间高效智能故障诊断。*机械系统信号处理* 158 (2021),
    107773。
- en: Zhang et al. (2022a) Kangkai Zhang, Chunhui Zhang, Shikun Li, Dan Zeng, and
    Shiming Ge. 2022a. Student Network Learning via Evolutionary Knowledge Distillation.
    *IEEE Trans. Circuits. Syst. Video Technol.* 32, 4 (2022), 2251–2263.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022a) Kangkai Zhang, Chunhui Zhang, Shikun Li, Dan Zeng, 和 Shiming
    Ge. 2022a. 通过进化知识蒸馏进行学生网络学习。*IEEE 电路与系统视频技术汇刊* 32, 4 (2022), 2251–2263。
- en: Zhang (2018) Mengjie Zhang. 2018. Evolutionary Deep Learning for Image Analysis.
    (2018), https://ieeetv.ieee.org/mengjie–zhang–evolutionary–deep–learning–for–image–analysis.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang (2018) Mengjie Zhang. 2018. 图像分析的进化深度学习。 (2018), https://ieeetv.ieee.org/mengjie–zhang–evolutionary–deep–learning–for–image–analysis。
- en: Zhang and Cagnoni (2020) Mengjie Zhang and Stefano Cagnoni. 2020. Evolutionary
    Computation and Evolutionary Deep Learning for Image Analysis, Signal Processing
    and Pattern Recognition. In *Proc. Genetic Evol. Comput. Conf.* 1221–1257.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Cagnoni (2020) Mengjie Zhang 和 Stefano Cagnoni. 2020. 图像分析、信号处理和模式识别中的演化计算与演化深度学习。见于
    *Proc. Genetic Evol. Comput. Conf.* 1221–1257。
- en: Zhang et al. (2020a) Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and Steven W.
    Su. 2020a. Overcoming Multi-Model Forgetting in One-Shot NAS With Diversity Maximization.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 7806–7815.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020a) Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, 和 Steven W.
    Su. 2020a. 通过多样性最大化克服一-shot NAS 中的多模型遗忘。见于 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.* 7806–7815。
- en: Zhang and Smart (2004) Mengjie Zhang and Will Smart. 2004. Genetic Programming
    with Gradient Descent Search for Multiclass Object Classification. In *Proc. Eur.
    Conf. Genetic Program*. 399–408.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Smart (2004) Mengjie Zhang 和 Will Smart. 2004. 使用梯度下降搜索的遗传编程用于多类别对象分类。见于
    *Proc. Eur. Conf. Genetic Program*. 399–408。
- en: Zhang et al. (2017) Yong Zhang, Dun-wei Gong, Xiao-yan Sun, and Yi-nan Guo.
    2017. A PSO-Based Multi-Objective Multi-Label Feature Selection Method in Classification.
    *Sci. Rep.* 7, 1 (2017), 1–12.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2017) Yong Zhang, Dun-wei Gong, Xiao-yan Sun, 和 Yi-nan Guo. 2017. 一种基于PSO的多目标多标签特征选择方法用于分类。*Sci.
    Rep.* 7, 1 (2017), 1–12。
- en: 'Zhang and Rockett (2005) Yang Zhang and Peter I. Rockett. 2005. Evolving Optimal
    Feature Extraction Using Multi-objective Genetic Programming: A Methodology and
    Preliminary Study on Edge Detection. In *Proc. Genetic Evol. Comput. Conf.* 795–802.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Rockett (2005) Yang Zhang 和 Peter I. Rockett. 2005. 使用多目标遗传编程演化最优特征提取：边缘检测方法及初步研究。见于
    *Proc. Genetic Evol. Comput. Conf.* 795–802。
- en: Zhang and Rockett (2011) Yang Zhang and Peter I. Rockett. 2011. A Generic Optimising
    Feature Extraction Method Using Multiobjective Genetic Programming. *Appl. Soft
    Comput.* 11, 1 (2011), 1087–1097.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Rockett (2011) Yang Zhang 和 Peter I. Rockett. 2011. 一种通用优化特征提取方法，使用多目标遗传编程。*Appl.
    Soft Comput.* 11, 1 (2011), 1087–1097。
- en: Zhang et al. (2021b) Yidan Zhang, Youheng Zhen, Zhenan He, and Gray G. Yen.
    2021b. Improvement of Efficiency in Evolutionary Pruning. In *Proc. Int. Joint
    Conf. Neural Netw.* 1–8.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2021b) Yidan Zhang, Youheng Zhen, Zhenan He, 和 Gray G. Yen. 2021b.
    演化修剪效率的提高。见于 *Proc. Int. Joint Conf. Neural Netw.* 1–8。
- en: Zhao et al. (2006) Qijun Zhao, David Zhang, and Hongtao Lu. 2006. A Direct Evolutionary
    Feature Extraction Algorithm for Classifying High Dimensional Data. In *Proc.
    AAAI Conf. Artif. Intell.*, Vol. 1. 561–566.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2006) Qijun Zhao, David Zhang, 和 Hongtao Lu. 2006. 一种直接演化特征提取算法用于分类高维数据。见于
    *Proc. AAAI Conf. Artif. Intell.*, Vol. 1. 561–566。
- en: Zhao et al. (2009) Qijun Zhao, David Dian Zhang, Lei Zhang, and Hongtao Lu.
    2009. Evolutionary Discriminant Feature Extraction with Application to Face Recognition.
    *EURASIP J. Adv. Signal. Process.* 2009 (2009), 1–12.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2009) Qijun Zhao, David Dian Zhang, Lei Zhang, 和 Hongtao Lu. 2009. 演化判别特征提取及其在人脸识别中的应用。*EURASIP
    J. Adv. Signal. Process.* 2009 (2009), 1–12。
- en: Zhao et al. (2007) Tianwen Zhao, Qijun Zhao, Hongtao Lu, and David Dian Zhang.
    2007. Bagging Evolutionary Feature Extraction Algorithm for Classification. In
    *Proc. Int. Conf. Neural Comput.*, Vol. 3\. 540–545.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2007) Tianwen Zhao, Qijun Zhao, Hongtao Lu, 和 David Dian Zhang. 2007.
    用于分类的袋装演化特征提取算法。见于 *Proc. Int. Conf. Neural Comput.*, Vol. 3. 540–545。
- en: Zhou et al. (2021a) Xun Zhou, A. K. Qin, Maoguo Gong, and Kay Chen Tan. 2021a.
    A Survey on Evolutionary Construction of Deep Neural Networks. *IEEE Trans. Evol.
    Comput.* 25, 5 (2021), 894–912.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2021a) Xun Zhou, A. K. Qin, Maoguo Gong, 和 Kay Chen Tan. 2021a. 关于演化构建深度神经网络的调查。*IEEE
    Trans. Evol. Comput.* 25, 5 (2021), 894–912。
- en: Zhou et al. (2020) Yao Zhou, Gary G. Yen, and Zhang Yi. 2020. Evolutionary Compression
    of Deep Neural Networks for Biomedical Image Segmentation. *IEEE Trans. Neural
    Netw. Learn. Syst.* 31, 8 (2020), 2916–2929.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2020) Yao Zhou, Gary G. Yen, 和 Zhang Yi. 2020. 用于生物医学图像分割的深度神经网络演化压缩。*IEEE
    Trans. Neural Netw. Learn. Syst.* 31, 8 (2020), 2916–2929。
- en: Zhou et al. (2021b) Yao Zhou, Gary G. Yen, and Zhang Yi. 2021b. Evolutionary
    Shallowing Deep Neural Networks at Block Levels. *IEEE Trans. Neural Netw. Learn.
    Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3059529](https://doi.org/10.1109/TNNLS.2021.3059529)
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2021b) Yao Zhou, Gary G. Yen, 和 Zhang Yi. 2021b. 在块级别演化浅层深度神经网络。*IEEE
    Trans. Neural Netw. Learn. Syst.* (2021). [https://doi.org/10.1109/TNNLS.2021.3059529](https://doi.org/10.1109/TNNLS.2021.3059529)
- en: Zhou et al. (2021c) Yao Zhou, Gary G. Yen, and Zhang Yi. 2021c. A Knee-Guided
    Evolutionary Algorithm for Compressing Deep Neural Networks. *IEEE Trans. Cybern.*
    51, 3 (2021), 1626–1638.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2021c) Yao Zhou, Gary G. Yen, 和 Zhang Yi. 2021c. 一种膝部引导的演化算法用于压缩深度神经网络。*IEEE
    Trans. Cybern.* 51, 3 (2021), 1626–1638。
- en: 'Zhu et al. (2019) Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, and Yongjun
    Xu. 2019. EENA: Efficient Evolution of Neural Architecture. In *Proc. IEEE Int.
    Conf. Comput. Vis.* 1891–1899.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人 (2019) Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu 和 Yongjun Xu.
    2019. EENA: Efficient Evolution of Neural Architecture. 见于 *Proc. IEEE Int. Conf.
    Comput. Vis.* 1891–1899.'
- en: Zhu and Jin (2022) Hangyu Zhu and Yaochu Jin. 2022. Real-Time Federated Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* 26, 2 (2022), 364–378.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Jin (2022) Hangyu Zhu 和 Yaochu Jin. 2022. Real-Time Federated Evolutionary
    Neural Architecture Search. *IEEE Trans. Evol. Comput.* 26, 2 (2022), 364–378.
- en: Zhu et al. (2007) Zexuan Zhu, Y. Ong, and Manoranjan Dash. 2007. Markov Blanket-Embedded
    Genetic Algorithm for Gene Selection. *Pattern Recognit.* 40, 11 (2007), 3236–3248.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2007) Zexuan Zhu, Y. Ong 和 Manoranjan Dash. 2007. Markov Blanket-Embedded
    Genetic Algorithm for Gene Selection. *Pattern Recognit.* 40, 11 (2007), 3236–3248.
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning Transferable Architectures for Scalable Image Recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.* 8697–8710.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 等人 (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens 和 Quoc V Le. 2018.
    Learning Transferable Architectures for Scalable Image Recognition. 见于 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.* 8697–8710.
