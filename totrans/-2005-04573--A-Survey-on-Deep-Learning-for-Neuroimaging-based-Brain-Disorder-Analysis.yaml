- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:01:14'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2005.04573] A Survey on Deep Learning for Neuroimaging-based Brain Disorder
    Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.04573](https://ar5iv.labs.arxiv.org/html/2005.04573)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Li Zhang¹, Mingliang Wang², Mingxia Liu^(3,∗) and Daoqiang Zhang^(2,∗)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning has been recently used for the analysis of neuroimages, such as
    structural magnetic resonance imaging (MRI), functional MRI, and positron emission
    tomography (PET), and has achieved significant performance improvements over traditional
    machine learning in computer-aided diagnosis of brain disorders. This paper reviews
    the applications of deep learning methods for neuroimaging-based brain disorder
    analysis. We first provide a comprehensive overview of deep learning techniques
    and popular network architectures, by introducing various types of deep neural
    networks and recent developments. We then review deep learning methods for computer-aided
    analysis of four typical brain disorders, including Alzheimer’s disease, Parkinson’s
    disease, Autism spectrum disorder, and Schizophrenia, where the first two diseases
    are neurodegenerative disorders and the last two are neurodevelopmental and psychiatric
    disorders, respectively. More importantly, we discuss the limitations of existing
    studies and present possible future directions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: College of Computer Science and Technology, Nanjing Forestry University, Nanjing
    210037, China.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: College of Computer Science and Technology, Nanjing University of Aeronautics
    and Astronautics, Nanjing 211106, China.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Department of Radiology and BRIC, University of North Carolina at Chapel Hill,
    NC 27599, USA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Keywords: Deep learning, Neuroimage, Alzheimer’s disease, Parkinson’s disease,
    Autism spectrum disorder, Schizophrenia.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical imaging refers to several different technologies that are used to provide
    visual representations of the interior of the human body in order to aid the radiologists
    and clinicians to early detect, diagnose or treat diseases more efficiently [[1](#bib.bib1)].
    Over the past few decades, medical imaging quickly becomes a dominant and effective
    tool, and represents various imaging modalities, including X-ray, mammography,
    ultrasound, computed tomography, magnetic resonance imaging(MRI) and positron
    emission tomography(PET) [[2](#bib.bib2)]. Each type of these technologies gives
    various anatomical and functional information about the different body organs
    for diagnosis as well as for research. In clinical practice, the detail interpretation
    of medical images needs to be performed by human experts such as the radiologists
    and clinicians. However, for the enormous number of medical images, the interpretations
    are time-consuming and easily cause by the biases and potential fatigue of human
    experts. Therefore, from the early 1980s, doctors and researchers have begun to
    use computer-assisted diagnosis(CAD) systems to interpret the medical images and
    to improve their efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In the CAD systems, machine learning is able to extract informative features
    which well describe the inherent patterns from data, and plays a vital role in
    medical image analysis [[3](#bib.bib3), [4](#bib.bib4)]. Several traditional machine
    learning algorithms, such as sparse learning, support vector machine (SVM), Gaussian
    networks, random forest, decision tree and hidden Markov model, etc., are wildly
    used [[5](#bib.bib5)]. However, the structures of the medical images are very
    complex, and the feature selection step is still done by the human experts on
    the basis of their domain-specific knowledge. This results in a challenge for
    non-experts to utilize machine learning techniques in medical image analysis.
    Therefore, the handcrafted feature selection is not suitable for medical images.
    In addition, the shallow architectures of these traditional machine learning algorithms
    limit their representational power [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, the logical next research direction is to let algorithms automatically
    learn features that can optimally represent the data. Deep learning perfectly
    coincides with this concept and rapidly becomes a methodology of choice for medical
    image analysis in recent years [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)].
    Compared to the traditional machine learning algorithms, deep learning automatically
    discoveries the informative representations without the professional knowledge
    of domain experts and allows the non-experts to effectively use deep learning
    techniques. Due to enhanced computer power with the high-tech central processing
    units(CPU) and graphical processing units(GPU), given the availability of big
    data, and designed novel algorithms to train deep neural networks, deep learning
    receives the unprecedented success in the most artificial intelligence applications,
    such as computer vision [[10](#bib.bib10)], natural language processing [[11](#bib.bib11)]
    and speech recognition [[12](#bib.bib12)]. Especially, the improvement and successes
    of computer vision simultaneously prompted the use of deep learning in the medical
    image analysis [[13](#bib.bib13), [14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, deep learning has fueled great strides in medical image analysis.
    We can divide the medical image analysis tasks into several major categories:
    classification, detection/localization, registration, segmentation [[15](#bib.bib15)].
    The classification is one the first task in which deep learning giving a major
    contribution to medical image analysis. This task aims to classify medical images
    into two or more classes.The stacked auto-encoder model was used to identify Alzheimer’s
    disease or mild cognitive impairment by combining medical images and biological
    features [[16](#bib.bib16)]. The detection/localization task consists of the localization
    and identification of the landmarks or lesion in the full medical image. For example,
    deep convolutional neural networks were used for the detection of lymph nodes
    in CT images [[17](#bib.bib17)]. The segmentation task is to partition a medical
    image into different meaningful segments, such as different tissue classes, organs,
    pathologies, or other biologically relevant structures.The U-net was the most
    well-known deep learning architecture, which used convolutional networks for biomedical
    image segmentation [[18](#bib.bib18)]. Registration of medical images is a process
    that searches for the correct alignment of images.Wu et al. utilized convolutional
    layers to extract features from input patches in an unsupervised manner. Then
    the obtained feature vectors were used to replace the handcrafted features in
    the HAMMER registration algorithm [[19](#bib.bib19)]. In addition, the medical
    image analysis contains other meaningful tasks, such as content-based image retrieval [[20](#bib.bib20)],
    image generation and enhancement [[21](#bib.bib21)], combination image data with
    reports [[22](#bib.bib22)].'
  prefs: []
  type: TYPE_NORMAL
- en: There are many papers have comprehensively surveyed the medical image analysis
    using deep learning techniques [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)].
    However, these papers usually reviewed all human tissues including the brain,
    chest, eye, breast, cardiac, abdomen, musculoskeletal and other. Almost no papers
    focus on one specific tissue or disease [[23](#bib.bib23)]. Brain disorders are
    among the most severe health problems facing our society, causing untold human
    suffering and enormous economic costs. Many studies successfully used medical
    imaging techniques for the early detection, diagnosis and treatment of the human
    brain disorders, such as neurodegenerative disorders, neurodevelopmental disorders
    and psychiatric disorders [[24](#bib.bib24), [25](#bib.bib25)]. Therefore, we
    pay more close attention to human brain disorders in this review.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of this review can roughly be divided into two parts, the deep
    learning architectures and the usage of deep learning in brain disorder analysis,
    and is organized as follows. In Section [2](#S2 "2 Deep Learning ‣ A Survey on
    Deep Learning for Neuroimaging-based Brain Disorder Analysis"), we briefly introduce
    some popular deep learning models. In Section [3](#S3 "3 Applications in Brain
    Disorder Analysis with Medical Images ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis"), we provide a detailed overview of recent studies using
    deep learning techniques for four brain disorders, including Alzheimer’s disease,
    Parkinson’s disease, Autism spectrum disorder, and Schizophrenia. Finally, we
    analyze the limitations of the deep learning techniques in medical image analysis,
    and provide some research directions for further study. For readers’ convenience,
    the abbreviations of terminologies used in the following context are listed in
    the  [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Abbreviation of terminologies used in following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Terminology | Abbr. | Terminology | Abbr. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Alzheimer’s Disease | AD | Two-dimensional CNN | 2D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Autism spectrum disorder | ASD | Three-dimensional CNN | 3D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Computer-assisted diagnosis | CAD | Auto-encoder | AE |'
  prefs: []
  type: TYPE_TB
- en: '| Converted MCI | cMCI | Artificial neural networks | ANN |'
  prefs: []
  type: TYPE_TB
- en: '| Cerebrospinal fluid | CSF | Back-propagation | BP |'
  prefs: []
  type: TYPE_TB
- en: '| Computed tomography | CT | Convolutional neural networks | CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Diffusion tensor imaging | DTI | Denoising auto-encoders | DAE |'
  prefs: []
  type: TYPE_TB
- en: '| Electroencephalogram | EEG | Deep belief networks | DBN |'
  prefs: []
  type: TYPE_TB
- en: '| Functional MRI | fMRI | Deep bolztman machine | DBM |'
  prefs: []
  type: TYPE_TB
- en: '| Gray matter | GM | Deep generative model | DGM |'
  prefs: []
  type: TYPE_TB
- en: '| Mild cognitive impairment | MCI | Deep neural networks | DNN |'
  prefs: []
  type: TYPE_TB
- en: '| Magnetic resonance imaging | MRI | Feed-forward neural network | FFNN |'
  prefs: []
  type: TYPE_TB
- en: '| Normal control | NC | Generative adversarial networks | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| Parkinson’s Disease | PD | Graph CNN | GCN |'
  prefs: []
  type: TYPE_TB
- en: '| Positron emission tomography | PET | Gated recurrent unit | GRU |'
  prefs: []
  type: TYPE_TB
- en: '| Region of interest | ROI | Long-short term memory | LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| Resting-state fMRI | rs-fMRI | Multi-layer perceptron | MLP |'
  prefs: []
  type: TYPE_TB
- en: '| Stable MCI | sMCI | Principle component analysis | PCA |'
  prefs: []
  type: TYPE_TB
- en: '| Structural MRI | sMRI | Restricted Bolztman machine | RBM |'
  prefs: []
  type: TYPE_TB
- en: '| Single photon emission CT | SPECT | Recurrent neural networks | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| Schizophrenia | SZ | Region with CNN | R-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| White matter | WM | Stacked auto-encoders | SAE |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Stacked sparse AE | SSAE |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Support vector machine | SVM |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Variational auto-encoders | VAE |'
  prefs: []
  type: TYPE_TB
- en: The abbreviations in the left column relate to the medical image analysis, and
    the abbreviations in the right column relate to the machine learning and deep
    learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the fundamental concept of basic deep learning
    models in the literature, which have been wildly applied to medical image analysis,
    especially human brain disorder diagnosis. These models include feed-forward neural
    networks, stacked auto-encoders, deep belief network, deep Boltzmann machine,
    generative adversarial networks, convolutional neural networks, graph convolutional
    networks and recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Feed-Forward Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In machine learning, artificial neural networks (ANN) aims to simulate intelligent
    behavior by mimicking the way that biological neural networks function. The simplest
    artificial neural networks is a single-layer architecture, which is composed of
    an input layer and an output layer (Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Feed-Forward
    Neural Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis").a). However, despite the use of non-linear activation
    functions in output layers, the single-layer neural network usually obtains poor
    performance for complicated data patterns. In order to circumvent the limitation,
    the multi-layer perceptron (MLP), also referred to as a feed-forward neural network
    (FFNN) (Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Feed-Forward Neural Networks ‣ 2 Deep
    Learning ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis").b),
    which includes a so-call hidden layer between the input layer and the output layer.
    Each layer contains multiple units which are fully connected to units of neighboring
    layers, but there are no connections between units in the same layer. Given an
    input visible vector $\bm{x}$, the composition function of output unit $\bm{y}$
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{k}(\bm{x};\bm{\theta})=f^{(2)}\left(\sum_{j=1}^{M}w_{k,j}^{(2)}f^{(1)}\left(\sum_{i=1}^{N}w_{j,i}^{(1)}x_{i}+b_{j}^{(1)}\right)+b_{k}^{(2)}\right)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where the superscript represents a layer index, $M$ is the number of hidden
    units, $b_{j}$ and $b_{k}$ represents the bias of input and hidden layer respectively.
    $f^{(1)}(\cdot)$ and $f^{(2)}(\cdot)$ denote the nonlinear activation function
    and the parameter set is $\bm{\theta}=\{\bm{w}_{j}^{(1)},\bm{w}_{k}^{(2)},b_{j}^{(1)},b_{k}^{(2)}\}$.
    The back-propagation(BP) is an efficient algorithm to evaluate a gradient in the
    FFNN [[26](#bib.bib26)]. The BP algorithm is to propagate the error values from
    the output layer back to the input layer through the network. Once the gradient
    vector of all the layers is obtained, the parameters $\bm{\theta}$ can be updated.
    Until the loss function is converged or the predefined number of iterations is
    reached, the update process stops and the network gets the model parameters $\bm{\theta}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53f3754f279aa9b14f045087c1cb939c.png)![Refer to caption](img/65598e8c997c092ae4ea3171948a8254.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Architectures of the single-layer(a) and multi-layer(b) neural networks.
    The blue, green and orange solid circles represent the input visible, hidden and
    output units respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Stacked Auto-Encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An auto-encoder(AE), also known as auto-associator, learns the latent representations
    of input data (called encode) in an unsupervised manner, and then uses these representations
    to reconstruct output data (called decode). Due to the simple and shallow structure,
    the power representation of a typical AE is relatively limited. However, when
    multiple AEs are stacked to form a deep network, called a stacked auto-encoders
    (SAE) (Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Stacked Auto-Encoders ‣ 2 Deep Learning
    ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis")),
    the representation power of an SAE can be obviously improved [[27](#bib.bib27)].
    Because of the deep structural characteristic, the SAE is able to learn and discover
    more complicated patterns inherent in the input data. The lower layers can only
    learn simpler data patterns, while the higher layers are able to extract more
    complicated data patterns. In a word, the different layers of an SAE represent
    different levels of data information. In addition, various AE variations, denoising
    auto-encoders (DAE) [[28](#bib.bib28)], sparse auto-encoder (sparse AE) [[29](#bib.bib29)]
    and variational auto-encoders(VAE) [[30](#bib.bib30)], have been proposed and
    also can be stacked as SAE, such as the stacked sparse AE (SSAE) [[31](#bib.bib31)]
    . These extensions of auto-encoders not only can learn more useful latent representations
    but also improve the robustness.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/880a47d830896ca4a5051356ec766a21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Architectures of a stacked auto-encoder. The blue and red dotted
    boxes represent the encoding an decoding stage respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: With respect to training parameters of an SAE, the greedy layer-wise approach
    is a well-known choice to learn the weight matrices and the biases. It can avoid
    the drawback of the BP algorithm, which can cause the gradient falling into a
    poor local optimum [[32](#bib.bib32)]. The important character of the greedy layer-wise
    is to pre-train each layer in turn. In other words, the output of the $l$-th hidden
    layers is used as input data for the $(l+1)$-th hidden layer. The process is performed
    as pre-training, which is conducted in an unsupervised manner with a standard
    BP algorithm. The important advantage of the pre-training is able to increase
    the size of the training dataset using unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Deep Belief Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Deep Belief Networks (DBN), stacks multiple RBMs for deep architecture construction [[33](#bib.bib33)].
    A DBN has one visible layer and multiple hidden layers as shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2.3 Deep Belief Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning
    for Neuroimaging-based Brain Disorder Analysis").a. The lower layers form directed
    generative models. However, the top two layers form the distribution of RBM, which
    is an undirected generative model. Therefore, given the visible units $\bm{v}$
    and $L$ hidden layers $\bm{h}^{(1)},\bm{h}^{(2)},\dots,\bm{h}^{(L)}$, the joint
    distribution of DBN is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(\bm{v},\bm{h^{(1)}},\dots,\bm{h^{(L)}})=P(\bm{v}&#124;\bm{h^{(1)}})\big{(}\prod_{l=1}^{L-2}P(\bm{h^{(l)}}&#124;\bm{h^{(l+1)}})\big{)}P(\bm{h^{(L-1)}},\bm{h^{(L)}})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $P(\bm{h^{(l)}}|\bm{h^{(l+1)}})$ represents the conditional distribution
    for the units of the hidden layer $l$ given the units of the hidden layer $l+1$,
    and $P(\bm{h^{(L-1)}},\bm{h^{(L)}})$ corresponds the joint distribution of the
    top hidden layers $L-1$ and $L$.
  prefs: []
  type: TYPE_NORMAL
- en: As for training DBN, there are two steps, including pre-training and fine-tuning.
    In the pre-training step, DBN is trained by stacking RBMs layer by layer to find
    the parameter space. Each layer is trained as an RBM. Specifically, the $l$-th
    hidden layer is trained as RBM using the observation data from output representation
    of the $(l-1)$-th hidden layer, then repeats training each layer until the top
    layer. After the pre-training is completed, the fine-tuning is performed to further
    optimize the network to search the optimum parameters. The wake-sleep algorithm
    and the standard back-propagation algorithm are good at fine-tuning for generative
    and discriminative models respectively [[34](#bib.bib34)]. For a practical application
    problem, the obtained parameters from the pre-training step are used to initiate
    a DNN, and then the deep model can be fine-tuned by a supervised learning algorithm
    like BP.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b710c5a300f1b899cc35ccea6752277f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Deep Belief Network
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45048d45e7a5b2ff2d1b06b2b37513ba.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Deep Boltzman Machine
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Schematic illustration of Deep Belief Networks and Deep Boltzmann
    Machine'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Deep Boltzmann Machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Deep Boltzmann Machine(DBM) is also constructed by stacking multiple RBMs
    as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Deep Belief Networks ‣ 2 Deep Learning
    ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis").b [[35](#bib.bib35),
    [36](#bib.bib36)]. However, unlike DBN, all the layers of DBM form an entirely
    undirected model, and each variable within the hidden layers are mutually independent.
    Thus, the hidden layer $l$ is conditioned on its two neighboring layer $l-1$ and
    $l+1$, and its probability distribution is $P(\bm{h}^{(l)}|\bm{h}^{(l-1)},\bm{h}^{(l+1)})$.
    Given the values of the neighboring layers, the conditional probabilities over
    the visible and the $L$ set of hidden units are given by logistic sigmoid functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P(v_{i}&#124;\bm{h}^{1})$ | $\displaystyle=\sigma\big{(}\sum_{j}W_{ij}^{(1)}h_{j}^{(1)}\big{)}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle P(h_{k}^{(l)}&#124;\bm{h}^{(l-1)},\bm{h}^{(l+1)})$ | $\displaystyle=\sigma\big{(}\sum_{m}W_{mk}^{(l)}h_{m}^{(l-1)}+\sum_{n}W_{kn}^{(l+1)}h_{n}^{(l+1)}\big{)}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle P(h_{t}^{(L)}&#124;\bm{h}^{(L-1)})$ | $\displaystyle=\sigma\big{(}\sum_{s}W_{st}^{(L)}h_{s}^{(L-1)}\big{)}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Note that in the computation of the conditional probability of the hidden unit
    $\bm{h}^{({l})}$, the probability incorporate both the lower hidden layer $\bm{h}^{({l-1})}$
    and the upper hidden layer $\bm{h}^{(l+1)}$. Due to incorporate the more information
    from the lower and upper layers, the representational power of a DBM is more robust
    to the noisy observed data [[37](#bib.bib37)]. However, this character makes that
    the conditional probability of DBM $P(\bm{h}^{(l)}|\bm{h}^{(l-1)},\bm{h}^{(l+1)})$
    is more complex than those of DBN, $P(\bm{h}^{(l)}|\bm{h}^{(l+1)})$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Generative Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Due to their capability in learning deep representations without extensively
    annotated training data, Generative Adversarial Networks (GAN) has gained a lot
    of attention in computer vision and natural language processing [[38](#bib.bib38)].
    GAN consists of two competing neural networks, a generator $G$ and a discriminator
    $D$, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.5 Generative Adversarial Networks
    ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder
    Analysis"). The generator $G$ parameterized by $\theta$ takes as input a random
    noise vector $\bm{z}$ from a prior distribution $p_{\bm{z}}(\bm{z};\theta)$, and
    outputs a sample $G(\bm{z})$, which can be regarded as a sample drawn from the
    generator data distribution $p_{g}$. The discriminator $D$ that takes an input
    $G(\bm{z})$ or $\bm{x}$, and outputs the probability $D(\bm{x})$ or $D(\bm{G(\bm{z})})$
    to evaluate that the sample is from the generator $G$ or the real data distribution.
    GAN simultaneously trains the generator and discriminator where the generator
    $G$ tries to generate realistic data to fool the discriminator, while the discriminator
    $D$ tries to distinguish between the real and fake samples. Inspired by the game
    theory, the training process is to form a two-player minimax game with the value
    function as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}V(G,D)=\mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}[\log
    D(\bm{x})]+\mathbb{E}_{\bm{z}\sim p_{\bm{z}}(\bm{z})}[\log(1-D(G(\bm{z})))]$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{data}(\bm{x})$ denotes the real data distribution. After training
    alternately, if $G$ and $D$ have enough capacity, they will reach a point at which
    both cannot improve because $p_{g}=p_{data}$. In other words, the discriminator
    is unable to distinguish the difference between a real and a generated sample,
    i.e.$D(\bm{x})=0.5$. Although vanilla GAN has attracted considerable attention
    in various applications, there still remain several challenges related to training
    and evaluating GAN, such as model collapse and saddle points [[39](#bib.bib39)].
    Therefore, many variants of GAN, such as WGAN [[40](#bib.bib40)] and DCGAN [[41](#bib.bib41)]
    have been proposed to overcome these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2de437f46ce79d65ee5cb5a7345808d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Architecture of Generative Adversarial Networks'
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compared to SAE, DBN and DBM, utilizing the inputs in vector form which inevitably
    destroys the structural information in images, CNN is designed to better retain
    and utilize the structural information among neighboring pixels or voxels, and
    to required minimal preprocessing by directly taking two-dimensional(2D) or three-dimensional(3D)
    images as inputs [[42](#bib.bib42)]. Structurally, a CNN is a sequence of layers,
    and each layer of the CNN transforms one volume of activations to another through
    a differentiable function. Figure [5](#S2.F5 "Figure 5 ‣ 2.6 Convolutional Neural
    Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis") shows a typical CNN architecture for a computer vision
    task, which consists of three type neural layers: convolutional layers, pooling
    layers and fully connected layers. The convolutional layers interspersed with
    pooling layers, eventually leading to the fully connected layers. The convolutional
    layer takes the pixels or voxels of a small patch of the input images, called
    the local receptive field, then utilizes various learnable kernels to convolve
    the receptive field to generate multiple feature maps. A pooling layer performs
    the non-linear downsampling to reduce the spatial dimensions of the input volume
    for the next convolutional layer. The fully connected layer input the 3D or 2D
    feature map to a 1D feature vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c1a754efeef7d83720018e461e65778.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Architecture of convolutional neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: The major issue in training deep models is the over-fitting, which arises from
    the gap between the limited number of training samples and a large number of learnable
    parameters. Therefore, various techniques are designed to make the models train
    and generalize better, such as dropout and batch normalization to just name a
    few. A dropout layer randomly drops a fraction of the units or connections during
    each training iteration [[43](#bib.bib43)]. And it has been demonstrated that
    dropout is able to successfully avoid over-fitting. In addition, batch normalization
    is another useful regularization and performs normalization with the running average
    of the mean–variance statistics of each mini-batch. It is shown that using batch
    normalization not only drastically speeds up the training time, but also improves
    the generalization performance [[44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Graph Convolutional Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While CNN has achieved the huge success to extract latent representations from
    Euclidean data(e.g. images, text and video), there are a rapidly increasing number
    of various applications where data is generated from the non-Euclidean domain
    and needs to be efficiently analyzed. Researchers straightforwardly borrow ideas
    from CNN to design the architecture of graph convolutional networks (GCN) to handle
    complexity graph data [[45](#bib.bib45)]. Figure [6](#S2.F6 "Figure 6 ‣ 2.7 Graph
    Convolutional Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis") shows the process of a simple GCN with graph pooling
    layers for a graph classification task. The first step is to transform the traditional
    data to graph data, then the graph structure and node content information is regarded
    as input. The graph convolutional layer plays a central role in extracting node
    hidden representations from aggregating the feature information from its neighbors.
    The graph pooling layers can be interleaved with the GCN layers and coarsened
    graphs into sub-graphs in order to obtained higher graph-level representations
    for each node on coarsened sub-graphs. After multiple fully connected layers,
    the softmax output layer is used to predict the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/907992e086dacb96edb214c9fcf46b96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Architecture of graph convolutional networks'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the types of graph convolutions, GCN can be categorized into spectral-based
    and spatial-based methods. Spectral-based methods formulated graph convolution
    by introducing filters from the perspective of graph single processing. Spatial-based
    methods defined graph convolution directly on the graph, which operates on spatial
    close neighbors to aggregate feature information. Due to drawbacks to spectral-based
    methods from three aspects, efficiency, generality and flexibility, spatial-based
    methods have attracted more attention recently [[46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A recurrent neural network (RNN) is an extension of an FFNN, which is able to
    learn features and long term dependencies from sequential and time-series data.
    The most popular RNN architecture is the long-short term memory(LSTM) [[47](#bib.bib47)],
    which is composed of a memory cell $C_{t}$, a forget gate $f_{t}$, the input gate
    $i_{t}$ and the output gate $o_{t}$ (Figure [7](#S2.F7 "Figure 7 ‣ 2.8 Recurrent
    Neural Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis").a). The memory cell transfers relevant information all
    the way to the sequence chain, and these gates control the activation singles
    from various sources to decide which information is added to and removed from
    the memory cell.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike vanilla RNN, LSTM is able to decide whether to preserve the existing
    memory by the above-introduced gates. Theoretically, if LSTM learns an important
    feature from the input sequential data, it can keep this feature over a long time,
    thus captures potential long-time dependencies. One popular LSTM variant is the
    Gated Recurrent Unit (GRU) (Figure [7](#S2.F7 "Figure 7 ‣ 2.8 Recurrent Neural
    Networks ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis").b), which merges the forget and input gates into a single
    “update gate”, and combines the memory cell state and hidden state into one state.
    The update gate decides how much information to add and throw away, and the reset
    gate decides how much previous information to forget. This makes the GRU is simpler
    than the standard LSTM [[48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6668492ba6e7683b5952cca0a22163a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Long Short-Term Memory
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/418b6344f4b7513be1f0e52ecf2f5c53.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Gated Recurrent Unit
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Architectures of long short-term memory(a) and gated recurrent unit(b).
    In the subfigure (a), the blue, green and yellow represent the forget gate, input
    gate and output gate, respectively. In the subfigure (b), the blue and yellow
    represent the reset gate and update gate, respectively. To keep the figure simple,
    biases are not shown.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Open Source Deep Learning Library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the great successes of deep learning techniques in various applications,
    some famous research groups and companies have released their source codes and
    tools in deep learning. Due to these open source toolkits, people are able to
    easily build deep models for their applications even if they are not acquainted
    with deep learning technique. Table [2](#S2.T2 "Table 2 ‣ 2.9 Open Source Deep
    Learning Library ‣ 2 Deep Learning ‣ A Survey on Deep Learning for Neuroimaging-based
    Brain Disorder Analysis") lists the most popular toolkits for deep learning and
    shows their main features. All the software in the table can support for using
    GUP acceleration. For now, there are numerous deep learning toolkits available,
    but the problem it brings to people is how to select the most suitable toolkit
    for their applications. Selecting the best toolkit depends on the goals of the
    projects, the characters of the available dataset, the skills and background of
    the researchers, the features of the available toolkits [[49](#bib.bib49), [50](#bib.bib50)].
    Therefore, when a project starts, it is worth spending time to evaluate candidate
    toolkits to be sure that the best suitable toolkit is chosen for the corresponding
    application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The popular open source toolkits for deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Creator | GitHub | License | Platform | Language | Interface |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Caffe [[51](#bib.bib51)] | Berkeley Center | BVLC/caffe | BSD | Linux, macOS,
    Windows | C++ | Python, MATLAB, C++ |'
  prefs: []
  type: TYPE_TB
- en: '| Deeplearning4j [[52](#bib.bib52)] | Skymind | deeplearning4j/ deeplearning4j
    | Apache 2.0 | Linux, macOS, Windows, Android | C++, Java | Java, Scala, Clojure,
    Python,  Kotlin |'
  prefs: []
  type: TYPE_TB
- en: '| Keras [[53](#bib.bib53)] | Franois Systems | fchollet/keras | MIT license
    | Linux, macOS, Windows | Python | Python, R |'
  prefs: []
  type: TYPE_TB
- en: '| MXNet [[54](#bib.bib54)] | Apache Software Foundation | apache/ incubator-mxnet
    | Apache 2.0 | Linux, macOS, Windows, AWS, Android, iOS, JavaScript |  C++ | C++,
    Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow [[55](#bib.bib55)] | Google | tensorflow/ tensorflow | Apache
    2.0 | Linux, macOS, Windows, Android | C++, Python | Python, C, C++, Java, Go,
    JavaScript, R, Julia, Swift |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch [[56](#bib.bib56)] | Adam Paszke et al | pytorch/ pytorch | BSD |
    Linux, macOS, Windows | Python, C | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Theano [[57](#bib.bib57)] | Université de Montréal | Theano/ Theano | BSD
    | Linux, macOS, Windows | Python | Python |'
  prefs: []
  type: TYPE_TB
- en: '| Torch [[58](#bib.bib58)] | Ronan Collobert et al | torch/ torch7 | BSD |
    Linux, macOS, Windows, Android, iOS | C,  Lua, LuaJIT | C,Lua, LuaJIT |'
  prefs: []
  type: TYPE_TB
- en: '| CNTK [[59](#bib.bib59)] | Microsoft | Microsoft/ CNTK | MIT license | Linux,
    macOS, Windows | C++ | Python, C++, C$\#$, Java |'
  prefs: []
  type: TYPE_TB
- en: '| MATLAB | MathWorks | - | Proprietary | Linux, macOS, Windows | C,C++, Java,
    MATLAB | MATLAB |'
  prefs: []
  type: TYPE_TB
- en: 3 Applications in Brain Disorder Analysis with Medical Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human brain is susceptible to many different disorders that strike at every
    stage of life. Developmental disorders usually first appear in early childhood,
    such as autism spectrum disorder and dyslexia. Although psychiatric disorders
    are typically diagnosed in teens or early adulthood, their origins may exist much
    earlier in life, such as depression and schizophrenia. Then, as people age, people
    become increasingly susceptible to Alzheimer’s disease, Parkinson’s disease, and
    other dementia diseases. In this section, we select four typical brain disorders,
    including Alzheimer’s disease, Parkinson’s disease, Autism spectrum disorder and
    Schizophrenia. Alzheimer’s disease and Parkinson’s disease are both neurodegenerative
    disorders. Autism spectrum disorder and Schizophrenia are neurodevelopmental and
    psychiatric disorders, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deep Learning for Alzheimer’s Disease Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alzheimer’s disease (AD) is a neurological, irreversible, progressive brain
    disorder and is the most common cause of dementia. Until now, the causes of AD
    are not yet fully understand, but accurate diagnosis of AD plays a significant
    role in patient care, especially at the early stage. For the study of AD diagnosis,
    the best-known public neuroimaging dataset is from the Alzheimer’s Disease Neuroimaging
    Initiative (ADNI), which is a multi-site study that aims to improve clinical trials
    for the prevention and treatment of AD. The ADNI study has been running since
    2004 and is now in its third phase [[60](#bib.bib60)]. Researchers collect, validate
    and utilize data, including MRI and PET images, genetics, cognitive tests, cerebrospinal
    fluid (CSF) and blood biomarkers as predictors of the disease. Up to now, the
    ADNI dataset consists of ADNI-1, ADNI-GO, ADNI-2 and ADNI-3 and contains more
    than 1000 patients. According to the Mini-Mental State Examination (MMSE) scores,
    these patients were in three stages of disease: normal control (NC) , mild cognitive
    impairment(MCI) and AD. The MCI subject can be divided into two subcategories:
    converted MCI (cMCI) and stable MCI (sMCI), based on whether a subject converted
    to AD within a period of time (e.g. 24 months). The ADNI-GO and ADNI-2 provided
    two different MCI groups: early mild cognitive impairment (EMCI) and late mild
    cognitive impairment (LMCI), determined by a Wechsler Memory Scale (WMS) neuropsychological
    test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, plenty of papers have been published on the deep learning techniques
    for AD diagnosis. According to different model architectures, these methods can
    be roughly divided into two subcategories: DGM-based and CNN-based methods. The
    DGM-based methods contained DBN, DNM, SAE and AE variants. Li et al. stacked multiple
    RBM to construct a robust deep learning framework, which incorporated the stability
    selection and the multi-task learning strategy [[61](#bib.bib61)]. Suk et al. proposed
    a series of methods based on deep learning models, such as DBM [[62](#bib.bib62)]
    and SAE [[16](#bib.bib16), [63](#bib.bib63)]. For example, the literature [[16](#bib.bib16)]
    applied SAE to learn the latent representations from sMRI, PET and CSF, respectively.
    Then, multi-kernel SVM classifier was used to fuse the selected multi-modal features.
    Liu et al. also used SAE to extract features from multi-modal data, then a zero-masking
    strategy was applied to fuse these learned features [[64](#bib.bib64)]. Shi et
    al. adopted multi-modality stacked denoising sparse AE (SDAE) to fuse cross-sectional
    and longitudinal features estimated from MR brain images [[65](#bib.bib65)]. Lu et
    al. developed a multiscale deep learning network, which took the multiscale patch-wise
    metabolism features as input [[66](#bib.bib66)]. And this study was perhaps the
    first study to utilize such a large number of FDG-PET images data. Martinez-Murcia et
    al. used a deep convolution AE (DCAE) architecture to extract features, which
    showed large correlations with clinical variables such as age, tau protein deposits,
    and especially neuropsychological examinations [[67](#bib.bib67)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN-based methods learned all levels of features from raw pixels and avoided
    the manual ROIs annotation procedure, and can be further subdivided into two subcategories:
    2D-CNN and 3D-CNN. Gupta et al. pre-trained a 2D-CNN based on sMRI data through
    a sparse AE on random patches of natural images [[68](#bib.bib68)]. The key technique
    was the use of cross-domain features to present MRI data. Liu et al. used a similar
    strategy and pre-trained a pre-trained deep CNN on ImageNet [[69](#bib.bib69)].
    Sarraf et al. first used the fMRI data in deep learning applications [[70](#bib.bib70)].
    The 4D rs-fMRI and 3D MRI data were decomposed into 2D format images in the preprocessing
    step, and then the CNN-based architecture received these images in its input layer.
    Billones et al. designed a DemNet model based on the 16-layers VGGNet. The DemNet
    only selected the coronal image slices with indices 111 to 130 in 2D format images
    under the assumption that these slices covered the areas, which had the important
    features for the classification task [[71](#bib.bib71)]. Liu et al. proposed a
    novel classification framework that learned features from a sequence of 2D slices
    by decomposing 3D PET images [[72](#bib.bib72)]. Then hierarchical 2D-CNN was
    built to capture the intra-slice features, while GRU was adopted to extract the
    inter-slice features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Overview of papers using deep learning techniques for AD diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Database | Subjects | Modality | Model |  |'
  prefs: []
  type: TYPE_TB
- en: '| AD | cMCI | sMCI | NC |  |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[61](#bib.bib61)] | 2015 | ADNI | 51 | 43 | 56 | 52 | sMRI+PET+CSF
    | DBN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[64](#bib.bib64)] | 2015 | ADNI | 85 | 67 | 102 | 77 | sMRI+PET
    | SAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[62](#bib.bib62)]. | 2014 | ADNI | 93 | 76 | 128 | 101 | sMRI+PET
    | DBM |  |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[16](#bib.bib16)] | 2015 | ADNI | 51 | 43 | 56 | 52 | sMRI+PET+CSF
    | SAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[63](#bib.bib63)] | 2016 | ADNI | 51 | 43 | 56 | 52 | sMRI+PET+CSF
    | SAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| - | 198 | 167 | 236 | 229 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. [[65](#bib.bib65)] | 2016 | ADNI | 95 | 121 | 123 | sMRI + Age
    | SDAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| Lu et al. [[66](#bib.bib66)] | 2018 | ADNI | 226 | 112 | 409 | 304 | PET
    | SAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| Martinez-Murcia et al. [[67](#bib.bib67)] | 2019 | ADNI | 99 | 212 | 168
    | rs-fMRI | DCAE |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gupta et al. [[68](#bib.bib68)] | 2013 | ADNI | 200 | 411 | 232 | sMRI |
    2D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[69](#bib.bib69)] | 2014 | ADNI | 200 | 411 | 232 | sMRI | 2D-CNN
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Billones et al. [[71](#bib.bib71)] | 2016 | ADNI | 300 | 300 | 300 | rs-fMRI
    | 2D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sarraf et al. [[70](#bib.bib70)] | 2016 | ADNI | 211 | - | - | 91 | sMRI
    | 2D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | - | - | 92 | rs-fMRI |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[72](#bib.bib72)] | 2017 | ADNI | 93 | 146 | 100 | PET | 2D-CNN+RNN
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Payan et al. [[73](#bib.bib73)] | 2015 | ADNI | 755 | 755 | 755 | sMRI |
    3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hosseini-Asl et al. [[74](#bib.bib74)] | 2016 | ADNI | 70 | 70 | 70 | sMRI
    | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Karasawa et al. [[75](#bib.bib75)] | 2017 | ADNI | 348 | 450 | 358 | 574
    | sMRI | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[76](#bib.bib76)] | 2018 | ADNI | 93 | 76 | 128 | 100 | sMRI+PET
    | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[77](#bib.bib77)] | 2014 | ADNI | 193 | 167 | 236 | 229 | sMRI+
    PET | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[78](#bib.bib78)] | 2018 | ADNI | 358 | 205 | 465 | 429 | sMRI
    | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[79](#bib.bib79)] | 2018 | ADNI | 358 | - | - | 429 | sMRI |
    3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pan et al. [[80](#bib.bib80)] | 2018 | ADNI | 358 | 205 | 465 | 429 | sMRI+
    PET | 3D-CNN+GAN |  |'
  prefs: []
  type: TYPE_TB
- en: Because the 3D brain images need to be decomposed into 2D slices in the preprocessing
    step, it resulted in 2D-CNN methods discarded the spatial information. Therefore,
    many 3D-CNN methods were proposed, which can directly input 3D brain images. Payan et
    al. was pre-trained a 3D-CNN through a sparse AE on small 3D patches from sMRI
    scans [[73](#bib.bib73)]. Hosseini-Asl et al.  proposed a deep 3D-CNN, which was
    built upon a 3D CAE to capture anatomical shape variations in sMRI scans [[74](#bib.bib74)].
    Liu et al. used multiple deep 3D-CNN on different local image patches to learn
    the discriminative features of MRI and PET images. Then a set of upper high-level
    CNN was cascaded to ensemble the learned local features and discovered the latent
    multi-modal features for AD classification [[76](#bib.bib76)]. Karasawa et al. proposed
    deeper 3D-CNN architecture with 39 layers based on residual learning framework(ResNet)
    to improve performance [[75](#bib.bib75)]. Liu et al. designed a landmark-based
    deep feature learning framework to learn the patch-level features, which were
    an intermediate scale between voxel-level and ROI-level [[79](#bib.bib79)]. The
    authors firstly used a data-driven manner to identify discriminative anatomical
    landmarks from MR images, and then proposed a 3D-CNN to learn patch-based features.
    This strategy can avoid the high-dimensional problem of voxel-level and manual
    definition of ROI-level. Subsequently, Liu et al. developed a deep multi-instance
    CNN framework [[78](#bib.bib78)], where multiple image patches were used as a
    bag of instances to represent each specific subject, and then the label of each
    bag was given by the whole-image-level class label. To overcome the missing modality
    in multi-modal image data, Li et al. [[77](#bib.bib77)] proposed a simple 3D-CNN
    to predict the missing PET images from the sMRI data. Results showed that the
    predicted PET data achieved similar classification accuracy as the true PET data.
    Additionally, the synthetic PET data and the real sMRI data obviously outperformed
    the single sMRI data. Pan et al. used Cycle-GAN to learn bi-directional mapping
    sMRI and PET, to synthesize missing PET scans based on its corresponding sMRI
    scans. Then, landmark-based 3D-CNN was adapted for AD classification on the mixed
    image data [[80](#bib.bib80)]. Table [3](#S3.T3 "Table 3 ‣ 3.1 Deep Learning for
    Alzheimer’s Disease Analysis ‣ 3 Applications in Brain Disorder Analysis with
    Medical Images ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder
    Analysis") and Table [4](#S3.T4 "Table 4 ‣ 3.1 Deep Learning for Alzheimer’s Disease
    Analysis ‣ 3 Applications in Brain Disorder Analysis with Medical Images ‣ A Survey
    on Deep Learning for Neuroimaging-based Brain Disorder Analysis") summarized the
    statistic information of each paper reviewed above for AD diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The classification performance of papers for AD diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Accuracy$(\%)$ |'
  prefs: []
  type: TYPE_TB
- en: '| AD/NC | AD/MCI | MCI/NC | cMCI/sMCI | 3-ways¹ | 4-ways² |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[61](#bib.bib61)] | 91.4$\pm$ 1.8 | 70.1$\pm$ 2.3 | 77.4$\pm$
    1.7 | 57.4$\pm$ 3.6 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[64](#bib.bib64)] | 91.4$\pm$ 5.56 | - | 82.10$\pm$ 4.91 | -
    | - | 53.79 |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[62](#bib.bib62)]. | 95.35 $\pm$ 5.23 | - | 85.67$\pm$ 5.22 |
    75.92$\pm$ 15.37 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[16](#bib.bib16)] | 98.8 $\pm$0.9 | 83.7$\pm$1.5 | 90.7$\pm$1.2
    | 83.3 $\pm$2.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[63](#bib.bib63)] | 95.09$\pm$ 2.28 | - | 80.11 $\pm$ 2.64 |
    74.15 $\pm$ 3.35 | 62.93 | 53.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 90.27 | - | 70.86 | 73.93 | 57.74 | 47.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. [[65](#bib.bib65)] | 91.95 $\pm$1.00 | - | 83.72$\pm$ 1.16 | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lu et al. [[66](#bib.bib66)] | 93.58 $\pm$ 5.2 | - | - | 81.55 $\pm$7.42
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Martinez-Murcia et al. [[67](#bib.bib67)] | 84.3$\pm$6 | - | - | 71.5$\pm$9
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Gupta et al. [[68](#bib.bib68)] | 94.74 | 88.10 | 86.35 | - | 85.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[69](#bib.bib69)] | 97.18$\pm$1.5 | 94.51$\pm$1.43 | 93.21$\pm$1.02
    | - | 91.72$\pm$1.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Billones  et al. [[71](#bib.bib71)] | 98.33 | 93.89 | 91.67 | - | 91.85 |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Sarraf et al. [[70](#bib.bib70)] | 98.84 /99.90 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu  et al. [[72](#bib.bib72)] | 91.92 | - | 78.9 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Payan et al. [[73](#bib.bib73)] | 95.39 | 86.84 | 92.11 | - | 89.47 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Hosseini-Asl et al. [[74](#bib.bib74)] | 99.3$\pm$1.6 | 100 | 94.2$\pm$2.0
    | - | 94.8$\pm$2.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Karasawa et al. [[75](#bib.bib75)] | 94.0 | - | 90.0 | - | 87.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[76](#bib.bib76)] | 93.26 | - | 73.34 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[77](#bib.bib77)] | 92.87$\pm$2.07 | - | 76.21$\pm$2.05 | 72.44$\pm$2.41
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[78](#bib.bib78)] | 91.09 | - | - | 76.90 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[79](#bib.bib79)] | 90.56 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pan et al. [[80](#bib.bib80)] | 92.50 | - | - | 79.06 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3-ways represents the comparison: AD vs. NC vs. MCI.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4-ways represents the comparison: AD vs. NC vs. cMCI vs. sMCI.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As an early stage of AD, MCI had a conversion rate as high as 10%-15% per year
    in 5 years, but MCI was also the best time for treatment. Therefore, an effective
    predictive model construction for the early diagnosis of MCI had become a hot
    topic. Recently, some research based on GCN has been done for MCI prediction.
    Zhao et al. and Yu et al. both used GCN which combines neuroimaging information
    and demographic relationship for MCI prediction [[81](#bib.bib81), [82](#bib.bib82)].
    Song et al.  implemented a multi-class GCN classifier for classification of subjects
    on the AD spectrum into four classes [[83](#bib.bib83)]. Guo et al. proposed PETNET
    model based on GCN to analyzes PET signals defined on a group-wise inferred graph
    structure [[84](#bib.bib84)]. Table [5](#S3.T5 "Table 5 ‣ 3.1 Deep Learning for
    Alzheimer’s Disease Analysis ‣ 3 Applications in Brain Disorder Analysis with
    Medical Images ‣ A Survey on Deep Learning for Neuroimaging-based Brain Disorder
    Analysis") and Table [6](#S3.T6 "Table 6 ‣ 3.1 Deep Learning for Alzheimer’s Disease
    Analysis ‣ 3 Applications in Brain Disorder Analysis with Medical Images ‣ A Survey
    on Deep Learning for Neuroimaging-based Brain Disorder Analysis") summarized the
    four papers for MCI prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Overview of papers using deep learning techniques for MCI prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Database | Subjects | Modality | Model |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NC | EMCI | LMCI | AD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[81](#bib.bib81)] | 2019 | ADNI | 67 | 77 | 40 | - | rs-fMRI
    | GCN |'
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. [[82](#bib.bib82)] | 2019 | ADNI | 44 | 44 | 38 | - | rs-fMRI |
    GCN |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[83](#bib.bib83)] | 2019 | ADNI | 12 | 12 | 12 | 12 | DTI |
    GCN |'
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. [[84](#bib.bib84)] | 2019 | ADNI | 100 | 96 | 137 | - | PET |
    GCN |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The classification performance of papers for MCI prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Accuracy$(\%)$ |'
  prefs: []
  type: TYPE_TB
- en: '| EMCI/NC | LMCI/NC | EMCI/LMIC | MCI/NC | 3-ways¹ | 4-ways² |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[81](#bib.bib81)] | 78.4 | 84.3 | 85.6 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. [[82](#bib.bib82)] | 87.5 | 89.02 | 79.27 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[83](#bib.bib83)] | - | - | - | - | - | 89.0$\pm$6 |'
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. [[84](#bib.bib84)] | - | - | - | 93.0³ | 77.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3-ways represents the comparison: NC vs. EMCI vs. LMCI.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4-ways represents the comparison: NC vs. EMCI vs. LMCI vs. AD.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCI = ECMI+LMCI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Deep Learning for Parkinson’s Disease Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parkinson’s disease (PD) is the most common neurodegenerative disorder, after
    Alzheimer’s disease, and is provoked by progressive impairment and deterioration
    of neurons, caused by a gradually halt in the production of a chemical messenger
    in the brain. Parkinson’s Progression Markers Initiative (PPMI) is an observational
    clinical study to verify progression markers in Parkinson’s disease. The PPMI
    cohort comprises 400 newly diagnosed PD cases, 200 healthy, and 70 individuals,
    while clinically diagnosed as PD cases, fail to show evidence of dopaminergic
    deficit. This latter group of patients is referred to as SWEDDs (Scans without
    Evidence of Dopamine Deficit) [[85](#bib.bib85)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Some efforts based on deep learning have been done to designed algorithms to
    help PD diagnosis. Martinez-Murci team has continuously published a series of
    papers using deep learning techniques for PD diagnose in SPECT image dataset.
    Ortiz et al. designed a framework to automatically diagnose PD using deep sparse
    filtering-based features [[86](#bib.bib86)]. Sparse filtering, based on $\ell_{2}$-norm
    regularization, extracted the suitable features, which can be used as the weight
    of hidden layers in a three-layer DNN. Subsequently, this team firstly applied
    3D-CNN in PD diagnosis. These methods achieved up to a 95.5$\%$ accuracy and 96.2$\%$
    sensitively [[87](#bib.bib87)]. However, this 3D-CNN architecture with only two
    convolutional layers was too shallow and limited the capability to extract more
    discriminative features. Therefore, Martinez-Murcia et al. proposed a deep convolutional
    AE (DCAE) architecture for feature extraction [[88](#bib.bib88)]. The DCAE overcome
    two common problems: the need for spatial normalization and the effect of imbalanced
    datasets. For a strongly imbalanced (5.69/1) PD dataset, DCAE achieved more than
    93$\%$ accuracy. Choi et al. developed a deep CNN model (PDNet) consisted of four
    3D convolutional layers [[89](#bib.bib89)]. PDNet obtained high classification
    accuracy compared to the quantitative results of expert assessment, and can further
    classify the SWEDD and NC subjects. Esmaeilzadeh et al. both utilized the sMRI
    scans and demographic information (i.e., age and gender) of patients to train
    a 3D-CNN model [[90](#bib.bib90)]. The proposed method firstly found that the
    $Superior$ $Parietal$ part on the right hemisphere of the brain was critical in
    PD diagnosis. Sivaranjini et al. directly introduced the AlexNet model, which
    was trained by the transfer learned network [[91](#bib.bib91)]. Shen et al. proposed
    an improved DBN model with an overlapping group lasso sparse penalty to learn
    useful low-level feature representations [[92](#bib.bib92)]. To incorporate multiple
    brain neuroimaging modalities, McDaniel et al. and Zheng et al. both used a GCN
    model and presented an end-to-end pipeline without extra parameters involved for
    view pooling and pairwise matching [[93](#bib.bib93), [94](#bib.bib94)]. Table
    [7](#S3.T7 "Table 7 ‣ 3.2 Deep Learning for Parkinson’s Disease Analysis ‣ 3 Applications
    in Brain Disorder Analysis with Medical Images ‣ A Survey on Deep Learning for
    Neuroimaging-based Brain Disorder Analysis") summarized each paper above reviewed
    for PD diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Overview of papers using deep learning techniques for PD diagnose.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Database | Modality | Method | Modality | Accuracy$(\%)$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | PD | NC | SWEED | PD/NC | SWEED/NC |'
  prefs: []
  type: TYPE_TB
- en: '| Ortiz et al. [[86](#bib.bib86)] | 2016 | PPMI | SPECT | DNN | - | - | - |
    95.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Martinez-Murcia et al. [[87](#bib.bib87)] | 2017 | PPMI | SPECT | 3D-CNN
    | 158 | 111 | 32 | 95.5$\pm$4.4 | 82.0$\pm$6.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Choi et al. [[89](#bib.bib89)] | 2017 | PPMI | SPECT | 3D-CNN | 431 | 193
    | 77 | 96.0 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SNUH¹ | SPECT | 72 | 10 | - | 98.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Esmaeilzadeh et al. [[90](#bib.bib90)] | 2018 | PPMI | sMRI+DI⁴ | 3D-CNN
    | 452 | 204 | - | 1.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Martinez-Murcia et al. [[88](#bib.bib88)] | 2018 | PPMI | SPECT | DCAE |
    1110 | 195 | - | 93.3$\pm$1.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Sivaranjini et al. [[91](#bib.bib91)] | 2019 | PPMI | SPECT | 2D-CNN | 100
    | 82 | - | 88.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| McDaniel et al. [[93](#bib.bib93)] | 2019 | PPMI | sMRI+DTI | GCNN | 117
    | 30 | - | 92.14 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | 2018 | PPMI | sMRI+DTI | GCNN | 596 | 158
    | - | 95.37(AUC) | - |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[92](#bib.bib92)] | 2019 | HSHU² | PET | DBN | 100 | 200 | -
    | 90.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| WXH³ | PET | 25 | 25 | - | 86.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SNUH: Seoul National University Hospital cohort.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HSH: HuaShan Hospital cohort.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WXH: WuXi 904 Hospital cohort.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DI: Demographic Information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3 Deep Learning for Austism Spectrum Disorder Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Autism spectrum disorder (ASD) is a common neurodevelopmental disorder, which
    has affected 62.2 million ASD cases in the world in 2015. The Autism Imaging Data
    Exchange (ABIDE) initiative had aggregated rs-fMRI brain scans, anatomical and
    phenotypic datasets, collected from laboratories around the world. The ABIDE initiative
    included two large scale collections: ABIDE I and ABIDE II, which were released
    in 2012 and 2016, respectively. The ABIDE I collection involved 17 international
    sites, and consisted of 1,112 subjects comprised of 539 from autism patients and
    573 from NC. To further enlarge the number of samples with better-characterized,
    the ABIDE II collection involved 19 international sites, and aggregated 1114 subjects
    from 521 individuals with ASD and 593 NC subjects [[95](#bib.bib95)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many methods have been proposed on the application of deep learning for ASD
    diagnosis. These methods can be divided into three categories: AE-based methods,
    convolutional-based methods, and RNN-based methods. AE-based methods used various
    AE variations or stacked multiple AE to reduce data dimension and discovery highly
    discriminative representations. Hazlett et al. implemented the basic SAE which
    primarily used surface area information from brain MRI at 6 and 12 month old infants
    to predict the 24 month diagnosis of autism in children at high familial risk
    for autism. The SAE contained 3 hidden layers to reduce 315 dimension measurements
    to only 2 features [[96](#bib.bib96)]. Two papers both used a stacked multiple
    sparse AE (SSAE) to learn low dimensional high-quality representations of functional
    connectivity patterns [[97](#bib.bib97), [98](#bib.bib98)]. But the difference
    was that Guo et al. input the whole-brain functional connectivity patterns and
    Kong et al. only selected top 3,000 ranked connectivity features by F-score in
    descending order. Dekhil et al. built an automated autism diagnosis system, which
    used 34 sparse AE for 34 spatial activation areas respectively [[99](#bib.bib99)].
    Each sparse AE extracted the power spectral densities (PSDs) of time courses in
    a higher level representation and simultaneously reduced the feature vectors dimensionality.
    Choi et al. used VAE to summarize the functional connectivity networks into two-dimensional
    features [[100](#bib.bib100)]. One feature was identified with a highly discrimination
    between ASD and NC, and was closely associated with ASD-related brain regions.
    Heinsfeld et al. used DAE to reduce the effect of multi-site heterogeneous data
    and improve the generalization [[101](#bib.bib101)]. Due to insufficient training
    samples, Li et al. developed a novel deep neural network framework with the transfer
    learning technique for enhancing ASD classification [[102](#bib.bib102)]. This
    framework firstly trained an SSAE to learn functional connectivity patterns from
    healthy subjects in the existing databases. Then the trained SSAE was transferred
    to a new classification with limited target subjects. Saeed et al. designed a
    data augmentation strategy to produce synthetic datasets needed for training the
    ASD-DiagNet model. This model was composed of an AE and a single-layer perceptron
    to improve the quality of extracted features [[103](#bib.bib103)].'
  prefs: []
  type: TYPE_NORMAL
- en: Due to collapsed the rs-fMRI scans into a feature vector, above methods discarded
    the spatial structure of the brain networks. To fully utilize the whole brain
    spatial fMRI information, Li et al. implemented 3D-CNN to capture spatial structure
    information and used sliding windows over time to measure temporal statistics [[104](#bib.bib104)].
    This model was able to learn ASD related biological markers from the output of
    the middle convolution layer. Khosla et al. proposed a 3D-CNN framework for connectome-based
    classification. The functional connectivity of each voxel to various target ROIs
    was used as input features, which reserved the spatial relationship between voxels.
    Then the ensemble learning strategy was employed to average the different ROI
    definitions to reduce the effect of empirical selections, and obtained more robust
    and accurate results [[105](#bib.bib105)]. Ktena et al. implemented a siamese
    GCN to learn a graph similarity metric, which took the graph structure into consideration
    for the similarity between a pair of graphs [[106](#bib.bib106)]. This was the
    first application of metric learning with graph convolutions on brain connectivity
    networks. Parisot et al. introduced a spectral GCN for brain analysis in populations
    combining imaging and non-imaging information [[107](#bib.bib107)]. The populations
    were represented as a sparse graph where each vertex corresponded to an imaging
    feature vector of a subject and the edge weights were associated with phenotypic
    data, such as age, gender, acquisition sites. Like the graph-based label propagation,
    a GCN model was used to infer the classes of unlabelled nodes on the partially
    labeled graphs. There existed no definitive method to construct reliable graphs
    in practice. Thus, Anirudh et al. proposed a bootstrapped version of GCN to reduce
    the sensitivity of models on the initial graph construction step [[108](#bib.bib108)].
    The bootstrapped GCN used an ensemble of weakly GCN, each of which was trained
    by a random graph. In addition, Yao et al. proposed a multi-scale triplet GCN
    to avoid the spatial limitation of a single template [[109](#bib.bib109)]. A multi-scale
    templates for coarse-to-fine ROI parcellation were applied to construct multi-scale
    functional connectivity patterns for each subject. Then a triple GCN model was
    developed to learn multi-scale graph features of brain networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Overview of papers using deep learning techniques for ASD diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Database | Subject | Modality | Model | Accuracy$(\%)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASD | NC |'
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. [[97](#bib.bib97)] | 2017 | ABIDE I | 55 | 55 | rs-fMRI | SSAE
    | 86.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. [[98](#bib.bib98)] | 2019 | ABIDE I | 78 | 104 | rs-fMRI | SSAE
    | 90.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[102](#bib.bib102)] | 2018 | ABIDE: UM¹ | 48 | 65 | rs-fMRI |
    SSAE | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ABIDE:UCLA² | 36 | 39 | 62.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ABIDE: USM³ | 38 | 23 | 70.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ABIDE: LEUVEN⁴ | 27 | 34 | 68.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Choi et al. [[100](#bib.bib100)] | 2017 | ABIDE | 465 | 507 | rs-fMRI | VAE
    | 0.60(AUC) |'
  prefs: []
  type: TYPE_TB
- en: '| Heinsfeld et al. [[101](#bib.bib101)] | 2018 | ABIDE | 505 | 530 | rs-fMRI
    | DAE | 70.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Hazlett et al. [[96](#bib.bib96)] | 2017 | NDAR⁵ | 106 | 42 | rs-fMRI | SAE
    | 88.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dekhil et al. [[99](#bib.bib99)] | 2018 | NDAR | 123 | 160 | rs-fMRI | SSAE
    | 91.0$\pm$3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Saeed et al. [[103](#bib.bib103)] | 2019 | ABIDE | 505 | 530 | rs-fMRI |
    AE | 70.1$\pm$3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[104](#bib.bib104)] | 2018 | - | 82 | 48 | rs-fMRI | 3D-CNN |
    89.0$\pm$5.0(F-score) |'
  prefs: []
  type: TYPE_TB
- en: '| Khosla et al. [[105](#bib.bib105)] | 2018 | ABIDE | 542 | 625 | rs-fMRI |
    3D-CNN | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Ktena et al. [[106](#bib.bib106)] | 2017 | ABIDE | 403 | 468 | rs-fMRI |
    GCN | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Parisot et al. [[107](#bib.bib107)] | 2017 | ABIDE | 403 | 468 | rs-fMRI
    | GCN | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Anirudh et al. [[108](#bib.bib108)] | 2017 | ABIDE | 404 | 468 | rs-fMRI
    | GCN | 70.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Yao et al. [[109](#bib.bib109)] | 2019 | ABIDE | 438 | 544 | rs-fMRI | GCN
    | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Bi et al. [[110](#bib.bib110)] | 2018 | ABIDE | 50 | 42 | rs-fMRI | RNN |
    84.7$\pm$3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Dvornek et al. [[111](#bib.bib111)] | 2017 | ABIDE | 1100 | - | rs-fMRI |
    LSTM | 68.5$\pm$5.5 |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: University of Michigan.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: University of California, Los Angeles.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: University of Utah School of Medicine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katholieke Universiteit Leuven.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: National Database of Autism Research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Several RNN-based methods were proposed to fully utilize the temporal information
    in the rs-fMRI time-series data. Bi et al. designed a random NN cluster, which
    combined multiple NNs into a model, to improve the classification performance
    in the diagnosis of ASD [[110](#bib.bib110)]. Compared to five different NNs,
    the random Elman cluster obtained the highest accuracy. It is because that the
    Elman NN fit handling the dynamic data. Dvornek et al. first applied LSTM to ASD
    classification, which directly used the rs-fMRI time-series data, rather than
    the pre-calculated measures of brain functional connectively [[111](#bib.bib111)].
    The authors thought that the rs-fMRI time-series data contained more useful information
    of dynamic brain activity than single and static functional connectivity measures.
    For clarity, the important information of the above-mentioned papers was summarized
    in Table [8](#S3.T8 "Table 8 ‣ 3.3 Deep Learning for Austism Spectrum Disorder
    Analysis ‣ 3 Applications in Brain Disorder Analysis with Medical Images ‣ A Survey
    on Deep Learning for Neuroimaging-based Brain Disorder Analysis").
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Deep Learning for Schizophrenia Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Schizophrenia (SZ) is a prevalent psychiatric disorder and affects 1% of the
    population worldwide. Due to the complex clinical symptoms, the pathological mechanism
    of schizophrenia remains unclear and there is no definitive standard in the diagnosis
    of SZ. Different from the ADNI for AD diagnosis, the PPMI for PD diagnosis and
    the ABIDE for ASD diagnosis, there was not a widely used neuroimaging dataset
    for the SZ diagnosis. Therefore, many studies utilized various source datasets
    that were available from the medical research centers, universities and hospitals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, some studies have been successfully applied deep learning algorithms
    to SZ diagnosis and received significant improvement. These methods were divided
    into two categories: unimodality and multi-modality, according to the types of
    input data, rather than according to deep learning architectures like AD or ASD
    diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unimodality category only used a single type of MRI, and can further be
    classified into subclasses: sMRI-methods and fMRI-methods. sMRI-methods discovery
    latent features from sMRI dataset, which can provide information on the tissue
    structure of the brain, such as gray matter, white matter and cerebrospinal fluid.
    Plis et al. and Pinaya et al. used the DBN model, which only contained three hidden
    layers, to automatically extract feature for SZ identification. The results achieved
    modestly higher predictive performance than the shallow-architecture SVM approach [[112](#bib.bib112),
    [113](#bib.bib113)]. Different from DBN model in [[113](#bib.bib113)], Pinaya et
    al. trained an SAE to create a normative model from 1113 NC subjects, then used
    this model to estimate total and regional neuroanatomical deviation in individual
    patients with SZ [[114](#bib.bib114)]. Ulloa et al. proposed a novel classification
    architecture that used synthetic sMRI scans to mitigate the effects of a limited
    sample size. To generate synthetic samples, a data-driven simulator was designed
    that can capture statistical properties from observed data using independent component
    analysis (ICA) and a random variable sampling method. Then a 10-layer DNN was
    trained exclusively on continuously generated synthetic data, and greatly improves
    generalization in the classification of SZ patients and NC [[115](#bib.bib115)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Overview of papers using deep learning techniques for SZ diagnosis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Database | Subject | Modality | Model | Accuracy$(\%)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SZ | NC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Plis et al. [[112](#bib.bib112)] | 2014 | Multi-site1¹ | 198 | 191 | sMRI
    | DBN | 91.0+14(F-score) |'
  prefs: []
  type: TYPE_TB
- en: '| Ulloa et al. [[115](#bib.bib115)] | 2015 | Multi-site1 | 198 | 191 | sMRI
    | DNN | 75.0$\pm$4(AUC) |'
  prefs: []
  type: TYPE_TB
- en: '| Pinaya et al. [[113](#bib.bib113)] | 2016 | UNIFESP² | 143 | 83 | sMRI |
    DBN | 73.55$\pm$6.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Pinaya et al. [[114](#bib.bib114)] | 2019 | NUSDAST³ | 30 | 40 | sMRI | SAE
    | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. [[116](#bib.bib116)] | 2015 | NITRC⁴ | 50 | 50 | rs-fMRI | DNN
    | 85.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Patel et al. [[117](#bib.bib117)] | 2016 | COBRE⁵ | 72 | 74 | rs-fMRI | SAE
    | 92.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al. [[118](#bib.bib118)] | 2018 | Multi-site2⁶ | 357 | 377 | rs-fMRI
    | SAE | 85.0$\pm$1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Qureshi  et al. [[119](#bib.bib119)] | 2019 | COBRE | 72 | 74 | rs-fMRI |
    3D-CNN | 98.09$\pm$1.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Dakka et al. [[120](#bib.bib120)] | 2017 | FBIRN⁷ | 46 | 49 | rs-fMRI | CNN+LSTM
    | 66.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Yan et al. [[121](#bib.bib121)] | 2019 | Multi-site3⁸ | 558 | 542 | rs-fMRI
    | CNN+GRU | 83.2$\pm$3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. [[122](#bib.bib122)] | 2016 | MLSP2014 | 69 | 75 | sMRI+fMRI |
    DCCA/DCCAE | 94.2/95.0(AUC) |'
  prefs: []
  type: TYPE_TB
- en: '| Srinivasagopalan et al. [[123](#bib.bib123)] | 2019 | MLSP2014 | 69 | 75
    | sMRI+fMRI | DNN | 94.44 |'
  prefs: []
  type: TYPE_TB
- en: '| Ulloa et al. [[124](#bib.bib124)] | 2018 | FBIRN | 135 | 169 | sMRI+fMRI
    | DNN | 85.0$\pm$5.0(AUC) |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johns Hopkins University; the Maryland Psychiatric Research Center; the Institute
    of Psychiatry; the Western Psychiatric Institute and Clinic at the University
    of Pittsburgh.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Universidade Federal de São Paulo.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Northwestern University Schizophrenia Data and Software Tool.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neuroimaging Informatics Tools and Resources Clearinghouse website.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Center for Biomedical Research Excellence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xijing Hospital; First Affliated Hospital of Anhui Medical University; Second
    Xiangya Hospital; COBRE; the University of California, Los Angles and Washington
    University School of Medicine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Function Biomedical Informatics Research Network Data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peking University Sixth Hospital; Beijing Huilongguan Hospital; Xinxiang Hospital;
    Xinxiang Hospital; Xijing Hospital; Renmin Hospital of Wuhan University; Zhumadian
    Psychiatric Hospital.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The fMRI-methods extracted discriminative features from rs-fMRI brain images
    with functional connectivity networks. Kim et al. learned lower-to-higher features
    via the DNN model, of which each hidden layer was added $L_{1}$-regulation to
    control the weight sparsity, and achieve 85.8% accuracy [[116](#bib.bib116)].
    Patel et al.  used an SAE model with four hidden layers to separately train on
    each brain region. The input layer directly uses the complete time series of all
    active voxels without converting them into region-wise mean time series. Thus,
    this made that the model retained more information [[117](#bib.bib117)]. Due to
    the limited size of SZ dataset, Zeng et al. collected a large multi-site rs-fMRI
    dataset from seven neuroimaging resources. An SAE with an optimized discriminant
    item was designed to learn imaging site-shared functional connectivity features.
    This model can achieve accurate SZ classification performance across multiple
    independent imaging sites, and the learned features found that dysfunctional integration
    of the cortical-striatal-cerebellar circuit may play an important role in SZ [[118](#bib.bib118)].
    Qureshi et al. built a 3D-CNN based deep learning classification framework, which
    used the 3D ICA functional network maps as input. These ICA maps served as highly
    discriminative 3D imaging features for the discrimination of SZ [[119](#bib.bib119)].
    To exploit both spatial and temporal information, Dakka et al.  and Yan et al. proposed
    a recurrent convolutional neural network involving CNN followed by LSTM and GRU,
    respectively. The CNN extracted spatial features, which then were fed to the followed
    RNN model to learn the temporal dependencies [[120](#bib.bib120), [121](#bib.bib121)]
    .
  prefs: []
  type: TYPE_NORMAL
- en: As known to all, combined multi-modality brain images can improve the performance
    of disorder diagnosis. The MLSP2014 (Machine Learning for Signal Processing) SZ
    classification challenge provided 75 NC and 69 SZ which both contained sMRI and
    rs-fMRI brain images. Qi et al. used deep canonical correlation analysis (DCCA)
    and deep canonically correlated auto-encoders (DCCAE) to fuse multi-modality features [[122](#bib.bib122)].
    But in the proposed method, two modalities features directly were combined as
    411 dimensional vector, then fed to 3-layer DNN model [[123](#bib.bib123)]. To
    alleviate the missing modality, the synthetic sMRI and rs-fMRI images were generated
    by a generator proposed, and then were used to train a multi-modalities DNN [[124](#bib.bib124)].
    For clarity, the important information of the above-mentioned papers was summarized
    in Table [9](#S3.T9 "Table 9 ‣ 3.4 Deep Learning for Schizophrenia Analysis ‣
    3 Applications in Brain Disorder Analysis with Medical Images ‣ A Survey on Deep
    Learning for Neuroimaging-based Brain Disorder Analysis"). From this table, it
    can be seen the datasets for SZ diagnosis come from different universities, hospitals
    and medical centers.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion and Future Direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although deep learning models had achieved great success in the field of neuroimaging-based
    brain disorder analysis, there are still some challenges that deserve further
    investigation. We summarize these potential challenges as follows and explore
    possible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, deep learning methods require a large number of samples to train neural
    networks, while it’s usually difficult to acquire training samples in many real-world
    scenarios, especially for neuroimaging data. The lack of sufficient training data
    in neuroimage analysis has been repeatedly mentioned as a challenge to apply deep
    learning algorithms. To address this challenge, data augmentation strategy has
    been proposed and widely used to enlarge the number of training samples. In addition,
    the use of transfer learning [[125](#bib.bib125), [126](#bib.bib126)] provides
    another solution, by transferring well-trained networks on big sample datasets
    (related to the to-be-analyzed disease) to a small sample dataset for further
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the missing data problem is unavoidable in multimodal neuroimaging
    studies, because subjects may lack some modalities due to patient dropouts and
    poor data quality. Conventional methods typically discard data-missing subjects,
    which will significantly reduce the number of training subjects and degrade the
    diagnosis performance. Although many data imputing methods have been proposed,
    most of them focus on imputing missing hand-crafted feature values that are defined
    by experts for representing neuroimages, while the hand-crafted features themselves
    could be not discriminative for disease diagnosis and prognosis. Several recent
    studies [[80](#bib.bib80), [127](#bib.bib127)] propose to directly impute missing
    neuroimages (e.g., PET) based on another modality neuroimages (e.g., MRI), while
    the correspondence between imaging data and non-imaging data has not been explored.
    We expect to see more deep network architectures in the near future to explore
    the association between different data modalities for imputing those missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, an effective fusion of multimodal data has always been a challenge
    in the field. Multimodal data reflects the morphology, structure and physiological
    functions of normal tissues and organs from different aspects, and has strong
    complementary characteristics between different models. Previous studies for multimodal
    data fusion can be divided into two categories, data-level fusion (focus on how
    to combine data from different modalities) and decision-level fusion (focus on
    ensembling classifiers). Deep neural network architectures allow a third form
    of multimodal fusion, i.e., the intermediate fusion of learned representations,
    offering a truly flexible approach to multimodal fusion. As deep-learning architectures
    learn a hierarchical representation of underlying data across its hidden layers,
    learned representations between different modalities can be fused at various levels
    of abstraction. Further investigation is desired to study which layer of deep
    integration is optimal for problems at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, different imaging modalities usually reflect different temporal
    and spatial scales information of the brain. For example, sMRI data reflect minute-scale
    time scales information of the brain, while fMRI data can provide second-scale
    time scales information. In the practical diagnosis of brain disorder, it shows
    great significance for the implementation of early diagnosis and medical intervention
    by correctly introducing the spatial relationship of the diseased brain regions
    and other regions and the time relationship of the development of the disease
    progress [[128](#bib.bib128), [129](#bib.bib129)]. Although previous studies have
    begun to study the pathological mechanisms of brain diseases on a broad temporal
    and spatial scales, those methods usually consider either temporal or spatial
    characteristics. Therefore, it is desired to develop a series of deep learning
    frameworks to fuse temporal and spatial information for automated diagnosis of
    brain disorder.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the utilization of multi-site data for disease analysis has recently
    attracted increasing attention [[101](#bib.bib101), [130](#bib.bib130), [131](#bib.bib131)],
    since a large number of subjects from multiple imaging sites are beneficial for
    investigating the pathological changes of disease-affected brains. Previous methods
    often suffer from inter-site heterogeneity caused by different scanning parameters
    and subject populations in different imaging sites, by assuming that these multi-site
    data are drawn from the same data distribution. Constructing accurate and robust
    learning models using heterogeneous multi-site data is still a challenging task.
    To alleviate the inter-site data heterogeneity, it could be a promising way to
    simultaneously learn adaptive classifiers and transferable features across multiple
    sites.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we reviewed the most recent studies on the subject of applying
    the deep learning techniques in neuroimaging-based brain disorder analysis, and
    focused on four typical disorders. AD and PD are both neurodegenerative disorder.
    ASD and SZ are neurodevelopmental and psychiatric disorders, respectively. Deep
    learning models have achieved state-of-the-art performance across the four brain
    disorders using brain images. Finally, we summarize these potential challenges
    and discuss possible research directions. With the clearer pathogenesis of human
    brain disorders, the further development of deep learning techniques, the larger
    size of open source datasets, a human-machine collaboration for medical diagnosis
    and treatment will ultimately become a symbiosis in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by National Natural Science Foundation of China
    (NSFC) under grants (Nos. 61802193, 61876082, 61861130366, and 61473149), the
    Natural Science Foundation of Jiangsu Province under grants (BK20170934), the
    Royal Society-Academy of Medical Sciences Newton Advanced Fellowship (No. NAF$\backslash$R1$\backslash$180371),
    and the Fundamental Research Funds for the Central Universities (No. NP2018104),
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Herb Brody. Medical imaging. Nature, 502(7473):S81–S81, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Axel Heidenreich, F Desgrandschamps, and F Terrier. Modern approach of
    diagnosis and management of acute flank pain: review of all imaging modalities.
    European urology, 41(4):351–362, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Miles N Wernick, Yongyi Yang, Jovan G Brankov, Grigori Yourganov, and Stephen C
    Strother. Machine learning in medical imaging. IEEE signal processing magazine,
    27(4):25–38, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Bradley J Erickson, Panagiotis Korfiatis, Zeynettin Akkus, and Timothy L
    Kline. Machine learning for medical imaging. Radiographics, 37(2):505–515, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Guorong Wu, Dinggang Shen, and Mert Sabuncu. Machine Learning and Medical
    Imaging. Academic Press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Mrudang D Pandya, Parth D Shah, and Sunil Jardosh. Medical image diagnosis
    for disease detection: A deep learning approach. In U-Healthcare Monitoring Systems,
    pages 37–60\. Elsevier, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning, volume 1. MIT press Cambridge, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural
    networks, 61:85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios
    Protopapadakis. Deep learning for computer vision: A brief review. Computational
    intelligence and neuroscience, 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ruhi Sarikaya, Geoffrey E Hinton, and Anoop Deoras. Application of deep
    belief networks for natural language understanding. IEEE/ACM Transactions on Audio,
    Speech and Language Processing (TASLP), 22(4):778–784, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and
    Yoshua Bengio. End-to-end attention-based large vocabulary speech recognition.
    In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference
    on, pages 4945–4949\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] June-Goo Lee, Sanghoon Jun, Young-Won Cho, Hyunna Lee, Guk Bae Kim, Joon Beom
    Seo, and Namkug Kim. Deep learning in medical imaging: general overview. Korean
    journal of radiology, 18(4):570–584, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical
    image analysis. Annual review of biomedical engineering, 19:221–248, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso
    Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen AWM van der Laak, Bram Van Ginneken,
    and Clara I Sánchez. A survey on deep learning in medical image analysis. Medical
    image analysis, 42:60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Heung-Il Suk, Seong-Whan Lee, Dinggang Shen, Alzheimer’s Disease Neuroimaging
    Initiative, et al. Latent feature representation with stacked auto-encoder for
    ad/mci diagnosis. Brain Structure and Function, 220(2):841–859, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Holger R Roth, Le Lu, Ari Seff, Kevin M Cherry, Joanne Hoffman, Shijun
    Wang, Jiamin Liu, Evrim Turkbey, and Ronald M Summers. A new 2.5 d representation
    for lymph node detection using random sets of deep convolutional neural network
    observations. In International conference on medical image computing and computer-assisted
    intervention, pages 520–527\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Guorong Wu, Minjeong Kim, Qian Wang, Yaozong Gao, Shu Liao, and Dinggang
    Shen. Unsupervised deep feature learning for deformable registration of mr brain
    images. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 649–656\. Springer, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Zhongyu Li, Xiaofan Zhang, Henning Müller, and Shaoting Zhang. Large-scale
    retrieval for medical image analytics: A comprehensive review. Medical Image Analysis,
    43:66–84, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Ozan Oktay, Wenjia Bai, Matthew Lee, Ricardo Guerrero, Konstantinos Kamnitsas,
    Jose Caballero, Antonio de Marvao, Stuart Cook, Declan O’Regan, and Daniel Rueckert.
    Multi-input cardiac image super-resolution using convolutional neural networks.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 246–254\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Thomas Schlegl, Sebastian M Waldstein, Wolf-Dieter Vogl, Ursula Schmidt-Erfurth,
    and Georg Langs. Predicting semantic descriptions from medical images with convolutional
    neural networks. In International Conference on Information Processing in Medical
    Imaging, pages 437–448\. Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Zilong Hu, Jinshan Tang, Ziming Wang, Kai Zhang, Lin Zhang, and Qingling
    Sun. Deep learning for image-based cancer detection and diagnosis—a survey. Pattern
    Recognition, 23:134–149, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Sandra Vieira, Walter HL Pinaya, and Andrea Mechelli. Using deep learning
    to investigate the neuroimaging correlates of psychiatric and neurological disorders:
    Methods and applications. Neuroscience & Biobehavioral Reviews, 74:58–75, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Daniel Durstewitz, Georgia Koppe, and Andreas Meyer-Lindenberg. Deep neural
    networks in psychiatry. Molecular psychiatry, page 1, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    representations by back-propagating errors. nature, 323(6088):533, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality
    of data with neural networks. science, 313(5786):504–507, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
    Extracting and composing robust features with denoising autoencoders. In Proceedings
    of the 25th international conference on Machine learning, pages 1096–1103\. ACM,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Christopher Poultney, Sumit Chopra, Yann L Cun, et al. Efficient learning
    of sparse representations with an energy-based model. In Advances in neural information
    processing systems, pages 1137–1144, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Hoo-Chang Shin, Matthew R Orton, David J Collins, Simon J Doran, and Martin O
    Leach. Stacked autoencoders for unsupervised feature learning and multiple organ
    detection in a pilot study using 4d patient data. IEEE transactions on pattern
    analysis and machine intelligence, 35(8):1930–1943, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, and Pascal Lamblin.
    Exploring strategies for training deep neural networks. Journal of machine learning
    research, 10(Jan):1–40, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm
    for deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The”
    wake-sleep” algorithm for unsupervised neural networks. Science, 268(5214):1158–1161,
    1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann
    machines. In Proceedings of the thirteenth international conference on artificial
    intelligence and statistics, pages 693–700, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Ruslan Salakhutdinov. Learning deep generative models. Annual Review of
    Statistics and Its Application, 2:361–385, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Juha Karhunen, Tapani Raiko, and KyungHyun Cho. Unsupervised deep learning:
    A short review. In Advances in Independent Component Analysis and Learning Machines,
    pages 125–142\. Elsevier, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa
    Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. IEEE
    Signal Processing Magazine, 35(1):53–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv
    preprint arXiv:1701.07875, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation
    learning with deep convolutional generative adversarial networks. arXiv preprint
    arXiv:1511.06434, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and
    Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting.
    The Journal of Machine Learning Research, 15(1):1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. arXiv preprint arXiv:1609.02907, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
    Philip S Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:1409.1259, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Bradley J Erickson, Panagiotis Korfiatis, Zeynettin Akkus, Timothy Kline,
    and Kenneth Philbrick. Toolkits and libraries for deep learning. Journal of digital
    imaging, 30(4):400–405, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Jan Zacharias, Michael Barz, and Daniel Sonntag. A survey on deep learning
    toolkits and libraries for intelligent user interfaces. arXiv preprint arXiv:1803.04818,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
    Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture
    for fast feature embedding. In Acm International Conference on Multimedia, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Eclipse Deeplearning4j Development Team. Deeplearning4j: Open-source distributed
    deep learning for the JVM. Apache Software Foundation License 2.0., http://deeplearning4j.org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Francois Chollet and Others. Keras. https://keras.io, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
    Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient
    machine learning library for heterogeneous distributed systems. CoRR, abs/1512.01274,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, and Matthieu Devin. Tensorflow:
    Large-scale machine learning on heterogeneous distributed systems. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
    Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic
    differentiation in pytorch. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Rami Alrfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
    Dzmitry Bahdanau, Nicolas Ballas, Frederic Bastien, Justin Bayer, Anatoly Belikov,
    Alexander Belopolsky, et al. Theano: A python framework for fast computation of
    mathematical expressions. arXiv: Symbolic Computation, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Collobert Ronan, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like
    environment for machine learning. In BigLearn, NIPS workshop, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning
    toolkit. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining, pages 2135–2135, New York, NY, USA, 2016\. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Susanne G Mueller, Michael W Weiner, Leon J Thal, Ronald C Petersen, Clifford
    Jack, William Jagust, John Q Trojanowski, Arthur W Toga, and Laurel Beckett. The
    alzheimer’s disease neuroimaging initiative. Neuroimaging Clinics, 15(4):869–877,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Feng Li, Loc Tran, Kim-Han Thung, Shuiwang Ji, Dinggang Shen, and Jiang
    Li. A robust deep model for improved classification of ad/mci patients. IEEE journal
    of biomedical and health informatics, 19(5):1610–1616, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Heung-Il Suk, Seong-Whan Lee, Dinggang Shen, Alzheimer’s Disease Neuroimaging
    Initiative, et al. Hierarchical feature representation and multimodal fusion with
    deep learning for ad/mci diagnosis. NeuroImage, 101:569–582, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Heung-Il Suk, Seong-Whan Lee, Dinggang Shen, Alzheimer’s Disease Neuroimaging
    Initiative, et al. Deep sparse multi-task learning for feature selection in alzheimer’s
    disease diagnosis. Brain Structure and Function, 221(5):2569–2587, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Siqi Liu, Sidong Liu, Weidong Cai, Hangyu Che, Sonia Pujol, Ron Kikinis,
    Dagan Feng, Michael J Fulham, et al. Multimodal neuroimaging feature learning
    for multiclass diagnosis of alzheimer’s disease. IEEE Transactions on Biomedical
    Engineering, 62(4):1132–1140, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Bibo Shi, Yani Chen, Pin Zhang, Charles D Smith, Jundong Liu, Alzheimer’s
    Disease Neuroimaging Initiative, et al. Nonlinear feature transformation and deep
    fusion for alzheimer’s disease staging analysis. Pattern recognition, 63:487–498,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Donghuan Lu, Karteek Popuri, Gavin Weiguang Ding, Rakesh Balachandar,
    Mirza Faisal Beg, Alzheimer’s Disease Neuroimaging Initiative, et al. Multiscale
    deep neural network based analysis of fdg-pet images for the early diagnosis of
    alzheimer’s disease. Medical image analysis, 46:26–34, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Francisco J Martinez-Murcia, Andres Ortiz, Juan-Manuel Gorriz, Javier
    Ramirez, and Diego Castillo-Barnes. Studying the manifold structure of alzheimer’s
    disease: A deep learning approach using convolutional autoencoders. IEEE Journal
    of Biomedical and Health Informatics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Ashish Gupta, Murat Ayhan, and Anthony Maida. Natural image bases to represent
    neuroimaging data. In International conference on machine learning, pages 987–994,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Fayao Liu and Chunhua Shen. Learning deep convolutional features for mri
    based alzheimer’s disease classification. arXiv preprint arXiv:1404.3366, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Saman Sarraf, Ghassem Tofighi, et al. Deepad: Alzheimer’s disease classification
    via deep convolutional neural networks using mri and fmri. bioRxiv, page 070441,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Ciprian D Billones, Olivia Jan Louville D Demetria, David Earl D Hostallero,
    and Prospero C Naval. Demnet: A convolutional neural network for the detection
    of alzheimer’s disease and mild cognitive impairment. In Region 10 Conference,
    2016 IEEE, pages 3724–3727\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Manhua Liu, Danni Cheng, and Weiwu Yan. Classification of alzheimer’s
    disease by combination of convolutional and recurrent neural networks using fdg-pet
    images. Frontiers in neuroinformatics, 12:35, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Adrien Payan and Giovanni Montana. Predicting alzheimer’s disease: a neuroimaging
    study with 3d convolutional neural networks. arXiv preprint arXiv:1502.02506,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Ehsan Hosseini-Asl, Georgy Gimel’farb, and Ayman El-Baz. Alzheimer’s disease
    diagnostics by a deeply supervised adaptable 3d convolutional network. arXiv preprint
    arXiv:1607.00556, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Hiroki Karasawa, Chien-Liang Liu, and Hayato Ohwada. Deep 3d convolutional
    neural network architectures for alzheimer’s disease diagnosis. In Asian Conference
    on Intelligent Information and Database Systems, pages 287–296\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Manhua Liu, Danni Cheng, Kundong Wang, Yaping Wang, Alzheimer’s Disease Neuroimaging
    Initiative, et al. Multi-modality cascaded convolutional neural networks for alzheimer’s
    disease diagnosis. Neuroinformatics, pages 1–14, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Rongjian Li, Wenlu Zhang, Heung-Il Suk, Li Wang, Jiang Li, Dinggang Shen,
    and Shuiwang Ji. Deep learning based imaging data completion for improved brain
    disease diagnosis. In International Conference on Medical Image Computing and
    Computer-Assisted Intervention, pages 305–312\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Mingxia Liu, Jun Zhang, Ehsan Adeli, and Dinggang Shen. Landmark-based
    deep multi-instance learning for brain disease diagnosis. Medical image analysis,
    43:157–168, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Mingxia Liu, Jun Zhang, Dong Nie, Pew-Thian Yap, and Dinggang Shen. Anatomical
    landmark based deep feature representation for mr images in brain disease diagnosis.
    IEEE Journal of Biomedical and Health Informatics, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Yongsheng Pan, Mingxia Liu, Chunfeng Lian, Tao Zhou, Yong Xia, and Dinggang
    Shen. Synthesizing missing pet from mri with cycle-consistent generative adversarial
    networks for alzheimer’s disease diagnosis. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, pages 455–463\. Springer,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Xin Zhao, Feng Zhou, Le Ou-Yang, Tianfu Wang, and Baiying Lei. Graph convolutional
    network analysis for mild cognitive impairment prediction. In 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), pages 1598–1601\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Shuangzhi Yu, Guanghui Yue, Ahmed Elazab, Xuegang Song, Tianfu Wang, and
    Baiying Lei. Multi-scale graph convolutional network for mild cognitive impairment
    detection. In International Workshop on Graph Learning in Medical Imaging, pages
    79–87\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Tzu-An Song, Samadrita Roy Chowdhury, Fan Yang, Heidi Jacobs, Georges
    El Fakhri, Quanzheng Li, Keith Johnson, and Joyita Dutta. Graph convolutional
    neural networks for alzheimer’s disease classification. In 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), pages 414–417\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Jiaming Guo, Wei Qiu, Xiang Li, Xuandong Zhao, Ning Guo, and Quanzheng
    Li. Predicting alzheimer’s disease by hierarchical graph convolution from positron
    emission tomography imaging. arXiv preprint arXiv:1910.00185, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Kenneth Marek, Danna Jennings, Shirley Lasch, Andrew Siderowf, Caroline
    Tanner, Tanya Simuni, Chris Coffey, Karl Kieburtz, Emily Flagg, Sohini Chowdhury,
    et al. The parkinson progression marker initiative (ppmi). Progress in neurobiology,
    95(4):629–635, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Andrés Ortiz, Francisco J Martínez-Murcia, María J García-Tarifa, Francisco
    Lozano, Juan M Górriz, and Javier Ramírez. Automated diagnosis of parkinsonian
    syndromes by deep sparse filtering-based features. In International Conference
    on Innovation in Medicine and Healthcare, pages 249–258\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Francisco Jesús Martinez-Murcia, Andres Ortiz, Juan Manuel Górriz, Javier
    Ramírez, Fermin Segovia, Diego Salas-Gonzalez, Diego Castillo-Barnes, and Ignacio A
    Illán. A 3d convolutional neural network approach for the diagnosis of parkinson’s
    disease. In International Work-Conference on the Interplay Between Natural and
    Artificial Computation, pages 324–333\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Francisco Jesús Martinez-Murcia, Andres Ortiz, Juan Manuel Gorriz, Javier
    Ramirez, Diego Castillo-Barnes, Diego Salas-Gonzalez, and Fermin Segovia. Deep
    convolutional autoencoders vs pca in a highly-unbalanced parkinson’s disease dataset:
    A datscan study. In The 13th International Conference on Soft Computing Models
    in Industrial and Environmental Applications, pages 47–56\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Hongyoon Choi, Seunggyun Ha, Hyung Jun Im, Sun Ha Paek, and Dong Soo Lee.
    Refining diagnosis of parkinson’s disease with deep learning-based interpretation
    of dopamine transporter imaging. NeuroImage: Clinical, 16:586–594, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Soheil Esmaeilzadeh, Yao Yang, and Ehsan Adeli. End-to-end parkinson disease
    diagnosis using brain mr-images by 3d-cnn. arXiv preprint arXiv:1806.05233, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S Sivaranjini and CM Sujatha. Deep learning based diagnosis of parkinson’s
    disease using convolutional neural network. Multimedia Tools and Applications,
    pages 1–13, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Ting Shen, Jiehui Jiang, Wei Lin, Jingjie Ge, Ping Wu, Yongjin Zhou, Chuantao
    Zuo, Jian Wang, Zhuangzhi Yan, and Kuangyu Shi. Use of overlapping group lasso
    sparse deep belief network to discriminate parkinson’s disease and normal control.
    Frontiers in neuroscience, 13:396, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Christian McDaniel and Shannon Quinn. Developing a graph convolution-based
    analysis pipeline for multi-modal neuroimage data: An application to parkinson’s
    disease. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Xi Zhang, Lifang He, Kun Chen, Yuan Luo, Jiayu Zhou, and Fei Wang. Multi-view
    graph convolutional network and its applications on neuroimage analysis for parkinson’s
    disease. In AMIA Annual Symposium Proceedings, volume 2018, page 1147. American
    Medical Informatics Association, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Martino A Di, C. G. Yan, Q. Li, E Denio, F. X. Castellanos, K Alaerts,
    J. S. Anderson, M Assaf, S. Y. Bookheimer, and M Dapretto. The autism brain imaging
    data exchange: Towards a large-scale evaluation of the intrinsic brain architecture
    in autism. Molecular Psychiatry, 19(6):659–667, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Heather Cody Hazlett, Hongbin Gu, Brent C Munsell, Sun Hyung Kim, Martin
    Styner, Jason J Wolff, Jed T Elison, Meghan R Swanson, Hongtu Zhu, Kelly N Botteron,
    et al. Early brain development in infants at high risk for autism spectrum disorder.
    Nature, 542(7641):348, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Xinyu Guo, Kelli C. Dominick, Ali A. Minai, Hailong Li, Craig A. Erickson,
    and Long J. Lu. Diagnosing autism spectrum disorder from brain resting-state functional
    connectivity patterns using a deep neural network with a novel feature selection
    method. Frontiers in Neuroscience, 11:1–19, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Yazhou Kong, Jianliang Gao, Yunpei Xu, Yi Pan, Jianxin Wang, and Jin Liu.
    Classification of autism spectrum disorder by combining brain connectivity and
    deep neural network classifier. Neurocomputing, 324:63–68, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Omar Dekhil, Hassan Hajjdiab, Ahmed Shalaby, Mohamed T Ali, Babajide Ayinde,
    Andy Switala, Aliaa Elshamekh, Mohamed Ghazal, Robert Keynton, Gregory Barnes,
    et al. Using resting state functional mri to build a personalized autism diagnosis
    system. PloS one, 13(10):e0206351, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Hongyoon Choi. Functional connectivity patterns of autism spectrum disorder
    identified by deep feature learning. arXiv preprint arXiv:1707.07932, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Anibal Sólon Heinsfeld, Alexandre Rosa Franco, R. Cameron Craddock, Augusto
    Buchweitz, and Felipe Meneguzzi. Identification of autism spectrum disorder using
    deep learning and the abide dataset. Neuroimage Clinical, 17:16–23, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Hailong Li, Nehal A Parikh, and Lili He. A novel transfer learning approach
    to enhance deep neural network classification of brain functional connectomes.
    Frontiers in neuroscience, 12:491, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Fahad Saeed, Taban Eslami, Vahid Mirjalili, Alvis Fong, and Angela Laird.
    Asd-diagnet: A hybrid learning approach for detection of autism spectrum disorder
    using fmri data. Frontiers in Neuroinformatics, 13:70, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Xiaoxiao Li, Nicha C Dvornek, Xenophon Papademetris, Juntang Zhuang,
    Lawrence H Staib, Pamela Ventola, and James S Duncan. 2-channel convolutional
    3d deep neural network (2cc3d) for fmri analysis: Asd classification and feature
    learning. In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
    2018), pages 1252–1255\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, and Mert R Sabuncu. 3d
    convolutional neural networks for classification of functional connectomes. In
    Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision
    Support, pages 137–145\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew
    Lee, Ben Glocker, and Daniel Rueckert. Metric learning with spectral graph convolutions
    on brain connectivity networks. NeuroImage, 169:431–442, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo Guerrerro
    Moreno, Ben Glocker, and Daniel Rueckert. Spectral graph convolutions for population-based
    disease prediction. In International Conference on Medical Image Computing and
    Computer-Assisted Intervention, pages 177–185\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Rushil Anirudh and Jayaraman J Thiagarajan. Bootstrapping graph convolutional
    neural networks for autism spectrum disorder classification. arXiv preprint arXiv:1704.07487,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Dongren Yao, Mingxia Liu, Mingliang Wang, Chunfeng Lian, Jie Wei, Li Sun,
    Jing Sui, and Dinggang Shen. Triplet graph convolutional network for multi-scale
    analysis of functional connectivity using functional mri. In International Workshop
    on Graph Learning in Medical Imaging, pages 70–78\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Xia-an Bi, Yingchao Liu, Qin Jiang, Qing Shu, Qi Sun, and Jianhua Dai.
    The diagnosis of autism spectrum disorder based on the random neural network cluster.
    Frontiers in human neuroscience, 12:257, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Nicha C Dvornek, Pamela Ventola, Kevin A Pelphrey, and James S Duncan.
    Identifying autism from resting-state fmri using long short-term memory networks.
    In International Workshop on Machine Learning in Medical Imaging, pages 362–370\.
    Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Sergey M. Plis, Devon R. Hjelm, Ruslan Salakhutdinov, Elena A. Allen,
    Henry J. Bockholt, Jeffrey D. Long, Hans J. Johnson, Jane S. Paulsen, Jessica A.
    Turner, and Vince D. Calhoun. Deep learning for neuroimaging: a validation study.
    Frontiers in Neuroscience, 8:1–11, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] W. H. Pinaya, A Gadelha, O. M. Doyle, C Noto, A Zugman, Q Cordeiro, A. P.
    Jackowski, R. A. Bressan, and J. R. Sato. Using deep belief network modelling
    to characterize differences in brain morphometry in schizophrenia. Scientific
    Reports, 6(38897):1–9, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Walter HL Pinaya, Andrea Mechelli, and João R Sato. Using deep autoencoders
    to identify abnormal brain structural patterns in neuropsychiatric disorders:
    A large-scale multi-sample study. Human brain mapping, 40(3):944–954, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Ulloa, S. Plis, E. Erhardt, and V. Calhoun. Synthetic structural magnetic
    resonance image generator improves deep learning prediction of schizophrenia.
    In 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing
    (MLSP), pages 1–6, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J Kim, V. D. Calhoun, E Shim, and J. H. Lee. Deep neural network with
    weight sparsity control and pre-training extracts hierarchical features and enhances
    classification performance: Evidence from whole-brain resting-state functional
    connectivity patterns of schizophrenia. Neuroimage, 124(Pt A):127–146, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Pinkal Patel, Priya Aggarwal, and Anubha Gupta. Classification of schizophrenia
    versus normal subjects using deep learning. In Tenth Indian Conference on Computer
    Vision, Graphics and Image Processing, page 28, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] L. L. Zeng, H. Wang, P. Hu, B. Yang, W. Pu, H. Shen, X. Chen, Z. Liu,
    H. Yin, and Q. Tan. Multi-site diagnostic classification of schizophrenia using
    discriminant deep learning with functional connectivity MRI. Ebiomedicine, 30:74–85,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Muhammad Naveed Iqbal Qureshi, Jooyoung Oh, and Boreom Lee. 3d-cnn based
    discrimination of schizophrenia using resting-state fmri. Artificial Intelligence
    in Medicine, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Jumana Dakka, Pouya Bashivan, Mina Gheiratmand, Irina Rish, Shantenu
    Jha, and Russell Greiner. Learning neural markers of schizophrenia disorder using
    recurrent neural networks. arXiv preprint arXiv:1712.00512, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Weizheng Yan, Vince Calhoun, Ming Song, Yue Cui, Hao Yan, Shengfeng Liu,
    Lingzhong Fan, Nianming Zuo, Zhengyi Yang, Kaibin Xu, et al. Discriminating schizophrenia
    using recurrent neural network applied on time courses of multi-site fmri data.
    EBioMedicine, 47:543–552, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Jun Qi and Javier Tejedor. Deep multi-view representation learning for
    multi-modal features of the schizophrenia and schizo-affective disorder. In IEEE
    International Conference on Acoustics, Speech and Signal Processing, pages 952–956,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Srivathsan Srinivasagopalan, Justin Barry, Varadraj Gurupur, and Sharma
    Thankachan. A deep learning approach for diagnosing schizophrenic patients. Journal
    of Experimental & Theoretical Artificial Intelligence, pages 1–14, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Alvaro Ulloa, Sergey Plis, and Vince Calhoun. Improving classification
    rate of schizophrenia using a multimodal multi-layer perceptron model with structural
    and functional mr. arXiv preprint arXiv:1804.04591, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Bo Cheng, Mingxia Liu, Dinggang Shen, Zuoyong Li, and Daoqiang Zhang.
    Multi-Domain Transfer Learning for Early Diagnosis of Alzheimer’s Disease. Neuroinformatics,
    15(2):115–132, April 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Bo Cheng, Mingxia Liu, Heung-Il Suk, Dinggang Shen, and Daoqiang Zhang.
    Multimodal manifold-regularized transfer learning for MCI conversion prediction.
    Brain imaging and behavior, 9(4):913–926, December 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Yongsheng Pan, Mingxia Liu, Chunfeng Lian, Yong Xia, and Dinggang Shen.
    Disease-image specific generative adversarial network for brain disease diagnosis
    with incomplete multi-modal neuroimages. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Mingliang Wang, Chunfeng Lian, Dongren Yao, Daoqiang Zhang, Mingxia Liu,
    and Dinggang Shen. Spatial-temporal dependency modeling and network hub detection
    for functional MRI analysis via convolutional-recurrent network. IEEE Transactions
    on Biomedical Engineering, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Biao Jie, Mingxia Liu, Chunfeng Lian, Feng Shi, and Dinggang Shen. Developing
    novel weighted correlation kernels for convolutional neural networks to extract
    hierarchical functional connectivities from fMRI for disease diagnosis. In International
    Workshop on Machine Learning in Medical Imaging, pages 1–9\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Mingliang Wang, Daoqiang Zhang, Jiashuang Huang, Dinggang Shen, and Mingxia
    Liu. Low-rank representation for multi-center autism spectrum disorder identification.
    In Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, pages
    647–654, Cham, 2018\. Springer International Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Mingliang Wang, Daoqiang Zhang, Jiashuang Huang, Pew-Thian Yap, Dinggang
    Shen, and Mingxia Liu. Identifying autism spectrum disorder with multi-site fmri
    via low-rank domain adaptation. IEEE Transactions on Medical Imaging, 39(3):644–655,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
