- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:51:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:51:09'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.01411] Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.01411] 深度强化学习与进化策略：比较调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.01411](https://ar5iv.labs.arxiv.org/html/2110.01411)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.01411](https://ar5iv.labs.arxiv.org/html/2110.01411)
- en: 'Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习与进化策略：比较调查
- en: Amjad Yousef Majid Delft University of Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Amjad Yousef Majid Delft University of Technology
- en: a.y.majid@tudelft.nl    Serge Saaybi Delft University of Technology
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: a.y.majid@tudelft.nl    Serge Saaybi Delft University of Technology
- en: s.c.e.saaybi@student.tudelft.nl    Tomas van Rietbergen {@IEEEauthorhalign}
    Vincent Francois-Lavet Delft University of Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: s.c.e.saaybi@student.tudelft.nl    Tomas van Rietbergen {@IEEEauthorhalign}
    Vincent Francois-Lavet Delft University of Technology
- en: T.L.vanRietbergen@student.tudelft.nl VU Amsterdam
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: T.L.vanRietbergen@student.tudelft.nl VU Amsterdam
- en: vincent.francoislavet@vu.nl    R Venkatesha Prasad Delft University of Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: vincent.francoislavet@vu.nl    R Venkatesha Prasad Delft University of Technology
- en: r.r.venkateshaprasad@tudelft.nl    Chris Verhoeven Delft University of Technology
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: r.r.venkateshaprasad@tudelft.nl    Chris Verhoeven Delft University of Technology
- en: C.J.M.Verhoeven@tudelft.nl
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: C.J.M.Verhoeven@tudelft.nl
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Reinforcement Learning (DRL) and Evolution Strategies (ESs) have surpassed
    human-level control in many sequential decision-making problems, yet many open
    challenges still exist. To get insights into the strengths and weaknesses of DRL
    versus ESs, an analysis of their respective capabilities and limitations is provided.
    After presenting their fundamental concepts and algorithms, a comparison is provided
    on key aspects such as scalability, exploration, adaptation to dynamic environments,
    and multi-agent learning. Then, the benefits of hybrid algorithms that combine
    concepts from DRL and ESs are highlighted. Finally, to have an indication about
    how they compare in real-world applications, a survey of the literature for the
    set of applications they support is provided.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）和进化策略（ESs）在许多顺序决策问题上已超越了人类水平控制，但仍存在许多未解的挑战。为了深入了解DRL与ESs的优缺点，提供了对它们各自能力和局限性的分析。在介绍了其基本概念和算法后，比较了关键方面，如可扩展性、探索、对动态环境的适应性和多智能体学习。接着，突出了结合DRL和ESs概念的混合算法的好处。最后，为了了解它们在实际应用中的比较，提供了支持它们的一组应用文献的综述。
- en: 'Index Terms:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep Reinforcement Learning, Evolution Strategies, Multi-agent
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，进化策略，多智能体
- en: I Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In the biological world, the intellectual capabilities of humans and animals
    have developed through a combination of evolution and learning. On the one hand,
    evolution has allowed living beings to improve genetically over successive generations
    such that higher forms of intelligence have appeared, on the other hand, adapting
    rapidly to new situations is possible due to the learning capability of animals
    and humans.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物世界中，人类和动物的智力能力是通过进化和学习的结合而发展的。一方面，进化使生物在连续几代中在遗传上得到改善，从而出现了更高形式的智力；另一方面，动物和人类的学习能力使它们能够迅速适应新情况。
- en: In the race for developing artificial general intelligence, these two phenomena
    have motivated the development of two distinct approaches that could both play
    an important role in the quest for intelligent machines. From the learning perspective,
    Reinforcement learning (RL) shows many parallels with how humans and animals can
    deal with new unknown sequential decision-making tasks. Meanwhile, Evolution Strategies
    (ESs) are engineering methods inspired by how the mechanism that let intelligence
    emerge in the biological world—repeatedly selecting the best performing individuals.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在发展人工通用智能的竞争中，这两种现象激励了两种不同的方法的开发，这两种方法都可能在寻求智能机器的过程中发挥重要作用。从学习的角度看，强化学习（RL）与人类和动物如何应对新的未知顺序决策任务有许多相似之处。与此同时，进化策略（ESs）是受生物世界中智力出现机制启发的工程方法——通过反复选择表现最佳的个体。
- en: In this paper, we discuss RL and ESs together analyzing their strengths and
    weaknesses regarding their sequential decision-making capabilities and shed light
    on potential directions for further development.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中，我们讨论了强化学习（RL）和进化策略（ESs），分析了它们在顺序决策能力方面的优缺点，并阐明了进一步发展的潜在方向。
- en: The RL framework is formalized as an agent acting on an environment with the
    goal of maximizing a cumulative reward over the trajectory of interaction with
    the environment [[1](#bib.bib1)]. Imagine playing a table tennis game (environment)
    with a robot (agent). The robot has not explicitly been programmed to play the
    game. Instead, it can observe the score of the game (rewards). The robot’s goal
    is to maximize its score. For that purpose, it tries different techniques of hitting
    the ball (actions), observes the outcome, and gradually enhances its playing strategy
    (policy).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RL框架被形式化为一个在环境中行动的代理，目标是最大化在与环境交互的轨迹上的累计奖励[[1](#bib.bib1)]。想象一下用一个机器人（代理）玩乒乓球（环境）。机器人并没有被明确编程来玩这个游戏。相反，它可以观察游戏的得分（奖励）。机器人的目标是最大化其得分。为此，它尝试不同的击球技术（动作），观察结果，并逐渐改进其游戏策略（策略）。
- en: Despite the proven convergence of RL algorithms to optimal policies—best solutions
    to the problems at hand—they face difficulties processing high-dimensional data
    (e.g., images). To tackle problems with high-dimensional data, RL algorithms are
    nowadays often combined with deep neural networks, giving raise to a whole field
    of research known as Deep RL (DRL) [[2](#bib.bib2)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RL算法已被证明能收敛到最佳策略——即当前问题的最佳解决方案——它们在处理高维数据（例如图像）时仍面临困难。为了解决高维数据问题，如今RL算法通常与深度神经网络结合，从而催生了一个称为深度强化学习（DRL）的研究领域[[2](#bib.bib2)]。
- en: '![Refer to caption](img/70763a0984d0226e1f5cd983cbe065ea.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70763a0984d0226e1f5cd983cbe065ea.png)'
- en: 'Figure 1: The structure of the survey'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查的结构
- en: As a contrasting approach to DRL, ES algorithms utilize a random process to
    iteratively generate candidate solutions. Then, they evaluate these solutions
    and bias the search in direction of the best scoring ones [[3](#bib.bib3)]. In
    recent years, ESs have seen an increase in popularity and has been successfully
    applied to several applications, including optimizing objective functions for
    many RL tasks [[4](#bib.bib4), [5](#bib.bib5)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对DRL的对比方法，ES算法利用随机过程迭代生成候选解决方案。然后，它们评估这些解决方案，并将搜索偏向于得分最高的那些[[3](#bib.bib3)]。近年来，ESs的受欢迎程度有所上升，并且成功应用于多个领域，包括优化许多RL任务的目标函数[[4](#bib.bib4),
    [5](#bib.bib5)]。
- en: '![Refer to caption](img/cea9655b1502ee5b214cb01d51032072.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cea9655b1502ee5b214cb01d51032072.png)'
- en: ((a)) RL
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ((a)) 强化学习
- en: '![Refer to caption](img/e03de81cff1bf7404455d1482dc64df1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e03de81cff1bf7404455d1482dc64df1.png)'
- en: ((b)) DRL
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ((b)) 深度强化学习
- en: '![Refer to caption](img/2ec90b421cc173d1e4f694a2fe1aa9a0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ec90b421cc173d1e4f694a2fe1aa9a0.png)'
- en: ((c)) ESs
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ((c)) 进化策略
- en: 'Figure 2: Iteration loops of (Deep) Reinforcement Learning and Evolutionary
    Strategies.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：（深度）强化学习与进化策略的迭代循环。
- en: 'The parallel development of DRL and ESs indicates that each has its advantages
    (and disadvantages), depending on the problem setup. To enable scientists and
    researchers to choose the best algorithm for the problem at hand, we summarized
    the pros and cons of these approaches through the development of a comparative
    survey: we compared DRL and ESs from different learning aspects such as scalability,
    exploration, the ability to learn in dynamic environments and from an application
    standpoint (Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey")). We also discuss
    how combining DRL and ESs in hybrid systems can leverage the advantages of both
    approaches.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DRL与ESs的并行发展表明，两者各有其优势（和劣势），这取决于问题的设置。为了帮助科学家和研究人员选择最适合当前问题的算法，我们通过制定一项比较调查总结了这些方法的优缺点：我们从不同的学习方面如可扩展性、探索、在动态环境中的学习能力以及应用视角对DRL和ESs进行了比较（图[1](#S1.F1
    "图1 ‣ I 引言 ‣ 深度强化学习与进化策略的比较调查")）。我们还讨论了如何在混合系统中结合DRL和ESs，以利用两者的优势。
- en: To date, there have been different papers summarizing different features of
    DRL and ESs. For example, derivative-free reinforcement learning (e.g., ESs) has
    been reviewed in [[6](#bib.bib6)], covering aspects such as scalability and exploration.
    A survey related to DRL for autonomous driving is provided in [[7](#bib.bib7)],
    and the challenges, solutions, and applications of multi-agent DRL systems are
    reviewed in [[8](#bib.bib8)]. However, contrasting with prior work, our paper
    surveys the literature with a bird’s-eye view, focusing on the main developmental
    directions instead of individual algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 至今，已有不同的论文总结了 DRL 和 ESs 的不同特性。例如，关于无导数强化学习（例如，ESs）的综述见[[6](#bib.bib6)]，涵盖了可扩展性和探索等方面。有关用于自动驾驶的
    DRL 的调查见[[7](#bib.bib7)]，多智能体 DRL 系统的挑战、解决方案和应用则在[[8](#bib.bib8)]中进行综述。然而，与以往工作对比，我们的论文以全局视角审视文献，专注于主要的发展方向而非单个算法。
- en: 'The rest of the paper is organized as follows: Section [II](#S2 "II Fundamentals
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    presents the fundamental architectural concepts behind RL and ESs; Section [III](#S3
    "III Fundamental Algorithms ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey") summarizes fundamental algorithms of RL, DRL and ESs; Sections [IV-A](#S4.SS1
    "IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey"),
    [IV-B](#S4.SS2 "IV-B Exploration ‣ IV Deep Reinforcement Learning Versus Evolution
    Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey"), [IV-C](#S4.SS3 "IV-C Non-Markov settings ‣ IV Deep Reinforcement Learning
    Versus Evolution Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey") and [IV-D](#S4.SS4 "IV-D Learning in multiagent settings
    ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey") compare the capabilities
    of DRL and ESs; In Section [V](#S5 "V Hybrid Deep Reinforcement Learning and Evolution
    Strategies Algorithms ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey"), we present hybrid systems that combine DRL and ESs. Section
     [VI](#S6 "VI Applications ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey") compares them from an applications’ point of view. Section
     [VII](#S7 "VII Challenges and future research directions ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey") outlines open challenges
    and potential research directions. Finally, we conclude the paper in Section [VIII](#S8
    "VIII Conclusion ‣ Deep Reinforcement Learning Versus Evolution Strategies: A
    Comparative Survey"). The main takeaways of each section are summarized in a concise
    sub-section titled “Comparison”.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下：第[II](#S2 "II Fundamentals ‣ Deep Reinforcement Learning Versus
    Evolution Strategies: A Comparative Survey")节介绍了 RL 和 ESs 背后的基本架构概念；第[III](#S3
    "III Fundamental Algorithms ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey")节总结了 RL、DRL 和 ESs 的基本算法；第[IV-A](#S4.SS1 "IV-A Parallelism
    ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey")、[IV-B](#S4.SS2 "IV-B
    Exploration ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep
    Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")、[IV-C](#S4.SS3
    "IV-C Non-Markov settings ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    和 [IV-D](#S4.SS4 "IV-D Learning in multiagent settings ‣ IV Deep Reinforcement
    Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning Versus Evolution
    Strategies: A Comparative Survey")节比较了 DRL 和 ESs 的能力；第[V](#S5 "V Hybrid Deep Reinforcement
    Learning and Evolution Strategies Algorithms ‣ Deep Reinforcement Learning Versus
    Evolution Strategies: A Comparative Survey")节介绍了结合 DRL 和 ESs 的混合系统。第[VI](#S6 "VI
    Applications ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey")节从应用的角度对其进行比较。第[VII](#S7 "VII Challenges and future research directions
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")节概述了开放的挑战和潜在的研究方向。最后，我们在第[VIII](#S8
    "VIII Conclusion ‣ Deep Reinforcement Learning Versus Evolution Strategies: A
    Comparative Survey")节总结了论文的主要内容。每节的主要内容在一个简洁的小节中总结，标题为“比较”。'
- en: II Fundamentals
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 基础
- en: This section covers the fundamental elements of DRL and ESs, including formal
    definitions and the main algorithmic families.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 DRL 和 ESs 的基本元素，包括正式定义和主要的算法类别。
- en: II-A Reinforcement Learning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 强化学习
- en: 'Reinforcement Learning (RL) is a computational approach to understanding and
    automating goal-directed learning and decision making [[1](#bib.bib1)]. The goal
    of an RL agent is to maximize the total reward it receives when interacting with
    an environment (Figure [2(a)](#S1.F2.sf1 "In Figure 2 ‣ I Introduction ‣ Deep
    Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")), which
    is generally modeled as a Markov Decision Process (MDP). An MDP is defined by
    the tuple $(\mathcal{S},\mathcal{A},T,R)$, where $\mathcal{S}$ denotes the state
    space; $\mathcal{A}$ is the action space; $T(s,a,s^{\prime})$ is a transition
    function that defines the probability of transitioning from the current state
    $s$ to the next state $s^{\prime}$ after an agent takes action $a$; $R(s,a,s^{\prime})$
    is the reward function that defines the immediate reward $r$ that the agent observes
    after taking action $a$ and the environment transition from $s$ to $s^{\prime}$.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种理解和自动化目标导向学习及决策的计算方法[[1](#bib.bib1)]。一个RL智能体的目标是最大化其在与环境交互时获得的总奖励（图[2(a)](#S1.F2.sf1
    "在图2 ‣ I 引言 ‣ 深度强化学习与进化策略的比较调查")），这通常被建模为马尔可夫决策过程（MDP）。MDP由元组$(\mathcal{S},\mathcal{A},T,R)$定义，其中$\mathcal{S}$表示状态空间；$\mathcal{A}$是动作空间；$T(s,a,s^{\prime})$是一个转移函数，定义了智能体在采取动作$a$后从当前状态$s$转移到下一状态$s^{\prime}$的概率；$R(s,a,s^{\prime})$是奖励函数，定义了智能体在采取动作$a$后，从状态$s$转移到$s^{\prime}$时观察到的即时奖励$r$。
- en: The total return starting from time $t$ until the end of the interaction between
    an agent and its environment is expressed as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间$t$开始到智能体与环境交互结束的总回报表示为
- en: '|  | $G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\text{,}$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\text{,}$ |  |'
- en: where $R_{t}$ and is a random variable that models the immediate reward, $r$,
    and $\gamma\in[0,1)$ is a discount factor that weights the immediate and future
    rewards. Value functions are the expected return of being in a state or taking
    a particular action. The state-value function $v_{\pi}(s)$ gives the expected
    return from state $s$ following policy $\pi$,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$R_{t}$是一个随机变量，表示即时奖励$r$，而$\gamma\in[0,1)$是折扣因子，用于加权即时奖励和未来奖励。价值函数表示在某状态下或采取某动作的期望回报。状态价值函数$v_{\pi}(s)$表示从状态$s$出发并遵循策略$\pi$的期望回报，
- en: '|  | $v^{\pi}(s)=\sum_{a}\pi(a&#124;s)\sum_{s^{\prime},r}p(s^{\prime},r&#124;s,a)[r+\gamma
    v^{\pi}(s^{\prime})]\text{.}$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $v^{\pi}(s)=\sum_{a}\pi(a\vert s)\sum_{s^{\prime},r}p(s^{\prime},r\vert
    s,a)[r+\gamma v^{\pi}(s^{\prime})]\text{.}$ |  | (1) |'
- en: The action-value function (or Q-function) $q^{\pi}(s,a)$ is the expected return
    of taking action $a$ in state $s$ and following policy $\pi$ thereafter,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值函数（或Q函数）$q^{\pi}(s,a)$是指在状态$s$下采取动作$a$并随后遵循策略$\pi$的期望回报，
- en: '|  | $q^{\pi}(s,a)=\sum_{s^{\prime},r}p(s^{\prime},r&#124;s,a)\bigg{[}r+\gamma\sum_{a^{\prime}}\pi(a^{\prime}&#124;s^{\prime})q^{\pi}(s^{\prime},a^{\prime})\bigg{]}\text{.}$
    |  | (2) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $q^{\pi}(s,a)=\sum_{s^{\prime},r}p(s^{\prime},r\vert s,a)\bigg{[}r+\gamma\sum_{a^{\prime}}\pi(a^{\prime}\vert
    s^{\prime})q^{\pi}(s^{\prime},a^{\prime})\bigg{]}\text{.}$ |  | (2) |'
- en: The action selection process of an agent is governed by its policy, which in
    the general stochastic case yields an action according to a probability distribution
    over the action space conditioned on a given state $\pi(s,a)$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个智能体的动作选择过程由其策略控制，在一般的随机情况下，根据给定状态$\pi(s,a)$，策略会在动作空间上产生一个动作的概率分布。
- en: 'There are four main RL algorithmic families:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）算法主要有四大类：
- en: 'Policy-based Algorithms. A policy-based algorithm optimizes and memorizes a
    policy explicitly, that is, it directly searches the policy space for an (approximate)
    optimal policy, $\pi^{*}$. Examples of such algorithms are policy iteration [[9](#bib.bib9)],
    policy gradient [[10](#bib.bib10)] and REINFORCE [[11](#bib.bib11)]. Policy-based
    algorithms can be applied to any type of action space: continuous, discrete or
    a mixture (multiactions). However, these algorithms generally have high variance
    and are sample-inefficient.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的算法。基于策略的算法显式地优化和记忆策略，即直接在策略空间中搜索一个（近似的）最优策略$\pi^{*}$。这类算法的例子有策略迭代[[9](#bib.bib9)]、策略梯度[[10](#bib.bib10)]和REINFORCE[[11](#bib.bib11)]。基于策略的算法可以应用于任何类型的动作空间：连续的、离散的或混合的（多动作）。然而，这些算法通常具有较高的方差且样本效率较低。
- en: Value-based Algorithms. A value-based algorithm learns a value function, $v^{\pi}(s)\text{
    or }q^{\pi}(s,a)$. Then, a policy is extracted according to the learned value
    function. Examples of such algorithms are value iteration [[12](#bib.bib12)],
    SARSA [[13](#bib.bib13)], Q-learning and DQN [[14](#bib.bib14)]. Value-based algorithms
    are more sample-efficient than policy-based ones. However, under ordinary circumstances
    the convergence of these algorithms is not guaranteed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的算法。基于价值的算法学习一个价值函数，$v^{\pi}(s)\text{ 或 }q^{\pi}(s,a)$。然后，根据学习到的价值函数提取一个策略。这类算法的例子包括价值迭代[[12](#bib.bib12)]，SARSA[[13](#bib.bib13)]，Q学习和DQN[[14](#bib.bib14)]。与基于策略的算法相比，基于价值的算法在样本效率上更高。然而，在普通情况下，这些算法的收敛性不能得到保证。
- en: Actor-critic-based Algorithms. The actor-critic approach tries to combine the
    strengths of policy- and value-based algorithms into a single algorithmic architecture [[15](#bib.bib15)].
    The actor is a policy-based algorithm that tries to learn the optimal policy,
    whereas the critic is a value-based algorithm that evaluates the actions taken
    by the actor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic算法。Actor-critic方法试图将基于策略的算法和基于价值的算法的优点结合成一个单一的算法架构[[15](#bib.bib15)]。Actor是一个基于策略的算法，试图学习最优策略，而Critic是一个基于价值的算法，评估Actor采取的行动。
- en: Model-based Algorithms. All of the algorithmic families mentioned previously
    concern model-free algorithms. In contrast, model-based algorithms learn or make
    use of a model of the transition dynamics of an environment. Once an agent has
    access to such a model, it can use it to “imagine” the consequences of taking
    a particular set of actions without acting on the environment. Such capability
    enables an RL agent to evaluate the expected actions of an opponent in games [[16](#bib.bib16),
    [17](#bib.bib17)] and to make better use of gathered data, which is very useful
    in tasks such as controlling a robot [[18](#bib.bib18)]. However, for many problems,
    it is difficult to produce close to reality models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的算法。前面提到的所有算法族都涉及无模型算法。相比之下，基于模型的算法学习或利用环境转移动态的模型。一旦代理可以访问这样的模型，它就可以利用该模型“想象”采取特定行动集合的后果，而不必实际对环境进行操作。这种能力使得强化学习代理能够评估对手在游戏中的预期动作[[16](#bib.bib16),
    [17](#bib.bib17)]，并更好地利用收集到的数据，这在控制机器人等任务中非常有用[[18](#bib.bib18)]。然而，对于许多问题，生产接近现实的模型是困难的。
- en: 'Deep Reinforcement Learning (DRL) refers to the combination of Deep Learning
    (DL) and RL (Figure [2(b)](#S1.F2.sf2 "In Figure 2 ‣ I Introduction ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey")) [[2](#bib.bib2)].
    DRL uses DNNs to approximate one of the learnable functions of RL. Correspondingly,
    there are three main families of DRL algorithms: value-based, policy-based, and
    model-based [[14](#bib.bib14), [19](#bib.bib19), [16](#bib.bib16)]. For example,
    the DNN of a policy-based DRL agent takes the state of the environment as input
    and produces an action as output (Figure [2(b)](#S1.F2.sf2 "In Figure 2 ‣ I Introduction
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")).
    The action selection process is governed by the parameters $\boldsymbol{\theta}$
    of the DNN. The parameters selection is optimized using a backpropagation algorithm
    during the training phase.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）指的是深度学习（DL）与强化学习（RL）的结合（图[2(b)](#S1.F2.sf2 "在图2 ‣ I 引言 ‣ 深度强化学习与进化策略的比较调查")）[[2](#bib.bib2)]。DRL使用深度神经网络（DNN）来逼近RL的可学习函数之一。相应地，DRL算法主要有三类：基于价值的、基于策略的和基于模型的[[14](#bib.bib14),
    [19](#bib.bib19), [16](#bib.bib16)]。例如，基于策略的DRL代理的DNN将环境的状态作为输入，并生成一个动作作为输出（图[2(b)](#S1.F2.sf2
    "在图2 ‣ I 引言 ‣ 深度强化学习与进化策略的比较调查")）。动作选择过程由DNN的参数$\boldsymbol{\theta}$控制。参数选择在训练阶段通过反向传播算法进行优化。
- en: II-B Evolution Strategies
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 进化策略
- en: Evolution Strategies (ESs) are set of a population-based black-box optimization
    algorithms often applied to continuous search spaces problems to find the optimal
    solutions [[20](#bib.bib20), [21](#bib.bib21)]. ESs do not require modeling the
    problem as an MDP, neither the objective function $f(\boldsymbol{x})$ has to be
    differentiable and continuous. The latter explains why ESs are gradient-free optimization
    techniques. They do however require the objective function $f(\boldsymbol{x})$
    to be able to assign a fitness value to (i.e., to evaluate) each input $\boldsymbol{x}\in\mathbb{R}^{n}$
    such that $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$, $\boldsymbol{x}\rightarrow
    f(\boldsymbol{x})$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 进化策略（ESs）是一种基于种群的黑箱优化算法，通常应用于连续搜索空间问题，以找到最优解 [[20](#bib.bib20), [21](#bib.bib21)]。ESs
    不需要将问题建模为马尔可夫决策过程（MDP），也不要求目标函数 $f(\boldsymbol{x})$ 必须是可微且连续的。后者解释了为什么 ESs 是无梯度优化技术。然而，它们确实要求目标函数
    $f(\boldsymbol{x})$ 能够为每个输入 $\boldsymbol{x}\in\mathbb{R}^{n}$ 分配适应度值（即，进行评估），使得
    $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$，$\boldsymbol{x}\rightarrow f(\boldsymbol{x})$。
- en: 'The basic idea behind ESs is to bias the sampling process of candidate solutions
    towards the best individuals found so far until a satisfactory solution is found.
    Samples can be drawn for instance from a (multivariate) normal distribution whose
    shape (i.e., the mean $m$ and the standard deviation $\sigma$) is described by
    what are called strategic parameters. These can be modified online to make the
    search process more efficient. The generic ESs process is shown in Figure [2(c)](#S1.F2.sf3
    "In Figure 2 ‣ I Introduction ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey") and its elements are explained below:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ESs 的基本思想是将候选解的采样过程偏向于迄今为止发现的最佳个体，直到找到满意的解决方案。例如，可以从（多变量）正态分布中抽样，其形状（即均值 $m$
    和标准差 $\sigma$）由所谓的战略参数描述。这些参数可以在线修改，以使搜索过程更加高效。通用的 ESs 过程如图 [2(c)](#S1.F2.sf3
    "在图 2 ‣ I 介绍 ‣ 深度强化学习与进化策略：比较调查") 所示，其元素如下所述：
- en: '1.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Initialization: the algorithm generates an initial population $P$ consisting
    of $\mu$ individuals.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化：算法生成一个由 $\mu$ 个个体组成的初始种群 $P$。
- en: '2.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Parent selection: a sub-set of the population is selected to function as parents
    during the recombination step.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 父代选择：从种群中选择一个子集作为重新组合步骤中的父代。
- en: '3.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Reproduction consists of two steps:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复制包括两个步骤：
- en: (a)
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Recombination: two or more parents are combined to produce a mean for the new
    generation.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重新组合：将两个或更多的父代结合起来，为新一代产生一个均值。
- en: (b)
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Mutation: a small amount of noise is added to the recombination results. A
    common way of implementing mutation is to sample from a multivariate normal distribution
    centered around the mean obtained from the previous recombination step:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变异：在重新组合结果中添加少量噪声。实现变异的一种常见方法是从以先前重新组合步骤获得的均值为中心的多变量正态分布中抽样：
- en: '|  | $\boldsymbol{x}_{k}^{g+1}\sim\mathcal{N}(\boldsymbol{m}^{(g)},\sigma^{(g)}I)=\boldsymbol{m}^{(g)}+\sigma^{(g)}\mathcal{N}(0,I)\text{,}$
    |  |'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\boldsymbol{x}_{k}^{g+1}\sim\mathcal{N}(\boldsymbol{m}^{(g)},\sigma^{(g)}I)=\boldsymbol{m}^{(g)}+\sigma^{(g)}\mathcal{N}(0,I)\text{,}$
    |  |'
- en: where $g$ is the generation index, $k$ is the number of offsprings, and $I$
    is the identity matrix.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $g$ 是代数索引，$k$ 是后代数量，$I$ 是单位矩阵。
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Evaluation: a fitness value is assigned to each candidate solution using the
    objective function $f(x_{i})$.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估：使用目标函数 $f(x_{i})$ 为每个候选解分配一个适应度值。
- en: '5.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Survivor selection: the best $\mu$ individuals are selected to form the population
    for the next generation. Generally, the algorithm iterates from step 2 to step
    5 until a satisfactory solution is found.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生存者选择：选择最好的 $\mu$ 个体形成下一代的种群。通常，算法从第 2 步迭代到第 5 步，直到找到满意的解决方案。
- en: The idea of employing ESs as an alternative to RL is not new [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)], but recently it has seen
    a renewed interest (e.g. [[4](#bib.bib4), [26](#bib.bib26)]).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将进化策略（ESs）作为强化学习（RL）替代方案的想法并不新鲜 [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25)]，但最近引起了新的关注（例如 [[4](#bib.bib4), [26](#bib.bib26)]）。
- en: II-C Comparison
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 比较
- en: 'Our main takeaways of the above fundamental concepts are:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对上述基本概念的主要总结是：
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The objective of an RL algorithm is to maximize the sum of discounted rewards,
    whereas an ESs algorithm does not require such formulation. However, the objective
    for RL settings can be converted to ESs settings with a terminal state that provides
    a reward equivalent to the fitness function.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RL 算法的目标是最大化折扣奖励的总和，而 ESs 算法则不需要这样的公式。然而，RL 设置的目标可以转换为 ESs 设置，其中终止状态提供等同于适应度函数的奖励。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The problem setup differs between RL and ESs. An ESs algorithm is a black-box
    optimization method that keeps a pool of multiple candidate solutions, while an
    RL method generally has a single agent that improves its policy by interacting
    with its environment.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RL 和 ESs 的问题设置不同。ESs 算法是一种黑箱优化方法，保持多个候选解的池，而 RL 方法通常有一个单独的代理，通过与环境互动来改进其策略。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An ESs algorithm aims at finding candidate solutions that optimize a fitness
    function, whereas the goal of DRL is to keep advancing one or two function approximators
    which in turn need to optimize the equivalent of the fitness function, usually
    defined by the discounted return.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ESs 算法旨在寻找优化适应度函数的候选解，而 DRL 的目标是不断提升一个或两个函数近似器，这些近似器需要优化等效于适应度函数的内容，通常由折扣回报定义。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The ESs approach is most similar to the policy-based DRL approach: both aim
    at finding parameters in a search space such that the resulting parameterized
    function optimizes certain objectives (expected return for DRL or fitness score
    for ESs). The main distinction is that ESs, unlike DRL, do not calculate gradients
    nor use backpropagation.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ESs 方法最类似于基于策略的 DRL 方法：两者都旨在在搜索空间中寻找参数，使得结果参数化函数优化某些目标（DRL 的预期回报或 ESs 的适应度评分）。主要区别在于
    ESs 与 DRL 不同，不计算梯度也不使用反向传播。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Value-based RL methods usually operate in discrete action spaces while the actor-critic
    architecture extends this ability to continuous action spaces. ESs can operate
    on discrete or continuous action spaces by default.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于值的 RL 方法通常在离散动作空间中操作，而 actor-critic 架构将这一能力扩展到连续动作空间。ESs 默认可以在离散或连续动作空间中操作。
- en: III Fundamental Algorithms
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基础算法
- en: 'TABLE I: Fundamental (Deep) Reinforcement Learning and Evolution Strategies
    algorithms'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基础 (深度) 强化学习和进化策略算法
- en: '| Algorithm | Classification | Action Space | Memory Consumed | Limitations
    | Backprop. | Ref. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 分类 | 动作空间 | 消耗的内存 | 局限性 | 反向传播 | 参考 |'
- en: '| SARSA | on-policy value-based RL | discrete | exponential in state and action
    spaces | tackling continuous space, does not generalize between similar states
    | X | [[1](#bib.bib1)] |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SARSA | 在策略值基 RL | 离散 | 状态和动作空间的指数级 | 处理连续空间，无法在类似状态之间泛化 | X | [[1](#bib.bib1)]
    |'
- en: '| Q-learning | off-policy value-based RL | discrete | exponential in state
    and action spaces | tackling continuous space, does not generalize between similar
    states | X | [[27](#bib.bib27)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Q-learning | 离策略值基 RL | 离散 | 状态和动作空间的指数级 | 处理连续空间，无法在类似状态之间泛化 | X | [[27](#bib.bib27)]
    |'
- en: '| REINFORCE | policy-based RL | discrete/continuous | typically, it requires
    storing DNN parameters | data inefficiency, higher variance compared to DQN |
    ✓ | [[11](#bib.bib11)] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| REINFORCE | 基于策略的 RL | 离散/连续 | 通常需要存储 DNN 参数 | 数据效率低，较高的方差相比 DQN | ✓ | [[11](#bib.bib11)]
    |'
- en: '| DQN | off-policy value-based RL | discrete | it requires storing DNN parameters
    and a replay buffer | the learning of the Q-function can suffer from instabilities
    | ✓ | [[28](#bib.bib28)] [[2](#bib.bib2)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| DQN | 离策略值基 RL | 离散 | 需要存储 DNN 参数和回放缓冲区 | Q 函数的学习可能会受到不稳定性的影响 | ✓ | [[28](#bib.bib28)]
    [[2](#bib.bib2)] |'
- en: '| CMA-ES | black-box ES optimization | discrete/continuous | high memory requirement
    | high space and time complexity when dealing with large scale optimization problems
    | X | [[29](#bib.bib29)] [[30](#bib.bib30)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| CMA-ES | 黑箱 ES 优化 | 离散/连续 | 高内存需求 | 处理大规模优化问题时具有高空间和时间复杂度 | X | [[29](#bib.bib29)]
    [[30](#bib.bib30)] |'
- en: '| NES & OpenAI-ES | black-box ES optimization | discrete/continuous | less
    memory usage than CMA-ES | data inefficiency due to gradient approximation | X
    | [[31](#bib.bib31)] [[32](#bib.bib32)] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| NES & OpenAI-ES | 黑箱 ES 优化 | 离散/连续 | 内存使用少于 CMA-ES | 由于梯度近似而导致的数据效率低 | X
    | [[31](#bib.bib31)] [[32](#bib.bib32)] |'
- en: Fundamental algorithms of (D)RL and ESs are introduced in this section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 (D)RL 和 ESs 的基础算法。
- en: III-A Reinforcement Learning Algorithms
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 强化学习算法
- en: 'SARSA is a model-free algorithm that leverages temporal-differences for prediction [[1](#bib.bib1)].
    It updates the Q-value, $Q(s_{t},a_{t})$, while following a policy. The interaction
    between the agent and environment results in the following sequence $\dots,s_{t},a_{t},r_{t+1},s_{t+1},a_{t+1},\dots$:
    the agent takes an action $a_{t}$ while being in a state $s_{t}$, and consequently,
    the environment transitions to a state $s_{t+1}$ and the agent observes a reward
    $r_{t+1}$. For action selection, SARSA uses $\varepsilon$-greedy algorithm, which
    selects the action with maximum $Q(s_{t},a_{t})$ with probability of $1-\varepsilon$,
    and otherwise, it draws an action uniformly from $\mathcal{A}$. SARSA is an on-policy
    algorithm, that is, it evaluates and improves the same policy that selects the
    taken actions. SARSA’s update equation is'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 是一种无模型算法，它利用时间差分方法进行预测 [[1](#bib.bib1)]。它在遵循某一策略的同时更新 Q 值 $Q(s_{t},a_{t})$。代理与环境的互动产生了以下序列
    $\dots,s_{t},a_{t},r_{t+1},s_{t+1},a_{t+1},\dots$：代理在状态 $s_{t}$ 下采取一个动作 $a_{t}$，随后环境过渡到状态
    $s_{t+1}$，代理观察到奖励 $r_{t+1}$。在动作选择中，SARSA 使用 $\varepsilon$-贪婪算法，它以 $1-\varepsilon$
    的概率选择具有最大 $Q(s_{t},a_{t})$ 的动作，否则从 $\mathcal{A}$ 中均匀抽取一个动作。SARSA 是一种策略算法，即它评估并改进选择的动作所遵循的策略。SARSA
    的更新方程是
- en: '|  | $Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\big{[}r_{t+1}+\\ \gamma
    Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})\big{]}\text{,}$ |  | (3) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\big{[}r_{t+1}+\\ \gamma
    Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})\big{]}\text{,}$ |  | (3) |'
- en: where $\alpha$ is the learning rate.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是学习率。
- en: 'Q-Learning [[1](#bib.bib1)] is similar to SARSA with a key difference: It is
    an off-policy algorithm, which means that it learns an optimal Q-value function
    from data obtained via any policy (without introducing a bias). In particular,
    The Q-learning update rule compares the Q-value of the current state-action pair,
    $Q(s_{t},a_{t})$, with a pair from the next state that has the maximum Q-value,
    $Q(s_{t+1},a_{t+1})$, which is not necessarily the one chosen by $\varepsilon$-greedy
    as in SARSA. The update rule of Q-learning is'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Learning [[1](#bib.bib1)] 类似于 SARSA，但有一个关键区别：它是一种离策略算法，这意味着它可以从通过任何策略获得的数据中学习最优
    Q 值函数（而不会引入偏差）。特别是，Q-learning 更新规则将当前状态-动作对的 Q 值 $Q(s_{t},a_{t})$ 与下一个状态中具有最大
    Q 值的对 $Q(s_{t+1},a_{t+1})$ 进行比较，这个值不一定是 SARSA 中由 $\varepsilon$-贪婪选择的值。Q-learning
    的更新规则是
- en: '|  | $Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\big{[}r_{(t+1)}+\\ \gamma\max_{a}Q(s_{(t+1)},a_{(t+1)})-Q(s_{t},a_{t})\big{]}\text{.}$
    |  | (4) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\big{[}r_{(t+1)}+\\ \gamma\max_{a}Q(s_{(t+1)},a_{(t+1)})-Q(s_{t},a_{t})\big{]}\text{.}$
    |  | (4) |'
- en: Off-policy algorithms are more data-efficient than on-policy ones, because they
    can use the collected data repeatedly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略算法比策略算法更具数据效率，因为它们可以重复利用收集到的数据。
- en: 'REINFORCE [[11](#bib.bib11)] is a fundamental stochastic gradient descent algorithm
    for policy gradient algorithms. It leverages a DNN to approximate the policy $\pi$
    and update its parameters $\boldsymbol{\theta}$. The network receives an input
    from the environment and outputs a probability distribution over the action space,
    $\mathcal{A}$. The steps involved in the implementation of REINFORCE are:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE [[11](#bib.bib11)] 是一种基本的随机梯度下降算法，用于策略梯度算法。它利用 DNN 来近似策略 $\pi$ 并更新其参数
    $\boldsymbol{\theta}$。网络从环境中接收输入，并输出一个动作空间 $\mathcal{A}$ 上的概率分布。REINFORCE 实现的步骤包括：
- en: '1.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Initialize a Random Policy (i.e., the parameters of a DNN)
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化一个随机策略（即，DNN 的参数）
- en: '2.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Use the policy $\pi_{\boldsymbol{\theta}}$ to collect a trajectory $\tau=(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...,a_{H},r_{H+1},s_{H+1})$
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用策略 $\pi_{\boldsymbol{\theta}}$ 收集一个轨迹 $\tau=(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...,a_{H},r_{H+1},s_{H+1})$
- en: '3.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Estimate the return for this trajectory
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 估计这个轨迹的回报
- en: '4.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Use the estimate of the return to calculate the policy gradient:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用回报的估计来计算策略梯度：
- en: '|  | $\nabla_{\boldsymbol{\theta}}\mathcal{J}(\boldsymbol{\theta})=\mathbb{E}_{\pi_{\boldsymbol{\theta}}}[\nabla\log\pi(a&#124;s;\boldsymbol{\theta})Q_{\pi}(s,a)]$
    |  | (5) |'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\nabla_{\boldsymbol{\theta}}\mathcal{J}(\boldsymbol{\theta})=\mathbb{E}_{\pi_{\boldsymbol{\theta}}}[\nabla\log\pi(a|s;\boldsymbol{\theta})Q_{\pi}(s,a)]$
    |  | (5) |'
- en: '5.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Adjust the weights $\boldsymbol{\theta}$ of the Policy:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整策略的权重 $\boldsymbol{\theta}$：
- en: $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\alpha\nabla_{\theta}J(\boldsymbol{\theta})$
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\alpha\nabla_{\theta}J(\boldsymbol{\theta})$
- en: '6.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Repeat from step 2 until termination.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从第 2 步开始重复，直到终止。
- en: Deep Q-network (DQN) [[28](#bib.bib28)] combines Q-learning with a convolutional
    neural network (CNN) [[33](#bib.bib33)] to act in environments with high-dimensional
    input spaces (e.g., images of Atari games). It gets a state (e.g., a mini-batch
    of images) as input and produces Q-values of all possible actions. The CNN is
    used to approximate the optimal action-value function (or Q-function). Such usage,
    however, causes the DRL agent to be unstable [[34](#bib.bib34)]. To counter that,
    DQN samples an experience replay [[35](#bib.bib35)] dataset $D_{t}=\{(s_{1},a_{1},r_{2},s_{2}),\dots,(s_{t},a_{t},r_{t+1},s_{t+1})\}$
    and uses a target network that is updated only after a certain number of iterations.
    To update the network parameters at iteration $i$, DQN uses the following loss
    function
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络（DQN）[[28](#bib.bib28)]将Q学习与卷积神经网络（CNN）[[33](#bib.bib33)]结合起来，以在高维输入空间（例如，Atari游戏的图像）中进行操作。它接收状态（例如，一小批图像）作为输入，并生成所有可能动作的Q值。CNN用于近似最优动作值函数（或Q函数）。然而，这种使用会导致DRL代理不稳定[[34](#bib.bib34)]。为此，DQN采样一个经验回放[[35](#bib.bib35)]数据集$D_{t}=\{(s_{1},a_{1},r_{2},s_{2}),\dots,(s_{t},a_{t},r_{t+1},s_{t+1})\}$，并使用一个仅在特定迭代次数后更新的目标网络。为了在迭代$i$中更新网络参数，DQN使用以下损失函数
- en: '|  | $L_{i}(\theta_{i})=\mathbb{E}_{(s,a,r,s^{\prime})}\sim U(D)\\ \bigg{[}\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime};\overline{\boldsymbol{\theta}_{i}})-Q(s^{\prime},a^{\prime};\boldsymbol{\theta}_{i})\right)^{2}\bigg{]}$
    |  | (6) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{i}(\theta_{i})=\mathbb{E}_{(s,a,r,s^{\prime})}\sim U(D)\\ \bigg{[}\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime};\overline{\boldsymbol{\theta}_{i}})-Q(s^{\prime},a^{\prime};\boldsymbol{\theta}_{i})\right)^{2}\bigg{]}$
    |  | (6) |'
- en: where $\theta_{i}$ and $\overline{\theta_{i}}$ are the parameters of the Q-network
    and target network, respectively; and the experiences, $(s,a,r,s^{\prime})$, are
    drawn from $D$ uniformly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta_{i}$和$\overline{\theta_{i}}$分别是Q网络和目标网络的参数；经验$(s,a,r,s^{\prime})$是从$D$中均匀抽取的。
- en: III-B Evolutionary Strategies Algorithms
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 进化策略算法
- en: 'The (1+1)-ES (one parent, one offspring) is the simplest ES conceived by Rechenberg
    [[36](#bib.bib36)]. First, a parent candidate solution, $\textbf{x}_{p}$, is drawn
    according to a uniform random distribution from an initial set of solutions, $\{\textbf{x}_{i},\textbf{x}_{j}\}$.
    The selected parent, $\textbf{x}_{p}$, together with its fitness values enter
    the evolution loop. In each generation (or iteration) an offspring candidate solution,
    $\textbf{x}_{o}$, is created by adding a vector drawn from an uncorrelated multivariate
    normal distribution to $\textbf{x}_{p}$ as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (1+1)-ES（一个父代，一个后代）是Rechenberg提出的最简单的ES[[36](#bib.bib36)]。首先，从初始解集$\{\textbf{x}_{i},\textbf{x}_{j}\}$中按均匀随机分布抽取一个父代候选解$\textbf{x}_{p}$。选择的父代$\textbf{x}_{p}$及其适应度值进入进化循环。在每一代（或迭代）中，通过将从无相关多元正态分布中抽取的向量添加到$\textbf{x}_{p}$中来创建一个后代候选解$\textbf{x}_{o}$，如下所示：
- en: '|  | $\textbf{x}_{o}=\textbf{x}_{p}+\textbf{y}\sigma,\textbf{y}\sim\mathcal{N}(0,\textbf{I})\text{.}$
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{x}_{o}=\textbf{x}_{p}+\textbf{y}\sigma,\textbf{y}\sim\mathcal{N}(0,\textbf{I})\text{.}$
    |  |'
- en: 'If the offspring $\textbf{x}_{o}$ is found to be fitter than the parent $\textbf{x}_{p}$
    then it becomes the new parent for the next generation, otherwise it is discarded.
    This process is repeated until a termination condition is met. The amount of mutation
    (or perturbation) added to $\textbf{x}_{p}$ is controlled by the stepsize parameter
    $\sigma$. The value of $\sigma$ is updated every predefined number of iterations
    according to the well-known $\frac{1}{5th}$ success rule  [[37](#bib.bib37), [38](#bib.bib38)]:
    if $\textbf{x}_{o}$ is fitter than $\textbf{x}_{p}$ $\frac{1}{5th}$ of the times
    then $\sigma$ should stay the same; if $\textbf{x}_{o}$ is fitter more than $\frac{1}{5th}$
    of the times then $\sigma$ should be increased, and otherwise it should be decreased.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果后代$\textbf{x}_{o}$被发现比父代$\textbf{x}_{p}$更优，则$\textbf{x}_{o}$将成为下一代的新父代，否则它会被丢弃。这个过程会重复直到满足终止条件。对$\textbf{x}_{p}$添加的突变（或扰动）的量由步长参数$\sigma$控制。$\sigma$的值会在每个预定义的迭代次数后根据著名的$\frac{1}{5th}$成功规则[[37](#bib.bib37),
    [38](#bib.bib38)]进行更新：如果$\textbf{x}_{o}$比$\textbf{x}_{p}$优越的次数占$\frac{1}{5th}$，则$\sigma$保持不变；如果$\textbf{x}_{o}$优越的次数超过$\frac{1}{5th}$，则$\sigma$应增加，否则应减少。
- en: The ($\mu/\rho\overset{+}{,}\lambda$)-ES was originally proposed by Schwefel
    [[39](#bib.bib39)] as an extension to the (1+1)-ES. Instead of using one parent
    to generate one offspring, it uses $\mu$ parents to generate $\lambda$ offsprings
    using both recombination and mutation. In the comma-variation of this algorithm
    (i.e., ($\mu/\rho{,}\lambda$)-ES) the selection of the parents for the next generation
    happens solely from the offsprings. Whereas in the plus-variation, the selection
    of the parents for the next generation happens from the union of the offsprings
    and old parents. The $\rho$ in the name of the algorithm refers to the number
    of parents used to generate each offspring.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ($\mu/\rho\overset{+}{,}\lambda$)-ES 最初由 Schwefel 提出[[39](#bib.bib39)]，作为对 (1+1)-ES
    的扩展。它不是使用一个父代生成一个后代，而是使用 $\mu$ 个父代通过重组和变异生成 $\lambda$ 个后代。在该算法的逗号变异版本（即 ($\mu/\rho{,}\lambda$)-ES）中，下一代的父代完全从后代中选择。而在加号变异中，下一代的父代从后代和旧父代的联合中选择。算法名称中的
    $\rho$ 指的是用于生成每个后代的父代数量。
- en: An element (or an individual) that the ($\mu/\rho\overset{+}{,}\lambda$)-ES
    evolves consists of (x, s, $f$) where x is the candidate solution, s are the strategy
    parameters that control the significance of the mutation, and $f$ holds the fitness
    value of x. Consequently, the evolution process itself tunes the strategy parameters
    which is known as self-adaptation. Thus, unlike (1+1)-ES, ($\mu/\rho\overset{+}{,}\lambda$)
    do not need external control settings to adjust the strategy parameters.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 ($\mu/\rho\overset{+}{,}\lambda$)-ES 演化的元素（或个体）由 (x, s, $f$) 组成，其中 x 是候选解，s
    是控制变异重要性的策略参数，$f$ 是 x 的适应度值。因此，演化过程本身调整策略参数，这称为自适应。因此，与 (1+1)-ES 不同，($\mu/\rho\overset{+}{,}\lambda$)
    不需要外部控制设置来调整策略参数。
- en: 'Covariance Matrix Adaptation Evolution Strategies (CMA-ES) is one of the most
    popular gradient-free optimisation algorithms [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. To search a solution space, it samples a
    population, $\lambda$, of new search points (offsprings) from a multivariate normal
    distribution:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵适应演化策略（CMA-ES）是最流行的无梯度优化算法之一[[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)]。为了搜索解空间，它从多元正态分布中采样一个新搜索点（后代）的种群 $\lambda$：
- en: '|  | $\boldsymbol{x}_{i}^{g+1}=\boldsymbol{m}^{(g)}+\sigma^{(g)}\mathcal{N}(\boldsymbol{0},\boldsymbol{C}^{(g)})\text{
    for }i=1\text{, \ldots,}\lambda\text{,}$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{x}_{i}^{g+1}=\boldsymbol{m}^{(g)}+\sigma^{(g)}\mathcal{N}(\boldsymbol{0},\boldsymbol{C}^{(g)})\text{
    for }i=1\text{, \ldots,}\lambda\text{,}$ |  |'
- en: 'where $g$ is the generation number (i.e., $g=1,2,3,\dots$), $\boldsymbol{x}_{i}\in\mathbb{R}^{n}$
    is the $i$-th offspring, $\boldsymbol{m}$ and $\sigma$ denote the mean and standard
    deviation of $\boldsymbol{x}$, $\boldsymbol{C}$ represents the covariance matrix,
    and $\mathcal{N}(\boldsymbol{0},\boldsymbol{C})$ is a multivariate normal distribution.
    To compute the mean for the next generation, $\boldsymbol{m}^{g+1}$, CMA-ES computes
    a weighted average of the best—according to their fitness values—$\mu$ candidate
    solutions, where $\mu<\lambda$ represents the parent population size. Through
    this selection and the assigned weights, CMA-ES biases the computed mean towards
    the best candidate solutions of the current population. It automatically adapts
    the stepsize $\sigma$ (the mutation strength) using the Cumulative Stepsize Adaption
    (CSA) algorithm [[40](#bib.bib40)] and an evolution path, $\boldsymbol{p}_{\sigma}$:
    if $\boldsymbol{p}_{\sigma}$ is longer than the expected length of the evolution
    path under random selection $\mathbb{E}||\mathcal{N}(\boldsymbol{0},\boldsymbol{I})||$,
    then increase the stepsize; otherwise, decrease it. To direct the search towards
    promising directions, CMA-ES updates the covariance matrix in each iteration.
    The update consists of two main parts: (i) rank-1 update, which computes an evolution
    path for the mutation distribution means, similarly to the stepsize evolution
    path; and (ii) rank-$\mu$ update, which computes a covariance matrix as a weighted
    sum of covariances of the best $\mu$ individuals. The obtained results from these
    steps are used to update the covariance matrix $\boldsymbol{C}$ itself. The algorithm
    iterates until a satisfactory solution is found (we refer the interested reader
    to  [[43](#bib.bib43)] for a more detailed explanation).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g$ 是代数编号（即 $g=1,2,3,\dots$），$\boldsymbol{x}_{i}\in\mathbb{R}^{n}$ 是第 $i$
    个后代，$\boldsymbol{m}$ 和 $\sigma$ 表示 $\boldsymbol{x}$ 的均值和标准差，$\boldsymbol{C}$ 代表协方差矩阵，$\mathcal{N}(\boldsymbol{0},\boldsymbol{C})$
    是多元正态分布。为了计算下一代的均值 $\boldsymbol{m}^{g+1}$，CMA-ES 计算了根据适应度值排序的最佳 $\mu$ 候选解的加权平均，其中
    $\mu<\lambda$ 表示父代种群规模。通过这种选择和分配的权重，CMA-ES 将计算出的均值偏向当前种群的最佳候选解。它使用累计步长自适应（CSA）算法
    [[40](#bib.bib40)] 和进化路径 $\boldsymbol{p}_{\sigma}$ 自动调整步长 $\sigma$（变异强度）：如果 $\boldsymbol{p}_{\sigma}$
    长于随机选择下期望的进化路径长度 $\mathbb{E}||\mathcal{N}(\boldsymbol{0},\boldsymbol{I})||$，则增加步长；否则，减少它。为了将搜索引导向有前途的方向，CMA-ES
    在每次迭代中更新协方差矩阵。更新包括两个主要部分：（i）秩-1 更新，计算变异分布均值的进化路径，与步长进化路径类似；（ii）秩-$\mu$ 更新，计算作为最佳
    $\mu$ 个体协方差加权和的协方差矩阵。这些步骤得到的结果用于更新协方差矩阵 $\boldsymbol{C}$ 本身。算法迭代直到找到满意的解（我们建议感兴趣的读者参考
    [[43](#bib.bib43)] 以获取更详细的解释）。
- en: 'Natural Evolution Strategies (NES) is similar in many ways to the previously
    defined ES algorithms, the core idea behind it relates to the use of gradients
    to adequately update a search distribution [[44](#bib.bib44)]. The basic idea
    behind NES consists of:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 自然进化策略（NES）在许多方面类似于之前定义的 ES 算法，其核心思想与使用梯度来适当地更新搜索分布有关 [[44](#bib.bib44)]。NES
    的基本思想包括：
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sampling: NES samples its individuals from a probability distribution (usually
    a Gaussian distribution) over the search space. The end goal of NES is to update
    the distribution parameters $\boldsymbol{\theta}$ to maximize the average fitness
    $F(\boldsymbol{x})$ of the sampled individuals $\boldsymbol{x}$.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采样：NES 从搜索空间上的概率分布（通常是高斯分布）中采样其个体。NES 的最终目标是更新分布参数 $\boldsymbol{\theta}$，以最大化被采样个体
    $\boldsymbol{x}$ 的平均适应度 $F(\boldsymbol{x})$。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Search gradient estimation: NES estimates a search gradient on the parameters
    by evaluating the samples previously computed. It then decides on the best direction
    to take to achieve a higher expected fitness.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索梯度估计：NES 通过评估之前计算的样本来估计参数上的搜索梯度。然后，它决定采取最佳方向以实现更高的期望适应度。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gradient ascent: NES computes gradient ascent along the estimated gradient'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度上升：NES 沿着估计的梯度计算梯度上升
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Iterates over the previous steps until a stopping criterion is met [[44](#bib.bib44)].
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在达到停止标准之前，对前面的步骤进行迭代 [[44](#bib.bib44)]。
- en: Salimans et al. [[4](#bib.bib4)] proposed a variant of NES for optimizing the
    policy parameters $\theta$. As gradients are unavailable, they are estimated via
    gaussian smoothing of the objective function $F(X)$ which represents the expected
    return.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans 等人 [[4](#bib.bib4)] 提出了 NES 的一种变体，用于优化策略参数 $\theta$。由于没有梯度，它们通过对目标函数
    $F(X)$ 进行高斯平滑来估计梯度，该函数表示期望回报。
- en: III-C Comparison
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 比较
- en: 'Our main observations of the fundamental algorithms are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对基本算法的主要观察如下：
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Both ES and on-policy RL algorithms are data inefficient: on-policy algorithms
    make use of data that is generated from the current policy and discard older data;
    ES discard all but a sub-set of candidate solutions in each iteration.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ES和基于策略的RL算法都是数据低效的：基于策略的算法使用当前策略生成的数据并丢弃旧数据；ES则在每次迭代中丢弃所有除了一个子集的候选解。
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The computation requirements per iteration of ESs are often lower than that
    of DRL as it does not require backpropagating error values.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每次迭代的ES计算需求通常低于DRL，因为它不需要反向传播误差值。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Value-based DRL algorithms such as DQN can be data-efficient because they can
    work with a replay memory that allows a reuse of off-policy data. However, they
    can become unstable for long horizons and high discount factors [[45](#bib.bib45)].
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于价值的DRL算法，如DQN，可能是数据高效的，因为它们可以使用重放记忆来重复利用离策略数据。然而，对于长时间范围和高折扣因子，它们可能变得不稳定 [[45](#bib.bib45)]。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Policy-based RL and ESs are similar in that they both search for good policies
    directly.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于策略的RL和ES相似之处在于它们都直接寻求好的策略。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Table [I](#S3.T1 "TABLE I ‣ III Fundamental Algorithms ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey") highlights important
    characteristics of the mentioned algorithms.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 [I](#S3.T1 "表 I ‣ III 基本算法 ‣ 深度强化学习与进化策略：比较调查")突出了提到的算法的重要特征。
- en: IV Deep Reinforcement Learning Versus Evolution Strategies
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度强化学习与进化策略
- en: This section compares different aspects of DRL and ESs, such as their ability
    to parallelize computations, explore an environment, and learn in multi-agent
    and dynamic settings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本节比较了DRL和ES的不同方面，如它们的并行计算能力、环境探索能力以及在多智能体和动态环境中的学习能力。
- en: IV-A Parallelism
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 并行性
- en: '![Refer to caption](img/3076894c270115b3fd2812c8723ab4b9.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3076894c270115b3fd2812c8723ab4b9.png)'
- en: ((a)) Gorila [[46](#bib.bib46)]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ((a)) Gorila [[46](#bib.bib46)]
- en: '![Refer to caption](img/38c159fb2416fbcfc5de0f35891688e5.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38c159fb2416fbcfc5de0f35891688e5.png)'
- en: ((b)) A3C [[47](#bib.bib47)]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ((b)) A3C [[47](#bib.bib47)]
- en: '![Refer to caption](img/4b0787709ca50422c6ea6939801aa587.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b0787709ca50422c6ea6939801aa587.png)'
- en: ((c)) Batched A2C [[48](#bib.bib48)]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ((c)) 批量A2C [[48](#bib.bib48)]
- en: '![Refer to caption](img/10eacb339a4f73982c90d09c33f9ae70.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10eacb339a4f73982c90d09c33f9ae70.png)'
- en: ((d)) Ape-X [[49](#bib.bib49)]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ((d)) Ape-X [[49](#bib.bib49)]
- en: '![Refer to caption](img/d971d351cdb5cff7fca3dcdf21e29648.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d971d351cdb5cff7fca3dcdf21e29648.png)'
- en: ((e)) Impala [[50](#bib.bib50)]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ((e)) Impala [[50](#bib.bib50)]
- en: '![Refer to caption](img/0133f8c0525c4c09c289e8450c92c79b.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0133f8c0525c4c09c289e8450c92c79b.png)'
- en: ((f)) SEED [[51](#bib.bib51)]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ((f)) SEED [[51](#bib.bib51)]
- en: 'Figure 3: Parallel Deep Reinforcement Learning algorithms architectures'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：并行深度强化学习算法架构
- en: Despite the success of DRL and ESs, they are still computationally intensive
    approaches to tackle sequential decision-making problems. Parallel execution is
    thus an important approach to speed up the computation [[46](#bib.bib46)]. Below,
    we look into the rich literature of parallel DRL and ES algorithms.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DRL和ES取得了成功，但它们仍然是计算密集型的方法来解决序列决策问题。因此，并行执行是加速计算的重要方法 [[46](#bib.bib46)]。接下来，我们将深入研究丰富的并行DRL和ES算法文献。
- en: IV-A1 Parallelism in Deep Reinforcement Learning
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 深度强化学习中的并行性
- en: 'In parallel-DRL many agents (or actors) run in parallel to accelerate the learning
    process. Each actor gathers its own learning experiences. These experiences are,
    then, shared to optimize a global network (Figure [3](#S4.F3 "Figure 3 ‣ IV-A
    Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep
    Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")) [[52](#bib.bib52),
    [53](#bib.bib53)]. The rest of this section presents important parallel-DRL algorithms.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行-DRL中，多个智能体（或演员）并行运行以加速学习过程。每个演员收集自己的学习经验。这些经验随后被共享以优化一个全球网络（图 [3](#S4.F3
    "图 3 ‣ IV-A 并行 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调查")）[[52](#bib.bib52), [53](#bib.bib53)]。本节其余部分介绍重要的并行-DRL算法。
- en: 'Gorila [[46](#bib.bib46)] is the first massively distributed architecture for
    DRL. It consists of four major components: actors, learners, a parameter server,
    and a replay buffer (Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ IV-A Parallelism
    ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey")). Each actor has
    its Q-network. It interacts with an instance of the same environment and stores
    the generated experiences (i.e., a set of {$s,a,r,s^{\prime}$}) in the replay
    buffer. Learners sample the experience replay buffer and use DQN to compute gradients.
    Sampling from a buffer reduces the correlation between data updates and the effect
    of non-stationarity in the data. These gradients are then sent asynchronously
    to the parameter server to update its Q-network. After that, the parameter server
    updates the actors’ and learners’ Q-networks to synchronize the learning process.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Gorila [[46](#bib.bib46)] 是第一个大规模分布式 DRL 架构。它由四个主要组件组成：演员、学习者、参数服务器和重放缓冲区（图
    [3(a)](#S4.F3.sf1 "图 3 ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调研")）。每个演员都有其
    Q 网络。它与相同环境的一个实例进行交互，并将生成的经验（即一组 {$s,a,r,s^{\prime}$}）存储在重放缓冲区中。学习者从经验重放缓冲区中采样，并使用
    DQN 计算梯度。从缓冲区中采样减少了数据更新之间的相关性和数据的非平稳性效应。这些梯度随后异步地发送到参数服务器以更新其 Q 网络。之后，参数服务器更新演员和学习者的
    Q 网络，以同步学习过程。
- en: 'A3C & GA3C. While using a replay buffer helps in stabilizing the learning process,
    it requires additional memory and computational power and can only be used with
    off-policy algorithms. Motivated by these limitations, Mnih et al. [[47](#bib.bib47)]
    introduced the Asynchronous Advantage Actor-Critic (A3C) as an alternative to
    Gorila. A3C consists of a global network and multiple agents each with its own
    network (Figure [3(b)](#S4.F3.sf2 "In Figure 3 ‣ IV-A Parallelism ‣ IV Deep Reinforcement
    Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning Versus Evolution
    Strategies: A Comparative Survey")). The agents are implemented as CPU threads
    within a single machine, which reduces the communication cost imposed by distributed
    systems such as Gorila. The agents interact in parallel with their independent
    copy of the environment. Each agent calculates the value and the policy gradients
    which are used to update the global network parameters. This method of learning
    diversifies and decorrelates data updates which stabilize the learning process.
    GA3C [[54](#bib.bib54)] makes use of GPUs and shows better scalability and performance
    than A3C.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 和 GA3C。虽然使用重放缓冲区有助于稳定学习过程，但它需要额外的内存和计算能力，并且只能用于离策略算法。基于这些限制，Mnih 等人 [[47](#bib.bib47)]
    引入了异步优势演员-评论家（A3C）作为 Gorila 的替代方案。A3C 包含一个全局网络和多个具有自己网络的代理（图 [3(b)](#S4.F3.sf2
    "图 3 ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调研")）。这些代理作为单台机器中的 CPU 线程实现，从而减少了像
    Gorila 这样的分布式系统带来的通信成本。代理与其独立环境副本并行交互。每个代理计算值函数和策略梯度，这些梯度用于更新全局网络参数。这种学习方法使数据更新多样化和去相关，从而稳定学习过程。GA3C
    [[54](#bib.bib54)] 利用 GPU，并显示出比 A3C 更好的可扩展性和性能。
- en: 'Batched A2C & DPPO. A downside of A3C is that asynchronous updates may lead
    to sub-optimal collective updates to the global network. To overcome this, Batched
    Advantage Actor-Critic (Batched A2C) employs a master node (or a coordinator)
    to synchronize the update process of the global network [[48](#bib.bib48)]. Batched
    A2C tries to capitalize on the advantages of both Gorila and A3C. Similar to Gorila,
    Batched A2C runs on GPUs and the number of actors is highly scalable while still
    running on a single machine akin to A3C and GA3C [[54](#bib.bib54)]. Figure [3(c)](#S4.F3.sf3
    "In Figure 3 ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution
    Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey") presents the Batched A2C architecture. At each time step, Batched A2C
    samples from the policy and generates a batch of actions for $n_{w}$ workers on
    $n_{e}$ environment instances. The resulting experiences are then stored and used
    by the master to update the policy (global network). The batched approach allows
    for easy parallelization by synchronously updating a unique copy of the parameters,
    with the drawback of higher communication costs. Distributed Proximal Policy Optimization
    (DPPO) [[55](#bib.bib55)] features architecture similar to that of A2C, and uses
    the PPO [[56](#bib.bib56)] algorithm for learning.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 批量A2C与DPPO。A3C的一个缺点是异步更新可能导致对全局网络的次优集合更新。为了解决这个问题，批量优势演员-评论家（Batched A2C）采用了一个主节点（或协调者）来同步全局网络的更新过程[[48](#bib.bib48)]。批量A2C试图利用Gorila和A3C的优点。与Gorila类似，批量A2C在GPU上运行，演员数量具有高度可扩展性，同时仍在单台机器上运行，类似于A3C和GA3C[[54](#bib.bib54)]。图[3(c)](#S4.F3.sf3
    "在图3 ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调查")展示了批量A2C架构。在每个时间步，批量A2C从策略中采样，并为$n_{w}$个工作者生成一批动作，作用于$n_{e}$个环境实例。生成的经验随后被存储，并由主节点用于更新策略（全局网络）。批量方法通过同步更新唯一的参数副本来实现简单的并行化，但缺点是通信成本较高。分布式近端策略优化（DPPO）[[55](#bib.bib55)]的架构与A2C类似，采用PPO[[56](#bib.bib56)]算法进行学习。
- en: '![Refer to caption](img/83dcdc243875badceb84f819efa085a8.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/83dcdc243875badceb84f819efa085a8.png)'
- en: 'Figure 4: Parallel Deep Reinforcement Learning and Evolution Strategies algorithms
    shown on a timeline'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示在时间轴上的并行深度强化学习和进化策略算法
- en: 'Ape-X & R2D2. Ape-X [[49](#bib.bib49)] extends the prioritized experience buffer
    to the parallel-DRL settings and shows that this approach is highly scalable.
    The Ape-X architecture consists of many actors, a single learner, and a prioritized
    replay buffer (Figure [3(d)](#S4.F3.sf4 "In Figure 3 ‣ IV-A Parallelism ‣ IV Deep
    Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning
    Versus Evolution Strategies: A Comparative Survey")). Each actor interacts with
    its instance of the environment, gathers data, and computes its initial priorities.
    The generated experiences are stored in a shared prioritized buffer. The learner
    samples the buffer to update its network and the priorities of the experiences
    in the buffer. In addition, the learner also periodically updates the network
    parameters of the actors. Ape-X’s distributed architecture can be coupled with
    different learning algorithms such as DQN [[28](#bib.bib28)] and DDPG [[19](#bib.bib19)].
    R2D2 [[57](#bib.bib57)] has a similar architecture but outperforms Ape-X using
    recurrent neural network (RNN)-based RL agents.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Ape-X与R2D2。Ape-X[[49](#bib.bib49)]将优先经验缓冲区扩展到并行DRL设置，并展示了这种方法的高度可扩展性。Ape-X架构包括多个演员、一个学习者和一个优先回放缓冲区（图[3(d)](#S4.F3.sf4
    "在图3 ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调查")）。每个演员与其环境实例互动，收集数据并计算初始优先级。生成的经验被存储在一个共享的优先缓冲区中。学习者从缓冲区中采样以更新其网络和缓冲区中经验的优先级。此外，学习者还定期更新演员的网络参数。Ape-X的分布式架构可以与DQN[[28](#bib.bib28)]和DDPG[[19](#bib.bib19)]等不同学习算法结合使用。R2D2[[57](#bib.bib57)]具有类似的架构，但使用基于递归神经网络（RNN）的RL代理，性能超越了Ape-X。
- en: 'IMPALA. Due to the use of an on-policy method in an off-policy setting GA3C [[54](#bib.bib54)]
    suffers from poor convergence. IMPALA [[50](#bib.bib50)] corrected this with the
    use of V-trance: an off-policy actor-critic algorithm that aims at mitigating
    the effect of the lag between when actions are taken by the actors and when the
    learner estimates the gradient in distributed settings. IMPALA’s architecture
    consists of multiple actors interacting with their environment instances (Figure [3(e)](#S4.F3.sf5
    "In Figure 3 ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution
    Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey")). However, different from A3C’s actors, IMPALA’s actors send the gathered
    experiences (instead of the gradients) to the learner. The learner then utilizes
    these experiences to optimizes its policy and value functions. After that, it
    updates the actors’ models parameters. The separation between acting and learning
    and V-trace enable IMPALA to have stable learning while achieving high throughput.
    When training a very deep model the speed of a single GPU is often the bottleneck.
    To overcome this challenge, IMPALA (in addition to a single learner) supports
    multiple synchronized learners.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: IMPALA。由于在离策略设置中使用了策略方法，GA3C [[54](#bib.bib54)]存在收敛性差的问题。IMPALA [[50](#bib.bib50)]通过使用V-trace进行了修正：这是一种离策略的演员-评论家算法，旨在减轻演员执行动作和学习者在分布式设置中估计梯度之间的延迟影响。IMPALA的架构由多个演员与其环境实例交互组成（图 [3(e)](#S4.F3.sf5
    "在图3 ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较研究")）。然而，与A3C的演员不同，IMPALA的演员将收集到的经验（而不是梯度）发送给学习者。然后，学习者利用这些经验来优化其策略和值函数。之后，它更新演员的模型参数。行动和学习的分离以及V-trace使IMPALA能够在实现高吞吐量的同时保持稳定的学习。当训练非常深的模型时，单个GPU的速度通常是瓶颈。为了克服这一挑战，IMPALA（除了一个学习者）支持多个同步学习者。
- en: 'SEED. SEED [[51](#bib.bib51)] improves on the IMPALA system by moving inference
    to the learner (Figure [3(f)](#S4.F3.sf6 "In Figure 3 ‣ IV-A Parallelism ‣ IV
    Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning
    Versus Evolution Strategies: A Comparative Survey")). Consequently, the trajectories
    collection becomes part of the learner and the actors only send observations and
    actions to the learner. SEED makes use of TUPs and GPUs and shows significant
    improvement over other approaches.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: SEED。SEED [[51](#bib.bib51)]通过将推理移至学习者来改进IMPALA系统（图 [3(f)](#S4.F3.sf6 "在图3 ‣
    IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较研究")）。因此，轨迹收集成为学习者的一部分，演员仅将观察和动作发送给学习者。SEED利用TUPs和GPU，显示出相较于其他方法的显著改进。
- en: 'TABLE II: Parallelized Deep Reinforcement Learning and Evolution Strategies
    systems'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：并行化的深度强化学习和进化策略系统
- en: '| Algorithms | Architecture | Experiments | Limitations | Ref. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 架构 | 实验 | 局限性 | 参考 |'
- en: '| Gorila | replay buffer, actors, learners, and the parameter server each runs
    on a separate machine; GPU | outperforms DQN in 41/49 Atari games with reduced
    wall-time | high bandwidth for communicating gradients and parameters | [[46](#bib.bib46)]
    [[50](#bib.bib50)] |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Gorila | 回放缓冲区、演员、学习者和参数服务器各自运行在不同的机器上；GPU | 在41/49个Atari游戏中超越DQN，且减少了实际时间
    | 通信梯度和参数的带宽高 | [[46](#bib.bib46)] [[50](#bib.bib50)] |'
- en: '| A3C | many actors each running on a CPU thread and update a global network
    | outperforms Gorila on the Atari games while training for half the time | possibility
    of inconsistent parameter updates; large bandwidth between learner and actors;
    does not make use of hardware accelerators | [[47](#bib.bib47)] [[50](#bib.bib50)]
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| A3C | 许多演员每个运行在一个CPU线程上，并更新一个全球网络 | 在Atari游戏中训练时间减少一半而超越了Gorila | 可能出现不一致的参数更新；学习者和演员之间的带宽大；不利用硬件加速器
    | [[47](#bib.bib47)] [[50](#bib.bib50)] |'
- en: '| Batched A2C | multi-actors, a master, which synchronizes actors’ updates,
    and a global network; GPU | requires less training time as compared to Gorila,
    A3C, and GA3C | high variance in complex environments limits performance; episodes
    of varying length cause a slowdown during initialization | [[48](#bib.bib48)]
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 批量A2C | 多个演员、一个主节点，负责同步演员的更新，以及一个全球网络；GPU | 相比于Gorila、A3C和GA3C，需要更少的训练时间
    | 在复杂环境中的高方差限制了性能；长度变化的剧集在初始化期间会导致减速 | [[48](#bib.bib48)] |'
- en: '| Ape-X | multi-actors, a shared learner, and prioritized replay memory; CPU/GPU
    | outperforms Gorila and A3C on the Atari domain with less wall-clock training
    time | inefficient CPUs usage; large bandwidth for communicating between actors
    and learner | [[49](#bib.bib49)] |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Ape-X | 多个演员、共享学习者和优先回放记忆；CPU/GPU | 在Atari领域中超越了Gorila和A3C，训练时间更少 | CPU使用效率低；在演员和学习者之间通信需要大量带宽
    | [[49](#bib.bib49)] |'
- en: '| IMPALA | multi-actors; single or multiple learners; replay buffer; GPUs |
    outperforms Batched A2C and A3C. Less sensitive to hyperparameters selection than
    A3C | uses CPUs for inference which is inefficient; requires large bandwidth for
    sending parameters and trajectories | [[50](#bib.bib50)] |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| IMPALA | 多个演员；单个或多个学习者；回放缓冲区；GPU | 在性能上超越了批量A2C和A3C。对超参数选择的敏感度低于A3C | 使用CPU进行推理效率低；需要大量带宽来传送参数和轨迹
    | [[50](#bib.bib50)] |'
- en: '| SEED | multi-actors and a single learning; GPU/TPU | surpasses the performance
    of IMPALA | centralized inference may lead to increase latency | [[51](#bib.bib51)]
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SEED | 多个演员和单个学习；GPU/TPU | 超越了IMPALA的性能 | 集中推理可能会增加延迟 | [[51](#bib.bib51)]
    |'
- en: '| OpenAI ES | set of parallel workers; CPUs | outperforms other solution on
    most Atari games with less training: better than A3C in 23 games and worse in
    28 | evaluates many episodes requiring a lot of CPU time: 4000 CPU hours for a
    single ES run | [[4](#bib.bib4)] [[5](#bib.bib5)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI ES | 一组并行工作者；CPU | 在大多数Atari游戏中超越其他解决方案，训练时间更少：在23个游戏中优于A3C，在28个游戏中表现较差
    | 评估许多回合需要大量CPU时间：单次ES运行需要4000 CPU小时 | [[4](#bib.bib4)] [[5](#bib.bib5)] |'
- en: IV-A2 Parallelism in Evolution Strategies
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 进化策略中的并行性
- en: '![Refer to caption](img/58edcd5f4a0a11ee2bea78cfed34967d.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/58edcd5f4a0a11ee2bea78cfed34967d.png)'
- en: 'Figure 5: Parallelization steps in OpenAI-ES [[4](#bib.bib4)]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：OpenAI-ES中的并行化步骤 [[4](#bib.bib4)]
- en: 'Compared to DRL, ES algorithms require significantly less bandwidth to parallelize
    a given task. Salimans et al. [[4](#bib.bib4)] proposed OpenAI-ES: an algorithm
    derived from NES (see Section [III](#S3 "III Fundamental Algorithms ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey")) that directly optimizes
    the parameters $\theta$ of a policy. By sharing the seeds of the random processes
    prior to the optimization process, OpenAI-ES requires exchanging only scalars
    (minimal bandwidth) between workers to parallelize the search process. The main
    steps of OpenAI-ES are illustrated in Figure [5](#S4.F5 "Figure 5 ‣ IV-A2 Parallelism
    in Evolution Strategies ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus
    Evolution Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey") and summarized as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于深度强化学习（DRL），进化策略（ES）算法在并行化给定任务时需要显著更少的带宽。Salimans 等人[[4](#bib.bib4)] 提出了OpenAI-ES：一种源自NES的算法（见第[III](#S3
    "III 基本算法 ‣ 深度强化学习与进化策略：比较调查")节），该算法直接优化策略的参数 $\theta$。通过在优化过程之前共享随机过程的种子，OpenAI-ES
    只需在工作者之间交换标量（最小带宽）即可并行化搜索过程。OpenAI-ES的主要步骤如图[5](#S4.F5 "图 5 ‣ IV-A2 进化策略中的并行性
    ‣ IV-A 并行性 ‣ IV 深度强化学习与进化策略 ‣ 深度强化学习与进化策略：比较调查")所示，并总结如下：
- en: '1.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Sample a Gaussian noise vector, $\varepsilon_{i}\sim N(0,I)$.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采样一个高斯噪声向量，$\varepsilon_{i}\sim N(0,I)$。
- en: '2.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Evaluate workers’ fitness functions, $F_{i}\leftarrow F(\boldsymbol{\theta}_{t},\sigma\varepsilon_{i})$.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估工作者的适应度函数，$F_{i}\leftarrow F(\boldsymbol{\theta}_{t},\sigma\varepsilon_{i})$。
- en: '3.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Exchange the fitness values, $F_{i}$, between the workers.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在工作者之间交换适应度值 $F_{i}$。
- en: '4.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Reconstruct $\varepsilon_{i}$ using known random seeds.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用已知的随机种子重建 $\varepsilon_{i}$。
- en: '5.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Adjust parameters according to $\boldsymbol{\theta}_{t+1}\leftarrow\boldsymbol{\theta}_{t}+\alpha\frac{1}{n\sigma}\sum_{j=1}^{n}F_{j}\varepsilon_{j}$,
    where $\boldsymbol{\theta}$ is a weighted vector of a DNN.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 $\boldsymbol{\theta}_{t+1}\leftarrow\boldsymbol{\theta}_{t}+\alpha\frac{1}{n\sigma}\sum_{j=1}^{n}F_{j}\varepsilon_{j}$
    调整参数，其中 $\boldsymbol{\theta}$ 是DNN的加权向量。
- en: '6.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Repeat from step 2 until termination.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从步骤2开始重复，直到终止。
- en: Several researchers proposed algorithms inspired by OpenAI-ES [[4](#bib.bib4)].
    For example, Conti et al. [[32](#bib.bib32)] proposed Novelty Search Evolution
    Strategy (NS-ES) algorithm which hybridizes OpenAI-ES with Novelty Search (NS)—a
    directed exploration algorithm. The authors also introduced a variant of NS-ES
    by replacing NS with Quality Diversity (QD) algorithm. Their results show that
    the NS- and QD-based algorithms improve ES algorithms performance on RL tasks
    with sparse rewards, as they help avoid local optima. Liu et al. [[58](#bib.bib58)]
    proposed Trust Region Evolution Strategies (TRES). TRES is more sampled data efficient
    than classical ESs. It optimizes a surrogate objective function that enable reusing
    sampled data multiple times. TRES utilizes random seeds sharing introduced by
    [[4](#bib.bib4)] to achieve extremely low bandwidth. Finally, Fuks et al. [[29](#bib.bib29)]
    proposed Evolution Strategy with Progressive Episode Lengths (PEL). The main idea
    of PEL is to allow an agent to do small and easy tasks to gain knowledge quickly
    and then to use this knowledge to tackle more complex tasks. PEL leverages the
    same parallelization idea as OpenAI-ES [[4](#bib.bib4)] and shows a great improvement
    over canonical ES algorithms.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员提出了受 OpenAI-ES [[4](#bib.bib4)] 启发的算法。例如，Conti 等人 [[32](#bib.bib32)] 提出了新颖性搜索进化策略（NS-ES）算法，该算法将
    OpenAI-ES 与新颖性搜索（NS）——一种定向探索算法——混合。作者还通过用质量多样性（QD）算法替代 NS 引入了 NS-ES 的一种变体。他们的结果表明，基于
    NS 和 QD 的算法改善了在稀疏奖励的 RL 任务上 ES 算法的性能，因为它们有助于避免局部最优。Liu 等人 [[58](#bib.bib58)] 提出了信任区域进化策略（TRES）。TRES
    比经典 ES 更具数据采样效率。它优化了一个替代目标函数，使得可以多次重用采样数据。TRES 利用 [[4](#bib.bib4)] 引入的随机种子共享来实现极低的带宽。最后，Fuks
    等人 [[29](#bib.bib29)] 提出了带有渐进集长度的进化策略（PEL）。PEL 的主要思想是让智能体执行小而简单的任务以迅速获得知识，然后利用这些知识来处理更复杂的任务。PEL
    利用与 OpenAI-ES [[4](#bib.bib4)] 相同的并行化理念，并在经典 ES 算法上显示出显著的改进。
- en: IV-A3 Comparison
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 比较
- en: 'Our observations about parallelizing DRL and ES are:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对并行化 DRL 和 ES 的观察是：
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Despite the additional complexity, parallelism accelerates the execution of
    DRL and ES algorithms.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管增加了额外的复杂性，但并行化加速了 DRL 和 ES 算法的执行。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Parallel DRL usually communicates network parameters or gradient vectors between
    nodes, while parallel ES share only scalar values between workers.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并行 DRL 通常在节点之间传递网络参数或梯度向量，而并行 ES 仅在工作节点之间共享标量值。
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Table [II](#S4.T2 "TABLE II ‣ IV-A1 Parallelism in Deep Reinforcement Learning
    ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    snapshots the main characteristics of the presented algorithms, and Figure [4](#S4.F4
    "Figure 4 ‣ IV-A1 Parallelism in Deep Reinforcement Learning ‣ IV-A Parallelism
    ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey") shows how parallel
    DRL and ES algorithms evolved over time.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '表 [II](#S4.T2 "TABLE II ‣ IV-A1 Parallelism in Deep Reinforcement Learning
    ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    快照了所呈现算法的主要特征，图 [4](#S4.F4 "Figure 4 ‣ IV-A1 Parallelism in Deep Reinforcement
    Learning ‣ IV-A Parallelism ‣ IV Deep Reinforcement Learning Versus Evolution
    Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative
    Survey") 展示了并行 DRL 和 ES 算法如何随时间演变。'
- en: IV-B Exploration
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 探索
- en: One of the fundamental challenges that a learning agent faces when interacting
    with a partially known environment is the exploration-exploitation dilemma. That
    is, when should an agent try out suboptimal actions to improve its estimation
    of the optimal policy, and when should it use its current optimal policy estimation
    to make useful progress? This dilemma has attracted ample attention. Below, we
    summarize the main exploration methods in DRL and ESs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 学习智能体在与部分已知环境交互时面临的基本挑战之一是探索-利用困境。也就是说，智能体应该在什么时候尝试次优行动以改进对最优策略的估计，而在什么时候应该使用当前的最优策略估计来取得有用的进展？这个困境引起了广泛关注。下面，我们总结了
    DRL 和 ES 中主要的探索方法。
- en: IV-B1 Exploration in (Deep) Reinforcement Learning
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 （深度）强化学习中的探索
- en: Simple exploration techniques balance exploration and exploitation by selecting
    estimated optimal actions most of the time and random ones on occasion. This is
    the case for the well-known $\epsilon$-greedy exploration algorithm [[1](#bib.bib1)]
    that acts greedily with probability $1-\epsilon$ and selects a random action with
    probability $\epsilon$.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的探索技术通过大多数时候选择估计的最佳动作以及偶尔选择随机动作来平衡探索与利用。这就是著名的 $\epsilon$-贪婪探索算法[[1](#bib.bib1)]
    的情况，它以概率 $1-\epsilon$ 贪婪地选择动作，并以概率 $\epsilon$ 选择随机动作。
- en: More complex exploration strategies estimate the value of an exploratory action
    by making use of the environment-agent interaction history. Upper confidence bound
    (UCB) [[59](#bib.bib59)] does that by making the reward signal equals the estimated
    value of a Q-function plus a value that reflects the algorithm’s lack of confidence
    about this estimate,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的探索策略通过利用环境与代理的互动历史来估计探索性动作的价值。上置信界限（UCB）[[59](#bib.bib59)] 通过将奖励信号设置为Q函数的估计值加上一个反映算法对该估计值信心不足的值来实现这一点，
- en: '|  | $r^{+}(s,a)=r(s,a)+B(N(s))\text{,}$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{+}(s,a)=r(s,a)+B(N(s))\text{,}$ |  |'
- en: where $N(s)$ represents the frequency of visiting state $s$, and $B(N(s))$ is
    a reward bonus decreases with $N(s)$. In other words, UCB promotes the selection
    of actions with high rewards, $r(s,a)$, or the ones with high uncertainty (less
    frequently visited). The Thompson sampling method (TS) [[60](#bib.bib60)] maintains
    a distribution over the parameters of a model. In the beginning, it samples parameters
    at random. But as the agent explores an environment, TS adapts the distribution
    to favor more promising parameter sets. As such, UCB and TS naturally reduce the
    probability of selecting exploratory actions and become more confident about the
    optimal policy over time. Therefore, they are inherently more efficient than $\epsilon$-greedy.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N(s)$ 表示访问状态 $s$ 的频率，而 $B(N(s))$ 是随着 $N(s)$ 的增加而减少的奖励奖金。换句话说，UCB 鼓励选择具有高奖励
    $r(s,a)$ 的动作，或具有高不确定性（访问频率较低）的动作。汤普森采样方法（TS）[[60](#bib.bib60)] 维护模型参数的分布。最初，它随机采样参数。但是随着代理对环境的探索，TS
    调整分布以更倾向于更有前景的参数集。因此，UCB 和 TS 自然减少了选择探索性动作的概率，并随着时间的推移对最优策略变得更加自信。因此，它们本质上比 $\epsilon$-贪婪算法更高效。
- en: From RL to DRL. DRL agents act on environments with continuous or high-dimensional
    state-action spaces (e.g., Montezuma’s Revenge, StarCraft II). Such spaces render
    count-based algorithms (e.g., UCB) and the ones that require maintaining a distribution
    over state-action spaces (e.g., TS) useless in their original formulation. To
    explore such challenging environments with sparse reward signals, many algorithms
    have been proposed. Generally, these algorithms couple approximation techniques
    with exploration algorithms proposed for simple RL settings [[61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)]. Below we outline important
    DRL exploration algorithms.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从RL到DRL。DRL 代理在具有连续或高维状态-动作空间的环境中进行操作（例如，Montezuma’s Revenge，StarCraft II）。这样的空间使得基于计数的算法（例如，UCB）和需要维护状态-动作空间分布的算法（例如，TS）在其原始形式中无用。为了探索具有稀疏奖励信号的挑战性环境，已经提出了许多算法。一般来说，这些算法将近似技术与为简单
    RL 设置提出的探索算法相结合[[61](#bib.bib61)，[62](#bib.bib62)，[63](#bib.bib63)，[64](#bib.bib64)]。以下是重要的
    DRL 探索算法的概述。
- en: pseudo-count methods. To extend count-based exploration methods (e.g., UCB)
    to DRL settings, Bellemare et al. [[65](#bib.bib65)] approximate the counting
    process using a Context Tree Switching (CTS) density model. The model’s goal is
    to provide a score that increases when a state is revisited. The score is then
    used to generate a reward bonus that is inversely proportional to the score value.
    This bonus is then added to the reward signal provided by the environment to incentive
    the agent to visit less-visited states. Ostrovski et al. [[66](#bib.bib66)] improved
    this approach by replacing the simple CTS density model with a neural density
    model called PixelCNN. Another approach to utilize counting to explore environments
    with high-dimensional spaces is by mapping the observed states to a hashing table [[67](#bib.bib67)]
    and counting the hashing codes instead of states. Then a reward bonus similar
    to that of UCB is designed utilizing the hash code counts.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 伪计数方法。为了将基于计数的探索方法（例如 UCB）扩展到深度强化学习设置中，Bellemare 等人 [[65](#bib.bib65)] 使用上下文树切换（CTS）密度模型来近似计数过程。模型的目标是提供一个在状态被重新访问时增加的分数。然后，使用这个分数生成一个与分数值成反比的奖励奖金。这个奖金随后被加到环境提供的奖励信号中，以激励代理访问不常访问的状态。Ostrovski
    等人 [[66](#bib.bib66)] 通过用称为 PixelCNN 的神经密度模型替换简单的 CTS 密度模型改进了这一方法。另一种利用计数来探索高维空间环境的方法是将观察到的状态映射到哈希表
    [[67](#bib.bib67)] 中，并计数哈希代码而不是状态。然后，设计一个类似于 UCB 的奖励奖金，利用哈希代码计数。
- en: Approximate posterior sampling. Inspired by TS, Osband et al. [[68](#bib.bib68)]
    introduced Bootstrapped DQN. Bootstrapped DQN trains a DNN with N bootstrapped
    heads to approximate a distribution over Q-functions (bootstrapping is the process
    of approximating a distribution by sampling with replacement from a population
    multiple times and then aggregating these samples). At the start of each episode,
    Bootstrapped DQN draws a sample at random from the ensemble Q-functions and acts
    greedily with respect to this sample. This strategy enables an RL agent to do
    temporally extended exploration (or deep exploration) which is particularly important
    when the agent receives a sparse environmental reward. Chen et al. [[69](#bib.bib69)]
    integrates UCB with Bootstrapped DQN by calculating the mean and variance of a
    subset of the ensemble Q-functions. O’Donoghue et al. [[70](#bib.bib70)] combined
    TS with uncertainty Bellman equations to propagate the uncertainty in the Q-values
    over multiple timesteps.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 近似后验采样。受 TS 启发，Osband 等人 [[68](#bib.bib68)] 提出了自助法 DQN。自助法 DQN 使用 N 个自助头训练 DNN，以近似
    Q 函数的分布（自助法是通过从总体中重复抽样来近似分布的过程，然后聚合这些样本）。在每次训练开始时，自助法 DQN 从 Q 函数集成中随机抽取一个样本，并根据该样本贪婪地行动。这种策略使得强化学习代理能够进行时间上扩展的探索（或深度探索），这在代理收到稀疏环境奖励时尤其重要。Chen
    等人 [[69](#bib.bib69)] 通过计算 Q 函数集成子集的均值和方差，将 UCB 与自助法 DQN 集成在一起。O’Donoghue 等人 [[70](#bib.bib70)]
    将 TS 与不确定性 Bellman 方程结合，以在多个时间步上传播 Q 值的不确定性。
- en: 'TABLE III: Deep Reinforcement Learning exploration algorithms'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 深度强化学习探索算法'
- en: '| Algorithm | Description | Experiments | Ref. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 | 实验 | 参考文献 |'
- en: '| Bootstrapped DQN | uses DNNs and ensemble Q-functions to explore an environment
    | outperforms DQN by orders of magnitude in terms of cumulative rewards | [[68](#bib.bib68)]
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 自助法 DQN | 使用 DNN 和 Q 函数集成来探索环境 | 在累计奖励方面比 DQN 提高了几个数量级 | [[68](#bib.bib68)]
    |'
- en: '| UCB$+$InfoGain | integrates UCB with Q-function ensemble | outperforms bootstrapped
    DQN | [[70](#bib.bib70)] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| UCB$+$InfoGain | 将 UCB 与 Q 函数集成结合 | 优于自助法 DQN | [[70](#bib.bib70)] |'
- en: '| State pseudo-count | uses density models and pseudo-count to approximate
    state visitation count which is used to compute the reward bonus | superior to
    DQN, especially in hard-to-explore environments | [[65](#bib.bib65)] |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 状态伪计数 | 使用密度模型和伪计数来近似状态访问计数，用于计算奖励奖金 | 优于 DQN，尤其是在难以探索的环境中 | [[65](#bib.bib65)]
    |'
- en: '| VIME | measures information gain as KL divergence between current and updated
    distribution after an observation | improves the performance of TRPO [[71](#bib.bib71)],
    REINFORCE [[11](#bib.bib11)] when added to them | [[72](#bib.bib72)] |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| VIME | 测量当前观察后的信息增益，作为当前分布和更新分布之间的 KL 散度 | 当与 TRPO [[71](#bib.bib71)]、REINFORCE
    [[11](#bib.bib11)] 结合使用时，能改善它们的表现 | [[72](#bib.bib72)] |'
- en: '| ICM | uses a forward dynamic model to predict states and measures information
    gain as the difference between the predicted and observed state | outperforms
    TRPO-VIME in VizDoom (a sparse 3D environment) | [[73](#bib.bib73)] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ICM | 使用前向动态模型预测状态，并将信息增益度量为预测状态与观察状态之间的差异 | 在VizDoom（一个稀疏的3D环境）中表现优于TRPO-VIME
    | [[73](#bib.bib73)] |'
- en: '| Episodic curiosity | uses episodic memory to form the novelty bonus | outperforms
    ICM in visually rich 3D environments from VizDoom and DMLab | [[74](#bib.bib74)]
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 叙事好奇心 | 使用情景记忆形成新奇奖励 | 在来自VizDoom和DMLab的视觉丰富的3D环境中表现优于ICM | [[74](#bib.bib74)]
    |'
- en: '| Go-Explore | The previously visited states are stored in memory. In phase
    one Go-explore explores until a solution is found. In phase two Go-explore Robustifies
    the found solution | Performance improvements on hard exploration problems over
    other methods such as DQN+PixelCNN, DQN+CTS, BASS-hash | [[75](#bib.bib75)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Go-Explore | 之前访问的状态会存储在内存中。在第一阶段，Go-Explore进行探索直到找到解决方案。在第二阶段，Go-Explore对找到的解决方案进行稳健化
    | 在处理困难探索问题时，相对于其他方法如DQN+PixelCNN、DQN+CTS、BASS-hash表现出性能改进 | [[75](#bib.bib75)]
    |'
- en: '| Never Give Up | combines both episodic and life-long novelties | obtains
    a median human normalized score of 1344%; the first algorithm that achieves non-zero
    rewards in the game of Pitfall | [[76](#bib.bib76)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 永不放弃 | 结合了叙事和终身的新奇性 | 获得了1344%的中位人类标准化评分；第一个在Pitfall游戏中实现非零奖励的算法 | [[76](#bib.bib76)]
    |'
- en: '| Agent57 | uses a meta-controller for adaptively selecting the right policy:
    ranging from purely exploratory to purely exploitative | first DRL agent that
    surpasses the standard human benchmark on all 57 Atari games | [[77](#bib.bib77)]
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Agent57 | 使用元控制器自适应地选择正确的策略：从纯粹的探索性到纯粹的开发性 | 第一个在所有57个Atari游戏中超越标准人类基准的DRL代理
    | [[77](#bib.bib77)] |'
- en: Information gain. In exploration based on information gain, the algorithm provides
    a reward bonus proportional to the information obtained after taking an action.
    This reward bonus is then added to the reward provided by the environment to push
    the agent to explore novel (or less known) states [[78](#bib.bib78)]. Houthooft
    et al. [[72](#bib.bib72)] proposed to learn a transition dynamic model with a
    Bayesian neural network. The information gain is measured as the KL divergence
    between the current and updated parameter distribution after a new observation.
    Based on this information the reward signal is augmented with a bonus. Pathak
    et al. [[73](#bib.bib73)] used a forward dynamic model to predict the next state.
    The reward bonus is then set to be proportional to the error between the predicted
    and observed next state. To make this method effective, the authors utilized an
    inverse model, removing irrelevant -for the comparison- state features. Burda
    et al. [[79](#bib.bib79)] defines the exploration bonus based on the error of
    a neural network in predicting features of the observations given by a fixed randomly
    initialized neural network.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益。在基于信息增益的探索中，算法提供与采取行动后获得的信息成正比的奖励奖金。然后将此奖励奖金添加到环境提供的奖励中，以促使智能体探索新奇（或较少了解的）状态
    [[78](#bib.bib78)]。Houthooft等人 [[72](#bib.bib72)] 提出了使用贝叶斯神经网络学习过渡动态模型。信息增益被度量为新观察后当前和更新参数分布之间的KL散度。基于这些信息，奖励信号会增加一个奖金。Pathak等人
    [[73](#bib.bib73)] 使用前向动态模型预测下一个状态。然后将奖励奖金设为与预测状态和观察到的下一个状态之间的误差成正比。为了使这种方法有效，作者利用了一个逆向模型，去除与比较无关的状态特征。Burda等人
    [[79](#bib.bib79)] 根据一个固定的随机初始化神经网络在预测观察特征时的误差定义探索奖励。
- en: 'Memory-based. Savinov et al. [[74](#bib.bib74)] proposed a new curiosity method
    that uses episodic memory to form the novelty bonus. The bonus is computed by
    comparing the current observation with the observations in memory and a reward
    is given for observations that require some effort to be reached (effort is materialized
    by the number of environment steps taken to reach an observation). Ecoffet et al.
    [[75](#bib.bib75)] introduced Go-explore: an RL agent that aims to solve hard
    exploration problems such as Montezuma’s Revenge and Pitfall. Go-explore runs
    in two phases. In phase one, the agent explore randomly, remembers interesting
    states and continues (after reset) random exploration from one of the interesting
    states (the authors assume the agent can deterministically go back to an interesting
    state). After finding a solution to the problem, phase two begins where the Go-explore
    agent robustifies its the best found solution by randomizing the environment and
    running imitation learning using the best solution. Badia et al. [[76](#bib.bib76)]
    proposed "Never give up" (NGU): an agent that also targets hard exploration problems.
    NGU augments the environmental reward with a combination of two intrinsic novelty
    rewards: (i) An episodic reward, which enables the agent to quickly adapt within
    an episode, and, (ii) the life-long novelty reward, which down-modulates states
    that become familiar across many episodes. Further, NGU uses a Universal Value
    Function Approximator (UVFA) to learn several exploration policies with different
    exploration-exploitation trade-offs at the same time. Agent57 [[77](#bib.bib77)]
    aims to manage the tradeoff between exploration and exploitation using a "meta-controller"
    that adaptively selects a correct policy (ranging from very exploratory to purely
    exploitative) for the training phase. Agent57 outperforms the standard human benchmark
    on all 57 Atari games.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 基于记忆的。Savinov 等人[[74](#bib.bib74)]提出了一种新的好奇心方法，利用情节记忆形成新颖性奖励。奖励通过将当前观察与记忆中的观察进行比较来计算，对于需要一定努力才能达到的观察给予奖励（努力通过到达观察所需的环境步骤数量来体现）。Ecoffet
    等人[[75](#bib.bib75)]引入了 Go-explore：一个旨在解决如 Montezuma’s Revenge 和 Pitfall 等难探索问题的
    RL 代理。Go-explore 运行两个阶段。在第一阶段，代理随机探索，记住有趣的状态，并从一个有趣的状态开始（重置后）继续随机探索（作者假设代理可以确定性地返回到一个有趣的状态）。在找到问题的解决方案后，第二阶段开始，Go-explore
    代理通过随机化环境和使用最佳解决方案进行模仿学习来增强找到的最佳解决方案。Badia 等人[[76](#bib.bib76)]提出了“Never give
    up”（NGU）：一个也针对难探索问题的代理。NGU通过两种内在新颖性奖励的组合来增强环境奖励：（i）情节奖励，使代理能够在一个情节内快速适应，和（ii）终身新颖奖励，降低跨多个情节变得熟悉的状态的奖励。此外，NGU
    使用通用价值函数逼近器（UVFA）同时学习多个具有不同探索-利用权衡的探索策略。Agent57[[77](#bib.bib77)]旨在使用“元控制器”管理探索与利用之间的权衡，该控制器自适应地选择合适的策略（从高度探索到纯粹利用）进行训练阶段。Agent57
    在所有 57 款 Atari 游戏中超越了标准人类基准。
- en: '![Refer to caption](img/25024529105b5e637188696eadceb36b.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/25024529105b5e637188696eadceb36b.png)'
- en: 'Figure 6: Deep Reinforcement Learning and Evolution Strategies exploration
    algorithms shown on a timeline'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：深度强化学习和进化策略探索算法的时间线展示
- en: IV-B2 Exploration in Evolution Strategies
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 进化策略中的探索
- en: ES algorithms optimize the fitness score while exploring around the best solutions
    found so far. The exploration is realized through the recombination and mutation
    steps. Despite their effectiveness in exploration, ESs may still get trapped in
    local optima [[58](#bib.bib58), [80](#bib.bib80)]. To overcome this limitation,
    many ESs algorithms with enhanced exploration techniques have been proposed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ES 算法在优化适应度评分的同时，会围绕已找到的最佳解决方案进行探索。这种探索通过重组和突变步骤实现。尽管在探索方面效果显著，但 ES 仍可能陷入局部最优解[[58](#bib.bib58),
    [80](#bib.bib80)]。为了克服这一限制，提出了许多具有增强探索技术的 ES 算法。
- en: 'One way to extract approximate gradients from a non-smooth objective function,
    $F(\boldsymbol{\theta})$, is by adding noise to its parameter vector, $\boldsymbol{\theta}$.
    This yields a new differentiable function, $F_{ES}(\boldsymbol{\theta})$. OpenAI-ES [[4](#bib.bib4)]
    exploits this idea by sampling noise from a Gaussian distribution and adding it
    to the parameter vector $\boldsymbol{\theta}$. The algorithm then optimizes using
    stochastic gradient ascent. Additionally, OpenAI-ES relays on a few auxiliary
    techniques to enhance its performance: virtual batch normalization [[31](#bib.bib31)]
    for enhanced exploration, antithetic sampling [[81](#bib.bib81)] for reduced variance,
    and fitness shaping [[44](#bib.bib44)] for improving local optima avoidance.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从非平滑目标函数 $F(\boldsymbol{\theta})$ 中提取近似梯度的一种方法是通过向其参数向量 $\boldsymbol{\theta}$
    添加噪声。这产生了一个新的可微分函数 $F_{ES}(\boldsymbol{\theta})$。OpenAI-ES [[4](#bib.bib4)] 利用这个思想，通过从高斯分布中采样噪声并将其添加到参数向量
    $\boldsymbol{\theta}$ 中来实现。然后，算法使用随机梯度上升进行优化。此外，OpenAI-ES 依赖于一些辅助技术来提高其性能：虚拟批量归一化
    [[31](#bib.bib31)] 用于增强探索，反对称采样 [[81](#bib.bib81)] 用于减少方差，以及适应性优化 [[44](#bib.bib44)]
    用于改进局部最优避免。
- en: 'Choromanski et al. [[82](#bib.bib82)] proposed two strategies to enhance the
    exploration of Derivative Free Optimization (DFO) methods such as OpenAI-ES [[4](#bib.bib4)]:
    (i) structured exploration, where the authors showed that random orthogonal and
    Quasi Monte Carlo finite difference directions are much more effective than random
    Gaussian directions for parameter exploration; and (ii) compact policies, whereby
    imposing a parameter sharing structure on the policy architecture, they were able
    to significantly reduce the dimensionality of the problem without losing accuracy
    and thus speeding up the learning process.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Choromanski 等人 [[82](#bib.bib82)] 提出了两种策略来增强 Derivative Free Optimization (DFO)
    方法（如 OpenAI-ES [[4](#bib.bib4)]）的探索：（i）结构化探索，作者展示了随机正交和准蒙特卡罗有限差分方向在参数探索上比随机高斯方向更有效；（ii）紧凑策略，通过对策略架构施加参数共享结构，他们能够显著降低问题的维度而不丧失准确性，从而加速学习过程。
- en: 'Maheswaranathan et al. [[83](#bib.bib83)] proposed Guided ES: a random search
    that is augmented using surrogate gradients which are correlated with the true
    gradient. The key idea is to track a low-dimensional subspace that is defined
    by the recent history of surrogate gradients. Sampling this subspace leads to
    a drastic reduction in the variance of the search direction. However, this approach
    has two shortcomings: (i) the bias of the surrogate gradients needs to be known;
    and (ii) when the bias is too small, Guided ES cannot find a better descent direction
    than the surrogate gradient. Meier et al. [[84](#bib.bib84)] draw inspiration
    from how momentum is used for optimizing DNNs to improve upon Guided ES [[83](#bib.bib83)].
    The authors showed how to optimally combine the surrogate gradient directions
    with random search directions and how to iteratively approach the true gradient
    for linear functions. They assessed their algorithm against a standard ESs algorithm
    on different tasks showing its superiority.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Maheswaranathan 等人 [[83](#bib.bib83)] 提出了 Guided ES：一种通过使用与真实梯度相关的替代梯度来增强的随机搜索方法。其关键思想是追踪由替代梯度最近历史定义的低维子空间。对该子空间进行采样能显著降低搜索方向的方差。然而，这种方法有两个缺点：（i）需要知道替代梯度的偏差；（ii）当偏差过小时，Guided
    ES 不能找到比替代梯度更好的下降方向。Meier 等人 [[84](#bib.bib84)] 从优化 DNN 时动量的使用中获得灵感，改进了 Guided
    ES [[83](#bib.bib83)]。作者展示了如何将替代梯度方向与随机搜索方向最佳结合，并如何迭代地接近线性函数的真实梯度。他们在不同任务上将他们的算法与标准
    ES 算法进行了比较，显示了其优越性。
- en: 'Choromanski et al. [[85](#bib.bib85)] noted that fixing the dimensionality
    of subspaces (as in Guided ES [[83](#bib.bib83)]) leads to suboptimal performance.
    Therefore, they proposed ASEBO: an algorithm that adaptively controls the dimensionality
    of subspaces based on gradient estimators from previous iterations. ASEBO was
    compared to several ESs and DRL algorithms and showed promising averaged performance.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Choromanski 等人 [[85](#bib.bib85)] 注意到，固定子空间的维度（如 Guided ES [[83](#bib.bib83)]
    中）会导致次优性能。因此，他们提出了 ASEBO：一种基于先前迭代的梯度估计器自适应控制子空间维度的算法。ASEBO 与几个 ES 和 DRL 算法进行了比较，显示了有前景的平均性能。
- en: 'Liu et al. [[86](#bib.bib86)] proposed Self-Guided Evolution Strategies (SGES).
    This work is inspired by both ASEBO [[85](#bib.bib85)] and Guided ES [[83](#bib.bib83)].
    Further, it is based on two main ideas: leveraging historical estimated gradients
    and building a guiding subspace from which search directions are sampled probabilistically.
    The results show that SGES outperforms Open-AI [[4](#bib.bib4)], Guided ES [[83](#bib.bib83)],
    CMA-ES and vanilla ES.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 [[86](#bib.bib86)] 提出了自引导进化策略（SGES）。该工作受到 ASEBO [[85](#bib.bib85)] 和
    Guided ES [[83](#bib.bib83)] 的启发。此外，它基于两个主要思想：利用历史估计的梯度和构建一个引导子空间，从中以概率方式采样搜索方向。结果表明，SGES
    优于 Open-AI [[4](#bib.bib4)]、Guided ES [[83](#bib.bib83)]、CMA-ES 和 vanilla ES。
- en: The aforementioned methods suffer from the curse of dimensionality due to the
    high variance of Monte Carlo gradient estimators. Motivated by this, Zhang et al.
    [[87](#bib.bib87)] proposed Directional Gaussian Smoothing Evolution Strategy
    (DGS-ES). It encourages non-local exploration and improves high-dimensional exploration.
    In contrast to regular Gaussian smoothing, directional Gaussian smoothing conducts
    $1D$ non-local explorations along $d$ orthogonal directions. The Gauss-Hermite
    quadrature is then used for improving the convergence speed of the algorithm.
    Its superior performance is showcased by comparing it to many algorithms including
    OpenAI-ES [[4](#bib.bib4)] and ASEBO [[85](#bib.bib85)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法由于蒙特卡罗梯度估计器的高方差而遭遇维数灾难。受到此问题的启发，Zhang 等人 [[87](#bib.bib87)] 提出了定向高斯平滑进化策略（DGS-ES）。它鼓励非局部探索，并改进高维探索。与常规高斯平滑相比，定向高斯平滑在
    $d$ 个正交方向上进行 $1D$ 非局部探索。然后使用高斯-厄尔米特积分提高算法的收敛速度。其优越的性能通过与许多算法的比较（包括 OpenAI-ES [[4](#bib.bib4)]
    和 ASEBO [[85](#bib.bib85)]）得到展示。
- en: 'To encourage exploration in environments with sparse or deceptive reward signals,
    Conti et al. [[32](#bib.bib32)] proposed hybridizing ESs with directed exploration
    methods (i.e., Novelty Search (NS) [[88](#bib.bib88)] and Quality Diversity (QD)
    [[89](#bib.bib89)]). The combination resulted in three algorithms: NS-ES, NSR-ES,
    and NSRA-ES. NS-ES builds on the OpenAI-ES exploration strategy. OpenAI-ES approximates
    a gradient and takes a step in that direction. In NS-ES, the gradient estimate
    is that of the expected novelty. It gives directions on how to change the current
    policy’s parameters $\boldsymbol{\theta}$ to increase the average novelty of the
    parameter distribution. NSR-ES is a variant of NS-ES. It combines both the reward
    and novelty signals to produce policies that are both novel and high-performing.
    NSRA-ES is an extension of NSR-ES that dynamically adapts the weights of the novelty
    and the reward gradients for more optimal performance.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在奖励信号稀疏或具有误导性的环境中鼓励探索，Conti 等人 [[32](#bib.bib32)] 提出了将进化策略（ESs）与定向探索方法（即 Novelty
    Search (NS) [[88](#bib.bib88)] 和 Quality Diversity (QD) [[89](#bib.bib89)]）混合的方案。这种组合产生了三种算法：NS-ES、NSR-ES
    和 NSRA-ES。NS-ES 基于 OpenAI-ES 探索策略。OpenAI-ES 通过近似梯度并朝该方向迈进一步。在 NS-ES 中，梯度估计是期望新奇性的梯度。它指示如何改变当前策略的参数
    $\boldsymbol{\theta}$ 以增加参数分布的平均新奇性。NSR-ES 是 NS-ES 的一种变体。它结合了奖励和新奇信号，以产生既新颖又高效的策略。NSRA-ES
    是 NSR-ES 的扩展，它动态调整新奇性和奖励梯度的权重，以获得更优的性能。
- en: 'TABLE IV: Evolution Strategies exploration algorithms'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 进化策略探索算法'
- en: '| Algorithm | Description | Experiments | Ref. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Algorithm | Description | Experiments | Ref. |'
- en: '| OpenAI-ES | adds Gaussian noise to the parameter vector, computes a gradient,
    and takes a step in its direction | improves exploratory behaviors as compared
    to TRPO on tasks such as learning gaits of the MuJoCo humanoid walker | [[4](#bib.bib4)]
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-ES | 向参数向量添加高斯噪声，计算梯度，并朝其方向迈进一步 | 在诸如学习 MuJoCo 人形行走者的步态等任务上，相比于 TRPO
    改进了探索行为 | [[4](#bib.bib4)] |'
- en: '| Structured Exploration | complements OpenAI-ES [[4](#bib.bib4)] with structured
    exploration and compact policies for efficient exploration | solves robotics tasks
    from OpenAI Gym using NN with 300 parameters (13x fewer than OpenAI-ES) and with
    near linear time complexity | [[82](#bib.bib82)] |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Structured Exploration | 补充 OpenAI-ES [[4](#bib.bib4)]，采用结构化探索和紧凑策略以实现高效探索
    | 使用 300 个参数（比 OpenAI-ES 少 13 倍）和近似线性时间复杂度的 NN 解决 OpenAI Gym 中的机器人任务 | [[82](#bib.bib82)]
    |'
- en: '| Guided ES | leverages surrogate gradients to define a low-dimensional subspace
    for efficient sampling | improves over vanilla ESs and first-order methods that
    directly follow the surrogate gradient | [[83](#bib.bib83)] |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Guided ES | 利用替代梯度定义低维子空间以实现高效采样 | 相比于直接跟随替代梯度的 vanilla ESs 和一阶方法有改进 | [[83](#bib.bib83)]
    |'
- en: '| ASEBO | adapts the dimensionality of the subspaces on-the-fly for efficient
    exploration | optimizes high-dimensional balck-box functions and performs consistently
    well across several tasks compared to state-of-the-art algorithms | [[85](#bib.bib85)]
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| ASEBO | 动态调整子空间的维度以实现高效探索 | 优化高维黑箱函数，并在多个任务中表现稳定，相比于最先进的算法表现更好 | [[85](#bib.bib85)]
    |'
- en: '| DGS-ES | uses directional Gaussian smoothing to explore along non-local orthogonal
    directions. It leverages Guss-Hermite quadrature for fast convergence. | improves
    on state-of-the-art algorithms (e.g., OpenAI-ES and ASEBO) on some problems |
    [[87](#bib.bib87)] |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| DGS-ES | 使用方向性高斯平滑沿非局部正交方向进行探索。利用Guss-Hermite积分实现快速收敛。 | 在某些问题上改进了最先进的算法（如OpenAI-ES和ASEBO）
    | [[87](#bib.bib87)] |'
- en: '| Iterative gradient estimation refinement | iteratively uses the last update
    direction as a surrogate gradient for the gradient estimator. Over time this will
    result in improved gradient estimates. | converges relatively fast to the true
    gradient for linear functions. It improves gradient estimation of ESs at no extra
    computational cost on MNIST and RL tasks | [[84](#bib.bib84)] |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 迭代梯度估计精炼 | 迭代地使用上一次更新方向作为梯度估计器的替代梯度。随着时间推移，这将导致梯度估计的改进。 | 对于线性函数相对较快地收敛于真实梯度。它在MNIST和RL任务中以无额外计算成本改进了ESs的梯度估计
    | [[84](#bib.bib84)] |'
- en: '| SGES | adapts a low-dimensional subspace on the fly for more efficient sampling
    and exploring | has lower gradient estimation variance as compared to OpenAI-ES.
    Superior performance over ESs algorithms such as OpenAI-ES, Guided ES, ASEBO,
    CMA-ES on blackbox functions and RL tasks | [[86](#bib.bib86)] |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| SGES | 动态适应低维子空间以实现更高效的采样和探索 | 相比于OpenAI-ES具有更低的梯度估计方差。在黑箱函数和RL任务中，优于OpenAI-ES、Guided
    ES、ASEBO、CMA-ES等ESs算法 | [[86](#bib.bib86)] |'
- en: '| NS-ES, NSR-ES, and NSRA-ES | Hybridize Novelty search (NS) and quality diversity
    (QD) algorithms with ESs to improve the performance of ESs on sparse RL problems.
    | avoid local optima encountered by ESs while achieving higher performance on
    Atari and simulated robot tasks | [[32](#bib.bib32)] |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| NS-ES、NSR-ES 和 NSRA-ES | 将新颖性搜索（NS）和质量多样性（QD）算法与ESs混合，以提高ESs在稀疏RL问题上的性能。
    | 避免了ESs遇到的局部最优，同时在Atari和模拟机器人任务上表现更高 | [[32](#bib.bib32)] |'
- en: IV-B3 Comparison
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 比较
- en: Our observations of this section are summarized below.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一部分的观察总结如下。
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The exploration-exploitation dilemma is still an active field of research and
    environments with sparse and deceptive reward signals require more sophisticated
    and capable exploration algorithms.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索-利用困境仍然是一个活跃的研究领域，对于奖励信号稀疏且具有欺骗性的环境，需要更复杂和更强大的探索算法。
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Benchmarking exploration strategies happens almost exclusively in simulated/gaming
    environments. Consequently, the efficacy of these algorithms in real-world applications
    is mostly unknown.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索策略的基准测试几乎专门在模拟/游戏环境中进行。因此，这些算法在实际应用中的有效性大多未知。
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Thanks to the recombination and mutation, ESs algorithms might suffer less from
    local optima than DRL ones.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于重组和突变，ESs算法可能比DRL算法更少受局部最优的困扰。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ESs still face some problems related to sample efficiency when exploring, as
    high dimensional optimization tasks can lead to high variance gradients estimates.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ESs在探索时仍然面临一些与样本效率相关的问题，因为高维优化任务可能导致高方差的梯度估计。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Table [III](#S4.T3 "TABLE III ‣ IV-B1 Exploration in (Deep) Reinforcement Learning
    ‣ IV-B Exploration ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    and Table [IV](#S4.T4 "TABLE IV ‣ IV-B2 Exploration in Evolution Strategies ‣
    IV-B Exploration ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    summarize some important characteristics of DRL and ESs exploration algorithms.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '表[III](#S4.T3 "TABLE III ‣ IV-B1 Exploration in (Deep) Reinforcement Learning
    ‣ IV-B Exploration ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")和表[IV](#S4.T4
    "TABLE IV ‣ IV-B2 Exploration in Evolution Strategies ‣ IV-B Exploration ‣ IV
    Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning
    Versus Evolution Strategies: A Comparative Survey")总结了DRL和ESs探索算法的一些重要特征。'
- en: IV-C Non-Markov settings
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 非马尔可夫设置
- en: The Markov property denotes the situation where the future states of a process
    depend only on the current state and not on events or states from the past. The
    degree to which agents can observe (changes in) the environment has an impact
    on their decision behavior. In certain favorable scenarios the state of the agent
    in its environment might be fully observable (e.g., using sensors) to an extent
    such that the Markov assumption holds. In other cases, the state of the environment
    is only partially observable and/or the agent faces a distribution of environments
    (Meta-RL).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质表示一个过程的未来状态仅依赖于当前状态，而不依赖于过去的事件或状态。代理观察（环境中的变化）的能力会影响其决策行为。在某些有利的场景中，代理在其环境中的状态可能是完全可观察的（例如，使用传感器），以至于马尔可夫假设成立。在其他情况下，环境状态仅部分可观察和/或代理面临环境分布（元强化学习）。
- en: IV-C1 Partially Observable
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 部分可观察
- en: In many real-world applications, agents can only partially observe the state
    of their environments and might only have access to their local observations.
    This means agents need to take into accent the history of observations—actions
    and rewards—to produce a better estimation of the underlying hidden state [[90](#bib.bib90),
    [91](#bib.bib91), [92](#bib.bib92)]. These problems are usually modeled as a partially
    observable Markov decision process (POMDP). Researchers have addressed the POMDP
    problem setup through the proposal of many RL models and evolutionary strategies.
    In DRL, one possibility is to employ a neural network with a recurrent architecture
    that enables agents to consider past observations [[93](#bib.bib93), [94](#bib.bib94)].
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界应用中，代理只能部分观察环境状态，可能只能访问其局部观察。这意味着代理需要考虑观察历史——动作和奖励——以更好地估计潜在的隐藏状态[[90](#bib.bib90),
    [91](#bib.bib91), [92](#bib.bib92)]。这些问题通常建模为部分可观察的马尔可夫决策过程（POMDP）。研究人员通过提出许多强化学习模型和进化策略来解决POMDP问题设置。在深度强化学习中，一种可能性是使用具有循环架构的神经网络，使代理能够考虑过去的观察[[93](#bib.bib93),
    [94](#bib.bib94)]。
- en: IV-C2 Meta Reinforcement Learning
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 元强化学习
- en: 'Meta-RL is concerned with learning a policy that can be quickly generalized
    across a distribution of tasks or environments (modeled as MDPs). Generally, a
    meta-learner achieves that through two stages optimization process: first, a meta-policy
    is trained on a distribution of similar tasks with the hope of learning the common
    dynamics across these tasks; then, the second stage fine-tunes the meta-policy
    while acting on a particular task sampled from a similar but unseen task distribution
    [[95](#bib.bib95)]. Examples of meta-RL tasks include: navigating towards distinct
    goals [[96](#bib.bib96)], going through different mazes [[97](#bib.bib97)], dealing
    with component failures [[98](#bib.bib98)], or driving different cars [[99](#bib.bib99)].'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习关注于学习一种能够在任务或环境的分布中迅速泛化的策略（建模为MDPs）。一般而言，元学习器通过两个阶段的优化过程来实现：首先，在一组相似任务的分布上训练一个元策略，以期学习这些任务中的共同动态；然后，第二阶段在从类似但未见任务分布中抽取的特定任务上微调元策略[[95](#bib.bib95)]。元强化学习任务的例子包括：朝向不同目标的导航[[96](#bib.bib96)]，通过不同的迷宫[[97](#bib.bib97)]，处理组件故障[[98](#bib.bib98)]，或驾驶不同的汽车[[99](#bib.bib99)]。
- en: 'Meta-RL can be subdivided into two categories [[96](#bib.bib96)]: RNN-based
    [[100](#bib.bib100), [101](#bib.bib101)] and gradient-based learners [[102](#bib.bib102),
    [103](#bib.bib103)].'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习可以分为两类[[96](#bib.bib96)]：基于RNN的[[100](#bib.bib100), [101](#bib.bib101)]和基于梯度的学习器[[102](#bib.bib102),
    [103](#bib.bib103)]。
- en: Recurrent Models (RNN-based learners). Leveraging the agent-environment interaction
    history provides more information, which leads to improved learning [[99](#bib.bib99),
    [104](#bib.bib104)]. This idea can be implemented using Recurrent Neural Networks
    (RNNs) (or other recurrent models) [[105](#bib.bib105), [101](#bib.bib101), [100](#bib.bib100),
    [97](#bib.bib97)]. The RNNs can be trained on a set of tasks to learn a hidden
    state (meta-policy), then this hidden state can be further adapted given new observations
    from an unseen task.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 循环模型（基于RNN的学习器）。利用代理-环境交互历史提供更多信息，从而提升学习效果[[99](#bib.bib99), [104](#bib.bib104)]。这个想法可以通过使用循环神经网络（RNNs）（或其他循环模型）来实现[[105](#bib.bib105),
    [101](#bib.bib101), [100](#bib.bib100), [97](#bib.bib97)]。RNN可以在一组任务上进行训练，以学习一个隐含状态（元策略），然后这个隐含状态可以在接收到来自未见任务的新观察时进一步适应。
- en: 'General architecture of a meta-RL algorithm is illustrated in Figure [7](#S4.F7
    "Figure 7 ‣ IV-C2 Meta Reinforcement Learning ‣ IV-C Non-Markov settings ‣ IV
    Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement Learning
    Versus Evolution Strategies: A Comparative Survey") [[106](#bib.bib106)], where
    an agent is modeled as two loops, both implementing RL algorithms. The outer loop
    samples a new environment in every iteration and tunes the parameters of the inner
    loop. Consequently, the inner loop can adjust more rapidly to new tasks by interacting
    with the associated environments and optimizing for maximal rewards.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '元强化学习算法的一般架构在图 [7](#S4.F7 "Figure 7 ‣ IV-C2 Meta Reinforcement Learning ‣ IV-C
    Non-Markov settings ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    [[106](#bib.bib106)] 中示意，其中一个代理被建模为两个循环，这两个循环都实现了 RL 算法。外循环在每次迭代中采样一个新环境，并调整内循环的参数。因此，内循环可以通过与相关环境交互并优化以获得最大奖励，从而更快速地适应新任务。'
- en: 'Duan et al. [[101](#bib.bib101)] and Wang et al. [[100](#bib.bib100)] proposed
    analogous recurrent Meta-RL agents: $R^{2}$ and DRL-meta, respectively. They implemented
    a long-short term memory (LSTM) and a gate recurrent unit (GRU) architecture in
    which the hidden states serve as a memory for tracking characteristics of interaction
    trajectories. The main difference between both approaches relates to the set of
    environments. Environments in [[100](#bib.bib100)] are issued from a parameterized
    distribution [[107](#bib.bib107)]. In contrast, those in [[101](#bib.bib101)]
    are relatively unrelated [[107](#bib.bib107)].'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Duan 等人 [[101](#bib.bib101)] 和 Wang 等人 [[100](#bib.bib100)] 提出了类似的递归 Meta-RL
    代理：$R^{2}$ 和 DRL-meta。它们实现了一个长短期记忆（LSTM）和一个门控递归单元（GRU）架构，其中隐藏状态作为跟踪交互轨迹特征的记忆。这两种方法之间的主要区别在于环境集的不同。[[100](#bib.bib100)]
    中的环境来自参数化分布 [[107](#bib.bib107)]。而 [[101](#bib.bib101)] 中的环境则相对不相关 [[107](#bib.bib107)]。
- en: Such RNN-based methods have proven to be efficient on many RL tasks. However,
    their performance decreases as the complexity of the task increases, especially
    with long temporal dependencies. Additionally, short-term memory is challenging
    for RNN due to the vanishing gradient problem. Furthermore, RNN-based meta-learners
    cannot pinpoint specific prior experiences [[97](#bib.bib97), [108](#bib.bib108)].
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此类基于 RNN 的方法在许多 RL 任务中已证明是有效的。然而，随着任务复杂性的增加，它们的性能会下降，尤其是在长时间依赖的情况下。此外，由于梯度消失问题，短期记忆对
    RNN 是一个挑战。此外，基于 RNN 的元学习者无法精确定位特定的先验经验 [[97](#bib.bib97), [108](#bib.bib108)]。
- en: 'To overcome these limitations, Mishra et al. [[97](#bib.bib97)] proposed Simple
    Neural Attentive Learner (SNAIL). It combines temporal convolutions and attention
    mechanisms. The former aggregates information from past experiences and the latter
    pinpoints specific pieces of information. SNAIL’s architecture consists of three
    main parts: (i) DenseBlock, a causal 1D-convolution with specific dilation rate;
    (ii) TCBlock, a series of DenseBlocks with exponentially increasing dilation rates;
    and (iii) AttentionBlock, where key-vlaue lookups take place. This general-purpose
    model has shown its efficacy on tasks ranging from supervised to reinforcement
    learning. Despite that, challenges such as the long time needed for getting the
    right architectures of TCBlocks and DenseBlocks. [[108](#bib.bib108)] persist.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，Mishra 等人 [[97](#bib.bib97)] 提出了简单神经注意力学习器（SNAIL）。它结合了时间卷积和注意力机制。前者聚合来自过去经验的信息，后者则精确定位特定信息。SNAIL
    的架构由三个主要部分组成：（i）DenseBlock，一个具有特定扩张率的因果 1D 卷积；（ii）TCBlock，一系列具有指数递增扩张率的 DenseBlocks；（iii）AttentionBlock，其中进行键值查找。这个通用模型在从监督学习到强化学习的任务中都显示出了其效能。尽管如此，仍然存在诸如获取正确的
    TCBlocks 和 DenseBlocks 架构所需的时间长等挑战 [[108](#bib.bib108)]。
- en: 'Gradient-Based Models. Model Agnostic Meta-Learning (MAML) [[102](#bib.bib102)]
    realizes meta-learning principles by learning an initial set of parameters, $\boldsymbol{\theta_{0}}$,
    of a model such that taking a few gradient steps is sufficient to tailor this
    model to a specific task. More precisely, MAML learns $\boldsymbol{\theta_{0}}$
    such that for any randomly sampled task, $\mathcal{T}$, with a loss function,
    $\mathcal{L}$, the agent will have a modest loss after $n$ updates:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的模型。模型无关元学习（MAML）[[102](#bib.bib102)] 通过学习模型的初始参数集 $\boldsymbol{\theta_{0}}$
    实现元学习原理，从而使得通过少量梯度步骤即可将该模型调整到特定任务上。更准确地说，MAML 学习 $\boldsymbol{\theta_{0}}$，使得对于任何随机采样的任务
    $\mathcal{T}$，具有损失函数 $\mathcal{L}$，代理在 $n$ 次更新后将具有适度的损失：
- en: '|  | $\boldsymbol{\theta_{0}}=\operatorname*{arg\,min}\limits_{\boldsymbol{\theta}}\mathbb{E_{\mathcal{T}}}\bigg{[}\mathcal{L}_{\mathcal{T}}\bigg{(}U_{\mathcal{T}}^{n}(\boldsymbol{\theta})\bigg{)}\bigg{]}$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta_{0}}=\operatorname*{arg\,min}\limits_{\boldsymbol{\theta}}\mathbb{E_{\mathcal{T}}}\bigg{[}\mathcal{L}_{\mathcal{T}}\bigg{(}U_{\mathcal{T}}^{n}(\boldsymbol{\theta})\bigg{)}\bigg{]}$
    |  |'
- en: where $U_{\mathcal{T}}^{n}(\boldsymbol{\theta})$ refers to an update rule such
    as gradient descent.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U_{\mathcal{T}}^{n}(\boldsymbol{\theta})$ 指的是更新规则，例如梯度下降。
- en: Nichol et al. [[109](#bib.bib109)] proposed Reptile a first-order meta-learning
    framework, that is considered to be an approximation of MAML. Similar to first-order
    MAML (FOMAML), Reptile does not calculate second derivatives, which makes it less
    computationally demanding. It starts by repeatedly sampling a task, then performing
    $N$ iterations of stochastic gradient descent (SGD) on each task to compute a
    new set of parameters. Then, it moves the model weights towards the new parameters.
    Next, we look at how meta-learning tries to make ESs more efficient.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Nichol等人 [[109](#bib.bib109)] 提出了Reptile，这是一种一阶元学习框架，被认为是MAML的近似。类似于一阶MAML (FOMAML)，Reptile不计算二阶导数，使其计算需求较低。它通过重复采样任务开始，然后在每个任务上执行$N$次随机梯度下降
    (SGD)以计算一组新的参数。接着，它将模型权重移动到新参数附近。接下来，我们来看元学习如何尝试使ESs更加高效。
- en: '![Refer to caption](img/02826fcf378c3bdf76d052fac33723c3.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02826fcf378c3bdf76d052fac33723c3.png)'
- en: 'Figure 7: Schematic of Meta-reinforcement Learning; illustrating the inner
    and outer loops of training [[106](#bib.bib106)]'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：元强化学习的示意图；展示了训练的内循环和外循环 [[106](#bib.bib106)]
- en: 'TABLE V: Gradient-based Meta Reinforcement Learning.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表V：基于梯度的元强化学习。
- en: '| Algorithms | Description | Experiments | Ref. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 | 实验 | 参考 |'
- en: '| DRL-meta | trains an RNN on a distribution of RL tasks. The RNN serves as
    a dynamic task embedding storage. DRL-meta uses the LSTM architecture | outperforms
    other benchmarks on the bandits’ problems; properly adapts to invariances in MDP
    tasks | [[100](#bib.bib100)] |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| DRL-meta | 在RL任务的分布上训练RNN。RNN作为动态任务嵌入存储。DRL-meta使用LSTM架构 | 在赌博机问题上优于其他基准；适应MDP任务中的不变性
    | [[100](#bib.bib100)] |'
- en: '| $RL^{2}$ | trains an RNN on a distribution of RL tasks. The RNN serves as
    a dynamic task embedding storage. $R^{2}$ uses the GRU architecture | comparable
    to theoretically optimal algorithms in small-scale settings. It has the potential
    to scale to high-dimensional tasks | [[101](#bib.bib101)] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| $RL^{2}$ | 在RL任务的分布上训练RNN。RNN作为动态任务嵌入存储。$R^{2}$使用GRU架构 | 与理论上在小规模设置中的最优算法可比。具有扩展到高维任务的潜力
    | [[101](#bib.bib101)] |'
- en: '| SNAIL | combines temporal convolution and attention mechanisms | outperforms
    LSTM and MAML | [[97](#bib.bib97)] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| SNAIL | 结合时间卷积和注意机制 | 优于LSTM和MAML | [[97](#bib.bib97)] |'
- en: '| MAML | given a task distribution, it searches for optimal initial parameters
    such that a few gradient steps are sufficient to solve tasks drawn from that distribution.
    | outperforms classical methods such as random and pretrained methods | [[102](#bib.bib102)]
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| MAML | 给定任务分布，它寻找最佳初始参数，使得少量梯度步骤足以解决从该分布中抽取的任务。 | 优于随机和预训练等经典方法 | [[102](#bib.bib102)]
    |'
- en: '| Reptile | similar to first-order MAML | on-par with the performance of MAML
    | [[109](#bib.bib109)] |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Reptile | 类似于一阶MAML | 性能与MAML相当 | [[109](#bib.bib109)] |'
- en: IV-C3 Meta Evolution Strategies
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 元进化策略
- en: Gajewski et al. [[110](#bib.bib110)] introduced “Evolvability ES”, an ES-based
    meta-learning algorithm for RL tasks. It combines concepts from evolvability search
    [[111](#bib.bib111)], ESs [[4](#bib.bib4)], and MAML [[102](#bib.bib102)] to encourage
    searching for individuals whose immediate offsprings show signs of behavioral
    diversity (that is, it searches for parameter vectors whose perturbations lead
    to differing behaviors) [[111](#bib.bib111)]. Consequently, Evolvability ES facilitates
    adaptation and generalization while leveraging the scalability of ESs [[110](#bib.bib110),
    [112](#bib.bib112)]. Evolvability ES shows a competitive performance to gradient-based
    meta-learning algorithms. Quality Evolvability ES [[112](#bib.bib112)] noted that
    the original Evolvability ES [[113](#bib.bib113)] can only be used to solve problems
    where the task performance and evolability align. To eliminate this restriction,
    Quality Evolvability ES optimizes for both -task performance and evolability-
    simultaneously.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Gajewski 等人 [[110](#bib.bib110)] 引入了“进化性 ES”，这是一种基于 ES 的元学习算法，旨在 RL 任务中使用。它结合了进化性搜索
    [[111](#bib.bib111)]、ESs [[4](#bib.bib4)] 和 MAML [[102](#bib.bib102)] 的概念，以鼓励寻找其直接后代表现出行为多样性的个体（即，它搜索那些扰动导致不同表现的参数向量）
    [[111](#bib.bib111)]。因此，进化性 ES 促进了适应和泛化，同时利用了 ESs [[110](#bib.bib110), [112](#bib.bib112)]
    的可扩展性。进化性 ES 显示出与基于梯度的元学习算法的竞争力。质量进化性 ES [[112](#bib.bib112)] 指出，原始进化性 ES [[113](#bib.bib113)]
    只能用于任务性能和进化性一致的问题。为了解除这一限制，质量进化性 ES 同时优化任务性能和进化性。
- en: Song et al. [[114](#bib.bib114)] argue that policy gradient-based Model Agnostic
    Meta Learning (MAML) algorithms [[102](#bib.bib102)] face significant difficulties
    when estimating second derivative using backpropagation on stochastic policies.
    Therefore, they introduced ES-MAML, a meta-learner that leverages ES [[4](#bib.bib4)]
    for solving MAML problems without estimating second derivatives. The authors empirically
    showed that ES-MAML is competitive with other Meta-RL algorithms. Song et al.
    [[115](#bib.bib115)] combined Hill-Climbing adaptation with ES-MAML to develop
    noise-tolerant meta-RL learner. The authors showcased the performance of their
    algorithm using a physical legged robot.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Song 等人 [[114](#bib.bib114)] 认为基于策略梯度的模型无关元学习（MAML）算法 [[102](#bib.bib102)] 在使用反向传播对随机策略进行二阶导数估计时面临显著困难。因此，他们引入了
    ES-MAML，这是一种利用 ES [[4](#bib.bib4)] 解决 MAML 问题而无需估计二阶导数的元学习器。作者通过实验证明了 ES-MAML
    与其他 Meta-RL 算法具有竞争力。Song 等人 [[115](#bib.bib115)] 将爬山算法与 ES-MAML 结合，开发了噪声容忍的元强化学习学习器。作者通过一个物理腿部机器人展示了他们算法的性能。
- en: Wang et al. [[116](#bib.bib116)] incorporated an instance weighting mechanism
    with ESs to generate an adaptable and salable meta-learner, Instance Weighted
    Incremental Evolution Strategies (IW-IES).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[116](#bib.bib116)] 将实例加权机制与 ESs 结合，生成了一个可适应且可扩展的元学习器，即实例加权增量进化策略（IW-IES）。
- en: 'Wang et al. [[116](#bib.bib116)] introduced Instance Weighted Incremental Evolution
    Strategies (IW-IES). It incorporates an instance weighting mechanism with ESs
    to generate an adaptable and salable meta-learner. IW-IES assigns weights to offsprings
    proportional to the amount of new knowledge they acquire. The weights are assigned
    based on one of the two metrics: instance novelty and instance quality. Compared
    to ES-MAML, IW-IES proved competitive for robot navigation tasks.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[116](#bib.bib116)] 引入了实例加权增量进化策略（IW-IES）。它将实例加权机制与 ESs 结合，生成一个可适应且可扩展的元学习器。IW-IES
    根据后代获得的新知识量为其分配权重。权重是根据两个指标之一分配的：实例新颖性和实例质量。与 ES-MAML 相比，IW-IES 在机器人导航任务中表现出竞争力。
- en: 'Meta-RL is particularly suited for tackling the sim-to-real problem: simulation
    provides previous experiences that are used to learn a general policy, and the
    data obtained from operating in the real world fine-tunes that policy [[117](#bib.bib117)].
    Examples of using Meta-RL to train physical robots include: Nagabandi et al. [[98](#bib.bib98)]
    built on top of MAML a model-based meta-RL agent to train a legged millirobot;
    Arndt et al. [[118](#bib.bib118)] proposed a similar framework to MAML to train
    a robot on a task of hitting a hockey puck; and Song et al. [[115](#bib.bib115)]
    introduced a variant of ES-MAML to train and quickly adapt the policy commanding
    a legged robot.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Meta-RL 特别适用于解决仿真到现实的问题：仿真提供了用于学习通用策略的先前经验，而从现实世界操作中获得的数据则对该策略进行微调[[117](#bib.bib117)]。使用
    Meta-RL 训练物理机器人示例包括：Nagabandi 等人 [[98](#bib.bib98)] 在 MAML 的基础上构建了一个基于模型的 meta-RL
    代理来训练一个腿式微型机器人；Arndt 等人 [[118](#bib.bib118)] 提出了一个类似于 MAML 的框架来训练一个击打冰球的机器人任务；Song
    等人 [[115](#bib.bib115)] 引入了一种 ES-MAML 的变体，用于训练和快速适应命令腿式机器人的策略。
- en: 'TABLE VI: Evolution Strategies for Meta Reinforcement learning'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：Meta 强化学习的进化策略
- en: '|  | Description | Experiments | ref. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | 描述 | 实验 | 参考文献 |'
- en: '| Evolvability ES | combines concepts from evolvability search, ES, and MAML
    to enable a quickly adaptable meta-learner | competitive with MAML on 2D and 3D
    locomotion tasks | [[110](#bib.bib110)] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Evolvability ES | 结合了进化性搜索、ES 和 MAML 的概念，以便快速适应的 meta-learner | 在 2D 和 3D
    运动任务中与 MAML 竞争 | [[110](#bib.bib110)] |'
- en: '| ES-MAML | uses ESs to overcome the limitations of MAML | competitive with
    policy gradient methods; yields better adaptation with fewer queries | [[114](#bib.bib114)]
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| ES-MAML | 使用 ES 克服 MAML 的限制 | 与策略梯度方法竞争；在查询较少的情况下提供更好的适应性 | [[114](#bib.bib114)]
    |'
- en: '| IW-IES | uses NES for updating the RL policy network parameters in a dynamic
    environment | outperforms ES-MAML on set of robot navigation tasks | [[116](#bib.bib116)]
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| IW-IES | 使用 NES 更新动态环境中的 RL 策略网络参数 | 在一组机器人导航任务中优于 ES-MAML | [[116](#bib.bib116)]
    |'
- en: IV-C4 Comparison
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 比较
- en: 'Key observations of this section can be summarized as:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的关键观察可以总结为：
- en: •
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In many cases, RL and ESs are faced with problems that are more complex than
    the traditional MDP setting, such as in the partially observable case and the
    meta-learning setting.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在许多情况下，RL 和 ES 面临比传统 MDP 设置更复杂的问题，例如在部分可观测情况下和 meta-learning 设置中。
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Meta-learning enables an agent to explore more intelligently and acquire useful
    knowledge more quickly.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-learning 使智能体能够更智能地探索并更快地获取有用知识。
- en: •
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'There are two main approaches for Meta-RL: gradient-based and recurrent models.
    Gradient-based Meta-RL is generally a two-stage optimization process: first, it
    optimizes on a task distribution level, and then, fine-tunes for a specific task.
    Meta-RL with recurrent models make use of specific recurrent architectures to
    learn how to act in a distribution of environments.'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-RL 主要有两种方法：基于梯度的方法和递归模型。基于梯度的 Meta-RL 通常是一个两阶段的优化过程：首先，在任务分布层面进行优化，然后针对具体任务进行微调。使用递归模型的
    Meta-RL 利用特定的递归架构来学习如何在环境分布中行动。
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There are many challenges in Meta-RL methods, such as estimating first and second-order
    derivatives, high variance, and high computation needs.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-RL 方法面临许多挑战，如估计一阶和二阶导数、高方差和高计算需求。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ES-based meta-RL attempts to address the limitations of gradient-based Meta-RL;
    however, ES-based meta-RL itself faces a different set of challenges such as the
    sample efficiency.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 ES 的 meta-RL 试图解决基于梯度的 Meta-RL 的局限性；然而，基于 ES 的 meta-RL 自身也面临一系列不同的挑战，如样本效率。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Meta-RL is particularly suited for tackling the sim-to-real problem. For instance,
    a generic policy is trained in simulation and fine-tuned via the interaction with
    real world.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-RL 特别适用于解决仿真到现实的问题。例如，通用策略在仿真中进行训练，并通过与现实世界的互动进行微调。
- en: IV-D Learning in multiagent settings
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 多智能体环境中的学习
- en: '![Refer to caption](img/8bd8fd97408d94f9968780e38e011edf.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/8bd8fd97408d94f9968780e38e011edf.png)'
- en: 'Figure 8: Multi-agent Reinforcement Learning Overview [[119](#bib.bib119)]'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：多智能体强化学习概述 [[119](#bib.bib119)]
- en: A multi-agent system (MAS) is a distributed system of multiple cooperating or
    competing (physical or virtual) agents, working towards maximizing their own objectives
    within a shared environment [[120](#bib.bib120)]. Currently, MAS form one of the
    leading research areas of Artificial Intelligence due to their wide applicability.
    Virtually any application that can be partitioned and parallelized can benefit
    from using multiple agents.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体系统（MAS）是由多个合作或竞争（物理或虚拟）智能体组成的分布式系统，这些智能体在共享环境中致力于最大化各自的目标[[120](#bib.bib120)]。目前，MAS
    是人工智能领域的主要研究方向之一，因为它们具有广泛的应用性。几乎任何可以被分割和并行化的应用都可以从使用多个智能体中受益。
- en: IV-D1 multi-agent Reinforcement Learning
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 多智能体强化学习
- en: 'An MAS can be combined with DRL to form a Multi-agent Deep Reinforcement Learning
    (MADRL) system which addresses sequential decision-making problems for multiple
    agents sharing a common environment (Figure [8](#S4.F8 "Figure 8 ‣ IV-D Learning
    in multiagent settings ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")).
    MADRL agents are trained to learn certain behaviors through interaction with the
    environment and optionally with other agents. Since the environment and the reward
    states are affected by the joint actions of all agents, the single-agent MDP model
    cannot be directly applied to MADRL systems, as they do not adhere to the Markov
    property. The Markov (or Stochastic) games (MG) [[121](#bib.bib121)] framework
    comes as a generalization of the MDP that captures the entanglement of the multiple
    agents. There are several important properties to be considered when considering
    MADRL systems. In the following section we will discuss each of these properties
    and their resulting impact on the overall system.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 'MAS 可以与深度强化学习（DRL）结合，形成多智能体深度强化学习（MADRL）系统，这解决了多个智能体在共享环境中进行的顺序决策问题（图 [8](#S4.F8
    "Figure 8 ‣ IV-D Learning in multiagent settings ‣ IV Deep Reinforcement Learning
    Versus Evolution Strategies ‣ Deep Reinforcement Learning Versus Evolution Strategies:
    A Comparative Survey")）。MADRL 智能体通过与环境及其他智能体的互动来学习某些行为。由于环境和奖励状态受到所有智能体联合行动的影响，单智能体
    MDP 模型不能直接应用于 MADRL 系统，因为它们不遵循马尔可夫性质。马尔可夫（或随机）游戏（MG）[[121](#bib.bib121)] 框架是 MDP
    的一种推广，它捕捉了多个智能体之间的纠缠。在考虑 MADRL 系统时，有几个重要属性需要考虑。在接下来的部分，我们将讨论这些属性及其对整体系统的影响。'
- en: 'Setup: Cooperative vs Competitive. In a cooperative game, also known as a team
    problem, the agents seek to maximize a common reward signal by taking actions
    that favor their outcome, while taking into account their effects on other agents.
    Most contemporary applications are based upon a cooperative setup. Examples of
    this scenario include foraging, exploration and warehouse robots.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 设置：合作与竞争。在合作游戏中，也称为团队问题，参与者通过采取有利于其结果的行动来最大化共同的奖励信号，同时考虑这些行动对其他参与者的影响。大多数现代应用都基于合作设置。此场景的例子包括觅食、探索和仓库机器人。
- en: One of the main challenges of learning in a cooperative setting is termed as
    multi-agent credit assignment problem which refers to how to divide a reward obtained
    on a team level amongst individual learners [[122](#bib.bib122)]. Due to the complex
    interaction dynamics of the agents, it is not trivial to determine whose actions
    were beneficial to the group reward.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 合作环境中的主要挑战之一被称为多智能体信用分配问题，这涉及如何将团队层面获得的奖励分配给各个学习者[[122](#bib.bib122)]。由于智能体之间的复杂互动动态，确定哪些行动对集体奖励有益并非易事。
- en: On the other hands, agents in a competitive game receive different reward signals
    based on the overall outcome of the joint actions. In this setup, certain actions
    might be beneficial to one set of agents while being indifferent or disadvantageous
    for the other agents.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，竞争游戏中的智能体根据联合行动的整体结果接收不同的奖励信号。在这种设置中，某些行动可能对一组智能体有利，而对其他智能体则无关紧要或不利。
- en: 'Control: Centralized vs Decentralized. Another important distinction to make
    for MADRL systems is the centralized versus decentralized control approach. In
    the case of centralized control, there exists a single control entity that governs
    the decisions of all agents based on all available joint actions, joint rewards
    and joint observations. While this approach enables optimal decisions, it quickly
    becomes computationally intractable as the number of agents in a system grows.
    Additionally, this creates the risk of a single point of failure since the whole
    system could fail if the central controller breaks.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 控制：中心化与去中心化。在MADRL系统中另一个重要的区别是中心化与去中心化控制方法。在中心化控制的情况下，存在一个单一的控制实体，根据所有可用的联合行动、联合奖励和联合观察来统治所有代理的决策。虽然这种方法能实现最佳决策，但随着系统中代理数量的增加，它很快变得计算上不可行。此外，这还带来了单点故障的风险，因为如果中央控制器出现故障，整个系统可能会崩溃。
- en: 'The decentralized approach does not make use of a central controller and relies
    on agents to make decisions independently, based on the information available
    to them locally. Decentralized systems can be subdivided into two categories:
    "A decentralized setting with networked agents", and ”A fully decentralized setting“
    [[123](#bib.bib123)]. The former setup involves agents which can communicate with
    other agents and use the shared information to optimize their actions. In the
    latter scenario, agents make independent decisions without information exchange.
    While this means that no explicit messages can be sent, it is still possible to
    influence the behavior of other agents by affecting their reward as seen in [[124](#bib.bib124)].
    While the decentralized approach can provide more scalability and robustness,
    it also significantly increases the complexity of the system as there is no central
    entity that has knowledge of and can control the state of each robot. An interesting
    future research direction might be semi-centralized MADRL systems in which one
    or more central entities possess partial information of a set of agents. Alternatively,
    it is possible to alternate techniques between different phases of the design.
    Chen [[125](#bib.bib125)] proposed a system with centralized training and exploration
    and decentralized execution which can increase inter-agent collaboration and sample
    efficiency.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化的方法不使用中央控制器，而是依赖于代理根据其本地可用的信息独立做出决策。去中心化系统可以细分为两类：“具有网络化代理的去中心化环境”和“完全去中心化环境”[[123](#bib.bib123)]。前者涉及可以与其他代理进行通信并利用共享信息优化其行动的代理。在后者的场景中，代理在没有信息交换的情况下做出独立决策。虽然这意味着不能发送明确的信息，但仍然可以通过影响代理的奖励来影响其他代理的行为，如[[124](#bib.bib124)]所示。虽然去中心化的方法可以提供更多的可扩展性和鲁棒性，但由于没有中央实体对每个机器人的状态有了解并进行控制，这也显著增加了系统的复杂性。一个有趣的未来研究方向可能是半中心化的MADRL系统，其中一个或多个中央实体拥有部分代理的信息。或者，也可以在设计的不同阶段交替使用不同的技术。陈[[125](#bib.bib125)]提出了一种具有中心化训练和探索以及去中心化执行的系统，这可以提高代理间的协作和样本效率。
- en: Challenges in Multi-agent Reinforcement Learning. Moving from a single-agent
    to a multi-agent environment brings about new complex challenges with respect
    to learning and evaluating outcomes. This can be attributed to several factors,
    including the exponential growth of the search space and the non-stationarity
    of the environment [[126](#bib.bib126)]. Next, MADRL challenges are discussed.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理强化学习中的挑战。从单代理环境转移到多代理环境带来了与学习和评估结果相关的新复杂挑战。这可以归因于几个因素，包括搜索空间的指数增长和环境的非平稳性[[126](#bib.bib126)]。接下来，将讨论MADRL的挑战。
- en: 'Non-stationarity: In a MADRL system, agents are learning concurrently and their
    actions reshape their shared surroundings repeatedly, resulting in a non-stationay
    environment. Consequently, the convergence of well-known algorithms such as Q-learning
    can no longer be guaranteed as the Markov property assumption of the environment
    is violated [[127](#bib.bib127), [128](#bib.bib128), [28](#bib.bib28)]. Many papers
    in the literature that attempt to address the non-stationarity problem. Castaneda
    [[129](#bib.bib129)] proposed two algorithms: Deep loosely coupled Q-network (DLCQN)
    and deep repeated update Q-network (DRUQN). DLCQN modifies an independence degree
    for each agent based on the agent’s negative rewards and observations. The agent
    then utilizes this independence degree to decide when to act independently or
    cooperatively. DRUQN tries to avoid policy bias by making the value of an action
    inversely proportional to the probability of selecting that action. The use of
    an experience replay buffer with DQN enables efficient learning. However, due
    to the non-stationarity of the environment in MADRL settings data stored in an
    experience replay buffer can become outdated. To counter this unwanted behavior,
    Lenient-DQN conceived by Palmer et al. [[130](#bib.bib130)] utilizes decaying
    temperature values for adjusting the policy updates sampled from the experience
    replay memory.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 非平稳性：在 MADRL 系统中，智能体同时学习，并且它们的行为反复重塑共享的环境，从而导致环境的非平稳性。因此，像 Q 学习这样著名算法的收敛性不再得到保证，因为环境的马尔可夫性质假设被违反了[[127](#bib.bib127),
    [128](#bib.bib128), [28](#bib.bib28)]。许多文献中尝试解决非平稳性问题。Castaneda [[129](#bib.bib129)]
    提出了两种算法：深度松散耦合 Q 网络 (DLCQN) 和深度重复更新 Q 网络 (DRUQN)。DLCQN 基于智能体的负奖励和观察修改每个智能体的独立度。然后，智能体利用这个独立度来决定何时独立行动或合作行动。DRUQN
    通过使动作的价值与选择该动作的概率成反比来尽量避免策略偏倚。使用经验回放缓冲区的 DQN 使学习更高效。然而，由于 MADRL 环境的非平稳性，存储在经验回放缓冲区中的数据可能会过时。为应对这一不希望出现的行为，Palmer
    等人 [[130](#bib.bib130)] 提出的宽容-DQN 使用衰减的温度值来调整从经验回放记忆中采样的策略更新。
- en: 'Scalability: One way to deal with the non-stationarity problem is to train
    the agents in a centralized fashion and let them act according to a joint policy.
    However, this approach is not scalable as the number of agents increases, the
    state-action spaces grow exponentially, a phenomenon known as "combinatorial complexity"
    [[131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133)]. To balance the challenges
    imposed by non-stationarity and scalability, a centralized training and decentralized
    execution approach has been proposed [[134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136), [137](#bib.bib137), [138](#bib.bib138)].'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性：解决非平稳性问题的一种方法是以集中方式训练智能体，并让它们根据联合策略行动。然而，随着智能体数量的增加，这种方法并不可扩展，因为状态-动作空间呈指数增长，这一现象称为“组合复杂性”
    [[131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133)]。为了平衡非平稳性和可扩展性带来的挑战，提出了集中训练和分散执行的方法
    [[134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138)]。
- en: Modeling Multi-agent Reinforcement Problems. This section summarizes the common
    approaches of modeling and solving MADRL problems.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习问题建模。本节总结了建模和解决 MADRL 问题的常见方法。
- en: 'Independent-learning: Under this approach each agent considers other agents
    as part of the environment; consequently each agent is trained independently [[128](#bib.bib128),
    [139](#bib.bib139), tampuu2017multi-agent]. This approach does not suffer from
    the scalability problem [tampuu2017multi-agent, [140](#bib.bib140)], but it makes
    the environment non-stationary from each agent’s perspective [[141](#bib.bib141)].
    Furthermore, it conflicts with the usage of experience replay that improves the
    DQN algorithm [[28](#bib.bib28)]. To stabilize the experience replay buffer in
    MADRL settings, Foerster et al. [[140](#bib.bib140)] used importance sampling
    and replay buffer samples aging.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 独立学习：在这种方法下，每个智能体将其他智能体视为环境的一部分；因此，每个智能体独立进行训练 [[128](#bib.bib128), [139](#bib.bib139),
    tampuu2017multi-agent]。这种方法不会受到可扩展性问题的困扰 [tampuu2017multi-agent, [140](#bib.bib140)]，但从每个智能体的角度来看，它使环境变得非平稳
    [[141](#bib.bib141)]。此外，它与改进 DQN 算法的经验回放的使用相冲突 [[28](#bib.bib28)]。为了稳定 MADRL 环境中的经验回放缓冲区，Foerster
    等人 [[140](#bib.bib140)] 使用了重要性采样和回放缓冲区样本老化技术。
- en: 'Fully observable critic: A way to deal with the non-stationarity of a MADRL
    environment is by leveraging an actor-critic approach. Lowe et al. [[136](#bib.bib136)]
    proposed a multi-agent deep deterministic policy gradient (MADDPG) algorithm,
    where the actor policy accesses only the local observations whereas the critic
    has access to the actions, observations, and target policies of all agents during
    training. As the critic has global observability, the environment becomes stationary
    even though the policies of other agents change. A number of extensions to MADDPG
    has been proposed [[142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145)].'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可观察的评论家：处理MADRL环境非平稳性的一种方法是利用演员-评论家方法。Lowe等人[[136](#bib.bib136)]提出了一种多智能体深度确定性策略梯度（MADDPG）算法，其中演员策略仅访问局部观察，而评论家在训练期间可以访问所有智能体的动作、观察和目标策略。由于评论家具有全球可观察性，尽管其他智能体的策略发生变化，环境仍然变得平稳。已经提出了多个对MADDPG的扩展[[142](#bib.bib142)、[143](#bib.bib143)、[144](#bib.bib144)、[145](#bib.bib145)]。
- en: 'Value function decomposition: Learning the optimal action-value function in
    fully cooperative MADRL settings is challenging. To coordinate the agents’ actions,
    learning a centralized action-value function, $Q_{tot}$, is desirable. However,
    when the number of agents is large, learning such a function is challenging. Independent-learning
    (where each agent learns its action-value function, $Q_{i}$) does not face such
    a challenge, but it also neglects interactions between agents, which results in
    sub-optimal collective performance. Value function decomposition methods try to
    capitalize on the advantages of these two approaches. It represents $Q_{tot}$
    as a mixing of $Q_{i}$ that is conditioned only on local information. Value-Decomposition
    Network (VDN) algorithm assumes that $Q_{tot}$ can be additively decomposed into
    $NQ_{i}$ for $N$ agents. QMIX [[135](#bib.bib135)] algorithm improves on VDN by
    relaxing some of the additivity constrains and enforcing positive weights on the
    mixer network.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数分解：在完全合作的MADRL环境中，学习最佳的动作-价值函数是具有挑战性的。为了协调智能体的动作，学习一个集中式的动作-价值函数$Q_{tot}$是理想的。然而，当智能体数量较多时，学习这样的函数是具有挑战性的。独立学习（每个智能体学习其动作-价值函数$Q_{i}$）不会面临这样的挑战，但也忽视了智能体之间的互动，从而导致次优的集体表现。价值函数分解方法试图利用这两种方法的优势。它将$Q_{tot}$表示为仅基于局部信息的$Q_{i}$的混合。价值分解网络（VDN）算法假设$Q_{tot}$可以分解为$N$个$Q_{i}$。QMIX[[135](#bib.bib135)]算法通过放宽一些加性约束并对混合网络施加正权重来改进VDN。
- en: 'Learning to communicate: Cooperative environments may allow agents to communicate.
    In such settings, the agents can learn a communication protocol to achieve their
    shared objective more optimally [[146](#bib.bib146), [147](#bib.bib147)]. Foerster
    et al. [[148](#bib.bib148)] proposed two algorithms, Reinforced Inter-Agent Learning
    (RIAL) and Differentiable Inter-Agent Learning (DIAL), that use deep networks
    to learn to communicate. RIAL is based on Deep Recurrent Q-Network with independent
    Q-learning. It shares the parameters of a single neural network between the agents.
    In contrast, DIAL passes gradients directly via the communication channel during
    learning. While a discrete communication channel is used in realizing RIAL and
    DIAL, CommNet [[149](#bib.bib149)] utilizes a continuous vector channel. Over
    this channel, agents obtain the summed transmissions of other agents. Results
    show that agents can learn to communicate and improve their performance over non-communicating
    agents.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 学习沟通：合作环境可能允许智能体进行沟通。在这种情况下，智能体可以学习一种沟通协议，以更优化地实现其共享目标[[146](#bib.bib146)、[147](#bib.bib147)]。Foerster等人[[148](#bib.bib148)]提出了两个算法，强化智能体间学习（RIAL）和可微分智能体间学习（DIAL），它们利用深度网络学习沟通。RIAL基于独立Q学习的深度递归Q网络。它在智能体之间共享一个神经网络的参数。相比之下，DIAL在学习过程中通过沟通通道直接传递梯度。虽然在实现RIAL和DIAL时使用了离散沟通通道，但CommNet[[149](#bib.bib149)]利用了连续向量通道。通过这个通道，智能体获得了其他智能体的累积传输。结果表明，智能体可以学会沟通，并在性能上超越非沟通智能体。
- en: 'Partial observability: Foerster et al. [[148](#bib.bib148)] introduced a deep
    distributed recurrent Q-network (DDRQN) algorithm based on a long short-term memory
    network to deal with POMDP problems in the multi-agent setting. Gupta et al. [[150](#bib.bib150)]
    extended three types of single-agent RL algorithms based on policy gradient, temporal-difference
    error, and actor-critic methods to the multi-agent systems domain. Their work
    shows the importance of using DRL with curriculum learning to address the problem
    of learning cooperative policies in partially observable complex environments.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 部分可观察性：Foerster 等人 [[148](#bib.bib148)] 引入了一种基于长短期记忆网络的深度分布式递归 Q 网络（DDRQN）算法，以解决多智能体环境中的
    POMDP 问题。Gupta 等人 [[150](#bib.bib150)] 将三种基于策略梯度、时间差分误差和演员-评论员方法的单智能体 RL 算法扩展到多智能体系统领域。他们的工作表明，使用带有课程学习的
    DRL 以解决部分可观察复杂环境中合作策略学习问题的重要性。
- en: 'We refer the interested reader to the following survey papers for a more in-depth
    discussion on the topic of multi-agent reinforcement learning: Hernandez-Leal
    et al. [[127](#bib.bib127)] provide a comprehensive survey on the non-stationarity
    problem in MADRL; OroojlooyJadid and Hajinezhad [[141](#bib.bib141)] scope their
    survey to include the papers that study decentralized MADRL models with a cooperative
    goal; Da Silva and Costa [[151](#bib.bib151)] focus on transfer learning for MADRL
    systems; a survey on MADRL from the perspective of challenges and applications
    is introduced by Du and Ding [[152](#bib.bib152)]; a selective overview of theories
    and algorithms is presented in [zhang2019multi-agent]; and a survey and critique
    of MADRL is given in [[132](#bib.bib132)].'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多智能体强化学习这一主题的深入讨论，我们建议感兴趣的读者参考以下综述论文：Hernandez-Leal 等人 [[127](#bib.bib127)]
    提供了关于 MADRL 中非平稳性问题的全面综述；OroojlooyJadid 和 Hajinezhad [[141](#bib.bib141)] 将他们的综述范围扩大到包括研究具有合作目标的去中心化
    MADRL 模型的论文；Da Silva 和 Costa [[151](#bib.bib151)] 重点关注 MADRL 系统中的迁移学习；Du 和 Ding
    [[152](#bib.bib152)] 介绍了从挑战和应用角度出发的 MADRL 综述；[zhang2019multi-agent] 中展示了理论和算法的选择性概述；而
    [[132](#bib.bib132)] 则提供了 MADRL 的综述和批评。
- en: IV-D2 Multi-agent Evolution Strategies
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 多智能体进化策略
- en: 'ES algorithms do not require the problem to be formulated as an MDP; therefore,
    they do not suffer from the non-stationarity of the environment. Consequently,
    it is relatively easy to extend a single-agent ES algorithm to the multi-agent
    domain and develop an application. Hiraga et al. [[153](#bib.bib153)] developed
    robotics controllers based on ESs for managing congestion in robotic swarms path
    formation using LEDs. The performed experiment covered a swarm of robots, each
    having seven distance sensors, a ground sensor, an omnidirectional camera, and
    RGB LEDs. An artificial neural network (three-layered neural network) represents
    the controller of the robot, having as inputs: the distance sensors, ground sensors,
    and the cameras, and as outputs: the motors and LEDs controls. ($\mu,\lambda$)-ES
    is utilized to optimize the weights of the controller. A copy of the controller
    is implemented on N different robots, before being evaluated and assessed depending
    on the swarm’s performance. Another similar approach was proposed in [[154](#bib.bib154)]
    for building a swarm capable of cooperatively transporting food to a nest and
    collectively distinguishing between foods and poisons. Hiraga et al. [[154](#bib.bib154)]
    developed a controller for a robotic swarm using CMA-ES, aiming to automatically
    generate the behavior of the robots.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ES 算法不要求问题被表述为 MDP，因此，它们不会受到环境非平稳性的影响。因此，相对容易将单智能体 ES 算法扩展到多智能体领域并开发应用。Hiraga
    等人 [[153](#bib.bib153)] 开发了基于 ES 的机器人控制器，用于管理机器人群体在使用 LED 进行路径形成时的拥堵。实验涵盖了一群机器人，每个机器人配备了七个距离传感器、一个地面传感器、一个全向摄像头和
    RGB LED。一个人工神经网络（三层神经网络）代表了机器人的控制器，其输入包括：距离传感器、地面传感器和摄像头，输出包括：电机和 LED 控制。使用 ($\mu,\lambda$)-ES
    来优化控制器的权重。在对机器人群体的表现进行评估和评估之前，控制器的副本在 N 个不同的机器人上实现。[[154](#bib.bib154)] 提出了另一种类似的方法，用于构建一个能够协作运输食物到巢穴并共同区分食物和毒药的群体。Hiraga
    等人 [[154](#bib.bib154)] 使用 CMA-ES 开发了一个机器人群体控制器，旨在自动生成机器人的行为。
- en: 'Tang et al. [[155](#bib.bib155)] proposed an adversarial training multi-agent
    learning system, in which a quadruped robot (protagonist) is trained to become
    more agile by hunting an ensemble of robots that are escaping (adversaries) following
    different strategies. An ensemble of adversaries is used, as each will propose
    a different escape strategy, thus improving agility (agility refers to coordinated
    control of legs, balance control, etc.). Training is done using ESs and more specifically
    by augmenting CMA-ES to the multi-agent framework. There are two steps for training:
    An outer loop which iteratively trains the protagonist and adversaries, and an
    inner loop for optimizing the policy of each. Policies are represented by feed-forward
    neural networks and are optimized with CMA-ES.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 唐等[[155](#bib.bib155)]提出了一种对抗性训练的多智能体学习系统，其中一个四足机器人（主体）通过追捕一组采用不同策略逃逸的机器人（对手）来提高灵活性。使用了一组对手，因为每个对手会提出不同的逃逸策略，从而提高灵活性（灵活性指的是腿部协调控制、平衡控制等）。训练使用ESs，特别是通过将CMA-ES扩展到多智能体框架中进行。训练分为两个步骤：外部循环，迭代训练主体和对手；内部循环，为每个策略优化。策略由前馈神经网络表示，并通过CMA-ES进行优化。
- en: 'Chen and Gao [[156](#bib.bib156)] proposed a predator-prey system that leverages
    ESs (OpenAI-ES, CMA-ES). It consists of having multiple predators trained to catch
    prey in a certain time frame. The predator controllers are homogeneous and are
    represented by neural networks which parameters are optimized with ESs (OpenAI-ES,
    CMA-ES) and Bayesian Optimization. The NN has three inputs (the inverse of the
    distance from the predator to the other nearest predator, the angle between the
    orientation of the predator and the direction of the prey relative to the predator,
    the distance between the predator itself and the prey), one hidden layer and two
    outputs for controlling the angular velocities of the two wheels. As for the prey’s
    controller, it follows a simple fixed evasion strategy: having computed a danger
    zone map, the prey navigates towards the least dangerous locations. After performing
    various experiments, the predators showcased a successful collective behavior:
    moving following a formation and avoiding collisions.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 陈和高[[156](#bib.bib156)]提出了一种利用进化策略（ESs）（OpenAI-ES，CMA-ES）的捕食者-猎物系统。该系统包括多个捕食者在一定时间内被训练以捕捉猎物。捕食者控制器是均质的，并由神经网络表示，其参数通过ESs（OpenAI-ES，CMA-ES）和贝叶斯优化进行优化。神经网络有三个输入（捕食者到其他最近捕食者的距离的倒数、捕食者的朝向与猎物相对于捕食者的方向之间的角度、捕食者本身与猎物之间的距离）、一个隐藏层和两个输出，用于控制两个轮子的角速度。至于猎物的控制器，它遵循一个简单的固定逃避策略：通过计算危险区域图，猎物向最不危险的位置导航。在进行各种实验后，捕食者展示了成功的集体行为：按照一定的队形移动并避免碰撞。
- en: 'Multi-agent credit assignment problem. In a multi-agent setting, agents often
    receive a shared reward for all the agents, making it harder to learn proper cooperative
    behaviors. Li et al. [[157](#bib.bib157)] thus proposed to use Parallelized ESs
    along with a Value Decomposition Network (useful for identifying each agent’s
    contribution to the training process) for solving cooperative multi-agent tasks.
    Figure [9](#S4.F9 "Figure 9 ‣ IV-D2 Multi-agent Evolution Strategies ‣ IV-D Learning
    in multiagent settings ‣ IV Deep Reinforcement Learning Versus Evolution Strategies
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    is an overview of the overall PES-VD algorithm, which consists of two phases.
    First, the policies of each agent are represented by a NN with parameters $\boldsymbol{\theta}$,
    optimized using Parallelized ES. Each agent thus identifies its actions independently
    following its policy and by interacting with its environment. In a second place,
    seeing how the reward is common to the whole team, a Value Decomposition Network
    is used to compute the fitness for each of the different policies. PES-VD is implemented
    in parallel on multiple cores: $M$ workers evaluate the policies and compute the
    gradients of the Value Decomposition Network and a master node collects the data
    and updates the policies and the Value Decomposition Network accordingly.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '多智能体信用分配问题。在多智能体环境中，智能体通常会获得所有智能体的共享奖励，这使得学习适当的合作行为变得更加困难。Li 等人 [[157](#bib.bib157)]
    因此提出了使用并行化的进化策略（ESs）结合价值分解网络（用于识别每个智能体在训练过程中的贡献）来解决合作多智能体任务。图 [9](#S4.F9 "Figure
    9 ‣ IV-D2 Multi-agent Evolution Strategies ‣ IV-D Learning in multiagent settings
    ‣ IV Deep Reinforcement Learning Versus Evolution Strategies ‣ Deep Reinforcement
    Learning Versus Evolution Strategies: A Comparative Survey") 是整体 PES-VD 算法的概述，包含两个阶段。首先，每个智能体的策略由参数为
    $\boldsymbol{\theta}$ 的神经网络（NN）表示，通过并行化的 ES 进行优化。因此，每个智能体根据其策略和与环境的交互独立识别其行为。在第二阶段，由于奖励对整个团队是共同的，使用价值分解网络计算不同策略的适应度。PES-VD
    在多个核心上并行实现：$M$ 个工作节点评估策略并计算价值分解网络的梯度，主节点则收集数据并相应地更新策略和价值分解网络。'
- en: '![Refer to caption](img/5cfc71aee7f95f9bc3f548ddd7655c39.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5cfc71aee7f95f9bc3f548ddd7655c39.png)'
- en: 'Figure 9: PES-VD overview'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：PES-VD 概述
- en: Various researchers proposed multi-agent solutions for swarm scenarios leveraging
    ESs. Each robot in the swarm runs the same network, thus maintaining collective
    behavior. Rais Martínez and Aznar Gregori [[158](#bib.bib158)] assess the performance
    of ESs (CMA-ES, PEPG, SES, GA, and OpenAI-ES) for multi-agent learning in the
    swarm aggregation task. In this problem, the robots controllers are represented
    by a NN with 2 hidden layers. Each has 8 infrared sensors and 4 microphones for
    inputs and 2 wheels and a speaker as output. Similarly, Fan et al. [[159](#bib.bib159)]
    used ESs on different multi-agent UAV swarm combat scenarios. Aznar et al. [[160](#bib.bib160)]
    developed a swarm foraging behavior using DRL and CMA-ES.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究者提出了利用进化策略（ESs）解决群体场景中的多智能体解决方案。每个群体中的机器人运行相同的网络，从而保持集体行为。Rais Martínez
    和 Aznar Gregori [[158](#bib.bib158)] 评估了 ESs（CMA-ES、PEPG、SES、GA 和 OpenAI-ES）在群体聚集任务中的多智能体学习表现。在这个问题中，机器人的控制器由具有
    2 个隐藏层的神经网络表示。每个机器人配备 8 个红外传感器和 4 个麦克风作为输入，2 个轮子和一个扬声器作为输出。类似地，Fan 等人 [[159](#bib.bib159)]
    在不同的多智能体无人机群体作战场景中使用了 ESs。Aznar 等人 [[160](#bib.bib160)] 使用深度强化学习（DRL）和 CMA-ES
    开发了一个群体觅食行为。
- en: IV-D3 Comparison
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D3 比较
- en: Here we summarize our observations of this section
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了这一部分的观察结果
- en: •
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training under a multi-agent setting is more challenging than training a single
    agent for a plethora of reasons. There are usually two types of agents in MADRL:
    cooperative and competitive agents. Algorithms can make use of a centralized or
    decentralized framework and will act in a partially or fully observable environment.'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多智能体环境下的训练比单一智能体训练更具挑战性，原因有很多。MADRL 中通常有两种类型的智能体：合作性智能体和竞争性智能体。算法可以使用集中式或分散式框架，并在部分或完全可观测的环境中进行操作。
- en: •
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: New algorithms such as PES-VD [[157](#bib.bib157)] propose a direct solution
    to some of the main challenges of MADRL. PES-VD uses a Value Decomposition Network
    for solving multi-agent credit assignment problems.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新算法如 PES-VD [[157](#bib.bib157)] 提出了直接解决 MADRL 主要挑战的一些方法。PES-VD 使用价值分解网络来解决多智能体信用分配问题。
- en: •
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Using ESs for multi-agent learning is still a growing field with a large potential
    as to the many advantages ESs can bring to concepts such as “collective robotic
    learning" and “cloud robotics" [[161](#bib.bib161)] with its improved approach
    to parallelism [[4](#bib.bib4)].
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多智能体学习中使用进化策略仍然是一个不断发展的领域，具有很大的潜力，因为进化策略可以为“集体机器人学习”和“云机器人”[[161](#bib.bib161)]等概念带来许多优势，尤其是在并行性改进方面[[4](#bib.bib4)]。
- en: •
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Semi-centralized MADRL systems are an interesting future research direction
    in which a few central entities possess partial information of a set of agents.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半集中式多智能体深度强化学习系统是一个有趣的未来研究方向，其中少数中心实体拥有一组智能体的部分信息。
- en: •
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The literature on ESs for multi-agent scenarios seems to focus on enabling applications.
    We hypothesis that this is because it is less challenging to extend single-agent
    ES algorithms to the multi-agent domain. This is because ES algorithms do not
    require Markov property in the formulation of the problem, and therefore, they
    do not suffer from non-stationary environments.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于多智能体场景中的进化策略（ES）的文献似乎集中在实现应用程序上。我们假设这是因为将单智能体进化策略算法扩展到多智能体领域挑战较小。这是因为进化策略算法在问题的表述中不需要马尔可夫性质，因此不会受到非平稳环境的影响。
- en: V Hybrid Deep Reinforcement Learning and Evolution Strategies Algorithms
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 混合深度强化学习和进化策略算法
- en: '![Refer to caption](img/7942d5732f85f749aa39922a2f90abb7.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7942d5732f85f749aa39922a2f90abb7.png)'
- en: 'Figure 10: CEM-RL [[162](#bib.bib162)]: a hybrid algorithm that combines cross-entropy
    method with (Twin) Deep Deterministic Gradient Policy [[163](#bib.bib163)].'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: CEM-RL [[162](#bib.bib162)]: 一种将交叉熵方法与（双）深度确定性梯度策略[[163](#bib.bib163)]结合的混合算法。'
- en: Although DRL and ES have the same objective—optimizing an objective function
    in a potentially unknown environment—they have different strengths and weaknesses
    [[164](#bib.bib164), [165](#bib.bib165)]. For example, DRL can be sample efficient
    thanks to the combination of RL and deep learning; while ES have robust convergence
    properties and exploration strategies. The hybrid approach combines DRL and ES
    to get the best of both worlds. Although the idea is not new [[166](#bib.bib166)],
    hybridizing DRL and ES has gained momentum, driven by the recent success of DRL
    and ES [[4](#bib.bib4), [167](#bib.bib167)]. Combining these strengths has, among
    others, let to very strong play in the most challenging Real-Time Strategy (RTS)
    games such as StarCraft [[168](#bib.bib168)]. We describe in the following a few
    population-guided parallel learning schemes that enhance the performance of RL
    algorithms.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度强化学习（DRL）和进化策略（ES）有着相同的目标——在潜在未知的环境中优化目标函数——它们各自的优势和劣势却不同[[164](#bib.bib164),
    [165](#bib.bib165)]。例如，得益于强化学习和深度学习的结合，DRL可以实现样本效率；而进化策略则具有强健的收敛特性和探索策略。混合方法将DRL和ES结合，以获得两者的最佳效果。尽管这一思想并不新颖[[166](#bib.bib166)]，但混合DRL和ES的趋势得到了一定的推动，受到DRL和ES近期成功的影响[[4](#bib.bib4),
    [167](#bib.bib167)]。结合这些优势，使得在最具挑战性的实时战略（RTS）游戏如《星际争霸》[[168](#bib.bib168)]中表现非常强劲。以下我们描述了一些由种群引导的并行学习方案，这些方案能够提升强化学习算法的性能。
- en: 'Pourchot and Sigaud [[162](#bib.bib162)] addressed the problem of policy search
    by proposing CEM-RL: a hybrid algorithm that combines a cross-entropy method (CEM)
    with either the Twin Delayed Deep Deterministic policy gradient (TD3) [[163](#bib.bib163)]
    or the Deep Deterministic Policy Gradient DDPG [[19](#bib.bib19)] algorithms (Figure [10](#S5.F10
    "Figure 10 ‣ V Hybrid Deep Reinforcement Learning and Evolution Strategies Algorithms
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")).
    The CEM-RL architecture consists of a population of actors that are generated
    using CEM, and of a single DDPG or TD3 agent. The actors generate diversified
    training data for the DDPG/TD3 agent, and the gradients obtained from DDPG/TD3
    are periodically inserted into the population of the CEM to optimize the searching
    process. The authors showed that CEM-RL is superior to CEM, TD3 [[163](#bib.bib163)],
    and Evolution Reinforcement Learning (ERL) [[169](#bib.bib169)]: a hybrid algorithm
    that combines a DDPG agent with an evolutionary algorithm. Shopov and Markova
    [[170](#bib.bib170)] combined ESs and multi-agent DRL (Deep Q-Networks) for Sequential
    Games and showcased the model’s efficiency as compared to Classical multi-agent
    reinforcement training with $\epsilon$-greedy. The experiment performed by Shopov
    and Markova [[170](#bib.bib170)] aims to optimize the behaviour of a group of
    autonomous agents (the pursuers) in a map. Tests were performed on two cases:
    one map with almost no obstacles and another with many obstacles (increased probability
    of falling into the local minimum). Using ESs on the latter yielded better performance.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pourchot 和 Sigaud [[162](#bib.bib162)] 通过提出 CEM-RL 解决了策略搜索的问题：这是一种混合算法，将交叉熵方法（CEM）与双延迟深度确定性策略梯度（TD3）
    [[163](#bib.bib163)] 或深度确定性策略梯度（DDPG） [[19](#bib.bib19)] 算法结合在一起（见图 [10](#S5.F10
    "Figure 10 ‣ V Hybrid Deep Reinforcement Learning and Evolution Strategies Algorithms
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")）。CEM-RL
    架构包括一个使用 CEM 生成的演员群体和一个 DDPG 或 TD3 代理。演员们为 DDPG/TD3 代理生成多样化的训练数据，并且从 DDPG/TD3
    获得的梯度会定期插入到 CEM 的群体中，以优化搜索过程。作者们展示了 CEM-RL 优于 CEM、TD3 [[163](#bib.bib163)] 和进化强化学习（ERL）
    [[169](#bib.bib169)]：一种将 DDPG 代理与进化算法结合的混合算法。Shopov 和 Markova [[170](#bib.bib170)]
    将进化策略和多智能体深度强化学习（深度 Q 网络）结合用于顺序游戏，并展示了该模型相对于经典的多智能体强化训练（$\epsilon$-贪婪）的效率。Shopov
    和 Markova [[170](#bib.bib170)] 进行的实验旨在优化一组自主代理（追捕者）在地图上的行为。实验在两个情况下进行：一个是几乎没有障碍的地图，另一个是有许多障碍的地图（增加了陷入局部最小值的概率）。在后者情况下使用进化策略取得了更好的性能。'
- en: 'Houthooft et al. [[171](#bib.bib171)] devised a hybrid RL agent, Evolved Policy
    Gradients (EPG), that, in addition to the policy, optimizes a loss function. EPG
    consists of two optimization loops: the inner loop uses stochastic gradient descent
    to optimize the agent’s policy, while the outer one utilizes ES to tune the parameters
    of a loss function that the inner loop minimizes. Thanks to this ability to fine
    tune the loss function according to the environment and agent history, EPG can
    learn faster than a standard RL agent.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Houthooft 等人 [[171](#bib.bib171)] 设计了一种混合强化学习（RL）代理，称为进化策略梯度（EPG），该代理除了优化策略外，还优化一个损失函数。EPG
    由两个优化循环组成：内部循环使用随机梯度下降优化代理的策略，而外部循环利用进化策略（ES）调整内部循环最小化的损失函数的参数。得益于这种根据环境和代理历史微调损失函数的能力，EPG
    的学习速度比标准 RL 代理更快。
- en: Diqi Chen and Gao [[172](#bib.bib172)] proposed a hybrid agent to approximate
    the Pareto frontier uniformly in a multi-objective decision-making problem. The
    authors argued that despite the fast convergence of DRL, it cannot guarantee a
    uniformly approximated Pareto frontier. On the other hand, ES achieve a well-distributed
    Pareto frontier, but they face difficulties optimizing a DNN. Therefore, Diqi Chen
    and Gao [[172](#bib.bib172)] proposed a two-stage multi-objective reinforcement
    learning (MORL) framework. In the first stage, a multi-policy soft actor-critic
    algorithm learns multiple policies collaboratively. And, in the second stage,
    a multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES)
    fine-tunes policy-independent parameters to approach a uniform Pareto frontier.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: Diqi Chen和Gao [[172](#bib.bib172)] 提出了一个混合代理来均匀地逼近多目标决策问题中的帕累托前沿。作者认为，尽管DRL具有快速收敛的优势，但无法保证均匀逼近帕累托前沿。另一方面，ES能够实现良好分布的帕累托前沿，但在优化DNN时面临困难。因此，Diqi
    Chen和Gao [[172](#bib.bib172)] 提出了一个两阶段多目标强化学习（MORL）框架。在第一阶段，一个多策略软演员-评论家算法协作学习多个策略。在第二阶段，一个多目标协方差矩阵适应进化策略（MO-CMA-ES）微调策略无关的参数，以接近均匀的帕累托前沿。
- en: 'De Bruin et al. [[173](#bib.bib173)] used a hybrid approach to train and fine-tune
    a DNN control policy. Their approach consists of two main steps: (i) learning
    a state representation and initial policy from high-dimensional input data using
    gradient-based methods (i.e., DQN or DDPG); and (ii) fine-tuning the final action
    selection parameters of the DNN using CMA-ES. This architecture enables the policy
    to surpass in performance its gradient-based counterpart while using fewer trials
    compared to a pure gradient-free policy.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: De Bruin等人[[173](#bib.bib173)]采用了一种混合方法来训练和微调DNN控制策略。他们的方法包括两个主要步骤：（i）使用基于梯度的方法（即DQN或DDPG）从高维输入数据中学习状态表示和初始策略；（ii）使用CMA-ES微调DNN的最终动作选择参数。这种架构使得策略在性能上超越了其基于梯度的对应物，同时使用的试验次数也比纯梯度无关策略少。
- en: Several other researchers have also proposed solutions hybridizing ES and DRL
    for various applications. For example, Song et al. [[174](#bib.bib174)] proposed
    ES-ENAS, a neural architecture search (NAS) algorithm for identifying RL policies
    using ES and Efficient NAS (ENAS); Ferreira et al. [[175](#bib.bib175)] used ES
    to learn agent-agnostic synthetic environments (SEs) for Reinforcement Learning.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究人员也提出了将ES和DRL混合用于各种应用的解决方案。例如，Song等人[[174](#bib.bib174)]提出了ES-ENAS，这是一种用于识别RL策略的神经架构搜索（NAS）算法，结合了ES和高效NAS（ENAS）；Ferreira等人[[175](#bib.bib175)]使用ES学习代理无关的合成环境（SEs）以进行强化学习。
- en: V-A Comparison
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 比较
- en: 'TABLE VII: Hybrid algorithms highlights'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表VII：混合算法亮点
- en: '| Algorithm | Description | Experiments | Ref. |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 | 实验 | 参考 |'
- en: '| CEM-RL | combines a cross-entropy method and Twin Delayed Deep Deterministic
    policy gradient [[163](#bib.bib163)] to find robust policies | outperforms CEM,
    TD3, multi-actor TD3, and Evolutionary Reinforcement Learning [[169](#bib.bib169)]
    | [[162](#bib.bib162)] |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| CEM-RL | 结合了交叉熵方法和双延迟深度确定性策略梯度[[163](#bib.bib163)]以寻找稳健的策略 | 优于CEM、TD3、多演员TD3和进化强化学习[[169](#bib.bib169)]
    | [[162](#bib.bib162)] |'
- en: '| Evolved Policy Gradients (EPG) | uses gradient descent and CMA-ES for policy
    and loss function optimization, respectively | achieves faster learning than policy
    gradient methods and provides qualitatively different behavior from other popular
    meta-learning algorithms | [[171](#bib.bib171)] |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 演化策略梯度（EPG） | 分别使用梯度下降和CMA-ES进行策略和损失函数优化 | 比策略梯度方法实现了更快的学习，并且提供了与其他流行的元学习算法不同的行为
    | [[171](#bib.bib171)] |'
- en: '| MO-CMA-ES | integrates a multi-policy soft actor-critic algorithm with a
    multi-objective covariance matrix adaptation evolution strategy to approach uniform
    Pareto frontier | exceeds other algorithms such as the hypervolume-based [[176](#bib.bib176)],
    radial [[177](#bib.bib177)], Pareto following [[177](#bib.bib177)], and Deep Neuroevolution [[178](#bib.bib178)]
    algorithm on computing the Pareto frontier | [[172](#bib.bib172)] |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| MO-CMA-ES | 结合了多策略软演员-评论家算法与多目标协方差矩阵适应进化策略，以接近均匀的帕累托前沿 | 超越了其他算法，如基于超体积的[[176](#bib.bib176)]、径向[[177](#bib.bib177)]、帕累托跟随[[177](#bib.bib177)]和深度神经进化[[178](#bib.bib178)]算法在计算帕累托前沿方面
    | [[172](#bib.bib172)] |'
- en: '| Fine-tuned DRL | combines CMA-ES and DQN or DDPG to train and fine-tune a
    DNN control policy | surpasses gradient-based methods while requiring less iterations
    than gradient-free ones | [[173](#bib.bib173)] |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 微调DRL | 结合CMA-ES和DQN或DDPG来训练和微调深度神经网络（DNN）控制策略 | 超越基于梯度的方法，同时比无梯度方法需要更少的迭代
    | [[173](#bib.bib173)] |'
- en: 'Here we summarize our observations of this section:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了本节的观察结果：
- en: •
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL suffers from temporal credit assignment, sensitivity in the hyperparameters’
    selection and might suffer from more brittle exploration due to its unique agent
    setting, while ES has low data efficiency and struggle with large optimization
    tasks.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL 遭遇时间信用分配问题、超参数选择的敏感性，并且由于其独特的代理设置可能面临更脆弱的探索，而ES则数据效率低，处理大型优化任务时存在困难。
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combining both approaches can help address some of these identified challenges.
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结合这两种方法可以帮助解决一些已识别的挑战。
- en: •
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some hybrid methods proposed throughout the literature seem to outperform the
    use of each method on its own.
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文献中提出的一些混合方法似乎比单独使用每种方法的效果更好。
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybridizing DRL and ES is still a relatively new field of research.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合深度强化学习（DRL）和进化策略（ES）仍然是一个相对新的研究领域。
- en: VI Applications
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 应用
- en: 'Next, we compare DRL and ESs based on the applications they support. The goal
    is to get an indication of their potential by tracking their application record
    so far. The results of querying Google Scholar is presented in Table [VIII](#S6.T8
    "TABLE VIII ‣ VI-A Deep Reinforcement Learning applications ‣ VI Applications
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")
    in conjunction with the keywords that were used¹¹1The search query template is
    “allintitle: “evolution strategies” OR “evolutionary strategies” key_word_1 OR
    key_word_1 -excluded_key_word” and “allintitle: “reinforcement learning” ….'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们根据它们支持的应用比较DRL和ES。目标是通过跟踪它们的应用记录来了解它们的潜力。查询Google Scholar的结果展示在表格 [VIII](#S6.T8
    "TABLE VIII ‣ VI-A Deep Reinforcement Learning applications ‣ VI Applications
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")中，并结合所使用的关键词¹¹1
    搜索查询模板是“allintitle: “evolution strategies” OR “evolutionary strategies” key_word_1
    OR key_word_1 -excluded_key_word”和“allintitle: “reinforcement learning” ….'
- en: VI-A Deep Reinforcement Learning applications
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 深度强化学习应用
- en: 'TABLE VIII: Searching Google Scholar for DRL and ESs applications (only papers’
    titles are considered).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE VIII: 在Google Scholar上搜索DRL和ES的应用（仅考虑论文标题）。'
- en: '| Industry field | Search terms | Results |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 行业领域 | 搜索词 | 结果 |'
- en: '| DRL | ESs |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| DRL | ESs |'
- en: '| Gaming | game, games, gaming, playing, play, mahjong, atari, tetris, soccer
    excluding survey and review | 1630 | 44 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 游戏 | game, games, gaming, playing, play, mahjong, atari, tetris, soccer 排除调查和综述
    | 1630 | 44 |'
- en: '| Robotics | robotics, “motion control”, robots, “robot navigation”, assembly,
    robot, grasping excluding survey and review | 2350 | 39 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 机器人技术 | robotics, “motion control”, robots, “robot navigation”, assembly,
    robot, grasping 排除调查和综述 | 2350 | 39 |'
- en: '| Finance | finance, financial, trading, portfolio, stock, price, liquidation,
    hedging, banking, trader, cryptocurrency, underpricing excluding survey and review
    | 475 | 34 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 财务 | finance, financial, trading, portfolio, stock, price, liquidation, hedging,
    banking, trader, cryptocurrency, underpricing 排除调查和综述 | 475 | 34 |'
- en: '| Communications | network, routing, communications, wireless, 5g, LTE, MAC,
    “access control”, “network slicing”, excluding “neural network” survey and review
    | 2020 | 53 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 通信 | network, routing, communications, wireless, 5g, LTE, MAC, “access control”,
    “network slicing”, 排除“neural network”调查和综述 | 2020 | 53 |'
- en: '| Energy | energy, power excluding survey and review | 1470 | 41 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | energy, power 排除调查和综述 | 1470 | 41 |'
- en: '| Transportation | transportation, transport, vehicle, traffic, fleet, driving
    excluding survey and review | 1580 | 25 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 交通运输 | transportation, transport, vehicle, traffic, fleet, driving 排除调查和综述
    | 1580 | 25 |'
- en: Gaming. Video games such as the Atari games [[179](#bib.bib179)] are excellent
    testbeds for DRL algorithms, given their well-defined problem settings and virtual
    environment. This makes evaluation safe and fast compared to real-world experiments
    [[180](#bib.bib180)].
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏。像Atari游戏[[179](#bib.bib179)]这样的电子游戏是DRL算法的优秀测试平台，鉴于其明确的问题设置和虚拟环境。这使得评估相比于现实世界实验更为安全和快速[[180](#bib.bib180)]。
- en: There have been two important triumphs for DRL with respect to perfect information
    games. First, in 2015, Mnih et al. [[181](#bib.bib181)] developed an algorithm
    that could learn to play different Atari 2600 games at a superhuman level using
    only the image pixels as input. This work paved the way for DRL applications trained
    on high-dimensional data based only on the reward signal. Soon after, in 2016,
    Silver et al. [[182](#bib.bib182)] developed AlphaGo, the first program ever to
    beat a world champion in Go. Instead of the handcrafted rules often seen in chess
    programs, AlphaGo consisted of neural networks trained using a combination of
    Supervised Learning (SL) and RL. Only a year later, this achievement was triumphed
    by Silver et al. [[183](#bib.bib183)], whose AlphaGo Zero program beat its predecessor
    AlphaGo. AlphaGo Zero was based solely on RL, omitting the need for human data.
    More recent works have also been successful in imperfect information games which,
    unlike Go and Atari games, only let agents observe part of the system. In OpenAI
    Five [[184](#bib.bib184)], agents were able to defeat the world’s esports champions
    in the game of Dota2, while AlphaStar [[185](#bib.bib185)] attained one of the
    highest rankings in the complex real-time strategy game of StarCraft II. [[46](#bib.bib46),
    [50](#bib.bib50), [76](#bib.bib76), [65](#bib.bib65)] further examined DRL algorithms’
    ability to scale, parallelize, and explore using Atari games. Lastly, an extensive
    survey on DRL in video games has been composed by Shao et al. [[180](#bib.bib180)].
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在完美信息游戏方面，DRL取得了两项重要的胜利。首先，在2015年，Mnih等人[[181](#bib.bib181)]开发了一种算法，该算法能够仅使用图像像素作为输入，学习玩不同的Atari
    2600游戏，并达到超人类水平。这项工作为基于奖励信号的高维数据训练的DRL应用铺平了道路。随后，在2016年，Silver等人[[182](#bib.bib182)]开发了AlphaGo，这是第一个击败围棋世界冠军的程序。与棋类程序中常见的手工规则不同，AlphaGo由使用监督学习（SL）和RL的神经网络组成。仅一年后，这一成就被Silver等人[[183](#bib.bib183)]的AlphaGo
    Zero程序所超越，AlphaGo Zero完全基于RL，省略了对人类数据的需求。最近的研究也在不完美信息游戏中取得了成功，与围棋和Atari游戏不同，这些游戏只允许代理观察系统的一部分。在OpenAI
    Five[[184](#bib.bib184)]中，代理能够击败世界电子竞技冠军在Dota2游戏中的表现，而AlphaStar[[185](#bib.bib185)]在复杂的实时战略游戏《StarCraft
    II》中达到了最高排名之一。[[46](#bib.bib46), [50](#bib.bib50), [76](#bib.bib76), [65](#bib.bib65)]进一步考察了DRL算法在Atari游戏中的扩展、并行化和探索能力。最后，Shao等人[[180](#bib.bib180)]对DRL在视频游戏中的应用进行了广泛的调查。
- en: Robotics is another domain which forms a prominent testbed for DRL algorithms
    [[186](#bib.bib186), [187](#bib.bib187)]. DRL can provide robots with navigation,
    obstacle avoidance and decision making capabilities, by mapping sensory data directly
    to actual motor commands [[188](#bib.bib188), [189](#bib.bib189)]. In some cases
    this has enabled robots to learn complex movements such as jumping or walking
     [[190](#bib.bib190), [191](#bib.bib191)]. Tai et al. [[192](#bib.bib192)] proposed
    a mapless motion planner which relies on training in simulation, after which,
    physical agents were able to navigate unknown static environments without fine-tuning.
    While most works involved simulation, Gu et al. [[161](#bib.bib161)] showed that
    DRL can be used to learn complex robotics 3D manipulation skills from scratch
    on real-world robots and further reduced training time by parallelizing the training
    across multiple robots. Haarnoja et al. [[191](#bib.bib191)] demonstrated that
    using DRL, one can also achieve stable quadrupedal locomotion on a physical robot
    within a reasonable time without prior training. For an in-depth review of the
    use of DRL for robot manipulation, we refer the interested reader to [[186](#bib.bib186),
    [187](#bib.bib187)].
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人技术是另一个重要的测试平台，用于检验DRL算法[[186](#bib.bib186), [187](#bib.bib187)]。DRL可以通过将传感数据直接映射到实际的运动指令来赋予机器人导航、避障和决策能力[[188](#bib.bib188),
    [189](#bib.bib189)]。在某些情况下，这使得机器人能够学习复杂的动作，如跳跃或行走[[190](#bib.bib190), [191](#bib.bib191)]。Tai等人[[192](#bib.bib192)]提出了一种无地图运动规划器，该规划器依赖于模拟训练，经过训练后，物理代理能够在无需精细调整的情况下导航未知的静态环境。虽然大多数工作涉及模拟，但Gu等人[[161](#bib.bib161)]展示了DRL可以用于从头开始在真实机器人上学习复杂的机器人3D操作技能，并通过在多个机器人上并行化训练进一步减少训练时间。Haarnoja等人[[191](#bib.bib191)]证明，通过使用DRL，可以在合理的时间内在物理机器人上实现稳定的四足运动，而无需事先训练。有关DRL在机器人操作中应用的深入评审，请参见[[186](#bib.bib186),
    [187](#bib.bib187)]。
- en: Finance. DRL also finds applications in trading [[193](#bib.bib193), [194](#bib.bib194)]
    and investment management [[195](#bib.bib195)], including cryptocurrency [[196](#bib.bib196)].
    Moody and Saffell [[197](#bib.bib197)] built a DRL agent for stock trading using
    raw financial data as the DNN input. Carapuço et al. [[198](#bib.bib198)] described
    a system for short-term speculation in the foreign exchange market, based on DRL.
    Wu et al. [[199](#bib.bib199)] proposed adaptive stock trading strategies leveraging
    DRL. A more recent DRL work by Lei et al. [[200](#bib.bib200)], adaptively selects
    between historical data and the changing trend of a stock, depending on the current
    state.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 金融领域。DRL 同样在交易[[193](#bib.bib193), [194](#bib.bib194)]和投资管理[[195](#bib.bib195)]中有应用，包括加密货币[[196](#bib.bib196)]。Moody
    和 Saffell [[197](#bib.bib197)] 使用原始金融数据作为深度神经网络（DNN）输入，构建了一个用于股票交易的 DRL 代理。Carapuço
    等人 [[198](#bib.bib198)] 描述了一个基于 DRL 的外汇市场短期投机系统。Wu 等人 [[199](#bib.bib199)] 提出了利用
    DRL 的自适应股票交易策略。Lei 等人 [[200](#bib.bib200)] 最近的 DRL 研究根据当前状态自适应地在历史数据和股票的变化趋势之间进行选择。
- en: Communications. Upcoming networks such as the 5G network, emphasize the need
    for efficient dynamic and large-scale solutions [[201](#bib.bib201)]. DRL has
    been emerging as an effective tool to tackle various problems and challenges within
    the field of networking [[202](#bib.bib202)]. For example, Wang et al. [[203](#bib.bib203)]
    applied a DQN to automatically optimize data transmission and reception in a multi-wireless-channel
    access problem. Ye and Li [[204](#bib.bib204)] developed a similar system for
    vehicle-to-vehicle communication. The optimal transmission bitrate can change
    over time. DRL can dynamically optimize the bitrate based on the quality of the
    last segment, the current buffer state [[205](#bib.bib205), [206](#bib.bib206)]and
    other channel statistics [[207](#bib.bib207), [208](#bib.bib208)]. Proactive caching
    can greatly reduce the number of transmissions over the network. However, deciding
    which content to cache is not trivial. Researchers have used DQNs to determine
    which information to keep in a cache based on observations of the channel state
    [[209](#bib.bib209)], cache state [[210](#bib.bib210)], request history [[211](#bib.bib211),
    [212](#bib.bib212)] and available base stations [[213](#bib.bib213), [214](#bib.bib214),
    [215](#bib.bib215)].
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 通信领域。即将到来的 5G 网络强调了对高效动态和大规模解决方案的需求[[201](#bib.bib201)]。DRL 正逐渐成为应对网络领域各种问题和挑战的有效工具[[202](#bib.bib202)]。例如，Wang
    等人 [[203](#bib.bib203)] 应用 DQN 自动优化多无线信道访问问题中的数据传输和接收。Ye 和 Li [[204](#bib.bib204)]
    开发了一个类似的车对车通信系统。最佳传输比特率可能随时间变化。DRL 可以根据上一个数据段的质量、当前缓冲状态[[205](#bib.bib205), [206](#bib.bib206)]和其他信道统计信息[[207](#bib.bib207),
    [208](#bib.bib208)]动态优化比特率。主动缓存可以大大减少网络中的传输次数。然而，决定缓存哪些内容并不简单。研究人员使用 DQN 基于对信道状态[[209](#bib.bib209)]、缓存状态[[210](#bib.bib210)]、请求历史[[211](#bib.bib211),
    [212](#bib.bib212)]和可用基站[[213](#bib.bib213), [214](#bib.bib214), [215](#bib.bib215)]的观察来确定应该保留哪些信息。
- en: Energy. Within the energy sector, smart grids make intelligent decisions with
    respect to electricity generation, transmission, distribution, consumption and
    control. DRL has been used in a variety of settings to tackle electric power system
    decision and control problems [[216](#bib.bib216)], such as in the context of
    microgrids [[217](#bib.bib217)] or buildings energy optimization [[218](#bib.bib218),
    [219](#bib.bib219)].
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 能源领域。在能源部门，智能电网在电力生成、传输、分配、消费和控制方面做出智能决策。DRL 已在各种环境中用于解决电力系统决策和控制问题[[216](#bib.bib216)]，例如在微电网[[217](#bib.bib217)]或建筑能效优化[[218](#bib.bib218),
    [219](#bib.bib219)]的背景下。
- en: Transportation. Congestion, safety and efficiency are important aspects of transportation.
    DRL is often used for adaptive traffic signal control to reduce waiting times
    [[220](#bib.bib220), [221](#bib.bib221), [222](#bib.bib222)]. Chen et al. [[223](#bib.bib223)]
    expanded upon this and conceived the first DRL control system which scales to
    thousands of traffic lights. Wang and Sun [[224](#bib.bib224)] developed a MADRL
    framework to prevent ‘bus bunching’ and streamline the flow of public transport.
    Manchella et al. [[225](#bib.bib225)] proposed a model-free DRL algorithm which
    packs ride-sharing passengers together with goods delivery to optimize fleet utilization
    and fuel efficiency.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 交通运输。拥堵、安全性和效率是交通运输的重要方面。DRL（深度强化学习）通常用于自适应交通信号控制，以减少等待时间[[220](#bib.bib220),
    [221](#bib.bib221), [222](#bib.bib222)]。陈等人[[223](#bib.bib223)]在此基础上扩展了第一个可扩展到数千个交通信号灯的DRL控制系统。王和孙[[224](#bib.bib224)]开发了一个MADRL框架，以防止“公交车集结”，并优化公共交通流量。曼切拉等人[[225](#bib.bib225)]提出了一种无模型的DRL算法，将共乘乘客与货物配送捆绑在一起，以优化车队利用率和燃油效率。
- en: VI-B Evolution Strategy applications
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 进化策略应用
- en: ESs applications are categorized and highlighted next.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ES应用将被分类和突出展示。
- en: Gaming. similarly to DRL, gaming represents one of the main testbeds for ESs.
    Most of the literature on ESs reviewed in this survey test their algorithms on
    Atari games [[4](#bib.bib4), [226](#bib.bib226), [5](#bib.bib5), [227](#bib.bib227),
    [228](#bib.bib228), [32](#bib.bib32), [229](#bib.bib229), [29](#bib.bib29)]. These
    are considered to be challenging as they present the agents with high dimensional
    visual inputs and a diverse and interesting set of tasks that were designed to
    be difficult for humans players [[14](#bib.bib14)].
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏。类似于DRL（深度强化学习），游戏是ES的主要测试平台之一。本调查中回顾的ES文献大多在Atari游戏上测试其算法[[4](#bib.bib4),
    [226](#bib.bib226), [5](#bib.bib5), [227](#bib.bib227), [228](#bib.bib228), [32](#bib.bib32),
    [229](#bib.bib229), [29](#bib.bib29)]。这些游戏被认为具有挑战性，因为它们为智能体提供了高维视觉输入和一组多样化且有趣的任务，这些任务被设计为对人类玩家来说很困难[[14](#bib.bib14)]。
- en: Robotics. The ability of ESs to continuously control actuators has been leverage
    in controlling simulated and real robotic systems [[4](#bib.bib4), [58](#bib.bib58),
    [80](#bib.bib80), [86](#bib.bib86), [230](#bib.bib230), [231](#bib.bib231), [232](#bib.bib232)].
    Hu et al. [[233](#bib.bib233)] used CMA-ES to make a robot learns how to grasp
    object under uncertainty. Uchitane et al. [[234](#bib.bib234)] augmented the ($\mu+\lambda$)-ES
    algorithm with a mask operation during the mutation step to tune the controller’s
    parameters of humanoid robots. With help of ESs Li et al. [[235](#bib.bib235)]
    design an indoor mobile robot navigation using monocular vision.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人技术。ES（进化策略）在模拟和实际机器人系统的连续控制中发挥了作用[[4](#bib.bib4), [58](#bib.bib58), [80](#bib.bib80),
    [86](#bib.bib86), [230](#bib.bib230), [231](#bib.bib231), [232](#bib.bib232)]。胡等人[[233](#bib.bib233)]使用CMA-ES使机器人在不确定性下学习如何抓取物体。内田和等人[[234](#bib.bib234)]在变异步骤中通过掩膜操作扩展了（$\mu+\lambda$）-ES算法，以调整类人机器人的控制器参数。在ES的帮助下，李等人[[235](#bib.bib235)]设计了一个使用单目视觉的室内移动机器人导航系统。
- en: Finance. Korczak and Lipinski [[236](#bib.bib236)] presented a portfolio optimization
    algorithm using ESs. Rimcharoen et al. [[237](#bib.bib237)], Sutheebanjard and
    Premchaiswadi [[238](#bib.bib238)] proposed the Adaptive and (1+1)-ES methods
    for predicting the Stock Exchange of Thailand index movement. Bonde and Khaled
    [[239](#bib.bib239)] predicted the changes (increase or decrease) of stock prices
    for different companies using ESs and Genetic Algorithms. Pai and Michel [[240](#bib.bib240)]
    proposed ESs with hall of fame (ES-HOF) for optimizing long–short portfolios with
    the 130-30-strategy-based constraint. Pai and Michel [[241](#bib.bib241)] used
    multi-objective ESs for futures portfolio optimization. Yu [[242](#bib.bib242)]
    proposed an ESs method for the multi-asset multi-period portfolio optimization.
    Sable et al. [[243](#bib.bib243)] proposed an ESs approach for predicting the
    short time prices of stocks. Sorensen et al. [[244](#bib.bib244)] applied meta-learning
    algorithms to ES for stock trading.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 财务。Korczak和Lipinski [[236](#bib.bib236)] 提出了一个使用ESs的投资组合优化算法。Rimcharoen等人 [[237](#bib.bib237)]，Sutheebanjard和Premchaiswadi
    [[238](#bib.bib238)] 提出了自适应和（1+1）-ES方法来预测泰国证券交易所指数的变动。Bonde和Khaled [[239](#bib.bib239)]
    使用ESs和遗传算法预测不同公司的股票价格变化（增加或减少）。Pai和Michel [[240](#bib.bib240)] 提出了具有名人堂（ES-HOF）的ESs，用于优化带有130-30策略约束的长短期投资组合。Pai和Michel
    [[241](#bib.bib241)] 使用多目标ESs进行期货投资组合优化。Yu [[242](#bib.bib242)] 提出了用于多资产多周期投资组合优化的ESs方法。Sable等人
    [[243](#bib.bib243)] 提出了用于预测股票短期价格的ESs方法。Sorensen等人 [[244](#bib.bib244)] 将元学习算法应用于ES以进行股票交易。
- en: Communications. Different methods are proposed throughout the literature that
    used ESs for communication. Pérez-Pérez et al. [[245](#bib.bib245)] used ESs with
    NSGAII (ESN) to approximate the Pareto frontier of the mobile adhoc network (MANETs).
    Krulikovska et al. [[246](#bib.bib246)] used ESs for the routing of multipoint
    connections. Additionally, they proposed methods for improving ESs. Nissen and
    Gold [[247](#bib.bib247)] used ESs for designing a survivable network while taking
    economics and reliability into consideration. He et al. [[248](#bib.bib248)] analyzed
    the data characteristics of wireless sensor network (WSN), and proposed a method
    for fault diagnosis of WSN based on a belief rule base (BRB) model which is optimized
    using CMA-ES.Srivastava and Singh [[249](#bib.bib249)] used ESs for solving the
    the total rotation minimization problem (TRMP) in directional sensor networks.
    Srivastava et al. [[250](#bib.bib250)] presented an ESs method for solving the
    Cover scheduling problem in wireless sensor networks (WSN-CSP). Gu and Potkonjak
    [[251](#bib.bib251)] proposed an ESs method to search for a network configuration
    able to produce and stabilize responses of a Physical Unclonable Functions (PUFs).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 通信。文献中提出了多种使用ESs进行通信的方法。Pérez-Pérez等人 [[245](#bib.bib245)] 使用带有NSGAII（ESN）的ESs来逼近移动自组织网络（MANETs）的Pareto前沿。Krulikovska等人
    [[246](#bib.bib246)] 使用ESs进行多点连接的路由。此外，他们还提出了改进ESs的方法。Nissen和Gold [[247](#bib.bib247)]
    使用ESs设计一个在考虑经济性和可靠性的情况下具有生存能力的网络。He等人 [[248](#bib.bib248)] 分析了无线传感器网络（WSN）的数据特征，并提出了一种基于信念规则库（BRB）模型的WSN故障诊断方法，该模型使用CMA-ES进行优化。Srivastava和Singh
    [[249](#bib.bib249)] 使用ESs解决方向传感器网络中的总旋转最小化问题（TRMP）。Srivastava等人 [[250](#bib.bib250)]
    提出了一个ESs方法来解决无线传感器网络（WSN-CSP）中的覆盖调度问题。Gu和Potkonjak [[251](#bib.bib251)] 提出了一个ESs方法，用于搜索能够产生和稳定物理不可克隆函数（PUFs）响应的网络配置。
- en: Energy. ESs have been used to optimize many energy-related systems. For example,
    Mendoza et al. [[252](#bib.bib252)] used ESs to select optimal size of feeders
    in radial power distribution system. Lezama et al. [[253](#bib.bib253)] used differential
    ESs for large-scale energy resource management in smart grids. Coelho et al. [[254](#bib.bib254)]
    used it for energy load forecasting in electric grids. Versloot et al. [[255](#bib.bib255)]
    optimized near-field wireless power transfer using ESs.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 能源。ESs已被用于优化许多与能源相关的系统。例如，Mendoza等人 [[252](#bib.bib252)] 使用ESs选择径向电力分配系统中馈线的最佳尺寸。Lezama等人
    [[253](#bib.bib253)] 使用差分ESs进行智能电网的大规模能源资源管理。Coelho等人 [[254](#bib.bib254)] 用于电网中的能源负荷预测。Versloot等人
    [[255](#bib.bib255)] 使用ESs优化近场无线电力传输。
- en: Transportation. A number of papers that use ESs for managing and optimizing
    vehicle traffic have been proposed. Balaji et al. [[256](#bib.bib256)] proposed
    a multi-agent-based real-time centralized evolutionary optimization technique
    for urban traffic management in the area of traffic signal control. Mester and
    Bräysy [[257](#bib.bib257)] combined ESs with guided local search to tackle large-scale
    vehicle routing problems with time windows. Mester and Bräysy [[257](#bib.bib257)]
    simulated and optimized traffic flow with help of ESs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 交通。已有一些论文使用 ESs 来管理和优化车辆交通。Balaji 等人 [[256](#bib.bib256)] 提出了一个基于多代理的实时集中进化优化技术，用于城市交通管理中的交通信号控制。Mester
    和 Bräysy [[257](#bib.bib257)] 将 ESs 与引导局部搜索结合，以解决具有时间窗的大规模车辆路线问题。Mester 和 Bräysy
    [[257](#bib.bib257)] 利用 ESs 模拟和优化了交通流量。
- en: VI-C Comparison
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 比较
- en: Next we list our observations about this section.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们列出关于本节的观察结果。
- en: •
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both DRL and ES algorithms have found adoption in many domains such as robotics,
    games, and finance.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL 和 ES 算法已在许多领域找到应用，如机器人技术、游戏和金融。
- en: •
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL-based solutions seem to excel in situations that require scalable and adaptive
    behavior.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 DRL 的解决方案在需要可扩展和自适应行为的情况下似乎表现优异。
- en: •
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DRL receives much more attention than ESs within the scientific (Table [VIII](#S6.T8
    "TABLE VIII ‣ VI-A Deep Reinforcement Learning applications ‣ VI Applications
    ‣ Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey")).
    We suspect that two of the main reasons for this gap are (i) DRL has a richer
    structure; therefore, it naturally allows for more research, and (ii) there are
    similar algorithmic families to ESs (e.g., genetic algorithms) which may result
    in reduced focus on ESs.'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '与进化策略（ESs）相比，深度强化学习（DRL）在科学领域获得了更多关注（见表 [VIII](#S6.T8 "TABLE VIII ‣ VI-A Deep
    Reinforcement Learning applications ‣ VI Applications ‣ Deep Reinforcement Learning
    Versus Evolution Strategies: A Comparative Survey")）。我们怀疑造成这一差距的两个主要原因是（i）DRL
    具有更丰富的结构，因此自然允许更多的研究，和（ii）有类似的算法家族（例如遗传算法）可能导致对 ESs 的关注减少。'
- en: •
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Many great-performing DRL and ES algorithms are benchmarked in simulated environments.
    Consequently, their performance in real-world applications are still questionable.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多表现优异的 DRL 和 ES 算法是在模拟环境中进行基准测试的。因此，它们在实际应用中的表现仍然值得怀疑。
- en: VII Challenges and future research directions
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 挑战与未来研究方向
- en: Although DRL and ESs have proven their worth in many AI fields, there are still
    many challenges to be addressed. We briefly list some of them in the sequel.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DRL 和 ESs 在许多 AI 领域证明了其价值，但仍然存在许多挑战需要解决。我们在下文中简要列出其中的一些。
- en: Sample Efficiency. DRL agents require a large number of samples (i.e., interactions
    with environments) to learn good-performing policies. Collecting so many samples
    is not always feasible due to either computational reasons or because the quantity
    of interactions with the environment is limited. Although this problem has been
    tackled in different ways (e.g., transfer learning, meta-learning), more innovation
    and research are still needed [[258](#bib.bib258), [259](#bib.bib259)]. One promising
    research direction to tackle this problem is model-based RL. However, getting
    an accurate model of the environment is usually hard.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率。DRL 代理需要大量样本（即与环境的交互）才能学习出性能良好的策略。由于计算原因或与环境的交互数量有限，收集如此多的样本并不总是可行的。尽管这个问题已经通过不同的方法得到解决（例如迁移学习、元学习），但仍需要更多的创新和研究
    [[258](#bib.bib258), [259](#bib.bib259)]。一个有前景的研究方向是基于模型的强化学习。然而，获得准确的环境模型通常很困难。
- en: ESs can provide more robust policies as compared to DRL; however, they are even
    less sample efficient, as they work with full-length episodes [[260](#bib.bib260),
    [261](#bib.bib261)], and they do not use any type of memory [[260](#bib.bib260)].
    Approaches to improve sample efficiency in ESs such sample resue and importance
    mixing [[262](#bib.bib262), [260](#bib.bib260)] have been proposed; however, more
    research and innovation are still required.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于 DRL，ESs 能提供更稳健的策略；然而，它们的样本效率更低，因为它们处理的是完整的回合 [[260](#bib.bib260), [261](#bib.bib261)]，且不使用任何类型的记忆
    [[260](#bib.bib260)]。已有一些方法（如样本重用和重要性混合 [[262](#bib.bib262), [260](#bib.bib260)]）提出以提高
    ESs 的样本效率，但仍需更多研究和创新。
- en: Exploration versus exploitation. The exploration versus exploitation dilemma
    is one of the most prominent problems in RL. Beyond classical balancing approaches
    such as $\varepsilon$-greedy [[1](#bib.bib1)], Upper Confidence Bound (UCB) [[59](#bib.bib59)],
    and Thompson Sampling [[60](#bib.bib60)], recent breakthroughs enable the exploration
    of novel environments. For example, Osband et al. [[68](#bib.bib68)] observed
    the importance of temporal correlation and proposed the bootstrapped DQN; and
    Bellemare et al. [[65](#bib.bib65)] used density models to scale UCB to problems
    with high-dimensional input data. In spite of that, exploring complex environments
    is still a very active field of research.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用。探索与利用的困境是强化学习中最突出的难题之一。除了经典的平衡方法，如$\varepsilon$-贪婪[[1](#bib.bib1)]、上置信界（UCB）[[59](#bib.bib59)]和汤普森采样[[60](#bib.bib60)]，近期的突破使得对新环境的探索成为可能。例如，Osband等人[[68](#bib.bib68)]观察到时间相关性的重要性，并提出了引导式DQN；Bellemare等人[[65](#bib.bib65)]使用密度模型将UCB扩展到高维输入数据的问题。尽管如此，探索复杂环境仍然是一个非常活跃的研究领域。
- en: ESs realize exploration through the recombination and mutation steps. Despite
    their effectiveness in exploration, ESs may still get trapped in local optima [[58](#bib.bib58),
    [80](#bib.bib80)]. Proposals have been made to enhance ESs exploration capabilities
    [[82](#bib.bib82), [83](#bib.bib83)]; however, more work in this direction is
    needed. In general, DRL and ESs are proposed to tackle ever more novel environments,
    and consequently, the exploration versus exploitation dilemma still poses a challenge
    that requires innovation.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ES通过重组和突变步骤实现探索。尽管它们在探索中的有效性，ES仍可能陷入局部最优[[58](#bib.bib58), [80](#bib.bib80)]。已有提议旨在增强ES的探索能力[[82](#bib.bib82),
    [83](#bib.bib83)]；然而，这方面仍需更多工作。总体而言，DRL和ES被提议用于应对越来越多的新环境，因此，探索与利用的困境仍然是一个需要创新的挑战。
- en: Sparse reward. A reward signal guides the learning process of an RL agent. When
    this signal is sparse learning becomes much harder. Although, different solutions
    have been introduced (e.g., reward shaping [[1](#bib.bib1)], curiosity-driven
    methods [[73](#bib.bib73)], curriculum learning [[263](#bib.bib263)], hierarchical
    learning [[264](#bib.bib264)] and inverse RL [[265](#bib.bib265)]), learning with
    sparse rewards still represents an open challenge.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏奖励。奖励信号指导RL代理的学习过程。当这种信号稀疏时，学习变得更加困难。虽然已经提出了不同的解决方案（例如，奖励塑造[[1](#bib.bib1)]、好奇心驱动的方法[[73](#bib.bib73)]、课程学习[[263](#bib.bib263)]、层级学习[[264](#bib.bib264)]和逆向RL[[265](#bib.bib265)]），稀疏奖励的学习仍然是一个未解的挑战。
- en: Direct the exploration of ES algorithms to counter the sparsity and/or deceptiveness
    of an RL task is one of the most important challenges to scale ESs to more complex
    environments and make them more efficient. A detailed summary of the challenges
    related to ESs, such as differential evolution and swarm optimization, is presented
    in [[21](#bib.bib21)].
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 直接指导ES算法以应对RL任务的稀疏性和/或欺骗性是将ES扩展到更复杂环境并提高其效率的重要挑战之一。有关ES的挑战（如差分进化和群体优化）的详细总结见[[21](#bib.bib21)]。
- en: 'Simulation-to-reality gap. Despite the benefits of simulations, they give rise
    to the sim-to-real gap: policies that are learned in simulations often do not
    work as expected in the real world. Different techniques are being adapted to
    mitigate the effect of this gap. For example, [[266](#bib.bib266), [267](#bib.bib267)]
    randomized the simulated environment to produce more generalized models. Rao et al.
    [[268](#bib.bib268)] noted that such randomization requires manually specifying
    which aspects of the simulator to randomize. Therefore, they used domain adaptation
    (i.e., many simulated examples and a few real ones) to train a robot on grasping
    tasks without manually instrumenting the simulator. Despite such efforts, the
    sim-to-real gap is still an open challenge to be addressed.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真与现实差距。尽管仿真有其好处，但它们产生了仿真与现实差距：在仿真中学习的策略在现实世界中往往无法按预期工作。不同的技术正在被调整以缓解这种差距的影响。例如，[[266](#bib.bib266),
    [267](#bib.bib267)]通过随机化仿真环境来生成更通用的模型。Rao等人[[268](#bib.bib268)]指出，这种随机化需要手动指定仿真器的哪些方面进行随机化。因此，他们使用领域适应（即许多模拟示例和少量真实示例）来训练机器人完成抓取任务，而无需手动调整仿真器。尽管有这些努力，仿真与现实差距仍然是一个亟待解决的挑战。
- en: VIII Conclusion
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: Deep Reinforcement Learning (DRL) and Evolution Strategies (ESs) have the same
    objective but make use of different mechanisms for learning sequential decision-making
    tasks. In this paper, we provided the necessary background of DRL and ESs in order
    to understand their relative strengths and weaknesses, which may lead to developing
    an algorithmic family that is superior to each one of them. Instead of focusing
    on individual algorithms, we considered major learning aspects such as parallelism,
    exploration, meta-learning, and multi-agent learning. We believe that hybridizing
    DRL and ESs has a high potential to drive the development of agents that operate
    reliably and efficiently in the real world.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）和进化策略（ESs）有相同的目标，但使用不同的机制来学习顺序决策任务。本文提供了 DRL 和 ESs 的必要背景，以便理解它们的相对优缺点，这可能会导致开发出比它们各自更优的算法家族。我们没有集中于单个算法，而是考虑了诸如并行性、探索、元学习和多智能体学习等主要学习方面。我们相信，混合
    DRL 和 ESs 有很高的潜力推动在现实世界中可靠且高效运行的智能体的发展。
- en: Acknowledgment
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been undertaken in the Internet of Swarms project sponsored by
    Cognizant Technology Solutions and Rijksdienst voor Ondernemend Nederland under
    PPS O&I.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作是在 Cognizant Technology Solutions 和 Rijksdienst voor Ondernemend Nederland
    在 PPS O&I 下赞助的 Internet of Swarms 项目中进行的。
- en: References
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton and Barto [2018] R. S. Sutton and A. G. Barto, *Reinforcement learning:
    An introduction*.   MIT press, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto [2018] R. S. Sutton 和 A. G. Barto, *强化学习：导论*。MIT press，2018年。
- en: François-Lavet et al. [2018] V. François-Lavet, P. Henderson, R. Islam, M. G.
    Bellemare, and J. Pineau, “An introduction to deep reinforcement learning,” *arXiv
    preprint arXiv:1811.12560*, 2018.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet 等 [2018] V. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare,
    和 J. Pineau, “深度强化学习导论，” *arXiv 预印本 arXiv:1811.12560*，2018年。
- en: Eiben and Smith [2003] A. E. Eiben and J. E. Smith, *Introduction to evolutionary
    computing*.   Springer, 2003.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eiben 和 Smith [2003] A. E. Eiben 和 J. E. Smith, *进化计算导论*。Springer，2003年。
- en: Salimans et al. [2017] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever,
    “Evolution strategies as a scalable alternative to reinforcement learning,” 2017.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等 [2017] T. Salimans, J. Ho, X. Chen, S. Sidor, 和 I. Sutskever, “进化策略作为强化学习的可扩展替代方案，”
    2017年。
- en: 'Chrabaszcz et al. [2018a] P. Chrabaszcz, I. Loshchilov, and F. Hutter, “Back
    to basics: Benchmarking canonical evolution strategies for playing atari,” 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chrabaszcz 等 [2018a] P. Chrabaszcz, I. Loshchilov, 和 F. Hutter, “回归基础：为玩 Atari
    游戏进行经典进化策略基准测试，” 2018年。
- en: 'Qian and Yu [2021] H. Qian and Y. Yu, “Derivative-free reinforcement learning:
    A review,” *Frontiers of Computer Science (electronic)*, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 和 Yu [2021] H. Qian 和 Y. Yu, “无导数强化学习：综述，” *计算机科学前沿（电子版）*，2021年。
- en: 'Kiran et al. [2021] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A.
    Sallab, S. Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous
    driving: A survey,” *IEEE Transactions on Intelligent Transportation Systems*,
    2021.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiran 等 [2021] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab,
    S. Yogamani, 和 P. Pérez, “用于自主驾驶的深度强化学习：综述，” *IEEE 智能交通系统期刊*，2021年。
- en: 'Nguyen et al. [2020] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
    learning for multiagent systems: A review of challenges, solutions, and applications,”
    *IEEE transactions on cybernetics*, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等 [2020] T. T. Nguyen, N. D. Nguyen, 和 S. Nahavandi, “多智能体系统的深度强化学习：挑战、解决方案和应用的综述，”
    *IEEE 网络系统学报*，2020年。
- en: 'Bertsekas [2011] D. P. Bertsekas, “Dynamic programming and optimal control
    3rd edition, volume ii,” *Belmont, MA: Athena Scientific*, 2011.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bertsekas [2011] D. P. Bertsekas, “动态规划与最优控制第3版，第2卷，” *Belmont, MA: Athena
    Scientific*，2011年。'
- en: Sutton et al. [1999] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour
    *et al.*, “Policy gradient methods for reinforcement learning with function approximation.”
    in *NIPs*.   Citeseer, 1999.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等 [1999] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour *等*，“用于强化学习的策略梯度方法与函数逼近。”
    见 *NIPs*。Citeseer，1999年。
- en: Williams [1992] R. J. Williams, “Simple statistical gradient-following algorithms
    for connectionist reinforcement learning,” *Machine learning*, 1992.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams [1992] R. J. Williams, “简单的统计梯度跟随算法用于连接主义强化学习，” *机器学习*，1992年。
- en: BELLMAN [1957] R. BELLMAN, “A markovian decision process,” *Journal of Mathematics
    and Mechanics*, 1957.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BELLMAN [1957] R. BELLMAN, “马尔可夫决策过程，” *数学与力学期刊*，1957年。
- en: Rummery and Niranjan [1994] G. Rummery and M. Niranjan, “On-line q-learning
    using connectionist systems,” *Technical Report CUED/F-INFENG/TR 166*, 1994.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rummery 和 Niranjan [1994] G. Rummery 和 M. Niranjan, “利用连接主义系统的在线 q 学习，” *技术报告
    CUED/F-INFENG/TR 166*，1994年。
- en: Mnih et al. [2013] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
    D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement learning,”
    2013.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2013] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
    D. Wierstra 和 M. Riedmiller，“利用深度强化学习玩 Atari 游戏，”2013年。
- en: Konda and Tsitsiklis [2000] V. R. Konda and J. N. Tsitsiklis, “Actor-critic
    algorithms,” in *Advances in neural information processing systems*, 2000.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda 和 Tsitsiklis [2000] V. R. Konda 和 J. N. Tsitsiklis，“演员-评论家算法，”发表于*神经信息处理系统进展*，2000年。
- en: Kaiser et al. [2019] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H.
    Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine *et al.*,
    “Model-based reinforcement learning for atari,” *arXiv preprint arXiv:1903.00374*,
    2019.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser 等 [2019] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell,
    K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine *等*，“基于模型的强化学习用于 Atari
    游戏，”*arXiv 预印本 arXiv:1903.00374*，2019年。
- en: Silver et al. [2016a] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
    J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel, and D. Hassabis, “Mastering the game of go with deep neural networks
    and tree search,” *Nature*, 2016.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等 [2016a] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D.
    Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel 和 D. Hassabis，“利用深度神经网络和树搜索掌握围棋游戏，”*自然*，2016年。
- en: 'Polydoros and Nalpantidis [2017] A. S. Polydoros and L. Nalpantidis, “Survey
    of model-based reinforcement learning: Applications on robotics,” *Journal of
    Intelligent & Robotic Systems*, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polydoros 和 Nalpantidis [2017] A. S. Polydoros 和 L. Nalpantidis，“基于模型的强化学习综述：在机器人领域的应用，”*智能与机器人系统期刊*，2017年。
- en: Lillicrap et al. [2019] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
    learning,” 2019.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap 等 [2019] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver 和 D. Wierstra，“深度强化学习的连续控制，”2019年。
- en: Hansen et al. [2015] N. Hansen, D. V. Arnold, and A. Auger, *Evolution Strategies*.   Springer,
    2015.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 等 [2015] N. Hansen, D. V. Arnold 和 A. Auger，*进化策略*。 Springer，2015年。
- en: 'Li et al. [2020a] Z. Li, X. Lin, Q. Zhang, and H. Liu, “Evolution strategies
    for continuous optimization: A survey of the state-of-the-art,” *Swarm and Evolutionary
    Computation*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2020a] Z. Li, X. Lin, Q. Zhang 和 H. Liu，“连续优化的进化策略：最新进展的综述，”*群体与进化计算*，2020年。
- en: Heidrich-Meisner and Igel [2008] V. Heidrich-Meisner and C. Igel, “Similarities
    and differences between policy gradient methods and evolution strategies,” in
    *ESANN 2008*, 2008.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidrich-Meisner 和 Igel [2008] V. Heidrich-Meisner 和 C. Igel，“政策梯度方法与进化策略的相似性和差异，”发表于*ESANN
    2008*，2008年。
- en: Heidrich-Meisner and Igel [2008] V. Heidrich-Meisner and C. Igel, “Evolution
    strategies for direct policy search,” in *International Conference on Parallel
    Problem Solving from Nature*, 2008.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidrich-Meisner 和 Igel [2008] V. Heidrich-Meisner 和 C. Igel，“用于直接政策搜索的进化策略，”发表于*国际自然问题解决大会*，2008年。
- en: Heidrich-Meisner [2009] C. Heidrich-Meisner, Verena and Igel, “Neuroevolution
    strategies for episodic reinforcement learning,” *Journal of Algorithms*, 2009.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidrich-Meisner [2009] C. Heidrich-Meisner, Verena 和 Igel，“用于情节强化学习的神经进化策略，”*算法期刊*，2009年。
- en: Heidrich-Meisner and Igel [2009] V. Heidrich-Meisner and C. Igel, “Hoeffding
    and bernstein races for selecting policies in evolutionary direct policy search,”
    in *Proceedings of the 26th Annual International Conference on Machine Learning*,
    2009.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidrich-Meisner 和 Igel [2009] V. Heidrich-Meisner 和 C. Igel，“Hoeffding 和 Bernstein
    竞赛在进化直接政策搜索中的应用，”发表于*第26届年度国际机器学习会议论文集*，2009年。
- en: 'Chrabaszcz et al. [2018b] P. Chrabaszcz, I. Loshchilov, and F. Hutter, “Back
    to basics: Benchmarking canonical evolution strategies for playing atari,” *CoRR*,
    2018. [Online]. Available: http://arxiv.org/abs/1802.08842'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chrabaszcz 等 [2018b] P. Chrabaszcz, I. Loschilov 和 F. Hutter，“回归基础：基准测试经典进化策略玩
    Atari 游戏，”*CoRR*，2018年。[在线]。可用链接：http://arxiv.org/abs/1802.08842
- en: Watkins and Dayan [1992] C. J. Watkins and P. Dayan, “Q-learning,” *Machine
    learning*, 1992.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins 和 Dayan [1992] C. J. Watkins 和 P. Dayan，“Q-learning，”*机器学习*，1992年。
- en: Mnih et al. [2015a] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness,
    M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
    and D. Hassabis, “Human-level control through deep reinforcement learning,” *Nature*,
    2015.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2015a] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare,
    A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, 和 D. Hassabis，“通过深度强化学习实现人类水平的控制”，*自然*，2015。
- en: Fuks et al. [2019] L. Fuks, N. Awad, F. Hutter, and M. Lindauer, “An evolution
    strategy with progressive episode lengths for playing games,” in *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI-19*.   International Joint Conferences on Artificial Intelligence Organization,
    2019.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fuks 等 [2019] L. Fuks, N. Awad, F. Hutter, 和 M. Lindauer，“一种用于玩游戏的具有渐进式回合长度的进化策略”，在*第二十八届国际人工智能联合会议论文集,
    IJCAI-19*。国际人工智能联合会议组织，2019。
- en: Varelas et al. [2018] K. Varelas, A. Auger, D. Brockhoff, N. Hansen, O. A. ElHara,
    Y. Semet, R. Kassab, and F. Barbaresco, “A comparative study of large-scale variants
    of cma-es,” in *International Conference on Parallel Problem Solving from Nature*,
    2018.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varelas 等 [2018] K. Varelas, A. Auger, D. Brockhoff, N. Hansen, O. A. ElHara,
    Y. Semet, R. Kassab, 和 F. Barbaresco，“大规模 CMA-ES 变体的比较研究”，在*国际自然问题求解大会*，2018。
- en: Salimans et al. [2016] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
    and X. Chen, “Improved techniques for training gans,” 2016.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等 [2016] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
    和 X. Chen，“改进的 GAN 训练技术”，2016。
- en: Conti et al. [2018] E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley,
    and J. Clune, “Improving exploration in evolution strategies for deep reinforcement
    learning via a population of novelty-seeking agents,” 2018.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conti 等 [2018] E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley,
    和 J. Clune，“通过一组寻求新奇的代理改进深度强化学习中的进化策略探索”，2018。
- en: Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet
    classification with deep convolutional neural networks,” *Advances in neural information
    processing systems*, 2012.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 [2012] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络的
    Imagenet 分类”，*神经信息处理系统进展*，2012。
- en: Tsitsiklis and Van Roy [1997] J. N. Tsitsiklis and B. Van Roy, “An analysis
    of temporal-difference learning with function approximation,” *IEEE transactions
    on automatic control*, 1997.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsitsiklis 和 Van Roy [1997] J. N. Tsitsiklis 和 B. Van Roy，“带有函数逼近的时间差分学习分析”，*IEEE
    自动控制学报*，1997。
- en: Lin [1993] L.-J. Lin, “Reinforcement learning for robots using neural networks,”
    Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, Tech. Rep., 1993.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [1993] L.-J. Lin，“使用神经网络的机器人强化学习”，卡内基梅隆大学匹兹堡计算机科学学院，技术报告，1993。
- en: 'Rechenberg [1973] I. Rechenberg, *Evolutionsstrategie Optimierung technischer
    Systeme nach Prinzipien der biologischen Evolution*.   Stuttgart-Bad Cannstatt:
    Friedrich Frommann Verlag, 1973.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rechenberg [1973] I. Rechenberg，*进化策略：根据生物进化原则优化技术系统*。斯图加特-巴德坎斯塔特：Friedrich
    Frommann Verlag，1973。
- en: Slowik and Kwasnicka [2020] A. Slowik and H. Kwasnicka, “Evolutionary algorithms
    and their applications to engineering problems,” *Neural Computing and Applications*,
    2020.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slowik 和 Kwasnicka [2020] A. Slowik 和 H. Kwasnicka，“进化算法及其在工程问题中的应用”，*神经计算与应用*，2020。
- en: Dianati et al. [2002] M. Dianati, I. Song, and M. Treiber, “An introduction
    to genetic algorithms and evolution strategies,” Citeseer, Tech. Rep., 2002.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dianati 等 [2002] M. Dianati, I. Song, 和 M. Treiber，“遗传算法和进化策略介绍”，Citeseer，技术报告，2002。
- en: Schwefel [1977] H.-P. Schwefel, *Numerische Optimierung von Computer-Modellen
    mittels der Evolutionsstrategie*.   Birkh  "a user, 1977.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwefel [1977] H.-P. Schwefel，*计算机模型的数值优化：进化策略方法*。Birkh  "a user, 1977。
- en: 'Hansen and Ostermeier [1996] N. Hansen and A. Ostermeier, “Adapting arbitrary
    normal mutation distributions in evolution strategies: The covariance matrix adaptation,”
    in *Proceedings of IEEE international conference on evolutionary computation*,
    1996.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 和 Ostermeier [1996] N. Hansen 和 A. Ostermeier，“在进化策略中适应任意正态突变分布：协方差矩阵适应”，在*IEEE
    国际进化计算会议论文集*，1996。
- en: Hansen and Ostermeier [2001] N. Hansen and A. Ostermeier, “Completely derandomized
    self-adaptation in evolution strategies,” *Evolutionary computation*, 2001.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 和 Ostermeier [2001] N. Hansen 和 A. Ostermeier，“在进化策略中完全去随机化的自适应”，*进化计算*，2001。
- en: Hansen et al. [2003] N. Hansen, S. D. Müller, and P. Koumoutsakos, “Reducing
    the time complexity of the derandomized evolution strategy with covariance matrix
    adaptation (cma-es),” *Evolutionary computation*, 2003.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 等 [2003] N. Hansen, S. D. Müller 和 P. Koumoutsakos, “通过协方差矩阵适应（cma-es）减少去随机化进化策略的时间复杂度，”
    *进化计算*，2003。
- en: 'Hansen [2016] N. Hansen, “The cma evolution strategy: A tutorial,” 2016.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen [2016] N. Hansen, “cma进化策略：教程，” 2016。
- en: Wierstra et al. [2014] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters,
    and J. Schmidhuber, “Natural evolution strategies,” *The Journal of Machine Learning
    Research*, 2014.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wierstra 等 [2014] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters
    和 J. Schmidhuber, “自然进化策略，” *机器学习研究杂志*，2014。
- en: 'François-Lavet et al. [2015] V. François-Lavet, R. Fonteneau, and D. Ernst,
    “How to discount deep reinforcement learning: Towards new dynamic strategies,”
    *arXiv preprint arXiv:1512.02011*, 2015.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet 等 [2015] V. François-Lavet, R. Fonteneau 和 D. Ernst, “如何折扣深度强化学习：迈向新的动态策略，”
    *arXiv 预印本 arXiv:1512.02011*，2015。
- en: Nair et al. [2015] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon,
    A. D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg,
    V. Mnih, K. Kavukcuoglu, and D. Silver, “Massively parallel methods for deep reinforcement
    learning,” 2015.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等 [2015] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A.
    D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V.
    Mnih, K. Kavukcuoglu 和 D. Silver, “深度强化学习的海量并行方法，” 2015。
- en: Mnih et al. [2016] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap,
    T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement
    learning,” 2016.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2016] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T.
    Harley, D. Silver 和 K. Kavukcuoglu, “深度强化学习的异步方法，” 2016。
- en: Clemente et al. [2017] A. V. Clemente, H. N. Castejón, and A. Chandra, “Efficient
    parallel methods for deep reinforcement learning,” 2017.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clemente 等 [2017] A. V. Clemente, H. N. Castejón 和 A. Chandra, “高效的并行深度强化学习方法，”
    2017。
- en: Horgan et al. [2018] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel,
    H. van Hasselt, and D. Silver, “Distributed prioritized experience replay,” 2018.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horgan 等 [2018] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H.
    van Hasselt 和 D. Silver, “分布式优先经验回放，” 2018。
- en: 'Espeholt et al. [2018] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih,
    T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu,
    “Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures,”
    2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Espeholt 等 [2018] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T.
    Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg 和 K. Kavukcuoglu, “Impala:
    可扩展的分布式深度强化学习与重要性加权的演员-学习者架构，” 2018。'
- en: 'Espeholt et al. [2020] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, and
    M. Michalski, “Seed rl: Scalable and efficient deep-rl with accelerated central
    inference,” 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Espeholt 等 [2020] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang 和 M. Michalski,
    “Seed rl: 可扩展且高效的深度强化学习与加速的中央推断，” 2020。'
- en: Grounds and Kudenko [2005] M. Grounds and D. Kudenko, “Parallel reinforcement
    learning with linear function approximation,” in *Adaptive Agents and Multi-Agent
    Systems III. Adaptation and Multi-Agent Learning*.   Springer, 2005.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grounds 和 Kudenko [2005] M. Grounds 和 D. Kudenko, “具有线性函数逼近的并行强化学习，” 载于 *自适应代理和多代理系统
    III：适应和多代理学习*。Springer, 2005。
- en: Dean et al. [2012] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V.
    Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng, “Large
    scale distributed deep networks,” in *NIPS*, 2012.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dean 等 [2012] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le,
    M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang 和 A. Y. Ng, “大规模分布式深度网络，”
    载于 *NIPS*, 2012。
- en: Babaeizadeh et al. [2017] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and
    J. Kautz, “Reinforcement learning through asynchronous advantage actor-critic
    on a gpu,” 2017.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babaeizadeh 等 [2017] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons 和 J. Kautz,
    “通过异步优势演员-评论员在 GPU 上进行强化学习，” 2017。
- en: Heess et al. [2017] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne,
    Y. Tassa, T. Erez, Z. Wang, S. Eslami *et al.*, “Emergence of locomotion behaviours
    in rich environments,” *arXiv preprint arXiv:1707.02286*, 2017.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等 [2017] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y.
    Tassa, T. Erez, Z. Wang, S. Eslami *等*，“丰富环境中的运动行为的出现，” *arXiv 预印本 arXiv:1707.02286*，2017。
- en: Schulman et al. [2017] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
    O. Klimov, “Proximal policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*,
    2017.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等 [2017] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov,
    “近端策略优化算法，” *arXiv 预印本 arXiv:1707.06347*，2017。
- en: Kapturowski et al. [2018] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and
    W. Dabney, “Recurrent experience replay in distributed reinforcement learning,”
    in *International conference on learning representations*, 2018.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski 等人 [2018] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, 和 W.
    Dabney，“分布式强化学习中的递归经验重放，”在 *国际学习表征会议*，2018。
- en: Liu et al. [2019] G. Liu, L. Zhao, F. Yang, J. Bian, T. Qin, N. Yu, and T. Liu,
    “Trust region evolution strategies,” in *AAAI*, 2019.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2019] G. Liu, L. Zhao, F. Yang, J. Bian, T. Qin, N. Yu, 和 T. Liu，“信任区域演化策略，”在
    *AAAI*，2019。
- en: Auer et al. [2002] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis
    of the multiarmed bandit problem,” *Machine learning*, 2002.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer 等人 [2002] P. Auer, N. Cesa-Bianchi, 和 P. Fischer，“多臂老虎机问题的有限时间分析，” *机器学习*，2002。
- en: Russo et al. [2017] D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen,
    “A tutorial on thompson sampling,” *arXiv preprint arXiv:1707.02038*, 2017.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russo 等人 [2017] D. Russo, B. Van Roy, A. Kazerouni, I. Osband, 和 Z. Wen，“Thompson
    采样教程，” *arXiv 预印本 arXiv:1707.02038*，2017。
- en: Achiam and Sastry [2017] J. Achiam and S. Sastry, “Surprise-based intrinsic
    motivation for deep reinforcement learning,” 2017.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 和 Sastry [2017] J. Achiam 和 S. Sastry，“基于惊讶的深度强化学习内在动机，”2017。
- en: Pathak et al. [2019] D. Pathak, D. Gandhi, and A. Gupta, “Self-supervised exploration
    via disagreement,” 2019.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak 等人 [2019] D. Pathak, D. Gandhi, 和 A. Gupta，“通过争议进行自监督探索，”2019。
- en: Shyam et al. [2019] P. Shyam, W. Jaśkowski, and F. Gomez, “Model-based active
    exploration,” 2019.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shyam 等人 [2019] P. Shyam, W. Jaśkowski, 和 F. Gomez，“基于模型的主动探索，”2019。
- en: 'Kim et al. [2019] H. Kim, J. Kim, Y. Jeong, S. Levine, and H. O. Song, “Emi:
    Exploration with mutual information,” 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2019] H. Kim, J. Kim, Y. Jeong, S. Levine, 和 H. O. Song，“Emi：通过互信息进行探索，”2019。
- en: Bellemare et al. [2016] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul,
    D. Saxton, and R. Munos, “Unifying count-based exploration and intrinsic motivation,”
    2016.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人 [2016] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul,
    D. Saxton, 和 R. Munos，“统一基于计数的探索和内在动机，”2016。
- en: Ostrovski et al. [2017] G. Ostrovski, M. G. Bellemare, A. van den Oord, and
    R. Munos, “Count-based exploration with neural density models,” 2017.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ostrovski 等人 [2017] G. Ostrovski, M. G. Bellemare, A. van den Oord, 和 R. Munos，“使用神经密度模型的基于计数的探索，”2017。
- en: 'Tang et al. [2017] H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan,
    J. Schulman, F. De Turck, and P. Abbeel, “# exploration: A study of count-based
    exploration for deep reinforcement learning,” in *31st Conference on Neural Information
    Processing Systems (NIPS)*, 2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2017] H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan,
    J. Schulman, F. De Turck, 和 P. Abbeel，“# 探索：基于计数的深度强化学习探索研究，”在 *第31届神经信息处理系统会议（NIPS）*，2017。
- en: Osband et al. [2016] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy, “Deep
    exploration via bootstrapped dqn,” *arXiv preprint arXiv:1602.04621*, 2016.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 等人 [2016] I. Osband, C. Blundell, A. Pritzel, 和 B. Van Roy，“通过自助 DQN
    的深度探索，” *arXiv 预印本 arXiv:1602.04621*，2016。
- en: Chen et al. [2017a] R. Y. Chen, J. Schulman, P. Abbeel, and S. Sidor, “Ucb and
    infogain exploration via q-ensembles,” *arXiv preprint arXiv:1706.01502*, 2017.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2017a] R. Y. Chen, J. Schulman, P. Abbeel, 和 S. Sidor，“通过 Q-集成进行 UCB
    和信息增益探索，” *arXiv 预印本 arXiv:1706.01502*，2017。
- en: O’Donoghue et al. [2018] B. O’Donoghue, I. Osband, R. Munos, and V. Mnih, “The
    uncertainty bellman equation and exploration,” in *International Conference on
    Machine Learning*, 2018.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Donoghue 等人 [2018] B. O’Donoghue, I. Osband, R. Munos, 和 V. Mnih，“不确定性贝尔曼方程与探索，”在
    *国际机器学习会议*，2018。
- en: Schulman et al. [2015] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz,
    “Trust region policy optimization,” in *International conference on machine learning*.   PMLR,
    2015.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 [2015] J. Schulman, S. Levine, P. Abbeel, M. Jordan, 和 P. Moritz，“信任区域策略优化，”在
    *国际机器学习会议*。 PMLR，2015。
- en: 'Houthooft et al. [2017] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. D.
    Turck, and P. Abbeel, “Vime: Variational information maximizing exploration,”
    2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等人 [2017] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. D. Turck,
    和 P. Abbeel，“Vime：变分信息最大化探索，”2017。
- en: Pathak et al. [2017] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven
    exploration by self-supervised prediction,” 2017.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak 等人 [2017] D. Pathak, P. Agrawal, A. A. Efros, 和 T. Darrell，“通过自监督预测驱动的好奇心探索，”2017。
- en: Savinov et al. [2018] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys,
    T. Lillicrap, and S. Gelly, “Episodic curiosity through reachability,” *arXiv
    preprint arXiv:1810.02274*, 2018.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Savinov 等人 [2018] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys,
    T. Lillicrap, 和 S. Gelly，“通过可达性进行情节好奇心，” *arXiv 预印本 arXiv:1810.02274*，2018。
- en: 'Ecoffet et al. [2020] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and
    J. Clune, “Go-explore: a new approach for hard-exploration problems,” 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ecoffet 等 [2020] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley 和 J. Clune,
    “Go-explore: 一种应对困难探索问题的新方法，” 2020。'
- en: 'Badia et al. [2020a] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot,
    S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt *et al.*, “Never
    give up: Learning directed exploration strategies,” *arXiv preprint arXiv:2002.06038*,
    2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badia 等 [2020a] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot,
    S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt *等*，“永不放弃：学习导向探索策略，”
    *arXiv 预印本 arXiv:2002.06038*，2020。
- en: 'Badia et al. [2020b] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi,
    D. Guo, and C. Blundell, “Agent57: Outperforming the atari human benchmark,” 2020.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Badia 等 [2020b] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi,
    D. Guo 和 C. Blundell, “Agent57: 超越 Atari 人类基准，” 2020。'
- en: Schmidhuber [2010] J. Schmidhuber, “Formal theory of creativity, fun, and intrinsic
    motivation (1990–2010),” *IEEE Transactions on Autonomous Mental Development*,
    2010.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber [2010] J. Schmidhuber, “创造力、乐趣和内在动机的形式理论 (1990–2010)，” *IEEE 自动化智能发展期刊*，2010。
- en: Burda et al. [2018] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration
    by random network distillation,” 2018.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burda 等 [2018] Y. Burda, H. Edwards, A. Storkey 和 O. Klimov, “通过随机网络蒸馏进行探索，”
    2018。
- en: Zhang et al. [2020a] J. Zhang, H. Tran, and G. Zhang, “Accelerating reinforcement
    learning with a directional-gaussian-smoothing evolution strategy,” 2020.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2020a] J. Zhang, H. Tran 和 G. Zhang, “使用方向高斯平滑演化策略加速强化学习，” 2020。
- en: Geweke [1988] J. Geweke, “Antithetic acceleration of monte carlo integration
    in bayesian inference,” *Journal of Econometrics*, 1988.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geweke [1988] J. Geweke, “贝叶斯推断中蒙特卡罗积分的反向加速，” *计量经济学期刊*，1988。
- en: Choromanski et al. [2018] K. Choromanski, M. Rowland, V. Sindhwani, R. Turner,
    and A. Weller, “Structured evolution with compact architectures for scalable policy
    optimization,” 2018.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等 [2018] K. Choromanski, M. Rowland, V. Sindhwani, R. Turner 和 A.
    Weller, “具有紧凑架构的结构化演化用于可扩展策略优化，” 2018。
- en: 'Maheswaranathan et al. [2019] N. Maheswaranathan, L. Metz, G. Tucker, D. Choi,
    and J. Sohl-Dickstein, “Guided evolutionary strategies: Augmenting random search
    with surrogate gradients,” 2019.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maheswaranathan 等 [2019] N. Maheswaranathan, L. Metz, G. Tucker, D. Choi 和 J.
    Sohl-Dickstein, “引导演化策略：用代理梯度增强随机搜索，” 2019。
- en: Meier et al. [2019] F. Meier, A. Mujika, M. M. Gauy, and A. Steger, “Improving
    gradient estimation in evolutionary strategies with past descent directions,”
    2019.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meier 等 [2019] F. Meier, A. Mujika, M. M. Gauy 和 A. Steger, “通过过去的下降方向改进演化策略中的梯度估计，”
    2019。
- en: 'Choromanski et al. [2019] K. Choromanski, A. Pacchiano, J. Parker-Holder, and
    Y. Tang, “From complexity to simplicity: Adaptive es-active subspaces for blackbox
    optimization,” 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等 [2019] K. Choromanski, A. Pacchiano, J. Parker-Holder 和 Y. Tang,
    “从复杂性到简约：适应性 es-active 子空间用于黑箱优化，” 2019。
- en: Liu et al. [2020] F.-Y. Liu, Z.-N. Li, and C. Qian, “Self-guided evolution strategies
    with historical estimated gradients,” in *Proceedings of the Twenty-Ninth International
    Joint Conference on Artificial Intelligence, IJCAI-20*, 2020.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2020] F.-Y. Liu, Z.-N. Li 和 C. Qian, “具有历史估计梯度的自引导演化策略，” 在 *第二十九届国际人工智能联合会议
    IJCAI-20 会议录*，2020。
- en: Zhang et al. [2020b] J. Zhang, H. Tran, D. Lu, and G. Zhang, “A novel evolution
    strategy with directional gaussian smoothing for blackbox optimization,” 2020.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2020b] J. Zhang, H. Tran, D. Lu 和 G. Zhang, “一种具有方向高斯平滑的黑箱优化新演化策略，”
    2020。
- en: Lehman and Stanley [2011] J. Lehman and K. Stanley, *Novelty Search and the
    Problem with Objectives*.   Springer, 11 2011.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehman 和 Stanley [2011] J. Lehman 和 K. Stanley, *新颖性搜索与目标问题*。Springer, 11 2011。
- en: 'Pugh et al. [2016] J. Pugh, L. Soros, and K. Stanley, “Quality diversity: A
    new frontier for evolutionary computation,” *Frontiers in Robotics and AI*, 2016.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pugh 等 [2016] J. Pugh, L. Soros 和 K. Stanley, “质量多样性：演化计算的新前沿，” *机器人与人工智能前沿*，2016。
- en: McCallum [1997] R. McCallum, “Reinforcement learning with selective perception
    and hidden state,” 1997.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallum [1997] R. McCallum, “具有选择性感知和隐状态的强化学习，” 1997。
- en: Ortner et al. [2014] R. Ortner, O.-A. Maillard, and D. Ryabko, “Selecting near-optimal
    approximate state representations in reinforcement learning,” in *International
    Conference on Algorithmic Learning Theory*, 2014.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ortner 等 [2014] R. Ortner, O.-A. Maillard 和 D. Ryabko, “在强化学习中选择近似最优状态表示，” 在
    *国际算法学习理论会议*，2014。
- en: François-Lavet et al. [2019] V. François-Lavet, G. Rabusseau, J. Pineau, D. Ernst,
    and R. Fonteneau, “On overfitting and asymptotic bias in batch reinforcement learning
    with partial observability,” *Journal of Artificial Intelligence Research*, 2019.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet et al. [2019] V. François-Lavet, G. Rabusseau, J. Pineau, D.
    Ernst 和 R. Fonteneau，《关于批量强化学习中的过拟合和渐近偏差（部分可观测性）》，*人工智能研究杂志*，2019年。
- en: Hausknecht and Stone [2015] M. Hausknecht and P. Stone, “Deep recurrent q-learning
    for partially observable mdps,” *arXiv preprint arXiv:1507.06527*, 2015.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausknecht and Stone [2015] M. Hausknecht 和 P. Stone，《用于部分可观测 MDPs 的深度递归 Q 学习》，*arXiv
    预印本 arXiv:1507.06527*，2015年。
- en: Igel [2003] C. Igel, “Neuroevolution for reinforcement learning using evolution
    strategies,” in *The 2003 Congress on Evolutionary Computation, 2003. CEC’03.*,
    2003.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igel [2003] C. Igel，《使用进化策略的强化学习中的神经进化》，发表于*2003年进化计算大会，2003. CEC'03*，2003年。
- en: Schaul and Schmidhuber [2010] T. Schaul and J. Schmidhuber, “Metalearning,”
    *Scholarpedia*, 2010.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul and Schmidhuber [2010] T. Schaul 和 J. Schmidhuber，《元学习》，*Scholarpedia*，2010年。
- en: Gupta et al. [2018] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine,
    “Meta-reinforcement learning of structured exploration strategies,” 2018.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. [2018] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel 和 S. Levine，《结构化探索策略的元强化学习》，2018年。
- en: Mishra et al. [2017] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, “A simple
    neural attentive meta-learner,” 2017.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra et al. [2017] N. Mishra, M. Rohaninejad, X. Chen 和 P. Abbeel，《简单的神经注意力元学习者》，2017年。
- en: Nagabandi et al. [2019] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel,
    S. Levine, and C. Finn, “Learning to adapt in dynamic, real-world environments
    through meta-reinforcement learning,” 2019.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi et al. [2019] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P.
    Abbeel, S. Levine 和 C. Finn，《通过元强化学习在动态现实环境中学习适应》，2019年。
- en: Garcia and Thomas [2019] F. M. Garcia and P. S. Thomas, “A meta-mdp approach
    to exploration for lifelong reinforcement learning,” 2019.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia and Thomas [2019] F. M. Garcia 和 P. S. Thomas，《元-MDP 方法用于终身强化学习的探索》，2019年。
- en: Wang et al. [2017] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z.
    Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, “Learning to reinforcement
    learn,” 2017.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2017] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z.
    Leibo, R. Munos, C. Blundell, D. Kumaran 和 M. Botvinick，《学习强化学习》，2017年。
- en: 'Duan et al. [2016] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever,
    and P. Abbeel, “Rl²: Fast reinforcement learning via slow reinforcement learning,”
    2016.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2016] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever
    和 P. Abbeel，《Rl²：通过慢强化学习实现快速强化学习》，2016年。
- en: Finn et al. [2017] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning
    for fast adaptation of deep networks,” 2017.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. [2017] C. Finn, P. Abbeel 和 S. Levine，《模型无关的元学习用于深度网络的快速适应》，2017年。
- en: 'Li et al. [2017] Z. Li, F. Zhou, F. Chen, and H. Li, “Meta-sgd: Learning to
    learn quickly for few-shot learning,” 2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2017] Z. Li, F. Zhou, F. Chen 和 H. Li，《Meta-SGD：快速学习以实现少量样本学习》，2017年。
- en: Hochreiter et al. [2001] S. Hochreiter, A. S. Younger, and P. R. Conwell, “Learning
    to learn using gradient descent,” in *International Conference on Artificial Neural
    Networks*.   Springer, 2001.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter et al. [2001] S. Hochreiter, A. S. Younger 和 P. R. Conwell，《使用梯度下降学习学习》，发表于*国际人工神经网络会议*。施普林格，2001年。
- en: Santoro et al. [2016] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
    T. Lillicrap, “One-shot learning with memory-augmented neural networks,” 2016.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santoro et al. [2016] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra 和 T.
    Lillicrap，《具有记忆增强神经网络的单次学习》，2016年。
- en: Botvinick et al. [2019] M. Botvinick, S. Ritter, J. Wang, Z. Kurth-Nelson, C. Blundell,
    and D. Hassabis, “Reinforcement learning, fast and slow,” *Trends in Cognitive
    Sciences*, 2019.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botvinick et al. [2019] M. Botvinick, S. Ritter, J. Wang, Z. Kurth-Nelson, C.
    Blundell 和 D. Hassabis，《强化学习，快与慢》，*认知科学趋势*，2019年。
- en: Robles and Vanschoren [2019] J. G. Robles and J. Vanschoren, “Learning to reinforcement
    learn for neural architecture search,” 2019.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robles and Vanschoren [2019] J. G. Robles 和 J. Vanschoren，《用于神经架构搜索的强化学习》，2019年。
- en: Huisman et al. [2020] M. Huisman, J. N. van Rijn, and A. Plaat, “A survey of
    deep meta-learning,” 2020.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huisman et al. [2020] M. Huisman, J. N. van Rijn 和 A. Plaat，《深度元学习的综述》，2020年。
- en: Nichol et al. [2018] A. Nichol, J. Achiam, and J. Schulman, “On first-order
    meta-learning algorithms,” 2018.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol et al. [2018] A. Nichol, J. Achiam 和 J. Schulman，《关于一阶元学习算法》，2018年。
- en: 'Gajewski et al. [2019] A. Gajewski, J. Clune, K. O. Stanley, and J. Lehman,
    “Evolvability es: Scalable and direct optimization of evolvability,” 2019.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gajewski et al. [2019] A. Gajewski, J. Clune, K. O. Stanley 和 J. Lehman，《可进化性
    ES：可扩展和直接优化可进化性》，2019年。
- en: 'Mengistu et al. [2016] H. Mengistu, J. Lehman, and J. Clune, “Evolvability
    search: directly selecting for evolvability in order to study and produce it,”
    in *Proceedings of the Genetic and Evolutionary Computation Conference 2016*,
    2016.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mengistu 等人 [2016] H. Mengistu, J. Lehman, 和 J. Clune, “可演化性搜索：直接选择可演化性以进行研究和生产,”
    收录于 *2016年遗传与进化计算会议论文集*, 2016。
- en: 'Katona et al. [2021] A. Katona, D. W. Franks, and J. A. Walker, “Quality evolvability
    es: Evolving individuals with a distribution of well performing and diverse offspring,”
    2021.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katona 等人 [2021] A. Katona, D. W. Franks, 和 J. A. Walker, “质量可演化性：进化具有分布良好的表现和多样化后代的个体,”
    2021。
- en: 'Hospedales et al. [2020] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey,
    “Meta-learning in neural networks: A survey,” 2020.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hospedales 等人 [2020] T. Hospedales, A. Antoniou, P. Micaelli, 和 A. Storkey,
    “神经网络中的元学习：调查,” 2020。
- en: 'Song et al. [2020a] X. Song, W. Gao, Y. Yang, K. Choromanski, A. Pacchiano,
    and Y. Tang, “Es-maml: Simple hessian-free meta learning,” 2020.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2020a] X. Song, W. Gao, Y. Yang, K. Choromanski, A. Pacchiano, 和 Y.
    Tang, “Es-maml：简单的无海森元学习,” 2020。
- en: Song et al. [2020b] X. Song, Y. Yang, K. Choromanski, K. Caluwaerts, W. Gao,
    C. Finn, and J. Tan, “Rapidly adaptable legged robots via evolutionary meta-learning,”
    2020.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2020b] X. Song, Y. Yang, K. Choromanski, K. Caluwaerts, W. Gao, C.
    Finn, 和 J. Tan, “通过进化元学习的快速适应四足机器人,” 2020。
- en: Wang et al. [2020a] Z. Wang, C. Chen, and D. Dong, “Instance weighted incremental
    evolution strategies for reinforcement learning in dynamic environments,” 2020.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020a] Z. Wang, C. Chen, 和 D. Dong, “动态环境下强化学习的实例加权增量进化策略,” 2020。
- en: 'Tan et al. [2018] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner,
    S. Bohez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for quadruped
    robots,” 2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人 [2018] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S.
    Bohez, 和 V. Vanhoucke, “从模拟到实际：学习四足机器人灵活的运动,” 2018。
- en: Arndt et al. [2019] K. Arndt, M. Hazara, A. Ghadirzadeh, and V. Kyrki, “Meta
    reinforcement learning for sim-to-real domain adaptation,” 2019.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arndt 等人 [2019] K. Arndt, M. Hazara, A. Ghadirzadeh, 和 V. Kyrki, “用于模拟到实际领域适应的元强化学习,”
    2019。
- en: Nowe et al. [2012] A. Nowe, P. Vrancx, and Y.-M. De Hauwere, *Game Theory and
    Multi-agent Reinforcement Learning*, 2012.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nowe 等人 [2012] A. Nowe, P. Vrancx, 和 Y.-M. De Hauwere, *博弈论与多智能体强化学习*, 2012。
- en: Hu et al. [2020] J. Hu, H. Niu, J. Carrasco, B. Lennox, and F. Arvin, “Voronoi-based
    multi-robot autonomous exploration in unknown environments via deep reinforcement
    learning,” *IEEE Transactions on Vehicular Technology*, 2020.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2020] J. Hu, H. Niu, J. Carrasco, B. Lennox, 和 F. Arvin, “基于Voronoi的多机器人自主探索未知环境，通过深度强化学习,”
    *IEEE车辆技术学报*, 2020。
- en: Shapley [1953] L. S. Shapley, “Stochastic games,” *Proceedings of the national
    academy of sciences*, 1953.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley [1953] L. S. Shapley, “随机博弈,” *国家科学院学报*, 1953。
- en: 'Panait and Luke [2005] L. Panait and S. Luke, “Cooperative multi-agent learning:
    The state of the art,” *Autonomous agents and multi-agent systems*, vol. 11, no. 3,
    pp. 387–434, 2005.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panait 和 Luke [2005] L. Panait 和 S. Luke, “合作多智能体学习：现状,” *自主智能体与多智能体系统*, 第 11
    卷，第 3 期，页 387–434, 2005。
- en: 'Zhang et al. [2019] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement
    learning: A selective overview of theories and algorithms,” 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2019] K. Zhang, Z. Yang, 和 T. Başar, “多智能体强化学习：理论和算法的选择性概述,” 2019。
- en: 'Sartoretti et al. [2019] G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. K. S.
    Kumar, S. Koenig, and H. Choset, “Primal: Pathfinding via reinforcement and imitation
    multi-agent learning,” *IEEE Robotics and Automation Letters*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sartoretti 等人 [2019] G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. K. S. Kumar,
    S. Koenig, 和 H. Choset, “Primal：通过强化和模仿多智能体学习进行路径规划,” *IEEE机器人与自动化快报*, 2019。
- en: Chen [2019a] G. Chen, “A new framework for multi-agent reinforcement learning
    – centralized training and exploration with decentralized execution via policy
    distillation,” 2019.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen [2019a] G. Chen, “用于多智能体强化学习的新框架——通过策略蒸馏的集中训练与探索和去中心化执行,” 2019。
- en: 'Lee et al. [2020] D. Lee, N. He, P. Kamalaruban, and V. Cevher, “Optimization
    for reinforcement learning: From a single agent to cooperative agents,” *IEEE
    Signal Processing Magazine*, 2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2020] D. Lee, N. He, P. Kamalaruban, 和 V. Cevher, “强化学习的优化：从单一智能体到合作智能体,”
    *IEEE信号处理杂志*, 2020。
- en: 'Hernandez-Leal et al. [2017] P. Hernandez-Leal, M. Kaisers, T. Baarslag, and
    E. M. de Cote, “A survey of learning in multiagent environments: Dealing with
    non-stationarity,” *arXiv preprint arXiv:1707.09183*, 2017.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez-Leal 等人 [2017] P. Hernandez-Leal, M. Kaisers, T. Baarslag, 和 E. M.
    de Cote, “多智能体环境中的学习调查：应对非平稳性,” *arXiv预印本 arXiv:1707.09183*, 2017。
- en: 'Tan [1993] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative
    agents,” in *Proceedings of the tenth international conference on machine learning*,
    1993.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan [1993] M. Tan, “多智能体强化学习：独立智能体与合作智能体，” 载于 *第十届国际机器学习会议论文集*，1993年。
- en: 'Castaneda [2016] A. O. Castaneda, “Deep reinforcement learning variants of
    multi-agent learning algorithms,” *Edinburgh: School of Informatics, University
    of Edinburgh*, 2016.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castaneda [2016] A. O. Castaneda, “深度强化学习的多智能体学习算法变体，” *爱丁堡：爱丁堡大学信息学学院*，2016年。
- en: Palmer et al. [2017] G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani, “Lenient
    multi-agent deep reinforcement learning,” *arXiv preprint arXiv:1707.04402*, 2017.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palmer et al. [2017] G. Palmer, K. Tuyls, D. Bloembergen, 和 R. Savani, “宽容的多智能体深度强化学习，”
    *arXiv预印本 arXiv:1707.04402*，2017年。
- en: Kartal et al. [2015] B. Kartal, J. Godoy, I. Karamouzas, and S. J. Guy, “Stochastic
    tree search with useful cycles for patrolling problems,” in *2015 IEEE International
    Conference on Robotics and Automation (ICRA)*, 2015.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kartal et al. [2015] B. Kartal, J. Godoy, I. Karamouzas, 和 S. J. Guy, “用于巡逻问题的带有有用循环的随机树搜索，”
    载于 *2015年IEEE国际机器人与自动化会议（ICRA）*，2015年。
- en: Hernandez-Leal et al. [2019] P. Hernandez-Leal, B. Kartal, and M. E. Taylor,
    “A survey and critique of multiagent deep reinforcement learning,” *Autonomous
    Agents and Multi-Agent Systems*, 2019.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez-Leal et al. [2019] P. Hernandez-Leal, B. Kartal, 和 M. E. Taylor, “多智能体深度强化学习的调查与批评，”
    *自主智能体与多智能体系统*，2019年。
- en: Yang and Wang [2020] Y. Yang and J. Wang, “An overview of multi-agent reinforcement
    learning from game theoretical perspective,” 2020.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang and Wang [2020] Y. Yang 和 J. Wang, “从博弈论视角的多智能体强化学习概述，” 2020年。
- en: Sunehag et al. [2017] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
    M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel,
    “Value-decomposition networks for cooperative multi-agent learning,” 2017.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sunehag et al. [2017] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V.
    Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, 和 T. Graepel,
    “用于合作性多智能体学习的值分解网络，” 2017年。
- en: 'Rashid et al. [2018] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster,
    and S. Whiteson, “Qmix: Monotonic value function factorisation for deep multi-agent
    reinforcement learning,” in *International Conference on Machine Learning*, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid et al. [2018] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J.
    Foerster, 和 S. Whiteson, “Qmix：深度多智能体强化学习的单调值函数分解，” 载于 *国际机器学习会议*，2018年。
- en: Lowe et al. [2017] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
    “Multi-agent actor-critic for mixed cooperative-competitive environments,” *arXiv
    preprint arXiv:1706.02275*, 2017.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe et al. [2017] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, 和 I. Mordatch,
    “用于混合合作-竞争环境的多智能体演员-评论家算法，” *arXiv预印本 arXiv:1706.02275*，2017年。
- en: Foerster et al. [2018] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and
    S. Whiteson, “Counterfactual multi-agent policy gradients,” 2018.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster et al. [2018] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, 和
    S. Whiteson, “反事实多智能体策略梯度，” 2018年。
- en: Chen [2019b] G. Chen, “A new framework for multi-agent reinforcement learning–centralized
    training and exploration with decentralized execution via policy distillation,”
    2019.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen [2019b] G. Chen, “一种用于多智能体强化学习的新框架——通过策略蒸馏进行集中训练与探索的分散执行，” 2019年。
- en: Lauer and Riedmiller [2000] M. Lauer and M. Riedmiller, “An algorithm for distributed
    reinforcement learning in cooperative multi-agent systems,” in *In Proceedings
    of the Seventeenth International Conference on Machine Learning*, 2000.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lauer and Riedmiller [2000] M. Lauer 和 M. Riedmiller, “合作性多智能体系统中分布式强化学习的算法，”
    载于 *第十七届国际机器学习会议论文集*，2000年。
- en: Foerster et al. [2017] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H.
    Torr, P. Kohli, and S. Whiteson, “Stabilising experience replay for deep multi-agent
    reinforcement learning,” in *International conference on machine learning*, 2017.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster et al. [2017] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P.
    H. Torr, P. Kohli, 和 S. Whiteson, “稳定深度多智能体强化学习的经验重放，” 载于 *国际机器学习会议*，2017年。
- en: OroojlooyJadid and Hajinezhad [2019] A. OroojlooyJadid and D. Hajinezhad, “A
    review of cooperative multi-agent deep reinforcement learning,” *arXiv preprint
    arXiv:1908.03963*, 2019.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OroojlooyJadid and Hajinezhad [2019] A. OroojlooyJadid 和 D. Hajinezhad, “合作性多智能体深度强化学习的综述，”
    *arXiv预印本 arXiv:1908.03963*，2019年。
- en: Chu and Ye [2017] X. Chu and H. Ye, “Parameter sharing deep deterministic policy
    gradient for cooperative multi-agent reinforcement learning,” *arXiv preprint
    arXiv:1710.00336*, 2017.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu and Ye [2017] X. Chu 和 H. Ye, “用于合作性多智能体强化学习的参数共享深度确定性策略梯度，” *arXiv预印本 arXiv:1710.00336*，2017年。
- en: Ryu et al. [2018] H. Ryu, H. Shin, and J. Park, “Multi-agent actor-critic with
    generative cooperative policy network,” *arXiv preprint arXiv:1810.09206*, 2018.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ryu 等人 [2018] H. Ryu、H. Shin 和 J. Park，“具有生成合作策略网络的多智能体演员-评论员”，*arXiv 预印本 arXiv:1810.09206*，2018。
- en: Mao et al. [2018] H. Mao, Z. Zhang, Z. Xiao, and Z. Gong, “Modelling the dynamic
    joint policy of teammates with attention multi-agent ddpg,” *arXiv preprint arXiv:1811.07029*,
    2018.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人 [2018] H. Mao、Z. Zhang、Z. Xiao 和 Z. Gong，“利用注意力机制的多智能体 DDPG 建模队友的动态联合策略”，*arXiv
    预印本 arXiv:1811.07029*，2018。
- en: Wang et al. [2020b] R. E. Wang, M. Everett, and J. P. How, “R-maddpg for partially
    observable environments and limited communication,” *arXiv preprint arXiv:2002.06684*,
    2020.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020b] R. E. Wang、M. Everett 和 J. P. How，“针对部分可观测环境和有限通信的 R-maddpg”，*arXiv
    预印本 arXiv:2002.06684*，2020。
- en: Kasai et al. [2008] T. Kasai, H. Tenmoto, and A. Kamiya, “Learning of communication
    codes in multi-agent reinforcement learning problem,” in *2008 IEEE Conference
    on Soft Computing in Industrial Applications*, 2008.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasai 等人 [2008] T. Kasai、H. Tenmoto 和 A. Kamiya，“多智能体强化学习问题中的沟通代码学习”，在 *2008
    IEEE 工业应用软计算会议*，2008。
- en: Giles and Jim [2002] C. L. Giles and K.-C. Jim, “Learning communication for
    multi-agent systems,” in *Workshop on Radical Agent Concepts*, 2002.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giles 和 Jim [2002] C. L. Giles 和 K.-C. Jim，“为多智能体系统学习沟通”，在 *激进代理概念研讨会*，2002。
- en: Foerster et al. [2016] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson,
    “Learning to communicate to solve riddles with deep distributed recurrent q-networks,”
    *arXiv preprint arXiv:1602.02672*, 2016.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster 等人 [2016] J. N. Foerster、Y. M. Assael、N. de Freitas 和 S. Whiteson，“通过深度分布式递归
    Q 网络学习解决谜题的沟通能力”，*arXiv 预印本 arXiv:1602.02672*，2016。
- en: Sukhbaatar et al. [2016] S. Sukhbaatar, A. Szlam, and R. Fergus, “Learning multiagent
    communication with backpropagation,” *arXiv preprint arXiv:1605.07736*, 2016.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等人 [2016] S. Sukhbaatar、A. Szlam 和 R. Fergus，“通过反向传播学习多智能体沟通”，*arXiv
    预印本 arXiv:1605.07736*，2016。
- en: Gupta et al. [2017] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative
    multi-agent control using deep reinforcement learning,” in *International Conference
    on Autonomous Agents and Multiagent Systems*, 2017.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 [2017] J. K. Gupta、M. Egorov 和 M. Kochenderfer，“使用深度强化学习的合作多智能体控制”，在
    *国际自主代理与多智能体系统会议*，2017。
- en: Da Silva and Costa [2019] F. L. Da Silva and A. H. R. Costa, “A survey on transfer
    learning for multiagent reinforcement learning systems,” *Journal of Artificial
    Intelligence Research*, 2019.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Da Silva 和 Costa [2019] F. L. Da Silva 和 A. H. R. Costa，“关于多智能体强化学习系统的迁移学习调查”，*人工智能研究期刊*，2019。
- en: 'Du and Ding [2020] W. Du and S. Ding, “A survey on multi-agent deep reinforcement
    learning: from the perspective of challenges and applications,” *Artificial Intelligence
    Review*, 2020.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 和 Ding [2020] W. Du 和 S. Ding，“关于多智能体深度强化学习的调查：从挑战与应用的角度”，*人工智能评论*，2020。
- en: Hiraga et al. [2018] M. Hiraga, Y. Wei, T. Yasuda, and K. Ohkura, “Evolving
    autonomous specialization in congested path formation task of robotic swarms,”
    *Artificial Life and Robotics*, 2018.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hiraga 等人 [2018] M. Hiraga、Y. Wei、T. Yasuda 和 K. Ohkura，“在拥堵路径形成任务中进化机器人群体的自主专长”，*人工生命与机器人*，2018。
- en: Hiraga et al. [2019] M. Hiraga, Y. Wei, and K. Ohkura, “Evolving collective
    cognition of robotic swarms in the foraging task with poison,” in *2019 IEEE Congress
    on Evolutionary Computation (CEC)*, 2019.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hiraga 等人 [2019] M. Hiraga、Y. Wei 和 K. Ohkura，“在有毒物质的觅食任务中进化机器人群体的集体认知”，在 *2019
    IEEE 进化计算大会 (CEC)*，2019。
- en: Tang et al. [2020] Y. Tang, J. Tan, and T. Harada, “Learning agile locomotion
    via adversarial training,” *arXiv preprint arXiv:2008.00603*, 2020.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2020] Y. Tang、J. Tan 和 T. Harada，“通过对抗训练学习灵活的运动”，*arXiv 预印本 arXiv:2008.00603*，2020。
- en: Chen and Gao [2020] J. Chen and Z. Gao, “A framework for learning predator-prey
    agents from simulation to real world,” 2020.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Gao [2020] J. Chen 和 Z. Gao，“从模拟到现实世界的捕食者-猎物智能体学习框架”，2020。
- en: Li et al. [2020b] G. Li, Q. Duan, and Y. Shi, *A Parallel Evolutionary Algorithm
    with Value Decomposition for Multi-agent Problems*, 2020.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020b] G. Li、Q. Duan 和 Y. Shi，*一种具有价值分解的多智能体问题并行进化算法*，2020。
- en: Rais Martínez and Aznar Gregori [2020] J. Rais Martínez and F. Aznar Gregori,
    “Comparison of evolutionary strategies for reinforcement learning in a swarm aggregation
    behaviour,” in *2020 The 3rd International Conference on Machine Learning and
    Machine Intelligence*, 2020.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rais Martínez 和 Aznar Gregori [2020] J. Rais Martínez 和 F. Aznar Gregori，“在群体聚合行为中的强化学习进化策略比较”，在
    *2020 第三届国际机器学习与机器智能会议*，2020。
- en: Fan et al. [2018] D. D. Fan, E. Theodorou, and J. Reeder, “Model-based stochastic
    search for large scale optimization of multi-agent uav swarms,” 2018.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 [2018] D. D. Fan, E. Theodorou, 和 J. Reeder，“基于模型的随机搜索用于大规模优化多智能体无人机群”，2018。
- en: Aznar et al. [2021] F. Aznar, M. Pujol, and R. Rizo, “Learning a swarm foraging
    behavior with microscopic fuzzy controllers using deep reinforcement learning,”
    *Applied Sciences*, 2021.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aznar 等人 [2021] F. Aznar, M. Pujol, 和 R. Rizo，“利用深度强化学习学习群体觅食行为的微观模糊控制器”，*《应用科学》*，2021。
- en: Gu et al. [2016] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement
    learning for robotic manipulation with asynchronous off-policy updates,” 2016.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人 [2016] S. Gu, E. Holly, T. Lillicrap, 和 S. Levine，“用于机器人操作的深度强化学习与异步离线策略更新”，2016。
- en: 'Pourchot and Sigaud [2019] A. Pourchot and O. Sigaud, “Cem-rl: Combining evolutionary
    and gradient-based methods for policy search,” 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pourchot 和 Sigaud [2019] A. Pourchot 和 O. Sigaud，“CEM-RL: 结合进化和基于梯度的方法进行政策搜索”，2019。'
- en: Fujimoto et al. [2018] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function
    approximation error in actor-critic methods,” in *International Conference on
    Machine Learning*, 2018.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujimoto 等人 [2018] S. Fujimoto, H. Hoof, 和 D. Meger，“解决演员-评论家方法中的函数逼近误差”，在 *《国际机器学习大会》*，2018。
- en: Hansel et al. [2021] K. Hansel, J. Moos, and C. Derstroff, *Benchmarking the
    Natural Gradient in Policy Gradient Methods and Evolution Strategies*.   Springer
    International Publishing, 2021.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansel 等人 [2021] K. Hansel, J. Moos, 和 C. Derstroff，*《在策略梯度方法和进化策略中的自然梯度基准测试》*。Springer
    International Publishing，2021。
- en: 'Ecoffet et al. [2021] P. Ecoffet, N. Fontbonne, J.-B. André, and N. Bredeche,
    “Policy search with rare significant events: Choosing the right partner to cooperate
    with,” 2021.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ecoffet 等人 [2021] P. Ecoffet, N. Fontbonne, J.-B. André, 和 N. Bredeche，“稀有重要事件的策略搜索：选择合适的合作伙伴”，2021。
- en: Stafylopatis and Blekas [1998] A. Stafylopatis and K. Blekas, “Autonomous vehicle
    navigation using evolutionary reinforcement learning,” *European Journal of Operational
    Research*, 1998.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stafylopatis 和 Blekas [1998] A. Stafylopatis 和 K. Blekas，“使用进化强化学习的自主车辆导航”，*《欧洲运筹学杂志》*，1998。
- en: Jaderberg et al. [2019] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris,
    G. Lever, A. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman
    *et al.*, “Human-level performance in 3d multiplayer games with population-based
    reinforcement learning,” *Science*, 2019.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等人 [2019] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G.
    Lever, A. G. Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman
    *等*，“基于人群的强化学习在三维多人游戏中的人类水平表现”，*《科学》*，2019。
- en: 'Vinyals et al. [2019a] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg,
    W. Czarnecki, A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan,
    M. Kroiss, I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky,
    S. Vezhnevets, J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfaff,
    T. Pohlen, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul, T. Lillicrap,
    C. Apps, K. Kavukcuoglu, D. Hassabis, and D. Silver, “AlphaStar: Mastering the
    Real-Time Strategy Game StarCraft II,” https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/,
    2019.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals 等人 [2019a] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg,
    W. Czarnecki, A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan,
    M. Kroiss, I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y.
    Sulsky, S. Vezhnevets, J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z.
    Wang, T. Pfaff, T. Pohlen, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul,
    T. Lillicrap, C. Apps, K. Kavukcuoglu, D. Hassabis, 和 D. Silver， “AlphaStar: 精通即时战略游戏《星际争霸
    II》”， [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)，2019。'
- en: Khadka and Tumer [2018] S. Khadka and K. Tumer, “Evolutionary reinforcement
    learning,” *arXiv preprint arXiv:1805.07917*, 2018.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadka 和 Tumer [2018] S. Khadka 和 K. Tumer，“进化强化学习”，*《arXiv 预印本 arXiv:1805.07917》*，2018。
- en: Shopov and Markova [2018] V. Shopov and V. Markova, “A study of the impact of
    evolutionary strategies on performance of reinforcement learning autonomous agents,”
    *ICAS 2018*, 2018.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shopov 和 Markova [2018] V. Shopov 和 V. Markova，“进化策略对强化学习自主体性能影响的研究”，*《ICAS
    2018》*，2018。
- en: Houthooft et al. [2018] R. Houthooft, R. Y. Chen, P. Isola, B. C. Stadie, F. Wolski,
    J. Ho, and P. Abbeel, “Evolved policy gradients,” 2018.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等人 [2018] R. Houthooft, R. Y. Chen, P. Isola, B. C. Stadie, F. Wolski,
    J. Ho, 和 P. Abbeel，“进化政策梯度”，2018。
- en: Diqi Chen and Gao [2020] Y. W. Diqi Chen and W. Gao, “Combining a gradient-based
    method and an evolution strategy for multi-objective reinforcement learning,”
    2020.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diqi Chen 和 Gao [2020] Y. W. Diqi Chen 和 W. Gao，“结合基于梯度的方法和进化策略进行多目标强化学习”，2020。
- en: De Bruin et al. [2020] T. De Bruin, J. Kober, K. Tuyls, and R. Babuška, “Fine-tuning
    deep rl with gradient-free optimization*,” in *Proceedings of the IFAC World Congress*,
    2020.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Bruin et al. [2020] T. De Bruin, J. Kober, K. Tuyls, 和 R. Babuška, “Fine-tuning
    deep rl with gradient-free optimization*,” 在 *IFAC 世界大会论文集* 中, 2020。
- en: 'Song et al. [2021] X. Song, K. Choromanski, J. Parker-Holder, Y. Tang, D. Peng,
    D. Jain, W. Gao, A. Pacchiano, T. Sarlos, and Y. Yang, “Es-enas: Combining evolution
    strategies with neural architecture search at no extra cost for reinforcement
    learning,” 2021.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. [2021] X. Song, K. Choromanski, J. Parker-Holder, Y. Tang, D. Peng,
    D. Jain, W. Gao, A. Pacchiano, T. Sarlos, 和 Y. Yang, “Es-enas: Combining evolution
    strategies with neural architecture search at no extra cost for reinforcement
    learning,” 2021。'
- en: Ferreira et al. [2021] F. Ferreira, T. Nierhoff, and F. Hutter, “Learning synthetic
    environments for reinforcement learning with evolution strategies,” 2021.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira et al. [2021] F. Ferreira, T. Nierhoff, 和 F. Hutter, “Learning synthetic
    environments for reinforcement learning with evolution strategies,” 2021。
- en: Van Moffaert et al. [2013] K. Van Moffaert, M. M. Drugan, and A. Nowé, “Hypervolume-based
    multi-objective reinforcement learning,” in *International Conference on Evolutionary
    Multi-Criterion Optimization*.   Springer, 2013.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Moffaert et al. [2013] K. Van Moffaert, M. M. Drugan, 和 A. Nowé, “Hypervolume-based
    multi-objective reinforcement learning,” 在 *国际进化多准则优化会议* 中。 Springer, 2013。
- en: Parisi et al. [2014] S. Parisi, M. Pirotta, N. Smacchia, L. Bascetta, and M. Restelli,
    “Policy gradient approaches for multi-objective sequential decision making,” in
    *2014 International Joint Conference on Neural Networks (IJCNN)*, 2014.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisi et al. [2014] S. Parisi, M. Pirotta, N. Smacchia, L. Bascetta, 和 M. Restelli,
    “Policy gradient approaches for multi-objective sequential decision making,” 在
    *2014 国际神经网络联合会议 (IJCNN)* 中, 2014。
- en: 'Such et al. [2018] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley,
    and J. Clune, “Deep neuroevolution: Genetic algorithms are a competitive alternative
    for training deep neural networks for reinforcement learning,” 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Such et al. [2018] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley,
    和 J. Clune, “Deep neuroevolution: Genetic algorithms are a competitive alternative
    for training deep neural networks for reinforcement learning,” 2018。'
- en: 'Bellemare et al. [2013] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling,
    “The arcade learning environment: An evaluation platform for general agents,”
    *Journal of Artificial Intelligence Research*, 2013.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bellemare et al. [2013] M. G. Bellemare, Y. Naddaf, J. Veness, 和 M. Bowling,
    “The arcade learning environment: An evaluation platform for general agents,”
    *人工智能研究期刊*, 2013。'
- en: Shao et al. [2019] K. Shao, Z. Tang, Y. Zhu, N. Li, and D. Zhao, “A survey of
    deep reinforcement learning in video games,” *arXiv preprint arXiv:1912.10944*,
    2019.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. [2019] K. Shao, Z. Tang, Y. Zhu, N. Li, 和 D. Zhao, “A survey of
    deep reinforcement learning in video games,” *arXiv 预印本 arXiv:1912.10944*, 2019。
- en: Mnih et al. [2015b] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness,
    M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
    and D. Hassabis, “Human-level control through deep reinforcement learning,” *Nature*,
    2015.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al. [2015b] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness,
    M. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
    和 D. Hassabis, “Human-level control through deep reinforcement learning,” *自然*,
    2015。
- en: Silver et al. [2016b] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe,
    J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,
    T. Graepel, and D. Hassabis, “Mastering the game of go with deep neural networks
    and tree search,” *Nature*, 2016.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. [2016b] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G.
    Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S.
    Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach,
    K. Kavukcuoglu, T. Graepel, 和 D. Hassabis, “Mastering the game of go with deep
    neural networks and tree search,” *自然*, 2016。
- en: Silver et al. [2017] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
    A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering
    the game of go without human knowledge,” *nature*, 2017.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. [2017] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
    A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton *等*, “Mastering the
    game of go without human knowledge,” *自然*, 2017。
- en: Berner et al. [2019] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse *et al.*, “Dota 2 with
    large scale deep reinforcement learning,” *arXiv preprint arXiv:1912.06680*, 2019.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berner et al. [2019] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse *等*, “Dota 2 with large
    scale deep reinforcement learning,” *arXiv 预印本 arXiv:1912.06680*, 2019。
- en: Vinyals et al. [2019b] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu,
    A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev *et al.*, “Grandmaster
    level in starcraft ii using multi-agent reinforcement learning,” *Nature*, 2019.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等 [2019b] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A.
    Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev *等*，“使用多智能体强化学习在星际争霸
    II 中达到大师级水平”，*自然*，2019年。
- en: 'Kober et al. [2013] J. Kober, J. Bagnell, and J. Peters, “Reinforcement learning
    in robotics: A survey,” *The International Journal of Robotics Research*, 2013.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kober 等 [2013] J. Kober, J. Bagnell, 和 J. Peters，“机器人学中的强化学习：综述”，*国际机器人研究杂志*，2013年。
- en: Nguyen and La [2019] H. Nguyen and H. La, “Review of deep reinforcement learning
    for robot manipulation,” in *2019 Third IEEE International Conference on Robotic
    Computing (IRC)*.   IEEE, 2019.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 La [2019] H. Nguyen 和 H. La，“机器人操作的深度强化学习综述”，在 *2019 第三届 IEEE 国际机器人计算会议
    (IRC)*。 IEEE，2019年。
- en: Chen et al. [2017b] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized
    non-communicating multiagent collision avoidance with deep reinforcement learning,”
    in *2017 IEEE international conference on robotics and automation (ICRA)*, 2017.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2017b] Y. F. Chen, M. Liu, M. Everett, 和 J. P. How，“去中心化非通信的多智能体碰撞规避与深度强化学习”，在
    *2017 IEEE 国际机器人与自动化会议 (ICRA)*，2017年。
- en: Kahn et al. [2018] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine,
    “Self-supervised deep reinforcement learning with generalized computation graphs
    for robot navigation,” in *2018 IEEE International Conference on Robotics and
    Automation (ICRA)*, 2018.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahn 等 [2018] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, 和 S. Levine，“自监督深度强化学习与广义计算图用于机器人导航”，在
    *2018 IEEE 国际机器人与自动化会议 (ICRA)*，2018年。
- en: Bellegarda and Nguyen [2020] G. Bellegarda and Q. Nguyen, “Robust quadruped
    jumping via deep reinforcement learning,” *arXiv preprint arXiv:2011.07089*, 2020.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellegarda 和 Nguyen [2020] G. Bellegarda 和 Q. Nguyen，“通过深度强化学习实现鲁棒的四足动物跳跃”，*arXiv
    预印本 arXiv:2011.07089*，2020年。
- en: Haarnoja et al. [2018] T. Haarnoja, A. Zhou, S. Ha, J. Tan, G. Tucker, and S. Levine,
    “Learning to walk via deep reinforcement learning,” *CoRR*, 2018.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja 等 [2018] T. Haarnoja, A. Zhou, S. Ha, J. Tan, G. Tucker, 和 S. Levine，“通过深度强化学习学习行走”，*CoRR*，2018年。
- en: 'Tai et al. [2017] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement
    learning: Continuous control of mobile robots for mapless navigation,” in *2017
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2017.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tai 等 [2017] L. Tai, G. Paolo, 和 M. Liu，“虚拟到现实深度强化学习：无地图导航的移动机器人连续控制”，在 *2017
    IEEE/RSJ 国际智能机器人与系统会议 (IROS)*，2017年。
- en: Li et al. [2019] Y. Li, W. Zheng, and Z. Zheng, “Deep robust reinforcement learning
    for practical algorithmic trading,” *IEEE Access*, 2019.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2019] Y. Li, W. Zheng, 和 Z. Zheng，“深度鲁棒强化学习用于实际算法交易”，*IEEE Access*，2019年。
- en: Deng et al. [2017] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep direct
    reinforcement learning for financial signal representation and trading,” *IEEE
    Transactions on Neural Networks and Learning Systems*, 2017.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 [2017] Y. Deng, F. Bao, Y. Kong, Z. Ren, 和 Q. Dai，“深度直接强化学习用于金融信号表示和交易”，*IEEE
    神经网络与学习系统汇刊*，2017年。
- en: Jiang et al. [2017] Z. Jiang, D. Xu, and J. Liang, “A deep reinforcement learning
    framework for the financial portfolio management problem,” 2017.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 [2017] Z. Jiang, D. Xu, 和 J. Liang，“用于金融投资组合管理问题的深度强化学习框架”，2017年。
- en: Jiang and Liang [2017] Z. Jiang and J. Liang, “Cryptocurrency portfolio management
    with deep reinforcement learning,” in *2017 Intelligent Systems Conference (IntelliSys)*,
    2017.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 和 Liang [2017] Z. Jiang 和 J. Liang，“基于深度强化学习的加密货币投资组合管理”，在 *2017 智能系统会议
    (IntelliSys)*，2017年。
- en: Moody and Saffell [1998] J. Moody and M. Saffell, “Reinforcement learning for
    trading systems and portfolios,” in *KDD*, 1998.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moody 和 Saffell [1998] J. Moody 和 M. Saffell，“用于交易系统和投资组合的强化学习”，在 *KDD*，1998年。
- en: Carapuço et al. [2018] J. Carapuço, R. Neves, and N. Horta, “Reinforcement learning
    applied to forex trading,” *Applied Soft Computing*, 2018.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carapuço 等 [2018] J. Carapuço, R. Neves, 和 N. Horta，“应用于外汇交易的强化学习”，*应用软计算*，2018年。
- en: Wu et al. [2020] X. Wu, H. Chen, J. Wang, L. Troiano, V. Loia, and H. Fujita,
    “Adaptive stock trading strategies with deep reinforcement learning methods,”
    *Information Sciences*, 2020.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2020] X. Wu, H. Chen, J. Wang, L. Troiano, V. Loia, 和 H. Fujita，“基于深度强化学习方法的自适应股票交易策略”，*信息科学*，2020年。
- en: Lei et al. [2020] K. Lei, B. Zhang, Y. Li, M. Yang, and Y. Shen, “Time-driven
    feature-aware jointly deep reinforcement learning for financial signal representation
    and algorithmic trading,” *Expert Systems with Applications*, 2020.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei et al. [2020] K. Lei, B. Zhang, Y. Li, M. Yang, 和 Y. Shen, “时间驱动特征感知联合深度强化学习用于金融信号表示和算法交易”，*Expert
    Systems with Applications*，2020年。
- en: 'Xiong et al. [2019] Z. Xiong, Y. Zhang, D. Niyato, R. Deng, P. Wang, and L. Wang,
    “Deep reinforcement learning for mobile 5g and beyond: Fundamentals, applications,
    and challenges,” *IEEE Vehicular Technology Magazine*, 2019.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. [2019] Z. Xiong, Y. Zhang, D. Niyato, R. Deng, P. Wang, 和 L. Wang,
    “移动5G及其后续技术中的深度强化学习：基础、应用与挑战”，*IEEE Vehicular Technology Magazine*，2019年。
- en: 'Luong et al. [2019] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang,
    Y. Liang, and D. I. Kim, “Applications of deep reinforcement learning in communications
    and networking: A survey,” *IEEE Communications Surveys Tutorials*, 2019.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong et al. [2019] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.
    Liang, 和 D. I. Kim, “深度强化学习在通信和网络中的应用：综述”，*IEEE Communications Surveys Tutorials*，2019年。
- en: Wang et al. [2018] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep
    reinforcement learning for dynamic multichannel access in wireless networks,”
    *CoRR*, 2018.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2018] S. Wang, H. Liu, P. H. Gomes, 和 B. Krishnamachari, “无线网络中的动态多通道接入深度强化学习”，*CoRR*，2018年。
- en: Ye and Li [2018] H. Ye and G. Y. Li, “Deep reinforcement learning for resource
    allocation in v2v communications,” 2018.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 和 Li [2018] H. Ye 和 G. Y. Li, “用于V2V通信的深度强化学习资源分配”，2018年。
- en: 'Gadaleta et al. [2017] M. Gadaleta, F. Chiariotti, M. Rossi, and A. Zanella,
    “D-dash: A deep q-learning framework for dash video streaming,” *IEEE Transactions
    on Cognitive Communications and Networking*, 2017.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gadaleta et al. [2017] M. Gadaleta, F. Chiariotti, M. Rossi, 和 A. Zanella, “D-dash：一种用于DASH视频流的深度Q学习框架”，*IEEE
    Transactions on Cognitive Communications and Networking*，2017年。
- en: Mao et al. [2017] H. Mao, R. Netravali, and M. Alizadeh, “Neural adaptive video
    streaming with pensieve,” in *Proceedings of the Conference of the ACM Special
    Interest Group on Data Communication*, 2017.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. [2017] H. Mao, R. Netravali, 和 M. Alizadeh, “使用Pensieve的神经自适应视频流”，发表于*Proceedings
    of the Conference of the ACM Special Interest Group on Data Communication*，2017年。
- en: Chinchali et al. [2018] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra,
    M. Pavone, and S. Katti, “Cellular network traffic scheduling with deep reinforcement
    learning,” in *AAAI*, 2018.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chinchali et al. [2018] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R.
    Misra, M. Pavone, 和 S. Katti, “使用深度强化学习的蜂窝网络流量调度”，发表于*AAAI*，2018年。
- en: Ferreira et al. [2018] P. Ferreira, R. Paffenroth, A. Wyglinski, T. Hackett,
    S. Bilén, R. Reinhart, and D. Mortensen, “Multi-objective reinforcement learning
    for cognitive satellite communications using deep neural network ensembles,” *IEEE
    Journal on Selected Areas in Communications*, 2018.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira et al. [2018] P. Ferreira, R. Paffenroth, A. Wyglinski, T. Hackett,
    S. Bilén, R. Reinhart, 和 D. Mortensen, “用于认知卫星通信的多目标强化学习，使用深度神经网络集成”，*IEEE Journal
    on Selected Areas in Communications*，2018年。
- en: He and Hu [2017] Y. He and S. Hu, “Cache-enabled wireless networks with opportunistic
    interference alignment,” *arXiv preprint arXiv:1706.09024*, 2017.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 Hu [2017] Y. He 和 S. Hu, “具有机会干扰对齐的缓存启用无线网络”，*arXiv预印本 arXiv:1706.09024*，2017年。
- en: 'He et al. [2017] Y. He, C. Liang, F. R. Yu, N. Zhao, and H. Yin, “Optimization
    of cache-enabled opportunistic interference alignment wireless networks: A big
    data deep reinforcement learning approach,” in *2017 IEEE International Conference
    on Communications (ICC)*, 2017.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2017] Y. He, C. Liang, F. R. Yu, N. Zhao, 和 H. Yin, “缓存启用的机会干扰对齐无线网络的优化：一种大数据深度强化学习方法”，发表于*2017
    IEEE International Conference on Communications (ICC)*，2017年。
- en: Zhong et al. [2018] C. Zhong, M. C. Gursoy, and S. Velipasalar, “A deep reinforcement
    learning-based framework for content caching,” in *2018 52nd Annual Conference
    on Information Sciences and Systems (CISS)*, 2018.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. [2018] C. Zhong, M. C. Gursoy, 和 S. Velipasalar, “基于深度强化学习的内容缓存框架”，发表于*2018年第52届年度信息科学与系统会议
    (CISS)*，2018年。
- en: He et al. [2017a] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, and
    Y. Zhang, “Deep-reinforcement-learning-based optimization for cache-enabled opportunistic
    interference alignment wireless networks,” *IEEE Transactions on Vehicular Technology*,
    2017.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2017a] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, 和
    Y. Zhang, “基于深度强化学习的缓存启用机会干扰对齐无线网络优化”，*IEEE Transactions on Vehicular Technology*，2017年。
- en: He et al. [2017b] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, “Deep
    reinforcement learning (drl)-based resource management in software-defined and
    virtualized vehicular ad hoc networks,” in *Proceedings of the 6th ACM Symposium
    on Development and Analysis of Intelligent Vehicular Networks and Applications*,
    2017.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2017b] Y. He, F. R. Yu, N. Zhao, H. Yin, 和 A. Boukerche, “基于深度强化学习（drl）的软件定义和虚拟化车载自组网中的资源管理，”
    在 *第6届ACM智能车载网络与应用发展与分析研讨会论文集*，2017。
- en: He et al. [2017] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, and Y. Zhang,
    “Resource allocation in software-defined and information-centric vehicular networks
    with mobile edge computing,” in *2017 IEEE 86th Vehicular Technology Conference
    (VTC-Fall)*, 2017.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2017] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, 和 Y. Zhang,
    “具有移动边缘计算的软件定义和信息中心车载网络中的资源分配，” 在 *2017 IEEE 第86届车载技术大会（VTC-Fall）*，2017。
- en: 'He et al. [2018] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching
    and computing for connected vehicles: A deep reinforcement learning approach,”
    *IEEE Transactions on Vehicular Technology*, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2018] Y. He, N. Zhao, 和 H. Yin, “连接车辆的集成网络、缓存和计算：一种深度强化学习方法，” *IEEE 车载技术期刊*，2018。
- en: '[216] M. Glavic, R. Fonteneau, and D. Ernst, “Reinforcement learning for electric
    power system decision and control: Past considerations and perspectives,” *IFAC-PapersOnLine*.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] M. Glavic, R. Fonteneau, 和 D. Ernst, “用于电力系统决策和控制的强化学习：过去的考虑与展望，” *IFAC-PapersOnLine*。'
- en: François-Lavet et al. [2016] V. François-Lavet, D. Taralla, D. Ernst, and R. Fonteneau,
    “Deep reinforcement learning solutions for energy microgrids management,” in *European
    Workshop on Reinforcement Learning (EWRL 2016)*, 2016.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet 等人 [2016] V. François-Lavet, D. Taralla, D. Ernst, 和 R. Fonteneau,
    “能源微电网管理的深度强化学习解决方案，” 在 *欧洲强化学习研讨会（EWRL 2016）*，2016。
- en: Mocanu et al. [2018] E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta, M. E.
    Webber, M. Gibescu, and J. G. Slootweg, “On-line building energy optimization
    using deep reinforcement learning,” *IEEE transactions on smart grid*, 2018.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocanu 等人 [2018] E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta, M. E. Webber,
    M. Gibescu, 和 J. G. Slootweg, “利用深度强化学习进行在线建筑能效优化，” *IEEE 智能电网期刊*，2018。
- en: Ruelens et al. [2019] F. Ruelens, B. J. Claessens, P. Vrancx, F. Spiessens,
    and G. Deconinck, “Direct load control of thermostatically controlled loads based
    on sparse observations using deep reinforcement learning,” *CSEE Journal of Power
    and Energy Systems*, 2019.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruelens 等人 [2019] F. Ruelens, B. J. Claessens, P. Vrancx, F. Spiessens, 和 G.
    Deconinck, “基于稀疏观测的温控负荷直接负荷控制，使用深度强化学习，” *CSEE 电力与能源系统期刊*，2019。
- en: Li et al. [2016] L. Li, Y. Lv, and F. Wang, “Traffic signal timing via deep
    reinforcement learning,” *IEEE/CAA Journal of Automatica Sinica*, 2016.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2016] L. Li, Y. Lv, 和 F. Wang, “通过深度强化学习进行交通信号时序优化，” *IEEE/CAA 自动化学报*，2016。
- en: Liang et al. [2019] X. Liang, X. Du, G. Wang, and Z. Han, “A deep reinforcement
    learning network for traffic light cycle control,” *IEEE Transactions on Vehicular
    Technology*, 2019.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2019] X. Liang, X. Du, G. Wang, 和 Z. Han, “一种用于交通信号周期控制的深度强化学习网络，”
    *IEEE 车载技术期刊*，2019。
- en: Chu et al. [2020] T. Chu, J. Wang, L. Codecà, and Z. Li, “Multi-agent deep reinforcement
    learning for large-scale traffic signal control,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2020.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 等人 [2020] T. Chu, J. Wang, L. Codecà, 和 Z. Li, “大规模交通信号控制的多代理深度强化学习，” *IEEE
    智能交通系统期刊*，2020。
- en: 'Chen et al. [2020] C. Chen, H. Wei, N. Xu, G. Zheng, M. Yang, Y. Xiong, K. Xu,
    and Z. Li, “Toward a thousand lights: Decentralized deep reinforcement learning
    for large-scale traffic signal control,” *Proceedings of the AAAI Conference on
    Artificial Intelligence*, 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2020] C. Chen, H. Wei, N. Xu, G. Zheng, M. Yang, Y. Xiong, K. Xu, 和
    Z. Li, “迈向千灯：大规模交通信号控制的分布式深度强化学习，” *AAAI 人工智能会议论文集*，2020。
- en: 'Wang and Sun [2020] J. Wang and L. Sun, “Dynamic holding control to avoid bus
    bunching: A multi-agent deep reinforcement learning framework,” *Transportation
    Research Part C: Emerging Technologies*, 2020.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Sun [2020] J. Wang 和 L. Sun, “动态保持控制以避免公交车集中：一种多代理深度强化学习框架，” *交通研究C部分：新兴技术*，2020。
- en: 'Manchella et al. [2021] K. Manchella, A. K. Umrawal, and V. Aggarwal, “Flexpool:
    A distributed model-free deep reinforcement learning algorithm for joint passengers
    and goods transportation,” *IEEE Transactions on Intelligent Transportation Systems*,
    2021.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manchella 等人 [2021] K. Manchella, A. K. Umrawal, 和 V. Aggarwal, “Flexpool：一种分布式无模型深度强化学习算法用于联合乘客和货物运输，”
    *IEEE 智能交通系统期刊*，2021。
- en: Pagliuca et al. [2020] P. Pagliuca, N. Milano, and S. Nolfi, “Efficacy of modern
    neuro-evolutionary strategies for continuous control optimization,” 2020.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pagliuca et al. [2020] P. Pagliuca, N. Milano, and S. Nolfi, “现代神经进化策略在连续控制优化中的有效性，”2020年。
- en: Zhou and Feng [2018] B. Zhou and J. Feng, “Sample efficient deep neuroevolution
    in low dimensional latent space,” 2018.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou and Feng [2018] B. Zhou and J. Feng, “低维潜在空间中的样本高效深度神经进化，”2018年。
- en: Chen et al. [2019] Z. Chen, Y. Zhou, X. He, and S. Jiang, “A restart-based rank-1
    evolution strategy for reinforcement learning.” in *IJCAI*, 2019.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2019] Z. Chen, Y. Zhou, X. He, and S. Jiang, “基于重启的等级1进化策略用于强化学习。”发表于*IJCAI*，2019年。
- en: Risi and Stanley [2019] S. Risi and K. O. Stanley, “Deep neuroevolution of recurrent
    and discrete world models,” 2019.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Risi and Stanley [2019] S. Risi and K. O. Stanley, “递归和离散世界模型的深度神经进化，”2019年。
- en: 'Veer and Majumdar [2020] S. Veer and A. Majumdar, “Cones: Convex natural evolutionary
    strategies,” 2020.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Veer and Majumdar [2020] S. Veer and A. Majumdar, “Cones: 凸自然进化策略，”2020年。'
- en: Shi et al. [2020] L. Shi, S. Li, Q. Zheng, L. Cao, L. Yang, and G. Pan, “Maximum
    entropy reinforcement learning with evolution strategies,” in *2020 International
    Joint Conference on Neural Networks (IJCNN)*, 2020.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. [2020] L. Shi, S. Li, Q. Zheng, L. Cao, L. Yang, and G. Pan, “最大熵强化学习与进化策略，”发表于*2020年国际联合神经网络会议（IJCNN）*，2020年。
- en: Jackson and Channon [2019] B. Jackson and A. Channon, “Neuroevolution of humanoids
    that walk further and faster with robust gaits,” 2019.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jackson and Channon [2019] B. Jackson and A. Channon, “步态稳健的类人机器人神经进化，以实现更远更快的步行，”2019年。
- en: Hu et al. [2018] Y. Hu, X. Wu, P. Geng, and Z. Li, “Evolution strategies learning
    with variable impedance control for grasping under uncertainty,” *IEEE Transactions
    on Industrial Electronics*, vol. 66, no. 10, pp. 7788–7799, 2018.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. [2018] Y. Hu, X. Wu, P. Geng, and Z. Li, “带有可变阻抗控制的进化策略学习以应对不确定性下的抓取，”*IEEE工业电子学报*，第66卷，第10期，页码7788–7799，2018年。
- en: Uchitane et al. [2010] T. Uchitane, T. Hatanaka, and K. Uosaki, “Evolution strategies
    for biped locomotion learning using nonlinear oscillators,” in *Proceedings of
    SICE Annual Conference 2010*.   IEEE, 2010, pp. 1458–1461.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uchitane et al. [2010] T. Uchitane, T. Hatanaka, and K. Uosaki, “利用非线性振荡器进行双足行走学习的进化策略，”发表于*2010年SICE年会论文集*。
    IEEE，2010年，页码1458–1461。
- en: Li et al. [2008] M.-H. Li, B.-R. Hong, Z.-S. Cai, S.-H. Piao, and Q.-C. Huang,
    “Novel indoor mobile robot navigation using monocular vision,” *Engineering Applications
    of Artificial Intelligence*, vol. 21, no. 3, pp. 485–497, 2008.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2008] M.-H. Li, B.-R. Hong, Z.-S. Cai, S.-H. Piao, and Q.-C. Huang,
    “基于单目视觉的新型室内移动机器人导航，”*人工智能工程应用*，第21卷，第3期，页码485–497，2008年。
- en: Korczak and Lipinski [2001] J. Korczak and P. Lipinski, “Evolutionary approach
    to portfolio optimization,” in *Proceedings of Workshop on Artificial Intelligence
    for Financial Time Series Analysis, Porto*, 2001.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korczak and Lipinski [2001] J. Korczak and P. Lipinski, “投资组合优化的进化方法，”发表于*金融时间序列分析人工智能研讨会论文集，波尔图*，2001年。
- en: Rimcharoen et al. [2005] S. Rimcharoen, D. Sutivong, and P. Chongstitvatana,
    “Prediction of the stock exchange of thailand using adaptive evolution strategies,”
    in *17th IEEE International Conference on Tools with Artificial Intelligence (ICTAI’05)*,
    2005.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rimcharoen et al. [2005] S. Rimcharoen, D. Sutivong, and P. Chongstitvatana,
    “使用自适应进化策略预测泰国证券交易所，”发表于*第17届IEEE国际工具与人工智能会议（ICTAI’05）*，2005年。
- en: Sutheebanjard and Premchaiswadi [2009] P. Sutheebanjard and W. Premchaiswadi,
    “Factors analysis on stock exchange of thailand (set) index movement,” in *2009
    7th International Conference on ICT and Knowledge Engineering*, 2009.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutheebanjard and Premchaiswadi [2009] P. Sutheebanjard and W. Premchaiswadi,
    “泰国证券交易所（SET）指数波动的因素分析，”发表于*2009年第七届国际信息通信技术与知识工程会议*，2009年。
- en: Bonde and Khaled [2012] G. Bonde and R. Khaled, “Stock price prediction using
    genetic algorithms and evolution strategies,” in *Proceedings of the International
    Conference on Genetic and Evolutionary Methods (GEM)*, 2012.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonde and Khaled [2012] G. Bonde and R. Khaled, “使用遗传算法和进化策略预测股票价格，”发表于*国际遗传与进化方法会议（GEM）论文集*，2012年。
- en: Pai and Michel [2012] G. A. V. Pai and T. Michel, “Integrated metaheuristic
    optimization of 130–30 investment-strategy-based long–short portfolios,” *International
    Journal of Intelligent Systems in Accounting and Finance Management*, 2012.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pai and Michel [2012] G. A. V. Pai and T. Michel, “130–30投资策略基础上的长期短期投资组合的综合元启发式优化，”*国际智能系统会计与财务管理杂志*，2012年。
- en: Pai and Michel [2014] G. V. Pai and T. Michel, “Metaheuristic multi-objective
    optimization of constrained futures portfolios for effective risk management,”
    *Swarm and Evolutionary Computation*, vol. 19, pp. 1–14, 2014.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pai 和 Michel [2014] G. V. Pai 和 T. Michel，“受约束的期货投资组合的元启发式多目标优化以有效管理风险，” *Swarm
    and Evolutionary Computation*, 第19卷，第1–14页，2014年。
- en: Yu [2017] K. Yu, “Dynamic portfolio optimization using evolution strategy,”
    2017.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu [2017] K. Yu，“使用进化策略的动态投资组合优化，” 2017年。
- en: Sable et al. [2017] S. Sable, A. Porwal, and U. Singh, “Stock price prediction
    using genetic algorithms and evolution strategies,” in *2017 International conference
    of Electronics, Communication and Aerospace Technology (ICECA)*, 2017.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sable 等 [2017] S. Sable, A. Porwal 和 U. Singh，“使用遗传算法和进化策略的股票价格预测，” 收录于 *2017
    International Conference of Electronics, Communication and Aerospace Technology
    (ICECA)*，2017年。
- en: Sorensen et al. [2020] E. Sorensen, R. Ozzello, R. Rogan, E. Baker, N. Parks,
    and W. Hu, “Meta-learning of evolutionary strategy for stock trading,” *Journal
    of Data Analysis and Information Processing*, 2020.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorensen 等 [2020] E. Sorensen, R. Ozzello, R. Rogan, E. Baker, N. Parks 和 W.
    Hu，“股票交易的进化策略元学习，” *Journal of Data Analysis and Information Processing*, 2020年。
- en: Pérez-Pérez et al. [2007] R. Pérez-Pérez, C. Luque, A. Cervantes, and P. Isasi,
    “Multiobjective algorithms to optimize broadcasting parameters in mobile ad-hoc
    networks,” in *2007 IEEE Congress on Evolutionary Computation*, 2007.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pérez-Pérez 等 [2007] R. Pérez-Pérez, C. Luque, A. Cervantes 和 P. Isasi，“优化移动自组网广播参数的多目标算法，”
    收录于 *2007 IEEE Congress on Evolutionary Computation*，2007年。
- en: Krulikovska et al. [2010] L. Krulikovska, J. Filanová, and J. Pavlovic, “Evolution
    strategies in the multipoint connections routing,” *Radioengineering*, 2010.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krulikovska 等 [2010] L. Krulikovska, J. Filanová 和 J. Pavlovic，“多点连接路由中的进化策略，”
    *Radioengineering*, 2010年。
- en: Nissen and Gold [2008] V. Nissen and S. Gold, *Survivable network design with
    an evolution strategy*.   Springer, 2008.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nissen 和 Gold [2008] V. Nissen 和 S. Gold，*具有进化策略的可生存网络设计*。Springer，2008年。
- en: He et al. [2018] W. He, P. Qiao, Z. Zhou, G. Hu, Z. Feng, and H. Wei, “A new
    belief-rule-based method for fault diagnosis of wireless sensor network,” *IEEE
    Access*, 2018.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 [2018] W. He, P. Qiao, Z. Zhou, G. Hu, Z. Feng 和 H. Wei，“一种基于信念规则的新方法用于无线传感器网络的故障诊断，”
    *IEEE Access*, 2018年。
- en: 'Srivastava and Singh [2018] G. Srivastava and A. Singh, “Boosting an evolution
    strategy with a preprocessing step: application to group scheduling problem in
    directional sensor networks,” *Applied Intelligence*, 2018.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 和 Singh [2018] G. Srivastava 和 A. Singh，“通过预处理步骤提升进化策略：应用于定向传感器网络中的分组调度问题，”
    *Applied Intelligence*, 2018年。
- en: Srivastava et al. [2020] G. Srivastava, P. Venkatesh, and A. Singh, “An evolution
    strategy based approach for cover scheduling problem in wireless sensor networks,”
    *Int. J. Mach. Learn. Cybern.*, 2020.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等 [2020] G. Srivastava, P. Venkatesh 和 A. Singh，“基于进化策略的无线传感器网络覆盖调度问题的解决方法，”
    *Int. J. Mach. Learn. Cybern.*, 2020年。
- en: Gu and Potkonjak [2021] H. Gu and M. Potkonjak, “Evolution-strategies-driven
    optimization on secure and reconfigurable interconnection puf networks,” *Electronics*,
    2021.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 和 Potkonjak [2021] H. Gu 和 M. Potkonjak，“基于进化策略的安全且可重构互连 PUF 网络优化，” *Electronics*,
    2021年。
- en: 'Mendoza et al. [2006] F. Mendoza, D. Requena, J. L. Bemal-Agustin, and J. A.
    Domínguez-Navarro, “Optimal conductor size selection in radial power distribution
    systems using evolutionary strategies,” in *2006 IEEE/PES Transmission & Distribution
    Conference and Exposition: Latin America*.   IEEE, 2006, pp. 1–5.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mendoza 等 [2006] F. Mendoza, D. Requena, J. L. Bemal-Agustin 和 J. A. Domínguez-Navarro，“利用进化策略在径向电力配电系统中选择最佳导体尺寸，”
    收录于 *2006 IEEE/PES Transmission & Distribution Conference and Exposition: Latin
    America*。IEEE，2006年，第1–5页。'
- en: Lezama et al. [2017] F. Lezama, L. E. Sucar, E. M. de Cote, J. Soares, and Z. Vale,
    “Differential evolution strategies for large-scale energy resource management
    in smart grids,” in *Proceedings of the Genetic and Evolutionary Computation Conference
    Companion*, 2017, pp. 1279–1286.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lezama 等 [2017] F. Lezama, L. E. Sucar, E. M. de Cote, J. Soares 和 Z. Vale，“在智能电网中用于大规模能源资源管理的差分进化策略，”
    收录于 *Genetic and Evolutionary Computation Conference Companion 会议论文集*，2017年，第1279–1286页。
- en: Coelho et al. [2014] V. N. Coelho, F. G. Guimarães, A. J. Reis, I. M. Coelho,
    B. N. Coelho, and M. J. Souza, “A heuristic fuzzy algorithm bio-inspired by evolution
    strategies for energy forecasting problems,” in *2014 IEEE International Conference
    on Fuzzy Systems (FUZZ-IEEE)*.   IEEE, 2014, pp. 338–345.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coelho 等 [2014] V. N. Coelho, F. G. Guimarães, A. J. Reis, I. M. Coelho, B.
    N. Coelho 和 M. J. Souza，“一种受进化策略启发的模糊启发式算法用于能源预测问题，” 收录于 *2014 IEEE International
    Conference on Fuzzy Systems (FUZZ-IEEE)*。IEEE，2014年，第338–345页。
- en: Versloot et al. [2014] T. W. Versloot, D. J. Barker, and X. O. One, “Optimization
    of near-field wireless power transfer using evolutionary strategies,” in *The
    8th European Conference on Antennas and Propagation, Netherlands*, 2014.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Versloot 等人 [2014] T. W. Versloot, D. J. Barker, 和 X. O. One, “使用进化策略优化近场无线电能传输，”
    在 *第八届欧洲天线与传播会议，荷兰*，2014年。
- en: Balaji et al. [2007] P. G. Balaji, G. Sachdeva, D. Srinivasan, and C. Tham,
    “Multi-agent system based urban traffic management,” in *2007 IEEE Congress on
    Evolutionary Computation*, 2007.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balaji 等人 [2007] P. G. Balaji, G. Sachdeva, D. Srinivasan, 和 C. Tham, “基于多智能体系统的城市交通管理，”
    在 *2007 IEEE 进化计算大会*，2007年。
- en: Mester and Bräysy [2005] D. Mester and O. Bräysy, “Active guided evolution strategies
    for large-scale vehicle routing problems with time windows,” *Computers & Operations
    Research*, vol. 32, no. 6, pp. 1593–1614, 2005.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mester 和 Bräysy [2005] D. Mester 和 O. Bräysy, “用于大规模车辆路径问题的主动引导进化策略，带时间窗，” *计算机与运筹研究*，第32卷，第6期，第1593-1614页，2005年。
- en: Nagabandi et al. [2020] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar,
    “Deep dynamics models for learning dexterous manipulation,” in *Conference on
    Robot Learning*, 2020.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等人 [2020] A. Nagabandi, K. Konolige, S. Levine, 和 V. Kumar, “用于学习灵巧操控的深度动态模型，”
    在 *机器人学习会议*，2020年。
- en: Tebbe et al. [2020] J. Tebbe, L. Krauch, Y. Gao, and A. Zell, “Sample-efficient
    reinforcement learning in robotic table tennis,” *arXiv preprint arXiv:2011.03275*,
    2020.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tebbe 等人 [2020] J. Tebbe, L. Krauch, Y. Gao, 和 A. Zell, “在机器人乒乓球中样本高效的强化学习，”
    *arXiv 预印本 arXiv:2011.03275*，2020年。
- en: 'Pourchot et al. [2018] A. Pourchot, N. Perrin, and O. Sigaud, “Importance mixing:
    Improving sample reuse in evolutionary policy search methods,” 2018.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pourchot 等人 [2018] A. Pourchot, N. Perrin, 和 O. Sigaud, “重要性混合：改进进化策略搜索方法中的样本重用，”
    2018年。
- en: 'Sigaud and Stulp [2019] O. Sigaud and F. Stulp, “Policy search in continuous
    action domains: an overview,” 2019.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigaud 和 Stulp [2019] O. Sigaud 和 F. Stulp, “连续动作领域中的策略搜索：概述，” 2019年。
- en: Sun et al. [2012] Y. Sun, D. Wierstra, T. Schaul, and J. Schmidhuber, “Efficient
    natural evolution strategies,” 2012.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2012] Y. Sun, D. Wierstra, T. Schaul, 和 J. Schmidhuber, “高效的自然进化策略，”
    2012年。
- en: 'Portelas et al. [2020] R. Portelas, C. Colas, L. Weng, K. Hofmann, and P.-Y.
    Oudeyer, “Automatic curriculum learning for deep rl: A short survey,” *arXiv preprint
    arXiv:2003.04664*, 2020.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Portelas 等人 [2020] R. Portelas, C. Colas, L. Weng, K. Hofmann, 和 P.-Y. Oudeyer,
    “深度强化学习的自动课程学习：简短综述，” *arXiv 预印本 arXiv:2003.04664*，2020年。
- en: 'Arulkumaran et al. [2017] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and
    A. A. Bharath, “Deep reinforcement learning: A brief survey,” *IEEE Signal Processing
    Magazine*, 2017.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arulkumaran 等人 [2017] K. Arulkumaran, M. P. Deisenroth, M. Brundage, 和 A. A.
    Bharath, “深度强化学习：简要综述，” *IEEE 信号处理杂志*，2017年。
- en: Ng and Russell [2000] A. Y. Ng and S. J. Russell, “Algorithms for inverse reinforcement
    learning,” in *Proceedings of the Seventeenth International Conference on Machine
    Learning*, 2000.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 和 Russell [2000] A. Y. Ng 和 S. J. Russell, “逆向强化学习的算法，” 在 *第十七届国际机器学习会议论文集*，2000年。
- en: 'Sadeghi and Levine [2016] F. Sadeghi and S. Levine, “Cad2rl: Real single-image
    flight without a single real image,” *arXiv preprint arXiv:1611.04201*, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sadeghi 和 Levine [2016] F. Sadeghi 和 S. Levine, “Cad2rl: 无需真实图像的真实单图像飞行，” *arXiv
    预印本 arXiv:1611.04201*，2016年。'
- en: Matas et al. [2018] J. Matas, S. James, and A. J. Davison, “Sim-to-real reinforcement
    learning for deformable object manipulation,” in *Conference on Robot Learning*,
    2018.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matas 等人 [2018] J. Matas, S. James, 和 A. J. Davison, “用于可变形物体操控的仿真到现实强化学习，”
    在 *机器人学习会议*，2018年。
- en: 'Rao et al. [2020] K. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari,
    “Rl-cyclegan: Reinforcement learning aware simulation-to-real,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rao 等人 [2020] K. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, 和 M. Khansari,
    “Rl-cyclegan: 关注强化学习的仿真到现实，” 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年。'
