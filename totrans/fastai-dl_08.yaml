- en: 'Deep Learning 2: Part 2 Lesson 8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习 2：第 2 部分第 8 课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493)'
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai 课程*](http://www.fast.ai/)* 中的个人笔记。这些笔记将在我继续审查课程以“真正”理解它时继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: Object Detection
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测
- en: '[**Forum**](http://forums.fast.ai/t/part-2-lesson-8-in-class/13556/1) **/**
    [**Video**](https://youtu.be/Z0ssNAbe81M) **/** [**Notebook**](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)
    **/** [**Slides**](https://github.com/fastai/fastai/blob/master/courses/dl2/ppt/lesson8.pptx)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[**论坛**](http://forums.fast.ai/t/part-2-lesson-8-in-class/13556/1) **/** [**视频**](https://youtu.be/Z0ssNAbe81M)
    **/** [**笔记本**](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)
    **/** [**幻灯片**](https://github.com/fastai/fastai/blob/master/courses/dl2/ppt/lesson8.pptx)'
- en: '**What we covered in Part 1 [**[**02:00**](https://youtu.be/Z0ssNAbe81M?t=2m)**]**'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们在第一部分中涵盖的内容 [**[**02:00**](https://youtu.be/Z0ssNAbe81M?t=2m)**]**'
- en: '**Differentiable layer [**[**02:11**](https://youtu.be/Z0ssNAbe81M?t=2m11s)**]**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**可微分层 [**[**02:11**](https://youtu.be/Z0ssNAbe81M?t=2m11s)**]**'
- en: Yann LeCun has been promoting the idea that we do not call this “deep learning”
    but “differentiable programming”. All we did in part 1 was really about setting
    up a differentiable function and a loss function that describes how good the parameters
    are and then pressing go and it makes it work. If you can configure a loss function
    that scores how good something is doing your task sand you have a reasonably flexible
    neural network architecture, you are done.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun 一直在推广这样一个观点，即我们不称之为“深度学习”，而是“可微分编程”。我们在第一部分所做的一切实际上都是关于建立一个可微分函数和一个描述参数好坏的损失函数，然后按下开始按钮，它就开始工作了。如果你能配置一个评分函数来评估某个任务的表现，并且有一个相当灵活的神经网络架构，那么你就完成了。
- en: Yeah, Differentiable Programming is little more than a rebranding of the modern
    collection Deep Learning techniques, the same way Deep Learning was a rebranding
    of the modern incarnations of neural nets with more than two layers.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是的，可微分编程只不过是现代深度学习技术的一个重新包装，就像深度学习是神经网络的现代化版本，具有超过两层的层次一样。
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The important point is that people are now building a new kind of software by
    assembling networks of parameterized functional blocks and by training them from
    examples using some form of gradient-based optimization….It’s really very much
    like a regular program, except it’s parameterized, automatically differentiated,
    and trainable/optimizable.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重要的一点是，人们现在通过组装参数化的功能块网络，并通过使用某种形式的基于梯度的优化从示例中训练它们来构建一种新型软件……这实际上非常类似于常规程序，只是它是参数化的，自动区分的，可训练/可优化的。
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Yann LeCun, Director of FAIR](https://www.facebook.com/yann.lecun/posts/10155003011462143)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [Yann LeCun，FAIR 主任](https://www.facebook.com/yann.lecun/posts/10155003011462143)'
- en: '**2\. Transfer Learning [**[**03:23**](https://youtu.be/Z0ssNAbe81M?t=3m23s)**]**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 迁移学习 [**[**03:23**](https://youtu.be/Z0ssNAbe81M?t=3m23s)**]**'
- en: 'Transfer learning is the most important single thing to be able to do to use
    deep learning effectively. You almost never would want to or need to start with
    random weights unless nobody had ever trained a model on a vaguely similar set
    of data with an even remotely connected kind of problem to solve as what you are
    doing — which almost never happens. Fastai library focuses on transfer learning
    which makes it different from other libraries. The basic idea of transfer learning
    is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是有效使用深度学习的最重要的单一事项。你几乎永远不会想要或需要从随机权重开始，除非没有人曾经在一个大致相似的数据集上训练过一个与你正在解决的问题有一定联系的模型
    — 这几乎不会发生。Fastai 库专注于迁移学习，这使它与其他库不同。迁移学习的基本思想是：
- en: Given a network that does thing A, remove the last layer.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个执行 A 任务的网络，移除最后一层。
- en: Replace it with a few random layers at the end
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后随机添加几个层
- en: Fine-tune those layers to do thing B while taking advantage of the features
    that the original network learned
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调这些层以执行 B 任务，同时利用原始网络学到的特征
- en: Then optionally fine tune the whole thing end-to-end and you now have something
    which probably uses orders of magnitude less data, is more accurate, and trains
    a lot faster.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后选择性地对整个模型进行微调，现在你有了一个可能使用数量级更少的数据，更准确，训练速度更快的东西。
- en: '**3\. Architecture design [**[**05:17**](https://youtu.be/Z0ssNAbe81M?t=5m17s)**]**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 架构设计 [**[**05:17**](https://youtu.be/Z0ssNAbe81M?t=5m17s)**]**'
- en: There is a pretty small range of architectures that generally works pretty well
    quite a lot of the time. We have been focusing on using CNN’s for generally fixed
    size ordered data, RNN’s for sequences that have some kind of state. We also fiddled
    around a tiny bit with activation functions — softmax if you have a single categorical
    outcome, or sigmoid if you have multiple outcomes. Some of the architecture design
    we will be studying in part 2 gets more interesting. Particularly this first session
    about object detection. But on the whole, we probably spend less time talking
    about architecture design than most courses or papers because it is generally
    not the hard bit.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有一小范围的架构通常在很多时候都表现得相当不错。我们一直专注于使用 CNN 处理通常大小固定的有序数据，RNN 处理具有某种状态的序列。我们还稍微调整了一下激活函数
    — 如果有单一分类结果，则使用 softmax，如果有多个结果，则使用 sigmoid。我们将在第 2 部分研究的一些架构设计更有趣。特别是关于目标检测的这个第一个会话。但总的来说，我们可能花更少的时间讨论架构设计，因为这通常不是难点。
- en: '**4\. Handling over-fitting [**[**06:26**](https://youtu.be/Z0ssNAbe81M?t=6m26s)**]**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 处理过拟合 [**[**06:26**](https://youtu.be/Z0ssNAbe81M?t=6m26s)**]**'
- en: 'The way Jeremy likes to build a model:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy 喜欢构建模型的方式：
- en: Create something that is definitely terribly over-parameterized which will massively
    overfit for sure, train it and make sure it does overfit. At that point, you’ve
    got a model that is capable of reflecting the training set. Then it is as simple
    as doing these things to reduce that overfitting.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一些明显过度参数化的东西，肯定会过度拟合，训练它并确保它确实过拟合。在那一点上，你已经有了一个能够反映训练集的模型。然后只需做这些事情来减少过拟合。
- en: 'If you don’t start with something that is overfitting, you are lost. So you
    start with something overfitting and to make it overfit less you can:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不从一个过拟合的地方开始，你就会迷失。所以你从一个过拟合的地方开始，为了让它过拟合得更少，你可以：
- en: add more data
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多数据
- en: add more data augmentation
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多数据增强
- en: do things like more batch norm layers, dense nets, or various things that can
    handle less data.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 做一些像更多的批量归一化层、稠密网络，或者各种可以处理更少数据的东西。
- en: add regularization like weight decay and dropout
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加正则化，如权重衰减和丢失
- en: finally (this is often the thing people do first but this should be the thing
    you do last) reduce the complexity of your architecture. have less layers or less
    activations.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后（这通常是人们首先做的事情，但这应该是你最后做的事情）减少你的架构复杂性。减少层数或激活数量。
- en: '**5\. Embeddings [**[**07:46**](https://youtu.be/Z0ssNAbe81M?t=7m46s)**]**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 嵌入[07:46]
- en: We have talked quite a bit about embeddings — both for NLP and the general idea
    of any kind of categorical data as being something you can now model with neural
    nets. Just earlier this year, there were almost no examples about using tabular
    data in deep learning, but it is becoming more and more popular approach to use
    neural nets for time series and tabular data analysis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经谈了很多关于嵌入 - 无论是用于自然语言处理还是任何种类的分类数据，现在你可以用神经网络来建模。就在今年初，几乎没有关于在深度学习中使用表格数据的例子，但现在越来越多的人开始使用神经网络来进行时间序列和表格数据分析。
- en: '**Part 1 to Part 2 [**[**08:54**](https://youtu.be/Z0ssNAbe81M?t=8m54s)**]**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分到第二部分[08:54]
- en: Part 1 really was all about introducing best practices in deep learning. We
    saw techniques which were mature enough that they definitely work reasonably reliably
    for practical real-world problems. Jeremy had researched and tuned enough over
    quite a long period of time, came up with a sequences of steps, architectures,
    etc, and put them into the fastai library in a way we could do that quickly and
    easily.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分真的是关于引入深度学习的最佳实践。我们看到的技术已经足够成熟，可以相对可靠地应用于实际的现实世界问题。Jeremy经过相当长一段时间的研究和调整，提出了一系列步骤、架构等，并将它们快速、轻松地放入fastai库中。
- en: Part 2 is cutting edge deep learning for coders, and what that means is Jeremy
    often does not know the exact best parameters, architecture details, and so forth
    to solve a particular problem. We do not necessarily know if it’s going to solve
    a problem well enough to be practically useful. It almost certainly won’t be integrated
    well enough into fastai or any other library that you can just press a few buttons
    and it will start working. Jeremy will not going to teach it unless he is very
    confident that it either is now or will be soon very practically useful technique.
    But it will require a lot of tweaking often and experimenting to get it to work
    on your particular problem because we don’t know the details well enough to know
    how to make it work for every data set or every example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是面向程序员的前沿深度学习，这意味着Jeremy通常不知道确切的最佳参数、架构细节等来解决特定问题。我们不一定知道它是否能够解决问题到足够实用的程度。它几乎肯定不会被很好地整合到fastai或任何其他库中，你不能只按几个按钮就开始工作。Jeremy不会教授它，除非他非常有信心，要么现在就是，要么很快就会成为非常实用的技术。但通常需要大量的调整和实验才能使其在你的特定问题上工作，因为我们不知道足够的细节来知道如何使其适用于每个数据集或每个示例。
- en: This means rather than Fastai and PyTorch being obscure black boxes which you
    just know these recipes for, you are going to learn the details of them well enough
    that you can customize them exactly the way you want, you can debug them, you
    can read the source code of them to see what’s happening. If you are not confident
    of object-oriented Python, then that is something you want to focus on studying
    during this course as we will not cover it in the class. But Jeremy will introduce
    some tools that he thinks are particularly helpful like the Python debugger, how
    to use your editor to jump through the code. In general, there will be a lot more
    detailed and specific code walkthroughs, coding technique discussions, as well
    as more detailed walkthroughs of papers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着与Fastai和PyTorch成为你只知道这些配方的晦涩黑匣子不同，你将学会足够了解它们的细节，以便可以按照自己的意愿定制它们，可以调试它们，可以阅读它们的源代码以了解发生了什么。如果你对面向对象的Python不自信，那么这是你在本课程中要专注学习的内容，因为我们不会在课堂上涵盖它。但Jeremy会介绍一些他认为特别有帮助的工具，比如Python调试器，如何使用你的编辑器跳转到代码中。总的来说，将会有更多详细和具体的代码演示，编码技术讨论，以及更详细的论文演示。
- en: Be aware of sample codes [[13:20](https://youtu.be/Z0ssNAbe81M?t=13m20s)]! The
    code academics have put up to go along with papers or example code somebody else
    has written on github, Jeremy nearly always find there is some massive critical
    flaw, so be careful of taking code from online resources and be ready to do some
    debugging.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意示例代码[13:20]！学术界提供的代码与论文配套或其他人在github上编写的示例代码，Jeremy几乎总是发现有一些重大的关键缺陷，所以小心从在线资源中获取代码，并准备做一些调试。
- en: '**How to use notebooks [**[**14:17**](https://youtu.be/Z0ssNAbe81M?t=14m17s)**]**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用笔记本[14:17]
- en: Building your own box [[16:50](https://youtu.be/Z0ssNAbe81M?t=16m50s)]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 构建你自己的盒子[16:50]
- en: Reading papers [[21:37](https://youtu.be/Z0ssNAbe81M?t=21m37s)]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读论文[21:37]
- en: Each week, we will be implementing a paper or two. On the left is an extract
    from the paper that implements adam (you have also seen adam as a single excel
    formula on a spreadsheet). In academic papers, people love to use Greek letters.
    They also hate to refactor, so you will often see a page long formula where when
    you look at it carefully you’ll realize the same sub equation appears 8 times.
    Academic papers are a bit weird, but in the end, it’s the way that the research
    community communicates their findings so we need to learn to read them. A great
    thing to do is to take a paper, put in the effort to understand it, then write
    a blog where you explain it in code and normal English. Lots of people who do
    that end up getting quite a following, end up getting some pretty great job offers
    and so forth because it is such a useful skill to be able to show that you can
    understand these papers, implement them in code, and explain them in English.
    It is very hard to read or understand something you cannot vocalize. So learn
    Greek letters!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每周，我们将实现一两篇论文。左边是一篇实现adam的论文摘录（你也在电子表格上看到过adam作为一个单独的excel公式）。在学术论文中，人们喜欢使用希腊字母。他们也不喜欢重构，所以你经常会看到一页长的公式，仔细看时你会意识到相同的子方程出现了8次。学术论文有点奇怪，但最终，这是研究界传达他们发现的方式，所以我们需要学会阅读它们。一个很好的做法是拿一篇论文，努力理解它，然后写一篇博客，在博客中用代码和普通英语解释它。许多这样做的人最终会得到相当多的关注，得到一些非常好的工作机会等，因为能够展示你能理解这些论文、在代码中实现它们并用英语解释它们是一种非常有用的技能。很难阅读或理解你无法口头表达的东西。所以学习希腊字母吧！
- en: More opportunities [[25:29](https://youtu.be/Z0ssNAbe81M?t=25m29s)]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更多机会
- en: Part 2’s Topics [[30:12](https://youtu.be/Z0ssNAbe81M?t=30m12s)]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的主题
- en: '**Generative Models**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成模型**'
- en: 'In part 1, the output of our neural network was generally a number or a category,
    where else, the outputs of a lot of the things in part 2 are going to be a whole
    a lot of things like:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们的神经网络的输出通常是一个数字或一个类别，而在第二部分中，很多东西的输出将是很多东西，比如：
- en: the top left and bottom right location of every object in an image along with
    what that object is
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中每个对象的左上角和右下角位置以及该对象是什么
- en: a complete picture with the class of every single pixel in that picture
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一幅完整的图片，显示该图片中每个像素的类别
- en: an enhanced super resolution version of the input image
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像的增强超分辨率版本
- en: the entire original input paragraph translated into French
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个原始输入段落翻译成法语
- en: Vast majority of the data we will be looking at will be either text or image
    data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的绝大多数数据要么是文本数据，要么是图像数据。
- en: We will be looking at some larger datasets both in terms of the number of objects
    in the dataset and the size of each of those objects. For those of you that are
    working with limited computational resources, please don’t let that put you off.
    Feel free to replace it with something smaller and simpler. Jeremy actually wrote
    large amount of the course with no internet (in Point Leo) on a surface book 15
    inch. Pretty much all of this course works well on Windows on a laptop. You can
    always use smaller batch sizes, cut-down version of the dataset. But if you have
    the resources, you will get better results with bigger datasets when they are
    available.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看一些更大的数据集，无论是数据集中的对象数量还是每个对象的大小。对于那些使用有限计算资源的人，请不要因此而退缩。随时可以用更小更简单的东西替代。Jeremy实际上在没有互联网的情况下（在Point
    Leo）用15英寸的surface book写了大部分课程。几乎所有这门课程在Windows笔记本电脑上都能很好地运行。你可以始终使用更小的批量大小、精简版本的数据集。但如果你有资源，当可用时，使用更大的数据集会获得更好的结果。
- en: Object Detection [[35:32](https://youtu.be/Z0ssNAbe81M?t=35m32s)]
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测
- en: 'Two main differences from what we are used to:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们习惯的两个主要区别：
- en: '**1.We have multiple things that we are classifying.**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.我们正在对多个事物进行分类。**'
- en: This if not unheard of — we did that in the planet satellite data in part 1.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不罕见——我们在第一部分的星球卫星数据中做过这个。
- en: '**2\. Bounding boxes around what we are classifying.**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.我们正在对我们分类的事物周围加上边界框。**'
- en: A bounding box has a very specific definition which is it’s a rectangle and
    the rectangle has the object entirely fitting within it but it is no bigger than
    it has to be.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框有一个非常具体的定义，即它是一个矩形，矩形内的对象完全适合其中，但它不会比必须的更大。
- en: Our job will be to take data that has been labeled in this way and on data that
    is unlabeled to generate classes of the objects and each one of those their bounding
    boxes. One thing to note is that labeling this kind of data is generally more
    expensive [[37:09](https://youtu.be/Z0ssNAbe81M?t=37m09s)]. For object detection
    datasets, annotators are given a list of object classes and asked to find every
    single one of them of any type in a picture along with where they are. In this
    case why isn’t there a tree or jump labeled? That is because for this particular
    dataset, they were not one of the classes that annotators were asked to find and
    therefore not part of this particular problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作将是采用这种方式标记的数据，并在未标记的数据上生成对象的类别和每个对象的边界框。需要注意的一点是，标记这种数据通常更昂贵。对于目标检测数据集，标注者会得到一个对象类别列表，并被要求在图片中找到每一个类型的对象以及它们的位置。在这种情况下，为什么没有一个树或跳跃被标记呢？因为对于这个特定的数据集，标注者没有被要求找到它们，因此不是这个特定问题的一部分。
- en: 'Stages [[38:33](https://youtu.be/Z0ssNAbe81M?t=38m33s)]:'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段：
- en: Classify the largest object in each image.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个图像中最大的对象进行分类。
- en: Find the location of the largest object at each image.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到每个图像中最大对象的位置。
- en: Finally we will try and do both at the same time (i.e. label what it is and
    where it is for the largest object in the picture).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将尝试同时做两件事（即标记它是什么以及在图片中的位置）。
- en: '[Pascal Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)
    [[40:06](https://youtu.be/Z0ssNAbe81M?t=40m06s)]'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[帕斯卡笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)'
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may find a line `torch.cuda.set_device(1)` left behind which will give you
    an error if you only have one GPU. This is how you select a GPU when you have
    multiple, so just set it to zero or take out the line entirely.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会发现一行`torch.cuda.set_device(1)`被遗留下来，如果您只有一个GPU，这会导致错误。这是在您有多个GPU时选择GPU的方法，所以只需将其设置为零或完全删除该行。
- en: There is a number of standard object detection datasets just like ImageNet being
    a standard object classification dataset [[41:12](https://youtu.be/Z0ssNAbe81M?t=41m12s)].
    The classic ImageNet equivalent is Pascal VOC.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 就像ImageNet是一个标准的对象分类数据集一样，还有许多标准的目标检测数据集[[41:12](https://youtu.be/Z0ssNAbe81M?t=41m12s)]。经典的ImageNet等价物是Pascal
    VOC。
- en: Pascal VOC
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pascal VOC
- en: We will be looking at the [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)
    dataset. It’s quite slow, so you may prefer to download from [this mirror](https://pjreddie.com/projects/pascal-voc-dataset-mirror/).
    There are two different competition/research datasets, from 2007 and 2012\. We’ll
    be using the 2007 version. You can use the larger 2012 for better results, or
    even combine them [[42:25](https://youtu.be/Z0ssNAbe81M?t=42m25s)](but be careful
    to avoid data leakage between the validation sets if you do this).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)数据集。这个数据集相当慢，所以您可能更喜欢从[这个镜像](https://pjreddie.com/projects/pascal-voc-dataset-mirror/)下载。有两个不同的竞赛/研究数据集，分别来自2007年和2012年。我们将使用2007年的版本。您可以使用更大的2012年版本获得更好的结果，甚至可以将它们结合起来[[42:25](https://youtu.be/Z0ssNAbe81M?t=42m25s)]（但如果这样做，请注意避免验证集之间的数据泄漏）。
- en: Unlike previous lessons, we are using the python 3 standard library `pathlib`
    for our paths and file access. Note that it returns an OS-specific class (on Linux,
    `PosixPath`) so your output may look a little different [[44:50](https://youtu.be/Z0ssNAbe81M?t=44m50s)].
    Most libraries that take paths as input can take a pathlib object - although some
    (like `cv2`) can't, in which case you can use `str()` to convert it to a string.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的课程不同，我们在路径和文件访问中使用了Python 3标准库`pathlib`。请注意，它返回一个特定于操作系统的类（在Linux上是`PosixPath`），因此您的输出可能会有些不同[[44:50](https://youtu.be/Z0ssNAbe81M?t=44m50s)]。大多数以路径作为输入的库可以接受`pathlib`对象
    - 尽管有些（如`cv2`）不能，这种情况下可以使用`str()`将其转换为字符串。
- en: '[Pathlib Cheat Sheet](http://pbpython.com/pathlib-intro.html)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pathlib Cheat Sheet](http://pbpython.com/pathlib-intro.html)'
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**A little bit about generator [**[**43:23**](https://youtu.be/Z0ssNAbe81M?t=43m23s)**]:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于生成器的一点说明[**[**43:23**](https://youtu.be/Z0ssNAbe81M?t=43m23s)**]：**'
- en: Generator is something in Python 3 which you can iterate over.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是Python 3中的一种可以迭代的东西。
- en: '`for i in PATH.iterdir(): print(i)`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`for i in PATH.iterdir(): print(i)`'
- en: '`[i for i in PATH.iterdir()]` (list comprehension)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[i for i in PATH.iterdir()]`（列表推导）'
- en: '`list(PATH.iterdir())` (turn a generator into a list)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`list(PATH.iterdir())`（将生成器转换为列表）'
- en: The reason that things generally return generators is that if the directory
    had 10 million items in, you don’t necessarily want 10 million long list. Generator
    lets you do things “lazily”.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常返回生成器的原因是，如果目录中有1000万个项目，您不一定希望有1000万个长列表。生成器让您可以“懒惰”地执行操作。
- en: Loading annotations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载注释
- en: As well as the images, there are also *annotations* — *bounding boxes* showing
    where each object is. These were hand labeled. The original version were in XML
    [[47:59](https://youtu.be/Z0ssNAbe81M?t=47m59s)], which is a little hard to work
    with nowadays, so we uses the more recent JSON version which you can download
    from [this link](https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像外，还有*注释* - 显示每个对象位置的*边界框*。这些是手工标记的。原始版本是XML格式[[47:59](https://youtu.be/Z0ssNAbe81M?t=47m59s)]，这在现在有点难以处理，所以我们使用了更近期的JSON版本，您可以从[此链接](https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip)下载。
- en: You can see here how `pathlib` includes the ability to open files (amongst many
    other capabilities).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里看到`pathlib`包含打开文件的能力（以及许多其他功能）。
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here `/` is not divided by but it is path slash [[45:55](https://youtu.be/Z0ssNAbe81M?t=45m55s)].
    `PATH/` gets you children in that path. `PATH/’pascal_train2007.json’` returns
    a `pathlib` object which has an `open` method. This JSON file contains not the
    images but the bounding boxes and the classes of the objects.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`/`不是除法符号，而是路径斜杠[[45:55](https://youtu.be/Z0ssNAbe81M?t=45m55s)]。`PATH/`可以让您获取该路径中的子项。`PATH/’pascal_train2007.json’`返回一个`pathlib`对象，该对象具有一个`open`方法。这个JSON文件不包含图像，而是包含对象的边界框和类别。
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Annotations [[49:16](https://youtu.be/Z0ssNAbe81M?t=49m16s)]
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注释 [[49:16](https://youtu.be/Z0ssNAbe81M?t=49m16s)]
- en: '`bbox` : column, row (of top left), height, width'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox`：列，行（左上角），高度，宽度'
- en: '`image_id` : you’d have join this up with `trn_j[IMAGES]` (above) to find `file_name`
    etc.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_id`：您需要将其与`trn_j[IMAGES]`（上面）连接起来，以查找`file_name`等。'
- en: '`category_id` : see `trn_j[CATEGORIES]` (below)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`category_id`：查看`trn_j[CATEGORIES]`（下面）'
- en: '`segmentation` : polygon segmentation (we will be using them)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation`：多边形分割（我们将使用它们）'
- en: '`ignore` : we will ignore the ignore flags'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore`：我们将忽略忽略标志'
- en: '`iscrowd` : specifies that it is a crowd of that object, not just one of them'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iscrowd`：指定这是该对象的一群，而不仅仅是其中一个'
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Categories [[50:15](https://youtu.be/Z0ssNAbe81M?t=50m15s)]
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别 [[50:15](https://youtu.be/Z0ssNAbe81M?t=50m15s)]
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It’s helpful to use constants instead of strings, since we get tab-completion
    and don’t mistype.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常量而不是字符串很有帮助，因为我们可以获得制表符补全，不会输错。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Side Note: What people most comment on when they see Jeremy working in real
    time having seen his classes [**[**51:21**](https://youtu.be/Z0ssNAbe81M?t=51m21s)**]:**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**侧记：当人们实时看到Jeremy在工作时，看到他的课程后最常评论的是[**[**51:21**](https://youtu.be/Z0ssNAbe81M?t=51m21s)**]：**'
- en: “Wow, you actually don’t know what you are doing, do you”. 99% of the things
    he does don’t work and small percentage of things that do work end up here. He
    mentioned this because machine learning, particularly deep learning is incredibly
    frustrating [[51:45](https://youtu.be/Z0ssNAbe81M?t=51m45s)]. In theory, you just
    define the correct loss function and the flexible enough architecture, and you
    press train and you are done. But if that was actually all that took, then nothing
    would take any time. The problem is that all the steps along the way until it
    works, it doesn’t work. Like it goes straight to infinity, crashes with an incorrect
    tensor size, etc. He will endeavor to show you some kind of debugging techniques
    as we go, but it is one of the hardest things to teach. The main thing it requires
    is tenacity. The difference between the people who are super effective and the
    ones who do not seem to go very far has never been about intellect. It’s always
    been about sticking with it — basically never giving up. It’s particularly important
    with this kind of deep learning because you don’t get that continuous reward cycle
    [[53:04](https://youtu.be/Z0ssNAbe81M?t=53m04s)]. It’s a constant stream of doesn’t
    work, doesn’t work, doesn’t work, until eventually it does so it’s kind of annoying.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “哇，你实际上不知道自己在做什么，是吧”。他做的99%的事情都不起作用，而那些确实起作用的事情只占很小的比例。他提到这一点是因为机器学习，特别是深度学习，非常令人沮丧。理论上，您只需定义正确的损失函数和足够灵活的架构，然后按下训练按钮，就完成了。但如果那确实是所有需要的，那么什么都不会花费任何时间。问题在于一直到它起作用的所有步骤，它都不起作用。就像它直接进入无限大，崩溃并显示不正确的张量大小等。他将努力向您展示一些调试技术，但这是最难教授的事情之一。它需要的主要是坚韧不拔。那些非常有效的人和那些似乎走得不远的人之间的区别从来不是智力问题。它总是关于坚持下去
    - 基本上是永不放弃。这在这种深度学习中尤为重要，因为您不会得到持续的奖励循环。它是一种持续的不起作用，不起作用，不起作用，直到最终起作用的过程，所以有点烦人。
- en: Let’s take a look at the images [[53:45](https://youtu.be/Z0ssNAbe81M?t=53m45s)]
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们看看这些图像。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Creating a dictionary (key: image ID, value: annotations) [[54:16](https://youtu.be/Z0ssNAbe81M?t=54m16s)]'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建字典（键：图像ID，值：注释）
- en: Each image has a unique ID.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都有一个唯一的ID。
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A `defaultdict` is useful any time you want to have a default dictionary entry
    for new keys [[55:05](https://youtu.be/Z0ssNAbe81M?t=55m05s)]. If you try and
    access a key that doesn’t exist, it magically makes itself exist and it sets itself
    equal to the return value of the function you specify (in this case `lambda:[]`).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`defaultdict`在任何时候都很有用，当您想要为新键设置默认字典条目时。如果尝试访问不存在的键，则它会自动使其存在，并将其设置为您指定的函数的返回值（在本例中为`lambda:[]`）。'
- en: Here we create a dict from image IDs to a list of annotations (tuple of bounding
    box and class id).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个从图像ID到注释列表（边界框和类别ID的元组）的字典。
- en: We convert VOC’s height/width into top-left/bottom-right, and switch x/y coords
    to be consistent with numpy. If given datasets are in crappy formats, take a couple
    of moments to make things consistent and make them the way you want them to be
    [[1:01:24](https://youtu.be/Z0ssNAbe81M?t=1h1m24s)]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将VOC的高度/宽度转换为左上角/右下角，并切换x/y坐标以与numpy保持一致。如果给定的数据集格式不佳，请花一点时间使事情保持一致，并使它们成为您想要的方式。
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Variable naming, coding style philosophy, etc [**[**56:15**](https://youtu.be/Z0ssNAbe81M?t=56m15s)**−**[**59:33**](https://youtu.be/Z0ssNAbe81M?t=59m33s)**]**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量命名，编码风格哲学等**'
- en: '**example 1**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1**'
- en: '`[ 96, 155, 269, 350]` : a bounding box [[59:53](https://youtu.be/Z0ssNAbe81M?t=59m53s)].
    As you see above, when we created the bounding box, we did a couple of things.
    The first is we switched the x and y coordinates. The reason for this that in
    computer vision world, when you say “my screen is 640 by 480” that is width by
    height. Or else, the math world, when you say “my array is 640 by 480” it is rows
    by columns. So pillow image library tends to do things in width by height or columns
    by rows, and numpy is the opposite way around. The second is that we are going
    to do things by describing the top-left xy coordinate and the bottom right xy
    coordinate — rather than x, y, height, width.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[ 96, 155, 269, 350]`：一个边界框。正如您在上面看到的，当我们创建边界框时，我们做了几件事。首先是我们交换了x和y坐标。这样做的原因是，在计算机视觉世界中，当您说“我的屏幕是640乘以480”时，宽度是高度。或者，在数学世界中，当您说“我的数组是640乘以480”时，是行乘以列。因此，pillow图像库倾向于按宽度和高度或列和行进行操作，而numpy则相反。其次，我们将通过描述左上角xy坐标和右下角xy坐标来进行操作，而不是x、y、高度、宽度。'
- en: '`7` : class label / category'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`7`：类标签/类别'
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**example 2**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例2**'
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Some libs take VOC format bounding boxes, so this let’s us convert back when
    required [[1:02:23](https://youtu.be/Z0ssNAbe81M?t=1h2m23s)]:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有些库采用VOC格式的边界框，因此当需要时，我们可以将其转换回来：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will use fast.ai’s `open_image` in order to display it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用fast.ai的`open_image`来显示它：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Introduction to Integrated Development Environment (IDE) [**[**1:03:13**](https://youtu.be/Z0ssNAbe81M?t=1h3m13s)**]**'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**集成开发环境（IDE）简介**'
- en: 'You can use [Visual Studio Code](https://code.visualstudio.com/) (vscode —
    open source editor that comes with recent versions of Anaconda, or can be installed
    separately), or most editors and IDEs, to find out all about the `open_image`
    function. vscode things to know:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用[Visual Studio Code](https://code.visualstudio.com/)（vscode - 附带最新版本的Anaconda的开源编辑器，或者可以单独安装），或者大多数编辑器和IDE，了解有关`open_image`函数的所有信息。vscode需要知道的事项：
- en: Command palette (`Ctrl-shift-p`)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令面板（`Ctrl-shift-p`）
- en: Select interpreter (for fastai env)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择解释器（用于fastai环境）
- en: Select terminal shell
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择终端shell
- en: Go to symbol (`Ctrl-t`)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到符号（`Ctrl-t`）
- en: Find references (`Shift-F12`)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找引用（`Shift-F12`）
- en: Go to definition (`F12`)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到定义（`F12`）
- en: Go back (`alt-left`)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回（`alt-left`）
- en: View documentation
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看文档
- en: Hide sidebar (`Ctrl-b`)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏侧边栏（`Ctrl-b`）
- en: Zen mode (`Ctrl-k,z`)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禅模式（`Ctrl-k,z`）
- en: 'If you are using PyCharm Professional Edition on Mac like I am:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像我一样在Mac上使用PyCharm专业版：
- en: Command palette (`Shift-command-a`)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令面板（`Shift-command-a`）
- en: Select interpreter (for fastai env) (`Shift-command-a` and then look for “interpreter”)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择解释器（用于fastai环境）（`Shift-command-a`然后搜索“解释器”）
- en: Select terminal shell (`Option-F12` )
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择终端外壳（`Option-F12`）
- en: Go to symbol (`Option-command-shift-n` and type name of the class, function,
    etc. If it’s in camelcase or underscore separated, you can type in first few letters
    of each bit)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到符号（`Option-command-shift-n`并输入类名、函数名等。如果是驼峰式或下划线分隔的，您可以输入每个部分的前几个字母）
- en: Find references (`Option-F7`), next occurrence (`Option-command-⬇︎`), previous
    occurrence (`Option-command-⬆︎`)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找引用（`Option-F7`），下一个出现（`Option-command-⬇︎`），上一个出现（`Option-command-⬆︎`）
- en: Go to definition (`Command-b`)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到定义（`Command-b`）
- en: Go back (`Option-command-⬅︎`)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回（`Option-command-⬅︎`）
- en: View documentation
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看文档
- en: Zen mode (`Control-`-4–2` or search for “distraction free mode”)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禅模式（`Control-`-4-2`或搜索“无干扰模式”）
- en: Let’s talk about open_image [[1:10:52](https://youtu.be/Z0ssNAbe81M?t=1h10m52s)]
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们谈谈open_image [[1:10:52](https://youtu.be/Z0ssNAbe81M?t=1h10m52s)]
- en: Fastai uses OpenCV. TorchVision uses PyTorch tensors for data augmentations
    etc. A lot of people use Pillow `PIL`. Jeremy did a lot of testing of all of these
    and he found OpenCV was about 5 to 10 times faster than TorchVision. For the [planet
    satellite image competition](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space)
    [[1:11:55](https://youtu.be/Z0ssNAbe81M?t=1h11m55s)], TorchVision was so slow
    that they could only get 25% GPU utilization because they were doing a lot of
    data augmentation. Profiler showed that it was all in TorchVision.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Fastai使用OpenCV。TorchVision使用PyTorch张量进行数据增强等。很多人使用Pillow `PIL`。Jeremy对所有这些进行了大量测试，他发现OpenCV比TorchVision快5到10倍。对于[星球卫星图像竞赛](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space)
    [[1:11:55](https://youtu.be/Z0ssNAbe81M?t=1h11m55s)]，TorchVision非常慢，因为他们进行了大量的数据增强，只能利用25%的GPU利用率。分析器显示这一切都在TorchVision中。
- en: Pillow is quite a bit faster but it is not as fast as OpenCV and also is not
    nearly as thread-safe [[1:12:19](https://youtu.be/Z0ssNAbe81M?t=1h12m19s)]. Python
    has this thing called the global interpreter lock (GIL) which means that two thread
    can’t do pythonic things at the same time — which makes Python a crappy language
    for modern programming but we are stuck with it. OpenCV releases the GIL. One
    of the reasons fast.ai library is so fast is because it does not use multiple
    processors like every other library does for data augmentations — it actually
    does multiple threads. The reason it could do multiple thread is because it uses
    OpenCV. Unfortunately OpenCV has an inscrutable API and documentations are somewhat
    obtuse. That is why Jeremy tried to make it so that no one using fast.ai needs
    to know that it’s using OpenCV. You don’t need to know what flags to pass to open
    an image. You don’t need to know that if the reading fails, it doesn’t show an
    exception — it silently returns `None`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Pillow速度相当快，但不及OpenCV快，也远不及线程安全[[1:12:19](https://youtu.be/Z0ssNAbe81M?t=1h12m19s)]。Python有一个叫做全局解释器锁（GIL）的东西，这意味着两个线程不能同时执行Pythonic的事情
    —— 这使得Python成为现代编程的糟糕语言，但我们却被困在其中。OpenCV释放了GIL。fast.ai库之所以如此快，是因为它不像其他库那样为数据增强使用多个处理器
    —— 它实际上使用多个线程。它能够使用多个线程的原因是因为它使用了OpenCV。不幸的是，OpenCV有一个晦涩的API，文档有些晦涩。这就是为什么Jeremy试图让使用fast.ai的人不需要知道它正在使用OpenCV。您不需要知道要传递哪些标志来打开一个图像。您不需要知道如果读取失败，它不会显示异常
    —— 它会静默地返回`None`。
- en: Don’t start using PyTorch for your data augmentation or start bringing in Pillow
    — you will find suddenly things slow down horribly or the multi-threading won’t
    work anymore. You should stick to using OpenCV for your processing [[1:14:10](https://youtu.be/Z0ssNAbe81M?t=1h14m10s)]
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 不要开始使用PyTorch进行数据增强或引入Pillow —— 您会发现事情突然变得非常缓慢，或者多线程将不再起作用。您应该坚持使用OpenCV进行处理[[1:14:10](https://youtu.be/Z0ssNAbe81M?t=1h14m10s)]
- en: Using Matplotlib better [[1:14:45](https://youtu.be/Z0ssNAbe81M?t=1h14m45s)]
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好地使用Matplotlib [[1:14:45](https://youtu.be/Z0ssNAbe81M?t=1h14m45s)]
- en: Matplotlib is so named because it was originally a clone of Matlab’s plotting
    library. Unfortunately, Matlab’s plotting library is not great, but at that time,
    it was what everybody knew. At some point, the matplotlib folks realized that
    and added a second API which was an object-oriented API. Unfortunately, because
    nobody who originally learnt matplotlib learnt the OO API, they then taught the
    next generation of people the old Matlab style API. Now there are not many examples
    or tutorials that use the much better, easier to understand, and simpler OO API.
    Because plotting is so important in deep learning, one of the things we are going
    to learn in this class is how to use this API.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib之所以被命名为Matplotlib，是因为它最初是Matlab绘图库的一个克隆。不幸的是，Matlab的绘图库并不好，但那时候，这是每个人都知道的。在某个时候，Matplotlib的开发人员意识到了这一点，并添加了第二个API，即面向对象的API。不幸的是，因为最初学习Matplotlib的人没有学习过OO
    API，他们随后教导下一代人使用旧的Matlab风格API。现在几乎没有例子或教程使用更好、更容易理解和更简单的OO API。由于绘图在深度学习中非常重要，我们在这门课程中要学习的一件事就是如何使用这个API。
- en: '**Trick 1: plt.subplots [**[**1:16:00**](https://youtu.be/Z0ssNAbe81M?t=1h16m)**]**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**技巧1：plt.subplots [**[**1:16:00**](https://youtu.be/Z0ssNAbe81M?t=1h16m)**]**'
- en: Matplotlib’s `plt.subplots` is a really useful wrapper for creating plots, regardless
    of whether you have more than one subplot. Note that Matplotlib has an optional
    object-oriented API which I think is much easier to understand and use (although
    few examples online use it!)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib的`plt.subplots`是一个非常有用的包装器，用于创建图表，无论您是否有多个子图。请注意，Matplotlib有一个可选的面向对象的API，我认为这个API更容易理解和使用（尽管在线上很少有例子使用它！）
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It returns two things — you probably won’t care about the first one (Figure
    object), the second one is Axes object (or an array of them). Basically anywhere
    you used to say `plt.` something, you now say `ax.` something, and it will now
    do the plotting to that particular subplot. This is helpful when you want to plot
    multiple plots so you can compare next to each other.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回两个东西 —— 你可能不会关心第一个（图形对象），第二个是 Axes 对象（或其数组）。基本上，你以前在哪里说 `plt.` 什么，现在你说 `ax.`
    什么，它将绘制到特定的子图。当你想绘制多个图以便进行比较时，这很有帮助。
- en: '**Trick 2: Visible text regardless of background color [**[**1:17:59**](https://youtu.be/Z0ssNAbe81M?t=1h17m59s)**]**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**技巧 2：无论背景颜色如何都可见的文本 [1:17:59]**'
- en: A simple but rarely used trick to making text visible regardless of background
    is to use white text with black outline, or visa versa. Here’s how to do it in
    matplotlib.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使文本在任何背景下都可见的一个简单但很少使用的技巧是使用白色文本和黑色轮廓，或者反之。这是如何在 matplotlib 中做到的。
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that `*` in argument lists is the [splat operator](https://stackoverflow.com/questions/5239856/foggy-on-asterisk-in-python).
    In this case it's a little shortcut compared to writing out `b[-2],b[-1]`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意参数列表中的 `*` 是 [splat 操作符](https://stackoverflow.com/questions/5239856/foggy-on-asterisk-in-python)。在这种情况下，与写出
    `b[-2],b[-1]` 相比，这是一个小快捷方式。
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Packaging it all up [**[**1:21:20**](https://youtu.be/Z0ssNAbe81M?t=1h21m20s)**]**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**将所有内容打包起来 [1:21:20]**'
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When you are working with a new dataset, getting to the point that you can rapidly
    explore it pays off.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用新数据集时，快速探索它的能力是值得的。
- en: Largest item classifier [[1:22:57](https://youtu.be/Z0ssNAbe81M?t=1h22m57s)]
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大项目分类器 [1:22:57]
- en: Rather than trying to solve everything at once, let’s make continual progress.
    We know how to find the biggest object in each image and classify it, so let’s
    start from there. Jeremy’s approach to Kaggle competition is half an hour every
    day [[1:24:00](https://youtu.be/Z0ssNAbe81M?t=1h24m)]. At the end of that half
    hour, submit something and try to make it a little bit better than yesterday’s.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与其一次性解决所有问题，不如持续取得进展。我们知道如何找到每个图像中最大的对象并对其进行分类，所以让我们从那里开始。Jeremy 在 Kaggle 竞赛中的方法是每天半小时
    [1:24:00]。在那半小时结束时，提交一些东西，并尝试比昨天稍微好一点。
- en: The first thing we need to do is to go through each of the bounding boxes in
    an image and get the largest one. A *lambda function* is simply a way to define
    an anonymous function inline. Here we use it to describe how to sort the annotation
    for each image — by bounding box size (descending).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是遍历图像中的每个边界框并获取最大的边界框。*lambda 函数* 只是一种内联定义匿名函数的方式。在这里，我们用它来描述如何对每个图像的注释进行排序
    —— 按边界框大小（降序）。
- en: 'We subtract the upper left from the bottom right and multiply (`np.product`)
    the values to get an area `lambda x: np.product(x[0][-2:]-x[0][:2])`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从左上角减去右下角并乘以（`np.product`）值以获得一个面积 `lambda x: np.product(x[0][-2:]-x[0][:2])`。'
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Dictionary comprehension [**[**1:27:04**](https://youtu.be/Z0ssNAbe81M?t=1h27m04s)**]**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**字典推导式 [1:27:04]**'
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we have a dictionary from image id to a single bounding box — the largest
    for that image.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个从图像 ID 到单个边界框的字典 —— 该图像的最大边界框。
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You need to look at every stage when you have any kind of processing pipeline
    [[1:28:01](https://youtu.be/Z0ssNAbe81M?t=1h28m1s)]. Assume that everything you
    do will be wrong the first time you do it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有任何类型的处理管道时，你需要查看每个阶段 [1:28:01]。假设你做的每件事第一次都会出错。
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Often it’s easiest to simply create a CSV of the data you want to model, rather
    than trying to create a custom dataset [[1:29:06](https://youtu.be/Z0ssNAbe81M?t=1h29m06s)].
    Here we use Pandas to help us create a CSV of the image filename and class. `columns=[‘fn’,’cat’]`
    is there because dictionary does not have an order and the order of columns matters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最简单的方法是简单地创建要建模的数据的 CSV，而不是尝试创建自定义数据集 [1:29:06]。在这里，我们使用 Pandas 帮助我们创建一个图像文件名和类别的
    CSV。`columns=[‘fn’,’cat’]` 是因为字典没有顺序，列的顺序很重要。
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: From here it’s just like Dogs vs Cats!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始就像狗与猫！
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Let’s take a look at this [**[**1:30:48**](https://youtu.be/Z0ssNAbe81M?t=1h30m48s)**]**'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**让我们来看看这个 [1:30:48]**'
- en: One thing that is different is `crop_type`. The default strategy for creating
    224 by 224 image in fast.ai is to first resize it so that the smallest side is
    224\. Then to take a random squared crop during the training. During validation,
    we take the center crop unless we use data augmentation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不同的地方是 `crop_type`。在 fast.ai 中创建 224x224 图像的默认策略是首先调整大小，使最小边为 224。然后在训练期间随机取一个正方形裁剪。在验证期间，我们取中心裁剪，除非我们使用数据增强。
- en: For bounding boxes, we do not want to do that because unlike an image net where
    the thing we care about is pretty much in the middle and pretty big, a lot of
    the things in object detection is quite small and close to the edge. By setting
    `crop_type` to `CropType.NO`, it will not crop and therefore, to make it square,
    it squishes it [[1:32:09](https://youtu.be/Z0ssNAbe81M?t=1h32m9s)]. Generally
    speaking, a lot of computer vision models work a little bit better if you crop
    rather than squish, but they still work pretty well if you squish. In this case,
    we definitely do not want to crop, so this is perfectly fine.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边界框，我们不想这样做，因为与图像网不同，我们关心的东西基本上在中间且相当大，而在目标检测中，很多东西相当小且靠近边缘。通过将 `crop_type`
    设置为 `CropType.NO`，它将不会裁剪，因此，为了使其成为正方形，它会压缩它 [1:32:09]。一般来说，许多计算机视觉模型在裁剪而不是压缩时效果稍好一些，但如果你压缩，它们仍然效果很好。在这种情况下，我们绝对不想裁剪，所以这是完全可以的。
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Data loaders [[1:33:04](https://youtu.be/Z0ssNAbe81M?t=1h33m4s)]
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器 [1:33:04]
- en: You already know that inside of a model data object, we have bunch of things
    which include training data loader and training data set. The main thing to know
    about data loader is that it is an iterator that each time you grab the next iteration
    of stuff from it, you get a mini batch. The mini batch you get is of whatever
    size you asked for and by default the batch size is 64\. In Python, the way you
    grab the next thing from an iterator is with next `next(md.trn_dl)` but you can’t
    just do that. The reason you can’t say that is because you need to say “start
    a new epoch now”. In general, not just in PyTorch but for any Python iterator,
    you need to say “start at the beginning of the sequence please”. The say you do
    that is to use`iter(md.trn_dl)` which will grab an iterator out of `md.trn_dl`
    — specifically as we will learn later, it means that this class has to have defined
    an `__iter__` method which returns some different object which then has an `__next__`
    method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经知道，在模型数据对象内部，我们有一堆东西，包括训练数据加载器和训练数据集。关于数据加载器的主要知识点是，它是一个迭代器，每次从中获取下一个迭代的内容时，您会得到一个小批量。您获得的小批量是您请求的任何大小，默认情况下批量大小为64。在Python中，从迭代器中获取下一个内容的方法是使用`next(md.trn_dl)`，但您不能直接这样做。您不能这样说的原因是您需要说“现在开始一个新的时期”。通常情况下，不仅仅是在PyTorch中，对于任何Python迭代器，您需要说“请从序列的开头开始”。您这样做的方式是使用`iter(md.trn_dl)`，它将从`md.trn_dl`中获取一个迭代器
    —— 具体来说，正如我们稍后将学到的那样，这意味着这个类必须定义一个`__iter__`方法，该方法返回一些不同的对象，然后该对象具有一个`__next__`方法。
- en: 'If you want to grab just a single batch, this is how you do it (`x`: independent
    variable, `y`: dependent variable):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想获取一个批次，这是您的操作方法（`x`：自变量，`y`：因变量）：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We cannot send this straight to `show_image`[[1:35:30](https://youtu.be/Z0ssNAbe81M?t=1h35m30s)].
    For example, `x` is not a numpy array, not on CPU, and the shape is all wrong
    (`3x224x224`). Further more, they are not numbers between 0 and 1 because all
    of the standard ImageNet pre-trained models expect our data to have been normalized
    to have a zero mean and 1 standard deviation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接将其发送到`show_image`[[1:35:30](https://youtu.be/Z0ssNAbe81M?t=1h35m30s)]。例如，`x`不是一个numpy数组，不在CPU上，并且形状完全错误（`3x224x224`）。此外，它们不是介于0和1之间的数字，因为所有标准ImageNet预训练模型都期望我们的数据已经被标准化为具有零均值和1标准差。
- en: 'As you see, there is a whole bunch of things that has been done to the input
    to get it ready to be passed to a pre-trained model. So we have a function called
    `denorm` for denormalize and also fixes up dimension order etc. Since denormalization
    depends on the transform [[1:37:52](https://youtu.be/Z0ssNAbe81M?t=1h37m52s)],
    and dataset knows what transform was used to create it, so that is why you have
    to do `md.val_ds.denorm` and pass the mini-batch after turning it into numpy array:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，对输入进行了大量处理，以便准备传递给预训练模型。因此我们有一个名为`denorm`的函数用于反标准化，还可以修复维度顺序等。由于反标准化取决于转换[[1:37:52](https://youtu.be/Z0ssNAbe81M?t=1h37m52s)]，并且数据集知道用于创建它的转换，这就是为什么您需要执行`md.val_ds.denorm`并将小批量转换为numpy数组后传递：
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Training with ResNet34 [[1:38:36](https://youtu.be/Z0ssNAbe81M?t=1h38m36s)]
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ResNet34进行训练[[1:38:36](https://youtu.be/Z0ssNAbe81M?t=1h38m36s)]
- en: '[PRE27]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We intentionally remove the first few points and the last few points [[1:38:54](https://youtu.be/Z0ssNAbe81M?t=1h38m54s)],
    because often the last few points shoots so high up towards infinity that you
    can’t see anything so it is generally a good idea. But when you have very few
    mini-batches, it is not a good idea. When your LR finder graph looks like above,
    you can ask for more points on each end (you can also make your batch size really
    small):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意删除了前几个点和最后几个点[[1:38:54](https://youtu.be/Z0ssNAbe81M?t=1h38m54s)]，因为通常最后几个点会向无穷大飙升，以至于您无法看到任何东西，所以这通常是个好主意。但是当您只有很少的小批量时，这并不是一个好主意。当您的LR查找器图像看起来像上面时，您可以要求在每一端获取更多点（您还可以将批量大小设置得非常小）：
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Unfreeze a couple of layers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 解冻几层：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Unfreeze the whole thing:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 解冻整个模型：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Accuracy isn’t improving much — since many images have multiple different objects,
    it’s going to be impossible to be that accurate.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率没有太大改善 —— 由于许多图像具有多个不同的对象，要达到那么高的准确率几乎是不可能的。
- en: Let’s look at the result [[1:40:48](https://youtu.be/Z0ssNAbe81M?t=1h40m48s)]
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们看看结果[[1:40:48](https://youtu.be/Z0ssNAbe81M?t=1h40m48s)]
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'How to understand the unfamiliar code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如何理解陌生的代码：
- en: Run each line of code step by step, print out the inputs and outputs.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐行运行代码，打印输入和输出。
- en: '**Method 1** [[1:42:28](https://youtu.be/Z0ssNAbe81M?t=1h42m28s)]: You can
    take the contents of the loop, copy it, create a cell above it, paste it, un-indent
    it, set `i=0` and put them all in separate cells.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法1**[[1:42:28](https://youtu.be/Z0ssNAbe81M?t=1h42m28s)]：您可以获取循环的内容，复制它，创建一个在其上方的单元格，粘贴它，取消缩进，设置`i=0`并将它们放在单独的单元格中。'
- en: '**Method 2** [[1:43:04](https://youtu.be/Z0ssNAbe81M?t=1h43m4s)]: Use Python
    debugger'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法2**[[1:43:04](https://youtu.be/Z0ssNAbe81M?t=1h43m4s)]：使用Python调试器'
- en: You can use the python debugger `pdb` to step through code.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Python调试器`pdb`逐步执行代码。
- en: '`pdb.set_trace()` to set a breakpoint'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pdb.set_trace()`设置断点'
- en: '`%debug` magic to trace an error (after the exception happened)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%debug`魔术以跟踪错误（在异常发生后）'
- en: 'Commands you need to know:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解的命令：
- en: '`h` (help)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h`（帮助）'
- en: '`s` (step into)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s`（步入）'
- en: '`n` (next line / step over — you can also hit enter)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`（下一行/跳过 —— 您也可以按回车键）'
- en: '`c` (continue to the next breakpoint)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`（继续到下一个断点）'
- en: '`u` (up the call stack)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u`（向上调用堆栈）'
- en: '`d` (down the call stack)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d`（向下调用堆栈）'
- en: '`p` (print) — force print when there is a single letter variable that’s also
    a command.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`（打印） —— 当有一个单字母变量也是一个命令时，强制打印。'
- en: '`l` (list) — show the line above and below it'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l`（列出） —— 显示上面和下面的行'
- en: '`q` (quit) — very important'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q`（退出） —— 非常重要'
- en: '**Comment [**[**1:49:10**](https://youtu.be/Z0ssNAbe81M?t=1h49m10s)**]:**`[IPython.core.debugger](http://ipython.readthedocs.io/en/stable/api/generated/IPython.core.debugger.html)`
    (on the right below) makes it all pretty:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释 [**[**1:49:10**](https://youtu.be/Z0ssNAbe81M?t=1h49m10s)**]**：`[IPython.core.debugger](http://ipython.readthedocs.io/en/stable/api/generated/IPython.core.debugger.html)`（右侧）使其看起来很漂亮：'
- en: Creating the bounding box [[1:52:51](https://youtu.be/Z0ssNAbe81M?t=1h52m51s)]
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建边界框[[1:52:51](https://youtu.be/Z0ssNAbe81M?t=1h52m51s)]
- en: Creating a bounding box around the largest object may seem like something you
    haven’t done before, but actually it is totally something you have done before.
    We can create a regression rather than a classification neural net. Classification
    neural net is the one that has a sigmoid or softmax output, and we use a cross
    entropy, binary cross entropy, or negative log likelihood loss function. That
    is basically what makes it classifier. If we don’t have the softmax or sigmoid
    at the end and we use mean squared error as a loss function, it is now a regression
    model which predict continuous number rather than a category. We also know that
    we can have multiple outputs like in the planet competition (multiple classification).
    What if we combine the two ideas and do a multiple column regression?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕最大对象创建边界框可能看起来像是您以前没有做过的事情，但实际上它完全是您以前做过的事情。我们可以创建一个回归而不是分类神经网络。分类神经网络是具有sigmoid或softmax输出的神经网络，我们使用交叉熵、二元交叉熵或负对数似然损失函数。这基本上是使其成为分类器的原因。如果我们在最后没有softmax或sigmoid，并且我们使用均方误差作为损失函数，那么现在它是一个预测连续数字而不是类别的回归模型。我们还知道我们可以有多个输出，就像在planet竞赛中一样（多分类）。如果我们将这两个想法结合起来并进行多列回归呢？
- en: 'This is where you are thinking about it like differentiable programming. It
    is not like “how do I create a bounding box model?” but it is more like:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您考虑它像可微编程的地方。不是“我如何创建一个边界框模型？”而是更像：
- en: We need four numbers, therefore, we need a neural network with 4 activations
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要四个数字，因此需要一个具有4个激活的神经网络
- en: For loss function, what is a function that when it is lower means that the four
    numbers are better? Mean squared loss function!
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于损失函数，什么样的函数在较低时意味着这四个数字更好？均方损失函数！
- en: That’s it. Let’s try it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。让我们试试看。
- en: Bbox only [[1:55:27](https://youtu.be/Z0ssNAbe81M?t=1h55m27s)]
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bbox only [[1:55:27](https://youtu.be/Z0ssNAbe81M?t=1h55m27s)]
- en: Now we’ll try to find the bounding box of the largest object. This is simply
    a regression with 4 outputs. So we can use a CSV with multiple ‘labels’. If you
    remember from part 1 to do a multiple label classification, your multiple labels
    have to be space separated, and the file name is comma separated.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试找到最大对象的边界框。这只是一个具有4个输出的回归。因此，我们可以使用具有多个“标签”的CSV。如果您还记得第1部分如何进行多标签分类，您的多个标签必须以空格分隔，并且文件名以逗号分隔。
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Training [[1:56:11](https://youtu.be/Z0ssNAbe81M?t=1h56m11s)]
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Training [[1:56:11](https://youtu.be/Z0ssNAbe81M?t=1h56m11s)]
- en: '[PRE34]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Set `continuous=True` to tell fastai this is a regression problem, which means
    it won't one-hot encode the labels, and will use MSE as the default crit.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将`continuous=True`设置为告诉fastai这是一个回归问题，这意味着它不会对标签进行独热编码，并且将使用MSE作为默认的crit。
- en: Note that we have to tell the transforms constructor that our labels are coordinates,
    so that it can handle the transforms correctly.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们必须告诉transforms构造函数我们的标签是坐标，以便它可以正确处理transforms。
- en: Also, we use CropType.NO because we want to ‘squish’ the rectangular images
    into squares, rather than center cropping, so that we don’t accidentally crop
    out some of the objects. (This is less of an issue in something like imagenet,
    where there is a single object to classify, and it’s generally large and centrally
    located).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用CropType.NO，因为我们希望将矩形图像“压缩”成正方形，而不是中心裁剪，以免意外裁剪掉一些对象。（在像imagenet这样的情况下，这不是太大的问题，因为有一个要分类的单个对象，通常很大且位于中心位置）。
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will look at `TfmType.COORD` next week, but for now, just realize that when
    we are doing scaling and data augmentation, that needs to happen to the bounding
    boxes, not just images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 下周我们将看一下`TfmType.COORD`，但现在，只需意识到当我们进行缩放和数据增强时，需要对边界框进行操作，而不仅仅是图像。
- en: '[PRE36]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s create a convolutional net based on ResNet34 [[1:56:57](https://youtu.be/Z0ssNAbe81M?t=1h56m57s)]:'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们基于ResNet34创建一个卷积网络[[1:56:57](https://youtu.be/Z0ssNAbe81M?t=1h56m57s)]：
- en: fastai lets you use a `custom_head` to add your own module on top of a convnet,
    instead of the adaptive pooling and fully connected net which is added by default.
    In this case, we don't want to do any pooling, since we need to know the activations
    of each grid cell.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: fastai允许您使用`custom_head`在卷积网络的顶部添加自己的模块，而不是默认添加的自适应池化和全连接网络。在这种情况下，我们不想进行任何池化，因为我们需要知道每个网格单元的激活。
- en: The final layer has 4 activations, one per bounding box coordinate. Our target
    is continuous, not categorical, so the MSE loss function used does not do any
    sigmoid or softmax to the module outputs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层有4个激活，每个激活对应一个边界框坐标。我们的目标是连续的，而不是分类的，因此使用的MSE损失函数不会对模块输出进行任何sigmoid或softmax处理。
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`Flatten()` : Normally the previous layer has `7x7x512` in ResNet34, so flatten
    that out into a single vector of length 2508'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flatten()`：通常在ResNet34中，前一层具有`7x7x512`，因此将其展平为长度为2508的单个向量。'
- en: '`L1Loss` [[1:58:22](https://youtu.be/Z0ssNAbe81M?t=1h58m22s)]: Rather than
    adding up the squared errors, add up the absolute values of the errors. It is
    normally what you want because adding up the squared errors really penalizes bad
    misses by too much. So L1Loss is generally better to work with.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L1Loss`[[1:58:22](https://youtu.be/Z0ssNAbe81M?t=1h58m22s)]：不是将平方误差相加，而是将误差的绝对值相加。这通常是您想要的，因为将平方误差相加会过度惩罚错误。因此，L1Loss通常更好地处理。'
- en: '[PRE38]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Validation loss is the mean of the absolute value with pixels were off by.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 验证损失是绝对值的平均值，像素偏离了。
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Take a look at the result [[1:59:18](https://youtu.be/Z0ssNAbe81M?t=1h59m18s)]
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 看一下结果[[1:59:18](https://youtu.be/Z0ssNAbe81M?t=1h59m18s)]
- en: '[PRE42]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will revise this more next week. Before this class, if you were asked “do
    you know how to create a bounding box model?”, you might have said “no, nobody’s
    taught me that”. But the question actually is:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下周进一步修改这个。在这堂课之前，如果有人问你“你知道如何创建一个边界框模型吗？”，你可能会说“不，没有人教过我”。但实际上问题是：
- en: Can you create a model with 4 continuous outputs? Yes.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以创建一个具有4个连续输出的模型吗？可以。
- en: Can you create a loss function that is lower if those 4 outputs are near to
    4 other numbers? Yes
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您能否创建一个损失函数，如果这4个输出接近另外4个数字，则较低？可以
- en: Then you are done.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你就完成了。
- en: As you look further down, it starts looking a bit crappy — anytime we have more
    than one object. This is not surprising. Overall, it did a pretty good job.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当你继续往下看时，它开始看起来有点糟糕 - 每当我们有多个对象时。这并不奇怪。总的来说，它做得相当不错。
