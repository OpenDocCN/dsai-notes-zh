- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2304.10031] Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.10031](https://ar5iv.labs.arxiv.org/html/2304.10031)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Architectures of Topological Deep Learning:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey of Message-Passing Topological Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: Mathilde Papillon^(∗1) ¹Department of Physics
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Electrical and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Santa Barbara
  prefs: []
  type: TYPE_NORMAL
- en: ³ Department of Data Science
  prefs: []
  type: TYPE_NORMAL
- en: University of San Francisco
  prefs: []
  type: TYPE_NORMAL
- en: '^∗Corresponding Author: papillon@ucsb.edu Sophia Sanborn² ¹Department of Physics'
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Electrical and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Santa Barbara
  prefs: []
  type: TYPE_NORMAL
- en: ³ Department of Data Science
  prefs: []
  type: TYPE_NORMAL
- en: University of San Francisco
  prefs: []
  type: TYPE_NORMAL
- en: '^∗Corresponding Author: papillon@ucsb.edu Mustafa Hajij³ ¹Department of Physics'
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Electrical and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Santa Barbara
  prefs: []
  type: TYPE_NORMAL
- en: ³ Department of Data Science
  prefs: []
  type: TYPE_NORMAL
- en: University of San Francisco
  prefs: []
  type: TYPE_NORMAL
- en: '^∗Corresponding Author: papillon@ucsb.edu Nina Miolane² ¹Department of Physics'
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Electrical and Computer Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Santa Barbara
  prefs: []
  type: TYPE_NORMAL
- en: ³ Department of Data Science
  prefs: []
  type: TYPE_NORMAL
- en: University of San Francisco
  prefs: []
  type: TYPE_NORMAL
- en: '^∗Corresponding Author: papillon@ucsb.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The natural world is full of complex systems characterized by intricate relations
    between their components: from social interactions between individuals in a social
    network to electrostatic interactions between atoms in a protein. Topological
    Deep Learning (TDL) provides a comprehensive framework to process and extract
    knowledge from data associated with these systems, such as predicting the social
    community to which an individual belongs or predicting whether a protein can be
    a reasonable target for drug development. TDL has demonstrated theoretical and
    practical advantages that hold the promise of breaking ground in the applied sciences
    and beyond. However, the rapid growth of the TDL literature for relational systems
    has also led to a lack of unification in notation and language across message-passing
    Topological Neural Network (TNN) architectures. This presents a real obstacle
    for building upon existing works and for deploying message-passing TNNs to new
    real-world problems. To address this issue, we provide an accessible introduction
    to TDL for relational systems, and compare the recently published message-passing
    TNNs using a unified mathematical and graphical notation. Through an intuitive
    and critical review of the emerging field of TDL, we extract valuable insights
    into current challenges and exciting opportunities for future development.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep learning, topology, message passing, graph, hypergraph, simplicial complex,
    cellular complex, combinatorial complex^†^†publicationid: pubid: 0000–0000/00$00.00 © 2023
    IEEE'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many natural systems as diverse as social networks [[1](#bib.bib1)] and proteins
    [[2](#bib.bib2)] are characterized by relational structure. This is the structure
    of interactions between components in the system, such as social interactions
    between individuals or electrostatic interactions between atoms. In Geometric
    Deep Learning [[3](#bib.bib3)], Graph Neural Networks (GNNs) [[4](#bib.bib4)]
    have demonstrated remarkable achievements in processing relational data using
    graphs—mathematical objects commonly used to encode pairwise relations.
  prefs: []
  type: TYPE_NORMAL
- en: However, the pairwise structure of graphs is limiting. Social interactions can
    involve more than two individuals, and electrostatic interactions more than two
    atoms. Topological Deep Learning (TDL) [[5](#bib.bib5), [6](#bib.bib6)] leverages
    more general abstractions to process data with higher-order relational structure.
    The theoretical guarantees [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)] of
    its models, Topological Neural Networks (TNNs), lead to state-of-the-art performance
    on many machine learning tasks [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]—and reveal high potential for the applied sciences and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the abstraction and fragmentation of mathematical notation across
    the TDL literature significantly limits the field’s accessibility, while complicating
    model comparison and obscuring opportunities for innovation. To address this,
    we present an intuitive and systematic comparison of published message-passing
    TNN architectures, heretofore referred to as TNNs. We contribute:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pedagogical resource accessible to newcomers interested in applying TNNs to
    real-world problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensive and critical review of TNNs, their implementations and practical
    applications, with equations rewritten in our notation available at https://github.com/awesome-tnns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of open research questions, challenges, and opportunities for innovation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By establishing a common and accessible language in the field, we hope to provide
    newcomers and experienced practitioners alike with a solid foundation for cutting-edge
    research in TDL.
  prefs: []
  type: TYPE_NORMAL
- en: Other literature reviews at the intersection of topology and machine learning
    have focused on data representation [[14](#bib.bib14)] and physics-inspired models [[15](#bib.bib15)].
    Message-passing TNNs are part of a broader spectrum of machine learning architectures
    leveraging topology. First surveyed in [[16](#bib.bib16)], this spectrum encompasses
    additional methods such as topological data analysis for machine learning. In
    such cases, features computed from techniques such as persistent homology are
    used to enhance data representation or model selection.
  prefs: []
  type: TYPE_NORMAL
- en: II Topological Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Topological Neural Networks (TNNs) are deep learning architectures that extract
    knowledge from data associated with topologically rich systems such as protein
    structures, city traffic maps, or citation networks. A TNN, like a GNN, is comprised
    of stacked layers that transform data into a series of features (Figure [1](#S2.F1
    "Figure 1 ‣ II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks")). Each layer
    leverages the fundamental concepts of data and computational domains, neighborhoods,
    and message passing—presented in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26ffcc604898b6af1afb2555d47cab91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Topological Neural Network: Data associated with a complex system
    are features defined on a data domain, which is preprocessed into a computational
    domain that encodes interactions between the system’s components with neighborhoods.
    The TNN’s layers use message passing to successively update features and yield
    an output, e.g. a categorical label in classification or a quantitative value
    in regression. The output represents new knowledge extracted from the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Topological Deep Learning (TDL), data are features defined on discrete domains
    [[5](#bib.bib5), [6](#bib.bib6)]. Traditional examples of discrete domains include
    sets and graphs (Figure [2](#S2.F2 "Figure 2 ‣ II-A Domains ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"), Left). A set is a collection of points called nodes
    without any additional structure. A graph is a set with edges that encode pairwise
    relations between nodes, representing either geometric proximity or more abstract
    relationships. For example, a graph may represent a protein, with nodes encoding
    its atoms and edges encoding the pairwise bonds between them. Alternatively, a
    graph may represent a social network, where nodes represent individuals and edges
    denote social relationships. The domains of TDL generalize the pairwise relations
    of graphs to part-whole and set-types relations that permit the representation
    of more complex relational structure (Figure [2](#S2.F2 "Figure 2 ‣ II-A Domains
    ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks"), Right) [[5](#bib.bib5)].
    Here, we describe the key attributes of each domain and highlight their suitability
    for different data types. We refer the reader to [[14](#bib.bib14)] and [[5](#bib.bib5)]
    for more extensive discussions.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture" height="221.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,221.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="193.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Beyond
    Graphs: The Domains of Topological Deep Learning Set + Pairwise Relations Graph:
    A set of points (nodes) connected with edges that denote pairwise relationships.
    Set + Part-Whole Relations Simplicial Complex (SC): A generalization of a graph
    in which three edges can form a triangular face, four triangles can form a tetrahedral
    volume, and so on. Edges only connect pairs of nodes. Cellular Complex (CC): A
    generalization of an SC in which faces, volumes, etc are not restricted to be
    triangles or tetrahedrons but may instead take any shape. Still, edges only connect
    pairs of nodes. Set + Set-Type Relations Hypergraph (HG): A generalization of
    a graph, in which higher-order edges called hyperedges can connect arbitrary sets
    of two or more nodes. Set + Part-Whole and Set-Type Relations Combinatorial Complex
    (CCC): A structure that combines features of HGs and CCs. Like an HG, edges may
    connect any number of nodes. Like a CC, cells can be combined to form higher-ranked
    structures.![Refer to caption](img/02ef42885fd7cd5fa4fc5fd3d9a25828.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Domains: Nodes in blue, (hyper)edges in pink, and faces in dark red.
    Figure adapted from [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplicial complexes (SCs) generalize graphs to incorporate hierarchical part-whole
    relations through the multi-scale construction of cells. Nodes are rank 0 cells
    that can be combined to form edges (rank 1 cells). Edges are, in turn, combined
    to form faces (rank 2 cells), which are combined to form volumes (rank 3 cells),
    and so on. As such, an SC’s faces must be triangles, volumes must be tetrahedrons,
    and so forth. SCs are commonly used to encode discrete representations of 3D geometric
    surfaces represented with triangular meshes (Figure [3](#S2.F3 "Figure 3 ‣ II-A
    Domains ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). They may also be used
    to represent more abstract relations; however, there is a risk of introducing
    spurious connections if the strict geometric constraints of an SC are not respected
    by the data—a point we elaborate on in Section [II-A2](#S2.SS1.SSS2 "II-A2 Limitations
    ‣ II-A Domains ‣ II Topological Neural Networks ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cellular complexes (CCs) generalize SCs such that cells are not limited to
    simplexes: faces can involve more than three nodes, volumes more than four faces,
    and so on. This flexibility endows CCs with greater expressivity than SCs [[8](#bib.bib8)].
    A practitioner should consider employing this domain when studying a system that
    features part-whole interactions between more than three nodes, such as a molecule
    with benzene rings (Figure [3](#S2.F3 "Figure 3 ‣ II-A Domains ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypergraphs (HGs) extend graphs in that their edges, called hyperedges, can
    connect more than two nodes. Connections in HGs represent set-type relationships,
    in which participation in an interaction is not implied by any other relation
    in the system. This makes HGs an ideal choice for data with abstract and arbitrarily
    large interactions of equal importance, such as semantic text and citation networks.
    Protein interaction networks (Figure [3](#S2.F3 "Figure 3 ‣ II-A Domains ‣ II
    Topological Neural Networks ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks")) also exhibit this property:
    an interaction between proteins requires a precise set of molecules—no more and
    no less. The interaction of Proteins A, B, and C does not imply an interaction
    between A and B on their own.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combinatorial complexes (CCCs) generalize CCs and HGs to incorporate both part-whole
    and set-type relationships [[11](#bib.bib11), [5](#bib.bib5)]. The benefit of
    this can be observed in the example of molecular representation. The strict geometric
    constraints of simplicial and cellular complexes are too rigid for capturing much
    of hierarchical structure observed in molecules. By contrast, the flexible but
    hierarchically ranked hyperedges of a combinatorial complex can capture the full
    richness of molecular structure, as depicted in Figure [3](#S2.F3 "Figure 3 ‣
    II-A Domains ‣ II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks"). This is the
    most recent and most general topological domain, introduced in 2022 by [[11](#bib.bib11)]
    and further theoretically established in [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d753fa0319e8ed403f32e90779c50ea4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of Data on Topological Domains. (a) Higher-order interactions
    in protein networks. (b) Limited molecular representation: rings can only contain
    three atoms. (c) Triangular mesh of a protein surface. (d) More flexible molecular
    representation, permitting the representation of any ring-shaped functional group.
    (e) Flexible mesh which includes arbitrarily shaped faces. (f) Fully flexible
    molecular representation, permitting the representation of the complex nested
    hierarchical structure characteristic of molecules and other natural systems.
    (g) Hierarchical higher-order interactions in protein networks.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Terminology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Across discrete domains, we use the term cell to denote any node or relation
    between nodes such as (hyper)edges, faces, or volumes. Cells possess two attributes:
    size—the number of cells it contains—and rank—where nodes are said to have rank
    0, edges and hyperedges rank 1, faces rank 2, and so on. The part-whole relationships
    of simplicial and cellular complexes impose a relationship between the rank of
    a cell and its size: cells of rank $r$ contains exactly (resp. at least) $r+1$
    cells of rank $r-1$: faces ($r=2$) contain exactly (resp. at least) three edges
    ($r-1=1$). By contrast, hypergraph cells do not encode part-whole relations and
    hyperedges may have any size. However, hypergraph cells are limited to ranks 0
    and 1\. A combinatorial complex is unrestricted in both rank and size: nodes have
    rank 0 and cells of any size $>$ 1 can have any rank.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an important distinction between the inherent domain of the data (the
    data domain) and the domain in which the data will be processed within a TNN:
    the computational domain. Data defined on a graph, for example, may be “lifted”
    (Figure [4](#S2.F4 "Figure 4 ‣ II-A1 Terminology ‣ II-A Domains ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")) to an alternative domain through a pre-processing
    stage (Figure [1](#S2.F1 "Figure 1 ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")).
    For instance, a protein originally given as the graph of its atoms (nodes) and
    covalent bounds (edges) may be lifted into a CC computational domain that explicitly
    represents its rings (faces). In this review, domain refers to the computational
    domain. Additionally, the computational domain may be dynamic, changing from layer
    to layer in a TNN.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture" height="82.73" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,82.73) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="55.18" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Dynamic Domains Static vs.
    Dynamic: In a TNN, a static domain is identical for each layer. For example, all
    three layers in Figure [1](#S2.F1 "Figure 1 ‣ II Topological Neural Networks ‣
    Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") operate on the same CCC, only features evolve across layers.
    A dynamic domain changes from layer to layer. Nodes can be added or removed, edges
    can be rewired, and so on.</foreignobject></g></g></svg>![Refer to caption](img/3af0cbe7ad2785ff22b1d939656055ee.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Lifting Topological Domains. (a) A graph is lifted to a hypergraph
    by adding hyperedges that connect groups of nodes. (b) A graph can be lifted to
    a cellular complex by adding faces of any shape. (c) Hyperedges can be added to
    a cellular complex to lift the structure to a combinatorial complex. Figure adopted
    from [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An important limitation of both SCs and CCs is that faces (and analogous higher-order
    structures) can only form rings; the nodes on the boundary of the face must be
    connected in pairs. In many cases, this requirement is too stringent and can introduce
    artificial connections to the domain [[17](#bib.bib17)]. For instance, lifting
    a citation network into an SC necessarily requires that any set of three co-authors
    having written a paper together (A, B, C) are also pairwise connected (A and B,
    A and C, B and C), even if no paper was ever exclusively authored by authors A
    and B, authors A and C, or authors B and C. [[17](#bib.bib17)] propose a “relaxed”
    definition of the SC that remedies this. They show how training a TNN on such
    a modified domain increases performance. We note that even with artificial connections,
    SCs and CCs allow TNNs to leverage richer topological structure and avoid computational
    problems faced by GNNs [[18](#bib.bib18)]. We further note that any topological
    domain is mathematically equivalent to a (possibly larger) graph [[19](#bib.bib19)].
    We choose to express domains in their form above in order to provide better intuition
    to newcomers and reflect the widely adopted approaches in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Features on a Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider a domain, denoted $\mathcal{X}$, encoding relationships between components
    of a system. Data on the domain are represented as features supported on the domain’s
    cells. Typically, features are vectors in $\mathbb{R}^{d}$ that encode attributes
    of each cell. For example, features may encode the atom (node), bond (edge), and
    functional group (face) types in a molecule. A feature associated with the interaction
    between a set of drugs (hyperedge) could indicate the probability of adverse reaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We denote with $\mathbf{h}_{x}^{t,(r)}$ a feature supported on the cell $x\in\mathcal{X}$
    at layer $t$ of the TNN, with $r$ indicating the rank of $x$ (Figure [5](#S2.F5
    "Figure 5 ‣ II-A3 Features on a Domain ‣ II-A Domains ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")). The domain is decomposed into ranks, with $X^{(r)}$,
    or $r$-skeleton, referring to all cells of rank $r$. Features can be categorical
    or quantitative. If the feature dimension varies across skeletons, the domain
    is heterogeneous.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture" height="71.84" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,71.84) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="44.28" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Heterogeneous Domains Homogeneity
    vs. Heterogeneity: In a heterogeneous domain, the dimension $d_{r}$ of a feature
    $\mathbf{h}_{x}^{(r)}$ depends on the rank $r$ of the cell $x$ supporting it.
    A homogeneous domain uses the same dimensionality $d$ for all ranks.</foreignobject></g></g></svg>![Refer
    to caption](img/089915783e08a51f6ef042e9e4b92b68.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Features on a Domain. Left: Features onto three cells—$x$, $y$, and
    $z$. Right: Skeletons for the entire complex: $X^{(0)}$ contains node features,
    $X^{(1)}$ contains edge features, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The features assigned to each cell may come directly from the data or be hand-designed
    by the practitioner. Alternatively, features can be assigned in a pre-processing
    stage using embedding methods, which compute cell feature vectors that encode
    the local structure of the space. For graphs, methods such as DeepWalk [[20](#bib.bib20)]
    and Node2Vec [[21](#bib.bib21)] are commonly used to embed nodes. Recent works
    have generalized these approaches to topological domains: Hyperedge2Vec [[22](#bib.bib22)]
    and Deep Hyperedge [[23](#bib.bib23)] for hypergraphs, Simplex2Vec [[24](#bib.bib24)]
    and k-Simplex2Vec [[25](#bib.bib25)] for simplicial complexes, and Cell2Vec [[26](#bib.bib26)]
    for cellular complexes.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Neighborhood Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ec96a538ca7319942af6443439dbe65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Incidence Matrices. Examples of an SC, a CC, an HG, and a CCC with
    their corresponding boundary matrices $B_{1}$ which map from 1-cells to 0-cells.
    The SC and CC maps are signed to encode edge orientation: the node appearing first
    in the arbitrary ordering (a,b,c,d) is always assigned -1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A TNN successively updates cell features throughout its layers by using a notion
    of nearness between cells: the neighborhood structure (Figure [1](#S2.F1 "Figure
    1 ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). Neighborhood structures
    are defined by boundary relations, which describe how cells of different ranks
    relate to each other. A cell $y$ of rank $r$ is said to be on the boundary of
    cell $x$ of rank $R$ if it is connected to $x$ and rank $r<R$. This relation is
    expressed as $y\prec x$. For example, a node connected to an edge is said to be
    on the boundary of that edge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Boundary relations are encoded in incidence matrices. Specifically, we denote
    with $B_{r}$ the matrix that records which (regular) cells of rank $r-1$ bound
    which cells of rank $r$ (Figure [6](#S2.F6 "Figure 6 ‣ II-B Neighborhood Structure
    ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). Formally, $B_{r}$
    is a matrix of size $n_{r-1}\crossproduct n_{r}$, with $n_{r}$ denoting the number
    of cells of rank $r\geq 1$, defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(B_{r})_{i,j}=\begin{cases}\pm 1&amp;x_{i}^{(r-1)}\prec x_{j}^{(r)}\\
    0&amp;\text{otherwise,}\end{cases}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{i}^{(r-1)}$, $x_{j}^{(r)}$ are two cells of ranks $r-1$ and $r$ respectively.
    The $\pm 1$ sign encodes a notion of orientation required for SCs and CCs [[27](#bib.bib27),
    [28](#bib.bib28)], and is always $+1$ for HGs and CCCs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incidence matrices can be used to encode the four most common neighborhood
    structures used in the literature, which we illustrate in Fig. [7](#S2.F7 "Figure
    7 ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") and define below. Here, $L_{\uparrow,0}$ denotes
    the typical graph Laplacian. Its higher order generalization, the $r$-Hodge Laplacian,
    is $H_{r}=L_{\downarrow,r}+L_{\uparrow,r}$ [[12](#bib.bib12), [29](#bib.bib29)].
    $D_{r}\in\mathbb{N}^{n_{r}\crossproduct n_{r}}$ denotes the degree matrix, a diagonal
    matrix representing the number of connections of $r$-cells with $(r+1)$-cells.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture" height="287.69" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,287.69) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="260.14" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Neighborhood
    Structures Boundary Adjacent Neighborhood $\mathcal{B}(y)=\{x\mid x\prec y\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of $y$-connected $x$ cells of next lower rank. The neighborhood is
    specified with the boundary matrix $B_{r}$. Example: The set of nodes $x$ connected
    to edge $y$. Co-Boundary Adjacent Neighborhood $\mathcal{C}(y)=\{x\mid y\prec
    x\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of $y$-connected $x$ cells of next higher rank. The neighborhood is
    specified with the co-boundary matrix $B^{T}_{r}$. Example: The set of edges $x$
    connected to node $y$. Lower Adjacent Neighborhood $\mathcal{L}_{\downarrow}(y)=\{x\mid\exists
    z$ s.t. $z\prec y$ and $z\prec x\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of $x$ cells that share a boundary $z$ with $y$. The neighborhood is
    specified with either the lower Laplacian matrix $L_{\downarrow,r}=B_{r}B_{r}^{T}$
    or the lower adjacency matrix $A_{\downarrow,r}=D_{r}-L_{\downarrow,r}$. Example:
    the set of edges $x$ that connect to any of the nodes $z$ that touch edge $y$.
    Upper Adjacent Neighborhood $\mathcal{L}_{\uparrow}(y)=\{x\mid\exists z$ s.t.
    $y\prec z$ and $x\prec z\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of $x$ cells that share a co-boundary $z$ with $y$. The neighborhood
    is specified with either the upper Laplacian matrix $L_{\uparrow,r}=B_{r+1}^{T}B_{r+1}$
    or the upper adjacency matrix $A_{\uparrow,r}=D_{r}-L_{\uparrow,r}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: The set of nodes $x$ that touch any of the edges $z$ that touch node
    $y$.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Message Passing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Message passing defines the computation performed by a single layer $t$ of
    the TNN. During message passing, each cell’s feature $\mathbf{h_{x}^{t,(r)}}$
    is updated to incorporate: (1) the features associated with cells in its neighborhood
    and (2) the layer’s learnable parameters denoted $\Theta^{t}$. The term “message
    passing” reflects that a signal is “traveling” through the network, passing between
    cells on paths laid out by the neighborhood structure. The output $\mathbf{h}^{t+1}$
    of layer $t$ becomes the input to layer $t+1$. In this way, deeper layers incorporate
    information from more distant cells, as information diffuses through the network.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 The Steps of Message Passing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We decompose message passing into four steps, adopted from the framework of
    [[5](#bib.bib5)]. Each step is represented with a different color—red, orange,
    green, or blue—illustrated in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of
    Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c44c9a046dccb95a60c0e6f6c8fbfcba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Neighborhood Structures: their neighborhood matrices and illustrations
    for a cell $x$ in the neighborhood of a cell $y$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43e0ef81dbb27691bbd2b0c0d738017e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Message passing steps: 1: Message (red), 2: Within-neighborhood aggregation
    (orange), 3: Between-neighborhood aggregation (green), 4: Update (blue). The scheme
    updates a feature $\mathbf{h}_{x}^{t,(r)}$ on a $r$-cell $x$ at layer $t$ (left
    column) into a new feature $\mathbf{h}_{x}^{t+1,(r)}$ on that same cell at the
    next layer $t+1$ (right column). Here, the scheme uses four neighborhood structures
    $\mathcal{N}_{k}$ for $k\in\{1,2,3,4\}$ (middle column). Figure adapted from [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture" height="346.18" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,346.18) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="318.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">The
    Steps of Message Passing 1\. Message (red): First, a message $m_{y\rightarrow
    x}^{\left(r^{\prime}\rightarrow r\right)}$ travels from a $r^{\prime}$-cell $y$
    to a $r$-cell $x$ through a neighborhood $k$ of $x$ denoted $\mathcal{N}_{k}(x)$:
    $\displaystyle{m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=M_{\mathcal{N}_{k}}\left(\mathbf{h}_{x}^{t,(r)},\mathbf{h}_{y}^{t,(r^{\prime})},\Theta^{t}\right).$
    (2) via the function $M_{\mathcal{N}_{k}}$ depicted in red in Figure [8](#S2.F8
    "Figure 8 ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"). Here, $\textbf{h}_{x}^{t,(r)}$ and $\textbf{h}_{y}^{t,(r^{\prime})}$
    are features of dimension $d_{r}$ and $d_{r^{\prime}}$ on cells $y$ and $x$ respectively,
    and $\Theta^{t}$ are learnable parameters. In the simplest case, this step looks
    like a neighborhood matrix $M$ propagating a feature $\mathbf{h}_{y}^{t,(r^{\prime})}$
    on $r^{\prime}$-cell $y$ to $r$-cell $x$ as: $\displaystyle m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ $\displaystyle=M_{xy}\cdot\textbf{h}_{y}^{t,(r^{\prime})}\cdot\Theta^{t},$
    (3) where $M_{xy}$ is the scalar entry of matrix $M$ at the row corresponding
    to cell $x$ and column corresponding to cell $y$ and $m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ and $\Theta$ is a $d_{r^{\prime}}\times d_{r}$ matrix. If $y$ is not
    in the neighborhood structure of $x$, then $M_{xy}$ will be 0, and $x$ cannot
    receive any message from $y$. 2\. Within-Neighborhood Aggregation (orange): Next,
    messages are aggregated across all cells $y$ belonging to the neighborhood $\mathcal{N}_{k}(x)$:
    $\displaystyle{m_{x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=AGG_{y\in\mathcal{N}_{k}(x)}m_{y\rightarrow
    x}^{\left(r^{\prime}\rightarrow r\right)},$ (4) resulting in the within-neighborhood
    aggregated message  $m_{x}^{\left(r^{\prime}\rightarrow r\right)}$. Here, $AGG$
    is an aggregation function, depicted in orange in Figure [8](#S2.F8 "Figure 8
    ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"), analogous to pooling in standard convolutional
    networks. 3\. Between-Neighborhood Aggregation (green): Then, messages are aggregated
    across neighborhoods in a neighborhood set $\mathcal{N}$: $\displaystyle{m_{x}^{(r)}}$
    $\displaystyle=AGG_{\mathcal{N}_{k}\in\mathcal{N}}m_{x}^{\left(r^{\prime}\rightarrow
    r\right)},$ (5) where AGG is a (potentially different) aggregation function depicted
    in green in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of Message Passing ‣
    II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks"), and
    $m_{x}^{(r)}$ is the message received by cell $x$ that triggers the update of
    its feature. 4\. Update (blue): Finally, the feature on cell $x$ is updated via
    a function $U$ depicted in blue in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps
    of Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks"),
    which may depend on the previous feature $\textbf{h}_{x}^{t,(r)}$ on cell $x$:
    $\displaystyle{\textbf{h}_{x}^{t+1,(r)}}$ $\displaystyle=U\left(\textbf{h}_{x}^{t,(r)},m_{x}^{(r)}\right),$
    (6) The result $\textbf{h}_{x}^{t+1,(r)}$ is the updated feature on cell $x$ that
    is input to layer $t+1$.'
  prefs: []
  type: TYPE_NORMAL
- en: In this review, we decompose the structure of TNN architectures proposed in
    the literature into these four message passing steps—a unified notational framework
    adopted from [[5](#bib.bib5)] that allows us to contrast existing approaches.
    Many architectures repeat steps and/or modify their order. We note that this conceptualization
    of message passing as a local, cell-specific operation is called the spatial approach
    [[30](#bib.bib30)]. In GNNs and TNNs alike, message passing can alternatively
    be expressed in its dual spectral form, using global Fourier analysis over the
    domain. For this review, we choose to write all equations in spatial form for
    intuitiveness and generality [[7](#bib.bib7), [26](#bib.bib26), [31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Tensor Diagrams
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We visually represent message passing schemes with an adapted version of the
    tensor diagram introduced in [[11](#bib.bib11)] and further developed in [[5](#bib.bib5)].
    A tensor diagram provides a graphical representation of a TNN architecture. Figure
    [9](#S2.F9 "Figure 9 ‣ II-C2 Tensor Diagrams ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") explains the recipe for constructing a tensor diagram
    from message passing steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8374ef7b8b5cf9e22a82977aaf9fdd1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Tensor Diagrams: a graphical notation for the four steps of a message
    passing scheme. A diagram depicts how a feature on cell $y$ at layer $t$, $\textbf{h}_{y}^{(t)}$,
    becomes a feature on cell $x$ at layer $t+1$, $\textbf{h}_{x}^{(t+1)}$.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 Types of Message Passing Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The message passing function $M_{\mathcal{N}_{k}}$ employed in Step 1 is defined
    by the practitioner. There are three kinds of functions commonly used in the literature,
    as outlined in Figure [10](#S2.F10 "Figure 10 ‣ II-C3 Types of Message Passing
    Functions ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    [[32](#bib.bib32)]. The variety used determines how layer parameters weight each
    incoming message from cell $y$ to cell $x$. The standard convolutional case multiplies
    each message by some learned scalar. The attentional convolutional case weights
    this multiplication depending on the features of the cells involved. The general
    case implements a potentially non-linear function that may or may not incorporate
    attention. Some schemes also make use of fixed, non-learned weights to assign
    different levels of importance to higher-order cells. Figure [10](#S2.F10 "Figure
    10 ‣ II-C3 Types of Message Passing Functions ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") illustrates each type with tensor diagrams.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04fe99c745afd218e78ae5489d0e3f7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Types of Message Passing Functions. In each case, a cell $x_{i}$
    (an edge) receives information from its various neighbors, cells $y_{j}$ (two
    nodes, an edge, and a face). The message received by cell $x_{i}$ from cell $y_{j}$
    is determined by a specific function $c(x_{i},y_{j})$, $a(x_{i},y_{j})$, or $g(x_{i},y_{j})$.
    Top: Each neighborhood cell $y_{j}$ sends a message to cell $x_{i}$. (Inspired
    by P. Veličković and [[32](#bib.bib32)]). Bottom: Illustration of the message-passing
    scheme above using tensor diagrams [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: III Literature Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now review the literature on topological neural networks (TNNs) over hypergraphs,
    simplicial complexes, cellular complexes, and combinatorial complexes, using the
    conceptual framework of Section [II](#S2 "II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks").
    We summarize and compare the TNNs in terms of their architectures (Section [III-A](#S3.SS1
    "III-A Architectures ‣ III Literature Review ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks")), the machine
    learning tasks to which they have been applied (Section [III-B](#S3.SS2 "III-B
    Tasks ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A
    Survey of Message-Passing Topological Neural Networks")), and their geometric
    properties (Section [III-C](#S3.SS3 "III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks")).'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature Review
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") summarizes TNN architectures according to the fundamental concepts
    introduced in Section [II](#S2 "II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks"),
    with the domain on the vertical axis, the message passing type on the horizontal
    axis, neighborhood structures and message passing equations visually represented
    with tensor diagrams. We share complete message passing equations for each architecture—decomposed
    according to the four steps introduced in Section [II-C1](#S2.SS3.SSS1 "II-C1
    The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") and rewritten in unifying notations —at github.com/awesome-tnns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2be5c7aecb4f969a76371d30e7bd452e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Topological Neural Networks (TNNs): A Graphical Literature Review.
    We organize TNNs according to the domain (rows) and the message passing type (columns).'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Hypergraphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of the domains considered here, hypergraph neural networks have been most extensively
    researched, and have been surveyed previously [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]. Many papers in the early
    literature do not use hypergraphs as the computational domain. Rather, algorithms
    like clique-expansion [[38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] are
    used to reduce hypergraphs to graphs, which are then processed by the model. This
    reduction adversely affects performance, as structural information is lost [[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. Many such graph-based models—including HGNN
    [[44](#bib.bib44)], HyperConv[[45](#bib.bib45)], HyperGCN [[46](#bib.bib46)],
    and HNHN [[10](#bib.bib10)]—are used as benchmarks for more recent models that
    do computationally operate on hypergraphs. Here, we focus on models that preserve
    hypergraph structure during learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many hypergraph models use a message passing scheme comprised of two phases,
    with information flowing from nodes to their hyperedges and then back to the nodes.
    We call this the two-phase scheme. The scheme appears in many tensor diagrams
    of Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature Review
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") where information flows from blue to pink (phase 1) and then
    from pink to blue (phase 2). The scheme is used in models with both standard and
    attentional message passing.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard
  prefs: []
  type: TYPE_NORMAL
- en: 'Of those using a standard message passing, the models from [[47](#bib.bib47)],
    [[48](#bib.bib48)], [[49](#bib.bib49)], and [[9](#bib.bib9)] use the two-phase
    scheme. [[48](#bib.bib48)] is unique in using a learnable weight matrix in the
    first phase of message passing. On the second phase, [[49](#bib.bib49)] and the
    UniGCN model from [[9](#bib.bib9)] are unique in using a fixed weight matrix on
    top of learnable weights. In [[50](#bib.bib50)], [[48](#bib.bib48)], and the UniGNN,
    UniSAGE, and UniGCNII models from [[9](#bib.bib9)], the initial feature on each
    node is recurrently used to update each incoming message—denoted with a looped
    black arrow in Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature
    Review ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"). We note that [[9](#bib.bib9)] systematically generalizes
    some of the most popular GNN architectures to hypergraphs with its unifying framework:
    UniGNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[10](#bib.bib10)], fixed weights are used on both the node to hyperedge
    and hyperedge to node phases. The paper AllSet [[51](#bib.bib51)] uses a similar
    structure while incorporating fully learnable multi-set functions for neighborhood
    aggregation, which imbues its TNNs with high expressivity and generality. EHNN
    [[52](#bib.bib52)] (excluded from Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") for its complexity; see written
    equations) proposes a maximally expressive model using sparse symmetric tensors
    to process data on hypergraphs with uniformly sized hyperedges.'
  prefs: []
  type: TYPE_NORMAL
- en: Attentional / General
  prefs: []
  type: TYPE_NORMAL
- en: The models from [[53](#bib.bib53)], [[54](#bib.bib54)], [[48](#bib.bib48)],
    [[36](#bib.bib36)], and the UniGAT model from [[9](#bib.bib9)] employ the two-phase
    scheme in concert with attentional message passing. The architectures of [[55](#bib.bib55)]
    and [[56](#bib.bib56)] apply multi-head attention. [[31](#bib.bib31)] adapts the
    two-phase scheme in order to update node features through two parallel paths.
    [[51](#bib.bib51)] and [[52](#bib.bib52)] offer transformer-based variants of
    their standard architectures, concurrently with [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Simplicial Complexes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Simplicial complexes were first explored from a signal processing perspective
    [[57](#bib.bib57)], with initial focus on edge flows [[58](#bib.bib58), [59](#bib.bib59)],
    Hodge Laplacians [[12](#bib.bib12), [60](#bib.bib60)], and convolution [[61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63)]. As a precursor to deep learning, [[64](#bib.bib64)]
    introduced $\mathcal{L}_{\downarrow,1}$ in HodgeNet to learn convolutions on edge
    features on graphs. This contrasts with former GNN approaches processing node
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Standard
  prefs: []
  type: TYPE_NORMAL
- en: '[[65](#bib.bib65)] (SNN) and [[66](#bib.bib66)] (SCCONV) first generalized
    the convolutional approach of [[64](#bib.bib64)] to features supported on faces
    and cells of higher ranks. Unlike HodgeNet, SNN and SCCONV use both $\mathcal{L}_{\downarrow,1}$
    and $\mathcal{L}_{\uparrow,1}$. In SNN [[65](#bib.bib65)] messages are not passed
    between adjacent ranks. By contrast, SCCONV uses independent boundary and co-boundary
    neighborhoods, hence incorporating features from adjacent ranks. [[67](#bib.bib67)]
    also makes use of this multi-neighborhood scheme for updating and classifying
    edge features. They also propose a single neighborhood scheme with an update including
    the initial cell’s feature. [[68](#bib.bib68)] and [[69](#bib.bib69)] devise schemes
    where messages coming from $\mathcal{L}_{\downarrow,1}$ and $\mathcal{L}_{\uparrow,1}$
    are weighted separately, providing greater learning flexibility. [[69](#bib.bib69)]
    allows features to travel multiple hops through the domain by using a polynomial
    form of the neighborhood structures, leveraging the simplicial convolutional filter
    from[[62](#bib.bib62)]. [[70](#bib.bib70)] extend this multiple-hop model with
    additional neighborhood structures. [[71](#bib.bib71)] used a modified version
    of $\mathcal{L}_{\downarrow}$ to find signals coiled around holes in the complex.
    BSCNet [[13](#bib.bib13)] combines node and edge-level shifting to predict links
    between nodes. This was the first model to pass messages between arbitrary ranks,
    leveraging a pseudo Hodge Laplacian.'
  prefs: []
  type: TYPE_NORMAL
- en: MPSN [[7](#bib.bib7)] explicitly details their message-passing scheme in the
    spatial domain, subsuming previous models described from a spectral approach.
    [[72](#bib.bib72)] introduces High Skip Networks (HSNs), in which each layer updates
    features through multiple sequential higher-order message passing steps, “skipping”
    it through higher ranks as a generalization of skip-connections in conventional
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Attentional / General
  prefs: []
  type: TYPE_NORMAL
- en: SAN [[73](#bib.bib73)], SAT [[74](#bib.bib74)], and SGAT [[75](#bib.bib75)]
    concurrently introduced attentional message passing networks on the simplicial
    domain. Each model makes use of a unique set of neighborhood structures and attention
    coefficients. SGAT is the only model as of yet developed for heterogeneous simplicial
    complexes of general rank. [[76](#bib.bib76)] introduces a variety of general
    message passing schemes with two neighborhood structures. [[7](#bib.bib7)] uses
    all four neighborhood structures, endowing each with a separate learnable matrix
    and general aggregation function.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Cellular Complexes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just as for simplicial complexes, cellular complex networks have been significantly
    influenced by work in signal processing [[12](#bib.bib12), [77](#bib.bib77), [78](#bib.bib78)].
    These works demonstrated that representing data in the CC domain yields substantially
    better results than the more rigid SC domain.
  prefs: []
  type: TYPE_NORMAL
- en: Standard
  prefs: []
  type: TYPE_NORMAL
- en: '[[78](#bib.bib78)] proposes theoretically possible message passing schemes
    for CCs inspired by works in the SC domain. As of yet, these models have not been
    implemented.'
  prefs: []
  type: TYPE_NORMAL
- en: Attentional / General
  prefs: []
  type: TYPE_NORMAL
- en: '[[26](#bib.bib26)] introduces the first TNNs to be theoretically defined on
    the CC domain. [[8](#bib.bib8)] was the first to implement and evaluate such a
    model, and demonstrated that TNNs on CCs outperform state-of-the-art graph-based
    models in expressivity and classification tests. The CAN model from [[79](#bib.bib79)]
    adapts a modified version of the message passing scheme from [[73](#bib.bib73)]
    onto the CC domain.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Combinatorial Complexes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The combinatorial complex domain was only recently mathematically defined by
    [[11](#bib.bib11)]. This work introduces four attentional message passing schemes
    for CCCs tailored to mesh and graph classification. A more extensive analysis
    is needed to quantify the advantages of this domain over other topological domains.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [I](#S3.T1 "TABLE I ‣ Hypergraphs. ‣ III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") reviews the tasks studied by
    each paper proposing TNNs. Tasks are first categorized into: node-level tasks
    assigning labels to nodes, as in node classification, regression or clustering;
    edge-level tasks assigning labels to edges, as in edge classification or link
    prediction; and complex-level tasks assigning labels to each complex as a whole,
    as in hypergraph classification. Tasks are additionally labeled according to their
    purpose (e.g. classification, regression, prediction). We also indicate the extent
    of benchmarking performed on each model and code availability.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Symmetries and Geometric Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Topological domains possess symmetries and other geometric properties that should
    be respected to ensure the quality of the features learned by a TNN [[80](#bib.bib80)].
    Here, we outline such properties harnessed by models in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: Hypergraphs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'On hypergraphs, the following symmetries are desirable:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Permutation Invariance: Relabeling the nodes and applying the TNN yields an
    output that is identical to the original output obtained without relabeling. This
    requires the aggregation functions to be permutation invariant, such as a mean
    or a sum [[47](#bib.bib47), [52](#bib.bib52), [51](#bib.bib51), [10](#bib.bib10),
    [71](#bib.bib71)]. This is also called hypergraph isomorphism invariance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE I: Applications of Topological Neural Networks (TNNs). We organize papers
    according to domain and task level, task purpose, and extent of benchmark testing
    (Graph: compared to graph-based models, GNN SOTA: compared to GNN state-of-the-art,
    TNN SOTA: compared to state-of-the-art on topological domain). We exclude papers
    without implementation, and use * to indicate that an implementation has not been
    shared.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain | Model | Task Level | Task Purpose | Comparisons |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  Node  |  Edge  |  Complex  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| HG | HyperSage [[47](#bib.bib47)] | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification (Inductive + Transductive) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | AllSet [[51](#bib.bib51)] | ✓ |  |  | Classification | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HyperGat [[54](#bib.bib54)] | ✓ |  |  | Classification | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HNHN [[10](#bib.bib10)] | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Dimensionality Reduction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HMPNN* [[31](#bib.bib31)] | ✓ |  |  | Classification | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | UniGNN [[9](#bib.bib9)] | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification (Inductive + Transductive) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | DHGNN [[53](#bib.bib53)] | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification (Multimodal) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | EHNN [[52](#bib.bib52)] | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Keypoint Matching &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HHNN [[55](#bib.bib55)] | ✓ |  |  | Link prediction | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HTNN [[56](#bib.bib56)] | ✓ |  |  | Classification | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SHARE* [[36](#bib.bib36)] | ✓ |  |  | Prediction | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | DHGCN* [[49](#bib.bib49)] |  |  | ✓ | Classification | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HGC-RNN* [[48](#bib.bib48)] | ✓ |  |  | Prediction | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '| SC | MPSN [[7](#bib.bib7)] |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Trajectory Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCCONV [[66](#bib.bib66)] |  |  | ✓ | Classification | Graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | BScNet [[13](#bib.bib13)] |  | ✓ |  | Link prediction | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SNN [[65](#bib.bib65)] |  | ✓ |  | Imputation | None |'
  prefs: []
  type: TYPE_TB
- en: '|  | SAN [[73](#bib.bib73)] |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Trajectory Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SAT [[74](#bib.bib74)] |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Trajectory Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | HSN* [[72](#bib.bib72)] | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, Link prediction, Vector embedding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCA* [[76](#bib.bib76)] |  |  | ✓ | Clustering | Graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dist2Cycle [[71](#bib.bib71)] |  | ✓ |  | Homology Localization | GNN
    SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SGAT [[75](#bib.bib75)] | ✓ |  |  | Classification | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCoNe [[68](#bib.bib68)] |  | ✓ |  | Trajectory Classification | TNN SOTA
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCNN* [[62](#bib.bib62)] |  | ✓ |  | Imputation | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCCNN [[70](#bib.bib70)] |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Link prediction, Trajectory Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCN [[67](#bib.bib67)] |  | ✓ |  | Classification | TNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '| CC | CWN [[8](#bib.bib8)] |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Classification, prediction, regression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '|  | CAN [[79](#bib.bib79)] |  |  | ✓ | Classification | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '| CCC | HOAN* [[11](#bib.bib11)] |  | ✓ | ✓ | Classification | GNN SOTA |'
  prefs: []
  type: TYPE_TB
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Global Neighborhood Invariance: The network’s representation of a node is invariant
    to hyperedge cardinality: a hyperedge connecting many nodes is weighted the same
    as a hyperedge connecting less nodes [[47](#bib.bib47)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Simplicial Complex.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For simplicial complexes, the following symmetries have been considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Permutation Invariance: Invariance to node relabeling; the same as for HGs.
    [[29](#bib.bib29), [68](#bib.bib68), [7](#bib.bib7)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orientation Equivariance: Changing the orientation of the simplicial complex
    (i.e. flipping the signs in the incidence matrix) re-orients the output of that
    network accordingly [[29](#bib.bib29), [68](#bib.bib68), [7](#bib.bib7)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simplicial Locality (geometric property): In each layer, messages are only
    passed between $r$-cells and $(r\pm 1)$-cells [[29](#bib.bib29)]. If that property
    is not verified, and messages can pass between any $r$- and $r^{\prime}$-cells,
    then the network has extended simplicial locality.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, simplicial awareness can be imposed, such that message passing
    on a simplicial complex with maximum cell rank $r$ depends on every rank $r^{\prime}\leq
    r$ [[68](#bib.bib68)].
  prefs: []
  type: TYPE_NORMAL
- en: Cellular Complex and Combinatorial Complex.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Permutation invariance is defined for CCs [[8](#bib.bib8)] and CCCs [[11](#bib.bib11)]
    just as for SCs and HGs. Beyond generalizing global neighborhood invariance to
    CCC, more research is required to understand the symmetries that can equip this
    general topological domain.
  prefs: []
  type: TYPE_NORMAL
- en: IV Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our literature review has revealed the diversity of TNN architectures as well
    as their main axes of comparison. Looking to the future, we highlight four salient
    opportunities for development.
  prefs: []
  type: TYPE_NORMAL
- en: Within-Domain and Between-Domain Benchmarking.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Table [I](#S3.T1 "TABLE I ‣ Hypergraphs. ‣ III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") shows that the domain choice
    strongly correlates with a TNN’s task level. This necessarily makes within-domain
    comparisons difficult, regardless of code sharing. We also emphasize that many
    TNNs are only benchmarked against graph-based models or early models in their
    respective domain, which makes between-domain comparisons equally difficult. As
    the field grows, improving within and between-domain benchmarking mechanisms will
    be critical to better informing model selection and quantifying progress.'
  prefs: []
  type: TYPE_NORMAL
- en: TNN Architectures on General Domains.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The diversity of implementations on HGs and SCs point to a strong potential
    for similar development in the cellular and combinatorial domains. For instance,
    only one attentional CC model has been proposed [[79](#bib.bib79)]. Moreover,
    any previously developed HG/SC/CC model can be reproduced in the CCC domain and,
    if desirable, improved with greater flexibility. Evaluating the impact of this
    added flexibility will directly characterize utility of richer topological structure
    in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to the Graph Literature.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The HG field’s ties to the graph community has led to GNN-based advancements
    not yet propagated to other domains. A first example are dynamic domains, successful
    with HGs for tasks like pose estimation [[81](#bib.bib81)], rail transit modeling
    [[82](#bib.bib82)], and co-authorship prediction [[53](#bib.bib53)]. No work in
    other discrete domains has explored dynamism. In addition, outside of the HG domain,
    TNNs are largely implemented as homogeneous networks. This leaves room for heterogeneous
    and non-Euclidean generalizations.
  prefs: []
  type: TYPE_NORMAL
- en: Going Deeper.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Over-smoothing occurs when a network is too effective at aggregating signal
    over multiple layers. This leads to very similar features across cells and poor
    performance on the downstream learning task. While this issue draws attention
    in the graph community [[83](#bib.bib83), [84](#bib.bib84), [18](#bib.bib18)],
    little of this work has been generalized to TNNs, causing them to remain mostly
    shallow. UniGCNII [[9](#bib.bib9)] achieves a 64-layer deep TNN by generalizing
    over-smoothing solutions from GNNs [[85](#bib.bib85)] to the HG domain. HSNs [[72](#bib.bib72)]
    generalize skip connections to allow signal to propagate further, but are still
    implemented as shallow networks.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have provided a comprehensive, intuitive and critical view
    of the advances in TNNs through unifying notations and graphical illustrations.
    We have characterized each neural network by its choice of data domain and its
    model, which we further specify through choice of neighboring structure(s) and
    message-passing scheme. We hope that this review will make this rich body of work
    more accessible to practitioners whose fields would benefit from topology-sensitive
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Science Foundation Grant Number 2134241.
  prefs: []
  type: TYPE_NORMAL
- en: VI References Section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D. Knoke and S. Yang, *Social network analysis*.   SAGE publications, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. Jha, S. Saha, and H. Singh, “Prediction of protein–protein interaction
    using graph neural networks,” *Scientific Reports*, vol. 12, no. 1, pp. 1–12,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” *arXiv preprint arXiv:2104.13478*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
    M. Sun, “Graph neural networks: A review of methods and applications,” *AI Open*,
    vol. 1, pp. 57–81, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, K. N.
    Ramamurthy, T. Birdal, T. Dey, S. Mukherjee, S. Samaga, N. Livesay, R. Walters,
    P. Rosen, and M. Schaub, “Topological deep learning: Going beyond graph data,”
    *arXiv preprint arXiv:1906.09068 (v3)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. Bodnar, “Topological deep learning: Graphs, complexes, sheaves,” Ph.D.
    dissertation, Apollo - University of Cambridge Repository, 2022\. [Online]. Available:
    https://www.repository.cam.ac.uk/handle/1810/350982'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Bodnar, F. Frasca, Y. Wang, N. Otter, G. F. Montufar, P. Lio, and M. Bronstein,
    “Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks,” in
    *International Conference on Machine Learning*.   PMLR, 2021, pp. 1026–1037.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Bodnar, F. Frasca, N. Otter, Y. Wang, P. Lio, G. F. Montufar, and M. Bronstein,
    “Weisfeiler and Lehman Go Cellular: CW Networks,” *Advances in Neural Information
    Processing Systems*, vol. 34, pp. 2625–2640, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. Huang and J. Yang, “Unignn: a unified framework for graph and hypergraph
    neural networks,” in *Proceedings of the Thirtieth International Joint Conference
    on Artificial Intelligence, IJCAI-21*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Dong, W. Sawin, and Y. Bengio, “Hnhn: Hypergraph networks with hyperedge
    neurons,” *ICML Graph Representation Learning and Beyond Workshop*, 2020\. [Online].
    Available: https://arxiv.org/abs/2006.12278'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, and K. N.
    Ramamurthy, “Higher-order attention networks,” *arXiv preprint arXiv:2206.00606
    (v1)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Barbarossa and S. Sardellitti, “Topological signal processing over
    simplicial complexes,” *IEEE Transactions on Signal Processing*, vol. 68, pp.
    2992–3007, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Chen, Y. R. Gel, and H. V. Poor, “Bscnets: Block simplicial complex
    neural networks,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 36, 2022, pp. 6333–6341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. Torres, A. S. Blevins, D. Bassett, and T. Eliassi-Rad, “The why, how,
    and when of representations for complex systems,” *SIAM Review*, vol. 63, no. 3,
    pp. 435–485, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Arruda, B. Franceschiello,
    I. Iacopini, S. Kéfi, V. Latora, Y. Moreno *et al.*, “The physics of higher-order
    interactions in complex systems,” *Nature Physics*, vol. 17, no. 10, pp. 1093–1098,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] F. Hensel, M. Moor, and B. Rieck, “A survey of topological machine learning
    methods,” *Frontiers in Artificial Intelligence*, vol. 4, 2021\. [Online]. Available:
    https://www.frontiersin.org/articles/10.3389/frai.2021.681108'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] R. Yang, F. Sala, and P. Bogdan, “Efficient representation learning for
    higher-order data with simplicial complexes,” in *Proceedings of the First Learning
    on Graphs Conference*, ser. Proceedings of Machine Learning Research, B. Rieck
    and R. Pascanu, Eds., vol. 198.   PMLR, 09–12 Dec 2022, pp. 13:1–13:21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. K. Rusch, M. M. Bronstein, and S. Mishra, “A survey on oversmoothing
    in graph neural networks,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] P. Veličković, “Message passing all the way up,” *arXiv preprint arXiv:2202.11097*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social
    representations,” in *Proceedings of the 20th ACM SIGKDD international conference
    on Knowledge discovery and data mining*, 2014, pp. 701–710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,”
    in *Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery
    and data mining*, 2016, pp. 855–864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Sharma, S. Joty, H. Kharkwal, and J. Srivastava, “Hyperedge2vec: Distributed
    representations for hyperedges,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Payne, “Deep hyperedges: a framework for transductive and inductive
    learning on hypergraphs,” *arXiv preprint arXiv:1910.02633*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. C. W. Billings, M. Hu, G. Lerda, A. N. Medvedev, F. Mottes, A. Onicas,
    A. Santoro, and G. Petri, “Simplex2vec embeddings for community detection in simplicial
    complexes,” *arXiv preprint arXiv:1906.09068*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] C. Hacker, “k-simplex2vec: a simplicial extension of node2vec,” *arXiv
    preprint arXiv:2010.05636*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Hajij, K. Istvan, and G. Zamzmi, “Cell Complex Neural Networks,” *NeurIPS
    2020 Workshop TDA and Beyond*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Aschbacher, “Combinatorial cell complexes,” in *Progress in Algebraic
    Combinatorics*.   Mathematical Society of Japan, 1996, pp. 1–80.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Klette, “Cell complexes through time,” in *Vision Geometry IX*, vol.
    4117.   SPIE, 2000, pp. 134–145.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, and S. Segarra, “Signal
    processing on higher-order networks: Livin’on the edge… and beyond,” *Signal Processing*,
    vol. 187, p. 108149, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International conference on
    machine learning*.   PMLR, 2017, pp. 1263–1272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Heydari and L. Livi, “Message passing neural networks for hypergraphs,”
    in *Proceedings of 31st International Conference on Artificial Neural Networks,
    Part II*, ser. Lecture Notes in Computer Science, E. Pimenidis, P. P. Angelov,
    C. Jayne, A. Papaleonidas, and M. Aydin, Eds., vol. 13530.   Springer, 2022, pp.
    583–592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Bronstein, “Beyond message passing: a physics-inspired paradigm for
    graph neural networks,” May 2022\. [Online]. Available: https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Ling, Z. Jinchuan, Z. Jinhao, Z. Wangtao, and Z. Xue, “A review of
    knowledge graphs: Representation, construction, reasoning and knowledge hypergraph
    theory [j],” *Computer Applications*, vol. 41, no. 08, pp. 2161–2186, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Y. Gao, Z. Zhang, H. Lin, X. Zhao, S. Du, and C. Zou, “Hypergraph learning:
    Methods and practices,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, no. 5, pp. 2548–2566, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] B.-D. Hu, X.-G. Wang, X.-Y. Wang, M.-L. Song, and C. Chen, “Survey on
    hypergraph learning: Algorithm classification and application analysis,” *Journal
    of Software*, vol. 33, no. 2, pp. 498–523, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Wang, K. Ding, Z. Zhu, and J. Caverlee, “Session-based recommendation
    with hypergraph attention networks,” in *Proceedings of the 2021 SIAM International
    Conference on Data Mining (SDM)*.   SIAM, 2021, pp. 82–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. T. Fischer, A. Frings, D. A. Keim, and D. Seebacher, “Towards a survey
    on static and dynamic hypergraph visualizations,” in *2021 IEEE visualization
    conference (VIS)*.   IEEE, 2021, pp. 81–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Y. Zien, M. D. Schlag, and P. K. Chan, “Multilevel spectral hypergraph
    partitioning with arbitrary vertex sizes,” *IEEE Transactions on computer-aided
    design of integrated circuits and systems*, vol. 18, no. 9, pp. 1389–1399, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, and S. Belongie,
    “Beyond pairwise clustering,” in *2005 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition (CVPR’05)*, vol. 2.   IEEE, 2005, pp. 838–845.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] D. Zhou, J. Huang, and B. Schölkopf, “Learning with hypergraphs: Clustering,
    classification, and embedding,” *Advances in neural information processing systems*,
    vol. 19, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Hein, S. Setzer, L. Jost, and S. S. Rangapuram, “The total variation
    on hypergraphs-learning on hypergraphs revisited,” *Advances in Neural Information
    Processing Systems*, vol. 26, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] G. Li, L. Qi, and G. Yu, “The z-eigenvalues of a symmetric tensor and
    its application to spectral hypergraph theory,” *Numerical Linear Algebra with
    Applications*, vol. 20, no. 6, pp. 1001–1029, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] I. E. Chien, H. Zhou, and P. Li, “$hs2$: Active learning over hypergraphs
    with pointwise and pairwise queries,” in *The 22nd International Conference on
    Artificial Intelligence and Statistics*.   PMLR, 2019, pp. 2466–2475.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, “Hypergraph neural networks,”
    in *Proceedings of the AAAI conference on artificial intelligence*, vol. 33, 2019,
    pp. 3558–3565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Bai, F. Zhang, and P. H. Torr, “Hypergraph convolution and hypergraph
    attention,” *Pattern Recognition*, vol. 110, p. 107637, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, and P. Talukdar,
    “Hypergcn: A new method for training graph convolutional networks on hypergraphs,”
    *Advances in neural information processing systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Arya, D. K. Gupta, S. Rudinac, and M. Worring, “Hypersage: Generalizing
    inductive representation learning on hypergraphs,” *arXiv preprint arXiv:2010.04558*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Yi and J. Park, “Hypergraph convolutional recurrent neural network,”
    in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 3366–3376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Wei, Y. Wang, M. Guo, P. Lv, X. Yang, and M. Xu, “Dynamic hypergraph
    convolutional networks for skeleton-based action recognition,” *arXiv preprint
    arXiv:2112.10570*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] D. Arya, S. Rudinac, and M. Worring, “Hyperlearn: a distributed approach
    for representation learning in datasets with many modalities,” in *Proceedings
    of the 27th ACM International Conference on Multimedia*, 2019, pp. 2245–2253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] E. Chien, C. Pan, J. Peng, and O. Milenkovic, “You are allset: A multiset
    function framework for hypergraph neural networks,” in *International Conference
    on Learning Representations*, 2022\. [Online]. Available: https://openreview.net/forum?id=hpBTIv2uy_E'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Kim, S. Oh, S. Cho, and S. Hong, “Equivariant hypergraph neural networks,”
    in *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XXI*.   Springer, 2022, pp. 86–103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J. Jiang, Y. Wei, Y. Feng, J. Cao, and Y. Gao, “Dynamic hypergraph neural
    networks,” in *Proceedings of International Joint Conferences on Artificial Intelligence*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] K. Ding, J. Wang, J. Li, D. Li, and H. Liu, “Be more with less: Hypergraph
    attention networks for inductive text classification,” in *Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.   Online:
    Association for Computational Linguistics, Nov. 2020, pp. 4927–4936\. [Online].
    Available: https://aclanthology.org/2020.emnlp-main.399'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Li, Z. Fan, J. Zhang, D. Shi, T. Xu, D. Yin, J. Deng, and X. Song,
    “Heterogeneous hypergraph neural network for friend recommendation with human
    mobility,” in *Proceedings of the 31st ACM International Conference on Information
    & Knowledge Management*, 2022, pp. 4209–4213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] M. Li, Y. Zhang, X. Li, Y. Zhang, and B. Yin, “Hypergraph transformer
    neural networks,” *ACM Transactions on Knowledge Discovery from Data (TKDD)*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania,
    J.-G. Young, and G. Petri, “Networks beyond pairwise interactions: structure and
    dynamics,” *Physics Reports*, vol. 874, pp. 1–92, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Jiang, L.-H. Lim, Y. Yao, and Y. Ye, “Statistical ranking and combinatorial
    hodge theory,” *Mathematical Programming*, vol. 127, no. 1, pp. 203–244, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. T. Schaub and S. Segarra, “Flow smoothing and denoising: Graph signal
    processing in the edge-space,” in *2018 IEEE Global Conference on Signal and Information
    Processing (GlobalSIP)*.   IEEE, 2018, pp. 735–739.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. T. Schaub, A. R. Benson, P. Horn, G. Lippner, and A. Jadbabaie, “Random
    walks on simplicial complexes and the normalized hodge 1-laplacian,” *SIAM Review*,
    vol. 62, no. 2, pp. 353–391, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] M. Yang, E. Isufi, M. T. Schaub, and G. Leus, “Finite impulse response
    filters for simplicial complexes,” in *2021 29th European Signal Processing Conference
    (EUSIPCO)*.   IEEE, 2021, pp. 2005–2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] ——, “Simplicial convolutional filters,” *IEEE Transactions on Signal Processing*,
    vol. 70, pp. 4633–4648, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] E. Isufi and M. Yang, “Convolutional filtering in simplicial complexes,”
    in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 5578–5582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. M. Roddenberry and S. Segarra, “Hodgenet: Graph neural networks for
    edge data,” in *2019 53rd Asilomar Conference on Signals, Systems, and Computers*.   IEEE,
    2019, pp. 220–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Ebli, M. Defferrard, and G. Spreemann, “Simplicial neural networks,”
    in *TDA & Beyond*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] E. Bunch, Q. You, G. Fung, and V. Singh, “Simplicial 2-complex convolutional
    neural networks,” in *TDA & Beyond*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Yang, F. Sala, and P. Bogdan, “Efficient representation learning for
    higher-order data with simplicial complexes,” in *Learning on Graphs Conference*.   PMLR,
    2022, pp. 13–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. M. Roddenberry, N. Glaze, and S. Segarra, “Principled simplicial neural
    networks for trajectory prediction,” in *International Conference on Machine Learning*.   PMLR,
    2021, pp. 9020–9029.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Yang, E. Isufi, and G. Leus, “Simplicial convolutional neural networks,”
    in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 8847–8851.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] M. Yang and E. Isufi, “Convolutional learning on simplicial complexes,”
    *arXiv preprint arXiv:2301.11163*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. D. Keros, V. Nanda, and K. Subr, “Dist2cycle: A simplicial neural network
    for homology localization,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 36, 2022, pp. 7133–7142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Hajij, K. N. Ramamurthy, A. Guzmán-Sáenz, and G. Za, “High Skip Networks:
    A Higher Order Generalization of Skip Connections,” in *ICLR 2022 Workshop on
    Geometrical and Topological Representation Learning*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] L. Giusti, C. Battiloro, P. Di Lorenzo, S. Sardellitti, and S. Barbarossa,
    “Simplicial attention networks,” *arXiv preprint arXiv:2203.07485*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. W. J. Goh, C. Bodnar, and P. Lio, “Simplicial attention networks,”
    *arXiv preprint arXiv:2204.09455*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. H. Lee, F. Ji, and W. P. Tay, “Sgat: Simplicial graph attention network,”
    in *International Joint Conference on Artificial Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Hajij, G. Zamzmi, T. Papamarkou, V. Maroulas, and X. Cai, “Simplicial
    complex representation learning,” in *Machine Learning on Graphs (MLoG) Workshop
    at 15th ACM International WSDM (2022) Conference, WSDM2022-MLoG ; Conference date:
    21-02-2022 Through 25-02-2022*, Jan. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Sardellitti, S. Barbarossa, and L. Testa, “Topological signal processing
    over cell complexes,” in *2021 55th Asilomar Conference on Signals, Systems, and
    Computers*.   IEEE, 2021, pp. 1558–1562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] T. M. Roddenberry, M. T. Schaub, and M. Hajij, “Signal processing on cell
    complexes,” in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*.   IEEE, 2022, pp. 8852–8856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, and
    S. Barbarossa, “Cell attention networks,” *arXiv preprint arXiv:2209.08179*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    vol. 34, no. 4, pp. 18–42, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Liu, P. Lv, Y. Zhang, J. Fu, J. Cheng, W. Li, B. Zhou, and M. Xu, “Semi-dynamic
    hypergraph neural network for 3d pose estimation.” in *IJCAI*, 2020, pp. 782–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Wang, Y. Zhang, Y. Wei, Y. Hu, X. Piao, and B. Yin, “Metro passenger
    flow prediction via dynamic hypergraph convolution networks,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 22, no. 12, pp. 7891–7903, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, “Measuring and relieving
    the over-smoothing problem for graph neural networks from the topological view,”
    in *Proceedings of the AAAI conference on artificial intelligence*, vol. 34, 2020,
    pp. 3438–3445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K. Oono and T. Suzuki, “Graph neural networks exponentially lose expressive
    power for node classification,” in *International Conference on Learning Representations*,
    2020\. [Online]. Available: https://openreview.net/forum?id=S1ldO2EFPr'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, “Simple and deep graph
    convolutional networks,” in *International conference on machine learning*.   PMLR,
    2020, pp. 1725–1735.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
