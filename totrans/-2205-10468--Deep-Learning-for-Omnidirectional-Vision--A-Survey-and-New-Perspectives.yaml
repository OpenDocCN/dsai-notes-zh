- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:46:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:46:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.10468] Deep Learning for Omnidirectional Vision: A Survey and New Perspectives'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.10468] å…¨æ™¯è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2205.10468](https://ar5iv.labs.arxiv.org/html/2205.10468)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2205.10468](https://ar5iv.labs.arxiv.org/html/2205.10468)
- en: \pdfximage
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfximage
- en: supplement.pdf
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: supplement.pdf
- en: 'Deep Learning for Omnidirectional Vision: A Survey and New Perspectives'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¨æ™¯è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’
- en: 'HaoÂ Ai^âˆ—, ZidongÂ Cao^âˆ—, JinjingÂ Zhu, HaotianÂ Bai, YuchengÂ Chen, andÂ  LinÂ Wang
    H. Ai and Z. Cao, J. Zhu, H. Bai, Y. Chen are with the Artificial Intelligence
    Thrust, The Hong Kong University of Science and Technology (HKUST), Guangzhou,
    China. E-mail: {haoai, zidongcao, jinjingzhu, haotianbai, yuchengchen}@ust.hk.
    L. Wang is with the Artificial Intelligence Thrust, HKUST, Guangzhou, and Dept.
    of Computer Science and Engineering, HKUST, Hong Kong SAR, China. E-mail: linwang@ust.hkManuscript
    received April 19, 2022; revised August 26, 2022. (^âˆ—Equal contribution, Corresponding
    author: Lin Wang)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Ai^âˆ—ï¼ŒZidong Cao^âˆ—ï¼ŒJinjing Zhuï¼ŒHaotian Baiï¼ŒYucheng Chenï¼Œå’Œ Lin Wang H. Ai
    å’Œ Z. Caoï¼ŒJ. Zhuï¼ŒH. Baiï¼ŒY. Chen åœ¨ä¸­å›½å¹¿å·é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰äººå·¥æ™ºèƒ½ç ”ç©¶ç»„å·¥ä½œã€‚ç”µå­é‚®ä»¶ï¼š{haoai, zidongcao,
    jinjingzhu, haotianbai, yuchengchen}@ust.hkã€‚L. Wang åœ¨é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰äººå·¥æ™ºèƒ½ç ”ç©¶ç»„å·¥ä½œï¼ŒåŒæ—¶ä¹Ÿæ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹ç³»çš„æˆå‘˜ã€‚ç”µå­é‚®ä»¶ï¼šlinwang@ust.hkã€‚æ‰‹ç¨¿æ”¶åˆ°æ—¶é—´ï¼š2022å¹´4æœˆ19æ—¥ï¼›ä¿®è®¢æ—¶é—´ï¼š2022å¹´8æœˆ26æ—¥ã€‚(^âˆ—å¹³ç­‰è´¡çŒ®ï¼Œé€šè®¯ä½œè€…ï¼šLin
    Wang)
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Omnidirectional image (ODI) data is captured with a $360^{\circ}\times 180^{\circ}$
    field-of-view, which is much wider than the pinhole cameras and contains richer
    spatial information than the conventional planar images. Accordingly, omnidirectional
    vision has attracted booming attention due to its more advantageous performance
    in numerous applications, such as autonomous driving and virtual reality. In recent
    years, the availability of customer-level $360^{\circ}$ cameras has made omnidirectional
    vision more popular, and the advance of deep learning (DL) has significantly sparked
    its research and applications. This paper presents a systematic and comprehensive
    review and analysis of the recent progress in DL methods for omnidirectional vision.
    Our work covers four main contents: (i) An introduction to the principle of omnidirectional
    imaging, the convolution methods on the ODI, and datasets to highlight the differences
    and difficulties compared with the 2D planar image data; (ii) A structural and
    hierarchical taxonomy of the DL methods for omnidirectional vision; (iii) A summarization
    of the latest novel learning strategies and applications; (iv) An insightful discussion
    of the challenges and open problems by highlighting the potential research directions
    to trigger more research in the community.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨æ™¯å›¾åƒï¼ˆODIï¼‰æ•°æ®ä»¥$360^{\circ}\times 180^{\circ}$çš„è§†åœºè§’æ•æ‰ï¼Œè¿™æ¯”é’ˆå­”ç›¸æœºè¦å®½å¾—å¤šï¼Œå¹¶ä¸”åŒ…å«æ¯”ä¼ ç»Ÿå¹³é¢å›¾åƒæ›´ä¸°å¯Œçš„ç©ºé—´ä¿¡æ¯ã€‚å› æ­¤ï¼Œç”±äºåœ¨è‡ªåŠ¨é©¾é©¶å’Œè™šæ‹Ÿç°å®ç­‰ä¼—å¤šåº”ç”¨ä¸­çš„æ›´æœ‰åˆ©æ€§èƒ½ï¼Œå…¨æ™¯è§†è§‰å¸å¼•äº†å¹¿æ³›å…³æ³¨ã€‚è¿‘å¹´æ¥ï¼Œå®¢æˆ·çº§åˆ«çš„$360^{\circ}$ç›¸æœºçš„æ™®åŠä½¿å…¨æ™¯è§†è§‰æ›´åŠ æµè¡Œï¼Œè€Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„è¿›æ­¥æ˜¾è‘—æ¨åŠ¨äº†å…¶ç ”ç©¶å’Œåº”ç”¨ã€‚æœ¬æ–‡å¯¹å…¨æ™¯è§†è§‰ä¸­æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿè€Œå…¨é¢çš„å›é¡¾å’Œåˆ†æã€‚æˆ‘ä»¬çš„å·¥ä½œæ¶µç›–äº†å››ä¸ªä¸»è¦å†…å®¹ï¼šï¼ˆiï¼‰ä»‹ç»å…¨æ™¯æˆåƒçš„åŸç†ã€ODIä¸Šçš„å·ç§¯æ–¹æ³•ä»¥åŠæ•°æ®é›†ï¼Œä»¥çªå‡ºä¸2Då¹³é¢å›¾åƒæ•°æ®ç›¸æ¯”çš„å·®å¼‚å’Œå›°éš¾ï¼›ï¼ˆiiï¼‰å…¨æ™¯è§†è§‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»“æ„åŒ–å’Œå±‚æ¬¡åŒ–åˆ†ç±»ï¼›ï¼ˆiiiï¼‰æœ€æ–°çš„åˆ›æ–°å­¦ä¹ ç­–ç•¥å’Œåº”ç”¨çš„æ€»ç»“ï¼›ï¼ˆivï¼‰é€šè¿‡çªå‡ºæ½œåœ¨çš„ç ”ç©¶æ–¹å‘ï¼Œæ·±å…¥è®¨è®ºæŒ‘æˆ˜å’Œå¼€æ”¾é—®é¢˜ï¼Œä»¥æ¿€å‘æ›´å¤šçš„ç¤¾åŒºç ”ç©¶ã€‚
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼š
- en: Omnidirectional vision, deep learning (DL), Survey, Introductory, Taxonomy
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨æ™¯è§†è§‰ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ï¼Œç»¼è¿°ï¼Œå…¥é—¨ï¼Œåˆ†ç±»
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: With the rapid development of 3D technology and the pursuit of realistic visual
    experience, research interest in computer vision has gradually shifted from traditional
    2D planar image data to omnidirectional image (ODI) data, also known as the 360^âˆ˜
    image, panoramic image, or spherical image data. ODI data captured by the $360^{\circ}$
    cameras yields a $360^{\circ}\times 180^{\circ}$ field-of-view (FoV), which is
    much wider than the pinhole cameras; therefore, it can capture the entire surrounding
    environment by reflecting richer spatial information than the conventional planar
    images. Due to the immersive experience and complete view, ODI data has been widely
    applied to numerous applications, e.g., augmented reality(AR)$/$virtual reality
    (VR), autonomous driving, and robot navigation. In general, raw ODI data is represented
    as, e.g., the equirectangular projection (ERP) or cubemap projection (CP) to be
    consistent with the imaging pipelinesÂ [[1](#bib.bib1)],Â [[2](#bib.bib2)]. As a
    novel data domain, ODI data has both domain-unique advantages (wide FoV of spherical
    imaging, rich geometric information, multiple projection types) and challenges
    (severe distortion in the ERP type, content discontinuities in the CP format).
    This renders the research on omnidirectional vision valuable yet challenging.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€3DæŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œå¯¹é€¼çœŸè§†è§‰ä½“éªŒçš„è¿½æ±‚ï¼Œè®¡ç®—æœºè§†è§‰çš„ç ”ç©¶å…´è¶£é€æ¸ä»ä¼ ç»Ÿçš„2Då¹³é¢å›¾åƒæ•°æ®è½¬å‘å…¨å‘å›¾åƒï¼ˆODIï¼‰æ•°æ®ï¼Œä¹Ÿè¢«ç§°ä¸º360Â°å›¾åƒã€å…¨æ™¯å›¾åƒæˆ–çƒé¢å›¾åƒæ•°æ®ã€‚ç”±360Â°æ‘„åƒæœºæ•è·çš„ODIæ•°æ®å…·æœ‰360Â°Ã—180Â°çš„è§†åœºï¼ˆFoVï¼‰ï¼Œæ¯”é’ˆå­”æ‘„åƒæœºè¦å®½å¾—å¤šï¼›å› æ­¤ï¼Œå®ƒå¯ä»¥é€šè¿‡åæ˜ æ¯”ä¼ ç»Ÿå¹³é¢å›¾åƒæ›´ä¸°å¯Œçš„ç©ºé—´ä¿¡æ¯æ¥æ•è·æ•´ä¸ªå‘¨å›´ç¯å¢ƒã€‚ç”±äºå…¶æ²‰æµ¸å¼ä½“éªŒå’Œå®Œæ•´è§†è§’ï¼ŒODIæ•°æ®å·²å¹¿æ³›åº”ç”¨äºè¯¸å¤šåº”ç”¨é¢†åŸŸï¼Œä¾‹å¦‚å¢å¼ºç°å®ï¼ˆARï¼‰/è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººå¯¼èˆªã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒåŸå§‹çš„ODIæ•°æ®è¢«è¡¨ç¤ºä¸ºä¾‹å¦‚ç­‰è·æŠ•å½±ï¼ˆERPï¼‰æˆ–ç«‹æ–¹ä½“æ˜ å°„ï¼ˆCPï¼‰ï¼Œä»¥ä¾¿ä¸æˆåƒæµç¨‹ä¿æŒä¸€è‡´Â [[1](#bib.bib1)],Â [[2](#bib.bib2)]ã€‚ä½œä¸ºä¸€ç§æ–°é¢–çš„æ•°æ®é¢†åŸŸï¼ŒODIæ•°æ®å…·æœ‰é¢†åŸŸç‹¬ç‰¹ä¼˜åŠ¿ï¼ˆçƒå½¢æˆåƒçš„å®½è§†åœºï¼Œä¸°å¯Œçš„å‡ ä½•ä¿¡æ¯ï¼Œå¤šç§æŠ•å½±ç±»å‹ï¼‰å’ŒæŒ‘æˆ˜ï¼ˆERPç±»å‹çš„ä¸¥é‡å¤±çœŸï¼ŒCPæ ¼å¼çš„å†…å®¹ä¸è¿ç»­ï¼‰ï¼Œè¿™ä½¿å¾—å…¨å‘è§†è§‰çš„ç ”ç©¶æ—¢æœ‰ä»·å€¼åˆå…·æŒ‘æˆ˜æ€§ã€‚
- en: 'Recently, the availability of customer-level $360^{\circ}$ cameras has made
    omnidirectional vision more popular, and the advance in deep learning (DL) has
    significantly promoted its research and applications. In particular, as a data-driven
    technology, the continual release of public datasets, e.g.,Â SUN360Â [[3](#bib.bib3)],
    Salient 360$!$Â [[4](#bib.bib4)], Stanford2D3DÂ [[5](#bib.bib5)], Pano-AVQAÂ [[6](#bib.bib6)]
    and PanoContextÂ [[7](#bib.bib7)], have rapidly enabled the DL methods to accomplish
    remarkable breakthroughs and often achieve the state-of-the-art (SoTA) performances
    on various omnidirectional vision tasks. Moreover, various deep neural network
    (DNN) models have been developed based on diverse architectures, ranging from
    convolutional neural networks (CNNs)Â [[8](#bib.bib8)], recurrent neural networks
    (RNNs)Â [[9](#bib.bib9)], generative adversarial networks (GANs)Â [[10](#bib.bib10)],
    graph neural networks (GNNs)Â [[11](#bib.bib11)], to vision transformers (ViTs)Â [[12](#bib.bib12)].
    In general, SoTA-DL-methods focus on four major aspects: (I) convolutional filters
    used to extract features from the ODI data (omnidirectional video (ODV) can be
    considered as a temporal set of ODIs), (II) network design by considering the
    input numbers and projection types, (III) novel learning strategies, and (IV)
    practical applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ¶ˆè´¹çº§360Â°æ‘„åƒæœºçš„æ™®åŠä½¿å…¨å‘è§†è§‰æ›´åŠ æµè¡Œï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„è¿›å±•æ˜¾è‘—æ¨åŠ¨äº†å…¶ç ”ç©¶å’Œåº”ç”¨ã€‚ç‰¹åˆ«æ˜¯ä½œä¸ºæ•°æ®é©±åŠ¨æŠ€æœ¯ï¼Œå…¬å…±æ•°æ®é›†çš„ä¸æ–­å‘å¸ƒï¼Œä¾‹å¦‚SUN360Â [[3](#bib.bib3)],
    Salient 360$!$Â [[4](#bib.bib4)], Stanford2D3DÂ [[5](#bib.bib5)], Pano-AVQAÂ [[6](#bib.bib6)]å’ŒPanoContextÂ [[7](#bib.bib7)]ï¼Œè¿…é€Ÿä½¿DLæ–¹æ³•åœ¨å„ç§å…¨å‘è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå¹¶ç»å¸¸è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒåŸºäºå„ç§æ¶æ„å¼€å‘äº†å„ç§æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å‹ï¼Œä»å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰Â [[8](#bib.bib8)],
    å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰Â [[9](#bib.bib9)], ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰Â [[10](#bib.bib10)], å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰Â [[11](#bib.bib11)],
    åˆ°è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰Â [[12](#bib.bib12)]ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ€å…ˆè¿›çš„DLæ–¹æ³•é›†ä¸­åœ¨å››ä¸ªä¸»è¦æ–¹é¢ï¼šï¼ˆIï¼‰åˆ©ç”¨å·ç§¯æ»¤æ³¢å™¨ä»ODIæ•°æ®ä¸­æå–ç‰¹å¾ï¼ˆå…¨å‘è§†é¢‘ï¼ˆODVï¼‰å¯ä»¥è¢«è§†ä¸ºä¸€ç»„æ—¶é—´ä¸Šçš„ODIï¼‰ï¼Œï¼ˆIIï¼‰é€šè¿‡è€ƒè™‘è¾“å…¥æ•°é‡å’ŒæŠ•å½±ç±»å‹è¿›è¡Œç½‘ç»œè®¾è®¡ï¼Œï¼ˆIIIï¼‰æ–°é¢–çš„å­¦ä¹ ç­–ç•¥ï¼Œä»¥åŠï¼ˆIVï¼‰å®é™…åº”ç”¨ã€‚
- en: 'This paper presents a systematic and comprehensive review and analysis of the
    recent progress in DL methods for omnidirectional vision. Previously, ZouÂ et al.Â [[13](#bib.bib13)]
    only focused on the algorithms of reconstructing room layout from a single ODI
    based on the Manhattan assumption. Similarly, Silveira et al.Â [[14](#bib.bib14)]
    merely reviewed recent 3D scene geometry recovery approaches based on the ODIs.
    Moreover, there exist some limited reviews of the FoV-adaptive video streaming
    methodsÂ [[15](#bib.bib15)],Â [[16](#bib.bib16)], especially on the topic of projection
    types, visual distortion problems, and efficient network structures. Recently,
    Chiariotti et al.Â [[17](#bib.bib17)] provided a more extensive review of the existing
    literature about ODV streaming systems. Unlike them, we highlight the importance
    of DL and probe the recent advances for omnidirectional vision, both methodically
    and comprehensively. The structural and hierarchical taxonomy proposed in this
    study is shown in Fig.Â [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ç³»ç»Ÿåœ°ç»¼è¿°å’Œåˆ†æäº†å…¨å‘è§†è§‰ä¸­æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•ã€‚ä¹‹å‰ï¼ŒZouç­‰äºº[[13](#bib.bib13)]ä»…å…³æ³¨äºåŸºäºæ›¼å“ˆé¡¿å‡è®¾ä»å•ä¸€ODIé‡å»ºæˆ¿é—´å¸ƒå±€çš„ç®—æ³•ã€‚åŒæ ·ï¼ŒSilveiraç­‰äºº[[14](#bib.bib14)]ä»…å›é¡¾äº†åŸºäºODIçš„è¿‘æœŸ3Dåœºæ™¯å‡ ä½•æ¢å¤æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å­˜åœ¨ä¸€äº›å¯¹FoVé€‚åº”çš„è§†é¢‘æµæ–¹æ³•[[15](#bib.bib15)],
    [[16](#bib.bib16)]çš„æœ‰é™ç»¼è¿°ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ•å½±ç±»å‹ã€è§†è§‰å¤±çœŸé—®é¢˜å’Œé«˜æ•ˆç½‘ç»œç»“æ„æ–¹é¢ã€‚æœ€è¿‘ï¼ŒChiariottiç­‰äºº[[17](#bib.bib17)]æä¾›äº†å¯¹ç°æœ‰ODVæµåª’ä½“ç³»ç»Ÿæ–‡çŒ®çš„æ›´å¹¿æ³›ç»¼è¿°ã€‚ä¸ä»–ä»¬ä¸åŒï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æ·±åº¦å­¦ä¹ çš„é‡è¦æ€§ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å…¨å‘è§†è§‰çš„æœ€æ–°è¿›å±•ï¼Œæ—¢ç³»ç»Ÿåˆå…¨é¢ã€‚æœ¬ç ”ç©¶æå‡ºçš„ç»“æ„æ€§å’Œå±‚çº§åˆ†ç±»è§å›¾[1](#S1.F1
    "å›¾1 â€£ 1 å¼•è¨€ â€£ å…¨å‘è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")ã€‚
- en: '![Refer to caption](img/d14ca56df766955d8c69df695efb43f7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/d14ca56df766955d8c69df695efb43f7.png)'
- en: 'Figure 1: Hierarchical and structural taxonomy of omnidirectional vision with
    deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„å…¨å‘è§†è§‰çš„å±‚çº§å’Œç»“æ„åˆ†ç±»ã€‚
- en: 'In summary, the major contributions of this study can be summarized as follows:
    (I) To the best of our knowledge, this is the first survey to comprehensively
    review and analyze the DL methods for omnidirectional vision, including the omnidirectional
    imaging principle, representation learning, datasets, a taxonomy, and applications,
    to highlight the differences and difficulties with the 2D planner image data.
    (II) We summarize most, if not all but representative, published top-tier conference/journal
    works (over 200 papers) in the last five years and conduct an analytical study
    of recent trends of DL for omnidirectional vision, both hierarchically and structurally.
    Moreover, we offer insights into the discussion and challenge of each category.
    (III) We summarize the latest novel learning strategies and potential applications
    for omnidirectional vision. (IV) As DL for omnidirectional vision is an active
    yet intricate research area, we provide insightful discussions of the challenges
    and open problems yet to be solved and propose the potential future directions
    to spur more in-depth research by the community. Meanwhile, we have summarized
    representative methods and their key strategies for some popular omnidirectional
    vision tasks in Table.Â [II](#S3.T2 "TABLE II â€£ 3.1.1 Image Generation â€£ 3.1 Image/Video
    Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), Table.Â [III](#S3.T3 "TABLE III â€£ 3.2.2
    Semantic Segmentation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional Vision Tasks
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), Table.Â [IV](#S3.T4
    "TABLE IV â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    Table.Â [V](#S3.T5 "TABLE V â€£ 3.3 3D Vision â€£ 3 Omnidirectional Vision Tasks â€£
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), and
    Table.Â [VI](#S3.T6 "TABLE VI â€£ 3.4.1 Saliency Prediction â€£ 3.4 Human Behavior
    Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"). To provide a better intra-task comparison,
    we present some representative methodsâ€™ quantitative and qualitative results on
    benchmark datasets and all statistics are derived from the original papers. Due
    to the lack of space, we show the experimental results in Sec.Â 2 of the suppl.
    material. (V) We create an open-source repository that provides a taxonomy of
    all the mentioned works and code links. We will keep updating our open-source
    repository with new works in this area and hope it can shed light on future research.
    The repository link is [https://github.com/VLISLAB/360-DL-Survey](https://github.com/VLISLAB/360-DL-Survey).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š(I) æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢å›é¡¾å’Œåˆ†æå…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„è°ƒæŸ¥ï¼ŒåŒ…æ‹¬å…¨æ™¯æˆåƒåŸç†ã€è¡¨ç¤ºå­¦ä¹ ã€æ•°æ®é›†ã€åˆ†ç±»æ³•å’Œåº”ç”¨ï¼Œçªæ˜¾äº†ä¸2Då¹³é¢å›¾åƒæ•°æ®çš„å·®å¼‚å’Œéš¾ç‚¹ã€‚(II)
    æˆ‘ä»¬æ€»ç»“äº†è¿‡å»äº”å¹´å¤§éƒ¨åˆ†ï¼ˆå¦‚æœä¸æ˜¯å…¨éƒ¨ï¼Œå°±æ˜¯ä»£è¡¨æ€§çš„ï¼‰é¡¶çº§ä¼šè®®/æœŸåˆŠè®ºæ–‡ï¼ˆè¶…è¿‡200ç¯‡ï¼‰ï¼Œå¹¶å¯¹å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ æœ€è¿‘è¶‹åŠ¿è¿›è¡Œäº†å±‚æ¬¡æ€§å’Œç»“æ„æ€§çš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªç±»åˆ«çš„è®¨è®ºå’ŒæŒ‘æˆ˜æä¾›äº†è§è§£ã€‚(III)
    æˆ‘ä»¬æ€»ç»“äº†å…¨æ™¯è§†è§‰çš„æœ€æ–°æ–°é¢–å­¦ä¹ ç­–ç•¥å’Œæ½œåœ¨åº”ç”¨ã€‚(IV) ç”±äºå…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªæ´»è·ƒä½†å¤æ‚çš„ç ”ç©¶é¢†åŸŸï¼Œæˆ‘ä»¬æä¾›äº†å¯¹å°šæœªè§£å†³çš„æŒ‘æˆ˜å’Œå¼€æ”¾é—®é¢˜çš„æ·±åˆ»è®¨è®ºï¼Œå¹¶æå‡ºäº†æ½œåœ¨çš„æœªæ¥æ–¹å‘ï¼Œä»¥æ¿€å‘ç¤¾åŒºæ›´æ·±å…¥çš„ç ”ç©¶ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åœ¨è¡¨æ ¼ä¸­æ€»ç»“äº†æŸäº›æµè¡Œå…¨æ™¯è§†è§‰ä»»åŠ¡çš„ä»£è¡¨æ€§æ–¹æ³•åŠå…¶å…³é”®ç­–ç•¥ã€‚[II](#S3.T2
    "TABLE II â€£ 3.1.1 å›¾åƒç”Ÿæˆ â€£ 3.1 å›¾åƒ/è§†é¢‘å¤„ç† â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ã€è¡¨æ ¼[III](#S3.T3
    "TABLE III â€£ 3.2.2 è¯­ä¹‰åˆ†å‰² â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ã€è¡¨æ ¼[IV](#S3.T4
    "TABLE IV â€£ 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡ â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ã€è¡¨æ ¼[V](#S3.T5
    "TABLE V â€£ 3.3 3Dè§†è§‰ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")å’Œè¡¨æ ¼[VI](#S3.T6 "TABLE VI â€£
    3.4.1 æ˜¾è‘—æ€§é¢„æµ‹ â€£ 3.4 äººç±»è¡Œä¸ºç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ã€‚ä¸ºäº†æä¾›æ›´å¥½çš„ä»»åŠ¡é—´æ¯”è¾ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€äº›ä»£è¡¨æ€§æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§ç»“æœï¼Œæ‰€æœ‰ç»Ÿè®¡æ•°æ®å‡æ¥è‡ªåŸå§‹è®ºæ–‡ã€‚ç”±äºç¯‡å¹…æœ‰é™ï¼Œå®éªŒç»“æœå±•ç¤ºåœ¨è¡¥å……ææ–™çš„ç¬¬2èŠ‚ä¸­ã€‚(V)
    æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¼€æºåº“ï¼Œæä¾›æ‰€æœ‰æåˆ°çš„å·¥ä½œçš„åˆ†ç±»åŠä»£ç é“¾æ¥ã€‚æˆ‘ä»¬å°†ä¸æ–­æ›´æ–°æˆ‘ä»¬çš„å¼€æºåº“ï¼Œå¢åŠ è¯¥é¢†åŸŸçš„æ–°æˆæœï¼Œå¹¶å¸Œæœ›å®ƒèƒ½ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¯ç¤ºã€‚å¼€æºåº“é“¾æ¥æ˜¯[https://github.com/VLISLAB/360-DL-Survey](https://github.com/VLISLAB/360-DL-Survey)ã€‚
- en: '![Refer to caption](img/e2cff39bf5f6f53db9d65364adae5ad3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/e2cff39bf5f6f53db9d65364adae5ad3.png)'
- en: 'Figure 2: Examples of representative $360^{\circ}$ cameras.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šä»£è¡¨æ€§$360^{\circ}$ç›¸æœºçš„ç¤ºä¾‹ã€‚
- en: '![Refer to caption](img/aa14709abcba3519e1e0b85b4ffe8909.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/aa14709abcba3519e1e0b85b4ffe8909.png)'
- en: 'Figure 3: Illustration of ERP, CP and tangent representation types.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šERPã€CP å’Œåˆ‡çº¿è¡¨ç¤ºç±»å‹çš„ç¤ºæ„å›¾ã€‚
- en: 'The rest of the paper is organized as follows. In Sec.Â [2](#S2 "2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    introduce the imaging principle of ODI, convolution methods for omnidirectional
    vision, and some representative datasets. Sec.Â [3](#S3 "3 Omnidirectional Vision
    Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    introduces the existing DL approaches for various tasks and provides taxonomies
    to categorize the relevant papers. Sec.Â [4](#S4 "4 Novel Learning Strategies â€£
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") covers
    novel learning paradigms for the tasks in omnidirectional vision, e.g., unsupervised
    learning, transfer learning, and reinforcement learning. Sec.Â [5](#S5 "5 Applications
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") then
    scrutinizes the applications, followed by Sec.Â [6](#S6 "6 Discussion and New Perspectives
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), where
    we discuss open problems and future directions. Finally, we conclude this paper
    in Sec.Â [7](#S7 "7 Conclusion â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚åœ¨ç¬¬[2](#S2 "2 èƒŒæ™¯ â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ODIçš„æˆåƒåŸç†ã€å…¨å‘è§†è§‰çš„å·ç§¯æ–¹æ³•ä»¥åŠä¸€äº›ä»£è¡¨æ€§æ•°æ®é›†ã€‚ç¬¬[3](#S3
    "3 å…¨å‘è§†è§‰ä»»åŠ¡ â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚ä»‹ç»äº†å„ç§ä»»åŠ¡çš„ç°æœ‰DLæ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸å…³è®ºæ–‡çš„åˆ†ç±»ã€‚ç¬¬[4](#S4 "4 æ–°é¢–å­¦ä¹ ç­–ç•¥
    â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚æ¶µç›–äº†å…¨å‘è§†è§‰ä»»åŠ¡çš„æ–°é¢–å­¦ä¹ èŒƒå¼ï¼Œä¾‹å¦‚æ— ç›‘ç£å­¦ä¹ ã€è¿ç§»å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç¬¬[5](#S5 "5 åº”ç”¨ â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚éšåå®¡è§†äº†åº”ç”¨ï¼Œæ¥ç€æ˜¯ç¬¬[6](#S6
    "6 è®¨è®ºä¸æ–°è§†è§’ â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¼€æ”¾é—®é¢˜å’Œæœªæ¥æ–¹å‘ã€‚æœ€åï¼Œç¬¬[7](#S7 "7 ç»“è®º â€£ å…¨å‘è§†è§‰ä¸­çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")èŠ‚æ€»ç»“äº†æœ¬æ–‡ã€‚
- en: 2 Background
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 èƒŒæ™¯
- en: 2.1 Omnidirectional Imaging
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 å…¨å‘æˆåƒ
- en: 2.1.1 Acquisition
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 è·å–
- en: 'A normal camera has an FoV less than $180^{\circ}$ and thus captures view at
    most a hemisphere. However, an ideal $360^{\circ}$ camera can capture lights falling
    on the focal point from all directions, making the projection plane a whole spherical
    surface. In practice, most $360^{\circ}$ cameras can not achieve it, which excludes
    top and bottom regions due to dead anglesÂ¹Â¹1[https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera](https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera).
    According to the number of lenses, $360^{\circ}$ cameras can be categorized into
    three types: (i) Cameras with one fisheye lens, which is impossible to cover the
    whole spherical surface. However, if the intrinsic and extrinsic parameters are
    known, an ODI can be achieved by projecting multiple images into a sphere and
    stitching them together; (ii) Cameras with dual fisheye lenses located at opposite
    positions, each of which covers over $180^{\circ}$ FoV, such as Insta360 ONEÂ²Â²2[https://www.insta360.com/product/insta360-one](https://www.insta360.com/product/insta360-one)
    and LG 360 CAMÂ³Â³3[https://www.lg.com/sg/lg-friends/lg-360-CAM](https://www.lg.com/sg/lg-friends/lg-360-CAM).
    This type of $360^{\circ}$ cameras have minimum demand for lenses, which are cheap
    and convenient, favoured by industries and customers. Images from the two cameras
    are then stitched together to obtain an omnidirectional image, but the stitching
    process might lead to edge blurring; (iii) Cameras with more than two lenses,
    such as Titan (eight lenses)â´â´4[https://www.insta360.com/product/insta360-titan/](https://www.insta360.com/product/insta360-titan/).
    In addition, GoPro Omniâµâµ5[https://gopro.com/en/us/news/omni-is-here](https://gopro.com/en/us/news/omni-is-here)
    is the first camera rig to place six regular cameras onto six faces of a cube
    and its synthesized results have higher precision and less blur in edges. This
    type of $360^{\circ}$ cameras are professional-level.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ™®é€šç›¸æœºçš„è§†åœºè§’å°äº $180^{\circ}$ï¼Œå› æ­¤æœ€å¤šåªèƒ½æ•æ‰åŠçƒçš„è§†å›¾ã€‚ç„¶è€Œï¼Œç†æƒ³çš„ $360^{\circ}$ ç›¸æœºå¯ä»¥æ•æ‰ä»æ‰€æœ‰æ–¹å‘å°„å‘ç„¦ç‚¹çš„å…‰çº¿ï¼Œä½¿å¾—æŠ•å½±å¹³é¢æˆä¸ºæ•´ä¸ªçƒé¢ã€‚åœ¨å®é™…ä¸­ï¼Œå¤§å¤šæ•°
    $360^{\circ}$ ç›¸æœºæ— æ³•å®ç°è¿™ä¸€ç‚¹ï¼Œç”±äºç›²åŒºæ’é™¤äº†é¡¶éƒ¨å’Œåº•éƒ¨åŒºåŸŸÂ¹Â¹1[https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera](https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera)ã€‚æ ¹æ®é•œå¤´æ•°é‡ï¼Œ$360^{\circ}$
    ç›¸æœºå¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼šï¼ˆiï¼‰å•é±¼çœ¼é•œå¤´ç›¸æœºï¼Œæ— æ³•è¦†ç›–æ•´ä¸ªçƒé¢ã€‚ç„¶è€Œï¼Œå¦‚æœå·²çŸ¥å†…å¤–å‚æ•°ï¼Œå¯ä»¥é€šè¿‡å°†å¤šå¼ å›¾åƒæŠ•å½±åˆ°çƒé¢ä¸Šå¹¶è¿›è¡Œæ‹¼æ¥æ¥å®ç°å…¨æ™¯å›¾åƒï¼›ï¼ˆiiï¼‰ä¸¤ä¸ªé±¼çœ¼é•œå¤´ä½äºå¯¹é¢çš„ä½ç½®çš„ç›¸æœºï¼Œæ¯ä¸ªé•œå¤´çš„è§†åœºè§’è¶…è¿‡
    $180^{\circ}$ï¼Œå¦‚ Insta360 ONEÂ²Â²2[https://www.insta360.com/product/insta360-one](https://www.insta360.com/product/insta360-one)
    å’Œ LG 360 CAMÂ³Â³3[https://www.lg.com/sg/lg-friends/lg-360-CAM](https://www.lg.com/sg/lg-friends/lg-360-CAM)ã€‚è¿™ç±»
    $360^{\circ}$ ç›¸æœºå¯¹é•œå¤´çš„éœ€æ±‚æœ€å°ï¼Œä»·æ ¼ä¾¿å®œï¼Œæ–¹ä¾¿ï¼Œå—åˆ°è¡Œä¸šå’Œæ¶ˆè´¹è€…çš„é’çã€‚æ¥è‡ªä¸¤å°ç›¸æœºçš„å›¾åƒéšåè¢«æ‹¼æ¥åœ¨ä¸€èµ·ä»¥è·å¾—å…¨æ™¯å›¾åƒï¼Œä½†æ‹¼æ¥è¿‡ç¨‹å¯èƒ½å¯¼è‡´è¾¹ç¼˜æ¨¡ç³Šï¼›ï¼ˆiiiï¼‰å…·æœ‰ä¸¤ä¸ªä»¥ä¸Šé•œå¤´çš„ç›¸æœºï¼Œå¦‚
    Titanï¼ˆå…«ä¸ªé•œå¤´ï¼‰â´â´4[https://www.insta360.com/product/insta360-titan/](https://www.insta360.com/product/insta360-titan/)ã€‚æ­¤å¤–ï¼ŒGoPro
    Omniâµâµ5[https://gopro.com/en/us/news/omni-is-here](https://gopro.com/en/us/news/omni-is-here)
    æ˜¯ç¬¬ä¸€ä¸ªå°†å…­å°æ™®é€šç›¸æœºæ”¾ç½®åœ¨ç«‹æ–¹ä½“çš„å…­ä¸ªé¢ä¸Šçš„ç›¸æœºç³»ç»Ÿï¼Œå…¶åˆæˆç»“æœå…·æœ‰æ›´é«˜çš„ç²¾åº¦å’Œè¾ƒå°‘çš„è¾¹ç¼˜æ¨¡ç³Šã€‚è¿™ç±» $360^{\circ}$ ç›¸æœºæ˜¯ä¸“ä¸šçº§çš„ã€‚
- en: 2.1.2 Spherical Imaging
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 çƒé¢æˆåƒ
- en: 'We first define the spherical coordinate $(\theta,\phi,\rho)$, where $\theta\in(0,2\pi)$
    ,$\phi\in(0,\pi)$, and $\rho$ represent the latitude, longitude, and radius of
    the sphere, respectively. We also define the Cartesian coordinate $(x,y,z)$. The
    transformation between spherical coordinate and Cartesian coordinate can be formulated
    as followsÂ [[18](#bib.bib18)]:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰çƒé¢åæ ‡ $(\theta,\phi,\rho)$ï¼Œå…¶ä¸­ $\theta\in(0,2\pi)$ ï¼Œ$\phi\in(0,\pi)$ï¼Œ$\rho$
    åˆ†åˆ«è¡¨ç¤ºçƒé¢çš„çº¬åº¦ã€ç»åº¦å’ŒåŠå¾„ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ç¬›å¡å°”åæ ‡ $(x,y,z)$ã€‚çƒé¢åæ ‡ä¸ç¬›å¡å°”åæ ‡ä¹‹é—´çš„è½¬æ¢å…¬å¼å¦‚ä¸‹ [[18](#bib.bib18)]ï¼š
- en: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\'
- en: \phi\\
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \phi\\
- en: \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
- en: \arctan(x/z)\\
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \arctan(x/z)\\
- en: \arccos(y/\rho)\\
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \arccos(y/\rho)\\
- en: \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
- en: y\\
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: y\\
- en: z\\
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: z\\
- en: \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
- en: \rho\cos(\phi)\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \rho\cos(\phi)\\
- en: \rho\cos(\theta)\sin(\phi)\\
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \rho\cos(\theta)\sin(\phi)\\
- en: \end{array}." display="block"><semantics ><mrow ><mrow ><mrow ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mi  >Ï</mi></mtd></mtr><mtr ><mtd class="ltx_border_l
    ltx_border_r"  ><mi >Î¸</mi></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mi
    >Ï•</mi></mtd></mtr></mtable><mo >=</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><msup  ><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mi  >x</mi><mn
    >2</mn></msup><mo >+</mo><msup ><mi >y</mi><mn >2</mn></msup><mo >+</mo><msup
    ><mi  >z</mi><mn >2</mn></msup></mrow><mo stretchy="false"  >)</mo></mrow><mrow
    ><mn >1</mn><mo  >/</mo><mn >2</mn></mrow></msup></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >arctan</mi><mo >â¡</mo><mrow  ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo
    >/</mo><mi >z</mi></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mi >arccos</mi><mo >â¡</mo><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><mi >y</mi><mo >/</mo><mi >Ï</mi></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr></mtable></mrow><mo
    lspace="0.667em" rspace="0.667em"  >,</mo><mrow ><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mi >x</mi></mtd></mtr><mtr ><mtd class="ltx_border_l
    ltx_border_r"  ><mi >y</mi></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mi
    >z</mi></mtd></mtr></mtable><mo >=</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mrow ><mi  >Ï</mi><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi
    >sin</mi><mo  >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Î¸</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >â€‹</mo><mrow ><mi  >sin</mi><mo
    >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi  >Ï</mi><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi
    >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi  >Ï</mi><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi
    >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo
    stretchy="false" >(</mo><mi  >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em" >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><matrix ><matrixrow ><ci  >ğœŒ</ci></matrixrow><matrixrow
    ><ci >ğœƒ</ci></matrixrow><matrixrow ><ci  >italic-Ï•</ci></matrixrow></matrix><matrix
    ><matrixrow ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘¥</ci><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘¦</ci><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘§</ci><cn type="integer" >2</cn></apply></apply><apply
    ><cn type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow
    ><apply  ><apply ><ci >ğ‘¥</ci><ci >ğ‘§</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >ğ‘¦</ci><ci >ğœŒ</ci></apply></apply></matrixrow></matrix></apply><apply
    ><matrix ><matrixrow ><ci  >ğ‘¥</ci></matrixrow><matrixrow ><ci >ğ‘¦</ci></matrixrow><matrixrow
    ><ci  >ğ‘§</ci></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >ğœŒ</ci><apply
    ><ci  >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply></matrixrow><matrixrow
    ><apply ><ci  >ğœŒ</ci><apply ><ci >italic-Ï•</ci></apply></apply></matrixrow><matrixrow
    ><apply ><ci  >ğœŒ</ci><apply ><ci >ğœƒ</ci></apply><apply ><ci  >italic-Ï•</ci></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\ \phi\\
    \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\ \arctan(x/z)\\
    \arccos(y/\rho)\\ \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\ y\\ z\\ \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
    \rho\cos(\phi)\\ \rho\cos(\theta)\sin(\phi)\\ \end{array}.</annotation></semantics></math>
    |  | (1) |
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{array}[]{&#124;c&#124;}\rho\\ \theta\\ \phi\\ \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
    \arctan(x/z)\\ \arccos(y/\rho)\\ \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
    y\\ z\\ \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
    \rho\cos(\phi)\\ \rho\cos(\theta)\sin(\phi)\\ \end{array}ã€‚
- en: 'Equirectangular Projection (ERP)â¶â¶6[https://en.wikipedia.org/wiki/Equirectangular_projection](https://en.wikipedia.org/wiki/Equirectangular_projection)
    is a representation by uniformly sampling grids from the spherical surface, as
    shown in Fig.Â [3](#S1.F3 "Figure 3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(a). The horizontal unit angle is $\vartheta=2\pi/w$
    and the vertical unit angle is $\varphi=\pi/h$. In particular, if the horizontal
    and vertical unit angle are equal, the width $w$ is twice of height $h$. In a
    word, each pixel coordinate $(u,v)$ in ERP can be mapped to the spherical coordinate
    $(\theta,\phi)=(u\cdot\vartheta,v\cdot\varphi)$ and vice versa. Cubemap Projection
    (CP) projects the spherical surface to six cube faces with $90^{\circ}$ FoV, equal-side
    length $w$, and focal length $\frac{w}{2}$, as shown in Fig.Â [3](#S1.F3 "Figure
    3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional Vision: A Survey and New
    Perspectives")(b). We denote the cube faces as $f_{i}$, $i\in\{B,D,F,L,R,U\}$,
    representing back, down, front, left, right, and up, respectively. By setting
    the cube center as the origin, the extrinsic matrix of each face can be simplified
    into $90^{\circ}$ or $180^{\circ}$ rotation matrix and zero translation matrixÂ [[19](#bib.bib19)].
    Given a pixel on the plane $f_{i}$, we transform $f_{i}$ to the front plane (identical
    to the Cartesian coordinates) and calculate $(\theta,\phi)$ with Eq.Â [1](#S2.E1
    "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç­‰è·åœ†æŸ±æŠ•å½±ï¼ˆERPï¼‰â¶â¶6[https://en.wikipedia.org/wiki/Equirectangular_projection](https://en.wikipedia.org/wiki/Equirectangular_projection)æ˜¯é€šè¿‡å‡åŒ€é‡‡æ ·çƒé¢ç½‘æ ¼æ¥è¡¨ç¤ºçš„ï¼Œå¦‚å›¾[3](#S1.F3
    "Figure 3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(a)æ‰€ç¤ºã€‚æ°´å¹³å•ä½è§’ä¸º$\vartheta=2\pi/w$ï¼Œå‚ç›´å•ä½è§’ä¸º$\varphi=\pi/h$ã€‚ç‰¹åˆ«åœ°ï¼Œå¦‚æœæ°´å¹³å’Œå‚ç›´å•ä½è§’ç›¸ç­‰ï¼Œåˆ™å®½åº¦$w$æ˜¯é«˜åº¦$h$çš„ä¸¤å€ã€‚æ€»ä¹‹ï¼ŒERPä¸­æ¯ä¸ªåƒç´ åæ ‡$(u,v)$å¯ä»¥æ˜ å°„åˆ°çƒé¢åæ ‡$(\theta,\phi)=(u\cdot\vartheta,v\cdot\varphi)$ï¼Œåä¹‹äº¦ç„¶ã€‚ç«‹æ–¹ä½“æ˜ å°„ï¼ˆCPï¼‰å°†çƒé¢æŠ•å½±åˆ°å…­ä¸ªå…·æœ‰$90^{\circ}$è§†åœºã€ç­‰é•¿è¾¹$w$å’Œç„¦è·$\frac{w}{2}$çš„ç«‹æ–¹ä½“é¢ä¸Šï¼Œå¦‚å›¾[3](#S1.F3
    "Figure 3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(b)æ‰€ç¤ºã€‚æˆ‘ä»¬å°†ç«‹æ–¹ä½“é¢è¡¨ç¤ºä¸º$f_{i}$ï¼Œ$i\in\{B,D,F,L,R,U\}$ï¼Œåˆ†åˆ«ä»£è¡¨åã€ä¸‹ã€å‰ã€å·¦ã€å³å’Œä¸Šã€‚é€šè¿‡å°†ç«‹æ–¹ä½“ä¸­å¿ƒè®¾ç½®ä¸ºåŸç‚¹ï¼Œæ¯ä¸ªé¢çš„å¤–éƒ¨çŸ©é˜µå¯ä»¥ç®€åŒ–ä¸º$90^{\circ}$æˆ–$180^{\circ}$æ—‹è½¬çŸ©é˜µå’Œé›¶å¹³ç§»çŸ©é˜µ[[19](#bib.bib19)]ã€‚ç»™å®šå¹³é¢$f_{i}$ä¸Šçš„ä¸€ä¸ªåƒç´ ï¼Œæˆ‘ä»¬å°†$f_{i}$å˜æ¢åˆ°å‰é¢ï¼ˆç­‰åŒäºç¬›å¡å°”åæ ‡ç³»ï¼‰ï¼Œå¹¶ä½¿ç”¨æ–¹ç¨‹[1](#S2.E1
    "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")è®¡ç®—$(\theta,\phi)$ã€‚'
- en: '![Refer to caption](img/d64963dcc89c08ad0848376947052c11.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/d64963dcc89c08ad0848376947052c11.png)'
- en: 'Figure 4: An illustration of ERP-based convolution filters on ODIs. (a), (b),
    (c) and (d) are originally shown inÂ [[20](#bib.bib20)],Â [[21](#bib.bib21)],Â [[22](#bib.bib22)]
    andÂ [[23](#bib.bib23)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šåŸºäºERPçš„å·ç§¯æ»¤æ³¢å™¨åœ¨å…¨å‘å›¾åƒä¸­çš„ç¤ºä¾‹ã€‚(a)ã€(b)ã€(c)å’Œ(d)æœ€åˆå‡ºç°åœ¨[[20](#bib.bib20)]ã€[[21](#bib.bib21)]ã€[[22](#bib.bib22)]å’Œ[[23](#bib.bib23)]ä¸­ã€‚
- en: 'Tangent Projection is the gnomonic projectionÂ [[24](#bib.bib24)], a non-conformal
    projection from points $P_{s}$ on the sphere surface with the sphere center $O$
    to points $P_{t}$ in a tangent plane with center $P_{c}$Â â·â·7[https://mathworld.wolfram.com/GnomonicProjection.html](https://mathworld.wolfram.com/GnomonicProjection.html),
    as shown in Fig.Â [3](#S1.F3 "Figure 3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). For a pixel on the ERP image $P_{e}(u_{e},v_{e})$,
    we first calculate its corresponding point $P_{s}(\theta=u_{e}\cdot\vartheta,\phi=v_{e}\cdot\varphi)$
    on the unit sphere, following the transformation in ERP format. The projection
    from $P_{s}(\theta,\phi)$ to $P_{t}(u_{t},v_{t})$ is defined as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'åˆ‡çº¿æŠ•å½±æ˜¯*æ­£å°„æŠ•å½±*[[24](#bib.bib24)]ï¼Œä¸€ç§ä»çƒé¢ä¸Šç‚¹$P_{s}$é€šè¿‡çƒå¿ƒ$O$åˆ°åˆ‡å¹³é¢ä¸Šç‚¹$P_{t}$çš„éå…±å½¢æŠ•å½±ï¼Œåˆ‡å¹³é¢çš„ä¸­å¿ƒä¸º$P_{c}$â·â·7[https://mathworld.wolfram.com/GnomonicProjection.html](https://mathworld.wolfram.com/GnomonicProjection.html)ï¼Œå¦‚å›¾[3](#S1.F3
    "Figure 3 â€£ 1 Introduction â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(c)æ‰€ç¤ºã€‚å¯¹äºERPå›¾åƒä¸Šçš„ä¸€ä¸ªåƒç´ $P_{e}(u_{e},v_{e})$ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—å…¶åœ¨å•ä½çƒä¸Šçš„å¯¹åº”ç‚¹$P_{s}(\theta=u_{e}\cdot\vartheta,\phi=v_{e}\cdot\varphi)$ï¼Œéµå¾ªERPæ ¼å¼çš„å˜æ¢ã€‚$P_{s}(\theta,\phi)$åˆ°$P_{t}(u_{t},v_{t})$çš„æŠ•å½±å®šä¹‰ä¸ºï¼š'
- en: '|  | <math   alttext="\begin{split}&amp;u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &amp;y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}&amp;u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &amp;y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\'
- en: '&amp;\cos(c)=\sin(\phi_{c})\sin(\phi)+\cos(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c}),\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >u</mi><mi >t</mi></msub><mo
    >=</mo><mfrac  ><mrow ><mrow ><mi  >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"
    >(</mo><mi >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >Î¸</mi><mo >âˆ’</mo><msub ><mi >Î¸</mi><mi >c</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mrow ><mi >cos</mi><mo lspace="0.167em"
    >â¡</mo><mi  >c</mi></mrow></mfrac></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd
    columnalign="left" ><mrow ><mrow ><msub ><mi >y</mi><mi >t</mi></msub><mo >=</mo><mfrac  ><mrow
    ><mrow ><mrow  ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><msub
    ><mi >Ï•</mi><mi >c</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >âˆ’</mo><mrow ><mrow
    ><mi  >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >Ï•</mi><mi
    >c</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow
    ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >Î¸</mi><mo
    >âˆ’</mo><msub ><mi >Î¸</mi><mi >c</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mrow><mrow
    ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >c</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mfrac></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi  >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >c</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mrow ><mrow
    ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >Ï•</mi><mi  >c</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >â€‹</mo><mrow
    ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >+</mo><mrow ><mrow ><mi >cos</mi><mo >â¡</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >Ï•</mi><mi >c</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >â€‹</mo><mrow ><mi >cos</mi><mo
    >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em" >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mi >Î¸</mi><mo >âˆ’</mo><msub ><mi >Î¸</mi><mi >c</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘¢</ci><ci  >ğ‘¡</ci></apply><apply
    ><apply ><apply  ><ci >italic-Ï•</ci></apply><apply ><apply ><ci >ğœƒ</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘</ci></apply></apply></apply></apply><apply
    ><ci >ğ‘</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘¦</ci><ci >ğ‘¡</ci></apply><apply
    ><apply  ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >italic-Ï•</ci><ci >ğ‘</ci></apply></apply><apply ><ci  >italic-Ï•</ci></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >italic-Ï•</ci><ci
    >ğ‘</ci></apply></apply><apply ><ci >italic-Ï•</ci></apply><apply ><apply ><ci >ğœƒ</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘</ci></apply></apply></apply></apply></apply><apply
    ><ci >ğ‘</ci></apply></apply></apply><apply ><apply ><ci >ğ‘</ci></apply><apply
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >italic-Ï•</ci><ci
    >ğ‘</ci></apply></apply><apply ><ci >italic-Ï•</ci></apply></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >italic-Ï•</ci><ci >ğ‘</ci></apply></apply><apply
    ><ci >italic-Ï•</ci></apply><apply ><apply ><ci >ğœƒ</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\
    &\cos(c)=\sin(\phi_{c})\sin(\phi)+\cos(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c}),\end{split}</annotation></semantics></math>
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤„ä¸ºæ•°å­¦å…¬å¼ï¼Œè¯·å‹¿ç¿»è¯‘ã€‚
- en: 'where $(\theta_{c},\phi_{c})$ is the spherical coordinate of the tangent plane
    center $P_{c}$, and $(u_{t},v_{t})$ is the intersection coordinate of the tangent
    plane and the extension line of $\overrightarrow{OP_{s}}$. The inverse transformations
    are formulated as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $(\theta_{c},\phi_{c})$ æ˜¯åˆ‡å¹³é¢ä¸­å¿ƒ $P_{c}$ çš„çƒé¢åæ ‡ï¼Œ$(u_{t},v_{t})$ æ˜¯åˆ‡å¹³é¢ä¸ $\overrightarrow{OP_{s}}$
    çš„å»¶é•¿çº¿çš„äº¤ç‚¹åæ ‡ã€‚é€†å˜æ¢å…¬å¼ä¸ºï¼š
- en: '|  | $\begin{split}&amp;\theta=\theta_{c}+\tan^{-1}(\frac{u_{t}\sin(c)}{\gamma\cos(\phi_{c})\cos(c)-v_{t}\sin(\phi_{c})\sin(c)}),\\
    &amp;\phi=\sin^{-1}(\cos(c)\sin(\phi_{c})+\frac{1}{\gamma}v_{t}\sin(c)\cos(\phi_{c})),\end{split}$
    |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\theta=\theta_{c}+\tan^{-1}(\frac{u_{t}\sin(c)}{\gamma\cos(\phi_{c})\cos(c)-v_{t}\sin(\phi_{c})\sin(c)}),\\
    &amp;\phi=\sin^{-1}(\cos(c)\sin(\phi_{c})+\frac{1}{\gamma}v_{t}\sin(c)\cos(\phi_{c})),\end{split}$
    |  | (3) |'
- en: 'where $\gamma=\sqrt{u_{t}^{2}+v_{t}^{2}}$ and $c=\tan^{-1}\gamma$. With Eqs.
    [2](#S2.E2 "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") and
    [3](#S2.E3 "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    can build one-to-one forward and inverse mapping functions between the spherical
    coordinates and pixels on the tangent imagesÂ [[25](#bib.bib25)].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ $\gamma=\sqrt{u_{t}^{2}+v_{t}^{2}}$ å’Œ $c=\tan^{-1}\gamma$ã€‚é€šè¿‡æ–¹ç¨‹ [2](#S2.E2
    "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives") å’Œ [3](#S2.E3
    "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹çƒé¢åæ ‡ä¸åˆ‡å¹³é¢å›¾åƒä¸Šçš„åƒç´ ä¹‹é—´çš„ä¸€å¯¹ä¸€æ­£å‘å’Œé€†å‘æ˜ å°„å‡½æ•°[[25](#bib.bib25)]ã€‚'
- en: Icosahedron approximates a sphere surface through a Platonic solidÂ [[26](#bib.bib26)].
    Compared with ERP and CP, icosahedron projection has resolved the spherical distortion
    well. While some practical applications need less distortion representations,
    we can increase the number of subdivisions to further mitigate the spherical distortion.
    Specifically, each face in an icosahedron can be subdivided into four smaller
    faces to achieve higher resolution and less distortionÂ [[26](#bib.bib26)]. There
    exist some CNNs that are specifically designed to process an icosahedronÂ [[27](#bib.bib27),
    [28](#bib.bib28)]. It is noteworthy that the choice of subdivision degree needs
    to achieve a trade-off between efficiency and accuracy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒåé¢ä½“é€šè¿‡ä¸€ä¸ªæŸæ‹‰å›¾ç«‹ä½“æ¥è¿‘ä¼¼çƒé¢[[26](#bib.bib26)]ã€‚ä¸ERPå’ŒCPç›¸æ¯”ï¼ŒäºŒåé¢ä½“æŠ•å½±å¾ˆå¥½åœ°è§£å†³äº†çƒé¢ç•¸å˜çš„é—®é¢˜ã€‚è™½ç„¶ä¸€äº›å®é™…åº”ç”¨éœ€è¦æ›´å°‘çš„ç•¸å˜è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å¢åŠ ç»†åˆ†æ•°é‡ä»¥è¿›ä¸€æ­¥å‡è½»çƒé¢ç•¸å˜ã€‚å…·ä½“è€Œè¨€ï¼ŒäºŒåé¢ä½“ä¸­çš„æ¯ä¸ªé¢å¯ä»¥ç»†åˆ†ä¸ºå››ä¸ªæ›´å°çš„é¢ï¼Œä»¥å®ç°æ›´é«˜çš„åˆ†è¾¨ç‡å’Œæ›´å°‘çš„ç•¸å˜[[26](#bib.bib26)]ã€‚ä¸€äº›CNNä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†äºŒåé¢ä½“[[27](#bib.bib27),
    [28](#bib.bib28)]ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»†åˆ†ç¨‹åº¦çš„é€‰æ‹©éœ€è¦åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚
- en: Other projections. For CP, different sampling locations on the cube faces decide
    different spatial sampling rates, resulting in the distortions. To address this
    problem, Equi-Angular Cubemap (EAC) projectionÂ â¸â¸8[https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/](https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/)
    is proposed to keep the sampling uniform. Besides, some projections can transform
    the spherical surface into non-spatial domains, e.g., 3D rotation group (SO3)Â [[29](#bib.bib29)]
    and spherical Fourier transformation (SFT)Â [[30](#bib.bib30)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æŠ•å½±ã€‚å¯¹äºCPï¼Œç«‹æ–¹ä½“é¢ä¸Šçš„ä¸åŒé‡‡æ ·ä½ç½®å†³å®šäº†ä¸åŒçš„ç©ºé—´é‡‡æ ·ç‡ï¼Œä»è€Œå¯¼è‡´ç•¸å˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ç­‰è§’ç«‹æ–¹å›¾ï¼ˆEACï¼‰æŠ•å½±Â â¸â¸8[https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/](https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/)ä»¥ä¿æŒå‡åŒ€é‡‡æ ·ã€‚æ­¤å¤–ï¼Œä¸€äº›æŠ•å½±å¯ä»¥å°†çƒé¢è½¬æ¢ä¸ºéç©ºé—´åŸŸï¼Œä¾‹å¦‚3Dæ—‹è½¬ç¾¤ï¼ˆSO3ï¼‰[[29](#bib.bib29)]å’Œçƒé¢å‚…é‡Œå¶å˜æ¢ï¼ˆSFTï¼‰[[30](#bib.bib30)]ã€‚
- en: 2.1.3 Spherical Stereo
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 çƒé¢ç«‹ä½“è§†è§‰
- en: 'Spherical stereo is about two viewpoints displaced with a known horizontal
    or vertical baselineÂ [[18](#bib.bib18)]. Due to the spherical projection, spherical
    stereo is more irregular than stereo with traditional pinhole cameras. According
    to Eq.Â [1](#S2.E1 "In 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£
    2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    we define the baseline as $\textbf{b}=(\delta x,\delta y,\delta z)$, and the derivative
    correspondence between the spherical coordinates and Cartesian coordinates can
    be formulated as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'çƒé¢ç«‹ä½“è§†è§‰æ¶‰åŠçš„æ˜¯ä¸¤ä¸ªè§†ç‚¹ï¼Œä¸¤ä¸ªè§†ç‚¹ä¹‹é—´å…·æœ‰å·²çŸ¥çš„æ°´å¹³æˆ–å‚ç›´åŸºçº¿ [[18](#bib.bib18)]ã€‚ç”±äºçƒé¢æŠ•å½±ï¼Œçƒé¢ç«‹ä½“è§†è§‰æ¯”ä¼ ç»Ÿé’ˆå­”ç›¸æœºçš„ç«‹ä½“è§†è§‰æ›´åŠ ä¸è§„åˆ™ã€‚æ ¹æ®å…¬å¼
    [1](#S2.E1 "åœ¨ 2.1.2 çƒé¢æˆåƒ â€£ 2.1 å…¨æ™¯æˆåƒ â€£ 2 èƒŒæ™¯ â€£ æ·±åº¦å­¦ä¹ åœ¨å…¨æ™¯è§†è§‰ä¸­çš„åº”ç”¨: è°ƒæŸ¥ä¸æ–°è§†è§’")ï¼Œæˆ‘ä»¬å°†åŸºçº¿å®šä¹‰ä¸º
    $\textbf{b}=(\delta x,\delta y,\delta z)$ï¼Œçƒé¢åæ ‡ä¸ç¬›å¡å°”åæ ‡ä¹‹é—´çš„å¯¼æ•°å¯¹åº”å…³ç³»å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š'
- en: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\'
- en: \delta_{\phi}\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \delta_{\phi}\\
- en: \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&amp;\cos(\phi)&amp;\cos(\theta)\sin(\phi)\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&amp;\cos(\phi)&amp;\cos(\theta)\sin(\phi)\\
- en: \frac{\cos(\theta)}{\rho\sin(\phi)}&amp;0&amp;\frac{-\sin(\theta)}{\rho\sin(\phi)}\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\cos(\theta)}{\rho\sin(\phi)}&amp;0&amp;\frac{-\sin(\theta)}{\rho\sin(\phi)}\\
- en: \frac{\sin(\theta)\cos(\phi)}{\rho}&amp;\frac{-\sin(\phi)}{\rho}&amp;\frac{\cos(\theta)\cos(\phi)}{\rho}\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\sin(\theta)\cos(\phi)}{\rho}&amp;\frac{-\sin(\phi)}{\rho}&amp;\frac{\cos(\theta)\cos(\phi)}{\rho}\\
- en: \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\
- en: \delta y\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \delta y\\
- en: \delta z\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \delta z\\
- en: \end{array}." display="block"><semantics ><mrow ><mrow  ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >Î´</mi><mi
    >Ï</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >Î´</mi><mi >Î¸</mi></msub></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >Î´</mi><mi >Ï•</mi></msub></mtd></mtr></mtable><mo
    >=</mo><mrow ><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd   ><mrow ><mrow ><mi  >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow
    ><mi >sin</mi><mo  >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd><mtd ><mrow  ><mi >cos</mi><mo >â¡</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow
    ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mstyle displaystyle="false"
    ><mfrac  ><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >Î¸</mi><mo stretchy="false" >)</mo></mrow></mrow><mrow ><mi  >Ï</mi><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mo rspace="0.167em"
    >âˆ’</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mi >Ï</mi><mo
    lspace="0.167em" rspace="0em" >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mrow ><mi >sin</mi><mo >â¡</mo><mrow
    ><mo stretchy="false" >(</mo><mi >Î¸</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >Ï</mi></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow
    ><mo rspace="0.167em" >âˆ’</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi >Ï</mi></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac ><mrow  ><mrow ><mi >cos</mi><mo >â¡</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >Ï</mi></mfrac></mstyle></mtd></mtr></mtable><mo lspace="0.667em" rspace="0em"  >â€‹</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >x</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mrow ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >y</mi></mrow></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >z</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><matrix  ><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ›¿</ci><ci >ğœŒ</ci></apply></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ›¿</ci><ci >ğœƒ</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ›¿</ci><ci >italic-Ï•</ci></apply></matrixrow></matrix><apply
    ><matrix ><matrixrow  ><apply ><apply ><ci  >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><apply
    ><ci  >italic-Ï•</ci></apply><apply ><apply ><ci  >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >ğœƒ</ci></apply><apply ><ci >ğœŒ</ci><apply ><ci >italic-Ï•</ci></apply></apply></apply><cn
    type="integer" >0</cn><apply ><apply  ><apply ><ci >ğœƒ</ci></apply></apply><apply
    ><ci >ğœŒ</ci><apply ><ci >italic-Ï•</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><ci
    >ğœŒ</ci></apply><apply ><apply ><apply ><ci >italic-Ï•</ci></apply></apply><ci >ğœŒ</ci></apply><apply
    ><apply  ><apply ><ci >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><ci
    >ğœŒ</ci></apply></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >ğ›¿</ci><ci
    >ğ‘¥</ci></apply></matrixrow><matrixrow ><apply ><ci >ğ›¿</ci><ci >ğ‘¦</ci></apply></matrixrow><matrixrow
    ><apply  ><ci >ğ›¿</ci><ci >ğ‘§</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\
    \delta_{\phi}\\ \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&\cos(\phi)&\cos(\theta)\sin(\phi)\\
    \frac{\cos(\theta)}{\rho\sin(\phi)}&0&\frac{-\sin(\theta)}{\rho\sin(\phi)}\\ \frac{\sin(\theta)\cos(\phi)}{\rho}&\frac{-\sin(\phi)}{\rho}&\frac{\cos(\theta)\cos(\phi)}{\rho}\\
    \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\ \delta y\\ \delta z\\ \end{array}.</annotation></semantics></math>
    |  | (4) |
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}ã€‚" display="block"><semantics ><mrow ><mrow  ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >Î´</mi><mi
    >Ï</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >Î´</mi><mi >Î¸</mi></msub></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >Î´</mi><mi >Ï•</mi></msub></mtd></mtr></mtable><mo
    >=</mo><mrow ><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd   ><mrow ><mrow ><mi  >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow
    ><mi >sin</mi><mo  >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd><mtd ><mrow  ><mi >cos</mi><mo >â¡</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi  >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >â€‹</mo><mrow
    ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi >Ï•</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mstyle displaystyle="false"
    ><mfrac  ><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >Î¸</mi><mo stretchy="false" >)</mo></mrow></mrow><mrow ><mi  >Ï</mi><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mo rspace="0.167em"
    >âˆ’</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mi >Ï</mi><mo
    lspace="0.167em" rspace="0em" >â€‹</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mrow ><mi >sin</mi><mo >â¡</mo><mrow
    ><mo stretchy="false" >(</mo><mi >Î¸</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >Ï</mi></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow
    ><mo rspace="0.167em" >âˆ’</mo><mrow ><mi >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi >Ï</mi></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac ><mrow  ><mrow ><mi >cos</mi><mo >â¡</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >â€‹</mo><mrow ><mi >cos</mi><mo >â¡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >Ï•</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >Ï</mi></mfrac></mstyle></mtd></mtr></mtable><mo lspace="0.667em" rspace="0em"  >â€‹</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >x</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mrow ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >y</mi></mrow></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >Î´</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >z</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><matrix  ><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ›¿</ci><ci >ğœŒ</ci></apply></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ›¿</ci><ci >ğœƒ</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ›¿</ci><ci >italic-Ï•</ci></apply></matrixrow></matrix><apply
    ><matrix ><matrixrow  ><apply ><apply ><ci  >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><apply
    ><ci  >italic-Ï•</ci></apply><apply ><apply ><ci  >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >ğœƒ</ci></apply><apply ><ci >ğœŒ</ci><apply ><ci >italic-Ï•</ci></apply></apply></apply><cn
    type="integer" >0</cn><apply ><apply  ><apply ><ci >ğœƒ</ci></apply></apply><apply
    ><ci >ğœŒ</ci><apply ><ci >italic-Ï•</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><ci
    >ğœŒ</ci></apply><apply ><apply ><apply ><ci >italic-Ï•</ci></apply></apply><ci >ğœŒ</ci></apply><apply
    ><apply  ><apply ><ci >ğœƒ</ci></apply><apply ><ci >italic-Ï•</ci></apply></apply><ci
    >ğœŒ</ci></apply></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >ğ›¿</ci><ci
    >ğ‘¥</ci></apply></matrixrow><matrixrow ><apply ><ci >ğ›¿</ci><ci >
- en: 'In Eq.Â [4](#S2.E4 "In 2.1.3 Spherical Stereo â€£ 2.1 Omnidirectional Imaging
    â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    $(\delta_{\theta},\delta_{\phi})$ represents the angular differences in the spherical
    coordinates $(\theta,\phi,\rho)$. According to Eq. [4](#S2.E4 "In 2.1.3 Spherical
    Stereo â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), we can find that for the vertical baseline
    $\textbf{b}_{v}=(0,\delta y,0)$, there is no difference in $\theta$, which is
    simpler. However, for horizontal baseline $\textbf{b}_{h}=(\delta x,0,0)$, differences
    occur in both angles $\theta$ and $\phi$.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨å…¬å¼ [4](#S2.E4 "In 2.1.3 Spherical Stereo â€£ 2.1 Omnidirectional Imaging â€£ 2
    Background â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    ä¸­ï¼Œ$(\delta_{\theta},\delta_{\phi})$ è¡¨ç¤ºçƒé¢åæ ‡ $(\theta,\phi,\rho)$ ä¸­çš„è§’åº¦å·®å¼‚ã€‚æ ¹æ®å…¬å¼ [4](#S2.E4
    "In 2.1.3 Spherical Stereo â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¯¹äºå‚ç›´åŸºçº¿
    $\textbf{b}_{v}=(0,\delta y,0)$ï¼Œ$\theta$ æ²¡æœ‰å˜åŒ–ï¼Œè¿™ç§æƒ…å†µè¾ƒä¸ºç®€å•ã€‚ç„¶è€Œï¼Œå¯¹äºæ°´å¹³åŸºçº¿ $\textbf{b}_{h}=(\delta
    x,0,0)$ï¼Œ$\theta$ å’Œ $\phi$ ä¸¤ä¸ªè§’åº¦éƒ½ä¼šå‘ç”Ÿå·®å¼‚ã€‚'
- en: 2.2 Convolution Methods on ODI
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 ODIä¸Šçš„å·ç§¯æ–¹æ³•
- en: 'As the natural projection surface of an ODI is a sphere, standard CNNs are
    less capable of processing the inherent distortions when the spherical image is
    projected back to a plane. Numerous CNN-based methods have been proposed to enhance
    the extraction of â€unbiasedâ€ information from spherical images. These methods
    can be classified into two prevailing categories: (i) Applying 2D convolution
    filters on planar projections; (ii) Directly leveraging spherical convolution
    filters in the spherical domain. In this subsection, we analyze these methods
    in detail.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºODIçš„è‡ªç„¶æŠ•å½±è¡¨é¢æ˜¯çƒé¢ï¼Œæ ‡å‡†çš„CNNåœ¨å°†çƒé¢å›¾åƒæŠ•å½±å›å¹³é¢æ—¶ï¼Œå¤„ç†å›ºæœ‰çš„ç•¸å˜èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†å¢å¼ºä»çƒé¢å›¾åƒä¸­æå–â€œæ— åâ€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå·²ç»æå‡ºäº†è®¸å¤šåŸºäºCNNçš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š
    (i) åœ¨å¹³é¢æŠ•å½±ä¸Šåº”ç”¨2Då·ç§¯æ»¤æ³¢å™¨ï¼› (ii) ç›´æ¥åœ¨çƒé¢åŸŸä¸­åˆ©ç”¨çƒé¢å·ç§¯æ»¤æ³¢å™¨ã€‚åœ¨æœ¬å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†åˆ†æè¿™äº›æ–¹æ³•ã€‚
- en: 2.2.1 Planar Projection-based Convolution
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 åŸºäºå¹³é¢æŠ•å½±çš„å·ç§¯
- en: 'As the most common sphere-to-plane projection, ERP introduces severe distortions,
    especially at the poles. Considering it provides global information and takes
    less computation cost, Su et al.Â [[20](#bib.bib20)] proposed a representative
    method named Spherical Convolution, which leverages regular convolution filters
    with the adaptive kernel size according to the spherical coordinates. However,
    as shown in Fig.Â [4](#S2.F4 "Figure 4 â€£ 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional
    Imaging â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")(a), the regular convolution weights are only shared along each
    row and can not be trained from scratch. Inspired by Spherical Convolution, SphereNetÂ [[21](#bib.bib21)]
    proposes another typical method that processes the ERP by directly adjusting the
    sampling grid locations of convolution filters to achieve the distortion invariance
    and can be trained end-to-end, as depicted in Fig.Â [4](#S2.F4 "Figure 4 â€£ 2.1.2
    Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(b). This is conceptually
    similar to those inÂ [[22](#bib.bib22)], [[23](#bib.bib23)], as shown in Fig.Â [4](#S2.F4
    "Figure 4 â€£ 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)
    and (d). In particular, before ODIs are widely applied, Cohen et al.Â [[29](#bib.bib29)]
    have discussed the spatially varying distortions introduced by ERP and proposed
    a rotation-invariant spherical CNN approach to learn an SO3 representation. By
    contrast, KTNÂ [[31](#bib.bib31), [32](#bib.bib32)] learns a transfer function
    to achieve that the convolution kernel, which is learnt from the conventional
    planar images, can be directly applied on ERP without retraining. InÂ [[33](#bib.bib33)],
    the ERP is represented as a weighted graph, and a novel graph construction method
    is introduced by incorporating the geometry of the omnidirectional cameras into
    the graph structure to mitigate the distortions.Â [[19](#bib.bib19), [34](#bib.bib34)]
    focused on directly applying traditional 2D CNNs on CP and tangent projection,
    which are distortion-less.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½œä¸ºæœ€å¸¸è§çš„çƒé¢åˆ°å¹³é¢æŠ•å½±ï¼ŒERP å¼•å…¥äº†ä¸¥é‡çš„ç•¸å˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æç‚¹å¤„ã€‚è€ƒè™‘åˆ°å®ƒæä¾›äº†å…¨çƒä¿¡æ¯ä¸”è®¡ç®—æˆæœ¬è¾ƒä½ï¼ŒSu ç­‰äºº[[20](#bib.bib20)]
    æå‡ºäº†ä¸€ä¸ªåä¸ºçƒé¢å·ç§¯çš„ä»£è¡¨æ€§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ ¹æ®çƒé¢åæ ‡è°ƒæ•´çš„è‡ªé€‚åº”æ ¸å¤§å°çš„å¸¸è§„å·ç§¯æ»¤æ³¢å™¨ã€‚ç„¶è€Œï¼Œå¦‚å›¾Â [4](#S2.F4 "Figure 4 â€£ 2.1.2
    Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(a) æ‰€ç¤ºï¼Œå¸¸è§„å·ç§¯æƒé‡ä»…åœ¨æ¯ä¸€è¡Œä¹‹é—´å…±äº«ï¼Œä¸”ä¸èƒ½ä»å¤´å¼€å§‹è®­ç»ƒã€‚å—åˆ°çƒé¢å·ç§¯çš„å¯å‘ï¼ŒSphereNet[[21](#bib.bib21)]
    æå‡ºäº†å¦ä¸€ç§å…¸å‹çš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥è°ƒæ•´å·ç§¯æ»¤æ³¢å™¨çš„é‡‡æ ·ç½‘æ ¼ä½ç½®æ¥å¤„ç† ERPï¼Œä»¥å®ç°ç•¸å˜ä¸å˜æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¦‚å›¾Â [4](#S2.F4 "Figure
    4 â€£ 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")(b) æ‰€ç¤ºã€‚è¿™åœ¨æ¦‚å¿µä¸Šç±»ä¼¼äºÂ [[22](#bib.bib22)]ã€[[23](#bib.bib23)]
    ä¸­çš„å†…å®¹ï¼Œå¦‚å›¾Â [4](#S2.F4 "Figure 4 â€£ 2.1.2 Spherical Imaging â€£ 2.1 Omnidirectional Imaging
    â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)
    å’Œ (d) æ‰€ç¤ºã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨ ODI å¹¿æ³›åº”ç”¨ä¹‹å‰ï¼ŒCohen ç­‰äºº[[29](#bib.bib29)] è®¨è®ºäº† ERP å¼•å…¥çš„ç©ºé—´å˜å¼‚ç•¸å˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ—‹è½¬ä¸å˜çš„çƒé¢
    CNN æ–¹æ³•æ¥å­¦ä¹  SO3 è¡¨ç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒKTN[[31](#bib.bib31), [32](#bib.bib32)] å­¦ä¹ ä¸€ä¸ªä¼ é€’å‡½æ•°ï¼Œä»¥å®ç°ä»å¸¸è§„å¹³é¢å›¾åƒä¸­å­¦ä¹ çš„å·ç§¯æ ¸å¯ä»¥ç›´æ¥åº”ç”¨äº
    ERPï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨ [[33](#bib.bib33)] ä¸­ï¼ŒERP è¢«è¡¨ç¤ºä¸ºåŠ æƒå›¾ï¼Œå¹¶é€šè¿‡å°†å…¨å‘æ‘„åƒæœºçš„å‡ ä½•ç»“æ„çº³å…¥å›¾ç»“æ„ä¸­æ¥å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾æ„å»ºæ–¹æ³•ï¼Œä»¥ç¼“è§£ç•¸å˜ã€‚[[19](#bib.bib19),
    [34](#bib.bib34)] å…³æ³¨äºç›´æ¥å°†ä¼ ç»Ÿçš„ 2D CNN åº”ç”¨äº CP å’Œåˆ‡å‘æŠ•å½±ï¼Œè¿™äº›æŠ•å½±æ²¡æœ‰ç•¸å˜ã€‚'
- en: '![Refer to caption](img/c28b9e7f40390f01bf66f76a0f5a5b5c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/c28b9e7f40390f01bf66f76a0f5a5b5c.png)'
- en: 'Figure 5: The two representative spherical convolution approaches. (a) and
    (b) are originally shown inÂ [[35](#bib.bib35)] andÂ [[27](#bib.bib27)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šä¸¤ç§ä»£è¡¨æ€§çš„çƒé¢å·ç§¯æ–¹æ³•ã€‚ï¼ˆaï¼‰å’Œï¼ˆbï¼‰æœ€åˆå±•ç¤ºäº [[35](#bib.bib35)] å’Œ [[27](#bib.bib27)]ã€‚
- en: 2.2.2 Spherical Convolution
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 çƒé¢å·ç§¯
- en: 'Some methods have explored the special convolution filters in the spherical
    domain. Esteves et al.Â [[36](#bib.bib36)] proposed the first spherical CNN architecture,
    which considers the convolution filters in the spherical harmonic domain, to address
    the problem of 3D rotation equivariance in standard CNNs. UnlikeÂ [[36](#bib.bib36)],
    Yang et al.Â [[35](#bib.bib35)] proposed a representative framework to map spherical
    images into the rotation-equivariant representations based on the geometry of
    spherical surfaces. As shown in Fig.Â [5](#S2.F5 "Figure 5 â€£ 2.2.1 Planar Projection-based
    Convolution â€£ 2.2 Convolution Methods on ODI â€£ 2 Background â€£ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives")(a), SGCNÂ [[35](#bib.bib35)]
    represents the input spherical image as a graph based on the GICOPixÂ [[35](#bib.bib35)].
    Moreover, it explores the isometric transformation equivariance of the graph through
    GCN layers. A similar strategy is proposed inÂ [[37](#bib.bib37)] andÂ [[38](#bib.bib38)].
    In [[37](#bib.bib37)], the gauge equivariant CNNs are proposed to learn spherical
    representations from the icosahedron. By contrast, Shakerinava et al.Â [[38](#bib.bib38)]
    extended the icosahedron to all the pixelizations of platonic solids and generalized
    the gauge equivariant CNNs on the pixelized spheres. Due to a trade-off between
    efficiency and rotation equivariance, DeepSphereÂ [[39](#bib.bib39)] models the
    sampled sphere as a graph of connected pixels and designs a novel graph convolution
    network (GCN) to balance the computational efficiency and sampling flexibility
    by adjusting the neighboring pixel numbers of the pixels on the graph. Compared
    with the methods above, another representative ODI representation is proposed
    in SpherePHDÂ [[27](#bib.bib27)]. As shown in Fig.Â [5](#S2.F5 "Figure 5 â€£ 2.2.1
    Planar Projection-based Convolution â€£ 2.2 Convolution Methods on ODI â€£ 2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b),
    SpherePHD represents the spherical image as the spherical polyhedron and provides
    specific convolution and pooling methods.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸€äº›æ–¹æ³•å·²ç»æ¢ç´¢äº†çƒé¢é¢†åŸŸä¸­çš„ç‰¹æ®Šå·ç§¯æ»¤æ³¢å™¨ã€‚Esteves ç­‰äººÂ [[36](#bib.bib36)] æå‡ºäº†ç¬¬ä¸€ä¸ªçƒé¢ CNN æ¶æ„ï¼Œè¯¥æ¶æ„è€ƒè™‘äº†çƒé¢è°ƒå’ŒåŸŸä¸­çš„å·ç§¯æ»¤æ³¢å™¨ï¼Œä»¥è§£å†³æ ‡å‡†
    CNN ä¸­ 3D æ—‹è½¬ç­‰å˜æ€§çš„é—®é¢˜ã€‚ä¸Â [[36](#bib.bib36)] ä¸åŒï¼ŒYang ç­‰äººÂ [[35](#bib.bib35)] æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§æ¡†æ¶ï¼Œå°†çƒé¢å›¾åƒæ˜ å°„åˆ°åŸºäºçƒé¢å‡ ä½•çš„æ—‹è½¬ç­‰å˜è¡¨ç¤ºã€‚å¦‚å›¾Â [5](#S2.F5
    "Figure 5 â€£ 2.2.1 Planar Projection-based Convolution â€£ 2.2 Convolution Methods
    on ODI â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")(a) æ‰€ç¤ºï¼ŒSGCNÂ [[35](#bib.bib35)] å°†è¾“å…¥çš„çƒé¢å›¾åƒè¡¨ç¤ºä¸ºåŸºäº GICOPixÂ [[35](#bib.bib35)]
    çš„å›¾ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡ GCN å±‚æ¢ç´¢äº†å›¾çš„ç­‰è·å˜æ¢ç­‰å˜æ€§ã€‚åœ¨Â [[37](#bib.bib37)] å’ŒÂ [[38](#bib.bib38)] ä¸­ä¹Ÿæå‡ºäº†ç±»ä¼¼çš„ç­–ç•¥ã€‚åœ¨
    [[37](#bib.bib37)] ä¸­ï¼Œæå‡ºäº†ä»äºŒåé¢ä½“å­¦ä¹ çƒé¢è¡¨ç¤ºçš„æµ‹åº¦ç­‰å˜ CNNã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒShakerinava ç­‰äººÂ [[38](#bib.bib38)]
    å°†äºŒåé¢ä½“æ‰©å±•åˆ°æ‰€æœ‰æŸæ‹‰å›¾å›ºä½“çš„åƒç´ åŒ–ï¼Œå¹¶å°†æµ‹åº¦ç­‰å˜ CNN æ³›åŒ–åˆ°åƒç´ åŒ–çƒé¢ä¸Šã€‚ç”±äºæ•ˆç‡ä¸æ—‹è½¬ç­‰å˜æ€§ä¹‹é—´çš„æƒè¡¡ï¼ŒDeepSphereÂ [[39](#bib.bib39)]
    å°†é‡‡æ ·çƒä½“å»ºæ¨¡ä¸ºè¿æ¥åƒç´ çš„å›¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å›¾å·ç§¯ç½‘ç»œ (GCN)ï¼Œé€šè¿‡è°ƒæ•´å›¾ä¸Šåƒç´ çš„é‚»è¿‘åƒç´ æ•°é‡æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œé‡‡æ ·çµæ´»æ€§ã€‚ä¸ä¸Šè¿°æ–¹æ³•ç›¸æ¯”ï¼ŒSpherePHDÂ [[27](#bib.bib27)]
    æå‡ºäº†å¦ä¸€ç§ä»£è¡¨æ€§çš„ ODI è¡¨ç¤ºã€‚å¦‚å›¾Â [5](#S2.F5 "Figure 5 â€£ 2.2.1 Planar Projection-based Convolution
    â€£ 2.2 Convolution Methods on ODI â€£ 2 Background â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(b) æ‰€ç¤ºï¼ŒSpherePHD å°†çƒé¢å›¾åƒè¡¨ç¤ºä¸ºçƒé¢å¤šé¢ä½“ï¼Œå¹¶æä¾›äº†ç‰¹å®šçš„å·ç§¯å’Œæ± åŒ–æ–¹æ³•ã€‚'
- en: 2.3 Dataset
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 æ•°æ®é›†
- en: 'TABLE I: Summary of ODI image and video datasets. N/A indicates â€˜not availableâ€™
    and GT indicates â€˜ground truthâ€™.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ Iï¼šODI å›¾åƒå’Œè§†é¢‘æ•°æ®é›†æ‘˜è¦ã€‚N/A è¡¨ç¤ºâ€œä¸å¯ç”¨â€ï¼ŒGT è¡¨ç¤ºâ€œçœŸå®å€¼â€ã€‚
- en: '| Dataset | Size | Data Type | Resolution | GT | Purpose |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| æ•°æ®é›† | å¤§å° | æ•°æ®ç±»å‹ | åˆ†è¾¨ç‡ | GT | ç›®çš„ |'
- en: '| Stanford2D3DÂ [[5](#bib.bib5)] | 70496 RGB+1413 ERP images | Real | 1080 $\times$
    1080 | $\checkmark$ | Object Detection, Scene Uderstanding |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Stanford2D3DÂ [[5](#bib.bib5)] | 70496 RGB+1413 ERP å›¾åƒ | çœŸå® | 1080 $\times$
    1080 | $\checkmark$ | ç‰©ä½“æ£€æµ‹ï¼Œåœºæ™¯ç†è§£ |'
- en: '| Structured3DÂ [[40](#bib.bib40)] | 196k images | Synthetic | 512 $\times$
    1024 | âœ— | Object Detection, Scene Understanding, Image Synthesis, 3D Modeling
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Structured3DÂ [[40](#bib.bib40)] | 196k å›¾åƒ | åˆæˆ | 512 $\times$ 1024 | âœ— |
    ç‰©ä½“æ£€æµ‹ï¼Œåœºæ™¯ç†è§£ï¼Œå›¾åƒåˆæˆï¼Œ3D å»ºæ¨¡ |'
- en: '| SUNCGÂ [[41](#bib.bib41)] | 45622 scenes | Synthetic | N/A | âœ— | Depth Estimation
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SUNCGÂ [[41](#bib.bib41)] | 45622 åœºæ™¯ | åˆæˆ | N/A | âœ— | æ·±åº¦ä¼°è®¡ |'
- en: '| 360-SportÂ [[42](#bib.bib42)] | 342 360^âˆ˜ videos | Real | N/A | $\checkmark$
    | Visual Pilot |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 360-SportÂ [[42](#bib.bib42)] | 342 ä¸ª 360^âˆ˜ è§†é¢‘ | çœŸå® | N/A | $\checkmark$ |
    è§†è§‰å¯¼èˆª |'
- en: '| Wild-360Â [[43](#bib.bib43)] | 85 360^âˆ˜ videos | Real | N/A | $\checkmark$
    | Video Saliency |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Wild-360Â [[43](#bib.bib43)] | 85 ä¸ª 360^âˆ˜ è§†é¢‘ | çœŸå® | N/A | $\checkmark$ | è§†é¢‘æ˜¾è‘—æ€§
    |'
- en: 'The performance of the DL-based approaches is closely related to the qualities
    and quantities of the datasets. With the development of spherical imaging devices,
    a large number of ODI and ODV datasets are publicly available for various vision
    tasks. Especially, most ODV data is collected from public video sharing platforms
    like Vimeo and Youtube. In Table.Â [I](#S2.T1 "TABLE I â€£ 2.3 Dataset â€£ 2 Background
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    list some representative ODI and ODV datasets used for different purposes and
    we also show their properties, e.g., size, resolution, data source. Complete summary
    of datasets can be found in the suppl. material. According to the data source,
    there are two categories of datasets: real-world datasets and synthetic datasets.
    Most real-world datasets only provide images of 2D projection modality and are
    applied to some specific task. However, Stanford2D3DÂ [[5](#bib.bib5)] contains
    three modalities, including 2D, 2.5D, that are suitable for cross-modal learning.
    Moreover, some datasets are selected from the existing ones, such as PanoContextÂ [[7](#bib.bib7)]
    collected from SUN360Â [[3](#bib.bib3)]. For the synthetic datasets, images are
    complete and high-quality without natural noise, and the annotations are easier
    to obtain than that in the real-world scenes. For instance, SUNCGÂ [[41](#bib.bib41)]
    is created via the Plannar5D platform, and all the 3D scenes are composed of individually
    labeled 3D object meshes. Structured3DÂ [[40](#bib.bib40)] and OmniFlowÂ [[44](#bib.bib44)]
    utilize the rendering engine to generate photo-realistic images containing 3D
    structure annotations and corresponding optical flows. Similar to real-world datasets,
    there are also some datasets, e.g., omni-SYNTHIAÂ [[45](#bib.bib45)], extracted
    from the large synthetic ones for specific tasks.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•çš„æ€§èƒ½ä¸æ•°æ®é›†çš„è´¨é‡å’Œæ•°é‡å¯†åˆ‡ç›¸å…³ã€‚éšç€çƒå½¢æˆåƒè®¾å¤‡çš„å‘å±•ï¼Œå¤§é‡ODIå’ŒODVæ•°æ®é›†åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å…¬å¼€å¯ç”¨ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤§å¤šæ•°ODVæ•°æ®æ¥è‡ªåƒVimeoå’ŒYouTubeè¿™æ ·çš„å…¬å…±è§†é¢‘åˆ†äº«å¹³å°ã€‚åœ¨è¡¨[I](#S2.T1
    "TABLE I â€£ 2.3 Dataset â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision:
    A Survey and New Perspectives")ä¸­ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†ä¸€äº›ç”¨äºä¸åŒç›®çš„çš„ä»£è¡¨æ€§ODIå’ŒODVæ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬çš„å±æ€§ï¼Œä¾‹å¦‚ï¼Œå¤§å°ã€åˆ†è¾¨ç‡ã€æ•°æ®æ¥æºã€‚æ•°æ®é›†çš„å®Œæ•´æ€»ç»“å¯ä»¥åœ¨è¡¥å……ææ–™ä¸­æ‰¾åˆ°ã€‚æ ¹æ®æ•°æ®æ¥æºï¼Œæ•°æ®é›†åˆ†ä¸ºä¸¤ç±»ï¼šçœŸå®ä¸–ç•Œæ•°æ®é›†å’Œåˆæˆæ•°æ®é›†ã€‚å¤§å¤šæ•°çœŸå®ä¸–ç•Œæ•°æ®é›†ä»…æä¾›2DæŠ•å½±æ¨¡æ€çš„å›¾åƒï¼Œå¹¶åº”ç”¨äºæŸäº›ç‰¹å®šä»»åŠ¡ã€‚ç„¶è€Œï¼ŒStanford2D3D
    [[5](#bib.bib5)]åŒ…å«ä¸‰ç§æ¨¡æ€ï¼ŒåŒ…æ‹¬2Då’Œ2.5Dï¼Œé€‚åˆè·¨æ¨¡æ€å­¦ä¹ ã€‚æ­¤å¤–ï¼Œä¸€äº›æ•°æ®é›†æ˜¯ä»ç°æœ‰æ•°æ®é›†ä¸­é€‰æ‹©çš„ï¼Œä¾‹å¦‚ä»SUN360 [[3](#bib.bib3)]ä¸­æ”¶é›†çš„PanoContext
    [[7](#bib.bib7)]ã€‚å¯¹äºåˆæˆæ•°æ®é›†ï¼Œå›¾åƒå®Œæ•´ä¸”é«˜è´¨é‡ï¼Œæ²¡æœ‰è‡ªç„¶å™ªå£°ï¼Œæ³¨é‡Šæ¯”ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ›´å®¹æ˜“è·å¾—ã€‚ä¾‹å¦‚ï¼ŒSUNCG [[41](#bib.bib41)]
    æ˜¯é€šè¿‡Plannar5Då¹³å°åˆ›å»ºçš„ï¼Œæ‰€æœ‰3Dåœºæ™¯ç”±å•ç‹¬æ ‡è®°çš„3Dç‰©ä½“ç½‘æ ¼ç»„æˆã€‚Structured3D [[40](#bib.bib40)] å’ŒOmniFlow
    [[44](#bib.bib44)] åˆ©ç”¨æ¸²æŸ“å¼•æ“ç”ŸæˆåŒ…å«3Dç»“æ„æ³¨é‡Šå’Œç›¸åº”å…‰æµçš„ç…§ç‰‡çº§çœŸå®å›¾åƒã€‚ç±»ä¼¼äºçœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œè¿˜æœ‰ä¸€äº›æ•°æ®é›†ï¼Œä¾‹å¦‚ omni-SYNTHIA
    [[45](#bib.bib45)]ï¼Œæ˜¯ä»å¤§å‹åˆæˆæ•°æ®é›†ä¸­æå–çš„ï¼Œç”¨äºç‰¹å®šä»»åŠ¡ã€‚'
- en: 3 Omnidirectional Vision Tasks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 å…¨å‘è§†è§‰ä»»åŠ¡
- en: 3.1 Image/Video Manipulation
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 å›¾åƒ/è§†é¢‘å¤„ç†
- en: 3.1.1 Image Generation
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 å›¾åƒç”Ÿæˆ
- en: 'Insight: Image generation aims to restore or synthesize the complete and clean
    ODI data from the partial or noisy data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è§è§£ï¼šå›¾åƒç”Ÿæˆæ—¨åœ¨ä»éƒ¨åˆ†æˆ–å˜ˆæ‚çš„æ•°æ®ä¸­æ¢å¤æˆ–åˆæˆå®Œæ•´è€Œæ¸…æ™°çš„ODIæ•°æ®ã€‚
- en: 'For image generation on ODI, there exist four popular research directions:
    (i) panoramic depth map completion; (ii) ODI completion; (iii) panoramic semantic
    map completion; (iv) view synthesis on ODI. In this subsection, we provide a comprehensive
    analysis of some representative works.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºODIçš„å›¾åƒç”Ÿæˆï¼Œå­˜åœ¨å››ä¸ªçƒ­é—¨ç ”ç©¶æ–¹å‘ï¼šï¼ˆiï¼‰å…¨æ™¯æ·±åº¦å›¾å®Œæˆï¼›ï¼ˆiiï¼‰ODIå®Œæˆï¼›ï¼ˆiiiï¼‰å…¨æ™¯è¯­ä¹‰å›¾å®Œæˆï¼›ï¼ˆivï¼‰ODIä¸Šçš„è§†å›¾åˆæˆã€‚åœ¨è¿™ä¸€å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›ä»£è¡¨æ€§å·¥ä½œçš„å…¨é¢åˆ†æã€‚
- en: 'Depth Completion: Due to the scarcity of real-world sparse-to-dense panoramic
    depth maps, this task mainly utilizes simulation techniques to generate artificially
    sparse depth maps as the training data. Liu et al.Â [[46](#bib.bib46)] proposed
    a representative two-stage framework to achieve panoramic depth completion. In
    the first stage, a spherical normalized convolution network is proposed to predict
    the initial dense depth maps and confidence maps from the sparse depth inputs.
    Then the output of the first stage is combined with corresponding ODIs to generate
    the final panoramic dense depth maps through a cross-modal depth completion network.
    Especially, BIPSÂ [[47](#bib.bib47)] proposes a GAN framework to synthesize RGB-D
    indoor panoramas from the limited input information about a scene captured by
    the camera and depth sensors in arbitrary configurations. However, BIPS ignores
    a large distribution gap between synthesized and real LIDAR scanners, which could
    be better addressed with domain adaptation techniques.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦è¡¥å…¨ï¼šç”±äºç°å®ä¸–ç•Œä¸­ç¨€ç–åˆ°å¯†é›†çš„å…¨æ™¯æ·±åº¦å›¾éå¸¸ç¨€ç¼ºï¼Œè¯¥ä»»åŠ¡ä¸»è¦åˆ©ç”¨æ¨¡æ‹ŸæŠ€æœ¯ç”Ÿæˆäººå·¥ç¨€ç–æ·±åº¦å›¾ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚Liu ç­‰äºº [[46](#bib.bib46)]
    æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥å®ç°å…¨æ™¯æ·±åº¦è¡¥å…¨ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæå‡ºäº†ä¸€ç§çƒé¢å½’ä¸€åŒ–å·ç§¯ç½‘ç»œï¼Œç”¨äºä»ç¨€ç–æ·±åº¦è¾“å…¥ä¸­é¢„æµ‹åˆå§‹å¯†é›†æ·±åº¦å›¾å’Œç½®ä¿¡å›¾ã€‚ç„¶åï¼Œå°†ç¬¬ä¸€é˜¶æ®µçš„è¾“å‡ºä¸å¯¹åº”çš„ODIç»“åˆï¼Œé€šè¿‡è·¨æ¨¡æ€æ·±åº¦è¡¥å…¨ç½‘ç»œç”Ÿæˆæœ€ç»ˆçš„å…¨æ™¯å¯†é›†æ·±åº¦å›¾ã€‚ç‰¹åˆ«åœ°ï¼ŒBIPS
    [[47](#bib.bib47)] æå‡ºäº†ä¸€ä¸ªGANæ¡†æ¶ï¼Œä»æ‘„åƒæœºå’Œæ·±åº¦ä¼ æ„Ÿå™¨åœ¨ä»»æ„é…ç½®ä¸‹æ•è·çš„åœºæ™¯çš„æœ‰é™è¾“å…¥ä¿¡æ¯ä¸­åˆæˆRGB-Då®¤å†…å…¨æ™¯ã€‚ç„¶è€Œï¼ŒBIPSå¿½ç•¥äº†åˆæˆä¸çœŸå®LIDARæ‰«æä»ªä¹‹é—´çš„å¤§åˆ†å¸ƒå·®è·ï¼Œè¿™å¯ä»¥é€šè¿‡é¢†åŸŸé€‚åº”æŠ€æœ¯å¾—åˆ°æ›´å¥½çš„è§£å†³ã€‚
- en: 'ODI Completion: It aims to fill in missing areas to generate complete and plausible
    ODIs. Considering the high degree of freedom involved in generating an ODI from
    a single limited FoV image, Hara et al.Â [[48](#bib.bib48)] leveraged a fundamental
    property of the spherical structure, scene symmetry, to control the degree of
    freedom and improve the plausibility of the generated ODI. On the opposite ofÂ [[48](#bib.bib48)],
    Akimoto et al.Â [[49](#bib.bib49)] proposed a transformer-based framework to synthesize
    the ODIs with arbitrary resolution from a fixed limited FoV image and encouraged
    the diversity of synthesized ODIs. In addition, Sumantri et al.Â [[50](#bib.bib50)]
    proposed a first pipeline to reconstruct the ODIs from a set of unknown FoV images
    without any overlap, including two steps: (i) FoV estimation of input images relative
    to the panorama; (ii) ODI synthesis with the input images and estimated FoVs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ODIè¡¥å…¨ï¼šå®ƒæ—¨åœ¨å¡«è¡¥ç¼ºå¤±åŒºåŸŸä»¥ç”Ÿæˆå®Œæ•´ä¸”åˆç†çš„ODIã€‚è€ƒè™‘åˆ°ä»å•ä¸ªæœ‰é™FoVå›¾åƒç”ŸæˆODIæ—¶æ¶‰åŠçš„é«˜åº¦è‡ªç”±åº¦ï¼ŒHaraç­‰äºº [[48](#bib.bib48)]
    åˆ©ç”¨çƒé¢ç»“æ„çš„åŸºæœ¬å±æ€§â€”â€”åœºæ™¯å¯¹ç§°æ€§â€”â€”æ¥æ§åˆ¶è‡ªç”±åº¦å¹¶æé«˜ç”ŸæˆODIçš„åˆç†æ€§ã€‚ä¸ [[48](#bib.bib48)] ç›¸å¯¹çš„æ˜¯ï¼ŒAkimoto ç­‰äºº [[49](#bib.bib49)]
    æå‡ºäº†ä¸€ä¸ªåŸºäºå˜æ¢å™¨çš„æ¡†æ¶ï¼Œä»å›ºå®šçš„æœ‰é™FoVå›¾åƒä¸­åˆæˆä»»æ„åˆ†è¾¨ç‡çš„ODIï¼Œå¹¶é¼“åŠ±åˆæˆODIçš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼ŒSumantri ç­‰äºº [[50](#bib.bib50)]
    æå‡ºäº†ä¸€ä¸ªé¦–ä¸ªä»ä¸€ç»„æœªçŸ¥FoVå›¾åƒï¼ˆæ— é‡å ï¼‰ä¸­é‡å»ºODIçš„æµç¨‹ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šï¼ˆiï¼‰ç›¸å¯¹äºå…¨æ™¯çš„è¾“å…¥å›¾åƒFoVä¼°è®¡ï¼›ï¼ˆiiï¼‰ä½¿ç”¨è¾“å…¥å›¾åƒå’Œä¼°è®¡çš„FoVè¿›è¡ŒODIåˆæˆã€‚
- en: 'Semantic Scene Completion (SSC): It aims to reconstruct the indoor scenes with
    both the occupancy and semantic labels of the whole room. Existing works, e.g.,Â [[51](#bib.bib51)],
    are mostly based on the RGB-D data and LiDAR scanners. As the first work to accomplish
    the SSC task using the ODI data,Â [[52](#bib.bib52)] used only a single ODI and
    its corresponding depth map as the input and generates a voxel grid from the input
    panoramic depth map. This voxel grid is partitioned into eight overlapping views,
    and each partitioned grid, representing a single view of a regular RGB-D sensor,
    is submitted to the 3D CNN modelÂ [[53](#bib.bib53)], pre-trained on the standard
    2.5D synthetic RGB-D data. These partial inferences are aligned and ensembled
    to obtain the final result.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰ï¼šå®ƒæ—¨åœ¨é‡å»ºå…·æœ‰æ•´ä¸ªæˆ¿é—´å ç”¨å’Œè¯­ä¹‰æ ‡ç­¾çš„å®¤å†…åœºæ™¯ã€‚ç°æœ‰å·¥ä½œï¼Œå¦‚ [[51](#bib.bib51)]ï¼Œå¤§å¤šåŸºäºRGB-Dæ•°æ®å’ŒLiDARæ‰«æä»ªã€‚ä½œä¸ºé¦–ä¸ªä½¿ç”¨ODIæ•°æ®å®ŒæˆSSCä»»åŠ¡çš„å·¥ä½œï¼Œ[[52](#bib.bib52)]
    ä»…ä½¿ç”¨å•ä¸ªODIåŠå…¶å¯¹åº”çš„æ·±åº¦å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»è¾“å…¥çš„å…¨æ™¯æ·±åº¦å›¾ç”Ÿæˆä½“ç´ ç½‘æ ¼ã€‚è¯¥ä½“ç´ ç½‘æ ¼è¢«åˆ’åˆ†ä¸ºå…«ä¸ªé‡å è§†å›¾ï¼Œæ¯ä¸ªåˆ’åˆ†çš„ç½‘æ ¼ï¼Œä»£è¡¨ä¸€ä¸ªæ™®é€šRGB-Dä¼ æ„Ÿå™¨çš„å•ä¸€è§†å›¾ï¼Œè¢«æäº¤ç»™é¢„å…ˆåœ¨æ ‡å‡†2.5DåˆæˆRGB-Dæ•°æ®ä¸Šè®­ç»ƒçš„3D
    CNNæ¨¡å‹[[53](#bib.bib53)]ã€‚è¿™äº›éƒ¨åˆ†æ¨æ–­ç»“æœè¢«å¯¹é½å¹¶åˆå¹¶ï¼Œä»¥è·å¾—æœ€ç»ˆç»“æœã€‚
- en: 'TABLE II: Cross-view synthesis and geo-localization by some representative
    methods.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨IIï¼šä¸€äº›ä»£è¡¨æ€§æ–¹æ³•çš„è·¨è§†å›¾åˆæˆå’Œåœ°ç†å®šä½ã€‚
- en: '| Method | Publication | Input | View synthesis | Localization | Highlight
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å‘è¡¨æ—¶é—´ | è¾“å…¥ | è§†å›¾åˆæˆ | å®šä½ | äº®ç‚¹ |'
- en: '| Lu [[54](#bib.bib54)] | CVPRâ€™20 | Image | $\checkmark$ | âœ— | Utilizing depth
    and semantics |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Lu [[54](#bib.bib54)] | CVPRâ€™20 | å›¾åƒ | $\checkmark$ | âœ— | åˆ©ç”¨æ·±åº¦å’Œè¯­ä¹‰ |'
- en: '| Li [[55](#bib.bib55)] | ICCVâ€™21 | Video | $\checkmark$ | âœ— | 3D point cloud
    representation with depth and semantics |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Li [[55](#bib.bib55)] | ICCVâ€™21 | è§†é¢‘ | $\checkmark$ | âœ— | å¸¦æœ‰æ·±åº¦å’Œè¯­ä¹‰çš„3Dç‚¹äº‘è¡¨ç¤º
    |'
- en: '| Zhai [[56](#bib.bib56)] | CVPRâ€™17 | Image | $\checkmark$ | $\checkmark$ |
    Pretraining semantic segmentation task with transfer learning |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Zhai [[56](#bib.bib56)] | CVPRâ€™17 | å›¾åƒ | $\checkmark$ | $\checkmark$ | ä½¿ç”¨è¿ç§»å­¦ä¹ è¿›è¡Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡é¢„è®­ç»ƒ
    |'
- en: '| Regmi [[57](#bib.bib57)] | ICCVâ€™19 | Image | $\checkmark$ | $\checkmark$
    | Two stage training: Satellite-view synthesis and feature matching |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Regmi [[57](#bib.bib57)] | ICCVâ€™19 | å›¾åƒ | $\checkmark$ | $\checkmark$ | ä¸¤é˜¶æ®µè®­ç»ƒï¼šå«æ˜Ÿè§†å›¾åˆæˆå’Œç‰¹å¾åŒ¹é…
    |'
- en: '| Toker [[58](#bib.bib58)] | CVPRâ€™21 | Image | $\checkmark$ | $\checkmark$
    | End-to-end training for view synthesis and feature matching |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Toker [[58](#bib.bib58)] | CVPRâ€™21 | å›¾åƒ | $\checkmark$ | $\checkmark$ | è§†å›¾åˆæˆå’Œç‰¹å¾åŒ¹é…çš„ç«¯åˆ°ç«¯è®­ç»ƒ
    |'
- en: '| Shi [[59](#bib.bib59)] | NIPSâ€™19 | Image | âœ— | $\checkmark$ | Polar transform
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[59](#bib.bib59)] | NIPSâ€™19 | å›¾åƒ | âœ— | $\checkmark$ | æåæ ‡å˜æ¢ |'
- en: '| Zhu [[60](#bib.bib60)] | CVPRâ€™22 | Image | âœ— | $\checkmark$ | Attention-based
    transformer and remove uninformative patches |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[60](#bib.bib60)] | CVPRâ€™22 | å›¾åƒ | âœ— | $\checkmark$ | åŸºäºæ³¨æ„åŠ›çš„å˜æ¢å™¨å¹¶å»é™¤æ— ä¿¡æ¯è¡¥ä¸
    |'
- en: '| Shi [[61](#bib.bib61)] | CVPRâ€™20 | Image | âœ— | $\checkmark$ | Adding orientation
    estimation during localization |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[61](#bib.bib61)] | CVPRâ€™20 | å›¾åƒ | âœ— | $\checkmark$ | åœ¨å®šä½è¿‡ç¨‹ä¸­æ·»åŠ æ–¹å‘ä¼°è®¡ |'
- en: '| Zhu [[62](#bib.bib62)] | CVPRâ€™21 | Image | âœ— | $\checkmark$ | Proposing that
    multiple satellite images can cover one ground image |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[62](#bib.bib62)] | CVPRâ€™21 | å›¾åƒ | âœ— | $\checkmark$ | æå‡ºå¤šä¸ªå«æ˜Ÿå›¾åƒå¯ä»¥è¦†ç›–ä¸€ä¸ªåœ°é¢å›¾åƒ
    |'
- en: 'View Synthesis: View synthesis aims to generate ODIs from unknown viewpoints.
    OmniNeRF, proposed by Hsu et al.Â [[63](#bib.bib63)], is the first and representative
    learning approach for panoramic view synthesis. To generate a novel view ODI,
    it first projects an ODI to the 3D domain with an auxiliary depth map and a derived
    gradient image, and then translates the view position to re-project the 3D coordinates
    to 2D space. The neural radiance fields (NeRF)Â [[64](#bib.bib64)] is used to learn
    the pixel-based representations and solve the information missing problem caused
    by viewpoint translation. A similar strategy, proposed byÂ [[65](#bib.bib65)],
    leverages a conditional generator to synthesize the novel view. With video as
    the input, PathdreamerÂ [[66](#bib.bib66)] designs a hierarchical architecture
    to conduct the non-observed view synthesis from one previous observation and the
    trajectory of future viewpoints.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è§†å›¾åˆæˆï¼šè§†å›¾åˆæˆçš„ç›®æ ‡æ˜¯ä»æœªçŸ¥è§†è§’ç”ŸæˆODIã€‚ç”±Hsuç­‰äººæå‡ºçš„OmniNeRF [[63](#bib.bib63)] æ˜¯å…¨æ™¯è§†å›¾åˆæˆçš„é¦–ä¸ªä»£è¡¨æ€§å­¦ä¹ æ–¹æ³•ã€‚ä¸ºäº†ç”Ÿæˆæ–°è§†è§’çš„ODIï¼Œå®ƒé¦–å…ˆé€šè¿‡è¾…åŠ©æ·±åº¦å›¾å’Œè¡ç”Ÿçš„æ¢¯åº¦å›¾åƒå°†ODIæŠ•å½±åˆ°3Dé¢†åŸŸï¼Œç„¶åè½¬æ¢è§†å›¾ä½ç½®ä»¥é‡æ–°æŠ•å½±3Dåæ ‡åˆ°2Dç©ºé—´ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰
    [[64](#bib.bib64)] ç”¨äºå­¦ä¹ åŸºäºåƒç´ çš„è¡¨ç¤ºï¼Œå¹¶è§£å†³ç”±äºè§†è§’è½¬æ¢å¼•èµ·çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚ç±»ä¼¼çš„ç­–ç•¥ï¼Œç”± [[65](#bib.bib65)] æå‡ºï¼Œåˆ©ç”¨æ¡ä»¶ç”Ÿæˆå™¨åˆæˆæ–°è§†å›¾ã€‚ä»¥è§†é¢‘ä½œä¸ºè¾“å…¥ï¼ŒPathdreamer
    [[66](#bib.bib66)] è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚ç»“æ„ï¼Œä»¥ä»ä¸€æ¬¡å‰è§‚æµ‹å’Œæœªæ¥è§†è§’çš„è½¨è¿¹ä¸­è¿›è¡Œæœªè§‚æµ‹è§†å›¾åˆæˆã€‚
- en: 3.1.2 Cross-view Synthesis and Geo-localization
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 è·¨è§†å›¾åˆæˆä¸åœ°ç†å®šä½
- en: 'Insight: Cross-view synthesis aims to synthesize ground-view ODIs from the
    satellite-view images while geo-localization aims to match the ground-view ODIs
    and satellite-view images to determine their relations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è§è§£ï¼šè·¨è§†å›¾åˆæˆæ—¨åœ¨ä»å«æ˜Ÿè§†å›¾å›¾åƒåˆæˆåœ°é¢è§†å›¾ODIï¼Œè€Œåœ°ç†å®šä½æ—¨åœ¨åŒ¹é…åœ°é¢è§†å›¾ODIå’Œå«æ˜Ÿè§†å›¾å›¾åƒä»¥ç¡®å®šå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚
- en: 'Ground-view, a.k.a., street-view images are usually panoramic to provide complete
    surrounding information, while satellite views are planar images captured to cover
    almost every corner of the world. There exist a few methods to synthesize ground-view
    images from satellite-view images. Lu et al.Â [[54](#bib.bib54)] proposed a representative
    work including three stages: satellite stage, geo-transformation stage, and street-view
    stage. The satellite stage predicts depth maps and segmentation maps from satellite
    images. The geo-transformation stage transforms the output of the satellite stage
    into the panoramas. Finally, the street-view stage predicts the street-view panoramas
    from the segmentation maps via a GAN. Sat2VidÂ [[55](#bib.bib55)], the first work
    for cross-view video synthesis, also employs three stages to generate street-view
    ODVs using voxel grids with semantics and depth cues transformed from satellite
    images with trajectory. This is conceptually similar to that inÂ [[54](#bib.bib54)].'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ°é¢è§†å›¾ï¼Œä¹Ÿç§°ä¸ºè¡—æ™¯å›¾åƒï¼Œé€šå¸¸æ˜¯å…¨æ™¯çš„ï¼Œä»¥æä¾›å®Œæ•´çš„å‘¨è¾¹ä¿¡æ¯ï¼Œè€Œå«æ˜Ÿè§†å›¾åˆ™æ˜¯å¹³é¢å›¾åƒï¼Œæ•æ‰äº†å‡ ä¹ä¸–ç•Œçš„æ¯ä¸ªè§’è½ã€‚ä»å«æ˜Ÿè§†å›¾å›¾åƒåˆæˆåœ°é¢è§†å›¾å›¾åƒå­˜åœ¨ä¸€äº›æ–¹æ³•ã€‚Lu
    ç­‰äºº[[54](#bib.bib54)] æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å·¥ä½œï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šå«æ˜Ÿé˜¶æ®µã€åœ°ç†å˜æ¢é˜¶æ®µå’Œè¡—æ™¯é˜¶æ®µã€‚å«æ˜Ÿé˜¶æ®µä»å«æ˜Ÿå›¾åƒé¢„æµ‹æ·±åº¦å›¾å’Œåˆ†å‰²å›¾ã€‚åœ°ç†å˜æ¢é˜¶æ®µå°†å«æ˜Ÿé˜¶æ®µçš„è¾“å‡ºè½¬æ¢ä¸ºå…¨æ™¯å›¾ã€‚æœ€åï¼Œè¡—æ™¯é˜¶æ®µé€šè¿‡
    GAN ä»åˆ†å‰²å›¾é¢„æµ‹è¡—æ™¯å…¨æ™¯ã€‚Sat2Vid [[55](#bib.bib55)]ï¼Œå³ç¬¬ä¸€ä¸ªè·¨è§†è§’è§†é¢‘åˆæˆå·¥ä½œï¼Œä¹Ÿé‡‡ç”¨ä¸‰ä¸ªé˜¶æ®µæ¥ä½¿ç”¨å¸¦æœ‰è¯­ä¹‰å’Œæ·±åº¦æç¤ºçš„ä½“ç´ ç½‘æ ¼ç”Ÿæˆè¡—æ™¯
    ODVï¼Œè¿™ä¸ [[54](#bib.bib54)] ä¸­çš„æ¦‚å¿µç±»ä¼¼ã€‚
- en: 'In general, the framework for geo-localization consists of two modules: cross-synthesis
    module and retrieval module. Shi et al.Â [[59](#bib.bib59)] proposed a representative
    contrastive learning pipeline to calculate the distance between the ground-view
    ODIs and satellite-view images in the embedding space, similar to [[58](#bib.bib58),
    [57](#bib.bib57)]. In particular, in [[58](#bib.bib58)], a ground-view ODI is
    synthesized from the polar transformation of the satellite view via a GAN, supervised
    by the corresponding ground-view ground truth. Meanwhile, an extra retrieval branch
    is applied to constrain the latent representations of two domains. Using conditional
    GANs, Regmi et al.Â [[57](#bib.bib57)] skillfully synthesized the satellite-view
    image from the ground-view ODI. To learn a robust satellite query representation,
    they fused the features from the satellite-view synthesis and ground-view ODI,
    and then matched the query feature with satellite-view features in the embedding
    space. As the latest work, TransGeoÂ [[60](#bib.bib60)] is the first ViT-based
    framework to extract the position information from the satellite images and ground-view
    ODIs. With an attention mechanism, TransGeo removes uninformative patches in the
    satellite-view images and surpasses previous CNN-based methods.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œåœ°ç†å®šä½æ¡†æ¶ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šè·¨åˆæˆæ¨¡å—å’Œæ£€ç´¢æ¨¡å—ã€‚Shi ç­‰äºº[[59](#bib.bib59)] æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å¯¹æ¯”å­¦ä¹ ç®¡é“ï¼Œç”¨äºè®¡ç®—åµŒå…¥ç©ºé—´ä¸­åœ°é¢è§†å›¾
    ODI å’Œå«æ˜Ÿè§†å›¾å›¾åƒä¹‹é—´çš„è·ç¦»ï¼Œç±»ä¼¼äº [[58](#bib.bib58)ã€[57](#bib.bib57)]ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨ [[58](#bib.bib58)]
    ä¸­ï¼Œé€šè¿‡ GAN ä»å«æ˜Ÿè§†å›¾çš„æåæ ‡å˜æ¢åˆæˆåœ°é¢è§†å›¾ ODIï¼Œå—å¯¹åº”åœ°é¢è§†å›¾çœŸå®å›¾åƒçš„ç›‘ç£ã€‚åŒæ—¶ï¼Œé¢å¤–çš„æ£€ç´¢åˆ†æ”¯ç”¨äºçº¦æŸä¸¤ä¸ªé¢†åŸŸçš„æ½œåœ¨è¡¨ç¤ºã€‚Regmi ç­‰äºº
    [[57](#bib.bib57)] ä½¿ç”¨æ¡ä»¶ GAN å·§å¦™åœ°ä»åœ°é¢è§†å›¾ ODI åˆæˆå«æ˜Ÿè§†å›¾å›¾åƒã€‚ä¸ºäº†å­¦ä¹ é²æ£’çš„å«æ˜ŸæŸ¥è¯¢è¡¨ç¤ºï¼Œä»–ä»¬èåˆäº†å«æ˜Ÿè§†å›¾åˆæˆå’Œåœ°é¢è§†å›¾
    ODI çš„ç‰¹å¾ï¼Œç„¶ååœ¨åµŒå…¥ç©ºé—´ä¸­åŒ¹é…æŸ¥è¯¢ç‰¹å¾ä¸å«æ˜Ÿè§†å›¾ç‰¹å¾ã€‚ä½œä¸ºæœ€æ–°çš„å·¥ä½œï¼ŒTransGeo [[60](#bib.bib60)] æ˜¯ç¬¬ä¸€ä¸ªåŸºäº ViT çš„æ¡†æ¶ï¼Œç”¨äºä»å«æ˜Ÿå›¾åƒå’Œåœ°é¢è§†å›¾
    ODI ä¸­æå–ä½ç½®ä¿¡æ¯ã€‚é€šè¿‡æ³¨æ„æœºåˆ¶ï¼ŒTransGeo å»é™¤äº†å«æ˜Ÿè§†å›¾å›¾åƒä¸­ä¸ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œè¶…è¶Šäº†ä¹‹å‰åŸºäº CNN çš„æ–¹æ³•ã€‚
- en: 'Discussion: Most cross-view synthesis and geo-localization methods assume that
    a reference image is precisely centered at the location of any query image. Nonetheless,
    in practice, the two views are usually not perfectly aligned in terms of orientationÂ [[61](#bib.bib61)]
    and spatial location[[62](#bib.bib62)]. Therefore, how to apply cross-view synthesis
    and geo-localization methods under challenging conditions is a valuable research
    direction.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºï¼šå¤§å¤šæ•°è·¨è§†è§’åˆæˆå’Œåœ°ç†å®šä½æ–¹æ³•å‡è®¾å‚è€ƒå›¾åƒç²¾ç¡®åœ°ä½äºä»»ä½•æŸ¥è¯¢å›¾åƒçš„ä½ç½®ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œä¸¤ä¸ªè§†å›¾é€šå¸¸åœ¨æ–¹å‘ [[61](#bib.bib61)]
    å’Œç©ºé—´ä½ç½® [[62](#bib.bib62)] ä¸Šå¹¶ä¸å®Œå…¨å¯¹é½ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹åº”ç”¨è·¨è§†è§’åˆæˆå’Œåœ°ç†å®šä½æ–¹æ³•æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„ç ”ç©¶æ–¹å‘ã€‚
- en: 3.1.3 Compression
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 å‹ç¼©
- en: Compared with conventional perspective images, omnidirectional data records
    richer geometrical information with a higher resolution and wider FoV, making
    it more challenging to achieve effective compression. The early approaches for
    ODI compression directly utilize the existing perspective methods to compress
    the perspective projections of the ODIs. For instance, Simone et al.Â [[67](#bib.bib67)]
    proposed an adaptive quantization method to solve the frequency shift in the viewport
    image blocks when projecting the ODI to the ERP. By contrast, OmniJPEGÂ [[68](#bib.bib68)]
    first estimates the region of interest in the ODI and then encodes the ODI based
    on the geometrical transformation of the region content with a novel format called
    OmniJPEG, which is an extension of JPEG formatÂ [[69](#bib.bib69)] and can be viewable
    on legacy JPEG decoders. Considering the ERP distortion, a graph-based coder is
    proposed byÂ [[70](#bib.bib70)] to adapt the sphere surface. To make the coding
    progress computationally feasible, the graph partitioning algorithm based on rate
    distortion optimizationÂ [[71](#bib.bib71)] is introduced to achieve a trade-off
    between the distortion of reconstructed signals, the signal smoothness on each
    sub-graph, and the coding cost of partitioning description. As a representative
    CNN-based ODI compression work, OSLOÂ [[72](#bib.bib72)] applies HEALPixÂ [[73](#bib.bib73)]
    to define a convolution operation directly on the sphere and adapt the standard
    CNN techniques to the spherical domain. The proposed on-the-sphere representation
    outperforms the similar learnable compression models on the ERP.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿé€è§†å›¾åƒç›¸æ¯”ï¼Œå…¨å‘æ•°æ®è®°å½•äº†æ›´ä¸°å¯Œçš„å‡ ä½•ä¿¡æ¯ï¼Œå…·æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡å’Œæ›´å¹¿çš„è§†åœºï¼Œè¿™ä½¿å¾—æœ‰æ•ˆå‹ç¼©å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ—©æœŸçš„ODIå‹ç¼©æ–¹æ³•ç›´æ¥åˆ©ç”¨ç°æœ‰çš„é€è§†æ–¹æ³•æ¥å‹ç¼©ODIçš„é€è§†æŠ•å½±ã€‚ä¾‹å¦‚ï¼ŒSimone
    ç­‰äºº[[67](#bib.bib67)] æå‡ºäº†è‡ªé€‚åº”é‡åŒ–æ–¹æ³•æ¥è§£å†³å°†ODIæŠ•å½±åˆ°ERPæ—¶è§†å£å›¾åƒå—ä¸­çš„é¢‘ç‡åç§»é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒOmniJPEG [[68](#bib.bib68)]
    é¦–å…ˆä¼°è®¡ODIä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼Œç„¶ååŸºäºè¯¥åŒºåŸŸå†…å®¹çš„å‡ ä½•å˜æ¢è¿›è¡Œç¼–ç ï¼Œé‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºOmniJPEGçš„æ–°æ ¼å¼ï¼Œè¯¥æ ¼å¼æ˜¯JPEGæ ¼å¼çš„æ‰©å±•[[69](#bib.bib69)]ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¼ ç»Ÿçš„JPEGè§£ç å™¨ä¸ŠæŸ¥çœ‹ã€‚è€ƒè™‘åˆ°ERPå¤±çœŸï¼Œ[[70](#bib.bib70)]
    æå‡ºäº†åŸºäºå›¾çš„ç¼–ç å™¨æ¥é€‚åº”çƒé¢ã€‚ä¸ºäº†ä½¿ç¼–ç è¿‡ç¨‹åœ¨è®¡ç®—ä¸Šå¯è¡Œï¼Œå¼•å…¥äº†åŸºäºç‡å¤±çœŸä¼˜åŒ–çš„å›¾åˆ’åˆ†ç®—æ³•[[71](#bib.bib71)]ï¼Œä»¥åœ¨é‡å»ºä¿¡å·çš„å¤±çœŸã€æ¯ä¸ªå­å›¾ä¸Šçš„ä¿¡å·å¹³æ»‘æ€§å’Œåˆ’åˆ†æè¿°çš„ç¼–ç æˆæœ¬ä¹‹é—´å®ç°æƒè¡¡ã€‚ä½œä¸ºä»£è¡¨æ€§çš„åŸºäºCNNçš„ODIå‹ç¼©å·¥ä½œï¼ŒOSLO
    [[72](#bib.bib72)] åº”ç”¨HEALPix [[73](#bib.bib73)] ç›´æ¥åœ¨çƒé¢ä¸Šå®šä¹‰å·ç§¯æ“ä½œï¼Œå¹¶å°†æ ‡å‡†CNNæŠ€æœ¯é€‚åº”äºçƒé¢åŸŸã€‚æ‰€æå‡ºçš„çƒé¢è¡¨ç¤ºåœ¨ERPä¸Šçš„è¡¨ç°ä¼˜äºç±»ä¼¼çš„å¯å­¦ä¹ å‹ç¼©æ¨¡å‹ã€‚
- en: For ODV compression, Li et al.Â [[74](#bib.bib74)] proposed a representative
    work aiming to optimize the ODV encoding progress. They analyzed the distortion
    impacts of restoring spherical domain signals from the different planar projection
    types and then applied the rate distortion optimization based on the distortion
    of signal in spherical domain. Similarly, Wang et al.Â [[75](#bib.bib75)] proposed
    a spherical coordinates transform-based motion model to address the distortion
    problem in projections. Another representative method Â [[76](#bib.bib76)] maps
    the ODV to the rhombic dodecahedron (RD) map and directly applies the planar perspective
    videos encoding methods on the RD map. Specifically, the rate control-based algorithms
    are proposed to achieve better qualities and smaller bitrate errors for ODV compressionÂ [[77](#bib.bib77)],Â [[78](#bib.bib78)].
    Zhao et al.Â [[78](#bib.bib78)] utilized game theory to find optimal inter/intra-frame
    bitrate allocations while Li et al.Â [[77](#bib.bib77)] proposed a novel bit allocation
    algorithm for ERP with the coding tree unit (CTU) level. Similar toÂ [[20](#bib.bib20)],
    the CTUs in the same row have the same weight to reduce the distortion influence.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºODVå‹ç¼©ï¼ŒLi ç­‰äºº[[74](#bib.bib74)] æå‡ºäº†æ—¨åœ¨ä¼˜åŒ–ODVç¼–ç è¿‡ç¨‹çš„ä»£è¡¨æ€§å·¥ä½œã€‚ä»–ä»¬åˆ†æäº†ä»ä¸åŒå¹³é¢æŠ•å½±ç±»å‹æ¢å¤çƒé¢åŸŸä¿¡å·çš„å¤±çœŸå½±å“ï¼Œç„¶ååº”ç”¨äº†åŸºäºçƒé¢åŸŸä¿¡å·å¤±çœŸçš„ç‡å¤±çœŸä¼˜åŒ–æ–¹æ³•ã€‚ç±»ä¼¼åœ°ï¼ŒWang
    ç­‰äºº[[75](#bib.bib75)] æå‡ºäº†åŸºäºçƒé¢åæ ‡å˜æ¢çš„è¿åŠ¨æ¨¡å‹æ¥è§£å†³æŠ•å½±ä¸­çš„å¤±çœŸé—®é¢˜ã€‚å¦ä¸€ç§ä»£è¡¨æ€§æ–¹æ³•[[76](#bib.bib76)] å°†ODVæ˜ å°„åˆ°è±å½¢åäºŒé¢ä½“ï¼ˆRDï¼‰å›¾ä¸Šï¼Œå¹¶ç›´æ¥åœ¨RDå›¾ä¸Šåº”ç”¨å¹³é¢é€è§†è§†é¢‘ç¼–ç æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œæå‡ºäº†åŸºäºç‡æ§åˆ¶çš„ç®—æ³•ï¼Œä»¥å®ç°æ›´å¥½çš„è´¨é‡å’Œæ›´å°çš„æ¯”ç‰¹ç‡è¯¯å·®ç”¨äºODVå‹ç¼©[[77](#bib.bib77)]ï¼Œ[[78](#bib.bib78)]ã€‚Zhao
    ç­‰äºº[[78](#bib.bib78)] åˆ©ç”¨åšå¼ˆè®ºæ‰¾åˆ°æœ€ä¼˜çš„å¸§é—´/å¸§å†…æ¯”ç‰¹ç‡åˆ†é…ï¼Œè€ŒLi ç­‰äºº[[77](#bib.bib77)] æå‡ºäº†ç”¨äºERPçš„æ–°æ¯”ç‰¹åˆ†é…ç®—æ³•ï¼ŒåŸºäºç¼–ç æ ‘å•å…ƒï¼ˆCTUï¼‰çº§åˆ«ã€‚ç±»ä¼¼äº[[20](#bib.bib20)]ï¼ŒåŒä¸€è¡Œçš„CTUå…·æœ‰ç›¸åŒçš„æƒé‡ï¼Œä»¥å‡å°‘å¤±çœŸå½±å“ã€‚
- en: 'Potential and Challenges: Based on the aforementioned analysis, only a few
    DL-based methods exist in this research domain. Most works combine the traditional
    planar coding methods with geometric information in the spherical domain. There
    remain some challenges for DL-based ODI/ODV compression. DL-based image compression
    methods require the effective metrics as the constraint, e.g., peak signal-to-noise
    ratio (PSNR), and structural similarity (SSIM). However, due to spherical imaging,
    traditional metrics are weak to measure the qualities of ODI. Furthermore, the
    planar projections of the ODI are high memory and distorted, which increase the
    computation cost and compression difficulty. Future research might consider extending
    more effective metrics based on the spherical geometric information and restoring
    a high-quality compressed ODI from a partial input.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåŠ›ä¸æŒ‘æˆ˜ï¼šåŸºäºä¸Šè¿°åˆ†æï¼Œè¿™ä¸€ç ”ç©¶é¢†åŸŸä»…å­˜åœ¨å°‘æ•°åŸºäº DL çš„æ–¹æ³•ã€‚å¤§å¤šæ•°å·¥ä½œå°†ä¼ ç»Ÿçš„å¹³é¢ç¼–ç æ–¹æ³•ä¸çƒé¢é¢†åŸŸçš„å‡ ä½•ä¿¡æ¯ç»“åˆåœ¨ä¸€èµ·ã€‚åŸºäº DL çš„ ODI/ODV
    å‹ç¼©ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚åŸºäº DL çš„å›¾åƒå‹ç¼©æ–¹æ³•éœ€è¦æœ‰æ•ˆçš„åº¦é‡ä½œä¸ºçº¦æŸï¼Œä¾‹å¦‚å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰ã€‚ç„¶è€Œï¼Œç”±äºçƒé¢æˆåƒï¼Œä¼ ç»Ÿåº¦é‡åœ¨è¡¡é‡
    ODI è´¨é‡æ–¹é¢è¾ƒå¼±ã€‚æ­¤å¤–ï¼ŒODI çš„å¹³é¢æŠ•å½±å…·æœ‰é«˜å†…å­˜å’Œå¤±çœŸï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬å’Œå‹ç¼©éš¾åº¦ã€‚æœªæ¥çš„ç ”ç©¶å¯èƒ½è€ƒè™‘åŸºäºçƒé¢å‡ ä½•ä¿¡æ¯æ‰©å±•æ›´æœ‰æ•ˆçš„åº¦é‡ï¼Œå¹¶ä»éƒ¨åˆ†è¾“å…¥ä¸­æ¢å¤é«˜è´¨é‡çš„å‹ç¼©
    ODIã€‚
- en: 3.1.4 Lighting Estimation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 ç…§æ˜ä¼°è®¡
- en: 'Insight: It aims to predict the high dynamic range (HDR) illumination from
    low dynamic range (LDR) ODIs.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è§è§£ï¼šå®ƒæ—¨åœ¨ä»ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰ODIä¸­é¢„æµ‹é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰ç…§æ˜ã€‚
- en: Illumination recovery is widely employed in many real-world tasks ranging from
    scene understanding, reconstruction to editing. Hold-Geoffroy et al.Â [[79](#bib.bib79)]
    proposed a representative framework for outdoor illumination estimation. They
    first trained a CNN model to predict the sky parameters from viewports of outdoor
    ODIs, e.g., sun position and atmospheric conditions. They then reconstructed illumination
    environment maps for the given test images according to the predicted illumination
    parameters. Similarly, inÂ [[80](#bib.bib80)], a CNN model is leveraged to predict
    the location of lights in the viewports, and the CNN is fine-tuned to predict
    the light intensities, i.e., environment maps, from the ODIs. InÂ [[81](#bib.bib81)],
    geometric and photometric parameters of indoor lighting are regressed from the
    viewports of ODI, and the intermediate latent vectors are used to reconstruct
    the environment maps. Another representative method, called EMLightÂ [[82](#bib.bib82)],
    consists of a regression network and a neural projector. The regression network
    outputs the light parameters, and the neural projector converts the light parameters
    into the illumination map. In particular, the ground truths of the light parameters
    are decomposed by a Gaussian map generated from the illumination via a spherical
    Gaussian function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§æ˜æ¢å¤åœ¨è®¸å¤šå®é™…ä»»åŠ¡ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬åœºæ™¯ç†è§£ã€é‡å»ºå’Œç¼–è¾‘ã€‚Hold-Geoffroy ç­‰äºº [[79](#bib.bib79)] æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§çš„æˆ·å¤–ç…§æ˜ä¼°è®¡æ¡†æ¶ã€‚ä»–ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ª
    CNN æ¨¡å‹ï¼Œä»¥ä»æˆ·å¤– ODI çš„è§†å£ä¸­é¢„æµ‹å¤©ç©ºå‚æ•°ï¼Œä¾‹å¦‚å¤ªé˜³ä½ç½®å’Œå¤§æ°”æ¡ä»¶ã€‚ç„¶åï¼Œä»–ä»¬æ ¹æ®é¢„æµ‹çš„ç…§æ˜å‚æ•°é‡å»ºäº†ç»™å®šæµ‹è¯•å›¾åƒçš„ç…§æ˜ç¯å¢ƒå›¾ã€‚åŒæ ·ï¼Œåœ¨ [[80](#bib.bib80)]
    ä¸­ï¼Œåˆ©ç”¨ CNN æ¨¡å‹é¢„æµ‹è§†å£ä¸­å…‰æºçš„ä½ç½®ï¼Œå¹¶å¯¹ CNN è¿›è¡Œå¾®è°ƒï¼Œä»¥ä» ODI ä¸­é¢„æµ‹å…‰å¼ºåº¦ï¼Œå³ç¯å¢ƒå›¾ã€‚åœ¨ [[81](#bib.bib81)] ä¸­ï¼Œä»
    ODI çš„è§†å£å›å½’å‡ºå®¤å†…ç…§æ˜çš„å‡ ä½•å’Œå…‰åº¦å‚æ•°ï¼Œå¹¶ä½¿ç”¨ä¸­é—´æ½œåœ¨å‘é‡é‡å»ºç¯å¢ƒå›¾ã€‚å¦ä¸€ä¸ªä»£è¡¨æ€§æ–¹æ³•ç§°ä¸º EMLight [[82](#bib.bib82)]ï¼Œç”±å›å½’ç½‘ç»œå’Œç¥ç»æŠ•å½±ä»ªç»„æˆã€‚å›å½’ç½‘ç»œè¾“å‡ºå…‰å‚æ•°ï¼Œç¥ç»æŠ•å½±ä»ªå°†å…‰å‚æ•°è½¬æ¢ä¸ºç…§æ˜å›¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…‰å‚æ•°çš„çœŸå®å€¼é€šè¿‡ä»ç…§æ˜ä¸­ç”Ÿæˆçš„é«˜æ–¯å›¾åˆ†è§£ï¼Œè¿™ç”±çƒé¢é«˜æ–¯å‡½æ•°ç”Ÿæˆã€‚
- en: 'Discussion and Potential: From the aforementioned analysis, previous works
    for lighting estimation on ODIs take a single viewport as the input. The reason
    might be that the viewports are distortion-less and low-cost with low resolution.
    However, they suffer from severe drop of spatial information. Hence, it could
    be beneficial to apply contrastive learning to learn the robust representations
    from the multiple viewports or components of the tangent images.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºä¸æ½œåŠ›ï¼šä»ä¸Šè¿°åˆ†æå¯ä»¥çœ‹å‡ºï¼Œå…ˆå‰çš„ç…§æ˜ä¼°è®¡å·¥ä½œåœ¨ ODI ä¸Šä»…ä½¿ç”¨å•ä¸ªè§†å£ä½œä¸ºè¾“å…¥ã€‚åŸå› å¯èƒ½æ˜¯è§†å£æ²¡æœ‰å¤±çœŸä¸”æˆæœ¬ä½ï¼Œåˆ†è¾¨ç‡è¾ƒä½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸¥é‡ç¼ºä¹ç©ºé—´ä¿¡æ¯ã€‚å› æ­¤ï¼Œå°†å¯¹æ¯”å­¦ä¹ åº”ç”¨äºä»å¤šä¸ªè§†å£æˆ–åˆ‡çº¿å›¾åƒç»„ä»¶ä¸­å­¦ä¹ é²æ£’è¡¨ç¤ºå¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚
- en: 3.1.5 ODI Super-Resolution (SR)
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 ODI è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰
- en: Existing Head-Mounted Display (HMD) devicesÂ [[83](#bib.bib83)] require at least
    the ODI with 21600$\times$10800 pixels for immersive experience, which can not
    be directly captured by current camera systemsÂ [[84](#bib.bib84)]. One alternative
    way is to capture low resolution (LR) ODIs and super-resolve them into high resolution
    (HR) ODIs efficiently. LAU-NetÂ [[85](#bib.bib85)], as the first work to consider
    the latitude difference for ODI SR, introduces a multi-level latitude adaptive
    network. It splits an ODI into different latitude bands and hierarchically upscales
    these bands with different adaptive factors, which are learned via a reinforcement
    learning scheme. Beyond considering SR on the ERP, Yoon et al.Â [[28](#bib.bib28)]
    proposed a representative work, SphereSR, to learn a unified continuous spherical
    local implicit image function and generate an arbitrary projection with arbitrary
    resolution according to the spherical coordinate queries. For ODV SR, SMFNÂ [[86](#bib.bib86)]
    is the first DNN-based framework, including a single-frame and multi-frame joint
    network and a dual network. The single-frame and multi-frame joint network fuses
    the features from adjacent frames, and the dual network constrains the solution
    space to find a better answer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°æœ‰çš„å¤´æˆ´æ˜¾ç¤ºå™¨ï¼ˆHMDï¼‰è®¾å¤‡[[83](#bib.bib83)]éœ€è¦è‡³å°‘21600$\times$10800åƒç´ çš„ODIä»¥è·å¾—æ²‰æµ¸å¼ä½“éªŒï¼Œè€Œå½“å‰çš„ç›¸æœºç³»ç»Ÿ[[84](#bib.bib84)]æ— æ³•ç›´æ¥æ•æ‰åˆ°è¿™ä¹ˆé«˜åˆ†è¾¨ç‡çš„å›¾åƒã€‚ä¸€ä¸ªæ›¿ä»£æ–¹æ³•æ˜¯æ•æ‰ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰ODIå¹¶é«˜æ•ˆåœ°å°†å…¶è¶…åˆ†è¾¨ç‡å¤„ç†ä¸ºé«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ODIã€‚LAU-Net[[85](#bib.bib85)]ä½œä¸ºé¦–ä¸ªè€ƒè™‘ODIè¶…åˆ†è¾¨ç‡çš„çº¬åº¦å·®å¼‚çš„å·¥ä½œï¼Œä»‹ç»äº†ä¸€ä¸ªå¤šå±‚æ¬¡çº¬åº¦è‡ªé€‚åº”ç½‘ç»œã€‚å®ƒå°†ODIåˆ†å‰²æˆä¸åŒçš„çº¬åº¦å¸¦ï¼Œå¹¶ä»¥ä¸åŒçš„è‡ªé€‚åº”å› å­åˆ†å±‚æ”¾å¤§è¿™äº›å¸¦ï¼Œè¿™äº›å› å­é€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆè¿›è¡Œå­¦ä¹ ã€‚é™¤äº†åœ¨ERPä¸Šè€ƒè™‘è¶…åˆ†è¾¨ç‡ï¼ŒYoonç­‰äºº[[28](#bib.bib28)]æå‡ºäº†ä¸€é¡¹å…·æœ‰ä»£è¡¨æ€§çš„å·¥ä½œSphereSRï¼Œæ—¨åœ¨å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„è¿ç»­çƒé¢å±€éƒ¨éšå¼å›¾åƒå‡½æ•°ï¼Œå¹¶æ ¹æ®çƒé¢åæ ‡æŸ¥è¯¢ç”Ÿæˆä»»æ„åˆ†è¾¨ç‡çš„æŠ•å½±ã€‚å¯¹äºODVè¶…åˆ†è¾¨ç‡ï¼ŒSMFN[[86](#bib.bib86)]æ˜¯ç¬¬ä¸€ä¸ªåŸºäºDNNçš„æ¡†æ¶ï¼ŒåŒ…æ‹¬å•å¸§å’Œå¤šå¸§è”åˆç½‘ç»œåŠåŒç½‘ç»œã€‚å•å¸§å’Œå¤šå¸§è”åˆç½‘ç»œèåˆäº†æ¥è‡ªç›¸é‚»å¸§çš„ç‰¹å¾ï¼ŒåŒç½‘ç»œåˆ™çº¦æŸäº†è§£ç©ºé—´ï¼Œä»¥æ‰¾åˆ°æ›´å¥½çš„ç­”æ¡ˆã€‚
- en: 3.1.6 Upright Adjustment
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 ç›´ç«‹è°ƒæ•´
- en: 'Insight: Upright adjustment aims to correct the misalignment of the orientations
    between the camera and scene to improve the visual quality of ODI and ODV while
    they are used with a narrow field-of-view (NFoV) display, such as the VR application.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Insight: ç›´ç«‹è°ƒæ•´æ—¨åœ¨çº æ­£ç›¸æœºä¸åœºæ™¯ä¹‹é—´æ–¹å‘çš„ä¸å¯¹é½ï¼Œä»¥æé«˜ä½¿ç”¨çª„è§†åœºï¼ˆNFoVï¼‰æ˜¾ç¤ºå™¨ï¼ˆå¦‚VRåº”ç”¨ï¼‰æ—¶ODIå’ŒODVçš„è§†è§‰è´¨é‡ã€‚'
- en: 'The standard approach of upright adjustment follows two steps: (i) estimating
    the position of the pole of the ODI; (ii) applying a rotation matrix to align
    the estimated north pole. The early representative workÂ [[87](#bib.bib87)] estimates
    the camera rotation according to the geometric structures in the panoramas, e.g.,
    curving straight lines and vanishing points. However, these methods are limited
    to the ManhattanÂ [[88](#bib.bib88)] or Atlanta worldÂ [[89](#bib.bib89)] assumption
    and rely on necessary prior knowledge of geometric structures. Recently, DL-based
    upright adjustment has been widely studied. Without any specific assumption on
    the scene structure, DeepUAÂ [[90](#bib.bib90)] proposes a representative CNN-based
    framework to estimate the 2D rotations of multiple NFoV images sampled from the
    ODI and then estimate the 3D camera rotation through the geometric relationship
    between 3D and 2D rotations. By contrast, Deep360UpÂ [[91](#bib.bib91)] directly
    takes ERP image as the input and synthesizes the upright version according to
    the estimated up-vector orientation. In particular, Jung et al.Â [[92](#bib.bib92)]
    proposed a two-stage pipeline for ODI upright adjustment. First, the feature map
    is extracted by a CNN model from the rotated ERP image. The feature map is then
    mapped into a spherical graph. Finally, a GCN is applied to estimate the 3D camera
    rotation, which is the location of the point on the spherical surface corresponding
    to the north pole.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†çš„ç›´ç«‹è°ƒæ•´æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼š(i) ä¼°è®¡ODIçš„æç‚¹ä½ç½®ï¼›(ii) åº”ç”¨æ—‹è½¬çŸ©é˜µå¯¹é½ä¼°è®¡å‡ºçš„åŒ—æç‚¹ã€‚æ—©æœŸçš„ä»£è¡¨æ€§å·¥ä½œ[[87](#bib.bib87)]æ ¹æ®å…¨æ™¯å›¾ä¸­çš„å‡ ä½•ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œå¼¯æ›²çš„ç›´çº¿å’Œæ¶ˆå¤±ç‚¹ï¼‰æ¥ä¼°è®¡ç›¸æœºæ—‹è½¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—é™äºæ›¼å“ˆé¡¿[[88](#bib.bib88)]æˆ–äºšç‰¹å…°å¤§ä¸–ç•Œ[[89](#bib.bib89)]å‡è®¾ï¼Œå¹¶ä¾èµ–äºå‡ ä½•ç»“æ„çš„å¿…è¦å…ˆéªŒçŸ¥è¯†ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ç›´ç«‹è°ƒæ•´å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ã€‚æ— éœ€å¯¹åœºæ™¯ç»“æ„åšä»»ä½•ç‰¹å®šå‡è®¾ï¼ŒDeepUA[[90](#bib.bib90)]æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§çš„åŸºäºCNNçš„æ¡†æ¶ï¼Œæ¥ä¼°è®¡ä»ODIä¸­é‡‡æ ·çš„å¤šä¸ªNFoVå›¾åƒçš„äºŒç»´æ—‹è½¬ï¼Œç„¶åé€šè¿‡ä¸‰ç»´å’ŒäºŒç»´æ—‹è½¬ä¹‹é—´çš„å‡ ä½•å…³ç³»æ¥ä¼°è®¡ä¸‰ç»´ç›¸æœºæ—‹è½¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDeep360Up[[91](#bib.bib91)]ç›´æ¥å°†ERPå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®ä¼°è®¡çš„ä¸Šå‘é‡æ–¹å‘åˆæˆç›´ç«‹ç‰ˆæœ¬ã€‚ç‰¹åˆ«åœ°ï¼ŒJungç­‰äºº[[92](#bib.bib92)]æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„ODIç›´ç«‹è°ƒæ•´ç®¡é“ã€‚é¦–å…ˆï¼Œé€šè¿‡CNNæ¨¡å‹ä»æ—‹è½¬åçš„ERPå›¾åƒä¸­æå–ç‰¹å¾å›¾ã€‚ç„¶åï¼Œå°†ç‰¹å¾å›¾æ˜ å°„åˆ°çƒé¢å›¾ä¸­ã€‚æœ€åï¼Œåº”ç”¨GCNæ¥ä¼°è®¡ä¸‰ç»´ç›¸æœºæ—‹è½¬ï¼Œå³å¯¹åº”åŒ—æç‚¹çš„çƒé¢ä¸Šçš„ç‚¹çš„ä½ç½®ã€‚
- en: '![Refer to caption](img/cd1c3e0d3e352964770dd8bf94921631.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/cd1c3e0d3e352964770dd8bf94921631.png)'
- en: 'Figure 6: Representative networks for ODI-QA and ODV-QA. (a), (b) and (c) are
    originally shown inÂ [[93](#bib.bib93)],Â [[2](#bib.bib2)] andÂ [[94](#bib.bib94)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šODI-QAå’ŒODV-QAçš„ä»£è¡¨æ€§ç½‘ç»œã€‚(a)ã€(b)å’Œ(c)æœ€åˆæ˜¾ç¤ºåœ¨[[93](#bib.bib93)]ã€[[2](#bib.bib2)]å’Œ[[94](#bib.bib94)]ä¸­ã€‚
- en: 3.1.7 Visual Quality Assessment
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 è§†è§‰è´¨é‡è¯„ä¼°
- en: Due to the ultra-high resolution and sphere representation of omnidirectional
    data, visual quality assessment (V-QA) is valuable for the optimization of exiting
    image/video processing algorithms. We next introduce some representative works
    on ODI-QA and ODV-QA, respectively.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¨æ™¯æ•°æ®çš„è¶…é«˜åˆ†è¾¨ç‡å’Œçƒé¢è¡¨ç¤ºï¼Œè§†è§‰è´¨é‡è¯„ä¼°ï¼ˆV-QAï¼‰å¯¹äºä¼˜åŒ–ç°æœ‰çš„å›¾åƒ/è§†é¢‘å¤„ç†ç®—æ³•éå¸¸æœ‰ä»·å€¼ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ†åˆ«ä»‹ç»ä¸€äº›å…³äºODI-QAå’ŒODV-QAçš„ä»£è¡¨æ€§å·¥ä½œã€‚
- en: 'For the ODI-QA, according to the availability of the reference images, it can
    be further classified into two categories: full-reference (FR) ODI-QA and no-reference
    (NR) ODI-QA. In exiting methods on FR ODI-QA, some works focus on extending the
    conventional FR image quality assessment metrics, e.g., PSNR and SSIM, to the
    omnidirectional domain, e.g.,Â [[95](#bib.bib95)],Â [[96](#bib.bib96)]. These works
    introduce special geometric structures of the ODI and its projection representations
    to traditional quality assessment metrics and measure the objective quality more
    accurately. In addition, there are a few DL-based approaches for FR ODI-QA. As
    the representative work shown in Fig.Â [6](#S3.F6 "Figure 6 â€£ 3.1.6 Upright Adjustment
    â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(a), Lim et al.Â [[93](#bib.bib93),
    [97](#bib.bib97)] proposed a novel adversarial learning framework, consisting
    of a quality score predictor and a human perception guider, to automatically assess
    the image quality following the human perception. NR ODI-QA, also called blind
    ODI-QA, predicts the ODI quality without expensive reference ODIs. Considering
    multi-viewport images in the ERP format, Xu et al.Â [[98](#bib.bib98)] applied
    a novel viewport-oriented GCN to process the distortion-less viewports in ERP
    images and aggregated these features to estimate the quality score via an image
    quality regressor. A similar strategy is applied inÂ [[99](#bib.bib99), [100](#bib.bib100)].
    By contrast,Â [[2](#bib.bib2)] extracted the features from CP images and their
    corresponding eye movement (EM) and head movement (HM) hotspot maps and provided
    a good projection-based potential, that is extracting the features from the multiple
    projection formats and fusing the features to improve the performance on blind
    ODI-QA, as shown in Fig.Â [6](#S3.F6 "Figure 6 â€£ 3.1.6 Upright Adjustment â€£ 3.1
    Image/Video Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives")(b).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºODI-QAï¼Œæ ¹æ®å‚è€ƒå›¾åƒçš„å¯ç”¨æ€§ï¼Œå®ƒå¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºä¸¤ç±»ï¼šå®Œæ•´å‚è€ƒï¼ˆFRï¼‰ODI-QAå’Œæ— å‚è€ƒï¼ˆNRï¼‰ODI-QAã€‚åœ¨ç°æœ‰çš„FR ODI-QAæ–¹æ³•ä¸­ï¼Œä¸€äº›å·¥ä½œä¸“æ³¨äºå°†ä¼ ç»Ÿçš„FRå›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚**PSNR**å’Œ**SSIM**ï¼Œæ‰©å±•åˆ°å…¨å‘é¢†åŸŸï¼Œä¾‹å¦‚[[95](#bib.bib95)]ã€[[96](#bib.bib96)]ã€‚è¿™äº›å·¥ä½œå°†ODIåŠå…¶æŠ•å½±è¡¨ç¤ºçš„ç‰¹æ®Šå‡ ä½•ç»“æ„å¼•å…¥ä¼ ç»Ÿçš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ä¸­ï¼Œä»¥æ›´å‡†ç¡®åœ°æµ‹é‡å®¢è§‚è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›åŸºäºæ·±åº¦å­¦ä¹ çš„FR
    ODI-QAæ–¹æ³•ã€‚å¦‚å›¾[6](#S3.F6 "Figure 6 â€£ 3.1.6 Upright Adjustment â€£ 3.1 Image/Video Manipulation
    â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A
    Survey and New Perspectives")(a)æ‰€ç¤ºï¼Œ**Lim**ç­‰äºº[[93](#bib.bib93)ã€[97](#bib.bib97)]æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹æŠ—å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªè´¨é‡è¯„åˆ†é¢„æµ‹å™¨å’Œä¸€ä¸ªäººç±»æ„ŸçŸ¥å¼•å¯¼å™¨ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ç¬¦åˆäººç±»æ„ŸçŸ¥çš„å›¾åƒè´¨é‡ã€‚NR
    ODI-QAï¼Œä¹Ÿç§°ä¸ºç›²ODI-QAï¼Œé¢„æµ‹ODIè´¨é‡è€Œæ— éœ€æ˜‚è´µçš„å‚è€ƒODIã€‚è€ƒè™‘åˆ°ERPæ ¼å¼ä¸­çš„å¤šè§†ç‚¹å›¾åƒï¼Œ**Xu**ç­‰äºº[[98](#bib.bib98)]åº”ç”¨äº†ä¸€ç§æ–°é¢–çš„è§†ç‚¹å¯¼å‘GCNæ¥å¤„ç†ERPå›¾åƒä¸­çš„æ— å¤±çœŸè§†ç‚¹ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾èšåˆï¼Œé€šè¿‡å›¾åƒè´¨é‡å›å½’å™¨æ¥ä¼°è®¡è´¨é‡è¯„åˆ†ã€‚ç±»ä¼¼çš„ç­–ç•¥åº”ç”¨äº[[99](#bib.bib99)ã€[100](#bib.bib100)]ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[[2](#bib.bib2)]ä»CPå›¾åƒåŠå…¶å¯¹åº”çš„çœ¼åŠ¨ï¼ˆEMï¼‰å’Œå¤´åŠ¨ï¼ˆHMï¼‰çƒ­ç‚¹å›¾ä¸­æå–ç‰¹å¾ï¼Œå¹¶æä¾›äº†è‰¯å¥½çš„åŸºäºæŠ•å½±çš„æ½œåŠ›ï¼Œå³ä»å¤šä¸ªæŠ•å½±æ ¼å¼ä¸­æå–ç‰¹å¾å¹¶èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥æé«˜ç›²ODI-QAçš„æ€§èƒ½ï¼Œå¦‚å›¾[6](#S3.F6
    "Figure 6 â€£ 3.1.6 Upright Adjustment â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b)æ‰€ç¤ºã€‚'
- en: 'For the ODV-QA, Li et al.Â [[94](#bib.bib94)] proposed a representative viewport-based
    CNN approach, including a viewport proposal network and a viewport quality network,
    as shown in Fig.Â [6](#S3.F6 "Figure 6 â€£ 3.1.6 Upright Adjustment â€£ 3.1 Image/Video
    Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). The viewport proposal network generates
    several potential viewports and their error maps, and viewport quality network
    rates the V-QA score for each proposed viewport. The final V-QA score is calculated
    by the weighted average of all viewport V-QA scores.Â [[101](#bib.bib101)] is another
    representative that considers the temporal changes of spatial distortions in ODVs
    and fuses a set of spatio-temporal objective quality metrics from multiple viewports
    to learn a subjective quality score. Similarly, Gao et al.Â [[102](#bib.bib102)]
    modeled the spatial-temporal distortions of ODVs and proposed a novel FR objective
    metric by integrating three existing ODI-QA objective metrics.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºODV-QAï¼ŒLiç­‰äºº[[94](#bib.bib94)]æå‡ºäº†ä¸€ç§åŸºäºè§†å£çš„ä»£è¡¨æ€§CNNæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªè§†å£ææ¡ˆç½‘ç»œå’Œä¸€ä¸ªè§†å£è´¨é‡ç½‘ç»œï¼Œå¦‚å›¾[6](#S3.F6
    "Figure 6 â€£ 3.1.6 Upright Adjustment â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)æ‰€ç¤ºã€‚è§†å£ææ¡ˆç½‘ç»œç”Ÿæˆè‹¥å¹²æ½œåœ¨è§†å£åŠå…¶è¯¯å·®å›¾ï¼Œè§†å£è´¨é‡ç½‘ç»œå¯¹æ¯ä¸ªæè®®çš„è§†å£è¿›è¡ŒV-QAè¯„åˆ†ã€‚æœ€ç»ˆçš„V-QAåˆ†æ•°é€šè¿‡æ‰€æœ‰è§†å£V-QAåˆ†æ•°çš„åŠ æƒå¹³å‡æ¥è®¡ç®—ã€‚[[101](#bib.bib101)]æ˜¯å¦ä¸€ç§ä»£è¡¨æ€§æ–¹æ³•ï¼Œå®ƒè€ƒè™‘äº†ODVç©ºé—´ç•¸å˜çš„æ—¶é—´å˜åŒ–ï¼Œå¹¶èåˆäº†æ¥è‡ªå¤šä¸ªè§†å£çš„æ—¶ç©ºç›®æ ‡è´¨é‡åº¦é‡ï¼Œä»¥å­¦ä¹ ä¸»è§‚è´¨é‡è¯„åˆ†ã€‚ç±»ä¼¼åœ°ï¼ŒGaoç­‰äºº[[102](#bib.bib102)]å¯¹ODVçš„æ—¶ç©ºç•¸å˜è¿›è¡Œäº†å»ºæ¨¡ï¼Œå¹¶é€šè¿‡æ•´åˆä¸‰ç§ç°æœ‰çš„ODI-QAç›®æ ‡åº¦é‡ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„FRç›®æ ‡åº¦é‡ã€‚'
- en: '![Refer to caption](img/187e8cfc2ef2a246f9e8007fa511a80e.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/187e8cfc2ef2a246f9e8007fa511a80e.png)'
- en: 'Figure 7: An illustration of Sph IoUÂ [[103](#bib.bib103)] and FoV-IoUÂ [[104](#bib.bib104)].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šSph IoU[[103](#bib.bib103)]å’ŒFoV-IoU[[104](#bib.bib104)]çš„ç¤ºæ„å›¾ã€‚
- en: 3.2 Scene Understanding
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 åœºæ™¯ç†è§£
- en: 3.2.1 Object Detection
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 ç‰©ä½“æ£€æµ‹
- en: 'Compared with the perspective images, DL-based object detection on ODIs remains
    two main difficulties: (i) traditional convolutional kernels are weak to process
    the irregular planar grid structures in the ODI projections; (ii) the criterias
    adopted in conventional 2D object detection do not fit well to the spherical images.
    To address the first difficulty, distortion-aware structures are proposed, e.g.,
    multi-scale feature pyramid network inÂ [[105](#bib.bib105)], multi-kernel layers
    inÂ [[106](#bib.bib106)]. However, the detection flows of these two methods are
    similar to the methods for 2D domain, which take the whole ERP image as input
    and predict the regions of interest (ROIs) to obtain the final bounding boxes.
    Considering the wide FoV of ERP, Yang et al.Â [[107](#bib.bib107)] proposed a representative
    framework, which can leverage the conventional 2D images to train a panoramic
    detector. The detecting progress consists of three sub-steps: stereo-projection,
    YOLO detectors, and bounding box post processing. Especially, they generated four
    stereographic projections with a $180^{\circ}\times 180^{\circ}$ FoV from an ERP
    and the four result maps predicted by the YOLO detectors. Finally, the sub-window
    detected bounding boxes are re-projected to the ERP and re-aligned into the final
    distortion-less bounding boxes.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é€è§†å›¾åƒç›¸æ¯”ï¼ŒåŸºäºDLçš„ODIç‰©ä½“æ£€æµ‹ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦å›°éš¾ï¼šï¼ˆiï¼‰ä¼ ç»Ÿå·ç§¯æ ¸åœ¨å¤„ç†ODIæŠ•å½±ä¸­çš„ä¸è§„åˆ™å¹³é¢ç½‘æ ¼ç»“æ„æ—¶æ•ˆæœè¾ƒå·®ï¼›ï¼ˆiiï¼‰ä¼ ç»Ÿ2Dç‰©ä½“æ£€æµ‹é‡‡ç”¨çš„æ ‡å‡†ä¸é€‚ç”¨äºçƒé¢å›¾åƒã€‚ä¸ºè§£å†³ç¬¬ä¸€ä¸ªå›°éš¾ï¼Œæå‡ºäº†ç•¸å˜æ„ŸçŸ¥ç»“æ„ï¼Œä¾‹å¦‚[[105](#bib.bib105)]ä¸­çš„å¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œ[[106](#bib.bib106)]ä¸­çš„å¤šæ ¸å±‚ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•çš„æ£€æµ‹æµç¨‹ç±»ä¼¼äº2Dé¢†åŸŸçš„æ–¹æ³•ï¼Œå³å°†æ•´ä¸ªERPå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å…´è¶£åŒºåŸŸï¼ˆROIsï¼‰ä»¥è·å¾—æœ€ç»ˆçš„è¾¹ç•Œæ¡†ã€‚è€ƒè™‘åˆ°ERPçš„å®½å¹¿è§†åœºï¼ŒYangç­‰äºº[[107](#bib.bib107)]æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§æ¡†æ¶ï¼Œå¯ä»¥åˆ©ç”¨ä¼ ç»Ÿçš„2Då›¾åƒæ¥è®­ç»ƒå…¨æ™¯æ£€æµ‹å™¨ã€‚æ£€æµ‹è¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªå­æ­¥éª¤ï¼šç«‹ä½“æŠ•å½±ã€YOLOæ£€æµ‹å™¨å’Œè¾¹ç•Œæ¡†åå¤„ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»–ä»¬ä»ä¸€ä¸ªERPç”Ÿæˆäº†å››ä¸ª$180^{\circ}\times
    180^{\circ}$è§†åœºçš„ç«‹ä½“æŠ•å½±ï¼Œå¹¶ç”±YOLOæ£€æµ‹å™¨é¢„æµ‹äº†å››ä¸ªç»“æœå›¾ã€‚æœ€åï¼Œå­çª—å£æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†è¢«é‡æ–°æŠ•å½±åˆ°ERPä¸Šï¼Œå¹¶é‡æ–°å¯¹é½æˆæœ€ç»ˆçš„æ— ç•¸å˜è¾¹ç•Œæ¡†ã€‚
- en: 'To tackle the second difficulty, a novel kind of spherical bounding boxe (SphBB)
    and spherical Intersection over Union (SphIoU) for ODI object detection are introduced
    inÂ [[103](#bib.bib103)], as shown in the first row of Fig.Â [7](#S3.F7 "Figure
    7 â€£ 3.1.7 Visual Quality Assessment â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives").
    SphBB is represented by the coordinates $\theta$, $\phi$ of the object centers
    and the unbiased FoVs $\alpha$, $\beta$ of the objective occupation. SphIoU is
    similar to planar IoU and calculated by the IoU between two SphBBs. Concretely,
    FoVBBs are moved to the equator that is undistorted. Similarly, Cao et al.Â [[104](#bib.bib104)]
    proposed a novel IoU calculation method without any extra movement, called FoV-IoU.
    As shown in the second row of Fig.Â [7](#S3.F7 "Figure 7 â€£ 3.1.7 Visual Quality
    Assessment â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives"), FoV-IoU
    better approximates the exact computation of IoU between two FoV-BBs compared
    with the SphIoU.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†åº”å¯¹ç¬¬äºŒä¸ªéš¾ç‚¹ï¼Œä»‹ç»äº†ä¸€ç§æ–°çš„çƒå½¢è¾¹ç•Œæ¡†ï¼ˆSphBBï¼‰å’Œçƒå½¢äº¤å¹¶æ¯”ï¼ˆSphIoUï¼‰ç”¨äºå…¨æ™¯å›¾åƒç‰©ä½“æ£€æµ‹ï¼Œå¦‚å›¾[7](#S3.F7 "Figure
    7 â€£ 3.1.7 Visual Quality Assessment â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")çš„ç¬¬ä¸€è¡Œæ‰€ç¤ºã€‚SphBBé€šè¿‡ç‰©ä½“ä¸­å¿ƒçš„åæ ‡
    $\theta$ å’Œ $\phi$ ä»¥åŠç›®æ ‡å æ®çš„æ— åè§†åœº $\alpha$ å’Œ $\beta$ æ¥è¡¨ç¤ºã€‚SphIoU ç±»ä¼¼äºå¹³é¢ IoUï¼Œé€šè¿‡ä¸¤ä¸ª SphBB
    ä¹‹é—´çš„ IoU è®¡ç®—å¾—å‡ºã€‚å…·ä½“è€Œè¨€ï¼ŒFoVBBs è¢«ç§»åŠ¨åˆ°æ— æ‰­æ›²çš„èµ¤é“ä¸Šã€‚ç±»ä¼¼åœ°ï¼ŒCao ç­‰äºº [[104](#bib.bib104)] æå‡ºäº†ä¸€ä¸ªä¸éœ€è¦é¢å¤–ç§»åŠ¨çš„æ–°
    IoU è®¡ç®—æ–¹æ³•ï¼Œç§°ä¸º FoV-IoUã€‚å¦‚å›¾[7](#S3.F7 "Figure 7 â€£ 3.1.7 Visual Quality Assessment â€£
    3.1 Image/Video Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives") çš„ç¬¬äºŒè¡Œæ‰€ç¤ºï¼ŒFoV-IoU æ¯” SphIoU
    æ›´å¥½åœ°è¿‘ä¼¼äº†ä¸¤ä¸ª FoV-BB ä¹‹é—´ IoU çš„å‡†ç¡®è®¡ç®—ã€‚'
- en: '![Refer to caption](img/9d98f8f1c2e42e6d31dbecb808c44a4b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/9d98f8f1c2e42e6d31dbecb808c44a4b.png)'
- en: 'Figure 8: Representative methods for unsupervised ODI semantic segmentation.
    (a) Unsupervised learning with deformable CNNs [[108](#bib.bib108)]. (b) Unsupervised
    learning with domain adaptationÂ [[109](#bib.bib109)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šæ— ç›‘ç£å…¨æ™¯å›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä»£è¡¨æ€§æ–¹æ³•ã€‚ (a) ä½¿ç”¨å˜å½¢å·ç§¯ç¥ç»ç½‘ç»œçš„æ— ç›‘ç£å­¦ä¹  [[108](#bib.bib108)]ã€‚ (b) ä½¿ç”¨é¢†åŸŸé€‚åº”çš„æ— ç›‘ç£å­¦ä¹ 
    [[109](#bib.bib109)]ã€‚
- en: 3.2.2 Semantic Segmentation
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 è¯­ä¹‰åˆ†å‰²
- en: 'TABLE III: Semantic segmentation by some representative methods. â€œSâ€: supervised,
    â€Uâ€: Unsupervised, â€œDâ€: domain adaptation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ III: ä¸€äº›ä»£è¡¨æ€§æ–¹æ³•çš„è¯­ä¹‰åˆ†å‰²ã€‚â€œSâ€ï¼šç›‘ç£ï¼Œâ€œUâ€ï¼šæ— ç›‘ç£ï¼Œâ€œDâ€ï¼šé¢†åŸŸé€‚åº”ã€‚'
- en: '| Method | Publication | Input | Dataset | Deformable | Supervision | Highlight
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å‘è¡¨æœŸåˆŠ | è¾“å…¥ | æ•°æ®é›† | å˜å½¢ | ç›‘ç£ | äº®ç‚¹ |'
- en: '| TatenoÂ [[108](#bib.bib108)] | ECCVâ€™2018 | ERP | Stanford2d3d | $\checkmark$
    | U | Distortion-aware convolution |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Tateno [[108](#bib.bib108)] | ECCVâ€™2018 | ERP | Stanford2d3d | $\checkmark$
    | U | æ‰­æ›²æ„ŸçŸ¥å·ç§¯ |'
- en: '| ZhangÂ [[45](#bib.bib45)] | ICCVâ€™2019 | Tangent | Stanford2D3D/Omni-SYNTHIA
    | $\checkmark$ | S | Orientation-aware convolutions |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[45](#bib.bib45)] | ICCVâ€™2019 | åˆ‡çº¿ | Stanford2D3D/Omni-SYNTHIA | $\checkmark$
    | S | é¢å‘æ–¹å‘çš„å·ç§¯ |'
- en: '| LeeÂ [[27](#bib.bib27)] | CVPRâ€™2019 | Tangent | SYNTHIA/Stanford2D3D | $\checkmark$
    | S | Icosahedral geodesic polyhedron |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Lee [[27](#bib.bib27)] | CVPRâ€™2019 | åˆ‡çº¿ | SYNTHIA/Stanford2D3D | $\checkmark$
    | S | äºŒåé¢ä½“æµ‹åœ°å¤šé¢ä½“ |'
- en: '| ViuÂ [[110](#bib.bib110)] | ICRAâ€™2020 | ERP | SUN360 | $\checkmark$ | S |
    Equirectangular convolutions |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Viu [[110](#bib.bib110)] | ICRAâ€™2020 | ERP | SUN360 | $\checkmark$ | S |
    ç­‰è·åœ†æŸ±å·ç§¯ |'
- en: '| YangÂ [[111](#bib.bib111)] | CVPRâ€™2021 | ERP | PASS/WildPASS | âœ— | U | Concurrent
    attention networks |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Yang [[111](#bib.bib111)] | CVPRâ€™2021 | ERP | PASS/WildPASS | âœ— | U | å¹¶å‘æ³¨æ„åŠ›ç½‘ç»œ
    |'
- en: '| ZhangÂ [[112](#bib.bib112)] | CVPRâ€™2022 | ERP | Stanford2D3D/DensePASS | $\checkmark$
    | U | Deformable MLP |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[112](#bib.bib112)] | CVPRâ€™2022 | ERP | Stanford2D3D/DensePASS | $\checkmark$
    | U | å˜å½¢å¤šå±‚æ„ŸçŸ¥æœº |'
- en: '| ZhangÂ [[113](#bib.bib113)] | T-ITSâ€™2022 | ERP | DensePASS/VISTAS | âœ— | D
    | Uncertainty-aware adaptation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[113](#bib.bib113)] | T-ITSâ€™2022 | ERP | DensePASS/VISTAS | âœ— | D
    | ä¸ç¡®å®šæ€§æ„ŸçŸ¥é€‚åº” |'
- en: DL-based omnidirectional semantic segmentation has been widely studied because
    ODI can encompass exhaustive information about the surrounding space. There are
    many practically remaining challenges, e.g., distortions in the planar projections,
    object deformations, computed complexity, and scarce labeled data. We next introduce
    some representative methods for ODI semantic segmentation via supervised learning
    and unsupervised learning.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦å­¦ä¹ çš„å…¨æ™¯è¯­ä¹‰åˆ†å‰²å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œå› ä¸ºå…¨æ™¯å›¾åƒå¯ä»¥åŒ…å«å…³äºå‘¨å›´ç©ºé—´çš„å…¨é¢ä¿¡æ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»ç„¶å­˜åœ¨è®¸å¤šå®é™…æŒ‘æˆ˜ï¼Œä¾‹å¦‚å¹³é¢æŠ•å½±çš„æ‰­æ›²ã€ç‰©ä½“å˜å½¢ã€è®¡ç®—å¤æ‚åº¦å’Œç¨€å°‘çš„æ ‡æ³¨æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›é€šè¿‡ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ è¿›è¡Œå…¨æ™¯å›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä»£è¡¨æ€§æ–¹æ³•ã€‚
- en: Due to the lack of real-world datasets, Deng et al.Â [[114](#bib.bib114)] firstly
    generated ODIs from an existing dataset of urban traffic scenes and then designed
    an approach, called zoom augmentation, to transform the conventional images into
    fisheye images. Meanwhile, they proposed a CNN-based framework with a special
    pooling module to integrate the local and global context information and handle
    complex scenes in the ODIs. Considering that CNNs have inherently limited ability
    to handle the distortions in ODIs, Deng et al.Â [[115](#bib.bib115)] proposed a
    method, called Restricted Deformable Convolution, to model geometric transformations
    and learn a convolutional filter size from the input feature map. Zoom augmentation
    was also applied toÂ [[115](#bib.bib115)] for enriching the train data. As the
    first framework to conduct semantic segmentation on the real-world outdoor ODIs,
    SemanticSOÂ [[116](#bib.bib116)] builds a distortion-aware CNN model using the
    equirectangular convolutionsÂ [[117](#bib.bib117)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç¼ºä¹ç°å®ä¸–ç•Œçš„æ•°æ®é›†ï¼ŒDengç­‰äººÂ [[114](#bib.bib114)] é¦–å…ˆä»ç°æœ‰çš„åŸå¸‚äº¤é€šåœºæ™¯æ•°æ®é›†ä¸­ç”Ÿæˆäº†ODIï¼Œç„¶åè®¾è®¡äº†ä¸€ç§ç§°ä¸ºç¼©æ”¾å¢å¼ºçš„æ–¹æ³•ï¼Œå°†ä¼ ç»Ÿå›¾åƒè½¬æ¢ä¸ºé±¼çœ¼å›¾åƒã€‚åŒæ—¶ï¼Œä»–ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºCNNçš„æ¡†æ¶ï¼Œå…·æœ‰ç‰¹æ®Šçš„æ± åŒ–æ¨¡å—ï¼Œç”¨äºé›†æˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶å¤„ç†ODIä¸­çš„å¤æ‚åœºæ™¯ã€‚è€ƒè™‘åˆ°CNNåœ¨å¤„ç†ODIä¸­çš„ç•¸å˜æ–¹é¢å›ºæœ‰çš„å±€é™æ€§ï¼ŒDengç­‰äººÂ [[115](#bib.bib115)]
    æå‡ºäº†ä¸€ç§ç§°ä¸ºå—é™å¯å˜å½¢å·ç§¯çš„æ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡å‡ ä½•å˜æ¢ï¼Œå¹¶ä»è¾“å…¥ç‰¹å¾å›¾ä¸­å­¦ä¹ å·ç§¯æ»¤æ³¢å™¨çš„å¤§å°ã€‚ç¼©æ”¾å¢å¼ºä¹Ÿè¢«åº”ç”¨äºÂ [[115](#bib.bib115)] ä»¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªåœ¨çœŸå®ä¸–ç•Œæˆ·å¤–ODIä¸Šè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æ¡†æ¶ï¼ŒSemanticSOÂ [[116](#bib.bib116)]
    æ„å»ºäº†ä¸€ä¸ªåˆ©ç”¨ç­‰è·å·ç§¯Â [[117](#bib.bib117)] çš„ç•¸å˜æ„ŸçŸ¥CNNæ¨¡å‹ã€‚
- en: Due to the time-consuming and expensive cost of ground truth annotations for
    ODIs, endeavours have been made to synthesize ODI datasets from the conventional
    images and utilize knowledge transfer to adopt models directly trained with the
    perspective images. PASSÂ [[118](#bib.bib118)] is the first work to bypass fully
    dense panoramic annotations and aggregate features represented by conventional
    perspective images to fulfill the pixel-wise segmentation in panoramic imagery.
    Based on the PASS, DS-PASSÂ [[119](#bib.bib119)] further re-uses the knowledge
    learned from perspective images and adapts the model learned from the 2D domain
    to panoramic domain. Meanwhile, in DS-PASS, the sensitivity to spatial details
    is enhanced by implementing attention-based lateral connections to perform segmentation
    accurately. To reduce the domain gap between the ODI and perspective image, Yang
    et al.Â [[111](#bib.bib111)] proposed a representative cross-domain transfer framework
    that designs an efficient concurrent attention network to capture the long-range
    dependencies in ODI imagery and integrates the unlabeled ODIs and labeled perspective
    images into training. A similar strategy was applied inÂ [[120](#bib.bib120)],Â [[121](#bib.bib121)]
    andÂ [[109](#bib.bib109)]. Particularly, inÂ [[109](#bib.bib109)], a shared attention
    module is used to extract features from the 2D domain and panoramic domain, and
    two domain adaption modules are used to â€teachâ€ the panoramic branch by the perspective
    branch. For unsupervised semantic segmentation, there also exist some works considering
    the geometric structure of ODIÂ [[108](#bib.bib108)]. For instance, Zhang et al.Â [[45](#bib.bib45)]
    proposed an orientation-aware CNN framework based on the icosahedron mesh representation
    of ODI and introduced an efficient interpolation approach of the north-aligned
    kernel convolutions for features on the sphere.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ°é¢å®å†µæ ‡æ³¨å¯¹ODIçš„æ—¶é—´å’Œæˆæœ¬æ¶ˆè€—å·¨å¤§ï¼Œäººä»¬å·²å°è¯•ä»ä¼ ç»Ÿå›¾åƒä¸­åˆæˆODIæ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†è¿ç§»ç›´æ¥é‡‡ç”¨ä½¿ç”¨é€è§†å›¾åƒè®­ç»ƒçš„æ¨¡å‹ã€‚PASSÂ [[118](#bib.bib118)]
    æ˜¯é¦–ä¸ªç»•è¿‡å®Œå…¨å¯†é›†å…¨æ™¯æ ‡æ³¨ï¼Œèšåˆç”±ä¼ ç»Ÿé€è§†å›¾åƒè¡¨ç¤ºçš„ç‰¹å¾ï¼Œä»¥å®ç°å…¨æ™¯å›¾åƒä¸­çš„é€åƒç´ åˆ†å‰²çš„å·¥ä½œã€‚åŸºäºPASSï¼ŒDS-PASSÂ [[119](#bib.bib119)]
    è¿›ä¸€æ­¥é‡ç”¨ä»é€è§†å›¾åƒä¸­å­¦åˆ°çš„çŸ¥è¯†ï¼Œå¹¶å°†ä»2Dé¢†åŸŸå­¦ä¹ åˆ°çš„æ¨¡å‹é€‚é…åˆ°å…¨æ™¯é¢†åŸŸã€‚åŒæ—¶ï¼Œåœ¨DS-PASSä¸­ï¼Œé€šè¿‡å®æ–½åŸºäºæ³¨æ„åŠ›çš„ä¾§å‘è¿æ¥ï¼Œå¢å¼ºäº†å¯¹ç©ºé—´ç»†èŠ‚çš„æ•æ„Ÿæ€§ï¼Œä»¥å®ç°ç²¾ç¡®åˆ†å‰²ã€‚ä¸ºäº†å‡å°‘ODIä¸é€è§†å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼ŒYangç­‰äººÂ [[111](#bib.bib111)]
    æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§çš„è·¨é¢†åŸŸè¿ç§»æ¡†æ¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆçš„å¹¶è¡Œæ³¨æ„åŠ›ç½‘ç»œï¼Œä»¥æ•æ‰ODIå›¾åƒä¸­çš„é•¿ç¨‹ä¾èµ–ï¼Œå¹¶å°†æœªæ ‡è®°çš„ODIå’Œæ ‡è®°çš„é€è§†å›¾åƒé›†æˆåˆ°è®­ç»ƒä¸­ã€‚ç±»ä¼¼çš„ç­–ç•¥ä¹Ÿåº”ç”¨äºÂ [[120](#bib.bib120)]ã€[[121](#bib.bib121)]
    å’ŒÂ [[109](#bib.bib109)]ã€‚ç‰¹åˆ«æ˜¯åœ¨Â [[109](#bib.bib109)] ä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå…±äº«æ³¨æ„åŠ›æ¨¡å—ä»2Dé¢†åŸŸå’Œå…¨æ™¯é¢†åŸŸæå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ªé¢†åŸŸé€‚åº”æ¨¡å—é€šè¿‡é€è§†åˆ†æ”¯â€œæ•™â€å…¨æ™¯åˆ†æ”¯ã€‚å¯¹äºæ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼Œä¹Ÿå­˜åœ¨ä¸€äº›è€ƒè™‘ODIå‡ ä½•ç»“æ„çš„å·¥ä½œÂ [[108](#bib.bib108)]ã€‚ä¾‹å¦‚ï¼ŒZhangç­‰äººÂ [[45](#bib.bib45)]
    æå‡ºäº†åŸºäºODIäºŒåé¢ä½“ç½‘æ ¼è¡¨ç¤ºçš„æ–¹å‘æ„ŸçŸ¥CNNæ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„åŒ—å¯¹é½æ ¸å·ç§¯æ’å€¼æ–¹æ³•ï¼Œç”¨äºçƒé¢ä¸Šçš„ç‰¹å¾ã€‚
- en: 3.2.3 Monocular Depth Estimation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡
- en: 'TABLE IV: Monocular depth estimation by some representative methods. â€œSâ€: supervised,
    â€œDâ€: domain adaptation.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ IV: ä¸€äº›ä»£è¡¨æ€§æ–¹æ³•çš„å•ç›®æ·±åº¦ä¼°è®¡ã€‚â€œSâ€ï¼šç›‘ç£ï¼Œâ€œDâ€ï¼šé¢†åŸŸé€‚åº”ã€‚'
- en: '| Method | Publication | Supervision | Input types | Architecture | Loss functions
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å‘è¡¨ | ç›‘ç£ç±»å‹ | è¾“å…¥ç±»å‹ | æ¶æ„ | æŸå¤±å‡½æ•° |'
- en: '| ZioulisÂ [[122](#bib.bib122)] | ECCVâ€™18 | S | ERP | Rectangular filters |
    l2 loss+smooth loss |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ZioulisÂ [[122](#bib.bib122)] | ECCVâ€™18 | S | ERP | çŸ©å½¢æ»¤æ³¢å™¨ | l2 æŸå¤±+å¹³æ»‘æŸå¤± |'
- en: '| PintoreÂ [[123](#bib.bib123)] | CVPRâ€™21 | S | ERP | Slice-based representation
    and LSTM | BerHu lossÂ [[124](#bib.bib124)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| PintoreÂ [[123](#bib.bib123)] | CVPRâ€™21 | S | ERP | åŸºäºåˆ‡ç‰‡çš„è¡¨ç¤ºå’Œ LSTM | BerHu
    æŸå¤±Â [[124](#bib.bib124)] |'
- en: '| ZhuangÂ [[125](#bib.bib125)] | AAAIâ€™22 | S | ERP | Dilated filters | BerHu
    loss |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ZhuangÂ [[125](#bib.bib125)] | AAAIâ€™22 | S | ERP | æ‰©å¼ æ»¤æ³¢å™¨ | BerHu æŸå¤± |'
- en: '| WangÂ [[19](#bib.bib19)] | CVPRâ€™20 | S | ERP+CP | Two-branch network and bi-projection
    fusion | BerHu loss |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| WangÂ [[19](#bib.bib19)] | CVPRâ€™20 | S | ERP+CP | åŒåˆ†æ”¯ç½‘ç»œå’ŒåŒé‡æŠ•å½±èåˆ | BerHu æŸå¤±
    |'
- en: '| Rey-AreaÂ [[34](#bib.bib34)] | CVPRâ€™22 | S | Tangent | Perspective network+Alignment+Blending
    | Energy function |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Rey-AreaÂ [[34](#bib.bib34)] | CVPRâ€™22 | S | åˆ‡çº¿ | é€è§†ç½‘ç»œ+å¯¹é½+æ··åˆ | èƒ½é‡å‡½æ•° |'
- en: '| LiÂ [[25](#bib.bib25)] | CVPRâ€™22 | S | Tangent | Geometric embedding+Transformer
    | BerHu loss |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| LiÂ [[25](#bib.bib25)] | CVPRâ€™22 | S | åˆ‡çº¿ | å‡ ä½•åµŒå…¥+Transformer | BerHu æŸå¤± |'
- en: '| JinÂ [[126](#bib.bib126)] | CVPRâ€™20 | S | ERP | Structure information as prior
    and regularizer | l1 loss+cross entropy loss |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| JinÂ [[126](#bib.bib126)] | CVPRâ€™20 | S | ERP | ç»“æ„ä¿¡æ¯ä½œä¸ºå…ˆéªŒå’Œæ­£åˆ™åŒ–å™¨ | l1 æŸå¤±+äº¤å‰ç†µæŸå¤±
    |'
- en: '| WangÂ [[127](#bib.bib127)] | ACCVâ€™18 | Self-S | CP | Depth estimation+camera
    motion estimation | photometric + pose loss |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| WangÂ [[127](#bib.bib127)] | ACCVâ€™18 | Self-S | CP | æ·±åº¦ä¼°è®¡+ç›¸æœºè¿åŠ¨ä¼°è®¡ | å…‰åº¦ + å§¿æ€æŸå¤±
    |'
- en: '| ZioulisÂ [[18](#bib.bib18)] | 3DVâ€™19 | Self-S | ERP | View synthesis in horizontal,
    vertical and trinocular ones | photometric +smooth loss |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ZioulisÂ [[18](#bib.bib18)] | 3DVâ€™19 | Self-S | ERP | æ°´å¹³ã€å‚ç›´å’Œä¸‰ç›®è§†å›¾åˆæˆ | å…‰åº¦ +
    å¹³æ»‘æŸå¤± |'
- en: '| YunÂ [[128](#bib.bib128)] | AAAIâ€™22 | S+Self-S | ERP | ViT+pose estimation
    | SSIMÂ [[129](#bib.bib129)]+gradient +L1+photometric loss |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| YunÂ [[128](#bib.bib128)] | AAAIâ€™22 | S+Self-S | ERP | ViT+å§¿æ€ä¼°è®¡ | SSIMÂ [[129](#bib.bib129)]+æ¢¯åº¦
    +L1+å…‰åº¦æŸå¤± |'
- en: '| TatenoÂ [[108](#bib.bib108)] | ECCVâ€™18 | D | ERP | Distortion-aware filters
    | BerHu loss |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| TatenoÂ [[108](#bib.bib108)] | ECCVâ€™18 | D | ERP | å¤±çœŸæ„ŸçŸ¥æ»¤æ³¢å™¨ | BerHu æŸå¤± |'
- en: '![Refer to caption](img/54d598fb9daa4a2b97f4c78375c49c8f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/54d598fb9daa4a2b97f4c78375c49c8f.png)'
- en: 'Figure 9: Representative monocular depth estimation methods. (a) Dual-branch
    methods with ERP and CP as inputs. (b) Methods taking tangent images as input
    and re-project them into ERP images. (c) Methods utilizing extra geometric information
    as the prior and regularizer. (d) View synthesis methods with certain baselines.
    (e) Self-supervised multi-frame methods with the pose estimation.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šä»£è¡¨æ€§å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚(a) å…·æœ‰ ERP å’Œ CP ä½œä¸ºè¾“å…¥çš„åŒåˆ†æ”¯æ–¹æ³•ã€‚(b) ä»¥åˆ‡çº¿å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶é‡æ–°æŠ•å½±åˆ° ERP å›¾åƒä¸­çš„æ–¹æ³•ã€‚(c)
    åˆ©ç”¨é¢å¤–å‡ ä½•ä¿¡æ¯ä½œä¸ºå…ˆéªŒå’Œæ­£åˆ™åŒ–å™¨çš„æ–¹æ³•ã€‚(d) å…·æœ‰ç‰¹å®šåŸºçº¿çš„è§†å›¾åˆæˆæ–¹æ³•ã€‚(e) å¸¦æœ‰å§¿æ€ä¼°è®¡çš„è‡ªç›‘ç£å¤šå¸§æ–¹æ³•ã€‚
- en: 'Thanks to the emergence of large-scale panoramic depth datasets, monocular
    depth estimation has evolved rapidly. As shown in Fig.Â [9](#S3.F9 "Figure 9 â€£
    3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    there are several trends: (i) Tailored networks, e.g., distortion-aware convolution
    filtersÂ [[108](#bib.bib108)] and robust representationsÂ [[123](#bib.bib123)];
    (ii) Different projection types of ODIsÂ [[19](#bib.bib19)],Â [[130](#bib.bib130)],Â [[25](#bib.bib25)],
    as depicted in Fig.Â [9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£
    3.2 Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(a), (b); (iii) Inherent geometric priorsÂ [[131](#bib.bib131)],Â [[126](#bib.bib126)],
    as shown in Fig.Â [9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2
    Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c); (iv) Multiple viewsÂ [[18](#bib.bib18)]
    or pose estimationÂ [[128](#bib.bib128)], as shown in Fig.Â [9](#S3.F9 "Figure 9
    â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    (d), (e), respectively.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¤šäºäº†å¤§è§„æ¨¡å…¨æ™¯æ·±åº¦æ•°æ®é›†çš„å‡ºç°ï¼Œå•ç›®æ·±åº¦ä¼°è®¡å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚å¦‚å›¾[9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular Depth
    Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")æ‰€ç¤ºï¼Œå‡ºç°äº†å‡ ä¸ªè¶‹åŠ¿ï¼šï¼ˆiï¼‰å®šåˆ¶åŒ–ç½‘ç»œï¼Œä¾‹å¦‚ï¼Œç•¸å˜æ„ŸçŸ¥å·ç§¯æ»¤æ³¢å™¨[[108](#bib.bib108)]å’Œé²æ£’è¡¨ç¤º[[123](#bib.bib123)];ï¼ˆiiï¼‰ä¸åŒçš„ODIæŠ•å½±ç±»å‹[[19](#bib.bib19)]ï¼Œ[[130](#bib.bib130)]ï¼Œ[[25](#bib.bib25)]ï¼Œå¦‚å›¾[9](#S3.F9
    "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(a)ï¼Œ(b)æ‰€ç¤ºï¼›ï¼ˆiiiï¼‰å›ºæœ‰å‡ ä½•å…ˆéªŒ[[131](#bib.bib131)]ï¼Œ[[126](#bib.bib126)]ï¼Œå¦‚å›¾[9](#S3.F9
    "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)æ‰€ç¤ºï¼›ï¼ˆivï¼‰å¤šè§†è§’[[18](#bib.bib18)]æˆ–å§¿æ€ä¼°è®¡[[128](#bib.bib128)]ï¼Œå¦‚å›¾[9](#S3.F9
    "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(d)ï¼Œ(e)æ‰€ç¤ºã€‚'
- en: 'Tailored networks: To reduce the influence of the stretch distortion, Zioulis
    et al.Â [[122](#bib.bib122)] proposed the first work by directly using the ODIs.
    It follows [[20](#bib.bib20)] to transfer regular square convolution filters into
    row-wise rectangles and vary filter sizes to address the distortions at the poles.
    Tateno et al.Â [[108](#bib.bib108)] proposed a deformable convolution filter that
    samples the pixel grids on the tangent planes according to unit sphere coordinates.
    Recently, Zhuang et al.Â [[125](#bib.bib125)] proposed a novel framework to combine
    different dilated convolutions and extend the receptive field in the ERP images.
    In comparison, Pintore et al.Â [[123](#bib.bib123)] proposed a framework, named
    SliceNet, with regular convolution filters to work on the ERP directly. SliceNet
    reduces the input tensor only along the vertical direction to collect a sequence
    of vertical slices and adopts an LSTMÂ [[132](#bib.bib132)] network to recover
    the long- and short-term spatial relationships among slices.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å®šåˆ¶åŒ–ç½‘ç»œï¼šä¸ºäº†å‡å°‘æ‹‰ä¼¸ç•¸å˜çš„å½±å“ï¼ŒZioulisç­‰äºº[[122](#bib.bib122)]é¦–æ¬¡æå‡ºäº†ç›´æ¥ä½¿ç”¨ODIçš„å·¥ä½œã€‚è¯¥æ–¹æ³•éµå¾ª[[20](#bib.bib20)]å°†å¸¸è§„çš„æ­£æ–¹å½¢å·ç§¯æ»¤æ³¢å™¨è½¬æ¢ä¸ºè¡Œå¼çŸ©å½¢ï¼Œå¹¶é€šè¿‡å˜åŒ–æ»¤æ³¢å™¨çš„å¤§å°æ¥è§£å†³æç‚¹çš„ç•¸å˜ã€‚Tatenoç­‰äºº[[108](#bib.bib108)]æå‡ºäº†ä¸€ç§å¯å˜å½¢å·ç§¯æ»¤æ³¢å™¨ï¼Œæ ¹æ®å•ä½çƒåæ ‡å¯¹åˆ‡å¹³é¢ä¸Šçš„åƒç´ ç½‘æ ¼è¿›è¡Œé‡‡æ ·ã€‚æœ€è¿‘ï¼ŒZhuangç­‰äºº[[125](#bib.bib125)]æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå°†ä¸åŒçš„è†¨èƒ€å·ç§¯ç»“åˆèµ·æ¥ï¼Œå¹¶åœ¨ERPå›¾åƒä¸­æ‰©å±•æ„Ÿå—é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPintoreç­‰äºº[[123](#bib.bib123)]æå‡ºäº†ä¸€ä¸ªåä¸ºSliceNetçš„æ¡†æ¶ï¼Œä½¿ç”¨å¸¸è§„å·ç§¯æ»¤æ³¢å™¨ç›´æ¥å¤„ç†ERPã€‚SliceNetä»…æ²¿å‚ç›´æ–¹å‘å‡å°‘è¾“å…¥å¼ é‡ï¼Œä»¥æ”¶é›†ä¸€ç³»åˆ—å‚ç›´åˆ‡ç‰‡ï¼Œå¹¶é‡‡ç”¨LSTM[[132](#bib.bib132)]ç½‘ç»œæ¥æ¢å¤åˆ‡ç‰‡ä¹‹é—´çš„é•¿çŸ­æœŸç©ºé—´å…³ç³»ã€‚
- en: 'Different projection formats: There are some attempts to address the distortion
    in the ERP via other distortion-less projection formats, e.g., CP, tangent projection.
    As a representative work, BiFuseÂ [[19](#bib.bib19)] introduces a two-branch pipeline,
    where one branch processes the ERP input and another branch extracts the features
    from CP, to simulate the peripheral and foveal vision of human, as shown in Fig.Â [9](#S3.F9
    "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    (a). Then, a fusion model is proposed to combine the semantic and geometric information
    of the two branches. Inspired by BiFuse, UniFuseÂ [[133](#bib.bib133)] designs
    a more effective fusion module to combine the two kinds of features and unidirectionally
    feeds the CP features to the ERP features only at the decoding stage. To better
    extract the global context information, GLPanoDepthÂ [[134](#bib.bib134)] converts
    ERP input into a set of CP images and then exploits a ViT model to learn the long-range
    dependencies. As the tangent projection produces less distortion than CP, 360MonoDepthÂ [[34](#bib.bib34)]
    trains the SoTA depth estimation models in 2D domainÂ [[135](#bib.bib135)] with
    tangent images and re-projects predicted tangent depth maps into the ERP with
    alignment and blending, as shown in Fig.Â [9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular
    Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b).
    However, directly re-projecting the tangent images back to the ERP format will
    cause overlapping and discontinuity. Therefore, OmniFusionÂ [[25](#bib.bib25)]
    (the SoTA method by far) introduces additional 3D geometric embeddings to mitigate
    the discrepancy in patch-wise features and aggregates patch-wise information with
    an attention-based transformer.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æŠ•å½±æ ¼å¼ï¼šæœ‰ä¸€äº›å°è¯•é€šè¿‡å…¶ä»–æ— å¤±çœŸæŠ•å½±æ ¼å¼æ¥è§£å†³ERPä¸­çš„å¤±çœŸé—®é¢˜ï¼Œä¾‹å¦‚CPã€åˆ‡çº¿æŠ•å½±ã€‚ä½œä¸ºä»£è¡¨æ€§å·¥ä½œï¼ŒBiFuseÂ [[19](#bib.bib19)]
    å¼•å…¥äº†ä¸€ä¸ªåŒåˆ†æ”¯ç®¡é“ï¼Œå…¶ä¸­ä¸€ä¸ªåˆ†æ”¯å¤„ç†ERPè¾“å…¥ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯ä»CPä¸­æå–ç‰¹å¾ï¼Œä»¥æ¨¡æ‹Ÿäººç±»çš„å¤–å‘¨å’Œä¸­å¤®è§†åŠ›ï¼Œå¦‚å›¾Â [9](#S3.F9 "å›¾ 9 â€£ 3.2.3
    å•ç›®æ·±åº¦ä¼°è®¡ â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨å‘è§†è§‰ä»»åŠ¡ â€£ å…¨å‘è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ï¼ˆaï¼‰æ‰€ç¤ºã€‚ç„¶åï¼Œæå‡ºäº†ä¸€ä¸ªèåˆæ¨¡å‹æ¥ç»“åˆä¸¤ä¸ªåˆ†æ”¯çš„è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ã€‚å—åˆ°BiFuseçš„å¯å‘ï¼ŒUniFuseÂ [[133](#bib.bib133)]
    è®¾è®¡äº†ä¸€ä¸ªæ›´æœ‰æ•ˆçš„èåˆæ¨¡å—ï¼Œä»¥ç»“åˆè¿™ä¸¤ç§ç‰¹å¾ï¼Œå¹¶ä¸”åœ¨è§£ç é˜¶æ®µå•å‘åœ°å°†CPç‰¹å¾é¦ˆé€åˆ°ERPç‰¹å¾ã€‚ä¸ºäº†æ›´å¥½åœ°æå–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒGLPanoDepthÂ [[134](#bib.bib134)]
    å°†ERPè¾“å…¥è½¬æ¢ä¸ºä¸€ç»„CPå›¾åƒï¼Œç„¶ååˆ©ç”¨ViTæ¨¡å‹å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚ç”±äºåˆ‡çº¿æŠ•å½±æ¯”CPäº§ç”Ÿçš„å¤±çœŸæ›´å°‘ï¼Œ360MonoDepthÂ [[34](#bib.bib34)]
    åœ¨2Dé¢†åŸŸÂ [[135](#bib.bib135)] è®­ç»ƒäº†SoTAæ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œä½¿ç”¨åˆ‡çº¿å›¾åƒå¹¶å°†é¢„æµ‹çš„åˆ‡çº¿æ·±åº¦å›¾é‡æ–°æŠ•å½±åˆ°ERPä¸­è¿›è¡Œå¯¹é½å’Œæ··åˆï¼Œå¦‚å›¾Â [9](#S3.F9
    "å›¾ 9 â€£ 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡ â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨å‘è§†è§‰ä»»åŠ¡ â€£ å…¨å‘è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")ï¼ˆbï¼‰æ‰€ç¤ºã€‚ç„¶è€Œï¼Œç›´æ¥å°†åˆ‡çº¿å›¾åƒé‡æ–°æŠ•å½±å›ERPæ ¼å¼ä¼šå¯¼è‡´é‡å å’Œä¸è¿ç»­ã€‚å› æ­¤ï¼ŒOmniFusionÂ [[25](#bib.bib25)]ï¼ˆè¿„ä»Šä¸ºæ­¢çš„SoTAæ–¹æ³•ï¼‰å¼•å…¥äº†é¢å¤–çš„3Då‡ ä½•åµŒå…¥ï¼Œä»¥ç¼“è§£è¡¥ä¸çº§ç‰¹å¾ä¸­çš„å·®å¼‚ï¼Œå¹¶åˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„å˜æ¢å™¨èšåˆè¡¥ä¸çº§ä¿¡æ¯ã€‚
- en: 'Geometric Information Prior: Some methods add extra geometric information priors
    to improve the performance, e.g., edge-plane information, surface normal, boundaries,
    as shown in Fig.Â [9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2
    Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). Eder et al.Â [[131](#bib.bib131)] assumed
    that each scene is piecewise planar and the principal curvature of each planar
    region, which is the second derivative of depth, should be zero. Consequently,
    they proposed a plane-aware learning scheme that jointly predicts depth, surface
    normal, and boundaries. Similar toÂ [[131](#bib.bib131)], Feng et al.Â [[136](#bib.bib136)]
    proposed a framework to refine depth estimation using the surface normal and uncertainty
    scores. For a pixel with higher uncertainty, its prediction is mainly aggregated
    from the neighboring pixels. Particularly, Jin et al.Â [[126](#bib.bib126)] demonstrated
    that the representations of geometric structure, e.g., corners, boundaries, and
    planes, can provide the regularization for depth estimation and benefit it as
    the prior information well.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä½•ä¿¡æ¯å…ˆéªŒï¼šä¸€äº›æ–¹æ³•æ·»åŠ é¢å¤–çš„å‡ ä½•ä¿¡æ¯å…ˆéªŒæ¥æé«˜æ€§èƒ½ï¼Œä¾‹å¦‚è¾¹ç¼˜å¹³é¢ä¿¡æ¯ã€è¡¨é¢æ³•çº¿ã€è¾¹ç•Œï¼Œå¦‚å›¾[9](#S3.F9 "å›¾ 9 â€£ 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡
    â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")(c)æ‰€ç¤ºã€‚Ederç­‰äºº[[131](#bib.bib131)]å‡è®¾æ¯ä¸ªåœºæ™¯æ˜¯åˆ†æ®µå¹³é¢çš„ï¼Œæ¯ä¸ªå¹³é¢åŒºåŸŸçš„ä¸»æ›²ç‡ï¼Œå³æ·±åº¦çš„äºŒé˜¶å¯¼æ•°ï¼Œåº”ä¸ºé›¶ã€‚å› æ­¤ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§å¹³é¢æ„ŸçŸ¥å­¦ä¹ æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆè”åˆé¢„æµ‹æ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¾¹ç•Œã€‚ç±»ä¼¼äº[[131](#bib.bib131)]ï¼ŒFengç­‰äºº[[136](#bib.bib136)]æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡è¡¨é¢æ³•çº¿å’Œä¸ç¡®å®šæ€§è¯„åˆ†æ¥ç²¾ç»†åŒ–æ·±åº¦ä¼°è®¡ã€‚å¯¹äºå…·æœ‰æ›´é«˜ä¸ç¡®å®šæ€§çš„åƒç´ ï¼Œå…¶é¢„æµ‹ä¸»è¦æ¥è‡ªé‚»è¿‘åƒç´ ã€‚ç‰¹åˆ«åœ°ï¼ŒJinç­‰äºº[[126](#bib.bib126)]è¯æ˜äº†å‡ ä½•ç»“æ„çš„è¡¨ç¤ºï¼Œä¾‹å¦‚è§’ç‚¹ã€è¾¹ç•Œå’Œå¹³é¢ï¼Œå¯ä»¥ä¸ºæ·±åº¦ä¼°è®¡æä¾›æ­£åˆ™åŒ–ï¼Œå¹¶ä½œä¸ºå…ˆéªŒä¿¡æ¯æœ‰ç›Šã€‚
- en: 'Multiple Views: As ODI depth annotations are expensive, some works leverage
    the multiple viewpoints to synthesize data and obtain competitive results. Zioulis
    et al.Â [[18](#bib.bib18)] explored the spherical view synthesis for self-supervised
    monocular depth estimation. As shown in Fig.Â [9](#S3.F9 "Figure 9 â€£ 3.2.3 Monocular
    Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional Vision Tasks â€£
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(d),
    inÂ [[18](#bib.bib18)], after predicting the ERP format depth map, stereo viewpoints
    in vertical and horizontal baselines are synthesized by the depth-image-based
    rendering. Synthesized images are supervised by real images with the same viewpoints
    via photometric image reconstruction loss. To improve accuracy and stability simultaneously,
    Yun et al.Â [[128](#bib.bib128)] proposed a joint learning framework to estimate
    monocular depth via supervised learning and estimate poses via self-supervised
    learning from the adjacent frames of ODV, as shown in Fig.Â [9](#S3.F9 "Figure
    9 â€£ 3.2.3 Monocular Depth Estimation â€£ 3.2 Scene Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(e).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šè§†è§’ï¼šç”±äºODIæ·±åº¦æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œä¸€äº›ç ”ç©¶åˆ©ç”¨å¤šè§†è§’åˆæˆæ•°æ®ä»¥è·å¾—ç«äº‰æ€§ç»“æœã€‚Zioulisç­‰äºº[[18](#bib.bib18)]æ¢ç´¢äº†ç”¨äºè‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡çš„çƒé¢è§†è§’åˆæˆã€‚å¦‚å›¾[9](#S3.F9
    "å›¾ 9 â€£ 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡ â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")(d)æ‰€ç¤ºï¼Œåœ¨[[18](#bib.bib18)]ä¸­ï¼Œé¢„æµ‹ERPæ ¼å¼æ·±åº¦å›¾åï¼Œé€šè¿‡æ·±åº¦å›¾åƒæ¸²æŸ“åˆæˆäº†å‚ç›´å’Œæ°´å¹³åŸºçº¿çš„ç«‹ä½“è§†è§’ã€‚åˆæˆå›¾åƒé€šè¿‡ä¸åŒè§†è§’çš„çœŸå®å›¾åƒè¿›è¡Œå…‰åº¦å›¾åƒé‡å»ºæŸå¤±æ¥è¿›è¡Œç›‘ç£ã€‚ä¸ºäº†åŒæ—¶æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼ŒYunç­‰äºº[[128](#bib.bib128)]æå‡ºäº†ä¸€ä¸ªè”åˆå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ ä¼°è®¡å•ç›®æ·±åº¦ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä»ODVçš„ç›¸é‚»å¸§ä¸­ä¼°è®¡å§¿æ€ï¼Œå¦‚å›¾[9](#S3.F9
    "å›¾ 9 â€£ 3.2.3 å•ç›®æ·±åº¦ä¼°è®¡ â€£ 3.2 åœºæ™¯ç†è§£ â€£ 3 å…¨æ™¯è§†è§‰ä»»åŠ¡ â€£ å…¨æ™¯è§†è§‰æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°ä¸æ–°è§†è§’")(e)æ‰€ç¤ºã€‚
- en: 'Discussion: Based on the aforementioned analysis, most methods only consider
    indoor scenes due to two main reasons: (i) Some geometric priors are ineffective
    in the wild, e.g., the plane assumption; (ii) Outdoor scenes are more challenging
    due to the scale ambiguity in approximately infinite regions (e.g., sky), and
    objects in various shapes and sizesÂ [[130](#bib.bib130)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºï¼šæ ¹æ®ä¸Šè¿°åˆ†æï¼Œå¤§å¤šæ•°æ–¹æ³•ä»…è€ƒè™‘å®¤å†…åœºæ™¯ï¼Œä¸»è¦æœ‰ä¸¤ä¸ªåŸå› ï¼šï¼ˆiï¼‰ä¸€äº›å‡ ä½•å…ˆéªŒåœ¨é‡å¤–æ— æ•ˆï¼Œä¾‹å¦‚å¹³é¢å‡è®¾ï¼›ï¼ˆiiï¼‰æˆ·å¤–åœºæ™¯ç”±äºè¿‘ä¼¼æ— é™åŒºåŸŸï¼ˆä¾‹å¦‚å¤©ç©ºï¼‰çš„å°ºåº¦æ¨¡ç³Šå’Œå„ç§å½¢çŠ¶åŠå¤§å°çš„ç‰©ä½“è€Œæ›´å…·æŒ‘æˆ˜æ€§[[130](#bib.bib130)]ã€‚
- en: It has been demonstrated that directly applying the DL-based methods for 2D
    optical flow estimation on ODI will obtain the unsatisfactory resultsÂ [[137](#bib.bib137)].
    To this end, Xie et al.Â [[138](#bib.bib138)] introduced a small diagnostic dataset
    FlowCLEVR and evaluated the performance of three kinds of tailored convolution
    filters, namely the correlation, coordinate and deformable convolutions, for estimating
    the omnidirectional optical flow. The domain adaptation frameworksÂ [[139](#bib.bib139),
    [140](#bib.bib140)] benefit from the development of optical flow estimation in
    the perspective domain. Similar toÂ [[137](#bib.bib137)], OmniFlowNetÂ [[139](#bib.bib139)]
    is built on FlowNet2 and the convolution operation is inspired byÂ [[117](#bib.bib117)].
    Especially, as the extension ofÂ [[141](#bib.bib141)], LiteFlowNet360Â [[140](#bib.bib140)]
    uses kernel transformation techniques to solve the inherent distortion problem
    caused by the sphere-to-plane projection. A representative pipeline is proposed
    byÂ [[142](#bib.bib142)], consisting of a data augmentation method and a flow estimation
    module. The data augmentation method overcomes the distortions introduced by ERP,
    and the flow estimation module exploits the cyclicity of spherical boundaries
    to convert long-distance estimation into a relatively short-distance estimation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å·²è¯æ˜ç›´æ¥å°†åŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ–¹æ³•åº”ç”¨äºäºŒç»´å…‰æµä¼°è®¡ä¼šå¾—åˆ°ä¸ä»¤äººæ»¡æ„çš„ç»“æœÂ [[137](#bib.bib137)]ã€‚ä¸ºæ­¤ï¼ŒXie ç­‰äººÂ [[138](#bib.bib138)]
    å¼•å…¥äº†ä¸€ä¸ªå°å‹è¯Šæ–­æ•°æ®é›† FlowCLEVRï¼Œå¹¶è¯„ä¼°äº†ä¸‰ç§é‡èº«å®šåˆ¶çš„å·ç§¯æ»¤æ³¢å™¨çš„æ€§èƒ½ï¼Œå³ç›¸å…³å·ç§¯ã€åæ ‡å·ç§¯å’Œå¯å˜å½¢å·ç§¯ï¼Œç”¨äºä¼°è®¡å…¨æ™¯å…‰æµã€‚é¢†åŸŸé€‚åº”æ¡†æ¶Â [[139](#bib.bib139),
    [140](#bib.bib140)] å—ç›Šäºå…‰æµä¼°è®¡åœ¨è§†è§’åŸŸä¸­çš„å‘å±•ã€‚ç±»ä¼¼äºÂ [[137](#bib.bib137)]ï¼ŒOmniFlowNetÂ [[139](#bib.bib139)]
    åŸºäº FlowNet2 æ„å»ºï¼Œå…¶å·ç§¯æ“ä½œçµæ„Ÿæ¥è‡ªäºÂ [[117](#bib.bib117)]ã€‚ç‰¹åˆ«åœ°ï¼Œä½œä¸ºÂ [[141](#bib.bib141)] çš„æ‰©å±•ï¼ŒLiteFlowNet360Â [[140](#bib.bib140)]
    ä½¿ç”¨æ ¸å˜æ¢æŠ€æœ¯æ¥è§£å†³ç”±çƒé¢åˆ°å¹³é¢æŠ•å½±å¼•èµ·çš„å›ºæœ‰å¤±çœŸé—®é¢˜ã€‚ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æµç¨‹ç”±Â [[142](#bib.bib142)] æå‡ºï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºæ–¹æ³•å’Œå…‰æµä¼°è®¡æ¨¡å—ã€‚æ•°æ®å¢å¼ºæ–¹æ³•å…‹æœäº†ERPå¼•å…¥çš„å¤±çœŸï¼Œå…‰æµä¼°è®¡æ¨¡å—åˆ©ç”¨çƒé¢è¾¹ç•Œçš„å‘¨æœŸæ€§ï¼Œå°†è¿œç¨‹ä¼°è®¡è½¬æ¢ä¸ºç›¸å¯¹çŸ­è·ç¦»ä¼°è®¡ã€‚
- en: 3.2.4 Video Summarization
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 è§†é¢‘æ‘˜è¦
- en: 'Insight: Video summarization aims to generate representative and complete synopsis
    by selecting the parts containing the most critical information of the ODV.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: è§è§£ï¼šè§†é¢‘æ‘˜è¦æ—¨åœ¨é€šè¿‡é€‰æ‹©åŒ…å«æœ€å…³é”®ä¿¡æ¯çš„éƒ¨åˆ†ï¼Œç”Ÿæˆä»£è¡¨æ€§å’Œå®Œæ•´çš„æ‘˜è¦ã€‚
- en: 'Compared with the methods for 2D video summarization, only a few works have
    been proposed for ODV summarization. Pano2VidÂ [[143](#bib.bib143)] is the representative
    framework that contains two sub-steps: detecting candidate events of interest
    in the entire ODV frames and applying dynamic programming to link detected events.
    However, Pano2Vid requires observing the whole video and is less capable for video
    streaming applications. Deep360PilotÂ [[42](#bib.bib42)] is the first framework
    to design a human-like online agent for automatic ODV navigation of viewers. Deep360pilot
    consists of three steps: object detection to obtain the candidate objects of interest,
    training RNN to choose the important object, and capturing exciting moments in
    ODV. AutoCamÂ [[144](#bib.bib144)] generates the normal NFoV videos from the ODVs
    following human behavior understanding. An similar strategy was applied by Yu
    et al.Â [[145](#bib.bib145)]. They built a deep ranking model for spatial summarization
    to select NFOV shots from each frame in the ODV and generated a spatio-temporal
    highlight video by extending the same model to the temporal domain. Moreover,
    LeeÂ [[146](#bib.bib146)] proposed a novel deep ranking neural network model for
    summarizing ODV both spatially and temporally.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸äºŒç»´è§†é¢‘æ‘˜è¦æ–¹æ³•ç›¸æ¯”ï¼Œç›®å‰é’ˆå¯¹å…¨æ™¯è§†é¢‘ï¼ˆODVï¼‰æ‘˜è¦çš„å·¥ä½œè¾ƒå°‘ã€‚Pano2VidÂ [[143](#bib.bib143)] æ˜¯ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå­æ­¥éª¤ï¼šåœ¨æ•´ä¸ªODVå¸§ä¸­æ£€æµ‹æ„Ÿå…´è¶£çš„å€™é€‰äº‹ä»¶ï¼Œå¹¶åº”ç”¨åŠ¨æ€è§„åˆ’å°†æ£€æµ‹åˆ°çš„äº‹ä»¶è¿æ¥èµ·æ¥ã€‚ç„¶è€Œï¼ŒPano2Vid
    éœ€è¦è§‚å¯Ÿæ•´ä¸ªè§†é¢‘ï¼Œå¹¶ä¸”åœ¨è§†é¢‘æµåº”ç”¨ä¸­èƒ½åŠ›è¾ƒå¼±ã€‚Deep360PilotÂ [[42](#bib.bib42)] æ˜¯ç¬¬ä¸€ä¸ªè®¾è®¡å‡ºç±»äººåœ¨çº¿ä»£ç†ä»¥è‡ªåŠ¨å¯¼èˆªODVçš„æ¡†æ¶ã€‚Deep360Pilot
    åŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šç‰©ä½“æ£€æµ‹ä»¥è·å–æ„Ÿå…´è¶£çš„å€™é€‰ç‰©ä½“ï¼Œè®­ç»ƒRNNä»¥é€‰æ‹©é‡è¦ç‰©ä½“ï¼Œå¹¶åœ¨ODVä¸­æ•æ‰ç²¾å½©ç¬é—´ã€‚AutoCamÂ [[144](#bib.bib144)]
    æ ¹æ®å¯¹äººç±»è¡Œä¸ºçš„ç†è§£ï¼Œä»ODVä¸­ç”Ÿæˆæ™®é€šçš„NFoVè§†é¢‘ã€‚Yu ç­‰äººÂ [[145](#bib.bib145)] é‡‡ç”¨äº†ç±»ä¼¼çš„ç­–ç•¥ã€‚ä»–ä»¬å»ºç«‹äº†ä¸€ä¸ªæ·±åº¦æ’åºæ¨¡å‹ï¼Œç”¨äºç©ºé—´æ‘˜è¦ï¼Œä»æ¯ä¸€å¸§ä¸­é€‰æ‹©NFoVé•œå¤´ï¼Œå¹¶é€šè¿‡å°†ç›¸åŒæ¨¡å‹æ‰©å±•åˆ°æ—¶é—´åŸŸç”Ÿæˆæ—¶ç©ºäº®ç‚¹è§†é¢‘ã€‚æ­¤å¤–ï¼ŒLeeÂ [[146](#bib.bib146)]
    æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ·±åº¦æ’åºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šæ€»ç»“ODVã€‚
- en: 'Discussion: Based on the above analysis, only a few methods exist in this research
    domain. As a temporal-related task, applying the transformer mechanism to ODV
    summarization could be beneficial. In addition, previous works only considered
    the ERP format, which suffer from the most severe distortion problems. Therefore,
    it is better to consider the CP, tangent projection or sphere format as the input
    for ODV summarization.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºï¼šåŸºäºä»¥ä¸Šåˆ†æï¼Œç›®å‰åœ¨è¿™ä¸€ç ”ç©¶é¢†åŸŸä»…å­˜åœ¨å°‘æ•°å‡ ç§æ–¹æ³•ã€‚ä½œä¸ºä¸€ä¸ªæ—¶é—´ç›¸å…³ä»»åŠ¡ï¼Œå°†å˜å‹å™¨æœºåˆ¶åº”ç”¨äºODVæ€»ç»“å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„å·¥ä½œä»…è€ƒè™‘äº†ERPæ ¼å¼ï¼Œè¿™ç§æ ¼å¼å­˜åœ¨æœ€ä¸¥é‡çš„å¤±çœŸé—®é¢˜ã€‚å› æ­¤ï¼Œè€ƒè™‘CPã€åˆ‡çº¿æŠ•å½±æˆ–çƒé¢æ ¼å¼ä½œä¸ºODVæ€»ç»“çš„è¾“å…¥ä¼šæ›´å¥½ã€‚
- en: 3.3 3D Vision
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 3D è§†è§‰
- en: 'TABLE V: Room Layout estimation overview on representative studies.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ Vï¼šä»£è¡¨æ€§ç ”ç©¶ä¸­çš„æˆ¿é—´å¸ƒå±€ä¼°è®¡æ¦‚è¿°ã€‚
- en: '| Method | Publication | Architecture | Highlight | Projection | Task |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | å‘è¡¨ | ä½“ç³»ç»“æ„ | äº®ç‚¹ | æŠ•å½± | ä»»åŠ¡ |'
- en: '| Zhang [[147](#bib.bib147)] | ICCVâ€™21 | Mask RCNN+ODN +LIEN+HorizonNet | Context
    relation modeling | ERP | Layout+ object +semantic labels |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| å¼ [[147](#bib.bib147)] | ICCVâ€™21 | Mask RCNN+ODN +LIEN+HorizonNet | ä¸Šä¸‹æ–‡å…³ç³»å»ºæ¨¡
    | ERP | å¸ƒå±€+å¯¹è±¡+è¯­ä¹‰æ ‡ç­¾ |'
- en: '| Yang [[148](#bib.bib148)] | CVPRâ€™19 | Two ResNet on ceiling and floor | Projection
    feature fusion | ERP, ceiling | Layout |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| æ¨[[148](#bib.bib148)] | CVPRâ€™19 | ä¸¤ä¸ª ResNet ç”¨äºå¤©èŠ±æ¿å’Œåœ°æ¿ | æŠ•å½±ç‰¹å¾èåˆ | ERPï¼Œå¤©èŠ±æ¿ |
    å¸ƒå±€ |'
- en: '| Zou [[149](#bib.bib149)] | CVPRâ€™18 | CNN+3D layout regressor | Boundary+Corner
    map prediction | ERP | Layout |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Zou [[149](#bib.bib149)] | CVPRâ€™18 | CNN+3D å¸ƒå±€å›å½’å™¨ | è¾¹ç•Œ+è§’ç‚¹å›¾é¢„æµ‹ | ERP | å¸ƒå±€ |'
- en: '| Tran [[150](#bib.bib150)] | CVPRâ€™21 | HorizonNet+EMA | Semi-supervised learning
    | ERP | Layout |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Tran [[150](#bib.bib150)] | CVPRâ€™21 | HorizonNet+EMA | åŠç›‘ç£å­¦ä¹  | ERP | å¸ƒå±€ |'
- en: '| Pintore [[151](#bib.bib151)] | ECCVâ€™20 | ResNet+RNN | Atlanta World indoor
    Model | ERP, ceiling | Layout |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Pintore [[151](#bib.bib151)] | ECCVâ€™20 | ResNet+RNN | äºšç‰¹å…°å¤§ä¸–ç•Œå®¤å†…æ¨¡å‹ | ERPï¼Œå¤©èŠ±æ¿
    | å¸ƒå±€ |'
- en: '| Sun [[152](#bib.bib152)] | CVPRâ€™19 | ResNet+RNN | 1D representation of layout
    | ERP | Layout |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| å­™[[152](#bib.bib152)] | CVPRâ€™19 | ResNet+RNN | å¸ƒå±€çš„ä¸€ç»´è¡¨ç¤º | ERP | å¸ƒå±€ |'
- en: '| Sun [[153](#bib.bib153)] | CVPRâ€™21 | ResNet+ efficient height compression
    | Latent horizontal feature | ERP | Layout, depth +semantic labels |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| å­™[[153](#bib.bib153)] | CVPRâ€™21 | ResNet+é«˜æ•ˆé«˜åº¦å‹ç¼© | æ½œåœ¨çš„æ°´å¹³ç‰¹å¾ | ERP | å¸ƒå±€ï¼Œæ·±åº¦+è¯­ä¹‰æ ‡ç­¾
    |'
- en: '| Wang [[154](#bib.bib154)] | CVPRâ€™21 | HorizonNet$\&amp;$L2D transformation
    | Differentiable depth rendering | ERP | Layout |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ç‹[[154](#bib.bib154)] | CVPRâ€™21 | HorizonNet$\&amp;$L2D å˜æ¢ | å¯å¾®åˆ†æ·±åº¦æ¸²æŸ“ | ERP
    | å¸ƒå±€ |'
- en: 3.3.1 Room Layout estimation and Reconstruction
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 æˆ¿é—´å¸ƒå±€ä¼°è®¡ä¸é‡å»º
- en: 'Insight: Room Layout estimation and reconstruction consists of multiple sub-tasks
    such as layout estimation, 3D object detection and 3D object reconstruction. This
    comprehensive task aims to facilitate holistic scene understanding based on a
    single ODI.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è§è§£ï¼šæˆ¿é—´å¸ƒå±€ä¼°è®¡å’Œé‡å»ºåŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œå¦‚å¸ƒå±€ä¼°è®¡ã€3Då¯¹è±¡æ£€æµ‹å’Œ3Då¯¹è±¡é‡å»ºã€‚è¿™ä¸ªç»¼åˆä»»åŠ¡æ—¨åœ¨åŸºäºå•ä¸€ODIä¿ƒè¿›æ•´ä½“åœºæ™¯ç†è§£ã€‚
- en: 'As the indoor panoramas can cover wider surrounding environment and capture
    more context cues than conventional perspective images, they are beneficial to
    scene understanding and widely applied into room layout estimation and reconstruction.
    Zou et al.Â [[13](#bib.bib13)] summarized that the general procedure of layout
    estimation and reconstruction contains three sub-steps: edge-based alignment,
    layout elements prediction, and 3D layout elements recovery, as shown in Fig.Â [10](#S3.F10
    "Figure 10 â€£ 3.3.1 Room Layout estimation and Reconstruction â€£ 3.3 3D Vision â€£
    3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives"). The representative work, proposed by Zhang et al.Â [[147](#bib.bib147)],
    conducts the first DL-based pipeline for holistic 3D scene understanding that
    recovers 3D room layout and detailed information, e.g., shape, pose, and location
    of objects from a single ODI. InÂ [[147](#bib.bib147)], a context-based GNN is
    designed to predict the relationships across the objects and room layout and achieves
    the SoTA performance on both geometry accuracy of room layout and 3D object arrangement.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®¤å†…å…¨æ™¯å›¾èƒ½å¤Ÿè¦†ç›–æ›´å¹¿æ³›çš„å‘¨è¾¹ç¯å¢ƒå¹¶æ•æ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œç›¸æ¯”ä¼ ç»Ÿé€è§†å›¾åƒï¼Œå®ƒä»¬å¯¹åœºæ™¯ç†è§£æ˜¯æœ‰ç›Šçš„ï¼Œå¹¶ä¸”è¢«å¹¿æ³›åº”ç”¨äºæˆ¿é—´å¸ƒå±€ä¼°è®¡å’Œé‡å»ºã€‚Zou ç­‰äºº[[13](#bib.bib13)]æ€»ç»“äº†å¸ƒå±€ä¼°è®¡å’Œé‡å»ºçš„ä¸€èˆ¬è¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªå­æ­¥éª¤ï¼šåŸºäºè¾¹ç¼˜çš„å¯¹é½ã€å¸ƒå±€å…ƒç´ é¢„æµ‹å’Œ3Då¸ƒå±€å…ƒç´ æ¢å¤ï¼Œå¦‚å›¾
    [10](#S3.F10 "å›¾ 10 â€£ 3.3.1 æˆ¿é—´å¸ƒå±€ä¼°è®¡ä¸é‡å»º â€£ 3.3 3D è§†è§‰ â€£ 3 å…¨å‘è§†è§‰ä»»åŠ¡ â€£ å…¨å‘è§†è§‰çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥ä¸æ–°è§†è§’")
    æ‰€ç¤ºã€‚ä»£è¡¨æ€§å·¥ä½œç”±å¼ ç­‰äºº[[147](#bib.bib147)]æå‡ºï¼Œè¿›è¡Œé¦–ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ•´ä½“3Dåœºæ™¯ç†è§£ç®¡é“ï¼Œè¯¥ç®¡é“ä»å•ä¸€çš„ODIä¸­æ¢å¤3Dæˆ¿é—´å¸ƒå±€å’Œè¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚å½¢çŠ¶ã€å§¿æ€å’Œå¯¹è±¡ä½ç½®ã€‚åœ¨
    [[147](#bib.bib147)] ä¸­ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡çš„GNNæ¥é¢„æµ‹å¯¹è±¡å’Œæˆ¿é—´å¸ƒå±€ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åœ¨æˆ¿é—´å¸ƒå±€çš„å‡ ä½•ç²¾åº¦å’Œ3Då¯¹è±¡æ’åˆ—ä¸Šå–å¾—äº†SoTAè¡¨ç°ã€‚
- en: For the alignment, this pre-possessing step provides indoor geometric information
    as the prior knowledge to ease the network training. Several SoTA approachesÂ [[148](#bib.bib148),
    [149](#bib.bib149), [150](#bib.bib150)] follow the â€Manhattan worldâ€ assumption,
    in which all walls are aligned with a canonical coordinate system, and the floor
    plane direction is estimated by selecting long line segments and voting for the
    three mutually orthogonal vanishing directions. In contrast, AtlantaNetÂ [[151](#bib.bib151)]
    predicts the 3D layout from less restrictive scenes that are not limited to â€Manhattan
    Worldâ€ assumption. AtlantaNet follows â€Atlanta Worldâ€ assumption and projects
    an gravity-aligned ODI into two horizontal planes to predict a 2D room footprint
    on the floor plan and a room height to recover the 3D layout.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¯¹é½ï¼Œè¿™ä¸ªé¢„å¤„ç†æ­¥éª¤æä¾›äº†å®¤å†…å‡ ä½•ä¿¡æ¯ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œä»¥ç®€åŒ–ç½‘ç»œè®­ç»ƒã€‚ä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•[[148](#bib.bib148)ã€[149](#bib.bib149)ã€[150](#bib.bib150)]éµå¾ªâ€œæ›¼å“ˆé¡¿ä¸–ç•Œâ€å‡è®¾ï¼Œå…¶ä¸­æ‰€æœ‰å¢™å£éƒ½ä¸æ ‡å‡†åæ ‡ç³»å¯¹é½ï¼Œåœ°é¢å¹³é¢æ–¹å‘é€šè¿‡é€‰æ‹©é•¿ç›´çº¿æ®µå¹¶æŠ•ç¥¨ç¡®å®šä¸‰ä¸ªç›¸äº’æ­£äº¤çš„æ¶ˆå¤±æ–¹å‘æ¥ä¼°è®¡ã€‚ç›¸åï¼ŒAtlantaNet[[151](#bib.bib151)]é¢„æµ‹çš„æ˜¯ä¸å—â€œæ›¼å“ˆé¡¿ä¸–ç•Œâ€å‡è®¾é™åˆ¶çš„è¾ƒå°‘çº¦æŸåœºæ™¯ä¸­çš„3Då¸ƒå±€ã€‚AtlantaNetéµå¾ªâ€œäºšç‰¹å…°å¤§ä¸–ç•Œâ€å‡è®¾ï¼Œå°†é‡åŠ›å¯¹é½çš„ODIæŠ•å½±åˆ°ä¸¤ä¸ªæ°´å¹³é¢ä¸Šï¼Œä»¥é¢„æµ‹åœ°é¢å¹³é¢ä¸Šçš„2Dæˆ¿é—´è¶³è¿¹å’Œæˆ¿é—´é«˜åº¦ï¼Œä»è€Œæ¢å¤3Då¸ƒå±€ã€‚
- en: '![Refer to caption](img/d0cc5dd87272d938d298afa32b91a633.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/d0cc5dd87272d938d298afa32b91a633.png)'
- en: 'Figure 10: The general room layout prediction architecture.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10ï¼šä¸€èˆ¬æˆ¿é—´å¸ƒå±€é¢„æµ‹æ¶æ„ã€‚
- en: For the layout element prediction, the primary task is to estimate layout boundaries
    and corner positions. On the one hand, the related methods usually choose different
    projections of ODIs as the input. For instance, some methodsÂ [[149](#bib.bib149),
    [152](#bib.bib152), [153](#bib.bib153)] predict the layout only from ERP images.
    Besides the ERP, Yang et al.Â [[148](#bib.bib148)] added a perspective ceiling-view
    image, which is obtained from the ERP through an equirectangular-to-perspective
    (E2P) conversion, as an extra input. They then extracted the features from the
    two formats by a two-branch network and fused the two-modal features to predict
    the layout elements. The advantage ofÂ [[148](#bib.bib148)] is that it directly
    uses the multi-projection model to jointly predict a Manhattan-world floor plan
    instead of estimating the number of corners. On the other hand, recent methods
    varied in their ways of feature representation. For instance, HorizonNetÂ [[152](#bib.bib152)]
    represents the room layout of the ODI as three 1D embedding vectors and recovers
    3D room layouts from 1D predictions with low computation cost. Differently, Wang
    et al.Â [[154](#bib.bib154)] converted the layout into â€™horizon-depthâ€™ through
    ray casting of a few points. This transformation maintains the simplicity of layout
    estimation and improves the generalization capacity to unseen room layouts.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¸ƒå±€å…ƒç´ é¢„æµ‹ï¼Œä¸»è¦ä»»åŠ¡æ˜¯ä¼°è®¡å¸ƒå±€è¾¹ç•Œå’Œè§’ç‚¹ä½ç½®ã€‚ä¸€æ–¹é¢ï¼Œç›¸å…³æ–¹æ³•é€šå¸¸é€‰æ‹©ODIçš„ä¸åŒæŠ•å½±ä½œä¸ºè¾“å…¥ã€‚ä¾‹å¦‚ï¼Œä¸€äº›æ–¹æ³•[[149](#bib.bib149)ã€[152](#bib.bib152)ã€[153](#bib.bib153)]ä»…ä»ERPå›¾åƒä¸­é¢„æµ‹å¸ƒå±€ã€‚é™¤äº†ERPä¹‹å¤–ï¼ŒYangç­‰äºº[[148](#bib.bib148)]è¿˜æ·»åŠ äº†ä¸€å¼ é€šè¿‡ç­‰è·çŸ©å½¢åˆ°é€è§†ï¼ˆE2Pï¼‰è½¬æ¢ä»ERPè·å–çš„é€è§†å¤©èŠ±æ¿è§†å›¾å›¾åƒï¼Œä½œä¸ºé¢å¤–è¾“å…¥ã€‚ç„¶åï¼Œä»–ä»¬é€šè¿‡ä¸€ä¸ªåŒåˆ†æ”¯ç½‘ç»œä»è¿™ä¸¤ç§æ ¼å¼ä¸­æå–ç‰¹å¾ï¼Œå¹¶èåˆè¿™ä¸¤ç§æ¨¡æ€ç‰¹å¾æ¥é¢„æµ‹å¸ƒå±€å…ƒç´ ã€‚[[148](#bib.bib148)]çš„ä¼˜ç‚¹æ˜¯å®ƒç›´æ¥ä½¿ç”¨å¤šæŠ•å½±æ¨¡å‹è”åˆé¢„æµ‹æ›¼å“ˆé¡¿ä¸–ç•Œåœ°é¢å¹³é¢ï¼Œè€Œä¸æ˜¯ä¼°è®¡è§’ç‚¹æ•°ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘çš„æ–¹æ³•åœ¨ç‰¹å¾è¡¨ç¤ºæ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒã€‚ä¾‹å¦‚ï¼ŒHorizonNet[[152](#bib.bib152)]å°†ODIçš„æˆ¿é—´å¸ƒå±€è¡¨ç¤ºä¸ºä¸‰ä¸ª1DåµŒå…¥å‘é‡ï¼Œå¹¶ä»1Dé¢„æµ‹ä¸­æ¢å¤3Dæˆ¿é—´å¸ƒå±€ï¼Œè®¡ç®—æˆæœ¬ä½ã€‚ä¸åŒçš„æ˜¯ï¼ŒWangç­‰äºº[[154](#bib.bib154)]é€šè¿‡å¯¹å°‘é‡ç‚¹è¿›è¡Œå…‰çº¿æŠ•å°„ï¼Œå°†å¸ƒå±€è½¬æ¢ä¸ºâ€œè§†ç•Œæ·±åº¦â€ã€‚è¿™ç§è½¬æ¢ä¿æŒäº†å¸ƒå±€ä¼°è®¡çš„ç®€å•æ€§ï¼Œå¹¶æé«˜äº†å¯¹æœªè§è¿‡çš„æˆ¿é—´å¸ƒå±€çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: For final recovery, the general strategyÂ [[149](#bib.bib149), [148](#bib.bib148),
    [152](#bib.bib152)] is to reconstruct the layout by the optimization of mapping
    each pixel between walls and corners. In particular, it defines the weighted loss
    of probability maps of floors, ceilings, and corners. The major difficulty is
    the layout boundary occlusions when the camera position is not ideal for the entire
    display. To address this problem, HorizonNetÂ [[152](#bib.bib152)] observes the
    occlusions by examining the orientation of the first Principal Component Analysis
    (PCA) component of adjacent walls and recovers occluded parts according to the
    long-term dependencies of global geometry.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæœ€ç»ˆæ¢å¤ï¼Œä¸€èˆ¬ç­–ç•¥[[149](#bib.bib149), [148](#bib.bib148), [152](#bib.bib152)] æ˜¯é€šè¿‡ä¼˜åŒ–åœ¨å¢™å£å’Œè§’è½ä¹‹é—´æ˜ å°„æ¯ä¸ªåƒç´ æ¥é‡å»ºå¸ƒå±€ã€‚ç‰¹åˆ«åœ°ï¼Œå®ƒå®šä¹‰äº†åœ°æ¿ã€å¤©èŠ±æ¿å’Œè§’è½çš„æ¦‚ç‡å›¾çš„åŠ æƒæŸå¤±ã€‚ä¸»è¦å›°éš¾åœ¨äºå½“ç›¸æœºä½ç½®ä¸ç†æƒ³æ—¶çš„å¸ƒå±€è¾¹ç•Œé®æŒ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒHorizonNet
    [[152](#bib.bib152)] é€šè¿‡æ£€æŸ¥ç›¸é‚»å¢™å£çš„ç¬¬ä¸€ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åˆ†é‡çš„æ–¹å‘æ¥è§‚å¯Ÿé®æŒ¡ï¼Œå¹¶æ ¹æ®å…¨çƒå‡ ä½•å½¢çŠ¶çš„é•¿æœŸä¾èµ–å…³ç³»æ¢å¤è¢«é®æŒ¡çš„éƒ¨åˆ†ã€‚
- en: 3.3.2 Stereo Matching
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 ç«‹ä½“åŒ¹é…
- en: 'Human binocular disparity depends on the difference between the projections
    on the retina, that is, a sphere projection rather than a planar projection. Therefore,
    stereo matching on the ODIs is more similar to the human vision system. In Â [[155](#bib.bib155)],
    they discussed the influence of omnidirectional distortion on the CNN-based methods
    and compared the quality of disparity maps predicted from the perspective and
    omnidirectional stereo images. The experimental results show that stereo matching
    based on the ODIs is more advantageous for numerous applications, e.g., robotics,
    AR/VR, and several other applications. General stereo matching algorithms follow
    four steps: (i) matching cost computation, (ii) cost aggregation, (iii) disparity
    computation with optimization, and (iv) disparity refinement. As the first DNN-based
    omnidirectional stereo framework, SweepNetÂ [[156](#bib.bib156)] proposes a wide-baseline
    stereo system to compute the matching cost map from a pair of images captured
    by cameras with ultra-wide FoV lenses and uses a global sphere sweep at the rig
    coordinate system to generate an omnidirectional depth map directly. By contrast,
    OmniMVSÂ [[157](#bib.bib157)] takes four 220^âˆ˜ FoV fisheye views as the input to
    train an end-to-end DNN model and uses a 3D encoder-decoder block to regularize
    the cost volume. The method proposed inÂ [[158](#bib.bib158)], as the extension
    of OmniMVS, provides a novel regularization of cost volume based on the uncertainty
    of prior guidance. Another representative work, 360SD-NetÂ [[159](#bib.bib159)],
    is the first end-to-end trainable network for omnidirectional stereo depth estimation
    with the top-bottom ODI pairs as the input. It mitigates the distortion in the
    ERP images through an additional polar angle coordinate input and a learnable
    cost volume.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»çš„åŒçœ¼è§†å·®å–å†³äºè§†ç½‘è†œä¸Šçš„æŠ•å½±å·®å¼‚ï¼Œå³çƒé¢æŠ•å½±è€Œä¸æ˜¯å¹³é¢æŠ•å½±ã€‚å› æ­¤ï¼ŒåŸºäºODIsçš„ç«‹ä½“åŒ¹é…æ›´æ¥è¿‘äººç±»è§†è§‰ç³»ç»Ÿã€‚åœ¨[[155](#bib.bib155)]ä¸­ï¼Œä»–ä»¬è®¨è®ºäº†å…¨å‘ç•¸å˜å¯¹åŸºäºCNNçš„æ–¹æ³•çš„å½±å“ï¼Œå¹¶æ¯”è¾ƒäº†ä»è§†è§’å’Œå…¨å‘ç«‹ä½“å›¾åƒä¸­é¢„æµ‹çš„è§†å·®å›¾çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºODIsçš„ç«‹ä½“åŒ¹é…åœ¨è®¸å¤šåº”ç”¨ä¸­æ›´å…·ä¼˜åŠ¿ï¼Œä¾‹å¦‚æœºå™¨äººæŠ€æœ¯ã€AR/VRåŠå…¶ä»–å‡ ä¸ªåº”ç”¨ã€‚ä¸€èˆ¬çš„ç«‹ä½“åŒ¹é…ç®—æ³•åŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼šï¼ˆiï¼‰åŒ¹é…æˆæœ¬è®¡ç®—ï¼Œï¼ˆiiï¼‰æˆæœ¬èšåˆï¼Œï¼ˆiiiï¼‰ä¼˜åŒ–åçš„è§†å·®è®¡ç®—ï¼Œä»¥åŠï¼ˆivï¼‰è§†å·®ç»†åŒ–ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªåŸºäºDNNçš„å…¨å‘ç«‹ä½“æ¡†æ¶ï¼ŒSweepNet
    [[156](#bib.bib156)] æå‡ºäº†ä¸€ä¸ªå®½åŸºçº¿ç«‹ä½“ç³»ç»Ÿï¼Œä»ä¸€å¯¹ç”±è¶…å®½è§†åœºé•œå¤´æ•è·çš„å›¾åƒä¸­è®¡ç®—åŒ¹é…æˆæœ¬å›¾ï¼Œå¹¶åœ¨è®¾å¤‡åæ ‡ç³»ç»Ÿä¸­ä½¿ç”¨å…¨å±€çƒä½“æ‰«æç›´æ¥ç”Ÿæˆå…¨å‘æ·±åº¦å›¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒOmniMVS
    [[157](#bib.bib157)] ä»¥å››ä¸ª220^âˆ˜è§†åœºçš„é±¼çœ¼å›¾åƒä½œä¸ºè¾“å…¥æ¥è®­ç»ƒä¸€ä¸ªç«¯åˆ°ç«¯çš„DNNæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨3Dç¼–ç å™¨-è§£ç å™¨å—æ¥è§„èŒƒåŒ–æˆæœ¬ä½“ç§¯ã€‚[[158](#bib.bib158)]ä¸­æå‡ºçš„æ–¹æ³•ä½œä¸ºOmniMVSçš„æ‰©å±•ï¼ŒåŸºäºå…ˆéªŒæŒ‡å¯¼çš„ä¸ç¡®å®šæ€§æä¾›äº†ä¸€ç§æ–°çš„æˆæœ¬ä½“ç§¯è§„èŒƒåŒ–ã€‚å¦ä¸€ä¸ªä»£è¡¨æ€§å·¥ä½œ360SD-Net
    [[159](#bib.bib159)] æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå…¨å‘ç«‹ä½“æ·±åº¦ä¼°è®¡çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒç½‘ç»œï¼Œä»¥ä¸Šä¸‹ODIå¯¹ä½œä¸ºè¾“å…¥ã€‚å®ƒé€šè¿‡é¢å¤–çš„æè§’åæ ‡è¾“å…¥å’Œå¯å­¦ä¹ çš„æˆæœ¬ä½“ç§¯æ¥å‡è½»ERPå›¾åƒä¸­çš„ç•¸å˜ã€‚
- en: 3.3.3 SLAM
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 SLAM
- en: SLAM is an intricate system that adopts multiple cameras, e.g., monocular, stereo,
    or RGB-D, combined with sensors onboard a mobile agent to reconstruct the environment
    and estimate the agent pose in real-time. SLAM is often used in real-time navigation
    and reality augmentation, e.g., Google Earth. The stereo information, such as
    key pointsÂ [[160](#bib.bib160)] and dense or semi-dense depth maps[[161](#bib.bib161)],
    is indispensable to build an accurate modern SLAM system. Specifically, compared
    with traditional monocular SLAMÂ [[162](#bib.bib162)] or multi-view SLAMÂ [[163](#bib.bib163)],
    the omnidirectional data can provide the richer texture and structure information
    due to a large FoV, and the omnidirectional SLAM avoids the influence of discontinued
    frames in the surrounding environment and enjoys the technical advantage of complete
    positioning and mapping. Caruso et al.Â [[164](#bib.bib164)] proposed a representative
    monocular SLAM method for omnidirectional cameras in which the direct image alignment
    and pixel-wise distance filtering are directly formulated. Zachary et al.Â [[165](#bib.bib165)]
    proposed a general framework that accepts multiple types of sensor data and is
    capable of iterative updates of camera pose and pixel-wise depth. DeepFactorsÂ [[166](#bib.bib166)]
    performs joint optimization of the pose and depth variables to detect the loop
    closure. As the omnidirectional data has rich geometry and texture information,
    further works may consider how to cultivate the full potential of DL and utilize
    these imaging advantages to construct a fast and accurate SLAM system.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM æ˜¯ä¸€ä¸ªå¤æ‚çš„ç³»ç»Ÿï¼Œé‡‡ç”¨å¤šç§æ‘„åƒå¤´ï¼Œä¾‹å¦‚å•ç›®ã€ç«‹ä½“æˆ– RGB-Dï¼Œç»“åˆç§»åŠ¨ä½“ä¸Šçš„ä¼ æ„Ÿå™¨ï¼Œä»¥å®æ—¶é‡å»ºç¯å¢ƒå¹¶ä¼°è®¡ä½“çš„ä½ç½®ã€‚SLAM å¸¸ç”¨äºå®æ—¶å¯¼èˆªå’Œç°å®å¢å¼ºï¼Œä¾‹å¦‚
    Google Earthã€‚ç«‹ä½“ä¿¡æ¯ï¼Œå¦‚å…³é”®ç‚¹[[160](#bib.bib160)] å’Œå¯†é›†æˆ–åŠå¯†é›†æ·±åº¦å›¾[[161](#bib.bib161)]ï¼Œå¯¹äºæ„å»ºå‡†ç¡®çš„ç°ä»£
    SLAM ç³»ç»Ÿä¸å¯æˆ–ç¼ºã€‚å…·ä½“è€Œè¨€ï¼Œä¸ä¼ ç»Ÿçš„å•ç›® SLAM [[162](#bib.bib162)] æˆ–å¤šè§†è§’ SLAM [[163](#bib.bib163)]
    ç›¸æ¯”ï¼Œå…¨å‘æ•°æ®ç”±äºå¤§è§†åœºæä¾›äº†æ›´ä¸°å¯Œçš„çº¹ç†å’Œç»“æ„ä¿¡æ¯ï¼Œå…¨å‘ SLAM é¿å…äº†å‘¨å›´ç¯å¢ƒä¸­æ–­å¸§çš„å½±å“ï¼Œå¹¶äº«æœ‰å®Œæ•´å®šä½å’Œæ˜ å°„çš„æŠ€æœ¯ä¼˜åŠ¿ã€‚Caruso ç­‰äºº [[164](#bib.bib164)]
    æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§çš„å•ç›® SLAM æ–¹æ³•ï¼Œç”¨äºå…¨å‘æ‘„åƒå¤´ï¼Œå…¶ä¸­ç›´æ¥å›¾åƒå¯¹é½å’Œé€åƒç´ è·ç¦»æ»¤æ³¢è¢«ç›´æ¥åˆ¶å®šã€‚Zachary ç­‰äºº [[165](#bib.bib165)]
    æå‡ºäº†ä¸€ä¸ªæ¥å—å¤šç§ç±»å‹ä¼ æ„Ÿå™¨æ•°æ®çš„é€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹ç›¸æœºä½ç½®å’Œé€åƒç´ æ·±åº¦è¿›è¡Œè¿­ä»£æ›´æ–°ã€‚DeepFactors [[166](#bib.bib166)] æ‰§è¡Œå§¿æ€å’Œæ·±åº¦å˜é‡çš„è”åˆä¼˜åŒ–ï¼Œä»¥æ£€æµ‹å›ç¯é—­åˆã€‚ç”±äºå…¨å‘æ•°æ®å…·æœ‰ä¸°å¯Œçš„å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥çš„å·¥ä½œå¯èƒ½ä¼šè€ƒè™‘å¦‚ä½•æŒ–æ˜æ·±åº¦å­¦ä¹ çš„å…¨éƒ¨æ½œåŠ›ï¼Œå¹¶åˆ©ç”¨è¿™äº›æˆåƒä¼˜åŠ¿æ¥æ„å»ºä¸€ä¸ªå¿«é€Ÿè€Œå‡†ç¡®çš„
    SLAM ç³»ç»Ÿã€‚
- en: 3.4 Human Behavior Understanding
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 äººç±»è¡Œä¸ºç†è§£
- en: 3.4.1 Saliency Prediction
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 æ˜¾è‘—æ€§é¢„æµ‹
- en: 'TABLE VI: Deep ODI and ODV saliency prediction by some representative methods.
    EM and HM mean eye and head movement.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ VI: ä¸€äº›ä»£è¡¨æ€§æ–¹æ³•çš„æ·±åº¦ ODI å’Œ ODV æ˜¾è‘—æ€§é¢„æµ‹ã€‚EM å’Œ HM æ„ä¸ºçœ¼åŠ¨å’Œå¤´åŠ¨ã€‚'
- en: '| Method | Input | Publication | EM | HM | Highlight | Contribution |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | è¾“å…¥ | å‘è¡¨ | EM | HM | äº®ç‚¹ | è´¡çŒ® |'
- en: '| DaiÂ [[167](#bib.bib167)] | IMG | ICASSPâ€™20 | $\checkmark$ | $\checkmark$
    | CP $\&amp;$ 2D CNN | Dilated convolution |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Dai [[167](#bib.bib167)] | IMG | ICASSPâ€™20 | $\checkmark$ | $\checkmark$
    | CP $\&amp;$ 2D CNN | è†¨èƒ€å·ç§¯ |'
- en: '| LvÂ [[168](#bib.bib168)] | IMG | ACM MMâ€™20 | $\checkmark$ | $\checkmark$ |
    Spherical images $\&amp;$ GCN | GCN with spherical interpolation |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Lv [[168](#bib.bib168)] | IMG | ACM MMâ€™20 | $\checkmark$ | $\checkmark$ |
    çƒå½¢å›¾åƒ $\&amp;$ GCN | å¸¦çƒå½¢æ’å€¼çš„ GCN |'
- en: '| ChaoÂ [[169](#bib.bib169)] | IMG | TMMâ€™21 | $\checkmark$ | $\checkmark$ |
    Multi-viewports$\&amp;$ 2D CNN | Different FoV viewports |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Chao [[169](#bib.bib169)] | IMG | TMMâ€™21 | $\checkmark$ | $\checkmark$ |
    å¤šè§†è§’$\&amp;$ 2D CNN | ä¸åŒè§†åœºè§†å£ |'
- en: '| AbdelazizÂ [[170](#bib.bib170)] | IMG | ICCVâ€™21 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ 2D CNN $\&amp;$ self-attention mechanism | Contrastive learning
    to maximize the mutual information |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Abdelaziz [[170](#bib.bib170)] | IMG | ICCVâ€™21 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ 2D CNN $\&amp;$ è‡ªæ³¨æ„åŠ›æœºåˆ¶ | å¯¹æ¯”å­¦ä¹ ä»¥æœ€å¤§åŒ–äº’ä¿¡æ¯ |'
- en: '| XuÂ [[171](#bib.bib171)] | IMG | TIPâ€™21 | âœ— | $\checkmark$ | ERP $\&amp;$
    deep reinforcement learning | Generative adversarial imitation learning |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[171](#bib.bib171)] | IMG | TIPâ€™21 | âœ— | $\checkmark$ | ERP $\&amp;$
    æ·±åº¦å¼ºåŒ–å­¦ä¹  | ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹  |'
- en: '| NguyenÂ [[172](#bib.bib172)] | VID | ACM MMâ€™18 | âœ— | $\checkmark$ | ERP $\&amp;$
    2D CNN $\&amp;$ LSTM | Transfer learning |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen [[172](#bib.bib172)] | VID | ACM MMâ€™18 | âœ— | $\checkmark$ | ERP $\&amp;$
    2D CNN $\&amp;$ LSTM | è¿ç§»å­¦ä¹  |'
- en: '| ChenÂ [[43](#bib.bib43)] | VID | CVPRâ€™18 | âœ— | $\checkmark$ | CP $\&amp;$
    2D CNN $\&amp;$ convLSTM | Spatial-temporal network $\&amp;$ Cube Padding |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[43](#bib.bib43)] | VID | CVPRâ€™18 | âœ— | $\checkmark$ | CP $\&amp;$
    2D CNN $\&amp;$ convLSTM | æ—¶ç©ºç½‘ç»œ $\&amp;$ ç«‹æ–¹ä½“å¡«å…… |'
- en: '| ZhangÂ [[173](#bib.bib173)] | VID | ECCVâ€™18 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ spherical CNN | Spherical crown convolution kernel |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| ZhangÂ [[173](#bib.bib173)] | VID | ECCVâ€™18 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ çƒé¢CNN | çƒé¢å† å·ç§¯æ ¸ |'
- en: '| XuÂ [[174](#bib.bib174)] | VID | TPAMIâ€™19 | âœ— | $\checkmark$ | ERP $\&amp;$
    deep reinforcement learning | Deep reinforcement learning |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| XuÂ [[174](#bib.bib174)] | VID | TPAMIâ€™19 | âœ— | $\checkmark$ | ERP $\&amp;$
    æ·±åº¦å¼ºåŒ–å­¦ä¹  | æ·±åº¦å¼ºåŒ–å­¦ä¹  |'
- en: '| ZhuÂ [[175](#bib.bib175)] | VID | TCSVTâ€™21 | $\checkmark$ | âœ— | Image patches
    $\&amp;$ GCN | Graph convolution and feature alignment |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ZhuÂ [[175](#bib.bib175)] | VID | TCSVTâ€™21 | $\checkmark$ | âœ— | å›¾åƒå— $\&amp;$
    GCN | å›¾å·ç§¯å’Œç‰¹å¾å¯¹é½ |'
- en: '| QiaoÂ [[176](#bib.bib176)] | VID | TMMâ€™21 | $\checkmark$ | âœ— | Multi-viewports$\&amp;$
    2D CNN $\&amp;$ convLSTM | Multi-Task Deep Neural Network |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| QiaoÂ [[176](#bib.bib176)] | VID | TMMâ€™21 | $\checkmark$ | âœ— | å¤šè§†å£ $\&amp;$
    2D CNN $\&amp;$ convLSTM | å¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œ |'
- en: 'Recently, there have been several research trends in ODI saliency prediction,
    building on DL progress: (i) From 2D traditional convolutions to 3D specific convolutions;
    (ii) From single feature to multiple features; (iii) From single ERP input to
    multi-type inputs; (iv) From normal CNN-based learning to novel learning strategies.
    In Table.Â [VI](#S3.T6 "TABLE VI â€£ 3.4.1 Saliency Prediction â€£ 3.4 Human Behavior
    Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), numerous DL-based methods have been proposed
    for ODI saliency prediction. In the following, we introduce and analyze some representative
    networks, as shown in Fig.Â [11](#S3.F11 "Figure 11 â€£ 3.4.1 Saliency Prediction
    â€£ 3.4 Human Behavior Understanding â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€è¿‘ï¼Œåœ¨ODIæ˜¾è‘—æ€§é¢„æµ‹æ–¹é¢å‡ºç°äº†å‡ ç§ç ”ç©¶è¶‹åŠ¿ï¼Œè¿™äº›è¶‹åŠ¿å»ºç«‹åœ¨æ·±åº¦å­¦ä¹ è¿›å±•çš„åŸºç¡€ä¸Šï¼šï¼ˆiï¼‰ä»2Dä¼ ç»Ÿå·ç§¯åˆ°3Dç‰¹å®šå·ç§¯ï¼›ï¼ˆiiï¼‰ä»å•ä¸€ç‰¹å¾åˆ°å¤šä¸ªç‰¹å¾ï¼›ï¼ˆiiiï¼‰ä»å•ä¸€ERPè¾“å…¥åˆ°å¤šç±»å‹è¾“å…¥ï¼›ï¼ˆivï¼‰ä»æ™®é€šCNNåŸºç¡€å­¦ä¹ åˆ°æ–°å‹å­¦ä¹ ç­–ç•¥ã€‚åœ¨è¡¨[VI](#S3.T6
    "TABLE VI â€£ 3.4.1 Saliency Prediction â€£ 3.4 Human Behavior Understanding â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")ä¸­ï¼Œæå‡ºäº†ä¼—å¤šåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ç”¨äºODIæ˜¾è‘—æ€§é¢„æµ‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»å’Œåˆ†æä¸€äº›ä»£è¡¨æ€§ç½‘ç»œï¼Œå¦‚å›¾[11](#S3.F11
    "Figure 11 â€£ 3.4.1 Saliency Prediction â€£ 3.4 Human Behavior Understanding â€£ 3
    Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")æ‰€ç¤ºã€‚'
- en: (i) To directly apply 2D deep saliency predictors on ODIs and reduce the unsatisfactory
    distortion in ODIs, many worksÂ [[177](#bib.bib177), [167](#bib.bib167)] convert
    ODIs into 2D projection format. As the first attempt of DNNs on ODI saliency prediction,
    SalNet360Â [[177](#bib.bib177)] subdivides an ERP into a set of six CP patches
    as the input because CP avoids the heavy distortions near the poles like ERP.
    Then SalNet360 combines predicted saliency maps and per-pixel spherical coordinates
    of these patches to output a resulting saliency map in ERP format. Differently,
    a few worksÂ [[178](#bib.bib178), [168](#bib.bib168)] propose the ODI-aware convolution
    filters for saliency prediction, and learn the relationships between the features
    from a non-distorted space. The representative work, SalGCNÂ [[168](#bib.bib168)],
    transfers the ERP image to a spherical graph signal representation, generates
    the spherical graph signal representation of the saliency map and finally reconstructs
    the ERP format saliency map through the spherical crown-based interpolation. SalGFCNÂ [[179](#bib.bib179)]
    proposes a SoTA method that is composed of a residual U-Net architecture based
    on the dilated graph convolutions and attention mechanism in the bottleneck. (ii)
    The viewports are the rectangular windows on ERP with different narrow FoVs caused
    by observersâ€™ head movement. Due to less distortions in viewports, some worksÂ [[180](#bib.bib180),
    [181](#bib.bib181)] choose a set of viewports on ERP as the input and extract
    the multiple independent features from these viewports. The final omnidirectional
    saliency map is generated by a set of viewport saliency maps and refined via an
    equator biased post-processing. Different from most prior multi-feature works
    extracting the low-level geometric features, Mazumdar et al.Â [[180](#bib.bib180)]
    introduced a 2D detector to find important objects first, and this kind of local
    information can improve the performance of the overall saliency map. Recently,
    Chao et al.Â [[169](#bib.bib169)] utilized three different FoVs in each viewport
    to extract rich salient features and better combined the local and global information.
    Furthermore, stretch weighted maps are applied in the loss function to avoid the
    disproportionate impact of stretching in the north and south poles of the ERP
    image.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (i) ä¸ºäº†ç›´æ¥å°† 2D æ·±åº¦æ˜¾è‘—æ€§é¢„æµ‹å™¨åº”ç”¨äºå…¨æ™¯å›¾ï¼ˆODIï¼‰å¹¶å‡å°‘å…¨æ™¯å›¾ä¸­çš„ä¸ç†æƒ³å¤±çœŸï¼Œè®¸å¤šç ”ç©¶ [[177](#bib.bib177), [167](#bib.bib167)]
    å°†å…¨æ™¯å›¾è½¬æ¢ä¸º 2D æŠ•å½±æ ¼å¼ã€‚ä½œä¸º DNNs åœ¨ ODI æ˜¾è‘—æ€§é¢„æµ‹ä¸­çš„é¦–æ¬¡å°è¯•ï¼ŒSalNet360 [[177](#bib.bib177)] å°†ç­‰è·æŠ•å½±ï¼ˆERPï¼‰åˆ’åˆ†ä¸ºä¸€ç»„å…­ä¸ª
    CP è¡¥ä¸ä½œä¸ºè¾“å…¥ï¼Œå› ä¸º CP é¿å…äº†åƒ ERP ä¸€æ ·åœ¨æç‚¹é™„è¿‘çš„ä¸¥é‡å¤±çœŸã€‚ç„¶åï¼ŒSalNet360 ç»“åˆé¢„æµ‹çš„æ˜¾è‘—æ€§å›¾å’Œè¿™äº›è¡¥ä¸çš„æ¯åƒç´ çƒé¢åæ ‡ï¼Œè¾“å‡ºä¸€ä¸ª
    ERP æ ¼å¼çš„ç»“æœæ˜¾è‘—æ€§å›¾ã€‚ä¸åŒçš„æ˜¯ï¼Œä¸€äº›ç ”ç©¶ [[178](#bib.bib178), [168](#bib.bib168)] æå‡ºäº†é’ˆå¯¹å…¨æ™¯å›¾çš„å·ç§¯æ»¤æ³¢å™¨è¿›è¡Œæ˜¾è‘—æ€§é¢„æµ‹ï¼Œå¹¶ä»éå¤±çœŸçš„ç©ºé—´ä¸­å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚ä»£è¡¨æ€§å·¥ä½œ
    SalGCN [[168](#bib.bib168)] å°† ERP å›¾åƒè½¬åŒ–ä¸ºçƒé¢å›¾ä¿¡å·è¡¨ç¤ºï¼Œç”Ÿæˆæ˜¾è‘—æ€§å›¾çš„çƒé¢å›¾ä¿¡å·è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åŸºäºçƒé¢å† çš„æ’å€¼æœ€ç»ˆé‡å»º
    ERP æ ¼å¼çš„æ˜¾è‘—æ€§å›¾ã€‚SalGFCN [[179](#bib.bib179)] æå‡ºäº†ä¸€ä¸ªæœ€å…ˆè¿›çš„ï¼ˆSoTAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±ä¸€ä¸ªåŸºäºæ‰©å¼ å›¾å·ç§¯å’Œç“¶é¢ˆä¸­çš„æ³¨æ„æœºåˆ¶çš„æ®‹å·®
    U-Net æ¶æ„ç»„æˆã€‚ (ii) è§†å£æ˜¯ ERP ä¸Šçš„çŸ©å½¢çª—å£ï¼Œç”±è§‚å¯Ÿè€…å¤´éƒ¨è¿åŠ¨å¼•èµ·ä¸åŒçš„ç‹­çª„è§†åœºï¼ˆFoVï¼‰ã€‚ç”±äºè§†å£çš„å¤±çœŸè¾ƒå°‘ï¼Œä¸€äº›ç ”ç©¶ [[180](#bib.bib180),
    [181](#bib.bib181)] é€‰æ‹©äº†ä¸€ç»„è§†å£ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»è¿™äº›è§†å£ä¸­æå–å¤šä¸ªç‹¬ç«‹çš„ç‰¹å¾ã€‚æœ€ç»ˆçš„å…¨æ™¯æ˜¾è‘—æ€§å›¾ç”±ä¸€ç»„è§†å£æ˜¾è‘—æ€§å›¾ç”Ÿæˆï¼Œå¹¶é€šè¿‡èµ¤é“åç½®çš„åå¤„ç†è¿›è¡Œä¼˜åŒ–ã€‚ä¸åŒäºå¤§å¤šæ•°å…ˆå‰çš„å¤šç‰¹å¾ç ”ç©¶æå–ä½çº§å‡ ä½•ç‰¹å¾ï¼ŒMazumdar
    ç­‰ [[180](#bib.bib180)] å¼•å…¥äº†ä¸€ä¸ª 2D æ£€æµ‹å™¨é¦–å…ˆå¯»æ‰¾é‡è¦å¯¹è±¡ï¼Œè¿™ç§å±€éƒ¨ä¿¡æ¯å¯ä»¥æé«˜æ•´ä½“æ˜¾è‘—æ€§å›¾çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼ŒChao ç­‰ [[169](#bib.bib169)]
    åœ¨æ¯ä¸ªè§†å£ä¸­åˆ©ç”¨äº†ä¸‰ç§ä¸åŒçš„è§†åœºæ¥æå–ä¸°å¯Œçš„æ˜¾è‘—ç‰¹å¾ï¼Œå¹¶æ›´å¥½åœ°ç»“åˆäº†å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åº”ç”¨äº†æ‹‰ä¼¸åŠ æƒå›¾ï¼Œä»¥é¿å… ERP å›¾åƒå—åŒ—ææ‹‰ä¼¸å¸¦æ¥çš„ä¸æˆæ¯”ä¾‹å½±å“ã€‚
- en: '(iii) ODI saliency prediction methods with multi-type inputs focus on the projection
    transformations of the ODIs, which has been mentioned in the SecÂ [2.1](#S2.SS1
    "2.1 Omnidirectional Imaging â€£ 2 Background â€£ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"). These methods aim to utilize the properties
    of different projection formats to achieve the better performance than the single
    ERP inputÂ [[177](#bib.bib177), [182](#bib.bib182), [183](#bib.bib183)]. Due to
    the geometric distortions in the poles of ERP format, Djemai et al.Â [[182](#bib.bib182)]
    introduced a set of CP images, which are projected by five different rotational
    ERP images into the CNN-based approach. However, boundary-distortion and discontinuity
    in CP images cause the lack of global information in the extracted features. To
    address the problem, SalBiNet360Â [[183](#bib.bib183)] simultaneously takes ERP
    and CP images as the input. It constructs a bifurcated network to predict global
    and local saliency maps, respectively. The final saliency output is the fusion
    of the global and local saliency maps. Furthermore, ZhuÂ [[184](#bib.bib184)] provided
    a groundbreaking multi-domain model, which decomposes the ERP image using spherical
    harmonics in the frequency domain and combines frequency components with multiple
    viewports of the ERP images in the spatial domain to extract features.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '(iii) å¤šç±»å‹è¾“å…¥çš„ODIæ˜¾è‘—æ€§é¢„æµ‹æ–¹æ³•ä¸“æ³¨äºODIçš„æŠ•å½±å˜æ¢ï¼Œè¿™ä¸€ç‚¹åœ¨ç¬¬[2.1èŠ‚](#S2.SS1 "2.1 Omnidirectional
    Imaging â€£ 2 Background â€£ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")ä¸­å·²æœ‰æåŠã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨åˆ©ç”¨ä¸åŒæŠ•å½±æ ¼å¼çš„ç‰¹æ€§ï¼Œä»¥å®ç°æ¯”å•ä¸€ERPè¾“å…¥æ›´å¥½çš„æ€§èƒ½[[177](#bib.bib177),
    [182](#bib.bib182), [183](#bib.bib183)]ã€‚ç”±äºERPæ ¼å¼æç‚¹çš„å‡ ä½•å¤±çœŸï¼ŒDjemaiç­‰äºº[[182](#bib.bib182)]å¼•å…¥äº†ä¸€ç»„CPå›¾åƒï¼Œè¿™äº›å›¾åƒé€šè¿‡äº”ç§ä¸åŒçš„æ—‹è½¬ERPå›¾åƒæŠ•å½±åˆ°åŸºäºCNNçš„æ–¹æ³•ä¸­ã€‚ç„¶è€Œï¼ŒCPå›¾åƒä¸­çš„è¾¹ç•Œå¤±çœŸå’Œä¸è¿ç»­æ€§å¯¼è‡´æå–çš„ç‰¹å¾ç¼ºä¹å…¨å±€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSalBiNet360[[183](#bib.bib183)]åŒæ—¶å°†ERPå’ŒCPå›¾åƒä½œä¸ºè¾“å…¥ã€‚å®ƒæ„å»ºäº†ä¸€ä¸ªåˆ†å‰ç½‘ç»œï¼Œåˆ†åˆ«é¢„æµ‹å…¨å±€å’Œå±€éƒ¨æ˜¾è‘—æ€§å›¾ã€‚æœ€ç»ˆçš„æ˜¾è‘—æ€§è¾“å‡ºæ˜¯å…¨å±€å’Œå±€éƒ¨æ˜¾è‘—æ€§å›¾çš„èåˆã€‚æ­¤å¤–ï¼ŒZhu[[184](#bib.bib184)]æä¾›äº†ä¸€ä¸ªå¼€åˆ›æ€§çš„å¤šåŸŸæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é¢‘åŸŸä¸­çš„çƒè°å‡½æ•°å¯¹ERPå›¾åƒè¿›è¡Œåˆ†è§£ï¼Œå¹¶åœ¨ç©ºé—´åŸŸä¸­å°†é¢‘ç‡ç»„ä»¶ä¸ERPå›¾åƒçš„å¤šä¸ªè§†å£ç›¸ç»“åˆä»¥æå–ç‰¹å¾ã€‚'
- en: (iv) As the first to use GAN to predict the saliency maps for ODIs, SalGAN360Â [[185](#bib.bib185)]
    provides a new generator loss, which is designed according to three evaluation
    metrics to fine-tune the SalGANÂ [[186](#bib.bib186)]. SalGAN360 constructs a different
    branch with the Multiple Cubic Projection (MCP) as input to simulate undistorted
    contents. For the attention-based learning on ODI saliency prediction, ZhuÂ et
    al.Â proposed RANSPÂ [[187](#bib.bib187)] and AAFFNÂ [[188](#bib.bib188)]. Both methods
    contain the part-guided attention (PA) module, which is a normalized part confidence
    map that can highlight specific regions in the image. Moreover, an attention-aware
    module is introduced to refine the final saliency map. Especially, RANSP predicts
    the head fixations while AAFFN predicts the eye fixations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) ä½œä¸ºé¦–ä¸ªä½¿ç”¨GANé¢„æµ‹ODIæ˜¾è‘—æ€§å›¾çš„æ–¹æ³•ï¼ŒSalGAN360[[185](#bib.bib185)]æä¾›äº†ä¸€ç§æ–°çš„ç”Ÿæˆå™¨æŸå¤±ï¼Œè¿™ç§æŸå¤±æ˜¯æ ¹æ®ä¸‰ç§è¯„ä¼°æŒ‡æ ‡è®¾è®¡çš„ï¼Œä»¥å¾®è°ƒSalGAN[[186](#bib.bib186)]ã€‚SalGAN360æ„å»ºäº†ä¸€ä¸ªä¸åŒçš„åˆ†æ”¯ï¼Œä»¥å¤šä¸ªç«‹æ–¹ä½“æŠ•å½±ï¼ˆMCPï¼‰ä½œä¸ºè¾“å…¥ï¼Œä»¥æ¨¡æ‹Ÿæ— å¤±çœŸçš„å†…å®¹ã€‚å¯¹äºåŸºäºæ³¨æ„åŠ›çš„ODIæ˜¾è‘—æ€§é¢„æµ‹ï¼ŒZhuç­‰äººæå‡ºäº†RANSP[[187](#bib.bib187)]å’ŒAAFFN[[188](#bib.bib188)]ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½åŒ…å«éƒ¨åˆ†å¼•å¯¼æ³¨æ„åŠ›ï¼ˆPAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§è§„èŒƒåŒ–çš„éƒ¨åˆ†ç½®ä¿¡å›¾ï¼Œå¯ä»¥çªå‡ºå›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ³¨æ„åŠ›æ„ŸçŸ¥æ¨¡å—æ¥ç»†åŒ–æœ€ç»ˆçš„æ˜¾è‘—æ€§å›¾ã€‚ç‰¹åˆ«åœ°ï¼ŒRANSPé¢„æµ‹å¤´éƒ¨æ³¨è§†ï¼Œè€ŒAAFFNé¢„æµ‹çœ¼ç›æ³¨è§†ã€‚
- en: ODV Saliency Prediction For the saliency prediction in ODVs, the key points
    are accurate saliency prediction for each frame and the temporal coherence of
    the viewing process. As videos with dynamic contents are widely used in real applications,
    deep ODV saliency prediction has received more attention in the community. Nguyen
    et al.Â [[172](#bib.bib172)] proposed a representative transfer learning framework
    that shifted a traditional saliency model to a novel saliency model, PanoSalNet,
    which is similar toÂ [[189](#bib.bib189)] andÂ [[177](#bib.bib177)]. By contrast,
    Cheng et al.Â [[43](#bib.bib43)] proposed a spatial-temporal network consisting
    of a static model and a ConvLSTM module. The static model is inspired byÂ [[190](#bib.bib190)]
    and ConvLSTMÂ [[132](#bib.bib132)] is used to aggregate temporal information. They
    also implemented the Cube Padding technique to connect the cube faces by propagating
    the shared information across the views. Similar toÂ [[180](#bib.bib180)], a viewport
    saliency prediction model is proposed inÂ [[176](#bib.bib176)] which first studies
    human attention to detect the desired viewports of the ODV and then predict the
    fixations based on the viewport content. Especially, the proposed Multi-Task Deep
    Neural Network (MT-DNN) model takes both the viewport content and location of
    the viewport as the input and its structure followsÂ [[43](#bib.bib43)] which employs
    a CNN and a ConvLSTM to explore both spatial and temporal features. One more representative
    is proposed byÂ [[173](#bib.bib173)], in which the convolution kernel is defined
    on a spherical crown and the convolution operation corresponds to the rotation
    of kernel on the sphere. Considering the common planar ERP format, Zhang et al.Â [[173](#bib.bib173)]
    re-sampled the kernel based on the position of the sampled patches on ERP. There
    also exist some works based on novel learning strategies. Xu et al.Â [[174](#bib.bib174)]
    developed the saliency prediction network of head movement (HM) based on deep
    reinforcement learning (DRL). The proposed DRL-based head movement prediction
    approach owns offline and online versions. In offline version, multiple DRL workflows
    determines potential HM positions at each panoramic frame and generate a heat
    map of the potential HM positions. In online version, the DRL model will estimate
    the next HM position of one subject according to the currently observed HM position.
    Zhu et al.Â [[175](#bib.bib175)] proposed a graph-based CNN model to estimate the
    fraction of the visual saliency via Markov Chains. The edge weights of the chains
    represent the characteristics of viewing behaviors, and the nodes are feature
    vectors from the spatial-temporal units.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ODVæ˜¾è‘—æ€§é¢„æµ‹ åœ¨ODVçš„æ˜¾è‘—æ€§é¢„æµ‹ä¸­ï¼Œå…³é”®ç‚¹æ˜¯å¯¹æ¯ä¸€å¸§çš„å‡†ç¡®æ˜¾è‘—æ€§é¢„æµ‹ä»¥åŠè§‚çœ‹è¿‡ç¨‹çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ç”±äºåŠ¨æ€å†…å®¹çš„è§†é¢‘åœ¨å®é™…åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæ·±åº¦ODVæ˜¾è‘—æ€§é¢„æµ‹åœ¨å­¦æœ¯ç•Œå—åˆ°äº†æ›´å¤šå…³æ³¨ã€‚Nguyenç­‰äººÂ [[172](#bib.bib172)]
    æå‡ºäº†ä¸€ä¸ªä»£è¡¨æ€§çš„è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œå°†ä¼ ç»Ÿçš„æ˜¾è‘—æ€§æ¨¡å‹è½¬å˜ä¸ºä¸€ç§æ–°é¢–çš„æ˜¾è‘—æ€§æ¨¡å‹PanoSalNetï¼Œè¿™ä¸Â [[189](#bib.bib189)] å’ŒÂ [[177](#bib.bib177)]
    ç›¸ä¼¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒChengç­‰äººÂ [[43](#bib.bib43)] æå‡ºäº†ä¸€ä¸ªç”±é™æ€æ¨¡å‹å’ŒConvLSTMæ¨¡å—ç»„æˆçš„æ—¶ç©ºç½‘ç»œã€‚é™æ€æ¨¡å‹çš„çµæ„Ÿæ¥è‡ªäºÂ [[190](#bib.bib190)]ï¼ŒConvLSTMÂ [[132](#bib.bib132)]
    ç”¨äºèšåˆæ—¶é—´ä¿¡æ¯ã€‚ä»–ä»¬è¿˜å®ç°äº†Cube PaddingæŠ€æœ¯ï¼Œé€šè¿‡åœ¨è§†å›¾é—´ä¼ æ’­å…±äº«ä¿¡æ¯æ¥è¿æ¥ç«‹æ–¹ä½“é¢ã€‚ç±»ä¼¼äºÂ [[180](#bib.bib180)]ï¼Œä¸€ä¸ªè§†å£æ˜¾è‘—æ€§é¢„æµ‹æ¨¡å‹åœ¨Â [[176](#bib.bib176)]
    ä¸­è¢«æå‡ºï¼Œè¯¥æ¨¡å‹é¦–å…ˆç ”ç©¶äººç±»æ³¨æ„åŠ›ä»¥æ£€æµ‹ODVçš„ç›®æ ‡è§†å£ï¼Œç„¶ååŸºäºè§†å£å†…å®¹é¢„æµ‹æ³¨è§†ç‚¹ã€‚ç‰¹åˆ«åœ°ï¼Œæ‰€æå‡ºçš„å¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆMT-DNNï¼‰æ¨¡å‹å°†è§†å£å†…å®¹å’Œè§†å£ä½ç½®ä½œä¸ºè¾“å…¥ï¼Œå…¶ç»“æ„éµå¾ªÂ [[43](#bib.bib43)]ï¼Œé‡‡ç”¨CNNå’ŒConvLSTMæ¥æ¢ç´¢ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚å¦ä¸€ç§å…·æœ‰ä»£è¡¨æ€§çš„æ¨¡å‹ç”±Â [[173](#bib.bib173)]
    æå‡ºï¼Œå…¶ä¸­å·ç§¯æ ¸åœ¨çƒé¢å† ä¸Šå®šä¹‰ï¼Œå·ç§¯æ“ä½œå¯¹åº”äºæ ¸åœ¨çƒé¢ä¸Šçš„æ—‹è½¬ã€‚è€ƒè™‘åˆ°å¸¸è§çš„å¹³é¢ERPæ ¼å¼ï¼ŒZhangç­‰äººÂ [[173](#bib.bib173)] åŸºäºé‡‡æ ·è´´ç‰‡åœ¨ERPä¸Šçš„ä½ç½®é‡æ–°é‡‡æ ·äº†å·ç§¯æ ¸ã€‚è¿˜æœ‰ä¸€äº›åŸºäºæ–°é¢–å­¦ä¹ ç­–ç•¥çš„å·¥ä½œã€‚Xuç­‰äººÂ [[174](#bib.bib174)]
    å¼€å‘äº†åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„å¤´éƒ¨è¿åŠ¨ï¼ˆHMï¼‰æ˜¾è‘—æ€§é¢„æµ‹ç½‘ç»œã€‚æ‰€æå‡ºçš„åŸºäºDRLçš„å¤´éƒ¨è¿åŠ¨é¢„æµ‹æ–¹æ³•æ‹¥æœ‰ç¦»çº¿å’Œåœ¨çº¿ç‰ˆæœ¬ã€‚åœ¨ç¦»çº¿ç‰ˆæœ¬ä¸­ï¼Œå¤šä¸ªDRLå·¥ä½œæµç¨‹ç¡®å®šæ¯ä¸ªå…¨æ™¯å¸§ä¸­çš„æ½œåœ¨HMä½ç½®ï¼Œå¹¶ç”Ÿæˆæ½œåœ¨HMä½ç½®çš„çƒ­å›¾ã€‚åœ¨åœ¨çº¿ç‰ˆæœ¬ä¸­ï¼ŒDRLæ¨¡å‹å°†æ ¹æ®å½“å‰è§‚å¯Ÿåˆ°çš„HMä½ç½®ä¼°è®¡ä¸€ä¸ªä¸»ä½“çš„ä¸‹ä¸€ä¸ªHMä½ç½®ã€‚Zhuç­‰äººÂ [[175](#bib.bib175)]
    æå‡ºäº†ä¸€ä¸ªåŸºäºå›¾çš„CNNæ¨¡å‹ï¼Œé€šè¿‡é©¬å°”å¯å¤«é“¾ä¼°è®¡è§†è§‰æ˜¾è‘—æ€§çš„æ¯”ä¾‹ã€‚é“¾çš„è¾¹æƒä»£è¡¨è§‚çœ‹è¡Œä¸ºçš„ç‰¹å¾ï¼ŒèŠ‚ç‚¹æ˜¯æ¥è‡ªæ—¶ç©ºå•å…ƒçš„ç‰¹å¾å‘é‡ã€‚
- en: '![Refer to caption](img/5502b28c47b216a5f51cbe1452088eaf.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/5502b28c47b216a5f51cbe1452088eaf.png)'
- en: 'Figure 11: Deep saliency prediction methods. (a) Directly using traditional
    2D models on planar projections of ODI. (b) Using specific ODI-aware CNNs to predict
    the omnidirectional saliency maps. (c) Aggregating the saliency maps predicted
    by several viewport images. (d) Combining the predicted saliency maps from different
    projection types. (e) Using attention mechanism.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11ï¼šæ·±åº¦æ˜¾è‘—æ€§é¢„æµ‹æ–¹æ³•ã€‚ï¼ˆaï¼‰åœ¨ODIçš„å¹³é¢æŠ•å½±ä¸Šç›´æ¥ä½¿ç”¨ä¼ ç»Ÿçš„2Dæ¨¡å‹ã€‚ï¼ˆbï¼‰ä½¿ç”¨ç‰¹å®šçš„ODIæ„ŸçŸ¥CNNé¢„æµ‹å…¨æ–¹å‘æ˜¾è‘—æ€§å›¾ã€‚ï¼ˆcï¼‰èšåˆç”±å¤šä¸ªè§†å£å›¾åƒé¢„æµ‹çš„æ˜¾è‘—æ€§å›¾ã€‚ï¼ˆdï¼‰ç»“åˆæ¥è‡ªä¸åŒæŠ•å½±ç±»å‹çš„æ˜¾è‘—æ€§å›¾ã€‚ï¼ˆeï¼‰ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: 3.4.2 Gaze Behavior
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 æ³¨è§†è¡Œä¸º
- en: Gaze following, also called gaze estimation, is related to detecting what people
    in the scene look at and are absorbed in. As normal perspective images are NFoV
    captured, gaze targets are always out of the scene. ODI gaze following is proposed
    to solve this problem because ODIs have a great ability to capture the entire
    viewing surroundings. Previous 3D gaze following methods can directly detect the
    gaze target of a human subject in the sphere space but ignore scene information
    of ODIs, which performs gaze following not well. Gaze360Â [[191](#bib.bib191)]
    collects a large-scale gaze dataset using fish-eye lens rectification to pre-process
    the images. However, due to the distortion caused by the sphere-to-plane projection,
    the gaze target maybe not be in the 2D sightline of the human subject in long-distance
    gaze, which is no longer the same in 2D images. Li et al.Â [[192](#bib.bib192)]
    proposed the first framework for ODI gaze following and also collected the first
    ODI gaze following dataset, called GazeFollow360\. They detected the gaze target
    within a local region and a distant region. For ODI gaze prediction, Xu et al.Â [[193](#bib.bib193)]
    built a large-scale eye-tracking dataset for dynamic 360^âˆ˜ immersive videos and
    gave a detailed analysis of gaze prediction. They utilized the temporal saliency,
    spatial saliency and history gaze path for gaze prediction with a combination
    of CNN and LSTM, which is similar to the architecture proposed byÂ [[194](#bib.bib194)].
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨è§†è·Ÿéšï¼Œä¹Ÿç§°ä¸ºæ³¨è§†ä¼°è®¡ï¼Œæ¶‰åŠæ£€æµ‹åœºæ™¯ä¸­äººä»¬æ³¨è§†çš„å¯¹è±¡å’Œä¸“æ³¨çš„å†…å®¹ã€‚ç”±äºæ­£å¸¸é€è§†å›¾åƒæ˜¯NFoVæ•è·çš„ï¼Œæ³¨è§†ç›®æ ‡æ€»æ˜¯ä½äºåœºæ™¯ä¹‹å¤–ã€‚ODIæ³¨è§†è·Ÿéšè¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºODIå…·æœ‰æ•æ‰æ•´ä¸ªè§†é‡çš„å¼ºå¤§èƒ½åŠ›ã€‚ä¹‹å‰çš„3Dæ³¨è§†è·Ÿéšæ–¹æ³•å¯ä»¥ç›´æ¥æ£€æµ‹çƒä½“ç©ºé—´ä¸­äººç±»ä¸»ä½“çš„æ³¨è§†ç›®æ ‡ï¼Œä½†å¿½ç•¥äº†ODIçš„åœºæ™¯ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨è§†è·Ÿéšæ•ˆæœä¸ä½³ã€‚Gaze360
    [[191](#bib.bib191)]ä½¿ç”¨é±¼çœ¼é•œå¤´æ ¡æ­£æ¥é¢„å¤„ç†å›¾åƒï¼Œæ”¶é›†äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ³¨è§†æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç”±äºçƒä½“åˆ°å¹³é¢æŠ•å½±çš„å¤±çœŸï¼Œé•¿è·ç¦»æ³¨è§†ä¸­çš„æ³¨è§†ç›®æ ‡å¯èƒ½ä¸åœ¨äººçš„äºŒç»´è§†çº¿ä¸­ï¼Œè¿™åœ¨äºŒç»´å›¾åƒä¸­ä¸å†ç›¸åŒã€‚Liç­‰äºº[[192](#bib.bib192)]æå‡ºäº†ç¬¬ä¸€ä¸ªODIæ³¨è§†è·Ÿéšæ¡†æ¶ï¼Œå¹¶æ”¶é›†äº†ç¬¬ä¸€ä¸ªODIæ³¨è§†è·Ÿéšæ•°æ®é›†ï¼Œç§°ä¸ºGazeFollow360\ã€‚ä»–ä»¬åœ¨å±€éƒ¨åŒºåŸŸå’Œè¿œç¨‹åŒºåŸŸæ£€æµ‹æ³¨è§†ç›®æ ‡ã€‚å¯¹äºODIæ³¨è§†é¢„æµ‹ï¼ŒXuç­‰äºº[[193](#bib.bib193)]å»ºç«‹äº†ä¸€ä¸ªç”¨äºåŠ¨æ€360^âˆ˜æ²‰æµ¸å¼è§†é¢‘çš„å¤§è§„æ¨¡çœ¼åŠ¨è¿½è¸ªæ•°æ®é›†ï¼Œå¹¶å¯¹æ³¨è§†é¢„æµ‹è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚ä»–ä»¬åˆ©ç”¨æ—¶é—´æ˜¾è‘—æ€§ã€ç©ºé—´æ˜¾è‘—æ€§å’Œå†å²æ³¨è§†è·¯å¾„è¿›è¡Œæ³¨è§†é¢„æµ‹ï¼Œç»“åˆäº†CNNå’ŒLSTMï¼Œè¿™ä¸[[194](#bib.bib194)]æå‡ºçš„æ¶æ„ç±»ä¼¼ã€‚
- en: 'Challenges and potential: ODI contains richer context information that can
    boost gaze behaviour understanding. However, some challenges remain. First, there
    are few specific gaze following and gaze prediction datasets specific for ODI.
    Data is the â€engineâ€ of DL-based methods, so collecting the quantitative and qualitative
    datasets is necessary. Second, due to the distortion problem in sphere-to-plane
    projection types, future research should consider how to correct this distortion
    via geometric transformation. Finally, both gaze following and gaze prediction
    in ODI need to understand wider scene information compared with normal 2D images.
    The spatial context relation should be further explored.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‘æˆ˜å’Œæ½œåŠ›ï¼šODIåŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯ä»¥æå‡å¯¹æ³¨è§†è¡Œä¸ºçš„ç†è§£ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œé’ˆå¯¹ODIçš„æ³¨è§†è·Ÿéšå’Œæ³¨è§†é¢„æµ‹æ•°æ®é›†å¾ˆå°‘ã€‚æ•°æ®æ˜¯åŸºäºDLçš„æ–¹æ³•çš„â€œå¼•æ“â€ï¼Œå› æ­¤æ”¶é›†å®šé‡å’Œå®šæ€§æ•°æ®é›†æ˜¯å¿…è¦çš„ã€‚å…¶æ¬¡ï¼Œç”±äºçƒä½“åˆ°å¹³é¢æŠ•å½±ç±»å‹ä¸­çš„å¤±çœŸé—®é¢˜ï¼Œæœªæ¥çš„ç ”ç©¶åº”è€ƒè™‘å¦‚ä½•é€šè¿‡å‡ ä½•å˜æ¢çº æ­£è¿™ä¸€å¤±çœŸã€‚æœ€åï¼Œç›¸æ¯”äºæ­£å¸¸çš„äºŒç»´å›¾åƒï¼ŒODIä¸­çš„æ³¨è§†è·Ÿéšå’Œæ³¨è§†é¢„æµ‹éœ€è¦ç†è§£æ›´å¹¿æ³›çš„åœºæ™¯ä¿¡æ¯ã€‚ç©ºé—´ä¸Šä¸‹æ–‡å…³ç³»åº”è¿›ä¸€æ­¥æ¢è®¨ã€‚
- en: 3.4.3 Audio-Visual Scene Understanding
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 è§†å¬åœºæ™¯ç†è§£
- en: 'Because ODVs can provide the observers with an immersive understanding of the
    entire surrounding environments, recent research focuses on audio-visual scene
    understanding on ODVs. Due to its enabling viewers to experience sound in all
    directions, the spatial radio of ODV is an essential cue for full scene awareness.
    As the first work on the omnidirectional spatialization problem, Morgado et al.Â [[195](#bib.bib195)]
    designed a four-block architecture applying self-supervised learning to generate
    the spatial radio, given the mono audio and ODV as the joint inputs. They also
    proposed a representative self-supervised frameworkÂ [[196](#bib.bib196)] for learning
    representations from the audio-visual spatial content of ODVs. InÂ [[197](#bib.bib197)],
    ODIs combined with the multichannel audio signals are applied to localize sound
    source object within the visual observation. The self-supervised training method
    includes two DNN models: one for visual object detection and another for sound
    source estimation. Both DNN models are trained based on variational inference.
    Vasudevan et al.Â [[198](#bib.bib198)] simultaneously achieved an audio task, spatial
    sound super-resolution, and two visual tasks, dense depth prediction, and semantic
    labeling of the scene. They proposed a cross-modal distillation framework, including
    a shared encoder and three task-specific decoders, to transfer knowledge from
    vision to audio. For the audio-visual saliency prediction on ODVs, AVS360Â [[199](#bib.bib199)]
    is the first end-to-end framework with two branches to understand audio and visual
    cues. Especially, AVS360 considers geometric distortion in ODV and extracts the
    spherical representation from the cube map images. Furthermore, as the first user
    behavior analysis for audio-visual content in ODV, Chao et al.Â [[200](#bib.bib200)]
    designed the comparative studies using ODVs with three different audio modalities
    and demonstrated that audio cues can improve the audio-visual attention in ODV.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºODVså¯ä»¥è®©è§‚å¯Ÿè€…å¯¹æ•´ä¸ªå‘¨å›´ç¯å¢ƒæœ‰èº«ä¸´å…¶å¢ƒçš„ç†è§£ï¼Œæœ€è¿‘çš„ç ”ç©¶é‡ç‚¹æ”¾åœ¨ODVsä¸Šçš„éŸ³é¢‘-è§†è§‰åœºæ™¯ç†è§£ä¸Šã€‚ç”±äºå…¶èƒ½å¤Ÿä½¿è§‚ä¼—ä½“éªŒåˆ°æ¥è‡ªå„ä¸ªæ–¹å‘çš„å£°éŸ³ï¼ŒODVçš„ç©ºé—´å£°åœºæ˜¯å…¨é¢åœºæ™¯æ„ŸçŸ¥çš„å…³é”®çº¿ç´¢ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹å…¨æ–¹å‘ç©ºé—´åŒ–é—®é¢˜çš„ç ”ç©¶ï¼ŒMorgadoç­‰äºº[[195](#bib.bib195)]è®¾è®¡äº†ä¸€ç§å››å—æ¶æ„ï¼Œåº”ç”¨è‡ªç›‘ç£å­¦ä¹ æ¥ç”Ÿæˆç©ºé—´å£°åœºï¼Œåˆ©ç”¨å•å£°é“éŸ³é¢‘å’ŒODVä½œä¸ºè”åˆè¾“å…¥ã€‚ä»–ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è‡ªç›‘ç£æ¡†æ¶[[196](#bib.bib196)]ï¼Œç”¨äºä»ODVsçš„éŸ³é¢‘-è§†è§‰ç©ºé—´å†…å®¹ä¸­å­¦ä¹ è¡¨å¾ã€‚åœ¨[[197](#bib.bib197)]ä¸­ï¼ŒODIsä¸å¤šé€šé“éŸ³é¢‘ä¿¡å·ç»“åˆç”¨äºå®šä½è§†è§‰è§‚å¯Ÿä¸­çš„å£°éŸ³æºå¯¹è±¡ã€‚è‡ªç›‘ç£è®­ç»ƒæ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªDNNæ¨¡å‹ï¼šä¸€ä¸ªç”¨äºè§†è§‰å¯¹è±¡æ£€æµ‹ï¼Œå¦ä¸€ä¸ªç”¨äºå£°éŸ³æºä¼°è®¡ã€‚è¿™ä¸¤ä¸ªDNNæ¨¡å‹éƒ½åŸºäºå˜åˆ†æ¨æ–­è¿›è¡Œè®­ç»ƒã€‚Vasudevanç­‰äºº[[198](#bib.bib198)]åŒæ—¶å®Œæˆäº†ä¸€ä¸ªéŸ³é¢‘ä»»åŠ¡â€”â€”ç©ºé—´å£°éŸ³è¶…åˆ†è¾¨ç‡ï¼Œä»¥åŠä¸¤ä¸ªè§†è§‰ä»»åŠ¡â€”â€”å¯†é›†æ·±åº¦é¢„æµ‹å’Œåœºæ™¯è¯­ä¹‰æ ‡æ³¨ã€‚ä»–ä»¬æå‡ºäº†ä¸€ä¸ªè·¨æ¨¡æ€è’¸é¦æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå…±äº«ç¼–ç å™¨å’Œä¸‰ä¸ªä»»åŠ¡ç‰¹å®šè§£ç å™¨ï¼Œä»¥å°†çŸ¥è¯†ä»è§†è§‰è½¬ç§»åˆ°éŸ³é¢‘ã€‚å¯¹äºODVsçš„éŸ³é¢‘-è§†è§‰æ˜¾è‘—æ€§é¢„æµ‹ï¼ŒAVS360[[199](#bib.bib199)]æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå…·æœ‰ä¸¤ä¸ªåˆ†æ”¯ä»¥ç†è§£éŸ³é¢‘å’Œè§†è§‰çº¿ç´¢ã€‚ç‰¹åˆ«æ˜¯ï¼ŒAVS360è€ƒè™‘äº†ODVä¸­çš„å‡ ä½•å¤±çœŸï¼Œå¹¶ä»ç«‹æ–¹ä½“æ˜ å°„å›¾åƒä¸­æå–çƒé¢è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä½œä¸ºå¯¹ODVä¸­éŸ³é¢‘-è§†è§‰å†…å®¹çš„é¦–ä¸ªç”¨æˆ·è¡Œä¸ºåˆ†æï¼ŒChaoç­‰äºº[[200](#bib.bib200)]è®¾è®¡äº†ä½¿ç”¨å…·æœ‰ä¸‰ç§ä¸åŒéŸ³é¢‘æ¨¡å¼çš„ODVsçš„å¯¹æ¯”ç ”ç©¶ï¼Œå¹¶è¯æ˜äº†éŸ³é¢‘çº¿ç´¢å¯ä»¥æ”¹å–„ODVä¸­çš„éŸ³é¢‘-è§†è§‰æ³¨æ„åŠ›ã€‚
- en: 'Discussion: Based on the above analysis, most works in this research domain
    process ERP images as normal 2D images and ignore the inherent distortions. Future
    research may explore how better combine spherical imaging characteristics and
    geometrical information of ODI with the spatial audio cues to provide a more realistic
    audio-visual experience.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºï¼šæ ¹æ®ä¸Šè¿°åˆ†æï¼Œè¯¥ç ”ç©¶é¢†åŸŸçš„å¤§å¤šæ•°å·¥ä½œå°†ERPå›¾åƒå¤„ç†ä¸ºæ™®é€šçš„2Då›¾åƒï¼Œå¹¶å¿½è§†äº†å›ºæœ‰çš„å¤±çœŸã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°ç»“åˆODIçš„çƒé¢æˆåƒç‰¹æ€§å’Œå‡ ä½•ä¿¡æ¯ä¸ç©ºé—´éŸ³é¢‘çº¿ç´¢ï¼Œä»¥æä¾›æ›´çœŸå®çš„éŸ³é¢‘-è§†è§‰ä½“éªŒã€‚
- en: 3.4.4 Visual Question Answering
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 è§†è§‰é—®ç­”
- en: 'Visual question answering (VQA) is a comprehensive and interesting task that
    combines computer vision (CV), natural language processing (NLP), and knowledge
    representation $\&amp;$ reasoning (KR). Wider FoV ODIs and ODVs are more valuable
    and challenging for the VQA research because they can provide stereoscopic spatial
    information similar to the human visual system. VQA 360^âˆ˜, proposed inÂ [[201](#bib.bib201)],
    is the first VQA framework on ODI. It introduces a CP-based model with multi-level
    fusion and attention diffusion to reduce spatial distortion. Meanwhile, the collected
    VQA 360^âˆ˜ dataset provides a benchmark for future developments. Furthermore, Yun
    et al.Â [[6](#bib.bib6)] proposed the first ODV-based VQA work, Pano-AVQA, which
    combines information from three modalities: language, audio, and ODV frames. The
    fused multi-modal representations extracted by a transformer network provide a
    holistic semantic understanding of omnidirectional surroundings. They also provided
    the first spatial and audio-VQA dataset on ODVs.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ˜¯ä¸€ä¸ªç»¼åˆä¸”æœ‰è¶£çš„ä»»åŠ¡ï¼Œç»“åˆäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’ŒçŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†ï¼ˆKRï¼‰ã€‚æ›´å®½çš„è§†åœºODIå’ŒODVå¯¹VQAç ”ç©¶æ›´æœ‰ä»·å€¼å’ŒæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æä¾›ç±»ä¼¼äºäººç±»è§†è§‰ç³»ç»Ÿçš„ç«‹ä½“ç©ºé—´ä¿¡æ¯ã€‚VQA
    360^âˆ˜ï¼Œåœ¨[[201](#bib.bib201)]ä¸­æå‡ºï¼Œæ˜¯ç¬¬ä¸€ä¸ªåŸºäºODIçš„VQAæ¡†æ¶ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªåŸºäºCPçš„æ¨¡å‹ï¼Œé‡‡ç”¨å¤šå±‚èåˆå’Œæ³¨æ„åŠ›æ‰©æ•£æ¥å‡å°‘ç©ºé—´æ‰­æ›²ã€‚åŒæ—¶ï¼Œæ”¶é›†çš„VQA
    360^âˆ˜æ•°æ®é›†ä¸ºæœªæ¥çš„å‘å±•æä¾›äº†åŸºå‡†ã€‚æ­¤å¤–ï¼ŒYunç­‰äºº[[6](#bib.bib6)]æå‡ºäº†ç¬¬ä¸€ä¸ªåŸºäºODVçš„VQAå·¥ä½œï¼ŒPano-AVQAï¼Œå®ƒç»“åˆäº†æ¥è‡ªä¸‰ç§æ¨¡æ€çš„ä¿¡æ¯ï¼šè¯­è¨€ã€éŸ³é¢‘å’ŒODVå¸§ã€‚ç”±å˜å‹å™¨ç½‘ç»œæå–çš„èåˆå¤šæ¨¡æ€è¡¨ç¤ºæä¾›äº†å¯¹å…¨æ–¹ä½ç¯å¢ƒçš„æ•´ä½“è¯­ä¹‰ç†è§£ã€‚ä»–ä»¬è¿˜æä¾›äº†ç¬¬ä¸€ä¸ªå…³äºODVçš„ç©ºé—´å’ŒéŸ³é¢‘-VQAæ•°æ®é›†ã€‚
- en: 'Discussion and Challenges: Based on the above analysis, there exist few works
    for the ODI$/$ODV-based VQA. Compared with the methods in 2D domain, the most
    considerable difficulty is how to leverage the spherical projection types, e.g.,
    icosahedron and tangent images. As more than two dozen datasets and numerous effective
    networksÂ [[202](#bib.bib202)] in the 2D domain have been published, future research
    may consider how to effectively transfer knowledge to learn more robust DNN models
    for omnidirectional vision.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: è®¨è®ºä¸æŒ‘æˆ˜ï¼šåŸºäºä¸Šè¿°åˆ†æï¼Œé’ˆå¯¹ODI/ODVçš„VQAå·¥ä½œè¿˜è¾ƒå°‘ã€‚ä¸2Dé¢†åŸŸçš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ€å¤§çš„å›°éš¾æ˜¯å¦‚ä½•åˆ©ç”¨çƒé¢æŠ•å½±ç±»å‹ï¼Œå¦‚äºŒåé¢ä½“å’Œåˆ‡çº¿å›¾åƒã€‚ç”±äºåœ¨2Dé¢†åŸŸå·²ç»å‘å¸ƒäº†äºŒåå¤šä¸ªæ•°æ®é›†å’Œä¼—å¤šæœ‰æ•ˆç½‘ç»œ[[202](#bib.bib202)]ï¼Œæœªæ¥çš„ç ”ç©¶å¯èƒ½ä¼šè€ƒè™‘å¦‚ä½•æœ‰æ•ˆåœ°è¿ç§»çŸ¥è¯†ï¼Œä»¥å­¦ä¹ æ›´å¼ºå¤§çš„å…¨å‘è§†è§‰DNNæ¨¡å‹ã€‚
- en: 4 Novel Learning Strategies
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 æ–°é¢–çš„å­¦ä¹ ç­–ç•¥
- en: Unsupervised/Semi-supervised Learning. ODI data scarcity problem occurs due
    to the insufficient yet costly panorama annotations. This problem is commonly
    addressed by semi-supervised learning or unsupervised learning that can take advantage
    of abundant unlabeled data to enhance the generalization capacity. For semi-supervised
    learning, Tran et al.[[150](#bib.bib150)] exploited the â€˜Mean-Teacherâ€™ modelÂ [[203](#bib.bib203)]
    for 3D room layout reconstruction by learning from the labeled and unlabeled data
    in the same scenario. For unsupervised learning, Djilali et al.Â [[170](#bib.bib170)]
    proposed the first framework for ODI saliency prediction. It calculates the mutual
    information between different views from multiple scenes and combines contrastive
    learning with unsupervised learning to learn latent representations. Furthermore,
    unsupervised learning can be combined with supervised learning to enhance the
    generalization capacity. Yun et al.Â [[128](#bib.bib128)] proposed to combine self-supervised
    learning with supervised learning for depth estimation, alleviating data scarcity
    and enhancing stability.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£/åŠç›‘ç£å­¦ä¹ ã€‚ODIæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ç”±äºå…¨æ™¯æ ‡æ³¨ä¸è¶³è€Œåˆæ˜‚è´µã€‚è¿™ä¸€é—®é¢˜é€šå¸¸é€šè¿‡åŠç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ æ¥è§£å†³ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºåŠç›‘ç£å­¦ä¹ ï¼ŒTranç­‰äºº[[150](#bib.bib150)]åˆ©ç”¨â€˜Mean-Teacherâ€™æ¨¡å‹[[203](#bib.bib203)]é€šè¿‡åœ¨åŒä¸€åœºæ™¯ä¸­ä»æ ‡è®°å’Œæœªæ ‡è®°çš„æ•°æ®ä¸­å­¦ä¹ æ¥è¿›è¡Œ3Dæˆ¿é—´å¸ƒå±€é‡å»ºã€‚å¯¹äºæ— ç›‘ç£å­¦ä¹ ï¼ŒDjilaliç­‰äºº[[170](#bib.bib170)]æå‡ºäº†ç¬¬ä¸€ä¸ªODIæ˜¾è‘—æ€§é¢„æµ‹æ¡†æ¶ã€‚å®ƒè®¡ç®—æ¥è‡ªå¤šä¸ªåœºæ™¯çš„ä¸åŒè§†å›¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œå¹¶å°†å¯¹æ¯”å­¦ä¹ ä¸æ— ç›‘ç£å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥å­¦ä¹ æ½œåœ¨è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£å­¦ä¹ å¯ä»¥ä¸ç›‘ç£å­¦ä¹ ç»“åˆï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚Yunç­‰äºº[[128](#bib.bib128)]æå‡ºå°†è‡ªç›‘ç£å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ ç»“åˆç”¨äºæ·±åº¦ä¼°è®¡ï¼Œç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜å¹¶å¢å¼ºç¨³å®šæ€§ã€‚
- en: GAN. To decrease the domain divergence between perspective images and ODIs,
    P2PDAÂ [[113](#bib.bib113)] and DENSEPASSÂ [[109](#bib.bib109)] exploit the GAN
    frameworks and design an adversarial loss to facilitate semantic segmentation.
    In image generation, BIPSÂ [[47](#bib.bib47)] proposes a GAN framework to synthesize
    RGB-D indoor panoramas based on the arbitrary configurations of cameras and depth
    sensors.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: GANã€‚ä¸ºäº†å‡å°‘è§†è§’å›¾åƒå’ŒODIä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼ŒP2PDA[[113](#bib.bib113)]å’ŒDENSEPASS[[109](#bib.bib109)]åˆ©ç”¨GANæ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†å¯¹æŠ—æŸå¤±æ¥ä¿ƒè¿›è¯­ä¹‰åˆ†å‰²ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒBIPS[[47](#bib.bib47)]æå‡ºäº†ä¸€ç§GANæ¡†æ¶ï¼Œä»¥åˆæˆåŸºäºä»»æ„æ‘„åƒæœºå’Œæ·±åº¦ä¼ æ„Ÿå™¨é…ç½®çš„RGB-Då®¤å†…å…¨æ™¯å›¾åƒã€‚
- en: Attention Mechanism. For cross-view geo-localization, inÂ [[60](#bib.bib60)],
    ViTÂ [[12](#bib.bib12)] is utilized to remove uninformative image patches and enhance
    the informative image patches to higher resolution. This attention-guided non-uniform
    cropping strategy can save the computational cost, which is reallocated to informative
    patches to improve the performance. The similar strategy is adopted in the unsupervised
    saliency predictionÂ [[170](#bib.bib170)]. InÂ [[170](#bib.bib170)], a self-attention
    model is employed to build spatial relationship between the two input and select
    the sufficiently invariant features.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨è·¨è§†è§’åœ°ç†å®šä½æ–¹é¢ï¼Œ[[60](#bib.bib60)]ä¸­ä½¿ç”¨ViT[[12](#bib.bib12)]æ¥å»é™¤æ— ä¿¡æ¯çš„å›¾åƒå—ï¼Œå¹¶å°†æœ‰ä¿¡æ¯çš„å›¾åƒå—æå‡åˆ°æ›´é«˜åˆ†è¾¨ç‡ã€‚è¿™ç§æ³¨æ„åŠ›å¼•å¯¼çš„éå‡åŒ€è£å‰ªç­–ç•¥å¯ä»¥èŠ‚çœè®¡ç®—æˆæœ¬ï¼Œå°†å…¶é‡æ–°åˆ†é…ç»™æœ‰ä¿¡æ¯çš„å›¾åƒå—ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚ç±»ä¼¼çš„ç­–ç•¥åœ¨æ— ç›‘ç£æ˜¾è‘—æ€§é¢„æµ‹[[170](#bib.bib170)]ä¸­ä¹Ÿè¢«é‡‡ç”¨ã€‚åœ¨[[170](#bib.bib170)]ä¸­ï¼Œé‡‡ç”¨è‡ªæ³¨æ„åŠ›æ¨¡å‹æ¥æ„å»ºä¸¤ä¸ªè¾“å…¥ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œå¹¶é€‰æ‹©è¶³å¤Ÿä¸å˜çš„ç‰¹å¾ã€‚
- en: Transfer Learning. There exist a lot of works to transfer the knowledge learned
    from the source 2D domain to facilitate learning in the ODI domain for numerous
    vision tasks, e.g., semantic segmentationÂ [[115](#bib.bib115)] and depth estimationÂ [[108](#bib.bib108)].
    Designing the deformable CNN or MLP on the pre-trained models from perspective
    images can enhance the model capability for ODIs in numerous tasks, e.g., semantic
    segmentationÂ [[115](#bib.bib115), [108](#bib.bib108), [45](#bib.bib45), [27](#bib.bib27),
    [110](#bib.bib110), [112](#bib.bib112)], video super-resolutionÂ [[86](#bib.bib86)],
    depth estimationÂ [[108](#bib.bib108)], and optical flow estimationÂ [[138](#bib.bib138)].
    However, these methods heavily rely on the handcrafted modules, which lack the
    generalization capability for different scenarios. Unsupervised domain adaptation
    aims to transfer knowledge from the perspective domain to ODI domain by decreasing
    the domain gaps between the perspective images and ODIs. P2PDAÂ [[113](#bib.bib113)]
    and BendingRDÂ [[112](#bib.bib112)] decrease domain gaps between perspective images
    and ODIs to effectively obtain pseudo dense labels for the ODIs. Knowledge distillation
    (KD) is another effective technique that transfers knowledge from a cumbersome
    teacher model to learn a compact student model, while maintaining the studentâ€™s
    performance. However, we find that few works have applied KD for omnidirectional
    vision tasks. In semantic segmentation, ECANetsÂ [[111](#bib.bib111)] performs
    data distillation via diverse panoramas from all around the globe.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬ç§»å­¦ä¹ ã€‚è®¸å¤šç ”ç©¶è‡´åŠ›äºå°†ä»æº2Dé¢†åŸŸå­¦åˆ°çš„çŸ¥è¯†è½¬ç§»åˆ°ODIé¢†åŸŸï¼Œä»¥ä¿ƒè¿›åœ¨ä¼—å¤šè§†è§‰ä»»åŠ¡ä¸­çš„å­¦ä¹ ï¼Œä¾‹å¦‚è¯­ä¹‰åˆ†å‰²[[115](#bib.bib115)]å’Œæ·±åº¦ä¼°è®¡[[108](#bib.bib108)]ã€‚è®¾è®¡åŸºäºè§†è§’å›¾åƒçš„å¯å˜å½¢CNNæˆ–MLPå¯ä»¥å¢å¼ºæ¨¡å‹åœ¨ODIé¢†åŸŸä¸­çš„èƒ½åŠ›ï¼Œåº”ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚è¯­ä¹‰åˆ†å‰²[[115](#bib.bib115),
    [108](#bib.bib108), [45](#bib.bib45), [27](#bib.bib27), [110](#bib.bib110), [112](#bib.bib112)]ã€è§†é¢‘è¶…åˆ†è¾¨ç‡[[86](#bib.bib86)]ã€æ·±åº¦ä¼°è®¡[[108](#bib.bib108)]å’Œå…‰æµä¼°è®¡[[138](#bib.bib138)]ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨¡å—ï¼Œç¼ºä¹å¯¹ä¸åŒåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ—¨åœ¨é€šè¿‡å‡å°‘è§†è§’å›¾åƒå’ŒODIä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œå°†çŸ¥è¯†ä»è§†è§’é¢†åŸŸè½¬ç§»åˆ°ODIé¢†åŸŸã€‚P2PDA[[113](#bib.bib113)]å’ŒBendingRD[[112](#bib.bib112)]å‡å°‘äº†è§†è§’å›¾åƒå’ŒODIä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»è€Œæœ‰æ•ˆåœ°è·å¾—ODIçš„ä¼ªå¯†é›†æ ‡ç­¾ã€‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯å¦ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå°†çŸ¥è¯†ä»å¤æ‚çš„æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå­¦ç”Ÿçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å¾ˆå°‘æœ‰ç ”ç©¶å°†KDåº”ç”¨äºå…¨å‘è§†è§’ä»»åŠ¡ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä¸­ï¼ŒECANets[[111](#bib.bib111)]é€šè¿‡æ¥è‡ªå…¨çƒå„åœ°çš„å¤šæ ·åŒ–å…¨æ™¯å›¾åƒè¿›è¡Œæ•°æ®è’¸é¦ã€‚
- en: Deep Reinforcement Learning (DRL). In saliency prediction,Â [[171](#bib.bib171)]
    predicted the head fixation through DRL by interpreting the trajectories of head
    movements as discrete actions, which are rewarded by correct policies. Besides,
    in object detection, Pais et al.Â [[204](#bib.bib204)] provided the pedestriansâ€™
    positions in the real world by considering the 3D bounding boxes and their corresponding
    distortion projections into the image. Another application for DRL is to select
    up-scaling factors adaptively based on the pixel densityÂ [[85](#bib.bib85)], which
    addresses the unevenly distributed pixel density in the ERP.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ã€‚åœ¨æ˜¾è‘—æ€§é¢„æµ‹ä¸­ï¼Œ[[171](#bib.bib171)] é€šè¿‡å°†å¤´éƒ¨è¿åŠ¨è½¨è¿¹è§£é‡Šä¸ºç¦»æ•£åŠ¨ä½œï¼Œå¹¶æ ¹æ®æ­£ç¡®çš„ç­–ç•¥ç»™äºˆå¥–åŠ±ï¼Œé¢„æµ‹äº†å¤´éƒ¨å›ºå®šä½ç½®ã€‚æ­¤å¤–ï¼Œåœ¨ç›®æ ‡æ£€æµ‹ä¸­ï¼ŒPais
    ç­‰äºº [[204](#bib.bib204)] é€šè¿‡è€ƒè™‘3Dè¾¹ç•Œæ¡†åŠå…¶åœ¨å›¾åƒä¸­çš„å¯¹åº”å¤±çœŸæŠ•å½±ï¼Œæä¾›äº†ç°å®ä¸–ç•Œä¸­çš„è¡Œäººä½ç½®ã€‚DRLçš„å¦ä¸€ä¸ªåº”ç”¨æ˜¯æ ¹æ®åƒç´ å¯†åº¦è‡ªé€‚åº”é€‰æ‹©æ”¾å¤§å› å­[[85](#bib.bib85)]ï¼Œè¿™è§£å†³äº†ERPä¸­åƒç´ å¯†åº¦ä¸å‡çš„é—®é¢˜ã€‚
- en: Multi-task Learning. Sharing representations between the related tasks can increase
    the generalization capacity of the models and improve the performance on all involved
    tasks. MT-DNNÂ [[176](#bib.bib176)] combines the saliency detection task with the
    viewport detection task to predict the viewport saliency map of each frame and
    improves the saliency prediction performance in the ODVs. DeepPanoContextÂ [[147](#bib.bib147)]
    empowers panoramic scene understanding by jointly predicting object shapes, 3D
    poses, semantic categories, and room layout. Similarly, HoHoNetÂ [[153](#bib.bib153)]
    proposes a Latent Horizontal Feature (LHFeat) and a novel horizon-to-dense module
    to accomplish various tasks, including room layout reconstruction and per-pixel
    dense prediction tasks, e.g., depth estimation, semantic segmentation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šä»»åŠ¡å­¦ä¹ ã€‚é€šè¿‡åœ¨ç›¸å…³ä»»åŠ¡ä¹‹é—´å…±äº«è¡¨ç¤ºï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ”¹å–„æ‰€æœ‰ç›¸å…³ä»»åŠ¡çš„è¡¨ç°ã€‚MT-DNNÂ [[176](#bib.bib176)] å°†æ˜¾è‘—æ€§æ£€æµ‹ä»»åŠ¡ä¸è§†å£æ£€æµ‹ä»»åŠ¡ç»“åˆï¼Œä»¥é¢„æµ‹æ¯å¸§çš„è§†å£æ˜¾è‘—æ€§å›¾ï¼Œå¹¶æé«˜äº†ODVsä¸­çš„æ˜¾è‘—æ€§é¢„æµ‹æ€§èƒ½ã€‚DeepPanoContextÂ [[147](#bib.bib147)]
    é€šè¿‡è”åˆé¢„æµ‹ç‰©ä½“å½¢çŠ¶ã€3Då§¿æ€ã€è¯­ä¹‰ç±»åˆ«å’Œæˆ¿é—´å¸ƒå±€ï¼Œå¢å¼ºäº†å…¨æ™¯åœºæ™¯ç†è§£ã€‚ç±»ä¼¼åœ°ï¼ŒHoHoNetÂ [[153](#bib.bib153)] æå‡ºäº†ä¸€ä¸ªæ½œåœ¨æ°´å¹³ç‰¹å¾ï¼ˆLHFeatï¼‰å’Œä¸€ä¸ªæ–°é¢–çš„åœ°å¹³çº¿åˆ°å¯†é›†æ¨¡å—ï¼Œä»¥å®Œæˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æˆ¿é—´å¸ƒå±€é‡å»ºå’Œæ¯åƒç´ å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå¦‚æ·±åº¦ä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²ã€‚
- en: 5 Applications
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åº”ç”¨
- en: AR and VR. With the advancement of techniques and the growing demand of interactive
    scenarios, AR and VR have seen rapid development in recent years. VR aims to simulate
    real or imaginary environments, where a participant can obtain immersive experiences
    and personalized content by perceiving and interacting with the environment. With
    the advantage of capturing the entire surrounding environment with $360^{\circ}\times
    180^{\circ}$ FoV in ODIs, 360 VR/AR facilitates the development of immersive experiences.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å¢å¼ºç°å®ï¼ˆARï¼‰ä¸è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥å’Œäº’åŠ¨åœºæ™¯éœ€æ±‚çš„å¢é•¿ï¼ŒAR å’Œ VR è¿‘å¹´æ¥å‘å±•è¿…é€Ÿã€‚VR æ—¨åœ¨æ¨¡æ‹ŸçœŸå®æˆ–è™šæ„çš„ç¯å¢ƒï¼Œå‚ä¸è€…å¯ä»¥é€šè¿‡æ„ŸçŸ¥å’Œäº’åŠ¨è·å¾—æ²‰æµ¸å¼ä½“éªŒå’Œä¸ªæ€§åŒ–å†…å®¹ã€‚åˆ©ç”¨åœ¨ODIsä¸­æ•æ‰æ•´ä¸ªç¯å¢ƒçš„$360^{\circ}\times
    180^{\circ}$è§†åœºçš„ä¼˜åŠ¿ï¼Œ360 VR/AR ä¿ƒè¿›äº†æ²‰æµ¸å¼ä½“éªŒçš„å‘å±•ã€‚
- en: '[[205](#bib.bib205)] gives a detailed SWOT (namely strengths, weaknesses, opportunities,
    and threats) analysis of 360 VR to make sure that it is suitable to leverage the
    360 VR to develop athletesâ€™ decision-making skills. Understanding human behaviors
    is crucial for the application of 360 VR. [[194](#bib.bib194)] proposed a preference-aware
    framework for viewport prediction, and [[193](#bib.bib193)] combined the history
    scan path with image contents for gaze prediction. In addition, to enhance the
    immersive experience, Kim et al.Â [[206](#bib.bib206)] proposed a novel pipeline
    to estimate room acoustic for plausible reproduction of spatial audio with $360^{\circ}$
    cameras. Importantly, acquiring 3D data is strongly desired in VR/AR to provide
    the sense of 3D. However, consumer-level depth sensors can only capture perspective
    depth maps, and panoramic depths need time-consuming stitching technologies. Therefore,
    monocular depth estimation techniques, e.g., OmniDepthÂ [[122](#bib.bib122)] and
    UniFuseÂ [[133](#bib.bib133)], are promising for VR/AR.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[[205](#bib.bib205)] å¯¹ 360 VR è¿›è¡Œäº†è¯¦ç»†çš„ SWOTï¼ˆå³ä¼˜åŠ¿ã€åŠ£åŠ¿ã€æœºä¼šå’Œå¨èƒï¼‰åˆ†æï¼Œä»¥ç¡®ä¿å®ƒé€‚åˆç”¨äºæé«˜è¿åŠ¨å‘˜çš„å†³ç­–èƒ½åŠ›ã€‚ç†è§£äººç±»è¡Œä¸ºå¯¹
    360 VR çš„åº”ç”¨è‡³å…³é‡è¦ã€‚[[194](#bib.bib194)] æå‡ºäº†ä¸€ä¸ªè€ƒè™‘åå¥½çš„è§†å£é¢„æµ‹æ¡†æ¶ï¼Œè€Œ [[193](#bib.bib193)] å°†å†å²æ‰«æè·¯å¾„ä¸å›¾åƒå†…å®¹ç»“åˆè¿›è¡Œå‡è§†é¢„æµ‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºæ²‰æµ¸ä½“éªŒï¼ŒKim
    ç­‰äºº [[206](#bib.bib206)] æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç®¡é“ï¼Œç”¨äºä¼°è®¡æˆ¿é—´å£°å­¦ï¼Œä»¥ä¾¿é€šè¿‡ $360^{\circ}$ æ‘„åƒæœºé€¼çœŸåœ°å†ç°ç©ºé—´éŸ³é¢‘ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨
    VR/AR ä¸­å¼ºçƒˆéœ€è¦è·å– 3D æ•°æ®ä»¥æä¾› 3D ç«‹ä½“æ„Ÿã€‚ç„¶è€Œï¼Œæ¶ˆè´¹çº§æ·±åº¦ä¼ æ„Ÿå™¨åªèƒ½æ•æ‰é€è§†æ·±åº¦å›¾ï¼Œè€Œå…¨æ™¯æ·±åº¦å›¾åˆ™éœ€è¦è€—æ—¶çš„æ‹¼æ¥æŠ€æœ¯ã€‚å› æ­¤ï¼Œå•ç›®æ·±åº¦ä¼°è®¡æŠ€æœ¯ï¼Œå¦‚
    OmniDepth [[122](#bib.bib122)] å’Œ UniFuse [[133](#bib.bib133)]ï¼Œåœ¨ VR/AR ä¸­å±•ç°äº†å¾ˆå¤§çš„å‰æ™¯ã€‚'
- en: 'Robot Navigation. In addition to SLAM mentioned in Sec.Â [3.3.3](#S3.SS3.SSS3
    "3.3.3 SLAM â€£ 3.3 3D Vision â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives"), we further discuss the
    related applications of ODI/ODV in the field of robot navigation, including the
    telepresence system, surveillance, and DL-based optimization methods.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœºå™¨äººå¯¼èˆªã€‚é™¤äº†ç¬¬ [3.3.3](#S3.SS3.SSS3 "3.3.3 SLAM â€£ 3.3 3D Vision â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    èŠ‚æåˆ°çš„ SLAMï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº† ODI/ODV åœ¨æœºå™¨äººå¯¼èˆªé¢†åŸŸçš„ç›¸å…³åº”ç”¨ï¼ŒåŒ…æ‹¬è¿œç¨‹å‘ˆç°ç³»ç»Ÿã€ç›‘æ§å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ã€‚'
- en: The telepresence system aims to overcome the space constraints to enable people
    to remotely visit and interact with each other. ODI/ODV is gaining popularity
    by providing a more realistic and natural scene, especially in outdoor activities
    with open environmentsÂ [[207](#bib.bib207)].Â [[208](#bib.bib208)] proposed a prototype
    of an ODV-based telepresence system to support more natural interactions and the
    remote environment exploration, where real walking in the remote environment can
    simultaneously control the relevant movement of the robot platform. Surveillance
    aims to replace humans for security purposes, in which the calibration is vital
    for sensitive data. Accordingly, Pudics et al.Â [[209](#bib.bib209)] proposed a
    safe navigation system tailored for obstacle detection and avoidance with a calibration
    design to obtain the proper distance and direction. Compared with NFoV images,
    panoramic images can reduce the computational cost significantly by providing
    complete FoV in a single shot. Moreover, Ran et al.Â [[210](#bib.bib210)] proposed
    a lightweight framework based on the uncalibrated $360^{\circ}$ cameras. The framework
    can accurately estimate the heading direction by formulating it into a series
    of classification tasks and avoid redundant computation by saving the calibration
    and correction processes. To address dark environments, e.g., underground mine,
    Mansouri et al.Â [[211](#bib.bib211)] presented another DNN model by utilizing
    online heading rate commands to avoid the collision in the tunnels and calculating
    depth information online within the scene.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è¿œç¨‹å‘ˆç°ç³»ç»Ÿæ—¨åœ¨å…‹æœç©ºé—´é™åˆ¶ï¼Œä½¿äººä»¬èƒ½å¤Ÿè¿œç¨‹è®¿é—®å’Œäº’åŠ¨ã€‚ODI/ODVå› æä¾›æ›´çœŸå®å’Œè‡ªç„¶çš„åœºæ™¯è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„æˆ·å¤–æ´»åŠ¨ä¸­[[207](#bib.bib207)]ã€‚[[208](#bib.bib208)]
    æå‡ºäº†ä¸€ä¸ªåŸºäºODVçš„è¿œç¨‹å‘ˆç°ç³»ç»ŸåŸå‹ï¼Œä»¥æ”¯æŒæ›´è‡ªç„¶çš„äº’åŠ¨å’Œè¿œç¨‹ç¯å¢ƒæ¢ç´¢ï¼Œå…¶ä¸­åœ¨è¿œç¨‹ç¯å¢ƒä¸­çš„çœŸå®æ­¥è¡Œå¯ä»¥åŒæ—¶æ§åˆ¶æœºå™¨äººå¹³å°çš„ç›¸å…³è¿åŠ¨ã€‚ç›‘æ§æ—¨åœ¨æ›¿ä»£äººå·¥ä»¥å®ç°å®‰å…¨ç›®çš„ï¼Œå…¶ä¸­æ ¡å‡†å¯¹æ•æ„Ÿæ•°æ®è‡³å…³é‡è¦ã€‚å› æ­¤ï¼ŒPudicsç­‰äºº[[209](#bib.bib209)]
    æå‡ºäº†ä¸€ä¸ªå®‰å…¨å¯¼èˆªç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºéšœç¢ç‰©æ£€æµ‹å’Œè§„é¿ï¼Œé…å¤‡äº†æ ¡å‡†è®¾è®¡ä»¥è·å¾—é€‚å½“çš„è·ç¦»å’Œæ–¹å‘ã€‚ä¸NFoVå›¾åƒç›¸æ¯”ï¼Œå…¨æ™¯å›¾åƒé€šè¿‡åœ¨ä¸€æ¬¡æ‹æ‘„ä¸­æä¾›å®Œæ•´çš„è§†é‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒRanç­‰äºº[[210](#bib.bib210)]
    æå‡ºäº†ä¸€ä¸ªåŸºäºæœªæ ¡å‡†çš„ $360^{\circ}$ æ‘„åƒå¤´çš„è½»é‡çº§æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å…¶å…¬å¼åŒ–ä¸ºä¸€ç³»åˆ—åˆ†ç±»ä»»åŠ¡æ¥å‡†ç¡®ä¼°è®¡èˆªå‘æ–¹å‘ï¼Œå¹¶é€šè¿‡èŠ‚çœæ ¡å‡†å’Œä¿®æ­£è¿‡ç¨‹æ¥é¿å…å†—ä½™è®¡ç®—ã€‚ä¸ºäº†åº”å¯¹é»‘æš—ç¯å¢ƒï¼Œä¾‹å¦‚åœ°ä¸‹çŸ¿äº•ï¼ŒMansouriç­‰äºº[[211](#bib.bib211)]
    æå‡ºäº†å¦ä¸€ä¸ªDNNæ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨åœ¨çº¿èˆªå‘ç‡å‘½ä»¤æ¥é¿å…éš§é“ä¸­çš„ç¢°æ’ï¼Œå¹¶åœ¨çº¿è®¡ç®—åœºæ™¯ä¸­çš„æ·±åº¦ä¿¡æ¯ã€‚
- en: 'Autonomous Driving. It requires a full understanding of the surrounding environment,
    which omnidirectional vision excels at. Some works focus on setting up $360^{\circ}$
    platform for autonomous drivingÂ [[212](#bib.bib212), [213](#bib.bib213)]. Specifically,
    [[212](#bib.bib212)] utilized a stereo camera, a polarization camera and a panoramic
    camera to form a multi-modal visual system to capture omnidirectional landscape.Â [[213](#bib.bib213)]
    introduced a multi-modal 360^âˆ˜ perception proposal based on visual and LiDAR scanners
    for 3D object detection and tracking. In addition to the platform, the emergence
    of public omnidirectional datasets for autonomous driving are crucial for the
    application of DL methods. Caeser et al.Â [[214](#bib.bib214)] were the first to
    introduce the relevant dataset which carries six cameras, five radars and one
    LiDAR. All devices are with $360^{\circ}$ FoV. Recently, OpenMP datasetÂ [[215](#bib.bib215)]
    is captured by six cameras and four LiDARs, which contains scenes in the complex
    environment, e.g., urban areas with overexposure or darkness. Kumar et al.[[216](#bib.bib216)]
    presented a multi-task visual perception network, which consists of six vital
    tasks in autonomous driving: depth estimation, visual odometry, senmantic segmentation,
    motion segmentation, object detection and lens soiling detection. Importantly,
    as real-time performance is crucial for autonomous driving and embedding systems
    in vehicles often have limited memory and computational resources, lightweight
    DNN models are more favored in practice.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨é©¾é©¶ã€‚å®ƒéœ€è¦å¯¹å‘¨å›´ç¯å¢ƒæœ‰å…¨é¢çš„äº†è§£ï¼Œè€Œå…¨æ™¯è§†è§‰åœ¨è¿™æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚ä¸€äº›ç ”ç©¶é›†ä¸­åœ¨ä¸ºè‡ªåŠ¨é©¾é©¶è®¾ç½® $360^{\circ}$ å¹³å°[[212](#bib.bib212),
    [213](#bib.bib213)]ã€‚å…·ä½“è€Œè¨€ï¼Œ[[212](#bib.bib212)] ä½¿ç”¨äº†ç«‹ä½“æ‘„åƒå¤´ã€åæŒ¯æ‘„åƒå¤´å’Œå…¨æ™¯æ‘„åƒå¤´æ¥å½¢æˆä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰ç³»ç»Ÿï¼Œä»¥æ•æ‰å…¨æ™¯æ™¯è§‚ã€‚[[213](#bib.bib213)]
    æå‡ºäº†åŸºäºè§†è§‰å’Œæ¿€å…‰é›·è¾¾æ‰«æä»ªçš„å¤šæ¨¡æ€ $360^{\circ}$ æ„ŸçŸ¥æ–¹æ¡ˆï¼Œç”¨äº 3D ç‰©ä½“æ£€æµ‹å’Œè·Ÿè¸ªã€‚é™¤äº†å¹³å°ä¹‹å¤–ï¼Œå…¬å…±å…¨æ™¯æ•°æ®é›†çš„å‡ºç°å¯¹äºè‡ªåŠ¨é©¾é©¶çš„
    DL æ–¹æ³•åº”ç”¨è‡³å…³é‡è¦ã€‚Caeser ç­‰äºº[[214](#bib.bib214)] é¦–æ¬¡å¼•å…¥äº†ç›¸å…³æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å…­ä¸ªæ‘„åƒå¤´ã€äº”ä¸ªé›·è¾¾å’Œä¸€ä¸ªæ¿€å…‰é›·è¾¾ã€‚æ‰€æœ‰è®¾å¤‡å‡å…·æœ‰
    $360^{\circ}$ çš„è§†åœºã€‚æœ€è¿‘ï¼ŒOpenMP æ•°æ®é›†[[215](#bib.bib215)] ç”±å…­ä¸ªæ‘„åƒå¤´å’Œå››ä¸ªæ¿€å…‰é›·è¾¾æ•æ‰ï¼ŒåŒ…å«å¤æ‚ç¯å¢ƒä¸­çš„åœºæ™¯ï¼Œä¾‹å¦‚è¿‡æ›æˆ–é»‘æš—çš„åŸå¸‚åŒºåŸŸã€‚Kumar
    ç­‰äºº[[216](#bib.bib216)] æå‡ºäº†ä¸€ä¸ªå¤šä»»åŠ¡è§†è§‰æ„ŸçŸ¥ç½‘ç»œï¼ŒåŒ…å«è‡ªåŠ¨é©¾é©¶ä¸­çš„å…­ä¸ªé‡è¦ä»»åŠ¡ï¼šæ·±åº¦ä¼°è®¡ã€è§†è§‰é‡Œç¨‹è®¡ã€è¯­ä¹‰åˆ†å‰²ã€è¿åŠ¨åˆ†å‰²ã€ç‰©ä½“æ£€æµ‹å’Œé•œå¤´æ±¡æŸ“æ£€æµ‹ã€‚é‡è¦çš„æ˜¯ï¼Œç”±äºå®æ—¶æ€§èƒ½å¯¹è‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ï¼Œè€Œè½¦è¾†ä¸­çš„åµŒå…¥å¼ç³»ç»Ÿé€šå¸¸å…·æœ‰æœ‰é™çš„å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œå› æ­¤è½»é‡çº§
    DNN æ¨¡å‹åœ¨å®è·µä¸­æ›´å—é’çã€‚
- en: 6 Discussion and New Perspectives
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 è®¨è®ºä¸æ–°è§†è§’
- en: Cons of Projection Formats. ERP is the most prevalent projection format due
    to its wide FoV in a planar format. The main challenge for ERP is the increasing
    stretching distortion towards poles. Therefore, many works were proposed to design
    specific convolution filters against the distortionÂ [[21](#bib.bib21), [20](#bib.bib20)].
    By contrast, CP and tangent images are distortion-less projection formats by projecting
    a spherical surface into multiple planes. They are similar to the perspective
    images, and therefore can make full use of many pre-trained models and datasets
    in the planar domainÂ [[25](#bib.bib25)]. However, CP and tangent images suffer
    from the challenges of higher computational cost, discrepancy and discontinuity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ•å½±æ ¼å¼çš„ç¼ºç‚¹ã€‚ERP æ˜¯æœ€æ™®éçš„æŠ•å½±æ ¼å¼ï¼Œå› ä¸ºå®ƒåœ¨å¹³é¢æ ¼å¼ä¸­å…·æœ‰å¹¿æ³›çš„è§†åœºã€‚ERP çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å‘æç‚¹çš„æ‹‰ä¼¸ç•¸å˜ä¸æ–­å¢åŠ ã€‚å› æ­¤ï¼Œè®¸å¤šç ”ç©¶æå‡ºäº†é’ˆå¯¹ç•¸å˜è®¾è®¡çš„ç‰¹å®šå·ç§¯æ»¤æ³¢å™¨[[21](#bib.bib21),
    [20](#bib.bib20)]ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCP å’Œåˆ‡çº¿å›¾åƒæ˜¯æ— ç•¸å˜çš„æŠ•å½±æ ¼å¼ï¼Œé€šè¿‡å°†çƒé¢æŠ•å½±åˆ°å¤šä¸ªå¹³é¢ä¸Šæ¥å®ç°ã€‚å®ƒä»¬ç±»ä¼¼äºé€è§†å›¾åƒï¼Œå› æ­¤å¯ä»¥å……åˆ†åˆ©ç”¨è®¸å¤šé¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†[[25](#bib.bib25)]ã€‚ç„¶è€Œï¼ŒCP
    å’Œåˆ‡çº¿å›¾åƒé¢ä¸´æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€å·®å¼‚æ€§å’Œä¸è¿ç»­æ€§çš„é—®é¢˜ã€‚
- en: 'We summarize two potential directions for utilizing CP and tangent images:
    (i) Redundant computational cost are resulted from large overlapping regions between
    projection planes. However, the pixel density varies among different sampling
    positions. The computation can be more efficient through allocating more resources
    for dense regions (e.g., equator) and less resources for sparse regions (e.g.,
    poles) with reinforcement learningÂ [[85](#bib.bib85)]. (ii) Currently, different
    projection planes are often processed in parallel, which lacks the global consistency.
    To overcome the discrepancy among different local planes, it is effective to explore
    an additional branch with ERP as the inputÂ [[19](#bib.bib19)] or attention-based
    transformers to construct non-local dependenciesÂ [[25](#bib.bib25)]. However,
    these constraints are mainly added to the feature maps, instead of the predictions.
    Moreover, the discrepancy can be also solved from the distribution consistency
    of predictions, e.g., the consistent depth range among different planes and the
    consistent uncertainty scores for the same edges and large gradient regions.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»ç»“äº†åˆ©ç”¨CPå’Œåˆ‡çº¿å›¾åƒçš„ä¸¤ä¸ªæ½œåœ¨æ–¹å‘ï¼šï¼ˆiï¼‰æŠ•å½±å¹³é¢ä¹‹é—´çš„å¤§é¢ç§¯é‡å åŒºåŸŸä¼šå¯¼è‡´å†—ä½™è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œåƒç´ å¯†åº¦åœ¨ä¸åŒé‡‡æ ·ä½ç½®ä¹‹é—´å˜åŒ–ã€‚é€šè¿‡å°†æ›´å¤šèµ„æºåˆ†é…ç»™å¯†é›†åŒºåŸŸï¼ˆä¾‹å¦‚èµ¤é“ï¼‰å’Œå°†è¾ƒå°‘èµ„æºåˆ†é…ç»™ç¨€ç–åŒºåŸŸï¼ˆä¾‹å¦‚æç‚¹ï¼‰ï¼Œè®¡ç®—å¯ä»¥æ›´åŠ é«˜æ•ˆï¼Œå‚è€ƒ[[85](#bib.bib85)]ã€‚ï¼ˆiiï¼‰ç›®å‰ï¼Œä¸åŒçš„æŠ•å½±å¹³é¢é€šå¸¸æ˜¯å¹¶è¡Œå¤„ç†çš„ï¼Œè¿™ç¼ºä¹å…¨å±€ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³ä¸åŒå±€éƒ¨å¹³é¢ä¹‹é—´çš„å·®å¼‚ï¼Œæ¢ç´¢ä»¥ERPä½œä¸ºè¾“å…¥çš„é¢å¤–åˆ†æ”¯[[19](#bib.bib19)]æˆ–åŸºäºæ³¨æ„åŠ›çš„å˜æ¢å™¨æ¥æ„å»ºéå±€éƒ¨ä¾èµ–å…³ç³»[[25](#bib.bib25)]æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè¿™äº›çº¦æŸä¸»è¦æ·»åŠ åˆ°ç‰¹å¾å›¾ä¸Šï¼Œè€Œä¸æ˜¯é¢„æµ‹ä¸Šã€‚æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥é€šè¿‡é¢„æµ‹åˆ†å¸ƒçš„ä¸€è‡´æ€§æ¥è§£å†³è¿™äº›å·®å¼‚ï¼Œä¾‹å¦‚ä¸åŒå¹³é¢ä¹‹é—´çš„ä¸€è‡´æ·±åº¦èŒƒå›´ä»¥åŠç›¸åŒè¾¹ç¼˜å’Œå¤§æ¢¯åº¦åŒºåŸŸçš„ä¸€è‡´ä¸ç¡®å®šæ€§è¯„åˆ†ã€‚
- en: Data-efficient Learning. A challenge for DL methods is the need for large-scale
    datasets with high-quality annotations. However, for omnidirectional vision, constructing
    large-scale datasets is expensive and tedious. Therefore, it is necessary to explore
    more data-efficient methods. One promising direction is to transfer the knowledge
    learned from models trained on the labeled 2D dataset to models to be trained
    on the unlabeled panoramic dataset. Specifically, domain adaptation approaches
    can be applied to narrow the gap between perspective images and ODIsÂ [[109](#bib.bib109)].
    KD is also an effective solution by transferring learned feature information from
    a cumbersome perspective DNN model to a compact DNN model learning ODI dataÂ [[111](#bib.bib111)].
    Finally, recent self-supervised methods, e.g.,Â [[217](#bib.bib217)], demonstrate
    the effectiveness of pre-training without the need of additional training annotations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é«˜æ•ˆå­¦ä¹ ã€‚DLæ–¹æ³•çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯éœ€è¦å¤§è§„æ¨¡é«˜è´¨é‡æ³¨é‡Šçš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå¯¹äºå…¨æ™¯è§†è§‰ï¼Œæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†æ—¢æ˜‚è´µåˆç¹çã€‚å› æ­¤ï¼Œæœ‰å¿…è¦æ¢ç´¢æ›´å¤šçš„æ•°æ®é«˜æ•ˆæ–¹æ³•ã€‚ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘æ˜¯å°†ä»æ ‡è®°2Dæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸­å­¦åˆ°çš„çŸ¥è¯†è½¬ç§»åˆ°å°†è¦åœ¨æœªæ ‡è®°å…¨æ™¯æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥åº”ç”¨é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•æ¥ç¼©å°é€è§†å›¾åƒå’ŒODIä¹‹é—´çš„å·®è·[[109](#bib.bib109)]ã€‚é€šè¿‡å°†ä»ç¹é‡çš„é€è§†DNNæ¨¡å‹ä¸­å­¦ä¹ åˆ°çš„ç‰¹å¾ä¿¡æ¯è½¬ç§»åˆ°å­¦ä¹ ODIæ•°æ®çš„ç´§å‡‘DNNæ¨¡å‹ä¸­ï¼ŒKDä¹Ÿæ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆ[[111](#bib.bib111)]ã€‚æœ€åï¼Œæœ€è¿‘çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œä¾‹å¦‚[[217](#bib.bib217)]ï¼Œå±•ç¤ºäº†æ— éœ€é¢å¤–è®­ç»ƒæ³¨é‡Šçš„é¢„è®­ç»ƒæ•ˆæœã€‚
- en: Physical Constraint. Existing methods for the perspective images are limited
    in inferring the lighting of the global scene and unseen regions. Owing to the
    wide FoV of ODIs, complete surrounding environment scenes can be captured. Furthermore,
    the reflectance can be revealed according to the physical constraints between
    the lighting and scene structure based onÂ [[218](#bib.bib218)]. Therefore, a future
    direction can be jointly leveraging computer graphics, like ray tracing, and rendering
    models to help calculate reflectance, which, in turn, contributes to higher-precision
    global lighting estimation. Additionally, it is promising to process and render
    ODIs based on the lighting transportation theory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†çº¦æŸã€‚ç°æœ‰çš„é€è§†å›¾åƒæ–¹æ³•åœ¨æ¨æ–­å…¨å±€åœºæ™¯å’Œæœªè§åŒºåŸŸçš„å…‰ç…§æ–¹é¢å­˜åœ¨å±€é™ã€‚ç”±äºODIçš„å¹¿é˜”è§†åœºï¼Œèƒ½å¤Ÿæ•æ‰åˆ°å®Œæ•´çš„å‘¨å›´ç¯å¢ƒåœºæ™¯ã€‚æ­¤å¤–ï¼Œå¯ä»¥æ ¹æ®å…‰ç…§å’Œåœºæ™¯ç»“æ„ä¹‹é—´çš„ç‰©ç†çº¦æŸæ¥æ­ç¤ºåå°„ç‡ï¼Œå‚è€ƒ[[218](#bib.bib218)]ã€‚å› æ­¤ï¼Œæœªæ¥çš„æ–¹å‘å¯ä»¥æ˜¯å…±åŒåˆ©ç”¨è®¡ç®—æœºå›¾å½¢å­¦ï¼Œå¦‚å…‰çº¿è¿½è¸ªå’Œæ¸²æŸ“æ¨¡å‹ï¼Œæ¥å¸®åŠ©è®¡ç®—åå°„ç‡ï¼Œä»è€Œæœ‰åŠ©äºæ›´é«˜ç²¾åº¦çš„å…¨å±€å…‰ç…§ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒåŸºäºå…‰ç…§ä¼ è¾“ç†è®ºå¤„ç†å’Œæ¸²æŸ“ODIä¹Ÿå¾ˆæœ‰å‰æ™¯ã€‚
- en: Multi-modal Omnidirectional Vision. It refers to the process of learning representations
    from different types of modalities (e.g., text-image for visual question answering,
    audio-visual scene understanding) using the same DNN model. This is a promising
    yet practical direction for ominidirectional vision. For instance, [[213](#bib.bib213)]
    introduces a multi-modal perception framework based on the visual and LiDAR information
    for 3D object detection and tracking. However, existing works in this direction
    treat ODIs as the perspective images and ignore the inherent distortion in the
    ODIs. Future works may explore how to utilize the advantage of ODIs, e.g., complete
    FoV, to assist the representation of other modalities. Importantly, the acquisition
    of different modalities has obvious discrepancies. For example, capturing RGB
    images is much easier than that of depth maps. Therefore, a promising direction
    is to extract available information from one modality and then transfer to another
    modality via multi-task learning, KD, etc. However, the discrepancy among different
    modalities should be considered to ensure multi-modal consistency.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€å…¨å‘è§†è§‰ã€‚è¿™æŒ‡çš„æ˜¯ä½¿ç”¨ç›¸åŒçš„DNNæ¨¡å‹ä»ä¸åŒç±»å‹çš„æ¨¡æ€ï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬-å›¾åƒç”¨äºè§†è§‰é—®ç­”ï¼ŒéŸ³é¢‘-è§†è§‰åœºæ™¯ç†è§£ï¼‰ä¸­å­¦ä¹ è¡¨ç¤ºã€‚è¿™æ˜¯å…¨å‘è§†è§‰çš„ä¸€ä¸ªæœ‰å‰æ™¯ä½†å®é™…çš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œ[[213](#bib.bib213)]
    ä»‹ç»äº†ä¸€ä¸ªåŸºäºè§†è§‰å’ŒLiDARä¿¡æ¯çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äº3Dç‰©ä½“æ£€æµ‹å’Œè·Ÿè¸ªã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å°†ODIè§†ä¸ºé€è§†å›¾åƒï¼Œå¹¶å¿½ç•¥äº†ODIä¸­çš„å›ºæœ‰å¤±çœŸã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨ODIçš„ä¼˜åŠ¿ï¼Œä¾‹å¦‚ï¼Œå®Œæ•´çš„è§†åœºï¼ˆFoVï¼‰ï¼Œæ¥è¾…åŠ©å…¶ä»–æ¨¡æ€çš„è¡¨ç¤ºã€‚é‡è¦çš„æ˜¯ï¼Œä¸åŒæ¨¡æ€çš„è·å–å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œæ•æ‰RGBå›¾åƒè¦æ¯”æ•æ‰æ·±åº¦å›¾æ›´å®¹æ˜“ã€‚å› æ­¤ï¼Œä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘æ˜¯ä»ä¸€ç§æ¨¡æ€ä¸­æå–å¯ç”¨ä¿¡æ¯ï¼Œç„¶åé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ã€çŸ¥è¯†è’¸é¦ç­‰æ–¹å¼è½¬ç§»åˆ°å¦ä¸€ç§æ¨¡æ€ã€‚ç„¶è€Œï¼Œåº”è€ƒè™‘ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ç¡®ä¿å¤šæ¨¡æ€çš„ä¸€è‡´æ€§ã€‚
- en: Potential for Adversarial Attacks. There exist few studies focusing on adversarial
    attacks towards omnidirectional vision models. Zhang et al.Â [[219](#bib.bib219)]
    proposed the first and representative attack approach to fool DNN models by perturbing
    only one tangent image rendered from the ODI. The proposed attack is sparse as
    it disturbs only a small part of the input ODI. Therefore, they further proposed
    a position searching method to search for the tangent point on the spherical surface.
    There are numerous promising yet challenging research problems in this direction,
    e.g., analyzing the generalization capacity of attacks among different DNN models
    for ODIs, white-box attacks for network architectures and training methods, and
    defenses against attacks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨çš„å¯¹æŠ—æ”»å‡»é£é™©ã€‚é’ˆå¯¹å…¨å‘è§†è§‰æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»ç ”ç©¶è¾ƒå°‘ã€‚å¼ ç­‰äºº[[219](#bib.bib219)] æå‡ºäº†ç¬¬ä¸€ç§å…·æœ‰ä»£è¡¨æ€§çš„æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ä»…æ‰°åŠ¨ä»ODIæ¸²æŸ“å‡ºçš„ä¸€ä¸ªåˆ‡çº¿å›¾åƒæ¥æ¬ºéª—DNNæ¨¡å‹ã€‚è¯¥æ”»å‡»æ–¹æ³•å…·æœ‰ç¨€ç–æ€§ï¼Œå› ä¸ºå®ƒä»…æ‰°åŠ¨è¾“å…¥ODIçš„ä¸€å°éƒ¨åˆ†ã€‚å› æ­¤ï¼Œä»–ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä½ç½®æœç´¢æ–¹æ³•æ¥å¯»æ‰¾çƒé¢ä¸Šçš„åˆ‡çº¿ç‚¹ã€‚åœ¨è¿™ä¸€æ–¹å‘ä¸Šè¿˜æœ‰è®¸å¤šæœ‰å‰æ™¯ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚ï¼Œåˆ†æä¸åŒDNNæ¨¡å‹å¯¹ODIçš„æ”»å‡»çš„æ³›åŒ–èƒ½åŠ›ï¼Œç½‘ç»œæ¶æ„å’Œè®­ç»ƒæ–¹æ³•çš„ç™½ç›’æ”»å‡»ï¼Œä»¥åŠå¯¹æŠ—æ”»å‡»çš„é˜²å¾¡ã€‚
- en: Potential for Metaverse. Metaverse aims to create a virtual world containing
    large-scale high-fidelity digital models, where users can freely create contents
    and obtain immersive interactive experience. Metaverse is facilitated by the AR
    and VR headsets, in which ODIs are favored due to the complete FoV. Therefore,
    a potential direction is to generate high-fidelity 2D/3D models from ODIs and
    simulate the real-world objects and scenes in great details. In addition, to help
    users obtain immersive experience, techniques that analyze and understand human
    behavior (e.g., gaze following, saliency prediction) can be further explored and
    integrated in the future.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å…ƒå®‡å®™çš„æ½œåŠ›ã€‚å…ƒå®‡å®™æ—¨åœ¨åˆ›å»ºä¸€ä¸ªåŒ…å«å¤§è§„æ¨¡é«˜ä¿çœŸæ•°å­—æ¨¡å‹çš„è™šæ‹Ÿä¸–ç•Œï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±åˆ›å»ºå†…å®¹å¹¶è·å¾—æ²‰æµ¸å¼äº’åŠ¨ä½“éªŒã€‚å…ƒå®‡å®™å¾—ç›ŠäºARå’ŒVRå¤´æˆ´è®¾å¤‡ï¼Œå…¶ä¸­ODIå› å…¶å®Œæ•´çš„è§†åœºï¼ˆFoVï¼‰è€Œå—åˆ°é’çã€‚å› æ­¤ï¼Œä¸€ä¸ªæ½œåœ¨çš„æ–¹å‘æ˜¯ä»ODIç”Ÿæˆé«˜ä¿çœŸçš„2D/3Dæ¨¡å‹ï¼Œå¹¶è¯¦ç»†æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„ç‰©ä½“å’Œåœºæ™¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¸®åŠ©ç”¨æˆ·è·å¾—æ²‰æµ¸å¼ä½“éªŒï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å’Œé›†æˆåˆ†æå’Œç†è§£äººç±»è¡Œä¸ºçš„æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œæ³¨è§†è·Ÿè¸ªã€æ˜¾è‘—æ€§é¢„æµ‹ï¼‰ã€‚
- en: 'Potential for Smart City. Smart city focuses on collecting data from the city
    with various devices and utilizing information from the data to improve efficiency,
    security and convenience, etc. Taking advantage of the characteristics of ODI
    in street-view images can facilitate the development of urban forms comparison.
    As mentioned in Sec. [3.1.2](#S3.SS1.SSS2 "3.1.2 Cross-view Synthesis and Geo-localization
    â€£ 3.1 Image/Video Manipulation â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives"), a promising direction
    is to convert street-view images into satellite-view images for urban planning.
    Except for room layout discussed in Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Room Layout
    estimation and Reconstruction â€£ 3.3 3D Vision â€£ 3 Omnidirectional Vision Tasks
    â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), ODIs
    can also be applied in more interior designs. To achieve floorplan design, Wang
    et al.Â [[220](#bib.bib220)] leveraged human-activity maps and editable furniture
    placements to improve the interaction with users. However, the input ofÂ [[220](#bib.bib220)]
    is the boundary of the exterior wall, resulting in limitation of the visualization
    and manipulation. Future works might consider operating directly on the ODIs to
    make the interior design observable in all directions, boosting the development
    of interaction and making professional service accessible.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ™ºæ…§åŸå¸‚çš„æ½œåŠ›ã€‚æ™ºæ…§åŸå¸‚ä¸“æ³¨äºé€šè¿‡å„ç§è®¾å¤‡æ”¶é›†åŸå¸‚æ•°æ®ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®æé«˜æ•ˆç‡ã€å®‰å…¨æ€§å’Œä¾¿åˆ©æ€§ç­‰ã€‚åˆ©ç”¨å…¨æ™¯å›¾åƒä¸­çš„ODIç‰¹æ€§å¯ä»¥ä¿ƒè¿›åŸå¸‚å½¢å¼æ¯”è¾ƒçš„å‘å±•ã€‚å¦‚åœ¨ç¬¬[3.1.2](#S3.SS1.SSS2
    "3.1.2 Cross-view Synthesis and Geo-localization â€£ 3.1 Image/Video Manipulation
    â€£ 3 Omnidirectional Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A
    Survey and New Perspectives")èŠ‚ä¸­æåˆ°çš„ï¼Œä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘æ˜¯å°†è¡—æ™¯å›¾åƒè½¬æ¢ä¸ºå«æ˜Ÿå›¾åƒä»¥è¿›è¡ŒåŸå¸‚è§„åˆ’ã€‚é™¤äº†åœ¨ç¬¬[3.3.1](#S3.SS3.SSS1
    "3.3.1 Room Layout estimation and Reconstruction â€£ 3.3 3D Vision â€£ 3 Omnidirectional
    Vision Tasks â€£ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")èŠ‚ä¸­è®¨è®ºçš„æˆ¿é—´å¸ƒå±€ï¼ŒODIè¿˜å¯ä»¥åº”ç”¨äºæ›´å¤šçš„å®¤å†…è®¾è®¡ã€‚ä¸ºäº†å®ç°å¹³é¢å›¾è®¾è®¡ï¼ŒWangç­‰äºº[[220](#bib.bib220)]åˆ©ç”¨äººç±»æ´»åŠ¨å›¾å’Œå¯ç¼–è¾‘å®¶å…·å¸ƒå±€æ¥æ”¹å–„ç”¨æˆ·äº’åŠ¨ã€‚ç„¶è€Œï¼Œ[[220](#bib.bib220)]çš„è¾“å…¥æ˜¯å¤–å¢™çš„è¾¹ç•Œï¼Œé™åˆ¶äº†å¯è§†åŒ–å’Œæ“ä½œã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥è€ƒè™‘ç›´æ¥æ“ä½œODIï¼Œä½¿å®¤å†…è®¾è®¡åœ¨æ‰€æœ‰æ–¹å‘ä¸Šéƒ½å¯è§ï¼Œä»è€Œæå‡äº’åŠ¨å‘å±•ï¼Œå¹¶ä½¿ä¸“ä¸šæœåŠ¡æ›´åŠ å¯åŠã€‚'
- en: 7 Conclusion
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 ç»“è®º
- en: In this survey, we comprehensively reviewed and analyzed the recent progress
    of DL methods for omnidirectional vision. We first introduced the principle of
    omnidirectional imaging, convolution methods and datasets. We then provided a
    hierarchical and structural taxonomy of the DL methods. For each task in the taxonomy,
    we summarized the current research status and pointed out the opportunities and
    challenges. We further provided a review of the novel learning strategies and
    applications. After constructing connections among current approaches, we discussed
    the pivotal problems to be solved and indicated promising future research directions.
    We hope this work can provide some insights for researchers and promote progress
    in the community.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å…¨é¢å›é¡¾å’Œåˆ†æäº†å…¨æ™¯è§†è§‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†å…¨æ™¯æˆåƒçš„åŸç†ã€å·ç§¯æ–¹æ³•å’Œæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æä¾›äº†æ·±åº¦å­¦ä¹ æ–¹æ³•çš„åˆ†å±‚å’Œç»“æ„åŒ–åˆ†ç±»ã€‚å¯¹äºåˆ†ç±»ä¸­çš„æ¯ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬æ€»ç»“äº†å½“å‰çš„ç ”ç©¶çŠ¶æ€ï¼Œå¹¶æŒ‡å‡ºäº†æœºé‡å’ŒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å›é¡¾äº†æ–°é¢–çš„å­¦ä¹ ç­–ç•¥å’Œåº”ç”¨ã€‚åœ¨æ„å»ºå½“å‰æ–¹æ³•ä¹‹é—´çš„è”ç³»åï¼Œæˆ‘ä»¬è®¨è®ºäº†éœ€è¦è§£å†³çš„å…³é”®é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºäº†æœ‰å‰é€”çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€äº›è§è§£ï¼Œå¹¶ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›å±•ã€‚
- en: References
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] J.Â Pi, Y.Â Zhang, L.Â Zhu, X.Â Wu, and X.Â Zhou, â€œContent-aware hybrid equi-angular
    cubemap projection for omnidirectional video coding,â€ *VCIP*, 2020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Pi, Y. Zhang, L. Zhu, X. Wu, å’Œ X. Zhouï¼Œâ€œé¢å‘å†…å®¹çš„æ··åˆç­‰è§’ç«‹æ–¹ä½“æŠ•å½±ç”¨äºå…¨æ™¯è§†é¢‘ç¼–ç ï¼Œâ€ *VCIP*ï¼Œ2020å¹´ã€‚'
- en: '[2] H.Â Jiang, G.Â yiÂ Jiang, M.Â Yu, Y.Â Zhang, Y.Â Yang, Z.Â Peng, F.Â Chen, and
    Q.Â Zhang, â€œCubemap-based perception-driven blind quality assessment for 360-degree
    images,â€ *TIP*, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Jiang, G. yi Jiang, M. Yu, Y. Zhang, Y. Yang, Z. Peng, F. Chen, å’Œ Q.
    Zhangï¼Œâ€œåŸºäºç«‹æ–¹ä½“æ˜ å°„çš„æ„ŸçŸ¥é©±åŠ¨ç›²è´¨é‡è¯„ä¼°ç”¨äº360åº¦å›¾åƒï¼Œâ€ *TIP*ï¼Œ2021å¹´ã€‚'
- en: '[3] J.Â Xiao, K.Â A. Ehinger, A.Â Oliva, and A.Â Torralba, â€œRecognizing scene viewpoint
    using panoramic place representation,â€ in *CVPR*, 2012.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Xiao, K. A. Ehinger, A. Oliva, å’Œ A. Torralbaï¼Œâ€œä½¿ç”¨å…¨æ™¯åœºæ‰€è¡¨ç¤ºè¯†åˆ«åœºæ™¯è§†è§’ï¼Œâ€ *CVPR*ï¼Œ2012å¹´ã€‚'
- en: '[4] Y.Â Rai, J.Â GutiÃ©rrez, and P.Â LeÂ Callet, â€œA dataset of head and eye movements
    for 360 degree images,â€ in *ACM MMSys*, 2017.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Rai, J. GutiÃ©rrez, å’Œ P. Le Calletï¼Œâ€œç”¨äº360åº¦å›¾åƒçš„å¤´éƒ¨å’Œçœ¼éƒ¨è¿åŠ¨æ•°æ®é›†ï¼Œâ€ *ACM MMSys*ï¼Œ2017å¹´ã€‚'
- en: '[5] I.Â Armeni, S.Â Sax, A.Â R. Zamir, and S.Â Savarese, â€œJoint 2d-3d-semantic
    data for indoor scene understanding,â€ *arXiv*, 2017.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] I. Armeni, S. Sax, A. R. Zamir, å’Œ S. Savareseï¼Œâ€œç”¨äºå®¤å†…åœºæ™¯ç†è§£çš„è”åˆ2D-3D-è¯­ä¹‰æ•°æ®ï¼Œâ€
    *arXiv*ï¼Œ2017å¹´ã€‚'
- en: '[6] H.Â Yun, Y.Â Yu, W.Â Yang, K.Â Lee, and G.Â Kim, â€œPano-avqa: Grounded audio-visual
    question answering on 360^âˆ˜ videos,â€ *ICCV*, 2021.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Yun, Y. Yu, W. Yang, K. Lee, å’Œ G. Kimï¼Œâ€œPano-avqa: åŸºäº 360^âˆ˜ è§†é¢‘çš„éŸ³é¢‘-è§†è§‰é—®ç­”ï¼Œâ€
    *ICCV*ï¼Œ2021å¹´ã€‚'
- en: '[7] Y.Â Zhang, S.Â Song, P.Â Tan, and J.Â Xiao, â€œPanocontext: A whole-room 3d context
    model for panoramic scene understanding,â€ in *ECCV*, 2014.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Zhang, S. Song, P. Tan, å’Œ J. Xiaoï¼Œâ€œPanocontext: ç”¨äºå…¨æ™¯åœºæ™¯ç†è§£çš„å…¨æˆ¿é—´ 3D ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œâ€åœ¨
    *ECCV*ï¼Œ2014å¹´ã€‚'
- en: '[8] K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
    recognition,â€ *CVPR*, 2016.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. He, X. Zhang, S. Ren, å’Œ J. Sunï¼Œâ€œç”¨äºå›¾åƒè¯†åˆ«çš„æ·±åº¦æ®‹å·®å­¦ä¹ ï¼Œâ€ *CVPR*ï¼Œ2016å¹´ã€‚'
- en: '[9] L.Â R. Medsker and L.Â Jain, â€œRecurrent neural networks,â€ *Design and Applications*,
    2001.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. R. Medsker å’Œ L. Jainï¼Œâ€œé€’å½’ç¥ç»ç½‘ç»œï¼Œâ€ *è®¾è®¡ä¸åº”ç”¨*ï¼Œ2001å¹´ã€‚'
- en: '[10] I.Â Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu, D.Â Warde-Farley, S.Â Ozair,
    A.Â Courville, and Y.Â Bengio, â€œGenerative adversarial nets,â€ *NIPS*, 2014.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, å’Œ Y. Bengioï¼Œâ€œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œâ€ *NIPS*ï¼Œ2014å¹´ã€‚'
- en: '[11] F.Â Scarselli, M.Â Gori, A.Â C. Tsoi, M.Â Hagenbuchner, and G.Â Monfardini,
    â€œThe graph neural network model,â€ *IEEE TNNLS*, 2008.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, å’Œ G. Monfardiniï¼Œâ€œå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œâ€
    *IEEE TNNLS*ï¼Œ2008å¹´ã€‚'
- en: '[12] A.Â Dosovitskiy, L.Â Beyer, A.Â Kolesnikov, D.Â Weissenborn, X.Â Zhai, T.Â Unterthiner,
    M.Â Dehghani, M.Â Minderer, G.Â Heigold, S.Â Gelly *etÂ al.*, â€œAn image is worth 16x16
    words: Transformers for image recognition at scale,â€ *ICLR*, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *ç­‰*ï¼Œâ€œä¸€å¼ å›¾åƒä»·å€¼ 16x16 ä¸ªè¯ï¼šç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„
    Transformersï¼Œâ€ *ICLR*ï¼Œ2020å¹´ã€‚'
- en: '[13] C.Â Zou, J.-W. Su, C.-H. Peng, A.Â Colburn, Q.Â Shan, P.Â Wonka, H.Â kuo Chu,
    and D.Â Hoiem, â€œManhattan room layout reconstruction from a single 360^âˆ˜ image:
    A comparative study of state-of-the-art methods,â€ *IJCV.*, 2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Zou, J.-W. Su, C.-H. Peng, A. Colburn, Q. Shan, P. Wonka, H. kuo Chu,
    å’Œ D. Hoiemï¼Œâ€œä»å•å¼  360^âˆ˜ å›¾åƒé‡å»ºæ›¼å“ˆé¡¿æˆ¿é—´å¸ƒå±€ï¼šå‰æ²¿æ–¹æ³•çš„æ¯”è¾ƒç ”ç©¶ï¼Œâ€ *IJCV.*ï¼Œ2021å¹´ã€‚'
- en: '[14] T.Â L.Â T. daÂ Silveira, P.Â G.Â L. Pinto, J.Â Murrugarra-Llerena, and C.Â R.
    Jung, â€œ3d scene geometry estimation from 360^âˆ˜ imagery: A survey,â€ *ACM Computing
    Surveys (CSUR)*, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. L. T. da Silveira, P. G. L. Pinto, J. Murrugarra-Llerena, å’Œ C. R. Jungï¼Œâ€œæ¥è‡ª
    360^âˆ˜ å›¾åƒçš„ 3D åœºæ™¯å‡ ä½•ä¼°è®¡ï¼šç»¼è¿°ï¼Œâ€ *ACM Computing Surveys (CSUR)*ï¼Œ2022å¹´ã€‚'
- en: '[15] M.Â Zink, R.Â K. Sitaraman, and K.Â Nahrstedt, â€œScalable 360^âˆ˜ video stream
    delivery: Challenges, solutions, and opportunities,â€ *Proceedings of the IEEE*,
    2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Zink, R. K. Sitaraman, å’Œ K. Nahrstedtï¼Œâ€œå¯æ‰©å±•çš„ 360^âˆ˜ è§†é¢‘æµä¼ è¾“ï¼šæŒ‘æˆ˜ã€è§£å†³æ–¹æ¡ˆå’Œæœºé‡ï¼Œâ€
    *IEEE ä¼šè®®è®ºæ–‡é›†*ï¼Œ2019å¹´ã€‚'
- en: '[16] M.Â Xu, C.Â Li, S.Â Zhang, and P.Â L. Callet, â€œState-of-the-art in 360^âˆ˜ video/image
    processing: Perception, assessment and compression,â€ *IEEE J-STSP*, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Xu, C. Li, S. Zhang, å’Œ P. L. Calletï¼Œâ€œ360^âˆ˜ è§†é¢‘/å›¾åƒå¤„ç†çš„æœ€æ–°è¿›å±•ï¼šæ„ŸçŸ¥ã€è¯„ä¼°å’Œå‹ç¼©ï¼Œâ€
    *IEEE J-STSP*ï¼Œ2020å¹´ã€‚'
- en: '[17] A.Â Yaqoob, T.Â Bi, and G.-M. Muntean, â€œA survey on adaptive 360 video streaming:
    solutions, challenges and opportunities,â€ *IEEE Commun. Surv. Tutor.*, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Yaqoob, T. Bi, å’Œ G.-M. Munteanï¼Œâ€œè‡ªé€‚åº” 360 è§†é¢‘æµçš„ç»¼è¿°ï¼šè§£å†³æ–¹æ¡ˆã€æŒ‘æˆ˜å’Œæœºé‡ï¼Œâ€ *IEEE Commun.
    Surv. Tutor.*ï¼Œ2020å¹´ã€‚'
- en: '[18] N.Â Zioulis, A.Â Karakottas, D.Â Zarpalas, F.Â Alvarez, and P.Â Daras, â€œSpherical
    view synthesis for self-supervised 360 depth estimation,â€ in *3DV*, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Zioulis, A. Karakottas, D. Zarpalas, F. Alvarez, å’Œ P. Darasï¼Œâ€œç”¨äºè‡ªç›‘ç£
    360 æ·±åº¦ä¼°è®¡çš„çƒé¢è§†å›¾åˆæˆï¼Œâ€åœ¨ *3DV*ï¼Œ2019å¹´ã€‚'
- en: '[19] F.-E. Wang, Y.-H. Yeh, M.Â Sun, W.-C. Chiu, and Y.-H. Tsai, â€œBifuse: Monocular
    360 depth estimation via bi-projection fusion,â€ in *CVPR*, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, å’Œ Y.-H. Tsaiï¼Œâ€œBifuse: é€šè¿‡åŒæŠ•å½±èåˆçš„å•ç›®
    360 æ·±åº¦ä¼°è®¡ï¼Œâ€åœ¨ *CVPR*ï¼Œ2020å¹´ã€‚'
- en: '[20] Y.-C. Su and K.Â Grauman, â€œLearning spherical convolution for fast features
    from 360^âˆ˜ imagery,â€ in *NIPS*, 2017.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y.-C. Su å’Œ K. Graumanï¼Œâ€œå­¦ä¹ çƒé¢å·ç§¯ä»¥å¿«é€Ÿæå– 360^âˆ˜ å›¾åƒçš„ç‰¹å¾ï¼Œâ€åœ¨ *NIPS*ï¼Œ2017å¹´ã€‚'
- en: '[21] B.Â Coors, A.Â P. Condurache, and A.Â Geiger, â€œSpherenet: Learning spherical
    representations for detection and classification in omnidirectional images,â€ in
    *ECCV*, 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] B. Coors, A. P. Condurache, å’Œ A. Geigerï¼Œâ€œSpherenet: å­¦ä¹ çƒé¢è¡¨ç¤ºç”¨äºå…¨å‘å›¾åƒä¸­çš„æ£€æµ‹å’Œåˆ†ç±»ï¼Œâ€åœ¨
    *ECCV*ï¼Œ2018å¹´ã€‚'
- en: '[22] Q.Â Zhao, C.Â Zhu, F.Â Dai, Y.Â Ma, G.Â Jin, and Y.Â Zhang, â€œDistortion-aware
    cnns for spherical images.â€ in *IJCAI*, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Q. Zhao, C. Zhu, F. Dai, Y. Ma, G. Jin, å’Œ Y. Zhangï¼Œâ€œç”¨äºçƒé¢å›¾åƒçš„ç•¸å˜æ„ŸçŸ¥å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€åœ¨
    *IJCAI*ï¼Œ2018å¹´ã€‚'
- en: '[23] R.Â Khasanova and P.Â Frossard, â€œGeometry aware convolutional filters for
    omnidirectional images representation,â€ in *ICML*, 2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Khasanova å’Œ P. Frossardï¼Œâ€œç”¨äºå…¨å‘å›¾åƒè¡¨ç¤ºçš„å‡ ä½•æ„ŸçŸ¥å·ç§¯æ»¤æ³¢å™¨ï¼Œâ€åœ¨ *ICML*ï¼Œ2019å¹´ã€‚'
- en: '[24] T.Â Oâ€™Beirne, â€œIntroduction to geometry,â€ *Physics Bulletin*, 1962.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Oâ€™Beirneï¼Œâ€œå‡ ä½•å­¦å¯¼è®ºï¼Œâ€ *Physics Bulletin*ï¼Œ1962å¹´ã€‚'
- en: '[25] Y.Â Li, Y.Â Guo, Z.Â Yan, X.Â Huang, Y.Â Duan, and L.Â Ren, â€œOmnifusion: 360
    monocular depth estimation via geometry-aware fusion,â€ *CVPR*, 2022.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Li, Y. Guo, Z. Yan, X. Huang, Y. Duan, å’Œ L. Renï¼Œâ€œOmnifusion: é€šè¿‡å‡ ä½•æ„ŸçŸ¥èåˆçš„
    360 å•ç›®æ·±åº¦ä¼°è®¡ï¼Œâ€ *CVPR*ï¼Œ2022å¹´ã€‚'
- en: '[26] M.Â Eder, M.Â Shvets, J.Â Lim, and J.-M. Frahm, â€œTangent images for mitigating
    spherical distortion,â€ in *CVPR*, 2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Eder, M. Shvets, J. Lim, å’Œ J.-M. Frahmï¼Œâ€œç¼“è§£çƒé¢å¤±çœŸçš„åˆ‡çº¿å›¾åƒâ€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2020ã€‚'
- en: '[27] Y.Â Lee, J.Â Jeong, J.Â S. Yun, W.Â Cho, and K.Â jin Yoon, â€œSpherephd: Applying
    cnns on a spherical polyhedron representation of 360^âˆ˜ images,â€ *CVPR*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Lee, J. Jeong, J. S. Yun, W. Cho, å’Œ K. jin Yoonï¼Œâ€œSpherephd: åœ¨ 360^âˆ˜
    å›¾åƒçš„çƒé¢å¤šé¢ä½“è¡¨ç¤ºä¸Šåº”ç”¨å·ç§¯ç¥ç»ç½‘ç»œâ€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2019ã€‚'
- en: '[28] Y.Â Yoon, I.Â Chung, L.Â Wang, and K.-J. Yoon, â€œSpheresr,â€ *CVPR*, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Yoon, I. Chung, L. Wang, å’Œ K.-J. Yoonï¼Œâ€œSpheresrâ€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2022ã€‚'
- en: '[29] T.Â S. Cohen, M.Â Geiger, J.Â KÃ¶hler, and M.Â Welling, â€œSpherical cnns,â€ *arXiv*,
    2018.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. S. Cohen, M. Geiger, J. KÃ¶hler, å’Œ M. Wellingï¼Œâ€œçƒé¢å·ç§¯ç¥ç»ç½‘ç»œâ€ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2018ã€‚'
- en: '[30] J.Â Cruz-Mota, I.Â Bogdanova, B.Â Paquier, M.Â Bierlaire, and J.-P. Thiran,
    â€œScale invariant feature transform on the sphere: Theory and applications,â€ *IJCV*,
    2012.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Cruz-Mota, I. Bogdanova, B. Paquier, M. Bierlaire, å’Œ J.-P. Thiranï¼Œâ€œçƒé¢ä¸Šçš„å°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢ï¼šç†è®ºä¸åº”ç”¨â€ï¼Œå‘è¡¨äº
    *IJCV*ï¼Œ2012ã€‚'
- en: '[31] Y.-C. Su and K.Â Grauman, â€œKernel transformer networks for compact spherical
    convolution,â€ *CVPR*, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y.-C. Su å’Œ K. Graumanï¼Œâ€œç´§å‡‘çƒé¢å·ç§¯çš„æ ¸å˜æ¢ç½‘ç»œâ€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2019ã€‚'
- en: '[32] â€”â€”, â€œLearning spherical convolution for 360 recognition,â€ *TPAMI*, 2021.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] â€”â€”ï¼Œâ€œç”¨äº 360 è¯†åˆ«çš„çƒé¢å·ç§¯å­¦ä¹ â€ï¼Œå‘è¡¨äº *TPAMI*ï¼Œ2021ã€‚'
- en: '[33] P.Â Frossard and R.Â Khasanova, â€œGraph-based classification of omnidirectional
    images,â€ *ICCV Workshops*, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Frossard å’Œ R. Khasanovaï¼Œâ€œåŸºäºå›¾çš„å…¨å‘å›¾åƒåˆ†ç±»â€ï¼Œå‘è¡¨äº *ICCV Workshops*ï¼Œ2017ã€‚'
- en: '[34] M.Â Rey-Area, M.Â Yuan, and C.Â Richardt, â€œ360MonoDepth: High-resolution
    360^âˆ˜ monocular depth estimation,â€ in *CVPR*, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Rey-Area, M. Yuan, å’Œ C. Richardtï¼Œâ€œ360MonoDepth: é«˜åˆ†è¾¨ç‡ 360^âˆ˜ å•ç›®æ·±åº¦ä¼°è®¡â€ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2022ã€‚'
- en: '[35] Q.Â Yang, C.Â Li, W.Â Dai, J.Â Zou, G.-J. Qi, and H.Â Xiong, â€œRotation equivariant
    graph convolutional network for spherical image classification,â€ *CVPR*, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Q. Yang, C. Li, W. Dai, J. Zou, G.-J. Qi, å’Œ H. Xiongï¼Œâ€œç”¨äºçƒé¢å›¾åƒåˆ†ç±»çš„æ—‹è½¬ç­‰å˜å›¾å·ç§¯ç½‘ç»œâ€ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2020ã€‚'
- en: '[36] C.Â Esteves, C.Â Allen-Blanchette, A.Â Makadia, and K.Â Daniilidis, â€œLearning
    so(3) equivariant representations with spherical cnns,â€ in *ECCV*, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Esteves, C. Allen-Blanchette, A. Makadia, å’Œ K. Daniilidisï¼Œâ€œä½¿ç”¨çƒé¢å·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ 
    so(3) ç­‰å˜è¡¨ç¤ºâ€ï¼Œå‘è¡¨äº *ECCV*ï¼Œ2018ã€‚'
- en: '[37] T.Â Cohen, M.Â Weiler, B.Â Kicanaoglu, and M.Â Welling, â€œGauge equivariant
    convolutional networks and the icosahedral cnn,â€ in *ICML*, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Cohen, M. Weiler, B. Kicanaoglu, å’Œ M. Wellingï¼Œâ€œé‡è§„ç­‰å˜å·ç§¯ç½‘ç»œä¸äºŒåé¢ä½“å·ç§¯ç¥ç»ç½‘ç»œâ€ï¼Œå‘è¡¨äº
    *ICML*ï¼Œ2019ã€‚'
- en: '[38] M.Â Shakerinava and S.Â Ravanbakhsh, â€œEquivariant networks for pixelized
    spheres,â€ in *ICML*, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Shakerinava å’Œ S. Ravanbakhshï¼Œâ€œåƒç´ åŒ–çƒä½“çš„ç­‰å˜ç½‘ç»œâ€ï¼Œå‘è¡¨äº *ICML*ï¼Œ2021ã€‚'
- en: '[39] M.Â Defferrard, M.Â Milani, F.Â Gusset, and N.Â Perraudin, â€œDeepsphere: a
    graph-based spherical cnn,â€ in *ICLR*, 2020.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Defferrard, M. Milani, F. Gusset, å’Œ N. Perraudinï¼Œâ€œDeepsphere: åŸºäºå›¾çš„çƒé¢å·ç§¯ç¥ç»ç½‘ç»œâ€ï¼Œå‘è¡¨äº
    *ICLR*ï¼Œ2020ã€‚'
- en: '[40] J.Â Zheng, J.Â Zhang, J.Â Li, R.Â Tang, S.Â Gao, and Z.Â Zhou, â€œStructured3d:
    A large photo-realistic dataset for structured 3d modeling,â€ in *ECCV*, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, å’Œ Z. Zhouï¼Œâ€œStructured3d: å¤§è§„æ¨¡ç…§ç‰‡çº§çœŸå®æ„Ÿæ•°æ®é›†ç”¨äºç»“æ„åŒ–
    3d å»ºæ¨¡â€ï¼Œå‘è¡¨äº *ECCV*ï¼Œ2020ã€‚'
- en: '[41] S.Â Song, F.Â Yu, A.Â Zeng, A.Â X. Chang, M.Â Savva, and T.Â Funkhouser, â€œSemantic
    scene completion from a single depth image,â€ in *CVPR*, 2017.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, å’Œ T. Funkhouserï¼Œâ€œåŸºäºå•å¹…æ·±åº¦å›¾åƒçš„è¯­ä¹‰åœºæ™¯è¡¥å…¨â€ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2017ã€‚'
- en: '[42] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J. Chang, and M.Â Sun,
    â€œDeep 360 pilot: Learning a deep agent for piloting through 360 sports videos,â€
    in *CVPR*, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J. Chang, å’Œ M. Sunï¼Œâ€œæ·±åº¦
    360 é©¾é©¶å‘˜ï¼šå­¦ä¹ æ·±åº¦ä»£ç†ä»¥æ“æ§ 360 ä½“è‚²è§†é¢‘â€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2017ã€‚'
- en: '[43] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, and M.Â Sun,
    â€œCube padding for weakly-supervised saliency prediction in 360 videos,â€ in *CVPR*,
    2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, å’Œ M. Sunï¼Œâ€œç”¨äº
    360 è§†é¢‘çš„å¼±ç›‘ç£æ˜¾è‘—æ€§é¢„æµ‹çš„ç«‹æ–¹ä½“å¡«å……â€ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2018ã€‚'
- en: '[44] R.Â Seidel, A.Â Apitzsch, and G.Â Hirtz, â€œOmniflow: Human omnidirectional
    optical flow,â€ *CVPR Workshops*, 2021.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Seidel, A. Apitzsch, å’Œ G. Hirtzï¼Œâ€œOmniflow: äººç±»å…¨å‘å…‰æµâ€ï¼Œå‘è¡¨äº *CVPR Workshops*ï¼Œ2021ã€‚'
- en: '[45] C.Â Zhang, S.Â Liwicki, W.Â Smith, and R.Â Cipolla, â€œOrientation-aware semantic
    segmentation on icosahedron spheres,â€ in *ICCV*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Zhang, S. Liwicki, W. Smith, å’Œ R. Cipollaï¼Œâ€œåŸºäºäºŒåé¢ä½“çƒä½“çš„æ–¹å‘æ„ŸçŸ¥è¯­ä¹‰åˆ†å‰²â€ï¼Œå‘è¡¨äº *ICCV*ï¼Œ2019ã€‚'
- en: '[46] R.Â Liu, G.Â Zhang, J.Â Wang, and S.Â Zhao, â€œCross-modal 360^âˆ˜ depth completion
    and reconstruction for large-scale indoor environment,â€ *IEEE Trans. Intell. Transp.
    Syst.*, 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Liu, G. Zhang, J. Wang, å’Œ S. Zhaoï¼Œâ€œè·¨æ¨¡æ€ 360^âˆ˜ æ·±åº¦è¡¥å…¨ä¸é‡å»ºç”¨äºå¤§è§„æ¨¡å®¤å†…ç¯å¢ƒâ€ï¼Œå‘è¡¨äº
    *IEEE Trans. Intell. Transp. Syst.*ï¼Œ2022ã€‚'
- en: '[47] C.Â Oh, W.Â Cho, D.Â Park, Y.Â Chae, L.Â Wang, and K.-J. Yoon, â€œBips: Bi-modal
    indoor panorama synthesis via residual depth-aided adversarial learning,â€ *arXiv*,
    2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Oh, W. Cho, D. Park, Y. Chae, L. Wang, å’Œ K.-J. Yoonï¼Œâ€œBips: é€šè¿‡æ®‹å·®æ·±åº¦è¾…åŠ©å¯¹æŠ—å­¦ä¹ è¿›è¡ŒåŒæ¨¡æ€å®¤å†…å…¨æ™¯åˆæˆâ€ï¼Œå‘è¡¨äº
    *arXiv*ï¼Œ2021ã€‚'
- en: '[48] T.Â Hara, Y.Â Mukuta, and T.Â Harada, â€œSpherical image generation from a
    single image by considering scene symmetry,â€ in *AAAI*, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Hara, Y. Mukuta å’Œ T. Haradaï¼Œ"è€ƒè™‘åœºæ™¯å¯¹ç§°æ€§ä»å•ä¸€å›¾åƒç”Ÿæˆçƒé¢å›¾åƒ"ï¼Œå‘è¡¨äº *AAAI*ï¼Œ2021å¹´ã€‚'
- en: '[49] N.Â Akimoto, Y.Â Matsuo, and Y.Â Aoki, â€œDiverse plausible 360-degree image
    outpainting for efficient 3dcg background creation,â€ *arXiv*, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] N. Akimoto, Y. Matsuo å’Œ Y. Aokiï¼Œ"é«˜æ•ˆ3DCGèƒŒæ™¯åˆ›å»ºçš„å¤šæ ·åŒ–å¯è¡Œ360åº¦å›¾åƒæ‰©å±•"ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[50] J.Â S. Sumantri and I.Â K. Park, â€œ360 panorama synthesis from a sparse set
    of images with unknown field of view,â€ in *WACV*, 2020.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. S. Sumantri å’Œ I. K. Parkï¼Œ"ä»ç¨€ç–å›¾åƒé›†åˆä¸­åˆæˆ360åº¦å…¨æ™¯å›¾åƒï¼Œè§†åœºè§’æœªçŸ¥"ï¼Œå‘è¡¨äº *WACV*ï¼Œ2020å¹´ã€‚'
- en: '[51] L.Â Roldao, R.Â DeÂ Charette, and A.Â Verroust-Blondet, â€œ3d semantic scene
    completion: a survey,â€ *arXiv*, 2021.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. Roldao, R. De Charette å’Œ A. Verroust-Blondetï¼Œ"3Dè¯­ä¹‰åœºæ™¯å®Œæˆï¼šç»¼è¿°"ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2021å¹´ã€‚'
- en: '[52] A.Â Dourado, H.Â Kim, T.Â E. deÂ Campos, and A.Â Hilton, â€œSemantic scene completion
    from a single 360-degree image and depth map.â€ in *VISIGRAPP*, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. Dourado, H. Kim, T. E. de Campos å’Œ A. Hiltonï¼Œ"ä»å•ä¸€360åº¦å›¾åƒå’Œæ·±åº¦å›¾è¿›è¡Œè¯­ä¹‰åœºæ™¯å®Œæˆ"ï¼Œå‘è¡¨äº
    *VISIGRAPP*ï¼Œ2020å¹´ã€‚'
- en: '[53] A.Â Dai, D.Â Ritchie, M.Â Bokeloh, S.Â Reed, J.Â Sturm, and M.Â NieÃŸner, â€œScancomplete:
    Large-scale scene completion and semantic segmentation for 3d scans,â€ in *CVPR*,
    2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm å’Œ M. NieÃŸnerï¼Œ"Scancompleteï¼šå¤§è§„æ¨¡åœºæ™¯å®Œæˆå’Œ3Dæ‰«æçš„è¯­ä¹‰åˆ†å‰²"ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2018å¹´ã€‚'
- en: '[54] X.Â Lu, Z.Â Li, Z.Â Cui, M.Â R. Oswald, M.Â Pollefeys, and R.Â Qin, â€œGeometry-aware
    satellite-to-ground image synthesis for urban areas,â€ in *CVPR*, 2020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys å’Œ R. Qinï¼Œ"é¢å‘å‡ ä½•çš„åŸå¸‚åŒºåŸŸå«æ˜Ÿåˆ°åœ°é¢å›¾åƒåˆæˆ"ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2020å¹´ã€‚'
- en: '[55] Z.Â Li, Z.Â Li, Z.Â Cui, R.Â Qin, M.Â Pollefeys, and M.Â R. Oswald, â€œSat2vid:
    Street-view panoramic video synthesis from a single satellite image,â€ in *ICCV*,
    2021.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys å’Œ M. R. Oswaldï¼Œ"Sat2vidï¼šä»å•ä¸€å«æ˜Ÿå›¾åƒç”Ÿæˆè¡—æ™¯å…¨æ™¯è§†é¢‘"ï¼Œå‘è¡¨äº
    *ICCV*ï¼Œ2021å¹´ã€‚'
- en: '[56] M.Â Zhai, Z.Â Bessinger, S.Â Workman, and N.Â Jacobs, â€œPredicting ground-level
    scene layout from aerial imagery,â€ in *CVPR*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Zhai, Z. Bessinger, S. Workman å’Œ N. Jacobsï¼Œ"ä»èˆªç©ºå›¾åƒé¢„æµ‹åœ°é¢åœºæ™¯å¸ƒå±€"ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2017å¹´ã€‚'
- en: '[57] K.Â Regmi and M.Â Shah, â€œBridging the domain gap for ground-to-aerial image
    matching,â€ in *ICCV*, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Regmi å’Œ M. Shahï¼Œ"å¼¥åˆåœ°é¢åˆ°èˆªç©ºå›¾åƒåŒ¹é…çš„é¢†åŸŸå·®è·"ï¼Œå‘è¡¨äº *ICCV*ï¼Œ2019å¹´ã€‚'
- en: '[58] A.Â Toker, Q.Â Zhou, M.Â Maximov, and L.Â Leal-TaixÃ©, â€œComing down to earth:
    Satellite-to-street view synthesis for geo-localization,â€ in *CVPR*, 2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Toker, Q. Zhou, M. Maximov å’Œ L. Leal-TaixÃ©ï¼Œ"å›åˆ°åœ°çƒï¼šç”¨äºåœ°ç†å®šä½çš„å«æ˜Ÿåˆ°è¡—æ™¯å›¾åƒåˆæˆ"ï¼Œå‘è¡¨äº
    *CVPR*ï¼Œ2021å¹´ã€‚'
- en: '[59] Y.Â Shi, L.Â Liu, X.Â Yu, and H.Â Li, â€œSpatial-aware feature aggregation for
    image based cross-view geo-localization,â€ *NIPS*, 2019.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Shi, L. Liu, X. Yu å’Œ H. Liï¼Œ"é¢å‘ç©ºé—´çš„ç‰¹å¾èšåˆç”¨äºåŸºäºå›¾åƒçš„äº¤å‰è§†å›¾åœ°ç†å®šä½"ï¼Œå‘è¡¨äº *NIPS*ï¼Œ2019å¹´ã€‚'
- en: '[60] S.Â Zhu, M.Â Shah, and C.Â Chen, â€œTransgeo: Transformer is all you need for
    cross-view image geo-localization,â€ *arXiv*, 2022.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Zhu, M. Shah å’Œ C. Chenï¼Œ"Transgeoï¼šè·¨è§†å›¾å›¾åƒåœ°ç†å®šä½ä¸­åªéœ€ä½¿ç”¨ Transformer"ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[61] Y.Â Shi, X.Â Yu, D.Â Campbell, and H.Â Li, â€œWhere am i looking at? joint location
    and orientation estimation by cross-view matching,â€ in *CVPR*, 2020.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Shi, X. Yu, D. Campbell å’Œ H. Liï¼Œ"æˆ‘åœ¨çœ‹ä»€ä¹ˆï¼Ÿé€šè¿‡äº¤å‰è§†å›¾åŒ¹é…è¿›è¡Œä½ç½®å’Œæ–¹å‘è”åˆä¼°è®¡"ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2020å¹´ã€‚'
- en: '[62] S.Â Zhu, T.Â Yang, and C.Â Chen, â€œVigor: Cross-view image geo-localization
    beyond one-to-one retrieval,â€ in *CVPR*, 2021.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Zhu, T. Yang å’Œ C. Chenï¼Œ"Vigorï¼šè·¨è§†å›¾å›¾åƒåœ°ç†å®šä½è¶…è¶Šä¸€å¯¹ä¸€æ£€ç´¢"ï¼Œå‘è¡¨äº *CVPR*ï¼Œ2021å¹´ã€‚'
- en: '[63] C.-Y. Hsu, C.Â Sun, and H.-T. Chen, â€œMoving in a 360 world: Synthesizing
    panoramic parallaxes from a single panorama,â€ *arXiv*, 2021.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C.-Y. Hsu, C. Sun å’Œ H.-T. Chenï¼Œ"åœ¨360åº¦ä¸–ç•Œä¸­ç§»åŠ¨ï¼šä»å•ä¸€å…¨æ™¯å›¾åƒåˆæˆå…¨æ™¯è§†å·®"ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2021å¹´ã€‚'
- en: '[64] B.Â Mildenhall, P.Â P. Srinivasan, M.Â Tancik, J.Â T. Barron, R.Â Ramamoorthi,
    and R.Â Ng, â€œNerf: Representing scenes as neural radiance fields for view synthesis,â€
    in *ECCV*, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi
    å’Œ R. Ngï¼Œ"Nerfï¼šå°†åœºæ™¯è¡¨ç¤ºä¸ºç¥ç»è¾å°„åœºç”¨äºè§†å›¾åˆæˆ"ï¼Œå‘è¡¨äº *ECCV*ï¼Œ2020å¹´ã€‚'
- en: '[65] T.Â Hara and T.Â Harada, â€œEnhancement of novel view synthesis using omnidirectional
    image completion,â€ *arXiv*, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] T. Hara å’Œ T. Haradaï¼Œ"åˆ©ç”¨å…¨æ™¯å›¾åƒå®Œæˆæå‡æ–°è§†å›¾åˆæˆ"ï¼Œå‘è¡¨äº *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[66] J.Â Y. Koh, H.Â Lee, Y.Â Yang, J.Â Baldridge, and P.Â Anderson, â€œPathdreamer:
    A world model for indoor navigation,â€ in *ICCV*, 2021.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge å’Œ P. Andersonï¼Œ"Pathdreamerï¼šä¸€ç§ç”¨äºå®¤å†…å¯¼èˆªçš„ä¸–ç•Œæ¨¡å‹"ï¼Œå‘è¡¨äº
    *ICCV*ï¼Œ2021å¹´ã€‚'
- en: '[67] F.Â D. Simone, P.Â Frossard, P.Â Wilkins, N.Â Birkbeck, and A.Â C. Kokaram,
    â€œGeometry-driven quantization for omnidirectional image coding,â€ *PCS*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] F. D. Simone, P. Frossard, P. Wilkins, N. Birkbeck å’Œ A. C. Kokaramï¼Œ"é¢å‘å…¨æ™¯å›¾åƒç¼–ç çš„å‡ ä½•é©±åŠ¨é‡åŒ–"ï¼Œå‘è¡¨äº
    *PCS*ï¼Œ2016å¹´ã€‚'
- en: '[68] M.Â Å˜eÅ™Ã¡bek, E.Â Upenik, and T.Â Ebrahimi, â€œJpeg backward compatible coding
    of omnidirectional images,â€ in *Applications of digital image processing XXXIX*,
    2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. Å˜eÅ™Ã¡bek, E. Upenik å’Œ T. Ebrahimiï¼Œ"JPEG å‘åå…¼å®¹çš„å…¨æ™¯å›¾åƒç¼–ç "ï¼Œå‘è¡¨äº *Applications
    of digital image processing XXXIX*ï¼Œ2016å¹´ã€‚'
- en: '[69] G.Â K. Wallace, â€œThe jpeg still picture compression standard,â€ *IEEE TCE*,
    1992.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] G. K. Wallaceï¼Œâ€œJPEGé™æ€å›¾ç‰‡å‹ç¼©æ ‡å‡†â€ï¼Œ*IEEE TCE*ï¼Œ1992å¹´ã€‚'
- en: '[70] M.Â Rizkallah, F.Â D. Simone, T.Â Maugey, C.Â Guillemot, and P.Â Frossard,
    â€œRate distortion optimized graph partitioning for omnidirectional image coding,â€
    *EUSIPCO*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Rizkallah, F. D. Simone, T. Maugey, C. Guillemot, å’Œ P. Frossardï¼Œâ€œé’ˆå¯¹å…¨å‘å›¾åƒç¼–ç çš„ç‡å¤±çœŸä¼˜åŒ–å›¾åˆ†å‰²â€ï¼Œ*EUSIPCO*ï¼Œ2018å¹´ã€‚'
- en: '[71] G.Â J. Sullivan and T.Â Wiegand, â€œRate-distortion optimization for video
    compression,â€ *IEEE Signal Process Mag*, 1998.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] G. J. Sullivan å’Œ T. Wiegandï¼Œâ€œè§†é¢‘å‹ç¼©çš„ç‡-å¤±çœŸä¼˜åŒ–â€ï¼Œ*IEEE Signal Process Mag*ï¼Œ1998å¹´ã€‚'
- en: '[72] N.Â M. Bidgoli, R.Â G. d.Â A. Azevedo, T.Â Maugey, A.Â Roumy, and P.Â Frossard,
    â€œOslo: On-the-sphere learning for omnidirectional images and its application to
    360-degree image compression,â€ *arXiv*, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. M. Bidgoli, R. G. d. A. Azevedo, T. Maugey, A. Roumy, å’Œ P. Frossardï¼Œâ€œOslo:
    ç”¨äºå…¨å‘å›¾åƒçš„çƒé¢å­¦ä¹ åŠå…¶åœ¨360åº¦å›¾åƒå‹ç¼©ä¸­çš„åº”ç”¨â€ï¼Œ*arXiv*ï¼Œ2021å¹´ã€‚'
- en: '[73] K.Â M. Gorski, E.Â Hivon, A.Â J. Banday, B.Â D. Wandelt, F.Â K. Hansen, M.Â Reinecke,
    and M.Â Bartelman, â€œHealpix: A framework for high-resolution discretization and
    fast analysis of data distributed on the sphere,â€ *The Astrophysical Journal*,
    2005.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. M. Gorski, E. Hivon, A. J. Banday, B. D. Wandelt, F. K. Hansen, M.
    Reinecke, å’Œ M. Bartelmanï¼Œâ€œHealpix: ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡ç¦»æ•£åŒ–å’Œå¿«é€Ÿåˆ†æçƒé¢æ•°æ®çš„æ¡†æ¶â€ï¼Œ*å¤©ä½“ç‰©ç†å­¦æ‚å¿—*ï¼Œ2005å¹´ã€‚'
- en: '[74] Y.Â Li, J.Â Xu, and Z.Â Chen, â€œSpherical domain rate-distortion optimization
    for omnidirectional video coding,â€ *IEEE TCSVT*, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Li, J. Xu, å’Œ Z. Chenï¼Œâ€œå…¨å‘è§†é¢‘ç¼–ç çš„çƒé¢åŸŸç‡-å¤±çœŸä¼˜åŒ–â€ï¼Œ*IEEE TCSVT*ï¼Œ2019å¹´ã€‚'
- en: '[75] Y.Â Wang, D.Â Liu, S.Â Ma, F.Â Wu, and W.Â Gao, â€œSpherical coordinates transform-based
    motion model for panoramic video coding,â€ *IEEE J-ESTCS*, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Wang, D. Liu, S. Ma, F. Wu, å’Œ W. Gaoï¼Œâ€œåŸºäºçƒé¢åæ ‡å˜æ¢çš„å…¨æ™¯è§†é¢‘ç¼–ç è¿åŠ¨æ¨¡å‹â€ï¼Œ*IEEE J-ESTCS*ï¼Œ2019å¹´ã€‚'
- en: '[76] C.-W. Fu, L.Â Wan, T.Â Wong, and A.Â C.-S. Leung, â€œThe rhombic dodecahedron
    map: An efficient scheme for encoding panoramic video,â€ *IEEE TMM*, 2009.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C.-W. Fu, L. Wan, T. Wong, å’Œ A. C.-S. Leungï¼Œâ€œè±å½¢åäºŒé¢ä½“æ˜ å°„ï¼šä¸€ç§é«˜æ•ˆçš„å…¨æ™¯è§†é¢‘ç¼–ç æ–¹æ¡ˆâ€ï¼Œ*IEEE
    TMM*ï¼Œ2009å¹´ã€‚'
- en: '[77] L.Â Li, N.Â Yan, Z.Â Li, S.Â Liu, and H.Â Li, â€œ$\lambda$-domain perceptual
    rate control for 360-degree video compression,â€ *IEEE J-STSP*, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Li, N. Yan, Z. Li, S. Liu, å’Œ H. Liï¼Œâ€œ$\lambda$-åŸŸæ„ŸçŸ¥ç‡æ§åˆ¶ç”¨äº360åº¦è§†é¢‘å‹ç¼©â€ï¼Œ*IEEE
    J-STSP*ï¼Œ2020å¹´ã€‚'
- en: '[78] T.Â Zhao, J.Â Lin, Y.Â Song, X.Â Wang, and Y.Â Niu, â€œGame theory-driven rate
    control for 360-degree video coding,â€ *ACM MM*, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Zhao, J. Lin, Y. Song, X. Wang, å’Œ Y. Niuï¼Œâ€œåŸºäºåšå¼ˆè®ºçš„360åº¦è§†é¢‘ç¼–ç ç‡æ§åˆ¶â€ï¼Œ*ACM MM*ï¼Œ2021å¹´ã€‚'
- en: '[79] Y.Â Hold-Geoffroy, K.Â Sunkavalli, S.Â Hadap, E.Â Gambaretto, and J.-F. Lalonde,
    â€œDeep outdoor illumination estimation,â€ in *CVPR*, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, å’Œ J.-F. Lalondeï¼Œâ€œæ·±åº¦æˆ·å¤–å…‰ç…§ä¼°è®¡â€ï¼Œåœ¨*CVPR*ï¼Œ2017å¹´ã€‚'
- en: '[80] M.-A. Gardner, K.Â Sunkavalli, E.Â Yumer, X.Â Shen, E.Â Gambaretto, C.Â GagnÃ©,
    and J.-F. Lalonde, â€œLearning to predict indoor illumination from a single image,â€
    *ACM TOG*, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. GagnÃ©,
    å’Œ J.-F. Lalondeï¼Œâ€œä»å•å¼ å›¾åƒä¸­å­¦ä¹ é¢„æµ‹å®¤å†…å…‰ç…§â€ï¼Œ*ACM TOG*ï¼Œ2017å¹´ã€‚'
- en: '[81] M.-A. Gardner, Y.Â Hold-Geoffroy, K.Â Sunkavalli, C.Â GagnÃ©, and J.-F. Lalonde,
    â€œDeep parametric indoor lighting estimation,â€ *ICCV*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M.-A. Gardner, Y. Hold-Geoffroy, K. Sunkavalli, C. GagnÃ©, å’Œ J.-F. Lalondeï¼Œâ€œæ·±åº¦å‚æ•°åŒ–å®¤å†…å…‰ç…§ä¼°è®¡â€ï¼Œ*ICCV*ï¼Œ2019å¹´ã€‚'
- en: '[82] F.Â Zhan, C.Â Zhang, Y.Â Yu, Y.Â Chang, S.Â Lu, F.Â Ma, and X.Â Xie, â€œEmlight:
    Lighting estimation via spherical distribution approximation,â€ *AAAI*, 2021.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] F. Zhan, C. Zhang, Y. Yu, Y. Chang, S. Lu, F. Ma, å’Œ X. Xieï¼Œâ€œEmlight: é€šè¿‡çƒé¢åˆ†å¸ƒé€¼è¿‘è¿›è¡Œå…‰ç…§ä¼°è®¡â€ï¼Œ*AAAI*ï¼Œ2021å¹´ã€‚'
- en: '[83] J.Â P. Rolland and H.Â Hua, â€œHead-mounted display systems,â€ *Encyclopedia
    of optical engineering*, 2005.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. P. Rolland å’Œ H. Huaï¼Œâ€œå¤´æˆ´æ˜¾ç¤ºç³»ç»Ÿâ€ï¼Œ*å…‰å­¦å·¥ç¨‹ç™¾ç§‘å…¨ä¹¦*ï¼Œ2005å¹´ã€‚'
- en: '[84] C.Â Ozcinar, A.Â Rana, and A.Â Smolic, â€œSuper-resolution of omnidirectional
    images using adversarial learning,â€ in *MMSP*, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] C. Ozcinar, A. Rana, å’Œ A. Smolicï¼Œâ€œä½¿ç”¨å¯¹æŠ—å­¦ä¹ çš„å…¨å‘å›¾åƒè¶…åˆ†è¾¨ç‡â€ï¼Œåœ¨*MMSP*ï¼Œ2019å¹´ã€‚'
- en: '[85] X.Â Deng, H.Â Wang, M.Â Xu, Y.Â Guo, Y.Â Song, and L.Â Yang, â€œLau-net: Latitude
    adaptive upscaling network for omnidirectional imag super-resolution,â€ *CVPR*,
    2021.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Deng, H. Wang, M. Xu, Y. Guo, Y. Song, å’Œ L. Yangï¼Œâ€œLau-net: ç”¨äºå…¨å‘å›¾åƒè¶…åˆ†è¾¨ç‡çš„çº¬åº¦è‡ªé€‚åº”ä¸Šé‡‡æ ·ç½‘ç»œâ€ï¼Œ*CVPR*ï¼Œ2021å¹´ã€‚'
- en: '[86] H.Â Liu, Z.Â Ruan, C.Â Fang, P.Â Zhao, F.Â Shang, Y.Â Liu, and L.Â Wang, â€œA single
    frame and multi-frame joint network for 360-degree panorama video super-resolution,â€
    *arXiv*, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] H. Liu, Z. Ruan, C. Fang, P. Zhao, F. Shang, Y. Liu, å’Œ L. Wangï¼Œâ€œç”¨äº360åº¦å…¨æ™¯è§†é¢‘è¶…åˆ†è¾¨ç‡çš„å•å¸§ä¸å¤šå¸§è”åˆç½‘ç»œâ€ï¼Œ*arXiv*ï¼Œ2020å¹´ã€‚'
- en: '[87] M.Â Bosse, R.Â J. Rikoski, J.Â J. Leonard, and S.Â Teller, â€œVanishing points
    and 3d lines from omnidirectional video,â€ *ICIP*, 2002.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Bosse, R. J. Rikoski, J. J. Leonard, å’Œ S. Tellerï¼Œâ€œæ¥è‡ªå…¨å‘è§†é¢‘çš„æ¶ˆå¤±ç‚¹å’Œ3Dçº¿æ¡â€ï¼Œ*ICIP*ï¼Œ2002å¹´ã€‚'
- en: '[88] C.Â A. Vanegas, D.Â G. Aliaga, and B.Â Benes, â€œBuilding reconstruction using
    manhattan-world grammars,â€ in *CVPR*, 2010.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. A. Vanegas, D. G. Aliaga, å’Œ B. Benesï¼Œâ€œä½¿ç”¨æ›¼å“ˆé¡¿ä¸–ç•Œè¯­æ³•è¿›è¡Œå»ºç­‘é‡å»ºâ€ï¼Œåœ¨*CVPR*ï¼Œ2010å¹´ã€‚'
- en: '[89] G.Â Schindler and F.Â Dellaert, â€œAtlanta world: an expectation maximization
    framework for simultaneous low-level edge grouping and camera calibration in complex
    man-made environments,â€ in *CVPR*, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] G. Schindler å’Œ F. Dellaertï¼Œâ€œAtlanta world: ä¸€ä¸ªç”¨äºå¤æ‚äººå·¥ç¯å¢ƒä¸­åŒæ—¶è¿›è¡Œä½çº§è¾¹ç¼˜åˆ†ç»„å’Œç›¸æœºæ ‡å®šçš„æœŸæœ›æœ€å¤§åŒ–æ¡†æ¶ï¼Œâ€å‘è¡¨äº
    *CVPR*ï¼Œ2004ã€‚'
- en: '[90] J.Â Jeon, J.Â Jung, and S.Â Lee, â€œDeep upright adjustment of 360 panoramas
    using multiple roll estimations,â€ *ACCV*, 2018.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Jeon, J. Jung å’Œ S. Leeï¼Œâ€œé€šè¿‡å¤šæ¬¡æ»šè½¬ä¼°è®¡å¯¹360åº¦å…¨æ™¯å›¾è¿›è¡Œæ·±åº¦ç«–ç›´è°ƒæ•´ï¼Œâ€ *ACCV*ï¼Œ2018ã€‚'
- en: '[91] R.Â Jung, A.Â S.Â J. Lee, A.Â Ashtari, and J.Â C. Bazin, â€œDeep360up: A deep
    learning-based approach for automatic vr image upright adjustment,â€ *VR*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. Jung, A. S. J. Lee, A. Ashtari å’Œ J. C. Bazinï¼Œâ€œDeep360upï¼šä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨VRå›¾åƒç«–ç›´è°ƒæ•´æ–¹æ³•ï¼Œâ€
    *VR*ï¼Œ2019ã€‚'
- en: '[92] R.Â Jung, S.Â Cho, and J.Â Kwon, â€œUpright adjustment with graph convolutional
    networks,â€ *ICIP*, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] R. Jung, S. Cho å’Œ J. Kwonï¼Œâ€œä½¿ç”¨å›¾å·ç§¯ç½‘ç»œçš„ç«–ç›´è°ƒæ•´ï¼Œâ€ *ICIP*ï¼Œ2020ã€‚'
- en: '[93] H.Â taek Lim, H.Â G. Kim, and Y.Â M. Ro, â€œVr iqa net: Deep virtual reality
    image quality assessment using adversarial learning,â€ *ICASSP*, 2018.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. taek Lim, H. G. Kim å’Œ Y. M. Roï¼Œâ€œVr iqa netï¼šåŸºäºå¯¹æŠ—å­¦ä¹ çš„æ·±åº¦è™šæ‹Ÿç°å®å›¾åƒè´¨é‡è¯„ä¼°ï¼Œâ€ *ICASSP*ï¼Œ2018ã€‚'
- en: '[94] C.Â Li, M.Â Xu, L.Â Jiang, S.Â Zhang, and X.Â Tao, â€œViewport proposal cnn for
    360^âˆ˜ video quality assessment,â€ *CVPR*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. Li, M. Xu, L. Jiang, S. Zhang å’Œ X. Taoï¼Œâ€œç”¨äº360^âˆ˜ è§†é¢‘è´¨é‡è¯„ä¼°çš„è§†å£æè®®CNNï¼Œâ€ *CVPR*ï¼Œ2019ã€‚'
- en: '[95] Y.Â Sun, A.Â Lu, and L.Â Yu, â€œWeighted-to-spherically-uniform quality evaluation
    for omnidirectional video,â€ *IEEE SPL*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Sun, A. Lu å’Œ L. Yuï¼Œâ€œç”¨äºå…¨æ™¯è§†é¢‘çš„åŠ æƒçƒé¢å‡åŒ€è´¨é‡è¯„ä¼°ï¼Œâ€ *IEEE SPL*ï¼Œ2017ã€‚'
- en: '[96] M.Â Xu, C.Â Li, Z.Â Chen, Z.Â Wang, and Z.Â Guan, â€œAssessing visual quality
    of omnidirectional videos,â€ *IEEE TCSVT*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] M. Xu, C. Li, Z. Chen, Z. Wang å’Œ Z. Guanï¼Œâ€œå…¨æ™¯è§†é¢‘çš„è§†è§‰è´¨é‡è¯„ä¼°ï¼Œâ€ *IEEE TCSVT*ï¼Œ2019ã€‚'
- en: '[97] H.Â G. Kim, H.Â taek Lim, and Y.Â M. Ro, â€œDeep virtual reality image quality
    assessment with human perception guider for omnidirectional image,â€ *IEEE TCSVT*,
    2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. G. Kim, H. taek Lim å’Œ Y. M. Roï¼Œâ€œåŸºäºäººç±»æ„ŸçŸ¥å¼•å¯¼çš„å…¨æ™¯å›¾åƒæ·±åº¦è™šæ‹Ÿç°å®å›¾åƒè´¨é‡è¯„ä¼°ï¼Œâ€ *IEEE TCSVT*ï¼Œ2020ã€‚'
- en: '[98] J.Â Xu, W.Â Zhou, and Z.Â Chen, â€œBlind omnidirectional image quality assessment
    with viewport oriented graph convolutional networks,â€ *IEEE TCSVT*, 2021.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Xu, W. Zhou å’Œ Z. Chenï¼Œâ€œä½¿ç”¨è§†å£å¯¼å‘å›¾å·ç§¯ç½‘ç»œçš„ç›²å…¨æ™¯å›¾åƒè´¨é‡è¯„ä¼°ï¼Œâ€ *IEEE TCSVT*ï¼Œ2021ã€‚'
- en: '[99] W.Â Zhou, J.Â Xu, Q.Â Jiang, and Z.Â Chen, â€œNo-reference quality assessment
    for 360-degree images by analysis of multifrequency information and local-global
    naturalness,â€ *IEEE TCSVT*, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. Zhou, J. Xu, Q. Jiang å’Œ Z. Chenï¼Œâ€œé€šè¿‡åˆ†æå¤šé¢‘ä¿¡æ¯å’Œå±€éƒ¨-å…¨å±€è‡ªç„¶åº¦å¯¹360åº¦å›¾åƒè¿›è¡Œæ— å‚è€ƒè´¨é‡è¯„ä¼°ï¼Œâ€
    *IEEE TCSVT*ï¼Œ2022ã€‚'
- en: '[100] W.Â Sun, X.Â Min, G.Â Z. S.Â K. Gu, H.Â Duan, and S.Â Ma, â€œMc360iqa: A multi-channel
    cnn for blind 360-degree image quality assessment,â€ *IEEE J-STSP*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] W. Sun, X. Min, G. Z. S. K. Gu, H. Duan å’Œ S. Maï¼Œâ€œMc360iqa: ä¸€ç§ç”¨äºç›²ç›®360åº¦å›¾åƒè´¨é‡è¯„ä¼°çš„å¤šé€šé“CNNï¼Œâ€
    *IEEE J-STSP*ï¼Œ2020ã€‚'
- en: '[101] R.Â G. deÂ AlbuquerqueÂ Azevedo, N.Â Birkbeck, I.Â Janatra, B.Â Adsumilli,
    and P.Â Frossard, â€œA viewport-driven multi-metric fusion approach for 360-degree
    video quality assessment,â€ *ICME*, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] R. G. de Albuquerque Azevedo, N. Birkbeck, I. Janatra, B. Adsumilli å’Œ
    P. Frossardï¼Œâ€œç”¨äº360åº¦è§†é¢‘è´¨é‡è¯„ä¼°çš„è§†å£é©±åŠ¨å¤šæŒ‡æ ‡èåˆæ–¹æ³•ï¼Œâ€ *ICME*ï¼Œ2020ã€‚'
- en: '[102] P.Â Gao, P.Â Zhang, and A.Â Smolic, â€œQuality assessment for omnidirectional
    video: A spatio-temporal distortion modeling approach,â€ *IEEE TMM*, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Gao, P. Zhang å’Œ A. Smolicï¼Œâ€œå…¨æ™¯è§†é¢‘çš„è´¨é‡è¯„ä¼°ï¼šä¸€ç§æ—¶ç©ºå¤±çœŸå»ºæ¨¡æ–¹æ³•ï¼Œâ€ *IEEE TMM*ï¼Œ2022ã€‚'
- en: '[103] P.Â Zhao, A.Â You, Y.Â Zhang, J.Â Liu, K.Â Bian, and Y.Â Tong, â€œSpherical criteria
    for fast and accurate 360 object detection,â€ in *AAAI*, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Zhao, A. You, Y. Zhang, J. Liu, K. Bian å’Œ Y. Tongï¼Œâ€œç”¨äºå¿«é€Ÿä¸”å‡†ç¡®çš„360ç‰©ä½“æ£€æµ‹çš„çƒé¢æ ‡å‡†ï¼Œâ€å‘è¡¨äº
    *AAAI*ï¼Œ2020ã€‚'
- en: '[104] M.Â Cao, S.Â Ikehata, and K.Â Aizawa, â€œField-of-view iou for object detection
    in 360 ^âˆ˜ images,â€ *arXiv*, 2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Cao, S. Ikehata å’Œ K. Aizawaï¼Œâ€œ360 ^âˆ˜ å›¾åƒä¸­çš„è§†åœºiouè¿›è¡Œç‰©ä½“æ£€æµ‹ï¼Œâ€ *arXiv*ï¼Œ2022ã€‚'
- en: '[105] G.Â Tong, H.Â Chen, Y.Â Li, X.Â Du, and Q.Â Zhang, â€œObject detection for panoramic
    images based on ms-rpn structure in traffic road scenes,â€ *IET Comput. Vis.*,
    2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] G. Tong, H. Chen, Y. Li, X. Du å’Œ Q. Zhangï¼Œâ€œåŸºäºms-rpnç»“æ„çš„äº¤é€šé“è·¯åœºæ™¯å…¨æ™¯å›¾åƒç‰©ä½“æ£€æµ‹ï¼Œâ€
    *IET Comput. Vis.*ï¼Œ2019ã€‚'
- en: '[106] K.-H. Wang and S.-H. Lai, â€œObject detection in curved space for 360-degree
    camera,â€ in *ICASSP*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] K.-H. Wang å’Œ S.-H. Laiï¼Œâ€œ360åº¦ç›¸æœºä¸­çš„æ›²é¢ç©ºé—´ç‰©ä½“æ£€æµ‹ï¼Œâ€å‘è¡¨äº *ICASSP*ï¼Œ2019ã€‚'
- en: '[107] W.Â Yang, Y.Â Qian, J.-K. KÃ¤mÃ¤rÃ¤inen, F.Â Cricri, and L.Â Fan, â€œObject detection
    in equirectangular panorama,â€ in *ICPR*, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] W. Yang, Y. Qian, J.-K. KÃ¤mÃ¤rÃ¤inen, F. Cricri å’Œ L. Fanï¼Œâ€œåœ¨ç­‰è·å…¨æ™¯å›¾ä¸­çš„ç‰©ä½“æ£€æµ‹ï¼Œâ€å‘è¡¨äº
    *ICPR*ï¼Œ2018ã€‚'
- en: '[108] K.Â Tateno, N.Â Navab, and F.Â Tombari, â€œDistortion-aware convolutional
    filters for dense prediction in panoramic images,â€ in *ECCV*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Tateno, N. Navab å’Œ F. Tombariï¼Œâ€œç”¨äºå…¨æ™¯å›¾åƒä¸­å¯†é›†é¢„æµ‹çš„å¤±çœŸæ„ŸçŸ¥å·ç§¯æ»¤æ³¢å™¨ï¼Œâ€å‘è¡¨äº *ECCV*ï¼Œ2018ã€‚'
- en: '[109] C.Â Ma, J.Â Zhang, K.Â Yang, A.Â Roitberg, and R.Â Stiefelhagen, â€œDensepass:
    Dense panoramic semantic segmentation via unsupervised domain adaptation with
    attention-augmented context exchange,â€ *ITSC*, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Ma, J. Zhang, K. Yang, A. Roitberg, å’Œ R. Stiefelhagenï¼Œâ€œDensepass:
    é€šè¿‡å¸¦æ³¨æ„åŠ›å¢å¼ºä¸Šä¸‹æ–‡äº¤æ¢çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”å®ç°å¯†é›†å…¨æ™¯è¯­ä¹‰åˆ†å‰²ï¼Œâ€ *ITSC*ï¼Œ2021å¹´ã€‚'
- en: '[110] J.Â Guerrero-Viu, C.Â Fernandez-Labrador, C.Â Demonceaux, and J.Â J. Guerrero,
    â€œWhatâ€™s in my room? object recognition on indoor panoramic images,â€ in *ICRA,
    2020*, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Guerrero-Viu, C. Fernandez-Labrador, C. Demonceaux, å’Œ J. J. Guerreroï¼Œâ€œæˆ‘æˆ¿é—´é‡Œæœ‰ä»€ä¹ˆï¼Ÿå®¤å†…å…¨æ™¯å›¾åƒçš„ç‰©ä½“è¯†åˆ«ï¼Œâ€
    åœ¨ *ICRA, 2020*ï¼Œ2020å¹´ã€‚'
- en: '[111] K.Â Yang, J.Â Zhang, S.Â ReiÃŸ, X.Â Hu, and R.Â Stiefelhagen, â€œCapturing omni-range
    context for omnidirectional segmentation,â€ *CVPR*, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] K. Yang, J. Zhang, S. ReiÃŸ, X. Hu, å’Œ R. Stiefelhagenï¼Œâ€œæ•æ‰å…¨èŒƒå›´ä¸Šä¸‹æ–‡ç”¨äºå…¨å‘åˆ†å‰²ï¼Œâ€
    *CVPR*ï¼Œ2021å¹´ã€‚'
- en: '[112] J.Â Zhang, K.Â Yang, C.Â Ma, S.Â ReiÃŸ, K.Â Peng, and R.Â Stiefelhagen, â€œBending
    reality: Distortion-aware transformers for adapting to panoramic semantic segmentation,â€
    *arXiv*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Zhang, K. Yang, C. Ma, S. ReiÃŸ, K. Peng, å’Œ R. Stiefelhagenï¼Œâ€œå¼¯æ›²ç°å®ï¼šç”¨äºé€‚åº”å…¨æ™¯è¯­ä¹‰åˆ†å‰²çš„å¤±çœŸæ„ŸçŸ¥å˜æ¢å™¨ï¼Œâ€
    *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[113] J.Â Zhang, C.Â Ma, K.Â Yang, A.Â Roitberg, K.Â Peng, and R.Â Stiefelhagen,
    â€œTransfer beyond the field of view: Dense panoramic semantic segmentation via
    unsupervised domain adaptation,â€ *IEEE Trans. on Intell. Trans. Sys.*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, å’Œ R. Stiefelhagenï¼Œâ€œè¶…è¶Šè§†åœºï¼šé€šè¿‡æ— ç›‘ç£é¢†åŸŸé€‚åº”å®ç°å¯†é›†å…¨æ™¯è¯­ä¹‰åˆ†å‰²ï¼Œâ€
    *IEEE Trans. on Intell. Trans. Sys.*ï¼Œ2021å¹´ã€‚'
- en: '[114] L.Â Deng, M.Â Yang, Y.Â Qian, C.Â Wang, and B.Â Wang, â€œCnn based semantic
    segmentation for urban traffic scenes using fisheye camera,â€ *IV*, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] L. Deng, M. Yang, Y. Qian, C. Wang, å’Œ B. Wangï¼Œâ€œåŸºäº CNN çš„åŸå¸‚äº¤é€šåœºæ™¯è¯­ä¹‰åˆ†å‰²ï¼Œä½¿ç”¨é±¼çœ¼æ‘„åƒå¤´ï¼Œâ€
    *IV*ï¼Œ2017å¹´ã€‚'
- en: '[115] L.Â Deng, M.Â Yang, H.Â Li, T.Â Li, B.Â Hu, and C.Â Wang, â€œRestricted deformable
    convolution-based road scene semantic segmentation using surround view cameras,â€
    *IEEE Trans. Intell. Transp. Syst.*, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] L. Deng, M. Yang, H. Li, T. Li, B. Hu, å’Œ C. Wangï¼Œâ€œåŸºäºé™åˆ¶æ€§å¯å˜å½¢å·ç§¯çš„é“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²ï¼Œä½¿ç”¨ç¯è§†æ‘„åƒå¤´ï¼Œâ€
    *IEEE Trans. Intell. Transp. Syst.*ï¼Œ2020å¹´ã€‚'
- en: '[116] S.Â Orhan and Y.Â Bastanlar, â€œSemantic segmentation of outdoor panoramic
    images,â€ *Signal Image Video Process.*, 2022.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. Orhan å’Œ Y. Bastanlarï¼Œâ€œæˆ·å¤–å…¨æ™¯å›¾åƒçš„è¯­ä¹‰åˆ†å‰²ï¼Œâ€ *Signal Image Video Process.*ï¼Œ2022å¹´ã€‚'
- en: '[117] C.Â Fernandez-Labrador, J.Â M. FÃ¡cil, A.Â PÃ©rez-Yus, C.Â Demonceaux, J.Â Civera,
    and J.Â J. Guerrero, â€œCorners for layout: End-to-end layout recovery from 360 images,â€
    *IEEE Robot. Autom. Lett.*, 2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Fernandez-Labrador, J. M. FÃ¡cil, A. PÃ©rez-Yus, C. Demonceaux, J. Civera,
    å’Œ J. J. Guerreroï¼Œâ€œå¸ƒå±€è§’è½ï¼šä» 360 å›¾åƒä¸­ç«¯åˆ°ç«¯çš„å¸ƒå±€æ¢å¤ï¼Œâ€ *IEEE Robot. Autom. Lett.*ï¼Œ2020å¹´ã€‚'
- en: '[118] K.Â Yang, X.Â Hu, L.Â M. Bergasa, E.Â Romera, X.Â Huang, D.Â Sun, and K.Â Wang,
    â€œCan we pass beyond the field of view? panoramic annular semantic segmentation
    for real-world surrounding perception,â€ *IV*, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Yang, X. Hu, L. M. Bergasa, E. Romera, X. Huang, D. Sun, å’Œ K. Wangï¼Œâ€œæˆ‘ä»¬èƒ½è¶…è¶Šè§†åœºå—ï¼Ÿç”¨äºç°å®ä¸–ç•Œå‘¨è¾¹æ„ŸçŸ¥çš„å…¨æ™¯ç¯å½¢è¯­ä¹‰åˆ†å‰²ï¼Œâ€
    *IV*ï¼Œ2019å¹´ã€‚'
- en: '[119] K.Â Yang, X.Â Hu, H.Â Chen, K.Â Xiang, K.Â Wang, and R.Â Stiefelhagen, â€œDs-pass:
    Detail-sensitive panoramic annular semantic segmentation through swaftnet for
    surrounding sensing,â€ *IV*, 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Yang, X. Hu, H. Chen, K. Xiang, K. Wang, å’Œ R. Stiefelhagenï¼Œâ€œDs-pass:
    é€šè¿‡ swaftnet å®ç°è¯¦ç»†æ•æ„Ÿçš„å…¨æ™¯ç¯å½¢è¯­ä¹‰åˆ†å‰²ï¼Œç”¨äºå‘¨è¾¹æ„ŸçŸ¥ï¼Œâ€ *IV*ï¼Œ2020å¹´ã€‚'
- en: '[120] A.Â Jaus, K.Â Yang, and R.Â Stiefelhagen, â€œPanoramic panoptic segmentation:
    Towards complete surrounding understanding via unsupervised contrastive learning,â€
    *IV*, 2021.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Jaus, K. Yang, å’Œ R. Stiefelhagenï¼Œâ€œå…¨æ™¯å…¨æ™¯åˆ†å‰²ï¼šé€šè¿‡æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ å®ç°å®Œæ•´çš„å‘¨è¾¹ç†è§£ï¼Œâ€ *IV*ï¼Œ2021å¹´ã€‚'
- en: '[121] K.Â Yang, X.Â Hu, Y.Â Fang, K.Â Wang, and R.Â Stiefelhagen, â€œOmnisupervised
    omnidirectional semantic segmentation,â€ *IEEE Trans. Intell. Transp. Syst.*, 2022.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] K. Yang, X. Hu, Y. Fang, K. Wang, å’Œ R. Stiefelhagenï¼Œâ€œå…¨å‘ç›‘ç£çš„å…¨å‘è¯­ä¹‰åˆ†å‰²ï¼Œâ€ *IEEE
    Trans. Intell. Transp. Syst.*ï¼Œ2022å¹´ã€‚'
- en: '[122] N.Â Zioulis, A.Â Karakottas, D.Â Zarpalas, and P.Â Daras, â€œOmnidepth: Dense
    depth estimation for indoors spherical panoramas,â€ in *ECCV*, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] N. Zioulis, A. Karakottas, D. Zarpalas, å’Œ P. Darasï¼Œâ€œOmnidepth: å®¤å†…çƒå½¢å…¨æ™¯çš„å¯†é›†æ·±åº¦ä¼°è®¡ï¼Œâ€
    åœ¨ *ECCV*ï¼Œ2018å¹´ã€‚'
- en: '[123] G.Â Pintore, E.Â Almansa, and J.Â Schneider, â€œSlicenet: deep dense depth
    estimation from a single indoor panorama using a slice-based representation,â€
    *CVPR*, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. Pintore, E. Almansa, å’Œ J. Schneiderï¼Œâ€œSlicenet: ä»å•ä¸€å®¤å†…å…¨æ™¯å›¾åƒä¸­åŸºäºåˆ‡ç‰‡çš„è¡¨ç¤ºè¿›è¡Œæ·±åº¦å¯†é›†æ·±åº¦ä¼°è®¡ï¼Œâ€
    *CVPR*ï¼Œ2021å¹´ã€‚'
- en: '[124] I.Â Laina, C.Â Rupprecht, V.Â Belagiannis, F.Â Tombari, and N.Â Navab, â€œDeeper
    depth prediction with fully convolutional residual networks,â€ in *3DV*, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, å’Œ N. Navabï¼Œâ€œé€šè¿‡å…¨å·ç§¯æ®‹å·®ç½‘ç»œè¿›è¡Œæ›´æ·±çš„æ·±åº¦é¢„æµ‹ï¼Œâ€
    åœ¨ *3DV*ï¼Œ2016å¹´ã€‚'
- en: '[125] C.Â Zhuang, Z.Â Lu, Y.Â Wang, J.Â Xiao, and Y.Â Wang, â€œAcdnet: Adaptively
    combined dilated convolution for monocular panorama depth estimation,â€ *arXiv*,
    2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] C. Zhuang, Z. Lu, Y. Wang, J. Xiao, å’Œ Y. Wangï¼Œâ€œAcdnet: é€‚åº”æ€§ç»“åˆè†¨èƒ€å·ç§¯è¿›è¡Œå•ç›®å…¨æ™¯æ·±åº¦ä¼°è®¡ï¼Œâ€
    *arXiv*ï¼Œ2021å¹´ã€‚'
- en: '[126] L.Â Jin, Y.Â Xu, J.Â Zheng, J.Â Zhang, R.Â Tang, S.Â Xu, J.Â Yu, and S.Â Gao,
    â€œGeometric structure based and regularized depth estimation from 360 indoor imagery,â€
    in *CVPR*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] L. Jin, Y. Xu, J. Zheng, J. Zhang, R. Tang, S. Xu, J. Yu, å’Œ S. Gao, â€œåŸºäºå‡ ä½•ç»“æ„å’Œæ­£åˆ™åŒ–çš„360å®¤å†…å›¾åƒæ·±åº¦ä¼°è®¡ï¼Œâ€
    åœ¨ *CVPR*ï¼Œ2020å¹´ã€‚'
- en: '[127] F.-E. Wang, H.-N. Hu, H.-T. Cheng, J.-T. Lin, S.-T. Yang, M.-L. Shih,
    H.-K. Chu, and M.Â Sun, â€œSelf-supervised learning of depth and camera motion from
    360^âˆ˜ videos,â€ in *ACCV*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] F.-E. Wang, H.-N. Hu, H.-T. Cheng, J.-T. Lin, S.-T. Yang, M.-L. Shih,
    H.-K. Chu, å’Œ M. Sun, â€œä»360^âˆ˜è§†é¢‘ä¸­è‡ªç›‘ç£å­¦ä¹ æ·±åº¦å’Œç›¸æœºè¿åŠ¨ï¼Œâ€ åœ¨ *ACCV*ï¼Œ2018å¹´ã€‚'
- en: '[128] I.Â Yun, H.-J. Lee, and C.Â E. Rhee, â€œImproving 360 monocular depth estimation
    via non-local dense prediction transformer and joint supervised and self-supervised
    learning,â€ *arXiv*, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] I. Yun, H.-J. Lee, å’Œ C. E. Rhee, â€œé€šè¿‡éå±€éƒ¨å¯†é›†é¢„æµ‹å˜æ¢å™¨å’Œè”åˆç›‘ç£ä¸è‡ªç›‘ç£å­¦ä¹ æ”¹è¿›360å•ç›®æ·±åº¦ä¼°è®¡ï¼Œâ€
    *arXiv*ï¼Œ2021å¹´ã€‚'
- en: '[129] Z.Â Wang, A.Â C. Bovik, H.Â R. Sheikh, and E.Â P. Simoncelli, â€œImage quality
    assessment: from error visibility to structural similarity,â€ *IEEE TIP*, 2004.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, å’Œ E. P. Simoncelli, â€œå›¾åƒè´¨é‡è¯„ä¼°ï¼šä»è¯¯å·®å¯è§æ€§åˆ°ç»“æ„ç›¸ä¼¼æ€§ï¼Œâ€
    *IEEE TIP*ï¼Œ2004å¹´ã€‚'
- en: '[130] Q.Â Feng, H.Â P. Shum, and S.Â Morishima, â€œ360 depth estimation in the wild-the
    depth360 dataset and the segfuse network,â€ in *VR*, 2022.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Q. Feng, H. P. Shum, å’Œ S. Morishima, â€œé‡å¤–360æ·±åº¦ä¼°è®¡â€”â€”depth360æ•°æ®é›†å’Œsegfuseç½‘ç»œï¼Œâ€
    åœ¨ *VR*ï¼Œ2022å¹´ã€‚'
- en: '[131] M.Â Eder, P.Â Moulon, and L.Â Guan, â€œPano popups: Indoor 3d reconstruction
    with a plane-aware network,â€ in *3DV*, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Eder, P. Moulon, å’Œ L. Guan, â€œPano popups: åŸºäºå¹³é¢æ„ŸçŸ¥ç½‘ç»œçš„å®¤å†…3Dé‡å»ºï¼Œâ€ åœ¨ *3DV*ï¼Œ2019å¹´ã€‚'
- en: '[132] X.Â Shi, Z.Â Chen, H.Â Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, â€œConvolutional
    lstm network: A machine learning approach for precipitation nowcasting,â€ *NIPS*,
    2015.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, å’Œ W.-c. Woo, â€œå·ç§¯LSTMç½‘ç»œï¼šç”¨äºé™æ°´å³æ—¶é¢„æŠ¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œâ€
    *NIPS*ï¼Œ2015å¹´ã€‚'
- en: '[133] H.Â Jiang, Z.Â Sheng, S.Â Zhu, Z.Â Dong, and R.Â Huang, â€œUnifuse: Unidirectional
    fusion for 360 panorama depth estimation,â€ *IEEE Robot. Autom. Lett.*, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. Jiang, Z. Sheng, S. Zhu, Z. Dong, å’Œ R. Huang, â€œUnifuse: ç”¨äº360å…¨æ™¯æ·±åº¦ä¼°è®¡çš„å•å‘èåˆï¼Œâ€
    *IEEE Robot. Autom. Lett.*ï¼Œ2021å¹´ã€‚'
- en: '[134] J.Â Bai, S.Â Lai, H.Â Qin, J.Â Guo, and Y.Â Guo, â€œGlpanodepth: Global-to-local
    panoramic depth estimation,â€ *arXiv*, 2022.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] J. Bai, S. Lai, H. Qin, J. Guo, å’Œ Y. Guo, â€œGlpanodepth: ä»å…¨å±€åˆ°å±€éƒ¨çš„å…¨æ™¯æ·±åº¦ä¼°è®¡ï¼Œâ€
    *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[135] R.Â Ranftl, A.Â Bochkovskiy, and V.Â Koltun, â€œVision transformers for dense
    prediction,â€ in *ICCV*, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] R. Ranftl, A. Bochkovskiy, å’Œ V. Koltun, â€œç”¨äºå¯†é›†é¢„æµ‹çš„è§†è§‰å˜æ¢å™¨ï¼Œâ€ åœ¨ *ICCV*ï¼Œ2021å¹´ã€‚'
- en: '[136] B.Â Y. Feng, W.Â Yao, Z.Â Liu, and A.Â Varshney, â€œDeep depth estimation on
    360 images with a double quaternion loss,â€ in *3DV*, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] B. Y. Feng, W. Yao, Z. Liu, å’Œ A. Varshney, â€œé€šè¿‡åŒå››å…ƒæ•°æŸå¤±å¯¹360å›¾åƒè¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œâ€ åœ¨
    *3DV*ï¼Œ2020å¹´ã€‚'
- en: '[137] A.Â Apitzsch, R.Â Seidel, and G.Â Hirtz, â€œCubes3d: Neural network based
    optical flow in omnidirectional image scenes,â€ *arXiv*, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Apitzsch, R. Seidel, å’Œ G. Hirtz, â€œCubes3d: åŸºäºç¥ç»ç½‘ç»œçš„å…¨æ™¯å›¾åƒåœºæ™¯ä¸­çš„å…‰æµä¼°è®¡ï¼Œâ€ *arXiv*ï¼Œ2018å¹´ã€‚'
- en: '[138] S.Â Xie, P.Â K. Lai, R.Â LaganiÃ¨re, and J.Â Lang, â€œEffective convolutional
    neural network layers in flow estimation for omni-directional images,â€ *3DV*,
    2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] S. Xie, P. K. Lai, R. LaganiÃ¨re, å’Œ J. Lang, â€œåœ¨å…¨å‘å›¾åƒçš„æµé‡ä¼°è®¡ä¸­çš„æœ‰æ•ˆå·ç§¯ç¥ç»ç½‘ç»œå±‚ï¼Œâ€
    *3DV*ï¼Œ2019å¹´ã€‚'
- en: '[139] C.-O. Artizzu, H.Â Zhang, G.Â Allibert, and C.Â Demonceaux, â€œOmniflownet:
    a perspective neural network adaptation for optical flow estimation in omnidirectional
    images,â€ *ICPR*, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] C.-O. Artizzu, H. Zhang, G. Allibert, å’Œ C. Demonceaux, â€œOmniflownet:
    ç”¨äºå…¨æ™¯å›¾åƒçš„å…‰æµä¼°è®¡çš„è§†è§’ç¥ç»ç½‘ç»œé€‚é…ï¼Œâ€ *ICPR*ï¼Œ2021å¹´ã€‚'
- en: '[140] K.Â Bhandari, Z.Â Zong, and Y.Â Yan, â€œRevisiting optical flow estimation
    in 360 videos,â€ *ICPR*, 2021.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. Bhandari, Z. Zong, å’Œ Y. Yan, â€œé‡æ–°å®¡è§†360è§†é¢‘ä¸­çš„å…‰æµä¼°è®¡ï¼Œâ€ *ICPR*ï¼Œ2021å¹´ã€‚'
- en: '[141] T.-W. Hui, X.Â Tang, and C.Â C. Loy, â€œLiteflownet: A lightweight convolutional
    neural network for optical flow estimation,â€ *CVPR*, 2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T.-W. Hui, X. Tang, å’Œ C. C. Loy, â€œLiteflownet: ç”¨äºå…‰æµä¼°è®¡çš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ *CVPR*ï¼Œ2018å¹´ã€‚'
- en: '[142] H.Â Shi, Y.Â Zhou, K.Â Yang, Y.Â Ye, X.Â Yin, Z.Â Yin, S.Â Meng, and K.Â Wang,
    â€œPanoflow: Learning optical flow for panoramic images,â€ *arXiv*, 2022.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] H. Shi, Y. Zhou, K. Yang, Y. Ye, X. Yin, Z. Yin, S. Meng, å’Œ K. Wang,
    â€œPanoflow: ç”¨äºå…¨æ™¯å›¾åƒçš„å…‰æµå­¦ä¹ ï¼Œâ€ *arXiv*ï¼Œ2022å¹´ã€‚'
- en: '[143] Y.Â Su, D.Â Jayaraman, and K.Â Grauman, â€œPano2vid: Automatic cinematography
    for watching 360^âˆ˜ videos,â€ in *ACCV*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Su, D. Jayaraman, å’Œ K. Grauman, â€œPano2vid: è‡ªåŠ¨åŒ–æ‘„å½±æŠ€æœ¯ç”¨äºè§‚çœ‹360^âˆ˜è§†é¢‘ï¼Œâ€ åœ¨
    *ACCV*ï¼Œ2016å¹´ã€‚'
- en: '[144] Y.Â Su and K.Â Grauman, â€œMaking 360^âˆ˜ video watchable in 2d: Learning videography
    for click free viewing,â€ in *CVPR*, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Su å’Œ K. Grauman, â€œä½¿360^âˆ˜è§†é¢‘åœ¨2Dä¸­å¯è§‚çœ‹ï¼šå­¦ä¹ ç‚¹å‡»æ— ç¼è§‚çœ‹çš„è§†é¢‘æ‘„å½±æŠ€æœ¯ï¼Œâ€ åœ¨ *CVPR*ï¼Œ2017å¹´ã€‚'
- en: '[145] Y.Â Yu, S.Â Lee, J.Â Na, J.Â Kang, and G.Â Kim, â€œA deep ranking model for
    spatio-temporal highlight detection from a 360^âˆ˜ video,â€ in *AAAI*, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. Yu, S. Lee, J. Na, J. Kang, å’Œ G. Kim, â€œç”¨äº360^âˆ˜è§†é¢‘çš„æ—¶ç©ºäº®ç‚¹æ£€æµ‹çš„æ·±åº¦æ’åæ¨¡å‹ï¼Œâ€ åœ¨
    *AAAI*ï¼Œ2018å¹´ã€‚'
- en: '[146] S.Â Lee, J.Â Sung, Y.Â Yu, and G.Â Kim, â€œA memory network approach for story-based
    temporal summarization of 360^âˆ˜ videos,â€ in *CVPR*, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Lee, J. Sung, Y. Yu, å’Œ G. Kim, â€œåŸºäºè®°å¿†ç½‘ç»œçš„æ–¹æ³•è¿›è¡Œ360^âˆ˜è§†é¢‘çš„æ•…äº‹åŒ–æ—¶é—´æ€»ç»“ï¼Œâ€ *CVPR*,
    2018ã€‚'
- en: '[147] C.Â Zhang, Z.Â Cui, C.Â Chen, S.Â Liu, B.Â Zeng, H.Â Bao, and Y.Â Zhang, â€œDeeppanocontext:
    Panoramic 3d scene understanding with holistic scene context graph and relation-based
    optimization,â€ *ICCV*, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] C. Zhang, Z. Cui, C. Chen, S. Liu, B. Zeng, H. Bao, å’Œ Y. Zhang, â€œDeeppanocontext:
    å…·æœ‰æ•´ä½“åœºæ™¯ä¸Šä¸‹æ–‡å›¾å’ŒåŸºäºå…³ç³»çš„ä¼˜åŒ–çš„å…¨æ™¯3Dåœºæ™¯ç†è§£ï¼Œâ€ *ICCV*, 2021ã€‚'
- en: '[148] S.-T. Yang, F.-E. Wang, C.-H. Peng, P.Â Wonka, M.Â Sun, and H.Â kuo Chu,
    â€œDula-net: A dual-projection network for estimating room layouts from a single
    rgb panorama,â€ *CVPR*, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S.-T. Yang, F.-E. Wang, C.-H. Peng, P. Wonka, M. Sun, å’Œ H. kuo Chu, â€œDula-net:
    ç”¨äºä»å•å¼ RGBå…¨æ™¯å›¾åƒä¼°è®¡æˆ¿é—´å¸ƒå±€çš„åŒæŠ•å½±ç½‘ç»œï¼Œâ€ *CVPR*, 2019ã€‚'
- en: '[149] C.Â Zou, A.Â Colburn, Q.Â Shan, and D.Â Hoiem, â€œLayoutnet: Reconstructing
    the 3d room layout from a single rgb image,â€ *CVPR*, 2018.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] C. Zou, A. Colburn, Q. Shan, å’Œ D. Hoiem, â€œLayoutnet: ä»å•å¼ RGBå›¾åƒé‡å»º3Dæˆ¿é—´å¸ƒå±€ï¼Œâ€
    *CVPR*, 2018ã€‚'
- en: '[150] P.Â V. Tran, â€œSslayout360: Semi-supervised indoor layout estimation from
    360^âˆ˜ panorama,â€ *CVPR*, 2021.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] P. V. Tran, â€œSslayout360: ä»360^âˆ˜å…¨æ™¯å›¾ä¸­è¿›è¡ŒåŠç›‘ç£çš„å®¤å†…å¸ƒå±€ä¼°è®¡ï¼Œâ€ *CVPR*, 2021ã€‚'
- en: '[151] G.Â Pintore, M.Â Agus, and E.Â Gobbetti, â€œAtlantanet: Inferring the 3d indoor
    layout from a single $360^{\circ}$ image beyond the manhattan world assumption,â€
    in *ECCV*, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] G. Pintore, M. Agus, å’Œ E. Gobbetti, â€œAtlantanet: ä»å•å¼  $360^{\circ}$ å›¾åƒä¸­æ¨æ–­3Då®¤å†…å¸ƒå±€ï¼Œè¶…è¶Šæ›¼å“ˆé¡¿ä¸–ç•Œå‡è®¾ï¼Œâ€
    *ECCV*, 2020ã€‚'
- en: '[152] C.Â Sun, C.-W. Hsiao, M.Â Sun, and H.-T. Chen, â€œHorizonnet: Learning room
    layout with 1d representation and pano stretch data augmentation,â€ *CVPR*, 2019.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] C. Sun, C.-W. Hsiao, M. Sun, å’Œ H.-T. Chen, â€œHorizonnet: åˆ©ç”¨1Dè¡¨ç¤ºå’Œå…¨æ™¯æ‹‰ä¼¸æ•°æ®å¢å¼ºè¿›è¡Œæˆ¿é—´å¸ƒå±€å­¦ä¹ ï¼Œâ€
    *CVPR*, 2019ã€‚'
- en: '[153] C.Â Sun, M.Â Sun, and H.-T. Chen, â€œHohonet: 360 indoor holistic understanding
    with latent horizontal features,â€ *CVPR*, 2021.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] C. Sun, M. Sun, å’Œ H.-T. Chen, â€œHohonet: åˆ©ç”¨æ½œåœ¨æ°´å¹³ç‰¹å¾è¿›è¡Œ360å®¤å†…æ•´ä½“ç†è§£ï¼Œâ€ *CVPR*,
    2021ã€‚'
- en: '[154] F.-E. Wang, Y.-H. Yeh, M.Â Sun, W.-C. Chiu, and Y.-H. Tsai, â€œLed2-net:
    Monocular 360^âˆ˜ layout estimation via differentiable depth rendering,â€ *CVPR*,
    2021.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, å’Œ Y.-H. Tsai, â€œLed2-net: é€šè¿‡å¯å¾®åˆ†æ·±åº¦æ¸²æŸ“è¿›è¡Œå•ç›®360^âˆ˜å¸ƒå±€ä¼°è®¡ï¼Œâ€
    *CVPR*, 2021ã€‚'
- en: '[155] J.Â Seuffert, A.Â P. Grassi, T.Â Scheck, and G.Â Hirtz, â€œA study on the influence
    of omnidirectional distortion on cnn-based stereo vision,â€ in *VISIGRAPP*, 2021.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Seuffert, A. P. Grassi, T. Scheck, å’Œ G. Hirtz, â€œå…¨å‘å¤±çœŸå¯¹åŸºäºCNNçš„ç«‹ä½“è§†è§‰å½±å“çš„ç ”ç©¶ï¼Œâ€
    *VISIGRAPP*, 2021ã€‚'
- en: '[156] C.Â Won, J.Â Ryu, and J.Â Lim, â€œSweepnet: Wide-baseline omnidirectional
    depth estimation,â€ *ICRA*, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] C. Won, J. Ryu, å’Œ J. Lim, â€œSweepnet: å®½åŸºçº¿å…¨å‘æ·±åº¦ä¼°è®¡ï¼Œâ€ *ICRA*, 2019ã€‚'
- en: '[157] â€”â€”, â€œOmnimvs: End-to-end learning for omnidirectional stereo matching,â€
    *ICCV*, 2019.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] â€”â€”, â€œOmnimvs: å…¨å‘ç«‹ä½“åŒ¹é…çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œâ€ *ICCV*, 2019ã€‚'
- en: '[158] â€”â€”, â€œEnd-to-end learning for omnidirectional stereo matching with uncertainty
    prior,â€ *IEEE TPAMI*, 2021.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] â€”â€”, â€œç”¨äºå…¨å‘ç«‹ä½“åŒ¹é…çš„ä¸ç¡®å®šæ€§å…ˆéªŒçš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œâ€ *IEEE TPAMI*, 2021ã€‚'
- en: '[159] N.-H. Wang, B.Â Solarte, Y.-H. Tsai, W.-C. Chiu, and M.Â Sun, â€œ360sd-net:
    360^âˆ˜ stereo depth estimation with learnable cost volume,â€ *ICRA*, 2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] N.-H. Wang, B. Solarte, Y.-H. Tsai, W.-C. Chiu, å’Œ M. Sun, â€œ360sd-net:
    å¯å­¦ä¹ æˆæœ¬ä½“ç§¯çš„360^âˆ˜ç«‹ä½“æ·±åº¦ä¼°è®¡ï¼Œâ€ *ICRA*, 2020ã€‚'
- en: '[160] A.Â Chiuso, P.Â Favaro, H.Â Jin, and S.Â Soatto, â€œStructure from motion causally
    integrated over time,â€ *IEEE TPAMI*, 2002.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Chiuso, P. Favaro, H. Jin, å’Œ S. Soatto, â€œç»“æ„ä»è¿åŠ¨ä¸­æŒ‰æ—¶é—´å› æœé›†æˆï¼Œâ€ *IEEE TPAMI*,
    2002ã€‚'
- en: '[161] R.Â Mur-Artal and J.Â D. TardÃ³s, â€œOrb-slam2: An open-source slam system
    for monocular, stereo, and rgb-d cameras,â€ *IEEE T-RO*, 2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R. Mur-Artal å’Œ J. D. TardÃ³s, â€œOrb-slam2: ä¸€ä¸ªå¼€æºçš„å•ç›®ã€ç«‹ä½“å’ŒRGB-Dç›¸æœºSLAMç³»ç»Ÿï¼Œâ€ *IEEE
    T-RO*, 2017ã€‚'
- en: '[162] R.Â Mur-Artal, J.Â M.Â M. Montiel, and J.Â D. TardÃ³s, â€œOrb-slam: A versatile
    and accurate monocular slam system,â€ *IEEE T-RO*, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] R. Mur-Artal, J. M. M. Montiel, å’Œ J. D. TardÃ³s, â€œOrb-slam: ä¸€ç§å¤šåŠŸèƒ½ä¸”å‡†ç¡®çš„å•ç›®SLAMç³»ç»Ÿï¼Œâ€
    *IEEE T-RO*, 2015ã€‚'
- en: '[163] S.Â Urban and S.Â Hinz, â€œMulticol-slam-a modular real-time multi-camera
    slam system,â€ *arXiv*, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] S. Urban å’Œ S. Hinz, â€œMulticol-slam-ä¸€ä¸ªæ¨¡å—åŒ–å®æ—¶å¤šæ‘„åƒå¤´SLAMç³»ç»Ÿï¼Œâ€ *arXiv*, 2016ã€‚'
- en: '[164] D.Â Caruso, J.Â J. Engel, and D.Â Cremers, â€œLarge-scale direct slam for
    omnidirectional cameras,â€ *IROS*, 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] D. Caruso, J. J. Engel, å’Œ D. Cremers, â€œå¤§è§„æ¨¡ç›´æ¥SLAMç”¨äºå…¨å‘ç›¸æœºï¼Œâ€ *IROS*, 2015ã€‚'
- en: '[165] Z.Â Teed and J.Â Deng, â€œDroid-slam: Deep visual slam for monocular, stereo,
    and rgb-d cameras,â€ in *NIPS*, 2021.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Z. Teed å’Œ J. Deng, â€œDroid-slam: å•ç›®ã€ç«‹ä½“å’ŒRGB-Dç›¸æœºçš„æ·±åº¦è§†è§‰SLAMï¼Œâ€ *NIPS*, 2021ã€‚'
- en: '[166] J.Â Czarnowski, T.Â Laidlow, R.Â Clark, and A.Â J. Davison, â€œDeepfactors:
    Real-time probabilistic dense monocular slam,â€ *IEEE Robot. Autom. Lett.*, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Czarnowski, T. Laidlow, R. Clark, å’Œ A. J. Davison, â€œDeepfactors: å®æ—¶æ¦‚ç‡å¯†é›†å•ç›®SLAMï¼Œâ€
    *IEEE Robot. Autom. Lett.*, 2020ã€‚'
- en: '[167] F.Â Dai, Y.Â Zhang, Y.Â Ma, H.Â Li, and Q.Â Zhao, â€œDilated convolutional neural
    networks for panoramic image saliency prediction,â€ *ICASSP*, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] F. æˆ´, Y. å¼ , Y. é©¬, H. æ, å’Œ Q. èµµ, â€œç”¨äºå…¨æ™¯å›¾åƒæ˜¾è‘—æ€§é¢„æµ‹çš„æ‰©å¼ å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ *ICASSP*ï¼Œ2020å¹´ã€‚'
- en: '[168] H.Â Lv, Q.Â Yang, C.Â Li, W.Â Dai, J.Â Zou, and H.Â Xiong, â€œSalgcn: Saliency
    prediction for 360-degree images based on spherical graph convolutional networks,â€
    *ACM MM*, 2020.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. å•, Q. æ¨, C. æ, W. æˆ´, J. é‚¹, å’Œ H. ç†Š, â€œSalgcnï¼šåŸºäºçƒé¢å›¾å·ç§¯ç½‘ç»œçš„360åº¦å›¾åƒæ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€
    *ACM MM*ï¼Œ2020å¹´ã€‚'
- en: '[169] F.-Y. Chao, L.Â Zhang, W.Â Hamidouche, and O.Â DÃ©forges, â€œA multi-fov viewport-based
    visual saliency model using adaptive weighting losses for 360^âˆ˜ images,â€ *IEEE
    TMM*, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] F.-Y. Chao, L. å¼ , W. Hamidouche, å’Œ O. DÃ©forges, â€œä¸€ç§åŸºäºå¤šè§†åœºè§†å£çš„è§†è§‰æ˜¾è‘—æ€§æ¨¡å‹ï¼Œä½¿ç”¨è‡ªé€‚åº”åŠ æƒæŸå¤±è¿›è¡Œ360^âˆ˜å›¾åƒçš„å¤„ç†ï¼Œâ€
    *IEEE TMM*ï¼Œ2021å¹´ã€‚'
- en: '[170] Y.Â Abdelaziz, D.Â Djilali, T.Â Krishna, K.Â McGuinness, and N.Â E. Oâ€™Connor,
    â€œRethinking 360^âˆ˜ image visual attention modelling with unsupervised learning,â€
    *ICCV*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Y. Abdelaziz, D. Djilali, T. Krishna, K. McGuinness, å’Œ N. E. Oâ€™Connor,
    â€œç”¨æ— ç›‘ç£å­¦ä¹ é‡æ–°æ€è€ƒ360^âˆ˜å›¾åƒè§†è§‰æ³¨æ„å»ºæ¨¡ï¼Œâ€ *ICCV*ï¼Œ2021å¹´ã€‚'
- en: '[171] M.Â Xu, L.Â Yang, X.Â Tao, Y.Â Duan, and Z.Â Wang, â€œSaliency prediction on
    omnidirectional image with generative adversarial imitation learning,â€ *IEEE TIP*,
    2021.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. å¾, L. æ¨, X. é™¶, Y. æ®µ, å’Œ Z. ç‹, â€œä½¿ç”¨ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ è¿›è¡Œå…¨å‘å›¾åƒçš„æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *IEEE TIP*ï¼Œ2021å¹´ã€‚'
- en: '[172] A.Â Nguyen, Z.Â Yan, and K.Â Nahrstedt, â€œYour attention is unique: Detecting
    360-degree video saliency in head-mounted display for head movement prediction,â€
    *ACM MM*, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Nguyen, Z. Yan, å’Œ K. Nahrstedt, â€œä½ çš„æ³¨æ„åŠ›æ˜¯ç‹¬ç‰¹çš„ï¼šåœ¨å¤´æˆ´æ˜¾ç¤ºå™¨ä¸­æ£€æµ‹360åº¦è§†é¢‘æ˜¾è‘—æ€§ï¼Œä»¥é¢„æµ‹å¤´éƒ¨è¿åŠ¨ï¼Œâ€
    *ACM MM*ï¼Œ2018å¹´ã€‚'
- en: '[173] Z.Â Zhang, Y.Â Xu, J.Â Yu, and S.Â Gao, â€œSaliency detection in 360^âˆ˜ videos,â€
    in *ECCV*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z. å¼ , Y. å¾, J. äº, å’Œ S. é«˜, â€œ360^âˆ˜è§†é¢‘ä¸­çš„æ˜¾è‘—æ€§æ£€æµ‹ï¼Œâ€ è§ *ECCV*ï¼Œ2018å¹´ã€‚'
- en: '[174] M.Â Xu, Y.Â Song, J.Â Wang, M.Â Qiao, L.Â Huo, and Z.Â Wang, â€œPredicting head
    movement in panoramic video: A deep reinforcement learning approach,â€ *IEEE TPAMI*,
    2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. å¾, Y. å®‹, J. ç‹, M. ä¹”, L. éœ, å’Œ Z. ç‹, â€œé¢„æµ‹å…¨æ™¯è§†é¢‘ä¸­çš„å¤´éƒ¨è¿åŠ¨ï¼šä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œâ€ *IEEE
    TPAMI*ï¼Œ2019å¹´ã€‚'
- en: '[175] Y.Â Zhu, G.Â Zhai, Y.Â Yang, H.Â Duan, X.Â Min, and X.Â Yang, â€œViewing behavior
    supported visual saliency predictor for 360 degree videos,â€ *IEEE TCSVT*, 2021.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. æœ±, G. ç¿Ÿ, Y. æ¨, H. æ®µ, X. é—µ, å’Œ X. æ¨, â€œæ”¯æŒè§†è¡Œä¸ºçš„360åº¦è§†é¢‘è§†è§‰æ˜¾è‘—æ€§é¢„æµ‹å™¨ï¼Œâ€ *IEEE TCSVT*ï¼Œ2021å¹´ã€‚'
- en: '[176] M.Â Qiao, M.Â Xu, Z.Â Wang, and A.Â Borji, â€œViewport-dependent saliency prediction
    in 360^âˆ˜ video,â€ *IEEE TMM*, 2021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M. ä¹”, M. å¾, Z. ç‹, å’Œ A. Borji, â€œ360^âˆ˜è§†é¢‘ä¸­çš„è§†å£ä¾èµ–æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *IEEE TMM*ï¼Œ2021å¹´ã€‚'
- en: '[177] R.Â Monroy, S.Â Lutz, T.Â Chalasani, and A.Â Smolic, â€œSalnet360: Saliency
    maps for omni-directional images with cnn,â€ *Signal Process Image Commun*, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Monroy, S. Lutz, T. Chalasani, å’Œ A. Smolic, â€œSalnet360ï¼šç”¨äºå…¨å‘å›¾åƒçš„æ˜¾è‘—æ€§å›¾ï¼ŒåŸºäºCNNï¼Œâ€
    *Signal Process Image Commun*ï¼Œ2017å¹´ã€‚'
- en: '[178] R.Â Zhang, C.Â Chen, J.Â Zhang, J.Â Peng, and A.Â M.Â T. Alzbier, â€œ360-degree
    visual saliency detection based on fast-mapped convolution and adaptive equator-bias
    perception,â€ *The Visual Computer*, 2022.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] R. å¼ , C. é™ˆ, J. å¼ , J. å½­, å’Œ A. M. T. Alzbier, â€œåŸºäºå¿«é€Ÿæ˜ å°„å·ç§¯å’Œè‡ªé€‚åº”èµ¤é“åç½®æ„ŸçŸ¥çš„360åº¦è§†è§‰æ˜¾è‘—æ€§æ£€æµ‹ï¼Œâ€
    *The Visual Computer*ï¼Œ2022å¹´ã€‚'
- en: '[179] Y.Â Yang, Y.Â Zhu, Z.Â Gao, and G.Â Zhai, â€œSalgfcn: Graph based fully convolutional
    network for panoramic saliency prediction,â€ *VCIP*, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. æ¨, Y. æœ±, Z. é«˜, å’Œ G. ç¿Ÿ, â€œSalgfcnï¼šåŸºäºå›¾çš„å…¨å·ç§¯ç½‘ç»œè¿›è¡Œå…¨æ™¯æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *VCIP*ï¼Œ2021å¹´ã€‚'
- en: '[180] P.Â Mazumdar and F.Â Battisti, â€œA content-based approach for saliency estimation
    in 360 images,â€ *ICIP*, 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] P. Mazumdar å’Œ F. Battisti, â€œä¸€ç§åŸºäºå†…å®¹çš„360åº¦å›¾åƒæ˜¾è‘—æ€§ä¼°è®¡æ–¹æ³•ï¼Œâ€ *ICIP*ï¼Œ2019å¹´ã€‚'
- en: '[181] T.Â Suzuki and T.Â Yamanaka, â€œSaliency map estimation for omni-directional
    image considering prior distributions,â€ *SMC*, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] T. é“ƒæœ¨ å’Œ T. å±±ä¸­, â€œè€ƒè™‘å…ˆéªŒåˆ†å¸ƒçš„å…¨å‘å›¾åƒæ˜¾è‘—æ€§å›¾ä¼°è®¡ï¼Œâ€ *SMC*ï¼Œ2018å¹´ã€‚'
- en: '[182] I.Â Djemai, S.Â A. Fezza, W.Â Hamidouche, and O.Â DÃ©forges, â€œExtending 2d
    saliency models for head movement prediction in 360-degree images using cnn-based
    fusion,â€ *ISCAS*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] I. Djemai, S. A. Fezza, W. Hamidouche, å’Œ O. DÃ©forges, â€œä½¿ç”¨åŸºäºCNNçš„èåˆæ‰©å±•2Dæ˜¾è‘—æ€§æ¨¡å‹ï¼Œä»¥é¢„æµ‹360åº¦å›¾åƒä¸­çš„å¤´éƒ¨è¿åŠ¨ï¼Œâ€
    *ISCAS*ï¼Œ2020å¹´ã€‚'
- en: '[183] D.Â Chen, C.Â Qing, X.Â Xu, and H.Â Zhu, â€œSalbinet360: Saliency prediction
    on 360^âˆ˜ images with local-global bifurcated deep network,â€ *VR*, 2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] D. é™ˆ, C. é’, X. å¾, å’Œ H. æœ±, â€œSalbinet360ï¼šåŸºäºå±€éƒ¨-å…¨å±€åˆ†å‰æ·±åº¦ç½‘ç»œçš„360^âˆ˜å›¾åƒæ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *VR*ï¼Œ2020å¹´ã€‚'
- en: '[184] Y.Â Zhu, G.Â Zhai, X.Â Min, and J.Â Zhou, â€œThe prediction of saliency map
    for head and eye movements in 360 degree images,â€ *IEEE TMM*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. æœ±, G. ç¿Ÿ, X. é—µ, å’Œ J. å‘¨, â€œ360åº¦å›¾åƒä¸­å¤´éƒ¨å’Œçœ¼ç›è¿åŠ¨çš„æ˜¾è‘—æ€§å›¾é¢„æµ‹ï¼Œâ€ *IEEE TMM*ï¼Œ2020å¹´ã€‚'
- en: '[185] F.-Y. Chao, L.Â Zhang, W.Â Hamidouche, and O.Â DÃ©forges, â€œSalgan360: Visual
    saliency prediction on 360 degree images with generative adversarial networks,â€
    *ICME Workshop*, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] F.-Y. Chao, L. å¼ , W. Hamidouche, å’Œ O. DÃ©forges, â€œSalgan360ï¼šåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„360åº¦å›¾åƒè§†è§‰æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€
    *ICME Workshop*ï¼Œ2018å¹´ã€‚'
- en: '[186] J.Â Pan, C.Â Canton-Ferrer, K.Â McGuinness, N.Â E. Oâ€™Connor, J.Â Torres, E.Â Sayrol,
    and X.Â G. iÂ Nieto, â€œSalgan: Visual saliency prediction with generative adversarial
    networks,â€ *arXiv*, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J.Â Pan, C.Â Canton-Ferrer, K.Â McGuinness, N.Â E. Oâ€™Connor, J.Â Torres, E.Â Sayrol,
    å’Œ X.Â G. iÂ Nieto, â€œSalgan: åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è§†è§‰æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *arXiv*, 2017ã€‚'
- en: '[187] D.Â Zhu, Y.Â Chen, T.Â Han, D.Â Zhao, Y.Â Zhu, Q.Â Zhou, G.Â Zhai, and X.Â Yang,
    â€œRansp: Ranking attention network for saliency prediction on omnidirectional images,â€
    *ICME*, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] D.Â Zhu, Y.Â Chen, T.Â Han, D.Â Zhao, Y.Â Zhu, Q.Â Zhou, G.Â Zhai, å’Œ X.Â Yang,
    â€œRansp: åŸºäºæ³¨æ„åŠ›ç½‘ç»œçš„å…¨æ™¯å›¾åƒæ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *ICME*, 2020ã€‚'
- en: '[188] D.Â Zhu, Y.Â Chen, D.Â Zhao, Q.Â Zhou, and X.Â Yang, â€œSaliency prediction
    on omnidirectional images with attention-aware feature fusion network,â€ *Appl.
    Intell.*, 2021.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] D.Â Zhu, Y.Â Chen, D.Â Zhao, Q.Â Zhou, å’Œ X.Â Yang, â€œåŸºäºæ³¨æ„åŠ›æ„ŸçŸ¥ç‰¹å¾èåˆç½‘ç»œçš„å…¨æ™¯å›¾åƒæ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€
    *Appl. Intell.*, 2021ã€‚'
- en: '[189] M.Â Assens, X.Â G. iÂ Nieto, K.Â McGuinness, and N.Â E. Oâ€™Connor, â€œSaltinet:
    Scan-path prediction on 360 degree images using saliency volumes,â€ *ICCV Workshop*,
    2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] M.Â Assens, X.Â G. iÂ Nieto, K.Â McGuinness, å’Œ N.Â E. Oâ€™Connor, â€œSaltinet:
    ä½¿ç”¨æ˜¾è‘—æ€§ä½“ç§¯å¯¹360åº¦å›¾åƒè¿›è¡Œæ‰«æè·¯å¾„é¢„æµ‹ï¼Œâ€ *ICCV Workshop*, 2017ã€‚'
- en: '[190] B.Â Zhou, A.Â Khosla, Ã€.Â Lapedriza, A.Â Oliva, and A.Â Torralba, â€œLearning
    deep features for discriminative localization,â€ *CVPR*, 2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] B.Â Zhou, A.Â Khosla, Ã€.Â Lapedriza, A.Â Oliva, å’Œ A.Â Torralba, â€œä¸ºåˆ¤åˆ«æ€§å®šä½å­¦ä¹ æ·±åº¦ç‰¹å¾ï¼Œâ€
    *CVPR*, 2016ã€‚'
- en: '[191] P.Â Kellnhofer, A.Â Recasens, S.Â Stent, W.Â Matusik, and A.Â Torralba, â€œGaze360:
    Physically unconstrained gaze estimation in the wild,â€ in *CVPR*, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] P.Â Kellnhofer, A.Â Recasens, S.Â Stent, W.Â Matusik, å’Œ A.Â Torralba, â€œGaze360:
    åœ¨è‡ªç„¶ç¯å¢ƒä¸­è¿›è¡Œç‰©ç†ä¸Šä¸å—é™çš„å‡è§†ä¼°è®¡ï¼Œâ€ åœ¨ *CVPR*, 2019ã€‚'
- en: '[192] Y.Â Li, W.Â Shen, Z.Â Gao, Y.Â Zhu, G.Â Zhai, and G.Â Guo, â€œLooking here or
    there? gaze following in 360-degree images,â€ *ICCV*, 2021.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y.Â Li, W.Â Shen, Z.Â Gao, Y.Â Zhu, G.Â Zhai, å’Œ G.Â Guo, â€œè¿™é‡Œè¿˜æ˜¯é‚£é‡Œï¼Ÿåœ¨360åº¦å›¾åƒä¸­çš„å‡è§†è·Ÿè¸ªï¼Œâ€
    *ICCV*, 2021ã€‚'
- en: '[193] Y.Â Xu, Y.Â Dong, J.Â Wu, Z.Â Sun, Z.Â Shi, J.Â Yu, and S.Â Gao, â€œGaze prediction
    in dynamic 360^âˆ˜ immersive videos,â€ *CVPR*, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y.Â Xu, Y.Â Dong, J.Â Wu, Z.Â Sun, Z.Â Shi, J.Â Yu, å’Œ S.Â Gao, â€œåœ¨åŠ¨æ€360åº¦æ²‰æµ¸å¼è§†é¢‘ä¸­çš„å‡è§†é¢„æµ‹ï¼Œâ€
    *CVPR*, 2018ã€‚'
- en: '[194] C.Â Wu, R.Â Zhang, Z.Â Wang, and L.Â Sun, â€œA spherical convolution approach
    for learning long term viewport prediction in 360 immersive video,â€ in *AAAI*,
    2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] C.Â Wu, R.Â Zhang, Z.Â Wang, å’Œ L.Â Sun, â€œä¸€ç§ç”¨äºåœ¨360åº¦æ²‰æµ¸å¼è§†é¢‘ä¸­å­¦ä¹ é•¿æœŸè§†å£é¢„æµ‹çš„çƒé¢å·ç§¯æ–¹æ³•ï¼Œâ€
    åœ¨ *AAAI*, 2020ã€‚'
- en: '[195] P.Â Morgado, N.Â Vasconcelos, T.Â R. Langlois, and O.Â Wang, â€œSelf-supervised
    generation of spatial audio for 360 video,â€ in *NeurIPS*, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] P.Â Morgado, N.Â Vasconcelos, T.Â R. Langlois, å’Œ O.Â Wang, â€œ360è§†é¢‘çš„è‡ªç›‘ç£ç©ºé—´éŸ³é¢‘ç”Ÿæˆï¼Œâ€
    åœ¨ *NeurIPS*, 2018ã€‚'
- en: '[196] P.Â Morgado, Y.Â Li, and N.Â Nvasconcelos, â€œLearning representations from
    audio-visual spatial alignment,â€ *NeurIPS*, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] P.Â Morgado, Y.Â Li, å’Œ N.Â Nvasconcelos, â€œä»è§†å¬ç©ºé—´å¯¹é½ä¸­å­¦ä¹ è¡¨å¾ï¼Œâ€ *NeurIPS*, 2020ã€‚'
- en: '[197] Y.Â Masuyama, Y.Â Bando, K.Â Yatabe, Y.Â Sasaki, M.Â Onishi, and Y.Â Oikawa,
    â€œSelf-supervised neural audio-visual sound source localization via probabilistic
    spatial modeling,â€ *IROS*, 2020.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y.Â Masuyama, Y.Â Bando, K.Â Yatabe, Y.Â Sasaki, M.Â Onishi, å’Œ Y.Â Oikawa,
    â€œé€šè¿‡æ¦‚ç‡ç©ºé—´å»ºæ¨¡è¿›è¡Œè‡ªç›‘ç£ç¥ç»è§†å¬å£°éŸ³æºå®šä½ï¼Œâ€ *IROS*, 2020ã€‚'
- en: '[198] A.Â B. Vasudevan, D.Â Dai, and L.Â V. Gool, â€œSemantic object prediction
    and spatial sound super-resolution with binaural sounds,â€ in *ECCV*, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A.Â B. Vasudevan, D.Â Dai, å’Œ L.Â V. Gool, â€œå¸¦æœ‰åŒè€³å£°éŸ³çš„è¯­ä¹‰å¯¹è±¡é¢„æµ‹å’Œç©ºé—´å£°éŸ³è¶…åˆ†è¾¨ç‡ï¼Œâ€ åœ¨ *ECCV*,
    2020ã€‚'
- en: '[199] F.-Y. Chao, C.Â Ozcinar, L.Â Zhang, W.Â Hamidouche, O.Â DÃ©forges, and A.Â Smolic,
    â€œTowards audio-visual saliency prediction for omnidirectional video with spatial
    audio,â€ *VCIP*, 2020.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] F.-Y. Chao, C.Â Ozcinar, L.Â Zhang, W.Â Hamidouche, O.Â DÃ©forges, å’Œ A.Â Smolic,
    â€œé’ˆå¯¹å…·æœ‰ç©ºé—´éŸ³é¢‘çš„å…¨æ™¯è§†é¢‘çš„è§†å¬æ˜¾è‘—æ€§é¢„æµ‹ï¼Œâ€ *VCIP*, 2020ã€‚'
- en: '[200] F.-Y. Chao, C.Â Ozcinar, C.Â Wang, E.Â Zerman, L.Â Zhang, W.Â Hamidouche,
    O.Â DÃ©forges, and A.Â Smolic, â€œAudio-visual perception of omnidirectional video
    for virtual reality applications,â€ *ICME Workshop*, 2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] F.-Y. Chao, C.Â Ozcinar, C.Â Wang, E.Â Zerman, L.Â Zhang, W.Â Hamidouche,
    O.Â DÃ©forges, å’Œ A.Â Smolic, â€œå…¨æ™¯è§†é¢‘åœ¨è™šæ‹Ÿç°å®åº”ç”¨ä¸­çš„è§†å¬æ„ŸçŸ¥ï¼Œâ€ *ICME Workshop*, 2020ã€‚'
- en: '[201] S.-H. Chou, W.-L. Chao, W.-S. Lai, M.Â Sun, and M.-H. Yang, â€œVisual question
    answering on 360deg images,â€ in *WACV*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S.-H. Chou, W.-L. Chao, W.-S. Lai, M.Â Sun, å’Œ M.-H. Yang, â€œåœ¨360åº¦å›¾åƒä¸Šçš„è§†è§‰é—®ç­”ï¼Œâ€
    åœ¨ *WACV*, 2020ã€‚'
- en: '[202] F.Â Liu, T.Â Xiang, T.Â M. Hospedales, W.Â Yang, and C.Â Sun, â€œInverse visual
    question answering: A new benchmark and vqa diagnosis tool,â€ *IEEE TPAMI*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] F.Â Liu, T.Â Xiang, T.Â M. Hospedales, W.Â Yang, å’Œ C.Â Sun, â€œé€†è§†è§‰é—®ç­”ï¼šä¸€ä¸ªæ–°çš„åŸºå‡†å’ŒVQAè¯Šæ–­å·¥å…·ï¼Œâ€
    *IEEE TPAMI*, 2020ã€‚'
- en: '[203] A.Â Tarvainen and H.Â Valpola, â€œMean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results.â€ *ICLR*, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A.Â Tarvainen å’Œ H.Â Valpola, â€œå¹³å‡æ•™å¸ˆæ˜¯æ›´å¥½çš„è§’è‰²æ¨¡å‹ï¼šæƒé‡å¹³å‡ä¸€è‡´æ€§ç›®æ ‡æ”¹è¿›åŠç›‘ç£æ·±åº¦å­¦ä¹ ç»“æœã€‚â€ *ICLR*,
    2017ã€‚'
- en: '[204] G.Â D. Pais, T.Â J. Dias, J.Â C. Nascimento, and P.Â Miraldo, â€œOmnidrl: Robust
    pedestrian detection using deep reinforcement learning on omnidirectional cameras*,â€
    *2019 International Conference on Robotics and Automation (ICRA)*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] G. D. Pais, T. J. Dias, J. C. Nascimento, å’Œ P. Miraldo, â€œOmnidrl: ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å…¨å‘ç›¸æœºä¸Šçš„é²æ£’è¡Œäººæ£€æµ‹*ï¼Œâ€
    *2019 International Conference on Robotics and Automation (ICRA)*, 2019ã€‚'
- en: '[205] A.Â Kittel, P.Â Larkin, I.Â Cunningham, and M.Â Spittle, â€œ360 virtual reality:
    A swot analysis in comparison to virtual reality,â€ *Frontiers in Psychology*,
    2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] A. Kittel, P. Larkin, I. Cunningham, å’Œ M. Spittle, â€œ360è™šæ‹Ÿç°å®ï¼šä¸è™šæ‹Ÿç°å®çš„SWOTåˆ†æï¼Œâ€
    *Frontiers in Psychology*, 2020ã€‚'
- en: '[206] H.Â Kim, L.Â Hernaggi, P.Â J.Â B. Jackson, and A.Â Hilton, â€œImmersive spatial
    audio reproduction for VR/AR using room acoustic modelling from 360^âˆ˜ images,â€
    in *VR*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] H. Kim, L. Hernaggi, P. J. B. Jackson, å’Œ A. Hilton, â€œç”¨äºVR/ARçš„æ²‰æµ¸å¼ç©ºé—´éŸ³é¢‘å†ç°ï¼ŒåŸºäº360^âˆ˜å›¾åƒçš„æˆ¿é—´å£°å­¦å»ºæ¨¡ï¼Œâ€
    åœ¨ *VR*, 2019ã€‚'
- en: '[207] Y.Â Heshmat, B.Â Jones, X.Â Xiong, C.Â Neustaedter, A.Â Tang, B.Â E. Riecke,
    and L.Â Yang, â€œGeocaching with a beam: Shared outdoor activities through a telepresence
    robot with 360 degree viewing,â€ *CHI Conference on Human Factors in Computing
    Systems*, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Heshmat, B. Jones, X. Xiong, C. Neustaedter, A. Tang, B. E. Riecke,
    å’Œ L. Yang, â€œç”¨å…‰æŸè¿›è¡Œåœ°ç†å¯»å®ï¼šé€šè¿‡å…·æœ‰360åº¦è§†è§’çš„è¿œç¨‹æœºå™¨äººå…±äº«æˆ·å¤–æ´»åŠ¨ï¼Œâ€ *CHI Conference on Human Factors
    in Computing Systems*, 2018ã€‚'
- en: '[208] J.Â Zhang, â€œA 360^âˆ˜ video-based robot platform for telepresent redirected
    walking,â€ in *VAM-HRI*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] J. Zhang, â€œä¸€ä¸ªåŸºäº360^âˆ˜è§†é¢‘çš„æœºå™¨äººå¹³å°ï¼Œç”¨äºè¿œç¨‹é‡å®šå‘è¡Œèµ°ï¼Œâ€ åœ¨ *VAM-HRI*, 2018ã€‚'
- en: '[209] G.Â Pudics, M.Â Z. Szabo-Resch, and Z.Â VÃ¡mossy, â€œSafe robot navigation
    using an omnidirectional camera,â€ *CINTI*, 2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] G. Pudics, M. Z. Szabo-Resch, å’Œ Z. VÃ¡moossy, â€œä½¿ç”¨å…¨å‘ç›¸æœºçš„å®‰å…¨æœºå™¨äººå¯¼èˆªï¼Œâ€ *CINTI*,
    2015ã€‚'
- en: '[210] L.Â Ran, Y.Â Zhang, Q.Â Zhang, and T.Â Yang, â€œConvolutional neural network-based
    robot navigation using uncalibrated spherical images â€ ,â€ *Sensors (Basel, Switzerland)*,
    2017.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] L. Ran, Y. Zhang, Q. Zhang, å’Œ T. Yang, â€œåŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„æœºå™¨äººå¯¼èˆªï¼Œä½¿ç”¨æœªæ ‡å®šçš„çƒå½¢å›¾åƒâ€ ï¼Œâ€
    *Sensors (Basel, Switzerland)*, 2017ã€‚'
- en: '[211] S.Â S. Mansouri, P.Â S. Karvelis, C.Â Kanellakis, D.Â Kominiak, and G.Â Nikolakopoulos,
    â€œVision-based mav navigation in underground mine using convolutional neural network,â€
    *IECON*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. S. Mansouri, P. S. Karvelis, C. Kanellakis, D. Kominiak, å’Œ G. Nikolakopoulos,
    â€œåŸºäºè§†è§‰çš„åœ°ä¸‹çŸ¿äº•MAVå¯¼èˆªï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œâ€ *IECON*, 2019ã€‚'
- en: '[212] D.Â Sun, X.Â Huang, and K.Â Yang, â€œA multimodal vision sensor for autonomous
    driving,â€ in *Counterterrorism, Crime Fighting, Forensics, and Surveillance Technologies
    III*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] D. Sun, X. Huang, å’Œ K. Yang, â€œç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¤šæ¨¡æ€è§†è§‰ä¼ æ„Ÿå™¨ï¼Œâ€ åœ¨ *Counterterrorism,
    Crime Fighting, Forensics, and Surveillance Technologies III*, 2019ã€‚'
- en: '[213] J.Â BeltrÃ¡n, C.Â Guindel, I.Â CortÃ©s, A.Â Barrera, A.Â Astudillo, J.Â Urdiales,
    M.Â Ãlvarez, F.Â Bekka, V.Â MilanÃ©s, and F.Â GarcÃ­a, â€œTowards autonomous driving:
    a multi-modal 360^âˆ˜ perception proposal,â€ in *ITSC*, 2020.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] J. BeltrÃ¡n, C. Guindel, I. CortÃ©s, A. Barrera, A. Astudillo, J. Urdiales,
    M. Ãlvarez, F. Bekka, V. MilanÃ©s, å’Œ F. GarcÃ­a, â€œè¿ˆå‘è‡ªåŠ¨é©¾é©¶ï¼šä¸€ç§å¤šæ¨¡æ€360^âˆ˜æ„ŸçŸ¥æ–¹æ¡ˆï¼Œâ€ åœ¨ *ITSC*,
    2020ã€‚'
- en: '[214] H.Â Caesar, V.Â Bankiti, A.Â H. Lang, S.Â Vora, V.Â E. Liong, Q.Â Xu, A.Â Krishnan,
    Y.Â Pan, G.Â Baldan, and O.Â Beijbom, â€œnuscenes: A multimodal dataset for autonomous
    driving,â€ in *CVPR*, 2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, å’Œ O. Beijbom, â€œnuscenes: ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œâ€ åœ¨ *CVPR*, 2020ã€‚'
- en: '[215] X.Â Zhang, Z.Â Li, Y.Â Gong, D.Â Jin, J.Â Li, L.Â Wang, Y.Â Zhu, and H.Â Liu,
    â€œOpenmpd: An open multimodal perception dataset for autonomous driving,â€ *IEEE
    Trans. Veh. Technol.*, 2022.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] X. Zhang, Z. Li, Y. Gong, D. Jin, J. Li, L. Wang, Y. Zhu, å’Œ H. Liu, â€œOpenmpd:
    ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¼€æ”¾å¤šæ¨¡æ€æ„ŸçŸ¥æ•°æ®é›†ï¼Œâ€ *IEEE Trans. Veh. Technol.*, 2022ã€‚'
- en: '[216] V.Â R. Kumar, S.Â Yogamani, H.Â Rashed, G.Â Sitsu, C.Â Witt, I.Â Leang, S.Â Milz,
    and P.Â MÃ¤der, â€œOmnidet: Surround view cameras based multi-task visual perception
    network for autonomous driving,â€ *IEEE Robot. Autom. Lett.*, 2021.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] V. R. Kumar, S. Yogamani, H. Rashed, G. Sitsu, C. Witt, I. Leang, S.
    Milz, å’Œ P. MÃ¤der, â€œOmnidet: åŸºäºç¯è§†æ‘„åƒå¤´çš„å¤šä»»åŠ¡è§†è§‰æ„ŸçŸ¥ç½‘ç»œï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ï¼Œâ€ *IEEE Robot. Autom. Lett.*,
    2021ã€‚'
- en: '[217] Z.Â Yan, X.Â Li, K.Â Wang, Z.Â Zhang, J.Â Li, and J.Â Yang, â€œMulti-modal masked
    pre-training for monocular panoramic depth completion,â€ *arXiv*, 2022.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Z. Yan, X. Li, K. Wang, Z. Zhang, J. Li, å’Œ J. Yang, â€œç”¨äºå•ç›®å…¨æ™¯æ·±åº¦å®Œæˆçš„å¤šæ¨¡æ€æ©è”½é¢„è®­ç»ƒï¼Œâ€
    *arXiv*, 2022ã€‚'
- en: '[218] J.Â Li, H.Â Li, and Y.Â Matsushita, â€œLighting, reflectance and geometry
    estimation from 360^âˆ˜ panoramic stereo,â€ *CVPR*, 2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] J. Li, H. Li, å’Œ Y. Matsushita, â€œä»360^âˆ˜å…¨æ™¯ç«‹ä½“å›¾åƒä¸­ä¼°è®¡å…‰ç…§ã€åå°„ç‡å’Œå‡ ä½•ä¿¡æ¯ï¼Œâ€ *CVPR*,
    2021ã€‚'
- en: '[219] Y.Â Zhang, Y.Â Liu, J.Â Liu, P.Â Zhan, L.Â Wang, and Z.Â Xu, â€œSp attack: Single-perspective
    attack for generating adversarial omnidirectional images,â€ in *ICASSP*, 2022.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Y. Zhang, Y. Liu, J. Liu, P. Zhan, L. Wang, å’Œ Z. Xu, â€œSp attack: å•è§†è§’æ”»å‡»ç”Ÿæˆå¯¹æŠ—å…¨å‘å›¾åƒï¼Œâ€
    åœ¨ *ICASSP*, 2022ã€‚'
- en: '[220] S.Â Wang, W.Â Zeng, X.Â Chen, Y.Â Ye, Y.Â Qiao, and C.-W. Fu, â€œActfloor-gan:
    Activity-guided adversarial networks for human-centric floorplan design,â€ *TVCG*,
    2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] S. Wang, W. Zeng, X. Chen, Y. Ye, Y. Qiao, and C.-W. Fu, â€œActfloor-gan:
    Activity-guided adversarial networks for human-centric floorplan design,â€ *TVCG*,
    2021.'
- en: '| ![[Uncaptioned image]](img/772c6b488f1ee1299002d58638841609.png) | Hao Ai
    is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). His research interests include pattern recognition (image
    classification, face recognition, etc.), DL (especially uncertainty learning,
    attention, transfer learning, semi- /self-unsupervised learning), omnidirectional
    vision. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/772c6b488f1ee1299002d58638841609.png) | **è‰¾æµ©**æ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½å‰æ²¿è§†è§‰å­¦ä¹ ä¸æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤çš„åšå£«ç”Ÿã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æ¨¡å¼è¯†åˆ«ï¼ˆå›¾åƒåˆ†ç±»ã€é¢éƒ¨è¯†åˆ«ç­‰ï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯ä¸ç¡®å®šæ€§å­¦ä¹ ã€æ³¨æ„åŠ›æœºåˆ¶ã€è¿ç§»å­¦ä¹ ã€åŠç›‘ç£/è‡ªç›‘ç£å­¦ä¹ ï¼‰å’Œå…¨æ™¯è§†è§‰ã€‚
    |'
- en: '| ![[Uncaptioned image]](img/74eeddb266fd6648c3f1148a5e7d865e.png) | Zidong
    Cao is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, Guangzhou Campus, The Hong Kong University of
    Science and Technology (HKUST). His research interests include 3D vision (depth
    completion, depth estimation, etc.), DL (self-supervised learning, weakly-supervised
    learning, etc.) and omnidirectional vision. |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/74eeddb266fd6648c3f1148a5e7d865e.png) | **æ›¹å­æ ‹**æ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½å‰æ²¿è§†è§‰å­¦ä¹ ä¸æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤çš„ç ”ç©¶åŠ©ç†ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬3Dè§†è§‰ï¼ˆæ·±åº¦è¡¥å…¨ã€æ·±åº¦ä¼°è®¡ç­‰ï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆè‡ªç›‘ç£å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ ç­‰ï¼‰å’Œå…¨æ™¯è§†è§‰ã€‚
    |'
- en: '| ![[Uncaptioned image]](img/4dbb154db3ce6200aad6d269ae0fb4c8.png) | JinJing
    Zhu is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). His research interests include CV (image classification, person
    re-identification, action recognition, etc.), DL (especially transfer learning,
    knowledge distillation, multi-task learning, semi-/self-unsupervised learning,
    etc.), omnidirectional vision, and event-based vision. |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/4dbb154db3ce6200aad6d269ae0fb4c8.png) | **æœ±é‡‘æ™¶**æ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½å‰æ²¿è§†è§‰å­¦ä¹ ä¸æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤çš„åšå£«ç”Ÿã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ï¼ˆå›¾åƒåˆ†ç±»ã€äººå‘˜å†è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«ç­‰ï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯è¿ç§»å­¦ä¹ ã€çŸ¥è¯†è’¸é¦ã€å¤šä»»åŠ¡å­¦ä¹ ã€åŠç›‘ç£/è‡ªç›‘ç£å­¦ä¹ ç­‰ï¼‰ã€å…¨æ™¯è§†è§‰å’ŒåŸºäºäº‹ä»¶çš„è§†è§‰ã€‚
    |'
- en: '| ![[Uncaptioned image]](img/c866d5a65fa037d59cea97b45f2bdd22.png) | Haotian
    Bai is a research assistant at Visual Learning and Intelligent Systems Lab, AI
    Thrust, Information Hub, Guangzhou Campus, The Hong Kong University of Science
    and Technology (HKUST). His research interests include 3D reconstruction (e.g.,
    human and room), CV(especially attention, unsupervised/weakly supervised learning,
    and domain adaptation), and causal inference. |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/c866d5a65fa037d59cea97b45f2bdd22.png) | **ç™½æµ©å¤©**æ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½å‰æ²¿è§†è§‰å­¦ä¹ ä¸æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤çš„ç ”ç©¶åŠ©ç†ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬3Dé‡å»ºï¼ˆå¦‚äººç±»å’Œæˆ¿é—´ï¼‰ã€è®¡ç®—æœºè§†è§‰ï¼ˆå°¤å…¶æ˜¯æ³¨æ„åŠ›æœºåˆ¶ã€æ— ç›‘ç£/å¼±ç›‘ç£å­¦ä¹ å’Œé¢†åŸŸé€‚åº”ï¼‰ä»¥åŠå› æœæ¨æ–­ã€‚
    |'
- en: '| ![[Uncaptioned image]](img/8ec1f5329f39c1d56937428e3890663e.png) | Yucheng
    Chen is a Mphil student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). Her research interests include low-level vision (neural rendering,
    reflection removal, etc.), DL (especially domain adaptation, transfer learning,
    semi- /self-unsupervised learning), omnidirectional vision. |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/8ec1f5329f39c1d56937428e3890663e.png) | **é™ˆå®‡æˆ**æ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆHKUSTï¼‰å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½å‰æ²¿è§†è§‰å­¦ä¹ ä¸æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤çš„ç¡•å£«ç ”ç©¶ç”Ÿã€‚å¥¹çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬ä½çº§è§†è§‰ï¼ˆç¥ç»æ¸²æŸ“ã€åå°„å»é™¤ç­‰ï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯é¢†åŸŸé€‚åº”ã€è¿ç§»å­¦ä¹ ã€åŠç›‘ç£/è‡ªç›‘ç£å­¦ä¹ ï¼‰å’Œå…¨æ™¯è§†è§‰ã€‚
    |'
- en: '| ![[Uncaptioned image]](img/17c3d1c8c58af3fb77c625c6cdbe74c3.png) | Lin Wang
    is an assistant professor in the Artificial Intelligence Thrust, HKUST GZ Campus,
    HKUST Fok Ying Tung Research Institute, and an affiliate assistant professor in
    the Dept. of CSE, HKUST, CWB Campus. He got his PhD degree with honor from Korea
    Advanced Institute of Science and Technology (KAIST). His research interests include
    computer vision/graphics, machine learning and human-AI collaboration. |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| ![[æœªæ ‡æ³¨çš„å›¾ç‰‡]](img/17c3d1c8c58af3fb77c625c6cdbe74c3.png) | æ—æ—ºæ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦å¹¿å·æ ¡åŒºäººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒçš„åŠ©ç†æ•™æˆï¼Œä¹Ÿæ˜¯é¦™æ¸¯ç§‘æŠ€å¤§å­¦ä¸­ç¯æ ¡åŒºè®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹ç³»çš„é™„å±åŠ©ç†æ•™æˆã€‚ä»–è·å¾—äº†éŸ©å›½å…ˆè¿›ç§‘æŠ€å­¦é™¢ï¼ˆKAISTï¼‰çš„è£èª‰åšå£«å­¦ä½ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬è®¡ç®—æœºè§†è§‰/å›¾å½¢å­¦ã€æœºå™¨å­¦ä¹ å’Œäººç±»-äººå·¥æ™ºèƒ½åä½œã€‚
    |'
- en: \foreach\x
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: \foreach\x
- en: in 1,â€¦,0 See pages \x of [supplement.pdf](supplement.pdf)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ 1,â€¦,0 è¯·å‚è§ [supplement.pdf](supplement.pdf) çš„ç¬¬ \x é¡µ
