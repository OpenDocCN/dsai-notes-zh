- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.10468] Deep Learning for Omnidirectional Vision: A Survey and New Perspectives'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.10468] 全景视觉中的深度学习：综述与新视角'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.10468](https://ar5iv.labs.arxiv.org/html/2205.10468)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.10468](https://ar5iv.labs.arxiv.org/html/2205.10468)
- en: \pdfximage
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfximage
- en: supplement.pdf
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: supplement.pdf
- en: 'Deep Learning for Omnidirectional Vision: A Survey and New Perspectives'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全景视觉中的深度学习：综述与新视角
- en: 'Hao Ai^∗, Zidong Cao^∗, Jinjing Zhu, Haotian Bai, Yucheng Chen, and  Lin Wang
    H. Ai and Z. Cao, J. Zhu, H. Bai, Y. Chen are with the Artificial Intelligence
    Thrust, The Hong Kong University of Science and Technology (HKUST), Guangzhou,
    China. E-mail: {haoai, zidongcao, jinjingzhu, haotianbai, yuchengchen}@ust.hk.
    L. Wang is with the Artificial Intelligence Thrust, HKUST, Guangzhou, and Dept.
    of Computer Science and Engineering, HKUST, Hong Kong SAR, China. E-mail: linwang@ust.hkManuscript
    received April 19, 2022; revised August 26, 2022. (^∗Equal contribution, Corresponding
    author: Lin Wang)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Ai^∗，Zidong Cao^∗，Jinjing Zhu，Haotian Bai，Yucheng Chen，和 Lin Wang H. Ai
    和 Z. Cao，J. Zhu，H. Bai，Y. Chen 在中国广州香港科技大学（HKUST）人工智能研究组工作。电子邮件：{haoai, zidongcao,
    jinjingzhu, haotianbai, yuchengchen}@ust.hk。L. Wang 在香港科技大学（HKUST）人工智能研究组工作，同时也是香港科技大学计算机科学与工程系的成员。电子邮件：linwang@ust.hk。手稿收到时间：2022年4月19日；修订时间：2022年8月26日。(^∗平等贡献，通讯作者：Lin
    Wang)
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Omnidirectional image (ODI) data is captured with a $360^{\circ}\times 180^{\circ}$
    field-of-view, which is much wider than the pinhole cameras and contains richer
    spatial information than the conventional planar images. Accordingly, omnidirectional
    vision has attracted booming attention due to its more advantageous performance
    in numerous applications, such as autonomous driving and virtual reality. In recent
    years, the availability of customer-level $360^{\circ}$ cameras has made omnidirectional
    vision more popular, and the advance of deep learning (DL) has significantly sparked
    its research and applications. This paper presents a systematic and comprehensive
    review and analysis of the recent progress in DL methods for omnidirectional vision.
    Our work covers four main contents: (i) An introduction to the principle of omnidirectional
    imaging, the convolution methods on the ODI, and datasets to highlight the differences
    and difficulties compared with the 2D planar image data; (ii) A structural and
    hierarchical taxonomy of the DL methods for omnidirectional vision; (iii) A summarization
    of the latest novel learning strategies and applications; (iv) An insightful discussion
    of the challenges and open problems by highlighting the potential research directions
    to trigger more research in the community.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 全景图像（ODI）数据以$360^{\circ}\times 180^{\circ}$的视场角捕捉，这比针孔相机要宽得多，并且包含比传统平面图像更丰富的空间信息。因此，由于在自动驾驶和虚拟现实等众多应用中的更有利性能，全景视觉吸引了广泛关注。近年来，客户级别的$360^{\circ}$相机的普及使全景视觉更加流行，而深度学习（DL）的进步显著推动了其研究和应用。本文对全景视觉中深度学习方法的最新进展进行了系统而全面的回顾和分析。我们的工作涵盖了四个主要内容：（i）介绍全景成像的原理、ODI上的卷积方法以及数据集，以突出与2D平面图像数据相比的差异和困难；（ii）全景视觉深度学习方法的结构化和层次化分类；（iii）最新的创新学习策略和应用的总结；（iv）通过突出潜在的研究方向，深入讨论挑战和开放问题，以激发更多的社区研究。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Omnidirectional vision, deep learning (DL), Survey, Introductory, Taxonomy
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 全景视觉，深度学习（DL），综述，入门，分类
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the rapid development of 3D technology and the pursuit of realistic visual
    experience, research interest in computer vision has gradually shifted from traditional
    2D planar image data to omnidirectional image (ODI) data, also known as the 360^∘
    image, panoramic image, or spherical image data. ODI data captured by the $360^{\circ}$
    cameras yields a $360^{\circ}\times 180^{\circ}$ field-of-view (FoV), which is
    much wider than the pinhole cameras; therefore, it can capture the entire surrounding
    environment by reflecting richer spatial information than the conventional planar
    images. Due to the immersive experience and complete view, ODI data has been widely
    applied to numerous applications, e.g., augmented reality(AR)$/$virtual reality
    (VR), autonomous driving, and robot navigation. In general, raw ODI data is represented
    as, e.g., the equirectangular projection (ERP) or cubemap projection (CP) to be
    consistent with the imaging pipelines [[1](#bib.bib1)], [[2](#bib.bib2)]. As a
    novel data domain, ODI data has both domain-unique advantages (wide FoV of spherical
    imaging, rich geometric information, multiple projection types) and challenges
    (severe distortion in the ERP type, content discontinuities in the CP format).
    This renders the research on omnidirectional vision valuable yet challenging.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D技术的快速发展和对逼真视觉体验的追求，计算机视觉的研究兴趣逐渐从传统的2D平面图像数据转向全向图像（ODI）数据，也被称为360°图像、全景图像或球面图像数据。由360°摄像机捕获的ODI数据具有360°×180°的视场（FoV），比针孔摄像机要宽得多；因此，它可以通过反映比传统平面图像更丰富的空间信息来捕获整个周围环境。由于其沉浸式体验和完整视角，ODI数据已广泛应用于诸多应用领域，例如增强现实（AR）/虚拟现实（VR）、自动驾驶和机器人导航。一般来说，原始的ODI数据被表示为例如等距投影（ERP）或立方体映射（CP），以便与成像流程保持一致 [[1](#bib.bib1)], [[2](#bib.bib2)]。作为一种新颖的数据领域，ODI数据具有领域独特优势（球形成像的宽视场，丰富的几何信息，多种投影类型）和挑战（ERP类型的严重失真，CP格式的内容不连续），这使得全向视觉的研究既有价值又具挑战性。
- en: 'Recently, the availability of customer-level $360^{\circ}$ cameras has made
    omnidirectional vision more popular, and the advance in deep learning (DL) has
    significantly promoted its research and applications. In particular, as a data-driven
    technology, the continual release of public datasets, e.g., SUN360 [[3](#bib.bib3)],
    Salient 360$!$ [[4](#bib.bib4)], Stanford2D3D [[5](#bib.bib5)], Pano-AVQA [[6](#bib.bib6)]
    and PanoContext [[7](#bib.bib7)], have rapidly enabled the DL methods to accomplish
    remarkable breakthroughs and often achieve the state-of-the-art (SoTA) performances
    on various omnidirectional vision tasks. Moreover, various deep neural network
    (DNN) models have been developed based on diverse architectures, ranging from
    convolutional neural networks (CNNs) [[8](#bib.bib8)], recurrent neural networks
    (RNNs) [[9](#bib.bib9)], generative adversarial networks (GANs) [[10](#bib.bib10)],
    graph neural networks (GNNs) [[11](#bib.bib11)], to vision transformers (ViTs) [[12](#bib.bib12)].
    In general, SoTA-DL-methods focus on four major aspects: (I) convolutional filters
    used to extract features from the ODI data (omnidirectional video (ODV) can be
    considered as a temporal set of ODIs), (II) network design by considering the
    input numbers and projection types, (III) novel learning strategies, and (IV)
    practical applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，消费级360°摄像机的普及使全向视觉更加流行，深度学习（DL）的进展显著推动了其研究和应用。特别是作为数据驱动技术，公共数据集的不断发布，例如SUN360 [[3](#bib.bib3)],
    Salient 360$!$ [[4](#bib.bib4)], Stanford2D3D [[5](#bib.bib5)], Pano-AVQA [[6](#bib.bib6)]和PanoContext [[7](#bib.bib7)]，迅速使DL方法在各种全向视觉任务上取得了显著突破，并经常达到最先进的性能。此外，基于各种架构开发了各种深度神经网络（DNN）模型，从卷积神经网络（CNNs） [[8](#bib.bib8)],
    循环神经网络（RNNs） [[9](#bib.bib9)], 生成对抗网络（GANs） [[10](#bib.bib10)], 图神经网络（GNNs） [[11](#bib.bib11)],
    到视觉变换器（ViTs） [[12](#bib.bib12)]。总体来说，最先进的DL方法集中在四个主要方面：（I）利用卷积滤波器从ODI数据中提取特征（全向视频（ODV）可以被视为一组时间上的ODI），（II）通过考虑输入数量和投影类型进行网络设计，（III）新颖的学习策略，以及（IV）实际应用。
- en: 'This paper presents a systematic and comprehensive review and analysis of the
    recent progress in DL methods for omnidirectional vision. Previously, Zou et al. [[13](#bib.bib13)]
    only focused on the algorithms of reconstructing room layout from a single ODI
    based on the Manhattan assumption. Similarly, Silveira et al. [[14](#bib.bib14)]
    merely reviewed recent 3D scene geometry recovery approaches based on the ODIs.
    Moreover, there exist some limited reviews of the FoV-adaptive video streaming
    methods [[15](#bib.bib15)], [[16](#bib.bib16)], especially on the topic of projection
    types, visual distortion problems, and efficient network structures. Recently,
    Chiariotti et al. [[17](#bib.bib17)] provided a more extensive review of the existing
    literature about ODV streaming systems. Unlike them, we highlight the importance
    of DL and probe the recent advances for omnidirectional vision, both methodically
    and comprehensively. The structural and hierarchical taxonomy proposed in this
    study is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文系统地综述和分析了全向视觉中深度学习方法的最新进展。之前，Zou等人[[13](#bib.bib13)]仅关注于基于曼哈顿假设从单一ODI重建房间布局的算法。同样，Silveira等人[[14](#bib.bib14)]仅回顾了基于ODI的近期3D场景几何恢复方法。此外，还存在一些对FoV适应的视频流方法[[15](#bib.bib15)],
    [[16](#bib.bib16)]的有限综述，特别是在投影类型、视觉失真问题和高效网络结构方面。最近，Chiariotti等人[[17](#bib.bib17)]提供了对现有ODV流媒体系统文献的更广泛综述。与他们不同，我们强调了深度学习的重要性，并深入探讨了全向视觉的最新进展，既系统又全面。本研究提出的结构性和层级分类见图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 全向视觉的深度学习：综述与新视角")。
- en: '![Refer to caption](img/d14ca56df766955d8c69df695efb43f7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d14ca56df766955d8c69df695efb43f7.png)'
- en: 'Figure 1: Hierarchical and structural taxonomy of omnidirectional vision with
    deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于深度学习的全向视觉的层级和结构分类。
- en: 'In summary, the major contributions of this study can be summarized as follows:
    (I) To the best of our knowledge, this is the first survey to comprehensively
    review and analyze the DL methods for omnidirectional vision, including the omnidirectional
    imaging principle, representation learning, datasets, a taxonomy, and applications,
    to highlight the differences and difficulties with the 2D planner image data.
    (II) We summarize most, if not all but representative, published top-tier conference/journal
    works (over 200 papers) in the last five years and conduct an analytical study
    of recent trends of DL for omnidirectional vision, both hierarchically and structurally.
    Moreover, we offer insights into the discussion and challenge of each category.
    (III) We summarize the latest novel learning strategies and potential applications
    for omnidirectional vision. (IV) As DL for omnidirectional vision is an active
    yet intricate research area, we provide insightful discussions of the challenges
    and open problems yet to be solved and propose the potential future directions
    to spur more in-depth research by the community. Meanwhile, we have summarized
    representative methods and their key strategies for some popular omnidirectional
    vision tasks in Table. [II](#S3.T2 "TABLE II ‣ 3.1.1 Image Generation ‣ 3.1 Image/Video
    Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), Table. [III](#S3.T3 "TABLE III ‣ 3.2.2
    Semantic Segmentation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional Vision Tasks
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), Table. [IV](#S3.T4
    "TABLE IV ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    Table. [V](#S3.T5 "TABLE V ‣ 3.3 3D Vision ‣ 3 Omnidirectional Vision Tasks ‣
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), and
    Table. [VI](#S3.T6 "TABLE VI ‣ 3.4.1 Saliency Prediction ‣ 3.4 Human Behavior
    Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"). To provide a better intra-task comparison,
    we present some representative methods’ quantitative and qualitative results on
    benchmark datasets and all statistics are derived from the original papers. Due
    to the lack of space, we show the experimental results in Sec. 2 of the suppl.
    material. (V) We create an open-source repository that provides a taxonomy of
    all the mentioned works and code links. We will keep updating our open-source
    repository with new works in this area and hope it can shed light on future research.
    The repository link is [https://github.com/VLISLAB/360-DL-Survey](https://github.com/VLISLAB/360-DL-Survey).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本研究的主要贡献可以概括为以下几点：(I) 据我们所知，这是首个全面回顾和分析全景视觉的深度学习方法的调查，包括全景成像原理、表示学习、数据集、分类法和应用，突显了与2D平面图像数据的差异和难点。(II)
    我们总结了过去五年大部分（如果不是全部，就是代表性的）顶级会议/期刊论文（超过200篇），并对全景视觉的深度学习最近趋势进行了层次性和结构性的分析。此外，我们对每个类别的讨论和挑战提供了见解。(III)
    我们总结了全景视觉的最新新颖学习策略和潜在应用。(IV) 由于全景视觉的深度学习是一个活跃但复杂的研究领域，我们提供了对尚未解决的挑战和开放问题的深刻讨论，并提出了潜在的未来方向，以激发社区更深入的研究。同时，我们在表格中总结了某些流行全景视觉任务的代表性方法及其关键策略。[II](#S3.T2
    "TABLE II ‣ 3.1.1 图像生成 ‣ 3.1 图像/视频处理 ‣ 3 全景视觉任务 ‣ 全景视觉的深度学习：调查与新视角")、表格[III](#S3.T3
    "TABLE III ‣ 3.2.2 语义分割 ‣ 3.2 场景理解 ‣ 3 全景视觉任务 ‣ 全景视觉的深度学习：调查与新视角")、表格[IV](#S3.T4
    "TABLE IV ‣ 3.2.3 单目深度估计 ‣ 3.2 场景理解 ‣ 3 全景视觉任务 ‣ 全景视觉的深度学习：调查与新视角")、表格[V](#S3.T5
    "TABLE V ‣ 3.3 3D视觉 ‣ 3 全景视觉任务 ‣ 全景视觉的深度学习：调查与新视角")和表格[VI](#S3.T6 "TABLE VI ‣
    3.4.1 显著性预测 ‣ 3.4 人类行为理解 ‣ 3 全景视觉任务 ‣ 全景视觉的深度学习：调查与新视角")。为了提供更好的任务间比较，我们展示了一些代表性方法在基准数据集上的定量和定性结果，所有统计数据均来自原始论文。由于篇幅有限，实验结果展示在补充材料的第2节中。(V)
    我们创建了一个开源库，提供所有提到的工作的分类及代码链接。我们将不断更新我们的开源库，增加该领域的新成果，并希望它能为未来的研究提供启示。开源库链接是[https://github.com/VLISLAB/360-DL-Survey](https://github.com/VLISLAB/360-DL-Survey)。
- en: '![Refer to caption](img/e2cff39bf5f6f53db9d65364adae5ad3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e2cff39bf5f6f53db9d65364adae5ad3.png)'
- en: 'Figure 2: Examples of representative $360^{\circ}$ cameras.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：代表性$360^{\circ}$相机的示例。
- en: '![Refer to caption](img/aa14709abcba3519e1e0b85b4ffe8909.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aa14709abcba3519e1e0b85b4ffe8909.png)'
- en: 'Figure 3: Illustration of ERP, CP and tangent representation types.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：ERP、CP 和切线表示类型的示意图。
- en: 'The rest of the paper is organized as follows. In Sec. [2](#S2 "2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    introduce the imaging principle of ODI, convolution methods for omnidirectional
    vision, and some representative datasets. Sec. [3](#S3 "3 Omnidirectional Vision
    Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    introduces the existing DL approaches for various tasks and provides taxonomies
    to categorize the relevant papers. Sec. [4](#S4 "4 Novel Learning Strategies ‣
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") covers
    novel learning paradigms for the tasks in omnidirectional vision, e.g., unsupervised
    learning, transfer learning, and reinforcement learning. Sec. [5](#S5 "5 Applications
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") then
    scrutinizes the applications, followed by Sec. [6](#S6 "6 Discussion and New Perspectives
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), where
    we discuss open problems and future directions. Finally, we conclude this paper
    in Sec. [7](#S7 "7 Conclusion ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。在第[2](#S2 "2 背景 ‣ 全向视觉中的深度学习：综述与新视角")节中，我们介绍了ODI的成像原理、全向视觉的卷积方法以及一些代表性数据集。第[3](#S3
    "3 全向视觉任务 ‣ 全向视觉中的深度学习：综述与新视角")节介绍了各种任务的现有DL方法，并提供了相关论文的分类。第[4](#S4 "4 新颖学习策略
    ‣ 全向视觉中的深度学习：综述与新视角")节涵盖了全向视觉任务的新颖学习范式，例如无监督学习、迁移学习和强化学习。第[5](#S5 "5 应用 ‣ 全向视觉中的深度学习：综述与新视角")节随后审视了应用，接着是第[6](#S6
    "6 讨论与新视角 ‣ 全向视觉中的深度学习：综述与新视角")节，我们讨论了开放问题和未来方向。最后，第[7](#S7 "7 结论 ‣ 全向视觉中的深度学习：综述与新视角")节总结了本文。
- en: 2 Background
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Omnidirectional Imaging
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 全向成像
- en: 2.1.1 Acquisition
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 获取
- en: 'A normal camera has an FoV less than $180^{\circ}$ and thus captures view at
    most a hemisphere. However, an ideal $360^{\circ}$ camera can capture lights falling
    on the focal point from all directions, making the projection plane a whole spherical
    surface. In practice, most $360^{\circ}$ cameras can not achieve it, which excludes
    top and bottom regions due to dead angles¹¹1[https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera](https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera).
    According to the number of lenses, $360^{\circ}$ cameras can be categorized into
    three types: (i) Cameras with one fisheye lens, which is impossible to cover the
    whole spherical surface. However, if the intrinsic and extrinsic parameters are
    known, an ODI can be achieved by projecting multiple images into a sphere and
    stitching them together; (ii) Cameras with dual fisheye lenses located at opposite
    positions, each of which covers over $180^{\circ}$ FoV, such as Insta360 ONE²²2[https://www.insta360.com/product/insta360-one](https://www.insta360.com/product/insta360-one)
    and LG 360 CAM³³3[https://www.lg.com/sg/lg-friends/lg-360-CAM](https://www.lg.com/sg/lg-friends/lg-360-CAM).
    This type of $360^{\circ}$ cameras have minimum demand for lenses, which are cheap
    and convenient, favoured by industries and customers. Images from the two cameras
    are then stitched together to obtain an omnidirectional image, but the stitching
    process might lead to edge blurring; (iii) Cameras with more than two lenses,
    such as Titan (eight lenses)⁴⁴4[https://www.insta360.com/product/insta360-titan/](https://www.insta360.com/product/insta360-titan/).
    In addition, GoPro Omni⁵⁵5[https://gopro.com/en/us/news/omni-is-here](https://gopro.com/en/us/news/omni-is-here)
    is the first camera rig to place six regular cameras onto six faces of a cube
    and its synthesized results have higher precision and less blur in edges. This
    type of $360^{\circ}$ cameras are professional-level.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 普通相机的视场角小于 $180^{\circ}$，因此最多只能捕捉半球的视图。然而，理想的 $360^{\circ}$ 相机可以捕捉从所有方向射向焦点的光线，使得投影平面成为整个球面。在实际中，大多数
    $360^{\circ}$ 相机无法实现这一点，由于盲区排除了顶部和底部区域¹¹1[https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera](https://en.wikipedia.org/wiki/Omnidirectional_(360-degree)_camera)。根据镜头数量，$360^{\circ}$
    相机可以分为三类：（i）单鱼眼镜头相机，无法覆盖整个球面。然而，如果已知内外参数，可以通过将多张图像投影到球面上并进行拼接来实现全景图像；（ii）两个鱼眼镜头位于对面的位置的相机，每个镜头的视场角超过
    $180^{\circ}$，如 Insta360 ONE²²2[https://www.insta360.com/product/insta360-one](https://www.insta360.com/product/insta360-one)
    和 LG 360 CAM³³3[https://www.lg.com/sg/lg-friends/lg-360-CAM](https://www.lg.com/sg/lg-friends/lg-360-CAM)。这类
    $360^{\circ}$ 相机对镜头的需求最小，价格便宜，方便，受到行业和消费者的青睐。来自两台相机的图像随后被拼接在一起以获得全景图像，但拼接过程可能导致边缘模糊；（iii）具有两个以上镜头的相机，如
    Titan（八个镜头）⁴⁴4[https://www.insta360.com/product/insta360-titan/](https://www.insta360.com/product/insta360-titan/)。此外，GoPro
    Omni⁵⁵5[https://gopro.com/en/us/news/omni-is-here](https://gopro.com/en/us/news/omni-is-here)
    是第一个将六台普通相机放置在立方体的六个面上的相机系统，其合成结果具有更高的精度和较少的边缘模糊。这类 $360^{\circ}$ 相机是专业级的。
- en: 2.1.2 Spherical Imaging
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 球面成像
- en: 'We first define the spherical coordinate $(\theta,\phi,\rho)$, where $\theta\in(0,2\pi)$
    ,$\phi\in(0,\pi)$, and $\rho$ represent the latitude, longitude, and radius of
    the sphere, respectively. We also define the Cartesian coordinate $(x,y,z)$. The
    transformation between spherical coordinate and Cartesian coordinate can be formulated
    as follows [[18](#bib.bib18)]:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义球面坐标 $(\theta,\phi,\rho)$，其中 $\theta\in(0,2\pi)$ ，$\phi\in(0,\pi)$，$\rho$
    分别表示球面的纬度、经度和半径。我们还定义了笛卡尔坐标 $(x,y,z)$。球面坐标与笛卡尔坐标之间的转换公式如下 [[18](#bib.bib18)]：
- en: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\'
- en: \phi\\
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \phi\\
- en: \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
- en: \arctan(x/z)\\
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \arctan(x/z)\\
- en: \arccos(y/\rho)\\
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \arccos(y/\rho)\\
- en: \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
- en: y\\
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: y\\
- en: z\\
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: z\\
- en: \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
- en: \rho\cos(\phi)\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \rho\cos(\phi)\\
- en: \rho\cos(\theta)\sin(\phi)\\
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \rho\cos(\theta)\sin(\phi)\\
- en: \end{array}." display="block"><semantics ><mrow ><mrow ><mrow ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mi  >ρ</mi></mtd></mtr><mtr ><mtd class="ltx_border_l
    ltx_border_r"  ><mi >θ</mi></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mi
    >ϕ</mi></mtd></mtr></mtable><mo >=</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><msup  ><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mi  >x</mi><mn
    >2</mn></msup><mo >+</mo><msup ><mi >y</mi><mn >2</mn></msup><mo >+</mo><msup
    ><mi  >z</mi><mn >2</mn></msup></mrow><mo stretchy="false"  >)</mo></mrow><mrow
    ><mn >1</mn><mo  >/</mo><mn >2</mn></mrow></msup></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >arctan</mi><mo >⁡</mo><mrow  ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo
    >/</mo><mi >z</mi></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mi >arccos</mi><mo >⁡</mo><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><mi >y</mi><mo >/</mo><mi >ρ</mi></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr></mtable></mrow><mo
    lspace="0.667em" rspace="0.667em"  >,</mo><mrow ><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mi >x</mi></mtd></mtr><mtr ><mtd class="ltx_border_l
    ltx_border_r"  ><mi >y</mi></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mi
    >z</mi></mtd></mtr></mtable><mo >=</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mrow ><mi  >ρ</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi
    >sin</mi><mo  >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >θ</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >​</mo><mrow ><mi  >sin</mi><mo
    >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi  >ρ</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi
    >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi  >ρ</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi
    >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo
    stretchy="false" >(</mo><mi  >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em" >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><matrix ><matrixrow ><ci  >𝜌</ci></matrixrow><matrixrow
    ><ci >𝜃</ci></matrixrow><matrixrow ><ci  >italic-ϕ</ci></matrixrow></matrix><matrix
    ><matrixrow ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑦</ci><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑧</ci><cn type="integer" >2</cn></apply></apply><apply
    ><cn type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow
    ><apply  ><apply ><ci >𝑥</ci><ci >𝑧</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >𝑦</ci><ci >𝜌</ci></apply></apply></matrixrow></matrix></apply><apply
    ><matrix ><matrixrow ><ci  >𝑥</ci></matrixrow><matrixrow ><ci >𝑦</ci></matrixrow><matrixrow
    ><ci  >𝑧</ci></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >𝜌</ci><apply
    ><ci  >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply></matrixrow><matrixrow
    ><apply ><ci  >𝜌</ci><apply ><ci >italic-ϕ</ci></apply></apply></matrixrow><matrixrow
    ><apply ><ci  >𝜌</ci><apply ><ci >𝜃</ci></apply><apply ><ci  >italic-ϕ</ci></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{&#124;c&#124;}\rho\\ \theta\\ \phi\\
    \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\ \arctan(x/z)\\
    \arccos(y/\rho)\\ \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\ y\\ z\\ \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
    \rho\cos(\phi)\\ \rho\cos(\theta)\sin(\phi)\\ \end{array}.</annotation></semantics></math>
    |  | (1) |
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{array}[]{&#124;c&#124;}\rho\\ \theta\\ \phi\\ \end{array}=\begin{array}[]{&#124;c&#124;}(x^{2}+y^{2}+z^{2})^{1/2}\\
    \arctan(x/z)\\ \arccos(y/\rho)\\ \end{array}\ ,\ \begin{array}[]{&#124;c&#124;}x\\
    y\\ z\\ \end{array}=\begin{array}[]{&#124;c&#124;}\rho\sin(\theta)\sin(\phi)\\
    \rho\cos(\phi)\\ \rho\cos(\theta)\sin(\phi)\\ \end{array}。
- en: 'Equirectangular Projection (ERP)⁶⁶6[https://en.wikipedia.org/wiki/Equirectangular_projection](https://en.wikipedia.org/wiki/Equirectangular_projection)
    is a representation by uniformly sampling grids from the spherical surface, as
    shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(a). The horizontal unit angle is $\vartheta=2\pi/w$
    and the vertical unit angle is $\varphi=\pi/h$. In particular, if the horizontal
    and vertical unit angle are equal, the width $w$ is twice of height $h$. In a
    word, each pixel coordinate $(u,v)$ in ERP can be mapped to the spherical coordinate
    $(\theta,\phi)=(u\cdot\vartheta,v\cdot\varphi)$ and vice versa. Cubemap Projection
    (CP) projects the spherical surface to six cube faces with $90^{\circ}$ FoV, equal-side
    length $w$, and focal length $\frac{w}{2}$, as shown in Fig. [3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional Vision: A Survey and New
    Perspectives")(b). We denote the cube faces as $f_{i}$, $i\in\{B,D,F,L,R,U\}$,
    representing back, down, front, left, right, and up, respectively. By setting
    the cube center as the origin, the extrinsic matrix of each face can be simplified
    into $90^{\circ}$ or $180^{\circ}$ rotation matrix and zero translation matrix [[19](#bib.bib19)].
    Given a pixel on the plane $f_{i}$, we transform $f_{i}$ to the front plane (identical
    to the Cartesian coordinates) and calculate $(\theta,\phi)$ with Eq. [1](#S2.E1
    "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '等距圆柱投影（ERP）⁶⁶6[https://en.wikipedia.org/wiki/Equirectangular_projection](https://en.wikipedia.org/wiki/Equirectangular_projection)是通过均匀采样球面网格来表示的，如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(a)所示。水平单位角为$\vartheta=2\pi/w$，垂直单位角为$\varphi=\pi/h$。特别地，如果水平和垂直单位角相等，则宽度$w$是高度$h$的两倍。总之，ERP中每个像素坐标$(u,v)$可以映射到球面坐标$(\theta,\phi)=(u\cdot\vartheta,v\cdot\varphi)$，反之亦然。立方体映射（CP）将球面投影到六个具有$90^{\circ}$视场、等长边$w$和焦距$\frac{w}{2}$的立方体面上，如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(b)所示。我们将立方体面表示为$f_{i}$，$i\in\{B,D,F,L,R,U\}$，分别代表后、下、前、左、右和上。通过将立方体中心设置为原点，每个面的外部矩阵可以简化为$90^{\circ}$或$180^{\circ}$旋转矩阵和零平移矩阵[[19](#bib.bib19)]。给定平面$f_{i}$上的一个像素，我们将$f_{i}$变换到前面（等同于笛卡尔坐标系），并使用方程[1](#S2.E1
    "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")计算$(\theta,\phi)$。'
- en: '![Refer to caption](img/d64963dcc89c08ad0848376947052c11.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d64963dcc89c08ad0848376947052c11.png)'
- en: 'Figure 4: An illustration of ERP-based convolution filters on ODIs. (a), (b),
    (c) and (d) are originally shown in [[20](#bib.bib20)], [[21](#bib.bib21)], [[22](#bib.bib22)]
    and [[23](#bib.bib23)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基于ERP的卷积滤波器在全向图像中的示例。(a)、(b)、(c)和(d)最初出现在[[20](#bib.bib20)]、[[21](#bib.bib21)]、[[22](#bib.bib22)]和[[23](#bib.bib23)]中。
- en: 'Tangent Projection is the gnomonic projection [[24](#bib.bib24)], a non-conformal
    projection from points $P_{s}$ on the sphere surface with the sphere center $O$
    to points $P_{t}$ in a tangent plane with center $P_{c}$ ⁷⁷7[https://mathworld.wolfram.com/GnomonicProjection.html](https://mathworld.wolfram.com/GnomonicProjection.html),
    as shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). For a pixel on the ERP image $P_{e}(u_{e},v_{e})$,
    we first calculate its corresponding point $P_{s}(\theta=u_{e}\cdot\vartheta,\phi=v_{e}\cdot\varphi)$
    on the unit sphere, following the transformation in ERP format. The projection
    from $P_{s}(\theta,\phi)$ to $P_{t}(u_{t},v_{t})$ is defined as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '切线投影是*正射投影*[[24](#bib.bib24)]，一种从球面上点$P_{s}$通过球心$O$到切平面上点$P_{t}$的非共形投影，切平面的中心为$P_{c}$⁷⁷7[https://mathworld.wolfram.com/GnomonicProjection.html](https://mathworld.wolfram.com/GnomonicProjection.html)，如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")(c)所示。对于ERP图像上的一个像素$P_{e}(u_{e},v_{e})$，我们首先计算其在单位球上的对应点$P_{s}(\theta=u_{e}\cdot\vartheta,\phi=v_{e}\cdot\varphi)$，遵循ERP格式的变换。$P_{s}(\theta,\phi)$到$P_{t}(u_{t},v_{t})$的投影定义为：'
- en: '|  | <math   alttext="\begin{split}&amp;u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &amp;y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}&amp;u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &amp;y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\'
- en: '&amp;\cos(c)=\sin(\phi_{c})\sin(\phi)+\cos(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c}),\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >u</mi><mi >t</mi></msub><mo
    >=</mo><mfrac  ><mrow ><mrow ><mi  >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mi >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >θ</mi><mo >−</mo><msub ><mi >θ</mi><mi >c</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mrow ><mi >cos</mi><mo lspace="0.167em"
    >⁡</mo><mi  >c</mi></mrow></mfrac></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd
    columnalign="left" ><mrow ><mrow ><msub ><mi >y</mi><mi >t</mi></msub><mo >=</mo><mfrac  ><mrow
    ><mrow ><mrow  ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><msub
    ><mi >ϕ</mi><mi >c</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >−</mo><mrow ><mrow
    ><mi  >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >ϕ</mi><mi
    >c</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em"
    rspace="0em"  >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >θ</mi><mo
    >−</mo><msub ><mi >θ</mi><mi >c</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mrow><mrow
    ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >c</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mfrac></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><mrow ><mi  >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >c</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mrow ><mrow
    ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >ϕ</mi><mi  >c</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >​</mo><mrow
    ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo >+</mo><mrow ><mrow ><mi >cos</mi><mo >⁡</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >ϕ</mi><mi >c</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em" >​</mo><mrow ><mi >cos</mi><mo
    >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em" >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mi >θ</mi><mo >−</mo><msub ><mi >θ</mi><mi >c</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑢</ci><ci  >𝑡</ci></apply><apply
    ><apply ><apply  ><ci >italic-ϕ</ci></apply><apply ><apply ><ci >𝜃</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑐</ci></apply></apply></apply></apply><apply
    ><ci >𝑐</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><ci >𝑡</ci></apply><apply
    ><apply  ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >italic-ϕ</ci><ci >𝑐</ci></apply></apply><apply ><ci  >italic-ϕ</ci></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >italic-ϕ</ci><ci
    >𝑐</ci></apply></apply><apply ><ci >italic-ϕ</ci></apply><apply ><apply ><ci >𝜃</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑐</ci></apply></apply></apply></apply></apply><apply
    ><ci >𝑐</ci></apply></apply></apply><apply ><apply ><ci >𝑐</ci></apply><apply
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >italic-ϕ</ci><ci
    >𝑐</ci></apply></apply><apply ><ci >italic-ϕ</ci></apply></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >italic-ϕ</ci><ci >𝑐</ci></apply></apply><apply
    ><ci >italic-ϕ</ci></apply><apply ><apply ><ci >𝜃</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><ci >𝑐</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&u_{t}=\frac{\cos(\phi)\sin(\theta-\theta_{c})}{\cos{c}},\\
    &y_{t}=\frac{\cos(\phi_{c})\sin(\phi)-\sin(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c})}{\cos(c)},\\
    &\cos(c)=\sin(\phi_{c})\sin(\phi)+\cos(\phi_{c})\cos(\phi)\cos(\theta-\theta_{c}),\end{split}</annotation></semantics></math>
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此处为数学公式，请勿翻译。
- en: 'where $(\theta_{c},\phi_{c})$ is the spherical coordinate of the tangent plane
    center $P_{c}$, and $(u_{t},v_{t})$ is the intersection coordinate of the tangent
    plane and the extension line of $\overrightarrow{OP_{s}}$. The inverse transformations
    are formulated as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\theta_{c},\phi_{c})$ 是切平面中心 $P_{c}$ 的球面坐标，$(u_{t},v_{t})$ 是切平面与 $\overrightarrow{OP_{s}}$
    的延长线的交点坐标。逆变换公式为：
- en: '|  | $\begin{split}&amp;\theta=\theta_{c}+\tan^{-1}(\frac{u_{t}\sin(c)}{\gamma\cos(\phi_{c})\cos(c)-v_{t}\sin(\phi_{c})\sin(c)}),\\
    &amp;\phi=\sin^{-1}(\cos(c)\sin(\phi_{c})+\frac{1}{\gamma}v_{t}\sin(c)\cos(\phi_{c})),\end{split}$
    |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\theta=\theta_{c}+\tan^{-1}(\frac{u_{t}\sin(c)}{\gamma\cos(\phi_{c})\cos(c)-v_{t}\sin(\phi_{c})\sin(c)}),\\
    &amp;\phi=\sin^{-1}(\cos(c)\sin(\phi_{c})+\frac{1}{\gamma}v_{t}\sin(c)\cos(\phi_{c})),\end{split}$
    |  | (3) |'
- en: 'where $\gamma=\sqrt{u_{t}^{2}+v_{t}^{2}}$ and $c=\tan^{-1}\gamma$. With Eqs.
    [2](#S2.E2 "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives") and
    [3](#S2.E3 "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    can build one-to-one forward and inverse mapping functions between the spherical
    coordinates and pixels on the tangent images [[25](#bib.bib25)].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\gamma=\sqrt{u_{t}^{2}+v_{t}^{2}}$ 和 $c=\tan^{-1}\gamma$。通过方程 [2](#S2.E2
    "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives") 和 [3](#S2.E3
    "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")，我们可以建立球面坐标与切平面图像上的像素之间的一对一正向和逆向映射函数[[25](#bib.bib25)]。'
- en: Icosahedron approximates a sphere surface through a Platonic solid [[26](#bib.bib26)].
    Compared with ERP and CP, icosahedron projection has resolved the spherical distortion
    well. While some practical applications need less distortion representations,
    we can increase the number of subdivisions to further mitigate the spherical distortion.
    Specifically, each face in an icosahedron can be subdivided into four smaller
    faces to achieve higher resolution and less distortion [[26](#bib.bib26)]. There
    exist some CNNs that are specifically designed to process an icosahedron [[27](#bib.bib27),
    [28](#bib.bib28)]. It is noteworthy that the choice of subdivision degree needs
    to achieve a trade-off between efficiency and accuracy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 二十面体通过一个柏拉图立体来近似球面[[26](#bib.bib26)]。与ERP和CP相比，二十面体投影很好地解决了球面畸变的问题。虽然一些实际应用需要更少的畸变表示，我们可以增加细分数量以进一步减轻球面畸变。具体而言，二十面体中的每个面可以细分为四个更小的面，以实现更高的分辨率和更少的畸变[[26](#bib.bib26)]。一些CNN专门设计用于处理二十面体[[27](#bib.bib27),
    [28](#bib.bib28)]。值得注意的是，细分程度的选择需要在效率和准确性之间取得平衡。
- en: Other projections. For CP, different sampling locations on the cube faces decide
    different spatial sampling rates, resulting in the distortions. To address this
    problem, Equi-Angular Cubemap (EAC) projection ⁸⁸8[https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/](https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/)
    is proposed to keep the sampling uniform. Besides, some projections can transform
    the spherical surface into non-spatial domains, e.g., 3D rotation group (SO3) [[29](#bib.bib29)]
    and spherical Fourier transformation (SFT) [[30](#bib.bib30)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其他投影。对于CP，立方体面上的不同采样位置决定了不同的空间采样率，从而导致畸变。为了解决这个问题，提出了等角立方图（EAC）投影 ⁸⁸8[https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/](https://blog.google/products/google-ar-vr/bringing-pixels-front-and-center-vr-video/)以保持均匀采样。此外，一些投影可以将球面转换为非空间域，例如3D旋转群（SO3）[[29](#bib.bib29)]和球面傅里叶变换（SFT）[[30](#bib.bib30)]。
- en: 2.1.3 Spherical Stereo
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 球面立体视觉
- en: 'Spherical stereo is about two viewpoints displaced with a known horizontal
    or vertical baseline [[18](#bib.bib18)]. Due to the spherical projection, spherical
    stereo is more irregular than stereo with traditional pinhole cameras. According
    to Eq. [1](#S2.E1 "In 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣
    2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    we define the baseline as $\textbf{b}=(\delta x,\delta y,\delta z)$, and the derivative
    correspondence between the spherical coordinates and Cartesian coordinates can
    be formulated as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '球面立体视觉涉及的是两个视点，两个视点之间具有已知的水平或垂直基线 [[18](#bib.bib18)]。由于球面投影，球面立体视觉比传统针孔相机的立体视觉更加不规则。根据公式
    [1](#S2.E1 "在 2.1.2 球面成像 ‣ 2.1 全景成像 ‣ 2 背景 ‣ 深度学习在全景视觉中的应用: 调查与新视角")，我们将基线定义为
    $\textbf{b}=(\delta x,\delta y,\delta z)$，球面坐标与笛卡尔坐标之间的导数对应关系可以表示如下：'
- en: '|  | <math   alttext="\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\'
- en: \delta_{\phi}\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \delta_{\phi}\\
- en: \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&amp;\cos(\phi)&amp;\cos(\theta)\sin(\phi)\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&amp;\cos(\phi)&amp;\cos(\theta)\sin(\phi)\\
- en: \frac{\cos(\theta)}{\rho\sin(\phi)}&amp;0&amp;\frac{-\sin(\theta)}{\rho\sin(\phi)}\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\cos(\theta)}{\rho\sin(\phi)}&amp;0&amp;\frac{-\sin(\theta)}{\rho\sin(\phi)}\\
- en: \frac{\sin(\theta)\cos(\phi)}{\rho}&amp;\frac{-\sin(\phi)}{\rho}&amp;\frac{\cos(\theta)\cos(\phi)}{\rho}\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\sin(\theta)\cos(\phi)}{\rho}&amp;\frac{-\sin(\phi)}{\rho}&amp;\frac{\cos(\theta)\cos(\phi)}{\rho}\\
- en: \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\
- en: \delta y\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \delta y\\
- en: \delta z\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \delta z\\
- en: \end{array}." display="block"><semantics ><mrow ><mrow  ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >δ</mi><mi
    >ρ</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >δ</mi><mi >θ</mi></msub></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >δ</mi><mi >ϕ</mi></msub></mtd></mtr></mtable><mo
    >=</mo><mrow ><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd   ><mrow ><mrow ><mi  >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >θ</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo  >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd><mtd ><mrow  ><mi >cos</mi><mo >⁡</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >θ</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mstyle displaystyle="false"
    ><mfrac  ><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mrow ><mi  >ρ</mi><mo lspace="0.167em"
    rspace="0em"  >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mo rspace="0.167em"
    >−</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mi >ρ</mi><mo
    lspace="0.167em" rspace="0em" >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mrow ><mi >sin</mi><mo >⁡</mo><mrow
    ><mo stretchy="false" >(</mo><mi >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >ρ</mi></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow
    ><mo rspace="0.167em" >−</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi >ρ</mi></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac ><mrow  ><mrow ><mi >cos</mi><mo >⁡</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >ρ</mi></mfrac></mstyle></mtd></mtr></mtable><mo lspace="0.667em" rspace="0em"  >​</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mrow ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi></mrow></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >z</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><matrix  ><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛿</ci><ci >𝜌</ci></apply></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝛿</ci><ci >𝜃</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝛿</ci><ci >italic-ϕ</ci></apply></matrixrow></matrix><apply
    ><matrix ><matrixrow  ><apply ><apply ><ci  >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><apply
    ><ci  >italic-ϕ</ci></apply><apply ><apply ><ci  >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >𝜃</ci></apply><apply ><ci >𝜌</ci><apply ><ci >italic-ϕ</ci></apply></apply></apply><cn
    type="integer" >0</cn><apply ><apply  ><apply ><ci >𝜃</ci></apply></apply><apply
    ><ci >𝜌</ci><apply ><ci >italic-ϕ</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><ci
    >𝜌</ci></apply><apply ><apply ><apply ><ci >italic-ϕ</ci></apply></apply><ci >𝜌</ci></apply><apply
    ><apply  ><apply ><ci >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><ci
    >𝜌</ci></apply></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >𝛿</ci><ci
    >𝑥</ci></apply></matrixrow><matrixrow ><apply ><ci >𝛿</ci><ci >𝑦</ci></apply></matrixrow><matrixrow
    ><apply  ><ci >𝛿</ci><ci >𝑧</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{&#124;c&#124;}\delta_{\rho}\\ \delta_{\theta}\\
    \delta_{\phi}\\ \end{array}=\begin{array}[]{&#124;c c c&#124;}\sin(\theta)\sin(\phi)&\cos(\phi)&\cos(\theta)\sin(\phi)\\
    \frac{\cos(\theta)}{\rho\sin(\phi)}&0&\frac{-\sin(\theta)}{\rho\sin(\phi)}\\ \frac{\sin(\theta)\cos(\phi)}{\rho}&\frac{-\sin(\phi)}{\rho}&\frac{\cos(\theta)\cos(\phi)}{\rho}\\
    \end{array}\ \begin{array}[]{&#124;c&#124;}\delta x\\ \delta y\\ \delta z\\ \end{array}.</annotation></semantics></math>
    |  | (4) |
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}。" display="block"><semantics ><mrow ><mrow  ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >δ</mi><mi
    >ρ</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >δ</mi><mi >θ</mi></msub></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><msub ><mi >δ</mi><mi >ϕ</mi></msub></mtd></mtr></mtable><mo
    >=</mo><mrow ><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd   ><mrow ><mrow ><mi  >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >θ</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo  >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd><mtd ><mrow  ><mi >cos</mi><mo >⁡</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi  >θ</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi >ϕ</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mstyle displaystyle="false"
    ><mfrac  ><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mi
    >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mrow ><mi  >ρ</mi><mo lspace="0.167em"
    rspace="0em"  >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mo rspace="0.167em"
    >−</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mi >ρ</mi><mo
    lspace="0.167em" rspace="0em" >​</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mrow ><mi >sin</mi><mo >⁡</mo><mrow
    ><mo stretchy="false" >(</mo><mi >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >ρ</mi></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow
    ><mo rspace="0.167em" >−</mo><mrow ><mi >sin</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi >ρ</mi></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac ><mrow  ><mrow ><mi >cos</mi><mo >⁡</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi >cos</mi><mo >⁡</mo><mrow ><mo
    stretchy="false"  >(</mo><mi >ϕ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mi
    >ρ</mi></mfrac></mstyle></mtd></mtr></mtable><mo lspace="0.667em" rspace="0em"  >​</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >x</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_border_l ltx_border_r"  ><mrow ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi></mrow></mtd></mtr><mtr ><mtd class="ltx_border_l ltx_border_r"  ><mrow
    ><mi >δ</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >z</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><matrix  ><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝛿</ci><ci >𝜌</ci></apply></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝛿</ci><ci >𝜃</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝛿</ci><ci >italic-ϕ</ci></apply></matrixrow></matrix><apply
    ><matrix ><matrixrow  ><apply ><apply ><ci  >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><apply
    ><ci  >italic-ϕ</ci></apply><apply ><apply ><ci  >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><ci >𝜃</ci></apply><apply ><ci >𝜌</ci><apply ><ci >italic-ϕ</ci></apply></apply></apply><cn
    type="integer" >0</cn><apply ><apply  ><apply ><ci >𝜃</ci></apply></apply><apply
    ><ci >𝜌</ci><apply ><ci >italic-ϕ</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><ci
    >𝜌</ci></apply><apply ><apply ><apply ><ci >italic-ϕ</ci></apply></apply><ci >𝜌</ci></apply><apply
    ><apply  ><apply ><ci >𝜃</ci></apply><apply ><ci >italic-ϕ</ci></apply></apply><ci
    >𝜌</ci></apply></matrixrow></matrix><matrix ><matrixrow ><apply  ><ci >𝛿</ci><ci
    >𝑥</ci></apply></matrixrow><matrixrow ><apply ><ci >𝛿</ci><ci >
- en: 'In Eq. [4](#S2.E4 "In 2.1.3 Spherical Stereo ‣ 2.1 Omnidirectional Imaging
    ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    $(\delta_{\theta},\delta_{\phi})$ represents the angular differences in the spherical
    coordinates $(\theta,\phi,\rho)$. According to Eq. [4](#S2.E4 "In 2.1.3 Spherical
    Stereo ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), we can find that for the vertical baseline
    $\textbf{b}_{v}=(0,\delta y,0)$, there is no difference in $\theta$, which is
    simpler. However, for horizontal baseline $\textbf{b}_{h}=(\delta x,0,0)$, differences
    occur in both angles $\theta$ and $\phi$.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '在公式 [4](#S2.E4 "In 2.1.3 Spherical Stereo ‣ 2.1 Omnidirectional Imaging ‣ 2
    Background ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    中，$(\delta_{\theta},\delta_{\phi})$ 表示球面坐标 $(\theta,\phi,\rho)$ 中的角度差异。根据公式 [4](#S2.E4
    "In 2.1.3 Spherical Stereo ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")，我们可以发现对于垂直基线
    $\textbf{b}_{v}=(0,\delta y,0)$，$\theta$ 没有变化，这种情况较为简单。然而，对于水平基线 $\textbf{b}_{h}=(\delta
    x,0,0)$，$\theta$ 和 $\phi$ 两个角度都会发生差异。'
- en: 2.2 Convolution Methods on ODI
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 ODI上的卷积方法
- en: 'As the natural projection surface of an ODI is a sphere, standard CNNs are
    less capable of processing the inherent distortions when the spherical image is
    projected back to a plane. Numerous CNN-based methods have been proposed to enhance
    the extraction of ”unbiased” information from spherical images. These methods
    can be classified into two prevailing categories: (i) Applying 2D convolution
    filters on planar projections; (ii) Directly leveraging spherical convolution
    filters in the spherical domain. In this subsection, we analyze these methods
    in detail.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ODI的自然投影表面是球面，标准的CNN在将球面图像投影回平面时，处理固有的畸变能力较差。为了增强从球面图像中提取“无偏”信息的能力，已经提出了许多基于CNN的方法。这些方法可以分为两类：
    (i) 在平面投影上应用2D卷积滤波器； (ii) 直接在球面域中利用球面卷积滤波器。在本小节中，我们将详细分析这些方法。
- en: 2.2.1 Planar Projection-based Convolution
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于平面投影的卷积
- en: 'As the most common sphere-to-plane projection, ERP introduces severe distortions,
    especially at the poles. Considering it provides global information and takes
    less computation cost, Su et al. [[20](#bib.bib20)] proposed a representative
    method named Spherical Convolution, which leverages regular convolution filters
    with the adaptive kernel size according to the spherical coordinates. However,
    as shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional
    Imaging ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")(a), the regular convolution weights are only shared along each
    row and can not be trained from scratch. Inspired by Spherical Convolution, SphereNet [[21](#bib.bib21)]
    proposes another typical method that processes the ERP by directly adjusting the
    sampling grid locations of convolution filters to achieve the distortion invariance
    and can be trained end-to-end, as depicted in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2
    Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(b). This is conceptually
    similar to those in [[22](#bib.bib22)], [[23](#bib.bib23)], as shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)
    and (d). In particular, before ODIs are widely applied, Cohen et al. [[29](#bib.bib29)]
    have discussed the spatially varying distortions introduced by ERP and proposed
    a rotation-invariant spherical CNN approach to learn an SO3 representation. By
    contrast, KTN [[31](#bib.bib31), [32](#bib.bib32)] learns a transfer function
    to achieve that the convolution kernel, which is learnt from the conventional
    planar images, can be directly applied on ERP without retraining. In [[33](#bib.bib33)],
    the ERP is represented as a weighted graph, and a novel graph construction method
    is introduced by incorporating the geometry of the omnidirectional cameras into
    the graph structure to mitigate the distortions. [[19](#bib.bib19), [34](#bib.bib34)]
    focused on directly applying traditional 2D CNNs on CP and tangent projection,
    which are distortion-less.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '作为最常见的球面到平面投影，ERP 引入了严重的畸变，特别是在极点处。考虑到它提供了全球信息且计算成本较低，Su 等人[[20](#bib.bib20)]
    提出了一个名为球面卷积的代表性方法，该方法利用根据球面坐标调整的自适应核大小的常规卷积滤波器。然而，如图 [4](#S2.F4 "Figure 4 ‣ 2.1.2
    Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(a) 所示，常规卷积权重仅在每一行之间共享，且不能从头开始训练。受到球面卷积的启发，SphereNet[[21](#bib.bib21)]
    提出了另一种典型的方法，通过直接调整卷积滤波器的采样网格位置来处理 ERP，以实现畸变不变性，并且可以端到端训练，如图 [4](#S2.F4 "Figure
    4 ‣ 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives")(b) 所示。这在概念上类似于 [[22](#bib.bib22)]、[[23](#bib.bib23)]
    中的内容，如图 [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spherical Imaging ‣ 2.1 Omnidirectional Imaging
    ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)
    和 (d) 所示。特别是，在 ODI 广泛应用之前，Cohen 等人[[29](#bib.bib29)] 讨论了 ERP 引入的空间变异畸变，并提出了一种旋转不变的球面
    CNN 方法来学习 SO3 表示。相比之下，KTN[[31](#bib.bib31), [32](#bib.bib32)] 学习一个传递函数，以实现从常规平面图像中学习的卷积核可以直接应用于
    ERP，而无需重新训练。在 [[33](#bib.bib33)] 中，ERP 被表示为加权图，并通过将全向摄像机的几何结构纳入图结构中来引入了一种新的图构建方法，以缓解畸变。[[19](#bib.bib19),
    [34](#bib.bib34)] 关注于直接将传统的 2D CNN 应用于 CP 和切向投影，这些投影没有畸变。'
- en: '![Refer to caption](img/c28b9e7f40390f01bf66f76a0f5a5b5c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c28b9e7f40390f01bf66f76a0f5a5b5c.png)'
- en: 'Figure 5: The two representative spherical convolution approaches. (a) and
    (b) are originally shown in [[35](#bib.bib35)] and [[27](#bib.bib27)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：两种代表性的球面卷积方法。（a）和（b）最初展示于 [[35](#bib.bib35)] 和 [[27](#bib.bib27)]。
- en: 2.2.2 Spherical Convolution
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 球面卷积
- en: 'Some methods have explored the special convolution filters in the spherical
    domain. Esteves et al. [[36](#bib.bib36)] proposed the first spherical CNN architecture,
    which considers the convolution filters in the spherical harmonic domain, to address
    the problem of 3D rotation equivariance in standard CNNs. Unlike [[36](#bib.bib36)],
    Yang et al. [[35](#bib.bib35)] proposed a representative framework to map spherical
    images into the rotation-equivariant representations based on the geometry of
    spherical surfaces. As shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2.1 Planar Projection-based
    Convolution ‣ 2.2 Convolution Methods on ODI ‣ 2 Background ‣ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives")(a), SGCN [[35](#bib.bib35)]
    represents the input spherical image as a graph based on the GICOPix [[35](#bib.bib35)].
    Moreover, it explores the isometric transformation equivariance of the graph through
    GCN layers. A similar strategy is proposed in [[37](#bib.bib37)] and [[38](#bib.bib38)].
    In [[37](#bib.bib37)], the gauge equivariant CNNs are proposed to learn spherical
    representations from the icosahedron. By contrast, Shakerinava et al. [[38](#bib.bib38)]
    extended the icosahedron to all the pixelizations of platonic solids and generalized
    the gauge equivariant CNNs on the pixelized spheres. Due to a trade-off between
    efficiency and rotation equivariance, DeepSphere [[39](#bib.bib39)] models the
    sampled sphere as a graph of connected pixels and designs a novel graph convolution
    network (GCN) to balance the computational efficiency and sampling flexibility
    by adjusting the neighboring pixel numbers of the pixels on the graph. Compared
    with the methods above, another representative ODI representation is proposed
    in SpherePHD [[27](#bib.bib27)]. As shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2.1
    Planar Projection-based Convolution ‣ 2.2 Convolution Methods on ODI ‣ 2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b),
    SpherePHD represents the spherical image as the spherical polyhedron and provides
    specific convolution and pooling methods.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '一些方法已经探索了球面领域中的特殊卷积滤波器。Esteves 等人 [[36](#bib.bib36)] 提出了第一个球面 CNN 架构，该架构考虑了球面调和域中的卷积滤波器，以解决标准
    CNN 中 3D 旋转等变性的问题。与 [[36](#bib.bib36)] 不同，Yang 等人 [[35](#bib.bib35)] 提出了一个代表性框架，将球面图像映射到基于球面几何的旋转等变表示。如图 [5](#S2.F5
    "Figure 5 ‣ 2.2.1 Planar Projection-based Convolution ‣ 2.2 Convolution Methods
    on ODI ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")(a) 所示，SGCN [[35](#bib.bib35)] 将输入的球面图像表示为基于 GICOPix [[35](#bib.bib35)]
    的图。此外，它通过 GCN 层探索了图的等距变换等变性。在 [[37](#bib.bib37)] 和 [[38](#bib.bib38)] 中也提出了类似的策略。在
    [[37](#bib.bib37)] 中，提出了从二十面体学习球面表示的测度等变 CNN。相比之下，Shakerinava 等人 [[38](#bib.bib38)]
    将二十面体扩展到所有柏拉图固体的像素化，并将测度等变 CNN 泛化到像素化球面上。由于效率与旋转等变性之间的权衡，DeepSphere [[39](#bib.bib39)]
    将采样球体建模为连接像素的图，并设计了一种新颖的图卷积网络 (GCN)，通过调整图上像素的邻近像素数量来平衡计算效率和采样灵活性。与上述方法相比，SpherePHD [[27](#bib.bib27)]
    提出了另一种代表性的 ODI 表示。如图 [5](#S2.F5 "Figure 5 ‣ 2.2.1 Planar Projection-based Convolution
    ‣ 2.2 Convolution Methods on ODI ‣ 2 Background ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(b) 所示，SpherePHD 将球面图像表示为球面多面体，并提供了特定的卷积和池化方法。'
- en: 2.3 Dataset
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据集
- en: 'TABLE I: Summary of ODI image and video datasets. N/A indicates ‘not available’
    and GT indicates ‘ground truth’.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：ODI 图像和视频数据集摘要。N/A 表示“不可用”，GT 表示“真实值”。
- en: '| Dataset | Size | Data Type | Resolution | GT | Purpose |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 数据类型 | 分辨率 | GT | 目的 |'
- en: '| Stanford2D3D [[5](#bib.bib5)] | 70496 RGB+1413 ERP images | Real | 1080 $\times$
    1080 | $\checkmark$ | Object Detection, Scene Uderstanding |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Stanford2D3D [[5](#bib.bib5)] | 70496 RGB+1413 ERP 图像 | 真实 | 1080 $\times$
    1080 | $\checkmark$ | 物体检测，场景理解 |'
- en: '| Structured3D [[40](#bib.bib40)] | 196k images | Synthetic | 512 $\times$
    1024 | ✗ | Object Detection, Scene Understanding, Image Synthesis, 3D Modeling
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Structured3D [[40](#bib.bib40)] | 196k 图像 | 合成 | 512 $\times$ 1024 | ✗ |
    物体检测，场景理解，图像合成，3D 建模 |'
- en: '| SUNCG [[41](#bib.bib41)] | 45622 scenes | Synthetic | N/A | ✗ | Depth Estimation
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SUNCG [[41](#bib.bib41)] | 45622 场景 | 合成 | N/A | ✗ | 深度估计 |'
- en: '| 360-Sport [[42](#bib.bib42)] | 342 360^∘ videos | Real | N/A | $\checkmark$
    | Visual Pilot |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 360-Sport [[42](#bib.bib42)] | 342 个 360^∘ 视频 | 真实 | N/A | $\checkmark$ |
    视觉导航 |'
- en: '| Wild-360 [[43](#bib.bib43)] | 85 360^∘ videos | Real | N/A | $\checkmark$
    | Video Saliency |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Wild-360 [[43](#bib.bib43)] | 85 个 360^∘ 视频 | 真实 | N/A | $\checkmark$ | 视频显著性
    |'
- en: 'The performance of the DL-based approaches is closely related to the qualities
    and quantities of the datasets. With the development of spherical imaging devices,
    a large number of ODI and ODV datasets are publicly available for various vision
    tasks. Especially, most ODV data is collected from public video sharing platforms
    like Vimeo and Youtube. In Table. [I](#S2.T1 "TABLE I ‣ 2.3 Dataset ‣ 2 Background
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), we
    list some representative ODI and ODV datasets used for different purposes and
    we also show their properties, e.g., size, resolution, data source. Complete summary
    of datasets can be found in the suppl. material. According to the data source,
    there are two categories of datasets: real-world datasets and synthetic datasets.
    Most real-world datasets only provide images of 2D projection modality and are
    applied to some specific task. However, Stanford2D3D [[5](#bib.bib5)] contains
    three modalities, including 2D, 2.5D, that are suitable for cross-modal learning.
    Moreover, some datasets are selected from the existing ones, such as PanoContext [[7](#bib.bib7)]
    collected from SUN360 [[3](#bib.bib3)]. For the synthetic datasets, images are
    complete and high-quality without natural noise, and the annotations are easier
    to obtain than that in the real-world scenes. For instance, SUNCG [[41](#bib.bib41)]
    is created via the Plannar5D platform, and all the 3D scenes are composed of individually
    labeled 3D object meshes. Structured3D [[40](#bib.bib40)] and OmniFlow [[44](#bib.bib44)]
    utilize the rendering engine to generate photo-realistic images containing 3D
    structure annotations and corresponding optical flows. Similar to real-world datasets,
    there are also some datasets, e.g., omni-SYNTHIA [[45](#bib.bib45)], extracted
    from the large synthetic ones for specific tasks.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的方法的性能与数据集的质量和数量密切相关。随着球形成像设备的发展，大量ODI和ODV数据集在各种视觉任务中公开可用。特别是，大多数ODV数据来自像Vimeo和YouTube这样的公共视频分享平台。在表[I](#S2.T1
    "TABLE I ‣ 2.3 Dataset ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision:
    A Survey and New Perspectives")中，我们列出了一些用于不同目的的代表性ODI和ODV数据集，并展示了它们的属性，例如，大小、分辨率、数据来源。数据集的完整总结可以在补充材料中找到。根据数据来源，数据集分为两类：真实世界数据集和合成数据集。大多数真实世界数据集仅提供2D投影模态的图像，并应用于某些特定任务。然而，Stanford2D3D
    [[5](#bib.bib5)]包含三种模态，包括2D和2.5D，适合跨模态学习。此外，一些数据集是从现有数据集中选择的，例如从SUN360 [[3](#bib.bib3)]中收集的PanoContext
    [[7](#bib.bib7)]。对于合成数据集，图像完整且高质量，没有自然噪声，注释比现实世界场景中更容易获得。例如，SUNCG [[41](#bib.bib41)]
    是通过Plannar5D平台创建的，所有3D场景由单独标记的3D物体网格组成。Structured3D [[40](#bib.bib40)] 和OmniFlow
    [[44](#bib.bib44)] 利用渲染引擎生成包含3D结构注释和相应光流的照片级真实图像。类似于真实世界数据集，还有一些数据集，例如 omni-SYNTHIA
    [[45](#bib.bib45)]，是从大型合成数据集中提取的，用于特定任务。'
- en: 3 Omnidirectional Vision Tasks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 全向视觉任务
- en: 3.1 Image/Video Manipulation
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 图像/视频处理
- en: 3.1.1 Image Generation
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 图像生成
- en: 'Insight: Image generation aims to restore or synthesize the complete and clean
    ODI data from the partial or noisy data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：图像生成旨在从部分或嘈杂的数据中恢复或合成完整而清晰的ODI数据。
- en: 'For image generation on ODI, there exist four popular research directions:
    (i) panoramic depth map completion; (ii) ODI completion; (iii) panoramic semantic
    map completion; (iv) view synthesis on ODI. In this subsection, we provide a comprehensive
    analysis of some representative works.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ODI的图像生成，存在四个热门研究方向：（i）全景深度图完成；（ii）ODI完成；（iii）全景语义图完成；（iv）ODI上的视图合成。在这一小节中，我们提供了一些代表性工作的全面分析。
- en: 'Depth Completion: Due to the scarcity of real-world sparse-to-dense panoramic
    depth maps, this task mainly utilizes simulation techniques to generate artificially
    sparse depth maps as the training data. Liu et al. [[46](#bib.bib46)] proposed
    a representative two-stage framework to achieve panoramic depth completion. In
    the first stage, a spherical normalized convolution network is proposed to predict
    the initial dense depth maps and confidence maps from the sparse depth inputs.
    Then the output of the first stage is combined with corresponding ODIs to generate
    the final panoramic dense depth maps through a cross-modal depth completion network.
    Especially, BIPS [[47](#bib.bib47)] proposes a GAN framework to synthesize RGB-D
    indoor panoramas from the limited input information about a scene captured by
    the camera and depth sensors in arbitrary configurations. However, BIPS ignores
    a large distribution gap between synthesized and real LIDAR scanners, which could
    be better addressed with domain adaptation techniques.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 深度补全：由于现实世界中稀疏到密集的全景深度图非常稀缺，该任务主要利用模拟技术生成人工稀疏深度图作为训练数据。Liu 等人 [[46](#bib.bib46)]
    提出了一个具有代表性的两阶段框架来实现全景深度补全。在第一阶段，提出了一种球面归一化卷积网络，用于从稀疏深度输入中预测初始密集深度图和置信图。然后，将第一阶段的输出与对应的ODI结合，通过跨模态深度补全网络生成最终的全景密集深度图。特别地，BIPS
    [[47](#bib.bib47)] 提出了一个GAN框架，从摄像机和深度传感器在任意配置下捕获的场景的有限输入信息中合成RGB-D室内全景。然而，BIPS忽略了合成与真实LIDAR扫描仪之间的大分布差距，这可以通过领域适应技术得到更好的解决。
- en: 'ODI Completion: It aims to fill in missing areas to generate complete and plausible
    ODIs. Considering the high degree of freedom involved in generating an ODI from
    a single limited FoV image, Hara et al. [[48](#bib.bib48)] leveraged a fundamental
    property of the spherical structure, scene symmetry, to control the degree of
    freedom and improve the plausibility of the generated ODI. On the opposite of [[48](#bib.bib48)],
    Akimoto et al. [[49](#bib.bib49)] proposed a transformer-based framework to synthesize
    the ODIs with arbitrary resolution from a fixed limited FoV image and encouraged
    the diversity of synthesized ODIs. In addition, Sumantri et al. [[50](#bib.bib50)]
    proposed a first pipeline to reconstruct the ODIs from a set of unknown FoV images
    without any overlap, including two steps: (i) FoV estimation of input images relative
    to the panorama; (ii) ODI synthesis with the input images and estimated FoVs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ODI补全：它旨在填补缺失区域以生成完整且合理的ODI。考虑到从单个有限FoV图像生成ODI时涉及的高度自由度，Hara等人 [[48](#bib.bib48)]
    利用球面结构的基本属性——场景对称性——来控制自由度并提高生成ODI的合理性。与 [[48](#bib.bib48)] 相对的是，Akimoto 等人 [[49](#bib.bib49)]
    提出了一个基于变换器的框架，从固定的有限FoV图像中合成任意分辨率的ODI，并鼓励合成ODI的多样性。此外，Sumantri 等人 [[50](#bib.bib50)]
    提出了一个首个从一组未知FoV图像（无重叠）中重建ODI的流程，包括两个步骤：（i）相对于全景的输入图像FoV估计；（ii）使用输入图像和估计的FoV进行ODI合成。
- en: 'Semantic Scene Completion (SSC): It aims to reconstruct the indoor scenes with
    both the occupancy and semantic labels of the whole room. Existing works, e.g., [[51](#bib.bib51)],
    are mostly based on the RGB-D data and LiDAR scanners. As the first work to accomplish
    the SSC task using the ODI data, [[52](#bib.bib52)] used only a single ODI and
    its corresponding depth map as the input and generates a voxel grid from the input
    panoramic depth map. This voxel grid is partitioned into eight overlapping views,
    and each partitioned grid, representing a single view of a regular RGB-D sensor,
    is submitted to the 3D CNN model [[53](#bib.bib53)], pre-trained on the standard
    2.5D synthetic RGB-D data. These partial inferences are aligned and ensembled
    to obtain the final result.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 语义场景补全（SSC）：它旨在重建具有整个房间占用和语义标签的室内场景。现有工作，如 [[51](#bib.bib51)]，大多基于RGB-D数据和LiDAR扫描仪。作为首个使用ODI数据完成SSC任务的工作，[[52](#bib.bib52)]
    仅使用单个ODI及其对应的深度图作为输入，并从输入的全景深度图生成体素网格。该体素网格被划分为八个重叠视图，每个划分的网格，代表一个普通RGB-D传感器的单一视图，被提交给预先在标准2.5D合成RGB-D数据上训练的3D
    CNN模型[[53](#bib.bib53)]。这些部分推断结果被对齐并合并，以获得最终结果。
- en: 'TABLE II: Cross-view synthesis and geo-localization by some representative
    methods.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：一些代表性方法的跨视图合成和地理定位。
- en: '| Method | Publication | Input | View synthesis | Localization | Highlight
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表时间 | 输入 | 视图合成 | 定位 | 亮点 |'
- en: '| Lu [[54](#bib.bib54)] | CVPR’20 | Image | $\checkmark$ | ✗ | Utilizing depth
    and semantics |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Lu [[54](#bib.bib54)] | CVPR’20 | 图像 | $\checkmark$ | ✗ | 利用深度和语义 |'
- en: '| Li [[55](#bib.bib55)] | ICCV’21 | Video | $\checkmark$ | ✗ | 3D point cloud
    representation with depth and semantics |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Li [[55](#bib.bib55)] | ICCV’21 | 视频 | $\checkmark$ | ✗ | 带有深度和语义的3D点云表示
    |'
- en: '| Zhai [[56](#bib.bib56)] | CVPR’17 | Image | $\checkmark$ | $\checkmark$ |
    Pretraining semantic segmentation task with transfer learning |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Zhai [[56](#bib.bib56)] | CVPR’17 | 图像 | $\checkmark$ | $\checkmark$ | 使用迁移学习进行语义分割任务预训练
    |'
- en: '| Regmi [[57](#bib.bib57)] | ICCV’19 | Image | $\checkmark$ | $\checkmark$
    | Two stage training: Satellite-view synthesis and feature matching |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Regmi [[57](#bib.bib57)] | ICCV’19 | 图像 | $\checkmark$ | $\checkmark$ | 两阶段训练：卫星视图合成和特征匹配
    |'
- en: '| Toker [[58](#bib.bib58)] | CVPR’21 | Image | $\checkmark$ | $\checkmark$
    | End-to-end training for view synthesis and feature matching |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Toker [[58](#bib.bib58)] | CVPR’21 | 图像 | $\checkmark$ | $\checkmark$ | 视图合成和特征匹配的端到端训练
    |'
- en: '| Shi [[59](#bib.bib59)] | NIPS’19 | Image | ✗ | $\checkmark$ | Polar transform
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[59](#bib.bib59)] | NIPS’19 | 图像 | ✗ | $\checkmark$ | 极坐标变换 |'
- en: '| Zhu [[60](#bib.bib60)] | CVPR’22 | Image | ✗ | $\checkmark$ | Attention-based
    transformer and remove uninformative patches |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[60](#bib.bib60)] | CVPR’22 | 图像 | ✗ | $\checkmark$ | 基于注意力的变换器并去除无信息补丁
    |'
- en: '| Shi [[61](#bib.bib61)] | CVPR’20 | Image | ✗ | $\checkmark$ | Adding orientation
    estimation during localization |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[61](#bib.bib61)] | CVPR’20 | 图像 | ✗ | $\checkmark$ | 在定位过程中添加方向估计 |'
- en: '| Zhu [[62](#bib.bib62)] | CVPR’21 | Image | ✗ | $\checkmark$ | Proposing that
    multiple satellite images can cover one ground image |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[62](#bib.bib62)] | CVPR’21 | 图像 | ✗ | $\checkmark$ | 提出多个卫星图像可以覆盖一个地面图像
    |'
- en: 'View Synthesis: View synthesis aims to generate ODIs from unknown viewpoints.
    OmniNeRF, proposed by Hsu et al. [[63](#bib.bib63)], is the first and representative
    learning approach for panoramic view synthesis. To generate a novel view ODI,
    it first projects an ODI to the 3D domain with an auxiliary depth map and a derived
    gradient image, and then translates the view position to re-project the 3D coordinates
    to 2D space. The neural radiance fields (NeRF) [[64](#bib.bib64)] is used to learn
    the pixel-based representations and solve the information missing problem caused
    by viewpoint translation. A similar strategy, proposed by [[65](#bib.bib65)],
    leverages a conditional generator to synthesize the novel view. With video as
    the input, Pathdreamer [[66](#bib.bib66)] designs a hierarchical architecture
    to conduct the non-observed view synthesis from one previous observation and the
    trajectory of future viewpoints.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 视图合成：视图合成的目标是从未知视角生成ODI。由Hsu等人提出的OmniNeRF [[63](#bib.bib63)] 是全景视图合成的首个代表性学习方法。为了生成新视角的ODI，它首先通过辅助深度图和衍生的梯度图像将ODI投影到3D领域，然后转换视图位置以重新投影3D坐标到2D空间。神经辐射场（NeRF）
    [[64](#bib.bib64)] 用于学习基于像素的表示，并解决由于视角转换引起的信息丢失问题。类似的策略，由 [[65](#bib.bib65)] 提出，利用条件生成器合成新视图。以视频作为输入，Pathdreamer
    [[66](#bib.bib66)] 设计了一个分层结构，以从一次前观测和未来视角的轨迹中进行未观测视图合成。
- en: 3.1.2 Cross-view Synthesis and Geo-localization
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 跨视图合成与地理定位
- en: 'Insight: Cross-view synthesis aims to synthesize ground-view ODIs from the
    satellite-view images while geo-localization aims to match the ground-view ODIs
    and satellite-view images to determine their relations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：跨视图合成旨在从卫星视图图像合成地面视图ODI，而地理定位旨在匹配地面视图ODI和卫星视图图像以确定它们之间的关系。
- en: 'Ground-view, a.k.a., street-view images are usually panoramic to provide complete
    surrounding information, while satellite views are planar images captured to cover
    almost every corner of the world. There exist a few methods to synthesize ground-view
    images from satellite-view images. Lu et al. [[54](#bib.bib54)] proposed a representative
    work including three stages: satellite stage, geo-transformation stage, and street-view
    stage. The satellite stage predicts depth maps and segmentation maps from satellite
    images. The geo-transformation stage transforms the output of the satellite stage
    into the panoramas. Finally, the street-view stage predicts the street-view panoramas
    from the segmentation maps via a GAN. Sat2Vid [[55](#bib.bib55)], the first work
    for cross-view video synthesis, also employs three stages to generate street-view
    ODVs using voxel grids with semantics and depth cues transformed from satellite
    images with trajectory. This is conceptually similar to that in [[54](#bib.bib54)].'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 地面视图，也称为街景图像，通常是全景的，以提供完整的周边信息，而卫星视图则是平面图像，捕捉了几乎世界的每个角落。从卫星视图图像合成地面视图图像存在一些方法。Lu
    等人[[54](#bib.bib54)] 提出了一个具有代表性的工作，包括三个阶段：卫星阶段、地理变换阶段和街景阶段。卫星阶段从卫星图像预测深度图和分割图。地理变换阶段将卫星阶段的输出转换为全景图。最后，街景阶段通过
    GAN 从分割图预测街景全景。Sat2Vid [[55](#bib.bib55)]，即第一个跨视角视频合成工作，也采用三个阶段来使用带有语义和深度提示的体素网格生成街景
    ODV，这与 [[54](#bib.bib54)] 中的概念类似。
- en: 'In general, the framework for geo-localization consists of two modules: cross-synthesis
    module and retrieval module. Shi et al. [[59](#bib.bib59)] proposed a representative
    contrastive learning pipeline to calculate the distance between the ground-view
    ODIs and satellite-view images in the embedding space, similar to [[58](#bib.bib58),
    [57](#bib.bib57)]. In particular, in [[58](#bib.bib58)], a ground-view ODI is
    synthesized from the polar transformation of the satellite view via a GAN, supervised
    by the corresponding ground-view ground truth. Meanwhile, an extra retrieval branch
    is applied to constrain the latent representations of two domains. Using conditional
    GANs, Regmi et al. [[57](#bib.bib57)] skillfully synthesized the satellite-view
    image from the ground-view ODI. To learn a robust satellite query representation,
    they fused the features from the satellite-view synthesis and ground-view ODI,
    and then matched the query feature with satellite-view features in the embedding
    space. As the latest work, TransGeo [[60](#bib.bib60)] is the first ViT-based
    framework to extract the position information from the satellite images and ground-view
    ODIs. With an attention mechanism, TransGeo removes uninformative patches in the
    satellite-view images and surpasses previous CNN-based methods.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，地理定位框架由两个模块组成：跨合成模块和检索模块。Shi 等人[[59](#bib.bib59)] 提出了一个具有代表性的对比学习管道，用于计算嵌入空间中地面视图
    ODI 和卫星视图图像之间的距离，类似于 [[58](#bib.bib58)、[57](#bib.bib57)]。特别是，在 [[58](#bib.bib58)]
    中，通过 GAN 从卫星视图的极坐标变换合成地面视图 ODI，受对应地面视图真实图像的监督。同时，额外的检索分支用于约束两个领域的潜在表示。Regmi 等人
    [[57](#bib.bib57)] 使用条件 GAN 巧妙地从地面视图 ODI 合成卫星视图图像。为了学习鲁棒的卫星查询表示，他们融合了卫星视图合成和地面视图
    ODI 的特征，然后在嵌入空间中匹配查询特征与卫星视图特征。作为最新的工作，TransGeo [[60](#bib.bib60)] 是第一个基于 ViT 的框架，用于从卫星图像和地面视图
    ODI 中提取位置信息。通过注意机制，TransGeo 去除了卫星视图图像中不信息丰富的区域，超越了之前基于 CNN 的方法。
- en: 'Discussion: Most cross-view synthesis and geo-localization methods assume that
    a reference image is precisely centered at the location of any query image. Nonetheless,
    in practice, the two views are usually not perfectly aligned in terms of orientation [[61](#bib.bib61)]
    and spatial location[[62](#bib.bib62)]. Therefore, how to apply cross-view synthesis
    and geo-localization methods under challenging conditions is a valuable research
    direction.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：大多数跨视角合成和地理定位方法假设参考图像精确地位于任何查询图像的位置。然而，在实践中，两个视图通常在方向 [[61](#bib.bib61)]
    和空间位置 [[62](#bib.bib62)] 上并不完全对齐。因此，如何在挑战性条件下应用跨视角合成和地理定位方法是一个有价值的研究方向。
- en: 3.1.3 Compression
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 压缩
- en: Compared with conventional perspective images, omnidirectional data records
    richer geometrical information with a higher resolution and wider FoV, making
    it more challenging to achieve effective compression. The early approaches for
    ODI compression directly utilize the existing perspective methods to compress
    the perspective projections of the ODIs. For instance, Simone et al. [[67](#bib.bib67)]
    proposed an adaptive quantization method to solve the frequency shift in the viewport
    image blocks when projecting the ODI to the ERP. By contrast, OmniJPEG [[68](#bib.bib68)]
    first estimates the region of interest in the ODI and then encodes the ODI based
    on the geometrical transformation of the region content with a novel format called
    OmniJPEG, which is an extension of JPEG format [[69](#bib.bib69)] and can be viewable
    on legacy JPEG decoders. Considering the ERP distortion, a graph-based coder is
    proposed by [[70](#bib.bib70)] to adapt the sphere surface. To make the coding
    progress computationally feasible, the graph partitioning algorithm based on rate
    distortion optimization [[71](#bib.bib71)] is introduced to achieve a trade-off
    between the distortion of reconstructed signals, the signal smoothness on each
    sub-graph, and the coding cost of partitioning description. As a representative
    CNN-based ODI compression work, OSLO [[72](#bib.bib72)] applies HEALPix [[73](#bib.bib73)]
    to define a convolution operation directly on the sphere and adapt the standard
    CNN techniques to the spherical domain. The proposed on-the-sphere representation
    outperforms the similar learnable compression models on the ERP.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统透视图像相比，全向数据记录了更丰富的几何信息，具有更高的分辨率和更广的视场，这使得有效压缩变得更加具有挑战性。早期的ODI压缩方法直接利用现有的透视方法来压缩ODI的透视投影。例如，Simone
    等人[[67](#bib.bib67)] 提出了自适应量化方法来解决将ODI投影到ERP时视口图像块中的频率偏移问题。相比之下，OmniJPEG [[68](#bib.bib68)]
    首先估计ODI中的感兴趣区域，然后基于该区域内容的几何变换进行编码，采用了一种称为OmniJPEG的新格式，该格式是JPEG格式的扩展[[69](#bib.bib69)]，并且可以在传统的JPEG解码器上查看。考虑到ERP失真，[[70](#bib.bib70)]
    提出了基于图的编码器来适应球面。为了使编码过程在计算上可行，引入了基于率失真优化的图划分算法[[71](#bib.bib71)]，以在重建信号的失真、每个子图上的信号平滑性和划分描述的编码成本之间实现权衡。作为代表性的基于CNN的ODI压缩工作，OSLO
    [[72](#bib.bib72)] 应用HEALPix [[73](#bib.bib73)] 直接在球面上定义卷积操作，并将标准CNN技术适应于球面域。所提出的球面表示在ERP上的表现优于类似的可学习压缩模型。
- en: For ODV compression, Li et al. [[74](#bib.bib74)] proposed a representative
    work aiming to optimize the ODV encoding progress. They analyzed the distortion
    impacts of restoring spherical domain signals from the different planar projection
    types and then applied the rate distortion optimization based on the distortion
    of signal in spherical domain. Similarly, Wang et al. [[75](#bib.bib75)] proposed
    a spherical coordinates transform-based motion model to address the distortion
    problem in projections. Another representative method  [[76](#bib.bib76)] maps
    the ODV to the rhombic dodecahedron (RD) map and directly applies the planar perspective
    videos encoding methods on the RD map. Specifically, the rate control-based algorithms
    are proposed to achieve better qualities and smaller bitrate errors for ODV compression [[77](#bib.bib77)], [[78](#bib.bib78)].
    Zhao et al. [[78](#bib.bib78)] utilized game theory to find optimal inter/intra-frame
    bitrate allocations while Li et al. [[77](#bib.bib77)] proposed a novel bit allocation
    algorithm for ERP with the coding tree unit (CTU) level. Similar to [[20](#bib.bib20)],
    the CTUs in the same row have the same weight to reduce the distortion influence.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ODV压缩，Li 等人[[74](#bib.bib74)] 提出了旨在优化ODV编码过程的代表性工作。他们分析了从不同平面投影类型恢复球面域信号的失真影响，然后应用了基于球面域信号失真的率失真优化方法。类似地，Wang
    等人[[75](#bib.bib75)] 提出了基于球面坐标变换的运动模型来解决投影中的失真问题。另一种代表性方法[[76](#bib.bib76)] 将ODV映射到菱形十二面体（RD）图上，并直接在RD图上应用平面透视视频编码方法。具体而言，提出了基于率控制的算法，以实现更好的质量和更小的比特率误差用于ODV压缩[[77](#bib.bib77)]，[[78](#bib.bib78)]。Zhao
    等人[[78](#bib.bib78)] 利用博弈论找到最优的帧间/帧内比特率分配，而Li 等人[[77](#bib.bib77)] 提出了用于ERP的新比特分配算法，基于编码树单元（CTU）级别。类似于[[20](#bib.bib20)]，同一行的CTU具有相同的权重，以减少失真影响。
- en: 'Potential and Challenges: Based on the aforementioned analysis, only a few
    DL-based methods exist in this research domain. Most works combine the traditional
    planar coding methods with geometric information in the spherical domain. There
    remain some challenges for DL-based ODI/ODV compression. DL-based image compression
    methods require the effective metrics as the constraint, e.g., peak signal-to-noise
    ratio (PSNR), and structural similarity (SSIM). However, due to spherical imaging,
    traditional metrics are weak to measure the qualities of ODI. Furthermore, the
    planar projections of the ODI are high memory and distorted, which increase the
    computation cost and compression difficulty. Future research might consider extending
    more effective metrics based on the spherical geometric information and restoring
    a high-quality compressed ODI from a partial input.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 潜力与挑战：基于上述分析，这一研究领域仅存在少数基于 DL 的方法。大多数工作将传统的平面编码方法与球面领域的几何信息结合在一起。基于 DL 的 ODI/ODV
    压缩仍面临一些挑战。基于 DL 的图像压缩方法需要有效的度量作为约束，例如峰值信噪比（PSNR）和结构相似性（SSIM）。然而，由于球面成像，传统度量在衡量
    ODI 质量方面较弱。此外，ODI 的平面投影具有高内存和失真，增加了计算成本和压缩难度。未来的研究可能考虑基于球面几何信息扩展更有效的度量，并从部分输入中恢复高质量的压缩
    ODI。
- en: 3.1.4 Lighting Estimation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 照明估计
- en: 'Insight: It aims to predict the high dynamic range (HDR) illumination from
    low dynamic range (LDR) ODIs.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：它旨在从低动态范围（LDR）ODI中预测高动态范围（HDR）照明。
- en: Illumination recovery is widely employed in many real-world tasks ranging from
    scene understanding, reconstruction to editing. Hold-Geoffroy et al. [[79](#bib.bib79)]
    proposed a representative framework for outdoor illumination estimation. They
    first trained a CNN model to predict the sky parameters from viewports of outdoor
    ODIs, e.g., sun position and atmospheric conditions. They then reconstructed illumination
    environment maps for the given test images according to the predicted illumination
    parameters. Similarly, in [[80](#bib.bib80)], a CNN model is leveraged to predict
    the location of lights in the viewports, and the CNN is fine-tuned to predict
    the light intensities, i.e., environment maps, from the ODIs. In [[81](#bib.bib81)],
    geometric and photometric parameters of indoor lighting are regressed from the
    viewports of ODI, and the intermediate latent vectors are used to reconstruct
    the environment maps. Another representative method, called EMLight [[82](#bib.bib82)],
    consists of a regression network and a neural projector. The regression network
    outputs the light parameters, and the neural projector converts the light parameters
    into the illumination map. In particular, the ground truths of the light parameters
    are decomposed by a Gaussian map generated from the illumination via a spherical
    Gaussian function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 照明恢复在许多实际任务中被广泛应用，包括场景理解、重建和编辑。Hold-Geoffroy 等人 [[79](#bib.bib79)] 提出了一个代表性的户外照明估计框架。他们首先训练了一个
    CNN 模型，以从户外 ODI 的视口中预测天空参数，例如太阳位置和大气条件。然后，他们根据预测的照明参数重建了给定测试图像的照明环境图。同样，在 [[80](#bib.bib80)]
    中，利用 CNN 模型预测视口中光源的位置，并对 CNN 进行微调，以从 ODI 中预测光强度，即环境图。在 [[81](#bib.bib81)] 中，从
    ODI 的视口回归出室内照明的几何和光度参数，并使用中间潜在向量重建环境图。另一个代表性方法称为 EMLight [[82](#bib.bib82)]，由回归网络和神经投影仪组成。回归网络输出光参数，神经投影仪将光参数转换为照明图。特别是，光参数的真实值通过从照明中生成的高斯图分解，这由球面高斯函数生成。
- en: 'Discussion and Potential: From the aforementioned analysis, previous works
    for lighting estimation on ODIs take a single viewport as the input. The reason
    might be that the viewports are distortion-less and low-cost with low resolution.
    However, they suffer from severe drop of spatial information. Hence, it could
    be beneficial to apply contrastive learning to learn the robust representations
    from the multiple viewports or components of the tangent images.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论与潜力：从上述分析可以看出，先前的照明估计工作在 ODI 上仅使用单个视口作为输入。原因可能是视口没有失真且成本低，分辨率较低。然而，它们严重缺乏空间信息。因此，将对比学习应用于从多个视口或切线图像组件中学习鲁棒表示可能是有益的。
- en: 3.1.5 ODI Super-Resolution (SR)
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 ODI 超分辨率（SR）
- en: Existing Head-Mounted Display (HMD) devices [[83](#bib.bib83)] require at least
    the ODI with 21600$\times$10800 pixels for immersive experience, which can not
    be directly captured by current camera systems [[84](#bib.bib84)]. One alternative
    way is to capture low resolution (LR) ODIs and super-resolve them into high resolution
    (HR) ODIs efficiently. LAU-Net [[85](#bib.bib85)], as the first work to consider
    the latitude difference for ODI SR, introduces a multi-level latitude adaptive
    network. It splits an ODI into different latitude bands and hierarchically upscales
    these bands with different adaptive factors, which are learned via a reinforcement
    learning scheme. Beyond considering SR on the ERP, Yoon et al. [[28](#bib.bib28)]
    proposed a representative work, SphereSR, to learn a unified continuous spherical
    local implicit image function and generate an arbitrary projection with arbitrary
    resolution according to the spherical coordinate queries. For ODV SR, SMFN [[86](#bib.bib86)]
    is the first DNN-based framework, including a single-frame and multi-frame joint
    network and a dual network. The single-frame and multi-frame joint network fuses
    the features from adjacent frames, and the dual network constrains the solution
    space to find a better answer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的头戴显示器（HMD）设备[[83](#bib.bib83)]需要至少21600$\times$10800像素的ODI以获得沉浸式体验，而当前的相机系统[[84](#bib.bib84)]无法直接捕捉到这么高分辨率的图像。一个替代方法是捕捉低分辨率（LR）ODI并高效地将其超分辨率处理为高分辨率（HR）ODI。LAU-Net[[85](#bib.bib85)]作为首个考虑ODI超分辨率的纬度差异的工作，介绍了一个多层次纬度自适应网络。它将ODI分割成不同的纬度带，并以不同的自适应因子分层放大这些带，这些因子通过强化学习方案进行学习。除了在ERP上考虑超分辨率，Yoon等人[[28](#bib.bib28)]提出了一项具有代表性的工作SphereSR，旨在学习一个统一的连续球面局部隐式图像函数，并根据球面坐标查询生成任意分辨率的投影。对于ODV超分辨率，SMFN[[86](#bib.bib86)]是第一个基于DNN的框架，包括单帧和多帧联合网络及双网络。单帧和多帧联合网络融合了来自相邻帧的特征，双网络则约束了解空间，以找到更好的答案。
- en: 3.1.6 Upright Adjustment
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 直立调整
- en: 'Insight: Upright adjustment aims to correct the misalignment of the orientations
    between the camera and scene to improve the visual quality of ODI and ODV while
    they are used with a narrow field-of-view (NFoV) display, such as the VR application.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Insight: 直立调整旨在纠正相机与场景之间方向的不对齐，以提高使用窄视场（NFoV）显示器（如VR应用）时ODI和ODV的视觉质量。'
- en: 'The standard approach of upright adjustment follows two steps: (i) estimating
    the position of the pole of the ODI; (ii) applying a rotation matrix to align
    the estimated north pole. The early representative work [[87](#bib.bib87)] estimates
    the camera rotation according to the geometric structures in the panoramas, e.g.,
    curving straight lines and vanishing points. However, these methods are limited
    to the Manhattan [[88](#bib.bib88)] or Atlanta world [[89](#bib.bib89)] assumption
    and rely on necessary prior knowledge of geometric structures. Recently, DL-based
    upright adjustment has been widely studied. Without any specific assumption on
    the scene structure, DeepUA [[90](#bib.bib90)] proposes a representative CNN-based
    framework to estimate the 2D rotations of multiple NFoV images sampled from the
    ODI and then estimate the 3D camera rotation through the geometric relationship
    between 3D and 2D rotations. By contrast, Deep360Up [[91](#bib.bib91)] directly
    takes ERP image as the input and synthesizes the upright version according to
    the estimated up-vector orientation. In particular, Jung et al. [[92](#bib.bib92)]
    proposed a two-stage pipeline for ODI upright adjustment. First, the feature map
    is extracted by a CNN model from the rotated ERP image. The feature map is then
    mapped into a spherical graph. Finally, a GCN is applied to estimate the 3D camera
    rotation, which is the location of the point on the spherical surface corresponding
    to the north pole.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的直立调整方法包括两个步骤：(i) 估计ODI的极点位置；(ii) 应用旋转矩阵对齐估计出的北极点。早期的代表性工作[[87](#bib.bib87)]根据全景图中的几何结构（例如，弯曲的直线和消失点）来估计相机旋转。然而，这些方法受限于曼哈顿[[88](#bib.bib88)]或亚特兰大世界[[89](#bib.bib89)]假设，并依赖于几何结构的必要先验知识。近年来，基于深度学习的直立调整得到了广泛研究。无需对场景结构做任何特定假设，DeepUA[[90](#bib.bib90)]提出了一个代表性的基于CNN的框架，来估计从ODI中采样的多个NFoV图像的二维旋转，然后通过三维和二维旋转之间的几何关系来估计三维相机旋转。相比之下，Deep360Up[[91](#bib.bib91)]直接将ERP图像作为输入，并根据估计的上向量方向合成直立版本。特别地，Jung等人[[92](#bib.bib92)]提出了一个两阶段的ODI直立调整管道。首先，通过CNN模型从旋转后的ERP图像中提取特征图。然后，将特征图映射到球面图中。最后，应用GCN来估计三维相机旋转，即对应北极点的球面上的点的位置。
- en: '![Refer to caption](img/cd1c3e0d3e352964770dd8bf94921631.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd1c3e0d3e352964770dd8bf94921631.png)'
- en: 'Figure 6: Representative networks for ODI-QA and ODV-QA. (a), (b) and (c) are
    originally shown in [[93](#bib.bib93)], [[2](#bib.bib2)] and [[94](#bib.bib94)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：ODI-QA和ODV-QA的代表性网络。(a)、(b)和(c)最初显示在[[93](#bib.bib93)]、[[2](#bib.bib2)]和[[94](#bib.bib94)]中。
- en: 3.1.7 Visual Quality Assessment
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 视觉质量评估
- en: Due to the ultra-high resolution and sphere representation of omnidirectional
    data, visual quality assessment (V-QA) is valuable for the optimization of exiting
    image/video processing algorithms. We next introduce some representative works
    on ODI-QA and ODV-QA, respectively.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于全景数据的超高分辨率和球面表示，视觉质量评估（V-QA）对于优化现有的图像/视频处理算法非常有价值。接下来，我们分别介绍一些关于ODI-QA和ODV-QA的代表性工作。
- en: 'For the ODI-QA, according to the availability of the reference images, it can
    be further classified into two categories: full-reference (FR) ODI-QA and no-reference
    (NR) ODI-QA. In exiting methods on FR ODI-QA, some works focus on extending the
    conventional FR image quality assessment metrics, e.g., PSNR and SSIM, to the
    omnidirectional domain, e.g., [[95](#bib.bib95)], [[96](#bib.bib96)]. These works
    introduce special geometric structures of the ODI and its projection representations
    to traditional quality assessment metrics and measure the objective quality more
    accurately. In addition, there are a few DL-based approaches for FR ODI-QA. As
    the representative work shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.6 Upright Adjustment
    ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")(a), Lim et al. [[93](#bib.bib93),
    [97](#bib.bib97)] proposed a novel adversarial learning framework, consisting
    of a quality score predictor and a human perception guider, to automatically assess
    the image quality following the human perception. NR ODI-QA, also called blind
    ODI-QA, predicts the ODI quality without expensive reference ODIs. Considering
    multi-viewport images in the ERP format, Xu et al. [[98](#bib.bib98)] applied
    a novel viewport-oriented GCN to process the distortion-less viewports in ERP
    images and aggregated these features to estimate the quality score via an image
    quality regressor. A similar strategy is applied in [[99](#bib.bib99), [100](#bib.bib100)].
    By contrast, [[2](#bib.bib2)] extracted the features from CP images and their
    corresponding eye movement (EM) and head movement (HM) hotspot maps and provided
    a good projection-based potential, that is extracting the features from the multiple
    projection formats and fusing the features to improve the performance on blind
    ODI-QA, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.6 Upright Adjustment ‣ 3.1
    Image/Video Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives")(b).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '对于ODI-QA，根据参考图像的可用性，它可以进一步分为两类：完整参考（FR）ODI-QA和无参考（NR）ODI-QA。在现有的FR ODI-QA方法中，一些工作专注于将传统的FR图像质量评估指标，如**PSNR**和**SSIM**，扩展到全向领域，例如[[95](#bib.bib95)]、[[96](#bib.bib96)]。这些工作将ODI及其投影表示的特殊几何结构引入传统的质量评估指标中，以更准确地测量客观质量。此外，还有一些基于深度学习的FR
    ODI-QA方法。如图[6](#S3.F6 "Figure 6 ‣ 3.1.6 Upright Adjustment ‣ 3.1 Image/Video Manipulation
    ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A
    Survey and New Perspectives")(a)所示，**Lim**等人[[93](#bib.bib93)、[97](#bib.bib97)]提出了一种新颖的对抗学习框架，包括一个质量评分预测器和一个人类感知引导器，以自动评估符合人类感知的图像质量。NR
    ODI-QA，也称为盲ODI-QA，预测ODI质量而无需昂贵的参考ODI。考虑到ERP格式中的多视点图像，**Xu**等人[[98](#bib.bib98)]应用了一种新颖的视点导向GCN来处理ERP图像中的无失真视点，并将这些特征聚合，通过图像质量回归器来估计质量评分。类似的策略应用于[[99](#bib.bib99)、[100](#bib.bib100)]。相比之下，[[2](#bib.bib2)]从CP图像及其对应的眼动（EM）和头动（HM）热点图中提取特征，并提供了良好的基于投影的潜力，即从多个投影格式中提取特征并融合这些特征，以提高盲ODI-QA的性能，如图[6](#S3.F6
    "Figure 6 ‣ 3.1.6 Upright Adjustment ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b)所示。'
- en: 'For the ODV-QA, Li et al. [[94](#bib.bib94)] proposed a representative viewport-based
    CNN approach, including a viewport proposal network and a viewport quality network,
    as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.6 Upright Adjustment ‣ 3.1 Image/Video
    Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). The viewport proposal network generates
    several potential viewports and their error maps, and viewport quality network
    rates the V-QA score for each proposed viewport. The final V-QA score is calculated
    by the weighted average of all viewport V-QA scores. [[101](#bib.bib101)] is another
    representative that considers the temporal changes of spatial distortions in ODVs
    and fuses a set of spatio-temporal objective quality metrics from multiple viewports
    to learn a subjective quality score. Similarly, Gao et al. [[102](#bib.bib102)]
    modeled the spatial-temporal distortions of ODVs and proposed a novel FR objective
    metric by integrating three existing ODI-QA objective metrics.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '对于ODV-QA，Li等人[[94](#bib.bib94)]提出了一种基于视口的代表性CNN方法，包括一个视口提案网络和一个视口质量网络，如图[6](#S3.F6
    "Figure 6 ‣ 3.1.6 Upright Adjustment ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)所示。视口提案网络生成若干潜在视口及其误差图，视口质量网络对每个提议的视口进行V-QA评分。最终的V-QA分数通过所有视口V-QA分数的加权平均来计算。[[101](#bib.bib101)]是另一种代表性方法，它考虑了ODV空间畸变的时间变化，并融合了来自多个视口的时空目标质量度量，以学习主观质量评分。类似地，Gao等人[[102](#bib.bib102)]对ODV的时空畸变进行了建模，并通过整合三种现有的ODI-QA目标度量，提出了一种新颖的FR目标度量。'
- en: '![Refer to caption](img/187e8cfc2ef2a246f9e8007fa511a80e.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/187e8cfc2ef2a246f9e8007fa511a80e.png)'
- en: 'Figure 7: An illustration of Sph IoU [[103](#bib.bib103)] and FoV-IoU [[104](#bib.bib104)].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Sph IoU[[103](#bib.bib103)]和FoV-IoU[[104](#bib.bib104)]的示意图。
- en: 3.2 Scene Understanding
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 场景理解
- en: 3.2.1 Object Detection
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 物体检测
- en: 'Compared with the perspective images, DL-based object detection on ODIs remains
    two main difficulties: (i) traditional convolutional kernels are weak to process
    the irregular planar grid structures in the ODI projections; (ii) the criterias
    adopted in conventional 2D object detection do not fit well to the spherical images.
    To address the first difficulty, distortion-aware structures are proposed, e.g.,
    multi-scale feature pyramid network in [[105](#bib.bib105)], multi-kernel layers
    in [[106](#bib.bib106)]. However, the detection flows of these two methods are
    similar to the methods for 2D domain, which take the whole ERP image as input
    and predict the regions of interest (ROIs) to obtain the final bounding boxes.
    Considering the wide FoV of ERP, Yang et al. [[107](#bib.bib107)] proposed a representative
    framework, which can leverage the conventional 2D images to train a panoramic
    detector. The detecting progress consists of three sub-steps: stereo-projection,
    YOLO detectors, and bounding box post processing. Especially, they generated four
    stereographic projections with a $180^{\circ}\times 180^{\circ}$ FoV from an ERP
    and the four result maps predicted by the YOLO detectors. Finally, the sub-window
    detected bounding boxes are re-projected to the ERP and re-aligned into the final
    distortion-less bounding boxes.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与透视图像相比，基于DL的ODI物体检测仍面临两个主要困难：（i）传统卷积核在处理ODI投影中的不规则平面网格结构时效果较差；（ii）传统2D物体检测采用的标准不适用于球面图像。为解决第一个困难，提出了畸变感知结构，例如[[105](#bib.bib105)]中的多尺度特征金字塔网络，[[106](#bib.bib106)]中的多核层。然而，这两种方法的检测流程类似于2D领域的方法，即将整个ERP图像作为输入，并预测兴趣区域（ROIs）以获得最终的边界框。考虑到ERP的宽广视场，Yang等人[[107](#bib.bib107)]提出了一个代表性框架，可以利用传统的2D图像来训练全景检测器。检测过程包括三个子步骤：立体投影、YOLO检测器和边界框后处理。特别是，他们从一个ERP生成了四个$180^{\circ}\times
    180^{\circ}$视场的立体投影，并由YOLO检测器预测了四个结果图。最后，子窗口检测到的边界框被重新投影到ERP上，并重新对齐成最终的无畸变边界框。
- en: 'To tackle the second difficulty, a novel kind of spherical bounding boxe (SphBB)
    and spherical Intersection over Union (SphIoU) for ODI object detection are introduced
    in [[103](#bib.bib103)], as shown in the first row of Fig. [7](#S3.F7 "Figure
    7 ‣ 3.1.7 Visual Quality Assessment ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives").
    SphBB is represented by the coordinates $\theta$, $\phi$ of the object centers
    and the unbiased FoVs $\alpha$, $\beta$ of the objective occupation. SphIoU is
    similar to planar IoU and calculated by the IoU between two SphBBs. Concretely,
    FoVBBs are moved to the equator that is undistorted. Similarly, Cao et al. [[104](#bib.bib104)]
    proposed a novel IoU calculation method without any extra movement, called FoV-IoU.
    As shown in the second row of Fig. [7](#S3.F7 "Figure 7 ‣ 3.1.7 Visual Quality
    Assessment ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep
    Learning for Omnidirectional Vision: A Survey and New Perspectives"), FoV-IoU
    better approximates the exact computation of IoU between two FoV-BBs compared
    with the SphIoU.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对第二个难点，介绍了一种新的球形边界框（SphBB）和球形交并比（SphIoU）用于全景图像物体检测，如图[7](#S3.F7 "Figure
    7 ‣ 3.1.7 Visual Quality Assessment ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")的第一行所示。SphBB通过物体中心的坐标
    $\theta$ 和 $\phi$ 以及目标占据的无偏视场 $\alpha$ 和 $\beta$ 来表示。SphIoU 类似于平面 IoU，通过两个 SphBB
    之间的 IoU 计算得出。具体而言，FoVBBs 被移动到无扭曲的赤道上。类似地，Cao 等人 [[104](#bib.bib104)] 提出了一个不需要额外移动的新
    IoU 计算方法，称为 FoV-IoU。如图[7](#S3.F7 "Figure 7 ‣ 3.1.7 Visual Quality Assessment ‣
    3.1 Image/Video Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives") 的第二行所示，FoV-IoU 比 SphIoU
    更好地近似了两个 FoV-BB 之间 IoU 的准确计算。'
- en: '![Refer to caption](img/9d98f8f1c2e42e6d31dbecb808c44a4b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9d98f8f1c2e42e6d31dbecb808c44a4b.png)'
- en: 'Figure 8: Representative methods for unsupervised ODI semantic segmentation.
    (a) Unsupervised learning with deformable CNNs [[108](#bib.bib108)]. (b) Unsupervised
    learning with domain adaptation [[109](#bib.bib109)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：无监督全景图像语义分割的代表性方法。 (a) 使用变形卷积神经网络的无监督学习 [[108](#bib.bib108)]。 (b) 使用领域适应的无监督学习
    [[109](#bib.bib109)]。
- en: 3.2.2 Semantic Segmentation
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 语义分割
- en: 'TABLE III: Semantic segmentation by some representative methods. “S”: supervised,
    ”U”: Unsupervised, “D”: domain adaptation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 一些代表性方法的语义分割。“S”：监督，“U”：无监督，“D”：领域适应。'
- en: '| Method | Publication | Input | Dataset | Deformable | Supervision | Highlight
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表期刊 | 输入 | 数据集 | 变形 | 监督 | 亮点 |'
- en: '| Tateno [[108](#bib.bib108)] | ECCV’2018 | ERP | Stanford2d3d | $\checkmark$
    | U | Distortion-aware convolution |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Tateno [[108](#bib.bib108)] | ECCV’2018 | ERP | Stanford2d3d | $\checkmark$
    | U | 扭曲感知卷积 |'
- en: '| Zhang [[45](#bib.bib45)] | ICCV’2019 | Tangent | Stanford2D3D/Omni-SYNTHIA
    | $\checkmark$ | S | Orientation-aware convolutions |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[45](#bib.bib45)] | ICCV’2019 | 切线 | Stanford2D3D/Omni-SYNTHIA | $\checkmark$
    | S | 面向方向的卷积 |'
- en: '| Lee [[27](#bib.bib27)] | CVPR’2019 | Tangent | SYNTHIA/Stanford2D3D | $\checkmark$
    | S | Icosahedral geodesic polyhedron |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Lee [[27](#bib.bib27)] | CVPR’2019 | 切线 | SYNTHIA/Stanford2D3D | $\checkmark$
    | S | 二十面体测地多面体 |'
- en: '| Viu [[110](#bib.bib110)] | ICRA’2020 | ERP | SUN360 | $\checkmark$ | S |
    Equirectangular convolutions |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Viu [[110](#bib.bib110)] | ICRA’2020 | ERP | SUN360 | $\checkmark$ | S |
    等距圆柱卷积 |'
- en: '| Yang [[111](#bib.bib111)] | CVPR’2021 | ERP | PASS/WildPASS | ✗ | U | Concurrent
    attention networks |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Yang [[111](#bib.bib111)] | CVPR’2021 | ERP | PASS/WildPASS | ✗ | U | 并发注意力网络
    |'
- en: '| Zhang [[112](#bib.bib112)] | CVPR’2022 | ERP | Stanford2D3D/DensePASS | $\checkmark$
    | U | Deformable MLP |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[112](#bib.bib112)] | CVPR’2022 | ERP | Stanford2D3D/DensePASS | $\checkmark$
    | U | 变形多层感知机 |'
- en: '| Zhang [[113](#bib.bib113)] | T-ITS’2022 | ERP | DensePASS/VISTAS | ✗ | D
    | Uncertainty-aware adaptation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[113](#bib.bib113)] | T-ITS’2022 | ERP | DensePASS/VISTAS | ✗ | D
    | 不确定性感知适应 |'
- en: DL-based omnidirectional semantic segmentation has been widely studied because
    ODI can encompass exhaustive information about the surrounding space. There are
    many practically remaining challenges, e.g., distortions in the planar projections,
    object deformations, computed complexity, and scarce labeled data. We next introduce
    some representative methods for ODI semantic segmentation via supervised learning
    and unsupervised learning.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的全景语义分割已被广泛研究，因为全景图像可以包含关于周围空间的全面信息。尽管如此，仍然存在许多实际挑战，例如平面投影的扭曲、物体变形、计算复杂度和稀少的标注数据。接下来，我们将介绍一些通过监督学习和无监督学习进行全景图像语义分割的代表性方法。
- en: Due to the lack of real-world datasets, Deng et al. [[114](#bib.bib114)] firstly
    generated ODIs from an existing dataset of urban traffic scenes and then designed
    an approach, called zoom augmentation, to transform the conventional images into
    fisheye images. Meanwhile, they proposed a CNN-based framework with a special
    pooling module to integrate the local and global context information and handle
    complex scenes in the ODIs. Considering that CNNs have inherently limited ability
    to handle the distortions in ODIs, Deng et al. [[115](#bib.bib115)] proposed a
    method, called Restricted Deformable Convolution, to model geometric transformations
    and learn a convolutional filter size from the input feature map. Zoom augmentation
    was also applied to [[115](#bib.bib115)] for enriching the train data. As the
    first framework to conduct semantic segmentation on the real-world outdoor ODIs,
    SemanticSO [[116](#bib.bib116)] builds a distortion-aware CNN model using the
    equirectangular convolutions [[117](#bib.bib117)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏现实世界的数据集，Deng等人 [[114](#bib.bib114)] 首先从现有的城市交通场景数据集中生成了ODI，然后设计了一种称为缩放增强的方法，将传统图像转换为鱼眼图像。同时，他们提出了一个基于CNN的框架，具有特殊的池化模块，用于集成局部和全局上下文信息，并处理ODI中的复杂场景。考虑到CNN在处理ODI中的畸变方面固有的局限性，Deng等人 [[115](#bib.bib115)]
    提出了一种称为受限可变形卷积的方法，用于建模几何变换，并从输入特征图中学习卷积滤波器的大小。缩放增强也被应用于 [[115](#bib.bib115)] 以丰富训练数据。作为第一个在真实世界户外ODI上进行语义分割的框架，SemanticSO [[116](#bib.bib116)]
    构建了一个利用等距卷积 [[117](#bib.bib117)] 的畸变感知CNN模型。
- en: Due to the time-consuming and expensive cost of ground truth annotations for
    ODIs, endeavours have been made to synthesize ODI datasets from the conventional
    images and utilize knowledge transfer to adopt models directly trained with the
    perspective images. PASS [[118](#bib.bib118)] is the first work to bypass fully
    dense panoramic annotations and aggregate features represented by conventional
    perspective images to fulfill the pixel-wise segmentation in panoramic imagery.
    Based on the PASS, DS-PASS [[119](#bib.bib119)] further re-uses the knowledge
    learned from perspective images and adapts the model learned from the 2D domain
    to panoramic domain. Meanwhile, in DS-PASS, the sensitivity to spatial details
    is enhanced by implementing attention-based lateral connections to perform segmentation
    accurately. To reduce the domain gap between the ODI and perspective image, Yang
    et al. [[111](#bib.bib111)] proposed a representative cross-domain transfer framework
    that designs an efficient concurrent attention network to capture the long-range
    dependencies in ODI imagery and integrates the unlabeled ODIs and labeled perspective
    images into training. A similar strategy was applied in [[120](#bib.bib120)], [[121](#bib.bib121)]
    and [[109](#bib.bib109)]. Particularly, in [[109](#bib.bib109)], a shared attention
    module is used to extract features from the 2D domain and panoramic domain, and
    two domain adaption modules are used to ”teach” the panoramic branch by the perspective
    branch. For unsupervised semantic segmentation, there also exist some works considering
    the geometric structure of ODI [[108](#bib.bib108)]. For instance, Zhang et al. [[45](#bib.bib45)]
    proposed an orientation-aware CNN framework based on the icosahedron mesh representation
    of ODI and introduced an efficient interpolation approach of the north-aligned
    kernel convolutions for features on the sphere.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于地面实况标注对ODI的时间和成本消耗巨大，人们已尝试从传统图像中合成ODI数据集，并利用知识迁移直接采用使用透视图像训练的模型。PASS [[118](#bib.bib118)]
    是首个绕过完全密集全景标注，聚合由传统透视图像表示的特征，以实现全景图像中的逐像素分割的工作。基于PASS，DS-PASS [[119](#bib.bib119)]
    进一步重用从透视图像中学到的知识，并将从2D领域学习到的模型适配到全景领域。同时，在DS-PASS中，通过实施基于注意力的侧向连接，增强了对空间细节的敏感性，以实现精确分割。为了减少ODI与透视图像之间的领域差距，Yang等人 [[111](#bib.bib111)]
    提出了一个代表性的跨领域迁移框架，设计了一个高效的并行注意力网络，以捕捉ODI图像中的长程依赖，并将未标记的ODI和标记的透视图像集成到训练中。类似的策略也应用于 [[120](#bib.bib120)]、[[121](#bib.bib121)]
    和 [[109](#bib.bib109)]。特别是在 [[109](#bib.bib109)] 中，使用了一个共享注意力模块从2D领域和全景领域提取特征，并使用两个领域适应模块通过透视分支“教”全景分支。对于无监督语义分割，也存在一些考虑ODI几何结构的工作 [[108](#bib.bib108)]。例如，Zhang等人 [[45](#bib.bib45)]
    提出了基于ODI二十面体网格表示的方向感知CNN框架，并引入了一种高效的北对齐核卷积插值方法，用于球面上的特征。
- en: 3.2.3 Monocular Depth Estimation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 单目深度估计
- en: 'TABLE IV: Monocular depth estimation by some representative methods. “S”: supervised,
    “D”: domain adaptation.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 一些代表性方法的单目深度估计。“S”：监督，“D”：领域适应。'
- en: '| Method | Publication | Supervision | Input types | Architecture | Loss functions
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表 | 监督类型 | 输入类型 | 架构 | 损失函数 |'
- en: '| Zioulis [[122](#bib.bib122)] | ECCV’18 | S | ERP | Rectangular filters |
    l2 loss+smooth loss |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Zioulis [[122](#bib.bib122)] | ECCV’18 | S | ERP | 矩形滤波器 | l2 损失+平滑损失 |'
- en: '| Pintore [[123](#bib.bib123)] | CVPR’21 | S | ERP | Slice-based representation
    and LSTM | BerHu loss [[124](#bib.bib124)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Pintore [[123](#bib.bib123)] | CVPR’21 | S | ERP | 基于切片的表示和 LSTM | BerHu
    损失 [[124](#bib.bib124)] |'
- en: '| Zhuang [[125](#bib.bib125)] | AAAI’22 | S | ERP | Dilated filters | BerHu
    loss |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Zhuang [[125](#bib.bib125)] | AAAI’22 | S | ERP | 扩张滤波器 | BerHu 损失 |'
- en: '| Wang [[19](#bib.bib19)] | CVPR’20 | S | ERP+CP | Two-branch network and bi-projection
    fusion | BerHu loss |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[19](#bib.bib19)] | CVPR’20 | S | ERP+CP | 双分支网络和双重投影融合 | BerHu 损失
    |'
- en: '| Rey-Area [[34](#bib.bib34)] | CVPR’22 | S | Tangent | Perspective network+Alignment+Blending
    | Energy function |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Rey-Area [[34](#bib.bib34)] | CVPR’22 | S | 切线 | 透视网络+对齐+混合 | 能量函数 |'
- en: '| Li [[25](#bib.bib25)] | CVPR’22 | S | Tangent | Geometric embedding+Transformer
    | BerHu loss |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Li [[25](#bib.bib25)] | CVPR’22 | S | 切线 | 几何嵌入+Transformer | BerHu 损失 |'
- en: '| Jin [[126](#bib.bib126)] | CVPR’20 | S | ERP | Structure information as prior
    and regularizer | l1 loss+cross entropy loss |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Jin [[126](#bib.bib126)] | CVPR’20 | S | ERP | 结构信息作为先验和正则化器 | l1 损失+交叉熵损失
    |'
- en: '| Wang [[127](#bib.bib127)] | ACCV’18 | Self-S | CP | Depth estimation+camera
    motion estimation | photometric + pose loss |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[127](#bib.bib127)] | ACCV’18 | Self-S | CP | 深度估计+相机运动估计 | 光度 + 姿态损失
    |'
- en: '| Zioulis [[18](#bib.bib18)] | 3DV’19 | Self-S | ERP | View synthesis in horizontal,
    vertical and trinocular ones | photometric +smooth loss |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Zioulis [[18](#bib.bib18)] | 3DV’19 | Self-S | ERP | 水平、垂直和三目视图合成 | 光度 +
    平滑损失 |'
- en: '| Yun [[128](#bib.bib128)] | AAAI’22 | S+Self-S | ERP | ViT+pose estimation
    | SSIM [[129](#bib.bib129)]+gradient +L1+photometric loss |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Yun [[128](#bib.bib128)] | AAAI’22 | S+Self-S | ERP | ViT+姿态估计 | SSIM [[129](#bib.bib129)]+梯度
    +L1+光度损失 |'
- en: '| Tateno [[108](#bib.bib108)] | ECCV’18 | D | ERP | Distortion-aware filters
    | BerHu loss |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Tateno [[108](#bib.bib108)] | ECCV’18 | D | ERP | 失真感知滤波器 | BerHu 损失 |'
- en: '![Refer to caption](img/54d598fb9daa4a2b97f4c78375c49c8f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/54d598fb9daa4a2b97f4c78375c49c8f.png)'
- en: 'Figure 9: Representative monocular depth estimation methods. (a) Dual-branch
    methods with ERP and CP as inputs. (b) Methods taking tangent images as input
    and re-project them into ERP images. (c) Methods utilizing extra geometric information
    as the prior and regularizer. (d) View synthesis methods with certain baselines.
    (e) Self-supervised multi-frame methods with the pose estimation.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：代表性单目深度估计方法。(a) 具有 ERP 和 CP 作为输入的双分支方法。(b) 以切线图像作为输入，并将其重新投影到 ERP 图像中的方法。(c)
    利用额外几何信息作为先验和正则化器的方法。(d) 具有特定基线的视图合成方法。(e) 带有姿态估计的自监督多帧方法。
- en: 'Thanks to the emergence of large-scale panoramic depth datasets, monocular
    depth estimation has evolved rapidly. As shown in Fig. [9](#S3.F9 "Figure 9 ‣
    3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"),
    there are several trends: (i) Tailored networks, e.g., distortion-aware convolution
    filters [[108](#bib.bib108)] and robust representations [[123](#bib.bib123)];
    (ii) Different projection types of ODIs [[19](#bib.bib19)], [[130](#bib.bib130)], [[25](#bib.bib25)],
    as depicted in Fig. [9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣
    3.2 Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(a), (b); (iii) Inherent geometric priors [[131](#bib.bib131)], [[126](#bib.bib126)],
    as shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2
    Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c); (iv) Multiple views [[18](#bib.bib18)]
    or pose estimation [[128](#bib.bib128)], as shown in Fig. [9](#S3.F9 "Figure 9
    ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    (d), (e), respectively.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '多亏了大规模全景深度数据集的出现，单目深度估计得到了快速发展。如图[9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular Depth
    Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives")所示，出现了几个趋势：（i）定制化网络，例如，畸变感知卷积滤波器[[108](#bib.bib108)]和鲁棒表示[[123](#bib.bib123)];（ii）不同的ODI投影类型[[19](#bib.bib19)]，[[130](#bib.bib130)]，[[25](#bib.bib25)]，如图[9](#S3.F9
    "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(a)，(b)所示；（iii）固有几何先验[[131](#bib.bib131)]，[[126](#bib.bib126)]，如图[9](#S3.F9
    "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(c)所示；（iv）多视角[[18](#bib.bib18)]或姿态估计[[128](#bib.bib128)]，如图[9](#S3.F9
    "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(d)，(e)所示。'
- en: 'Tailored networks: To reduce the influence of the stretch distortion, Zioulis
    et al. [[122](#bib.bib122)] proposed the first work by directly using the ODIs.
    It follows [[20](#bib.bib20)] to transfer regular square convolution filters into
    row-wise rectangles and vary filter sizes to address the distortions at the poles.
    Tateno et al. [[108](#bib.bib108)] proposed a deformable convolution filter that
    samples the pixel grids on the tangent planes according to unit sphere coordinates.
    Recently, Zhuang et al. [[125](#bib.bib125)] proposed a novel framework to combine
    different dilated convolutions and extend the receptive field in the ERP images.
    In comparison, Pintore et al. [[123](#bib.bib123)] proposed a framework, named
    SliceNet, with regular convolution filters to work on the ERP directly. SliceNet
    reduces the input tensor only along the vertical direction to collect a sequence
    of vertical slices and adopts an LSTM [[132](#bib.bib132)] network to recover
    the long- and short-term spatial relationships among slices.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 定制化网络：为了减少拉伸畸变的影响，Zioulis等人[[122](#bib.bib122)]首次提出了直接使用ODI的工作。该方法遵循[[20](#bib.bib20)]将常规的正方形卷积滤波器转换为行式矩形，并通过变化滤波器的大小来解决极点的畸变。Tateno等人[[108](#bib.bib108)]提出了一种可变形卷积滤波器，根据单位球坐标对切平面上的像素网格进行采样。最近，Zhuang等人[[125](#bib.bib125)]提出了一个新颖的框架，将不同的膨胀卷积结合起来，并在ERP图像中扩展感受野。相比之下，Pintore等人[[123](#bib.bib123)]提出了一个名为SliceNet的框架，使用常规卷积滤波器直接处理ERP。SliceNet仅沿垂直方向减少输入张量，以收集一系列垂直切片，并采用LSTM[[132](#bib.bib132)]网络来恢复切片之间的长短期空间关系。
- en: 'Different projection formats: There are some attempts to address the distortion
    in the ERP via other distortion-less projection formats, e.g., CP, tangent projection.
    As a representative work, BiFuse [[19](#bib.bib19)] introduces a two-branch pipeline,
    where one branch processes the ERP input and another branch extracts the features
    from CP, to simulate the peripheral and foveal vision of human, as shown in Fig. [9](#S3.F9
    "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    (a). Then, a fusion model is proposed to combine the semantic and geometric information
    of the two branches. Inspired by BiFuse, UniFuse [[133](#bib.bib133)] designs
    a more effective fusion module to combine the two kinds of features and unidirectionally
    feeds the CP features to the ERP features only at the decoding stage. To better
    extract the global context information, GLPanoDepth [[134](#bib.bib134)] converts
    ERP input into a set of CP images and then exploits a ViT model to learn the long-range
    dependencies. As the tangent projection produces less distortion than CP, 360MonoDepth [[34](#bib.bib34)]
    trains the SoTA depth estimation models in 2D domain [[135](#bib.bib135)] with
    tangent images and re-projects predicted tangent depth maps into the ERP with
    alignment and blending, as shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular
    Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(b).
    However, directly re-projecting the tangent images back to the ERP format will
    cause overlapping and discontinuity. Therefore, OmniFusion [[25](#bib.bib25)]
    (the SoTA method by far) introduces additional 3D geometric embeddings to mitigate
    the discrepancy in patch-wise features and aggregates patch-wise information with
    an attention-based transformer.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的投影格式：有一些尝试通过其他无失真投影格式来解决ERP中的失真问题，例如CP、切线投影。作为代表性工作，BiFuse [[19](#bib.bib19)]
    引入了一个双分支管道，其中一个分支处理ERP输入，另一个分支从CP中提取特征，以模拟人类的外周和中央视力，如图 [9](#S3.F9 "图 9 ‣ 3.2.3
    单目深度估计 ‣ 3.2 场景理解 ‣ 3 全向视觉任务 ‣ 全向视觉的深度学习：调查与新视角")（a）所示。然后，提出了一个融合模型来结合两个分支的语义和几何信息。受到BiFuse的启发，UniFuse [[133](#bib.bib133)]
    设计了一个更有效的融合模块，以结合这两种特征，并且在解码阶段单向地将CP特征馈送到ERP特征。为了更好地提取全局上下文信息，GLPanoDepth [[134](#bib.bib134)]
    将ERP输入转换为一组CP图像，然后利用ViT模型学习长期依赖关系。由于切线投影比CP产生的失真更少，360MonoDepth [[34](#bib.bib34)]
    在2D领域 [[135](#bib.bib135)] 训练了SoTA深度估计模型，使用切线图像并将预测的切线深度图重新投影到ERP中进行对齐和混合，如图 [9](#S3.F9
    "图 9 ‣ 3.2.3 单目深度估计 ‣ 3.2 场景理解 ‣ 3 全向视觉任务 ‣ 全向视觉的深度学习：调查与新视角")（b）所示。然而，直接将切线图像重新投影回ERP格式会导致重叠和不连续。因此，OmniFusion [[25](#bib.bib25)]（迄今为止的SoTA方法）引入了额外的3D几何嵌入，以缓解补丁级特征中的差异，并利用基于注意力的变换器聚合补丁级信息。
- en: 'Geometric Information Prior: Some methods add extra geometric information priors
    to improve the performance, e.g., edge-plane information, surface normal, boundaries,
    as shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2
    Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives")(c). Eder et al. [[131](#bib.bib131)] assumed
    that each scene is piecewise planar and the principal curvature of each planar
    region, which is the second derivative of depth, should be zero. Consequently,
    they proposed a plane-aware learning scheme that jointly predicts depth, surface
    normal, and boundaries. Similar to [[131](#bib.bib131)], Feng et al. [[136](#bib.bib136)]
    proposed a framework to refine depth estimation using the surface normal and uncertainty
    scores. For a pixel with higher uncertainty, its prediction is mainly aggregated
    from the neighboring pixels. Particularly, Jin et al. [[126](#bib.bib126)] demonstrated
    that the representations of geometric structure, e.g., corners, boundaries, and
    planes, can provide the regularization for depth estimation and benefit it as
    the prior information well.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 几何信息先验：一些方法添加额外的几何信息先验来提高性能，例如边缘平面信息、表面法线、边界，如图[9](#S3.F9 "图 9 ‣ 3.2.3 单目深度估计
    ‣ 3.2 场景理解 ‣ 3 全景视觉任务 ‣ 全景视觉深度学习：综述与新视角")(c)所示。Eder等人[[131](#bib.bib131)]假设每个场景是分段平面的，每个平面区域的主曲率，即深度的二阶导数，应为零。因此，他们提出了一种平面感知学习方案，该方案联合预测深度、表面法线和边界。类似于[[131](#bib.bib131)]，Feng等人[[136](#bib.bib136)]提出了一个框架，通过表面法线和不确定性评分来精细化深度估计。对于具有更高不确定性的像素，其预测主要来自邻近像素。特别地，Jin等人[[126](#bib.bib126)]证明了几何结构的表示，例如角点、边界和平面，可以为深度估计提供正则化，并作为先验信息有益。
- en: 'Multiple Views: As ODI depth annotations are expensive, some works leverage
    the multiple viewpoints to synthesize data and obtain competitive results. Zioulis
    et al. [[18](#bib.bib18)] explored the spherical view synthesis for self-supervised
    monocular depth estimation. As shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.2.3 Monocular
    Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional Vision Tasks ‣
    Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(d),
    in [[18](#bib.bib18)], after predicting the ERP format depth map, stereo viewpoints
    in vertical and horizontal baselines are synthesized by the depth-image-based
    rendering. Synthesized images are supervised by real images with the same viewpoints
    via photometric image reconstruction loss. To improve accuracy and stability simultaneously,
    Yun et al. [[128](#bib.bib128)] proposed a joint learning framework to estimate
    monocular depth via supervised learning and estimate poses via self-supervised
    learning from the adjacent frames of ODV, as shown in Fig. [9](#S3.F9 "Figure
    9 ‣ 3.2.3 Monocular Depth Estimation ‣ 3.2 Scene Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")(e).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角：由于ODI深度标注成本高昂，一些研究利用多视角合成数据以获得竞争性结果。Zioulis等人[[18](#bib.bib18)]探索了用于自监督单目深度估计的球面视角合成。如图[9](#S3.F9
    "图 9 ‣ 3.2.3 单目深度估计 ‣ 3.2 场景理解 ‣ 3 全景视觉任务 ‣ 全景视觉深度学习：综述与新视角")(d)所示，在[[18](#bib.bib18)]中，预测ERP格式深度图后，通过深度图像渲染合成了垂直和水平基线的立体视角。合成图像通过与同视角的真实图像进行光度图像重建损失来进行监督。为了同时提高准确性和稳定性，Yun等人[[128](#bib.bib128)]提出了一个联合学习框架，通过监督学习估计单目深度，并通过自监督学习从ODV的相邻帧中估计姿态，如图[9](#S3.F9
    "图 9 ‣ 3.2.3 单目深度估计 ‣ 3.2 场景理解 ‣ 3 全景视觉任务 ‣ 全景视觉深度学习：综述与新视角")(e)所示。
- en: 'Discussion: Based on the aforementioned analysis, most methods only consider
    indoor scenes due to two main reasons: (i) Some geometric priors are ineffective
    in the wild, e.g., the plane assumption; (ii) Outdoor scenes are more challenging
    due to the scale ambiguity in approximately infinite regions (e.g., sky), and
    objects in various shapes and sizes [[130](#bib.bib130)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：根据上述分析，大多数方法仅考虑室内场景，主要有两个原因：（i）一些几何先验在野外无效，例如平面假设；（ii）户外场景由于近似无限区域（例如天空）的尺度模糊和各种形状及大小的物体而更具挑战性[[130](#bib.bib130)]。
- en: It has been demonstrated that directly applying the DL-based methods for 2D
    optical flow estimation on ODI will obtain the unsatisfactory results [[137](#bib.bib137)].
    To this end, Xie et al. [[138](#bib.bib138)] introduced a small diagnostic dataset
    FlowCLEVR and evaluated the performance of three kinds of tailored convolution
    filters, namely the correlation, coordinate and deformable convolutions, for estimating
    the omnidirectional optical flow. The domain adaptation frameworks [[139](#bib.bib139),
    [140](#bib.bib140)] benefit from the development of optical flow estimation in
    the perspective domain. Similar to [[137](#bib.bib137)], OmniFlowNet [[139](#bib.bib139)]
    is built on FlowNet2 and the convolution operation is inspired by [[117](#bib.bib117)].
    Especially, as the extension of [[141](#bib.bib141)], LiteFlowNet360 [[140](#bib.bib140)]
    uses kernel transformation techniques to solve the inherent distortion problem
    caused by the sphere-to-plane projection. A representative pipeline is proposed
    by [[142](#bib.bib142)], consisting of a data augmentation method and a flow estimation
    module. The data augmentation method overcomes the distortions introduced by ERP,
    and the flow estimation module exploits the cyclicity of spherical boundaries
    to convert long-distance estimation into a relatively short-distance estimation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 已证明直接将基于深度学习（DL）的方法应用于二维光流估计会得到不令人满意的结果 [[137](#bib.bib137)]。为此，Xie 等人 [[138](#bib.bib138)]
    引入了一个小型诊断数据集 FlowCLEVR，并评估了三种量身定制的卷积滤波器的性能，即相关卷积、坐标卷积和可变形卷积，用于估计全景光流。领域适应框架 [[139](#bib.bib139),
    [140](#bib.bib140)] 受益于光流估计在视角域中的发展。类似于 [[137](#bib.bib137)]，OmniFlowNet [[139](#bib.bib139)]
    基于 FlowNet2 构建，其卷积操作灵感来自于 [[117](#bib.bib117)]。特别地，作为 [[141](#bib.bib141)] 的扩展，LiteFlowNet360 [[140](#bib.bib140)]
    使用核变换技术来解决由球面到平面投影引起的固有失真问题。一个具有代表性的流程由 [[142](#bib.bib142)] 提出，包括数据增强方法和光流估计模块。数据增强方法克服了ERP引入的失真，光流估计模块利用球面边界的周期性，将远程估计转换为相对短距离估计。
- en: 3.2.4 Video Summarization
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 视频摘要
- en: 'Insight: Video summarization aims to generate representative and complete synopsis
    by selecting the parts containing the most critical information of the ODV.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：视频摘要旨在通过选择包含最关键信息的部分，生成代表性和完整的摘要。
- en: 'Compared with the methods for 2D video summarization, only a few works have
    been proposed for ODV summarization. Pano2Vid [[143](#bib.bib143)] is the representative
    framework that contains two sub-steps: detecting candidate events of interest
    in the entire ODV frames and applying dynamic programming to link detected events.
    However, Pano2Vid requires observing the whole video and is less capable for video
    streaming applications. Deep360Pilot [[42](#bib.bib42)] is the first framework
    to design a human-like online agent for automatic ODV navigation of viewers. Deep360pilot
    consists of three steps: object detection to obtain the candidate objects of interest,
    training RNN to choose the important object, and capturing exciting moments in
    ODV. AutoCam [[144](#bib.bib144)] generates the normal NFoV videos from the ODVs
    following human behavior understanding. An similar strategy was applied by Yu
    et al. [[145](#bib.bib145)]. They built a deep ranking model for spatial summarization
    to select NFOV shots from each frame in the ODV and generated a spatio-temporal
    highlight video by extending the same model to the temporal domain. Moreover,
    Lee [[146](#bib.bib146)] proposed a novel deep ranking neural network model for
    summarizing ODV both spatially and temporally.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与二维视频摘要方法相比，目前针对全景视频（ODV）摘要的工作较少。Pano2Vid [[143](#bib.bib143)] 是一个具有代表性的框架，包含两个子步骤：在整个ODV帧中检测感兴趣的候选事件，并应用动态规划将检测到的事件连接起来。然而，Pano2Vid
    需要观察整个视频，并且在视频流应用中能力较弱。Deep360Pilot [[42](#bib.bib42)] 是第一个设计出类人在线代理以自动导航ODV的框架。Deep360Pilot
    包含三个步骤：物体检测以获取感兴趣的候选物体，训练RNN以选择重要物体，并在ODV中捕捉精彩瞬间。AutoCam [[144](#bib.bib144)]
    根据对人类行为的理解，从ODV中生成普通的NFoV视频。Yu 等人 [[145](#bib.bib145)] 采用了类似的策略。他们建立了一个深度排序模型，用于空间摘要，从每一帧中选择NFoV镜头，并通过将相同模型扩展到时间域生成时空亮点视频。此外，Lee [[146](#bib.bib146)]
    提出了一个新颖的深度排序神经网络模型，用于在空间和时间上总结ODV。
- en: 'Discussion: Based on the above analysis, only a few methods exist in this research
    domain. As a temporal-related task, applying the transformer mechanism to ODV
    summarization could be beneficial. In addition, previous works only considered
    the ERP format, which suffer from the most severe distortion problems. Therefore,
    it is better to consider the CP, tangent projection or sphere format as the input
    for ODV summarization.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：基于以上分析，目前在这一研究领域仅存在少数几种方法。作为一个时间相关任务，将变压器机制应用于ODV总结可能是有益的。此外，以前的工作仅考虑了ERP格式，这种格式存在最严重的失真问题。因此，考虑CP、切线投影或球面格式作为ODV总结的输入会更好。
- en: 3.3 3D Vision
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 3D 视觉
- en: 'TABLE V: Room Layout estimation overview on representative studies.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：代表性研究中的房间布局估计概述。
- en: '| Method | Publication | Architecture | Highlight | Projection | Task |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表 | 体系结构 | 亮点 | 投影 | 任务 |'
- en: '| Zhang [[147](#bib.bib147)] | ICCV’21 | Mask RCNN+ODN +LIEN+HorizonNet | Context
    relation modeling | ERP | Layout+ object +semantic labels |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 张[[147](#bib.bib147)] | ICCV’21 | Mask RCNN+ODN +LIEN+HorizonNet | 上下文关系建模
    | ERP | 布局+对象+语义标签 |'
- en: '| Yang [[148](#bib.bib148)] | CVPR’19 | Two ResNet on ceiling and floor | Projection
    feature fusion | ERP, ceiling | Layout |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 杨[[148](#bib.bib148)] | CVPR’19 | 两个 ResNet 用于天花板和地板 | 投影特征融合 | ERP，天花板 |
    布局 |'
- en: '| Zou [[149](#bib.bib149)] | CVPR’18 | CNN+3D layout regressor | Boundary+Corner
    map prediction | ERP | Layout |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Zou [[149](#bib.bib149)] | CVPR’18 | CNN+3D 布局回归器 | 边界+角点图预测 | ERP | 布局 |'
- en: '| Tran [[150](#bib.bib150)] | CVPR’21 | HorizonNet+EMA | Semi-supervised learning
    | ERP | Layout |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Tran [[150](#bib.bib150)] | CVPR’21 | HorizonNet+EMA | 半监督学习 | ERP | 布局 |'
- en: '| Pintore [[151](#bib.bib151)] | ECCV’20 | ResNet+RNN | Atlanta World indoor
    Model | ERP, ceiling | Layout |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Pintore [[151](#bib.bib151)] | ECCV’20 | ResNet+RNN | 亚特兰大世界室内模型 | ERP，天花板
    | 布局 |'
- en: '| Sun [[152](#bib.bib152)] | CVPR’19 | ResNet+RNN | 1D representation of layout
    | ERP | Layout |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 孙[[152](#bib.bib152)] | CVPR’19 | ResNet+RNN | 布局的一维表示 | ERP | 布局 |'
- en: '| Sun [[153](#bib.bib153)] | CVPR’21 | ResNet+ efficient height compression
    | Latent horizontal feature | ERP | Layout, depth +semantic labels |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 孙[[153](#bib.bib153)] | CVPR’21 | ResNet+高效高度压缩 | 潜在的水平特征 | ERP | 布局，深度+语义标签
    |'
- en: '| Wang [[154](#bib.bib154)] | CVPR’21 | HorizonNet$\&amp;$L2D transformation
    | Differentiable depth rendering | ERP | Layout |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 王[[154](#bib.bib154)] | CVPR’21 | HorizonNet$\&amp;$L2D 变换 | 可微分深度渲染 | ERP
    | 布局 |'
- en: 3.3.1 Room Layout estimation and Reconstruction
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 房间布局估计与重建
- en: 'Insight: Room Layout estimation and reconstruction consists of multiple sub-tasks
    such as layout estimation, 3D object detection and 3D object reconstruction. This
    comprehensive task aims to facilitate holistic scene understanding based on a
    single ODI.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：房间布局估计和重建包含多个子任务，如布局估计、3D对象检测和3D对象重建。这个综合任务旨在基于单一ODI促进整体场景理解。
- en: 'As the indoor panoramas can cover wider surrounding environment and capture
    more context cues than conventional perspective images, they are beneficial to
    scene understanding and widely applied into room layout estimation and reconstruction.
    Zou et al. [[13](#bib.bib13)] summarized that the general procedure of layout
    estimation and reconstruction contains three sub-steps: edge-based alignment,
    layout elements prediction, and 3D layout elements recovery, as shown in Fig. [10](#S3.F10
    "Figure 10 ‣ 3.3.1 Room Layout estimation and Reconstruction ‣ 3.3 3D Vision ‣
    3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives"). The representative work, proposed by Zhang et al. [[147](#bib.bib147)],
    conducts the first DL-based pipeline for holistic 3D scene understanding that
    recovers 3D room layout and detailed information, e.g., shape, pose, and location
    of objects from a single ODI. In [[147](#bib.bib147)], a context-based GNN is
    designed to predict the relationships across the objects and room layout and achieves
    the SoTA performance on both geometry accuracy of room layout and 3D object arrangement.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于室内全景图能够覆盖更广泛的周边环境并捕捉更多的上下文线索，相比传统透视图像，它们对场景理解是有益的，并且被广泛应用于房间布局估计和重建。Zou 等人[[13](#bib.bib13)]总结了布局估计和重建的一般过程包括三个子步骤：基于边缘的对齐、布局元素预测和3D布局元素恢复，如图
    [10](#S3.F10 "图 10 ‣ 3.3.1 房间布局估计与重建 ‣ 3.3 3D 视觉 ‣ 3 全向视觉任务 ‣ 全向视觉的深度学习：调查与新视角")
    所示。代表性工作由张等人[[147](#bib.bib147)]提出，进行首个基于深度学习的整体3D场景理解管道，该管道从单一的ODI中恢复3D房间布局和详细信息，例如形状、姿态和对象位置。在
    [[147](#bib.bib147)] 中，设计了一种基于上下文的GNN来预测对象和房间布局之间的关系，并在房间布局的几何精度和3D对象排列上取得了SoTA表现。
- en: For the alignment, this pre-possessing step provides indoor geometric information
    as the prior knowledge to ease the network training. Several SoTA approaches [[148](#bib.bib148),
    [149](#bib.bib149), [150](#bib.bib150)] follow the ”Manhattan world” assumption,
    in which all walls are aligned with a canonical coordinate system, and the floor
    plane direction is estimated by selecting long line segments and voting for the
    three mutually orthogonal vanishing directions. In contrast, AtlantaNet [[151](#bib.bib151)]
    predicts the 3D layout from less restrictive scenes that are not limited to ”Manhattan
    World” assumption. AtlantaNet follows ”Atlanta World” assumption and projects
    an gravity-aligned ODI into two horizontal planes to predict a 2D room footprint
    on the floor plan and a room height to recover the 3D layout.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对齐，这个预处理步骤提供了室内几何信息作为先验知识，以简化网络训练。一些最先进的方法[[148](#bib.bib148)、[149](#bib.bib149)、[150](#bib.bib150)]遵循“曼哈顿世界”假设，其中所有墙壁都与标准坐标系对齐，地面平面方向通过选择长直线段并投票确定三个相互正交的消失方向来估计。相反，AtlantaNet[[151](#bib.bib151)]预测的是不受“曼哈顿世界”假设限制的较少约束场景中的3D布局。AtlantaNet遵循“亚特兰大世界”假设，将重力对齐的ODI投影到两个水平面上，以预测地面平面上的2D房间足迹和房间高度，从而恢复3D布局。
- en: '![Refer to caption](img/d0cc5dd87272d938d298afa32b91a633.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d0cc5dd87272d938d298afa32b91a633.png)'
- en: 'Figure 10: The general room layout prediction architecture.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：一般房间布局预测架构。
- en: For the layout element prediction, the primary task is to estimate layout boundaries
    and corner positions. On the one hand, the related methods usually choose different
    projections of ODIs as the input. For instance, some methods [[149](#bib.bib149),
    [152](#bib.bib152), [153](#bib.bib153)] predict the layout only from ERP images.
    Besides the ERP, Yang et al. [[148](#bib.bib148)] added a perspective ceiling-view
    image, which is obtained from the ERP through an equirectangular-to-perspective
    (E2P) conversion, as an extra input. They then extracted the features from the
    two formats by a two-branch network and fused the two-modal features to predict
    the layout elements. The advantage of [[148](#bib.bib148)] is that it directly
    uses the multi-projection model to jointly predict a Manhattan-world floor plan
    instead of estimating the number of corners. On the other hand, recent methods
    varied in their ways of feature representation. For instance, HorizonNet [[152](#bib.bib152)]
    represents the room layout of the ODI as three 1D embedding vectors and recovers
    3D room layouts from 1D predictions with low computation cost. Differently, Wang
    et al. [[154](#bib.bib154)] converted the layout into ’horizon-depth’ through
    ray casting of a few points. This transformation maintains the simplicity of layout
    estimation and improves the generalization capacity to unseen room layouts.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于布局元素预测，主要任务是估计布局边界和角点位置。一方面，相关方法通常选择ODI的不同投影作为输入。例如，一些方法[[149](#bib.bib149)、[152](#bib.bib152)、[153](#bib.bib153)]仅从ERP图像中预测布局。除了ERP之外，Yang等人[[148](#bib.bib148)]还添加了一张通过等距矩形到透视（E2P）转换从ERP获取的透视天花板视图图像，作为额外输入。然后，他们通过一个双分支网络从这两种格式中提取特征，并融合这两种模态特征来预测布局元素。[[148](#bib.bib148)]的优点是它直接使用多投影模型联合预测曼哈顿世界地面平面，而不是估计角点数。另一方面，最近的方法在特征表示方式上有所不同。例如，HorizonNet[[152](#bib.bib152)]将ODI的房间布局表示为三个1D嵌入向量，并从1D预测中恢复3D房间布局，计算成本低。不同的是，Wang等人[[154](#bib.bib154)]通过对少量点进行光线投射，将布局转换为“视界深度”。这种转换保持了布局估计的简单性，并提高了对未见过的房间布局的泛化能力。
- en: For final recovery, the general strategy [[149](#bib.bib149), [148](#bib.bib148),
    [152](#bib.bib152)] is to reconstruct the layout by the optimization of mapping
    each pixel between walls and corners. In particular, it defines the weighted loss
    of probability maps of floors, ceilings, and corners. The major difficulty is
    the layout boundary occlusions when the camera position is not ideal for the entire
    display. To address this problem, HorizonNet [[152](#bib.bib152)] observes the
    occlusions by examining the orientation of the first Principal Component Analysis
    (PCA) component of adjacent walls and recovers occluded parts according to the
    long-term dependencies of global geometry.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最终恢复，一般策略[[149](#bib.bib149), [148](#bib.bib148), [152](#bib.bib152)] 是通过优化在墙壁和角落之间映射每个像素来重建布局。特别地，它定义了地板、天花板和角落的概率图的加权损失。主要困难在于当相机位置不理想时的布局边界遮挡。为了解决这个问题，HorizonNet
    [[152](#bib.bib152)] 通过检查相邻墙壁的第一主成分分析（PCA）分量的方向来观察遮挡，并根据全球几何形状的长期依赖关系恢复被遮挡的部分。
- en: 3.3.2 Stereo Matching
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 立体匹配
- en: 'Human binocular disparity depends on the difference between the projections
    on the retina, that is, a sphere projection rather than a planar projection. Therefore,
    stereo matching on the ODIs is more similar to the human vision system. In  [[155](#bib.bib155)],
    they discussed the influence of omnidirectional distortion on the CNN-based methods
    and compared the quality of disparity maps predicted from the perspective and
    omnidirectional stereo images. The experimental results show that stereo matching
    based on the ODIs is more advantageous for numerous applications, e.g., robotics,
    AR/VR, and several other applications. General stereo matching algorithms follow
    four steps: (i) matching cost computation, (ii) cost aggregation, (iii) disparity
    computation with optimization, and (iv) disparity refinement. As the first DNN-based
    omnidirectional stereo framework, SweepNet [[156](#bib.bib156)] proposes a wide-baseline
    stereo system to compute the matching cost map from a pair of images captured
    by cameras with ultra-wide FoV lenses and uses a global sphere sweep at the rig
    coordinate system to generate an omnidirectional depth map directly. By contrast,
    OmniMVS [[157](#bib.bib157)] takes four 220^∘ FoV fisheye views as the input to
    train an end-to-end DNN model and uses a 3D encoder-decoder block to regularize
    the cost volume. The method proposed in [[158](#bib.bib158)], as the extension
    of OmniMVS, provides a novel regularization of cost volume based on the uncertainty
    of prior guidance. Another representative work, 360SD-Net [[159](#bib.bib159)],
    is the first end-to-end trainable network for omnidirectional stereo depth estimation
    with the top-bottom ODI pairs as the input. It mitigates the distortion in the
    ERP images through an additional polar angle coordinate input and a learnable
    cost volume.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的双眼视差取决于视网膜上的投影差异，即球面投影而不是平面投影。因此，基于ODIs的立体匹配更接近人类视觉系统。在[[155](#bib.bib155)]中，他们讨论了全向畸变对基于CNN的方法的影响，并比较了从视角和全向立体图像中预测的视差图的质量。实验结果表明，基于ODIs的立体匹配在许多应用中更具优势，例如机器人技术、AR/VR及其他几个应用。一般的立体匹配算法包括四个步骤：（i）匹配成本计算，（ii）成本聚合，（iii）优化后的视差计算，以及（iv）视差细化。作为第一个基于DNN的全向立体框架，SweepNet
    [[156](#bib.bib156)] 提出了一个宽基线立体系统，从一对由超宽视场镜头捕获的图像中计算匹配成本图，并在设备坐标系统中使用全局球体扫描直接生成全向深度图。相比之下，OmniMVS
    [[157](#bib.bib157)] 以四个220^∘视场的鱼眼图像作为输入来训练一个端到端的DNN模型，并使用3D编码器-解码器块来规范化成本体积。[[158](#bib.bib158)]中提出的方法作为OmniMVS的扩展，基于先验指导的不确定性提供了一种新的成本体积规范化。另一个代表性工作360SD-Net
    [[159](#bib.bib159)] 是第一个用于全向立体深度估计的端到端可训练网络，以上下ODI对作为输入。它通过额外的极角坐标输入和可学习的成本体积来减轻ERP图像中的畸变。
- en: 3.3.3 SLAM
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 SLAM
- en: SLAM is an intricate system that adopts multiple cameras, e.g., monocular, stereo,
    or RGB-D, combined with sensors onboard a mobile agent to reconstruct the environment
    and estimate the agent pose in real-time. SLAM is often used in real-time navigation
    and reality augmentation, e.g., Google Earth. The stereo information, such as
    key points [[160](#bib.bib160)] and dense or semi-dense depth maps[[161](#bib.bib161)],
    is indispensable to build an accurate modern SLAM system. Specifically, compared
    with traditional monocular SLAM [[162](#bib.bib162)] or multi-view SLAM [[163](#bib.bib163)],
    the omnidirectional data can provide the richer texture and structure information
    due to a large FoV, and the omnidirectional SLAM avoids the influence of discontinued
    frames in the surrounding environment and enjoys the technical advantage of complete
    positioning and mapping. Caruso et al. [[164](#bib.bib164)] proposed a representative
    monocular SLAM method for omnidirectional cameras in which the direct image alignment
    and pixel-wise distance filtering are directly formulated. Zachary et al. [[165](#bib.bib165)]
    proposed a general framework that accepts multiple types of sensor data and is
    capable of iterative updates of camera pose and pixel-wise depth. DeepFactors [[166](#bib.bib166)]
    performs joint optimization of the pose and depth variables to detect the loop
    closure. As the omnidirectional data has rich geometry and texture information,
    further works may consider how to cultivate the full potential of DL and utilize
    these imaging advantages to construct a fast and accurate SLAM system.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM 是一个复杂的系统，采用多种摄像头，例如单目、立体或 RGB-D，结合移动体上的传感器，以实时重建环境并估计体的位置。SLAM 常用于实时导航和现实增强，例如
    Google Earth。立体信息，如关键点[[160](#bib.bib160)] 和密集或半密集深度图[[161](#bib.bib161)]，对于构建准确的现代
    SLAM 系统不可或缺。具体而言，与传统的单目 SLAM [[162](#bib.bib162)] 或多视角 SLAM [[163](#bib.bib163)]
    相比，全向数据由于大视场提供了更丰富的纹理和结构信息，全向 SLAM 避免了周围环境中断帧的影响，并享有完整定位和映射的技术优势。Caruso 等人 [[164](#bib.bib164)]
    提出了一个代表性的单目 SLAM 方法，用于全向摄像头，其中直接图像对齐和逐像素距离滤波被直接制定。Zachary 等人 [[165](#bib.bib165)]
    提出了一个接受多种类型传感器数据的通用框架，能够对相机位置和逐像素深度进行迭代更新。DeepFactors [[166](#bib.bib166)] 执行姿态和深度变量的联合优化，以检测回环闭合。由于全向数据具有丰富的几何和纹理信息，进一步的工作可能会考虑如何挖掘深度学习的全部潜力，并利用这些成像优势来构建一个快速而准确的
    SLAM 系统。
- en: 3.4 Human Behavior Understanding
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 人类行为理解
- en: 3.4.1 Saliency Prediction
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 显著性预测
- en: 'TABLE VI: Deep ODI and ODV saliency prediction by some representative methods.
    EM and HM mean eye and head movement.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 一些代表性方法的深度 ODI 和 ODV 显著性预测。EM 和 HM 意为眼动和头动。'
- en: '| Method | Input | Publication | EM | HM | Highlight | Contribution |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 输入 | 发表 | EM | HM | 亮点 | 贡献 |'
- en: '| Dai [[167](#bib.bib167)] | IMG | ICASSP’20 | $\checkmark$ | $\checkmark$
    | CP $\&amp;$ 2D CNN | Dilated convolution |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Dai [[167](#bib.bib167)] | IMG | ICASSP’20 | $\checkmark$ | $\checkmark$
    | CP $\&amp;$ 2D CNN | 膨胀卷积 |'
- en: '| Lv [[168](#bib.bib168)] | IMG | ACM MM’20 | $\checkmark$ | $\checkmark$ |
    Spherical images $\&amp;$ GCN | GCN with spherical interpolation |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Lv [[168](#bib.bib168)] | IMG | ACM MM’20 | $\checkmark$ | $\checkmark$ |
    球形图像 $\&amp;$ GCN | 带球形插值的 GCN |'
- en: '| Chao [[169](#bib.bib169)] | IMG | TMM’21 | $\checkmark$ | $\checkmark$ |
    Multi-viewports$\&amp;$ 2D CNN | Different FoV viewports |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Chao [[169](#bib.bib169)] | IMG | TMM’21 | $\checkmark$ | $\checkmark$ |
    多视角$\&amp;$ 2D CNN | 不同视场视口 |'
- en: '| Abdelaziz [[170](#bib.bib170)] | IMG | ICCV’21 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ 2D CNN $\&amp;$ self-attention mechanism | Contrastive learning
    to maximize the mutual information |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Abdelaziz [[170](#bib.bib170)] | IMG | ICCV’21 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ 2D CNN $\&amp;$ 自注意力机制 | 对比学习以最大化互信息 |'
- en: '| Xu [[171](#bib.bib171)] | IMG | TIP’21 | ✗ | $\checkmark$ | ERP $\&amp;$
    deep reinforcement learning | Generative adversarial imitation learning |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[171](#bib.bib171)] | IMG | TIP’21 | ✗ | $\checkmark$ | ERP $\&amp;$
    深度强化学习 | 生成对抗模仿学习 |'
- en: '| Nguyen [[172](#bib.bib172)] | VID | ACM MM’18 | ✗ | $\checkmark$ | ERP $\&amp;$
    2D CNN $\&amp;$ LSTM | Transfer learning |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen [[172](#bib.bib172)] | VID | ACM MM’18 | ✗ | $\checkmark$ | ERP $\&amp;$
    2D CNN $\&amp;$ LSTM | 迁移学习 |'
- en: '| Chen [[43](#bib.bib43)] | VID | CVPR’18 | ✗ | $\checkmark$ | CP $\&amp;$
    2D CNN $\&amp;$ convLSTM | Spatial-temporal network $\&amp;$ Cube Padding |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[43](#bib.bib43)] | VID | CVPR’18 | ✗ | $\checkmark$ | CP $\&amp;$
    2D CNN $\&amp;$ convLSTM | 时空网络 $\&amp;$ 立方体填充 |'
- en: '| Zhang [[173](#bib.bib173)] | VID | ECCV’18 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ spherical CNN | Spherical crown convolution kernel |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[173](#bib.bib173)] | VID | ECCV’18 | $\checkmark$ | $\checkmark$
    | ERP $\&amp;$ 球面CNN | 球面冠卷积核 |'
- en: '| Xu [[174](#bib.bib174)] | VID | TPAMI’19 | ✗ | $\checkmark$ | ERP $\&amp;$
    deep reinforcement learning | Deep reinforcement learning |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[174](#bib.bib174)] | VID | TPAMI’19 | ✗ | $\checkmark$ | ERP $\&amp;$
    深度强化学习 | 深度强化学习 |'
- en: '| Zhu [[175](#bib.bib175)] | VID | TCSVT’21 | $\checkmark$ | ✗ | Image patches
    $\&amp;$ GCN | Graph convolution and feature alignment |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[175](#bib.bib175)] | VID | TCSVT’21 | $\checkmark$ | ✗ | 图像块 $\&amp;$
    GCN | 图卷积和特征对齐 |'
- en: '| Qiao [[176](#bib.bib176)] | VID | TMM’21 | $\checkmark$ | ✗ | Multi-viewports$\&amp;$
    2D CNN $\&amp;$ convLSTM | Multi-Task Deep Neural Network |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Qiao [[176](#bib.bib176)] | VID | TMM’21 | $\checkmark$ | ✗ | 多视口 $\&amp;$
    2D CNN $\&amp;$ convLSTM | 多任务深度神经网络 |'
- en: 'Recently, there have been several research trends in ODI saliency prediction,
    building on DL progress: (i) From 2D traditional convolutions to 3D specific convolutions;
    (ii) From single feature to multiple features; (iii) From single ERP input to
    multi-type inputs; (iv) From normal CNN-based learning to novel learning strategies.
    In Table. [VI](#S3.T6 "TABLE VI ‣ 3.4.1 Saliency Prediction ‣ 3.4 Human Behavior
    Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"), numerous DL-based methods have been proposed
    for ODI saliency prediction. In the following, we introduce and analyze some representative
    networks, as shown in Fig. [11](#S3.F11 "Figure 11 ‣ 3.4.1 Saliency Prediction
    ‣ 3.4 Human Behavior Understanding ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，在ODI显著性预测方面出现了几种研究趋势，这些趋势建立在深度学习进展的基础上：（i）从2D传统卷积到3D特定卷积；（ii）从单一特征到多个特征；（iii）从单一ERP输入到多类型输入；（iv）从普通CNN基础学习到新型学习策略。在表[VI](#S3.T6
    "TABLE VI ‣ 3.4.1 Saliency Prediction ‣ 3.4 Human Behavior Understanding ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")中，提出了众多基于深度学习的方法用于ODI显著性预测。接下来，我们将介绍和分析一些代表性网络，如图[11](#S3.F11
    "Figure 11 ‣ 3.4.1 Saliency Prediction ‣ 3.4 Human Behavior Understanding ‣ 3
    Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey
    and New Perspectives")所示。'
- en: (i) To directly apply 2D deep saliency predictors on ODIs and reduce the unsatisfactory
    distortion in ODIs, many works [[177](#bib.bib177), [167](#bib.bib167)] convert
    ODIs into 2D projection format. As the first attempt of DNNs on ODI saliency prediction,
    SalNet360 [[177](#bib.bib177)] subdivides an ERP into a set of six CP patches
    as the input because CP avoids the heavy distortions near the poles like ERP.
    Then SalNet360 combines predicted saliency maps and per-pixel spherical coordinates
    of these patches to output a resulting saliency map in ERP format. Differently,
    a few works [[178](#bib.bib178), [168](#bib.bib168)] propose the ODI-aware convolution
    filters for saliency prediction, and learn the relationships between the features
    from a non-distorted space. The representative work, SalGCN [[168](#bib.bib168)],
    transfers the ERP image to a spherical graph signal representation, generates
    the spherical graph signal representation of the saliency map and finally reconstructs
    the ERP format saliency map through the spherical crown-based interpolation. SalGFCN [[179](#bib.bib179)]
    proposes a SoTA method that is composed of a residual U-Net architecture based
    on the dilated graph convolutions and attention mechanism in the bottleneck. (ii)
    The viewports are the rectangular windows on ERP with different narrow FoVs caused
    by observers’ head movement. Due to less distortions in viewports, some works [[180](#bib.bib180),
    [181](#bib.bib181)] choose a set of viewports on ERP as the input and extract
    the multiple independent features from these viewports. The final omnidirectional
    saliency map is generated by a set of viewport saliency maps and refined via an
    equator biased post-processing. Different from most prior multi-feature works
    extracting the low-level geometric features, Mazumdar et al. [[180](#bib.bib180)]
    introduced a 2D detector to find important objects first, and this kind of local
    information can improve the performance of the overall saliency map. Recently,
    Chao et al. [[169](#bib.bib169)] utilized three different FoVs in each viewport
    to extract rich salient features and better combined the local and global information.
    Furthermore, stretch weighted maps are applied in the loss function to avoid the
    disproportionate impact of stretching in the north and south poles of the ERP
    image.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 为了直接将 2D 深度显著性预测器应用于全景图（ODI）并减少全景图中的不理想失真，许多研究 [[177](#bib.bib177), [167](#bib.bib167)]
    将全景图转换为 2D 投影格式。作为 DNNs 在 ODI 显著性预测中的首次尝试，SalNet360 [[177](#bib.bib177)] 将等距投影（ERP）划分为一组六个
    CP 补丁作为输入，因为 CP 避免了像 ERP 一样在极点附近的严重失真。然后，SalNet360 结合预测的显著性图和这些补丁的每像素球面坐标，输出一个
    ERP 格式的结果显著性图。不同的是，一些研究 [[178](#bib.bib178), [168](#bib.bib168)] 提出了针对全景图的卷积滤波器进行显著性预测，并从非失真的空间中学习特征之间的关系。代表性工作
    SalGCN [[168](#bib.bib168)] 将 ERP 图像转化为球面图信号表示，生成显著性图的球面图信号表示，并通过基于球面冠的插值最终重建
    ERP 格式的显著性图。SalGFCN [[179](#bib.bib179)] 提出了一个最先进的（SoTA）方法，该方法由一个基于扩张图卷积和瓶颈中的注意机制的残差
    U-Net 架构组成。 (ii) 视口是 ERP 上的矩形窗口，由观察者头部运动引起不同的狭窄视场（FoV）。由于视口的失真较少，一些研究 [[180](#bib.bib180),
    [181](#bib.bib181)] 选择了一组视口作为输入，并从这些视口中提取多个独立的特征。最终的全景显著性图由一组视口显著性图生成，并通过赤道偏置的后处理进行优化。不同于大多数先前的多特征研究提取低级几何特征，Mazumdar
    等 [[180](#bib.bib180)] 引入了一个 2D 检测器首先寻找重要对象，这种局部信息可以提高整体显著性图的性能。最近，Chao 等 [[169](#bib.bib169)]
    在每个视口中利用了三种不同的视场来提取丰富的显著特征，并更好地结合了局部和全局信息。此外，在损失函数中应用了拉伸加权图，以避免 ERP 图像南北极拉伸带来的不成比例影响。
- en: '(iii) ODI saliency prediction methods with multi-type inputs focus on the projection
    transformations of the ODIs, which has been mentioned in the Sec [2.1](#S2.SS1
    "2.1 Omnidirectional Imaging ‣ 2 Background ‣ Deep Learning for Omnidirectional
    Vision: A Survey and New Perspectives"). These methods aim to utilize the properties
    of different projection formats to achieve the better performance than the single
    ERP input [[177](#bib.bib177), [182](#bib.bib182), [183](#bib.bib183)]. Due to
    the geometric distortions in the poles of ERP format, Djemai et al. [[182](#bib.bib182)]
    introduced a set of CP images, which are projected by five different rotational
    ERP images into the CNN-based approach. However, boundary-distortion and discontinuity
    in CP images cause the lack of global information in the extracted features. To
    address the problem, SalBiNet360 [[183](#bib.bib183)] simultaneously takes ERP
    and CP images as the input. It constructs a bifurcated network to predict global
    and local saliency maps, respectively. The final saliency output is the fusion
    of the global and local saliency maps. Furthermore, Zhu [[184](#bib.bib184)] provided
    a groundbreaking multi-domain model, which decomposes the ERP image using spherical
    harmonics in the frequency domain and combines frequency components with multiple
    viewports of the ERP images in the spatial domain to extract features.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '(iii) 多类型输入的ODI显著性预测方法专注于ODI的投影变换，这一点在第[2.1节](#S2.SS1 "2.1 Omnidirectional
    Imaging ‣ 2 Background ‣ Deep Learning for Omnidirectional Vision: A Survey and
    New Perspectives")中已有提及。这些方法旨在利用不同投影格式的特性，以实现比单一ERP输入更好的性能[[177](#bib.bib177),
    [182](#bib.bib182), [183](#bib.bib183)]。由于ERP格式极点的几何失真，Djemai等人[[182](#bib.bib182)]引入了一组CP图像，这些图像通过五种不同的旋转ERP图像投影到基于CNN的方法中。然而，CP图像中的边界失真和不连续性导致提取的特征缺乏全局信息。为了解决这个问题，SalBiNet360[[183](#bib.bib183)]同时将ERP和CP图像作为输入。它构建了一个分叉网络，分别预测全局和局部显著性图。最终的显著性输出是全局和局部显著性图的融合。此外，Zhu[[184](#bib.bib184)]提供了一个开创性的多域模型，该模型利用频域中的球谐函数对ERP图像进行分解，并在空间域中将频率组件与ERP图像的多个视口相结合以提取特征。'
- en: (iv) As the first to use GAN to predict the saliency maps for ODIs, SalGAN360 [[185](#bib.bib185)]
    provides a new generator loss, which is designed according to three evaluation
    metrics to fine-tune the SalGAN [[186](#bib.bib186)]. SalGAN360 constructs a different
    branch with the Multiple Cubic Projection (MCP) as input to simulate undistorted
    contents. For the attention-based learning on ODI saliency prediction, Zhu et
    al. proposed RANSP [[187](#bib.bib187)] and AAFFN [[188](#bib.bib188)]. Both methods
    contain the part-guided attention (PA) module, which is a normalized part confidence
    map that can highlight specific regions in the image. Moreover, an attention-aware
    module is introduced to refine the final saliency map. Especially, RANSP predicts
    the head fixations while AAFFN predicts the eye fixations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) 作为首个使用GAN预测ODI显著性图的方法，SalGAN360[[185](#bib.bib185)]提供了一种新的生成器损失，这种损失是根据三种评估指标设计的，以微调SalGAN[[186](#bib.bib186)]。SalGAN360构建了一个不同的分支，以多个立方体投影（MCP）作为输入，以模拟无失真的内容。对于基于注意力的ODI显著性预测，Zhu等人提出了RANSP[[187](#bib.bib187)]和AAFFN[[188](#bib.bib188)]。这两种方法都包含部分引导注意力（PA）模块，这是一种规范化的部分置信图，可以突出图像中的特定区域。此外，引入了一个注意力感知模块来细化最终的显著性图。特别地，RANSP预测头部注视，而AAFFN预测眼睛注视。
- en: ODV Saliency Prediction For the saliency prediction in ODVs, the key points
    are accurate saliency prediction for each frame and the temporal coherence of
    the viewing process. As videos with dynamic contents are widely used in real applications,
    deep ODV saliency prediction has received more attention in the community. Nguyen
    et al. [[172](#bib.bib172)] proposed a representative transfer learning framework
    that shifted a traditional saliency model to a novel saliency model, PanoSalNet,
    which is similar to [[189](#bib.bib189)] and [[177](#bib.bib177)]. By contrast,
    Cheng et al. [[43](#bib.bib43)] proposed a spatial-temporal network consisting
    of a static model and a ConvLSTM module. The static model is inspired by [[190](#bib.bib190)]
    and ConvLSTM [[132](#bib.bib132)] is used to aggregate temporal information. They
    also implemented the Cube Padding technique to connect the cube faces by propagating
    the shared information across the views. Similar to [[180](#bib.bib180)], a viewport
    saliency prediction model is proposed in [[176](#bib.bib176)] which first studies
    human attention to detect the desired viewports of the ODV and then predict the
    fixations based on the viewport content. Especially, the proposed Multi-Task Deep
    Neural Network (MT-DNN) model takes both the viewport content and location of
    the viewport as the input and its structure follows [[43](#bib.bib43)] which employs
    a CNN and a ConvLSTM to explore both spatial and temporal features. One more representative
    is proposed by [[173](#bib.bib173)], in which the convolution kernel is defined
    on a spherical crown and the convolution operation corresponds to the rotation
    of kernel on the sphere. Considering the common planar ERP format, Zhang et al. [[173](#bib.bib173)]
    re-sampled the kernel based on the position of the sampled patches on ERP. There
    also exist some works based on novel learning strategies. Xu et al. [[174](#bib.bib174)]
    developed the saliency prediction network of head movement (HM) based on deep
    reinforcement learning (DRL). The proposed DRL-based head movement prediction
    approach owns offline and online versions. In offline version, multiple DRL workflows
    determines potential HM positions at each panoramic frame and generate a heat
    map of the potential HM positions. In online version, the DRL model will estimate
    the next HM position of one subject according to the currently observed HM position.
    Zhu et al. [[175](#bib.bib175)] proposed a graph-based CNN model to estimate the
    fraction of the visual saliency via Markov Chains. The edge weights of the chains
    represent the characteristics of viewing behaviors, and the nodes are feature
    vectors from the spatial-temporal units.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ODV显著性预测 在ODV的显著性预测中，关键点是对每一帧的准确显著性预测以及观看过程的时间一致性。由于动态内容的视频在实际应用中广泛使用，深度ODV显著性预测在学术界受到了更多关注。Nguyen等人 [[172](#bib.bib172)]
    提出了一个代表性的迁移学习框架，将传统的显著性模型转变为一种新颖的显著性模型PanoSalNet，这与 [[189](#bib.bib189)] 和 [[177](#bib.bib177)]
    相似。相比之下，Cheng等人 [[43](#bib.bib43)] 提出了一个由静态模型和ConvLSTM模块组成的时空网络。静态模型的灵感来自于 [[190](#bib.bib190)]，ConvLSTM [[132](#bib.bib132)]
    用于聚合时间信息。他们还实现了Cube Padding技术，通过在视图间传播共享信息来连接立方体面。类似于 [[180](#bib.bib180)]，一个视口显著性预测模型在 [[176](#bib.bib176)]
    中被提出，该模型首先研究人类注意力以检测ODV的目标视口，然后基于视口内容预测注视点。特别地，所提出的多任务深度神经网络（MT-DNN）模型将视口内容和视口位置作为输入，其结构遵循 [[43](#bib.bib43)]，采用CNN和ConvLSTM来探索空间和时间特征。另一种具有代表性的模型由 [[173](#bib.bib173)]
    提出，其中卷积核在球面冠上定义，卷积操作对应于核在球面上的旋转。考虑到常见的平面ERP格式，Zhang等人 [[173](#bib.bib173)] 基于采样贴片在ERP上的位置重新采样了卷积核。还有一些基于新颖学习策略的工作。Xu等人 [[174](#bib.bib174)]
    开发了基于深度强化学习（DRL）的头部运动（HM）显著性预测网络。所提出的基于DRL的头部运动预测方法拥有离线和在线版本。在离线版本中，多个DRL工作流程确定每个全景帧中的潜在HM位置，并生成潜在HM位置的热图。在在线版本中，DRL模型将根据当前观察到的HM位置估计一个主体的下一个HM位置。Zhu等人 [[175](#bib.bib175)]
    提出了一个基于图的CNN模型，通过马尔可夫链估计视觉显著性的比例。链的边权代表观看行为的特征，节点是来自时空单元的特征向量。
- en: '![Refer to caption](img/5502b28c47b216a5f51cbe1452088eaf.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5502b28c47b216a5f51cbe1452088eaf.png)'
- en: 'Figure 11: Deep saliency prediction methods. (a) Directly using traditional
    2D models on planar projections of ODI. (b) Using specific ODI-aware CNNs to predict
    the omnidirectional saliency maps. (c) Aggregating the saliency maps predicted
    by several viewport images. (d) Combining the predicted saliency maps from different
    projection types. (e) Using attention mechanism.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：深度显著性预测方法。（a）在ODI的平面投影上直接使用传统的2D模型。（b）使用特定的ODI感知CNN预测全方向显著性图。（c）聚合由多个视口图像预测的显著性图。（d）结合来自不同投影类型的显著性图。（e）使用注意力机制。
- en: 3.4.2 Gaze Behavior
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 注视行为
- en: Gaze following, also called gaze estimation, is related to detecting what people
    in the scene look at and are absorbed in. As normal perspective images are NFoV
    captured, gaze targets are always out of the scene. ODI gaze following is proposed
    to solve this problem because ODIs have a great ability to capture the entire
    viewing surroundings. Previous 3D gaze following methods can directly detect the
    gaze target of a human subject in the sphere space but ignore scene information
    of ODIs, which performs gaze following not well. Gaze360 [[191](#bib.bib191)]
    collects a large-scale gaze dataset using fish-eye lens rectification to pre-process
    the images. However, due to the distortion caused by the sphere-to-plane projection,
    the gaze target maybe not be in the 2D sightline of the human subject in long-distance
    gaze, which is no longer the same in 2D images. Li et al. [[192](#bib.bib192)]
    proposed the first framework for ODI gaze following and also collected the first
    ODI gaze following dataset, called GazeFollow360\. They detected the gaze target
    within a local region and a distant region. For ODI gaze prediction, Xu et al. [[193](#bib.bib193)]
    built a large-scale eye-tracking dataset for dynamic 360^∘ immersive videos and
    gave a detailed analysis of gaze prediction. They utilized the temporal saliency,
    spatial saliency and history gaze path for gaze prediction with a combination
    of CNN and LSTM, which is similar to the architecture proposed by [[194](#bib.bib194)].
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注视跟随，也称为注视估计，涉及检测场景中人们注视的对象和专注的内容。由于正常透视图像是NFoV捕获的，注视目标总是位于场景之外。ODI注视跟随被提出以解决这个问题，因为ODI具有捕捉整个视野的强大能力。之前的3D注视跟随方法可以直接检测球体空间中人类主体的注视目标，但忽略了ODI的场景信息，导致注视跟随效果不佳。Gaze360
    [[191](#bib.bib191)]使用鱼眼镜头校正来预处理图像，收集了一个大规模的注视数据集。然而，由于球体到平面投影的失真，长距离注视中的注视目标可能不在人的二维视线中，这在二维图像中不再相同。Li等人[[192](#bib.bib192)]提出了第一个ODI注视跟随框架，并收集了第一个ODI注视跟随数据集，称为GazeFollow360\。他们在局部区域和远程区域检测注视目标。对于ODI注视预测，Xu等人[[193](#bib.bib193)]建立了一个用于动态360^∘沉浸式视频的大规模眼动追踪数据集，并对注视预测进行了详细分析。他们利用时间显著性、空间显著性和历史注视路径进行注视预测，结合了CNN和LSTM，这与[[194](#bib.bib194)]提出的架构类似。
- en: 'Challenges and potential: ODI contains richer context information that can
    boost gaze behaviour understanding. However, some challenges remain. First, there
    are few specific gaze following and gaze prediction datasets specific for ODI.
    Data is the ”engine” of DL-based methods, so collecting the quantitative and qualitative
    datasets is necessary. Second, due to the distortion problem in sphere-to-plane
    projection types, future research should consider how to correct this distortion
    via geometric transformation. Finally, both gaze following and gaze prediction
    in ODI need to understand wider scene information compared with normal 2D images.
    The spatial context relation should be further explored.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战和潜力：ODI包含更丰富的上下文信息，可以提升对注视行为的理解。然而，仍然存在一些挑战。首先，针对ODI的注视跟随和注视预测数据集很少。数据是基于DL的方法的“引擎”，因此收集定量和定性数据集是必要的。其次，由于球体到平面投影类型中的失真问题，未来的研究应考虑如何通过几何变换纠正这一失真。最后，相比于正常的二维图像，ODI中的注视跟随和注视预测需要理解更广泛的场景信息。空间上下文关系应进一步探讨。
- en: 3.4.3 Audio-Visual Scene Understanding
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 视听场景理解
- en: 'Because ODVs can provide the observers with an immersive understanding of the
    entire surrounding environments, recent research focuses on audio-visual scene
    understanding on ODVs. Due to its enabling viewers to experience sound in all
    directions, the spatial radio of ODV is an essential cue for full scene awareness.
    As the first work on the omnidirectional spatialization problem, Morgado et al. [[195](#bib.bib195)]
    designed a four-block architecture applying self-supervised learning to generate
    the spatial radio, given the mono audio and ODV as the joint inputs. They also
    proposed a representative self-supervised framework [[196](#bib.bib196)] for learning
    representations from the audio-visual spatial content of ODVs. In [[197](#bib.bib197)],
    ODIs combined with the multichannel audio signals are applied to localize sound
    source object within the visual observation. The self-supervised training method
    includes two DNN models: one for visual object detection and another for sound
    source estimation. Both DNN models are trained based on variational inference.
    Vasudevan et al. [[198](#bib.bib198)] simultaneously achieved an audio task, spatial
    sound super-resolution, and two visual tasks, dense depth prediction, and semantic
    labeling of the scene. They proposed a cross-modal distillation framework, including
    a shared encoder and three task-specific decoders, to transfer knowledge from
    vision to audio. For the audio-visual saliency prediction on ODVs, AVS360 [[199](#bib.bib199)]
    is the first end-to-end framework with two branches to understand audio and visual
    cues. Especially, AVS360 considers geometric distortion in ODV and extracts the
    spherical representation from the cube map images. Furthermore, as the first user
    behavior analysis for audio-visual content in ODV, Chao et al. [[200](#bib.bib200)]
    designed the comparative studies using ODVs with three different audio modalities
    and demonstrated that audio cues can improve the audio-visual attention in ODV.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因为ODVs可以让观察者对整个周围环境有身临其境的理解，最近的研究重点放在ODVs上的音频-视觉场景理解上。由于其能够使观众体验到来自各个方向的声音，ODV的空间声场是全面场景感知的关键线索。作为首个针对全方向空间化问题的研究，Morgado等人[[195](#bib.bib195)]设计了一种四块架构，应用自监督学习来生成空间声场，利用单声道音频和ODV作为联合输入。他们还提出了一个具有代表性的自监督框架[[196](#bib.bib196)]，用于从ODVs的音频-视觉空间内容中学习表征。在[[197](#bib.bib197)]中，ODIs与多通道音频信号结合用于定位视觉观察中的声音源对象。自监督训练方法包括两个DNN模型：一个用于视觉对象检测，另一个用于声音源估计。这两个DNN模型都基于变分推断进行训练。Vasudevan等人[[198](#bib.bib198)]同时完成了一个音频任务——空间声音超分辨率，以及两个视觉任务——密集深度预测和场景语义标注。他们提出了一个跨模态蒸馏框架，包括一个共享编码器和三个任务特定解码器，以将知识从视觉转移到音频。对于ODVs的音频-视觉显著性预测，AVS360[[199](#bib.bib199)]是第一个端到端框架，具有两个分支以理解音频和视觉线索。特别是，AVS360考虑了ODV中的几何失真，并从立方体映射图像中提取球面表示。此外，作为对ODV中音频-视觉内容的首个用户行为分析，Chao等人[[200](#bib.bib200)]设计了使用具有三种不同音频模式的ODVs的对比研究，并证明了音频线索可以改善ODV中的音频-视觉注意力。
- en: 'Discussion: Based on the above analysis, most works in this research domain
    process ERP images as normal 2D images and ignore the inherent distortions. Future
    research may explore how better combine spherical imaging characteristics and
    geometrical information of ODI with the spatial audio cues to provide a more realistic
    audio-visual experience.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：根据上述分析，该研究领域的大多数工作将ERP图像处理为普通的2D图像，并忽视了固有的失真。未来的研究可以探索如何更好地结合ODI的球面成像特性和几何信息与空间音频线索，以提供更真实的音频-视觉体验。
- en: 3.4.4 Visual Question Answering
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 视觉问答
- en: 'Visual question answering (VQA) is a comprehensive and interesting task that
    combines computer vision (CV), natural language processing (NLP), and knowledge
    representation $\&amp;$ reasoning (KR). Wider FoV ODIs and ODVs are more valuable
    and challenging for the VQA research because they can provide stereoscopic spatial
    information similar to the human visual system. VQA 360^∘, proposed in [[201](#bib.bib201)],
    is the first VQA framework on ODI. It introduces a CP-based model with multi-level
    fusion and attention diffusion to reduce spatial distortion. Meanwhile, the collected
    VQA 360^∘ dataset provides a benchmark for future developments. Furthermore, Yun
    et al. [[6](#bib.bib6)] proposed the first ODV-based VQA work, Pano-AVQA, which
    combines information from three modalities: language, audio, and ODV frames. The
    fused multi-modal representations extracted by a transformer network provide a
    holistic semantic understanding of omnidirectional surroundings. They also provided
    the first spatial and audio-VQA dataset on ODVs.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（VQA）是一个综合且有趣的任务，结合了计算机视觉（CV）、自然语言处理（NLP）和知识表示与推理（KR）。更宽的视场ODI和ODV对VQA研究更有价值和挑战，因为它们可以提供类似于人类视觉系统的立体空间信息。VQA
    360^∘，在[[201](#bib.bib201)]中提出，是第一个基于ODI的VQA框架。它引入了一个基于CP的模型，采用多层融合和注意力扩散来减少空间扭曲。同时，收集的VQA
    360^∘数据集为未来的发展提供了基准。此外，Yun等人[[6](#bib.bib6)]提出了第一个基于ODV的VQA工作，Pano-AVQA，它结合了来自三种模态的信息：语言、音频和ODV帧。由变压器网络提取的融合多模态表示提供了对全方位环境的整体语义理解。他们还提供了第一个关于ODV的空间和音频-VQA数据集。
- en: 'Discussion and Challenges: Based on the above analysis, there exist few works
    for the ODI$/$ODV-based VQA. Compared with the methods in 2D domain, the most
    considerable difficulty is how to leverage the spherical projection types, e.g.,
    icosahedron and tangent images. As more than two dozen datasets and numerous effective
    networks [[202](#bib.bib202)] in the 2D domain have been published, future research
    may consider how to effectively transfer knowledge to learn more robust DNN models
    for omnidirectional vision.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论与挑战：基于上述分析，针对ODI/ODV的VQA工作还较少。与2D领域的方法相比，最大的困难是如何利用球面投影类型，如二十面体和切线图像。由于在2D领域已经发布了二十多个数据集和众多有效网络[[202](#bib.bib202)]，未来的研究可能会考虑如何有效地迁移知识，以学习更强大的全向视觉DNN模型。
- en: 4 Novel Learning Strategies
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 新颖的学习策略
- en: Unsupervised/Semi-supervised Learning. ODI data scarcity problem occurs due
    to the insufficient yet costly panorama annotations. This problem is commonly
    addressed by semi-supervised learning or unsupervised learning that can take advantage
    of abundant unlabeled data to enhance the generalization capacity. For semi-supervised
    learning, Tran et al.[[150](#bib.bib150)] exploited the ‘Mean-Teacher’ model [[203](#bib.bib203)]
    for 3D room layout reconstruction by learning from the labeled and unlabeled data
    in the same scenario. For unsupervised learning, Djilali et al. [[170](#bib.bib170)]
    proposed the first framework for ODI saliency prediction. It calculates the mutual
    information between different views from multiple scenes and combines contrastive
    learning with unsupervised learning to learn latent representations. Furthermore,
    unsupervised learning can be combined with supervised learning to enhance the
    generalization capacity. Yun et al. [[128](#bib.bib128)] proposed to combine self-supervised
    learning with supervised learning for depth estimation, alleviating data scarcity
    and enhancing stability.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督/半监督学习。ODI数据稀缺的问题由于全景标注不足而又昂贵。这一问题通常通过半监督学习或无监督学习来解决，这些方法可以利用大量未标记的数据来增强泛化能力。对于半监督学习，Tran等人[[150](#bib.bib150)]利用‘Mean-Teacher’模型[[203](#bib.bib203)]通过在同一场景中从标记和未标记的数据中学习来进行3D房间布局重建。对于无监督学习，Djilali等人[[170](#bib.bib170)]提出了第一个ODI显著性预测框架。它计算来自多个场景的不同视图之间的互信息，并将对比学习与无监督学习相结合，以学习潜在表示。此外，无监督学习可以与监督学习结合，以增强泛化能力。Yun等人[[128](#bib.bib128)]提出将自监督学习与监督学习结合用于深度估计，缓解数据稀缺问题并增强稳定性。
- en: GAN. To decrease the domain divergence between perspective images and ODIs,
    P2PDA [[113](#bib.bib113)] and DENSEPASS [[109](#bib.bib109)] exploit the GAN
    frameworks and design an adversarial loss to facilitate semantic segmentation.
    In image generation, BIPS [[47](#bib.bib47)] proposes a GAN framework to synthesize
    RGB-D indoor panoramas based on the arbitrary configurations of cameras and depth
    sensors.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: GAN。为了减少视角图像和ODI之间的领域差异，P2PDA[[113](#bib.bib113)]和DENSEPASS[[109](#bib.bib109)]利用GAN框架，并设计了对抗损失来促进语义分割。在图像生成方面，BIPS[[47](#bib.bib47)]提出了一种GAN框架，以合成基于任意摄像机和深度传感器配置的RGB-D室内全景图像。
- en: Attention Mechanism. For cross-view geo-localization, in [[60](#bib.bib60)],
    ViT [[12](#bib.bib12)] is utilized to remove uninformative image patches and enhance
    the informative image patches to higher resolution. This attention-guided non-uniform
    cropping strategy can save the computational cost, which is reallocated to informative
    patches to improve the performance. The similar strategy is adopted in the unsupervised
    saliency prediction [[170](#bib.bib170)]. In [[170](#bib.bib170)], a self-attention
    model is employed to build spatial relationship between the two input and select
    the sufficiently invariant features.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制。在跨视角地理定位方面，[[60](#bib.bib60)]中使用ViT[[12](#bib.bib12)]来去除无信息的图像块，并将有信息的图像块提升到更高分辨率。这种注意力引导的非均匀裁剪策略可以节省计算成本，将其重新分配给有信息的图像块，从而提高性能。类似的策略在无监督显著性预测[[170](#bib.bib170)]中也被采用。在[[170](#bib.bib170)]中，采用自注意力模型来构建两个输入之间的空间关系，并选择足够不变的特征。
- en: Transfer Learning. There exist a lot of works to transfer the knowledge learned
    from the source 2D domain to facilitate learning in the ODI domain for numerous
    vision tasks, e.g., semantic segmentation [[115](#bib.bib115)] and depth estimation [[108](#bib.bib108)].
    Designing the deformable CNN or MLP on the pre-trained models from perspective
    images can enhance the model capability for ODIs in numerous tasks, e.g., semantic
    segmentation [[115](#bib.bib115), [108](#bib.bib108), [45](#bib.bib45), [27](#bib.bib27),
    [110](#bib.bib110), [112](#bib.bib112)], video super-resolution [[86](#bib.bib86)],
    depth estimation [[108](#bib.bib108)], and optical flow estimation [[138](#bib.bib138)].
    However, these methods heavily rely on the handcrafted modules, which lack the
    generalization capability for different scenarios. Unsupervised domain adaptation
    aims to transfer knowledge from the perspective domain to ODI domain by decreasing
    the domain gaps between the perspective images and ODIs. P2PDA [[113](#bib.bib113)]
    and BendingRD [[112](#bib.bib112)] decrease domain gaps between perspective images
    and ODIs to effectively obtain pseudo dense labels for the ODIs. Knowledge distillation
    (KD) is another effective technique that transfers knowledge from a cumbersome
    teacher model to learn a compact student model, while maintaining the student’s
    performance. However, we find that few works have applied KD for omnidirectional
    vision tasks. In semantic segmentation, ECANets [[111](#bib.bib111)] performs
    data distillation via diverse panoramas from all around the globe.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习。许多研究致力于将从源2D领域学到的知识转移到ODI领域，以促进在众多视觉任务中的学习，例如语义分割[[115](#bib.bib115)]和深度估计[[108](#bib.bib108)]。设计基于视角图像的可变形CNN或MLP可以增强模型在ODI领域中的能力，应用于多个任务，例如语义分割[[115](#bib.bib115),
    [108](#bib.bib108), [45](#bib.bib45), [27](#bib.bib27), [110](#bib.bib110), [112](#bib.bib112)]、视频超分辨率[[86](#bib.bib86)]、深度估计[[108](#bib.bib108)]和光流估计[[138](#bib.bib138)]。然而，这些方法严重依赖于手工设计的模块，缺乏对不同场景的泛化能力。无监督领域自适应旨在通过减少视角图像和ODI之间的领域差距，将知识从视角领域转移到ODI领域。P2PDA[[113](#bib.bib113)]和BendingRD[[112](#bib.bib112)]减少了视角图像和ODI之间的领域差距，从而有效地获得ODI的伪密集标签。知识蒸馏（KD）是另一种有效的技术，将知识从复杂的教师模型转移到紧凑的学生模型，同时保持学生的性能。然而，我们发现很少有研究将KD应用于全向视角任务。在语义分割中，ECANets[[111](#bib.bib111)]通过来自全球各地的多样化全景图像进行数据蒸馏。
- en: Deep Reinforcement Learning (DRL). In saliency prediction, [[171](#bib.bib171)]
    predicted the head fixation through DRL by interpreting the trajectories of head
    movements as discrete actions, which are rewarded by correct policies. Besides,
    in object detection, Pais et al. [[204](#bib.bib204)] provided the pedestrians’
    positions in the real world by considering the 3D bounding boxes and their corresponding
    distortion projections into the image. Another application for DRL is to select
    up-scaling factors adaptively based on the pixel density [[85](#bib.bib85)], which
    addresses the unevenly distributed pixel density in the ERP.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）。在显著性预测中，[[171](#bib.bib171)] 通过将头部运动轨迹解释为离散动作，并根据正确的策略给予奖励，预测了头部固定位置。此外，在目标检测中，Pais
    等人 [[204](#bib.bib204)] 通过考虑3D边界框及其在图像中的对应失真投影，提供了现实世界中的行人位置。DRL的另一个应用是根据像素密度自适应选择放大因子[[85](#bib.bib85)]，这解决了ERP中像素密度不均的问题。
- en: Multi-task Learning. Sharing representations between the related tasks can increase
    the generalization capacity of the models and improve the performance on all involved
    tasks. MT-DNN [[176](#bib.bib176)] combines the saliency detection task with the
    viewport detection task to predict the viewport saliency map of each frame and
    improves the saliency prediction performance in the ODVs. DeepPanoContext [[147](#bib.bib147)]
    empowers panoramic scene understanding by jointly predicting object shapes, 3D
    poses, semantic categories, and room layout. Similarly, HoHoNet [[153](#bib.bib153)]
    proposes a Latent Horizontal Feature (LHFeat) and a novel horizon-to-dense module
    to accomplish various tasks, including room layout reconstruction and per-pixel
    dense prediction tasks, e.g., depth estimation, semantic segmentation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习。通过在相关任务之间共享表示，可以提高模型的泛化能力，并改善所有相关任务的表现。MT-DNN [[176](#bib.bib176)] 将显著性检测任务与视口检测任务结合，以预测每帧的视口显著性图，并提高了ODVs中的显著性预测性能。DeepPanoContext [[147](#bib.bib147)]
    通过联合预测物体形状、3D姿态、语义类别和房间布局，增强了全景场景理解。类似地，HoHoNet [[153](#bib.bib153)] 提出了一个潜在水平特征（LHFeat）和一个新颖的地平线到密集模块，以完成各种任务，包括房间布局重建和每像素密集预测任务，如深度估计、语义分割。
- en: 5 Applications
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: AR and VR. With the advancement of techniques and the growing demand of interactive
    scenarios, AR and VR have seen rapid development in recent years. VR aims to simulate
    real or imaginary environments, where a participant can obtain immersive experiences
    and personalized content by perceiving and interacting with the environment. With
    the advantage of capturing the entire surrounding environment with $360^{\circ}\times
    180^{\circ}$ FoV in ODIs, 360 VR/AR facilitates the development of immersive experiences.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 增强现实（AR）与虚拟现实（VR）。随着技术的进步和互动场景需求的增长，AR 和 VR 近年来发展迅速。VR 旨在模拟真实或虚构的环境，参与者可以通过感知和互动获得沉浸式体验和个性化内容。利用在ODIs中捕捉整个环境的$360^{\circ}\times
    180^{\circ}$视场的优势，360 VR/AR 促进了沉浸式体验的发展。
- en: '[[205](#bib.bib205)] gives a detailed SWOT (namely strengths, weaknesses, opportunities,
    and threats) analysis of 360 VR to make sure that it is suitable to leverage the
    360 VR to develop athletes’ decision-making skills. Understanding human behaviors
    is crucial for the application of 360 VR. [[194](#bib.bib194)] proposed a preference-aware
    framework for viewport prediction, and [[193](#bib.bib193)] combined the history
    scan path with image contents for gaze prediction. In addition, to enhance the
    immersive experience, Kim et al. [[206](#bib.bib206)] proposed a novel pipeline
    to estimate room acoustic for plausible reproduction of spatial audio with $360^{\circ}$
    cameras. Importantly, acquiring 3D data is strongly desired in VR/AR to provide
    the sense of 3D. However, consumer-level depth sensors can only capture perspective
    depth maps, and panoramic depths need time-consuming stitching technologies. Therefore,
    monocular depth estimation techniques, e.g., OmniDepth [[122](#bib.bib122)] and
    UniFuse [[133](#bib.bib133)], are promising for VR/AR.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[[205](#bib.bib205)] 对 360 VR 进行了详细的 SWOT（即优势、劣势、机会和威胁）分析，以确保它适合用于提高运动员的决策能力。理解人类行为对
    360 VR 的应用至关重要。[[194](#bib.bib194)] 提出了一个考虑偏好的视口预测框架，而 [[193](#bib.bib193)] 将历史扫描路径与图像内容结合进行凝视预测。此外，为了增强沉浸体验，Kim
    等人 [[206](#bib.bib206)] 提出了一个新颖的管道，用于估计房间声学，以便通过 $360^{\circ}$ 摄像机逼真地再现空间音频。重要的是，在
    VR/AR 中强烈需要获取 3D 数据以提供 3D 立体感。然而，消费级深度传感器只能捕捉透视深度图，而全景深度图则需要耗时的拼接技术。因此，单目深度估计技术，如
    OmniDepth [[122](#bib.bib122)] 和 UniFuse [[133](#bib.bib133)]，在 VR/AR 中展现了很大的前景。'
- en: 'Robot Navigation. In addition to SLAM mentioned in Sec. [3.3.3](#S3.SS3.SSS3
    "3.3.3 SLAM ‣ 3.3 3D Vision ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for
    Omnidirectional Vision: A Survey and New Perspectives"), we further discuss the
    related applications of ODI/ODV in the field of robot navigation, including the
    telepresence system, surveillance, and DL-based optimization methods.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '机器人导航。除了第 [3.3.3](#S3.SS3.SSS3 "3.3.3 SLAM ‣ 3.3 3D Vision ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")
    节提到的 SLAM，我们进一步讨论了 ODI/ODV 在机器人导航领域的相关应用，包括远程呈现系统、监控和基于深度学习的优化方法。'
- en: The telepresence system aims to overcome the space constraints to enable people
    to remotely visit and interact with each other. ODI/ODV is gaining popularity
    by providing a more realistic and natural scene, especially in outdoor activities
    with open environments [[207](#bib.bib207)]. [[208](#bib.bib208)] proposed a prototype
    of an ODV-based telepresence system to support more natural interactions and the
    remote environment exploration, where real walking in the remote environment can
    simultaneously control the relevant movement of the robot platform. Surveillance
    aims to replace humans for security purposes, in which the calibration is vital
    for sensitive data. Accordingly, Pudics et al. [[209](#bib.bib209)] proposed a
    safe navigation system tailored for obstacle detection and avoidance with a calibration
    design to obtain the proper distance and direction. Compared with NFoV images,
    panoramic images can reduce the computational cost significantly by providing
    complete FoV in a single shot. Moreover, Ran et al. [[210](#bib.bib210)] proposed
    a lightweight framework based on the uncalibrated $360^{\circ}$ cameras. The framework
    can accurately estimate the heading direction by formulating it into a series
    of classification tasks and avoid redundant computation by saving the calibration
    and correction processes. To address dark environments, e.g., underground mine,
    Mansouri et al. [[211](#bib.bib211)] presented another DNN model by utilizing
    online heading rate commands to avoid the collision in the tunnels and calculating
    depth information online within the scene.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 远程呈现系统旨在克服空间限制，使人们能够远程访问和互动。ODI/ODV因提供更真实和自然的场景而越来越受欢迎，特别是在开放环境下的户外活动中[[207](#bib.bib207)]。[[208](#bib.bib208)]
    提出了一个基于ODV的远程呈现系统原型，以支持更自然的互动和远程环境探索，其中在远程环境中的真实步行可以同时控制机器人平台的相关运动。监控旨在替代人工以实现安全目的，其中校准对敏感数据至关重要。因此，Pudics等人[[209](#bib.bib209)]
    提出了一个安全导航系统，专门用于障碍物检测和规避，配备了校准设计以获得适当的距离和方向。与NFoV图像相比，全景图像通过在一次拍摄中提供完整的视野，可以显著降低计算成本。此外，Ran等人[[210](#bib.bib210)]
    提出了一个基于未校准的 $360^{\circ}$ 摄像头的轻量级框架。该框架通过将其公式化为一系列分类任务来准确估计航向方向，并通过节省校准和修正过程来避免冗余计算。为了应对黑暗环境，例如地下矿井，Mansouri等人[[211](#bib.bib211)]
    提出了另一个DNN模型，通过利用在线航向率命令来避免隧道中的碰撞，并在线计算场景中的深度信息。
- en: 'Autonomous Driving. It requires a full understanding of the surrounding environment,
    which omnidirectional vision excels at. Some works focus on setting up $360^{\circ}$
    platform for autonomous driving [[212](#bib.bib212), [213](#bib.bib213)]. Specifically,
    [[212](#bib.bib212)] utilized a stereo camera, a polarization camera and a panoramic
    camera to form a multi-modal visual system to capture omnidirectional landscape. [[213](#bib.bib213)]
    introduced a multi-modal 360^∘ perception proposal based on visual and LiDAR scanners
    for 3D object detection and tracking. In addition to the platform, the emergence
    of public omnidirectional datasets for autonomous driving are crucial for the
    application of DL methods. Caeser et al. [[214](#bib.bib214)] were the first to
    introduce the relevant dataset which carries six cameras, five radars and one
    LiDAR. All devices are with $360^{\circ}$ FoV. Recently, OpenMP dataset [[215](#bib.bib215)]
    is captured by six cameras and four LiDARs, which contains scenes in the complex
    environment, e.g., urban areas with overexposure or darkness. Kumar et al.[[216](#bib.bib216)]
    presented a multi-task visual perception network, which consists of six vital
    tasks in autonomous driving: depth estimation, visual odometry, senmantic segmentation,
    motion segmentation, object detection and lens soiling detection. Importantly,
    as real-time performance is crucial for autonomous driving and embedding systems
    in vehicles often have limited memory and computational resources, lightweight
    DNN models are more favored in practice.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶。它需要对周围环境有全面的了解，而全景视觉在这方面表现优越。一些研究集中在为自动驾驶设置 $360^{\circ}$ 平台[[212](#bib.bib212),
    [213](#bib.bib213)]。具体而言，[[212](#bib.bib212)] 使用了立体摄像头、偏振摄像头和全景摄像头来形成一个多模态视觉系统，以捕捉全景景观。[[213](#bib.bib213)]
    提出了基于视觉和激光雷达扫描仪的多模态 $360^{\circ}$ 感知方案，用于 3D 物体检测和跟踪。除了平台之外，公共全景数据集的出现对于自动驾驶的
    DL 方法应用至关重要。Caeser 等人[[214](#bib.bib214)] 首次引入了相关数据集，该数据集包含六个摄像头、五个雷达和一个激光雷达。所有设备均具有
    $360^{\circ}$ 的视场。最近，OpenMP 数据集[[215](#bib.bib215)] 由六个摄像头和四个激光雷达捕捉，包含复杂环境中的场景，例如过曝或黑暗的城市区域。Kumar
    等人[[216](#bib.bib216)] 提出了一个多任务视觉感知网络，包含自动驾驶中的六个重要任务：深度估计、视觉里程计、语义分割、运动分割、物体检测和镜头污染检测。重要的是，由于实时性能对自动驾驶至关重要，而车辆中的嵌入式系统通常具有有限的内存和计算资源，因此轻量级
    DNN 模型在实践中更受青睐。
- en: 6 Discussion and New Perspectives
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与新视角
- en: Cons of Projection Formats. ERP is the most prevalent projection format due
    to its wide FoV in a planar format. The main challenge for ERP is the increasing
    stretching distortion towards poles. Therefore, many works were proposed to design
    specific convolution filters against the distortion [[21](#bib.bib21), [20](#bib.bib20)].
    By contrast, CP and tangent images are distortion-less projection formats by projecting
    a spherical surface into multiple planes. They are similar to the perspective
    images, and therefore can make full use of many pre-trained models and datasets
    in the planar domain [[25](#bib.bib25)]. However, CP and tangent images suffer
    from the challenges of higher computational cost, discrepancy and discontinuity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 投影格式的缺点。ERP 是最普遍的投影格式，因为它在平面格式中具有广泛的视场。ERP 的主要挑战是向极点的拉伸畸变不断增加。因此，许多研究提出了针对畸变设计的特定卷积滤波器[[21](#bib.bib21),
    [20](#bib.bib20)]。相比之下，CP 和切线图像是无畸变的投影格式，通过将球面投影到多个平面上来实现。它们类似于透视图像，因此可以充分利用许多预训练模型和数据集[[25](#bib.bib25)]。然而，CP
    和切线图像面临更高的计算成本、差异性和不连续性的问题。
- en: 'We summarize two potential directions for utilizing CP and tangent images:
    (i) Redundant computational cost are resulted from large overlapping regions between
    projection planes. However, the pixel density varies among different sampling
    positions. The computation can be more efficient through allocating more resources
    for dense regions (e.g., equator) and less resources for sparse regions (e.g.,
    poles) with reinforcement learning [[85](#bib.bib85)]. (ii) Currently, different
    projection planes are often processed in parallel, which lacks the global consistency.
    To overcome the discrepancy among different local planes, it is effective to explore
    an additional branch with ERP as the input [[19](#bib.bib19)] or attention-based
    transformers to construct non-local dependencies [[25](#bib.bib25)]. However,
    these constraints are mainly added to the feature maps, instead of the predictions.
    Moreover, the discrepancy can be also solved from the distribution consistency
    of predictions, e.g., the consistent depth range among different planes and the
    consistent uncertainty scores for the same edges and large gradient regions.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了利用CP和切线图像的两个潜在方向：（i）投影平面之间的大面积重叠区域会导致冗余计算成本。然而，像素密度在不同采样位置之间变化。通过将更多资源分配给密集区域（例如赤道）和将较少资源分配给稀疏区域（例如极点），计算可以更加高效，参考[[85](#bib.bib85)]。（ii）目前，不同的投影平面通常是并行处理的，这缺乏全局一致性。为了解决不同局部平面之间的差异，探索以ERP作为输入的额外分支[[19](#bib.bib19)]或基于注意力的变换器来构建非局部依赖关系[[25](#bib.bib25)]是有效的。然而，这些约束主要添加到特征图上，而不是预测上。此外，也可以通过预测分布的一致性来解决这些差异，例如不同平面之间的一致深度范围以及相同边缘和大梯度区域的一致不确定性评分。
- en: Data-efficient Learning. A challenge for DL methods is the need for large-scale
    datasets with high-quality annotations. However, for omnidirectional vision, constructing
    large-scale datasets is expensive and tedious. Therefore, it is necessary to explore
    more data-efficient methods. One promising direction is to transfer the knowledge
    learned from models trained on the labeled 2D dataset to models to be trained
    on the unlabeled panoramic dataset. Specifically, domain adaptation approaches
    can be applied to narrow the gap between perspective images and ODIs [[109](#bib.bib109)].
    KD is also an effective solution by transferring learned feature information from
    a cumbersome perspective DNN model to a compact DNN model learning ODI data [[111](#bib.bib111)].
    Finally, recent self-supervised methods, e.g., [[217](#bib.bib217)], demonstrate
    the effectiveness of pre-training without the need of additional training annotations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 数据高效学习。DL方法的挑战之一是需要大规模高质量注释的数据集。然而，对于全景视觉，构建大规模数据集既昂贵又繁琐。因此，有必要探索更多的数据高效方法。一种有前景的方向是将从标记2D数据集上训练的模型中学到的知识转移到将要在未标记全景数据集上训练的模型中。具体来说，可以应用领域自适应方法来缩小透视图像和ODI之间的差距[[109](#bib.bib109)]。通过将从繁重的透视DNN模型中学习到的特征信息转移到学习ODI数据的紧凑DNN模型中，KD也是一种有效的解决方案[[111](#bib.bib111)]。最后，最近的自监督方法，例如[[217](#bib.bib217)]，展示了无需额外训练注释的预训练效果。
- en: Physical Constraint. Existing methods for the perspective images are limited
    in inferring the lighting of the global scene and unseen regions. Owing to the
    wide FoV of ODIs, complete surrounding environment scenes can be captured. Furthermore,
    the reflectance can be revealed according to the physical constraints between
    the lighting and scene structure based on [[218](#bib.bib218)]. Therefore, a future
    direction can be jointly leveraging computer graphics, like ray tracing, and rendering
    models to help calculate reflectance, which, in turn, contributes to higher-precision
    global lighting estimation. Additionally, it is promising to process and render
    ODIs based on the lighting transportation theory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 物理约束。现有的透视图像方法在推断全局场景和未见区域的光照方面存在局限。由于ODI的广阔视场，能够捕捉到完整的周围环境场景。此外，可以根据光照和场景结构之间的物理约束来揭示反射率，参考[[218](#bib.bib218)]。因此，未来的方向可以是共同利用计算机图形学，如光线追踪和渲染模型，来帮助计算反射率，从而有助于更高精度的全局光照估计。此外，基于光照传输理论处理和渲染ODI也很有前景。
- en: Multi-modal Omnidirectional Vision. It refers to the process of learning representations
    from different types of modalities (e.g., text-image for visual question answering,
    audio-visual scene understanding) using the same DNN model. This is a promising
    yet practical direction for ominidirectional vision. For instance, [[213](#bib.bib213)]
    introduces a multi-modal perception framework based on the visual and LiDAR information
    for 3D object detection and tracking. However, existing works in this direction
    treat ODIs as the perspective images and ignore the inherent distortion in the
    ODIs. Future works may explore how to utilize the advantage of ODIs, e.g., complete
    FoV, to assist the representation of other modalities. Importantly, the acquisition
    of different modalities has obvious discrepancies. For example, capturing RGB
    images is much easier than that of depth maps. Therefore, a promising direction
    is to extract available information from one modality and then transfer to another
    modality via multi-task learning, KD, etc. However, the discrepancy among different
    modalities should be considered to ensure multi-modal consistency.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态全向视觉。这指的是使用相同的DNN模型从不同类型的模态（例如，文本-图像用于视觉问答，音频-视觉场景理解）中学习表示。这是全向视觉的一个有前景但实际的方向。例如，[[213](#bib.bib213)]
    介绍了一个基于视觉和LiDAR信息的多模态感知框架，用于3D物体检测和跟踪。然而，现有的研究将ODI视为透视图像，并忽略了ODI中的固有失真。未来的工作可以探索如何利用ODI的优势，例如，完整的视场（FoV），来辅助其他模态的表示。重要的是，不同模态的获取存在明显差异。例如，捕捉RGB图像要比捕捉深度图更容易。因此，一个有前景的方向是从一种模态中提取可用信息，然后通过多任务学习、知识蒸馏等方式转移到另一种模态。然而，应考虑不同模态之间的差异，以确保多模态的一致性。
- en: Potential for Adversarial Attacks. There exist few studies focusing on adversarial
    attacks towards omnidirectional vision models. Zhang et al. [[219](#bib.bib219)]
    proposed the first and representative attack approach to fool DNN models by perturbing
    only one tangent image rendered from the ODI. The proposed attack is sparse as
    it disturbs only a small part of the input ODI. Therefore, they further proposed
    a position searching method to search for the tangent point on the spherical surface.
    There are numerous promising yet challenging research problems in this direction,
    e.g., analyzing the generalization capacity of attacks among different DNN models
    for ODIs, white-box attacks for network architectures and training methods, and
    defenses against attacks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在的对抗攻击风险。针对全向视觉模型的对抗攻击研究较少。张等人[[219](#bib.bib219)] 提出了第一种具有代表性的攻击方法，通过仅扰动从ODI渲染出的一个切线图像来欺骗DNN模型。该攻击方法具有稀疏性，因为它仅扰动输入ODI的一小部分。因此，他们进一步提出了一种位置搜索方法来寻找球面上的切线点。在这一方向上还有许多有前景但具有挑战性的研究问题，例如，分析不同DNN模型对ODI的攻击的泛化能力，网络架构和训练方法的白盒攻击，以及对抗攻击的防御。
- en: Potential for Metaverse. Metaverse aims to create a virtual world containing
    large-scale high-fidelity digital models, where users can freely create contents
    and obtain immersive interactive experience. Metaverse is facilitated by the AR
    and VR headsets, in which ODIs are favored due to the complete FoV. Therefore,
    a potential direction is to generate high-fidelity 2D/3D models from ODIs and
    simulate the real-world objects and scenes in great details. In addition, to help
    users obtain immersive experience, techniques that analyze and understand human
    behavior (e.g., gaze following, saliency prediction) can be further explored and
    integrated in the future.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 元宇宙的潜力。元宇宙旨在创建一个包含大规模高保真数字模型的虚拟世界，用户可以自由创建内容并获得沉浸式互动体验。元宇宙得益于AR和VR头戴设备，其中ODI因其完整的视场（FoV）而受到青睐。因此，一个潜在的方向是从ODI生成高保真的2D/3D模型，并详细模拟现实世界的物体和场景。此外，为了帮助用户获得沉浸式体验，可以进一步探索和集成分析和理解人类行为的技术（例如，注视跟踪、显著性预测）。
- en: 'Potential for Smart City. Smart city focuses on collecting data from the city
    with various devices and utilizing information from the data to improve efficiency,
    security and convenience, etc. Taking advantage of the characteristics of ODI
    in street-view images can facilitate the development of urban forms comparison.
    As mentioned in Sec. [3.1.2](#S3.SS1.SSS2 "3.1.2 Cross-view Synthesis and Geo-localization
    ‣ 3.1 Image/Video Manipulation ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning
    for Omnidirectional Vision: A Survey and New Perspectives"), a promising direction
    is to convert street-view images into satellite-view images for urban planning.
    Except for room layout discussed in Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Room Layout
    estimation and Reconstruction ‣ 3.3 3D Vision ‣ 3 Omnidirectional Vision Tasks
    ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives"), ODIs
    can also be applied in more interior designs. To achieve floorplan design, Wang
    et al. [[220](#bib.bib220)] leveraged human-activity maps and editable furniture
    placements to improve the interaction with users. However, the input of [[220](#bib.bib220)]
    is the boundary of the exterior wall, resulting in limitation of the visualization
    and manipulation. Future works might consider operating directly on the ODIs to
    make the interior design observable in all directions, boosting the development
    of interaction and making professional service accessible.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '智慧城市的潜力。智慧城市专注于通过各种设备收集城市数据，并利用这些数据提高效率、安全性和便利性等。利用全景图像中的ODI特性可以促进城市形式比较的发展。如在第[3.1.2](#S3.SS1.SSS2
    "3.1.2 Cross-view Synthesis and Geo-localization ‣ 3.1 Image/Video Manipulation
    ‣ 3 Omnidirectional Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A
    Survey and New Perspectives")节中提到的，一个有前途的方向是将街景图像转换为卫星图像以进行城市规划。除了在第[3.3.1](#S3.SS3.SSS1
    "3.3.1 Room Layout estimation and Reconstruction ‣ 3.3 3D Vision ‣ 3 Omnidirectional
    Vision Tasks ‣ Deep Learning for Omnidirectional Vision: A Survey and New Perspectives")节中讨论的房间布局，ODI还可以应用于更多的室内设计。为了实现平面图设计，Wang等人[[220](#bib.bib220)]利用人类活动图和可编辑家具布局来改善用户互动。然而，[[220](#bib.bib220)]的输入是外墙的边界，限制了可视化和操作。未来的工作可以考虑直接操作ODI，使室内设计在所有方向上都可见，从而提升互动发展，并使专业服务更加可及。'
- en: 7 Conclusion
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this survey, we comprehensively reviewed and analyzed the recent progress
    of DL methods for omnidirectional vision. We first introduced the principle of
    omnidirectional imaging, convolution methods and datasets. We then provided a
    hierarchical and structural taxonomy of the DL methods. For each task in the taxonomy,
    we summarized the current research status and pointed out the opportunities and
    challenges. We further provided a review of the novel learning strategies and
    applications. After constructing connections among current approaches, we discussed
    the pivotal problems to be solved and indicated promising future research directions.
    We hope this work can provide some insights for researchers and promote progress
    in the community.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们全面回顾和分析了全景视觉的深度学习方法的最新进展。我们首先介绍了全景成像的原理、卷积方法和数据集。然后，我们提供了深度学习方法的分层和结构化分类。对于分类中的每个任务，我们总结了当前的研究状态，并指出了机遇和挑战。我们进一步回顾了新颖的学习策略和应用。在构建当前方法之间的联系后，我们讨论了需要解决的关键问题，并指出了有前途的未来研究方向。我们希望这项工作能够为研究人员提供一些见解，并促进该领域的进展。
- en: References
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Pi, Y. Zhang, L. Zhu, X. Wu, and X. Zhou, “Content-aware hybrid equi-angular
    cubemap projection for omnidirectional video coding,” *VCIP*, 2020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Pi, Y. Zhang, L. Zhu, X. Wu, 和 X. Zhou，“面向内容的混合等角立方体投影用于全景视频编码，” *VCIP*，2020年。'
- en: '[2] H. Jiang, G. yi Jiang, M. Yu, Y. Zhang, Y. Yang, Z. Peng, F. Chen, and
    Q. Zhang, “Cubemap-based perception-driven blind quality assessment for 360-degree
    images,” *TIP*, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Jiang, G. yi Jiang, M. Yu, Y. Zhang, Y. Yang, Z. Peng, F. Chen, 和 Q.
    Zhang，“基于立方体映射的感知驱动盲质量评估用于360度图像，” *TIP*，2021年。'
- en: '[3] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, “Recognizing scene viewpoint
    using panoramic place representation,” in *CVPR*, 2012.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Xiao, K. A. Ehinger, A. Oliva, 和 A. Torralba，“使用全景场所表示识别场景视角，” *CVPR*，2012年。'
- en: '[4] Y. Rai, J. Gutiérrez, and P. Le Callet, “A dataset of head and eye movements
    for 360 degree images,” in *ACM MMSys*, 2017.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Rai, J. Gutiérrez, 和 P. Le Callet，“用于360度图像的头部和眼部运动数据集，” *ACM MMSys*，2017年。'
- en: '[5] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, “Joint 2d-3d-semantic
    data for indoor scene understanding,” *arXiv*, 2017.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] I. Armeni, S. Sax, A. R. Zamir, 和 S. Savarese，“用于室内场景理解的联合2D-3D-语义数据，”
    *arXiv*，2017年。'
- en: '[6] H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, “Pano-avqa: Grounded audio-visual
    question answering on 360^∘ videos,” *ICCV*, 2021.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Yun, Y. Yu, W. Yang, K. Lee, 和 G. Kim，“Pano-avqa: 基于 360^∘ 视频的音频-视觉问答，”
    *ICCV*，2021年。'
- en: '[7] Y. Zhang, S. Song, P. Tan, and J. Xiao, “Panocontext: A whole-room 3d context
    model for panoramic scene understanding,” in *ECCV*, 2014.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Zhang, S. Song, P. Tan, 和 J. Xiao，“Panocontext: 用于全景场景理解的全房间 3D 上下文模型，”在
    *ECCV*，2014年。'
- en: '[8] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *CVPR*, 2016.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，” *CVPR*，2016年。'
- en: '[9] L. R. Medsker and L. Jain, “Recurrent neural networks,” *Design and Applications*,
    2001.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. R. Medsker 和 L. Jain，“递归神经网络，” *设计与应用*，2001年。'
- en: '[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *NIPS*, 2014.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，” *NIPS*，2014年。'
- en: '[11] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE TNNLS*, 2008.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, 和 G. Monfardini，“图神经网络模型，”
    *IEEE TNNLS*，2008年。'
- en: '[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *ICLR*, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像价值 16x16 个词：用于大规模图像识别的
    Transformers，” *ICLR*，2020年。'
- en: '[13] C. Zou, J.-W. Su, C.-H. Peng, A. Colburn, Q. Shan, P. Wonka, H. kuo Chu,
    and D. Hoiem, “Manhattan room layout reconstruction from a single 360^∘ image:
    A comparative study of state-of-the-art methods,” *IJCV.*, 2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Zou, J.-W. Su, C.-H. Peng, A. Colburn, Q. Shan, P. Wonka, H. kuo Chu,
    和 D. Hoiem，“从单张 360^∘ 图像重建曼哈顿房间布局：前沿方法的比较研究，” *IJCV.*，2021年。'
- en: '[14] T. L. T. da Silveira, P. G. L. Pinto, J. Murrugarra-Llerena, and C. R.
    Jung, “3d scene geometry estimation from 360^∘ imagery: A survey,” *ACM Computing
    Surveys (CSUR)*, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. L. T. da Silveira, P. G. L. Pinto, J. Murrugarra-Llerena, 和 C. R. Jung，“来自
    360^∘ 图像的 3D 场景几何估计：综述，” *ACM Computing Surveys (CSUR)*，2022年。'
- en: '[15] M. Zink, R. K. Sitaraman, and K. Nahrstedt, “Scalable 360^∘ video stream
    delivery: Challenges, solutions, and opportunities,” *Proceedings of the IEEE*,
    2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Zink, R. K. Sitaraman, 和 K. Nahrstedt，“可扩展的 360^∘ 视频流传输：挑战、解决方案和机遇，”
    *IEEE 会议论文集*，2019年。'
- en: '[16] M. Xu, C. Li, S. Zhang, and P. L. Callet, “State-of-the-art in 360^∘ video/image
    processing: Perception, assessment and compression,” *IEEE J-STSP*, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Xu, C. Li, S. Zhang, 和 P. L. Callet，“360^∘ 视频/图像处理的最新进展：感知、评估和压缩，”
    *IEEE J-STSP*，2020年。'
- en: '[17] A. Yaqoob, T. Bi, and G.-M. Muntean, “A survey on adaptive 360 video streaming:
    solutions, challenges and opportunities,” *IEEE Commun. Surv. Tutor.*, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Yaqoob, T. Bi, 和 G.-M. Muntean，“自适应 360 视频流的综述：解决方案、挑战和机遇，” *IEEE Commun.
    Surv. Tutor.*，2020年。'
- en: '[18] N. Zioulis, A. Karakottas, D. Zarpalas, F. Alvarez, and P. Daras, “Spherical
    view synthesis for self-supervised 360 depth estimation,” in *3DV*, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Zioulis, A. Karakottas, D. Zarpalas, F. Alvarez, 和 P. Daras，“用于自监督
    360 深度估计的球面视图合成，”在 *3DV*，2019年。'
- en: '[19] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, and Y.-H. Tsai, “Bifuse: Monocular
    360 depth estimation via bi-projection fusion,” in *CVPR*, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, 和 Y.-H. Tsai，“Bifuse: 通过双投影融合的单目
    360 深度估计，”在 *CVPR*，2020年。'
- en: '[20] Y.-C. Su and K. Grauman, “Learning spherical convolution for fast features
    from 360^∘ imagery,” in *NIPS*, 2017.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y.-C. Su 和 K. Grauman，“学习球面卷积以快速提取 360^∘ 图像的特征，”在 *NIPS*，2017年。'
- en: '[21] B. Coors, A. P. Condurache, and A. Geiger, “Spherenet: Learning spherical
    representations for detection and classification in omnidirectional images,” in
    *ECCV*, 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] B. Coors, A. P. Condurache, 和 A. Geiger，“Spherenet: 学习球面表示用于全向图像中的检测和分类，”在
    *ECCV*，2018年。'
- en: '[22] Q. Zhao, C. Zhu, F. Dai, Y. Ma, G. Jin, and Y. Zhang, “Distortion-aware
    cnns for spherical images.” in *IJCAI*, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Q. Zhao, C. Zhu, F. Dai, Y. Ma, G. Jin, 和 Y. Zhang，“用于球面图像的畸变感知卷积神经网络，”在
    *IJCAI*，2018年。'
- en: '[23] R. Khasanova and P. Frossard, “Geometry aware convolutional filters for
    omnidirectional images representation,” in *ICML*, 2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Khasanova 和 P. Frossard，“用于全向图像表示的几何感知卷积滤波器，”在 *ICML*，2019年。'
- en: '[24] T. O’Beirne, “Introduction to geometry,” *Physics Bulletin*, 1962.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. O’Beirne，“几何学导论，” *Physics Bulletin*，1962年。'
- en: '[25] Y. Li, Y. Guo, Z. Yan, X. Huang, Y. Duan, and L. Ren, “Omnifusion: 360
    monocular depth estimation via geometry-aware fusion,” *CVPR*, 2022.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Li, Y. Guo, Z. Yan, X. Huang, Y. Duan, 和 L. Ren，“Omnifusion: 通过几何感知融合的
    360 单目深度估计，” *CVPR*，2022年。'
- en: '[26] M. Eder, M. Shvets, J. Lim, and J.-M. Frahm, “Tangent images for mitigating
    spherical distortion,” in *CVPR*, 2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Eder, M. Shvets, J. Lim, 和 J.-M. Frahm，“缓解球面失真的切线图像”，发表于 *CVPR*，2020。'
- en: '[27] Y. Lee, J. Jeong, J. S. Yun, W. Cho, and K. jin Yoon, “Spherephd: Applying
    cnns on a spherical polyhedron representation of 360^∘ images,” *CVPR*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Lee, J. Jeong, J. S. Yun, W. Cho, 和 K. jin Yoon，“Spherephd: 在 360^∘
    图像的球面多面体表示上应用卷积神经网络”，发表于 *CVPR*，2019。'
- en: '[28] Y. Yoon, I. Chung, L. Wang, and K.-J. Yoon, “Spheresr,” *CVPR*, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Yoon, I. Chung, L. Wang, 和 K.-J. Yoon，“Spheresr”，发表于 *CVPR*，2022。'
- en: '[29] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, “Spherical cnns,” *arXiv*,
    2018.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. S. Cohen, M. Geiger, J. Köhler, 和 M. Welling，“球面卷积神经网络”，发表于 *arXiv*，2018。'
- en: '[30] J. Cruz-Mota, I. Bogdanova, B. Paquier, M. Bierlaire, and J.-P. Thiran,
    “Scale invariant feature transform on the sphere: Theory and applications,” *IJCV*,
    2012.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Cruz-Mota, I. Bogdanova, B. Paquier, M. Bierlaire, 和 J.-P. Thiran，“球面上的尺度不变特征变换：理论与应用”，发表于
    *IJCV*，2012。'
- en: '[31] Y.-C. Su and K. Grauman, “Kernel transformer networks for compact spherical
    convolution,” *CVPR*, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y.-C. Su 和 K. Grauman，“紧凑球面卷积的核变换网络”，发表于 *CVPR*，2019。'
- en: '[32] ——, “Learning spherical convolution for 360 recognition,” *TPAMI*, 2021.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] ——，“用于 360 识别的球面卷积学习”，发表于 *TPAMI*，2021。'
- en: '[33] P. Frossard and R. Khasanova, “Graph-based classification of omnidirectional
    images,” *ICCV Workshops*, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Frossard 和 R. Khasanova，“基于图的全向图像分类”，发表于 *ICCV Workshops*，2017。'
- en: '[34] M. Rey-Area, M. Yuan, and C. Richardt, “360MonoDepth: High-resolution
    360^∘ monocular depth estimation,” in *CVPR*, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Rey-Area, M. Yuan, 和 C. Richardt，“360MonoDepth: 高分辨率 360^∘ 单目深度估计”，发表于
    *CVPR*，2022。'
- en: '[35] Q. Yang, C. Li, W. Dai, J. Zou, G.-J. Qi, and H. Xiong, “Rotation equivariant
    graph convolutional network for spherical image classification,” *CVPR*, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Q. Yang, C. Li, W. Dai, J. Zou, G.-J. Qi, 和 H. Xiong，“用于球面图像分类的旋转等变图卷积网络”，发表于
    *CVPR*，2020。'
- en: '[36] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, “Learning
    so(3) equivariant representations with spherical cnns,” in *ECCV*, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Esteves, C. Allen-Blanchette, A. Makadia, 和 K. Daniilidis，“使用球面卷积神经网络学习
    so(3) 等变表示”，发表于 *ECCV*，2018。'
- en: '[37] T. Cohen, M. Weiler, B. Kicanaoglu, and M. Welling, “Gauge equivariant
    convolutional networks and the icosahedral cnn,” in *ICML*, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Cohen, M. Weiler, B. Kicanaoglu, 和 M. Welling，“量规等变卷积网络与二十面体卷积神经网络”，发表于
    *ICML*，2019。'
- en: '[38] M. Shakerinava and S. Ravanbakhsh, “Equivariant networks for pixelized
    spheres,” in *ICML*, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Shakerinava 和 S. Ravanbakhsh，“像素化球体的等变网络”，发表于 *ICML*，2021。'
- en: '[39] M. Defferrard, M. Milani, F. Gusset, and N. Perraudin, “Deepsphere: a
    graph-based spherical cnn,” in *ICLR*, 2020.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Defferrard, M. Milani, F. Gusset, 和 N. Perraudin，“Deepsphere: 基于图的球面卷积神经网络”，发表于
    *ICLR*，2020。'
- en: '[40] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou, “Structured3d:
    A large photo-realistic dataset for structured 3d modeling,” in *ECCV*, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, 和 Z. Zhou，“Structured3d: 大规模照片级真实感数据集用于结构化
    3d 建模”，发表于 *ECCV*，2020。'
- en: '[41] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in *CVPR*, 2017.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. Funkhouser，“基于单幅深度图像的语义场景补全”，发表于
    *CVPR*，2017。'
- en: '[42] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J. Chang, and M. Sun,
    “Deep 360 pilot: Learning a deep agent for piloting through 360 sports videos,”
    in *CVPR*, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] H.-N. Hu, Y.-C. Lin, M.-Y. Liu, H.-T. Cheng, Y.-J. Chang, 和 M. Sun，“深度
    360 驾驶员：学习深度代理以操控 360 体育视频”，发表于 *CVPR*，2017。'
- en: '[43] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, and M. Sun,
    “Cube padding for weakly-supervised saliency prediction in 360 videos,” in *CVPR*,
    2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, 和 M. Sun，“用于
    360 视频的弱监督显著性预测的立方体填充”，发表于 *CVPR*，2018。'
- en: '[44] R. Seidel, A. Apitzsch, and G. Hirtz, “Omniflow: Human omnidirectional
    optical flow,” *CVPR Workshops*, 2021.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Seidel, A. Apitzsch, 和 G. Hirtz，“Omniflow: 人类全向光流”，发表于 *CVPR Workshops*，2021。'
- en: '[45] C. Zhang, S. Liwicki, W. Smith, and R. Cipolla, “Orientation-aware semantic
    segmentation on icosahedron spheres,” in *ICCV*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Zhang, S. Liwicki, W. Smith, 和 R. Cipolla，“基于二十面体球体的方向感知语义分割”，发表于 *ICCV*，2019。'
- en: '[46] R. Liu, G. Zhang, J. Wang, and S. Zhao, “Cross-modal 360^∘ depth completion
    and reconstruction for large-scale indoor environment,” *IEEE Trans. Intell. Transp.
    Syst.*, 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Liu, G. Zhang, J. Wang, 和 S. Zhao，“跨模态 360^∘ 深度补全与重建用于大规模室内环境”，发表于
    *IEEE Trans. Intell. Transp. Syst.*，2022。'
- en: '[47] C. Oh, W. Cho, D. Park, Y. Chae, L. Wang, and K.-J. Yoon, “Bips: Bi-modal
    indoor panorama synthesis via residual depth-aided adversarial learning,” *arXiv*,
    2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Oh, W. Cho, D. Park, Y. Chae, L. Wang, 和 K.-J. Yoon，“Bips: 通过残差深度辅助对抗学习进行双模态室内全景合成”，发表于
    *arXiv*，2021。'
- en: '[48] T. Hara, Y. Mukuta, and T. Harada, “Spherical image generation from a
    single image by considering scene symmetry,” in *AAAI*, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Hara, Y. Mukuta 和 T. Harada，"考虑场景对称性从单一图像生成球面图像"，发表于 *AAAI*，2021年。'
- en: '[49] N. Akimoto, Y. Matsuo, and Y. Aoki, “Diverse plausible 360-degree image
    outpainting for efficient 3dcg background creation,” *arXiv*, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] N. Akimoto, Y. Matsuo 和 Y. Aoki，"高效3DCG背景创建的多样化可行360度图像扩展"，发表于 *arXiv*，2022年。'
- en: '[50] J. S. Sumantri and I. K. Park, “360 panorama synthesis from a sparse set
    of images with unknown field of view,” in *WACV*, 2020.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. S. Sumantri 和 I. K. Park，"从稀疏图像集合中合成360度全景图像，视场角未知"，发表于 *WACV*，2020年。'
- en: '[51] L. Roldao, R. De Charette, and A. Verroust-Blondet, “3d semantic scene
    completion: a survey,” *arXiv*, 2021.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. Roldao, R. De Charette 和 A. Verroust-Blondet，"3D语义场景完成：综述"，发表于 *arXiv*，2021年。'
- en: '[52] A. Dourado, H. Kim, T. E. de Campos, and A. Hilton, “Semantic scene completion
    from a single 360-degree image and depth map.” in *VISIGRAPP*, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. Dourado, H. Kim, T. E. de Campos 和 A. Hilton，"从单一360度图像和深度图进行语义场景完成"，发表于
    *VISIGRAPP*，2020年。'
- en: '[53] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner, “Scancomplete:
    Large-scale scene completion and semantic segmentation for 3d scans,” in *CVPR*,
    2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm 和 M. Nießner，"Scancomplete：大规模场景完成和3D扫描的语义分割"，发表于
    *CVPR*，2018年。'
- en: '[54] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys, and R. Qin, “Geometry-aware
    satellite-to-ground image synthesis for urban areas,” in *CVPR*, 2020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys 和 R. Qin，"面向几何的城市区域卫星到地面图像合成"，发表于
    *CVPR*，2020年。'
- en: '[55] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys, and M. R. Oswald, “Sat2vid:
    Street-view panoramic video synthesis from a single satellite image,” in *ICCV*,
    2021.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys 和 M. R. Oswald，"Sat2vid：从单一卫星图像生成街景全景视频"，发表于
    *ICCV*，2021年。'
- en: '[56] M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, “Predicting ground-level
    scene layout from aerial imagery,” in *CVPR*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Zhai, Z. Bessinger, S. Workman 和 N. Jacobs，"从航空图像预测地面场景布局"，发表于 *CVPR*，2017年。'
- en: '[57] K. Regmi and M. Shah, “Bridging the domain gap for ground-to-aerial image
    matching,” in *ICCV*, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Regmi 和 M. Shah，"弥合地面到航空图像匹配的领域差距"，发表于 *ICCV*，2019年。'
- en: '[58] A. Toker, Q. Zhou, M. Maximov, and L. Leal-Taixé, “Coming down to earth:
    Satellite-to-street view synthesis for geo-localization,” in *CVPR*, 2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Toker, Q. Zhou, M. Maximov 和 L. Leal-Taixé，"回到地球：用于地理定位的卫星到街景图像合成"，发表于
    *CVPR*，2021年。'
- en: '[59] Y. Shi, L. Liu, X. Yu, and H. Li, “Spatial-aware feature aggregation for
    image based cross-view geo-localization,” *NIPS*, 2019.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Shi, L. Liu, X. Yu 和 H. Li，"面向空间的特征聚合用于基于图像的交叉视图地理定位"，发表于 *NIPS*，2019年。'
- en: '[60] S. Zhu, M. Shah, and C. Chen, “Transgeo: Transformer is all you need for
    cross-view image geo-localization,” *arXiv*, 2022.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Zhu, M. Shah 和 C. Chen，"Transgeo：跨视图图像地理定位中只需使用 Transformer"，发表于 *arXiv*，2022年。'
- en: '[61] Y. Shi, X. Yu, D. Campbell, and H. Li, “Where am i looking at? joint location
    and orientation estimation by cross-view matching,” in *CVPR*, 2020.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Shi, X. Yu, D. Campbell 和 H. Li，"我在看什么？通过交叉视图匹配进行位置和方向联合估计"，发表于 *CVPR*，2020年。'
- en: '[62] S. Zhu, T. Yang, and C. Chen, “Vigor: Cross-view image geo-localization
    beyond one-to-one retrieval,” in *CVPR*, 2021.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Zhu, T. Yang 和 C. Chen，"Vigor：跨视图图像地理定位超越一对一检索"，发表于 *CVPR*，2021年。'
- en: '[63] C.-Y. Hsu, C. Sun, and H.-T. Chen, “Moving in a 360 world: Synthesizing
    panoramic parallaxes from a single panorama,” *arXiv*, 2021.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C.-Y. Hsu, C. Sun 和 H.-T. Chen，"在360度世界中移动：从单一全景图像合成全景视差"，发表于 *arXiv*，2021年。'
- en: '[64] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in *ECCV*, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi
    和 R. Ng，"Nerf：将场景表示为神经辐射场用于视图合成"，发表于 *ECCV*，2020年。'
- en: '[65] T. Hara and T. Harada, “Enhancement of novel view synthesis using omnidirectional
    image completion,” *arXiv*, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] T. Hara 和 T. Harada，"利用全景图像完成提升新视图合成"，发表于 *arXiv*，2022年。'
- en: '[66] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, “Pathdreamer:
    A world model for indoor navigation,” in *ICCV*, 2021.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge 和 P. Anderson，"Pathdreamer：一种用于室内导航的世界模型"，发表于
    *ICCV*，2021年。'
- en: '[67] F. D. Simone, P. Frossard, P. Wilkins, N. Birkbeck, and A. C. Kokaram,
    “Geometry-driven quantization for omnidirectional image coding,” *PCS*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] F. D. Simone, P. Frossard, P. Wilkins, N. Birkbeck 和 A. C. Kokaram，"面向全景图像编码的几何驱动量化"，发表于
    *PCS*，2016年。'
- en: '[68] M. Řeřábek, E. Upenik, and T. Ebrahimi, “Jpeg backward compatible coding
    of omnidirectional images,” in *Applications of digital image processing XXXIX*,
    2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. Řeřábek, E. Upenik 和 T. Ebrahimi，"JPEG 向后兼容的全景图像编码"，发表于 *Applications
    of digital image processing XXXIX*，2016年。'
- en: '[69] G. K. Wallace, “The jpeg still picture compression standard,” *IEEE TCE*,
    1992.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] G. K. Wallace，“JPEG静态图片压缩标准”，*IEEE TCE*，1992年。'
- en: '[70] M. Rizkallah, F. D. Simone, T. Maugey, C. Guillemot, and P. Frossard,
    “Rate distortion optimized graph partitioning for omnidirectional image coding,”
    *EUSIPCO*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Rizkallah, F. D. Simone, T. Maugey, C. Guillemot, 和 P. Frossard，“针对全向图像编码的率失真优化图分割”，*EUSIPCO*，2018年。'
- en: '[71] G. J. Sullivan and T. Wiegand, “Rate-distortion optimization for video
    compression,” *IEEE Signal Process Mag*, 1998.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] G. J. Sullivan 和 T. Wiegand，“视频压缩的率-失真优化”，*IEEE Signal Process Mag*，1998年。'
- en: '[72] N. M. Bidgoli, R. G. d. A. Azevedo, T. Maugey, A. Roumy, and P. Frossard,
    “Oslo: On-the-sphere learning for omnidirectional images and its application to
    360-degree image compression,” *arXiv*, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. M. Bidgoli, R. G. d. A. Azevedo, T. Maugey, A. Roumy, 和 P. Frossard，“Oslo:
    用于全向图像的球面学习及其在360度图像压缩中的应用”，*arXiv*，2021年。'
- en: '[73] K. M. Gorski, E. Hivon, A. J. Banday, B. D. Wandelt, F. K. Hansen, M. Reinecke,
    and M. Bartelman, “Healpix: A framework for high-resolution discretization and
    fast analysis of data distributed on the sphere,” *The Astrophysical Journal*,
    2005.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. M. Gorski, E. Hivon, A. J. Banday, B. D. Wandelt, F. K. Hansen, M.
    Reinecke, 和 M. Bartelman，“Healpix: 一个用于高分辨率离散化和快速分析球面数据的框架”，*天体物理学杂志*，2005年。'
- en: '[74] Y. Li, J. Xu, and Z. Chen, “Spherical domain rate-distortion optimization
    for omnidirectional video coding,” *IEEE TCSVT*, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Li, J. Xu, 和 Z. Chen，“全向视频编码的球面域率-失真优化”，*IEEE TCSVT*，2019年。'
- en: '[75] Y. Wang, D. Liu, S. Ma, F. Wu, and W. Gao, “Spherical coordinates transform-based
    motion model for panoramic video coding,” *IEEE J-ESTCS*, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Wang, D. Liu, S. Ma, F. Wu, 和 W. Gao，“基于球面坐标变换的全景视频编码运动模型”，*IEEE J-ESTCS*，2019年。'
- en: '[76] C.-W. Fu, L. Wan, T. Wong, and A. C.-S. Leung, “The rhombic dodecahedron
    map: An efficient scheme for encoding panoramic video,” *IEEE TMM*, 2009.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C.-W. Fu, L. Wan, T. Wong, 和 A. C.-S. Leung，“菱形十二面体映射：一种高效的全景视频编码方案”，*IEEE
    TMM*，2009年。'
- en: '[77] L. Li, N. Yan, Z. Li, S. Liu, and H. Li, “$\lambda$-domain perceptual
    rate control for 360-degree video compression,” *IEEE J-STSP*, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Li, N. Yan, Z. Li, S. Liu, 和 H. Li，“$\lambda$-域感知率控制用于360度视频压缩”，*IEEE
    J-STSP*，2020年。'
- en: '[78] T. Zhao, J. Lin, Y. Song, X. Wang, and Y. Niu, “Game theory-driven rate
    control for 360-degree video coding,” *ACM MM*, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Zhao, J. Lin, Y. Song, X. Wang, 和 Y. Niu，“基于博弈论的360度视频编码率控制”，*ACM MM*，2021年。'
- en: '[79] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, and J.-F. Lalonde,
    “Deep outdoor illumination estimation,” in *CVPR*, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, 和 J.-F. Lalonde，“深度户外光照估计”，在*CVPR*，2017年。'
- en: '[80] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagné,
    and J.-F. Lalonde, “Learning to predict indoor illumination from a single image,”
    *ACM TOG*, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagné,
    和 J.-F. Lalonde，“从单张图像中学习预测室内光照”，*ACM TOG*，2017年。'
- en: '[81] M.-A. Gardner, Y. Hold-Geoffroy, K. Sunkavalli, C. Gagné, and J.-F. Lalonde,
    “Deep parametric indoor lighting estimation,” *ICCV*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M.-A. Gardner, Y. Hold-Geoffroy, K. Sunkavalli, C. Gagné, 和 J.-F. Lalonde，“深度参数化室内光照估计”，*ICCV*，2019年。'
- en: '[82] F. Zhan, C. Zhang, Y. Yu, Y. Chang, S. Lu, F. Ma, and X. Xie, “Emlight:
    Lighting estimation via spherical distribution approximation,” *AAAI*, 2021.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] F. Zhan, C. Zhang, Y. Yu, Y. Chang, S. Lu, F. Ma, 和 X. Xie，“Emlight: 通过球面分布逼近进行光照估计”，*AAAI*，2021年。'
- en: '[83] J. P. Rolland and H. Hua, “Head-mounted display systems,” *Encyclopedia
    of optical engineering*, 2005.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. P. Rolland 和 H. Hua，“头戴显示系统”，*光学工程百科全书*，2005年。'
- en: '[84] C. Ozcinar, A. Rana, and A. Smolic, “Super-resolution of omnidirectional
    images using adversarial learning,” in *MMSP*, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] C. Ozcinar, A. Rana, 和 A. Smolic，“使用对抗学习的全向图像超分辨率”，在*MMSP*，2019年。'
- en: '[85] X. Deng, H. Wang, M. Xu, Y. Guo, Y. Song, and L. Yang, “Lau-net: Latitude
    adaptive upscaling network for omnidirectional imag super-resolution,” *CVPR*,
    2021.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Deng, H. Wang, M. Xu, Y. Guo, Y. Song, 和 L. Yang，“Lau-net: 用于全向图像超分辨率的纬度自适应上采样网络”，*CVPR*，2021年。'
- en: '[86] H. Liu, Z. Ruan, C. Fang, P. Zhao, F. Shang, Y. Liu, and L. Wang, “A single
    frame and multi-frame joint network for 360-degree panorama video super-resolution,”
    *arXiv*, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] H. Liu, Z. Ruan, C. Fang, P. Zhao, F. Shang, Y. Liu, 和 L. Wang，“用于360度全景视频超分辨率的单帧与多帧联合网络”，*arXiv*，2020年。'
- en: '[87] M. Bosse, R. J. Rikoski, J. J. Leonard, and S. Teller, “Vanishing points
    and 3d lines from omnidirectional video,” *ICIP*, 2002.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Bosse, R. J. Rikoski, J. J. Leonard, 和 S. Teller，“来自全向视频的消失点和3D线条”，*ICIP*，2002年。'
- en: '[88] C. A. Vanegas, D. G. Aliaga, and B. Benes, “Building reconstruction using
    manhattan-world grammars,” in *CVPR*, 2010.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. A. Vanegas, D. G. Aliaga, 和 B. Benes，“使用曼哈顿世界语法进行建筑重建”，在*CVPR*，2010年。'
- en: '[89] G. Schindler and F. Dellaert, “Atlanta world: an expectation maximization
    framework for simultaneous low-level edge grouping and camera calibration in complex
    man-made environments,” in *CVPR*, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] G. Schindler 和 F. Dellaert，“Atlanta world: 一个用于复杂人工环境中同时进行低级边缘分组和相机标定的期望最大化框架，”发表于
    *CVPR*，2004。'
- en: '[90] J. Jeon, J. Jung, and S. Lee, “Deep upright adjustment of 360 panoramas
    using multiple roll estimations,” *ACCV*, 2018.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Jeon, J. Jung 和 S. Lee，“通过多次滚转估计对360度全景图进行深度竖直调整，” *ACCV*，2018。'
- en: '[91] R. Jung, A. S. J. Lee, A. Ashtari, and J. C. Bazin, “Deep360up: A deep
    learning-based approach for automatic vr image upright adjustment,” *VR*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. Jung, A. S. J. Lee, A. Ashtari 和 J. C. Bazin，“Deep360up：一种基于深度学习的自动VR图像竖直调整方法，”
    *VR*，2019。'
- en: '[92] R. Jung, S. Cho, and J. Kwon, “Upright adjustment with graph convolutional
    networks,” *ICIP*, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] R. Jung, S. Cho 和 J. Kwon，“使用图卷积网络的竖直调整，” *ICIP*，2020。'
- en: '[93] H. taek Lim, H. G. Kim, and Y. M. Ro, “Vr iqa net: Deep virtual reality
    image quality assessment using adversarial learning,” *ICASSP*, 2018.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. taek Lim, H. G. Kim 和 Y. M. Ro，“Vr iqa net：基于对抗学习的深度虚拟现实图像质量评估，” *ICASSP*，2018。'
- en: '[94] C. Li, M. Xu, L. Jiang, S. Zhang, and X. Tao, “Viewport proposal cnn for
    360^∘ video quality assessment,” *CVPR*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. Li, M. Xu, L. Jiang, S. Zhang 和 X. Tao，“用于360^∘ 视频质量评估的视口提议CNN，” *CVPR*，2019。'
- en: '[95] Y. Sun, A. Lu, and L. Yu, “Weighted-to-spherically-uniform quality evaluation
    for omnidirectional video,” *IEEE SPL*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Sun, A. Lu 和 L. Yu，“用于全景视频的加权球面均匀质量评估，” *IEEE SPL*，2017。'
- en: '[96] M. Xu, C. Li, Z. Chen, Z. Wang, and Z. Guan, “Assessing visual quality
    of omnidirectional videos,” *IEEE TCSVT*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] M. Xu, C. Li, Z. Chen, Z. Wang 和 Z. Guan，“全景视频的视觉质量评估，” *IEEE TCSVT*，2019。'
- en: '[97] H. G. Kim, H. taek Lim, and Y. M. Ro, “Deep virtual reality image quality
    assessment with human perception guider for omnidirectional image,” *IEEE TCSVT*,
    2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. G. Kim, H. taek Lim 和 Y. M. Ro，“基于人类感知引导的全景图像深度虚拟现实图像质量评估，” *IEEE TCSVT*，2020。'
- en: '[98] J. Xu, W. Zhou, and Z. Chen, “Blind omnidirectional image quality assessment
    with viewport oriented graph convolutional networks,” *IEEE TCSVT*, 2021.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Xu, W. Zhou 和 Z. Chen，“使用视口导向图卷积网络的盲全景图像质量评估，” *IEEE TCSVT*，2021。'
- en: '[99] W. Zhou, J. Xu, Q. Jiang, and Z. Chen, “No-reference quality assessment
    for 360-degree images by analysis of multifrequency information and local-global
    naturalness,” *IEEE TCSVT*, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. Zhou, J. Xu, Q. Jiang 和 Z. Chen，“通过分析多频信息和局部-全局自然度对360度图像进行无参考质量评估，”
    *IEEE TCSVT*，2022。'
- en: '[100] W. Sun, X. Min, G. Z. S. K. Gu, H. Duan, and S. Ma, “Mc360iqa: A multi-channel
    cnn for blind 360-degree image quality assessment,” *IEEE J-STSP*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] W. Sun, X. Min, G. Z. S. K. Gu, H. Duan 和 S. Ma，“Mc360iqa: 一种用于盲目360度图像质量评估的多通道CNN，”
    *IEEE J-STSP*，2020。'
- en: '[101] R. G. de Albuquerque Azevedo, N. Birkbeck, I. Janatra, B. Adsumilli,
    and P. Frossard, “A viewport-driven multi-metric fusion approach for 360-degree
    video quality assessment,” *ICME*, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] R. G. de Albuquerque Azevedo, N. Birkbeck, I. Janatra, B. Adsumilli 和
    P. Frossard，“用于360度视频质量评估的视口驱动多指标融合方法，” *ICME*，2020。'
- en: '[102] P. Gao, P. Zhang, and A. Smolic, “Quality assessment for omnidirectional
    video: A spatio-temporal distortion modeling approach,” *IEEE TMM*, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Gao, P. Zhang 和 A. Smolic，“全景视频的质量评估：一种时空失真建模方法，” *IEEE TMM*，2022。'
- en: '[103] P. Zhao, A. You, Y. Zhang, J. Liu, K. Bian, and Y. Tong, “Spherical criteria
    for fast and accurate 360 object detection,” in *AAAI*, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Zhao, A. You, Y. Zhang, J. Liu, K. Bian 和 Y. Tong，“用于快速且准确的360物体检测的球面标准，”发表于
    *AAAI*，2020。'
- en: '[104] M. Cao, S. Ikehata, and K. Aizawa, “Field-of-view iou for object detection
    in 360 ^∘ images,” *arXiv*, 2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Cao, S. Ikehata 和 K. Aizawa，“360 ^∘ 图像中的视场iou进行物体检测，” *arXiv*，2022。'
- en: '[105] G. Tong, H. Chen, Y. Li, X. Du, and Q. Zhang, “Object detection for panoramic
    images based on ms-rpn structure in traffic road scenes,” *IET Comput. Vis.*,
    2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] G. Tong, H. Chen, Y. Li, X. Du 和 Q. Zhang，“基于ms-rpn结构的交通道路场景全景图像物体检测，”
    *IET Comput. Vis.*，2019。'
- en: '[106] K.-H. Wang and S.-H. Lai, “Object detection in curved space for 360-degree
    camera,” in *ICASSP*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] K.-H. Wang 和 S.-H. Lai，“360度相机中的曲面空间物体检测，”发表于 *ICASSP*，2019。'
- en: '[107] W. Yang, Y. Qian, J.-K. Kämäräinen, F. Cricri, and L. Fan, “Object detection
    in equirectangular panorama,” in *ICPR*, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] W. Yang, Y. Qian, J.-K. Kämäräinen, F. Cricri 和 L. Fan，“在等距全景图中的物体检测，”发表于
    *ICPR*，2018。'
- en: '[108] K. Tateno, N. Navab, and F. Tombari, “Distortion-aware convolutional
    filters for dense prediction in panoramic images,” in *ECCV*, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Tateno, N. Navab 和 F. Tombari，“用于全景图像中密集预测的失真感知卷积滤波器，”发表于 *ECCV*，2018。'
- en: '[109] C. Ma, J. Zhang, K. Yang, A. Roitberg, and R. Stiefelhagen, “Densepass:
    Dense panoramic semantic segmentation via unsupervised domain adaptation with
    attention-augmented context exchange,” *ITSC*, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Ma, J. Zhang, K. Yang, A. Roitberg, 和 R. Stiefelhagen，“Densepass:
    通过带注意力增强上下文交换的无监督领域适应实现密集全景语义分割，” *ITSC*，2021年。'
- en: '[110] J. Guerrero-Viu, C. Fernandez-Labrador, C. Demonceaux, and J. J. Guerrero,
    “What’s in my room? object recognition on indoor panoramic images,” in *ICRA,
    2020*, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Guerrero-Viu, C. Fernandez-Labrador, C. Demonceaux, 和 J. J. Guerrero，“我房间里有什么？室内全景图像的物体识别，”
    在 *ICRA, 2020*，2020年。'
- en: '[111] K. Yang, J. Zhang, S. Reiß, X. Hu, and R. Stiefelhagen, “Capturing omni-range
    context for omnidirectional segmentation,” *CVPR*, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] K. Yang, J. Zhang, S. Reiß, X. Hu, 和 R. Stiefelhagen，“捕捉全范围上下文用于全向分割，”
    *CVPR*，2021年。'
- en: '[112] J. Zhang, K. Yang, C. Ma, S. Reiß, K. Peng, and R. Stiefelhagen, “Bending
    reality: Distortion-aware transformers for adapting to panoramic semantic segmentation,”
    *arXiv*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Zhang, K. Yang, C. Ma, S. Reiß, K. Peng, 和 R. Stiefelhagen，“弯曲现实：用于适应全景语义分割的失真感知变换器，”
    *arXiv*，2022年。'
- en: '[113] J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, and R. Stiefelhagen,
    “Transfer beyond the field of view: Dense panoramic semantic segmentation via
    unsupervised domain adaptation,” *IEEE Trans. on Intell. Trans. Sys.*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, 和 R. Stiefelhagen，“超越视场：通过无监督领域适应实现密集全景语义分割，”
    *IEEE Trans. on Intell. Trans. Sys.*，2021年。'
- en: '[114] L. Deng, M. Yang, Y. Qian, C. Wang, and B. Wang, “Cnn based semantic
    segmentation for urban traffic scenes using fisheye camera,” *IV*, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] L. Deng, M. Yang, Y. Qian, C. Wang, 和 B. Wang，“基于 CNN 的城市交通场景语义分割，使用鱼眼摄像头，”
    *IV*，2017年。'
- en: '[115] L. Deng, M. Yang, H. Li, T. Li, B. Hu, and C. Wang, “Restricted deformable
    convolution-based road scene semantic segmentation using surround view cameras,”
    *IEEE Trans. Intell. Transp. Syst.*, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] L. Deng, M. Yang, H. Li, T. Li, B. Hu, 和 C. Wang，“基于限制性可变形卷积的道路场景语义分割，使用环视摄像头，”
    *IEEE Trans. Intell. Transp. Syst.*，2020年。'
- en: '[116] S. Orhan and Y. Bastanlar, “Semantic segmentation of outdoor panoramic
    images,” *Signal Image Video Process.*, 2022.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. Orhan 和 Y. Bastanlar，“户外全景图像的语义分割，” *Signal Image Video Process.*，2022年。'
- en: '[117] C. Fernandez-Labrador, J. M. Fácil, A. Pérez-Yus, C. Demonceaux, J. Civera,
    and J. J. Guerrero, “Corners for layout: End-to-end layout recovery from 360 images,”
    *IEEE Robot. Autom. Lett.*, 2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Fernandez-Labrador, J. M. Fácil, A. Pérez-Yus, C. Demonceaux, J. Civera,
    和 J. J. Guerrero，“布局角落：从 360 图像中端到端的布局恢复，” *IEEE Robot. Autom. Lett.*，2020年。'
- en: '[118] K. Yang, X. Hu, L. M. Bergasa, E. Romera, X. Huang, D. Sun, and K. Wang,
    “Can we pass beyond the field of view? panoramic annular semantic segmentation
    for real-world surrounding perception,” *IV*, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Yang, X. Hu, L. M. Bergasa, E. Romera, X. Huang, D. Sun, 和 K. Wang，“我们能超越视场吗？用于现实世界周边感知的全景环形语义分割，”
    *IV*，2019年。'
- en: '[119] K. Yang, X. Hu, H. Chen, K. Xiang, K. Wang, and R. Stiefelhagen, “Ds-pass:
    Detail-sensitive panoramic annular semantic segmentation through swaftnet for
    surrounding sensing,” *IV*, 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Yang, X. Hu, H. Chen, K. Xiang, K. Wang, 和 R. Stiefelhagen，“Ds-pass:
    通过 swaftnet 实现详细敏感的全景环形语义分割，用于周边感知，” *IV*，2020年。'
- en: '[120] A. Jaus, K. Yang, and R. Stiefelhagen, “Panoramic panoptic segmentation:
    Towards complete surrounding understanding via unsupervised contrastive learning,”
    *IV*, 2021.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Jaus, K. Yang, 和 R. Stiefelhagen，“全景全景分割：通过无监督对比学习实现完整的周边理解，” *IV*，2021年。'
- en: '[121] K. Yang, X. Hu, Y. Fang, K. Wang, and R. Stiefelhagen, “Omnisupervised
    omnidirectional semantic segmentation,” *IEEE Trans. Intell. Transp. Syst.*, 2022.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] K. Yang, X. Hu, Y. Fang, K. Wang, 和 R. Stiefelhagen，“全向监督的全向语义分割，” *IEEE
    Trans. Intell. Transp. Syst.*，2022年。'
- en: '[122] N. Zioulis, A. Karakottas, D. Zarpalas, and P. Daras, “Omnidepth: Dense
    depth estimation for indoors spherical panoramas,” in *ECCV*, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] N. Zioulis, A. Karakottas, D. Zarpalas, 和 P. Daras，“Omnidepth: 室内球形全景的密集深度估计，”
    在 *ECCV*，2018年。'
- en: '[123] G. Pintore, E. Almansa, and J. Schneider, “Slicenet: deep dense depth
    estimation from a single indoor panorama using a slice-based representation,”
    *CVPR*, 2021.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. Pintore, E. Almansa, 和 J. Schneider，“Slicenet: 从单一室内全景图像中基于切片的表示进行深度密集深度估计，”
    *CVPR*，2021年。'
- en: '[124] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab, “Deeper
    depth prediction with fully convolutional residual networks,” in *3DV*, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, 和 N. Navab，“通过全卷积残差网络进行更深的深度预测，”
    在 *3DV*，2016年。'
- en: '[125] C. Zhuang, Z. Lu, Y. Wang, J. Xiao, and Y. Wang, “Acdnet: Adaptively
    combined dilated convolution for monocular panorama depth estimation,” *arXiv*,
    2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] C. Zhuang, Z. Lu, Y. Wang, J. Xiao, 和 Y. Wang，“Acdnet: 适应性结合膨胀卷积进行单目全景深度估计，”
    *arXiv*，2021年。'
- en: '[126] L. Jin, Y. Xu, J. Zheng, J. Zhang, R. Tang, S. Xu, J. Yu, and S. Gao,
    “Geometric structure based and regularized depth estimation from 360 indoor imagery,”
    in *CVPR*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] L. Jin, Y. Xu, J. Zheng, J. Zhang, R. Tang, S. Xu, J. Yu, 和 S. Gao, “基于几何结构和正则化的360室内图像深度估计，”
    在 *CVPR*，2020年。'
- en: '[127] F.-E. Wang, H.-N. Hu, H.-T. Cheng, J.-T. Lin, S.-T. Yang, M.-L. Shih,
    H.-K. Chu, and M. Sun, “Self-supervised learning of depth and camera motion from
    360^∘ videos,” in *ACCV*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] F.-E. Wang, H.-N. Hu, H.-T. Cheng, J.-T. Lin, S.-T. Yang, M.-L. Shih,
    H.-K. Chu, 和 M. Sun, “从360^∘视频中自监督学习深度和相机运动，” 在 *ACCV*，2018年。'
- en: '[128] I. Yun, H.-J. Lee, and C. E. Rhee, “Improving 360 monocular depth estimation
    via non-local dense prediction transformer and joint supervised and self-supervised
    learning,” *arXiv*, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] I. Yun, H.-J. Lee, 和 C. E. Rhee, “通过非局部密集预测变换器和联合监督与自监督学习改进360单目深度估计，”
    *arXiv*，2021年。'
- en: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE TIP*, 2004.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli, “图像质量评估：从误差可见性到结构相似性，”
    *IEEE TIP*，2004年。'
- en: '[130] Q. Feng, H. P. Shum, and S. Morishima, “360 depth estimation in the wild-the
    depth360 dataset and the segfuse network,” in *VR*, 2022.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Q. Feng, H. P. Shum, 和 S. Morishima, “野外360深度估计——depth360数据集和segfuse网络，”
    在 *VR*，2022年。'
- en: '[131] M. Eder, P. Moulon, and L. Guan, “Pano popups: Indoor 3d reconstruction
    with a plane-aware network,” in *3DV*, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Eder, P. Moulon, 和 L. Guan, “Pano popups: 基于平面感知网络的室内3D重建，” 在 *3DV*，2019年。'
- en: '[132] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional
    lstm network: A machine learning approach for precipitation nowcasting,” *NIPS*,
    2015.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, 和 W.-c. Woo, “卷积LSTM网络：用于降水即时预报的机器学习方法，”
    *NIPS*，2015年。'
- en: '[133] H. Jiang, Z. Sheng, S. Zhu, Z. Dong, and R. Huang, “Unifuse: Unidirectional
    fusion for 360 panorama depth estimation,” *IEEE Robot. Autom. Lett.*, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. Jiang, Z. Sheng, S. Zhu, Z. Dong, 和 R. Huang, “Unifuse: 用于360全景深度估计的单向融合，”
    *IEEE Robot. Autom. Lett.*，2021年。'
- en: '[134] J. Bai, S. Lai, H. Qin, J. Guo, and Y. Guo, “Glpanodepth: Global-to-local
    panoramic depth estimation,” *arXiv*, 2022.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] J. Bai, S. Lai, H. Qin, J. Guo, 和 Y. Guo, “Glpanodepth: 从全局到局部的全景深度估计，”
    *arXiv*，2022年。'
- en: '[135] R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense
    prediction,” in *ICCV*, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] R. Ranftl, A. Bochkovskiy, 和 V. Koltun, “用于密集预测的视觉变换器，” 在 *ICCV*，2021年。'
- en: '[136] B. Y. Feng, W. Yao, Z. Liu, and A. Varshney, “Deep depth estimation on
    360 images with a double quaternion loss,” in *3DV*, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] B. Y. Feng, W. Yao, Z. Liu, 和 A. Varshney, “通过双四元数损失对360图像进行深度估计，” 在
    *3DV*，2020年。'
- en: '[137] A. Apitzsch, R. Seidel, and G. Hirtz, “Cubes3d: Neural network based
    optical flow in omnidirectional image scenes,” *arXiv*, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Apitzsch, R. Seidel, 和 G. Hirtz, “Cubes3d: 基于神经网络的全景图像场景中的光流估计，” *arXiv*，2018年。'
- en: '[138] S. Xie, P. K. Lai, R. Laganière, and J. Lang, “Effective convolutional
    neural network layers in flow estimation for omni-directional images,” *3DV*,
    2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] S. Xie, P. K. Lai, R. Laganière, 和 J. Lang, “在全向图像的流量估计中的有效卷积神经网络层，”
    *3DV*，2019年。'
- en: '[139] C.-O. Artizzu, H. Zhang, G. Allibert, and C. Demonceaux, “Omniflownet:
    a perspective neural network adaptation for optical flow estimation in omnidirectional
    images,” *ICPR*, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] C.-O. Artizzu, H. Zhang, G. Allibert, 和 C. Demonceaux, “Omniflownet:
    用于全景图像的光流估计的视角神经网络适配，” *ICPR*，2021年。'
- en: '[140] K. Bhandari, Z. Zong, and Y. Yan, “Revisiting optical flow estimation
    in 360 videos,” *ICPR*, 2021.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. Bhandari, Z. Zong, 和 Y. Yan, “重新审视360视频中的光流估计，” *ICPR*，2021年。'
- en: '[141] T.-W. Hui, X. Tang, and C. C. Loy, “Liteflownet: A lightweight convolutional
    neural network for optical flow estimation,” *CVPR*, 2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T.-W. Hui, X. Tang, 和 C. C. Loy, “Liteflownet: 用于光流估计的轻量级卷积神经网络，” *CVPR*，2018年。'
- en: '[142] H. Shi, Y. Zhou, K. Yang, Y. Ye, X. Yin, Z. Yin, S. Meng, and K. Wang,
    “Panoflow: Learning optical flow for panoramic images,” *arXiv*, 2022.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] H. Shi, Y. Zhou, K. Yang, Y. Ye, X. Yin, Z. Yin, S. Meng, 和 K. Wang,
    “Panoflow: 用于全景图像的光流学习，” *arXiv*，2022年。'
- en: '[143] Y. Su, D. Jayaraman, and K. Grauman, “Pano2vid: Automatic cinematography
    for watching 360^∘ videos,” in *ACCV*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Su, D. Jayaraman, 和 K. Grauman, “Pano2vid: 自动化摄影技术用于观看360^∘视频，” 在
    *ACCV*，2016年。'
- en: '[144] Y. Su and K. Grauman, “Making 360^∘ video watchable in 2d: Learning videography
    for click free viewing,” in *CVPR*, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Su 和 K. Grauman, “使360^∘视频在2D中可观看：学习点击无缝观看的视频摄影技术，” 在 *CVPR*，2017年。'
- en: '[145] Y. Yu, S. Lee, J. Na, J. Kang, and G. Kim, “A deep ranking model for
    spatio-temporal highlight detection from a 360^∘ video,” in *AAAI*, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. Yu, S. Lee, J. Na, J. Kang, 和 G. Kim, “用于360^∘视频的时空亮点检测的深度排名模型，” 在
    *AAAI*，2018年。'
- en: '[146] S. Lee, J. Sung, Y. Yu, and G. Kim, “A memory network approach for story-based
    temporal summarization of 360^∘ videos,” in *CVPR*, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Lee, J. Sung, Y. Yu, 和 G. Kim, “基于记忆网络的方法进行360^∘视频的故事化时间总结，” *CVPR*,
    2018。'
- en: '[147] C. Zhang, Z. Cui, C. Chen, S. Liu, B. Zeng, H. Bao, and Y. Zhang, “Deeppanocontext:
    Panoramic 3d scene understanding with holistic scene context graph and relation-based
    optimization,” *ICCV*, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] C. Zhang, Z. Cui, C. Chen, S. Liu, B. Zeng, H. Bao, 和 Y. Zhang, “Deeppanocontext:
    具有整体场景上下文图和基于关系的优化的全景3D场景理解，” *ICCV*, 2021。'
- en: '[148] S.-T. Yang, F.-E. Wang, C.-H. Peng, P. Wonka, M. Sun, and H. kuo Chu,
    “Dula-net: A dual-projection network for estimating room layouts from a single
    rgb panorama,” *CVPR*, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S.-T. Yang, F.-E. Wang, C.-H. Peng, P. Wonka, M. Sun, 和 H. kuo Chu, “Dula-net:
    用于从单张RGB全景图像估计房间布局的双投影网络，” *CVPR*, 2019。'
- en: '[149] C. Zou, A. Colburn, Q. Shan, and D. Hoiem, “Layoutnet: Reconstructing
    the 3d room layout from a single rgb image,” *CVPR*, 2018.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] C. Zou, A. Colburn, Q. Shan, 和 D. Hoiem, “Layoutnet: 从单张RGB图像重建3D房间布局，”
    *CVPR*, 2018。'
- en: '[150] P. V. Tran, “Sslayout360: Semi-supervised indoor layout estimation from
    360^∘ panorama,” *CVPR*, 2021.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] P. V. Tran, “Sslayout360: 从360^∘全景图中进行半监督的室内布局估计，” *CVPR*, 2021。'
- en: '[151] G. Pintore, M. Agus, and E. Gobbetti, “Atlantanet: Inferring the 3d indoor
    layout from a single $360^{\circ}$ image beyond the manhattan world assumption,”
    in *ECCV*, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] G. Pintore, M. Agus, 和 E. Gobbetti, “Atlantanet: 从单张 $360^{\circ}$ 图像中推断3D室内布局，超越曼哈顿世界假设，”
    *ECCV*, 2020。'
- en: '[152] C. Sun, C.-W. Hsiao, M. Sun, and H.-T. Chen, “Horizonnet: Learning room
    layout with 1d representation and pano stretch data augmentation,” *CVPR*, 2019.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] C. Sun, C.-W. Hsiao, M. Sun, 和 H.-T. Chen, “Horizonnet: 利用1D表示和全景拉伸数据增强进行房间布局学习，”
    *CVPR*, 2019。'
- en: '[153] C. Sun, M. Sun, and H.-T. Chen, “Hohonet: 360 indoor holistic understanding
    with latent horizontal features,” *CVPR*, 2021.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] C. Sun, M. Sun, 和 H.-T. Chen, “Hohonet: 利用潜在水平特征进行360室内整体理解，” *CVPR*,
    2021。'
- en: '[154] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, and Y.-H. Tsai, “Led2-net:
    Monocular 360^∘ layout estimation via differentiable depth rendering,” *CVPR*,
    2021.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] F.-E. Wang, Y.-H. Yeh, M. Sun, W.-C. Chiu, 和 Y.-H. Tsai, “Led2-net: 通过可微分深度渲染进行单目360^∘布局估计，”
    *CVPR*, 2021。'
- en: '[155] J. Seuffert, A. P. Grassi, T. Scheck, and G. Hirtz, “A study on the influence
    of omnidirectional distortion on cnn-based stereo vision,” in *VISIGRAPP*, 2021.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Seuffert, A. P. Grassi, T. Scheck, 和 G. Hirtz, “全向失真对基于CNN的立体视觉影响的研究，”
    *VISIGRAPP*, 2021。'
- en: '[156] C. Won, J. Ryu, and J. Lim, “Sweepnet: Wide-baseline omnidirectional
    depth estimation,” *ICRA*, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] C. Won, J. Ryu, 和 J. Lim, “Sweepnet: 宽基线全向深度估计，” *ICRA*, 2019。'
- en: '[157] ——, “Omnimvs: End-to-end learning for omnidirectional stereo matching,”
    *ICCV*, 2019.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] ——, “Omnimvs: 全向立体匹配的端到端学习，” *ICCV*, 2019。'
- en: '[158] ——, “End-to-end learning for omnidirectional stereo matching with uncertainty
    prior,” *IEEE TPAMI*, 2021.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] ——, “用于全向立体匹配的不确定性先验的端到端学习，” *IEEE TPAMI*, 2021。'
- en: '[159] N.-H. Wang, B. Solarte, Y.-H. Tsai, W.-C. Chiu, and M. Sun, “360sd-net:
    360^∘ stereo depth estimation with learnable cost volume,” *ICRA*, 2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] N.-H. Wang, B. Solarte, Y.-H. Tsai, W.-C. Chiu, 和 M. Sun, “360sd-net:
    可学习成本体积的360^∘立体深度估计，” *ICRA*, 2020。'
- en: '[160] A. Chiuso, P. Favaro, H. Jin, and S. Soatto, “Structure from motion causally
    integrated over time,” *IEEE TPAMI*, 2002.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Chiuso, P. Favaro, H. Jin, 和 S. Soatto, “结构从运动中按时间因果集成，” *IEEE TPAMI*,
    2002。'
- en: '[161] R. Mur-Artal and J. D. Tardós, “Orb-slam2: An open-source slam system
    for monocular, stereo, and rgb-d cameras,” *IEEE T-RO*, 2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R. Mur-Artal 和 J. D. Tardós, “Orb-slam2: 一个开源的单目、立体和RGB-D相机SLAM系统，” *IEEE
    T-RO*, 2017。'
- en: '[162] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardós, “Orb-slam: A versatile
    and accurate monocular slam system,” *IEEE T-RO*, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] R. Mur-Artal, J. M. M. Montiel, 和 J. D. Tardós, “Orb-slam: 一种多功能且准确的单目SLAM系统，”
    *IEEE T-RO*, 2015。'
- en: '[163] S. Urban and S. Hinz, “Multicol-slam-a modular real-time multi-camera
    slam system,” *arXiv*, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] S. Urban 和 S. Hinz, “Multicol-slam-一个模块化实时多摄像头SLAM系统，” *arXiv*, 2016。'
- en: '[164] D. Caruso, J. J. Engel, and D. Cremers, “Large-scale direct slam for
    omnidirectional cameras,” *IROS*, 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] D. Caruso, J. J. Engel, 和 D. Cremers, “大规模直接SLAM用于全向相机，” *IROS*, 2015。'
- en: '[165] Z. Teed and J. Deng, “Droid-slam: Deep visual slam for monocular, stereo,
    and rgb-d cameras,” in *NIPS*, 2021.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Z. Teed 和 J. Deng, “Droid-slam: 单目、立体和RGB-D相机的深度视觉SLAM，” *NIPS*, 2021。'
- en: '[166] J. Czarnowski, T. Laidlow, R. Clark, and A. J. Davison, “Deepfactors:
    Real-time probabilistic dense monocular slam,” *IEEE Robot. Autom. Lett.*, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Czarnowski, T. Laidlow, R. Clark, 和 A. J. Davison, “Deepfactors: 实时概率密集单目SLAM，”
    *IEEE Robot. Autom. Lett.*, 2020。'
- en: '[167] F. Dai, Y. Zhang, Y. Ma, H. Li, and Q. Zhao, “Dilated convolutional neural
    networks for panoramic image saliency prediction,” *ICASSP*, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] F. 戴, Y. 张, Y. 马, H. 李, 和 Q. 赵, “用于全景图像显著性预测的扩张卷积神经网络，” *ICASSP*，2020年。'
- en: '[168] H. Lv, Q. Yang, C. Li, W. Dai, J. Zou, and H. Xiong, “Salgcn: Saliency
    prediction for 360-degree images based on spherical graph convolutional networks,”
    *ACM MM*, 2020.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. 吕, Q. 杨, C. 李, W. 戴, J. 邹, 和 H. 熊, “Salgcn：基于球面图卷积网络的360度图像显著性预测，”
    *ACM MM*，2020年。'
- en: '[169] F.-Y. Chao, L. Zhang, W. Hamidouche, and O. Déforges, “A multi-fov viewport-based
    visual saliency model using adaptive weighting losses for 360^∘ images,” *IEEE
    TMM*, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] F.-Y. Chao, L. 张, W. Hamidouche, 和 O. Déforges, “一种基于多视场视口的视觉显著性模型，使用自适应加权损失进行360^∘图像的处理，”
    *IEEE TMM*，2021年。'
- en: '[170] Y. Abdelaziz, D. Djilali, T. Krishna, K. McGuinness, and N. E. O’Connor,
    “Rethinking 360^∘ image visual attention modelling with unsupervised learning,”
    *ICCV*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Y. Abdelaziz, D. Djilali, T. Krishna, K. McGuinness, 和 N. E. O’Connor,
    “用无监督学习重新思考360^∘图像视觉注意建模，” *ICCV*，2021年。'
- en: '[171] M. Xu, L. Yang, X. Tao, Y. Duan, and Z. Wang, “Saliency prediction on
    omnidirectional image with generative adversarial imitation learning,” *IEEE TIP*,
    2021.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. 徐, L. 杨, X. 陶, Y. 段, 和 Z. 王, “使用生成对抗模仿学习进行全向图像的显著性预测，” *IEEE TIP*，2021年。'
- en: '[172] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique: Detecting
    360-degree video saliency in head-mounted display for head movement prediction,”
    *ACM MM*, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Nguyen, Z. Yan, 和 K. Nahrstedt, “你的注意力是独特的：在头戴显示器中检测360度视频显著性，以预测头部运动，”
    *ACM MM*，2018年。'
- en: '[173] Z. Zhang, Y. Xu, J. Yu, and S. Gao, “Saliency detection in 360^∘ videos,”
    in *ECCV*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z. 张, Y. 徐, J. 于, 和 S. 高, “360^∘视频中的显著性检测，” 见 *ECCV*，2018年。'
- en: '[174] M. Xu, Y. Song, J. Wang, M. Qiao, L. Huo, and Z. Wang, “Predicting head
    movement in panoramic video: A deep reinforcement learning approach,” *IEEE TPAMI*,
    2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. 徐, Y. 宋, J. 王, M. 乔, L. 霍, 和 Z. 王, “预测全景视频中的头部运动：一种深度强化学习方法，” *IEEE
    TPAMI*，2019年。'
- en: '[175] Y. Zhu, G. Zhai, Y. Yang, H. Duan, X. Min, and X. Yang, “Viewing behavior
    supported visual saliency predictor for 360 degree videos,” *IEEE TCSVT*, 2021.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. 朱, G. 翟, Y. 杨, H. 段, X. 闵, 和 X. 杨, “支持视行为的360度视频视觉显著性预测器，” *IEEE TCSVT*，2021年。'
- en: '[176] M. Qiao, M. Xu, Z. Wang, and A. Borji, “Viewport-dependent saliency prediction
    in 360^∘ video,” *IEEE TMM*, 2021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M. 乔, M. 徐, Z. 王, 和 A. Borji, “360^∘视频中的视口依赖显著性预测，” *IEEE TMM*，2021年。'
- en: '[177] R. Monroy, S. Lutz, T. Chalasani, and A. Smolic, “Salnet360: Saliency
    maps for omni-directional images with cnn,” *Signal Process Image Commun*, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Monroy, S. Lutz, T. Chalasani, 和 A. Smolic, “Salnet360：用于全向图像的显著性图，基于CNN，”
    *Signal Process Image Commun*，2017年。'
- en: '[178] R. Zhang, C. Chen, J. Zhang, J. Peng, and A. M. T. Alzbier, “360-degree
    visual saliency detection based on fast-mapped convolution and adaptive equator-bias
    perception,” *The Visual Computer*, 2022.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] R. 张, C. 陈, J. 张, J. 彭, 和 A. M. T. Alzbier, “基于快速映射卷积和自适应赤道偏置感知的360度视觉显著性检测，”
    *The Visual Computer*，2022年。'
- en: '[179] Y. Yang, Y. Zhu, Z. Gao, and G. Zhai, “Salgfcn: Graph based fully convolutional
    network for panoramic saliency prediction,” *VCIP*, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. 杨, Y. 朱, Z. 高, 和 G. 翟, “Salgfcn：基于图的全卷积网络进行全景显著性预测，” *VCIP*，2021年。'
- en: '[180] P. Mazumdar and F. Battisti, “A content-based approach for saliency estimation
    in 360 images,” *ICIP*, 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] P. Mazumdar 和 F. Battisti, “一种基于内容的360度图像显著性估计方法，” *ICIP*，2019年。'
- en: '[181] T. Suzuki and T. Yamanaka, “Saliency map estimation for omni-directional
    image considering prior distributions,” *SMC*, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] T. 铃木 和 T. 山中, “考虑先验分布的全向图像显著性图估计，” *SMC*，2018年。'
- en: '[182] I. Djemai, S. A. Fezza, W. Hamidouche, and O. Déforges, “Extending 2d
    saliency models for head movement prediction in 360-degree images using cnn-based
    fusion,” *ISCAS*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] I. Djemai, S. A. Fezza, W. Hamidouche, 和 O. Déforges, “使用基于CNN的融合扩展2D显著性模型，以预测360度图像中的头部运动，”
    *ISCAS*，2020年。'
- en: '[183] D. Chen, C. Qing, X. Xu, and H. Zhu, “Salbinet360: Saliency prediction
    on 360^∘ images with local-global bifurcated deep network,” *VR*, 2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] D. 陈, C. 青, X. 徐, 和 H. 朱, “Salbinet360：基于局部-全局分叉深度网络的360^∘图像显著性预测，” *VR*，2020年。'
- en: '[184] Y. Zhu, G. Zhai, X. Min, and J. Zhou, “The prediction of saliency map
    for head and eye movements in 360 degree images,” *IEEE TMM*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. 朱, G. 翟, X. 闵, 和 J. 周, “360度图像中头部和眼睛运动的显著性图预测，” *IEEE TMM*，2020年。'
- en: '[185] F.-Y. Chao, L. Zhang, W. Hamidouche, and O. Déforges, “Salgan360: Visual
    saliency prediction on 360 degree images with generative adversarial networks,”
    *ICME Workshop*, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] F.-Y. Chao, L. 张, W. Hamidouche, 和 O. Déforges, “Salgan360：基于生成对抗网络的360度图像视觉显著性预测，”
    *ICME Workshop*，2018年。'
- en: '[186] J. Pan, C. Canton-Ferrer, K. McGuinness, N. E. O’Connor, J. Torres, E. Sayrol,
    and X. G. i Nieto, “Salgan: Visual saliency prediction with generative adversarial
    networks,” *arXiv*, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Pan, C. Canton-Ferrer, K. McGuinness, N. E. O’Connor, J. Torres, E. Sayrol,
    和 X. G. i Nieto, “Salgan: 基于生成对抗网络的视觉显著性预测，” *arXiv*, 2017。'
- en: '[187] D. Zhu, Y. Chen, T. Han, D. Zhao, Y. Zhu, Q. Zhou, G. Zhai, and X. Yang,
    “Ransp: Ranking attention network for saliency prediction on omnidirectional images,”
    *ICME*, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] D. Zhu, Y. Chen, T. Han, D. Zhao, Y. Zhu, Q. Zhou, G. Zhai, 和 X. Yang,
    “Ransp: 基于注意力网络的全景图像显著性预测，” *ICME*, 2020。'
- en: '[188] D. Zhu, Y. Chen, D. Zhao, Q. Zhou, and X. Yang, “Saliency prediction
    on omnidirectional images with attention-aware feature fusion network,” *Appl.
    Intell.*, 2021.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] D. Zhu, Y. Chen, D. Zhao, Q. Zhou, 和 X. Yang, “基于注意力感知特征融合网络的全景图像显著性预测，”
    *Appl. Intell.*, 2021。'
- en: '[189] M. Assens, X. G. i Nieto, K. McGuinness, and N. E. O’Connor, “Saltinet:
    Scan-path prediction on 360 degree images using saliency volumes,” *ICCV Workshop*,
    2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] M. Assens, X. G. i Nieto, K. McGuinness, 和 N. E. O’Connor, “Saltinet:
    使用显著性体积对360度图像进行扫描路径预测，” *ICCV Workshop*, 2017。'
- en: '[190] B. Zhou, A. Khosla, À. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” *CVPR*, 2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] B. Zhou, A. Khosla, À. Lapedriza, A. Oliva, 和 A. Torralba, “为判别性定位学习深度特征，”
    *CVPR*, 2016。'
- en: '[191] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba, “Gaze360:
    Physically unconstrained gaze estimation in the wild,” in *CVPR*, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, 和 A. Torralba, “Gaze360:
    在自然环境中进行物理上不受限的凝视估计，” 在 *CVPR*, 2019。'
- en: '[192] Y. Li, W. Shen, Z. Gao, Y. Zhu, G. Zhai, and G. Guo, “Looking here or
    there? gaze following in 360-degree images,” *ICCV*, 2021.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Li, W. Shen, Z. Gao, Y. Zhu, G. Zhai, 和 G. Guo, “这里还是那里？在360度图像中的凝视跟踪，”
    *ICCV*, 2021。'
- en: '[193] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and S. Gao, “Gaze prediction
    in dynamic 360^∘ immersive videos,” *CVPR*, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, 和 S. Gao, “在动态360度沉浸式视频中的凝视预测，”
    *CVPR*, 2018。'
- en: '[194] C. Wu, R. Zhang, Z. Wang, and L. Sun, “A spherical convolution approach
    for learning long term viewport prediction in 360 immersive video,” in *AAAI*,
    2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] C. Wu, R. Zhang, Z. Wang, 和 L. Sun, “一种用于在360度沉浸式视频中学习长期视口预测的球面卷积方法，”
    在 *AAAI*, 2020。'
- en: '[195] P. Morgado, N. Vasconcelos, T. R. Langlois, and O. Wang, “Self-supervised
    generation of spatial audio for 360 video,” in *NeurIPS*, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] P. Morgado, N. Vasconcelos, T. R. Langlois, 和 O. Wang, “360视频的自监督空间音频生成，”
    在 *NeurIPS*, 2018。'
- en: '[196] P. Morgado, Y. Li, and N. Nvasconcelos, “Learning representations from
    audio-visual spatial alignment,” *NeurIPS*, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] P. Morgado, Y. Li, 和 N. Nvasconcelos, “从视听空间对齐中学习表征，” *NeurIPS*, 2020。'
- en: '[197] Y. Masuyama, Y. Bando, K. Yatabe, Y. Sasaki, M. Onishi, and Y. Oikawa,
    “Self-supervised neural audio-visual sound source localization via probabilistic
    spatial modeling,” *IROS*, 2020.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Masuyama, Y. Bando, K. Yatabe, Y. Sasaki, M. Onishi, 和 Y. Oikawa,
    “通过概率空间建模进行自监督神经视听声音源定位，” *IROS*, 2020。'
- en: '[198] A. B. Vasudevan, D. Dai, and L. V. Gool, “Semantic object prediction
    and spatial sound super-resolution with binaural sounds,” in *ECCV*, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. B. Vasudevan, D. Dai, 和 L. V. Gool, “带有双耳声音的语义对象预测和空间声音超分辨率，” 在 *ECCV*,
    2020。'
- en: '[199] F.-Y. Chao, C. Ozcinar, L. Zhang, W. Hamidouche, O. Déforges, and A. Smolic,
    “Towards audio-visual saliency prediction for omnidirectional video with spatial
    audio,” *VCIP*, 2020.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] F.-Y. Chao, C. Ozcinar, L. Zhang, W. Hamidouche, O. Déforges, 和 A. Smolic,
    “针对具有空间音频的全景视频的视听显著性预测，” *VCIP*, 2020。'
- en: '[200] F.-Y. Chao, C. Ozcinar, C. Wang, E. Zerman, L. Zhang, W. Hamidouche,
    O. Déforges, and A. Smolic, “Audio-visual perception of omnidirectional video
    for virtual reality applications,” *ICME Workshop*, 2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] F.-Y. Chao, C. Ozcinar, C. Wang, E. Zerman, L. Zhang, W. Hamidouche,
    O. Déforges, 和 A. Smolic, “全景视频在虚拟现实应用中的视听感知，” *ICME Workshop*, 2020。'
- en: '[201] S.-H. Chou, W.-L. Chao, W.-S. Lai, M. Sun, and M.-H. Yang, “Visual question
    answering on 360deg images,” in *WACV*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S.-H. Chou, W.-L. Chao, W.-S. Lai, M. Sun, 和 M.-H. Yang, “在360度图像上的视觉问答，”
    在 *WACV*, 2020。'
- en: '[202] F. Liu, T. Xiang, T. M. Hospedales, W. Yang, and C. Sun, “Inverse visual
    question answering: A new benchmark and vqa diagnosis tool,” *IEEE TPAMI*, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] F. Liu, T. Xiang, T. M. Hospedales, W. Yang, 和 C. Sun, “逆视觉问答：一个新的基准和VQA诊断工具，”
    *IEEE TPAMI*, 2020。'
- en: '[203] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results.” *ICLR*, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Tarvainen 和 H. Valpola, “平均教师是更好的角色模型：权重平均一致性目标改进半监督深度学习结果。” *ICLR*,
    2017。'
- en: '[204] G. D. Pais, T. J. Dias, J. C. Nascimento, and P. Miraldo, “Omnidrl: Robust
    pedestrian detection using deep reinforcement learning on omnidirectional cameras*,”
    *2019 International Conference on Robotics and Automation (ICRA)*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] G. D. Pais, T. J. Dias, J. C. Nascimento, 和 P. Miraldo, “Omnidrl: 使用深度强化学习在全向相机上的鲁棒行人检测*，”
    *2019 International Conference on Robotics and Automation (ICRA)*, 2019。'
- en: '[205] A. Kittel, P. Larkin, I. Cunningham, and M. Spittle, “360 virtual reality:
    A swot analysis in comparison to virtual reality,” *Frontiers in Psychology*,
    2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] A. Kittel, P. Larkin, I. Cunningham, 和 M. Spittle, “360虚拟现实：与虚拟现实的SWOT分析，”
    *Frontiers in Psychology*, 2020。'
- en: '[206] H. Kim, L. Hernaggi, P. J. B. Jackson, and A. Hilton, “Immersive spatial
    audio reproduction for VR/AR using room acoustic modelling from 360^∘ images,”
    in *VR*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] H. Kim, L. Hernaggi, P. J. B. Jackson, 和 A. Hilton, “用于VR/AR的沉浸式空间音频再现，基于360^∘图像的房间声学建模，”
    在 *VR*, 2019。'
- en: '[207] Y. Heshmat, B. Jones, X. Xiong, C. Neustaedter, A. Tang, B. E. Riecke,
    and L. Yang, “Geocaching with a beam: Shared outdoor activities through a telepresence
    robot with 360 degree viewing,” *CHI Conference on Human Factors in Computing
    Systems*, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Heshmat, B. Jones, X. Xiong, C. Neustaedter, A. Tang, B. E. Riecke,
    和 L. Yang, “用光束进行地理寻宝：通过具有360度视角的远程机器人共享户外活动，” *CHI Conference on Human Factors
    in Computing Systems*, 2018。'
- en: '[208] J. Zhang, “A 360^∘ video-based robot platform for telepresent redirected
    walking,” in *VAM-HRI*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] J. Zhang, “一个基于360^∘视频的机器人平台，用于远程重定向行走，” 在 *VAM-HRI*, 2018。'
- en: '[209] G. Pudics, M. Z. Szabo-Resch, and Z. Vámossy, “Safe robot navigation
    using an omnidirectional camera,” *CINTI*, 2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] G. Pudics, M. Z. Szabo-Resch, 和 Z. Vámoossy, “使用全向相机的安全机器人导航，” *CINTI*,
    2015。'
- en: '[210] L. Ran, Y. Zhang, Q. Zhang, and T. Yang, “Convolutional neural network-based
    robot navigation using uncalibrated spherical images †,” *Sensors (Basel, Switzerland)*,
    2017.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] L. Ran, Y. Zhang, Q. Zhang, 和 T. Yang, “基于卷积神经网络的机器人导航，使用未标定的球形图像†，”
    *Sensors (Basel, Switzerland)*, 2017。'
- en: '[211] S. S. Mansouri, P. S. Karvelis, C. Kanellakis, D. Kominiak, and G. Nikolakopoulos,
    “Vision-based mav navigation in underground mine using convolutional neural network,”
    *IECON*, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. S. Mansouri, P. S. Karvelis, C. Kanellakis, D. Kominiak, 和 G. Nikolakopoulos,
    “基于视觉的地下矿井MAV导航，使用卷积神经网络，” *IECON*, 2019。'
- en: '[212] D. Sun, X. Huang, and K. Yang, “A multimodal vision sensor for autonomous
    driving,” in *Counterterrorism, Crime Fighting, Forensics, and Surveillance Technologies
    III*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] D. Sun, X. Huang, 和 K. Yang, “用于自动驾驶的多模态视觉传感器，” 在 *Counterterrorism,
    Crime Fighting, Forensics, and Surveillance Technologies III*, 2019。'
- en: '[213] J. Beltrán, C. Guindel, I. Cortés, A. Barrera, A. Astudillo, J. Urdiales,
    M. Álvarez, F. Bekka, V. Milanés, and F. García, “Towards autonomous driving:
    a multi-modal 360^∘ perception proposal,” in *ITSC*, 2020.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] J. Beltrán, C. Guindel, I. Cortés, A. Barrera, A. Astudillo, J. Urdiales,
    M. Álvarez, F. Bekka, V. Milanés, 和 F. García, “迈向自动驾驶：一种多模态360^∘感知方案，” 在 *ITSC*,
    2020。'
- en: '[214] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *CVPR*, 2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, 和 O. Beijbom, “nuscenes: 一个用于自动驾驶的多模态数据集，” 在 *CVPR*, 2020。'
- en: '[215] X. Zhang, Z. Li, Y. Gong, D. Jin, J. Li, L. Wang, Y. Zhu, and H. Liu,
    “Openmpd: An open multimodal perception dataset for autonomous driving,” *IEEE
    Trans. Veh. Technol.*, 2022.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] X. Zhang, Z. Li, Y. Gong, D. Jin, J. Li, L. Wang, Y. Zhu, 和 H. Liu, “Openmpd:
    一个用于自动驾驶的开放多模态感知数据集，” *IEEE Trans. Veh. Technol.*, 2022。'
- en: '[216] V. R. Kumar, S. Yogamani, H. Rashed, G. Sitsu, C. Witt, I. Leang, S. Milz,
    and P. Mäder, “Omnidet: Surround view cameras based multi-task visual perception
    network for autonomous driving,” *IEEE Robot. Autom. Lett.*, 2021.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] V. R. Kumar, S. Yogamani, H. Rashed, G. Sitsu, C. Witt, I. Leang, S.
    Milz, 和 P. Mäder, “Omnidet: 基于环视摄像头的多任务视觉感知网络，用于自动驾驶，” *IEEE Robot. Autom. Lett.*,
    2021。'
- en: '[217] Z. Yan, X. Li, K. Wang, Z. Zhang, J. Li, and J. Yang, “Multi-modal masked
    pre-training for monocular panoramic depth completion,” *arXiv*, 2022.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Z. Yan, X. Li, K. Wang, Z. Zhang, J. Li, 和 J. Yang, “用于单目全景深度完成的多模态掩蔽预训练，”
    *arXiv*, 2022。'
- en: '[218] J. Li, H. Li, and Y. Matsushita, “Lighting, reflectance and geometry
    estimation from 360^∘ panoramic stereo,” *CVPR*, 2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] J. Li, H. Li, 和 Y. Matsushita, “从360^∘全景立体图像中估计光照、反射率和几何信息，” *CVPR*,
    2021。'
- en: '[219] Y. Zhang, Y. Liu, J. Liu, P. Zhan, L. Wang, and Z. Xu, “Sp attack: Single-perspective
    attack for generating adversarial omnidirectional images,” in *ICASSP*, 2022.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Y. Zhang, Y. Liu, J. Liu, P. Zhan, L. Wang, 和 Z. Xu, “Sp attack: 单视角攻击生成对抗全向图像，”
    在 *ICASSP*, 2022。'
- en: '[220] S. Wang, W. Zeng, X. Chen, Y. Ye, Y. Qiao, and C.-W. Fu, “Actfloor-gan:
    Activity-guided adversarial networks for human-centric floorplan design,” *TVCG*,
    2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] S. Wang, W. Zeng, X. Chen, Y. Ye, Y. Qiao, and C.-W. Fu, “Actfloor-gan:
    Activity-guided adversarial networks for human-centric floorplan design,” *TVCG*,
    2021.'
- en: '| ![[Uncaptioned image]](img/772c6b488f1ee1299002d58638841609.png) | Hao Ai
    is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). His research interests include pattern recognition (image
    classification, face recognition, etc.), DL (especially uncertainty learning,
    attention, transfer learning, semi- /self-unsupervised learning), omnidirectional
    vision. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/772c6b488f1ee1299002d58638841609.png) | **艾浩**是香港科技大学（HKUST）广州校区人工智能前沿视觉学习与智能系统实验室的博士生。他的研究兴趣包括模式识别（图像分类、面部识别等）、深度学习（尤其是不确定性学习、注意力机制、迁移学习、半监督/自监督学习）和全景视觉。
    |'
- en: '| ![[Uncaptioned image]](img/74eeddb266fd6648c3f1148a5e7d865e.png) | Zidong
    Cao is a research assistant in the Visual Learning and Intelligent Systems Lab,
    Artificial Intelligence Thrust, Guangzhou Campus, The Hong Kong University of
    Science and Technology (HKUST). His research interests include 3D vision (depth
    completion, depth estimation, etc.), DL (self-supervised learning, weakly-supervised
    learning, etc.) and omnidirectional vision. |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/74eeddb266fd6648c3f1148a5e7d865e.png) | **曹子栋**是香港科技大学（HKUST）广州校区人工智能前沿视觉学习与智能系统实验室的研究助理。他的研究兴趣包括3D视觉（深度补全、深度估计等）、深度学习（自监督学习、弱监督学习等）和全景视觉。
    |'
- en: '| ![[Uncaptioned image]](img/4dbb154db3ce6200aad6d269ae0fb4c8.png) | JinJing
    Zhu is a Ph.D. student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). His research interests include CV (image classification, person
    re-identification, action recognition, etc.), DL (especially transfer learning,
    knowledge distillation, multi-task learning, semi-/self-unsupervised learning,
    etc.), omnidirectional vision, and event-based vision. |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/4dbb154db3ce6200aad6d269ae0fb4c8.png) | **朱金晶**是香港科技大学（HKUST）广州校区人工智能前沿视觉学习与智能系统实验室的博士生。他的研究兴趣包括计算机视觉（图像分类、人员再识别、动作识别等）、深度学习（尤其是迁移学习、知识蒸馏、多任务学习、半监督/自监督学习等）、全景视觉和基于事件的视觉。
    |'
- en: '| ![[Uncaptioned image]](img/c866d5a65fa037d59cea97b45f2bdd22.png) | Haotian
    Bai is a research assistant at Visual Learning and Intelligent Systems Lab, AI
    Thrust, Information Hub, Guangzhou Campus, The Hong Kong University of Science
    and Technology (HKUST). His research interests include 3D reconstruction (e.g.,
    human and room), CV(especially attention, unsupervised/weakly supervised learning,
    and domain adaptation), and causal inference. |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/c866d5a65fa037d59cea97b45f2bdd22.png) | **白浩天**是香港科技大学（HKUST）广州校区人工智能前沿视觉学习与智能系统实验室的研究助理。他的研究兴趣包括3D重建（如人类和房间）、计算机视觉（尤其是注意力机制、无监督/弱监督学习和领域适应）以及因果推断。
    |'
- en: '| ![[Uncaptioned image]](img/8ec1f5329f39c1d56937428e3890663e.png) | Yucheng
    Chen is a Mphil student in the Visual Learning and Intelligent Systems Lab, Artificial
    Intelligence Thrust, Guangzhou Campus, The Hong Kong University of Science and
    Technology (HKUST). Her research interests include low-level vision (neural rendering,
    reflection removal, etc.), DL (especially domain adaptation, transfer learning,
    semi- /self-unsupervised learning), omnidirectional vision. |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8ec1f5329f39c1d56937428e3890663e.png) | **陈宇成**是香港科技大学（HKUST）广州校区人工智能前沿视觉学习与智能系统实验室的硕士研究生。她的研究兴趣包括低级视觉（神经渲染、反射去除等）、深度学习（尤其是领域适应、迁移学习、半监督/自监督学习）和全景视觉。
    |'
- en: '| ![[Uncaptioned image]](img/17c3d1c8c58af3fb77c625c6cdbe74c3.png) | Lin Wang
    is an assistant professor in the Artificial Intelligence Thrust, HKUST GZ Campus,
    HKUST Fok Ying Tung Research Institute, and an affiliate assistant professor in
    the Dept. of CSE, HKUST, CWB Campus. He got his PhD degree with honor from Korea
    Advanced Institute of Science and Technology (KAIST). His research interests include
    computer vision/graphics, machine learning and human-AI collaboration. |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/17c3d1c8c58af3fb77c625c6cdbe74c3.png) | 林旺是香港科技大学广州校区人工智能研究中心的助理教授，也是香港科技大学中环校区计算机科学与工程系的附属助理教授。他获得了韩国先进科技学院（KAIST）的荣誉博士学位。他的研究兴趣包括计算机视觉/图形学、机器学习和人类-人工智能协作。
    |'
- en: \foreach\x
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: \foreach\x
- en: in 1,…,0 See pages \x of [supplement.pdf](supplement.pdf)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1,…,0 请参见 [supplement.pdf](supplement.pdf) 的第 \x 页
