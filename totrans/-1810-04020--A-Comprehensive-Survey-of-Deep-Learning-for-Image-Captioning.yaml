- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:07:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1810.04020] A Comprehensive Survey of Deep Learning for Image Captioning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1810.04020] 深度学习图像标注的综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1810.04020](https://ar5iv.labs.arxiv.org/html/1810.04020)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1810.04020](https://ar5iv.labs.arxiv.org/html/1810.04020)
- en: A Comprehensive Survey of Deep Learning for Image Captioning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习图像标注的综合调查
- en: Md. Zakir Hossain [0000-0003-1212-4652](https://orcid.org/0000-0003-1212-4652
    "ORCID identifier") Murdoch UniversityAustralia [MdZakir.Hossain@murdoch.edu.au](mailto:MdZakir.Hossain@murdoch.edu.au)
    ,  Ferdous Sohel Murdoch UniversityAustralia [F.Sohel@murdoch.edu.au](mailto:F.Sohel@murdoch.edu.au)
    ,  Mohd Fairuz Shiratuddin Murdoch UniversityAustralia [f.shiratuddin@murdoch.edu.au](mailto:f.shiratuddin@murdoch.edu.au)
     and  Hamid Laga Murdoch UniversityAustralia [H.Laga@murdoch.edu.au](mailto:H.Laga@murdoch.edu.au)(2018;
    April 2018; October 2018; October 2018)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Md. Zakir Hossain [0000-0003-1212-4652](https://orcid.org/0000-0003-1212-4652
    "ORCID identifier") 默多克大学澳大利亚 [MdZakir.Hossain@murdoch.edu.au](mailto:MdZakir.Hossain@murdoch.edu.au)
    ，Ferdous Sohel 默多克大学澳大利亚 [F.Sohel@murdoch.edu.au](mailto:F.Sohel@murdoch.edu.au)
    ，Mohd Fairuz Shiratuddin 默多克大学澳大利亚 [f.shiratuddin@murdoch.edu.au](mailto:f.shiratuddin@murdoch.edu.au)
    和 Hamid Laga 默多克大学澳大利亚 [H.Laga@murdoch.edu.au](mailto:H.Laga@murdoch.edu.au)(2018年；2018年4月；2018年10月；2018年10月)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Generating a description of an image is called image captioning. Image captioning
    requires to recognize the important objects, their attributes and their relationships
    in an image. It also needs to generate syntactically and semantically correct
    sentences. Deep learning-based techniques are capable of handling the complexities
    and challenges of image captioning. In this survey paper, we aim to present a
    comprehensive review of existing deep learning-based image captioning techniques.
    We discuss the foundation of the techniques to analyze their performances, strengths
    and limitations. We also discuss the datasets and the evaluation metrics popularly
    used in deep learning based automatic image captioning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像描述称为图像标注。图像标注需要识别图像中的重要对象、它们的属性及其关系。它还需要生成语法和语义上正确的句子。基于深度学习的技术能够处理图像标注的复杂性和挑战。在这篇综述论文中，我们旨在全面回顾现有的基于深度学习的图像标注技术。我们讨论了这些技术的基础，以分析它们的性能、优点和局限性。我们还讨论了在深度学习自动图像标注中常用的数据集和评估指标。
- en: 'Image Captioning, Deep Learning, Computer Vision, Natural Language Processing,
    CNN, LSTM.^†^†journal: CSUR^†^†journalvolume: 0^†^†journalnumber: 0^†^†article:
    0^†^†copyright: acmcopyright^†^†doi: 0000001.0000001^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: Computing methodologies Supervised learning^†^†ccs: Computing
    methodologies Unsupervised learning^†^†ccs: Computing methodologies Reinforcement
    learning^†^†ccs: Computing methodologies Neural networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标注，深度学习，计算机视觉，自然语言处理，CNN，LSTM。^†^†期刊：CSUR^†^†期刊卷号：0^†^†期刊号：0^†^†文章：0^†^†版权：acmcopyright^†^†doi：0000001.0000001^†^†ccs：计算方法
    机器学习^†^†ccs：计算方法 监督学习^†^†ccs：计算方法 无监督学习^†^†ccs：计算方法 强化学习^†^†ccs：计算方法 神经网络
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Every day, we encounter a large number of images from various sources such as
    the internet, news articles, document diagrams and advertisements. These sources
    contain images that viewers would have to interpret themselves. Most images do
    not have a description, but the human can largely understand them without their
    detailed captions. However, machine needs to interpret some form of image captions
    if humans need automatic image captions from it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，我们会遇到大量来自不同来源的图像，如互联网、新闻文章、文档图表和广告。这些来源包含的图像需要观众自行解读。大多数图像没有描述，但人类可以在没有详细说明的情况下理解它们。然而，如果人类需要自动图像描述，机器需要以某种形式解读图像说明。
- en: Image captioning is important for many reasons. For example, they can be used
    for automatic image indexing. Image indexing is important for Content-Based Image
    Retrieval (CBIR) and therefore, it can be applied to many areas, including biomedicine,
    commerce, the military, education, digital libraries, and web searching. Social
    media platforms such as Facebook and Twitter can directly generate descriptions
    from images. The descriptions can include where we are (e.g., beach, cafe), what
    we wear and importantly what we are doing there.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述具有许多重要的应用。例如，它们可以用于自动图像索引。图像索引对于基于内容的图像检索（Content-Based Image Retrieval，CBIR）很重要，因此它可以应用于许多领域，包括生物医学、商业、军事、教育、数字图书馆和网络搜索。社交媒体平台，如Facebook和Twitter，可以直接从图像生成描述。这些描述可以包括我们所在的位置（例如，海滩、咖啡馆）、我们穿的衣物以及我们在那里的活动。
- en: Image captioning is a popular research area of Artificial Intelligence (AI)
    that deals with image understanding and a language description for that image.
    Image understanding needs to detect and recognize objects. It also needs to understand
    scene type or location, object properties and their interactions. Generating well-formed
    sentences requires both syntactic and semantic understanding of the language (Vinyals
    et al., [2017](#bib.bib144)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述是人工智能（Artificial Intelligence，AI）中的一个热门研究领域，涉及图像理解和对图像的语言描述。图像理解需要检测和识别物体。它还需要理解场景类型或位置、物体属性及其相互作用。生成结构良好的句子需要对语言的语法和语义有理解（Vinyals
    等，[2017](#bib.bib144)）。
- en: 'Understanding an image largely depends on obtaining image features. The techniques
    used for this purpose can be broadly divided into two categories: (1) Traditional
    machine learning based techniques and (2) Deep machine learning based techniques.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解图像在很大程度上依赖于获取图像特征。用于此目的的技术可以大致分为两类：（1）传统机器学习技术和（2）深度机器学习技术。
- en: In traditional machine learning, hand crafted features such as Local Binary
    Patterns (LBP) (Ojala et al., [2000](#bib.bib108)), Scale-Invariant Feature Transform
    (SIFT) (Lowe, [2004](#bib.bib88)), the Histogram of Oriented Gradients (HOG) (Dalal
    and Triggs, [2005](#bib.bib28)), and a combination of such features are widely
    used. In these techniques, features are extracted from input data. They are then
    passed to a classifier such as Support Vector Machines (SVM) (Boser et al., [1992](#bib.bib18))
    in order to classify an object. Since hand crafted features are task specific,
    extracting features from a large and diverse set of data is not feasible. Moreover,
    real world data such as images and video are complex and have different semantic
    interpretations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习中，手工设计的特征，如局部二值模式（Local Binary Patterns，LBP）（Ojala 等，[2000](#bib.bib108)）、尺度不变特征变换（Scale-Invariant
    Feature Transform，SIFT）（Lowe，[2004](#bib.bib88)）、方向梯度直方图（Histogram of Oriented
    Gradients，HOG）（Dalal 和 Triggs，[2005](#bib.bib28)）以及这些特征的组合被广泛使用。在这些技术中，特征是从输入数据中提取出来的。然后，这些特征会传递给分类器，如支持向量机（Support
    Vector Machines，SVM）（Boser 等，[1992](#bib.bib18)），以对物体进行分类。由于手工设计的特征是任务特定的，因此从大量和多样化的数据集中提取特征是不切实际的。此外，现实世界的数据，如图像和视频，是复杂的，并且具有不同的语义解释。
- en: On the other hand, in deep machine learning based techniques, features are learned
    automatically from training data and they can handle a large and diverse set of
    images and videos. For example, Convolutional Neural Networks (CNN) (LeCun et al.,
    [1998](#bib.bib80)) are widely used for feature learning, and a classifier such
    as Softmax is used for classification. CNN is generally followed by Recurrent
    Neural Networks (RNN) in order to generate captions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在深度机器学习技术中，特征是从训练数据中自动学习的，并且它们可以处理大量和多样化的图像和视频。例如，卷积神经网络（Convolutional
    Neural Networks，CNN）（LeCun 等，[1998](#bib.bib80)）被广泛用于特征学习，分类器如Softmax用于分类。CNN通常会后接递归神经网络（Recurrent
    Neural Networks，RNN）以生成描述。
- en: In the last 5 years, a large number of articles have been published on image
    captioning with deep machine learning being popularly used. Deep learning algorithms
    can handle complexities and challenges of image captioning quite well. So far,
    only three survey papers (Bernardi et al., [2016](#bib.bib14); Kumar and Goel,
    [2017](#bib.bib76); Bai and An, [2018](#bib.bib9)) have been published on this
    research topic. Although the papers have presented a good literature survey of
    image captioning, they could only cover a few papers on deep learning because
    the bulk of them was published after the survey papers. These survey papers mainly
    discussed template based, retrieval based, and a very few deep learning-based
    novel image caption generating models. However, a large number of works have been
    done on deep learning-based image captioning. Moreover, the availability of large
    and new datasets has made the learning-based image captioning an interesting research
    area. To provide an abridged version of the literature, we present a survey mainly
    focusing on the deep learning-based papers on image captioning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的5年里，大量的文章使用深度机器学习进行图像字幕生成已经被出版。深度学习算法能够很好地处理图像字幕生成中的复杂性和挑战。到目前为止，关于这一研究主题只有三篇调查论文（Bernardi等，[2016](#bib.bib14);
    Kumar和Goel，[2017](#bib.bib76); Bai和An，[2018](#bib.bib9)）被发表。尽管这些论文对图像字幕生成进行了良好的文献调研，但由于大部分文章是在调查论文发表后出版的，因此只能覆盖很少的深度学习相关的文章。这些调查论文主要讨论了基于模板、基于检索和少量基于深度学习的新颖图像字幕生成模型。然而，已经有大量的工作是基于深度学习的图像字幕生成。此外，大量和新的数据集的可用性使得基于学习的图像字幕生成成为一个有趣的研究领域。为了提供文献的简化版本，我们主要关注基于深度学习的图像字幕生成论文的调查。
- en: 'The main aim of this paper is to provide a comprehensive survey of deep learning
    for image captioning. First, we group the existing image captioning articles into
    three main categories: (1) Template-based Image captioning, (2) Retrieval-based
    image captioning, and (3) Novel image caption generation. The categories are discussed
    briefly in Section 2\. Most deep learning based image captioning methods fall
    into the category of novel caption generation. Therefore, we focus only on novel
    caption generation with deep learning. Second, we group the deep learning-based
    image captioning methods into different categories namely (1) Visual space-based,
    (2) Multimodal space-based, (3) Supervised learning, (4) Other deep learning,
    (5) Dense captioning, (6) Whole scene-based, (7) Encoder-Decoder Architecture-based,
    (8) Compositional Architecture-based, (9) LSTM (Long Short-Term Memory) (Hochreiter
    and Schmidhuber, [1997](#bib.bib55)) language model-based, (10) Others language
    model-based, (11) Attention-Based, (12) Semantic concept-based, (13) Stylized
    captions, and (12) Novel object-based image captioning. We discuss all the categories
    in Section 3\. We provide an overview of the datasets and commonly used evaluation
    metrics for measuring the quality of image captions in Section 4\. We also discuss
    and compare the results of different methods in Section 5\. Finally, we give a
    brief discussion and future research directions in Section 6 and then a conclusion
    in Section 7.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要目标是提供深度学习在图像字幕生成中的全面调查。首先，我们将现有的图像字幕生成文章分为三类：(1) 基于模板的图像字幕生成，(2) 基于检索的图像字幕生成，以及(3)
    新颖的图像字幕生成。这些类别将在第2节中简要讨论。大多数基于深度学习的图像字幕生成方法属于新颖字幕生成的类别。因此，我们仅关注深度学习的新颖字幕生成。其次，我们将深度学习的图像字幕生成方法分为不同的类别，包括：(1)
    基于视觉空间的，(2) 多模态空间的，(3) 监督学习的，(4) 其他深度学习的，(5) 密集字幕生成，(6) 整体场景基础的，(7) 编码器-解码器架构基础的，(8)
    组合架构基础的，(9) LSTM（长短期记忆）语言模型基础的，(10) 其他语言模型基础的，(11) 基于注意力的，(12) 语义概念基础的，(13) 风格化字幕，以及(12)
    新颖物体基础的图像字幕生成。我们在第3节讨论所有这些类别。在第4节中，我们概述数据集和常用的评估指标，用于衡量图像字幕的质量。我们还在第5节中讨论和比较不同方法的结果。最后，在第6节中，我们简要讨论未来的研究方向，并在第7节中做出结论。
- en: '![Refer to caption](img/654017c459be4fe8ef4cb6fb02bac3a1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/654017c459be4fe8ef4cb6fb02bac3a1.png)'
- en: Figure 1\. An overall taxonomy of deep learning-based image captioning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 1\. 深度学习基础的图像字幕的总体分类。
- en: 2\. Image Captioning Methods
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 图像字幕方法
- en: In this section, we review and describe the main categories of existing image
    captioning methods and they include template-based image captioning, retrieval-based
    image captioning, and novel caption generation. Template-based approaches have
    fixed templates with a number of blank slots to generate captions. In these approaches,
    different objects, attributes, actions are detected first and then the blank spaces
    in the templates are filled. For example, Farhadi et al. (Farhadi et al., [2010](#bib.bib35))
    use a triplet of scene elements to fill the template slots for generating image
    captions. Li et al. (Li et al., [2011](#bib.bib81)) extract the phrases related
    to detected objects, attributes and their relationships for this purpose. A Conditional
    Random Field (CRF) is adopted by Kulkarni et al. (Kulkarni et al., [2011](#bib.bib75))
    to infer the objects, attributes, and prepositions before filling in the gaps.
    Template-based methods can generate grammatically correct captions. However, templates
    are predefined and cannot generate variable-length captions. Moreover, later on,
    parsing based language models have been introduced in image captioning (Aker and
    Gaizauskas, [2010](#bib.bib3); Elliott and Keller, [2013](#bib.bib33); Kuznetsova
    et al., [2012](#bib.bib77), [2014](#bib.bib78); Mitchell et al., [2012](#bib.bib102))
    which are more powerful than fixed template-based methods. Therefore, in this
    paper, we do not focus on these template based methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾并描述了现有图像标题生成方法的主要类别，包括基于模板的图像标题生成、基于检索的图像标题生成和新颖标题生成。基于模板的方法具有固定的模板，并有若干空白插槽来生成标题。在这些方法中，首先检测不同的对象、属性和动作，然后填充模板中的空白。例如，Farhadi
    et al.（Farhadi et al., [2010](#bib.bib35)）使用场景元素的三元组来填充模板槽以生成图像标题。Li et al.（Li
    et al., [2011](#bib.bib81)）提取与检测到的对象、属性及其关系相关的短语。Kulkarni et al.（Kulkarni et al.,
    [2011](#bib.bib75)）采用条件随机场（CRF）来推断对象、属性和介词，然后填补空白。基于模板的方法可以生成语法正确的标题。然而，模板是预定义的，不能生成可变长度的标题。此外，之后引入了基于解析的语言模型用于图像标题生成（Aker
    and Gaizauskas, [2010](#bib.bib3); Elliott and Keller, [2013](#bib.bib33); Kuznetsova
    et al., [2012](#bib.bib77), [2014](#bib.bib78); Mitchell et al., [2012](#bib.bib102)），这些模型比固定模板的方法更强大。因此，在本文中，我们不重点关注这些基于模板的方法。
- en: Captions can be retrieved from visual space and multimodal space. In retrieval-based
    approaches, captions are retrieved from a set of existing captions. Retrieval
    based methods first find the visually similar images with their captions from
    the training data set. These captions are called candidate captions. The captions
    for the query image are selected from these captions pool (Ordonez et al., [2011](#bib.bib109);
    Hodosh et al., [2013](#bib.bib56); Sun et al., [2015](#bib.bib131); Gong et al.,
    [2014](#bib.bib48)). These methods produce general and syntactically correct captions.
    However, they cannot generate image specific and semantically correct captions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标题可以从视觉空间和多模态空间中检索。在基于检索的方法中，标题是从一组现有的标题中检索出来的。检索方法首先从训练数据集中找到与其标题视觉上相似的图像。这些标题被称为候选标题。查询图像的标题从这些标题池中选择（Ordonez
    et al., [2011](#bib.bib109); Hodosh et al., [2013](#bib.bib56); Sun et al., [2015](#bib.bib131);
    Gong et al., [2014](#bib.bib48)）。这些方法生成的是一般性和语法正确的标题。然而，它们无法生成特定于图像且语义准确的标题。
- en: Novel captions can be generated from both visual space and multimodal space.
    A general approach of this category is to analyze the visual content of the image
    first and then generate image captions from the visual content using a language
    model (Kiros et al., [2014b](#bib.bib71); Xu et al., [2015](#bib.bib153); Yao
    et al., [2017b](#bib.bib156); You et al., [2016](#bib.bib157)). These methods
    can generate new captions for each image that are semantically more accurate than
    previous approaches. Most novel caption generation methods use deep machine learning
    based techniques. Therefore, deep learning based novel image caption generating
    methods are our main focus in this literature.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖的标题可以从视觉空间和多模态空间中生成。这类方法的一般思路是首先分析图像的视觉内容，然后使用语言模型从视觉内容生成图像标题（Kiros et al.,
    [2014b](#bib.bib71); Xu et al., [2015](#bib.bib153); Yao et al., [2017b](#bib.bib156);
    You et al., [2016](#bib.bib157)）。这些方法可以为每张图像生成语义上比以前的方法更准确的新标题。大多数新颖标题生成方法使用基于深度机器学习的技术。因此，基于深度学习的新颖图像标题生成方法是我们文献中的主要关注点。
- en: 'An overall taxonomy of deep learning-based image captioning methods is depicted
    in Figure 1\. The figure illustrates the comparisons of different categories of
    image captioning methods. Novel caption generation-based image caption methods
    mostly use visual space and deep machine learning based techniques. Captions can
    also be generated from multimodal space. Deep learning-based image captioning
    methods can also be categorized on learning techniques: Supervised learning, Reinforcement
    learning, and Unsupervised learning. We group the reinforcement learning and unsupervised
    learning into Other Deep Learning. Usually captions are generated for a whole
    scene in the image. However, captions can also be generated for different regions
    of an image (Dense captioning). Image captioning methods can use either simple
    Encoder-Decoder architecture or Compositional architecture. There are methods
    that use attention mechanism, semantic concept, and different styles in image
    descriptions. Some methods can also generate description for unseen objects. We
    group them into one category as “Others”. Most of the image captioning methods
    use LSTM as language model. However, there are a number of methods that use other
    language models such as CNN and RNN. Therefore, we include a language model-based
    category as “LSTM vs. Others”.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的图像描述方法的总体分类见图1\. 该图展示了不同类别的图像描述方法的比较。新颖的描述生成图像描述方法大多使用视觉空间和深度机器学习技术。描述也可以从多模态空间生成。基于深度学习的图像描述方法还可以按学习技术分类：监督学习、强化学习和无监督学习。我们将强化学习和无监督学习归入“其他深度学习”类别。通常，描述是针对图像中的整个场景生成的。然而，描述也可以针对图像的不同区域（密集描述）生成。图像描述方法可以使用简单的编码器-解码器架构或组合架构。有些方法使用注意力机制、语义概念和不同风格的图像描述。有些方法还可以生成对未见物体的描述。我们将它们归为一个“其他”类别。大多数图像描述方法使用LSTM作为语言模型。然而，也有一些方法使用其他语言模型，如CNN和RNN。因此，我们包括了一个基于语言模型的“LSTM与其他”类别。
- en: 3\. Deep Learning Based Image Captioning Methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于深度学习的图像描述方法
- en: We draw an overall taxonomy in Figure 1 for deep learning-based image captioning
    methods. We discuss their similarities and dissimilarities by grouping them into
    visual space vs. multimodal space, dense captioning vs. captions for the whole
    scene, Supervised learning vs. Other deep learning, Encoder-Decoder architecture
    vs. Compositional architecture, and one ‘Others’ group that contains Attention-Based,
    Semantic Concept-Based, Stylized captions, and Novel Object-Based captioning.
    We also create a category named LSTM vs. Others.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图1中绘制了基于深度学习的图像描述方法的总体分类。我们通过将它们分为视觉空间与多模态空间、密集描述与整个场景的描述、监督学习与其他深度学习、编码器-解码器架构与组合架构，以及包含注意力机制、语义概念、风格化描述和新颖物体描述的“其他”组来讨论它们的相似性和差异性。我们还创建了一个名为LSTM与其他的类别。
- en: A brief overview of the deep learning-based image captioning methods is shown
    in Table 1\. Table 1 contains the name of the image captioning methods, the type
    of deep neural networks used to encode image information, and the language models
    used in describing the information. In the final column, we give a category label
    to each captioning technique based on the taxonomy in Figure 1.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习图像描述方法的简要概述见表1\. 表1包含了图像描述方法的名称、用于编码图像信息的深度神经网络类型，以及用于描述信息的语言模型。在最后一列中，我们根据图1中的分类法对每种描述技术进行分类标签。
- en: 3.1\. Visual Space vs. Multimodal Space
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 视觉空间与多模态空间
- en: Deep learning-based image captioning methods can generate captions from both
    visual space and multimodal space. Understandably image captioning datasets have
    the corresponding captions as text. In the visual space-based methods, the image
    features and the corresponding captions are independently passed to the language
    decoder. In contrast, in a multimodal space case, a shared multimodal space is
    learned from the images and the corresponding caption-text. This multimodal representation
    is then passed to the language decoder.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的图像描述方法可以从视觉空间和多模态空间生成描述。显然，图像描述数据集有相应的文本描述。在基于视觉空间的方法中，图像特征和相应的描述被独立传递给语言解码器。相反，在多模态空间的情况下，从图像和相应的描述文本中学习到一个共享的多模态空间。这种多模态表示随后被传递给语言解码器。
- en: 3.1.1\. Visual Space
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 视觉空间
- en: Bulk of the image captioning methods use visual space for generating captions.
    These methods are discussed in Section 3.2 to Section 3.5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数图像字幕生成方法使用视觉空间来生成字幕。这些方法将在第 3.2 节至第 3.5 节中讨论。
- en: '![Refer to caption](img/484e6a6c725e9892636a1de9f08a2a6f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/484e6a6c725e9892636a1de9f08a2a6f.png)'
- en: Figure 2\. A block diagram of multimodal space-based image captioning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 多模态空间图像字幕生成的框图。
- en: 3.1.2\. Multimodal Space
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 多模态空间
- en: The architecture of a typical multimodal space-based method contains a language
    Encoder part, a vision part, a multimodal space part, and a language decoder part.
    A general diagram of multimodal space-based image captioning methods is shown
    in Figure 2\. The vision part uses a deep convolutional neural network as a feature
    extractor to extract the image features. The language encoder part extracts the
    word features and learns a dense feature embedding for each word. It then forwards
    the semantic temporal context to the recurrent layers. The multimodal space part
    maps the image features into a common space with the word features. The resulting
    map is then passed to the language decoder which generates captions by decoding
    the map.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的多模态空间方法的架构包含语言编码器部分、视觉部分、多模态空间部分和语言解码器部分。图像字幕生成方法的一般示意图如图 2 所示。视觉部分使用深度卷积神经网络作为特征提取器提取图像特征。语言编码器部分提取单词特征，并为每个单词学习一个密集特征嵌入。然后将语义时间上下文传递给递归层。多模态空间部分将图像特征映射到与单词特征共享的空间中。生成的映射随后传递给语言解码器，该解码器通过解码映射生成字幕。
- en: 'The methods in this category follow the following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别中的方法遵循以下步骤：
- en: (1)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Deep neural networks and multimodal neural language model are used to learn
    both image and text jointly in a multimodal space.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度神经网络和多模态神经语言模型用于在多模态空间中共同学习图像和文本。
- en: (2)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The language generation part generates captions using the information from Step
    1 .
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言生成部分使用第 1 步中的信息生成字幕。
- en: '| \multirow2*Reference | \multirow2*Image Encoder | \multirow2*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| \multirow2*参考文献 | \multirow2*图像编码器 | \multirow2*'
- en: '&#124; Language Model &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语言模型 &#124;'
- en: '| \multirow2*Category |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| \multirow2*类别 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Kiros et al. 2014 (Kiros et al., [2014a](#bib.bib70)) | AlexNet | LBL | MS,SL,WS,EDA
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Kiros et al. 2014 (Kiros et al., [2014a](#bib.bib70)) | AlexNet | LBL | MS,SL,WS,EDA
    |'
- en: '| Kiros et al. 2014 (Kiros et al., [2014b](#bib.bib71)) | AlexNet, VGGNet |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Kiros et al. 2014 (Kiros et al., [2014b](#bib.bib71)) | AlexNet, VGGNet |'
- en: '&#124; 1\. LSTM &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. LSTM &#124;'
- en: '&#124; 2\. SC-NLM &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. SC-NLM &#124;'
- en: '| MS,SL,WS,EDA |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MS,SL,WS,EDA |'
- en: '| Mao et al. 2014 (Mao et al., [2014](#bib.bib96)) | AlexNet | RNN | MS,SL,WS
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Mao et al. 2014 (Mao et al., [2014](#bib.bib96)) | AlexNet | RNN | MS,SL,WS
    |'
- en: '| Karpathy et al. 2014 (Karpathy et al., [2014](#bib.bib67)) | AlexNet | DTR
    | MS,SL,WS,EDA |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Karpathy et al. 2014 (Karpathy et al., [2014](#bib.bib67)) | AlexNet | DTR
    | MS,SL,WS,EDA |'
- en: '| Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) | AlexNet, VGGNet | RNN
    | MS,SL,WS |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) | AlexNet, VGGNet | RNN
    | MS,SL,WS |'
- en: '| Chen et al. 2015 (Chen and Lawrence Zitnick, [2015](#bib.bib24)) | VGGNet
    | RNN | VS,SL,WS,EDA |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. 2015 (Chen and Lawrence Zitnick, [2015](#bib.bib24)) | VGGNet
    | RNN | VS,SL,WS,EDA |'
- en: '| Fang et al. 2015 (Fang et al., [2015](#bib.bib34)) | AlexNet, VGGNet | MELM
    | VS,SL,WS,CA |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Fang et al. 2015 (Fang et al., [2015](#bib.bib34)) | AlexNet, VGGNet | MELM
    | VS,SL,WS,CA |'
- en: '| Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VGGNet | LSTM | VS,SL,WS,EDA
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VGGNet | LSTM | VS,SL,WS,EDA
    |'
- en: '| Karpathy et al. 2015 (Karpathy and Fei-Fei, [2015](#bib.bib66)) | VGGNet
    | RNN | MS,SL,WS,EDA |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Karpathy et al. 2015 (Karpathy and Fei-Fei, [2015](#bib.bib66)) | VGGNet
    | RNN | MS,SL,WS,EDA |'
- en: '| Vinyals et al. 2015 (Vinyals et al., [2015](#bib.bib143)) | GoogLeNet | LSTM
    | VS,SL,WS,EDA |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Vinyals et al. 2015 (Vinyals et al., [2015](#bib.bib143)) | GoogLeNet | LSTM
    | VS,SL,WS,EDA |'
- en: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | AlexNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | AlexNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Jin et al. 2015 (Jin et al., [2015](#bib.bib62)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Jin et al. 2015 (Jin et al., [2015](#bib.bib62)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Wu et al. 2016 (Wu and Cohen, [2016](#bib.bib152)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. 2016 (Wu and Cohen, [2016](#bib.bib152)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Sugano et at. 2016 (Sugano and Bulling, [2016](#bib.bib130)) | VGGNet | LSTM
    | VS,SL,WS,EDA,AB |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Sugano et al. 2016 (Sugano and Bulling, [2016](#bib.bib130)) | VGGNet | LSTM
    | VS,SL,WS,EDA,AB |'
- en: '| Mathews et al. 2016 (Mathews et al., [2016](#bib.bib98)) | GoogLeNet | LSTM
    | VS,SL,WS,EDA,SC |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Mathews et al. 2016 (Mathews et al., [2016](#bib.bib98)) | GoogLeNet | LSTM
    | VS,SL,WS,EDA,SC |'
- en: '| Wang et al. 2016 (Wang et al., [2016b](#bib.bib145)) | AlexNet, VGGNet |
    LSTM | VS,SL,WS,EDA |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. 2016 (Wang et al., [2016b](#bib.bib145)) | AlexNet, VGGNet |
    LSTM | VS,SL,WS,EDA |'
- en: '| Johnson et al. 2016 (Johnson et al., [2016](#bib.bib63)) | VGGNet | LSTM
    | VS,SL,DC,EDA |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Johnson 等人 2016 (Johnson et al., [2016](#bib.bib63)) | VGGNet | LSTM | VS,SL,DC,EDA
    |'
- en: '| Mao et al. 2016 (Mao et al., [2016](#bib.bib93)) | VGGNet | LSTM | VS,SL,WS,EDA
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Mao 等人 2016 (Mao et al., [2016](#bib.bib93)) | VGGNet | LSTM | VS,SL,WS,EDA
    |'
- en: '| Wang et al. 2016 (Wang et al., [2016a](#bib.bib147)) | VGGNet | LSTM | VS,SL,WS,CA
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2016 (Wang et al., [2016a](#bib.bib147)) | VGGNet | LSTM | VS,SL,WS,CA
    |'
- en: '| Tran et al. 2016 (Tran et al., [2016](#bib.bib136)) | ResNet | MELM | VS,SL,WS,CA
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Tran 等人 2016 (Tran et al., [2016](#bib.bib136)) | ResNet | MELM | VS,SL,WS,CA
    |'
- en: '| Ma et al. 2016 (Ma and Han, [2016](#bib.bib91)) | AlexNet | LSTM | VS,SL,WS,CA
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Ma 等人 2016 (Ma and Han, [2016](#bib.bib91)) | AlexNet | LSTM | VS,SL,WS,CA
    |'
- en: '| You et al. 2016 (You et al., [2016](#bib.bib157)) | GoogLeNet | RNN | VS,SL,WS,EDA,SCB
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| You 等人 2016 (You et al., [2016](#bib.bib157)) | GoogLeNet | RNN | VS,SL,WS,EDA,SCB
    |'
- en: '| Yang et al. 2016 (Yang et al., [2016](#bib.bib154)) | VGGNet | LSTM | VS,SL,DC,EDA
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 2016 (Yang et al., [2016](#bib.bib154)) | VGGNet | LSTM | VS,SL,DC,EDA
    |'
- en: '| Anne et al. 2016 (Anne Hendricks et al., [2016](#bib.bib7)) | VGGNet | LSTM
    | VS,SL,WS,CA,NOB |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Anne 等人 2016 (Anne Hendricks et al., [2016](#bib.bib7)) | VGGNet | LSTM |
    VS,SL,WS,CA,NOB |'
- en: '| Yao et al. 2017 (Yao et al., [2017b](#bib.bib156)) | GoogLeNet | LSTM | VS,SL,WS,EDA,SCB
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Yao 等人 2017 (Yao et al., [2017b](#bib.bib156)) | GoogLeNet | LSTM | VS,SL,WS,EDA,SCB
    |'
- en: '| Lu et al. 2017 (Lu et al., [2017](#bib.bib89)) | ResNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等人 2017 (Lu et al., [2017](#bib.bib89)) | ResNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Chen et al. 2017 (Chen et al., [2017b](#bib.bib22)) | VGGNet, ResNet | LSTM
    | VS,SL,WS,EDA,AB |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 2017 (Chen et al., [2017b](#bib.bib22)) | VGGNet, ResNet | LSTM |
    VS,SL,WS,EDA,AB |'
- en: '| Gan et al. 2017 (Gan et al., [2017b](#bib.bib42)) | ResNet | LSTM | VS,SL,WS,CA,SCB
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Gan 等人 2017 (Gan et al., [2017b](#bib.bib42)) | ResNet | LSTM | VS,SL,WS,CA,SCB
    |'
- en: '| Pedersoli et al. 2017 (Pedersoli et al., [2017](#bib.bib113)) | VGGNet |
    RNN | VS,SL,WS,EDA,AB |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Pedersoli 等人 2017 (Pedersoli et al., [2017](#bib.bib113)) | VGGNet | RNN
    | VS,SL,WS,EDA,AB |'
- en: '| Ren et al. 2017 (Ren et al., [2017](#bib.bib120)) | VGGNet | LSTM | VS,ODL,WS,EDA
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Ren 等人 2017 (Ren et al., [2017](#bib.bib120)) | VGGNet | LSTM | VS,ODL,WS,EDA
    |'
- en: '| Park et al. 2017 (Park et al., [2017](#bib.bib112)) | ResNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Park 等人 2017 (Park et al., [2017](#bib.bib112)) | ResNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Wang et al. 2017 (Wang et al., [2017](#bib.bib149)) | ResNet | LSTM | VS,SL,WS,EDA
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2017 (Wang et al., [2017](#bib.bib149)) | ResNet | LSTM | VS,SL,WS,EDA
    |'
- en: '| Tavakoli et al. 2017 (Tavakoli et al., [2017](#bib.bib135)) | VGGNet | LSTM
    | VS,SL,WS,EDA,AB |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Tavakoli 等人 2017 (Tavakoli et al., [2017](#bib.bib135)) | VGGNet | LSTM |
    VS,SL,WS,EDA,AB |'
- en: '| Liu et al. 2017 (Liu et al., [2017a](#bib.bib85)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 2017 (Liu et al., [2017a](#bib.bib85)) | VGGNet | LSTM | VS,SL,WS,EDA,AB
    |'
- en: '| Gan et al. 2017 (Gan et al., [2017a](#bib.bib40)) | ResNet | LSTM | VS,SL,WS,EDA,SC
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Gan 等人 2017 (Gan et al., [2017a](#bib.bib40)) | ResNet | LSTM | VS,SL,WS,EDA,SC
    |'
- en: '| Dai et al. 2017 (Dai et al., [2017](#bib.bib27)) | VGGNet | LSTM | VS,ODL,WS,EDA
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 2017 (Dai et al., [2017](#bib.bib27)) | VGGNet | LSTM | VS,ODL,WS,EDA
    |'
- en: '| Shetty et al. 2017 (Shetty et al., [2017](#bib.bib127)) | GoogLeNet | LSTM
    | VS,ODL,WS,EDA |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Shetty 等人 2017 (Shetty et al., [2017](#bib.bib127)) | GoogLeNet | LSTM |
    VS,ODL,WS,EDA |'
- en: '| Liu et al. 2017 (Liu et al., [2017b](#bib.bib86)) | Inception-V3 | LSTM |
    VS,ODL,WS,EDA |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 2017 (Liu et al., [2017b](#bib.bib86)) | Inception-V3 | LSTM | VS,ODL,WS,EDA
    |'
- en: '| Gu et al. 2017 (Gu et al., [2017](#bib.bib52)) | VGGNet |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Gu 等人 2017 (Gu et al., [2017](#bib.bib52)) | VGGNet |'
- en: '&#124; 1\. Language CNN &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '| 1\. Language CNN |'
- en: '&#124; 2\. LSTM &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '| 2\. LSTM |'
- en: '| VS,SL,WS,EDA |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| VS,SL,WS,EDA |'
- en: '| Yao et al. 2017 (Yao et al., [2017a](#bib.bib155)) | VGGNet | LSTM | VS,SL,WS,CA,NOB
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Yao 等人 2017 (Yao et al., [2017a](#bib.bib155)) | VGGNet | LSTM | VS,SL,WS,CA,NOB
    |'
- en: '| Rennie et al. 2017 (Rennie et al., [2017](#bib.bib121)) | ResNet | LSTM |
    VS,ODL,WS,EDA |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Rennie 等人 2017 (Rennie et al., [2017](#bib.bib121)) | ResNet | LSTM | VS,ODL,WS,EDA
    |'
- en: '| Vsub et al. 2017 (Venugopalan et al., [2017](#bib.bib141)) | VGGNet | LSTM
    | VS,SL,WS,CA,NOB |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Vsub 等人 2017 (Venugopalan et al., [2017](#bib.bib141)) | VGGNet | LSTM |
    VS,SL,WS,CA,NOB |'
- en: '| Zhang et al. 2017  (Zhang et al., [2017](#bib.bib162)) | Inception-V3 | LSTM
    | VS,ODL,WS,EDA |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 2017  (Zhang et al., [2017](#bib.bib162)) | Inception-V3 | LSTM
    | VS,ODL,WS,EDA |'
- en: '| Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VGGNet | LSTM | VS,SL,WS,EDA,SCB
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 2018 (Wu et al., [2018](#bib.bib151)) | VGGNet | LSTM | VS,SL,WS,EDA,SCB
    |'
- en: '| Aneja et al. 2018 (Aneja et al., [2018](#bib.bib6)) | VGGNet | Language CNN
    | VS,SL,WS,EDA |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Aneja 等人 2018 (Aneja et al., [2018](#bib.bib6)) | VGGNet | Language CNN |
    VS,SL,WS,EDA |'
- en: '| Wang et al. 2018 (Wang and Chan, [2018](#bib.bib148)) | VGGNet | Language
    CNN | VS,SL,WS,EDA |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2018 (Wang and Chan, [2018](#bib.bib148)) | VGGNet | Language CNN
    | VS,SL,WS,EDA |'
- en: Table 1\. An overview of the deep learning-based approaches for image captioning
    (VS=Visual Space, MS=Multimodal Space, SL=Supervised Learning, ODL=Other Deep
    Learning, DC=Dense Captioning, WS=Whole Scene, EDA=Encoder-Decoder Architecture,
    CA=Compositional Architecture, AB=Attention-Based, SCB=Semantic Concept-Based,
    NOB=Novel Object-Based, SC=Stylized Caption).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 基于深度学习的图像标题生成方法概述（VS=视觉空间，MS=多模态空间，SL=监督学习，ODL=其他深度学习，DC=密集标题生成，WS=整体场景，EDA=编码器-解码器架构，CA=组合架构，AB=基于注意力的，SCB=基于语义概念的，NOB=基于新颖对象的，SC=风格化标题）。
- en: An initial work in this area proposed by Kiros et al. (Kiros et al., [2014a](#bib.bib70)).
    The method applies a CNN for extracting image features in generating image captions.
    It uses a multimodal space that represents both image and text jointly for multimodal
    representation learning and image caption generation. It also introduces the multimodal
    neural language models such as Modality-Biased Log-Bilinear Model (MLBL-B) and
    the Factored 3-way Log-Bilinear Model (MLBL-F) of (Mnih and Hinton, [2007a](#bib.bib105))
    followed by AlexNet (Krizhevsky et al., [2012](#bib.bib74)). Unlike most previous
    approaches, this method does not rely on any additional templates, structures,
    or constraints. Instead it depends on the high level image features and word representations
    learned from deep neural networks and multimodal neural language models respectively.
    The neural language models have limitations to handle a large amount of data and
    are inefficient to work with long term memory (Jozefowicz et al., [2016](#bib.bib65)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域的初步工作由 Kiros 等人提出（Kiros et al., [2014a](#bib.bib70)）。该方法应用 CNN 提取图像特征以生成图像标题。它使用一个表示图像和文本的多模态空间进行多模态表示学习和图像标题生成。它还引入了多模态神经语言模型，例如
    Modality-Biased Log-Bilinear Model（MLBL-B）和 Factored 3-way Log-Bilinear Model（MLBL-F）（Mnih
    和 Hinton, [2007a](#bib.bib105)），随后是 AlexNet（Krizhevsky et al., [2012](#bib.bib74)）。与大多数以往方法不同，该方法不依赖于任何额外的模板、结构或约束。相反，它依赖于从深度神经网络和多模态神经语言模型中分别学到的高级图像特征和词语表示。神经语言模型在处理大量数据方面存在局限性，并且在长期记忆方面效率较低（Jozefowicz
    et al., [2016](#bib.bib65)）。
- en: Kiros et al. (Kiros et al., [2014a](#bib.bib70)) extended their work in (Kiros
    et al., [2014b](#bib.bib71)) to learn a joint image sentence embedding where LSTM
    is used for sentence encoding and a new neural language model called the structure-content
    neural language model (SC-NLM) is used for image captions generations. The SC-NLM
    has an advantage over existing methods in that it can extricate the structure
    of the sentence to its content produced by the encoder. It also helps them to
    achieve significant improvements in generating realistic image captions over the
    approach proposed by (Kiros et al., [2014a](#bib.bib70))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kiros 等人（Kiros et al., [2014a](#bib.bib70)）在（Kiros et al., [2014b](#bib.bib71)）中扩展了他们的工作，以学习一个联合图像句子嵌入，其中
    LSTM 被用于句子编码，而一种新的神经语言模型称为结构-内容神经语言模型（SC-NLM）则用于图像标题生成。SC-NLM 相较于现有方法的优势在于，它可以将句子的结构从编码器生成的内容中解脱出来。它还帮助他们在生成逼真的图像标题方面取得了显著的改进，相较于（Kiros
    et al., [2014a](#bib.bib70)）提出的方法。
- en: 'Karpathy et al. (Karpathy et al., [2014](#bib.bib67)) proposed a deep, multimodal
    model, embedding of image and natural language data for the task of bidirectional
    images and sentences retrieval. The previous multimodal-based methods use a common,
    embedding space that directly maps images and sentences. However, this method
    works at a finer level and embeds fragments of images and fragments of sentences.
    This method breaks down the images into a number of objects and sentences into
    a dependency tree relations (DTR) (De Marneffe et al., [2006](#bib.bib29)) and
    reasons about their latent, inter-modal alignment. It shows that the method achieves
    significant improvements in the retrieval task compared to other previous methods.
    This method has a few limitations as well. In terms of modelling, the dependency
    tree can model relations easily but they are not always appropriate. For example,
    a single visual entity might be described by a single complex phrase that can
    be split into multiple sentence fragments. The phrase “black and white dog” can
    be formed into two relations (CONJ, black, white) and (AMOD, white, dog). Again,
    for many dependency relations we do not find any clear mapping in the image (For
    example: “each other” cannot be mapped to any object).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Karpathy等人（Karpathy et al., [2014](#bib.bib67)）提出了一种深度的多模态模型，将图像和自然语言数据嵌入到双向图像和句子检索任务中。之前的多模态方法使用一个通用的嵌入空间，直接映射图像和句子。然而，这种方法在更细的层面上工作，将图像的片段和句子的片段进行嵌入。该方法将图像分解为多个对象，将句子分解为依赖树关系（DTR）（De
    Marneffe et al., [2006](#bib.bib29)），并推理它们潜在的跨模态对齐情况。研究表明，与其他先前的方法相比，这种方法在检索任务中取得了显著的改进。该方法也有一些局限性。在建模方面，依赖树可以轻松建模关系，但并不总是适用。例如，一个单一的视觉实体可能会被一个复杂的短语描述，而该短语可以被拆分成多个句子片段。短语“黑白狗”可以形成两个关系（CONJ,
    black, white）和（AMOD, white, dog）。此外，对于许多依赖关系，我们在图像中找不到任何明确的映射（例如：“彼此”无法映射到任何对象）。
- en: 'Mao et al. (Mao et al., [2015b](#bib.bib95)) proposed a multimodal Recurrent
    Neural Network (m-RNN) method for generating novel image captions. This method
    has two sub-networks: a deep recurrent neural network for sentences and a deep
    convolutional network for images. These two sub-networks interact with each other
    in a multimodal layer to form the whole m-RNN model. Both image and fragments
    of sentences are given as input in this method. It calculates the probabilty distribution
    to generate the next word of captions. There are five more layers in this model:
    Two-word embedding layers, a recurrent layer, a multimodal layer and a SoftMax
    layer. Kiros et al. (Kiros et al., [2014a](#bib.bib70)) proposed a method that
    is built on a Log-Bilinear model and used AlexNet to extract visual features.
    This multimodal recurrent neural network method is closely related to the method
    of Kiros et al. (Kiros et al., [2014a](#bib.bib70)). Kiros et al. use a fixed
    length context (i.e. five words), whereas in this method, the temporal context
    is stored in a recurrent architecture, which allows an arbitrary context length.
    The two word embedding layers use one hot vector to generate a dense word representation.
    It encodes both the syntactic and semantic meaning of the words. The semantically
    relevant words can be found by calculating the Euclidean distance between two
    dense word vectors in embedding layers. Most sentence-image multimodal methods
    (Karpathy et al., [2014](#bib.bib67); Frome et al., [2013](#bib.bib39); Socher
    et al., [2014](#bib.bib129); Kiros et al., [2014b](#bib.bib71)) use pre-computed
    word embedding vectors to initialize their model. In contrast, this method randomly
    initializes word embedding layers and learn them from the training data. This
    helps them to generate better image captions than the previous methods. Many image
    captioning methods (Mao et al., [2014](#bib.bib96); Kiros et al., [2014a](#bib.bib70);
    Karpathy et al., [2014](#bib.bib67)) are built on recurrent neural networks at
    the contemporary times. They use a recurrent layer for storing visual information.
    However, (m-RNN) use both image representations and sentence fragments to generate
    captions. It utilizes the capacity of the recurrent layer more efficiently that
    helps to achieve a better performance using a relatively small dimensional recurrent
    layer.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Mao 等人 (Mao et al., [2015b](#bib.bib95)) 提出了一种多模态递归神经网络 (m-RNN) 方法，用于生成新颖的图像标题。该方法包括两个子网络：一个用于句子的深度递归神经网络和一个用于图像的深度卷积网络。这两个子网络在一个多模态层中相互作用，形成整个
    m-RNN 模型。在这种方法中，图像和句子片段都作为输入。它计算概率分布来生成标题的下一个词。该模型还包括五个更多的层次：两个词嵌入层、一个递归层、一个多模态层和一个
    SoftMax 层。Kiros 等人 (Kiros et al., [2014a](#bib.bib70)) 提出了一种基于对数双线性模型的方法，并使用 AlexNet
    提取视觉特征。这种多模态递归神经网络方法与 Kiros 等人的方法 (Kiros et al., [2014a](#bib.bib70)) 密切相关。Kiros
    等人使用固定长度的上下文（即五个词），而在这种方法中，时间上下文存储在递归架构中，允许任意长度的上下文。两个词嵌入层使用独热向量生成密集的词表示。它编码了单词的句法和语义含义。通过计算嵌入层中两个密集词向量之间的欧氏距离可以找到语义相关的词。大多数句子-图像多模态方法
    (Karpathy et al., [2014](#bib.bib67); Frome et al., [2013](#bib.bib39); Socher
    et al., [2014](#bib.bib129); Kiros et al., [2014b](#bib.bib71)) 使用预先计算的词嵌入向量来初始化他们的模型。相反，这种方法随机初始化词嵌入层并从训练数据中学习。这有助于比以前的方法生成更好的图像标题。许多图像字幕方法
    (Mao et al., [2014](#bib.bib96); Kiros et al., [2014a](#bib.bib70); Karpathy et al.,
    [2014](#bib.bib67)) 在当代都建立在递归神经网络上。它们使用递归层存储视觉信息。然而，(m-RNN) 使用图像表示和句子片段生成标题。它更有效地利用递归层的容量，有助于使用相对较小的维度递归层实现更好的性能。
- en: Chen et al. (Chen and Lawrence Zitnick, [2015](#bib.bib24)) proposed another
    multimodal space-based image captioning method. The method can generate novel
    captions from image and restore visual features from the given description. It
    also can describe a bidirectional mapping between images and their captions. Many
    of the existing methods (Hodosh et al., [2013](#bib.bib56); Socher et al., [2014](#bib.bib129);
    Karpathy et al., [2014](#bib.bib67)) use joint embedding to generate image captions.
    However, they do not use reverse projection that can generate visual features
    from captions. On the other hand, this method dynamically updates the visual representations
    of the image from the generated words. It has an additional recurrent visual hidden
    layer with RNN that makes reverse projection.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人（Chen and Lawrence Zitnick, [2015](#bib.bib24)）提出了一种基于多模态空间的图像描述方法。该方法可以从图像生成新颖的描述，并从给定的描述中恢复视觉特征。它还可以描述图像与其描述之间的双向映射。许多现有的方法（Hodosh
    et al., [2013](#bib.bib56)；Socher et al., [2014](#bib.bib129)；Karpathy et al.,
    [2014](#bib.bib67)）使用联合嵌入来生成图像描述。然而，它们不使用能够从描述生成视觉特征的反向投影。另一方面，这种方法动态更新图像的视觉表示，从生成的词语中进行反向投影。它具有一个额外的与RNN的递归视觉隐藏层，实现反向投影。
- en: 3.2\. Supervised Learning vs. Other Deep Learning
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 监督学习 vs. 其他深度学习
- en: In supervised learning, training data come with desired output called label.
    Unsupervised learning, on the other hand, deals with unlabeled data. Generative
    Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib49)) are a type
    of unsupervised learning techniques. Reinforcement learning is another type of
    machine learning approach where the aims of an agent are to discover data and/or
    labels through exploration and a reward signal. A number of image captioning methods
    use reinforcement learning and GAN based approaches. These methods sit in the
    category of “Other Deep Learning”.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，训练数据带有称为标签的期望输出。另一方面，无监督学习处理的是没有标签的数据。生成对抗网络（GANs）（Goodfellow et al.,
    [2014](#bib.bib49)）是一种无监督学习技术。强化学习是另一种机器学习方法，其目标是通过探索和奖励信号来发现数据和/或标签。许多图像描述方法使用强化学习和基于GAN的方法。这些方法属于“其他深度学习”类别。
- en: 3.2.1\. Supervised Learning-Based Image Captioning
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 基于监督学习的图像描述
- en: 'Supervised learning-based networks have successfully been used for many years
    in image classification (Krizhevsky et al., [2012](#bib.bib74); He et al., [2016](#bib.bib54);
    Simonyan and Zisserman, [2015](#bib.bib128); Szegedy et al., [2015](#bib.bib134)),
    object detection (Girshick, [2015](#bib.bib45); Girshick et al., [2014](#bib.bib46);
    Ren et al., [2015a](#bib.bib117)), and attribute learning (Gan et al., [2016](#bib.bib41)).
    This progress makes researchers interested in using them in automatic image captioning
    (Vinyals et al., [2015](#bib.bib143); Mao et al., [2015b](#bib.bib95); Karpathy
    and Fei-Fei, [2015](#bib.bib66); Chen and Lawrence Zitnick, [2015](#bib.bib24)).
    In this paper, we have identified a large number of supervised learning-based
    image captioning methods. We classify them into different categories: (i) Encoder-Decoder
    Architecture, (ii) Compositional Architecture, (iii) Attention-based, (iv) Semantic
    concept-based, (v) Stylized captions, (vi) Novel object-based, and (vii) Dense
    image captioning.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于监督学习的网络多年来在图像分类（Krizhevsky et al., [2012](#bib.bib74)；He et al., [2016](#bib.bib54)；Simonyan
    and Zisserman, [2015](#bib.bib128)；Szegedy et al., [2015](#bib.bib134)）、物体检测（Girshick,
    [2015](#bib.bib45)；Girshick et al., [2014](#bib.bib46)；Ren et al., [2015a](#bib.bib117)）和属性学习（Gan
    et al., [2016](#bib.bib41)）中取得了成功。这一进展使研究者们对将其用于自动图像描述（Vinyals et al., [2015](#bib.bib143)；Mao
    et al., [2015b](#bib.bib95)；Karpathy and Fei-Fei, [2015](#bib.bib66)；Chen and
    Lawrence Zitnick, [2015](#bib.bib24)）产生了兴趣。在本文中，我们已识别出大量基于监督学习的图像描述方法。我们将它们分为不同的类别：（i）编码器-解码器架构，（ii）组合架构，（iii）基于注意力的，（iv）基于语义概念的，（v）风格化描述，（vi）新颖对象基的，以及（vii）密集图像描述。
- en: '![Refer to caption](img/b68ca18c87e9e5c1014130a67524497c.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见描述](img/b68ca18c87e9e5c1014130a67524497c.png)'
- en: Figure 3\. A block diagram of other deep learning-based captioning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 其他基于深度学习的图像描述的框图。
- en: 3.2.2\. Other Deep Learning-Based Image Captioning
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 其他基于深度学习的图像描述
- en: In our day to day life, data are increasing with unlabled data because it is
    often impractical to accurately annotate data. Therefore, recently, researchers
    are focusing more on reinforcement learning and unsupervised learning-based techniques
    for image captioning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们日常生活中，数据中无标签的数据越来越多，因为准确标注数据通常不切实际。因此，最近，研究者们更多地关注于基于强化学习和无监督学习的图像描述技术。
- en: A reinforcement learning agent chooses an action, receives reward values, and
    moves to a new state. The agent attempts to select the action with the expectation
    of having a maximum long-term reward. It needs continuous state and action information,
    to provide the guarantees of a value function. Traditional reinforcement learning
    approaches face a number of limitations such as the lack of guarantees of a value
    function and uncertain state-action information. Policy gradient methods (Sutton
    et al., [2000](#bib.bib133)) are a type of reinforcement learning that can choose
    a specific policy for a specific action using gradient descent and optimization
    techniques. The policy can incorporate domain knowledge for the action that guarantees
    convergence. Thus, policy gradient methods require fewer parameters than value-function
    based approaches.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理选择一个动作，获得奖励值，并移动到一个新的状态。代理试图选择期望具有最大长期奖励的动作。它需要连续的状态和动作信息，以提供价值函数的保证。传统的强化学习方法面临许多限制，例如缺乏价值函数的保证和不确定的状态-动作信息。策略梯度方法（Sutton
    et al., [2000](#bib.bib133)）是一种强化学习方法，可以使用梯度下降和优化技术为特定动作选择特定策略。该策略可以结合领域知识来保证收敛。因此，策略梯度方法比基于价值函数的方法需要更少的参数。
- en: 'Existing deep learning-based image captioning methods use variants of image
    encoders to extract image features. The features are then fed into the neural
    network-based language decoders to generate captions. The methods have two main
    issues: (i) They are trained using maximum likelihood estimation and back-propagation (Ranzato
    et al., [2016](#bib.bib115)) approaches. In this case, the next word is predicted
    given the image and all the previously generated ground-truth words. Therefore,
    the generated captions look-like ground-truth captions. This phenomenon is called
    exposure bias (Bengio et al., [2015](#bib.bib11)) problem. (ii) Evaluation metrics
    at test time are non-differentiable. Ideally sequence models for image captioning
    should be trained to avoid exposure bias and directly optimise metrics for the
    test time. In actor-critic-based reinforcement learning algorithm, critic can
    be used in estimating the expected future reward to train the actor (captioning
    policy network). Reinforcement learning-based image captioning methods sample
    the next token from the model based on the rewards they receive in each state.
    Policy gradient methods in reinforcement learning can optimize the gradient in
    order to predict the cumulative long-term rewards. Therefore, it can solve the
    non-differentiable problem of evaluation metrics.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现有基于深度学习的图像描述方法使用变体的图像编码器来提取图像特征。这些特征随后被输入到基于神经网络的语言解码器中生成描述。这些方法存在两个主要问题：（i）它们使用最大似然估计和反向传播（Ranzato
    et al., [2016](#bib.bib115)）的方法进行训练。在这种情况下，给定图像和所有之前生成的真实词语，预测下一个词。因此，生成的描述看起来像真实描述。这种现象被称为曝光偏差（Bengio
    et al., [2015](#bib.bib11)）问题。（ii）测试时的评估指标是不可微分的。理想情况下，图像描述的序列模型应该被训练以避免曝光偏差，并直接优化测试时的指标。在基于演员-评论家的强化学习算法中，评论家可以用来估计预期的未来奖励以训练演员（描述政策网络）。基于强化学习的图像描述方法根据它们在每个状态下获得的奖励从模型中采样下一个标记。强化学习中的策略梯度方法可以优化梯度，以预测累积的长期奖励。因此，它可以解决评估指标的不可微分问题。
- en: 'The methods in this category follow the following steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本类别的方法遵循以下步骤：
- en: (1)
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: A CNN and RNN based combined network generates captions.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于CNN和RNN的联合网络生成描述。
- en: (2)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Another CNN-RNN based network evaluates the captions and send feedback to the
    first network to generate high quality captions.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个基于CNN-RNN的网络评估描述并将反馈发送给第一个网络，以生成高质量的描述。
- en: A block diagram of a typical method of this category is shown in Figure 3.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本类别的典型方法的框图见图3。
- en: Ren et al. 2017 (Ren et al., [2017](#bib.bib120)) introduced a novel reinforcement
    learning based image captioning method. The architecture of this method has two
    networks that jointly compute the next best word at each time step. The “policy
    network” works as local guidance and helps to predict next word based on the current
    state. The “value network”’ works as global guidance and evaluates the reward
    value considering all the possible extensions of the current state. This mechanism
    is able to adjust the networks in predicting the correct words. Therefore, it
    can generate good captions similar to ground truth captions at the end. It uses
    an actor-critic reinforcement learning model (Konda and Tsitsiklis, [2000](#bib.bib72))
    to train the whole network. Visual semantic embedding (Ren et al., [2015b](#bib.bib118),
    [2016](#bib.bib119)) is used to compute the actual reward value in predicting
    the correct word. It also helps to measure the similarity between images and sentences
    that can evaluate the correctness of generated captions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Ren 等人（Ren et al., [2017](#bib.bib120)）提出了一种新颖的基于强化学习的图像描述生成方法。该方法的架构包含两个网络，这两个网络在每个时间步共同计算下一个最佳词汇。
    “策略网络”作为局部指导，帮助根据当前状态预测下一个词汇。 “价值网络”作为全局指导，评估奖励值，考虑当前状态所有可能的扩展。这一机制能够调整网络以预测正确的词汇。因此，最终能够生成类似于真实标签的良好描述。该方法使用了一个演员-评论家强化学习模型（Konda
    和 Tsitsiklis, [2000](#bib.bib72)）来训练整个网络。视觉语义嵌入（Ren et al., [2015b](#bib.bib118),
    [2016](#bib.bib119)）用于计算预测正确词汇的实际奖励值。它还帮助衡量图像和句子之间的相似性，从而评估生成描述的正确性。
- en: Rennie et al. (Rennie et al., [2017](#bib.bib121)) proposed another reinforcement
    learning based image captioning method. The method utilizes the test-time inference
    algorithm to normalize the reward rather than estimating the reward signal and
    normalization in training time. It shows that this test-time decoding is highly
    effective for generating quality image captions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Rennie 等人（Rennie et al., [2017](#bib.bib121)）提出了另一种基于强化学习的图像描述生成方法。该方法利用测试时推断算法来规范化奖励，而不是在训练时估计奖励信号和规范化。结果表明，这种测试时解码对于生成高质量的图像描述非常有效。
- en: Zhang et al. (Zhang et al., [2017](#bib.bib162)) proposed an actor-critic reinforcement
    learning-based image captioning method. The method can directly optimize non-differentiable
    problems of the existing evaluation metrics. The architecture of the actor-critic
    method consists of a policy network (actor) and a value network (critic). The
    actor treats the job as sequential decision problem and can predict the next token
    of the sequence. In each state of the sequence, the network will receive a task-specific
    reward (in this case, it is evaluation metrics score). The job of the critic is
    to predict the reward. If it can predict the expected reward, the actor will continue
    to sample outputs according to its probability distribution.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人（Zhang et al., [2017](#bib.bib162)）提出了一种基于演员-评论家强化学习的图像描述生成方法。该方法可以直接优化现有评估指标的非差分问题。演员-评论家方法的架构包括一个策略网络（演员）和一个价值网络（评论家）。演员将任务视为序列决策问题，并可以预测序列中的下一个标记。在序列的每个状态中，网络将接收一个特定任务的奖励（在这种情况下，是评估指标得分）。评论家的任务是预测奖励。如果它能预测预期的奖励，演员将继续根据其概率分布进行输出采样。
- en: 'GAN based methods can learn deep features from unlabeled data. They achieve
    this representations applying a competitive process between a pair of networks:
    the Generator and the Discriminator. GANs have already been used successfully
    in a variety of applications, including image captioning(Dai et al., [2017](#bib.bib27);
    Shetty et al., [2017](#bib.bib127)), image to image translation (Isola et al.,
    [2017](#bib.bib57)), text to image synthesis (Reed et al., [2016](#bib.bib116);
    Bodnar, [2018](#bib.bib16)), and text generation (Fedus et al., [2018](#bib.bib37);
    Wang et al., [2018](#bib.bib146)).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAN 的方法可以从未标记的数据中学习深层特征。它们通过一对网络之间的竞争过程来实现这些表示：生成器和判别器。GAN 已经成功地应用于各种应用，包括图像描述生成（Dai
    et al., [2017](#bib.bib27); Shetty et al., [2017](#bib.bib127)）、图像到图像的转换（Isola
    et al., [2017](#bib.bib57)）、文本到图像的合成（Reed et al., [2016](#bib.bib116); Bodnar,
    [2018](#bib.bib16)）和文本生成（Fedus et al., [2018](#bib.bib37); Wang et al., [2018](#bib.bib146)）。
- en: There are two issues with GAN. First, GAN can work well in generating natural
    images from real images because GANs are proposed for real-valued data. However,
    text processing is based on discrete numbers. Therefore, such operations are non-differentiable,
    making it difficult to apply back-propagation directly. Policy gradients apply
    a parametric function to allow gradients to be back-propagated. Second, the evaluator
    faces problems in vanishing gradients and error propagation for sequence generation.
    It needs a probable future reward value for every partial description. Monte Carlo
    rollouts (Yu et al., [2017](#bib.bib158)) is used to compute this future reward
    value.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 存在两个问题。首先，GAN 可以很好地从真实图像中生成自然图像，因为 GAN 是为真实值数据提出的。然而，文本处理是基于离散数字的。因此，这些操作是不可微分的，使得直接应用反向传播变得困难。策略梯度应用参数函数以允许梯度反向传播。其次，评估器在序列生成中面临梯度消失和误差传播的问题。它需要每个部分描述的可能未来奖励值。蒙特卡罗模拟（Yu
    et al., [2017](#bib.bib158)）被用来计算这个未来的奖励值。
- en: GAN based image captioning methods can generate a diverse set of image captions
    in contrast to conventional deep convolutional network and deep recurrent network
    based model. Dai et al. (Dai et al., [2017](#bib.bib27)) also proposed a GAN based
    image captioning method. However, they do not consider multiple captions for a
    single image. Shetty et al. (Shetty et al., [2017](#bib.bib127)) introduced a
    new GAN based image captioning method. This method can generate multiple captions
    for a single image and showed impressive improvements in generating diverse captions.
    GANs have limitations in backpropagating the discrete data. Gumbel sampler (Jang
    et al., [2017](#bib.bib59); Maddison et al., [2017](#bib.bib92)) is used to overcome
    the discrete data problem. The two main parts of this adversarial network are
    the generator and the discriminator. During training, generator learns the loss
    value provided by the discriminator instead of learning it from explicit sources.
    Discriminator has true data distribution and can discriminate between generator-generated
    samples and true data samples. This allows the network to learn diverse data distribution.
    Moreover, the network classifies the generated caption sets either real or fake.
    Thus, it can generate captions similar to human generated one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAN 的图像标注方法能够生成多样化的图像描述，这与传统的深度卷积网络和深度递归网络模型不同。Dai 等人（Dai et al., [2017](#bib.bib27)）也提出了一种基于
    GAN 的图像标注方法。然而，他们没有考虑单张图像的多个描述。Shetty 等人（Shetty et al., [2017](#bib.bib127)）介绍了一种新的基于
    GAN 的图像标注方法。该方法能够为单张图像生成多个描述，并在生成多样化描述方面显示了显著的改进。GAN 在反向传播离散数据时存在局限性。Gumbel 采样器（Jang
    et al., [2017](#bib.bib59); Maddison et al., [2017](#bib.bib92)）被用来克服离散数据问题。该对抗网络的两个主要部分是生成器和判别器。在训练过程中，生成器学习判别器提供的损失值，而不是从显式来源学习。判别器拥有真实数据分布，可以区分生成器生成的样本和真实数据样本。这使得网络能够学习多样的数据分布。此外，网络将生成的描述集分类为真实或虚假。因此，它可以生成类似于人类生成的描述。
- en: '![Refer to caption](img/3bcef97697cfde62a4589dfa4ebff26e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3bcef97697cfde62a4589dfa4ebff26e.png)'
- en: Figure 4\. A block diagram of dense captioning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 密集标注的框图。
- en: 3.3\. Dense Captioning vs. Captions for the whole scene
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 密集标注与整个场景的描述
- en: In dense captioning, captions are generated for each region of the scene. Other
    methods generate captions for the whole scene.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在密集标注中，为场景的每个区域生成描述。其他方法为整个场景生成描述。
- en: 3.3.1\. Dense Captioning
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 密集标注
- en: The previous image captioning methods can generate only one caption for the
    whole image. They use different regions of the image to obtain information of
    various objects. However, these methods do not generate region wise captions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的图像标注方法只能为整张图像生成一个描述。它们使用图像的不同区域来获取各种对象的信息。然而，这些方法没有生成按区域的描述。
- en: Johnson et al. (Johnson et al., [2016](#bib.bib63)) proposed an image captioning
    method called DenseCap. This method localizes all the salient regions of an image
    and then it generates descriptions for those regions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Johnson 等人（Johnson et al., [2016](#bib.bib63)）提出了一种称为 DenseCap 的图像标注方法。该方法对图像中的所有显著区域进行定位，然后为这些区域生成描述。
- en: 'A typical method of this category has the following steps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的典型方法包括以下步骤：
- en: (1)
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Region proposals are generated for the different regions of the given image.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为给定图像的不同区域生成区域提议。
- en: (2)
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: CNN is used to obtain the region-based image features.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CNN 被用来获取基于区域的图像特征。
- en: (3)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: The outputs of Step 2 are used by a language model to generate captions for
    every region.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤2的输出由语言模型用来为每个区域生成标题。
- en: A block diagram of a typical dense captioning method is given in Figure 4.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图4中给出了典型密集标题生成方法的框图。
- en: Dense captioning (Johnson et al., [2016](#bib.bib63)) proposes a fully convolutional
    localization network architecture, which is composed of a convolutional network,
    a dense localization layer, and an LSTM (Hochreiter and Schmidhuber, [1997](#bib.bib55))
    language model. The dense localization layer processes an image with a single,
    efficient forward pass, which implicitly predicts a set of region of interest
    in the image. Thereby, it requires no external region proposals unlike to Fast
    R-CNN or a full network (i.e., RPN (Region Proposal Network (Girshick, [2015](#bib.bib45))))
    of Faster R-CNN. The working principle of the localization layer is related to
    the work of Faster R-CNN (Ren et al., [2015a](#bib.bib117)). However, Johnson
    et al. (Johnson et al., [2016](#bib.bib63)) use a differential, spatial soft attention
    mechanism (Gregor et al., [2015](#bib.bib50); Jaderberg et al., [2015](#bib.bib58))
    and bilinear interpolation (Jaderberg et al., [2015](#bib.bib58)) instead of ROI
    pooling mechanism (Girshick, [2015](#bib.bib45)). This modification helps the
    method to backpropagate through the network and smoothly select the active regions.
    It uses Visual Genome (Krishna et al., [2017](#bib.bib73)) dataset for the experiments
    in generating region level image captions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 密集标题生成（Johnson et al., [2016](#bib.bib63)）提出了一种完全卷积的定位网络架构，该架构由卷积网络、密集定位层和LSTM（Hochreiter
    and Schmidhuber, [1997](#bib.bib55)）语言模型组成。密集定位层通过一次高效的前向传递处理图像，隐式地预测图像中的一组兴趣区域。因此，它不像Fast
    R-CNN或完整网络（即RPN（区域建议网络（Girshick, [2015](#bib.bib45)）））那样需要外部区域建议。定位层的工作原理与Faster
    R-CNN（Ren et al., [2015a](#bib.bib117)）的工作有关。然而，Johnson et al.（Johnson et al.,
    [2016](#bib.bib63)）使用了差分的空间软注意力机制（Gregor et al., [2015](#bib.bib50); Jaderberg
    et al., [2015](#bib.bib58)）和双线性插值（Jaderberg et al., [2015](#bib.bib58)），而不是ROI池化机制（Girshick,
    [2015](#bib.bib45)）。这一修改有助于该方法在网络中进行反向传播，并平滑地选择活动区域。它使用Visual Genome（Krishna et
    al., [2017](#bib.bib73)）数据集进行区域级图像标题生成实验。
- en: One description of the entire visual scene is quite subjective and is not enough
    to bring out the complete understanding. Region-based descriptions are more objective
    and detailed than global image description. The region-based description is known
    as dense captioning. There are some challenges in dense captioning. As regions
    are dense, one object may have multiple overlapping regions of interest. Moreover,
    it is very difficult to recognize each target region for all the visual concepts.
    Yang et al. (Yang et al., [2016](#bib.bib154)) proposed another dense captioning
    method. This method can tackle these challenges. First, it addresses an inference
    mechanism that jointly depends on the visual features of the region and the predicted
    captions for that region. This allows the model to find an appropriate position
    of the bounding box. Second, they apply a context fusion that can combine context
    features with the visual features of respective regions to provide a rich semantic
    description.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个视觉场景的描述非常主观，不足以完全展现理解。基于区域的描述比全局图像描述更为客观和详细。基于区域的描述称为密集标题生成。密集标题生成面临一些挑战。由于区域较密集，一个对象可能具有多个重叠的兴趣区域。此外，识别所有视觉概念的每个目标区域非常困难。Yang等（Yang
    et al., [2016](#bib.bib154)）提出了另一种密集标题生成方法，该方法可以应对这些挑战。首先，它提出了一种推理机制，该机制共同依赖于区域的视觉特征和该区域预测的标题。这使得模型能够找到边界框的适当位置。其次，他们应用了上下文融合，将上下文特征与各自区域的视觉特征结合，以提供丰富的语义描述。
- en: 3.3.2\. Captions for the whole scene
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 整个场景的标题
- en: Encoder-Decoder architecture, Compositional architecture, attention-based, semantic
    concept-based, stylized captions, Novel object-based image captioning, and other
    deep learning networks-based image captioning methods generate single or multiple
    captions for the whole scene.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构、组合架构、基于注意力的、基于语义概念的、风格化标题、新颖对象基础的图像标题生成方法以及其他基于深度学习网络的图像标题生成方法，为整个场景生成单个或多个标题。
- en: 3.4\. Encoder-Decoder Architecture vs. Compositional Architecture
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 编码器-解码器架构与组合架构
- en: Some methods use just simple vanilla encoder and decoder to generate captions.
    However, other methods use multiple networks for it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法仅使用简单的原始编码器和解码器生成标题。然而，其他方法则使用多个网络来完成这一任务。
- en: 3.4.1\. Encoder-Decoder Architecture-Based Image captioning
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 基于编码器-解码器架构的图像描述
- en: The neural network-based image captioning methods work as just simple end to
    end manner. These methods are very similar to the encoder-decoder framework-based
    neural machine translation (Sutskever et al., [2014](#bib.bib132)). In this network,
    global image features are extracted from the hidden activations of CNN and then
    fed them into an LSTM to generate a sequence of words.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的图像描述方法仅以简单的端到端方式运作。这些方法与基于编码器-解码器框架的神经机器翻译（Sutskever et al., [2014](#bib.bib132)）非常相似。在这个网络中，全球图像特征从CNN的隐藏激活中提取出来，然后输入到LSTM中以生成单词序列。
- en: '![Refer to caption](img/654bcf29155a85f484ec172d4dc96c8c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/654bcf29155a85f484ec172d4dc96c8c.png)'
- en: Figure 5\. A block diagram of simple Encoder-Decoder architecture-based image
    captioning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 基于简单编码器-解码器架构的图像描述框图。
- en: 'A typical method of this category has the following general steps:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别的典型方法具有以下一般步骤：
- en: (1)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: A vanilla CNN is used to obtain the scene type, to detect the objects and their
    relationships.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用普通CNN来获取场景类型，检测对象及其关系。
- en: (2)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The output of Step 1 is used by a language model to convert them into words,
    combined phrases that produce an image captions.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤1的输出由语言模型使用，将其转换为单词，组合成生成图像描述的短语。
- en: A simple block diagram of this category is given in Figure 5.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此类别的简单框图如图5所示。
- en: Vinyals et al. (Vinyals et al., [2015](#bib.bib143)) proposed a method called
    Neural Image Caption Generator (NIC). The method uses a CNN for image representations
    and an LSTM for generating image captions. This special CNN uses a novel method
    for batch normalization and the output of the last hidden layer of CNN is used
    as an input to the LSTM decoder. This LSTM is capable of keeping track of the
    objects that already have been described using text. NIC is trained based on maximum
    likelihood estimation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Vinyals et al.（Vinyals et al., [2015](#bib.bib143)）提出了一种方法，称为神经图像描述生成器（NIC）。该方法使用CNN进行图像表示，使用LSTM生成图像描述。这个特殊的CNN使用了一种新颖的批归一化方法，CNN最后隐藏层的输出用作LSTM解码器的输入。这个LSTM能够跟踪已经用文本描述的对象。NIC基于最大似然估计进行训练。
- en: In generating image captions, image information is included to the initial state
    of an LSTM. The next words are generated based on the current time step and the
    previous hidden state. This process continues until it gets the end token of the
    sentence. Since image information is fed only at the beginning of the process,
    it may face vanishing gradient problems. The role of the words generated at the
    beginning is also becoming weaker and weaker. Therefore, LSTM is still facing
    challenges in generating long length sentences (Bahdanau et al., [2015](#bib.bib8);
    Cho et al., [2014](#bib.bib25)). Therefore, Jia et al. (Jia et al., [2015](#bib.bib60))
    proposed an extension of LSTM called guided LSTM (gLSTM). This gLSTM can generate
    long sentences. In this architecture, it adds global semantic information to each
    gate and cell state of LSTM. It also considers different length normalization
    strategies to control the length of captions. Semantic information is extracted
    in different ways. First, it uses a cross-modal retrieval task for retrieving
    image captions and then semantic information is extracted from these captions.
    The semantic based information can also be extracted using a multimodal embedding
    space.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成图像描述时，图像信息被包含到LSTM的初始状态中。接下来的单词基于当前时间步和之前的隐藏状态生成。这个过程持续进行，直到获得句子的结束标记。由于图像信息仅在过程开始时输入，可能会面临梯度消失问题。开始生成的单词的作用也变得越来越弱。因此，LSTM在生成长句子时仍然面临挑战（Bahdanau
    et al., [2015](#bib.bib8); Cho et al., [2014](#bib.bib25)）。因此，Jia et al.（Jia et
    al., [2015](#bib.bib60)）提出了一种LSTM的扩展，称为指导LSTM（gLSTM）。这种gLSTM可以生成长句子。在这种架构中，它将全局语义信息添加到LSTM的每个门和单元状态中。它还考虑了不同的长度归一化策略来控制描述的长度。语义信息以不同的方式提取。首先，它使用跨模态检索任务来检索图像描述，然后从这些描述中提取语义信息。语义基础信息也可以通过多模态嵌入空间提取。
- en: Mao et al. (Mao et al., [2016](#bib.bib93)) proposed a special type of text
    generation method for images. This method can generate a description for an specific
    object or region that is called referring expression (van Deemter et al., [2006](#bib.bib137);
    Viethen and Dale, [2008](#bib.bib142); Mitchell et al., [2010](#bib.bib103), [2013](#bib.bib104);
    FitzGerald et al., [2013](#bib.bib38); Golland et al., [2010](#bib.bib47); Kazemzadeh
    et al., [2014](#bib.bib69)). Using this expression it can then infer the object
    or region which is being described. Therefore, generated description or expression
    is quite unambiguous. In order to address the referring expression, this method
    uses a new dataset called ReferIt dataset (Kazemzadeh et al., [2014](#bib.bib69))
    based on popular MS COCO dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 毛等（Mao et al., [2016](#bib.bib93)）提出了一种针对图像的特殊文本生成方法。该方法可以为特定对象或区域生成描述，这种描述被称为**指称表达**（van
    Deemter et al., [2006](#bib.bib137)；Viethen 和 Dale, [2008](#bib.bib142)；Mitchell
    et al., [2010](#bib.bib103)，[2013](#bib.bib104)；FitzGerald et al., [2013](#bib.bib38)；Golland
    et al., [2010](#bib.bib47)；Kazemzadeh et al., [2014](#bib.bib69)）。使用这种表达可以推断出正在描述的对象或区域。因此，生成的描述或表达是相当明确的。为了处理指称表达，该方法使用了一个新的数据集，称为ReferIt数据集（Kazemzadeh
    et al., [2014](#bib.bib69)），该数据集基于流行的MS COCO数据集。
- en: Previous CNN-RNN based image captioning methods use LSTM that are unidirectional
    and relatively shallow in depth. In unidirectional language generation techniques,
    the next word is predicted based on visual context and all the previous textual
    contexts. Unidirectional LSTM cannot generate contextually well formed captions.
    Moreover, recent object detection and classification methods (Krizhevsky et al.,
    [2012](#bib.bib74); Simonyan and Zisserman, [2015](#bib.bib128)) show that deep,
    hierarchical methods are better at learning than shallower ones. Wang et al. (Wang
    et al., [2016b](#bib.bib145)) proposed a deep bidirectional LSTM-based method
    for image captioning. This method is capable of generating contextually and semantically
    rich image captions. The proposed architecture consists of a CNN and two separate
    LSTM networks. It can utilize both past and future context information to learn
    long term visual language interactions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 之前基于CNN-RNN的图像字幕生成方法使用的是单向且相对较浅的LSTM。在单向语言生成技术中，下一词是基于视觉上下文和所有先前的文本上下文预测的。单向LSTM无法生成上下文良好的标题。此外，最近的对象检测和分类方法（Krizhevsky
    et al., [2012](#bib.bib74)；Simonyan 和 Zisserman, [2015](#bib.bib128)）显示，深层次的层级方法在学习上优于较浅的方法。王等（Wang
    et al., [2016b](#bib.bib145)）提出了一种基于深度双向LSTM的图像字幕生成方法。这种方法能够生成上下文和语义丰富的图像标题。所提出的架构包括一个CNN和两个独立的LSTM网络。它可以利用过去和未来的上下文信息来学习长期的视觉语言交互。
- en: 3.4.2\. Compositional Architecture-Based Image captioning
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. **组成架构基础的图像字幕生成**
- en: 'Compositional architecture-based methods composed of several independent functional
    building blocks: First, a CNN is used to extract the semantic concepts from the
    image. Then a language model is used to generate a set of candidate captions.
    In generating the final caption, these candidate captions are re-ranked using
    a deep multimodal similarity model.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由几个独立功能模块组成的**组成架构**方法：首先，使用CNN从图像中提取语义概念。然后，使用语言模型生成一组候选标题。在生成最终标题时，这些候选标题会通过深度多模态相似性模型进行重新排序。
- en: '![Refer to caption](img/40853cbcd4e3af5e68761ebcd9b3e6ab.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/40853cbcd4e3af5e68761ebcd9b3e6ab.png)'
- en: Figure 6\. A block diagram of a compositional network-based captioning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 组成网络基于图像字幕生成的框图。
- en: 'A typical method of this category maintains the following steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类别的典型方法保持以下步骤：
- en: (1)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Image features are obtained using a CNN.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像特征是通过CNN获得的。
- en: (2)
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Visual concepts (e.g. attributes) are obtained from visual features.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉概念（例如属性）是从视觉特征中获得的。
- en: (3)
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Multiple captions are generated by a language model using the information of
    Step 1 and Step 2.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言模型使用步骤1和步骤2中的信息生成多个标题。
- en: (4)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: The generated captions are re-ranked using a deep multimodal similarity model
    to select high quality image captions.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的标题使用深度多模态相似性模型重新排序，以选择高质量的图像标题。
- en: A common block diagram of compositional network-based image captioning methods
    is given in Figure 6.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图6中给出了一个基于组成网络的图像字幕生成方法的常见框图。
- en: Fang et al.(Fang et al., [2015](#bib.bib34)) introduced generation-based image
    captioning. It uses visual detectors, a language model, and a multimodal similarity
    model to train the model on an image captioning dataset. Image captions can contain
    nouns, verbs, and adjectives. A vocabulary is formed using 1000 most common words
    from the training captions. The system works with the image sub-regions rather
    that the full image. Convolutional neural networks (both AlexNet (Krizhevsky et al.,
    [2012](#bib.bib74)) and VGG16Net) are used for extracting features for the sub-regions
    of an image. The features of sub-regions are mapped with the words of the vocabulary
    that likely to be contained in the image captions. Multiple instance learning
    (MIL) (Maron and Lozano-Pérez, [1998](#bib.bib97)) is used to train the model
    for learning discriminative visual signatures of each word. A maximum entropy
    (ME) (Berger et al., [1996](#bib.bib13)) language model is used for generating
    image captions from these words. Generated captions are ranked by a linear weighting
    of sentence features. Minimum Error rate training (MERT) (Och, [2003](#bib.bib107))
    is used to learn these weights. Similarity between image and sentence can be easily
    measured using a common vector representation. Image and sentence fragments are
    mapped with the common vector representation by a deep multimodal similarity model
    (DMSM). It achieves a significant improvement in choosing high quality image captions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 方等人（Fang et al., [2015](#bib.bib34)）介绍了基于生成的图像描述技术。它使用视觉检测器、语言模型和多模态相似性模型来在图像描述数据集上训练模型。图像描述可以包含名词、动词和形容词。词汇表是由训练描述中的1000个最常见的词汇组成。系统处理的是图像子区域，而不是完整图像。卷积神经网络（包括AlexNet（Krizhevsky
    et al., [2012](#bib.bib74)）和VGG16Net）用于提取图像子区域的特征。子区域的特征与词汇中的可能出现在图像描述中的词语进行映射。多实例学习（MIL）（Maron
    和 Lozano-Pérez，[1998](#bib.bib97)）用于训练模型，以学习每个词的判别视觉特征。最大熵（ME）（Berger et al.,
    [1996](#bib.bib13)）语言模型用于从这些词生成图像描述。生成的描述通过线性加权的句子特征进行排序。最小误差率训练（MERT）（Och，[2003](#bib.bib107)）用于学习这些权重。图像和句子之间的相似性可以通过共同的向量表示轻松测量。图像和句子片段通过深度多模态相似性模型（DMSM）与共同的向量表示进行映射。它在选择高质量图像描述方面取得了显著改进。
- en: 'Until now a significant number of methods have achieved satisfactory progress
    in generating image captions. The methods use training and testing samples from
    the same domain. Therefore, there is no certainty that these methods can perform
    well in open-domain images. Moreover, they are only good at recognizing generic
    visual content. There are certain key entities such as celebrities and landmarks
    that are out of their scope. The generated captions of these methods are evaluated
    on automatic metrics such as BLEU (Papineni et al., [2002](#bib.bib111)), METEOR
    (Agarwal and Lavie, [2008](#bib.bib2)), and CIDEr (Vedantam et al., [2015](#bib.bib140)).
    These evaluation metrics have already shown good results on these methods. However,
    in terms of performance there exists a large gap between the evaluation of the
    metrics and human judgement of evaluation (Devlin et al., [2015](#bib.bib31);
    Callison-Burch et al., [2006](#bib.bib21); Kulkarni et al., [2011](#bib.bib75)).
    If it is considered real life entity information, the performance could be weaker.
    However, Tran et al. (Tran et al., [2016](#bib.bib136)) introduced a different
    image captioning method. This method is capable of generating image captions even
    for open domain images. It can detect a diverse set of visual concepts and generate
    captions for celebrities and landmarks. It uses an external knowledge base Freebase
    (Bollacker et al., [2008](#bib.bib17)) in recognizing a broad range of entities
    such as celebrities and landmarks. A series of human judgments are applied for
    evaluating the performances of generated captions. In experiments, it uses three
    datasets: MS COCO, Adobe-MIT FiveK (Bychkovsky et al., [2011](#bib.bib20)), and
    images from Instagram. The images of MS COCO dataset were collected from the same
    domain but the images of other datasets were chosen from an open domain. The method
    achieves notable performances especially on the challenging Instagram dataset.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，许多方法在生成图像标题方面取得了令人满意的进展。这些方法使用来自相同领域的训练和测试样本。因此，这些方法在开放领域图像中的表现尚无定论。此外，它们只擅长识别通用视觉内容。某些关键实体，如名人和地标，超出了它们的范围。这些方法生成的标题是通过自动化指标如BLEU
    (Papineni et al., [2002](#bib.bib111))、METEOR (Agarwal and Lavie, [2008](#bib.bib2))
    和CIDEr (Vedantam et al., [2015](#bib.bib140))来评估的。这些评估指标已在这些方法上显示出良好的结果。然而，在性能方面，评估指标与人工评估之间存在较大差距
    (Devlin et al., [2015](#bib.bib31); Callison-Burch et al., [2006](#bib.bib21);
    Kulkarni et al., [2011](#bib.bib75))。如果考虑到真实的实体信息，性能可能会更弱。然而，Tran et al. (Tran
    et al., [2016](#bib.bib136)) 引入了一种不同的图像标题生成方法。该方法能够生成开放领域图像的标题。它可以检测多种视觉概念，并为名人和地标生成标题。它使用外部知识库
    Freebase (Bollacker et al., [2008](#bib.bib17)) 来识别广泛的实体，如名人和地标。一系列人工判断被应用于评估生成标题的表现。在实验中，它使用了三个数据集：MS
    COCO、Adobe-MIT FiveK (Bychkovsky et al., [2011](#bib.bib20)) 和来自 Instagram 的图像。MS
    COCO 数据集的图像来自相同领域，但其他数据集的图像则来自开放领域。该方法在挑战性的 Instagram 数据集上表现尤为突出。
- en: Ma et al. (Ma and Han, [2016](#bib.bib91)) proposed another compositional network-based
    image captioning method. This method uses structural words $<$object, attribute,
    activity, scene$>$ to generate semantically meaningful descriptions. It also uses
    a multi-task method similar to multiple instance learning method (Fang et al.,
    [2015](#bib.bib34)), and multi-layer optimization method (Han and Li, [2015](#bib.bib53))
    to generate structural words. An LSTM encoder-decoder-based machine translation
    method (Sutskever et al., [2014](#bib.bib132)) is then used to translate the structural
    words into image captions.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Ma et al. (Ma and Han, [2016](#bib.bib91)) 提出了另一种基于组成网络的图像标题生成方法。该方法使用结构词 $<$object,
    attribute, activity, scene$>$ 来生成语义有意义的描述。它还使用类似于多实例学习方法 (Fang et al., [2015](#bib.bib34))
    和多层优化方法 (Han and Li, [2015](#bib.bib53)) 的多任务方法来生成结构词。随后，使用基于 LSTM 编码器-解码器的机器翻译方法
    (Sutskever et al., [2014](#bib.bib132)) 将结构词翻译成图像标题。
- en: Wang et al. (Wang et al., [2016a](#bib.bib147)) proposed a parallel-fusion RNN-LSTM
    architecture for image caption generation. The architecture of the method divides
    the hidden units of RNN and LSTM into a number of same-size parts. The parts work
    in parallel with corresponding ratios to generate image captions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Wang et al. (Wang et al., [2016a](#bib.bib147)) 提出了一个并行融合 RNN-LSTM 架构用于图像标题生成。该方法的架构将
    RNN 和 LSTM 的隐藏单元划分为多个相同大小的部分。这些部分以相应的比例并行工作以生成图像标题。
- en: 3.5\. Others
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 其他
- en: Attention-based, Semantic concept-based, Novel object-based methods, and Stylized
    captions are put together into “Others” group because these categories are independent
    to other methods.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力机制、基于语义概念、基于新颖对象的方法以及风格化标题被归入“其他”组，因为这些类别与其他方法是独立的。
- en: 3.5.1\. Attention based Image Captioning
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 基于注意力的图像标注
- en: Neural encoder-decoder based approaches were mainly used in machine translation
    (Sutskever et al., [2014](#bib.bib132)). Following these trends, they have also
    been used for the task of image captioning and found very effective. In image
    captioning, a CNN is used as an encoder to extract the visual features from the
    input image and an RNN is used as a decoder to convert this representation word-by-word
    into natural language description of the image. However, these methods are unable
    to analyze the image over time while they generate the descriptions for the image.
    In addition to this, the methods do not consider the spatial aspects of the image
    that is relevant to the parts of the image captions. Instead, they generate captions
    considering the scene as a whole. Attention based mechanisms are becoming increasingly
    popular in deep learning because they can address these limitations. They can
    dynamically focus on the various parts of the input image while the output sequences
    are being produced.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经编码器-解码器的方法主要用于机器翻译（Sutskever等，[2014](#bib.bib132)）。遵循这些趋势，这些方法也被用于图像标注任务，并且非常有效。在图像标注中，CNN用作编码器，从输入图像中提取视觉特征，而RNN用作解码器，将这种表示逐字转换为图像的自然语言描述。然而，这些方法在生成图像描述时无法随时间分析图像。此外，这些方法不考虑与图像标题部分相关的图像空间方面。相反，它们在生成标题时考虑的是整个场景。基于注意力的机制在深度学习中越来越受欢迎，因为它们可以解决这些限制。它们可以在输出序列生成的过程中动态地关注输入图像的各个部分。
- en: '![Refer to caption](img/a6dd1c52c38695286524352235f6f70d.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a6dd1c52c38695286524352235f6f70d.png)'
- en: Figure 7\. A block diagram of a typical attention-based image captioning technique.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 一种典型的基于注意力的图像标注技术的框图。
- en: 'A typical method of this category adopts the following steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别的典型方法包括以下步骤：
- en: (1)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Image information is obtained based on the whole scene by a CNN.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过CNN基于整个场景获取图像信息。
- en: (2)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The language generation phase generates words or phrases based on the output
    of Step 1.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言生成阶段根据步骤1的输出生成单词或短语。
- en: (3)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Salient regions of the given image are focused in each time step of the language
    generation model based on generated words or phrases.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在语言生成模型的每一个时间步骤中，根据生成的单词或短语，聚焦于给定图像的显著区域。
- en: (4)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: Captions are updated dynamically until the end state of language generation
    model.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标题会在语言生成模型的结束状态之前动态更新。
- en: A block diagram of the attention-based image captioning method is shown in Figure
    7.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力机制的图像标注方法的框图见图7。
- en: 'Xu et al. (Xu et al., [2015](#bib.bib153)) were the first to introduce an attention-based
    image captioning method. The method describes the salient contents of an image
    automatically. The main difference between the attention-based methods with other
    methods is that they can concentrate on the salient parts of the image and generate
    the corresponding words at the same time. This method applies two different techniques:
    stochastic hard attention and deterministic soft attention to generate attentions.
    Most CNN-based approaches use the top layer of ConvNet for extracting information
    of the salient objects from the image. A drawback of these techniques is that
    they may lose certain information which is useful to generate detailed captions.
    In order to preserve the information, the attention method uses features from
    the lower convolutional layer instead of fully connected layer.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Xu等（Xu et al., [2015](#bib.bib153)）是首个引入基于注意力的图像标注方法的研究者。该方法自动描述图像的显著内容。基于注意力的方法与其他方法的主要区别在于，它们能够集中注意图像的显著部分，并同时生成相应的单词。这种方法应用了两种不同的技术：随机硬注意力和确定性软注意力来生成注意力。大多数基于CNN的方法使用ConvNet的顶层来提取图像中显著对象的信息。这些技术的一个缺点是可能会丢失一些生成详细描述时有用的信息。为了保留信息，注意力方法使用来自较低卷积层的特征，而不是全连接层。
- en: Jin et al. (Jin et al., [2015](#bib.bib62)) proposed another attention-based
    image captioning method. This method is capable to extract the flow of abstract
    meaning based on the semantic relationship between visual information and textual
    information. It can also obtain higher level semantic information by proposing
    a scene specific context. The main difference between this method with other attention-based
    methods is that it introduces multiple visual regions of an image at multiple
    scales. This technique can extract proper visual information of a particular object.
    For extracting scene specific context, it first uses the Latent Dirichlet Allocation
    (LDA) (Blei et al., [2003](#bib.bib15)) for generating a dictionary from all the
    captions of the dataset. Then a multilayer perceptron is used to predict a topic
    vector for every image. A scene factored LSTM that has two stacked layers are
    used to generate a description for the overall context of the image.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Jin等人（Jin et al., [2015](#bib.bib62)）提出了另一种基于注意力的图像描述方法。该方法能够基于视觉信息和文本信息之间的语义关系提取抽象意义的流。它还可以通过提出场景特定上下文来获得更高层次的语义信息。该方法与其他基于注意力的方法的主要区别在于它引入了多尺度的图像多个视觉区域。这项技术可以提取特定物体的适当视觉信息。为了提取场景特定的上下文，它首先使用潜在狄利克雷分配（LDA）（Blei
    et al., [2003](#bib.bib15)）从数据集的所有描述中生成词典。然后使用多层感知器预测每个图像的主题向量。一个具有两层堆叠的场景分解LSTM用于生成图像整体背景的描述。
- en: 'Wu et al. (Wu and Cohen, [2016](#bib.bib152)) proposed a review-based attention
    method for image captioning. It introduces a review model that can perform multiple
    review steps with attention on CNN hidden states. The output of the CNN is a number
    of fact vectors that can obtain the global facts of the image. The vectors are
    given as input to the attention mechanism of the LSTM. For example, a reviewer
    module can first review: What are the objects in the image? Then it can review
    the relative positions of the objects and another review can extract the information
    of the overall context of the image. These information is passed to the decoder
    to generate image captions.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 吴等人（Wu and Cohen, [2016](#bib.bib152)）提出了一种基于回顾的注意力方法用于图像描述。该方法引入了一种回顾模型，可以在CNN隐藏状态上执行多次回顾步骤。CNN的输出是一系列可以获取图像全球信息的事实向量。这些向量被作为输入传递给LSTM的注意力机制。例如，一个回顾模块可以首先回顾：图像中有哪些物体？然后它可以回顾物体的相对位置，另一种回顾可以提取图像整体背景的信息。这些信息被传递给解码器以生成图像描述。
- en: Pedersoli et al. (Pedersoli et al., [2017](#bib.bib113)) proposed an area based
    attention mechanism for image captioning. Previous attention based methods map
    image regions only to the state of RNN language model. However, this approach
    associates image regions with caption words given the RNN state. It can predict
    the next caption word and corresponding image region in each time-step of RNN.
    It is capable of predicting the next word as well as corresponding image regions
    in each time-step of RNN for generating image captions. In order to find the areas
    of attention, previous attention-based image caption methods use either the position
    of CNN activation grid or object proposals. In contrast, this method uses an end
    to end trainable convolutional spatial transformer along with CNN activation gird
    and object proposal methods. A combination of these techniques help this method
    to compute image adaptive areas of attention. In experiments, the method shows
    that this new attention mechanism together with the spatial transformer network
    can produce high quality image captions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Pedersoli等人（Pedersoli et al., [2017](#bib.bib113)）提出了一种基于区域的注意力机制用于图像描述。之前基于注意力的方法仅将图像区域映射到RNN语言模型的状态。然而，该方法将图像区域与给定RNN状态的描述词关联起来。它可以预测RNN每个时间步的下一个描述词和相应的图像区域。它能够预测下一个词以及RNN每个时间步的相应图像区域，从而生成图像描述。为了找到关注区域，以前基于注意力的图像描述方法使用CNN激活网格的位置或目标提案。相比之下，该方法使用端到端可训练的卷积空间变换器，以及CNN激活网格和目标提案方法的组合。这些技术的结合帮助该方法计算图像自适应的关注区域。在实验中，该方法表明，这种新的注意力机制与空间变换网络结合可以产生高质量的图像描述。
- en: 'Lu et al. (Lu et al., [2017](#bib.bib89)) proposed another attention-based
    image captioning method. The method is based on adaptive attention model with
    a visual sentinel. Current attention-based image captioning methods focus on the
    image in every time step of RNN. However, there are some words or phrase (for
    example: a, of) that do not need to attend visual signals. Moreover, these unnecessary
    visual signals could affect the caption generation process and degrade the overall
    performance. Therefore, their proposed method can determine when it will focus
    on image region and when it will just focus on language generation model. Once
    it determines to look on the image then it must have to choose the spatial location
    of the image. The first contribution of this method is to introduce a novel spatial
    attention method that can compute spatial features from the image. Then in their
    adaptive attention method, they introduced a new LSTM extension. Generally, an
    LSTM works as a decoder that can produce a hidden state at every time step. However,
    this extension is capable of producing an additional visual sentinel that provides
    a fallback option to the decoder. It also has a sentinel gate that can control
    how much information the decoder will get from the image.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 陆等人（Lu et al., [2017](#bib.bib89)）提出了另一种基于注意力的图像字幕生成方法。该方法基于具有视觉哨兵的自适应注意力模型。目前的基于注意力的图像字幕生成方法在RNN的每个时间步都专注于图像。然而，有些词语或短语（例如：a，of）不需要关注视觉信号。此外，这些不必要的视觉信号可能会影响字幕生成过程并降低整体表现。因此，他们提出的方法可以确定何时关注图像区域以及何时仅关注语言生成模型。一旦确定要查看图像，它必须选择图像的空间位置。该方法的第一个贡献是引入了一种新颖的空间注意力方法，可以从图像中计算空间特征。然后在他们的自适应注意力方法中，引入了新的LSTM扩展。通常，LSTM作为解码器在每个时间步产生一个隐藏状态。然而，这种扩展能够产生额外的视觉哨兵，为解码器提供备选选项。它还有一个哨兵门，可以控制解码器从图像中获得多少信息。
- en: 'While attention-based methods look to find the different areas of the image
    at the time of generating words or phrases for image captions, the attention maps
    generated by these methods cannot always correspond to the proper region of the
    image. It can affect the performance of image caption generation. Liu et al. (Liu
    et al., [2017a](#bib.bib85)) proposed a method for neural image captioning. This
    method can evaluate and correct the attention map at time step. Correctness means
    to make consistent map between image regions and generated words. In order to
    achieve these goals, this method introduced a quantitative evaluation metric to
    compute the attention maps. It uses Flickr30k entity dataset (Plummer et al.,
    [2015](#bib.bib114)) and MS COCO (Lin et al., [2014](#bib.bib84)) dataset for
    measuring both ground truth attention map and semantic labelings of image regions.
    In order to learn a better attention function, it proposed supervised attention
    model. Two types of supervised attention models are used here: strong supervision
    with alignment annotation and weak supervision with semantic labelling. In strong
    supervision with alignment annotation model, it can directly map ground truth
    word to a region. However, ground truth alignment is not always possible because
    collecting and annotating data is often very expensive. Weak supervision is performed
    to use bounding box or segmentation masks on MS COCO dataset. In experiments,
    the method shows that supervised attention model performs better in mapping attention
    as well as image captioning.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于注意力的方法试图在生成图像字幕的词语或短语时找到图像的不同区域，但这些方法生成的注意力图并不总是能对应到图像的正确区域。这可能会影响图像字幕生成的表现。刘等人（Liu
    et al., [2017a](#bib.bib85)）提出了一种神经图像字幕生成的方法。该方法可以在时间步上评估和纠正注意力图。正确性意味着在图像区域和生成的词语之间保持一致的映射。为了实现这些目标，该方法引入了一个定量评估指标来计算注意力图。它使用Flickr30k实体数据集（Plummer
    et al., [2015](#bib.bib114)）和MS COCO（Lin et al., [2014](#bib.bib84)）数据集来测量地面真值注意力图和图像区域的语义标记。为了学习更好的注意力功能，它提出了监督注意力模型。这里使用了两种类型的监督注意力模型：具有对齐注释的强监督和具有语义标记的弱监督。在具有对齐注释的强监督模型中，它可以直接将地面真值词汇映射到一个区域。然而，由于收集和注释数据通常非常昂贵，地面真值对齐并不总是可能的。弱监督是在MS
    COCO数据集上使用边界框或分割掩码进行的。在实验中，该方法显示出监督注意力模型在映射注意力以及图像字幕生成方面表现更好。
- en: Chen et al. (Chen et al., [2017b](#bib.bib22)) proposed another attention-based
    image captioning method. This method considers both spatial and channel wise attentions
    to compute an attention map. The existing attention-based image captioning methods
    only consider spatial information for generating an attention map. A common drawback
    of these spatial attention methods are that they compute weighted pooling only
    on attentive feature map. As a result, these methods lose the spatial information
    gradually. Moreover, they use the spatial information only from the last conv-layer
    of the CNN. The receptive field regions of this layer are quite large that make
    the limited gap between the regions. Therefore, they do not get significant spatial
    attentions for an image. However, in this method, CNN features are extracted not
    only from spatial locations but also from different channels and multiple layers.
    Therefore, it gets significant spatial attention. In addition to this, in this
    method, each filter of a convolutional layer acts as semantic detectors (Zeiler
    and Fergus, [2014](#bib.bib160)) while other methods use external sources for
    obtaining semantic information.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等（Chen et al., [2017b](#bib.bib22)）提出了另一种基于注意力的图像描述方法。该方法考虑了空间和通道注意力来计算注意力图。现有的基于注意力的图像描述方法仅考虑空间信息来生成注意力图。这些空间注意力方法的一个常见缺点是它们仅在关注的特征图上计算加权池化。因此，这些方法逐渐丧失了空间信息。此外，它们仅使用来自
    CNN 的最后一个卷积层的空间信息。该层的感受野区域相当大，使得区域之间的间隔有限。因此，它们无法为图像获得显著的空间注意力。然而，在该方法中，CNN 特征不仅从空间位置提取，还从不同的通道和多个层中提取。因此，它能获得显著的空间注意力。此外，在该方法中，卷积层的每个滤波器充当语义检测器（Zeiler
    and Fergus, [2014](#bib.bib160)），而其他方法则使用外部资源来获取语义信息。
- en: In order to reduce the gap between human generated description and machine generated
    description Tavakoli et al. (Tavakoli et al., [2017](#bib.bib135)) introduced
    an attention-based image captioning method. This is a bottom up saliency based
    attention model that can take advantages for comparisons with other attention-based
    image captioning methods. It found that humans first describe the more important
    objects than less important ones. It also shows that the method performs better
    on unseen data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少人类生成描述和机器生成描述之间的差距，Tavakoli 等（Tavakoli et al., [2017](#bib.bib135)）引入了一种基于注意力的图像描述方法。这是一个自下而上的显著性基础注意力模型，可以在与其他基于注意力的图像描述方法进行比较时发挥优势。研究发现，人类首先描述更重要的对象而不是不太重要的对象。该方法在未见过的数据上表现更好。
- en: Most previous image captioning methods applied top-down approach for constructing
    a visual attention map. These mechanisms typically focused on some selective regions
    obtained from the output of one or two layers of a CNN. The input regions are
    of the same size and have the same shape of receptive field. This approach has
    a little consideration to the content of the image. However, the method of Anderson
    et al. (Anderson et al., [2017](#bib.bib5)) applied both top down and bottom up
    approaches. The bottom up attention mechanism uses Faster R-CNN (Ren et al., [2015a](#bib.bib117))
    for region proposals that can select salient regions of an image . Therefore,
    this method can attend both object level regions as well as other salient image
    regions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数先前的图像描述方法采用自上而下的方法来构建视觉注意力图。这些机制通常关注从 CNN 的一两层输出中获得的一些选择区域。输入区域的大小相同，具有相同的感受野形状。该方法对图像内容的考虑较少。然而，Anderson
    等（Anderson et al., [2017](#bib.bib5)）的方法应用了自上而下和自下而上的方法。自下而上的注意力机制使用 Faster R-CNN（Ren
    et al., [2015a](#bib.bib117)）进行区域提议，可以选择图像的显著区域。因此，该方法可以关注对象级区域以及其他显著的图像区域。
- en: 'Park et al. (Park et al., [2017](#bib.bib112)) introduced a different type
    of attention-based image captioning method. This method can generate image captions
    addressing personal issues of an image. It mainly considers two tasks : hashtag
    prediction and post generation. This method uses a Context Sequence Memory Network
    (CSMN) to obtain the context information from the image. Description of an image
    from personalized view has a lot of applications in social media networks. For
    example, everyday people share a lot of images as posts in Facebook, Instagram
    or other social media. Photo-taking or uploading is a very easy task. However,
    describing them is not easy because it requires theme, sentiment, and context
    of the image. Therefore, the method considers the past knowledge about the user''s
    vocabularies or writing styles from the prior documents for generating image descriptions.
    In order to work with this new type of image captioning, the CSMN method has three
    contributions: first, the memory of this network can work as a repository and
    retain multiple types of context information. Second, the memory is designed in
    such a way that it can store all the previously generated words sequentially.
    As a result, it does not suffer from vanishing gradient problem. Third, the proposed
    CNN can correlate with multiple memory slots that is helpful for understanding
    contextual concepts.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Park 等（Park 等，[2017](#bib.bib112)）提出了一种不同类型的基于注意力的图像描述方法。这种方法可以生成针对图像个人问题的图像描述。它主要考虑两个任务：标签预测和帖子生成。这种方法使用上下文序列记忆网络（CSMN）来获取图像的上下文信息。从个性化视角描述图像在社交媒体网络中有很多应用。例如，日常生活中人们会在
    Facebook、Instagram 或其他社交媒体上分享大量图像。拍摄或上传照片是非常简单的任务。然而，描述这些照片并不容易，因为这需要考虑图像的主题、情感和背景。因此，该方法考虑了用户之前文档中的词汇或写作风格知识以生成图像描述。为了处理这种新型图像描述，CSMN
    方法有三个贡献：首先，该网络的记忆可以作为存储库，保留多种类型的上下文信息。其次，记忆的设计方式可以顺序存储所有先前生成的词汇。因此，它不会遭受梯度消失问题。第三，提出的
    CNN 可以与多个记忆槽相关联，这对于理解上下文概念非常有帮助。
- en: Attention-based methods have already shown good performance and efficiency in
    image captioning as well as other computer vision tasks. However, attention maps
    generated by these attention based methods are only machine dependent. They do
    not consider any supervision from human attention. This creates the necessity
    to think about the gaze information whether it can improve the performance of
    these attention methods in image captioning. Gaze indicates the cognition and
    perception of humans about a scene. Human gaze can identify the important locations
    of objects in an image. Thus, gaze mechanisms have already shown their potential
    performances in eye-based user modeling (Bulling et al., [2011](#bib.bib19); Fathi
    et al., [2012](#bib.bib36); Papadopoulos et al., [2014](#bib.bib110); Sattar et al.,
    [2015](#bib.bib123); Shanmuga Vadivel et al., [2015](#bib.bib125)), object localization
    (Mishra et al., [2012](#bib.bib101)) or recognition (Karthikeyan et al., [2013](#bib.bib68))
    and holistic scene understanding (Yun et al., [2013](#bib.bib159); Zelinsky, [2013](#bib.bib161)).
    However, Sugano et al. (Sugano and Bulling, [2016](#bib.bib130)) claimed that
    gaze information has not yet been integrated in image captioning methods. This
    method introduced human gaze with the attention mechanism of deep neural networks
    in generating image captions. The method incorporates human gaze information into
    an attention-based LSTM model (Xu et al., [2015](#bib.bib153)). For experiments,
    it uses SALICON dataset (Jiang et al., [2015](#bib.bib61)) and achieves good results.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法在图像描述和其他计算机视觉任务中已经展示了良好的性能和效率。然而，这些基于注意力的方法生成的注意力图仅依赖于机器，它们没有考虑来自人类注意力的任何监督。这就需要考虑注视信息是否能提高这些注意力方法在图像描述中的表现。注视指示了人类对场景的认知和感知。人类的注视可以识别图像中对象的重要位置。因此，注视机制在基于眼动的用户建模（Bulling
    等，[2011](#bib.bib19)；Fathi 等，[2012](#bib.bib36)；Papadopoulos 等，[2014](#bib.bib110)；Sattar
    等，[2015](#bib.bib123)；Shanmuga Vadivel 等，[2015](#bib.bib125)），对象定位（Mishra 等，[2012](#bib.bib101)）或识别（Karthikeyan
    等，[2013](#bib.bib68)）以及整体场景理解（Yun 等，[2013](#bib.bib159)；Zelinsky，[2013](#bib.bib161)）中已经展示了其潜在的性能。然而，Sugano
    等（Sugano 和 Bulling，[2016](#bib.bib130)）声称注视信息尚未融入图像描述方法中。这种方法在生成图像描述时引入了人类注视与深度神经网络的注意力机制。该方法将人类注视信息融入基于注意力的
    LSTM 模型（Xu 等，[2015](#bib.bib153)）中。在实验中，它使用了 SALICON 数据集（Jiang 等，[2015](#bib.bib61)）并取得了良好的结果。
- en: '![Refer to caption](img/2f4decca6766430c2cca237daada0247.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考描述](img/2f4decca6766430c2cca237daada0247.png)'
- en: Figure 8\. A block diagram of a semantic concept-based image captioning.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 基于语义概念的图像描述的框图。
- en: 3.5.2\. Semantic Concept-Based Image Captioning
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2\. 基于语义概念的图像描述
- en: Semantic concept-based methods selectively attend to a set of semantic concept
    proposals extracted from the image. These concepts are then combined into hidden
    states and the outputs of recurrent neural networks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义概念的方法选择性地关注从图像中提取的一组语义概念提案。这些概念然后被组合到隐藏状态和递归神经网络的输出中。
- en: 'The methods in this category follow the following steps:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法遵循以下步骤：
- en: (1)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: CNN based encoder is used to encode the image features and semantic concepts.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用基于 CNN 的编码器对图像特征和语义概念进行编码。
- en: (2)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Image features are fed into the input of language generation model.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像特征被输入到语言生成模型的输入中。
- en: (3)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Semantic concepts are added to the different hidden states of the language model.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义概念被添加到语言模型的不同隐藏状态中。
- en: (4)
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: The language generation part produces captions with semantic concepts.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言生成部分产生具有语义概念的描述。
- en: A typical block diagram of this category is shown in Figure 8.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的典型框图如图 8 所示。
- en: Karpathy et al. extended their method (Karpathy et al., [2014](#bib.bib67))
    in (Karpathy and Fei-Fei, [2015](#bib.bib66)). The later method can generate natural
    language descriptions for both images as well as for their regions. This method
    employs a novel combination of CNN over the image regions, bidirectional Recurrent
    Neural Networks over sentences, and a common multimodal embedding that associates
    the two modalities. It also demonstrates a multimodal recurrent neural network
    architecture that utilizes the resultant alignments to train the model for generating
    novel descriptions of image regions. In this method, dependency tree relations
    (DTR) are used to train to map the sentence segments with the image regions that
    have a fixed window context. In contrast to their previous method, this method
    uses a bidirectional neural network to obtain word representations in the sentence.
    It considers contiguous fragments of sentences to align in embedding space which
    is more meaningful, interpretable, and not fixed in length. Generally an RNN considers
    the current word and the contexts from all the previously generated words for
    estimating a probability distribution of the next word in a sequence. However,
    this method extends it for considering the generative process on the content of
    an input image. This addition is simple but it makes it very effective for generating
    novel image captions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Karpathy 等人将他们的方法（Karpathy et al., [2014](#bib.bib67)）扩展到（Karpathy and Fei-Fei,
    [2015](#bib.bib66)）。该方法可以为图像及其区域生成自然语言描述。该方法采用了一种新颖的组合，结合了图像区域上的 CNN、句子上的双向递归神经网络和一个共同的多模态嵌入，将这两种模态关联起来。它还展示了一种多模态递归神经网络架构，利用结果对齐来训练模型，以生成图像区域的新描述。在这种方法中，依赖树关系（DTR）用于训练，以将句子段映射到具有固定窗口上下文的图像区域。与他们之前的方法相比，这种方法使用双向神经网络来获取句子中的词表示。它考虑了句子的连续片段在嵌入空间中的对齐，这种对齐更有意义、可解释且长度不固定。一般来说，RNN
    考虑当前词和从所有先前生成的词中获得的上下文，以估计序列中下一个词的概率分布。然而，该方法扩展了这一过程，以考虑输入图像内容的生成过程。这个附加是简单的，但它使生成新图像描述非常有效。
- en: Attributes of an image are considered as rich semantic cues. The method of Yao
    et al. (Yao et al., [2017b](#bib.bib156)) has different architectures to incorporate
    attributes with image representations. Mainly, two types of architectural representations
    are introduced here. In the first group, it inserts only attributes to the LSTM
    or image representations to the LSTM first and then attributes and vice versa.
    In the second group, it can control the time step of LSTM. It decides whether
    image representation and attributes will be inputted once or every time step.
    These variants of architectures are tested on MS COCO dataset and common evaluation
    metrics.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的属性被视为丰富的语义线索。Yao 等人（Yao et al., [2017b](#bib.bib156)）的方法具有不同的架构来将属性与图像表示结合。主要介绍了两种类型的架构表示。在第一组中，它首先将属性插入
    LSTM 或图像表示到 LSTM，然后是属性，反之亦然。在第二组中，它可以控制 LSTM 的时间步长。它决定图像表示和属性是否每次时间步长都输入。这些架构的变体在
    MS COCO 数据集和常见评估指标上进行了测试。
- en: You et al. (You et al., [2016](#bib.bib157)) proposed a semantic attention-based
    image captioning method. The method provides a detailed, coherent description
    of semantically important objects. The top-down paradigms (Chen and Lawrence Zitnick,
    [2015](#bib.bib24); Vinyals et al., [2015](#bib.bib143); Mao et al., [2015b](#bib.bib95);
    Karpathy and Fei-Fei, [2015](#bib.bib66); Donahue et al., [2015](#bib.bib32);
    Xu et al., [2015](#bib.bib153); Mao et al., [2015a](#bib.bib94)) are used for
    extracting visual features first and then convert them into words. In bottom up
    approaches, (Farhadi et al., [2010](#bib.bib35); Kulkarni et al., [2011](#bib.bib75);
    Li et al., [2011](#bib.bib81); Elliott and Keller, [2013](#bib.bib33); Kuznetsova
    et al., [2012](#bib.bib77); Lebret et al., [2015](#bib.bib79)) visual concepts
    (e.g., regions, objects, and attributes) are extracted first from various aspects
    of an image and then combine them. Fine details of an image are often very important
    for generating a description of an image. Top- down approaches have limitations
    in obtaining fine details of the image. Bottom up approaches are capable of operating
    on any image resolution and therefore they can do work on fine details of the
    image. However, they have problems in formulating an end to end process. Therefore,
    semantic based attention model applied both top-down and bottom up approaches
    for generating image captions. In top-down approaches, the image features are
    obtained using the last 1024-dimensional convolutional layer of the GoogleNet
    (Szegedy et al., [2015](#bib.bib134)) CNN model. The visual concepts are collected
    using different non-parametric and parametric method. Nearest neighbour image
    retrieval technique is used for computing non-parametric visual concepts. Fully
    convolutional network (FCN) (Long et al., [2015](#bib.bib87)) is used to learn
    attribute from local patches for parametric attribute prediction. Although Xu
    et al. (Xu et al., [2015](#bib.bib153)) considered attention-based captioning,
    it works on fixed and pre-defined spatial location. However, this semantic attention-based
    method can work on any resolution and any location of the image. Moreover, this
    method also considers a feedback process that accelerates to generate better image
    captions.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尤等人（You et al., [2016](#bib.bib157)）提出了一种基于语义注意力的图像描述方法。该方法提供了对语义重要对象的详细且连贯的描述。自上而下的范式（Chen
    和 Lawrence Zitnick, [2015](#bib.bib24); Vinyals 等人, [2015](#bib.bib143); Mao 等人,
    [2015b](#bib.bib95); Karpathy 和 Fei-Fei, [2015](#bib.bib66); Donahue 等人, [2015](#bib.bib32);
    Xu 等人, [2015](#bib.bib153); Mao 等人, [2015a](#bib.bib94)）首先提取视觉特征，然后将其转化为词语。在自下而上的方法中，（Farhadi
    等人, [2010](#bib.bib35); Kulkarni 等人, [2011](#bib.bib75); Li 等人, [2011](#bib.bib81);
    Elliott 和 Keller, [2013](#bib.bib33); Kuznetsova 等人, [2012](#bib.bib77); Lebret
    等人, [2015](#bib.bib79)）视觉概念（例如，区域、对象和属性）首先从图像的不同方面提取，然后将它们结合起来。图像的细节通常对生成图像描述非常重要。自上而下的方法在获取图像细节方面存在局限性。自下而上的方法能够在任何图像分辨率下操作，因此可以处理图像的细节。然而，它们在制定端到端的过程中存在问题。因此，基于语义的注意力模型结合了自上而下和自下而上的方法来生成图像描述。在自上而下的方法中，图像特征是通过GoogleNet（Szegedy
    等人, [2015](#bib.bib134)）CNN模型的最后一个1024维卷积层获得的。视觉概念使用不同的非参数和参数方法收集。最近邻图像检索技术用于计算非参数视觉概念。全卷积网络（FCN）（Long
    等人, [2015](#bib.bib87)）用于从局部图像块中学习属性以进行参数化属性预测。尽管Xu 等人（Xu et al., [2015](#bib.bib153)）考虑了基于注意力的描述，但它在固定和预定义的空间位置上工作。然而，这种基于语义的注意力方法可以在任何分辨率和图像的任何位置上工作。此外，该方法还考虑了加速生成更好图像描述的反馈过程。
- en: Previous image captioning methods do not include high level semantic concepts
    explicitly. However, Wu et al. (Wu16W) proposed a high-level semantic concept-based
    image captioning. It uses an intermediate attribute prediction layer in a neural
    network-based CNN-LSTM framework. First, attributes are extracted by a CNN-based
    classifier from training image captions. Then these attributes are used as high
    level semantic concepts in generating semantically rich image captions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图像描述方法没有明确包含高级语义概念。然而，吴等人（Wu16W）提出了一种基于高级语义概念的图像描述方法。它在基于神经网络的CNN-LSTM框架中使用了一个中间属性预测层。首先，属性由基于CNN的分类器从训练图像描述中提取。然后，这些属性作为高级语义概念用于生成语义丰富的图像描述。
- en: '![Refer to caption](img/f627b63366c07df17dabb5aaad1ddf8f.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f627b63366c07df17dabb5aaad1ddf8f.png)'
- en: Figure 9\. A block diagram of a typical novel object-based image captioning.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 一种典型的新型对象基础图像描述的框图。
- en: Recent semantic concept based image captioning methods (Wu et al., [2018](#bib.bib151);
    You et al., [2016](#bib.bib157)) applied semantic-concept-detection process (Gan
    et al., [2016](#bib.bib41))to obtain explicit semantic concepts. They use these
    high level semantic concepts in CNN-LSTM based encoder-decoder and achieves significant
    improvements in image captioning. However, they have problems in generating semantically
    sound captions. They cannot distribute semantic concepts evenly in the whole sentence.
    For example, Wu et al. (Wu et al., [2018](#bib.bib151)) consider the initial state
    of the LSTM to add semantic concepts. Moreover, it encodes visual features vector
    or an inferred scene vector from the CNN and then feeds them to LSTM for generating
    captions. However, Gan et al. (Gan et al., [2017b](#bib.bib42)) introduced a Semantic
    Compositional Network (SCN) for image captioning. In this method, a semantic concept
    vector is constructed from all the probable concepts (called tags here) found
    in the image. This semantic vector has more potential than visual feature vector
    and scene vector and can generate captions covering the overall meaning of the
    image. This is called compositional network because it can compose most semantic
    concepts.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最近基于语义概念的图像描述方法（Wu et al., [2018](#bib.bib151); You et al., [2016](#bib.bib157)）应用了语义概念检测过程（Gan
    et al., [2016](#bib.bib41)）以获取明确的语义概念。他们在基于CNN-LSTM的编码器-解码器中使用这些高级语义概念，并在图像描述中取得了显著的改进。然而，他们在生成语义上合理的描述时遇到问题。他们无法在整个句子中均匀分布语义概念。例如，Wu
    et al.（Wu et al., [2018](#bib.bib151)）考虑在LSTM的初始状态中添加语义概念。此外，它将视觉特征向量或从CNN推断出的场景向量编码后输入LSTM生成描述。然而，Gan
    et al.（Gan et al., [2017b](#bib.bib42)）引入了一个语义组成网络（SCN）用于图像描述。在这种方法中，一个语义概念向量是由图像中发现的所有可能概念（这里称为标签）构建的。这个语义向量比视觉特征向量和场景向量更有潜力，可以生成涵盖图像整体意义的描述。这被称为组成网络，因为它可以组合大多数语义概念。
- en: 'Existing LSTM based image captioning methods have limitations in generating
    a diverse set of captions because they have to predict the next word on a predefined
    word by word format. However, a combination of attributes, subjects and their
    relationship in a sentence irrespective of their location can generate a broad
    range of image captions. Wang et al. (Wang et al., [2017](#bib.bib149)) proposed
    a method that locates the objects and their interactions first and then identifies
    and extracts the relevant attributes to generate image captions. The main aim
    of this method is to decompose the ground truth image captions into two parts:
    Skeleton sentence and attribute phrases. The method is also called Skeleton Key.
    The architecture of this method has ResNet (He et al., [2016](#bib.bib54)) and
    two LSTMs called Skel-LSTM and Attr-LSTM. During training, skeleton sentences
    are trained by Skel-LSTM network and attribute phrases are trained by the Attr-LSTM
    network. In the testing phase, skeleton sentences are generated first that contain
    the words for main objects of the image and their relationships. Then these objects
    look back through the image again to obtain the relevant attributes. It is tested
    on MS COCO dataset and a new Stock3M dataset and can generate more accurate and
    novel captions.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的基于LSTM的图像描述方法在生成多样化描述方面存在局限，因为它们必须按照预定义的逐词格式预测下一个词。然而，句子中属性、主题及其关系的组合可以生成广泛的图像描述。Wang
    et al.（Wang et al., [2017](#bib.bib149)）提出了一种方法，首先定位物体及其交互，然后识别并提取相关属性以生成图像描述。这种方法的主要目的是将真实图像描述分解为两个部分：骨架句子和属性短语。这种方法也被称为Skeleton
    Key。该方法的架构包括ResNet（He et al., [2016](#bib.bib54)）和两个LSTM，分别称为Skel-LSTM和Attr-LSTM。在训练过程中，骨架句子由Skel-LSTM网络进行训练，属性短语由Attr-LSTM网络进行训练。在测试阶段，首先生成包含图像主要物体及其关系的骨架句子。然后这些物体会重新查看图像以获取相关属性。该方法在MS
    COCO数据集和新的Stock3M数据集上进行了测试，能够生成更准确和新颖的描述。
- en: 3.5.3\. Novel Object-based Image Captioning
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3. 新颖的基于物体的图像描述
- en: Despite recent deep learning-based image captioning methods have achieved promising
    results, they largely depend on the paired image and sentence caption datasets.
    These type of methods can only generate description of the objects within the
    context. Therefore, the methods require a large set of training image-sentence
    pairs. Novel object-based image captioning methods can generate descriptions of
    novel objects which are not present in paired image-captions datasets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来基于深度学习的图像描述方法取得了令人满意的结果，但它们在很大程度上依赖于配对的图像和句子描述数据集。这些方法只能生成上下文中对象的描述。因此，这些方法需要大量的训练图像-句子对。基于新颖对象的图像描述方法可以生成在配对图像-描述数据集中不存在的新颖对象的描述。
- en: 'The methods of this category follow the following general steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法遵循以下一般步骤：
- en: (1)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: A separate lexical classifier and a language model are trained on unpaired image
    data and unpaired text data.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在未配对的图像数据和未配对的文本数据上训练一个单独的词汇分类器和语言模型。
- en: (2)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: A deep caption model is trained on paired image caption data.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在配对的图像描述数据上训练一个深度描述模型。
- en: (3)
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Finally, both models are combined together to train jointly in that can generate
    captions for novel object.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终，将两个模型结合起来进行联合训练，以生成新颖对象的描述。
- en: '![Refer to caption](img/75d17d71f80404c668aa9536b1e0f822.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/75d17d71f80404c668aa9536b1e0f822.png)'
- en: Figure 10\. A block diagram of image captioning based on different styles.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 基于不同风格的图像描述块图。
- en: A simple block diagram of a novel object-based image captioning method is given
    in Figure 9.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9 给出了基于新颖对象的图像描述方法的简单块图。
- en: Current image captioning methods are trained on image-captions paired datasets.
    As a result, if they get unseen objects in the test images, they cannot present
    them in their generated captions. Anne et al. (Anne Hendricks et al., [2016](#bib.bib7))
    proposed a Deep Compositional Captioner (DCC) that can represent the unseen objects
    in generated captions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的图像描述方法是在图像-描述配对数据集上训练的。因此，如果它们在测试图像中遇到未见过的对象，就无法在生成的描述中呈现这些对象。Anne 等人（Anne
    Hendricks 等，[2016](#bib.bib7)）提出了一种深度组合描述生成器（DCC），可以在生成的描述中表示未见过的对象。
- en: Yao et al. (Yao et al., [2017a](#bib.bib155)) proposed a copying mechanism to
    generate description for novel objects. This method uses a separate object recognition
    dataset to develop classifiers for novel objects. It integrates the appropriate
    words in the output captions by a decoder RNN with copying mechanism. The architecture
    of the method adds a new network to recognize the unseen objects from unpaired
    images and incorporate them with LSTM to generate captions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 姚等人（姚等，[2017a](#bib.bib155)）提出了一种复制机制来生成新颖对象的描述。这种方法使用一个单独的对象识别数据集来开发新颖对象的分类器。它通过一个具有复制机制的解码器
    RNN 将适当的词语整合到输出的描述中。该方法的架构增加了一个新网络，用于识别来自未配对图像的未知对象，并将其与 LSTM 结合生成描述。
- en: Generating captions for the unseen images is a challenging research problem.
    Venugopalan et al. (Venugopalan et al., [2017](#bib.bib141)) introduced a Novel
    Object Captioner (NOC) for generating captions for unseen objects in the image.
    They used external sources for recognizing unseen objects and learning semantic
    knowledge.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为未见过的图像生成描述是一个具有挑战性的研究问题。Venugopalan 等人（Venugopalan 等，[2017](#bib.bib141)）介绍了一种新颖对象描述生成器（NOC），用于为图像中的未见对象生成描述。他们使用外部资源来识别未见对象并学习语义知识。
- en: 3.5.4\. Stylized Caption
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4\. 风格化描述
- en: Existing image captioning systems generate captions just based on only the image
    content that can also be called factual description. They do not consider the
    stylized part of the text separately from other linguistic patterns. However,
    the stylized captions can be more expressive and attractive than just only the
    flat description of an image.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的图像描述系统仅根据图像内容生成描述，这也可以称为事实描述。它们没有单独考虑文本的风格化部分与其他语言模式。然而，风格化的描述可能比仅仅平铺的图像描述更具表现力和吸引力。
- en: 'The methods of this category follow the following general steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法遵循以下一般步骤：
- en: (1)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: CNN based image encoder is used to obtain the image information.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用基于 CNN 的图像编码器获取图像信息。
- en: (2)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'A separate text corpus is prepared to extract various stylized concepts (For
    example: romantic, humorous) from training data.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准备一个单独的文本语料库，从训练数据中提取各种风格化的概念（例如：浪漫、幽默）。
- en: (3)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: The language generation part can generate stylized and attractive captions using
    the information of Step 1 and Step 2.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言生成部分可以使用步骤 1 和步骤 2 的信息生成风格化和吸引人的描述。
- en: A simple block diagram of stylized image captioning is given in Figure 10.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图像10中给出了风格化图像描述的简单框图。
- en: Such captions have become popular because they are particularly valuable for
    many real-world applications. For example, everyday people are uploading a lot
    of photos in different social media. The photos need stylized and attractive descriptions.
    Gan et al. (Gan et al., [2017a](#bib.bib40)) proposed a novel image captioning
    system called StyleNet. This method can generate attractive captions adding various
    styles. The architecture of this method consists of a CNN and a factored LSTM
    that can separate factual and style factors from the captions. It uses multitask
    sequence to sequence training (Luong et al., [2016](#bib.bib90)) for identifying
    the style factors and then add these factors at run time for generating attractive
    captions. More interestingly, it uses an external monolingual stylized language
    corpus for training instead of paired images. However, it uses a new stylized
    image caption dataset called FlickrStyle10k and can generate captions with different
    styles.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的描述变得流行，因为它们对许多现实世界应用特别有价值。例如，日常生活中，人们在不同的社交媒体上上传大量照片。这些照片需要风格化和吸引人的描述。Gan等（Gan
    et al., [2017a](#bib.bib40)）提出了一种新颖的图像描述系统，称为StyleNet。该方法可以生成具有各种风格的吸引人描述。该方法的架构包括一个CNN和一个可以将事实性和风格因素从描述中分离开的LSTM。它使用多任务序列到序列训练（Luong
    et al., [2016](#bib.bib90)）来识别风格因素，然后在运行时添加这些因素以生成吸引人的描述。更有趣的是，它使用一个外部的单语风格化语言语料库进行训练，而不是配对图像。然而，它使用了一个新的风格化图像描述数据集FlickrStyle10k，并可以生成不同风格的描述。
- en: Existing image captioning methods consider the factual description about the
    objects, scene, and their interactions of an image in generating image captions.
    In our day to day conversations, communications, interpersonal relationships,
    and decision making we use various stylized and non-factual expressions such as
    emotions, pride, and shame. However, Mathews et al. (Mathews et al., [2016](#bib.bib98))
    claimed that automatic image descriptions are missing this non-factual aspects.
    Therefore, they proposed a method called SentiCap. This method can generate image
    descriptions with positive or negative sentiments. It introduces a novel switching
    RNN model that combines two CNN+RNNs running in parallel. In each time step, this
    switching model generates the probability of switching between two RNNs. One generates
    captions considering the factual words and other considers the words with sentiments.
    It then takes inputs from the hidden states of both two RNNs for generating captions.
    This method can generate captions successfully given the appropriate sentiments.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的图像描述方法在生成图像描述时会考虑图像中对象、场景及其相互作用的事实描述。在我们日常的对话、交流、人际关系和决策中，我们会使用各种风格化和非事实性的表达，如情感、自豪感和羞耻感。然而，Mathews等（Mathews
    et al., [2016](#bib.bib98)）声称自动图像描述缺乏这些非事实性方面。因此，他们提出了一种叫做SentiCap的方法。该方法可以生成带有正面或负面情感的图像描述。它引入了一种新颖的切换RNN模型，该模型结合了两个并行运行的CNN+RNN。在每个时间步，该切换模型生成在两个RNN之间切换的概率。其中一个RNN生成考虑事实性词汇的描述，而另一个RNN则考虑带有情感的词汇。然后，该方法从两个RNN的隐藏状态中获取输入以生成描述。这个方法可以在适当的情感条件下成功生成描述。
- en: 3.6\. LSTM vs. Others
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. LSTM与其他模型
- en: Image captioning intersects computer vision and natural language processing
    (NLP) research. NLP tasks, in general, can be formulated as a sequence to sequence
    learning. Several neural language models such as neural probabilistic language
    model (Bengio et al., [2003](#bib.bib12)), log-bilinear models (Mnih and Hinton,
    [2007b](#bib.bib106)), skip-gram models (Mikolov et al., [2013](#bib.bib99)),
    and recurrent neural networks (RNNs) (Mikolov et al., [2010](#bib.bib100)) have
    been proposed for learning sequence to sequence tasks. RNNs have widely been used
    in various sequence learning tasks. However, traditional RNNs suffer from vanishing
    and exploding gradient problems and cannot adequately handle long-term temporal
    dependencies.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述交叉了计算机视觉和自然语言处理（NLP）研究。NLP任务通常可以被表述为序列到序列学习。已经提出了几种神经语言模型，如神经概率语言模型（Bengio
    et al., [2003](#bib.bib12)）、对数双线性模型（Mnih和Hinton, [2007b](#bib.bib106)）、跳字模型（Mikolov
    et al., [2013](#bib.bib99)）和递归神经网络（RNNs）（Mikolov et al., [2010](#bib.bib100)），用于学习序列到序列任务。RNNs被广泛应用于各种序列学习任务。然而，传统RNN存在梯度消失和梯度爆炸问题，无法充分处理长期时间依赖。
- en: LSTM (Hochreiter and Schmidhuber, [1997](#bib.bib55)) networks are a type of
    RNN that has special units in addition to standard units. LSTM units use a memory
    cell that can maintain information in memory for long periods of time. In recent
    years, LSTM based models have dominantly been used in sequence to sequence learning
    tasks. Another network, Gated Recurrent Unit (GRU) (Chung et al., [2014](#bib.bib26))
    has a similar structure to LSTM but it does not use separate memory cells and
    uses fewer gates to control the flow of information.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（Hochreiter和Schmidhuber，[1997](#bib.bib55)）网络是一种RNN，除了标准单元外还有特殊单元。LSTM单元使用可以长时间保持信息的存储单元。近年来，基于LSTM的模型在序列到序列学习任务中占主导地位。另一个网络，门控循环单元（GRU）（Chung等人，[2014](#bib.bib26)），结构类似于LSTM，但不使用单独的存储单元，使用较少的门来控制信息流动。
- en: However, LSTMs ignore the underlying hierarchical structure of a sentence. They
    also require significant storage due to long-term dependencies through a memory
    cell. In contrast, CNNs can learn the internal hierarchical structure of the sentences
    and they are faster in processing than LSTMs. Therefore, recently, convolutional
    architectures are used in other sequence to sequence tasks, e.g., conditional
    image generation (van den Oord et al., [2016](#bib.bib138)) and machine translation
    (Gehring et al., [2016](#bib.bib43); Gehring et al., [2017](#bib.bib44); Vaswani
    et al., [2017](#bib.bib139)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LSTM忽略了句子的基础层次结构。由于通过存储单元存在长期依赖，它们还需要大量存储空间。相比之下，CNN可以学习句子的内部层次结构，并且它们在处理速度上比LSTM更快。因此，最近在其他序列到序列任务中使用了卷积架构，例如条件图像生成（van den
    Oord等人，[2016](#bib.bib138)）和机器翻译（Gehring等人，[2016](#bib.bib43); Gehring等人，[2017](#bib.bib44);
    Vaswani等人，[2017](#bib.bib139))。
- en: 'Inspired by the above success of CNNs in sequence learning tasks, Gu et al.
    (Gu et al., [2017](#bib.bib52)) proposed a CNN language model-based image captioning
    method. This method uses a language-CNN for statistical language modelling. However,
    the method cannot model the dynamic temporal behaviour of the language model only
    using a language-CNN. It combines a recurrent network with the language-CNN to
    model the temporal dependencies properly. Aneja et al. (Aneja et al., [2018](#bib.bib6))
    proposed a convolutional architecture for the task of image captioning. They use
    a feed-forward network without any recurrent function. The architecture of the
    method has four components: (i) input embedding layer (ii) image embedding layer
    (iii) convolutional module, and (iv) output embedding layer. It also uses an attention
    mechanism to leverage spatial image features. They evaluate their architecture
    on the challenging MSCOCO dataset and shows comparable performance to an LSTM
    based method on standard metrics.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 受CNN在序列学习任务中的成功启发，顾等人（Gu等人，[2017](#bib.bib52)）提出了一种基于CNN语言模型的图像字幕方法。该方法使用语言-CNN进行统计语言建模。然而，该方法无法仅使用语言-CNN来建模语言模型的动态时间行为。它结合了循环网络和语言-CNN来正确建模时间依赖关系。Aneja等人（Aneja等人，[2018](#bib.bib6)）针对图像字幕任务提出了一种卷积架构。他们使用了一个不带任何循环功能的前向网络。该方法的架构有四个组成部分：(i)
    输入嵌入层 (ii) 图像嵌入层 (iii) 卷积模块，和 (iv) 输出嵌入层。它还使用注意机制来利用空间图像特征。他们在具有挑战性的MSCOCO数据集上评估他们的架构，并展示了与基于LSTM的方法在标准指标上可比的性能。
- en: Wang et al. (Wang and Chan, [2018](#bib.bib148)) proposed another CNN+CNN based
    image captioning method. It is similar to the method of Aneja et al. except that
    it uses a hierarchical attention module to connect the vision-CNN with the language-CNN.
    The authors of this method also investigate the use of various hyperparameters,
    including the number of layers and the kernel width of the language-CNN. They
    show that the influence of the hyperparameters can improve the performance of
    the method in image captioning.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人（Wang和Chan，[2018](#bib.bib148)）提出了另一种基于CNN+CNN的图像字幕方法。它类似于Aneja等人的方法，不同之处在于它使用了一个分层注意模块来连接视觉-CNN和语言-CNN。该方法的作者还研究了包括语言-CNN的层数和核宽度在内的各种超参数的使用。他们表明，超参数的影响可以提高图像字幕方法的性能。
- en: 4\. Datasets and Evaluation Metrics
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 数据集和评估指标
- en: 'A number of datasets are used for training, testing, and evaluation of the
    image captioning methods. The datasets differ in various perspective such as the
    number of images, the number of captions per image, format of the captions, and
    image size. Three datasets: Flickr8k (Hodosh et al., [2013](#bib.bib56)), Flickr30k
    (Plummer et al., [2015](#bib.bib114)), and MS COCO Dataset (Lin et al., [2014](#bib.bib84))
    are popularly used. These datasets together with others are described in Section
    4.1\. In this section, we show sample images with their captions generated by
    image captioning methods on MS COCO, Flickr30k, and Flickr8k datasets. A number
    of evaluation metrics are used to measure the quality of the generated captions
    compared to the ground-truth. Each metric applies its own technique for computation
    and has distinct advantages. The commonly used evaluation metrics are discussed
    in Section 4.2\. A summary of deep learning-based image captioning methods with
    their datasets and evaluation metrics are listed in Table 2.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、测试和评估图像标注方法使用了多个数据集。这些数据集在多个方面有所不同，如图像数量、每张图像的标注数量、标注格式和图像大小。三个数据集：Flickr8k（Hodosh
    等，[2013](#bib.bib56)）、Flickr30k（Plummer 等，[2015](#bib.bib114)）和 MS COCO 数据集（Lin
    等，[2014](#bib.bib84)）被广泛使用。这些数据集及其他数据集在第 4.1 节中进行了描述。在本节中，我们展示了在 MS COCO、Flickr30k
    和 Flickr8k 数据集上由图像标注方法生成的示例图像及其标注。使用了多个评估指标来衡量生成标注与真实标注的质量。每个指标采用自己的计算技术，并具有独特的优势。常用的评估指标在第
    4.2 节中进行了讨论。基于深度学习的图像标注方法及其数据集和评估指标的总结列在表 2 中。
- en: '![Refer to caption](img/c75cf5288668855022b948fbf01c528e.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c75cf5288668855022b948fbf01c528e.png)'
- en: Figure 11\. Captions generated by Wu et al. (Wu et al., [2015](#bib.bib150))
    on some sample images from the MS COCO dataset.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. Wu 等（Wu 等，[2015](#bib.bib150)）在 MS COCO 数据集的一些示例图像上生成的标注。
- en: 4.1\. Datasets
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 数据集
- en: 4.1.1\. MS COCO Dataset
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. MS COCO 数据集
- en: Microsoft COCO Dataset (Lin et al., [2014](#bib.bib84)) is a very large dataset
    for image recognition, segmentation, and captioning. There are various features
    of MS COCO dataset such as object segmentation, recognition in context, multiple
    objects per class, more than 300,000 images, more than 2 million instances, 80
    object categories, and 5 captions per image. Many image captioning methods (Jin
    et al., [2015](#bib.bib62); Wu and Cohen, [2016](#bib.bib152); Tran et al., [2016](#bib.bib136);
    Wang et al., [2016b](#bib.bib145); You et al., [2016](#bib.bib157); Gan et al.,
    [2017a](#bib.bib40); Pedersoli et al., [2017](#bib.bib113); Ren et al., [2017](#bib.bib120);
    Dai et al., [2017](#bib.bib27); Shetty et al., [2017](#bib.bib127); Wu et al.,
    [2015](#bib.bib150)) use the dataset in their experiments. For example, Wu et
    al. (Wu et al., [2015](#bib.bib150)) use MS COCO dataset in their method and the
    generated captions of two sample images are shown in Figure 11.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft COCO 数据集（Lin 等，[2014](#bib.bib84)）是一个非常大的图像识别、分割和标注的数据集。MS COCO 数据集具有多种特性，如对象分割、上下文中的识别、每类多个对象、超过
    300,000 张图像、超过 200 万个实例、80 个对象类别以及每张图像 5 个标注。许多图像标注方法（Jin 等，[2015](#bib.bib62)；Wu
    和 Cohen，[2016](#bib.bib152)；Tran 等，[2016](#bib.bib136)；Wang 等，[2016b](#bib.bib145)；You
    等，[2016](#bib.bib157)；Gan 等，[2017a](#bib.bib40)；Pedersoli 等，[2017](#bib.bib113)；Ren
    等，[2017](#bib.bib120)；Dai 等，[2017](#bib.bib27)；Shetty 等，[2017](#bib.bib127)；Wu
    等，[2015](#bib.bib150)）在他们的实验中使用了该数据集。例如，Wu 等（Wu 等，[2015](#bib.bib150)）在他们的方法中使用了
    MS COCO 数据集，两个示例图像生成的标注如图 11 所示。
- en: 4.1.2\. Flickr30K Dataset
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. Flickr30K 数据集
- en: Flickr30K (Plummer et al., [2015](#bib.bib114)) is a dataset for automatic image
    description and grounded language understanding. It contains 30k images collected
    from Flickr with 158k captions provided by human annotators. It does not provide
    any fixed split of images for training, testing, and validation. Researchers can
    choose their own choice of numbers for training, testing, and validation. The
    dataset also contains detectors for common objects, a color classifier, and a
    bias towards selecting larger objects. Image captioning methods such as (Karpathy
    and Fei-Fei, [2015](#bib.bib66); Vinyals et al., [2015](#bib.bib143); Wang et al.,
    [2016b](#bib.bib145); Wu et al., [2018](#bib.bib151); Chen et al., [2017a](#bib.bib23))
    use this dataset for their experiments. For example, performed their experiment
    on Flickr30k dataset. The generated captions by Chen et al. (Chen et al., [2017a](#bib.bib23))
    of two sample images of the dataset are shown in Figure 12.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr30K (Plummer 等人, [2015](#bib.bib114)) 是一个用于自动图像描述和基础语言理解的数据集。它包含从 Flickr
    收集的 30k 张图像，配有 158k 张由人工标注者提供的说明。该数据集未提供任何固定的图像划分用于训练、测试和验证。研究人员可以根据自己的需要选择训练、测试和验证的数量。该数据集还包括常见物体的检测器、一个颜色分类器，以及偏向选择较大物体的倾向。图像字幕生成方法如
    (Karpathy 和 Fei-Fei, [2015](#bib.bib66); Vinyals 等人, [2015](#bib.bib143); Wang
    等人, [2016b](#bib.bib145); Wu 等人, [2018](#bib.bib151); Chen 等人, [2017a](#bib.bib23))
    使用此数据集进行实验。例如，在 Flickr30k 数据集上进行实验。Chen 等人 (Chen 等人, [2017a](#bib.bib23)) 生成的该数据集两张示例图像的说明如图
    12 所示。
- en: '| \multirow2*Reference | \multirow2*Datasets | \multirow2*Evaluation Metrics
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| \multirow2*参考文献 | \multirow2*数据集 | \multirow2*评估指标 |'
- en: '| --- | --- | --- |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kiros et al. 2014 (Kiros et al., [2014a](#bib.bib70)) | IAPR TC-12,SBU |
    BLEU, PPLX |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Kiros 等人 2014 (Kiros 等人, [2014a](#bib.bib70)) | IAPR TC-12, SBU | BLEU, PPLX
    |'
- en: '| Kiros et al. 2014 (Kiros et al., [2014b](#bib.bib71)) | Flickr 8K, Flickr
    30K | R@K, mrank |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Kiros 等人 2014 (Kiros 等人, [2014b](#bib.bib71)) | Flickr 8K, Flickr 30K | R@K,
    mrank |'
- en: '| Mao et al. 2014  (Mao et al., [2014](#bib.bib96)) | IAPR TC-12, Flickr 8K/30K
    | BLEU, R@K, mrank |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Mao 等人 2014 (Mao 等人, [2014](#bib.bib96)) | IAPR TC-12, Flickr 8K/30K | BLEU,
    R@K, mrank |'
- en: '| Karpathy et al. 2014 (Karpathy et al., [2014](#bib.bib67)) | PASCAL1K, Flickr
    8K/30K | R@K, mrank |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Karpathy 等人 2014 (Karpathy 等人, [2014](#bib.bib67)) | PASCAL1K, Flickr 8K/30K
    | R@K, mrank |'
- en: '| Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Mao 等人 2015 (Mao 等人, [2015b](#bib.bib95)) |'
- en: '&#124; IAPR TC-12, Flickr 8K/30K, &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IAPR TC-12, Flickr 8K/30K, &#124;'
- en: '&#124; MS COCO &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MS COCO &#124;'
- en: '| BLEU, R@K, mrank |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| BLEU, R@K, mrank |'
- en: '| Chen et al. 2015 (Chen and Lawrence Zitnick, [2015](#bib.bib24)) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 2015 (Chen 和 Lawrence Zitnick, [2015](#bib.bib24)) |'
- en: '&#124; PASCAL, Flickr 8K/30K, &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PASCAL, Flickr 8K/30K, &#124;'
- en: '&#124; MS COCO &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MS COCO &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BLEU, METEOR, &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BLEU, METEOR, &#124;'
- en: '&#124; CIDEr &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CIDEr &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Fang et al. 2015 (Fang et al., [2015](#bib.bib34)) | PASCAL, MS COCO | BLEU,
    METEOR, PPLX |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Fang 等人 2015 (Fang 等人, [2015](#bib.bib34)) | PASCAL, MS COCO | BLEU, METEOR,
    PPLX |'
- en: '| Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR, CIDEr |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Jia 等人 2015 (Jia 等人, [2015](#bib.bib60)) | Flickr 8K/30K, MS COCO | BLEU,
    METEOR, CIDEr |'
- en: '| Karpathy et al. 2015 (Karpathy and Fei-Fei, [2015](#bib.bib66)) | Flickr
    8K/30K, MS COCO | BLEU, METEOR, CIDEr |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Karpathy 等人 2015 (Karpathy 和 Fei-Fei, [2015](#bib.bib66)) | Flickr 8K/30K,
    MS COCO | BLEU, METEOR, CIDEr |'
- en: '| Vinyals et al. 2015 (Vinyals et al., [2015](#bib.bib143)) | Flickr 8K/30K,
    MS COCO | BLEU, METEOR, CIDEr |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Vinyals 等人 2015 (Vinyals 等人, [2015](#bib.bib143)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR, CIDEr |'
- en: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 2015 (Xu 等人, [2015](#bib.bib153)) | Flickr 8K/30K, MS COCO | BLEU,
    METEOR |'
- en: '| Jin et al. 2015 (Jin et al., [2015](#bib.bib62)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Jin 等人 2015 (Jin 等人, [2015](#bib.bib62)) | Flickr 8K/30K, MS COCO | BLEU,
    METEOR, ROUGE, CIDEr |'
- en: '| Wu et al. 2016 (Wu and Cohen, [2016](#bib.bib152)) | MS COCO | BLEU, METEOR,
    CIDEr |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 2016 (Wu 和 Cohen, [2016](#bib.bib152)) | MS COCO | BLEU, METEOR, CIDEr
    |'
- en: '| Sugano et at. 2016 (Sugano and Bulling, [2016](#bib.bib130)) | MS COCO |
    BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Sugano 等人 2016 (Sugano 和 Bulling, [2016](#bib.bib130)) | MS COCO | BLEU,
    METEOR, ROUGE, CIDEr |'
- en: '| Mathews et al. 2016 (Mathews et al., [2016](#bib.bib98)) | MS COCO, SentiCap
    | BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Mathews 等人 2016 (Mathews 等人, [2016](#bib.bib98)) | MS COCO, SentiCap | BLEU,
    METEOR, ROUGE, CIDEr |'
- en: '| Wang et al. 2016 (Wang et al., [2016b](#bib.bib145)) | Flickr 8K/30K, MS
    COCO | BLEU, R@K |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2016 (Wang 等人, [2016b](#bib.bib145)) | Flickr 8K/30K, MS COCO | BLEU,
    R@K |'
- en: '| Johnson et al. 2016 (Johnson et al., [2016](#bib.bib63)) | Visual Genome
    | METEOR, AP, IoU |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Johnson 等人 2016 (Johnson 等人, [2016](#bib.bib63)) | Visual Genome | METEOR,
    AP, IoU |'
- en: '| Mao et al. 2016 (Mao et al., [2016](#bib.bib93)) | ReferIt | BLEU, METEOR,
    CIDEr |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Mao 等人 2016 (Mao 等人, [2016](#bib.bib93)) | ReferIt | BLEU, METEOR, CIDEr
    |'
- en: '| Wang et al. 2016 (Wang et al., [2016a](#bib.bib147)) | Flickr 8K | BLEU,
    PPL, METEOR |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2016 (Wang 等人，[2016a](#bib.bib147)) | Flickr 8K | BLEU, PPL, METEOR
    |'
- en: '| Tran et al. 2016 (Tran et al., [2016](#bib.bib136)) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Tran 等人 2016 (Tran 等人，[2016](#bib.bib136)) |'
- en: '&#124; MS COCO, Adobe-MIT, &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MS COCO, Adobe-MIT, &#124;'
- en: '&#124; Instagram &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Instagram &#124;'
- en: '| Human Evaluation |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 人类评估 |'
- en: '| Ma et al. 2016 (Ma and Han, [2016](#bib.bib91)) | Flickr 8k, UIUC | BLEU,
    R@K |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Ma 等人 2016 (Ma 和 Han，[2016](#bib.bib91)) | Flickr 8k, UIUC | BLEU, R@K |'
- en: '| You et al. 2016 (You et al., [2016](#bib.bib157)) | Flickr 30K, MS COCO |
    BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| You 等人 2016 (You 等人，[2016](#bib.bib157)) | Flickr 30K, MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
- en: '| Yang et al. 2016 (Yang et al., [2016](#bib.bib154)) | Visual Genome | METEOR,
    AP, IoU |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 2016 (Yang 等人，[2016](#bib.bib154)) | Visual Genome | METEOR, AP,
    IoU |'
- en: '| Anne et al. 2016 (Anne Hendricks et al., [2016](#bib.bib7)) | MS COCO, ImageNet
    | BLEU, METEOR |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Anne 等人 2016 (Anne Hendricks 等人，[2016](#bib.bib7)) | MS COCO, ImageNet |
    BLEU, METEOR |'
- en: '| Yao et al. 2017 (Yao et al., [2017b](#bib.bib156)) | MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Yao 等人 2017 (Yao 等人，[2017b](#bib.bib156)) | MS COCO | BLEU, METEOR, ROUGE,
    CIDEr |'
- en: '| Lu et al. 2017 (Lu et al., [2017](#bib.bib89)) | Flickr 30K, MS COCO | BLEU,
    METEOR, CIDEr |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等人 2017 (Lu 等人，[2017](#bib.bib89)) | Flickr 30K, MS COCO | BLEU, METEOR,
    CIDEr |'
- en: '| Chen et al. 2017 (Chen et al., [2017b](#bib.bib22)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 2017 (Chen 等人，[2017b](#bib.bib22)) | Flickr 8K/30K, MS COCO | BLEU,
    METEOR, ROUGE, CIDEr |'
- en: '| Gan et al. 2017 (Gan et al., [2017b](#bib.bib42)) | Flickr 30K, MS COCO |
    BLEU, METEOR, CIDEr |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Gan 等人 2017 (Gan 等人，[2017b](#bib.bib42)) | Flickr 30K, MS COCO | BLEU, METEOR,
    CIDEr |'
- en: '| Pedersoli et al. 2017 (Pedersoli et al., [2017](#bib.bib113)) | MS COCO |
    BLEU, METEOR, CIDEr |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Pedersoli 等人 2017 (Pedersoli 等人，[2017](#bib.bib113)) | MS COCO | BLEU, METEOR,
    CIDEr |'
- en: '| Ren et al. 2017 (Ren et al., [2017](#bib.bib120)) | MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Ren 等人 2017 (Ren 等人，[2017](#bib.bib120)) | MS COCO | BLEU, METEOR, ROUGE,
    CIDEr |'
- en: '| Park et al. 2017 (Park et al., [2017](#bib.bib112)) | Instagram | BLEU, METEOR,
    ROUGE, CIDEr |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Park 等人 2017 (Park 等人，[2017](#bib.bib112)) | Instagram | BLEU, METEOR, ROUGE,
    CIDEr |'
- en: '| Wang et al. 2017 (Wang et al., [2017](#bib.bib149)) | MS COCO, Stock3M |
    SPICE, METEOR, ROUGE, CIDEr |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2017 (Wang 等人，[2017](#bib.bib149)) | MS COCO, Stock3M | SPICE, METEOR,
    ROUGE, CIDEr |'
- en: '| Tavakoli et al. 2017 (Tavakoli et al., [2017](#bib.bib135)) | MS COCO, PASCAL
    50S | BLEU, METEOR, ROUGE, CIDEr |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Tavakoli 等人 2017 (Tavakoli 等人，[2017](#bib.bib135)) | MS COCO, PASCAL 50S
    | BLEU, METEOR, ROUGE, CIDEr |'
- en: '| Liu et al. 2017 (Liu et al., [2017a](#bib.bib85)) | Flickr 30K, MS COCO |
    BLEU, METEOR |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 2017 (Liu 等人，[2017a](#bib.bib85)) | Flickr 30K, MS COCO | BLEU, METEOR
    |'
- en: '| Gan et al. 2017 (Gan et al., [2017a](#bib.bib40)) | FlickrStyle10K | BLEU,
    METEOR, ROUGE, CIDEr |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Gan 等人 2017 (Gan 等人，[2017a](#bib.bib40)) | FlickrStyle10K | BLEU, METEOR,
    ROUGE, CIDEr |'
- en: '| Dai et al. 2017 (Dai et al., [2017](#bib.bib27)) | Flickr 30K, MS COCO |
    E-NGAN, E-GAN, SPICE, CIDEr |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 2017 (Dai 等人，[2017](#bib.bib27)) | Flickr 30K, MS COCO | E-NGAN, E-GAN,
    SPICE, CIDEr |'
- en: '| Shetty et al. 2017 (Shetty et al., [2017](#bib.bib127)) | MS COCO |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Shetty 等人 2017 (Shetty 等人，[2017](#bib.bib127)) | MS COCO |'
- en: '&#124; Human Evaluation, &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类评估, &#124;'
- en: '&#124; SPICE, METEOR &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SPICE, METEOR &#124;'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Liu et al. 2017 (Liu et al., [2017b](#bib.bib86)) | MS COCO | SPIDEr, Human
    Evaluation |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 2017 (Liu 等人，[2017b](#bib.bib86)) | MS COCO | SPIDEr, 人类评估 |'
- en: '| Gu et al. 2017 (Gu et al., [2017](#bib.bib52)) | Flickr 30K, MS COCO | BLEU,
    METEOR, CIDEr, SPICE |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Gu 等人 2017 (Gu 等人，[2017](#bib.bib52)) | Flickr 30K, MS COCO | BLEU, METEOR,
    CIDEr, SPICE |'
- en: '| Yao et al. 2017 (Yao et al., [2017a](#bib.bib155)) | MS COCO, ImageNet |
    METEOR |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Yao 等人 2017 (Yao 等人，[2017a](#bib.bib155)) | MS COCO, ImageNet | METEOR |'
- en: '| Rennie et al. 2017 (Rennie et al., [2017](#bib.bib121)) | MS COCO | BLEU,
    METEOR, CIDEr, ROUGE |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Rennie 等人 2017 (Rennie 等人，[2017](#bib.bib121)) | MS COCO | BLEU, METEOR,
    CIDEr, ROUGE |'
- en: '| Vsub et al. 2017 (Venugopalan et al., [2017](#bib.bib141)) | MS COCO, ImageNet
    | METEOR |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Vsub 等人 2017 (Venugopalan 等人，[2017](#bib.bib141)) | MS COCO, ImageNet | METEOR
    |'
- en: '| Zhang et al. 2017  (Zhang et al., [2017](#bib.bib162)) | MS COCO | BLEU,
    METEOR, ROUGE, CIDEr |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 2017  (Zhang 等人，[2017](#bib.bib162)) | MS COCO | BLEU, METEOR, ROUGE,
    CIDEr |'
- en: '| Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | Flickr 8K/30K, MS COCO
    | BLEU, METEOR, CIDEr |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 2018 (Wu 等人，[2018](#bib.bib151)) | Flickr 8K/30K, MS COCO | BLEU, METEOR,
    CIDEr |'
- en: '| Aneja et al. 2018 (Aneja et al., [2018](#bib.bib6)) | MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Aneja 等人 2018 (Aneja 等人，[2018](#bib.bib6)) | MS COCO | BLEU, METEOR, ROUGE,
    CIDEr |'
- en: '| Wang et al. 2018 (Wang and Chan, [2018](#bib.bib148)) | MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2018 (Wang 和 Chan，[2018](#bib.bib148)) | MS COCO | BLEU, METEOR,
    ROUGE, CIDEr |'
- en: Table 2\. An overview of methods, datasets, and evaluation metrics
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2\. 方法、数据集和评估指标概述
- en: '![Refer to caption](img/0f13eff8062dbe6372b06ccb860a6ec0.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0f13eff8062dbe6372b06ccb860a6ec0.png)'
- en: Figure 12\. Captions generated by Chen et al. (Chen et al., [2017a](#bib.bib23))
    on some sample images from the Flickr30k dataset.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. Chen et al. (Chen et al., [2017a](#bib.bib23)) 在 Flickr30k 数据集上的一些样本图像生成的描述。
- en: 4.1.3\. Flickr8K Dataset
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. Flickr8K 数据集
- en: Flickr8k (Hodosh et al., [2013](#bib.bib56)) is a popular dataset and has 8000
    images collected from Flickr. The training data consists of 6000 images, the test
    and development data, each consists of 1,000 images. Each image in the dataset
    has 5 reference captions annotated by humans. A number of image captioning methods
    (Jia et al., [2015](#bib.bib60); Jin et al., [2015](#bib.bib62); Xu et al., [2015](#bib.bib153);
    Wang et al., [2016b](#bib.bib145); Wu et al., [2018](#bib.bib151); Chen et al.,
    [2017b](#bib.bib22)) have performed experiments using the dataset. Two sample
    results by Jia et al. (Jia et al., [2015](#bib.bib60)) on this dataset are shown
    in Figure 13.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr8k (Hodosh et al., [2013](#bib.bib56)) 是一个流行的数据集，包含从 Flickr 收集的 8000 张图像。训练数据包括
    6000 张图像，测试和开发数据各包含 1000 张图像。数据集中的每张图像都有 5 个由人工标注的参考描述。许多图像描述方法 (Jia et al., [2015](#bib.bib60);
    Jin et al., [2015](#bib.bib62); Xu et al., [2015](#bib.bib153); Wang et al., [2016b](#bib.bib145);
    Wu et al., [2018](#bib.bib151); Chen et al., [2017b](#bib.bib22)) 已在该数据集上进行实验。Jia
    et al. (Jia et al., [2015](#bib.bib60)) 在该数据集上的两个样本结果如图 13 所示。
- en: 4.1.4\. Visual Genome Dataset
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. Visual Genome 数据集
- en: 'Visual Genome dataset (Krishna et al., [2017](#bib.bib73)) is another dataset
    for image captioning. Image captioning requires not only to recognise the objects
    of an image but it also needs reasoning their interactions and attributes. Unlike
    the first three datasets where a caption is given to the whole scene, Visual Genome
    dataset has separate captions for multiple regions in an image. The dataset has
    seven main parts: region descriptions, objects, attributes, relationships, region
    graphs, scene graphs, and question answer pairs. The dataset has more than 108k
    images. Each image contains an average of 35 objects, 26 attributes, and 21 pairwise
    relationships between objects.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Visual Genome 数据集 (Krishna et al., [2017](#bib.bib73)) 是另一个用于图像描述的数据集。图像描述不仅需要识别图像中的物体，还需要推理它们的互动和属性。与前三个数据集为整个场景提供描述不同，Visual
    Genome 数据集为图像中的多个区域提供了单独的描述。该数据集包含七个主要部分：区域描述、物体、属性、关系、区域图、场景图和问答对。数据集拥有超过 108k
    张图像。每张图像平均包含 35 个物体、26 个属性和 21 对物体之间的关系。
- en: 4.1.5\. Instagram Dataset
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5\. Instagram 数据集
- en: Tran et al. (Tran et al., [2016](#bib.bib136)) and Park et al. (Park et al.,
    [2017](#bib.bib112)) created two datasets using images from Instagram which is
    a photo-sharing social networking services. The dataset of Tran et al. has about
    10k images which are mostly from celebrities. However, Park et al. used their
    dataset for hashtag prediction and post-generation tasks in social media networks.
    This dataset contains 1.1m posts on a wide range of topics and a long hashtag
    lists from 6.3k users.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Tran et al. (Tran et al., [2016](#bib.bib136)) 和 Park et al. (Park et al., [2017](#bib.bib112))
    创建了两个使用 Instagram 图像的数据集，Instagram 是一个照片分享社交网络服务。Tran et al. 的数据集包含约 10k 张图像，主要来自名人。然而，Park
    et al. 使用他们的数据集来进行社交媒体网络中的标签预测和帖子生成任务。该数据集包含 110 万个帖子，涵盖广泛的主题，并且有来自 6.3k 用户的长标签列表。
- en: '![Refer to caption](img/19d287d1345f72b7bf1f9c7ef6c752ed.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/19d287d1345f72b7bf1f9c7ef6c752ed.png)'
- en: Figure 13\. Captions generated by Jia et al. (Jia et al., [2015](#bib.bib60))
    on some sample images from the Flickr8k dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. Jia et al. (Jia et al., [2015](#bib.bib60)) 在 Flickr8k 数据集上的一些样本图像生成的描述。
- en: 4.1.6\. IAPR TC-12 Dataset
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6\. IAPR TC-12 数据集
- en: IAPR TC-12 dataset (Grubinger et al., [2006](#bib.bib51)) has 20k images. The
    images are collected from various sources such as sports, photographs of people,
    animals, landscapes and many other locations around the world. The images of this
    dataset have captions in multiple languages. Images have multiple objects as well.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: IAPR TC-12 数据集 (Grubinger et al., [2006](#bib.bib51)) 包含 20k 张图像。这些图像来自各种来源，如体育、人物摄影、动物、风景以及世界各地的许多其他位置。该数据集的图像包含多种语言的描述。图像中也包含多个物体。
- en: 4.1.7\. Stock3M Dataset
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.7\. Stock3M 数据集
- en: Stock3M dataset has 3,217,654 images uploaded by users and it is 26 times larger
    than MSCOCO dataset. The images of this dataset have a diversity of content.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Stock3M 数据集包含 3,217,654 张由用户上传的图像，规模是 MSCOCO 数据集的 26 倍。该数据集的图像内容多样。
- en: 4.1.8\. MIT-Adobe FiveK dataset
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.8\. MIT-Adobe FiveK 数据集
- en: MIT-Adobe FiveK (Bychkovsky et al., [2011](#bib.bib20)) dataset consists of
    5,000 images. These images contain a diverse set of scenes, subjects, and lighting
    conditions and they are mainly about people, nature, and man-made objects.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: MIT-Adobe FiveK (Bychkovsky 等人，[2011](#bib.bib20)) 数据集包含 5,000 张图像。这些图像涵盖了各种场景、主题和光照条件，主要涉及人物、自然和人造物品。
- en: 4.1.9\. FlickrStyle10k Dataset
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.9\. FlickrStyle10k 数据集
- en: FlickrStyle10k dataset has 10,000 Flickr images with stylized captions. The
    training data consists of 7000 images. The validation and test data consists of
    2,000 and 1,000 images respectively. Each image contains romantic, humorous, and
    factual captions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: FlickrStyle10k 数据集包含 10,000 张带有风格化标题的 Flickr 图像。训练数据包含 7,000 张图像。验证和测试数据分别包含
    2,000 张和 1,000 张图像。每张图像包含浪漫、幽默和事实性的标题。
- en: 4.2\. Evaluation Metrics
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 评估指标
- en: 4.2.1\. BLEU
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. BLEU
- en: BLEU (Bilingual evaluation understudy) (Papineni et al., [2002](#bib.bib111))
    is a metric that is used to measure the quality of machine generated text. Individual
    text segments are compared with a set of reference texts and scores are computed
    for each of them. In estimating the overall quality of the generated text, the
    computed scores are averaged. However, syntactical correctness is not considered
    here. The performance of the BLEU metric is varied depending on the number of
    reference translations and the size of the generated text. Subsequently, Papineni
    et al. introduced a modified precision metric. This metrics uses n-grams. BLEU
    is popular because it is a pioneer in automatic evaluation of machine translated
    text and has a reasonable correlation with human judgements of quality (Denoual
    and Lepage, [2005](#bib.bib30); Callison-Burch et al., [2006](#bib.bib21)). However,
    it has a few limitations such as BLEU scores are good only if the generated text
    is short (Callison-Burch et al., [2006](#bib.bib21)). There are some cases where
    an increase in BLEU score does not mean that the quality of the generated text
    is good (Lin and Och, [2004](#bib.bib83)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（Bilingual evaluation understudy）（Papineni 等人，[2002](#bib.bib111)）是一种用于测量机器生成文本质量的指标。各个文本段落与一组参考文本进行比较，并计算每个段落的得分。在估计生成文本的整体质量时，计算的得分会被平均。然而，语法正确性在这里没有被考虑。BLEU
    指标的表现因参考翻译数量和生成文本的大小而异。随后，Papineni 等人引入了一种改进的精度指标，该指标使用 n-grams。BLEU 受到欢迎是因为它在自动评估机器翻译文本方面开创了先河，并且与人类质量评判有合理的相关性（Denoual
    和 Lepage，[2005](#bib.bib30)；Callison-Burch 等人，[2006](#bib.bib21)）。然而，它有一些局限性，比如
    BLEU 分数仅在生成文本较短时才较好（Callison-Burch 等人，[2006](#bib.bib21)）。有些情况下，BLEU 分数的提高并不意味着生成文本的质量很好（Lin
    和 Och，[2004](#bib.bib83)）。
- en: 4.2.2\. ROUGE
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. ROUGE
- en: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, [2004](#bib.bib82))
    is a set of metrics that are used for measuring the quality of text summary. It
    compares word sequences, word pairs, and n-grams with a set of reference summaries
    created by humans. Different types of ROUGE such as ROUGE-1, 2, ROUGE-W, ROUGE-SU4
    are used for different tasks. For example, ROUGE-1 and ROUGE-W are appropriate
    for single document evaluation whereas ROUGE-2 and ROUGE-SU4 have good performance
    in short summaries. However, ROUGE has problems in evaluating multi-document text
    summary.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE（Recall-Oriented Understudy for Gisting Evaluation）（Lin，[2004](#bib.bib82)）是一组用于测量文本摘要质量的指标。它比较词序列、词对和
    n-grams 与由人类创建的参考摘要集。不同类型的 ROUGE，如 ROUGE-1、ROUGE-2、ROUGE-W、ROUGE-SU4 用于不同的任务。例如，ROUGE-1
    和 ROUGE-W 适用于单文档评估，而 ROUGE-2 和 ROUGE-SU4 在短摘要中表现良好。然而，ROUGE 在评估多文档文本摘要时存在问题。
- en: 4.2.3\. METEOR
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. METEOR
- en: METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee
    and Lavie, [2005](#bib.bib10)) is another metric used to evaluate the machine
    translated language. Standard word segments are compared with the reference texts.
    In addition to this, stems of a sentence and synonyms of words are also considered
    for matching. METEOR can make better correlation at the sentence or the segment
    level.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: METEOR（Metric for Evaluation of Translation with Explicit ORdering）（Banerjee
    和 Lavie，[2005](#bib.bib10)）是另一种用于评估机器翻译语言的指标。标准词段与参考文本进行比较。此外，还考虑了句子的词根和词汇的同义词匹配。METEOR
    在句子或段落级别上能更好地进行相关性匹配。
- en: 4.2.4\. CIDEr
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. CIDEr
- en: CIDEr (Consensus-based Image Descripton Evaluation) (Vedantam et al., [2015](#bib.bib140))
    is an automatic consensus metric for evaluating image descriptions. Most existing
    datasets have only five captions per image. Previous evaluation metrics work with
    these small number of sentences and are not enough to measure the consensus between
    generated captions and human judgement. However, CIDEr achieves human consensus
    using term frequency-inverse document frequency (TF-IDF) (Robertson, [2004](#bib.bib122)).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: CIDEr（共识图像描述评估）（Vedantam et al., [2015](#bib.bib140)）是一种用于评估图像描述的自动共识指标。大多数现有数据集每张图像只有五个描述。以前的评估指标仅使用这些少量句子，无法充分衡量生成的描述与人工判断之间的共识。然而，CIDEr
    使用词频-逆文档频率（TF-IDF）（Robertson, [2004](#bib.bib122)）实现了人类共识。
- en: 4.2.5\. SPICE
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. SPICE
- en: SPICE (Semantic Propositional Image Caption Evaluation) (Anderson et al., [2016](#bib.bib4))
    is a new caption evaluation metric based on semantic concept. It is based on a
    graph-based semantic representation called scene-graph (Johnson et al., [2015](#bib.bib64);
    Schuster et al., [2015](#bib.bib124)). This graph can extract the information
    of different objects, attributes and their relationships from the image descriptions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: SPICE（语义命题图像描述评估）（Anderson et al., [2016](#bib.bib4)）是一种基于语义概念的新型描述评估指标。它基于一种称为场景图的图形语义表示（Johnson
    et al., [2015](#bib.bib64); Schuster et al., [2015](#bib.bib124)）。该图可以从图像描述中提取不同对象、属性及其关系的信息。
- en: Existing image captioning methods compute log-likelihood scores to evaluate
    their generated captions. They use BLEU, METEOR, ROUGE, SPICE, and CIDEr as evaluation
    metrics. However, BLEU, METEOR, ROUGE are not well correlated with human assessments
    of quality. SPICE and CIDEr have better correlation but they are hard to optimize.
    Liu et al. (Liu et al., [2017b](#bib.bib86)) introduced a new captions evaluation
    metric that is a good choice by human raters. It is developed by a combination
    of SPICE and CIDEr, and termed as SPIDEr. It uses a policy gradient method to
    optimize the metrics.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的图像描述方法计算对数似然分数来评估生成的描述。他们使用 BLEU、METEOR、ROUGE、SPICE 和 CIDEr 作为评估指标。然而，BLEU、METEOR
    和 ROUGE 与人类质量评估的相关性不高。SPICE 和 CIDEr 的相关性更好，但优化难度较大。Liu 等人（Liu et al., [2017b](#bib.bib86)）提出了一种新的描述评估指标，被人类评估者认为是一个不错的选择。它通过结合
    SPICE 和 CIDEr 开发而成，称为 SPIDEr。它使用策略梯度方法来优化这些指标。
- en: 'The quality of image captioning depends on the assessment of two main aspects:
    adequacy and fluency. An evaluation metric needs to focus on a diverse set of
    linguistic features to achieve these aspects. However, commonly used evaluation
    metrics consider only some specific features (e.g., lexical or semantic) of languages.
    Sharif et al. (Sharif et al., [2018](#bib.bib126)) proposed learning-based composite
    metrics for evaluation of image captions. The composite metric incorporates a
    set of linguistic features to achieve the two main aspects of assessment and shows
    improved performances.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述的质量依赖于两个主要方面的评估：适当性和流畅性。评估指标需要关注多样的语言特征来实现这两个方面。然而，常用的评估指标仅考虑一些特定的语言特征（例如，词汇或语义）。Sharif
    等人（Sharif et al., [2018](#bib.bib126)）提出了基于学习的复合指标来评估图像描述。该复合指标结合了一组语言特征，以实现两个主要评估方面，并显示了改进的性能。
- en: 5\. Comparison on benchmark datasets and common evaluation metrics
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 在基准数据集和常见评估指标上的比较
- en: 'While formal experimental evaluation was left out of the scope of this paper,
    we present a brief analysis of the experimental results and the performance of
    various techniques as reported. We cover three sets of results:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正式的实验评估超出了本文的范围，但我们提供了对实验结果和报告的各种技术性能的简要分析。我们涵盖了三组结果：
- en: (1)
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: We find a number of methods use the first three datasets listed in Section 4.1\.
    and a number of commonly used evaluation metrics to present the results. These
    results are shown in Table 3.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现一些方法使用了第 4.1 节中列出的前三个数据集以及一些常用的评估指标来呈现结果。这些结果见表 3。
- en: (2)
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'A few methods fall into the following groups: Attention-based and Other deep
    learning-based (Reinforcement learning and GAN-based methods) image captioning.
    The results of such methods are shown in Tables 4 and 5, respectively.'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些方法属于以下几类：基于注意力机制和其他深度学习方法（强化学习和基于 GAN 的方法）图像描述。这些方法的结果分别见表 4 和表 5。
- en: (3)
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: We also list the methods that proivide top two results scored on each evaluation
    metric on the MSCOCO dataset. These results are shown in Table 6.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还列出了在 MSCOCO 数据集上在每个评估指标上得分前两名的方法。这些结果见表 6。
- en: As shown in Table 3, on Flickr8k, Mao et al. achieved 0.565, 0.386, 0.256, and
    0.170 on BLEU-1, BLEU-2, BLEU-3, and BLEU-4 , respectively. For Flickr30k dataset,
    the scores are 0.600, 0.410, 0.280, and 0.190, respectively which are higher than
    the Flickr8k scores. The highest scores were achieved on the MSCOCO dataset. The
    higher results on a larger dataset follows the fact that a large dataset has more
    data, comprehensive representation of various scenes, complexities, and their
    own natural context. The results of Jia et al. are similar for Flickr8k and Flickr30k
    datasets but higher on MSCOCO dataset. The method uses visual space for mapping
    image-features and text features. Mao et al. use multimodal space for the mapping
    of image-features and text features. On the other hand, Jia et al. use visual
    space for the mapping. Moreover, the method uses an Encoder-Decoder architecture
    where it can guide the decoder part dynamically. Consequently, this method performs
    better than Mao et al.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 3 所示，在 Flickr8k 数据集中，Mao et al. 在 BLEU-1、BLEU-2、BLEU-3 和 BLEU-4 上分别取得了 0.565、0.386、0.256
    和 0.170 的分数。对于 Flickr30k 数据集，分数分别为 0.600、0.410、0.280 和 0.190，均高于 Flickr8k 的分数。在
    MSCOCO 数据集中取得了最高的分数。更大的数据集通常结果更高，因为它包含了更多的数据，全面表示了各种场景、复杂性及其自然背景。Jia et al. 的结果在
    Flickr8k 和 Flickr30k 数据集中相似，但在 MSCOCO 数据集中更高。该方法使用视觉空间来映射图像特征和文本特征。Mao et al.
    使用多模态空间进行映射。另一方面，Jia et al. 使用视觉空间进行映射。此外，该方法使用了编码器-解码器架构，可以动态引导解码器部分。因此，该方法的表现优于
    Mao et al.
- en: Xu et al. also perform better on MSCOCO dataset. This method outperformed both
    Mao et al. and Jia et al. The main reason behind this is that it uses an attention
    mechanism which focuses only on relevant objects of the image. The semantic concept-based
    methods can generate semantically rich captions. Wu et al. proposed a semantic
    concept-based image captioning method. This method first predicts the attributes
    of different objects from the image and then adds these attributes with the captions
    which are semantically meaningful. In terms of performance, the method is superior
    to all the methods mentioned in Table 3.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: Xu et al. 在 MSCOCO 数据集上的表现也更佳。该方法优于 Mao et al. 和 Jia et al.，主要原因在于其使用了关注机制，只关注图像中的相关对象。基于语义概念的方法可以生成语义丰富的描述。Wu
    et al. 提出了基于语义概念的图像描述方法。该方法首先预测图像中不同对象的属性，然后将这些属性与语义有意义的描述进行结合。在性能方面，该方法优于表 3
    中提到的所有方法。
- en: '| Dataset | Method | Category | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 类别 | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| \multirow4*Flickr8k | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) |
    MS,SL,WS | 0.565 | 0.386 | 0.256 | 0.170 | - |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| \multirow4*Flickr8k | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) |
    MS,SL,WS | 0.565 | 0.386 | 0.256 | 0.170 | - |'
- en: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.647
    | 0.459 | 0.318 | 0.216 | 0.201 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.647
    | 0.459 | 0.318 | 0.216 | 0.201 |'
- en: '|  | Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | VS,SL,WS,EDA,AB | 0.670
    | 0.457 | 0.314 | 0.213 | 0.203 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | VS,SL,WS,EDA,AB | 0.670
    | 0.457 | 0.314 | 0.213 | 0.203 |'
- en: '|  | Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VS,SL,WS,EDA,SCB | 0.740
    | 0.540 | 0.380 | 0.270 | - |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VS,SL,WS,EDA,SCB | 0.740
    | 0.540 | 0.380 | 0.270 | - |'
- en: '| \multirow4*Flickr30k | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95))
    | MS,SL,WS | 0.600 | 0.410 | 0.280 | 0.190 | - |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| \multirow4*Flickr30k | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95))
    | MS,SL,WS | 0.600 | 0.410 | 0.280 | 0.190 | - |'
- en: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.646
    | 0.466 | 0.305 | 0.206 | 0.179 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.646
    | 0.466 | 0.305 | 0.206 | 0.179 |'
- en: '|  | Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | VS,SL,WS,EDA,AB | 0.669
    | 0.439 | 0.296 | 0.199 | 0.184 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | VS,SL,WS,EDA,AB | 0.669
    | 0.439 | 0.296 | 0.199 | 0.184 |'
- en: '|  | Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VS,SL,WS,EDA,SCB | 0.730
    | 0.550 | 0.400 | 0.280 | - |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VS,SL,WS,EDA,SCB | 0.730
    | 0.550 | 0.400 | 0.280 | - |'
- en: '| \multirow4*MSCOCO | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) | MS,SL,WS
    | 0.670 | 0.490 | 0.350 | 0.250 | - |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| \multirow4*MSCOCO | Mao et al. 2015 (Mao et al., [2015b](#bib.bib95)) | MS,SL,WS
    | 0.670 | 0.490 | 0.350 | 0.250 | - |'
- en: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.670
    | 0.491 | 0.358 | 0.264 | 0.227 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | Jia et al. 2015 (Jia et al., [2015](#bib.bib60)) | VS,SL,WS,EDA | 0.670
    | 0.491 | 0.358 | 0.264 | 0.227 |'
- en: '|  | Xu et al. 2015 (Xu et al., [2015](#bib.bib153)) | VS,SL,WS,EDA,AB | 0.718
    | 0.504 | 0.357 | 0.250 | 0.230 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | Xu 等人 2015（Xu 等人，[2015](#bib.bib153)） | VS,SL,WS,EDA,AB | 0.718 | 0.504
    | 0.357 | 0.250 | 0.230 |'
- en: '|  | Wu et al. 2018 (Wu et al., [2018](#bib.bib151)) | VS,SL,WS,EDA,SCB | 0.740
    | 0.560 | 0.420 | 0.310 | 0.260 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | Wu 等人 2018（Wu 等人，[2018](#bib.bib151)） | VS,SL,WS,EDA,SCB | 0.740 | 0.560
    | 0.420 | 0.310 | 0.260 |'
- en: Table 3\. Performance of different image captioning methods on three benchmark
    datasets and commonly used evaluation metrics.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 不同图像描述方法在三个基准数据集上的表现及常用评估指标。
- en: Table 4 shows the results of attention-based based methods on MSCOCO dataset.
    Xu et al.’s stochastic hard attention produced better results than deterministic
    soft attention. However, these results were outperformed by Jin et al. which can
    update its attention based on the scene-specific context.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4 显示了基于注意力的方法在 MSCOCO 数据集上的结果。Xu 等人的随机硬注意力产生的结果优于确定性软注意力。然而，这些结果被 Jin 等人超越，他们的方法可以基于特定场景的上下文更新注意力。
- en: Wu et al. 2016 and Pedersoli et al. 2017 only show BLEU-4 and METEOR scores
    which are higher than the aforementioned methods. The method of Wu et al. uses
    an attention mechanism with a review process. The review process checks the focused
    attention in every time step and updates it if necessary. This mechanism helps
    to achieve better results than the prior attention-based methods. Pedersoli et
    al. propose a different attention mechanism that maps the focused image regions
    directly with the caption words instead of LSTM state. This behavior of the method
    drives it to achieve top performances among the mentioned attention-based methods
    in Table 4.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 2016 和 Pedersoli 等人 2017 仅展示了 BLEU-4 和 METEOR 分数，这些分数高于前述方法。Wu 等人的方法使用了带有复审过程的注意力机制。复审过程检查每个时间步的注意力集中情况，并在必要时进行更新。该机制帮助实现了比之前的注意力方法更好的结果。Pedersoli
    等人提出了一种不同的注意力机制，该机制将关注的图像区域直接映射到描述词而不是 LSTM 状态。这种方法的行为使其在表 4 中提到的基于注意力的方法中取得了最好的表现。
- en: Reinforcement learning-based (RL) and GAN-based methods are becoming increasingly
    popular. We name them as “Other Deep Learning-based Image Captioning”. The results
    of the methods of this group are shown in Table 5\. The methods do not have results
    on commonly used evaluation metrics. However, they have their own potentials to
    generate the descriptions for the image.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 基于强化学习（RL）和 GAN 的方法正变得越来越受欢迎。我们将其称为“其他深度学习图像描述”。这一组方法的结果显示在表 5\. 这些方法在常用评估指标上没有结果。然而，它们具有生成图像描述的潜力。
- en: Shetty et al. employed adversarial training in their image captioning method.
    This method is capable to generate diverse captions. The captions are less-biased
    with the ground-truth captions compared to the methods use maximum likelihood
    estimation. To take the advantages of RL, Ren et al. proposed a method that can
    predict all possible next words for the current word in current time step. This
    mechanism helps them to generate contextually more accurate captions. Actor-critic
    of RL are similar to the Generator and the Discriminator of GAN. However, at the
    beginning of the training, both actor and critic do not have any knowledge about
    data. Zhang et al. proposed an actor-critic-based image captioning method. This
    method is capable of predicting the ultimate captions at its early stage and can
    generate more accurate captions than other reinforcement learning-based methods.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Shetty 等人采用了对抗训练在其图像描述方法中。该方法能够生成多样化的描述。与使用最大似然估计的方法相比，这些描述与真实描述的偏差较小。为了利用强化学习的优势，Ren
    等人提出了一种方法，可以预测当前时间步长的所有可能的下一个词。这一机制帮助他们生成语境上更准确的描述。强化学习的 Actor-Critic 类似于 GAN
    的生成器和判别器。然而，在训练开始时，演员和评论员对数据没有任何了解。Zhang 等人提出了一种基于 Actor-Critic 的图像描述方法。该方法能够在早期阶段预测最终描述，并且可以生成比其他基于强化学习的方法更准确的描述。
- en: '| \multirow2*Method | \multirow2*Category | MS COCO |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| \multirow2*方法 | \multirow2*类别 | MS COCO |'
- en: '| --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)), soft | VS,SL,WS,EDA,VC |
    0.707 | 0.492 | 0.344 | 0.243 | 0.239 | - | - |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 2015（Xu 等人，[2015](#bib.bib153)），软 | VS,SL,WS,EDA,VC | 0.707 | 0.492
    | 0.344 | 0.243 | 0.239 | - | - |'
- en: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)), hard | VS,SL,WS,EDA,VC |
    0.718 | 0.504 | 0.357 | 0.250 | 0.230 | - | - |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. 2015 (Xu et al., [2015](#bib.bib153)), hard | VS,SL,WS,EDA,VC |
    0.718 | 0.504 | 0.357 | 0.250 | 0.230 | - | - |'
- en: '| Jin et al. 2015 (Jin et al., [2015](#bib.bib62)) | VS,SL,WS,EDA,VC | 0.697
    | 0.519 | 0.381 | 0.282 | 0.235 | 0.509 | 0.838 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Jin et al. 2015 (Jin et al., [2015](#bib.bib62)) | VS,SL,WS,EDA,VC | 0.697
    | 0.519 | 0.381 | 0.282 | 0.235 | 0.509 | 0.838 |'
- en: '| Wu et al. 2016 (Wu and Cohen, [2016](#bib.bib152)) | VS,SL,WS,EDA,VC | -
    | - | - | 0.290 | 0.237 | - | 0.886 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. 2016 (Wu and Cohen, [2016](#bib.bib152)) | VS,SL,WS,EDA,VC | -
    | - | - | 0.290 | 0.237 | - | 0.886 |'
- en: '| Pedersoli et al. 2017 (Pedersoli et al., [2017](#bib.bib113)) | VS,SL,WS,EDA,VC
    | - | - | - | 0.307 | 0.245 | - | 0.938 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| Pedersoli et al. 2017 (Pedersoli et al., [2017](#bib.bib113)) | VS,SL,WS,EDA,VC
    | - | - | - | 0.307 | 0.245 | - | 0.938 |'
- en: Table 4\. Performance of attention-based image captioning methods on MSCOCO
    dataset and commonly used evaluation metrics.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 基于注意力机制的图像描述方法在 MSCOCO 数据集上的性能及常用评价指标。
- en: '| \multirow2*Method | \multirow2*Category | MS COCO |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| \multirow2*Method | \multirow2*Category | MS COCO |'
- en: '| --- | --- | --- |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Shetty et al. 2017[GAN] (Shetty et al., [2017](#bib.bib127)) | VS,ODL,WS,EDA
    | - | - | - | - | 0.239 | - | - | 0.167 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Shetty et al. 2017[GAN] (Shetty et al., [2017](#bib.bib127)) | VS,ODL,WS,EDA
    | - | - | - | - | 0.239 | - | - | 0.167 |'
- en: '| Ren et al. 2017[RL] (Ren et al., [2017](#bib.bib120)) | VS,ODL,WS,EDA | 0.713
    | 0.539 | 0.403 | 0.304 | 0.251 | 0.525 | 0.937 | - |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| Ren et al. 2017[RL] (Ren et al., [2017](#bib.bib120)) | VS,ODL,WS,EDA | 0.713
    | 0.539 | 0.403 | 0.304 | 0.251 | 0.525 | 0.937 | - |'
- en: '| Zhang et al. 2017[RL] (Zhang et al., [2017](#bib.bib162)) | VS,ODL,WS,EDA
    | - | - | - | 0.344 | 0.267 | 0.558 | 1.162 | - |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2017[RL] (Zhang et al., [2017](#bib.bib162)) | VS,ODL,WS,EDA
    | - | - | - | 0.344 | 0.267 | 0.558 | 1.162 | - |'
- en: Table 5\. Performance of Other Deep learning-based image captioning methods
    on MSCOCO dataset and commonly used evaluation metrics.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 其他基于深度学习的图像描述方法在 MSCOCO 数据集上的性能及常用评价指标。
- en: '| \multirow2*Method | \multirow2*Category | MSCOCO |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| \multirow2*Method | \multirow2*Category | MSCOCO |'
- en: '| --- | --- | --- |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE
    |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Lu et al. 2017 (Lu et al., [2017](#bib.bib89)) | VS,SL,WS,EDA,AB | 0.742
    | 0.580 | 0.439 | 0.332 | 0.266 | - | 1.085 | - |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Lu et al. 2017 (Lu et al., [2017](#bib.bib89)) | VS,SL,WS,EDA,AB | 0.742
    | 0.580 | 0.439 | 0.332 | 0.266 | - | 1.085 | - |'
- en: '| Gan et al. 2017 (Gan et al., [2017b](#bib.bib42)) | VS,SL,WS,CA,SCB | 0.741
    | 0.578 | 0.444 | 0.341 | 0.261 | - | 1.041 | - |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| Gan et al. 2017 (Gan et al., [2017b](#bib.bib42)) | VS,SL,WS,CA,SCB | 0.741
    | 0.578 | 0.444 | 0.341 | 0.261 | - | 1.041 | - |'
- en: '| Zhang et al. 2017 (Zhang et al., [2017](#bib.bib162)) | VS,ODL,WS,EDA | -
    | - | - | 0.344 | 0.267 | 0.558 | 1.162 | - |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2017 (Zhang et al., [2017](#bib.bib162)) | VS,ODL,WS,EDA | -
    | - | - | 0.344 | 0.267 | 0.558 | 1.162 | - |'
- en: '| Rennie et al. 2017 (Rennie et al., [2017](#bib.bib121)) | VS,ODL,WS,EDA |
    - | - | - | .319 | 0.255 | 0.543 | 1.06 | - |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Rennie et al. 2017 (Rennie et al., [2017](#bib.bib121)) | VS,ODL,WS,EDA |
    - | - | - | 0.319 | 0.255 | 0.543 | 1.06 | - |'
- en: '| Yao et al. 2017 (Yao et al., [2017b](#bib.bib156)) | VS,SL,WS,EDA,SCB | 0.734
    | 0.567 | 0.430 | 0.326 | 0.254 | 0.540 | 1.00 | 0.186 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Yao et al. 2017 (Yao et al., [2017b](#bib.bib156)) | VS,SL,WS,EDA,SCB | 0.734
    | 0.567 | 0.430 | 0.326 | 0.254 | 0.540 | 1.00 | 0.186 |'
- en: '| Gu et al. 2017 (Gu et al., [2017](#bib.bib52)) | VS,SL,WS,EDA | 0.720 | 0.550
    | 0.410 | 0.300 | 0.240 | - | 0.960 | 0.176 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| Gu et al. 2017 (Gu et al., [2017](#bib.bib52)) | VS,SL,WS,EDA | 0.720 | 0.550
    | 0.410 | 0.300 | 0.240 | - | 0.960 | 0.176 |'
- en: Table 6\. Top two methods based on different evaluation metrics and MSCOCO dataset
    (Bold and Italic indicates the best result; Bold indicates the second best result).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 基于不同评价指标和 MSCOCO 数据集的前两种方法（粗体和斜体表示最佳结果；粗体表示第二最佳结果）。
- en: We found that the performance of a technique can vary across different metrics.
    Table 6 shows the methods based on the top two scores on every individual evaluation
    metric. For example, Lu et al., Gan et al., and Zhang et al. are within the top
    two methods based on the scores achieved on BLEU-n and METEOR metrics. BLEU-n
    metrics use variable length phrases of generated captions to match against ground-truth
    captions. METEOR (Banerjee and Lavie, [2005](#bib.bib10)) considers the precision,
    recall, and the alignments of the matched tokens. Therefore, the generated captions
    by these methods have good precision and recall accuracy as well as the good similarity
    in word level. ROUGE-L evaluates the adequacy and fluency of generated captions,
    whereas CIDEr focuses on grammaticality and saliency. SPICE can analyse the semantics
    of the generated captions. Zhang et al., Rennie et al., and Lu et al. can generate
    captions, which have adequacy, fluency, saliency, and are grammaticality correct
    than other methods in Table 6\. Gu et al. and Yao et al. perform well in generating
    semantically correct captions.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现技术的性能在不同指标上可能有所不同。表 6 显示了每个评价指标上前两名的方法。例如，Lu 等、Gan 等和 Zhang 等在 BLEU-n 和
    METEOR 指标上位于前两名。BLEU-n 指标使用生成的描述中的可变长度短语与真实描述进行匹配。METEOR（Banerjee 和 Lavie，[2005](#bib.bib10)）考虑了匹配词元的精度、召回率和对齐。因此，这些方法生成的描述具有良好的精度和召回率，以及较好的词汇层次相似性。ROUGE-L
    评估生成描述的充分性和流畅性，而 CIDEr 关注语法和显著性。SPICE 可以分析生成描述的语义。Zhang 等、Rennie 等和 Lu 等能够生成在充分性、流畅性、显著性以及语法正确性方面优于表
    6 中其他方法的描述。Gu 等和 Yao 等在生成语义正确的描述方面表现良好。
- en: 6\. Discussions and Future Research Directions
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论与未来研究方向
- en: Many deep learning-based methods have been proposed for generating automatic
    image captions in the recent years. Supervised learning, reinforcement learning,
    and GAN based methods are commonly used in generating image captions. Both visual
    space and multimodal space can be used in supervised learning-based methods. The
    main difference between visual space and multimodal space occurs in mapping. Visual
    space-based methods perform explicit mapping from images to descriptions. In contrast,
    multimodal space-based methods incorporate implicit vision and language models.
    Supervised learning-based methods are further categorized into Encoder-Decoder
    architecture-based, Compositional architecture-based, Attention-based, Semantic
    concept-based, Stylized captions, Dense image captioning, and Novel object-based
    image captioning.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多基于深度学习的方法被提出用于生成自动图像描述。监督学习、强化学习和基于 GAN 的方法在生成图像描述中被广泛使用。监督学习的方法可以使用视觉空间和多模态空间。视觉空间和多模态空间的主要区别在于映射。视觉空间基的方法从图像到描述进行显式映射。相比之下，多模态空间的方法则结合了隐式的视觉和语言模型。监督学习方法进一步分为基于编码器-解码器架构的方法、基于组合架构的方法、基于注意力机制的方法、基于语义概念的方法、风格化描述、密集图像描述和新颖对象基的图像描述方法。
- en: Encoder-Decoder architecture-based methods use a simple CNN and a text generator
    for generating image captions. Attention-based image captioning methods focus
    on different salient parts of the image and achieve better performance than encoder-decoder
    architecture-based methods. Semantic concept-based image captioning methods selectively
    focus on different parts of the image and can generate semantically rich captions.
    Dense image captioning methods can generate region based image captions. Stylized
    image captions express various emotions such as romance, pride, and shame. GAN
    and RL based image captioning methods can generate diverse and multiple captions.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 基于编码器-解码器架构的方法使用简单的 CNN 和文本生成器来生成图像描述。基于注意力机制的图像描述方法关注图像的不同显著部分，表现出比基于编码器-解码器架构的方法更好的性能。基于语义概念的图像描述方法选择性地关注图像的不同部分，可以生成语义丰富的描述。密集图像描述方法能够生成基于区域的图像描述。风格化的图像描述表达各种情感，如浪漫、自豪和羞耻。基于
    GAN 和 RL 的图像描述方法能够生成多样化和多重的描述。
- en: MSCOCO, Flickr30k and Flickr8k dataset are common and popular datasets used
    for image captioning. MSCOCO dataset is very large dataset and all the images
    in these datasets have multiple captions. Visual Genome dataset is mainly used
    for region based image captioning. Different evaluation metrics are used for measuring
    the performances of image captions. BLEU metric is good for small sentence evaluation.
    ROUGE has different types and they can be used for evaluating different types
    of texts. METEOR can perform an evaluation on various segments of a caption. SPICE
    is better in understanding semantic details of captions compared to other evaluation
    metrics.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: MSCOCO、Flickr30k 和 Flickr8k 数据集是常用且受欢迎的图像描述数据集。MSCOCO 数据集非常大，所有图像都有多个描述。Visual
    Genome 数据集主要用于基于区域的图像描述。不同的评价指标用于衡量图像描述的性能。BLEU 指标适用于小句子的评价。ROUGE 有不同的类型，可用于评价不同类型的文本。METEOR
    能对描述的各个部分进行评价。SPICE 在理解描述的语义细节方面比其他评价指标更好。
- en: Although success has been achieved in recent years, there is still a large scope
    for improvement. Generation based methods can generate novel captions for every
    image. However, these methods fail to detect prominent objects and attributes
    and their relationships to some extent in generating accurate and multiple captions.
    In addition to this, the accuracy of the generated captions largely depends on
    syntactically correct and diverse captions which in turn rely on powerful and
    sophisticated language generation model. Existing methods show their performances
    on the datasets where images are collected from the same domain. Therefore, working
    on open domain dataset will be an interesting avenue for research in this area.
    Image-based factual descriptions are not enough to generate high-quality captions.
    External knowledge can be added in order to generate attractive image captions.
    Supervised learning needs a large amount of labelled data for training. Therefore,
    unsupervised learning and reinforcement learning will be more popular in future
    in image captioning.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来已取得成功，但仍有很大的改进空间。基于生成的方法可以为每张图像生成新颖的描述。然而，这些方法在生成准确且多样的描述时，往往在检测显著物体及属性及其关系方面存在不足。此外，生成的描述的准确性在很大程度上依赖于语法正确且多样的描述，而这又依赖于强大且复杂的语言生成模型。现有方法在从相同领域收集的图像数据集上表现良好。因此，研究开放领域数据集将是该领域一个有趣的方向。基于图像的事实描述不足以生成高质量的描述。可以加入外部知识来生成吸引人的图像描述。监督学习需要大量标记数据进行训练。因此，未来无监督学习和强化学习将在图像描述生成中更受欢迎。
- en: 7\. Conclusions
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we have reviewed deep learning-based image captioning methods.
    We have given a taxonomy of image captioning techniques, shown generic block diagram
    of the major groups and highlighted their pros and cons. We discussed different
    evaluation metrics and datasets with their strengths and weaknesses. A brief summary
    of experimental results is also given. We briefly outlined potential research
    directions in this area. Although deep learning-based image captioning methods
    have achieved a remarkable progress in recent years, a robust image captioning
    method that is able to generate high quality captions for nearly all images is
    yet to be achieved. With the advent of novel deep learning network architectures,
    automatic image captioning will remain an active research area for some time.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了基于深度学习的图像描述生成方法。我们给出了图像描述技术的分类，展示了主要组的通用框图，并突出其优缺点。我们讨论了不同的评价指标和数据集及其优缺点。还给出了实验结果的简要总结。我们简要概述了该领域的潜在研究方向。尽管基于深度学习的图像描述生成方法近年来取得了显著进展，但尚未实现能够为几乎所有图像生成高质量描述的鲁棒方法。随着新型深度学习网络架构的出现，自动图像描述将继续是一个活跃的研究领域。
- en: Acknowledgements
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was partially supported by an Australian Research Council grant DE120102960.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到澳大利亚研究委员会资助 DE120102960 的支持。
- en: References
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Agarwal and Lavie (2008) Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu
    and m-ter: Evaluation metrics for high-correlation with human rankings of machine
    translation output. In *Proceedings of the Third Workshop on Statistical Machine
    Translation*. Association for Computational Linguistics, 115–118.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 和 Lavie (2008) Abhaya Agarwal 和 Alon Lavie. 2008. Meteor, m-bleu 和 m-ter：与人类翻译输出排名高相关性的评价指标。在
    *第三届统计机器翻译研讨会论文集* 中。计算语言学协会，115–118。
- en: Aker and Gaizauskas (2010) Ahmet Aker and Robert Gaizauskas. 2010. Generating
    image descriptions using dependency relational patterns. In *Proceedings of the
    48th annual meeting of the association for computational linguistics*. Association
    for Computational Linguistics, 1250–1258.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aker 和 Gaizauskas（2010）Ahmet Aker 和 Robert Gaizauskas。2010。利用依赖关系模式生成图像描述。在
    *第48届计算语言学协会年会论文集*。计算语言学协会，1250–1258。
- en: 'Anderson et al. (2016) Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
    Gould. 2016. Spice: Semantic propositional image caption evaluation. In *European
    Conference on Computer Vision*. Springer, 382–398.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等（2016）Peter Anderson、Basura Fernando、Mark Johnson 和 Stephen Gould。2016。SPICE：语义命题图像描述评估。在
    *欧洲计算机视觉会议*。Springer，382–398。
- en: Anderson et al. (2017) Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney,
    Mark Johnson, Stephen Gould, and Lei Zhang. 2017. Bottom-up and top-down attention
    for image captioning and vqa. *arXiv preprint arXiv:1707.07998* (2017).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等（2017）Peter Anderson、Xiaodong He、Chris Buehler、Damien Teney、Mark Johnson、Stephen
    Gould 和 Lei Zhang。2017。用于图像描述和 VQA 的自下而上和自上而下注意力。*arXiv 预印本 arXiv:1707.07998*（2017）。
- en: Aneja et al. (2018) Jyoti Aneja, Aditya Deshpande, and Alexander G Schwing.
    2018. Convolutional image captioning. In *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*. 5561–5570.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aneja 等（2018）Jyoti Aneja、Aditya Deshpande 和 Alexander G Schwing。2018。卷积图像描述。在
    *IEEE 计算机视觉与模式识别会议论文集*。5561–5570。
- en: 'Anne Hendricks et al. (2016) Lisa Anne Hendricks, Subhashini Venugopalan, Marcus
    Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell, Junhua Mao, Jonathan Huang,
    Alexander Toshev, Oana Camburu, et al. 2016. Deep compositional captioning: Describing
    novel object categories without paired training data. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anne Hendricks 等（2016）Lisa Anne Hendricks、Subhashini Venugopalan、Marcus Rohrbach、Raymond
    Mooney、Kate Saenko、Trevor Darrell、Junhua Mao、Jonathan Huang、Alexander Toshev、Oana
    Camburu 等。2016。深度组合描述：在没有配对训练数据的情况下描述新颖的对象类别。在 *IEEE 计算机视觉与模式识别会议论文集*。
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural machine translation by jointly learning to align and translate. In *International
    Conference on Learning Representations (ICLR)*.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等（2015）Dzmitry Bahdanau、Kyunghyun Cho 和 Yoshua Bengio。2015。通过联合学习对齐和翻译进行神经机器翻译。在
    *国际学习表征会议（ICLR）*。
- en: Bai and An (2018) Shuang Bai and Shan An. 2018. A Survey on Automatic Image
    Caption Generation. *Neurocomputing*.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 和 An（2018）Shuang Bai 和 Shan An。2018。自动图像描述生成的综述。*神经计算*。
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An automatic metric for MT evaluation with improved correlation with human judgments.
    In *Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures
    for machine translation and/or summarization*, Vol. 29\. 65–72.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie（2005）Satanjeev Banerjee 和 Alon Lavie。2005。METEOR：一种自动评估机器翻译的度量，改善了与人工评估的相关性。在
    *ACL 机器翻译和/或摘要内在与外在评估措施研讨会论文集*，第 29 卷。65–72。
- en: Bengio et al. (2015) Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
    2015. Scheduled sampling for sequence prediction with recurrent neural networks.
    In *Advances in Neural Information Processing Systems*. 1171–1179.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等（2015）Samy Bengio、Oriol Vinyals、Navdeep Jaitly 和 Noam Shazeer。2015。用于序列预测的调度采样与递归神经网络。
    在 *神经信息处理系统进展*。1171–1179。
- en: Bengio et al. (2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian
    Jauvin. 2003. A neural probabilistic language model. *Journal of machine learning
    research* 3, Feb, 1137–1155.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等（2003）Yoshua Bengio、Réjean Ducharme、Pascal Vincent 和 Christian Jauvin。2003。神经概率语言模型。*机器学习研究杂志*
    3，2 月，1137–1155。
- en: Berger et al. (1996) Adam L Berger, Vincent J Della Pietra, and Stephen A Della
    Pietra. 1996. A maximum entropy approach to natural language processing. *Computational
    linguistics* 22, 1 (1996), 39–71.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berger 等（1996）Adam L Berger、Vincent J Della Pietra 和 Stephen A Della Pietra。1996。最大熵方法在自然语言处理中的应用。*计算语言学*
    22, 1（1996），39–71。
- en: 'Bernardi et al. (2016) Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut
    Erdem, Erkut Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara
    Plank, et al. 2016. Automatic Description Generation from Images: A Survey of
    Models, Datasets, and Evaluation Measures. *Journal of Artificial Intelligence
    Research (JAIR)* 55, 409–442.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernardi 等（2016）Raffaella Bernardi、Ruket Cakici、Desmond Elliott、Aykut Erdem、Erkut
    Erdem、Nazli Ikizler-Cinbis、Frank Keller、Adrian Muscat、Barbara Plank 等。2016。从图像自动生成描述：模型、数据集和评估指标综述。*人工智能研究杂志（JAIR）*
    55，409–442。
- en: Blei et al. (2003) David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent
    dirichlet allocation. *Journal of machine Learning research* 3, Jan (2003), 993–1022.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei et al. (2003) David M Blei, Andrew Y Ng, 和 Michael I Jordan. 2003. 潜在狄利克雷分配。*机器学习研究杂志*
    3, Jan (2003), 993–1022.
- en: Bodnar (2018) Cristian Bodnar. 2018. Text to Image Synthesis Using Generative
    Adversarial Networks. *arXiv preprint arXiv:1805.00676*.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bodnar (2018) Cristian Bodnar. 2018. 使用生成对抗网络的文本到图像合成。*arXiv 预印本 arXiv:1805.00676*.
- en: 'Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
    Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database
    for structuring human knowledge. In *Proceedings of the 2008 ACM SIGMOD international
    conference on Management of data*. AcM, 1247–1250.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,
    和 Jamie Taylor. 2008. Freebase：一个用于结构化人类知识的协作创建的图数据库。见于 *2008 年 ACM SIGMOD 国际数据管理会议论文集*。ACM,
    1247–1250.
- en: Boser et al. (1992) Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik.
    1992. A training algorithm for optimal margin classifiers. In *Proceedings of
    the fifth annual workshop on Computational learning theory*. ACM, 144–152.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boser et al. (1992) Bernhard E Boser, Isabelle M Guyon, 和 Vladimir N Vapnik.
    1992. 优化边际分类器的训练算法。见于 *第五届计算学习理论年会论文集*。ACM, 144–152.
- en: Bulling et al. (2011) Andreas Bulling, Jamie A Ward, Hans Gellersen, and Gerhard
    Troster. 2011. Eye movement analysis for activity recognition using electrooculography.
    *IEEE transactions on pattern analysis and machine intelligence* 33, 4 (2011),
    741–753.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulling et al. (2011) Andreas Bulling, Jamie A Ward, Hans Gellersen, 和 Gerhard
    Troster. 2011. 基于眼电图的活动识别眼动分析。*IEEE 模式分析与机器智能汇刊* 33, 4 (2011), 741–753.
- en: Bychkovsky et al. (2011) Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and
    Frédo Durand. 2011. Learning photographic global tonal adjustment with a database
    of input/output image pairs. In *Computer Vision and Pattern Recognition (CVPR),
    2011 IEEE Conference on*. IEEE, 97–104.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bychkovsky et al. (2011) Vladimir Bychkovsky, Sylvain Paris, Eric Chan, 和 Frédo
    Durand. 2011. 使用输入/输出图像对数据库学习摄影全局色调调整。见于 *2011 IEEE 计算机视觉与模式识别会议 (CVPR)*。IEEE,
    97–104.
- en: Callison-Burch et al. (2006) Chris Callison-Burch, Miles Osborne, and Philipp
    Koehn. 2006. Re-evaluation the Role of Bleu in Machine Translation Research..
    In *EACL*, Vol. 6\. 249–256.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Callison-Burch et al. (2006) Chris Callison-Burch, Miles Osborne, 和 Philipp
    Koehn. 2006. 重新评估 Bleu 在机器翻译研究中的作用。见于 *EACL*，第 6 卷，249–256.
- en: 'Chen et al. (2017b) Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao,
    and Tat-Seng Chua. 2017b. SCA-CNN: Spatial and Channel-wise Attention in Convolutional
    Networks for Image Captioning. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition (CVPR)*. 6298–6306.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2017b) Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao,
    和 Tat-Seng Chua. 2017b. SCA-CNN：卷积网络中的空间和通道注意力用于图像描述。见于 *IEEE 计算机视觉与模式识别会议 (CVPR)
    论文集*。6298–6306.
- en: 'Chen et al. (2017a) Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting
    Hsu, Jianlong Fu, and Min Sun. 2017a. Show, Adapt and Tell: Adversarial Training
    of Cross-domain Image Captioner. In *The IEEE International Conference on Computer
    Vision (ICCV)*, Vol. 2.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2017a) Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting
    Hsu, Jianlong Fu, 和 Min Sun. 2017a. 展示、适应和讲述：跨领域图像描述生成的对抗训练。见于 *IEEE 国际计算机视觉会议
    (ICCV)*，第 2 卷.
- en: 'Chen and Lawrence Zitnick (2015) Xinlei Chen and C Lawrence Zitnick. 2015.
    Mind’s eye: A recurrent visual representation for image caption generation. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2422–2431.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Lawrence Zitnick (2015) Xinlei Chen 和 C Lawrence Zitnick. 2015. 眼中的思维：用于图像描述生成的递归视觉表示。见于
    *IEEE 计算机视觉与模式识别会议论文集*。2422–2431.
- en: 'Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder
    approaches. In *Association for Computational Linguistics*. 103–111.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, 和 Yoshua
    Bengio. 2014. 神经机器翻译的特性：编码器-解码器方法。见于 *计算语言学协会*。103–111.
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. *arXiv preprint arXiv:1412.3555*.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, 和 Yoshua
    Bengio. 2014. 门控递归神经网络在序列建模上的实证评估。*arXiv 预印本 arXiv:1412.3555*.
- en: Dai et al. (2017) Bo Dai, Dahua Lin, Raquel Urtasun, and Sanja Fidler. 2017.
    Towards Diverse and Natural Image Descriptions via a Conditional GAN. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition (CVPR)*. 2989–2998.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2017) Bo Dai, Dahua Lin, Raquel Urtasun, 和 Sanja Fidler. 2017. 通过条件GAN实现多样化和自然的图像描述。见
    *IEEE计算机视觉与模式识别会议论文集 (CVPR)*。2989–2998。
- en: Dalal and Triggs (2005) Navneet Dalal and Bill Triggs. 2005. Histograms of oriented
    gradients for human detection. In *Computer Vision and Pattern Recognition, 2005\.
    CVPR 2005\. IEEE Computer Society Conference on*, Vol. 1\. IEEE, 886–893.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalal and Triggs (2005) Navneet Dalal 和 Bill Triggs. 2005. 人体检测的定向梯度直方图。见 *计算机视觉与模式识别，2005\.
    CVPR 2005\. IEEE计算机学会会议*，第1卷。IEEE，886–893。
- en: De Marneffe et al. (2006) Marie-Catherine De Marneffe, Bill MacCartney, and
    Christopher D Manning. 2006. Generating typed dependency parses from phrase structure
    parses. In *Proceedings of LREC*, Vol. 6\. Genoa Italy, 449–454.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Marneffe et al. (2006) Marie-Catherine De Marneffe, Bill MacCartney, 和 Christopher
    D Manning. 2006. 从短语结构分析生成类型化依赖分析。见 *LREC会议论文集*，第6卷。意大利热那亚，449–454。
- en: 'Denoual and Lepage (2005) Etienne Denoual and Yves Lepage. 2005. BLEU in characters:
    towards automatic MT evaluation in languages without word delimiters. In *Companion
    Volume to the Proceedings of the Second International Joint Conference on Natural
    Language Processing*. 81–86.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denoual and Lepage (2005) Etienne Denoual 和 Yves Lepage. 2005. 字符级BLEU：向无词分隔符语言的自动机器翻译评估迈进。见
    *第二届国际联合自然语言处理会议论文集附录*。81–86。
- en: 'Devlin et al. (2015) Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng,
    Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. 2015. Language models for
    image captioning: The quirks and what works. *arXiv preprint arXiv:1505.01809*
    (2015).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2015) Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng,
    Xiaodong He, Geoffrey Zweig, 和 Margaret Mitchell. 2015. 图像描述的语言模型：奇异之处和有效方法。*arXiv
    预印本 arXiv:1505.01809* (2015)。
- en: Donahue et al. (2015) Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015.
    Long-term recurrent convolutional networks for visual recognition and description.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2625–2634.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donahue et al. (2015) Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, 和 Trevor Darrell. 2015.
    用于视觉识别和描述的长期递归卷积网络。见 *IEEE计算机视觉与模式识别会议论文集*。2625–2634。
- en: Elliott and Keller (2013) Desmond Elliott and Frank Keller. 2013. Image description
    using visual dependency representations. In *Proceedings of the 2013 Conference
    on Empirical Methods in Natural Language Processing*. 1292–1302.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elliott and Keller (2013) Desmond Elliott 和 Frank Keller. 2013. 使用视觉依赖表示进行图像描述。见
    *2013年自然语言处理实证方法会议论文集*。1292–1302。
- en: Fang et al. (2015) Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava,
    Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, and John C
    Platt. 2015. From captions to visual concepts and back. In *Proceedings of the
    IEEE conference on computer vision and pattern recognition*. 1473–1482.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2015) Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava,
    Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, 和 John C
    Platt. 2015. 从描述到视觉概念及其反向。见 *IEEE计算机视觉与模式识别会议论文集*。1473–1482。
- en: 'Farhadi et al. (2010) Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter
    Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture
    tells a story: Generating sentences from images. In *European conference on computer
    vision*. Springer, 15–29.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farhadi et al. (2010) Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter
    Young, Cyrus Rashtchian, Julia Hockenmaier, 和 David Forsyth. 2010. 每张图片讲述一个故事：从图像生成句子。见
    *欧洲计算机视觉会议*。Springer，15–29。
- en: Fathi et al. (2012) Alireza Fathi, Yin Li, and James M Rehg. 2012. Learning
    to recognize daily actions using gaze. In *European Conference on Computer Vision*.
    Springer, 314–327.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fathi et al. (2012) Alireza Fathi, Yin Li, 和 James M Rehg. 2012. 学习识别日常动作通过注视。见
    *欧洲计算机视觉会议*。Springer，314–327。
- en: 'Fedus et al. (2018) William Fedus, Ian Goodfellow, and Andrew M Dai. 2018.
    Maskgan: Better text generation via filling in the _. *arXiv preprint arXiv:1801.07736*.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fedus et al. (2018) William Fedus, Ian Goodfellow, 和 Andrew M Dai. 2018. Maskgan：通过填补_改进文本生成。*arXiv
    预印本 arXiv:1801.07736*。
- en: FitzGerald et al. (2013) Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer.
    2013. Learning distributions over logical forms for referring expression generation.
    In *Proceedings of the 2013 conference on empirical methods in natural language
    processing*. 1914–1925.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FitzGerald et al. (2013) Nicholas FitzGerald, Yoav Artzi, 和 Luke Zettlemoyer.
    2013. 学习逻辑形式上的分布以生成指代表达。见 *2013年自然语言处理实证方法会议论文集*。1914–1925。
- en: 'Frome et al. (2013) Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
    Jeff Dean, and Tomas Mikolov. 2013. Devise: A deep visual-semantic embedding model.
    In *Advances in neural information processing systems*. 2121–2129.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frome et al. (2013) Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
    Jeff Dean, 和 Tomas Mikolov. 2013. Devise: 深度视觉-语义嵌入模型。见 *神经信息处理系统进展*。2121–2129。'
- en: 'Gan et al. (2017a) Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng.
    2017a. Stylenet: Generating attractive visual captions with styles. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 3137–3146.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gan et al. (2017a) Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, 和 Li Deng.
    2017a. Stylenet: 用风格生成吸引人的视觉描述。见 *IEEE 计算机视觉与模式识别会议论文集*。3137–3146。'
- en: Gan et al. (2016) Chuang Gan, Tianbao Yang, and Boqing Gong. 2016. Learning
    attributes equals multi-source domain generalization. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 87–97.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan et al. (2016) Chuang Gan, Tianbao Yang, 和 Boqing Gong. 2016. 学习属性等于多源领域泛化。见
    *IEEE 计算机视觉与模式识别会议论文集*。87–97。
- en: Gan et al. (2017b) Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran,
    Jianfeng Gao, Lawrence Carin, and Li Deng. 2017b. Semantic compositional networks
    for visual captioning. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition (CVPR)*. 1141–1150.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan et al. (2017b) Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran,
    Jianfeng Gao, Lawrence Carin, 和 Li Deng. 2017b. 用于视觉描述的语义组合网络。见 *IEEE 计算机视觉与模式识别会议论文集
    (CVPR)*。1141–1150。
- en: Gehring et al. (2016) Jonas Gehring, Michael Auli, David Grangier, and Yann N
    Dauphin. 2016. A convolutional encoder model for neural machine translation. *arXiv
    preprint arXiv:1611.02344*.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring et al. (2016) Jonas Gehring, Michael Auli, David Grangier, 和 Yann N
    Dauphin. 2016. 用于神经机器翻译的卷积编码器模型。*arXiv 预印本 arXiv:1611.02344*。
- en: Gehring et al. (2017) Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
    and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. *arXiv
    preprint arXiv:1705.03122*.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring et al. (2017) Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
    和 Yann N Dauphin. 2017. 卷积序列到序列学习。*arXiv 预印本 arXiv:1705.03122*。
- en: Girshick (2015) Ross Girshick. 2015. Fast r-cnn. In *Proceedings of the IEEE
    international conference on computer vision*. 1440–1448.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick (2015) Ross Girshick. 2015. 快速 R-CNN。见 *IEEE 国际计算机视觉会议论文集*。1440–1448。
- en: Girshick et al. (2014) Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
    Malik. 2014. Rich feature hierarchies for accurate object detection and semantic
    segmentation. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 580–587.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick et al. (2014) Ross Girshick, Jeff Donahue, Trevor Darrell, 和 Jitendra
    Malik. 2014. 用于准确物体检测和语义分割的丰富特征层次结构。见 *IEEE 计算机视觉与模式识别会议论文集*。580–587。
- en: Golland et al. (2010) Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic
    approach to generating spatial descriptions. In *Proceedings of the 2010 conference
    on empirical methods in natural language processing*. Association for Computational
    Linguistics, 410–419.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golland et al. (2010) Dave Golland, Percy Liang, 和 Dan Klein. 2010. 生成空间描述的博弈论方法。见
    *2010年自然语言处理实证方法会议论文集*。计算语言学协会，410–419。
- en: Gong et al. (2014) Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hockenmaier,
    and Svetlana Lazebnik. 2014. Improving image-sentence embeddings using large weakly
    annotated photo collections. In *European Conference on Computer Vision*. Springer,
    529–545.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2014) Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hockenmaier,
    和 Svetlana Lazebnik. 2014. 使用大规模弱标注照片集改善图像-句子嵌入。见 *欧洲计算机视觉会议*。Springer，529–545。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. In *Advances in neural information processing systems*.
    2672–2680.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。见 *神经信息处理系统进展*。2672–2680。
- en: 'Gregor et al. (2015) Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez
    Rezende, and Daan Wierstra. 2015. DRAW: A recurrent neural network for image generation.
    In *Proceedings of Machine Learning Research*. 1462–1471.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gregor et al. (2015) 卡罗尔·格雷戈尔、伊沃·丹尼赫卡、亚历克斯·格雷夫斯、达尼洛·吉门内斯·雷泽德和丹·维尔斯特拉。2015年。DRAW：一种用于图像生成的递归神经网络。在
    *机器学习研究会议论文集*。1462–1471。
- en: 'Grubinger et al. (2006) Michael Grubinger, Paul Clough, Henning Müller, and
    Thomas Deselaers. 2006. The iapr tc-12 benchmark: A new evaluation resource for
    visual information systems. In *International workshop ontoImage*, Vol. 5\. 10.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grubinger et al. (2006) 迈克尔·格鲁宾格、保罗·克劳夫、亨宁·穆勒和托马斯·德塞拉斯。2006年。iapr tc-12基准测试：视觉信息系统的新评估资源。在
    *国际研讨会ontoImage*，第5卷。10。
- en: Gu et al. (2017) Jiuxiang Gu, Gang Wang, Jianfei Cai, and Tsuhan Chen. 2017.
    An empirical study of language cnn for image captioning. In *Proceedings of the
    International Conference on Computer Vision (ICCV)*. 1231–1240.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2017) 九湘顾、钢王、建飞蔡和祖涵陈。2017年。语言CNN在图像描述中的实证研究。在 *国际计算机视觉会议 (ICCV) 论文集*。1231–1240。
- en: Han and Li (2015) Yahong Han and Guang Li. 2015. Describing images with hierarchical
    concepts and object class localization. In *Proceedings of the 5th ACM on International
    Conference on Multimedia Retrieval*. ACM, 251–258.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han and Li (2015) 亚宏韩和光李。2015年。通过层次概念和对象类定位描述图像。在 *第5届ACM国际多媒体检索会议论文集*。ACM，251–258。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) 凯明何、向宇张、少青任和建孙。2016年。用于图像识别的深度残差学习。在 *IEEE计算机视觉与模式识别会议论文集*。770–778。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) 塞普·霍赫雷特和于尔根·施密德胡贝尔。1997年。长短期记忆。 *神经计算* 9,
    8 (1997), 1735–1780。
- en: 'Hodosh et al. (2013) Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013.
    Framing image description as a ranking task: Data, models and evaluation metrics.
    *Journal of Artificial Intelligence Research* 47 (2013), 853–899.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hodosh et al. (2013) 米卡·霍多什、彼得·杨和朱莉娅·霍肯迈尔。2013年。将图像描述框架化为排名任务：数据、模型和评估指标。 *人工智能研究期刊*
    47 (2013), 853–899。
- en: Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    2017. Image-to-image translation with conditional adversarial networks. In *Proceedings
    of the IEEE International Conference on Computer Vision (CVPR)*. 5967–5976.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola et al. (2017) 菲利普·伊索拉、俊彦朱、廷辉周和阿列克谢·A·埃夫罗斯。2017年。基于条件对抗网络的图像到图像翻译。在 *IEEE国际计算机视觉会议
    (CVPR) 论文集*。5967–5976。
- en: Jaderberg et al. (2015) Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
    2015. Spatial transformer networks. In *Advances in Neural Information Processing
    Systems*. 2017–2025.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg et al. (2015) 马克斯·贾德伯格、凯伦·西蒙扬、安德鲁·齐瑟曼等。2015年。空间变换网络。在 *神经信息处理系统进展*。2017–2025。
- en: Jang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical
    reparameterization with gumbel-softmax. In *International Conference on Learning
    Representations (ICLR)*.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang et al. (2017) 埃里克·张、石翔顾和本·普尔。2017年。具有Gumbel-Softmax的分类重参数化。在 *学习表示国际会议
    (ICLR)*。
- en: Jia et al. (2015) Xu Jia, Efstratios Gavves, Basura Fernando, and Tinne Tuytelaars.
    2015. Guiding the long-short term memory model for image caption generation. In
    *Proceedings of the IEEE International Conference on Computer Vision*. 2407–2415.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. (2015) 徐佳、埃夫斯特拉蒂奥斯·加夫维斯、巴苏拉·费尔南多和蒂内·图特拉尔斯。2015年。指导长短期记忆模型进行图像描述生成。在
    *IEEE国际计算机视觉会议论文集*。2407–2415。
- en: 'Jiang et al. (2015) Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao.
    2015. Salicon: Saliency in context. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1072–1080.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2015) 明江、盛盛黄、娟永段和齐赵。2015年。Salicon: 胶体背景下的显著性。在 *IEEE计算机视觉与模式识别会议论文集*。1072–1080。'
- en: 'Jin et al. (2015) Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, and Changshui Zhang.
    2015. Aligning where to see and what to tell: image caption with region-based
    attention and scene factorization. *arXiv preprint arXiv:1506.06272*.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2015) 军奇金、坤福、润鹏崔、飞沙和常帅张。2015年。对齐看什么和说什么：带有基于区域的注意力和场景分解的图像描述。 *arXiv预印本
    arXiv:1506.06272*。
- en: 'Johnson et al. (2016) Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016.
    Densecap: Fully convolutional localization networks for dense captioning. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 4565–4574.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等 (2016) Justin Johnson, Andrej Karpathy 和 Li Fei-Fei。2016。Densecap：用于密集字幕生成的全卷积定位网络。在
    *IEEE计算机视觉与模式识别会议论文集*。4565–4574。
- en: Johnson et al. (2015) Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia
    Li, David Shamma, Michael Bernstein, and Li Fei-Fei. 2015. Image retrieval using
    scene graphs. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*. 3668–3678.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等 (2015) Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David
    Shamma, Michael Bernstein 和 Li Fei-Fei。2015。使用场景图进行图像检索。在 *IEEE计算机视觉与模式识别会议论文集*。3668–3678。
- en: Jozefowicz et al. (2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. *arXiv
    preprint arXiv:1602.02410* (2016).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jozefowicz 等 (2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer
    和 Yonghui Wu。2016。探索语言建模的极限。*arXiv预印本 arXiv:1602.02410* (2016)。
- en: Karpathy and Fei-Fei (2015) Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic
    alignments for generating image descriptions. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*. 3128–3137.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 和 Fei-Fei (2015) Andrej Karpathy 和 Li Fei-Fei。2015。用于生成图像描述的深度视觉-语义对齐。在
    *IEEE计算机视觉与模式识别会议论文集*。3128–3137。
- en: Karpathy et al. (2014) Andrej Karpathy, Armand Joulin, and Fei Fei F Li. 2014.
    Deep fragment embeddings for bidirectional image sentence mapping. In *Advances
    in neural information processing systems*. 1889–1897.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 等 (2014) Andrej Karpathy, Armand Joulin 和 Fei Fei F Li。2014。用于双向图像句子映射的深度片段嵌入。在
    *神经信息处理系统进展*。1889–1897。
- en: Karthikeyan et al. (2013) S Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel
    Ecksteinz, and BS Manjunath. 2013. From where and how to what we see. In *Proceedings
    of the IEEE International Conference on Computer Vision*. 625–632.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karthikeyan 等 (2013) S Karthikeyan, Vignesh Jagadeesh, Renuka Shenoy, Miguel
    Ecksteinz 和 BS Manjunath。2013。从我们看到的“从哪里”和“如何”到“是什么”。在 *IEEE国际计算机视觉会议论文集*。625–632。
- en: 'Kazemzadeh et al. (2014) Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
    Tamara L Berg. 2014. ReferItGame: Referring to Objects in Photographs of Natural
    Scenes.. In *EMNLP*. 787–798.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazemzadeh 等 (2014) Sahar Kazemzadeh, Vicente Ordonez, Mark Matten 和 Tamara
    L Berg。2014。ReferItGame：在自然场景照片中指代对象。在 *EMNLP*。787–798。
- en: Kiros et al. (2014a) Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel. 2014a.
    Multimodal neural language models. In *Proceedings of the 31st International Conference
    on Machine Learning (ICML-14)*. 595–603.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiros 等 (2014a) Ryan Kiros, Ruslan Salakhutdinov 和 Rich Zemel。2014a。多模态神经语言模型。在
    *第31届国际机器学习会议 (ICML-14) 论文集*。595–603。
- en: Kiros et al. (2014b) Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel.
    2014b. Unifying visual-semantic embeddings with multimodal neural language models.
    In *Workshop on Neural Information Processing Systems (NIPS))*.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiros 等 (2014b) Ryan Kiros, Ruslan Salakhutdinov 和 Richard S Zemel。2014b。通过多模态神经语言模型统一视觉-语义嵌入。在
    *神经信息处理系统研讨会 (NIPS)*。
- en: Konda and Tsitsiklis (2000) Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic
    algorithms. In *Advances in neural information processing systems*. 1008–1014.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda 和 Tsitsiklis (2000) Vijay R Konda 和 John N Tsitsiklis。2000。演员-评论家算法。在
    *神经信息处理系统进展*。1008–1014。
- en: 'Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A
    Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced
    dense image annotations. *International Journal of Computer Vision* 123, 1 (2017),
    32–73.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等 (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji
    Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma
    等。2017。视觉基因组：通过众包密集图像注释连接语言与视觉。*国际计算机视觉杂志* 123, 1 (2017)，32–73。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097–1105.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton。2012。使用深度卷积神经网络进行Imagenet分类。在
    *神经信息处理系统进展*。1097–1105。
- en: 'Kulkarni et al. (2011) Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
    Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Baby talk: Understanding
    and generating image descriptions. In *Proceedings of the 24th CVPR*. Citeseer.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等 (2011) Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li,
    Yejin Choi, Alexander C Berg 和 Tamara L Berg。2011。婴儿语言：理解和生成图像描述。在 *第24届CVPR*。Citeseer。
- en: Kumar and Goel (2017) Akshi Kumar and Shivali Goel. 2017. A survey of evolution
    of image captioning techniques. *International Journal of Hybrid Intelligent Systems*
    Preprint, 1–19.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar and Goel (2017) Akshi Kumar 和 Shivali Goel. 2017. 图像描述技术演变的调查。*国际混合智能系统期刊*
    预印本，1–19。
- en: 'Kuznetsova et al. (2012) Polina Kuznetsova, Vicente Ordonez, Alexander C Berg,
    Tamara L Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions.
    In *Proceedings of the 50th Annual Meeting of the Association for Computational
    Linguistics: Long Papers-Volume 1*. Association for Computational Linguistics,
    359–368.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuznetsova et al. (2012) Polina Kuznetsova, Vicente Ordonez, Alexander C Berg,
    Tamara L Berg, 和 Yejin Choi. 2012. 自然图像描述的集体生成。发表于*第50届计算语言学协会年会：长篇论文集-第1卷*。计算语言学协会，359–368。
- en: 'Kuznetsova et al. (2014) Polina Kuznetsova, Vicente Ordonez, Tamara L Berg,
    and Yejin Choi. 2014. TREETALK: Composition and Compression of Trees for Image
    Descriptions. *TACL* 2, 10 (2014), 351–362.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuznetsova et al. (2014) Polina Kuznetsova, Vicente Ordonez, Tamara L Berg,
    和 Yejin Choi. 2014. TREETALK: 用于图像描述的树的组合与压缩。*TACL* 2, 10 (2014), 351–362。'
- en: Lebret et al. (2015) Rémi Lebret, Pedro O Pinheiro, and Ronan Collobert. 2015.
    Simple image description generator via a linear phrase-based approach. *Workshop
    on International Conference on Learning Representations (ICLR)*.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lebret et al. (2015) Rémi Lebret, Pedro O Pinheiro, 和 Ronan Collobert. 2015.
    通过线性短语方法生成简单图像描述。*国际学习表示大会(ICLR)研讨会*。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner.
    1998. 应用于文档识别的基于梯度的学习。*IEEE期刊* 86, 11 (1998), 2278–2324。
- en: Li et al. (2011) Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg,
    and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams.
    In *Proceedings of the Fifteenth Conference on Computational Natural Language
    Learning*. Association for Computational Linguistics, 220–228.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2011) Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg,
    和 Yejin Choi. 2011. 使用网络规模n-gram组成简单的图像描述。发表于*第十五届计算自然语言学习会议论文集*。计算语言学协会，220–228。
- en: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of
    summaries. In *Text summarization branches out: Proceedings of the ACL-04 workshop*,
    Vol. 8\. Barcelona, Spain.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: 自动评估摘要的工具包。发表于*文本摘要的拓展：ACL-04研讨会论文集*，第8卷。西班牙巴塞罗那。'
- en: Lin and Och (2004) Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation
    of machine translation quality using longest common subsequence and skip-bigram
    statistics. In *Proceedings of the 42nd Annual Meeting on Association for Computational
    Linguistics*. Association for Computational Linguistics, 605.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin and Och (2004) Chin-Yew Lin 和 Franz Josef Och. 2004. 使用最长公共子序列和跳跃-二元组统计的机器翻译质量自动评估。发表于*第42届计算语言学协会年会论文集*。计算语言学协会，605。
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft
    coco: Common objects in context. In *European conference on computer vision*.
    Springer, 740–755.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick. 2014. Microsoft
    coco: 上下文中的常见对象。发表于*欧洲计算机视觉大会*。Springer，740–755。'
- en: Liu et al. (2017a) Chenxi Liu, Junhua Mao, Fei Sha, and Alan L Yuille. 2017a.
    Attention Correctness in Neural Image Captioning. In *AAAI*. 4176–4182.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2017a) Chenxi Liu, Junhua Mao, Fei Sha, 和 Alan L Yuille. 2017a.
    神经图像描述中的注意力正确性。发表于*AAAI*。4176–4182。
- en: Liu et al. (2017b) Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin
    Murphy. 2017b. Improved image captioning via policy gradient optimization of spider.
    In *Proceedings of the IEEE Internationa Conference on Computer Vision (ICCV)*,
    Vol. 3\. 873–881.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2017b) Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, 和 Kevin
    Murphy. 2017b. 通过蜘蛛的策略梯度优化改进图像描述。发表于*IEEE国际计算机视觉大会(ICCV)论文集*，第3卷。873–881。
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. 3431–3440.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2015) Jonathan Long, Evan Shelhamer, 和 Trevor Darrell. 2015. 用于语义分割的完全卷积网络。发表于*IEEE计算机视觉与模式识别会议论文集*。3431–3440。
- en: Lowe (2004) David G Lowe. 2004. Distinctive image features from scale-invariant
    keypoints. *International journal of computer vision* 60, 2 (2004), 91–110.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe (2004) David G Lowe. 2004. 从尺度不变关键点提取的独特图像特征。*国际计算机视觉期刊* 60, 2 (2004),
    91–110。
- en: 'Lu et al. (2017) Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.
    2017. Knowing when to look: Adaptive attention via A visual sentinel for image
    captioning. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. 3242–3250.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2017）Jiasen Lu, Caiming Xiong, Devi Parikh, 和 Richard Socher。2017年。知道何时查看：通过视觉哨兵的自适应注意力用于图像字幕生成。在
    *IEEE 计算机视觉与模式识别会议（CVPR）* 上。3242–3250。
- en: Luong et al. (2016) Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals,
    and Lukasz Kaiser. 2016. Multi-task sequence to sequence learning. In *International
    Conference on Learning Representations (ICLR)*.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong 等（2016）Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, 和 Lukasz
    Kaiser。2016年。多任务序列到序列学习。在 *国际学习表征会议（ICLR）* 上。
- en: Ma and Han (2016) Shubo Ma and Yahong Han. 2016. Describing images by feeding
    LSTM with structural words. In *Multimedia and Expo (ICME), 2016 IEEE International
    Conference on*. IEEE, 1–6.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 和 Han（2016）Shubo Ma 和 Yahong Han。2016年。通过将结构化词喂入 LSTM 描述图像。在 *2016 IEEE 国际多媒体与博览会（ICME）*
    上。IEEE, 1–6。
- en: 'Maddison et al. (2017) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2017.
    The concrete distribution: A continuous relaxation of discrete random variables.
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maddison 等（2017）Chris J Maddison, Andriy Mnih, 和 Yee Whye Teh。2017年。具体分布：离散随机变量的连续放松。在
    *国际学习表征会议（ICLR）* 上。
- en: Mao et al. (2016) Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu,
    Alan L Yuille, and Kevin Murphy. 2016. Generation and comprehension of unambiguous
    object descriptions. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 11–20.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2016）Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan
    L Yuille, 和 Kevin Murphy。2016年。生成和理解明确的对象描述。在 *IEEE 计算机视觉与模式识别会议* 上。11–20。
- en: 'Mao et al. (2015a) Junhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng Huang,
    and Alan L Yuille. 2015a. Learning like a child: Fast novel visual concept learning
    from sentence descriptions of images. In *Proceedings of the IEEE International
    Conference on Computer Vision*. 2533–2541.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2015a）Junhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng Huang, 和 Alan L
    Yuille。2015a年。像孩子一样学习：通过图像的句子描述快速学习新视觉概念。在 *IEEE 国际计算机视觉会议* 上。2533–2541。
- en: Mao et al. (2015b) Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and
    Alan Yuille. 2015b. Deep captioning with multimodal recurrent neural networks
    (m-rnn). In *International Conference on Learning Representations (ICLR)*.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2015b）Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, 和 Alan Yuille。2015b年。使用多模态递归神经网络（m-rnn）进行深度字幕生成。在
    *国际学习表征会议（ICLR）* 上。
- en: Mao et al. (2014) Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L Yuille.
    2014. Explain images with multimodal recurrent neural networks. *arXiv preprint
    arXiv:1410.1090* (2014).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2014）Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, 和 Alan L Yuille。2014年。通过多模态递归神经网络解释图像。*arXiv
    预印本 arXiv:1410.1090*（2014）。
- en: Maron and Lozano-Pérez (1998) Oded Maron and Tomás Lozano-Pérez. 1998. A framework
    for multiple-instance learning. In *Advances in neural information processing
    systems*. 570–576.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maron 和 Lozano-Pérez（1998）Oded Maron 和 Tomás Lozano-Pérez。1998年。多实例学习框架。在 *神经信息处理系统进展*
    上。570–576。
- en: 'Mathews et al. (2016) Alexander Patrick Mathews, Lexing Xie, and Xuming He.
    2016. SentiCap: Generating Image Descriptions with Sentiments.. In *AAAI*. 3574–3580.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mathews 等（2016）Alexander Patrick Mathews, Lexing Xie, 和 Xuming He。2016年。SentiCap：生成带有情感的图像描述。在
    *AAAI* 上。3574–3580。
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient estimation of word representations in vector space. *arXiv preprint
    arXiv:1301.3781*.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2013）Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean。2013年。在向量空间中高效估计词表示。*arXiv
    预印本 arXiv:1301.3781*。
- en: Mikolov et al. (2010) Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ,
    and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In
    *Eleventh Annual Conference of the International Speech Communication Association*.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2010）Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, 和
    Sanjeev Khudanpur。2010年。基于递归神经网络的语言模型。在 *第十一届国际语音通信协会年会* 上。
- en: Mishra et al. (2012) Ajay K Mishra, Yiannis Aloimonos, Loong Fah Cheong, and
    Ashraf Kassim. 2012. Active visual segmentation. *IEEE transactions on pattern
    analysis and machine intelligence* 34, 4 (2012), 639–653.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等（2012）Ajay K Mishra, Yiannis Aloimonos, Loong Fah Cheong, 和 Ashraf Kassim。2012年。主动视觉分割。*IEEE
    模式分析与机器智能汇刊* 34, 4 (2012), 639–653。
- en: 'Mitchell et al. (2012) Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch,
    Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daumé III.
    2012. Midge: Generating image descriptions from computer vision detections. In
    *Proceedings of the 13th Conference of the European Chapter of the Association
    for Computational Linguistics*. Association for Computational Linguistics, 747–756.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell 等 (2012) Margaret Mitchell、Xufeng Han、Jesse Dodge、Alyssa Mensch、Amit
    Goyal、Alex Berg、Kota Yamaguchi、Tamara Berg、Karl Stratos 和 Hal Daumé III。2012年。Midge：从计算机视觉检测生成图像描述。发表于
    *第13届欧洲计算语言学协会会议论文集*。计算语言学协会，747–756。
- en: Mitchell et al. (2010) Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
    2010. Natural reference to objects in a visual domain. In *Proceedings of the
    6th international natural language generation conference*. Association for Computational
    Linguistics, 95–104.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell 等 (2010) Margaret Mitchell、Kees van Deemter 和 Ehud Reiter。2010年。在视觉领域自然引用对象。发表于
    *第6届国际自然语言生成会议论文集*。计算语言学协会，95–104。
- en: Mitchell et al. (2013) Margaret Mitchell, Kees Van Deemter, and Ehud Reiter.
    2013. Generating Expressions that Refer to Visible Objects.. In *HLT-NAACL*. 1174–1184.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell 等 (2013) Margaret Mitchell、Kees Van Deemter 和 Ehud Reiter。2013年。生成指向可见对象的表达式。发表于
    *HLT-NAACL*。1174–1184。
- en: Mnih and Hinton (2007a) Andriy Mnih and Geoffrey Hinton. 2007a. Three new graphical
    models for statistical language modelling. In *Proceedings of the 24th international
    conference on Machine learning*. ACM, 641–648.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 和 Hinton (2007a) Andriy Mnih 和 Geoffrey Hinton。2007a年。用于统计语言建模的三种新图模型。发表于
    *第24届国际机器学习会议论文集*。ACM，641–648。
- en: Mnih and Hinton (2007b) Andriy Mnih and Geoffrey Hinton. 2007b. Three new graphical
    models for statistical language modelling. In *Proceedings of the 24th international
    conference on Machine learning*. ACM, 641–648.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 和 Hinton (2007b) Andriy Mnih 和 Geoffrey Hinton。2007b年。用于统计语言建模的三种新图模型。发表于
    *第24届国际机器学习会议论文集*。ACM，641–648。
- en: Och (2003) Franz Josef Och. 2003. Minimum error rate training in statistical
    machine translation. In *Proceedings of the 41st Annual Meeting on Association
    for Computational Linguistics-Volume 1*. Association for Computational Linguistics,
    160–167.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Och (2003) Franz Josef Och。2003年。统计机器翻译中的最小误差率训练。发表于 *第41届计算语言学协会年会论文集-第1卷*。计算语言学协会，160–167。
- en: Ojala et al. (2000) Timo Ojala, Matti Pietikäinen, and Topi Mäenpää. 2000. Gray
    scale and rotation invariant texture classification with local binary patterns.
    In *European Conference on Computer Vision*. Springer, 404–420.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojala 等 (2000) Timo Ojala、Matti Pietikäinen 和 Topi Mäenpää。2000年。使用局部二值模式的灰度和旋转不变纹理分类。发表于
    *欧洲计算机视觉会议*。Springer，404–420。
- en: 'Ordonez et al. (2011) Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
    2011. Im2text: Describing images using 1 million captioned photographs. In *Advances
    in Neural Information Processing Systems*. 1143–1151.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ordonez 等 (2011) Vicente Ordonez、Girish Kulkarni 和 Tamara L Berg。2011年。Im2text：使用100万张带注释的照片描述图像。发表于
    *神经信息处理系统进展*。1143–1151。
- en: Papadopoulos et al. (2014) Dim P Papadopoulos, Alasdair DF Clarke, Frank Keller,
    and Vittorio Ferrari. 2014. Training object class detectors from eye tracking
    data. In *European Conference on Computer Vision*. Springer, 361–376.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papadopoulos 等 (2014) Dim P Papadopoulos、Alasdair DF Clarke、Frank Keller 和 Vittorio
    Ferrari。2014年。从眼动追踪数据训练对象类别检测器。发表于 *欧洲计算机视觉会议*。Springer，361–376。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting on association for computational linguistics*.
    Association for Computational Linguistics, 311–318.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等 (2002) Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu。2002年。BLEU：一种自动评估机器翻译的方法。发表于
    *第40届计算语言学协会年会论文集*。计算语言学协会，311–318。
- en: 'Park et al. (2017) Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. 2017.
    Attend to You: Personalized Image Captioning with Context Sequence Memory Networks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*. 6432–6440.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2017) Cesc Chunseong Park、Byeongchang Kim 和 Gunhee Kim。2017年。关注你：具有上下文序列记忆网络的个性化图像标题生成。发表于
    *IEEE计算机视觉与模式识别会议 (CVPR)*。6432–6440。
- en: Pedersoli et al. (2017) Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and
    Jakob Verbeek. 2017. Areas of Attention for Image Captioning. In *Proceedings
    of the IEEE international conference on computer vision*. 1251–1259.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedersoli 等 (2017) Marco Pedersoli、Thomas Lucas、Cordelia Schmid 和 Jakob Verbeek。2017年。图像标题生成的关注区域。发表于
    *IEEE国际计算机视觉会议论文集*。1251–1259。
- en: 'Plummer et al. (2015) Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C
    Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting
    region-to-phrase correspondences for richer image-to-sentence models. In *Proceedings
    of the IEEE international conference on computer vision*. 2641–2649.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plummer et al. (2015) Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C
    Caicedo, Julia Hockenmaier, 和 Svetlana Lazebnik. 2015. Flickr30k 实体：收集区域到短语的对应关系以丰富图像到句子的模型。在
    *IEEE 国际计算机视觉会议论文集* 中，2641–2649。
- en: Ranzato et al. (2016) Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and
    Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks.
    In *International Conference on learning Representations (ICLR)*.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranzato et al. (2016) Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, 和 Wojciech
    Zaremba. 2016. 使用递归神经网络进行序列级训练。在 *国际学习表征会议 (ICLR)* 中。
- en: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis.
    In *Proceedings of Machine Learning Research*, Vol. 48\. 1060–1069.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, 和 Honglak Lee. 2016. 生成对抗文本到图像合成。在 *机器学习研究会议论文集* 中，第 48 卷，1060–1069。
- en: 'Ren et al. (2015a) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015a.
    Faster R-CNN: Towards real-time object detection with region proposal networks.
    In *Advances in neural information processing systems*. 91–99.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren et al. (2015a) Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. 2015a.
    Faster R-CNN: 朝向实时物体检测的区域提议网络。在 *神经信息处理系统进展* 中，91–99。'
- en: Ren et al. (2015b) Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, and Alan Yuille.
    2015b. Multi-instance visual-semantic embedding. *arXiv preprint arXiv:1512.06963*
    (2015).
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2015b) Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, 和 Alan Yuille.
    2015b. 多实例视觉-语义嵌入。*arXiv 预印本 arXiv:1512.06963* (2015)。
- en: Ren et al. (2016) Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, and Alan Yuille.
    2016. Joint image-text representation by gaussian visual-semantic embedding. In
    *Proceedings of the 2016 ACM on Multimedia Conference*. ACM, 207–211.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2016) Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, 和 Alan Yuille. 2016.
    通过高斯视觉-语义嵌入进行联合图像-文本表示。在 *2016 ACM 多媒体会议论文集* 中，ACM，207–211。
- en: Ren et al. (2017) Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and Li-Jia Li.
    2017. Deep Reinforcement Learning-based Image Captioning with Embedding Reward.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*. 1151–1159.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2017) Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, 和 Li-Jia Li.
    2017. 基于深度强化学习的图像描述生成与嵌入奖励。在 *IEEE 计算机视觉与模式识别会议论文集 (CVPR)* 中，1151–1159。
- en: Rennie et al. (2017) Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret
    Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*. 1179–1195.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rennie et al. (2017) Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret
    Ross, 和 Vaibhava Goel. 2017. 自我批评序列训练用于图像描述生成。在 *IEEE 计算机视觉与模式识别会议论文集 (CVPR)*
    中，1179–1195。
- en: 'Robertson (2004) Stephen Robertson. 2004. Understanding inverse document frequency:
    on theoretical arguments for IDF. *Journal of documentation* 60, 5 (2004), 503–520.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson (2004) Stephen Robertson. 2004. 理解逆文档频率：关于 IDF 的理论论证。*文献学期刊* 60, 5
    (2004), 503–520。
- en: Sattar et al. (2015) Hosnieh Sattar, Sabine Muller, Mario Fritz, and Andreas
    Bulling. 2015. Prediction of search targets from fixations in open-world settings.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    981–990.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sattar et al. (2015) Hosnieh Sattar, Sabine Muller, Mario Fritz, 和 Andreas Bulling.
    2015. 从注视中预测开放世界设置下的搜索目标。在 *IEEE 计算机视觉与模式识别会议论文集* 中，981–990。
- en: Schuster et al. (2015) Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei,
    and Christopher D Manning. 2015. Generating semantically precise scene graphs
    from textual descriptions for improved image retrieval. In *Proceedings of the
    fourth workshop on vision and language*, Vol. 2.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster et al. (2015) Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei,
    和 Christopher D Manning. 2015. 从文本描述生成语义精确的场景图以改善图像检索。在 *第四届视觉与语言研讨会论文集* 中，第 2
    卷。
- en: Shanmuga Vadivel et al. (2015) Karthikeyan Shanmuga Vadivel, Thuyen Ngo, Miguel
    Eckstein, and BS Manjunath. 2015. Eye tracking assisted extraction of attentionally
    important objects from videos. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*. 3241–3250.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanmuga Vadivel et al. (2015) Karthikeyan Shanmuga Vadivel, Thuyen Ngo, Miguel
    Eckstein, 和 BS Manjunath. 2015. 通过眼动追踪辅助从视频中提取重要关注对象。在 *IEEE 计算机视觉与模式识别会议论文集*
    中，3241–3250。
- en: Sharif et al. (2018) Naeha Sharif, Lyndon White, Mohammed Bennamoun, and Syed
    Afaq Ali Shah. 2018. Learning-based Composite Metrics for Improved Caption Evaluation.
    In *Proceedings of ACL 2018, Student Research Workshop*. 14–20.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharif 等（2018）Naeha Sharif、Lyndon White、Mohammed Bennamoun 和 Syed Afaq Ali Shah。2018年。基于学习的复合度量用于改进描述评估。在
    *ACL 2018 会议，学生研究研讨会*。14–20。
- en: 'Shetty et al. (2017) Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks,
    Mario Fritz, and Bernt Schiele. 2017. Speaking the Same Language: Matching Machine
    to Human Captions by Adversarial Training. In *IEEE International Conference on
    Computer Vision (ICCV)*. 4155–4164.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shetty 等（2017）Rakshith Shetty、Marcus Rohrbach、Lisa Anne Hendricks、Mario Fritz
    和 Bernt Schiele。2017年。说同一种语言：通过对抗训练将机器描述与人类描述匹配。在 *IEEE国际计算机视觉会议（ICCV）*。4155–4164。
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    deep convolutional networks for large-scale image recognition. In *International
    Conference on Learning Representations (ICLR)*.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2015）Karen Simonyan 和 Andrew Zisserman。2015年。用于大规模图像识别的非常深的卷积网络。在
    *国际学习表示会议（ICLR）*。
- en: Socher et al. (2014) Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D
    Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and
    describing images with sentences. *Transactions of the Association for Computational
    Linguistics* 2 (2014), 207–218.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher 等（2014）Richard Socher、Andrej Karpathy、Quoc V Le、Christopher D Manning
    和 Andrew Y Ng。2014年。通过句子查找和描述图像的基础组合语义。 *计算语言学学会会刊* 2（2014），207–218。
- en: 'Sugano and Bulling (2016) Yusuke Sugano and Andreas Bulling. 2016. Seeing with
    humans: Gaze-assisted neural image captioning. *arXiv preprint arXiv:1608.05203*
    (2016).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sugano 和 Bulling（2016）Yusuke Sugano 和 Andreas Bulling。2016年。与人类共同观察：注视辅助的神经图像描述。
    *arXiv 预印本 arXiv:1608.05203*（2016年）。
- en: Sun et al. (2015) Chen Sun, Chuang Gan, and Ram Nevatia. 2015. Automatic concept
    discovery from parallel text and visual corpora. In *Proceedings of the IEEE International
    Conference on Computer Vision*. 2596–2604.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2015）Chen Sun、Chuang Gan 和 Ram Nevatia。2015年。从平行文本和视觉语料库中自动发现概念。在 *IEEE国际计算机视觉会议论文集*。2596–2604。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. In *Advances in neural information
    processing systems*. 3104–3112.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）Ilya Sutskever、Oriol Vinyals 和 Quoc V Le。2014年。使用神经网络进行序列到序列学习。在
    *神经信息处理系统进展*。3104–3112。
- en: Sutton et al. (2000) Richard S Sutton, David A McAllester, Satinder P Singh,
    and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with
    function approximation. In *Advances in neural information processing systems*.
    1057–1063.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等（2000）Richard S Sutton、David A McAllester、Satinder P Singh 和 Yishay
    Mansour。2000年。具有函数逼近的强化学习策略梯度方法。在 *神经信息处理系统进展*。1057–1063。
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1–9.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2015）Christian Szegedy、Wei Liu、Yangqing Jia、Pierre Sermanet、Scott
    Reed、Dragomir Anguelov、Dumitru Erhan、Vincent Vanhoucke 和 Andrew Rabinovich。2015年。通过卷积深入探索。在
    *IEEE计算机视觉与模式识别会议论文集*。1–9。
- en: Tavakoli et al. (2017) Hamed R Tavakoli, Rakshith Shetty, Ali Borji, and Jorma
    Laaksonen. 2017. Paying Attention to Descriptions Generated by Image Captioning
    Models. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*. 2487–2496.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavakoli 等（2017）Hamed R Tavakoli、Rakshith Shetty、Ali Borji 和 Jorma Laaksonen。2017年。关注图像描述生成模型生成的描述。在
    *IEEE计算机视觉与模式识别会议论文集*。2487–2496。
- en: Tran et al. (2016) Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia
    Carapcea, Chris Thrasher, Chris Buehler, and Chris Sienkiewicz. 2016. Rich image
    captioning in the wild. In *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Workshops*. 49–56.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等（2016）Kenneth Tran、Xiaodong He、Lei Zhang、Jian Sun、Cornelia Carapcea、Chris
    Thrasher、Chris Buehler 和 Chris Sienkiewicz。2016年。自然环境中的丰富图像描述。在 *IEEE计算机视觉与模式识别会议研讨会论文集*。49–56。
- en: van Deemter et al. (2006) Kees van Deemter, Ielka van der Sluis, and Albert
    Gatt. 2006. Building a semantically transparent corpus for the generation of referring
    expressions. In *Proceedings of the Fourth International Natural Language Generation
    Conference*. Association for Computational Linguistics, 130–132.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Deemter 等（2006）Kees van Deemter、Ielka van der Sluis 和 Albert Gatt。2006年。构建一个语义透明的语料库用于生成指代表达。在
    *第四届国际自然语言生成会议论文集*。计算语言学学会，130–132。
- en: van den Oord et al. (2016) Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
    Oriol Vinyals, Alex Graves, et al. 2016. Conditional image generation with pixelcnn
    decoders. In *Advances in Neural Information Processing Systems*. 4790–4798.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Oord 等人（2016）Aaron van den Oord、Nal Kalchbrenner、Lasse Espeholt、Oriol
    Vinyals、Alex Graves 等人。2016年。基于 PixelCNN 解码器的条件图像生成。在 *神经信息处理系统进展*。4790–4798。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in Neural Information Processing Systems*. 5998–6008.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017）Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N Gomez、Łukasz Kaiser 和 Illia Polosukhin。2017年。注意力机制是你所需要的一切。在 *神经信息处理系统进展*。5998–6008。
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    2015. Cider: Consensus-based image description evaluation. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*. 4566–4575.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vedantam 等人（2015）Ramakrishna Vedantam、C Lawrence Zitnick 和 Devi Parikh。2015年。Cider:
    基于共识的图像描述评估。在 *IEEE 计算机视觉与模式识别会议论文集*。4566–4575。'
- en: Venugopalan et al. (2017) Subhashini Venugopalan, Lisa Anne Hendricks, Marcus
    Rohrbach, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2017. Captioning images
    with diverse objects. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1170–1178.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venugopalan 等人（2017）Subhashini Venugopalan、Lisa Anne Hendricks、Marcus Rohrbach、Raymond
    Mooney、Trevor Darrell 和 Kate Saenko。2017年。对包含多样物体的图像进行描述。在 *IEEE 计算机视觉与模式识别会议论文集*。1170–1178。
- en: Viethen and Dale (2008) Jette Viethen and Robert Dale. 2008. The use of spatial
    relations in referring expression generation. In *Proceedings of the Fifth International
    Natural Language Generation Conference*. Association for Computational Linguistics,
    59–67.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viethen 和 Dale（2008）Jette Viethen 和 Robert Dale。2008年。空间关系在指代表达生成中的应用。在 *第五届国际自然语言生成会议论文集*。计算语言学协会，59–67。
- en: 'Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. 2015. Show and tell: A neural image caption generator. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 3156–3164.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals 等人（2015）Oriol Vinyals、Alexander Toshev、Samy Bengio 和 Dumitru Erhan。2015年。Show
    and tell: 一种神经图像描述生成器。在 *IEEE 计算机视觉与模式识别会议论文集*。3156–3164。'
- en: 'Vinyals et al. (2017) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. 2017. Show and tell: Lessons learned from the 2015 mscoco image captioning
    challenge. *IEEE transactions on pattern analysis and machine intelligence* 39,
    4 (2017), 652–663.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals 等人（2017）Oriol Vinyals、Alexander Toshev、Samy Bengio 和 Dumitru Erhan。2017年。Show
    and tell: 从 2015 年 MSCoco 图像描述挑战中获得的经验。*IEEE 模式分析与机器智能学报* 39，4（2017），652–663。'
- en: Wang et al. (2016b) Cheng Wang, Haojin Yang, Christian Bartz, and Christoph
    Meinel. 2016b. Image captioning with deep bidirectional LSTMs. In *Proceedings
    of the 2016 ACM on Multimedia Conference*. ACM, 988–997.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2016b）Cheng Wang、Haojin Yang、Christian Bartz 和 Christoph Meinel。2016b年。利用深度双向
    LSTM 进行图像描述。在 *2016 ACM 多媒体会议论文集*。ACM，988–997。
- en: Wang et al. (2018) Heng Wang, Zengchang Qin, and Tao Wan. 2018. Text Generation
    Based on Generative Adversarial Nets with Latent Variables. In *Pacific-Asia Conference
    on Knowledge Discovery and Data Mining*. Springer, 92–103.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018）Heng Wang、Zengchang Qin 和 Tao Wan。2018年。基于生成对抗网络的文本生成。*太平洋-亚洲知识发现与数据挖掘会议*。Springer，92–103。
- en: Wang et al. (2016a) Minsi Wang, Li Song, Xiaokang Yang, and Chuanfei Luo. 2016a.
    A parallel-fusion RNN-LSTM architecture for image caption generation. In *Image
    Processing (ICIP), 2016 IEEE International Conference on*. IEEE, 4448–4452.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2016a）Minsi Wang、Li Song、Xiaokang Yang 和 Chuanfei Luo。2016a年。用于图像描述生成的并行融合
    RNN-LSTM 架构。在 *2016 IEEE 国际图像处理会议（ICIP）*。IEEE，4448–4452。
- en: 'Wang and Chan (2018) Qingzhong Wang and Antoni B Chan. 2018. CNN+ CNN: Convolutional
    Decoders for Image Captioning. *arXiv preprint arXiv:1805.09019*.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Chan（2018）Qingzhong Wang 和 Antoni B Chan。2018年。CNN+ CNN: 用于图像描述的卷积解码器。*arXiv
    预印本 arXiv:1805.09019*。'
- en: 'Wang et al. (2017) Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, and Garrison W
    Cottrell. 2017. Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*. 7378–7387.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2017）Yufei Wang、Zhe Lin、Xiaohui Shen、Scott Cohen 和 Garrison W Cottrell。2017年。Skeleton
    Key: 通过 Skeleton-Attribute 分解进行图像描述。在 *IEEE 计算机视觉与模式识别会议论文集 (CVPR)*。7378–7387。'
- en: Wu et al. (2015) Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, and
    Anthony Dick. 2015. Image captioning with an intermediate attributes layer. *arXiv
    preprint arXiv:1506.01144* (2015).
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2015) 吴琪，沈春华，安东·范·登·亨格尔，刘灵桥，和安东尼·迪克。2015。带有中间属性层的图像描述。*arXiv 预印本
    arXiv:1506.01144*（2015）。
- en: Wu et al. (2018) Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and Anton van den
    Hengel. 2018. Image captioning and visual question answering based on attributes
    and external knowledge. *IEEE transactions on pattern analysis and machine intelligence*
    40, 6, 1367–1381.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2018) 吴琪，沈春华，彭旺，安东尼·迪克，和安东·范·登·亨格尔。2018。基于属性和外部知识的图像描述和视觉问答。*IEEE模式分析与机器智能交易*
    40，第6期，1367–1381。
- en: 'Wu and Cohen (2016) Zhilin Yang Ye Yuan Yuexin Wu and Ruslan Salakhutdinov
    William W Cohen. 2016. Encode, Review, and Decode: Reviewer Module for Caption
    Generation. In *30th Conference on Neural Image Processing System(NIPS)*.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Cohen (2016) 吴志林，袁叶，岳欣吴和鲁斯兰·萨拉赫乌丁诺夫，威廉·W·科恩。2016。编码、回顾和解码：描述生成的评论员模块。
    在*第30届神经图像处理系统会议（NIPS）*。
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell:
    Neural image caption generation with visual attention. In *International Conference
    on Machine Learning*. 2048–2057.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2015) 许凯文，吉米·巴，瑞安·基罗斯，崔京贤，亚伦·库维尔，鲁斯兰·萨拉赫乌丁诺夫，瑞奇·泽梅尔，和约书亚·本吉奥。2015。展示、关注和讲述：具有视觉注意力的神经图像描述生成。
    在*国际机器学习会议*。2048–2057。
- en: Yang et al. (2016) Linjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li. 2016.
    Dense Captioning with Joint Inference and Visual Context. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 1978–1987.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2016) 杨林杰，唐凯文，杨建超，和李李佳。2016。通过联合推理和视觉上下文进行密集描述。在*IEEE计算机视觉与模式识别会议（CVPR）*。1978–1987。
- en: Yao et al. (2017a) Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2017a. Incorporating
    copying mechanism in image captioning for learning novel objects. In *2017 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE, 5263–5271.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2017a) 姚婷，潘英伟，李业昊，和梅涛。2017a。在图像描述中引入复制机制以学习新对象。 在*2017 IEEE计算机视觉与模式识别会议（CVPR）*。IEEE，5263–5271。
- en: Yao et al. (2017b) Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei.
    2017b. Boosting image captioning with attributes. In *IEEE International Conference
    on Computer Vision (ICCV)*. 4904–4912.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2017b) 姚婷，潘英伟，李业昊，邱兆凡，和梅涛。2017b。通过属性提升图像描述。 在*IEEE国际计算机视觉会议（ICCV）*。4904–4912。
- en: You et al. (2016) Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo
    Luo. 2016. Image captioning with semantic attention. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 4651–4659.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2016) 游全增，金海林，王兆文，方晨，和罗洁波。2016。带有语义注意力的图像描述。 在*IEEE计算机视觉与模式识别会议论文集*。4651–4659。
- en: Yu et al. (2017) Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu Seqgan. 2017.
    Sequence generative adversarial nets with policy gradient.. In *Proceedings of
    the Thirty-First AAAI Conference on Artificial Intelligence*.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2017) 余兰涛，张伟南，王军，和杨宇。2017。具有策略梯度的序列生成对抗网络。 在*第三十一届AAAI人工智能会议论文集*。
- en: Yun et al. (2013) Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J Zelinsky,
    and Tamara L Berg. 2013. Exploring the role of gaze behavior and object detection
    in scene understanding. *Frontiers in psychology* 4 (2013).
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun et al. (2013) 尹基元，彭一凡，迪米特里斯·萨马拉斯，格雷戈里·J·泽林斯基，和塔玛拉·L·伯格。2013。探讨注视行为和物体检测在场景理解中的作用。*心理学前沿*
    4（2013）。
- en: Zeiler and Fergus (2014) Matthew D Zeiler and Rob Fergus. 2014. Visualizing
    and understanding convolutional networks. In *European conference on computer
    vision*. Springer, 818–833.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler and Fergus (2014) 马修·D·泽伊勒和罗布·费格斯。2014。可视化和理解卷积网络。 在*欧洲计算机视觉会议*。Springer，818–833。
- en: Zelinsky (2013) Gregory J Zelinsky. 2013. Understanding scene understanding.
    *Frontiers in psychology* 4 (2013).
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelinsky (2013) 格雷戈里·J·泽林斯基。2013。理解场景理解。*心理学前沿* 4（2013）。
- en: Zhang et al. (2017) Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Shaogang Gong,
    Yongxin Yang, and Timothy M Hospedales. 2017. Actor-critic sequence training for
    image captioning. *arXiv preprint arXiv:1706.09601*.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2017) 李张，洪泛，刘峰，向涛，龚绍刚，杨永鑫，和蒂莫西·M·霍斯佩代利斯。2017。用于图像描述的演员-评论家序列训练。*arXiv
    预印本 arXiv:1706.09601*。
