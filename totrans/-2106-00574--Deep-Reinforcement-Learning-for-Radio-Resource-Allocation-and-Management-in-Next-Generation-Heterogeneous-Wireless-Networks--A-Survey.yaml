- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:54:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:54:52'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.00574] Deep Reinforcement Learning for Radio Resource Allocation and
    Management in Next Generation Heterogeneous Wireless Networks: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.00574] 深度强化学习在下一代异质无线网络中的无线资源分配和管理：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.00574](https://ar5iv.labs.arxiv.org/html/2106.00574)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.00574](https://ar5iv.labs.arxiv.org/html/2106.00574)
- en: 'Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在下一代异质无线网络中的无线资源分配和管理：综述
- en: Abdulmalik Alwarafy, Mohamed Abdallah, , Bekir Sait Ciftler, ,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Abdulmalik Alwarafy, Mohamed Abdallah, , Bekir Sait Ciftler, ,
- en: 'Ala Al-Fuqaha, , and Mounir Hamdi, The authors are with the Division of Information
    and Computing Technology, College of Science and Engineering, Hamad Bin Khalifa
    University, Qatar (e-mail: aalwarafy@hbku.edu.qa; moabdallah@hbku.edu.qa; bciftler@hbku.edu.qa;
    aalfuqaha@hbku.edu.qa; mhamdi@hbku.edu.qa). This work has been submitted to the
    IEEE for possible publication. Copyright may be transferred without notice, after
    which this version may no longer be accessible.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ala Al-Fuqaha, 和 Mounir Hamdi，作者来自卡塔尔哈马德本哈利法大学科学与工程学院信息与计算技术部门（电子邮件：aalwarafy@hbku.edu.qa;
    moabdallah@hbku.edu.qa; bciftler@hbku.edu.qa; aalfuqaha@hbku.edu.qa; mhamdi@hbku.edu.qa）。此工作已提交给IEEE，以便可能发表。版权可能会在没有通知的情况下转移，转移后该版本可能不再可访问。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: Next generation wireless networks are expected to be extremely complex due to
    their massive heterogeneity in terms of the types of network architectures they
    incorporate, the types and numbers of smart IoT devices they serve, and the types
    of emerging applications they support. In such large-scale and heterogeneous networks
    (HetNets), radio resource allocation and management (RRAM) becomes one of the
    major challenges encountered during system design and deployment. In this context,
    emerging Deep Reinforcement Learning (DRL) techniques are expected to be one of
    the main enabling technologies to address the RRAM in future wireless HetNets.
    In this paper, we conduct a systematic in-depth, and comprehensive survey of the
    applications of DRL techniques in RRAM for next generation wireless networks.
    Towards this, we first overview the existing traditional RRAM methods and identify
    their limitations that motivate the use of DRL techniques in RRAM. Then, we provide
    a comprehensive review of the most widely used DRL algorithms to address RRAM
    problems, including the value- and policy-based algorithms. The advantages, limitations,
    and use-cases for each algorithm are provided. We then conduct a comprehensive
    and in-depth literature review and classify existing related works based on both
    the radio resources they are addressing and the type of wireless networks they
    are investigating. To this end, we carefully identify the types of DRL algorithms
    utilized in each related work, the elements of these algorithms, and the main
    findings of each related work. Finally, we highlight important open challenges
    and provide insights into several future research directions in the context of
    DRL-based RRAM. This survey is intentionally designed to guide and stimulate more
    research endeavors towards building efficient and fine-grained DRL-based RRAM
    schemes for future wireless networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代无线网络由于其包含的网络架构类型、所服务的智能物联网设备的类型和数量以及支持的新兴应用类型等方面的巨大异质性，预计将极其复杂。在这种大规模异质网络（HetNets）中，无线资源分配和管理（RRAM）成为系统设计和部署中面临的主要挑战之一。在这种背景下，新兴的深度强化学习（DRL）技术预计将成为解决未来无线HetNets中RRAM的主要
    enabling 技术。本文系统地深入且全面地调查了DRL技术在下一代无线网络RRAM中的应用。为此，我们首先概述了现有的传统RRAM方法，并识别出它们的局限性，这些局限性促使了DRL技术在RRAM中的应用。然后，我们提供了最广泛使用的DRL算法的全面回顾，包括基于值和策略的算法。提供了每种算法的优点、局限性和使用案例。接着，我们进行了全面而深入的文献回顾，并根据它们所涉及的无线资源类型和研究的无线网络类型对现有相关工作进行了分类。为此，我们仔细识别了每项相关工作中使用的DRL算法类型、这些算法的元素及每项相关工作的主要发现。最后，我们突出显示了重要的开放挑战，并提供了关于DRL基础的RRAM未来研究方向的见解。本调查旨在指导和激发更多研究工作，致力于为未来无线网络构建高效和精细的DRL基础的RRAM方案。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Index Terms:'
- en: Radio Resource Allocation and Management, Deep Reinforcement Learning, Next
    Generation Wireless Networks, HetNets, Power, Bandwidth, Rate, Access Control.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无线资源分配与管理，深度强化学习，下一代无线网络，异质网络，功率，带宽，速率，接入控制。
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Radio resource allocation and management (RRAM) is regarded as one of the essential
    challenges encountered in modern wireless communication networks [[1](#bib.bib1)].
    Nowadays, modern wireless networks are becoming more heterogeneous and complex
    in terms of the types of emerging radio access networks (RANs) they integrate,
    the explosive number and types of smart devices they serve, and the types of disruptive
    applications and services they support [[2](#bib.bib2), [3](#bib.bib3)]. It is
    envisaged that future networks will integrate land, air, space, and deep-sea wireless
    networks into a single network to meet the stringent requirements of a fully-connected
    world vision [[4](#bib.bib4), [5](#bib.bib5)], as shown in Fig. [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey").
    This will ensure ubiquitous connectivity for user devices with enhanced quality
    of service (QoS) in terms of coverage, reliability, and throughput. In addition,
    future user devices will also witness an unprecedented increase in their numbers
    and types of data-hungry applications they require/support [[3](#bib.bib3), [6](#bib.bib6)].
    It is expected that by 2023, the number of user networked devices and connections,
    including smart-phones, tablets, wearable devices, and sensors, will reach 29.3
    billion [[6](#bib.bib6)], and generate a data rate exceeding 50 trillion GB [[1](#bib.bib1)].
    All these trends will exacerbate the burdens during system design, planning, deployment,
    operation, and management. In particular, RRAM will become crucial in such complex
    and large-scale networks in order to guarantee an enhanced communications experience.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '无线资源分配和管理（RRAM）被认为是现代无线通信网络中遇到的关键挑战之一[[1](#bib.bib1)]。如今，现代无线网络在集成的新兴无线接入网络（RANs）类型、服务的智能设备数量和类型、以及支持的颠覆性应用和服务类型方面变得越来越异质和复杂[[2](#bib.bib2),
    [3](#bib.bib3)]。预计未来的网络将整合陆地、空中、空间和深海无线网络，形成一个单一的网络，以满足全连接世界愿景的严格要求[[4](#bib.bib4),
    [5](#bib.bib5)]，如图[1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")所示。这将确保用户设备在覆盖范围、可靠性和吞吐量方面的服务质量（QoS）得到增强。此外，未来用户设备也将面临前所未有的数据需求应用数量和类型的增加[[3](#bib.bib3),
    [6](#bib.bib6)]。预计到2023年，用户网络设备和连接的数量，包括智能手机、平板电脑、可穿戴设备和传感器，将达到293亿[[6](#bib.bib6)]，并生成超过50万亿GB的数据率[[1](#bib.bib1)]。所有这些趋势将加重系统设计、规划、部署、操作和管理中的负担。特别是在如此复杂和大规模的网络中，RRAM将变得至关重要，以保证更好的通信体验。'
- en: '![Refer to caption](img/471a098b86857f924662197e1618b5b1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/471a098b86857f924662197e1618b5b1.png)'
- en: 'Figure 1: A pictorial illustration of next generation wireless networks characterized
    by their massive heterogeneity in terms of RANs infrastructures, types and numbers
    of user devices served, and types of applications and services supported.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：展示了下一代无线网络的示意图，这些网络在RAN基础设施、服务的用户设备类型和数量、以及支持的应用和服务类型方面具有巨大的异质性。
- en: RRAM plays a pivotal role during infrastructure planning, implementation, and
    resource optimization of modern wireless networks. Efficient RRAM solutions will
    guarantee enhanced network connectivity, increased system efficiency, and reduced
    energy consumption. The performance of wireless networks heavily relies on two
    aspects. First, how network radio resources are being utilized, managed, and orchestrated,
    including transmit power control, spectrum channel allocations, and user access
    control. Second, how efficiently the system can react to the rapid changes of
    network dynamics, including wireless channel statistics, users mobility patterns,
    instantaneous radio resources availability, and variability in traffic loads.
    Efficient RRAM techniques must efficiently and dynamically account for such design
    aspects in order to ensure high network QoS and enhanced users’ Quality of Experience
    (QoE).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RRAM在现代无线网络的基础规划、实施和资源优化过程中发挥着关键作用。高效的RRAM解决方案将保证增强网络连接性，提高系统效率，并降低能源消耗。无线网络的性能在很大程度上取决于两个方面。首先，网络无线资源的利用、管理和编排，包括发射功率控制，频谱信道分配和用户接入控制。其次，系统如何有效地响应网络动态的快速变化，包括无线信道统计，用户移动模式，瞬时无线资源的可用性和流量负荷的可变性。高效的RRAM技术必须有效动态地考虑这些设计方面，以确保高网络服务质量（QoS）和增强用户体验质量（QoE）。
- en: I-A Motivations of the Paper
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 论文的动机
- en: The main motivations of this work stem from three aspects. First, the paramount
    importance of allocating radio resources in future wireless networks. Second,
    the limitations and shortcomings of existing state-of-the-art RRAM techniques.
    Third, the robustness of Deep reinforcement techniques in alleviating these limitations
    and providing efficient performance in the context of RRAM. Here we elaborate
    more on each aspect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要动机源于三个方面。首先，未来无线网络中分配无线资源的至关重要性。其次，现有最先进的RRAM技术的局限性和不足之处。第三，深度强化学习技术在缓解这些限制并在RRAM环境下提供高效性能方面的稳健性。在这里，我们更详细地阐述每个方面。
- en: I-A1 Importance of RRAM in Modern Wireless Networks
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-A1 RRAM在现代无线网络中的重要性
- en: The explosive growth in the number and types of modern smart devices, such as
    smartphones/tablets and wearable devices, has led to the emergence of disruptive
    wireless communications and networking technologies, such as 5G NR cellular networks,
    IoT networks, personal (or wireless body area networks), device-to-device (D2D)
    communications, holographic imaging and haptic communications, and vehicular networks
    [[3](#bib.bib3), [7](#bib.bib7), [8](#bib.bib8), [4](#bib.bib4), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]. Such networks are envisaged to meet the stringent
    requirements of the emerging applications and services via supporting high data
    rates, coverage, and connectivity with significant enhancements in reliability,
    reduction in latency, and mitigation of energy consumption.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代智能设备数量和类型的爆炸式增长，如智能手机/平板电脑和可穿戴设备等，导致了颠覆性的无线通信和网络技术的出现，例如5G NR蜂窝网络，物联网网络，个人（或无线体域网络），设备对设备（D2D）通信，全息成像和触觉通信，以及车载网络[[3](#bib.bib3),
    [7](#bib.bib7), [8](#bib.bib8), [4](#bib.bib4), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)]。预计这类网络将通过支持高数据速率，覆盖范围和连接性，并显着提高可靠性，降低延迟，减少能源消耗来满足新兴应用和服务的严格要求。
- en: However, achieving this goal in such large-scale, versatile, and complex wireless
    networks is quite challenging, as it requires a judicious allocation and management
    of the networks’ limited radio resources [[12](#bib.bib12), [13](#bib.bib13)].
    In particular, efficient and more advanced RRAM solutions must be developed to
    balance the tradeoff between enhancing network performance while guaranteeing
    an efficient utilization of radio resources. Furthermore, efficient RRAM solutions
    must also strike and intelligent tradeoff between optimizing network radio resources
    and satisfying users’ QoE. For example, RRAM techniques must jointly enhance network
    spectral efficiency (SE), energy efficiency (EE), and throughput while mitigating
    interference, reducing latency, and enhancing rate for user devices.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在如此大规模、多功能和复杂的无线网络中实现这一目标相当具有挑战性，因为这需要对网络有限的无线资源进行合理的分配和管理[[12](#bib.bib12),
    [13](#bib.bib13)]。特别是，必须开发更高效和更先进的RRAM解决方案，以平衡提升网络性能与保证无线资源有效利用之间的权衡。此外，高效的RRAM解决方案还必须在优化网络无线资源和满足用户QoE之间进行智能的权衡。例如，RRAM技术必须共同提升网络的频谱效率（SE）、能效（EE）和吞吐量，同时减轻干扰、减少延迟并提高用户设备的速率。
- en: Efficient and advanced RRAM schemes can considerably enhance the system’s SE
    compared to the traditional techniques by relying on the advanced channel and/or
    source coding methods. RRAM is essential in broadcast wireless networks covering
    wide geographical areas as well as in modern cellular communication networks comprised
    of several adjacent and dense access points (APs) that typically share and reuse
    the same radio frequencies.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统技术相比，高效且先进的RRAM方案可以通过依赖先进的信道和/或源编码方法显著提升系统的SE。RRAM在覆盖广泛地理区域的广播无线网络以及由多个相邻和密集的接入点（APs）组成的现代蜂窝通信网络中至关重要，这些接入点通常共享和重用相同的无线频率。
- en: From a cost point of view, the deployment of wireless APs and sites, e.g., base
    stations (BSs), including the real estate costs, planning, maintenance, and energy,
    is the most critical aspect alongside with the frequency license fees. Hence,
    the goal of RRAM is maximizing the network’s SE in terms of bits/sec/Hz/area unit
    or Erlang/MHz/site, under some constraints related to user fairness. For instance,
    the service grade must meet a minimum acceptable level of QoS, including the coverage
    of certain geographical areas while mitigating network outages caused by interference,
    noise, large-scale fading (due to path losses and shadowing), and small-scale
    fading (due to multi-path). The service grade also depends on blocking caused
    by admission control, scheduling errors, or inability to meet certain QoS demands
    of edge devices (EDs).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从成本角度来看，部署无线AP和站点，例如基站（BS），包括房地产成本、规划、维护和能源，是与频率许可费用一起最关键的方面。因此，RRAM的目标是在用户公平相关的一些约束条件下，最大化网络的SE，以比特/秒/Hz/面积单位或Erlang/MHz/站点为单位。例如，服务等级必须满足最低可接受的QoS水平，包括覆盖某些地理区域，同时减少由于干扰、噪声、大规模衰落（由于路径损失和阴影效应）和小规模衰落（由于多径效应）引起的网络中断。服务等级还取决于由于接入控制、调度错误或无法满足边缘设备（EDs）某些QoS需求而导致的阻塞。
- en: I-A2 Where Do Traditional RRAM Techniques Fail?
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-A2 传统RRAM技术在哪里失败？
- en: Future wireless communication networks are complex due to their large-scale,
    versatile, and heterogeneous nature. To optimally allocate and manage radio resources
    in such networks, we typically formulate RRAM as complex optimization problems.
    The objective of such problems is to achieve a particular goal, such as maximizing
    network sum-rate, SE, and EE, given the available radio resources and QoS requirements
    of user devices. Unfortunately, the massive heterogeneity nature of modern networks
    poses tremendous challenges during the process of formulating optimization problems
    as well as applying conventional techniques to solve them, such as optimization,
    heuristic, and game theory algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的无线通信网络由于其大规模、多功能和异质性质而变得复杂。为了在这种网络中最佳分配和管理无线资源，我们通常将RRAM形式化为复杂的优化问题。这些问题的目标是在给定可用无线资源和用户设备的QoS需求的情况下，实现特定目标，例如最大化网络总速率、SE和EE。不幸的是，现代网络的巨大异质性在制定优化问题以及应用传统技术（如优化、启发式和博弈论算法）解决这些问题的过程中带来了巨大的挑战。
- en: The large-scale nature of next generation networks makes it quite difficult
    to formulate RRAM optimization problems that are often intractable non-convex.
    Also, conventional techniques used to solve the RRAM problems require complete
    or quasi-complete knowledge of the wireless environment, including accurate channel
    models and real-time CSI. However, obtaining such information in a real-time fashion
    in these large-scale networks is quite difficult or even impossible. Furthermore,
    conventional techniques are often computationally-expensive and incur considerable
    timing overhead. This renders them inefficient for most emerging time-sensitive
    applications, such as autonomous vehicles and robotics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代网络的大规模特性使得制定RRAM优化问题变得非常困难，这些问题通常是不可处理的非凸问题。此外，解决RRAM问题的传统技术需要对无线环境有完全或准完全的了解，包括准确的信道模型和实时CSI。然而，在这些大规模网络中实时获取这些信息是非常困难甚至不可能的。此外，传统技术通常计算开销大，耗时
    considerable。这使得它们对于大多数新兴的时间敏感应用（如自动驾驶车辆和机器人）来说效率低下。
- en: Moreover, game theory-based techniques are unsuitable for future heterogeneous
    networks (HetNets) as such techniques are devised for homogeneous players. Also,
    the explosive number of network APs and user devices will create extra burdens
    on game theory-based techniques. In particular, network players, such as BSs,
    APs, and user devices, need to exchange a tremendous amount of data and signaling.
    This will induce unmanageable overhead that largely increases delay, computation,
    and energy/memory consumption of network elements.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于博弈论的技术不适用于未来的异构网络（HetNets），因为这些技术是为同质玩家设计的。网络APs和用户设备数量的爆炸性增长将对基于博弈论的技术造成额外的负担。特别是，网络玩家（如BSs、APs和用户设备）需要交换大量的数据和信令。这将引起难以管理的开销，极大地增加了网络元素的延迟、计算和能量/内存消耗。
- en: I-A3 How Can DRL Overcome these Challenges and Provide Efficient RRAM Solutions?
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-A3 DRL如何克服这些挑战并提供高效的RRAM解决方案？
- en: Emerging artificial intelligence (AI) techniques, such as deep reinforcement
    learning (DRL), have shown efficient performance in addressing various issues
    in modern wireless communication networks, including solving complex RRAM optimization
    problems [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)].
    In the context of RRAM, DRL methods are mainly used as an alternative to overcome
    the shortcomings and limitations of the conventional RRAM techniques discussed
    above. In particular, DRL techniques can solve complex network RRAM optimization
    problems and take judicious control decisions with only limited information about
    the network statistics. They achieve this by enabling network entities, such as
    BSs, RAN’s APs, edge servers (ESs), gateways nodes, and user devices, to make
    intelligent and autonomous control decisions, such as RRAM, user association,
    and RAN’s selection, in order to achieve various network goals such as sum-rate
    maximization, reliability enhancement, delay reduction, and SE/EE maximization.
    In addition, DRL techniques are model-free that enable different network entities
    to learn optimal policies about the network, such as RRAM and user association,
    based on their continuous interactions with the wireless environment, without
    knowing the exact channel models or other network statistics a-priori. These appealing
    features make DRL methods one of the main key enabling technologies to address
    the RRAM issue in modern wireless communication networks [[3](#bib.bib3), [2](#bib.bib2)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的人工智能（AI）技术，如深度强化学习（DRL），在解决现代无线通信网络中的各种问题方面表现出高效的性能，包括解决复杂的RRAM优化问题[[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]。在RRAM的背景下，DRL方法主要被用作克服上述传统RRAM技术的缺陷和局限性的替代方案。特别是，DRL技术能够解决复杂的网络RRAM优化问题，并在只有有限的网络统计信息的情况下做出明智的控制决策。它们通过使网络实体（如基站（BSs）、无线接入网（RAN）的接入点（APs）、边缘服务器（ESs）、网关节点和用户设备）能够做出智能和自主的控制决策（如RRAM、用户关联和RAN选择），从而实现各种网络目标，如总速率最大化、可靠性增强、延迟减少和频谱效率/能源效率最大化。此外，DRL技术是无模型的，允许不同的网络实体根据与无线环境的持续互动学习关于网络的最佳策略，如RRAM和用户关联，而无需事先了解确切的信道模型或其他网络统计数据。这些吸引人的特点使得DRL方法成为解决现代无线通信网络中的RRAM问题的主要关键技术之一[[3](#bib.bib3),
    [2](#bib.bib2)]。
- en: I-B Related Work
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 相关工作
- en: There is a limited number of surveys that focus on the role of DRL in RRAM.
    Existing related surveys are listed in Table 1\. The table also summarizes the
    topics covered in these surveys along with a mapping to the relevant sections
    of this paper and a categorical discussion of the improvements and value-added
    in this paper relative to these surveys. In general, as reported in Table 1, these
    published surveys still have several research gaps that are addressed in this
    survey. We summarize them as follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前专注于 DRL 在 RRAM 中作用的调查数量有限。现有相关调查列在表 1 中。该表还总结了这些调查所涵盖的主题，并与本文的相关部分进行映射，同时对相对于这些调查的改进和增值进行了分类讨论。总体而言，如表
    1 所示，这些已发布的调查仍存在几个研究空白，而本调查旨在填补这些空白。我们将其总结如下。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some of the existing surveys focus on DRL applications in wireless communications
    and networking in general, without paying much attention to RRAM [[15](#bib.bib15),
    [18](#bib.bib18)]. For example, existing surveys cover topics related to DRL enabling
    technologies, use-cases, architectures, security, scheduling, clustering and data
    aggregation, traffic management, etc.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些现有调查集中于无线通信和网络中的 DRL 应用，但未过多关注 RRAM [[15](#bib.bib15), [18](#bib.bib18)]。例如，现有调查涵盖了与
    DRL 使能技术、使用案例、架构、安全性、调度、聚类和数据聚合、流量管理等相关的主题。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Some of the published surveys focus on RRAM for wireless networks using ML and/or
    DL techniques without paying much attention to DRL techniques [[1](#bib.bib1),
    [23](#bib.bib23), [12](#bib.bib12), [24](#bib.bib24)]. For example, they consider
    ML techniques such as convolutional neural networks (CNN), recurrent neural networks
    (RNN), supervised learning, Bayesian learning, K-means clustering, Principal Component
    Analysis (PCA), etc.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些已发布的调查集中于使用 ML 和/或 DL 技术的无线网络中的 RRAM，而未过多关注 DRL 技术 [[1](#bib.bib1), [23](#bib.bib23),
    [12](#bib.bib12), [24](#bib.bib24)]。例如，他们考虑了诸如卷积神经网络（CNN）、递归神经网络（RNN）、监督学习、贝叶斯学习、K-means
    聚类、主成分分析（PCA）等 ML 技术。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Even the surveys that address DRL for RRAM in wireless networks focus on specific
    wireless network types or applications [[17](#bib.bib17), [19](#bib.bib19), [25](#bib.bib25),
    [16](#bib.bib16), [20](#bib.bib20)], missing some of the recent research, not
    providing an adequate overview of the most widely used DRL algorithms for RRAM
    [[20](#bib.bib20)], or not covering the RRAM in-depth, but, rather, just covering
    a limited number of radio resources.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使是针对无线网络中 RRAM 的 DRL 的调查，也集中在特定的无线网络类型或应用上 [[17](#bib.bib17), [19](#bib.bib19),
    [25](#bib.bib25), [16](#bib.bib16), [20](#bib.bib20)]，遗漏了一些近期研究，没有提供对最广泛使用的 DRL
    算法在 RRAM 中的充分概述 [[20](#bib.bib20)]，或未深入探讨 RRAM，而只是覆盖了有限数量的无线资源。
- en: Hence, the role of this paper to fill these research gaps and overcome these
    shortcomings. In particular, we provide a comprehensive survey on the application
    of DRL techniques in RRAM for next generation wireless communication networks.
    We have carefully cited up-to-date surveys and related research works. We should
    emphasize here that the scope of this paper is focused only on radio (or communication)
    resources, and no computation resources are included during the study and analysis.
    Fig. 2 shows the radio resources or issues addressed in this survey. However,
    computation resource aspects such as offloading, storage, task scheduling, caching,
    etc., can be found in other studies such as [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)] and the references therein.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文的作用是填补这些研究空白并克服这些不足。特别地，我们提供了一个关于 DRL 技术在下一代无线通信网络中应用的全面调查。我们仔细引用了最新的调查和相关研究工作。我们需要强调的是，本文的范围仅集中在无线（或通信）资源上，研究和分析中不包括计算资源。图
    2 显示了本文所涉及的无线资源或问题。然而，计算资源方面如卸载、存储、任务调度、缓存等，可以在其他研究中找到，例如 [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)] 及其引用文献。
- en: 'TABLE I: Relationship Between this Survey and Existing Surveys on DRL-Based
    RRAM For Wireless Networks'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：本调查与现有基于 DRL 的 RRAM 无线网络调查之间的关系
- en: '| Paper | Summary of the survey’s contributions |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 调查贡献总结 |'
- en: '&#124; Related contents &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关内容 &#124;'
- en: '&#124; in this paper &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文中的内容 &#124;'
- en: '| Value added in this paper |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 本文增值部分 |'
- en: '| Luong et al.[[15](#bib.bib15)] | Applications of DRL in communications and
    networking | Section [III](#S3 "III Overview of DRL Techniques Used for RRAM ‣
    Deep Reinforcement Learning for Radio Resource Allocation and Management in Next
    Generation Heterogeneous Wireless Networks: A Survey")/[IV](#S4 "IV DRL-Based
    Radio Resource Allocation and Management for Next Generation Wireless Networks
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") | Particularly focus
    on DRL usage for RRAM and enhanced list of papers |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Luong 等人[[15](#bib.bib15)] | DRL 在通信和网络中的应用 | 第 [III](#S3 "III Overview of
    DRL Techniques Used for RRAM ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey")/[IV](#S4 "IV DRL-Based Radio Resource Allocation and Management for
    Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") 节 | 特别关注 DRL 在 RRAM 中的使用以及增强的文献列表 |'
- en: '| Hussain et al.[[1](#bib.bib1)] | ML- and DL-based resource management mechanisms
    in cellular wireless and IoT networks | Sections [II](#S2 "II Radio Resource Allocation
    and Management Techniques ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey")/[IV](#S4
    "IV DRL-Based Radio Resource Allocation and Management for Next Generation Wireless
    Networks ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") | In-depth and
    holistic coverage of DRL algorithms used for RRAM, intensive review of existing
    papers related to DRL for RRAM, and the coverage of more types of wireless networks
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Hussain 等人[[1](#bib.bib1)] | 基于 ML 和 DL 的蜂窝无线和 IoT 网络资源管理机制 | 第 [II](#S2
    "II Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")/[IV](#S4 "IV DRL-Based Radio Resource Allocation
    and Management for Next Generation Wireless Networks ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") 节 | 对用于 RRAM 的 DRL 算法的深入和全面覆盖，对 DRL 在 RRAM 中的现有论文进行深入回顾，并覆盖更多类型的无线网络
    |'
- en: '| Lin et al.[[31](#bib.bib31)] | Applications of AI approaches in resource
    management, such as spectrum, computing, and caching. | Section [V](#S5 "V Open
    Challenges and Future Research Directions ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") | Particularly focus on DRL methods, including more radio resources,
    and intensive literature review |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等人[[31](#bib.bib31)] | AI 方法在资源管理中的应用，如频谱、计算和缓存 | 第 [V](#S5 "V Open Challenges
    and Future Research Directions ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") 节 | 特别关注 DRL 方法，包括更多的无线电资源，以及深入的文献综述 |'
- en: '| Liang et al.[[12](#bib.bib12)] | DL-Based resource allocation with application
    to vehicular networks | Sections [II](#S2 "II Radio Resource Allocation and Management
    Techniques ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey")/[IV](#S4 "IV DRL-Based
    Radio Resource Allocation and Management for Next Generation Wireless Networks
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") | Focus on DRL techniques
    for RRAM, in-depth literature review, and include various types of modern wireless
    networks |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Liang 等人[[12](#bib.bib12)] | 基于 DL 的资源分配及其在车载网络中的应用 | 第 [II](#S2 "II Radio
    Resource Allocation and Management Techniques ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey")/[IV](#S4 "IV DRL-Based Radio Resource Allocation and Management
    for Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") 节 | 关注 DRL 技术用于 RRAM 的情况，深入的文献综述，并包括各种类型的现代无线网络 |'
- en: '| Obite et al.[[17](#bib.bib17)] | Spectrum sensing in cognitive radio networks
    | Section [IV](#S4 "IV DRL-Based Radio Resource Allocation and Management for
    Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") | Detailed investigation of more radio resources, holistic coverage
    of more wireless networks, and an intensive up-to-date review of existing papers
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Obite 等人[[17](#bib.bib17)] | 认知无线电网络中的频谱感知 | 第 [IV](#S4 "IV DRL-Based Radio
    Resource Allocation and Management for Next Generation Wireless Networks ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey") 节 | 对更多无线电资源的详细调查，对更多无线网络的全面覆盖，以及对现有论文的最新深入回顾
    |'
- en: '| Chen et al.[[23](#bib.bib23)] | Applications of ML algorithms in solving
    wireless networking problems | NA | Focus on applications of DRL in solving RRAM
    wireless problems, and coverage of more wireless networks |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 [[23](#bib.bib23)] | ML 算法在解决无线网络问题中的应用 | 无 | 关注 DRL 在解决 RRAM 无线问题中的应用，以及覆盖更多无线网络
    |'
- en: '| Gupta et al.[[18](#bib.bib18)] | General research and simulation tools used
    for DRL | Section [III](#S3 "III Overview of DRL Techniques Used for RRAM ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey") | Specifically focus on DRL algorithms
    along with the related research conducted specifically in the context of RRAM
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Gupta 等人 [[18](#bib.bib18)] | 用于 DRL 的一般研究和仿真工具 | 第 [III](#S3 "III 用于 RRAM
    的 DRL 技术概述 ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述") 节 | 专门关注 DRL 算法以及在 RRAM 语境下进行的相关研究
    |'
- en: '| Du et al.[[19](#bib.bib19)] | Investigates how to achieve green DRL for radio
    resource management via energy allocation based on architecture and algorithm
    innovations | Section [IV](#S4 "IV DRL-Based Radio Resource Allocation and Management
    for Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") | Further extend to more radio resources and more modern wireless networks
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Du 等人 [[19](#bib.bib19)] | 探讨如何通过基于架构和算法创新的能量分配实现绿色 DRL 进行无线资源管理 | 第 [IV](#S4
    "IV 基于 DRL 的无线资源分配和管理用于下一代无线网络 ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述") 节 | 进一步扩展到更多无线资源和更现代的无线网络
    |'
- en: '| Pham et al.[[32](#bib.bib32)] | A layered-based classification of resource
    management techniques in Wireless Access Networks | Section [II](#S2 "II Radio
    Resource Allocation and Management Techniques ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey") | A holistic study of conventional and emerging ML-based
    techniques for RRAM applied to modern wireless networks and including more radio
    resources |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Pham 等人 [[32](#bib.bib32)] | 无线接入网络中资源管理技术的分层分类 | 第 [II](#S2 "II 无线资源分配和管理技术
    ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述") 节 | 全面研究传统和新兴的基于 ML 的 RRAM 技术，应用于现代无线网络，并包含更多无线资源
    |'
- en: '| Almazrouei et al. [[33](#bib.bib33)] | Potential benefits and the challenges
    when using ML for in radio spectrum | Section [II](#S2 "II Radio Resource Allocation
    and Management Techniques ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey")
    | Extend the analysis to incorporate more radio resources, focus on DRL methods
    for RRAM, and provide in-depth literature review |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Almazrouei 等人 [[33](#bib.bib33)] | 使用 ML 在无线电频谱中可能的好处和挑战 | 第 [II](#S2 "II
    无线资源分配和管理技术 ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述") 节 | 扩展分析以纳入更多无线资源，关注 DRL 方法用于
    RRAM，并提供深入的文献综述 |'
- en: '| Dhilipkumar et al.[[34](#bib.bib34)] | Discusses various resource allocation
    scheme for D2D networks in cellular network | Sections [II](#S2 "II Radio Resource
    Allocation and Management Techniques ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey")/[IV](#S4 "IV DRL-Based Radio Resource Allocation and Management for
    Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") | Focus on DRL-based RRAM for various applications in services in wireless
    networks |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Dhilipkumar 等人 [[34](#bib.bib34)] | 讨论 D2D 网络在蜂窝网络中的各种资源分配方案 | 第 [II](#S2
    "II 无线资源分配和管理技术 ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述")/[IV](#S4 "IV 基于 DRL 的无线资源分配和管理用于下一代无线网络
    ‣ 下一代异构无线网络中的深度强化学习用于无线资源分配和管理：综述") 节 | 关注 DRL 基于 RRAM 的无线网络服务中各种应用的资源分配方案 |'
- en: '| Arulkumaran et al.[[35](#bib.bib35)], Zhang et al.[[36](#bib.bib36)] | Overview
    of DRL approaches in general including, applications and models | Section [III](#S3
    "III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey") | Focus on DRL approaches utilized in RRAM for wireless networks,
    and also provide detailed literature review |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Arulkumaran 等人[[35](#bib.bib35)], Zhang 等人[[36](#bib.bib36)] | DRL 方法的总体概述，包括应用和模型
    | 第 [III](#S3 "III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") 节 | 关注于 RRAM 在无线网络中使用的 DRL 方法，并提供详细的文献综述 |'
- en: '| Zappone et al.[[24](#bib.bib24)] | Motivations, applications, visions, and
    case studies for the usage of DL techniques in wireless communication networks
    | NA | Particularly focusing on DRL techniques for wireless communication networks
    in the context of RRAM |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Zappone 等人[[24](#bib.bib24)] | 关于在无线通信网络中使用 DL 技术的动机、应用、愿景和案例研究 | NA | 特别关注于无线通信网络中
    RRAM 背景下的 DRL 技术 |'
- en: '| Lee et al.[[16](#bib.bib16)] | DRL-based resource management schemes for
    5G HetNets in energy harvesting, network slicing, cognitive HetNets, coordinated
    multi-point transmission, and big data | Section [III](#S3 "III Overview of DRL
    Techniques Used for RRAM ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey")
    | In-depth analysis of DRL methods used for RRAM including; DRL algorithms, types
    of wireless networks, types of radio resources investigated, and extensive literature
    review |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Lee 等人[[16](#bib.bib16)] | 基于 DRL 的资源管理方案，涉及 5G HetNets 的能量采集、网络切片、认知 HetNets、协调多点传输和大数据
    | 第 [III](#S3 "III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") 节 | 深入分析了用于 RRAM 的 DRL 方法，包括 DRL 算法、无线网络类型、研究的无线电资源类型以及广泛的文献综述
    |'
- en: '| Qian et al.[[20](#bib.bib20)] | Applications of RL and DRL in three technologies:
    mobile edge computing, software defined network, and network virtualization in
    5G | NA | Focus on DRL for RRAM for cellular and other emerging wireless networks
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Qian 等人[[20](#bib.bib20)] | RL 和 DRL 在三项技术中的应用：移动边缘计算、软件定义网络和 5G 网络虚拟化 |
    NA | 关注于细胞和其他新兴无线网络中的 RRAM 的 DRL |'
- en: '| Khorasgani et al.[[25](#bib.bib25)] | Key limitations and challenges in using
    DRL to address the problem of dynamic dispatching in the mining industry | Section
    [IV](#S4 "IV DRL-Based Radio Resource Allocation and Management for Next Generation
    Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey")
    | Extend the investigation to include various wireless networks with an extensive
    focus on radio resources |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Khorasgani 等人[[25](#bib.bib25)] | 使用 DRL 解决矿业动态调度问题的主要限制和挑战 | 第 [IV](#S4
    "IV DRL-Based Radio Resource Allocation and Management for Next Generation Wireless
    Networks ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") 节 | 扩展研究，涵盖各种无线网络，并重点关注无线电资源
    |'
- en: I-C Paper Contributions
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-C 论文贡献
- en: The main contributions of this paper are summarized as follows.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下。
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We provide a detailed discussion on the state-of-the-art techniques used for
    RRAM in wireless networks, including their types, shortcomings, and limitations
    that led to the adoption of DRL solutions.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们详细讨论了在无线网络中使用的最先进的 RRAM 技术，包括它们的类型、缺点和局限性，这些因素促使了 DRL 解决方案的采用。
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We identify the most widely used DRL techniques utilized in RRAM of wireless
    networks and provide a comprehensive overview of them. The advantages, features,
    and limitations of each technique are discussed. Hence, the reader is provided
    with an in-depth knowledge of which DRL techniques should be leveraged for each
    RRAM problem under investigation.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别出在无线网络 RRAM 中最广泛使用的 DRL 技术，并对这些技术进行了全面概述。讨论了每种技术的优点、特性和局限性。因此，读者可以深入了解应针对每个
    RRAM 问题采用哪些 DRL 技术。
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We conduct an extensive and up-to-date literature review and classify the papers
    as reported in the literature based on the type of radio resources they address
    (as shown in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey")) and the types of
    wireless networks, applications, and services they consider (as shown in Fig.
    [3](#S1.F3 "Figure 3 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")). Specifically, for each paper reviewed, we identify
    the problem it addresses, type of wireless network it investigates, type of DRL
    model(s) it implements, main elements of the DRL models (i.e., agent, state space,
    action space, and reward function), and its main findings. This provides the reader
    with in-depth technical knowledge of how to efficiently engineer DRL models for
    RRAM problems in wireless communications.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们进行了一次广泛而最新的文献综述，并根据文献中报告的无线资源类型（如图 [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions
    ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation and
    Management in Next Generation Heterogeneous Wireless Networks: A Survey") 所示）和它们所考虑的无线网络类型、应用和服务（如图
    [3](#S1.F3 "Figure 3 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") 所示）对论文进行了分类。具体来说，对于每篇被审阅的论文，我们确定其所解决的问题、研究的无线网络类型、实现的DRL模型类型、DRL模型的主要元素（即代理、状态空间、动作空间和奖励函数）以及主要发现。这为读者提供了有关如何高效工程化DRL模型以解决无线通信中RRAM问题的深入技术知识。'
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Based on the papers reviewed in this survey, we outline and identify some of
    the existing challenges and provide deep insights into some promising future research
    directions in the context of using DRL for RRAM in wireless networks.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于本次综述中审阅的论文，我们概述并识别了一些现有挑战，并对使用DRL解决无线网络中RRAM问题的一些有前景的未来研究方向提供了深入的见解。
- en: 'Fig. [4](#S1.F4 "Figure 4 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey") shows the percentage of the related
    works, classified based on the types of radio resources discussed in each pape,
    Fig. [4](#S1.F4 "Figure 4 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") (a), and based on the types of wireless networks
    studied in each paper, Fig. [4](#S1.F4 "Figure 4 ‣ I-C Paper Contributions ‣ I
    Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") (b). This survey
    is designed by carefully following the review protocol illustrated in Fig. [5](#S1.F5
    "Figure 5 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"). Since this survey mainly focuses on deep reinforcement
    learning for RRAM in wireless networks, we included the following terms during
    the search stage along with ”AND/OR” combinations of them; ”deep reinforcement
    learning,” ”DRL,” ”resource allocation,” ”resource management,” ”power,” ”spectrum,”
    ”bandwidth,” ”access control,” ”user association,” ”network selection,” ”cell
    selection,” ”rate control,” ”joint resources,” ”wireless networks,” ”satellite
    networks,” ”cellular networks,” and ”Heterogeneous networks.” The number of papers
    found and the databases searched are detailed in Fig. [5](#S1.F5 "Figure 5 ‣ I-C
    Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey"). The inclusion criteria are papers that address the use of DRL techniques
    to manage and allocate the radio resources shown in Fig. [2](#S1.F2 "Figure 2
    ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") for the wireless networks shown in Fig. [3](#S1.F3 "Figure 3 ‣ I-C
    Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey"). The exclusion criteria are papers that: 1) address computation resources,
    e.g., task offloading, storage, scheduling, etc., 2) use conventional RRAM approaches,
    i.e., not using DRL techniques, 3) use ML/DL techniques, or 4) address non-wireless
    networks, e.g., wired networks, optical networks, etc. In Fig. [5](#S1.F5 "Figure
    5 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey"), the number of papers excluded after a detailed check of
    the body is 43, which are directly related to our survey but do not clearly identify
    the types of DRL algorithms used, elements of DRL models (i.e., agents, state
    space, action space, and reward function), type of wireless networks covered,
    and/or not well written.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S1.F4 "图 4 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") 显示了相关工作的百分比，这些工作根据每篇论文中讨论的无线资源类型进行了分类，如图
    [4](#S1.F4 "图 4 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") (a)，以及根据每篇论文中研究的无线网络类型进行了分类，如图
    [4](#S1.F4 "图 4 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") (b)。本综述设计时仔细遵循了图
    [5](#S1.F5 "图 5 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") 中说明的评审协议。由于本综述主要关注无线网络中用于无线资源分配与管理的深度强化学习，因此我们在搜索阶段包含了以下术语及其”AND/OR”组合：”深度强化学习”，”DRL”，”资源分配”，”资源管理”，”功率”，”频谱”，”带宽”，”接入控制”，”用户关联”，”网络选择”，”小区选择”，”速率控制”，”联合资源”，”无线网络”，”卫星网络”，”蜂窝网络”，以及”异构网络”。找到的论文数量和搜索的数据库在图
    [5](#S1.F5 "图 5 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") 中详细说明。纳入标准是那些处理图
    [2](#S1.F2 "图 2 ‣ I-C 论文贡献 ‣ I 引言 ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") 中展示的无线网络的无线资源管理和分配的
    DRL 技术的论文。排除标准包括：1) 处理计算资源的论文，例如任务卸载、存储、调度等，2) 使用传统 RRAM 方法的论文，即未使用 DRL 技术，3)
    使用 ML/DL 技术的论文，或 4) 处理非无线网络的论文，例如有线网络、光网络等。在图 [5](#S1.F5 "图 5 ‣ I-C 论文贡献 ‣ I 引言
    ‣ 下一代异构无线网络中无线资源分配与管理的深度强化学习：综述") 中，经过详细检查后排除的论文数量为 43，这些论文与我们的综述直接相关，但未明确识别使用的
    DRL 算法类型、DRL 模型的元素（即代理、状态空间、动作空间和奖励函数）、涵盖的无线网络类型，和/或写作不佳。
- en: 'In general, the research questions that this survey aims to address are stated
    as follows. How can DRL techniques be implemented to address the RRAM problems
    in modern wireless networks? What are the performance advantages achieved when
    using DRL tools compared to the state-of-the-art RRAM approaches? What are the
    challenges and possible research directions that stem from the reviewed papers
    in the context of using DRL for RRAM in wireless networks? The retrieved papers
    shown in Fig. [5](#S1.F5 "Figure 5 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey"), i.e., the 94 papers,
    are selected carefully to help with answering these questions, as we will elaborate
    in the next sections.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '一般而言，本调查旨在解决的研究问题如下。如何实施DRL技术以解决现代无线网络中的RRAM问题？使用DRL工具相比于最先进的RRAM方法能够获得哪些性能优势？在无线网络中使用DRL进行RRAM的背景下，从所审查的论文中产生了哪些挑战和可能的研究方向？图[5](#S1.F5
    "Figure 5 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")中显示的94篇论文被仔细挑选，以帮助回答这些问题，正如我们将在接下来的部分中详细说明的。'
- en: 'It is observed from Fig. [4](#S1.F4 "Figure 4 ‣ I-C Paper Contributions ‣ I
    Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") (a) that the majority
    of related works are on the Spectrum and Access Control radio resources, followed
    by the Joint radio resources. Also, as shown in Fig. [4](#S1.F4 "Figure 4 ‣ I-C
    Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") (b), the related works on the IoT and Other Emerging Wireless Networks
    have received more attention than the other wireless network types, followed by
    the Cellular and Homogeneous Networks (HomNets).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '从图[4](#S1.F4 "Figure 4 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")（a）中观察到，大多数相关工作集中在频谱和接入控制无线资源上，其次是联合无线资源。此外，正如图[4](#S1.F4
    "Figure 4 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")（b）所示，关于物联网和其他新兴无线网络的相关工作受到了比其他无线网络类型更多的关注，其次是蜂窝网络和同质网络（HomNets）。'
- en: '![Refer to caption](img/a07129051d8e05cc3597276d4c2060ef.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a07129051d8e05cc3597276d4c2060ef.png)'
- en: 'Figure 2: Classification based on radio resources (or issues) addressed in
    the papers.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于论文中处理的无线资源（或问题）的分类。
- en: '![Refer to caption](img/1df614de2422ff273309fae20acc15d0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1df614de2422ff273309fae20acc15d0.png)'
- en: 'Figure 3: Classification based on networks types covered in the papers.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于论文中涵盖的网络类型的分类。
- en: '![Refer to caption](img/d987f710a7752831a9966b97706733e5.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d987f710a7752831a9966b97706733e5.png)'
- en: 'Figure 4: Percentages of related work based on (a) types of radio resources
    covered and (b) types of networks and application investigated. RA: resource allocation.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基于（a）覆盖的无线资源类型和（b）调查的网络类型及应用的相关工作百分比。RA：资源分配。
- en: '![Refer to caption](img/80dcd99c716053aa7bd11f097acb426b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/80dcd99c716053aa7bd11f097acb426b.png)'
- en: 'Figure 5: The review protocol followed in this survey.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：本调查中遵循的审查协议。
- en: 'The rest of this paper is organized as follows. Table [II](#S1.T2 "TABLE II
    ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") lists the acronyms used in this paper and their definitions. Section
    [II](#S2 "II Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") discusses existing RRAM techniques, including conventional
    methods and DRL-based methods. The definitions, types, and limitations of existing
    techniques are discussed. Also, the advantages of employing DRL techniques for
    RRAM are explained. Section [III](#S3 "III Overview of DRL Techniques Used for
    RRAM ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") provides an overview
    of the DRL techniques widely employed for RRAM, including their types and architectures.
    In-depth classifications of the existing research works is provided in Section
    [IV](#S4 "IV DRL-Based Radio Resource Allocation and Management for Next Generation
    Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey").
    Existing papers are classified based on the radio resources and the network types
    they cover. Section [V](#S5 "V Open Challenges and Future Research Directions
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") provides key open
    challenges along with insights for future research directions. Finally, Section
    [VI](#S6 "VI Conclusion ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey")
    concludes the paper. The organization of the paper is pictorially illustrated
    in Fig. [6](#S1.F6 "Figure 6 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。表[II](#S1.T2 "TABLE II ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey")列出了本文中使用的缩写词及其定义。第[II](#S2
    "II Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")节讨论了现有的RRAM技术，包括传统方法和基于DRL的方法。讨论了现有技术的定义、类型和局限性。此外，还解释了使用DRL技术进行RRAM的优点。第[III](#S3
    "III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey")节提供了广泛用于RRAM的DRL技术的概述，包括它们的类型和架构。第[IV](#S4 "IV DRL-Based Radio
    Resource Allocation and Management for Next Generation Wireless Networks ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey")节提供了对现有研究工作的深入分类。现有论文根据无线电资源和所涵盖的网络类型进行了分类。第[V](#S5
    "V Open Challenges and Future Research Directions ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")节提供了关键的开放挑战及未来研究方向的见解。最后，第[VI](#S6 "VI Conclusion
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey")节总结了本文。本文的组织结构在图[6](#S1.F6
    "Figure 6 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")中图示。'
- en: '![Refer to caption](img/0f6509b5e42271d526f8752d22c3c038.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f6509b5e42271d526f8752d22c3c038.png)'
- en: 'Figure 6: Organization of the paper.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：论文的组织结构。
- en: 'TABLE II: List of Acronyms Used and Their Definitions'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：使用的缩写及其定义
- en: '| Acronym | Definition | Acronym | Definition | Acronym | Definition |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 定义 | 缩写 | 定义 | 缩写 | 定义 |'
- en: '| RRA | Radio Resource Allocation | RRAM | Radio Resource Allocation and Management
    | UE | User End |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| RRA | 无线电资源分配 | RRAM | 无线电资源分配与管理 | UE | 用户端 |'
- en: '| VLC | Visible Light Communication | RL | Reinforcement Learning | EE | Energy
    Efficiency |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| VLC | 可见光通信 | RL | 强化学习 | EE | 能效 |'
- en: '| DRL | Deep Reinforcement Learning | DL | Deep Learning | TD | Time Difference
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| DRL | 深度强化学习 | DL | 深度学习 | TD | 时间差 |'
- en: '| ML | Machine Learning | DQN | Deep Q-Network | OU | Ornstein–Uhlenbeck |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ML | 机器学习 | DQN | 深度Q网络 | OU | 奥恩斯坦-乌伦贝克 |'
- en: '| MDP | Markov Decision Process | DDQN | Double Deep Q-Network | D2D | Device-to-Device
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MDP | 马尔可夫决策过程 | DDQN | 双重深度Q网络 | D2D | 设备对设备 |'
- en: '| DNN | Deep Neural Network | DDPG | Deep Deterministic Policy Gradient | BS
    | Base Station |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 | DDPG | 深度确定性策略梯度 | BS | 基站 |'
- en: '| QoS | Quality of Service | RAT | Radio Access Technology | mmWave | Millimeter
    Wave |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| QoS | 服务质量 | RAT | 无线接入技术 | mmWave | 毫米波 |'
- en: '| A2C | advantage actor-critic | MADRL | Multi-Agent Deep Reinforcement Learning
    | RF | Radio Frequency |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| A2C | 优势演员评论家 | MADRL | 多智能体深度强化学习 | RF | 无线电频率 |'
- en: '| NOMA | Non-Orthogonal Multiple Access | FL | Federated Learning | AP | Access
    Point |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| NOMA | 非正交多重接入 | FL | 联邦学习 | AP | 接入点 |'
- en: '| HetNets | Heterogeneous Networks | NTNs | Non-Terrestrial Networks | SE |
    Spectral Efficiency |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| HetNets | 异构网络 | NTNs | 非地面网络 | SE | 频谱效率 |'
- en: '| IAB | Integrated Access and Backhaul | WMMSE | Weighted Minimum Mean Square
    Error | IoT | Internet of Things |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| IAB | 集成接入和回传 | WMMSE | 加权最小均方误差 | IoT | 物联网 |'
- en: '| CSI | Channel State Information | WLAN | Wireless Local Area Network | V2V
    | Vehicle to Vehicle |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| CSI | 通道状态信息 | WLAN | 无线局域网 | V2V | 车对车 |'
- en: '| M2M | Machine-to-Machine | UAV | Unmanned Aerial Vehicles | V2X | Vehicle
    to Everything |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| M2M | 机器对机器 | UAV | 无人机 | V2X | 车对一切'
- en: '| CRN | Cognitive Radio Network | PPO | Proximal Policy Optimization | V2I
    | Vehicle to Infrastructure |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| CRN | 认知无线电网络 | PPO | 近端策略优化 | V2I | 车对基础设施 |'
- en: '| DSA | Dynamic Spectrum Access | CV2X | Cellular Vehicular Communication |
    MeNB | Macro eNodeB |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DSA | 动态频谱接入 | CV2X | 蜂窝车载通信 | MeNB | 大型 eNodeB |'
- en: '| SNR | Signal to Noise Ratio | SINR | Signal to Interference plus Noise Ratio
    | RSU | Road Side Unit |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SNR | 信噪比 | SINR | 信号与干扰加噪声比 | RSU | 道路侧单元 |'
- en: '| SIoT | Satellite Internet of Things | A3C | Asynchronous Actor Critic Algorithm
    | PU | Primary User |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SIoT | 卫星物联网 | A3C | 异步演员评论家算法 | PU | 主用户 |'
- en: '| IIoT | Industrial Internet of Things | AI | Artificial Intelligence | RB
    | Resource Block |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| IIoT | 工业物联网 | AI | 人工智能 | RB | 资源块 |'
- en: '| SU | Secondary User | OFDM | Orthogonal Frequency Division Multiplexing |
    TDD | Time Division Duplex |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SU | 次要用户 | OFDM | 正交频分复用 | TDD | 时分双工 |'
- en: '| LTE | Long-Term Evolution | D3QN | Dueling Double Deep Q-Network | UDN |
    Ultra-Dense Network |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LTE | 长期演进 | D3QN | 对抗双重深度 Q 网络 | UDN | 超密集网络 |'
- en: '| C-RAN | Cloud Radio Access Network | RRH | Remote Radio Head | BBU | Base-Band
    Unit |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| C-RAN | 云无线接入网 | RRH | 远程射频头 | BBU | 基带单元 |'
- en: '| FP | Fractional Programming | ADMM | Alternating Direction Method of Multipliers
    | QoE | Quality of Experience |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| FP | 分数编程 | ADMM | 交替方向乘子法 | QoE | 体验质量 |'
- en: '| OMA | Orthogonal Multiple Access | MCC | Mission-critical communication |
    LEO | Low Earth Satellite |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OMA | 正交多重接入 | MCC | 任务关键通信 | LEO | 低地球轨道卫星 |'
- en: '| ANN | Attention Neural Network | RIS | Reconfigurable Intelligent Surface
    | HSR | High-Speed Railway |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ANN | 注意力神经网络 | RIS | 可重构智能表面 | HSR | 高速铁路 |'
- en: '| RAN | Radio Access Network | VANETs | Vehicular Ad Hoc Networks | PED | Patient
    Edge Device |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| RAN | 无线接入网 | VANETs | 车载自组织网络 | PED | 患者边缘设备 |'
- en: '| RNC | Radio Network Controller | WSN | Wireless Sensor Network | NE | Nash
    Equilibrium |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| RNC | 无线网络控制器 | WSN | 无线传感器网络 | NE | 纳什均衡 |'
- en: '| DPG | Deterministic Policy Gradient | CUAV | Cognitive Unmanned Aerial Vehicle
    | XAI | Explainable AI |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DPG | 确定性策略梯度 | CUAV | 认知无人机 | XAI | 可解释的人工智能 |'
- en: '| GAN | Generative Adversarial Network | KPI | Key Performance Indicator |
    MCA | Multi-Channel Access |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GAN | 生成对抗网络 | KPI | 关键性能指标 | MCA | 多信道接入 |'
- en: II Radio Resource Allocation and Management Techniques
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 无线资源分配与管理技术
- en: In this section, we define the main radio resources of interest and provide
    a summary of the conventional techniques and tools used for RRAM in wireless networks.
    Also, the limitations of these conventional techniques that motivate the use of
    DRL solutions will be highlighted. Then we discuss how DRL techniques can be efficient
    alternatives to these traditional approaches.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了主要的无线资源，并总结了无线网络中用于 RRAM 的传统技术和工具。同时，强调了这些传统技术的局限性，这些局限性促使了 DRL 解决方案的使用。随后，我们讨论了
    DRL 技术如何成为这些传统方法的有效替代方案。
- en: 'II-A Radio Resources: Definitions and Types (or Issues)'
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 无线资源：定义与类型（或问题）
- en: In general, allocation and management of wireless network resources include
    radio (i.e., communication) and computation resources. This paper focuses only
    on the RRAM issue. This involves strategies and algorithms used to control and
    manage wireless network parameters and resources, such as transmit power, spectrum
    allocation, user association/assignment, rate control, access control, etc. The
    main goal of wireless networks, in general, is to utilize and manage these available
    radio resources as efficiently as possible to provide enhanced network QoS, such
    as enhanced data rate, SE, EE, reliability, connectivity, and coverage while meeting
    users’ QoS demands.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，无线网络资源的分配和管理包括无线（即通信）和计算资源。本文仅关注RRAM问题。这涉及到控制和管理无线网络参数和资源的策略和算法，如发射功率、频谱分配、用户关联/分配、速率控制、接入控制等。无线网络的一般目标是尽可能高效地利用和管理这些可用的无线资源，以提供增强的网络QoS，如提升的数据速率、SE、EE、可靠性、连接性和覆盖范围，同时满足用户的QoS需求。
- en: Efficient RRAM schemes can considerably enhance the system’s SE compared to
    the traditional techniques relying on the advanced channel and/or source coding
    methods. RRAM is essential in next generation HetNet as it will cover broad geographical
    areas with ultra-dense network (UDN) deployments. In these UDNs, a massive number
    of adjacent APs typically require sharing and reusing the same communication resources,
    such as radio frequencies and channels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的RRAM方案相比传统依赖于先进信道和/或源编码方法的技术，可以显著提升系统的SE。RRAM在下一代异构网络（HetNet）中至关重要，因为它将覆盖广泛的地理区域，采用超密集网络（UDN）部署。在这些UDN中，大量相邻的AP通常需要共享和重用相同的通信资源，如无线频率和信道。
- en: The most crucial radio resources or issues that play a fundamental role in controlling
    wireless networks’ performance are summarized below.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 控制无线网络性能的最关键的无线资源或问题总结如下。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Power resource: which is one of the most critical issues in the RRAM of modern
    wireless networks. Transmit power allocation in the downlink/uplink from/to network
    APs, such as BSs, edge servers (ESs), and gateways, is essential to guarantee
    a satisfactory QoS for communication links. Power control is essential from two
    perspectives; physical-limitations and communication links perspectives. Practically,
    the maximum power is limited by the capability of APs’ power amplifiers or government
    regulation. Power control is even more critical in battery-driven user devices,
    as high transmission power tends to drain battery storage very quickly. Hence,
    it is a common practice to incorporate the limited power resource as a constraint
    during the design and implementation of wireless networks. On the other hand,
    power control is also needed to guarantee enhanced networks’ QoS and user devices’
    QoE. For example, in large-scale and extra-dense modern wireless networks such
    as the mmWave and THz band systems [[37](#bib.bib37), [3](#bib.bib3), [2](#bib.bib2)],
    signal attenuation due to path and penetration losses must be accounted for during
    the power link budget analysis. Also, the coverage of BSs’ cells and the inter-and
    intra-cell interference issues become crucial, which are mainly defined by the
    transmit power level. Hence, it is essential to develop sophisticated and fine-grained
    power allocation and interference management strategies to address such challenges.
    Moreover, many emerging wireless networks, applications, and services are of heterogeneous/homogeneous
    nature with extremely varying network topology, network traffic, users’ QoS demands,
    and channel characteristics [[3](#bib.bib3)]. Therefore, it becomes quite challenging
    to allocate the transmit power adaptively and dynamically in response to the rapid
    changes of physical channels, network conditions, and users’ QoS demands.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 电源资源：这是现代无线网络中RRAM最关键的问题之一。从网络接入点（APs），如基站（BSs）、边缘服务器（ESs）和网关的下行/上行传输功率分配，对于保证通信链路的满意QoS至关重要。从两个角度来看，功率控制是必不可少的：物理限制和通信链路角度。实际上，最大功率受到APs功率放大器的能力或政府法规的限制。在电池驱动的用户设备中，功率控制更加关键，因为高传输功率往往会迅速耗尽电池存储。因此，在无线网络的设计和实施过程中，将有限的电源资源作为约束条件是一种常见做法。另一方面，功率控制也是保证增强网络的QoS和用户设备的QoE所必需的。例如，在大规模和极密集的现代无线网络中，如毫米波和THz频段系统[[37](#bib.bib37),
    [3](#bib.bib3), [2](#bib.bib2)]，信号衰减由于路径和穿透损耗必须在功率链路预算分析中加以考虑。此外，基站小区的覆盖范围以及小区间和小区内的干扰问题变得至关重要，这些问题主要由传输功率水平决定。因此，必须制定复杂且精细的功率分配和干扰管理策略以应对这些挑战。此外，许多新兴无线网络、应用和服务具有异质/同质的特性，网络拓扑、网络流量、用户QoS需求和信道特性极为多样化[[3](#bib.bib3)]。因此，适应性和动态地分配传输功率以应对物理信道、网络条件和用户QoS需求的快速变化变得相当具有挑战性。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Spectrum resource and access control: this is also another main issue in RRAM
    of modern wireless networks. User devices must be allocated frequency channels
    to start transmitting/receiving data with acceptable SNR. Existing wireless networks,
    such as the sub 6 GHz, suffers from a severe bandwidth shortage which is even
    exacerbated with the explosive increase in the user devices [[6](#bib.bib6)].
    Fortunately, the mmWave and the emerging THz bands can considerably overcome this
    shortcoming by providing an extra 3.25 GHz and 10-100 GHz bandwidth, respectively
    [[38](#bib.bib38)]. It is also expected that modern wireless user devices will
    be equipped with advanced capabilities that enable them to aggregate all these
    three frequency bands, i.e., the sub 6GHz, mmWave, and THz, to support future
    disruptive technologies and services [[39](#bib.bib39)]. However, allocating and
    managing the radio channels across these three frequency bands across multi-RAN
    to a massive number of user devices mandate developing advanced signal processing
    techniques. Unfortunately, such techniques require perfect knowledge of network
    statistics and CSI, which is quite difficult or even impossible due to the large-scale
    and massive heterogeneity of modern wireless networks. Hence, it is expected that
    modern wireless networks will integrate DRL with advanced signal processing techniques
    to overcome this issue.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User association: with the ever-increase in the number of future IoT smart
    devices and the varying QoS demands of future applications, it becomes necessary
    to ensure reliable network hyperconnectivity to these devices [[2](#bib.bib2),
    [37](#bib.bib37)]. In particular, user association defines which BS(s), wireless
    RAN’s AP(s), or wireless edge server that each user device must connect/associate
    to/with at each time to guarantee its QoS demands. Taken into consideration the
    multi-RAN and multi-connectivity nature of modern wireless networks [[3](#bib.bib3)],
    it is expected that future smart devices will be equipped with SDR capabilities
    that enable them to support multi-association/assignment to multiple RANs simultaneously
    [[39](#bib.bib39)]. Based on users’ QoS demands, devices can operate in a multi-mode
    or a multi-homing fashion. In the multi-mode fashion, each device will be associated
    with a single RAN AP at a time [[40](#bib.bib40), [39](#bib.bib39)] similar to
    the traditional fashion. Whereas in the multi-homing fashion, each user device
    can be simultaneously associated with multiple RANs APs to aggregate their radio
    resources, such as radio channels and data rate. Performing such a goal, however,
    is also another challenging issue. Obtaining real-time information on the network
    statistics such as CSI, traffic load, RANs occupancy, and the number of user devices
    and their QoS demands require unmanageable and intolerable overhead. This will
    considerably degrade the performance of the system. Hence, DRL techniques can
    be adopted efficiently in such a scenario to dynamically learn the channel and
    perform autonomous user association/assignment decisions. In designing user association
    schemes, we commonly include constraining parameters for the assignment process,
    such as the available radio resources of the wireless networks, QoS requirements
    of users, and the quality of communication links.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户关联：随着未来物联网智能设备数量的不断增加以及未来应用程序 QoS 需求的变化，确保这些设备的网络超连接性变得必要[[2](#bib.bib2)，[37](#bib.bib37)]。具体而言，用户关联定义了每个用户设备在每个时间点必须连接/关联到的基站（BS）、无线
    RAN 的接入点（AP）或无线边缘服务器，以保证其 QoS 需求。考虑到现代无线网络的多 RAN 和多连接特性[[3](#bib.bib3)]，预计未来智能设备将配备
    SDR 能力，使其能够同时支持多关联/分配到多个 RAN[[39](#bib.bib39)]。根据用户的 QoS 需求，设备可以以多模式或多接入方式运行。在多模式方式下，每个设备将与单个
    RAN AP 关联[[40](#bib.bib40)，[39](#bib.bib39)]，类似于传统方式。而在多接入方式下，每个用户设备可以同时与多个 RAN
    AP 关联，以聚合其无线资源，如无线信道和数据速率。然而，实现这一目标也是另一个挑战性问题。获取网络统计信息，如 CSI、流量负载、RAN 占用情况、用户设备数量及其
    QoS 需求，需要难以管理和不可容忍的开销，这将显著降低系统性能。因此，在这种情况下，可以有效采用 DRL 技术以动态学习信道并进行自主用户关联/分配决策。在设计用户关联方案时，我们通常包括约束分配过程的参数，例如无线网络的可用无线资源、用户的
    QoS 要求和通信链路的质量。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rate control: often, the main objective of RRAM is to maximize the QoS of wireless
    networks in terms of the network’s sum-rate or SE. This is typically achieved
    by formulating complex wireless network optimization problems and deriving their
    optimal solutions subjected to available network’s radio resources, such as power
    budget and spectrum availability, while respecting the data rate demands of user
    devices. The optimal solutions obtained represent bound on network performance
    for practical RRAM algorithms.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 速率控制：通常，RRAM 的主要目标是最大化无线网络的 QoS，就网络的总速率或 SE 来说。这通常通过制定复杂的无线网络优化问题并推导其最佳解决方案来实现，需考虑网络的可用无线资源，如功率预算和频谱可用性，同时尊重用户设备的数据速率需求。获得的最佳解决方案代表了实际
    RRAM 算法对网络性能的界限。
- en: However, accurate solutions for such optimization problems require full knowledge
    of wireless channel gain, including the large-scale and small-scale fading [[41](#bib.bib41)].
    However, obtaining such parameters in real-time is quite difficult, especially
    for modern wireless networks, due to their rapid increases in the underlying RANs,
    the number of user devices, and the type of applications. Moreover, multi-RANs
    data rate aggregation has also been proposed recently [[42](#bib.bib42), [39](#bib.bib39)]
    to support the multi-Gbps data rate requirements of the emerging data-hungry wireless
    applications. Hence, it becomes imperative to develop efficient schemes that enable
    rate aggregation while having limited knowledge of the wireless channels. DRL
    techniques can be efficiently employed to achieve this goal [[42](#bib.bib42),
    [39](#bib.bib39), [40](#bib.bib40)].
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，对于这种优化问题的准确解决方案需要完全了解无线信道增益，包括大尺度和小尺度衰落 [[41](#bib.bib41)]。然而，由于现代无线网络的基础
    RANs、用户设备数量和应用类型的快速增长，实时获取这些参数非常困难。此外，最近还提出了多 RAN 数据速率聚合 [[42](#bib.bib42), [39](#bib.bib39)]，以支持新兴数据密集型无线应用的多
    Gbps 数据速率需求。因此，开发能够在对无线信道了解有限的情况下实现速率聚合的高效方案变得至关重要。DRL 技术可以有效地实现这一目标 [[42](#bib.bib42),
    [39](#bib.bib39), [40](#bib.bib40)]。
- en: II-B Conventional RRAM Techniques
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 传统 RRAM 技术
- en: In this subsection, we overview the state-of-the-art approaches and tools that
    are used for RRAM in modern wireless systems. In general, RRAM techniques can
    be classified into two broad categories based on their adaptivity to the wireless
    environment; namely, static or dynamic approaches. Each of which can be further
    classified based on various criteria, such as centralized or distributed, instantaneous
    or ergodic, optimal or sub-optimal, single-cell or multi-cell, cooperative or
    noncooperative, in addition to different combinations of these variants. In this
    paper, we discuss the features of the static and dynamic techniques along with
    their types.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们概述了用于现代无线系统中的 RRAM 的最先进的方法和工具。一般来说，RRAM 技术可以根据其对无线环境的适应性分为两大类；即静态或动态方法。每种方法还可以根据各种标准进一步分类，例如集中式或分布式、瞬时或遍历、最优或次优、单小区或多小区、合作或非合作，以及这些变体的不同组合。在本文中，我们讨论了静态和动态技术的特点以及它们的类型。
- en: 'RRAM has been one of the major research interests in wireless networks using
    conventional approaches. It has been extensively surveyed for various wireless
    networks and systems. Table [III](#S2.T3 "TABLE III ‣ II-B Conventional RRAM Techniques
    ‣ II Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") lists some of the existing surveys for resource
    allocation and management using conventional methods along with the types of wireless
    networks and systems they study.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'RRAM 一直是使用传统方法的无线网络中的主要研究兴趣之一。它已经对各种无线网络和系统进行了广泛的调查。表 [III](#S2.T3 "TABLE III
    ‣ II-B Conventional RRAM Techniques ‣ II Radio Resource Allocation and Management
    Techniques ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") 列出了使用传统方法进行资源分配和管理的一些现有调查以及它们研究的无线网络和系统类型。'
- en: 'TABLE III: Existing Surveys on Resource Allocation and Management for Wireless
    Networks and Systems Using Conventional Approaches'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：使用传统方法的无线网络和系统资源分配与管理的现有调查
- en: '| Paper | Types of wireless networks and systems studied |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 研究的无线网络和系统类型 |'
- en: '| [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45)] | Cognitive radio
    networks (CRNs) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45)] | 认知无线电网络 (CRNs) |'
- en: '| [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]
    | Wireless HetNets |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]
    | 无线异构网络 |'
- en: '| [[50](#bib.bib50)] | M2M communication networks |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[50](#bib.bib50)] | 机器对机器通信网络 |'
- en: '| [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]
    | OFDM systems |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]
    | OFDM 系统 |'
- en: '| [[55](#bib.bib55)] | MIMO-OFDM systems |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | MIMO-OFDM 系统 |'
- en: '| [[56](#bib.bib56), [57](#bib.bib57)] | D2D communication networks |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56), [57](#bib.bib57)] | 设备间通信网络 |'
- en: '| [[58](#bib.bib58)] | UAV communications |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[58](#bib.bib58)] | 无人机通信 |'
- en: '| [[59](#bib.bib59), [60](#bib.bib60)] | Vehicular communications (V2X) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59), [60](#bib.bib60)] | 车载通信 (V2X) |'
- en: '| [[61](#bib.bib61)] | Railway communications |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | 铁路通信 |'
- en: '| Value added in this paper | Focus on the applications of DRL techniques for
    RRAM in next generation wireless networks, such as cellular HomNets, IoT networks,
    satellite networks, multi-RATs networks, HetNet, etc. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 本文增值 | 专注于DRL技术在下一代无线网络中的RRAM应用，如蜂窝HomNets、物联网（IoT）网络、卫星网络、多RAT网络、异构网络（HetNet）等。
    |'
- en: II-B1 Static Techniques
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 静态技术
- en: Static approaches are designed based on a priori statistical information and
    cannot adapt to wireless network parameters, such as traffic load, users’ mobility
    pattern, channel conditions/quality, network spectrum occupancy, and users’ QoS
    demands. These techniques are simple; however, they suffer from several degradations,
    such as severe underutilization of radio resources, increased network outage,
    reduced network throughput, and poor network QoS.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 静态方法是基于先验统计信息设计的，不能适应无线网络参数，如流量负载、用户移动模式、信道条件/质量、网络频谱占用和用户QoS需求。这些技术虽然简单，但存在多个降级问题，如无线电资源严重不足、网络中断增加、网络吞吐量降低和网络QoS差。
- en: Static RRAM techniques are employed in several traditional wireless communication
    networks, such as cellular communication networks and WLANs. Examples of static
    RRAM techniques include circuit-mode communication using frequency division multiple
    access (FDMA) and time division multiple access (TDMA) schemes and fixed radio
    resource allocation, such as fixed power and channel allocation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 静态RRAM技术被应用于多个传统无线通信网络中，例如蜂窝通信网络和WLAN。静态RRAM技术的例子包括使用频分多址（FDMA）和时分多址（TDMA）方案的电路模式通信，以及固定无线电资源分配，例如固定功率和信道分配。
- en: II-B2 Dynamic Techniques
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 动态技术
- en: On the contrary, dynamic or adaptive RRAM approaches are more efficient as they
    can dynamically adjust the network radio resources to accurately track variations
    in propagation conditions and user QoS requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，动态或自适应RRAM方法更为高效，因为它们可以动态调整网络无线电资源，以准确跟踪传播条件和用户QoS需求的变化。
- en: Dynamic RRAM schemes are widely utilized in designing modern wireless systems.
    They have shown efficient results in reducing the expensive manual network planning
    and achieving tighter radio resource utilization, which will lead eventually to
    enhanced network SE. Some RRAMs schemes are centralized, where several BSs, ESs,
    wireless APs, and network gateways are controlled by a central Radio Network Controller
    (RNC). Others are distributed, either autonomous algorithms implemented in smart
    user devices, BSs, ESs, or APs, or coordinated by exchanging information among
    these network entities. Examples of dynamic RRAM schemes include power control
    algorithms, dynamic spectrum/channel allocation algorithms, multi-access control
    schemes, link adaptation algorithms, precoding schemes, traffic adaption algorithms,
    channel-dependent scheduling schemes, and cognitive radio approaches.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 动态RRAM方案在现代无线系统设计中被广泛应用。它们在减少昂贵的人工网络规划和实现更紧密的无线电资源利用方面显示了高效的结果，这将最终提高网络的SE。一些RRAM方案是集中式的，其中多个基站（BS）、边缘服务器（ES）、无线接入点（AP）和网络网关由中央无线网络控制器（RNC）控制。其他则是分布式的，或在智能用户设备、BS、ES或AP中实施自主算法，或通过在这些网络实体之间交换信息进行协调。动态RRAM方案的例子包括功率控制算法、动态频谱/信道分配算法、多接入控制方案、链路自适应算法、预编码方案、流量自适应算法、信道依赖调度方案和认知无线电方法。
- en: In dynamic RRAM, we typically formulate the wireless radio resource allocation
    problem as a complex optimization problem. The main objective of such a problem
    is maximizing/minimizing some utility/cost functions, e.g., network sum-rate,
    EE, and SE, while constraining the available network’s radio resources such as
    the available power and bandwidth. The state-of-the-art approaches to solve these
    wireless RRAM optimization problems are heuristic-based, optimization-based, and
    game theory-based approaches. Such approaches employ advanced algorithms to solve
    the RRAM problem either optimally or sub-optimally.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态RRAM中，我们通常将无线电资源分配问题表述为一个复杂的优化问题。此类问题的主要目标是最大化/最小化某些效用/成本函数，例如网络总速率、能效（EE）和谱效（SE），同时约束可用网络的无线电资源，例如可用功率和带宽。解决这些无线RRAM优化问题的最先进方法包括基于启发式的方法、基于优化的方法和基于博弈论的方法。这些方法采用先进的算法来优化或次优地解决RRAM问题。
- en: II-B2a   Heuristic-Based Techniques
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: II-B2a   基于启发式的技术
- en: These techniques allocate radio resources in a sub-optimal fashion and without
    any performance guarantee. They are typically used to provide approximate and
    sub-optimal solutions in cases the solution of the formulated optimization problem
    is quite complex or intractable. Modern wireless communication systems such as
    4G LTE implements some types of greedy heuristics [[34](#bib.bib34)]. Examples
    of heuristic algorithms include the recursive branch-and-bound state-space search
    algorithm [[62](#bib.bib62)], alpha-beta search algorithm [[63](#bib.bib63)],
    and particle swarm optimization (PSO).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术以次优的方式分配无线资源，并且没有任何性能保证。它们通常用于提供近似和次优解，当所制定的优化问题的解相当复杂或难以处理时。现代无线通信系统如 4G
    LTE 实施了一些类型的贪婪启发式算法[[34](#bib.bib34)]。启发式算法的例子包括递归分支限界状态空间搜索算法[[62](#bib.bib62)]，alpha-beta
    搜索算法[[63](#bib.bib63)]，以及粒子群优化（PSO）。
- en: II-B2b   Optimization-Based Techniques
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: II-B2b 优化基础技术
- en: Typically, most of the RRAM optimization problems in modern wireless networks
    are non-convex (e.g., continuous power allocation) [[64](#bib.bib64)], combinatorial
    (e.g., user association and channel access) [[12](#bib.bib12)], or mixed-integer
    nonlinear programming (MINP) (e.g., combined of continuous- and discrete-type
    problems) [[39](#bib.bib39)]. Many algorithms have been developed to systematically
    solve such problems and find either the global optimum [[12](#bib.bib12)] solution
    or sub-optimal solution. Such algorithms include, fractional programming (FP)
    [[65](#bib.bib65), [64](#bib.bib64)], genetic [[66](#bib.bib66)], Weighted Minimum
    Mean Square Error (WMMSE) [[65](#bib.bib65), [64](#bib.bib64)], among others.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，现代无线网络中的大多数 RRAM 优化问题都是非凸的（例如，连续功率分配）[[64](#bib.bib64)]，组合的（例如，用户关联和频道访问）[[12](#bib.bib12)]，或混合整数非线性规划（MINP）（例如，连续型和离散型问题的结合）[[39](#bib.bib39)]。已经开发了许多算法来系统地解决这些问题，并找到全球最优[[12](#bib.bib12)]解或次优解。这些算法包括，分数规划（FP）[[65](#bib.bib65),
    [64](#bib.bib64)]，遗传算法[[66](#bib.bib66)]，加权最小均方误差（WMMSE）[[65](#bib.bib65), [64](#bib.bib64)]，等。
- en: These algorithms are extremely computationally-extensive and typically executed
    in a central RNC with full and real-time information about network statistics
    and CSI.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法计算量极大，通常在一个具有网络统计和 CSI 的完整实时信息的中央 RNC 中执行。
- en: II-B2c   Game Theory-Based Methods
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: II-B2c 基于博弈论的方法
- en: Game theory techniques are typically used for distributed RRAM of modern wireless
    networks when network entities (i.e., players) cooperate or compete on radio resources.
    Such techniques have shown efficient results, and they are widely used as tools
    to model complex wireless optimization problems in a decentralized fashion [[1](#bib.bib1)].
    In particular, the RRAM problem is formulated as a cooperative or non-cooperative
    game/optimization problem between network entities, such as BSs, RANs’ APs, and
    user devices. In cooperative game techniques, players collaboratively solve the
    underlying RRAM game using heuristic- or optimization-based techniques to achieve
    a specific network goal such as sum-rate or SE maximization. However, in non-cooperative
    game techniques, players try to solve the RRAM game in a greedy and non-collaborative
    fashion in order to achieve their own goal (e.g., to satisfy their own QoS demands).
    The main goal of most game theory algorithms is to find the Nash Equilibrium (NE)
    solution for the underlying RRAM problem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论技术通常用于现代无线网络的分布式 RRAM，当网络实体（即玩家）在无线资源上进行合作或竞争时。这些技术已显示出高效的结果，广泛用于以去中心化的方式建模复杂的无线优化问题[[1](#bib.bib1)]。特别地，RRAM
    问题被构建为网络实体之间的合作或非合作博弈/优化问题，例如基站（BSs），RAN 的接入点（APs）和用户设备。在合作博弈技术中，玩家通过启发式或基于优化的技术共同解决底层
    RRAM 博弈，以实现特定的网络目标，如总速率或 SE 最大化。然而，在非合作博弈技术中，玩家试图以贪婪和非合作的方式解决 RRAM 博弈，以实现他们自己的目标（例如，满足他们自己的
    QoS 需求）。大多数博弈论算法的主要目标是找到底层 RRAM 问题的纳什均衡（NE）解。
- en: II-C Limitation of Conventional RRAM Techniques
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 传统 RRAM 技术的局限性
- en: Unfortunately, all these state-of-the-art approaches will encounter severe limitations
    in future wireless networks, which mainly motivate the usage of DRL in RRAM. Here
    we summarize the main limitations, and the interested reader can also refer to
    [[1](#bib.bib1)].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，所有这些最先进的方法在未来的无线网络中将面临严重的局限性，这主要激励了 DRL 在 RRAM 中的使用。我们在这里总结了主要的局限性，有兴趣的读者还可以参考[[1](#bib.bib1)]。
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most of these approaches require complete or quasi-complete knowledge of the
    wireless environment, including accurate channel models and real-time channel
    state information (CSI). However, obtaining such accurate information in next
    generation wireless networks is quite difficult or even impossible due to the
    large-scale, ultra-dense, and massive heterogeneity nature of the system.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数这些方法需要对无线环境进行完全或准完全的了解，包括准确的信道模型和实时信道状态信息（CSI）。然而，在下一代无线网络中获得这样准确的信息是相当困难甚至不可能的，因为系统的大规模、超密集和大量的异质性特性。
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: These approaches are generally not scalable, and they encounter several challenges
    when the number of user devices becomes very large or when used in ultra-dense
    wireless networks (UDNs) with multi-cell multi-objective homogeneous/heterogeneous
    scenarios. The main reason is that the optimization space becomes prohibitively
    large to cater to the whole wireless network. This will lead to a significant
    increase in computational complexity when finding optimal solutions. With this
    large-scale and massive heterogeneity nature of future wireless networks, it becomes
    essential to engineer and devise more efficient and practical implementations
    from computation and performance perspectives. Also, it becomes even quite challenging
    in many scenarios to mathematically formulate RRAM optimization problems, or we
    may end up with non-well-defined or even intractable optimization problems. Such
    cases are typically encountered for many reasons, such as the uncertain nature
    of wireless channels, network traffic load, users’ mobility patterns, etc. Hence,
    new innovative RRAM solutions must be developed to address such challenges. In
    this context, the data-driven artificial intelligence (AI)-based RRAM techniques
    are feasible solutions in such scenarios, and they have shown efficient adaptivity
    when applied on the dynamic wireless networks.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当用户设备数量变得非常庞大或在具有多小区多目标同质/异质场景的超密集无线网络（UDNs）中使用时，这些方法通常缺乏可扩展性，并且会遇到几个挑战。主要原因是优化空间变得过大，以至于无法涵盖整个无线网络。这将导致在寻找最佳解决方案时计算复杂性显著增加。随着未来无线网络的大规模和大量异质性特性，从计算和性能的角度来看，必须开发更高效和实用的实施方案变得至关重要。此外，在许多场景中，数学上规划RRAM优化问题甚至变得相当具有挑战性，或者我们最终可能会遇到的是一个不明确甚至是难以处理的优化问题。出现这种情况通常是由于诸多原因，如无线信道的不确定性、网络流量负载、用户的移动模式等。因此，必须开发新的创新性的RRAM解决方案来解决这些挑战。在这种情况下，在动态无线网络上应用基于数据驱动的人工智能（AI）RRAM技术成为了可行的解决方案，并且在动态无线网络上的应用时表现出了高效的适应性。
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Such approaches are heavily system-dependent and will not be accurate for rapidly
    varying systems or wireless environments. They need, however, reconfiguration
    to reflect the new system settings. Unfortunately, modern wireless networks need
    to support highly dynamic systems characterized by massive rapidities, such as
    vehicular and railway networks. This renders conventional approaches impractical
    for such scenarios.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法严重依赖系统，并且对于快速变化的系统或无线环境不够准确。然而，它们需要重新配置以反映新的系统设置。不幸的是，现代无线网络需要支持具有大规模快速性的高度动态系统，例如车载和铁路网络。这使得传统方法在这种情况下变得不切实际。
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most of these approaches are computationally expensive and incur considerable
    timing overhead. This renders them inefficient for most emerging time-sensitive
    applications, such as autonomous vehicles/drones applications. Also, the computational
    complexity of these approaches proportionally increases with the increase in the
    network size, making them unscalable and unsuitable for modern large-scale wireless
    networks. Furthermore, since most of the conventional algorithms are computationally
    expensive, they can be implemented only in sophisticated infrastructures with
    high computational capabilities, such as supercomputers and servers. Hence, tiny
    and self-powered user devices will not be able to support them.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数这些方法在计算上都很昂贵，且会产生相当大的时序开销。这使它们对于大多数新兴的对时间敏感的应用，如自动车辆/无人机应用，效率不高。此外，这些方法的计算复杂性随着网络规模的增加成比例地增加，导致它们不具备可扩展性，不适合现代大规模无线网络。而且，由于大多数传统算法在计算上都很昂贵，它们只能在拥有高计算能力的复杂基础设施中实施，例如超级计算机和服务器。因此，微小的、自供电的用户设备将无法支持它们。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RRAM optimization problems in wireless networks are generally complex and non-convex
    [[39](#bib.bib39)]. Hence, leveraging conventional optimization algorithms to
    solve such problems will likely result in local optimal solutions rather than
    global ones. This case is regularly encountered in wireless optimization problems,
    which have too many local optima.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Game theory-based techniques are unsuitable for networks characterized by massive
    heterogeneity in system architecture and user devices. In particular, NE solutions
    are obtained based on the assumption that all players are homogeneous, have statistically
    equal capabilities, and have complete network information. Unfortunately, this
    is not the case in modern wireless networks, in which network entities are massively
    heterogeneous in terms of physical, communication, and computational capabilities.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the complexity of game theory-based techniques and the amount of information
    exchanged between competing players are proportional to the number of playing
    nodes. Unfortunately, future wireless networks are expected to be prohibitively
    large-scale in terms of the number of network APs and user devices [[2](#bib.bib2),
    [6](#bib.bib6)]. Hence, such techniques will fail. In particular, exchanging and
    updating the tremendous amount of data and signaling among the massive number
    of players will create extra and unmanageable overhead as well as a drastic increase
    in delay, computation, and energy/memory consumption of network players.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-D Advantages of Using DRL-Based Techniques for RRAM
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Emerging AI tools, such as ML, DL, and DRL methods, have been recently used
    to effectively address various problems and challenges in different areas of wireless
    communications and networking, including RRAM [[1](#bib.bib1), [24](#bib.bib24),
    [23](#bib.bib23), [67](#bib.bib67), [68](#bib.bib68), [21](#bib.bib21), [12](#bib.bib12),
    [15](#bib.bib15), [16](#bib.bib16)]. Next generation wireless networks will generate
    a tremendous amount of data related to network statistics, such as user traffic,
    channel occupancy, channel quality, etc. AI algorithms can leverage this data
    to develop automated and fine-grained schemes to optimize network radio resources.
    This paper is solely dedicated to providing a comprehensive survey on DRL applications
    for RRAM in modern wireless networks. However, the applications of ML and DL techniques
    in various wireless fields can be found in [[1](#bib.bib1), [12](#bib.bib12),
    [23](#bib.bib23), [24](#bib.bib24)] and the references therein.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: DRL is an advanced data-driven AI technique that combines neural networks (NNs)
    with traditional reinforcement learning (RL). It is mainly utilized to enhance
    the learning rate of RL algorithms and address wireless communication and networking
    problems having high dimensionality [[16](#bib.bib16), [35](#bib.bib35), [17](#bib.bib17),
    [36](#bib.bib36)]. DRL techniques have gained considerable fame lately to their
    superiority in making judicious control decisions in uncertain environments like
    the wireless channels. They enable various network components such as BSs, RAT
    APs, edge servers (ESs), gateways nodes, and user devices to make autonomous and
    local decisions, such as RRAM, RATs selection, caching, and offloading, that achieve
    the objectives of various wireless networks, including sum-rate maximization and
    SE/EE maximization. Since traditional approaches will not be able to address the
    RRAM issue of future wireless networks, DRL methods have been proposed lately
    to be alternative solutions. In particular, DRL techniques are appealing for next
    generation wireless communication networks due to the following distinct features.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 是一种先进的数据驱动人工智能技术，它将神经网络（NNs）与传统的强化学习（RL）结合起来。它主要用于提高 RL 算法的学习速度，并解决高维无线通信和网络问题
    [[16](#bib.bib16), [35](#bib.bib35), [17](#bib.bib17), [36](#bib.bib36)]。由于 DRL
    技术在不确定环境下（如无线信道）做出明智的控制决策方面的优越性，近年来它们获得了相当大的声誉。它们使得各种网络组件（如 BSs、RAT APs、边缘服务器（ESs）、网关节点和用户设备）能够做出自主和本地的决策，例如
    RRAM、RATs 选择、缓存和卸载，以实现各种无线网络的目标，包括总速率最大化和 SE/EE 最大化。由于传统方法无法解决未来无线网络的 RRAM 问题，最近提出了
    DRL 方法作为替代解决方案。特别是，由于以下独特的特点，DRL 技术对于下一代无线通信网络具有吸引力。
- en: First, they enable network controllers to solve complex network optimization
    problems, including RRAM and other wireless control problems, with only limited
    information about the wireless networks. Second, DRL methods enable network entities
    (e.g., BSs, RAT APs, ESs, gateways nodes, and user devices) to act as agents (i.e.,
    decision-makers) to learn and build knowledge about the wireless environment.
    This is achieved by learning optimal policies, such as radio resource allocation,
    RATs selection, and scheduling decisions, based on continuous interaction between
    agents and the wireless environment, without knowing the accurate channel models
    or statistics of the underlying systems a-priori. DRL algorithms employ the data
    collected during the continuous interaction with the environment as a training
    data-set to train their models. Once DRL agents learned the optimal policies,
    they can be deployed in an online fashion to make intelligent and autonomous decisions
    based on local observations made on the wireless environment.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它们使网络控制器能够在仅有关于无线网络的有限信息的情况下解决复杂的网络优化问题，包括 RRAM 和其他无线控制问题。其次，DRL 方法使网络实体（例如
    BSs、RAT APs、ESs、网关节点和用户设备）能够作为代理（即决策者）学习并构建关于无线环境的知识。这是通过基于代理与无线环境之间的持续互动学习最优策略（如无线资源分配、RATs
    选择和调度决策）来实现的，而无需事先知道准确的信道模型或基础系统的统计数据。DRL 算法利用在与环境持续互动过程中收集的数据作为训练数据集来训练它们的模型。一旦
    DRL 代理学会了最优策略，它们可以以在线的方式部署，以根据在无线环境中进行的本地观察做出智能和自主的决策。
- en: DRL techniques provide efficient solutions from both the network and user devices’
    points of view to overcome the problems of the conventional RRAM approaches. By
    employing DRL techniques, various network entities are enabled to learn wireless
    environments in order to optimize system configuration. Networks entities will
    be able to optimally and autonomously allocate the optimal transmitting power
    to mitigate signals interference and reduce energy consumption. For this purpose,
    advanced DRL techniques such as the deep deterministic policy gradient (DDPG)
    method and its variants can be utilized. On the other hand, DRL can also enable
    smart devices to autonomously access the radio channels. For this purpose, deep
    Q-network (DQN) and its variants can be leveraged. The wireless channels are extremely
    stochastic due to, e.g., the rapid mobility of user devices and channel objects.
    Hence, accurate and real-time knowledge of channel state information (CSI) becomes
    quite difficult, and DRL techniques can be efficiently used to learn wireless
    channel statistics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 技术从网络和用户设备的角度提供了高效的解决方案，以克服传统 RRAM 方法的问题。通过使用 DRL 技术，各种网络实体可以学习无线环境，以优化系统配置。网络实体将能够以最优方式和自主分配最佳的发射功率，以减轻信号干扰和降低能耗。为此，可以利用先进的
    DRL 技术，如深度确定性策略梯度（DDPG）方法及其变体。另一方面，DRL 还可以使智能设备自主访问无线电通道。为此，可以利用深度 Q 网络（DQN）及其变体。由于用户设备和通道对象的快速移动等因素，无线通道非常随机。因此，准确的实时通道状态信息（CSI）变得非常困难，而
    DRL 技术可以有效地用于学习无线通道统计信息。
- en: Finally, spectrum prediction and forecasting is also another promising field
    enabled by DRL techniques. Emerging DL models, such as recurrent neural networks
    (RNNs) and convolutional neural networks (CNNs), can be integrated with DRL to
    add the ”prediction” capability to the DRL algorithms. Also, conventional optimization
    techniques do not incorporate the context, and hence they cannot adapt and react
    according to the sudden variations and changes in the wireless environments. Therefore,
    such conventional approaches will result in unreliable and poor resource management
    and utilization. DRL techniques can, however, dynamically adapt and learn the
    context of wireless environments, which makes their RRAM solutions more accurate
    and reliable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，频谱预测和预报也是 DRL 技术支持的一个有前途的领域。新兴的深度学习模型，如递归神经网络（RNN）和卷积神经网络（CNN），可以与 DRL 集成，为
    DRL 算法增加“预测”能力。此外，传统优化技术未考虑上下文，因此无法根据无线环境中的突发变化和变化进行适应和反应。因此，这些传统方法将导致资源管理和利用不可靠且效果差。然而，DRL
    技术可以动态适应并学习无线环境的上下文，使其 RRAM 解决方案更加准确和可靠。
- en: To sum up, DRL techniques are required in RRAM problems in four main scenarios;
    when there is insufficient knowledge about the statistics of the wireless networks,
    accurate mathematical models do not exist, inference information is required to
    be incorporated into the decision process, or a mathematical model exists, but
    applying conventional algorithms is not possible. In general, most of the RRAM
    problems in modern wireless networks fall under the above scenarios. The main
    reason is the large-scale and massive heterogeneity nature of networks in terms
    of types and numbers of underlying infrastructures, user devices, and QoS demands
    of applications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，DRL 技术在 RRAM 问题中的应用主要有四种情境；当对无线网络统计信息了解不足时，缺乏准确的数学模型时，需要将推断信息纳入决策过程时，或者存在数学模型但无法应用传统算法时。通常，现代无线网络中的大多数
    RRAM 问题都属于上述情境。主要原因是网络的规模庞大且异质性严重，包括基础设施的类型和数量、用户设备以及应用的服务质量（QoS）需求。
- en: All the aforementioned unique features of DRL techniques make them one of the
    leading AI-based enabling technologies that can be leveraged to address the RRAM
    in future wireless communication networks [[2](#bib.bib2), [3](#bib.bib3)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述独特的深度强化学习（DRL）技术特性使其成为领先的基于人工智能的赋能技术之一，可以用于解决未来无线通信网络中的RRAM问题[[2](#bib.bib2),
    [3](#bib.bib3)]。
- en: III Overview of DRL Techniques Used for RRAM
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III DRL 技术在 RRAM 中的概述
- en: 'In this section, we briefly review the foundations of DRL, such as the Markov
    Decision Process (MDP), and show how RRAM problems can be modeled as MDPs. Fig.
    [7](#S3.F7 "Figure 7 ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") shows a detailed taxonomy of existing DRL techniques/algorithms.
    Reviewing all these techniques is beyond the scope of this paper, and we rather
    focus on the most widely used ones in the literature to address RRAM problems.
    Interested readers, however, can refer to [[14](#bib.bib14), [15](#bib.bib15)]
    for a thorough review of the remaining algorithms. Furthermore, we briefly review
    other emerging technologies used for RRAM problems, such as federated/distributed
    learning (FL) and multi-agent DRL (MARDL) models. Hence, this section is deliberately
    designed to provide the reader with adequate knowledge of the basics, advantages,
    limitations, and use-cases of the most widely used DRL techniques employed in
    the RRAM field.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们简要回顾了DRL的基础知识，例如马尔可夫决策过程（MDP），并展示了如何将RRAM问题建模为MDP。图[7](#S3.F7 "Figure
    7 ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")展示了现有DRL技术/算法的详细分类。回顾所有这些技术超出了本文的范围，我们更关注文献中最广泛使用的技术，以解决RRAM问题。然而，有兴趣的读者可以参考[[14](#bib.bib14),
    [15](#bib.bib15)]进行对其余算法的全面回顾。此外，我们还简要回顾了用于RRAM问题的其他新兴技术，例如联邦/分布式学习（FL）和多智能体DRL（MARDL）模型。因此，本节专门设计了提供读者有关最广泛使用的DRL技术在RRAM领域中的基础知识、优点、局限性和使用案例。'
- en: 'Table [IV](#S3.T4 "TABLE IV ‣ III Overview of DRL Techniques Used for RRAM
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") lists the most widely
    used DRL techniques/algorithms in RRAM of modern wireless networks. Note that
    all of them are model-free learning algorithms, which means that the agent does
    not build a model of the wireless environment or reward; instead, it directly
    maps states to the corresponding actions.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '表[IV](#S3.T4 "TABLE IV ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey")列出了现代无线网络中广泛使用的RRAM DRL技术/算法。请注意，所有这些都是无模型学习算法，这意味着代理不会构建无线环境或奖励的模型；相反，它直接将状态映射到相应的动作。'
- en: Depending on the dimensionality of the RRAM problem, we can select the most
    appropriate DRL algorithm that fits the problem settings. For example, RRAM problems
    could have discrete action space, such as channel access, user association, RAN
    assignment, etc., or could have continuous action space, such as power allocation
    and continuous spectrum allocation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据RRAM问题的维度，我们可以选择最适合问题设置的DRL算法。例如，RRAM问题可能具有离散的动作空间，例如频道接入、用户关联、RAN分配等，或具有连续的动作空间，例如功率分配和连续频谱分配。
- en: '![Refer to caption](img/b56facc7f675915c99dd3ae5d2057e58.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b56facc7f675915c99dd3ae5d2057e58.png)'
- en: 'Figure 7: Taxonomy of all DRL algorithms. Algorithms colored in blue are covered
    in Section [III](#S3 "III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '图7：所有DRL算法的分类。蓝色标记的算法在第[III](#S3 "III Overview of DRL Techniques Used for RRAM
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey")节中介绍。'
- en: 'TABLE IV: List of the Model-Free DRL Algorithms That Are Widely Used in RRAM
    for Modern Wireless Networks'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：现代无线网络中广泛使用的无模型DRL算法列表
- en: '| Family | Algorithm | Action Space | Policy Type |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 家族 | 算法 | 动作空间 | 策略类型 |'
- en: '|  | $Q$-Learning | Discrete (also discrete state space) |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q$-学习 | 离散（也包括离散状态空间） |  |'
- en: '|  | DQN | Discrete |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | DQN | 离散 |  |'
- en: '|  | Double DQN | Discrete |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 双重DQN | 离散 |  |'
- en: '| Value-Based | Dueling DQN | Discrete | Off |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 基于值的 | 对抗DQN | 离散 | 关 |'
- en: '|  | REINFORCE | Discrete & Continuous | On |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | REINFORCE | 离散 & 连续 | 开 |'
- en: '|  | A2C-A3C | Discrete & Continuous | On |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | A2C-A3C | 离散 & 连续 | 开 |'
- en: '| Policy-Based | DDPG | Continuous | Off |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 基于策略的 | DDPG | 连续 | 关 |'
- en: III-A The Markov Decision Process (MDP)
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 马尔可夫决策过程（MDP）
- en: Under the uncertain and stochastic environments of modern wireless networks,
    the problem of RRAM, or any decision-making problem including control problems,
    are typically modeled by the so-called Markov Decision Process (MDP). It provides
    a mathematical framework for modeling decision-making problems whose outcome is
    random and controlled by a decision-maker, aka agent. The MDP also has another
    variant, called partially observable MDP (POMDP), which models decision-making
    problems in partially observable wireless environments.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代无线网络的不确定和随机环境下，RRAM 或任何决策问题，包括控制问题，通常通过所谓的马尔可夫决策过程（MDP）来建模。它为建模结果随机且由决策者（即代理）控制的决策问题提供了一个数学框架。MDP
    还有另一种变体，称为部分可观察的 MDP（POMDP），它用于建模部分可观察无线环境中的决策问题。
- en: 'The general practice in RRAM is to formulate the radio resource allocation
    as an optimization problem whose objective is to maximize/minimize some network
    utility/cost function while constraining on the available network radio resources
    and optional QoS demands of user devices. However, as we discussed in Section
    [II](#S2 "II Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"), tremendous challenges are encountered during formulating
    such problems or/and even during solving them, which renders conventional approaches
    inapplicable. Hence, RL/DRL techniques are utilized instead.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 'RRAM 中的常规做法是将无线资源分配公式化为一个优化问题，其目标是最大化/最小化某些网络效用/成本函数，同时对可用的网络无线资源和用户设备的可选 QoS
    需求进行约束。然而，正如我们在第 [II](#S2 "II Radio Resource Allocation and Management Techniques
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey")节中讨论的那样，在公式化这些问题或/和甚至在解决这些问题时，会遇到巨大的挑战，这使得传统方法不适用。因此，RL/DRL
    技术被用来替代。'
- en: 'In order to apply DRL to solve radio resource allocation (RRA) problems, we
    need first to convert the formulated optimization problem into the MDP framework.
    The resultant MDP-based model must contain seven elements: the agent(s), environment,
    action space $\mathcal{A}$, state space $\mathcal{S}$, instantaneous reward function
    $r$, a transition probability $p$, and policy $\pi$, as shown in Fig. [8](#S3.F8
    "Figure 8 ‣ III-A The Markov Decision Process (MDP) ‣ III Overview of DRL Techniques
    Used for RRAM ‣ Deep Reinforcement Learning for Radio Resource Allocation and
    Management in Next Generation Heterogeneous Wireless Networks: A Survey"). The
    MDP is typically represented mathematically by the tuple ($\mathcal{S}$, $\mathcal{A}$,
    $p$, $r$).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将 DRL 应用于解决无线资源分配（RRA）问题，我们首先需要将公式化的优化问题转换为 MDP 框架。得到的基于 MDP 的模型必须包含七个元素：代理（s）、环境、动作空间
    $\mathcal{A}$、状态空间 $\mathcal{S}$、即时奖励函数 $r$、转移概率 $p$ 和策略 $\pi$，如图 [8](#S3.F8 "Figure
    8 ‣ III-A The Markov Decision Process (MDP) ‣ III Overview of DRL Techniques Used
    for RRAM ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey")所示。MDP 通常用元组 ($\mathcal{S}$,
    $\mathcal{A}$, $p$, $r$) 在数学上表示。'
- en: 'In RRAM problems, the dynamicity of the agent’s learning process according
    to the MDP framework is shown in Fig. [8](#S3.F8 "Figure 8 ‣ III-A The Markov
    Decision Process (MDP) ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"), and described as follows. At time $t$, the agent
    observes a state $s_{t}$ from the state space $\mathcal{S}$. The state space should
    contain useful and effective information about the wireless network environment,
    such as available radio resources, SNR, the number of user devices, required QoS,
    rate, etc. Then, the agent takes action $a_{t}$ from the action space $\mathcal{A}$
    such as the RRA, RAN assignment, etc. The taken action must achieve network utility
    goal, such as sum-rate maximization, SE/EE maximization, etc. Then the state moves
    to a new state $s_{t+1}$ with a transition probability $p$, and the agent receives
    a feedback numerical instantaneous reward $r_{t}$ which quantifies the quality
    of the taken action. This interaction, i.e., $(s_{t},a_{t},r_{t},s_{t+1})$, between
    the agent and the wireless environment repeatedly continues, and the agent will
    utilize the received instantaneous reward signal to adjust its strategy until
    it learns the optimal policy $\pi^{*}$. The agent’s policy $\pi$ defines the mapping
    from states to the corresponding actions $\mathcal{S}$ $\leftarrow$ $\mathcal{A}$
    , i.e., $a_{t}=\pi(s_{t})$. Typically, we define the long-term reward as the expected
    accumulated discounted instantaneous reward over the time horizon $T$, which is
    given by ${\cal R}=\mathbb{E}\left[\sum_{t=1}^{T}{\gamma r_{t}(s_{t},\pi(s_{t}))}\right]$.
    The parameter $0\leq\gamma\leq 1$ is the discounted factor, which trades-off between
    instantaneous and future rewards. The main goal of the agent in MDP is to obtain
    the optimal decision policy $\pi^{*}$ (i.e., selecting optimal radio resources)
    that maximizes the long-term reward, i.e., $\pi^{*}=\underset{\pi}{\text{max}}~{}{\cal
    R}$.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RRAM 问题中，按照 MDP 框架，智能体学习过程的动态性如图 [8](#S3.F8 "图 8 ‣ III-A 马尔可夫决策过程 (MDP) ‣
    III RRAM 使用的 DRL 技术概述 ‣ 下一代异构无线网络中的无线资源分配和管理的深度强化学习：综述") 所示，描述如下。在时间 $t$，智能体从状态空间
    $\mathcal{S}$ 中观察到状态 $s_{t}$。状态空间应包含有关无线网络环境的有用和有效的信息，如可用无线资源、信噪比、用户设备数量、所需服务质量、速率等。然后，智能体从动作空间
    $\mathcal{A}$ 中采取动作 $a_{t}$，如 RRA、RAN 分配等。所采取的动作必须实现网络效用目标，如总速率最大化、谱效用/能效最大化等。然后，状态转移到新状态
    $s_{t+1}$，具有转移概率 $p$，智能体接收到反馈的即时奖励 $r_{t}$，以量化所采取动作的质量。这个交互，即 $(s_{t},a_{t},r_{t},s_{t+1})$，在智能体和无线环境之间反复进行，智能体将利用收到的即时奖励信号调整其策略，直到学到最优策略
    $\pi^{*}$。智能体的策略 $\pi$ 定义了从状态到相应动作 $\mathcal{S}$ $\leftarrow$ $\mathcal{A}$ 的映射，即
    $a_{t}=\pi(s_{t})$。通常，我们定义长期奖励为时间范围 $T$ 上的期望累积折扣即时奖励，由 ${\cal R}=\mathbb{E}\left[\sum_{t=1}^{T}{\gamma
    r_{t}(s_{t},\pi(s_{t}))}\right]$ 给出。参数 $0\leq\gamma\leq 1$ 是折扣因子，用于权衡即时奖励和未来奖励。MDP
    中智能体的主要目标是获得最优决策策略 $\pi^{*}$（即选择最优无线资源），从而最大化长期奖励，即 $\pi^{*}=\underset{\pi}{\text{max}}~{}{\cal
    R}$。
- en: 'Next, we discuss the most widely used DRL algorithms to handle MDP problems,
    i.e., RRAM problems. As shown in Fig. [7](#S3.F7 "Figure 7 ‣ III Overview of DRL
    Techniques Used for RRAM ‣ Deep Reinforcement Learning for Radio Resource Allocation
    and Management in Next Generation Heterogeneous Wireless Networks: A Survey"),
    these algorithms belong to two main families of methods; namely, the value-based
    and the policy-based methods.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论最广泛使用的 DRL 算法来处理 MDP 问题，即 RRAM 问题。如图 [7](#S3.F7 "图 7 ‣ III RRAM 使用的
    DRL 技术概述 ‣ 下一代异构无线网络中的无线资源分配和管理的深度强化学习：综述") 所示，这些算法属于两大主要方法类别；即基于价值的方法和基于策略的方法。
- en: '![Refer to caption](img/9ca49a36aca37eccf0fedbdcc2436e66.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9ca49a36aca37eccf0fedbdcc2436e66.png)'
- en: 'Figure 8: Framework of DRL models.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：DRL 模型的框架。
- en: III-B Value-Based Algorithms
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于价值的算法
- en: 'This family of methods is used to estimate the value function of the agent.
    This value function is then utilized to implicitly and greedily obtain the optimal
    policy. Two value functions exists; the value function $V^{\pi}(s)$ and the state-action
    function $Q(s_{t},a_{t})$. Both represent the expected accumulated discounted
    rewards received when taking action $a_{t}$ (in state $s_{t}$ for the value function)
    (or at pair $(s_{t},a_{t})$ for the state-action function) and then following
    the policy $\pi$ thereafter. These functions are quite important as they represent
    the link between the MDP mathematical formulation and the DRL formulation, and
    they are given by [[14](#bib.bib14)]:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法用于估计代理的值函数。然后利用该值函数隐式且贪婪地获得最优策略。存在两种值函数：值函数 $V^{\pi}(s)$ 和状态-动作函数 $Q(s_{t},a_{t})$。两者都表示在采取动作
    $a_{t}$（在值函数中为状态 $s_{t}$）或在对状态-动作对 $(s_{t},a_{t})$ 进行处理后，跟随策略 $\pi$ 所获得的期望累计折扣奖励。这些函数非常重要，因为它们代表了
    MDP 数学公式与 DRL 公式之间的联系，它们的表示形式如下 [[14](#bib.bib14)]：
- en: '|  | $V^{\pi}(s)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t},s_{t+1})&#124;a_{t}\sim\pi(.&#124;s_{t}),s_{0}=s\right],$
    |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t},s_{t+1})\mid
    a_{t}\sim\pi(. \mid s_{t}),s_{0}=s\right],$ |  |'
- en: '|  | $\displaystyle\begin{split}Q^{\pi}(s,a)=\mathbb{E}\Big{[}\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t},s_{t+1})&#124;\\
    a_{t}\sim\pi(.&#124;s_{t}),s_{0}=s,a_{0}=a\Big{]}.\end{split}$ |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{split}Q^{\pi}(s,a)=\mathbb{E}\Big{[}\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t},s_{t+1})&\mid\\
    a_{t}\sim\pi(. \mid s_{t}),s_{0}=s,a_{0}=a\Big{]}.\end{split}$ |  |'
- en: 'The optimal value function $V^{*}(s)$ and state-action function $Q^{*}(s,a)$
    are obtained by solving the following Bellman optimality equations [[14](#bib.bib14),
    [22](#bib.bib22), [15](#bib.bib15)]:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值函数 $V^{*}(s)$ 和状态-动作函数 $Q^{*}(s,a)$ 通过解以下贝尔曼最优性方程获得 [[14](#bib.bib14), [22](#bib.bib22),
    [15](#bib.bib15)]：
- en: '|  | $V^{*}(s)=\underset{a_{t}}{\text{max}}\Big{[}r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}V^{*}(s_{t+1})\Big{]},$
    |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{*}(s)=\underset{a_{t}}{\text{max}}\Big{[}r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}V^{*}(s_{t+1})\Big{]},$
    |  |'
- en: '|  | $Q^{*}(s,a)=r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}\left[\underset{a_{t+1}}{\text{max}}Q^{*}(s_{t+1},a_{t+1})\right].$
    |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s,a)=r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}\left[\underset{a_{t+1}}{\text{max}}Q^{*}(s_{t+1},a_{t+1})\right].$
    |  |'
- en: Recall that the main goal of MDP is to obtain the optimal policy $\pi^{*}$ (i.e.,
    mapping states to optimum actions), which is given by $\pi^{*}=\underset{\pi}{\text{argmax}}~{}{\cal
    R}=\underset{\pi}{\text{argmax}}~{}=\mathbb{E}\left[\sum_{t=1}^{T}{\gamma r_{t}(s_{t},\pi(s_{t}))}\right]$.
    Hence, the optimal actions can be obtained to be the ones that maximize the above
    value functions, and the optimal policy will be the one that maximizes these values
    functions [[14](#bib.bib14)]. In particular, the $Q$-function $Q^{\pi}(s,a)$ is
    commonly used, and the problem of obtaining the optimal policy becomes $\pi^{*}(s)=\underset{a}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$.
    The ultimate goal of all the value-based DRL algorithm is to approximate this
    function as discussed in the following subsections.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，MDP 的主要目标是获得最优策略 $\pi^{*}$（即将状态映射到最佳动作），其表示为 $\pi^{*}=\underset{\pi}{\text{argmax}}~{}{\cal
    R}=\underset{\pi}{\text{argmax}}~{}=\mathbb{E}\left[\sum_{t=1}^{T}{\gamma r_{t}(s_{t},\pi(s_{t}))}\right]$。因此，可以通过最大化上述值函数来获得最优动作，最优策略将是最大化这些值函数的策略
    [[14](#bib.bib14)]。特别地，$Q$-函数 $Q^{\pi}(s,a)$ 常被使用，获得最优策略的问题变为 $\pi^{*}(s)=\underset{a}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$。所有基于值的
    DRL 算法的最终目标是近似这个函数，如下文小节中所讨论。
- en: III-B1 $Q$-Learning Technique
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 $Q$-学习技术
- en: 'In RL, $Q$-learning is one of the most widely used algorithms to address MDPs.
    It obtains the optimal values of the $Q$-function iteratively using the following
    Bellman equation updating rule:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 中，$Q$-学习是解决 MDP 的最广泛使用的算法之一。它通过以下贝尔曼方程更新规则迭代获得 $Q$-函数的最优值。
- en: '|  | $\begin{split}Q(s_{t},a_{t})&amp;=Q(s_{t},a_{t})+\\ &amp;\alpha_{t}\left[r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})\right]\end{split}$
    |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Q(s_{t},a_{t})&=Q(s_{t},a_{t})+\\ &\alpha_{t}\left[r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})\right]\end{split}$
    |  |'
- en: where $\alpha_{t}$ is the learning rate that defines how much the new information
    contributes to the existing $Q$-value. The main idea of this Bellman rule relies
    on finding the Temporal Difference (TD) between the current $Q$-value ($Q(s_{t},a_{t})$)
    and the predicted $Q$-value ($r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})$).
    The $Q$-learning algorithm uses this Bellman equation to construct a table of
    all possible $Q$ values for each stat-action pair. The algorithm terminates when
    we reach a certain number of iterations or when all Q-values have converged. In
    such a case, the optimal policy will determine the optimal action to take at each
    state such that $Q^{\pi^{*}}(s_{t},a_{t})$ is maximized for all states in the
    state space, i.e., $\pi^{*}=\underset{a_{t+1}}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$.
    This optimal policy says that at any state we take the action that will eventually
    obtain the highest cumulative reward.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{t}$ 是学习率，它定义了新信息对现有 $Q$ 值的贡献程度。这个 Bellman 规则的主要思想是找到当前 $Q$ 值 ($Q(s_{t},a_{t})$)
    与预测的 $Q$ 值 ($r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})$)
    之间的时间差（TD）。$Q$-学习算法使用这个 Bellman 方程来构建每个状态-动作对的所有可能 $Q$ 值的表格。当我们达到一定次数的迭代或所有 $Q$
    值都已收敛时，算法终止。在这种情况下，最优策略将确定在每个状态下采取的最优动作，以使 $Q^{\pi^{*}}(s_{t},a_{t})$ 在状态空间中的所有状态中最大化，即
    $\pi^{*}=\underset{a_{t+1}}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$。这一最优策略表示在任何状态下，我们采取的行动将最终获得最高的累积奖励。
- en: However, the $Q$-learning algorithm has many limitations when applied for RRAM
    in modern wireless networks. First, it is applicable only to problems with low
    dimensionality of both state and action spaces, making it unscalable. Second,
    it is applicable only on RRAM with discrete state space and action space, such
    as channel access and RANs assignment. If, however, they are applied to problems
    with continuous action space nature, such as power allocation, the action space
    must be digitized. This renders them inaccurate due to the quantization error.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，$Q$-学习算法在现代无线网络中应用于 RRAM 时存在许多局限性。首先，它仅适用于状态和动作空间维度较低的问题，这使得它不可扩展。其次，它仅适用于具有离散状态空间和动作空间的
    RRAM，如信道访问和 RAN 分配。然而，如果它们应用于具有连续动作空间性质的问题，如功率分配，则动作空间必须被离散化。这使得它们由于量化误差变得不准确。
- en: III-B2 Deep Q Network (DQN) Technique
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 深度 Q 网络（DQN）技术
- en: 'Since the $Q$-learning algorithm relies on building a table for the $Q$ values,
    it will fail to obtain the optimal policy when the state space and action space
    become prohibitively large. This case is commonly encountered in the RRAM problems
    of modern wireless systems. To overcome this issue, the DQN algorithm has been
    developed , which inherits the advantages of $Q$-learning and DL techniques. The
    main idea is to replace the table in the $Q$-learning algorithms with a DNN that
    tries to approximate the $Q$ values. Hence, the DNN is also called the function
    approximator and denoted as $Q(s_{t},a_{t}|\theta)$, where $\theta$ represents
    the training parameters (i.e., weights) of the DNN. Fig. [9](#S3.F9 "Figure 9
    ‣ III-B2 Deep Q Network (DQN) Technique ‣ III-B Value-Based Algorithms ‣ III Overview
    of DRL Techniques Used for RRAM ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") shows the DQN architecture. The replay memory is denoted by $\mathcal{D}$,
    and it is mainly used to break the correlation between the training samples or
    transitions, i.e., ($s_{t},a_{t},r_{t},s_{t+1}$), by making them independently
    and identically distributed i.i.d. During the learning process of the policy,
    we store the training transitions that are generated during the interaction with
    the wireless environment in $\mathcal{D}$. The DQN’s agent will then randomly
    select minibatch samples of transitions from $\mathcal{D}$ to train its DNN. To
    enhance the stability of the DQN model, the target $Q$ network is used, whose
    weights will be periodically updated to track those of the main $Q$ network.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $Q$-学习算法依赖于为 $Q$ 值构建表格，当状态空间和动作空间变得过于庞大时，它将无法获得最优策略。这种情况在现代无线系统的 RRAM 问题中较为常见。为了克服这一问题，开发了
    DQN 算法，它继承了 $Q$-学习和深度学习技术的优点。其主要思想是用一个 DNN 代替 $Q$-学习算法中的表格，以近似 $Q$ 值。因此，DNN 也称为函数逼近器，表示为
    $Q(s_{t},a_{t}|\theta)$，其中 $\theta$ 表示 DNN 的训练参数（即权重）。图 [9](#S3.F9 "图 9 ‣ III-B2
    深度 Q 网络 (DQN) 技术 ‣ III-B 基于价值的算法 ‣ III 深度强化学习技术概述 ‣ 下一代异构无线网络中的无线资源分配与管理的深度强化学习：综述")
    显示了 DQN 架构。回放记忆表示为 $\mathcal{D}$，主要用于打破训练样本或过渡之间的相关性，即通过使其独立同分布 i.i.d.。在策略学习过程中，我们将与无线环境交互时生成的训练过渡存储在
    $\mathcal{D}$ 中。然后，DQN 的智能体将从 $\mathcal{D}$ 中随机选择小批量过渡样本来训练其 DNN。为了提高 DQN 模型的稳定性，使用了目标
    $Q$ 网络，其权重将定期更新以跟踪主 $Q$ 网络的权重。
- en: 'Since the DQN algorithm is mainly used to learn the optimal policy, i.e., $\pi^{*}=\underset{a}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$,
    the optimal $Q$-function is derived from the following iterative Bellman equation:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DQN 算法主要用于学习最优策略，即 $\pi^{*}=\underset{a}{\text{argmax}}~{}Q^{\pi^{*}}(s_{t},a_{t})$，因此最优的
    $Q$-函数源自以下迭代贝尔曼方程：
- en: '|  | $Q(s_{t},a_{t})=r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t}),$
    |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=r_{t}(s_{t},a_{t})+\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t}),$
    |  |'
- en: and the DQN algorithm is then optimized by iteratively updating the weights
    $\theta$ of its DNN to minimize the following Bellman loss function;
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过迭代更新其 DNN 的权重 $\theta$ 来优化 DQN 算法，以最小化以下贝尔曼损失函数；
- en: '|  | $\begin{split}L(\theta_{t})&amp;=\mathbb{E}_{{s_{t},a_{t},r_{t},s_{t+1}}\in\mathcal{D}}\Big{[}r_{t}(s_{t},a_{t})+\\
    &amp;\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t}&#124;\theta^{{}^{\prime}})-Q(s_{t},a_{t}&#124;\theta)\Big{]}^{2}.\end{split}$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L(\theta_{t})&amp;=\mathbb{E}_{{s_{t},a_{t},r_{t},s_{t+1}}\in\mathcal{D}}\Big{[}r_{t}(s_{t},a_{t})+\\
    &amp;\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t}&#124;\theta^{{}^{\prime}})-Q(s_{t},a_{t}&#124;\theta)\Big{]}^{2}.\end{split}$
    |  |'
- en: where $\theta^{{}^{\prime}}$ is the weights of the target $Q$ network.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta^{{}^{\prime}}$ 是目标 $Q$ 网络的权重。
- en: 'The DQN algorithm is applicable to a wide variety of RRAM problems, specifically
    for problems characterized by their discrete action space. As we will elaborate
    in-depth in Section [IV](#S4 "IV DRL-Based Radio Resource Allocation and Management
    for Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey"), the DQN technique can be used efficiently for channel allocation,
    access control, spectrum access, user association, and RANs assignment. The DQN
    algorithm can also be used for RRAM problems with continuous action space, such
    as power control, by discretizing the action space. However, such a methodology
    makes DQN vulnerable to serious quantization error that may considerably deteriorate
    its accuracy. There are also other limitations in the basic DQN, and various $Q$-learning
    algorithms have been proposed to overcome them, as we discuss in the following
    sections.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'DQN 算法适用于各种 RRAM 问题，特别是那些具有离散动作空间的问题。正如我们将在第 [IV](#S4 "IV DRL-Based Radio Resource
    Allocation and Management for Next Generation Wireless Networks ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey") 节中深入探讨的那样，DQN 技术可以高效地用于频道分配、访问控制、频谱访问、用户关联和 RAN
    分配。DQN 算法也可以通过离散化动作空间来处理具有连续动作空间的 RRAM 问题，如功率控制。然而，这种方法使得 DQN 易受严重的量化误差影响，这可能会显著降低其准确性。基本
    DQN 还有其他限制，我们将在以下章节中讨论各种 $Q$-学习算法来克服这些限制。'
- en: '![Refer to caption](img/a5e63ab9a41fcae511756730dd7a20ff.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a5e63ab9a41fcae511756730dd7a20ff.png)'
- en: 'Figure 9: Illustration of the DQN architecture.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：DQN 架构示意图。
- en: III-B3 Double DQN Algorithm
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 双重 DQN 算法
- en: The Double DQN technique has been proposed in [[69](#bib.bib69)] to enhance
    the basic DQN algorithm. The DQN algorithm tends to overestimate the $Q$ values,
    which can degrade the training process and lead to suboptimal policies. The overestimation
    results from the positive bias caused by the max operation employed in the Bellman
    equation. Specifically, the root cause is that the same training transitions are
    utilized in selecting and evaluating an action. As a solution to this problem,
    the authors in [[69](#bib.bib69)] propose to use two $Q$ value functions, one
    for selecting the best action and the other to evaluate the best action. The action
    selection is still based on the online weights parameters $\theta$, while the
    second weights parameters $\theta^{{}^{\prime}}$ are used to evaluate the value
    of this policy. So, as in the conventional $Q$ learning, the value of the policy
    is still estimated based on the current $Q$ values. The weights parameters $\theta^{{}^{\prime}}$
    are updated via switching between $\theta$ and $\theta^{{}^{\prime}}$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 双重 DQN 技术在 [[69](#bib.bib69)] 中被提出，以增强基本的 DQN 算法。DQN 算法倾向于高估 $Q$ 值，这可能会降低训练过程并导致次优策略。高估的原因是
    Bellman 方程中的最大操作引起的正偏差。具体来说，根本原因是相同的训练过渡在选择和评估动作时被使用。作为解决方案，[[69](#bib.bib69)]
    的作者建议使用两个 $Q$ 值函数，一个用于选择最佳动作，另一个用于评估最佳动作。动作选择仍然基于在线权重参数 $\theta$，而第二个权重参数 $\theta^{{}^{\prime}}$
    用于评估该策略的值。因此，与传统的 $Q$ 学习一样，策略的值仍然基于当前的 $Q$ 值进行估计。权重参数 $\theta^{{}^{\prime}}$ 通过在
    $\theta$ 和 $\theta^{{}^{\prime}}$ 之间切换来更新。
- en: 'Hence, the target $Q$ values are derived from the following modified Bellman
    equation [[69](#bib.bib69)]:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标 $Q$ 值是根据以下修改的 Bellman 方程推导出的 [[69](#bib.bib69)]：
- en: '|  | $Q(s_{t},a_{t})=r_{t}(s_{t},a_{t})+\gamma Q(s_{t+1},\underset{a_{t+1}}{\text{argmax}}Q(s_{t+1},a_{t}&#124;\theta_{t}),\theta_{t}^{{}^{\prime}}),$
    |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=r_{t}(s_{t},a_{t})+\gamma Q(s_{t+1},\underset{a_{t+1}}{\text{argmax}}Q(s_{t+1},a_{t}&#124;\theta_{t}),\theta_{t}^{{}^{\prime}}),$
    |  |'
- en: and the Double DQN algorithm uses the following modified Bellman loss function
    to update its weights;
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 双重 DQN 算法使用以下修改的 Bellman 损失函数来更新其权重；
- en: '|  | $\begin{split}L(\theta_{t})&amp;=\mathbb{E}_{{s_{t},a_{t},r_{t},s_{t+1}}\in\mathcal{D}}[r_{t}(s_{t},a_{t})+\\
    &amp;\gamma Q(s_{t+1},\underset{a_{t+1}}{\text{argmax}}Q(s_{t+1},a_{t}&#124;\theta_{t}),\theta_{t}^{{}^{\prime}})-Q(s_{t},a_{t}&#124;\theta_{t})]^{2}.\end{split}$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L(\theta_{t})&amp;=\mathbb{E}_{{s_{t},a_{t},r_{t},s_{t+1}}\in\mathcal{D}}[r_{t}(s_{t},a_{t})+\\
    &amp;\gamma Q(s_{t+1},\underset{a_{t+1}}{\text{argmax}}Q(s_{t+1},a_{t}&#124;\theta_{t}),\theta_{t}^{{}^{\prime}})-Q(s_{t},a_{t}&#124;\theta_{t})]^{2}.\end{split}$
    |  |'
- en: The Double DQN algorithm is also widely used in RRAM problems, as we will discuss
    in the next section. Although this algorithm has advantages over the basic DQN
    algorithm, they both share the same shortcomings.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN 算法在 RRAM 问题中也被广泛使用，正如我们将在下一节讨论的那样。尽管该算法相较于基础 DQN 算法有其优势，但它们也共享相同的缺点。
- en: III-B4 Dueling DQN Algorithm
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 对战 DQN 算法
- en: 'This algorithm is another enhancement to the basic DQN proposed in [[70](#bib.bib70)].
    Recall that the goal of the network is to estimate the $Q$ values, i.e., $Q(s_{t},a_{t})$.
    This function can be divided into two terms; the state-value function $V(s)$,
    which tells the importance of being in a particular state, and the action-value
    function (or the advantage function) $A(s,a)$, which tells the importance of selecting
    a particular action among all available actions. Hence, the $Q$ value function
    can be written as $Q(s,a)=V(s)+A(s,a)$. The authors in [[70](#bib.bib70)] utilized
    this concept and suggested having two main independent paths of fully-connected
    layers instead of having only a single path as the case in the basic DQN. One
    path will estimate $V(s)$, and the other path will estimate $A(s,a)$. The two
    paths will eventually be combined to produce one single output, which is $Q(s,a)$
    as [[70](#bib.bib70)]:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法是对基本 DQN 的另一种改进，提议于 [[70](#bib.bib70)]。回顾网络的目标是估计 $Q$ 值，即 $Q(s_{t},a_{t})$。该函数可以分为两个部分：状态价值函数
    $V(s)$，它表示处于某一状态的重要性，以及动作价值函数（或优势函数）$A(s,a)$，它表示在所有可用动作中选择特定动作的重要性。因此，$Q$ 值函数可以写成
    $Q(s,a)=V(s)+A(s,a)$。[[70](#bib.bib70)] 的作者利用了这一概念，建议拥有两个主要独立的全连接层路径，而不是基础 DQN
    中的单一路径。一条路径将估计 $V(s)$，另一条路径将估计 $A(s,a)$。这两条路径最终将结合生成一个单一的输出，即 $Q(s,a)$ 如 [[70](#bib.bib70)]
    所述。
- en: '|  | $\begin{split}Q(s_{t},a_{t};\theta,\alpha,\beta)&amp;=V(s_{t};\theta,\beta)+\\
    &amp;\left(A(s_{t},a_{t};\theta,\alpha)-\frac{1}{\mathcal{A}}\sum_{a_{t+1}}{A(s_{t},a_{t+1};\theta,\alpha)}\right)\end{split}$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Q(s_{t},a_{t};\theta,\alpha,\beta)&amp;=V(s_{t};\theta,\beta)+\\
    &amp;\left(A(s_{t},a_{t};\theta,\alpha)-\frac{1}{\mathcal{A}}\sum_{a_{t+1}}{A(s_{t},a_{t+1};\theta,\alpha)}\right)\end{split}$
    |  |'
- en: where $\mathcal{A}$ is the number of actions in the action space, and $\beta$
    and $\alpha$ are the weights of the fully-connected layers of the two paths $V(s_{t};\theta,\beta)$
    and $A(s_{t},a_{t};\theta,\alpha)$, respectively. Here, the loss function is obtained
    similar to the DQN and Double DQN algorithms.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{A}$ 是动作空间中的动作数量，$\beta$ 和 $\alpha$ 分别是两个路径 $V(s_{t};\theta,\beta)$
    和 $A(s_{t},a_{t};\theta,\alpha)$ 的全连接层的权重。这里，损失函数的获取方式类似于 DQN 和 Double DQN 算法。
- en: III-C Policy-Based Algorithm
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于策略的算法
- en: The policy-based techniques are part of the policy gradient family of methods.
    They provide an alternative way to solve the MDP problems having high dimensionality
    and continuous action spaces. Recall that the main idea of the value-based methods
    discussed in the previous subsection is the state-action value function $Q(s,a)$.
    This function is defined as the expected total discounted reward received by taking
    a particular action from the state. If these $Q$ values are known, the optimal
    policy is defined by selecting actions that maximize the $Q$ values in each state.
    However, in environments with continuous action spaces, such as power control
    in wireless systems, the $Q$ function cannot be obtained as it is impossible to
    conduct a full search in a continuous action space to obtain the optimal action.
    Hence value-based approaches inapplicable to problems characterized by their continuous
    action space, and the policy-based methods are applied instead.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的技术是策略梯度方法家族的一部分。它们提供了解决具有高维度和连续动作空间的 MDP 问题的替代方法。回顾前述小节讨论的基于价值的方法的主要思想是状态-动作价值函数
    $Q(s,a)$。该函数定义为从某一状态采取特定动作所获得的期望总折扣奖励。如果这些 $Q$ 值已知，则通过选择在每个状态中最大化 $Q$ 值的动作来定义最优策略。然而，在具有连续动作空间的环境中，如无线系统中的功率控制，由于无法在连续动作空间中进行全面搜索以获得最优动作，因此
    $Q$ 函数无法获得。因此，基于价值的方法不适用于连续动作空间的问题，而应使用基于策略的方法。
- en: 'In policy-based approaches [[71](#bib.bib71), [14](#bib.bib14)], we avoid calculating
    $Q$ values and directly obtain the optimal policy $\pi_{\theta}(a|s)$ that maximizes
    the agent’s expected accumulated reward $J$, i.e., $J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t})\right]$.
    The policy gradient approaches learn the optimal network weights $\theta^{*}$
    via performing gradient ascent on the function $J$. In particular, the policy
    gradients are derived from trajectories obtained via the current policy, and they
    are given by [[71](#bib.bib71)]:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法 [[71](#bib.bib71), [14](#bib.bib14)] 中，我们避免计算 $Q$ 值，直接获得最大化代理预期累计奖励
    $J$ 的最优策略 $\pi_{\theta}(a|s)$，即 $J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}(s_{t},a_{t})\right]$。策略梯度方法通过对函数
    $J$ 执行梯度上升来学习最优的网络权重 $\theta^{*}$。特别地，策略梯度是从当前策略获得的轨迹中推导出来的，它们由 [[71](#bib.bib71)]
    给出：
- en: '|  | $\begin{split}\triangledown_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T}{\triangledown\log\pi_{\theta}(a_{t}&#124;s_{t})Q^{\pi_{\theta}}(s_{t},a_{t})}\right].\end{split}$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\triangledown_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T}{\triangledown\log\pi_{\theta}(a_{t}\mid
    s_{t})Q^{\pi_{\theta}}(s_{t},a_{t})}\right].\end{split}$ |  |'
- en: 'In each gradient update, the agent interacts with the environment to collect
    new and fresh trajectories, and this is why policy-gradient methods are called
    on-policy algorithms. In this formula, the function $Q^{\pi_{\theta}}(s_{t},a_{t})$
    is unknown, and some of the algorithms used to estimate it are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次梯度更新中，代理与环境交互以收集新的和新鲜的轨迹，这也是为什么策略梯度方法被称为 on-policy 算法。在这个公式中，函数 $Q^{\pi_{\theta}}(s_{t},a_{t})$
    是未知的，以下是一些用于估计它的算法：
- en: III-C1 REINFORCE Algorithm
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 REINFORCE 算法
- en: The main idea of this algorithm is to increase the probabilities of good actions
    and reduce the probabilities of bad actions. In general, the REINFORCE algorithm
    differs from the $Q$ learning methods in three core aspects. First, REINFORCE
    algorithm does not need a replay buffer $\mathcal{D}$ during training as it belongs
    to the on-policy family, which requires only fresh training transitions. Although
    this enhances its convergence speed, it needs much more interaction with the environment.
    Second, the REINFORCE algorithm implicitly performs the exploration process, as
    it depends on the probabilities returned by the network, which incorporate uniform
    random agent behavior. Third, no target network is required in the REINFORCE algorithm
    as the $Q$ values are obtained from the experiences in the environment.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的主要思想是增加好动作的概率，并减少坏动作的概率。通常，REINFORCE 算法与 $Q$ 学习方法在三个核心方面有所不同。首先，REINFORCE
    算法在训练过程中不需要重放缓存 $\mathcal{D}$，因为它属于 on-policy 家族，仅需新鲜的训练转移。尽管这提高了其收敛速度，但它需要更多的环境交互。其次，REINFORCE
    算法隐式执行探索过程，因为它依赖于网络返回的概率，这些概率包含均匀随机的代理行为。第三，REINFORCE 算法不需要目标网络，因为 $Q$ 值是从环境经验中获得的。
- en: 'The weights $\theta$ of the network in the REINFORCE algorithm are updated
    to minimize the following loss function:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法中网络的权重 $\theta$ 被更新以最小化以下损失函数：
- en: '|  | $\begin{split}L=-\sum_{t=0}^{T}{Q^{\pi_{\theta}}(s_{t},a_{t})\log\pi_{\theta}(a_{t}&#124;s_{t})}.\end{split}$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L=-\sum_{t=0}^{T}{Q^{\pi_{\theta}}(s_{t},a_{t})\log\pi_{\theta}(a_{t}\mid
    s_{t})}.\end{split}$ |  |'
- en: The disadvantage of the REINFORCE algorithm is that it suffers from high variance,
    meaning that any small shift in the return leads to a different policy. This limitation
    motivated the actor-critic algorithms.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法的缺点是其方差较高，这意味着回报的任何小变化都会导致不同的策略。这一限制促使了 actor-critic 算法的出现。
- en: III-C2 Actor-Critic Algorithm
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 Actor-Critic 算法
- en: The actor-critic methods are mainly developed to enhance the convergence speed
    and stability (i.e., reducing the variance) of the policy-gradient method. Like
    the policy-based methods, it utilizes the accumulated discounted reward $J$ to
    obtain the gradient of the policy $\triangledown J$, which provides the direction
    that enhances the policy. This algorithm learns a critic to reduce the variance
    of gradient estimates since it utilizes various samples, whereas the REINFORCE
    algorithm utilizes only a single sample trajectory.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic 方法主要是为了提高策略梯度方法的收敛速度和稳定性（即，减少方差）。与基于策略的方法类似，它利用累计的折扣奖励 $J$ 来获得策略的梯度
    $\triangledown J$，从而提供增强策略的方向。该算法学习一个评价器来减少梯度估计的方差，因为它利用了各种样本，而 REINFORCE 算法只利用单一的样本轨迹。
- en: To select the best action in any state, the total discount reward of the action
    is used, i.e., $Q(s,a)$. The total reward can be decomposed into state-value function
    $V(s)$ and advantage function $A(s,a)$, i.e., as $Q(s,a)=V(s)+A(s,a)$. So, another
    DNN is utilized to estimate $V(s)$, which is trained based on the Bellman equation.
    The estimated $V(s)$ is then leveraged to obtain the policy gradient and update
    the policy network such that the probabilities of actions with good advantage
    values are increased. Hence, the actor is the policy network $\pi(a|s)$ that takes
    actions by returning the probability distribution of actions, while the critic
    network evaluates the quality of the taken actions, $V(s)$. This algorithm is
    also called the advantage actor-critic method (A2C).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在任何状态下选择最佳动作，使用动作的总折扣奖励，即 $Q(s,a)$。总奖励可以分解为状态值函数 $V(s)$ 和优势函数 $A(s,a)$，即 $Q(s,a)=V(s)+A(s,a)$。因此，另一个
    DNN 用于估计 $V(s)$，该网络基于贝尔曼方程进行训练。估计的 $V(s)$ 然后被用来获取策略梯度并更新策略网络，以增加具有良好优势值的动作的概率。因此，演员是返回动作概率分布的策略网络
    $\pi(a|s)$，而评论员网络评估所采取动作的质量，即 $V(s)$。该算法也称为优势演员评论员方法（A2C）。
- en: 'In the A2C algorithm, the weights of the actor network $\theta_{\pi}$ and critic
    network $\theta_{\text{v}}$ are updated using the accumulated policy gradients
    $\partial\theta_{\pi}$ and value gradients $\partial\theta_{\text{v}}$, respectively,
    according to the following formulas [[72](#bib.bib72)]:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在 A2C 算法中，演员网络的权重 $\theta_{\pi}$ 和评论员网络的权重 $\theta_{\text{v}}$ 分别使用累计策略梯度 $\partial\theta_{\pi}$
    和价值梯度 $\partial\theta_{\text{v}}$ 进行更新，更新公式如下 [[72](#bib.bib72)]：
- en: '|  | $\begin{split}\partial\theta_{\pi}\leftarrow\partial\theta_{\pi}+\triangledown_{\theta}\log\pi_{\theta}(a_{t}&#124;s_{t})(R-V_{\theta}(s_{t})),\\
    \partial\theta_{\text{v}}\leftarrow\partial\theta_{\text{v}}+\frac{\partial\left(R-V_{\theta}(s_{t})\right)^{2}}{\partial\theta_{\text{v}}}.\end{split}$
    |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\partial\theta_{\pi}\leftarrow\partial\theta_{\pi}+\triangledown_{\theta}\log\pi_{\theta}(a_{t}&#124;s_{t})(R-V_{\theta}(s_{t})),\\
    \partial\theta_{\text{v}}\leftarrow\partial\theta_{\text{v}}+\frac{\partial\left(R-V_{\theta}(s_{t})\right)^{2}}{\partial\theta_{\text{v}}}.\end{split}$
    |  |'
- en: where $R\leftarrow r_{t}(s_{t},a_{t})+R$. The weights are updated to move in
    the direction of the policy gradients and in the opposite direction of the value
    gradients.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R\leftarrow r_{t}(s_{t},a_{t})+R$。权重被更新以朝向策略梯度的方向，并在价值梯度的相反方向移动。
- en: III-C3 A3C Algorithm
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 A3C 算法
- en: 'The asynchronous advantage actor-critic (A3C) algorithm is an extension of
    the basic A2C [[72](#bib.bib72)]. This algorithm is used to solve the high variance
    issue in gradients that results in non-optimal policies. A3C algorithm conducts
    a parallel implementation of the actor-critic algorithm, where the actor and critic
    share the network layers. A global NN is trained to output action probabilities
    and an estimate of the advantage function $A(s_{t},a_{t}|\theta_{\pi},\theta_{\text{v}})$
    given by $\sum_{i=0}^{k-1}\gamma^{i}r_{t+1}+\gamma^{k}V(s_{t+k}|\theta_{\text{v}})-V(s_{t}|\theta_{\text{v}})$,
    where $k$ depends on the state and upper-bounded by the maximum number of time
    steps. The update rule in A3C is given by [[72](#bib.bib72)]:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 异步优势演员评论员（A3C）算法是基本 A2C 的扩展 [[72](#bib.bib72)]。该算法用于解决梯度中的高方差问题，这会导致非最优策略。A3C
    算法并行实现了演员-评论员算法，其中演员和评论员共享网络层。一个全局 NN 被训练以输出动作概率和优势函数 $A(s_{t},a_{t}|\theta_{\pi},\theta_{\text{v}})$
    的估计值，由 $\sum_{i=0}^{k-1}\gamma^{i}r_{t+1}+\gamma^{k}V(s_{t+k}|\theta_{\text{v}})-V(s_{t}|\theta_{\text{v}})$
    给出，其中 $k$ 依赖于状态并且上限为最大时间步数。A3C 中的更新规则如下 [[72](#bib.bib72)]：
- en: '|  | $\begin{split}\triangledown_{\theta_{\pi}^{{}^{\prime}}}\log\pi(a_{t}&#124;s_{t};\theta^{{}^{\prime}})A(s_{t},a_{t}&#124;\theta_{\pi},\theta_{\text{v}}).\end{split}$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\triangledown_{\theta_{\pi}^{{}^{\prime}}}\log\pi(a_{t}&#124;s_{t};\theta^{{}^{\prime}})A(s_{t},a_{t}&#124;\theta_{\pi},\theta_{\text{v}}).\end{split}$
    |  |'
- en: where $\theta_{\pi}^{{}^{\prime}}$ is thread-specific weights.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{\pi}^{{}^{\prime}}$ 是特定线程的权重。
- en: Several parallel actor learners are instantiated with copies of both the environment
    and global NN weights. Each learner independently interacts with its environment
    and gathers training transitions to derive the gradients with respect to its NN
    weights. Learners will then propagate their gradients to the global NN to update
    its weights. This mechanism ensures a periodic update of the global model with
    diverse transitions from each learner.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化了多个并行的 actor 学习者，每个学习者都拥有环境和全局神经网络（NN）权重的副本。每个学习者独立地与其环境互动，并收集训练过渡来推导相对于其
    NN 权重的梯度。学习者随后将其梯度传播到全局 NN 以更新其权重。这个机制确保了全局模型定期更新，并且每个学习者提供的过渡是多样的。
- en: III-C4 Deep Deterministic Policy Gradient (DDPG) Algorithm
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C4 深度确定性策略梯度（DDPG）算法
- en: DDPG is one of the most widely used DRL techniques in addressing RRAM problems
    for wireless networks characterized by their high dimensionality and continuous
    action space [[73](#bib.bib73)]. DDPG algorithm belongs to the actor-critic family,
    and it combines both $Q$-learning and policy gradients algorithms. It consists
    of actor and critic networks. The actor network takes the state as its input,
    and it outputs the exact ”deterministic” action, not probability distribution
    over actions as in the actor-critic algorithm. Whereas the critic is a $Q$-value
    network that takes both the state and action as inputs, and it outputs the $Q$-value
    as a single output.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是解决无线网络中高维和连续动作空间的 RRAM 问题的最广泛使用的深度强化学习（DRL）技术之一 [[73](#bib.bib73)]。DDPG
    算法属于 actor-critic 家族，它结合了 $Q$-学习和策略梯度算法。它由 actor 和 critic 网络组成。actor 网络将状态作为输入，并输出精确的
    “确定性” 动作，而不是像 actor-critic 算法中的动作概率分布。critic 则是一个 $Q$-值网络，它同时接收状态和动作作为输入，并输出一个
    $Q$-值作为单一输出。
- en: The deterministic policy gradient (DPG) algorithm is proposed in [[74](#bib.bib74)]
    to overcome the limitation caused by the $max$ operator in the $Q$-learning algorithm,
    i.e., $\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t})$. It simultaneously learns
    both the $Q$-function and the policy. In particular, the DPG algorithm has a parameterized
    actor function $\mu(s|\theta^{\mu})$ with weights $\theta$, which learns the deterministic
    policy that gives the optimal action corresponding to $\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t})$.
    The critic $Q(s,a)$ is leaned via minimizing the Bellman loss function as in the
    $Q$-learning algorithm.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略梯度（DPG）算法在 [[74](#bib.bib74)] 中被提出，以克服 $Q$-学习算法中 $max$ 操作符的限制，即 $\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t})$。它同时学习
    $Q$-函数和策略。特别地，DPG 算法有一个参数化的 actor 函数 $\mu(s|\theta^{\mu})$，其权重为 $\theta$，它学习给出对应于
    $\underset{a_{t+1}}{\text{max}}Q(s_{t+1},a_{t})$ 的最优动作的确定性策略。critic $Q(s,a)$ 通过最小化
    Bellman 损失函数来学习，就像 $Q$-学习算法中一样。
- en: 'The learning process of the actor policy is updated using gradient ascent with
    respect to $\theta^{\mu}$ in order to solve the objective given by the following
    chain rule [[74](#bib.bib74)]:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: actor 策略的学习过程通过相对于 $\theta^{\mu}$ 的梯度上升来更新，以解决由以下链式法则给出的目标 [[74](#bib.bib74)]：
- en: '|  | $\displaystyle J(\theta)=\mathbb{E}_{s\in\mathcal{D}}\Big{[}Q(s,\mu(s&#124;\theta^{\mu}))\Big{]},$
    |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\theta)=\mathbb{E}_{s\in\mathcal{D}}\Big{[}Q(s,\mu(s&#124;\theta^{\mu}))\Big{]},$
    |  |'
- en: '|  | $\displaystyle\triangledown_{\theta^{\mu}}J=\mathbb{E}_{s\in\mathcal{D}}\Big{[}\triangledown_{a}Q(s,a&#124;\theta^{Q})&#124;_{s=s_{t},a=\mu(s_{t})}\triangledown_{\theta^{\mu}}\mu(s&#124;\theta^{\mu})&#124;_{s=s_{t}}\Big{]}.$
    |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\triangledown_{\theta^{\mu}}J=\mathbb{E}_{s\in\mathcal{D}}\Big{[}\triangledown_{a}Q(s,a&#124;\theta^{Q})&#124;_{s=s_{t},a=\mu(s_{t})}\triangledown_{\theta^{\mu}}\mu(s&#124;\theta^{\mu})&#124;_{s=s_{t}}\Big{]}.$
    |  |'
- en: 'The DDPG algorithm proposed in [[73](#bib.bib73)] is built based on the DPG
    algorithm, where both the policy and critic are DNNs, as shown in Fig. [10](#S3.F10
    "Figure 10 ‣ III-C4 Deep Deterministic Policy Gradient (DDPG) Algorithm ‣ III-C
    Policy-Based Algorithm ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"). The DDPG algorithm creates a copy of both the actor
    and critic networks, $Q^{{}^{\prime}}(s,a|\theta^{Q^{{}^{\prime}}})$ and $\mu^{{}^{\prime}}(s|\theta^{\mu^{{}^{\prime}}})$,
    respectively, to compute the target values. The weights of these two target networks,
    $\theta^{Q^{{}^{\prime}}}$ and $\theta^{\mu^{{}^{\prime}}}$, are then updated
    to slowly track the weight of the learned network to provide more stable training
    using $\theta^{{}^{\prime}}\leftarrow\tau\theta+(1-\tau)\theta^{{}^{\prime}}$
    with $\tau\ll 1$. The critic network is updated to minimize the following Bellman
    loss function [[73](#bib.bib73)];'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[73](#bib.bib73)]中提出的DDPG算法是基于DPG算法构建的，其中策略和评论家都是深度神经网络，如图[10](#S3.F10 "Figure
    10 ‣ III-C4 Deep Deterministic Policy Gradient (DDPG) Algorithm ‣ III-C Policy-Based
    Algorithm ‣ III Overview of DRL Techniques Used for RRAM ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")所示。DDPG算法创建了演员和评论家网络的副本$Q^{{}^{\prime}}(s,a|\theta^{Q^{{}^{\prime}}})$和$\mu^{{}^{\prime}}(s|\theta^{\mu^{{}^{\prime}}})$来计算目标值。然后，这两个目标网络的权重$\theta^{Q^{{}^{\prime}}}$和$\theta^{\mu^{{}^{\prime}}}$会被更新，以缓慢追踪已学习网络的权重，从而提供更稳定的训练，使用$\theta^{{}^{\prime}}\leftarrow\tau\theta+(1-\tau)\theta^{{}^{\prime}}$，其中$\tau\ll
    1$。评论家网络会被更新以最小化以下贝尔曼损失函数[[73](#bib.bib73)]；'
- en: '|  | $\begin{split}L(\theta^{Q})&amp;=\mathbb{E}_{s_{t},a_{t},r_{t},s_{t+1}\in\mathcal{D}}\Bigg{[}\bigg{(}r_{t}(s_{t},a_{t})+\\
    &amp;\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},\mu(s_{t+1}&#124;\theta^{\pi^{{}^{\prime}}})&#124;\theta^{Q^{{}^{\prime}}})-Q(s_{t},a_{t}&#124;\theta^{Q})\bigg{)}^{2}\Bigg{]}.\end{split}$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L(\theta^{Q})&amp;=\mathbb{E}_{s_{t},a_{t},r_{t},s_{t+1}\in\mathcal{D}}\Bigg{[}\bigg{(}r_{t}(s_{t},a_{t})+\\
    &amp;\gamma\underset{a_{t+1}}{\text{max}}Q(s_{t+1},\mu(s_{t+1}&#124;\theta^{\pi^{{}^{\prime}}})&#124;\theta^{Q^{{}^{\prime}}})-Q(s_{t},a_{t}&#124;\theta^{Q})\bigg{)}^{2}\Bigg{]}.\end{split}$
    |  |'
- en: Note that the DDPG algorithm is off-policy, which means that we use a replay
    buffer $\mathcal{D}$ to store training transitions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DDPG算法是离线策略算法，这意味着我们使用回放缓冲区$\mathcal{D}$来存储训练过渡。
- en: The exploration-exploitation issue is addressed by adding the Ornstein–Uhlenbeck
    (OU) process or some Gaussian noise $\mathcal{N}$ to the action selected by the
    policy, i.e., $\mu(s_{t}|\theta_{t}^{\mu})+\varepsilon\mathcal{N}$ [[73](#bib.bib73)].
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在策略选择的动作中添加Ornstein–Uhlenbeck (OU)过程或一些高斯噪声$\mathcal{N}$来解决探索-利用问题，即$\mu(s_{t}|\theta_{t}^{\mu})+\varepsilon\mathcal{N}$[[73](#bib.bib73)]。
- en: '![Refer to caption](img/05b662c081ad23176b6e172a857df935.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05b662c081ad23176b6e172a857df935.png)'
- en: 'Figure 10: Illustration of the DDPG actor-critic architecture.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：DDPG演员-评论家架构的示意图。
- en: III-D Other DRL Algorithms
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 其他DRL算法
- en: The DRL algorithms discussed above are the commonly used approaches to address
    the problem of RRAM in wireless networks, as we will discuss in the next section.
    Although there are several other algorithms, they are rarely utilized for such
    types of problems. Therefore, they are not included in this article. However,
    generally speaking, all the other variants are mainly developed to enhance the
    performance of the basic algorithms discussed above. For completeness, this section
    highlights some of these variants for the interested reader.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论的DRL算法是解决无线网络中RRAM问题的常用方法，我们将在下一节中讨论。尽管还有其他几种算法，但它们在此类问题中的应用非常少。因此，它们未被包含在本文中。然而，一般来说，所有其他变体主要是为了提高上述基本算法的性能。为了完整性，本节将突出介绍一些感兴趣的读者可能会关注的这些变体。
- en: Other variants of the value-based algorithms are developed to enhance the performance
    of vanilla DQN algorithm in terms of stability, convergence speed, implementation
    complexity, sample/learning efficiency, etc. Such variants include prioritized
    experience replay DQN [[75](#bib.bib75)], distributed prioritized experience replay
    DQN [[76](#bib.bib76)], distributional DQN [[77](#bib.bib77)], Rainbow DQN [[78](#bib.bib78)],
    and recurrent DQN [[79](#bib.bib79)].
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的算法的其他变体是为了提高原始DQN算法在稳定性、收敛速度、实现复杂性、样本/学习效率等方面的性能。这些变体包括优先经验回放DQN[[75](#bib.bib75)]、分布式优先经验回放DQN[[76](#bib.bib76)]、分布式DQN[[77](#bib.bib77)]、Rainbow
    DQN[[78](#bib.bib78)]和递归DQN[[79](#bib.bib79)]。
- en: For the policy-based algorithms, several algorithms are envisioned to enhance
    the overestimation issue, such as the Twin Delayed DDPG (TD3) [[80](#bib.bib80)],
    enhance stability and robustness, such as the Soft Actor-Critic (SAC) [[81](#bib.bib81)],
    and to enhance stability, convergence, and sample efficiency, such as the distributed
    distributional DDPG (D4PG) [[82](#bib.bib82)].
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: III-E Multi-Agent DRL Algorithms
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-agent DRL (MADRL) is a natural generalization of the single-agent DRL
    that allows multiple agents to concurrently learn optimal policies based on their
    interactions with the environment and with each other. These agents can either
    be deployed cooperatively, in which all agents interact with each other to learn
    the same global policy, or non-cooperatively, in which each agent learns its own
    policy. MADRL provides several performance advantages over the single-agent case
    regarding the quality of the learned policies, convergence speed, etc. However,
    it encounters several challenges such as scalability, partial observability, and
    agents’ non-stationarity. Nguyen et al. [[83](#bib.bib83)] provide a survey on
    MADRL systems and their applications. Different methods are reviewed along with
    their advantages and disadvantages. In [[84](#bib.bib84)], the authors provide
    a selective overview of the theories and algorithms for MARL.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: MADRL is widely employed in addressing various problems in wireless networks.
    The authors in [[22](#bib.bib22)] provide an overview of the MADRL algorithms
    and highlight their applications in future wireless networks. The learning frameworks
    in MADRL are also investigated. The application of MARL in solving problems for
    vehicular networks is studied in [[85](#bib.bib85)]. In [[86](#bib.bib86)], an
    overview of the evolution of cooperative MARL algorithms is presented with an
    emphasis on distributed optimization.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Most of the RRAM problems in modern wireless networks are of a multi-agent nature
    [[22](#bib.bib22)]. Network entities such as user devices, BSs, and wireless APs
    can act as cooperative/non-cooperative multi-agents to learn optimal radio resource
    allocation policies and solve complex network optimization problems. For example,
    channel access control may be formulated as a MADRL problem in which each user
    device represents a learning agent that senses the radio channels and coordinates
    with other agents to avoid collisions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we discuss how RRAM problems in wireless communication
    networks are formulated and solved using these algorithms.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: IV DRL-Based Radio Resource Allocation and Management for Next Generation Wireless
    Networks
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides an extensive and in-depth review of the related works
    for RRAM using DRL techniques. We classify them based on the radio resources (or
    issues) they investigate as well as based on the wireless network types they cover,
    as shown in Figs. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") and [3](#S1.F3 "Figure
    3 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning for
    Radio Resource Allocation and Management in Next Generation Heterogeneous Wireless
    Networks: A Survey"), respectively. It must be noted that this survey is dedicated
    to only the application of DRL algorithms for radio resources, i.e., no computation
    resources are covered, which can be found in [[15](#bib.bib15)].'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '本节对使用DRL技术进行RRAM的相关工作进行了广泛且深入的回顾。我们根据它们所研究的无线资源（或问题）以及它们覆盖的无线网络类型进行分类，如图[2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")和[3](#S1.F3 "Figure 3 ‣ I-C Paper Contributions ‣
    I Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation and
    Management in Next Generation Heterogeneous Wireless Networks: A Survey")所示。需要注意的是，本调查仅限于DRL算法在无线资源上的应用，即不包括计算资源，这些内容可以在[[15](#bib.bib15)]中找到。'
- en: 'DRL algorithms enable various network entities to efficiently learn the wireless
    networks, which allows them to make optimal control decisions that achieve some
    network utility function. For example, DRL methods can be deployed to maximize
    network sum-rate, minimize network energy consumption, or enhance spectral efficiency.
    In this section, we review the applications of DRL methods in the following RRAM
    issues: power allocation, spectrum allocation and access control, rate control,
    and the joint use of these radio resources.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: DRL算法使各种网络实体能够高效地学习无线网络，从而使它们能够做出最佳的控制决策，实现某些网络效用函数。例如，DRL方法可以用于最大化网络总速率、最小化网络能耗或提高频谱效率。在这一部分，我们回顾了DRL方法在以下RRAM问题中的应用：功率分配、频谱分配和访问控制、速率控制，以及这些无线资源的联合使用。
- en: IV-A DRL for Power Allocation
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A DRL用于功率分配
- en: 'Energy-efficient communication is one of the main objectives of modern wireless
    networks. It is achieved via efficient power allocation to ensure high QoS, better
    coverage, and enhanced data rate, as shown in Fig. [11](#S4.F11 "Figure 11 ‣ IV-A
    DRL for Power Allocation ‣ IV DRL-Based Radio Resource Allocation and Management
    for Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey"). Power allocation is mainly involved in vital network operations such
    as modulation and coding schemes, path loss compensation, interference management,
    etc. On the other hand, almost all modern user devices and IoT sensors are battery-powered
    with very limited battery capacity and charging capabilities. Hence, designing
    energy-efficient resource allocation schemes, protocols, and algorithms becomes
    fundamental in dynamic wireless network environments.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '能源高效通信是现代无线网络的主要目标之一。通过高效的功率分配来实现，以确保高QoS、更好的覆盖范围和增强的数据速率，如图[11](#S4.F11 "Figure
    11 ‣ IV-A DRL for Power Allocation ‣ IV DRL-Based Radio Resource Allocation and
    Management for Next Generation Wireless Networks ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")所示。功率分配主要涉及关键网络操作，如调制和编码方案、路径损耗补偿、干扰管理等。另一方面，几乎所有现代用户设备和物联网传感器都是电池供电的，电池容量和充电能力非常有限。因此，设计能源高效的资源分配方案、协议和算法在动态无线网络环境中变得尤为重要。'
- en: 'Several conventional approaches have been applied for power allocation and
    management. Most of them rely on solving power-constrained optimization problems,
    such as FP algorithm [[65](#bib.bib65)] and WMMSE algorithm [[87](#bib.bib87)].
    These approaches are iterative and model-driven, which means that they need a
    mathematically tractable and accurate model. They are typically executed in a
    centralized fashion in which a network controller has full CSI. In such a mechanism,
    BSs, wireless APs, and/or user devices require to wait until the centralized controller’s
    iterations converge and send the outcome back over backhaul links. However, as
    discussed in Section [II](#S2 "II Radio Resource Allocation and Management Techniques
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey"), such approaches become
    impractical due to the large-scale nature of modern wireless networks and the
    difficulty in obtaining accurate and instantaneous CSI. Hence, DRL techniques
    are used instead due to their superiority in obtaining optimal power allocation
    policies based on limited CSI.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '传统上，已经应用了几种功率分配和管理的方法。大多数方法依赖于解决功率约束优化问题，例如 FP 算法 [[65](#bib.bib65)] 和 WMMSE
    算法 [[87](#bib.bib87)]。这些方法是迭代式和模型驱动的，这意味着它们需要一个数学上可处理且准确的模型。它们通常以集中式方式执行，其中网络控制器拥有完整的
    CSI。在这种机制下，基站（BS）、无线接入点（AP）和/或用户设备需要等待集中式控制器的迭代收敛，然后通过回程链路发送结果。然而，如第[II](#S2 "II
    Radio Resource Allocation and Management Techniques ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")节所述，由于现代无线网络的大规模性质和获取准确且即时 CSI 的困难，这些方法变得不切实际。因此，使用
    DRL 技术，因为它们在基于有限 CSI 获取最佳功率分配策略方面具有优势。'
- en: '![Refer to caption](img/18dd674f7f79222315dbff26a56c65be.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18dd674f7f79222315dbff26a56c65be.png)'
- en: 'Figure 11: Importance of power allocation in modern wireless communication
    networks.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：现代无线通信网络中功率分配的重要性。
- en: IV-A1 In Cellular and HomNets
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 在蜂窝和 HomNets 中
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the power allocation problem in cellular, cellular IoT, and wireless homogeneous
    networks (HomNets) depicted in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions
    ‣ I Introduction ‣ Deep Reinforcement Learning for Radio Resource Allocation and
    Management in Next Generation Heterogeneous Wireless Networks: A Survey").'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的段落中，我们回顾了使用深度强化学习（DRL）算法来解决蜂窝网络、蜂窝物联网（IoT）和无线同质网络（HomNets）中的功率分配问题的工作，如图[2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")所示。'
- en: In [[88](#bib.bib88)], the authors propose a multi-agent $Q$-learning-based
    RL model to address the problem of downlink power control and rate adaptation
    in cellular systems. The agent is a network entity that resides in the cell, such
    as an eNB. The action space is discrete, corresponding to allocating downlink
    transmit power of the cell. The state space comprises four elements; cell power,
    average reference signal received power, average interference, and cell reward.
    The reward function is continuous, defined based on the $\alpha$-fair resource
    allocation utility function. It is defined by the radio measurement or performance
    indicator associated with all users in the radio cell. System-level simulations
    show that their proposed method quickly learns the power control policy to provide
    significant energy savings and fairness across the network users.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[88](#bib.bib88)] 中，作者提出了一种基于多智能体 $Q$-学习的 RL 模型，以解决蜂窝系统中的下行功率控制和速率适配问题。智能体是一个位于小区中的网络实体，例如
    eNB。动作空间是离散的，对应于分配小区的下行发射功率。状态空间包括四个元素：小区功率、平均参考信号接收功率、平均干扰和小区奖励。奖励函数是连续的，根据 $\alpha$-公平资源分配效用函数定义。它由与无线小区中所有用户相关的无线测量或性能指标定义。系统级仿真显示，他们提出的方法能够快速学习功率控制策略，为网络用户提供显著的节能和公平性。
- en: The authors in [[89](#bib.bib89)] propose a single-agent DQN method to address
    the problem of power resource use in fuel cell powered data center power grids.
    In the model, the agent is the data center architecture whose state space is discrete
    and comprised of the following elements; 1) the number of data centers in the
    previous section at the current and next time slots, 2) the action at the current
    time slot, 3) and the reward function at the current time slot. The action space
    is choosing the data center, and the reward is a function of the sum of all the
    data centers’ power demands in each time slot. Their approach is compared to the
    state-of-the-art approaches and evaluated based on real-world traces, which shows
    that it reduces the energy gap and saves more energy at around 5%.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[[89](#bib.bib89)]中的作者提出了一种单智能体DQN方法，用于解决燃料电池供电的数据中心电网中的电力资源使用问题。在模型中，智能体是数据中心架构，其状态空间是离散的，包括以下元素：1）当前和下一个时间槽中前一节的数据中心数量，2）当前时间槽的动作，3）当前时间槽的奖励函数。动作空间是选择数据中心，而奖励是每个时间槽中所有数据中心电力需求的总和的函数。他们的方法与最先进的方法进行了比较，并基于现实世界的追踪数据进行了评估，结果显示其能减少能源差距，并节省大约5%的能源。'
- en: In [[90](#bib.bib90)], the authors propose single- and multi-agent actor-critic
    DRL methods to tackle the problem of downlink sum-rate maximization through power
    allocation in multi-cell, multi-user cellular networks. In their model, the agents
    are the base stations (BSs), whose state space is continuous and comprises network
    CSI and the transmit power allocation by previous BSs. The action space is continuous,
    which represents the power allocation, while the reward function is the cellular
    network sum SE. Experimental results demonstrate that their proposed DRL-based
    method can both achieve higher SE than conventional optimization algorithms, such
    as fractional programming (FP) and weighted minimum mean-squared error (WMMSE)
    while performing two times faster than these conventional methods.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[90](#bib.bib90)]中，作者提出了单智能体和多智能体演员-评论家（actor-critic）DRL方法，用于通过功率分配解决多小区、多用户蜂窝网络中下行链路总速率最大化的问题。在他们的模型中，智能体是基站（BSs），其状态空间是连续的，包括网络CSI和之前基站的功率分配。动作空间是连续的，代表功率分配，而奖励函数是蜂窝网络的总谱效（SE）。实验结果表明，他们提出的基于DRL的方法在SE上优于传统的优化算法，如分数规划（FP）和加权最小均方误差（WMMSE），并且比这些传统方法快两倍。
- en: The work in [[91](#bib.bib91)] proposes a distributive multi-agent DDPG-based
    DRL algorithm to address the problem of sum-rate maximization via continuous power
    control in wireless mobile networks. The agents are each transmitter (e.g., mobile
    devices, links, etc.) whose state is a combination of three feature groups; the
    local information, interfering neighbors, and interfered neighbors feature groups.
    Each agent’s action is choosing the transmit power level, while the reward is
    a function of the sum-rate maximization problem. Simulation results show that
    their proposed method gives better performance results than the conventional FP
    methods and comparable results with the WMMSE methods.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[[91](#bib.bib91)]中的工作提出了一种基于分布式多智能体DDPG的深度强化学习（DRL）算法，用于解决无线移动网络中通过连续功率控制最大化总速率的问题。智能体是每个发射器（如移动设备、链路等），其状态是三个特征组的组合：本地信息、干扰邻居和被干扰邻居特征组。每个智能体的动作是选择发射功率水平，而奖励是总速率最大化问题的一个函数。模拟结果表明，他们提出的方法在性能上优于传统的FP方法，并与WMMSE方法的结果相当。'
- en: D2D communication has emerged as one of the main enabling technologies for modern
    wireless networks. In [[92](#bib.bib92)], the authors present a centralized multi-agent
    DQN-based DRL algorithm to address the problem of power allocation of D2D cellular
    communication in a time-varying environment. The agents are the D2D transmitters,
    whose state space is continuous, comprised of the SINR and channel gain of users.
    The action space is discrete, representing the transmit power of each D2D user,
    while the reward is a function of system throughput. Simulation results show that
    their proposed algorithm outperforms the traditional RL methods in terms of network
    capacity and user’s achieved QoS.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: D2D通信已成为现代无线网络的主要支持技术之一。在[[92](#bib.bib92)]中，作者提出了一种基于集中式多智能体DQN的DRL算法，用于解决时变环境中D2D蜂窝通信的功率分配问题。智能体是D2D发射器，其状态空间是连续的，包括用户的SINR和信道增益。动作空间是离散的，表示每个D2D用户的发射功率，而奖励是系统吞吐量的函数。模拟结果表明，他们提出的算法在网络容量和用户实现的QoS方面优于传统的强化学习方法。
- en: 5G UDNs are characterized by their high vulnerability to inter-cell interference,
    which can be greatly reduced via judicious power management. Towards this, Saeidian
    et al. [[93](#bib.bib93)] propose a data-driven approach based on a multi-agent
    DQN algorithm to tackle the downlink power control in dense 5G cellular networks.
    The agents are the BSs, whose state space is continuous, comprised of path-gain,
    SINR, downlink rate, and downlink power. The action space is discrete, representing
    the downlink power, while the reward is a function of the network-wide harmonic-mean
    of throughput. Simulation results indicate that their approach can improve data
    rates at the cell edge while ensuring a reduced transmitted power compared to
    the baseline fixed power allocation approaches.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Non-orthogonal multiple access (NOMA) technology has recently emerged as an
    efficient tool to enhance the QoS and EE of millimeter-wave (mmWave) communication
    systems via enhancing the power level of received signals. In this context, the
    authors in [[94](#bib.bib94)] propose a multi-agent DQN-based DRL framework to
    optimize the EE in downlink full-duplex cooperative NOMA of mmWave UDNs. The agents
    are the relay near users, whose state space is continuous, consisting of information
    related to wireless environment and channel, the user’s battery capacity, energy
    power transfer coefficient, self-interference cancellation residue coefficient,
    and the buffer size of nearby relay users. The action space is to specify the
    required user pairing between the near relay user group and edge user group, along
    with the pre-processing of EE power allocation. The reward is a function of the
    EE of the mmWave network. Experimental results are compared with a conventional
    centralized iteration algorithm, which demonstrate both the superiority of their
    proposed algorithm in terms of the convergence speed and the efficiency to provide
    near-optimal results.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Due to the importance of the power allocation in multi-user cellular networks,
    the authors in [[95](#bib.bib95)] address this issue by building on their initial
    investigation in [[96](#bib.bib96)]. A multi-agent DQN-based DRL algorithm is
    proposed in which each BS-user link is considered as an agent. The state space
    is continuous, comprised of a logarithmic normalized interferer, the link’s corresponding
    downlink rate, and the transmitting power. The action space is discrete, corresponding
    to the downlink power allocation, while the reward is continuous, which is a function
    of the downlink data rate of the communication link. Experimental results indicate
    that their proposed DQN outperforms benchmark algorithms such as FP, WMMSE, random
    power allocation, and maximum power allocation in terms of achievable averaged
    sum-rate and the convergence time when considering different user densities.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: In another interesting work in [[97](#bib.bib97)], the authors design a multi-agent
    DQN and DDPG-based DRL framework to address the problem of power control in HetNets.
    A centralized-training-distributed-execution algorithm is designed in which the
    APs are the agents, each of which implements a local DNN. The state space of each
    local DNN is continuous, representing the local state information, while the local
    action space is continuous, representing the transmit power. Then, multiple-actor-shared-critic
    method (MASC) is proposed to separately train each of these local DNN in an online
    fashion. The main idea is that the MASC training method is composed of multiple
    actor DNNs and a shared critic DNN. An actor DNN is first established in the core
    network for each local DNN, and the structure of each actor DNN is the same as
    the corresponding local DNN. Then, a shared critic DNN is established in the core
    network for these actor DNNs. Historical global information is provided into the
    critic DNN, and the output of the critic DNN will evaluate whether the output
    power of each actor DNN is optimal or not from a global view. The reward function
    is continuous, representing the data rate between each AP and its associated user.
    Simulation results show that their proposed algorithm outperforms the WMMSE and
    FP algorithms in terms of both convergence rate and computational complexity.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Managing radio resources in virtualized networks is attracting more attention
    lately. In this context, the authors in [[98](#bib.bib98)] address the problem
    of throughput maximization in C-RANs via power allocation in virtualized 5G networks.
    The authors propose a multi-agent DQN-based DRL algorithm to solve the problem
    in which the agents are each link between RRH and user. The action space is discrete,
    corresponding to the transmit power. The state space is continuous, representing
    the current partial CSI and the respective power set. The reward of each slice
    is discrete, defined as a function of the sum of its tenants’ rates. Via simulation,
    the authors show that their proposed scheme achieves a higher sum-rate compared
    to greedy search-based power allocation approaches.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 In IoT and Other Emerging Wireless Networks
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that utilize DRL algorithms to
    address the power allocation issue in IoT and other emerging wireless networks
    shown in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey").'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Developing efficient spectrum sharing schemes is regarded as one of the main
    persistent objectives and challenges in CRNs. In [[99](#bib.bib99)], the authors
    propose a non-cooperative single-agent DQN-based DRL scheme to address the problem
    of spectrum sharing via power control in CRNs. In their model, the agent is the
    SU, whose action space is discrete, corresponding to selecting the transmit power
    from a pre-defined power set. The state space is discrete, defined by four parts;
    the transmit power of PU and SU, the path loss between PU and a sensor that measures
    the RSS, the path loss between the SU and a sensor that measures the RSS, and
    some Gaussian random variable. The reward is a discrete function defined by the
    achieved SINR level and the minimum SINR requirements of both PU and SU. Simulation
    results show that their proposed algorithm is robust against random variation
    in state observations, and the SU interacts with PU efficiently until they reach
    a state in which both users successfully transmit their own data.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 开发高效的频谱共享方案被视为CRN中的主要持久目标和挑战之一。在[[99](#bib.bib99)]中，作者提出了一种基于非合作单智能体DQN的DRL方案，以通过功率控制解决CRN中的频谱共享问题。在他们的模型中，智能体是SU，其行动空间是离散的，对应于从预定义功率集选择发射功率。状态空间是离散的，由四部分定义；PU和SU的发射功率、PU与测量RSS的传感器之间的路径损失、SU与测量RSS的传感器之间的路径损失，以及一些高斯随机变量。奖励是一个离散函数，由实现的SINR水平和PU与SU的最小SINR要求定义。仿真结果表明，他们提出的算法对状态观察的随机变化具有鲁棒性，SU与PU有效互动，直到达到两个用户都成功传输自己数据的状态。
- en: In other work in [[13](#bib.bib13)], the authors present a non-cooperative multi-agent
    algorithm to address the problem of power allocation in D2D communication networks
    based on three DQNs, namely, DQN, Double DQN, and Dueling DQN. The agents are
    the D2D transmitters in each D2D pair, whose state space is discrete, comprised
    of the level of the interference indicator function. The action space is discrete,
    representing the set of transmitting power levels, while the reward is a function
    of the system EE. Simulation results show the ability of such DQN-based models
    to provide energy-efficient power allocation for the underlying D2D network.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[13](#bib.bib13)]中，作者提出了一种非合作的多智能体算法，以解决基于三种DQN（即DQN、Double DQN和Dueling DQN）的D2D通信网络中的功率分配问题。智能体是每对D2D中的D2D发射机，其状态空间是离散的，由干扰指示函数的级别组成。行动空间是离散的，表示发射功率级别的集合，而奖励是系统EE的一个函数。仿真结果显示，这种基于DQN的模型能够为基础D2D网络提供节能的功率分配。
- en: In [[100](#bib.bib100)], a DRL algorithm based on multi-agent DQN is proposed
    to tackle the problem of downlink power allocation in multi-cell cellular networks.
    The agents are the base stations (BSs), whose stat space is hybrid, comprised
    of the number of users connected to the BSs and the corresponding power allocation.
    The action space corresponds to selecting the adjustment value of the power on
    the subcarriers of the BSs, while the reward is a function of the overall network
    capacity. Experimental results demonstrate that their proposed algorithm is better
    than both the water-filling power allocation and $Q$-learning methods in terms
    of model stability and convergence speed.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[100](#bib.bib100)]中，提出了一种基于多智能体DQN的DRL算法，以解决多小区蜂窝网络中的下行功率分配问题。智能体是基站（BS），其状态空间是混合的，包括连接到基站的用户数量和相应的功率分配。行动空间对应于选择基站子载波上的功率调整值，而奖励是网络总体容量的函数。实验结果表明，他们提出的算法在模型稳定性和收敛速度方面优于水填充功率分配方法和$Q$-学习方法。
- en: UAV networks are attracting considerable attention recently due to their ability
    to provide enhanced QoS communication in harsh and vital environments. However,
    power management is one of the key challenges in such networks. In this context,
    the authors in [[101](#bib.bib101)] address the problem of downlink power control
    in ultra-dense UAV networks with the aim of maximizing the network’s EE. A multi-agent
    DQN-based DRL model is proposed in which the agents are the UAVs in the network.
    The state space is continuous, representing the remaining energy of the UAV and
    the interference caused by neighboring UAVs. The action space is discrete, representing
    the set of possible discrete transmit power values, while the reward function
    is the EE of the UAV network. Simulation results are compared with $Q$-learning
    and random algorithms, which show the superiority of their proposed scheme in
    terms of both the convergence speed and EE.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Other inserting work [[102](#bib.bib102)] proposes a multi-agent DQN-based DRL
    method to study the problem of transmit power control in wireless networks. The
    agents are the transmitters whose state space is continuous, consisting of three
    main feature groups; local information, interfering neighbors, and interfered
    neighbors. The action space is discrete corresponds to discrete power levels,
    while the reward is a function of the weighted sum-rate of the whole network.
    Experimental results demonstrate that the proposed distributed algorithm provides
    comparable and even better performance results to the state-of-the-art optimization-based
    algorithms available in the literature.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting work is proposed in [[103](#bib.bib103)]. The author proposes
    a single-agent smoothing DQN-network to control power allocation in cognitive
    radio-based wireless networks. The agent is the sensor, whose state space is discrete,
    comprising of a set of received signal strength (RSS) information. The action
    space is discrete, corresponding to selecting the transmit power, while the reward
    is a function of the SINR. Using simulation results, the author shows that the
    proposed smoothing DQN outperforms the conventional DQN in cognitive-based radio
    networks.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: In [[104](#bib.bib104)], the authors propose a multi-agent DDPG-based DRL to
    address the problem of joint trajectory design and power allocation in multi-UAV
    wireless networks. In their scheme, the agents are the UAVs, whose state space
    is a discrete binary indicator function representing whether the QoS of the user
    ends (UEs) are satisfied or not. The action space is also discrete, corresponding
    to selecting UAVs’ trajectory and transmission power. The reward is a continuous
    function defined by the joint trajectory design and power allocation as well as
    the number of UEs covered by the UAVs. Simulation results show that the proposed
    algorithm achieves higher network utility and capacity than the other optimization
    methods in wireless UAV networks with reduced computational complexity.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: High-speed railway (HSR) systems are one of the emerging IoT applications for
    next-generation wireless networks. Such systems are characterized by their rapid
    variations in the wireless environment, which mandate the development of light-weighted
    RRAM solutions. As a response to this, Xu et al. [[105](#bib.bib105)] propose
    a multi-agent DDPG-based DRL model to address the problem of sum-rate maximization
    via power allocation in hybrid beamforming-based mmWave HSR systems. In their
    approach, each mobile relay (MR) acts as an agent. The action space is continuous,
    corresponding to the transmit power level of each MR agent. Also, the state space
    is continuous, defined by; each MR own signal channel, local observation information
    of each MR, i.e., beamforming design, each MR achievable rate, and each MR transmit
    power in the last time step. The reward function is continuous, defined by the
    achievable sum-rate of the network. Simulation results demonstrate that the SE
    of their proposed algorithm is comparable to the full digital beamforming scheme,
    and it outperforms conventional approaches such as maximum power allocation, random
    power allocation, DQN, and FP.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Federated deep reinforcement learning (FDRL) is an emerging technique that integrates
    FD and DRL methods. FDRL can be utilized as an efficient tool to enhance the RRAM
    solutions in large-scale distributed systems). As an example, an interesting approach
    is proposed in [[106](#bib.bib106)], in which the authors propose a cooperative
    multi-agent actor-critic-based FDRL framework for distributed wireless networks.
    The authors particularly address the problem of energy/spectrum efficiency maximization
    via distributed power allocation for network edge users. In their proposed model,
    the agents are the edge users, whose action space is continuous, defined as the
    power allocation strategies. The state space is continuous, defined by the allocated
    transmit power, the SINR on the assigned RBs, and the reward of the previous training
    time step. The system is defined in terms of a local continuous cost function
    expressed in terms of SINR, power, path loss, and environmental noise. Using simulation
    results, the authors demonstrate that their proposed framework achieves better
    performance accuracy in terms of power allocation than other approaches such as
    greedy, non-cooperation power allocation, and traditional FL.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 In Satellite Networks
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following paragraphs, we review works that employ DRL techniques to address
    the power allocation issue in satellite networks as well as emerging satellite
    IoT systems.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Managing downlink transmit power in satellite networks is also one of the major
    persistent challenges. To this end, the authors in [[107](#bib.bib107)] extended
    their work in [[108](#bib.bib108)] and present a single-agent Proximal Policy
    Optimization (PPO)-based DRL model to solve the problem of power allocation in
    multi-beam satellite systems. In their model, the agent is the processing engine
    that allocates power within the satellite, whose state space is continuous, comprises
    the set of demand requirements per beam, and the optimal power allocations for
    the two previous time steps. The action space is continuous, representing the
    allocation of the power for each beam, while the reward is a function of both
    the link data rate achieved by the beam and the power set of the agent. Experimental
    results demonstrate the robustness of their proposed DRL algorithm in dynamic
    power resource allocation for multi-beam satellite systems.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 管理卫星网络中的下行传输功率也是主要的持久性挑战之一。为此，[[107](#bib.bib107)] 的作者扩展了他们在 [[108](#bib.bib108)]
    中的工作，并提出了一个基于单代理的 Proximal Policy Optimization (PPO) 深度强化学习模型，以解决多波束卫星系统中的功率分配问题。在他们的模型中，代理是分配卫星内功率的处理引擎，其状态空间是连续的，由每个波束的需求要求集和前两次时间步骤的最优功率分配组成。动作空间是连续的，表示每个波束的功率分配，而奖励是波束实现的链路数据速率和代理设置的功率的函数。实验结果表明，他们提出的深度强化学习算法在多波束卫星系统的动态功率资源分配中表现出稳健性。
- en: NOMA technique has shown efficient results in improving the performance of terrestrial
    mmWave cellular systems [[109](#bib.bib109)]. This has motivated the use of NOMA
    for satellite communication systems. However, managing the radio resources in
    such a system becomes an imperative issue. In this context, Yan et al. [[110](#bib.bib110)]
    study the problem of power allocation for NOMA-enabled SIoT using a single-agent
    DQN-based DRL scheme. In their system, the agent is the satellite, whose action
    space is discrete, corresponding to selecting the power allocation coefficient
    for each NOMA user. The state space is continuous, consisting of the average SNR,
    link budget, and delay-QoS requirements of NOMA users, while the reward is discrete,
    which is a function of the effective capacity of each NOMA user. Experimental
    results demonstrate that their proposed DRL-based power allocation scheme can
    produce optimal/near-optimal actions, and it provides superior performance to
    both the fixed power allocation strategies and OMA scheme.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: NOMA 技术在提高地面毫米波蜂窝系统性能方面已显示出高效的结果 [[109](#bib.bib109)]。这激励了在卫星通信系统中使用 NOMA。然而，在这种系统中管理无线资源成为一个迫切问题。在这种背景下，Yan
    等人 [[110](#bib.bib110)] 研究了使用单代理 DQN 基于深度强化学习方案的 NOMA 支持 SIoT 的功率分配问题。在他们的系统中，代理是卫星，其动作空间是离散的，对应于为每个
    NOMA 用户选择功率分配系数。状态空间是连续的，由 NOMA 用户的平均 SNR、链路预算和延迟 QoS 要求组成，而奖励是离散的，是每个 NOMA 用户有效容量的函数。实验结果表明，他们提出的基于深度强化学习的功率分配方案可以产生最优/接近最优的动作，并且提供了优于固定功率分配策略和
    OMA 方案的性能。
- en: IV-A4 In Multi-RAT HetNets
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 在多无线接入技术异构网络中
- en: Multi-RAT wireless HetNets is one of the main enabling technologies for modern
    wireless systems, including 6G networks [[3](#bib.bib3)]. In HetNets, several
    RATs with different operating characteristics coexist to enhance network coverage
    and reliability while providing enhanced QoE to users. The underlying RATs have
    non-overlapping radio resources; therefore, there would not be typically interference
    in the network.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 多无线接入技术无线异构网络是现代无线系统（包括 6G 网络）的一项主要支撑技术 [[3](#bib.bib3)]。在异构网络中，几种具有不同操作特性的无线接入技术共同存在，以增强网络覆盖和可靠性，同时为用户提供更好的体验。基础无线接入技术拥有非重叠的无线资源，因此网络中通常不会出现干扰。
- en: 'Since a stand-alone network with a single RAT would not be able to support
    the stringent requirements of emerging disruptive applications, modern smart user
    devices are equipped with advanced capabilities that enable them to aggregate
    various radio resources to boost their QoE. Modern user devices can operate in
    a multi-mode scenario, in which each user device can be connected to a single
    RAT at any time. Alternatively, user devices can operate in a multi-homing scenario
    such that they can be connected simultaneously to various RATs to aggregate their
    radio resources, such as bandwidth and data rate. Multi-RAT networks include the
    coexistence of RATs, such as the licensed band networks, unlicensed bands networks,
    hybrid systems, and any combination of the wireless networks that are shown in
    Fig. [3](#S1.F3 "Figure 3 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '由于单独的网络与单一的无线接入技术（RAT）无法满足新兴颠覆性应用的严格要求，因此现代智能用户设备配备了先进的功能，使其能够聚合各种无线资源以提升其**用户体验**（QoE）。现代用户设备可以在多模式场景中操作，在这种场景中，每个用户设备可以随时连接到一个单一的RAT。或者，用户设备也可以在多重连接场景中操作，从而可以同时连接到多个RAT，以聚合其无线资源，如带宽和数据速率。多RAT网络包括RAT的共存，如授权频段网络、非授权频段网络、混合系统以及图中所示的任何组合的无线网络[3](#S1.F3
    "Figure 3 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey")。'
- en: Visible Light Communication (VLC) is a promising RAT that can support multi-Gbps
    of data rates over wireless links [[111](#bib.bib111)]. It is mainly developed
    for indoor applications; however, it is gaining considerable attention lately
    for outdoor applications as well [[112](#bib.bib112)]. This has motivated researchers
    to propose solutions that integrate VLC with conventional radio systems to boost
    aggregate data rates. Managing radio resources in these integrated systems, however,
    becomes a challenge. In this context, in [[40](#bib.bib40)], the authors propose
    a multi-agent $Q$-learning-based two-time scale scheme to address the power allocation
    issue for multi-Homing hybrid RF/VLC networks. In their technique, the agents
    are the RF and VLC APs, whose action space is discrete, corresponding to selecting
    the downlink power level that ensures the QoS’s satisfaction of the multi-homing
    users. The state space is discrete, which is a function of users’ achievable and
    target rates from the RF and VLC APs. The reward is also discrete, which is a
    function of the achieved and target rates from all RF and VLC APs. Experimental
    results demonstrate that not only the users’ target rates are satisfied, but also
    the ability of their algorithm to adapt to the network’s dynamics.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 可见光通信（VLC）是一种有前景的RAT，能够在无线链路上支持多Gbps的数据速率[[111](#bib.bib111)]。它主要用于室内应用；然而，近年来它也受到相当大的关注用于室外应用[[112](#bib.bib112)]。这促使研究人员提出将VLC与传统无线系统集成以提升总数据速率的解决方案。然而，在这些集成系统中管理无线资源成为一项挑战。在这种背景下，在[[40](#bib.bib40)]中，作者提出了一种基于多智能体$Q$-学习的双时间尺度方案，以解决多重连接混合RF/VLC网络的功率分配问题。在他们的技术中，智能体是RF和VLC接入点（AP），其动作空间是离散的，对应于选择确保多重连接用户的**服务质量**（QoS）满足的下行功率级别。状态空间也是离散的，它是RF和VLC
    AP的用户可实现和目标速率的函数。奖励也是离散的，它是所有RF和VLC AP的实际和目标速率的函数。实验结果表明，用户的目标速率不仅得到了满足，而且算法也能够适应网络的动态变化。
- en: For the same network settings in [[40](#bib.bib40)], Ciftler et al. [[42](#bib.bib42)]
    propose a DRL-based scheme to enhance the results and overcome the shortcomings.
    In particular, the authors in [[42](#bib.bib42)] propose a non-cooperative multi-agent
    DQN-based algorithm to address the problem of power allocation in hybrid RF/VLC
    networks. The agents are the RF and VLC APs whose action space is discrete, representing
    the transmit power. The state space is continuous, comprised of the actual and
    target rates, while the reward function is continuous and is a function of target
    rate band, target rate, and actual rate. Using simulation results, the authors
    demonstrate that the DQN-based algorithm converges with the rate of 96.1% compared
    with the $Q$-learning-based algorithm’s convergence rate of 72.3%.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis and Reflections
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we review the applications of DRL techniques for power allocation
    and management in modern wireless networks. The reviewed papers are summarized
    in Table [V](#S4.T5 "TABLE V ‣ Synthesis and Reflections ‣ IV-A4 In Multi-RAT
    HetNets ‣ IV-A DRL for Power Allocation ‣ IV DRL-Based Radio Resource Allocation
    and Management for Next Generation Wireless Networks ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"). We observe that various DRL techniques can efficiently
    solve the power allocation optimization problems in diversified wireless network
    scenarios, and their performance outperforms the state-of-the-art heuristic approaches.
    Besides, as we discussed in the previous paragraphs, DRL methods can provide comparable
    results to the conventional centralized optimization-based approaches that have
    full knowledge of the wireless environments as reported in [[91](#bib.bib91)],
    or even better results as reported in [[97](#bib.bib97)].'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We also observe that most of the papers implement multi-agent DRL interactions,
    and the value-based DRL algorithms, such as DQN and $Q$-learning, are utilized
    more than the policy-based counterparts. Since the power allocation problem falls
    in the continuous action space, the use of value-based algorithms to address these
    types of problems makes the learned policies vulnerable to discretization errors
    that degrade the accuracy and reliability of the learned models. Hence, the policy-based
    algorithms, such as DDPG and actor-critic, have received more attention lately,
    and they have shown more accurate and reliable results compared to the value-based
    counterparts with additional complexity, as discussed in [[91](#bib.bib91), [90](#bib.bib90),
    [97](#bib.bib97), [105](#bib.bib105)]. In addition, most of the papers consider
    the rate maximization, SE, and EE as key performance metrics (e.g., [[105](#bib.bib105),
    [90](#bib.bib90), [94](#bib.bib94)]). However, other KPI metrics must be considered
    as well during the design of DRL frameworks, such as latency, reliability, and
    coverage, especially for emerging real-time and time-sensitive IoT applications.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we observe from Table [V](#S4.T5 "TABLE V ‣ Synthesis and Reflections
    ‣ IV-A4 In Multi-RAT HetNets ‣ IV-A DRL for Power Allocation ‣ IV DRL-Based Radio
    Resource Allocation and Management for Next Generation Wireless Networks ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey") that both the cellular HomeNets and
    emerging IoT wireless networks gain more attention than satellite and multi-RAT
    networks that still in their early stages and require more in-depth investigation.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A Summary List of Papers Related to DRL for Power Allocation.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Learning Algorithm |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| Network Type | Ref. | Radio Resource (or Issues Addressed) | Mode | Algorithm
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Cellular & HomNets | Cellular systems | Ghadimi et al.[[88](#bib.bib88)]
    | power control & rate adaptation | Multi-agent | $Q$-learning |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Data center power grids | Hu et al.[[89](#bib.bib89)] | Power resource use
    | Single-agent | DQN |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| Multi-cell cellular | Khan et al.[[90](#bib.bib90)] | Downlink power allocation
    | Single- & multi-agent | Actor-critic |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| Wireless mobile networks | Nasir et al.[[91](#bib.bib91)] | Continuous power
    control | Multi-agent | DDPG |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| D2D cellular | Bi et al.[[92](#bib.bib92)] | Power allocation | multi-agent
    | DQN |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| Dense 5G cellular | Saeidian et al.[[93](#bib.bib93)] | Downlink power control
    | Multi-agent | DQN |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| NOMA mmWave UDNs | Zhang et al.[[94](#bib.bib94)] | EE power allocation |
    Multi-agent | DQN |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| Cellular networks | Meng et al.[[96](#bib.bib96), [95](#bib.bib95)] | Downlink
    power allocation | multi-agent | DQN |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| HetNets | Zhang et al.[[97](#bib.bib97)] | Power control | multi-agent |
    DQN & DDPG |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| Virtualized 5G networks | Mohsenivatani et al.[[98](#bib.bib98)] | Power
    allocation | multi-agent | DQN |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '|   IoT & Emerging Nets | CRNs | Li et al.[[99](#bib.bib99)] | Power control
    | Single-agent | DQN |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| D2D networks | Nguyen et al.[[13](#bib.bib13)] | Power allocation | multi-agent
    | DQN, DDQN, & Dueling DQN |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Multi-cell cellular | Zhang et al.[[100](#bib.bib100)] | Downlink power allocation
    | Multi-agent | DQN |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| Ultra-dense UAV | Li et al.[[101](#bib.bib101)] | Downlink power control
    | Multi-agent | DQN |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| Wireless Networks | Nasir et al.[[102](#bib.bib102)] | Transmit power control
    | Multi-agent | DQN |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Wang et al.[[103](#bib.bib103)] | Power allocation | Single-agent
    | DQN |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| Multi-UAV | Zhao et al.[[104](#bib.bib104)] | Power allocation | Multi-agent
    | DDPG |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| mmWave HSR systems | Xu et al.[[105](#bib.bib105)] | Power allocation | Multi-agent
    | DDPG |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| Distributed networks | Yan et al.[[106](#bib.bib106)] | Distributed power
    allocation | Multi-agent | Actor-critic |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '|   Satellite | Multi-beam satellites | Luis et al.[[108](#bib.bib108), [107](#bib.bib107)]
    | Power allocation | Single-agent | PPO |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '|  | NOMA-enabled SIoT | Yan et al.[[110](#bib.bib110)] | Power allocation
    | Single-agent | DQN |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '|   Multi-RAT | Hybrid RF/VLC networks | Kong et al.[[40](#bib.bib40)] | Power
    allocation | Multi-agent | $Q$-learning |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '|  | Hybrid RF/VLC networks | Ciftler et al.[[42](#bib.bib42)] | Power allocation
    | multi-agent | DQN |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: IV-B DRL for Spectrum Allocation and Access Control
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the significant challenges in modern wireless communication networks
    that still needs more investigation is spectrum management and access control.
    In this context, DRL techniques have attracted considerable research interest
    recently due to their robustness in making optimal decisions in dynamic and stochastic
    environments. This section presents the related works to the applications of DRL
    algorithms for radio spectrum allocation in modern wireless networks. This includes
    issues, such as dynamic network access, user association or cell selection, spectrum
    access or channels selection/assignment, and the joint of any of these issues,
    as shown in Fig. [3](#S1.F3 "Figure 3 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey").'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In modern wireless networks, a massive number of user devices may request to
    access the wireless channel simultaneously. This may drastically overload and
    congest the channel, causing communication failure and unreliable QoS. Hence,
    efficient communication schemes and protocols must be developed to address this
    issue in channel access via employing various access scheduling and prioritization
    techniques. RRAM for modern wireless networks requires considering dynamic load
    balancing and access management methods to support the massive capacity and connectivity
    requirements of the future wireless networks while utilizing their radio resources
    efficiently. DRL methods have been used recently to address these issues, and
    they have demonstrated efficient results in the context of massive channel access.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, user devices in cellular networks are required to associate
    or be assigned to BS(s) or network AP(s) to get a service. The association process
    could be symmetric, i.e., both uplink and downlink are from the same BS/AP, or
    it may be asymmetric in which the uplink and downlink may associate to different
    BSs/APs. This association or cell selection process must be carefully addressed
    as it strongly affects the allocation of network radio resources. Unfortunately,
    such types of problems are typically non-convex and combinatorial [[39](#bib.bib39)]
    and need accurate network information to obtain the optimal solution. In this
    context, DRL techniques have also shown efficient results in addressing user association
    and cell selection issues for modern wireless networks.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 In Cellular and HomNets
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the spectrum and access control problem in cellular and HomNets depicted
    in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep
    Reinforcement Learning for Radio Resource Allocation and Management in Next Generation
    Heterogeneous Wireless Networks: A Survey").'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: RRAM in UAV-assisted cellular networks is also one of the main emerging challenges.
    Towards this end, in [[113](#bib.bib113)], a multi-agent DQN model addresses the
    joint user association, spectrum allocation, and content caching in an LTE network
    consisting of UAVs serving ground users. In their model, the agents are the UAVs,
    which have storage units and have the ability to cached contents in LTE-BSs. These
    UAV agents can access the licensed as well as the unlicensed spectrum bands, and
    a remote cloud-based server is used to control them. The licensed cellular spectrum
    band is used in the transmissions from the cloud to the UAVs. Each UAV agent has
    to obtain 1) its user association, 2) bandwidth assignment indicators in the licensed
    spectrum band, 3) time slot indicators in the unlicensed spectrum band, and 4)
    content that the users request. The input of the DQL is the other agents’ actions
    (the UAV-user association schemes), and the output is the set of users that the
    UAV can handle. Simulation results demonstrate that their proposed DQL strategy
    enhances the number of users up to 50% compared to the standard $Q$-learning strategy.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Based on their initial work in [[114](#bib.bib114)], the authors in [[115](#bib.bib115)]
    propose a multi-agent Dueling Double deep Q-network (D3QN)-based DRL model to
    handle the joint BS and channel selections in macro and femto BS networks sharing
    a set of radio channels. In their scheme, the agents are the UEs, whose state
    space is a discrete binary vector that shows whether UEs’ SINR higher than the
    minimum QoS requirement or not. The action space is discrete, corresponding to
    the BS and channel association. The reward function is discrete in which the UE
    agent will receive a utility as a reward if the QoS is met; otherwise, it will
    receive a negative value for the reward. Simulation results demonstrate that their
    proposed strategy outperforms the standard $Q$-learning strategy in terms of generalization,
    system capacity, and convergence speed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The problem of user association problem in cellular IoT networks is studied
    in [[116](#bib.bib116)]. The goal is to assign IoT devices to particular cellular
    users to maximize the sum-rate of the IoT network. Two single-agent DQN DRL algorithms
    are proposed; the first one utilizes global information to makes decisions for
    all IoT devices at one time, while the other algorithm uses local information
    to make a distributed decision for only a single IoT device at one time. In their
    model, the BS acts as the agent whose state space is continuous, consisting of
    both historical CSI and interference information. The action space is discrete,
    representing both all possible association schemes of the network and the individual
    association for only a single IoT device. The reward function of the first DQN
    algorithm is the sum-rate of all IoT devices, while for the second DQN includes
    both the current transmission rate of IoT devices and the interference with other
    IoT devices. Experimental results demonstrate that their proposed DRL framework
    both scalable and achieves performance comparable to the optimal user association
    policy.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: In another interesting work in [[117](#bib.bib117)], the authors study the problem
    of spectrum allocation in the emerging integrated access and backhaul (IAB) cellular
    networks characterized by their dynamic environment and large-scale deployment.
    The problem is formulated as a non-convex mix-integer and non-linear programming,
    and a DRL framework based on single-agent Double DQN and actor-critic algorithms
    is proposed to solve it. In their model, the agent is a center-located controller
    or distributed UE. The state space is discrete, indicating the status of UEs’
    QoS, and the action space is discrete, corresponding to the allocation matrix
    for the donor BS and IAB nodes. The reward function is modeled to optimize the
    proportional fairness allocation of the network. Experimental results demonstrate
    that their framework has promising results compared to other conventional spectrum
    allocation policies.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: The problem of load balancing in large-scale and dynamic wireless networks is
    also another important issue. In this context, the authors in [[118](#bib.bib118)]
    present a multi-agent $Q$-learning-based algorithm to address the problem of user
    association for load balancing in vehicular networks. In their scheme, the agents
    are the BSs, whose action space is discrete, representing the associations with
    the network’s vehicles. The state space is a hybrid (continuous and discrete),
    consisting of the service resources and its service demands, SINR matrix, and
    association matrix. The reward is a continuous function defined through the association
    and SINR matrices. Using experiments on real-field taxi movements, the authors
    evaluate the performance of their proposed algorithm, and they show that it provides
    higher quality load balancing compared to conventional association methods.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: In other work in [[119](#bib.bib119)], the authors propose a multi-agent $Q$-learning-based
    algorithm for the cognitive RAT selection issue in 5G HetNets. In their model,
    the agents are the terminal devices whose action is discrete, corresponding to
    identifying the available RATs. The state space is also discrete that corresponds
    to the currently active RAT association, while the reward function is continuous,
    which is defined by the average measured throughput and handovers call drops.
    Using experimental results, the authors show that their proposed framework outperforms
    the random and max-SINR decision approaches.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Zheng et al. [[120](#bib.bib120)] propose a single agent actor-critic-based
    DRL algorithm to address the problem of channel assignment for hybrid NOMA-based
    5G cellular networks. The agent is the BS, whose action space is discrete, corresponding
    to assigning channels for users. The state space is a hybrid (continuous and discrete)
    comprised of three elements; the CSI matrix, achieved users’ data rate in the
    previous time slot, and the assigned channels in the previous time slot. The reward
    is a discrete function defined in terms of users’ SE, the number of channels that
    use NOMA for transmission, and the number of users whose data rate is zero. Simulation
    results demonstrate that their proposed method outperforms some conventional approaches,
    such as greedy, random, match theory-based, and Genetic Algorithms, in terms of
    both network SE and sum-rate.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The problem of spectrum management in wireless DSA is addressed in [[121](#bib.bib121)]
    based on distributed multi-agent DQN. In their approach, the agents are each DSA
    user, whose action space is discrete, corresponding to the transmit power change
    for each channel. The state space is discrete, defined as the transmit power on
    wireless channels. The reward is a continuous function defined by the SE and the
    penalty caused by the interference to PUs. Experimental results show that their
    proposed model with echo state network-based DQN achieves a higher reward with
    both the achievable data rate and PU protections.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 In IoT and Other Emerging Wireless Networks
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the spectrum and access control problem in IoT and emerging wireless networks
    illustrated in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey").'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: In [[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)], the authors
    propose a single-agent DQN-based DRL scheme to tackle the problem of dynamic channel
    access for IoT sensor networks. In their scheme, the agent is the sensor, and
    its action is discrete, corresponding to selecting one channel to transmit its
    packets at each time slot. The state space is discrete, which is a combination
    of rewards and actions in the previous time slots. The reward function is also
    discrete, which is ”+1” if the selected channel is in low interference in such
    case a successful transmission occurs; otherwise, the reward is ”-1” in such case
    the selected channel is in high interference, and a transmission failure occurs.
    Simulation results show that their proposed scheme achieves an average reward
    of 4.4 compared to 4.5 obtained using the conventional myopic policy [[125](#bib.bib125)],
    which needs a compact knowledge of the transition matrix of the system.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Other work in [[126](#bib.bib126)] develops a single-agent DQN-based DRL model
    to address the channel selection in energy harvesting-based IoT sensors. In that
    work, the agent is one BS, which controls the channel assignments for energy harvesting-enabled
    sensors. The problem of the agent is to predict the battery level of the sensors
    and to assign channels to sensors such that the total rate is maximized. The DQL
    model used to solve this problem has two long-short-term-memory (LSTM) neural
    network (NN) layers, one for predicting the sensor’s battery state and one for
    obtaining channel access policy based on the predicted states obtained from the
    first layer. The agent’s action is all the available sensors that require to access
    the channels. The state contains the history of channel access scheduling, true
    and predicted battery information history and the current sensor’s CSI. Simulation
    results show that the total rates obtained using the DQL scheme are 6.8 kbps compared
    to 7 kbps obtained from the optimal scheme rate.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: A multi-agent DQN-based DRL model is proposed in [[127](#bib.bib127)] in order
    to address the cooperative spectrum sensing issue in CRNs. In their scheme, the
    agents are the SUs whose action is discrete, corresponding to sensing the spectrum
    for possible transmission without interfering with the PUs. The state space is
    discrete, and it is comprised of four elements representing cases when the spectrum
    is sensed as occupied, the spectrum is not sensed in a particular time slot, the
    spectrum is sensed as idle, and one of the other SUs broadcast the sensing result
    first. The reward function is the binary indicator, which is ”+1” if the spectrum
    is sensed as idle and ”0” otherwise. Simulation results show that their proposed
    algorithm has a faster convergence speed and better reward performance than the
    conventional $Q$-learning algorithm.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Xu et al. [[128](#bib.bib128)] propose a single-agent DQN and DDQN-based DRL
    approaches to address the problem of dynamic spectrum access in wireless networks.
    In their model, the agent is a wireless node (e.g., a user) whose action is discrete,
    corresponding to sensing the discrete frequency channel for possible data transmission.
    The state space is discrete, defining if the channel is occupied or idle at time
    slot $t$. The reward function is discrete, which is ranging from 0 to 100 for
    successful transmission; otherwise, the reward is ”-10” if the channel state is
    occupied and the user transmission fails. It is shown using simulation results
    that both DQN and DDQN can learn different nodes’ communication patterns and achieve
    near-optimal performance without prior knowledge of system dynamics.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Based on their initial work in [[129](#bib.bib129)], the authors in [[130](#bib.bib130)]
    propose a distributed single- and multi-agent DQN-based DRL schemes to address
    the spectrum sharing problem in V2X networks. In their proposed system, multiple
    V2V links reuse the frequency spectrum preoccupied with V2I links. The agents
    are the V2V links whose action space is discrete, corresponding to spectrum sub-band
    and power selection. Each agent’s local observation space includes local channel
    information (such as its own signal channel gain, interference channels from other
    V2V transmitters, interference channel from its own transmitter to the BS, and
    the interference channel from all V2I transmitters), the remaining V2V payload,
    and the remaining time budget. The reward is continuous, which is a function of
    both the instantaneous sum capacity of all V2I links and the effective V2V transmission
    rate until the payload is delivered. Experimental results show that the agents
    cooperatively learn a policy that enables them to simultaneously improve the sum
    capacity of V2I links and payload delivery rate of V2V links. The authors also
    show that their proposed models for the single-agent and multi-agent settings
    provide very close performance to the conventional exhaustive search.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[131](#bib.bib131)] propose a single-agent DQN model to address
    the joint channel access and packet forwarding in a multi-sensor scenario. In
    the proposed scheme, one sensor is the agent, which acts as a relay to forward
    packets arriving from its surrounding sensors to the sink. The agent has a buffer
    to store arriving packets. The agent’s action is to choose channels for the packet
    forwarding, the packets transmitted on these channels, and a modulation scheme
    at each time slot to maximize its utility (defined as the ratio of the transmitted
    packets number to the transmit power). The state is the combination of the buffer
    and channel states. The input of the DQL model is the state, while the output
    is the corresponding action selection. Simulation results demonstrate that the
    proposed DQL scheme enhances system utility (i.e., 0.63) compared to the conventional
    random action selection scheme (i.e., 0.37).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Khan et al. [[132](#bib.bib132)] propose a multi-agent A3C-based DRL to address
    the problem of vehicle-cell association in mmWave V2X networks. The agents are
    the RSUs whose action is discrete, corresponding to determining the optimal vehicle-RSU
    association for RSU. The state space is a hybrid (discrete and continuous) defined
    in terms of the last channel observations, rate threshold violation indicator,
    and experienced data rate of vehicles. The reward function is continuous, which
    is defined in terms of the average rate per vehicle and threshold rate. Using
    experimental results, it is shown that their proposed algorithm achieves around
    15% sum-rate gains and a 20% reduction in vehicular user outages compared to baseline
    approaches.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: The problem of dynamic spectrum access in CRNs is investigated in [[133](#bib.bib133)]
    through combining DRL and evolutionary game theory. In particular, uncooperative
    multi-agent DQN is considered in which the agents are the SUs whose action is
    discrete, corresponding to selecting the access channel. The state space is discrete,
    which includes two main parts; the channel selected by the agent and the utility
    obtained after transmission on the selected channel. The reward function is defined
    in terms of evolutionary game theory. Simulation results indicate the performance
    enhancement of their proposed algorithm over the case without learning in terms
    of average system capacity.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting work is presented in [[134](#bib.bib134)] in which the authors
    propose a multi-agent DQN-based DRL algorithm to address the problem of optimum
    multi-user access control in Non-Terrestrial Networks (NTNs). In their model,
    UEs are the independent agents that report their experiences and local observations
    to a centralized trainer controller located at the backhaul network. The latter
    will then utilize the collected experiences to update the global DQN parameter.
    The agent’s state space is continuous, comprised of the connected NT-BS of UEs
    at the previous time slots, the RSS of UEs, the number of connected UEs of each
    NT-BS, and the transmission rate of UEs. The action space is discrete, representing
    the binary indicator functions of UEs, while the reward is a function of the transmission
    rate of UEs. Experimental performance results show that their proposed scheme
    is efficient in addressing the fundamental issues in the deployment of NTNs infrastructure,
    and it outperforms the traditional algorithms in terms of both the data rate and
    the number of handovers.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: In [[135](#bib.bib135)], the authors propose a multi-agent DQN-based DRL scheme
    to address the problem of dynamic joint spectrum access and mode selection (SAMS)
    in cognitive radio networks (CRNs). The agents are the secondary users (SUs) whose
    action space is discrete, corresponding to selecting the access channel and access
    mode. The state space of each SU agent is discrete, comprised of the action taken
    by the $m$th SU agent, the ACKs of all SUs agents, and the ACK of the $m$th SU
    agent. The reward function is discrete, which is ”1” if the action selection process
    is successful; otherwise, there is a collision, and the agent will receive a ”0”
    reward.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Tomovic et al. [[136](#bib.bib136)] propose a single-agent DRL model based on
    the integration of Double deep $Q$-learning architecture and RNN to address the
    problem of DSA in multi-channel wireless networks. In particular, the agent is
    the SU node, whose action space is discrete, representing the selection of a channel
    for sensing. The state space is also discrete, comprised of a history of the binary
    observations and history of taken actions. The reward function is a discrete binary
    function, which is ”1” if the observation is ”1” and ”0” otherwise. Simulation
    results show that their proposed method is able to find a near-optimal policy
    in a smaller number of iterations, and it can support a wide range of communication
    environment conditions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: In other work in [[137](#bib.bib137)], the authors propose both a single-agent
    and multi-agent deep actor-critic DRL-based framework for dynamic multi-channel
    access in wireless networks. In their system, the agents are the users whose action
    space is discrete, corresponding to selecting a channel. The observation space
    is also discrete, which is defined based on the status of the channel and collision
    status. The reward function is discrete, which is ”+1” if the selected channel
    is good; otherwise, it is ”-1”. Using simulation results, the authors show that
    their proposed actor-critic framework outperforms the DQN-based algorithm, random
    access, and the optimal policy when there is full knowledge of the channel dynamics.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: The problem of DSA for the CRN is studied in [[138](#bib.bib138)] based on an
    uncoordinated and distributed multi-agent DQN model. The agents are CRs, whose
    action is discrete, representing the possible transmit powers. The state space
    is discrete, reflecting whether the limits for DSA are being met or not, depending
    on the relative throughput change at all the primary network links. The reward
    is also discrete, which is a function of the throughput of the links and the environment’s
    state. Experimental results reveal that their proposed scheme finds policies that
    yield performance within 3% of an exhaustive search solution, and it finds the
    optimal policy in nearly 70% of cases.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Industrial IoT (IIoT) has emerged recently as an innovative networking ecosystem
    that facilitates data collection and exchange in order to improve network efficiency,
    productivity, and other economic benefits [[139](#bib.bib139)]. RRAM in such a
    sophisticated ecosystem is also a challenge that needs more investigation. In
    this context, recently in [[140](#bib.bib140)], the authors propose a solution
    for spectrum resource management for IIoT networks, with the goal of enabling
    spectrum sharing between different kinds of UEs. In particular, a single-agent
    DQN algorithm is proposed in which the agent is the BS. The action space is discrete,
    which corresponds to the allocation of spectrum resources for all UEs. The observation
    space is a hybrid (continuous and discrete) consisting of four elements; the current
    action (i.e., the allocation of spectrum resources), the data priority of type
    I UEs, the buffer length of type II UE, and the communication status of the first
    type of UEs. The reward function is continuous, defined to address their optimization
    problem. It is divided into four objectives; 1) maximizing the spectrum resource
    utilization; 2) quickly transmitting the high-priority data; 3) meeting the threshold
    rate requirement of the first type of UEs; 4) ensuring that the second type of
    UEs completes the transmission in time. Using simulation results, it is demonstrated
    that their proposed algorithm achieves better network performance with a faster
    convergence rate compared with other algorithms.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: In [[141](#bib.bib141)], the authors propose a multi-agent Double DQN-based
    DRL model to address the problem of DSA in distributed wireless networks. In particular,
    they design a channel access scheme to maximize channel throughput regarding fair
    channel access. The agents in their scheme are the users. The action space is
    discrete, which is ”0” if the user does not attempt to transmit packets during
    the current time slot, and it is ”1” if it has attempted to transmit. The state
    space is discrete, consisting of four main elements; each user action taken on
    the current time slot, channel capacity (which could be negative, positive, or
    zero), a binary acknowledgment signal showing if the user transmits successfully
    or not, and a parameter that enables each user to estimate other users’ situations.
    The reward is a discrete binary function that takes the value of ”1” if the user
    transmits successfully; otherwise, it is ”0” meaning that the user transmitted
    with collision. It is shown using simulation results that their scheme can maximize
    the total throughput while trying to make fair resource allocation among users.
    Also, it is shown that their proposed scheme outperforms the conventional Slotted-Aloha
    scheme in terms of sum-throughput.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Wang et al. [[142](#bib.bib142)] address the problem of DSA in VANETs, by proposing
    an interesting scheme that combines multi-hop forwarding via vehicles and DSA.
    The optimal DSA policy is defined to be the joint maximization of channel utilization
    and minimization of the packet loss rate. A multi-agent DRL network structure
    is presented that combines RNN and DQN for learning the time-varying process of
    the system. In their scheme, each user acts as an agent whose action space is
    discrete, corresponding to choosing a channel for transmission at time slot $t$.
    The state space is discrete, composed of three components; a binary transmission
    condition $\eta$, which is ”1” if the transmission is successful and ”0” otherwise,
    the channel selection action, and the channel status indicator after each dynamic
    access process. The reward is a discrete binary function, which takes a positive
    value if $\eta=1$, otherwise it takes the value of ”0”. Simulation results show
    that their proposed scheme: 1) reduces the packet loss rate from 0.8 to around
    0.1, 2) outperforms Slotted-Aloha and DQN in terms of reducing collision probability
    and channel idle probability by about 60%, and 3) enhances the transmission success
    rate by around 20%.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Most recently in [[143](#bib.bib143)], the authors propose multi-agent DQN-based
    DRL schemes to address the problem of joint cooperative spectrum sensing and channel
    access in clustered cognitive UAV (CUAV) communication Networks. Three algorithms
    are proposed: 1) a time slot multi-round revisit exhaustive search based on virtual
    controller (VC-EXH), 2) a $Q$-learning based on independent learner (IL-Q) and
    3) a DQN based on independent learner (IL-DQN). The agents are the CUAVs in the
    network. The action space of any CUAV agent is a discrete function defined by
    the steps that this agent moves clockwise in time slot $t$ relative to the channel
    selected in time slot $t-1$ on the PU channel ring. The state space is a discrete
    set consisting of two main elements: 1) the number of CUAVs agents that have selected
    a particular channel to sense and access in the previous time slot and 2) a binary
    indicator function that shows the occupancy status of a particular channel in
    the previous time slot. The reward is a discrete function defined in terms of
    spectrum sensing, channel access, utility, and cost. Experimental results show
    that all the three algorithms proposed show efficient results in terms of convergence
    speed and the enhancement of utilization of idle spectrum resources.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[144](#bib.bib144)] propose a multi-agent deep recurrent Q-network-based
    DRL model to address the problem of DSA in dynamic heterogeneous environments
    with partial observations. In their work, the authors consider a case-study with
    multiple heterogeneous PUs sharing multiple independent radio channels. The agents
    are the SUs, whose action space is discrete, corresponding to deciding whether
    to transmit in a particular band or wait during the next time slot. The state
    space is discrete, representing whether the channels are occupied, idle, or unknown.
    The reward function is discrete, represented by two values; 100 per channel for
    successful transmission and -50 per channel for collision. Using simulation results,
    the authors show that their proposed algorithm handles various dynamic communication
    environments, and its performance outperforms the myopic conventional methods
    and very close to the optimization-based approaches.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 In Satellite Networks
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following paragraphs, we review works that employ DRL algorithms to address
    the spectrum and access control problem in satellite networks and emerging satellite
    IoT systems.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: The work in [[145](#bib.bib145)] proposes a single-agent DQN-based DRL algorithm
    that considers the problem of channel assignment in multi-beam satellite systems.
    In their scheme, the agent is the satellite, whose action is discrete, including
    an index that indicates the channel that the newly arrived user has occupied.
    The agent’s reward is discrete, which contains a positive value if the service
    is satisfied, and a negative value if the service is not satisfied or blocked.
    The state space is also discrete, which comprises three elements; the current
    users, the current channel assignment matrix, and a list of the new user arrivals.
    Experimental results demonstrate that their proposed scheme decreases the blocking
    probability and improves the carried traffic up to 24.4% as well as enhances the
    spectrum efficiency compared to the conventional fixed channel assignment approach.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[146](#bib.bib146)] propose a single-agent DQN-based DRL algorithm
    to address the problem of dynamic channel allocation in multi-beam satellite systems.
    In particular, an image-like tensor formulation on the system environments is
    considered in order to extract traffic spatial and temporal features. The agent
    in their model is the satellite, whose action space is discrete, corresponding
    to determining the resource allocation schemes. The state space is continuous,
    consisting of two elements; the system resource allocation state and the users’
    service request state. The reward function is discrete, which is defined in terms
    of the optimization objective function.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the authors in [[147](#bib.bib147)] propose a single-agent DQN-based
    DRL approach for energy-efficient channel allocation in SIoT. The agent in their
    model is the LEO satellite, whose action space is discrete, corresponding to mapping
    from newly coming node tasks to channels to be allocated. The state space is discrete,
    including information about user tasks, such as the size and location of tasks.
    The reward is continuous, which is divided into two normalized reward function
    components; the power efficiency reward and the normalized value of the service
    blocking rate. Both of these reward components are functions of power set up by
    the agent, the optimal power decided by the location of the beam, and the number
    of served nodes. Experimental results demonstrate that their proposed algorithm
    saves energy consumption of around 67.86% compared to some conventional approaches.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: In [[148](#bib.bib148)], the authors propose a centralized single-agent DQN-based
    scheme to address the problem of dynamic channel allocation in SIoT. The agent
    in their model is the satellite, whose action is discrete, corresponding to selecting
    which sensors to allocate channels to. The state space is discrete, comprised
    of three parts; the number of tasks in each time step, the bandwidth that a sensor
    node requires, and the duration of a new task. The reward is continuous, which
    is a function of the duration of data transmission for the sensor. Using simulation
    results, it is shown that their proposed algorithm both provides higher transmission
    success rates and reduces data transmission latency by at least 87.4% compared
    to the conventional channel allocation algorithms.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Zheng et al. [[149](#bib.bib149)] propose a single-agent $Q$-learning-based
    RL model to address the problem of combination allocation of fixed channel pre-allocation
    and dynamic channel scheduling in a network architecture of LEO satellites that
    utilizes a centralized resource pool. In their model, the satellite serves as
    an agent whose action is discrete, corresponding to assigning channels to users.
    The state space is discrete, defined by the channel assignment of users in each
    beam. The reward is continuous, which is a function of the user’s supply-demand
    ratio. Experimental results demonstrate that their proposed approach enhances
    system supply-demand ratio by 14% and 18% compared to the static channel allocation
    the Lagrange algorithm channel allocation method.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: IV-B4 In Multi-RAT HetNets
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the spectrum and access control problem in multi-RAT HetNets. This includes
    the coexistence of various variants of the wireless networks shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: In [[150](#bib.bib150)], a multi-agent DQN-based model that jointly tackles
    the dynamic channel selection and interference management in Small Base Stations
    (SBSs) cellular networks that share a set of unlicensed channels in Long Term
    Evolution (LTE) networks. In the proposed scheme, the SBSs are the agents who
    choose one of the available channels for transmitting packets in each time slot.
    The agent’s action is channel access and channel selection probability. The DQL
    input includes the channels’ traffic history of both the SBSs and Wireless Local
    Area Networks (WLAN), while the output is the agent’s predicted action vectors.
    Simulation results reveal that their proposed DQL strategy enhances the average
    data rate by up to 28% when compared to the conventional $Q$-learning scheme.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: In [[151](#bib.bib151)], a single-agent DQN-based model is proposed to tackle
    the dynamic spectrum allocation for multiple users that share a set of $K$ channels
    in the same network settings of [[150](#bib.bib150)]. In their scheme, the agent
    is the user whose action is either choosing a channel with a particular attempt
    probability or selecting not to transmit. The agent’s state includes the history
    of the actions of the agent and its current observations. The DQL model input
    is the previous actions along with their observations, while the output is the
    Q-values corresponding to the actions. Simulation results demonstrate that their
    proposed DQL strategy achieves a double data rate compared to the state-of-the-art
    Slotted-Aloha scheme.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. [[152](#bib.bib152)] propose a single-agent prediction-DDPG-based
    DRL algorithm to study the problem of the dynamic multichannel access (MCA) for
    the hybrid long-term evolution and wireless local area network (LTE-WLAN) aggregation
    in dynamic HetNets. The agent is the central BS controller, whose state space
    is continuous, consisting of both the channels’ service rates and the users’ requirement
    rates. The action space, on the other hand, is discrete, representing the users’
    index. Two reward functions are provided; online traffic real reward and online
    traffic prediction reward, each of which are functions of users’ requirements,
    channels’ supplies, degree of system fluctuation, the relative resource utilization,
    and the quality of user experience. Using simulation results, the authors demonstrate
    the efficiency of the proposed prediction-DDPG model in solving the dynamic MCA
    problem compared to conventional methods.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Other interesting work in [[153](#bib.bib153)], the authors investigate the
    joint allocation of the spectrum, computing, and storing resources in multi-access
    edge computing (MEC)-based vehicular networks. In particular, the authors propose
    multi-agent DDPG-based DRL algorithms to address the problem in a hierarchical
    fashion considering a network comprised of macro eNodeB (MeNB) and Wi-Fi APs.
    The agents are the controller installed at MEC servers. The agents’ action space
    is discrete including the spectrum slicing ratio set, spectrum allocation fraction
    sets for the MeNB and for each Wi-Fi AP, computing resource allocation fraction,
    and storing resource allocation fraction. The state space is discrete representing
    information of the vehicles within the coverage area of the MEC server, including
    vehicles’ number, x-y coordinates, moving state, position, and task information.
    The reward function is discrete, defined in terms of the delay requirement, and
    requested storing resources required to guarantee the QoS demands of an offloaded
    task. Provided experimental results reveal that their proposed schemes achieve
    high QoS satisfaction ratios compared with the random assignment techniques.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently in [[154](#bib.bib154)], the authors propose a single-agent DQN algorithm
    based on Monte Carlo Tree Search (MCTS) to address the problem of dynamic spectrum
    sharing between 4G LTE and 5G NR systems. In particular, the authors used the
    MuZero algorithm to enable a proactive BW split between 4G LTE and 5G NR. The
    agent is a controller located at the network core, whose action space is discrete,
    corresponding to a horizontal line splitting the BW to both 4G LTE and 5G NR.
    The state space is discrete, defined by five elements: 1) an indicator if the
    user is an NR user or not, 2) the number of bits in the user’s buffer, 3) an indicator
    of whether the user is configured with multimedia broadcast single frequency network
    (MBSFN) or not, 4) the number of bits that can be transmitted for the user in
    a given subframe, and 5) the number of bits that will arrive for each user in
    the future subframes. The reward function is a continuous function defined as
    a summation of the exponential of the delayed packet per user. Experimental results
    show that their proposed scheme provides comparable performance to the state-of-the-art
    optimal solutions.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis and Reflections
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section reviews the applications of DRL for dynamic spectrum allocation
    and access control in modern wireless networks. These types of radio resources
    are inherently coupled with user association, network/RAT selection, dynamic multi-channel
    access, and DSA. Table [VI](#S4.T6 "TABLE VI ‣ Synthesis and Reflections ‣ IV-B4
    In Multi-RAT HetNets ‣ IV-B DRL for Spectrum Allocation and Access Control ‣ IV
    DRL-Based Radio Resource Allocation and Management for Next Generation Wireless
    Networks ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management
    in Next Generation Heterogeneous Wireless Networks: A Survey") summarizes the
    reviewed papers in this section. In general, the application of DRL for spectrum
    allocation and access control problems has received considerable attention lately.
    We observe that most DRL algorithms, when deployed for non-IoT networks, are implemented
    in centralized fashions at network controllers, such as BSs, RSUs, and satellites
    [[116](#bib.bib116), [132](#bib.bib132), [145](#bib.bib145)]. This is done to
    utilize the controllers’ powerful and advanced hardware capabilities in collecting
    network information and designing cross-layer policies. Hence, we observe that
    DRL models are deployed as a single-agent at the network controllers [[140](#bib.bib140)].
    On the contrary, DRL provides a flexible tool in diversified IoT networks and
    systems, conventionally involving dynamic system modeling and multi-agent interactions,
    such as CRNs and distributed systems.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the management of such types of radio resources falls in the discrete
    action space. Therefore, the value-based algorithms are utilized more than the
    policy-based ones. We also observe from Table [VI](#S4.T6 "TABLE VI ‣ Synthesis
    and Reflections ‣ IV-B4 In Multi-RAT HetNets ‣ IV-B DRL for Spectrum Allocation
    and Access Control ‣ IV DRL-Based Radio Resource Allocation and Management for
    Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio Resource
    Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") that the use of DRL techniques for IoT and emerging wireless networks
    receives more attention than other networks, especially for the cognitive radio-based
    systems as in [[143](#bib.bib143)].'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: The exponential increase in smart IoT devices mandates to take autonomous decisions
    locally, especially for delay-sensitive IoT applications and services. In this
    context, we anticipate that the research on spectrum allocation and access control
    using distributed multi-agent DRL algorithms for future IoT networks will attract
    more attention as in [[130](#bib.bib130), [132](#bib.bib132), [142](#bib.bib142),
    [143](#bib.bib143)]
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: A Summary List of Papers Related to DRL for Spectrum Allocation and
    Access Control.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Learning Algorithm |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| Network Type | Ref. | Radio Resource (or Issues Addressed) | Mode | Algorithm
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| Cellular & HomNets | UAV-assisted LTE | Chen et al.[[113](#bib.bib113)] |
    Joint user association, spectrum allocation, & content caching | Multi-agent |
    DQN |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| Macro & femto BS | Zhao et al. [[115](#bib.bib115), [114](#bib.bib114)] |
    Joint BS & channel selections | Multi-agent | Dueling DDQN |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| Cellular IoT | Zhang et al.[[116](#bib.bib116)] | User association | Single-agent
    | DQN |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| IAB cellular | Lei et al.[[117](#bib.bib117)] | Dynamic spectrum allocation
    | Single-agent | DDQN & actor-critic |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| CV2X | Li et al.[[118](#bib.bib118)] | User association | Multi-agent | $Q$-learning
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| 5G HetNets | Perez et al.[[119](#bib.bib119)] | User association | Multi-agent
    | $Q$-learning |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| Hybrid NOMA-based 5G | Zheng et al.[[120](#bib.bib120)] | Dynamic spectrum
    allocation | Single-agent | actor-critic |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| Wireless DSA | Song et al.[[121](#bib.bib121)] | Dynamic spectrum allocation
    | Multi-agent | DQN |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '|   IoT & Other Emerging Wireless Networks | IoT sensor networks | Wang et
    al.[[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)] | Dynamic multi-channel
    access | Single-agent | DQN |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Energy harvesting-based IoT sensors | Chu et al.[[126](#bib.bib126)] | Dynamic
    spectrum allocation | Single-agent | DQN |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Zhang et al.[[127](#bib.bib127)] | Dynamic multi-channel access |
    Multi-agent | DQN |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| Wireless networks | Xu et al.[[128](#bib.bib128)] | DSA | Single-agent |
    DQN & DDQN |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| V2X | Liang et al.[[130](#bib.bib130), [129](#bib.bib129)] | Dynamic spectrum
    sharing | Single-& multi-agent | DQN |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| Multi-sensor scenario | Zhu et al.[[131](#bib.bib131)] | Joint channel access
    & packet forwarding | Single-agent | DQN |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| mmWave V2X | Khan et al.[[132](#bib.bib132)] | User association | Multi-agent
    | A3C |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Yang et al.[[133](#bib.bib133)] | DSA | Multi-agent | DQN |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| NTNs | Cao et al.[[134](#bib.bib134)] | Dynamic multi-channel access | Multi-agent
    | DQN |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Yang et al.[[135](#bib.bib135)] | Joint dynamic spectrum access &
    mode selection | Multi-agent | DQN |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| Multi-channel wireless networks | Tomovic et al.[[136](#bib.bib136)] | DSA
    | Single-agent | RNN-base DDQN |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| Wireless networks | Zhong et al.[[137](#bib.bib137)] | Dynamic multi-channel
    access | Single-& multi-agent | actor-critic |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Tondwalkar et al.[[138](#bib.bib138)] | DSA | Multi-agent | DQN |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| IIoT networks | Shi et al.[[140](#bib.bib140)] | Dynamic spectrum sharing
    | Single-agent | DQN |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Distributed wireless networks | Janiar et al.[[141](#bib.bib141)] | DSA |
    Multi-agent | DDQN |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| VANETs | Wang et al.[[142](#bib.bib142)] | DSA | Multi-agent | RNN-based
    DQN |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| CUAV | Jiang et al.[[143](#bib.bib143)] | Joint spectrum sensing & channel
    access | Multi-agent | DQN |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| Heterogeneous environments | Xu et al.[[144](#bib.bib144)] | DSA | Multi-agent
    | RNN-based DQN |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '|   Satellite Nets | Multi-beam satellite systems | Liu et al.[[145](#bib.bib145)]
    | Dynamic spectrum allocation | Single-agent | DQN |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| Multi-beam satellite systems | Hu et al.[[146](#bib.bib146)] | Dynamic spectrum
    allocation | Single-agent | DQN |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| SIoT | Zhao et al.[[147](#bib.bib147)] | Dynamic spectrum allocation | Single-agent
    | DQN |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| SIoT | Liu et al.[[148](#bib.bib148)] | Dynamic spectrum allocation | Single-agent
    | DQN |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '|  | LEO satellites | Zheng et al.[[149](#bib.bib149)] | Joint channel pre-allocation
    & dynamic channel scheduling | Single-agent | $Q$-learning |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '|   Multi-RAT | Small BSs cellular | Challita et al.[[150](#bib.bib150)] |
    Joint dynamic channel selection & interference management | Multi-agent | DQN
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| Small BSs cellular | Naparstek et al.[[151](#bib.bib151)] | Dynamic spectrum
    allocation | Single-agent | DQN |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| LTE-WLAN HetNets | Wang et al.[[152](#bib.bib152)] | Dynamic multi-channel
    access | Single-agent | prediction-DDPG |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| MEC-based V2X | Peng et al.[[153](#bib.bib153)] | Joint allocation of spectrum,
    computing, & storing | Multi-agent | DDPG |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| 4G LTE and 5G NR systems | Challita et al.[[154](#bib.bib154)] | Dynamic
    spectrum sharing | Single-agent | DQN |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: IV-C DRL for Rate Control
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This refers to the adaptive rate control in the uplink and downlink of wireless
    networks. With the explosive increase in the number of user devices and the emergence
    of massive types of data-hungry applications [[3](#bib.bib3)], it becomes essential
    to keep high network KPIs in terms of data rates and users’ QoE. Adaptive rate
    control schemes must ensure satisfactory QoS in highly dynamic and unpredictable
    wireless environments. In this context, DRL techniques can be efficiently deployed
    to solve adaptive rate control problems instead of conventional approaches that
    possess high complexity and heavily rely on accurate network information and instantaneous
    CSI.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: In the following paragraphs, we review works that employ DRL algorithms to address
    the rate control issue in cellular and HomNets.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[155](#bib.bib155)] address the problem of network resource allocation,
    in terms of rate, for 5G network slices. The problem is decomposed into a master-slave,
    and a multi-agent DDPG-based DRL scheme is then proposed to solve it. The agents
    are located in every network slice, whose action space is continuous, defining
    the resource allocation to users in the network slice. The state space is continuous
    and has two main parts; the first one shows how much utility the user obtained
    compared to its minimum utility requirement, while the second part shows the auxiliary
    and dual variables from the master problem. The reward is a continuous function
    defined in terms of utility, utility requirements, and auxiliary and dual variables.
    Simulation results demonstrate that their proposed algorithm outperforms the baseline
    approaches and gives a near-optimal solution.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: In [[156](#bib.bib156)], the authors propose a single-agent DQN-based DRL algorithm
    to address the problem of dynamic uplink/downlink radio resources allocation in
    terms of network capacity in high mobility 5G HetNets. Their proposed algorithm
    is based on Time Division Duplex (TDD )configuration in which the agent is the
    BS, whose action space is discrete, corresponding to the configurations of TDD
    sub-frame allocation at the BS. The sate space is discrete, comprised of different
    kinds of features of the BS, including uplink/downlink occupancy, buffer occupancy,
    and channel condition of all uplinks/downlinks to the BS. The reward is discrete,
    defined as a function of the uplink and downlink channel utility, which mainly
    depends on the channel occupancy with chosen TDD configuration. Using experimental
    results, the authors show that their proposed algorithm achieves performance improvement
    in terms of both network throughput and packet loss rate, compared to some conventional
    TDD resource allocation algorithms.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis and Reflections
  id: totrans-448
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This section reviews the use of DRL techniques for adaptive rate control in
    modern wireless networks. In general, there is limited research that is solely
    dedicated to addressing the rate radio resource. Most of the research is dedicated
    to video streaming applications, as summarized in [[15](#bib.bib15)]. However,
    as we discussed in the previous sections, the data rate control issue is typically
    addressed via controlling other radio resources such as power and spectrum. In
    addition, the adaptive rate control is typically addressed as a joint optimization
    with other radio resources, as we will elaborate in the next section, e.g., as
    in [[157](#bib.bib157), [158](#bib.bib158)].
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: We also observe that DRL approaches for cellular and HomNets receive more attention
    than other wireless networks, and there is a lack of research on adaptive rate
    control for IoT and satellite networks. This also deserves more in-depth investigation
    and analysis.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: IV-D DRL for Joint RRAM
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the massive complexity and large-scale nature of modern wireless networks,
    it becomes necessary to design efficient schemes that account for the joint radio
    resources. In many scenarios, the design problem in wireless networks might end
    up with competing objectives. For example, in UDNs, increasing the power level
    is beneficial in combating path loss and enhancing the received signal quality.
    However, this might cause serious interference to the neighboring user devices
    and BSs. Hence, the joint design of power level control and interference management
    becomes mandatory. Conventional approaches for solving joint RRAM problems require
    complete and instantaneous knowledge about network statistics, such as traffic
    load, channel quality, and radio resources availability. However, obtaining such
    information is not possible in such large-scale networks. In this context, DRL
    techniques can be adopted to learn system dynamics and communication context to
    overcome the limited knowledge of wireless parameters.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'This section intensively reviews the works that implement DRL algorithms for
    the problem of joint RRAM in modern wireless networks. Particularly, we present
    related works that jointly optimize the radio resources shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey"), such as power allocation, spectrum resources, user
    association, dynamic channel access, cell selection, etc.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: IV-D1 In Cellular and HomNets
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the joint RRAM issue in cellular and HomNets shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: The work in [[159](#bib.bib159)] presents a single-agent DDPG-based DRL model
    to study the joint optimization of user mode selection, bandwidth allocation,
    power control, and channel selection with the aim for maximizing the EE in D2D-enabled
    cellular HetNets. The agent is a controller in the macrocell, whose state space
    is continuous, representing the user’s QoS satisfaction degree. The action space
    is continuous, which is to select power, channel, and bandwidth allocation factor,
    while the reward is a continuous function defined by the system EE. Experimental
    results demonstrate the efficiency of their proposed algorithm in terms of both
    the convergence speed and improving the EE in a D2D-enabled HetNets compared with
    other benchmark schemes.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: In another interesting work in [[160](#bib.bib160)], the authors study the problem
    of joint optimization of transmission mode selection and resource allocation for
    CV2X. They propose single-agent settings in which DQN and federated learning (FL)
    models are integrated to improve the model’s robustness. The agent in their model
    is each V2V pair. The action space is discrete, representing the resource block
    (RB) allocation, communication mode selection, and transmit power level of the
    V2V transmitter. The state space is a hybrid (continuous and discrete) consisting
    of five main parts; the received interference power at the V2V receiver and the
    BS on each RB at the previous subframe, the number of selected neighbors on each
    RB at the previous subframe, the large-scale channel gains from the V2V transmitter
    to its corresponding V2V receiver and the BS, current load, and remaining time
    to meet the latency threshold. The reward is a continuous function defined in
    terms of the sum-capacity of vehicular UEs as well as the QoS requirements of
    both vehicular UEs and V2V pairs. Using experimental results, the authors show
    that their proposed two-timescale federated DRL algorithm outperforms other decentralized
    baselines.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Jang et al. [[161](#bib.bib161)] propose a multi-agent DQN-based algorithm to
    address the problem of sum-rate maximization via a joint optimization of resource
    allocation and power control in small cell wireless networks. The agents in their
    proposed model are the small cell BSs, whose action space is discrete, corresponding
    to selecting the resource allocation and power control of small BS on RB. The
    state space is continuous, including all the CSI that the small BS collects on
    RB, such as local CSI, local CSI at the transmitter, etc. The reward is a continuous
    function expressed by the average sum-rate of its own serving users and the other
    small BSs. Experimental results show that their proposed approach both outperforms
    the conventional algorithms under the same CSI assumptions and provides a flexible
    tradeoff between the amount of CSI and the achievable sum-rate.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: In another interesting work in [[162](#bib.bib162)], the authors propose a model-driven
    multi-agent Double DQN-based framework for resource allocation in UDNs. In particular,
    They first develop a DNN-based optimization framework comprised of a series of
    ADMM iterative procedures that uses the CSI as the learned weights. Then, a channel
    information absent $Q$-learning resource allocation algorithm is presented to
    train the DNN-based optimization scheme without massive labeling data, where the
    EE, SE, and fairness are jointly optimized. The agents are each D2D transmitter,
    whose action space is discrete, corresponding to selecting a subcarrier and corresponding
    transmission power. The state space is a hybrid (continuous and discrete) consisting
    of two parts; user association information and interference power. The reward
    function is continuous, comprised of two components; the network EE and the fairness
    of service quality, which is expressed by the variance of throughput between authorized
    users. Using experimental results, it is demonstrated that their proposed algorithm
    has a rapid convergence speed, well characterizes the extent of optimization objective
    with partial CSI, and outperforms other existing resource allocation algorithms.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[163](#bib.bib163)] propose a single-agent Dueling DQN-based DRL
    model to address the problem of joint energy efficiency (EE) and spectral efficiency
    (SE) in 5G ultra-dense networks (UDNs). In their approach, the agent is a macro
    gNodeB (MgNB) whose state space is continuous, comprised of the number and throughput
    of all small cells as well as the allocation of all resource blocks (RBs) in the
    network. The agent’s action space is to define which RBs are reused by the small
    cell in the network, while the reward is a function of the tradeoff between SE
    and EE. Experimental results demonstrate that their proposed algorithm performs
    better than both the conventional $Q$-learning and DQN.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in [[164](#bib.bib164)] propose a multi-agent Double DQN-based
    scheme to address the problem of joint subcarrier assignment and power allocation
    in D2D underlying 5G cellular networks. The agents in their model are the D2D
    pairs, whose action space is discrete, corresponding to determining the transmit
    power allocation on the available subcarriers. The state space is a hybrid (continuous
    and discrete), comprised of four components: 1) local information (including the
    previous transmit power, previous SE achieved by transmitters, channel gain, and
    SINR), 2) the interference that each agent causes at the BS side, 3) the interference
    received from agent’s interfering neighbors and the SE achieved by agent’s neighbors,
    and 4) the interference that each agent causes to its neighbors. The reward is
    a continuous function comprised of three elements: 1) the SE achieved by each
    agent, 2) the SE degradation of the agent’s interfered neighbors, and 3) the penalty
    due to the interference generated at the BS. Experimental results show that their
    proposed algorithm outperforms both the exhaustive and random subcarrier and even
    power (RSEP) assignment methods in terms of SE of D2D pairs.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: In previous work in [[165](#bib.bib165)], the authors propose a multi-agent
    $Q$-learning-based scheme to address the problem of joint channel and power level
    selection in autonomous D2D-based heterogeneous cellular networks. Two spectrum
    usage scenarios are considered; when the D2D pairs transmit over the dedicated
    spectrum bands and when they shared cellular/D2D channels. The agents are the
    D2D pairs whose action space is discrete, defined as the set of their possible
    channel and power level decisions. The state space is also discrete, which takes
    the value of ”1” if the SINR is greater than some predefined threshold; otherwise,
    the value of the state is ”0”. The agent’s reward is a continuous function, defined
    by the current agent’s state and its partially observable actions. Experimental
    results show that their proposed algorithm has relatively fast convergence and
    near-optimal performance after a few number of iterations.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Most recently in [[166](#bib.bib166)], the authors propose a multi-agent DQN-based
    DRL scheme to address the problem of spectrum allocation and power control for
    Mission-critical communication (MCC) in 5G networks. In MCC, multiple D2D users
    reuse non-orthogonal wireless resources of cellular users without BS in order
    to enhance the network’s reliability. The paper aims to help the D2D users autonomously
    select the channel and allocate power to maximize system capacity and SE while
    minimizing interference to cellular users. The agents are the D2D transmitters
    whose action space is discrete, corresponding to channel and power level selection.
    The state space is discrete, defined in a three-dimensional matrix, which includes
    information on the channel state of uses, the state of power level, and the number
    of the D2D pairs. The reward function is discrete, defined in terms of the total
    system capacity and constraints. Simulation results show that their proposed learning
    approach significantly improves spectrum allocation and power control compared
    to traditional methods.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[167](#bib.bib167)] propose a multi-agent DQN-based model to
    address the problem of joint user association and power control in OFDMA-based
    wireless HetNet. The agents are the UEs, whose action space is discrete, corresponding
    to jointly associate with the BS and determine the transmit power. The state space
    is discrete, which is defined by the situation of all UEs association with BS
    and power control. The reward function is continuous, which is defined in terms
    of the sum-EE of all UEs. Using simulation results, it is shown that their proposed
    method outperforms the $Q$-learning method in terms of convergence and EE.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[168](#bib.bib168)] propose a decentralized multi-agent DQN-based
    scheme to address the problem of joint channel resource and transmit power in
    D2D underlay cellular networks. In their scheme, the agents are each D2D transmitter.
    The action space is discrete, corresponding to the selection of each D2D’s channel
    resource and transmit power for data transmission. The state space is discrete
    consisting of six elements; the transmit power level of the cellular users, the
    maximum transmit power of each D2D transmitter, the distances between the cellular
    users and the D2D receiver devices, distances between D2D transmit devices and
    D2D receiver devices, selected channel of other agents, and selected transmit
    power level of other agents. The reward is a continuous function defined by the
    total effective throughput of D2D pairs in the network. Simulation results show
    that their proposed algorithm achieves better performance in terms of the total
    effective data rate than the random resource allocation method.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: In [[169](#bib.bib169)], a single-agent DQN-based DRL model is proposed to address
    the problem of joint optimization of user association, resource allocation, and
    power allocation in HetNets. The agent is the BS, whose action is discrete, corresponding
    to power allocation to users. The state space is discrete, defined by the channel
    gain matrix and the set of users association. The reward function is continuous,
    defined by the utility function of users’ achieved data rate. Using simulation
    results, the authors show that their proposed algorithm outperforms some of the
    existing methods in terms of SE and convergence speed.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Huang et al. [[170](#bib.bib170)] propose a single-agent DQN model to address
    the problem of delay minimization via joint spectrum and power resource allocation
    in mmWave mobile hybrid access network. The agent is located in the roadside BS,
    whose action space is discrete, corresponding to allocating spectrum and power
    resources for data. The state space is discrete, consisting of information about
    the current power and spectrum of the resource pool, required spectrum and power,
    and the number of spectrum and power levels. The reward signal is a continuous
    function defined in terms of queueing delay and the resource length required for
    each data. Using simulation results, it is shown that their proposed model guarantees
    the URLLC delay constraint when the load does not exceed 130%, which outperforms
    other conventional methods such as random and greedy algorithms.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 In IoT and Other Emerging Wireless Networks
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the joint RRAM issue in IoT and emerging wireless networks depicted in
    Fig. [2](#S1.F2 "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement
    Learning for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: For the same system settings in [[102](#bib.bib102)], the authors in [[64](#bib.bib64)]
    extended their work and propose a multi-agent DDPG-based DRL framework to address
    the problem of the joint spectrum and power allocation in wireless networks. Two
    DRL-based algorithms are proposed, which are executed and trained simultaneously
    in two layers in order to jointly optimize the discrete subband selection and
    continuous power allocation. The agent in their approach is each transmitter.
    In the top layer, the action space of all agents is discrete, representing the
    discrete subband selection, while the bottom layer has a continuous action space
    corresponding to the transmit power allocation. The state space is a hybrid (continuous
    and discrete), containing information on achieved SE, transmit power, sub-band
    selection, rank, and downlink channel gain. The reward is shared by both layers,
    which is a continuous function defined in terms of the externality of agents to
    interference and the spectral efficiency. Using experimental results, the authors
    show that their proposed framework outperforms the conventional fractional programming
    algorithm.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Based on their initial work in [[171](#bib.bib171)], the authors in [[172](#bib.bib172)]
    extended their work and propose a distributed multi-agent DQN-based DRL scheme
    to address the problem of joint channel selection and power control in D2D networks.
    The agents in their model are the D2D pairs, whose action space is discrete, corresponding
    to selecting a channel and a transmit power. The state space of each agent is
    a hybrid (continuous and discrete) which contains three sets of information; local
    information, non-local information from the agent’s receiver-neighbor set, and
    non-local information from the agent’s transmitter-neighbor set. The reward function
    of each agent is continuous, which is decomposed into the following elements;
    its own received signal power, its own total received SINR, its interference caused
    to transmitter-neighbors, the received signal power, and the total received SINR
    of transmitter-neighbors. Using simulation results, it is shown that the performance
    of their scheme closely approaches that of the FP-based algorithm even without
    knowing the instantaneous global CSI.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: In [[173](#bib.bib173)], the authors extended their previous work in [[174](#bib.bib174),
    [175](#bib.bib175)] and present a distributed multi-agent DQN-based model to address
    the problem of joint sub-band selection and power level control in V2V communication
    networks. Their proposed model is applicable to both unicast and broadcast scenarios.
    The agents are the V2V link or vehicles whose action space is discrete, corresponding
    to the selection of the frequency band and transmission power level that generate
    small interference to all V2V and V2I links while ensuring enough resources to
    meet latency constraints. The state space is continuous, containing the following
    information; the CSI of the V2I link, the received interference signal strength
    in the previous time slot, the channel indices selected by neighbors in the previous
    time slot, the remaining load for transmission, and the time left before violating
    the latency constraint. The reward function is continuous, consisting of three
    components; the capacity of V2I links, the capacity of V2V links, and the latency
    constraint. Using experimental results, it is shown that agents learn to satisfy
    the latency constraints on V2V links while minimizing the interference to V2I
    communications.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[176](#bib.bib176)] propose a single-agent Double DQN-based
    DRL to address the problem of joint channel selection and power allocation with
    network slicing in CRNs. The aim of their study is to provide high SE and QoS
    for cognitive users. The agent is the overall CRN, whose action space is discrete,
    corresponding to the channel selection and power allocation of SUs. The state
    space is continuous, defined by the SINR of the PU. The reward function is continuous,
    which is a function of the system SE, user QoS, interference temperature, and
    the interference temperature threshold. Experimental results show that their proposed
    algorithm improves the SE and QoS and provides faster convergence and more stable
    performance compared to the $Q$-learning and DQN algorithms.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Summary of the Related Works that Address the Joint RRAM.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Learning Algorithm |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| Network Type | Ref. | Types of Joint Radio Resources (or Issues Addressed)
    | Mode | Algorithm |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| Cellular & HomNets | D2D-enabled cellular | Zhang et al.[[159](#bib.bib159)]
    | User mode selection, bandwidth allocation, power control, & channel selection
    | Single-agent | DDPG |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| CV2X | Zhang et al.[[160](#bib.bib160)] | Transmission mode selection & resource
    allocation | Single-agent | DQN |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| Small cell networks | Jang et al.[[161](#bib.bib161)] | Resource allocation
    & power control | Multi-agent | DQN |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| UDNs | Liao et al.[[162](#bib.bib162)] | Subcarrier selection & transmission
    power | Multi-agent | DDQN |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| 5G UDNs | Liu et al.[[163](#bib.bib163)] | EE and SE | Single-agent | Dueling
    DQN |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| D2D underlying 5G cellular | Zhang et al.[[164](#bib.bib164)] | Subcarrier
    assignment & power allocation | Multi-agent | DDQN |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| D2D-based heterogeneous cellular | Asheralieva et al.[[165](#bib.bib165)]
    | Channel & power level selection | Multi-agent | $Q$-learning |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| Mission-critical in 5G | Wang et al.[[166](#bib.bib166)] | Spectrum allocation
    & power control | Multi-agent | DQN |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| OFDMA-based networks | Ding et al.[[167](#bib.bib167)] | User association
    & power control | Multi-agent | DQN |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| D2D underlay cellular | Yu et al.[[168](#bib.bib168)] | Channel resource
    & transmit power | Multi-agent | DQN |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| HetNets Cellular | Zhang et al.[[169](#bib.bib169)] | User association, resource
    allocation, & power allocation | Single-agent | DQN |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| mmWave mobile hybrid access | Huang et al.[[170](#bib.bib170)] | Spectrum
    & power resource allocation | Single-agent | DQN |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '|   IoT & Emerging Nets | Wireless networks | Nasir et al.[[64](#bib.bib64)]
    | Spectrum & power allocation | Multi-agent | DDPG |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
- en: '| D2D networks | Tan et al.[[172](#bib.bib172), [171](#bib.bib171)] | Channel
    selection & power control | Multi-agent | DQN |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| V2V networks | Ye et al.[[173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)]
    | Sub-band selection & power level control | Multi-agent | DQN |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| CRNs | Yuan et al.[[176](#bib.bib176)] | Channel selection & power allocation
    | Single-agent | DDQN |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| 5G-based NOMA systems | Zhang et al.[[177](#bib.bib177)] | Subcarrier assignment
    & power allocation | Multi-agent | DQN & DDPG |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| Multi-carrier NOMA-based systems | He et al.[[178](#bib.bib178)] | Channel
    assignment & power allocation | Single-agent | DQN |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| Wireless networks | Jiang et al.[[179](#bib.bib179)] | Channel selection
    & power allocation | Multi-agent | DQN |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| NOMA-based V2X networks | Xu et al.[[180](#bib.bib180)] | Spectrum & power
    allocation | Multi-agent | DDPG |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| UAV-assisted IoT networks | Munaye et al.[[158](#bib.bib158)] | Bandwidth,
    throughput, & power | Multi-agent | DQN |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '|   Multi-RAT | Hybrid RF/VLC systems | Shrivastava et al.[[181](#bib.bib181)]
    | Bandwidth, power, & user association | Multi-agent | DQN |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '|  | Multi-RAT HetNets | Alwarafy et al.[[39](#bib.bib39)] | RAT selection
    & power control | Single& multi-agent | DQN & DDPG |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '|  | Heterogeneous health systems | Chkirbene et al.[[157](#bib.bib157)] |
    RAT selection, data split control, & compression ratio control | Single-agent
    | DDPG |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: The problem of joint subcarrier assignment and power allocation in an uplink
    multi-user 5G-based NOMA systems is addressed in [[177](#bib.bib177)]. A multi-agent
    two-step DRL algorithm is proposed; the first step employs the DQN algorithm to
    output the optimum subcarrier assignment policy, while the second step employs
    the DDPG algorithm to dynamically allocate the transmit power for the network’s
    users. The agent is a controller located at the BS, whose action space is a hybrid
    (discrete and continuous), corresponding to the subcarrier assignment decisions
    and power allocation decisions. The state space is continuous, which is defined
    by the users’ channel gains at each subcarrier. The reward function is defined
    as the sum EE of the NOMA system. Experimental results show that their proposed
    algorithm provides better EE than the fixed and DQN-based power allocation schemes.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: In [[178](#bib.bib178)], the authors propose a single-agent DQN-based algorithm
    to address the problem of joint channel assignment and power allocation in multi-carrier
    NOMA-based systems. The authors used an attention-based neural network (ANN) to
    perform the channel assignment process. The agent in their approach is the BS,
    whose action is discrete, representing the selection of a channel for a user.
    The state space is discrete, characterized by the channel information that is
    defined by user-channel assignment pairs. The reward function is continuous, defined
    by the user’s achieved data rate. Using simulation results, the authors show that
    their proposed algorithm achieves better system performance in terms of sum-rate,
    compared to the state-of-the-art exhaustive search approaches and the method reported
    in [[182](#bib.bib182)].
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: In another work [[179](#bib.bib179)], the authors address the problem of dynamic
    optimization of channel selection and power allocation in wireless networks based
    on distributed multi-agent DQN algorithm. The authors adopted a partially distributed
    framework with the aim of minimizing the long-term average data backlog of the
    network’s users. In their model, the agents are the users whose action space is
    discrete and finite, representing the users’ transmit power allocation for data
    blocks in the data buffer. The state space is also discrete and finite comprised
    of two parts; the users’ data size and data block remaining lifetime. The reward
    function is continuous, defined in terms of the size of the remaining data in
    the data block and the transmitted data taken from the data block. Experimental
    results demonstrate that their proposed solution outperforms the conventional
    exhaustive search and Block Coordinate Descent-based optimization algorithms in
    terms of EE and data backlog.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[180](#bib.bib180)] propose a multi-agent DDPG-based model to
    address the problem of joint power and spectrum allocation in NOMA-based V2X networks.
    In particular, the authors are looking to maximize the sum-rate of V2I communications.
    The agents are the V2V communication links. The state space is discrete, defined
    by a set of actions performed by V2I and V2V communication links. The set includes
    the transmit power of both V2I and V2V links as well as the spectrum multiplexing
    factor of both V2V and V2V links. The state space is continuous, defined by five
    parts; the local channel gain information of each V2V link, interference channels
    from other V2V communication links, interference channel from each link’s own
    transmitter to the BS, interference channel from all V2I transmitters, and the
    state of queue length in the buffer of each V2V transmitter. The reward function
    is continuous, defined by the achieved sum-rate of V2I communication links and
    the delivery probability of V2V communication links. Compared with both the DQN
    algorithm and random resource allocation scheme, simulation results show that
    their proposed algorithm outperforms both of them in terms of maximizing the sum-rate
    of V2I communication links while meeting the latency and reliability requirements
    of V2V communications.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Munaye et al. [[158](#bib.bib158)] propose a multi-agent DQN-based DRL model
    to address the problem of joint radio resources of bandwidth, throughput, and
    power in UAV-assisted IoT networks. The agents are the UAVs, whose action space
    is discrete, corresponding to jointly selecting channel allocation of bandwidth,
    throughput, and power. The state space is discrete, comprising three components;
    the air-to-ground channel used by users, the rate of power consumption, and the
    interference. The reward is a discrete function, defined in terms of throughput,
    power allocation, bandwidth, and SINR levels. Simulation results show that their
    proposed algorithm outperforms other algorithms in terms of accuracy, convergence
    speed, and error.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: IV-D3 In Multi-RAT HetNets
  id: totrans-506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we review works that employ DRL algorithms to
    address the joint RRAM problem in multi-RAT HetNets. This includes the coexistence
    of various variants of the wireless networks as illustrated in Fig. [2](#S1.F2
    "Figure 2 ‣ I-C Paper Contributions ‣ I Introduction ‣ Deep Reinforcement Learning
    for Radio Resource Allocation and Management in Next Generation Heterogeneous
    Wireless Networks: A Survey").'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: Most recently, the authors in [[181](#bib.bib181)] present a multi-agent DQN-based
    algorithm to address the problem of joint optimization of bandwidth, power, and
    user association in hybrid RF/VLC systems. The APs are the agents whose action
    is discrete, representing the bandwidth, association function, and power level.
    The state space is discrete, which is a function of the problem constraints such
    as system bandwidth, association function, and power levels. The reward is discrete,
    which is a function of the rates delivered by the APs. Experimental results show
    that their algorithms improve the achievable sum-rate and number of iterations
    for convergence by 10% and 54% compared to that obtained using conventional optimization
    approaches.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting work is proposed by Alwarafy et al. in [[39](#bib.bib39)].
    In that work, the authors propose a hierarchical multi-agent DQN and DDPG-based
    algorithm to address the problem of sum-rate maximization in multi-RAT multi-connectivity
    wireless HetNets. The authors addressed the problem of multi-RATs assignment and
    continuous power allocation that maximize the network sum rate. In their model,
    single and multi-agents are proposed. The edge server acts as a single agent employed
    by DQN, while RATs APs behave as multi-agents employed by DDPG. For the single-agent
    DQN model, the action space is discrete, corresponding to the RATs-EDs assigning
    process. The state space of the DQN is continuous, comprised of the link gains
    and the required data rates of users. The reward function of the DQN agent is
    continuous, defined by the difference between the achieved rate and the required
    rate by users. For the multi-agent DDPG models, the action space is continuous,
    representing the power allocation of each RAT AP agent. The state space is a hybrid
    (continuous and discrete) consisting of four elements: the RATs-EDs assignment
    process performed by the DQN agent, the minimum data rate of users, the gains
    of the links, and the achieved data rate. Experimental results show that their
    algorithm’s performance is approximately 98.1% and 95.6% compared to the conventional
    CVXPY solver that assumes full knowledge of the wireless environment.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting work is presented in [[157](#bib.bib157)], in which the authors
    address the problem of network selection with the aim of optimizing medical data
    delivery over heterogeneous health systems. In particular, an optimization problem
    is formulated in which the network selection problem is integrated with adaptive
    compression to minimize network energy consumption and latency while meeting applications’
    QoS requirements. A single-agent DDPG-based DRL model is proposed to solve it.
    The agent is a centralized entity that can access all radio access networks (RANs)
    information and Patient Edge Node (PEN) data running in the core network. The
    action space is discrete, corresponding to the joint selection of data split over
    the existing RANs and the adequate compression ratio. The state space is a hybrid
    (continuous and discrete) defined by two elements: the fraction of time that the
    PENs should use over a particular RAN and the PEN investigated in the current
    timestamp. The reward is a continuous function, which is defined in terms of:
    the fraction of data of PEN that will be transferred through RAN, the energy consumed
    by PEN to transfer bits over RAN, distortion, expected latency of RANs, the monetary
    cost of PENs to use RANs, the resource share, the fraction of time that the PENs
    should use over a particular, and the data rate. Simulation results demonstrate
    that their proposed scheme outperforms the greedy techniques in terms of minimizing
    energy consumption and latency while satisfying different PENs requirements.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Synthesis and Reflections
  id: totrans-511
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section reviews the use of DRL methods for joint radio resources. Table
    [VII](#S4.T7 "TABLE VII ‣ IV-D2 In IoT and Other Emerging Wireless Networks ‣
    IV-D DRL for Joint RRAM ‣ IV DRL-Based Radio Resource Allocation and Management
    for Next Generation Wireless Networks ‣ Deep Reinforcement Learning for Radio
    Resource Allocation and Management in Next Generation Heterogeneous Wireless Networks:
    A Survey") summarizes the reviewed papers in this section. We observe that DRL
    tools can be efficiently deployed to address different types of joint radio resources
    for diversified network scenarios. The results obtained using DRL models are better
    than the heuristic methods [[158](#bib.bib158), [180](#bib.bib180)] and comparable
    to the state-of-the-art optimization approaches [[172](#bib.bib172)].'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: We also observe that multi-agent DRL deployment based on value-based algorithms
    receives more attention than policy-based algorithms. The reason is that users
    tend to have more control over their channel selection, data control, and transmission
    mode selection, and hence we find a more popular implementation of DRL agents
    at local IoT devices. In addition, the integration of value-based and policy-based
    algorithms for joint RRAM is also an interesting concept as presented in [[177](#bib.bib177),
    [39](#bib.bib39)], which requires more investigation, especially for multi-agent
    deployment scenarios.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: We also observe that DRL methods for cellular and HomNets as well as IoT wireless
    networks gain more attention than multi-RAT networks, particularly for D2D communications.
    In addition, there is a lack of research on applications of DRL for emerging IoT
    applications, such as healthcare systems as investigated recently in [[157](#bib.bib157)],
    which is also a promising field that requires more attention. Furthermore, we
    observe a lack of research on DRL applications for joint RRAM in satellite networks,
    which also deserves more in-depth investigation.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: V Open Challenges and Future Research Directions
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout the previous section, we have demonstrated the superiority of DRL
    algorithms over traditional methods in addressing complex RRAM problems for modern
    wireless networks. However, there are still several challenges and open issues
    that either not explored yet or need further exploration. This section provides
    highlights these open challenges and provides insights on future research directions
    in the context of DRL-based RRAM for next generation wireless networks.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: V-1 Open Challenges
  id: totrans-517
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: V-1a   Centralized vs. Decentralized RRAM Techniques
  id: totrans-518
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Future wireless networks are characterized mainly by their massive heterogeneity
    in wireless RANs, the number of user devices, and types of applications. Centralized
    DRL-based RRAM schemes are efficient in guaranteeing enhanced network QoS and
    fairness in allocating radio resources. They also ensure that RRAM optimization
    problems will not get stuck in local minima due to their holistic view of the
    system. However, formulating and solving RRAM optimization problems become tough
    tasks in such large-scale HetNets. Hence, centralized DRL-based RRAM solutions
    are typically unscalable. This motivates distributed multi-agent DRL-based algorithms
    that enable edge devices to make resource allocation decisions locally. Stochastic
    Game-based DRL algorithms are one promising research direction in this context
    [[22](#bib.bib22)]. However, the rapid increase in the number of edge devices
    (players) makes information exchange in such networks unmanageable. Also, the
    partial observability of agents might lead to suboptimal RRAM policies. Therefore,
    it is an open challenge to develop DRL-assisted algorithms that optimally balance
    between the centralization and distribution issue in RRAM.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: V-1b   Dimensionality of State Space in HetNets
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In modern wireless HetNets, service requirements and network conditions are
    rapidly changing. Hence, single-agent DRL algorithms must be designed to capture
    and respond to these fast network changes. To this end, it is required to reduce
    the state space and action space during the learning process, which inevitably
    degrades the quality of the learned policies. The existence of multi-agents and
    their interactions will also complicate the agents’ environment and prohibitively
    increase the dimensionality of state space, which will slow down the learning
    algorithms.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: V-1c   Reliability of Training Dataset
  id: totrans-522
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although the DRL-based solutions for RRAM we reviewed previously demonstrate
    efficient performance results, almost all the models are developed based on simulated
    training and testing datasets. The simulated dataset is typically produced based
    on some stochastic models, which provide simplified versions of practical systems
    and greatly ignore hidden system patterns. This methodology greatly weakens the
    reliability of the developed policies as their performance on practical networks
    would be skeptical. Hence, it is imperative to develop more effective and reliable
    approaches that generate precise simulation datasets and capture practical system
    settings as much as possible. This ensures high reliability and confidence during
    the training and testing modes of the developed RRAM policies. Developing such
    approaches is still a challenge due to the large-scale nature and rapid variations
    of future wireless environments.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the DRL models are sensitive to any change in the input data.
    Any minor changes in the input data will cause considerable change in the models’
    output. This mainly deteriorates the reliability of DRL algorithms, especially
    when deployed for modern IoT applications that require ultra-reliability, such
    as remote surgery or any other mission-critical IoT applications. Hence, ensuring
    high reliability for DRL models is a challenging issue.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: V-1d   Engineering of DRL Models for RRAM
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since DRL employs DNNs as function approximators for the reward functions, DRL
    models will inherit some of the challenges that exist in the DNN world. For example,
    it is still challenging to optimize the DNN hyperparameters, such as the type
    of DNNs used (e.g., convolutional, fully connected, or RNN), the number of hidden
    layers, the number of neurons per hidden layer, the learning rate, etc. This challenge
    is even exacerbated in multi-agent settings as all agents share the same radio
    resources and must converge simultaneously to some policies.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the engineering of DRL parameters such as state space and
    reward function is challenging for RRAM. The state space must be engineered to
    capture useful and representative information about the wireless environment,
    such as the available radio resources, users’ QoS requirements, channel quality,
    etc. Such information is crucial and heavily defines the learning and convergence
    behaviors of DRL agents. Again, the presence of multi-agents will even make it
    more challenging, as discussed in [[22](#bib.bib22)]. Also, since DRL models are
    reward-driven learning algorithms, the design of the reward function is also essential
    to guide the agent during the policy-learning stage. Formulating reward functions
    that capture the network objective and account for the available radio resources
    is also challenging.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: V-1e   System Dependency of DRL models
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DRL models are system-dependent as they are trained and tested for specific
    wireless environments and networks. Therefore, they provide effective results
    when employed to solve specific types of problems for which they are trained.
    However, if there would be a significant change in the characteristics of the
    wireless environment or the nature of the RRAM problem, such as network topology
    and available radio resources, the DRL model must be retrained as the old model
    is no longer reflecting the new training experiences. In modern wireless HetNets,
    such cases are frequently encountered, especially with real-time applications
    or in highly dynamic environments. In such a case, it becomes quite challenging
    for DRL agents to update and retrain their DNNs with rapidly changing input information
    from the HetNet environment [[1](#bib.bib1)].
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: V-1f   Continuous Training of DRL Models
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DRL algorithms require big datasets to train their models, which is typically
    associated with a high cost [[15](#bib.bib15)]. The network system pays this cost
    during the information collection process due to, e.g., the high delays, extra
    overhead, and energy consumption. The emergence of a large number of real-time
    applications and services has even increased this training cost. In this context,
    DRL models require to be continuously retrained with fresh data collected from
    the wireless environment to be up-to-date and to ensure accurate and long-term
    control decisions. It is not practical to conduct manual retraining of the models
    in such large-scale HetNets settings. Therefore, continuous retraining is preferred,
    in which a dedicated autonomous system can be employed to continuously assess
    and retrain old DRL models. However, it would be quite challenging for the new
    autonomous system to promptly perform this operation due to the environment’s
    rapid variations. Also, monitoring and updating DRL models in multi-agent scenarios
    becomes an expensive task.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: V-1g   Context of RRAM
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The implementation of DRL algorithms basically depends on the use-cases. The
    context and deployment scenarios in which RRAM is required must be considered
    during the development of DRL models. For example, RRAM in health-sector IoT applications
    is different from the environmental IoT applications counterparts. Due to the
    high sensitivity of data in the health-sector applications, extra data pre-processing
    must be performed, including data compression and encryption [[157](#bib.bib157)].
    This will directly affect the number of radio resources to be allocated for such
    applications. Hence, DRL models must be aware of the context aspect of applications,
    which is considered another challenge.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: V-1h   Competing Objective Design of DRL Models
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next generation wireless networks are expected to provide enhanced system QoS
    in terms of high data rate, high EE/SE, and reduced latency in order to support
    the emerging IoT vital applications. Depending on the deployment scenario, formulating
    multi-objective RRAM optimization problems usually ends with many competing objectives
    and/or constraints. For instance, in cellular UDNs, high resource utilization
    of, e.g., power allocation or channel may cause severe interference. Also, for
    IoT applications such as vehicular communications, we require to ensure ultra-reliable
    and low-latency communication links, which are usually competing objectives. Therefore,
    developing multi-objective DRL-based RRAM models that accommodate these competing
    requirements is still a persisting challenge.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: V-2 Future Research Directions
  id: totrans-536
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: V-2a   DRL with Explainable AI (XDRL) for RRAM
  id: totrans-537
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Explainable AI (XAI) has recently emerged as an efficient technology to improve
    the performance of DRL models. It is mainly envisioned to unlock the ”black-box”
    nature of conventional ML approaches and provide interpretability for DRL models
    [[183](#bib.bib183)]. In particular, XAI explains the reasons behind certain predictions
    made by DRL models (or ML models in general) by fully understanding the precise
    working principle of these models. Hence, ensuring trust, reliability, and transparency
    in the DRL algorithms’ policy devlopment and decision-making processes. The research
    on XAI technologies in wireless communication is still at its initial stages,
    and there are still some key issues for future research in the context of RRAM
    for next generation wireless networks. For example, DRL models can get stuck easily
    into local optimal solutions when utilized to solve complex RRAM problems. This
    issue can be significantly avoided with the help of XAI. Fortunately, the heterogeneity
    of information in modern wireless HetNets will significantly help to achieve the
    interpretation for DRL algorithms. In this context, developing RRAM schemes for
    wireless HetNets, through entity recognition, entity-relationship extraction,
    and representation learning, makes the DRL models’ interpretation more reliable,
    accurate, and intuitive, which is a promising research direction.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: V-2b   Integrating DRL and Blockchain Techniques
  id: totrans-539
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Blockchain-based RRAM has emerged recently as one of the promising enabling
    technologies for future wireless HetNets [[3](#bib.bib3)]. It has gained considerable
    momentum lately due to its ability to provide intelligent, secure, and highly
    efficient distributed resource sharing and management. The integration of DRL
    with Blockchain is also an interesting research direction, as in [[184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186)]. For example, DRL algorithms can be distributively
    deployed within participants or within the centralized spectrum-access systems
    to facilitate spectrum auctions and transactions [[186](#bib.bib186)]. Also, many
    of the auction’s winner-determination problems in future wireless HetNets are
    expected to be extremely complex and intractable due to the massive increase in
    the number of participants, e.g., bidders and sellers. Hence, DRL algorithms are
    efficient tools that can be utilized to solve such types of problems.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: V-2c   Federated DRL (FDRL)-Based RRAM
  id: totrans-541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Federated learning (FL) framework is envisioned mainly to preserve data privacy
    in ML algorithms [[187](#bib.bib187), [188](#bib.bib188)]. In FL, ML algorithms
    are locally distributed at the wireless network edge, and the data is processed
    locally and not shared globally. The local ML models are then utilized for training
    a centralized global model. In this context, the federated DRL learning (FDRL)
    scheme can be leveraged when many wireless user devices require making autonomous
    local decisions. In such a case, DRL agents do not exchange their local observations,
    and also, not necessarily all agents receive reward signals [[189](#bib.bib189)].
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Summary of Challenges and Future Research Directions in the Context
    of using DRL for RRAM in Future Wireless Networks.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Developing DRL-based algorithms that optimally balance the centralization
    and distribution issue of RRAM in future large-scale massive HetNets. |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '|  | Reducing the dimensionality of state space in distributed MADRL algorithms
    during the learning process without slowing down or degrading the quality of learned
    RRAM policies. |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '|  | Developing more effective and reliable training approaches that generate
    accurate simulation datasets and capture practical system settings. |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizing DRL models’ hyperparameters, especially in MADRL scenarios,
    and engineering the state space and reward functions to capture representative
    information about system dynamics. |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '|  | Designing agile DRL algorithms that can quickly update and retrain the
    DNNs in response to the rapid change of input information from the highly dynamic
    HetNet environment. |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '|  | Performing continuous retraining for the DRL models, especially MADRL,
    with fresh data in future large-scale and rapidly changing wireless environments.
    |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| Open Challenges | Developing DRL models that are aware of the context aspect
    and use cases of various emerging applications. |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '|  | Developing DRL algorithms that accommodate competing multi-objectives
    relevant to emerging applications. |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '|  | Developing efficient and reliable DRL algorithms for RRAM in next-generation
    HetNets based on the XAI concept through, e.g., entity recognition, entity-relationship
    extraction, and representation learning. |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
- en: '|  | Developing DRL-based blockchain techniques to address the problem of distributed
    resource sharing and management for future large-scale HetNets, e.g., to facilitate
    spectrum auctions and transactions, solving the problem of auction’s winner-determination,
    etc. |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '|  | Developing FDRL algorithms that ensure global solutions for complex RRAM
    optimization problems while guaranteeing data and models privacy during information
    sharing and models updating. |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '|  | Developing DRL models to achieve intelligent load balancing in future
    self-sustaining (or self-organization) HetNets. |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
- en: '|  | Developing light-weighted and agile networked MADRL algorithms that enable
    cooperation between agents with different heterogeneous reward functions and adapt
    to environments with rapid mobility. |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '|  | Developing ultra-reliable RRAM schemes by integrating DRL algorithms and
    GANs techniques to support emerging IoT applications with high-reliability demands.
    |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Future Research Directions | Developing end-to-end DRL-based algorithms that
    jointly optimize the configuration of RIS systems, i.e., elements’ phases and
    amplitudes, and radio resources of networks, e.g., downlink transmit power. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: Developing fine-grained policies in DRL becomes challenging when the state space
    is small and the training dataset is very limited [[190](#bib.bib190)]. In FDRL,
    the direct exchange of data between agents is not possible as this will preach
    the privacy promise of FL scheme. Instead, local DRL models can be developed and
    trained for agents with the help of other agents while preserving users’ data
    privacy, as in [[191](#bib.bib191)]. Hence, developing algorithms and schemes
    that guarantee data and models privacy during both information sharing and models
    updating is an interesting research direction.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: FDRL framework can also be exploited in the RRAM of modern wireless networks.
    For example, it can be deployed for solving complex wireless network optimization
    problems, such as power control in cellular UDNs. In this context, FDRL can ensure
    a global solution for complex network optimization problems without sharing information
    between BSs; each BS solves its optimization problem locally and shares the results
    with neighboring BSs.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: V-2d   DRL-Based Load Balancing for Self-Sustaining Networks
  id: totrans-561
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Load balancing in modern wireless UDNs is another promising research direction.
    The objective is to balance the wireless networks by moving some users from the
    heavily congested BSs to uncongested ones, thus improving BSs utilization and
    providing enhanced QoS provisioning. Although the load balancing field has been
    heavily investigated in the literature using conventional resource management
    approaches, as in [[192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194)],
    there still a research gap in applying DRL for such a field. In this context,
    DRL can be adopted to realize the self-sustaining (or self-organization) vision
    of next generation wireless networks [[3](#bib.bib3)]. Hence, developing single/multi-agent
    DRL models to achieve intelligent load balancing in future HetNets, is a possible
    research direction.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: V-2e   MADRL Algorithms in Support of Massive Heterogeneity and Mobility
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In modern wireless networks, massive heterogeneity is one of the main enabling
    technologies to provide ubiquitous coverage and enhance system reliability. However,
    some mathematical frameworks, such as the Markov game, are basically developed
    for homogeneous systems. To overcome this challenge, there are proposals on adopting
    networked MADRL algorithms that enable the cooperation between agents with different
    reward functions [[22](#bib.bib22)], which is an interesting research direction.
    Also, RRAM for networks with high mobility, such as vehicular and railway communications,
    is a persistent challenge. In this context, developing light-weighted and agile
    MADRL algorithms that account for and adapt to rapid network mobility is another
    interesting research direction.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: V-2f   DRL-Based RRAM with Generative Adversarial Networks (GANs) for RRAM
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Ensuring the reliability of DRL algorithms is one of the major challenges and
    objectives in DRL-based RRAM methods. In many real-life scenarios, we may require
    to deploy DRL models to allocate resources in vital systems requiring ultra-reliability,
    such as IoT healthcare applications [[157](#bib.bib157)]. In this context, there
    are proposals on Generative Adversarial Networks (GANs), which have emerged recently
    as an effective technique to enhance the reliability of DRL algorithms [[195](#bib.bib195)].
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the shortage of realistic training datasets that are required to
    train DRL models and learn optimal policies is a challenging issue. To overcome
    this, GANs are utilized, which generate large amounts of realistic datasets synthetically
    by expanding the available limited amounts of real-time datasets. From a DRL perspective,
    GANs-generated synthetic data is more effective and reliable than traditional
    augmentation methods [[67](#bib.bib67)]. This is because DRL agents will be exposed
    to various extreme challenging and practical situations by merging the realistic
    and synthetic data, enabling DRL models to be trained on unpredicted and rare
    events. Another advantage of GAN over traditional data augmentation methods is
    that it eliminates dataset biases in the synthetic data, which greatly enhances
    the quality of the generated data and leads to more reliability in DRL models’
    training and learning processes.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: In general, the research in the GANs-based DRL methods for RRAM is still in
    its early stages, and we believe that this research direction will take further
    pace in the future.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: V-2g   DRL for RRAM in RIS-Assisted Wireless Networks
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Reconfigurable Intelligent Surfaces (RIS) have emerged recently as an innovative
    technology to enhance the QoS of future wireless networks [[196](#bib.bib196),
    [197](#bib.bib197)]. RIS can be deployed in cellular networks as passive reflecting
    elements to provide near line-of-sight communication links to users, hence enhancing
    communication reliability and reducing latency [[198](#bib.bib198), [199](#bib.bib199)].
    Deploying RIS to assist cellular communication, however, requires judicious RRAM
    schemes to optimize network performance. This research field is still nascent,
    and there is much to do for future research and investigation, especially in the
    context of DRL-based RRAM techniques. Towards this, it is required to develop
    end-to-end DRL-based algorithms that jointly optimize the configuration of the
    RIS system, i.e., elements’ phases and amplitudes, and radio resources of BSs.
    For instance, designing DRL models that intelligently and optimally allocate the
    downlink BSs’ transmit power from one side and the amplitude and phase shifts
    of the RIS elements on the other side is a promising research direction, as in
    [[41](#bib.bib41)]. We also believe that the currently ongoing research in RIS-assisted
    wireless networks, e.g., [[41](#bib.bib41), [200](#bib.bib200), [201](#bib.bib201),
    [202](#bib.bib202)] will be cornerstones.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [VIII](#S5.T8 "TABLE VIII ‣ V-2c Federated DRL (FDRL)-Based RRAM ‣ V-2
    Future Research Directions ‣ V Open Challenges and Future Research Directions
    ‣ Deep Reinforcement Learning for Radio Resource Allocation and Management in
    Next Generation Heterogeneous Wireless Networks: A Survey") summarizes the open
    challenges and future research directions provided in this section.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presented a comprehensive survey on the applications of DRL techniques
    in RRAM for next generation wireless HetNets. We thoroughly reviewed the conventional
    approaches for RRAM, including their types, advantages, and limitations. We have
    then illustrated how the emerging DRL approaches can overcome these shortcomings
    to enable DRL-based RRAM. After that, we illustrated how the RRAM optimization
    problems can be formulated as an MDP before solving them using DRL techniques.
    Furthermore, we conducted an extensive overview of the most efficient DRL algorithms
    that are widely leveraged in addressing RRAM problems, including the value- and
    policy-based algorithms. The advantages, limitations, and use-cases for each algorithm
    are provided. We then conducted a comprehensive and in-depth literature review
    and classified the existing related works based on both the radio resources they
    are addressing and the type of wireless networks they are considering. To this
    end, the types of DRL models developed in these related works and their main elements
    are carefully identified. Finally, we outlined important open challenges and provided
    insights into future research directions in the context of DRL-based RRAM.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  id: totrans-574
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This publication was made possible by NPRP-Standard (NPRP-S) Thirteen ($13^{\text{th}}$)
    Cycle grant # NPRP13S-0201-200219 from the Qatar National Research Fund (a member
    of Qatar Foundation). The findings herein reflect the work, and are solely the
    responsibility, of the authors.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] F. Hussain, S. A. Hassan, R. Hussain, and E. Hossain, “Machine learning
    for resource management in cellular and IoT networks: Potentials, current solutions,
    and open challenges,” *IEEE Communications Surveys & Tutorials*, vol. 22, no. 2,
    pp. 1251–1275, 2020.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, “The roadmap
    to 6G: AI empowered wireless networks,” *IEEE Communications Magazine*, vol. 57,
    no. 8, pp. 84–90, 2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems: Applications,
    trends, technologies, and open research problems,” *IEEE network*, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,
    and P. Fan, “6G wireless networks: Vision, requirements, architecture, and key
    technologies,” *IEEE Vehicular Technology Magazine*, vol. 14, no. 3, pp. 28–41,
    2019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] “6G summit connecting the unconnected,” [https://6gsummit.org](https://6gsummit.org),
    accessed: .'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] G. Forecast, “Cisco visual networking index: global mobile data traffic
    forecast update, 2017–2022,” *Update*, vol. 2017, p. 2022, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] I. Tomkos, D. Klonidis, E. Pikasis, and S. Theodoridis, “Toward the 6G
    network era: Opportunities and challenges,” *IT Professional*, vol. 22, no. 1,
    pp. 34–38, 2020.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] P. Yang, Y. Xiao, M. Xiao, and S. Li, “6G wireless communications: Vision
    and potential techniques,” *IEEE Network*, vol. 33, no. 4, pp. 70–75, 2019.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] K. David and H. Berndt, “6G vision and requirements: Is there any need
    for beyond 5G?” *IEEE Vehicular Technology Magazine*, vol. 13, no. 3, pp. 72–80,
    2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Elmeadawy and R. M. Shubair, “6G wireless communications: Future technologies
    and research challenges,” in *2019 International Conference on Electrical and
    Computing Technologies and Applications (ICECTA)*.   IEEE, Conference Proceedings,
    pp. 1–5.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] T. Huang, W. Yang, J. Wu, J. Ma, X. Zhang, and D. Zhang, “A survey on
    green 6G network: Architecture and technologies,” *IEEE Access*, vol. 7, pp. 175 758–175 768,
    2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. Liang, H. Ye, G. Yu, and G. Y. Li, “Deep-learning-based wireless resource
    allocation with application to vehicular networks,” *Proceedings of the IEEE*,
    vol. 108, no. 2, pp. 341–356, 2020.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] K. K. Nguyen, T. Q. Duong, N. A. Vien, N.-A. Le-Khac, and M.-N. Nguyen,
    “Non-cooperative energy efficient power allocation game in D2D communication:
    A multi-agent deep reinforcement learning approach,” *IEEE Access*, vol. 7, pp.
    100 480–100 490, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim, “Applications of deep reinforcement learning in communications and
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. L. Lee and D. Qin, “A survey on applications of deep reinforcement
    learning in resource management for 5G heterogeneous networks,” in *2019 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC)*.   IEEE, 2019, pp. 1856–1862.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] F. Obite, A. D. Usman, and E. Okafor, “An overview of deep reinforcement
    learning for spectrum sensing in cognitive radio networks,” *Digital Signal Processing*,
    p. 103014, 2021.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Gupta, G. Singal, and D. Garg, “Deep reinforcement learning techniques
    in diversified domains: A survey,” *Archives of Computational Methods in Engineering*,
    pp. 1–40, 2021.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Z. Du, Y. Deng, W. Guo, A. Nallanathan, and Q. Wu, “Green deep reinforcement
    learning for radio resource management: Architecture, algorithm compression, and
    challenges,” *IEEE Vehicular Technology Magazine*, vol. 16, no. 1, pp. 29–39,
    2021.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Qian, J. Wu, R. Wang, F. Zhu, and W. Zhang, “Survey on reinforcement
    learning applications in communication networks,” *Journal of Communications and
    Information Networks*, vol. 4, no. 2, pp. 30–39, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Z. Xiong, Y. Zhang, D. Niyato, R. Deng, P. Wang, and L.-C. Wang, “Deep
    reinforcement learning for mobile 5G and beyond: Fundamentals, applications, and
    challenges,” *IEEE Vehicular Technology Magazine*, vol. 14, no. 2, pp. 44–52,
    2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Feriani and E. Hossain, “Single and multi-agent deep reinforcement
    learning for AI-enabled wireless networks: A tutorial,” *IEEE Communications Surveys
    & Tutorials*, pp. 1–1, 2021.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artificial neural
    networks-based machine learning for wireless networks: A tutorial,” *IEEE Communications
    Surveys & Tutorials*, vol. 21, no. 4, pp. 3039–3071, 2019.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Zappone, M. Di Renzo, and M. Debbah, “Wireless networks design in the
    era of deep learning: Model-based, AI-based, or both?” *IEEE Transactions on Communications*,
    vol. 67, no. 10, pp. 7331–7376, 2019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Khorasgani, H. Wang, and C. Gupta, “Challenges of applying deep reinforcement
    learning in dynamic dispatching,” *arXiv preprint arXiv:2011.05570*, 2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] G. S. Rahman, T. Dang, and M. Ahmed, “Deep reinforcement learning based
    computation offloading and resource allocation for low-latency fog radio access
    networks,” *Intelligent and Converged Networks*, vol. 1, no. 3, pp. 243–257, 2020.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Mohammed, H. Nahom, A. Tewodros, Y. Habtamu, and G. Hayelom, “Deep
    reinforcement learning for computation offloading and resource allocation in blockchain-based
    multi-UAV-enabled mobile edge computing,” in *2020 17th International Computer
    Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)*.   IEEE,
    2020, pp. 295–299.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Sheng, P. Chen, Z. Chen, L. Wu, and Y. Yao, “Deep reinforcement learning-based
    task scheduling in IoT edge computing,” *Sensors*, vol. 21, no. 5, p. 1666, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] X. Chen and G. Liu, “Energy-efficient task offloading and resource allocation
    via deep reinforcement learning for augmented reality in mobile edge networks,”
    *IEEE Internet of Things Journal*, 2021.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Q. Liu, T. Han, and E. Moges, “Edgeslice: Slicing wireless edge computing
    network with decentralized deep reinforcement learning,” *arXiv preprint arXiv:2003.12911*,
    2020.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Lin and Y. Zhao, “Artificial intelligence-empowered resource management
    for future wireless communications: A survey,” *China Communications*, vol. 17,
    no. 3, pp. 58–77, 2020.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Q. T. A. Pham, K. Piamrat, and C. Viho, “Resource management in wireless
    access networks: A layer-based classification-version 1.0,” 2014.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] E. Almazrouei, G. Gianini, N. Almoosa, and E. Damiani, “What can machine
    learning do for radio spectrum management?” in *Proceedings of the 16th ACM Symposium
    on QoS and Security for Wireless and Mobile Networks*, 2020, pp. 15–21.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Dhilipkumar, C. Arunachalaperumal, and K. Thanigaivelu, “A comparative
    study of resource allocation schemes for D2D networks underlay cellular networks,”
    *Wireless Personal Communications*, vol. 106, no. 3, pp. 1075–1087, 2019.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Z. Zhang, D. Zhang, and R. C. Qiu, “Deep reinforcement learning for power
    system applications: An overview,” *CSEE Journal of Power and Energy Systems*,
    vol. 6, no. 1, pp. 213–225, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] T. S. Rappaport, Y. Xing, O. Kanhere, S. Ju, A. Madanayake, S. Mandal,
    A. Alkhateeb, and G. C. Trichopoulos, “Wireless communications and applications
    above 100 ghz: Opportunities and challenges for 6G and beyond,” *IEEE Access*,
    vol. 7, pp. 78 729–78 757, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjöland, and F. Tufvesson,
    “6G wireless systems: Vision, requirements, challenges, insights, and opportunities,”
    *arXiv preprint arXiv:2008.03213*, 2020.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Alwarafy, B. S. Ciftler, M. Abdallah, and M. Hamdi, “DeepRAT: A DRL-based
    framework for multi-RAT assignment and power allocation in hetnets,” in *2021
    IEEE International Conference on Communications (ICC)*.   IEEE, 2021, pp. 1–6.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Kong, Z.-Y. Wu, M. Ismail, E. Serpedin, and K. A. Qaraqe, “Q-learning
    based two-timescale power allocation for multi-homing hybrid RF/VLC networks,”
    *IEEE Wireless Communications Letters*, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] G. Lee, M. Jung, A. T. Z. Kasgari, W. Saad, and M. Bennis, “Deep reinforcement
    learning for energy-efficient networking with reconfigurable intelligent surfaces,”
    in *ICC 2020-2020 IEEE International Conference on Communications (ICC)*.   IEEE,
    2020, pp. 1–6.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] B. S. Ciftler, A. Alwarafy, M. Abdallah, and M. Hamdi, “Dqn-based multi-user
    power allocation for hybrid RF/VLC networks,” *arXiv preprint arXiv:2102.01884*,
    2021.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Ahmad, S. Ahmad, M. H. Rehmani, and N. U. Hassan, “A survey on radio
    resource allocation in cognitive radio sensor networks,” *IEEE Communications
    Surveys & Tutorials*, vol. 17, no. 2, pp. 888–917, 2015.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. El Tanab and W. Hamouda, “Resource allocation for underlay cognitive
    radio networks: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 19,
    no. 2, pp. 1249–1276, 2017.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Naeem, A. Anpalagan, M. Jaseemuddin, and D. C. Lee, “Resource allocation
    techniques in cooperative cognitive radio networks,” *IEEE Communications Surveys
    & Tutorials*, vol. 16, no. 2, pp. 729–744, 2014.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Manap, K. Dimyati, M. N. Hindia, M. S. Abu Talip, and R. Tafazolli,
    “Survey of radio resource management in 5G heterogeneous networks,” *IEEE Access*,
    vol. 8, pp. 131 202–131 223, 2020.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Peng, C. Wang, J. Li, H. Xiang, and V. Lau, “Recent advances in underlay
    heterogeneous networks: Interference control, resource allocation, and self-organization,”
    *IEEE Communications Surveys & Tutorials*, vol. 17, no. 2, pp. 700–729, 2015.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Teng, M. Liu, F. R. Yu, V. C. M. Leung, M. Song, and Y. Zhang, “Resource
    allocation for ultra-dense networks: A survey, some research issues and challenges,”
    *IEEE Communications Surveys & Tutorials*, vol. 21, no. 3, pp. 2134–2168, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] K. Piamrat, A. Ksentini, J.-M. Bonnin, and C. Viho, “Radio resource management
    in emerging heterogeneous wireless networks,” *Computer Communications*, vol. 34,
    no. 9, pp. 1066–1076, 2011.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Xia, H. Chen, and C. Yang, “Radio resource management in machine-to-machine
    communications—a survey,” *IEEE Communications Surveys & Tutorials*, vol. 20,
    no. 1, pp. 791–828, 2018.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Sadr, A. Anpalagan, and K. Raahemifar, “Radio resource allocation algorithms
    for the downlink of multiuser ofdm communication systems,” *IEEE Communications
    Surveys & Tutorials*, vol. 11, no. 3, pp. 92–106, 2009.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] E. Yaacoub and Z. Dawy, “A survey on uplink resource allocation in ofdma
    wireless networks,” *IEEE Communications Surveys & Tutorials*, vol. 14, no. 2,
    pp. 322–337, 2012.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R. O. Afolabi, A. Dadlani, and K. Kim, “Multicast scheduling and resource
    allocation algorithms for ofdma-based systems: A survey,” *IEEE Communications
    Surveys & Tutorials*, vol. 15, no. 1, pp. 240–254, 2013.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Chieochan and E. Hossain, “Adaptive radio resource allocation in ofdma
    systems: a survey of the state-of-the-art approaches,” *Wireless Communications
    and Mobile Computing*, vol. 9, no. 4, pp. 513–527, 2009.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] D. Niyato and E. Hossain, “Radio resource management in MIMO-OFDM- mesh
    networks: Issues and approaches,” *IEEE Communications Magazine*, vol. 45, no. 11,
    pp. 100–107, 2007.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] W. Zhao and S. Wang, “Resource sharing scheme for device-to-device communication
    underlaying cellular networks,” *IEEE Transactions on Communications*, vol. 63,
    no. 12, pp. 4838–4848, 2015.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] L. Song, D. Niyato, Z. Han, and E. Hossain, “Game-theoretic resource allocation
    methods for device-to-device communication,” *IEEE Wireless Communications*, vol. 21,
    no. 3, pp. 136–144, 2014.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Kawamoto, H. Nishiyama, N. Kato, F. Ono, and R. Miura, “Toward future
    unmanned aerial vehicle networks: Architecture, resource allocation and field
    experiments,” *IEEE Wireless Communications*, vol. 26, no. 1, pp. 94–99, 2019.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Masmoudi, K. Mnif, and F. Zarai, “A survey on radio resource allocation
    for V2X communication,” *Wireless Communications and Mobile Computing*, vol. 2019,
    2019.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Allouch, S. Kallel, A. Soua, O. Shagdar, and S. Tohme, “Survey on radio
    resource allocation in long-term evolution-vehicle,” *Concurrency and Computation:
    Practice and Experience*, p. e6228, 2021.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Xu, G. Zhu, B. Ai, and Z. Zhong, “A survey on high-speed railway communications:
    A radio resource management perspective,” *Computer Communications*, vol. 86,
    pp. 12–28, 2016.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Clausen, “Branch and bound algorithms-principles and examples,” *Department
    of Computer Science, University of Copenhagen*, pp. 1–30, 1999.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] D. S. Nau, V. Kumar, and L. Kanal, “General branch and bound, and its
    relation to ${A}^{\ast}$ and ${AO}^{\ast}$,” *Artificial Intelligence*, vol. 23,
    no. 1, pp. 29–58, 1984.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. S. Nasir and D. Guo, “Deep reinforcement learning for joint spectrum
    and power allocation in cellular networks,” *arXiv preprint arXiv:2012.10682*,
    2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] K. Shen and W. Yu, “Fractional programming for communication systems—part
    i: Power control and beamforming,” *IEEE Transactions on Signal Processing*, vol. 66,
    no. 10, pp. 2616–2630, 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. Kim and I.-Y. Ko, “An efficient resource allocation approach based
    on a genetic algorithm for composite services in IoT environments,” in *2015 IEEE
    International Conference on Web Services*.   IEEE, 2015, pp. 543–550.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Naeem, S. T. H. Rizvi, and A. Coronato, “A gentle introduction to reinforcement
    learning and its application in different fields,” *IEEE Access*, 2020.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos, “Learning
    to optimize: Training deep neural networks for interference management,” *IEEE
    Transactions on Signal Processing*, vol. 66, no. 20, pp. 5438–5453, 2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 30, no. 1, 2016.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    conference on machine learning*.   PMLR, 2016, pp. 1995–2003.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] R. SUTTON, “Policy gradient methods for reinforcement learning with function
    approximation,” *Advances in Neural information Processing Systems*, vol. 12,
    pp. 1057–1063, 2000.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*.   PMLR, 2016, pp. 1928–1937.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    2015.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *Proceedings of the 31st International
    Conference on Machine Learning*, vol. 32, no. 1.   PMLR, 2014, pp. 387–395\. [Online].
    Available: [http://proceedings.mlr.press/v32/silver14.html](http://proceedings.mlr.press/v32/silver14.html)'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt,
    and D. Silver, “Distributed prioritized experience replay,” *arXiv preprint arXiv:1803.00933*,
    2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International Conference on Machine Learning*.   PMLR,
    2017, pp. 449–458.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining improvements in
    deep reinforcement learning,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 32, no. 1, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Hausknecht and P. Stone, “Deep recurrent Q-learning for partially observable
    mdps,” 2015.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 1587–1596.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement.”'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, D. Tb,
    A. Muldal, N. Heess, and T. Lillicrap, “Distributed distributional deterministic
    policy gradients,” *arXiv preprint arXiv:1804.08617*, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multiagent systems: A review of challenges, solutions, and applications,”
    *IEEE transactions on cybernetics*, vol. 50, no. 9, pp. 3826–3839, 2020.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning:
    A selective overview of theories and algorithms,” *arXiv preprint arXiv:1911.10635*,
    2019.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] I. Althamary, C.-W. Huang, and P. Lin, “A survey on multi-agent reinforcement
    learning methods for vehicular networks,” in *2019 15th International Wireless
    Communications & Mobile Computing Conference (IWCMC)*.   IEEE, 2019, pp. 1154–1159.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. Lee, N. He, P. Kamalaruban, and V. Cevher, “Optimization for reinforcement
    learning: From a single agent to cooperative agents,” *IEEE Signal Processing
    Magazine*, vol. 37, no. 3, pp. 123–135, 2020.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Q. Shi, M. Razaviyayn, Z.-Q. Luo, and C. He, “An iteratively weighted
    MMSE approach to distributed sum-utility maximization for a MIMO interfering broadcast
    channel,” *IEEE Transactions on Signal Processing*, vol. 59, no. 9, pp. 4331–4340,
    2011.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] E. Ghadimi, F. D. Calabrese, G. Peters, and P. Soldati, “A reinforcement
    learning approach to power control and rate adaptation in cellular networks,”
    in *2017 IEEE International Conference on Communications (ICC)*.   IEEE, 2017,
    pp. 1–7.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Hu and Y. Sun, “A deep reinforcement learning-based power resource
    management for fuel cell powered data centers,” *Electronics*, vol. 9, no. 12,
    p. 2054, 2020.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. A. Khan and R. S. Adve, “Centralized and distributed deep reinforcement
    learning methods for downlink sum-rate optimization,” *IEEE Transactions on Wireless
    Communications*, vol. 19, no. 12, pp. 8410–8426, 2020.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. S. Nasir and D. Guo, “Deep actor-critic learning for distributed power
    control in wireless mobile networks,” *arXiv preprint arXiv:2009.06681*, 2020.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Bi and W. Zhou, “Deep reinforcement learning based power allocation
    for D2D network,” in *2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)*.   IEEE,
    2020, pp. 1–5.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Saeidian, S. Tayamon, and E. Ghadimi, “Downlink power control in dense
    5G radio access networks through deep reinforcement learning,” in *ICC 2020-2020
    IEEE International Conference on Communications (ICC)*.   IEEE, 2020, pp. 1–6.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Zhang, H. Qu, J. Zhao, and W. Wang, “Deep reinforcement learning method
    for energy efficient resource allocation in next generation wireless networks,”
    in *Proceedings of the 2020 International Conference on Computing, Networks and
    Internet of Things*, 2020, pp. 18–24.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] F. Meng, P. Chen, L. Wu, and J. Cheng, “Power allocation in multi-user
    cellular networks: Deep reinforcement learning approaches,” *IEEE Transactions
    on Wireless Communications*, vol. 19, no. 10, pp. 6255–6267, 2020.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Meng, P. Chen, and L. Wu, “Power allocation in multi-user cellular
    networks with deep Q learning approach,” in *ICC 2019-2019 IEEE International
    Conference on Communications (ICC)*.   IEEE, 2019, pp. 1–6.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] L. Zhang and Y.-C. Liang, “Deep reinforcement learning for multi-agent
    power control in heterogeneous networks,” *IEEE Transactions on Wireless Communications*,
    2020.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Mohsenivatani, M. Darabi, S. Parsaeefard, M. Ardebilipour, and B. Maham,
    “Throughput maximization in c-ran enabled virtualized wireless networks via multi-agent
    deep reinforcement learning,” in *2020 IEEE 31st Annual International Symposium
    on Personal, Indoor and Mobile Radio Communications*.   IEEE, pp. 1–6.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, and H. Li, “Intelligent power
    control for spectrum sharing in cognitive radios: A deep reinforcement learning
    approach,” *IEEE access*, vol. 6, pp. 25 463–25 473, 2018.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Y. Zhang, C. Kang, T. Ma, Y. Teng, and D. Guo, “Power allocation in multi-cell
    networks using deep reinforcement learning,” in *2018 IEEE 88th Vehicular Technology
    Conference (VTC-Fall)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Li, Q. Cheng, K. Xue, C. Yang, and Z. Han, “Downlink transmit power
    control in ultra-dense UAV network based on mean field game and deep reinforcement
    learning,” *IEEE Transactions on Vehicular Technology*, 2020.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for
    dynamic power allocation in wireless networks,” *IEEE Journal on Selected Areas
    in Communications*, vol. 37, no. 10, pp. 2239–2250, 2019.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] W. Wang, “Smoothing deep reinforcement learning for power control for
    spectrum sharing in cognitive radios.”'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] N. Zhao, Z. Liu, and Y. Cheng, “Multi-agent deep reinforcement learning
    for trajectory design and power allocation in multi-UAV networks,” *IEEE Access*,
    vol. 8, pp. 139 670–139 679, 2020.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] J. Xu and B. Ai, “Experience-driven power allocation using multi-agent
    deep reinforcement learning for millimeter-wave high-speed railway systems,” *IEEE
    Transactions on Intelligent Transportation Systems*, 2021.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Yan, B. Chen, G. Feng, and S. Qin, “Federated cooperation and augmentation
    for power allocation in decentralized wireless networks,” *IEEE Access*, vol. 8,
    pp. 48 088–48 100, 2020.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. G. Luis, M. Guerster, I. del Portillo, E. Crawley, and B. Cameron,
    “Deep reinforcement learning architecture for continuous power allocation in high
    throughput satellites,” *arXiv preprint arXiv:1906.00571*, 2019.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. J. G. Luis, M. Guerster, I. del Portillo, E. Crawley, and B. Cameron,
    “Deep reinforcement learning for continuous power allocation in flexible high
    throughput satellites,” in *2019 IEEE Cognitive Communications for Aerospace Applications
    Workshop (CCAAW)*.   IEEE, 2019, pp. 1–4.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] O. Maraqa, A. S. Rajasekaran, S. Al-Ahmadi, H. Yanikomeroglu, and S. M.
    Sait, “A survey of rate-optimal power domain NOMA with enabling technologies of
    future wireless networks,” *IEEE Communications Surveys & Tutorials*, vol. 22,
    no. 4, pp. 2192–2235, 2020.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] X. Yan, K. An, Q. Zhang, G. Zheng, S. Chatzinotas, and J. Han, “Delay
    constrained resource allocation for noma enabled satellite internet of things
    with deep reinforcement learning,” *IEEE Internet of Things Journal*, 2020.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A. Alwarafy, M. Alresheedi, A. F. Abas, and A. Alsanie, “Performance
    evaluation of space time coding techniques for indoor visible light communication
    systems,” in *2018 International Conference on Optical Network Design and Modeling
    (ONDM)*, 2018, pp. 88–93.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. Memedi and F. Dressler, “Vehicular visible light communications: A
    survey,” *IEEE Communications Surveys & Tutorials*, vol. 23, no. 1, pp. 161–181,
    2021.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for resource
    allocation in a network of cache-enabled LTE-U UAVs,” in *GLOBECOM 2017-2017 IEEE
    Global Communications Conference*.   IEEE, Conference Proceedings, pp. 1–6.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, and Y. Jiang, “Deep reinforcement
    learning for user association and resource allocation in heterogeneous networks,”
    in *2018 IEEE Global Communications Conference (GLOBECOM)*.   IEEE, 2018, pp.
    1–6.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, “Deep reinforcement
    learning for user association and resource allocation in heterogeneous cellular
    networks,” *IEEE Transactions on Wireless Communications*, vol. 18, no. 11, pp.
    5141–5152, 2019.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Q. Zhang, Y.-C. Liang, and H. V. Poor, “Intelligent user association
    for symbiotic radio networks using deep reinforcement learning,” *IEEE Transactions
    on Wireless Communications*, 2020.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] W. Lei, Y. Ye, and M. Xiao, “Deep reinforcement learning based spectrum
    allocation in integrated access and backhaul networks,” *IEEE Transactions on
    Cognitive Communications and Networking*, 2020.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Z. Li, C. Wang, and C.-J. Jiang, “User association for load balancing
    in vehicular networks: An online reinforcement learning approach,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 18, no. 8, pp. 2217–2228, 2017.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. S. Perez, S. K. Jayaweera, and S. Lane, “Machine learning aided cognitive
    RAT selection for 5G heterogeneous networks,” in *2017 IEEE International Black
    Sea Conference on Communications and Networking (BlackSeaCom)*.   IEEE, 2017,
    pp. 1–5.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Zheng, X. Tang, X. Wei, H. Shen, and L. Zhao, “Channel assignment
    for hybrid noma systems with deep reinforcement learning,” *IEEE Wireless Communications
    Letters*, 2021.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. Song, L. Liu, J. Ashdown, and Y. Yi, “A deep reinforcement learning
    framework for spectrum management in dynamic spectrum access,” *IEEE Internet
    of Things Journal*, 2021.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access in wireless networks,” *IEEE Transactions
    on Cognitive Communications and Networking*, vol. 4, no. 2, pp. 257–265, 2018.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Wang, H. Liu, H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access in wireless networks,” 2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Wang, H. Liu, P. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access,” in *International Conference on Computing,
    Networking and Communications (ICNC)*, 2017, pp. 257–265.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. H. A. Ahmad, M. Liu, T. Javidi, Q. Zhao, and B. Krishnamachari, “Optimality
    of myopic sensing in multichannel opportunistic access,” *IEEE Transactions on
    Information Theory*, vol. 55, no. 9, pp. 4040–4050, 2009.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning-based multiaccess
    control and battery prediction with energy harvesting in IoT systems,” *IEEE Internet
    of Things Journal*, vol. 6, no. 2, pp. 2009–2020, 2018.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Y. Zhang, P. Cai, C. Pan, and S. Zhang, “Multi-agent deep reinforcement
    learning-based cooperative spectrum sensing with upper confidence bound exploration,”
    *IEEE Access*, vol. 7, pp. 118 898–118 906, 2019.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. Xu, J. Yu, W. C. Headley, and R. M. Buehrer, “Deep reinforcement learning
    for dynamic spectrum access in wireless networks,” in *MILCOM 2018-2018 IEEE Military
    Communications Conference (MILCOM)*.   IEEE, Conference Proceedings, pp. 207–212.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] L. Liang, H. Ye, and G. Y. Li, “Multi-agent reinforcement learning for
    spectrum sharing in vehicular networks,” in *2019 IEEE 20th International Workshop
    on Signal Processing Advances in Wireless Communications (SPAWC)*.   IEEE, 2019,
    pp. 1–5.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] ——, “Spectrum sharing in vehicular networks based on multi-agent reinforcement
    learning,” *IEEE Journal on Selected Areas in Communications*, vol. 37, no. 10,
    pp. 2282–2292, 2019.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-q-learning-based
    transmission scheduling mechanism for the cognitive internet of things,” *IEEE
    Internet of Things Journal*, vol. 5, no. 4, pp. 2375–2385, 2017.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] H. Khan, A. Elgabli, S. Samarakoon, M. Bennis, and C. S. Hong, “Reinforcement
    learning-based vehicle-cell association algorithm for highly mobile millimeter
    wave communication,” *IEEE Transactions on Cognitive Communications and Networking*,
    vol. 5, no. 4, pp. 1073–1085, 2019.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] P. Yang, L. Li, J. Yin, H. Zhang, W. Liang, W. Chen, and Z. Han, “Dynamic
    spectrum access in cognitive radio networks using deep reinforcement learning
    and evolutionary game,” in *2018 IEEE/CIC International Conference on Communications
    in China (ICCC)*.   IEEE, 2018, pp. 405–409.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Cao, S.-Y. Lien, and Y.-C. Liang, “Deep reinforcement learning for
    multi-user access control in non-terrestrial networks,” *IEEE Transactions on
    Communications*, 2020.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] N. Yang, H. Zhang, and R. Berry, “Partially observable multi-agent deep
    reinforcement learning for cognitive resource management,” in *GLOBECOM 2020-2020
    IEEE Global Communications Conference*.   IEEE, 2020, pp. 1–6.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Tomovic and I. Radusinovic, “A novel deep q-learning method for dynamic
    spectrum access,” in *2020 28th Telecommunications Forum (TELFOR)*.   IEEE, 2020,
    pp. 1–4.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] C. Zhong, Z. Lu, M. C. Gursoy, and S. Velipasalar, “A deep actor-critic
    reinforcement learning framework for dynamic multichannel access,” *IEEE Transactions
    on Cognitive Communications and Networking*, vol. 5, no. 4, pp. 1125–1139, 2019.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Tondwalkar and D. A. Kwasinski, “Deep reinforcement learning for distributed
    uncoordinated cognitive radios resource allocation,” *arXiv preprint arXiv:1911.03366*,
    2019.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] K. Al-Gumaei, K. Schuba, A. Friesen, S. Heymann, C. Pieper, F. Pethig,
    and S. Schriegel, “A survey of internet of things and big data integrated solutions
    for industrie 4.0,” in *2018 IEEE 23rd International Conference on Emerging Technologies
    and Factory Automation (ETFA)*, vol. 1, 2018, pp. 1417–1424.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Z. Shi, X. Xie, H. Lu, H. Yang, M. Kadoch, and M. Cheriet, “Deep reinforcement
    learning based spectrum resource management for industrial internet of things,”
    *IEEE Internet of Things Journal*, 2020.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S. B. Janiar and V. Pourahmadi, “Deep-reinforcement learning for fair
    distributed dynamic spectrum access in wireless networks,” in *2021 IEEE 18th
    Annual Consumer Communications & Networking Conference (CCNC)*.   IEEE, 2021,
    pp. 1–4.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Y. Wang, X. Li, P. Wan, and R. Shao, “Intelligent dynamic spectrum access
    using deep reinforcement learning for vanets,” *IEEE Sensors Journal*, 2021.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] W. Jiang and W. Yu, “Multi-agent reinforcement learning based joint cooperative
    spectrum sensing and channel access for cognitive UAV networks,” *arXiv preprint
    arXiv:2103.08181*, 2021.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Xu, J. Yu, and R. M. Buehrer, “The application of deep reinforcement
    learning to distributed spectrum access in dynamic heterogeneous environments
    with partial observations,” *IEEE Transactions on Wireless Communications*, vol. 19,
    no. 7, pp. 4494–4506, 2020.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] S. Liu, X. Hu, and W. Wang, “Deep reinforcement learning based dynamic
    channel allocation algorithm in multibeam satellite systems,” *IEEE Access*, vol. 6,
    pp. 15 733–15 742, 2018.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Hu, S. Liu, R. Chen, W. Wang, and C. Wang, “A deep reinforcement learning-based
    framework for dynamic resource allocation in multibeam satellite systems,” *IEEE
    Communications Letters*, vol. 22, no. 8, pp. 1612–1615, 2018.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] B. Zhao, J. Liu, Z. Wei, and I. You, “A deep reinforcement learning based
    approach for energy-efficient channel allocation in satellite internet of things,”
    *IEEE Access*, vol. 8, pp. 62 197–62 206, 2020.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Liu, B. Zhao, Q. Xin, and H. Liu, “Dynamic channel allocation for
    satellite internet of things via deep reinforcement learning,” in *2020 International
    Conference on Information Networking (ICOIN)*.   IEEE, 2020, pp. 465–470.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] F. Zheng, Z. Pi, Z. Zhou, and K. Wang, “Leo satellite channel allocation
    scheme based on reinforcement learning,” *Mobile Information Systems*, vol. 2020,
    2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] U. Challita, L. Dong, and W. Saad, “Proactive resource management in
    lte-u systems: A deep learning perspective,” *arXiv preprint arXiv:1702.07031*,
    2017.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning for
    distributed dynamic spectrum access,” *IEEE Transactions on Wireless Communications*,
    vol. 18, no. 1, pp. 310–323, 2018.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] S. Wang and T. Lv, “Deep reinforcement learning based dynamic multichannel
    access in hetnets,” in *2019 IEEE Wireless Communications and Networking Conference
    (WCNC)*.   IEEE, 2019, pp. 1–6.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] H. Peng and X. S. Shen, “Deep reinforcement learning based resource management
    for multi-access edge computing in vehicular networks,” *IEEE Transactions on
    Network Science and Engineering*, 2020.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] U. Challita and D. Sandberg, “Deep reinforcement learning for dynamic
    spectrum sharing of lte and nr,” *arXiv preprint arXiv:2102.11176*, 2021.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Q. Liu, T. Han, N. Zhang, and Y. Wang, “Deepslicing: Deep reinforcement
    learning assisted resource allocation for network slicing,” *arXiv preprint arXiv:2008.07614*,
    2020.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] F. Tang, Y. Zhou, and N. Kato, “Deep reinforcement learning for dynamic
    uplink/downlink resource allocation in high mobility 5G hetnet,” *IEEE Journal
    on Selected Areas in Communications*, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Z. Chkirbene, A. Awad, A. Mohamed, A. Erbad, and M. Guizani, “Deep reinforcement
    learning for network selection over heterogeneous health systems,” *IEEE Transactions
    on Network Science and Engineering*, 2021.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. Y. Munaye, R.-T. Juang, H.-P. Lin, G. B. Tarekegn, and D.-B. Lin,
    “Deep reinforcement learning based resource management in UAV-assisted IoT networks,”
    *Applied Sciences*, vol. 11, no. 5, p. 2163, 2021.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] T. Zhang, K. Zhu, and J. Wang, “Energy-efficient mode selection and resource
    allocation for D2D-enabled heterogeneous networks: A deep reinforcement learning
    approach,” *IEEE Transactions on Wireless Communications*, 2020.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] X. Zhang, M. Peng, S. Yan, and Y. Sun, “Deep-reinforcement-learning-based
    mode selection and resource allocation for cellular V2X communications,” *IEEE
    Internet of Things Journal*, vol. 7, no. 7, pp. 6380–6391, 2019.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Jang and H. J. Yang, “Deep reinforcement learning-based resource allocation
    and power control in small cells with limited information exchange,” *IEEE Transactions
    on Vehicular Technology*, vol. 69, no. 11, pp. 13 768–13 783, 2020.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] X. Liao, J. Shi, Z. Li, L. Zhang, and B. Xia, “A model-driven deep reinforcement
    learning heuristic algorithm for resource allocation in ultra-dense cellular networks,”
    *IEEE Transactions on Vehicular Technology*, vol. 69, no. 1, pp. 983–997, 2020.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z. Liu, X. Chen, Y. Chen, and Z. Li, “Deep reinforcement learning based
    dynamic resource allocation in 5G ultra-dense networks,” in *2019 IEEE International
    Conference on Smart Internet of Things (SmartIoT)*.   IEEE, 2019, pp. 168–174.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] X. Zhang, Z. Lin, B. Ding, B. Gu, and Y. Han, “Deep multi-agent reinforcement
    learning for resource allocation in D2D communication underlaying cellular networks,”
    in *2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)*.   IEEE,
    2020, pp. 55–60.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. Asheralieva and Y. Miyanaga, “An autonomous learning-based algorithm
    for joint channel and power level selection by D2D pairs in heterogeneous cellular
    networks,” *IEEE transactions on communications*, vol. 64, no. 9, pp. 3996–4012,
    2016.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] D. Wang, H. Qin, B. Song, K. Xu, X. Du, and M. Guizani, “Joint resource
    allocation and power control for d2d communication with deep reinforcement learning
    in mcc,” *Physical Communication*, vol. 45, p. 101262, 2021.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] H. Ding, F. Zhao, J. Tian, D. Li, and H. J. A. H. N. Zhang, “A deep reinforcement
    learning for user association and power control in heterogeneous networks,” vol.
    102, p. 102069, 2020.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] S. Yu, Y. J. Jeong, and J. W. Lee, “Resource allocation scheme based
    on deep reinforcement learning for device-to-device communications,” in *2021
    International Conference on Information Networking (ICOIN)*.   IEEE, 2021, pp.
    712–714.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Zhang, C. Kang, Y. Teng, S. Li, W. Zheng, and J. Fang, “Deep reinforcement
    learning framework for joint resource allocation in heterogeneous networks,” in
    *2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall)*.   IEEE, 2019,
    pp. 1–6.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Q. Huang, X. Xie, and M. Cheriet, “Reinforcement learning-based hybrid
    spectrum resource allocation scheme for the high load of URLLC services,” *EURASIP
    Journal on Wireless Communications and Networking*, vol. 2020, no. 1, pp. 1–21,
    2020.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] J. Tan, L. Zhang, and Y.-C. Liang, “Deep reinforcement learning for channel
    selection and power control in D2D networks,” in *2019 IEEE Global Communications
    Conference (GLOBECOM)*.   IEEE, 2019, pp. 1–6.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Tan, Y.-C. Liang, L. Zhang, and G. Feng, “Deep reinforcement learning
    for joint channel selection and power control in D2D networks,” *IEEE Transactions
    on Wireless Communications*, 2020.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning based
    resource allocation for V2V communications,” *IEEE Transactions on Vehicular Technology*,
    vol. 68, no. 4, pp. 3163–3173, 2019.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] H. Ye and G. Y. Li, “Deep reinforcement learning for resource allocation
    in V2V communications,” in *2018 IEEE International Conference on Communications
    (ICC)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] H. Ye and G. Li, “Deep reinforcement learning based distributed resource
    allocation for V2V broadcasting,” in *2018 14th International Wireless Communications
    & Mobile Computing Conference (IWCMC)*.   IEEE, 2018, pp. 440–445.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Yuan, Y. Zhang, W. Qie, T. Ma, and S. Li, “Deep reinforcement learning
    for resource allocation with network slicing in cognitive radio network,” *Computer
    Science and Information Systems*, no. 00, pp. 55–55, 2020.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Y. Zhang, X. Wang, and Y. Xu, “Energy-efficient resource allocation in
    uplink noma systems with deep reinforcement learning,” in *2019 11th International
    Conference on Wireless Communications and Signal Processing (WCSP)*.   IEEE, 2019,
    pp. 1–6.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] C. He, Y. Hu, Y. Chen, and B. Zeng, “Joint power allocation and channel
    assignment for noma with deep reinforcement learning,” *IEEE Journal on Selected
    Areas in Communications*, vol. 37, no. 10, pp. 2200–2210, 2019.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Z. Jiang, C. Hao, Y. Huang, Q. Wu, and F. Zhou, “Partially distributed
    channel and power management based on reinforcement learning,” *Journal of Communications
    and Information Networks*, vol. 5, no. 4, pp. 423–437, 2020.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Y.-H. Xu, C.-C. Yang, M. Hua, and W. Zhou, “Deep deterministic policy
    gradient (ddpg)-based resource allocation scheme for noma vehicular communications,”
    *IEEE Access*, vol. 8, pp. 18 797–18 807, 2020.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] S. Shrivastava, B. Chen, C. Chen, H. Wang, and M. Dai, “Deep q-network
    learning based downlink resource allocation for hybrid RF/VLC systems,” *IEEE
    Access*, vol. 8, pp. 149 412–149 434, 2020.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. Zhu, J. Wang, Y. Huang, S. He, X. You, and L. Yang, “On optimal power
    allocation for downlink non-orthogonal multiple access systems,” *IEEE Journal
    on Selected Areas in Communications*, vol. 35, no. 12, pp. 2744–2757, 2017.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] A. Adadi and M. Berrada, “Peeking inside the black-box: a survey on explainable
    artificial intelligence (XAI),” *IEEE access*, vol. 6, pp. 52 138–52 160, 2018.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y. He, Y. Wang, C. Qiu, Q. Lin, J. Li, and Z. Ming, “Blockchain-based
    edge computing resource allocation in IoT: A deep reinforcement learning approach,”
    *IEEE Internet of Things Journal*, vol. 8, no. 4, pp. 2226–2237, 2021.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] F. Guo, F. R. Yu, H. Zhang, H. Ji, M. Liu, and V. C. M. Leung, “Adaptive
    resource allocation in future wireless networks with blockchain and mobile edge
    computing,” *IEEE Transactions on Wireless Communications*, vol. 19, no. 3, pp.
    1689–1703, 2020.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] S. Hu, Y. C. Liang, Z. Xiong, and D. Niyato, “Blockchain and artificial
    intelligence for dynamic resource sharing in 6G and beyond,” *IEEE Wireless Communications*,
    pp. 1–7, 2021.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] O. A. Wahab, A. Mourad, H. Otrok, and T. Taleb, “Federated machine learning:
    Survey, multi-level classification, desirable criteria and future directions in
    communication and networking systems,” *IEEE Communications Surveys & Tutorials*,
    pp. 1–1, 2021.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] N. H. Tran, W. Bao, A. Zomaya, M. N. Nguyen, and C. S. Hong, “Federated
    learning over wireless networks: Optimization model design and analysis,” in *IEEE
    INFOCOM 2019-IEEE Conference on Computer Communications*.   IEEE, 2019, pp. 1387–1395.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
    “Adaptive federated learning in resource constrained edge computing systems,”
    *IEEE Journal on Selected Areas in Communications*, vol. 37, no. 6, pp. 1205–1221,
    2019.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] X. Wang, C. Wang, X. Li, V. C. Leung, and T. Taleb, “Federated deep reinforcement
    learning for internet of things with decentralized cooperative edge caching,”
    *IEEE Internet of Things Journal*, vol. 7, no. 10, pp. 9441–9455, 2020.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] H. H. Zhuo, W. Feng, Q. Xu, Q. Yang, and Y. Lin, “Federated reinforcement
    learning,” *arXiv preprint arXiv:1901.08277*, 2019.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] B. Das and S. Roy, “Load balancing techniques for wireless mesh networks:
    A survey,” in *2013 International Symposium on Computational and Business Intelligence*,
    2013, pp. 247–253.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Zhu, W. Shen, S. Pan, R. Li, and Z. Li, “A dynamic load balancing
    method for spatial data network service,” in *2009 5th International Conference
    on Wireless Communications, Networking and Mobile Computing*, 2009, pp. 1–3.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] H. Desai and R. Oza, “A study of dynamic load balancing in grid environment,”
    in *2016 International Conference on Wireless Communications, Signal Processing
    and Networking (WiSPNET)*, 2016, pp. 128–132.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] A. T. Z. Kasgari, W. Saad, M. Mozaffari, and H. V. Poor, “Experienced
    deep reinforcement learning with generative adversarial networks (gans) for model-free
    ultra reliable low latency communication,” *IEEE Transactions on Communications*,
    vol. 69, no. 2, pp. 884–899, 2021.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] R. Alghamdi, R. Alhadrami, D. Alhothali, H. Almorad, A. Faisal, S. Helal,
    R. Shalabi, R. Asfour, N. Hammad, A. Shams, N. Saeed, H. Dahrouj, T. Y. Al-Naffouri,
    and M. S. Alouini, “Intelligent surfaces for 6G wireless networks: A survey of
    optimization and performance analysis techniques,” *IEEE Access*, vol. 8, pp.
    202 795–202 818, 2020.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] C. Huang, S. Hu, G. C. Alexandropoulos, A. Zappone, C. Yuen, R. Zhang,
    M. Di Renzo, and M. Debbah, “Holographic MIMO surfaces for 6G wireless networks:
    Opportunities, challenges, and trends,” *IEEE Wireless Communications*, vol. 27,
    no. 5, pp. 118–125, 2020.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Q. Wu and R. Zhang, “Towards smart and reconfigurable environment: Intelligent
    reflecting surface aided wireless network,” *IEEE Communications Magazine*, vol. 58,
    no. 1, pp. 106–112, 2020.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] C. Pan, H. Ren, K. Wang, M. Elkashlan, M. Chen, M. Di Renzo, Y. Hao,
    J. Wang, A. L. Swindlehurst, X. You *et al.*, “Reconfigurable intelligent surface
    for 6g and beyond: Motivations, principles, applications, and research directions,”
    *arXiv preprint arXiv:2011.04300*, 2020.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Taha, Y. Zhang, F. B. Mismar, and A. Alkhateeb, “Deep reinforcement
    learning for intelligent reflecting surfaces: Towards standalone operation,” in
    *2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless
    Communications (SPAWC)*, 2020, pp. 1–5.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] C. Huang, R. Mo, and C. Yuen, “Reconfigurable intelligent surface assisted
    multiuser miso systems exploiting deep reinforcement learning,” *IEEE Journal
    on Selected Areas in Communications*, vol. 38, no. 8, pp. 1839–1850, 2020.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Z. Yang, Y. Liu, Y. Chen, and J. T. Zhou, “Deep reinforcement learning
    for ris-aided non-orthogonal multiple access downlink networks,” in *GLOBECOM
    2020 - 2020 IEEE Global Communications Conference*, 2020, pp. 1–6.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
