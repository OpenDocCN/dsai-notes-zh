- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:45:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2206.05394] Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial
    and Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.05394](https://ar5iv.labs.arxiv.org/html/2206.05394)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alzayat Saleh alzayat.saleh@my.jcu.edu.au    Marcus Sheaves marcus.sheaves@jcu.edu.au
       Dean Jerry dean.jerry@jcu.edu.au    Mostafa Rahimi Azghadi mostafa.rahimiazghadi@jcu.edu.au
    College of Science and Engineering, James Cook University, 1 James Cook Drive,
    Townsville, 4811, QLD, Australia ARC Research Hub for Supercharging Tropical Aquaculture
    through Genetic Solutions, James Cook University, 1 James Cook Drive, Townsville,
    4811, QLD, Australia
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Marine ecosystems and their fish habitats are becoming increasingly important
    due to their integral role in providing a valuable food source and conservation
    outcomes. Due to their remote and difficult to access nature, marine environments
    and fish habitats are often monitored using underwater cameras to record videos
    and images for understanding fish life and ecology, as well as for preserving
    the environment. There are currently many permanent underwater camera systems
    deployed at different places around the globe. In addition, there exists numerous
    studies that use temporary cameras to survey fish habitats. These cameras generate
    a massive volume of digital data, which cannot be efficiently analysed by current
    manual processing methods, which involve a human observer. Deep Learning (DL)
    is a cutting-edge Artificial Intelligence (AI) technology that has demonstrated
    unprecedented performance in analysing visual data. Despite its application to
    a myriad of domains, its use in underwater fish habitat monitoring remains under
    explored. In this paper, we provide a tutorial that covers the key concepts of
    DL, which help the reader grasp a high-level understanding of how DL works. The
    tutorial also explains a step-by-step procedure on how DL algorithms should be
    developed for challenging applications such as underwater fish monitoring. In
    addition, we provide a comprehensive survey of key deep learning techniques for
    fish habitat monitoring including classification, counting, localization, and
    segmentation. Furthermore, we survey publicly available underwater fish datasets,
    and compare various DL techniques in the underwater fish monitoring domains. We
    also discuss some challenges and opportunities in the emerging field of deep learning
    for fish habitat processing. This paper is written to serve as a tutorial for
    marine scientists who would like to grasp a high-level understanding of DL, develop
    it for their applications by following our step-by-step tutorial, and see how
    it is evolving to facilitate their research efforts. At the same time, it is suitable
    for computer scientists who would like to survey state-of-the-art DL-based methodologies
    for fish habitat monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Marine Science\sepComputer Vision\sepConvolutional Neural Networks\sepImage
    and Video Processing\sepMachine Learning\sepDeep Learning\sepDeep Neural Networks.\UseRawInputEncoding\cortext
  prefs: []
  type: TYPE_NORMAL
- en: '[cor1]Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proper understanding of our planet and its ecosystems is not possible unless
    suitable tools are developed to explore and learn about our largest ecosystem,
    the marine environment. Computer Vision (CV) technology through deployment of
    its underwater cameras can help us better comprehend and manage remote marine
    fish habitats. However, due to the sheer volume of their visual data, manual processing
    is time- and cost-prohibitive, requiring a new radical shift in data analysis,
    through advanced technologies such as Deep Learning (DL).
  prefs: []
  type: TYPE_NORMAL
- en: DL is at the frontier of computer vision. Its deep neural network architectures
    are capable of learning complex mappings from high-dimensional data to interpretable
    feature representations, hence, DL has been successfully applied to various challenging
    computer vision tasks such as semantic image segmentation (Jing et al., [2020](#bib.bib46);
    Pathak et al., [2015](#bib.bib96); Laradji et al., [2021a](#bib.bib66); [Qi et al.,](#bib.bib98)
    ; Chuang et al., [2011](#bib.bib18)), visual object detection (Wang et al., [2018](#bib.bib126);
    Villon et al., [2016](#bib.bib123); Kim et al., [2016](#bib.bib53); Pathak et al.,
    [2018](#bib.bib95)), and tracking (Garcia et al., [2016](#bib.bib28); Duan and
    Deng, [2019](#bib.bib25); Kang et al., [2018](#bib.bib49); Lumauag and Nava, [2019](#bib.bib81)).
    These applications have the potential to radically alter the way we interact with
    the world through computers. Recently, the applications of DL and its underlying
    Deep Neural Networks for underwater visual processing have received significant
    attention (Saleh et al., [2020a](#bib.bib108); Laradji et al., [2021b](#bib.bib68);
    Villon et al., [2018](#bib.bib124); Chuang et al., [2016](#bib.bib17); Nilssen
    et al., [2017](#bib.bib91); Mandal et al., [2018](#bib.bib84); Naseer et al.,
    [2020](#bib.bib89); Salman et al., [2020](#bib.bib113); Siddiqui et al., [2018](#bib.bib118)).
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of deep learning is its ability to learn features in different
    data types, such as underwater fish images, through end-to-end training. Training
    of DNNs is often thought to be easy. Many frameworks take delight in providing
    few lines of code that solve some CV tasks, providing the misleading impression
    that all that is needed is then plug and play, using some general Application
    Programming Interfaces (APIs). In these APIs, the developers have lifted the burden
    from us and, in doing so, disguised the complexity behind a few lines of code
    needed to achieve the task at hand. The framework developers have achieved the
    purpose of "providing a few lines of code" but we, the end-users, have been fooled
    to believe we need to spend only a few hours learning the intricacies of the provided
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to training a DL algorithm, things become more complicated.
    The task of training a DNN is actually as complicated as the problem it is intended
    to solve. In fish monitoring for example, the number of input images you use,
    how you pre-process your images, how you build your models, how you fine-tune
    the model (using dropout or regularization, for example), how you extract the
    features, how you combine them to produce final predictions, what metric you use
    to report your model performance, and your choice of which layer to extract features
    from to feed to your classifier, are among some of the many variables to consider
    when training a DNN. You can include any number of variations on these factors
    to further optimize your model and to achieve the best possible accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the above intricacies, most of the time DNNs are not simply an "off-the-shelf"
    technology that works with all kind of datasets, even those similar to the one
    that has been meticulously customised for it. The fact that training a customised
    high-performance DNN is rigorous and challenging is now widely accepted. However,
    this challenging process can be facilitated by being patient, paying attention
    to details, and working systematically. Developing customised DNNs with a specific
    application, for example, for underwater fish monitoring, should follow the same
    systematic steps of developing any other computer vision applications ( e.g. detection
    of vehicles in traffic). The only difference lies in the type of data being fed
    to the DNN.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we first present a tutorial that covers the background of DL
    to help understand the above-mentioned common DL terminologies. The tutorial also
    provides a comprehensive overview of the essential systematic steps to help better
    develop a supervised DL model, with a focus on underwater fish habitat monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the paper, we survey state-of-the-art research and development
    on the use of DL for fish monitoring. We synthesize the literature into four main
    categories covering the common CV tasks of classification, counting, localization,
    and segmentation of fish images. We investigate different deep learning architectures
    and their performance. We also survey publicly available underwater fish image
    datasets. Finally, we provide a comprehensive overview of the challenges in applying
    DL to marine fish monitoring domains. We also draw a roadmap for future research
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Although a number of previous relevant review articles (Goodwin et al., [2022](#bib.bib32);
    Li and Du, [2021](#bib.bib73); Zhao et al., [2021](#bib.bib138); Yang et al.,
    [2021](#bib.bib135); Li et al., [2020](#bib.bib76); Moniruzzaman et al., [2017](#bib.bib88);
    Saleh et al., [2022](#bib.bib111)) exist, our paper has a different approach and
    motivation that compliments prior surveys. Compared to (Goodwin et al., [2022](#bib.bib32)),
    which provides a survey of the general domain of ecological data analysis, covering
    a wide array of studies on plankton, fish, marine mammals, pollution, and nutrient
    cycling, we focus only on fish monitoring. We also provide a detailed analysis
    of fish datasets and comprehensively review the literature on four key tasks in
    underwater fish video and image processing. This detailed analysis and review
    is not provided in (Goodwin et al., [2022](#bib.bib32)), or any of the previous
    works, making our paper useful for readers who would like to study fish monitoring
    using DL in more details and depth, while seeing a comprehensive literature review.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, (Li and Du, [2021](#bib.bib73)) provides a review of studies on
    fish condition, growth, and behavior monitoring in aquaculture settings. It briefly
    covers and reviews various DL architectures and their aquaculture applications,
    unlike the present communication that is focused mainly on Convolutional Neural
    Network (CNN) and provides a detailed survey and analysis of the underwater fish
    monitoring literature.
  prefs: []
  type: TYPE_NORMAL
- en: The work presented in (Zhao et al., [2021](#bib.bib138)) covers the general
    domain of Machine Learning, as opposed to the specific domain of DL in our paper.
    This is done for aquaculture applications as wide as fish biomass and behavior
    analysis to water quality predictions, while also briefly covering and reviewing
    fish classification and detection methods.
  prefs: []
  type: TYPE_NORMAL
- en: A survey of computer vision models for fish detection and behavior analysis
    in digital aquaculture is provided in (Yang et al., [2021](#bib.bib135)). An interested
    reader should study (Yang et al., [2021](#bib.bib135)) before reading our paper,
    due to the background technical details provided on image acquisition, which are
    key to developing effective DL datasets and models, as we discussed in our paper.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the DL-based studies presented in (Li et al., [2020](#bib.bib76))
    and (Moniruzzaman et al., [2017](#bib.bib88)) are mainly around the two specific
    tasks of underwater fish tracking, and underwater object detection, respectively.
    These applications are different to our study. However, since our underwater fish
    monitoring task are related to these applications, our paper can complement these
    works.
  prefs: []
  type: TYPE_NORMAL
- en: In (Saleh et al., [2022](#bib.bib111)), we have provided a historical survey
    of fish classification methods between the years 2003-2021\. These methods cover
    traditional CV techniques and modern DL methods, only for fish classification
    in underwater habitats and not for the general domain of underwater fish habitat
    monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a sub-field of machine learning composed of interrelated algorithms
    and concepts used in training a deep neural network (Saleh et al., [2022](#bib.bib111)).
    One of the main reasons behind the extereme popularity of deep learning is the
    unprecedented and unparalleled performance it has achieved across different fields
    especially image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning utilizes multi-layered neural networks for automatic learning
    of input features. Features are distinguishing properties of learning inputs e.g.
    the color or shape of different fish. The deep learning concept was first proposed
    based on the idea that the traditional multi-layer artificial neural networks,
    could learn complex nonlinear features and their relations with more generalization
    and at a rapid speed. To learn deep features efficiently, researchers found that
    a modified version of neural networks, i.e. CNN, works very well in the image
    processing field (Saleh et al., [2022](#bib.bib111)). In the following sections,
    we will first introduce the basic concepts of neural networks in general and then
    describe CNNs and explain how they learn and then process input images.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A ’neural network’ is a computational model that is inspired by biological neural
    systems and uses simple, non-linear, computational rules to mimic these systems.
    Neural networks are composed of simple processing elements called, neurons. By
    organising neurons in a layered structure, interconnecting them and changing the
    weights associated with each interconnection, a ’neural network’ can be trained
    to solve a complex problem, such as recognising if a fish is present in an image.
    It is then possible to store the connections between neurons for later use. Training
    a neural network to perform different tasks e.g. recognizing fish in an image,
    or determining where a fish is in an underwater image, is called the ’learning
    process’. During supervised learning (explained later), the inputs to the network
    are presented with each input having a desired output. The learning process determines
    which interconnections (weights) are most important to the system for learning
    the task at hand and mapping all the inputs to all their desired outputs, as best
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea of neural networks is to have layers of neurons for learning
    the input data. There are three consecutive layer types in a neural network, i.e.
    input, hidden, and output. The hidden layers can learn the patterns in the data
    passed to the network through the input layer. It is within the hidden layers
    that classification, or in some cases regression, of the input data takes place.
    The hidden layers can learn abstract patterns and features in the data on their
    own. In general, there will be more layers in a DNN compared to artificial (shallow)
    networks for image classification tasks, and this is why DNNs are called deep
    and can achieve higher accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Neuron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The neuron, also known as a node or perceptron in a neural network, is its
    basic unit of computing. The neuron takes inputs from other nodes and produces
    an output. Every input has a weight $w$ that is allocated based on its relative
    significance to other inputs. As depicted in Figure [1](#S2.F1 "Figure 1 ‣ 2.1.4
    Loss Function ‣ 2.1 Neural Networks ‣ 2 Deep Learning ‣ Applications of Deep Learning
    in Fish Habitat Monitoring: A Tutorial and Survey"), the node applies the activation
    function $f$ (described below) on the weighted sum of its inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Activation Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The activation function (Vogels et al., [2005](#bib.bib125)) in a neural network
    defines whether a given node is "activated" or not based on the weighted sum of
    input features. The sigmoid function is one of the most commonly used activation
    functions. It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S(x)=\frac{1}{1+e^{-x}}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $S(x)$ is the sigmoid function output that will be used as the input for
    the following node and $x$ is the weighted sum of input features from the previous
    layer. The sigmoid function is non-linear and its value ranges between 0 and 1\.
    Sigmoid is popular in image classification because its 0-1 range can be represented
    as the probability of "activating" each output class. The output with the largest
    "activation" value is then selected, thus facilitating the network’s ability to
    classify the image.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Bias Node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another important component in successful neural networks are the "bias" nodes,
    which, as shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1.4 Loss Function ‣ 2.1 Neural
    Networks ‣ 2 Deep Learning ‣ Applications of Deep Learning in Fish Habitat Monitoring:
    A Tutorial and Survey"), add a bias value $b$ to the sum of input-weight multiplications
    to increase the model’s flexibility. In particular, when all input features equal
    to 0, the network can adjust to the data and decrease the distance between the
    fitted values in other data spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Loss Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In machine learning, there is always a function that needs to be decreased
    or increased to reach the closest possible mapping between the input and output
    domains. This function is usually known as the objective function. When it needs
    to be minimised, for instance for the case of neural network supervised learning,
    we might refer to it as the cost, loss, or error function. Although different
    DL publications may define specific meanings for some of these terms, we use them
    indiscriminately in this paper. In general, loss functions measure the performance
    of a data-based Machine Learning (ML) model. The loss function is important to
    consider, as it measures and presents learning error in the form of a single real
    number between predicted values and expected values. As an example, the loss function
    for linear regression is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\frac{1}{2m}\sum_{i=1}^{m}(\hat{y}-y)^{2},$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is the number of training examples, $\hat{y}$ is the predicted value
    of the model, and $y$ is the true value of the inputs in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: For classification tasks, such as fish species classification, the loss function
    $L$ is generally a cross-entropy loss function. Cross-entropy loss measures the
    performance of a classification model with a probability value ranging from 0
    to 1\. The loss of cross-entropy functions will increase as the predicted probability
    differs from the ground truth. Another classification loss is Hinge Loss. In Hinge
    Loss, the correct category score should, by some safety margin, be higher than
    the sum of values for all incorrect categories.
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture ltx_centering" height="212.99" overflow="visible"
    version="1.1" width="291.73"><g transform="translate(0,212.99) matrix(1 0 0 -1
    0 0) translate(145.87,0) translate(0,67.13)"><g stroke-width="0.28453pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -124 116.38)" color="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    stroke-width="0.28453pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -124 37.64)" color="#000000"><foreignobject width="11.78" height="8.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    stroke-width="0.28453pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -124 -41.1)" color="#000000"><foreignobject width="11.78" height="8.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    stroke-width="0.85358pt" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0
    0.0 0.0 1.0 -44.75 41.1)" color="#0000FF"><foreignobject width="89.5" height="24.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f(\sum_{i=1}^{n}{x_{i}w_{i}+b})$</foreignobject></g><g
    stroke-width="0.28453pt" fill="#000000" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 112.56 34.64)" color="#000000"><foreignobject width="11.11" height="9.46"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g><g
    stroke="#000000" fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -77.94 96.69)" fill="#000000" stroke="#000000"><foreignobject width="14.15"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -77.94 47.48)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -77.94 6.14)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$w_{3}$</foreignobject></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Diagram of the perceptron neuron. Inputs $x_{i}$ are multiplied in
    the weights $w_{i}$. The neuron body (blue) accumulates the sum of all multiplication
    inputs and then fires an output signal $Y$ according to its activation function
    $f$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.5 Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In supervised learning, the learning task can be reduced to an optimization
    problem in the form of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta^{*}=\arg\min_{\theta}g(\theta),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\theta$ is a parameter vector, at which the loss function $g(\theta)$
    that usually represent the average loss for all training examples, reaches its
    minimum. $g$ can be represented as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g(\theta)=\frac{1}{n}\sum_{i=1}^{n}L\left(f_{\theta}\left(x_{i}\right),y_{i}\right),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $(x_{i},y_{i})$ represents a (input, desired output) training pair.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in DL, an optimization method is used to train the neural network
    by minimising the error function $E$ that is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $E(W,b)=\sum_{i=1}^{m}L\left(\hat{y}_{i},{y}_{i}\right)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $W$ and $b$ are the weights and biases of the network, respectively. The
    value of the error function $E$ is thus the sum of the mean squared loss $L$ between
    the predicted value $\hat{y}$ and true value $y$, for m training examples. The
    value of $\hat{y}$ is obtained during the forward propagation step and makes use
    of the previously-mentioned weights and biases of the network, which can be initialised
    in different ways. Optimization minimizes the value of the error function $E$
    by updating the values of the trainable parameters $W$ and $b$.
  prefs: []
  type: TYPE_NORMAL
- en: The error function $E$ is usually minimised by using its gradient slopes for
    the parameters. The most commonly used optimization method is Gradient Descent
    (Sun et al., [2019](#bib.bib119)), in which the gradient is optimized by calculating
    a matrix of partial derivatives (computed using backpropagation, as detailed in
    the next subsection). These derivatives provide the slope of $g$ simultaneously
    at each dimension of $\theta$. Therefore, the gradient is used to determine the
    next direction to search for the Global Optimum. To enhance $\theta$ and reach
    a lower $g$, a small quantity is subtracted from $\theta$ in the optimal direction
    (since the gradient provides the direction of the rise and conversely the descent
    in $g$), such that the global optimum is eventually reached and $g$ is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.6 Backpropagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Backpropagation is probably the most important part of learning in neural networks.
    It is performed after a forward propagation or pass, in which a subset of the
    training dataset (named a batch) $\left\{\left(x_{i},y_{i}\right)\right\}_{i=1}^{m}$
    and the current network parameters $\theta$ are used to calculate the final layer
    output and the loss. During the forward pass, the data input is passed to the
    first layer to process according to its activation function and their values are
    passed on to the next layer, hence the term "forward pass". After the forward
    pass and calculating the final layer loss, backpropagation happens, through which
    we start to calculate the loss backwards, layer by layer, and the layer derivatives
    are then "chained" by the local gradients to minimise the overall loss, $g$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.7 Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Regularization is another important concept in neural networks learning. It
    is a technique that makes small changes to the learning algorithm to improve the
    performance of the model on testing or out-of-sample data (Bisong and Bisong,
    [2019](#bib.bib8)). In other words, it avoids the risk of over-fitting the training
    data by discouraging the formation of complex mapping functions or models. Model
    regularization involves a regularization term being added to the general model
    loss function, which takes into account the loss function value for all the training
    dataset examples. Thus, when using regularization, the loss function $g(\theta)$
    (described in Eq. [4](#S2.E4 "In 2.1.5 Optimization ‣ 2.1 Neural Networks ‣ 2
    Deep Learning ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial
    and Survey")) becomes'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g(\theta)=\frac{1}{n}\sum_{i=1}^{n}L\left(f_{\theta}\left(x_{i}\right),y_{i}\right)+R\left(f_{\theta}\right),$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where, $R\left(f_{\theta}\right)$ is the added regularization function.
  prefs: []
  type: TYPE_NORMAL
- en: The most common forms of regularization are L1 and L2 (Ng, [2004](#bib.bib90)).
    The difference between them is that L2 is the sum of the square of the weights,
    while L1 is the sum of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Convolutional Neural Network (CNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most powerful class of DNNs are convolutional neural networks. As their
    name infers, convolutional networks work by performing a convolution (filtering)
    operation on the input data. A CNN is usually composed of several convolution
    layers, which extract useful features from the input data by sliding convolution
    filters across the input image represented to the network as matrices. One of
    the first successful examples of the use of CNNs in computer vision was AlexNet
    proposed by Krizhevsky *et al.* in 2012\. AlexNet achieved great success only
    using four convolutional layers. Since 2012, many different flavours of CNNs have
    been proposed using different architectures and count of convolutional and other
    complementary layers. These architectures have revolutionized computer vision
    and image processing in different domains from agriculture (Olsen et al., [2019](#bib.bib93))
    to medicine (Saleh et al., [2021](#bib.bib110)). CNNs have also been widely applied
    in underwater visual monitoring and processing for counting, localizing, classifying,
    and segmenting objects of interest such as fish (Saleh et al., [2020b](#bib.bib109)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b4cf941d5aaa31c95ca23e67ac8038b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Schematic diagram of a CNN architecture used for the classification
    of fish images. The architecture consists of five convolutional layers that include
    the batch norm operation within them, followed by pooling layers (conv1-conv5).
    In this model, the feature maps from convolutional layers are pooled through pooling
    layers then flattened through two fully connected layers (fc6 and fc7). The classification
    output is the result of a fully connected layer and a softmax activation layer
    (fc8+softmax).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical CNN architecture is composed of convolutional layers, pooling layers,
    non-linear activation layers, and final output layers, as shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2.2 Convolutional Neural Network (CNN) ‣ 2 Deep Learning ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey"). It is through
    the filtering convolution operation combined with other parts of the CNN that
    useful features of the input data are extracted and learned automatically. The
    learning of a CNN usually involves finding the appropriate number, size, and structure
    of convolution filters, pooling layers, and activation functions and their parameters
    during training and seeing various examples of the inputs. In the below subsections,
    we will cover these basic building blocks and layers of a typical CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convolutional layer: As already mentioned, a convolutional layer applies a
    filtering (convolution) operation on its input matrix data to generate another
    matrix called a feature map. The input matrix can contain the input image information
    or the feature map generated by a previous CNN layer. The feature maps are the
    core of a CNN, where useful features of an input are extracted and learned across
    several convolutional layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch Normalization: The goal of this operation, which follows the convolutional
    operation, is to normalize the learning of the network across the current set
    of training data (batch), hence the name batch normalization. This is done to
    improve the speed of learning and the convergence of the deep learning model,
    because otherwise, the network may see very wide variety of features extracted
    in its convolutional layers, due to wide input variations. Batch normalization
    happens by subtracting its input mean and dividing the result by its standard
    deviation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activation layer: This layer that follows the batch normalization layer is
    the normal neuron activation function explained earlier. It is used to increase
    the non-linearity of the convolutional layer output and increase its power in
    learning complex data. The most common activation functions used in conjunction
    with convolutional layers are Rectified Linear Unit (ReLU) and Sigmoid. Activation
    functions are also used in the final non-convolutional fully-connected layers
    of a CNN. A common output activation function is Softmax.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pooling layer: The output feature map of the convolutional layer that is batch
    normalized and passed the activation function, is often too big for the next convolutional
    layer to handle. To reduce its size and improve the efficiency of computation,
    it can be pooled in a pooling layer to generate a reduced sized feature map, while
    keeping important features. Pooling is a common operation in CNNs and is used
    in almost all practical convolutional networks. The most common pooling layers
    are max pooling and average pooling.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout: To avoid overfitting to the training data, dropout operations is introduced
    after the pooling layers. Their task is to cut the network’s dependence to a single
    data instance at each traing step, by randomly removing (dropping out) features
    extracted using the previous convolutional layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fully connected layer: Fully connected layer, also known as dense layer, is
    the second last layer of a CNN, before the output layer. This layer contains a
    small number of neurons, each of which connected to every neuron in the previous
    layer. So the network is said to be fully connected. The fully connected layer
    takes all the inputs and weights from the previous layer, and combines them together
    into a single vector or matrix. This vector is then passed through an activation
    function, such as the sigmoid, to calculate output values of the CNN generated
    by its final output layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two main approaches to learning in general DL. These include unsupervised
    and supervised learning. Unsupervised learning is often used to discover the structure
    and composition of the input and output domains without explicit and supervised
    target domain. This approach enables generalization from one input domain to another
    by transforming data representations that are not directly related to the data
    distribution of target domain.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised learning approach, on the other hand, is designed to explicitly
    map data from the input domain to its output domain via training pairs that exhibit
    matching representations. These pairs are carefully crafted by a human (supervisor),
    hence the name. The training process of supervised learning can suffer from instability
    and is less effective than the unsupervised learning method, because it learns
    with an accurate target distribution without domain-specific knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised deep learning uses a subtle deep neural network mechanism to extract
    useful features from large amounts of input training data that are labelled to
    show their desired output domain. The learning is done by using the repetitive
    backpropagation process (Rojas and Rojas, [1996](#bib.bib105)) explained earlier,
    to adjust the DL architecture internal parameters, such as the shape, number,
    and size of convolutional, pooling, and fully connected layers, that have been
    used to determine the representation in each layer from the representation in
    the preceding layer. In general, adjusting the DL architecture and its parameters
    to do the best mapping of the input training data to their desired output, as
    best as possible, is the same as optimising a function $f$, through backpropagation,
    to map the input domain $X$, to its matching output domain $Y$, i.e. ($f:X\mapsto
    Y$).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Developing Deep Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a71bbdc093c9a88f6304a86954a60b11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A schematic diagram showing the steps and components required for
    training a deep learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A comprehensive overview of the essential systematic steps for training a DL
    model is summarized in Figure [3](#S3.F3 "Figure 3 ‣ 3 Developing Deep Learning
    Models ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial
    and Survey"). Even though these steps are general in DL training, we included
    useful tips arising from our experience in developing DL applications in various
    domains from medical imaging to marine science applications. Nevertheless, we
    put an emphasis on the development of DL for underwater fish habitat monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Training Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The available training data is essential for developing an efficient DL model.
    Datasets are becoming increasingly crucial, even more so than algorithms. Perhaps,
    the most important factor when considering a supervised learning dataset is its
    size. The requirement for a large training dataset to achieve high accuracy is
    often a big obstacle. Because visual algorithms are trained by pairs of images
    and labels, in a supervised manner, they can only identify what has already been
    given to them. As a result, depending on the project, the number of objects to
    identify, and the required performance, training datasets might contain hundreds
    to millions of images. However, smaller training datasets with only a few hundred
    samples per class may also achieve good results (Saleh et al., [2020b](#bib.bib109);
    Konovalov et al., [2019a](#bib.bib56), [2018](#bib.bib57), [b](#bib.bib58)). Nevertheless,
    the larger the training dataset, the greater the recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the scarcity of datasets and the difficulty of acquiring reliable
    data, approaches for boosting the accuracy rate from small samples will inevitably
    become a focus of future studies. The problem of limited sample data can be also
    alleviated by transfer learning (Mathur et al., [2020](#bib.bib85); Molchanov
    et al., [2016](#bib.bib87); Lee et al., [2018](#bib.bib71)). Furthermore, data
    augmentation will become increasingly critical. Section [5.3](#S5.SS3 "5.3 Dataset
    Limitation ‣ 5 Challenges in underwater fish monitoring ‣ Applications of Deep
    Learning in Fish Habitat Monitoring: A Tutorial and Survey") covers some challenges
    of limited data and some approaches to address these challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: The second factor to consider when preparing a dataset for DL training is having
    a balance. This is critical to ensure that each class to be identified contains
    a sufficient number of instances to minimise class imbalance biases. These biases
    happen when the DL favours one or more classes due to seeing them more often when
    being trained.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the training dataset is typically divided into two subsets, the training
    subset for efficiently training the model and the validation/test subset for assessing
    the trained model’s performance. For the training subset, a subset of the training
    dataset is reserved for training the model. If the training subset is too large,
    it can prolong the model training. If, on the other hand, the training subset
    is too small, the resulting model may not generalise well to unseen inputs. The
    validation/test subset is typically used to avoid overfitting, which is a common
    problem in machine learning and happens when the developed model simply memorises
    the inputs rather than properly learning them. Cross-validation is another widely
    used methodology for testing a DL model’s training performance, by splitting the
    training dataset into multiple mutually exclusive subsets of training and testing
    data. One method of cross-validation is called $k-fold$ cross-validation, in which
    the training dataset is split into $k$ equally sized subsets. In this method,
    $k-1$ folds are used for training the model, while the remaining fold is used
    to test the learning performance. This process is repeated until all the folds
    have been used once as a test/validation set.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the above, it is usually vital to, initially and before embarking
    on code development, perform a comprehensive inspection of the dataset. This will
    help to clean the dataset, for instance by finding and removing duplicate data
    instances. It also helps identify imbalances and biases, as well as data distribution,
    trends, or outliers, which will help in better model design and understanding
    of possible wrong DNN predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in the domain of fish habitat monitoring, researchers currently
    have access to a variety of datasets. Table [1](#S3.T1 "Table 1 ‣ 3.2 Development
    framework ‣ 3 Developing Deep Learning Models ‣ Applications of Deep Learning
    in Fish Habitat Monitoring: A Tutorial and Survey") lists publicly available underwater
    fish datasets, their sources, and where to get them, in addition to a summary
    of their features, their labels, and their sizes. The main point to note about
    these datasets is that they differ in both size and the number of features. Although
    the number of these fish datasets is still small (17), the diversity of aquatic
    species they cover is already quite wide. They cover a large number of aquatic
    species, as indicated in Fig.  [4](#S3.F4 "Figure 4 ‣ 3.2 Development framework
    ‣ 3 Developing Deep Learning Models ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey"). Moreover, each dataset features a different
    number of images that have varying resolutions. For each image, there is also
    a ground truth annotated by a human expert, which make them very useful. For instance,
    these datasets can be used by researchers to test their DL models or to pre-train
    them, as the first step, for their more specific fish monitoring tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: After preparing the training dataset or utilising alternative approaches to
    addressing insufficient data challenge, one can start developing their DL model
    using a machine-learning development framework.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Development framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rapid evolution of DL has led to the creation of a vast number of development
    libraries and packages that enable the setting up of DNNs with insignificant effort.
    Usability and availability of resources, architectural support, customisability,
    and hardware support are all various benefits of using existing machine-learning
    frameworks. The most commonly used frameworks are PyTorch, Tensorflow, MATLAB,
    Microsoft Cognitive Toolkit (CNTK) and Apache MXNET. In the context of DL for
    marine research, as will be shown later in Tables [3](#S4.T3 "Table 3 ‣ 4.2 Counting
    ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey") to [5](#S4.T5
    "Table 5 ‣ 4.4 Segmentation ‣ 4 Applications of Deep Learning in Underwater Fish
    Monitoring ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial
    and Survey"), PyTorch and TensorFlow are the dominant frameworks, while Matlab
    and Caffe have been used only in a few works. Overall, details such as the project
    needs and the programmer and developer preference should be taken into account,
    when choosing the development framework.'
  prefs: []
  type: TYPE_NORMAL
- en: When the development framework is chosen, the next step is to find the most
    suitable network architecture for the task at hand. This sometimes depends on
    the framework, as some recent methods may not immediately be supported by all
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of some publicly available datasets containing fish for training
    and testing deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Summary | Labels | Dataset size | Website |'
  prefs: []
  type: TYPE_TB
- en: '| A - Deepfish | Videos from coastal habitats in north-eastern and western
    Australia | fish/no fish | 40k classification labels, 3.2k images with point-level
    annotations, 310 segmentation masks | [github.com/alzayats/DeepFish](https://github.com/alzayats/DeepFish)
    |'
  prefs: []
  type: TYPE_TB
- en: '| B - Croatian Fish Dataset | 12 species of fish found in Croatian waters |
    species names | 794 classification labels | [www.inf-cv.uni-jena.de/fine_grained_recognition.html#datasets](http://www.inf-cv.uni-jena.de/fine_grained_recognition.html#datasets)
    |'
  prefs: []
  type: TYPE_TB
- en: '| C - Fish in seagrass habitats | RUV taken in Australian seagrass habitat
    of 2 species | species | 9k classification labels, bounding boxes and segmentation
    masks | [github.com/globalwetlands/luderick-seagrass](https://github.com/globalwetlands/luderick-seagrass)
    |'
  prefs: []
  type: TYPE_TB
- en: '| D - Fish4Knowledge | Fish detection and tracking dataset, 17 videos at 10
    min long, rate of 5 fps. | fish/no fish | 3.5k bounding boxes | [groups.inf.ed.ac.uk/f4k/index.html](https://groups.inf.ed.ac.uk/f4k/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| E - Fish-Pak | Image dataset of 6 different fish species from 3 locations
    in Pakistan | species | 1k classification labels | [data.mendeley.com/datasets/n3ydw29sbz/3](https://data.mendeley.com/datasets/n3ydw29sbz/3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| F - Labeled Fishes in the Wild | Rockfish (Sebastes spp.) and other species
    (non-fish) near the seabed | fish/non-fish | 1k bounding boxes (fish), 3k (non-fish)
    | [swfscdata.nmfs.noaa.gov/labeled-fishes-in-the-wild/](https://swfscdata.nmfs.noaa.gov/labeled-fishes-in-the-wild/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| G - OzFish | Large data set comprising of 507 species of fish. | species,
    fish/no fish | 80k labeled cropped images, 45k bounding box annotations (fish/no
    fish) | [github.com/open-AIMS/ozfish](https://github.com/open-AIMS/ozfish) |'
  prefs: []
  type: TYPE_TB
- en: '| H - QUT Fish Dataset | 468 species in varying ex-situ and in-situ habitats.
    | species name | 4k classification images | [www.dropbox.com/s/e2xya1pzr2tm9xr/QUT_fish_data.zip?dl=0](https://www.dropbox.com/s/e2xya1pzr2tm9xr/QUT_fish_data.zip?dl=0)
    |'
  prefs: []
  type: TYPE_TB
- en: '| I - Whale Shark ID | 543 individual whale sharks (Rhincodon typus) | individuals
    | 7.8k bounding boxes | [http://lila.science/datasets/whale-shark-id](http://lila.science/datasets/whale-shark-id)
    |'
  prefs: []
  type: TYPE_TB
- en: '| J - Large Scale Fish Dataset | 9 different seafood types collected from a
    supermarket in Izmir, Turkey | species name | For each class, there are 1000 augmented
    images and their pair-wise augmented ground truths | [www.kaggle.com/crowww/a-large-scale-fish-dataset](https://www.kaggle.com/crowww/a-large-scale-fish-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| K - NCFM | Image dataset of 8 different fish species | species name | ~16000
    classification images | [www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/data](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| L - Mugil liza sonar | Sonar-based underwater videos of schools of migratory
    mullets (Mugil liza) | number of fish | 500 counting images | [zenodo.org/record/4751942#.YKzfUKgzayk](https://zenodo.org/record/4751942#.YKzfUKgzayk)
    |'
  prefs: []
  type: TYPE_TB
- en: '| M - MSRB Dataset | Real underwater images without marine snow and synthesized
    with marine snow | NA | ~6000 images | [github.com/ychtanaka/marine-snow](https://github.com/ychtanaka/marine-snow)
    |'
  prefs: []
  type: TYPE_TB
- en: '| N - WildFish | 1,000 fish categories | species name | ~54000 classification
    images | [github.com/PeiqinZhuang/WildFish](https://github.com/PeiqinZhuang/WildFish)
    |'
  prefs: []
  type: TYPE_TB
- en: '| O - SUIM | Image dataset of 8 different underwater objects | object name
    | ~1500 annotated images semantic segmentation mask | [github.com/xahidbuffon/SUIM](https://github.com/xahidbuffon/SUIM)
    |'
  prefs: []
  type: TYPE_TB
- en: '| P - DZPeru fish-datasets | Several species in varying ex-situ and in-situ
    habitats. | species name | ~17000 annotated images segmentation mask | [github.com/DZPeru/fish-datasets](https://github.com/DZPeru/fish-datasets)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Q - LifeCLEF | 10 different fish species | species name | ~1000 annotated
    videos | [www.imageclef.org/](https://www.imageclef.org/) | ![Refer to caption](img/377a6bb4ef0649359ca147eb9de0b61f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Sample images from publicly available datasets detailed in Table
    [1](#S3.T1 "Table 1 ‣ 3.2 Development framework ‣ 3 Developing Deep Learning Models
    ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network architecture is the structure of the DL model, which depends on what
    it intends to achieve and its expected input and output. Therefore, the type of
    training dataset and the expected outcome influence the architecture’s choice
    and its performance. DL network architectures can differ in a variety of ways
    such as the type and number of layers, their structure, and their order. Before
    selecting a network architecture, it is critical to understand the dataset you
    have and the task you are going to complete. For example, convolutional neural
    networks or CNNs are known to learn higher-order features, such as colours and
    shapes, from data within their convolution layers. Therefore, they are ideally
    adapted in image-based object recognition. On the other hand, Recurrent Neural
    Networks (RNNs) have the capability of processing temporal information or sequential
    data, such as the order of words in a sentence. This feature is ideal for tasks
    such as handwriting or speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of fish habitat monitoring, if you are working on a task that
    requires you to learn temporal information of the input sequence, for example
    fish image sequence analysis, the DL architecture you choose can be very important.
    For example, a CNNs architecture is more suited for *image-based* object recognition
    such as fish classification, while the RNN architecture is more suitable for tasks
    where the input sequence is temporal in nature such as generating fish habitat
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find a suitable architecture, you first need to define your problem. This
    problem is defined by two questions: (1) What features will you extract? (2) How
    will you label these features? The features you extract are defined by your data.
    In other words, you are interested in the representation of the data you have.
    The number of features you choose to extract is defined by the task you are trying
    to solve. As described above, the DL architectures can learn features such as
    colours and shapes from image-based object recognition. Before trying to construct
    your network, you first need to decide what data type you will use and how will
    you encode the information. After you have defined your task, you should think
    about what features are important for the task. You will need to define this in
    order to construct your network. For example, if the features you want to extract
    are fish shape and fish location, then you could define a convolutional architecture.
    The features you choose to define should be a subset of all the features in the
    data. For example, for an image-based object recognition network, you would extract
    features such as fish species. However, your extracted features will also need
    to cover all the data. For example, you will also need features of the type of
    water or the type of background. It is important to take all these features into
    account when defining your network. For a complete discussion on different DL
    architectures see (Khan et al., [2020](#bib.bib50)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Network Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a general network architecture is selected, the next step is to select,
    or sometimes develop, a network model of that architecture. For instance, when
    you decided to use a CNN, you can use different varieties of CNN models. The rule
    of thumb for selecting a CNN is to choose a model that results in a satisfactory
    training loss for your dataset. Creating an exotic and creative model is not recommended
    at this stage. It is usually recommended to avoid the temptation and choose a
    model big enough to overfit your dataset, and then regularise it properly to improve
    the validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, one may pick a well-known CNN model, e.g. ResNet, which can be
    used out-of-the-box, if their task is simple, e.g. fish classification. In later
    stages, they can customise their model to adequately capture their dataset. We
    show in Tables [3](#S4.T3 "Table 3 ‣ 4.2 Counting ‣ 4 Applications of Deep Learning
    in Underwater Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey") to [5](#S4.T5 "Table 5 ‣ 4.4 Segmentation
    ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey") in the next
    section that ResNet is the most commonly used model for fish counting (Table [3](#S4.T3
    "Table 3 ‣ 4.2 Counting ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring
    ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey")),
    fish localization (Table [4](#S4.T4 "Table 4 ‣ 4.3 Localization ‣ 4 Applications
    of Deep Learning in Underwater Fish Monitoring ‣ Applications of Deep Learning
    in Fish Habitat Monitoring: A Tutorial and Survey")), and fish segmentation (Table
    [5](#S4.T5 "Table 5 ‣ 4.4 Segmentation ‣ 4 Applications of Deep Learning in Underwater
    Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat Monitoring: A
    Tutorial and Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After choosing the best model is the time to set up a full train/validation
    pipeline. The below steps are recommended at this stage of development.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with a simple model (i.e. a small number of convolutional layers) that
    can hardly go wrong and visualise the model performance metrics. Do not use an
    out-of-the-box large model like ResNet, just yet. It is recommended to plot training
    loss to see how the network is progressing during learning and if the loss is
    getting smaller. This also shows the speed of learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better understand the process, it is recommended to use a fixed random seed
    (for randomly initialising the network parameters) to ensure that the same results
    can be achieved when running the code twice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Do not perform any data augmentation at this stage as it may introduce errors.
    You can do data augmentation at a later stage after confirming that your network
    works properly. You can see a brief introduction to data augmentation and other
    methods at subsection [5.2](#S5.SS2 "5.2 Model Generalisation ‣ 5 Challenges in
    underwater fish monitoring ‣ Applications of Deep Learning in Fish Habitat Monitoring:
    A Tutorial and Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use ADAM algorithm (Kingma and Ba, [2014](#bib.bib54)), which helps the learning
    by applying adaptive optimisation to the learning rate of the network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate is an important hyperparameter of a deep learning model. It
    is usually the most crucial value during training and should be configured using
    trial and error. Depending on the size of your dataset, a specific learning rate
    decay may be needed. The learning rate decay is a technique that allows the learning
    rate to fall during successive training epochs, until it converges. A high learning
    rate at the start prevents the network from memorising noisy data, whereas decaying
    the learning rate improves complex pattern learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement early stopping and monitor the learning process by looking at the
    training loss plot to prevent overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add complexity to your model gradually, e.g. add more layers or use off-the-shelf
    CNN models, and obtain a performance improvement over time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.6 Testing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the model is trained, its accuracy and performance should be tested using
    the test subset of the training dataset. A test set can also be independent to
    the training dataset to evaluate the model performance. The main point to remember
    is that the test set should not have been used for the training or evaluation
    of the model, at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model’s performance should be measured by computing appropriate metrics
    suitable to the task at hand. A list of most common metrics used in testing fish
    monitoring models are given in Tabel [2](#S3.T2 "Table 2 ‣ 3.8 Deploying the model
    ‣ 3 Developing Deep Learning Models ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey"). For classification tasks, Classification
    Accuracy (CA), Precision and Recall rates are appropriate metrics, while F1-score,
    which is a combination of precision and recall, can provide a better measure of
    model performance and is used in fish counting and localization tasks as shown
    in Tables [3](#S4.T3 "Table 3 ‣ 4.2 Counting ‣ 4 Applications of Deep Learning
    in Underwater Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey") and [4](#S4.T4 "Table 4 ‣ 4.3 Localization
    ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey"). The Intersection-Over-Union
    (IoU) is the appropriate metric for segmentation tasks, while the mean average
    precision (mAP) metric suits pixel-wise localization of fish in images. Looking
    at Tables [3](#S4.T3 "Table 3 ‣ 4.2 Counting ‣ 4 Applications of Deep Learning
    in Underwater Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey") to [5](#S4.T5 "Table 5 ‣ 4.4 Segmentation
    ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey"), other metrics
    such as Mean Square Error (MSE) and Root MSE (RMSE) have also been used in the
    marine fish monitoring literature. These can be considered and used if required.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Fine Tuning the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performance and accuracy of the model could be improved if needed. The amount
    of this improvement is, though, strongly influenced by its current accuracy. This
    step may quickly become complicated, since increasing the model accuracy might
    require several steps such as adjusting the learning rate, collecting new data,
    or fully modifying the model’s architecture. You should keep this fine tuning
    step to a reasonable level. Otherwise, the model might overfit the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Deploying the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, the model deployment mode should be chosen. This depends on the application
    and the deployment requirements. The model can be deployed to run on a local or
    remote device (on a web server, a docker container, a virtual private server (VPS),
    etc). This will determine whether the results can be accessed remotely or only
    within the local network. It is recommended to use a cross-platform deployment
    method to avoid issues such as input/output data format, or the type of files
    used for storing data.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used cross-platform model deployment method is Docker (Potdar
    et al., [2020](#bib.bib97); Abdul et al., [2019](#bib.bib1)), which is a virtualization
    software that allows setting-up and running other software environments on top
    of a base Linux distribution without the need to set-up virtual machines. Docker
    helps build, configure, and run applications using the same Docker file. Typically,
    Docker is the recommended approach for web applications. In this method, you can
    use Docker container or Docker host on your development machine. Docker container
    may be the easiest option for web applications. You can also deploy your network
    to a remote machine via Docker. The advantage of using a container is that you
    can share the development environment and run tests of your model using multiple
    docker containers. You can also install the Docker tool on your local machine
    to manage containers, so it is convenient.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8251d3cc46434c73d6c19e0cbe1c5c7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Illustration of four typical fish monitoring tasks. From left: Fish
    Classification (i.e. is there a fish in the image, or what type (class) of fish
    is in the image?); Fish Detection/Localization/Counting; Fish Semantic Segmentation,
    and Fish Instance Segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Performance metrics used to compare various surveyed works.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Performance Metric | Symbol Used | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Accuracy | CA | The percentage of correct predictions. For
    multi-class classification, CA is averaged among all the classes. $CA=(TP+TN)/(TP+TN+FP+FN)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | P | The fraction of true positives ($TP$), to the sum of $TP$
    and false positives ($FP$). $P=TP/(TP+FP)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | R | The fraction of true positives (TP) to the sum of TP and false
    negatives (FN). $R=TP/(TP+FN)$ |'
  prefs: []
  type: TYPE_TB
- en: '| F1 score | F1 | The harmonic mean of precision and recall. $F1=2~{}\times(P\times
    R)/(P+R)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Square Error | MSE | Mean of the square of the errors between predicted
    and observed values |'
  prefs: []
  type: TYPE_TB
- en: '| Root Mean Square Error | RMSE | Is the square root of the mean of the square
    of all of the errors. |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Relative Error | MRE | The mean error between predicted and observed
    values, in percentage |'
  prefs: []
  type: TYPE_TB
- en: '| L2 error | L2 | Root of the squares of the sums of the differences between
    predicted counts and the actual counts |'
  prefs: []
  type: TYPE_TB
- en: '| Intersection over Union | IoU | A metric that evaluates how similar the predicted
    bounding box is to the ground truth bounding box.  by dividing the area of overlap
    between the predicted and the ground truth boxes, by the area of their union.
    |'
  prefs: []
  type: TYPE_TB
- en: '| The maximum number | MaxN | MaxN, the maximum number of the target species
    in any one frame. |'
  prefs: []
  type: TYPE_TB
- en: '| Mean average precision | mAP | Depending on the detection difficulty, the
    mean $AP$ across all classes and/or total $IoU$ thresholds are used. |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Error | CE | Is how often is the classifier incorrect and
    also known as "Misclassification Rate". $CE=(FP+FN)/(TP+TN+FP+FN)$ |'
  prefs: []
  type: TYPE_TB
- en: 4 Applications of Deep Learning in Underwater Fish Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning has been widely used in marine environments with applications
    spanning from deep-sea mineral exploration (Juliani and Juliani, [2021](#bib.bib48))
    to automatic vessel detection (Chen et al., [2019](#bib.bib15)). However, we confine
    the scope of this paper to only marine fish image processing, which typically
    includes four tasks of classification, counting, localization, and segmentation
    of underwater fish images, as shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.8 Deploying
    the model ‣ 3 Developing Deep Learning Models ‣ Applications of Deep Learning
    in Fish Habitat Monitoring: A Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the goal is to assist the reader in understanding the similarities and
    differences across these tasks and their relevant DL models and techniques. We
    provide a background of what each task involves, what previous works have been
    published toward addressing it using deep learning, and synthesize the literature
    on each task.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As its name infers, in visual processing, classification is the task of classifying
    images into different categories. There can be only two categories, i.e. a binary
    classification, in which the images are classified into two groups, e.g. "fish"
    and "no fish", depending on the presence or absence of fish in an image (e.g.
    Deepfish dataset described in the first row of Table [1](#S3.T1 "Table 1 ‣ 3.2
    Development framework ‣ 3 Developing Deep Learning Models ‣ Applications of Deep
    Learning in Fish Habitat Monitoring: A Tutorial and Survey")). The classification
    can also involve multiple "classes" or groups. For instance, consider assigning
    different underwater fish images into different groups based on the species (e.g.
    FishPak dataset in Table [1](#S3.T1 "Table 1 ‣ 3.2 Development framework ‣ 3 Developing
    Deep Learning Models ‣ Applications of Deep Learning in Fish Habitat Monitoring:
    A Tutorial and Survey")) present in them.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a manual procedure, in which images in a dataset are compared and relative
    ones are classified based on similar features, but without necessarily knowing
    what you are searching for in advance. This is a difficult assignment as there
    could be thousands of images in the dataset. Moreover, many image classification
    tasks involve images of different objects. It rapidly becomes clear that an automatic
    system, such as a DNN, is required to complete this task quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Classification is the most widely-used and -studied underwater image processing
    task using DL. In a previous work, we have covered the use of DNNs specifically
    for the task of underwater fish classification. We refer the reader to (Saleh
    et al., [2022](#bib.bib111)) for a comprehensive review of prior art on classification.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Counting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of the counting task is to predict the number of objects existing
    in an image or video. Object counting is a key part of the workflow in many major
    CV applications, such as traffic monitoring (Khazukov et al., [2020](#bib.bib51);
    Zhang et al., [2017](#bib.bib136)). In the context of marine application and fish
    monitoring, counting may be used to map distinct species and monitor fish populations
    for effective conservation. With the use of commercially available underwater
    cameras, data gathering can be done more comprehensively. It is, however, difficult
    to correctly count fish in underwater habitats. To perform effective counting,
    models must understand the diversity of the items in terms of posture, shape,
    dimension, and features, which makes them complex. Meanwhile, manual counting
    is very time-consuming, costly, and prone to human error.
  prefs: []
  type: TYPE_NORMAL
- en: 'DL affords a faster, less expensive, and more accurate alternative to the manual
    data processing methods currently employed to monitor and analyse fish counts.
    Table [3](#S4.T3 "Table 3 ‣ 4.2 Counting ‣ 4 Applications of Deep Learning in
    Underwater Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat Monitoring:
    A Tutorial and Survey") lists several of the recent DL techniques used for fish
    counting. Saleh et al. (Saleh et al., [2020a](#bib.bib108)) created a novel large-scale
    dataset of fish from 20 underwater habitats. They used Fully Convolutional Networks
    for several monitoring tasks including fish counting and reported a Mean Average
    Error (MAE) of $0.38\%$. DL has the potential to be a more accurate method for
    assessing fish abundance than humans, with results that are stable and transferable
    between survey locations. Ditria et al. (Ditria et al., [2021](#bib.bib22), [2020a](#bib.bib23),
    [2020b](#bib.bib24)) compared the accuracy and speed of DL algorithms for estimating
    fish population in underwater pictures and video recordings to human counterparts
    in order to test their efficacy and usability. In single image test datasets,
    a DL method performed $7.1\%$ better than human marine specialists and $13.4\%$
    better than citizen scientists. For video datasets, DL was better by $1.5\%$ and
    $7.8\%$ compared to marine and citizen scientists, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of recent DL research works performing the task of fish counting'
  prefs: []
  type: TYPE_NORMAL
- en: '| Article | DL Model | Framework | Data | Annotation/Pre-processing/Augmentation
    | Classes and Labels | Perf. Metric | Metric Value | Comparisons with other methods
    |'
  prefs: []
  type: TYPE_TB
- en: '| A realistic fish-habitat dataset to evaluate algorithms for underwater visual
    analysis Saleh et al. ([2020a](#bib.bib108)) | ResNet-50 CNN | Pytorch | Authors-created
    database containing 39,766 images for 20 habitats from remote coastal marine environments
    of tropical Australia and split to sub-dataset for four computer vision tasks:
    classification, counting, localization, and segmentation. | Each image was annotated
    by point-level and semantic segmentation labels | 20 classes of 20 different fish
    habitat. | MAE | 0.38 | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Annotated Video Footage for Automated Identification and Counting of Fish
    in Unconstrained Seagrass Habitats Ditria et al. ([2021](#bib.bib22)) | ResNet-50
    CNN | Pytorch | The dataset consists of 4,281 images and 9,429 annotations (9,304
    luderick, 125 bream) at the standard high resolution (1920 x 1080 p). | Each image
    was annotated by drawing a bounding box and segmentation mask | 2 classes of fish
    | F1 | 92% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Automating the Analysis of Fish Abundance Using Object Detection Optimizing
    Animal Ecology With Deep Learning Ditria et al. ([2020a](#bib.bib23)) | Mask R-CNN
    ResNet50 | Pytorch | Authors-created database containing 6,080 fish images from
    20 habitats from Tweed River Estuary in southeast Queensland | Each image was
    annotated by segmentation mask | 1 class of fish | F1 | Image (95.4%) Video (86.8%)
    | The computer’s performance in determining abundance was 7.1% better than human
    marine experts and 13.4% better than citizen scientists in single image test datasets,
    and 1.5% and 7.8% higher in video datasets, respectively. |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning for automated analysis of fish abundance: the benefits of training
    across multiple habitats Ditria et al. ([2020b](#bib.bib24)) | Mask R-CNN ResNet50
    | Pytorch | Authors created five datasets, each consisting of 4700 annotated luderick,
    total of 23500 images | Each image was annotated by drawing a Polygonal segmentation
    masks around the region of interest (ROI) | 1 fish class | F1 | 87–92% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning with self-supervision and uncertainty regularization to count
    fish in underwater images Tarling et al. ([2021](#bib.bib122)) | ResNet50 CNN
    | Tensorflow | Authors created a data set of 500 labelled sonar images from video
    sequences | Each image was annotated by dot annotation | 3 classes of fish according
    to number of fish | MAE | 0.30% | Comparison between DeepFish dataset 0.38% and
    authors’ benchmark result and their model 0.30%. |'
  prefs: []
  type: TYPE_TB
- en: '| Counting Fish and Dolphins in Sonar Images Using Deep Learning Schneider
    and Zhuang ([2020](#bib.bib115)) | CNN | NA | Authors created a data set of 143
    labeled sonar images from the Amazon River | Each image was annotated by counting
    number of fishes | 35 classes for fish and 4 for dolphin | MSE | Fish 2.11% Dolphins
    0.133% | Comparing four Network Architectures, DenseNet201, InceptionNetV2, Xception,
    and MobileNetV2 |'
  prefs: []
  type: TYPE_TB
- en: '| Counting Fish in Sonar Images Liu et al. ([2018](#bib.bib80)) | CNN | NA
    | Authors created a dataset of 537 labelled sonar images from video sequences
    | Each image was annotated by dot annotation | 1 class of fish | RMSE | 16.48%
    | Comparison with other state-of-the-art approaches |'
  prefs: []
  type: TYPE_TB
- en: '| Assessing fish abundance from underwater video using deep neural networks
    Mandal et al. ([2018](#bib.bib84)) | Faster R-CNN | Caffe | Authors created a
    dataset of 4909 labelled images from video sequences | Each image was annotated
    by drawing a bounding box | 50 classes from 50 Different fish habitat. | mAP |
    82.4% | NA |'
  prefs: []
  type: TYPE_TB
- en: Despite this high potential, DL has not been thoroughly investigated for counting
    underwater fish. One possible reason for the lack of comprehensive research for
    fish counting is the scarcity of large publicly available underwater fish datasets.
    In addition, properly annotating fish datasets to train robust DL models is time-prohibitive
    and expensive. Although the underwater fish counting is limited in the literature,
    several previous works have advanced the field in this area. For instance, Tarling
    et al. (Tarling et al., [2021](#bib.bib122)) created a novel dataset of sonar
    video footage of mullet fish labelled manually with point annotations and developed
    a density-based DL model to count fish from sonar images. They counted fish by
    using a regression method (Xue et al., [2016](#bib.bib134)) and achieved a MAE
    of $0.30\%$. Other researchers (Schneider and Zhuang, [2020](#bib.bib115); Liu
    et al., [2018](#bib.bib80)) used sonar images as well because they present substantially
    different visual characteristics compared to natural images. Counting fish in
    sonar images, however, is substantially different from counting fish in underwater
    video surveillance (Mandal et al., [2018](#bib.bib84)). Unlike natural images,
    sonar images present unique visual characteristics and are in lower resolution
    due to the specific imaging forming principle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using DL, a computer can be taught to identify fish in underwater images, thus
    eliminating the subjectivity of humans in counting fish. However, its use for
    fish population and count analysis is dependent on the model performance on a
    set of well-defined performance metrics and parameters, which is in itself a challenge.
    In section [3](#S3 "3 Developing Deep Learning Models ‣ Applications of Deep Learning
    in Fish Habitat Monitoring: A Tutorial and Survey"), we discussed how one can
    train high-performance DL models, how the use of the current DL pipeline (and
    other methodologies) can be improved, and how future DL models can be designed
    for better assessing fish population including their abundance and their location,
    which is the subject of the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object localization is an essential task in CV, where the goal is to locate
    all instances of specified objects (e.g. fish, aquatic plants and coral reef)
    in images. Marine scientists assess the relative abundance of fish species in
    their environments regularly and track population variations. Various CV-based
    fish sample methods in underwater videos have been offered as an alternative to
    this tedious manual assessment. Though, there is no perfect method for automated
    fish localization. This is mostly owing to the difficulties that underwater videos
    bring, such as illumination fluctuations, fish movements, vibrant backgrounds,
    shape deformations, and variety of fish species.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these issues, several research works have been carried out, which
    are listed in Table [4](#S4.T4 "Table 4 ‣ 4.3 Localization ‣ 4 Applications of
    Deep Learning in Underwater Fish Monitoring ‣ Applications of Deep Learning in
    Fish Habitat Monitoring: A Tutorial and Survey"). Saleh et al. (Saleh et al.,
    [2020a](#bib.bib108)) have developed a fully convolutional neural network that
    performs localizing of fish in realistic fish-habitat images with a high accuracy.
    Jalal et al. (Jalal et al., [2020](#bib.bib45)) introduced a hybrid method based
    on motion-based feature extraction that combines optical flow (Beauchemin and
    Barron, [1995](#bib.bib7)) and Gaussian mixture models (Zivkovic and van der Heijden,
    [2006](#bib.bib142)) with the YOLO deep learning technique (Chaudhari et al.,
    [2020](#bib.bib13)) to identify and categorise fish in unconstrained underwater
    videos using temporal information. They achieved fish detection F-scores of $95.47\%$
    and $91.2\%$ on LifeCLEF 2015 benchmark (Joly et al., [2014](#bib.bib47)) and
    their own dataset, respectively. Gaussian mixture is an unsupervised generative
    modelling approach that may be used to learn first and second order statistical
    estimates of input data features (Zivkovic and van der Heijden, [2006](#bib.bib142)).
    Within an overall population, this is used to indicate Normally Distributed subpopulations.
    The weakness of Gaussian mixture is when trained on videos with some fish but
    no pure background, the fish are modelled as background as well, resulting in
    misdetections in subsequent video frames (Salman et al., [2019](#bib.bib112)).
    In order to compensate for the Gaussian mixture’s weakness, optical flow can be
    used to extract features which are solely caused by underwater video motion. The
    pattern of apparent motion of objects, surfaces, and edges in a visual scene generated
    by the relative motion of an observer and a scene is known as optic flow (Beauchemin
    and Barron, [1995](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: Knausgard et al. (Knausgård et al., [2021](#bib.bib55)) also implemented YOLO
    (Chaudhari et al., [2020](#bib.bib13)) for fish localization. To overcome their
    small training samples, they employed transfer learning (explained in the next
    Section). The YOLO technique achieved Mean Average Precision (mAP) of $86.96\%$
    on the Fish4Knowledge dataset (Giordano et al., [2016](#bib.bib29)). YOLO-based
    object detection systems have been also used in several other research to robustly
    localize and count fish (Jalal et al., [2020](#bib.bib45); Xu and Matzner, [2018](#bib.bib133);
    Knausgård et al., [2021](#bib.bib55)). To test how well Yolo could generalise
    to new datasets, (Xu and Matzner, [2018](#bib.bib133)) used it to localize fish
    in underwater video using three very different datasets. The model was trained
    using examples from only two of the datasets and then tested on examples from
    all three datasets. However, the resulting model could not recognise fish in the
    dataset that was not part of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Other CNN models have also been adapted to robustly detect fish under a variety
    of benthic background and illumination conditions. For instance, (Villon et al.,
    [2016](#bib.bib123)) and ([Choi,](#bib.bib16) ) used GoogLeNet (Szegedy et al.,
    [2015a](#bib.bib120)), while (Labao and Naval, [2019a](#bib.bib63)) used an ensemble
    of Region-based Convolutional Neural Networks (Ren et al., [2015](#bib.bib103))
    that are linked in a cascade structure by Long Short-Term Memory networks (Hochreiter
    and Schmidhuber, [1997](#bib.bib35)). In addition, Inception (Szegedy et al.,
    [2015b](#bib.bib121)) and ResNet-50 (He et al., [2015](#bib.bib34)) were examined
    in (Zhuang et al., [2017](#bib.bib141)) for fish detection and recognition based
    on weakly-labelled images. Furthermore, (Han et al., [2020](#bib.bib33)) and (Li
    et al., [2015](#bib.bib75)) used Fast R-CNN (Region-based Convolutional Neural
    Network) (Ren et al., [2015](#bib.bib103)) to detect and count fish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.3 Localization ‣ 4 Applications of Deep Learning
    in Underwater Fish Monitoring ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey") demonstrates that state-of-the-art methods
    (e.g. YOLO and Fast R-CNN) can achieve high accuracy in localization tasks. These
    methods generally train object detectors from a wide variety of training images
    (Felzenszwalb et al., [2010](#bib.bib26); Girshick et al., [2014](#bib.bib30))
    in a fully supervised manner. The drawback is that these models depend on instance-level
    annotations, e.g. tight bounding boxes need to be drawn around fish in training
    datasets. This is time-consuming and labour-intensive and make the use of DL in
    marine research very challenging, if not impossible. In Section [5.3.4](#S5.SS3.SSS4
    "5.3.4 Weakly-Supervised Learning ‣ 5.3 Dataset Limitation ‣ 5 Challenges in underwater
    fish monitoring ‣ Applications of Deep Learning in Fish Habitat Monitoring: A
    Tutorial and Survey") we discuss how this critical issue can be addressed using
    weakly supervised localization of objects, where only binary image-level labels
    showing the existence or absence of an object type are needed for training.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to fish classification, counting, and localization, fish segmentation,
    i.e. detecting the entire body of fish in an image is a critical task in marine
    research and applications. In the next subsection, we discuss how DL can be used
    to perform fish segmentation and how it is useful in marine research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of recent DL research works performing the task of fish localization'
  prefs: []
  type: TYPE_NORMAL
- en: '| Article | DL Model | Framework | Data | Annotation/Pre-processing/Augmentation
    | Classes and Labels | Perf. Metric | Metric Value | Comparisons with other methods
    |'
  prefs: []
  type: TYPE_TB
- en: '| Marine Animal Detection and Recognition with Advanced Deep Learning Models
    (Zhuang et al., [2017](#bib.bib141)) | ResNet-10 CNN | NA | The dataset is made
    of 73 videos from the public datasets Fish4Knowledge | Each image was annotated
    by drawing a bounding box | 1 class of fish | F1 | 0.07% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Fish detection and species classification in underwater environments using
    deep learning with temporal information (Jalal et al., [2020](#bib.bib45)) | Yolo
    - CNN | TensorFlow | The dataset is made of two datasets 93 videos from LifeCLEF
    2015 fish dataset And an authors-created database containing 4418 videos | Each
    image was annotated by drawing a bounding box and species name | 15 classes of
    15 different fish species. | F1 | LCF-15 95.47% UWA 91.2% | Comparison with other
    state-of-the-art approaches |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic fish detection in underwater videos by a deep neural network-based
    hybrid motion learning system (Salman et al., [2019](#bib.bib112)) | ResNet-152
    CNN | TensorFlow | The dataset is made of 110 videos from two public datasets
    Fish4Knowledge and LifeCLEF 2015 fish dataset | Each image was annotated by drawing
    a bounding box | 15 classes of 15 different fish species. | F1 | 87.44% and 80.02%
    respectively | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Temperate fish detection and classification: a deep learning based approach
    (Knausgård et al., [2021](#bib.bib55)) | YoloV3 - CNN | Pytorch | total of 27230
    images catalogued into 23 different species from the public datasets Fish4Knowledge
    | Each image was annotated by drawing a bounding box | 23 classes of 23 different
    fish species. | mAP | 86.96% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Underwater Fish Detection Using Deep Learning for Water Power Applications
    (Xu and Matzner, [2018](#bib.bib133)) | YoloV3 - CNN | Keras - TensorFlow | Authors-created
    database of underwater video sequences for a total of 70000 train/test frame |
    Each image was annotated by drawing a bounding box | 3 classes of fish | mAP |
    54.74% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Coral Reef Fish Detection and Recognition in Underwater Videos by Supervised
    Machine Learning: Comparison Between Deep Learning and HOG+SVM Methods (Villon
    et al., [2016](#bib.bib123)) | GoogLeNet CNN | NA | Authors-created database containing
    13000 fish thumbnails from videos | Each image was annotated by drawing a bounding
    box | 11 classes of 8 different fish species. | F1 | 98 % | Compare HOG+SVM With
    Deep Learning |'
  prefs: []
  type: TYPE_TB
- en: '| Fish identification in underwater video with deep convolutional neural network:
    SNUMedinfo at LifeCLEF fish task 2015 ([Choi,](#bib.bib16) ) | GoogLeNet CNN |
    NA | 20 videos from LifeCLEF 2015 fish dataset | Each image was annotated by drawing
    a bounding box | 15 classes of 15 different fish species. | AP | 81% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Cascaded deep network systems with linked ensemble components for underwater
    fish detection in the wild (Labao and Naval, [2019a](#bib.bib63)) | RNN- LSTM
    | NA | Authors-created database containing 18 underwater video sequences for a
    total of 327 train/test frame | Each image was annotated by drawing a bounding
    box and species name | 1 class of fish | F1 | 67.76% | Comparison with R-CNN Baseline
    |'
  prefs: []
  type: TYPE_TB
- en: '| A realistic fish-habitat dataset to evaluate algorithms for underwater visual
    analysis (Saleh et al., [2020a](#bib.bib108)) | ResNet-50 CNN | Pytorch | Authors-created
    database containing 39,766 images for 20 habitats from remote coastal marine environments
    of tropical Australia and split to sub-dataset for classification, counting, localization,
    and segmentation. | Each image was annotated by point-level and semantic segmentation
    labels | 20 classes of 20 different fish habitat. | MAE | 0.38 | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Marine Organism Detection and Classification from Underwater Vision Based
    on the Deep CNN Method (Han et al., [2020](#bib.bib33)) | VGG16 -RCNN | NA | The
    dataset is obtained from the video provided by the Underwater Robot Picking Contest,
    test set contains 8800 images. | Each image was annotated by drawing a bounding
    box | 3 classes of fish | mAP | 91.2% | NA |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semantic segmentation task is to predict a label from a set of pre-defined
    object classes for each pixel in an image (Shelhamer et al., [2017](#bib.bib116)).
    In the context of marine research, fish segmentation provides a visual representation
    of fish contour, which might be helpful for human expert visual verification or
    to estimate fish size and weight. Table [5](#S4.T5 "Table 5 ‣ 4.4 Segmentation
    ‣ 4 Applications of Deep Learning in Underwater Fish Monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey") lists a number
    of research addressing the task of fish segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Saleh et al. (Saleh et al., [2020a](#bib.bib108)) developed a FCN model that
    performs fish Segmentation in realistic fish-habitat images with a high accuracy.
    Labao et al. (Labao and Naval, [2019b](#bib.bib64)) proposed a DL model that can
    simultaneously localize fish, estimate bounding boxes around them and segment
    them using a unified multi-task CNN in underwater videos. Unlike previous approaches
    (Qian et al., [2016](#bib.bib99); Wang and Kanwar, [2021](#bib.bib129)) that relied
    on motion information to identify fish body, their proposed method predicts fish
    object spatial coordinates and per-pixel segmentation using just video frames
    independent of motion information. Their suggested approach is more resilient
    to camera motions or jitters since it is not dependent on motion information,
    making it more suitable for processing underwater videos captured by Autonomous
    Underwater Vehicles (AUVs). Region Proposal Networks (RPN) (Ren et al., [2017](#bib.bib104))
    have been also used for fish segmentation in underwater videos (Alshdaifat et al.,
    [2020](#bib.bib3)). RPN is a FCN that generates boxes around identified objects
    and gives them confidence scores of belonging to a specific class, simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of recent DL research works performing the task of fish segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Article | DL Model | Framework | Data | Annotation/Pre-processing/Augmentation
    | Classes and Labels | Perf. Metric | Metric Value | Comparisons with other methods
    |'
  prefs: []
  type: TYPE_TB
- en: '| A realistic fish-habitat dataset to evaluate algorithms for underwater visual
    analysis (Saleh et al., [2020a](#bib.bib108)) | ResNet-50 CNN | Pytorch | Authors-created
    database containing 39,766 images from 20 habitats from remote coastal marine
    environments of tropical Australia and split to sub-dataset for classification,
    counting and localization, and segmentation. | Each image was annotated by point-level
    and semantic segmentation labels | 20 classes of 20 Different fish habitat. |
    mIoU | 0.93% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Weakly supervised underwater fish segmentation using affinity LCFCN (Laradji
    et al., [2021b](#bib.bib68)) | ResNet-CNN | Pytorch | Public DeepFish dataset
    (Saleh et al., [2020b](#bib.bib109)) | Each image was annotated by segmentation
    labels | 20 classes of 20 Different fish habitat | mIoU | 0.749% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Simultaneous Localization and Segmentation of Fish Objects Using Multi-task
    CNN and Dense CRF (Labao and Naval, [2019b](#bib.bib64)) | ResNet-CNN | TensorFlow
    | Authors-created dataset containing 1525 images from ten 10 different sites in
    central Philippines | Each image was annotated by drawing a bounding box and segmentation
    labels | 1 class of fish | AP | 93.77% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Segmentation of Underwater Imagery: Dataset and Benchmark (Islam
    et al., [2020](#bib.bib43)) | VGG16 -CNN | Keras - TensorFlow | Authors-created
    dataset containing 1525 images of 8 object categories | Each image was annotated
    by segmentation labels | 8 classes of 8 different object categories. | mIoU |
    84.14% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| DPANet: Dual Pooling-aggregated Attention Network for fish segmentation (Zhang
    et al., [2022](#bib.bib137)) | ResNet-50 CNN | Pytorch, | Two public datasets
    DeepFish (Saleh et al., [2020b](#bib.bib109)) and SUIM (Islam et al., [2020](#bib.bib43))
    | Each image was annotated by segmentation labels | 20 classes: 20 Different fish
    habitat. | mIoU | 91.08%, 85.39% | Comparison with other state-of-the-art approaches
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weakly-Labelled semantic segmentation of fish objects in underwater videos
    using a deep residual network (Labao and Naval, [2017](#bib.bib62)) | ResNet-FCN
    | TensorFlow | Authors-created dataset containing several underwater videos from
    six different sites in Verde Island Passage, Philippines. | Each image was annotated
    with weakly-labelled ground truth derived from a motion-based background subtraction
    (BGS) | 1 class of fish | AP | 65.91% | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Improved deep learning framework for fish segmentation in underwater videos
    (Alshdaifat et al., [2020](#bib.bib3)) | ResNet-CNN | TensorFlow | Two datasets
    extracted from the Fish4Knowledge to produce 2000 frames | Each image was annotated
    by drawing a bounding box and segmentation labels | 15 classes of 15 different
    fish species. | AP | 95.20% | NA |'
  prefs: []
  type: TYPE_TB
- en: Computational efficiency is essential in the autonomy pipeline of visually-guided
    underwater robots. For this reason, (Islam et al., [2020](#bib.bib43)) developed
    SUIM-Net, a fully-convolutional encoder-decoder model that balances the trade-off
    between performance and computational efficiency. On the other hand, for higher
    performance, (Zhang et al., [2022](#bib.bib137)) proposed Dual Pooling-aggregated
    Attention Network (DPANet) to adaptively capture long-range dependencies through
    a computationally friendly manner to enhance feature representation and improve
    not only the segmentation performance, but also its computational resources and
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'All previously discussed models use fully-supervised methods that require a
    large amount of pixel-wise annotations, which is very time-consuming and expensive,
    because a human expert must segment and label, for example, each fish in an image.
    To overcome this serious issue, weakly-supervised semantic segmentation models
    are used. These models do not need to be trained with pixel-wise annotation (Rajchl
    et al., [2016](#bib.bib101)). However, due to a lower level of supervision, training
    weakly-supervised semantic segmentation models is often a more challenging task.
    Applying weakly labelled ground truth derived from motion-based adaptive Mixture
    of Gaussians Background Subtraction, (Labao and Naval, [2017](#bib.bib62)) managed
    to get an average precision of 65.91%, and an average recall of 83.99%. Recently,
    several other weakly-supervised methods have been introduced to overcome the cost
    of a large amount of pixel-wise annotations. These new methods include bounding
    boxes (Khoreva et al., [2017](#bib.bib52); Dai et al., [2015](#bib.bib19)), scribbles
    (Lin et al., [2016](#bib.bib77)), points (Laradji et al., [2021b](#bib.bib68);
    Bearman et al., [2016](#bib.bib6)), and even image-level annotation (Pathak et al.,
    [2015](#bib.bib96); Wang et al., [2018](#bib.bib126); Ahn and Kwak, [2018](#bib.bib2);
    Huang et al., [2018](#bib.bib39); Wei et al., [2018](#bib.bib130)). Since weakly-supervised
    methods are integral to success of important DL-based segmentation tasks, in Section
    [5.3](#S5.SS3 "5.3 Dataset Limitation ‣ 5 Challenges in underwater fish monitoring
    ‣ Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey"),
    we discuss them further.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous subsections, we discussed how DL is useful in a number of key
    applications in fish habitat monitoring. In the following Section, we discuss
    the many challenges on the way of developing DL models for such applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges in underwater fish monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: Underwater fish monitoring presents a series of challenges for DL, which have
    been the focus of many research works. In this section, we first introduce the
    major enviromental challenges faced when developing underwater fish monitoring
    models. We then show that one of the approaches to properly address these enviromental
    challenges is to use DL. However, DL training for fish monitoring has its own
    challenges, which will be discussed in details.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Environmental challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to work in underwater environments, monitoring models must be able
    to recognize objects and scenes in complex, non-trivial backgrounds. This presents
    both a challenge in the development and training of these models and in robustly
    testing them. The main environmental challenges in underwater visual fish monitoring
    can be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The environment is noisy including very large lighting variation. An object
    viewed from a distance is much less bright than a close-up object. These problems
    become more acute when the background is not uniform.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Underwater scenes are highly dynamic, i.e. the scene’s content and objects change
    very quickly. The background can change from being completely occluded to being
    visible and vice versa.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depth and distance perception can be incorrect due to refraction. This is more
    severe for short distances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Images are affected by water turbidity, light scattering, shading, and multiple
    scattering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image data are frequently under-sampled due to low-resolution cameras and
    power constraints underwater.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One of the main approaches used in literature to address these challenges is
    for the monitoring models to use hand-crafted features (Rova et al., [2007](#bib.bib106);
    Hu et al., [2012](#bib.bib37); Fouad et al., [2014](#bib.bib27); Huang et al.,
    [2014](#bib.bib38); Chuang et al., [2016](#bib.bib17); Ogunlana et al., [2015](#bib.bib92);
    Hossain et al., [2016](#bib.bib36); Wang et al., [2017](#bib.bib127); Islam et al.,
    [2019](#bib.bib42)). Hand-crafted features are defined by a human to describe
    a fish image. For example, a low-level feature can be the histogram of a texture
    or a Gabor filter response. As a more complex and representative feature, a mid-level
    feature can be a Scale-Invariant Feature Transform (SIFT) (Lindeberg, [2012](#bib.bib79)),
    or a Histogram of Oriented Gradient (HOG) (Dalal and Triggs, [2005](#bib.bib20)).
    However, human-defined features cannot be applied to other datasets, and the definition
    of a human-defined feature is a time-consuming task, which restricts real-time
    detection and requires manual effort. Moreover, hand-crafted features are limited
    by human experiences, which may contain noise and are difficult to design. For
    example, a SIFT descriptor doesn’t work well with lighting changes and blur.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a fish image is transformed into a feature space that a computer
    can understand. The feature space is often based on a combination of low-level
    image features (for example, colour distribution and gradient), and other features
    in the image such as edges, shapes, and textures. Models using hand-crafted features,
    however, do not perform well under varying environmental conditions, and the feature
    space cannot be easily or robustly created. Additionally, the features created
    are too low-level and cannot be easily used for processing images from different
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative way to build prediction models capable of working in the presence
    of these significant environmental challenges is to use DNNs. However, training
    effective DNNs require resolving some other challenges, which we discuss in the
    below subsections. We also describe some of the approaches in literature addressing
    them. The reviewed approaches in addressing these common challenges can provide
    a quick reference for future researchers developing DL-based fish monitoring models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Model Generalisation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Improving the generalization abilities of DNNs is one of the most difficult
    tasks in DL. Generalization refers to the gap between a model’s performance on
    previously observed data (i.e training data) and data it has never seen before
    (i.e testing data). This is a fundamental problem, with implications for any applications
    using deep neural networks to process image data, videos, etc. This challenge
    is even more pronounced when more difficult tasks such as fish recognition in
    underwater environments.
  prefs: []
  type: TYPE_NORMAL
- en: Generalization problem happens usually because during training the network over-fits
    to the training data. In other words, the weights of the network are adapted to
    produce a response that is best suited for reproducing the training examples.
    During testing, the network produces a response that is a compromise between the
    different training examples. This mismatch is a common cause of poor performance
    on test data, which is often referred to as a network over-fitting to the training
    data, even when the network has been trained for many epochs. The reason it occurs
    is that the network "memorizes" the training data during the training. The training
    data can become quite large, consisting of hundreds of thousands or millions of
    examples. This makes the issue of network over-fitting quite significant. In the
    last few years, there has been significant research efforts toward solving the
    problem of over-fitting to improve model generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Previous works have shown that it is possible to prevent the network from over-fitting
    using techniques called regularisation (Kukačka et al., [2017](#bib.bib60)). There
    are also some theoretical techniques to make the network more robust to training
    data. Below, we provide a brief overview of some of these techniques and how they
    have been applied to solve the problem of deep network over-fitting to training
    data, to improve generalisation in DL.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularisation Term: It is hypothesised that neural networks with fewer weight
    matrices can result in simpler models with the same capability as the complete
    model. A regularisation term is, therefore, added to the model loss function to
    remove some of the weight matrices components. The most popular methods of regularisation
    are L1 and L2. For example, Tarling et al. (Tarling et al., [2021](#bib.bib122))
    showed that incorporating uncertainty regularisation improves performance of their
    multi-task network with ResNet-50 (He et al., [2015](#bib.bib34)) backend to count
    fish in underwater images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch normalisation: Introduced in Section [2.2](#S2.SS2 "2.2 Convolutional
    Neural Network (CNN) ‣ 2 Deep Learning ‣ Applications of Deep Learning in Fish
    Habitat Monitoring: A Tutorial and Survey") as part of the convolutional layer
    in CNNs, batch normalisation was first introduced by Ioffe and Szegedy (Ioffe
    and Szegedy, [2015](#bib.bib40)) to decrease the effect of internal covariate
    shift. Internal covariate shift is the shift in the mean and covariance of inputs
    and network parameters across a batch of examples. Internal covariate shift can
    impede the training of deep neural networks. Batch normalisation is used in almost
    any DL model training, to improve the model generalisation. In the fish monitoring
    domain, for instance, Islam et al. (Islam et al., [2020](#bib.bib43)) proposed
    an optional residual skip block consisting of three convolutional layers with
    batch normalisation and ReLU non-linearity after each convolutional layer to perform
    effective semantic segmentation of underwater imagery.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout: Introduced in Section [2.2](#S2.SS2 "2.2 Convolutional Neural Network
    (CNN) ‣ 2 Deep Learning ‣ Applications of Deep Learning in Fish Habitat Monitoring:
    A Tutorial and Survey") as a common operation in CNNs, dropout reduces the network
    dependency to a small selection of neurons and encourages more useful and robust
    properties and features of the dataset to be learnt. When working with a complex
    neural network structure, dropout is frequently recommended to introduce additional
    randomisation, which helps with the generalisation capability of the network.
    For example, Iqpal et al. (Iqbal et al., [2021](#bib.bib41)) claimed that the
    inclusion of dropout layer has enhanced the overall performance of their proposed
    model for automatic fish classification.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.3 Dataset Limitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preparing training datasets is one of the central and most time-consuming bottlenecks
    in developing DL models, which require a large amount of data, e.g. a variety
    of underwater fish images in different environmental conditions, which should
    also be labelled and analyzed by humans for supervised learning. Due to these
    requirements, making a large dataset is most of the time, very challenging, which
    makes the datasets limited and small. However, When compared with DL models trained
    with a large dataset, the convergence speed and training accuracy of the models
    trained with small datasets are much lower. Generally, increasing the size of
    training datasets by adding more data to them is the classic way to accelerate
    the training and improved accuracy of DL models, but it is expensive. Therefore,
    in recent years, researchers have tackled the dataset limitation challenge by
    devising new ways described below.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data augmentation is a technique to increase the number of labelled examples
    required for DL training. It artificially enlarges the original training dataset
    by introducing various transformations such as translation, rotation, scaling,
    and even noise, to the original data instances, to make new instances. It is particularly
    relevant to the challenge posed when the quantity or quality of labelled data
    is insufficient to train a DL model. At the same time, data augmentation can be
    used to reduce the probability of overfitting and increase model generalisability.
    In contrast to the techniques listed above for improving model generalisation,
    data Augmentation addresses overfitting from the source of the problem (i.e. the
    original dataset). This is done under the notion that augmentations can extract
    additional information from the original dataset by artificially increasing the
    size of the training dataset. It is also critical to consider data augmentation’s
    "safety" (i.e. the possibility of misleading the network post-transformation).
    For example, rotation and horizontal flipping are typically safe data augmentation
    techniques for fish classification tasks (Saleh et al., [2020a](#bib.bib108);
    Sarigül and Avci, [2017](#bib.bib114)) but not safe on digit classification tasks,
    due to the similarities between 6 and 9. A data augmentation technique is to use
    the super-resolution reconstruction method (Ledig et al., [2017](#bib.bib70))
    based on Generative Adversarial Network (GAN) (Goodfellow et al., [2014](#bib.bib31))
    to enlarge the dataset with high-quality images. This has been previously used
    to improve small-scale fine-grained fish classification (Qiu et al., [2018](#bib.bib100)),
    and to increase models predictive performance (i.e. ability to generalise to new
    data) (Konovalov et al., [2019a](#bib.bib56)) for underwater fish detection and
    automatic fish classification (Chen et al., [2018](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: Using augmentation techniques such as cropping, flipping, colour changes, and
    random erasing together can result in enormously inflated dataset sizes. For example,
    Islam et al. (Islam et al., [2020](#bib.bib43)) used rotation, width shift, height
    shift, shear, zoom and horizontal flip for semantic segmentation of underwater
    imagery to significantly increase their dataset size. Another data augmentation
    technique used during training DL models is scale jittering, which has been used
    in (Mandal et al., [2018](#bib.bib84)) for assessing fish abundance in underwater
    videos. Gaussian filtering to blur images and different degrees of rotation for
    fish recognition in underwater-drone with a panoramic camera is another augmentation
    technique used in the marine monitoring domain (Meng et al., [2018](#bib.bib86)).
  prefs: []
  type: TYPE_NORMAL
- en: However, augmentation is not always favourable, as it might lead to large overfitting
    in cases with very few data samples. As a result, it is critical to determine
    the best subset of augmentation techniques to train your DL model using a limited
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Transfer Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transfer Learning is preserving information obtained while solving one problem,
    and transferring the learned knowledge to another similar problem. For instance,
    one may initially train a network on a large object dataset, such as ImageNet
    that includes 1000 different object classes, and then utilise the learned network
    parameters from that training as the initial learning parameters in a new classification
    task, e.g. fish classification. In most cases, just the weights in convolutional
    layers are transferred, rather than the complete network, including fully connected
    layers. This is extremely useful since many image datasets have low-level spatial
    features and properties that are better learnt in massive datasets. For example,
    Zurowietz *et al.* (Zurowietz and Nattkemper, [2020](#bib.bib143)) presented unsupervised
    knowledge transfer to use their limited amount of training data in order to avoid
    time-consuming annotation for object detection in marine environmental monitoring
    and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Hybrid Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL architectures have demonstrated excellent capabilities in capturing semantic
    knowledge that is latent in image features. Handcrafted features, on the other
    hand, can provide specific physical descriptions if they are carefully chosen.
    In addition, attributes of natural images have been demonstrated to be described
    differently by CNN features and hand-crafted features. This means a feature’s
    discriminative ability may behave differently on different datasets. Therefore,
    these two types of features may complement each other for better learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, increasing feature dimensions by fusing hand-crafted and DL-generated
    features can result in increased computational requirement. One way to avoid this
    is to initially utilise DL features for a particular dataset, and later add hybrid
    features to enhance the performance. As a result, when working with difficult
    datasets, such as uncommon and rare marine species, more sophisticated algorithms
    and techniques based on hybrid features may be required. In fact, several research
    groups have used such strategies to improve the performance of marine species
    recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Mahmood et al. (Mahmood et al., [2016](#bib.bib83)) used texture-
    and colour-based hand-crafted features extracted from their CNN training data
    to complement generic CNN-extracted features and achieved a classification accuracy
    higher than when using only generic CNN features when classifying corals. A combination
    of CNN and hand-designed features have also been used in (Cao et al., [2016](#bib.bib12))
    for marine animal classification, again showing that their method achieves higher
    accuracy than applying CNN alone. In another work, Blanchet et al. showed that
    aggregation of multiple features outperforms models using single feature-extraction
    techniques, for automated coral annotation in natural scenes (Blanchet et al.,
    [2016](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Weakly-Supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL methods (LeCun et al., [2015](#bib.bib69)) have consistently achieved state-of-the-art
    results in a variety of applications, specifically in fully supervised learning
    tasks like classification and regression (Li et al., [2009](#bib.bib74); Lin et al.,
    [2014](#bib.bib78)). Fully supervised learning methods create predictive algorithms
    by learning from a vast amount of training patterns, where each pattern has a
    label showing its ground-truth output (Kotsiantis, [2007](#bib.bib59)). Although
    the current fully supervised methods have been very successful in certain activities
    (De Vos et al., [2017](#bib.bib21); Wörz and Rohr, [2006](#bib.bib131); Mader
    et al., [2018](#bib.bib82)), they come with a caveat of requiring a large portion
    of the data to be labelled, and it is sometimes difficult or extremely time consuming
    to obtain ground-truth labels for the dataset. Thus, it is desirable to develop
    learning algorithms that are able to work with less labelled data (i.e. weakly
    supervised) (Zhou, [2018](#bib.bib140); Oquab et al., [2015](#bib.bib94)).
  prefs: []
  type: TYPE_NORMAL
- en: Weak supervision in particular can be very useful in underwater fish monitoring,
    where the limited dataset size and the time- and cost-prohibitive nature of labelling
    limits achieving a useful dataset for developing effective, smart, and automated
    habitat monitoring tools and techniques. A number of works in literature have
    already used weak supervision for underwater fish habitat monitoring. For example,
    Laradji et al. (Laradji et al., [2020](#bib.bib67)) proposed a segmentation model
    that can efficiently train on underwater fish images, not manually segmented for
    training, but only labeled with simple point-level supervision. This work demonstrated
    that in the marine monitoring context, weakly-supervised learning can effectively
    improve the accuracy and speed of model development with limited dataset sizes
    and limited labelling budget.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e4893ef8fef581cdd37e67fd525d71c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Schematic diagram of Active Learning'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.5 Active Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Active learning is a sub-field of ML and, more broadly, of AI. In active learning,
    the proposed algorithm is allowed to be "inquisitive", that is, it is allowed
    to pick the data to learn, which in theory means the algorithm can do more with
    less guidance, similar to weak supervision. Active learning systems are seeking
    to solve the constraint of labelling by posing a questionnaire in the context
    of unlabeled examples to be labelled by an oracle (e.g. a human annotator). In
    this manner, the goal of the active learner is to attain high precision by using
    as few labelled examples as possible, thus minimising the expense of acquiring
    labelled data; see Figure [6](#S5.F6 "Figure 6 ‣ 5.3.4 Weakly-Supervised Learning
    ‣ 5.3 Dataset Limitation ‣ 5 Challenges in underwater fish monitoring ‣ Applications
    of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the labels come for little or no cost, like the "spam" label
    that is used to mark spam emails, or the five-star rating that a user could post
    for a movie on a social networking platform. Learning methods use these labels
    and scores to help screen your spam email and recommend movies that you might
    enjoy. In these cases, certain labels are given free of charge, but for more sophisticated
    supervised learning tasks, such as when you need to segment a fish in an underwater
    environment, this is not the case. For example, in (Nilssen et al., [2017](#bib.bib91))
    active learning has been used for the classification of species in underwater
    images from a fixed observatory. The authors proposed an active learning method
    that assigns taxonomic categories to single patches based on a set of human expert
    annotations, making use of cluster structures and relevance scores. This active
    learning method, compared to traditional sampling strategies, used significantly
    fewer manual labels to train a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f7b4ca6b5fdac3a072a75d898e17024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Schematic diagram of knowledge distillation'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Opportunities in applications of DL to underwater fish monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New methodologies and strategies should be developed to advance DL models for
    various underwater visual monitoring applications, including fish monitoring,
    and to bring them closer to their terrestrial monitoring equivalents. In a previous
    study that was focused on the task of fish classification (Saleh et al., [2022](#bib.bib111)),
    we have discussed some of the future research opportunities including (i) utilizing
    spatio-temporal data to add space and time domain information to the current training
    algorithms that mainly learn fish images regardless of their spatial and/or temporal
    correlation; (ii) Developing efficient and compact DL models that can be deployed
    underwater for real-time parsing of the fish images at the collection edge; (iii)
    Combining image data from multiple collection platforms for improved multi-faceted
    learning; and (iv) Automated fish measurement and monitoring from underwater captured
    images. Below, we expand on some of the previously discussed opportunities in
    (Saleh et al., [2022](#bib.bib111)) and explore a few other prospective research
    areas for increasing the performance and usability of visual fish monitoring tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Knowledge Distillation for Underwater Embedded and Edge Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL models used for fish monitoring applications are usually very large containing
    millions of parameters and requiring extensive computational power. To deploy
    these models on resource-limited devices and in resource-constrained environments
    such as undersea monitoring sites, different hardware-emabled compression techniques
    such as quantizing and binarizing DNN parameters (Lammie et al., [2019](#bib.bib65))
    can be used, as discussed in (Saleh et al., [2022](#bib.bib111)). Another method
    that has seen a lot of interest and attention for compressing large-scale DL models
    is knowledge distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowledge distillation is a technique for training a student (i.e. a small
    network) to emulate a teacher (i.e. ensemble of networks), as shown in Figure
    [7](#S5.F7 "Figure 7 ‣ 5.3.5 Active Learning ‣ 5.3 Dataset Limitation ‣ 5 Challenges
    in underwater fish monitoring ‣ Applications of Deep Learning in Fish Habitat
    Monitoring: A Tutorial and Survey"). The primary assumption is that in order to
    achieve a competitive or even superior performance, the student model should imitate
    the teacher model. The main issue is, however, transferring the knowledge from
    a large teacher to a smaller student. To that end, Bucilua et al. (Bucilǎ et al.,
    [2006](#bib.bib11)) proposed model compression as a way to transfer knowledge
    from a large model into a small model without sacrificing accuracy. In addition,
    several other model compression approaches have been developed, and the community
    has shown an increasing interest in knowledge distillation, due to its potentials
    (Amadori, [2019](#bib.bib4); Wang et al., [2020](#bib.bib128); Rassadin and Savchenko,
    [2017](#bib.bib102); Kushawaha et al., [2021](#bib.bib61)).'
  prefs: []
  type: TYPE_NORMAL
- en: A significant research opportunity lies in applying Knowledge distillation into
    embedded devices and underwater video processors to achieve online and more effective
    surveillance with high accuracy while using limited resources. This is particularly
    useful because of the limitations of transferring data from underwater sensors
    and cameras, and due to the challenging underwater communication in the Internet
    of Underwater Things  (Jahanbakht et al., [2021](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Merging Image Data from Multiple Sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in (Saleh et al., [2022](#bib.bib111)), to train more effective
    DNNs, multiple data collection platforms like Autonomous Underwater Vehicles (AUVs)
    or inhabited submarines can give varied visual data from the same monitoring subject.
    This can provide additional monitoring information, such as fish distribution
    patterns. Although it is straightforward to combine multiple data sources for
    training a DL network, several issues should be addressed in future research.
    These include possible preprocessing on part of data to make it compatible with
    the rest of the training dataset, class-wise weights (i.e. when you have an imbalanced
    dataset), and the number of outputs of a network. In addition, multiple training
    data sources, in particular, when using AUVs or submarines, incurs significant
    data collection and manual labelling cost, which is not always viable.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, some researchers have focused on learning from data with the
    least amount of human-labeling. To reduce human-labelled data cost, several methods
    have been proposed to train models on data that are unlabeled (Shimada et al.,
    [2021](#bib.bib117)) or only have pseudo-labels (Wu and Prasad, [2018](#bib.bib132)).
    Future research can advance this further by developing faster and cheaper annotating
    tools for underwater fish images.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Automatic Fish Phenotyping From Underwater Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic fish phenotyping, i.e. extracting their weight, size, and length,
    in their natural habitats can provide invaluable information in better understanding
    marine echosystems and fish ecology (Goodwin et al., [2022](#bib.bib32)). Although
    many studies have addressed fish monitoring in aquaculture and fish farm settings
    (Li and Du, [2021](#bib.bib73); Zhao et al., [2021](#bib.bib138)), monitoring
    fish for measurement in natural habitats remain mostly unexplored, and can be
    investigated in future research. These research should address problems such as
    low visibility and light, fish occlusion and overlap, which are shared with aquculture
    monitoring. However, other problems unique to natural habitats such as cluttered
    background environments and underwater distance measurement should be addressed
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Visual Monitoring of Fish Behavior and Movements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although some telemetry and satellite tracking devices can be used in limited
    settings (Lennox et al., [2017](#bib.bib72)), fish monitoring in their natural
    habitats over a period of time is not achievable using these techniques mainly
    due to the hostile underwater signal communication medium (Jahanbakht et al.,
    [2021](#bib.bib44)). For instance for tracking fish movements, schooling, and
    behavior, new visual monitoring techniques should be devised. A possible direction
    for future studies is to devise better understanding of fish vision characteristics
    (Boudhane and Nsiri, [2016](#bib.bib10)) and their implications in the current
    and next generation of automated DL-based tracking systems (Li et al., [2020](#bib.bib76))
    and marine object detection (Moniruzzaman et al., [2017](#bib.bib88)). An example
    of an alternative tracking method is presented in (Zhao et al., [2019](#bib.bib139)),
    where the image-based identification and tracking method for fish is designed
    based on biological water quality monitoring. To improve the fish tracking task,
    some techniques can also be combined with visual image enhancement algorithms.
    For instance, when the image enhancement methods are used, the underwater images
    can be corrected for distortion and noise, and the fish tracking task can be easily
    performed. In (Saberioon and Cisar, [2016](#bib.bib107)), the authors studied
    the potential of underwater fish monitoring by using visual and underwater sensing
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenging research area is developing novel underwater fish tracking
    algorithms, using DL or other technologies, with low power consumption and real-time
    speed. For this, various hardware technologies and techniques used in other domains
    such as biomedical applications  (Azghadi et al., [2020](#bib.bib5)) can be explored.
    Of course, any automated vision-based tracking system should be validated through
    real-world trials, which is a significant undertaking requiring many resources,
    in order to ensure the accurate and real-time tracking of fish.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Summary and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this article was to provide researchers and practitioners a summary
    of the contemporary applications of DL in underwater visual monitoring of fish,
    as well as to make it easier to apply DL to tackle real challenges in fish-related
    marine science.
  prefs: []
  type: TYPE_NORMAL
- en: DL has progressed as a technology capable of providing unprecedented benefits
    to various aspects of marine research and fish habitat monitoring. We envision
    a future where DL, complemented by many other advances in monitoring hardware
    and underwater communication technologies (Jahanbakht et al., [2021](#bib.bib44)),
    is widely used in marine habitat monitoring for (1) data collection and feature
    extraction to improve the quality of automatic monitoring tools; and (2) to provide
    a reliable means of surveying fish habitats and understanding their dynamics.
    We expect that such a future will allow marine ecosystem researchers and practitioners
    to increase the efficiency of their monitoring efforts. To achieve this, we need
    concentrated and coordinated data collection, model development, and model deployment
    efforts. We also need transparent and reproducible research data and tools, which
    help us reach our target sooner.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is supported by an Australian Research Training Program (RTP)
    Scholarship and Food Agility HDR Top-Up Scholarship. We also acknowledge the Australian
    Research Council for funding awarded under their Industrial Transformation Research
    Program.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abdul et al. (2019) Abdul, M.S., Sam, S.M., Mohamed, N., Kamardin, K., Dziyauddin,
    R.A., 2019. Docker Containers Usage in the Internet of Things: A Survey. Open
    International Journal of Informatics (OIJI) 7, 208–220. URL: [http://apps.razak.utm.my/ojs/index.php/oiji/article/view/233](http://apps.razak.utm.my/ojs/index.php/oiji/article/view/233).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn and Kwak (2018) Ahn, J., Kwak, S., 2018. Learning Pixel-Level Semantic
    Affinity with Image-Level Supervision for Weakly Supervised Semantic Segmentation,
    in: Proceedings of the IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition. doi:[10.1109/CVPR.2018.00523](https:/doi.org/10.1109/CVPR.2018.00523).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alshdaifat et al. (2020) Alshdaifat, N.F.F., Talib, A.Z., Osman, M.A., 2020.
    Improved deep learning framework for fish segmentation in underwater videos. Ecological
    Informatics 59, 101121. doi:[10.1016/j.ecoinf.2020.101121](https:/doi.org/10.1016/j.ecoinf.2020.101121).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amadori (2019) Amadori, A., 2019. Distilling knowledge from Neural Networks
    to build smaller and faster models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azghadi et al. (2020) Azghadi, M.R., Lammie, C., Eshraghian, J.K., Payvand,
    M., Donati, E., Linares-Barranco, B., Indiveri, G., 2020. Hardware Implementation
    of Deep Network Accelerators towards Healthcare and Biomedical Applications. IEEE
    Transactions on Biomedical Circuits and Systems 14, 1138–1159. doi:[10.1109/TBCAS.2020.3036081](https:/doi.org/10.1109/TBCAS.2020.3036081).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bearman et al. (2016) Bearman, A.L., Russakovsky, O., Ferrari, V., Fei-Fei,
    L., Bearman, A.L., Ferrari, V., Li, F.F., Russakovsky, O., Ferrari, V., Fei-Fei,
    L., 2016. What’s the point: Semantic segmentation with point supervision. ECCV
    abs/1506.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beauchemin and Barron (1995) Beauchemin, S.S., Barron, J.L., 1995. The Computation
    of Optical Flow. ACM Computing Surveys (CSUR) 27, 433–466. URL: [https://dl.acm.org/doi/abs/10.1145/212094.212141](https://dl.acm.org/doi/abs/10.1145/212094.212141),
    doi:[10.1145/212094.212141](https:/doi.org/10.1145/212094.212141).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisong and Bisong (2019) Bisong, E., Bisong, E., 2019. Regularization for Deep
    Learning, in: Building Machine Learning and Deep Learning Models on Google Cloud
    Platform. doi:[10.1007/978-1-4842-4470-8_34](https:/doi.org/10.1007/978-1-4842-4470-8_34).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blanchet et al. (2016) Blanchet, J.N., Déry, S., Landry, J.A., Osborne, K.,
    2016. Automated annotation of corals in natural scene images using multiple texture
    representations. PeerJ doi:[10.7287/peerj.preprints.2026](https:/doi.org/10.7287/peerj.preprints.2026).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boudhane and Nsiri (2016) Boudhane, M., Nsiri, B., 2016. Underwater image processing
    method for fish localization and detection in submarine environment. Journal of
    Visual Communication and Image Representation 39, 226–238. URL: [https://linkinghub.elsevier.com/retrieve/pii/S1047320316300840](https://linkinghub.elsevier.com/retrieve/pii/S1047320316300840),
    doi:[10.1016/j.jvcir.2016.05.017](https:/doi.org/10.1016/j.jvcir.2016.05.017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bucilǎ et al. (2006) Bucilǎ, C., Caruana, R., Niculescu-Mizil, A., 2006. Model
    compression, in: Proceedings of the ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining. doi:[10.1145/1150402.1150464](https:/doi.org/10.1145/1150402.1150464).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2016) Cao, Z., Principe, J.C., Ouyang, B., Dalgleish, F., Vuorenkoski,
    A., 2016. Marine animal classification using combined CNN and hand-designed image
    features, in: OCEANS 2015 - MTS/IEEE Washington. doi:[10.23919/oceans.2015.7404375](https:/doi.org/10.23919/oceans.2015.7404375).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhari et al. (2020) Chaudhari, S., Malkan, N., Momin, A., Bonde, M., 2020.
    Yolo Real Time Object Detection. International Journal of Computer Trends and
    Technology doi:[10.14445/22312803/ijctt-v68i6p112](https:/doi.org/10.14445/22312803/ijctt-v68i6p112).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Chen, G., Sun, P., Shang, Y., 2018. Automatic fish classification
    system using deep learning, in: Proceedings - International Conference on Tools
    with Artificial Intelligence, ICTAI. doi:[10.1109/ICTAI.2017.00016](https:/doi.org/10.1109/ICTAI.2017.00016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019) Chen, L., Xia, Y., Pan, D., Wang, C., 2019. Deep learning
    based active monitoring for anti-collision between vessels and bridges, in: IABSE
    Symposium, Guimaraes 2019: Towards a Resilient Built Environment Risk and Asset
    Management - Report. doi:[10.2749/guimaraes.2019.0487](https:/doi.org/10.2749/guimaraes.2019.0487).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(16) Choi, S., . Fish identification in underwater video with deep convolutional
    neural network: SNUMedinfo at LifeCLEF fish task 2015. Technical Report. other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chuang et al. (2016) Chuang, M.C., Hwang, J.N., Williams, K., 2016. A feature
    learning and object recognition framework for underwater fish images. IEEE Trans.
    Image Process. 25, 1862–1872. doi:[10.1109/TIP.2016.2535342](https:/doi.org/10.1109/TIP.2016.2535342).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuang et al. (2011) Chuang, M.C., Hwang, J.N., Williams, K., Towler, R., 2011.
    Automatic fish segmentation via double local thresholding for trawl-based underwater
    camera systems, in: Proceedings - International Conference on Image Processing,
    ICIP, pp. 3145–3148. doi:[10.1109/ICIP.2011.6116334](https:/doi.org/10.1109/ICIP.2011.6116334).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2015) Dai, J., He, K., Sun, J., 2015. Boxsup: Exploiting bounding
    boxes to supervise convolutional networks for semantic segmentation, in: ICCV,
    pp. 1635–1643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dalal and Triggs (2005) Dalal, N., Triggs, B., 2005. Histograms of oriented
    gradients for human detection, in: Proceedings - 2005 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, CVPR 2005, pp. 886–893. doi:[10.1109/CVPR.2005.177](https:/doi.org/10.1109/CVPR.2005.177).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Vos et al. (2017) De Vos, B.D., Wolterink, J.M., De Jong, P.A., Leiner, T.,
    Viergever, M.A., Isgum, I., 2017. ConvNet-Based Localization of Anatomical Structures
    in 3-D Medical Images. IEEE Transactions on Medical Imaging doi:[10.1109/TMI.2017.2673121](https:/doi.org/10.1109/TMI.2017.2673121).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ditria et al. (2021) Ditria, E.M., Connolly, R.M., Jinks, E.L., Lopez-Marcano,
    S., 2021. Annotated Video Footage for Automated Identification and Counting of
    Fish in Unconstrained Seagrass Habitats. Frontiers in Marine Science 8. URL: [https://www.frontiersin.org/articles/10.3389/fmars.2021.629485/full](https://www.frontiersin.org/articles/10.3389/fmars.2021.629485/full),
    doi:[10.3389/fmars.2021.629485](https:/doi.org/10.3389/fmars.2021.629485).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ditria et al. (2020a) Ditria, E.M., Lopez-Marcano, S., Sievers, M., Jinks,
    E.L., Brown, C.J., Connolly, R.M., 2020a. Automating the Analysis of Fish Abundance
    Using Object Detection: Optimizing Animal Ecology With Deep Learning. Frontiers
    in Marine Science 7. URL: [https://www.frontiersin.org/article/10.3389/fmars.2020.00429/full](https://www.frontiersin.org/article/10.3389/fmars.2020.00429/full),
    doi:[10.3389/fmars.2020.00429](https:/doi.org/10.3389/fmars.2020.00429).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ditria et al. (2020b) Ditria, E.M., Sievers, M., Lopez-Marcano, S., Jinks,
    E.L., Connolly, R.M., 2020b. Deep learning for automated analysis of fish abundance:
    the benefits of training across multiple habitats. Environmental Monitoring and
    Assessment doi:[10.1007/s10661-020-08653-z](https:/doi.org/10.1007/s10661-020-08653-z).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan and Deng (2019) Duan, Z., Deng, J., 2019. Automatic video tracking of
    chinese mitten crab using particle filter based on multi features, in: 2019 IEEE
    3rd International Conference on Electronic Information Technology and Computer
    Engineering, EITCE 2019. doi:[10.1109/EITCE47263.2019.9095032](https:/doi.org/10.1109/EITCE47263.2019.9095032).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Felzenszwalb et al. (2010) Felzenszwalb, P.F., Girshick, R.B., McAllester, D.,
    Ramanan, D., 2010. Object detection with discriminatively trained part-based models.
    IEEE Transactions on Pattern Analysis and Machine Intelligence doi:[10.1109/TPAMI.2009.167](https:/doi.org/10.1109/TPAMI.2009.167).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fouad et al. (2014) Fouad, M.M.M., Zawbaa, H.M., El-Bendary, N., Hassanien,
    A.E., 2014. Automatic Nile Tilapia fish classification approach using machine
    learning techniques, in: 13th International Conference on Hybrid Intelligent Systems,
    HIS 2013, pp. 173–178. doi:[10.1109/HIS.2013.6920477](https:/doi.org/10.1109/HIS.2013.6920477).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia et al. (2016) Garcia, J.A., Masip, D., Sbragaglia, V., Aguzzi, J., 2016.
    Automated identification and tracking of nephrops norvegicus (L.) using infrared
    and monochromatic blue light, in: Frontiers in Artificial Intelligence and Applications.
    doi:[10.3233/978-1-61499-696-5-9](https:/doi.org/10.3233/978-1-61499-696-5-9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giordano et al. (2016) Giordano, D., Palazzo, S., Spampinato, C., 2016. Fish4Knowledge:
    Collecting and Analyzing Massive Coral Reef Fish Video Data. Intelligent Systems
    Reference Library .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick et al. (2014) Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation,
    in: Proceedings of the IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition. doi:[10.1109/CVPR.2014.81](https:/doi.org/10.1109/CVPR.2014.81).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y., 2014. Generative
    Adversarial Networks. ArXiv abs/1406.2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodwin et al. (2022) Goodwin, M., Halvorsen, K.T., Jiao, L., Knausgård, K.M.,
    Martin, A.H., Moyano, M., Oomen, R.A., Rasmussen, J.H., Sørdalen, T.K., Thorbjørnsen,
    S.H., 2022. Unlocking the potential of deep learning for marine ecology: overview,
    applications, and outlook. ICES Journal of Marine Science 79, 319–336. URL: [https://arxiv.org/abs/2109.14737v1](https://arxiv.org/abs/2109.14737v1),
    doi:[10.1093/icesjms/fsab255](https:/doi.org/10.1093/icesjms/fsab255).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2020) Han, F., Yao, J., Zhu, H., Wang, C., 2020. Marine Organism
    Detection and Classification from Underwater Vision Based on the Deep CNN Method.
    Mathematical Problems in Engineering doi:[10.1155/2020/3937580](https:/doi.org/10.1155/2020/3937580).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2015) He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep Residual Learning
    for Image Recognition. Computer Vision and Pattern Recognition (CVPR) .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S., Schmidhuber, J., 1997. Long
    Short-Term Memory. Neural Computation 9, 1735–1780. doi:[https://doi.org/10.1162/neco.1997.9.8.1735](https:/doi.org/https://doi.org/10.1162/neco.1997.9.8.1735).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hossain et al. (2016) Hossain, E., Alam, S.M., Ali, A.A., Amin, M.A., 2016.
    Fish activity tracking and species identification in underwater video, in: 2016
    5th International Conference on Informatics, Electronics and Vision, ICIEV 2016,
    pp. 62–66. doi:[10.1109/ICIEV.2016.7760189](https:/doi.org/10.1109/ICIEV.2016.7760189).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2012) Hu, Y., Mian, A.S., Owens, R., 2012. Face Recognition Using
    Sparse Approximated Nearest Points between Image Sets. IEEE Transactions on Pattern
    Analysis and Machine Intelligence 34, 1992–2004. doi:[10.1109/TPAMI.2011.283](https:/doi.org/10.1109/TPAMI.2011.283).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2014) Huang, P.X., Boom, B.J., Fisher, R.B., 2014. GMM improves
    the reject option in hierarchical classification for fish recognition, in: IEEE
    Winter Conference on Applications of Computer Vision, IEEE. pp. 371–376. URL:
    [https://ieeexplore.ieee.org/document/6836076](https://ieeexplore.ieee.org/document/6836076),
    doi:[10.1109/WACV.2014.6836076](https:/doi.org/10.1109/WACV.2014.6836076).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Huang, Z., XinggangWang, J., Liu, W., Wang, J., 2018. Weakly-supervised
    semantic segmentation network with deep seeded region growing, pp. 7014–7023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy (2015) Ioffe, S., Szegedy, C., 2015. Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift, in: International
    Conference on Machine Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iqbal et al. (2021) Iqbal, M.A., Wang, Z., Ali, Z.A., Riaz, S., 2021. Automatic
    Fish Species Classification Using Deep Convolutional Neural Networks. Wireless
    Personal Communications doi:[10.1007/s11277-019-06634-1](https:/doi.org/10.1007/s11277-019-06634-1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam et al. (2019) Islam, M.A., Howlader, M.R., Habiba, U., Faisal, R.H.,
    Rahman, M.M., 2019. Indigenous Fish Classification of Bangladesh using Hybrid
    Features with SVM Classifier, in: 5th International Conference on Computer, Communication,
    Chemical, Materials and Electronic Engineering, IC4ME2 2019. doi:[10.1109/IC4ME247184.2019.9036679](https:/doi.org/10.1109/IC4ME247184.2019.9036679).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam et al. (2020) Islam, M.J., Edge, C., Xiao, Y., Luo, P., Mehtaz, M., Morse,
    C., Enan, S.S., Sattar, J., 2020. Semantic Segmentation of Underwater Imagery:
    Dataset and Benchmark. http://arxiv.org/abs/2004.01241 URL: [http://arxiv.org/abs/2004.01241](http://arxiv.org/abs/2004.01241).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jahanbakht et al. (2021) Jahanbakht, M., Xiang, W., Hanzo, L., Azghadi, M.R.,
    2021. Internet of Underwater Things and Big Marine Data Analytics - A Comprehensive
    Survey. IEEE Communications Surveys and Tutorials 23, 904–956. URL: [https://ieeexplore.ieee.org/document/9328873/](https://ieeexplore.ieee.org/document/9328873/),
    doi:[10.1109/COMST.2021.3053118](https:/doi.org/10.1109/COMST.2021.3053118).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jalal et al. (2020) Jalal, A., Salman, A., Mian, A., Shortis, M., Shafait, F.,
    2020. Fish detection and species classification in underwater environments using
    deep learning with temporal information. Ecological Informatics 57, 101088. doi:[10.1016/j.ecoinf.2020.101088](https:/doi.org/10.1016/j.ecoinf.2020.101088).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jing et al. (2020) Jing, L., Chen, Y., Tian, Y., 2020. Coarse-to-Fine Semantic
    Segmentation from Image-Level Labels. IEEE Transactions on Image Processing doi:[10.1109/TIP.2019.2926748](https:/doi.org/10.1109/TIP.2019.2926748).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joly et al. (2014) Joly, A., Goëau, H., Glotin, H., Spampinato, C., Bonnet,
    P., Vellinga, W.P., Planque, R., Rauber, A., Fisher, R., Müller, H., 2014. LifeCLEF
    2014: Multimedia Life Species Identification Challenges, in: Kanoulas, E., Lupu,
    M., Clough, P., Sanderson, M., Hall, M., Hanbury, A., Toms, E. (Eds.), Information
    Access Evaluation. Multilinguality, Multimodality, and Interaction, Springer International
    Publishing, Cham. pp. 229–249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juliani and Juliani (2021) Juliani, C., Juliani, E., 2021. Deep learning of
    terrain morphology and pattern discovery via network-based representational similarity
    analysis for deep-sea mineral exploration. Ore Geology Reviews doi:[10.1016/j.oregeorev.2020.103936](https:/doi.org/10.1016/j.oregeorev.2020.103936).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2018) Kang, D., Ma, Z., Chan, A.B., 2018. Beyond counting: comparisons
    of density maps for crowd analysis tasks-counting, detection, and tracking. IEEE
    Transactions on Circuits and Systems for Video Technology .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan et al. (2020) Khan, A., Sohail, A., Zahoora, U., Qureshi, A.S., 2020. A
    survey of the recent architectures of deep convolutional neural networks. Artificial
    Intelligence Review 53, 5455–5516. doi:[10.1007/s10462-020-09825-6](https:/doi.org/10.1007/s10462-020-09825-6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khazukov et al. (2020) Khazukov, K., Shepelev, V., Karpeta, T., Shabiev, S.,
    Slobodin, I., Charbadze, I., Alferova, I., 2020. Real-time monitoring of traffic
    parameters. Journal of Big Data 7. doi:[10.1186/s40537-020-00358-x](https:/doi.org/10.1186/s40537-020-00358-x).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khoreva et al. (2017) Khoreva, A., Benenson, R., Hosang, J.H., Hein, M., Schiele,
    B., 2017. Simple Does It: Weakly Supervised Instance and Semantic Segmentation.
    CVPR , 876–885.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2016) Kim, S., Park, B., Song, B.S., Yang, S., 2016. Deep belief
    network based statistical feature learning for fingerprint liveness detection.
    Pattern Recognition Letters 77, 58–65. URL: [https://linkinghub.elsevier.com/retrieve/pii/S0167865516300198](https://linkinghub.elsevier.com/retrieve/pii/S0167865516300198),
    doi:[10.1016/j.patrec.2016.03.015](https:/doi.org/10.1016/j.patrec.2016.03.015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2014) Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic
    optimization. arXiv preprint arXiv:1412.6980 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knausgård et al. (2021) Knausgård, K.M., Wiklund, A., Sørdalen, T.K., Halvorsen,
    K.T., Kleiven, A.R., Jiao, L., Goodwin, M., 2021. Temperate fish detection and
    classification: a deep learning based approach. Applied Intelligence doi:[10.1007/s10489-020-02154-9](https:/doi.org/10.1007/s10489-020-02154-9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Konovalov et al. (2019a) Konovalov, D.A., Saleh, A., Bradley, M., Sankupellay,
    M., Marini, S., Sheaves, M., 2019a. Underwater Fish Detection with Weak Multi-Domain
    Supervision, in: 2019 International Joint Conference on Neural Networks (IJCNN),
    IEEE. pp. 1–8. URL: [https://ieeexplore.ieee.org/document/8851907/](https://ieeexplore.ieee.org/document/8851907/),
    doi:[10.1109/IJCNN.2019.8851907](https:/doi.org/10.1109/IJCNN.2019.8851907).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konovalov et al. (2018) Konovalov, D.A., Saleh, A., Domingos, J.A., White, R.D.,
    Jerry, D.R., 2018. Estimating Mass of Harvested Asian Seabass Lates calcarifer
    from Images. World Journal of Engineering and Technology 6, 15. doi:[10.4236/wjet.2018.63b003](https:/doi.org/10.4236/wjet.2018.63b003).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Konovalov et al. (2019b) Konovalov, D.A., Saleh, A., Efremova, D.B., Domingos,
    J.A., Jerry, D.R., 2019b. Automatic weight estimation of harvested fish from images,
    in: Digital Image Computing: Techniques and Applications (DICTA), pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kotsiantis (2007) Kotsiantis, S.B., 2007. Supervised machine learning: A review
    of classification techniques. doi:[10.31449/inf.v31i3.148](https:/doi.org/10.31449/inf.v31i3.148).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kukačka et al. (2017) Kukačka, J., Golkov, V., Cremers, D., 2017. Regularization
    for deep learning: A taxonomy. arXiv preprint arXiv:1710.10686 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kushawaha et al. (2021) Kushawaha, R.K., Kumar, S., Banerjee, B., Velmurugan,
    R., 2021. Distilling Spikes: Knowledge Distillation in Spiking Neural Networks.
    doi:[10.1109/icpr48806.2021.9412147](https:/doi.org/10.1109/icpr48806.2021.9412147).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labao and Naval (2017) Labao, A.B., Naval, P.C., 2017. Weakly-Labelled semantic
    segmentation of fish objects in underwater videos using a deep residual network,
    in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
    Intelligence and Lecture Notes in Bioinformatics). doi:[10.1007/978-3-319-54430-4_25](https:/doi.org/10.1007/978-3-319-54430-4_25).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labao and Naval (2019a) Labao, A.B., Naval, P.C., 2019a. Cascaded deep network
    systems with linked ensemble components for underwater fish detection in the wild.
    Ecological Informatics doi:[10.1016/j.ecoinf.2019.05.004](https:/doi.org/10.1016/j.ecoinf.2019.05.004).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labao and Naval (2019b) Labao, A.B., Naval, P.C., 2019b. Simultaneous Localization
    and Segmentation of Fish Objects Using Multi-task CNN and Dense CRF, in: Lecture
    Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence
    and Lecture Notes in Bioinformatics). doi:[10.1007/978-3-030-14799-0_52](https:/doi.org/10.1007/978-3-030-14799-0_52).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lammie et al. (2019) Lammie, C., Olsen, A., Carrick, T., Rahimi Azghadi, M.,
    2019. Low-power and high-speed deep FPGA inference engines for weed classification
    at the edge. IEEE Access doi:[10.1109/ACCESS.2019.2911709](https:/doi.org/10.1109/ACCESS.2019.2911709).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laradji et al. (2021a) Laradji, I., Rodriguez, P., Manas, O., Lensink, K.,
    Law, M., Kurzman, L., Parker, W., Vazquez, D., Nowrouzezahrai, D., 2021a. A Weakly
    Supervised Consistency-based Learning Method for COVID-19 Segmentation in CT Images,
    in: WACV.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laradji et al. (2020) Laradji, I., Saleh, A., Rodriguez, P., Nowrouzezahrai,
    D., Azghadi, M.R., Vazquez, D., 2020. Affinity LCFCN: Learning to Segment Fish
    with Weak Supervision URL: [http://arxiv.org/abs/2011.03149](http://arxiv.org/abs/2011.03149).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laradji et al. (2021b) Laradji, I.H., Saleh, A., Rodriguez, P., Nowrouzezahrai,
    D., Azghadi, M.R., Vazquez, D., 2021b. Weakly supervised underwater fish segmentation
    using affinity LCFCN. Scientific reports 11, 17379. URL: [https://www.nature.com/articles/s41598-021-96610-2http://www.ncbi.nlm.nih.gov/pubmed/34462458http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8405733](https://www.nature.com/articles/s41598-021-96610-2http://www.ncbi.nlm.nih.gov/pubmed/34462458http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8405733),
    doi:[10.1038/s41598-021-96610-2](https:/doi.org/10.1038/s41598-021-96610-2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning.
    Nature 521, 436–444. URL: [http://www.nature.com/articles/nature14539](http://www.nature.com/articles/nature14539),
    doi:[10.1038/nature14539](https:/doi.org/10.1038/nature14539).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ledig et al. (2017) Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham,
    A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., Shi, W., 2017. Photo-realistic
    single image super-resolution using a generative adversarial network, in: Proceedings
    - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
    pp. 105–114. doi:[10.1109/CVPR.2017.19](https:/doi.org/10.1109/CVPR.2017.19).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2018) Lee, K.H., He, X., Zhang, L., Yang, L., 2018. CleanNet: Transfer
    Learning for Scalable Image Classifier Training with Label Noise, in: Proceedings
    of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.
    doi:[10.1109/CVPR.2018.00571](https:/doi.org/10.1109/CVPR.2018.00571).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lennox et al. (2017) Lennox, R.J., Aarestrup, K., Cooke, S.J., Cowley, P.D.,
    Deng, Z.D., Fisk, A.T., Harcourt, R.G., Heupel, M., Hinch, S.G., Holland, K.N.,
    Hussey, N.E., Iverson, S.J., Kessel, S.T., Kocik, J.F., Lucas, M.C., Flemming,
    J.M., Nguyen, V.M., Stokesbury, M.J., Vagle, S., Vanderzwaag, D.L., Whoriskey,
    F.G., Young, N., 2017. Envisioning the Future of Aquatic Animal Tracking: Technology,
    Science, and Application. doi:[10.1093/biosci/bix098](https:/doi.org/10.1093/biosci/bix098).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Du (2021) Li, D., Du, L., 2021. Recent advances of deep learning algorithms
    for aquacultural machine vision systems with emphasis on fish. Artificial Intelligence
    Review , 1–40URL: [https://link.springer.com/article/10.1007/s10462-021-10102-3https://link.springer.com/10.1007/s10462-021-10102-3](https://link.springer.com/article/10.1007/s10462-021-10102-3https://link.springer.com/10.1007/s10462-021-10102-3),
    doi:[10.1007/s10462-021-10102-3](https:/doi.org/10.1007/s10462-021-10102-3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2009) Li, L.J., Li, K., Li, F.F., Deng, J., Dong, W., Socher, R.,
    Fei-Fei, L., 2009. ImageNet: a Large-Scale Hierarchical Image Database Shrimp
    Project View project hybrid intrusion detction systems View project ImageNet:
    A Large-Scale Hierarchical Image Database. 2009 IEEE Conference on Computer Vision
    and Pattern Recognition .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2015) Li, X., Shang, M., Qin, H., Chen, L., 2015. Fast accurate
    fish detection and recognition of underwater images with Fast R-CNN, in: OCEANS
    2015 - MTS/IEEE Washington, pp. 1–5. doi:[https://doi.org/10.23919/OCEANS.2015.7404464](https:/doi.org/https://doi.org/10.23919/OCEANS.2015.7404464).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Li, Z., Li, W., Li, F., Yuan, M., 2020. A Review of Computer
    Vision Technologies for Fish Tracking URL: [https://arxiv.org/abs/2110.02551v3](https://arxiv.org/abs/2110.02551v3),
    doi:[10.48550/arxiv.2110.02551](https:/doi.org/10.48550/arxiv.2110.02551).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2016) Lin, D., Dai, J., Jia, J., He, K., Sun, J., 2016. Scribblesup:
    Scribble-supervised convolutional networks for semantic segmentation, pp. 3159–3167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014. Microsoft COCO: Common objects in
    context, in: Lecture Notes in Computer Science (including subseries Lecture Notes
    in Artificial Intelligence and Lecture Notes in Bioinformatics). doi:[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lindeberg (2012) Lindeberg, T., 2012. Scale Invariant Feature Transform. Scholarpedia
    7, 10491. URL: [http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform](http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform),
    doi:[10.4249/scholarpedia.10491](https:/doi.org/10.4249/scholarpedia.10491).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Liu, L., Lu, H., Cao, Z., Xiao, Y., 2018. Counting Fish in
    Sonar Images, in: Proceedings - International Conference on Image Processing,
    ICIP. doi:[10.1109/ICIP.2018.8451154](https:/doi.org/10.1109/ICIP.2018.8451154).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lumauag and Nava (2019) Lumauag, R., Nava, M., 2019. Fish tracking and counting
    using image processing, in: 2018 IEEE 10th International Conference on Humanoid,
    Nanotechnology, Information Technology, Communication and Control, Environment
    and Management, HNICEM 2018. doi:[10.1109/HNICEM.2018.8666369](https:/doi.org/10.1109/HNICEM.2018.8666369).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mader et al. (2018) Mader, A.O., Lorenz, C., Bergtholdt, M., von Berg, J., Schramm,
    H., Modersitzki, J., Meyer, C., 2018. Detection and localization of spatially
    correlated point landmarks in medical images using an automatically learned conditional
    random field. Computer Vision and Image Understanding doi:[10.1016/j.cviu.2018.09.009](https:/doi.org/10.1016/j.cviu.2018.09.009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahmood et al. (2016) Mahmood, A., Bennamoun, M., An, S., Sohel, F., Boussaid,
    F., Hovey, R., Kendrick, G., Fisher, R.B., 2016. Coral classification with hybrid
    feature representations, in: Proceedings - International Conference on Image Processing,
    ICIP. doi:[10.1109/ICIP.2016.7532411](https:/doi.org/10.1109/ICIP.2016.7532411).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mandal et al. (2018) Mandal, R., Connolly, R.M., Schlacher, T.A., Stantic,
    B., 2018. Assessing fish abundance from underwater video using deep neural networks,
    in: Proceedings of the International Joint Conference on Neural Networks. doi:[10.1109/IJCNN.2018.8489482](https:/doi.org/10.1109/IJCNN.2018.8489482).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. (2020) Mathur, M., Vasudev, D., Sahoo, S., Jain, D., Goel, N.,
    2020. Crosspooled FishNet: transfer learning based fish species classification
    model. Multimedia Tools and Applications doi:[10.1007/s11042-020-09371-x](https:/doi.org/10.1007/s11042-020-09371-x).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2018) Meng, L., Hirayama, T., Oyanagi, S., 2018. Underwater-Drone
    with Panoramic Camera for Automatic Fish Recognition Based on Deep Learning. IEEE
    Access doi:[10.1109/ACCESS.2018.2820326](https:/doi.org/10.1109/ACCESS.2018.2820326).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Molchanov et al. (2016) Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz,
    J., 2016. Pruning Convolutional Neural Networks for Resource Efficient Transfer
    Learning. CoRR abs/1611.0. URL: [http://arxiv.org/abs/1611.06440](http://arxiv.org/abs/1611.06440).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moniruzzaman et al. (2017) Moniruzzaman, M., Islam, S.M.S., Bennamoun, M.,
    Lavery, P., 2017. Deep learning on underwater marine object detection: A survey,
    in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
    Intelligence and Lecture Notes in Bioinformatics), Springer Verlag. pp. 150–160.
    URL: [http://link.springer.com/10.1007/978-3-319-70353-4_13](http://link.springer.com/10.1007/978-3-319-70353-4_13),
    doi:[10.1007/978-3-319-70353-4_13](https:/doi.org/10.1007/978-3-319-70353-4_13).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naseer et al. (2020) Naseer, A., Baro, E.N., Khan, S.D., Gordillo, Y.V., 2020.
    Automatic Detection of Nephrops norvegicus Burrows in Underwater Images Using
    Deep Learning, in: 2020 Global Conference on Wireless and Optical Technologies,
    GCWOT 2020. doi:[10.1109/GCWOT49901.2020.9391590](https:/doi.org/10.1109/GCWOT49901.2020.9391590).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng (2004) Ng, A.Y., 2004. Feature selection, L1 vs. L2 regularization, and
    rotational invariance, in: Proceedings, Twenty-First International Conference
    on Machine Learning, ICML 2004. doi:[10.1145/1015330.1015435](https:/doi.org/10.1145/1015330.1015435).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nilssen et al. (2017) Nilssen, I., Moller, T., Nattkemper, T.W., 2017. Active
    Learning for the Classification of Species in Underwater Images from a Fixed Observatory,
    in: Proceedings - 2017 IEEE International Conference on Computer Vision Workshops,
    ICCVW 2017, pp. 2891–2897. doi:[10.1109/ICCVW.2017.341](https:/doi.org/10.1109/ICCVW.2017.341).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ogunlana et al. (2015) Ogunlana, S.O., Olabode, O., Oluwadare, S.A.A., 2015.
    Fish Classification Using Support Vector Machine. African Journal of Computing
    & ICT 8, 75–82.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Olsen et al. (2019) Olsen, A., Konovalov, D.A., Philippa, B., Ridd, P., Wood,
    J.C., Johns, J., Banks, W., Girgenti, B., Kenny, O., Whinney, J., Calvert, B.,
    Azghadi, M.R., White, R.D., 2019. DeepWeeds: A Multiclass Weed Species Image Dataset
    for Deep Learning. Scientific Reports 9, 2058. URL: [https://www.nature.com/articles/s41598-018-38343-3http://www.nature.com/articles/s41598-018-38343-3](https://www.nature.com/articles/s41598-018-38343-3http://www.nature.com/articles/s41598-018-38343-3),
    doi:[10.1038/s41598-018-38343-3](https:/doi.org/10.1038/s41598-018-38343-3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oquab et al. (2015) Oquab, M., Bottou, L., Laptev, I., Sivic, J., 2015. Is
    object localization for free? - Weakly-supervised learning with convolutional
    neural networks, in: Proceedings of the IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition. doi:[10.1109/CVPR.2015.7298668](https:/doi.org/10.1109/CVPR.2015.7298668).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pathak et al. (2018) Pathak, A.R., Pandey, M., Rautaray, S., 2018. Application
    of Deep Learning for Object Detection, in: Procedia Computer Science. doi:[10.1016/j.procs.2018.05.144](https:/doi.org/10.1016/j.procs.2018.05.144).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. (2015) Pathak, D., Krähenbühl, P., Darrell, T., Krahenbuhl, P.,
    Darrell, T., 2015. Constrained Convolutional Neural Networks for Weakly Supervised
    Segmentation. 2015 IEEE International Conference on Computer Vision (ICCV) , 1796–1804.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Potdar et al. (2020) Potdar, A.M., Narayan, D.G., Kengond, S., Mulla, M.M.,
    2020. Performance Evaluation of Docker Container and Virtual Machine, in: Procedia
    Computer Science, pp. 1419–1428. doi:[10.1016/j.procs.2020.04.152](https:/doi.org/10.1016/j.procs.2020.04.152).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(98) Qi, X., Liu, Z., Shi, J., Zhao, H., Jia, J., . Augmented feedback in semantic
    segmentation under image level supervision, in: European Conference on Computer
    Vision, Springer. pp. 90–105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2016) Qian, Z.M., Wang, S.H., Cheng, X.E., Chen, Y.Q., 2016. An
    effective and robust method for tracking multiple fish in video image based on
    fish head detection. BMC Bioinformatics 17, 251. URL: [http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1138-y](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1138-y),
    doi:[10.1186/s12859-016-1138-y](https:/doi.org/10.1186/s12859-016-1138-y).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2018) Qiu, C., Zhang, S., Wang, C., Yu, Z., Zheng, H., Zheng, B.,
    2018. Improving transfer learning and squeeze- and-excitation networks for small-scale
    fine-grained fish image classification. IEEE Access 6, 78503–78512. doi:[10.1109/ACCESS.2018.2885055](https:/doi.org/10.1109/ACCESS.2018.2885055).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajchl et al. (2016) Rajchl, M., Lee, M.C.H., Oktay, O., Kamnitsas, K., Passerat-Palmbach,
    J., Bai, W., Damodaram, M., Rutherford, M.A., Hajnal, J.V., Kainz, B., 2016. Deepcut:
    Object segmentation from bounding box annotations using convolutional neural networks.
    IEEE transactions on medical imaging 36, 683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rassadin and Savchenko (2017) Rassadin, A.G., Savchenko, A.V., 2017. Compressing
    deep convolutional neural networks in visual emotion recognition, in: CEUR Workshop
    Proceedings. doi:[10.18287/1613-0073-2017-1901-207-213](https:/doi.org/10.18287/1613-0073-2017-1901-207-213).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks, in: NIPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2017) Ren, S., He, K., Girshick, R., Sun, J., 2017. Faster R-CNN:
    Towards Real-Time Object Detection with Region Proposal Networks. IEEE Trans.
    Pattern Anal. Mach. Intell. 39, 1137–1149. URL: [https://doi.org/10.1109/TPAMI.2016.2577031](https://doi.org/10.1109/TPAMI.2016.2577031),
    doi:[10.1109/TPAMI.2016.2577031](https:/doi.org/10.1109/TPAMI.2016.2577031).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rojas and Rojas (1996) Rojas, R., Rojas, R., 1996. The Backpropagation Algorithm,
    in: Neural Networks. doi:[10.1007/978-3-642-61068-4_7](https:/doi.org/10.1007/978-3-642-61068-4_7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rova et al. (2007) Rova, A., Mori, G., Dill, L.M., 2007. One fish, two fish,
    butterfish, trumpeter: Recognizing fish in underwater video, in: Proceedings of
    IAPR Conference on Machine Vision Applications, MVA 2007, pp. 404–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saberioon and Cisar (2016) Saberioon, M.M., Cisar, P., 2016. Automated multiple
    fish tracking in three-Dimension using a Structured Light Sensor. Computers and
    Electronics in Agriculture doi:[10.1016/j.compag.2015.12.014](https:/doi.org/10.1016/j.compag.2015.12.014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saleh et al. (2020a) Saleh, A., Laradji, I.H., Konovalov, D.A., Bradley, M.,
    Vazquez, D., Sheaves, M., 2020a. A realistic fish-habitat dataset to evaluate
    algorithms for underwater visual analysis. Scientific Reports 10, 14671. URL:
    [http://www.ncbi.nlm.nih.gov/pubmed/32887922http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC7473859https://www.nature.com/articles/s41598-020-71639-x](http://www.ncbi.nlm.nih.gov/pubmed/32887922http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC7473859https://www.nature.com/articles/s41598-020-71639-x),
    doi:[10.1038/s41598-020-71639-x](https:/doi.org/10.1038/s41598-020-71639-x).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saleh et al. (2020b) Saleh, A., Laradji, I.H., Konovalov, D.A., Bradley, M.,
    Vazquez, D., Sheaves, M., 2020b. A realistic fish-habitat dataset to evaluate
    algorithms for underwater visual analysis. Scientific Reports 10, 14671. URL:
    [https://www.nature.com/articles/s41598-020-71639-x](https://www.nature.com/articles/s41598-020-71639-x),
    doi:[10.1038/s41598-020-71639-x](https:/doi.org/10.1038/s41598-020-71639-x).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saleh et al. (2021) Saleh, A., Laradji, I.H., Lammie, C., Vazquez, D., Flavell,
    C.A., Azghadi, M.R., 2021. A Deep Learning Localization Method for Measuring Abdominal
    Muscle Dimensions in Ultrasound Images. IEEE Journal of Biomedical and Health
    Informatics 25, 3865–3873. URL: [https://ieeexplore.ieee.org/document/9444630/](https://ieeexplore.ieee.org/document/9444630/),
    doi:[10.1109/JBHI.2021.3085019](https:/doi.org/10.1109/JBHI.2021.3085019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saleh et al. (2022) Saleh, A., Sheaves, M., Rahimi Azghadi, M., 2022. Computer
    vision and deep learning for fish classification in underwater habitats: A survey.
    Fish and Fisheries URL: [https://onlinelibrary.wiley.com/doi/10.1111/faf.12666](https://onlinelibrary.wiley.com/doi/10.1111/faf.12666),
    doi:[10.1111/faf.12666](https:/doi.org/10.1111/faf.12666).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salman et al. (2019) Salman, A., Siddiqui, S.A., Shafait, F., Mian, A., Shortis,
    M.R., Khurshid, K., Ulges, A., Schwanecke, U., 2019. Automatic fish detection
    in underwater videos by a deep neural network-based hybrid motion learning system.
    ICES Journal of Marine Science .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salman et al. (2020) Salman, A., Siddiqui, S.A., Shafait, F., Mian, A., Shortis,
    M.R., Khurshid, K., Ulges, A., Schwanecke, U., 2020. Automatic fish detection
    in underwater videos by a deep neural network-based hybrid motion learning system.
    ICES Journal of Marine Science doi:[10.1093/icesjms/fsz025](https:/doi.org/10.1093/icesjms/fsz025).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarigül and Avci (2017) Sarigül, M., Avci, M., 2017. Comparison of Different
    Deep Structures for Fish Classification. International Journal of Computer Theory
    and Engineering doi:[10.7763/ijcte.2017.v9.1167](https:/doi.org/10.7763/ijcte.2017.v9.1167).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schneider and Zhuang (2020) Schneider, S., Zhuang, A., 2020. Counting Fish
    and Dolphins in Sonar Images Using Deep Learning. arXiv preprint arXiv:2007.12808
    URL: [http://arxiv.org/abs/2007.12808](http://arxiv.org/abs/2007.12808).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shelhamer et al. (2017) Shelhamer, E., Long, J., Darrell, T., 2017. Fully Convolutional
    Networks for Semantic Segmentation. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 39, 640–651. doi:[10.1109/TPAMI.2016.2572683](https:/doi.org/10.1109/TPAMI.2016.2572683).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shimada et al. (2021) Shimada, T., Bao, H., Sato, I., Sugiyama, M., 2021. Classification
    From Pairwise Similarities/Dissimilarities and Unlabeled Data via Empirical Risk
    Minimization. Neural Computation 33, 1234–1268. URL: [https://direct.mit.edu/neco/article/33/5/1234/97483/Classification-From-Pairwise-Similarities](https://direct.mit.edu/neco/article/33/5/1234/97483/Classification-From-Pairwise-Similarities),
    doi:[10.1162/neco_a_01373](https:/doi.org/10.1162/neco_a_01373).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siddiqui et al. (2018) Siddiqui, S.A., Salman, A., Malik, M.I., Shafait, F.,
    Mian, A., Shortis, M.R., Harvey, E.S., 2018. Automatic fish species classification
    in underwater videos: Exploiting pre-trained deep neural network models to compensate
    for limited labelled data. ICES Journal of Marine Science doi:[10.1093/icesjms/fsx109](https:/doi.org/10.1093/icesjms/fsx109).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Sun, S., Cao, Z., Zhu, H., Zhao, J., 2019. A Survey of Optimization
    Methods From a Machine Learning Perspective. IEEE Transactions on Cybernetics
    , 1–14doi:[10.1109/tcyb.2019.2950779](https:/doi.org/10.1109/tcyb.2019.2950779).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szegedy et al. (2015a) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015a. Going deeper with
    convolutions, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015b) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2015b. Rethinking the Inception Architecture for Computer Vision. CoRR abs/1512.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarling et al. (2021) Tarling, P., Cantor, M., Clapés, A., Escalera, S., 2021.
    DEEP LEARNING WITH SELF-SUPERVISION AND UNCERTAINTY REGULARIZATION TO COUNT FISH
    IN UNDERWATER IMAGES. Technical Report. Other. URL: [http://www.echoview.com.](http://www.echoview.com.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Villon et al. (2016) Villon, S., Chaumont, M., Subsol, G., Villéger, S., Claverie,
    T., Mouillot, D., 2016. Coral reef fish detection and recognition in underwater
    videos by supervised machine learning: Comparison between deep learning and HOG+SVM
    methods, in: Lecture Notes in Computer Science (including subseries Lecture Notes
    in Artificial Intelligence and Lecture Notes in Bioinformatics). doi:[10.1007/978-3-319-48680-2_15](https:/doi.org/10.1007/978-3-319-48680-2_15).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villon et al. (2018) Villon, S., Mouillot, D., Chaumont, M., Darling, E.S.,
    Subsol, G., Claverie, T., Villéger, S., 2018. A Deep learning method for accurate
    and fast identification of coral reef fishes in underwater images. Ecological
    Informatics doi:[10.1016/j.ecoinf.2018.09.007](https:/doi.org/10.1016/j.ecoinf.2018.09.007).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vogels et al. (2005) Vogels, T.P., Rajan, K., Abbott, L.F., 2005. Neural network
    dynamics. doi:[10.1146/annurev.neuro.28.061604.135637](https:/doi.org/10.1146/annurev.neuro.28.061604.135637).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Wang, D., Vinson, R., Holmes, M., Seibel, G., 2018. Convolutional
    neural network guided blue crab knuckle detection for autonomous crab meat picking
    machine. Optical Engineering doi:[10.1117/1.oe.57.4.043103](https:/doi.org/10.1117/1.oe.57.4.043103).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Wang, G., Hwang, J.N., Williams, K., Wallace, F., Rose,
    C.S., 2017. Shrinking encoding with two-level codebook learning for fine-grained
    fish recognition, in: Proceedings - 2nd Workshop on Computer Vision for Analysis
    of Underwater Imagery, CVAUI 2016 - In Conjunction with International Conference
    on Pattern Recognition, ICPR 2016, pp. 31–36. doi:[10.1109/CVAUI.2016.18](https:/doi.org/10.1109/CVAUI.2016.18).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wang, P., Chen, Q., He, X., Cheng, J., 2020. Towards Accurate
    Post-training Network Quantization via Bit-Split and Stitching, in: PMLR. PMLR,
    pp. 9847–9856. URL: [http://proceedings.mlr.press/v119/wang20c.html](http://proceedings.mlr.press/v119/wang20c.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Kanwar (2021) Wang, S., Kanwar, P., 2021. BFloat16: The secret to
    high performance on Cloud TPUs {$\vert$} Google Cloud Blog. URL: [https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2018) Wei, Y., Xiao, H., Shi, H., Jie, Z., Feng, J., Huang, T.S.,
    2018. Revisiting dilated convolution: A simple approach for weakly-and semi-supervised
    semantic segmentation, pp. 7268–7277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wörz and Rohr (2006) Wörz, S., Rohr, K., 2006. Localization of anatomical point
    landmarks in 3D medical images by fitting 3D parametric intensity models. Medical
    Image Analysis doi:[10.1016/j.media.2005.02.003](https:/doi.org/10.1016/j.media.2005.02.003).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu and Prasad (2018) Wu, H., Prasad, S., 2018. Semi-Supervised Deep Learning
    Using Pseudo Labels for Hyperspectral Image Classification. IEEE Transactions
    on Image Processing 27, 1259–1270. URL: [http://ieeexplore.ieee.org/document/8105856/](http://ieeexplore.ieee.org/document/8105856/),
    doi:[10.1109/TIP.2017.2772836](https:/doi.org/10.1109/TIP.2017.2772836).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu and Matzner (2018) Xu, W., Matzner, S., 2018. Underwater Fish Detection
    Using Deep Learning for Water Power Applications, in: 2018 International Conference
    on Computational Science and Computational Intelligence (CSCI), IEEE. pp. 313–318.
    URL: [https://ieeexplore.ieee.org/document/8947884/](https://ieeexplore.ieee.org/document/8947884/),
    doi:[10.1109/CSCI46756.2018.00067](https:/doi.org/10.1109/CSCI46756.2018.00067).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2016) Xue, Y., Ray, N., Hugh, J., Bigras, G., 2016. Cell counting
    by regression using convolutional neural network, in: Lecture Notes in Computer
    Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
    Notes in Bioinformatics). doi:[10.1007/978-3-319-46604-0_20](https:/doi.org/10.1007/978-3-319-46604-0_20).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Yang, L., Liu, Y., Yu, H., Fang, X., Song, L., Li, D., Chen,
    Y., 2021. Computer Vision Models in Intelligent Aquaculture with Emphasis on Fish
    Detection and Behavior Analysis: A Review. Archives of Computational Methods in
    Engineering 28, 2785–2816. URL: [https://link.springer.com/10.1007/s11831-020-09486-2](https://link.springer.com/10.1007/s11831-020-09486-2),
    doi:[10.1007/s11831-020-09486-2](https:/doi.org/10.1007/s11831-020-09486-2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Zhang, S., Wu, G., Costeira, J.P., Moura, J.M.F., 2017.
    Understanding Traffic Density from Large-Scale Web Camera Data, in: 2017 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5898–5907. doi:[https://doi.org/10.1109/CVPR.2017.454](https:/doi.org/https://doi.org/10.1109/CVPR.2017.454).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, W., Wu, C., Bao, Z., 2022. DPANet: Dual Pooling-aggregated
    Attention Network for fish segmentation. IET Computer Vision 16, 67–82. URL: [https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12065](https://onlinelibrary.wiley.com/doi/10.1049/cvi2.12065),
    doi:[10.1049/cvi2.12065](https:/doi.org/10.1049/cvi2.12065).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zhao, S., Zhang, S., Liu, J., Wang, H., Zhu, J., Li, D.,
    Zhao, R., 2021. Application of machine learning in intelligent fish aquaculture:
    A review. Aquaculture 540, 736724. URL: [https://linkinghub.elsevier.com/retrieve/pii/S0044848621003860](https://linkinghub.elsevier.com/retrieve/pii/S0044848621003860),
    doi:[10.1016/j.aquaculture.2021.736724](https:/doi.org/10.1016/j.aquaculture.2021.736724).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Zhao, X., Yan, S., Gao, Q., 2019. An Algorithm for Tracking
    Multiple Fish Based on Biological Water Quality Monitoring. IEEE Access doi:[10.1109/ACCESS.2019.2895072](https:/doi.org/10.1109/ACCESS.2019.2895072).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou (2018) Zhou, Z.H., 2018. A brief introduction to weakly supervised learning.
    doi:[10.1093/nsr/nwx106](https:/doi.org/10.1093/nsr/nwx106).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2017) Zhuang, P., Xing, L., Liu, Y., Guo, S., Qiao, Y., 2017.
    Marine Animal detection and Recognition with advanced deep learning models, in:
    CEUR Workshop Proceedings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zivkovic and van der Heijden (2006) Zivkovic, Z., van der Heijden, F., 2006.
    Efficient adaptive density estimation per image pixel for the task of background
    subtraction. Pattern Recognition Letters 27, 773–780. doi:[https://doi.org/10.1016/j.patrec.2005.11.005](https:/doi.org/https://doi.org/10.1016/j.patrec.2005.11.005).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zurowietz and Nattkemper (2020) Zurowietz, M., Nattkemper, T.W., 2020. Unsupervised
    Knowledge Transfer for Object Detection in Marine Environmental Monitoring and
    Exploration. IEEE Access 8, 143558–143568. doi:[10.1109/ACCESS.2020.3014441](https:/doi.org/10.1109/ACCESS.2020.3014441).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
