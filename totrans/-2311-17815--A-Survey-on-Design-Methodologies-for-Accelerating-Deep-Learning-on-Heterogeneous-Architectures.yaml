- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:35:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:35:56
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2311.17815] A Survey on Design Methodologies for Accelerating Deep Learning
    on Heterogeneous Architectures'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2311.17815] 加速异构架构上深度学习的设计方法调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.17815](https://ar5iv.labs.arxiv.org/html/2311.17815)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.17815](https://ar5iv.labs.arxiv.org/html/2311.17815)
- en: \SetWatermarkText
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \SetWatermarkText
- en: Preprint \SetWatermarkScale1 \forestsetqtree/.style=for tree=parent anchor=south,
    child anchor=north,align=center,inner sep=0pt
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 预印本 \SetWatermarkScale1 \forestsetqtree/.style=for tree=parent anchor=south,
    child anchor=north,align=center,inner sep=0pt
- en: A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速异构架构上深度学习的设计方法调查
- en: Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)
    ,  Serena Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it) , 
    Leandro Fiorin [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it) , 
    Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)
    ,  Cristina Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it)
    Politecnico di MilanoItaly ,  Francesco Conti [f.conti@unibo.it](mailto:f.conti@unibo.it)
    ,  Alessio Burrello [alessio.burrello@unibo.it](mailto:alessio.burrello@unibo.it)
    ,  Francesco Barchi [francesco.barchi@unibo.it](mailto:francesco.barchi@unibo.it)
    ,  Luca Benini [luca.benini@unibo.it](mailto:luca.benini@unibo.it) Università
    di BolognaViale Carlo Pepoli, 3/240123BolognaItaly ,  Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)
    ,  Teodoro Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) Politecnico
    di TorinoItaly ,  Enrico Calore [enrico.calore@infn.fe.it](mailto:enrico.calore@infn.fe.it)
    ,  Sebastiano Fabio Schifano [schsst@unife.it](mailto:schsst@unife.it) ,  Cristian
    Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it) Università
    degli Studi di FerraraVia Giuseppe Saragat, 144122FerraraItaly ,  Maurizio Palesi
    [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it) ,  Giuseppe Ascia
    [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it) ,  Enrico Russo [enrico.russo@phd.unict.it](mailto:enrico.russo@phd.unict.it)
    Università degli Studi di CataniaItaly ,  Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)
    ,  Davide De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it) ,  Gennaro Di
    Meo [gennaro.dimeo@unina.it](mailto:gennaro.dimeo@unina.it%20) Università degli
    Studi di Napoli Federico IIItaly ,  Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)
    ,  Salvatore Filippone [salvatore.filippone@uniroma2.it](mailto:salvatore.filippone@uniroma2.it)
    ,  Francesco Lo Presti [lopresti@info.uniroma2.it](mailto:lopresti@info.uniroma2.it)
    Università degli Studi di Roma “Tor Vergata”Italy ,  Francesco Silvestri [francesco.silvestri@unipd.it](mailto:francesco.silvestri@unipd.it)
    Università degli Studi di PadovaItaly ,  Paolo Palazzari [paolo.palazzari@enea.it](mailto:paolo.palazzari@enea.it)
    ENEAItaly  and  Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it) Università
    degli Studi della CalabriaItaly(2023)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)，Serena
    Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it)，Leandro Fiorin
    [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it)，Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)，Cristina
    Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it) 意大利米兰理工大学，Francesco
    Conti [f.conti@unibo.it](mailto:f.conti@unibo.it)，Alessio Burrello [alessio.burrello@unibo.it](mailto:alessio.burrello@unibo.it)，Francesco
    Barchi [francesco.barchi@unibo.it](mailto:francesco.barchi@unibo.it)，Luca Benini
    [luca.benini@unibo.it](mailto:luca.benini@unibo.it) 意大利博洛尼亚大学Viale Carlo Pepoli,
    3/240123Bologna意大利，Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)，Teodoro
    Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) 意大利都灵理工大学，Enrico
    Calore [enrico.calore@infn.fe.it](mailto:enrico.calore@infn.fe.it)，Sebastiano
    Fabio Schifano [schsst@unife.it](mailto:schsst@unife.it)，Cristian Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it)
    意大利费拉拉大学Via Giuseppe Saragat, 144122Ferrara意大利，Maurizio Palesi [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it)，Giuseppe
    Ascia [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it)，Enrico Russo [enrico.russo@phd.unict.it](mailto:enrico.russo@phd.unict.it)
    意大利卡塔尼亚大学，Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)，Davide
    De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it)，Gennaro Di Meo [gennaro.dimeo@unina.it](mailto:gennaro.dimeo@unina.it)
    意大利那不勒斯费德里科二世大学，Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)，Salvatore
    Filippone [salvatore.filippone@uniroma2.it](mailto:salvatore.filippone@uniroma2.it)，Francesco
    Lo Presti [lopresti@info.uniroma2.it](mailto:lopresti@info.uniroma2.it) 意大利罗马“托尔·维尔加塔”大学，Francesco
    Silvestri [francesco.silvestri@unipd.it](mailto:francesco.silvestri@unipd.it)
    意大利帕多瓦大学，Paolo Palazzari [paolo.palazzari@enea.it](mailto:paolo.palazzari@enea.it)
    意大利ENEA，Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it) 意大利卡拉布里亚大学（2023）
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In recent years, the field of Deep Learning has seen many disruptive and impactful
    advancements. Given the increasing complexity of deep neural networks, the need
    for efficient hardware accelerators has become more and more pressing to design
    heterogeneous HPC platforms. The design of Deep Learning accelerators requires
    a multidisciplinary approach, combining expertise from several areas, spanning
    from computer architecture to approximate computing, computational models, and
    machine learning algorithms. Several methodologies and tools have been proposed
    to design accelerators for Deep Learning, including hardware-software co-design
    approaches, high-level synthesis methods, specific customized compilers, and methodologies
    for design space exploration, modeling, and simulation. These methodologies aim
    to maximize the exploitable parallelism and minimize data movement to achieve
    high performance and energy efficiency. This survey provides a holistic review
    of the most influential design methodologies and EDA tools proposed in recent
    years to implement Deep Learning accelerators, offering the reader a wide perspective
    in this rapidly evolving field. In particular, this work complements the previous
    survey proposed by the same authors in ([silvano2023survey,](#bib.bib203) ), which
    focuses on Deep Learning hardware accelerators for heterogeneous HPC platforms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习领域出现了许多颠覆性和影响力大的进展。鉴于深度神经网络日益复杂的趋势，对高效硬件加速器的需求变得越来越迫切，以设计异构高性能计算平台。深度学习加速器的设计需要一种多学科的方法，结合计算机体系结构、近似计算、计算模型和机器学习算法等多个领域的专业知识。已经提出了几种方法和工具来设计深度学习加速器，包括硬件-软件协同设计方法、高层次综合方法、特定定制的编译器以及设计空间探索、建模和仿真的方法。这些方法旨在最大化可利用的并行性，最小化数据移动，以实现高性能和能源效率。这项调查全面回顾了近年来提出的最具影响力的设计方法和EDA工具，为读者提供了在这个快速发展的领域中的广泛视角。特别地，这项工作补充了同一作者在
    ([silvano2023survey,](#bib.bib203) ) 提出的之前的调查，该调查聚焦于异构高性能计算平台的深度学习硬件加速器。
- en: '^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†journal: CSUR^†^†copyright:
    none^†^†ccs: Hardware Methodologies for EDA^†^†ccs: Hardware High-level and register-transfer
    level synthesis^†^†ccs: Computer systems organization Architectures^†^†ccs: Hardware Very
    large scale integration design^†^†ccs: Hardware Reconfigurable logic and FPGAs^†^†ccs:
    Computing methodologies Artificial intelligence^†^†ccs: Computing methodologies Machine
    learning'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†版权：acmcopyright^†^†期刊年份：2023^†^†期刊：CSUR^†^†版权：无^†^†ccs：EDA的硬件方法^†^†ccs：硬件高层次和寄存器传输级综合^†^†ccs：计算机系统组织架构^†^†ccs：硬件超大规模集成设计^†^†ccs：硬件可重构逻辑和FPGA^†^†ccs：计算方法人工智能^†^†ccs：计算方法机器学习
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Over the past few years, Deep Learning (DL) has made remarkable progress thanks
    to significant research advancements exploiting DL techniques in several applications
    such as computer vision, natural language processing, speech and audio processing,
    recommendation systems, autonomous systems, environmental sciences, and many more.
    DL is a subset of machine learning that uses artificial Deep Neural Networks (DNNs)
    with multiple layers of artificial neurons to mimic human brain behavior by learning
    from large amounts of data. This technology has the potential to revolutionize
    the way we approach complex problems and make data-driven decisions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，深度学习（DL）取得了显著的进展，这要归功于在计算机视觉、自然语言处理、语音和音频处理、推荐系统、自动驾驶系统、环境科学等多个应用领域中利用深度学习技术的研究突破。深度学习是机器学习的一个子集，利用具有多层人工神经元的人工深度神经网络（DNNs）来模拟人脑行为，通过从大量数据中学习。这项技术有潜力彻底改变我们处理复杂问题和做出数据驱动决策的方式。
- en: To implement a DL application, the developer must have knowledge not only of
    abstract algorithms and training methods but also the ability to map the DNN model
    into an efficient hardware architecture. The task of designing efficient hardware
    accelerators for DL requires expertise from various fields, such as computer architecture,
    approximate computing, computational models, and DL algorithms. Moreover, current
    architectures are evolving to handle multiple design objectives that cater to
    a wide range of systems, from ultra-low power edge devices to high-performance
    architectures. Due to the broad range of architectural alternatives, the EDA community
    has developed various tools and methodologies to support designers in their development
    efforts. These tools and technologies include high-level synthesis approaches,
    specific customized compilers, and tools to support the design space exploration
    as well as the modeling, partitioning, and mapping. Achieving high performance
    and low power consumption with resource constraints in heterogeneous architectures
    requires a comprehensive understanding of existing EDA frameworks and tools. This
    understanding enables the adoption of appropriate methods to exploit parallelism
    and minimize data movement, thereby fostering innovation in this domain.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现深度学习（DL）应用，开发人员必须具备不仅是抽象算法和训练方法的知识，还要有将深度神经网络（DNN）模型映射到高效硬件架构的能力。设计高效的硬件加速器用于深度学习的任务需要来自多个领域的专业知识，如计算机架构、近似计算、计算模型和深度学习算法。此外，当前的架构正不断演进，以处理多种设计目标，适应从超低功耗边缘设备到高性能架构的广泛系统。由于架构选择的广泛性，EDA（电子设计自动化）社区已经开发了各种工具和方法，以支持设计师的开发工作。这些工具和技术包括高级综合方法、特定定制的编译器，以及支持设计空间探索、建模、分区和映射的工具。在异构架构中实现高性能和低功耗，并满足资源约束，需要对现有EDA框架和工具有全面的理解。这种理解使得采用适当的方法来利用并行性并最小化数据移动，从而促进该领域的创新。
- en: Scope of the survey. This work is an attempt to provide a holistic overview
    of the most influential frameworks, design methodologies and tools to implement
    DL for high-performance applications. The survey highlights various approaches
    that support the optimization and mapping of a DL model to different hardware
    accelerators, including GPU-based accelerators, Tensor Processor Units, FPGA-based
    accelerators, and ASIC-based accelerators, including Neural Processing Units and
    co-processors on the open-hardware RISC-V architecture. The survey also describes
    methodologies supporting the implementation of accelerators based on emerging
    technologies and computing paradigms, such as 3D-stacked Processor-In-Memory architectures.
    As summarized in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on
    Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    several design methodologies and EDA tools used to design DL hardware accelerators
    are reviewed in this survey. Although we do not claim to provide a comprehensive
    survey, with approximately 250 works referenced from the past two decades on design
    methodologies for DL acceleration, we aim to present a holistic perspective on
    the field of EDA tools for DL accelerators. Moreover, this survey refers to previous
    surveys on the AI and DL algorithms on how a DL model can solve many existing
    problems, such as  ([CSR19,](#bib.bib177) ; [goodfellow2016deep,](#bib.bib95)
    ), and previous surveys about hardware architecture implementing DL networks,
    such as  ([Akkad23,](#bib.bib13) ; [chen2020engineering,](#bib.bib48) ; [Hassanpour2022,](#bib.bib102)
    ; [gao2023acm,](#bib.bib89) ; [rathi2023acm,](#bib.bib183) ; [silvano2023survey,](#bib.bib203)
    ).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 调查范围。本工作旨在提供对实施深度学习（DL）以满足高性能应用的最具影响力的框架、设计方法和工具的全面概述。该调查突出展示了支持优化和将DL模型映射到不同硬件加速器的各种方法，包括基于GPU的加速器、张量处理单元、基于FPGA的加速器和基于ASIC的加速器，包括神经处理单元和开放硬件RISC-V架构上的协处理器。调查还描述了支持基于新兴技术和计算范式的加速器实现的方法论，例如3D堆叠的处理器内存架构。正如图 [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") 所总结的，这项调查回顾了几种用于设计DL硬件加速器的设计方法和EDA工具。尽管我们不声称提供全面的调查，但参考了过去二十年中约250篇关于DL加速设计方法的工作，我们旨在呈现EDA工具在DL加速器领域的全面视角。此外，本调查还参考了关于AI和DL算法如何解决许多现有问题的先前调查，例如 ([CSR19,](#bib.bib177)
    ; [goodfellow2016deep,](#bib.bib95) )，以及关于实现DL网络的硬件架构的先前调查，例如 ([Akkad23,](#bib.bib13)
    ; [chen2020engineering,](#bib.bib48) ; [Hassanpour2022,](#bib.bib102) ; [gao2023acm,](#bib.bib89)
    ; [rathi2023acm,](#bib.bib183) ; [silvano2023survey,](#bib.bib203) )。
- en: Organization of the survey. The survey covers methodologies that support the
    generation of new DL operators by leveraging approximate computing and automatic
    translation of high-level operators into hardware architectures, as well as compilers
    and modeling tools that aid in the design space exploration to appropriately map
    heterogeneous DL applications on existing computing nodes, regardless of the non-functional
    requirements such as resource usage or performance that the application considers.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    the proposed classification of tools and methodologies is based on the main design
    flows and frameworks that have been developed in recent years. Moreover, Section [2](#S2
    "2\. An overview of computational models for DL on HPC systems ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    summarizes the most widely models used in DL application workloads. Section [3](#S3
    "3\. Hardware/Software Codesign Methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") introduces approaches
    and tools appropriate to distribute, partition, and map DL models on heterogeneous
    processing systems. Section [4](#S4 "4\. Approximate Computing Methodologies ‣
    A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") describes representative approaches based on the approximate computing
    paradigm. Section [5](#S5 "5\. HLS-based design methodologies ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    presents the design methodologies suitable for the high-level synthesis of hardware
    accelerators, whereas Section [6](#S6 "6\. Deep Learning Compilers ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    discusses automated compilation and deployment technologies for DL applications.
    Then, Section [7](#S7 "7\. Modeling, Simulation, Profiling and Exploration ‣ A
    Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") describes the modeling, simulation, profiling, and design exploration
    frameworks currently adopted for DL applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的组织。该调查涵盖了通过利用近似计算和将高级操作符自动转换为硬件架构来支持生成新深度学习（DL）操作符的方法论，以及帮助设计空间探索的编译器和建模工具，以适当地将异构深度学习应用程序映射到现有计算节点上，而不考虑应用程序所考虑的资源使用或性能等非功能需求。如图 [1](#S1.F1
    "图 1 ‣ 1\. 介绍 ‣ 加速深度学习在异构架构上的设计方法调查")所示，所提出的工具和方法论分类基于近年来开发的主要设计流程和框架。此外，第 [2](#S2
    "2\. 高性能计算系统中深度学习的计算模型概述 ‣ 加速深度学习在异构架构上的设计方法调查")节总结了在深度学习应用工作负载中最广泛使用的模型。第 [3](#S3
    "3\. 硬件/软件协同设计方法论 ‣ 加速深度学习在异构架构上的设计方法调查")节介绍了适用于在异构处理系统上分配、划分和映射深度学习模型的方法和工具。第 [4](#S4
    "4\. 近似计算方法论 ‣ 加速深度学习在异构架构上的设计方法调查")节描述了基于近似计算范式的代表性方法。第 [5](#S5 "5\. 基于HLS的设计方法论
    ‣ 加速深度学习在异构架构上的设计方法调查")节介绍了适用于硬件加速器的高级综合设计方法，而第 [6](#S6 "6\. 深度学习编译器 ‣ 加速深度学习在异构架构上的设计方法调查")节讨论了深度学习应用程序的自动编译和部署技术。然后，第 [7](#S7
    "7\. 建模、仿真、分析和探索 ‣ 加速深度学习在异构架构上的设计方法调查")节描述了当前用于深度学习应用程序的建模、仿真、分析和设计探索框架。
- en: To conclude, we hope this survey could be useful for a wide range of readers,
    including computer architects, hardware and software developers, tool developers,
    HPC engineers, researchers, and technical professionals. A major effort was spent
    using a clear and concise technical writing style to be useful, particularly,
    to the young generations of master and Ph.D. students. To facilitate the reading,
    a list of acronyms is reported in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures").
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们希望这项调查对计算机架构师、硬件和软件开发人员、工具开发人员、高性能计算工程师、研究人员以及技术专业人员等广泛读者有用。我们花费了大量精力以清晰简洁的技术写作风格来撰写，特别是对年轻一代硕士和博士生尤其有用。为了方便阅读，缩略语列表见表 [1](#S1.T1
    "表 1 ‣ 1\. 介绍 ‣ 加速深度学习在异构架构上的设计方法调查")。
- en: \forestset
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \forestset
- en: dir tree/.style= for tree= parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: dir tree/.style= for tree= parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
- en: '{forest}'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: dir tree [Survey Organization [§ [2](#S2 "2\. An overview of computational models
    for DL on HPC systems ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") An overview of computational models
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: dir tree [调查组织 [§ [2](#S2 "2\. 关于计算模型在高性能计算系统中的应用 ‣ 加速深度学习的异构架构设计方法综述") 计算模型概述
- en: 'for DL on HPC systems ] [§ [3](#S3 "3\. Hardware/Software Codesign Methodologies
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") Hardware/Software Codesign Methodologies][§ [4](#S4 "4\. Approximate
    Computing Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") Approximate Computing Methodologies]
    [§ [5](#S5 "5\. HLS-based design methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") HLS Design-based
    methodologies [Vitis High-Level Synthesis ] [Bambu: open-source High-level Synthesis]
    [Other HLS tools] [HLS-based design flows for deep learning] [IP block integration]
    ] ]'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '对于高性能计算系统中的深度学习] [§ [3](#S3 "3\. 硬件/软件协同设计方法 ‣ 加速深度学习的异构架构设计方法综述") 硬件/软件协同设计方法][§
    [4](#S4 "4\. 近似计算方法 ‣ 加速深度学习的异构架构设计方法综述") 近似计算方法] [§ [5](#S5 "5\. 基于HLS的设计方法 ‣
    加速深度学习的异构架构设计方法综述") 基于HLS的设计方法 [Vitis高层综合] [Bambu: 开源高层综合] [其他HLS工具] [深度学习的HLS设计流程]
    [IP块集成] ] ]'
- en: (a)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '{forest}'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: dir tree [ [§ [6](#S6 "6\. Deep Learning Compilers ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") Deep Learning
    Compilers [Memory hierarchy management in DNN Accelerators] [Deep Learning Compilers
    for MCUs] [Deep Learning Compilers for High-Performance] ] [§ [7](#S7 "7\. Modeling,
    Simulation, Profiling and Exploration ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") Modeling  Simulation  Profiling
    and Exploration [Modeling  Simulation  and Exploration Frameworks] [Simulation
    tools for emerging memories-based
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: dir tree [ [§ [6](#S6 "6\. 深度学习编译器 ‣ 加速深度学习的异构架构设计方法综述") 深度学习编译器 [DNN加速器中的内存层次管理]
    [MCU的深度学习编译器] [高性能的深度学习编译器] ] [§ [7](#S7 "7\. 建模、仿真、分析与探索 ‣ 加速深度学习的异构架构设计方法综述")
    建模  仿真  分析与探索 [建模  仿真  和探索框架] [用于新兴存储器的仿真工具
- en: DNN accelerator] [Cycle-Accurate Simulators] [Modeling and Profiling FPGAs for
    custom accelerators] ] ]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DNN加速器] [周期精确模拟器] [定制加速器的FPGA建模与分析] ] ]
- en: (b)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 1\. Organization of the survey
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 调查组织
- en: Table 1. List of acronyms
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 缩写词列表
- en: '| Acronym | Acronym | Acronym |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 缩写词 | 缩写词 | 缩写词 |'
- en: '| AI: Artificial Intelligence | ASIC: Application Specific Integrated Circuit
    | AXI: Advanced eXtensible Interface |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| AI: 人工智能 | ASIC: 应用特定集成电路 | AXI: 高级可扩展接口 |'
- en: '| CNN: Convolutional Neural Network | CPU: Central Processing Unit | DL: Deep
    Learning |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| CNN: 卷积神经网络 | CPU: 中央处理单元 | DL: 深度学习 |'
- en: '| DMA: Direct Memory Access | DNN: Deep Neural Network | DP: Double Precision
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| DMA: 直接内存访问 | DNN: 深度神经网络 | DP: 双精度 |'
- en: '| DRAM: Dynamic Random Access Memory | DSP: Digital Signal Processing | DSE:
    Design Space Exploration |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| DRAM: 动态随机存取内存 | DSP: 数字信号处理 | DSE: 设计空间探索 |'
- en: '| FFT: Fast Fourier Transform | FP: Floating Point | FPGA: Field-Programmable
    Gate Array |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| FFT: 快速傅里叶变换 | FP: 浮点 | FPGA: 现场可编程门阵列 |'
- en: '| GEMM: General Matrix Multiply | GPU: Graphics Processing Unit | HBM: High
    Bandwidth Memory |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| GEMM: 一般矩阵乘法 | GPU: 图形处理单元 | HBM: 高带宽内存 |'
- en: '| HLS: High Level Synthesis | HPC: High-Performance Computing | HPZMO: High-Performance
    Zero-Memory Overhead |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| HLS: 高级综合 | HPC: 高性能计算 | HPZMO: 高性能零内存开销 |'
- en: '| IP: Intellectual Property | ISA: Instruction Set Architecture | LUT: Lookup
    Table |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| IP: 知识产权 | ISA: 指令集架构 | LUT: 查找表 |'
- en: '| MCU: Microcontroller Unit | MEC: Memory-Efficient Convolution | ML: Machine
    Learning |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| MCU: 微控制器单元 | MEC: 内存高效卷积 | ML: 机器学习 |'
- en: '| MLIR: Multi-Level Intermediate Representation | NVDLA: NVIDIA Deep Learning
    Accelerator | PE: Processing Element |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MLIR: 多级中间表示 | NVDLA: NVIDIA 深度学习加速器 | PE: 处理元素 |'
- en: '| PIM: Processing In-Memory | PRAM: Parallel Random Access Machine | RAM: Random
    Access Machine |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| PIM: 内存处理 | PRAM: 并行随机访问机器 | RAM: 随机访问机器 |'
- en: '| RISC: Reduced Instruction Set Computer | RTL: Register Transfer Level | SIMD:
    Single Instruction Multiple Data |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| RISC: 精简指令集计算机 | RTL: 寄存器传输级 | SIMD: 单指令多数据 |'
- en: '| SoC: System on Chip | SPP: Structured Parallel Programming | SRAM: Static
    Random Access Memory |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SoC: 芯片系统 | SPP: 结构化并行编程 | SRAM: 静态随机存取存储器 |'
- en: 2\. An overview of computational models for DL on HPC systems
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 针对HPC系统的深度学习计算模型概述
- en: The design of hardware accelerators for next-generation HPC systems requires
    proper tools and design methodologies, as well as appropriate models of computations.
    Computational models provide the designer with an adequate level of abstraction
    from the low-level characteristics and allow estimating detailed insights of features
    and capabilities to improve the behavior of the accelerators under design in terms
    of power consumption, performance, and resource requirements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设计下一代高性能计算（HPC）系统的硬件加速器需要合适的工具和设计方法，以及适当的计算模型。计算模型为设计师提供了从低级特征中抽象出来的足够层次，使其能够评估加速器在设计中的详细特性和能力，从而改进其在功耗、性能和资源需求方面的表现。
- en: Among the computational models suitable to minimize the number of CPU operations
    and investigate the computational limits of the designed algorithm, the Random
    Access Machine (RAM) model is one of the most representative and it is certainly
    a cornerstone of the history of computing. However, with the advent of modern
    hardware technologies, more advanced computational models have become necessary
    to capture different data access policies and new data flows. On the one hand,
    algorithms and data structures that fully exploit the memory hierarchy can be
    designed and analyzed through the *External Memory model* ([Vitter01,](#bib.bib230)
    ). On the other hand, parallel architectures, such as BSP, PRAM, LogP, and MapReduce
    can be profiled as *Parallel Computational* models ([Bilardi2011,](#bib.bib24)
    ).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在适合最小化CPU操作数量和研究设计算法计算极限的计算模型中，随机访问机器（RAM）模型是最具代表性的模型之一，它无疑是计算历史上的基石。然而，随着现代硬件技术的出现，更先进的计算模型变得必要，以捕捉不同的数据访问策略和新的数据流。一方面，可以通过*外部内存模型*（[Vitter01,](#bib.bib230)）设计和分析完全利用内存层次结构的算法和数据结构。另一方面，像BSP、PRAM、LogP和MapReduce这样的并行架构可以被归类为*并行计算*模型（[Bilardi2011,](#bib.bib24)）。
- en: Stimulated by the benefits offered by tensor cores ([Dakkak19,](#bib.bib62)
    ; [Sorna18,](#bib.bib208) ; [LuChen21,](#bib.bib144) ) and processing-in-memory
    (PIM) architectures ([Zois18,](#bib.bib254) ; [Choe19,](#bib.bib54) ), several
    computational models have been defined specifically for these technologies. As
    an example, the computational model introduced in ([ChowdhurySV21,](#bib.bib56)
    ) captures the ability of a tensor core to efficiently perform dense matrix multiplication
    of fixed size $\sqrt{m}\times\sqrt{m}$ in time $O(m+\ell)$, where $\ell$ is a
    latency cost. This model can analyze the performances of algorithms for linear
    algebra, graph, stencil, sparse matrix multiplication ([labini22,](#bib.bib134)
    ), and similarity search ([AhleS20,](#bib.bib9) ).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 受张量核心（[Dakkak19,](#bib.bib62)；[Sorna18,](#bib.bib208)；[LuChen21,](#bib.bib144)）和内存处理（PIM）架构（[Zois18,](#bib.bib254)；[Choe19,](#bib.bib54)）带来的好处启发，已为这些技术专门定义了几种计算模型。例如，在（[ChowdhurySV21,](#bib.bib56)）中介绍的计算模型捕捉了张量核心高效执行固定大小$\sqrt{m}\times\sqrt{m}$的稠密矩阵乘法的能力，时间为$O(m+\ell)$，其中$\ell$是延迟成本。该模型可以分析线性代数、图形、模板、稀疏矩阵乘法（[labini22,](#bib.bib134)）和相似性搜索（[AhleS20,](#bib.bib9)）算法的性能。
- en: The efficient performance model provided in ([Liu17,](#bib.bib143) ) combines
    a CPU, consisting of a certain number of parallel cores enabled to fast access
    a small shared memory of $M$ data words, and a PIM accelerator consisting of $P$
    elements, each provided with a local memory of $\Theta(n/P)$ data words, with
    $n$ denoting the input size of the problem. This model has been successfully adopted
    for the theoretical analysis of a skip-list ([Kang21,](#bib.bib126) ) and an index
    for a skewed data ([kang2022pim,](#bib.bib127) ) algorithm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的高效性能模型（[Liu17,](#bib.bib143)）结合了一个由若干并行核心组成的CPU，这些核心能够快速访问小规模的共享内存（$M$个数据字），以及一个由$P$个元素组成的PIM加速器，每个元素都配有$\Theta(n/P)$个数据字的本地内存，其中$n$表示问题的输入大小。该模型已成功应用于跳表的理论分析（[Kang21,](#bib.bib126)）和一个用于偏斜数据的索引（[kang2022pim,](#bib.bib127)）算法。
- en: Given the high level of parallelism required by DL workloads, Structured Parallel
    Programming (SPP) ([Skeletons,](#bib.bib58) ; [MacroDF,](#bib.bib67) ) and Parallel
    Design Patterns (PDP) ([PPatterns,](#bib.bib64) ) are widely recognized as efficient
    methodologies to build powerful and reliable computational models for mapping
    data parallel computations onto both multi-cores and GPU-based platforms. Over
    the years, a broad space of programming tools and frameworks adopting SPP and
    PDP methodologies have been provided ([Skeletons-Survey,](#bib.bib94) ). Among
    them, the prototype framework FastFlow ([FastFlow,](#bib.bib14) ) is worth special
    attention for bringing the SPP approach to widespread application domains, such
    as DL and HPC. FastFlow is an environment based on a layered structure that incorporates
    algorithmic skeletons, mechanisms, and concepts useful to prototype different
    kinds of parallel patterns. The first layer collects a set of high-level skeletons
    modeling built-in parallel patterns. The intermediate layer makes available the
    structures suitable to instantiate parallel patterns that are not directly modeled
    by built-in skeletons, like parallel blocks, sequential blocks, and pipeline blocks,
    that allow modeling of both temporal and spatial parallelism, and the all-to-all
    block that implements the complete connection between blocks. The last layer of
    the FastFlow hierarchy is composed of a set of low-level mechanisms that depend
    on the target hardware platform and are invisible to both domain experts, who
    use high-level parallel skeletons, and to programmers, who use the intermediate
    building blocks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度学习工作负载对并行性的高要求，结构化并行编程（SPP）（[Skeletons,](#bib.bib58)；[MacroDF,](#bib.bib67)）和并行设计模式（PDP）（[PPatterns,](#bib.bib64)）被广泛认可为构建强大且可靠的计算模型的有效方法，用于将数据并行计算映射到多核和基于GPU的平台。多年来，采用SPP和PDP方法论的编程工具和框架已得到广泛提供（[Skeletons-Survey,](#bib.bib94)）。其中，原型框架FastFlow（[FastFlow,](#bib.bib14)）因其将SPP方法应用于广泛的应用领域（如深度学习和高性能计算）而值得特别关注。FastFlow是一个基于分层结构的环境，结合了算法骨架、机制和概念，适用于原型设计不同类型的并行模式。第一层收集了一组建模内置并行模式的高层次骨架。中间层提供了适合实例化未由内置骨架直接建模的并行模式的结构，如并行块、顺序块和管道块，这些块允许对时间和空间并行性进行建模，以及实现块间完全连接的全对全块。FastFlow层次结构的最后一层由一组低层机制组成，这些机制依赖于目标硬件平台，对使用高层次并行骨架的领域专家和使用中间构建块的程序员均不可见。
- en: Linear Algebra for Deep Learning. Several typical workloads of DL applications
    are dominated by linear algebra kernels. Therefore, a plethora of computational
    models have been proposed in the literature to make the use of linear algebra
    approachable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的线性代数。深度学习应用的若干典型工作负载由线性代数内核主导。因此，文献中提出了大量计算模型，使得线性代数的使用变得更加易于接近。
- en: The so-called task-based runtime environments ([Agullo2017,](#bib.bib7) ; [Agullo2023,](#bib.bib8)
    ; [Carratala2020,](#bib.bib40) ; [Herault2022,](#bib.bib103) ; [Pei2022,](#bib.bib173)
    ) allow complex algorithms to be encoded by specifying a set of dependencies among
    various building blocks, thus increasing the programmability of various kinds
    of complex linear algebra algorithms ([Schatz2016,](#bib.bib195) ; [Quintana2009,](#bib.bib181)
    ; [Gates2022,](#bib.bib90) ). It is worth noting that, in such environments, developing
    libraries to collect application-specific building blocks is crucial to provide
    an optimal mapping from problem to function calls  ([Psarras2022,](#bib.bib179)
    ). An essential ingredient to develop building blocks and libraries for DL applications
    is heterogeneity ([Cardellini2014,](#bib.bib39) ; [Filippone2017,](#bib.bib82)
    ; [Carter2014,](#bib.bib42) ). Moreover, the reduction of communication costs,
    including both latency and bandwidth, is a fundamental requirement to achieve
    efficient hardware accelerator architectures ([6496136,](#bib.bib69) ; [6514719,](#bib.bib148)
    ; [doi:10.1137/120881191,](#bib.bib41) ). Finally, the possibility of exploiting
    mixed-precision operation modes ([doi:10.1177/10943420211003313,](#bib.bib1) ;
    [10.1145/3441850,](#bib.bib84) ; [higham_mary_2022,](#bib.bib105) ) and randomization
    ([murray2023randomized,](#bib.bib157) ) is a very attractive feature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的任务基础运行时环境 ([Agullo2017,](#bib.bib7) ; [Agullo2023,](#bib.bib8) ; [Carratala2020,](#bib.bib40)
    ; [Herault2022,](#bib.bib103) ; [Pei2022,](#bib.bib173) ) 允许通过指定各种构建模块之间的依赖关系来编码复杂算法，从而提高各种复杂线性代数算法的可编程性
    ([Schatz2016,](#bib.bib195) ; [Quintana2009,](#bib.bib181) ; [Gates2022,](#bib.bib90)
    )。值得注意的是，在这些环境中，开发收集特定应用的构建模块的库是至关重要的，以提供从问题到函数调用的最佳映射 ([Psarras2022,](#bib.bib179)
    )。开发深度学习应用的构建模块和库的一个重要因素是异质性 ([Cardellini2014,](#bib.bib39) ; [Filippone2017,](#bib.bib82)
    ; [Carter2014,](#bib.bib42) )。此外，减少通信成本，包括延迟和带宽，是实现高效硬件加速器架构的基本要求 ([6496136,](#bib.bib69)
    ; [6514719,](#bib.bib148) ; [doi:10.1137/120881191,](#bib.bib41) )。最后，利用混合精度操作模式
    ([doi:10.1177/10943420211003313,](#bib.bib1) ; [10.1145/3441850,](#bib.bib84)
    ; [higham_mary_2022,](#bib.bib105) ) 和随机化 ([murray2023randomized,](#bib.bib157)
    ) 的可能性是一个非常吸引人的特性。
- en: LAPACK, ScaLAPACK ([doi:10.1137/1037042,](#bib.bib72) ), Trilinos ([10.1145/1089014.1089021,](#bib.bib104)
    ) and PETSc ([petsc-web-page,](#bib.bib22) ) are important high-performance libraries
    that collect efficient software routines for linear algebra techniques. Recently,
    libraries for Sparse linear algebra algorithms have been introduced ([doi:10.1137/20M134914X,](#bib.bib63)
    ; [DAMBRA2023100463,](#bib.bib74) ), with the main objective of supporting machine
    learning ([10.14778/2994509.2994515,](#bib.bib75) ; [9355309,](#bib.bib88) ; [7324551,](#bib.bib166)
    ; [stylianou2023optimizing,](#bib.bib217) ) and DL models, with special attention
    to DNNs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LAPACK、ScaLAPACK ([doi:10.1137/1037042,](#bib.bib72) )、Trilinos ([10.1145/1089014.1089021,](#bib.bib104)
    ) 和 PETSc ([petsc-web-page,](#bib.bib22) ) 是重要的高性能库，它们收集了高效的线性代数技术软件例程。最近，引入了用于稀疏线性代数算法的库
    ([doi:10.1137/20M134914X,](#bib.bib63) ; [DAMBRA2023100463,](#bib.bib74) )，其主要目标是支持机器学习
    ([10.14778/2994509.2994515,](#bib.bib75) ; [9355309,](#bib.bib88) ; [7324551,](#bib.bib166)
    ; [stylianou2023optimizing,](#bib.bib217) ) 和深度学习模型，特别关注DNN。
- en: Convolutional layers in DNNs can be computed in many different ways. Typically,
    the convolution is implemented using a traditional sliding window approach across
    the activation data matrix, together with the application of a kernel function ([Sermanet-2014,](#bib.bib196)
    ). However, this type of computation is not efficient due to the irregularity
    of the data access pattern.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DNN中的卷积层可以通过多种方式计算。通常，卷积是通过在激活数据矩阵上使用传统的滑动窗口方法来实现的，同时应用一个内核函数 ([Sermanet-2014,](#bib.bib196)
    )。然而，这种计算方式由于数据访问模式的不规则性，效率较低。
- en: In order to reduce the number of floating point operations, Fast Fourier Transform
    (FFT)-based implementations  ([Vasilache-2015,](#bib.bib227) ; [Mathieu-2014,](#bib.bib145)
    ) can be exploited to compute convolution in the frequency domain as a Hadamard
    product (element-wise matrix multiplication), after Fourier transforming the activation
    data. Obtained results are then transformed back into the frequency domain through
    the inverse FFT. Even though FFT provides an asymptotically superior approach,
    it is often very inefficient and incurs a significant memory overhead, especially
    when the convolutional kernels are small ([Zhang-2018,](#bib.bib247) ).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少浮点运算的数量，可以利用基于快速傅里叶变换（FFT）的实现 ([Vasilache-2015,](#bib.bib227) ; [Mathieu-2014,](#bib.bib145)
    )，在对激活数据进行傅里叶变换后，在频域中将卷积计算为Hadamard积（逐元素矩阵乘法）。然后，通过逆FFT将获得的结果转换回频域。尽管FFT提供了渐近优越的方法，但它通常效率很低，并且会产生显著的内存开销，特别是当卷积核很小的时候 ([Zhang-2018,](#bib.bib247)
    )。
- en: In the case of small kernels (size $\leq 3$) and unitary stride, the Winograd
    minimal filter algorithm ([Winograd-1980,](#bib.bib233) ; [Abdelouahab-2018,](#bib.bib3)
    ) is particularly efficient. The Winograd convolution algorithm divides the output
    activation matrix into tiles and computes each tile as $A^{T}[(Gg)\odot(B^{T}d)]$,
    where $\odot$ denotes the Hadamard product, $g$ is the convolution kernel and
    $d$ is the input activation matrix. $A$, $B$, and $G$ are transformation matrices,
    which are constants for given tile and convolution kernel sizes. The Winograd
    convolution reduces the number of multiplications and, as the matrix multiplication
    of smaller transformed matrices has more independent workloads, increases the
    thread-level parallelism. However, this comes at the cost of extra floating-point
    additions and the extra global memory accesses that are needed to implement the
    matrices transformations. This process, for large convolution filters, may overwhelm
    the benefits of multiplication reduction ([Ji-2019,](#bib.bib116) ).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小卷积核（大小 $\leq 3$）和单位步幅的情况，Winograd最小滤波器算法 ([Winograd-1980,](#bib.bib233) ;
    [Abdelouahab-2018,](#bib.bib3) )特别高效。Winograd卷积算法将输出激活矩阵划分为块，并将每个块计算为$A^{T}[(Gg)\odot(B^{T}d)]$，其中$\odot$表示Hadamard积，$g$是卷积核，$d$是输入激活矩阵。$A$、$B$和$G$是变换矩阵，对于给定的块和卷积核大小是常数。Winograd卷积减少了乘法运算的数量，并且由于较小的变换矩阵的矩阵乘法具有更多的独立工作负载，增加了线程级并行性。然而，这也带来了额外的浮点加法和实现矩阵变换所需的额外全局内存访问的代价。对于大型卷积滤波器，这一过程可能会超过乘法减少的好处 ([Ji-2019,](#bib.bib116)
    )。
- en: Another common approach is to reshape and selectively duplicate parts of the
    original input activation data to create a lowered matrix ([Chellapilla-2006,](#bib.bib44)
    ; [Cho-2017,](#bib.bib53) ; [Anderson-2020,](#bib.bib19) ) that is then multiplied
    by a properly arranged matrix of kernel weights. This allows leveraging the highly
    optimized high-performance matrix-matrix multiplication routines available in
    Level 3 Basic Linear Algebra Subprogram (BLAS) libraries ([Dongarra-1990,](#bib.bib71)
    ). The image-to-column (im2col) algorithm ([Chellapilla-2006,](#bib.bib44) ) transforms
    the input activation matrix into a Toeplitz matrix by unrolling overlapping patches
    into columns, as schematized in Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2\. An
    overview of computational models for DL on HPC systems ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures"). In the dual approach,
    image-to-row (im2row), the lowered matrix is created by unrolling the patches
    into rows ([Anderson-2020,](#bib.bib19) ). Both methods require an additional
    memory space of size $(K\times K\times C_{I})\times(H_{O}\times W_{O})$ for storing
    the lowered input matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的方法是重塑并选择性地重复原始输入激活数据的部分，生成一个降低的矩阵 ([Chellapilla-2006,](#bib.bib44) ; [Cho-2017,](#bib.bib53)
    ; [Anderson-2020,](#bib.bib19) )，然后将其与适当排列的卷积核权重矩阵相乘。这允许利用在第3层基础线性代数子程序（BLAS）库中提供的高度优化的高性能矩阵-矩阵乘法例程 ([Dongarra-1990,](#bib.bib71)
    )。图像到列（im2col）算法 ([Chellapilla-2006,](#bib.bib44) )通过将重叠的补丁展开成列，将输入激活矩阵转换为Toeplitz矩阵，如图 [2(a)](#S2.F2.sf1
    "图2 ‣ 2\. 深度学习在高性能计算系统上的计算模型概述 ‣ 加速异构架构上深度学习的设计方法综述")所示。在双重方法中，图像到行（im2row），降低的矩阵是通过将补丁展开为行来创建的 ([Anderson-2020,](#bib.bib19)
    )。这两种方法都需要额外的内存空间，其大小为$(K\times K\times C_{I})\times(H_{O}\times W_{O})$，用于存储降低的输入矩阵。
- en: '![Refer to caption](img/9ccdeab89fc051682409f3ed03302b23.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9ccdeab89fc051682409f3ed03302b23.png)'
- en: (a)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6945bd63fa9b35714cd3c4cda0aa1f8d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6945bd63fa9b35714cd3c4cda0aa1f8d.png)'
- en: (b)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 2. Activation data packing: (a) im2col; (b) kn2row.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 激活数据打包：（a）im2col；（b）kn2row。
- en: In the Memory-efficient Convolution (MEC) algorithm ([Cho-2017,](#bib.bib53)
    ), $[H_{I}]\times[K]\times[C_{I}]$ submatrices are transformed into rows, thus
    leading to a lowered matrix of size $W_{O}\times H_{I}\times K\times C_{I}$, which
    is $K$ times smaller than that generated by the im2row algorithm. To compute the
    convolution, the weights matrix is multiplied by $H_{O}$ submatrices of size $[W_{O}]\times[K\times
    K\times C_{I}]$, obtained by shifting, over the lowered matrix, the submatrix
    to the right by $s\times K$. MEC intuitively eliminates the vertical redundancy
    of the im2row approach, while recovering the information by shifting the submatrix
    by a constant interval ([Cho-2017,](#bib.bib53) ).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存高效卷积（MEC）算法 ([Cho-2017,](#bib.bib53) ）中，$[H_{I}]\times[K]\times[C_{I}]$ 子矩阵被转换成行，从而得到一个大小为
    $W_{O}\times H_{I}\times K\times C_{I}$ 的降维矩阵，比 im2row 算法生成的矩阵小 $K$ 倍。为了计算卷积，权重矩阵与
    $[W_{O}]\times[K\times K\times C_{I}]$ 大小的 $H_{O}$ 个子矩阵相乘，这些子矩阵是通过将降维矩阵上的子矩阵向右移动
    $s\times K$ 得到的。MEC 直观地消除了 im2row 方法的垂直冗余，同时通过以常量间隔移动子矩阵来恢复信息 ([Cho-2017,](#bib.bib53)
    )。
- en: While enjoying the speed-up given in the execution by the use of architecture-optimized
    routines, these approaches suffer from the time penalty of implementing the bandwidth-bounded
    packing of the input matrix, as well as from the mismatch between the sizes of
    the matrices used in the calculation of the convolution and those for which traditional
    high-performance systems are optimized. Moreover, additional memory space is needed
    for storing the lowered matrices. In contrast to the previous approaches, Direct
    methods do not pack the matrices before the computation. Rather, the kernel-to-row
    (kn2row) algorithm  ([Anderson-2020,](#bib.bib19) ) avoids data replication in
    the input and, as shown in Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2\. An overview
    of computational models for DL on HPC systems ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures"), the convolution
    is computed as the sum of $K\times K$ separate $1\times 1$ convolutions, thus
    increasing the size of the output. Indeed, each $1\times 1$ convolution is calculated
    by considering only one of the $K\times K$ kernel components at the time and multiplying
    it by the input activation matrix, therefore by performing a matrix-matrix multiplication
    between the corresponding $[C_{O}]\times[C_{I}]$ weight matrix and the $[C_{I}]\times[H_{I}\times
    W_{I}]$ activation matrix. All the $K\times K$ separate $1\times 1$ convolutions
    can be computed using a single matrix multiplication by reordering the filter
    matrix, i.e. by laying out contiguously the $C_{O}$ channel data. The consequent
    multiplication between a $[K\times K\times C_{O}]\times[C_{I}]$ weight matrix
    and a $[C_{I}]\times[H_{I}\times W_{I}]$ activation matrix furnishes the resulting
    matrix of size $[K\times K\times C_{O}]\times[H_{I}\times W_{I}]$, which is stored
    in memory at the end of the multiplication.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过使用架构优化的例程在执行时享受到了加速，但这些方法仍然受到输入矩阵带宽受限的打包时间惩罚，并且计算卷积时使用的矩阵大小与传统高性能系统优化的矩阵大小不匹配。此外，还需要额外的内存空间来存储降维后的矩阵。与之前的方法不同，直接方法在计算前不对矩阵进行打包。相反，内核到行（kn2row）算法 ([Anderson-2020,](#bib.bib19)
    ) 避免了输入数据的重复，如图 [2(b)](#S2.F2.sf2 "在图 2 ‣ 2\. DL 在 HPC 系统上的计算模型概述 ‣ 加速深度学习的设计方法调研")
    所示，卷积被计算为 $K\times K$ 个单独的 $1\times 1$ 卷积的总和，从而增加了输出的大小。实际上，每个 $1\times 1$ 卷积通过仅考虑
    $K\times K$ 核组件中的一个，并将其与输入激活矩阵相乘来计算，因此执行的是 $[C_{O}]\times[C_{I}]$ 权重矩阵和 $[C_{I}]\times[H_{I}\times
    W_{I}]$ 激活矩阵之间的矩阵乘法。所有 $K\times K$ 个单独的 $1\times 1$ 卷积可以通过重新排序滤波器矩阵来使用单次矩阵乘法计算，即将
    $C_{O}$ 通道数据连续排列。结果是 $[K\times K\times C_{O}]\times[C_{I}]$ 权重矩阵和 $[C_{I}]\times[H_{I}\times
    W_{I}]$ 激活矩阵之间的乘法提供了大小为 $[K\times K\times C_{O}]\times[H_{I}\times W_{I}]$ 的结果矩阵，该矩阵在乘法结束时存储在内存中。
- en: In order to obtain the desired $[C_{O}]\times[H_{O}\times W_{O}]$ output activation
    matrix, the results of the $K\times K$ separate $1\times 1$ convolutions are added
    together by appropriately shifting the data vertically and/or horizontally, depending
    on the position of the relative weight with respect to the central kernel weight
    (i.e., top, bottom, left, right, and diagonal positions). Some of the results
    of the intermediate $1\times 1$ convolutions are outside the boundaries of the
    final result matrix and they are discarded during the final sum. Filter weights
    are arranged in the desired position ahead of time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得所需的 $[C_{O}]\times[H_{O}\times W_{O}]$ 输出激活矩阵，$K\times K$ 单独的 $1\times 1$
    卷积的结果通过适当的垂直和/或水平移位将数据加在一起，具体取决于相对权重相对于中心核权重的位置（即，上、下、左、右和对角位置）。一些 $1\times 1$
    卷积的中间结果超出了最终结果矩阵的边界，并在最终求和过程中被丢弃。滤波器权重提前安排到所需位置。
- en: In ([Anderson-2020,](#bib.bib19) ), the kn2row approach is modified by performing
    the shift-add operation at the end of the calculation of each separate $1\times
    1$ convolution. The needed temporary storage is reduced to $[2\delta+C_{O}]\times[H_{I}\times
    W_{I}]$, where $\delta$ is the number of extra rows in the result matrix needed
    to support the shifting of the result data. By swapping the dimensions of the
    filter and of the input activation matrices to make $C_{I}$ the innermost dimension,
    it is possible to obtain the dual methods (kernel-to-col).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([Anderson-2020](#bib.bib19)) 中，通过在每个单独的 $1\times 1$ 卷积计算结束时执行移位加操作，修改了 kn2row
    方法。所需的临时存储减少为 $[2\delta+C_{O}]\times[H_{I}\times W_{I}]$，其中 $\delta$ 是结果矩阵中支持结果数据移位所需的额外行数。通过交换滤波器和输入激活矩阵的维度，使
    $C_{I}$ 成为最内层维度，可以获得双重方法（kernel-to-col）。
- en: The High Performance Zero-Memory Overhead (HPZMO) direct convolutions approach ([Zhang-2018,](#bib.bib247)
    ) rearranges and optimizes the naive convolutional algorithm to enable its execution
    over multi-threaded Single-Instruction Multiple-Data (SIMD) architectures and
    an output-tiled approach, in which the partial results of the convolution of $W_{o,b}$
    elements are accumulated into the register file. As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 2\. An overview of computational models for DL on HPC systems ‣
    A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures"), the HPZMO approach relies on the vector units of the computing
    architecture. Indeed, it reads the activation data and the filter weights directly
    from memory, without requiring additional input/output memory space, and parallelizes
    the output channel ($C_{O}$) dimension, which allows sharing of input data among
    threads/PEs for calculating different sets of output channels. Both input and
    output activation data, as well as the filter weights, are organized in the channel-last
    structure, and into blocks of $H\times W\times C_{b}$, where $C_{b}$ is a multiple
    of the SIMD vector length.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能零内存开销（HPZMO）直接卷积方法（[Zhang-2018](#bib.bib247)）通过重新排列和优化简单的卷积算法，使其能够在多线程单指令多数据（SIMD）架构和输出平铺方法上执行，其中卷积的
    $W_{o,b}$ 元素的部分结果累积到寄存器文件中。如算法 [1](#alg1 "Algorithm 1 ‣ 2\. An overview of computational
    models for DL on HPC systems ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") 所示，HPZMO 方法依赖于计算架构的向量单元。实际上，它直接从内存中读取激活数据和滤波器权重，无需额外的输入/输出内存空间，并且并行化输出通道（$C_{O}$）维度，这允许线程/PEs
    之间共享输入数据以计算不同的输出通道集合。输入和输出激活数据以及滤波器权重都组织成通道最后结构，并分块为 $H\times W\times C_{b}$，其中
    $C_{b}$ 是 SIMD 向量长度的倍数。
- en: Algorithm 1 Parallelized Direct Convolution Algorithm - HPZMO
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 并行化直接卷积算法 - HPZMO
- en: Activation I, Filter weights, Stride s = 1;Activation O;for $i\leftarrow 1$
    to $C_{O}/C_{O,b}$  in Parallel do    for $k\leftarrow 1$ to $H_{O}$ do       for $l\leftarrow
    1$ to $W_{O}/W_{O,b}$ do          for $m\leftarrow 1$ to $H_{K}$ do             for $n\leftarrow
    1$ to $W_{K}$ do                for $j\leftarrow 1$ to $C_{I}$ do                   for $ll\leftarrow
    1$ to $W_{O,b}$ do                      for $ii\leftarrow 1$ to $C_{O,b}$ do                          $O_{i\cdot
    C_{O,b}+ii,l\cdot W_{O,b}+ll,k}\mathrel{+}=$  $I_{j,l\cdot W_{O,b}+ll+n,k+m}\times$  $F_{j,i\cdot
    C_{O,b}+ii,n,m}$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 激活 I, 过滤器权重, 步幅 s = 1; 激活 O; 对于 $i\leftarrow 1$ 到 $C_{O}/C_{O,b}$ 并行执行 do     对于
    $k\leftarrow 1$ 到 $H_{O}$ do       对于 $l\leftarrow 1$ 到 $W_{O}/W_{O,b}$ do         对于
    $m\leftarrow 1$ 到 $H_{K}$ do             对于 $n\leftarrow 1$ 到 $W_{K}$ do                 对于
    $j\leftarrow 1$ 到 $C_{I}$ do                     对于 $ll\leftarrow 1$ 到 $W_{O,b}$
    do                         对于 $ii\leftarrow 1$ 到 $C_{O,b}$ do                             $O_{i\cdot
    C_{O,b}+ii,l\cdot W_{O,b}+ll,k}\mathrel{+}=$  $I_{j,l\cdot W_{O,b}+ll+n,k+m}\times$  $F_{j,i\cdot
    C_{O,b}+ii,n,m}$
- en: 3\. Hardware/Software Codesign Methodologies
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 硬件/软件协同设计方法
- en: In this Section, we review some approaches and tools proposed in the literature
    to distribute, partition, and map DL training and inference applications on the
    processing nodes in the underlying computing infrastructure. First, we briefly
    analyze how DNN models can be optimized for execution on a plethora of hardware
    devices. Then, we focus on the approaches for training DL models in the context
    of distributed computing infrastructures. Figure [3](#S3.F3 "Figure 3 ‣ 3\. Hardware/Software
    Codesign Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") shows the main references to methodologies,
    frameworks, and tools that we discuss in this Section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了文献中提出的一些方法和工具，这些方法和工具用于在底层计算基础设施中的处理节点上分布、分区和映射深度学习训练和推理应用。首先，我们简要分析了如何优化DNN模型以在大量硬件设备上执行。然后，我们重点关注在分布式计算基础设施环境下训练深度学习模型的方法。图[3](#S3.F3
    "图 3 ‣ 3\. 硬件/软件协同设计方法 ‣ 加速异构架构上深度学习的设计方法综述")显示了我们在本节中讨论的主要方法、框架和工具的参考。
- en: '{forest}'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Application
    partitioning and mapping [Hardware-based search ], [Data parallelism , ], [Model
    parallelism , ], [Pipeline parallelism [ GPipe ([YY19,](#bib.bib107) )
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [应用分区和映射
    [基于硬件的搜索], [数据并行,], [模型并行,], [流水线并行 [ GPipe ([YY19,](#bib.bib107) )
- en: PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155) ) ] ],
    [Hybrid solutions [ Layer-wise parallelism ([JQLA18,](#bib.bib117) )
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155) ) ] ],
    [混合解决方案 [ 层级并行 ([JQLA18,](#bib.bib117) )
- en: DAPPLE ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168) ) ] ]
    ]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DAPPLE ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168) ) ] ]
    ]
- en: Figure 3. Application partitioning and mapping discussed in Section [3](#S3
    "3\. Hardware/Software Codesign Methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures")
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 应用分区和映射见第[3](#S3 "3\. 硬件/软件协同设计方法 ‣ 加速异构架构上深度学习的设计方法综述")节
- en: 'Training large DL models with vast amounts of data and serving them for inference
    is a non-trivial task. Today, it is often performed in a distributed infrastructure
    composed of multiple, possibly heterogeneous compute nodes. The complexity is
    further exacerbated by the recent trend to integrate the high-performing computing
    and storage equipment in the cloud and HPC data centers with the constrained resources
    provided by devices at the edges of the network. The goals of this compute continuum
    trend are to achieve better privacy, higher autonomy, and energy efficiency as
    well as to reduce response latency, cost, and bandwidth demand to the cloud ([Casamayor:2023,](#bib.bib43)
    ). In this complex and heterogeneous setting, designers need to optimize the complete
    system stack: from ML/DNN algorithms, to model optimization and compression, implementation
    of algorithms onto the hardware platforms enriched with DL accelerators as well
    as the underlying hardware architecture design ([han2016deep,](#bib.bib98) ; [jacob2018quantization,](#bib.bib115)
    ; [jin2019towards,](#bib.bib121) ; [tung2018clip,](#bib.bib219) ).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型深度学习模型需要大量的数据，并对其进行推理服务是一项复杂的任务。如今，这通常在由多个可能异构的计算节点组成的分布式基础设施中进行。最近将高性能计算和存储设备集成到云端和高性能计算数据中心，以及将网络边缘设备的受限资源与之结合的趋势，使得复杂性进一步加剧。该计算连续性趋势的目标是实现更好的隐私、更高的自主性和能源效率，同时减少响应延迟、成本和对云端带宽的需求 ([Casamayor:2023,](#bib.bib43)
    )。在这种复杂且异构的环境中，设计者需要优化整个系统堆栈：从机器学习/深度神经网络（ML/DNN）算法，到模型优化和压缩，再到将算法实现到配备深度学习加速器的硬件平台以及底层硬件架构设计 ([han2016deep,](#bib.bib98)
    ; [jacob2018quantization,](#bib.bib115) ; [jin2019towards,](#bib.bib121) ; [tung2018clip,](#bib.bib219)
    )。
- en: The hardware-aware design of DNNs has recently received increasing attention
    to tackle hardware device heterogeneity, especially for DNN inference. Indeed,
    as discussed in Section [4](#S4 "4\. Approximate Computing Methodologies ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    to deploy computationally demanding DNNs for model inference in power- and resource-constrained
    edge systems while maintaining acceptable performance, designers have to trade
    off model accuracy against energy and implementation efficiency. However, the
    plethora of hardware devices available makes it very difficult to choose one solution
    for all cases. Therefore, in addition to techniques for approximate computing,
    model compression, such as quantization-aware training and pruning (e.g., ([jacob2018quantization,](#bib.bib115)
    ; [tung2018clip,](#bib.bib219) )), hardware-aware neural architecture search ([chitty-venkata2023,](#bib.bib51)
    ), which takes hardware characteristics like latency, power, or area into account,
    has become a central aspect in automating the process of designing efficient architectures
    for DNN applications executed at the network edges. Different methodologies have
    been exploited to search for the optimal performing model architecture, ranging
    from reinforcement learning to evolutionary algorithms. For example, in ([abdelfattah2020best,](#bib.bib2)
    ; [jiang2020hardware,](#bib.bib120) ), reinforcement learning-based neural architecture
    search is extended to include a search for an accelerator configuration on FPGAs
    and optimize it for latency and area.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）的硬件感知设计最近受到越来越多的关注，以应对硬件设备的异质性，特别是在DNN推理方面。确实，正如在第[4](#S4 "4\. Approximate
    Computing Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures")节中讨论的，为了在功率和资源受限的边缘系统中部署计算密集型DNN模型进行推理，同时保持可接受的性能，设计者必须在模型准确性与能源和实现效率之间做出权衡。然而，众多的硬件设备使得选择一种适用于所有情况的解决方案变得非常困难。因此，除了近似计算和模型压缩技术，如量化感知训练和剪枝（例如，([jacob2018quantization,](#bib.bib115)
    ; [tung2018clip,](#bib.bib219) )），硬件感知神经架构搜索 ([chitty-venkata2023,](#bib.bib51)
    )，它考虑了延迟、功耗或面积等硬件特性，已成为自动化设计高效DNN应用架构的关键方面。这些方法包括从强化学习到进化算法等各种技术。例如，在 ([abdelfattah2020best,](#bib.bib2)
    ; [jiang2020hardware,](#bib.bib120) )中，基于强化学习的神经架构搜索被扩展到包括对FPGA的加速器配置进行搜索并优化其延迟和面积。
- en: Nevertheless, in the context of a distributed infrastructure with an ever-increasing
    number of available nodes and resources, it is parallelization that appears to
    offer the solution for the ever-growing need to accelerate the training of DNN
    applications. DNN models lead themselves with many possibilities for parallelization,
    namely data, model, pipeline, and hybrid parallelism.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在具有不断增加的可用节点和资源的分布式基础设施中，**并行化**似乎提供了加速深度神经网络（DNN）应用训练需求的解决方案。DNN模型自身支持多种并行化方式，即数据、模型、管道和混合并行。
- en: In data parallelism, a number of workers (machines or devices, e.g., GPUs) load
    an identical copy of the DL model. The training data is split into non-overlapping
    portions and fed into the model replicas of the workers for training ([krizhevsky2012,](#bib.bib131)
    ). Each worker performs the training on its portion of training data, which leads
    to updates of the model parameters. Hence, the parameters of the model among the
    workers need to be synchronized. The main advantage of data parallelism is that
    it is applicable to any DL model architecture without further domain knowledge
    of the model. It scales well for operations that are computationally intensive
    but have only a few parameters, such as CNNs. However, data parallelism is limited
    to operations that have many parameters, as the parameter synchronization leads
    to a significant communication overhead and may become the bottleneck ([JQLA18,](#bib.bib117)
    ). To address such scalability and single point of failure bottleneck, the parameters
    synchronization can occur in a decentralized manner ([Mayer2020,](#bib.bib146)
    ), with the main disadvantage of increasing the communication cost among workers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行中，多个工作者（机器或设备，例如GPU）加载相同的深度学习模型副本。训练数据被拆分成不重叠的部分，并输入到工作者的模型副本中进行训练 ([krizhevsky2012,](#bib.bib131)
    )。每个工作者在其数据部分上执行训练，从而更新模型参数。因此，工作者之间的模型参数需要同步。数据并行的主要优势在于它适用于任何深度学习模型架构，而无需进一步的模型领域知识。它对计算密集型但参数较少的操作（如CNN）扩展良好。然而，数据并行对参数较多的操作有局限，因为参数同步会导致显著的通信开销，可能成为瓶颈 ([JQLA18,](#bib.bib117)
    )。为了解决这种扩展性和单点故障瓶颈，参数同步可以以去中心化的方式进行 ([Mayer2020,](#bib.bib146) )，主要缺点是增加了工作者之间的通信成本。
- en: On the contrary, in model parallelism, the DL model is partitioned into multiple
    parts and each worker loads a different part of the ML/DNN model for training.
    A major challenge of model parallelism is how to split the model into partitions
    that are assigned to the parallel workers ([MM17,](#bib.bib147) ). In the context
    of ML/DNN workloads, model partitioning across different devices has initially
    mostly been a manual process driven by human experts. A common approach for finding
    a good model splitting is to use reinforcement learning ([MPL+17,](#bib.bib153)
    ; [MGP+18,](#bib.bib152) ). Starting from some initial partitioning, permutations
    on that partitioning are performed, and performance is measured (e.g., for one
    training iteration) or learn a placement policy that can then be adjusted for
    new workloads via transfer learning, see e.g., ([ABVG+19,](#bib.bib5) ; [ZRA+19,](#bib.bib250)
    ) or used to bootstrap a genetic algorithm ([PGN+20,](#bib.bib169) ). Unfortunately,
    these methods are computationally expensive, as they need to evaluate large numbers
    of placements and measure the runtime of several inference/training steps. Alternatively,
    the problem is cast into an offline optimization problem of finding good partitions
    and schedules. This includes classic results in scheduling on multiple machines
    and/or devices ([LLKS93,](#bib.bib140) ; [Gra66,](#bib.bib96) ; [KL70,](#bib.bib128)
    ; [PY90,](#bib.bib170) ; [SW99,](#bib.bib206) ; [ST93,](#bib.bib201) ), as well
    as modern DNN scheduling works ([JQLA18,](#bib.bib117) ; [NHP+19,](#bib.bib163)
    ). Such algorithms use profiled compute time of each node (layer or operator)
    and data-transfer requirements between nodes in a graph, and the target deployment
    system infrastructure such as machine and network properties (e.g., measured bandwidths).
    However, such techniques do not evaluate the performance of splits in an online
    fashion. Nevertheless, it has been demonstrated that for well-defined cost models,
    the objective function closely matches real performance (see, e.g., ([NHP+19,](#bib.bib163)
    ; [JZA19,](#bib.bib118) )).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在模型并行中，深度学习模型被划分成多个部分，每个工作节点加载机器学习/深度神经网络模型的不同部分进行训练。模型并行的一大挑战是如何将模型划分为分配给并行工作节点的多个部分（[MM17,](#bib.bib147)）。在机器学习/深度神经网络任务的背景下，模型划分到不同设备上的过程最初主要由人工专家进行。寻找良好模型分割的常见方法是使用强化学习（[MPL+17,](#bib.bib153)；[MGP+18,](#bib.bib152)）。从某些初始划分开始，对该划分进行排列组合，并测量性能（例如，对于一次训练迭代），或者学习一个可以通过迁移学习调整的新工作负载的放置策略，参见例如（[ABVG+19,](#bib.bib5)；[ZRA+19,](#bib.bib250)），或者用于引导遗传算法（[PGN+20,](#bib.bib169)）。不幸的是，这些方法计算开销大，因为它们需要评估大量的放置并测量多个推理/训练步骤的运行时间。另一种方法是将问题转化为离线优化问题，即寻找良好的划分和调度。这包括多台机器和/或设备上的经典调度结果（[LLKS93,](#bib.bib140)；[Gra66,](#bib.bib96)；[KL70,](#bib.bib128)；[PY90,](#bib.bib170)；[SW99,](#bib.bib206)；[ST93,](#bib.bib201)），以及现代深度神经网络调度工作（[JQLA18,](#bib.bib117)；[NHP+19,](#bib.bib163)）。这些算法使用图中每个节点（层或运算符）的计算时间概况和节点之间的数据传输要求，以及目标部署系统基础设施，如机器和网络属性（例如，测量带宽）。然而，这些技术并没有以在线方式评估分割的性能。尽管如此，已证明对于定义良好的成本模型，目标函数与实际性能密切匹配（参见，例如，（[NHP+19,](#bib.bib163)；[JZA19,](#bib.bib118)））。
- en: 'Pipeline parallelism combines model parallelism with data parallelism. In pipeline
    parallelism, the model is split and each worker loads a different part of the
    DL model for training. Recent approaches that support pipeline parallelism include
    GPipe ([YY19,](#bib.bib107) ) and PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155)
    ). Specifically, in pipeline parallelism the model is divided among available
    workers, assigning a group of consecutive operators (called layers in DNN terminology)
    in the operator graph to each of them, and then overlapping the computation and
    communication of different inputs in a pipelined fashion. This process can greatly
    reduce inter-worker communication. While pipelining is a simple and widely adopted
    idea, DNN training poses an important challenge not present in traditional pipelining:
    DNN training is bi-directional, being the forward pass followed by a backward
    pass through the same layers in reverse order, using state and intermediate results
    from the forward pass. This results in low hardware efficiency or low statistical
    efficiency unless resorting to parallelization optimization ([NHP+19,](#bib.bib163)
    ).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线并行将模型并行与数据并行结合。在流水线并行中，模型被拆分，每个工作者加载 DL 模型的不同部分进行训练。支持流水线并行的近期方法包括 GPipe
    ([YY19,](#bib.bib107)) 和 PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155))。具体而言，在流水线并行中，模型在可用的工作者之间进行划分，将操作图中的一组连续操作（在
    DNN 术语中称为层）分配给每个工作者，然后以流水线方式重叠不同输入的计算和通信。这一过程可以大大减少工作者之间的通信。尽管流水线是一种简单且被广泛采用的想法，但
    DNN 训练提出了传统流水线没有的重要挑战：DNN 训练是双向的，即前向传递后跟随通过相同层的反向传递，使用前向传递的状态和中间结果。这导致了硬件效率低或统计效率低，除非借助并行化优化
    ([NHP+19,](#bib.bib163))。
- en: Proposals related to pipeline training can be classified according to the temporal
    aspect, by differentiating between synchronous vs. asynchronous training. The
    first requires executing gradient synchronizations between adjacent training iterations
    to ensure convergence ([YY19,](#bib.bib107) ). However, it suffers from a significant
    memory consumption, that can be partially mitigated by re-computation. Asynchronous
    training inserts micro-batches into the pipeline concurrently to achieve maximum
    throughput ([NHP+19,](#bib.bib163) ). However, storing multiple versions of model
    parameters is not a common practice due to convergence concerns and increased
    memory demand.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与流水线训练相关的提案可以根据时间方面进行分类，区分同步训练与异步训练。同步训练需要在相邻训练迭代之间执行梯度同步，以确保收敛 ([YY19,](#bib.bib107))。然而，它面临着显著的内存消耗，这可以通过重新计算部分缓解。异步训练则在流水线中并发插入微批次，以实现最大吞吐量
    ([NHP+19,](#bib.bib163))。然而，由于收敛问题和内存需求增加，存储模型参数的多个版本并不是一种常见的做法。
- en: A few frameworks attempt to find a hybrid solution that instead combines some
    of the best properties of each model of parallelism and diminishes some of the
    drawbacks. For example, layer-wise parallelism ([JQLA18,](#bib.bib117) ) proposes
    to apply different parallelization strategies to each individual layer of the
    neural network rather than the same parallelization strategy (i.e., data or model
    parallelism) to all layers. The solution for finding the optimal parallelization
    strategy for each layer is based on a dynamic programming-based graph search algorithm.
    DAPPLE ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168) ) is a synchronous
    training framework that combines data parallelism and pipeline parallelism for
    large DNN models to ensure training convergence and reduce memory consumption.
    To this end, it schedules tasks backward as early as possible to release the memory
    occupied by activations produced by corresponding forward tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一些框架尝试寻找一种混合解决方案，结合每种并行模型的最佳属性，并减少一些缺点。例如，逐层并行主义 ([JQLA18,](#bib.bib117)) 提出对神经网络的每一层应用不同的并行策略，而不是对所有层应用相同的并行策略（即数据并行或模型并行）。为每一层找到最佳并行策略的解决方案基于一种动态编程的图搜索算法。DAPPLE
    ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168)) 是一种同步训练框架，结合了数据并行和流水线并行，用于大型
    DNN 模型，以确保训练收敛并减少内存消耗。为此，它尽可能早地反向调度任务，以释放由相应前向任务产生的激活所占用的内存。
- en: The approaches to distribute DL training and inference that we reviewed above
    aim typically to speed up the performance, for example by achieving better throughput
    and scalability and by reducing communication costs, while improving (or at least
    without deteriorating) model accuracy. In recent years, following a general trend
    within the industry at large, the reduction of carbon emission, the so-called
    green carbon footprint, has started to receive increasing attention also within
    the HPC and ML/DNN communities in order to realize environmentally-responsible
    solutions, e.g., ([wu2022sustainableai,](#bib.bib235) ). Given the high computational
    demand of DL training and inference jobs, there is a large opportunity for energy
    saving. For instance, it is possible to save energy while maintaining an adequate
    level of accuracy at the software level by trading off model variants, i.e., low
    and high-quality models. At the hardware level, multiple solutions can be exploited,
    ranging from the adoption of energy-efficient FPGAs to novel GPU partitioning
    schemes, that can reduce energy consumption by allowing GPU sharing ([li2023green,](#bib.bib141)
    ). Coupling with proper distributed resources scheduling, there is therefore a
    large opportunity for improving performance while reducing cost and carbon emission.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上述审查的分布式 DL 训练和推理方法通常旨在提高性能，例如通过实现更好的吞吐量和可扩展性，减少通信成本，同时改善（或至少不恶化）模型准确性。近年来，随着整个行业普遍趋势的推进，减少碳排放，即所谓的绿色碳足迹，也开始在
    HPC 和 ML/DNN 社区中受到越来越多的关注，以实现环境友好的解决方案，例如 ([wu2022sustainableai,](#bib.bib235)
    )。鉴于 DL 训练和推理工作的高计算需求，节能的机会很大。例如，通过权衡模型变体，即低质量和高质量模型，可以在保持足够准确性水平的同时节省能量。在硬件层面，可以利用多种解决方案，从采用节能的
    FPGA 到新型 GPU 划分方案，这些方案可以通过允许 GPU 共享来减少能耗 ([li2023green,](#bib.bib141) )。因此，配合适当的分布式资源调度，有很大的机会在减少成本和碳排放的同时提高性能。
- en: Within this context, DL application partitioning and mapping strategies are
    crucial to designing autonomic strategies optimized for both the training and
    inference phases, which account for different non-functional requirements such
    as performance, energy consumption, accuracy, and heterogeneity. Among the existing
    methodologies to develop hardware/software platforms, reinforcement learning is
    widely used  ([jiang2020hardware,](#bib.bib120) ; [MPL+17,](#bib.bib153) ; [russorusso2023,](#bib.bib189)
    ) to account for the large state space that characterizes the edge scenarios,
    whereby multiple nodes, possibly characterized by their own processing, memory,
    networking capabilities, and energy footprint are pooled to train and serve ML/DNN
    models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，DL 应用程序的划分和映射策略对设计优化的自主策略至关重要，这些策略针对训练和推理阶段的不同非功能性需求，如性能、能耗、准确性和异质性。在现有的硬件/软件平台开发方法中，强化学习被广泛使用
    ([jiang2020hardware,](#bib.bib120) ; [MPL+17,](#bib.bib153) ; [russorusso2023,](#bib.bib189)
    )，以应对边缘场景中大状态空间的特点，其中多个节点，可能具备各自的处理、内存、网络能力和能耗足迹，被汇集起来以训练和服务 ML/DNN 模型。
- en: 4\. Approximate Computing Methodologies
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 近似计算方法
- en: In the last years, the design of efficient hardware architectures suitable to
    DL applications has received a great deal of attention and several implementations
    have been proposed for both ASIC and FPGA-based platforms. Special attention was
    paid to design methodologies suitable to achieve efficient computational units
    to be integrated as IP blocks within acceleration architectures for DNNs and HPC
    workloads. At first glance, to comply with the computational complexity of HPC
    applications, with special attention to DL and in particular to DNNs, hardware
    designers focused their efforts mainly on solutions able to reach very high-speed
    performances. In order to comply with this objective, even-increasing levels of
    parallelism have been introduced and increasingly complex computational architectures
    have been realized. Unfortunately, this trend poses several concerns in terms
    of power consumption and costs, especially in power- and resource-constrained
    edge systems. For this reason, more recently, unconventional design methodologies
    have emerged to trade off computational speed with power consumption, while keeping
    cost and resource utilization under control. Among these emerging approaches,
    approximate computing has gained popularity as a powerful technique to reduce
    energy consumption and computational delay in error-resilient applications, such
    as multimedia processing, DL, digital signal processing, and wireless communications
    ([Alioto_2017,](#bib.bib15) ; [Alioto_2018,](#bib.bib16) ). Indeed, hardware accelerators
    designed by exploiting approximate computing methodologies can be integrated as
    custom IPs to provide an overall system with high speed and energy efficiency.
    Computational IPs implementing approximate computing approaches at both algorithmic
    and architecture levels can be exploited in both ASIC and FPGA-based HLS designs,
    with the objectives of reducing, on the one hand, the computational complexity
    of layers typically employed within DL models, and, on the other hand, to optimize
    speed and power consumption introducing a reasonable accuracy loss.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，适合深度学习应用的高效硬件架构设计受到了极大的关注，并且已为ASIC和FPGA平台提出了几种实现方案。特别关注于设计方法论，旨在实现高效的计算单元，以便将其集成作为加速架构中的IP块，用于深度神经网络（DNNs）和高性能计算（HPC）负载。乍一看，为了满足HPC应用的计算复杂性，特别是深度学习（DL）及特别是DNNs，硬件设计师主要将精力集中在能够实现非常高速性能的解决方案上。为了实现这一目标，引入了越来越高的并行级别，并实现了越来越复杂的计算架构。不幸的是，这种趋势在功耗和成本方面带来了若干问题，特别是在功耗和资源受限的边缘系统中。因此，最近，出现了非传统的设计方法，以权衡计算速度与功耗，同时保持成本和资源利用的控制。在这些新兴方法中，近似计算作为一种有效的技术，已经获得了广泛关注，它可以减少能源消耗和计算延迟，适用于容错应用，如多媒体处理、深度学习、数字信号处理和无线通信（[Alioto_2017,](#bib.bib15)
    ; [Alioto_2018,](#bib.bib16)）。实际上，利用近似计算方法设计的硬件加速器可以作为定制IP集成，以提供高速度和能源效率的整体系统。实现近似计算方法的计算IP可以在ASIC和FPGA基础的HLS设计中得到利用，旨在一方面减少深度学习模型中通常使用的层的计算复杂性，另一方面通过引入合理的准确度损失来优化速度和功耗。
- en: Even though the basic principle of the approximate computing paradigm is very
    simple, i.e. by relaxing the requirement of an exact computation, it is possible
    to trade off the quality of the computation result for speed performances and
    energy dissipation, achieving the expected benefits is not trivial.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近似计算范式的基本原理非常简单，即通过放宽对精确计算的要求，可以在速度性能和能量消耗之间进行权衡，但实现预期的好处并非易事。
- en: '{forest}'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Approximate
    computing approaches [Algorithmic level [Approximate SoftMax ([Zhu_2020,](#bib.bib251)
    ; [Cardarilli_2021,](#bib.bib38) ; [Spagnolo_2022,](#bib.bib211) )
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [近似计算方法 [算法级别
    [近似SoftMax（[Zhu_2020,](#bib.bib251) ; [Cardarilli_2021,](#bib.bib38) ; [Spagnolo_2022,](#bib.bib211)）
- en: Approximate sensors, memory,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 近似传感器，存储器，
- en: compute and communication ([Ghosh_2020,](#bib.bib93) )
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和通信（[Ghosh_2020,](#bib.bib93)）
- en: Approximate Pooling layers ([Sayal_2021,](#bib.bib194) ; [Spagnolo_2022_2,](#bib.bib212)
    )
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 近似池化层（[Sayal_2021,](#bib.bib194) ; [Spagnolo_2022_2,](#bib.bib212)）
- en: 'Super-Resolution ([Spagnolo_2023,](#bib.bib209) ), Denoising ([Spagnolo_2023_2,](#bib.bib210)
    ) ] ], [Architecture, Gate- and transistor-level [ASIC-based [Encoded: yes ([Chen_2012,](#bib.bib49)
    ; [Cho_2004,](#bib.bib52) ; [Song_2007,](#bib.bib207) ), no ([Esposito_2017,](#bib.bib77)
    ; [Farshchi_2013,](#bib.bib80) ; [Strollo_2022,](#bib.bib216) )'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率 ([Spagnolo_2023,](#bib.bib209) ), 去噪 ([Spagnolo_2023_2,](#bib.bib210) )
    ] ], [架构，门级和晶体管级 [基于 ASIC 的 [已编码：是 ([Chen_2012,](#bib.bib49) ; [Cho_2004,](#bib.bib52)
    ; [Song_2007,](#bib.bib207) ), 否 ([Esposito_2017,](#bib.bib77) ; [Farshchi_2013,](#bib.bib80)
    ; [Strollo_2022,](#bib.bib216) )
- en: Dynamic ([Frustaci_2020,](#bib.bib87) ), Approx. compressors ([Esposito_2018,](#bib.bib78)
    ; [Strollo_2020,](#bib.bib215) )
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 动态 ([Frustaci_2020,](#bib.bib87) ), 近似压缩器 ([Esposito_2018,](#bib.bib78) ; [Strollo_2020,](#bib.bib215)
    )
- en: Segmentation ([Strollo_2022,](#bib.bib216) ), Approximate MACs ([Kim_2021,](#bib.bib129)
    ) ], ], [FPGA-based [Adders ([Perri_2020,](#bib.bib175) )
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分割 ([Strollo_2022,](#bib.bib216) ), 近似 MACs ([Kim_2021,](#bib.bib129) ) ], ],
    [基于 FPGA 的 [加法器 ([Perri_2020,](#bib.bib175) )
- en: Multipliers ([Ullah_2021,](#bib.bib221) ; [Ullah_2022,](#bib.bib220) ; [Perri_2022,](#bib.bib176)
    ; [Waris_2021,](#bib.bib232) )
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法器 ([Ullah_2021,](#bib.bib221) ; [Ullah_2022,](#bib.bib220) ; [Perri_2022,](#bib.bib176)
    ; [Waris_2021,](#bib.bib232) )
- en: '] ] ] ]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] ] ]'
- en: Figure 4. Taxonomy of the approximate computing approaches
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 近似计算方法的分类
- en: 'Approximate computing offers several opportunities to design efficient hardware
    accelerators for DL. In fact, as summarized in Figure [4](#S4.F4 "Figure 4 ‣ 4\.
    Approximate Computing Methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures"), it can be exploited at different
    design levels: starting from the algorithm, passing through the architecture,
    up to the gate- and the transistor-level circuit topologies ([Jiang_2020,](#bib.bib119)
    ). Several approximation strategies furnish hardware-oriented solutions to be
    adopted at the algorithmic level to significantly reduce the complexity and energy
    consumption of critical layers employed in DL models. As an example, the approximate
    approach demonstrated in ([Spagnolo_2022,](#bib.bib211) ) allows realizing the
    SoftMax layer in hardware by exploiting simple additions and logical bit-shifting
    operations to replace the computationally expensive exponentiations and divisions.
    When adopted in the realization of DNN accelerators supporting widespread models,
    like VGG-16 and ResNet-50, this approach guarantees computational times and power
    dissipation up to 7 and 12 times lower, respectively, than the accurate counterparts,
    introducing an accuracy penalty lower than 2% ([Spagnolo_2022,](#bib.bib211) ).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 近似计算提供了设计高效硬件加速器的多个机会。事实上，如图 [4](#S4.F4 "Figure 4 ‣ 4\. Approximate Computing
    Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep Learning
    on Heterogeneous Architectures") 所总结，它可以在不同的设计层次中加以利用：从算法开始，经过架构，到门级和晶体管级电路拓扑
    ([Jiang_2020,](#bib.bib119) )。几种近似策略提供了硬件导向的解决方案，可以在算法级别采用，以显著降低深度学习模型中关键层的复杂性和能耗。例如，([Spagnolo_2022,](#bib.bib211)
    ) 中演示的近似方法通过利用简单的加法和逻辑位移操作来替代计算开销大的指数运算和除法，实现了硬件中的 SoftMax 层。当应用于支持广泛模型的 DNN 加速器（如
    VGG-16 和 ResNet-50）时，该方法可以确保计算时间和功耗分别低于准确模型的 7 倍和 12 倍，同时引入的精度损失低于 2% ([Spagnolo_2022,](#bib.bib211)
    )。
- en: As discussed in ([Spagnolo_2022_2,](#bib.bib212) ), convolutional layers interleaved
    by non-linear activations and down-sampling can benefit from approximate computing
    methodologies, at the expense of a reasonable accuracy loss. The computations
    performed within convolutional layers followed by down-sampling layers are approximated
    through a prediction method that identifies potential predominant features. This
    approximation down-sampling strategy has been hardware-customized for DNN inference
    and, when applied to several benchmark models, it led to an overall energy saving
    of up to 70% with an accuracy loss lower than 3%, with respect to accurate designs.
    Recently, approximate computing has been exploited also to reconstruct high-resolution
    images ([Spagnolo_2023,](#bib.bib209) ) and to reduce the computational complexity
    of image denoising ([Spagnolo_2023_2,](#bib.bib210) ). Indeed, such applications
    have to meet tight constraints in terms of frame rate and energy consumption,
    thus making innovative and specific design methodologies highly desirable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如([Spagnolo_2022_2,](#bib.bib212))中讨论的那样，通过非线性激活和下采样交替的卷积层可以从近似计算方法中受益，尽管这会导致合理的准确性损失。卷积层和下采样层中的计算通过预测方法来近似，该方法识别潜在的主要特征。这种近似下采样策略已经为DNN推理进行了硬件定制，并且在应用于多个基准模型时，总体节能高达70%，准确性损失低于3%，相对于精确设计。最近，近似计算也被用于重建高分辨率图像([Spagnolo_2023,](#bib.bib209))和减少图像去噪的计算复杂度([Spagnolo_2023_2,](#bib.bib210))。确实，这些应用必须满足严格的帧率和能耗限制，因此创新和特定的设计方法极为重要。
- en: 'Efficient solutions to exploit approximate computing at gate- and transistor-level
    have been proposed to design approximate adders, multipliers and multiply-accumulate
    units for both ASIC ([Frustaci_2019,](#bib.bib86) ; [Frustaci_2020,](#bib.bib87)
    ; [Strollo_2020,](#bib.bib215) ; [Strollo_2022,](#bib.bib216) ) and FPGA devices
    ([Prabakaran_2018,](#bib.bib178) ; [Ahmad_2021,](#bib.bib10) ; [Ullah_2022,](#bib.bib220)
    ; [Perri_2022,](#bib.bib176) ). Such arithmetic operators receive a great deal
    of attention since they are the basic computational elements extensively used
    in DL models. Typically, the operands to be processed by approximate arithmetic
    circuits are split into sub-words: some of the least significant bits are processed
    inaccurately, whereas the remaining most significant bits are passed to the accurate
    circuits. Some strategies exploit static approximation that inflexibly sets the
    achieved accuracy at design time, while other solutions adopt dynamic approximation
    that allows tuning the quality target at runtime, thus leveraging the specificity
    of the data being processed with a graceful quality degradation. In a similar
    way, several approximation techniques can be adopted in the design of multipliers
    ([Akbari_2017,](#bib.bib12) ; [Esposito_2018,](#bib.bib78) ; [Strollo_2020,](#bib.bib215)
    ; [Frustaci_2019,](#bib.bib86) ; [Frustaci_2020,](#bib.bib87) ). Some techniques
    use dynamic and static segmentation methods ([Hashemi_2015,](#bib.bib100) ; [Narayanamoorthy_2015,](#bib.bib162)
    ; [Strollo_2022,](#bib.bib216) ; [Di_Meo_2023,](#bib.bib70) ): the former downsizes
    the multiplier by selecting only a segment of the inputs starting from the leading
    one bit, whereas the latter processes only predefined portions of the multiplicands.
    Besides the approximation strategies oriented to ASIC designs, appropriate methodologies
    are available to achieve high-performance and low-power designs of approximate
    modular multipliers also on FPGAs ([Ullah_2022,](#bib.bib220) ; [Perri_2022,](#bib.bib176)
    ). Finally, efficient approaches suitable to approximate divisions are demonstrated
    in ([Saadat_2019,](#bib.bib190) ; [Imani_2019,](#bib.bib109) ; [Zendegani_2016,](#bib.bib245)
    ) that either exploit approximate subtractors ([Chen_2015,](#bib.bib45) ; [Chen_2016,](#bib.bib46)
    ) or implement the signal segmentation technique ([Hashemi_2016,](#bib.bib101)
    ).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 设计近似加法器、乘法器和乘加单元的有效解决方案已被提出，以利用门级和晶体管级的近似计算，这些解决方案适用于ASIC设备（[Frustaci_2019,](#bib.bib86)
    ; [Frustaci_2020,](#bib.bib87) ; [Strollo_2020,](#bib.bib215) ; [Strollo_2022,](#bib.bib216)
    ）和FPGA设备（[Prabakaran_2018,](#bib.bib178) ; [Ahmad_2021,](#bib.bib10) ; [Ullah_2022,](#bib.bib220)
    ; [Perri_2022,](#bib.bib176)）。这些算术运算符受到极大关注，因为它们是深度学习模型中广泛使用的基本计算元素。通常，待处理的近似算术电路的操作数被拆分成子字：一些最低有效位被不准确处理，而剩余的最高有效位则传递给准确电路。一些策略利用静态近似，在设计时将精度设置为固定值，而其他解决方案则采用动态近似，在运行时调整质量目标，从而利用数据处理的特性，并实现平滑的质量退化。类似地，多种近似技术可以应用于乘法器的设计（[Akbari_2017,](#bib.bib12)
    ; [Esposito_2018,](#bib.bib78) ; [Strollo_2020,](#bib.bib215) ; [Frustaci_2019,](#bib.bib86)
    ; [Frustaci_2020,](#bib.bib87)）。一些技术使用动态和静态分段方法（[Hashemi_2015,](#bib.bib100) ;
    [Narayanamoorthy_2015,](#bib.bib162) ; [Strollo_2022,](#bib.bib216) ; [Di_Meo_2023,](#bib.bib70)）：前者通过选择从最高有效位开始的输入段来缩小乘法器的规模，而后者则仅处理预定义的乘数部分。除了面向ASIC设计的近似策略外，还可以采用适当的方法来实现高性能和低功耗的近似模块乘法器设计，适用于FPGA（[Ullah_2022,](#bib.bib220)
    ; [Perri_2022,](#bib.bib176)）。最后，在（[Saadat_2019,](#bib.bib190) ; [Imani_2019,](#bib.bib109)
    ; [Zendegani_2016,](#bib.bib245)）中展示了适用于近似除法的有效方法，这些方法要么利用近似减法器（[Chen_2015,](#bib.bib45)
    ; [Chen_2016,](#bib.bib46)），要么实现信号分段技术（[Hashemi_2016,](#bib.bib101)）。
- en: 5\. HLS-based design methodologies
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 基于HLS的设计方法
- en: To design efficient accelerators for DL on heterogeneous architectures, both
    software and hardware modeling steps are crucial. Moreover, an appropriate link
    between them is necessary to combine the algorithmic description with the constraints
    used to define desired performances, resource usage, and energy consumption. This
    link is provided by the High-level synthesis (HLS) that offers a variety of advantages
    to the designer, who not only can work at a higher level of abstraction when developing
    high-performance hardware, which leads to faster design changes and much faster
    functional verification, but also can create various solutions on several platforms
    (e.g., larger or smaller FPGAs) without altering the C/C++ source code, by just
    changing design directives. Such advantages make the exploration of design spaces
    and the identification of the best implementation much faster than with low-level
    hardware designs. However, the code must be written with a hardware implementation
    in mind in order to meet given performance and resource usage requirements. In
    fact, arbitrary software code, written for a CPU target, can achieve very low
    performance, since it typically does not expose enough parallelism to exploit
    the spatial concurrency available on an FPGA or an ASIC. Figure [5](#S5.F5 "Figure
    5 ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for
    Accelerating Deep Learning on Heterogeneous Architectures") provides a taxonomy
    of the HLS-based design methodologies reviewed in the following sub-sections,
    with a focus on the acceleration of deep learning (DL) models. The previous survey
    ([hlssurvey,](#bib.bib59) ) analyzed the evolution of HLS tools and the challenges
    that lay ahead of them to support newer application domains and performance requirements.
    In this paper, we focus our attention mainly on the commercial tool Vitis HLS
    and the open-source tool PandA-Bambu, both suitable for the HLS of complex DL
    applications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 设计高效的加速器以实现DL在异构架构上的应用时，软件和硬件建模步骤都是至关重要的。此外，它们之间的适当联系对于将算法描述与定义所需性能、资源使用和能耗的约束结合起来也是必要的。这一联系由高级综合（HLS）提供，它为设计师提供了多种优势，设计师不仅可以在开发高性能硬件时在更高的抽象层次上工作，从而实现更快的设计更改和功能验证，而且还可以通过只更改设计指令在多个平台（例如更大或更小的FPGA）上创建各种解决方案，而无需更改C/C++源代码。这些优势使得设计空间的探索和最佳实现的识别比低级硬件设计快得多。然而，代码必须考虑到硬件实现，以满足给定的性能和资源使用要求。实际上，为CPU目标编写的任意软件代码可能会实现非常低的性能，因为它通常没有暴露足够的并行性来利用FPGA或ASIC上的空间并发性。图 [5](#S5.F5
    "图5 ‣ 5\. 基于HLS的设计方法 ‣ 综述：加速深度学习在异构架构上的设计方法") 提供了接下来的子章节中回顾的基于HLS的设计方法的分类，重点关注深度学习（DL）模型的加速。之前的综述
    ([hlssurvey,](#bib.bib59) ) 分析了HLS工具的演变及其面临的挑战，以支持更新的应用领域和性能要求。本文主要关注商业工具Vitis
    HLS和开源工具PandA-Bambu，这两者都适用于复杂DL应用的HLS。
- en: '{forest}'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [HLS Design-based
    methodologies [HLS tools [ Vitis High-Level Synthesis ([vitisug212,](#bib.bib238)
    )
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [基于HLS设计的方法论
    [HLS工具 [ Vitis高级综合 ([vitisug212,](#bib.bib238) )
- en: Bambu ([ferrandi2021bambu,](#bib.bib81) )
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Bambu ([ferrandi2021bambu,](#bib.bib81) )
- en: Intel HLS Compiler ([IntelHLS2022,](#bib.bib113) )
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Intel HLS Compiler ([IntelHLS2022,](#bib.bib113) )
- en: Catapult ([CatapultHLS2022,](#bib.bib202) ), Stratus HLS ([StratusHLS2022,](#bib.bib31)
    )
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Catapult ([CatapultHLS2022,](#bib.bib202) ), Stratus HLS ([StratusHLS2022,](#bib.bib31)
    )
- en: LegUp  ([LegUpHLS2013,](#bib.bib36) ), MLIR ([lattner2021mlir,](#bib.bib137)
    ) - CIRCT ([circt,](#bib.bib57) ) ], ], [HLS-based DL frameworks [ hls4ml ([duarte2018fast,](#bib.bib73)
    )
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: LegUp  ([LegUpHLS2013,](#bib.bib36) ), MLIR ([lattner2021mlir,](#bib.bib137)
    ) - CIRCT ([circt,](#bib.bib57) ) ], ], [基于HLS的DL框架 [ hls4ml ([duarte2018fast,](#bib.bib73)
    )
- en: FINN ([blott2018finn,](#bib.bib25) )
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: FINN ([blott2018finn,](#bib.bib25) )
- en: ScaleHLS ([ye2022scalehls,](#bib.bib243) ; [scalehls2022dac,](#bib.bib244) )
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ScaleHLS ([ye2022scalehls,](#bib.bib243) ; [scalehls2022dac,](#bib.bib244) )
- en: SODA Synthesizer ([sodaMICRO,](#bib.bib27) ; [sodaDAC,](#bib.bib6) ) ], ], [IP
    block integration and interface protocols [ OpenFPGA CoreLib core library interoperability
    effort ([parCo,](#bib.bib234) )
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SODA综合器 ([sodaMICRO,](#bib.bib27) ; [sodaDAC,](#bib.bib6) ) ], ], [IP块集成和接口协议
    [ OpenFPGA CoreLib核心库互操作性工作 ([parCo,](#bib.bib234) )
- en: AXI protocol interface ([arm2013,](#bib.bib20) )
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AXI协议接口 ([arm2013,](#bib.bib20) )
- en: IP-XACT ([ipxact,](#bib.bib4) )
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: IP-XACT ([ipxact,](#bib.bib4) )
- en: IEEE 1735-2014 ([IEEE1735,](#bib.bib108) )
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE 1735-2014 ([IEEE1735,](#bib.bib108) )
- en: Vitis block integration ([IPflow,](#bib.bib239) ; [vitisug212,](#bib.bib238)
    ) ] ], ]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Vitis模块集成（[IPflow](#bib.bib239)；[vitisug212](#bib.bib238)）
- en: Figure 5. Taxonomy of HLS-based design methodologies discussed in Section [5](#S5
    "5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures")
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 在第[5](#S5 "5. HLS基础设计方法论 ‣ 加速深度学习在异构架构上的设计方法论综述")节中讨论的HLS基础设计方法论的分类
- en: 5.1\. The Vitis High-Level Synthesis
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. Vitis高层次综合
- en: Vitis HLS is the Xilinx High-Level Synthesis toolchain that provides FPGA designers
    with efficient support to describe their hardware designs using high-level languages,
    such as C and C++. These codes are then translated into a register-transfer level
    (RTL) language, i.e. VHDL or Verilog, automatically, thus simplifying the time-consuming
    and error-prone process based on RTL codes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Vitis HLS是Xilinx高层次综合工具链，为FPGA设计人员提供了高效的支持，使他们能够使用高级语言（如C和C++）描述硬件设计。这些代码随后被自动转换为寄存器传输级（RTL）语言，即VHDL或Verilog，从而简化了基于RTL代码的耗时且容易出错的过程。
- en: '![Refer to caption](img/60cc048bc43d451b364c1f893f5333bc.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60cc048bc43d451b364c1f893f5333bc.png)'
- en: Figure 6. Vitis HLS design flow
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. Vitis HLS设计流程
- en: As schematized in Figure [6](#S5.F6 "Figure 6 ‣ 5.1\. The Vitis High-Level Synthesis
    ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures"), in this design flow, C/C++ languages
    are used to describe both the hardware functionality and the testbench. The C
    simulation examines the behavior of the design just at the functional level and
    must be successful to continue with the synthesis process. The latter generates
    the RTL description of the top function complying with optimization directives
    and constraints furnished by the designer. The comprehensive report produced at
    the end of this step estimates time and hardware resource usage and can be used
    as a reference for any further refinement and optimization. The subsequent C/RTL
    Co-Simulation is performed to verify and validate the synthesized RTL design using
    the same C/C++ testbench previously used for the C simulation. It provides the
    designer with cycle-accurate performance information and it can also spot synthesis
    tool bugs. The IP core obtained by the HLS can be finally exported to be embedded
    within more complex designs by using Vivado and Vitis tools. Through appropriate
    synthesis directives, also known as *HLS pragmas* ([vitisug212,](#bib.bib238)
    ; [Sestito_2023,](#bib.bib197) ), Vitis HLS provides the designers with the ability
    to control both micro- and macro-architectural characteristics of their designs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[6](#S5.F6 "图6 ‣ 5.1. Vitis高层次综合 ‣ 5. HLS基础设计方法论 ‣ 加速深度学习在异构架构上的设计方法论综述")所示，在此设计流程中，使用C/C++语言描述硬件功能和测试平台。C模拟仅在功能级别检查设计行为，必须成功才能继续综合过程。综合过程生成符合优化指令和设计约束的顶层函数的RTL描述。此步骤结束时生成的综合报告估算时间和硬件资源使用情况，可作为进一步改进和优化的参考。随后进行C/RTL联合仿真，以使用之前用于C模拟的相同C/C++测试平台验证和确认综合后的RTL设计。它为设计人员提供了周期准确的性能信息，并且还可以发现综合工具中的错误。HLS获得的IP核可以最终导出，嵌入到使用Vivado和Vitis工具的更复杂设计中。通过适当的综合指令，也称为*HLS
    pragma*（[vitisug212](#bib.bib238)；[Sestito_2023](#bib.bib197)），Vitis HLS为设计人员提供了控制设计微观和宏观架构特性的能力。
- en: '![Refer to caption](img/761d7184fefd8eb1b61cfa17b3e52afb.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/761d7184fefd8eb1b61cfa17b3e52afb.png)'
- en: 'Figure 7. Effects of loop-level pragmas: (a) loop-pipelining; (b) loop-unrolling;
    (c) execution without pipelining'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 循环级pragma的效果： (a) 循环流水线； (b) 循环展开； (c) 不使用流水线时的执行
- en: '![Refer to caption](img/f1b9b16b66509bc6b32239f9f17794c8.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f1b9b16b66509bc6b32239f9f17794c8.png)'
- en: 'Figure 8. The DATAFLOW pragma: (a) the syntax; (b) execution with pragma; (c)
    execution without pragma'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. DATAFLOW pragma： (a) 语法； (b) 使用pragma时的执行； (c) 不使用pragma时的执行
- en: As shown in Figures [7](#S5.F7 "Figure 7 ‣ 5.1\. The Vitis High-Level Synthesis
    ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") and  [8](#S5.F8 "Figure 8 ‣ 5.1\.
    The Vitis High-Level Synthesis ‣ 5\. HLS-based design methodologies ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    to reduce the latency and increase the throughput, loop-level transformations
    (such as *Loop pipelining*, *Loop unrolling* and *Loop flattening*) and dataflow
    pragmas can be exploited. The former allows optimizing the hardware execution
    of loops, whereas, the latter allows sequential functions to be overlapped in
    their hardware execution. Conversely, to control the hardware resources utilization,
    variable-level (such as *array partitioning* and *array reshaping*) and operation-level
    pragmas (such as *bind op* and *allocate*) can be used to affect, respectively,
    the memory structures and the organization of computational units implemented
    for specific operations. Finally, to define the communication protocol to be adopted
    at the interface ports of the design, the interface pragma is available.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7](#S5.F7 "Figure 7 ‣ 5.1\. The Vitis High-Level Synthesis ‣ 5\. HLS-based
    design methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") 和 [8](#S5.F8 "Figure 8 ‣ 5.1\. The Vitis
    High-Level Synthesis ‣ 5\. HLS-based design methodologies ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    所示，为了减少延迟和提高吞吐量，可以利用循环级别的转换（如 *循环流水线*、*循环展开* 和 *循环扁平化*）以及数据流 pragma。前者允许优化循环的硬件执行，而后者允许将顺序函数重叠在硬件执行中。相反，为了控制硬件资源的使用，可以使用变量级别的
    pragma（如 *数组分区* 和 *数组重塑*）以及操作级别的 pragma（如 *绑定操作* 和 *分配*），分别影响内存结构和为特定操作实现的计算单元的组织。最后，为了定义在设计接口端口处采用的通信协议，可以使用接口
    pragma。
- en: 5.2\. The Bambu Open-source High-Level Synthesis
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. Bambu 开源高级综合工具
- en: Bambu ([ferrandi2021bambu,](#bib.bib81) ) is an open-source command-line HLS
    tool developed at Politecnico di Milano supporting the synthesis of most C/C++
    constructs, including function calls, pointer arithmetic, and dynamic resolution
    of memory accesses, accesses to arrays and structs, parameters passed by reference
    or copy, and more. The Bambu design flow is schematized in Figure [9](#S5.F9 "Figure
    9 ‣ 5.2\. The Bambu Open-source High-Level Synthesis ‣ 5\. HLS-based design methodologies
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures").
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Bambu ([ferrandi2021bambu,](#bib.bib81)) 是一个开源的命令行 HLS 工具，由米兰理工大学开发，支持大多数 C/C++
    语言构造的综合，包括函数调用、指针运算、动态内存访问解析、对数组和结构体的访问、通过引用或复制传递的参数等。Bambu 的设计流程如图 [9](#S5.F9
    "Figure 9 ‣ 5.2\. The Bambu Open-source High-Level Synthesis ‣ 5\. HLS-based design
    methodologies ‣ A Survey on Design Methodologies for Accelerating Deep Learning
    on Heterogeneous Architectures") 所示。
- en: '![Refer to caption](img/70ca3653d72ac124b01b30248085d43f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70ca3653d72ac124b01b30248085d43f.png)'
- en: Figure 9. Bambu Compilation flow.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. Bambu 编译流程。
- en: At the front end, Bambu interfaces with existing compilers, such as GCC and
    Clang, to parse the input code and translate it into an Intermediate Representation
    (IR) with a Static Single Assignment in-memory. This approach decouples the compiler
    front-end from the rest of the HLS process and allows different compilers to be
    rapidly and easily integrated. Starting from the IR, Bambu builds data structures
    (such as the Call Graph, Control Data Flow Graphs, and Program Dependence Graphs)
    and applies a set of device-independent analyses and transformations (e.g. data
    flow analysis, loop recognition, dead code elimination, constant propagation,
    LUT expression insertion, etc.). Often, transformations at the hardware level
    are also adopted to improve area utilization and delay of the final accelerator
    (e.g. multiplications and divisions by constant values are transformed into expressions
    that use only shifts and additions). Differently from general-purpose software
    compilers, designed to target a processor with a fixed-sized data path of 32 or
    64 bits, Bambu selects the minimal number of bits required for specific operations
    and value storage through Bitwidth and Range Analysis. These passes are crucial
    during the optimization process to fulfill requirements in terms of speed performances,
    area, and power, without affecting the functional behavior of the synthesized
    design.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前端，Bambu 与现有的编译器（如 GCC 和 Clang）接口，以解析输入代码并将其转换为具有静态单赋值（Static Single Assignment）的中间表示（Intermediate
    Representation, IR）在内存中。这种方法将编译器前端与 HLS 过程的其余部分解耦，并允许不同的编译器能够快速且轻松地集成。从 IR 开始，Bambu
    构建数据结构（如调用图、控制数据流图和程序依赖图），并应用一系列设备无关的分析和转换（例如数据流分析、循环识别、死代码消除、常量传播、LUT 表达式插入等）。通常，硬件级别的转换也会被采用，以提高最终加速器的面积利用率和延迟（例如，将常量值的乘法和除法转换为仅使用移位和加法的表达式）。与旨在面向具有固定大小数据路径（32
    位或 64 位）的处理器的一般软件编译器不同，Bambu 通过比特宽度和范围分析选择特定操作和数值存储所需的最小比特数。这些过程在优化过程中至关重要，以满足速度性能、面积和功耗方面的要求，而不会影响合成设计的功能行为。
- en: 'At the back end, Bambu performs the actual architectural synthesis and furnishes
    the VHDL/Verilog code. This process acts on each function separately, taking into
    account that a single function includes at least two sub-modules: the control
    logic and the data path. The first step of this process is the Function Allocation
    which associates functions in the IR with specific resources available in the
    target technology. The Bambu technology library contains standard functions, standard
    system libraries, such as libc and libm, and custom components, written in Verilog
    or VHDL. Bambu supports function pointers and sharing of (sub)modules across module
    boundaries through function proxies ([FPL2015,](#bib.bib151) ), which provides
    valuable area savings when complex call graphs are considered, with no significant
    impact on the execution delays. Subsequently, the Memory Allocation step defines
    the memory types to be used. Statically analyzing the memory accesses, Bambu builds
    a hierarchical data path where memories can be classified as read-only, local,
    with aligned or unaligned memory accesses, requiring dynamic resolutions. Multiple
    buses connect load/store components to their respective memories; the same memory
    infrastructure can also be connected to external components such as scratchpads,
    caches, and off-chip DRAMs.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在后端，Bambu 执行实际的架构合成并提供 VHDL/Verilog 代码。此过程对每个函数分别处理，考虑到单个函数包括至少两个子模块：控制逻辑和数据路径。该过程的第一步是函数分配，将
    IR 中的函数与目标技术中的特定资源关联。Bambu 技术库包含标准函数、标准系统库（如 libc 和 libm）以及用 Verilog 或 VHDL 编写的自定义组件。Bambu
    支持函数指针和通过函数代理在模块边界之间共享（子）模块（[FPL2015](#bib.bib151)），这在考虑复杂调用图时提供了有价值的面积节省，而不会对执行延迟产生显著影响。随后，内存分配步骤定义了要使用的内存类型。通过静态分析内存访问，Bambu
    构建了一个分层数据路径，其中内存可以被分类为只读、本地、具有对齐或不对齐内存访问的动态解析。多个总线将加载/存储组件连接到各自的内存；相同的内存基础设施也可以连接到外部组件，如快速存储器、缓存和外部
    DRAM。
- en: Then, the Resource Allocation associates the operations that are not mapped
    on a library function to resource units available in the resource library. Floating-point
    operations are supported either through a soft-float library or by the FloPoCo
    framework ([DinechinPasca2011-DaT,](#bib.bib68) ). Resources are pre-characterized
    and their description includes information about their latency, area, and number
    of pipeline stages. When more than one matching between operation and resource
    is feasible, the selection is driven by design constraints.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，资源分配将未映射到库函数上的操作与资源库中可用的资源单元关联。浮点操作可以通过软浮点库或FloPoCo框架 ([DinechinPasca2011-DaT,](#bib.bib68))
    来支持。资源是预先表征的，其描述包括延迟、面积和流水线阶段数量的信息。当多个操作与资源匹配的可能性时，选择会受到设计约束的驱动。
- en: 'To associate operations with control steps, Bambu employs a list scheduling
    algorithm. In addition to this old but efficient method, Bambu also implements
    the more aggressive speculative scheduling algorithm based on System of Difference
    Constraints ([ICCAD2015B,](#bib.bib139) ). Within the computed schedule, operations
    that execute concurrently cannot share the same resource, so module binding is
    performed through a clique covering algorithm on a weighted compatibility graph ([LStok94,](#bib.bib214)
    ). Variables with non-overlapping life intervals may share the same register,
    so liveness analysis is applied before binding storage values to registers. Interconnections
    are bound according to the outcome of the previous steps: if a functional or memory
    resource is shared, then the algorithm introduces steering logic on its inputs
    and identifies the set of control signals driven by the controller.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将操作与控制步骤关联，Bambu 使用了一个列表调度算法。除了这种古老但有效的方法外，Bambu 还实现了基于差分约束系统的更具攻击性的猜测调度算法
    ([ICCAD2015B,](#bib.bib139))。在计算出的调度中，同时执行的操作不能共享同一资源，因此通过加权兼容性图上的团覆盖算法进行模块绑定
    ([LStok94,](#bib.bib214))。具有非重叠生命周期的变量可以共享同一寄存器，因此在将存储值绑定到寄存器之前会应用活跃性分析。互连根据之前步骤的结果进行绑定：如果共享功能或内存资源，则算法会在其输入上引入引导逻辑，并确定由控制器驱动的控制信号集。
- en: The final architecture is then generated and represented through a hyper-graph
    highlighting the interconnection between modules. Finally, depending on the technology/device
    chosen as the target, the netlist generation step furnishes the final RTL description
    of the entire design.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最终架构随后生成，并通过高阶图表示，突出模块之间的互连。最后，根据选择的技术/设备，网表生成步骤提供整个设计的最终 RTL 描述。
- en: 5.3\. Other HLS tools
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 其他HLS工具
- en: 'In most cases, HLS tools are provided by FPGA vendors within an FPGA full design
    suite by the same company. The aforementioned Vitis HLS, for example, is part
    of the AMD/Xilinx tool suite and only supports Xilinx FPGAs. The Intel HLS Compiler
    ([IntelHLS2022,](#bib.bib113) ) is part of the Quartus design suite: it compiles
    C++ functions into an RTL implementation for Intel FPGAs and optimizes them through
    a simple command-line interface. Intel recently announced that the HLS compiler
    will be substituted by the oneAPI toolkit ([oneapi,](#bib.bib112) ) to enable
    developers a seamless porting of OpenCL code across CPUs, GPUs, and FPGAs. Catapult
    ([CatapultHLS2022,](#bib.bib202) ) is a multi-target HLS and verification tool
    provided by Siemens, synthesizing C++ and SystemC code for FPGA and ASIC. Stratus
    HLS ([StratusHLS2022,](#bib.bib31) ) from Cadence synthesizes SystemC code written
    with a lower-level perspective, i.e., requiring users to explicitly describe interface
    protocols between components. LegUp ([LegUpHLS2013,](#bib.bib36) ) is an open-source,
    LLVM-based HLS tool developed in academia and later acquired by Microchip and
    rebranded as SmartHLS ([smartHLS,](#bib.bib150) ).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，HLS 工具由 FPGA 供应商在同一公司的 FPGA 完整设计套件中提供。例如，上述的 Vitis HLS 是 AMD/Xilinx
    工具套件的一部分，只支持 Xilinx FPGAs。Intel HLS 编译器 ([IntelHLS2022,](#bib.bib113)) 是 Quartus
    设计套件的一部分：它将 C++ 函数编译为 Intel FPGA 的 RTL 实现，并通过简单的命令行界面进行优化。Intel 最近宣布，HLS 编译器将被
    oneAPI 工具包 ([oneapi,](#bib.bib112)) 替代，以实现 OpenCL 代码在 CPU、GPU 和 FPGA 之间的无缝移植。Catapult
    ([CatapultHLS2022,](#bib.bib202)) 是 Siemens 提供的多目标 HLS 和验证工具，合成 FPGA 和 ASIC 的
    C++ 和 SystemC 代码。Cadence 的 Stratus HLS ([StratusHLS2022,](#bib.bib31)) 从更底层的视角合成
    SystemC 代码，即要求用户明确描述组件之间的接口协议。LegUp ([LegUpHLS2013,](#bib.bib36)) 是一个开源的、基于 LLVM
    的 HLS 工具，最初由学术界开发，后来被 Microchip 收购，并重新命名为 SmartHLS ([smartHLS,](#bib.bib150))。
- en: The Multi-Level Intermediate Representation (MLIR) ([lattner2021mlir,](#bib.bib137)
    ) is a reusable and extensible infrastructure in the LLVM project for the development
    of domain-specific compilers. MLIR allows defining specialized IRs called dialects
    to implement analysis and transformation passes at different levels of abstraction,
    and it can interface with multiple software programming frameworks, including
    the ones used to implement DL algorithms. The CIRCT project ([circt,](#bib.bib57)
    ) intends to use MLIR to build a new generation of interoperable tools and compilers
    for hardware design, starting from the definition of circuit-level IRs and working
    upwards to higher levels of abstraction (e.g., dataflow models or finite state
    machines). Part of the project is dedicated to HLS ([circt-hls,](#bib.bib222)
    ), particularly to the implementation of static and dynamic scheduling through
    MLIR and CIRCT dialects. CIRCT could be an essential building block for future
    industrial and academic design flows; however, its degree of maturity is lower
    compared to HLS tools with optimized synthesis algorithms and resource libraries
    supported by decades of research.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 多层次中间表示（MLIR） ([lattner2021mlir,](#bib.bib137) ) 是 LLVM 项目中的一个可重用且可扩展的基础设施，用于开发领域特定的编译器。MLIR
    允许定义专门的 IR 称为方言，以在不同的抽象层次上实现分析和转换过程，并且它可以与多种软件编程框架接口，包括用于实现 DL 算法的框架。CIRCT 项目
    ([circt,](#bib.bib57) ) 旨在利用 MLIR 构建新一代可互操作的硬件设计工具和编译器，从电路级 IR 的定义开始，逐步向更高层次的抽象（例如数据流模型或有限状态机）发展。项目的一部分致力于
    HLS ([circt-hls,](#bib.bib222) )，特别是通过 MLIR 和 CIRCT 方言实现静态和动态调度。CIRCT 可能成为未来工业和学术设计流程的一个重要构建模块；然而，与具有经过数十年研究的优化合成算法和资源库的
    HLS 工具相比，其成熟度较低。
- en: 5.4\. HLS-based Design Flows for Deep Learning
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 基于 HLS 的深度学习设计流程
- en: Two popular frameworks that help automate the design of ML accelerators are
    hls4ml ([duarte2018fast,](#bib.bib73) ) and FINN ([blott2018finn,](#bib.bib25)
    ). They parse ML models and replace operators with C/C++ functions taken from
    a library of templates that already contain pragmas. The HLS tool used as the
    backend (mainly Vivado or Vitis HLS) processes this intermediate C/C++ representation
    and produces a corresponding accelerator design without requiring further manual
    intervention.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 两个流行的框架，hls4ml ([duarte2018fast,](#bib.bib73) ) 和 FINN ([blott2018finn,](#bib.bib25)
    )，帮助自动化机器学习加速器的设计。它们解析 ML 模型，并用从包含已有 pragma 的模板库中提取的 C/C++ 函数替换操作符。作为后端使用的 HLS
    工具（主要是 Vivado 或 Vitis HLS）处理这种中间 C/C++ 表示，并生成相应的加速器设计，无需进一步的人工干预。
- en: The library of templates in hls4ml and FINN is necessarily tied to a specific
    HLS tool and a narrow set of models. This is required by expert HLS developers
    to implement in advance the best version of all necessary operators for a pre-determined
    backend tool. Given that, each tool has its own coding patterns, annotations,
    and configuration directives that are not recognized by other tools, a new version
    of the library is needed to switch to a new hardware target to achieve efficient
    designs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: hls4ml 和 FINN 中的模板库必然与特定的 HLS 工具和狭窄的模型集合绑定。这是 HLS 专家开发人员在预定的后端工具上提前实现所有必要操作符的最佳版本所要求的。因此，每个工具都有其自己的编码模式、注释和配置指令，这些是其他工具所不认可的，要切换到新的硬件目标以实现高效设计，就需要新的库版本。
- en: Typically, models of interest are Deep and Convolutional Neural Networks (DNNs/CNNs),
    sometimes with a scope limited by application requirements that can significantly
    affect resource utilization. For example, the original implementation of hls4ml
    was optimized for small, fully connected models under tight latency constraints,
    reflecting the needs of a high-energy physics experiment at CERN. To comply with
    those requirements, hls4ml proposed to store network weights inside on-chip logic
    and unroll all loops to increase parallelism, which quickly depletes FPGA resources
    when considering more complex models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，关注的模型是深度神经网络（DNNs）和卷积神经网络（CNNs），有时其范围受应用要求的限制，这可能显著影响资源利用。例如，hls4ml 的原始实现针对的是在严格延迟约束下的小型全连接模型，反映了
    CERN 高能物理实验的需求。为了满足这些要求，hls4ml 提议将网络权重存储在片上逻辑中，并展开所有循环以增加并行性，但考虑到更复杂的模型时，这会迅速耗尽
    FPGA 资源。
- en: While it is true that DNNs and CNNs cover a significant part of ML applications
    (especially in the computer vision field), there is ample room for exploring other
    classes of models, for example, to accelerate scientific applications that work
    on sparse data structures or graphs. Existing HLS-based design flows are good
    at extracting data- and instruction-level parallelism (e.g. by unrolling loops),
    but they are not equipped to deal with irregular task-based patterns.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DNN 和 CNN 确实涵盖了 ML 应用的很大一部分（特别是在计算机视觉领域），但仍有广阔的空间去探索其他类别的模型，例如加速处理稀疏数据结构或图形的科学应用。现有的基于
    HLS 的设计流程擅长于提取数据和指令级的并行性（例如通过展开循环），但它们不适合处理不规则的任务型模式。
- en: A narrow focus limits the possibility of quickly adapting to new algorithmic
    approaches, which would instead be desirable in a rapidly evolving field such
    as ML (and data science in general). For this reason, many efforts have been made
    to use existing HLS tools as ”black boxes”, thus exploiting their optimization
    opportunities as much as possible. However, there is also a trend toward the democratization
    of hardware design, as attested for example by the open-source release of the
    Xilinx Vitis HLS frontend ([vitis-llvm,](#bib.bib18) ) or by the OpenROAD project
    for ASIC synthesis ([openroad,](#bib.bib11) ).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 狭窄的关注点限制了快速适应新算法方法的可能性，而这在像 ML（以及数据科学）这样的快速发展的领域中是非常值得期望的。因此，许多努力已经投入使用现有的 HLS
    工具作为“黑匣子”，从而尽可能利用它们的优化机会。然而，也有一种向硬件设计民主化的趋势，例如 Xilinx Vitis HLS 前端的开源发布 ([vitis-llvm,](#bib.bib18)
    ) 或 OpenROAD 项目用于 ASIC 合成 ([openroad,](#bib.bib11) )。
- en: Other efficient frameworks suitable to analyze input codes from C or PyTorch
    and to generate annotated code for Vivado HLS are ScaleHLS ([ye2022scalehls,](#bib.bib243)
    ; [scalehls2022dac,](#bib.bib244) ) and SOftware Defined Architectures (SODA)
    ([sodaMICRO,](#bib.bib27) ; [sodaDAC,](#bib.bib6) ). Through the multiple levels
    of abstraction provided by existing MLIR dialects, ScaleHLS reasons about graph-level,
    loop-level, and directive-level optimizations; a custom dialect helps the translation
    into C++ with pragmas. A quality of results estimator and a DSE engine automatically
    identify the best combination of optimizations following user-defined constraints,
    without requiring long simulation or synthesis runs to evaluate the effect of
    changes in the optimization directives.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其他适合分析来自 C 或 PyTorch 的输入代码并为 Vivado HLS 生成注释代码的高效框架包括 ScaleHLS ([ye2022scalehls,](#bib.bib243)
    ; [scalehls2022dac,](#bib.bib244) ) 和软件定义架构（SODA） ([sodaMICRO,](#bib.bib27) ;
    [sodaDAC,](#bib.bib6) )。通过现有 MLIR 方言提供的多级抽象，ScaleHLS 关注图级、循环级和指令级的优化；自定义方言有助于将其翻译成带有指示的
    C++。质量结果估计器和 DSE 引擎会自动识别在用户定义的约束下最佳的优化组合，而无需长时间的仿真或合成运行来评估优化指示变化的效果。
- en: On the contrary, SODA is an open-source, multi-level, modular, extensible, no-human-in-the-loop
    hardware compiler that translates high-level ML models into domain-specific accelerators.
    It comprises a compiler-based frontend that leverages MLIR (SODA-OPT ([sodaopt,](#bib.bib26)
    )) and a compiler-based backend that integrates state-of-the-art HLS methodologies
    (Bambu); it generates highly specialized designs that can be synthesized with
    both commercial and open-source tools on FPGAs or ASICs, and it allows exploring
    design metrics through compilation passes and parameters, enabling the identification
    of architectural trade-offs depending on the target application requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，SODA 是一个开源的、多级的、模块化的、可扩展的、无人工干预的硬件编译器，它将高层次的 ML 模型转换为领域特定的加速器。它包含一个基于编译器的前端，利用
    MLIR（SODA-OPT ([sodaopt,](#bib.bib26) )）和一个基于编译器的后端，集成了最先进的 HLS 方法论（Bambu）；它生成高度专业化的设计，这些设计可以用商业和开源工具在
    FPGA 或 ASIC 上合成，并且通过编译过程和参数探索设计指标，使得根据目标应用需求识别架构权衡成为可能。
- en: 5.5\. IP block integration
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. IP 块集成
- en: It is a common practice to use pre-designed blocks supplied by third parties
    (Intellectual Properties, IPs) to add the desired functionalities to a system
    being developed. The use of IPs not only saves development time, but also the
    huge testing time that has been spent to qualify it. The high level of abstraction
    given by HLS flows and their maturity provide the interesting possibility to export
    functionalities developed through HLS as IPs. A first attempt to address this
    feature in a systematic way dates back to 2008 ([parCo,](#bib.bib234) ), where
    various strategies used by HLS tools to integrate IPs were analyzed. The standardization
    of IPs structure for their distribution ([IEEE1735,](#bib.bib108) ) does not cover
    IPs generated by HLS, so IP integration and export may be supported differently
    by each tool (e.g., Vitis HLS provides its own block integration capabilities
    ([IPflow,](#bib.bib239) ; [vitisug212,](#bib.bib238) )).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的做法是使用第三方（知识产权，IPs）提供的预设计模块，将所需功能添加到正在开发的系统中。使用IPs不仅节省了开发时间，还节省了大量用于验证的测试时间。HLS
    流提供的高度抽象及其成熟度提供了通过 HLS 将开发的功能导出为 IPs 的有趣可能性。2008 年 ([parCo,](#bib.bib234)) 尝试以系统化方式解决这一特性，分析了
    HLS 工具用于集成 IPs 的各种策略。IP 结构的标准化以供分发 ([IEEE1735,](#bib.bib108)) 并未涵盖 HLS 生成的 IPs，因此每个工具对
    IP 集成和导出的支持可能有所不同（例如，Vitis HLS 提供了其自身的块集成功能 ([IPflow,](#bib.bib239); [vitisug212,](#bib.bib238)
    )）。
- en: To allow IP generation and reuse, an interfacing standard is mandatory to allow
    interoperability among IPs. Currently, AXI4 ([arm2013,](#bib.bib20) ) is used
    as a de facto standard. The AXI4 standard includes AXI4-Full, AXI4-stream, and
    AXI4-Lite protocols used to access memory banks, streaming channels, and memory-mapped
    registers. Other than a common interface, a common language to describe the IP
    interfaces and the IP organization on the filesystem is needed. IP-XACT ([ipxact,](#bib.bib4)
    ) is an XML format describing meta-data and interfaces of IPs and is widely adopted
    by IP providers to describe their IPs in terms of file system organization, interfaces,
    source files, constraint files, and so on.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许 IP 生成和重用，必须有一个接口标准以支持 IP 之间的互操作性。目前，AXI4 ([arm2013,](#bib.bib20)) 被用作事实上的标准。AXI4
    标准包括 AXI4-Full、AXI4-stream 和 AXI4-Lite 协议，用于访问内存银行、流通道和内存映射寄存器。除了通用接口外，还需要一种描述
    IP 接口和 IP 文件系统组织的通用语言。IP-XACT ([ipxact,](#bib.bib4)) 是一种描述 IP 的元数据和接口的 XML 格式，被
    IP 提供商广泛采用，以描述其 IP 的文件系统组织、接口、源文件、约束文件等。
- en: 6\. Deep Learning Compilers
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 深度学习编译器
- en: The development of innovative hardware architectures, particularly for highly
    parallelizable applications such as DL, is only half of the picture. The other
    half is the effective automated deployment technologies that allow the use of
    novel architectures to run complex real-world applications. Managing the memory
    hierarchy and compiling high-level signal processing and machine learning graphs
    into a representation is a complex research problem, that has been extensively
    studied in the past few years, as summarized in Figure  [10](#S6.F10 "Figure 10
    ‣ 6\. Deep Learning Compilers ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures").
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创新的硬件架构开发，特别是针对如 DL 等高度可并行化的应用程序，只是完成了一半的工作。另一半是有效的自动化部署技术，这些技术允许使用新颖的架构来运行复杂的实际应用程序。管理内存层次结构和将高级信号处理及机器学习图编译成表示是一个复杂的研究问题，近年来已被广泛研究，如图
    [10](#S6.F10 "图 10 ‣ 6. 深度学习编译器 ‣ 加速异构架构上的深度学习设计方法论综述") 所总结。
- en: '{forest}'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Deep Learning
    Compilers [Memory hierarchy management in DNN Accelerators [ Ivanov et al. ([ivanov2020data,](#bib.bib114)
    )
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [深度学习编译器
    [内存层次管理在 DNN 加速器中的应用 [ Ivanov 等 ([ivanov2020data,](#bib.bib114))
- en: DMazeRunner ([dave2019dmazerunner,](#bib.bib65) )
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: DMazeRunner ([dave2019dmazerunner,](#bib.bib65))
- en: Maestro ([kwon2020maestro,](#bib.bib132) )
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Maestro ([kwon2020maestro,](#bib.bib132))
- en: Interstellar ([yang2020interstellar,](#bib.bib240) ), Timeloop ([parashar2019timeloop,](#bib.bib171)
    ) ], ], [DL Compilers for MCUs [ TFLite Micro ([david2020tensorflow,](#bib.bib66)
    )
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Interstellar ([yang2020interstellar,](#bib.bib240)), Timeloop ([parashar2019timeloop,](#bib.bib171))
    ], ], [DL 编译器用于 MCU [ TFLite Micro ([david2020tensorflow,](#bib.bib66))
- en: Larq Computing Engine (LCE) ([Larq,](#bib.bib91) )
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Larq 计算引擎 (LCE) ([Larq,](#bib.bib91))
- en: X-CUBE-AI ([CubeAI,](#bib.bib213) )
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: X-CUBE-AI ([CubeAI,](#bib.bib213))
- en: GWT AutoTiler ([GAP8Auto-tilerManual,](#bib.bib97) ), DORY ([burrello2021dory,](#bib.bib30)
    ) ], ], [DL Compilers for High-Performance and Embedded [ TVM ([chen2018tvm,](#bib.bib47)
    ), HTVM ([vandelmHTVMEfficientNeural2023a,](#bib.bib223) ), Halide ([ragan2013halide,](#bib.bib182)
    )
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: GWT AutoTiler（[GAP8Auto-tilerManual](#bib.bib97)）、DORY（[burrello2021dory](#bib.bib30)）、[DL编译器用于高性能和嵌入式
    [ TVM（[chen2018tvm](#bib.bib47)）、HTVM（[vandelmHTVMEfficientNeural2023a](#bib.bib223)）、Halide（[ragan2013halide](#bib.bib182)）
- en: Tensor Comprehensions ([vasilacheTensorComprehensionsFrameworkAgnostic2018,](#bib.bib228)
    )
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Comprehensions（[vasilacheTensorComprehensionsFrameworkAgnostic2018](#bib.bib228)）
- en: Glow ([rotemGlowGraphLowering2019,](#bib.bib187) ), Relay ([roeschRelayNewIR2018,](#bib.bib185)
    )
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Glow（[rotemGlowGraphLowering2019](#bib.bib187)）、Relay（[roeschRelayNewIR2018](#bib.bib185)）
- en: ONNC ([linONNCCompilationFramework2019,](#bib.bib142) ), MLIR ([lattnerMLIRScalingCompiler2021,](#bib.bib138)
    ; [jinCompilingONNXNeural2020,](#bib.bib122) ) ] ] ]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ONNC（[linONNCCompilationFramework2019](#bib.bib142)）、MLIR（[lattnerMLIRScalingCompiler2021](#bib.bib138)；[jinCompilingONNXNeural2020](#bib.bib122)）
- en: Figure 10. Taxonomy of Deep Learning compilers discussed in Section [6](#S6
    "6\. Deep Learning Compilers ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures")
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 深度学习编译器的分类，详见第[6](#S6 "6\. 深度学习编译器 ‣ 加速深度学习在异构架构上的设计方法综述")节
- en: 6.1\. Memory hierarchy management in DNN Accelerators
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. DNN加速器中的内存层次结构管理
- en: 'Effective management of memory hierarchy is a critical challenge in deploying
    DNNs, which generate high amounts of weights and activations traffic between different
    levels of memory hierarchy. To tackle this problem, various methods have been
    proposed for data flow scheduling and generation across three broad classes of
    devices: high-performance computing systems, DNN accelerators, and embedded systems.
    For high-performance computing systems, Ivanov et al. ([ivanov2020data,](#bib.bib114)
    ) have proposed new transformer primitives to exploit data reuse and limit data
    movement. Meanwhile, DMazeRunner ([dave2019dmazerunner,](#bib.bib65) ), Maestro ([kwon2020maestro,](#bib.bib132)
    ), Interstellar ([yang2020interstellar,](#bib.bib240) ), Timeloop ([parashar2019timeloop,](#bib.bib171)
    ) discuss DNN optimization on AI-specialized accelerators based on systolic arrays
    of processing elements (PEs), with a focus on loop tiling and/or reordering to
    optimize PE utilization. These tools can output an accelerator model to run a
    given DNN or spatial scheduling to maximize PE array utilization. MCU data flow
    scheduling tools are similar to frameworks like DMazeRunner as both optimize dataflow
    schedules given an externally known architecture. However, DNN execution on MCUs
    presents unique challenges such as adapting to a general-purpose architecture
    and limited memory. Additionally, kernel instructions are heavily influenced by
    the limited size of the register file, resulting in increased load-store operations
    and demand for optimal loop sizing to avoid register spilling overhead. Academic
    researchers and industries have investigated this aspect by incorporating specialized
    caches or explicitly managed scratchpad memories into their edge-node solutions.
    For example, NXP offers specialized caches in their Cortex M4/M0 MCU, as does
    STMicroelectronics with its STM32 Cube-AI toolflow; on the other hand, GreenWaves
    Technologies provides explicitly managed scratchpad memories ([flamand2018gap,](#bib.bib83)
    ), with a GAPFlow tool dedicated to managing them appropriately.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构的有效管理是部署深度神经网络（DNN）中的一个关键挑战，因为这些网络在不同层次的内存之间产生大量的权重和激活流量。为了解决这个问题，已经提出了各种方法用于在三大类设备上进行数据流调度和生成：高性能计算系统、DNN加速器和嵌入式系统。对于高性能计算系统，Ivanov
    等人（[ivanov2020data](#bib.bib114)）提出了新的变换器原语，以利用数据重用并限制数据移动。与此同时，DMazeRunner（[dave2019dmazerunner](#bib.bib65)）、Maestro（[kwon2020maestro](#bib.bib132)）、Interstellar（[yang2020interstellar](#bib.bib240)）、Timeloop（[parashar2019timeloop](#bib.bib171)）讨论了基于处理单元（PEs）的系统阵列的AI专用加速器上的DNN优化，重点关注循环分块和/或重排序以优化PE利用率。这些工具可以输出一个加速器模型来运行给定的DNN或空间调度以最大化PE阵列的利用率。MCU数据流调度工具类似于像DMazeRunner这样的框架，因为它们都在已知架构的情况下优化数据流调度。然而，在MCUs上执行DNN呈现出独特的挑战，如适应通用架构和有限内存。此外，内核指令受到寄存器文件大小限制的严重影响，导致负载-存储操作增加，并且需要优化循环大小以避免寄存器溢出开销。学术研究人员和工业界通过将专用缓存或显式管理的临时存储器纳入其边缘节点解决方案中，研究了这一方面。例如，NXP在其Cortex
    M4/M0 MCU中提供了专用缓存，STMicroelectronics的STM32 Cube-AI工具流也是如此；另一方面，GreenWaves Technologies提供了显式管理的临时存储器（[flamand2018gap](#bib.bib83)），并有一个专门的GAPFlow工具来适当地管理它们。
- en: 6.2\. Deep Learning Compilers for MCUs
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 深度学习编译器用于MCU
- en: 'The introduction of the first generation of low-power NN-oriented MCUs has
    increased this need, as these platforms need to utilize optimized software and
    ISA extensions for DNN computing alongside traditional control and I/O-bound activities.
    To enable optimal execution of both types of tasks, these MCUs employ parallel
    and heterogeneous processing. ST Microelectronics¹¹1[https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html](https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html)
    and NXP have recently introduced new-generation dual-core microcontrollers with
    an ARM M0 processor dedicated to I/O and an ARM M4 processor with single-cycle
    multiply-and-accumulate and SIMD capabilities. These platforms show an increased
    complexity in terms of memory hierarchy compared to conventional flat-memory MCUs,
    with an L1 memory optimized for speed and an L2 optimized for capacity. At the
    same time, there is a trend towards explicit management of memory hierarchy, with
    hand-tunable data caches featuring locking for hand-crafted data management. For
    instance, the Kendrite K210²²2https://canaan.io/product/kendryteai is a RISC-V
    dual-core 64 bits system-on-chip with a neural network processor (KPU) on which
    the cores can offload the computation. It also includes dedicated memory banks
    for the NN accelerator and a DMA unit to explicitly manage data transfers. The
    SONY Spresense board³³3https://developer.sony.com/develop/spresense/ features
    a 6-cores M4 accelerator with a maximum clock speed of 156 MHz, 1.5 MB of SRAM,
    and 8 MB of Flash. The GreenWaves Technologies GAP-8 ([flamand2018gap,](#bib.bib83)
    ) system-on-chip was introduced in 2018 as a commercial embodiment of the Parallel
    Ultra-Low-Power paradigm ([conti2016pulp,](#bib.bib60) ): it features one I/O
    core and an 8-core SIMD-optimized DSP cluster accelerator using an extension of
    the RISC-V ISA. To manage this complexity, these MCUs include dedicated infrastructure
    for data marshaling, such as general-purpose DMA controllers to speed up memory
    transfers and reduce the memory access bottleneck.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代低功耗神经网络（NN）定向微控制器（MCU）的引入增加了这一需求，因为这些平台需要利用优化的软件和ISA扩展来进行深度神经网络（DNN）计算，同时处理传统的控制和I/O绑定活动。为了实现这两种任务的最佳执行，这些MCU采用了并行和异构处理。ST
    微电子¹¹1[https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html](https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html)和NXP最近推出了新一代双核微控制器，其中包括一个专门用于I/O的ARM
    M0处理器和一个具有单周期乘加和SIMD功能的ARM M4处理器。这些平台在内存层次结构方面的复杂性比传统的平面内存MCU有所增加，具有针对速度优化的L1内存和针对容量优化的L2内存。同时，内存层次结构的显式管理趋势越来越明显，手动调整的数据缓存具备数据管理锁定功能。例如，Kendrite
    K210²²2https://canaan.io/product/kendryteai 是一个基于RISC-V的双核64位片上系统，具有神经网络处理器（KPU），可将计算卸载到核心上。它还包括专门用于NN加速器的内存银行和一个DMA单元，用于显式管理数据传输。SONY
    Spresense板³³3https://developer.sony.com/develop/spresense/ 配备了一个最大时钟频率为156 MHz的6核M4加速器，1.5
    MB的SRAM和8 MB的Flash。GreenWaves Technologies GAP-8 ([flamand2018gap,](#bib.bib83)
    )片上系统于2018年推出，作为并行超低功耗范式 ([conti2016pulp,](#bib.bib60) )的商业体现：它配备一个I/O核心和一个使用RISC-V
    ISA扩展的8核SIMD优化DSP集群加速器。为了管理这种复杂性，这些MCU包括专门的数据封送基础设施，如通用DMA控制器，以加速内存传输并减少内存访问瓶颈。
- en: New tools such as TFLite Micro ([david2020tensorflow,](#bib.bib66) ) and the
    Larq Computing Engine (LCE) ([Larq,](#bib.bib91) ) offer a model-agnostic deployment
    framework and overcome these problems. Both are non-vendor-locked tools supporting
    ARM Cortex-M and RISC-V cores. Their library memory footprints require only 16
    kB on a Cortex-M3; however, by default they rely on graph interpretation at runtime,
    limiting achievable performance. To offset this limitation, TFLite Micro allows
    plugging in optimized kernels and declaring vectors in different memory regions.
    However, it does not include any tiling mechanism to execute layers that do not
    fit on-chip memory.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 新工具如TFLite Micro ([david2020tensorflow,](#bib.bib66) )和Larq计算引擎（LCE） ([Larq,](#bib.bib91)
    )提供了一个与模型无关的部署框架，并解决了这些问题。两者都是不受供应商锁定的工具，支持ARM Cortex-M和RISC-V核心。它们的库内存占用在Cortex-M3上仅需16
    kB；然而，默认情况下它们依赖于运行时图解释，限制了可实现的性能。为了弥补这一限制，TFLite Micro允许插入优化内核并在不同内存区域声明向量。然而，它不包括任何用于执行无法适配片上内存的层的切分机制。
- en: The two most powerful DNN deployment tools for microcontrollers available in
    the state-of-the-art have been proposed by the industry as proprietary, vendor-locked
    solutions for their own MCUs. X-CUBE-AI ([CubeAI,](#bib.bib213) ) from STMicroelectronics
    is an automatic NN library generator optimized on computation and memory. It converts
    a pre-trained DNN model from DNN tools such as Tensorflow into a precompiled library
    for the ARM Cortex-M cores embedded in STM32 series MCUs. X-CUBE-AI relies on
    relatively large on-chip L1 caches (up to 16 kB) to deliver performance on STM32
    MCUs, and it does not tackle software-based memory management. On the other hand,
    GWT designed a tool called AutoTiler, to target the GAP-8 RISC-V-based multi-core
    ultra-low-power microcontroller. One of its primary functions is to take a pre-trained
    DNN and generate code for memory tiling and efficient transfers of weight and
    activation data between all memory levels (on- and off-chip). The GWT AutoTiler
    directly tackles the data-movement and tile sizing challenge to optimize memory
    access, reaching state-of-the-art performance on the execution of many networks.
    The tool is proprietary, but its backend basic kernels are available as open-source
    as part of the GAP-8 SDK⁴⁴4https://github.com/GreenWaves-Technologies/gap_sdk.
    DORY ([burrello2021dory,](#bib.bib30) ) targets the same platform with an open-source
    tool. It optimizes the memory traffic for DNN deployment on specialized edge devices.
    By generating C code that tiles the execution of a dedicated kernel library, DORY
    reduces the size of intermediate buffers. This is crucial since microcontrollers
    often have limited level-1 (L1) memory. To achieve this, DORY formalizes tiling
    as an optimized constraint programming problem with kernel-specific heuristics.
    The produced code is more optimized but less general than previous solutions.
    Using DORY on a new architecture requires creating a new dedicated kernel library,
    new templates, and reprogramming the tiler to tailor it to specific hardware.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在最先进的技术中，行业提出了两个最强大的DNN部署工具，作为针对其自身MCU的专有、厂商锁定解决方案。STMicroelectronics的X-CUBE-AI
    ([CubeAI,](#bib.bib213)) 是一个自动化的神经网络库生成器，针对计算和内存进行了优化。它将来自DNN工具（如Tensorflow）的预训练DNN模型转换为针对STM32系列MCU中ARM
    Cortex-M核心的预编译库。X-CUBE-AI依赖于相对较大的片上L1缓存（最多16 kB）来提供STM32 MCU上的性能，并且没有处理基于软件的内存管理。另一方面，GWT设计了一个名为AutoTiler的工具，目标是GAP-8
    RISC-V基础的多核超低功耗微控制器。其主要功能之一是接收一个预训练的DNN并生成用于内存分块和高效传输权重及激活数据的代码，涵盖所有内存层级（片上和片外）。GWT
    AutoTiler直接解决了数据移动和块大小的挑战，以优化内存访问，实现了许多网络执行中的最先进性能。该工具是专有的，但其基本后端内核作为开源软件提供，作为GAP-8
    SDK的一部分⁴⁴([链接](https://github.com/GreenWaves-Technologies/gap_sdk))。DORY ([burrello2021dory,](#bib.bib30))
    使用开源工具针对相同平台。它优化了专用边缘设备上DNN部署的内存流量。通过生成C代码来分块执行专用内核库，DORY减少了中间缓冲区的大小。这一点至关重要，因为微控制器通常具有有限的一级（L1）内存。为此，DORY将分块形式化为具有内核特定启发式的优化约束编程问题。生成的代码比以前的解决方案更优化但更少通用。在新架构上使用DORY需要创建新的专用内核库、新模板，并重新编程分块器以适应特定硬件。
- en: 6.3\. Deep Learning Compilers for High-Performance
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 高性能深度学习编译器
- en: 'A popular DNN deployment framework that targets both high-performance embedded
    and edge devices is TVM ([chen2018tvm,](#bib.bib47) ). Its primary optimization
    mechanism is autotuning: it quickly compiles differently-scheduled yet equivalent
    kernel implementations, and after running those on hardware, the most optimal
    kernel is selected. As such, TVM can implicitly improve the execution time on
    CPUs and GPUs and fine-grained general matrix multiply (GEMM) accelerators like
    VTA ([moreau2019hardware,](#bib.bib154) ). Moreover, TVM runtime can link in (vendor-provided)
    optimized kernels in LLVM IR, CUDA C, C/C++ into a standalone artifact with the
    bring your own codegen (BYOC) ([chen2021bring,](#bib.bib50) ) infrastructure.
    However, using the autotuning pipeline of TVM is impractical for specialized coarse-grained
    accelerators since proving coarse-grained kernel equivalence requires complex
    loop nest analysis. This can be bypassed by using BYOC, but in this way, many
    of the automatic optimization opportunities presented by the framework are lost.
    HTVM ([vandelmHTVMEfficientNeural2023a,](#bib.bib223) ) uses DORY as a backend
    of TVM employing this technique.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个受欢迎的 DNN 部署框架，旨在支持高性能的嵌入式和边缘设备，是 TVM ([chen2018tvm,](#bib.bib47))。它的主要优化机制是自动调优：它快速编译不同调度但等效的内核实现，并在硬件上运行这些实现，最终选择最优的内核。因此，TVM
    可以隐式地提高 CPU 和 GPU 以及像 VTA ([moreau2019hardware,](#bib.bib154)) 这样的细粒度通用矩阵乘法（GEMM）加速器的执行时间。此外，TVM
    运行时可以将（供应商提供的）优化内核以 LLVM IR、CUDA C、C/C++ 的形式链接成一个独立的产物，借助自定义代码生成（BYOC） ([chen2021bring,](#bib.bib50))
    基础设施。然而，使用 TVM 的自动调优管道对于专用的粗粒度加速器是不切实际的，因为证明粗粒度内核等效性需要复杂的循环嵌套分析。可以通过使用 BYOC 绕过这一点，但这样一来，框架提供的许多自动优化机会就会丧失。HTVM
    ([vandelmHTVMEfficientNeural2023a,](#bib.bib223)) 使用 DORY 作为 TVM 的后端，采用这种技术。
- en: A popular research avenue has been to increase the level of abstraction to compile
    DL-based applications, using Domain Specific Language that mainly addresses tensor-level
    representations, such as the early examples of Halide ([ragan2013halide,](#bib.bib182)
    ) and Tensor Comprehensions ([vasilacheTensorComprehensionsFrameworkAgnostic2018,](#bib.bib228)
    ). Dedicated DL compilers such as Glow ([rotemGlowGraphLowering2019,](#bib.bib187)
    ) have been focused on graph-lowering techniques, using these earlier developments
    and ideas to build up systems that take a high-level description of a DL program,
    typically in the form a data-flow graph of operators, lower it into a set of IRs
    still centered on tensor-aware operations, and then deploy on target machine-specific
    code. A common graphical format for the input of such lowering passes is ONNX⁵⁵5https://onnx.ai/,
    whereas intermediate representations can be custom and dedicated to one particular
    framework (e.g., Relay ([roeschRelayNewIR2018,](#bib.bib185) ) for Amazon’s open
    source NNVM compiler) or deployed as a specialization of a more general IR ([linONNCCompilationFramework2019,](#bib.bib142)
    ). In this regard, the most relevant example is MLIR ([lattner2021mlir,](#bib.bib137)
    ; [jinCompilingONNXNeural2020,](#bib.bib122) ), a framework proposed in the context
    of the LLVM project that enables building custom intermediate representations
    for domain-specific computing. While this tool is not exclusive to DL, it has
    been proposed in response to the needs of the DL community and has quickly risen
    to prominence.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个受欢迎的研究方向是通过增加抽象层级来编译基于 DL 的应用程序，使用主要处理张量级表示的领域特定语言，例如早期的 Halide ([ragan2013halide,](#bib.bib182))
    和 Tensor Comprehensions ([vasilacheTensorComprehensionsFrameworkAgnostic2018,](#bib.bib228))。专用的
    DL 编译器，如 Glow ([rotemGlowGraphLowering2019,](#bib.bib187))，专注于图形降低技术，利用这些早期的发展和想法构建系统，这些系统接收
    DL 程序的高层描述，通常以操作符的数据流图形式，将其降低到以张量感知操作为中心的一组 IR 中，然后部署到目标机器的特定代码上。此类降低过程的常见图形格式是
    ONNX⁵⁵5https://onnx.ai/，而中间表示可以是自定义的，专用于某一特定框架（例如，Amazon 的开源 NNVM 编译器的 Relay ([roeschRelayNewIR2018,](#bib.bib185)))
    或作为更通用 IR 的专门化（[linONNCCompilationFramework2019,](#bib.bib142)）。在这方面，最相关的例子是 MLIR
    ([lattner2021mlir,](#bib.bib137); [jinCompilingONNXNeural2020,](#bib.bib122))，这是在
    LLVM 项目的背景下提出的框架，能够为领域特定计算构建自定义中间表示。虽然这个工具并非专属于 DL，但它是为了满足 DL 社区的需求而提出的，并迅速崛起。
- en: 7\. Modeling, Simulation, Profiling and Exploration
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 建模、仿真、剖析与探索
- en: To effectively design a hardware accelerator for DL, it is essential to have
    access to powerful modeling tools that can provide detailed insights into the
    power consumption, performance, and area requirements of the accelerator. In this
    section, we will explore some of the most popular and effective tools available
    for modeling hardware accelerators for DL, and discuss their key features and
    capabilities. These tools enable designers to experiment with various design choices
    and configurations and to optimize their designs for specific Power Performance
    Area (PPA) metrics.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效设计用于深度学习的硬件加速器，必须使用强大的建模工具，这些工具可以提供关于加速器的功耗、性能和面积需求的详细见解。在本节中，我们将探讨一些用于建模深度学习硬件加速器的最受欢迎和有效的工具，并讨论它们的主要特性和能力。这些工具使设计人员能够尝试各种设计选择和配置，并优化其设计以满足特定的功率性能面积（PPA）指标。
- en: Figure [11](#S7.F11 "Figure 11 ‣ 7\. Modeling, Simulation, Profiling and Exploration
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") provides a taxonomy of the various tools, frameworks, and methodologies
    discussed in the next subsections.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [11](#S7.F11 "Figure 11 ‣ 7\. Modeling, Simulation, Profiling and Exploration
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") 提供了下一个小节中讨论的各种工具、框架和方法的分类。
- en: '{forest}'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Modeling,
    Simulation, Profiling, and Exploration [Modeling, Simulation,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 圆形/.style=ellipse,draw, 方形/.style=rectangle,draw, qtree, [建模、仿真、分析和探索 [建模、仿真，
- en: and Exploration [ NVDLA ([nvdla,](#bib.bib167) ), MLPAT ([tang_dossa18,](#bib.bib218)
    )
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 和探索 [ NVDLA ([nvdla,](#bib.bib167) ), MLPAT ([tang_dossa18,](#bib.bib218) )
- en: MAESTRO ([kwon_micro20,](#bib.bib133) ), Timeloop ([parashar_ispass19,](#bib.bib172)
    )
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: MAESTRO ([kwon_micro20,](#bib.bib133) ), Timeloop ([parashar_ispass19,](#bib.bib172)
    )
- en: LAMBDA ([russo_percom21,](#bib.bib188) ), DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249)
    )
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: LAMBDA ([russo_percom21,](#bib.bib188) ), DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249)
    )
- en: DNNExplorer ([zhang2021beingahead,](#bib.bib248) ), Gemmini ([genc_dac21,](#bib.bib92)
    )
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DNNExplorer ([zhang2021beingahead,](#bib.bib248) ), Gemmini ([genc_dac21,](#bib.bib92)
    )
- en: Interstellar ([yang_asplos20,](#bib.bib241) ), Aladdin ([shao_isca14,](#bib.bib199)
    ) ] ], [Simulation tools for emerging
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Interstellar ([yang_asplos20,](#bib.bib241) ), Aladdin ([shao_isca14,](#bib.bib199)
    ) ] ], [新兴模拟工具
- en: memories-based DNN accelerator [ DNN+NeuroSim ([peng2019iedm,](#bib.bib174)
    )
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内存的DNN加速器 [ DNN+NeuroSim ([peng2019iedm,](#bib.bib174) )
- en: SySCIM ([shadmehri2022date,](#bib.bib198) )
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: SySCIM ([shadmehri2022date,](#bib.bib198) )
- en: MemTorch ([Lammie2022,](#bib.bib135) )
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: MemTorch ([Lammie2022,](#bib.bib135) )
- en: MNSIM ([xia2016date,](#bib.bib237) )
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: MNSIM ([xia2016date,](#bib.bib237) )
- en: Reiser et al. ([reiser2023newcas,](#bib.bib184) ) ] ], [Cycle-Accurate Simulators
    [ SCALE-SIM ([samajdar2018scal,](#bib.bib193) )
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Reiser等人 ([reiser2023newcas,](#bib.bib184) ) ] ], [周期精确模拟器 [ SCALE-SIM ([samajdar2018scal,](#bib.bib193)
    )
- en: STONNE ([munozmartinez2020stonne,](#bib.bib158) )
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: STONNE ([munozmartinez2020stonne,](#bib.bib158) )
- en: SimuNN ([cao_jestcs20,](#bib.bib37) )
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: SimuNN ([cao_jestcs20,](#bib.bib37) )
- en: AccTLMSim ([kim2020transactionlevel,](#bib.bib130) )
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: AccTLMSim ([kim2020transactionlevel,](#bib.bib130) )
- en: QADAM ([inci2022qadam,](#bib.bib110) ), QAPPA ([inci2022qappa,](#bib.bib111)
    )
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: QADAM ([inci2022qadam,](#bib.bib110) ), QAPPA ([inci2022qappa,](#bib.bib111)
    )
- en: Juraci et al. ([juracy_tcs22,](#bib.bib124) ) ] ], [Modeling and Profiling FPGAs
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Juraci等人 ([juracy_tcs22,](#bib.bib124) ) ] ], [建模和分析FPGAs
- en: for custom accelerators [ Shuhai ([shuhai,](#bib.bib231) ), HPCChallenge ([hpcc-fpga,](#bib.bib149)
    )
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定制加速器 [ Shuhai ([shuhai,](#bib.bib231) ), HPCChallenge ([hpcc-fpga,](#bib.bib149)
    )
- en: HPCG Benchmark ([hpcg-fpga,](#bib.bib246) )
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: HPCG Benchmark ([hpcg-fpga,](#bib.bib246) )
- en: Da Silva et al. ([roofline-fpga-hls,](#bib.bib61) ), Siracusa et al. ([roofline-fpga-cad,](#bib.bib205)
    ; [roofline-fpga-cad2,](#bib.bib204) )
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Da Silva等人 ([roofline-fpga-hls,](#bib.bib61) ), Siracusa等人 ([roofline-fpga-cad,](#bib.bib205)
    ; [roofline-fpga-cad2,](#bib.bib204) )
- en: Muralidharan et al. ([roofline-multibench-fpga,](#bib.bib156) )
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Muralidharan等人 ([roofline-multibench-fpga,](#bib.bib156))
- en: ERT ([ert-opencl-fpga,](#bib.bib165) ; [ert-opencl-fpga2,](#bib.bib164) ) ]
    ] ]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ERT ([ert-opencl-fpga,](#bib.bib165) ; [ert-opencl-fpga2,](#bib.bib164) ) ]
    ] ]
- en: Figure 11. Modeling, Simulation, Profiling, and Exploration tools and methodologies
    discussed in Section [7](#S7 "7\. Modeling, Simulation, Profiling and Exploration
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures")
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图11. 本节讨论的建模、仿真、分析和探索工具及方法 [7](#S7 "7\. Modeling, Simulation, Profiling and
    Exploration ‣ A Survey on Design Methodologies for Accelerating Deep Learning
    on Heterogeneous Architectures")
- en: Table 2. Modeling, simulation, and exploration tools.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 定制加速器的建模、仿真和探索工具。
- en: '|  | Integration with NN frameworks | Model type | Full SoC | Evaluation metrics
    | Target | Estimation error |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 与NN框架的集成 | 模型类型 | 完整SoC | 评估指标 | 目标 | 估算误差 |'
- en: '| MLPAT ([tang_dossa18,](#bib.bib218) ) | No | Analytical | No | PPA | ASIC
    | ¡5% area ¡10% power |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| MLPAT ([tang_dossa18,](#bib.bib218) ) | 无 | 分析型 | 无 | PPA | ASIC | ¡5% 面积
    ¡10% 功耗 |'
- en: '| MAESTRO ([kwon_micro20,](#bib.bib133) ) | No | Empirical | No | Performance
    | ASIC | 5% |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| MAESTRO ([kwon_micro20,](#bib.bib133) ) | 无 | 实证型 | 无 | 性能 | ASIC | 5% |'
- en: '| Timeloop ([parashar_ispass19,](#bib.bib172) ) | No | Analytical/ Empirical
    | No | PPA | ASIC | 5% |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Timeloop ([parashar_ispass19,](#bib.bib172) ) | 无 | 分析型/实证型 | 无 | PPA | ASIC
    | 5% |'
- en: '| LAMBDA ([russo_percom21,](#bib.bib188) ) | No | Analytical/ Empirical | No
    | PPA | ASIC | 5% |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| LAMBDA ([russo_percom21,](#bib.bib188) ) | 无 | 分析型/实证型 | 无 | PPA | ASIC |
    5% |'
- en: '| DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249) ) | No | Analytical |
    No | Performance Energy | FPGA/ASIC | ¡18% |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249) ) | 无 | 分析型 | 无 | 性能与能量
    | FPGA/ASIC | ¡18% |'
- en: '| DNNExplorer ([zhang2021beingahead,](#bib.bib248) ) | Caffe, PyTorch | – |
    No | Performance | FPGA | – |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| DNNExplorer ([zhang2021beingahead,](#bib.bib248) ) | Caffe, PyTorch | – |
    无 | 性能 | FPGA | – |'
- en: '| Gemmini ([genc_dac21,](#bib.bib92) ) | No | Simulation | Yes + OS support
    | Performance | FPGA/ASIC | – |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Gemmini ([genc_dac21,](#bib.bib92) ) | 无 | 仿真 | 是 + 操作系统支持 | 性能 | FPGA/ASIC
    | – |'
- en: '| Interstellar ([yang_asplos20,](#bib.bib241) ) | No | Analytical | No | PPA
    | ASIC | 2% |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Interstellar ([yang_asplos20,](#bib.bib241) ) | 无 | 分析型 | 无 | PPA | ASIC
    | 2% |'
- en: '| Aladdin ([shao_isca14,](#bib.bib199) ) | No | Simulation Analytical | No
    | PPA | ASIC | 1% performance 5% power 7% area |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Aladdin ([shao_isca14,](#bib.bib199) ) | 无 | 仿真分析 | 无 | PPA | ASIC | 性能下降1%，功耗下降5%，面积下降7%
    |'
- en: '| SCALE-SIM ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) | No | Empirical | Yes | Performance, Area | ASIC | – |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| SCALE-SIM ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) | 无 | 实证型 | 有 | 性能，面积 | ASIC | – |'
- en: '| STONNE ([munozmartinez2020stonne,](#bib.bib158) ) | Caffe | Cycle level simulation
    | Yes | Performance | ASIC | ¡3% |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| STONNE ([munozmartinez2020stonne,](#bib.bib158) ) | Caffe | 周期级仿真 | 有 | 性能
    | ASIC | ¡3% |'
- en: '| SimuNN ([cao_jestcs20,](#bib.bib37) ) | TensorFlow | Cycle level simulation
    | Yes | PPA | FPGA/ASIC | – |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| SimuNN ([cao_jestcs20,](#bib.bib37) ) | TensorFlow | 周期级仿真 | 有 | PPA | FPGA/ASIC
    | – |'
- en: '| AccTLMSim ([kim2020transactionlevel,](#bib.bib130) ) | No | Cycle level simulation
    | Yes | Performance | ASIC | 3% |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| AccTLMSim ([kim2020transactionlevel,](#bib.bib130) ) | 无 | 周期级仿真 | 有 | 性能
    | ASIC | 3% |'
- en: '| Juracy *et al.* ([juracy_tcs22,](#bib.bib124) ) | TensorFlow | Cycle level
    simulation | No | PPA | ASIC | ¡7% |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Juracy *et al.* ([juracy_tcs22,](#bib.bib124) ) | TensorFlow | 周期级仿真 | 无
    | PPA | ASIC | ¡7% |'
- en: '| DNN-NeuroSim ([peng2019iedm,](#bib.bib174) ) | Tensorflow, PyTorch | Instruction
    accurate simulation | Yes | PPA | ASIC | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| DNN-NeuroSim ([peng2019iedm,](#bib.bib174) ) | Tensorflow, PyTorch | 指令精确仿真
    | 有 | PPA | ASIC | - |'
- en: '| SySCIM ([shadmehri2022date,](#bib.bib198) ) | No | Circuit level simulation
    | No | Accuracy | ASIC | $<$4% accuracy |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| SySCIM ([shadmehri2022date,](#bib.bib198) ) | 无 | 电路级仿真 | 无 | 精度 | ASIC |
    <$4% 精度 |'
- en: '| Memtorch ([Lammie2022,](#bib.bib135) ) | PyTorch | Analytical/ Empirical
    | Yes | PPA | ASIC | - |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Memtorch ([Lammie2022,](#bib.bib135) ) | PyTorch | 分析型/实证型 | 有 | PPA | ASIC
    | - |'
- en: '| MNSIM ([xia2016date,](#bib.bib237) ) | No | Cycle level simulation | Yes
    | PPA | ASIC | - |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| MNSIM ([xia2016date,](#bib.bib237) ) | 无 | 周期级仿真 | 有 | PPA | ASIC | - |'
- en: 7.1\. Modeling, Simulation, and Exploration Frameworks
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 建模、仿真和探索框架
- en: In this section, we overview the most recent and influential frameworks utilized
    for modeling, simulating, and exploring the design space of hardware accelerators
    for DL.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了用于建模、仿真和探索深度学习硬件加速器设计空间的最新和最具影响力的框架。
- en: 'The NVIDIA DL Accelerator (NVDLA) ([nvdla,](#bib.bib167) ) is an open-source
    framework designed to facilitate the implementation of machine learning (ML) applications.
    It includes a training infrastructure and a compiler to convert existing models
    for use by the NVDLA software. NVDLA can read a neural network from a front-end
    environment, like Caffe, and map it to the NVIDIA accelerator. The MLPAT framework ([tang_dossa18,](#bib.bib218)
    ) enables the modeling of power, area, and timing for ML accelerators including
    architecture components such as systolic arrays, memories, dataflows and activation
    pipelines, as well as different precision types and technologies. MAESTRO ([kwon_micro20,](#bib.bib133)
    ) is a framework designed to analyze and describe neural network processing engines
    to explore the power/performance tradeoffs to implement a target architecture.
    It features a domain-specific language for dataflow description, which enables
    the specification of parameters such as the number of processing elements, memory
    size, and Network-on-Chip bandwidth. The Timeloop ([parashar_ispass19,](#bib.bib172)
    ) infrastructure helps to explore the architecture design space of DNN accelerators.
    It consists of two main components: a model that provides projections for performance,
    area, and energy, and a mapper that constructs and searches through the design
    space of a given workload on a targeted architecture. To use Timeloop, the user
    describes the architecture’s organization using a configurable template that includes
    abstractions for compute units, memories, and communication links. The mapper
    then constructs the mapspace and searches for an optimal mapping using the model’s
    speed and accuracy. Accelergy ([wu_iccad19,](#bib.bib236) ) is a versatile energy
    estimation technique for accelerators: it enables designers to create specifications
    using custom high-level compound components and low-level primitive components,
    which can be evaluated using third-party energy estimation plug-ins. LAMBDA ([russo_percom21,](#bib.bib188)
    ) is a framework based on the Timeloop/Accelergy infrastructure to explore the
    design space of configurable DNN accelerators taking into account a variety of
    architectural and microarchitectural parameters.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA DL 加速器（NVDLA）（[nvdla,](#bib.bib167)）是一个开源框架，旨在促进机器学习（ML）应用的实现。它包括一个训练基础设施和一个编译器，用于将现有模型转换为
    NVDLA 软件可用的格式。NVDLA 可以从前端环境（如 Caffe）读取神经网络，并将其映射到 NVIDIA 加速器上。MLPAT 框架（[tang_dossa18,](#bib.bib218)）使得对包括体系结构组件（如流水阵列、内存、数据流和激活管道）在内的
    ML 加速器进行功耗、面积和时序建模成为可能，并支持不同的精度类型和技术。MAESTRO（[kwon_micro20,](#bib.bib133)）是一个用于分析和描述神经网络处理引擎的框架，以探索实现目标架构的功耗/性能权衡。它具有一种用于数据流描述的领域特定语言，可以指定如处理单元数量、内存大小和芯片网络带宽等参数。Timeloop（[parashar_ispass19,](#bib.bib172)）基础设施有助于探索
    DNN 加速器的架构设计空间。它由两个主要组件组成：一个提供性能、面积和能量预测的模型，以及一个在目标架构上构建和搜索给定工作负载设计空间的映射器。使用 Timeloop
    时，用户通过一个包含计算单元、内存和通信链路抽象的可配置模板描述架构的组织。然后，映射器构建映射空间，并利用模型的速度和准确性搜索最佳映射。Accelergy（[wu_iccad19,](#bib.bib236)）是一种多用途的加速器能量估算技术：它使设计人员能够使用自定义的高级复合组件和低级原始组件创建规格，这些组件可以通过第三方能量估算插件进行评估。LAMBDA（[russo_percom21,](#bib.bib188)）是一个基于
    Timeloop/Accelergy 基础设施的框架，用于探索考虑各种架构和微架构参数的可配置 DNN 加速器的设计空间。
- en: 'The DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249) ) can predict the energy
    consumption, throughput, and latency of DNN accelerators before they are implemented.
    It offers two advantages: (1) it uses an analytical performance formulation to
    enable rapid exploration and optimization of DNN ASIC/FPGA accelerator designs;
    (2) it supports different algorithm-to-hardware mappings and architectures. DNNExplorer ([zhang2021beingahead,](#bib.bib248)
    ) can be used to evaluate customized hardware DNN accelerators and to explore
    new accelerator designs with respect to performance and efficiency. It supports
    popular ML frameworks (Caffe and PyTorch) for analyzing DNN workloads and providing
    analytical models for accelerator benchmarking. It has a high-dimensional design
    space and fine-grained adjustability to overcome design limitations, and a design
    space exploration engine to generate optimized accelerators based on target AI
    workloads and hardware resources.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249)) 可以在 DNN 加速器实现之前预测其能源消耗、吞吐量和延迟。它有两个优点：（1）它使用分析性能公式，能够快速探索和优化
    DNN ASIC/FPGA 加速器设计；（2）它支持不同的算法到硬件映射和架构。DNNExplorer ([zhang2021beingahead,](#bib.bib248))
    可用于评估定制硬件 DNN 加速器，并探索新加速器设计的性能和效率。它支持流行的 ML 框架（Caffe 和 PyTorch），用于分析 DNN 工作负载并提供加速器基准测试的分析模型。它具有高维度设计空间和精细调节能力，克服设计限制，并具有设计空间探索引擎，以根据目标
    AI 工作负载和硬件资源生成优化的加速器。
- en: Gemmini ([genc_dac21,](#bib.bib92) ) is an open-source DNN accelerator generator
    that enables users to design custom hardware accelerator systems for rapidly evolving
    DNN workloads. It provides a complete solution that spans both hardware and software
    stack, and it is compatible with the RISC-V ecosystem. Gemmini’s hardware design
    options can be tuned for performance, efficiency, and extensibility. It implements
    a multi-level software stack with an easy-to-use programming interface and tight
    integration with Linux-capable SoCs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Gemmini ([genc_dac21,](#bib.bib92)) 是一个开源 DNN 加速器生成器，允许用户设计用于快速演变的 DNN 工作负载的定制硬件加速器系统。它提供了一个涵盖硬件和软件堆栈的完整解决方案，并且与
    RISC-V 生态系统兼容。Gemmini 的硬件设计选项可以针对性能、效率和扩展性进行调优。它实现了一个多层次的软件堆栈，具有易于使用的编程接口，并与支持
    Linux 的 SoC 紧密集成。
- en: DNN accelerator micro-architectures and their program mappings are specific
    choices of loop order and hardware parallelism for computing the nested loops
    of DNNs. It has been observed that these hardware variants can be precisely and
    concisely represented by Halide’s scheduling language. In Interstellar ([yang_asplos20,](#bib.bib241)
    ), modifications were made to the Halide compiler to generate hardware for fair
    comparisons with prior accelerators. Interstellar highlights the significance
    of optimizing the memory hierarchy with a higher impact on energy metrics than
    the dataflow selection. Aladdin ([shao_isca14,](#bib.bib199) ) is a simulation
    tool for the quick exploration of design options of systems focused on accelerators.
    It is a pre-RTL and power-performance simulator that takes as input algorithm
    descriptions in high-level languages and uses dynamic data dependence graphs to
    represent an accelerator without the need to generate RTL.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 加速器的微架构及其程序映射是针对 DNN 嵌套循环的特定循环顺序和硬件并行性的选择。观察发现，这些硬件变体可以通过 Halide 的调度语言准确而简洁地表示。在
    Interstellar ([yang_asplos20,](#bib.bib241)) 中，对 Halide 编译器进行了修改，以生成硬件以便与先前的加速器进行公平比较。Interstellar
    强调了优化内存层次结构的重要性，它对能源指标的影响高于数据流选择。Aladdin ([shao_isca14,](#bib.bib199)) 是一个用于快速探索专注于加速器系统设计选项的仿真工具。它是一个前
    RTL 和功耗性能模拟器，输入高层次语言的算法描述，使用动态数据依赖图表示加速器，无需生成 RTL。
- en: 7.2\. Simulation tools for emerging memories-based DNN accelerator
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 基于新兴存储器的 DNN 加速器仿真工具
- en: Another set of tools for DNN modeling, simulation, and profiling is that related
    to emerging memories-based accelerators. DNN+NeuroSim ([peng2019iedm,](#bib.bib174)
    ) is an integrated framework to benchmark compute-in-memory (CIM) accelerators
    for DNNs, with hierarchical design options from the device level, to circuit-level
    and up to the algorithm level. A Python wrapper is developed to interface NeuroSim
    with popular ML platforms such as Pytorch and Tensorflow. The framework supports
    the automatic mapping of algorithms to hardware and the evaluation of chip-level
    performance and inference accuracy with hardware constraints. SySCIM ([shadmehri2022date,](#bib.bib198)
    ) considers the impact of the non-idealities of the CIM components, including
    memristor device, memristor crossbar (interconnects), analog-to-digital converter,
    and trans-impedance amplifier, on the vector-matrix multiplication performed by
    the CIM unit. The CIM modules are described in SystemC and SystemC-AMS to reach
    a high simulation speed while maintaining simulation accuracy. MemTorch ([Lammie2022,](#bib.bib135)
    ) is an open-source framework for customized large-scale memristive DL simulations,
    with a refined focus on the co-simulation of device non-idealities. MemTorch also
    facilitates the co-modeling of key crossbar peripheral circuitry. MemTorch adopts
    a modernized software engineering methodology and integrates directly with the
    well-known PyTorch ML library.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组用于 DNN 建模、模拟和分析的工具是与新兴记忆体加速器相关的。DNN+NeuroSim ([peng2019iedm,](#bib.bib174))
    是一个集成框架，用于对 DNN 的计算内存 (CIM) 加速器进行基准测试，具有从器件级到电路级再到算法级的分层设计选项。开发了一个 Python 包装器，以将
    NeuroSim 与流行的 ML 平台如 Pytorch 和 Tensorflow 连接。该框架支持算法到硬件的自动映射，并评估具有硬件约束的芯片级性能和推理准确性。SySCIM
    ([shadmehri2022date,](#bib.bib198)) 考虑了 CIM 组件的非理想性，包括忆阻器设备、忆阻器十字交叉（互连）、模拟到数字转换器和跨阻放大器，对
    CIM 单元执行的向量矩阵乘法的影响。CIM 模块在 SystemC 和 SystemC-AMS 中描述，以实现高模拟速度，同时保持模拟精度。MemTorch
    ([Lammie2022,](#bib.bib135)) 是一个开源框架，用于定制的大规模忆阻 DL 模拟，重点关注设备非理想性的协同模拟。MemTorch
    还促进了关键十字交叉外围电路的共同建模。MemTorch 采用了现代化的软件工程方法，并与著名的 PyTorch ML 库直接集成。
- en: MNSIM ([xia2016date,](#bib.bib237) ) proposes a simulation platform for the
    memristor-based neuromorphic system with a hierarchical structure and flexible
    interfaces for customization. A detailed reference design is provided for large-scale
    applications like ISAAC or PRIME accelerators demonstrated in the previous deliverable.
    A behavior-level computing accuracy model is incorporated to evaluate the computing
    error rate affected by interconnect lines and nonideal device factors. Experimental
    results show that MNSIM achieves over 7000 times more speed-up than SPICE simulation.
    MNSIM can optimize the design and estimate the tradeoff relationships among different
    performance metrics for users. In ([reiser2023newcas,](#bib.bib184) ), it is proposed
    a simulation framework together with suitable abstractions to propagate the effects
    of RRAM crossbar configuration parameters to their ultimate implications over
    inference performance stability. RRAM devices’ non-idealities result in significant
    inference accuracy drops compared to software baseline accuracy. A critical issue
    is related to the drift of the conductance states appearing immediately at the
    end of the program and verifying algorithms that are mandatory for accurate multi-level
    conductance operation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: MNSIM ([xia2016date,](#bib.bib237)) 提出了一个用于基于忆阻器的神经形态系统的模拟平台，该平台具有分层结构和灵活的定制接口。为像
    ISAAC 或 PRIME 加速器等大规模应用提供了详细的参考设计，这些应用在之前的交付物中有所展示。包含一个行为级计算精度模型，用于评估由互连线路和非理想设备因素影响的计算误差率。实验结果表明，MNSIM
    比 SPICE 模拟快超过 7000 倍。MNSIM 可以优化设计，并为用户估算不同性能指标之间的权衡关系。在 ([reiser2023newcas,](#bib.bib184))
    中，提出了一个模拟框架以及合适的抽象，以传播 RRAM 十字交叉配置参数的影响到其对推理性能稳定性的最终影响。与软件基准准确度相比，RRAM 设备的非理想性导致了显著的推理准确度下降。一个关键问题与程序结束时出现的导电状态漂移有关，并验证了准确的多级导电操作所必需的算法。
- en: 7.3\. Cycle-Accurate Simulators
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 循环精确模拟器
- en: 'For accurate simulations, it is crucial to model the behavior of the hardware
    accelerator at the cycle-by-cycle level, accounting for all the interactions between
    the different hardware components. Among these simulation tools, SCALE-SIM (SystoliC
    AcceLErator SIMulator) ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) is a simulator that provides cycle-accurate energy/performance modeling for
    DNN accelerators by considering various factors such as on-chip and off-chip memory
    accesses, and interface bandwidth. It has two primary components: (i) a compute
    unit that utilizes a systolic array that can be customized according to size and
    aspect ratio, and (ii) an accelerator memory system that features three double-buffered
    SRAM memories with user-specified sizes. STONNE (Simulation Tool for Neural Network
    Engines) ([munozmartinez2020stonne,](#bib.bib158) ) is a highly modular and extensible
    simulation framework for the end-to-end evaluation of flexible accelerator DNN
    architectures with cycle accuracy. Like Timeloop, STONNE uses the Accelergy energy
    estimation tool to estimate energy and area. SimuNN ([cao_jestcs20,](#bib.bib37)
    ) is a pre-RTL neural network simulator for early phase verification and fast
    prototyping before the design is converted into hardware. It supports different
    data precisions and it is compatible with TensorFlow. SimuNN provides multi-level
    trace results that can be used as a reference for the final hardware design. Additionally,
    it can evaluate the hardware performance under various quantizations, dataflows,
    and configurations based on a generalized hardware model. AccTLMSim ([kim2020transactionlevel,](#bib.bib130)
    ) is a pre-RTL simulation tool based on SystemC transaction-level modeling (TLM)
    to simulate CNN accelerators with cycle accuracy. The tool includes a detailed
    model of the interface with the DRAM for precise tracking of each bus transaction
    between the accelerator and DRAM while considering the communication bandwidth.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于准确的仿真，至关重要的是在周期级别建模硬件加速器的行为，考虑不同硬件组件之间的所有相互作用。在这些仿真工具中，SCALE-SIM（SystoliC
    AcceLErator SIMulator） ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) 是一种提供 DNN 加速器的周期准确能量/性能建模的仿真器，通过考虑各种因素，如片上和片外内存访问以及接口带宽。它有两个主要组件：(i) 一个利用可以根据大小和纵横比定制的
    systolic array 的计算单元，(ii) 一个加速器内存系统，具有三种用户指定大小的双缓冲 SRAM 内存。STONNE（Simulation Tool
    for Neural Network Engines） ([munozmartinez2020stonne,](#bib.bib158) ) 是一个高度模块化和可扩展的仿真框架，用于对具有周期准确性的灵活加速器
    DNN 架构进行端到端评估。像 Timeloop 一样，STONNE 使用 Accelergy 能量估算工具来估算能量和面积。SimuNN ([cao_jestcs20,](#bib.bib37)
    ) 是一种用于设计转换为硬件之前进行早期验证和快速原型制作的预 RTL 神经网络仿真器。它支持不同的数据精度，并与 TensorFlow 兼容。SimuNN
    提供多级跟踪结果，可作为最终硬件设计的参考。此外，它可以基于通用硬件模型评估各种量化、数据流和配置下的硬件性能。AccTLMSim ([kim2020transactionlevel,](#bib.bib130)
    ) 是一种基于 SystemC 事务级建模（TLM）的预 RTL 仿真工具，用于以周期准确性模拟 CNN 加速器。该工具包括与 DRAM 接口的详细模型，以精确跟踪加速器与
    DRAM 之间的每个总线事务，同时考虑通信带宽。
- en: QADAM ([inci2022qadam,](#bib.bib110) ) and its evolution QAPPA ([inci2022qappa,](#bib.bib111)
    ) are parameterized RTL frameworks designed to model power, performance, and area
    of quantization-aware DNN accelerators. These frameworks support design space
    exploration and Pareto-efficiency analysis for a range of design choices, including
    bit precision, processing element (PE) type, scratchpad sizes of PEs, global buffer
    size, total number of PEs, and DNN configurations. The DSE approach proposed in ([juracy_tcs22,](#bib.bib124)
    ) for CNNs employs an analytical model derived from the physical synthesis of
    hardware accelerators. This model is integrated into CNN frameworks such as TensorFlow,
    enabling precise outcomes. The analytical model provides estimates for various
    factors, including area, performance, power, energy, and memory accesses. The
    accuracy of the model was tested by comparing it to data obtained from physical
    synthesis, and it was observed that the average error was less than 7%.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: QADAM ([inci2022qadam,](#bib.bib110) )及其演变版 QAPPA ([inci2022qappa,](#bib.bib111)
    )是参数化的 RTL 框架，旨在建模量化感知 DNN 加速器的功耗、性能和面积。这些框架支持设计空间探索和 Pareto 效率分析，涵盖各种设计选择，包括位精度、处理元素（PE）类型、PE
    的临时存储器大小、全局缓冲区大小、PE 总数以及 DNN 配置。 ([juracy_tcs22,](#bib.bib124) ) 提出的 CNN 设计空间探索方法采用了从硬件加速器物理合成中得出的分析模型。该模型集成到如
    TensorFlow 等 CNN 框架中，实现了精确的结果。该分析模型提供了各种因素的估算，包括面积、性能、功耗、能量和内存访问。通过将模型与从物理合成中获得的数据进行比较来测试其准确性，结果显示平均误差小于
    7%。
- en: 7.4\. Modeling and Profiling FPGAs for custom accelerators
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 为自定义加速器建模和分析 FPGA
- en: The use of off-the-shelf highly-parallel hardware accelerators to boost the
    performance of software applications, in particular DL algorithms, represents
    nowadays a very common option, adopted by a large and increasing number of HPC
    systems. In this scenario, GPUs are definitively the most common accelerators,
    while some data centers have only recently started to adopt FPGAs to speed-up
    network interconnects ([project-catapult,](#bib.bib180) ), and specific workloads ([fpga-datacenter,](#bib.bib17)
    ) such as ML inference algorithms ([fpga-dl,](#bib.bib200) ; [project-brainwave,](#bib.bib85)
    ). FPGAs could represent an interesting trade-off, allowing user customizations,
    as well as the use of off-the-shelf hardware, to implement custom DL accelerators.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，使用现成的高度并行硬件加速器来提升软件应用的性能，尤其是深度学习算法，已经成为一个非常普遍的选项，被越来越多的 HPC 系统所采用。在这种情况下，GPU
    绝对是最常见的加速器，而一些数据中心直到最近才开始采用 FPGA 来加速网络互连 ([project-catapult,](#bib.bib180) )，以及特定的工作负载 ([fpga-datacenter,](#bib.bib17)
    )，例如 ML 推断算法 ([fpga-dl,](#bib.bib200) ; [project-brainwave,](#bib.bib85) )。FPGA
    可能代表一种有趣的折中选择，既允许用户自定义，也可以使用现成的硬件，以实现定制的 DL 加速器。
- en: Given the rapidly increasing use of ML methods in several application fields,
    and the interest in reconfigurable architectures, which is rising in the HPC community
    since several years ([fpga-hpc,](#bib.bib225) ; [fpga-hpc-trends,](#bib.bib229)
    ; [fpga-hpc2,](#bib.bib76) ), we might expect FPGAs to become a more common option,
    as accelerators, for next-generation HPC systems. In the past, several reasons
    have prevented this shift. First, FPGAs were not designed to provide high floating-point
    (FP) computing performance ([fpga-hpc-trends,](#bib.bib229) ), while typical HPC
    workloads usually require double-precision (DP) and single-precision (SP) FP computations.
    Secondly, FPGA programming could be a very time-consuming process, requiring the
    use of specific hardware programming skills and the use of programming languages
    not common among HPC developers communities ([fpga-programming,](#bib.bib21) ).
    Thirdly, the code written for one FPGA could hardly run across different devices
    without a complete re-design, causing serious portability issues not acceptable
    for a wide set of HPC applications, for which even the porting to GPUs had been
    a long and suffered process ([decade-gpu-porting,](#bib.bib224) ). However, more
    recently, these barriers started to fade thanks to improvements in hardware architectures
    and programming frameworks. The latest generations of FPGAs integrate thousands
    of programmable DSPs (Digital Signal Processors) able to implement SP- and DP-FP
    operations ([fpga-dsp-fp,](#bib.bib29) ; [mapping-dsp,](#bib.bib186) ; [xilinx-dsp-flops,](#bib.bib226)
    ), and might also embed custom FP DSP blocks. This is leading to devices capable
    to reach a performance in the same order of magnitude as commodity HPC processors
    (i.e. TFLOP/s), and in some cases able to deliver a better energy-efficiency ([altera-dsp-flops,](#bib.bib23)
    ; [fpga-opencl-hpc,](#bib.bib252) ). At the same time, the recent improvements
    in synthesis tools, and the development of new programming approaches such as
    HLS  ([survey-hls,](#bib.bib161) ), allow programmers to develop codes using high-level
    languages. These approaches are very similar to those (e.g. OpenMP and OpenACC)
    commonly used by HPC developers to target multi-core CPUs and other accelerators,
    which are also able to guarantee a fair level of code portability ([ompss-fpga2,](#bib.bib28)
    ). All the above improvements combined with the urging quest for higher energy-efficiency
    and lower latency interconnects in exascale HPC systems, are leading to a significant
    increase in the interest in heterogeneity and specialized computing in the form
    of reconfigurable accelerators ([exa-dataflow,](#bib.bib242) ). This makes the
    use of FPGAs very attractive to scale-out resources by enabling distributed computing
    and can be programmed to be network-capable processors implementing custom interconnects
    featuring low-latency communications without involving the CPU control ([euroexa-fpga-net,](#bib.bib136)
    ).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习方法在多个应用领域的快速增加使用，以及在高性能计算（HPC）社区中对可重构架构的兴趣不断上升（[fpga-hpc,](#bib.bib225)；[fpga-hpc-trends,](#bib.bib229)；[fpga-hpc2,](#bib.bib76)），我们可能会看到FPGA作为加速器在下一代HPC系统中成为更常见的选项。过去，几个原因阻碍了这种转变。首先，FPGA并未设计为提供高浮点（FP）计算性能（[fpga-hpc-trends,](#bib.bib229)），而典型的HPC工作负载通常需要双精度（DP）和单精度（SP）FP计算。其次，FPGA编程可能是一个非常耗时的过程，需要使用特定的硬件编程技能和不常见的编程语言（[fpga-programming,](#bib.bib21)）。第三，为一个FPGA编写的代码很难在不同设备上运行，除非进行完全重新设计，这导致了对广泛HPC应用不可接受的严重可移植性问题，甚至移植到GPU也曾经历过漫长而艰难的过程（[decade-gpu-porting,](#bib.bib224)）。然而，最近这些障碍开始由于硬件架构和编程框架的改进而逐渐消退。最新一代的FPGA集成了成千上万的可编程DSP（数字信号处理器），能够实现SP和DP浮点操作（[fpga-dsp-fp,](#bib.bib29)；[mapping-dsp,](#bib.bib186)；[xilinx-dsp-flops,](#bib.bib226)），并且可能还会嵌入自定义的FP
    DSP模块。这使得这些设备能够达到与商品HPC处理器（即TFLOP/s）相同数量级的性能，在某些情况下还能够提供更好的能效（[altera-dsp-flops,](#bib.bib23)；[fpga-opencl-hpc,](#bib.bib252)）。与此同时，合成工具的最新改进和新编程方法的发展，如HLS（[survey-hls,](#bib.bib161)），使得程序员能够使用高级语言开发代码。这些方法与HPC开发人员常用的（例如OpenMP和OpenACC）类似，这些方法也能保证代码的公平可移植性（[ompss-fpga2,](#bib.bib28)）。所有这些改进，加上对更高能效和更低延迟互连的迫切需求，在超大规模HPC系统中，导致了对异质性和专业计算的显著兴趣，以可重构加速器的形式呈现（[exa-dataflow,](#bib.bib242)）。这使得FPGA的使用变得非常有吸引力，通过启用分布式计算来扩展资源，并且可以编程为网络能力处理器，实现定制互连，具有低延迟通信，而无需涉及CPU控制（[euroexa-fpga-net,](#bib.bib136)）。
- en: First prototypes of FPGA accelerated HPC systems are already being designed
    and deployed. One example is the Alveo FPGA Cluster installed at ETH Zurich in
    the context of the Xilinx Adaptive Compute Clusters (XACC) initiative, using commodity
    hardware to support novel research in adaptive compute acceleration for HPC. Another
    example is the EU-H2020 EuroEXA Project, which has developed a HPC system prototype
    with custom hardware, adopting FPGA-based accelerators for both computing and
    networking ([euroexa-fpga-net,](#bib.bib136) ). As a future scenario, we could
    expect the next generations of HPC systems to be equipped with FPGA-based accelerators,
    probably alongside other accelerators, such as GPUs, being programmed with high-level
    languages, possibly based on pragma directives, allowing to address different
    types of accelerators in an uniform way ([ompss-fpga2,](#bib.bib28) ). In this
    context, application developers need to estimate the performance achievable on
    target FPGAs, to decide whether an application kernel is worth to be ported, or
    which FPGA better fits its computing requirements. At the same time, system architects
    and engineers need to estimate the performance of a single FPGA, to feed performance
    models to tune, balance and optimize the performance at system level ([exa-dataflow,](#bib.bib242)
    ).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA加速的HPC系统的原型已经开始设计和部署。一个例子是ETH Zurich安装的Alveo FPGA集群，作为Xilinx Adaptive Compute
    Clusters (XACC)计划的一部分，使用普通硬件来支持HPC自适应计算加速方面的创新研究。另一个例子是EU-H2020 EuroEXA项目，该项目开发了一个带有定制硬件的HPC系统原型，采用了基于FPGA的加速器用于计算和网络（[euroexa-fpga-net,](#bib.bib136)）。作为未来的场景，我们可以预期下一代HPC系统将配备FPGA加速器，可能还会配合其他加速器，如GPU，并使用高级语言进行编程，可能基于pragma指令，从而以统一的方式处理不同类型的加速器（[ompss-fpga2,](#bib.bib28)）。在这种背景下，应用开发者需要估计在目标FPGA上可实现的性能，以决定是否值得迁移应用内核，或者哪个FPGA更适合其计算需求。与此同时，系统架构师和工程师需要估计单个FPGA的性能，以提供性能模型，以在系统层面调整、平衡和优化性能（[exa-dataflow,](#bib.bib242)）。
- en: Several research works have investigated FPGAs performance when used as hardware
    accelerators, mostly using synthetic benchmarks to estimate the bandwidth of off-chip
    memories ([shuhai,](#bib.bib231) ; [fpga-stream-opencl,](#bib.bib159) ; [intel-fpga-mem,](#bib.bib253)
    ), and OpenCL kernels to measure the FPGA computing performance ([fpga-fp-eval,](#bib.bib123)
    ; [hpcc-fpga,](#bib.bib149) ; [hpcg-fpga,](#bib.bib246) ). In ([shuhai,](#bib.bib231)
    ) is presented the Shuhai Verilog benchmark, used to characterize the performance
    of HBM and DDR off-chip memories embedded in the Xilinx Alveo U280. In ([hpcc-fpga,](#bib.bib149)
    ) is presented an OpenCL implementation of the HPCChallenge Benchmark Suite, reporting
    the results for different FPGAs. In ([hpcg-fpga,](#bib.bib246) ), it is reported
    a C/HLS implementation of the HPCG benchmark targeting FPGAs. Interestingly, in
    this case, the Roofline Model has been used, but only to assess the optimization
    level of the specific application, with respect to theoretical estimations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究工作探讨了FPGAs在用作硬件加速器时的性能，主要使用合成基准测试来估计离线存储器的带宽（[shuhai,](#bib.bib231)；[fpga-stream-opencl,](#bib.bib159)；[intel-fpga-mem,](#bib.bib253)），以及OpenCL内核来测量FPGA计算性能（[fpga-fp-eval,](#bib.bib123)；[hpcc-fpga,](#bib.bib149)；[hpcg-fpga,](#bib.bib246)）。在（[shuhai,](#bib.bib231)）中介绍了Shuhai
    Verilog基准测试，用于表征嵌入在Xilinx Alveo U280中的HBM和DDR离线存储器的性能。在（[hpcc-fpga,](#bib.bib149)）中介绍了HPCChallenge基准套件的OpenCL实现，报告了不同FPGA的结果。在（[hpcg-fpga,](#bib.bib246)）中，报告了一种针对FPGA的HPCG基准的C/HLS实现。有趣的是，在这种情况下，使用了Roofline模型，但仅用于评估特定应用的优化水平，与理论估算相比。
- en: The Roofline Model has already been used in the past to evaluate the performance
    of specific applications ([fpga-roofline-tsunami,](#bib.bib160) ), being ported
    to FPGAs. However, few works provide a generic application-independent extension
    of this model for these architectures, mainly due to the difficulty in defining
    the maximum compute performance for a reconfigurable device. The first comprehensive
    work extending the Roofline Model to FPGAs has been presented in ([roofline-fpga-hls,](#bib.bib61)
    ), here authors focus mainly on aiding developers to explore the design space
    options. Building on the same principle, more recently, in ([roofline-fpga-cad,](#bib.bib205)
    ) and in its extended version ([roofline-fpga-cad2,](#bib.bib204) ), a semi-automated
    performance optimization methodology based on the Roofline model for FPGAs has
    been proposed. In this case, the authors, aim for a tool to explore the design
    space, while in our case, we aim to provide a benchmarking tool.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Roofline模型过去已经用于评估特定应用程序的性能（[fpga-roofline-tsunami](#bib.bib160)），并移植到FPGAs。然而，少有工作提供了该模型在这些架构上的通用应用无关扩展，这主要是因为难以定义可重配置设备的最大计算性能。第一个将Roofline模型扩展到FPGAs的全面工作在（[roofline-fpga-hls](#bib.bib61)）中提出，作者主要关注于帮助开发者探索设计空间选项。基于相同的原则，最近在（[roofline-fpga-cad](#bib.bib205)）及其扩展版本（[roofline-fpga-cad2](#bib.bib204)）中，提出了一种基于Roofline模型的FPGAs的半自动性能优化方法。在这种情况下，作者旨在提供一个探索设计空间的工具，而我们则致力于提供一个基准测试工具。
- en: The first work proposing a methodology for the performance analysis of FPGAs
    allowing to make Roofline plots and cross-architectural comparisons, has been
    reported in ([roofline-multibench-fpga,](#bib.bib156) ). In this case, the authors
    use OpenCL as a programming language to provide mini-apps, such as SHOCL0, LINPACK,
    and STREAM, to measure the computing performance and the memory bandwidth of the
    off-chip memory. Using OpenCL also the ERT benchmark has been reported to run
    on FPGAs in ([ert-opencl-fpga,](#bib.bib165) ) and in its extension ([ert-opencl-fpga2,](#bib.bib164)
    ).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个提出FPGAs性能分析方法的工作，允许制作Roofline图和跨架构比较，已在（[roofline-multibench-fpga](#bib.bib156)）中报告。在这种情况下，作者使用OpenCL作为编程语言，提供了mini-apps，如SHOCL0、LINPACK和STREAM，以测量计算性能和片外内存的带宽。使用OpenCL的ERT基准也已在（[ert-opencl-fpga](#bib.bib165)）及其扩展版（[ert-opencl-fpga2](#bib.bib164)）中报告在FPGAs上运行。
- en: In ([parco19-fp,](#bib.bib33) ), the first C/HLS benchmark tool capable of providing
    empirical Roofline plots for FPGAs was presented. The work was later extended
    to support the Xilinx Vitis workflow to allow for a wider adoption ([fer-fpl,](#bib.bib34)
    ). This tool, named FER (FPGA Empirical Roofline) ([fer,](#bib.bib35) ), and available
    as Free Software ([fer-code,](#bib.bib32) ), has been developed by INFN and the
    University of Ferrara, and it allows for application-agnostic performance assessment
    of FPGA-based accelerators, aiming for comprehensive machine characterization,
    allowing for cross-architectural comparisons and for performance estimations of
    generic HPC kernels on a given device. To this aim, FER is able to measure both
    the computing peak performance of FPGAs, and the bandwidths of on-chip and off-chip
    memories. It is based on the Roofline Model and it implements at its core a directive
    annotated C/HLS kernel, with tunable operational intensity and hardware resources
    usage. Moreover, it relies on a theoretical model aiming to strictly link the
    performance results to the available hardware resources. The choice of C/HLS allows
    at the same time to expose to the users low-level fine-tuning knobs, as well as
    using a high-level programming paradigm that can easily be used by the HPC user
    community for development and porting.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在（[parco19-fp](#bib.bib33)）中，首次提出了能够为FPGAs提供实证Roofline图的C/HLS基准工具。该工作随后扩展支持Xilinx
    Vitis工作流，以便更广泛地采用（[fer-fpl](#bib.bib34)）。这个名为FER（FPGA实证Roofline）（[fer](#bib.bib35)）的工具，由INFN和费拉拉大学开发，并以自由软件（[fer-code](#bib.bib32)）的形式提供，能够对基于FPGA的加速器进行应用无关的性能评估，旨在实现全面的机器特征表征，允许跨架构比较以及对特定设备上通用HPC内核的性能估计。为此，FER能够测量FPGAs的计算峰值性能，以及片内和片外内存的带宽。它基于Roofline模型，并在其核心实现了一个指令注释的C/HLS内核，具有可调的操作强度和硬件资源使用。此外，它依赖于一个理论模型，旨在严格将性能结果与可用硬件资源相关联。C/HLS的选择同时允许用户进行低级别的精细调节，同时使用高层次的编程范式，这可以被HPC用户社区用于开发和移植。
- en: 8\. Conclusions and Perspectives
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论与展望
- en: The design of Deep Learning accelerators for heterogeneous architectures requires
    a plethora of methodologies and EDA tools to assist the designer at different
    design, simulation and verification stages. The complexity of recent DL heterogeneous
    System-on-Chip architectures reached billions of transistors, such as the 80 billion
    transistors included in the Nvidia Hopper H100 GPU ([choquetteNVIDIAHopperH1002023,](#bib.bib55)
    ). Recent advancements in EDA tools enabled to handle the increased complexity
    of heterogeneous architectures, improving design productivity while meeting the
    design goals in terms of speedup and energy efficiency.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 针对异构架构的深度学习加速器设计需要大量的方法论和EDA工具，以协助设计师在不同的设计、仿真和验证阶段。近期DL异构芯片架构的复杂性达到了数十亿个晶体管，例如Nvidia
    Hopper H100 GPU中包含的80亿个晶体管([choquetteNVIDIAHopperH1002023,](#bib.bib55))。EDA工具的最新进展使得能够处理异构架构的增加复杂性，提高了设计生产力，同时在加速和能效方面满足了设计目标。
- en: In this scenario, this survey provides an overview of the main flows and frameworks
    developed in recent years to manage the complexity and design goals of DL accelerators.
    In particular, the survey started at a high level, discussing the models and characteristics
    used in DL application workloads. Then, the first methodologies described the
    hardware-software partitioning problem that decomposes a DL model into software
    and hardware parts, choosing the best architecture for each component defined.
    In this context, approximate computing methodologies are essential in reducing
    the computational complexity and memory requirements. We then discussed methods
    to implement hardware accelerators by exploiting the high-level synthesis methodologies
    that may efficiently target technologies such as ASIC or FPGAs. In addition, HLS
    flows allow higher flexibility and support an easy integration of future-developed
    DL operators. Finally, we present the set of methodologies to compile software,
    model, profile, evaluate and explore the various design knobs that a DL application
    might have.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，本调查提供了对近年来开发的主要流程和框架的概述，以管理DL加速器的复杂性和设计目标。特别地，调查从高层开始，讨论了在DL应用工作负载中使用的模型和特征。然后，首批方法论描述了硬件-软件划分问题，这将DL模型分解为软件和硬件部分，为每个定义的组件选择最佳架构。在这种背景下，近似计算方法对于减少计算复杂性和内存需求至关重要。接着，我们讨论了利用高级综合方法实施硬件加速器的方法，这些方法可以高效地针对ASIC或FPGA等技术。此外，高级综合流程允许更高的灵活性，并支持未来开发的DL操作符的轻松集成。最后，我们展示了一组方法论，用于编译软件、建模、分析、评估和探索DL应用可能具有的各种设计调节选项。
- en: Before concluding this survey, we briefly summarize some interesting challenges
    and trends in EDA. One interesting trend is the application of Machine Learning
    and Deep Learning techniques in the optimization of EDA tasks. Some recent surveys
    discuss this trend, such as  ([ML4EDA21,](#bib.bib106) ) and ([RenHu23,](#bib.bib99)
    ), and the current understanding is that we still need to find the right combination
    of existing heuristic solutions with ML-based approaches. The idea is that the
    ability of DL-based techniques to extract meaningful knowledge from a large amount
    of data will be of some help in driving the design automatization process. In
    particular, Machine Learning allows us to improve predictions and optimizations,
    important steps of many design automation tasks. Moreover, Machine Learning can
    support the scalability requirements of modern design automation tools ([Kahng23,](#bib.bib125)
    ). Challenges in this integration lie in the explainability of Machine Learning
    models and their relations with the design objectives and constraints that are
    only sometimes straightforward ([SALEEM22,](#bib.bib191) ).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结这项调查之前，我们简要概述了一些EDA中的有趣挑战和趋势。一个有趣的趋势是机器学习和深度学习技术在EDA任务优化中的应用。一些近期的调查讨论了这一趋势，例如([ML4EDA21,](#bib.bib106))和([RenHu23,](#bib.bib99))，当前的理解是我们仍然需要找到现有启发式解决方案与基于ML的方法的正确组合。想法是，DL技术从大量数据中提取有意义知识的能力将有助于推动设计自动化过程。特别是，机器学习允许我们改善预测和优化，这是许多设计自动化任务的重要步骤。此外，机器学习可以支持现代设计自动化工具的可扩展性要求([Kahng23,](#bib.bib125))。这种整合的挑战在于机器学习模型的可解释性及其与设计目标和约束的关系，这些关系有时并不直接([SALEEM22,](#bib.bib191))。
- en: Acknowledgements.
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work has been (partially) supported by the Spoke 1 ”FutureHPC & BigData”
    of the Italian Research Center on High-Performance Computing, Big Data and Quantum
    Computing (ICSC) funded by MUR Missione 4 - Next Generation EU (NGEU).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了意大利高性能计算、大数据和量子计算研究中心（ICSC）“FutureHPC & BigData”项目（由MUR Missione 4 - Next
    Generation EU (NGEU)资助）的（部分）支持。
- en: References
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Abdelfattah, A., Anzt, H., Boman, E. G., Carson, E., Cojean, T., Dongarra,
    J., Fox, A., Gates, M., Higham, N. J., Li, X. S., Loe, J., Luszczek, P., Pranesh,
    S., Rajamanickam, S., Ribizel, T., Smith, B. F., Swirydowicz, K., Thomas, S.,
    Tomov, S., Tsai, Y. M., and Yang, U. M. A survey of numerical linear algebra methods
    utilizing mixed-precision arithmetic. The International Journal of High Performance
    Computing Applications 35, 4 (2021), 344–369.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Abdelfattah, A., Anzt, H., Boman, E. G., Carson, E., Cojean, T., Dongarra,
    J., Fox, A., Gates, M., Higham, N. J., Li, X. S., Loe, J., Luszczek, P., Pranesh,
    S., Rajamanickam, S., Ribizel, T., Smith, B. F., Swirydowicz, K., Thomas, S.,
    Tomov, S., Tsai, Y. M., 和 Yang, U. M. 一项利用混合精度算术的数值线性代数方法的综述。《国际高性能计算应用期刊》35,
    4 (2021), 344–369。
- en: '(2) Abdelfattah, M. S., Dudziak, Ł., Chau, T., Lee, R., Kim, H., and Lane,
    N. D. Best of Both Worlds: AutoML Codesign of a CNN and Its Hardware Accelerator.
    In Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference (2020),
    DAC ’20, IEEE.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Abdelfattah, M. S., Dudziak, Ł., Chau, T., Lee, R., Kim, H., 和 Lane, N.
    D. 两全其美：CNN及其硬件加速器的AutoML联合设计。收录于第57届ACM/EDAC/IEEE设计自动化会议论文集（2020），DAC ’20, IEEE。
- en: '(3) Abdelouahab, K., Pelcat, M., Sérot, J., and Berry, F. Accelerating CNN
    inference on FPGAs: A Survey. CoRR abs/1806.01683 (2018).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Abdelouahab, K., Pelcat, M., Sérot, J., 和 Berry, F. FPGA上加速CNN推理：综述。CoRR
    abs/1806.01683 (2018)。
- en: '(4) Accellera. Accellera IP-XACT working group: IP-XACT User Guide, 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Accellera. Accellera IP-XACT工作组：IP-XACT用户指南，2018。
- en: '(5) Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., and Alizadeh,
    M. Placeto: Learning Generalizable Device Placement Algorithms for Distributed
    Machine Learning. In Proceedings of the 33rd International Conference on Neural
    Information Processing Systems (Red Hook, NY, USA, 2019), Curran Associates Inc.,
    p. 3981–3991.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., 和 Alizadeh, M.
    Placeto：学习可推广的分布式机器学习设备布局算法。收录于第33届国际神经信息处理系统大会论文集（美国纽约红钩，2019），Curran Associates
    Inc., p. 3981–3991。
- en: '(6) Agostini, N. B., Curzel, S., Limaye, A., Amatya, V., Minutoli, M., Castellana,
    V. G., Manzano, J., Tumeo, A., and Ferrandi, F. The SODA Approach: Leveraging
    High-Level Synthesis for Hardware/Software Co-Design and Hardware Specialization.
    In Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC) (2022),
    pp. 1359–1362.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Agostini, N. B., Curzel, S., Limaye, A., Amatya, V., Minutoli, M., Castellana,
    V. G., Manzano, J., Tumeo, A., 和 Ferrandi, F. SODA方法：利用高级综合进行硬件/软件协同设计和硬件专门化。收录于第59届ACM/IEEE设计自动化会议（DAC）（2022），页1359–1362。
- en: (7) Agullo, E., Aumage, O., Faverge, M., Furmento, N., Pruvost, F., Sergent,
    M., and Thibault, S. P. Achieving High Performance on Supercomputers with a Sequential
    Task-based Programming Model. IEEE Transactions on Parallel and Distributed Systems
    (2017), 1–1.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Agullo, E., Aumage, O., Faverge, M., Furmento, N., Pruvost, F., Sergent,
    M., 和 Thibault, S. P. 使用基于任务的顺序编程模型在超级计算机上实现高性能。《IEEE并行与分布式系统汇刊》（2017），1–1。
- en: (8) Agullo, E., Buttari, A., Guermouche, A., Herrmann, J., and Jego, A. Task-Based
    Parallel Programming for Scalable Matrix Product Algorithms. ACM Trans. Math.
    Softw. (feb 2023). Just Accepted.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Agullo, E., Buttari, A., Guermouche, A., Herrmann, J., 和 Jego, A. 基于任务的并行编程用于可扩展矩阵乘积算法。ACM
    Trans. Math. Softw. (2023年2月)。刚刚接受。
- en: (9) Ahle, T. D., and Silvestri, F. Similarity Search with Tensor Core Units.
    In Proc. 13th Int. Conf. Similarity Search and Application (SISAP) (2020), vol. 12440,
    pp. 76–84.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Ahle, T. D., 和 Silvestri, F. 使用张量核心单元的相似性搜索。收录于第13届国际相似性搜索与应用会议论文集（SISAP）（2020），卷12440，页76–84。
- en: (10) Ahmad, W., Ayrancioglu, B., and Hamzaoglu, I. Low Error Efficient Approximate
    Adders for FPGAs. IEEE Access 9 (2021), 117232–117243.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Ahmad, W., Ayrancioglu, B., 和 Hamzaoglu, I. FPGA上的低误差高效近似加法器。IEEE Access
    9 (2021), 117232–117243。
- en: '(11) Ajayi, T., Chhabria, V. A., Fogaça, M., Hashemi, S., Hosny, A., Kahng,
    A. B., Kim, M., Lee, J., Mallappa, U., Neseem, M., Pradipta, G., Reda, S., Saligane,
    M., Sapatnekar, S. S., Sechen, C., Shalan, M., Swartz, W., Wang, L., Wang, Z.,
    Woo, M., and Xu, B. Toward an Open-Source Digital Flow: First Learnings from the
    OpenROAD Project. In Proceedings of the 56th Annual Design Automation Conference
    (DAC) (2019), pp. 1–4.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Ajayi, T., Chhabria, V. A., Fogaça, M., Hashemi, S., Hosny, A., Kahng,
    A. B., Kim, M., Lee, J., Mallappa, U., Neseem, M., Pradipta, G., Reda, S., Saligane,
    M., Sapatnekar, S. S., Sechen, C., Shalan, M., Swartz, W., Wang, L., Wang, Z.,
    Woo, M., 和 Xu, B. 朝向开源数字流：OpenROAD项目的首次学习。在第56届年设计自动化会议（DAC）论文集中（2019年），第1–4页。
- en: (12) Akbari, O., Kamal, M., Afzali-Kusha, A., and Pedram, M. Dual-Quality 4:2
    Compressors for Utilizing in Dynamic Accuracy Configurable Multipliers. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 25, 4 (2017), 1352–1361.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Akbari, O., Kamal, M., Afzali-Kusha, A., 和 Pedram, M. 双质量4:2压缩器用于动态精度可配置乘法器。IEEE超大规模集成（VLSI）系统期刊
    25, 4 (2017年)，1352–1361页。
- en: '(13) Akkad, G., Mansour, A., and Inaty, E. Embedded deep learning accelerators:
    A survey on recent advances. IEEE Transactions on Artificial Intelligence (2023),
    1–19.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Akkad, G., Mansour, A., 和 Inaty, E. 嵌入式深度学习加速器：最近进展的调查。IEEE人工智能期刊（2023年），1–19页。
- en: '(14) Aldinucci, M., Danelutto, M., Kilpatrick, P., and Torquati, M. Fastflow:
    High-Level and Efficient Streaming on Multicore. John Wiley & Sons, Ltd, 2017,
    ch. 13, pp. 261–280.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(14) Aldinucci, M., Danelutto, M., Kilpatrick, P., 和 Torquati, M. Fastflow:
    高级且高效的多核流处理。John Wiley & Sons, Ltd, 2017年，第13章，第261–280页。'
- en: (15) Alioto, M. Energy-quality scalable adaptive VLSI circuits and systems beyond
    approximate computing. In Design, Automation & Test in Europe Conference & Exhibition
    (DATE), 2017 (2017), pp. 127–132.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Alioto, M. 能源质量可扩展的自适应VLSI电路和系统超越近似计算。在设计、自动化与测试欧洲会议及展览（DATE），2017年（2017年），第127–132页。
- en: '(16) Alioto, M., De, V., and Marongiu, A. Guest Editorial for the Special Issue
    on Energy-Quality Scalable Circuits and Systems for Sensing and Computing: from
    Approximate, to Communication-Inspired and Learning-Based. IEEE Journal on Emerging
    and Selected Topics in Circuits and Systems 8 (08 2018), 1–1.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Alioto, M., De, V., 和 Marongiu, A. 能源质量可扩展电路和系统的特刊编辑。IEEE电路与系统新兴与精选主题期刊
    8 (2018年8月)，1–1页。
- en: '(17) Alonso, G. Research for practice: FPGAs in datacenters. Communications
    of the ACM 61, 9 (2018), 48–49.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Alonso, G. 实践中的研究：数据中心中的FPGA。ACM通讯 61, 9 (2018年)，48–49页。
- en: (18) AMD-Xilinx. Vitis HLS LLVM 2021.2, 2021.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) AMD-Xilinx. Vitis HLS LLVM 2021.2，2021年。
- en: '(19) Anderson, A., Vasudevan, A., Keane, C., and Gregg, D. High-Performance
    Low-Memory Lowering: GEMM-based Algorithms for DNN Convolution. In 2020 IEEE 32nd
    International Symposium on Computer Architecture and High Performance Computing
    (SBAC-PAD) (2020), pp. 99–106.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Anderson, A., Vasudevan, A., Keane, C., 和 Gregg, D. 高性能低内存降维：基于GEMM的DNN卷积算法。在2020
    IEEE第32届计算机架构与高性能计算国际研讨会（SBAC-PAD）（2020年），第99–106页。
- en: (20) ARM. AMBA® AXI™ and ACE™ Protocol Specification, 2013.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) ARM. AMBA® AXI™ 和 ACE™ 协议规范，2013年。
- en: (21) Bacon, D. F., Rabbah, R., and Shukla, S. FPGA Programming for the Masses.
    Commun. ACM 56, 4 (Apr. 2013), 56–63.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Bacon, D. F., Rabbah, R., 和 Shukla, S. 面向大众的FPGA编程。Commun. ACM 56, 4 (2013年4月)，56–63页。
- en: (22) Balay, S., Abhyankar, S., Adams, M. F., Benson, S., Brown, J., Brune, P.,
    Buschelman, K., Constantinescu, E. M., Dalcin, L., Dener, A., Eijkhout, V., Faibussowitsch,
    J., Gropp, W. D., Hapla, V., Isaac, T., Jolivet, P., Karpeev, D., Kaushik, D.,
    Knepley, M. G., Kong, F., Kruger, S., May, D. A., McInnes, L. C., Mills, R. T.,
    Mitchell, L., Munson, T., Roman, J. E., Rupp, K., Sanan, P., Sarich, J., Smith,
    B. F., Zampini, S., Zhang, H., Zhang, H., and Zhang, J. PETSc Web page. [https://petsc.org/](https://petsc.org/),
    2023.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Balay, S., Abhyankar, S., Adams, M. F., Benson, S., Brown, J., Brune, P.,
    Buschelman, K., Constantinescu, E. M., Dalcin, L., Dener, A., Eijkhout, V., Faibussowitsch,
    J., Gropp, W. D., Hapla, V., Isaac, T., Jolivet, P., Karpeev, D., Kaushik, D.,
    Knepley, M. G., Kong, F., Kruger, S., May, D. A., McInnes, L. C., Mills, R. T.,
    Mitchell, L., Munson, T., Roman, J. E., Rupp, K., Sanan, P., Sarich, J., Smith,
    B. F., Zampini, S., Zhang, H., Zhang, H., 和 Zhang, J. PETSc网页。[https://petsc.org/](https://petsc.org/)，2023年。
- en: (23) BDT. Floating-point DSP Energy Efficiency on Altera 28 nm FPGAs. Tech.
    rep., Berkeley Design Technology Inc., Feb 2013. An Independent Evaluation.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) BDT. 在Altera 28纳米FPGA上的浮点DSP能效。技术报告，Berkeley Design Technology Inc., 2013年2月。独立评估。
- en: (24) Bilardi, G., and Pietracaprina, A. Models of Computation, Theoretical.
    Springer US, Boston, MA, 2011, pp. 1150–1158.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) Bilardi, G., 和 Pietracaprina, A. 计算模型，理论。Springer US, 波士顿, MA, 2011年，第1150–1158页。
- en: '(25) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., et al. FINN-R: An end-to-end deep-learning framework for fast exploration
    of quantized neural networks. ACM Transactions on Reconfigurable Technology and
    Systems 11, 3 (2018), 1–23.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., 等。FINN-R：一个端到端深度学习框架，用于快速探索量化神经网络。ACM 重新配置技术与系统交易 11, 3 (2018)，1–23。
- en: (26) Bohm Agostini, N., Curzel, S., Amatya, V., Tan, C., Minutoli, M., Castellana,
    V. G., Manzano, J., Kaeli, D., and Tumeo, A. An MLIR-based Compiler Flow for System-Level
    Design and Hardware Acceleration. In IEEE/ACM International Conference On Computer
    Aided Design (ICCAD) (2022), pp. 1–9.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) Bohm Agostini, N., Curzel, S., Amatya, V., Tan, C., Minutoli, M., Castellana,
    V. G., Manzano, J., Kaeli, D., 和 Tumeo, A. 基于 MLIR 的系统级设计和硬件加速编译器流程。在 IEEE/ACM
    计算机辅助设计国际会议 (ICCAD) (2022)，第 1–9 页。
- en: '(27) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., and
    Tumeo, A. Bridging Python to Silicon: The SODA Toolchain. IEEE Micro 42, 5 (2022),
    78–88.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., 和 Tumeo,
    A. 将 Python 与硅结合：SODA 工具链。IEEE Micro 42, 5 (2022)，78–88。
- en: (28) Bosch, J., Tan, X., Filgueras, A., Vidal, M., Mateu, M., Jiménez-González,
    D., Álvarez, C., Martorell, X., Ayguade, E., and Labarta, J. Application Acceleration
    on FPGAs with OmpSs@FPGA. In 2018 International Conference on Field-Programmable
    Technology (FPT) (Dec 2018), pp. 70–77.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Bosch, J., Tan, X., Filgueras, A., Vidal, M., Mateu, M., Jiménez-González,
    D., Álvarez, C., Martorell, X., Ayguade, E., 和 Labarta, J. 使用 OmpSs@FPGA 在 FPGA
    上加速应用。在 2018 国际现场可编程技术会议 (FPT) (2018 年 12 月)，第 70–77 页。
- en: (29) Brosser, F., Cheah, H. Y., and Fahmy, S. A. Iterative floating point computation
    using FPGA DSP blocks. In 2013 23rd International Conference on Field programmable
    Logic and Applications (2013), pp. 1–6.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Brosser, F., Cheah, H. Y., 和 Fahmy, S. A. 使用 FPGA DSP 块的迭代浮点计算。在 2013 第
    23 届国际现场可编程逻辑与应用会议 (2013)，第 1–6 页。
- en: '(30) Burrello, A., Garofalo, A., Bruschi, N., Tagliavini, G., Rossi, D., and
    Conti, F. DORY: Automatic end-to-end deployment of real-world DNNs on low-cost
    IoT MCUs. IEEE Trans Comput. (2021).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) Burrello, A., Garofalo, A., Bruschi, N., Tagliavini, G., Rossi, D., 和 Conti,
    F. DORY：将现实世界 DNN 自动端到端部署到低成本 IoT MCU 上。IEEE 计算机学报 (2021)。
- en: (31) Cadence. Stratus High-Level Synthesis, 2022.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Cadence。Stratus 高级综合，2022。
- en: (32) Calore, E., 2020. https://baltig.infn.it/EuroEXA/FER.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Calore, E., 2020。https://baltig.infn.it/EuroEXA/FER。
- en: (33) Calore, E., and Schifano, S. Energy-efficiency evaluation of FPGAs for
    floating-point intensive workloads. In Parallel Computing is Everywhere (2020),
    vol. 36 of Advances in Parallel Computing, pp. 555–564.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) Calore, E., 和 Schifano, S. FPGA 在浮点密集型工作负载中的能效评估。在 Parallel Computing is
    Everywhere (2020)，第 36 卷，Parallel Computing 进展，第 555–564 页。
- en: (34) Calore, E., and Schifano, S. F. Performance assessment of FPGAs as HPC
    accelerators using the FPGA Empirical Roofline. In 2021 31st International Conference
    on Field-Programmable Logic and Applications (FPL) (2021), pp. 83–90.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) Calore, E., 和 Schifano, S. F. 使用 FPGA 实证 Roofline 的 FPGA 作为 HPC 加速器的性能评估。在
    2021 第 31 届国际现场可编程逻辑与应用会议 (FPL) (2021)，第 83–90 页。
- en: '(35) Calore, E., and Schifano, S. F. FER: A Benchmark for the Roofline Analysis
    of FPGA Based HPC Accelerators. IEEE Access 10 (2022), 94220–94234.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Calore, E., 和 Schifano, S. F. FER：FPGA 基于 HPC 加速器的 Roofline 分析基准。IEEE Access
    10 (2022)，94220–94234。
- en: '(36) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski,
    T. S., Brown, S. D., and Anderson, J. H. LegUp: An open-source high-level synthesis
    tool for FPGA-based processor/accelerator systems. ACM Trans. Embed. Comput. Syst.
    13, 2 (2013), 24:1–24:27.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski, T. S.,
    Brown, S. D., 和 Anderson, J. H. LegUp：一个开源高级综合工具，用于 FPGA 基处理器/加速器系统。ACM 嵌入式计算系统交易
    13, 2 (2013)，24:1–24:27。
- en: '(37) Cao, S., Deng, W., Bao, Z., Xue, C., Xu, S., and Zhang, S. SimuNN: A Pre-RTL
    Inference, Simulation and Evaluation Framework for Neural Networks. IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems 10, 2 (2020), 217–230.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Cao, S., Deng, W., Bao, Z., Xue, C., Xu, S., 和 Zhang, S. SimuNN：神经网络的预
    RTL 推断、模拟和评估框架。IEEE 新兴与精选电路与系统学报 10, 2 (2020)，217–230。
- en: (38) Cardarilli, G. C., Di Nunzio, L., Fazzolari, R., Giardino, D., Nannarelli,
    A., Re, M., and Spanò, S. A pseudo-softmax function for hardware-based high speed
    image classification. Scientific Reports 11, 1 (Jul 2021), 15307.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Cardarilli, G. C., Di Nunzio, L., Fazzolari, R., Giardino, D., Nannarelli,
    A., Re, M., 和 Spanò, S. 一种用于硬件高速度图像分类的伪 softmax 函数。Scientific Reports 11, 1 (2021
    年 7 月)，15307。
- en: (39) Cardellini, V., Filippone, S., and Rouson, D. W. I. Design Patterns for
    Sparse-Matrix Computations on Hybrid CPU/GPU Platforms. Sci. Program. 22, 1 (jan
    2014), 1–19.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Cardellini, V., Filippone, S., 和 Rouson, D. W. I. 针对混合 CPU/GPU 平台的稀疏矩阵计算设计模式。Sci.
    Program. 22, 1 (2014年1月), 1–19.
- en: (40) Carratalá-Sáez, R., Faverge, M., Pichon, G., Sylvand, G., and Quintana-Ortí,
    E. S. Tiled Algorithms for Efficient Task-Parallel H-Matrix Solvers. In 2020 IEEE
    International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
    (2020), pp. 757–766.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Carratalá-Sáez, R., Faverge, M., Pichon, G., Sylvand, G., 和 Quintana-Ortí,
    E. S. 高效任务并行 H 矩阵求解器的分块算法。发表于 2020 IEEE 国际并行与分布式处理研讨会工作坊 (IPDPSW) (2020), pp.
    757–766.
- en: (41) Carson, E., Knight, N., and Demmel, J. Avoiding Communication in Nonsymmetric
    Lanczos-Based Krylov Subspace Methods. SIAM Journal on Scientific Computing 35,
    5 (2013), S42–S61.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) Carson, E., Knight, N., 和 Demmel, J. 避免在非对称 Lanczos 基 Krylov 子空间方法中的通信。SIAM
    Journal on Scientific Computing 35, 5 (2013), S42–S61.
- en: '(42) Carter Edwards, H., Trott, C. R., and Sunderland, D. Kokkos: Enabling
    manycore performance portability through polymorphic memory access patterns. Journal
    of Parallel and Distributed Computing 74, 12 (2014), 3202–3216. Domain-Specific
    Languages and High-Level Frameworks for High-Performance Computing.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Carter Edwards, H., Trott, C. R., 和 Sunderland, D. Kokkos: 通过多态内存访问模式实现多核性能移植性。Journal
    of Parallel and Distributed Computing 74, 12 (2014), 3202–3216. 特定领域语言和高性能计算的高级框架。'
- en: (43) Casamayor Pujol, V., Morichetta, A., Murturi, I., Kumar Donta, P., and
    Dustdar, S. Fundamental Research Challenges for Distributed Computing Continuum
    Systems. Information 14, 3 (2023).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (43) Casamayor Pujol, V., Morichetta, A., Murturi, I., Kumar Donta, P., 和 Dustdar,
    S. 分布式计算连续系统的基本研究挑战。Information 14, 3 (2023).
- en: (44) Chellapilla, K., Puri, S., and Simard, P. High-performance convolutional
    neural networks for document processing. In Tenth International Workshop on Frontiers
    in Handwriting Recognition, Suvisoft (2006), pp. 99–106.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) Chellapilla, K., Puri, S., 和 Simard, P. 高性能卷积神经网络用于文档处理。发表于第十届手写识别前沿国际研讨会，Suvisoft
    (2006), pp. 99–106.
- en: (45) Chen, L., Han, J., Liu, W., and Lombardi, F. Design of Approximate Unsigned
    Integer Non-Restoring Divider for Inexact Computing. In Proceedings of the 25th
    Edition on Great Lakes Symposium on VLSI (New York, NY, USA, 2015), GLSVLSI ’15,
    Association for Computing Machinery, p. 51–56.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (45) Chen, L., Han, J., Liu, W., 和 Lombardi, F. 设计用于不精确计算的近似无符号整数非恢复除法器。发表于第25届大湖区
    VLSI 研讨会 (纽约, NY, USA, 2015), GLSVLSI ’15, Association for Computing Machinery,
    pp. 51–56.
- en: (46) Chen, L., Han, J., Liu, W., and Lombardi, F. On the Design of Approximate
    Restoring Dividers for Error-Tolerant Applications. IEEE Transactions on Computers
    65, 8 (2016), 2522–2533.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Chen, L., Han, J., Liu, W., 和 Lombardi, F. 设计用于容错应用的近似恢复除法器。IEEE Transactions
    on Computers 65, 8 (2016), 2522–2533.
- en: '(47) Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan,
    M., Wang, L., Hu, Y., Ceze, L., et al. TVM: An automated End-to-End optimizing
    compiler for deep learning. In OSDI (2018).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M.,
    Wang, L., Hu, Y., Ceze, L., 等. TVM：用于深度学习的自动化端到端优化编译器。发表于 OSDI (2018).
- en: (48) Chen, Y., Xie, Y., Song, L., Chen, F., and Tang, T. A Survey of Accelerator
    Architectures for Deep Neural Networks. Engineering 6, 3 (2020), 264–274.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Chen, Y., Xie, Y., Song, L., Chen, F., 和 Tang, T. 深度神经网络加速器架构综述。Engineering
    6, 3 (2020), 264–274.
- en: '(49) Chen, Y.-H., and Chang, T.-Y. A High-Accuracy Adaptive Conditional-Probability
    Estimator for Fixed-Width Booth Multipliers. IEEE Transactions on Circuits and
    Systems I: Regular Papers 59, 3 (2012), 594–603.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(49) Chen, Y.-H., 和 Chang, T.-Y. 高精度自适应条件概率估计器用于固定宽度的 Booth 乘法器。IEEE Transactions
    on Circuits and Systems I: Regular Papers 59, 3 (2012), 594–603.'
- en: (50) Chen, Z., Yu, C. H., Morris, T., Tuyls, J., Lai, Y.-H., Roesch, J., Delaye,
    E., Sharma, V., and Wang, Y. Bring Your Own Codegen to Deep Learning Compiler.
    arXiv preprint arXiv:2105.03215 (2021).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) Chen, Z., Yu, C. H., Morris, T., Tuyls, J., Lai, Y.-H., Roesch, J., Delaye,
    E., Sharma, V., 和 Wang, Y. 将自己的代码生成器带入深度学习编译器。arXiv 预印本 arXiv:2105.03215 (2021).
- en: '(51) Chitty-Venkata, K. T., and Somani, A. K. Neural Architecture Search Survey:
    A Hardware Perspective. ACM Computing Surveys 55, 4 (nov 2023).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Chitty-Venkata, K. T., 和 Somani, A. K. 神经架构搜索综述：硬件视角。ACM Computing Surveys
    55, 4 (2023年11月).
- en: (52) Cho, K.-J., Lee, K.-C., Chung, J.-G., and Parhi, K. Design of low-error
    fixed-width modified booth multiplier. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 12, 5 (2004), 522–531.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Cho, K.-J., Lee, K.-C., Chung, J.-G., 和 Parhi, K. 低误差固定宽度修改 Booth 乘法器的设计。IEEE
    Transactions on Very Large Scale Integration (VLSI) Systems 12, 5 (2004), 522–531.
- en: '(53) Cho, M., and Brand, D. MEC: Memory-Efficient Convolution for Deep Neural
    Network. In Proceedings of the 34th International Conference on Machine Learning
    - Volume 70 (2017), ICML’17, JMLR.org, p. 815–824.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Cho, M., 和 Brand, D. 《MEC：深度神经网络的内存高效卷积》。发表于第34届国际机器学习会议论文集 - 第70卷（2017年），ICML’17，JMLR.org，第815–824页。
- en: '(54) Choe, J., Huang, A., Moreshet, T., Herlihy, M., and Bahar, R. I. Concurrent
    Data Structures with Near-Data-Processing: An Architecture-Aware Implementation.
    In The 31st ACM Symposium on Parallelism in Algorithms and Architectures (New
    York, NY, USA, 2019), SPAA ’19, Association for Computing Machinery, p. 297–308.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Choe, J., Huang, A., Moreshet, T., Herlihy, M., 和 Bahar, R. I. 《具有近数据处理的并发数据结构：一种面向架构的实现》。发表于第31届ACM并行算法与架构研讨会（纽约，纽约州，美国，2019年），SPAA
    ’19，计算机协会，第297–308页。
- en: '(55) Choquette, J. NVIDIA Hopper H100 GPU: Scaling Performance. IEEE Micro
    (2023), 1–13.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Choquette, J. 《NVIDIA Hopper H100 GPU：性能扩展》。IEEE《微型计算机》 （2023年），1–13。
- en: '(56) Chowdhury, R., Silvestri, F., and Vella, F. Algorithm Design for Tensor
    Units. In Euro-Par 2021: Parallel Processing (2021), L. Sousa, N. Roma, and P. Tomás,
    Eds., Springer International Publishing, pp. 353–367.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Chowdhury, R., Silvestri, F., 和 Vella, F. 《Tensor单元的算法设计》。在Euro-Par 2021：并行处理（2021年），L.
    Sousa, N. Roma, 和 P. Tomás，编辑，Springer国际出版，第353–367页。
- en: (57) CIRCT Developers. CIRCT / Circuit IR Compilers and Tools, 2020.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) CIRCT开发者。《CIRCT / Circuit IR编译器和工具》，2020年。
- en: '(58) Cole, M. Algorithmic Skeletons: Structured Management of Parallel Computation.
    MIT Press, Cambridge, MA, USA, 1991.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Cole, M. 《算法骨架：并行计算的结构化管理》。麻省理工学院出版社，剑桥，马萨诸塞州，美国，1991年。
- en: '(59) Cong, J., Lau, J., Liu, G., Neuendorffer, S., Pan, P., Vissers, K., and
    Zhang, Z. FPGA HLS Today: Successes, Challenges, and Opportunities. ACM Trans.
    Reconfigurable Technol. Syst. 15, 4 (aug 2022).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) Cong, J., Lau, J., Liu, G., Neuendorffer, S., Pan, P., Vissers, K., 和 Zhang,
    Z. 《FPGA HLS现状：成功、挑战与机遇》。ACM《可重配置技术系统交易》15卷，第4期（2022年8月）。
- en: '(60) Conti, F., Rossi, D., Pullini, A., Loi, I., and Benini, L. PULP: A Ultra-Low
    Power Parallel Accelerator for Energy-Efficient and Flexible Embedded Vision.
    Journal of Signal Processing Systems 84, 3 (2016), 339–354.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) Conti, F., Rossi, D., Pullini, A., Loi, I., 和 Benini, L. 《PULP：一个超低功耗的并行加速器，用于能源高效和灵活的嵌入式视觉》。信号处理系统期刊84卷，第3期（2016年），339–354。
- en: '(61) Da Silva, B., Braeken, A., D’Hollander, E. H., and Touhafi, A. Performance
    modeling for FPGAs: extending the roofline model with high-level synthesis tools.
    International Journal of Reconfigurable Computing 2013 (2013).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Da Silva, B., Braeken, A., D’Hollander, E. H., 和 Touhafi, A. 《FPGA性能建模：使用高级综合工具扩展屋脊线模型》。国际《可重配置计算期刊》2013（2013年）。
- en: (62) Dakkak, A., Li, C., Xiong, J., Gelado, I., and Hwu, W.-M. Accelerating
    Reduction and Scan Using Tensor Core Units. In Proc. Int. Conf. Supercomputing
    (ICS) (2019), pp. 46–57.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) Dakkak, A., Li, C., Xiong, J., Gelado, I., 和 Hwu, W.-M. 《利用Tensor Core单元加速归约和扫描》。在国际超级计算会议论文集（ICS）（2019年），第46–57页。
- en: (63) D’Ambra, P., Durastante, F., and Filippone, S. AMG Preconditioners for
    Linear Solvers towards Extreme Scale. SIAM Journal on Scientific Computing 43,
    5 (2021), S679–S703.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) D’Ambra, P., Durastante, F., 和 Filippone, S. 《用于极大规模的线性求解器的AMG预处理器》。SIAM《科学计算期刊》43卷，第5期（2021年），S679–S703。
- en: (64) Danelutto, M., Mencagli, G., Torquati, M., González-Vélez, H., and Kilpatrick,
    P. Algorithmic Skeletons and Parallel Design Patterns in Mainstream Parallel Programming.
    Int. J. Parallel Program. 49, 2 (2021), 177–198.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) Danelutto, M., Mencagli, G., Torquati, M., González-Vélez, H., 和 Kilpatrick,
    P. 《主流并行编程中的算法骨架和并行设计模式》。国际《并行程序设计期刊》49卷，第2期（2021年），177–198。
- en: '(65) Dave, S., Kim, Y., Avancha, S., Lee, K., and Shrivastava, A. Dmazerunner:
    Executing perfectly nested loops on dataflow accelerators. ACM Transactions on
    Embedded Computing Systems (TECS) 18, 5s (2019), 1–27.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Dave, S., Kim, Y., Avancha, S., Lee, K., 和 Shrivastava, A. 《Dmazerunner：在数据流加速器上执行完美嵌套的循环》。ACM《嵌入式计算系统交易》18卷，第5s期（2019年），1–27。
- en: '(66) David, R., Duke, J., Jain, A., Reddi, V. J., Jeffries, N., Li, J., Kreeger,
    N., Nappier, I., Natraj, M., Regev, S., et al. TensorFlow Lite Micro: Embedded
    Machine Learning on TinyML Systems. arXiv preprint arXiv:2010.08678 (2020).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) David, R., Duke, J., Jain, A., Reddi, V. J., Jeffries, N., Li, J., Kreeger,
    N., Nappier, I., Natraj, M., Regev, S., 等。《TensorFlow Lite Micro：TinyML系统上的嵌入式机器学习》。arXiv预印本
    arXiv:2010.08678（2020年）。
- en: (67) Davis, E. C., Strout, M. M., and Olschanowsky, C. Transforming Loop Chains
    via Macro Dataflow Graphs. In Proceedings of the 2018 International Symposium
    on Code Generation and Optimization (New York, NY, USA, 2018), CGO 2018, Association
    for Computing Machinery, p. 265–277.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) Davis, E. C., Strout, M. M., 和 Olschanowsky, C. 《通过宏数据流图转换循环链》。发表于2018年国际代码生成与优化研讨会（纽约，纽约州，美国，2018年），CGO
    2018，计算机协会，第265–277页。
- en: (68) de Dinechin, F., et al. Designing Custom Arithmetic Data Paths with FloPoCo.
    IEEE Design & Test of Computers 28, 4 (July 2011), 18–27.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) de Dinechin, F. 等。使用 FloPoCo 设计定制算术数据路径。IEEE Design & Test of Computers
    28, 4 (2011 年 7 月)，第 18–27 页。
- en: '(69) Demmel, J. Communication avoiding algorithms. In 2012 SC Companion: High
    Performance Computing, Networking Storage and Analysis (2012), pp. 1942–2000.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(69) Demmel, J. 避免通信的算法。在 2012 SC Companion: 高性能计算、网络存储与分析 (2012)，第 1942–2000
    页。'
- en: (70) Di Meo, G., Saggese, G., Strollo, A. G. M., and De Caro, D. Design of Generalized
    Enhanced Static Segment Multiplier with Minimum Mean Square Error for Uniform
    and Nonuniform Input Distributions. Electronics 12, 2 (2023).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) Di Meo, G., Saggese, G., Strollo, A. G. M. 和 De Caro, D. 具有最小均方误差的广义增强静态段乘法器设计，适用于均匀和非均匀输入分布。Electronics
    12, 2 (2023)。
- en: (71) Dongarra, J. J., Du Croz, J., Hammarling, S., and Duff, I. S. A Set of
    Level 3 Basic Linear Algebra Subprograms. ACM Trans. Math. Softw. 16, 1 (mar 1990),
    1–17.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) Dongarra, J. J., Du Croz, J., Hammarling, S. 和 Duff, I. S. 一组第 3 级基本线性代数子程序。ACM
    Trans. Math. Softw. 16, 1 (1990 年 3 月)，第 1–17 页。
- en: (72) Dongarra, J. J., and Walker, D. W. Software Libraries for Linear Algebra
    Computations on High Performance Computers. SIAM Review 37, 2 (1995), 151–180.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) Dongarra, J. J. 和 Walker, D. W. 高性能计算机上线性代数计算的软件库。SIAM Review 37, 2 (1995)，第
    151–180 页。
- en: (73) Duarte, J., Han, S., Harris, P., Jindariani, S., Kreinar, E., Kreis, B.,
    Ngadiuba, J., Pierini, M., Tran, N., and Wu, Z. Fast inference of deep neural
    networks in FPGAs for particle physics. Journal of Instrumentation 13, 07 (2018),
    P07027.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) Duarte, J., Han, S., Harris, P., Jindariani, S., Kreinar, E., Kreis, B.,
    Ngadiuba, J., Pierini, M., Tran, N. 和 Wu, Z. FPGA 上的深度神经网络快速推断用于粒子物理学。Journal
    of Instrumentation 13, 07 (2018)，P07027。
- en: (74) D’Ambra, P., Durastante, F., and Filippone, S. Parallel Sparse Computation
    Toolkit. Software Impacts 15 (2023), 100463.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) D’Ambra, P., Durastante, F. 和 Filippone, S. 并行稀疏计算工具包。Software Impacts
    15 (2023)，100463。
- en: (75) Elgohary, A., Boehm, M., Haas, P. J., Reiss, F. R., and Reinwald, B. Compressed
    Linear Algebra for Large-Scale Machine Learning. Proc. VLDB Endow. 9, 12 (aug
    2016), 960–971.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) Elgohary, A., Boehm, M., Haas, P. J., Reiss, F. R. 和 Reinwald, B. 大规模机器学习的压缩线性代数。Proc.
    VLDB Endow. 9, 12 (2016 年 8 月)，第 960–971 页。
- en: (76) Escobar, F. A., Chang, X., and Valderrama, C. Suitability Analysis of FPGAs
    for Heterogeneous Platforms in HPC. IEEE Transactions on Parallel and Distributed
    Systems 27, 2 (2016), 600–612.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) Escobar, F. A., Chang, X. 和 Valderrama, C. FPGA 在高性能计算异构平台上的适用性分析。IEEE
    Transactions on Parallel and Distributed Systems 27, 2 (2016)，第 600–612 页。
- en: (77) Esposito, D., Strollo, A. G. M., and Alioto, M. Low-power approximate MAC
    unit. In 2017 13th Conference on Ph.D. Research in Microelectronics and Electronics
    (PRIME) (2017), pp. 81–84.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) Esposito, D., Strollo, A. G. M. 和 Alioto, M. 低功耗近似 MAC 单元。在 2017 年第 13
    届博士研究微电子学与电子学会议 (PRIME) (2017)，第 81–84 页。
- en: '(78) Esposito, D., Strollo, A. G. M., Napoli, E., De Caro, D., and Petra, N.
    Approximate Multipliers Based on New Approximate Compressors. IEEE Transactions
    on Circuits and Systems I: Regular Papers 65, 12 (2018), 4169–4182.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(78) Esposito, D., Strollo, A. G. M., Napoli, E., De Caro, D. 和 Petra, N. 基于新近似压缩器的近似乘法器。IEEE
    Transactions on Circuits and Systems I: Regular Papers 65, 12 (2018)，第 4169–4182
    页。'
- en: '(79) Fan, S., Rong, Y., Meng, C., Cao, Z., Wang, S., Zheng, Z., Wu, C., Long,
    G., Yang, J., Xia, L., Diao, L., Liu, X., and Lin, W. DAPPLE: A Pipelined Data
    Parallel Approach for Training Large Models. In Proceedings of the 26th ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming (New York, NY, USA,
    2021), PPoPP ’21, ACM, pp. 431–445.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) Fan, S., Rong, Y., Meng, C., Cao, Z., Wang, S., Zheng, Z., Wu, C., Long,
    G., Yang, J., Xia, L., Diao, L., Liu, X. 和 Lin, W. DAPPLE：一种用于训练大规模模型的流水线数据并行方法。在第
    26 届 ACM SIGPLAN 并行程序设计原则与实践研讨会 (纽约，NY，USA，2021)，PPoPP ’21，ACM，第 431–445 页。
- en: (80) Farshchi, F., Abrishami, M. S., and Fakhraie, S. M. New approximate multiplier
    for low power digital signal processing. In The 17th CSI International Symposium
    on Computer Architecture & Digital Systems (CADS 2013) (2013), pp. 25–30.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) Farshchi, F., Abrishami, M. S. 和 Fakhraie, S. M. 低功耗数字信号处理的新近似乘法器。在第 17
    届 CSI 国际计算机体系结构与数字系统研讨会 (CADS 2013) (2013)，第 25–30 页。
- en: '(81) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an Open-Source
    Research Framework for the High-Level Synthesis of Complex Applications. In 2021
    58th ACM/IEEE Design Automation Conference (DAC) (Dec 2021), IEEE, pp. 1327–1330.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C. 和 Tumeo, A. 邀请：Bambu：一个用于复杂应用高级综合的开源研究框架。在
    2021 年第 58 届 ACM/IEEE 设计自动化大会 (DAC) (2021 年 12 月)，IEEE，第 1327–1330 页。
- en: (82) Filippone, S., Cardellini, V., Barbieri, D., and Fanfarillo, A. Sparse
    Matrix-Vector Multiplication on GPGPUs. ACM Trans. Math. Softw. 43, 4 (jan 2017).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) Filippone, S., Cardellini, V., Barbieri, D., 和 Fanfarillo, A. 在 GPGPU 上的稀疏矩阵-向量乘法。ACM
    数学软件期刊 43, 4 (2017年1月)。
- en: '(83) Flamand, E., Rossi, D., Conti, F., Loi, I., Pullini, A., Rotenberg, F.,
    and Benini, L. GAP-8: A RISC-V SoC for AI at the Edge of the IoT. In 2018 IEEE
    29th International Conference on Application-specific Systems, Architectures and
    Processors (ASAP) (2018), IEEE, pp. 1–4.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(83) Flamand, E., Rossi, D., Conti, F., Loi, I., Pullini, A., Rotenberg, F.,
    和 Benini, L. GAP-8: 一款用于物联网边缘的 RISC-V SoC。 在 2018 IEEE 第29届应用特定系统、架构和处理器国际会议 (ASAP)
    (2018), IEEE, pp. 1–4。'
- en: (84) Flegar, G., Anzt, H., Cojean, T., and Quintana-Ortí, E. S. Adaptive Precision
    Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra
    Software. ACM Trans. Math. Softw. 47, 2 (apr 2021).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (84) Flegar, G., Anzt, H., Cojean, T., 和 Quintana-Ortí, E. S. 高性能预处理中的自适应精度块-雅可比。在
    Ginkgo 线性代数软件中的应用。ACM 数学软件期刊 47, 2 (2021年4月)。
- en: (85) Fowers, J., Ovtcharov, K., Papamichael, M. K., Massengill, T., Liu, M.,
    Lo, D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P.,
    Sapek, A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M.,
    Chung, E. S., and Burger, D. Inside Project Brainwave’s Cloud-Scale, Real-Time
    AI Processor. IEEE Micro 39, 3 (2019), 20–28.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) Fowers, J., Ovtcharov, K., Papamichael, M. K., Massengill, T., Liu, M.,
    Lo, D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P.,
    Sapek, A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M.,
    Chung, E. S., 和 Burger, D. Project Brainwave 的云规模实时 AI 处理器内部。IEEE 微型期刊 39, 3 (2019),
    20–28。
- en: (86) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Energy-Quality
    Scalable Adders Based on Nonzeroing Bit Truncation. IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems PP (12 2018), 1–5.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) Frustaci, F., Perri, S., Corsonello, P., 和 Alioto, M. 基于非零位截断的能源质量可扩展加法器。IEEE
    大规模集成（VLSI）系统期刊 PP (2018年12月), 1–5。
- en: '(87) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Approximate Multipliers
    With Dynamic Truncation for Energy Reduction via Graceful Quality Degradation.
    IEEE Transactions on Circuits and Systems II: Express Briefs PP (06 2020), 1–1.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(87) Frustaci, F., Perri, S., Corsonello, P., 和 Alioto, M. 通过优雅的质量退化实现能源减少的动态截断近似乘法器。IEEE
    电路与系统 II: 快报 PP (2020年6月), 1–1。'
- en: '(88) Gale, T., Zaharia, M., Young, C., and Elsen, E. Sparse GPU Kernels for
    Deep Learning. In SC20: International Conference for High Performance Computing,
    Networking, Storage and Analysis (2020), pp. 1–14.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(88) Gale, T., Zaharia, M., Young, C., 和 Elsen, E. 用于深度学习的稀疏 GPU 内核。在 SC20:
    高性能计算、网络、存储和分析国际会议 (2020), pp. 1–14。'
- en: (89) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., and Wang, Y. A Systematic
    Survey of General Sparse Matrix-Matrix Multiplication. ACM Comput. Surv. 55, 12
    (2023).
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (89) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., 和 Wang, Y. 一项关于通用稀疏矩阵-矩阵乘法的系统调查。ACM
    计算机调查 55, 12 (2023)。
- en: (90) Gates, M., YarKhan, A., Sukkari, D., Akbudak, K., Cayrols, S., Bielich,
    D., Abdelfattah, A., Farhan, M. A., and Dongarra, J. Portable and Efficient Dense
    Linear Algebra in the Beginning of the Exascale Era. In 2022 IEEE/ACM International
    Workshop on Performance, Portability and Productivity in HPC (P3HPC) (Nov 2022),
    pp. 36–46.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) Gates, M., YarKhan, A., Sukkari, D., Akbudak, K., Cayrols, S., Bielich,
    D., Abdelfattah, A., Farhan, M. A., 和 Dongarra, J. 在超大规模计算时代初期的便携且高效的密集线性代数。在
    2022 IEEE/ACM 国际高性能计算性能、可移植性和生产力研讨会 (P3HPC) (2022年11月), pp. 36–46。
- en: '(91) Geiger, L., and Team, P. Larq: An Open-Source Library for Training Binarized
    Neural Networks. Journal of Open Source Software 5, 45 (Jan. 2020), 1746.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(91) Geiger, L., 和 Team, P. Larq: 一个用于训练二值神经网络的开源库。开源软件杂志 5, 45 (2020年1月),
    1746。'
- en: '(92) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., and Shao, Y. S. Gemmini:
    Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC) (2021), pp. 769–774.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(92) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., 和 Shao, Y. S. Gemmini:
    通过全栈集成实现系统化深度学习架构评估。在 2021 第58届 ACM/IEEE 设计自动化会议 (DAC) (2021), pp. 769–774。'
- en: '(93) Ghosh, S., Raha, A., and Raghunathan, V. Approximate inference systems
    (AxIS): end-to-end approximations for energy-efficient inference at the edge.
    In ISLPED ’20: Proceedings of the ACM/IEEE International Symposium on Low Power
    Electronics and Design (08 2020), pp. 7–12.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) Ghosh, S., Raha, A., 和 Raghunathan, V. 近似推理系统（AxIS）：用于边缘的能源高效推理的端到端近似。在ISLPED
    ’20：ACM/IEEE国际低功耗电子与设计研讨会论文集（2020年8月），第7–12页。
- en: '(94) González-Vélez, H., and Leyton, M. A Survey of Algorithmic Skeleton Frameworks:
    High-Level Structured Parallel Programming Enablers. Softw. Pract. Exper. 40,
    12 (nov 2010), 1135–1160.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) González-Vélez, H., 和 Leyton, M. 算法骨架框架综述：高层次结构化并行编程的推动者。软件实践与经验 40, 12（2010年11月），1135–1160。
- en: (95) Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. Adaptive computation
    and machine learning. MIT Press, 2016.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Goodfellow, I., Bengio, Y., 和 Courville, A. 深度学习。自适应计算与机器学习。麻省理工学院出版社，2016年。
- en: (96) Graham, R. L. Bounds for certain multiprocessing anomalies. The Bell System
    Technical Journal 45, 9 (1966), 1563–1581.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) Graham, R. L. 某些多处理异常的界限。贝尔系统技术杂志 45, 9（1966），1563–1581。
- en: (97) GreenWaves Technologies. GAP8 Auto-tiler Manual. [https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html](https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html),
    2020.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) GreenWaves Technologies. GAP8自动排版器手册。 [https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html](https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html)，2020年。
- en: '(98) Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. In Proceedings
    of the 4th International Conference on Learning Representations (2016), ICLR ’16.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) Han, S., Mao, H., 和 Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。在第4届国际学习表示大会论文集中（2016年），ICLR
    ’16。
- en: (99) Haoxing Ren, J. H., Ed. Machine Learning Applications in Electronic Design
    Automation, 1st. ed. Mathematics and Statistics, Mathematics and Statistics (R0).
    Springer Cham, January 2023.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) Haoxing Ren, J. H., 编. 电子设计自动化中的机器学习应用，第1版。数学与统计学，数学与统计学（R0）。Springer Cham，2023年1月。
- en: '(100) Hashemi, S., Bahar, R. I., and Reda, S. DRUM: A Dynamic Range Unbiased
    Multiplier for approximate applications. In 2015 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD) (2015), pp. 418–425.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) Hashemi, S., Bahar, R. I., 和 Reda, S. DRUM：一种用于近似应用的动态范围无偏乘法器。在2015年IEEE/ACM国际计算机辅助设计大会（ICCAD）（2015年），第418–425页。
- en: (101) Hashemi, S., Bahar, R. I., and Reda, S. A low-power dynamic divider for
    approximate applications. In 2016 53nd ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2016), pp. 1–6.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Hashemi, S., Bahar, R. I., 和 Reda, S. 一种用于近似应用的低功耗动态分频器。发表于2016年第53届ACM/EDAC/IEEE设计自动化会议（DAC）（2016），第1–6页。
- en: (102) Hassanpour, M., Riera, M., and González, A. A Survey of Near-Data Processing
    Architectures for Neural Networks. Machine Learning and Knowledge Extraction 4,
    1 (2022), 66–102.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Hassanpour, M., Riera, M., 和 González, A. 神经网络的近数据处理架构综述。机器学习与知识提取 4,
    1（2022），66–102。
- en: '(103) Herault, T., Schuchart, J., Valeev, E. F., and Bosilca, G. Composition
    of Algorithmic Building Blocks in Template Task Graphs. In 2022 IEEE/ACM Parallel
    Applications Workshop: Alternatives To MPI+X (PAW-ATM) (2022), pp. 26–38.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) Herault, T., Schuchart, J., Valeev, E. F., 和 Bosilca, G. 算法构建块在模板任务图中的组合。在2022年IEEE/ACM并行应用研讨会：MPI+X的替代方案（PAW-ATM）（2022年），第26–38页。
- en: (104) Heroux, M. A., Bartlett, R. A., Howle, V. E., Hoekstra, R. J., Hu, J. J.,
    Kolda, T. G., Lehoucq, R. B., Long, K. R., Pawlowski, R. P., Phipps, E. T., Salinger,
    A. G., Thornquist, H. K., Tuminaro, R. S., Willenbring, J. M., Williams, A., and
    Stanley, K. S. An Overview of the Trilinos Project. ACM Trans. Math. Softw. 31,
    3 (sep 2005), 397–423.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) Heroux, M. A., Bartlett, R. A., Howle, V. E., Hoekstra, R. J., Hu, J.
    J., Kolda, T. G., Lehoucq, R. B., Long, K. R., Pawlowski, R. P., Phipps, E. T.,
    Salinger, A. G., Thornquist, H. K., Tuminaro, R. S., Willenbring, J. M., Williams,
    A., 和 Stanley, K. S. Trilinos项目概述。ACM Trans. Math. Softw. 31, 3（2005年9月），397–423。
- en: (105) Higham, N. J., and Mary, T. Mixed precision algorithms in numerical linear
    algebra. Acta Numerica 31 (2022), 347–414.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) Higham, N. J., 和 Mary, T. 数值线性代数中的混合精度算法。Acta Numerica 31（2022），347–414。
- en: '(106) Huang, G., Hu, J., He, Y., Liu, J., Ma, M., Shen, Z., Wu, J., Xu, Y.,
    Zhang, H., Zhong, K., Ning, X., Ma, Y., Yang, H., Yu, B., Yang, H., and Wang,
    Y. Machine learning for electronic design automation: A survey. ACM Trans. Des.
    Autom. Electron. Syst. 26, 5 (jun 2021).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Huang, G., Hu, J., He, Y., Liu, J., Ma, M., Shen, Z., Wu, J., Xu, Y.,
    Zhang, H., Zhong, K., Ning, X., Ma, Y., Yang, H., Yu, B., Yang, H., 和 Wang, Y.
    电子设计自动化中的机器学习：一项综述。ACM Trans. Des. Autom. Electron. Syst. 26, 5（2021年6月）。
- en: '(107) Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee,
    H., Ngiam, J., Le, Q. V., Wu, Y., and Chen, Z. GPipe: Efficient Training of Giant
    Neural Networks Using Pipeline Parallelism. In Proceedings of the 33rd International
    Conference on Neural Information Processing Systems (Red Hook, NY, USA, 2019),
    NeurIPS ’19, Curran Associates Inc.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee,
    H., Ngiam, J., Le, Q. V., Wu, Y., 和 Chen, Z. GPipe：使用管道并行性高效训练大型神经网络。见于第33届国际神经信息处理系统会议论文集（红钩，纽约，美国，2019），NeurIPS
    ’19，Curran Associates Inc.
- en: (108) IEEE. IEEE Draft Recommended Practice for Encryption and Management of
    Electronic Design Intellectual Property (IP) , February 2018.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) IEEE. IEEE草案推荐实践：电子设计知识产权（IP）的加密与管理，2018年2月。
- en: '(109) Imani, M., Garcia, R., Huang, A., and Rosing, T. CADE: Configurable Approximate
    Divider for Energy Efficiency. In 2019 Design, Automation & Test in Europe Conference
    & Exhibition (DATE) (2019), pp. 586–589.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (109) Imani, M., Garcia, R., Huang, A., 和 Rosing, T. CADE：用于能源效率的可配置近似除法器。见于2019设计、自动化与欧洲会议与展览（DATE）（2019），第586–589页。
- en: '(110) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., and
    Marculescu, D. QADAM: Quantization-Aware DNN Accelerator Modeling for Pareto-Optimality,
    2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (110) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., 和 Marculescu,
    D. QADAM：量化感知的DNN加速器建模用于帕累托最优，2022年。
- en: '(111) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., and
    Marculescu, D. QAPPA: Quantization-Aware Power, Performance, and Area Modeling
    of DNN Accelerators, 2022.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (111) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., 和 Marculescu,
    D. QAPPA：量化感知的DNN加速器功耗、性能和面积建模，2022年。
- en: (112) Intel. oneAPI Programming Model. [https://www.oneapi.io](https://www.oneapi.io),
    2020.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (112) Intel. oneAPI编程模型。 [https://www.oneapi.io](https://www.oneapi.io)，2020年。
- en: (113) Intel. Intel® High Level Synthesis Compiler Reference Manual, 2022.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (113) Intel. Intel®高级综合编译器参考手册，2022年。
- en: '(114) Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler, T. Data Movement
    Is All You Need: A Case Study of Transformer Networks. arXiv preprint arXiv:2007.00072
    (2020).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (114) Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., 和 Hoefler, T. 数据移动就是你所需的一切：Transformer网络的案例研究。arXiv预印本arXiv:2007.00072（2020）。
- en: (115) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam,
    H., and Kalenichenko, D. Quantization and Training of Neural Networks for Efficient
    Integer-arithmetic-only Inference. In Proceedings of the 2018 IEEE Conference
    on Computer Vision and Pattern Recognition (2018), pp. 2704–2713.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (115) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam,
    H., 和 Kalenichenko, D. 神经网络的量化与训练，用于高效的仅整数算术推理。见于2018 IEEE计算机视觉与模式识别会议论文集（2018），第2704–2713页。
- en: '(116) Ji, Z. HNMTP Conv: Optimize Convolution Algorithm for Single-Image Convolution
    Neural Network Inference on Mobile GPUs. CoRR abs/1909.02765 (2019).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (116) Ji, Z. HNMTP Conv：优化单图像卷积神经网络推理的卷积算法在移动GPU上的应用。CoRR abs/1909.02765（2019）。
- en: (117) Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring Hidden Dimensions
    in Parallelizing Convolutional Neural Networks. In Proceedings of the 35th International
    Conference on Machine Learning (2018), J. G. Dy and A. Krause, Eds., vol. 80 of
    ICML ’18, PMLR, pp. 2279–2288.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (117) Jia, Z., Lin, S., Qi, C. R., 和 Aiken, A. 探索并行卷积神经网络中的隐藏维度。见于第35届国际机器学习会议论文集（2018），J.
    G. Dy 和 A. Krause 编辑，第80卷，ICML ’18，PMLR，第2279–2288页。
- en: (118) Jia, Z., Zaharia, M., and Aiken, A. Beyond Data and Model Parallelism
    for Deep Neural Networks. In Proceedings of Machine Learning and Systems 1 (2019),
    vol. 1 of MLSys ’19, pp. 1–13.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) Jia, Z., Zaharia, M., 和 Aiken, A. 超越深度神经网络的数据与模型并行性。见于机器学习与系统第1卷（2019），MLSYS
    ’19第1卷，第1–13页。
- en: '(119) Jiang, H., Santiago, F. J. H., Mo, H., Liu, L., and Han, J. Approximate
    Arithmetic Circuits: A Survey, Characterization, and Recent Applications. Proceedings
    of the IEEE 108, 12 (2020), 2108–2135.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (119) Jiang, H., Santiago, F. J. H., Mo, H., Liu, L., 和 Han, J. 近似算术电路：综述、特征化及最新应用。IEEE期刊，第108卷，第12期（2020），第2108–2135页。
- en: (120) Jiang, W., Yang, L., Sha, E. H.-M., Zhuge, Q., Gu, S., Dasgupta, S., Shi,
    Y., and Hu, J. Hardware/software co-exploration of neural architectures. IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 12
    (2020), 4805–4815.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (120) Jiang, W., Yang, L., Sha, E. H.-M., Zhuge, Q., Gu, S., Dasgupta, S., Shi,
    Y., 和 Hu, J. 神经架构的硬件/软件协同探索。IEEE计算机辅助设计集成电路与系统期刊，第39卷，第12期（2020），第4805–4815页。
- en: (121) Jin, Q., Yang, L., and Liao, Z. Towards Efficient Training for Neural
    Network Quantization. arXiv preprint arXiv:1912.10207 abs/1912.10207 (2019).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (121) Jin, Q., Yang, L., 和 Liao, Z. 迈向高效的神经网络量化训练。arXiv预印本arXiv:1912.10207 abs/1912.10207（2019）。
- en: (122) Jin, T., Bercea, G.-T., Le, T. D., Chen, T., Su, G., Imai, H., Negishi,
    Y., Leu, A., O’Brien, K., Kawachiya, K., and Eichenberger, A. E. Compiling ONNX
    Neural Network Models Using MLIR, Sept. 2020.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (122) Jin, T., Bercea, G.-T., Le, T. D., Chen, T., Su, G., Imai, H., Negishi,
    Y., Leu, A., O’Brien, K., Kawachiya, K., 和 Eichenberger, A. E. 使用 MLIR 编译 ONNX
    神经网络模型，2020年9月。
- en: '(123) Jin, Z., Finkel, H., Yoshii, K., and Cappello, F. Evaluation of a Floating-Point
    Intensive Kernel on FPGA. In Euro-Par 2017: Parallel Processing Workshops (2018),
    pp. 664–675.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(123) Jin, Z., Finkel, H., Yoshii, K., 和 Cappello, F. 在 FPGA 上评估浮点密集型内核。载于
    Euro-Par 2017: 并行处理研讨会（2018），页码 664–675。'
- en: '(124) Juracy, L. R., de Morais Amory, A., and Moraes, F. G. A Fast, Accurate,
    and Comprehensive PPA Estimation of Convolutional Hardware Accelerators. IEEE
    Transactions on Circuits and Systems I: Regular Papers 69, 12 (2022), 5171–5184.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(124) Juracy, L. R., de Morais Amory, A., 和 Moraes, F. G. 对卷积硬件加速器进行快速、准确和全面的
    PPA 估算。IEEE Transactions on Circuits and Systems I: Regular Papers 69, 12 (2022),
    5171–5184。'
- en: '(125) Kahng, A. B. Machine learning for CAD/EDA: The road ahead. IEEE Design
    & Test 40, 1 (2023), 8–16.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (125) Kahng, A. B. CAD/EDA 领域的机器学习：未来之路。IEEE Design & Test 40, 1 (2023), 8–16。
- en: (126) Kang, H., Gibbons, P. B., Blelloch, G. E., Dhulipala, L., Gu, Y., and
    McGuffey, C. The Processing-in-Memory Model. In Proceedings of the 33rd ACM Symposium
    on Parallelism in Algorithms and Architectures (New York, NY, USA, 2021), SPAA
    ’21, Association for Computing Machinery, p. 295–306.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (126) Kang, H., Gibbons, P. B., Blelloch, G. E., Dhulipala, L., Gu, Y., 和 McGuffey,
    C. 内存处理模型。载于第33届 ACM 并行算法与架构研讨会论文集（纽约, NY, USA, 2021），SPAA ’21，计算机协会，页码 295–306。
- en: '(127) Kang, H., Zhao, Y., Blelloch, G. E., Dhulipala, L., Gu, Y., McGuffey,
    C., and Gibbons, P. B. PIM-tree: A Skew-resistant Index for Processing-in-Memory.
    arXiv preprint arXiv:2211.10516 (2022).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(127) Kang, H., Zhao, Y., Blelloch, G. E., Dhulipala, L., Gu, Y., McGuffey,
    C., 和 Gibbons, P. B. PIM-tree: 一种抗偏斜的内存处理索引。arXiv 预印本 arXiv:2211.10516 (2022)。'
- en: (128) Kernighan, B. W., and Lin, S. An efficient heuristic procedure for partitioning
    graphs. The Bell System Technical Journal 49, 2 (1970), 291–307.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (128) Kernighan, B. W., 和 Lin, S. 一种高效的图划分启发式方法。贝尔系统技术杂志 49, 2 (1970), 291–307。
- en: (129) Kim, M. S., Del Barrio, A. A., Kim, H., and Bagherzadeh, N. The Effects
    of Approximate Multiplication on Convolutional Neural Networks. IEEE Transactions
    on Emerging Topics in Computing 10, 2 (2022), 904–916.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (129) Kim, M. S., Del Barrio, A. A., Kim, H., 和 Bagherzadeh, N. 近似乘法对卷积神经网络的影响。IEEE
    Transactions on Emerging Topics in Computing 10, 2 (2022), 904–916。
- en: (130) Kim, S., Wang, J., Seo, Y., Lee, S., Park, Y., Park, S., and Park, C. S.
    Transaction-level Model Simulator for Communication-Limited Accelerators, 2020.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (130) Kim, S., Wang, J., Seo, Y., Lee, S., Park, Y., Park, S., 和 Park, C. S.
    用于通信受限加速器的事务级模型模拟器，2020年。
- en: (131) Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classification
    with Deep Convolutional Neural Networks. In Proceedings of the 25th International
    Conference on Neural Information Processing Systems - Volume 1 (2012), NIPS’12,
    Curran Associates Inc., p. 1097–1105.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (131) Krizhevsky, A., Sutskever, I., 和 Hinton, G. E. 使用深度卷积神经网络进行 ImageNet 分类。载于第25届国际神经信息处理系统会议论文集
    - 第1卷（2012），NIPS’12，Curran Associates Inc.，页码 1097–1105。
- en: '(132) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., and Parashar,
    A. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware
    Cost of DNN Mappings. IEEE Micro 40, 3 (2020), 20–29.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(132) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., 和 Parashar,
    A. MAESTRO: 一种以数据为中心的方法，用于理解 DNN 映射的重用、性能和硬件成本。IEEE Micro 40, 3 (2020), 20–29。'
- en: '(133) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., and Parashar,
    A. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware
    Cost of DNN Mappings. IEEE Micro 40, 3 (2020), 20–29.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(133) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., 和 Parashar,
    A. MAESTRO: 一种以数据为中心的方法，用于理解 DNN 映射的重用、性能和硬件成本。IEEE Micro 40, 3 (2020), 20–29。'
- en: '(134) Labini, P. S., Bernaschi, M., Nutt, W., Silvestri, F., and Vella, F.
    Blocking Sparse Matrices to Leverage Dense-Specific Multiplication. In 2022 IEEE/ACM
    Workshop on Irregular Applications: Architectures and Algorithms (IA3) (2022),
    pp. 19–24.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (134) Labini, P. S., Bernaschi, M., Nutt, W., Silvestri, F., 和 Vella, F. 阻塞稀疏矩阵以利用专用的密集乘法。在2022年
    IEEE/ACM 非规则应用：架构与算法研讨会（IA3）中（2022），页码 19–24。
- en: '(135) Lammie, C., Xiang, W., Linares-Barranco, B., and Azghadi, M. R. MemTorch:
    An Open-source Simulation Framework for Memristive Deep Learning Systems. Neurocomputing
    (2022).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(135) Lammie, C., Xiang, W., Linares-Barranco, B., 和 Azghadi, M. R. MemTorch:
    一个开源的记忆电阻深度学习系统仿真框架。Neurocomputing (2022)。'
- en: '(136) Lant, J., Navaridas, J., Luján, M., and Goodacre, J. Toward FPGA-Based
    HPC: Advancing Interconnect Technologies. IEEE Micro 40, 1 (2020), 25–34.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (136) Lant, J., Navaridas, J., Luján, M., 和 Goodacre, J. 朝向基于FPGA的高性能计算：推动互连技术。IEEE
    Micro 40, 1 (2020), 25–34。
- en: '(137) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In IEEE/ACM International Symposium
    on Code Generation and Optimization (CGO) (2021), pp. 2–14.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (137) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., 和 Zinenko, O. MLIR：面向特定领域计算的编译器基础设施扩展。载于IEEE/ACM国际代码生成与优化研讨会
    (CGO)（2021年），第2–14页。
- en: '(138) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In 2021 IEEE/ACM International
    Symposium on Code Generation and Optimization (CGO) (Feb. 2021), pp. 2–14.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (138) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., 和 Zinenko, O. MLIR：面向特定领域计算的编译器基础设施扩展。载于2021
    IEEE/ACM国际代码生成与优化研讨会 (CGO)（2021年2月），第2–14页。
- en: (139) Lattuada, M., and Ferrandi, F. Code Transformations Based on Speculative
    SDC Scheduling. In IEEE/ACM International Conference on Computer-Aided Design
    (Nov 2015), ICCAD ’15, pp. 71–77.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (139) Lattuada, M., 和 Ferrandi, F. 基于猜测性SDC调度的代码转换。载于IEEE/ACM国际计算机辅助设计会议 (Nov
    2015)，ICCAD ’15，第71–77页。
- en: '(140) Lawler, E. L., Lenstra, J. K., Rinnooy Kan, A. H., and Shmoys, D. B.
    Chapter 9 Sequencing and scheduling: Algorithms and complexity. In Logistics of
    Production and Inventory, vol. 4 of Handbooks in Operations Research and Management
    Science. Elsevier, 1993, pp. 445–522.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (140) Lawler, E. L., Lenstra, J. K., Rinnooy Kan, A. H., 和 Shmoys, D. B. 第9章
    序列化和调度：算法与复杂性。载于《生产与库存的物流》，第4卷，《运筹学与管理科学手册》。Elsevier, 1993年，第445–522页。
- en: (141) Li, B., Samsi, S., Gadepally, V., and Tiwari, D. Green Carbon Footprint
    for Model Inference Serving via Exploiting Mixed-Quality Models and GPU Partitioning.
    CoRR abs/2304.09781 (2023).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (141) Li, B., Samsi, S., Gadepally, V., 和 Tiwari, D. 通过利用混合质量模型和GPU分区的模型推理服务的绿色碳足迹。CoRR
    abs/2304.09781 (2023)。
- en: '(142) Lin, W.-F., Tsai, D.-Y., Tang, L., Hsieh, C.-T., Chou, C.-Y., Chang,
    P.-H., and Hsu, L. ONNC: A Compilation Framework Connecting ONNX to Proprietary
    Deep Learning Accelerators. In 2019 IEEE International Conference on Artificial
    Intelligence Circuits and Systems (AICAS) (Mar. 2019), pp. 214–218.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (142) Lin, W.-F., Tsai, D.-Y., Tang, L., Hsieh, C.-T., Chou, C.-Y., Chang, P.-H.,
    和 Hsu, L. ONNC：连接ONNX与专有深度学习加速器的编译框架。载于2019 IEEE国际人工智能电路与系统会议 (AICAS)（2019年3月），第214–218页。
- en: (143) Liu, Z., Calciu, I., Herlihy, M., and Mutlu, O. Concurrent Data Structures
    for Near-Memory Computing. In Proceedings of the 29th ACM Symposium on Parallelism
    in Algorithms and Architectures (New York, NY, USA, 2017), SPAA ’17, Association
    for Computing Machinery, p. 235–245.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (143) Liu, Z., Calciu, I., Herlihy, M., 和 Mutlu, O. 近存储计算的并发数据结构。载于第29届ACM并行算法与架构研讨会
    (New York, NY, USA, 2017)，SPAA ’17，计算机协会，第235–245页。
- en: (144) Lu, T., Chen, Y.-F., Hechtman, B., Wang, T., and Anderson, J. Large-Scale
    Discrete Fourier Transform on TPUs. IEEE Access 9 (2021), 93422–93432.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (144) Lu, T., Chen, Y.-F., Hechtman, B., Wang, T., 和 Anderson, J. 大规模离散傅里叶变换在TPU上的实现。IEEE
    Access 9 (2021), 93422–93432。
- en: (145) Mathieu, M., Henaff, M., and LeCun, Y. Fast Training of Convolutional
    Networks through FFTs, 2014.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (145) Mathieu, M., Henaff, M., 和 LeCun, Y. 通过FFT快速训练卷积网络，2014年。
- en: '(146) Mayer, R., and Jacobsen, H.-A. Scalable Deep Learning on Distributed
    Infrastructures: Challenges, Techniques, and Tools. ACM Comput. Surv. 53, 1 (2021).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (146) Mayer, R., 和 Jacobsen, H.-A. 在分布式基础设施上的可扩展深度学习：挑战、技术与工具。ACM Comput. Surv.
    53, 1 (2021)。
- en: '(147) Mayer, R., Mayer, C., and Laich, L. The Tensorflow Partitioning and Scheduling
    Problem: It’s the Critical Path! In Proceedings of the 1st Workshop on Distributed
    Infrastructures for Deep Learning (New York, NY, USA, 2017), DIDL ’17, ACM.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (147) Mayer, R., Mayer, C., 和 Laich, L. Tensorflow分区与调度问题：关键路径！载于第1届深度学习分布式基础设施研讨会
    (New York, NY, USA, 2017)，DIDL ’17，ACM。
- en: (148) MehriDehnavi, M., El-Kurdi, Y., Demmel, J., and Giannacopoulos, D. Communication-Avoiding
    Krylov Techniques on Graphic Processing Units. IEEE Transactions on Magnetics
    49, 5 (2013), 1749–1752.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (148) MehriDehnavi, M., El-Kurdi, Y., Demmel, J., 和 Giannacopoulos, D. 在图形处理单元上的避免通信的Krylov技术。IEEE
    Transactions on Magnetics 49, 5 (2013), 1749–1752。
- en: (149) Meyer, M., Kenter, T., and Plessl, C. Evaluating FPGA Accelerator Performance
    with a Parameterized OpenCL Adaptation of Selected Benchmarks of the HPCChallenge
    Benchmark Suite. In 2020 IEEE/ACM International Workshop on Heterogeneous High-performance
    Reconfigurable Computing (H2RC) (2020), pp. 10–18.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (149) Meyer, M., Kenter, T., 和 Plessl, C. 使用参数化的OpenCL适配对HPCChallenge基准套件中选择的基准进行FPGA加速器性能评估。2020
    IEEE/ACM国际异构高性能可重构计算研讨会（H2RC）（2020年），第10–18页。
- en: (150) Microchip. Smart High-Level Synthesis Tool Suite. [https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler](https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler),
    2020.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (150) Microchip. 智能高级综合工具套件。 [https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler](https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler)，2020年。
- en: (151) Minutoli, M., et al. Inter-procedural resource sharing in High Level Synthesis
    through function proxies. In International Conference on Field Programmable Logic
    and Applications, FPL (Sept 2015), pp. 1–8.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (151) Minutoli, M., 等。通过函数代理的高层次综合中的跨过程资源共享。国际现场可编程逻辑与应用会议，FPL（2015年9月），第1–8页。
- en: (152) Mirhoseini, A., Goldie, A., Pham, H., Steiner, B., Le, Q. V., and Dean,
    J. A Hierarchical Model for Device Placement. In Proceedings of Machine Learning
    and Systems (2018), MLSys ’18.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (152) Mirhoseini, A., Goldie, A., Pham, H., Steiner, B., Le, Q. V., 和 Dean,
    J. 设备放置的分层模型。机器学习与系统会议论文集（2018年），MLSys ’18。
- en: (153) Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y.,
    Kumar, N., Norouzi, M., Bengio, S., and Dean, J. Device Placement Optimization
    with Reinforcement Learning. In Proceedings of the 34th International Conference
    on Machine Learning - Volume 70 (2017), ICML ’17, JMLR.org, pp. 2430–2439.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (153) Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y.,
    Kumar, N., Norouzi, M., Bengio, S., 和 Dean, J. 使用强化学习进行设备放置优化。第34届国际机器学习会议论文集
    - 第70卷（2017年），ICML ’17，JMLR.org，第2430–2439页。
- en: (154) Moreau, T., Chen, T., Vega, L., Roesch, J., Yan, E., Zheng, L., Fromm,
    J., Jiang, Z., Ceze, L., Guestrin, C., et al. A hardware–software blueprint for
    flexible deep learning specialization. Micro (2019).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (154) Moreau, T., Chen, T., Vega, L., Roesch, J., Yan, E., Zheng, L., Fromm,
    J., Jiang, Z., Ceze, L., Guestrin, C., 等。灵活深度学习专业化的硬件–软件蓝图。Micro（2019年）。
- en: '(155) ”msr fiddle”. PipeDream: Pipeline Parallelism for DNN Training, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (155) “msr fiddle”。PipeDream：DNN训练的流水线并行，2020年。
- en: (156) Muralidharan, S., O’Brien, K., and Lalanne, C. A Semi-Automated Tool Flow
    for Roofline Anaylsis of OpenCL Kernels on Accelerators. In First International
    Workshop on Heterogeneous High-performance Reconfigurable Computing (2015).
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (156) Muralidharan, S., O’Brien, K., 和 Lalanne, C. 针对加速器上的OpenCL内核的屋顶线分析的半自动化工具流程。第一次国际异构高性能可重构计算研讨会（2015年）。
- en: '(157) Murray, R., Demmel, J., Mahoney, M. W., Erichson, N. B., Melnichenko,
    M., Malik, O. A., Grigori, L., Luszczek, P., Dereziński, M., Lopes, M. E., Liang,
    T., Luo, H., and Dongarra, J. Randomized Numerical Linear Algebra : A Perspective
    on the Field With an Eye to Software, 2023.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (157) Murray, R., Demmel, J., Mahoney, M. W., Erichson, N. B., Melnichenko,
    M., Malik, O. A., Grigori, L., Luszczek, P., Dereziński, M., Lopes, M. E., Liang,
    T., Luo, H., 和 Dongarra, J. 随机数值线性代数：对该领域的视角及软件的展望，2023年。
- en: '(158) Muñoz-Martínez, F., Abellán, J. L., Acacio, M. E., and Krishna, T. STONNE:
    A Detailed Architectural Simulator for Flexible Neural Network Accelerators, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (158) Muñoz-Martínez, F., Abellán, J. L., Acacio, M. E., 和 Krishna, T. STONNE：一个详细的灵活神经网络加速器架构模拟器，2020年。
- en: '(159) Nabi, S. W., and Vanderbauwhede, W. MP-STREAM: A Memory Performance Benchmark
    for Design Space Exploration on Heterogeneous HPC Devices. In 2018 IEEE International
    Parallel and Distributed Processing Symposium Workshops (IPDPSW) (2018), pp. 194–197.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (159) Nabi, S. W., 和 Vanderbauwhede, W. MP-STREAM：用于异构HPC设备设计空间探索的内存性能基准。2018
    IEEE国际并行与分布式处理研讨会研讨会（IPDPSW）（2018年），第194–197页。
- en: '(160) Nagasu, K., Sano, K., Kono, F., and Nakasato, N. FPGA-based tsunami simulation:
    Performance comparison with GPUs, and roofline model for scalability analysis.
    Journal of Parallel and Distributed Computing 106 (2017), 153–169.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (160) Nagasu, K., Sano, K., Kono, F., 和 Nakasato, N. 基于FPGA的海啸模拟：与GPU的性能比较及可扩展性分析的屋顶线模型。并行与分布式计算杂志
    106（2017年），第153–169页。
- en: (161) Nane, R., Sima, V., Pilato, C., Choi, J., Fort, B., Canis, A., Chen, Y. T.,
    Hsiao, H., Brown, S., Ferrandi, F., Anderson, J., and Bertels, K. A Survey and
    Evaluation of FPGA High-Level Synthesis Tools. IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems 35, 10 (Oct 2016), 1591–1604.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (161) Nane, R., Sima, V., Pilato, C., Choi, J., Fort, B., Canis, A., Chen, Y.
    T., Hsiao, H., Brown, S., Ferrandi, F., Anderson, J., and Bertels, K. FPGA 高级综合工具的调查与评估。IEEE《计算机辅助设计集成电路与系统汇刊》35，10（2016年10月），1591–1604。
- en: (162) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., and Kim, N. S.
    Energy-Efficient Approximate Multiplication for Digital Signal Processing and
    Classification Applications. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 23, 6 (2015), 1180–1184.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (162) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., and Kim, N.
    S. 数字信号处理与分类应用中的节能近似乘法。IEEE《超大规模集成（VLSI）系统汇刊》23，6（2015年），1180–1184。
- en: '(163) Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R.,
    Ganger, G. R., Gibbons, P. B., and Zaharia, M. PipeDream: Generalized Pipeline
    Parallelism for DNN Training. In Proceedings of the 27th ACM Symposium on Operating
    Systems Principles (New York, NY, USA, 2019), SOSP ’19, ACM.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (163) Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N.
    R., Ganger, G. R., Gibbons, P. B., and Zaharia, M. PipeDream：DNN训练的广义管道并行性。在第27届ACM操作系统原理研讨会论文集中（纽约，美国，2019年），SOSP
    ’19，ACM。
- en: '(164) Nguyen, T., MacLean, C., Siracusa, M., Doerfler, D., Wright, N. J., and
    Williams, S. FPGA-based HPC accelerators: An evaluation on performance and energy
    efficiency. Concurrency and Computation: Practice and Experience n/a, n/a (2021),
    e6570.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (164) Nguyen, T., MacLean, C., Siracusa, M., Doerfler, D., Wright, N. J., and
    Williams, S. 基于FPGA的HPC加速器：性能和能效评估。并发计算：实践与经验 n/a, n/a（2021年），e6570。
- en: (165) Nguyen, T., Williams, S., Siracusa, M., MacLean, C., Doerfler, D., and
    Wright, N. J. The Performance and Energy Efficiency Potential of FPGAs in Scientific
    Computing. In 2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation
    of High Performance Computer Systems (PMBS) (2020), pp. 8–19.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (165) Nguyen, T., Williams, S., Siracusa, M., MacLean, C., Doerfler, D., and
    Wright, N. J. FPGA在科学计算中的性能与能效潜力。见于2020年IEEE/ACM高性能计算系统性能建模、基准测试与模拟研讨会（PMBS）（2020年），第8–19页。
- en: (166) Nurvitadhi, E., Mishra, A., and Marr, D. A sparse matrix vector multiply
    accelerator for support vector machine. In 2015 International Conference on Compilers,
    Architecture and Synthesis for Embedded Systems (CASES) (2015), pp. 109–116.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (166) Nurvitadhi, E., Mishra, A., and Marr, D. 用于支持向量机的稀疏矩阵向量乘法加速器。见于2015年嵌入式系统编译器、架构与综合国际会议（CASES）（2015年），第109–116页。
- en: '(167) NVIDIA. NVIDIA Deep Learning Accelerator. [http://nvdla.org/](http://nvdla.org/),
    2022. Accessed: 2023-04-18.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (167) NVIDIA. NVIDIA 深度学习加速器。 [http://nvdla.org/](http://nvdla.org/)，2022年。访问时间：2023-04-18。
- en: (168) PAI”, A. G. DAPPLE, 2020.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (168) PAI, A. G. DAPPLE, 2020。
- en: (169) Paliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli, P., and Vinyals,
    O. Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs. In
    Proceedings of the 8th International Conference on Learning Representations (2020),
    ICLR ’20.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (169) Paliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli, P., and Vinyals,
    O. 强化遗传算法学习用于优化计算图。在第8届国际学习表征会议论文集中（2020年），ICLR ’20。
- en: (170) Papadimitriou, C. H., and Yannakakis, M. Towards an Architecture-Independent
    Analysis of Parallel Algorithms. SIAM J. Comput. 19, 2 (1990), 322–328.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (170) Papadimitriou, C. H., and Yannakakis, M. 面向架构无关的并行算法分析。SIAM《计算杂志》19，2（1990年），322–328。
- en: '(171) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop: A systematic
    approach to dnn accelerator evaluation. In 2019 IEEE international symposium on
    performance analysis of systems and software (ISPASS) (2019), IEEE, pp. 304–315.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (171) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop：DNN加速器评估的系统方法。见于2019年IEEE系统与软件性能分析国际研讨会（ISPASS）（2019年），IEEE，第304–315页。
- en: '(172) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In 2019 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS) (2019), pp. 304–315.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (172) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop：DNN加速器评估的系统方法。见于2019年IEEE系统与软件性能分析国际研讨会（ISPASS）（2019年），第304–315页。
- en: (173) Pei, Y., Bosilca, G., and Dongarra, J. Sequential Task Flow Runtime Model
    Improvements and Limitations. In 2022 IEEE/ACM International Workshop on Runtime
    and Operating Systems for Supercomputers (ROSS) (Nov 2022), pp. 1–8.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (173) Pei, Y., Bosilca, G., 和 Dongarra, J. 顺序任务流运行时模型的改进与局限性。见于 2022 IEEE/ACM
    国际超级计算机运行时与操作系统研讨会 (ROSS) (2022年11月), 页码：1–8。
- en: '(174) Peng, X., Huang, S., Luo, Y., Sun, X., and Yu, S. DNN+NeuroSim: An End-to-End
    Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device
    Technologies. In 2019 IEEE International Electron Devices Meeting (IEDM) (2019),
    pp. 32.5.1–32.5.4.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (174) Peng, X., Huang, S., Luo, Y., Sun, X., 和 Yu, S. DNN+NeuroSim：一个用于具有多种设备技术的内存计算加速器的端到端基准测试框架。见于
    2019 IEEE 国际电子设备会议 (IEDM) (2019), 页码：32.5.1–32.5.4。
- en: (175) Perri, S., Spagnolo, F., Frustaci, F., and Corsonello, P. Efficient Approximate
    Adders for FPGA-Based Data-Paths. Electronics 9, 9 (2020).
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (175) Perri, S., Spagnolo, F., Frustaci, F., 和 Corsonello, P. FPGA 基于数据路径的高效近似加法器。Electronics
    9, 9 (2020)。
- en: (176) Perri, S., Spagnolo, F., Frustaci, F., and Corsonello, P. Designing Energy-Efficient
    Approximate Multipliers. Journal of Low Power Electronics and Applications 12,
    4 (2022).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (176) Perri, S., Spagnolo, F., Frustaci, F., 和 Corsonello, P. 设计节能的近似乘法器。低功耗电子学与应用期刊
    12, 4 (2022)。
- en: '(177) Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P., Shyu,
    M.-L., Chen, S.-C., and Iyengar, S. S. A survey on deep learning: Algorithms,
    techniques, and applications. ACM Comput. Surv. 51, 5 (sep 2018).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (177) Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P., Shyu,
    M.-L., Chen, S.-C., 和 Iyengar, S. S. 深度学习调查：算法、技术与应用。ACM Comput. Surv. 51, 5 (2018年9月)。
- en: '(178) Prabakaran, B. S., Rehman, S., Hanif, M. A., Ullah, S., Mazaheri, G.,
    Kumar, A., and Shafique, M. DeMAS: An efficient design methodology for building
    approximate adders for FPGA-based systems. In 2018 Design, Automation & Test in
    Europe Conference & Exhibition (DATE) (2018), pp. 917–920.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (178) Prabakaran, B. S., Rehman, S., Hanif, M. A., Ullah, S., Mazaheri, G.,
    Kumar, A., 和 Shafique, M. DeMAS：一种高效的设计方法学，用于构建 FPGA 基于系统的近似加法器。见于 2018 设计、自动化与欧洲会议及展览
    (DATE) (2018), 页码：917–920。
- en: (179) Psarras, C., Barthels, H., and Bientinesi, P. The Linear Algebra Mapping
    Problem. Current State of Linear Algebra Languages and Libraries. ACM Trans. Math.
    Softw. 48, 3 (sep 2022).
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (179) Psarras, C., Barthels, H., 和 Bientinesi, P. 线性代数映射问题。线性代数语言和库的现状。ACM Trans.
    Math. Softw. 48, 3 (2022年9月)。
- en: (180) Putnam, A., Caulfield, A. M., Chung, E. S., Chiou, D., Constantinides,
    K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G. P., Gray, J., Haselman,
    M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson,
    E., Pope, S., Smith, A., Thong, J., Xiao, P. Y., and Burger, D. A Reconfigurable
    Fabric for Accelerating Large-Scale Datacenter Services. IEEE Micro 35, 3 (2015),
    10–22.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (180) Putnam, A., Caulfield, A. M., Chung, E. S., Chiou, D., Constantinides,
    K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G. P., Gray, J., Haselman,
    M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson,
    E., Pope, S., Smith, A., Thong, J., Xiao, P. Y., 和 Burger, D. 一种用于加速大规模数据中心服务的可重构结构。IEEE
    Micro 35, 3 (2015), 页码：10–22。
- en: (181) Quintana-Ortí, G., Quintana-Ortí, E. S., Geijn, R. A. V. D., Zee, F. G. V.,
    and Chan, E. Programming Matrix Algorithms-by-Blocks for Thread-Level Parallelism.
    ACM Trans. Math. Softw. 36, 3 (jul 2009).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (181) Quintana-Ortí, G., Quintana-Ortí, E. S., Geijn, R. A. V. D., Zee, F. G.
    V., 和 Chan, E. 为线程级并行性编程矩阵算法。ACM Trans. Math. Softw. 36, 3 (2009年7月)。
- en: '(182) Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., and Amarasinghe,
    S. Halide: a language and compiler for optimizing parallelism, locality, and recomputation
    in image processing pipelines. Acm Sigplan Notices 48, 6 (2013), 519–530.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (182) Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., 和 Amarasinghe,
    S. Halide：一种用于优化图像处理管道中并行性、局部性和重计算的语言和编译器。ACM Sigplan Notices 48, 6 (2013), 页码：519–530。
- en: '(183) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., and Roy, K. Exploring Neuromorphic Computing Based on Spiking Neural Networks:
    Algorithms to Hardware. ACM Comput. Surv. 55, 12 (2023).'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (183) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., 和 Roy, K. 探索基于脉冲神经网络的神经形态计算：从算法到硬件。ACM Comput. Surv. 55, 12 (2023)。
- en: (184) Reiser, D., Reichenbach, M., Rizzi, T., Baroni, A., Fritscher, M., Wenger,
    C., Zambelli, C., and Bertozzi, D. Technology-Aware Drift Resilience Analysis
    of RRAM Crossbar Array Configurations. In 2023 IEEE NEWCAS (2023), p. in press.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (184) Reiser, D., Reichenbach, M., Rizzi, T., Baroni, A., Fritscher, M., Wenger,
    C., Zambelli, C., 和 Bertozzi, D. 技术感知的 RRAM 十字交叉阵列配置的漂移弹性分析。见于 2023 IEEE NEWCAS
    (2023), 页码：待印刷。
- en: '(185) Roesch, J., Lyubomirsky, S., Weber, L., Pollock, J., Kirisame, M., Chen,
    T., and Tatlock, Z. Relay: A New IR for Machine Learning Frameworks. In Proceedings
    of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages (New York, NY, USA, June 2018), MAPL 2018, Association for Computing
    Machinery, pp. 58–68.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (185) Roesch, J., Lyubomirsky, S., Weber, L., Pollock, J., Kirisame, M., Chen,
    T., 和 Tatlock, Z. Relay：用于机器学习框架的新型 IR。发表于第二届 ACM SIGPLAN 国际机器学习与编程语言研讨会（纽约，NY，美国，2018年6月），MAPL
    2018，计算机协会，第58–68页。
- en: (186) Ronak, B., and Fahmy, S. A. Mapping for Maximum Performance on FPGA DSP
    Blocks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
    Systems 35, 4 (April 2016), 573–585.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (186) Ronak, B., 和 Fahmy, S. A. FPGA DSP 块的最大性能映射。IEEE 计算机辅助设计集成电路与系统汇刊 35,
    4（2016年4月），573–585页。
- en: '(187) Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S., Dzhabarov,
    R., Gibson, N., Hegeman, J., Lele, M., Levenstein, R., Montgomery, J., Maher,
    B., Nadathur, S., Olesen, J., Park, J., Rakhov, A., Smelyanskiy, M., and Wang,
    M. Glow: Graph Lowering Compiler Techniques for Neural Networks. arXiv:1805.00907
    [cs] (Apr. 2019).'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (187) Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S., Dzhabarov,
    R., Gibson, N., Hegeman, J., Lele, M., Levenstein, R., Montgomery, J., Maher,
    B., Nadathur, S., Olesen, J., Park, J., Rakhov, A., Smelyanskiy, M., 和 Wang, M.
    Glow：神经网络的图形降级编译技术。arXiv:1805.00907 [cs]（2019年4月）。
- en: '(188) Russo, E., Palesi, M., Monteleone, S., Patti, D., Ascia, G., and Catania,
    V. LAMBDA: An Open Framework for Deep Neural Network Accelerators Simulation.
    In 2021 IEEE International Conference on Pervasive Computing and Communications
    Workshops and other Affiliated Events (PerCom Workshops) (2021), pp. 161–166.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (188) Russo, E., Palesi, M., Monteleone, S., Patti, D., Ascia, G., 和 Catania,
    V. LAMBDA：用于深度神经网络加速器仿真的开放框架。发表于 2021 年 IEEE 国际普适计算与通信会议研讨会及其他相关活动（PerCom Workshops）（2021年），第161–166页。
- en: (189) Russo Russo, G., Cardellini, V., and Lo Presti, F. Hierarchical Auto-Scaling
    Policies for Data Stream Processing on Heterogeneous Resources. ACM Trans. Auton.
    Adapt. Syst. (2023).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (189) Russo Russo, G., Cardellini, V., 和 Lo Presti, F. 数据流处理的分层自动缩放策略在异构资源上的应用。ACM
    Trans. Auton. Adapt. Syst.（2023年）。
- en: (190) Saadat, H., Javaid, H., and Parameswaran, S. Approximate Integer and Floating-Point
    Dividers with Near-Zero Error Bias. In 2019 56th ACM/IEEE Design Automation Conference
    (DAC) (06 2019), pp. 1–6.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (190) Saadat, H., Javaid, H., 和 Parameswaran, S. 近零误差偏差的近似整数和浮点除法器。发表于 2019年第56届
    ACM/IEEE 设计自动化会议（DAC）（2019年6月），第1–6页。
- en: '(191) Saleem, R., Yuan, B., Kurugollu, F., Anjum, A., and Liu, L. Explaining
    deep neural networks: A survey on the global interpretation methods. Neurocomputing
    513 (2022), 165–180.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (191) Saleem, R., Yuan, B., Kurugollu, F., Anjum, A., 和 Liu, L. 解释深度神经网络：全球解释方法的综述。Neurocomputing
    513（2022年），165–180页。
- en: (192) Samajdar, A., Joseph, J. M., Zhu, Y., Whatmough, P., Mattina, M., and
    Krishna, T. A systematic methodology for characterizing scalability of DNN accelerators
    using SCALE-sim. In 2020 IEEE International Symposium on Performance Analysis
    of Systems and Software (ISPASS) (2020), IEEE, pp. 58–68.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (192) Samajdar, A., Joseph, J. M., Zhu, Y., Whatmough, P., Mattina, M., 和 Krishna,
    T. 使用 SCALE-sim 系统性方法表征 DNN 加速器的可扩展性。发表于 2020 年 IEEE 系统和软件性能分析国际研讨会（ISPASS）（2020年），IEEE，第58–68页。
- en: '(193) Samajdar, A., Zhu, Y., Whatmough, P., Mattina, M., and Krishna, T. SCALE-Sim:
    Systolic CNN Accelerator Simulator. arXiv preprint arXiv:1811.02883 (2018).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (193) Samajdar, A., Zhu, Y., Whatmough, P., Mattina, M., 和 Krishna, T. SCALE-Sim：脉动
    CNN 加速器模拟器。arXiv 预印本 arXiv:1811.02883（2018年）。
- en: '(194) Sayal, A., Fathima, S., Nibhanupudi, S., and Kulkarni, J. COMPAC: Compressed
    Time-Domain, Pooling-Aware Convolution CNN Engine With Reduced Data Movement for
    Energy-Efficient AI Computing. IEEE Journal of Solid-State Circuits PP (12 2020),
    1–1.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (194) Sayal, A., Fathima, S., Nibhanupudi, S., 和 Kulkarni, J. COMPAC：压缩时域、池化感知卷积
    CNN 引擎，减少数据移动以实现节能 AI 计算。IEEE 固态电路汇刊 PP（2020年12月），1–1页。
- en: '(195) Schatz, M. D., van de Geijn, R. A., and Poulson, J. Parallel Matrix Multiplication:
    A Systematic Journey. SIAM J. Sci. Comput. 38, 6 (jan 2016), C748–C781.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (195) Schatz, M. D., van de Geijn, R. A., 和 Poulson, J. 并行矩阵乘法：系统性探索。SIAM J.
    Sci. Comput. 38, 6（2016年1月），C748–C781。
- en: '(196) Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun,
    Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional
    Networks, 2014.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (196) Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., 和 LeCun,
    Y. OverFeat：使用卷积网络的集成识别、定位和检测，2014年。
- en: (197) Sestito C., Perri S., S. R. FPGA Design of Transposed Convolutions for
    Deep Learning Using High-Level Synthesis. Journal of Signal Processing Systems
    (08 2023), 1–19.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (197) Sestito C., Perri S., S. R. FPGA 设计的转置卷积用于深度学习，采用高层次综合。信号处理系统期刊（2023年8月），1–19页。
- en: '(198) Shadmehri, S. H. H., BanaGozar, A., Kamal, M., Stuijk, S., Afzali-Kusha,
    A., Pedram, M., and Corporaal, H. SySCIM: SystemC-AMS Simulation of Memristive
    Computation In-Memory. In 2022 Design, Automation and Test in Europe Conference
    and Exhibition (DATE) (2022), pp. 1467–1472.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (198) Shadmehri, S. H. H., BanaGozar, A., Kamal, M., Stuijk, S., Afzali-Kusha,
    A., Pedram, M., 和 Corporaal, H. SySCIM：基于 SystemC-AMS 的忆阻计算内存仿真。在2022年欧洲设计、自动化与测试会议（DATE）（2022年），pp.
    1467–1472。
- en: '(199) Shao, Y. S., Reagen, B., Wei, G.-Y., and Brooks, D. Aladdin: A pre-RTL,
    power-performance accelerator simulator enabling large design space exploration
    of customized architectures. In 2014 ACM/IEEE 41st International Symposium on
    Computer Architecture (ISCA) (2014), pp. 97–108.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (199) Shao, Y. S., Reagen, B., Wei, G.-Y., 和 Brooks, D. Aladdin：一种预 RTL 的功耗性能加速器模拟器，可实现对定制架构的大规模设计空间探索。在2014年
    ACM/IEEE 第41届国际计算机架构研讨会（ISCA）（2014年），pp. 97–108。
- en: '(200) Shawahna, A., Sait, S. M., and El-Maleh, A. FPGA-Based Accelerators of
    Deep Learning Networks for Learning and Classification: A Review. IEEE Access
    7 (2019), 7823–7859.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (200) Shawahna, A., Sait, S. M., 和 El-Maleh, A. 基于 FPGA 的深度学习网络加速器：学习与分类的综述。《IEEE
    Access》7（2019年），7823–7859。
- en: (201) Shmoys, D. B., and Tardos, É. An Approximation Algorithm for the Generalized
    Assignment Problem. Mathematical Programming 62, 1 (1993), 461–474.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (201) Shmoys, D. B., 和 Tardos, É. 一种通用分配问题的近似算法。《数学编程》62, 1（1993年），461–474。
- en: (202) Siemens. Catapult C++/Systemc Synthesis, 2022.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (202) Siemens. Catapult C++/Systemc 综合，2022年。
- en: (203) Silvano, C., Ielmini, D., Ferrandi, F., Fiorin, L., Curzel, S., Benini,
    L., Conti, F., Garofalo, A., Zambelli, C., Calore, E., Schifano, S. F., Palesi,
    M., Ascia, G., Patti, D., Perri, S., Petra, N., Caro, D. D., Lavagno, L., Urso,
    T., Cardellini, V., Cardarilli, G. C., and Birke, R. A survey on deep learning
    hardware accelerators for heterogeneous hpc platforms, 2023.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (203) Silvano, C., Ielmini, D., Ferrandi, F., Fiorin, L., Curzel, S., Benini,
    L., Conti, F., Garofalo, A., Zambelli, C., Calore, E., Schifano, S. F., Palesi,
    M., Ascia, G., Patti, D., Perri, S., Petra, N., Caro, D. D., Lavagno, L., Urso,
    T., Cardellini, V., Cardarilli, G. C., 和 Birke, R. 关于异构 HPC 平台的深度学习硬件加速器的调查，2023年。
- en: (204) Siracusa, M., Delsozzo, E., Rabozzi, M., Di Tucci, L., Williams, S., Sciuto,
    D., and Santambrogio, M. D. A Comprehensive Methodology to Optimize FPGA Designs
    via the Roofline Model. IEEE Transactions on Computers (2021), 1–1.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (204) Siracusa, M., Delsozzo, E., Rabozzi, M., Di Tucci, L., Williams, S., Sciuto,
    D., 和 Santambrogio, M. D. 一种通过 Roofline 模型优化 FPGA 设计的综合方法。《IEEE 计算机学报》（2021年），1–1。
- en: (205) Siracusa, M., Rabozzi, M., Del Sozzo, E., Di Tucci, L., Williams, S.,
    and Santambrogio, M. D. A CAD-based methodology to optimize HLS code via the Roofline
    model. In 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)
    (2020), pp. 1–9.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (205) Siracusa, M., Rabozzi, M., Del Sozzo, E., Di Tucci, L., Williams, S.,
    和 Santambrogio, M. D. 一种基于 CAD 的方法，通过 Roofline 模型优化 HLS 代码。在2020年 IEEE/ACM 计算机辅助设计国际会议（ICCAD）（2020年），pp.
    1–9。
- en: (206) Skutella, M., and Woeginger, G. J. A PTAS for Minimizing the Weighted
    Sum of Job Completion Times on Parallel Machines. In Proceedings of the 31 Annual
    ACM Symposium on Theory of Computing (New York, NY, USA, 1999), STOC ’99, ACM,
    pp. 400–407.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (206) Skutella, M., 和 Woeginger, G. J. 一种最小化并行机器上作业完成时间加权总和的 PTAS。在第31届年度 ACM
    计算理论研讨会（纽约，NY，美国，1999年）上，STOC ’99，ACM，pp. 400–407。
- en: (207) Song, M.-A., Van, L.-D., and Kuo, S.-Y. Adaptive Low-Error Fixed-Width
    Booth Multipliers. IEICE Transactions on Fundamentals of Electronics, Communications
    and Computer Sciences E90-A (06 2007).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (207) Song, M.-A., Van, L.-D., 和 Kuo, S.-Y. 自适应低误差固定宽度 Booth 乘法器。《IEICE 基础电子、通信与计算机科学论文集》E90-A（06
    2007）。
- en: (208) Sorna, A., Cheng, X., D’Azevedo, E., Won, K., and Tomov, S. Optimizing
    the Fast Fourier Transform Using Mixed Precision on Tensor Core Hardware. In Proc.
    25th Int. Conf. on High Performance Computing Workshops (HiPCW) (2018), pp. 3–7.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (208) Sorna, A., Cheng, X., D’Azevedo, E., Won, K., 和 Tomov, S. 在 Tensor Core
    硬件上使用混合精度优化快速傅里叶变换。在第25届国际高性能计算工作坊（HiPCW）（2018年），pp. 3–7。
- en: (209) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of a
    Low-Power Super-Resolution Architecture for Virtual Reality Wearable Devices.
    IEEE Sensors Journal 23, 8 (2023), 9009–9016.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (209) Spagnolo, F., Corsonello, P., Frustaci, F., 和 Perri, S. 低功耗超分辨率架构设计，用于虚拟现实可穿戴设备。《IEEE
    传感器期刊》23, 8（2023年），9009–9016。
- en: (210) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of Approximate
    Bilateral Filters for Image Denoising on FPGAs. IEEE Access 11 (2023), 1990–2000.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (210) Spagnolo, F., Corsonello, P., Frustaci, F., 和 Perri, S. 针对 FPGA 的图像去噪的近似双边滤波器设计。《IEEE
    Access》11（2023年），1990–2000。
- en: '(211) Spagnolo, F., Perri, S., and Corsonello, P. Aggressive Approximation
    of the SoftMax Function for Power-Efficient Hardware Implementations. IEEE Transactions
    on Circuits and Systems II: Express Briefs 69, 3 (2022), 1652–1656.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(211) Spagnolo, F., Perri, S., 和 Corsonello, P. 为功率效率硬件实现而激进近似SoftMax函数。IEEE电路与系统II期刊:
    快速简报69, 3 (2022), 1652–1656。'
- en: (212) Spagnolo, F., Perri, S., and Corsonello, P. Approximate Down-Sampling
    Strategy for Power-Constrained Intelligent Systems. IEEE Access 10 (2022), 7073–7081.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (212) Spagnolo, F., Perri, S., 和 Corsonello, P. 针对功率受限智能系统的近似下采样策略。IEEE Access
    10 (2022), 7073–7081。
- en: (213) ST Microelectronics. X-CUBE-AI, 2017.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (213) STMicroelectronics. X-CUBE-AI, 2017。
- en: (214) Stok, L. Data path synthesis. Integration 18, 1 (1994), 1–71.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (214) Stok, L. 数据路径综合。Integration 18, 1 (1994), 1–71。
- en: '(215) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., and Meo, G. D.
    Comparison and Extension of Approximate 4-2 Compressors for Low-Power Approximate
    Multipliers. IEEE Transactions on Circuits and Systems I: Regular Papers 67, 9
    (2020), 3021–3034.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(215) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., 和 Meo, G. D. 低功耗近似乘法器的近似4-2压缩器的比较与扩展。IEEE电路与系统I期刊:
    常规论文67, 9 (2020), 3021–3034。'
- en: '(216) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., and
    Di Meo, G. Approximate Multipliers Using Static Segmentation: Error Analysis and
    Improvements. IEEE Transactions on Circuits and Systems I: Regular Papers 69,
    6 (2022), 2449–2462.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(216) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., 和
    Di Meo, G. 使用静态分段的近似乘法器: 错误分析与改进。IEEE电路与系统I期刊: 常规论文69, 6 (2022), 2449–2462。'
- en: (217) Stylianou, C., and Weiland, M. Optimizing Sparse Linear Algebra Through
    Automatic Format Selection and Machine Learning, 2023.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (217) Stylianou, C., 和 Weiland, M. 通过自动格式选择和机器学习优化稀疏线性代数，2023。
- en: '(218) Tang, T., and Xie., Y. MLPAT: A power area timing modeling framework
    for machine learning accelerators. In EEE International Workshop on Domain Specific
    System Architecture (DOSSA) (2018).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(218) Tang, T., 和 Xie., Y. MLPAT: 一种用于机器学习加速器的功率区域时间建模框架。在IEEE国际领域特定系统架构研讨会（DOSSA）（2018）。'
- en: '(219) Tung, F., and Mori, G. CLIP-Q: Deep Network Compression Learning by In-parallel
    Pruning-Quantization. In Proceedings of the 2018 IEEE Conference on Computer Vision
    and Pattern Recognition (2018), IEEE, pp. 7873–7882.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(219) Tung, F., 和 Mori, G. CLIP-Q: 通过并行修剪-量化的深度网络压缩学习。在2018年IEEE计算机视觉与模式识别会议（2018），IEEE，第7873–7882页。'
- en: (220) Ullah, S., Rehman, S., Shafique, M., and Kumar, A. High-Performance Accurate
    and Approximate Multipliers for FPGA-Based Hardware Accelerators. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems 41, 2 (2022), 211–224.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (220) Ullah, S., Rehman, S., Shafique, M., 和 Kumar, A. 面向FPGA硬件加速器的高性能精确与近似乘法器。IEEE计算机辅助设计集成电路与系统期刊41,
    2 (2022), 211–224。
- en: (221) Ullah, S., Schmidl, H., Sahoo, S. S., Rehman, S., and Kumar, A. Area-Optimized
    Accurate and Approximate Softcore Signed Multiplier Architectures. IEEE Transactions
    on Computers 70, 3 (2021), 384–392.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (221) Ullah, S., Schmidl, H., Sahoo, S. S., Rehman, S., 和 Kumar, A. 面积优化的精确和近似软核有符号乘法器架构。IEEE计算机期刊70,
    3 (2021), 384–392。
- en: (222) Urbach, M., and Petersen, M. B. HLS from PyTorch to System Verilog with
    MLIR and CIRCT, 2022. 2nd Workshop on Languages, Tools, and Techniques for Accelerator
    Design (LATTE).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (222) Urbach, M., 和 Petersen, M. B. 从PyTorch到System Verilog的HLS与MLIR和CIRCT，2022。第二届加速器设计语言、工具与技术研讨会（LATTE）。
- en: '(223) Van Delm, J., Vandersteegen, M., Burrello, A., Sarda, G. M., Conti, F.,
    Jahier Pagliari, D., Benini, L., and Verhelst, M. HTVM: Efficient Neural Network
    Deployment On Heterogeneous TinyML Platforms. In Proceedings of the 2023 Conference
    & Exhibition on Design, Automation & Test in Europe (Antwerp, 2023).'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(223) Van Delm, J., Vandersteegen, M., Burrello, A., Sarda, G. M., Conti, F.,
    Jahier Pagliari, D., Benini, L., 和 Verhelst, M. HTVM: 在异构TinyML平台上的高效神经网络部署。在2023年欧洲设计、自动化与测试会议（安特卫普，2023）论文集。'
- en: (224) van Werkhoven, B., Palenstijn, W. J., and Sclocco, A. Lessons Learned
    in a Decade of Research Software Engineering GPU Applications. In Computational
    Science – ICCS 2020 (Cham, 2020), Springer, pp. 399–412.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (224) van Werkhoven, B., Palenstijn, W. J., 和 Sclocco, A. 在十年的研究软件工程GPU应用中的经验教训。在计算科学
    – ICCS 2020（沙姆，2020），Springer，第399–412页。
- en: (225) Vanderbauwhede, W., and Benkrid, K. High-performance computing using FPGAs,
    vol. 3. Springer, 2013.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (225) Vanderbauwhede, W., 和 Benkrid, K. 使用FPGA的高性能计算，第3卷。Springer，2013。
- en: '(226) Vanevenhoven, T. High-Level Implementation of Bit- and Cycle-Accurate
    Floating-Point DSP Algorithms with Xilinx FPGAs. Tech. rep., Xilinx, Oct 2011.
    White Paper: 7 Series FPGAs.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (226) Vanevenhoven, T. 基于Xilinx FPGA的高层次比特级和周期级浮点DSP算法实现。技术报告，Xilinx，2011年10月。白皮书：7系列FPGA。
- en: '(227) Vasilache, N., Johnson, J., Mathieu, M., Chintala, S., Piantino, S.,
    and LeCun, Y. Fast Convolutional Nets With fbfft: A GPU Performance Evaluation,
    2015.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (227) Vasilache, N., Johnson, J., Mathieu, M., Chintala, S., Piantino, S., 和
    LeCun, Y. 使用fbfft的快速卷积网络：GPU性能评估，2015年。
- en: '(228) Vasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., DeVito, Z., Moses,
    W. S., Verdoolaege, S., Adams, A., and Cohen, A. Tensor Comprehensions: Framework-Agnostic
    High-Performance Machine Learning Abstractions. arXiv:1802.04730 [cs] (June 2018).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(228) Vasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., DeVito, Z., Moses,
    W. S., Verdoolaege, S., Adams, A., 和 Cohen, A. Tensor Comprehensions: 框架无关的高性能机器学习抽象。arXiv:1802.04730
    [cs]（2018年6月）。'
- en: (229) Véstias, M., and Neto, H. Trends of CPU, GPU and FPGA for high-performance
    computing. In 2014 24th International Conference on Field Programmable Logic and
    Applications (FPL) (Sep. 2014), pp. 1–6.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (229) Véstias, M., 和 Neto, H. CPU、GPU和FPGA在高性能计算中的趋势。发表于2014年第24届国际现场可编程逻辑与应用会议（FPL）（2014年9月），第1–6页。
- en: '(230) Vitter, J. S. External Memory Algorithms and Data Structures: Dealing
    with Massive Data. ACM Comput. Surv. 33, 2 (jun 2001), 209–271.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (230) Vitter, J. S. 外部存储算法和数据结构：处理海量数据。ACM计算机调查，33卷，第2期（2001年6月），第209–271页。
- en: '(231) Wang, Z., Huang, H., Zhang, J., and Alonso, G. Shuhai: Benchmarking High
    Bandwidth Memory On FPGAs. In 2020 IEEE 28th Annual International Symposium on
    Field-Programmable Custom Computing Machines (FCCM) (2020), pp. 111–119.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(231) 王, Z., 黄, H., 张, J., 和 阿隆索, G. Shuhai: FPGA上高带宽内存的基准测试。发表于2020年IEEE第28届年度国际现场可编程定制计算机会议（FCCM）（2020年），第111–119页。'
- en: '(232) Waris, H., Wang, C., Liu, W., and Lombardi, F. AxBMs: Approximate Radix-8
    Booth Multipliers for High-Performance FPGA-Based Accelerators. IEEE Transactions
    on Circuits and Systems II: Express Briefs 68, 5 (2021), 1566–1570.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(232) Waris, H., 王, C., 刘, W., 和 Lombardi, F. AxBMs: 用于高性能FPGA加速器的近似Radix-8
    Booth乘法器。IEEE电路与系统II：快报，68卷，第5期（2021年），第1566–1570页。'
- en: (233) Winograd, S. Arithmetic Complexity of Computations. Society for Industrial
    and Applied Mathematics, 1980.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (233) Winograd, S. 计算的算术复杂性。工业与应用数学学会，1980年。
- en: (234) Wirthlin, M., Poznanovic, D., Sundararajan, P., Coppola, A., Pellerin,
    D., Najjar, W., Bruce, R., Babst, M., Pritchard, O., Palazzari, P., and Kuzmanov,
    G. OpenFPGA CoreLib core library interoperability effort. Parallel Computing 34
    (2008), 231–244.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (234) Wirthlin, M., Poznanovic, D., Sundararajan, P., Coppola, A., Pellerin,
    D., Najjar, W., Bruce, R., Babst, M., Pritchard, O., Palazzari, P., 和 Kuzmanov,
    G. OpenFPGA CoreLib核心库互操作性努力。并行计算，34卷（2008年），第231–244页。
- en: '(235) Wu, C., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K.,
    Chang, G., Behram, F. A., Huang, J., Bai, C., Gschwind, M., Gupta, A., Ott, M.,
    Melnikov, A., Candido, S., Brooks, D., Chauhan, G., Lee, B., Lee, H. S., Akyildiz,
    B., Balandat, M., Spisak, J., Jain, R., Rabbat, M., and Hazelwood, K. M. Sustainable
    AI: Environmental Implications, Challenges and Opportunities. In Proceedings of
    Machine Learning and Systems 2022, MLSys 2022 (2022).'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(235) 吴, C., 拉哈文德拉, R., 古普塔, U., 阿昆, B., 阿尔达拉尼, N., 孟, K., 常, G., 贝赫拉姆, F.
    A., 黄, J., 白, C., 格施温德, M., 古普塔, A., 奥特, M., 梅尔尼科夫, A., 坎迪多, S., 布鲁克斯, D., 乔汉,
    G., 李, B., 李, H. S., 阿基尔迪兹, B., 巴兰达特, M., 斯皮萨克, J., 贾因, R., 拉巴特, M., 和 海兹尔伍德,
    K. M. 可持续AI: 环境影响、挑战和机遇。发表于2022年机器学习与系统会议论文集，MLSys 2022（2022年）。'
- en: '(236) Wu, Y. N., Emer, J. S., and Sze, V. Accelergy: An Architecture-Level
    Energy Estimation Methodology for Accelerator Designs. In 2019 IEEE/ACM International
    Conference on Computer-Aided Design (ICCAD) (2019), pp. 1–8.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(236) 吴, Y. N., Emer, J. S., 和 Sze, V. Accelergy: 一种用于加速器设计的架构级能量估计方法。发表于2019年IEEE/ACM国际计算机辅助设计会议（ICCAD）（2019年），第1–8页。'
- en: '(237) Xia, L., Li, B., Tang, T., Gu, P., Yin, X., Huangfu, W., Chen, P.-Y.,
    Yu, S., Cao, Y., Wang, Y., Xie, Y., and Yang, H. MNSIM: Simulation platform for
    memristor-based neuromorphic computing system. In 2016 Design, Automation and
    Test in Europe Conference and Exhibition (DATE) (2016), pp. 469–474.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(237) 夏, L., 李, B., 唐, T., 顾, P., 尹, X., 黄富, W., 陈, P.-Y., 于, S., 曹, Y., 王,
    Y., 谢, Y., 和 杨, H. MNSIM: 基于忆阻器的神经形态计算系统的模拟平台。发表于2016年设计、自动化与欧洲测试会议及展览（DATE）（2016年），第469–474页。'
- en: (238) Xilinx Inc. Vitis High-Level Synthesis User Guide, 2022.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (238) Xilinx Inc. Vitis高级合成用户指南，2022年。
- en: '(239) Xilinx Inc. Vivado Design Suite User Guide: Designing IP Subsystems Using
    IP Integrator. UG994 (v2022.2), 2022.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (239) Xilinx Inc. Vivado Design Suite 用户指南：使用IP Integrator设计IP子系统。UG994（v2022.2），2022年。
- en: '(240) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., et al. Interstellar: Using Halide’s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the Twenty-Fifth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (2020), pp. 369–383.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(240) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., 等人。Interstellar: 使用 Halide 的调度语言分析 DNN 加速器。发表于第二十五届国际编程语言与操作系统体系结构支持会议论文集（2020），第
    369–383 页。'
- en: '(241) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., Kozyrakis, C., and Horowitz, M. Interstellar: Using
    Halide’s Scheduling Language to Analyze DNN Accelerators. In Proceedings of the
    Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems (New York, NY, USA, 2020), ASPLOS ’20, Association
    for Computing Machinery, pp. 369–383.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(241) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., Kozyrakis, C., 和 Horowitz, M. Interstellar: 使用 Halide
    的调度语言分析 DNN 加速器。发表于第二十五届国际编程语言与操作系统体系结构支持会议论文集（纽约，NY，美国，2020），ASPLoS ’20，由计算机协会主办，第
    369–383 页。'
- en: (242) Yasudo, R., Coutinho, J., Varbanescu, A., Luk, W., Amano, H., and Becker,
    T. Performance Estimation for Exascale Reconfigurable Dataflow Platforms. In 2018
    International Conference on Field-Programmable Technology (FPT) (2018), pp. 314–317.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (242) Yasudo, R., Coutinho, J., Varbanescu, A., Luk, W., Amano, H., 和 Becker,
    T. 对 Exascale 可重配置数据流平台的性能估计。发表于 2018 年国际现场可编程技术大会（FPT）（2018），第 314–317 页。
- en: '(243) Ye, H., Hao, C., Cheng, J., Jeong, H., Huang, J., Neuendorffer, S., and
    Chen, D. ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level
    Intermediate Representation. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA) (2022), IEEE, pp. 741–755.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(243) Ye, H., Hao, C., Cheng, J., Jeong, H., Huang, J., Neuendorffer, S., 和
    Chen, D. ScaleHLS: 一种新的可扩展高层次综合框架，基于多层次中间表示。发表于 2022 年 IEEE 高性能计算架构国际研讨会（HPCA）（2022），IEEE，第
    741–755 页。'
- en: '(244) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., and Chen, D. ScaleHLS:
    A Scalable High-Level Synthesis Framework with Multi-Level Transformations and
    Optimizations. In Proceedings of the 59th ACM/IEEE Design Automation Conference
    (DAC) (2022), pp. 1355–1358.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(244) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., 和 Chen, D. ScaleHLS: 一种具有多层次变换和优化的可扩展高层次综合框架。发表于第
    59 届 ACM/IEEE 设计自动化会议（DAC）（2022），第 1355–1358 页。'
- en: '(245) Zendegani, R., Kamal, M., Fayyazi, A., Afzali-Kusha, A., Safari, S.,
    and Pedram, M. SEERAD: A high speed yet energy-efficient rounding-based approximate
    divider. In 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)
    (2016), pp. 1481–1484.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(245) Zendegani, R., Kamal, M., Fayyazi, A., Afzali-Kusha, A., Safari, S.,
    和 Pedram, M. SEERAD: 一种高速而又节能的基于舍入的近似除法器。发表于 2016 年欧洲设计、自动化与测试会议（DATE）（2016），第
    1481–1484 页。'
- en: '(246) Zeni, A., O’Brien, K., Blott, M., and Santambrogio, M. D. Optimized Implementation
    of the HPCG Benchmark on Reconfigurable Hardware. In Euro-Par 2021: Parallel Processing
    (2021), L. Sousa, N. Roma, and P. Tomás, Eds., pp. 616–630.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (246) Zeni, A., O’Brien, K., Blott, M., 和 Santambrogio, M. D. 在可重配置硬件上优化 HPCG
    基准测试的实现。发表于 2021 年欧洲并行处理大会（Euro-Par 2021），由 L. Sousa, N. Roma, 和 P. Tomás 编辑，第
    616–630 页。
- en: (247) Zhang, J., Franchetti, F., and Low, T. M. High Performance Zero-Memory
    Overhead Direct Convolutions. CoRR abs/1809.10170 (2018).
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (247) Zhang, J., Franchetti, F., 和 Low, T. M. 高性能零内存开销直接卷积。CoRR abs/1809.10170（2018）。
- en: '(248) Zhang, X., Ye, H., and Chen, D. Being-ahead: Benchmarking and Exploring
    Accelerators for Hardware-Efficient AI Deployment, 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(248) Zhang, X., Ye, H., 和 Chen, D. Being-ahead: 针对硬件高效 AI 部署的加速器基准测试与探索，2021
    年。'
- en: '(249) Zhao, Y., Li, C., Wang, Y., Xu, P., Zhang, Y., and Lin, Y. DNN-Chip Predictor:
    An Analytical Performance Predictor for DNN Accelerators with Various Dataflows
    and Hardware Architectures, 2021.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(249) Zhao, Y., Li, C., Wang, Y., Xu, P., Zhang, Y., 和 Lin, Y. DNN-Chip Predictor:
    一种针对具有多种数据流和硬件架构的 DNN 加速器的分析性能预测器，2021 年。'
- en: '(250) Zhou, Y., Roy, S., Abdolrashidi, A., Wong, D. L., Ma, P. C., Xu, Q.,
    Zhong, M., Liu, H., Goldie, A., Mirhoseini, A., and Laudon, J. GDP: Generalized
    Device Placement for Dataflow Graphs. CoRR abs/1910.01578 (2019).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(250) Zhou, Y., Roy, S., Abdolrashidi, A., Wong, D. L., Ma, P. C., Xu, Q.,
    Zhong, M., Liu, H., Goldie, A., Mirhoseini, A., 和 Laudon, J. GDP: 数据流图的通用设备布局。CoRR
    abs/1910.01578（2019）。'
- en: '(251) Zhu, D., Lu, S., Wang, M., Lin, J., and Wang, Z. Efficient Precision-Adjustable
    Architecture for Softmax Function in Deep Learning. IEEE Transactions on Circuits
    and Systems II: Express Briefs 67 (2020), 3382–3386.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (251) Zhu, D., Lu, S., Wang, M., Lin, J., 和 Wang, Z. 用于深度学习中 Softmax 函数的高效精度可调架构。IEEE
    电路与系统 II：快速简报 67（2020），第 3382–3386 页。
- en: '(252) Zohouri, H. R., Maruyama, N., Smith, A., Matsuda, M., and Matsuoka, S.
    Evaluating and Optimizing OpenCL Kernels for High Performance Computing with FPGAs.
    In SC ’16: Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis (Nov 2016), pp. 409–420.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (252) Zohouri, H. R., Maruyama, N., Smith, A., Matsuda, M., 和 Matsuoka, S. 评估和优化用于高性能计算的
    OpenCL 内核与 FPGA。发表于SC ’16：国际高性能计算、网络、存储与分析会议论文集（2016年11月），第409–420页。
- en: '(253) Zohouri, H. R., and Matsuoka, S. The memory controller wall: Benchmarking
    the intel FPGA SDK for OpenCL memory interface. In 2019 IEEE/ACM International
    Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC) (2019),
    IEEE, pp. 11–18.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (253) Zohouri, H. R. 和 Matsuoka, S. 内存控制器壁垒：基准测试英特尔 FPGA SDK 的 OpenCL 内存接口。发表于2019年IEEE/ACM国际异构高性能可重构计算研讨会（H2RC）（2019年），IEEE，第11–18页。
- en: (254) Zois, V., Gupta, D., Tsotras, V. J., Najjar, W. A., and Roy, J.-F. Massively
    Parallel Skyline Computation for Processing-in-Memory Architectures. In Proceedings
    of the 27th International Conference on Parallel Architectures and Compilation
    Techniques (New York, NY, USA, 2018), PACT ’18, ACM.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (254) Zois, V., Gupta, D., Tsotras, V. J., Najjar, W. A., 和 Roy, J.-F. 用于处理内存架构的大规模并行天际线计算。发表于第27届国际并行架构与编译技术会议论文集（纽约，NY，美国，2018年），PACT
    ’18，ACM。
