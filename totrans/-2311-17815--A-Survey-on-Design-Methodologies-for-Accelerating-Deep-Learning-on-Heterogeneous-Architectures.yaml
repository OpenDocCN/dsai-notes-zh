- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:35:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2311.17815] A Survey on Design Methodologies for Accelerating Deep Learning
    on Heterogeneous Architectures'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.17815](https://ar5iv.labs.arxiv.org/html/2311.17815)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \SetWatermarkText
  prefs: []
  type: TYPE_NORMAL
- en: Preprint \SetWatermarkScale1 \forestsetqtree/.style=for tree=parent anchor=south,
    child anchor=north,align=center,inner sep=0pt
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)
    ,  Serena Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it) , 
    Leandro Fiorin [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it) , 
    Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)
    ,  Cristina Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it)
    Politecnico di MilanoItaly ,  Francesco Conti [f.conti@unibo.it](mailto:f.conti@unibo.it)
    ,  Alessio Burrello [alessio.burrello@unibo.it](mailto:alessio.burrello@unibo.it)
    ,  Francesco Barchi [francesco.barchi@unibo.it](mailto:francesco.barchi@unibo.it)
    ,  Luca Benini [luca.benini@unibo.it](mailto:luca.benini@unibo.it) Università
    di BolognaViale Carlo Pepoli, 3/240123BolognaItaly ,  Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)
    ,  Teodoro Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) Politecnico
    di TorinoItaly ,  Enrico Calore [enrico.calore@infn.fe.it](mailto:enrico.calore@infn.fe.it)
    ,  Sebastiano Fabio Schifano [schsst@unife.it](mailto:schsst@unife.it) ,  Cristian
    Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it) Università
    degli Studi di FerraraVia Giuseppe Saragat, 144122FerraraItaly ,  Maurizio Palesi
    [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it) ,  Giuseppe Ascia
    [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it) ,  Enrico Russo [enrico.russo@phd.unict.it](mailto:enrico.russo@phd.unict.it)
    Università degli Studi di CataniaItaly ,  Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)
    ,  Davide De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it) ,  Gennaro Di
    Meo [gennaro.dimeo@unina.it](mailto:gennaro.dimeo@unina.it%20) Università degli
    Studi di Napoli Federico IIItaly ,  Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)
    ,  Salvatore Filippone [salvatore.filippone@uniroma2.it](mailto:salvatore.filippone@uniroma2.it)
    ,  Francesco Lo Presti [lopresti@info.uniroma2.it](mailto:lopresti@info.uniroma2.it)
    Università degli Studi di Roma “Tor Vergata”Italy ,  Francesco Silvestri [francesco.silvestri@unipd.it](mailto:francesco.silvestri@unipd.it)
    Università degli Studi di PadovaItaly ,  Paolo Palazzari [paolo.palazzari@enea.it](mailto:paolo.palazzari@enea.it)
    ENEAItaly  and  Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it) Università
    degli Studi della CalabriaItaly(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, the field of Deep Learning has seen many disruptive and impactful
    advancements. Given the increasing complexity of deep neural networks, the need
    for efficient hardware accelerators has become more and more pressing to design
    heterogeneous HPC platforms. The design of Deep Learning accelerators requires
    a multidisciplinary approach, combining expertise from several areas, spanning
    from computer architecture to approximate computing, computational models, and
    machine learning algorithms. Several methodologies and tools have been proposed
    to design accelerators for Deep Learning, including hardware-software co-design
    approaches, high-level synthesis methods, specific customized compilers, and methodologies
    for design space exploration, modeling, and simulation. These methodologies aim
    to maximize the exploitable parallelism and minimize data movement to achieve
    high performance and energy efficiency. This survey provides a holistic review
    of the most influential design methodologies and EDA tools proposed in recent
    years to implement Deep Learning accelerators, offering the reader a wide perspective
    in this rapidly evolving field. In particular, this work complements the previous
    survey proposed by the same authors in ([silvano2023survey,](#bib.bib203) ), which
    focuses on Deep Learning hardware accelerators for heterogeneous HPC platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†journal: CSUR^†^†copyright:
    none^†^†ccs: Hardware Methodologies for EDA^†^†ccs: Hardware High-level and register-transfer
    level synthesis^†^†ccs: Computer systems organization Architectures^†^†ccs: Hardware Very
    large scale integration design^†^†ccs: Hardware Reconfigurable logic and FPGAs^†^†ccs:
    Computing methodologies Artificial intelligence^†^†ccs: Computing methodologies Machine
    learning'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the past few years, Deep Learning (DL) has made remarkable progress thanks
    to significant research advancements exploiting DL techniques in several applications
    such as computer vision, natural language processing, speech and audio processing,
    recommendation systems, autonomous systems, environmental sciences, and many more.
    DL is a subset of machine learning that uses artificial Deep Neural Networks (DNNs)
    with multiple layers of artificial neurons to mimic human brain behavior by learning
    from large amounts of data. This technology has the potential to revolutionize
    the way we approach complex problems and make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: To implement a DL application, the developer must have knowledge not only of
    abstract algorithms and training methods but also the ability to map the DNN model
    into an efficient hardware architecture. The task of designing efficient hardware
    accelerators for DL requires expertise from various fields, such as computer architecture,
    approximate computing, computational models, and DL algorithms. Moreover, current
    architectures are evolving to handle multiple design objectives that cater to
    a wide range of systems, from ultra-low power edge devices to high-performance
    architectures. Due to the broad range of architectural alternatives, the EDA community
    has developed various tools and methodologies to support designers in their development
    efforts. These tools and technologies include high-level synthesis approaches,
    specific customized compilers, and tools to support the design space exploration
    as well as the modeling, partitioning, and mapping. Achieving high performance
    and low power consumption with resource constraints in heterogeneous architectures
    requires a comprehensive understanding of existing EDA frameworks and tools. This
    understanding enables the adoption of appropriate methods to exploit parallelism
    and minimize data movement, thereby fostering innovation in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Scope of the survey. This work is an attempt to provide a holistic overview
    of the most influential frameworks, design methodologies and tools to implement
    DL for high-performance applications. The survey highlights various approaches
    that support the optimization and mapping of a DL model to different hardware
    accelerators, including GPU-based accelerators, Tensor Processor Units, FPGA-based
    accelerators, and ASIC-based accelerators, including Neural Processing Units and
    co-processors on the open-hardware RISC-V architecture. The survey also describes
    methodologies supporting the implementation of accelerators based on emerging
    technologies and computing paradigms, such as 3D-stacked Processor-In-Memory architectures.
    As summarized in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on
    Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    several design methodologies and EDA tools used to design DL hardware accelerators
    are reviewed in this survey. Although we do not claim to provide a comprehensive
    survey, with approximately 250 works referenced from the past two decades on design
    methodologies for DL acceleration, we aim to present a holistic perspective on
    the field of EDA tools for DL accelerators. Moreover, this survey refers to previous
    surveys on the AI and DL algorithms on how a DL model can solve many existing
    problems, such as  ([CSR19,](#bib.bib177) ; [goodfellow2016deep,](#bib.bib95)
    ), and previous surveys about hardware architecture implementing DL networks,
    such as  ([Akkad23,](#bib.bib13) ; [chen2020engineering,](#bib.bib48) ; [Hassanpour2022,](#bib.bib102)
    ; [gao2023acm,](#bib.bib89) ; [rathi2023acm,](#bib.bib183) ; [silvano2023survey,](#bib.bib203)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Organization of the survey. The survey covers methodologies that support the
    generation of new DL operators by leveraging approximate computing and automatic
    translation of high-level operators into hardware architectures, as well as compilers
    and modeling tools that aid in the design space exploration to appropriately map
    heterogeneous DL applications on existing computing nodes, regardless of the non-functional
    requirements such as resource usage or performance that the application considers.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    the proposed classification of tools and methodologies is based on the main design
    flows and frameworks that have been developed in recent years. Moreover, Section [2](#S2
    "2\. An overview of computational models for DL on HPC systems ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    summarizes the most widely models used in DL application workloads. Section [3](#S3
    "3\. Hardware/Software Codesign Methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") introduces approaches
    and tools appropriate to distribute, partition, and map DL models on heterogeneous
    processing systems. Section [4](#S4 "4\. Approximate Computing Methodologies ‣
    A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") describes representative approaches based on the approximate computing
    paradigm. Section [5](#S5 "5\. HLS-based design methodologies ‣ A Survey on Design
    Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    presents the design methodologies suitable for the high-level synthesis of hardware
    accelerators, whereas Section [6](#S6 "6\. Deep Learning Compilers ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures")
    discusses automated compilation and deployment technologies for DL applications.
    Then, Section [7](#S7 "7\. Modeling, Simulation, Profiling and Exploration ‣ A
    Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") describes the modeling, simulation, profiling, and design exploration
    frameworks currently adopted for DL applications.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, we hope this survey could be useful for a wide range of readers,
    including computer architects, hardware and software developers, tool developers,
    HPC engineers, researchers, and technical professionals. A major effort was spent
    using a clear and concise technical writing style to be useful, particularly,
    to the young generations of master and Ph.D. students. To facilitate the reading,
    a list of acronyms is reported in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures").
  prefs: []
  type: TYPE_NORMAL
- en: \forestset
  prefs: []
  type: TYPE_NORMAL
- en: dir tree/.style= for tree= parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: dir tree [Survey Organization [§ [2](#S2 "2\. An overview of computational models
    for DL on HPC systems ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") An overview of computational models
  prefs: []
  type: TYPE_NORMAL
- en: 'for DL on HPC systems ] [§ [3](#S3 "3\. Hardware/Software Codesign Methodologies
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") Hardware/Software Codesign Methodologies][§ [4](#S4 "4\. Approximate
    Computing Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") Approximate Computing Methodologies]
    [§ [5](#S5 "5\. HLS-based design methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") HLS Design-based
    methodologies [Vitis High-Level Synthesis ] [Bambu: open-source High-level Synthesis]
    [Other HLS tools] [HLS-based design flows for deep learning] [IP block integration]
    ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: dir tree [ [§ [6](#S6 "6\. Deep Learning Compilers ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures") Deep Learning
    Compilers [Memory hierarchy management in DNN Accelerators] [Deep Learning Compilers
    for MCUs] [Deep Learning Compilers for High-Performance] ] [§ [7](#S7 "7\. Modeling,
    Simulation, Profiling and Exploration ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") Modeling  Simulation  Profiling
    and Exploration [Modeling  Simulation  and Exploration Frameworks] [Simulation
    tools for emerging memories-based
  prefs: []
  type: TYPE_NORMAL
- en: DNN accelerator] [Cycle-Accurate Simulators] [Modeling and Profiling FPGAs for
    custom accelerators] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Organization of the survey
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. List of acronyms
  prefs: []
  type: TYPE_NORMAL
- en: '| Acronym | Acronym | Acronym |'
  prefs: []
  type: TYPE_TB
- en: '| AI: Artificial Intelligence | ASIC: Application Specific Integrated Circuit
    | AXI: Advanced eXtensible Interface |'
  prefs: []
  type: TYPE_TB
- en: '| CNN: Convolutional Neural Network | CPU: Central Processing Unit | DL: Deep
    Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DMA: Direct Memory Access | DNN: Deep Neural Network | DP: Double Precision
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRAM: Dynamic Random Access Memory | DSP: Digital Signal Processing | DSE:
    Design Space Exploration |'
  prefs: []
  type: TYPE_TB
- en: '| FFT: Fast Fourier Transform | FP: Floating Point | FPGA: Field-Programmable
    Gate Array |'
  prefs: []
  type: TYPE_TB
- en: '| GEMM: General Matrix Multiply | GPU: Graphics Processing Unit | HBM: High
    Bandwidth Memory |'
  prefs: []
  type: TYPE_TB
- en: '| HLS: High Level Synthesis | HPC: High-Performance Computing | HPZMO: High-Performance
    Zero-Memory Overhead |'
  prefs: []
  type: TYPE_TB
- en: '| IP: Intellectual Property | ISA: Instruction Set Architecture | LUT: Lookup
    Table |'
  prefs: []
  type: TYPE_TB
- en: '| MCU: Microcontroller Unit | MEC: Memory-Efficient Convolution | ML: Machine
    Learning |'
  prefs: []
  type: TYPE_TB
- en: '| MLIR: Multi-Level Intermediate Representation | NVDLA: NVIDIA Deep Learning
    Accelerator | PE: Processing Element |'
  prefs: []
  type: TYPE_TB
- en: '| PIM: Processing In-Memory | PRAM: Parallel Random Access Machine | RAM: Random
    Access Machine |'
  prefs: []
  type: TYPE_TB
- en: '| RISC: Reduced Instruction Set Computer | RTL: Register Transfer Level | SIMD:
    Single Instruction Multiple Data |'
  prefs: []
  type: TYPE_TB
- en: '| SoC: System on Chip | SPP: Structured Parallel Programming | SRAM: Static
    Random Access Memory |'
  prefs: []
  type: TYPE_TB
- en: 2\. An overview of computational models for DL on HPC systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design of hardware accelerators for next-generation HPC systems requires
    proper tools and design methodologies, as well as appropriate models of computations.
    Computational models provide the designer with an adequate level of abstraction
    from the low-level characteristics and allow estimating detailed insights of features
    and capabilities to improve the behavior of the accelerators under design in terms
    of power consumption, performance, and resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Among the computational models suitable to minimize the number of CPU operations
    and investigate the computational limits of the designed algorithm, the Random
    Access Machine (RAM) model is one of the most representative and it is certainly
    a cornerstone of the history of computing. However, with the advent of modern
    hardware technologies, more advanced computational models have become necessary
    to capture different data access policies and new data flows. On the one hand,
    algorithms and data structures that fully exploit the memory hierarchy can be
    designed and analyzed through the *External Memory model* ([Vitter01,](#bib.bib230)
    ). On the other hand, parallel architectures, such as BSP, PRAM, LogP, and MapReduce
    can be profiled as *Parallel Computational* models ([Bilardi2011,](#bib.bib24)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Stimulated by the benefits offered by tensor cores ([Dakkak19,](#bib.bib62)
    ; [Sorna18,](#bib.bib208) ; [LuChen21,](#bib.bib144) ) and processing-in-memory
    (PIM) architectures ([Zois18,](#bib.bib254) ; [Choe19,](#bib.bib54) ), several
    computational models have been defined specifically for these technologies. As
    an example, the computational model introduced in ([ChowdhurySV21,](#bib.bib56)
    ) captures the ability of a tensor core to efficiently perform dense matrix multiplication
    of fixed size $\sqrt{m}\times\sqrt{m}$ in time $O(m+\ell)$, where $\ell$ is a
    latency cost. This model can analyze the performances of algorithms for linear
    algebra, graph, stencil, sparse matrix multiplication ([labini22,](#bib.bib134)
    ), and similarity search ([AhleS20,](#bib.bib9) ).
  prefs: []
  type: TYPE_NORMAL
- en: The efficient performance model provided in ([Liu17,](#bib.bib143) ) combines
    a CPU, consisting of a certain number of parallel cores enabled to fast access
    a small shared memory of $M$ data words, and a PIM accelerator consisting of $P$
    elements, each provided with a local memory of $\Theta(n/P)$ data words, with
    $n$ denoting the input size of the problem. This model has been successfully adopted
    for the theoretical analysis of a skip-list ([Kang21,](#bib.bib126) ) and an index
    for a skewed data ([kang2022pim,](#bib.bib127) ) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Given the high level of parallelism required by DL workloads, Structured Parallel
    Programming (SPP) ([Skeletons,](#bib.bib58) ; [MacroDF,](#bib.bib67) ) and Parallel
    Design Patterns (PDP) ([PPatterns,](#bib.bib64) ) are widely recognized as efficient
    methodologies to build powerful and reliable computational models for mapping
    data parallel computations onto both multi-cores and GPU-based platforms. Over
    the years, a broad space of programming tools and frameworks adopting SPP and
    PDP methodologies have been provided ([Skeletons-Survey,](#bib.bib94) ). Among
    them, the prototype framework FastFlow ([FastFlow,](#bib.bib14) ) is worth special
    attention for bringing the SPP approach to widespread application domains, such
    as DL and HPC. FastFlow is an environment based on a layered structure that incorporates
    algorithmic skeletons, mechanisms, and concepts useful to prototype different
    kinds of parallel patterns. The first layer collects a set of high-level skeletons
    modeling built-in parallel patterns. The intermediate layer makes available the
    structures suitable to instantiate parallel patterns that are not directly modeled
    by built-in skeletons, like parallel blocks, sequential blocks, and pipeline blocks,
    that allow modeling of both temporal and spatial parallelism, and the all-to-all
    block that implements the complete connection between blocks. The last layer of
    the FastFlow hierarchy is composed of a set of low-level mechanisms that depend
    on the target hardware platform and are invisible to both domain experts, who
    use high-level parallel skeletons, and to programmers, who use the intermediate
    building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Algebra for Deep Learning. Several typical workloads of DL applications
    are dominated by linear algebra kernels. Therefore, a plethora of computational
    models have been proposed in the literature to make the use of linear algebra
    approachable.
  prefs: []
  type: TYPE_NORMAL
- en: The so-called task-based runtime environments ([Agullo2017,](#bib.bib7) ; [Agullo2023,](#bib.bib8)
    ; [Carratala2020,](#bib.bib40) ; [Herault2022,](#bib.bib103) ; [Pei2022,](#bib.bib173)
    ) allow complex algorithms to be encoded by specifying a set of dependencies among
    various building blocks, thus increasing the programmability of various kinds
    of complex linear algebra algorithms ([Schatz2016,](#bib.bib195) ; [Quintana2009,](#bib.bib181)
    ; [Gates2022,](#bib.bib90) ). It is worth noting that, in such environments, developing
    libraries to collect application-specific building blocks is crucial to provide
    an optimal mapping from problem to function calls  ([Psarras2022,](#bib.bib179)
    ). An essential ingredient to develop building blocks and libraries for DL applications
    is heterogeneity ([Cardellini2014,](#bib.bib39) ; [Filippone2017,](#bib.bib82)
    ; [Carter2014,](#bib.bib42) ). Moreover, the reduction of communication costs,
    including both latency and bandwidth, is a fundamental requirement to achieve
    efficient hardware accelerator architectures ([6496136,](#bib.bib69) ; [6514719,](#bib.bib148)
    ; [doi:10.1137/120881191,](#bib.bib41) ). Finally, the possibility of exploiting
    mixed-precision operation modes ([doi:10.1177/10943420211003313,](#bib.bib1) ;
    [10.1145/3441850,](#bib.bib84) ; [higham_mary_2022,](#bib.bib105) ) and randomization
    ([murray2023randomized,](#bib.bib157) ) is a very attractive feature.
  prefs: []
  type: TYPE_NORMAL
- en: LAPACK, ScaLAPACK ([doi:10.1137/1037042,](#bib.bib72) ), Trilinos ([10.1145/1089014.1089021,](#bib.bib104)
    ) and PETSc ([petsc-web-page,](#bib.bib22) ) are important high-performance libraries
    that collect efficient software routines for linear algebra techniques. Recently,
    libraries for Sparse linear algebra algorithms have been introduced ([doi:10.1137/20M134914X,](#bib.bib63)
    ; [DAMBRA2023100463,](#bib.bib74) ), with the main objective of supporting machine
    learning ([10.14778/2994509.2994515,](#bib.bib75) ; [9355309,](#bib.bib88) ; [7324551,](#bib.bib166)
    ; [stylianou2023optimizing,](#bib.bib217) ) and DL models, with special attention
    to DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers in DNNs can be computed in many different ways. Typically,
    the convolution is implemented using a traditional sliding window approach across
    the activation data matrix, together with the application of a kernel function ([Sermanet-2014,](#bib.bib196)
    ). However, this type of computation is not efficient due to the irregularity
    of the data access pattern.
  prefs: []
  type: TYPE_NORMAL
- en: In order to reduce the number of floating point operations, Fast Fourier Transform
    (FFT)-based implementations  ([Vasilache-2015,](#bib.bib227) ; [Mathieu-2014,](#bib.bib145)
    ) can be exploited to compute convolution in the frequency domain as a Hadamard
    product (element-wise matrix multiplication), after Fourier transforming the activation
    data. Obtained results are then transformed back into the frequency domain through
    the inverse FFT. Even though FFT provides an asymptotically superior approach,
    it is often very inefficient and incurs a significant memory overhead, especially
    when the convolutional kernels are small ([Zhang-2018,](#bib.bib247) ).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of small kernels (size $\leq 3$) and unitary stride, the Winograd
    minimal filter algorithm ([Winograd-1980,](#bib.bib233) ; [Abdelouahab-2018,](#bib.bib3)
    ) is particularly efficient. The Winograd convolution algorithm divides the output
    activation matrix into tiles and computes each tile as $A^{T}[(Gg)\odot(B^{T}d)]$,
    where $\odot$ denotes the Hadamard product, $g$ is the convolution kernel and
    $d$ is the input activation matrix. $A$, $B$, and $G$ are transformation matrices,
    which are constants for given tile and convolution kernel sizes. The Winograd
    convolution reduces the number of multiplications and, as the matrix multiplication
    of smaller transformed matrices has more independent workloads, increases the
    thread-level parallelism. However, this comes at the cost of extra floating-point
    additions and the extra global memory accesses that are needed to implement the
    matrices transformations. This process, for large convolution filters, may overwhelm
    the benefits of multiplication reduction ([Ji-2019,](#bib.bib116) ).
  prefs: []
  type: TYPE_NORMAL
- en: Another common approach is to reshape and selectively duplicate parts of the
    original input activation data to create a lowered matrix ([Chellapilla-2006,](#bib.bib44)
    ; [Cho-2017,](#bib.bib53) ; [Anderson-2020,](#bib.bib19) ) that is then multiplied
    by a properly arranged matrix of kernel weights. This allows leveraging the highly
    optimized high-performance matrix-matrix multiplication routines available in
    Level 3 Basic Linear Algebra Subprogram (BLAS) libraries ([Dongarra-1990,](#bib.bib71)
    ). The image-to-column (im2col) algorithm ([Chellapilla-2006,](#bib.bib44) ) transforms
    the input activation matrix into a Toeplitz matrix by unrolling overlapping patches
    into columns, as schematized in Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2\. An
    overview of computational models for DL on HPC systems ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures"). In the dual approach,
    image-to-row (im2row), the lowered matrix is created by unrolling the patches
    into rows ([Anderson-2020,](#bib.bib19) ). Both methods require an additional
    memory space of size $(K\times K\times C_{I})\times(H_{O}\times W_{O})$ for storing
    the lowered input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ccdeab89fc051682409f3ed03302b23.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6945bd63fa9b35714cd3c4cda0aa1f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2. Activation data packing: (a) im2col; (b) kn2row.'
  prefs: []
  type: TYPE_NORMAL
- en: In the Memory-efficient Convolution (MEC) algorithm ([Cho-2017,](#bib.bib53)
    ), $[H_{I}]\times[K]\times[C_{I}]$ submatrices are transformed into rows, thus
    leading to a lowered matrix of size $W_{O}\times H_{I}\times K\times C_{I}$, which
    is $K$ times smaller than that generated by the im2row algorithm. To compute the
    convolution, the weights matrix is multiplied by $H_{O}$ submatrices of size $[W_{O}]\times[K\times
    K\times C_{I}]$, obtained by shifting, over the lowered matrix, the submatrix
    to the right by $s\times K$. MEC intuitively eliminates the vertical redundancy
    of the im2row approach, while recovering the information by shifting the submatrix
    by a constant interval ([Cho-2017,](#bib.bib53) ).
  prefs: []
  type: TYPE_NORMAL
- en: While enjoying the speed-up given in the execution by the use of architecture-optimized
    routines, these approaches suffer from the time penalty of implementing the bandwidth-bounded
    packing of the input matrix, as well as from the mismatch between the sizes of
    the matrices used in the calculation of the convolution and those for which traditional
    high-performance systems are optimized. Moreover, additional memory space is needed
    for storing the lowered matrices. In contrast to the previous approaches, Direct
    methods do not pack the matrices before the computation. Rather, the kernel-to-row
    (kn2row) algorithm  ([Anderson-2020,](#bib.bib19) ) avoids data replication in
    the input and, as shown in Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2\. An overview
    of computational models for DL on HPC systems ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures"), the convolution
    is computed as the sum of $K\times K$ separate $1\times 1$ convolutions, thus
    increasing the size of the output. Indeed, each $1\times 1$ convolution is calculated
    by considering only one of the $K\times K$ kernel components at the time and multiplying
    it by the input activation matrix, therefore by performing a matrix-matrix multiplication
    between the corresponding $[C_{O}]\times[C_{I}]$ weight matrix and the $[C_{I}]\times[H_{I}\times
    W_{I}]$ activation matrix. All the $K\times K$ separate $1\times 1$ convolutions
    can be computed using a single matrix multiplication by reordering the filter
    matrix, i.e. by laying out contiguously the $C_{O}$ channel data. The consequent
    multiplication between a $[K\times K\times C_{O}]\times[C_{I}]$ weight matrix
    and a $[C_{I}]\times[H_{I}\times W_{I}]$ activation matrix furnishes the resulting
    matrix of size $[K\times K\times C_{O}]\times[H_{I}\times W_{I}]$, which is stored
    in memory at the end of the multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain the desired $[C_{O}]\times[H_{O}\times W_{O}]$ output activation
    matrix, the results of the $K\times K$ separate $1\times 1$ convolutions are added
    together by appropriately shifting the data vertically and/or horizontally, depending
    on the position of the relative weight with respect to the central kernel weight
    (i.e., top, bottom, left, right, and diagonal positions). Some of the results
    of the intermediate $1\times 1$ convolutions are outside the boundaries of the
    final result matrix and they are discarded during the final sum. Filter weights
    are arranged in the desired position ahead of time.
  prefs: []
  type: TYPE_NORMAL
- en: In ([Anderson-2020,](#bib.bib19) ), the kn2row approach is modified by performing
    the shift-add operation at the end of the calculation of each separate $1\times
    1$ convolution. The needed temporary storage is reduced to $[2\delta+C_{O}]\times[H_{I}\times
    W_{I}]$, where $\delta$ is the number of extra rows in the result matrix needed
    to support the shifting of the result data. By swapping the dimensions of the
    filter and of the input activation matrices to make $C_{I}$ the innermost dimension,
    it is possible to obtain the dual methods (kernel-to-col).
  prefs: []
  type: TYPE_NORMAL
- en: The High Performance Zero-Memory Overhead (HPZMO) direct convolutions approach ([Zhang-2018,](#bib.bib247)
    ) rearranges and optimizes the naive convolutional algorithm to enable its execution
    over multi-threaded Single-Instruction Multiple-Data (SIMD) architectures and
    an output-tiled approach, in which the partial results of the convolution of $W_{o,b}$
    elements are accumulated into the register file. As shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 2\. An overview of computational models for DL on HPC systems ‣
    A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures"), the HPZMO approach relies on the vector units of the computing
    architecture. Indeed, it reads the activation data and the filter weights directly
    from memory, without requiring additional input/output memory space, and parallelizes
    the output channel ($C_{O}$) dimension, which allows sharing of input data among
    threads/PEs for calculating different sets of output channels. Both input and
    output activation data, as well as the filter weights, are organized in the channel-last
    structure, and into blocks of $H\times W\times C_{b}$, where $C_{b}$ is a multiple
    of the SIMD vector length.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Parallelized Direct Convolution Algorithm - HPZMO
  prefs: []
  type: TYPE_NORMAL
- en: Activation I, Filter weights, Stride s = 1;Activation O;for $i\leftarrow 1$
    to $C_{O}/C_{O,b}$  in Parallel do    for $k\leftarrow 1$ to $H_{O}$ do       for $l\leftarrow
    1$ to $W_{O}/W_{O,b}$ do          for $m\leftarrow 1$ to $H_{K}$ do             for $n\leftarrow
    1$ to $W_{K}$ do                for $j\leftarrow 1$ to $C_{I}$ do                   for $ll\leftarrow
    1$ to $W_{O,b}$ do                      for $ii\leftarrow 1$ to $C_{O,b}$ do                          $O_{i\cdot
    C_{O,b}+ii,l\cdot W_{O,b}+ll,k}\mathrel{+}=$  $I_{j,l\cdot W_{O,b}+ll+n,k+m}\times$  $F_{j,i\cdot
    C_{O,b}+ii,n,m}$
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Hardware/Software Codesign Methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this Section, we review some approaches and tools proposed in the literature
    to distribute, partition, and map DL training and inference applications on the
    processing nodes in the underlying computing infrastructure. First, we briefly
    analyze how DNN models can be optimized for execution on a plethora of hardware
    devices. Then, we focus on the approaches for training DL models in the context
    of distributed computing infrastructures. Figure [3](#S3.F3 "Figure 3 ‣ 3\. Hardware/Software
    Codesign Methodologies ‣ A Survey on Design Methodologies for Accelerating Deep
    Learning on Heterogeneous Architectures") shows the main references to methodologies,
    frameworks, and tools that we discuss in this Section.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Application
    partitioning and mapping [Hardware-based search ], [Data parallelism , ], [Model
    parallelism , ], [Pipeline parallelism [ GPipe ([YY19,](#bib.bib107) )
  prefs: []
  type: TYPE_NORMAL
- en: PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155) ) ] ],
    [Hybrid solutions [ Layer-wise parallelism ([JQLA18,](#bib.bib117) )
  prefs: []
  type: TYPE_NORMAL
- en: DAPPLE ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168) ) ] ]
    ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. Application partitioning and mapping discussed in Section [3](#S3
    "3\. Hardware/Software Codesign Methodologies ‣ A Survey on Design Methodologies
    for Accelerating Deep Learning on Heterogeneous Architectures")
  prefs: []
  type: TYPE_NORMAL
- en: 'Training large DL models with vast amounts of data and serving them for inference
    is a non-trivial task. Today, it is often performed in a distributed infrastructure
    composed of multiple, possibly heterogeneous compute nodes. The complexity is
    further exacerbated by the recent trend to integrate the high-performing computing
    and storage equipment in the cloud and HPC data centers with the constrained resources
    provided by devices at the edges of the network. The goals of this compute continuum
    trend are to achieve better privacy, higher autonomy, and energy efficiency as
    well as to reduce response latency, cost, and bandwidth demand to the cloud ([Casamayor:2023,](#bib.bib43)
    ). In this complex and heterogeneous setting, designers need to optimize the complete
    system stack: from ML/DNN algorithms, to model optimization and compression, implementation
    of algorithms onto the hardware platforms enriched with DL accelerators as well
    as the underlying hardware architecture design ([han2016deep,](#bib.bib98) ; [jacob2018quantization,](#bib.bib115)
    ; [jin2019towards,](#bib.bib121) ; [tung2018clip,](#bib.bib219) ).'
  prefs: []
  type: TYPE_NORMAL
- en: The hardware-aware design of DNNs has recently received increasing attention
    to tackle hardware device heterogeneity, especially for DNN inference. Indeed,
    as discussed in Section [4](#S4 "4\. Approximate Computing Methodologies ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    to deploy computationally demanding DNNs for model inference in power- and resource-constrained
    edge systems while maintaining acceptable performance, designers have to trade
    off model accuracy against energy and implementation efficiency. However, the
    plethora of hardware devices available makes it very difficult to choose one solution
    for all cases. Therefore, in addition to techniques for approximate computing,
    model compression, such as quantization-aware training and pruning (e.g., ([jacob2018quantization,](#bib.bib115)
    ; [tung2018clip,](#bib.bib219) )), hardware-aware neural architecture search ([chitty-venkata2023,](#bib.bib51)
    ), which takes hardware characteristics like latency, power, or area into account,
    has become a central aspect in automating the process of designing efficient architectures
    for DNN applications executed at the network edges. Different methodologies have
    been exploited to search for the optimal performing model architecture, ranging
    from reinforcement learning to evolutionary algorithms. For example, in ([abdelfattah2020best,](#bib.bib2)
    ; [jiang2020hardware,](#bib.bib120) ), reinforcement learning-based neural architecture
    search is extended to include a search for an accelerator configuration on FPGAs
    and optimize it for latency and area.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in the context of a distributed infrastructure with an ever-increasing
    number of available nodes and resources, it is parallelization that appears to
    offer the solution for the ever-growing need to accelerate the training of DNN
    applications. DNN models lead themselves with many possibilities for parallelization,
    namely data, model, pipeline, and hybrid parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: In data parallelism, a number of workers (machines or devices, e.g., GPUs) load
    an identical copy of the DL model. The training data is split into non-overlapping
    portions and fed into the model replicas of the workers for training ([krizhevsky2012,](#bib.bib131)
    ). Each worker performs the training on its portion of training data, which leads
    to updates of the model parameters. Hence, the parameters of the model among the
    workers need to be synchronized. The main advantage of data parallelism is that
    it is applicable to any DL model architecture without further domain knowledge
    of the model. It scales well for operations that are computationally intensive
    but have only a few parameters, such as CNNs. However, data parallelism is limited
    to operations that have many parameters, as the parameter synchronization leads
    to a significant communication overhead and may become the bottleneck ([JQLA18,](#bib.bib117)
    ). To address such scalability and single point of failure bottleneck, the parameters
    synchronization can occur in a decentralized manner ([Mayer2020,](#bib.bib146)
    ), with the main disadvantage of increasing the communication cost among workers.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, in model parallelism, the DL model is partitioned into multiple
    parts and each worker loads a different part of the ML/DNN model for training.
    A major challenge of model parallelism is how to split the model into partitions
    that are assigned to the parallel workers ([MM17,](#bib.bib147) ). In the context
    of ML/DNN workloads, model partitioning across different devices has initially
    mostly been a manual process driven by human experts. A common approach for finding
    a good model splitting is to use reinforcement learning ([MPL+17,](#bib.bib153)
    ; [MGP+18,](#bib.bib152) ). Starting from some initial partitioning, permutations
    on that partitioning are performed, and performance is measured (e.g., for one
    training iteration) or learn a placement policy that can then be adjusted for
    new workloads via transfer learning, see e.g., ([ABVG+19,](#bib.bib5) ; [ZRA+19,](#bib.bib250)
    ) or used to bootstrap a genetic algorithm ([PGN+20,](#bib.bib169) ). Unfortunately,
    these methods are computationally expensive, as they need to evaluate large numbers
    of placements and measure the runtime of several inference/training steps. Alternatively,
    the problem is cast into an offline optimization problem of finding good partitions
    and schedules. This includes classic results in scheduling on multiple machines
    and/or devices ([LLKS93,](#bib.bib140) ; [Gra66,](#bib.bib96) ; [KL70,](#bib.bib128)
    ; [PY90,](#bib.bib170) ; [SW99,](#bib.bib206) ; [ST93,](#bib.bib201) ), as well
    as modern DNN scheduling works ([JQLA18,](#bib.bib117) ; [NHP+19,](#bib.bib163)
    ). Such algorithms use profiled compute time of each node (layer or operator)
    and data-transfer requirements between nodes in a graph, and the target deployment
    system infrastructure such as machine and network properties (e.g., measured bandwidths).
    However, such techniques do not evaluate the performance of splits in an online
    fashion. Nevertheless, it has been demonstrated that for well-defined cost models,
    the objective function closely matches real performance (see, e.g., ([NHP+19,](#bib.bib163)
    ; [JZA19,](#bib.bib118) )).
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline parallelism combines model parallelism with data parallelism. In pipeline
    parallelism, the model is split and each worker loads a different part of the
    DL model for training. Recent approaches that support pipeline parallelism include
    GPipe ([YY19,](#bib.bib107) ) and PipeDream ([NHP+19,](#bib.bib163) ; [code2020pipedream,](#bib.bib155)
    ). Specifically, in pipeline parallelism the model is divided among available
    workers, assigning a group of consecutive operators (called layers in DNN terminology)
    in the operator graph to each of them, and then overlapping the computation and
    communication of different inputs in a pipelined fashion. This process can greatly
    reduce inter-worker communication. While pipelining is a simple and widely adopted
    idea, DNN training poses an important challenge not present in traditional pipelining:
    DNN training is bi-directional, being the forward pass followed by a backward
    pass through the same layers in reverse order, using state and intermediate results
    from the forward pass. This results in low hardware efficiency or low statistical
    efficiency unless resorting to parallelization optimization ([NHP+19,](#bib.bib163)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: Proposals related to pipeline training can be classified according to the temporal
    aspect, by differentiating between synchronous vs. asynchronous training. The
    first requires executing gradient synchronizations between adjacent training iterations
    to ensure convergence ([YY19,](#bib.bib107) ). However, it suffers from a significant
    memory consumption, that can be partially mitigated by re-computation. Asynchronous
    training inserts micro-batches into the pipeline concurrently to achieve maximum
    throughput ([NHP+19,](#bib.bib163) ). However, storing multiple versions of model
    parameters is not a common practice due to convergence concerns and increased
    memory demand.
  prefs: []
  type: TYPE_NORMAL
- en: A few frameworks attempt to find a hybrid solution that instead combines some
    of the best properties of each model of parallelism and diminishes some of the
    drawbacks. For example, layer-wise parallelism ([JQLA18,](#bib.bib117) ) proposes
    to apply different parallelization strategies to each individual layer of the
    neural network rather than the same parallelization strategy (i.e., data or model
    parallelism) to all layers. The solution for finding the optimal parallelization
    strategy for each layer is based on a dynamic programming-based graph search algorithm.
    DAPPLE ([fan2021dapple,](#bib.bib79) ; [code2020dapple,](#bib.bib168) ) is a synchronous
    training framework that combines data parallelism and pipeline parallelism for
    large DNN models to ensure training convergence and reduce memory consumption.
    To this end, it schedules tasks backward as early as possible to release the memory
    occupied by activations produced by corresponding forward tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The approaches to distribute DL training and inference that we reviewed above
    aim typically to speed up the performance, for example by achieving better throughput
    and scalability and by reducing communication costs, while improving (or at least
    without deteriorating) model accuracy. In recent years, following a general trend
    within the industry at large, the reduction of carbon emission, the so-called
    green carbon footprint, has started to receive increasing attention also within
    the HPC and ML/DNN communities in order to realize environmentally-responsible
    solutions, e.g., ([wu2022sustainableai,](#bib.bib235) ). Given the high computational
    demand of DL training and inference jobs, there is a large opportunity for energy
    saving. For instance, it is possible to save energy while maintaining an adequate
    level of accuracy at the software level by trading off model variants, i.e., low
    and high-quality models. At the hardware level, multiple solutions can be exploited,
    ranging from the adoption of energy-efficient FPGAs to novel GPU partitioning
    schemes, that can reduce energy consumption by allowing GPU sharing ([li2023green,](#bib.bib141)
    ). Coupling with proper distributed resources scheduling, there is therefore a
    large opportunity for improving performance while reducing cost and carbon emission.
  prefs: []
  type: TYPE_NORMAL
- en: Within this context, DL application partitioning and mapping strategies are
    crucial to designing autonomic strategies optimized for both the training and
    inference phases, which account for different non-functional requirements such
    as performance, energy consumption, accuracy, and heterogeneity. Among the existing
    methodologies to develop hardware/software platforms, reinforcement learning is
    widely used  ([jiang2020hardware,](#bib.bib120) ; [MPL+17,](#bib.bib153) ; [russorusso2023,](#bib.bib189)
    ) to account for the large state space that characterizes the edge scenarios,
    whereby multiple nodes, possibly characterized by their own processing, memory,
    networking capabilities, and energy footprint are pooled to train and serve ML/DNN
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Approximate Computing Methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last years, the design of efficient hardware architectures suitable to
    DL applications has received a great deal of attention and several implementations
    have been proposed for both ASIC and FPGA-based platforms. Special attention was
    paid to design methodologies suitable to achieve efficient computational units
    to be integrated as IP blocks within acceleration architectures for DNNs and HPC
    workloads. At first glance, to comply with the computational complexity of HPC
    applications, with special attention to DL and in particular to DNNs, hardware
    designers focused their efforts mainly on solutions able to reach very high-speed
    performances. In order to comply with this objective, even-increasing levels of
    parallelism have been introduced and increasingly complex computational architectures
    have been realized. Unfortunately, this trend poses several concerns in terms
    of power consumption and costs, especially in power- and resource-constrained
    edge systems. For this reason, more recently, unconventional design methodologies
    have emerged to trade off computational speed with power consumption, while keeping
    cost and resource utilization under control. Among these emerging approaches,
    approximate computing has gained popularity as a powerful technique to reduce
    energy consumption and computational delay in error-resilient applications, such
    as multimedia processing, DL, digital signal processing, and wireless communications
    ([Alioto_2017,](#bib.bib15) ; [Alioto_2018,](#bib.bib16) ). Indeed, hardware accelerators
    designed by exploiting approximate computing methodologies can be integrated as
    custom IPs to provide an overall system with high speed and energy efficiency.
    Computational IPs implementing approximate computing approaches at both algorithmic
    and architecture levels can be exploited in both ASIC and FPGA-based HLS designs,
    with the objectives of reducing, on the one hand, the computational complexity
    of layers typically employed within DL models, and, on the other hand, to optimize
    speed and power consumption introducing a reasonable accuracy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the basic principle of the approximate computing paradigm is very
    simple, i.e. by relaxing the requirement of an exact computation, it is possible
    to trade off the quality of the computation result for speed performances and
    energy dissipation, achieving the expected benefits is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Approximate
    computing approaches [Algorithmic level [Approximate SoftMax ([Zhu_2020,](#bib.bib251)
    ; [Cardarilli_2021,](#bib.bib38) ; [Spagnolo_2022,](#bib.bib211) )
  prefs: []
  type: TYPE_NORMAL
- en: Approximate sensors, memory,
  prefs: []
  type: TYPE_NORMAL
- en: compute and communication ([Ghosh_2020,](#bib.bib93) )
  prefs: []
  type: TYPE_NORMAL
- en: Approximate Pooling layers ([Sayal_2021,](#bib.bib194) ; [Spagnolo_2022_2,](#bib.bib212)
    )
  prefs: []
  type: TYPE_NORMAL
- en: 'Super-Resolution ([Spagnolo_2023,](#bib.bib209) ), Denoising ([Spagnolo_2023_2,](#bib.bib210)
    ) ] ], [Architecture, Gate- and transistor-level [ASIC-based [Encoded: yes ([Chen_2012,](#bib.bib49)
    ; [Cho_2004,](#bib.bib52) ; [Song_2007,](#bib.bib207) ), no ([Esposito_2017,](#bib.bib77)
    ; [Farshchi_2013,](#bib.bib80) ; [Strollo_2022,](#bib.bib216) )'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic ([Frustaci_2020,](#bib.bib87) ), Approx. compressors ([Esposito_2018,](#bib.bib78)
    ; [Strollo_2020,](#bib.bib215) )
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation ([Strollo_2022,](#bib.bib216) ), Approximate MACs ([Kim_2021,](#bib.bib129)
    ) ], ], [FPGA-based [Adders ([Perri_2020,](#bib.bib175) )
  prefs: []
  type: TYPE_NORMAL
- en: Multipliers ([Ullah_2021,](#bib.bib221) ; [Ullah_2022,](#bib.bib220) ; [Perri_2022,](#bib.bib176)
    ; [Waris_2021,](#bib.bib232) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4. Taxonomy of the approximate computing approaches
  prefs: []
  type: TYPE_NORMAL
- en: 'Approximate computing offers several opportunities to design efficient hardware
    accelerators for DL. In fact, as summarized in Figure [4](#S4.F4 "Figure 4 ‣ 4\.
    Approximate Computing Methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures"), it can be exploited at different
    design levels: starting from the algorithm, passing through the architecture,
    up to the gate- and the transistor-level circuit topologies ([Jiang_2020,](#bib.bib119)
    ). Several approximation strategies furnish hardware-oriented solutions to be
    adopted at the algorithmic level to significantly reduce the complexity and energy
    consumption of critical layers employed in DL models. As an example, the approximate
    approach demonstrated in ([Spagnolo_2022,](#bib.bib211) ) allows realizing the
    SoftMax layer in hardware by exploiting simple additions and logical bit-shifting
    operations to replace the computationally expensive exponentiations and divisions.
    When adopted in the realization of DNN accelerators supporting widespread models,
    like VGG-16 and ResNet-50, this approach guarantees computational times and power
    dissipation up to 7 and 12 times lower, respectively, than the accurate counterparts,
    introducing an accuracy penalty lower than 2% ([Spagnolo_2022,](#bib.bib211) ).'
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in ([Spagnolo_2022_2,](#bib.bib212) ), convolutional layers interleaved
    by non-linear activations and down-sampling can benefit from approximate computing
    methodologies, at the expense of a reasonable accuracy loss. The computations
    performed within convolutional layers followed by down-sampling layers are approximated
    through a prediction method that identifies potential predominant features. This
    approximation down-sampling strategy has been hardware-customized for DNN inference
    and, when applied to several benchmark models, it led to an overall energy saving
    of up to 70% with an accuracy loss lower than 3%, with respect to accurate designs.
    Recently, approximate computing has been exploited also to reconstruct high-resolution
    images ([Spagnolo_2023,](#bib.bib209) ) and to reduce the computational complexity
    of image denoising ([Spagnolo_2023_2,](#bib.bib210) ). Indeed, such applications
    have to meet tight constraints in terms of frame rate and energy consumption,
    thus making innovative and specific design methodologies highly desirable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient solutions to exploit approximate computing at gate- and transistor-level
    have been proposed to design approximate adders, multipliers and multiply-accumulate
    units for both ASIC ([Frustaci_2019,](#bib.bib86) ; [Frustaci_2020,](#bib.bib87)
    ; [Strollo_2020,](#bib.bib215) ; [Strollo_2022,](#bib.bib216) ) and FPGA devices
    ([Prabakaran_2018,](#bib.bib178) ; [Ahmad_2021,](#bib.bib10) ; [Ullah_2022,](#bib.bib220)
    ; [Perri_2022,](#bib.bib176) ). Such arithmetic operators receive a great deal
    of attention since they are the basic computational elements extensively used
    in DL models. Typically, the operands to be processed by approximate arithmetic
    circuits are split into sub-words: some of the least significant bits are processed
    inaccurately, whereas the remaining most significant bits are passed to the accurate
    circuits. Some strategies exploit static approximation that inflexibly sets the
    achieved accuracy at design time, while other solutions adopt dynamic approximation
    that allows tuning the quality target at runtime, thus leveraging the specificity
    of the data being processed with a graceful quality degradation. In a similar
    way, several approximation techniques can be adopted in the design of multipliers
    ([Akbari_2017,](#bib.bib12) ; [Esposito_2018,](#bib.bib78) ; [Strollo_2020,](#bib.bib215)
    ; [Frustaci_2019,](#bib.bib86) ; [Frustaci_2020,](#bib.bib87) ). Some techniques
    use dynamic and static segmentation methods ([Hashemi_2015,](#bib.bib100) ; [Narayanamoorthy_2015,](#bib.bib162)
    ; [Strollo_2022,](#bib.bib216) ; [Di_Meo_2023,](#bib.bib70) ): the former downsizes
    the multiplier by selecting only a segment of the inputs starting from the leading
    one bit, whereas the latter processes only predefined portions of the multiplicands.
    Besides the approximation strategies oriented to ASIC designs, appropriate methodologies
    are available to achieve high-performance and low-power designs of approximate
    modular multipliers also on FPGAs ([Ullah_2022,](#bib.bib220) ; [Perri_2022,](#bib.bib176)
    ). Finally, efficient approaches suitable to approximate divisions are demonstrated
    in ([Saadat_2019,](#bib.bib190) ; [Imani_2019,](#bib.bib109) ; [Zendegani_2016,](#bib.bib245)
    ) that either exploit approximate subtractors ([Chen_2015,](#bib.bib45) ; [Chen_2016,](#bib.bib46)
    ) or implement the signal segmentation technique ([Hashemi_2016,](#bib.bib101)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. HLS-based design methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To design efficient accelerators for DL on heterogeneous architectures, both
    software and hardware modeling steps are crucial. Moreover, an appropriate link
    between them is necessary to combine the algorithmic description with the constraints
    used to define desired performances, resource usage, and energy consumption. This
    link is provided by the High-level synthesis (HLS) that offers a variety of advantages
    to the designer, who not only can work at a higher level of abstraction when developing
    high-performance hardware, which leads to faster design changes and much faster
    functional verification, but also can create various solutions on several platforms
    (e.g., larger or smaller FPGAs) without altering the C/C++ source code, by just
    changing design directives. Such advantages make the exploration of design spaces
    and the identification of the best implementation much faster than with low-level
    hardware designs. However, the code must be written with a hardware implementation
    in mind in order to meet given performance and resource usage requirements. In
    fact, arbitrary software code, written for a CPU target, can achieve very low
    performance, since it typically does not expose enough parallelism to exploit
    the spatial concurrency available on an FPGA or an ASIC. Figure [5](#S5.F5 "Figure
    5 ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for
    Accelerating Deep Learning on Heterogeneous Architectures") provides a taxonomy
    of the HLS-based design methodologies reviewed in the following sub-sections,
    with a focus on the acceleration of deep learning (DL) models. The previous survey
    ([hlssurvey,](#bib.bib59) ) analyzed the evolution of HLS tools and the challenges
    that lay ahead of them to support newer application domains and performance requirements.
    In this paper, we focus our attention mainly on the commercial tool Vitis HLS
    and the open-source tool PandA-Bambu, both suitable for the HLS of complex DL
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [HLS Design-based
    methodologies [HLS tools [ Vitis High-Level Synthesis ([vitisug212,](#bib.bib238)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Bambu ([ferrandi2021bambu,](#bib.bib81) )
  prefs: []
  type: TYPE_NORMAL
- en: Intel HLS Compiler ([IntelHLS2022,](#bib.bib113) )
  prefs: []
  type: TYPE_NORMAL
- en: Catapult ([CatapultHLS2022,](#bib.bib202) ), Stratus HLS ([StratusHLS2022,](#bib.bib31)
    )
  prefs: []
  type: TYPE_NORMAL
- en: LegUp  ([LegUpHLS2013,](#bib.bib36) ), MLIR ([lattner2021mlir,](#bib.bib137)
    ) - CIRCT ([circt,](#bib.bib57) ) ], ], [HLS-based DL frameworks [ hls4ml ([duarte2018fast,](#bib.bib73)
    )
  prefs: []
  type: TYPE_NORMAL
- en: FINN ([blott2018finn,](#bib.bib25) )
  prefs: []
  type: TYPE_NORMAL
- en: ScaleHLS ([ye2022scalehls,](#bib.bib243) ; [scalehls2022dac,](#bib.bib244) )
  prefs: []
  type: TYPE_NORMAL
- en: SODA Synthesizer ([sodaMICRO,](#bib.bib27) ; [sodaDAC,](#bib.bib6) ) ], ], [IP
    block integration and interface protocols [ OpenFPGA CoreLib core library interoperability
    effort ([parCo,](#bib.bib234) )
  prefs: []
  type: TYPE_NORMAL
- en: AXI protocol interface ([arm2013,](#bib.bib20) )
  prefs: []
  type: TYPE_NORMAL
- en: IP-XACT ([ipxact,](#bib.bib4) )
  prefs: []
  type: TYPE_NORMAL
- en: IEEE 1735-2014 ([IEEE1735,](#bib.bib108) )
  prefs: []
  type: TYPE_NORMAL
- en: Vitis block integration ([IPflow,](#bib.bib239) ; [vitisug212,](#bib.bib238)
    ) ] ], ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5. Taxonomy of HLS-based design methodologies discussed in Section [5](#S5
    "5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures")
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. The Vitis High-Level Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vitis HLS is the Xilinx High-Level Synthesis toolchain that provides FPGA designers
    with efficient support to describe their hardware designs using high-level languages,
    such as C and C++. These codes are then translated into a register-transfer level
    (RTL) language, i.e. VHDL or Verilog, automatically, thus simplifying the time-consuming
    and error-prone process based on RTL codes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60cc048bc43d451b364c1f893f5333bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Vitis HLS design flow
  prefs: []
  type: TYPE_NORMAL
- en: As schematized in Figure [6](#S5.F6 "Figure 6 ‣ 5.1\. The Vitis High-Level Synthesis
    ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures"), in this design flow, C/C++ languages
    are used to describe both the hardware functionality and the testbench. The C
    simulation examines the behavior of the design just at the functional level and
    must be successful to continue with the synthesis process. The latter generates
    the RTL description of the top function complying with optimization directives
    and constraints furnished by the designer. The comprehensive report produced at
    the end of this step estimates time and hardware resource usage and can be used
    as a reference for any further refinement and optimization. The subsequent C/RTL
    Co-Simulation is performed to verify and validate the synthesized RTL design using
    the same C/C++ testbench previously used for the C simulation. It provides the
    designer with cycle-accurate performance information and it can also spot synthesis
    tool bugs. The IP core obtained by the HLS can be finally exported to be embedded
    within more complex designs by using Vivado and Vitis tools. Through appropriate
    synthesis directives, also known as *HLS pragmas* ([vitisug212,](#bib.bib238)
    ; [Sestito_2023,](#bib.bib197) ), Vitis HLS provides the designers with the ability
    to control both micro- and macro-architectural characteristics of their designs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/761d7184fefd8eb1b61cfa17b3e52afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7. Effects of loop-level pragmas: (a) loop-pipelining; (b) loop-unrolling;
    (c) execution without pipelining'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f1b9b16b66509bc6b32239f9f17794c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8. The DATAFLOW pragma: (a) the syntax; (b) execution with pragma; (c)
    execution without pragma'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figures [7](#S5.F7 "Figure 7 ‣ 5.1\. The Vitis High-Level Synthesis
    ‣ 5\. HLS-based design methodologies ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures") and  [8](#S5.F8 "Figure 8 ‣ 5.1\.
    The Vitis High-Level Synthesis ‣ 5\. HLS-based design methodologies ‣ A Survey
    on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures"),
    to reduce the latency and increase the throughput, loop-level transformations
    (such as *Loop pipelining*, *Loop unrolling* and *Loop flattening*) and dataflow
    pragmas can be exploited. The former allows optimizing the hardware execution
    of loops, whereas, the latter allows sequential functions to be overlapped in
    their hardware execution. Conversely, to control the hardware resources utilization,
    variable-level (such as *array partitioning* and *array reshaping*) and operation-level
    pragmas (such as *bind op* and *allocate*) can be used to affect, respectively,
    the memory structures and the organization of computational units implemented
    for specific operations. Finally, to define the communication protocol to be adopted
    at the interface ports of the design, the interface pragma is available.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. The Bambu Open-source High-Level Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bambu ([ferrandi2021bambu,](#bib.bib81) ) is an open-source command-line HLS
    tool developed at Politecnico di Milano supporting the synthesis of most C/C++
    constructs, including function calls, pointer arithmetic, and dynamic resolution
    of memory accesses, accesses to arrays and structs, parameters passed by reference
    or copy, and more. The Bambu design flow is schematized in Figure [9](#S5.F9 "Figure
    9 ‣ 5.2\. The Bambu Open-source High-Level Synthesis ‣ 5\. HLS-based design methodologies
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70ca3653d72ac124b01b30248085d43f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9. Bambu Compilation flow.
  prefs: []
  type: TYPE_NORMAL
- en: At the front end, Bambu interfaces with existing compilers, such as GCC and
    Clang, to parse the input code and translate it into an Intermediate Representation
    (IR) with a Static Single Assignment in-memory. This approach decouples the compiler
    front-end from the rest of the HLS process and allows different compilers to be
    rapidly and easily integrated. Starting from the IR, Bambu builds data structures
    (such as the Call Graph, Control Data Flow Graphs, and Program Dependence Graphs)
    and applies a set of device-independent analyses and transformations (e.g. data
    flow analysis, loop recognition, dead code elimination, constant propagation,
    LUT expression insertion, etc.). Often, transformations at the hardware level
    are also adopted to improve area utilization and delay of the final accelerator
    (e.g. multiplications and divisions by constant values are transformed into expressions
    that use only shifts and additions). Differently from general-purpose software
    compilers, designed to target a processor with a fixed-sized data path of 32 or
    64 bits, Bambu selects the minimal number of bits required for specific operations
    and value storage through Bitwidth and Range Analysis. These passes are crucial
    during the optimization process to fulfill requirements in terms of speed performances,
    area, and power, without affecting the functional behavior of the synthesized
    design.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the back end, Bambu performs the actual architectural synthesis and furnishes
    the VHDL/Verilog code. This process acts on each function separately, taking into
    account that a single function includes at least two sub-modules: the control
    logic and the data path. The first step of this process is the Function Allocation
    which associates functions in the IR with specific resources available in the
    target technology. The Bambu technology library contains standard functions, standard
    system libraries, such as libc and libm, and custom components, written in Verilog
    or VHDL. Bambu supports function pointers and sharing of (sub)modules across module
    boundaries through function proxies ([FPL2015,](#bib.bib151) ), which provides
    valuable area savings when complex call graphs are considered, with no significant
    impact on the execution delays. Subsequently, the Memory Allocation step defines
    the memory types to be used. Statically analyzing the memory accesses, Bambu builds
    a hierarchical data path where memories can be classified as read-only, local,
    with aligned or unaligned memory accesses, requiring dynamic resolutions. Multiple
    buses connect load/store components to their respective memories; the same memory
    infrastructure can also be connected to external components such as scratchpads,
    caches, and off-chip DRAMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, the Resource Allocation associates the operations that are not mapped
    on a library function to resource units available in the resource library. Floating-point
    operations are supported either through a soft-float library or by the FloPoCo
    framework ([DinechinPasca2011-DaT,](#bib.bib68) ). Resources are pre-characterized
    and their description includes information about their latency, area, and number
    of pipeline stages. When more than one matching between operation and resource
    is feasible, the selection is driven by design constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'To associate operations with control steps, Bambu employs a list scheduling
    algorithm. In addition to this old but efficient method, Bambu also implements
    the more aggressive speculative scheduling algorithm based on System of Difference
    Constraints ([ICCAD2015B,](#bib.bib139) ). Within the computed schedule, operations
    that execute concurrently cannot share the same resource, so module binding is
    performed through a clique covering algorithm on a weighted compatibility graph ([LStok94,](#bib.bib214)
    ). Variables with non-overlapping life intervals may share the same register,
    so liveness analysis is applied before binding storage values to registers. Interconnections
    are bound according to the outcome of the previous steps: if a functional or memory
    resource is shared, then the algorithm introduces steering logic on its inputs
    and identifies the set of control signals driven by the controller.'
  prefs: []
  type: TYPE_NORMAL
- en: The final architecture is then generated and represented through a hyper-graph
    highlighting the interconnection between modules. Finally, depending on the technology/device
    chosen as the target, the netlist generation step furnishes the final RTL description
    of the entire design.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Other HLS tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In most cases, HLS tools are provided by FPGA vendors within an FPGA full design
    suite by the same company. The aforementioned Vitis HLS, for example, is part
    of the AMD/Xilinx tool suite and only supports Xilinx FPGAs. The Intel HLS Compiler
    ([IntelHLS2022,](#bib.bib113) ) is part of the Quartus design suite: it compiles
    C++ functions into an RTL implementation for Intel FPGAs and optimizes them through
    a simple command-line interface. Intel recently announced that the HLS compiler
    will be substituted by the oneAPI toolkit ([oneapi,](#bib.bib112) ) to enable
    developers a seamless porting of OpenCL code across CPUs, GPUs, and FPGAs. Catapult
    ([CatapultHLS2022,](#bib.bib202) ) is a multi-target HLS and verification tool
    provided by Siemens, synthesizing C++ and SystemC code for FPGA and ASIC. Stratus
    HLS ([StratusHLS2022,](#bib.bib31) ) from Cadence synthesizes SystemC code written
    with a lower-level perspective, i.e., requiring users to explicitly describe interface
    protocols between components. LegUp ([LegUpHLS2013,](#bib.bib36) ) is an open-source,
    LLVM-based HLS tool developed in academia and later acquired by Microchip and
    rebranded as SmartHLS ([smartHLS,](#bib.bib150) ).'
  prefs: []
  type: TYPE_NORMAL
- en: The Multi-Level Intermediate Representation (MLIR) ([lattner2021mlir,](#bib.bib137)
    ) is a reusable and extensible infrastructure in the LLVM project for the development
    of domain-specific compilers. MLIR allows defining specialized IRs called dialects
    to implement analysis and transformation passes at different levels of abstraction,
    and it can interface with multiple software programming frameworks, including
    the ones used to implement DL algorithms. The CIRCT project ([circt,](#bib.bib57)
    ) intends to use MLIR to build a new generation of interoperable tools and compilers
    for hardware design, starting from the definition of circuit-level IRs and working
    upwards to higher levels of abstraction (e.g., dataflow models or finite state
    machines). Part of the project is dedicated to HLS ([circt-hls,](#bib.bib222)
    ), particularly to the implementation of static and dynamic scheduling through
    MLIR and CIRCT dialects. CIRCT could be an essential building block for future
    industrial and academic design flows; however, its degree of maturity is lower
    compared to HLS tools with optimized synthesis algorithms and resource libraries
    supported by decades of research.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. HLS-based Design Flows for Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two popular frameworks that help automate the design of ML accelerators are
    hls4ml ([duarte2018fast,](#bib.bib73) ) and FINN ([blott2018finn,](#bib.bib25)
    ). They parse ML models and replace operators with C/C++ functions taken from
    a library of templates that already contain pragmas. The HLS tool used as the
    backend (mainly Vivado or Vitis HLS) processes this intermediate C/C++ representation
    and produces a corresponding accelerator design without requiring further manual
    intervention.
  prefs: []
  type: TYPE_NORMAL
- en: The library of templates in hls4ml and FINN is necessarily tied to a specific
    HLS tool and a narrow set of models. This is required by expert HLS developers
    to implement in advance the best version of all necessary operators for a pre-determined
    backend tool. Given that, each tool has its own coding patterns, annotations,
    and configuration directives that are not recognized by other tools, a new version
    of the library is needed to switch to a new hardware target to achieve efficient
    designs.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, models of interest are Deep and Convolutional Neural Networks (DNNs/CNNs),
    sometimes with a scope limited by application requirements that can significantly
    affect resource utilization. For example, the original implementation of hls4ml
    was optimized for small, fully connected models under tight latency constraints,
    reflecting the needs of a high-energy physics experiment at CERN. To comply with
    those requirements, hls4ml proposed to store network weights inside on-chip logic
    and unroll all loops to increase parallelism, which quickly depletes FPGA resources
    when considering more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that DNNs and CNNs cover a significant part of ML applications
    (especially in the computer vision field), there is ample room for exploring other
    classes of models, for example, to accelerate scientific applications that work
    on sparse data structures or graphs. Existing HLS-based design flows are good
    at extracting data- and instruction-level parallelism (e.g. by unrolling loops),
    but they are not equipped to deal with irregular task-based patterns.
  prefs: []
  type: TYPE_NORMAL
- en: A narrow focus limits the possibility of quickly adapting to new algorithmic
    approaches, which would instead be desirable in a rapidly evolving field such
    as ML (and data science in general). For this reason, many efforts have been made
    to use existing HLS tools as ”black boxes”, thus exploiting their optimization
    opportunities as much as possible. However, there is also a trend toward the democratization
    of hardware design, as attested for example by the open-source release of the
    Xilinx Vitis HLS frontend ([vitis-llvm,](#bib.bib18) ) or by the OpenROAD project
    for ASIC synthesis ([openroad,](#bib.bib11) ).
  prefs: []
  type: TYPE_NORMAL
- en: Other efficient frameworks suitable to analyze input codes from C or PyTorch
    and to generate annotated code for Vivado HLS are ScaleHLS ([ye2022scalehls,](#bib.bib243)
    ; [scalehls2022dac,](#bib.bib244) ) and SOftware Defined Architectures (SODA)
    ([sodaMICRO,](#bib.bib27) ; [sodaDAC,](#bib.bib6) ). Through the multiple levels
    of abstraction provided by existing MLIR dialects, ScaleHLS reasons about graph-level,
    loop-level, and directive-level optimizations; a custom dialect helps the translation
    into C++ with pragmas. A quality of results estimator and a DSE engine automatically
    identify the best combination of optimizations following user-defined constraints,
    without requiring long simulation or synthesis runs to evaluate the effect of
    changes in the optimization directives.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, SODA is an open-source, multi-level, modular, extensible, no-human-in-the-loop
    hardware compiler that translates high-level ML models into domain-specific accelerators.
    It comprises a compiler-based frontend that leverages MLIR (SODA-OPT ([sodaopt,](#bib.bib26)
    )) and a compiler-based backend that integrates state-of-the-art HLS methodologies
    (Bambu); it generates highly specialized designs that can be synthesized with
    both commercial and open-source tools on FPGAs or ASICs, and it allows exploring
    design metrics through compilation passes and parameters, enabling the identification
    of architectural trade-offs depending on the target application requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. IP block integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a common practice to use pre-designed blocks supplied by third parties
    (Intellectual Properties, IPs) to add the desired functionalities to a system
    being developed. The use of IPs not only saves development time, but also the
    huge testing time that has been spent to qualify it. The high level of abstraction
    given by HLS flows and their maturity provide the interesting possibility to export
    functionalities developed through HLS as IPs. A first attempt to address this
    feature in a systematic way dates back to 2008 ([parCo,](#bib.bib234) ), where
    various strategies used by HLS tools to integrate IPs were analyzed. The standardization
    of IPs structure for their distribution ([IEEE1735,](#bib.bib108) ) does not cover
    IPs generated by HLS, so IP integration and export may be supported differently
    by each tool (e.g., Vitis HLS provides its own block integration capabilities
    ([IPflow,](#bib.bib239) ; [vitisug212,](#bib.bib238) )).
  prefs: []
  type: TYPE_NORMAL
- en: To allow IP generation and reuse, an interfacing standard is mandatory to allow
    interoperability among IPs. Currently, AXI4 ([arm2013,](#bib.bib20) ) is used
    as a de facto standard. The AXI4 standard includes AXI4-Full, AXI4-stream, and
    AXI4-Lite protocols used to access memory banks, streaming channels, and memory-mapped
    registers. Other than a common interface, a common language to describe the IP
    interfaces and the IP organization on the filesystem is needed. IP-XACT ([ipxact,](#bib.bib4)
    ) is an XML format describing meta-data and interfaces of IPs and is widely adopted
    by IP providers to describe their IPs in terms of file system organization, interfaces,
    source files, constraint files, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Deep Learning Compilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of innovative hardware architectures, particularly for highly
    parallelizable applications such as DL, is only half of the picture. The other
    half is the effective automated deployment technologies that allow the use of
    novel architectures to run complex real-world applications. Managing the memory
    hierarchy and compiling high-level signal processing and machine learning graphs
    into a representation is a complex research problem, that has been extensively
    studied in the past few years, as summarized in Figure  [10](#S6.F10 "Figure 10
    ‣ 6\. Deep Learning Compilers ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures").
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Deep Learning
    Compilers [Memory hierarchy management in DNN Accelerators [ Ivanov et al. ([ivanov2020data,](#bib.bib114)
    )
  prefs: []
  type: TYPE_NORMAL
- en: DMazeRunner ([dave2019dmazerunner,](#bib.bib65) )
  prefs: []
  type: TYPE_NORMAL
- en: Maestro ([kwon2020maestro,](#bib.bib132) )
  prefs: []
  type: TYPE_NORMAL
- en: Interstellar ([yang2020interstellar,](#bib.bib240) ), Timeloop ([parashar2019timeloop,](#bib.bib171)
    ) ], ], [DL Compilers for MCUs [ TFLite Micro ([david2020tensorflow,](#bib.bib66)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Larq Computing Engine (LCE) ([Larq,](#bib.bib91) )
  prefs: []
  type: TYPE_NORMAL
- en: X-CUBE-AI ([CubeAI,](#bib.bib213) )
  prefs: []
  type: TYPE_NORMAL
- en: GWT AutoTiler ([GAP8Auto-tilerManual,](#bib.bib97) ), DORY ([burrello2021dory,](#bib.bib30)
    ) ], ], [DL Compilers for High-Performance and Embedded [ TVM ([chen2018tvm,](#bib.bib47)
    ), HTVM ([vandelmHTVMEfficientNeural2023a,](#bib.bib223) ), Halide ([ragan2013halide,](#bib.bib182)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Comprehensions ([vasilacheTensorComprehensionsFrameworkAgnostic2018,](#bib.bib228)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Glow ([rotemGlowGraphLowering2019,](#bib.bib187) ), Relay ([roeschRelayNewIR2018,](#bib.bib185)
    )
  prefs: []
  type: TYPE_NORMAL
- en: ONNC ([linONNCCompilationFramework2019,](#bib.bib142) ), MLIR ([lattnerMLIRScalingCompiler2021,](#bib.bib138)
    ; [jinCompilingONNXNeural2020,](#bib.bib122) ) ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10. Taxonomy of Deep Learning compilers discussed in Section [6](#S6
    "6\. Deep Learning Compilers ‣ A Survey on Design Methodologies for Accelerating
    Deep Learning on Heterogeneous Architectures")
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Memory hierarchy management in DNN Accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Effective management of memory hierarchy is a critical challenge in deploying
    DNNs, which generate high amounts of weights and activations traffic between different
    levels of memory hierarchy. To tackle this problem, various methods have been
    proposed for data flow scheduling and generation across three broad classes of
    devices: high-performance computing systems, DNN accelerators, and embedded systems.
    For high-performance computing systems, Ivanov et al. ([ivanov2020data,](#bib.bib114)
    ) have proposed new transformer primitives to exploit data reuse and limit data
    movement. Meanwhile, DMazeRunner ([dave2019dmazerunner,](#bib.bib65) ), Maestro ([kwon2020maestro,](#bib.bib132)
    ), Interstellar ([yang2020interstellar,](#bib.bib240) ), Timeloop ([parashar2019timeloop,](#bib.bib171)
    ) discuss DNN optimization on AI-specialized accelerators based on systolic arrays
    of processing elements (PEs), with a focus on loop tiling and/or reordering to
    optimize PE utilization. These tools can output an accelerator model to run a
    given DNN or spatial scheduling to maximize PE array utilization. MCU data flow
    scheduling tools are similar to frameworks like DMazeRunner as both optimize dataflow
    schedules given an externally known architecture. However, DNN execution on MCUs
    presents unique challenges such as adapting to a general-purpose architecture
    and limited memory. Additionally, kernel instructions are heavily influenced by
    the limited size of the register file, resulting in increased load-store operations
    and demand for optimal loop sizing to avoid register spilling overhead. Academic
    researchers and industries have investigated this aspect by incorporating specialized
    caches or explicitly managed scratchpad memories into their edge-node solutions.
    For example, NXP offers specialized caches in their Cortex M4/M0 MCU, as does
    STMicroelectronics with its STM32 Cube-AI toolflow; on the other hand, GreenWaves
    Technologies provides explicitly managed scratchpad memories ([flamand2018gap,](#bib.bib83)
    ), with a GAPFlow tool dedicated to managing them appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Deep Learning Compilers for MCUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The introduction of the first generation of low-power NN-oriented MCUs has
    increased this need, as these platforms need to utilize optimized software and
    ISA extensions for DNN computing alongside traditional control and I/O-bound activities.
    To enable optimal execution of both types of tasks, these MCUs employ parallel
    and heterogeneous processing. ST Microelectronics¹¹1[https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html](https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html)
    and NXP have recently introduced new-generation dual-core microcontrollers with
    an ARM M0 processor dedicated to I/O and an ARM M4 processor with single-cycle
    multiply-and-accumulate and SIMD capabilities. These platforms show an increased
    complexity in terms of memory hierarchy compared to conventional flat-memory MCUs,
    with an L1 memory optimized for speed and an L2 optimized for capacity. At the
    same time, there is a trend towards explicit management of memory hierarchy, with
    hand-tunable data caches featuring locking for hand-crafted data management. For
    instance, the Kendrite K210²²2https://canaan.io/product/kendryteai is a RISC-V
    dual-core 64 bits system-on-chip with a neural network processor (KPU) on which
    the cores can offload the computation. It also includes dedicated memory banks
    for the NN accelerator and a DMA unit to explicitly manage data transfers. The
    SONY Spresense board³³3https://developer.sony.com/develop/spresense/ features
    a 6-cores M4 accelerator with a maximum clock speed of 156 MHz, 1.5 MB of SRAM,
    and 8 MB of Flash. The GreenWaves Technologies GAP-8 ([flamand2018gap,](#bib.bib83)
    ) system-on-chip was introduced in 2018 as a commercial embodiment of the Parallel
    Ultra-Low-Power paradigm ([conti2016pulp,](#bib.bib60) ): it features one I/O
    core and an 8-core SIMD-optimized DSP cluster accelerator using an extension of
    the RISC-V ISA. To manage this complexity, these MCUs include dedicated infrastructure
    for data marshaling, such as general-purpose DMA controllers to speed up memory
    transfers and reduce the memory access bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: New tools such as TFLite Micro ([david2020tensorflow,](#bib.bib66) ) and the
    Larq Computing Engine (LCE) ([Larq,](#bib.bib91) ) offer a model-agnostic deployment
    framework and overcome these problems. Both are non-vendor-locked tools supporting
    ARM Cortex-M and RISC-V cores. Their library memory footprints require only 16
    kB on a Cortex-M3; however, by default they rely on graph interpretation at runtime,
    limiting achievable performance. To offset this limitation, TFLite Micro allows
    plugging in optimized kernels and declaring vectors in different memory regions.
    However, it does not include any tiling mechanism to execute layers that do not
    fit on-chip memory.
  prefs: []
  type: TYPE_NORMAL
- en: The two most powerful DNN deployment tools for microcontrollers available in
    the state-of-the-art have been proposed by the industry as proprietary, vendor-locked
    solutions for their own MCUs. X-CUBE-AI ([CubeAI,](#bib.bib213) ) from STMicroelectronics
    is an automatic NN library generator optimized on computation and memory. It converts
    a pre-trained DNN model from DNN tools such as Tensorflow into a precompiled library
    for the ARM Cortex-M cores embedded in STM32 series MCUs. X-CUBE-AI relies on
    relatively large on-chip L1 caches (up to 16 kB) to deliver performance on STM32
    MCUs, and it does not tackle software-based memory management. On the other hand,
    GWT designed a tool called AutoTiler, to target the GAP-8 RISC-V-based multi-core
    ultra-low-power microcontroller. One of its primary functions is to take a pre-trained
    DNN and generate code for memory tiling and efficient transfers of weight and
    activation data between all memory levels (on- and off-chip). The GWT AutoTiler
    directly tackles the data-movement and tile sizing challenge to optimize memory
    access, reaching state-of-the-art performance on the execution of many networks.
    The tool is proprietary, but its backend basic kernels are available as open-source
    as part of the GAP-8 SDK⁴⁴4https://github.com/GreenWaves-Technologies/gap_sdk.
    DORY ([burrello2021dory,](#bib.bib30) ) targets the same platform with an open-source
    tool. It optimizes the memory traffic for DNN deployment on specialized edge devices.
    By generating C code that tiles the execution of a dedicated kernel library, DORY
    reduces the size of intermediate buffers. This is crucial since microcontrollers
    often have limited level-1 (L1) memory. To achieve this, DORY formalizes tiling
    as an optimized constraint programming problem with kernel-specific heuristics.
    The produced code is more optimized but less general than previous solutions.
    Using DORY on a new architecture requires creating a new dedicated kernel library,
    new templates, and reprogramming the tiler to tailor it to specific hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Deep Learning Compilers for High-Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A popular DNN deployment framework that targets both high-performance embedded
    and edge devices is TVM ([chen2018tvm,](#bib.bib47) ). Its primary optimization
    mechanism is autotuning: it quickly compiles differently-scheduled yet equivalent
    kernel implementations, and after running those on hardware, the most optimal
    kernel is selected. As such, TVM can implicitly improve the execution time on
    CPUs and GPUs and fine-grained general matrix multiply (GEMM) accelerators like
    VTA ([moreau2019hardware,](#bib.bib154) ). Moreover, TVM runtime can link in (vendor-provided)
    optimized kernels in LLVM IR, CUDA C, C/C++ into a standalone artifact with the
    bring your own codegen (BYOC) ([chen2021bring,](#bib.bib50) ) infrastructure.
    However, using the autotuning pipeline of TVM is impractical for specialized coarse-grained
    accelerators since proving coarse-grained kernel equivalence requires complex
    loop nest analysis. This can be bypassed by using BYOC, but in this way, many
    of the automatic optimization opportunities presented by the framework are lost.
    HTVM ([vandelmHTVMEfficientNeural2023a,](#bib.bib223) ) uses DORY as a backend
    of TVM employing this technique.'
  prefs: []
  type: TYPE_NORMAL
- en: A popular research avenue has been to increase the level of abstraction to compile
    DL-based applications, using Domain Specific Language that mainly addresses tensor-level
    representations, such as the early examples of Halide ([ragan2013halide,](#bib.bib182)
    ) and Tensor Comprehensions ([vasilacheTensorComprehensionsFrameworkAgnostic2018,](#bib.bib228)
    ). Dedicated DL compilers such as Glow ([rotemGlowGraphLowering2019,](#bib.bib187)
    ) have been focused on graph-lowering techniques, using these earlier developments
    and ideas to build up systems that take a high-level description of a DL program,
    typically in the form a data-flow graph of operators, lower it into a set of IRs
    still centered on tensor-aware operations, and then deploy on target machine-specific
    code. A common graphical format for the input of such lowering passes is ONNX⁵⁵5https://onnx.ai/,
    whereas intermediate representations can be custom and dedicated to one particular
    framework (e.g., Relay ([roeschRelayNewIR2018,](#bib.bib185) ) for Amazon’s open
    source NNVM compiler) or deployed as a specialization of a more general IR ([linONNCCompilationFramework2019,](#bib.bib142)
    ). In this regard, the most relevant example is MLIR ([lattner2021mlir,](#bib.bib137)
    ; [jinCompilingONNXNeural2020,](#bib.bib122) ), a framework proposed in the context
    of the LLVM project that enables building custom intermediate representations
    for domain-specific computing. While this tool is not exclusive to DL, it has
    been proposed in response to the needs of the DL community and has quickly risen
    to prominence.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Modeling, Simulation, Profiling and Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To effectively design a hardware accelerator for DL, it is essential to have
    access to powerful modeling tools that can provide detailed insights into the
    power consumption, performance, and area requirements of the accelerator. In this
    section, we will explore some of the most popular and effective tools available
    for modeling hardware accelerators for DL, and discuss their key features and
    capabilities. These tools enable designers to experiment with various design choices
    and configurations and to optimize their designs for specific Power Performance
    Area (PPA) metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [11](#S7.F11 "Figure 11 ‣ 7\. Modeling, Simulation, Profiling and Exploration
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures") provides a taxonomy of the various tools, frameworks, and methodologies
    discussed in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Modeling,
    Simulation, Profiling, and Exploration [Modeling, Simulation,
  prefs: []
  type: TYPE_NORMAL
- en: and Exploration [ NVDLA ([nvdla,](#bib.bib167) ), MLPAT ([tang_dossa18,](#bib.bib218)
    )
  prefs: []
  type: TYPE_NORMAL
- en: MAESTRO ([kwon_micro20,](#bib.bib133) ), Timeloop ([parashar_ispass19,](#bib.bib172)
    )
  prefs: []
  type: TYPE_NORMAL
- en: LAMBDA ([russo_percom21,](#bib.bib188) ), DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249)
    )
  prefs: []
  type: TYPE_NORMAL
- en: DNNExplorer ([zhang2021beingahead,](#bib.bib248) ), Gemmini ([genc_dac21,](#bib.bib92)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Interstellar ([yang_asplos20,](#bib.bib241) ), Aladdin ([shao_isca14,](#bib.bib199)
    ) ] ], [Simulation tools for emerging
  prefs: []
  type: TYPE_NORMAL
- en: memories-based DNN accelerator [ DNN+NeuroSim ([peng2019iedm,](#bib.bib174)
    )
  prefs: []
  type: TYPE_NORMAL
- en: SySCIM ([shadmehri2022date,](#bib.bib198) )
  prefs: []
  type: TYPE_NORMAL
- en: MemTorch ([Lammie2022,](#bib.bib135) )
  prefs: []
  type: TYPE_NORMAL
- en: MNSIM ([xia2016date,](#bib.bib237) )
  prefs: []
  type: TYPE_NORMAL
- en: Reiser et al. ([reiser2023newcas,](#bib.bib184) ) ] ], [Cycle-Accurate Simulators
    [ SCALE-SIM ([samajdar2018scal,](#bib.bib193) )
  prefs: []
  type: TYPE_NORMAL
- en: STONNE ([munozmartinez2020stonne,](#bib.bib158) )
  prefs: []
  type: TYPE_NORMAL
- en: SimuNN ([cao_jestcs20,](#bib.bib37) )
  prefs: []
  type: TYPE_NORMAL
- en: AccTLMSim ([kim2020transactionlevel,](#bib.bib130) )
  prefs: []
  type: TYPE_NORMAL
- en: QADAM ([inci2022qadam,](#bib.bib110) ), QAPPA ([inci2022qappa,](#bib.bib111)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Juraci et al. ([juracy_tcs22,](#bib.bib124) ) ] ], [Modeling and Profiling FPGAs
  prefs: []
  type: TYPE_NORMAL
- en: for custom accelerators [ Shuhai ([shuhai,](#bib.bib231) ), HPCChallenge ([hpcc-fpga,](#bib.bib149)
    )
  prefs: []
  type: TYPE_NORMAL
- en: HPCG Benchmark ([hpcg-fpga,](#bib.bib246) )
  prefs: []
  type: TYPE_NORMAL
- en: Da Silva et al. ([roofline-fpga-hls,](#bib.bib61) ), Siracusa et al. ([roofline-fpga-cad,](#bib.bib205)
    ; [roofline-fpga-cad2,](#bib.bib204) )
  prefs: []
  type: TYPE_NORMAL
- en: Muralidharan et al. ([roofline-multibench-fpga,](#bib.bib156) )
  prefs: []
  type: TYPE_NORMAL
- en: ERT ([ert-opencl-fpga,](#bib.bib165) ; [ert-opencl-fpga2,](#bib.bib164) ) ]
    ] ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11. Modeling, Simulation, Profiling, and Exploration tools and methodologies
    discussed in Section [7](#S7 "7\. Modeling, Simulation, Profiling and Exploration
    ‣ A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous
    Architectures")
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. Modeling, simulation, and exploration tools.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Integration with NN frameworks | Model type | Full SoC | Evaluation metrics
    | Target | Estimation error |'
  prefs: []
  type: TYPE_TB
- en: '| MLPAT ([tang_dossa18,](#bib.bib218) ) | No | Analytical | No | PPA | ASIC
    | ¡5% area ¡10% power |'
  prefs: []
  type: TYPE_TB
- en: '| MAESTRO ([kwon_micro20,](#bib.bib133) ) | No | Empirical | No | Performance
    | ASIC | 5% |'
  prefs: []
  type: TYPE_TB
- en: '| Timeloop ([parashar_ispass19,](#bib.bib172) ) | No | Analytical/ Empirical
    | No | PPA | ASIC | 5% |'
  prefs: []
  type: TYPE_TB
- en: '| LAMBDA ([russo_percom21,](#bib.bib188) ) | No | Analytical/ Empirical | No
    | PPA | ASIC | 5% |'
  prefs: []
  type: TYPE_TB
- en: '| DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249) ) | No | Analytical |
    No | Performance Energy | FPGA/ASIC | ¡18% |'
  prefs: []
  type: TYPE_TB
- en: '| DNNExplorer ([zhang2021beingahead,](#bib.bib248) ) | Caffe, PyTorch | – |
    No | Performance | FPGA | – |'
  prefs: []
  type: TYPE_TB
- en: '| Gemmini ([genc_dac21,](#bib.bib92) ) | No | Simulation | Yes + OS support
    | Performance | FPGA/ASIC | – |'
  prefs: []
  type: TYPE_TB
- en: '| Interstellar ([yang_asplos20,](#bib.bib241) ) | No | Analytical | No | PPA
    | ASIC | 2% |'
  prefs: []
  type: TYPE_TB
- en: '| Aladdin ([shao_isca14,](#bib.bib199) ) | No | Simulation Analytical | No
    | PPA | ASIC | 1% performance 5% power 7% area |'
  prefs: []
  type: TYPE_TB
- en: '| SCALE-SIM ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) | No | Empirical | Yes | Performance, Area | ASIC | – |'
  prefs: []
  type: TYPE_TB
- en: '| STONNE ([munozmartinez2020stonne,](#bib.bib158) ) | Caffe | Cycle level simulation
    | Yes | Performance | ASIC | ¡3% |'
  prefs: []
  type: TYPE_TB
- en: '| SimuNN ([cao_jestcs20,](#bib.bib37) ) | TensorFlow | Cycle level simulation
    | Yes | PPA | FPGA/ASIC | – |'
  prefs: []
  type: TYPE_TB
- en: '| AccTLMSim ([kim2020transactionlevel,](#bib.bib130) ) | No | Cycle level simulation
    | Yes | Performance | ASIC | 3% |'
  prefs: []
  type: TYPE_TB
- en: '| Juracy *et al.* ([juracy_tcs22,](#bib.bib124) ) | TensorFlow | Cycle level
    simulation | No | PPA | ASIC | ¡7% |'
  prefs: []
  type: TYPE_TB
- en: '| DNN-NeuroSim ([peng2019iedm,](#bib.bib174) ) | Tensorflow, PyTorch | Instruction
    accurate simulation | Yes | PPA | ASIC | - |'
  prefs: []
  type: TYPE_TB
- en: '| SySCIM ([shadmehri2022date,](#bib.bib198) ) | No | Circuit level simulation
    | No | Accuracy | ASIC | $<$4% accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Memtorch ([Lammie2022,](#bib.bib135) ) | PyTorch | Analytical/ Empirical
    | Yes | PPA | ASIC | - |'
  prefs: []
  type: TYPE_TB
- en: '| MNSIM ([xia2016date,](#bib.bib237) ) | No | Cycle level simulation | Yes
    | PPA | ASIC | - |'
  prefs: []
  type: TYPE_TB
- en: 7.1\. Modeling, Simulation, and Exploration Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we overview the most recent and influential frameworks utilized
    for modeling, simulating, and exploring the design space of hardware accelerators
    for DL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NVIDIA DL Accelerator (NVDLA) ([nvdla,](#bib.bib167) ) is an open-source
    framework designed to facilitate the implementation of machine learning (ML) applications.
    It includes a training infrastructure and a compiler to convert existing models
    for use by the NVDLA software. NVDLA can read a neural network from a front-end
    environment, like Caffe, and map it to the NVIDIA accelerator. The MLPAT framework ([tang_dossa18,](#bib.bib218)
    ) enables the modeling of power, area, and timing for ML accelerators including
    architecture components such as systolic arrays, memories, dataflows and activation
    pipelines, as well as different precision types and technologies. MAESTRO ([kwon_micro20,](#bib.bib133)
    ) is a framework designed to analyze and describe neural network processing engines
    to explore the power/performance tradeoffs to implement a target architecture.
    It features a domain-specific language for dataflow description, which enables
    the specification of parameters such as the number of processing elements, memory
    size, and Network-on-Chip bandwidth. The Timeloop ([parashar_ispass19,](#bib.bib172)
    ) infrastructure helps to explore the architecture design space of DNN accelerators.
    It consists of two main components: a model that provides projections for performance,
    area, and energy, and a mapper that constructs and searches through the design
    space of a given workload on a targeted architecture. To use Timeloop, the user
    describes the architecture’s organization using a configurable template that includes
    abstractions for compute units, memories, and communication links. The mapper
    then constructs the mapspace and searches for an optimal mapping using the model’s
    speed and accuracy. Accelergy ([wu_iccad19,](#bib.bib236) ) is a versatile energy
    estimation technique for accelerators: it enables designers to create specifications
    using custom high-level compound components and low-level primitive components,
    which can be evaluated using third-party energy estimation plug-ins. LAMBDA ([russo_percom21,](#bib.bib188)
    ) is a framework based on the Timeloop/Accelergy infrastructure to explore the
    design space of configurable DNN accelerators taking into account a variety of
    architectural and microarchitectural parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DNN-Chip Predictor ([zhao2021dnnchip,](#bib.bib249) ) can predict the energy
    consumption, throughput, and latency of DNN accelerators before they are implemented.
    It offers two advantages: (1) it uses an analytical performance formulation to
    enable rapid exploration and optimization of DNN ASIC/FPGA accelerator designs;
    (2) it supports different algorithm-to-hardware mappings and architectures. DNNExplorer ([zhang2021beingahead,](#bib.bib248)
    ) can be used to evaluate customized hardware DNN accelerators and to explore
    new accelerator designs with respect to performance and efficiency. It supports
    popular ML frameworks (Caffe and PyTorch) for analyzing DNN workloads and providing
    analytical models for accelerator benchmarking. It has a high-dimensional design
    space and fine-grained adjustability to overcome design limitations, and a design
    space exploration engine to generate optimized accelerators based on target AI
    workloads and hardware resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Gemmini ([genc_dac21,](#bib.bib92) ) is an open-source DNN accelerator generator
    that enables users to design custom hardware accelerator systems for rapidly evolving
    DNN workloads. It provides a complete solution that spans both hardware and software
    stack, and it is compatible with the RISC-V ecosystem. Gemmini’s hardware design
    options can be tuned for performance, efficiency, and extensibility. It implements
    a multi-level software stack with an easy-to-use programming interface and tight
    integration with Linux-capable SoCs.
  prefs: []
  type: TYPE_NORMAL
- en: DNN accelerator micro-architectures and their program mappings are specific
    choices of loop order and hardware parallelism for computing the nested loops
    of DNNs. It has been observed that these hardware variants can be precisely and
    concisely represented by Halide’s scheduling language. In Interstellar ([yang_asplos20,](#bib.bib241)
    ), modifications were made to the Halide compiler to generate hardware for fair
    comparisons with prior accelerators. Interstellar highlights the significance
    of optimizing the memory hierarchy with a higher impact on energy metrics than
    the dataflow selection. Aladdin ([shao_isca14,](#bib.bib199) ) is a simulation
    tool for the quick exploration of design options of systems focused on accelerators.
    It is a pre-RTL and power-performance simulator that takes as input algorithm
    descriptions in high-level languages and uses dynamic data dependence graphs to
    represent an accelerator without the need to generate RTL.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Simulation tools for emerging memories-based DNN accelerator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another set of tools for DNN modeling, simulation, and profiling is that related
    to emerging memories-based accelerators. DNN+NeuroSim ([peng2019iedm,](#bib.bib174)
    ) is an integrated framework to benchmark compute-in-memory (CIM) accelerators
    for DNNs, with hierarchical design options from the device level, to circuit-level
    and up to the algorithm level. A Python wrapper is developed to interface NeuroSim
    with popular ML platforms such as Pytorch and Tensorflow. The framework supports
    the automatic mapping of algorithms to hardware and the evaluation of chip-level
    performance and inference accuracy with hardware constraints. SySCIM ([shadmehri2022date,](#bib.bib198)
    ) considers the impact of the non-idealities of the CIM components, including
    memristor device, memristor crossbar (interconnects), analog-to-digital converter,
    and trans-impedance amplifier, on the vector-matrix multiplication performed by
    the CIM unit. The CIM modules are described in SystemC and SystemC-AMS to reach
    a high simulation speed while maintaining simulation accuracy. MemTorch ([Lammie2022,](#bib.bib135)
    ) is an open-source framework for customized large-scale memristive DL simulations,
    with a refined focus on the co-simulation of device non-idealities. MemTorch also
    facilitates the co-modeling of key crossbar peripheral circuitry. MemTorch adopts
    a modernized software engineering methodology and integrates directly with the
    well-known PyTorch ML library.
  prefs: []
  type: TYPE_NORMAL
- en: MNSIM ([xia2016date,](#bib.bib237) ) proposes a simulation platform for the
    memristor-based neuromorphic system with a hierarchical structure and flexible
    interfaces for customization. A detailed reference design is provided for large-scale
    applications like ISAAC or PRIME accelerators demonstrated in the previous deliverable.
    A behavior-level computing accuracy model is incorporated to evaluate the computing
    error rate affected by interconnect lines and nonideal device factors. Experimental
    results show that MNSIM achieves over 7000 times more speed-up than SPICE simulation.
    MNSIM can optimize the design and estimate the tradeoff relationships among different
    performance metrics for users. In ([reiser2023newcas,](#bib.bib184) ), it is proposed
    a simulation framework together with suitable abstractions to propagate the effects
    of RRAM crossbar configuration parameters to their ultimate implications over
    inference performance stability. RRAM devices’ non-idealities result in significant
    inference accuracy drops compared to software baseline accuracy. A critical issue
    is related to the drift of the conductance states appearing immediately at the
    end of the program and verifying algorithms that are mandatory for accurate multi-level
    conductance operation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Cycle-Accurate Simulators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For accurate simulations, it is crucial to model the behavior of the hardware
    accelerator at the cycle-by-cycle level, accounting for all the interactions between
    the different hardware components. Among these simulation tools, SCALE-SIM (SystoliC
    AcceLErator SIMulator) ([samajdar2018scal,](#bib.bib193) ; [samajdar_ispass20,](#bib.bib192)
    ) is a simulator that provides cycle-accurate energy/performance modeling for
    DNN accelerators by considering various factors such as on-chip and off-chip memory
    accesses, and interface bandwidth. It has two primary components: (i) a compute
    unit that utilizes a systolic array that can be customized according to size and
    aspect ratio, and (ii) an accelerator memory system that features three double-buffered
    SRAM memories with user-specified sizes. STONNE (Simulation Tool for Neural Network
    Engines) ([munozmartinez2020stonne,](#bib.bib158) ) is a highly modular and extensible
    simulation framework for the end-to-end evaluation of flexible accelerator DNN
    architectures with cycle accuracy. Like Timeloop, STONNE uses the Accelergy energy
    estimation tool to estimate energy and area. SimuNN ([cao_jestcs20,](#bib.bib37)
    ) is a pre-RTL neural network simulator for early phase verification and fast
    prototyping before the design is converted into hardware. It supports different
    data precisions and it is compatible with TensorFlow. SimuNN provides multi-level
    trace results that can be used as a reference for the final hardware design. Additionally,
    it can evaluate the hardware performance under various quantizations, dataflows,
    and configurations based on a generalized hardware model. AccTLMSim ([kim2020transactionlevel,](#bib.bib130)
    ) is a pre-RTL simulation tool based on SystemC transaction-level modeling (TLM)
    to simulate CNN accelerators with cycle accuracy. The tool includes a detailed
    model of the interface with the DRAM for precise tracking of each bus transaction
    between the accelerator and DRAM while considering the communication bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: QADAM ([inci2022qadam,](#bib.bib110) ) and its evolution QAPPA ([inci2022qappa,](#bib.bib111)
    ) are parameterized RTL frameworks designed to model power, performance, and area
    of quantization-aware DNN accelerators. These frameworks support design space
    exploration and Pareto-efficiency analysis for a range of design choices, including
    bit precision, processing element (PE) type, scratchpad sizes of PEs, global buffer
    size, total number of PEs, and DNN configurations. The DSE approach proposed in ([juracy_tcs22,](#bib.bib124)
    ) for CNNs employs an analytical model derived from the physical synthesis of
    hardware accelerators. This model is integrated into CNN frameworks such as TensorFlow,
    enabling precise outcomes. The analytical model provides estimates for various
    factors, including area, performance, power, energy, and memory accesses. The
    accuracy of the model was tested by comparing it to data obtained from physical
    synthesis, and it was observed that the average error was less than 7%.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Modeling and Profiling FPGAs for custom accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of off-the-shelf highly-parallel hardware accelerators to boost the
    performance of software applications, in particular DL algorithms, represents
    nowadays a very common option, adopted by a large and increasing number of HPC
    systems. In this scenario, GPUs are definitively the most common accelerators,
    while some data centers have only recently started to adopt FPGAs to speed-up
    network interconnects ([project-catapult,](#bib.bib180) ), and specific workloads ([fpga-datacenter,](#bib.bib17)
    ) such as ML inference algorithms ([fpga-dl,](#bib.bib200) ; [project-brainwave,](#bib.bib85)
    ). FPGAs could represent an interesting trade-off, allowing user customizations,
    as well as the use of off-the-shelf hardware, to implement custom DL accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Given the rapidly increasing use of ML methods in several application fields,
    and the interest in reconfigurable architectures, which is rising in the HPC community
    since several years ([fpga-hpc,](#bib.bib225) ; [fpga-hpc-trends,](#bib.bib229)
    ; [fpga-hpc2,](#bib.bib76) ), we might expect FPGAs to become a more common option,
    as accelerators, for next-generation HPC systems. In the past, several reasons
    have prevented this shift. First, FPGAs were not designed to provide high floating-point
    (FP) computing performance ([fpga-hpc-trends,](#bib.bib229) ), while typical HPC
    workloads usually require double-precision (DP) and single-precision (SP) FP computations.
    Secondly, FPGA programming could be a very time-consuming process, requiring the
    use of specific hardware programming skills and the use of programming languages
    not common among HPC developers communities ([fpga-programming,](#bib.bib21) ).
    Thirdly, the code written for one FPGA could hardly run across different devices
    without a complete re-design, causing serious portability issues not acceptable
    for a wide set of HPC applications, for which even the porting to GPUs had been
    a long and suffered process ([decade-gpu-porting,](#bib.bib224) ). However, more
    recently, these barriers started to fade thanks to improvements in hardware architectures
    and programming frameworks. The latest generations of FPGAs integrate thousands
    of programmable DSPs (Digital Signal Processors) able to implement SP- and DP-FP
    operations ([fpga-dsp-fp,](#bib.bib29) ; [mapping-dsp,](#bib.bib186) ; [xilinx-dsp-flops,](#bib.bib226)
    ), and might also embed custom FP DSP blocks. This is leading to devices capable
    to reach a performance in the same order of magnitude as commodity HPC processors
    (i.e. TFLOP/s), and in some cases able to deliver a better energy-efficiency ([altera-dsp-flops,](#bib.bib23)
    ; [fpga-opencl-hpc,](#bib.bib252) ). At the same time, the recent improvements
    in synthesis tools, and the development of new programming approaches such as
    HLS  ([survey-hls,](#bib.bib161) ), allow programmers to develop codes using high-level
    languages. These approaches are very similar to those (e.g. OpenMP and OpenACC)
    commonly used by HPC developers to target multi-core CPUs and other accelerators,
    which are also able to guarantee a fair level of code portability ([ompss-fpga2,](#bib.bib28)
    ). All the above improvements combined with the urging quest for higher energy-efficiency
    and lower latency interconnects in exascale HPC systems, are leading to a significant
    increase in the interest in heterogeneity and specialized computing in the form
    of reconfigurable accelerators ([exa-dataflow,](#bib.bib242) ). This makes the
    use of FPGAs very attractive to scale-out resources by enabling distributed computing
    and can be programmed to be network-capable processors implementing custom interconnects
    featuring low-latency communications without involving the CPU control ([euroexa-fpga-net,](#bib.bib136)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: First prototypes of FPGA accelerated HPC systems are already being designed
    and deployed. One example is the Alveo FPGA Cluster installed at ETH Zurich in
    the context of the Xilinx Adaptive Compute Clusters (XACC) initiative, using commodity
    hardware to support novel research in adaptive compute acceleration for HPC. Another
    example is the EU-H2020 EuroEXA Project, which has developed a HPC system prototype
    with custom hardware, adopting FPGA-based accelerators for both computing and
    networking ([euroexa-fpga-net,](#bib.bib136) ). As a future scenario, we could
    expect the next generations of HPC systems to be equipped with FPGA-based accelerators,
    probably alongside other accelerators, such as GPUs, being programmed with high-level
    languages, possibly based on pragma directives, allowing to address different
    types of accelerators in an uniform way ([ompss-fpga2,](#bib.bib28) ). In this
    context, application developers need to estimate the performance achievable on
    target FPGAs, to decide whether an application kernel is worth to be ported, or
    which FPGA better fits its computing requirements. At the same time, system architects
    and engineers need to estimate the performance of a single FPGA, to feed performance
    models to tune, balance and optimize the performance at system level ([exa-dataflow,](#bib.bib242)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Several research works have investigated FPGAs performance when used as hardware
    accelerators, mostly using synthetic benchmarks to estimate the bandwidth of off-chip
    memories ([shuhai,](#bib.bib231) ; [fpga-stream-opencl,](#bib.bib159) ; [intel-fpga-mem,](#bib.bib253)
    ), and OpenCL kernels to measure the FPGA computing performance ([fpga-fp-eval,](#bib.bib123)
    ; [hpcc-fpga,](#bib.bib149) ; [hpcg-fpga,](#bib.bib246) ). In ([shuhai,](#bib.bib231)
    ) is presented the Shuhai Verilog benchmark, used to characterize the performance
    of HBM and DDR off-chip memories embedded in the Xilinx Alveo U280. In ([hpcc-fpga,](#bib.bib149)
    ) is presented an OpenCL implementation of the HPCChallenge Benchmark Suite, reporting
    the results for different FPGAs. In ([hpcg-fpga,](#bib.bib246) ), it is reported
    a C/HLS implementation of the HPCG benchmark targeting FPGAs. Interestingly, in
    this case, the Roofline Model has been used, but only to assess the optimization
    level of the specific application, with respect to theoretical estimations.
  prefs: []
  type: TYPE_NORMAL
- en: The Roofline Model has already been used in the past to evaluate the performance
    of specific applications ([fpga-roofline-tsunami,](#bib.bib160) ), being ported
    to FPGAs. However, few works provide a generic application-independent extension
    of this model for these architectures, mainly due to the difficulty in defining
    the maximum compute performance for a reconfigurable device. The first comprehensive
    work extending the Roofline Model to FPGAs has been presented in ([roofline-fpga-hls,](#bib.bib61)
    ), here authors focus mainly on aiding developers to explore the design space
    options. Building on the same principle, more recently, in ([roofline-fpga-cad,](#bib.bib205)
    ) and in its extended version ([roofline-fpga-cad2,](#bib.bib204) ), a semi-automated
    performance optimization methodology based on the Roofline model for FPGAs has
    been proposed. In this case, the authors, aim for a tool to explore the design
    space, while in our case, we aim to provide a benchmarking tool.
  prefs: []
  type: TYPE_NORMAL
- en: The first work proposing a methodology for the performance analysis of FPGAs
    allowing to make Roofline plots and cross-architectural comparisons, has been
    reported in ([roofline-multibench-fpga,](#bib.bib156) ). In this case, the authors
    use OpenCL as a programming language to provide mini-apps, such as SHOCL0, LINPACK,
    and STREAM, to measure the computing performance and the memory bandwidth of the
    off-chip memory. Using OpenCL also the ERT benchmark has been reported to run
    on FPGAs in ([ert-opencl-fpga,](#bib.bib165) ) and in its extension ([ert-opencl-fpga2,](#bib.bib164)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: In ([parco19-fp,](#bib.bib33) ), the first C/HLS benchmark tool capable of providing
    empirical Roofline plots for FPGAs was presented. The work was later extended
    to support the Xilinx Vitis workflow to allow for a wider adoption ([fer-fpl,](#bib.bib34)
    ). This tool, named FER (FPGA Empirical Roofline) ([fer,](#bib.bib35) ), and available
    as Free Software ([fer-code,](#bib.bib32) ), has been developed by INFN and the
    University of Ferrara, and it allows for application-agnostic performance assessment
    of FPGA-based accelerators, aiming for comprehensive machine characterization,
    allowing for cross-architectural comparisons and for performance estimations of
    generic HPC kernels on a given device. To this aim, FER is able to measure both
    the computing peak performance of FPGAs, and the bandwidths of on-chip and off-chip
    memories. It is based on the Roofline Model and it implements at its core a directive
    annotated C/HLS kernel, with tunable operational intensity and hardware resources
    usage. Moreover, it relies on a theoretical model aiming to strictly link the
    performance results to the available hardware resources. The choice of C/HLS allows
    at the same time to expose to the users low-level fine-tuning knobs, as well as
    using a high-level programming paradigm that can easily be used by the HPC user
    community for development and porting.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusions and Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design of Deep Learning accelerators for heterogeneous architectures requires
    a plethora of methodologies and EDA tools to assist the designer at different
    design, simulation and verification stages. The complexity of recent DL heterogeneous
    System-on-Chip architectures reached billions of transistors, such as the 80 billion
    transistors included in the Nvidia Hopper H100 GPU ([choquetteNVIDIAHopperH1002023,](#bib.bib55)
    ). Recent advancements in EDA tools enabled to handle the increased complexity
    of heterogeneous architectures, improving design productivity while meeting the
    design goals in terms of speedup and energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, this survey provides an overview of the main flows and frameworks
    developed in recent years to manage the complexity and design goals of DL accelerators.
    In particular, the survey started at a high level, discussing the models and characteristics
    used in DL application workloads. Then, the first methodologies described the
    hardware-software partitioning problem that decomposes a DL model into software
    and hardware parts, choosing the best architecture for each component defined.
    In this context, approximate computing methodologies are essential in reducing
    the computational complexity and memory requirements. We then discussed methods
    to implement hardware accelerators by exploiting the high-level synthesis methodologies
    that may efficiently target technologies such as ASIC or FPGAs. In addition, HLS
    flows allow higher flexibility and support an easy integration of future-developed
    DL operators. Finally, we present the set of methodologies to compile software,
    model, profile, evaluate and explore the various design knobs that a DL application
    might have.
  prefs: []
  type: TYPE_NORMAL
- en: Before concluding this survey, we briefly summarize some interesting challenges
    and trends in EDA. One interesting trend is the application of Machine Learning
    and Deep Learning techniques in the optimization of EDA tasks. Some recent surveys
    discuss this trend, such as  ([ML4EDA21,](#bib.bib106) ) and ([RenHu23,](#bib.bib99)
    ), and the current understanding is that we still need to find the right combination
    of existing heuristic solutions with ML-based approaches. The idea is that the
    ability of DL-based techniques to extract meaningful knowledge from a large amount
    of data will be of some help in driving the design automatization process. In
    particular, Machine Learning allows us to improve predictions and optimizations,
    important steps of many design automation tasks. Moreover, Machine Learning can
    support the scalability requirements of modern design automation tools ([Kahng23,](#bib.bib125)
    ). Challenges in this integration lie in the explainability of Machine Learning
    models and their relations with the design objectives and constraints that are
    only sometimes straightforward ([SALEEM22,](#bib.bib191) ).
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work has been (partially) supported by the Spoke 1 ”FutureHPC & BigData”
    of the Italian Research Center on High-Performance Computing, Big Data and Quantum
    Computing (ICSC) funded by MUR Missione 4 - Next Generation EU (NGEU).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Abdelfattah, A., Anzt, H., Boman, E. G., Carson, E., Cojean, T., Dongarra,
    J., Fox, A., Gates, M., Higham, N. J., Li, X. S., Loe, J., Luszczek, P., Pranesh,
    S., Rajamanickam, S., Ribizel, T., Smith, B. F., Swirydowicz, K., Thomas, S.,
    Tomov, S., Tsai, Y. M., and Yang, U. M. A survey of numerical linear algebra methods
    utilizing mixed-precision arithmetic. The International Journal of High Performance
    Computing Applications 35, 4 (2021), 344–369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(2) Abdelfattah, M. S., Dudziak, Ł., Chau, T., Lee, R., Kim, H., and Lane,
    N. D. Best of Both Worlds: AutoML Codesign of a CNN and Its Hardware Accelerator.
    In Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference (2020),
    DAC ’20, IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(3) Abdelouahab, K., Pelcat, M., Sérot, J., and Berry, F. Accelerating CNN
    inference on FPGAs: A Survey. CoRR abs/1806.01683 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(4) Accellera. Accellera IP-XACT working group: IP-XACT User Guide, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(5) Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., and Alizadeh,
    M. Placeto: Learning Generalizable Device Placement Algorithms for Distributed
    Machine Learning. In Proceedings of the 33rd International Conference on Neural
    Information Processing Systems (Red Hook, NY, USA, 2019), Curran Associates Inc.,
    p. 3981–3991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(6) Agostini, N. B., Curzel, S., Limaye, A., Amatya, V., Minutoli, M., Castellana,
    V. G., Manzano, J., Tumeo, A., and Ferrandi, F. The SODA Approach: Leveraging
    High-Level Synthesis for Hardware/Software Co-Design and Hardware Specialization.
    In Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC) (2022),
    pp. 1359–1362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) Agullo, E., Aumage, O., Faverge, M., Furmento, N., Pruvost, F., Sergent,
    M., and Thibault, S. P. Achieving High Performance on Supercomputers with a Sequential
    Task-based Programming Model. IEEE Transactions on Parallel and Distributed Systems
    (2017), 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (8) Agullo, E., Buttari, A., Guermouche, A., Herrmann, J., and Jego, A. Task-Based
    Parallel Programming for Scalable Matrix Product Algorithms. ACM Trans. Math.
    Softw. (feb 2023). Just Accepted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (9) Ahle, T. D., and Silvestri, F. Similarity Search with Tensor Core Units.
    In Proc. 13th Int. Conf. Similarity Search and Application (SISAP) (2020), vol. 12440,
    pp. 76–84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (10) Ahmad, W., Ayrancioglu, B., and Hamzaoglu, I. Low Error Efficient Approximate
    Adders for FPGAs. IEEE Access 9 (2021), 117232–117243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(11) Ajayi, T., Chhabria, V. A., Fogaça, M., Hashemi, S., Hosny, A., Kahng,
    A. B., Kim, M., Lee, J., Mallappa, U., Neseem, M., Pradipta, G., Reda, S., Saligane,
    M., Sapatnekar, S. S., Sechen, C., Shalan, M., Swartz, W., Wang, L., Wang, Z.,
    Woo, M., and Xu, B. Toward an Open-Source Digital Flow: First Learnings from the
    OpenROAD Project. In Proceedings of the 56th Annual Design Automation Conference
    (DAC) (2019), pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (12) Akbari, O., Kamal, M., Afzali-Kusha, A., and Pedram, M. Dual-Quality 4:2
    Compressors for Utilizing in Dynamic Accuracy Configurable Multipliers. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 25, 4 (2017), 1352–1361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(13) Akkad, G., Mansour, A., and Inaty, E. Embedded deep learning accelerators:
    A survey on recent advances. IEEE Transactions on Artificial Intelligence (2023),
    1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(14) Aldinucci, M., Danelutto, M., Kilpatrick, P., and Torquati, M. Fastflow:
    High-Level and Efficient Streaming on Multicore. John Wiley & Sons, Ltd, 2017,
    ch. 13, pp. 261–280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (15) Alioto, M. Energy-quality scalable adaptive VLSI circuits and systems beyond
    approximate computing. In Design, Automation & Test in Europe Conference & Exhibition
    (DATE), 2017 (2017), pp. 127–132.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(16) Alioto, M., De, V., and Marongiu, A. Guest Editorial for the Special Issue
    on Energy-Quality Scalable Circuits and Systems for Sensing and Computing: from
    Approximate, to Communication-Inspired and Learning-Based. IEEE Journal on Emerging
    and Selected Topics in Circuits and Systems 8 (08 2018), 1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(17) Alonso, G. Research for practice: FPGAs in datacenters. Communications
    of the ACM 61, 9 (2018), 48–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) AMD-Xilinx. Vitis HLS LLVM 2021.2, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(19) Anderson, A., Vasudevan, A., Keane, C., and Gregg, D. High-Performance
    Low-Memory Lowering: GEMM-based Algorithms for DNN Convolution. In 2020 IEEE 32nd
    International Symposium on Computer Architecture and High Performance Computing
    (SBAC-PAD) (2020), pp. 99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (20) ARM. AMBA® AXI™ and ACE™ Protocol Specification, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (21) Bacon, D. F., Rabbah, R., and Shukla, S. FPGA Programming for the Masses.
    Commun. ACM 56, 4 (Apr. 2013), 56–63.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (22) Balay, S., Abhyankar, S., Adams, M. F., Benson, S., Brown, J., Brune, P.,
    Buschelman, K., Constantinescu, E. M., Dalcin, L., Dener, A., Eijkhout, V., Faibussowitsch,
    J., Gropp, W. D., Hapla, V., Isaac, T., Jolivet, P., Karpeev, D., Kaushik, D.,
    Knepley, M. G., Kong, F., Kruger, S., May, D. A., McInnes, L. C., Mills, R. T.,
    Mitchell, L., Munson, T., Roman, J. E., Rupp, K., Sanan, P., Sarich, J., Smith,
    B. F., Zampini, S., Zhang, H., Zhang, H., and Zhang, J. PETSc Web page. [https://petsc.org/](https://petsc.org/),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (23) BDT. Floating-point DSP Energy Efficiency on Altera 28 nm FPGAs. Tech.
    rep., Berkeley Design Technology Inc., Feb 2013. An Independent Evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (24) Bilardi, G., and Pietracaprina, A. Models of Computation, Theoretical.
    Springer US, Boston, MA, 2011, pp. 1150–1158.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(25) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., et al. FINN-R: An end-to-end deep-learning framework for fast exploration
    of quantized neural networks. ACM Transactions on Reconfigurable Technology and
    Systems 11, 3 (2018), 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (26) Bohm Agostini, N., Curzel, S., Amatya, V., Tan, C., Minutoli, M., Castellana,
    V. G., Manzano, J., Kaeli, D., and Tumeo, A. An MLIR-based Compiler Flow for System-Level
    Design and Hardware Acceleration. In IEEE/ACM International Conference On Computer
    Aided Design (ICCAD) (2022), pp. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., and
    Tumeo, A. Bridging Python to Silicon: The SODA Toolchain. IEEE Micro 42, 5 (2022),
    78–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (28) Bosch, J., Tan, X., Filgueras, A., Vidal, M., Mateu, M., Jiménez-González,
    D., Álvarez, C., Martorell, X., Ayguade, E., and Labarta, J. Application Acceleration
    on FPGAs with OmpSs@FPGA. In 2018 International Conference on Field-Programmable
    Technology (FPT) (Dec 2018), pp. 70–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (29) Brosser, F., Cheah, H. Y., and Fahmy, S. A. Iterative floating point computation
    using FPGA DSP blocks. In 2013 23rd International Conference on Field programmable
    Logic and Applications (2013), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(30) Burrello, A., Garofalo, A., Bruschi, N., Tagliavini, G., Rossi, D., and
    Conti, F. DORY: Automatic end-to-end deployment of real-world DNNs on low-cost
    IoT MCUs. IEEE Trans Comput. (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (31) Cadence. Stratus High-Level Synthesis, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (32) Calore, E., 2020. https://baltig.infn.it/EuroEXA/FER.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (33) Calore, E., and Schifano, S. Energy-efficiency evaluation of FPGAs for
    floating-point intensive workloads. In Parallel Computing is Everywhere (2020),
    vol. 36 of Advances in Parallel Computing, pp. 555–564.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (34) Calore, E., and Schifano, S. F. Performance assessment of FPGAs as HPC
    accelerators using the FPGA Empirical Roofline. In 2021 31st International Conference
    on Field-Programmable Logic and Applications (FPL) (2021), pp. 83–90.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(35) Calore, E., and Schifano, S. F. FER: A Benchmark for the Roofline Analysis
    of FPGA Based HPC Accelerators. IEEE Access 10 (2022), 94220–94234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(36) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski,
    T. S., Brown, S. D., and Anderson, J. H. LegUp: An open-source high-level synthesis
    tool for FPGA-based processor/accelerator systems. ACM Trans. Embed. Comput. Syst.
    13, 2 (2013), 24:1–24:27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(37) Cao, S., Deng, W., Bao, Z., Xue, C., Xu, S., and Zhang, S. SimuNN: A Pre-RTL
    Inference, Simulation and Evaluation Framework for Neural Networks. IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems 10, 2 (2020), 217–230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (38) Cardarilli, G. C., Di Nunzio, L., Fazzolari, R., Giardino, D., Nannarelli,
    A., Re, M., and Spanò, S. A pseudo-softmax function for hardware-based high speed
    image classification. Scientific Reports 11, 1 (Jul 2021), 15307.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (39) Cardellini, V., Filippone, S., and Rouson, D. W. I. Design Patterns for
    Sparse-Matrix Computations on Hybrid CPU/GPU Platforms. Sci. Program. 22, 1 (jan
    2014), 1–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (40) Carratalá-Sáez, R., Faverge, M., Pichon, G., Sylvand, G., and Quintana-Ortí,
    E. S. Tiled Algorithms for Efficient Task-Parallel H-Matrix Solvers. In 2020 IEEE
    International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
    (2020), pp. 757–766.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (41) Carson, E., Knight, N., and Demmel, J. Avoiding Communication in Nonsymmetric
    Lanczos-Based Krylov Subspace Methods. SIAM Journal on Scientific Computing 35,
    5 (2013), S42–S61.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(42) Carter Edwards, H., Trott, C. R., and Sunderland, D. Kokkos: Enabling
    manycore performance portability through polymorphic memory access patterns. Journal
    of Parallel and Distributed Computing 74, 12 (2014), 3202–3216. Domain-Specific
    Languages and High-Level Frameworks for High-Performance Computing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (43) Casamayor Pujol, V., Morichetta, A., Murturi, I., Kumar Donta, P., and
    Dustdar, S. Fundamental Research Challenges for Distributed Computing Continuum
    Systems. Information 14, 3 (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (44) Chellapilla, K., Puri, S., and Simard, P. High-performance convolutional
    neural networks for document processing. In Tenth International Workshop on Frontiers
    in Handwriting Recognition, Suvisoft (2006), pp. 99–106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (45) Chen, L., Han, J., Liu, W., and Lombardi, F. Design of Approximate Unsigned
    Integer Non-Restoring Divider for Inexact Computing. In Proceedings of the 25th
    Edition on Great Lakes Symposium on VLSI (New York, NY, USA, 2015), GLSVLSI ’15,
    Association for Computing Machinery, p. 51–56.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (46) Chen, L., Han, J., Liu, W., and Lombardi, F. On the Design of Approximate
    Restoring Dividers for Error-Tolerant Applications. IEEE Transactions on Computers
    65, 8 (2016), 2522–2533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(47) Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan,
    M., Wang, L., Hu, Y., Ceze, L., et al. TVM: An automated End-to-End optimizing
    compiler for deep learning. In OSDI (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (48) Chen, Y., Xie, Y., Song, L., Chen, F., and Tang, T. A Survey of Accelerator
    Architectures for Deep Neural Networks. Engineering 6, 3 (2020), 264–274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(49) Chen, Y.-H., and Chang, T.-Y. A High-Accuracy Adaptive Conditional-Probability
    Estimator for Fixed-Width Booth Multipliers. IEEE Transactions on Circuits and
    Systems I: Regular Papers 59, 3 (2012), 594–603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (50) Chen, Z., Yu, C. H., Morris, T., Tuyls, J., Lai, Y.-H., Roesch, J., Delaye,
    E., Sharma, V., and Wang, Y. Bring Your Own Codegen to Deep Learning Compiler.
    arXiv preprint arXiv:2105.03215 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(51) Chitty-Venkata, K. T., and Somani, A. K. Neural Architecture Search Survey:
    A Hardware Perspective. ACM Computing Surveys 55, 4 (nov 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (52) Cho, K.-J., Lee, K.-C., Chung, J.-G., and Parhi, K. Design of low-error
    fixed-width modified booth multiplier. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 12, 5 (2004), 522–531.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(53) Cho, M., and Brand, D. MEC: Memory-Efficient Convolution for Deep Neural
    Network. In Proceedings of the 34th International Conference on Machine Learning
    - Volume 70 (2017), ICML’17, JMLR.org, p. 815–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(54) Choe, J., Huang, A., Moreshet, T., Herlihy, M., and Bahar, R. I. Concurrent
    Data Structures with Near-Data-Processing: An Architecture-Aware Implementation.
    In The 31st ACM Symposium on Parallelism in Algorithms and Architectures (New
    York, NY, USA, 2019), SPAA ’19, Association for Computing Machinery, p. 297–308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(55) Choquette, J. NVIDIA Hopper H100 GPU: Scaling Performance. IEEE Micro
    (2023), 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(56) Chowdhury, R., Silvestri, F., and Vella, F. Algorithm Design for Tensor
    Units. In Euro-Par 2021: Parallel Processing (2021), L. Sousa, N. Roma, and P. Tomás,
    Eds., Springer International Publishing, pp. 353–367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (57) CIRCT Developers. CIRCT / Circuit IR Compilers and Tools, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(58) Cole, M. Algorithmic Skeletons: Structured Management of Parallel Computation.
    MIT Press, Cambridge, MA, USA, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(59) Cong, J., Lau, J., Liu, G., Neuendorffer, S., Pan, P., Vissers, K., and
    Zhang, Z. FPGA HLS Today: Successes, Challenges, and Opportunities. ACM Trans.
    Reconfigurable Technol. Syst. 15, 4 (aug 2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(60) Conti, F., Rossi, D., Pullini, A., Loi, I., and Benini, L. PULP: A Ultra-Low
    Power Parallel Accelerator for Energy-Efficient and Flexible Embedded Vision.
    Journal of Signal Processing Systems 84, 3 (2016), 339–354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(61) Da Silva, B., Braeken, A., D’Hollander, E. H., and Touhafi, A. Performance
    modeling for FPGAs: extending the roofline model with high-level synthesis tools.
    International Journal of Reconfigurable Computing 2013 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (62) Dakkak, A., Li, C., Xiong, J., Gelado, I., and Hwu, W.-M. Accelerating
    Reduction and Scan Using Tensor Core Units. In Proc. Int. Conf. Supercomputing
    (ICS) (2019), pp. 46–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (63) D’Ambra, P., Durastante, F., and Filippone, S. AMG Preconditioners for
    Linear Solvers towards Extreme Scale. SIAM Journal on Scientific Computing 43,
    5 (2021), S679–S703.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (64) Danelutto, M., Mencagli, G., Torquati, M., González-Vélez, H., and Kilpatrick,
    P. Algorithmic Skeletons and Parallel Design Patterns in Mainstream Parallel Programming.
    Int. J. Parallel Program. 49, 2 (2021), 177–198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(65) Dave, S., Kim, Y., Avancha, S., Lee, K., and Shrivastava, A. Dmazerunner:
    Executing perfectly nested loops on dataflow accelerators. ACM Transactions on
    Embedded Computing Systems (TECS) 18, 5s (2019), 1–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(66) David, R., Duke, J., Jain, A., Reddi, V. J., Jeffries, N., Li, J., Kreeger,
    N., Nappier, I., Natraj, M., Regev, S., et al. TensorFlow Lite Micro: Embedded
    Machine Learning on TinyML Systems. arXiv preprint arXiv:2010.08678 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (67) Davis, E. C., Strout, M. M., and Olschanowsky, C. Transforming Loop Chains
    via Macro Dataflow Graphs. In Proceedings of the 2018 International Symposium
    on Code Generation and Optimization (New York, NY, USA, 2018), CGO 2018, Association
    for Computing Machinery, p. 265–277.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (68) de Dinechin, F., et al. Designing Custom Arithmetic Data Paths with FloPoCo.
    IEEE Design & Test of Computers 28, 4 (July 2011), 18–27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(69) Demmel, J. Communication avoiding algorithms. In 2012 SC Companion: High
    Performance Computing, Networking Storage and Analysis (2012), pp. 1942–2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (70) Di Meo, G., Saggese, G., Strollo, A. G. M., and De Caro, D. Design of Generalized
    Enhanced Static Segment Multiplier with Minimum Mean Square Error for Uniform
    and Nonuniform Input Distributions. Electronics 12, 2 (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (71) Dongarra, J. J., Du Croz, J., Hammarling, S., and Duff, I. S. A Set of
    Level 3 Basic Linear Algebra Subprograms. ACM Trans. Math. Softw. 16, 1 (mar 1990),
    1–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (72) Dongarra, J. J., and Walker, D. W. Software Libraries for Linear Algebra
    Computations on High Performance Computers. SIAM Review 37, 2 (1995), 151–180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (73) Duarte, J., Han, S., Harris, P., Jindariani, S., Kreinar, E., Kreis, B.,
    Ngadiuba, J., Pierini, M., Tran, N., and Wu, Z. Fast inference of deep neural
    networks in FPGAs for particle physics. Journal of Instrumentation 13, 07 (2018),
    P07027.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (74) D’Ambra, P., Durastante, F., and Filippone, S. Parallel Sparse Computation
    Toolkit. Software Impacts 15 (2023), 100463.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (75) Elgohary, A., Boehm, M., Haas, P. J., Reiss, F. R., and Reinwald, B. Compressed
    Linear Algebra for Large-Scale Machine Learning. Proc. VLDB Endow. 9, 12 (aug
    2016), 960–971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (76) Escobar, F. A., Chang, X., and Valderrama, C. Suitability Analysis of FPGAs
    for Heterogeneous Platforms in HPC. IEEE Transactions on Parallel and Distributed
    Systems 27, 2 (2016), 600–612.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (77) Esposito, D., Strollo, A. G. M., and Alioto, M. Low-power approximate MAC
    unit. In 2017 13th Conference on Ph.D. Research in Microelectronics and Electronics
    (PRIME) (2017), pp. 81–84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(78) Esposito, D., Strollo, A. G. M., Napoli, E., De Caro, D., and Petra, N.
    Approximate Multipliers Based on New Approximate Compressors. IEEE Transactions
    on Circuits and Systems I: Regular Papers 65, 12 (2018), 4169–4182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(79) Fan, S., Rong, Y., Meng, C., Cao, Z., Wang, S., Zheng, Z., Wu, C., Long,
    G., Yang, J., Xia, L., Diao, L., Liu, X., and Lin, W. DAPPLE: A Pipelined Data
    Parallel Approach for Training Large Models. In Proceedings of the 26th ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming (New York, NY, USA,
    2021), PPoPP ’21, ACM, pp. 431–445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (80) Farshchi, F., Abrishami, M. S., and Fakhraie, S. M. New approximate multiplier
    for low power digital signal processing. In The 17th CSI International Symposium
    on Computer Architecture & Digital Systems (CADS 2013) (2013), pp. 25–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(81) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an Open-Source
    Research Framework for the High-Level Synthesis of Complex Applications. In 2021
    58th ACM/IEEE Design Automation Conference (DAC) (Dec 2021), IEEE, pp. 1327–1330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (82) Filippone, S., Cardellini, V., Barbieri, D., and Fanfarillo, A. Sparse
    Matrix-Vector Multiplication on GPGPUs. ACM Trans. Math. Softw. 43, 4 (jan 2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(83) Flamand, E., Rossi, D., Conti, F., Loi, I., Pullini, A., Rotenberg, F.,
    and Benini, L. GAP-8: A RISC-V SoC for AI at the Edge of the IoT. In 2018 IEEE
    29th International Conference on Application-specific Systems, Architectures and
    Processors (ASAP) (2018), IEEE, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (84) Flegar, G., Anzt, H., Cojean, T., and Quintana-Ortí, E. S. Adaptive Precision
    Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra
    Software. ACM Trans. Math. Softw. 47, 2 (apr 2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (85) Fowers, J., Ovtcharov, K., Papamichael, M. K., Massengill, T., Liu, M.,
    Lo, D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P.,
    Sapek, A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M.,
    Chung, E. S., and Burger, D. Inside Project Brainwave’s Cloud-Scale, Real-Time
    AI Processor. IEEE Micro 39, 3 (2019), 20–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (86) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Energy-Quality
    Scalable Adders Based on Nonzeroing Bit Truncation. IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems PP (12 2018), 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(87) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Approximate Multipliers
    With Dynamic Truncation for Energy Reduction via Graceful Quality Degradation.
    IEEE Transactions on Circuits and Systems II: Express Briefs PP (06 2020), 1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(88) Gale, T., Zaharia, M., Young, C., and Elsen, E. Sparse GPU Kernels for
    Deep Learning. In SC20: International Conference for High Performance Computing,
    Networking, Storage and Analysis (2020), pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (89) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., and Wang, Y. A Systematic
    Survey of General Sparse Matrix-Matrix Multiplication. ACM Comput. Surv. 55, 12
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (90) Gates, M., YarKhan, A., Sukkari, D., Akbudak, K., Cayrols, S., Bielich,
    D., Abdelfattah, A., Farhan, M. A., and Dongarra, J. Portable and Efficient Dense
    Linear Algebra in the Beginning of the Exascale Era. In 2022 IEEE/ACM International
    Workshop on Performance, Portability and Productivity in HPC (P3HPC) (Nov 2022),
    pp. 36–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(91) Geiger, L., and Team, P. Larq: An Open-Source Library for Training Binarized
    Neural Networks. Journal of Open Source Software 5, 45 (Jan. 2020), 1746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(92) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., and Shao, Y. S. Gemmini:
    Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC) (2021), pp. 769–774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(93) Ghosh, S., Raha, A., and Raghunathan, V. Approximate inference systems
    (AxIS): end-to-end approximations for energy-efficient inference at the edge.
    In ISLPED ’20: Proceedings of the ACM/IEEE International Symposium on Low Power
    Electronics and Design (08 2020), pp. 7–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(94) González-Vélez, H., and Leyton, M. A Survey of Algorithmic Skeleton Frameworks:
    High-Level Structured Parallel Programming Enablers. Softw. Pract. Exper. 40,
    12 (nov 2010), 1135–1160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (95) Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. Adaptive computation
    and machine learning. MIT Press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (96) Graham, R. L. Bounds for certain multiprocessing anomalies. The Bell System
    Technical Journal 45, 9 (1966), 1563–1581.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (97) GreenWaves Technologies. GAP8 Auto-tiler Manual. [https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html](https://greenwaves-technologies.com/manuals/BUILD/AUTOTILER/html/index.html),
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(98) Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. In Proceedings
    of the 4th International Conference on Learning Representations (2016), ICLR ’16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (99) Haoxing Ren, J. H., Ed. Machine Learning Applications in Electronic Design
    Automation, 1st. ed. Mathematics and Statistics, Mathematics and Statistics (R0).
    Springer Cham, January 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(100) Hashemi, S., Bahar, R. I., and Reda, S. DRUM: A Dynamic Range Unbiased
    Multiplier for approximate applications. In 2015 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD) (2015), pp. 418–425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (101) Hashemi, S., Bahar, R. I., and Reda, S. A low-power dynamic divider for
    approximate applications. In 2016 53nd ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2016), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (102) Hassanpour, M., Riera, M., and González, A. A Survey of Near-Data Processing
    Architectures for Neural Networks. Machine Learning and Knowledge Extraction 4,
    1 (2022), 66–102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(103) Herault, T., Schuchart, J., Valeev, E. F., and Bosilca, G. Composition
    of Algorithmic Building Blocks in Template Task Graphs. In 2022 IEEE/ACM Parallel
    Applications Workshop: Alternatives To MPI+X (PAW-ATM) (2022), pp. 26–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (104) Heroux, M. A., Bartlett, R. A., Howle, V. E., Hoekstra, R. J., Hu, J. J.,
    Kolda, T. G., Lehoucq, R. B., Long, K. R., Pawlowski, R. P., Phipps, E. T., Salinger,
    A. G., Thornquist, H. K., Tuminaro, R. S., Willenbring, J. M., Williams, A., and
    Stanley, K. S. An Overview of the Trilinos Project. ACM Trans. Math. Softw. 31,
    3 (sep 2005), 397–423.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (105) Higham, N. J., and Mary, T. Mixed precision algorithms in numerical linear
    algebra. Acta Numerica 31 (2022), 347–414.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(106) Huang, G., Hu, J., He, Y., Liu, J., Ma, M., Shen, Z., Wu, J., Xu, Y.,
    Zhang, H., Zhong, K., Ning, X., Ma, Y., Yang, H., Yu, B., Yang, H., and Wang,
    Y. Machine learning for electronic design automation: A survey. ACM Trans. Des.
    Autom. Electron. Syst. 26, 5 (jun 2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(107) Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee,
    H., Ngiam, J., Le, Q. V., Wu, Y., and Chen, Z. GPipe: Efficient Training of Giant
    Neural Networks Using Pipeline Parallelism. In Proceedings of the 33rd International
    Conference on Neural Information Processing Systems (Red Hook, NY, USA, 2019),
    NeurIPS ’19, Curran Associates Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (108) IEEE. IEEE Draft Recommended Practice for Encryption and Management of
    Electronic Design Intellectual Property (IP) , February 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(109) Imani, M., Garcia, R., Huang, A., and Rosing, T. CADE: Configurable Approximate
    Divider for Energy Efficiency. In 2019 Design, Automation & Test in Europe Conference
    & Exhibition (DATE) (2019), pp. 586–589.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(110) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., and
    Marculescu, D. QADAM: Quantization-Aware DNN Accelerator Modeling for Pareto-Optimality,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(111) Inci, A., Virupaksha, S. G., Jain, A., Thallam, V. V., Ding, R., and
    Marculescu, D. QAPPA: Quantization-Aware Power, Performance, and Area Modeling
    of DNN Accelerators, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (112) Intel. oneAPI Programming Model. [https://www.oneapi.io](https://www.oneapi.io),
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (113) Intel. Intel® High Level Synthesis Compiler Reference Manual, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(114) Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler, T. Data Movement
    Is All You Need: A Case Study of Transformer Networks. arXiv preprint arXiv:2007.00072
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (115) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam,
    H., and Kalenichenko, D. Quantization and Training of Neural Networks for Efficient
    Integer-arithmetic-only Inference. In Proceedings of the 2018 IEEE Conference
    on Computer Vision and Pattern Recognition (2018), pp. 2704–2713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(116) Ji, Z. HNMTP Conv: Optimize Convolution Algorithm for Single-Image Convolution
    Neural Network Inference on Mobile GPUs. CoRR abs/1909.02765 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (117) Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring Hidden Dimensions
    in Parallelizing Convolutional Neural Networks. In Proceedings of the 35th International
    Conference on Machine Learning (2018), J. G. Dy and A. Krause, Eds., vol. 80 of
    ICML ’18, PMLR, pp. 2279–2288.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (118) Jia, Z., Zaharia, M., and Aiken, A. Beyond Data and Model Parallelism
    for Deep Neural Networks. In Proceedings of Machine Learning and Systems 1 (2019),
    vol. 1 of MLSys ’19, pp. 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(119) Jiang, H., Santiago, F. J. H., Mo, H., Liu, L., and Han, J. Approximate
    Arithmetic Circuits: A Survey, Characterization, and Recent Applications. Proceedings
    of the IEEE 108, 12 (2020), 2108–2135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (120) Jiang, W., Yang, L., Sha, E. H.-M., Zhuge, Q., Gu, S., Dasgupta, S., Shi,
    Y., and Hu, J. Hardware/software co-exploration of neural architectures. IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 12
    (2020), 4805–4815.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (121) Jin, Q., Yang, L., and Liao, Z. Towards Efficient Training for Neural
    Network Quantization. arXiv preprint arXiv:1912.10207 abs/1912.10207 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (122) Jin, T., Bercea, G.-T., Le, T. D., Chen, T., Su, G., Imai, H., Negishi,
    Y., Leu, A., O’Brien, K., Kawachiya, K., and Eichenberger, A. E. Compiling ONNX
    Neural Network Models Using MLIR, Sept. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(123) Jin, Z., Finkel, H., Yoshii, K., and Cappello, F. Evaluation of a Floating-Point
    Intensive Kernel on FPGA. In Euro-Par 2017: Parallel Processing Workshops (2018),
    pp. 664–675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(124) Juracy, L. R., de Morais Amory, A., and Moraes, F. G. A Fast, Accurate,
    and Comprehensive PPA Estimation of Convolutional Hardware Accelerators. IEEE
    Transactions on Circuits and Systems I: Regular Papers 69, 12 (2022), 5171–5184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(125) Kahng, A. B. Machine learning for CAD/EDA: The road ahead. IEEE Design
    & Test 40, 1 (2023), 8–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (126) Kang, H., Gibbons, P. B., Blelloch, G. E., Dhulipala, L., Gu, Y., and
    McGuffey, C. The Processing-in-Memory Model. In Proceedings of the 33rd ACM Symposium
    on Parallelism in Algorithms and Architectures (New York, NY, USA, 2021), SPAA
    ’21, Association for Computing Machinery, p. 295–306.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(127) Kang, H., Zhao, Y., Blelloch, G. E., Dhulipala, L., Gu, Y., McGuffey,
    C., and Gibbons, P. B. PIM-tree: A Skew-resistant Index for Processing-in-Memory.
    arXiv preprint arXiv:2211.10516 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (128) Kernighan, B. W., and Lin, S. An efficient heuristic procedure for partitioning
    graphs. The Bell System Technical Journal 49, 2 (1970), 291–307.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (129) Kim, M. S., Del Barrio, A. A., Kim, H., and Bagherzadeh, N. The Effects
    of Approximate Multiplication on Convolutional Neural Networks. IEEE Transactions
    on Emerging Topics in Computing 10, 2 (2022), 904–916.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (130) Kim, S., Wang, J., Seo, Y., Lee, S., Park, Y., Park, S., and Park, C. S.
    Transaction-level Model Simulator for Communication-Limited Accelerators, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (131) Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classification
    with Deep Convolutional Neural Networks. In Proceedings of the 25th International
    Conference on Neural Information Processing Systems - Volume 1 (2012), NIPS’12,
    Curran Associates Inc., p. 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(132) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., and Parashar,
    A. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware
    Cost of DNN Mappings. IEEE Micro 40, 3 (2020), 20–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(133) Kwon, H., Chatarasi, P., Sarkar, V., Krishna, T., Pellauer, M., and Parashar,
    A. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware
    Cost of DNN Mappings. IEEE Micro 40, 3 (2020), 20–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(134) Labini, P. S., Bernaschi, M., Nutt, W., Silvestri, F., and Vella, F.
    Blocking Sparse Matrices to Leverage Dense-Specific Multiplication. In 2022 IEEE/ACM
    Workshop on Irregular Applications: Architectures and Algorithms (IA3) (2022),
    pp. 19–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(135) Lammie, C., Xiang, W., Linares-Barranco, B., and Azghadi, M. R. MemTorch:
    An Open-source Simulation Framework for Memristive Deep Learning Systems. Neurocomputing
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(136) Lant, J., Navaridas, J., Luján, M., and Goodacre, J. Toward FPGA-Based
    HPC: Advancing Interconnect Technologies. IEEE Micro 40, 1 (2020), 25–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(137) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In IEEE/ACM International Symposium
    on Code Generation and Optimization (CGO) (2021), pp. 2–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(138) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In 2021 IEEE/ACM International
    Symposium on Code Generation and Optimization (CGO) (Feb. 2021), pp. 2–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (139) Lattuada, M., and Ferrandi, F. Code Transformations Based on Speculative
    SDC Scheduling. In IEEE/ACM International Conference on Computer-Aided Design
    (Nov 2015), ICCAD ’15, pp. 71–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(140) Lawler, E. L., Lenstra, J. K., Rinnooy Kan, A. H., and Shmoys, D. B.
    Chapter 9 Sequencing and scheduling: Algorithms and complexity. In Logistics of
    Production and Inventory, vol. 4 of Handbooks in Operations Research and Management
    Science. Elsevier, 1993, pp. 445–522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (141) Li, B., Samsi, S., Gadepally, V., and Tiwari, D. Green Carbon Footprint
    for Model Inference Serving via Exploiting Mixed-Quality Models and GPU Partitioning.
    CoRR abs/2304.09781 (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(142) Lin, W.-F., Tsai, D.-Y., Tang, L., Hsieh, C.-T., Chou, C.-Y., Chang,
    P.-H., and Hsu, L. ONNC: A Compilation Framework Connecting ONNX to Proprietary
    Deep Learning Accelerators. In 2019 IEEE International Conference on Artificial
    Intelligence Circuits and Systems (AICAS) (Mar. 2019), pp. 214–218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (143) Liu, Z., Calciu, I., Herlihy, M., and Mutlu, O. Concurrent Data Structures
    for Near-Memory Computing. In Proceedings of the 29th ACM Symposium on Parallelism
    in Algorithms and Architectures (New York, NY, USA, 2017), SPAA ’17, Association
    for Computing Machinery, p. 235–245.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (144) Lu, T., Chen, Y.-F., Hechtman, B., Wang, T., and Anderson, J. Large-Scale
    Discrete Fourier Transform on TPUs. IEEE Access 9 (2021), 93422–93432.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (145) Mathieu, M., Henaff, M., and LeCun, Y. Fast Training of Convolutional
    Networks through FFTs, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(146) Mayer, R., and Jacobsen, H.-A. Scalable Deep Learning on Distributed
    Infrastructures: Challenges, Techniques, and Tools. ACM Comput. Surv. 53, 1 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(147) Mayer, R., Mayer, C., and Laich, L. The Tensorflow Partitioning and Scheduling
    Problem: It’s the Critical Path! In Proceedings of the 1st Workshop on Distributed
    Infrastructures for Deep Learning (New York, NY, USA, 2017), DIDL ’17, ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (148) MehriDehnavi, M., El-Kurdi, Y., Demmel, J., and Giannacopoulos, D. Communication-Avoiding
    Krylov Techniques on Graphic Processing Units. IEEE Transactions on Magnetics
    49, 5 (2013), 1749–1752.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (149) Meyer, M., Kenter, T., and Plessl, C. Evaluating FPGA Accelerator Performance
    with a Parameterized OpenCL Adaptation of Selected Benchmarks of the HPCChallenge
    Benchmark Suite. In 2020 IEEE/ACM International Workshop on Heterogeneous High-performance
    Reconfigurable Computing (H2RC) (2020), pp. 10–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (150) Microchip. Smart High-Level Synthesis Tool Suite. [https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler](https://www.microchip.com/en-us/products/fpgas-and-plds/fpga-and-soc-design-tools/smarthls-compiler),
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (151) Minutoli, M., et al. Inter-procedural resource sharing in High Level Synthesis
    through function proxies. In International Conference on Field Programmable Logic
    and Applications, FPL (Sept 2015), pp. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (152) Mirhoseini, A., Goldie, A., Pham, H., Steiner, B., Le, Q. V., and Dean,
    J. A Hierarchical Model for Device Placement. In Proceedings of Machine Learning
    and Systems (2018), MLSys ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (153) Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y.,
    Kumar, N., Norouzi, M., Bengio, S., and Dean, J. Device Placement Optimization
    with Reinforcement Learning. In Proceedings of the 34th International Conference
    on Machine Learning - Volume 70 (2017), ICML ’17, JMLR.org, pp. 2430–2439.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (154) Moreau, T., Chen, T., Vega, L., Roesch, J., Yan, E., Zheng, L., Fromm,
    J., Jiang, Z., Ceze, L., Guestrin, C., et al. A hardware–software blueprint for
    flexible deep learning specialization. Micro (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(155) ”msr fiddle”. PipeDream: Pipeline Parallelism for DNN Training, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (156) Muralidharan, S., O’Brien, K., and Lalanne, C. A Semi-Automated Tool Flow
    for Roofline Anaylsis of OpenCL Kernels on Accelerators. In First International
    Workshop on Heterogeneous High-performance Reconfigurable Computing (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(157) Murray, R., Demmel, J., Mahoney, M. W., Erichson, N. B., Melnichenko,
    M., Malik, O. A., Grigori, L., Luszczek, P., Dereziński, M., Lopes, M. E., Liang,
    T., Luo, H., and Dongarra, J. Randomized Numerical Linear Algebra : A Perspective
    on the Field With an Eye to Software, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(158) Muñoz-Martínez, F., Abellán, J. L., Acacio, M. E., and Krishna, T. STONNE:
    A Detailed Architectural Simulator for Flexible Neural Network Accelerators, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(159) Nabi, S. W., and Vanderbauwhede, W. MP-STREAM: A Memory Performance Benchmark
    for Design Space Exploration on Heterogeneous HPC Devices. In 2018 IEEE International
    Parallel and Distributed Processing Symposium Workshops (IPDPSW) (2018), pp. 194–197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(160) Nagasu, K., Sano, K., Kono, F., and Nakasato, N. FPGA-based tsunami simulation:
    Performance comparison with GPUs, and roofline model for scalability analysis.
    Journal of Parallel and Distributed Computing 106 (2017), 153–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (161) Nane, R., Sima, V., Pilato, C., Choi, J., Fort, B., Canis, A., Chen, Y. T.,
    Hsiao, H., Brown, S., Ferrandi, F., Anderson, J., and Bertels, K. A Survey and
    Evaluation of FPGA High-Level Synthesis Tools. IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems 35, 10 (Oct 2016), 1591–1604.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (162) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., and Kim, N. S.
    Energy-Efficient Approximate Multiplication for Digital Signal Processing and
    Classification Applications. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 23, 6 (2015), 1180–1184.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(163) Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R.,
    Ganger, G. R., Gibbons, P. B., and Zaharia, M. PipeDream: Generalized Pipeline
    Parallelism for DNN Training. In Proceedings of the 27th ACM Symposium on Operating
    Systems Principles (New York, NY, USA, 2019), SOSP ’19, ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(164) Nguyen, T., MacLean, C., Siracusa, M., Doerfler, D., Wright, N. J., and
    Williams, S. FPGA-based HPC accelerators: An evaluation on performance and energy
    efficiency. Concurrency and Computation: Practice and Experience n/a, n/a (2021),
    e6570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (165) Nguyen, T., Williams, S., Siracusa, M., MacLean, C., Doerfler, D., and
    Wright, N. J. The Performance and Energy Efficiency Potential of FPGAs in Scientific
    Computing. In 2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation
    of High Performance Computer Systems (PMBS) (2020), pp. 8–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (166) Nurvitadhi, E., Mishra, A., and Marr, D. A sparse matrix vector multiply
    accelerator for support vector machine. In 2015 International Conference on Compilers,
    Architecture and Synthesis for Embedded Systems (CASES) (2015), pp. 109–116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(167) NVIDIA. NVIDIA Deep Learning Accelerator. [http://nvdla.org/](http://nvdla.org/),
    2022. Accessed: 2023-04-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (168) PAI”, A. G. DAPPLE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (169) Paliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli, P., and Vinyals,
    O. Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs. In
    Proceedings of the 8th International Conference on Learning Representations (2020),
    ICLR ’20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (170) Papadimitriou, C. H., and Yannakakis, M. Towards an Architecture-Independent
    Analysis of Parallel Algorithms. SIAM J. Comput. 19, 2 (1990), 322–328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(171) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop: A systematic
    approach to dnn accelerator evaluation. In 2019 IEEE international symposium on
    performance analysis of systems and software (ISPASS) (2019), IEEE, pp. 304–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(172) Parashar, A., Raina, P., Shao, Y. S., Chen, Y.-H., Ying, V. A., Mukkara,
    A., Venkatesan, R., Khailany, B., Keckler, S. W., and Emer, J. Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation. In 2019 IEEE International Symposium on
    Performance Analysis of Systems and Software (ISPASS) (2019), pp. 304–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (173) Pei, Y., Bosilca, G., and Dongarra, J. Sequential Task Flow Runtime Model
    Improvements and Limitations. In 2022 IEEE/ACM International Workshop on Runtime
    and Operating Systems for Supercomputers (ROSS) (Nov 2022), pp. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(174) Peng, X., Huang, S., Luo, Y., Sun, X., and Yu, S. DNN+NeuroSim: An End-to-End
    Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device
    Technologies. In 2019 IEEE International Electron Devices Meeting (IEDM) (2019),
    pp. 32.5.1–32.5.4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (175) Perri, S., Spagnolo, F., Frustaci, F., and Corsonello, P. Efficient Approximate
    Adders for FPGA-Based Data-Paths. Electronics 9, 9 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (176) Perri, S., Spagnolo, F., Frustaci, F., and Corsonello, P. Designing Energy-Efficient
    Approximate Multipliers. Journal of Low Power Electronics and Applications 12,
    4 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(177) Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P., Shyu,
    M.-L., Chen, S.-C., and Iyengar, S. S. A survey on deep learning: Algorithms,
    techniques, and applications. ACM Comput. Surv. 51, 5 (sep 2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(178) Prabakaran, B. S., Rehman, S., Hanif, M. A., Ullah, S., Mazaheri, G.,
    Kumar, A., and Shafique, M. DeMAS: An efficient design methodology for building
    approximate adders for FPGA-based systems. In 2018 Design, Automation & Test in
    Europe Conference & Exhibition (DATE) (2018), pp. 917–920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (179) Psarras, C., Barthels, H., and Bientinesi, P. The Linear Algebra Mapping
    Problem. Current State of Linear Algebra Languages and Libraries. ACM Trans. Math.
    Softw. 48, 3 (sep 2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (180) Putnam, A., Caulfield, A. M., Chung, E. S., Chiou, D., Constantinides,
    K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G. P., Gray, J., Haselman,
    M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson,
    E., Pope, S., Smith, A., Thong, J., Xiao, P. Y., and Burger, D. A Reconfigurable
    Fabric for Accelerating Large-Scale Datacenter Services. IEEE Micro 35, 3 (2015),
    10–22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (181) Quintana-Ortí, G., Quintana-Ortí, E. S., Geijn, R. A. V. D., Zee, F. G. V.,
    and Chan, E. Programming Matrix Algorithms-by-Blocks for Thread-Level Parallelism.
    ACM Trans. Math. Softw. 36, 3 (jul 2009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(182) Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., and Amarasinghe,
    S. Halide: a language and compiler for optimizing parallelism, locality, and recomputation
    in image processing pipelines. Acm Sigplan Notices 48, 6 (2013), 519–530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(183) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., and Roy, K. Exploring Neuromorphic Computing Based on Spiking Neural Networks:
    Algorithms to Hardware. ACM Comput. Surv. 55, 12 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (184) Reiser, D., Reichenbach, M., Rizzi, T., Baroni, A., Fritscher, M., Wenger,
    C., Zambelli, C., and Bertozzi, D. Technology-Aware Drift Resilience Analysis
    of RRAM Crossbar Array Configurations. In 2023 IEEE NEWCAS (2023), p. in press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(185) Roesch, J., Lyubomirsky, S., Weber, L., Pollock, J., Kirisame, M., Chen,
    T., and Tatlock, Z. Relay: A New IR for Machine Learning Frameworks. In Proceedings
    of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages (New York, NY, USA, June 2018), MAPL 2018, Association for Computing
    Machinery, pp. 58–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (186) Ronak, B., and Fahmy, S. A. Mapping for Maximum Performance on FPGA DSP
    Blocks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
    Systems 35, 4 (April 2016), 573–585.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(187) Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S., Dzhabarov,
    R., Gibson, N., Hegeman, J., Lele, M., Levenstein, R., Montgomery, J., Maher,
    B., Nadathur, S., Olesen, J., Park, J., Rakhov, A., Smelyanskiy, M., and Wang,
    M. Glow: Graph Lowering Compiler Techniques for Neural Networks. arXiv:1805.00907
    [cs] (Apr. 2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(188) Russo, E., Palesi, M., Monteleone, S., Patti, D., Ascia, G., and Catania,
    V. LAMBDA: An Open Framework for Deep Neural Network Accelerators Simulation.
    In 2021 IEEE International Conference on Pervasive Computing and Communications
    Workshops and other Affiliated Events (PerCom Workshops) (2021), pp. 161–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (189) Russo Russo, G., Cardellini, V., and Lo Presti, F. Hierarchical Auto-Scaling
    Policies for Data Stream Processing on Heterogeneous Resources. ACM Trans. Auton.
    Adapt. Syst. (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (190) Saadat, H., Javaid, H., and Parameswaran, S. Approximate Integer and Floating-Point
    Dividers with Near-Zero Error Bias. In 2019 56th ACM/IEEE Design Automation Conference
    (DAC) (06 2019), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(191) Saleem, R., Yuan, B., Kurugollu, F., Anjum, A., and Liu, L. Explaining
    deep neural networks: A survey on the global interpretation methods. Neurocomputing
    513 (2022), 165–180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (192) Samajdar, A., Joseph, J. M., Zhu, Y., Whatmough, P., Mattina, M., and
    Krishna, T. A systematic methodology for characterizing scalability of DNN accelerators
    using SCALE-sim. In 2020 IEEE International Symposium on Performance Analysis
    of Systems and Software (ISPASS) (2020), IEEE, pp. 58–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(193) Samajdar, A., Zhu, Y., Whatmough, P., Mattina, M., and Krishna, T. SCALE-Sim:
    Systolic CNN Accelerator Simulator. arXiv preprint arXiv:1811.02883 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(194) Sayal, A., Fathima, S., Nibhanupudi, S., and Kulkarni, J. COMPAC: Compressed
    Time-Domain, Pooling-Aware Convolution CNN Engine With Reduced Data Movement for
    Energy-Efficient AI Computing. IEEE Journal of Solid-State Circuits PP (12 2020),
    1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(195) Schatz, M. D., van de Geijn, R. A., and Poulson, J. Parallel Matrix Multiplication:
    A Systematic Journey. SIAM J. Sci. Comput. 38, 6 (jan 2016), C748–C781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(196) Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun,
    Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional
    Networks, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (197) Sestito C., Perri S., S. R. FPGA Design of Transposed Convolutions for
    Deep Learning Using High-Level Synthesis. Journal of Signal Processing Systems
    (08 2023), 1–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(198) Shadmehri, S. H. H., BanaGozar, A., Kamal, M., Stuijk, S., Afzali-Kusha,
    A., Pedram, M., and Corporaal, H. SySCIM: SystemC-AMS Simulation of Memristive
    Computation In-Memory. In 2022 Design, Automation and Test in Europe Conference
    and Exhibition (DATE) (2022), pp. 1467–1472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(199) Shao, Y. S., Reagen, B., Wei, G.-Y., and Brooks, D. Aladdin: A pre-RTL,
    power-performance accelerator simulator enabling large design space exploration
    of customized architectures. In 2014 ACM/IEEE 41st International Symposium on
    Computer Architecture (ISCA) (2014), pp. 97–108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(200) Shawahna, A., Sait, S. M., and El-Maleh, A. FPGA-Based Accelerators of
    Deep Learning Networks for Learning and Classification: A Review. IEEE Access
    7 (2019), 7823–7859.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (201) Shmoys, D. B., and Tardos, É. An Approximation Algorithm for the Generalized
    Assignment Problem. Mathematical Programming 62, 1 (1993), 461–474.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (202) Siemens. Catapult C++/Systemc Synthesis, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (203) Silvano, C., Ielmini, D., Ferrandi, F., Fiorin, L., Curzel, S., Benini,
    L., Conti, F., Garofalo, A., Zambelli, C., Calore, E., Schifano, S. F., Palesi,
    M., Ascia, G., Patti, D., Perri, S., Petra, N., Caro, D. D., Lavagno, L., Urso,
    T., Cardellini, V., Cardarilli, G. C., and Birke, R. A survey on deep learning
    hardware accelerators for heterogeneous hpc platforms, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (204) Siracusa, M., Delsozzo, E., Rabozzi, M., Di Tucci, L., Williams, S., Sciuto,
    D., and Santambrogio, M. D. A Comprehensive Methodology to Optimize FPGA Designs
    via the Roofline Model. IEEE Transactions on Computers (2021), 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (205) Siracusa, M., Rabozzi, M., Del Sozzo, E., Di Tucci, L., Williams, S.,
    and Santambrogio, M. D. A CAD-based methodology to optimize HLS code via the Roofline
    model. In 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)
    (2020), pp. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (206) Skutella, M., and Woeginger, G. J. A PTAS for Minimizing the Weighted
    Sum of Job Completion Times on Parallel Machines. In Proceedings of the 31 Annual
    ACM Symposium on Theory of Computing (New York, NY, USA, 1999), STOC ’99, ACM,
    pp. 400–407.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (207) Song, M.-A., Van, L.-D., and Kuo, S.-Y. Adaptive Low-Error Fixed-Width
    Booth Multipliers. IEICE Transactions on Fundamentals of Electronics, Communications
    and Computer Sciences E90-A (06 2007).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (208) Sorna, A., Cheng, X., D’Azevedo, E., Won, K., and Tomov, S. Optimizing
    the Fast Fourier Transform Using Mixed Precision on Tensor Core Hardware. In Proc.
    25th Int. Conf. on High Performance Computing Workshops (HiPCW) (2018), pp. 3–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (209) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of a
    Low-Power Super-Resolution Architecture for Virtual Reality Wearable Devices.
    IEEE Sensors Journal 23, 8 (2023), 9009–9016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (210) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of Approximate
    Bilateral Filters for Image Denoising on FPGAs. IEEE Access 11 (2023), 1990–2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(211) Spagnolo, F., Perri, S., and Corsonello, P. Aggressive Approximation
    of the SoftMax Function for Power-Efficient Hardware Implementations. IEEE Transactions
    on Circuits and Systems II: Express Briefs 69, 3 (2022), 1652–1656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (212) Spagnolo, F., Perri, S., and Corsonello, P. Approximate Down-Sampling
    Strategy for Power-Constrained Intelligent Systems. IEEE Access 10 (2022), 7073–7081.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (213) ST Microelectronics. X-CUBE-AI, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (214) Stok, L. Data path synthesis. Integration 18, 1 (1994), 1–71.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(215) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., and Meo, G. D.
    Comparison and Extension of Approximate 4-2 Compressors for Low-Power Approximate
    Multipliers. IEEE Transactions on Circuits and Systems I: Regular Papers 67, 9
    (2020), 3021–3034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(216) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., and
    Di Meo, G. Approximate Multipliers Using Static Segmentation: Error Analysis and
    Improvements. IEEE Transactions on Circuits and Systems I: Regular Papers 69,
    6 (2022), 2449–2462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (217) Stylianou, C., and Weiland, M. Optimizing Sparse Linear Algebra Through
    Automatic Format Selection and Machine Learning, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(218) Tang, T., and Xie., Y. MLPAT: A power area timing modeling framework
    for machine learning accelerators. In EEE International Workshop on Domain Specific
    System Architecture (DOSSA) (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(219) Tung, F., and Mori, G. CLIP-Q: Deep Network Compression Learning by In-parallel
    Pruning-Quantization. In Proceedings of the 2018 IEEE Conference on Computer Vision
    and Pattern Recognition (2018), IEEE, pp. 7873–7882.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (220) Ullah, S., Rehman, S., Shafique, M., and Kumar, A. High-Performance Accurate
    and Approximate Multipliers for FPGA-Based Hardware Accelerators. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems 41, 2 (2022), 211–224.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (221) Ullah, S., Schmidl, H., Sahoo, S. S., Rehman, S., and Kumar, A. Area-Optimized
    Accurate and Approximate Softcore Signed Multiplier Architectures. IEEE Transactions
    on Computers 70, 3 (2021), 384–392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (222) Urbach, M., and Petersen, M. B. HLS from PyTorch to System Verilog with
    MLIR and CIRCT, 2022. 2nd Workshop on Languages, Tools, and Techniques for Accelerator
    Design (LATTE).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(223) Van Delm, J., Vandersteegen, M., Burrello, A., Sarda, G. M., Conti, F.,
    Jahier Pagliari, D., Benini, L., and Verhelst, M. HTVM: Efficient Neural Network
    Deployment On Heterogeneous TinyML Platforms. In Proceedings of the 2023 Conference
    & Exhibition on Design, Automation & Test in Europe (Antwerp, 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (224) van Werkhoven, B., Palenstijn, W. J., and Sclocco, A. Lessons Learned
    in a Decade of Research Software Engineering GPU Applications. In Computational
    Science – ICCS 2020 (Cham, 2020), Springer, pp. 399–412.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (225) Vanderbauwhede, W., and Benkrid, K. High-performance computing using FPGAs,
    vol. 3. Springer, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(226) Vanevenhoven, T. High-Level Implementation of Bit- and Cycle-Accurate
    Floating-Point DSP Algorithms with Xilinx FPGAs. Tech. rep., Xilinx, Oct 2011.
    White Paper: 7 Series FPGAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(227) Vasilache, N., Johnson, J., Mathieu, M., Chintala, S., Piantino, S.,
    and LeCun, Y. Fast Convolutional Nets With fbfft: A GPU Performance Evaluation,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(228) Vasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., DeVito, Z., Moses,
    W. S., Verdoolaege, S., Adams, A., and Cohen, A. Tensor Comprehensions: Framework-Agnostic
    High-Performance Machine Learning Abstractions. arXiv:1802.04730 [cs] (June 2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (229) Véstias, M., and Neto, H. Trends of CPU, GPU and FPGA for high-performance
    computing. In 2014 24th International Conference on Field Programmable Logic and
    Applications (FPL) (Sep. 2014), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(230) Vitter, J. S. External Memory Algorithms and Data Structures: Dealing
    with Massive Data. ACM Comput. Surv. 33, 2 (jun 2001), 209–271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(231) Wang, Z., Huang, H., Zhang, J., and Alonso, G. Shuhai: Benchmarking High
    Bandwidth Memory On FPGAs. In 2020 IEEE 28th Annual International Symposium on
    Field-Programmable Custom Computing Machines (FCCM) (2020), pp. 111–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(232) Waris, H., Wang, C., Liu, W., and Lombardi, F. AxBMs: Approximate Radix-8
    Booth Multipliers for High-Performance FPGA-Based Accelerators. IEEE Transactions
    on Circuits and Systems II: Express Briefs 68, 5 (2021), 1566–1570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (233) Winograd, S. Arithmetic Complexity of Computations. Society for Industrial
    and Applied Mathematics, 1980.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (234) Wirthlin, M., Poznanovic, D., Sundararajan, P., Coppola, A., Pellerin,
    D., Najjar, W., Bruce, R., Babst, M., Pritchard, O., Palazzari, P., and Kuzmanov,
    G. OpenFPGA CoreLib core library interoperability effort. Parallel Computing 34
    (2008), 231–244.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(235) Wu, C., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K.,
    Chang, G., Behram, F. A., Huang, J., Bai, C., Gschwind, M., Gupta, A., Ott, M.,
    Melnikov, A., Candido, S., Brooks, D., Chauhan, G., Lee, B., Lee, H. S., Akyildiz,
    B., Balandat, M., Spisak, J., Jain, R., Rabbat, M., and Hazelwood, K. M. Sustainable
    AI: Environmental Implications, Challenges and Opportunities. In Proceedings of
    Machine Learning and Systems 2022, MLSys 2022 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(236) Wu, Y. N., Emer, J. S., and Sze, V. Accelergy: An Architecture-Level
    Energy Estimation Methodology for Accelerator Designs. In 2019 IEEE/ACM International
    Conference on Computer-Aided Design (ICCAD) (2019), pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(237) Xia, L., Li, B., Tang, T., Gu, P., Yin, X., Huangfu, W., Chen, P.-Y.,
    Yu, S., Cao, Y., Wang, Y., Xie, Y., and Yang, H. MNSIM: Simulation platform for
    memristor-based neuromorphic computing system. In 2016 Design, Automation and
    Test in Europe Conference and Exhibition (DATE) (2016), pp. 469–474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (238) Xilinx Inc. Vitis High-Level Synthesis User Guide, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(239) Xilinx Inc. Vivado Design Suite User Guide: Designing IP Subsystems Using
    IP Integrator. UG994 (v2022.2), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(240) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., et al. Interstellar: Using Halide’s Scheduling Language
    to Analyze DNN Accelerators. In Proceedings of the Twenty-Fifth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (2020), pp. 369–383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(241) Yang, X., Gao, M., Liu, Q., Setter, J., Pu, J., Nayak, A., Bell, S.,
    Cao, K., Ha, H., Raina, P., Kozyrakis, C., and Horowitz, M. Interstellar: Using
    Halide’s Scheduling Language to Analyze DNN Accelerators. In Proceedings of the
    Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems (New York, NY, USA, 2020), ASPLOS ’20, Association
    for Computing Machinery, pp. 369–383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (242) Yasudo, R., Coutinho, J., Varbanescu, A., Luk, W., Amano, H., and Becker,
    T. Performance Estimation for Exascale Reconfigurable Dataflow Platforms. In 2018
    International Conference on Field-Programmable Technology (FPT) (2018), pp. 314–317.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(243) Ye, H., Hao, C., Cheng, J., Jeong, H., Huang, J., Neuendorffer, S., and
    Chen, D. ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level
    Intermediate Representation. In 2022 IEEE International Symposium on High-Performance
    Computer Architecture (HPCA) (2022), IEEE, pp. 741–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(244) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., and Chen, D. ScaleHLS:
    A Scalable High-Level Synthesis Framework with Multi-Level Transformations and
    Optimizations. In Proceedings of the 59th ACM/IEEE Design Automation Conference
    (DAC) (2022), pp. 1355–1358.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(245) Zendegani, R., Kamal, M., Fayyazi, A., Afzali-Kusha, A., Safari, S.,
    and Pedram, M. SEERAD: A high speed yet energy-efficient rounding-based approximate
    divider. In 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)
    (2016), pp. 1481–1484.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(246) Zeni, A., O’Brien, K., Blott, M., and Santambrogio, M. D. Optimized Implementation
    of the HPCG Benchmark on Reconfigurable Hardware. In Euro-Par 2021: Parallel Processing
    (2021), L. Sousa, N. Roma, and P. Tomás, Eds., pp. 616–630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (247) Zhang, J., Franchetti, F., and Low, T. M. High Performance Zero-Memory
    Overhead Direct Convolutions. CoRR abs/1809.10170 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(248) Zhang, X., Ye, H., and Chen, D. Being-ahead: Benchmarking and Exploring
    Accelerators for Hardware-Efficient AI Deployment, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(249) Zhao, Y., Li, C., Wang, Y., Xu, P., Zhang, Y., and Lin, Y. DNN-Chip Predictor:
    An Analytical Performance Predictor for DNN Accelerators with Various Dataflows
    and Hardware Architectures, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(250) Zhou, Y., Roy, S., Abdolrashidi, A., Wong, D. L., Ma, P. C., Xu, Q.,
    Zhong, M., Liu, H., Goldie, A., Mirhoseini, A., and Laudon, J. GDP: Generalized
    Device Placement for Dataflow Graphs. CoRR abs/1910.01578 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(251) Zhu, D., Lu, S., Wang, M., Lin, J., and Wang, Z. Efficient Precision-Adjustable
    Architecture for Softmax Function in Deep Learning. IEEE Transactions on Circuits
    and Systems II: Express Briefs 67 (2020), 3382–3386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(252) Zohouri, H. R., Maruyama, N., Smith, A., Matsuda, M., and Matsuoka, S.
    Evaluating and Optimizing OpenCL Kernels for High Performance Computing with FPGAs.
    In SC ’16: Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis (Nov 2016), pp. 409–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(253) Zohouri, H. R., and Matsuoka, S. The memory controller wall: Benchmarking
    the intel FPGA SDK for OpenCL memory interface. In 2019 IEEE/ACM International
    Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC) (2019),
    IEEE, pp. 11–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (254) Zois, V., Gupta, D., Tsotras, V. J., Najjar, W. A., and Roy, J.-F. Massively
    Parallel Skyline Computation for Processing-in-Memory Architectures. In Proceedings
    of the 27th International Conference on Parallel Architectures and Compilation
    Techniques (New York, NY, USA, 2018), PACT ’18, ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
