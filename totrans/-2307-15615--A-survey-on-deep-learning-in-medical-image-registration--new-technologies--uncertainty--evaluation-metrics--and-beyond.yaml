- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2307.15615] A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.15615](https://ar5iv.labs.arxiv.org/html/2307.15615)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Junyu Chen¹¹1Contributed equally to this work. Yihao Liu²²2Contributed equally
    to this work. Shuwen Wei³³3Contributed equally to this work. Zhangxing Bian Shalini
    Subramanian Aaron Carass Jerry L. Prince Yong Du Department of Radiology and Radiological
    Science, Johns Hopkins School of Medicine, MD, USA Department of Electrical and
    Computer Engineering, Johns Hopkins University, MD, USA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning technologies have dramatically reshaped the field of medical image
    registration over the past decade. The initial developments, such as ResNet-based
    and U-Net-based networks, established the foundation for deep learning in image
    registration. Subsequent progress has been made in various aspects of deep learning-based
    registration, including similarity measures, deformation regularizations, and
    uncertainty estimation. These advancements have not only enriched the field of
    image registration but have also facilitated its application in a wide range of
    tasks, including atlas construction, multi-atlas segmentation, motion estimation,
    and 2D-3D registration. In this paper, we present a comprehensive overview of
    the most recent advancements in deep learning-based image registration. We begin
    with a concise introduction to the core concepts of deep learning-based image
    registration. Then, we delve into innovative network architectures, loss functions
    specific to registration, and methods for estimating registration uncertainty.
    Additionally, this paper explores appropriate evaluation metrics for assessing
    the performance of deep learning models in registration tasks. Finally, we highlight
    the practical applications of these novel techniques in medical imaging and discuss
    the future prospects of deep learning-based image registration.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: \KWDImage Registration, Deep Neural Networks, Medical Imaging\DeclareMathOperator
  prefs: []
  type: TYPE_NORMAL
- en: '*\argmaxarg max \DeclareMathOperator*\argminarg min'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical image registration involves estimating the optimal spatial transformation
    to align the structures of interest in a pair of fixed and moving images. The
    choice of spatial transformation depends on the specific application and can be
    categorized as either rigid/affine or non-rigid/deformable. In rigid/affine registration,
    all spatial coordinates are transformed using the same rigid/affine matrix. On
    the other hand, non-rigid/deformable registration employs independent transformations
    for individual local regions of spatial coordinates. Both types of registration
    are of great importance to many medical imaging tasks. Rigid registration is commonly
    used when the rigid body assumption holds. For example, it is used to align a
    structural scan—*e.g.*, magnetic resonance image (MRI) or computed tomography (CT)—with
    a functional scan—*e.g.*, functional magnetic resonance image (fMRI) or positron
    emission tomography (PET)—of the same patient for attenuation correction [[139](#bib.bib139)]
    or interpretation of functional activities [[309](#bib.bib309)]. On the other
    hand, deformable image registration (DIR) is often used in cases where more complex,
    spatially varying deformations are needed. Examples of such applications include
    constructing deformable templates for a patient cohort [[51](#bib.bib51), [100](#bib.bib100)]
    or registering atlases to a patient image for multi-atlas segmentation [[270](#bib.bib270),
    [30](#bib.bib30), [2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/970ecd8b4d7822b2a98b8528ba966857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Statistics of the articles investigated in this survey paper. The left
    panel displays a histogram of the number of papers by year; the vast majority
    of the surveyed papers were proposed within the last five years. The right panel
    illustrates the sources of the investigated articles, demonstrating that our survey
    draws from sources associated with the field of medical image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, image registration has been accomplished by iteratively solving
    an optimization problem (*e.g.*, demons [[323](#bib.bib323)], LDDMM [[18](#bib.bib18)],
    SyN [[9](#bib.bib9)], DARTEL [[6](#bib.bib6)], and Elastix [[176](#bib.bib176)]).
    These methods are well-established and supported by strong mathematical theory.
    However, they can be computationally expensive and slow in practice, as the optimization
    problem must be solved for each individual pair of moving and fixed images. Several
    review papers have covered traditional medical image registration methods extensively [[220](#bib.bib220),
    [135](#bib.bib135), [288](#bib.bib288), [90](#bib.bib90), [306](#bib.bib306),
    [245](#bib.bib245), [325](#bib.bib325)]. Interested readers can refer to these
    references for more information on these methods. In the last decade, deep learning-based
    methods have shown promise in improving the accuracy and efficiency of image registration.
    Unlike traditional methods, deep learning-based methods train a general network
    by optimizing a global objective function on a training dataset. Then in the testing
    phase, the trained network is directly applied to each image pair with fixed network
    weights, resulting in a significant speedup compared to traditional methods. Initially,
    ResNet-like network architectures [[122](#bib.bib122)], which consist of a convolutional
    encoder and a multilayer perceptron [[344](#bib.bib344), [226](#bib.bib226)],
    were explored. During the training process, ground truth transformations have
    to be provided for direct supervision. In rigid/affine transformations, the ground
    truth is represented as a transformation matrix; while a dense displacement field
    is often used for deformable registration. With the introduction of spatial transformer
    networks [[156](#bib.bib156)] and the success of U-Net [[279](#bib.bib279)] in
    medical imaging applications, learning-based deformable registration methods adopted
    an encoder-decoder design in either supervised [[362](#bib.bib362), [275](#bib.bib275)]
    or unsupervised [[330](#bib.bib330), [188](#bib.bib188), [14](#bib.bib14), [172](#bib.bib172),
    [37](#bib.bib37)] training schemes. These methods typically output a high-resolution
    dense deformation field. On the other hand, learning-based rigid/affine registration
    methods continue to adopt encoder-only networks [[226](#bib.bib226), [147](#bib.bib147),
    [59](#bib.bib59), [46](#bib.bib46), [37](#bib.bib37), [233](#bib.bib233)], with
    the output being the rigid or affine parameters. While there are papers that provide
    general reviews of learning-based registration methods [[95](#bib.bib95), [45](#bib.bib45),
    [348](#bib.bib348), [384](#bib.bib384)], it is important to note that these reviews
    may not be fully up-to-date due to the rapid advancement of the field of deep
    learning. Recent advancements, including learning-based similarity metrics and
    regularizers, novel network architectures, and innovative evaluation metrics and
    uncertainty estimation methods, have demonstrated promising potential for medical
    image registration. This paper provides a timely review of learning-based methods
    in medical image registration, highlighting the latest technologies that have
    been proposed and discussing their respective characteristics and applications.
    In addition, we investigate and formally define registration uncertainty for deep
    learning-based image registration and address the appropriate evaluation metrics
    for these methods that have been overlooked in previous review papers. For simplicity,
    we refer to deep learning-based methods as learning-based methods throughout the
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we surveyed over 250 articles on learning-based medical image
    registration. As depicted in Fig. [1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ A survey
    on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"), the focus is primarily on recent advancements
    proposed in the last five years. Our search covers well-established medical imaging
    journals, such as Medical Image Analysis, IEEE Transactions on Medical Imaging,
    Medical Physics, and NeuroImage, as well as conference proceedings related to
    medical imaging and image registration, such as MICCAI, IPMI, WBIR, CVPR, ECCV,
    ICCV, and NeurIPS. The remainder of the paper is organized as follows: Section [2](#S2
    "2 Fundamentals of Learning-based Image Registration ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond") offers a brief overview of the fundamentals of learning-based image
    registration. Section [3](#S3 "3 Loss Functions ‣ A survey on deep learning in
    medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond") explores widely-used loss functions for learning-based registration
    methods which resemble objective functions in traditional methods, and discusses
    other novel loss functions enabled by deep learning. Section [4](#S4 "4 Network
    Architectures ‣ A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond") investigates network architectures
    developed for medical image registration, with a focus on recent developments.
    Section [5](#S5 "5 Uncertainty in Learning-based Registration ‣ A survey on deep
    learning in medical image registration: new technologies, uncertainty, evaluation
    metrics, and beyond") delves into methods for estimating registration uncertainty
    in learning-based registration. Section [6](#S6 "6 Registration Evaluation Metrics
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond") considers appropriate evaluation metrics for
    learning-based methods and examines methods for quantifying the regularity of
    generated deformation fields. Section [7](#S7 "7 Applications of Medical Image
    Registration ‣ A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond") summarizes recent applications of
    learning-based registration in medical imaging. Finally, Section [8](#S8 "8 Challenges
    and Future Perspectives ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond") discusses current
    challenges and provides future perspectives for deep learning in medical image
    registration.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Fundamentals of Learning-based Image Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image registration aims to estimate the optimal coordinate transformation that
    minimizes an energy function of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\phi}=\operatorname*{\argmin}_{\phi}E(I_{f},I_{m}\circ\phi)+\lambda
    R(\phi),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{f}$ and $I_{m}$ denote the fixed and moving image, respectively, $\phi$
    represents the deformation field that maps $I_{m}$ to $I_{f}$, and $R$ is a functional
    of $\phi$. The first term in the energy function measures the image similarity
    between the fixed image and the transformed moving image. The second term enforces
    regularization on the deformation field, with $\lambda$ being a hyperparameter
    that determines the trade-off between image similarity and deformation field regularity.
    The purpose of the image similarity measure is to quantify the discrepancy between
    the fixed image and the transformed moving image. The regularization term is typically
    used in DIR, as it allows for the integration of prior knowledge about the desired
    characteristics of the deformation field, such as spatial smoothness. Moreover,
    regularization prevents the deformation field from exhibiting physically implausible
    behaviors, such as “folding” or rearranging of voxels [[276](#bib.bib276)]. This
    is particularly important for medical images because such unrealistic behavior
    does not accurately reflect the way that organs deform in reality and may lead
    to a misinterpretation of the registration results. Regularization is often not
    required for rigid/affine registration because the deformation field is guaranteed
    to be spatially uniform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ca9c65f73dfd24489682cf2d11dc142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Overview of learning-based image registration. The top panel depicts
    the common pipeline for supervised learning in medical image registration, which
    necessitates ground truth transformations. The bottom panel demonstrates the unsupervised
    learning pipeline, wherein the network learns to perform registration using only
    input images. The left panel presents the learning-based DIR pipeline, typically
    employing an encoder-decoder-style network architecture. The right panel exhibits
    the learning-based rigid/affine registration, which usually involves only an encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Supervised vs. Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning-based registration methods can be broadly categorized as supervised
    and unsupervised. In the machine learning paradigm, supervised learning typically
    refers to the use of extrinsic information during learning (such as labels) whereas
    unsupervised methods are concerned with discovering properties intrinsic to the
    data. Both supervised and unsupervised learning-based registration methods require
    a training stage that uses pairs of inputs and their corresponding target outputs.
    Supervised registration methods use ground truth transformations as target output
    during the training process. Unsupervised methods refer to those that do not require
    ground truth transformations. Yet, methods that employ landmark correspondences
    or anatomical label maps during their training phase are still categorized under
    supervised learning. This is because landmark correspondences are a sparse representation
    of the ground truth transformations, and matching label maps act as a surrogate
    for evaluating registration performance. When this extrinsic information is used
    alongside the image data to aim learning, these methods are referred to as semi-supervised.
    In certain contexts, the term "unsupervised" might be misleading. A more precise
    term could be “self-supervised” to underscore the training aspect of deep learning.
    However, for the purposes of clarity and consistency in this discussion, we will
    use conventional terminology and refer to methods that do not require supervision
    from extrinsic information as unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: During the early stages of development, the majority of learning-based registration
    methods were supervised. The ground truth transformations required for the training
    process are typically generated using traditional registration methods, such as
    [[362](#bib.bib362), [275](#bib.bib275), [31](#bib.bib31), [148](#bib.bib148),
    [85](#bib.bib85)]. However, generating ground-truth transformations this way is
    a time-consuming process, which is a notable drawback of such methods. In addition,
    since these networks are trained to mimic the function of traditional methods,
    their registration performance may not surpass that of the methods they are based
    on. In some cases, post-processing of the deformation fields may be required to
    further improve registration accuracy [[362](#bib.bib362)]. Alternatively, artificial
    deformations can also be used as ground truth transformations in certain cases [[226](#bib.bib226),
    [178](#bib.bib178), [303](#bib.bib303), [82](#bib.bib82), [79](#bib.bib79)].
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the introduction of spatial transformer networks [[156](#bib.bib156)]
    has led to a shift towards developing unsupervised methods that do not rely on
    ground-truth transformation [[330](#bib.bib330), [188](#bib.bib188), [59](#bib.bib59),
    [14](#bib.bib14), [56](#bib.bib56), [229](#bib.bib229), [230](#bib.bib230), [232](#bib.bib232),
    [172](#bib.bib172), [37](#bib.bib37), [38](#bib.bib38)]. These methods use the
    difference between the deformed moving image and the fixed image to update the
    network, enabling end-to-end training. By removing the reliance on ground truth
    transformation, these methods offer greater flexibility in modeling different
    properties of the deformation fields (*e.g.*, smoothness, invertibility).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Paradigm for Learning-based Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent progress in the field of learning-based medical image registration has
    been focusing on exploring different ways to improve registration accuracy, such
    as through modifications to network architectures, loss functions, and training
    methods, which will be discussed in detail in subsequent sections. Despite these
    efforts, the fundamental principles of learning-based registration have remained
    unchanged. Figure [2](#S2.F2 "Fig. 2 ‣ 2 Fundamentals of Learning-based Image
    Registration ‣ A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond") illustrates the conventional paradigms
    of learning-based rigid/affine and DIR. Typically, these paradigms consist of
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving and fixed images as input
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A deep neural network
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The spatial transformer (for unsupervised methods)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A loss function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The way in which moving and fixed images are inputted into deep neural networks (DNNs)
    varies depending on the architecture of the network. They can either be concatenated
    and sent in as a single input (*e.g.*, VoxelMorph [[14](#bib.bib14)]) or each
    image can be processed separately by the DNN, with the feature maps being combined
    in a deeper stage (*e.g.*, Quicksilver [[362](#bib.bib362)]).
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of DNNs can vary depending on the specific task they are designed
    to perform and the learning method they will undergo. For affine/rigid registration
    methods, DNN encoders are used for feature extraction and fully connected layers
    are used to output the parameters of the predicted transformation. DIR methods
    use DNNs with both an encoder and decoder, and the result is a deformation field
    of equal sizes to the input images. In the supervised setting, the network output
    is compared to ground truth transformations (generated from synthetic transformations
    or traditional image registration methods) or landmark correspondences using a
    loss function. In the unsupervised setting, the predicted transformation is used
    by the spatial transformer [[156](#bib.bib156)] to warp the moving image, and
    the transformed image is then evaluated against the fixed image using a loss function
    that incorporates an image similarity measure. When anatomical label maps for
    the fixed and moving images are available, the warped moving label map can also
    be produced by using the predicted transformation and the spatial transformer.
    An anatomy loss can be computed using the warped moving label map and the fixed
    label maps to provide extra guidance during network training.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a diverse range of loss functions to choose from, depending on the
    learning mode. These are thoroughly discussed in Section [3](#S3 "3 Loss Functions
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"). The networks are trained by globally optimizing
    the loss function during the training stage using a training dataset. The trained
    networks are then applied to unseen testing images for inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the self-supervised nature of image registration, the difference between
    the transformed moving image and the fixed image can be further reduced at test
    time. This is commonly known as instance-specific optimization [[14](#bib.bib14),
    [294](#bib.bib294), [234](#bib.bib234), [127](#bib.bib127), [39](#bib.bib39)].
    Specifically, the network weights can be optimized during test time to reduce
    the dissimilarity of each fixed and moving image pair in the test dataset and
    further boost the performance. Registration networks can also be specifically
    designed to produce diffeomorphic transformations, which are highly desirable
    in DIR methods and will be discussed further in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Diffeomorphic Image Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many learning-based DIR methods follow a small deformation model [[14](#bib.bib14),
    [172](#bib.bib172), [59](#bib.bib59), [303](#bib.bib303), [232](#bib.bib232),
    [124](#bib.bib124), [148](#bib.bib148)]. In this model, $\phi$ in Eqn. [1](#S2.E1
    "In 2 Fundamentals of Learning-based Image Registration ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond") is represented by a displacement field, $v$, expressed as $\phi=id+v$,
    where the displacement is added to the identity transform, $id$. Since $\phi$
    may not be a one-to-one mapping, this model does not guarantee the invertibility
    of the deformation. In some cases, the "inverse" transformation is roughly approximated
    by subtracting the displacement [[6](#bib.bib6)]. In many applications (*e.g.*,
    Avants et al. [[9](#bib.bib9)], Oishi et al. [[244](#bib.bib244)], Christensen
    et al. [[50](#bib.bib50)]), diffeomorphic image registration is highly desirable
    because it provides transformation invertibility and topological preservation.
    Diffeomorphic transformations are defined as smooth and continuous one-to-one
    mappings with a smooth and continuous inverse (*i.e.*, positive Jacobian determinants).
    They are achieved mainly through two approaches: the time-dependent velocity field [[18](#bib.bib18),
    [9](#bib.bib9)] or the time-stationary velocity field [[5](#bib.bib5), [6](#bib.bib6),
    [323](#bib.bib323), [134](#bib.bib134)] approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The time-dependent velocity field approach involves integrating sufficiently
    smooth velocity fields that change over time. The diffeomorphism is established
    by using a velocity field $v^{(t)}$ at time $t$, and evolving it through [[18](#bib.bib18)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{d\phi^{(t)}}{dt}=v^{(t)}(\phi^{(t)}).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'The diffeomorphic transformation is achieved by starting with an identity transformation,
    *i.e.* $\phi^{(0)}=id$, and integrating over the unit time period:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\phi^{(1)}=\phi^{(0)}+\int^{1}_{0}v^{(t)}(\phi^{(t)})dt.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: However, the complexity of the differential equations involved in the time-varying
    setting has led to limited use of this approach in current learning-based registration
    models. Only a handful of studies, such as [[268](#bib.bib268), [253](#bib.bib253),
    [290](#bib.bib290), [362](#bib.bib362), [361](#bib.bib361), [117](#bib.bib117),
    [334](#bib.bib334)], have integrated it into a DNN framework. These studies primarily
    involve using a DNN to predict an initial momentum field and then updating it
    through geodesic shooting [[228](#bib.bib228), [371](#bib.bib371)] to derive the
    velocity fields. As a result, end-to-end training is not feasible without re-implementing
    the geodesic shooting framework with modern DNN libraries. To date, only one previous
    work has achieved this for medical image registration [[290](#bib.bib290)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The time-stationary velocity field approach considers velocity fields that
    remain constant throughout time. By using this setting, the evolution of the diffeomorphism
    in Eqn. [2](#S2.E2 "In 2.3 Diffeomorphic Image Registration ‣ 2 Fundamentals of
    Learning-based Image Registration ‣ A survey on deep learning in medical image
    registration: new technologies, uncertainty, evaluation metrics, and beyond")
    can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{d\phi^{(t)}}{dt}=v(\phi^{(t)}),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where the velocity field, $v$, is now independent of time. Dalca et al. [[56](#bib.bib56)] were
    the first to use this setting in a DNN model through the scaling-and-squaring
    method [[5](#bib.bib5), [6](#bib.bib6)]. This method has since become dominant
    in learning-based diffeomorphic registration models [[229](#bib.bib229), [37](#bib.bib37),
    [230](#bib.bib230), [115](#bib.bib115), [374](#bib.bib374), [266](#bib.bib266),
    [375](#bib.bib375), [177](#bib.bib177)]. The scaling-and-squaring method considers
    the velocity field as a member of the Lie algebra and the deformation field as
    a member of the Lie group. The velocity field lies in the tangent space of the
    identity element in the Lie group and its connection to the deformation field
    is described by an exponential map:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\phi=\exp{(v)},$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: which is equivalent to integrating along the velocity field over the unit time
    period. An alternative perspective is that the Jacobian determinant of a deformation
    resulting from exponentiating the velocity field is always positive, similar to
    how the derivative of the exponential of a real number is always positive [[6](#bib.bib6)].
    For further information on the implementation of this method, we direct interested
    readers to the references cited [[6](#bib.bib6), [5](#bib.bib5), [56](#bib.bib56)].
    It is important to note that the scaling-and-squaring method cannot guarantee
    a folding-free transformation in the digital domain when measured by the finite
    difference approximated Jacobian determinant. This is because the scaling-and-squaring
    method involves bilinear or trilinear interpolation that is inconsistent with
    the piecewise linear transformation assumed by the finite difference based Jacobian
    determinant computation [[198](#bib.bib198)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: A compilation of unsupervised deformable image registration models
    (models are listed in alphabetical order). The table summarizes the models’ choices
    of similarity and auxiliary loss functions, regularization techniques, accuracy
    measures, and regularity measures.'
  prefs: []
  type: TYPE_NORMAL
- en: '|       | Similarity Loss     | Aux. Loss     | Regularizer     | Accuracy
    Measure     | Regularity Measure |'
  prefs: []
  type: TYPE_TB
- en: '|     |  MSE  |  NCC  |  Correlation  |  NGF  |  MI  |  MIND-SSC     |  Anatomy  |  Landmark
        |  Diffusion  |  Curvature  |  Bending  |  Jacobian  |  Consistency     |  TRE  |  MSE  |  SSIM  |  Dice  |  HdD
        |  $\%\text{of}&#124;J_{\phi}\leq 0$  |  $\#\text{of}&#124;J_{\phi}&#124;\leq
    0$  |  std.$(&#124;J_{\phi}&#124;)$  |  $&#124;\nabla J_{\phi}&#124;$  |'
  prefs: []
  type: TYPE_TB
- en: '|   ADMIR [[312](#bib.bib312)] |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Attention-Reg [[305](#bib.bib305)]     |  |  |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| BIRNet [[85](#bib.bib85)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     |  |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CondLapIRN [[232](#bib.bib232)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| CycleMorph [[172](#bib.bib172)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |    
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| de Vos *et al.* [[329](#bib.bib329)]     |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |
        |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Deformer [[42](#bib.bib42)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| DiffuseMorph [[171](#bib.bib171)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DIRNet [[60](#bib.bib60)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     |  |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DLIR [[59](#bib.bib59)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| DNVF [[115](#bib.bib115)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DTN [[374](#bib.bib374)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dual-PRNet [[146](#bib.bib146)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dual-PRNet++ [[167](#bib.bib167)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| FAIM [[180](#bib.bib180)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fan *et al.* [[83](#bib.bib83)]     |  |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fourier-Net [[159](#bib.bib159)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |    
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GraformerDIR [[360](#bib.bib360)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Han *et al.* [[116](#bib.bib116)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hering *et al.* [[132](#bib.bib132)]     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| HyperMorph [[142](#bib.bib142)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| im2grid [[199](#bib.bib199)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Krebs *et al.* [[177](#bib.bib177)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| LapIRN [[230](#bib.bib230)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| LKU-Net [[160](#bib.bib160)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.* [[188](#bib.bib188)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.* [[195](#bib.bib195)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     |  |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MIDIR [[266](#bib.bib266)]     |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |
        |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| MS-DIRNet [[186](#bib.bib186)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MS-ODENet [[352](#bib.bib352)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| NODEO [[347](#bib.bib347)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PDD-Net 2.5D [[126](#bib.bib126)]     |  |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| PDD-Net 3D [[124](#bib.bib124)]     |  |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| PC-SwinMorph [[196](#bib.bib196)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SDHNet [[379](#bib.bib379)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Shao *et al.* [[289](#bib.bib289)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |
        |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| SVF-R2Net [[165](#bib.bib165)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SYMNet [[229](#bib.bib229)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
        |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SymTrans [[212](#bib.bib212)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SynthMorph [[138](#bib.bib138)]     |  |  |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TM-DCA [[40](#bib.bib40)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| TM-TVF [[36](#bib.bib36)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| TransMorph [[37](#bib.bib37)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-V-Net [[38](#bib.bib38)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VoxelMorph [[14](#bib.bib14)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VoxelMorph-diff [[56](#bib.bib56)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |    
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VoxelMorph++ [[127](#bib.bib127)]     |  |  |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |
        |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| VR-Net [[161](#bib.bib161)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| VTN [[377](#bib.bib377)]     |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$  |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| XMorpher [[291](#bib.bib291)]     |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$     |  |  |  |
    $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[372](#bib.bib372)]     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  |  |  |     |  |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |     |  |  |  | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$
    |     | $\mathbin{\vbox{\hbox{\scalebox{1.5}{$\bullet$}}}}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|       |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3 Loss Functions ‣ A survey on deep learning in
    medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond") provides a compilation of unsupervised DIR models, summarizing the
    similarity and auxiliary loss functions, as well as other details. See the text
    for complete details and discussion.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In supervised learning, where the ground truth transformation is used, the loss
    function is typically easy to define, with the mean square error (MSE) [[226](#bib.bib226),
    [178](#bib.bib178), [79](#bib.bib79), [275](#bib.bib275), [31](#bib.bib31), [85](#bib.bib85)],
    the equivalent end-point-error (EPE), and mean absolute error (MAE) [[362](#bib.bib362),
    [303](#bib.bib303)] being the most popular choices.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Unsupervised & Semi-supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In unsupervised learning, where there is no ground truth transformation to
    reference, regularization is usually used to enforce smoothness in the transformation.
    As a result, the loss function is often similar to the energy function used in
    traditional methods (*i.e.*, Eqn. [1](#S2.E1 "In 2 Fundamentals of Learning-based
    Image Registration ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond")), which includes
    an image similarity measure and a transformation regularizer. The following subsections
    provide a summary of commonly used and recently proposed loss functions for image
    registration.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Similarity Measure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mono-modality. The choice of image similarity measure can vary depending on
    each specific application. For mono-modal registration, MSE is still a popular
    choice and has the advantage of having a straightforward probabilistic interpretation
    of the Gaussian likelihood approximation [[56](#bib.bib56), [37](#bib.bib37),
    [172](#bib.bib172), [14](#bib.bib14), [223](#bib.bib223), [161](#bib.bib161),
    [199](#bib.bib199)]. However, a disadvantage of MSE is that it averages the difference
    across all voxels in the image, making it sensitive to local intensity variations
    within the image. Normalized cross-correlation (NCC) is known to be more robust
    to local intensity variations and has been found to be superior in brain MR registration
    applications [[9](#bib.bib9)]. NCC has been extended as a loss function for training
    learning-based models, with the local window computation often being done through
    convolution operations [[180](#bib.bib180), [37](#bib.bib37), [172](#bib.bib172),
    [14](#bib.bib14), [370](#bib.bib370), [229](#bib.bib229), [230](#bib.bib230),
    [232](#bib.bib232)]. One disadvantage of NCC is its higher computing cost in comparison
    to MSE, which is mainly attributable to the comparatively large convolution kernel
    size (typically chosen between $5\times 5\times 5$ and $9\times 9\times 9$ voxels [[9](#bib.bib9),
    [14](#bib.bib14), [229](#bib.bib229)]). The structural similarity index (SSIM) [[336](#bib.bib336)]
    has also been demonstrated to be an effective loss function for mono-modal image
    registration [[39](#bib.bib39), [216](#bib.bib216), [282](#bib.bib282)]. SSIM
    takes into account luminance, contrast, and structure. It can be thought of as
    an extension of the NCC, with the structure term in SSIM being the square root
    of NCC. This allows SSIM to capture more information about the similarity of two
    images beyond just the degree of correlation between them.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modality. For multi-modal applications, traditional methods often use
    mutual information (MI) [[326](#bib.bib326)], correlation ratio [[274](#bib.bib274)],
    self-similarity context (SSC) [[129](#bib.bib129)], or normalized gradient fields (NGF) [[114](#bib.bib114)]
    as similarity measures. Both MI and correlation ratio evaluate the relationship
    between the two images by calculating intensity statistics, such as intensity
    histograms, to measure statistical dependence. However, the standard method for
    calculating intensity histograms, which involves counting, is not differentiable,
    so a Parzen window formulation [[316](#bib.bib316)] is often used to allow the
    loss to be backpropagated during network training. Parzen-window-based MI has
    been employed as a loss function in many multi-modal applications [[266](#bib.bib266),
    [329](#bib.bib329), [238](#bib.bib238), [111](#bib.bib111), [138](#bib.bib138)],
    but it can be relatively difficult to implement and also sensitive to factors
    such as the number of intensity bins and the smoothness of the Gaussian function.
    As far as we are aware, the correlation ratio has not been used in learning-based
    medical image registration. It should be noted that these intensity-statistic-based
    measurements do not take into account local structural information, making them
    more suitable for rigid/affine registration and less suitable for deformable registration
    applications [[259](#bib.bib259), [129](#bib.bib129)]. SSC is another commonly
    used loss function for multi-modal applications, and it is an improvement on the
    modality-independent neighborhood descriptor (MIND) [[128](#bib.bib128)]. Both
    SSC and MIND operate by calculating the descriptor between a voxel and its neighboring
    voxels within a given image, turning an image of any modality into a feature representation
    of these descriptors. The similarity is determined by summing the absolute differences
    between the descriptors of the two images. As SSC and MIND consider local structural
    information, they are not limited in the same way as MI or correlation ratio,
    making them more useful for multi-modal deformable registration [[119](#bib.bib119),
    [231](#bib.bib231), [357](#bib.bib357), [354](#bib.bib354), [24](#bib.bib24)].
    NGF compares images by focusing on the intensity changes, or edges, in the images.
    The similarity between the two images is determined by the presence of intensity
    changes at the same locations, regardless of the modalities of the images being
    compared. NGF was originally developed for multi-modal applications like brain
    MR T1-to-T2 and PET-to-CT [[114](#bib.bib114)]. However, it is now mostly used
    in learning-based registration models for lung CT registration [[131](#bib.bib131),
    [132](#bib.bib132), [231](#bib.bib231)]. This is because the complex structure
    of the lung, including bronchi, fissures, and vessels, can hinder accurate registration [[132](#bib.bib132)].
    NGF focuses on edges rather than intensity values, making it a more suitable measure
    for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Recent Advancements. There have been many efforts to improve upon or propose
    new loss functions due to some limitations of the aforementioned similarity measures.
    Terpstra et al. [[314](#bib.bib314)] showed that the $\ell^{2}$ loss (equivalent
    to MSE) is not optimal for MRI applications, because it does not fully leverage
    the magnitude and phase information contained in the complex data of MRI. The
    authors introduced $\bot$-loss, a loss function that is based on the polar representation
    of complex numbers and promotes symmetry in the overall loss landscape. They demonstrated
    that a network trained with a combination of $\bot$-loss and $\ell^{2}$ loss outperforms
    a network trained with $\ell^{2}$ loss alone in terms of registration performance.
    Czolbe et al. [[54](#bib.bib54)] leveraged a ConvNet feature extractor to obtain
    image features from the deformed and fixed images, and then computed the NCC between
    these features as a similarity measure. The benefit of this approach is that the
    features produced by the ConvNet feature extractor have less noise, resulting
    in a more consistent similarity measure in areas with noise which leads to a smoother
    transformation. Haskins et al. [[121](#bib.bib121)] were the first to propose
    using a ConvNet to learn a similarity measure for image registration. However,
    this method relies on having ground truth target registration error for the training
    dataset to learn such a similarity measure. Grzech et al. [[108](#bib.bib108)] went
    one step further and introduced a technique for learning a similarity measure
    using a variational Bayesian method. The method involves initializing the convolution
    kernels in the network architecture to model MSE and NCC, and then using variational
    inference to learn a similarity measure that optimizes the likelihood of the images
    in the dataset when aligning them to the atlas. Building on the success of adversarial
    networks in computer vision [[221](#bib.bib221), [106](#bib.bib106)], researchers
    have developed a number of techniques for image registration that leverage adversarial
    training [[83](#bib.bib83), [217](#bib.bib217), [208](#bib.bib208)]. These methods
    can be used standalone or in conjunction with a traditional similarity measure.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Deformation Regularizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A deformation regularizer, as the terminology implies, is used for DIR, with
    its usage being not necessary for rigid/affine transformations. For DIR algorithms,
    producing smooth deformations is not only a desirable property but a necessary
    requirement: while diffeomorphic transformations may not be required for certain
    applications, smoothness remains imperative in almost all cases to avoid trivial
    solutions such as rearranging voxels [[276](#bib.bib276)], with which an almost
    perfect similarity measure can be achieved but result in unrealistic transformation (also
    see Section [6](#S6 "6 Registration Evaluation Metrics ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond")). The regularizer can be considered as a prior in a maximum a posteriori (MAP)
    framework, while the similarity measure acts as the data likelihood (*e.g.*, in
    the case of MSE, the data likelihood becomes a Gaussian likelihood). The diffusion
    regularizer is a commonly employed deformation regularizer, as demonstrated by
    its frequent appearance in Table [1](#S3.T1 "Table 1 ‣ 3 Loss Functions ‣ A survey
    on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"). This regularization computes the squared $\ell^{2}$-norm
    of the gradients of the displacement field, effectively penalizing the disparities
    between adjacent displacements. Other alternatives for regularization include
    using the $\ell^{1}$-norm instead of the $\ell^{2}$-norm to impart equal penalties
    on the neighboring disparities, or penalizing the second derivative of the displacements,
    commonly referred to as bending energy [[280](#bib.bib280)]. It is important to
    note that since bending energy and curvature-based regularizers penalize the second
    derivatives, thereby zeroing out any affine contributions, pre-affine alignment
    prior to the deformable registration step may not be necessary, as demonstrated
    in [[68](#bib.bib68), [89](#bib.bib89)]. These conventional regularizers enforce
    an isotropic regularization on the displacement field [[248](#bib.bib248)]. As
    a result, they discourage discontinuities in the displacements in applications
    where sliding motion may occur in organs, such as registering exhale and inhale
    CT scans of the lung. Historically, various improvements have been made to address
    this issue, including the isotropic Total Variation (TV) regularization [[327](#bib.bib327)],
    anisotropic diffusion regularization [[248](#bib.bib248)], and adaptive bilateral
    filtering-based regularization [[250](#bib.bib250)]. However, these regularization
    techniques have not been widely adopted in learning-based image registration.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent Advancements. Enforcing spatial smoothness alone is insufficient to ensure
    the regularity of the transformations. A different strategy is to penalize the
    “folding” of voxels directly during training, in addition to applying the aforementioned
    regularizers to enforce smoothness in the deformation. These foldings can be evaluated
    using local Jacobian determinants, where the magnitude of the Jacobian determinant
    indicates if the volume is expanding or shrinking near the voxel location. A non-positive
    Jacobian determinant represents a locally non-invertible transformation. Several
    regularization methods based on local Jacobian determinants have been proposed
    to penalize such transformations [[180](#bib.bib180), [229](#bib.bib229)]. Meanwhile,
    with the advent of deep learning, new methods have emerged that leverage the deep
    learning of deformation regularization from data. One such method by Niethammer
    et al. [[242](#bib.bib242)], introduced a method that learns a spatially-varying
    deformation regularization using training data. Spatially-varying regularization
    offers the advantage of accommodating variations in deformation that may be required
    for different regions within an image, such as the movement of the lungs in relation
    to other organs (*e.g.*, rib cage) due to respiratory processes. The technique
    proposed by Niethammer *et al.* involves training a registration network to produce
    not only a deformation field but also a set of weight maps, each of which corresponds
    to the weight of a Gaussian smoothing kernel in a multi-Gaussian kernel configuration.
    The weighted multi-Gaussian kernel is then applied to the deformation field via
    convolution. To further impose spatial smoothness, an optimal mass transport (OMT)
    loss function was introduced to encourage the network to assign larger weights
    to Gaussian kernels with larger variances. While this method was developed for
    a time-stationary velocity field setting, Shen et al. [[290](#bib.bib290)] later
    expanded upon it by incorporating it into a time-varying velocity field setting.
    In this setup, a different set of weight maps are produced for each time point.
    More recently, Chen et al. [[41](#bib.bib41)] introduced a weighted diffusion
    regularizer that applies spatially-varying regularization to the deformation field.
    The neural network generates a weight volume, assigning a unique regularization
    weight to each voxel and thus allows for spatially-varying levels of regularization
    strength. As the diffusion regularizer is related to Gaussian smoothing, using
    spatially-varying strengths of diffusion regularization can be considered equivalent
    to employing a multi-Gaussian kernel, as originally proposed by Niethammer et al.
    [[242](#bib.bib242)]. This is because the convolution of multiple Gaussian kernels
    still results in a Gaussian kernel. To promote the overall smoothness of the deformation,
    they further applied a log loss to the weight volume, which encourages the maximum
    regularization strength when possible. In a different approach, Wang et al. [[335](#bib.bib335)] employed
    a regression network to learn the optimal regularization parameter for an optimization-based
    method, specifically Flash [[371](#bib.bib371)]. Flash is a geodesic shooting
    method in the Fourier space that requires only the initial velocity field to compute
    the time-dependent transformation. Wang *et al.* generated ground truth optimal
    regularization parameters by assuming the prior of the initial velocity field
    given the regularization parameter as a multivariate Gaussian distribution. Using
    gradient descent, they obtained the optimal regularization parameter for each
    image pair through MAP estimation. A ConvNet regression encoder then estimates
    the optimal regularization parameter based on the image pair. This approach achieved
    registration performance comparable to Flash while significantly improving runtime
    and memory efficiency. Alternatively, Laves et al. [[184](#bib.bib184)] were inspired
    by the deep image prior [[320](#bib.bib320)]. They used a randomly initialized
    ConvNet as a regularization prior. They then fed a random image (*i.e.*, a noise
    image) as input and the network gradually transformed it into a smooth deformation
    field through iterative optimization. The deep image prior provided by the ConvNet
    enables the network to produce a smooth deformation in the early iterations, then
    gradually adds non-smooth high-frequency deformations. As a result, early stopping
    is used for the network to generate a smooth deformation field without the need
    for explicitly encouraging smoothness in the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations can also be implicitly regularized by imposing invertibility
    constraints. This is achieved by using a symmetric consistency loss or cycle consistency
    loss. Symmetric consistency typically uses a single DNN to output both the forward
    and reverse deformation fields, which transform the moving image to the fixed
    image and vice versa, respectively. The similarity between the warped image and
    the target image is then calculated and backpropagated to update the network [[229](#bib.bib229),
    [196](#bib.bib196)]. Alternatively, a consistency loss can be calculated by composing
    the network-generated forward and backward deformation fields, and then comparing
    the outcome with the identity transformation [[107](#bib.bib107), [317](#bib.bib317)].
    The underlying concept is that, theoretically, an invertible mapping should cancel
    itself when composed with its inverse. Such an approach by itself imposes invertibility
    but does not explicitly enforce spatial smoothness over the deformation field.
    Greer et al. [[107](#bib.bib107)] demonstrated that incorporating such a loss
    within a DNN framework implicitly imposes spatial regularity on the deformation
    field without necessitating an additional regularizer to enforce smoothness. The
    authors showed that the errors of the DNN in computing the inverse, combined with
    the implicit bias of DNN favoring more regular outputs, enable such a consistency
    loss to entail a $H^{1}-$ or Sobolev-type regularization over the deformation
    field, thereby implicitly enforcing spatial smoothness. Later, Tian et al. [[317](#bib.bib317)] expanded
    on this regularizer and proposed to regularize deviations of the Jacobian of the
    composition from the identity matrix. This improved regularizer led to faster
    convergence while offering greater flexibility, while maintaining an approximated
    diffeomorphic transformation.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, cycle consistency employs two identical networks, where the
    first network generates a forward deformation field that deforms the moving image
    and the second network produces a reverse field that aims to warp the deformed
    image back to the original moving image [[377](#bib.bib377), [179](#bib.bib179),
    [172](#bib.bib172)]. Both consistency losses have been shown to improve the registration
    performance and provide regularization to the deformation field. However, since
    this regularization is not explicitly applied to the deformation fields, a separate
    deformation regularizer is often required in addition to the consistency loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Auxiliary Anatomical Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The overlap of anatomical label maps of the fixed and transformed moving images
    is a widely used evaluation metric for image registration. Hence, to improve registration
    performance on this metric, learning-based methods often incorporate an anatomy
    loss in their network training. Various loss functions used in image segmentation
    tasks, such as Dice loss, cross-entropy, and focal loss (see Ma et al. [[210](#bib.bib210)] for
    a comprehensive review of such loss functions), can be borrowed as the choice
    of anatomy loss. Despite the availability of different loss functions, Dice loss
    remains the most commonly used loss function in learning-based image registration,
    as evidenced by Table [1](#S3.T1 "Table 1 ‣ 3 Loss Functions ‣ A survey on deep
    learning in medical image registration: new technologies, uncertainty, evaluation
    metrics, and beyond"). This is likely because Dice loss is confined within the
    range of $[0,1]$, like NCC, which makes it easier to adjust hyperparameters when
    used in conjunction with NCC.'
  prefs: []
  type: TYPE_NORMAL
- en: When anatomical landmarks are present in both the moving and fixed images, the
    transformation generated by the DNN can be applied to the landmarks of the moving
    image. The resulting transformed landmarks can then be compared with the landmarks
    of the fixed image to create a loss. This landmark supervision has been utilized
    in optimization-based registration methods to improve performance, as demonstrated
    in a number of studies  [[77](#bib.bib77), [261](#bib.bib261), [281](#bib.bib281),
    [125](#bib.bib125), [88](#bib.bib88)]. Hering et al. [[132](#bib.bib132)] were
    the first to incorporate landmark supervision into a DNN framework by comparing
    the MSE between the transformed and target landmarks, which resulted in a substantial
    improvement in the target registration error of the landmark. Subsequently, [[127](#bib.bib127)] confirmed
    the superiority of landmark supervision on multiple benchmark datasets in their
    work. It is worth mentioning that the landmarks can be generated automatically
    before or during the training stage without manual labeling using automatic landmark
    detection algorithms [[125](#bib.bib125), [281](#bib.bib281), [261](#bib.bib261)],
    making it straightforward to integrate into most learning-based registration frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The combination of anatomy loss and deformation regularization without an intensity-based
    similarity measure is also common, and in these cases, the anatomy loss serves
    as a modality-independent similarity measure [[148](#bib.bib148), [305](#bib.bib305),
    [23](#bib.bib23)]. However, the drawback of using anatomy loss without a similarity
    measure is clear: it does not penalize deformations in areas where anatomical
    labels are missing or ambiguous. Thus, to achieve accurate and realistic deformations,
    the anatomical labels should be as detailed as possible, ideally with a unique
    label for each organ or structure. However, obtaining such detailed labels is
    often challenging as anatomical label maps in medical imaging are usually manually
    delineated, which is a time-consuming and expensive process.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Network Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application of ConvNets has been the dominant trend in learning-based image
    registration since its inception. Among different ConvNets architectures, the
    U-Net-like architectures [[279](#bib.bib279)], which were initially designed for
    image segmentation tasks, have played an important role. Many noteworthy ConvNet-based
    registration models, including RegNet [[303](#bib.bib303)], DIRNet [[60](#bib.bib60)],
    QuickSilver [[362](#bib.bib362)]VoxelMorph [[14](#bib.bib14), [56](#bib.bib56)],
    VTN [[377](#bib.bib377)], DeepFlash [[334](#bib.bib334)], and CycleMorph [[172](#bib.bib172)],
    have demonstrated promising performance in various registration applications.
    More recently, registration neural networks have witnessed notable advancements
    beyond the conventional ConvNet designs, owing to the progress of DNN architectures
    in computer vision and the development of architectures that are specifically
    tailored for registration tasks. Notably, models such as Transformers, diffusion
    models, and Neural ODEs are gaining increasing attention in the field of image
    registration. This section provides a comprehensive overview of these recent advancements.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Adversarial Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of adversarial learning applied to image registration relies on
    the foundational principles of generative adversarial networks (GANs). The concept
    of GANs is derived from a two-player zero-sum game involving a generator and a
    discriminator [[106](#bib.bib106)]. The objective of the generator is to generate
    new samples by learning the data distribution, while the discriminator functions
    as a binary classifier, aiming to accurately distinguish between real and generated
    samples. In the context of image registration, the registration network acts as
    the generator, producing a deformation field and subsequently warping the moving
    image. Meanwhile, the discriminator functions as an image similarity measure,
    distinguishing between the warped image and the fixed image. This offers the advantage
    of alleviating the need for an explicit similarity measure, making the approach
    adaptable to both mono- and multi-modality applications.
  prefs: []
  type: TYPE_NORMAL
- en: In early applications of adversarial learning to image registration, Fan et al.
    [[84](#bib.bib84)] and Yan et al. [[356](#bib.bib356)] adhered to the aforementioned
    approach. The former utilized the generator to produce a deformation field, while
    the latter employed a ConvNet encoder to generate affine transformation parameters.
    Subsequently, a binary discriminator served as a similarity measure between the
    transformed and fixed images. In a similar vein, Mahapatra *et al.* [[216](#bib.bib216),
    [218](#bib.bib218), [217](#bib.bib217)] applied adversarial learning to multi-modal
    image registration, with the additional implementation of CycleGAN [[382](#bib.bib382),
    [264](#bib.bib264)] to further ensure the inverse consistency of the generated
    deformation field. Elmahdy et al. [[78](#bib.bib78)] proposed incorporating anatomical
    label maps into a Wasserstein-GAN (WGAN) to enhance the segmentation performance
    of the registration network. Their generator was a U-Net-based network that generated
    a deformation field, which warped both the moving image and the associated anatomical
    label map. The discriminator’s role was to evaluate the alignment between the
    warped and fixed image, as well as the warped and fixed label maps. In their approach,
    image and anatomical similarity measures were still employed, while the discriminator
    served as an additional measure of the alignment. Similar approaches can be found
    in Duan et al. [[75](#bib.bib75)], Li and Ogino [[192](#bib.bib192)], and Luo
    et al. [[208](#bib.bib208)], where the authors used the discriminator in conjunction
    with image similarity measures as additional alignment indicators. In another
    study, Fan et al. [[83](#bib.bib83)] proposed a GAN-based registration framework
    applicable to both mono- and multi-modality registration. Their generator was
    also a registration network based on U-Net, with the discriminator serving as
    the sole measure of image alignment. However, the definition of positive pairs
    sent to the discriminator deviated from previous methods. Ideally, in mono-modality
    registration, a positive pair would consist of identical images, but this strict
    requirement is impractical. Given this observation, the authors proposed that
    the positive pair comprise the fixed image and an alpha-blended image created
    from the fixed and moving images. For multi-modality registration, a positive
    pair consisted of pre-aligned multi-modal images from the same patient. The method
    was evaluated on mono-modal brain MRI registration and multi-modal pelvic MR and
    CT registration tasks, demonstrating favorable performance compared to the state-of-the-art
    at the time.
  prefs: []
  type: TYPE_NORMAL
- en: Given the promising results GANs have demonstrated in image translation, i.e.,
    synthesizing one image modality into another, researchers have made efforts to
    leverage their capabilities in addressing multi-modal image registration. This
    approach involves first synthesizing multi-modal images into the same modality
    and then applying a registration network to perform the image registration task.
    Xu et al. [[354](#bib.bib354)] tackled the challenge of multi-modal registration
    of CT and MR images using a CycleGAN-based approach to translate CT images into
    MR images. To ensure that the translated images maintained anatomical consistency
    with the original images, the authors introduced additional loss functions, including
    MIND and identity loss, alongside the standard CycleGAN loss. They then employed
    a three-stage registration framework to align the original and translated images.
    In the first stage, a U-Net-based registration network learned the multi-modal
    registration between CT and MR images. In the second stage, a network with the
    same architecture learned the mono-modal registration between the translated CT
    and the target MR images. Finally, the deformation fields created by both registration
    networks were fused using a convolutional layer to produce the final deformation
    field. A similar concept was presented in Wei et al. [[339](#bib.bib339)], where
    mutual information was used instead of MIND to enforce structural consistency.
    Zheng et al. [[378](#bib.bib378)] integrated an image translation network within
    a GAN-based image registration framework, where the modality of the moving image
    was first translated to the modality of the target image before a registration
    network was applied to register the two images. The discriminator in this approach
    acted as an image similarity metric for both the registration and image translation
    networks. Additionally, this approach employed a symmetric pipeline that reversed
    the order of the moving and fixed images, ensuring symmetric consistency in the
    resulting synthesized and deformation images. More recently, Han et al. [[116](#bib.bib116)] proposed
    tackling the multi-modal registration between CT and MR images using a dual-channel
    framework. Within each channel, an imaging modality was transformed into a target
    modality using a probabilistic CycleGAN, which was then followed by a registration
    network that predicted the deformation in the target modality. The deformation
    fields from both channels were then fused, taking advantage of the uncertainty
    weighting generated by the synthesis networks. This proposed dual-channel framework
    can be trained end-to-end, resulting in improved registration accuracy and faster
    runtime compared to baseline methods.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial learning has also been employed for knowledge distillation, enabling
    the transfer of information from a larger teacher network to a smaller student
    network (*i.e.*, in terms of the number of parameters). Tran et al. [[318](#bib.bib318)] aimed
    to compress the size of a registration network by transferring information from
    a computationally expensive VTN [[377](#bib.bib377)] to a smaller registration
    network with only one-tenth of its parameters. The training process for the student
    network involved calculating a correlation-based image similarity measure [[377](#bib.bib377)]
    between the warped image generated by the student network and the fixed image.
    Meanwhile, a discriminator was used to differentiate the deformation field created
    by the student network and the pre-trained teacher network. After training, the
    teacher network was discarded, and only the lightweight student network was used
    for inference. Despite having only one-tenth of the network parameters, the lightweight
    registration network demonstrated comparable performance to baseline learning-based
    methods with larger parameter sizes, in terms of both anatomical overlaps and
    deformation smoothness.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Contrastive Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principle of contrastive learning enables DNNs to learn by comparing various
    examples instead of focusing on single data points independently. This comparison
    process typically involves examining positive pairs of similar inputs and negative
    pairs of dissimilar inputs. For a comprehensive understanding of this concept
    and a detailed overview of the evolution of contrastive learning, we recommend
    interested readers refer to Le-Khac et al. [[185](#bib.bib185)]. In the context
    of image registration, contrastive learning could be particularly beneficial as
    an alternative to using explicit image similarity metrics, which can be challenging
    to optimize due to their task-specific nature. For example, different similarity
    metrics may be preferred for lung CT registration versus brain MRI registration
    or multi-modal versus mono-modal registration tasks. Whereas, contrastive learning
    empowers the DNN to determine whether two images are registered or not without
    relying on a specific image similarity metric, making it a more versatile approach
    for handling different registration tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Hu et al. [[145](#bib.bib145)] were the pioneers in applying contrastive learning
    to multi-modal affine registration, concentrating on the inter-patient alignment
    of 2D CT and MR scans for patients with Nasopharyngeal Cancer. Their method involved
    using an automatic keypoint detecting algorithm to identify keypoints in the CT
    and MR scans. Subsequently, they extracted a patch centered on each keypoint and
    employed a Siamese network to minimize the contrastive loss, which minimized the
    distance between corresponding keypoints and maximized the distance between non-corresponding
    keypoints. In the testing phase, after establishing correspondences between all
    keypoints in the CT and MR scans, the optimal affine transformation parameter
    was determined by means of least-squares fitting. In another study, Pielawski
    et al. [[257](#bib.bib257)] applied contrastive learning to transform multi-modal
    images into similar contrastive representations with equivariant properties. Their
    method used two independent U-Nets to learn the representations for each modality
    such that the InfoNCE-based [[246](#bib.bib246)] loss between the learned representations
    is minimized. This minimization can be understood as maximizing the mutual information
    between the two learned representations. Finally, conventional affine registration
    methods were used to align the learned representations as if they had undergone
    a mono-modal registration task. Wetzer et al. [[342](#bib.bib342)] later investigated
    the contrastive learning approach proposed in Pielawski et al. [[257](#bib.bib257)] to
    determine whether applying contrastive learning supervisions to the U-Nets’ intermediate
    layers could improve multi-modal image registration performance. However, they
    concluded that the best representations for the evaluated registration task were
    achieved when the contrastive loss was applied only to the features of the final
    layers. Casamitjana et al. [[33](#bib.bib33)] proposed a contrastive learning-based
    approach for multi-modal deformable registration. They introduced a synthesis-by-registration
    method, where they trained a registration network for mono-modal registration
    on the target modality domain, and then froze the network’s weight for training
    an image synthesis network using a loss function that leverages the registration
    network. The image synthesis network’s ability to accurately translate the moving
    image into the target modality directly influenced the performance of the registration
    network. To enhance synthesis performance and ensure geometric consistency, a
    PatchNCE-based [[252](#bib.bib252)] contrastive loss was used, maximizing the
    mutual information between pre- and post-synthesis images at the patch level.
    This method demonstrated promising results in multi-modal brain MRI registration
    applications, outperforming both MI-based registration and other image synthesis-based
    registration methods. Dey et al. [[62](#bib.bib62)] also addressed the multi-modal
    registration task using contrastive loss. In their method, feature-extracting
    autoencoders were first pre-trained for each modality to derive modality-specific
    features. These autoencoders were then used on the deformed moving image and the
    fixed image to extract features for a PatchNCE-based [[252](#bib.bib252)] contrastive
    loss. In order to optimize contrastive learning, a single positive pair was sampled,
    corresponding to the multi-scale feature patches of the same spatial location
    across both modalities, while multiple negative pairs were sampled, corresponding
    to the feature patches of different spatial locations.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, the methods based on contrastive learning have been centered on multi-modal
    image registration. However, Liu et al. [[194](#bib.bib194)] proposed the integration
    of contrastive learning in the intermediate stages of the network architecture
    for mono-modal brain MRI registration. In their method, two identical ConvNet
    encoders of shared weights were applied to the moving and fixed images, each followed
    by a fully-connected layer to project ConvNet extracted features onto a latent
    space where the contrastive loss is applied. The positive pair for computing the
    contrastive loss consists of the unregistered moving and fixed image pair, while
    any other pair apart from the current image pair under registration is considered
    a negative pair. In an extension of their work, Liu et al. [[196](#bib.bib196)] proposed
    to compute the contrastive loss in a similar way, but between patches of the moving
    and fixed images. However, it is important to note that the positive pair used
    in these two methods contained structural dissimilarities as it was the unregistered
    image pair, as opposed to the registered images used in the methods mentioned
    earlier. The authors argued that this was because the image contents, including
    the number of brain structures, were consistent for brain registration. Nonetheless,
    further research is needed to fully uncover the potential of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key factors in designing ConvNets is the size of the receptive fields.
    While incorporating consecutive convolutional layers and pooling operations can
    increase the theoretical receptive fields of ConvNets, its effective receptive
    fields are still limited [[207](#bib.bib207)]. This makes them less effective
    at capturing long-range spatial correspondence, which is important to image registration
    since it aims to identify the correspondence between different parts of the images.
    In contrast, Transformers are widely acknowledged for their superior ability to
    capture long-range dependencies and achieve exceptional performance when trained
    on large datasets [[189](#bib.bib189)]. Transformers differ from ConvNets in that
    they employ the self-attention mechanism, in which each local part of an image
    is compared in relation to the other parts, guiding the network on where to focus.
    Originally developed for natural language processing tasks [[322](#bib.bib322)],
    Transformers have recently become prevalent in various computer vision applications [[72](#bib.bib72),
    [200](#bib.bib200), [202](#bib.bib202), [35](#bib.bib35), [369](#bib.bib369),
    [71](#bib.bib71), [32](#bib.bib32)]. Inspired by their success, many Transformer-based
    models have been proposed and have demonstrated promising performance in medical
    imaging applications. For a comprehensive review of the current Transformer-based
    models in medical imaging, interested readers are directed to a review paper by Li
    et al. [[189](#bib.bib189)]. Despite their potential, Transformers have certain
    drawbacks, such as larger computational complexity and a lack of inductive bias
    when compared to ConvNets, hindering the training process. To address these shortcomings,
    Transformers are commonly used in conjunction with ConvNets in medical image registration
    applications. Chen et al. [[38](#bib.bib38)] were the first to utilize Transformers
    for registration-based tasks. They proposed ViT-V-Net, which employs a ConvNet
    for extracting high-level features, followed by a Vision Transformer (ViT) [[73](#bib.bib73)]
    and a ConvNet decoder to generate a dense displacement field. Subsequently, they
    proposed TransMorph [[37](#bib.bib37)], which employs a Swin Transformer [[200](#bib.bib200)]
    in the encoder, replacing the ConvNet feature extractor and ViT. TransMorph is
    capable of both affine registration and deformable registration. The study provided
    empirical evidence that Transformer-based models have larger effective receptive
    fields than baseline ConvNets. In inter-subject and atlas-to-subject brain MRI
    registration, as well as XCAT-to-CT abdomen registration applications, TransMorph
    achieved significantly improved registration performance when compared to top-performing
    traditional and ConvNet-based registration models. Zhang et al. [[374](#bib.bib374)] proposed
    DTN, which consists of two encoder branches with identical architecture. Each
    branch contains a ConvNet feature extractor and a ViT. In DTN, the moving and
    fixed images are first fed consecutively into one encoder branch, then concatenated
    and sent to the other branch. The encoder outputs are then concatenated and sent
    to a ConvNet decoder to produce a deformation field. Mok and Chung [[233](#bib.bib233)] introduced
    a Transformer encoder, C2FViT, specifically designed to tackle the affine registration
    problem. Their Transformer architecture was inspired by ViT, but with augmented
    patch embedding and feed-forward layers to introduce locality into the model.
    C2FViT adopts a coarse-to-fine strategy with an image pyramid for affine registration.
    The registration process is carried out in multiple stages of ViTs with identical
    architectures, each corresponding to a different resolution of the fixed and moving
    images. The affine parameters are estimated in each stage, and the moving image
    is affine-transformed using the parameters from the previous stage to refine the
    registration progressively. C2FViT was evaluated on several benchmark datasets
    and demonstrated superior performance compared to multiple ConvNet-based and traditional
    affine registration methods. Chen et al. [[42](#bib.bib42)] proposed a Deformer
    module, which leverages the attention mechanism on feature maps produced by a
    ConvNet encoder. The authors argued that the Deformer module facilitated the image-to-spatial
    transformation mapping process by estimating the displacement vector prediction
    as a weighted sum of multiple bases. Employing a coarse-to-fine strategy, the
    proposed model outperformed both the ConvNet and Transformer models in the comparative
    analysis. Song et al. [[305](#bib.bib305)] introduced Attention-Reg, a model that
    adopts cross-attention to correlate features extracted from multi-modal input
    images by a ConvNet encoder. To expedite the training process, they applied a
    contrastive pre-training strategy to the ConvNet feature extractor, allowing for
    the extraction of similar features from different modality images. The Dice loss
    was used as the multi-modal similarity measure, and they developed both rigid
    and deformable variations of the model. The results showed that Attention-Reg
    performed favorably against several learning-based rigid and deformable registration
    models. Similarly, Shi et al. [[291](#bib.bib291)] introduced XMorpher, a full
    Transformer architecture featuring dual parallel feature extractors that exchange
    information via a cross-attention mechanism. The cross-attention module developed
    in their study is based on a Swin Transformer, where attention is computed between
    base windows of one image and searching windows of another image with differing
    sizes. This cross-attention mechanism exhibited improved performance compared
    to self-attention-based Transformers and ConvNet models. Chen et al. [[40](#bib.bib40)] made
    further improvements to the cross-attention technique used in XMorpher. They proposed
    a novel deformable cross-attention module that enables tokens to be sampled from
    regions beyond the conventional rectangular window, while also reducing computational
    complexity. A lightweight ConvNet was introduced to deform the sampling window
    in a reference. The attention is then computed between the tokens sampled from
    the deformed window in the reference and those sampled from a rectangular window
    in a base. This enables tokens sampled from a larger reference region to guide
    the network on where to focus within each local window in the base. The proposed
    network includes two encoding paths. In one path, the moving and fixed images
    are used as the base and reference, respectively. In the other path, the roles
    of the base and reference are switched, with the moving image used as the reference
    and the fixed image used as the base. A ConvNet decoder then fuses the features
    extracted from the two encoders to generate a deformation field. Their method
    was evaluated on brain MRI registration tasks, and it performed favorably against
    self-attention, cross-attention, and ConvNet-based models. Liu et al. [[199](#bib.bib199)] proposed
    im2grid, a model that uses cross-attention to explicitly guide the neural network
    in comprehending the coordinate system for image registration, which is usually
    learned implicitly from data. Their approach uses ConvNet encoders to independently
    extract hierarchical features from the fixed and moving images. Subsequently,
    their proposed coordinate translator block computes a softmax score function by
    comparing the extracted fixed image feature at a voxel location with the features
    of the moving image within a search window. Spatial correspondence between the
    voxel location in the fixed and moving images is established by linearly combining
    the coordinates of all voxel locations weighted by the score function. Their approach
    is implemented as cross-attention with coordinates as one of the inputs. This
    model was evaluated on inter-patient brain MRI registration tasks using publicly
    available datasets and demonstrated superior performance compared to the comparative
    ConvNets and Transformer-based models.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanisms of Transformers have inspired various ConvNet designs in computer
    vision, leading to a debate on whether Transformers could replace ConvNets for
    image-related tasks [[189](#bib.bib189)]. ConvNet models such as ConvNeXt [[201](#bib.bib201)]
    and RepLKNet [[64](#bib.bib64)] have built upon Transformer concepts and demonstrated
    performance comparable to Transformers. Inspired by these models, Jia et al. [[160](#bib.bib160)] proposed
    a U-Net with increased kernel sizes to expand the effective receptive field of
    the U-Net. Their method compared favorable against several Transformer-based registration
    methods. Currently, ConvNets still possess inherent advantages over Transformers,
    such as their invariance to input image sizes and the incorporation of inductive
    bias due to the nature of the convolution operation. Therefore, there has been
    a growing interest in advancing ConvNets using Transformer concepts in computer
    vision. It is anticipated that further research in this area will lead to improved
    ConvNet architectures for medical image registration applications.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Diffusion Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, diffusion models [[301](#bib.bib301), [136](#bib.bib136)] have
    garnered significant research interest in computer vision. Initially designed
    for generative tasks, such as image synthesis, inpainting, and super-resolution,
    diffusion models have now been widely explored in various applications in the
    field of medical image analysis (see Kazerouni et al. [[168](#bib.bib168)] for
    a survey). In contrast to other generative models like GANs and VAEs, which are
    either confined to data with limited variability or generating low-quality samples [[136](#bib.bib136),
    [168](#bib.bib168)], diffusion models have no such restrictions, making them an
    attractive alternative. The goal of diffusion models is to use the known forward
    process of gradual diffusion of information caused by noise to learn the reverse
    process of recovery of information from noise. The forward process is similar
    to the behavior of particles in thermodynamics, where particles spread (*i.e.*,
    diffuse) from areas of high concentration to those of low concentration [[175](#bib.bib175),
    [301](#bib.bib301)]. The existing diffusion models use iterative steps of diffusion,
    which can include up to several thousand steps, to carry out the diffusion process.
    As a result, inference with these models, which requires the reverse diffusion
    process, is time-consuming. To date, only Kim et al. [[171](#bib.bib171)] have
    used a diffusion model in medical image registration. They proposed DiffuseMorph,
    which involves a diffusion network and a deformation network. The diffusion network
    learns a conditional score function (*i.e.*, the added noise), while the deformation
    network uses the latent feature in the reverse diffusion process to estimate the
    deformation field. The registration process of DiffuseMorph is a one-step procedure
    as the fixed image is the target image at the end of the reverse diffusion process (*i.e.*,
    $t=0$), and it is already given. As a result, there is no need for time-consuming
    reverse diffusion steps to synthesize a target image from the moving image. Furthermore,
    DiffuseMorph offers the added capability of producing continuous deformations
    through the interpolation of the learned latent space. The method demonstrated
    promising results when compared to several ConvNet-based methods on a publicly
    available Cardiac MRI dataset and a human facial expression dataset. However,
    since their forward process adopts the strategy of adding Gaussian noise to the
    fixed image, their diffusion network learns a conditional score function for the
    fixed images instead of the deformation between the fixed and moving images. Hence,
    additional exploration is imperative to gain a more comprehensive insight into
    the benefits of diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Neural ODEs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inspired by Euler’s method for discretizing the derivative of ordinary differential
    equations (ODEs) into discrete time step updates, Chen et al. [[44](#bib.bib44)]
    proposed a new family of DNN models called Neural ODEs. In their method, DNN elements
    that progressively update their input (*e.g.*, residual connections, or recurrent
    networks) are interpreted as updates of time steps in Euler’s method. Consequently,
    a chain of these elements in a neural network is essentially a solution of the
    ODE with Euler’s method of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{dh(t)}{dt}=f_{\theta}(h(t),t),$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h(t+1)=h(t)+f_{\theta}(h(t),t),$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $h(t)$ represents the $t$-th element, which may be a residual block or
    a network. The final output at $t=T$ can be computed by integrating $f$ over the
    time interval $[0,T]$, which is evaluated by a numerical solver taking many small
    time steps, thus approximating a neural network with infinite depth.
  prefs: []
  type: TYPE_NORMAL
- en: The first application of the NeuralODE framework for medical image registration
    was introduced by Xu et al. [[352](#bib.bib352)]. They proposed MS-ODENet, which
    parameterizes $h$ at the final time point $T$ (*i.e.*, $h(T)$) as the deformation
    field that warps the moving image to the fixed image, and $\frac{dh(t)}{dt}$ as
    the small increment of deformation produced by a network at state $t$ from the
    preceding state $h(t-1)$. To alleviate the computational burden of numerical solvers
    and accelerate the runtime, they proposed solving ODEs at different resolutions
    in a coarse-to-fine manner. However, the loss function, consisting of a similarity
    measure and a deformation regularizer, is applied only to the final deformation
    field $h(T)$. Similarly, Wu et al. [[347](#bib.bib347)] proposed NODEO, which
    formulated $h(t)$ as the voxel movement at time $t$ and the trajectory of the
    movement as the solution to the ODE. Drawing inspiration from dynamical systems,
    they expressed the ODE as $\frac{dh(t)}{dt}=\mathcal{K}v_{\theta}(h(t),t)$, where
    $\mathcal{K}$ is a Gaussian smoothing kernel, $v_{\theta}$ denotes the velocity
    of the voxel movement produced by a neural network, and the initial condition
    $h(0)$ is an identity. It is noteworthy that this formulation bears similarities
    to LDDMM [[18](#bib.bib18)], an influential optimization-based method that considers
    image registration as an energy-minimizing flow of particles over time. In contrast
    to MS-ODENet [[352](#bib.bib352)], which applies loss solely to the deformation
    at $t=T$, NODEO optimizes image similarity at each $t$ while minimizing the energy
    of the flow and encouraging spatial smoothness and regularity of the velocity
    fields through the Gaussian kernel, diffusion regularizer, and Jacobian determinant
    loss. The authors compared NODEO to various widely-used traditional methods and
    a ConvNet model on brain MRI registration tasks. It demonstrated superior registration
    performance measured by Dice while attaining diffeomorphic registration.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Implicit Neural Representations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image registration can be formulated as an implicit problem of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{C}(\pmb{x},\psi)=0,\ \ \psi:\pmb{x}\rightarrow\psi(\pmb{x}),$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\pmb{x}\in\mathbb{R}^{2,3}$ is the 2D or 3D spatial coordinate (*i.e.*,
    from an integer grid), and $\psi$ represents a neural network that maps each coordinate
    $\pmb{x}$ to a value of interest, subject to the constraint $\mathcal{C}$. In
    the context of image registration, $\psi$ typically maps the coordinate $\pmb{x}$
    to its deformation $\psi(\pmb{x})$, while $\mathcal{C}$ comprises a similarity
    measure and a deformation regularizer. The neural network $\psi$ can be considered
    as an implicit function of $\pmb{x}$, defined by the relation modeled by $\mathcal{C}$
    (Eqn. [8](#S4.E8 "In 4.6 Implicit Neural Representations ‣ 4 Network Architectures
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond")). This concept is commonly referred to as implicit
    neural representations in computer vision [[299](#bib.bib299), [225](#bib.bib225),
    [240](#bib.bib240), [227](#bib.bib227)]. Although $\pmb{x}$’s used during training
    are discrete, the implicit function $\psi(\pmb{x})$ parameterized by a neural
    network is a continuous and differentiable function. As a result, implicit neural
    representations provide a more compact representation of a continuous function
    and facilitate smooth manipulation of that function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Han et al. [[115](#bib.bib115)] proposed to parameterize a continuous deformation
    field using a multi-layer perceptron (MLP) introduced in [[299](#bib.bib299)],
    given an integer grid representing the spatial coordinates of the voxels. The
    MLP thus serves as the implicit function of the integer grid. Since the MLP is
    not conditioned on the images and the only input is the coordinates that are deterministic
    for all images of the same resolution, optimization of the MLP is carried out
    iteratively and pair-wise for each image pair (similar to how the traditional
    registration methods are performed). To further improve the registration performance,
    the authors proposed a cascade framework that combines the benefits of learning-based
    registration DNNs with the optimization-based implicit neural representations
    provided by the MLP. Within this framework, the learning-based DNN predicts an
    initial deformation field, while the MLP produces the residual deformation that
    refines the initial deformation field, leading to an enhanced overall registration
    performance. However, the proposed method shares the same limitation as traditional
    methods in that the optimization is done pair-wise without learning from a dataset.
    Therefore, it cannot benefit from the supervision provided by anatomical label
    maps if these maps are not available during inference. Meanwhile, Sun et al. [[310](#bib.bib310)] applied
    implicit neural representations to a task of organ shape registration. Their approach
    was based on the idea of DeepSDF [[251](#bib.bib251)], where an auto-decoder maps
    a latent code representing a unique organ shape and the 3D coordinates of a sampled
    point to a signed distance function (SDF). The value of an SDF determines whether
    the point lies inside ($<0$), outside ($>0$), or on the surface ($=0$) of the
    shape, consequently providing an implicit description of the organ shapes. The
    resulting SDF is a continuous function, and the auto-decoder serves as the implicit
    neural representation of the discrete coordinates. To register points from different
    organ shapes, the authors modeled the trajectory of the point movement in space
    as the solution to an ODE, akin to the formulation proposed in NODEO [[347](#bib.bib347)].
    In this formulation, the time derivative corresponds to the velocity of the point
    movement at time $t$. The authors solved this ODE using a NeuralODE solver (as
    briefly discussed in section [4.5](#S4.SS5 "4.5 Neural ODEs ‣ 4 Network Architectures
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond")), resulting in a diffeomorphic mapping between
    shapes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Hyperparameter Conditioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspired by HyperNetworks [[113](#bib.bib113)] and Hyperparameter Optimization [[92](#bib.bib92)],
    recent research has introduced methods that integrate hyperparameters directly
    into the architecture of the registration DNNs. This allows for the capturing
    of a wide range of hyperparameters within a single training process, consequently
    speeding up the hyperparameter tuning process without requiring multiple networks
    to be trained from scratch for each hyperparameter value. In the training process
    of these methods, a distinct hyperparameter value is randomly selected, and the
    network generates a deformation field associated with that value. Subsequently,
    the registration loss is calculated using the same hyperparameter value, which
    is then used to update the network parameters. The hyperparameter being conditioned
    typically relates to the weight of the deformation regularizer, which affects
    the smoothness of the deformation produced by the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoopes et al. [[142](#bib.bib142)] introduced HyperMorph, which is based on
    the concept of HyperNetworks [[113](#bib.bib113)]. HyperMorph comprises two ConvNets:
    a hypernetwork and a U-Net-like registration network (i.e.VoxelMorph [[14](#bib.bib14)]).
    The hypernetwork estimates the weights of the U-Net based on the provided hyperparameter
    value for the diffusion regularizer, while the U-Net generates a deformation field
    to warp the moving image. In each training step, the hyperparameter value is randomly
    sampled from a uniform distribution, and the loss is computed using the same sampled
    value. After training, the best-performing hyperparameter value is acquired using
    gradient descent. In this process, the network weights are fixed, and an optimizer
    iteratively updates the hyperparameter based on a target objective function (commonly
    the Dice score) applied to a validation dataset. In a parallel work, Mok and Chung
    [[232](#bib.bib232)] proposed conditioning the regularization hyperparameter through
    conditional instance normalization [[76](#bib.bib76)]. In this approach, the feature
    map statistics within the regularization network are normalized and shifted according
    to two affine parameters. These affine parameters are generated by a lightweight
    mapping network, which takes the sampled hyperparameter value as input. Later,
    Chen et al. [[41](#bib.bib41)] expanded the conditional instance normalization
    to a conditional layer normalization for application in Transformer-based registration
    models. The training processes in both [[232](#bib.bib232), [41](#bib.bib41)]
    are similar to the one used in HyperMorph, where the hyperparameter value is sampled
    from a uniform distribution and then employed for loss computation. However, it
    is worth noting that Mok and Chung [[232](#bib.bib232)] and Chen et al. [[41](#bib.bib41)]
    obtain the best-performing hyperparameter value through a grid search, whereas
    HyperMorph acquires it via gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Discontinuity Permitting Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To facilitate a spatially discontinuous deformation, which is important for
    many registration applications as delineated in Section [3](#S3 "3 Loss Functions
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"), Chen et al. [[47](#bib.bib47)] proposed an alternative
    approach. Rather than employing a discontinuity-permitted deformation regularization
    (as briefly mentioned in Section [3](#S3 "3 Loss Functions ‣ A survey on deep
    learning in medical image registration: new technologies, uncertainty, evaluation
    metrics, and beyond")), the authors proposed using anatomical label maps to segregate
    the moving and fixed images into different regions of interest and subsequently
    generate deformation fields for each region using multiple registration networks.
    These deformation fields are then combined to yield a final deformation via addition.
    However, this method has an immediate drawback in necessitating the anatomical
    label maps throughout both the training and inference stages. When label maps
    are not available, this method becomes infeasible.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 Correlation Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Optical flow is the name given by the computer vision community to image registration.
    In learning-based optical flow, it is common to employ a correlation layer [[74](#bib.bib74)]
    to aid neural networks in pinpointing explicit correspondences between points
    in images. This involves computing the correlation between the neighboring features
    of a spatial location in the moving image and the neighboring features of a range
    of spatial locations in the fixed image. The correlation is computed between two
    feature patches centered at $\pmb{x}_{m}$ and $\pmb{x}_{f}$ in the moving and
    fixed images, respectively, using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c(\pmb{x}_{m},\pmb{x}_{f})=\sum_{\pmb{o}\in[-k,k]}\langle F_{m}(\pmb{x}_{m}+\pmb{o}),F_{f}(\pmb{x}_{f}+\pmb{o})\rangle,$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $F_{m}$ and $F_{f}$ denote the feature patches of the moving and fixed
    images, respectively, and $k$ defines the patch size. The selection of locations
    $\pmb{x}_{m}$ and $\pmb{x}_{f}$ is based on a maximum displacement $d$, meaning
    that for each $\pmb{x}_{m}$, the range of $\pmb{x}_{f}$ is limited to the locations
    that are at most $d$ distance away. The output of the correlation layer is a set
    of correlation values that represent the correlation between one feature patch
    in the moving image and another feature patch in the fixed image. The output has
    a size of $H\times W\times D\times d$, where $H\times W\times D$ represents the
    size of the feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the concept of directing networks with explicit correspondences between
    voxels or patches has been employed in computer vision since 2015, it was only
    recently embraced in medical image registration. This delay can be attributed
    to the potential computational challenges introduced by Eqn. [9](#S4.E9 "In 4.9
    Correlation Layer ‣ 4 Network Architectures ‣ A survey on deep learning in medical
    image registration: new technologies, uncertainty, evaluation metrics, and beyond").
    Since medical images are typically volumetric, the search space for each voxel
    location would be in a 3D volume, quickly becoming unmanageable as the search
    distance $d$ grows. [[124](#bib.bib124)] was the first to implement a correlation
    layer in their network design by introducing the PDD-net. Instead of calculating
    the scalar product between two features as done in Eqn. [9](#S4.E9 "In 4.9 Correlation
    Layer ‣ 4 Network Architectures ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond"), PDD-net computes
    the correlation as the mean squared error between feature patches centered at
    each control point in the moving and fixed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c(\pmb{x}_{m},\pmb{x}_{f})=\sum_{\pmb{o}\in[-k,k]}\&#124;F_{m}(\pmb{x}_{m}+\pmb{o})-F_{f}(\pmb{x}_{f}+\pmb{o})\&#124;^{2}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Moreover, in their correlation layer, the search distance is represented by
    a 3D matrix, $\pmb{d}^{3}$, with each element in the vector, $\pmb{d}$, defining
    a discrete displacement distance from the current center of the feature patch.
    This correlation layer produces a 6D matrix, where the first three dimensions
    outline the shape of the feature maps, and the final three dimensions describe
    the shape of the search space. Due to the sparsity of the control points in comparison
    to the image size, the computational burden of this correlation layer remains
    relatively low. The correlation layer is applied to features independently extracted
    from the moving and fixed images using a ConvNet that incorporated deformable
    convolutional layers as introduced in Heinrich et al. [[130](#bib.bib130)]. Subsequently,
    min-convolutions and mean-field inference are employed to spatially smooth the
    dissimilarities produced by the correlation layer. A softmax operation is then
    applied to the 6D matrix, converting the dissimilarities into pseudo-probabilities.
    The displacement field is subsequently generated by multiplying the probabilities
    with the displacement distance in $\pmb{d}^{3}$, resulting in a weighted average
    of these probabilistic estimates for the 3D displacement field. The deformation
    field is then trilinearly interpolated to align with the image resolution. Heinrich
    and Hansen [[126](#bib.bib126)] later extended this approach by proposing a 2.5D
    approximation of the quantized 3D displacement, significantly reducing the memory
    burden of the original Pdd-net. Instead of creating a 6D dissimilarity matrix,
    they generated three 5D matrices (i.e., the 2.5D dissimilarity matrices), with
    each matrix representing the dissimilarities computed for two out of the three
    dimensions. The 2.5D probabilities produced at the end of the network are interpolated
    to 3D using B-splines. To minimize the error during the conversion from 2.5D to
    3D, a two-step instance normalization is applied for each pair of test scans using
    gradient descent. More recently, Heinrich and Hansen [[127](#bib.bib127)] further
    expanded the concept of probabilistic displacement and incorporated keypoint supervision
    into VoxelMorph [[14](#bib.bib14)] through the introduction of VoxelMorph++. They
    advanced VoxelMorph in two respects: probabilistic displacement via heatmap prediction
    and multi-channel instance optimization using one-hot embeddings of the anatomical
    label maps generated by a segmentation network. In their model, high-level features
    are initially extracted from the VoxelMorph decoder, and feature vectors are then
    sampled at given keypoint locations. These feature vectors are converted into
    larger heatmap patches through a convolution block followed by a softmax operation.
    Consequently, each heatmap represents the probabilistic displacements of the corresponding
    keypoint. The final displacement field is generated as the sum of the displacements
    weighted by the heatmap. During the testing phase for each image pair, an instance
    optimization strategy [[293](#bib.bib293)] refines the displacement field using
    the supervision provided by the anatomical labels generated from a segmentation
    network. The methods discussed in this subsection were evaluated on abdomen and
    lung CT datasets, where large deformations are necessary for accurate registration.
    The architectures proposed in these methods proved to be efficient and demonstrated
    superior performance compared to traditional methods and learning-based networks
    that only generate dense displacement fields.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 Progressive and Pyramid Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent research has also demonstrated that employing a network to progressively
    warp a moving image towards a fixed image, or performing registration through
    a multi-scale image pyramid employing a coarse-to-fine technique, may significantly
    improve registration performance. Zhao et al. [[377](#bib.bib377)] introduced
    the VTN, which leverages cascade registration networks to align moving images
    with fixed images. Drawing inspiration from FlowNet2.0 [[153](#bib.bib153)], each
    subnetwork is responsible for aligning the current moving image with the fixed
    image, with the resulting warped image sent into the subsequent subnetwork as
    the new moving image. The final deformation field is the composition of the intermediate
    deformation fields produced by the subnetworks. This approach has been shown effective
    in handling large displacements. In a similar fashion, Chen et al. [[36](#bib.bib36)] proposed
    a method for progressive image alignment within a single network. Their method
    employs multiple convolution blocks in the decoding stage, each responsible for
    aligning the current moving image to the fixed image.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently, there have been efforts to apply progressive registration using
    a multi-scale image pyramid approach. Given the widespread adoption of hourglass-shaped
    network architectures in image registration, convolution blocks within the decoder
    generate deformation fields at multiple resolutions in a coarse-to-fine manner.
    These deformation fields at different resolutions are subsequently upsampled and
    composited to form the final deformation field. Notable methods that adopt this
    scheme include [[163](#bib.bib163), [167](#bib.bib167), [199](#bib.bib199), [209](#bib.bib209)].
    In addition to network architecture, the coarse-to-fine training scheme has also
    been adopted in learning-based image registration. Taking inspiration from conventional
    registration methods that often employ multiple stages with varying resolutions,
    De Vos et al. [[59](#bib.bib59)] pioneered a multi-scale training strategy for
    deformable image registration. Their approach involves sequentially training the
    ConvNet in each stage for a specific image resolution by optimizing the image
    similarity measure. Notably, a B-spline framework is adopted thus alleviating
    the need for a deformation regularizer. During training, the weights of the preceding
    ConvNets are held fixed, and after training, the registration is performed through
    a single pass of input images to the multi-stage ConvNets. Eppenhof et al. [[80](#bib.bib80)] proposed
    a novel progressive and multi-scale training scheme for learning-based image registration.
    Instead of training a large network on the registration task all at once, they
    first train smaller versions of the network on lower-resolution images. The resolution
    of the training images is then gradually increased, and additional convolutional
    layers are added to increase the network size. Similarly, Mok and Chung [[230](#bib.bib230)] proposed
    LapIRN, which adopts a similar pyramid training scheme. However, unlike the previous
    training approach, which progressively increases the image resolution and network
    size of the same network, LapIRN employs three different networks, each producing
    a deformation field for a specific resolution. Each network is equipped with a
    skip connection that propagates feature embeddings from a lower-resolution network
    to a higher-resolution network. The networks are trained in a coarse-to-fine manner,
    with each network producing a deformation field that refines the upsampled deformation
    field from the previous resolution. However, using multiple networks to generate
    a pyramid of deformation fields can be computationally inefficient and increase
    the network size, which can hinder training. To address this issue, Hu et al.
    [[144](#bib.bib144)] proposed a self-recursive contextual network that employs
    a single feature extractor to produce features at different resolutions. Then,
    a weight-sharing deformation generator and receptive module are then used to recursively
    generate and refine deformation fields in a coarse-to-fine manner. Since the network
    weights are shared between resolutions, this method reduces the computational
    burden and the size of the network, resulting in more efficient training. Zhou
    et al. [[379](#bib.bib379)] proposed a novel network architecture to leverage
    progressive registration at both single and multi-scale resolution. The proposed
    method iteratively refines the deformation field generated from the previous iteration,
    with each iteration composing deformation fields of various resolutions to form
    the new deformation field.
  prefs: []
  type: TYPE_NORMAL
- en: The registration methods discussed in this subsection have demonstrated the
    efficacy of decomposing the registration process into multiple steps, where each
    step refines the deformation fields from the previous step. These approaches have
    consistently shown significant performance gains while enforcing a smoother deformation
    field for image registration tasks compared to using a single network to generate
    a deformation field all at once.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Uncertainty in Learning-based Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DNNs are capable of learning complex representations. However, their predictions
    are typically deterministic and assumed to be accurate, which is usually not the
    case. Estimating the uncertainty is important for evaluating what the models learn
    from the data and helps reduce risk in decision-making based on the model prediction.
    In medical image analysis, uncertainty estimation has been widely used in tasks
    such as image segmentation, image classification, and image registration. For
    example, registration uncertainty empowers surgeons to evaluate the surgical risk
    tied to the registration model’s prediction, thereby avoiding undesirable consequences.
    Prior to the deep learning-based registration, traditional registration uncertainty
    is based on the framework of probabilistic registration, where the probabilistic
    distribution of the transformation parameters is estimated.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we focus on registration uncertainty estimation using deep
    learning methods, though many concepts have been drawn from traditional registration
    uncertainty estimation methods. We start with the general framework for estimating
    uncertainty using deep learning methods. Next, we formally define the different
    types of registration uncertainty and elaborate on how the uncertainty estimation
    methods are used in learning-based registration. Finally, we provide a summary
    of the methods used for evaluating the quality of uncertainty estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Bayesian Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8612fba4d7f2ca1dfa34132252f40ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Various types of registration uncertainty can be estimated using DNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [3](#S5.F3 "Fig. 3 ‣ 5.1 Bayesian Deep Learning ‣ 5 Uncertainty
    in Learning-based Registration ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond"), in general, uncertainty
    can be categorized into two types: aleatoric and epistemic uncertainty [[169](#bib.bib169)].
    Aleatoric uncertainty, also known as data uncertainty or inherent uncertainty,
    refers to the inherent randomness or variability present in observed data. It
    can be thought of as the variability of the data given the underlying true data
    generation model due to factors such as measurement errors, sensor noise, or the
    intrinsic stochastic nature of the data generation process. Epistemic uncertainty,
    also known as model uncertainty or knowledge uncertainty, refers to variability
    present in the model structure, model parameters, and model assumptions. It arises
    due to our limited knowledge or understanding of the underlying model. Aleatoric
    uncertainty may be reduced by improving the data quality, while epistemic uncertainty
    may be mitigated by improving model selection, refining parameter estimation,
    or acquiring additional relevant information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict aleatoric and epistemic uncertainty using deep learning, we train
    a model $W$ using data set $D$ that takes an input $x$ to generate an output $y(x,W)$
    and a variance prediction $\sigma^{2}(x,W)$. Aleatoric uncertainty describes the
    uncertainty that is inherent in the training data $D$. It is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{a}=E_{p(W&#124;D)}\left[\sigma^{2}(x,W)\right],$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $E$ represents taking the average of $\sigma^{2}(x,W)$ over the distribution
    $p(W|D)$. Epistemic uncertainty describes the uncertainty of the model $W$. It
    is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{e}=V_{p(W&#124;D)}\left[y(x,W)\right],$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'where $V$ represents taking the variance of $y(x,W)$ over the distribution
    $p(W|D)$. Directly computing aleatoric uncertainty using Eqn. [11](#S5.E11 "In
    5.1 Bayesian Deep Learning ‣ 5 Uncertainty in Learning-based Registration ‣ A
    survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond") and epistemic uncertainty using Eqn. [12](#S5.E12
    "In 5.1 Bayesian Deep Learning ‣ 5 Uncertainty in Learning-based Registration
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond") is usually impractical, as it requires the integration
    of high dimensional numerical functions. Instead, these uncertainties are approximated
    from a set of outputs by using the model weights $W$ sampled from the posterior
    distribution $p(W|D)$.'
  prefs: []
  type: TYPE_NORMAL
- en: In theory, the posterior distribution $p(W|D)$ can be obtained through Bayes’
    rule,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(W&#124;D)=\frac{p(D&#124;W)p(W)}{p(D)},$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where $p(W)$ is an assumed prior. However, it is not feasible to obtain $p(D)$
    in the denominator due to the intractable integral. As an alternative, variational
    inference is used to approximate $p(W|D)$ as a distribution $q_{\theta}(W)$ with
    parameter $\theta$ by minimizing the Kullback-Leibler (KL) divergence between
    them. This process can be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\theta}=\argmin_{\theta}D_{KL}\Big{[}q_{\theta}(W)\&#124;p(W)\Big{]}-E_{q_{\theta}}\Big{[}\log{p(D&#124;W)}\Big{]},$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D_{KL}$ represents KL divergence and $E_{q_{\theta}}$ represents taking
    the average over the distribution $q_{\theta}(W)$. With this, the aleatoric uncertainty
    in Eqn. [11](#S5.E11 "In 5.1 Bayesian Deep Learning ‣ 5 Uncertainty in Learning-based
    Registration ‣ A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond") and the epistemic uncertainty in
    Eqn. [12](#S5.E12 "In 5.1 Bayesian Deep Learning ‣ 5 Uncertainty in Learning-based
    Registration ‣ A survey on deep learning in medical image registration: new technologies,
    uncertainty, evaluation metrics, and beyond") can be approximated by sampling
    $W$ from $q_{\hat{\theta}}(W)$.'
  prefs: []
  type: TYPE_NORMAL
- en: Many sampling methods can be used for uncertainty estimation, including Monte
    Carlo dropout, bootstrap, and snapshot techniques. The Monte Carlo dropout sampling
    method operates under the assumption that $q_{\theta}(W)$ follows a Bernoulli
    distribution [[99](#bib.bib99)]. It leverages dropout layers during the testing
    phase to perform multiple forward inferences. This method is widely used in learning-based
    registration models, likely due to its straightforward implementation [[361](#bib.bib361),
    [362](#bib.bib362), [214](#bib.bib214), [37](#bib.bib37), [353](#bib.bib353)].
    Bootstrap sampling, a traditional method, involves training the registration model
    multiple times on independent training sets to produce multiple inferences [[181](#bib.bib181)].
    Snapshot sampling uses the cyclic learning rate in one training process for perturbing
    the model to converge to multiple different local minimums [[150](#bib.bib150)].
    It has shown that snapshot sampling performs better uncertainty estimation than
    other methods for the medical image registration use case [[105](#bib.bib105)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Registration Uncertainty Estimation for DNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both aleatoric and epistemic uncertainty are present in image registration.
    Aleatoric uncertainty in image registration may arise from factors such as image
    noise, image artifacts, lack of image features or image contrast, and natural
    anatomical variation between images. There may be two types of aleatoric uncertainty
    in image registration. One is that given the underlying true deformation field,
    two aligned images may not be exactly the same due to different image noise, image
    artifacts or natural anatomical variation between images. Another type is that
    multiple deformation fields may align two images with similar performance due
    to lack of image features or image contrast. On the other hand, epistemic uncertainty
    represents the limitations inherent in the modeling process. In image registration,
    this relates to the limited ability of the model to precisely capture the complex
    deformation field. This form of uncertainty can be attributed to factors like
    inadequate training data, choices in model architecture, or the inherent complexity
    posed by the inverse problem of estimating the deformation fields. The following
    subsections provide details on each type of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Aleatoric Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In medical image registration, the output of the network is usually a deformation
    field $\phi(I_{f},I_{m},W)$ as a function of the fixed image $I_{f}$, the moving
    image $I_{m}$ and the model $W$. To help the model estimate the aleatoric uncertainty
    inherent from data, the model needs to predict a variance $\sigma(I_{f},I_{m},W)$
    of the output. Assuming the deformation field $\phi$ follows a voxel-wise Gaussian
    distribution, the model $W$ can be trained by minimizing the loss function,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum_{p}\frac{(\phi(p)-\phi^{*}(p))^{2}}{\sigma^{2}(p)}+\log{\sigma^{2}(p)},$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi^{*}$ is the ground truth for the deformation field. However, in
    unsupervised learning-based image registration, the ground truth deformation $\phi^{*}$
    is unavailable, and the loss may be calculated in the image domain (*i.e.*, in
    the form of image similarity measure) rather than directly comparing the deformation
    fields as in Eqn. [15](#S5.E15 "In 5.2.1 Aleatoric Uncertainty ‣ 5.2 Registration
    Uncertainty Estimation for DNN ‣ 5 Uncertainty in Learning-based Registration
    ‣ A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"). To overcome this issue, the aleatoric uncertainty
    for image registration is frequently estimated using a variational inference strategy,
    which optimizes a global neural network to produce distributions of deformation
    fields [[56](#bib.bib56), [108](#bib.bib108), [338](#bib.bib338), [300](#bib.bib300),
    [52](#bib.bib52), [177](#bib.bib177)]. This approach circumvents the direct computation
    of the intractable posterior $p(\phi|I_{f};I_{m})$ by introducing an approximate
    posterior $q_{\theta}(\phi)$, where the parameter $\theta$ can be predicted by
    a network based on the inputs $I_{f}$ and $I_{m}$. The KL divergence between the
    two posteriors is then minimized, which maximizes the evidence lower bound (ELBO):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\theta}=\argmin_{\theta}D_{KL}\Big{[}q_{\theta}(\phi)\&#124;p(\phi)\Big{]}-E_{q_{\theta}}\Big{[}\log
    p(I_{f}&#124;\phi;I_{m})\Big{]},$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $D_{KL}$ represents KL divergence and $E_{q_{\theta}}$ represents taking
    the average over the distribution $q_{\theta}(\phi)$. Here, the approximate posterior
    $q_{\theta}(\phi)$ is frequently modeled as a multivariate normal distribution
    (i.e., $\phi\sim\mathcal{N}(\mu_{\phi},\sigma_{\phi}^{2})$). Moreover, the conditional
    probability $p(I_{f}|\phi;I_{m})$ typically takes the Gaussian form (i.e., $I_{f}\sim\mathcal{N}(I_{m}\circ\phi,\sigma_{I}^{2})$).
    In practical applications, the mean $\mu_{\phi}$ and the standard deviation $\sigma_{\phi}$
    of the deformation field can be predicted by the registration network, in a similar
    manner to a variational autoencoder. In this case, the variance $\sigma^{2}_{\phi}$
    represents the aleatoric uncertainty associated with the deformation field, given
    the input images $I_{f}$ and $I_{m}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Epistemic Uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [3](#S5.F3 "Fig. 3 ‣ 5.1 Bayesian Deep Learning ‣ 5
    Uncertainty in Learning-based Registration ‣ A survey on deep learning in medical
    image registration: new technologies, uncertainty, evaluation metrics, and beyond"),
    the epistemic uncertainty in registration can be divided into two different measures
    [[206](#bib.bib206), [353](#bib.bib353), [37](#bib.bib37)]: transformation uncertainty
    and appearance uncertainty. These measures refer to the uncertainty in generating
    the transformation and the plausibility of the transformation, respectively [[22](#bib.bib22)].
    The former quantifies the uncertainty in the deformation space and tends to be
    large when the model is uncertain about establishing specific correspondences,
    such as when registering regions with piecewise constant intensity. In contrast,
    the latter is often based on the assumption that high image similarity indicates
    correct alignment. Consequently, this uncertainty would be large when the appearance
    differences between the warped and fixed images are significant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformation uncertainty can be described as the variance of the sampled
    deformation fields, which derives from Eqn. [12](#S5.E12 "In 5.1 Bayesian Deep
    Learning ‣ 5 Uncertainty in Learning-based Registration ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond") by stochastic sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{e,trans}=\frac{1}{N}\sum_{i=1}^{N}(\phi_{i}-\frac{1}{N}\sum_{j=1}^{N}\phi_{j})^{2},$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi_{i}$ is generated by using the model weight $W_{i}$ sampled from
    the estimated variational distribution $q_{\hat{\theta}}(W)$ with the parameter
    $\hat{\theta}$ optimized by Eqn. [14](#S5.E14 "In 5.1 Bayesian Deep Learning ‣
    5 Uncertainty in Learning-based Registration ‣ A survey on deep learning in medical
    image registration: new technologies, uncertainty, evaluation metrics, and beyond"),
    and $N$ is the total sampling number. Appearance uncertainty is expressed as the
    variance of the warped images, which are created using the sampled deformation
    fields [[206](#bib.bib206), [353](#bib.bib353)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{e,appea}=\frac{1}{N}\sum_{i=1}^{N}(I_{m}\circ\phi_{i}-\frac{1}{N}\sum_{j=1}^{N}I_{m}\circ\phi_{j})^{2},$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi_{i}$ is generated by using the sampled $W_{i}$ as the model, $N$
    is the total sampling number and $I_{m}$ is the moving image. However, it should
    be noted that the uncertainty estimated using this equation for appearance uncertainty
    can be biased due to overfitting, as shown in Chen et al. [[37](#bib.bib37)].
    To correct this, the authors suggest using the following formulation instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{e,appea}=\frac{1}{N}\sum_{i=1}^{N}(I_{m}\circ\phi_{i}-I_{f})^{2},$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where the predictive mean is replaced by the fixed image $I_{f}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Uncertainty Evaluation in Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One significant challenge in uncertainty estimation lies in its evaluation due
    to the absence of ground truth, especially in unsupervised learning-based registration.
    To access the quality of uncertainty, sparsification plots are usually used for
    voxel-wise uncertainty evaluation [[213](#bib.bib213), [337](#bib.bib337), [152](#bib.bib152)].
  prefs: []
  type: TYPE_NORMAL
- en: Sparsification plots demonstrate how the registration error changes by gradually
    removing voxels ranked by the uncertainty measure. It is anticipated that removing
    a voxel with higher uncertainty will result in a greater reduction in registration
    error, and the opposite holds true for voxels with lower uncertainty. If all voxels
    are arranged in descending order of uncertainty, and the uncertainty ranking matches
    the actual registration error ranking, the accumulated registration error under
    the sparsification plot will be small. Therefore, the area under sparsification
    plots is also used as an evaluation metric to gauge the quality of the uncertainty
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Registration Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manual correspondences are usually regarded as the gold standard for evaluating
    the performance of a registration algorithm [[255](#bib.bib255)]. Landmark correspondences
    are the most frequently used type, although surfaces or lines may also serve as
    manual correspondences. The evaluation of registration performance using landmark
    correspondences is relatively simple for rigid and affine transformations, since
    these transformations can be expressed as matrix multiplication and the ground
    truth transformation can be determined through several pairs of manual landmark
    correspondences. In contrast, determining the parameters of deformable transformations
    requires dense manual landmark correspondences, which are typically not obtainable.
    Even in cases where manual landmark correspondences are available, they are often
    restricted to highly selective intensity features [[34](#bib.bib34)] and neglect
    regions with homogeneous intensities. Therefore, validating deformable registration
    algorithms is still considered a non-trivial task [[325](#bib.bib325)]. In current
    literature, the performance of deformable registration algorithms is most commonly
    evaluated in terms of accuracy and regularity.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Accuracy Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the manual landmark correspondences are available, the accuracy of the
    transformation can be evaluated by target registration error (TRE),
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{TRE}_{\text{forward}}=\sum_{i=1}^{N}&#124;&#124;T_{\text{forward}}(l_{m}^{i})-l_{f}^{i}&#124;&#124;_{k},$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: 'where $T_{\text{forward}}$ is the estimated forward transformation that takes
    the moving image to the fixed image; $l_{m}^{i}$ and $l_{f}^{i}$ are the $i^{\,\text{th}}$
    pair of landmarks in the moving and fixed image, and $k\in\{1,2\}$ denoting either
    the $\ell^{1}$-norm or $\ell^{2}$-norm. Both $l_{m}^{i}$ and $l_{f}^{i}$ as well
    as the warped moving landmark $T(l_{m}^{i})$ can be non-integer locations. Note
    that we used the forward transformation $T_{\text{forward}}$ in Eqn. [20](#S6.E20
    "In 6.1 Accuracy Measures ‣ 6 Registration Evaluation Metrics ‣ A survey on deep
    learning in medical image registration: new technologies, uncertainty, evaluation
    metrics, and beyond"), but it is more common in practice to estimate the transformation
    $T_{\text{backward}}$ that maps the fixed image to the moving image. In order
    to generate the warped image, $T_{\text{backward}}^{-1}$ can be applied in place
    of $T_{\text{forward}}$. Both $T_{\text{forward}}$ and $T_{\text{backward}}$ are
    mappings from integer locations to non-integer locations. The difference between
    these two schemes is manifested when rendering the warped image. When $T_{\text{forward}}$
    is applied to $I_{m}$, integer locations are mapped to non-integer locations,
    which necessitates interpolating scattered data [[53](#bib.bib53), [383](#bib.bib383)].
    On the other hand, $T_{\text{backward}}^{-1}$ maps non-integer locations back
    to integer locations. Thus, rendering the warped image only requires interpolating
    the moving image, which is defined on a regular grid. For algorithms that only
    output $T_{\text{backward}}$, TRE can be computed as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{TRE}_{\text{backward}}=\sum_{i=1}^{N}&#124;&#124;l_{m}^{i}-T_{\text{backward}}(l_{f}^{i})&#124;&#124;_{k}.$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: Landmark correspondences can also be generated using artificial deformations [[17](#bib.bib17),
    [286](#bib.bib286), [103](#bib.bib103)]. Different from manual landmark correspondences,
    artificial deformation can produce dense correspondences that are not limited
    to regions with highly selective intensity features. However, the performance
    of algorithms on artificial deformation may not accurately reflect their actual
    performance due to the discrepancy between the artificial and real deformations [[243](#bib.bib243),
    [260](#bib.bib260)]. To overcome this issue, many works have been focused on generating
    deformations that are more akin to those observed in practical applications. For
    instance, Lobachev et al. [[203](#bib.bib203)] proposed a pipeline for simulating
    sectioning-induced deformation fields. Vlachopoulos et al. [[328](#bib.bib328)] used
    a thin-plate kernel spline model to simulate lung deformations arising from respiration.
    Biomechanical simulation [[98](#bib.bib98), [315](#bib.bib315)] and phantoms [[346](#bib.bib346),
    [13](#bib.bib13)] are other techniques used to generate artificial deformations.
  prefs: []
  type: TYPE_NORMAL
- en: In situations where manual landmark correspondences are not available, surrogate
    measures are used to evaluate accuracy. The most straightforward measures of this
    kind include absolute intensity differences and the root-mean-square intensity
    difference between the warped image and the fixed image. Other similarity measures
    such as mutual information, structural similarity index (SSIM) can also be used.
    When anatomic labels are available, evaluating the overlaps between the warped
    and fixed label images is a popular technique. The Dice coefficient and Jaccard
    Index are examples of such measures. However, Rohlfing [[276](#bib.bib276)] demonstrated
    that by simply reordering the voxels from the moving image based on the intensity
    values ranking without any geometrical constraints, one can achieve significantly
    better performance compared with the state-of-the-art registration algorithms
    in most of the surrogate measures. They concluded that surrogate measures might
    still be useful to detect inaccurate registrations but many times they do not
    provide sufficient positive evidence for accurate registrations. Only the overlap
    of sufficiently local labels among the surrogate measures was found to distinguish
    between reasonable and poor registrations in their experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Label surface distances from segmentation maps offers an alternative to overlap
    measures. Dalca et al. [[56](#bib.bib56)] converted segmentation maps into signed
    distance functions to approximate the distance between the fixed and warped surfaces.
    They also showed that incorporating a similar surface distance loss during network
    training enhanced the surface alignment of anatomical structures. Cheng et al.
    [[48](#bib.bib48)] used the mean minimum distance (MMD), computed as the average
    Euclidean distance between manually defined surface points of anatomical structures
    and their corresponding nearest points on the warped surface, to measure the discrepancy
    between the surfaces. Additionally, the Hausdorff distance has been extensively
    used [[133](#bib.bib133)].
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies [[205](#bib.bib205), [302](#bib.bib302)] have explored the
    use of machine learning algorithms for quantifying registration errors. Compared
    to manual landmark correspondences, those methods provide dense error estimations
    that can be easily visualized. More recently, several deep learning techniques
    have been employed, offering a speed advantage over traditional machine learning
    algorithms, especially when a graphical processing unit (GPU) is available [[304](#bib.bib304)].
    Most of these methods were trained to predict the registration errors between
    a fixed and a warped image inputs. During training, artificial deformations are
    used to produce the warped image and the accuracy of these methods were validated
    using manual landmark correspondences [[81](#bib.bib81), [304](#bib.bib304)].
    Additionally, these techniques can also be applied to inter-modality registration
    tasks by incorporating an extra image synthesis step [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Regularity Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the challenge of acquiring dense manual landmark correspondences and the
    aforementioned limitation of surrogate measures, the regularity of the transformations
    is often used alongside accuracy measures to obtain a more comprehensive understanding
    of the transformations. The underlying assumption is that accurate transformations
    should be spatially smooth. Particularly, transformations that fold the space
    result in physically un-realistic anatomy structures, which usually indicate errors.
    For continuous transformations, their Jacobian determinant $|J|$ must be positive
    everywhere to avoid folding of space. This concept is extended to digital transformations
    where the number or the percentage of voxels with non-positive Jacobian determinant
    $|J|\leq 0$ are reported to measure the irregularity [[223](#bib.bib223), [199](#bib.bib199),
    [235](#bib.bib235), [62](#bib.bib62), [42](#bib.bib42), [229](#bib.bib229), [161](#bib.bib161),
    [347](#bib.bib347)]. For a 3D transformation $T(x,y,z)=[T_{x},T_{y},T_{z}]$, the
    Jacobian is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S6.E22.m1.1" class="ltx_Math" alttext="J=\vmatrix\frac{\partial
    T_{x}}{\partial x}&amp;\frac{\partial T_{x}}{\partial y}\frac{\partial T_{x}}{\partial
    z}\\ \frac{\partial T_{y}}{\partial x}\frac{\partial T_{y}}{\partial y}\frac{\partial
    T_{y}}{\partial z}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial T_{z}}{\partial x}\frac{\partial T_{z}}{\partial y}\frac{\partial
    T_{z}}{\partial z}\\
  prefs: []
  type: TYPE_NORMAL
- en: ." display="block"><semantics id="S6.E22.m1.1a"><mrow id="S6.E22.m1.1.1.1" xref="S6.E22.m1.1.1.1.1.cmml"><mrow
    id="S6.E22.m1.1.1.1.1" xref="S6.E22.m1.1.1.1.1.cmml"><mi id="S6.E22.m1.1.1.1.1.2"
    xref="S6.E22.m1.1.1.1.1.2.cmml">J</mi><mo id="S6.E22.m1.1.1.1.1.1" xref="S6.E22.m1.1.1.1.1.1.cmml">=</mo><mrow
    id="S6.E22.m1.1.1.1.1.3" xref="S6.E22.m1.1.1.1.1.3.cmml"><merror class="ltx_ERROR
    undefined undefined" id="S6.E22.m1.1.1.1.1.3.2" xref="S6.E22.m1.1.1.1.1.3.2b.cmml"><mtext
    id="S6.E22.m1.1.1.1.1.3.2a" xref="S6.E22.m1.1.1.1.1.3.2b.cmml">{vmatrix}</mtext></merror><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.3" xref="S6.E22.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.3.2"
    xref="S6.E22.m1.1.1.1.1.3.3.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.3.2.1"
    xref="S6.E22.m1.1.1.1.1.3.3.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.3.2.2"
    xref="S6.E22.m1.1.1.1.1.3.3.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.3.2.2.2" xref="S6.E22.m1.1.1.1.1.3.3.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.3.2.2.3" xref="S6.E22.m1.1.1.1.1.3.3.2.2.3.cmml">x</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.3.3" xref="S6.E22.m1.1.1.1.1.3.3.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.3.3.1" xref="S6.E22.m1.1.1.1.1.3.3.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.3.3.2" xref="S6.E22.m1.1.1.1.1.3.3.3.2.cmml">x</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1a" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mi
    mathvariant="normal" id="S6.E22.m1.1.1.1.1.3.4" xref="S6.E22.m1.1.1.1.1.3.4.cmml">&</mi><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1b" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.5" xref="S6.E22.m1.1.1.1.1.3.5.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.5.2"
    xref="S6.E22.m1.1.1.1.1.3.5.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.5.2.1"
    xref="S6.E22.m1.1.1.1.1.3.5.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.5.2.2"
    xref="S6.E22.m1.1.1.1.1.3.5.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.5.2.2.2" xref="S6.E22.m1.1.1.1.1.3.5.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.5.2.2.3" xref="S6.E22.m1.1.1.1.1.3.5.2.2.3.cmml">x</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.5.3" xref="S6.E22.m1.1.1.1.1.3.5.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.5.3.1" xref="S6.E22.m1.1.1.1.1.3.5.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.5.3.2" xref="S6.E22.m1.1.1.1.1.3.5.3.2.cmml">y</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1c" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.6" xref="S6.E22.m1.1.1.1.1.3.6.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.6.2"
    xref="S6.E22.m1.1.1.1.1.3.6.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.6.2.1"
    xref="S6.E22.m1.1.1.1.1.3.6.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.6.2.2"
    xref="S6.E22.m1.1.1.1.1.3.6.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.6.2.2.2" xref="S6.E22.m1.1.1.1.1.3.6.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.6.2.2.3" xref="S6.E22.m1.1.1.1.1.3.6.2.2.3.cmml">x</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.6.3" xref="S6.E22.m1.1.1.1.1.3.6.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.6.3.1" xref="S6.E22.m1.1.1.1.1.3.6.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.6.3.2" xref="S6.E22.m1.1.1.1.1.3.6.3.2.cmml">z</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1d" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.7" xref="S6.E22.m1.1.1.1.1.3.7.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.7.2"
    xref="S6.E22.m1.1.1.1.1.3.7.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.7.2.1"
    xref="S6.E22.m1.1.1.1.1.3.7.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.7.2.2"
    xref="S6.E22.m1.1.1.1.1.3.7.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.7.2.2.2" xref="S6.E22.m1.1.1.1.1.3.7.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.7.2.2.3" xref="S6.E22.m1.1.1.1.1.3.7.2.2.3.cmml">y</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.7.3" xref="S6.E22.m1.1.1.1.1.3.7.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.7.3.1" xref="S6.E22.m1.1.1.1.1.3.7.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.7.3.2" xref="S6.E22.m1.1.1.1.1.3.7.3.2.cmml">x</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1e" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.8" xref="S6.E22.m1.1.1.1.1.3.8.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.8.2"
    xref="S6.E22.m1.1.1.1.1.3.8.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.8.2.1"
    xref="S6.E22.m1.1.1.1.1.3.8.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.8.2.2"
    xref="S6.E22.m1.1.1.1.1.3.8.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.8.2.2.2" xref="S6.E22.m1.1.1.1.1.3.8.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.8.2.2.3" xref="S6.E22.m1.1.1.1.1.3.8.2.2.3.cmml">y</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.8.3" xref="S6.E22.m1.1.1.1.1.3.8.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.8.3.1" xref="S6.E22.m1.1.1.1.1.3.8.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.8.3.2" xref="S6.E22.m1.1.1.1.1.3.8.3.2.cmml">y</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1f" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.9" xref="S6.E22.m1.1.1.1.1.3.9.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.9.2"
    xref="S6.E22.m1.1.1.1.1.3.9.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.9.2.1"
    xref="S6.E22.m1.1.1.1.1.3.9.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.9.2.2"
    xref="S6.E22.m1.1.1.1.1.3.9.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.9.2.2.2" xref="S6.E22.m1.1.1.1.1.3.9.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.9.2.2.3" xref="S6.E22.m1.1.1.1.1.3.9.2.2.3.cmml">y</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.9.3" xref="S6.E22.m1.1.1.1.1.3.9.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.9.3.1" xref="S6.E22.m1.1.1.1.1.3.9.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.9.3.2" xref="S6.E22.m1.1.1.1.1.3.9.3.2.cmml">z</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1g" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.10" xref="S6.E22.m1.1.1.1.1.3.10.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.10.2"
    xref="S6.E22.m1.1.1.1.1.3.10.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.10.2.1"
    xref="S6.E22.m1.1.1.1.1.3.10.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.10.2.2"
    xref="S6.E22.m1.1.1.1.1.3.10.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.10.2.2.2" xref="S6.E22.m1.1.1.1.1.3.10.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.10.2.2.3" xref="S6.E22.m1.1.1.1.1.3.10.2.2.3.cmml">z</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.10.3" xref="S6.E22.m1.1.1.1.1.3.10.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.10.3.1" xref="S6.E22.m1.1.1.1.1.3.10.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.10.3.2" xref="S6.E22.m1.1.1.1.1.3.10.3.2.cmml">x</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1h" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.11" xref="S6.E22.m1.1.1.1.1.3.11.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.11.2"
    xref="S6.E22.m1.1.1.1.1.3.11.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.11.2.1"
    xref="S6.E22.m1.1.1.1.1.3.11.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.11.2.2"
    xref="S6.E22.m1.1.1.1.1.3.11.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.11.2.2.2" xref="S6.E22.m1.1.1.1.1.3.11.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.11.2.2.3" xref="S6.E22.m1.1.1.1.1.3.11.2.2.3.cmml">z</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.11.3" xref="S6.E22.m1.1.1.1.1.3.11.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.11.3.1" xref="S6.E22.m1.1.1.1.1.3.11.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.11.3.2" xref="S6.E22.m1.1.1.1.1.3.11.3.2.cmml">y</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" id="S6.E22.m1.1.1.1.1.3.1i" xref="S6.E22.m1.1.1.1.1.3.1.cmml">​</mo><mfrac
    id="S6.E22.m1.1.1.1.1.3.12" xref="S6.E22.m1.1.1.1.1.3.12.cmml"><mrow id="S6.E22.m1.1.1.1.1.3.12.2"
    xref="S6.E22.m1.1.1.1.1.3.12.2.cmml"><mo rspace="0em" id="S6.E22.m1.1.1.1.1.3.12.2.1"
    xref="S6.E22.m1.1.1.1.1.3.12.2.1.cmml">∂</mo><msub id="S6.E22.m1.1.1.1.1.3.12.2.2"
    xref="S6.E22.m1.1.1.1.1.3.12.2.2.cmml"><mi id="S6.E22.m1.1.1.1.1.3.12.2.2.2" xref="S6.E22.m1.1.1.1.1.3.12.2.2.2.cmml">T</mi><mi
    id="S6.E22.m1.1.1.1.1.3.12.2.2.3" xref="S6.E22.m1.1.1.1.1.3.12.2.2.3.cmml">z</mi></msub></mrow><mrow
    id="S6.E22.m1.1.1.1.1.3.12.3" xref="S6.E22.m1.1.1.1.1.3.12.3.cmml"><mo rspace="0em"
    id="S6.E22.m1.1.1.1.1.3.12.3.1" xref="S6.E22.m1.1.1.1.1.3.12.3.1.cmml">∂</mo><mi
    id="S6.E22.m1.1.1.1.1.3.12.3.2" xref="S6.E22.m1.1.1.1.1.3.12.3.2.cmml">z</mi></mrow></mfrac></mrow></mrow><mo
    lspace="0em" id="S6.E22.m1.1.1.1.2" xref="S6.E22.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S6.E22.m1.1b"><apply id="S6.E22.m1.1.1.1.1.cmml"
    xref="S6.E22.m1.1.1.1"><ci id="S6.E22.m1.1.1.1.1.2.cmml" xref="S6.E22.m1.1.1.1.1.2">𝐽</ci><apply
    id="S6.E22.m1.1.1.1.1.3.cmml" xref="S6.E22.m1.1.1.1.1.3"><ci id="S6.E22.m1.1.1.1.1.3.2b.cmml"
    xref="S6.E22.m1.1.1.1.1.3.2"><merror class="ltx_ERROR undefined undefined" id="S6.E22.m1.1.1.1.1.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.2"><mtext id="S6.E22.m1.1.1.1.1.3.2a.cmml" xref="S6.E22.m1.1.1.1.1.3.2">{vmatrix}</mtext></merror></ci><apply
    id="S6.E22.m1.1.1.1.1.3.3.cmml" xref="S6.E22.m1.1.1.1.1.3.3"><apply id="S6.E22.m1.1.1.1.1.3.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.3.2"><apply id="S6.E22.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.3.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.3.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.3.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.3.2.2.3">𝑥</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.3.3.cmml" xref="S6.E22.m1.1.1.1.1.3.3.3"><ci id="S6.E22.m1.1.1.1.1.3.3.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.3.3.2">𝑥</ci></apply></apply><ci id="S6.E22.m1.1.1.1.1.3.4.cmml"
    xref="S6.E22.m1.1.1.1.1.3.4">&</ci><apply id="S6.E22.m1.1.1.1.1.3.5.cmml" xref="S6.E22.m1.1.1.1.1.3.5"><apply
    id="S6.E22.m1.1.1.1.1.3.5.2.cmml" xref="S6.E22.m1.1.1.1.1.3.5.2"><apply id="S6.E22.m1.1.1.1.1.3.5.2.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.5.2.2"><csymbol cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.5.2.2.1.cmml"
    xref="S6.E22.m1.1.1.1.1.3.5.2.2">subscript</csymbol><ci id="S6.E22.m1.1.1.1.1.3.5.2.2.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.5.2.2.2">𝑇</ci><ci id="S6.E22.m1.1.1.1.1.3.5.2.2.3.cmml"
    xref="S6.E22.m1.1.1.1.1.3.5.2.2.3">𝑥</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.5.3.cmml"
    xref="S6.E22.m1.1.1.1.1.3.5.3"><ci id="S6.E22.m1.1.1.1.1.3.5.3.2.cmml" xref="S6.E22.m1.1.1.1.1.3.5.3.2">𝑦</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.6.cmml" xref="S6.E22.m1.1.1.1.1.3.6"><apply id="S6.E22.m1.1.1.1.1.3.6.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.6.2"><apply id="S6.E22.m1.1.1.1.1.3.6.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.6.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.6.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.6.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.6.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.6.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.6.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.6.2.2.3">𝑥</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.6.3.cmml" xref="S6.E22.m1.1.1.1.1.3.6.3"><ci id="S6.E22.m1.1.1.1.1.3.6.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.6.3.2">𝑧</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.7.cmml"
    xref="S6.E22.m1.1.1.1.1.3.7"><apply id="S6.E22.m1.1.1.1.1.3.7.2.cmml" xref="S6.E22.m1.1.1.1.1.3.7.2"><apply
    id="S6.E22.m1.1.1.1.1.3.7.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.7.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.7.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.7.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.7.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.7.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.7.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.7.2.2.3">𝑦</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.7.3.cmml" xref="S6.E22.m1.1.1.1.1.3.7.3"><ci id="S6.E22.m1.1.1.1.1.3.7.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.7.3.2">𝑥</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.8.cmml"
    xref="S6.E22.m1.1.1.1.1.3.8"><apply id="S6.E22.m1.1.1.1.1.3.8.2.cmml" xref="S6.E22.m1.1.1.1.1.3.8.2"><apply
    id="S6.E22.m1.1.1.1.1.3.8.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.8.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.8.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.8.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.8.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.8.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.8.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.8.2.2.3">𝑦</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.8.3.cmml" xref="S6.E22.m1.1.1.1.1.3.8.3"><ci id="S6.E22.m1.1.1.1.1.3.8.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.8.3.2">𝑦</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.9.cmml"
    xref="S6.E22.m1.1.1.1.1.3.9"><apply id="S6.E22.m1.1.1.1.1.3.9.2.cmml" xref="S6.E22.m1.1.1.1.1.3.9.2"><apply
    id="S6.E22.m1.1.1.1.1.3.9.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.9.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.9.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.9.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.9.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.9.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.9.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.9.2.2.3">𝑦</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.9.3.cmml" xref="S6.E22.m1.1.1.1.1.3.9.3"><ci id="S6.E22.m1.1.1.1.1.3.9.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.9.3.2">𝑧</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.10.cmml"
    xref="S6.E22.m1.1.1.1.1.3.10"><apply id="S6.E22.m1.1.1.1.1.3.10.2.cmml" xref="S6.E22.m1.1.1.1.1.3.10.2"><apply
    id="S6.E22.m1.1.1.1.1.3.10.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.10.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.10.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.10.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.10.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.10.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.10.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.10.2.2.3">𝑧</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.10.3.cmml" xref="S6.E22.m1.1.1.1.1.3.10.3"><ci id="S6.E22.m1.1.1.1.1.3.10.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.10.3.2">𝑥</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.11.cmml"
    xref="S6.E22.m1.1.1.1.1.3.11"><apply id="S6.E22.m1.1.1.1.1.3.11.2.cmml" xref="S6.E22.m1.1.1.1.1.3.11.2"><apply
    id="S6.E22.m1.1.1.1.1.3.11.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.11.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.11.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.11.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.11.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.11.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.11.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.11.2.2.3">𝑧</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.11.3.cmml" xref="S6.E22.m1.1.1.1.1.3.11.3"><ci id="S6.E22.m1.1.1.1.1.3.11.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.11.3.2">𝑦</ci></apply></apply><apply id="S6.E22.m1.1.1.1.1.3.12.cmml"
    xref="S6.E22.m1.1.1.1.1.3.12"><apply id="S6.E22.m1.1.1.1.1.3.12.2.cmml" xref="S6.E22.m1.1.1.1.1.3.12.2"><apply
    id="S6.E22.m1.1.1.1.1.3.12.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.12.2.2"><csymbol
    cd="ambiguous" id="S6.E22.m1.1.1.1.1.3.12.2.2.1.cmml" xref="S6.E22.m1.1.1.1.1.3.12.2.2">subscript</csymbol><ci
    id="S6.E22.m1.1.1.1.1.3.12.2.2.2.cmml" xref="S6.E22.m1.1.1.1.1.3.12.2.2.2">𝑇</ci><ci
    id="S6.E22.m1.1.1.1.1.3.12.2.2.3.cmml" xref="S6.E22.m1.1.1.1.1.3.12.2.2.3">𝑧</ci></apply></apply><apply
    id="S6.E22.m1.1.1.1.1.3.12.3.cmml" xref="S6.E22.m1.1.1.1.1.3.12.3"><ci id="S6.E22.m1.1.1.1.1.3.12.3.2.cmml"
    xref="S6.E22.m1.1.1.1.1.3.12.3.2">𝑧</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S6.E22.m1.1c">J=\vmatrix\frac{\partial T_{x}}{\partial
    x}&\frac{\partial T_{x}}{\partial y}\frac{\partial T_{x}}{\partial z}\\ \frac{\partial
    T_{y}}{\partial x}\frac{\partial T_{y}}{\partial y}\frac{\partial T_{y}}{\partial
    z}\\ \frac{\partial T_{z}}{\partial x}\frac{\partial T_{z}}{\partial y}\frac{\partial
    T_{z}}{\partial z}\\ .</annotation></semantics></math> |  | (22) |
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23f7f39f0785585a6ad3c91d33dedb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Examples of the checkerboard problem (a) and the self-intersection
    problem (b) for the central difference approximated Jacobian determinant $|J|$
    on a $3\times 3$ grid. The transformations are visualized as a displacement field
    and the displacement of the center pixel is highlighted in red. In (a), the central
    difference approximated $|J|$ for the center pixel equals one but the displacement
    of the center pixel is *not* involved in the computation. Even if the center pixel
    moves outside the field of view, the central difference approximated $|J|$ still
    equals one. In (b), the transformation around the center pixel already introduced
    folding in space regardless of the displacement of the center pixel but the central
    difference approximated $|J|$ is positive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ashburner et al. [[7](#bib.bib7)] considered the transformations to be locally
    affine, and the Jacobian determinant could be computed using singular value decomposition.
    More generally, the Jacobian of a dense nonlinear transformation is estimated
    through numerical approximation using finite difference methods. In a recent study, Liu
    et al. [[198](#bib.bib198)] showed that when approximating the Jacobian using
    forward or backward difference, it is implicitly assumed that the digital transformations
    are linearly interpolated on a tetrahedron mesh grid. Importantly, they showed
    that the Jacobian determinant, when approximated using central difference, results
    in the checkerboard and self-intersection problems. Consequently, it consistently
    underestimates non-diffeomorphic spaces. Figure [4](#S6.F4 "Fig. 4 ‣ 6.2 Regularity
    Measures ‣ 6 Registration Evaluation Metrics ‣ A survey on deep learning in medical
    image registration: new technologies, uncertainty, evaluation metrics, and beyond")
    shows examples of the checkerboard problem and the self-intersection problem in
    2D. In both cases, the central difference approximated Jacobian determinants are
    positive, but the underlying transformations introduce folding of space (under
    the assumption that the digital transformations are piecewise linear). They conclude
    that for a 2D transformation, four unique finite difference approximations of
    $|J|$’s must be positive to ensure the entire domain is invertible and free of
    folding; in 3D, ten unique finite differences approximations of $|J|$’s are required
    to be positive. Note that their method is closely related to simplex counting [[140](#bib.bib140)]
    used in deformation-based volumetric change estimation. Because of the issues
    associated with central difference approximation of $|J|$’s, Liu et al. [[198](#bib.bib198)]
    recommend using non-diffeomorphic volume to accurately reflect the non-diffeomorphism
    introduced by transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: The logarithm of the Jacobian determinant is also an important measure, especially
    for applications where it requires the volume of the underlying anatomy to be
    preserved [[277](#bib.bib277), [162](#bib.bib162)]. The logarithm is used to symmetrically
    weight local expansion and compression [[277](#bib.bib277), [183](#bib.bib183)].
    In recent works such as [[133](#bib.bib133), [37](#bib.bib37), [62](#bib.bib62),
    [42](#bib.bib42), [305](#bib.bib305)], the standard deviation of the logarithm
    of the Jacobian determinant has been used to quantify the smoothness of the displacement
    field. Additionally, the statistical distribution of the logarithm of the Jacobian
    determinant can be used as a visualization tool to reveal differences between
    registration algorithms [[187](#bib.bib187), [183](#bib.bib183)].
  prefs: []
  type: TYPE_NORMAL
- en: Similar to many surrogate measures, the regularity of the transformations can
    detect inaccurate transformations, but by itself, it is insufficient to provide
    adequate positive evidence for accurate transformations. For example, the identity
    transformation would be deemed a perfectly regularized transformation, but it
    would not provide a meaningful registration.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Applications of Medical Image Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Atlas Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In computational anatomy, atlases have been an essential tool for investigating
    the variability of human organs across populations and facilitating the segmentation
    of organs in individual patients. Typically, atlases are constructed through an
    iterative averaging process (*i.e.*, procrustean averaging [[211](#bib.bib211)])
    using a population of patient images [[3](#bib.bib3), [58](#bib.bib58), [110](#bib.bib110),
    [166](#bib.bib166), [211](#bib.bib211), [10](#bib.bib10)]. This procedure commences
    with the registration of images to a common frame of reference, followed by the
    computation of an average based on the registered images, which serves as the
    atlas for the current iteration. The iteration cycle continues until convergence
    has been achieved, resulting in the final atlas. However, these traditional methods
    tend to blur regions exhibiting high-frequency deformations [[61](#bib.bib61)].
    This shortcoming arises from the averaging of intensities when constructing the
    atlas, which invariably results in the loss of high-frequency information essential
    for capturing anatomical details.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in learning-based registration have demonstrated significant
    improvements in the quality of constructed atlases while concurrently expediting
    the atlas construction process. Dalca et al. [[55](#bib.bib55)] pioneered the
    development of a brain atlas within a deep learning framework, in which an initial
    approximation of the atlas is derived from the mean of the brain images under
    study. This atlas is then jointly optimized with a registration network, employing
    the VoxelMorph architecture to align the atlas with individual patient images.
    Throughout the training process, both the atlas and the registration network weights
    are updated. To promote an unbiased atlas and enhance spatial smoothness in the
    resulting deformation fields, the authors introduce a Gaussian-inspired prior.
    This prior serves to penalize sharp deformation changes while simultaneously encouraging
    minimal average deformation across the entire dataset. Moreover, patient demographic
    information is conditioned into the network architecture, facilitating the generation
    of conditional atlases that vary according to the specific attributes of different
    individuals. This work has inspired a variety of applications. For instance, Cheng
    et al. [[49](#bib.bib49)] establish continuous spatio-temporal cortical surface
    atlases for neonatal brains. Similarly, both Zhao et al. [[376](#bib.bib376)]
    and Bastiaansen et al. [[16](#bib.bib16)] construct continuous spatio-temporal
    atlases for fetal and infant brains. Zhao *et al.* developed a multi-scale spherical
    registration network featuring group-wise registration, while Bastiaansen *et
    al.* applied group-wise registration to volumetric ultrasound images. Alternatively, Yu
    et al. [[366](#bib.bib366)] constructed an unconditional and universal atlas while
    incorporating demographic information into the displacement field generation.
    This approach explicitly models morphological changes related to attributes as
    a diffeomorphic deformation, which captures variations in shape and size. Recognizing
    that the necessity for images to be affinely aligned in a preprocessing step as
    suggested in [[55](#bib.bib55)] could not adequately capture the dynamic size
    and shape development of fetal brain structures, Chen et al. [[43](#bib.bib43)]
    proposed incorporating an affine network, conditioned on patient demographic data,
    to register the constructed atlas to individual patient images. This approach
    preserves the dynamic size and shape variations of patients at different ages.
    Li et al. [[190](#bib.bib190)] proposed integrating the segmentation produced
    by a segmentation network into the atlas construction method proposed in [[55](#bib.bib55)].
    This method enables the joint training of segmentation and registration networks
    while simultaneously constructing both image and segmentation atlases. Similarly, Sinclair
    et al. [[298](#bib.bib298)] embraced the concept of jointly training segmentation
    and registration networks. They were motivated by the observation that segmentation
    networks often yield spurious voxel-wise predictions. By warping the label map
    of the constructed atlas to match the segmentation prediction through learning-based
    diffeomorphic registration, the topology of the original anatomical structure
    can be preserved, thus avoiding the potential segmentation errors produced by
    the segmentation network. Drawing inspiration from Dalca et al. [[55](#bib.bib55)]
    and Shu et al. [[292](#bib.bib292)], Siebert et al. [[295](#bib.bib295)] proposed
    using a shared encoder to extract features from input images, followed by two
    decoders. One decoder generates an unconditional atlas, while the other produces
    deformation fields that warp the atlas to individual images. To improve registration
    performance and enforce unbiased atlas construction, they introduced an inverse
    consistency and a bias reduction loss, in addition to the commonly seen similarity
    measure and deformation regularizer. In a related study, Wu et al. [[345](#bib.bib345)]
    proposed a closed-form update for constructing the atlas by leveraging pre-trained
    registration networks as a priori knowledge of the deformation field. Their approach
    involves an alternating update process for both the deformation field, which warps
    the atlas, and the atlas itself. This method results in an atlas construction
    framework that is independent of the registration model choice, offering flexibility
    in its application.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have explored various strategies to enhance the quality of the constructed
    atlases. Dey et al. [[61](#bib.bib61)] improved the constructed atlas by incorporating
    adversarial learning, which improved both the sharpness and centrality of the
    resulting atlas. In a similar vein, He and Chung [[123](#bib.bib123)] aimed to
    improve the atlas’ sharpness through adversarial learning and by integrating edge
    information derived from anatomical label maps. Additionally, Pei et al. [[254](#bib.bib254)]
    leveraged anatomical label maps to improve the quality of the constructed atlas
    by applying anatomical consistency supervision. However, Ding and Niethammer [[68](#bib.bib68)]
    contended that the importance of atlas sharpness is secondary to the registration
    model’s ability to align corresponding points between images in the atlas space.
    Therefore, they focused on the registration model upon which the atlas construction
    is based and proposed using the constructed atlas as a bridge. In their method,
    an image is first warped to align with an atlas and then further warped to match
    the target image using the registration network. This process facilitates a direct
    comparison between the warped image and the target image while enabling the construction
    and evaluation of the atlas without requiring segmentation of the atlas itself.
    Inspired by implicit neural shape representations [[225](#bib.bib225)],  Yang
    et al. [[359](#bib.bib359)] proposed constructing atlases of anatomical shapes
    using a continuous occupancy grid instead of representing them in a voxel-based
    manner. Given the latent representation of the shape, this alternative approach
    constructs an atlas based on the linear combination of a learned template matrix.
    Their method offers a novel perspective on atlas representation, diverging from
    traditional voxel-based representations.
  prefs: []
  type: TYPE_NORMAL
- en: Advancements in learning-based atlas construction methods have facilitated the
    fast construction of high-quality atlases. The following subsection explores the
    application of the atlases and learning-based image registration in achieving
    the goal of image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Multi-atlas Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-atlas segmentation is a well-established registration-based segmentation
    technique in existence for several decades [[278](#bib.bib278), [151](#bib.bib151)].
    The typical approach involves registering atlas images or their patches to a target
    image and fusing the propagated atlas labels. For deformable registration-based
    multi-atlas methods, the process of pairwise registration between atlas images
    and the target image can be computationally expensive and time-consuming. However,
    recent advancements in deep learning-based deformable registration algorithms
    provide a promising solution to address the speed issue and potentially improve
    the accuracy of registration, which can subsequently improve the accuracy of multi-atlas
    segmentation. While many works have explored the use of deep networks to improve
    the fusion of multiple registered atlas images [[381](#bib.bib381), [86](#bib.bib86),
    [349](#bib.bib349), [350](#bib.bib350), [358](#bib.bib358)], there are relatively
    few studies that incorporate deep learning-based deformable registration algorithms
    into their pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Ding *et al.* [[65](#bib.bib65), [66](#bib.bib66)] proposed VoteNet, which predicts
    a voxel probability of the agreement between registered atlas images and the segmentation
    target image. They adopted Quicksilver [[362](#bib.bib362)] as their registration
    algorithm to speed up the pairwise registration process. Their follow-up work [[67](#bib.bib67)]
    experimented with improving the initial registration results from Quicksilver
    by incorporating a registration refinement step. The results showed that registration
    accuracy is a critical factor in achieving accurate multi-atlas segmentation.
    In Ding et al. [[63](#bib.bib63)], the authors addressed the challenging problem
    of cross-modality multi-atlas segmentation. They proposed a deep network that
    learns the bi-directional registration between atlas images and the target image,
    as well as a second network that estimates the weights for label fusion. To account
    for the modality differences between the atlas and target images, they used Dice
    loss as a similarity measure to train their registration network and conditional
    entropy to train the fusion network. For registering 3D first-trimester ultrasound
    images, Bastiaansen et al. [[15](#bib.bib15)] proposed a two-stage network for
    learning an affine transformation. They then applied the VoxelMorph architecture
    to perform deformable registration on the affinely aligned images. The segmentation
    of the target images was achieved by propagating the labels of the atlas images
    and combining them using majority voting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good performance of supervised training in image segmentation could be
    a reason for the relative lack of research on deep learning-based registration
    in multi-atlas segmentation. Deep neural networks have demonstrated impressive
    results in supervised image segmentation tasks, making them a popular choice for
    many researchers. However, the performance of single atlas segmentation is often
    used to evaluate the accuracy of a registration algorithm, as discussed in Section [3.5](#S3.SS5
    "3.5 Auxiliary Anatomical Information ‣ 3 Loss Functions ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond"). Due to the close relationship between registration and segmentation,
    there is an increasing interest in exploring the possibility of integrating the
    learning of segmentation and registration [[298](#bib.bib298), [170](#bib.bib170),
    [355](#bib.bib355)]. Overall, the use of deep learning-based registration in multi-atlas
    segmentation is still in its early stages, and there is a significant opportunity
    for further research.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accurate registration is critical for many medical image analysis applications,
    such as image-guided surgery, radiation therapy, and longitudinal studies. However,
    registration uncertainty can arise due to factors such as training data artifacts
    or predictive model variances. To address this issue, incorporating registration
    uncertainty into medical image analysis can help guide the interpretation of the
    registration results and improve the reliability of various analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In clinical decision-making, understanding registration uncertainty is critical
    for image-guided surgery and radiation therapy. The absence of proper registration
    uncertainty awareness may lead surgeons to presume a substantial registration
    error throughout the entire region based on a large error in a single location,
    resulting in the total disregard of registration. Furthermore, the lack of registration
    uncertainty may also cause surgeons to place unwarranted confidence in regions
    with inaccurate registration, resulting in potentially severe consequences.
  prefs: []
  type: TYPE_NORMAL
- en: For image-guided surgery, Risholm et al. [[272](#bib.bib272)] showed that the
    registration uncertainty increased at the site of resection using clinical data
    from neurosurgery for resection of brain tumors, which demonstrated the potential
    utility of registration uncertainty in recognizing the surgical regions and guiding
    surgery. For radiation therapy, Risholm et al. [[271](#bib.bib271)] had previously
    presented a probabilistic framework to estimate the accumulated radiation dose
    and corresponding dose uncertainty delivered to significant anatomical structures
    during radiation therapy, such as the primary tumor and healthy surrounding organs.
    The uncertainty in the estimated dose directly results from registration uncertainty
    in the deformation used to align daily cone-beam CT images with planning CT. The
    accumulated radiation dose is an important metric to monitor during treatment,
    potentially requiring treatment plan adaptation to conform to the current patient
    anatomy.
  prefs: []
  type: TYPE_NORMAL
- en: A study by Nenoff et al. [[239](#bib.bib239)] employed six different deformable
    registration algorithms to analyze dose uncertainty in proton therapy and investigate
    their impact on dose accumulation for non-small cell lung cancer patients with
    inter-fractional anatomy variations. The results show that dose degradation caused
    by anatomical changes was more pronounced than the uncertainty arising from using
    different deformable image registration algorithms for dose accumulation. However,
    accumulated dose variations between these algorithms can still be substantial,
    leading to additional dose uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: In longitudinal medical image analysis, registration is an essential step because
    it enables the comparison of measurements taken at different time points, which
    is necessary for correcting anatomical variability and tracking changes over time.
    Registration uncertainty estimation can be beneficial for longitudinal image processing
    tasks, such as image smoothing, segmentation prior propagation, joint label fusion,
    and others. Simpson et al. [[297](#bib.bib297)] proposed an approach to calculate
    the deformable registration uncertainty using a probabilistic registration framework,
    integrating the uncertainty into spatially normalized statistics for adaptive
    image smoothing. This method showed improved classification results in longitudinal
    MR brain images acquired from Alzheimer’s Disease Neuroimaging Initiative compared
    to not smoothing or using a straightforward Gaussian filter kernel.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, incorporating registration uncertainty into medical image registration
    can facilitate interpreting registration results and improve the reliability of
    various medical image analysis tasks. It is crucial for clinicians to understand
    registration uncertainty and its potential applications in clinical decision-making.
    Further research is needed to explore other potential applications of registration
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Motion Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of medical images, deep learning-based motion estimation has
    been closely associated with the unsupervised optical flow [[164](#bib.bib164),
    [308](#bib.bib308), [19](#bib.bib19)] and point tracking [[182](#bib.bib182),
    [120](#bib.bib120), [269](#bib.bib269), [19](#bib.bib19)] techniques within the
    computer vision domain. However, the application of motion estimation in medical
    imaging presents unique challenges, including limited training data, heterogeneous
    patient data for testing, and special desired properties on the motion field,
    such as diffeomorphism (to preserve anatomical relationships) and incompressibility (to
    preserve anatomical integrity). Deep learning-based registration has demonstrated
    successful outcomes in estimating motion for various organs, such as the human
    heart, brain, lungs, and tongue. Registration-based motion estimation plays a
    significant role in enabling the assessment of changes in the position, shape,
    and size of organs over time. Multiple dynamic imaging modalities are used for
    motion estimation in medical imaging, including but not limited to cine images,
    tagged-MRI [[11](#bib.bib11), [12](#bib.bib12)], and echocardiography.
  prefs: []
  type: TYPE_NORMAL
- en: Cine images are a temporal sequence of MR images captured in quick succession,
    allowing for the monitoring of organ movement and deformation over time. Recent
    research [[263](#bib.bib263), [236](#bib.bib236), [224](#bib.bib224), [368](#bib.bib368),
    [265](#bib.bib265), [204](#bib.bib204), [367](#bib.bib367)] has successfully applied
    deep learning-based registration techniques to cine images. For example, FOAL [[368](#bib.bib368)]
    proposed online optimization to mitigate distribution mismatch between the training
    and testing datasets for motion estimation, using meta-learning techniques to
    enable more efficient online optimization with fewer gradient descent steps and
    smaller data samples, which differs from instance-specific optimization [[14](#bib.bib14)].
    Yu et al. [[367](#bib.bib367)] applied similarity and smoothness loss to multiple
    scales of motion fields (pyramid) using a deep supervision strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The relatively uniform signal within tissues from cine images and the lack of
    reliable, identifiable landmarks have motivated the exploration of additional
    regularization methods for estimating motion that is biologically plausible and
    clinically reliable. For example, Qin *et al.* [[265](#bib.bib265)] trained a
    variational autoencoder-based generative model to capture the prior of biomechanically
    plausible deformations by reconstructing simulated deformations using finite element
    models. This prior is then used as regularization during the training of the registration
    network. Lopez *et al.* [[204](#bib.bib204)] incorporated hyperelastic regularization
    terms into the framework of physics-informed neural networks [[267](#bib.bib267)]
    to estimate incompressible motion fields.
  prefs: []
  type: TYPE_NORMAL
- en: Tagged-MRI, on the other hand, employs a spatially modulated periodic pattern
    to magnetize tissue temporarily, producing transient tags in the image sequence
    that move with the tissue and capture motion information. It allows for tracking
    the motion of inner tissue where the region does not have contrast on cine images.
    DeepTag [[364](#bib.bib364)] takes raw 2D tagged images as input and estimates
    the incremental motion between two consecutive frames using a bi-directional registration
    network. Then it composes the incremental motion field to estimate motion between
    any two time frames. Harmonic phase images [[247](#bib.bib247)] have been found
    to be more robust to tag fading and imaging artifacts during motion tracking than
    raw tagged images. DRIMET [[20](#bib.bib20)] proposed a simple sinusoidal transformation
    on the harmonic phase images, enabling end-to-end training for estimating a 3D
    dense motion field from tagged-MRI. It also incorporates a Jacobian determinant-based
    loss that penalizes symmetrically for contraction and expansion to estimate a
    biologically-plaussible incompressible motion field. DRIMET shows promising results
    in terms of superior registration accuracy, a comparable degree of incompressibility,
    and faster speed over its traditional iterative-based counterparts [[351](#bib.bib351),
    [222](#bib.bib222)].
  prefs: []
  type: TYPE_NORMAL
- en: Numerous deep learning-based techniques have been devised to estimate 2D motion,
    and although this may be adequate for certain applications, tracking dense 3D
    motion is typically necessary or highly desirable when estimating the motion of
    biological structures. To address this issue, Meng *et al.* [[224](#bib.bib224)]
    integrate features extracted from multi-view 2D cine CMR images captured in both
    short-axis and long-axis planes to learn a 3D motion field of the heart. The edge
    map of myocardial wall is used as a shape regularization of the estimated motion
    field. Alternatively, DRIMET [[20](#bib.bib20)] uses sparsely acquired tagged
    images and interpolates them onto an isotropic grid with a resolution based on
    the in-plane resolution. This approach is based on the observation that the tag
    pattern changes slowly in the through-plane direction and therefore will not cause
    aliasing issues during sampling. By doing so, DRIMET is capable of tracking dense
    3D motion.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies have shown that joint learning of segmentation and motion estimation
    can be mutually beneficial [[263](#bib.bib263), [311](#bib.bib311), [1](#bib.bib1)].
    For instance, Qin *et al.* [[263](#bib.bib263)] employ a dual-branch framework
    consisting of a segmentation branch and a motion estimation branch to simultaneously
    estimate motion and segmentation from a sequence of cardiac cine images. During
    training, a shared feature encoder is learned under the premise that joint features
    can complement both tasks. In contrast, Ta *et al.* [[311](#bib.bib311)] and Ahn [[1](#bib.bib1)]
    adopt a task-level approach to jointly tackle motion estimation and segmentation
    in the context of estimating cardiac motion from echocardiography. Specifically,
    they warp the segmentation (of one time frame) using the estimated motion field
    and regularize the motion field by incorporating shape information obtained from
    the segmentation. This approach differs from previous studies which couple motion
    estimation and segmentation at the feature-level, and may offer a novel perspective
    on joint learning of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to MRIs and echocardiography, numerous deep learning-based algorithms
    have been developed for motion estimation with 4D-CT [[96](#bib.bib96), [137](#bib.bib137),
    [343](#bib.bib343), [87](#bib.bib87), [132](#bib.bib132), [158](#bib.bib158)].
    4D-CT imaging captures images at different phases of respiratory or cardiac cycles,
    providing valuable insights for lung imaging applications, including radiation
    therapy planning and lung function assessment. DIR-LAB [[34](#bib.bib34)] is a
    widely-used dataset, containing 4D CT images of ten patients, to evaluate 4D-CT
    registration techniques, with the aim of registering inspiration images to expiration
    images. This task is challenging due to the superimposed motion of the heart and
    lungs, which is larger in scale than the small lung structures being studied.
  prefs: []
  type: TYPE_NORMAL
- en: LungRegNet [[96](#bib.bib96)] trains two separate networks to handle large lung
    motion. One network predicts large motion on a coarse scale, and the other network
    takes the coarsely warped image and fixed image as input to predict fine motion.
    In addition to similarity and smoothness losses, an adversarial loss is applied
    as extra regularization to prevent unrealistic deformed images. Hering *et al.* [[132](#bib.bib132)]
    employs a coarse-to-fine multi-level optimization strategy. The deformations of
    coarse levels provide an initial guess for subsequent finer levels. Networks are
    trained progressively, with each handling one level and initialized with parameters
    from the previous level. It incorporates a penalty for volume change and utilizes
    an $l2$ loss function to match corresponding keypoints that are automatically
    detected. Ho *et al.* [[137](#bib.bib137)] applied cycle-consistent training [[179](#bib.bib179)]
    to reduce foldings using two networks. After the first network’s forward pass,
    the warped and moving images are sent to the second network to predict inverse
    deformation, with a similarity loss applied to maximize the similarity between
    the moving images and inversely-deformed moving images. IDIR [[343](#bib.bib343)]
    use a multi-layer perceptron to represent the transformation function of coordinates
    and demonstrate the ability to incorporate the Jacobian regularizer, hyperelastic
    regularizer [[29](#bib.bib29)], and bending energy [[280](#bib.bib280)] into the
    framework. The resulting deformation is void of foldings and achieves a mean target
    registration error (TRE) of 1.07 mm on DIR-LAB datasets. However, this method
    requires more time compared to CNN-based approaches, prompting researchers to
    consider acceleration as a potential future direction.
  prefs: []
  type: TYPE_NORMAL
- en: In order to accurately register previously unseen images outside of training
    datasets, the application of one-shot learning has been employed for the estimation
    of lung motion [[87](#bib.bib87), [158](#bib.bib158)]. Fechter *et al.* [[87](#bib.bib87)]
    concatenated images captured at different phases in the channel dimension in order
    to leverage temporal information. To minimize memory requirements, they partitioned
    images into non-overlapping patches and applied a boundary smoothness constraint
    on the transitions between patches. Additionally, they utilized a coarse-to-fine
    approach by constructing an image pyramid, where the estimated vector fields of
    finer scales were added to the upsampled vector fields of coarser scales. The
    proposed method showed a competitive performance without the need for training
    in advance.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 2D-3D Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent progress in the field of interventional procedures for invasive treatment
    protocols has been associated with high precision in surgeries performed at a
    reasonable cost [[256](#bib.bib256), [69](#bib.bib69)]. In these procedures, 2D-3D
    registration plays a significant role in determining the spatial relationship
    between the 3D anatomical structures and 2D images, such as X-Ray fluoroscopic
    images, ultrasound image frames, or endoscopic images. 2D-3D medical image registration
    primarily involves registering 2D interventional images to 3D pre-operative CT/MR
    images, *i.e.*, to obtain the 3D geometric transformation that aligns with the
    2D view available. Conventional 2D-3D registration methods involve iterative optimization
    methods with similarity metrics [[215](#bib.bib215)] based on image intensity
    as the objective function. Due to the sparsity of spatial information derived
    from 2D images, the problem is non-convex, which may lead to convergence at a
    local minimum if the initial estimate is not sufficiently close to the correct
    one. 2D-3D registration is a problem with a minimum of six degrees of freedom
    which may also lead to registration ambiguity as the spatial information along
    each projection line is compressed to a single point in the 2D plane. This high-dimensional
    optimization problem increases the difficulty of determining the parameters associated
    with the depth of anatomical features in the 3D volume. Alternatively, deep-learning-based
    methods have gained popularity for this application as they do not require explicit
    functional mappings [[321](#bib.bib321)]. In this discussion, we briefly highlight
    recent advancements in 2D-3D registration, while directing interested readers
    to [[321](#bib.bib321)] for a comprehensive review of the influence of various
    learning-based methods in this area.
  prefs: []
  type: TYPE_NORMAL
- en: Common 2D-3D registration applications and examples include registration of
    2D fluoroscopic/angiography images to 3D CT/MR images of pelvic, lung, or brain
    regions [[109](#bib.bib109), [193](#bib.bib193), [102](#bib.bib102), [101](#bib.bib101),
    [157](#bib.bib157), [149](#bib.bib149)], registering endoscopy images to CT/MR
    images [[197](#bib.bib197), [25](#bib.bib25)], and registering 2D Ultrasound (US)
    frames to 3D MR images to facilitate interventional procedures, such as liver
    tumor ablation [[340](#bib.bib340)] or prostate cancer biopsy[[112](#bib.bib112)].
  prefs: []
  type: TYPE_NORMAL
- en: In [[109](#bib.bib109), [340](#bib.bib340), [149](#bib.bib149)], the 2D-3D registration
    problem was modeled as a regression learning problem where the network is trained
    to directly predict the desired geometric parameters. These models are trained
    by completely relying on the data, *i.e.*, it has little to no tie to the actual
    image formation physics involved. Specifically, in [[109](#bib.bib109)] a 2D X-Ray
    image is registered to a 3D CT volume using a ConvNet, which takes the X-Ray image
    and a digitally reconstructed radiograph (DRR) from the CT volume at some known
    pose as input. The ConvNet regresses a geodesic loss function over the geometric
    parameter space to estimate the relative pose between the fixed X-ray image and
    the DRR from the CT volume without the need for accurate pose initialization.
  prefs: []
  type: TYPE_NORMAL
- en: In [[340](#bib.bib340)], Wei *et al.* propose a two-step registration process
    to determine the position and orientation of the ultrasound plane in the 3D MR
    volume data. In the first step, a ResNet-18 network is employed to determine the
    US probe orientation. Following this, a U-Net is used to regress a weighted dice
    loss function, which facilitates the determination of the orientation and position
    of the corresponding XY plane in the resampled 3D MR volume associated with the
    US frame. In [[149](#bib.bib149)], Huang *et al.* also implemented a two-step
    registration process for aligning 3D MR vessel wall images (VWI) with 2D Digital
    Subtraction Angiography (DSA) images. This approach encompasses a ConvNet regressor [[226](#bib.bib226)]
    that estimates the initial pose, followed by an instance-based centroid alignment,
    which serves to further minimize parameter estimation errors between the images.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to formulating registration as a regression problem that necessitates
    ground truth transformation parameters, several recent studies [[193](#bib.bib193),
    [102](#bib.bib102), [101](#bib.bib101), [157](#bib.bib157), [112](#bib.bib112)]
    have explored framing it as an unsupervised optimization problem. In such a formulation,
    the cost function is determined by a similarity metric measured between the transformed
    and fixed images. Liao *et al.* [[193](#bib.bib193)] trained a network to track
    a set of points of interest (POIs) derived from the 3D CT volume in the 2D DRR
    and in the multi-view fluoroscopic 2D images (used as fixed images), enabling
    the network to learn the spatial correspondences between the POIs. In this method,
    a Siamese U-Net architecture is employed to extract features from the DRRs and
    fixed images, subsequently tracking the POIs within the extracted features. A
    triangulation layer is incorporated to pinpoint the locations of the tracked POIs
    within the fixed image in 3D space. Finally, the geometric transformation between
    the estimated locations of POIs derived from the fixed image and their true positions
    is determined analytically. In [[102](#bib.bib102)], Gao *et al.* proposed a novel
    differential volume rendering transformer network combined with a feature extraction
    encoder to approximate the image similarity metric in a manner that renders the
    geometric parameter estimation as a convex problem with respect to the pose parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The examples and applications discussed thus far have primarily focused on rigid
    2D-3D registration. However, non-rigid 2D-3D registration is essential in certain
    applications, such as cephalometry [[191](#bib.bib191)] and lung tumor tracking
    in radiation therapy [[91](#bib.bib91), [70](#bib.bib70)]. Cephalometry, for instance,
    involves formulating the problem as deformed 2D-3D registration with the objective
    of generating a 3D volumetric image from a 2D X-ray image using a 3D skull atlas.
    Li *et al.* [[191](#bib.bib191)] developed a convolutional encoder that uniquely
    codes the cephalogram image into a volumetric image. The network is trained by
    minimizing the NCC between the synthesized DRR originating from the volumetric
    image and the 2D cephalogram.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous deep learning-based models and metrics have been developed to improve
    the performance of 2D-3D registration in specific applications, although these
    methods are specialized and not as versatile as traditional optimization methods.
    Nonetheless, Machine Learning/Deep Learning has been instrumental in tackling
    the persistent challenges associated with algorithmic approaches. These techniques
    have tackled a narrow optimal range of parameters, while also decreasing registration
    ambiguity. CNN-based approaches are also comparably fast. These factors encourage
    users to further improve learning-based 2D-3D registration pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Challenges and Future Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Over the past decade, learning-based registration models have been attracting
    increasing research interest. As illustrated in the left panel of Fig. [5](#S8.F5
    "Fig. 5 ‣ 8 Challenges and Future Perspectives ‣ A survey on deep learning in
    medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond"), there has been a growing trend in developing and applying these
    models since 2013\. Unlike other medical image analysis tasks, such as segmentation
    or classification, which typically necessitate labor-intensive and time-consuming
    manual annotations to develop high-performing models, registration is inherently
    a self-supervised task. Traditional registration models have predominantly been
    unsupervised, requiring only moving and fixed images to execute registration.
    While traditional registration models are typically unsupervised, learning-based
    registration models initially began as a supervised process, generating ground
    truth deformation fields using traditional registration methods. However, these
    supervised models often could not surpass the performance of traditional methods.
    Instead, they often served as an intermediate step to expedite conventional approaches
    like geodetic shooting [[290](#bib.bib290)], FLASH [[334](#bib.bib334)], etc.
    Despite traditional methods providing appealing deformation properties such as
    time-dependent diffeomorphic transformations, researchers have recently begun
    exploring the unsupervised nature of learning-based registration (as seen in the
    left panel of Fig. [5](#S8.F5 "Fig. 5 ‣ 8 Challenges and Future Perspectives ‣
    A survey on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond")). By training DNNs using loss functions adapted
    from traditional methods’ objective functions, these methods aim to improve both
    registration accuracy and speed. Incorporating segmentation and landmark correspondences
    during training can further enhance registration accuracy, providing capabilities
    not achievable with traditional methods. Given the rapid progress of deep learning
    and its growing adoption in medical applications, we anticipate an increasing
    focus on learning-based medical image registration.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we provide future perspectives and discuss potential avenues
    for advancing learning-based medical image registration. Our discussion will focus
    on the development of registration models, assessment of registration uncertainty,
    and prospective applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b19a9986819a5a8e32592aea00b7c98e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: The statistics of the paper count from PubMed (as of Fed. 13th, 2023)
    are depicted in two figures. The first figure displays the statistics obtained
    by counting the number of papers that have the keywords "Image Registration" and
    "Neural Networks" in their title or abstract. The second figure presents the statistics
    obtained by counting the papers that include the keywords "Image Registration"
    and either "Unsupervised" or "End-to-end" in their title or abstract.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Deep Learning-based Registration Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.1.1 Network Architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Network architectures employed in image registration occasionally draw inspiration
    from other image analysis tasks, such as segmentation. For instance, VoxelMorph [[14](#bib.bib14)],
    CycleMorph [[172](#bib.bib172)], SYMNet [[229](#bib.bib229)], and DiffuseMorph [[171](#bib.bib171)]
    all borrow U-Net-like architectures, originally developed for image segmentation.
    In such cases, they often generate deformation fields at a single resolution.
    In contrast, traditional registration algorithms have demonstrated the benefits
    of adopting a multi-resolution registration strategy, which decomposes deformations
    across multiple scales. This method not only improves registration performance
    but also imparts beneficial deformation properties, such as the ability to enforce
    larger deformations. This aspect can be particularly beneficial for lung or abdominal
    organ registration, where organ displacement between scans can be significant.
    As discussed in section [4.10](#S4.SS10 "4.10 Progressive and Pyramid Registration
    ‣ 4 Network Architectures ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond"), there has been
    a growing interest in integrating multi-resolution strategies into network architecture,
    and these methods have consistently demonstrated notable performance improvements
    compared to using a single resolution alone. It is worth noting that this finding
    has parallels with observations in other image analysis fields, where adopting
    deep supervision can significantly boost performance [[380](#bib.bib380), [154](#bib.bib154)].'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, registration task is intrinsically different from other tasks, as
    it requires the network to capture the correspondences between images rather than
    comprehending the context contained within the images themselves. This concept
    is exemplified by a recent work, SynthMorph [[138](#bib.bib138)], where the authors
    demonstrated that training a viable medical image registration network does not
    strictly require medical images. Instead, random shapes or synthetic images can
    also serve as training datasets for registration networks. Consequently, when
    designing network architectures for image registration, the primary focus should
    be on their ability to capture spatial correspondences between images. Architectures
    such as Transformers (particularly cross-attention Transformers), contrastive
    learning, Siamese networks, and correlation layers, which leverage comparisons
    between moving and fixed images, are of special interest for image registration.
    We expect to see an increasing number of studies incorporating these designs in
    the future, along with other advancements in deep learning applied to image registration.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Loss Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In unsupervised models, the image similarity measures predominantly used for
    mono-modal registration are MSE and NCC, as shown in Table [1](#S3.T1 "Table 1
    ‣ 3 Loss Functions ‣ A survey on deep learning in medical image registration:
    new technologies, uncertainty, evaluation metrics, and beyond"). NCC is generally
    considered a better choice than MSE, as it is locally adaptive and less sensitive
    to local intensity variations [[9](#bib.bib9)]. For multi-modal registration,
    MI has historically been the preferred choice [[215](#bib.bib215), [341](#bib.bib341)].
    However, to auto-differentiate MI using modern deep learning frameworks for end-to-end
    training, joint and marginal probabilities are often approximated using the Parzen
    window. A notable drawback of this approximation is the increased computational
    burden. In actual implementation, each voxel location expands to include a vector,
    with the elements in the vector representing the probability of the voxel belonging
    to each intensity bin. Increasing the number of intensity bins effectively results
    in an increased channel dimension, ultimately leading to a higher computational
    burden. Conversely, using a small number of bins often limits the registration
    performance. Recent learning-based methods have explored surrogates to tackle
    multi-modal registration problems. For instance, given the advantage of learning,
    anatomical loss functions like Dice can serve as a modality-independent loss function
    for training the registration network [[138](#bib.bib138)]. The trained network
    can then be applied to images without requiring anatomical segmentation, offering
    an advantage that traditional methods cannot provide. Multi-modal registration
    can also be addressed using advanced learning methods, such as contrastive learning
    and adversarial learning, as discussed in sections [4.2](#S4.SS2 "4.2 Contrastive
    Learning ‣ 4 Network Architectures ‣ A survey on deep learning in medical image
    registration: new technologies, uncertainty, evaluation metrics, and beyond")
    and [4.1](#S4.SS1 "4.1 Adversarial Learning ‣ 4 Network Architectures ‣ A survey
    on deep learning in medical image registration: new technologies, uncertainty,
    evaluation metrics, and beyond"). These methods guide the neural network in understanding
    similarities and dissimilarities between images across different modalities using
    paired data without requiring explicit multi-modal similarity measures. We expect
    future research to continue to develop more efficient and innovative approaches
    to tackle multi-modal registration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the use of regularization in learning-based deformable registration,
    there is currently an inadequate emphasis on the development and application of
    spatially-varying regularization. Despite being a significant area of research
    historically [[284](#bib.bib284), [283](#bib.bib283), [324](#bib.bib324), [307](#bib.bib307),
    [313](#bib.bib313), [258](#bib.bib258), [104](#bib.bib104), [296](#bib.bib296),
    [248](#bib.bib248), [237](#bib.bib237), [250](#bib.bib250), [97](#bib.bib97),
    [273](#bib.bib273), [249](#bib.bib249)], spatially-varying regularization has
    been largely overshadowed by the rise of learning-based registration, with only
    a few studies addressing it within a deep learning framework framework [[242](#bib.bib242),
    [290](#bib.bib290), [41](#bib.bib41), [47](#bib.bib47)]. As illustrated in Table
    [1](#S3.T1 "Table 1 ‣ 3 Loss Functions ‣ A survey on deep learning in medical
    image registration: new technologies, uncertainty, evaluation metrics, and beyond"),
    most methods opt for a simple spatially-invariant regularization, predominantly
    employing the diffusion regularizer. However, as outlined in section [3.4](#S3.SS4
    "3.4 Deformation Regularizer ‣ 3 Loss Functions ‣ A survey on deep learning in
    medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond"), spatially-varying regularization provides the advantage of accommodating
    spatially-varying deformations, preserving discontinuities, and facilitating sliding
    motion, all of which are essential for a variety of applications. Advancements
    in modeling spatially-varying regularization within or through deep learning frameworks
    are eagerly anticipated in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Registration Uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Registration uncertainty in medical image analysis is an ongoing challenge and
    opportunity. On the one hand, advancements in deep learning have the potential
    to improve registration accuracy and reduce registration uncertainty by extracting
    features that are robust to noise and other artifacts. On the other hand, uncertainty
    can be estimated for use in interpreting the registration results and providing
    valuable information for clinical decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are several limitations that restrict the further usage of uncertainty
    estimation in various applications. One significant limitation is the lack of
    ground truth for evaluating the quality of uncertainty estimation. Without ground
    truth, it is challenging to validate the accuracy of uncertainty estimation directly.
    Instead, most existing evaluation methods rely on indirect proofs such as sparsification
    analysis. This not only affects the reliability of uncertainty estimation, but
    also limits further developments for better uncertainty estimations. Another limitation
    is the computational complexity of estimating uncertainty, which can be time-consuming
    and may limit its usage in real-time clinical applications. Additionally, interpreting
    uncertainty estimates can be challenging for clinicians, as some statistical measures
    are not always straightforward. This can limit the adoption of uncertainty estimation
    in clinical decision-making, where clear and concise information is essential
    for making informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, it may be helpful to develop improved evaluation
    methods that rely on direct validation rather than indirect proofs, such as the
    creation of synthetic data or the use of simulation frameworks where the ground
    truth is known. This could enhance the accuracy and reliability of uncertainty
    estimation. Moreover, new computational techniques and algorithms such as incorporating
    Markov Chain Monte Carlo into a multilevel framework [[285](#bib.bib285)] and
    quantifying image registration uncertainty based on a low dimensional representation
    of geometric deformations [[332](#bib.bib332)] can reduce the computational workload.
    Last, efforts should be made to provide clinicians with more accessible and intuitive
    ways to interpret uncertainty estimates, such as visual aids or simpler statistic
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the limitations of uncertainty estimation in medical image analysis,
    there are also many potential applications of registration uncertainty that remain
    unexplored. One promising area of application is atlas-based segmentation, where
    registration is often used to align an atlas image to a target image for the purpose
    of segmenting anatomical structures. In this context, registration uncertainty
    can be used as a criterion for generating a soft segmentation mask, where the
    probability of each voxel belonging to a particular anatomical structure is weighted
    by the uncertainty estimate. Another potential application of registration uncertainty
    is multi-atlas-based segmentation, where multiple atlases are registered to a
    target image and combined to produce a final segmentation result. In this context,
    registration uncertainty can be used to weight different segmentation results,
    producing a more reliable segmentation. This approach could be particularly useful
    in cases where some atlases are more appropriate for a particular image than others.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these limitations and potential applications of registration uncertainty
    in medical image analysis offer an exciting range of opportunities for future
    research, and it is likely that continued progress in this area will have a substantial
    impact on the field of medical image analysis and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Towards Zero-shot Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classical registration algorithms, while potentially slower, are usually available
    for immediate use and provide end-users with the flexibility to choose the similarity
    measure and weighting of regularization terms that best meet their needs. In contrast,
    deep learning algorithms are susceptible to the domain shift problem, which arises
    when a trained network struggles to perform well when presented with input images
    from a different distribution than the training data. Several sources of domain
    shift can arise in learning-based registration algorithms, such as changes in
    the input image modality, different populations of subjects, or variations in
    the direction of registration. To address this challenge, researchers have explored
    several methods to improve the generalizability of registration networks. For
    instance, SynthMorph [[138](#bib.bib138)] uses synthetic images to force the network
    to learn contrast-invariant features, while HyperMorph [[143](#bib.bib143)] uses
    a hypernetwork to enable the adjustment of the regularization term during test
    time. Although these approaches have shown promising results, they have not yet
    been widely adopted, and further studies and validations are necessary to establish
    their effectiveness in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Recent developments in zero-shot learning offer a promising avenue for further
    improving the generalizability of learning-based registration algorithms. In particular,
    Foundation models that are pretrained on a broad range of data have shown competitive
    or even superior zero-shot performance compared to prior supervised models in
    various tasks [[174](#bib.bib174), [28](#bib.bib28)], without requiring specific
    training data for each new task. Leveraging these techniques can potentially reduce
    the time and resource requirements for developing deep learning registration algorithms
    in clinical pipelines, making the existing registration algorithms more accessible
    and useful to a wider range of users.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Metamorphic Image Registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As outlined in Section [2.3](#S2.SS3 "2.3 Diffeomorphic Image Registration
    ‣ 2 Fundamentals of Learning-based Image Registration ‣ A survey on deep learning
    in medical image registration: new technologies, uncertainty, evaluation metrics,
    and beyond"), diffeomorphic registration is a bijective mapping that preserves
    topology. In clinical scenarios, however, registration often involves the deformation
    of a healthy control or an atlas to fit patient images that may contain tumors
    or other anomalies. For example, longitudinal scans of the same patient with a
    tumor may need to be mapped to one another to facilitate the study of the tumor
    progression or response. Such situations violate the one-to-one mapping assumption
    of diffeomorphisms due to topological changes between scans. To address this challenge,
    alternative registration methods such as metamorphic registration models [[27](#bib.bib27),
    [287](#bib.bib287), [241](#bib.bib241), [141](#bib.bib141), [93](#bib.bib93)]
    have been proposed, which can accommodate changes in topology and appearance.
    For a mathematical definition of metamorphosis, readers can refer to [[319](#bib.bib319),
    [365](#bib.bib365)]. However, these methods often require manual segmentation
    of the anomalies and are optimization-based, which can be time-consuming and computationally
    expensive, thereby limiting their practical adoption.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently proposed learning-based metamorphic registration methods [[333](#bib.bib333),
    [118](#bib.bib118), [26](#bib.bib26), [219](#bib.bib219)] have been built upon
    a metamorphic framework [[319](#bib.bib319), [365](#bib.bib365)], which adds time-varying
    intensity variations on top of the diffeomorphic flow, thereby enabling topological
    changes over time. The registration networks learn to disentangle geometric and
    appearance changes and sometimes leverage available segmentation to constrain
    changes within a desired location (e.g., a tumor). With the success of learning-based
    image segmentation, the time-consuming manual segmentation previously required
    by classical methods to guide the metamorphosis can now be addressed using segmentation
    networks. Several recent approaches take advantage of segmentation networks through
    joint training [[333](#bib.bib333)] or by integrating segmentation capabilities
    directly into the registration network [[118](#bib.bib118)]. Alternatively, some
    studies learn to disentangle appearance and shape changes directly from data without
    requiring to explicitly define a region [[26](#bib.bib26), [219](#bib.bib219)].
    Despite these recent advancements, learning-based metamorphic registration is
    still in its infancy. Successfully capturing topological changes still greatly
    depends on the accuracy of the segmentation network. Meanwhile, the effective
    modeling of time-varying diffeomorphic flow using DNNs continues to be an area
    of ongoing research. Considering the practical potential of metamorphic registration,
    metamorphic registration represents an appealing direction for future investigation
    in learning-based registration research.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Spatial-temporal Image Registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Learning-based image registration methods have primarily been centered around
    aligning just one pair of images. Yet, there is a crucial but underexplored need
    in medical imaging applications: tracking tissue motion across multiple frames.
    This is particularly relevant in modalities such as tagged/cine MRIs, 4D-CT, and
    echocardiography.'
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the challenge of motion tracking involves overcoming several challenges
    and taking into account key questions that require careful consideration. For
    instance, how can one ensure the preservation of desired properties, such as smoothness,
    diffeomorphism, and incompressibility, throughout temporally long-range tracking?
    Achieving accurate 4D tracking while maintaining a reasonable computational burden
    in terms of both temporal and spatial complexity poses another challenge. Moreover,
    how can varying input frame lengths be effectively managed? These questions demand
    further exploration and investigation in order to advance our understanding and
    capability in motion tracking. Encouragingly, recent advancements in computer
    vision, particularly in the context of natural video, have demonstrated promising
    results. Methods such as correspondence learning [[155](#bib.bib155), [19](#bib.bib19),
    [373](#bib.bib373), [4](#bib.bib4)] and spatial-temporal representation learning [[173](#bib.bib173),
    [363](#bib.bib363), [331](#bib.bib331), [262](#bib.bib262), [57](#bib.bib57)]
    may offer valuable insights to tackle these challenges we face. By building upon
    these recent advancements, we may pave the way for more effective and efficient
    motion-tracking methods for medical imaging applications.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Spatial Normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spatial normalization is the process of aligning medical images to a common
    atlas to reduce anatomical variations [[8](#bib.bib8), [94](#bib.bib94)]. This
    process facilitates voxel-based analysis, allowing for comparisons between individual
    patients as well as between a patient and a larger population. Additionally, it
    is useful for mitigating anatomical differences that arise due to factors like
    motion across various image modalities from the same patient. However, a primary
    challenge of this approach is achieving accurate registration, often hindered
    by significant anatomical differences among patients. Moreover, traditionally
    constructed atlases tend to be of subpar quality, primarily because conventional
    atlas construction methods usually involve averaging, leading to significant blurring
    of anatomical features. However, as discussed in section [7.1](#S7.SS1 "7.1 Atlas
    Construction ‣ 7 Applications of Medical Image Registration ‣ A survey on deep
    learning in medical image registration: new technologies, uncertainty, evaluation
    metrics, and beyond"), deep learning is currently playing a transformative role.
    It is not only accelerating the atlas construction process but also significantly
    enhancing the quality of the atlases in aspects like contrast and sharpness. Historically,
    atlases have been largely used in brain research owing to the comparatively smaller
    anatomical differences among patients. But with the advancements introduced by
    deep learning, combined with learning-based registration models, there is potential
    to broaden their application beyond just the brain to include other body parts.
    Such an expansion can be invaluable in various medical imaging applications, from
    cancer treatment planning and monitoring tumor progression or therapy response,
    to the creation of patient-specific digital twins.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we presented a thorough examination of deep learning for medical
    image registration. In contrast to existing review papers, which might not fully
    capture the most recent advancements and tend to be systematic in nature with
    a limited focus on technical aspects, our comprehensive survey analyzed over 250
    papers with an emphasis on the most recent technological advancements. Beginning
    with a review of the fundamentals of learning-based image registration, our investigation
    incorporated widely-used and novel loss functions, as well as network architectures
    for image registration. We also thoroughly investigated the estimation methods
    of registration uncertainty and appropriate metrics of registration accuracy and
    regularity. Furthermore, we provided insights into potential clinical applications,
    future perspectives, and challenges, aiming to guide future research in this rapidly
    evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Junyu Chen and Yong Du were supported by grants from the National Institutes
    of Health (NIH), United States, U01-CA140204 (PI: Y. Du), R01-EB031023 (PI: Y. Du),
    and U01-EB031798 (PI: G. Sgouros). Yihao Liu, Shuwen Wei, Zhangxing Bian, Aaron
    Carass, and Jerry L. Prince were supported by the NIH from National Eye Institute
    grants R01-EY024655 (PI: J.L. Prince) and R01-EY032284 (PI: J.L. Prince), as well
    as the National Science Foundation grant 1819326 (Co-PI: S. Scott, Co-PI: A. Carass).
    The views expressed in written conference materials or publications and by speakers
    and moderators do not necessarily reflect the official policies of the NIH; nor
    does mention by trade names, commercial practices, or organizations imply endorsement
    by the U.S. Government.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ahn et al. [2020] Ahn, S.S., Ta, K., Lu, A., Stendahl, J.C., Sinusas, A.J.,
    Duncan, J.S., 2020. Unsupervised motion tracking of left ventricle in echocardiography,
    in: Medical imaging 2020: Ultrasonic imaging and tomography, SPIE. pp. 196–202.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aljabar et al. [2009] Aljabar, P., Heckemann, R.A., Hammers, A., Hajnal, J.V.,
    Rueckert, D., 2009. Multi-atlas based segmentation of brain images: atlas selection
    and its effect on accuracy. NeuroImage 46, 726–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allassonnière et al. [2007] Allassonnière, S., Amit, Y., Trouvé, A., 2007.
    Towards a coherent statistical framework for dense deformable template estimation.
    Journal of the Royal Statistical Society: Series B (Statistical Methodology) 69,
    3–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Araslanov et al. [2021] Araslanov, N., Schaub-Meyer, S., Roth, S., 2021. Dense
    unsupervised learning for video segmentation. Advances in Neural Information Processing
    Systems 34, 25308–25319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arsigny et al. [2006] Arsigny, V., Commowick, O., Pennec, X., Ayache, N., 2006.
    A log-euclidean framework for statistics on diffeomorphisms, in: 9${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2006),
    Springer. pp. 924–931.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashburner [2007] Ashburner, J., 2007. A fast diffeomorphic image registration
    algorithm. NeuroImage 38, 95–113.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashburner et al. [1999] Ashburner, J., Andersson, J.L., Friston, K.J., 1999.
    High-dimensional image registration using symmetric priors. NeuroImage 9, 619–628.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashburner and Friston [1999] Ashburner, J., Friston, K.J., 1999. Nonlinear spatial
    normalization using basis functions. Human brain mapping 7, 254–266.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Avants et al. [2008] Avants, B.B., Epstein, C.L., Grossman, M., Gee, J.C.,
    2008. Symmetric diffeomorphic image registration with cross-correlation: evaluating
    automated labeling of elderly and neurodegenerative brain. Medical Image Analysis
    12, 26–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avants et al. [2010] Avants, B.B., Yushkevich, P., Pluta, J., Minkoff, D., Korczykowski,
    M., Detre, J., Gee, J.C., 2010. The optimal template effect in hippocampus studies
    of diseased populations. NeuroImage 49, 2457–2466.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Axel and Dougherty [1989a] Axel, L., Dougherty, L., 1989a. Heart wall motion:
    Improved method of spatial modulation of magnetization for MR imaging. Radiology
    172, 349–350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axel and Dougherty [1989b] Axel, L., Dougherty, L., 1989b. MR imaging of motion
    with spatial modulation of magnetization. Radiology 171, 841–845.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ayyalusamy et al. [2021] Ayyalusamy, A., Vellaiyan, S., Subramanian, S., Satpathy,
    S., 2021. Performance of a deformable image registration algorithm for CT and
    cone beam CT using physical multi-density geometric and digital anatomic phantoms.
    La Radiologia Medica 126, 106–116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balakrishnan et al. [2019] Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag,
    J., Dalca, A.V., 2019. Voxelmorph: a learning framework for deformable medical
    image registration. IEEE Trans. Med. Imag. 38, 1788–1800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bastiaansen et al. [2022a] Bastiaansen, W.A., Rousian, M., Steegers-Theunissen,
    R.P., Niessen, W.J., Koning, A.H., Klein, S., 2022a. Multi-atlas segmentation
    and spatial alignment of the human embryo in first trimester 3d ultrasound. arXiv
    preprint arXiv:2202.06599 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bastiaansen et al. [2022b] Bastiaansen, W.A., Rousian, M., Steegers-Theunissen,
    R.P., Niessen, W.J., Koning, A.H., Klein, S., 2022b. Towards a 4d spatio-temporal
    atlas of the embryonic and fetal brain using a deep learning approach for groupwise
    image registration, in: Biomedical Image Registration: 10th International Workshop,
    WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings, Springer. pp. 29–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bauer et al. [2021] Bauer, D.F., Russ, T., Waldkirch, B.I., Tönnes, C., Segars,
    W.P., Schad, L.R., Zöllner, F.G., Golla, A.K., 2021. Generation of annotated multimodal
    ground truth datasets for abdominal medical image registration. International
    journal of computer assisted radiology and surgery 16, 1277–1285.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beg et al. [2005] Beg, M.F., Miller, M.I., Trouvé, A., Younes, L., 2005. Computing
    large deformation metric mappings via geodesic flows of diffeomorphisms. International
    Journal of Computer Vision 61, 139–157.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bian et al. [2022] Bian, Z., Jabri, A., Efros, A.A., Owens, A., 2022. Learning
    pixel trajectories with multiscale contrastive random walks, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6508–6519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bian et al. [2023] Bian, Z., Xing, F., Yu, J., Shao, M., Liu, Y., Carass, A.,
    Woo, J., Prince, J.L., 2023. Deep Unsupervised Phase-based 3D Incompressible Motion
    Estimation in Tagged-MRI. arXiv preprint arXiv:2301.07234 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bierbrier et al. [2023] Bierbrier, J., Eskandari, M., Di Giovanni, D.A., Collins,
    D.L., 2023. Towards Estimating MRI-Ultrasound Registration Error in Image-Guided
    Neurosurgery. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency
    Control .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bierbrier et al. [2022] Bierbrier, J., Gueziri, H.E., Collins, D.L., 2022.
    Estimating medical image registration error and confidence: A taxonomy and scoping
    review. Medical Image Analysis , 102531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blendowski et al. [2020] Blendowski, M., Bouteldja, N., Heinrich, M.P., 2020.
    Multimodal 3D medical image registration guided by shape encoder–decoder networks.
    International journal of computer assisted radiology and surgery 15, 269–276.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blendowski et al. [2021] Blendowski, M., Hansen, L., Heinrich, M.P., 2021. Weakly-supervised
    learning of multi-modal features for regularised iterative descent in 3D image
    registration. Medical Image Analysis 67, 101822.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bobrow et al. [2022] Bobrow, T.L., Golhar, M., Vijayan, R., Akshintala, V.S.,
    Garcia, J.R., Durr, N.J., 2022. Colonoscopy 3D Video Dataset with Paired Depth
    from 2D-3D Registration. arXiv preprint arXiv:2206.08903 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bône et al. [2020] Bône, A., Vernhet, P., Colliot, O., Durrleman, S., 2020.
    Learning joint shape and appearance representations with metamorphic auto-encoders,
    in: 23${}^{\mbox{\tiny{rd}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2020), Springer. pp. 202–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brett et al. [2001] Brett, M., Leff, A.P., Rorden, C., Ashburner, J., 2001.
    Spatial normalization of brain images with focal lesions using cost function masking.
    NeuroImage 14, 486–500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.
    Language models are few-shot learners. Advances in Neural Information Processing
    Systems 33, 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burger et al. [2013] Burger, M., Modersitzki, J., Ruthotto, L., 2013. A hyperelastic
    regularization energy for image registration. SIAM Journal on Scientific Computing
    35, B132–B148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabezas et al. [2011] Cabezas, M., Oliver, A., Lladó, X., Freixenet, J., Cuadra,
    M.B., 2011. A review of atlas-based segmentation for magnetic resonance brain
    images. Computer Methods and Programs in Biomedicine 104, e158–e177.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2017] Cao, X., Yang, J., Zhang, J., Nie, D., Kim, M., Wang, Q.,
    Shen, D., 2017. Deformable image registration based on similarity-steered cnn
    regression, in: 20${}^{\mbox{\tiny{th}}}$ International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI 2017), Springer. pp.
    300–308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carion et al. [2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
    A., Zagoruyko, S., 2020. End-to-end object detection with transformers, in: Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part I 16, Springer. pp. 213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Casamitjana et al. [2021] Casamitjana, A., Mancini, M., Iglesias, J.E., 2021.
    Synth-by-reg (sbr): Contrastive learning for synthesis-based registration of paired
    images, in: Simulation and Synthesis in Medical Imaging: 6th International Workshop,
    SASHIMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September
    27, 2021, Proceedings 6, Springer. pp. 44–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castillo et al. [2009] Castillo, R., Castillo, E., Guerra, R., Johnson, V.E.,
    McPhail, T., Garg, A.K., Guerrero, T., 2009. A framework for evaluation of deformable
    image registration spatial accuracy using large landmark point sets. Physics in
    Medicine & Biology 54, 1849.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021a] Chen, C.F.R., Fan, Q., Panda, R., 2021a. Crossvit: Cross-attention
    multi-scale vision transformer for image classification, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 357–366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022a] Chen, J., Frey, E.C., Du, Y., 2022a. Unsupervised learning
    of diffeomorphic image registration via transmorph, in: Biomedical Image Registration:
    10th International Workshop, WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings,
    Springer. pp. 96–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022b] Chen, J., Frey, E.C., He, Y., Segars, W.P., Li, Y., Du,
    Y., 2022b. Transmorph: Transformer for unsupervised medical image registration.
    Medical Image Analysis 82, 102615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021b] Chen, J., He, Y., Frey, E., Li, Y., Du, Y., 2021b. Vit-v-net:
    Vision transformer for unsupervised volumetric medical image registration, in:
    Medical Imaging with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Chen, J., Li, Y., Du, Y., Frey, E.C., 2020. Generating anthropomorphic
    phantoms using fully unsupervised deformable image registration with convolutional
    neural networks. Medical physics 47, 6366–6380.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023a] Chen, J., Liu, Y., He, Y., Du, Y., 2023a. Deformable cross-attention
    transformer for medical image registration. arXiv preprint arXiv:2303.06179 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023b] Chen, J., Liu, Y., He, Y., Du, Y., 2023b. Spatially-varying
    regularization with conditional transformer for unsupervised image registration.
    arXiv preprint arXiv:2303.06168 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022c] Chen, J., Lu, D., Zhang, Y., Wei, D., Ning, M., Shi, X.,
    Xu, Z., Zheng, Y., 2022c. Deformer: Towards displacement field learning for unsupervised
    medical image registration, in: 25${}^{\mbox{\tiny{th}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022), Springer.
    pp. 141–151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021c] Chen, L., Wu, Z., Hu, D., Pei, Y., Zhao, F., Sun, Y., Wang,
    Y., Lin, W., Wang, L., Li, G., et al., 2021c. Construction of longitudinally consistent
    4d infant cerebellum atlases based on deep learning, in: 24${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 139–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] Chen, R.T., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.,
    2018. Neural ordinary differential equations. Advances in Neural Information Processing
    Systems 31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021d] Chen, X., Diaz-Pinto, A., Ravikumar, N., Frangi, A.F., 2021d.
    Deep learning in medical image registration. Progress in Biomedical Engineering
    3, 012003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021e] Chen, X., Meng, Y., Zhao, Y., Williams, R., Vallabhaneni,
    S.R., Zheng, Y., 2021e. Learning unsupervised parameter-specific affine transformation
    for medical images registration, in: 24${}^{\mbox{\tiny{th}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021), Springer.
    pp. 24–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021f] Chen, X., Xia, Y., Ravikumar, N., Frangi, A.F., 2021f.
    A deep discontinuity-preserving image registration network, in: 24${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 46–55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. [2020a] Cheng, J., Dalca, A.V., Fischl, B., Zöllei, L., Alzheimer’s
    Disease Neuroimaging Initiative, et al., 2020a. Cortical surface registration
    using unsupervised learning. NeuroImage 221, 117161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2020b] Cheng, J., Dalca, A.V., Zöllei, L., 2020b. Unbiased atlas
    construction for neonatal cortical surfaces via unsupervised learning, in: Medical
    Ultrasound, and Preterm, Perinatal and Paediatric Image Analysis: First International
    Workshop, ASMUS 2020, and 5th International Workshop, PIPPI 2020, Held in Conjunction
    with MICCAI 2020, Lima, Peru, October 4-8, 2020, Proceedings 1, Springer. pp.
    334–342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christensen et al. [1997] Christensen, G.E., Joshi, S.C., Miller, M.I., 1997.
    Volumetric transformation of brain anatomy. IEEE Trans. Med. Imag. 16, 864–877.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christensen et al. [1996] Christensen, G.E., Rabbitt, R.D., Miller, M.I., 1996.
    Deformable templates using large deformation kinematics. IEEE transactions on
    image processing 5, 1435–1447.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Croquet et al. [2021] Croquet, B., Christiaens, D., Weinberg, S.M., Bronstein,
    M., Vandermeulen, D., Claes, P., 2021. Unsupervised diffeomorphic surface registration
    and non-linear modelling, in: Medical Image Computing and Computer Assisted Intervention–MICCAI
    2021: 24th International Conference, Strasbourg, France, September 27–October
    1, 2021, Proceedings, Part IV 24, Springer. pp. 118–128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crum et al. [2007] Crum, W.R., Camara, O., Hawkes, D.J., 2007. Methods for
    inverting dense displacement fields: Evaluation in brain image registration, in:
    10${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2007), Springer. pp. 900–907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Czolbe et al. [2021] Czolbe, S., Krause, O., Feragen, A., 2021. Semantic similarity
    metrics for learned image registration, in: Medical Imaging with Deep Learning,
    PMLR. pp. 105–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalca et al. [2019a] Dalca, A., Rakic, M., Guttag, J., Sabuncu, M., 2019a. Learning
    conditional deformable templates with convolutional networks. Advances in Neural
    Information Processing Systems 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalca et al. [2019b] Dalca, A.V., Balakrishnan, G., Guttag, J., Sabuncu, M.R.,
    2019b. Unsupervised learning of probabilistic diffeomorphic registration for images
    and surfaces. Medical Image Analysis 57, 226–236.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dave et al. [2022] Dave, I., Gupta, R., Rizve, M.N., Shah, M., 2022. Tclr:
    Temporal contrastive learning for video representation. Computer Vision and Image
    Understanding 219, 103406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davis et al. [2004] Davis, B., Lorenzen, P., Joshi, S.C., 2004. Large deformation
    minimum mean squared error template estimation for computational anatomy., in:
    2${}^{\mbox{\tiny{nd}}}$ International Symposium on Biomedical Imaging (ISBI 2004),
    pp. 173–176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Vos et al. [2019] De Vos, B.D., Berendsen, F.F., Viergever, M.A., Sokooti,
    H., Staring, M., Išgum, I., 2019. A deep learning framework for unsupervised affine
    and deformable image registration. Medical Image Analysis 52, 128–143.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Vos et al. [2017] De Vos, B.D., Berendsen, F.F., Viergever, M.A., Staring,
    M., Išgum, I., 2017. End-to-end unsupervised deformable image registration with
    a convolutional neural network, in: Deep Learning in Medical Image Analysis and
    Multimodal Learning for Clinical Decision Support: Third International Workshop,
    DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with
    MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3, Springer. pp.
    204–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dey et al. [2021] Dey, N., Ren, M., Dalca, A.V., Gerig, G., 2021. Generative
    adversarial registration for improved conditional deformable templates, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3929–3941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dey et al. [2022] Dey, N., Schlemper, J., Salehi, S.S.M., Zhou, B., Gerig,
    G., Sofka, M., 2022. Contrareg: Contrastive learning of multi-modality unsupervised
    deformable image registration, in: 25${}^{\mbox{\tiny{th}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022), Springer.
    pp. 66–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. [2022a] Ding, W., Li, L., Zhuang, X., Huang, L., 2022a. Cross-modality
    multi-atlas segmentation via deep registration and label fusion. IEEE Journal
    of Biomedical and Health Informatics 26, 3104–3115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2022b] Ding, X., Zhang, X., Han, J., Ding, G., 2022b. Scaling
    up your kernels to 31x31: Revisiting large kernel design in cnns, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11963–11975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2019] Ding, Z., Han, X., Niethammer, M., 2019. Votenet: A deep
    learning label fusion method for multi-atlas segmentation, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 202–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2020] Ding, Z., Han, X., Niethammer, M., 2020. Votenet+: An improved
    deep learning label fusion method for multi-atlas segmentation, in: 17${}^{\mbox{\tiny{th}}}$
    International Symposium on Biomedical Imaging (ISBI 2020), IEEE. pp. 363–367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding and Niethammer [2021] Ding, Z., Niethammer, M., 2021. Votenet++: Registration
    refinement for multi-atlas segmentation, in: 18${}^{\mbox{\tiny{th}}}$ International
    Symposium on Biomedical Imaging (ISBI 2021), IEEE. pp. 275–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding and Niethammer [2022] Ding, Z., Niethammer, M., 2022. Aladdin: Joint atlas
    building and diffeomorphic registration learning with pairwise alignment, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 20784–20793.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dlouhy et al. [2014] Dlouhy, B.J., Rao, R.C., Page, P., Julià, D., Gómez, N.,
    Codina-Cazador, A., 2014. Surgical skill and complication rates after bariatric
    surgery. The New England journal of medicine 370, 285–285.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2023] Dong, G., Dai, J., Li, N., Zhang, C., He, W., Liu, L., Chan,
    Y., Li, Y., Xie, Y., Liang, X., 2023. 2D/3D Non-Rigid Image Registration via Two
    Orthogonal X-ray Projection Images for Lung Tumor Tracking. Bioengineering 10,
    144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2022] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,
    Chen, D., Guo, B., 2022. Cswin transformer: A general vision transformer backbone
    with cross-shaped windows, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 12124–12134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Transformers
    for image recognition at scale, in: International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al., 2020. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2015] Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P.,
    Hazirbas, C., Golkov, V., Van Der Smagt, P., Cremers, D., Brox, T., 2015. Flownet:
    Learning optical flow with convolutional networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2758–2766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2019] Duan, L., Yuan, G., Gong, L., Fu, T., Yang, X., Chen, X.,
    Zheng, J., 2019. Adversarial learning for deformable registration of brain MR
    image using a multi-scale fully convolutional network. Biomedical Signal Processing
    and Control 53, 101562.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dumoulin et al. [2017] Dumoulin, V., Shlens, J., Kudlur, M., 2017. A learned
    representation for artistic style, in: International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ehrhardt et al. [2010] Ehrhardt, J., Werner, R., Schmidt-Richberg, A., Handels,
    H., 2010. Automatic landmark detection and non-linear landmark-and surface-based
    registration of lung CT images. Medical Image Analysis for the Clinic-A Grand
    Challenge, MICCAI 2010, 165–174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elmahdy et al. [2019] Elmahdy, M.S., Wolterink, J.M., Sokooti, H., Išgum, I.,
    Staring, M., 2019. Adversarial optimization for joint registration and segmentation
    in prostate CT radiotherapy, in: 22${}^{\mbox{\tiny{nd}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019), Springer.
    pp. 366–374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eppenhof et al. [2018] Eppenhof, K.A., Lafarge, M.W., Moeskops, P., Veta, M.,
    Pluim, J.P., 2018. Deformable image registration using convolutional neural networks,
    in: Medical Imaging 2018: Image Processing, SPIE. pp. 192–197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eppenhof et al. [2019] Eppenhof, K.A., Lafarge, M.W., Veta, M., Pluim, J.P.,
    2019. Progressively trained convolutional neural networks for deformable image
    registration. IEEE Trans. Med. Imag. 39, 1594–1604.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eppenhof and Pluim [2018a] Eppenhof, K.A., Pluim, J.P., 2018a. Error estimation
    of deformable image registration of pulmonary CT scans using convolutional neural
    networks. Journal of Medical Imaging 5, 024003–024003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eppenhof and Pluim [2018b] Eppenhof, K.A., Pluim, J.P., 2018b. Pulmonary CT
    registration through supervised learning with convolutional neural networks. IEEE
    Trans. Med. Imag. 38, 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. [2019a] Fan, J., Cao, X., Wang, Q., Yap, P.T., Shen, D., 2019a. Adversarial
    learning for mono-or multi-modal registration. Medical Image Analysis 58, 101545.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. [2018] Fan, J., Cao, X., Xue, Z., Yap, P.T., Shen, D., 2018. Adversarial
    similarity network for evaluating image alignment in deep learning based registration,
    in: 21${}^{\mbox{\tiny{st}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2018), Springer. pp. 739–746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. [2019b] Fan, J., Cao, X., Yap, P.T., Shen, D., 2019b. Birnet: Brain
    image registration using dual-supervised fully convolutional networks. Medical
    Image Analysis 54, 193–206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. [2019] Fang, L., Zhang, L., Nie, D., Cao, X., Rekik, I., Lee, S.W.,
    He, H., Shen, D., 2019. Automatic brain labeling via multi-atlas guided fully
    convolutional networks. Medical Image Analysis 51, 157–168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fechter and Baltas [2020] Fechter, T., Baltas, D., 2020. One-shot learning for
    deformable medical image registration and periodic motion tracking. IEEE Trans.
    Med. Imag. 39, 2506–2517.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fischer and Modersitzki [2003a] Fischer, B., Modersitzki, J., 2003a. Combination
    of automatic non-rigid and landmark-based registration: the best of both worlds,
    in: Medical Imaging 2003: Image Processing, SPIE. pp. 1037–1048.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischer and Modersitzki [2003b] Fischer, B., Modersitzki, J., 2003b. Curvature
    based image registration. Journal of Mathematical Imaging and Vision 18, 81–85.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluck et al. [2011] Fluck, O., Vetter, C., Wein, W., Kamen, A., Preim, B., Westermann,
    R., 2011. A survey of medical image registration on graphics hardware. Computer
    Methods and Programs in Biomedicine 104, e45–e57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Foote et al. [2019] Foote, M.D., Zimmerman, B.E., Sawant, A., Joshi, S.C.,
    2019. Real-time 2D-3D deformable registration with deep learning and application
    to lung radiotherapy targeting, in: Information Processing in Medical Imaging:
    26th International Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings
    26, Springer. pp. 265–276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Franceschi et al. [2018] Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R.,
    Pontil, M., 2018. Bilevel programming for hyperparameter optimization and meta-learning,
    in: 35${}^{\mbox{\tiny{th}}}$ International Conference on Machine Learning (ICML 2016),
    PMLR. pp. 1568–1577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'François et al. [2022] François, A., Maillard, M., Oppenheim, C., Pallud, J.,
    Bloch, I., Gori, P., Glaunès, J., 2022. Weighted metamorphosis for registration
    of images with different topologies, in: Biomedical Image Registration: 10th International
    Workshop, WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings, Springer.
    pp. 8–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Friston et al. [1995] Friston, K.J., Ashburner, J., Frith, C.D., Poline, J.B.,
    Heather, J.D., Frackowiak, R.S., 1995. Spatial registration and normalization
    of images. Human brain mapping 3, 165–189.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2020a] Fu, Y., Lei, Y., Wang, T., Curran, W.J., Liu, T., Yang, X.,
    2020a. Deep learning in medical image registration: a review. Physics in Medicine
    & Biology 65, 20TR01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2020b] Fu, Y., Lei, Y., Wang, T., Higgins, K., Bradley, J.D., Curran,
    W.J., Liu, T., Yang, X., 2020b. LungRegNet: an unsupervised deformable image registration
    method for 4D-CT lung. Medical physics 47, 1763–1774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2018] Fu, Y., Liu, S., Li, H.H., Li, H., Yang, D., 2018. An adaptive
    motion regularization technique to support sliding motion in deformable image
    registration. Medical physics 45, 735–747.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2021] Fu, Y., Wang, T., Lei, Y., Patel, P., Jani, A.B., Curran, W.J.,
    Liu, T., Yang, X., 2021. Deformable MR-CBCT prostate registration using biomechanically
    constrained deep learning networks. Medical Physics 48, 253–263.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal and Ghahramani [2016] Gal, Y., Ghahramani, Z., 2016. Dropout as a bayesian
    approximation: Representing model uncertainty in deep learning, in: 33${}^{\mbox{\tiny{rd}}}$
    International Conference on Machine Learning (ICML 2016), PMLR. pp. 1050–1059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ganser et al. [2004] Ganser, K.A., Dickhaus, H., Metzner, R., Wirtz, C.R., 2004.
    A deformable digital brain atlas system according to talairach and tournoux. Medical
    Image Analysis 8, 3–22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020a] Gao, C., Grupp, R.B., Unberath, M., Taylor, R.H., Armand,
    M., 2020a. Fiducial-free 2D/3D registration of the proximal femur for robot-assisted
    femoroplasty, in: Medical Imaging 2020: Image-Guided Procedures, Robotic Interventions,
    and Modeling, SPIE. pp. 350–355.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020b] Gao, C., Liu, X., Gu, W., Killeen, B., Armand, M., Taylor,
    R., Unberath, M., 2020b. Generalizing spatial transformers to projective geometry
    with applications to 2D/3D registration, in: 23${}^{\mbox{\tiny{rd}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020),
    Springer. pp. 329–339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ger et al. [2017] Ger, R.B., Yang, J., Ding, Y., Jacobsen, M.C., Fuller, C.D.,
    Howell, R.M., Li, H., Jason Stafford, R., Zhou, S., Court, L.E., 2017. Accuracy
    of deformable image registration on magnetic resonance images in digital and physical
    phantoms. Medical Physics 44, 5153–5161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gerig et al. [2014] Gerig, T., Shahim, K., Reyes, M., Vetter, T., Lüthi, M.,
    2014. Spatially varying registration using gaussian processes, in: 17${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2014),
    Springer. pp. 413–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2022] Gong, X., Khaidem, L., Zhu, W., Zhang, B., Doermann, D.,
    2022. Uncertainty learning towards unsupervised deformable medical image registration,
    in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision, pp. 2484–2493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2020] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2020. Generative adversarial
    networks. Communications of the ACM 63, 139–144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greer et al. [2021] Greer, H., Kwitt, R., Vialard, F.X., Niethammer, M., 2021.
    Icon: Learning regular maps through inverse consistency, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3396–3405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grzech et al. [2022] Grzech, D., Azampour, M.F., Glocker, B., Schnabel, J.,
    Navab, N., Kainz, B., Le Folgoc, L., 2022. A variational bayesian method for similarity
    learning in non-rigid image registration, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 119–128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. [2020] Gu, W., Gao, C., Grupp, R., Fotouhi, J., Unberath, M., 2020.
    Extended Capture Range of Rigid 2D/3D Registration by Estimating Riemannian Pose
    Gradients. Machine learning in medical imaging. MLMI 12436, 281–291.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guimond et al. [2000] Guimond, A., Meunier, J., Thirion, J.P., 2000. Average
    brain models: A convergence study. Computer Vision and Image Understanding 77,
    192–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo [2019] Guo, C.K., 2019. Multi-modal image registration with unsupervised
    deep learning. Ph.D. thesis. Massachusetts Institute of Technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2022] Guo, H., Xu, X., Song, X., Xu, S., Chao, H., Myers, J., Turkbey,
    B., Pinto, P.A., Wood, B.J., Yan, P., 2022. Ultrasound frame-to-volume registration
    via deep learning for interventional guidance. IEEE Transactions on Ultrasonics,
    Ferroelectrics, and Frequency Control .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ha et al. [2017] Ha, D., Dai, A.M., Le, Q.V., 2017. Hypernetworks, in: International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haber and Modersitzki [2006] Haber, E., Modersitzki, J., 2006. Intensity gradient
    based registration and fusion of multi-modal images, in: 9${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2006),
    Springer. pp. 726–733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2023] Han, K., Sun, S., Yan, X., You, C., Tang, H., Naushad, J.,
    Ma, H., Kong, D., Xie, X., 2023. Diffeomorphic image registration with neural
    velocity field, in: Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision, pp. 1869–1879.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2022] Han, R., Jones, C.K., Lee, J., Wu, P., Vagdargi, P., Uneri,
    A., Helm, P.A., Luciano, M., Anderson, W.S., Siewerdsen, J.H., 2022. Deformable
    mr-ct image registration using an unsupervised, dual-channel network for neurosurgical
    guidance. Medical image analysis 75, 102292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2021] Han, X., Hong, J., Reyngold, M., Crane, C., Cuaron, J., Hajj,
    C., Mann, J., Zinovoy, M., Greer, H., Yorke, E., et al., 2021. Deep-learning-based
    image registration and automatic segmentation of organs-at-risk in cone-beam CT
    scans from high-dose radiation treatment of pancreatic cancer. Medical physics
    48, 3084–3095.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2020] Han, X., Shen, Z., Xu, Z., Bakas, S., Akbari, H., Bilello,
    M., Davatzikos, C., Niethammer, M., 2020. A deep network for joint registration
    and reconstruction of images with pathologies, in: Machine Learning in Medical
    Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI
    2020, Lima, Peru, October 4, 2020, Proceedings 11, Springer. pp. 342–352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hansen and Heinrich [2021] Hansen, L., Heinrich, M.P., 2021. GraphRegNet: Deep
    graph regularisation networks on sparse keypoints for dense registration of 3D
    lung CTs. IEEE Trans. Med. Imag. 40, 2246–2257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harley et al. [2022] Harley, A.W., Fang, Z., Fragkiadaki, K., 2022. Particle
    video revisited: Tracking through occlusions using point trajectories, in: Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXII, Springer. pp. 59–75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haskins et al. [2019] Haskins, G., Kruecker, J., Kruger, U., Xu, S., Pinto,
    P.A., Wood, B.J., Yan, P., 2019. Learning deep similarity metric for 3D MR–TRUS
    image registration. International journal of computer assisted radiology and surgery
    14, 417–425.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He and Chung [2021] He, Z., Chung, A.C., 2021. Learning-based template synthesis
    for groupwise image registration, in: Simulation and Synthesis in Medical Imaging:
    6th International Workshop, SASHIMI 2021, Held in Conjunction with MICCAI 2021,
    Strasbourg, France, September 27, 2021, Proceedings 6, Springer. pp. 55–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich [2019] Heinrich, M.P., 2019. Closing the gap between deep and conventional
    image registration using probabilistic dense displacement networks, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 50–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. [2015] Heinrich, M.P., Handels, H., Simpson, I.J., 2015. Estimating
    large lung motion in copd patients by symmetric regularised correspondence fields,
    in: 18${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2015), Springer. pp. 338–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich and Hansen [2020] Heinrich, M.P., Hansen, L., 2020. Highly accurate
    and memory efficient unsupervised learning-based discrete CT registration using
    2.5 D displacement search, in: 23${}^{\mbox{\tiny{rd}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020), Springer.
    pp. 190–200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich and Hansen [2022] Heinrich, M.P., Hansen, L., 2022. Voxelmorph++ going
    beyond the cranial vault with keypoint supervision and multi-channel instance
    optimisation, in: Biomedical Image Registration: 10th International Workshop,
    WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings, Springer. pp. 85–95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. [2012] Heinrich, M.P., Jenkinson, M., Bhushan, M., Matin, T.,
    Gleeson, F.V., Brady, M., Schnabel, J.A., 2012. Mind: Modality independent neighbourhood
    descriptor for multi-modal deformable registration. Medical Image Analysis 16,
    1423–1435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. [2013] Heinrich, M.P., Jenkinson, M., Papież, B.W., Brady,
    S.M., Schnabel, J.A., 2013. Towards realtime multimodal fusion for image-guided
    interventions using self-similarities, in: 16${}^{\mbox{\tiny{th}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2013),
    Springer. pp. 187–194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. [2019] Heinrich, M.P., Oktay, O., Bouteldja, N., 2019. OBELISK-Net:
    Fewer layers to solve 3D multi-organ segmentation with sparse deformable convolutions.
    Medical Image Analysis 54, 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hering et al. [2019] Hering, A., Ginneken, B.v., Heldmann, S., 2019. mlvirnet:
    Multilevel variational image registration network, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 257–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hering et al. [2021] Hering, A., Häger, S., Moltz, J., Lessmann, N., Heldmann,
    S., van Ginneken, B., 2021. CNN-based lung CT registration with multiple anatomical
    constraints. Medical Image Analysis 72, 102139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hering et al. [2022] Hering, A., Hansen, L., Mok, T.C., Chung, A.C., Siebert,
    H., Häger, S., Lange, A., Kuckertz, S., Heldmann, S., Shao, W., et al., 2022.
    Learn2reg: comprehensive multi-task medical image registration challenge, dataset
    and evaluation in the era of deep learning. IEEE Trans. Med. Imag. .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernandez et al. [2009] Hernandez, M., Bossa, M.N., Olmos, S., 2009. Registration
    of anatomical images using paths of diffeomorphisms parameterized with stationary
    vector field flows. International Journal of Computer Vision 85, 291–306.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hill et al. [2001] Hill, D.L., Batchelor, P.G., Holden, M., Hawkes, D.J., 2001.
    Medical image registration. Physics in Medicine & Biology 46, R1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. [2020] Ho, J., Jain, A., Abbeel, P., 2020. Denoising diffusion probabilistic
    models. Advances in Neural Information Processing Systems 33, 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. [2023] Ho, T.T., Kim, W.J., Lee, C.H., Jin, G.Y., Chae, K.J., Choi,
    S., 2023. An unsupervised image registration method employing chest computed tomography
    images and deep neural networks. Computers in Biology and Medicine 154, 106612.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoffmann et al. [2021] Hoffmann, M., Billot, B., Greve, D.N., Iglesias, J.E.,
    Fischl, B., Dalca, A.V., 2021. Synthmorph: learning contrast-invariant registration
    without acquired images. IEEE Trans. Med. Imag. 41, 543–558.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hofmann et al. [2008] Hofmann, M., Steinke, F., Scheel, V., Charpiat, G., Farquhar,
    J., Aschoff, P., Brady, M., Schölkopf, B., Pichler, B.J., 2008. MRI-based attenuation
    correction for PET/MRI: a novel approach combining pattern recognition and atlas
    registration. Journal of nuclear medicine 49, 1875–1883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holland et al. [2011] Holland, D., Dale, A.M., Alzheimer’s Disease Neuroimaging
    Initiative, et al., 2011. Nonlinear registration of longitudinal images and measurement
    of change in regions of interest. Medical Image Analysis 15, 489–497.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2012] Hong, Y., Joshi, S., Sanchez, M., Styner, M., Niethammer,
    M., 2012. Metamorphic geodesic regression, in: 15${}^{\mbox{\tiny{th}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2012),
    Springer. pp. 197–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoopes et al. [2022] Hoopes, A., Hoffman, M., Greve, D.N., Fischl, B., Guttag,
    J., Dalca, A.V., 2022. Learning the effect of registration hyperparameters with
    hypermorph. Machine Learning for Biomedical Imaging 1, 1–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoopes et al. [2021] Hoopes, A., Hoffmann, M., Fischl, B., Guttag, J., Dalca,
    A.V., 2021. Hypermorph: Amortized hyperparameter learning for image registration,
    in: Information Processing in Medical Imaging: 27th International Conference,
    IPMI 2021, Virtual Event, June 28–June 30, 2021, Proceedings 27, Springer. pp.
    3–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2020] Hu, B., Zhou, S., Xiong, Z., Wu, F., 2020. Self-recursive
    contextual network for unsupervised 3D medical image registration, in: Machine
    Learning in Medical Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction
    with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings 11, Springer. pp. 60–69.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2019a] Hu, J., Sun, S., Yang, X., Zhou, S., Wang, X., Fu, Y., Zhou,
    J., Yin, Y., Cao, K., Song, Q., et al., 2019a. Towards accurate and robust multi-modal
    medical image registration using contrastive metric learning. IEEE Access 7, 132816–132827.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2019b] Hu, X., Kang, M., Huang, W., Scott, M.R., Wiest, R., Reyes,
    M., 2019b. Dual-stream pyramid registration network, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 382–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2018a] Hu, Y., Modat, M., Gibson, E., Ghavami, N., Bonmati, E.,
    Moore, C.M., Emberton, M., Noble, J.A., Barratt, D.C., Vercauteren, T., 2018a.
    Label-driven weakly-supervised learning for multimodal deformable image registration,
    in: 15${}^{\mbox{\tiny{th}}}$ International Symposium on Biomedical Imaging (ISBI 2018),
    IEEE. pp. 1070–1074.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2018b] Hu, Y., Modat, M., Gibson, E., Li, W., Ghavami, N., Bonmati,
    E., Wang, G., Bandula, S., Moore, C.M., Emberton, M., et al., 2018b. Weakly-supervised
    convolutional neural networks for multimodal image registration. Medical Image
    Analysis 49, 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022] Huang, D.X., Zhou, X.H., Xie, X.L., Liu, S.Q., Feng, Z.Q.,
    Hao, J.L., Hou, Z.G., Ma, N., Yan, L., 2022. A Novel Two-Stage Framework for 2D/3D
    Registration in Neurological Interventions, in: 2022 IEEE International Conference
    on Robotics and Biomimetics (ROBIO), IEEE. pp. 266–271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2017] Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E.,
    Weinberger, K.Q., 2017. Snapshot ensembles: Train 1, get m for free, in: International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iglesias and Sabuncu [2015] Iglesias, J.E., Sabuncu, M.R., 2015. Multi-atlas
    segmentation of biomedical images: a survey. Medical Image Analysis 24, 205–219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilg et al. [2018] Ilg, E., Cicek, O., Galesso, S., Klein, A., Makansi, O.,
    Hutter, F., Brox, T., 2018. Uncertainty estimates and multi-hypotheses networks
    for optical flow, in: Proceedings of the European Conference on Computer Vision
    (ECCV), pp. 652–667.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilg et al. [2017] Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy,
    A., Brox, T., 2017. Flownet 2.0: Evolution of optical flow estimation with deep
    networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 2462–2470.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isensee et al. [2021] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J.,
    Maier-Hein, K.H., 2021. nnU-Net: a self-configuring method for deep learning-based
    biomedical image segmentation. Nature Methods 18, 203–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jabri et al. [2020] Jabri, A., Owens, A., Efros, A., 2020. Space-time correspondence
    as a contrastive random walk. Advances in Neural Information Processing Systems
    33, 19545–19560.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. [2015] Jaderberg, M., Simonyan, K., Zisserman, A., et al.,
    2015. Spatial transformer networks. Advances in Neural Information Processing
    Systems 28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaganathan et al. [2023] Jaganathan, S., Kukla, M., Wang, J., Shetty, K., Maier,
    A., 2023. Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion, in:
    Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
    pp. 2788–2798.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. [2022] Ji, Y., Zhu, Z., Wei, Y., 2022. A One-shot Lung 4D-CT Image
    Registration Method with Temporal-spatial Features, in: 2022 IEEE Biomedical Circuits
    and Systems Conference (BioCAS), IEEE. pp. 203–207.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. [2022a] Jia, X., Bartlett, J., Chen, W., Song, S., Zhang, T., Cheng,
    X., Lu, W., Qiu, Z., Duan, J., 2022a. Fourier-net: Fast image registration with
    band-limited deformation. arXiv preprint arXiv:2211.16342 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. [2022b] Jia, X., Bartlett, J., Zhang, T., Lu, W., Qiu, Z., Duan,
    J., 2022b. U-net vs transformer: Is u-net outdated in medical image registration?,
    in: Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022,
    Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings,
    Springer. pp. 151–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. [2021] Jia, X., Thorley, A., Chen, W., Qiu, H., Shen, L., Styles,
    I.B., Chang, H.J., Leonardis, A., De Marvao, A., O’Regan, D.P., et al., 2021.
    Learning a model-driven variational network for deformable image registration.
    IEEE Trans. Med. Imag. 41, 199–212.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jian et al. [2022] Jian, B., Azampour, M.F., De Benetti, F., Oberreuter, J.,
    Bukas, C., Gersing, A.S., Foreman, S.C., Dietrich, A.S., Rischewski, J., Kirschke,
    J.S., et al., 2022. Weakly-supervised Biomechanically-constrained CT/MRI Registration
    of the Spine, in: 25${}^{\mbox{\tiny{th}}}$ International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI 2022), Springer. pp.
    227–236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2020] Jiang, Z., Yin, F.F., Ge, Y., Ren, L., 2020. A multi-scale
    framework with unsupervised joint training of convolutional neural networks for
    pulmonary deformable image registration. Physics in Medicine & Biology 65, 015011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jonschkowski et al. [2020] Jonschkowski, R., Stone, A., Barron, J.T., Gordon,
    A., Konolige, K., Angelova, A., 2020. What matters in unsupervised optical flow,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part II 16, Springer. pp. 557–572.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi and Hong [2022] Joshi, A., Hong, Y., 2022. Diffeomorphic image registration
    using lipschitz continuous residual networks, in: International Conference on
    Medical Imaging with Deep Learning, PMLR. pp. 605–617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. [2004] Joshi, S., Davis, B., Jomier, M., Gerig, G., 2004. Unbiased
    diffeomorphic atlas construction for computational anatomy. NeuroImage 23, S151–S160.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. [2022] Kang, M., Hu, X., Huang, W., Scott, M.R., Reyes, M., 2022.
    Dual-stream pyramid registration network. Medical Image Analysis 78, 102379.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazerouni et al. [2022] Kazerouni, A., Aghdam, E.K., Heidari, M., Azad, R.,
    Fayyaz, M., Hacihaliloglu, I., Merhof, D., 2022. Diffusion models for medical
    image analysis: A comprehensive survey. arXiv preprint arXiv:2211.07804 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall and Gal [2017] Kendall, A., Gal, Y., 2017. What uncertainties do we
    need in bayesian deep learning for computer vision? Advances in Neural Information
    Processing Systems 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khor et al. [2023] Khor, H.G., Ning, G., Sun, Y., Lu, X., Zhang, X., Liao, H.,
    2023. Anatomically constrained and attention-guided deep feature fusion for joint
    segmentation and deformable medical image registration. Medical Image Analysis
    , 102811.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2022] Kim, B., Han, I., Ye, J.C., 2022. Diffusemorph: Unsupervised
    deformable image registration using diffusion model, in: Proceedings of the European
    Conference on Computer Vision (ECCV), Springer. pp. 347–364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2021] Kim, B., Kim, D.H., Park, S.H., Kim, J., Lee, J.G., Ye, J.C.,
    2021. Cyclemorph: cycle consistent unsupervised deformable image registration.
    Medical Image Analysis 71, 102036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2019] Kim, D., Cho, D., Kweon, I.S., 2019. Self-supervised video
    representation learning with space-time cubic puzzles, in: Proceedings of the
    AAAI conference on artificial intelligence, pp. 8545–8552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. [2023] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023.
    Segment anything. arXiv preprint arXiv:2304.02643 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirkwood et al. [1960] Kirkwood, J.G., Baldwin, R.L., Dunlop, P.J., Gosting,
    L.J., Kegeles, G., 1960. Flow equations and frames of reference for isothermal
    diffusion in liquids. The Journal of Chemical Physics 33, 1505–1513.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klein et al. [2009] Klein, S., Staring, M., Murphy, K., Viergever, M.A., Pluim,
    J.P., 2009. Elastix: a toolbox for intensity-based medical image registration.
    IEEE Trans. Med. Imag. 29, 196–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krebs et al. [2019] Krebs, J., Delingette, H., Mailhé, B., Ayache, N., Mansi,
    T., 2019. Learning a probabilistic model for diffeomorphic registration. IEEE
    Trans. Med. Imag. 38, 2165–2176.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krebs et al. [2017] Krebs, J., Mansi, T., Delingette, H., Zhang, L., Ghesu,
    F.C., Miao, S., Maier, A.K., Ayache, N., Liao, R., Kamen, A., 2017. Robust non-rigid
    registration through agent-based action learning, in: 20${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2017),
    Springer. pp. 344–352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuang [2019] Kuang, D., 2019. Cycle-consistent training for reducing negative
    jacobian determinant in deep registration networks, in: Simulation and Synthesis
    in Medical Imaging: 4th International Workshop, SASHIMI 2019, Held in Conjunction
    with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 4, Springer.
    pp. 120–129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuang and Schmah [2019] Kuang, D., Schmah, T., 2019. Faim–a convnet method
    for unsupervised 3D medical image registration, in: International Workshop on
    Machine Learning in Medical Imaging, Springer. pp. 646–654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kybic [2009] Kybic, J., 2009. Bootstrap resampling for image registration uncertainty
    estimation without ground truth. IEEE Transactions on Image Processing 19, 64–73.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lai and Xie [2019] Lai, Z., Xie, W., 2019. Self-supervised learning for video
    correspondence flow. arXiv preprint arXiv:1905.00875 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lange et al. [2020] Lange, F.J., Ashburner, J., Smith, S.M., Andersson, J.L.,
    2020. A symmetric prior for the regularisation of elastic deformations: Improved
    anatomical plausibility in nonlinear image registration. NeuroImage 219, 116962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laves et al. [2019] Laves, M.H., Ihler, S., Ortmaier, T., 2019. Deformable
    medical image registration using a randomly-initialized CNN as regularization
    prior, in: Medical Imaging with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le-Khac et al. [2020] Le-Khac, P.H., Healy, G., Smeaton, A.F., 2020. Contrastive
    representation learning: A framework and review. IEEE Access 8, 193907–193934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. [2020] Lei, Y., Fu, Y., Wang, T., Liu, Y., Patel, P., Curran, W.J.,
    Liu, T., Yang, X., 2020. 4D-CT deformable image registration using multiscale
    unsupervised deep learning. Physics in Medicine & Biology 65, 085003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leow et al. [2007] Leow, A.D., Yanovsky, I., Chiang, M.C., Lee, A.D., Klunder,
    A.D., Lu, A., Becker, J.T., Davis, S.W., Toga, A.W., Thompson, P.M., 2007. Statistical
    properties of jacobian maps and the realization of unbiased large-deformation
    nonlinear image registration. IEEE Trans. Med. Imag. 26, 822–832.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Fan [2018] Li, H., Fan, Y., 2018. Non-rigid image registration using
    self-supervised fully convolutional networks without training data, in: 15${}^{\mbox{\tiny{th}}}$
    International Symposium on Biomedical Imaging (ISBI 2018), IEEE. pp. 1075–1078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023] Li, J., Chen, J., Tang, Y., Wang, C., Landman, B.A., Zhou,
    S.K., 2023. Transforming medical imaging with transformers? a comparative review
    of key properties, current progresses, and future perspectives. Medical Image
    Analysis , 102762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Li, L., Sinclair, M., Makropoulos, A., Hajnal, J.V., David Edwards,
    A., Kainz, B., Rueckert, D., Alansary, A., 2021. CAS-Net: conditional atlas generation
    and brain segmentation for fetal MRI, in: Uncertainty for Safe Utilization of
    Machine Learning in Medical Imaging, and Perinatal Imaging, Placental and Preterm
    Image Analysis: 3rd International Workshop, UNSURE 2021, and 6th International
    Workshop, PIPPI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France,
    October 1, 2021, Proceedings 3, Springer. pp. 221–230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2020] Li, P., Pei, Y., Guo, Y., Ma, G., Xu, T., Zha, H., 2020. Non-Rigid
    2D-3D Registration Using Convolutional Autoencoders, in: 17${}^{\mbox{\tiny{th}}}$
    International Symposium on Biomedical Imaging (ISBI 2020), pp. 700–704. doi:[10.1109/ISBI45749.2020.9098602](http://dx.doi.org/10.1109/ISBI45749.2020.9098602).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Ogino [2019] Li, Z., Ogino, M., 2019. Adversarial learning for deformable
    image registration: Application to 3d ultrasound image fusion, in: Smart Ultrasound
    Imaging and Perinatal, Preterm and Paediatric Image Analysis: First International
    Workshop, SUSI 2019, and 4th International Workshop, PIPPI 2019, Held in Conjunction
    with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, Proceedings 4, Springer.
    pp. 56–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. [2019] Liao, H., Lin, W.A., Zhang, J., Zhang, J., Luo, J., Zhou,
    S.K., 2019. Multiview 2D/3D rigid registration via a point-of-interest network
    for tracking and triangulation, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 12638–12647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2020a] Liu, L., Aviles-Rivero, A.I., Schönlieb, C.B., 2020a. Contrastive
    registration for unsupervised medical image segmentation. arXiv preprint arXiv:2011.08894
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2019] Liu, L., Hu, X., Zhu, L., Heng, P.A., 2019. Probabilistic
    multilayer regularization network for unsupervised 3d brain image registration,
    in: 22${}^{\mbox{\tiny{nd}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2019), Springer. pp. 346–354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022a] Liu, L., Huang, Z., Liò, P., Schönlieb, C.B., Aviles-Rivero,
    A.I., 2022a. Pc-swinmorph: patch representation for unsupervised medical image
    registration and segmentation. arXiv preprint arXiv:2203.05684 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2020b] Liu, X., Zheng, Y., Killeen, B., Ishii, M., Hager, G.D.,
    Taylor, R.H., Unberath, M., 2020b. Extremely dense point correspondences using
    a learned feature descriptor, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 4847–4856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2022b] Liu, Y., Chen, J., Wei, S., Carass, A., Prince, J., 2022b.
    On finite difference jacobian computation in deformable image registration. arXiv
    preprint arXiv:2212.06060 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022c] Liu, Y., Zuo, L., Han, S., Xue, Y., Prince, J.L., Carass,
    A., 2022c. Coordinate translator for learning deformable medical image registration,
    in: International Workshop on Multiscale Multimodal Medical Imaging, Springer.
    pp. 98–109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., Guo, B., 2021. Swin transformer: Hierarchical vision transformer using shifted
    windows, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 10012–10022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022d] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell,
    T., Xie, S., 2022d. A convnet for the 2020s, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11976–11986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022e] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S.,
    Hu, H., 2022e. Video swin transformer, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 3202–3211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lobachev et al. [2021] Lobachev, O., Funatomi, T., Pfaffenroth, A., Förster,
    R., Knudsen, L., Wrede, C., Guthe, M., Haberthür, D., Hlushchuk, R., Salaets,
    T., et al., 2021. Evaluating registrations of serial sections with distortions
    of the ground truths. IEEE Access 9, 152514–152535.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'López et al. [2022] López, P.A., Mella, H., Uribe, S., Hurtado, D.E., Costabal,
    F.S., 2022. WarpPINN: Cine-MR image registration with physics-informed neural
    networks. arXiv preprint arXiv:2211.12549 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lotfi et al. [2013] Lotfi, T., Tang, L., Andrews, S., Hamarneh, G., 2013. Improving
    probabilistic image registration via reinforcement learning and uncertainty evaluation,
    in: Machine Learning in Medical Imaging: 4th International Workshop, MLMI 2013,
    Held in Conjunction with MICCAI 2013, Nagoya, Japan, September 22, 2013\. Proceedings
    4, Springer. pp. 187–194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2019] Luo, J., Sedghi, A., Popuri, K., Cobzas, D., Zhang, M., Preiswerk,
    F., Toews, M., Golby, A., Sugiyama, M., Wells, W.M., et al., 2019. On the applicability
    of registration uncertainty, in: MICCAI19, Springer. pp. 410–419.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2016] Luo, W., Li, Y., Urtasun, R., Zemel, R., 2016. Understanding
    the effective receptive field in deep convolutional neural networks. Advances
    in Neural Information Processing Systems 29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2021] Luo, Y., Cao, W., He, Z., Zou, W., He, Z., 2021. Deformable
    adversarial registration network with multiple loss constraints. Computerized
    Medical Imaging and Graphics 91, 101931.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lv et al. [2022] Lv, J., Wang, Z., Shi, H., Zhang, H., Wang, S., Wang, Y., Li,
    Q., 2022. Joint progressive and coarse-to-fine registration of brain MRI via deformation
    field integration and non-rigid feature fusion. IEEE Trans. Med. Imag. 41, 2788–2802.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2021] Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang,
    X., Martel, A.L., 2021. Loss odyssey in medical image segmentation. Medical Image
    Analysis 71, 102035.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2008] Ma, J., Miller, M.I., Trouvé, A., Younes, L., 2008. Bayesian
    template estimation in computational anatomy. NeuroImage 42, 252–261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2022] Ma, M., Xu, Y., Song, L., Liu, G., 2022. Symmetric transformer-based
    network for unsupervised image registration. Knowledge-Based Systems 257, 109959.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mac Aodha et al. [2012] Mac Aodha, O., Humayun, A., Pollefeys, M., Brostow,
    G.J., 2012. Learning a confidence measure for optical flow. IEEE transactions
    on pattern analysis and machine intelligence 35, 1107–1120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madsen et al. [2020] Madsen, D., Morel-Forster, A., Kahr, P., Rahbani, D.,
    Vetter, T., Lüthi, M., 2020. A closest point proposal for mcmc-based probabilistic
    surface registration, in: Computer Vision–ECCV 2020: 16th European Conference,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, Springer. pp. 281–296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maes et al. [1997] Maes, F., Collignon, A., Vandermeulen, D., Marchal, G., Suetens,
    P., 1997. Multimodality image registration by maximization of mutual information.
    IEEE Trans. Med. Imag. 16, 187–198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahapatra et al. [2018a] Mahapatra, D., Antony, B., Sedai, S., Garnavi, R.,
    2018a. Deformable medical image registration using generative adversarial networks,
    in: 15${}^{\mbox{\tiny{th}}}$ International Symposium on Biomedical Imaging (ISBI 2018),
    IEEE. pp. 1449–1453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahapatra and Ge [2020] Mahapatra, D., Ge, Z., 2020. Training data independent
    image registration using generative adversarial networks and domain adaptation.
    Pattern Recognition 100, 107109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahapatra et al. [2018b] Mahapatra, D., Ge, Z., Sedai, S., Chakravorty, R.,
    2018b. Joint registration and segmentation of xray images using generative adversarial
    networks, in: Machine Learning in Medical Imaging: 9th International Workshop,
    MLMI 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16,
    2018, Proceedings 9, Springer. pp. 73–80.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maillard et al. [2022] Maillard, M., François, A., Glaunès, J., Bloch, I.,
    Gori, P., 2022. A deep residual learning implementation of metamorphosis, in:
    19${}^{\mbox{\tiny{th}}}$ International Symposium on Biomedical Imaging (ISBI 2022),
    IEEE. pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintz and Viergever [1998] Maintz, J.A., Viergever, M.A., 1998. A survey of
    medical image registration. Medical Image Analysis 2, 1–36.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makhzani et al. [2015] Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I.,
    Frey, B., 2015. Adversarial autoencoders. arXiv preprint arXiv:1511.05644 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mansi et al. [2011] Mansi, T., Pennec, X., Sermesant, M., Delingette, H., Ayache,
    N., 2011. iLogDemons: A demons-based registration algorithm for tracking incompressible
    elastic biological tissues. International Journal of Computer Vision 92, 92–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. [2022a] Meng, M., Bi, L., Fulham, M., Feng, D.D., Kim, J., 2022a.
    Enhancing medical image registration via appearance adjustment networks. NeuroImage
    259, 119444.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. [2022b] Meng, Q., Qin, C., Bai, W., Liu, T., de Marvao, A., O’Regan,
    D.P., Rueckert, D., 2022b. MulViMotion: Shape-aware 3D Myocardial Motion Tracking
    from Multi-View Cardiac MRI. IEEE Trans. Med. Imag. 41, 1961–1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mescheder et al. [2019] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin,
    S., Geiger, A., 2019. Occupancy networks: Learning 3D reconstruction in function
    space, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4460–4470.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. [2016] Miao, S., Wang, Z.J., Liao, R., 2016. A CNN regression approach
    for real-time 2D/3D registration. IEEE Trans. Med. Imag. 35, 1352–1363.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mildenhall et al. [2021] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron,
    J.T., Ramamoorthi, R., Ng, R., 2021. Nerf: Representing scenes as neural radiance
    fields for view synthesis. Communications of the ACM 65, 99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miller et al. [2006] Miller, M.I., Trouvé, A., Younes, L., 2006. Geodesic shooting
    for computational anatomy. Journal of Mathematical Imaging and Vision 24, 209–228.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2020a] Mok, T.C., Chung, A., 2020a. Fast symmetric diffeomorphic
    image registration with convolutional neural networks, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4644–4653.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2020b] Mok, T.C., Chung, A., 2020b. Large deformation diffeomorphic
    image registration with laplacian pyramid networks, in: 23${}^{\mbox{\tiny{rd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020),
    Springer. pp. 211–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2021a] Mok, T.C., Chung, A., 2021a. Conditional deep laplacian
    pyramid image registration network in learn2reg challenge, in: 24${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 161–167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2021b] Mok, T.C., Chung, A., 2021b. Conditional deformable image
    registration with convolutional neural network, in: 24${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 35–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2022a] Mok, T.C., Chung, A., 2022a. Affine medical image registration
    with coarse-to-fine vision transformer, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 20835–20844.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mok and Chung [2022b] Mok, T.C., Chung, A., 2022b. Robust Image Registration
    with Absent Correspondences in Pre-operative and Follow-up Brain MRI Scans of
    Diffuse Glioma Patients. arXiv preprint arXiv:2210.11045 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mok and Chung [2022c] Mok, T.C., Chung, A.C., 2022c. Unsupervised Deformable
    Image Registration with Absent Correspondences in Pre-operative and Post-recurrence
    Brain Tumor MRI Scans, in: 25${}^{\mbox{\tiny{th}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022), Springer.
    pp. 25–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morales et al. [2019] Morales, M.A., Izquierdo-Garcia, D., Aganj, I., Kalpathy-Cramer,
    J., Rosen, B.R., Catana, C., 2019. Implementation and validation of a three-dimensional
    cardiac motion estimation network. Radiology: Artificial Intelligence 1, e180080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myronenko and Song [2010] Myronenko, A., Song, X., 2010. Intensity-based image
    registration by minimizing residual complexity. IEEE Trans. Med. Imag. 29, 1882–1891.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. [2020] Nan, A., Tennant, M., Rubin, U., Ray, N., 2020. Drmime: Differentiable
    mutual information and matrix exponential for multi-resolution image registration,
    in: Medical Imaging with Deep Learning, PMLR. pp. 527–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nenoff et al. [2020] Nenoff, L., Ribeiro, C.O., Matter, M., Hafner, L., Josipovic,
    M., Langendijk, J.A., Persson, G.F., Walser, M., Weber, D.C., Lomax, A.J., et al.,
    2020. Deformable image registration uncertainty for inter-fractional dose accumulation
    of lung cancer proton therapy. Radiotherapy and Oncology 147, 178–185.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niemeyer et al. [2019] Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.,
    2019. Occupancy flow: 4d reconstruction by learning particle dynamics, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5379–5389.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niethammer et al. [2011] Niethammer, M., Hart, G.L., Pace, D.F., Vespa, P.M.,
    Irimia, A., Van Horn, J.D., Aylward, S.R., 2011. Geometric metamorphosis, in:
    14${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2011), NIH Public Access. p. 639.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niethammer et al. [2019] Niethammer, M., Kwitt, R., Vialard, F.X., 2019. Metric
    learning for image registration, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 8463–8472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obeidat et al. [2016] Obeidat, M., Narayanasamy, G., Cline, K., Stathakis, S.,
    Pouliot, J., Kim, H., Kirby, N., 2016. Comparison of different qa methods for
    deformable image registration to the known errors for prostate and head-and-neck
    virtual phantoms. Biomedical Physics & Engineering Express 2, 067002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oishi et al. [2009] Oishi, K., Faria, A., Jiang, H., Li, X., Akhter, K., Zhang,
    J., Hsu, J.T., Miller, M.I., van Zijl, P.C., Albert, M., et al., 2009. Atlas-based
    whole brain white matter analysis using large deformation diffeomorphic metric
    mapping: application to normal elderly and Alzheimer’s disease participants. NeuroImage
    46, 486–499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oliveira and Tavares [2014] Oliveira, F.P., Tavares, J.M.R., 2014. Medical
    image registration: a review. Computer Methods in Biomechanics and Biomedical
    Engineering 17, 73–93.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oord et al. [2018] Oord, A.v.d., Li, Y., Vinyals, O., 2018. Representation learning
    with contrastive predictive coding. arXiv preprint arXiv:1807.03748 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osman et al. [1999] Osman, N.F., Kerwin, W.S., McVeigh, E.R., Prince, J.L.,
    1999. Cardiac motion tracking using CINE harmonic phase (HARP) magnetic resonance
    imaging. Mag. Reson. Med. 42, 1048–1060.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pace et al. [2013] Pace, D.F., Aylward, S.R., Niethammer, M., 2013. A locally
    adaptive regularization based on anisotropic diffusion for deformable image registration
    of sliding organs. IEEE Trans. Med. Imag. 32, 2114–2126.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papież et al. [2015] Papież, B.W., Franklin, J., Heinrich, M.P., Gleeson, F.V.,
    Schnabel, J.A., 2015. Liver motion estimation via locally adaptive over-segmentation
    regularization, in: 18${}^{\mbox{\tiny{th}}}$ International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI 2015), Springer. pp.
    427–434.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papież et al. [2014] Papież, B.W., Heinrich, M.P., Fehrenbach, J., Risser, L.,
    Schnabel, J.A., 2014. An implicit sliding-motion preserving regularisation via
    bilateral filtering for deformable image registration. Medical Image Analysis
    18, 1299–1311.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2019] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove,
    S., 2019. Deepsdf: Learning continuous signed distance functions for shape representation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 165–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2020] Park, T., Efros, A.A., Zhang, R., Zhu, J.Y., 2020. Contrastive
    learning for unpaired image-to-image translation, in: Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX
    16, Springer. pp. 319–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pathan and Hong [2018] Pathan, S., Hong, Y., 2018. Predictive image regression
    for longitudinal studies with missing data, in: Medical Imaging with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pei et al. [2021] Pei, Y., Chen, L., Zhao, F., Wu, Z., Zhong, T., Wang, Y.,
    Chen, C., Wang, L., Zhang, H., Wang, L., et al., 2021. Learning spatiotemporal
    probabilistic atlas of fetal brains with anatomically constrained registration
    network, in: 24${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image
    Computing and Computer Assisted Intervention (MICCAI 2021), Springer. pp. 239–248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peter et al. [2021] Peter, L., Alexander, D.C., Magnain, C., Iglesias, J.E.,
    2021. Uncertainty-aware annotation protocol to evaluate deformable registration
    algorithms. IEEE Trans. Med. Imag. 40, 2053–2065.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfandler et al. [2019] Pfandler, M., Stefan, P., Mehren, C., Lazarovici, M.,
    Weigl, M., 2019. Technical and nontechnical skills in surgery: a simulated operating
    room environment study. Spine 44, E1396–E1400.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pielawski et al. [2020] Pielawski, N., Wetzer, E., Öfverstedt, J., Lu, J.,
    Wählby, C., Lindblad, J., Sladoje, N., 2020. Comir: Contrastive multimodal image
    representation for registration. Advances in Neural Information Processing Systems
    33, 18433–18444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitiot and Guimond [2008] Pitiot, A., Guimond, A., 2008. Geometrical regularization
    of displacement fields for histological image registration. Medical Image Analysis
    12, 16–25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluim et al. [2000] Pluim, J., Maintz, J., Viergever, M., 2000. Image registration
    by maximization of combined mutual information and gradient information. IEEE
    Trans. Med. Imag. 19, 809–814.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pluim et al. [2016] Pluim, J.P., Muenzing, S.E., Eppenhof, K.A., Murphy, K.,
    2016. The truth is hard to make: Validation of medical image registration, in:
    2016 23rd International Conference on Pattern Recognition (ICPR), IEEE. pp. 2294–2300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polzin et al. [2013] Polzin, T., Rühaak, J., Werner, R., Strehlow, J., Heldmann,
    S., Handels, H., Modersitzki, J., 2013. Combining automatic landmark detection
    and variational methods for lung CT registration, in: Fifth international workshop
    on pulmonary image analysis, pp. 85–96.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. [2021] Qian, R., Meng, T., Gong, B., Yang, M.H., Wang, H., Belongie,
    S., Cui, Y., 2021. Spatiotemporal contrastive video representation learning, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 6964–6974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2018] Qin, C., Bai, W., Schlemper, J., Petersen, S.E., Piechnik,
    S.K., Neubauer, S., Rueckert, D., 2018. Joint learning of motion estimation and
    segmentation for cardiac MR image sequences, in: 21${}^{\mbox{\tiny{st}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2018),
    Springer. pp. 472–480.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2019] Qin, C., Shi, B., Liao, R., Mansi, T., Rueckert, D., Kamen,
    A., 2019. Unsupervised deformable registration for multi-modal images via disentangled
    representations, in: Information Processing in Medical Imaging: 26th International
    Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26, Springer.
    pp. 249–261.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. [2023] Qin, C., Wang, S., Chen, C., Bai, W., Rueckert, D., 2023.
    Generative myocardial motion tracking via latent space exploration with biomechanics-informed
    prior. Medical Image Analysis 83, 102682.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. [2021] Qiu, H., Qin, C., Schuh, A., Hammernik, K., Rueckert, D.,
    2021. Learning diffeomorphic and modality-invariant registration using b-splines,
    in: Medical Imaging with Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raissi et al. [2019] Raissi, M., Perdikaris, P., Karniadakis, G.E., 2019. Physics-informed
    neural networks: A deep learning framework for solving forward and inverse problems
    involving nonlinear partial differential equations. Journal of Computational physics
    378, 686–707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramon et al. [2022] Ramon, U., Hernandez, M., Mayordomo, E., 2022. Lddmm meets
    gans: Generative adversarial networks for diffeomorphic registration, in: Biomedical
    Image Registration: 10th International Workshop, WBIR 2022, Munich, Germany, July
    10–12, 2022, Proceedings, Springer. pp. 18–28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ranjan et al. [2019] Ranjan, A., Jampani, V., Balles, L., Kim, K., Sun, D.,
    Wulff, J., Black, M.J., 2019. Competitive collaboration: Joint unsupervised learning
    of depth, camera motion, optical flow and motion segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12240–12249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. [2009] Reed, V.K., Woodward, W.A., Zhang, L., Strom, E.A., Perkins,
    G.H., Tereffe, W., Oh, J.L., Yu, T.K., Bedrosian, I., Whitman, G.J., et al., 2009.
    Automatic segmentation of whole breast using atlas approach and deformable image
    registration. International Journal of Radiation Oncology* Biology* Physics 73,
    1493–1500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Risholm et al. [2011] Risholm, P., Balter, J., Wells, W.M., 2011. Estimation
    of delivered dose in radiotherapy: the influence of registration uncertainty,
    in: 14${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2011), Springer. pp. 548–555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risholm et al. [2013] Risholm, P., Janoos, F., Norton, I., Golby, A.J., Wells III,
    W.M., 2013. Bayesian characterization of uncertainty in intra-subject non-rigid
    registration. Medical Image Analysis 17, 538–555.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Risser et al. [2013] Risser, L., Vialard, F.X., Baluwala, H.Y., Schnabel, J.A.,
    2013. Piecewise-diffeomorphic image registration: Application to the motion estimation
    between 3D CT lung images with sliding conditions. Medical Image Analysis 17,
    182–193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roche et al. [1998] Roche, A., Malandain, G., Pennec, X., Ayache, N., 1998.
    The correlation ratio as a new similarity measure for multimodal image registration,
    in: 1${}^{\mbox{\tiny{st}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 1998), Springer. pp. 1115–1124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohé et al. [2017] Rohé, M.M., Datar, M., Heimann, T., Sermesant, M., Pennec,
    X., 2017. Svf-net: learning deformable image registration using shape matching,
    in: 20${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2017), Springer. pp. 266–274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohlfing [2011] Rohlfing, T., 2011. Image similarity and tissue overlaps as
    surrogates for image registration accuracy: widely used but unreliable. IEEE Trans.
    Med. Imag. 31, 153–163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rohlfing et al. [2003a] Rohlfing, T., Maurer, C.R., Bluemke, D.A., Jacobs, M.A.,
    2003a. Volume-preserving nonrigid registration of MR breast images using free-form
    deformation with an incompressibility constraint. IEEE Trans. Med. Imag. 22, 730–741.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohlfing et al. [2003b] Rohlfing, T., Russakoff, D.B., Maurer, C.R., 2003b.
    Expectation maximization strategies for multi-atlas multi-label segmentation,
    in: 18${}^{\mbox{\tiny{th}}}$ Inf. Proc. in Med. Imaging (IPMI 2003), Springer.
    pp. 210–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
    Convolutional networks for biomedical image segmentation, in: 18${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2015),
    Springer. pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rueckert et al. [1999] Rueckert, D., Sonoda, L.I., Hayes, C., Hill, D.L., Leach,
    M.O., Hawkes, D.J., 1999. Nonrigid registration using free-form deformations:
    application to breast MR images. IEEE Trans. Med. Imag. 18, 712–721.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rühaak et al. [2017] Rühaak, J., Polzin, T., Heldmann, S., Simpson, I.J., Handels,
    H., Modersitzki, J., Heinrich, M.P., 2017. Estimation of large motion in lung
    CT by integrating regularized keypoint correspondences into dense deformable registration.
    IEEE Trans. Med. Imag. 36, 1746–1757.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandkühler et al. [2018] Sandkühler, R., Jud, C., Andermatt, S., Cattin, P.C.,
    2018. Airlab: autograd image registration laboratory. arXiv preprint arXiv:1806.09907
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmah et al. [2013] Schmah, T., Risser, L., Vialard, F.X., 2013. Left-invariant
    metrics for diffeomorphic image registration with spatially-varying regularisation,
    in: 16${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2013), Springer. pp. 203–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schnabel et al. [2016] Schnabel, J.A., Heinrich, M.P., Papież, B.W., Brady,
    J.M., 2016. Advances and challenges in deformable image registration: from image
    fusion to complex motion modelling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schultz et al. [2018] Schultz, S., Handels, H., Ehrhardt, J., 2018. A multilevel
    markov chain monte carlo approach for uncertainty quantification in deformable
    registration, in: Medical Imaging 2018: Image Processing, SPIE. pp. 162–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sdika [2008] Sdika, M., 2008. A fast nonrigid image registration with constraints
    on the jacobian using large scale constrained optimization. IEEE Trans. Med. Imag.
    27, 271–281.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sdika and Pelletier [2009] Sdika, M., Pelletier, D., 2009. Nonrigid registration
    of multiple sclerosis brain images using lesion inpainting for morphometry or
    lesion mapping. Technical Report. Wiley Online Library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shams et al. [2010] Shams, R., Sadeghi, P., Kennedy, R.A., Hartley, R.I., 2010.
    A survey of medical image registration on multicore and the gpu. IEEE signal processing
    magazine 27, 50–60.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. [2022] Shao, S., Pei, Z., Chen, W., Zhu, W., Wu, X., Zhang, B.,
    2022. A multi-scale unsupervised learning for deformable image registration. International
    Journal of Computer Assisted Radiology and Surgery , 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2019] Shen, Z., Vialard, F.X., Niethammer, M., 2019. Region-specific
    diffeomorphic metric mapping. Advances in Neural Information Processing Systems
    32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2022] Shi, J., He, Y., Kong, Y., Coatrieux, J.L., Shu, H., Yang,
    G., Li, S., 2022. Xmorpher: Full transformer for deformable medical image registration
    via cross attention, in: 25${}^{\mbox{\tiny{th}}}$ International Conference on
    Medical Image Computing and Computer Assisted Intervention (MICCAI 2022), Springer.
    pp. 217–226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. [2018] Shu, Z., Sahasrabudhe, M., Guler, R.A., Samaras, D., Paragios,
    N., Kokkinos, I., 2018. Deforming autoencoders: Unsupervised disentangling of
    shape and appearance, in: Proceedings of the European Conference on Computer Vision
    (ECCV), pp. 650–665.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siebert et al. [2022] Siebert, H., Hansen, L., Heinrich, M.P., 2022. Fast 3D
    registration with accurate optimisation and little learning for Learn2Reg 2021,
    in: Biomedical Image Registration, Domain Generalisation and Out-of-Distribution
    Analysis: MICCAI 2021 Challenges: MIDOG 2021, MOOD 2021, and Learn2Reg 2021, Held
    in Conjunction with MICCAI 2021, Strasbourg, France, September 27–October 1, 2021,
    Proceedings. Springer, pp. 174–179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siebert and Heinrich [2022] Siebert, H., Heinrich, M.P., 2022. Learn to fuse
    input features for large-deformation registration with differentiable convex-discrete
    optimisation, in: Biomedical Image Registration: 10th International Workshop,
    WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings, Springer. pp. 119–123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siebert et al. [2021] Siebert, H., Rajamani, K.T., Heinrich, M.P., 2021. Learning
    inverse consistent 3d groupwise registration with deforming autoencoders, in:
    Medical Imaging 2021: Image Processing, SPIE. pp. 89–95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simpson et al. [2015] Simpson, I.J., Cardoso, M.J., Modat, M., Cash, D.M., Woolrich,
    M.W., Andersson, J.L., Schnabel, J.A., Ourselin, S., Initiative, A.D.N., et al.,
    2015. Probabilistic non-linear registration with spatially adaptive regularisation.
    Medical Image Analysis 26, 203–216.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simpson et al. [2011] Simpson, I.J., Woolrich, M., Groves, A.R., Schnabel,
    J.A., 2011. Longitudinal brain MRI analysis with uncertain registration, in: 14${}^{\mbox{\tiny{th}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2011),
    Springer. pp. 647–654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinclair et al. [2022] Sinclair, M., Schuh, A., Hahn, K., Petersen, K., Bai,
    Y., Batten, J., Schaap, M., Glocker, B., 2022. Atlas-istn: joint segmentation,
    registration and atlas construction with image-and-spatial transformer networks.
    Medical Image Analysis 78, 102383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sitzmann et al. [2020] Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein,
    G., 2020. Implicit neural representations with periodic activation functions.
    Advances in Neural Information Processing Systems 33, 7462–7473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smolders et al. [2022] Smolders, A., Lomax, T., Weber, D., Albertini, F., 2022.
    Deformable image registration uncertainty quantification using deep learning for
    dose accumulation in adaptive proton therapy, in: Biomedical Image Registration:
    10th International Workshop, WBIR 2022, Munich, Germany, July 10–12, 2022, Proceedings,
    Springer. pp. 57–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sohl-Dickstein et al. [2015] Sohl-Dickstein, J., Weiss, E., Maheswaranathan,
    N., Ganguli, S., 2015. Deep unsupervised learning using nonequilibrium thermodynamics,
    in: 32${}^{\mbox{\tiny{nd}}}$ International Conference on Machine Learning (ICML 2016),
    PMLR. pp. 2256–2265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sokooti et al. [2016] Sokooti, H., Saygili, G., Glocker, B., Lelieveldt, B.P.,
    Staring, M., 2016. Accuracy estimation for medical image registration using regression
    forests, in: 19${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image
    Computing and Computer Assisted Intervention (MICCAI 2016), Springer. pp. 107–115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sokooti et al. [2017] Sokooti, H., Vos, B.d., Berendsen, F., Lelieveldt, B.P.,
    Išgum, I., Staring, M., 2017. Nonrigid image registration using multi-scale 3D
    convolutional neural networks, in: 20${}^{\mbox{\tiny{th}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2017), Springer.
    pp. 232–239.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sokooti et al. [2021] Sokooti, H., Yousefi, S., Elmahdy, M.S., Lelieveldt,
    B.P., Staring, M., 2021. Hierarchical prediction of registration misalignment
    using a convolutional LSTM: Application to chest CT scans. IEEE Access 9, 62008–62020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2022] Song, X., Chao, H., Xu, X., Guo, H., Xu, S., Turkbey, B.,
    Wood, B.J., Sanford, T., Wang, G., Yan, P., 2022. Cross-modal attention for multi-modal
    image registration. Medical Image Analysis 82, 102612.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sotiras et al. [2013] Sotiras, A., Davatzikos, C., Paragios, N., 2013. Deformable
    medical image registration: A survey. IEEE Trans. Med. Imag. 32, 1153–1190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stefanescu et al. [2004] Stefanescu, R., Pennec, X., Ayache, N., 2004. Grid
    powered nonlinear image registration with locally adaptive regularization. Medical
    Image Analysis 8, 325–342.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stone et al. [2021] Stone, A., Maurer, D., Ayvaci, A., Angelova, A., Jonschkowski,
    R., 2021. Smurf: Self-teaching multi-frame unsupervised raft with full-image warping,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 3887–3896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studholme et al. [2000] Studholme, C., Constable, R.T., Duncan, J.S., 2000.
    Accurate alignment of functional EPI data to anatomical MRI using a physics-based
    distortion model. IEEE Trans. Med. Imag. 19, 1115–1127.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022] Sun, S., Han, K., Kong, D., Tang, H., Yan, X., Xie, X., 2022.
    Topology-preserving shape reconstruction and registration via neural diffeomorphic
    flow, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 20845–20855.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ta et al. [2020] Ta, K., Ahn, S.S., Stendahl, J.C., Sinusas, A.J., Duncan,
    J.S., 2020. A semi-supervised joint network for simultaneous left ventricular
    motion tracking and segmentation in 4d echocardiography, in: 23${}^{\mbox{\tiny{rd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020),
    Springer. pp. 468–477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. [2020] Tang, K., Li, Z., Tian, L., Wang, L., Zhu, Y., 2020. Admir–affine
    and deformable medical image registration for drug-addicted brain images. IEEE
    Access 8, 70960–70968.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2010] Tang, L., Hamarneh, G., Abugharbieh, R., 2010. Reliability-driven,
    spatially-adaptive regularization for deformable registration, in: Biomedical
    Image Registration: 4th International Workshop, WBIR 2010, Lübeck, Germany, July
    11-13, 2010\. Proceedings 4, Springer. pp. 173–185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Terpstra et al. [2022] Terpstra, M.L., Maspero, M., Sbrizzi, A., van den Berg,
    C.A., 2022. $\bot$-loss: a symmetric loss function for magnetic resonance imaging
    reconstruction and image registration with deep learning. Medical Image Analysis
    , 102509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teske et al. [2017] Teske, H., Bartelheimer, K., Meis, J., Bendl, R., Stoiber,
    E.M., Giske, K., 2017. Construction of a biomechanical head and neck motion model
    as a guide to evaluation of deformable image registration. Physics in Medicine
    & Biology 62, N271.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thévenaz and Unser [2000] Thévenaz, P., Unser, M., 2000. Optimization of mutual
    information for multiresolution image registration. IEEE transactions on image
    processing 9, 2083–2099.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. [2022] Tian, L., Greer, H., Vialard, F.X., Kwitt, R., Estépar,
    R.S.J., Niethammer, M., 2022. Gradicon: Approximate diffeomorphisms via gradient
    inverse consistency. arXiv preprint arXiv:2206.05897 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. [2022] Tran, M.Q., Do, T., Tran, H., Tjiputra, E., Tran, Q.D., Nguyen,
    A., 2022. Light-weight deformable registration using adversarial learning with
    distilling knowledge. IEEE Trans. Med. Imag. 41, 1443–1453.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trouvé and Younes [2005] Trouvé, A., Younes, L., 2005. Metamorphoses through
    lie group action. Foundations of computational mathematics 5, 173–198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulyanov et al. [2018] Ulyanov, D., Vedaldi, A., Lempitsky, V., 2018. Deep image
    prior, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 9446–9454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unberath et al. [2021] Unberath, M., Gao, C., Hu, Y., Judish, M., Taylor, R.H.,
    Armand, M., Grupp, R., 2021. The impact of machine learning on 2D/3D registration
    for image-guided interventions: A systematic review and perspective. Frontiers
    in Robotics and AI 8, 716007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need.
    Advances in Neural Information Processing Systems 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vercauteren et al. [2009] Vercauteren, T., Pennec, X., Perchant, A., Ayache,
    N., 2009. Diffeomorphic demons: Efficient non-parametric image registration. NeuroImage
    45, S61–S72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vialard and Risser [2014] Vialard, F.X., Risser, L., 2014. Spatially-varying
    metric learning for diffeomorphic image registration: A variational framework,
    in: 17${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2014), Springer. pp. 227–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viergever et al. [2016] Viergever, M.A., Maintz, J.A., Klein, S., Murphy, K.,
    Staring, M., Pluim, J.P., 2016. A survey of medical image registration–under review.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola and Wells III [1997] Viola, P., Wells III, W.M., 1997. Alignment by maximization
    of mutual information. International Journal of Computer Vision 24, 137–154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vishnevskiy et al. [2016] Vishnevskiy, V., Gass, T., Szekely, G., Tanner, C.,
    Goksel, O., 2016. Isotropic total variation regularization of displacements in
    parametric image registration. IEEE Trans. Med. Imag. 36, 385–395.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vlachopoulos et al. [2015] Vlachopoulos, G., Korfiatis, P., Skiadopoulos, S.,
    Kazantzi, A., Kalogeropoulou, C., Pratikakis, I., Costaridou, L., 2015. Selecting
    registration schemes in case of interstitial lung disease follow-up in CT. Medical
    physics 42, 4511–4525.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Vos et al. [2020] de Vos, B.D., van der Velden, B.H., Sander, J., Gilhuijs,
    K.G., Staring, M., Išgum, I., 2020. Mutual information for unsupervised deep learning
    image registration, in: Medical Imaging 2020: Image Processing, SPIE. pp. 155–161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vos et al. [2017] Vos, B.D.d., Berendsen, F.F., Viergever, M.A., Staring, M.,
    Išgum, I., 2017. End-to-end unsupervised deformable image registration with a
    convolutional neural network, in: Deep learning in medical image analysis and
    multimodal learning for clinical decision support. Springer, pp. 204–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2019a] Wang, J., Jiao, J., Bao, L., He, S., Liu, Y., Liu, W.,
    2019a. Self-supervised spatio-temporal representation learning for videos by predicting
    motion and appearance statistics, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 4006–4015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019b] Wang, J., Wells III, W.M., Golland, P., Zhang, M., 2019b.
    Registration uncertainty quantification via low-dimensional characterization of
    geometric deformations. Magnetic resonance imaging 64, 122–131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Wang, J., Xing, J., Druzgal, J., Wells III, W.M., Zhang,
    M., 2023. Metamorph: Learning metamorphic image transformation with appearance
    changes. arXiv preprint arXiv:2303.04849 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Zhang [2020] Wang, J., Zhang, M., 2020. Deepflash: An efficient network
    for learning-based medical image registration, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 4444–4452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2022] Wang, J., Zhang, M., et al., 2022. Deep learning for regularization
    prediction in diffeomorphic image registration. Machine Learning for Biomedical
    Imaging 1, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2004] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004.
    Image quality assessment: from error visibility to structural similarity. IEEE
    transactions on image processing 13, 600–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wannenwetsch et al. [2017] Wannenwetsch, A.S., Keuper, M., Roth, S., 2017.
    Probflow: Joint optical flow and uncertainty estimation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1173–1182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2021a] Wei, D., Ahmad, S., Guo, Y., Chen, L., Huang, Y., Ma, L.,
    Wu, Z., Li, G., Wang, L., Lin, W., et al., 2021a. Recurrent tissue-aware network
    for deformable registration of infant brain mr images. IEEE transactions on medical
    imaging 41, 1219–1229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2019] Wei, D., Ahmad, S., Huo, J., Peng, W., Ge, Y., Xue, Z., Yap,
    P.T., Li, W., Shen, D., Wang, Q., 2019. Synthesis and inpainting-based MR-CT registration
    for image-guided thermal ablation of liver tumors, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 512–520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2021b] Wei, W., Haishan, X., Alpers, J., Rak, M., Hansen, C., 2021b.
    A deep learning approach for 2D ultrasound and 3D CT/MR image registration in
    liver tumor ablation. Computer Methods and Programs in Biomedicine 206, 106117.
    doi:[10.1016/j.cmpb.2021.106117](http://dx.doi.org/10.1016/j.cmpb.2021.106117).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wells III et al. [1996] Wells III, W.M., Viola, P., Atsumi, H., Nakajima, S.,
    Kikinis, R., 1996. Multi-modal volume registration by maximization of mutual information.
    Medical Image Analysis 1, 35–51.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wetzer et al. [2023] Wetzer, E., Lindblad, J., Sladoje, N., 2023. Can representation
    learning for multimodal image registration be improved by supervision of intermediate
    layers? arXiv preprint arXiv:2303.00403 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolterink et al. [2022] Wolterink, J.M., Zwienenberg, J.C., Brune, C., 2022.
    Implicit neural representations for deformable image registration, in: International
    Conference on Medical Imaging with Deep Learning, PMLR. pp. 1349–1359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2013] Wu, G., Kim, M., Wang, Q., Gao, Y., Liao, S., Shen, D., 2013.
    Unsupervised deep feature learning for deformable registration of MR brain images,
    in: 16${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2013), Springer. pp. 649–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022a] Wu, N., Wang, J., Zhang, M., Zhang, G., Peng, Y., Shen, C.,
    2022a. Hybrid atlas building with deep registration priors, in: 19${}^{\mbox{\tiny{th}}}$
    International Symposium on Biomedical Imaging (ISBI 2022), IEEE. pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2019] Wu, R.Y., Liu, A.Y., Wisdom, P., Zhu, X.R., Frank, S.J., Fuller,
    C.D., Gunn, G.B., Palmer, M.B., Wages, C.A., Gillin, M.T., et al., 2019. Characterization
    of a new physical phantom for testing rigid and deformable image registration.
    Journal of Applied Clinical Medical Physics 20, 145–153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2022b] Wu, Y., Jiahao, T.Z., Wang, J., Yushkevich, P.A., Hsieh,
    M.A., Gee, J.C., 2022b. Nodeo: A neural ordinary differential equation based optimization
    framework for deformable image registration, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 20804–20813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2021] Xiao, H., Teng, X., Liu, C., Li, T., Ren, G., Yang, R., Shen,
    D., Cai, J., 2021. A review of deep learning-based three-dimensional medical image
    registration methods. Quantitative Imaging in Medicine and Surgery 11, 4895.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2019] Xie, L., Wang, J., Dong, M., Wolk, D.A., Yushkevich, P.A.,
    2019. Improving multi-atlas segmentation by convolutional neural network based
    patch error estimation, in: 22${}^{\mbox{\tiny{nd}}}$ International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019), Springer.
    pp. 347–355.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2023] Xie, L., Wisse, L.E., Wang, J., Ravikumar, S., Khandelwal,
    P., Glenn, T., Luther, A., Lim, S., Wolk, D.A., Yushkevich, P.A., 2023. Deep label
    fusion: A generalizable hybrid multi-atlas and deep convolutional neural network
    for medical image segmentation. Medical Image Analysis 83, 102683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xing et al. [2017] Xing, F., Woo, J., Gomez, A.D., Pham, D.L., Bayly, P.V.,
    Stone, M., Prince, J.L., 2017. Phase vector incompressible registration algorithm
    for motion estimation from tagged magnetic resonance images. IEEE Trans. Med.
    Imag. 36, 2116–2128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] Xu, J., Chen, E.Z., Chen, X., Chen, T., Sun, S., 2021. Multi-scale
    neural odes for 3D medical image registration, in: 24${}^{\mbox{\tiny{th}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 213–223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2022] Xu, Z., Luo, J., Lu, D., Yan, J., Frisken, S., Jagadeesan,
    J., Wells III, W.M., Li, X., Zheng, Y., Tong, R.K.y., 2022. Double-uncertainty
    guided spatial and temporal consistency regularization weighting for learning-based
    abdominal registration, in: MICCAI22, Springer. pp. 14–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2020] Xu, Z., Luo, J., Yan, J., Pulya, R., Li, X., Wells, W., Jagadeesan,
    J., 2020. Adversarial uni-and multi-modal stream networks for multimodal image
    registration, in: 23${}^{\mbox{\tiny{rd}}}$ International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI 2020), Springer. pp.
    222–232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu and Niethammer [2019] Xu, Z., Niethammer, M., 2019. Deepatlas: Joint semi-supervised
    learning of image registration and segmentation, in: 22${}^{\mbox{\tiny{nd}}}$
    International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019),
    Springer. pp. 420–429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2018] Yan, P., Xu, S., Rastinehad, A.R., Wood, B.J., 2018. Adversarial
    image registration with application for MR and TRUS image fusion, in: Machine
    Learning in Medical Imaging: 9th International Workshop, MLMI 2018, Held in Conjunction
    with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 9, Springer.
    pp. 197–204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2020] Yang, H., Sun, J., Carass, A., Zhao, C., Lee, J., Prince,
    J.L., Xu, Z., 2020. Unsupervised MR-to-CT synthesis using structure-constrained
    CycleGAN. IEEE Trans. Med. Imag. 39, 4249–4261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2018] Yang, H., Sun, J., Li, H., Wang, L., Xu, Z., 2018. Neural
    multi-atlas label fusion: Application to cardiac MR images. Medical Image Analysis
    49, 60–75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022a] Yang, J., Wickramasinghe, U., Ni, B., Fua, P., 2022a. Implicitatlas:
    learning deformable shape templates in medical imaging, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15861–15871.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022b] Yang, T., Bai, X., Cui, X., Gong, Y., Li, L., 2022b. Graformerdir:
    Graph convolution transformer for deformable image registration. Computers in
    Biology and Medicine 147, 105799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2016] Yang, X., Kwitt, R., Niethammer, M., 2016. Fast predictive
    image registration, in: Deep Learning and Data Labeling for Medical Applications:
    First International Workshop, LABELS 2016, and Second International Workshop,
    DLMIA 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 21,
    2016, Proceedings 1, Springer. pp. 48–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2017] Yang, X., Kwitt, R., Styner, M., Niethammer, M., 2017. Quicksilver:
    Fast predictive image registration–a deep learning approach. NeuroImage 158, 378–396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2020] Yao, Y., Liu, C., Luo, D., Zhou, Y., Ye, Q., 2020. Video
    playback rate perception for self-supervised spatio-temporal representation learning,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 6548–6557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2021] Ye, M., Kanski, M., Yang, D., Chang, Q., Yan, Z., Huang, Q.,
    Axel, L., Metaxas, D., 2021. DeepTag: An unsupervised deep learning method for
    motion tracking on cardiac tagging magnetic resonance images, in: 2021 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 7261–7271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Younes [2010] Younes, L., 2010. Shapes and diffeomorphisms. volume 171. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2020a] Yu, E.M., Dalca, A.V., Sabuncu, M.R., 2020a. Learning conditional
    deformable shape templates for brain anatomy, in: Machine Learning in Medical
    Imaging: 11th International Workshop, MLMI 2020, Held in Conjunction with MICCAI
    2020, Lima, Peru, October 4, 2020, Proceedings 11, Springer. pp. 353–362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2020b] Yu, H., Chen, X., Shi, H., Chen, T., Huang, T.S., Sun, S.,
    2020b. Motion pyramid networks for accurate and efficient cardiac motion estimation,
    in: 23${}^{\mbox{\tiny{rd}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2020), Springer. pp. 436–446.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2020c] Yu, H., Sun, S., Yu, H., Chen, X., Shi, H., Huang, T.S.,
    Chen, T., 2020c. Foal: Fast online adaptive learning for cardiac motion estimation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 4313–4323.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2021] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H.,
    Tay, F.E., Feng, J., Yan, S., 2021. Tokens-to-token vit: Training vision transformers
    from scratch on imagenet, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 558–567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang [2018] Zhang, J., 2018. Inverse-consistent deep networks for unsupervised
    deformable image registration. arXiv preprint arXiv:1809.03443 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Fletcher [2019] Zhang, M., Fletcher, P.T., 2019. Fast diffeomorphic
    image registration via fourier-approximated lie algebras. International Journal
    of Computer Vision 127, 61–73.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Zhang, S., Liu, P.X., Zheng, M., Shi, W., 2020. A diffeomorphic
    unsupervised method for deformable soft tissue image registration. Computers in
    Biology and Medicine 120, 103708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Zhang, Y., Li, L., Wang, W., Xie, R., Song, L., Zhang, W.,
    2023. Boosting video object segmentation via space-time correspondence learning.
    arXiv preprint arXiv:2304.06211 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, Y., Pei, Y., Zha, H., 2021. Learning dual transformer
    network for diffeomorphic registration, in: 24${}^{\mbox{\tiny{th}}}$ International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021),
    Springer. pp. 129–138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2021a] Zhao, F., Wu, Z., Wang, F., Lin, W., Xia, S., Shen, D.,
    Wang, L., Li, G., 2021a. S3reg: Superfast spherical surface registration based
    on deep learning. IEEE Trans. Med. Imag. 40, 1964–1976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2021b] Zhao, F., Wu, Z., Wang, L., Lin, W., Xia, S., Li, G., Consortium,
    U.B.C.P., 2021b. Learning 4d infant cortical surface atlas with unsupervised spherical
    networks, in: 24${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image
    Computing and Computer Assisted Intervention (MICCAI 2021), Springer. pp. 262–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2019] Zhao, S., Lau, T., Luo, J., Eric, I., Chang, C., Xu, Y.,
    2019. Unsupervised 3D end-to-end medical image registration with volume tweening
    network. IEEE Journal of Biomedical and Health Informatics 24, 1394–1404.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2021] Zheng, Y., Sui, X., Jiang, Y., Che, T., Zhang, S., Yang,
    J., Li, H., 2021. Symreg-gan: symmetric image registration with generative adversarial
    networks. IEEE transactions on pattern analysis and machine intelligence 44, 5631–5646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2023] Zhou, S., Hu, B., Xiong, Z., Wu, F., 2023. Self-distilled
    hierarchical network for unsupervised deformable image registration. IEEE Trans.
    Med. Imag. .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019] Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., 2019.
    Unet++: Redesigning skip connections to exploit multiscale features in image segmentation.
    IEEE Trans. Med. Imag. 39, 1856–1867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2020] Zhu, H., Adeli, E., Shi, F., Shen, D., Initiative, A.D.N.,
    2020. Fcn based label correction for multi-atlas guided organ segmentation. NeuroImage
    18, 319–331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2017] Zhu, J.Y., Park, T., Isola, P., Efros, A.A., 2017. Unpaired
    image-to-image translation using cycle-consistent adversarial networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. [2008] Zhuang, X., Rhode, K., Arridge, S., Razavi, R., Hill,
    D., Hawkes, D., Ourselin, S., 2008. An atlas-based segmentation propagation framework
    using locally affine registration–application to automatic whole heart segmentation,
    in: 11${}^{\mbox{\tiny{th}}}$ International Conference on Medical Image Computing
    and Computer Assisted Intervention (MICCAI 2008), Springer. pp. 425–433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2022] Zou, J., Gao, B., Song, Y., Qin, J., 2022. A review of deep
    learning-based deformable medical image registration. Frontiers in Oncology 12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
