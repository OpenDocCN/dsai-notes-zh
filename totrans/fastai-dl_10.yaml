- en: 'Deep Learning 2: Part 2 Lesson 10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第2部分第10课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自*[*fast.ai课程*](http://www.fast.ai/)*的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: '[Video](https://youtu.be/h5Tz7gZT9Fo) / [Forum](http://forums.fast.ai/t/part-2-lesson-10-wiki/14364/1)'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/h5Tz7gZT9Fo) / [论坛](http://forums.fast.ai/t/part-2-lesson-10-wiki/14364/1)'
- en: Review of Last Week [[0:16](https://youtu.be/h5Tz7gZT9FoY?t=16s)]
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上周回顾[[0:16](https://youtu.be/h5Tz7gZT9FoY?t=16s)]
- en: Many students are struggling with last week’s material, so if you are finding
    it difficult, that’s fine. The reason Jeremy put it up there up front is so that
    we have something to cogitate about, think about, and gradually work towards,
    so by lesson 14, you’ll get a second crack at it.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多学生在上周的内容上遇到了困难，所以如果你觉得困难，没关系。Jeremy之所以提前放上这些内容，是为了让我们有东西可以思考、考虑，并逐渐努力，所以到第14课时，你将有第二次机会。
- en: To understand the pieces, you’ll need to understand the shapes of convolutional
    layer outputs, receptive fields, and loss functions — which are all the things
    you’ll need to understand for all of your deep learning studies anyway.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要理解这些部分，您需要了解卷积层输出的形状、感受野和损失函数——这些都是您在深度学习研究中需要理解的东西。
- en: One key thing is that we started out with something simple — a single object
    classifier, single object bounding box without a classifier, and then single object
    classifier and bounding box. The bit where we go to multiple objects is actually
    almost identical to that except we first have to solve the matching problem. We
    ended up creating far more activations than we need for our our number of ground
    truth bounding boxes, so we match each ground truth object to a subset of those
    activations. Once we’ve done that, the loss function that we then do to each matched
    pair is almost identical to this loss function (i.e. the one for single object
    classifier and bounding box).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关键的事情是，我们从简单的开始——一个单一对象分类器，一个没有分类器的单一对象边界框，然后是一个单一对象分类器和边界框。我们转向多个对象的部分实际上几乎与此相同，只是我们首先必须解决匹配问题。我们最终创建了比我们需要的地面真实边界框更多的激活，因此我们将每个地面真实对象与这些激活的子集进行匹配。一旦我们做到了这一点，我们对每个匹配对执行的损失函数几乎与这个损失函数相同（即单一对象分类器和边界框的损失函数）。
- en: If you are feeling stuck, go back to lesson 8 and make sure you understand Dataset,
    DataLoader, and most importantly the loss function.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您感到困惑，请回到第8课，并确保您理解数据集、数据加载器，尤其是损失函数。
- en: So once we have something which can predict the class and bounding box for one
    object, we went to multiple objects by just creating more activations [[2:40](https://youtu.be/h5Tz7gZT9Fo?t=2m40s)].
    We had to then deal with the matching problem, having dealt with a matching problem,
    we then moved each of those anchor boxes in and out a little bit and around a
    little bit, so they tried to line up with particular ground truth objects.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，一旦我们有了可以预测一个对象的类别和边界框的东西，我们通过创建更多的激活来转向多个对象[[2:40](https://youtu.be/h5Tz7gZT9Fo?t=2m40s)]。然后我们必须处理匹配问题，处理完匹配问题后，我们将每个锚框稍微移动一下，围绕一下，使其尽量与特定的地面真实对象对齐。
- en: 'We talked about how we took advantage of the convolutional nature of the network
    to try to have activations that had a receptive field that was similar to the
    ground truth object we were predicting. Chloe provided the following fantastic
    picture to talk about what SSD_MultiHead.forward does line by line:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了如何利用网络的卷积特性，尝试使激活具有类似于我们正在预测的地面真实对象的感受野。Chloe提供了以下出色的图片，逐行讨论了SSD_MultiHead.forward的功能：
- en: by [Chloe Sultan](http://forums.fast.ai/u/chloews)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Chloe Sultan](http://forums.fast.ai/u/chloews)提供
- en: What Chloe’s done here is she’s focused particularly on the dimensions of the
    tensor at each point in the path as we gradually downsampled using stride 2 convolutions,
    making sure she understands why those grid sizes happen then understanding how
    the outputs come out of those.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Chloe在这里所做的是特别关注每个路径中张量的维度，逐渐使用步幅2的卷积进行下采样，确保她理解为什么会出现这些网格大小，然后理解输出是如何从中产生的。
- en: This is where you’ve got to remember this `pbd.set_trace()`. I just went in
    just before the class and went into `SSD_MultiHead.forward` and entered `pdb.set_trace()`
    and then I ran a single batch. Then I could just print out the size of all these.
    We make mistakes and that’s why we have debuggers and know how to check things
    and do things in small little bits along the way.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这就是你必须记住这个`pbd.set_trace()`的地方。我在课前刚进入`SSD_MultiHead.forward`，输入了`pdb.set_trace()`，然后运行了一个批次。然后我可以打印出所有这些的大小。我们会犯错误，这就是为什么我们有调试器并知道如何检查事物并逐步进行小的操作的原因。
- en: We then talked about increasing *k* [[5:49](https://youtu.be/h5Tz7gZT9Fo?t=5m49s)]
    which is the number of anchor boxes for each convolutional grid cell which we
    can do with different zooms, aspect ratios, and that gives us a plethora of activations
    and therefore predicted bounding boxes.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们接着讨论了增加每个卷积网格单元的锚框数量*k*[[5:49](https://youtu.be/h5Tz7gZT9Fo?t=5m49s)]，我们可以通过不同的缩放比例、长宽比来实现，这给我们带来了大量的激活，从而预测边界框。
- en: Then we went down to a small number using Non Maximum Suppression.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们使用非极大值抑制将数量减少到一个较小的值。
- en: Non Maximum Suppression is kind of hacky, ugly, and totally heuristic, and we
    did not even talk about the code because it seems hideous. Somebody actually came
    up with a paper recently which attempts to do an end-to-end conv net to replace
    that NMS piece ([https://arxiv.org/abs/1705.02950](https://arxiv.org/abs/1705.02950)).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非极大值抑制有点糟糕、丑陋、完全启发式，我们甚至没有讨论代码，因为它看起来很丑陋。最近有人提出了一篇论文，试图使用端到端的卷积网络来替换NMS部分（[https://arxiv.org/abs/1705.02950](https://arxiv.org/abs/1705.02950)）。
- en: Not enough people are reading papers! What we are doing in class now is implementing
    papers, the papers are the real ground truth. And I think you know from talking
    to people a lot of the reason people aren’t reading paper is because a lot of
    people don’g think they are capable of reading papers. They don’t think they are
    the kind of people that read papers, but you are. You are here. We started looking
    at a paper last week and we read the words that were in English and we largely
    understood them. If you look at the picture above carefully, you’ll realize `SSD_MultiHead.forward`
    is not doing the same. You might then wonder if this is better. My answer is probably.
    Because SSD_MultiHead.forward was the first thing I tried just to get something
    out there. Between this and YOLO3 paper, they are probably much better ways.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不够多的人在阅读论文！我们现在在课堂上所做的是实现论文，论文是真正的真相。我认为你从与人们交谈中知道，很多人不阅读论文的原因是因为很多人认为他们没有能力阅读论文。他们认为他们不是那种阅读论文的人，但你是。你在这里。我们上周开始看一篇论文，我们读到了用英语写的文字，我们大部分都理解了。如果你仔细看上面的图片，你会意识到`SSD_MultiHead.forward`并不是在做同样的事情。你可能会想知道这样是否更好。我的答案可能是。因为SSD_MultiHead.forward是我尝试的第一件事，只是为了让一些东西出现。在这个和YOLO3论文之间，可能有更好的方法。
- en: One thing you’ll notice in particular they use a smaller k but they have a lot
    more sets of grids 1x1, 3x3, 5x5, 10x10, 19x19, 38x38 — 8732 per class. A lot
    more than we had, so that’ll be an interesting thing to experiment with.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别要注意的一点是，他们使用了更小的k，但他们有更多的网格集合1x1、3x3、5x5、10x10、19x19、38x38——每类8732个。比我们拥有的要多得多，所以这将是一个有趣的实验。
- en: Another thing I noticed is that we had 4x4, 2x2, 1c1 which means there are a
    lot of overlap — every set fits within every other set. In this case where you’ve
    got 1, 3, 5, you don’t have that overlap. So it might actually make it easier
    to learn. There’s lots of interesting you can play with.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我注意到的另一件事是，我们有4x4、2x2、1x1，这意味着有很多重叠——每组都适合其他组。在这种情况下，你有1、3、5，你没有那种重叠。所以这可能会使学习变得更容易。有很多有趣的东西可以玩。
- en: Perhaps most important thing I would recommend is to put the code and the equations
    next to each other. You are either math person or code person. By having them
    side by side, you will learn a little bit of the other.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我最推荐的可能是将代码和方程式放在一起。你可能是数学人或者代码人。将它们并排放置，你将学到一些另一方面的知识。
- en: Learning the math is hard because of the notation might seem hard to look up
    but there are good resources such as [wikipedia](https://en.wikipedia.org/wiki/List_of_mathematical_symbols).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习数学很难，因为符号可能看起来很难查找，但有一些好的资源，比如[wikipedia](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)。
- en: Another thing you should try doing is to re-create things that you see in the
    papers. Here was the key most important figure 1 from the focal loss paper.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一件你应该尝试做的事情是重新创建你在论文中看到的东西。这里是来自焦点损失论文的最重要的图1。
- en: I did discover a minor bug in my code last week [[12:14](https://youtu.be/h5Tz7gZT9Fo?t=12m14s)]
    — the way I was flattening out the convolutional activations did not line up with
    how I was using them in the loss function, and fixing that made it quite a bit
    better.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我上周发现了我的代码中一个小错误——我展平卷积激活的方式与我在损失函数中使用它们的方式不一致，修复这个问题使它变得更好了。
- en: '**Question**: Usually, when we down sample, we increase the number of filters,
    or depth. when we’re doing sampling from 7x7 to 4x4 etc, why are we decreasing
    the number from 512 to 256? Why not decrease dimension in SSD head? (performance
    related?) [[12:58](https://youtu.be/_ercfViGrgY?t=12m58s)] We have a number of
    out paths and we want each one to be the same, so we don’t want each one to have
    different number of filters and also this is what the papers did so I was trying
    to match up with that. Having these 256 — it’s a different concept because we
    are taking advantage of not just the last layer but the layers before that as
    well. Life is easier if we make them more consistent.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们降采样时，我们会增加滤波器的数量，或者深度。当我们从7x7采样到4x4等时，为什么要将数量从512减少到256呢？为什么不在SSD头部减少维度？（与性能相关？）我们有许多输出路径，我们希望每个输出路径都是相同的，所以我们不希望每个输出路径具有不同数量的滤波器，这也是论文中所做的，所以我试图与之匹配。拥有这256个——这是一个不同的概念，因为我们不仅利用了最后一层，还利用了之前的层。如果我们使它们更一致，生活会更容易。
- en: Natural Language Processing [[14:10](https://youtu.be/h5Tz7gZT9Fo?t=14m10s)]
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: 'Where we are going :'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的目标是：
- en: We have seen in every lesson this idea of taking a pre-trained model, whip off
    some some stuff on the top, replace it with something new, and get it to do something
    similar. We’ve kind of dived in a little bit deeper to that to say with `ConvLearner.pretrained`
    it had a standard way of sticking stuff on the top which does a particular thing
    (i.e. classification). Then we learned actually we can stick any PyTorch module
    we like on the end and have it do anything we like with a `custom_head` and so
    suddenly you discover there’s some really interesting things we can do.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每节课中都看到了这个想法，即采用预训练模型，去掉一些顶部的东西，用新的东西替换它，并让它做一些类似的事情。我们深入研究了这一点，`ConvLearner.pretrained`有一种标准的方法，可以在顶部添加一些东西，做一些特定的事情（即分类）。然后我们发现实际上我们可以在末尾添加任何我们喜欢的PyTorch模块，并使用`custom_head`让它做任何我们喜欢的事情，所以突然间你会发现我们可以做一些非常有趣的事情。
- en: In fact, Yang Lu said “what if we did a different kind of custom head?” and
    the different custom head was let’s take the original pictures, rotate them, and
    the make our dependent variable the opposite of that rotation and see if it can
    learn to un-rotate it. This is a super useful thing, in fact, I think Google photos
    nowadays has this option that it’ll actually automatically rotate your photos
    for you. But the cool thing is, as he showed here, you can build that network
    right now by doing exactly the same as our previous lesson. But your custom head
    is one that spits out a single number which is how much to rotate by, and your
    dataset has a dependent variable which is how much you rotated by.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，杨露说“如果我们做一个不同类型的自定义头部会怎样？”不同的自定义头部是让我们拍摄原始图片，旋转它们，然后将我们的因变量设为该旋转的相反方向，看看它是否能学会将其旋转回来。事实上，我认为现在Google相册有这个选项，它会自动为您旋转照片。但酷的是，正如他在这里展示的，你可以通过完全按照我们之前的课程来构建这个网络。但是你的自定义头部会输出一个单一数字，即旋转的角度，你的数据集有一个因变量，即旋转的角度。
- en: '[http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1](http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1](http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1)'
- en: So you suddenly realize with this idea of a backbone plus a custom head, you
    can do almost anything you can think about [[16:30](https://youtu.be/h5Tz7gZT9Fo?t=16m30s)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你突然意识到，有了这个骨干加自定义头部的想法，你几乎可以做任何你想做的事情 [[16:30](https://youtu.be/h5Tz7gZT9Fo?t=16m30s)]。
- en: Today, we are going to look at the same idea and see how that applies to NLP.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天，我们将探讨相同的想法，并看看它如何应用于自然语言处理。
- en: In the next lesson, we are going to go further and say if NLP and computer vision
    lets you do the same basic ideas, how do we combine the two. We are going to learn
    about a model that can actually learn to find word structures from images, images
    from word structures, or images from images. That will form the basis if you wanted
    to go further of doing things like going from an image to a sentence (i.e. image
    captioning) or going from a sentence to an image which we kind of started to do,
    a phrase to image.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一课中，我们将进一步探讨，如果自然语言处理和计算机视觉让你可以做相同的基本想法，我们如何将两者结合起来。我们将学习一个模型，实际上可以从图像中学习找到单词结构，从单词结构中找到图像，或从图像中找到图像。如果你想进一步做像从图像到句子（即图像字幕）或从句子到图像这样的事情，那将是基础。
- en: From there, we’ve got to go deeper then into computer vision to think what other
    kinds of things we can do with this idea of pre-trained network plus a custom
    head. So we will look at various kinds of image enhancement like increasing the
    resolution of a low-res photo to guess what was missing or adding artistic filters
    on top of photos, or changing photos of horses into photos of zebras, etc.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从那里开始，我们必须更深入地思考计算机视觉，看看我们可以用这个预训练网络加自定义头部的想法做些什么其他事情。因此，我们将研究各种图像增强技术，如增加低分辨率照片的分辨率以猜测缺失的部分，或在照片上添加艺术滤镜，或将马的照片变成斑马的照片等等。
- en: Then finally that’s going to bring us all the way back to bounding boxes again.
    To get there, we’re going to first of all learn about segmentation which is not
    just figuring out where a bounding box is, but figuring out what every single
    pixel in an image is a part of — so this pixel is a part of a person, this pixel
    is a part of a car. Then we are going to use that idea, particularly an idea called
    UNet, which turns out that this idea of UNet, we can apply to bounding boxes —
    where it’s called feature pyramids. We’ll use that to get really good results
    with bounding boxes. That’s kind of our path from here. It’s all going to build
    on each other but take us into lots of different areas.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，这将使我们回到边界框。为了达到这个目标，我们首先要学习分割，这不仅仅是找出边界框在哪里，还要找出图像中每个像素所属的部分 - 所以这个像素是人的一部分，这个像素是汽车的一部分。然后我们将使用这个想法，特别是一个叫做UNet的想法，事实证明UNet的这个想法，我们可以应用到边界框上
    - 这被称为特征金字塔。我们将使用这个方法来获得边界框的非常好的结果。这就是我们从这里开始的路径。这一切都将相互建立，但会带领我们进入许多不同的领域。
- en: 'torchtext to fastai.text [[18:56](https://youtu.be/h5Tz7gZT9Fo?t=18m56s)]:'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchtext转fastai.text [[18:56](https://youtu.be/h5Tz7gZT9Fo?t=18m56s)]：
- en: For NLP, last part, we relied on a library called torchtext but as good as it
    was, I’ve since then found the limitation of it too problematic to keep using
    it. As a lot of you complained on the forums, it’s pretty darn slow partly that’s
    because it’s not doing parallel processing and partly it’s because it doesn’t
    remember what you did last time and it does it all over again from scratch. Then
    it’s hard to do fairly simple things like a lot of you were trying to get into
    the toxic comment competition on Kaggle which was a multi-label problem and trying
    to do that with torchtext, I eventually got it working but it took me like a week
    of hacking away which is kind of ridiculous. To fix all these problems, we’ve
    created a new library called fastai.text. Fastai.text is a replacement for the
    combination of torchtext and fastai.nlp. So don’t use fastai.nlp anymore — that’s
    obsolete. It’s slower, it’s more confusing, it’s less good in every way, but there’s
    a lot of overlaps. Intentionally, a lot of the classes and functions have the
    same names, but this is the non-torchtext version.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理，我们过去依赖于一个叫做torchtext的库，但尽管它很好用，我后来发现它的限制太令人困扰，无法继续使用。正如你们很多人在论坛上抱怨的那样，它非常慢，部分原因是它不进行并行处理，部分原因是它不记得你上次做了什么，而是从头开始重新做。然后很难做一些相当简单的事情，比如很多人试图参加Kaggle上的有毒评论竞赛，这是一个多标签问题，试图用torchtext做到这一点，我最终搞定了，但花了我大约一周的时间，这有点荒谬。为了解决所有这些问题，我们创建了一个名为fastai.text的新库。Fastai.text是torchtext和fastai.nlp的组合的替代品。所以不要再使用fastai.nlp了
    - 那已经过时了。它更慢，更令人困惑，各方面都不如意，但有很多重叠。故意地，很多类和函数的名称都是相同的，但这是非torchtext版本。
- en: IMDb [[20:32](https://youtu.be/h5Tz7gZT9Fo?t=20m32s)]
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IMDb [[20:32](https://youtu.be/h5Tz7gZT9Fo?t=20m32s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb)'
- en: We will work with IMDb again. For those of you who have forgotten, go back and
    checkout [lesson 4](/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa).
    This is a dataset of movie reviews, and we used it to find out whether we might
    enjoy Zombiegeddon or not, and we thought probably my kind of thing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用IMDb。对于那些忘记了的人，请返回查看[lesson 4](/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa)。这是一个电影评论数据集，我们用它来找出我们是否会喜欢“Zombiegeddon”，我们认为可能是我喜欢的类型。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We need to download the IMDB large movie reviews from this site: [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    Direct link : [Link](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要从这个网站下载IMDB大型电影评论：[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    直接链接：[链接](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Standardize format [[21:27](https://youtu.be/h5Tz7gZT9Fo?t=21m27s)]
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化格式[[21:27](https://youtu.be/h5Tz7gZT9Fo?t=21m27s)]
- en: The basic paths for NLP is that we have to take sentences and turn them into
    numbers, and there is a couple to get there. At the moment, somewhat intentionally,
    fastai.text does not provide that many helper functions. It’s really designed
    more to let you handle things in a fairly flexible way.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的基本路径是我们必须将句子转换为数字，有几种方法可以实现这一点。目前，有点故意地，fastai.text并没有提供太多的辅助函数。它真的更多地设计为让你以一种相当灵活的方式处理事情。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see here [[21:59](https://youtu.be/h5Tz7gZT9Fo?t=21m59s)], I wrote
    something called get_texts which goes through each thing in `CLASSES`. There are
    three classes in IMDb: negative, positive, and then there’s another folder “unsupervised”
    which contains the ones they haven’t gotten around to labeling yet — so we will
    just call that a class for now. So we just go through each one of those classes,
    then find every file in that folder, and open it up, read it, and chuck it into
    the end of the array. As you can see, with pathlib, it’s super easy to grab stuff
    and pull it in, and then the label is just whatever class we are up to so far.
    We will do that for both training set and test set.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这里看到的[[21:59](https://youtu.be/h5Tz7gZT9Fo?t=21m59s)]，我写了一个名为get_texts的东西，它遍历了`CLASSES`中的每一个东西。IMDb中有三个类别：负面、正面，然后还有另一个文件夹“unsupervised”，其中包含他们尚未标记的样本，所以我们暂时将其称为一个类别。所以我们只是遍历每一个类别，然后找到该文件夹中的每个文件，打开它，读取它，并将其放入数组的末尾。正如你所看到的，使用pathlib，很容易获取并导入东西，然后标签就是到目前为止的任何类别。我们将为训练集和测试集都这样做。
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: there are 75,000 in train, 25,000 in test. 50,000 in the train set are unsupervised,
    and we won’t actually be able to use them when we get to the classification. Jeremy
    found this much easier than torch.text approach of having lots of layers and wrappers
    because in the end, reading text files is not that hard.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集中有75,000个样本，测试集中有25,000个样本。训练集中的50,000个样本是无监督的，当我们进行分类时，实际上我们将无法使用它们。Jeremy发现这比torch.text方法更容易，后者需要很多层和包装器，因为最终，读取文本文件并不那么困难。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One thing that’s always good idea is to sort things randomly [[23:19](https://youtu.be/h5Tz7gZT9Fo?t=23m19s)].
    It is useful to know this simple trick for sorting things randomly particularly
    when you’ve got multiple things you have to sort the same way. In this case, you
    have labels and `texts. np.random.permutation`, if you give it an integer, it
    gives you a random list from 0 up to and not including the number you give it
    in some random order.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个总是好的想法是随机排序[[23:19](https://youtu.be/h5Tz7gZT9Fo?t=23m19s)]。特别是当你有多个需要以相同方式排序的东西时，了解这个简单的随机排序技巧是很有用的。在这种情况下，你有标签和`texts`。np.random.permutation，如果你给它一个整数，它会给你一个从0到该数字之间的随机列表，但不包括该数字，顺序是随机的。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can them pass that in as an indexer to give you a list that’s sorted in
    that random order. So in this case, it is going to sort `trn_texts` and `trn_labels`
    in the same random way. So that’s a useful little idiom to use.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其作为索引器传递，以便得到一个按照那种随机顺序排序的列表。所以在这种情况下，它将以相同的随机方式对`trn_texts`和`trn_labels`进行排序。这是一个有用的小技巧。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we have our texts and labels sorted, we can create a dataframe from them
    [[24:07](https://youtu.be/h5Tz7gZT9Fo?t=24m7s)]. Why are we doing this? The reason
    is because there is a somewhat standard approach starting to appear for text classification
    datasets which is to have your training set as a CSV file with the labels first,
    and the text of the NLP documents second. So it basically looks like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了排序好的文本和标签，我们可以从中创建一个数据框[[24:07](https://youtu.be/h5Tz7gZT9Fo?t=24m7s)]。我们为什么要这样做呢？原因是因为在文本分类数据集中，开始出现了一种有点标准的方法，即将训练集作为一个CSV文件，其中标签在前，NLP文档的文本在后。所以它基本上看起来像这样：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So you have your labels and texts, and then a file called classes.txt which
    just lists the classes. I say somewhat standard because in a reasonably recent
    academic paper Yann LeCun and a team of researcher looked at quite a few datasets
    and they use this format for all of them. So that’s what I started using as well
    for my recent paper. You’ll find that this notebook, if you put your data into
    this format, the whole notebook will work every time [[25:17](https://youtu.be/h5Tz7gZT9Fo?t=25m17s)].
    So rather than having a thousand different formats, I just said let’s just pick
    a standard format and your job is to put your data in that format which is the
    CSV file. The CSV files have no header by default.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你有你的标签和文本，然后有一个名为classes.txt的文件，其中只列出了类别。我说“有点标准”，因为在最近的一篇学术论文中，Yann LeCun和一组研究人员查看了相当多的数据集，并且他们对所有数据集都使用了这种格式。所以这就是我最近一篇论文开始使用的格式。你会发现，如果你将你的数据放入这种格式的笔记本中，整个笔记本每次都会运行[[25:17](https://youtu.be/h5Tz7gZT9Fo?t=25m17s)]。所以，与其有一千种不同的格式，我只是说让我们选择一个标准格式，你的工作就是将你的数据放入那个格式，即CSV文件。CSV文件默认没有标题。
- en: You’ll notice at the start, we have two different paths [[25:51](https://youtu.be/h5Tz7gZT9Fo?t=25m51s)].
    One was the classification path, and the other was the language model path. In
    NLP, you’ll see LM all the time. LM means language model. The classification path
    is going to contain the information that we are going to use to create a sentiment
    analysis model. The language model path is going to contain the information we
    need to create a language model. So they are a little bit different. One thing
    that is different is that when we create the train.csv in the classification path,
    we remove everything that has a label of 2 because label of 2 is “unsupervised”
    and we can’t use it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在开始时，我们有两条不同的路径[[25:51](https://youtu.be/h5Tz7gZT9Fo?t=25m51s)]。一条是分类路径，另一条是语言模型路径。在自然语言处理中，你会一直看到LM。LM代表语言模型。分类路径将包含我们将用来创建情感分析模型的信息。语言模型路径将包含我们需要创建语言模型的信息。所以它们有一点不同。一个不同之处是，在分类路径中创建train.csv时，我们会删除所有标签为2的内容，因为标签为2是“无监督”的，我们不能使用它。
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The second difference is the labels [[26:51](https://youtu.be/h5Tz7gZT9Fo?t=26m51s)].
    For the classification path, the labels are the actual labels, but for the language
    model, there are no labels so we just use a bunch of zeros and that just makes
    it a little easier because we can use a consistent dataframe/CSV format.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个不同之处是标签[[26:51](https://youtu.be/h5Tz7gZT9Fo?t=26m51s)]。对于分类路径，标签是实际标签，但对于语言模型，没有标签，所以我们只使用一堆零，这样做会更容易一些，因为我们可以使用一致的数据框/CSV格式。
- en: Now the language model, we can create our own validation set, so you’ve probably
    come across by now, `sklearn.model_selection.train_test_split` which is a really
    simple function that grabs a dataset and randomly splits it into a training set
    and a validation set according to whatever proportion you specify. In this case,
    we concatenate our classification training and validation together, split it by
    10%, now we have 90,000 training, 10,000 validation for our language model. So
    that’s getting the data in a standard format for our language model and our classifier.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在语言模型，我们可以创建自己的验证集，所以你可能已经遇到了 `sklearn.model_selection.train_test_split`，这是一个非常简单的函数，根据你指定的比例随机将数据集分割成训练集和验证集。在这种情况下，我们将我们的分类训练和验证合并在一起，按10%进行分割，现在我们有90,000个训练数据，10,000个验证数据用于我们的语言模型。这样就为我们的语言模型和分类器以标准格式获取了数据。
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Language model tokens [[28:03](https://youtu.be/h5Tz7gZT9Fo?t=28m3s)]
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型标记[[28:03](https://youtu.be/h5Tz7gZT9Fo?t=28m3s)]
- en: The next thing we need to do is tokenization. Tokenization means at this stage,
    for a document (i.e. a movie review), we have a big long string and we want to
    turn it into a list of tokens which is similar to a list of words but not quite.
    For example, `don’t` we want it to be `do` and `n’t`, we probably want full stop
    to be a token, and so forth. Tokenization is something that we passed off to a
    terrific library called spaCy — partly terrific because the Australian wrote it
    and partly terrific because it’s good at what it does. We put a bit of stuff on
    top of spaCy but the vast majority of the work’s been done by spaCy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是标记化。标记化意味着在这个阶段，对于一个文档（比如一部电影评论），我们有一个很长的字符串，我们想将其转换为一个标记列表，类似于一个单词列表但不完全相同。例如，`don’t`我们希望它变成`do`和`n’t`，我们可能希望句号成为一个标记，等等。标记化是我们交给一个名为spaCy的绝妙库的事情
    — 部分原因是因为它是由澳大利亚人编写的，部分原因是因为它擅长它所做的事情。我们在spaCy之上加了一些东西，但绝大部分工作都是由spaCy完成的。
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before we pass it to spaCy, Jeremy wrote this simple `fixup` function which
    is each time he’s looked at different datasets (about a dozen in building this),
    every one had different weird things that needed to be replaced. So here are all
    the ones he’s come up with so far, and hopefully this will help you out as well.
    All the entities are html unescaped and there are bunch more things we replace.
    Have a look at the result of running this on text that you put in and make sure
    there’s no more weird tokens in there.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在将其传递给spaCy之前，Jeremy编写了这个简单的`fixup`函数，每次他查看不同的数据集（在构建过程中大约有十几个），每个数据集都有不同的奇怪之处需要替换。所以这是他迄今为止想出的所有内容，希望这也能帮助到你。所有实体都是html未转义的，还有更多我们替换的内容。看看在你输入的文本上运行这个函数的结果，并确保里面没有更多奇怪的标记。
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`get_all function` calls `get_texts` and get_texts is going to do a few things
    [[29:40](https://youtu.be/h5Tz7gZT9Fo?t=29m40s)]. One of which is to apply that
    `fixup` that we just mentioned.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_all`函数调用`get_texts`，而`get_texts`将做一些事情[[29:40](https://youtu.be/h5Tz7gZT9Fo?t=29m40s)]。其中之一是应用我们刚提到的`fixup`。'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s look through this because there is some interesting things to point out
    [[29:57](https://youtu.be/h5Tz7gZT9Fo?t=29m57s)]. We are going to use pandas to
    open our train.csv from the language model path, but we are passing in an extra
    parameter you may not have seen before called `chunksize`. Python and pandas can
    both be pretty inefficient when it comes to storing and using text data. So you’ll
    see that very few people in NLP are working with large corpuses. And Jeremy thinks
    the part of the reason is that traditional tools made it really difficult — you
    run out of memory all the time. So this process he is showing us today, he has
    used on corpuses of over a billion words successfully using this exact code. One
    of the simple trick is this thing called `chunksize` with pandas. That that means
    is that pandas does not return a data frame, but it returns an iterator that we
    can iterate through chunks of a data frame. That is why we don’t say `tok_trn
    = get_text(df_trn)` but instead we call `get_all` which loops through the data
    frame but actually what it’s really doing is it’s looping through chunks of the
    data frame so each of those chunks is basically a data frame representing a subset
    of the data [[31:05]](https://youtu.be/h5Tz7gZT9Fo?t=31m5s).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看一下，因为有一些有趣的事情要指出。我们将使用pandas打开我们的train.csv文件，但是我们传入了一个你可能以前没有见过的额外参数，叫做`chunksize`。当涉及存储和使用文本数据时，Python和pandas都可能非常低效。所以你会发现，在NLP领域很少有人在处理大型语料库。Jeremy认为部分原因是传统工具使得这一过程非常困难——你总是会耗尽内存。所以他今天向我们展示的这个过程，他已经成功地在超过十亿字的语料库上使用了这段代码。其中一个简单的技巧就是pandas中的`chunksize`。这意味着pandas不会返回一个数据框，而是返回一个我们可以迭代遍历数据框块的迭代器。这就是为什么我们不说`tok_trn
    = get_text(df_trn)`，而是调用`get_all`，它会遍历数据框，但实际上它正在遍历数据框的块，因此每个块基本上是代表数据子集的数据框。
- en: '**Question**: When I’m working with NLP data, many times I come across data
    with foreign texts/characters. Is it better to discard them or keep them [[31:31](https://youtu.be/h5Tz7gZT9Fo?t=31m31s)]?
    No no, definitely keep them. This whole process is unicode and I’ve actually used
    this on Chinese text. This is designed to work on pretty much anything. In general,
    most of the time, it’s not a good idea to remove anything. Old-fashioned NLP approaches
    tended to do all this like lemmatization and all these normalization steps to
    get rid things, lower case everything, etc. But that’s throwing away information
    which you don’t know ahead of time whether it’s useful or not. So don’t throw
    away information.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：当我处理NLP数据时，很多时候我会遇到包含外文文本/字符的数据。是更好地丢弃它们还是保留它们？不，绝对要保留它们。整个过程都是unicode的，我实际上已经在中文文本上使用过这个过程。这个过程设计用于几乎任何东西。一般来说，大多数情况下，删除任何东西都不是一个好主意。老式的NLP方法倾向于执行所有这些词形还原和所有这些规范化步骤来摆脱东西，将所有东西转换为小写等等。但这是在丢弃你事先不知道是否有用的信息，所以不要丢弃信息。'
- en: 'So we go through each chunk each of which is a data frame and we call `get_texts`
    [[32:19](https://youtu.be/h5Tz7gZT9Fo?t=32m19s)]. `get_texts` will grab the labels
    and makes them into integers, and it’s going to grab the texts. A couple things
    to point out:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们遍历每个块，每个块都是一个数据框，然后我们调用`get_texts`。`get_texts`将获取标签并将它们转换为整数，并且它将获取文本。有几点需要指出：
- en: Before we include the text, we have “beginning of stream” (`BOS`) token which
    we defined in the beginning. There’s nothing special about these particular strings
    of letters — they are just ones I figured don’t appear in normal texts very often.
    So every text is going to start with `‘xbos’` — why is that? Because it’s often
    useful for your model to know when a new text is starting. For example, if it’s
    a language model, we are going to concatenate all the texts together. So it would
    be really helpful for it to know all this articles finished and a new one started
    so I should probably forget some of their context now.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在包含文本之前，我们有一个“流的开始”（`BOS`）标记，我们在开始时定义了。这些特定的字母字符串并没有什么特别之处——它们只是我发现在正常文本中很少出现的。因此，每个文本都将以`‘xbos’`开头——为什么呢？因为对于你的模型来说，知道何时开始一个新文本通常是有用的。例如，如果是一个语言模型，我们将把所有文本连接在一起。因此，让它知道所有这些文章何时结束以及新文章何时开始是非常有帮助的，这样我可能应该忘记它们的一些上下文了。
- en: Ditto is quite often texts have multiple fields like a title and abstract, and
    then a main document. So by the same token, we’ve got this thing here which lets
    us actually have multiple fields in our CSV. So this process is designed to be
    very flexible. Again at the start of each one, we put a special “field starts
    here” token followed by the number of the field that’s starting here for as many
    fields as we have. Then we apply `fixup` to it.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ditto经常出现的情况是文本有多个字段，比如标题和摘要，然后是主要文档。因此，同样地，我们在CSV中可以有多个字段。这个过程设计得非常灵活。在每个字段的开始，我们放置一个特殊的“字段开始于此”标记，后面跟着这个字段开始的编号，对于我们有多少个字段就有多少个。然后我们对其应用`fixup`。
- en: Then most importantly [[33:54](https://youtu.be/h5Tz7gZT9Fo?t=33m54s)], we tokenize
    it — we tokenize it by doing a “process all multiprocessing” (`proc_all_mp`).
    Tokenizing tends to be pretty slow but we’ve all got multiple cores in our machines
    now, and some of the better machines on AWS can have dozens of cores. spaCy is
    not very amenable to multi processing but Jeremy finally figured out how to get
    it to work. The good news is that it’s all wrapped up in this one function now.
    So all you need to pass to that function is a list of things to tokenize which
    each part of that list will be tokenized on a different core. There is also a
    function called `partition_by_cores` which takes a list and splits it into sublists.
    The number of sublists is the number of cores that you have in your computer.
    On Jeremy’s machine without multiprocessing, this takes about an hour and a half,
    and with multiprocessing, it takes about 2 minutes. So it’s a really hand thing
    to have. Feel free to look inside it and take advantage of it for your own stuff.
    Remember, we all have multiple cores even in our laptops and very few things in
    Python take advantage or it unless you make a bit of an effort to make it work.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后最重要的是[[33:54](https://youtu.be/h5Tz7gZT9Fo?t=33m54s)], 我们对其进行标记化 - 通过进行“process
    all multiprocessing” (`proc_all_mp`) 进行标记化。标记化往往会很慢，但现在我们的机器都有多个核心，AWS上一些更好的机器可以有几十个核心。spaCy不太适合多处理，但Jeremy最终找到了让它工作的方法。好消息是现在所有这些都包含在这一个函数中。所以你只需要传递给该函数一个要标记化的事物列表，该列表的每个部分将在不同的核心上进行标记化。还有一个名为`partition_by_cores`的函数，它接受一个列表并将其拆分为子列表。子列表的数量就是您计算机上的核心数量。在Jeremy的机器上，没有多处理，这需要大约一个半小时，而使用多处理，大约需要2分钟。所以这是一个非常方便的东西。随时查看并利用它来处理您自己的东西。记住，我们的笔记本电脑中都有多个核心，而且很少有Python中的东西能够利用它，除非您稍微努力使其工作。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here is the result at the end [[35:42](https://youtu.be/h5Tz7gZT9Fo?t=35m42s)].
    Beginning of the stream token (`xbos`), beginning of field number 1 token (`xfld
    1`), and tokenized text. You’ll see that the punctuation is on whole now a separate
    token.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终结果[[35:42](https://youtu.be/h5Tz7gZT9Fo?t=35m42s)]。流的开始标记（`xbos`），第1个字段的开始标记（`xfld
    1`），以及标记化的文本。您会看到标点现在是一个单独的标记。
- en: '`**t_up**`: `t_up mgm` — MGM was originally capitalized. But the interesting
    thing is that normally people either lowercase everything or they leave the case
    as is. Now if you leave the case as is, then “SCREW YOU” and “screw you” are two
    totally different sets of tokens that have to be learnt from scratch. Or if you
    lowercase them all, then there is no difference at all. So how do you fix this
    so that you both get a semantic impact of “I’M SHOUTING NOW” but not have to learn
    the shouted version vs. the normal version. So the idea is to come up with a unique
    token to mean the next thing is all uppercase. Then we lowercase it, so now whatever
    used to be uppercase is lowercased, and then we can learn the semantic meaning
    of all uppercase.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`**t_up**`: `t_up mgm` - MGM最初是大写的。但有趣的是，通常人们要么全部小写，要么保持大小写不变。现在如果保持大小写不变，那么“SCREW
    YOU”和“screw you”是两组完全不同的标记，必须从头开始学习。或者如果全部小写，那么根本没有区别。那么如何解决这个问题，以便既获得“我现在在大声喊叫”的语义影响，又不必学习大声喊叫版本与正常版本。所以想法是想出一个唯一的标记，表示下一个事物全是大写。然后我们将其小写，所以现在以前大写的部分被小写，然后我们可以学习全部大写的语义含义。'
- en: '`**tk_rep**`: Similarly, if you have 29 `!` in a row, we don’t learn a separate
    token for 29 exclamation marks — instead we put in a special token for “the next
    thing repeats lots of times” and then put the number 29 and an exclamation mark
    (i.e. `tk_rep 29 !`). So there are a few tricks like that. If you are interested
    in NLP, have a look at the tokenizer code for these little tricks that Jeremy
    added in because some of them are kind of fun.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`**tk_rep**`: 同样，如果您连续有29个`!`，我们不会为29个感叹号学习一个单独的标记 - 而是为“下一个事物重复很多次”放入一个特殊的标记，然后放入数字29和一个感叹号（即`tk_rep
    29 !`）。所以有一些类似的技巧。如果您对NLP感兴趣，请查看Jeremy添加的这些小技巧的标记器代码，因为其中一些很有趣。'
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The nice thing with doing things this way is we can now just `np.save` that
    and load it back up later [[37:44](https://youtu.be/h5Tz7gZT9Fo?t=37m44s)]. We
    don’t have to recalculate all this stuff each time like we tend to have to do
    with torchtext or a lot of other libraries. Now that we got it tokenized, the
    next thing we need to do is to turn it into numbers which we call numericalizing
    it. The way we numericalize it is very simple.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 用这种方式做事情的好处是我们现在可以只需`np.save`一下，稍后再加载回来[[37:44](https://youtu.be/h5Tz7gZT9Fo?t=37m44s)]。我们不必像我们通常需要在torchtext或许多其他库中那样每次都重新计算所有这些东西。现在我们已经将其标记化，下一步需要做的是将其转换为数字，我们称之为数字化。我们数字化的方式非常简单。
- en: We make a list list of all the words that appear in some order
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们制作一个按某种顺序出现的所有单词的列表
- en: Then we replace every word with its index into that list
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们用该列表中的索引替换每个单词。
- en: The list of all the tokens, we call that the vocabulary.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有标记的列表，我们称之为词汇表。
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here is an example of some of the vocabulary [[38:28](https://youtu.be/h5Tz7gZT9Fo?t=38m28s)].
    The Counter class in Python is very handy for this. It basically gives us a list
    of unique items and their counts. Here are the 25 most common things in the vocabulary.
    Generally speaking, we don’t want every unique token in our vocabulary. If it
    doesn’t appear at least twice then might just be a spelling mistake or a word
    we can’t learn anything about it if it doesn’t appear that often. Also the stuff
    we are going to be learning about so far in this part gets a bit clunky once you’ve
    got a vocabulary bigger than 60,000\. Time permitting, we may look at some work
    Jeremy has been doing recently on handling larger vocabularies, otherwise that
    might have to come in a future course. But actually for classification, doing
    more than about 60,000 words doesn’t seem to help anyway.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些词汇的例子。Python中的Counter类对此非常有用。它基本上为我们提供了一个独特项目和它们的计数的列表。这里是词汇中最常见的25个东西。一般来说，我们不希望在我们的词汇表中有每个独特的标记。如果它不至少出现两次，那可能只是一个拼写错误或者一个我们无法学到任何东西的词，如果它不经常出现的话。此外，在这一部分我们将要学习的东西一旦词汇量超过60,000就会变得有些笨重。如果时间允许，我们可能会看一下Jeremy最近在处理更大词汇量方面所做的一些工作，否则这可能会在未来的课程中出现。但实际上，对于分类来说，超过60,000个词并没有什么帮助。
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So we are going to limit our vocabulary to 60,000 words, things that appear
    at least twice [[39:33](https://youtu.be/h5Tz7gZT9Fo?t=39m33s)]. Here is a simple
    way to do that. Use `.most_common`, pass in the max vocab size. That’ll sort it
    by the frequency and if it appears less often than a minimum frequency, then don’t
    bother with it at all. That gives us `itos` — that’s the same name that torchtext
    used and it means integer-to-string. This is just the list of unique tokens in
    the vocab. We’ll insert two more tokens — a vocab item for unknown (`_unk_`) and
    a vocab item for padding (`_pad_`).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将把我们的词汇表限制在60,000个词，至少出现两次的东西。这里有一个简单的方法。使用`.most_common`，传入最大词汇大小。这将按频率排序，如果出现的频率低于最小频率，则根本不要理会。这给我们了`itos`
    - 这是torchtext使用的相同名称，意思是整数到字符串。这只是词汇表中独特标记的列表。我们将插入两个额外的标记 - 一个未知的词汇项（`_unk_`）和一个填充的词汇项（`_pad_`）。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can then create the dictionary which goes in the opposite direction (string
    to integer)[[40:19](https://youtu.be/h5Tz7gZT9Fo?t=40m19s)]. That won’t cover
    everything because we intentionally truncated it down to 60,000 words. If we come
    across something that is not in the dictionary, we want to replace it with zero
    for unknown so we can use `defaultdict` with a lambda function that always returns
    zero.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个字典，它是相反的（从字符串到整数）。这不会覆盖所有内容，因为我们故意将它截断到60,000个词。如果我们遇到字典中没有的东西，我们希望用零替换它，表示未知，所以我们可以使用带有lambda函数的defaultdict，它总是返回零。
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So now we have our `stoi` dictionary defined, we can then call that for every
    word for every sentence [[40:50](https://youtu.be/h5Tz7gZT9Fo?t=40m50s)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义了我们的`stoi`字典，我们可以为每个句子的每个单词调用它。
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is our numericalized version:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数字化版本：
- en: Of course, the nice thing is we can save that step as well. Each time we get
    to another step, we can save it. These are not very big files compared to what
    you are used with images. Text is generally pretty small.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，好处是我们也可以保存这一步。每次我们到达另一个步骤时，我们都可以保存它。与您用于图像的文件相比，这些文件并不是很大。文本通常很小。
- en: Very important to also save that vocabulary (`itos`). The list of numbers means
    nothing unless you know what each number refers to, and that’s what `itos` tells
    you.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是也保存那个词汇表（`itos`）。数字列表没有意义，除非你知道每个数字指的是什么，这就是`itos`告诉你的。
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So you save those three things, and later on you can load them back up.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你保存这三样东西，以后你可以重新加载它们。
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now our vocab size is 60,002 and our training language model has 90,000 documents
    in it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的词汇量是60,002，我们的训练语言模型中有90,000个文档。
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That’s the preprocessing you do [[42:01](https://youtu.be/h5Tz7gZT9Fo?t=42m1s)].
    We can probably wrap a little bit more of that in utility functions if we want
    to but it’s all pretty straight forward and that exact code will work for any
    dataset you have once you’ve got it in that CSV format.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你要做的预处理。如果我们想的话，我们可以将更多的内容包装在实用函数中，但这一切都非常简单明了，一旦你将数据集转换为CSV格式，这段代码就可以适用于任何数据集。
- en: Pre-training [[42:19](https://youtu.be/h5Tz7gZT9Fo?t=42m19s)]
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练
- en: Here is kind of a new insight that’s not new at all which is that we’d like
    to pre-train something. We know from lesson 4 that if we pre-train our classifier
    by first creating a language model and then fine-tuning that as a classifier,
    that was helpful. It actually got us a new state-of-the-art result — we got the
    best IMDb classifier result that had been published by quite a bit. We are not
    going that far enough though, because IMDb movie reviews are not that different
    to any other English document; compared to how different they are to a random
    string or even to a Chinese document. So just like ImageNet allowed us to train
    things that recognize stuff that kind of looks like pictures, and we could use
    it on stuff that was nothing to do with ImageNet like satellite images. Why don’t
    we train a language model that’s good at English and then fine-tune it to be good
    at movie reviews.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种新的见解，实际上并不新，我们想要预先训练一些东西。我们从第4课中知道，如果我们通过首先创建一个语言模型，然后将其微调为分类器来预训练我们的分类器，那是有帮助的。实际上，这给我们带来了一个新的最先进的结果
    - 我们得到了最好的IMDb分类器结果，这个结果比之前发布的要好得多。然而，我们还没有走得那么远，因为IMDb电影评论与任何其他英文文档并没有太大不同；与随机字符串或甚至中文文档相比，它们之间的差异并不大。所以就像ImageNet让我们能够训练识别看起来像图片的东西的模型一样，我们可以将其用于与ImageNet无关的东西，比如卫星图像。为什么我们不训练一个擅长英语的语言模型，然后微调它以擅长电影评论呢。
- en: So this basic insight led Jeremy to try building a language model on Wikipedia.
    Stephen Merity has already processed Wikipedia, found a subset of nearly the most
    of it, but throwing away the stupid little articles leaving bigger articles. He
    calls that wikitext103\. Jeremy grabbed wikitext103 and trained a language model
    on it. He used exactly the same approach he’s about to show you for training an
    IMDb language model, but instead he trained a wikitext103 language model. He saved
    it and made it available for anybody who wants to use it at [this URL](http://files.fast.ai/models/wt103/).
    The idea now is let’s train an IMDb language model which starts with these weights.
    Hopefully to you folks, this is an extremely obvious, extremely non-controversial
    idea because it’s basically what we’ve done in nearly every class so far. But
    when Jeremy first mentioned this to people in the NLP community June or July of
    last year, there couldn’t have been less interest and was told it was stupid [[45:03](https://youtu.be/h5Tz7gZT9Fo?t=45m3s)].
    Because Jeremy was obstreperous, he ignored them even though they know much more
    about NLP and tried it anyway. And let’s see what happened.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的想法让Jeremy尝试在维基百科上构建一个语言模型。Stephen Merity已经处理了维基百科，找到了几乎大部分内容的子集，但是丢弃了一些无关紧要的小文章，只留下了较大的文章。他称之为wikitext103。Jeremy拿到了wikitext103并在上面训练了一个语言模型。他使用了与他即将向你展示的训练IMDb语言模型完全相同的方法，但是他训练了一个wikitext103语言模型。他保存了这个模型，并且让任何想要使用它的人都可以在这个URL上找到。现在的想法是让我们训练一个IMDb语言模型，它以这些权重为起点。希望对你们来说，这是一个非常明显、非常不具争议的想法，因为这基本上是我们迄今为止在几乎每一堂课上所做的。但是当Jeremy去年六月或七月首次向NLP社区的人们提到这一点时，他们对此毫无兴趣，并被告知这是愚蠢的。因为Jeremy很固执，他忽略了他们，尽管他们对NLP了解更多，但还是尝试了。让我们看看发生了什么。
- en: wikitext103 conversion [[46:11](https://youtu.be/h5Tz7gZT9Fo?t=46m11s)]
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: wikitext103 转换 [[46:11](https://youtu.be/h5Tz7gZT9Fo?t=46m11s)]
- en: Here is how we do it. Grab the wikitext models. If you do `wget -r`, it will
    recursively grab the whole directory which has a few things in it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何做的。获取wikitext模型。如果你使用`wget -r`，它将递归地抓取整个目录，其中有一些东西。
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We need to make sure that our language model has exactly the same embedding
    size, number of hidden, and number of layers as Jeremy’s wikitext one did otherwise
    you can’t load the weights in.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们的语言模型具有与Jeremy的wikitext相同的嵌入大小、隐藏数量和层数，否则你无法加载这些权重。
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here are our pre-trained path and our pre-trained language model path.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的预训练路径和我们的预训练语言模型路径。
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s go ahead and `torch.load` in those weights from the forward wikitext103
    model. We don’t normally use torch.load, but that’s the PyTorch way of grabbing
    a file. It basically gives you a dictionary containing the name of the layer and
    a tensor/array of those weights.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续从前向wikitext103模型中`torch.load`这些权重。我们通常不使用torch.load，但这是PyTorch抓取文件的方式。它基本上给你一个包含层名称和这些权重的张量/数组的字典。
- en: 'Now the problem is that wikitext language model was built with a certain vocabulary
    which was not the same as ours [[47:14](https://youtu.be/h5Tz7gZT9Fo?t=47m14s)].
    Our #40 is not the same as wikitext103 model’s #40\. So we need to map one to
    the other. That’s very very simple because luckily Jeremy saved `itos` for the
    wikitext vocab.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，wikitext语言模型是建立在一个特定词汇表上的，这个词汇表与我们的不同。我们的#40不同于wikitext103模型的#40。所以我们需要将一个映射到另一个。这非常简单，因为幸运的是Jeremy保存了wikitext词汇表的`itos`。
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here is the list of what each word is for wikitext103 model, and we can do the
    same `defaultdict` trick to map it in reverse. We’ll use -1 to mean that it is
    not in the wikitext dictionary.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是wikitext103模型中每个单词的列表，我们可以使用相同的`defaultdict`技巧来反向映射。我们将使用-1来表示它不在wikitext词典中。
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So now we can just say our new set of weights is just a whole bunch of zeros
    with vocab size by embedding size (i.e. we are going to create an embedding matrix)
    [[47:57](https://youtu.be/h5Tz7gZT9Fo?t=47m57s)]. We then go through every one
    of the words in our IMDb vocabulary. We are going to look it up in `stoi2` (string-to-integer
    for the wikitext103 vocabulary) and see if it’s a word there. If that is a word
    there, then we won’t get the `-1`. So `r` will be greater than or equal to zero,
    so in that case, we will just set that row of the embedding matrix to the weight
    which was stored inside the named element `‘0.encoder.weight’`. You can look at
    this dictionary `wgts` and it’s pretty obvious what each name corresponds to.
    It looks very similar to the names that you gave it when you set up your module,
    so here are the encoder weights.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以说我们的新权重集只是一个由词汇大小乘以嵌入大小（即我们将创建一个嵌入矩阵）的一大堆零。然后我们遍历我们IMDb词汇表中的每一个单词。我们将在wikitext103词汇表的`stoi2`（字符串到整数）中查找它，并查看它是否是一个单词。如果那是一个单词，那么我们就不会得到`-1`。所以`r`将大于或等于零，那么在这种情况下，我们将把嵌入矩阵的那一行设置为存储在名为`‘0.encoder.weight’`的元素内的权重。你可以查看这个字典`wgts`，很明显每个名称对应什么。它看起来非常类似于你在设置模块时给它的名称，所以这里是编码器权重。
- en: If we don’t find it [[49:02](https://youtu.be/h5Tz7gZT9Fo?t=49m2s)], we will
    use the row mean — in other words, here is the average embedding weight across
    all of the wikitext103\. So we will end up with an embedding matrix for every
    word that’s in both our vocabulary for IMDb and the wikitext103 vocab, we will
    use the wikitext103 embedding matrix weights; for anything else, we will just
    use whatever was the average weight from the wikitext103 embedding matrix.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找不到它 [[49:02](https://youtu.be/h5Tz7gZT9Fo?t=49m2s)], 我们将使用行均值——换句话说，这是wikitext103中所有嵌入权重的平均值。因此，我们将得到一个嵌入矩阵，其中包含我们的IMDb词汇表和wikitext103词汇表中的每个单词；我们将使用wikitext103嵌入矩阵权重；对于其他任何单词，我们将只使用wikitext103嵌入矩阵中的平均权重。
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We will then replace the encoder weights with `new_w` turn into a tensor [[49:35](https://youtu.be/h5Tz7gZT9Fo?t=49m35s)].
    We haven’t talked much about weight tying, but basically the decoder (the thing
    that turns the final prediction back into a word) uses exactly the same weights,
    so we pop it there as well. Then there is a bit of weird thing with how we do
    embedding dropout that ends up with a whole separate copy of them for a reason
    that doesn’t matter much. So we popped the weights back where they need to go.
    So this is now a set of torch state which we can load in.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将用`new_w`替换编码器权重，变成一个张量[[49:35](https://youtu.be/h5Tz7gZT9Fo?t=49m35s)]。我们没有谈论过权重绑定，但基本上解码器（将最终预测转换回单词的部分）使用完全相同的权重，所以我们也将它放在那里。然后有一个关于我们如何进行嵌入丢弃的奇怪事情，最终导致它们有一个完全独立的副本，原因并不重要。所以我们把权重放回它们需要去的地方。所以现在这是一组torch状态，我们可以加载进去。
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Language model [[50:18](https://youtu.be/h5Tz7gZT9Fo?t=50m18s)]
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型[[50:18](https://youtu.be/h5Tz7gZT9Fo?t=50m18s)]
- en: 'Let’s create our language model. Basic approach we are going to use is we are
    going to concatenate all of the documents together into a single list of tokens
    of length 24,998,320\. That is going to be what we pass in as a training set.
    So for the language model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的语言模型。我们将使用的基本方法是将所有文档连接在一起，形成一个长度为24,998,320的单词标记列表。这将是我们作为训练集传入的内容。所以对于语言模型：
- en: We take all our documents and just concatenate them back to back.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将所有文档连接在一起。
- en: We are going to be continuously trying to predict what’s the next word after
    these words.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不断尝试预测这些单词之后的下一个单词。
- en: We will set up a whole bunch of dropouts.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将设置一系列的丢弃。
- en: Once we have a model data object, we can grab the model from it, so that’s going
    to give us a learner.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们有了一个模型数据对象，我们就可以从中获取模型，这样就会给我们一个学习者。
- en: Then as per usual, we can call `learner.fit`. We do a single epoch on the last
    layer just to get that okay. The way it’s set up is the last layer is the embedding
    words because that’s obviously the thing that’s going to be the most wrong because
    a lot of those embedding weights didn’t even exist in the vocab. So we will train
    a single epoch of just the embedding weights.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后像往常一样，我们可以调用`learner.fit`。我们在最后一层上进行一个周期，只是为了确认一下。它的设置是最后一层是嵌入单词，因为显然这是最可能出错的地方，因为很多这些嵌入权重甚至在词汇表中都不存在。所以我们将训练一个周期，只是针对嵌入权重。
- en: Then we’ll start doing a few epochs of the full model. How is it looking? In
    lesson 4, we had the loss of 4.23 after 14 epochs. In this case, we have 4.12
    loss after 1 epoch. So by pre-training on wikitext103, we have a better loss after
    1 epoch than the best loss we got for the language model otherwise.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将开始对完整模型进行几个周期的训练。看起来怎么样？在第4课中，我们在14个周期后的损失为4.23。在这种情况下，我们在1个周期后的损失为4.12。因此，通过在wikitext103上进行预训练，我们在1个周期后的损失比其他情况下语言模型的最佳损失更好。
- en: '**Question**: What is the wikitext103 model? Is it a AWD LSTM again [[52:41](https://youtu.be/h5Tz7gZT9Fo?t=52m41s)]?
    Yes, we are about to dig into that. The way I trained it was literally the same
    lines of code that you see above, but without pre-training it on wikitext103.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：wikitext103模型是什么？它再次是AWD LSTM吗[[52:41](https://youtu.be/h5Tz7gZT9Fo?t=52m41s)]？是的，我们即将深入研究。我训练它的方式实际上与您在上面看到的代码行完全相同，但没有在wikitext103上进行预训练。'
- en: A quick discussion about fastai doc project [[53:07](https://youtu.be/h5Tz7gZT9Fo?t=53m7s)]
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于fastai文档项目的简要讨论[[53:07](https://youtu.be/h5Tz7gZT9Fo?t=53m7s)]
- en: 'The goal of fastai doc project is to create documentation that makes readers
    say “wow, that’s the most fantastic documentation I’ve ever read” and we have
    some specific ideas about how to do that. It’s the same kind of idea of top-down,
    thoughtful, take full advantage of the medium approach, interactive experimental
    code first that we are all familiar with. If you are interested in getting involved,
    you can see the basic approach in [the docs directory](https://github.com/fastai/fastai/tree/master/docs).
    In there, there is, amongst other things, [transforms-tmpl.adoc](https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc).
    `adoc` is [AsciiDoc](http://asciidoc.org/). AsciiDoc is like markdown but it’s
    like what markdown needs to be to create actual books. A lot of actual books are
    written in AsciiDoc and it’s as easy to use as markdown but there’s way more cool
    stuff you can do with it. [Here](https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc)
    is more standard asciiDoc example. You can do things like inserting a table of
    contents (`:toc:`). `::` means put a definition list here. `+` means this is a
    continuation of the previous list item. So there are many super handy features
    and it is like turbo-charged markdown. So this asciidoc creates this HTML and
    no custom CSS or anything added:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: fastai文档项目的目标是创建让读者说“哇，这是我读过的最棒的文档”并且我们有一些关于如何做到这一点的具体想法。这是一种自上而下、深思熟虑、充分利用媒体的方法，交互式实验代码优先，我们都很熟悉。如果您有兴趣参与，您可以在[docs目录](https://github.com/fastai/fastai/tree/master/docs)中看到基本方法。在那里，除其他内容外，还有[transforms-tmpl.adoc](https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc)。`adoc`是[AsciiDoc](http://asciidoc.org/)。AsciiDoc类似于markdown，但它更像是markdown需要成为创建实际书籍的工具。许多实际的书籍都是用AsciiDoc编写的，它和markdown一样易于使用，但你可以用它做更多很酷的事情。[这里](https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc)是更标准的AsciiDoc示例。您可以做一些像插入目录(`:toc:`)这样的事情。`::`表示在这里放一个定义列表。`+`表示这是前一个列表项的延续。所以有许多非常方便的功能，它就像是增强版的markdown。因此，这个asciidoc会创建这个HTML，没有添加自定义CSS或其他内容：
- en: We literally started this project 4 hours ago. So you have a table of contents
    with hyper links to specific sections. We have cross reference we can click on
    to jump straight to the cross reference. Each method comes along with its details
    and so on. To make things even easier, they’ve created a special template for
    argument, cross reference, method, etc. The idea is, it will almost be like a
    book. There will be tables, pictures, video segments, and hyperlink throughout.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开始这个项目4个小时。所以你有一个带有超链接到特定部分的目录。我们有交叉引用，我们可以点击跳转到交叉引用。每种方法都附带其详细信息等等。为了使事情更加简单，他们创建了一个专门的模板用于参数、交叉引用、方法等。这个想法是，它几乎会像一本书一样。将会有表格、图片、视频片段和整个超链接。
- en: You might be wondering what about docstrings. But actually, if you look at the
    Python standard library and look at the docstring for `re.compile()`, for example,
    it’s a single line. Nearly every docstring in Python is a single line. And Python
    then does exactly this — they have a website containing the documentation that
    says “this is what regular expressions are, and this is what you need to know
    about them, and if you want do them fast, you need to compile, and here is some
    information about compile” etc. These information is not in the docstring and
    that’s how we are going to do as well — our docstring will be one line unless
    you need like two sometimes. Everybody is welcome to help contribute to the documentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想到文档字符串怎么办。但实际上，如果你查看Python标准库并查看`re.compile()`的文档字符串，例如，它只有一行。几乎每个Python的文档字符串都是一行。然后Python确实这样做——他们有一个包含文档的网站，上面写着“这就是正则表达式是什么，这就是你需要知道的关于它们的东西，如果你想要快速执行它们，你需要编译，这里有一些关于编译的信息”等等。这些信息不在文档字符串中，这也是我们将要做的——我们的文档字符串将是一行，除非有时候你需要两行。欢迎每个人帮助贡献文档。
- en: '**Question**: Hoes this compare to word2vec [[58:31](https://youtu.be/h5Tz7gZT9Fo?t=58m31s)]?
    This is actually a great thing for you to spend time thinking about during the
    week. I’ll give you the summary now but it’s a very important conceptual difference.
    The main conceptual difference is “what is word2vec?” Word2vec is a single embedding
    matrix — each word has a vector and that’s it. In other words, it’s a single layer
    from a pre-trained model — specifically that layer is the input layer. Also specifically
    that pre-trained model is a linear model that is pre-trained on something called
    a co-occurrence matrix. So we have no particular reason to believe that this model
    has learned anything much about English language or that it has any particular
    capabilities because it’s just a single linear layer and that’s it. What’s this
    wikitext103 model? It’s a language model and it has a 400 dimensional embedding
    matrix, 3 hidden layers with 1,150 activations per layer, and regularization and
    all that stuff tied input output matrices — it’s basically a state-of-the-art
    AWD LSTM. What’s the difference between a single layer of a single linear model
    vs. a three layer recurrent neural network? Everything! They are very different
    levels of capabilities. So you will see when you try using a pre-trained language
    model vs. word2vec layer, you’ll get very different results for the vast majority
    of tasks.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这与word2vec有什么比较？这实际上是一个很好的事情，你可以在这一周花时间思考。我现在会给你总结，但这是一个非常重要的概念区别。主要的概念区别是“word2vec是什么？”Word2vec是一个单一的嵌入矩阵——每个单词都有一个向量，就是这样。换句话说，它是一个来自预训练模型的单一层——具体来说，该层是输入层。而且具体来说，那个预训练模型是一个线性模型，它是在一个叫做共现矩阵的东西上进行预训练的。所以我们没有特别的理由相信这个模型已经学到了关于英语语言的很多东西，或者它有任何特殊的能力，因为它只是一个单一的线性层，就是这样。那么这个wikitext103模型呢？它是一个语言模型，有一个400维的嵌入矩阵，3个隐藏层，每层有1,150个激活，还有正则化和所有那些与输入输出矩阵相关的东西——基本上是一个最先进的AWD
    LSTM。一个单一线性模型的单一层与一个三层循环神经网络之间的区别是什么？一切！它们具有非常不同的能力水平。所以当你尝试使用一个预训练语言模型与word2vec层时，你会发现在绝大多数任务中得到非常不同的结果。
- en: '**Question**: What if the numpy array does not fit in memory? Is it possible
    to write a PyTorch data loader directly from a large CSV file [[1:00:32](https://youtu.be/h5Tz7gZT9Fo?t=1h32s)]?
    It almost certainly won’t come up, so I’m not going to spend time on it. These
    things are tiny — they are just integers. Think about how many integers you would
    need to run out of memories? That’s not gonna happen. They don’t have to fit in
    GPU memory, just in your memory. I’ve actually done another Wikipedia model which
    I called giga wiki which was on all of Wikipedia and even that easily fits in
    memory. The reason I’m not using it is because it turned out not to really help
    very much vs. wikitext103\. I’ve built a bigger model than anybody else I’ve found
    in the academic literature and it fits in memory on a single machine.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果numpy数组不适合内存怎么办？是否可以直接从大型CSV文件中编写PyTorch数据加载器？几乎肯定不会出现这种情况，所以我不会花时间在这上面。这些东西很小——它们只是整数。想想你需要多少整数才会耗尽内存？那是不会发生的。它们不必适合GPU内存，只需适合你的内存。我实际上做过另一个维基百科模型，我称之为giga
    wiki，它包含了整个维基百科，甚至那个也很容易适合内存。我之所以不使用它，是因为事实证明它与wikitext103相比并没有真正帮助太多。我建立了一个比我在学术文献中找到的任何其他人都要大的模型，并且它适合单台机器的内存。
- en: '**Question**: What is the idea behind averaging the weights of embeddings [[1:01:24](https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s)]?
    They have to be set to something. These are words that weren’t there, so the other
    option is we could leave them as zero. But that seems like a very extreme thing
    to do. Zero is a very extreme number. Why would it be zero? We could set it equal
    to some random numbers, but if so, what would be the mean and standard deviation
    of those random numbers? Should they be uniform? If we just average the rest of
    the embeddings, then we have something that’s reasonably scaled. Just to clarify,
    this is how we are initializing words that didn’t appear in the training corpus.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于嵌入权重进行平均化的背后思想是什么[[1:01:24](https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s)]？它们必须被设置为某个值。这些是之前没有出现过的单词，所以另一个选择是我们可以将它们设置为零。但这似乎是一个非常极端的做法。零是一个非常极端的数字。为什么要设置为零？我们可以将它设置为一些随机数，但如果是这样，那么这些随机数的均值和标准差是多少？它们应该是均匀的吗？如果我们只是平均化其他嵌入，那么我们就得到了一个合理缩放的东西。只是澄清一下，这就是我们如何初始化在训练语料库中没有出现过的单词。'
- en: Back to Language Model [[1:02:20](https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s)]
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到语言模型[[1:02:20](https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s)]
- en: This is a ton of stuff we’ve seen before, but it’s changed a little bit. It’s
    actually a lot easier than it was in part 1, but I want to go a little bit deeper
    into the language model loader.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前见过的大量内容，但有些地方有所改变。实际上，与第1部分相比，这部分要容易得多，但我想深入一点了解语言模型加载器。
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is the `LanguageModelLoader` and I really hope that by now, you’ve learned
    in your editor or IDE how to jump to symbols [[1:02:37](https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s)].
    I don’t want it to be a burden for you to find out what the source code of `LanguageModelLoader`
    is. If your editor doesn’t make it easy, don’t use that editor anymore. There’s
    lots of good free editors that make this easy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`LanguageModelLoader`，我真的希望到现在为止，你已经学会了在你的编辑器或IDE中如何跳转到符号[[1:02:37](https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s)]。我不希望你为了找出`LanguageModelLoader`的源代码而感到困扰。如果你的编辑器没有做到这一点，就不要再使用那个编辑器了。有很多好用的免费编辑器可以轻松实现这一点。
- en: So this is the source code for LanguageModelLoader, and it’s interesting to
    notice that it’s not doing anything particularly tricky. It’s not deriving from
    anything at all. What makes something that’s capable of being a data loader is
    that it’s something you can iterate over.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`LanguageModelLoader`的源代码，有趣的是它并没有做任何特别复杂的事情。它并没有从任何地方派生。能够作为数据加载器的关键是它可以被迭代。
- en: Here is the `fit` function inside fastai.model [[1:03:41](https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s)].
    This is where everything ends up eventually which goes through each epoch, creates
    an iterator from the data loader, and then just does a for loop through it. So
    anything you can do a for loop through can be a data loader. Specifically it needs
    to return tuples of independent and dependent variables for mini-batches.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是fastai.model中的`fit`函数[[1:03:41](https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s)]。这是最终所有东西都会经过的地方，它会遍历每个时代，从数据加载器创建一个迭代器，然后通过一个for循环进行遍历。所以任何你可以通过for循环遍历的东西都可以作为数据加载器。具体来说，它需要返回独立和依赖变量的元组，用于小批量。
- en: 'So anything with a `__iter__` method is something that can act as an iterator
    [[1:04:09](https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s)]. `yield` is a neat little Python
    keywords you probably should learn about if you don’t already know it. But it
    basically spits out a thing and waits for you to ask for another thing — normally
    in a for loop or something. In this case, we start by initializing the language
    model passing it in the numbers `nums` this is the numericalized long list of
    all of our documents concatenated together. The first thing we do is to “batchfy”
    it. This is the thing which quite a few of you got confused about last time. If
    our batch size is 64 and we have 25 million numbers in our list. We are not creating
    items of length 64 — we are creating 64 items in total. So each of them is of
    size `t` divided by 64 which is 390k. So that’s what we do here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以任何具有`__iter__`方法的东西都可以作为迭代器[[1:04:09](https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s)]。`yield`是一个很棒的Python关键字，如果你还不了解的话，你可能应该学习一下。它基本上会输出一个东西，然后等待你请求另一个东西——通常在一个for循环中。在这种情况下，我们通过传入数字`nums`来初始化语言模型，这是我们所有文档连接在一起的数字化长列表。我们首先要做的是“批量化”它。这是上次很多人感到困惑的地方。如果我们的批量大小是64，我们的列表中有2500万个数字。我们不是创建长度为64的项目——我们总共创建了64个项目。因此，每个项目的大小是`t`除以64，即390k。这就是我们在这里做的事情：
- en: '`data = data.view(self.bs, -1).t().contiguous()`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`data = data.view(self.bs, -1).t().contiguous()`'
- en: We reshape it so that this axis is of length 64 and `-1` is everything else
    (390k blob), and we transpose it. So that means that we now have 64 columns, 390k
    rows. Then what we do each time we do an iterate is we grab one batch of some
    sequence length, which is approximately equal to `bptt` (back prop through time)
    which we set to 70\. We just grab that many rows. So from `i` to `i+70` rows,
    we try to predict that plus one. Remember, we are trying to predict one past where
    we are up to.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对它进行重塑，使得这个轴的长度为64，`-1`表示其他所有内容（390k blob），然后我们对它进行转置。这意味着我们现在有64列，390k行。然后每次迭代时，我们抓取一批序列长度为`bptt`（通过时间反向传播）大约等于70的数据。我们只抓取那么多行。所以从第`i`行到第`i+70`行，我们尝试预测下一个。请记住，我们试图预测我们当前位置的下一个位置。
- en: So we have 64 columns and each of those is 1/64th of our 25 million tokens,
    and hundreds of thousands long, and we just grab 70 at a time [[1:06:29](https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s)].
    So each of those columns, each time we grab it, it’s going to kind of hook up
    to the previous column. That’s why we get this consistency. This language model
    is stateful which is really important.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有64列，每列是我们2500万个标记的1/64，数以百万计的长，我们每次只抓取70个[[1:06:29](https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s)]。所以每次我们抓取每列时，它都会与前一列连接起来。这就是为什么我们会得到这种一致性。这个语言模型是有状态的，这一点非常重要。
- en: 'Pretty much all of the cool stuff in the language model is stolen from Stephen
    Merity’s AWD-LSTM [[1:06:59](https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s)] including
    this little trick here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的几乎所有很酷的东西都是从Stephen Merity的AWD-LSTM中偷来的[[1:06:59](https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s)]，包括这里的这个小技巧：
- en: If we always grab 70 at a time and then we go back and do a new epoch, we’re
    going to grab exactly the same batches every time — there is no randomness. Normally,
    we shuffle our data every time we do an epoch or every time we grab some data
    we grab it at random. You can’t do that with a language model because this set
    has to join up to the previous set because it’s trying to learn the sentence.
    If you suddenly jump somewhere else, that doesn’t make any sense as a sentence.
    So Stephen’s idea is to say “okay, since we can’t shuffle the order, let’s instead
    randomly change the sequence length”. Basically, 95% of the time, we will use
    `bptt` (i.e. 70) but 5% of the time, we’ll use half that. Then he says “you know
    what, I’m not even going to make that the sequence length, I’m going to create
    a normally distributed random number with that average and a standard deviation
    of 5, and I’ll make that the sequence length.” So the sequence length is seventy-ish
    and that means every time we go through, we are getting slightly different batches.
    So we’ve got that little bit of extra randomness. Jeremy asked Stephen Merity
    where he came up with this idea, did he think of it? and he said “I think I thought
    of it, but it seemed so obvious that I bet I didn’t think of it” — which is true
    of every time Jeremy comes up with an idea in deep learning. It always seems so
    obvious that you just assume somebody else has thought of it. But Jeremy thinks
    Stephen thought of it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们每次都拿70个，然后回去做一个新的时代，我们每次都会拿到完全相同的批次 — 没有随机性。通常，我们每次做一个时代或每次拿一些数据时，我们都会随机洗牌我们的数据。但是对于语言模型来说，我们不能这样做，因为这个集合必须与之前的集合连接起来，因为它试图学习句子。如果你突然跳到别的地方，那作为一个句子就没有意义了。所以Stephen的想法是说“好吧，既然我们不能洗牌顺序，那么我们就随机改变序列长度”。基本上，95%的时间，我们会使用`bptt`（即70），但5%的时间，我们会使用一半。然后他说“你知道吗，我甚至不会把那作为序列长度，我会创建一个平均值为那个的正态分布随机数，标准差为5，然后我会把那作为序列长度。”
    所以序列长度大约是70，这意味着每次我们经过时，我们得到的批次会稍微不同。所以我们有了那一点额外的随机性。Jeremy问Stephen Merity他是从哪里得到这个想法的，他是自己想出来的吗？他说“我想我是自己想出来的，但似乎这么明显，以至于我觉得我可能没有想到”
    — 这对于Jeremy在深度学习中想出一个想法来说是真的。每次Jeremy想出一个想法时，它总是看起来如此明显，以至于你会假设别人已经想到了。但Jeremy认为Stephen是自己想出来的。
- en: '`LanguageModelLoader` is a nice thing to look at if you are trying to do something
    a bit unusual with a data loader [[1:08:55](https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s)].
    It’s a simple role model you can use as to creating a data loader from scratch
    — something that spits out batches of data.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`LanguageModelLoader` 是一个很好的东西，如果你想用数据加载器做一些不太常见的事情的话，可以看一下[[1:08:55](https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s)]。这是一个简单的角色模型，你可以用它来从头开始创建一个数据加载器
    — 一个可以输出数据批次的东西。'
- en: Our language model loader took in all of the documents concatenated together
    along with batch size and bptt [[1:09:14](https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s)].
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言模型加载器接收了所有文档连接在一起以及批次大小和bptt[[1:09:14](https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s)]。
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now generally speaking, we want to create a learner and the way we normally
    do that is by getting a model data object and calling some kind of method which
    have various names but often we call that method `get_model`. The idea is that
    the model data object has enough information to know what kind of model to give
    you. So we have to create that model data object which means we need LanguageModelData
    class which is very easy to do [[1:09:51](https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s)].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们想要创建一个学习器，通常我们这样做是通过获取一个模型数据对象并调用某种方法，这个方法有各种各样的名字，但通常我们称这个方法为`get_model`。这个想法是模型数据对象有足够的信息来知道给你什么样的模型。所以我们必须创建那个模型数据对象，这意味着我们需要一个非常容易做的LanguageModelData类[[1:09:51](https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s)]。
- en: Here are all of the pieces. We are going to create a custom learner, a custom
    model data class, and a custom model class. So a model data class, again this
    one doesn’t inherit from anything so you really see there’s almost nothing to
    do. You need to tell it most importantly what’s your training set (give it a data
    loader), what’s the validation set (give it a data loader), and optionally, give
    it a test set (data loader), plus anything else that needs to know. It might need
    to know the bptt, it needs to know number of tokens(i.e. the vocab size), and
    it needs to know what is the padding index. And so that it can save temporary
    files and models, model datas as always need to know the path. So we just grab
    all that stuff and we dump it. That’s it. That’s the entire initializer. There
    is no logic there at all.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有所有的部分。我们将创建一个自定义的学习器，一个自定义的模型数据类，和一个自定义的模型类。所以一个模型数据类，同样它不继承任何东西，所以你真的可以看到几乎没有什么要做的。你需要告诉它最重要的是你的训练集是什么（给它一个数据加载器），你的验证集是什么（给它一个数据加载器），还可以选择地，给它一个测试集（数据加载器），再加上其他需要知道的任何东西。它可能需要知道bptt，它需要知道标记的数量（即词汇表大小），它需要知道填充索引是什么。这样它就可以保存临时文件和模型，模型数据一直需要知道路径。所以我们只是获取所有这些东西然后把它们倒出来。就是这样。这就是整个初始化器。那里根本没有逻辑。
- en: 'Then all of the work happens inside `get_model`[[1:10:55](https://youtu.be/h5Tz7gZT9Fo?t=1h10m55s)].
    get_model calls something we will look at later, which just grabs a normal PyTorch
    nn.Module architecture, and chucks it on GPU. Note: with PyTorch, we would say
    `.cuda()`, with fastai it’s better to say `to_gpu()`, the reason is that if you
    don’t have GPU, it will leave it on the CPU. It also provides a global variable
    you can set to choose whether it goes on the GPU or not, so it’s a better approach.
    We wrapped the model in a `LanguageModel` and the `LanguageModel` is a subclass
    of `BasicModel` which almost does nothing except it defines layer groups. Remember
    when we do discriminative learning rates where different layers have different
    learning rates or we freeze different amounts, we don’t provide a different learning
    rate for every layer because there can be a thousand layers. We provide a different
    learning rate for every layer group. So when you create a custom model, you just
    have to override this one thing which returns a list of all of your layer groups.
    In this case, the last layer group contains the last part of the model and one
    bit of dropout. The rest of it (`*` here means pull this apart) so this is going
    to be one layer per RNN layer. So that’s all that is.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后所有的工作都发生在`get_model`内部。get_model调用我们稍后将看到的东西，它只是获取一个普通的PyTorch nn.Module架构，并将其放在GPU上。注意：在PyTorch中，我们会说`.cuda()`，在fastai中最好说`to_gpu()`，原因是如果你没有GPU，它会留在CPU上。它还提供了一个全局变量，你可以设置选择是否将其放在GPU上，所以这是一个更好的方法。我们将模型包装在`LanguageModel`中，而`LanguageModel`是`BasicModel`的子类，除了定义层组之外几乎什么都不做。记住当我们进行区分性学习率时，不同的层有不同的学习率或者我们冻结不同的数量时，我们不会为每一层提供不同的学习率，因为可能有一千层。我们为每个层组提供不同的学习率。所以当你创建一个自定义模型时，你只需要覆盖这一点，它返回所有层组的列表。在这种情况下，最后一个层组包含模型的最后部分和一个dropout位。其余部分（`*`这里表示拆分）所以这将是每个RNN层一个层。这就是全部。
- en: Then finally turn that into a learner [[1:12:41](https://youtu.be/h5Tz7gZT9Fo?t=1h12m41s)].
    So a learner, you just pass in the model and it turns it into a learner. In this
    case, we have overridden learner and the only thing we’ve done is to say I want
    the default loss function to be cross entropy. This entire set of custom model,
    custom model data, custom learner all fits on a single screen. They always basically
    look like this.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后将其转换为一个learner。所以一个learner，你只需传入模型，它就会变成一个learner。在这种情况下，我们已经重写了learner，唯一做的事情就是说我希望默认的损失函数是交叉熵。这整套自定义模型、自定义模型数据、自定义learner都适合在一个屏幕上。它们基本上看起来都是这样。
- en: The interesting part of this code base is `get_language_model` [[1:13:18](https://youtu.be/h5Tz7gZT9Fo?t=1h13m18s)].
    Because that gives us our AWD LSTM. It actually contains the big idea. The big,
    incredibly simple idea that everybody else here thinks it’s really obvious that
    everybody in the NLP community Jeremy spoke to thought was insane. That is, every
    model can be thought of as a backbone plus a head, and if you pre-train the backbone
    and stick on a random head, you can do fine-tuning and that’s a good idea.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码库中有趣的部分是`get_language_model`。因为这给了我们我们的AWD LSTM。实际上它包含了一个重要的想法。一个大而极其简单的想法，其他人都认为这是非常明显的，Jeremy与NLP社区中的每个人都认为这是疯狂的。也就是说，每个模型都可以被看作是一个骨干加一个头部，如果你预训练骨干并添加一个随机头部，你可以进行微调，这是一个好主意。
- en: These two bits of code, literally right next to each other, this is all there
    is inside `fastai.lm_rnn`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这两段代码，字面上紧挨在一起，这就是`fastai.lm_rnn`中的全部内容。
- en: '`get_language_model`: Creates an RNN encoder and then creates a sequential
    model that sticks on top of that — a linear decoder.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_language_model`：创建一个RNN编码器，然后创建一个顺序模型，将其放在顶部 - 一个线性解码器。'
- en: '`get_rnn_classifier`: Creates an RNN encoder, then a sequential model that
    sticks on top of that — a pooling linear classifier.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_rnn_classifier`：创建一个RNN编码器，然后创建一个顺序模型，将其放在顶部 - 一个池化线性分类器。'
- en: We’ll see what these differences are in a moment, but you get the basic idea.
    They are doing pretty much the same thing. They’ve got this head and they are
    sticking on a simple linear layer on top.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们马上会看到这些差异是什么，但你已经得到了基本的想法。它们基本上在做同样的事情。它们有这个头部，然后在顶部添加一个简单的线性层。
- en: '**Question**: There was a question earlier about whether that any of this translates
    to other languages [[1:14:52](https://youtu.be/h5Tz7gZT9Fo?t=1h14m52s)]. Yes,
    this whole thing works in any languages. Would you have to retrain your language
    model on a corpus from that language? Absolutely! So the wikitext103 pre-trained
    language model knows English. You could use it maybe as a pre-trained start for
    like French or German model, start by retraining the embedding layer from scratch
    might be helpful. Chinese, maybe not so much. But given that a language model
    can be trained from any unlabeled documents at all, you’ll never have to do that.
    Because almost every language in the world has plenty of documents — you can grab
    newspapers, web pages, parliamentary records, etc. As long as you have a few thousand
    documents showing somewhat normal usage of that language, you can create a language
    model. One of our students tried this approach for Thai and he said the first
    model he built easily beat the previous state-of-the-art Thai classifier. For
    those of you that are international fellow, this is an easy way for you to whip
    out a paper in which you either create the first ever classifier in your language
    or beat everybody else’s classifier in your language. Then you can tell them that
    you’ve been a student of deep learning for six months and piss off all the academics
    in your country.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：之前有一个问题，关于这是否适用于其他语言。是的，这整个过程适用于任何语言。你需要重新训练语言模型以适应那种语言的语料库吗？绝对需要！所以wikitext103预训练语言模型了解英语。你可以将其用作法语或德语模型的预训练起点，重新训练嵌入层可能会有所帮助。对于中文，可能效果不太好。但是鉴于语言模型可以从任何未标记的文档中训练，你永远不必这样做。因为世界上几乎每种语言都有大量文档
    - 你可以获取报纸、网页、议会记录等。只要你有几千份展示该语言正常使用的文档，你就可以创建一个语言模型。我们的一位学生尝试了这种方法来处理泰语，他说他建立的第一个模型轻松击败了以前最先进的泰语分类器。对于那些国际同学，这是一个简单的方法，让你能够撰写一篇论文，要么创建你所在语言的第一个分类器，要么击败其他人的分类器。然后你可以告诉他们，你已经学习深度学习六个月了，惹恼你所在国家的所有学者。
- en: Here is our RNN encoder [[1:16:49](https://youtu.be/h5Tz7gZT9Fo?t=1h16m49s)].
    It is a standard nn.Module. It looks like there is more going on in it than there
    actually is, but really all there is is we create an embedding layer, create an
    LSTM for each layer that’s been asked for, that’s it. Everything else in it is
    dropout. Basically all of the interesting stuff (just about) in the AWS LSTM paper
    is all of the places you can put dropout. Then the forward is basically the same
    thing. Call the embedding layer, add some dropout, go through each layer, call
    that RNN layer, append it to our list of outputs, add dropout, that’s about it.
    So it’s pretty straight forward.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的RNN编码器。它是一个标准的nn.Module。看起来似乎有更多的东西在里面，但实际上我们只是创建一个嵌入层，为每个被要求的层创建一个LSTM，就是这样。它里面的其他所有东西都是dropout。基本上，AWS
    LSTM论文中所有有趣的东西（几乎都是）都是你可以放置dropout的地方。然后前向传播基本上是相同的。调用嵌入层，添加一些dropout，通过每一层，调用那个RNN层，将其附加到我们的输出列表中，添加dropout，就这样。所以这很简单。
- en: The paper you want to be reading is the AWD LSTM paper which is [Regularizing
    and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182). It’s well
    written, pretty accessible, and entirely implemented inside fastai as well — so
    you can see all of the code for that paper. A lot of the code actually is shamelessly
    plagiarized with Stephen’s permission from his excellent GitHub repo [AWD LSTM](https://github.com/Smerity/awd-lstm-lm).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该阅读的论文是AWD LSTM论文，标题是[Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)。它写得很好，非常易懂，并且完全在fastai中实现
    - 所以你可以看到那篇论文的所有代码。实际上，很多代码都是在得到Stephen的许可后无耻地剽窃自他优秀的GitHub仓库AWD LSTM。
- en: The paper refers to other papers. For things like why is it that the encoder
    weight and the decoder weight are the same. It’s because there is this thing called
    “tie weights.” Inside `get_language_model`, there is a thing called `tie_weights`
    which defaults to true. If it’s true, then we literally use the same weight matrix
    for the encoder and the decoder. They are pointing at the same block of memory.
    Why is that? What’s the result of it? That’s one of the citations in Stephen’s
    paper which is also a well written paper you can look up and learn about weight
    tying.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文提到了其他论文。比如为什么编码器权重和解码器权重是相同的。这是因为有一种叫做“绑定权重”的东西。在`get_language_model`中，有一个叫做`tie_weights`的东西，默认为true。如果为true，那么我们实际上使用相同的权重矩阵用于编码器和解码器。它们指向同一块内存。为什么会这样？结果是什么？这是Stephen的论文中的一个引用，也是一篇写得很好的论文，你可以查阅并了解权重绑定。
- en: We have basically a standard RNN [[1:19:52](https://youtu.be/h5Tz7gZT9Fo?t=1h19m52s)].
    The only reason where it’s not standard is it has lots more types of dropout in
    it. In a sequential model on top of the RNN, we stick a linear decoder which is
    literally half the screen of code. It has a single linear layer, we initialize
    the weights to some range, we add some dropout, and that’s it. So it’s a linear
    layer with dropout.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上有一个标准的RNN。唯一不标准的地方是它有更多类型的dropout。在RNN的顶部顺序模型中，我们放置一个线性解码器，这实际上是代码屏幕的一半。它有一个单一的线性层，我们将权重初始化为某个范围，添加一些dropout，就这样。所以它是一个带有dropout的线性层。
- en: 'So the language model is:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所以语言模型是：
- en: RNN → A linear layer with dropout
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN → 一个带有dropout的线性层
- en: Choosing dropout [[1:20:36](https://youtu.be/h5Tz7gZT9Fo?t=1h20m36s)]
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择dropout
- en: What dropout you choose matters a lot .Through a lot of experimentation, Jeremy
    found a bunch of dropouts that tend to work pretty well for language models. But
    if you have less data for your language model, you’ll need more dropout. If you
    have more data, you can benefit from less dropout. You don’t want to regularize
    more than you have to. Rather than having to tune every one of these five things,
    Jeremy’s claim is they are already pretty good ratios to each other, so just tune
    this number (`0.7` below), we just multiply it all by something. If you are overfitting,
    then you’ll need to increase the number, if you are underfitting, you’ll need
    to decrease this. Because other than that, these ratio seem pretty good.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的dropout很重要。通过大量实验，Jeremy发现了一些对语言模型非常有效的dropout。但如果你的语言模型数据较少，你需要更多的dropout。如果你有更多的数据，你可以从更少的dropout中受益。你不想过度正则化。Jeremy的观点是，这些比例已经相当不错，所以只需调整这个数字（下面的`0.7`），我们只需将其乘以某个值。如果你过拟合，那么你需要增加这个数字，如果你欠拟合，你需要减少这个数字。因为除此之外，这些比例似乎相当不错。
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Measuring accuracy [[1:21:45](https://youtu.be/h5Tz7gZT9Fo?t=1h21m45s)]
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量准确性
- en: One important idea which may seem minor but again it’s incredibly controversial
    is that we should measure accuracy when we look at a language model . Normally
    for language models, we look at a loss value which is just cross entropy loss
    but specifically we nearly always take e to the power of that which the NLP community
    calls “perplexity”. So perplexity is just `e^(cross entropy)`. There is a lot
    of problems with comparing things based on cross entropy loss. Not sure if there’s
    time to go into it in detail now, but the basic problem is that it is like that
    thing we learned about focal loss. Cross entropy loss — if you are right, it wants
    you to be really confident that you are right. So it really penalizes a model
    that doesn’t say “I’m so sure this is wrong” and it’s wrong. Whereas accuracy
    doesn’t care at all about how confident you are — it cares about whether you are
    right. This is much more often the thing which you care about in real life. The
    accuracy is how often do we guess the next word correctly and it’s a much more
    stable number to keep track of. So that’s a simple little thing that Jeremy does.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个看似不起眼但实际上非常有争议的重要观点是，当我们看语言模型时，我们应该衡量准确性。通常对于语言模型，我们看的是一个损失值，即交叉熵损失，但具体来说，我们几乎总是取e的幂次方，NLP社区称之为“困惑度”。所以困惑度就是`e^(交叉熵)`。基于交叉熵损失进行比较存在很多问题。不确定现在是否有时间详细讨论，但基本问题就像我们学到的关于焦点损失的那个东西。交叉熵损失
    - 如果你是对的，它希望你非常确信自己是对的。因此，它会严厉惩罚那些不说“我非常确定这是错误”的模型，而实际上是错误的。而准确性完全不关心你有多自信 - 它关心的是你是否正确。这在现实生活中更常见。准确性是我们猜测下一个词正确的频率，这是一个更稳定的数字来跟踪。这是Jeremy做的一个简单小事。
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We train for a while and we get down to a 3.9 cross entropy loss which is equivalent
    of ~49.40 perplexity (`e^3.9`) [[1:23:14](https://youtu.be/h5Tz7gZT9Fo?t=1h23m14s)].
    To give you a sense of what’s happening with language models, if you look at academic
    papers from about 18 months ago, you’ll see them talking about state-of-the-art
    perplexity of over a hundred. The rate at which our ability to understand language
    and measuring language model accuracy or perplexity is not a terrible proxy for
    understanding language. If I can guess what you are going to say next, I need
    to understand language well and the kind of things you might talk about pretty
    well. The perplexity number has just come down so much that it’s been amazing,
    and it will come down a lot more. NLP in the last 12–18 months, it really feels
    like 2011–2012 computer vision. We are starting to understand transfer learning
    and fine-tuning, and basic models are getting so much better. Everything you thought
    about what NLP can and can’t do is rapidly going out of date. There’s still lots
    of things NLP is not good at to be clear. Just like in 2012, there were lots of
    stuff computer vision wasn’t good at. But it’s changing incredibly rapidly and
    now is a very very good time to be getting very good at NLP or starting startups
    base on NLP because there is a whole bunch of stuff which computers would absolutely
    terrible at two years ago and now not quite good as people and then next year,
    they’ll be much better than people.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练一段时间，将交叉熵损失降至3.9，相当于约49.40的困惑度（`e^3.9`）。要让你了解语言模型的情况，如果你看一下大约18个月前的学术论文，你会看到他们谈论的最先进的困惑度超过一百。我们理解语言的能力以及衡量语言模型准确性或困惑度的速度并不是理解语言的一个可怕的代理。如果我能猜到你接下来要说什么，我需要很好地理解语言以及你可能会谈论的事情。困惑度数字已经下降了很多，这是令人惊讶的，而且它还会下降很多。在过去的12-18个月里，NLP真的感觉像是2011-2012年的计算机视觉。我们开始理解迁移学习和微调，基本模型变得更好了很多。你对NLP能做什么和不能做什么的想法正在迅速过时。当然，NLP仍然有很多不擅长的地方。就像在2012年，计算机视觉有很多不擅长的地方一样。但它的变化速度非常快，现在是非常好的时机，要么变得非常擅长NLP，要么基于NLP创办初创公司，因为两年前计算机绝对擅长的一堆事情，现在还不如人类，而明年，它们将比人类好得多。
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Question**: What is your ratio of paper reading vs. coding in a week [[1:25:24](https://youtu.be/h5Tz7gZT9Fo?t=1h25m24s)]?
    Gosh, what do you think, Rachel? You see me. I mean, it’s more coding, right?
    “It’s a lot more coding. I feel like it also really varies from week to week”
    (Rachel). With that bounding box stuff, there were all these papers and no map
    through them, so I didn’t even know which one to read first and then I’d read
    the citations and didn’t understand any of them. So there was a few weeks of just
    kind of reading papers before I even know what to start coding. That’s unusual
    though. Anytime I start reading a paper, I’m always convinced that I’m not smart
    enough to understand it, always, regardless of the paper. And somehow eventually
    I do. But I try to spend as much time as I can coding.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您一周内阅读论文与编码的比例是多少？天哪，你觉得呢，Rachel？你看到我。我的意思是，更多的是编码，对吧？“编码要多得多。我觉得每周也会有很大的变化”（Rachel）。有了那些边界框的东西，有很多论文，但没有明确的指引，所以我甚至不知道该先读哪一篇，然后我读了引用，但一个也不懂。所以有几周时间只是读论文，然后才知道从哪里开始编码。不过这种情况很少见。每次我开始读一篇论文，我总是确信自己不够聪明，无法理解，无论是哪篇论文。但最终我总能理解。但我尽量花尽可能多的时间编码。'
- en: Nearly always after I’ve read a paper [[1:26:34](https://youtu.be/h5Tz7gZT9Fo?t=1h26m34s)],
    even after I’ve read the bit that says this is the problem I’m trying to solve,
    I’ll stop there and try to implement something that I think might solve that problem.
    And then I’ll go back and read the paper, and I read little bits about these are
    how I solve these problem bits, and I’ll be like “oh that’s a good idea” and then
    I’ll try to implement those. That’s why for example, I didn’t actually implement
    SSD. My custom head is not the same as their head. It’s because I kind of read
    the gist of it and then I tried to create something as best as I could, then go
    back to the papers and try to see why. So by the time I got to the focal loss
    paper, Rachel will tell you, I was driving myself crazy with how come I can’t
    find small objects? How come it’s always predicting background? I read the focal
    loss paper and I was like “that’s why!!” It’s so much better when you deeply understand
    the problem they are trying to solve. I do find the vast majority of the time,
    by the time I read that bit of the paper which is solving a problem, I’m then
    like “yeah, but these three ideas I came up with, they didn’t try.” Then you suddenly
    realize that you’ve got new ideas. Or else, if you just implement the paper mindlessly,
    you tend not to have these insights about better ways to do it .
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎总是在我读完一篇论文后，即使我读完了说这是我要解决的问题的部分，我会停下来，尝试实现我认为可能解决这个问题的东西。然后我会回去读论文，读一点关于如何解决这些问题的部分，然后我会说“哦，这是个好主意”，然后尝试实现这些。这就是为什么例如，我实际上没有实现SSD。我的自定义头部与他们的头部不同。因为我大致了解了它，然后尝试尽力创建一些东西，然后回到论文中看原因。所以当我看到焦点损失论文时，Rachel会告诉你，我因为为什么找不到小物体而把自己逼疯了？为什么总是预测背景？我读了焦点损失论文，然后我说“原来如此！”当你深刻理解他们试图解决的问题时，情况会好得多。我发现绝大多数时间，当我读到解决问题的部分时，我会说“是的，但是我想出的这三个想法他们没有尝试。”然后你突然意识到你有了新的想法。否则，如果你只是机械地实现论文，你往往不会有关于更好方法的这些见解。
- en: '**Question**: Is your dropout rate the same through the training or do you
    adjust it and weights accordingly [[1:26:27](https://youtu.be/h5Tz7gZT9Fo?t=1h26m27s)]?
    Varying dropout is really interesting and there are some recent papers that suggest
    gradually changing dropout [[1:28:09](https://youtu.be/h5Tz7gZT9Fo?t=1h28m9s)].
    It was either good idea to gradually make it smaller or gradually make it bigger,
    I’m not sure which. Maybe one of us can try and find it during the week. I haven’t
    seen it widely used. I tried it a little bit with the most recent paper I wrote
    and I had some good results. I think I was gradually make it smaller, but I can’t
    remember.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您的辍学率在培训过程中是否保持不变，或者您是否相应地调整它和权重？变化的辍学率真的很有趣，最近有一些论文建议逐渐改变辍学率。逐渐减小或逐渐增加它可能是个好主意，我不确定哪个更好。也许我们中的某个人可以在这一周尝试找到它。我还没有看到它被广泛使用。我在最近写的论文中尝试了一点，取得了一些好结果。我想我是逐渐减小它的，但我记不清了。'
- en: '**Question**: Am I correct in thinking that this language model is build on
    word embeddings? Would it be valuable to try this with phrase or sentence embeddings?
    I ask this because I saw from Google the other day, universal sentence encoder
    [[1:28:45](https://youtu.be/h5Tz7gZT9Fo?t=1h28m45s)]. This is much better than
    that. This is not just an embedding of a sentence, this is an entire model. An
    embedding by definition is like a fixed thing. A sentence or a phase embedding
    is always a model that creates that. We’ve got a model that’s trying to understand
    language. It’s not just as phrase or as sentence — it’s a document in the end,
    and it’s not just an embedding that we are training through the whole thing. This
    has been a huge problem with NLP for years now is this attachment they have to
    embeddings. Even the paper that the community has been most excited about recently
    from [AI2](http://allenai.org/) (Allen Institute for Artificial Intelligence)
    called ELMo — they found much better results across lots of models, but again
    it was an embedding. They took a fixed model and created a fixed set of numbers
    which they then fed into a model. But in computer vision, we’ve known for years
    that that approach of having fixed set of features, they’re called hyper columns
    in computer vision, people stopped using them like 3 or 4 years ago because fine-tuning
    the entire model works much better. For those of you that have spent quite a lot
    of time with NLP and not much time with computer vision, you’re going to have
    to start re-learning. All that stuff you have been told about this idea that there
    are these things called embeddings and that you learn them ahead of time and then
    you apply these fixed things whether it be word level or phrase level or whatever
    level — don’t do that. You want to actually create a pre-trained model and fine-tune
    it end-to-end, then you’ll see some specific results.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我是否正确地认为这个语言模型是建立在词嵌入上的？尝试使用短语或句子嵌入是否有价值？我问这个问题是因为我前几天从谷歌那里看到了通用句子编码器。这比那个好多了。这不仅仅是一个句子的嵌入，这是一个完整的模型。嵌入的定义就像一个固定的东西。一个句子或短语嵌入总是创建一个模型。我们有一个试图理解语言的模型。这不仅仅是一个短语或句子——最终是一个文档，而且我们不仅仅是通过整个过程训练嵌入。多年来，这一直是自然语言处理的一个巨大问题，就是他们对嵌入的依赖。即使是最近最令人兴奋的来自AI2（艾伦人工智能研究所）的论文，他们发现在许多模型中取得了更好的结果，但再次，这是一个嵌入。他们采用了一个固定的模型，并创建了一组固定的数字，然后将其输入到模型中。但是在计算机视觉中，多年来我们已经知道，拥有固定的特征集的方法，称为计算机视觉中的超列，人们在3或4年前就停止使用了，因为对整个模型进行微调效果更好。对于那些在自然语言处理方面花费了很多时间而在计算机视觉方面花费不多时间的人，你们将不得不开始重新学习。关于这个想法，你们被告知的所有关于这些叫做嵌入的东西以及你提前学习它们然后应用这些固定的东西，无论是单词级别还是短语级别或其他级别的所有东西——不要这样做。你实际上想要创建一个预训练模型并对其进行端到端的微调，然后你会看到一些具体的结果。'
- en: '**Question**: For using accuracy instead of perplexity as a metric for the
    model, could we work that into the loss function rather than just use it as a
    metric [[1:31:21](https://youtu.be/h5Tz7gZT9Fo?t=1h31m21s)]? No, you never want
    to do that whether it be computer vision or NLP or whatever. It’s too bumpy. So
    cross entropy is fine as a loss function. And I’m not saying instead of, I use
    it in addition to. I think it’s good to look at the accuracy and to look at the
    cross entropy. But for your loss function, you need something nice and smoothy.
    Accuracy doesn’t work very well.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于使用准确度而不是困惑度作为模型的度量标准，我们是否可以将其纳入损失函数而不仅仅将其用作度量标准？不，无论是计算机视觉还是自然语言处理或其他任何领域，你都不想这样做。这太颠簸了。所以交叉熵作为损失函数是可以的。我并不是说取代，我是同时使用。我认为查看准确度和查看交叉熵是好的。但对于你的损失函数，你需要一些平滑的东西。准确度效果不是很好。'
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`save_encoder` [[1:31:55](https://youtu.be/h5Tz7gZT9Fo?t=1h31m55s)]'
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`save_encoder`'
- en: 'You’ll see there are two different versions of `save`. `save` saves the whole
    model as per usual. `save_encoder` just saves that bit:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到有两个不同版本的`save`。`save`像往常一样保存整个模型。`save_encoder`只保存那一部分。
- en: In other words, in the sequential model, it saves just `rnn_enc` and not `LinearDecoder(n_tok,
    emb_sz, dropout, tie_encoder=enc)` (which is the bit that actually makes it into
    a language model). We don’t care about that bit in the classifier, we just care
    about `rnn_end`. That’s why we save two different models here.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在顺序模型中，它只保存`rnn_enc`而不保存`LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc)`（这实际上是将其转换为语言模型的部分）。在分类器中，我们不关心那部分，我们只关心`rnn_end`。这就是为什么我们在这里保存两个不同的模型。
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Classifier tokens [[1:32:31](https://youtu.be/h5Tz7gZT9Fo?t=1h32m31s)]
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器标记
- en: Let’s now create the classifier. We will go through this pretty quickly because
    it’s the same. But when you go back during the week and look at the code, convince
    yourself it’s the same.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建分类器。我们将快速浏览一下，因为它是相同的。但当你在这一周回顾代码时，确信它是相同的。
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We don’t create a new `itos` vocabulary, we obviously want to use the same vocabulary
    we had in the language model because we are about to reload the same encoder [[1:32:48](https://youtu.be/h5Tz7gZT9Fo?t=1h32m48s)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不创建一个新的`itos`词汇表，显然我们想要使用语言模型中已有的相同词汇表，因为我们即将重新加载相同的编码器。
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Classifier
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The construction of the model hyper parameters are the same [[1:33:16](https://youtu.be/h5Tz7gZT9Fo?t=1h33m16s)].
    We can change the dropout. Pick a batch size that is as big as you can that doesn’t
    run out of memory.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 模型超参数的构建是相同的。我们可以更改丢失率。选择一个尽可能大的批量大小，以防内存不足。
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: TextDataset [[1:33:37](https://youtu.be/h5Tz7gZT9Fo?t=1h33m37s)]
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TextDataset
- en: This bit is interesting. There’s fun stuff going on here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分很有趣。这里有一些有趣的东西。
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The basic idea here is that for the classifier, we do really want to look at
    one document. Is this document positive or negative? So we do want to shuffle
    the documents. But those documents have different lengths and so if we stick them
    all into one batch (this is a handy thing that fastai does for you) — you can
    stick things of different lengths into a batch and it will automatically pat them,
    so you don’t have to worry about that. But if they are wildly different lengths,
    then you’re going to be wasting a lot of computation times. If there is one thing
    that’s 2,000 words long and everything else is 50 words long, that means you end
    up with 2000 wide tensor. That’s pretty annoying. So James Bradbury who is one
    of Stephen Merity’s colleagues and the guy who came up with torchtext came up
    with a neat idea which was “let’s sort the dataset by length-ish”. So kind of
    make it so the first things in the list are, on the whole, shorter than the things
    at the end, but a little bit random as well.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本思想是，对于分类器，我们确实希望查看一个文档。这个文档是积极的还是消极的？所以我们确实希望打乱文档。但是这些文档的长度不同，所以如果我们把它们全部放入一个批次中（这是fastai为您做的一个方便的事情）-
    您可以将不同长度的东西放入一个批次中，它会自动填充它们，所以您不必担心这个问题。但是如果它们的长度差异很大，那么您将浪费大量的计算时间。如果有一件事是2000个字长，而其他所有东西都是50个字长，那意味着您最终会得到一个2000宽的张量。这相当恼人。所以詹姆斯·布拉德伯里是斯蒂芬·梅里蒂的同事之一，也是提出torchtext的人，他提出了一个聪明的想法，即“让我们按长度对数据集进行排序”。因此，使得列表中的前几个东西总体上比最后的东西短，但也有一点随机性。
- en: 'Here is how Jeremy implemented that [[1:35:10](https://youtu.be/h5Tz7gZT9Fo?t=1h35m10s)].
    The first thing we need is a Dataset. So we have a Dataset passing in the documents
    and their labels. Here is `TextDataSet` which inherits from `Dataset` and `Dataset`
    from PyTorch is also shown below:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Jeremy如何实现的。我们需要的第一件事是一个数据集。因此，我们有一个传递文档及其标签的数据集。这里有一个继承自Dataset的TextDataSet，下面还显示了PyTorch中的Dataset：
- en: Actually `Dataset` doesn’t do anything at all [[1:35:34](https://youtu.be/h5Tz7gZT9Fo?t=1h35m34s)].
    It says you need `__getitem__` if you don’t have one, you’re going to get an error.
    Same is true for `__len__`. So this is an abstract class. To `TextDataset`, we
    are going to pass in our `x` and `y`, and `__getitem__` will grab `x` and `y`,
    and return them — it couldn’t be much simpler. Optionally, 1\. they could reverse
    it, 2\. stick an end of stream at the end, 3\. stick start of stream at the beginning.
    But we are not doing any of those things, so literally all we are doing is putting
    `x` and `y` and `__getitem__` returns them as a tuple. The length is however long
    the `x` is. That’s all `Dataset` is — something with a length that you can index.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Dataset什么也不做。它说如果您没有`__getitem__`，您将会收到一个错误。对于`__len__`也是如此。因此，这是一个抽象类。对于TextDataset，我们将传入我们的x和y，`__getitem__`将获取x和y，并将它们返回-这不能更简单。可选地，1.他们可以颠倒它，2.在末尾添加一个流的结束，3.在开头添加一个流的开始。但我们没有做这些事情，所以我们实际上所做的就是将x和y放在一起，`__getitem__`将它们作为元组返回。长度是x的长度。这就是Dataset的全部内容-一个具有长度的东西，您可以对其进行索引。
- en: Turning it to a DataLoader [[1:36:27](https://youtu.be/h5Tz7gZT9Fo?t=1h36m27s)]
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将其转换为DataLoader
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To turn it into a DataLoader, you simply pass the Dataset to the DataLoader
    constructor, and it’s now going to give you a batch of that at a time. Normally
    you can say shuffle equals true or shuffle equals false, it’ll decide whether
    to randomize it for you. In this case though, we are actually going to pass in
    a sampler parameter and sampler is a class we are going to define that tells the
    data loader how to shuffle.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转换为DataLoader，您只需将数据集传递给DataLoader构造函数，现在它将每次给您一个批次。通常您可以说shuffle等于true或shuffle等于false，它会决定是否为您随机化。但在这种情况下，我们实际上将传递一个sampler参数，sampler是一个我们将定义的类，告诉数据加载器如何进行洗牌。
- en: For validation set, we are going to define something that actually just sorts.
    It just deterministically sorts it so that all the shortest documents will be
    at the start, all the longest documents will be at the end, and that’s going to
    minimize the amount of padding.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于验证集，我们将定义一个实际上只是排序的东西。它只是确定性地对其进行排序，以便所有最短的文档将在开头，所有最长的文档将在末尾，这将最小化填充的数量。
- en: For training sampler, we are going to create this thing called sort-ish sampler
    which also sorts (ish!)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练采样器，我们将创建一个称为sort-ish采样器的东西，它也进行排序（ish！）
- en: What’s great about PyTorch is that they came up with this idea for an API for
    their data loader where we can hook in new classes to make it behave in different
    ways [[1:37:27](https://youtu.be/h5Tz7gZT9Fo?t=1h37m27s)]. SortSampler is something
    which has a length which is the length of the data source and has an iterator
    which is simply an iterator which goes through the data source sorted by length
    (which is passed in as `key`). For the SortishSampler, it basically does the same
    thing with a little bit of randomness. It’s just another of those beautiful design
    things in PyTorch that Jeremy discovered. He could take James Bradbury’s ideas
    which he had written a whole new set of classes around, and he could just use
    inbuilt hooks inside PyTorch. You will notice data loader is not actually PyTorch’s
    data loader — it’s actually fastai’s data loader. But it’s basically almost entirely
    plagiarized from PyTorch but customized in some ways to make it faster mainly
    using multi-threading instead of multi-processing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的一个伟大之处在于，他们为数据加载器提出了一个API的想法，我们可以通过其中的新类来使其以不同的方式运行。SortSampler是一个具有数据源长度的长度和一个迭代器的东西，迭代器只是一个按长度排序的数据源的迭代器（作为key传入）。对于SortishSampler，它基本上做了同样的事情，稍微有些随机性。这只是PyTorch中Jeremy发现的另一个美丽设计。他可以采用詹姆斯·布拉德伯里的想法，他围绕这个想法写了一整套新的类，并且可以在PyTorch内部使用内置的钩子。您会注意到数据加载器实际上不是PyTorch的数据加载器-它实际上是fastai的数据加载器。但它基本上几乎完全抄袭了PyTorch，但在某些方面进行了定制，以使其更快，主要是使用多线程而不是多处理。
- en: '**Question**: Does the pre-trained LSTM depth and `bptt` need to match with
    the new one we are training [[1:39:00](https://youtu.be/h5Tz7gZT9Fo?t=1h39m)]?
    No, the `bptt` doesn’t need to match at all. That’s just like how many things
    we look at at a time. It has nothing to do with the architecture.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：预训练的LSTM深度和`bptt`需要与我们正在训练的新模型匹配吗[[1:39:00](https://youtu.be/h5Tz7gZT9Fo?t=1h39m)]？不，`bptt`根本不需要匹配。这就像我们一次看多少东西。这与架构无关。'
- en: 'So now we can call that function we just saw before `get_rnn_classifier` [[1:39:16](https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s)].
    It’s going to create exactly the same encoder more or less, and we are going to
    pass in the same architectural details as before. But this time, with the head
    we add on, you have a few more things you can do. One is you can add more than
    one hidden layer. In `layers=[em_sz*3, 50, c]`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用我们之前看到的`get_rnn_classifier`函数[[1:39:16](https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s)]。它将创建几乎完全相同的编码器，我们将传入与之前相同的架构细节。但这次，我们添加的头部有一些额外的功能。其中一个是你可以添加多个隐藏层。在`layers=[em_sz*3,
    50, c]`中：
- en: '`em_sz * 3`: this is what the input to my head (i.e. classifier section) is
    going to be.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`em_sz * 3`：这是我头部（即分类器部分）的输入。'
- en: '`50`: this is the output of the first layer'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`50`：这是第一层的输出'
- en: '`c`: this is the output of the second layer'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`：这是第二层的输出'
- en: And you can add as many as you like. So you can basically create a little multi-layer
    neural net classifier at the end. Similarly, for `drops=[dps[4], 0.1]`, these
    are the dropouts to go after each of these layers.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以添加任意数量的层。所以你基本上可以在最后创建一个小型多层神经网络分类器。同样，对于`drops=[dps[4], 0.1]`，这些是在每个层之后要进行的丢弃。
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We are going to use RNN_Learner just like before.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像以前一样使用RNN_Learner。
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We are going to use discriminative learning rates for different layers [[1:40:20](https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为不同层使用判别学习率[[1:40:20](https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s)]。
- en: '[PRE45]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You can try using weight decay or not. Jeremy has been fiddling around a bit
    with that to see what happens.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试使用权重衰减或不使用。Jeremy已经在尝试一些东西。
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We start out just training the last layer and we get 92.9% accuracy:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始只训练最后一层，得到92.9%的准确率：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then we unfreeze one more layer, get 93.3% accuracy:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们再解冻一层，得到93.3%的准确率：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then we fine-tune the whole thing [[1:40:47](https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s)].
    This was the main attempt before our paper came along at using a pre-trained model:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们微调整个模型[[1:40:47](https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s)]。这是在我们的论文出现之前使用预训练模型的主要尝试：
- en: '[Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[在翻译中学到：上下文化的词向量](https://arxiv.org/abs/1708.00107)'
- en: What they did is they used a pre-trained translation model but they didn’t fine
    tune the whole thing. They just took the activations of the translation model
    and when they tried IMDb, they got 91.8% — which we beat easily after only fine-tuning
    one layer. They weren’t state-of-the-art, the state-of-the-art is 94.1% which
    we beat after fine-tuning the whole thing for 3 epochs and by the end, we are
    at 94.8% which is obviously a huge difference because in terms of error rate,
    that’s gone done from 5.9%. A simple little trick is go back to the start of this
    notebook and reverse the order of all of the documents, and then re-run the whole
    thing. When you get to the bit that says `fwd_wt_103`, replace `fwd` for forward
    with `bwd` for backward. That’s a backward English language model that learns
    to read English backward. So if you redo this whole thing, put all the documents
    in reverse, and change this to backward, you now have a second classifier which
    classifies things by positive or negative sentiment based on the reverse document.
    If you then take the two predictions and take the average of them, you basically
    have a bi-directional model (which you trained each bit separately)and that gets
    you to 95.4% accuracy. So we basically lowered it from 5.9% to 4.6%. So this kind
    of 20% change in the state-of-the-art is almost unheard of. It doesn’t happen
    very often. So you can see this idea of using transfer learning, it’s ridiculously
    powerful that every new field thinks their new field is too special and you can’t
    do it. So it’s a big opportunity for all of us.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 他们所做的是他们使用了一个预训练的翻译模型，但他们没有微调整个模型。他们只是取出了翻译模型的激活，当他们尝试IMDb时，他们得到了91.8% —— 而我们只是微调了一个层就轻松超过了这个结果。他们不是最先进的，最先进的是94.1%，而我们在微调整个模型3个epochs后达到了94.8%，这显然是一个巨大的差异，因为从错误率来看，这已经从5.9%下降到了4.6%。一个简单的小技巧是回到这个笔记本的开头，颠倒所有文档的顺序，然后重新运行整个过程。当你到达`fwd_wt_103`这部分时，用`bwd`替换`fwd`，即将forward替换为backward。这是一个向后的英语语言模型，学习如何向后阅读英语。因此，如果你重新做这整个过程，将所有文档倒置，并将其更改为向后，你现在有了第二个分类器，根据反向文档的情感将事物分类为正面或负面。然后，你可以取这两个预测的平均值，基本上你就有了一个双向模型（你分别训练了每个部分），这将使你达到95.4%的准确率。所以我们基本上将它从5.9%降低到了4.6%。这种最先进技术的20%变化几乎是闻所未闻的。这种情况并不经常发生。所以你可以看到使用迁移学习的这个想法，它是非常强大的，每个新领域都认为自己的领域太特殊，无法做到。所以这对我们所有人来说都是一个巨大的机会。
- en: Universal Language Model Fine-tuning for Text Classification [[1:44:02](https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s)]
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类的通用语言模型微调[[1:44:02](https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s)]
- en: 'So we turned this into a paper, and when I say we, I did it with this guy Sebastian
    Ruder. Now you might remember his name because in lesson 5, I told you that I
    actually had shared lesson 4 with Sebastian because I think he is an awesome researcher
    who I thought might like it. I didn’t know him personally at all. Much to my surprise,
    he actually watched the video. He watched the whole video and said:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们把这个变成了一篇论文，当我说“我们”时，是和这个家伙Sebastian Ruder一起做的。现在你可能记得他的名字，因为在第5课时，我告诉过你我实际上和Sebastian分享了第4课，因为我认为他是一个很棒的研究者，我认为他可能会喜欢。我根本不认识他。令我惊讶的是，他实际上看了这个视频。他看了整个视频并说：
- en: 'Sebastian: “That’s actually quite fantastic! We should turn this into a paper.”'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian：“这实际上相当棒！我们应该把这变成一篇论文。”
- en: 'Jeremy: “I don’t write papers. I don’t care about papers and am not interested
    in papers — that sounds really boring”'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：“我不写论文。我不关心论文，对论文不感兴趣——那听起来真的很无聊”
- en: 'Sebastian: “Okay, how about I write the paper for you.”'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian：“好的，我替你写论文。”
- en: 'Jeremy: “You can’t really write a paper about this yet because you’d have to
    do like studies to compare it to other things (they are called ablation studies)
    to see which bit actually works. There’s no rigor here, I just put in everything
    that came in my head and chucked it all together and it happened to work”'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：“你现在真的不能写关于这个的论文，因为你必须做研究来将其与其他事物进行比较（称为消融研究），看看哪部分实际起作用。这里没有严谨性，我只是把我脑子里想到的一切都放进去，然后把它们全部放在一起，结果竟然奏效了”
- en: 'Sebastian: “Okay, what if I write all the paper and do all your ablation studies,
    then can we write the paper?”'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian：“好的，如果我写所有的论文并做所有你的消融研究，那我们可以写论文吗？”
- en: 'Jeremy: “Well, it’s like a whole library that I haven’t documented and I’m
    not going to yet and you don’t know how it all works”'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：“嗯，这就像一个我还没有记录的整个库，我现在还不会记录，你也不知道它是如何工作的”
- en: 'Sebastian: “Okay, if I wrote the paper, and do the ablation studies, and figure
    out from scratch how the code works without bothering you, then can we write the
    paper?”'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian：“好的，如果我写了论文，做了消融研究，从头开始弄清楚代码的工作原理而不打扰你，那我们可以写论文吗？”
- en: 'Jeremy: “Um… yeah, if you did all those things, then we can write the paper.
    Okay!”'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：“嗯……是的，如果你做了所有这些事情，那我们可以写论文。好吧！”
- en: Then two days later, he comes back and says “okay, I’ve done a draft of the
    paper.” So, I share this story to say, if you are some student in Ireland and
    you want to do good work, don’t let anybody stop you. I did not encourage him
    to say the least. But in the end, he said “I want to do this work, I think it’s
    going to be good, and I’ll figure it out” and he wrote a fantastic paper. He did
    the ablation study and he figured out how fastai works, and now we are planning
    to write another paper together. You’ve got to be a bit careful because sometimes
    I get messages from random people saying like “I’ve got lots of good ideas, can
    we have coffee?” — “I don’t want… I can have coffee in my office anytime, thank
    you”. But it’s very different to say “hey, I took your ideas and I wrote a paper,
    and I did a bunch of experiments, and I figured out how your code works, and I
    added documentation to it — should we submit this to a conference?” You see what
    I mean? There is nothing to stop you doing amazing work and if you do amazing
    work that helps somebody else, in this case, I’m happy that we have a paper. I
    don’t particularly care about papers but I think it’s cool that these ideas now
    have this rigorous study.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然后两天后，他回来说“好的，我已经起草了论文。” 所以，我分享这个故事是想说，如果你是爱尔兰的某个学生，想做好工作，不要让任何人阻止你。我至少没有鼓励他。但最后，他说“我想做这项工作，我认为会很好，我会弄清楚的”，他写了一篇很棒的论文。他做了消融研究，弄清楚了fastai的工作原理，现在我们计划一起写另一篇论文。你必须小心，因为有时我会收到陌生人的消息，说“我有很多好主意，我们可以喝咖啡吗？”——“我不想……我随时可以在办公室喝咖啡，谢谢”。但是，说“嘿，我采纳了你的想法，写了一篇论文，做了一堆实验，弄清楚了你的代码如何工作，并为其添加了文档——我们应该提交到会议上吗？”就很不一样了。你明白我的意思吗？没有什么能阻止你做出惊人的工作，如果你做出了有助于他人的惊人工作，比如这种情况，我很高兴我们有了一篇论文。我并不特别关心论文，但我认为这些想法现在有了这样严谨的研究很酷。
- en: Let me show you what he did [[1:47:19](https://youtu.be/h5Tz7gZT9Fo?t=1h47m19s)]
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我展示一下他做了什么。
- en: 'He took all my code, so I’d already done all the fastai.text and as you have
    seen, it lets us work with large corpuses. Sebastian is fantastically well-read
    and he said “here’s a paper that Yann LeCun and some guys just came out with where
    they tried lots of classification datasets so I’m going to try running your code
    on all these datasets.” So these are the datasets:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 他拿走了我所有的代码，所以我已经做了所有的fastai.text，正如你所看到的，它让我们能够处理大型语料库。Sebastian读书很多，他说“这里有一篇Yann
    LeCun和一些人刚刚发表的论文，他们尝试了很多分类数据集，所以我打算在所有这些数据集上运行你的代码。” 所以这些数据集是：
- en: Some of them had many many hundreds of thousands of documents and they were
    far bigger than I had tried — but I thought it should work.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些文档有成千上万的文件，比我尝试的要大得多——但我认为它应该可以工作。
- en: 'And he had a few good ideas as we went along and so you should totally make
    sure you read the paper. He said “well, this thing that you called in the lessons
    differential learning rates, differential kind of means something else. Maybe
    we should rename it” so we renamed it. It’s now called discriminative learning
    rate. So this idea that we had from part one where we use different learning rates
    for different layers, after doing some literature research, it does seem like
    that hasn’t been done before so it’s now officially a thing — discriminative learning
    rates. This is something we learnt in lesson 1 but it now has an equation with
    Greek and everything [[1:48:41](https://youtu.be/h5Tz7gZT9Fo?t=1h48m41s)]:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行下去的过程中，他有一些好主意，所以你应该确保阅读这篇论文。他说“嗯，你在课程中称之为差异学习率的东西，差异有点意思。也许我们应该重新命名它”，所以我们重新命名了。现在它被称为区分性学习率。所以我们在第一部分学到的这个想法，即对不同层使用不同的学习率，经过一些文献研究，似乎以前没有做过，所以现在正式成为一个事情——区分性学习率。这是我们在第一课中学到的东西，但现在它有了一个带有希腊字母和一切的方程式：
- en: When you see an equation with Greek and everything, that doesn’t necessarily
    mean it’s more complex than anything we did in lesson 1 because this one isn’t.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到一个带有希腊字母的方程式时，并不一定意味着它比我们在第一课中做的任何事情更复杂，因为这个并不复杂。
- en: Again, that idea of like unfreezing a layer at a time, also seems to never been
    done before so it’s now a thing and it’s got the very clever name “gradual unfreezing”
    [[1:48:57](https://youtu.be/h5Tz7gZT9Fo?t=1h48m57s)].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，像逐层解冻一样的想法，似乎以前从未做过，所以现在是一个事情，它有一个非常聪明的名字“逐步解冻”。
- en: Slanted triangular learning rate [[1:49:10](https://youtu.be/h5Tz7gZT9Fo?t=1h49m10s)]
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 倾斜三角形学习率
- en: 'So then, as promised, we will look at slanted triangular learning rates . This
    actually was not my idea. Leslie Smith, one of my favorite researchers who you
    all now know about, emailed me a while ago and said “I’m so over cyclical learning
    rates. I don’t do that anymore. I now do a slightly different version where I
    have one cycle which goes up quickly at the start, and then slowly down afterwards.
    I often find it works better.” I’ve tried going back over all of my old datasets
    and it works better for all of them — every one I tried. So this is what the learning
    rate look like. You can use it in fastai just by adding `use_clr=` to your `fit`.
    The first number is the ratio between the highest learning rate and the lowest
    learning rate so the initial learning rate is 1/32 of the peak. The second number
    is the ratio between the first peak and the last peak. The basic idea is if you
    are doing a cycle length 10, that you want the first epoch to be the upward bit
    and the other 9 epochs to be the downward bit, then you would use 10\. I find
    that works pretty well and that was also Leslie’s suggestion is make about 1/10
    of it the upward bit and 9/10 the downward bit. Since he told me about it, maybe
    two days ago, he wrote this amazing paper: [A DISCIPLINED APPROACH TO NEURAL NETWORK
    HYPER-PARAMETERS](https://arxiv.org/abs/1803.09820). In which, he describes something
    very slightly different to this again, but the same basic idea. This is a must
    read paper. It’s got all the kinds of ideas that fastai talks about a lot in great
    depth and nobody else is talking about this. It’s kind of a slog, unfortunately
    Leslie had to go away on a trop before he really had time to edit it properly,
    so it’s a little bit slow reading, but don’t let that stop you. It’s amazing.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如约，我们将看一下倾斜三角形学习率。这实际上不是我的想法。Leslie Smith，你们现在都知道的我最喜欢的研究人员之一，前段时间给我发了封邮件说：“我对循环学习率已经厌倦了。我不再这样做了。我现在做一个稍微不同的版本，其中有一个快速上升的周期，然后慢慢下降。我经常发现这样效果更好。”我试着回顾我所有的旧数据集，对所有数据集都效果更好——我尝试过的每一个。这就是学习率的样子。你可以通过在`fit`中添加`use_clr=`来在fastai中使用它。第一个数字是最高学习率和最低学习率之间的比率，因此初始学习率是峰值的1/32。第二个数字是第一个峰值和最后一个峰值之间的比率。基本思想是，如果你正在进行一个长度为10的周期，你希望第一个时期是上升的部分，其他9个时期是下降的部分，那么你会使用10。我发现这样效果非常好，这也是Leslie的建议，大约两天前，他写了这篇令人惊叹的论文：[神经网络超参数的纪律方法](https://arxiv.org/abs/1803.09820)。在这篇论文中，他描述了与此略有不同的内容，但基本思想相同。这是一篇必读的论文。它包含了fastai经常深入讨论的各种想法，其他人都没有谈论过。不幸的是，Leslie在真正有时间编辑它之前不得不离开度假，所以阅读起来有点慢，但不要让这阻止你。它很棒。
- en: The equation on the right is from my paper with Sebastian. Sebastian asked “Jeremy,
    can you send me the math equation behind that code you wrote?” and I said “no,
    I just wrote the code. I could not turn it into math” so he figured out the math
    for it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的方程是我和Sebastian的论文中的。Sebastian问：“Jeremy，你能把你写的代码背后的数学方程发给我吗？”我说：“不行，我只是写了代码。我无法把它转化为数学”，所以他为此找到了数学解。
- en: Concat pooling [[1:51:36](https://youtu.be/h5Tz7gZT9Fo?t=1h51m36s)]
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接池[1:51:36]
- en: So you might have noticed, the first layer of our classifier was equal to embedding
    size*3 . Why times 3? Times 3 because, and again, this seems to be something which
    people haven’t done before, so a new idea “concat pooling”. It is that we take
    the average pooling over the sequence of the activations, the max pooling of the
    sequence over the activations, and the final set of activations, and just concatenate
    them all together. This is something which we talked about in part 1 but doesn’t
    seem to be in the literature before so it’s now called “concat pooling” and it’s
    now got an equation and everything but this is the entirety of the implementation.
    So you can go through this paper and see how the fastai code implements each piece.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能已经注意到，我们分类器的第一层等于嵌入大小的3倍。为什么是3倍？因为，再次强调，这似乎是以前没有人做过的事情，所以一个新的想法“连接池”。我们对激活序列进行平均池化，对激活序列进行最大池化，以及最终的一组激活，并将它们全部连接在一起。这是我们在第一部分讨论过的内容，但在文献中似乎没有出现过，所以现在称为“连接池”，现在有一个方程，但这就是全部的实现。所以你可以阅读这篇论文，看看fastai代码如何实现每个部分。
- en: BPT3C [[1:52:46](https://youtu.be/h5Tz7gZT9Fo?t=1h52m46s)]
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BPT3C[1:52:46]
- en: One of the kind of interesting pieces is the difference between `RNN_Encoder`
    which you’ve already seen and MultiBatchRNN encoder. So what’s the difference
    there? The key difference is that the normal RNN encoder for the language model,
    we could just do `bptt` chunk at a time. But for the classifier, we need to do
    the whole document. We need to do the whole movie review before we decide if it’s
    positive or negative. And the whole movie review can easily be 2,000 words long
    and we can’t fit 2.000 words worth of gradients in my GPU memory for every single
    one of my weights. So what do we do? So the idea was very simple which is I go
    through my whole sequence length one batch of `bptt` at a time. And I call `super().forward`
    (in other words, the `RNN_Encoder`) to grab its outputs, and then I’ve got this
    maximum sequence length parameter where it says “okay, as long as you are doing
    no more than that sequence length, then start appending it to my list of outputs.”
    So in other words, the thing that it sends back to this pooling is only as many
    activations as we’ve asked it to keep. That way, you can figure out what `max_seq`
    can your particular GPU handle. So it’s still using the whole document, but let’s
    say `max_seq` is 1,000 words and your longest document length is 2, 000 words.
    It’s still going through RNN creating states for those first thousand words, but
    it’s not actually going to store the activations for the backprop of the first
    thousand. It’s only going to keep the last thousand. So that means that it can’t
    back-propagate the loss back to any state that was created in the first thousand
    words — basically that’s now gone. So it’s a really simple piece of code and honestly
    when I wrote it I didn’t spend much time thinking about it, it seems so obviously
    the only way this could possibly work. But again, it seems to be a new thing,
    so we now have backprop through time for text classification. You can see there’s
    lots of little pieces in this paper.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的地方是`RNN_Encoder`和MultiBatchRNN编码器之间的区别。那里有什么不同？关键区别在于，语言模型的普通RNN编码器，我们可以一次只做一个`bptt`块。但是对于分类器，我们需要处理整个文档。在我们决定它是积极的还是消极的之前，我们需要处理整个电影评论。整个电影评论可能很容易就有2000个字，而且我无法将2000个字的梯度适应我的GPU内存中的每一个权重。那我们该怎么办？所以这个想法非常简单，就是我一次处理整个序列长度的一个`bptt`批次。然后我调用`super().forward`（换句话说，`RNN_Encoder`）来获取它的输出，然后我有这个最大序列长度参数，它说“好的，只要你不超过那个序列长度，就开始将其附加到我的输出列表中。”换句话说，它发送回这个池的东西只有我们要求它保留的那么多激活。这样，你可以弄清楚你的特定GPU可以处理多少`max_seq`。所以它仍然使用整个文档，但是假设`max_seq`是1000个字，你最长的文档长度是2000个字。它仍然会通过RNN为这前1000个字创建状态，但实际上不会存储前1000个字的激活以进行反向传播。它只会保留最后1000个字。这意味着它无法将损失反向传播到在前1000个字中创建的任何状态，基本上那已经消失了。所以这是一个非常简单的代码片段，老实说，当我写它时，我没有花太多时间考虑，因为它似乎显而易见这是唯一可能起作用的方式。但是再次，这似乎是一个新事物，所以我们现在有了文本分类的时间反向传播。你可以看到这篇论文中有很多小片段。
- en: Results [[1:55:56](https://youtu.be/h5Tz7gZT9Fo?t=1h55m56s)]
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: What was the result? On every single dataset we tried, we got better result
    than any previous academic paper for text classification. All different types.
    Honestly, IMDb was the only one I spent any time trying to optimize the model,
    so most of them, we just did it whatever came out first. So if we actually spent
    time with it, I think this would be a lot better. The things that these are comparing
    to, most of them are different on each table because they are customized algorithms
    on the whole. So this is saying one simple fine-tuning algorithm can beat these
    really customized algorithms.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是什么？在我们尝试的每个数据集上，我们得到的结果都比以往任何一篇学术论文在文本分类方面都要好。各种不同类型。老实说，IMDb是我花时间尝试优化模型的唯一一个，所以大多数情况下，我们只是用第一个出来的结果。所以如果我们真的花时间在上面，我认为结果会更好。这些比较的对象大多数在每个表格上都不同，因为它们是整体上定制的算法。所以这就是说一个简单的微调算法可以击败这些真正定制的算法。
- en: Ablation studies [[1:56:56](https://youtu.be/h5Tz7gZT9Fo?t=1h56m56s)]
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消融研究
- en: Here is the ablation studies Sebastian did. I was really keen that if you are
    going to publish a paper, we had to say why it works. So Sebastian went through
    and tried removing all of those different contributions I mentioned. So what is
    we don’t use gradual freezing? What if we don’t use discriminative learning rates?
    What if instead of discrimination rates, we use cosign annealing? What if we don’t
    do any pre-training with Wikipedia? What if we don’t do any fine tuning? And the
    really interesting one to me was, what’s the validation error rate on IMDb if
    we only used a hundred training examples (vs. 200, vs. 500, etc). And you can
    see, very interestingly, the full version of this approach is nearly as accurate
    on just a hundred training examples — it’s still very accurate vs. full 20,000
    training examples. Where as if you are training from scratch on 100, it’s almost
    random. It’s what I expected. I’ve said to Sebastian I really think that this
    is most beneficial when you don’t have much data. This is where fastai is most
    interested in contributing — small data regimes, small compute regimes, and so
    forth. So he did these studies to check.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Sebastian做的消融研究。我非常希望如果要发表一篇论文，我们必须说明它为什么有效。所以Sebastian去尝试移除我提到的所有不同贡献。如果我们不使用逐渐冻结会怎样？如果我们不使用区分性学习率会怎样？如果我们使用余弦退火而不是区分性学习率会怎样？如果我们不使用维基百科进行任何预训练会怎样？如果我们不进行任何微调会怎样？对我来说真正有趣的是，如果我们只使用一百个训练样本（与200、500等相比），在IMDb上的验证错误率是多少。你可以看到，非常有趣的是，这种方法的完整版本在只有一百个训练样本时几乎与完整的20000个训练样本一样准确。而如果你从一百开始训练，几乎是随机的。这是我预料到的。我告诉Sebastian我真的认为这在没有太多数据时最有益。这就是fastai最感兴趣的地方——小数据范围，小计算范围等等。所以他进行了这些研究来检查。
- en: Tricks to run ablation studies [[1:58:32](https://youtu.be/h5Tz7gZT9Fo?t=1h58m32s)]
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行消融研究的技巧
- en: 'Trick #1: VNC'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧＃1：VNC
- en: 'The first trick is something which I know you’re all going to find really handy.
    I know you’ve all been annoyed when you are running something in a Jupyter notebook,
    and you lose your internet connection for long enough that it decides you’ve gone
    away, and then your session disappears, and you have to start it again from scratch.
    So what do you do? There is a very simple cool thing called VNC where you can
    install on your AWS instance or PaperSpace, or whatever:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个技巧是我知道你们都会觉得非常方便的。我知道当你在Jupyter笔记本中运行某些东西时，当你失去互联网连接的时间足够长，它会认为你已经离开，然后你的会话消失了，你必须从头开始。那么你该怎么办？有一个非常简单而酷炫的东西叫做VNC，你可以在您的AWS实例或PaperSpace上安装它，或者其他地方：
- en: X Windows (`xorg`)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X Windows（`xorg`）
- en: Lightweight window manager (`lxde-core`)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级窗口管理器（`lxde-core`）
- en: VNC server (`tightvncserver`)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VNC服务器（`tightvncserver`）
- en: Firefox (`firefox`)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firefox（`firefox`）
- en: Terminal (`lxterminal`)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终端（`lxterminal`）
- en: Some fonts (`xfonts-100dpi`)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些字体（`xfonts-100dpi`）
- en: 'Chuck the lines at the end of your `./vnc/xstartup` configuration file, and
    then run this command (`tightvncserver :13 -geometry 1200x900`):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些行添加到您的`./vnc/xstartup`配置文件的末尾，然后运行这个命令（`tightvncserver :13 -geometry 1200x900`）：
- en: 'It’s now running a server where you can then run the TightVNC Viewer or any
    VNC viewer on your computer and you point it at your server. But specifically,
    what you do is you use SSH port forwarding to forward :5913 to localhost:5913:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在正在运行一个服务器，您可以在您的计算机上运行TightVNC Viewer或任何VNC查看器，然后将其指向您的服务器。但具体来说，您要做的是使用SSH端口转发将：5913转发到localhost：5913：
- en: Then you connect to port 5013 on localhost. It will send it off to port 5913
    on your server which is the VNC port (because you said `:13`) and it will display
    an X Windows desktop. Then you can click on the Linux start like button and click
    on Firefox and you now have Firefox. You see here in Firefox, it says localhost
    because this Firefox is running on my AWS server. So you now run Firefox, you
    start your thing running, and then you close your VNC viewer remembering that
    Firefox is displaying on this virtual VNC display, not in a real display, so then
    later on that day, you log back into VNC viewer and it pops up again. So it’s
    like a persistent desktop, and it’s shockingly fast. It works really well. There’s
    lots of different VNC servers and clients, but this one works fine for me.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后连接到本地主机的端口5013。它会将其发送到服务器上的端口5913，这是VNC端口（因为你说了`:13`），它会显示一个X Windows桌面。然后你可以点击Linux的开始按钮，点击Firefox，现在你有了Firefox。你在Firefox中看到这里写着localhost，因为这个Firefox是在我的AWS服务器上运行的。所以你现在运行Firefox，启动你的东西，然后关闭你的VNC查看器，记住Firefox显示在这个虚拟VNC显示上，而不是在一个真实的显示器上，所以稍后那天，你再次登录VNC查看器，它会再次弹出。所以这就像一个持久的桌面，而且速度惊人快。它运行得非常好。有很多不同的VNC服务器和客户端，但这个对我来说效果很好。
- en: 'Trick #2: Google Fire [[2:01:27](https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s)]'
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧＃2：Google Fire[[2:01:27](https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s)]
- en: 'Trick #2 is to create Python scripts, and this is what we ended up doing. So
    I ended up creating a little Python script for Sebastian to kind of say this is
    the basic steps you need to do, and now you need to create different versions
    for everything else. And I suggested to him that he tried using this thing called
    Google Fire. What Google Fire does is, you create a function with tons of parameters,
    so these are all the things that Sebastian wanted to try doing — different dropout
    amounts, different learning rates, do I use pre-training or not, do I use CLR
    or not, do I use discriminative learning rate or not, etc. So you create a function,
    and then you add something saying:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧＃2是创建Python脚本，这就是我们最终做的事情。所以我最终为Sebastian创建了一个小Python脚本，告诉他这是你需要做的基本步骤，现在你需要为其他所有事情创建不同的版本。我建议他尝试使用一个叫做Google
    Fire的东西。Google Fire的作用是，你创建一个带有大量参数的函数，这些都是Sebastian想要尝试的不同事情 - 不同的dropout数量，不同的学习率，我是否使用预训练，我是否使用CLR，我是否使用区分性学习率等等。所以你创建一个函数，然后添加一些内容说：
- en: '[PRE49]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: You do nothing else at all — you don’t have to add any metadata, any docstrings,
    anything at all, and you then call that script and automatically you now have
    a command line interface. That’s a super fantastic easy way to run lots of different
    variations in a terminal. This ends up being easier if you want to do lots of
    variations than using a notebook because you can just have a bash script that
    tries all of them and spits them all out.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你什么都不做 - 你不必添加任何元数据，任何文档字符串，什么都不用添加，然后你调用那个脚本，你现在自动拥有了一个命令行界面。这是在终端中运行许多不同变体的超级简单方法。如果你想要做很多变体，这种方法比使用笔记本更容易，因为你可以编写一个bash脚本来尝试所有这些变体并将它们全部输出。
- en: 'Trick #3: IMDb scripts [[2:02:47](https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s)]'
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧＃3：IMDb脚本[[2:02:47](https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s)]
- en: You’ll find inside the `courses/dl2`, there’s now something called `imdb_scripts`,
    and I put all the scripts Sebastian and I used. Because we needed to tokenize
    and numericalize every dataset, then train a language model and a classifier for
    every dataset. And we had to do all of those things in a variety of different
    ways to compare them, so we had scripts for all those things. You can check out
    and see all of the scripts that we used.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在`courses/dl2`中找到一个名为`imdb_scripts`的文件夹，里面有Sebastian和我使用的所有脚本。因为我们需要对每个数据集进行标记化和数值化，然后为每个数据集训练一个语言模型和一个分类器。我们必须以各种不同的方式做所有这些事情来进行比较，所以我们为所有这些事情都准备了脚本。你可以查看并看到我们使用的所有脚本。
- en: 'Trick #4: pip install -e [[2:03:32](https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s)]'
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧＃4：pip install -e[[2:03:32](https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s)]
- en: When you are doing a lot of scripts, you got different code all over the place.
    Eventually it might get frustrating that you don’t want to symlink your fastai
    library again and again. But you probably don’t want to pip install it because
    that version tends to be a little bit old as we move so fast that you want to
    use the current version in Git. If you say `pip install -e .` from fastai repo
    base, it does something quite neat which is basically creates a symlink to the
    fastai library (i.e. your locally cloned Git repo) inside site-packages directory.
    Your site-packages directory is your main Python library. So if you do this, you
    can then access fastai from anywhere but every time you do `git pull`, you’ve
    got the most recent version. One downside of this is that it installs any updated
    versions of packages from pip which can confuse Conda a little bit, so another
    alternative here is just do symlink the fastai library to your site packages library.
    That works just as well. You can use fastai from anywhere and it’s quite handy
    when you want to run scripts that use fastai from different directories on your
    system.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在做很多脚本时，你会在各个地方得到不同的代码。最终，你可能会感到沮丧，不想一遍又一遍地创建 fastai 库的符号链接。但你可能也不想 pip 安装它，因为那个版本往往有点旧，我们进展如此之快，你想使用
    Git 中的当前版本。如果你从 fastai 仓库基础目录运行 `pip install -e .`，它会做一些相当巧妙的事情，基本上是在 site-packages
    目录内创建一个到 fastai 库的符号链接（即你本地克隆的 Git 仓库）。你的 site-packages 目录是你的主 Python 库。所以如果你这样做，你就可以从任何地方访问
    fastai，但每次你执行 `git pull` 时，你都会得到最新版本。这样做的一个缺点是它会安装 pip 中的任何更新版本的包，这可能会让 Conda
    有点困惑，所以另一个选择是只需将 fastai 库符号链接到你的 site-packages 库。这同样有效。你可以从任何地方使用 fastai，当你想要在系统的不同目录中运行使用
    fastai 的脚本时，这是非常方便的。
- en: 'Trick #5: SentencePiece [[2:05:06](https://youtu.be/h5Tz7gZT9Fo?t=2h5m6s)]'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '技巧 #5: SentencePiece'
- en: This is something you can try if you like. You don’t have to tokenize. Instead
    of tokenizing words, you can tokenize what are called sub-word units.For example,
    “unsupervised” could be tokenized as “un” and “supervised”. “Tokenizer” can be
    tokenized as [“token”, “izer”]. Then you could do the same thing. The language
    model that works on sub-word units, a classifier that works on sub-word units,
    etc. How well does that work? I started playing with it and with not too much
    playing, I was getting classification results that were nearly as good as using
    word level tokenization — not quite as good, but nearly as good. I suspect with
    more careful thinking and playing around, maybe I could have gotten as good or
    better. But even if I couldn’t, if you create a sub-word-unit wikitext model,
    then IMDb language model, and then classifier forwards and backwards and then
    ensemble it with the forwards and backwards word level ones, you should be able
    to beat us. So here is an approach you may be able to beat our state-of-the-art
    result.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，这是你可以尝试的东西。你不必进行标记化。你可以标记化所谓的子词单元，而不是标记化单词。例如，“unsupervised”可以被标记化为“un”和“supervised”。“Tokenizer”可以被标记化为[“token”,
    “izer”]。然后你可以做同样的事情。使用子词单元的语言模型，使用子词单元的分类器等。这样做效果如何？我开始尝试并且没有花太多时间，我得到的分类结果几乎和使用单词级标记化一样好
    —— 不完全一样，但几乎一样好。我怀疑通过更仔细的思考和尝试，也许我可以得到同样好甚至更好的结果。但即使我不能，如果你创建一个子词单元维基文本模型，然后 IMDb
    语言模型，然后分类器正向和反向，然后将其与正向和反向的单词级模型合并，你应该能够超越我们。所以这是一个你可能能够超越我们最先进结果的方法。
- en: Sebastian told me this particular project — Google has a project called sentence
    peace which actually uses a neural net to figure out the optimal splitting up
    of words and so you end up with vocabulary of sub-word units. In my playing around,
    I found that create vocabulary of about 30,000 sub-word units seems to be about
    optimal. If you are interested, there is something you can try. It is a bit of
    a pain to install — it’s C++, doesn’t have create error message, but it will work.
    There is a Python library for it. If anybody tries this, I’m happy to help them
    get it working. There’s been little, if any, experiments with ensembling sub-word
    and word level classification, and I do think it should be the best approach.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian 告诉我这个特定项目 —— 谷歌有一个名为 sentence peace 的项目，实际上使用神经网络来找出单词的最佳拆分方式，因此你最终会得到一个子词单元的词汇表。在我的尝试中，我发现创建大约
    30,000 个子词单元的词汇表似乎是最佳的。如果你感兴趣，这是你可以尝试的东西。安装起来有点麻烦 —— 它是 C++，没有创建错误消息，但它会工作。有一个
    Python 库可以用于此。如果有人尝试这个，我很乐意帮助他们让它工作。对于子词和单词级别分类的集成，几乎没有什么实验，我认为这应该是最佳的方法。
- en: Have a great week!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你有一个美好的一周！
