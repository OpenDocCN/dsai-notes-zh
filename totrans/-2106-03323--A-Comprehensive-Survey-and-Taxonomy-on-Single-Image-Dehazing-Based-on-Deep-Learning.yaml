- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:54:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:54:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.03323] A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.03323] 《基于深度学习的单幅图像去雾的综合调查与分类》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.03323](https://ar5iv.labs.arxiv.org/html/2106.03323)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.03323](https://ar5iv.labs.arxiv.org/html/2106.03323)
- en: A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《基于深度学习的单幅图像去雾的综合调查与分类》
- en: Jie Gui [guijie@seu.edu.cn](mailto:guijie@seu.edu.cn) School of Cyber Science
    and Engineering, Southeast University and Purple Mountain LaboratoriesNanjingJiangsuChina210000
    ,  Xiaofeng Cong School of Cyber Science and Engineering, Southeast UniversityNanjingChina
    [cxf_svip@163.com](mailto:cxf_svip@163.com) ,  Yuan Cao Ocean University of China
    QingdaoChina [cy8661@ouc.edu.cn](mailto:cy8661@ouc.edu.cn) ,  Wenqi Ren Institute
    of Information Engineering, Chinese Academy of SciencesBeijingChina [renwenqi@iie.ac.cn](mailto:renwenqi@iie.ac.cn)
    ,  Jun Zhang Anhui UniversityHefeiChina [wwwzhangjun@163.com](mailto:wwwzhangjun@163.com)
    ,  Jing Zhang The University of SydneySydneyAustralia [jing.zhang1@sydney.edu.au](mailto:jing.zhang1@sydney.edu.au)
    ,  Jiuxin Cao School of Cyber Science and Engineering, Southeast UniversityNanjingChina
     and  Dacheng Tao JD Explore Academy, China and The University of SydneySydneyAustralia
    [dacheng.tao@gmail.com](mailto:dacheng.tao@gmail.com)(2022)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jie Gui [guijie@seu.edu.cn](mailto:guijie@seu.edu.cn) 东南大学网络科学与工程学院及紫金山实验室 南京
    江苏 中国 210000，Xiaofeng Cong 东南大学网络科学与工程学院 南京 中国 [cxf_svip@163.com](mailto:cxf_svip@163.com)，Yuan
    Cao 中国海洋大学 青岛 中国 [cy8661@ouc.edu.cn](mailto:cy8661@ouc.edu.cn)，Wenqi Ren 中国科学院信息工程研究所
    北京 中国 [renwenqi@iie.ac.cn](mailto:renwenqi@iie.ac.cn)，Jun Zhang 安徽大学 合肥 中国 [wwwzhangjun@163.com](mailto:wwwzhangjun@163.com)，Jing
    Zhang 悉尼大学 悉尼 澳大利亚 [jing.zhang1@sydney.edu.au](mailto:jing.zhang1@sydney.edu.au)，Jiuxin
    Cao 东南大学网络科学与工程学院 南京 中国 以及 Dacheng Tao 京东探索学院，中国和悉尼大学 悉尼 澳大利亚 [dacheng.tao@gmail.com](mailto:dacheng.tao@gmail.com)（2022）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: With the development of convolutional neural networks, hundreds of deep learning
    based dehazing methods have been proposed. In this paper, we provide a comprehensive
    survey on supervised, semi-supervised, and unsupervised single image dehazing.
    We first discuss the physical model, datasets, network modules, loss functions,
    and evaluation metrics that are commonly used. Then, the main contributions of
    various dehazing algorithms are categorized and summarized. Further, quantitative
    and qualitative experiments of various baseline methods are carried out. Finally,
    the unsolved issues and challenges that can inspire the future research are pointed
    out. A collection of useful dehazing materials is available at [https://github.com/Xiaofeng-life/AwesomeDehazing](https://github.com/Xiaofeng-life/AwesomeDehazing).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卷积神经网络的发展，已经提出了数百种基于深度学习的去雾方法。本文提供了关于监督学习、半监督学习和无监督学习单幅图像去雾的综合调查。我们首先讨论了常用的物理模型、数据集、网络模块、损失函数和评估指标。然后，对各种去雾算法的主要贡献进行了分类和总结。此外，还进行了各种基线方法的定量和定性实验。最后，指出了尚未解决的问题和挑战，这些问题和挑战可以激发未来的研究。更多有用的去雾材料可以在
    [https://github.com/Xiaofeng-life/AwesomeDehazing](https://github.com/Xiaofeng-life/AwesomeDehazing)
    获取。
- en: 'image dehazing, supervised, semi-supervised, unsupervised, atmospheric scattering
    model.^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†doi: XXXXXXX.XXXXXXX^†^†conference:
    Make sure to enter the correct conference title from your rights confirmation
    emai; June 03–05, 2018; Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Vision for robotics^†^†ccs: Computing methodologies Computational
    photography^†^†ccs: Computing methodologies Computer vision'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去雾，监督学习，半监督学习，无监督学习，大气散射模型。^†^†版权：acm版权^†^†期刊年份：2022^†^†doi：XXXXXXX.XXXXXXX^†^†会议：请确保输入您权利确认邮件中的正确会议标题；2018年6月03–05日；纽约伍德斯托克^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法
    机器人视觉^†^†ccs：计算方法 计算摄影^†^†ccs：计算方法 计算机视觉
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Due to the absorption by floating particles contained in the hazy environment,
    the quality of the image captured by the camera will be reduced. The phenomenon
    of image quality degradation in hazy weather has a negative impact on photography
    work. The contrast of the image will decrease and the color will shift. Meantime,
    the texture and edge of objects in the scene will become blurred. As shown in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), there is an obvious difference
    between the pixel histograms of hazy and haze-free images. For computer vision
    tasks such as object detection and image segmentation, low-quality inputs can
    degrade the performance of the models trained on haze-free images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雾霾环境中漂浮颗粒的吸收，摄像机拍摄的图像质量会降低。雾霾天气中的图像质量下降对摄影工作产生负面影响。图像的对比度会降低，颜色会偏移。同时，场景中物体的纹理和边缘会变得模糊。如图[1](#S1.F1
    "图 1 ‣ 1\. 介绍 ‣ 基于深度学习的单幅图像去雾的全面调查与分类")所示，雾霾图像和无雾图像的像素直方图存在明显差异。对于目标检测和图像分割等计算机视觉任务，低质量的输入会降低在无雾图像上训练的模型的性能。
- en: Therefore, many researchers try to recover high-quality clear scenes from hazy
    images. Before deep learning was widely used in computer vision tasks, image dehazing
    algorithms had mainly relied on various prior assumptions (He et al., [2010](#bib.bib52))
    and atmospheric scattering model (ASM) (McCartney, [1976](#bib.bib94)). The processing
    flow of these statistical rule based methods has good interpretability. However,
    they may exhibit shortcomings when facing complex real world scenarios. For example,
    the well-known dark channel prior (He et al., [2010](#bib.bib52)) (DCP, best paper
    of CVPR 2009) cannot handle regions containing sky well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多研究人员试图从雾霾图像中恢复高质量的清晰场景。在深度学习广泛应用于计算机视觉任务之前，图像去雾算法主要依赖于各种先验假设（He等，[2010](#bib.bib52)）和大气散射模型（ASM）（McCartney，[1976](#bib.bib94)）。这些基于统计规则的方法的处理流程具有良好的可解释性。然而，它们在面对复杂的真实世界场景时可能会显示出缺陷。例如，著名的暗通道先验（He等，[2010](#bib.bib52)）（DCP，CVPR
    2009最佳论文）无法很好地处理包含天空的区域。
- en: Inspired by deep learning, (Ren et al., [2016](#bib.bib111); Cai et al., [2016](#bib.bib13);
    Ren et al., [2020](#bib.bib113)) combine ASM and convolutional neural network
    (CNN) to estimate the parameters of the ASM. Quantitative and qualitative experimental
    results show that deep learning can help the prediction of these parameters in
    a supervised way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 受到深度学习的启发，（Ren等，[2016](#bib.bib111)；Cai等，[2016](#bib.bib13)；Ren等，[2020](#bib.bib113)）将ASM和卷积神经网络（CNN）结合起来，以估计ASM的参数。定量和定性实验结果表明，深度学习可以在监督方式下帮助预测这些参数。
- en: Following this, (Qin et al., [2020](#bib.bib109); Liu et al., [2019a](#bib.bib91);
    Liang et al., [2019](#bib.bib85); Zhang et al., [2022c](#bib.bib182); Zheng et al.,
    [2021](#bib.bib190)) have demonstrated that end-to-end supervised dehazing networks
    can be implemented independently of the ASM. Thanks to the powerful feature extraction
    capability of CNN, these non-ASM-based dehazing algorithms can achieve comparable
    accuracy as ASM-based algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 继而，（秦等，[2020](#bib.bib109)；刘等，[2019a](#bib.bib91)；梁等，[2019](#bib.bib85)；张等，[2022c](#bib.bib182)；郑等，[2021](#bib.bib190)）已经证明，端到端的监督去雾网络可以独立于ASM进行实现。得益于CNN强大的特征提取能力，这些非ASM基础的去雾算法可以达到与ASM基础算法相当的准确性。
- en: ASM-based and non-ASM-based supervised algorithms have shown impressive performance.
    However, they often require synthetic paired images that are inconsistent with
    real world hazy images. Therefore, recent research focus on methods that are more
    suitable to the real world dehazing task. (Cong et al., [2020](#bib.bib24); Golts
    et al., [2020](#bib.bib44); Li et al., [2020b](#bib.bib72)) explore unsupervised
    algorithms that do not require synthetic data while other studies (Li et al.,
    [2020a](#bib.bib79); An et al., [2022](#bib.bib2); Chen et al., [2021b](#bib.bib21);
    Zhang and Li, [2021](#bib.bib172)) propose semi-supervised algorithms that exploit
    both synthetic paired data and real world unpaired data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ASM的和非ASM基础的监督算法都表现出了令人印象深刻的性能。然而，它们通常需要与真实世界雾霾图像不一致的合成配对图像。因此，最近的研究重点是更加适合真实世界去雾任务的方法。（Cong等，[2020](#bib.bib24)；Golts等，[2020](#bib.bib44)；Li等，[2020b](#bib.bib72)）探索了不需要合成数据的无监督算法，而其他研究（Li等，[2020a](#bib.bib79)；An等，[2022](#bib.bib2)；Chen等，[2021b](#bib.bib21)；张和李，[2021](#bib.bib172)）提出了利用合成配对数据和真实世界无配对数据的半监督算法。
- en: '![Refer to caption](img/50395ece082c77eeeddb8770c5ca443b.png)![Refer to caption](img/3711e6faf7811490083dce87156b8618.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/50395ece082c77eeeddb8770c5ca443b.png)![参见图例](img/3711e6faf7811490083dce87156b8618.png)'
- en: (a) A clear image                                (c) A hazy image                               
    ![Refer to caption](img/ce29973e98b8d683a3a16a2a78041334.png) ![Refer to caption](img/cd6060c43bbf613b833cc7a28e19ae1b.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 清晰图像                                (c) 有雾图像                               
    ![参见图例](img/ce29973e98b8d683a3a16a2a78041334.png) ![参见图例](img/cd6060c43bbf613b833cc7a28e19ae1b.png)
- en: (b) Histogram for (a)                   (d) Histogram for (c)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (b) (a) 的直方图                   (d) (c) 的直方图
- en: Figure 1\. Pixel histograms of clear (a) and hazy (c) images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 清晰图像 (a) 和有雾图像 (c) 的像素直方图。
- en: With the rapid development in this area, hundreds of dehazing methods have been
    proposed. To inspire and guide the future research, a comprehensive survey is
    urgently needed. Some papers have attempted to partially review the recent development
    of the dehazing research. For example,  (Singh and Kumar, [2019](#bib.bib129);
    Li et al., [2017b](#bib.bib84); Xu et al., [2015](#bib.bib152)) gives a summary
    of the non-deep learning dehazing methods, including depth estimation, wavelet,
    enhancement, filtering, but lacks of research on recent CNN-based methods. Parihar
    et al. (Parihar et al., [2020](#bib.bib106)) provides a survey about supervised
    dehazing models, but it does not pay enough attention to the latest explorations
    of semi-supervised and unsupervised methods. Banerjee et al. (Banerjee and Chaudhuri,
    [2021](#bib.bib11)) introduce and group the existing nighttime image dehazing
    methods, however, the methods of daytime are rarely analyzed. Gui et al. (Gui
    et al., [2021](#bib.bib46)) briefly classify and analyze supervised and unsupervised
    algorithms, but does not summarize the various recently proposed semi-supervised
    methods. Unlike existing reviews, we give a comprehensive survey on the supervised,
    semi-supervised and unsupervised daytime dehazing models based on deep learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一领域的迅速发展，已经提出了数百种去雾方法。为了激发和指导未来的研究，迫切需要一项全面的调查。虽然有一些论文尝试部分回顾去雾研究的最新进展，但大多数研究仍有所欠缺。例如，(Singh
    and Kumar, [2019](#bib.bib129); Li et al., [2017b](#bib.bib84); Xu et al., [2015](#bib.bib152))
    总结了非深度学习的去雾方法，包括深度估计、小波、增强、滤波，但缺乏对近期基于 CNN 的方法的研究。Parihar et al. (Parihar et al.,
    [2020](#bib.bib106)) 提供了关于监督去雾模型的调查，但没有充分关注半监督和无监督方法的最新探索。Banerjee et al. (Banerjee
    and Chaudhuri, [2021](#bib.bib11)) 介绍并归类了现有的夜间图像去雾方法，但白天的去雾方法很少被分析。Gui et al.
    (Gui et al., [2021](#bib.bib46)) 简要分类和分析了监督与无监督算法，但未总结最近提出的各种半监督方法。与现有的综述不同，我们对基于深度学习的监督、半监督和无监督的白天去雾模型进行了全面的调查。
- en: '![Refer to caption](img/ce35310209484fc8e93d3ea80527fe22.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ce35310209484fc8e93d3ea80527fe22.png)'
- en: Figure 2\. Atmospheric Scattering Model (ASM), same as (Cai et al., [2016](#bib.bib13)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 大气散射模型（ASM），与 (Cai et al., [2016](#bib.bib13)) 相同。
- en: Table 1\. A taxonomy of dehazing methods. Red number index represents ASM-based
    methods, and black number index represents non-ASM-based methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 去雾方法的分类。红色编号索引代表基于 ASM 的方法，黑色编号索引代表非 ASM 基础的方法。
- en: '| Category | Key Idea | Methods |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 关键思想 | 方法 |'
- en: '| Supervised | Learning of $t(x)$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 监督学习 | $t(x)$ 的学习 |'
- en: '&#124; DehazeNet (Cai et al., [2016](#bib.bib13)), ABC-Net (Wang et al., [2020](#bib.bib134)),
    MSCNN (Ren et al., [2016](#bib.bib111)), &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DehazeNet (Cai et al., [2016](#bib.bib13)), ABC-Net (Wang et al., [2020](#bib.bib134)),
    MSCNN (Ren et al., [2016](#bib.bib111)), &#124;'
- en: '&#124; MSCNN-HE (Ren et al., [2020](#bib.bib113)), SID-JMP (Huang et al., [2018](#bib.bib59)),
    LATPN (Liu et al., [2018](#bib.bib89)) &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSCNN-HE (Ren et al., [2020](#bib.bib113)), SID-JMP (Huang et al., [2018](#bib.bib59)),
    LATPN (Liu et al., [2018](#bib.bib89)) &#124;'
- en: '|'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Joint learning of $t(x)$ and $A$ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $t(x)$ 和 $A$ 的联合学习 |'
- en: '&#124; DCPDN (Zhang and Patel, [2018](#bib.bib164)), DSIEN (Guo et al., [2019b](#bib.bib49)),
    LDPID (Liu et al., [2019b](#bib.bib92)), &#124;'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DCPDN (Zhang and Patel, [2018](#bib.bib164)), DSIEN (Guo et al., [2019b](#bib.bib49)),
    LDPID (Liu et al., [2019b](#bib.bib92)), &#124;'
- en: '&#124; PMHLD (Chen et al., [2020](#bib.bib19)), HRGAN (Pang et al., [2018](#bib.bib105))
    &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PMHLD (Chen et al., [2020](#bib.bib19)), HRGAN (Pang et al., [2018](#bib.bib105))
    &#124;'
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Non-explicitly embedded ASM |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 非显式嵌入 ASM |'
- en: '&#124; AOD-Net (Li et al., [2017a](#bib.bib73)), FAMED-Net (Zhang and Tao,
    [2020](#bib.bib171)), DehazeGAN (Zhu et al., [2018](#bib.bib192)), PFDN (Dong
    and Pan, [2020](#bib.bib31)), &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AOD-Net (Li et al., [2017a](#bib.bib73)), FAMED-Net (Zhang and Tao,
    [2020](#bib.bib171)), DehazeGAN (Zhu et al., [2018](#bib.bib192)), PFDN (Dong
    and Pan, [2020](#bib.bib31)), &#124;'
- en: '&#124; SI-DehazeGAN (Zhu et al., [2021](#bib.bib191)) &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SI-DehazeGAN (Zhu et al., [2021](#bib.bib191)) &#124;'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Generative adversarial network |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 生成对抗网络 |'
- en: '&#124; EPDN (Qu et al., [2019](#bib.bib110)), PGC-UNet (Zhao et al., [2021a](#bib.bib187)),
    RI-GAN (Dudhane et al., [2019](#bib.bib37)), &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EPDN (屈等，[2019](#bib.bib110)), PGC-UNet (赵等，[2021a](#bib.bib187)), RI-GAN (杜丹等，[2019](#bib.bib37)),
    &#124;'
- en: '&#124; DHGAN (Sim et al., [2018](#bib.bib126)), SA-CGAN (Sharma et al., [2020](#bib.bib121))
    &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DHGAN (沈等，[2018](#bib.bib126)), SA-CGAN (香玛等，[2020](#bib.bib121)) &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Level-aware | LAP-Net (Li et al., [2019b](#bib.bib83)), HardGAN (Deng et al.,
    [2020](#bib.bib26)) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 级别感知 | LAP-Net (李等，[2019b](#bib.bib83)), HardGAN (邓等，[2020](#bib.bib26))
    |'
- en: '| Multi-function fusion | DMMFD (Deng et al., [2019](#bib.bib27)) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 多功能融合 | DMMFD (邓等，[2019](#bib.bib27)) |'
- en: '| Transformation and decomposition of input | GFN (Ren et al., [2018a](#bib.bib112)),
    MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)), DPDP-Net (Yang et al., [2019](#bib.bib154)),
    DIDH (Shyam et al., [2021](#bib.bib124)) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 输入的变换与分解 | GFN (任等，[2018a](#bib.bib112)), MSRL-DehazeNet (叶等，[2019](#bib.bib158)),
    DPDP-Net (杨等，[2019](#bib.bib154)), DIDH (夏姆等，[2021](#bib.bib124)) |'
- en: '| Knowledge distillation | KDDN (Hong et al., [2020](#bib.bib55)), KTDN (Wu
    et al., [2020](#bib.bib148)), SRKTDN (Chen et al., [2021a](#bib.bib17)), DALF (Fang
    et al., [2021](#bib.bib39)) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 | KDDN (洪等，[2020](#bib.bib55)), KTDN (吴等，[2020](#bib.bib148)), SRKTDN (陈等，[2021a](#bib.bib17)),
    DALF (方等，[2021](#bib.bib39)) |'
- en: '| Transformation of colorspace | AIP-Net (Wang et al., [2018a](#bib.bib133)),
    MSRA-Net (Sheng et al., [2022](#bib.bib122)), TheiaNet (Mehra et al., [2021](#bib.bib95)),
    RYF-Net (Dudhane and Murala, [2019b](#bib.bib36)) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 颜色空间变换 | AIP-Net (王等，[2018a](#bib.bib133)), MSRA-Net (盛等，[2022](#bib.bib122)),
    TheiaNet (梅赫拉等，[2021](#bib.bib95)), RYF-Net (杜丹和穆拉拉，[2019b](#bib.bib36)) |'
- en: '| Contrastive learning | AECR-Net  (Wu et al., [2021](#bib.bib149)) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 对比学习 | AECR-Net  (吴等，[2021](#bib.bib149)) |'
- en: '| Non-deterministic output | pWAE (Kim et al., [2021](#bib.bib67)), DehazeFlow (Li
    et al., [2021b](#bib.bib77)) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 非确定性输出 | pWAE (金等，[2021](#bib.bib67)), DehazeFlow (李等，[2021b](#bib.bib77))
    |'
- en: '| Retinex model | RDN (Li et al., [2021c](#bib.bib80)) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Retinex模型 | RDN (李等，[2021c](#bib.bib80)) |'
- en: '| Residual learning | GCA-Net(Chen et al., [2019c](#bib.bib14)), DRL (Du and
    Li, [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)), POGAN (Du
    and Li, [2019](#bib.bib34)) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 残差学习 | GCA-Net(陈等，[2019c](#bib.bib14)), DRL (杜和李，[2018](#bib.bib33)), SID-HL (肖等，[2020](#bib.bib150)),
    POGAN (杜和李，[2019](#bib.bib34)) |'
- en: '| Frequency domain |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 频域 |'
- en: '&#124; Wavelet U-net  (Yang and Fu, [2019](#bib.bib156)), MsGWN (Dong et al.,
    [2020c](#bib.bib30)), EMRA-Net (Wang et al., [2021d](#bib.bib136)), &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Wavelet U-net  (杨和傅，[2019](#bib.bib156)), MsGWN (董等，[2020c](#bib.bib30)),
    EMRA-Net (王等，[2021d](#bib.bib136)), &#124;'
- en: '&#124; TDN (Liu et al., [2020b](#bib.bib86)), DW-GAN (Fu et al., [2021](#bib.bib40))
    &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TDN (刘等，[2020b](#bib.bib86)), DW-GAN (傅等，[2021](#bib.bib40)) &#124;'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Joint dehazing and depth estimation |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 联合去雾与深度估计 |'
- en: '&#124; SDDE (Lee et al., [2020a](#bib.bib69)), S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)), DDRL (Guo and Monga, [2020](#bib.bib50)), &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SDDE (李等，[2020a](#bib.bib69)), S2DNet (汉巴尔德和穆拉拉，[2020](#bib.bib51)),
    DDRL (郭和蒙戈，[2020](#bib.bib50)), &#124;'
- en: '&#124; DeAID (Yang and Zhang, [2022](#bib.bib155)), TSDCN-Net  (Cheng and Zhao,
    [2021](#bib.bib23)) &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeAID (杨和张，[2022](#bib.bib155)), TSDCN-Net  (程和赵，[2021](#bib.bib23))
    &#124;'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Detection and segmentation with dehazing | LEAAL (Li et al., [2020c](#bib.bib82)),
    SDNet  (Zhang et al., [2022a](#bib.bib178)), UDnD (Zhang et al., [2020g](#bib.bib186))
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 去雾检测与分割 | LEAAL (李等，[2020c](#bib.bib82)), SDNet  (张等，[2022a](#bib.bib178)),
    UDnD (张等，[2020g](#bib.bib186)) |'
- en: '| End-to-end CNN |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 端到端CNN |'
- en: '&#124; FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu et al.,
    [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)) &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FFA-Net (秦等，[2020](#bib.bib109)), GridDehazeNet (刘等，[2019a](#bib.bib91)),
    SAN (梁等，[2019](#bib.bib85)) &#124;'
- en: '&#124; HFF (Zhang et al., [2022c](#bib.bib182)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)) &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HFF (张等，[2022c](#bib.bib182)), 4kDehazing (郑等，[2021](#bib.bib190)) &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semi-supervised | Pretrain backbone and fine-tune | PSD (Chen et al., [2021b](#bib.bib21)),
    SSDT (Zhang and Li, [2021](#bib.bib172)) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 半监督 | 预训练骨干网络和微调 | PSD (陈等，[2021b](#bib.bib21)), SSDT (张和李，[2021](#bib.bib172))
    |'
- en: '| Disentangled and reconstruction | DCNet (Chen et al., [2021c](#bib.bib22)),
    FSR (Liu et al., [2021](#bib.bib93)), CCDM (Zhang et al., [2020f](#bib.bib180))
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 解耦与重建 | DCNet (陈等，[2021c](#bib.bib22)), FSR (刘等，[2021](#bib.bib93)), CCDM (张等，[2020f](#bib.bib180))
    |'
- en: '| Two-branches training | DAID (Shao et al., [2020](#bib.bib119)), SSID (Li
    et al., [2020a](#bib.bib79)), SSIDN (An et al., [2022](#bib.bib2)) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 两分支训练 | DAID (邵等，[2020](#bib.bib119)), SSID (李等，[2020a](#bib.bib79)), SSIDN (安等，[2022](#bib.bib2))
    |'
- en: '| Unsupervised | Unsupervised domain translation |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | 无监督领域转换 |'
- en: '&#124; Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), CDNet (Dudhane and
    Murala, [2019a](#bib.bib35)), E-CycleGAN (Liu et al., [2020a](#bib.bib90)) &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), CDNet (Dudhane and
    Murala, [2019a](#bib.bib35)), E-CycleGAN (Liu et al., [2020a](#bib.bib90)) &#124;'
- en: '&#124; USID (Huang et al., [2019](#bib.bib57)), DCA-CycleGAN (Mo et al., [2022](#bib.bib99)),
    DHL (Cong et al., [2020](#bib.bib24)) &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; USID (Huang et al., [2019](#bib.bib57)), DCA-CycleGAN (Mo et al., [2022](#bib.bib99)),
    DHL (Cong et al., [2020](#bib.bib24)) &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Learning without haze-free images | Deep-DCP (Golts et al., [2020](#bib.bib44))
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 无需去雾图像的学习 | Deep-DCP (Golts et al., [2020](#bib.bib44)) |'
- en: '| Unsupervised image decomposition | Double-DIP (Gandelsman et al., [2019](#bib.bib42))
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 无监督图像分解 | Double-DIP (Gandelsman et al., [2019](#bib.bib42)) |'
- en: '| Zero-shot learning | ZID (Li et al., [2020b](#bib.bib72)), YOLY (Li et al.,
    [2021a](#bib.bib71)) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 零样本学习 | ZID (Li et al., [2020b](#bib.bib72)), YOLY (Li et al., [2021a](#bib.bib71))
    |'
- en: 1.1\. Scope and Goals of This Survey
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 本调查的范围和目标
- en: 'This survey does not cover all themes of dehazing research. We focus our attention
    on deep learning based algorithms that employ monocular daytime images. This means
    that we will not discuss in detail about non-deep learning dehazing, underwater
    dehazing (Wang et al., [2021b](#bib.bib141), [2017](#bib.bib142); Li et al., [2016](#bib.bib76)),
    video dehazing (Ren et al., [2018b](#bib.bib114)), hyperspectral dehazing (Mehta
    et al., [2020](#bib.bib96)), nighttime dehazing (Zhang et al., [2020a](#bib.bib169),
    [2017a](#bib.bib167), [2014](#bib.bib168)), binocular dehazing (Pang et al., [2020](#bib.bib104)),
    etc. Therefore, when we refer to “dehazing” in this paper, we usually mean deep
    learning based algorithms whose input data satisfies four conditions: single frame
    image, daytime, monocular, on the ground. In summary, there are three contributions
    to this survey as follows.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查不涵盖所有去雾研究主题。我们重点关注基于深度学习的算法，这些算法使用单目白天图像。这意味着我们不会详细讨论非深度学习去雾、潜水去雾 (Wang et
    al., [2021b](#bib.bib141), [2017](#bib.bib142); Li et al., [2016](#bib.bib76))、视频去雾
    (Ren et al., [2018b](#bib.bib114))、高光谱去雾 (Mehta et al., [2020](#bib.bib96))、夜间去雾
    (Zhang et al., [2020a](#bib.bib169), [2017a](#bib.bib167), [2014](#bib.bib168))、双目去雾
    (Pang et al., [2020](#bib.bib104)) 等。因此，当我们在本文中提到“去雾”时，通常指基于深度学习的算法，其输入数据满足四个条件：单帧图像、白天、单目、地面。总之，本文的贡献有以下三点。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Commonly used physical models, datasets, network modules, loss functions and
    evaluation metrics are summarized.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常用的物理模型、数据集、网络模块、损失函数和评估指标已被总结。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A classification and introduction of supervised, semi-supervised, and unsupervised
    methods is presented.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍了监督、半监督和无监督方法的分类。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to the existing achievements and unsolved problems, the future research
    directions are prospected.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据现有成果和未解决的问题，展望了未来的研究方向。
- en: 1.2\. A Guide for Reading This Survey
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 阅读本调查的指南
- en: Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") introduces physical model ASM, synthetic
    & generated & real world datasets, loss functions, basic modules commonly used
    in dehazing networks and evaluation metrics for various algorithms. Section [3](#S3
    "3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") provides a comprehensive discussion of supervised
    dehazing algorithms. A review of semi-supervised and unsupervised dehazing methods
    is in Section [4](#S4 "4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") and Section [5](#S5
    "5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), respectively. Section [6](#S6 "6\. Experiment
    and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") presents quantitative and qualitative experimental
    results for three categories of baseline algorithms. Section [7](#S7 "7\. Challenges
    and Opportunities ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing
    Based on Deep Learning") discusses the open issues of dehazing research.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 部分[2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")介绍了物理模型ASM、合成与生成以及现实世界数据集、损失函数、去雾网络中常用的基本模块以及各种算法的评估指标。部分[3](#S3
    "3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")提供了对监督去雾算法的全面讨论。半监督和无监督去雾方法的综述分别在部分[4](#S4 "4\.
    Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")和[5](#S5 "5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")中进行。部分[6](#S6
    "6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")展示了三类基线算法的定量和定性实验结果。部分[7](#S7
    "7\. Challenges and Opportunities ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")讨论了去雾研究中的开放问题。
- en: A critical challenge for a logical and comprehensive review of dehazing research
    is how to properly classify existing methods. In the classification of Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), there are several items that need to be pointed
    out as follows.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对去雾研究进行逻辑性和全面性综述的一个关键挑战是如何正确分类现有方法。在表格[1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")的分类中，有几个项目需要指出如下。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The DCP is validated on the dehazing task, and the inference process utilizes
    ASM. Therefore, those methods that utilize DCP are considered to be ASM-based.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DCP在去雾任务上得到了验证，并且推断过程利用了ASM。因此，利用DCP的方法被认为是基于ASM的。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey treat the knowledge distillation based supervised dehazing network
    as a supervised algorithm rather than a weakly supervised/semi-supervised algorithm.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查将基于知识蒸馏的监督去雾网络视为一种监督算法，而不是弱监督/半监督算法。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Supervised dehazing methods using total variation loss or GAN loss are still
    classified as supervised.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用总变差损失或生成对抗网络（GAN）损失的监督去雾方法仍然被归类为监督学习方法。
- en: 'Here we give the notational conventions for this survey. Unless otherwise specified,
    all symbols have the following meanings: $I(x)$ means hazy image; $J(x)$ denotes
    haze-free image. $t(x)$ stands for transmission map; $x$ in $I(x)$, $J(x)$ and
    $t(x)$ is pixel location. The subscript “$rec$” denotes “reconstructed”, such
    as $I_{rec}(x)$ is the reconstructed hazy image. The subscript “$pred$” refers
    to the prediction output. According to (Gandelsman et al., [2019](#bib.bib42)),
    atmospheric light may be regarded as a constant $A$ or a non-uniform matrix $A(x)$.
    In this survey, atmospheric light is uniformly denoted as $A$. In addition, many
    papers give the proposed algorithm an abbreviated name, such as GFN (Ren et al.,
    [2018a](#bib.bib112)) (gated fusion network for single image dehazing). For readability,
    this survey uses abbreviations as references to these papers. For a small amount
    of papers that do not give a name for their algorithms, we designate the abbreviated
    name according to the title of the corresponding paper.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出本调查的符号约定。除非另有说明，所有符号均有如下含义：$I(x)$ 表示有雾图像；$J(x)$ 表示无雾图像。$t(x)$ 代表传输图；$x$
    在 $I(x)$、$J(x)$ 和 $t(x)$ 中为像素位置。下标“$rec$”表示“重建”，例如 $I_{rec}(x)$ 是重建的有雾图像。下标“$pred$”表示预测输出。根据
    (Gandelsman 等，[2019](#bib.bib42))，大气光可以视为常量 $A$ 或非均匀矩阵 $A(x)$。在本调查中，大气光统一记作 $A$。此外，许多论文为其提出的算法提供了缩写名称，例如
    GFN (Ren 等，[2018a](#bib.bib112))（用于单图像去雾的门控融合网络）。为了可读性，本调查使用这些缩写作为参考。对于少量没有为其算法命名的论文，我们根据相应论文的标题指定缩写名称。
- en: 2\. Related Work
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: The commonalities of dehazing algorithms are mainly reflected in four aspects.
    First, the modeling of the network relies on physical model ASM or is completely
    based on neural networks. Second, the dataset used for training the network needs
    to contain transmission map, atmospheric light value, or paired supervision information.
    Third, the dehazing networks employ different kinds of basic modules. Fourth,
    different kinds of loss functions are used for training. Based on these four factors,
    the researchers designed a variety of effective dehazing algorithms. Thus, this
    section introduces ASM, datasets, loss functions, and network architecture modules.
    In addition, how to evaluate the dehazing results is also discussed in this section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 去雾算法的共同点主要体现在四个方面。首先，网络建模依赖于物理模型 ASM 或完全基于神经网络。其次，用于训练网络的数据集需要包含传输图、大气光值或配对监督信息。第三，去雾网络采用不同类型的基本模块。第四，使用不同类型的损失函数进行训练。基于这四个因素，研究人员设计了多种有效的去雾算法。因此，本节介绍了
    ASM、数据集、损失函数和网络架构模块。此外，本节还讨论了如何评估去雾结果。
- en: 2.1\. Modeling of the Dehazing Process
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 去雾过程建模
- en: Haze is a natural phenomenon that can be approximately explained by ASM. McCartney (McCartney,
    [1976](#bib.bib94)) first proposed the basic ASM to describe the principles of
    haze formation. Then, Narasimhan (Narasimhan and Nayar, [2003](#bib.bib102)) and
    Nayar (Nayar and Narasimhan, [1999](#bib.bib103)) extended and developed the ASM
    that is currently widely used. The ASM provides a reliable theoretical basis for
    the research of image dehazing. Its formula is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 雾霾是一种自然现象，可以通过 ASM 近似解释。McCartney（McCartney，[1976](#bib.bib94)）首次提出了基本的 ASM
    以描述雾霾形成的原理。随后，Narasimhan（Narasimhan 和 Nayar，[2003](#bib.bib102)）和 Nayar（Nayar
    和 Narasimhan，[1999](#bib.bib103)）扩展和发展了目前广泛使用的 ASM。ASM 为图像去雾研究提供了可靠的理论基础。其公式为
- en: '| (1) |  | $I(x)=J(x)t(x)+A(1-t(x)),$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $I(x)=J(x)t(x)+A(1-t(x)),$ |  |'
- en: where $x$ is the pixel location and $A$ means the global atmospheric light.
    In different papers, $A$ may be referred to as airlight or ambient light. For
    ease of understanding, $A$ is noted as atmospheric light in this survey. For the
    dehazing methods based on ASM, $A$ is usually unknown. $I(x)$ stands for the hazy
    image and $J(x)$ denotes the clear scene image. For most dehazing models, $I(x)$
    is the input and $J(x)$ is the desired output. The $t(x)$ means the medium transmission
    map which is defined as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是像素位置，$A$ 表示全球大气光。在不同的论文中，$A$ 可能被称为气光或环境光。为了便于理解，在本调查中 $A$ 记作大气光。对于基于
    ASM 的去雾方法，$A$ 通常是未知的。$I(x)$ 代表有雾图像，$J(x)$ 表示清晰场景图像。对于大多数去雾模型，$I(x)$ 是输入，$J(x)$
    是期望输出。$t(x)$ 表示介质传输图，其定义为
- en: '| (2) |  | $t(x)=e^{-\beta{d(x)}},$ |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $t(x)=e^{-\beta{d(x)}},$ |  |'
- en: 'where $\beta$ and $d(x)$ stands for the atmosphere scattering parameter and
    the depth of $I(x)$, respectively. Thus, the $t(x)$ is determined by $d(x)$, which
    can be used for the synthesis of hazy image. If the $t(x)$ and $A$ can be estimated,
    the haze-free image $J(x)$ can be obtained by the following formula:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 和 $d(x)$ 分别表示大气散射参数和 $I(x)$ 的深度。因此，$t(x)$ 由 $d(x)$ 决定，可用于合成雾霾图像。如果可以估计
    $t(x)$ 和 $A$，则可以通过以下公式获得无雾图像 $J(x)$：
- en: '| (3) |  | $J(x)=\frac{I(x)-A(1-t(x))}{t(x)}.$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $J(x)=\frac{I(x)-A(1-t(x))}{t(x)}.$ |  |'
- en: The imaging principle of ASM is shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning").
    It can be seen that the light reaching the camera from the object is affected
    by the particles in the air. Some works use ASM to describe the formation process
    of haze, and the parameters included in the atmospheric scattering model are solved
    in an explicit or implicit way. As shown in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"),
    ASM has a profound impact on dehazing research, including supervised (Ren et al.,
    [2016](#bib.bib111); Cai et al., [2016](#bib.bib13); Li et al., [2017a](#bib.bib73);
    Zhang and Patel, [2018](#bib.bib164)), semi-supervised (Li et al., [2020a](#bib.bib79);
    Liu et al., [2021](#bib.bib93); Chen et al., [2021b](#bib.bib21)), and unsupervised
    algorithms (Li et al., [2021a](#bib.bib71); Golts et al., [2020](#bib.bib44)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ASM 的成像原理如图 [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") 所示。可以看到，从物体到达相机的光线受到空气中颗粒的影响。一些工作使用
    ASM 描述雾霾的形成过程，并以显式或隐式的方式求解大气散射模型中的参数。如表 [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    所示，ASM 对去雾研究有深远的影响，包括监督式 (Ren et al., [2016](#bib.bib111); Cai et al., [2016](#bib.bib13);
    Li et al., [2017a](#bib.bib73); Zhang and Patel, [2018](#bib.bib164))，半监督式 (Li
    et al., [2020a](#bib.bib79); Liu et al., [2021](#bib.bib93); Chen et al., [2021b](#bib.bib21))，以及无监督算法 (Li
    et al., [2021a](#bib.bib71); Golts et al., [2020](#bib.bib44))。
- en: 2.2\. Datasets for Dehazing Task
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 去雾任务的数据集
- en: Table 2\. Datasets for image dehazing task. Syn stands for synthetic hazy images.
    HG denotes the hazy images generated from a haze generator. Real means real world
    scenes. S&R denotes Syn&Real. I and O denotes indoor and outdoor, respectively.
    P and NP means pair and non-pair, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 图像去雾任务的数据集。Syn 代表合成的雾霾图像。HG 表示从雾霾生成器生成的雾霾图像。Real 代表真实世界场景。S&R 代表 Syn&Real。I
    和 O 分别表示室内和室外。P 和 NP 分别表示配对和非配对。
- en: '| Dataset | type | Nums | I/O | P/NP |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 数量 | I/O | P/NP |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| D-HAZY (Ancuti et al., [2016](#bib.bib3)) | Syn | 1400+ | I | P |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| D-HAZY (Ancuti et al., [2016](#bib.bib3)) | Syn | 1400+ | I | P |'
- en: '| HazeRD (Zhang et al., [2017b](#bib.bib185)) | Syn | 15 | O | P |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| HazeRD (Zhang et al., [2017b](#bib.bib185)) | Syn | 15 | O | P |'
- en: '| I-HAZE (Ancuti et al., [2018b](#bib.bib5)) | HG | 35 | I | P |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| I-HAZE (Ancuti et al., [2018b](#bib.bib5)) | HG | 35 | I | P |'
- en: '| O-HAZE (Ancuti et al., [2018c](#bib.bib8)) | HG | 45 | O | P |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| O-HAZE (Ancuti et al., [2018c](#bib.bib8)) | HG | 45 | O | P |'
- en: '| RESIDE (Li et al., [2019c](#bib.bib74)) | S&R | 10000+ | I&O | P&NP |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RESIDE (Li et al., [2019c](#bib.bib74)) | S&R | 10000+ | I&O | P&NP |'
- en: '| Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) | HG | 33 | O | P |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) | HG | 33 | O | P |'
- en: '| NH-HAZE (Ancuti et al., [2020a](#bib.bib7)) | HG | 55 | O | P |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| NH-HAZE (Ancuti et al., [2020a](#bib.bib7)) | HG | 55 | O | P |'
- en: '| MRFID (Liu et al., [2020a](#bib.bib90)) | Real | 200 | O | P |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MRFID (Liu et al., [2020a](#bib.bib90)) | Real | 200 | O | P |'
- en: '| BeDDE (Zhao et al., [2020](#bib.bib188)) | Real | 200+ | O | P |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| BeDDE (Zhao et al., [2020](#bib.bib188)) | Real | 200+ | O | P |'
- en: '| 4KID (Zheng et al., [2021](#bib.bib190)) | Syn | 10000 | O | P |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4KID (Zheng et al., [2021](#bib.bib190)) | Syn | 10000 | O | P |'
- en: 'For computer vision tasks such as object detection, image segmentation, and
    image classification, accurate ground-truth labels can be obtained with careful
    annotation. However, sharp, accurate and pixel-wise labels (i.e., paired haze-free
    images) for hazy images in natural scenes are almost impossible to obtain. Currently,
    there are mainly two approaches for obtaining paired hazy and haze-free images.
    The first way is to obtain synthetic data with the help of ASM, such as the D-HAZY (Ancuti
    et al., [2016](#bib.bib3)), HazeRD (Zhang et al., [2017b](#bib.bib185)) and RESIDE (Li
    et al., [2019c](#bib.bib74)). By selecting different parameters for ASM, researchers
    can easily obtain hazy images with different haze densities. Four components are
    needed to synthesize a hazy image: a clear image, a depth map $d(x)$ corresponding
    to the content of the clear image, atmospheric light $A$ and atmosphere scattering
    parameter $\beta$. Thus, we can divide the synthetic dataset into two stages.
    In the first stage, clear images and corresponding depth maps are needed to be
    collected in pairs. In order to ensure that the synthesized haze is as close as
    possible to the real world haze, the depth information must be sufficiently accurate.
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the clear image and corresponding depth map in the NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)). In the second stage, atmospheric light $A$ and atmosphere
    scattering parameter $\beta$ are designated as fixed values or randomly selected.
    The commonly used D-HAZY (Ancuti et al., [2016](#bib.bib3)) dataset is synthesized
    when both $A$ and $\beta$ are $1$. Several researches choose different $A$ and
    $\beta$ in order to increase the diversity of the synthesized images and thus
    improve the generalization ability of the trained model. For example, MSCNN (Ren
    et al., [2016](#bib.bib111)) sets $A\in(0.7,1.0)$ and $\beta\in(0.5,1.5)$. Fig.
    [4](#S2.F4 "Figure 4 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the corresponding hazy image when $\beta$ takes $6$ different values.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机视觉任务，如目标检测、图像分割和图像分类，准确的真实标签可以通过仔细标注获得。然而，对于自然场景中的雾霾图像，几乎不可能获得清晰、准确且逐像素的标签（即配对的无雾图像）。目前，主要有两种方法来获得配对的雾霾和无雾图像。第一种方法是借助ASM获得合成数据，如D-HAZY（Ancuti
    et al., [2016](#bib.bib3)）、HazeRD（Zhang et al., [2017b](#bib.bib185)）和RESIDE（Li
    et al., [2019c](#bib.bib74)）。通过选择不同的ASM参数，研究人员可以轻松获得具有不同雾霾密度的图像。合成雾霾图像需要四个组件：清晰图像、与清晰图像内容对应的深度图$d(x)$、大气光$A$和大气散射参数$\beta$。因此，我们可以将合成数据集分为两个阶段。在第一阶段，需要成对收集清晰图像及其对应的深度图。为了确保合成的雾霾尽可能接近现实世界的雾霾，深度信息必须足够准确。图[3](#S2.F3
    "Figure 3 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")展示了NYU-Depth
    数据集中清晰图像及其对应的深度图（Silberman et al., [2012](#bib.bib125)）。在第二阶段，大气光$A$和大气散射参数$\beta$被指定为固定值或随机选择。常用的D-HAZY（Ancuti
    et al., [2016](#bib.bib3)）数据集是在$A$和$\beta$均为$1$时合成的。若干研究选择不同的$A$和$\beta$以增加合成图像的多样性，从而提高训练模型的泛化能力。例如，MSCNN（Ren
    et al., [2016](#bib.bib111)）将$A\in(0.7,1.0)$和$\beta\in(0.5,1.5)$。图[4](#S2.F4 "Figure
    4 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")展示了当$\beta$取$6$个不同值时的对应雾霾图像。
- en: '![Refer to caption](img/60136141ecd63dd2e71a51ede116fec1.png)![Refer to caption](img/e41d2dfc40739fce93d6721617f524dd.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60136141ecd63dd2e71a51ede116fec1.png)![参见说明](img/e41d2dfc40739fce93d6721617f524dd.png)'
- en: (a) A clear image       (b) Depth for the clear image
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 清晰图像       (b) 清晰图像的深度
- en: Figure 3\. A clear image and corresponding depth map in NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. NYU-Depth 数据集中清晰图像及对应的深度图（Silberman et al., [2012](#bib.bib125)）。
- en: '![Refer to caption](img/289611583db612323c711b7b83db8a21.png)![Refer to caption](img/f779c97fa911a7a57446c982c83b1f81.png)![Refer
    to caption](img/33c380ec6c011ec9104f9d6cd80d0ff5.png)![Refer to caption](img/09c95952de08c447a55508d1f2f46b53.png)![Refer
    to caption](img/57e98b1225188fd083f91867742a762a.png)![Refer to caption](img/316ebd886e9811388ed98d6cf6f33776.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/289611583db612323c711b7b83db8a21.png)![参见说明](img/f779c97fa911a7a57446c982c83b1f81.png)![参见说明](img/33c380ec6c011ec9104f9d6cd80d0ff5.png)![参见说明](img/09c95952de08c447a55508d1f2f46b53.png)![参见说明](img/57e98b1225188fd083f91867742a762a.png)![参见说明](img/316ebd886e9811388ed98d6cf6f33776.png)'
- en: Figure 4\. Synthesized hazy images of different densities by setting different
    values for the atmosphere scattering parameter $\beta$ based on NYU-Depth dataset
    (Silberman et al., [2012](#bib.bib125)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 通过设置不同的大气散射参数 $\beta$，基于 NYU-Depth 数据集 (Silberman 等人，[2012](#bib.bib125))
    合成的不同密度的雾霾图像。
- en: The second way is to generate the hazy image by using a haze generator, such
    as I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti et al., [2018c](#bib.bib8)),
    Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
    The well-known competition New Trends in Image Restoration and Enhance (NTIRE
    2018-2020) dehazing challenge  (Ancuti et al., [2018a](#bib.bib4), [2019b](#bib.bib9),
    [2020b](#bib.bib10)) are based on these generated datasets. Fig. [5](#S2.F5 "Figure
    5 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows four pairs
    of hazy and haze-free examples contained in the datasets simulated by the haze
    generator. The images in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2\. Datasets for Dehazing
    Task ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") (a) are from indoor scenes, while (b), (c) and
    (d) are all pictured at outdoor views. There are differences in the pattern of
    haze in these three outdoor datasets. The haze in (b) and (c) is evenly distributed
    throughout the entire image, while the haze in (d) are non-homogeneous in the
    whole scenes. In addition, the density of haze in (c) is significantly higher
    than that in (b) and (d). These datasets with different characteristics provide
    useful insights for the design of dehazing algorithms. For example, in order to
    remove the high density of haze in Dense-Haze, it is necessary to design dehazing
    models with stronger feature extraction and recovery capabilities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是使用雾生成器生成雾霾图像，例如 I-HAZE (Ancuti 等人，[2018b](#bib.bib5))、O-HAZE (Ancuti 等人，[2018c](#bib.bib8))、Dense-Haze
    (Ancuti 等人，[2019a](#bib.bib6)) 和 NH-HAZE (Ancuti 等人，[2020a](#bib.bib7))。著名的“新趋势图像修复与增强”（NTIRE
    2018-2020）去雾挑战赛 (Ancuti 等人，[2018a](#bib.bib4)、[2019b](#bib.bib9)、[2020b](#bib.bib10))
    基于这些生成的数据集。图 [5](#S2.F5 "图5 ‣ 2.2\. 去雾任务的数据集 ‣ 2\. 相关工作 ‣ 基于深度学习的单图像去雾的综合调查和分类")
    显示了四对包含在由雾生成器模拟的数据集中的雾霾和无雾霾示例。图 [5](#S2.F5 "图5 ‣ 2.2\. 去雾任务的数据集 ‣ 2\. 相关工作 ‣ 基于深度学习的单图像去雾的综合调查和分类")
    中的图像 (a) 来自室内场景，而 (b)、(c) 和 (d) 都是户外视图。这三种户外数据集的雾霾模式存在差异。图像 (b) 和 (c) 的雾霾均匀分布在整个图像中，而
    (d) 中的雾霾则在整个场景中不均匀。此外，图像 (c) 中的雾霾密度显著高于 (b) 和 (d)。这些具有不同特征的数据集为去雾算法的设计提供了有用的见解。例如，为了去除
    Dense-Haze 中的高密度雾霾，需要设计具有更强特征提取和恢复能力的去雾模型。
- en: '![Refer to caption](img/0a67f29b9c7336e6502f2312160584d5.png)![Refer to caption](img/e89637664c4362468ae836fd15460f09.png)![Refer
    to caption](img/23ee74b6da9b40e50dc696f12b44dcf8.png)![Refer to caption](img/12a9d3b98e907b5bbbbf7b77ddeff179.png)![Refer
    to caption](img/00c58a42576e10d7fc02f256cf87990e.png)![Refer to caption](img/5bec5c0b347541aa68e504234a21dc28.png)![Refer
    to caption](img/c976c8c9abad588ceb996b8f311bcee9.png)![Refer to caption](img/bce4a4be864141dde51642ecac05e4b9.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0a67f29b9c7336e6502f2312160584d5.png)![参见说明](img/e89637664c4362468ae836fd15460f09.png)![参见说明](img/23ee74b6da9b40e50dc696f12b44dcf8.png)![参见说明](img/12a9d3b98e907b5bbbbf7b77ddeff179.png)![参见说明](img/00c58a42576e10d7fc02f256cf87990e.png)![参见说明](img/5bec5c0b347541aa68e504234a21dc28.png)![参见说明](img/c976c8c9abad588ceb996b8f311bcee9.png)![参见说明](img/bce4a4be864141dde51642ecac05e4b9.png)'
- en: (a) Indoor Haze             (b) Outdoor Haze         (c) Dense Haze      (d)
    Non-Homogeneous Haze
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 室内雾霾             (b) 户外雾霾         (c) 密集雾霾      (d) 非均匀雾霾
- en: Figure 5\. Examples from I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti
    et al., [2018c](#bib.bib8)), Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and
    NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 例图来自 I-HAZE (Ancuti 等人，[2018b](#bib.bib5))、O-HAZE (Ancuti 等人，[2018c](#bib.bib8))、Dense-Haze
    (Ancuti 等人，[2019a](#bib.bib6)) 和 NH-HAZE (Ancuti 等人，[2020a](#bib.bib7))。
- en: The main advantage of synthetic and generated haze is that it alleviates the
    difficulty during data acquisition. However, the hazy images synthesized based
    on ASM or generated by haze generator cannot perfectly simulate the formation
    process of real world haze. Therefore, there is an inherent difference between
    the synthetic and real world data. Several researches have noticed the problems
    of artificial data and tried to construct real world datasets, such as MRFID (Liu
    et al., [2020a](#bib.bib90)) and BeDDE (Zhao et al., [2020](#bib.bib188)). However,
    due to the high costs and difficulties of data collection, the current real world
    datasets do not contain enough examples as the synthetic dataset, like RESIDE (Li
    et al., [2019c](#bib.bib74)). To facilitate the comparison of different datasets,
    we summarize the characteristics of various datasets in Table [2](#S2.T2 "Table
    2 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning").
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 合成和生成的雾的主要优点是减轻了数据采集过程中的困难。然而，基于ASM合成的雾图像或由雾生成器生成的雾图像无法完美模拟现实世界雾的形成过程。因此，合成数据和真实世界数据之间存在固有差异。一些研究已注意到人工数据的问题，并尝试构建真实世界的数据集，如MRFID（Liu
    et al., [2020a](#bib.bib90)）和BeDDE（Zhao et al., [2020](#bib.bib188)）。然而，由于数据收集的高成本和困难，当前的真实世界数据集无法包含像合成数据集那样的足够多的例子，例如RESIDE（Li
    et al., [2019c](#bib.bib74)）。为了便于不同数据集的比较，我们在表[2](#S2.T2 "Table 2 ‣ 2.2. Datasets
    for Dehazing Task ‣ 2. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")中总结了各种数据集的特点。
- en: 2.3\. Network Block
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 网络块
- en: CNNs are widely used in current deep learning based dehazing networks. Commonly
    adopted modules are standard convolution, dilated convolution, multi-scale fusion,
    feature pyramid, cross-layer connection and attention. Usually, multiple basic
    blocks are formed into a dehazing network. In order to facilitate the understanding
    of the principles of different dehazing algorithms, the basic blocks commonly
    used in network architectures are summarized as follows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs在当前基于深度学习的去雾网络中被广泛使用。常用的模块包括标准卷积、扩张卷积、多尺度融合、特征金字塔、跨层连接和注意力机制。通常，多个基本块被组成一个去雾网络。为了便于理解不同去雾算法的原理，总结了网络架构中常用的基本块如下。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Standard convolution: It is shown that using standard convolution in a sequential
    connection way to build neural networks is effective. Therefore, standard convolution
    are often used in dehazing models (Li et al., [2017a](#bib.bib73); Ren et al.,
    [2016](#bib.bib111); Sharma et al., [2020](#bib.bib121); Zhang et al., [2020e](#bib.bib184))
    together with other blocks.'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准卷积：研究表明，使用标准卷积以顺序连接的方式构建神经网络是有效的。因此，标准卷积通常与其他块一起用于去雾模型（Li et al., [2017a](#bib.bib73);
    Ren et al., [2016](#bib.bib111); Sharma et al., [2020](#bib.bib121); Zhang et
    al., [2020e](#bib.bib184)）。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dilated convolution: Dilated convolution can increase the receptive field while
    keeping the size of the convolution kernel unchanged. Studies (Chen et al., [2019c](#bib.bib14);
    Zhang et al., [2020c](#bib.bib176); Zhang and He, [2020](#bib.bib174); Lee et al.,
    [2020b](#bib.bib70); Yan et al., [2020](#bib.bib153)) have shown that dilated
    convolution can improve the performance of global feature extraction. Moreover,
    fusing convolution layers with different dilation rates can extract features from
    different receptive fields.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩张卷积：扩张卷积可以在保持卷积核大小不变的情况下增加感受野。研究（Chen et al., [2019c](#bib.bib14); Zhang et
    al., [2020c](#bib.bib176); Zhang and He, [2020](#bib.bib174); Lee et al., [2020b](#bib.bib70);
    Yan et al., [2020](#bib.bib153)）表明，扩张卷积可以提高全局特征提取的性能。此外，融合不同扩张率的卷积层可以从不同的感受野中提取特征。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-scale fusion: CNNs with multi-scale convolution kernels have been proven
    effective in extracting features in a variety of visual tasks (Szegedy et al.,
    [2015](#bib.bib131)). By using convolution kernels at different scales and fusing
    the extracted feature together, dehazing methods (Wang et al., [2018a](#bib.bib133);
    Tang et al., [2019](#bib.bib132); Dudhane et al., [2019](#bib.bib37); Wang et al.,
    [2020](#bib.bib134)) have demonstrated that the fusion strategy can obtain the
    multi-scale details that are useful for image restoration. In the process of feature
    fusion, a common way is to spatially concatenate or add output features obtained
    by convolution kernels of different sizes.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多尺度融合：具有多尺度卷积核的卷积神经网络（CNN）在各种视觉任务中的特征提取上已被证明有效（Szegedy 等人，[2015](#bib.bib131)）。通过使用不同尺度的卷积核并融合提取的特征，去雾方法（Wang
    等人，[2018a](#bib.bib133)；Tang 等人，[2019](#bib.bib132)；Dudhane 等人，[2019](#bib.bib37)；Wang
    等人，[2020](#bib.bib134)）已经证明融合策略可以获得对图像恢复有用的多尺度细节。在特征融合过程中，一种常见的方法是空间上连接或添加由不同大小卷积核获得的输出特征。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature pyramid: In the research of digital image processing, the image pyramid
    can be used to obtain information of different resolutions. The dehazing network
    based on deep learning (Zhang et al., [2020e](#bib.bib184); Zhang and Patel, [2018](#bib.bib164);
    Zhang et al., [2018b](#bib.bib165); Singh et al., [2020](#bib.bib128); Zhao et al.,
    [2021a](#bib.bib187); Yin et al., [2020](#bib.bib160); Chen et al., [2019a](#bib.bib16))
    uses this strategy in the middle layer of the network to extract multiple scales
    of space and channel information.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征金字塔：在数字图像处理的研究中，图像金字塔可以用来获取不同分辨率的信息。基于深度学习的去雾网络（Zhang 等人，[2020e](#bib.bib184)；Zhang
    和 Patel，[2018](#bib.bib164)；Zhang 等人，[2018b](#bib.bib165)；Singh 等人，[2020](#bib.bib128)；Zhao
    等人，[2021a](#bib.bib187)；Yin 等人，[2020](#bib.bib160)；Chen 等人，[2019a](#bib.bib16)）在网络的中间层使用了这种策略来提取多尺度的空间和通道信息。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-layer connection: In order to enhance the information exchange between
    different layers and improve the feature extraction ability of the network, cross-layer
    connections are often used in CNNs. There are mainly three types of cross-layer
    connections used in dehazing networks, which are residual connection (Zhang et al.,
    [2020g](#bib.bib186); Qu et al., [2019](#bib.bib110); Hong et al., [2020](#bib.bib55);
    Liang et al., [2019](#bib.bib85); Chen et al., [2019d](#bib.bib20)) proposed by
    ResNet (He et al., [2016](#bib.bib53)), dense connection (Zhu et al., [2018](#bib.bib192);
    Zhang et al., [2022a](#bib.bib178); Dong et al., [2020a](#bib.bib32); Chen and
    Lai, [2019](#bib.bib15); Guo et al., [2019a](#bib.bib48); Li et al., [2019a](#bib.bib75))
    designed by DenseNet (Huang et al., [2017](#bib.bib56)), and skip connection (Zhao
    et al., [2021a](#bib.bib187); Dudhane et al., [2019](#bib.bib37); Yang and Zhang,
    [2022](#bib.bib155); Lee et al., [2020b](#bib.bib70)) inspired by U-Net (Ronneberger
    et al., [2015](#bib.bib115)).'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨层连接：为了增强不同层之间的信息交换并提高网络的特征提取能力，卷积神经网络中常常使用跨层连接。在去雾网络中主要有三种跨层连接类型，分别是残差连接（Zhang
    等人，[2020g](#bib.bib186)；Qu 等人，[2019](#bib.bib110)；Hong 等人，[2020](#bib.bib55)；Liang
    等人，[2019](#bib.bib85)；Chen 等人，[2019d](#bib.bib20)）由 ResNet（He 等人，[2016](#bib.bib53)）提出，密集连接（Zhu
    等人，[2018](#bib.bib192)；Zhang 等人，[2022a](#bib.bib178)；Dong 等人，[2020a](#bib.bib32)；Chen
    和 Lai，[2019](#bib.bib15)；Guo 等人，[2019a](#bib.bib48)；Li 等人，[2019a](#bib.bib75)）由
    DenseNet（Huang 等人，[2017](#bib.bib56)）设计，以及跳跃连接（Zhao 等人，[2021a](#bib.bib187)；Dudhane
    等人，[2019](#bib.bib37)；Yang 和 Zhang，[2022](#bib.bib155)；Lee 等人，[2020b](#bib.bib70)）受到
    U-Net（Ronneberger 等人，[2015](#bib.bib115)）的启发。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention in dehazing: The attention mechanism has been successfully applied
    in the research of natural language processing. Commonly used attention blocks
    in computer vision include channel attention and spatial attention. For the feature
    extraction and reconstruction process of 2D image, channel attention can emphasize
    the useful channels of the feature map. This unequal feature map processing strategy
    allows the model to focus more on effective feature information. The spatial attention
    mechanism focuses on the differences in the internal location regions of the feature
    map, such as the distribution of haze on the entire map. By embedding the attention
    module in the network, several dehazing methods (Liang et al., [2019](#bib.bib85);
    Chen et al., [2019d](#bib.bib20); Liu et al., [2019a](#bib.bib91); Qin et al.,
    [2020](#bib.bib109); Yin et al., [2020](#bib.bib160); Lee et al., [2020b](#bib.bib70);
    Yan et al., [2020](#bib.bib153); Dong et al., [2020c](#bib.bib30); Yan et al.,
    [2020](#bib.bib153); Zhang et al., [2020e](#bib.bib184); Yin et al., [2021](#bib.bib161);
    Metwaly et al., [2020](#bib.bib97); Wang et al., [2021d](#bib.bib136)) have achieved
    excellent dehazing performance.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去雾中的注意力机制：注意力机制已经成功应用于自然语言处理的研究中。在计算机视觉中，常用的注意力模块包括通道注意力和空间注意力。在二维图像的特征提取和重建过程中，通道注意力可以强调特征图的有用通道。这种不均等的特征图处理策略使模型能够更加关注有效的特征信息。空间注意力机制则关注特征图内部位置区域的差异，如整个图像上的雾霾分布。通过在网络中嵌入注意力模块，一些去雾方法（Liang等，[2019](#bib.bib85)；Chen等，[2019d](#bib.bib20)；Liu等，[2019a](#bib.bib91)；Qin等，[2020](#bib.bib109)；Yin等，[2020](#bib.bib160)；Lee等，[2020b](#bib.bib70)；Yan等，[2020](#bib.bib153)；Dong等，[2020c](#bib.bib30)；Yan等，[2020](#bib.bib153)；Zhang等，[2020e](#bib.bib184)；Yin等，[2021](#bib.bib161)；Metwaly等，[2020](#bib.bib97)；Wang等，[2021d](#bib.bib136)）已取得了优异的去雾效果。
- en: 2.4\. Loss Function
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 损失函数
- en: This section introduces the commonly adopted loss functions in supervised, semi-supervised
    and unsupervised dehazing models, which can be used for transmission map estimation,
    clear image prediction, hazy image reconstruction, atmospheric light regression,
    etc. Several algorithms use multiple losses in combination to obtain better dehazing
    performance. A detailed classification and summary of the loss functions used
    by different dehazing methods is presented in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\.
    Total Variation Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"). In the
    loss function introduced below, $X$ and $Y$ denote the predicted value and ground
    truth value, respectively.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在监督式、半监督式和无监督式去雾模型中常用的损失函数，这些函数可用于传输图估计、清晰图像预测、雾霾图像重建、大气光回归等。一些算法通过组合多个损失函数来获得更好的去雾性能。不同去雾方法使用的损失函数的详细分类和总结见表[3](#S2.T3
    "表 3 ‣ 2.4.5\. 总变差损失 ‣ 2.4\. 损失函数 ‣ 2\. 相关工作 ‣ 基于深度学习的单幅图像去雾的全面调查和分类")。在下文介绍的损失函数中，$X$和$Y$分别表示预测值和真实值。
- en: 2.4.1\. Fidelity Loss
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 保真度损失
- en: 'The widely used pixel-wise loss functions for dehazing research are L1 loss
    and L2 loss, which are defined as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛用于去雾研究的逐像素损失函数有L1损失和L2损失，其定义如下：
- en: '| (4) |  | $L1=&#124;&#124;X-Y&#124;&#124;_{1}.$ |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $L1=&#124;&#124;X-Y&#124;&#124;_{1}.$ |  |'
- en: '| (5) |  | $L2=&#124;&#124;X-Y&#124;&#124;_{2}.$ |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $L2=&#124;&#124;X-Y&#124;&#124;_{2}.$ |  |'
- en: 2.4.2\. Perceptual Loss
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 感知损失
- en: The research on image super resolution (Ledig et al., [2017](#bib.bib68); Wang
    et al., [2018b](#bib.bib140)) and style transfer (Johnson et al., [2016](#bib.bib64))
    indicates that the attributes of the human visual system in the process of perceptual
    evaluation is not fully reflected by L1 or L2 loss. Meanwhile, L2 loss may lead
    to over-smooth outputs (Ledig et al., [2017](#bib.bib68)). Recent researches use
    pre-trained classification neural networks to calculate the perceptual loss in
    the feature space. The most commonly used pre-training model is VGG (Simonyan
    and Zisserman, [2015](#bib.bib127)), and some of its layers are used to calculate
    the distance between the predicted image and the reference image in the feature
    space, i.e.,
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像超分辨率的研究（Ledig 等，[2017](#bib.bib68)；Wang 等，[2018b](#bib.bib140)）和风格迁移（Johnson
    等，[2016](#bib.bib64)）表明，人类视觉系统在感知评估过程中的属性并不能完全通过 L1 或 L2 损失来反映。同时，L2 损失可能导致过度平滑的输出（Ledig
    等，[2017](#bib.bib68)）。最近的研究使用预训练的分类神经网络来计算特征空间中的感知损失。最常用的预训练模型是 VGG（Simonyan 和
    Zisserman，[2015](#bib.bib127)），其某些层被用来计算预测图像和参考图像在特征空间中的距离，即，
- en: '| (6) |  | $L_{per}(X,Y)=\sum_{i=1}^{N}&#124;&#124;\psi_{i}(X)-\psi_{i}(Y)&#124;&#124;_{2},$
    |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $L_{per}(X,Y)=\sum_{i=1}^{N}&#124;&#124;\psi_{i}(X)-\psi_{i}(Y)&#124;&#124;_{2},$
    |  |'
- en: where $N$ represents the number of features selected for calculation; $i$ means
    the index of feature map; $\psi(\cdot)$ denotes the pretrained VGG. During the
    calculation of perceptual loss and network optimization, the parameters of VGG
    are always frozen.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 代表用于计算的特征数量；$i$ 表示特征图的索引；$\psi(\cdot)$ 表示预训练的 VGG。在计算感知损失和网络优化过程中，VGG
    的参数始终被冻结。
- en: 2.4.3\. Structure Loss
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 结构损失
- en: As a metric of the dehazing methods, Structural Similarity (SSIM) (Wang et al.,
    [2004](#bib.bib144)) is also used as a loss function in the optimization process.
    Studies (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162)) have
    shown that SSIM loss can improve the structural similarity during image restoration.
    MS-SSIM (Wang et al., [2003](#bib.bib145)) introduces multi-scale evaluation into
    SSIM, which is also used as a loss function by the dehazing algorithms, i.e.,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作为去雾方法的度量，结构相似性（SSIM）（Wang 等，[2004](#bib.bib144)）也在优化过程中用作损失函数。研究（Dong 等，[2020a](#bib.bib32)；Yu
    等，[2020](#bib.bib162)）表明，SSIM 损失可以在图像恢复过程中提高结构相似性。MS-SSIM（Wang 等，[2003](#bib.bib145)）将多尺度评估引入
    SSIM，这也被去雾算法用作损失函数，即，
- en: '| (7) |  | $L_{ssim}=1-SSIM(X,Y),$ |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $L_{ssim}=1-SSIM(X,Y),$ |  |'
- en: '| (8) |  | $L_{msssim}=1-MSSSIM(X,Y).$ |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $L_{msssim}=1-MSSSIM(X,Y).$ |  |'
- en: 2.4.4\. Gradient Loss
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4\. 梯度损失
- en: 'Gradient loss, also known as edge loss, is used to better restore the contour
    and edge information of the haze-free image. The edge extraction can be implemented
    as Laplacian operator, Canny operator, and so on. For example, SA-CGAN  (Sharma
    et al., [2020](#bib.bib121)) uses the Laplacian of Gaussian with standard deviation
    $\sigma$ to perform quadratic differentiation on the two-dimensional image $F$:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度损失，也称为边缘损失，用于更好地恢复无雾图像的轮廓和边缘信息。边缘提取可以通过拉普拉斯算子、Canny 算子等来实现。例如，SA-CGAN（Sharma
    等，[2020](#bib.bib121)）使用标准差为 $\sigma$ 的高斯拉普拉斯算子对二维图像 $F$ 进行二次微分：
- en: '| (9) |  | $\displaystyle L(m,n)$ | $\displaystyle=\bigtriangledown^{2}F(m,n)=\frac{\partial^{2}F}{\partial{m^{2}}}+\frac{\partial^{2}F}{\partial{n^{2}}}=-\frac{1}{\pi{\sigma^{4}}}[1-\frac{m^{2}+n^{2}}{2\sigma^{2}}]\exp{(-\frac{m^{2}+n^{2}}{2\sigma^{2}})},$
    |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle L(m,n)$ | $\displaystyle=\bigtriangledown^{2}F(m,n)=\frac{\partial^{2}F}{\partial{m^{2}}}+\frac{\partial^{2}F}{\partial{n^{2}}}=-\frac{1}{\pi{\sigma^{4}}}[1-\frac{m^{2}+n^{2}}{2\sigma^{2}}]\exp{(-\frac{m^{2}+n^{2}}{2\sigma^{2}})},$
    |  |'
- en: where $(m,n)$ means pixel location and $L(m,n)$ is calculated for both $X$ and
    $Y$, respectively. Then, regression objective functions such as L1 and L2 are
    used for the calculation of gradient loss.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(m,n)$ 表示像素位置，$L(m,n)$ 分别对 $X$ 和 $Y$ 进行计算。然后，L1 和 L2 等回归目标函数用于计算梯度损失。
- en: 2.4.5\. Total Variation Loss
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.5\. 总变差损失
- en: 'Total variation (TV) loss (Rudin et al., [1992](#bib.bib116)) can be used to
    smooth image and remove noise. The training objective is to minimize the following
    function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差（TV）损失（Rudin 等，[1992](#bib.bib116)）可以用来平滑图像并去除噪声。训练目标是最小化以下函数：
- en: '| (10) |  | $L_{TV}=&#124;&#124;\partial_{m}{X}&#124;&#124;_{1}+&#124;&#124;\partial_{n}{X}&#124;&#124;_{1},$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $L_{TV}=&#124;&#124;\partial_{m}{X}&#124;&#124;_{1}+&#124;&#124;\partial_{n}{X}&#124;&#124;_{1},$
    |  |'
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    It can be seen from the formula that the TV loss can be added to networks trained
    in an unsupervised manner without using ground-truth $Y$.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别表示水平和垂直坐标。从公式中可以看出，TV 损失可以添加到以无监督方式训练的网络中，而无需使用真实值 $Y$。
- en: Table 3\. Loss functions for dehazing task
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 去雾任务的损失函数
- en: '| Loss Function | Algorithms |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | 算法 |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| L1 |  (Mondal et al., [2018](#bib.bib100); Chen et al., [2019b](#bib.bib18);
    Deng et al., [2019](#bib.bib27); Liang et al., [2019](#bib.bib85); Yin et al.,
    [2019](#bib.bib159); Dong and Pan, [2020](#bib.bib31); Chen et al., [2020](#bib.bib19);
    Yan et al., [2020](#bib.bib153); Qin et al., [2020](#bib.bib109); Hong et al.,
    [2020](#bib.bib55); Li et al., [2020d](#bib.bib81); Zhang et al., [2020b](#bib.bib175);
    Zhao et al., [2021a](#bib.bib187); Park et al., [2020](#bib.bib107); Shin et al.,
    [2022](#bib.bib123); Zhang et al., [2022a](#bib.bib178)) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| L1 |  (Mondal 等, [2018](#bib.bib100); Chen 等, [2019b](#bib.bib18); Deng 等,
    [2019](#bib.bib27); Liang 等, [2019](#bib.bib85); Yin 等, [2019](#bib.bib159); Dong
    和 Pan, [2020](#bib.bib31); Chen 等, [2020](#bib.bib19); Yan 等, [2020](#bib.bib153);
    Qin 等, [2020](#bib.bib109); Hong 等, [2020](#bib.bib55); Li 等, [2020d](#bib.bib81);
    Zhang 等, [2020b](#bib.bib175); Zhao 等, [2021a](#bib.bib187); Park 等, [2020](#bib.bib107);
    Shin 等, [2022](#bib.bib123); Zhang 等, [2022a](#bib.bib178)) |'
- en: '| L2 |  (Li et al., [2017a](#bib.bib73); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2018](#bib.bib192); Zhang et al., [2018a](#bib.bib179); Guo et al.,
    [2019b](#bib.bib49); Morales et al., [2019](#bib.bib101); Chen et al., [2019d](#bib.bib20);
    Yang et al., [2019](#bib.bib154); Tang et al., [2019](#bib.bib132); Dong et al.,
    [2020b](#bib.bib29); Zhang et al., [2020d](#bib.bib177); Yin et al., [2020](#bib.bib160);
    Zhang et al., [2021b](#bib.bib183), [a](#bib.bib181), [2022c](#bib.bib182); Huang
    et al., [2021](#bib.bib58); Sheng et al., [2022](#bib.bib122)) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| L2 |  (Li 等, [2017a](#bib.bib73); Pang 等, [2018](#bib.bib105); Zhu 等, [2018](#bib.bib192);
    Zhang 等, [2018a](#bib.bib179); Guo 等, [2019b](#bib.bib49); Morales 等, [2019](#bib.bib101);
    Chen 等, [2019d](#bib.bib20); Yang 等, [2019](#bib.bib154); Tang 等, [2019](#bib.bib132);
    Dong 等, [2020b](#bib.bib29); Zhang 等, [2020d](#bib.bib177); Yin 等, [2020](#bib.bib160);
    Zhang 等, [2021b](#bib.bib183), [a](#bib.bib181), [2022c](#bib.bib182); Huang 等,
    [2021](#bib.bib58); Sheng 等, [2022](#bib.bib122)) |'
- en: '| SSIM |  (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162);
    Metwaly et al., [2020](#bib.bib97); Wei et al., [2020](#bib.bib146); Singh et al.,
    [2020](#bib.bib128); Li et al., [2020e](#bib.bib78); Jo and Sim, [2021](#bib.bib63);
    Shyam et al., [2021](#bib.bib124); Zhao et al., [2021a](#bib.bib187)) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| SSIM |  (Dong 等, [2020a](#bib.bib32); Yu 等, [2020](#bib.bib162); Metwaly
    等, [2020](#bib.bib97); Wei 等, [2020](#bib.bib146); Singh 等, [2020](#bib.bib128);
    Li 等, [2020e](#bib.bib78); Jo 和 Sim, [2021](#bib.bib63); Shyam 等, [2021](#bib.bib124);
    Zhao 等, [2021a](#bib.bib187)) |'
- en: '| MS-SSIM |  (Sun et al., [2021](#bib.bib130); Guo et al., [2019a](#bib.bib48);
    Cong et al., [2020](#bib.bib24); Yu et al., [2021](#bib.bib163); Fu et al., [2021](#bib.bib40))
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MS-SSIM |  (Sun 等, [2021](#bib.bib130); Guo 等, [2019a](#bib.bib48); Cong
    等, [2020](#bib.bib24); Yu 等, [2021](#bib.bib163); Fu 等, [2021](#bib.bib40)) |'
- en: '| Perceptual |  (Sim et al., [2018](#bib.bib126); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2021](#bib.bib191); Zhang et al., [2022c](#bib.bib182); Wang et al.,
    [2021c](#bib.bib135); Deng et al., [2020](#bib.bib26); Liu et al., [2019a](#bib.bib91);
    Hong et al., [2020](#bib.bib55); Qu et al., [2019](#bib.bib110); Chen and Lai,
    [2019](#bib.bib15); Li et al., [2019a](#bib.bib75); Chen et al., [2019d](#bib.bib20);
    Singh et al., [2020](#bib.bib128); Dong et al., [2020a](#bib.bib32); Shyam et al.,
    [2021](#bib.bib124); Engin et al., [2018](#bib.bib38)) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 感知损失 |  (Sim 等, [2018](#bib.bib126); Pang 等, [2018](#bib.bib105); Zhu 等,
    [2021](#bib.bib191); Zhang 等, [2022c](#bib.bib182); Wang 等, [2021c](#bib.bib135);
    Deng 等, [2020](#bib.bib26); Liu 等, [2019a](#bib.bib91); Hong 等, [2020](#bib.bib55);
    Qu 等, [2019](#bib.bib110); Chen 和 Lai, [2019](#bib.bib15); Li 等, [2019a](#bib.bib75);
    Chen 等, [2019d](#bib.bib20); Singh 等, [2020](#bib.bib128); Dong 等, [2020a](#bib.bib32);
    Shyam 等, [2021](#bib.bib124); Engin 等, [2018](#bib.bib38)) |'
- en: '| TV Loss |  (Das and Dutta, [2020](#bib.bib25); Li et al., [2020a](#bib.bib79);
    Shao et al., [2020](#bib.bib119); Huang et al., [2019](#bib.bib57); Wang et al.,
    [2021c](#bib.bib135); He et al., [2019](#bib.bib54)) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| TV 损失 |  (Das 和 Dutta, [2020](#bib.bib25); Li 等, [2020a](#bib.bib79); Shao
    等, [2020](#bib.bib119); Huang 等, [2019](#bib.bib57); Wang 等, [2021c](#bib.bib135);
    He 等, [2019](#bib.bib54)) |'
- en: '| Gradient |  (Zhang and Patel, [2018](#bib.bib164); Zhang et al., [2019a](#bib.bib166),
    [2020b](#bib.bib175), [2020e](#bib.bib184); Yin et al., [2020](#bib.bib160); Zhang
    et al., [2022b](#bib.bib170); Li et al., [2021c](#bib.bib80); Dudhane et al.,
    [2019](#bib.bib37); Yin et al., [2021](#bib.bib161)) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Gradient |  (Zhang和Patel，[2018](#bib.bib164)；Zhang等，[2019a](#bib.bib166)，[2020b](#bib.bib175)，[2020e](#bib.bib184)；Yin等，[2020](#bib.bib160)；Zhang等，[2022b](#bib.bib170)；Li等，[2021c](#bib.bib80)；Dudhane等，[2019](#bib.bib37)；Yin等，[2021](#bib.bib161))
    |'
- en: 2.5\. Image Quality Metrics
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 图像质量指标
- en: Due to the presence of haze, the saturation and contrast of the image are reduced,
    and color of the image is distorted by the uncertainty mode. To measure the difference
    between the dehazed image and the ground truth haze-free image, objective metrics
    are needed to evaluate the results obtained by various dehazing algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在雾霾，图像的饱和度和对比度降低，图像的颜色由于不确定性模式而失真。为了测量去雾图像与真实无雾图像之间的差异，需要使用客观指标来评估各种去雾算法得到的结果。
- en: 'Most papers use Peak Signal-to-Noise Ratio (PSNR) (Huynh-Thu and Ghanbari,
    [2008](#bib.bib60)) and SSIM (Wang et al., [2004](#bib.bib144)) to evaluate the
    image quality after dehazing. The computation of PSNR needs to use the formula
    ([11](#S2.E11 "In 2.5\. Image Quality Metrics ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")) to obtain
    the mean square error (MSE):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数论文使用峰值信噪比（PSNR） (Huynh-Thu和Ghanbari，[2008](#bib.bib60))和SSIM (Wang等，[2004](#bib.bib144))来评估去雾后的图像质量。计算PSNR需要使用公式（[11](#S2.E11
    "In 2.5\. Image Quality Metrics ‣ 2\. Related Work ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning")）来获得均方误差（MSE）：
- en: '| (11) |  | $MSE=\frac{1}{H\times{W}}\sum_{i=1}^{H}\sum_{j=1}^{W}(X(i,j)-Y(i,j))^{2},$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $MSE=\frac{1}{H\times{W}}\sum_{i=1}^{H}\sum_{j=1}^{W}(X(i,j)-Y(i,j))^{2},$
    |  |'
- en: 'where $X$ and $Y$ respectively represent two images to be evaluated. $H$ and
    $W$ are their height and width, that is, the dimensionalities of $X$ and $Y$ should
    be strictly the same. The pixel position index of the image is represented by
    $i$ and $j$. Then, the PSNR can be obtained by logarithmic calculation as following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$和$Y$分别表示要评估的两幅图像。$H$和$W$是它们的高度和宽度，即$X$和$Y$的维度应完全一致。图像的像素位置索引由$i$和$j$表示。然后，可以通过对数计算得到PSNR，如下所示：
- en: '| (12) |  | $PSNR=10log_{10}[\frac{({2^{N}-1)^{2}}}{MSE}],$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $PSNR=10log_{10}[\frac{({2^{N}-1)^{2}}}{MSE}],$ |  |'
- en: 'where $N$ equals $8$ for $8$-bit images. SSIM is based on the correlation between
    human visual perception and structural information, and its formula is defined
    as following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$等于$8$用于$8$位图像。SSIM基于人类视觉感知与结构信息之间的相关性，其公式定义如下：
- en: '| (13) |  | $SSIM(X,Y)=\frac{{2u_{x}}{u_{y}}+C_{1}}{u_{x}^{2}+u_{y}^{2}+C_{1}}\frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $SSIM(X,Y)=\frac{{2u_{x}}{u_{y}}+C_{1}}{u_{x}^{2}+u_{y}^{2}+C_{1}}\frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  |'
- en: where $\mu_{x}$, $\mu_{y}$, $\sigma_{x}$ and $\sigma_{y}$ represent the mean
    and variance of $X$ and $Y$, respectively; $\sigma_{xy}$ is the covariance between
    two variables; $C_{1}$ and $C_{2}$ are constants used to ensure numerical stability.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu_{x}$、$\mu_{y}$、$\sigma_{x}$和$\sigma_{y}$分别表示$X$和$Y$的均值和方差；$\sigma_{xy}$是两个变量之间的协方差；$C_{1}$和$C_{2}$是用于确保数值稳定性的常数。
- en: Since haze can cause the color of scenes and objects to change, some works use
    CIEDE2000 (Sharma et al., [2005](#bib.bib120)) as an assessment of the degree
    of color shift. PSNR, SSIM and CIEDE belong to full-reference evaluation metrics,
    which means that a clear image corresponding to a hazy image must be used as a
    reference. However, real world pairs of hazy and haze-free images are difficult
    to keep exactly the same in content. For example, the lighting and objects in
    the scene may change before and after the haze appears. Therefore, in order to
    maintain the accuracy of the evaluation process, it is necessary to synthesize
    the corresponding hazy image with a clear image (Min et al., [2019](#bib.bib98)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雾霾可能导致场景和物体的颜色发生变化，一些研究使用CIEDE2000 (Sharma等，[2005](#bib.bib120))作为颜色偏移度的评估。PSNR、SSIM和CIEDE属于全参考评估指标，这意味着必须使用与雾霾图像对应的清晰图像作为参考。然而，现实世界中的雾霾和无雾图像对很难在内容上完全相同。例如，场景中的光照和物体在雾霾出现前后可能发生变化。因此，为了保持评估过程的准确性，有必要合成对应的雾霾图像与清晰图像 (Min等，[2019](#bib.bib98))。
- en: In addition to the full-reference metric PSNR, SSIM and CIEDE, the recent works (Li
    et al., [2020c](#bib.bib82), [2019c](#bib.bib74)) utilizes the no-reference metrics
    SSEQ (Liu et al., [2014](#bib.bib87)) and BLIINDS-II (Saad et al., [2012](#bib.bib117))
    to evaluate dehazed images without ground truth. No-reference metric is of crucial
    value for real world dehazing evaluation. Nevertheless, the evaluation of current
    dehazing algorithms are usually conducted on datasets with pairs of hazy and haze-free
    images. Since the full-reference metric is more suitable for paired datasets,
    it is more widely used than the no-reference metric.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了全参考度量 PSNR、SSIM 和 CIEDE 外，近期的研究（Li et al., [2020c](#bib.bib82), [2019c](#bib.bib74)）还利用了无参考度量
    SSEQ (Liu et al., [2014](#bib.bib87)) 和 BLIINDS-II (Saad et al., [2012](#bib.bib117))
    来评估没有真实标签的去雾图像。无参考度量对于现实世界的去雾评估至关重要。然而，当前的去雾算法通常在包含雾霾图像和去雾图像配对的数据集上进行评估。由于全参考度量更适用于配对数据集，因此它比无参考度量使用得更广泛。
- en: 3\. Supervised Dehazing
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 有监督去雾
- en: Supervised dehazing models usually require different types of supervisory signals
    to guide the training process, such as transmission map, atmospheric light, haze-free
    image label, etc. Conceptually, supervised dehazing methods can be divided into
    ASM-based and non-ASM-based ones. However, there may be overlaps in this way,
    since both ASM-based and non-ASM-based algorithms may entangle with other computer
    vision tasks such as segmentation, detection, and depth estimation. Therefore,
    this section categorizes supervised algorithms according to their main contributions,
    so that the techniques that prove valuable for dehazing research are clearly observed.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督去雾模型通常需要不同类型的监督信号来指导训练过程，如传输图、大气光、去雾图像标签等。概念上，有监督去雾方法可以分为基于 ASM 的和非 ASM 基于的方法。然而，由于基于
    ASM 和非 ASM 的算法可能涉及其他计算机视觉任务，如分割、检测和深度估计，这种分类方法可能会有重叠。因此，本节根据主要贡献对有监督算法进行分类，以便清晰地观察对去雾研究有价值的技术。
- en: 3.1\. Learning of $t(x)$
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. $t(x)$ 的学习
- en: 'According to the ASM, the dehazing process can be divided into three parts:
    transmission map estimation, atmospheric light prediction, and haze-free image
    recovery. MSCNN (Ren et al., [2016](#bib.bib111)) proposes the following three
    steps for solving ASM: (1) use CNN to estimate the transmission map $t(x)$, (2)
    adopt statistical rules to predict atmospheric light $A$, and (3) solve $J(x)$
    by $t(x)$ and $A$ jointly. MSCNN adopts a multi-scale convolutional model for
    transmission map estimation and optimizes it with L2 loss. In addition, $A$ can
    be obtained by selecting $0.1\%$ darkest pixels in $t(x)$ corresponding the one
    with the highest intensity in $I(x)$ (He et al., [2010](#bib.bib52)). Thus the
    clear image $J(x)$ can be obtained by'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 ASM，去雾过程可以分为三个部分：传输图估计、大气光预测和去雾图像恢复。MSCNN (Ren et al., [2016](#bib.bib111))
    提出了以下三个步骤来解决 ASM： (1) 使用 CNN 估计传输图 $t(x)$，(2) 采用统计规则预测大气光 $A$，(3) 通过 $t(x)$ 和
    $A$ 共同解决 $J(x)$。MSCNN 采用多尺度卷积模型进行传输图估计，并使用 L2 损失优化。此外，可以通过选择 $t(x)$ 中 $0.1\%$
    最暗的像素来获得 $A$，这些像素对应于 $I(x)$ 中的最高强度（He et al., [2010](#bib.bib52)）。从而可以获得清晰图像 $J(x)$。
- en: '| (14) |  | $J(x)=\frac{I(x)-A}{max{\{0.1,t(x)\}}}+A.$ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $J(x)=\frac{I(x)-A}{max{\{0.1,t(x)\}}}+A.$ |  |'
- en: Different papers may use different statistical priors to estimate $A$, but the
    strategies they use for dehazing are similar to MSCNN. ABC-Net (Wang et al., [2020](#bib.bib134))
    uses the max pooling operation to obtain maximum value from each channel of $I(x)$.
    SID-JPM (Huang et al., [2018](#bib.bib59)) filters each channel of a RGB input
    image by a minimum filter kernel. Then the maximum value of each channel is used
    as the estimated $A$. LAPTN (Liu et al., [2018](#bib.bib89)) also applies the
    minimum filter together with the maximum filter for the prediction of $A$. These
    methods generally do not require atmospheric light annotations, but need paired
    of hazy images and transmission maps.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的论文可能使用不同的统计先验来估计 $A$，但它们用于去雾的策略与 MSCNN 相似。ABC-Net (Wang et al., [2020](#bib.bib134))
    使用最大池化操作从 $I(x)$ 的每个通道中获得最大值。SID-JPM (Huang et al., [2018](#bib.bib59)) 通过最小滤波器内核过滤
    RGB 输入图像的每个通道。然后，将每个通道的最大值用作估计的 $A$。LAPTN (Liu et al., [2018](#bib.bib89)) 也结合使用最小滤波器和最大滤波器来预测
    $A$。这些方法通常不需要大气光注释，但需要雾霾图像和传输图的配对。
- en: 3.2\. Joint Learning of $t(x)$ and $A$
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. $t(x)$ 和 $A$ 的联合学习
- en: Instead of using convolutional networks and statistical priors jointly to estimate
    the physical parameters of ASM, some works implement the prediction of physical
    parameters entirely through CNN. DCPDN (Zhang and Patel, [2018](#bib.bib164))
    estimates the transmission map through a pyramid densely connected encoder-decoder
    network, and uses a symmetric U-Net to predict the atmospheric light $A$. In order
    to improve the edge accuracy of the transmission map, DCPDN has designed a hybrid
    edge-preserving loss, which includes L2 loss, two-directional gradient loss, and
    feature edge loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究通过完全使用CNN来实现物理参数的预测，而不是将卷积网络和统计先验联合使用来估计ASM的物理参数。DCPDN (Zhang 和 Patel, [2018](#bib.bib164))
    通过一个金字塔密集连接的编码器-解码器网络估计传输图，并使用对称U-Net来预测大气光 $A$。为了提高传输图的边缘准确性，DCPDN设计了一种混合边缘保留损失，包括L2损失、双向梯度损失和特征边缘损失。
- en: DHD-Net (Xie et al., [2020](#bib.bib151)) designs a segmentation-based haze
    density estimation algorithm, which can segment dense haze areas and divide the
    global atmosphere light $A$ candidate areas. HRGAN (Pang et al., [2018](#bib.bib105))
    utilizes a multi-scale fused dilated convolutional network to predict $t(x)$,
    and employs a single-layer convolutional model to estimate $A$. PMHLD (Chen et al.,
    [2020](#bib.bib19)) uses a patch map generator and refine the network for transmission
    map estimation, and utilizes VGG-16 for atmospheric light estimation. It is worth
    noting that if using regression training to obtain $A$, the ground truth labels
    are generally required.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DHD-Net (Xie 等, [2020](#bib.bib151)) 设计了一种基于分割的雾霾密度估计算法，能够分割密集的雾霾区域并划分全球大气光
    $A$ 的候选区域。HRGAN (Pang 等, [2018](#bib.bib105)) 利用多尺度融合的扩张卷积网络来预测 $t(x)$，并使用单层卷积模型来估计
    $A$。PMHLD (Chen 等, [2020](#bib.bib19)) 使用补丁图生成器和细化网络来估计传输图，并利用VGG-16进行大气光估计。值得注意的是，如果使用回归训练来获得
    $A$，通常需要真实标签。
- en: 3.3\. Non-explicitly Embedded ASM
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 非显式嵌入ASM
- en: ASM can be incorporated into CNN in a reformulated or embedded way. AOD-Net (Li
    et al., [2017a](#bib.bib73)) founds that the end-to-end neural network can still
    be used to solve the ASM without directly using ground truth $t(x)$ and $A$. According
    to the original ASM, the expression of $J(x)$ is
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ASM 可以以重新公式化或嵌入的方式融入CNN。AOD-Net (Li 等, [2017a](#bib.bib73)) 发现端到端的神经网络仍然可以用来解决ASM，而不直接使用真实的
    $t(x)$ 和 $A$。根据原始ASM，$J(x)$ 的表达式为
- en: '| (15) |  | $J(x)=\frac{1}{t(x)}I(x)-A\frac{1}{t(x)}+A.$ |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $J(x)=\frac{1}{t(x)}I(x)-A\frac{1}{t(x)}+A.$ |  |'
- en: AOD-Net proposes $K(x)$, which has no actual physical meaning, as an intermediate
    parameter describing $t(x)$ and $A$. $K(x)$ is defined as
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AOD-Net 提出了 $K(x)$ 作为描述 $t(x)$ 和 $A$ 的中间参数，它没有实际的物理意义。$K(x)$ 被定义为
- en: '| (16) |  | $K(x)=\frac{\frac{1}{t(x)}(I(x)-A)+(A-b)}{I(x)-1},$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $K(x)=\frac{\frac{1}{t(x)}(I(x)-A)+(A-b)}{I(x)-1},$ |  |'
- en: where $b$ equals $1$. According to the ASM theory, $J(x)$ can be uniquely determined
    by $K(x)$ as
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 等于 $1$。根据ASM理论，$J(x)$ 可以通过 $K(x)$ 唯一确定，如下所示
- en: '| (17) |  | $J(x)=K(x)I(x)-K(x)+1.$ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $J(x)=K(x)I(x)-K(x)+1.$ |  |'
- en: AOD-Net considers that the non-joint transmission map and atmospheric light
    prediction process may produce accumulated errors. Therefore, independent estimation
    of $K(x)$ can reduce the systematic error. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    extends this formulation in a multi-scale framework and utilizes fully point-wise
    convolutions to achieve fast and accurate dehazing performance. DehazeGAN (Zhu
    et al., [2018](#bib.bib192)) incorporates the idea of differentiable programming
    into the estimation process of $A$ and $t(x)$. Combined with a reformulated ASM,
    DehazeGAN also implements an end-to-end dehazing pipeline. PFDN (Dong and Pan,
    [2020](#bib.bib31)) embeds ASM in the network design and proposes a feature dehazing
    unit, which removes haze in a well-designed feature space rather in the raw image
    space. It is instructive that ASM can still help the dehazing task for non-explicit
    parameter estimation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: AOD-Net 认为非联合的传输图和大气光预测过程可能会产生累积误差。因此，独立估计 $K(x)$ 可以减少系统误差。FAMED-Net (Zhang
    和 Tao, [2020](#bib.bib171)) 在多尺度框架中扩展了这一公式，并利用全点卷积实现快速准确的去雾性能。DehazeGAN (Zhu 等,
    [2018](#bib.bib192)) 将可微分编程的思想融入 $A$ 和 $t(x)$ 的估计过程中。结合重新公式化的ASM，DehazeGAN 还实现了端到端的去雾管道。PFDN
    (Dong 和 Pan, [2020](#bib.bib31)) 将ASM嵌入网络设计中，并提出了一种特征去雾单元，该单元在精心设计的特征空间中去除雾霾，而不是在原始图像空间中。值得指出的是，ASM仍然可以帮助非显式参数估计的去雾任务。
- en: '![Refer to caption](img/8fdddfd7395438914517d66d6a263617.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8fdddfd7395438914517d66d6a263617.png)'
- en: Figure 6\. Generative adversarial network for dehazing, where gt means ground
    truth.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 生成对抗网络用于去雾，其中 gt 表示真实值。
- en: 3.4\. Generative Adversarial Networks
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 生成对抗网络
- en: 'Generative adversarial networks have an important impact on dehazing research.
    In general, supervised dehazing networks that rely on paired data can use adversarial
    loss as an auxiliary supervisory signal. Adversarial loss (Gui et al., [2022](#bib.bib47))
    can be seen as two parts: the training objective of the generator is to generate
    images that the discriminator considers to be real. The optimization purpose of
    the discriminator is to distinguish the generated image from the real image contained
    in the dataset as possible as it could. For dehazing task, the effect of the adversarial
    loss is to make the generated image closer to the real one, which is beneficial
    for the optimization of the haze-free $J(x)$ and transmission map $t(x)$ (Zhang
    et al., [2019a](#bib.bib166)), as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Non-explicitly
    Embedded ASM ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning").'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络对去雾研究有着重要影响。一般来说，依赖配对数据的监督去雾网络可以将对抗损失作为辅助监督信号。对抗损失 (Gui et al., [2022](#bib.bib47))
    可以看作是两个部分：生成器的训练目标是生成被判别器认为真实的图像。判别器的优化目标是尽可能区分生成的图像与数据集中包含的真实图像。在去雾任务中，对抗损失的作用是使生成的图像更接近真实图像，这有利于优化无雾的
    $J(x)$ 和传输图 $t(x)$ (Zhang et al., [2019a](#bib.bib166))，如图 [6](#S3.F6 "Figure
    6 ‣ 3.3\. Non-explicitly Embedded ASM ‣ 3\. Supervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") 所示。
- en: Inspired by patchGAN (Isola et al., [2017](#bib.bib61)), which can better preserve
    high-frequency information, DH-GAN (Sim et al., [2018](#bib.bib126)), RI-GAN (Dudhane
    et al., [2019](#bib.bib37)) and DehazingGAN (Zhang et al., [2020c](#bib.bib176))
    use $N\times N$ patches instead of a single value as the output of the discriminator.
    Several works explore joint training mechanisms of multiple discriminators, such
    as EPDN (Qu et al., [2019](#bib.bib110)) and PGC-UNet (Zhao et al., [2021a](#bib.bib187)).
    Discriminator $D_{1}$ is used to guide the generator on a fine scale, while discriminator
    $D_{2}$ helps the generator to produce a global realistic output on a coarse scale.
    In order to realize the joint training of the two discriminators, EPDN downsamples
    the input image of $D_{1}$ by a factor of $2$ as the input of $D_{2}$. The adversarial
    loss is
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 受 patchGAN (Isola et al., [2017](#bib.bib61)) 启发，它可以更好地保留高频信息，DH-GAN (Sim et al.,
    [2018](#bib.bib126))、RI-GAN (Dudhane et al., [2019](#bib.bib37)) 和 DehazingGAN (Zhang
    et al., [2020c](#bib.bib176)) 使用 $N\times N$ 补丁而不是单一值作为判别器的输出。一些研究探讨了多个判别器的联合训练机制，例如
    EPDN (Qu et al., [2019](#bib.bib110)) 和 PGC-UNet (Zhao et al., [2021a](#bib.bib187))。判别器
    $D_{1}$ 用于在细尺度上引导生成器，而判别器 $D_{2}$ 帮助生成器在粗尺度上生成全局逼真的输出。为了实现两个判别器的联合训练，EPDN 将 $D_{1}$
    的输入图像下采样 $2$ 倍作为 $D_{2}$ 的输入。对抗损失为
- en: '| (18) |  | $L_{adv}=\min_{G}[\max_{D_{1},D_{2}}\sum_{k=1,2}\ell_{A}(G,D_{k})],$
    |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $L_{adv}=\min_{G}[\max_{D_{1},D_{2}}\sum_{k=1,2}\ell_{A}(G,D_{k})],$
    |  |'
- en: where the form of adversarial loss $\ell_{A}$ is the same as the single GAN.
    The exploration of GAN brings plug-and-play tools to supervised algorithms. Since
    the training of the discriminator can be done in an unsupervised manner, the quality
    of the dehazed images can be improved without the requirement of extra labels.
    However, the training of GAN sometimes suffers from instability and non-convergence,
    which may bring certain additional difficulties to the training of the dehazing
    network.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失 $\ell_{A}$ 的形式与单一 GAN 相同。GAN 的探索为监督算法带来了即插即用的工具。由于判别器的训练可以以无监督的方式进行，因此去雾图像的质量可以在无需额外标签的情况下得到改善。然而，GAN
    的训练有时会遭遇不稳定性和非收敛性，这可能为去雾网络的训练带来某些额外困难。
- en: 3.5\. Level-aware
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 级别感知
- en: According to the scattering theory, the farther the scene is from the camera,
    the more aerosol particles pass through. This means that areas within a single
    hazy image that are farther from the camera have higher densities of haze. Therefore,
    LAP-Net (Li et al., [2019b](#bib.bib83)) proposes that the algorithm should consider
    the difference in haze density inside the image. Through multi-stage joint training,
    LAP-Net implements an easy-to-hard model that focuses on a specific haze density
    level by a stage-wise loss
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 根据散射理论，场景距离相机越远，通过的气溶胶颗粒越多。这意味着在单一雾霾图像中，距离相机较远的区域具有更高的雾霾密度。因此，LAP-Net (Li et
    al., [2019b](#bib.bib83)) 提出算法应考虑图像内的雾霾密度差异。通过多阶段联合训练，LAP-Net 实现了一种由易到难的模型，通过阶段性损失关注特定的雾霾密度水平。
- en: '| (19) |  | $\hat{t}^{s}(x)=\begin{cases}\mathcal{F}(I(x),\theta^{s}),\text{if
    }s=1,\\ \mathcal{F}(I(x),\theta^{s},\hat{t}^{s-1}(x)),\text{if }s>1,\end{cases}$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\hat{t}^{s}(x)=\begin{cases}\mathcal{F}(I(x),\theta^{s}),\text{if
    }s=1,\\ \mathcal{F}(I(x),\theta^{s},\hat{t}^{s-1}(x)),\text{if }s>1,\end{cases}$
    |  |'
- en: where $\mathcal{F}$ represents the transmission map prediction network with
    parameter $\theta^{s}$ in stage $s$. In the first stage, the transmission map
    prediction network is responsible for estimating the case with mild haze. In the
    second and subsequent stages, the prediction result of the previous stage and
    the hazy image are used as joint input for processing higher haze density.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}$ 表示带有参数 $\theta^{s}$ 的传输图预测网络。在第一阶段，传输图预测网络负责估计轻度雾霾的情况。在第二阶段及后续阶段，使用上一阶段的预测结果和雾霾图像作为联合输入来处理更高密度的雾霾。
- en: The density of haze may be related to conditions such as temperature, wind,
    altitude, and humidity. Thus, the formation of haze should be space-variant and
    non-homogeneous. Based on this observation, HardGAN (Deng et al., [2020](#bib.bib26))
    argues that estimating the transmission map for dehazing may be inaccurate. By
    encoding the atmospheric brightness as $1\times 1\times 2$ matrix $\gamma_{i}^{G}\&amp;\beta_{i}^{G}$
    and pixel-wise spatial information as $H\times W\times 2$ matrix $\gamma_{i}^{L}\&amp;\beta_{i}^{L}$
    for $i$-th channel of the input $x$, HardGAN designs the control function of atmospheric
    brightness $G_{i}$ and spatial information $L_{i}$ as
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 雾的密度可能与温度、风速、海拔和湿度等条件有关。因此，雾的形成应该是空间变化的且不均匀的。基于这一观察，HardGAN (Deng et al., [2020](#bib.bib26))
    认为估计去雾的传输图可能不准确。通过将大气亮度编码为 $1\times 1\times 2$ 矩阵 $\gamma_{i}^{G}\&\beta_{i}^{G}$
    和逐像素空间信息编码为 $H\times W\times 2$ 矩阵 $\gamma_{i}^{L}\&\beta_{i}^{L}$，针对输入 $x$ 的
    $i$-th 通道，HardGAN 设计了大气亮度的控制函数 $G_{i}$ 和空间信息的控制函数 $L_{i}$ 如下：
- en: '|  | $\displaystyle G_{i}=\gamma_{i}^{G}\frac{x-\mu}{\sigma}+\beta_{i}^{G},$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{i}=\gamma_{i}^{G}\frac{x-\mu}{\sigma}+\beta_{i}^{G},$
    |  |'
- en: '| (20) |  | $\displaystyle L_{i}=\gamma_{i}^{L}\frac{x-\mu}{\sigma}+\beta_{i}^{L},$
    |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle L_{i}=\gamma_{i}^{L}\frac{x-\mu}{\sigma}+\beta_{i}^{L},$
    |  |'
- en: where $\mu$ and $\sigma$ denote the mean and standard deviation for $x$, respectively.
    After obtaining $G_{i}$ and $L_{i}$, HardGAN uses a linear model to fuse them
    for recovering haze-free image
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu$ 和 $\sigma$ 分别表示 $x$ 的均值和标准差。获取 $G_{i}$ 和 $L_{i}$ 后，HardGAN 使用线性模型将它们融合以恢复无雾图像。
- en: '| (21) |  | $J_{pred_{i}}(x)=(1-HA_{i})*G_{i}+HA_{i}*L_{i},$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $J_{pred_{i}}(x)=(1-HA_{i})*G_{i}+HA_{i}*L_{i},$ |  |'
- en: where $HA$ is calculated by the intermediate feature map of HardGAN via using
    instance normalization followed by a sigmoid layer, and $*$ denotes element-wise
    product.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $HA$ 是通过 HardGAN 的中间特征图计算的，使用实例归一化后跟随 sigmoid 层，而 $*$ 表示逐元素乘积。
- en: 3.6\. Multi-function Fusion
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 多功能融合
- en: 'DMMFD (Deng et al., [2019](#bib.bib27)) designs a layer separation and fusion
    model for improving learning ability, including reformulated ASM, multiplication,
    addition, exponentiation and logarithmic decomposition:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: DMMFD (Deng et al., [2019](#bib.bib27)) 设计了一种层分离和融合模型以提高学习能力，包括重新构造的 ASM、乘法、加法、指数和对数分解：
- en: '|  | $\displaystyle J_{0}(x)=\frac{I(x)-A_{0}\times(1-t_{0}(x))}{t_{0}(x)},$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{0}(x)=\frac{I(x)-A_{0}\times(1-t_{0}(x))}{t_{0}(x)},$
    |  |'
- en: '|  | $\displaystyle J_{1}(x)=I(x)\times R_{1}(x),$ |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{1}(x)=I(x)\times R_{1}(x),$ |  |'
- en: '|  | $\displaystyle J_{2}(x)=I(x)+R_{2}(x),$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{2}(x)=I(x)+R_{2}(x),$ |  |'
- en: '|  | $\displaystyle J_{3}(x)=(I(x))^{R_{3}(x)},$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{3}(x)=(I(x))^{R_{3}(x)},$ |  |'
- en: '| (22) |  | $\displaystyle J_{4}(x)=\log(1+I(x)\times R_{4}(x)),$ |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\displaystyle J_{4}(x)=\log(1+I(x)\times R_{4}(x)),$ |  |'
- en: where $R_{i}(x)$ stands for layers in the network; $A_{0}$ and $t_{0}(x)$ are
    the atmospheric light and transmission map estimated by the feature extraction
    network, respectively. $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$, and $J_{4}{(x)}$
    can be used as four independent haze-layer separation models. It is based on the
    assumption that the input hazy image $I(x)$ can be separated into a haze-free
    layer $J(x)$ and another layer $H(x)$, denoted as $I(x)=\phi(J(x),H(x))$. The
    final dehazing result is obtained by weighted fusion of the intermediate outputs
    $J_{0}{(x)}$, $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$ and $J_{4}{(x)}$ by five
    learned attention maps $W_{0}$, $W_{1}$, $W_{2}$, $W_{3}$ and $W_{4}$ as
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\displaystyle J_{pred}(x)=$ | $\displaystyle W_{0}\times J_{0}{(x)}+W_{1}\times
    J_{1}{(x)}+W_{2}\times J_{2}{(x)}+W_{3}\times J_{3}{(x)}+W_{4}\times J_{4}{(x)}.$
    |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: Through ablation studies, DMMFD has demonstrated that the fusion of multiple
    layers can improve the quality of the scene restoration process.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Transformation and Decomposition of Input
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GFN (Ren et al., [2018a](#bib.bib112)) proposes two observations on the influence
    of haze. First, under the influence of atmospheric light, the color of a hazy
    image may be distorted to some extent. Second, due to the existence of scattering
    and attenuation phenomena, the visibility of objects far away from the camera
    in the scene will be reduced. Therefore, GFN uses three enhancement strategies
    to process the original hazy image and use them as inputs to the dehazing network
    together. The white balanced input $I_{wb}(x)$ is obtained from the gray world
    assumption. The contrast enhanced input $I_{ce}(x)$ is composed of average luminance
    value $\widetilde{I}(x)$ and the control factor $\mu$, as following:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $I_{ce}{(x)}=\mu(I(x)-\widetilde{I}{(x)}),$ |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: where $\mu=2\cdot(0.5+\widetilde{I}(x))$. By using the the nonlinear gamma correction,
    the $I_{gc}{(x)}$ used to enhance the visibility of the $I(x)$ can be obtained
    by
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $I_{gc}{(x)}=\alpha{I(x)^{\gamma}},$ |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha=1$, and $\gamma=2.5$. The final dehazing image $J(x)$ is determined
    by the combination of three inputs, where $C_{wb}$, $C_{ce}$ and $C_{gc}$ are
    confidences map for fusion process:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $J(x)=C_{wb}\circ{I_{wb}{(x)}}+C_{ce}\circ{I_{ce}{(x)}}+C_{gc}\circ{I_{gc}{(x)}}.$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: 'MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)) decomposes the hazy image
    into low frequency and high frequency as the base component $I_{base}{(x)}$ and
    the detail component $I_{detail}{(x)}$, respectively. The basic component can
    be thought as the main content of the image, while the high frequency component
    denotes the edge and texture. Therefore, the dehazed image can be obtained by
    the dehazing function $D(\cdot)$ of the basic component and the enhancement function
    $E(\cdot)$ of the detail component. The whole process can be represented by a
    linear model as following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I(x)=I_{base}{(x)}+I_{detail}{(x)},$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| (27) |  | $\displaystyle J(x)=D(I_{base}{(x)})+E(I_{detail}{(x)}).$ |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: DIDH (Shyam et al., [2021](#bib.bib124)) decomposes both the input hazy image
    $I(x)$ and the predicted haze-free image $J(x)$ to obtain $(I(x),LF(I(x)))$, $(I(x),HF(I(x)))$,
    $(J(x),LF(J(x)))$ and $J(x),HF(J(x))$, where $LF(\cdot)$ and $HF(\cdot)$ denotes
    Gaussian and Laplacian filter, respectively. By fusing the decomposed data with
    the pre-decomposition data as the input of the discriminator, DIDH can improve
    the quality of the image generated by the adversarial training process. With the
    help of the discriminators $D_{LF}$ and $D_{HF}$, the dehazing network can be
    optimized in an adversarial way.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Compared with conventional data augmentation, such as rotation, horizontal or
    vertical mirror symmetry, and random cropping, the transformation and decomposition
    of the input is a more efficient strategy for the usage of hazy images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a81a177c411c6c06da3da995439ee4cd.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. A general knowledge distillation strategy used in KDDN (Hong et al.,
    [2020](#bib.bib55)), KTDN (Wu et al., [2020](#bib.bib148)), SRKTDN (Chen et al.,
    [2021a](#bib.bib17)), etc.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 3.8\. Knowledge Distillation
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation (Gou et al., [2021](#bib.bib45)) provides a strategy
    to transfer the knowledge learned by the teacher network to the student network,
    which has been applied in high-level computer vision tasks like object detection
    and image classification (Wang et al., [2019](#bib.bib137)). Recent work (Hong
    et al., [2020](#bib.bib55)) presents three challenges for applying knowledge distillation
    to the dehazing task. First, what kind of teacher task can help the dehazing task.
    Second, how the teacher network helps the dehazing network during training. Third,
    which similarity measure between teacher task and student task should be chosen.
    Fig. [7](#S3.F7 "Figure 7 ‣ 3.7\. Transformation and Decomposition of Input ‣
    3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") shows the knowledge distillation strategy adopted
    by various dehazing algorithms. Different methods may use different numbers and
    locations of output features to compute feature loss.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: KDDN (Hong et al., [2020](#bib.bib55)) designs a process-oriented learning mechanism,
    where the teacher network $T$ is an auto-encoder for high-quality haze-free image
    reconstruction. When training the dehazing network, the teacher network assists
    in feature learning, and optimizes $L_{T}=||J(x)-T(J(x))||_{1}$. Therefore, the
    teacher task and the student task proposed by KDDN are two different tasks. In
    order to make full use of the feature information learned by the teacher network
    and help the training of the dehazing network. KDDN uses feature matching loss
    and haze density aware loss by a linear transformation function $g$, which are
    represented by ([28](#S3.E28 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")) and ([29](#S3.E29 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")), respectively.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: KDDN（Hong 等，[2020](#bib.bib55)）设计了一种面向过程的学习机制，其中教师网络 $T$ 是用于高质量无雾图像重建的自编码器。在训练去雾网络时，教师网络协助特征学习，并优化
    $L_{T}=||J(x)-T(J(x))||_{1}$。因此，KDDN 提出的教师任务和学生任务是两个不同的任务。为了充分利用教师网络学习到的特征信息并帮助去雾网络的训练，KDDN
    使用通过线性变换函数 $g$ 表示的特征匹配损失和雾密度感知损失，这些分别由 ([28](#S3.E28 "In 3.8\. Knowledge Distillation
    ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")) 和 ([29](#S3.E29 "In 3.8\. Knowledge Distillation
    ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")) 表示。
- en: '| (28) |  | $L_{rm}=\sum_{(m,n)\in{C}}&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;,$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $L_{rm}=\sum_{(m,n)\in{C}}|T^{m}(J(x))-g(S^{n}(I(x)))|,$ |  |'
- en: '| (29) |  | $L_{wrm}=\sum_{(m,n)\in{C}}\psi\times{&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;},$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $L_{wrm}=\sum_{(m,n)\in{C}}\psi\times{|T^{m}(J(x))-g(S^{n}(I(x)))|},$
    |  |'
- en: where $T^{m}$ represents the $m$-th layer of the teacher network, and the corresponding
    $S^{n}$ represents the $n$-th layer of the student network; $\psi$ is obtained
    by the normalization operation. KDDN can be trained without real transmission
    map, replaced by the residual between hazy and haze-free images.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T^{m}$ 表示教师网络的第 $m$ 层，相应的 $S^{n}$ 表示学生网络的第 $n$ 层；$\psi$ 是通过归一化操作获得的。KDDN
    可以在没有真实传输图的情况下进行训练，用雾图和无雾图之间的残差替代。
- en: KTDN (Wu et al., [2020](#bib.bib148)) is jointly trained using a teacher network
    and a dehazing network with the same structure. Through feature level loss, the
    prior knowledge possessed by the teacher network can be transferred to the dehazing
    network. SRKTDN (Chen et al., [2021a](#bib.bib17)) uses ResNet18 pre-trained on
    ImageNet (with classification layers removed) as the teacher network, transferring
    many statistical experiences to the Res2Net101 encoder for dehazing. DALF (Fang
    et al., [2021](#bib.bib39)) integrates dual adversarial training into the training
    process of knowledge distillation to improve the imitation ability of the student
    network to the teacher network. Applying knowledge distillation to dehazing networks
    provides a new and efficient way to introduce external prior knowledge.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: KTDN（Wu 等，[2020](#bib.bib148)）使用具有相同结构的教师网络和去雾网络进行联合训练。通过特征层级的损失，教师网络所拥有的先验知识可以转移到去雾网络中。SRKTDN（Chen
    等，[2021a](#bib.bib17)）使用在 ImageNet 上预训练的 ResNet18（去掉分类层）作为教师网络，将许多统计经验转移到 Res2Net101
    编码器中以进行去雾。DALF（Fang 等，[2021](#bib.bib39)）将双重对抗训练整合到知识蒸馏的训练过程中，以提高学生网络对教师网络的模仿能力。将知识蒸馏应用于去雾网络提供了一种引入外部先验知识的新方法和高效途径。
- en: 3.9\. Transformation of Colorspace
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9\. 颜色空间的转换
- en: The input data of the dehazing network are usually three channel color images
    in RGB mode. By calculating the mean square error of hazy and haze-free images
    in RGB space and YCrCb space on the Dense-Haze dataset, Bianco et al. (Bianco
    et al., [2019](#bib.bib12)) find that haze shows obvious numerical differences
    in the two spaces. The error values for the red, green and blue channels in RGB
    space are very close. However, in the YCrCb space, the error value of the luminance
    channel is significantly larger than that of the blue and red chroma components.
    AIP-Net (Wang et al., [2018a](#bib.bib133)) performs a similar error comparison
    of color space transformation on synthetic datasets and obtained the same conclusion
    as  (Bianco et al., [2019](#bib.bib12)). Quantitative results (Wang et al., [2018a](#bib.bib133);
    Bianco et al., [2019](#bib.bib12)) obtained by training the dehazing network in
    the YCrCb color space show that RGB space is not the only effective color mode
    for deep learning based dehazing methods. Furthermore, TheiaNet (Mehra et al.,
    [2021](#bib.bib95)) comprehensively analyzes the performance obtained by training
    the dehazing model in RGB, YCrCb, HSV and LAB color spaces. Experiments (Singh
    et al., [2020](#bib.bib128); Sheng et al., [2022](#bib.bib122); Chen et al., [2021a](#bib.bib17);
    Dudhane and Murala, [2019b](#bib.bib36)) show that converting images from RGB
    space to other color spaces for model training is an effective scheme.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 3.10\. Contrastive Learning
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the process of training a non-ASM-based supervised dehazing network, a common
    way is to use the hazy image as the input of the network and expect to obtain
    a clear image. In this process, the clear image is used as a positive example
    to guide the optimization of the network. By designing pairs of positive and negative
    examples, AECR-Net provides a new perspective for treating hazy and haze-free
    images. Specifically, the clear image $J(x)$ and the dehazed image $J_{pred}(x)$
    are taken as a positive sample pair, while the hazy image $I(x)$ and the dehazed
    image $J_{pred}{(x)}$ are taken as a negative sample pair. According to contrastive
    learning (Khosla et al., [2020](#bib.bib66)), $J_{pred}(x)$, $J(x)$ and $I(x)$
    can be regarded as anchor, positive and negative, respectively. For the pre-trained
    model $G(\cdot)$, the dehazing loss can be regarded as the sum of the reconstruction
    loss and the regularization term, as following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\min{&#124;&#124;J(x)-\psi(I(x))&#124;&#124;_{1}}+\lambda\sum_{i=1}^{N}{\omega_{i}}\cdot\frac{&#124;&#124;G_{i}(J(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}}{&#124;&#124;G_{i}(I(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}},$
    |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: where $G_{i}$ represents the output feature of the $i$-th layer of the pre-trained
    model; $\lambda$ is the weight ratio between the image reconstruction loss and
    the regularization term; $w_{i}$ is weight factor for feature output; $\phi$ is
    the dehazing network. AECR-Net provides a universal contrastive regularization
    strategy for existing methods without adding extra parameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 3.11\. Non-deterministic Output
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dehazing methods based on deep learning usually set the optimization objective
    to obtain a single dehazed image. By introducing 2-dimensional latent tensors,
    pWAE (Kim et al., [2021](#bib.bib67)) can generate different styles of dehazed
    images, which extends the general training purpose. pWAE proposes dehazing latent
    space $z_{h}$ and style latent space $z_{s}$ for dehazing, and applies the mean
    function $\mu(\cdot)$ and standard deviation function $\sigma(\cdot)$ to perform
    the transformation of the space:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $z_{h\to{s}}=\sigma(z_{s})(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})})+\mu{(z_{s})}.$
    |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: A natural question is how to adjust the magnitude of the spatial mapping to
    control the degree of style transformation. pWAE uses linear module $z_{h}^{s}=\alpha{z_{h\to{s}}}+(1-\alpha)z_{h}$
    to adjust the weight between the dehazed image and the style information. By controlling
    $\alpha$, different degrees of stylized dehazed images corresponding to the style
    image can be obtained.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: DehazeFlow (Li et al., [2021b](#bib.bib77)) proposes that the dehazing task
    itself is ill-posedness, so it is unreliable to learn a model with deterministic
    one-to-one mapping. With a conditional normalizing flow network, DehazeFlow can
    compute the conditional distribution of clear images from a given hazy image.
    Therefore, it can learn a one-to-many mapping and obtain different dehazing results
    in the inference stage. The non-deterministic output (Kim et al., [2021](#bib.bib67);
    Li et al., [2021b](#bib.bib77)) brings interpretable flexibility to the dehazing
    algorithm. Since the visual evaluation criteria of individual human beings are
    inherently different, one-to-many dehazing mapping provides more options for photographic
    work.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 3.12\. Retinex Model
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Non-deep learning based research (Galdran et al., [2018](#bib.bib41)) demonstrates
    the duality between Retinex and image dehazing. Apart from the already widely
    used ASM, recent work (Li et al., [2021c](#bib.bib80)) explores the combination
    of Retinex model and CNN for dehazing. Retinex theory proposes that the image
    can be regarded as the element-wise product of the reflectance image $R$ and the
    illumination map $L$. Assuming the reflectance $R$ is illumination invariant,
    the relationship between hazy and haze-free image can be modeled by a retinex-based
    decomposition model as following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $I(x)=J(x)\ast\frac{L_{I}}{L_{J}}=J(x)\ast L_{r},$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: where $*$ means the multiplication in an element-wise way. $L_{r}$ can be seen
    as absorbed and scattered light caused by haze, which is determined by the ratio
    of the hazy image illumination map $L_{I}$ and natural illumination map $L_{J}$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $*$ 表示逐元素的乘法。$L_{r}$ 可以被看作是由雾霾引起的吸收和散射光，这由有雾图像的光照图 $L_{I}$ 和自然光照图 $L_{J}$
    的比率决定。
- en: Compared with ASM, which has been proven to be reliable, Retinex has a more
    compact physical form and fewer parameters to be estimated, but is not widely
    combined with CNN for dehazing, yet.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与已经被证明可靠的ASM相比，Retinex具有更紧凑的物理形态和更少的参数需要估计，但尚未广泛与CNN结合用于去雾。
- en: 3.13\. Residual Learning
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.13\. 残差学习
- en: 'Rather than directly learning the mapping from hazy to haze-free image, several
    methods argue that the residual learning method can reduce the learning difficulty
    of the network. The dehazing methods based on residual learning is performed at
    the image level. GCANet (Chen et al., [2019c](#bib.bib14)) uses the residual {$J(x)-I(x)$}
    between haze-free and hazy images as the optimization objective. DRL (Du and Li,
    [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)) and POGAN (Du and
    Li, [2019](#bib.bib34)) believe that the way of residual learning is related to
    ASM, and the relationship can be obtained by reformulated ASM:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与其直接学习从有雾到无雾图像的映射，几种方法认为残差学习方法可以减少网络的学习难度。基于残差学习的去雾方法在图像层面上进行。GCANet (Chen et
    al., [2019c](#bib.bib14)) 使用去雾图像和有雾图像之间的残差 {$J(x)-I(x)$} 作为优化目标。DRL (Du和Li, [2018](#bib.bib33))、SID-HL
    (Xiao et al., [2020](#bib.bib150)) 和POGAN (Du和Li, [2019](#bib.bib34)) 认为残差学习的方法与ASM相关，且可以通过重新表述ASM获得这种关系：
- en: '| (33) |  | $\displaystyle I(x)$ | $\displaystyle=J(x)+(A-J(x))(1-t(x))=J(x)+r(x),$
    |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle I(x)$ | $\displaystyle=J(x)+(A-J(x))(1-t(x))=J(x)+r(x),$
    |  |'
- en: where $r(x)=(A-J(x))(1-t(x))$ can be interpreted as a structural error term.
    By predicting nonlinear signal-dependent degradation $r(x)$, a clear image $J(x)=I(x)-r(x)$
    can be obtained.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r(x)=(A-J(x))(1-t(x))$ 可以解释为结构误差项。通过预测非线性信号依赖的退化 $r(x)$，可以得到清晰图像 $J(x)=I(x)-r(x)$。
- en: 3.14\. Frequency Domain
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.14\. 频率域
- en: The dehazing methods based on CNN usually use down-sampling and upsampling for
    feature extraction and clear image reconstruction, and this process less considers
    the frequency information contained in the image. There are currently two approaches
    to combine frequency analysis and dehazing networks. The first is to embed the
    frequency decomposition function into the convolutional network, and the second
    is to use frequency decomposition as a constraint for loss computation. For a
    given image, one low-frequency component and three high-frequency components can
    be obtained by wavelet decomposition. Wavelet U-net  (Yang and Fu, [2019](#bib.bib156))
    uses discrete wavelet transform (DWT) and inverse discrete wavelet transform (IDWT)
    to facilitate high quality restoration of edge information in a clear image. Through
    the 1D scaling function $\phi(\cdot)$ and the wavelet function $\psi(\cdot)$,
    the 2D wavelets low-low (LL), low-high (LH), high-low (HL) and high-high (HH)
    are calculated as follows
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的去雾方法通常使用下采样和上采样进行特征提取和清晰图像重建，这一过程较少考虑图像中的频率信息。目前有两种方法将频率分析与去雾网络结合。第一种是将频率分解函数嵌入卷积网络中，第二种是将频率分解作为损失计算的约束。对于给定图像，通过小波分解可以得到一个低频分量和三个高频分量。Wavelet
    U-net (Yang和Fu, [2019](#bib.bib156)) 使用离散小波变换（DWT）和逆离散小波变换（IDWT）以便高质量恢复清晰图像中的边缘信息。通过1D尺度函数$\phi(\cdot)$和小波函数$\psi(\cdot)$，计算二维小波低-低（LL）、低-高（LH）、高-低（HL）和高-高（HH）如下
- en: '|  | $\displaystyle\Phi_{LL}(m,n)=\phi(m)\phi(n),$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Phi_{LL}(m,n)=\phi(m)\phi(n),$ |  |'
- en: '|  | $\displaystyle\Psi_{LH}(m,n)=\phi(m)\psi(n),$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi_{LH}(m,n)=\phi(m)\psi(n),$ |  |'
- en: '|  | $\displaystyle\Psi_{HL}(m,n)=\psi(m)\phi(n),$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi_{HL}(m,n)=\psi(m)\phi(n),$ |  |'
- en: '| (34) |  | $\displaystyle\Psi_{HH}(m,n)=\psi(m)\psi(n).$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\Psi_{HH}(m,n)=\psi(m)\psi(n).$ |  |'
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    MsGWN (Dong et al., [2020c](#bib.bib30)) uses Gabor wavelet decomposition for
    feature extraction and reconstruction. By setting different degree values, the
    feature extraction module of MsGWN can obtain frequency information in different
    directions. EMRA-Net (Wang et al., [2021d](#bib.bib136)) uses Haar wavelet decomposition
    as a downsampling operation instead of nearest downsampling and strided-convolution
    to avoid the loss of image texture details. By embedding wavelet analysis into
    convolutional network or loss function, recent studies (Yang et al., [2020](#bib.bib157);
    Dharejo et al., [2021](#bib.bib28); Fu et al., [2021](#bib.bib40)) have shown
    that 2D wavelet transform can improve the recovery of high-frequency information
    in the wavelet domain. These works successfully apply wavelet theory and neural
    networks to dehazing tasks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别表示水平和垂直坐标。MsGWN (Dong et al., [2020c](#bib.bib30)) 使用Gabor小波分解进行特征提取和重建。通过设置不同的角度值，MsGWN
    的特征提取模块可以获得不同方向的频率信息。EMRA-Net (Wang et al., [2021d](#bib.bib136)) 使用Haar小波分解作为下采样操作，而不是最近邻下采样和步幅卷积，以避免图像纹理细节的丢失。通过将小波分析嵌入卷积网络或损失函数，近期研究
    (Yang et al., [2020](#bib.bib157); Dharejo et al., [2021](#bib.bib28); Fu et al.,
    [2021](#bib.bib40)) 已经显示2D小波变换可以提高小波域高频信息的恢复。这些工作成功地将小波理论和神经网络应用于去雾任务。
- en: Besides, TDN (Liu et al., [2020b](#bib.bib86)) introduces the Fast Fourier transform
    (FFT) loss as a constraint in the frequency domain. With the help of supervised
    training on amplitude and phase, the visual perceptual quality of images is improved
    without any additional computation in the inference stage.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TDN (Liu et al., [2020b](#bib.bib86)) 在频域引入了快速傅里叶变换（FFT）损失作为约束。借助于对幅度和相位的监督训练，图像的视觉感知质量得到了提升，而推理阶段没有任何额外的计算。
- en: 3.15\. Joint Dehazing and Depth Estimation
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.15\. 联合去雾与深度估计
- en: 'The scattering particles in the hazy environment will affect the accuracy of
    the depth information collected by the LiDAR equipment. S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)) proves that high-quality depth estimation algorithms can help
    the dehazing task. According to ASM, it can be known that the transmission map
    $t(x)$ and the depth map $d(x)$ have a negative exponential relationship $t(x)=e^{-\beta{d(x)}}$.
    Based on this physical dependency, SDDE (Lee et al., [2020a](#bib.bib69)) uses
    four decoders to integrate estimates of atmospheric light, clear image, transmission
    map, and depth map into an end-to-end training pipeline. In particular, SDDE proposes
    a depth-transmission consistency loss based on the observation that the standard
    deviation value ($std$) of the transmission map and the depth map pair should
    tend to zero:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在雾霾环境中的散射颗粒会影响LiDAR设备收集的深度信息的准确性。S2DNet (Hambarde and Murala, [2020](#bib.bib51))
    证明了高质量的深度估计算法可以帮助去雾任务。根据ASM，可以知道传输图 $t(x)$ 和深度图 $d(x)$ 具有负指数关系 $t(x)=e^{-\beta{d(x)}}$。基于这种物理依赖性，SDDE
    (Lee et al., [2020a](#bib.bib69)) 使用四个解码器将大气光、清晰图像、传输图和深度图的估计集成到端到端的训练流程中。特别地，SDDE
    提出了基于观察的深度-传输一致性损失，标准偏差值 ($std$) 的传输图和深度图对应该趋近于零：
- en: '| (35) |  | $L=&#124;&#124;std(\ln(t_{pred}{(x)}/d_{pred}{(x)}))&#124;&#124;_{2},$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $L=&#124;&#124;std(\ln(t_{pred}{(x)}/d_{pred}{(x)}))&#124;&#124;_{2},$
    |  |'
- en: where $t_{pred}{(x)}$ and $d_{pred}{(x)}$ represent the predicted transmission
    map and depth map, respectively. Aiming at better dehazing, TSDCN-Net  (Cheng
    and Zhao, [2021](#bib.bib23)) designs a cascaded network for two-stage methods
    with depth information prediction. Quantitative experimental results (Yang and
    Zhang, [2022](#bib.bib155)) show that this joint estimation method can improve
    the accuracy of dehazing task.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{pred}{(x)}$ 和 $d_{pred}{(x)}$ 分别表示预测的传输图和深度图。为了更好的去雾，TSDCN-Net (Cheng
    and Zhao, [2021](#bib.bib23)) 设计了一个级联网络用于具有深度信息预测的两阶段方法。定量实验结果 (Yang and Zhang,
    [2022](#bib.bib155)) 显示，这种联合估计方法可以提高去雾任务的准确性。
- en: 3.16\. Segmentation and Detection with Dehazing
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.16\. 分割与去雾检测
- en: Existing experiments (Li et al., [2017a](#bib.bib73)) show that the existence
    of image haze may cause various problems in detection algorithms, such as missing
    targets, inaccurate localizations and unconfident category prediction. Recent
    work (Sakaridis et al., [2018](#bib.bib118)) has shown that haze also brings difficulties
    to semantic scene understanding. As a preprocessing module for high-level computer
    vision tasks, the dehazing process is usually separated from high-level computer
    vision tasks.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现有实验（Li et al., [2017a](#bib.bib73)）表明，图像雾霾的存在可能导致检测算法出现各种问题，如目标遗漏、不准确的定位和不自信的类别预测。最近的工作（Sakaridis
    et al., [2018](#bib.bib118)）显示，雾霾还给语义场景理解带来了困难。作为高层计算机视觉任务的预处理模块，去雾过程通常与高层计算机视觉任务分开。
- en: Li et al. (Li et al., [2017a](#bib.bib73)) jointly optimize the object detection
    algorithm and AOD-Net, and the result proves that the dehazing algorithm can promote
    the detection task. LEAAL (Li et al., [2020c](#bib.bib82)) proposes that fine-tuning
    the parameters of the object detector during joint training may lead to a detector
    biased towards the haze-free images generated by the pretrained dehazing network.
    Different from the fine-tuning operation of Li et al. (Li et al., [2017a](#bib.bib73)),
    LEAAL uses object detection as an auxiliary task for dehazing and the parameters
    of the object detector are not fully updated during the training process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人（Li et al., [2017a](#bib.bib73)）联合优化了目标检测算法和 AOD-Net，结果证明去雾算法可以促进检测任务。LEAAL（Li
    et al., [2020c](#bib.bib82)）提出，在联合训练过程中微调目标检测器的参数可能导致检测器偏向于由预训练去雾网络生成的无雾图像。不同于
    Li 等人（Li et al., [2017a](#bib.bib73)）的微调操作，LEAAL 将目标检测作为去雾的辅助任务，在训练过程中目标检测器的参数不会被完全更新。
- en: UDnD (Zhang et al., [2020g](#bib.bib186)) takes advantage of each other by jointly
    training a dehazing network and a dense-aware multi-domain object detector. The
    object detection network is trained by adopting the classification and localization
    terms used by Region Proposal Network and Region of Interest. The multi-task training
    approach used by UDnD can consider the reduced inter-domain gaps and the remained
    intra-domain gaps for different density levels.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: UDnD（Zhang et al., [2020g](#bib.bib186)）通过联合训练去雾网络和密集感知多域目标检测器相互利用对方的优势。目标检测网络通过采用
    Region Proposal Network 和 Region of Interest 使用的分类和定位术语进行训练。UDnD 使用的多任务训练方法可以考虑不同密度级别的减少的领域间差距和剩余的领域内差距。
- en: Recent work has explored performing dehazing and high-level computer vision
    tasks simultaneously without using ASM. SDNet (Zhang et al., [2022a](#bib.bib178))
    combines semantic segmentation and dehazing into a unified framework in order
    to use semantic prior as a constraint for the optimization process. By embedding
    the predictions of the segmentation map into the dehazing network, SDNet performs
    a joint optimization of pixel-wise classification loss and regression loss. The
    classification loss is
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探讨了在不使用 ASM 的情况下同时执行去雾和高层计算机视觉任务。SDNet（Zhang et al., [2022a](#bib.bib178)）将语义分割和去雾结合成一个统一的框架，以利用语义先验作为优化过程的约束。通过将分割图的预测嵌入去雾网络，SDNet
    执行像素级分类损失和回归损失的联合优化。分类损失是
- en: '| (36) |  | $L_{sem}(s,s^{*})=-\frac{1}{P}\sum_{i}s_{i}^{*}{\log(s_{i})},$
    |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $L_{sem}(s,s^{*})=-\frac{1}{P}\sum_{i}s_{i}^{*}{\log(s_{i})},$
    |  |'
- en: where $P$ is the total number of pixels; $s_{i}$ is the class prediction at
    position $i$; $s^{*}$ denotes the ground truth semantic annotation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 是像素的总数；$s_{i}$ 是位置 $i$ 的类别预测；$s^{*}$ 表示真实的语义标注。
- en: (Zhang et al., [2022a](#bib.bib178), [2020g](#bib.bib186); Li et al., [2020c](#bib.bib82))
    show that segmentation and detection can be performed by embedding with dehazing
    networks. The way of joint dehazing task and high-level vision task may reduce
    the computational load to a certain extent by sharing the learned features, which
    can expand the goal of dehazing research.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (Zhang et al., [2022a](#bib.bib178), [2020g](#bib.bib186); Li et al., [2020c](#bib.bib82))
    表明，可以通过将去雾网络嵌入实现分割和检测。联合去雾任务和高层视觉任务的方式可能通过共享学习到的特征在一定程度上减少计算负担，这可以扩展去雾研究的目标。
- en: 3.17\. End-to-end CNN
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.17\. 端到端 CNN
- en: The “end-to-end” CNN stands for the non-ASM-based supervised algorithms, which
    usually consist of well-designed neural networks that take a single hazy image
    as input and a haze-free image as output. Networks based on different ideas are
    adopted, which are summarized as follows.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: “端到端” CNN 代表非 ASM 基于的监督算法，这些算法通常由设计良好的神经网络组成，该网络以单张有雾图像作为输入，输出无雾图像。采用了基于不同思想的网络，概述如下。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention mechanism: FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)), HFF (Zhang
    et al., [2022c](#bib.bib182)).'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力机制：FFA-Net（Qin 等，[2020](#bib.bib109)）、GridDehazeNet（Liu 等，[2019a](#bib.bib91)）、SAN（Liang
    等，[2019](#bib.bib85)）、HFF（Zhang 等，[2022c](#bib.bib182)）。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Encoder-Decoder: CAE (Chen and Lai, [2019](#bib.bib15)).'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器-解码器：CAE（Chen 和 Lai，[2019](#bib.bib15)）。
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Based on dense block: 123-CEDH (Guo et al., [2019a](#bib.bib48)).'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于密集块：123-CEDH（Guo 等，[2019a](#bib.bib48)）。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'U-shaped structure: DSEU (Lee et al., [2020b](#bib.bib70)), MSBDN (Dong et al.,
    [2020b](#bib.bib29)).'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: U 型结构：DSEU（Lee 等，[2020b](#bib.bib70)）、MSBDN（Dong 等，[2020b](#bib.bib29)）。
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical network: DMHN (Das and Dutta, [2020](#bib.bib25)).'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次网络：DMHN（Das 和 Dutta，[2020](#bib.bib25)）。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion with bilateral grid learning: 4kDehazing (Zheng et al., [2021](#bib.bib190)).'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双边网格学习融合：4kDehazing（Zheng 等，[2021](#bib.bib190)）。
- en: The end-to-end dehazing networks have an important impact on the entire dehazing
    field, proving that numerous deep learning models are beneficial to the dehazing
    task.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端去雾网络对整个去雾领域产生了重要影响，证明了众多深度学习模型对去雾任务是有益的。
- en: 4\. Semi-supervised Dehazing
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 半监督去雾
- en: Compared with the research on supervised methods, semi-supervised dehazing (Zhao
    et al., [2021b](#bib.bib189); Chen et al., [2021b](#bib.bib21)) algorithms have
    received relatively less attention. An important advantage of semi-supervised
    methods is the ability to utilize both labeled and unlabeled datasets. Therefore,
    compared to the fully supervised dehazing model, the semi-supervised dehazing
    model can alleviate the requirement of paired hazy and haze-free images. For dehazing
    task, labeled datasets are usually synthetic or artificially generated, while
    unlabeled datasets usually contain real world hazy images. According to the analysis
    of the dataset in Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning"), there are inherent
    differences between synthetic data and real world data. Therefore, semi-supervised
    algorithms usually have the ability to mitigate the gaps between synthetic domain
    and the real domain.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 与对监督方法的研究相比，半监督去雾（Zhao 等，[2021b](#bib.bib189); Chen 等，[2021b](#bib.bib21)）算法获得的关注相对较少。半监督方法的一个重要优势是能够利用标记数据集和未标记数据集。因此，与完全监督的去雾模型相比，半监督去雾模型可以缓解配对的有雾和无雾图像的需求。对于去雾任务，标记数据集通常是合成的或人工生成的，而未标记数据集通常包含真实世界的有雾图像。根据第
    [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") 节的数据集分析，合成数据和真实世界数据之间存在固有差异。因此，半监督算法通常具有减轻合成域和真实域之间差距的能力。
- en: Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the principles
    of various semi-supervised dehazing models, where the network is a schematic diagram.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") 展示了各种半监督去雾模型的原理，其中网络是示意图。
- en: '![Refer to caption](img/9dbf265897b637151ab61c1b6178b0e4.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9dbf265897b637151ab61c1b6178b0e4.png)'
- en: (a) Pretrain and finetune              (b) Disentangled and reconstruction          
    (c) Two branches training
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预训练和微调              (b) 解耦和重建           (c) 两分支训练
- en: Figure 8\. Semi-supervised dehazing methods.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 半监督去雾方法。
- en: 4.1\. Pretrain Backbone and Finetune
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 预训练骨干网和微调
- en: PSD (Chen et al., [2021b](#bib.bib21)) proposes a domain adaptation method that
    can be extended by existing models. It first uses a powerful backbone network
    for pre-training to obtain a base model suitable for synthetic data. Then the
    fine-tuning of the real domain in an unsupervised manner is applied to the well-trained
    network, thereby improving the generalization ability of the model to real world
    hazy images. To achieve fine-tuning in the real domain, PSD combines DCP loss,
    Bright Channel Prior (BCP) (Wang et al., [2013](#bib.bib143)) loss and Contrast
    Limited Adaptive Histogram Equalization (CLAHE) loss together. BCP loss and CLAHE
    loss are
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: PSD（Chen 等人, [2021b](#bib.bib21)）提出了一种可以通过现有模型扩展的领域适应方法。它首先使用强大的骨干网络进行预训练，以获得适用于合成数据的基础模型。然后，对真实领域进行无监督微调，从而提高模型对真实世界雾图像的泛化能力。为了在真实领域实现微调，PSD
    将 DCP 损失、亮通道先验（BCP）（Wang 等人, [2013](#bib.bib143)）损失和对比度限制自适应直方图均衡（CLAHE）损失结合在一起。BCP
    损失和 CLAHE 损失为
- en: '| (37) |  | $L_{BCP}=&#124;&#124;t_{BCP}(x)-t_{pred}(x)&#124;&#124;_{1},$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $L_{BCP}=&#124;&#124;t_{BCP}(x)-t_{pred}(x)&#124;&#124;_{1},$ |  |'
- en: '| (38) |  | $L_{CLAHE}=&#124;&#124;I(x)-I_{CLAHE}(x)&#124;&#124;_{1},$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $L_{CLAHE}=&#124;&#124;I(x)-I_{CLAHE}(x)&#124;&#124;_{1},$ |  |'
- en: where $t_{BCP}$ represents the transmission map estimated by BCP and $t_{pred}$
    is the predicted output of the network. $I_{CLAHE}$ is the hazy image reconstructed
    by CLAHE and other physical parameters.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{BCP}$ 表示 BCP 估计的传输图，$t_{pred}$ 是网络的预测输出。$I_{CLAHE}$ 是由 CLAHE 和其他物理参数重建的雾图像。
- en: 'During fine-tuning, the model may forget the useful knowledge it learned during
    the pre-training phase. Therefore, PSD proposes a feature-level constraint, which
    is achieved by calculating the feature map difference between the fine-tuning
    network and the pre-trained network. By feeding the synthetic data and real data
    into the fine-tuning network and the pre-trained network, respectively, four feature
    maps can be obtained: $F_{syn}^{tune}$, $F_{syn}^{pre}$, $F_{real}^{tune}$, and
    $F_{real}^{pre}$. Then, the loss for preventing knowledge forgetting can be calculated
    as'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，模型可能会忘记在预训练阶段学到的有用知识。因此，PSD 提出了特征级约束，通过计算微调网络与预训练网络之间的特征图差异来实现。通过分别将合成数据和真实数据输入微调网络和预训练网络，可以获得四个特征图：$F_{syn}^{tune}$、$F_{syn}^{pre}$、$F_{real}^{tune}$
    和 $F_{real}^{pre}$。然后，可以计算防止知识遗忘的损失：
- en: '| (39) |  | $L_{lwf}=&#124;&#124;F_{syn}^{tune}-F_{syn}^{pre}&#124;&#124;_{1}+&#124;&#124;F_{real}^{tune}-F_{real}^{pre}&#124;&#124;_{1}.$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $L_{lwf}=&#124;&#124;F_{syn}^{tune}-F_{syn}^{pre}&#124;&#124;_{1}+&#124;&#124;F_{real}^{tune}-F_{real}^{pre}&#124;&#124;_{1}.$
    |  |'
- en: As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(a), supervised
    pre-training is performed first, and then the dehazing network is fine-tuned in
    an unsupervised form.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")(a) 所示，首先进行监督预训练，然后在无监督形式下对去雾网络进行微调。
- en: SSDT (Zhang and Li, [2021](#bib.bib172)) uses the encoder-decoder network for
    pre-training in the form of domain translation. After pre-training, two encoders
    and one decoder are selected to obtain the holistic dehazing network $G(\cdot)$.
    Then, $G(\cdot)$ is fine-tuned with synthetic hazy images and real world hazy
    images. The two-stage method described above needs to ensure that the pre-training
    process can meet the accuracy requirements, otherwise it may bring accumulated
    errors to the fine-tuning process.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: SSDT（Zhang 和 Li, [2021](#bib.bib172)）使用编码器-解码器网络进行领域翻译形式的预训练。预训练后，选择两个编码器和一个解码器来获得整体去雾网络
    $G(\cdot)$。然后，$G(\cdot)$ 通过合成雾图像和真实世界雾图像进行微调。上述两阶段方法需要确保预训练过程能够满足准确性要求，否则可能会将累积误差带入微调过程。
- en: 4.2\. Disentangled and Reconstruction
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 解耦与重建
- en: From the perspective of dual learning, the dehazing task and the haze generation
    task can be able to assist each other. Based on this assumption, DCNet (Chen et al.,
    [2021c](#bib.bib22)) proposes a dual-task cycle network that jointly utilizes
    labeled dataset $N$ and unlabeled dataset $M$ by dehazing network $DN(\cdot)$
    and haze generation network $HGN(\cdot)$. The total loss is combine by dehazing
    loss $L_{DN}$ and reconstruction loss $L_{HGN}$, that is $L_{total}=L_{DN}+L_{HGN}$,
    as
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\displaystyle L_{total}=$ | $\displaystyle\sum_{i=1}^{M+N}P(I_{i}(x))L(DN(I_{i}(x)),J_{i}(x),\epsilon_{1})+\lambda{L(HGN(DN(I_{i}(x))),I_{i}(x),\epsilon_{2})},$
    |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: where $\epsilon_{1}$ and $\epsilon_{2}$ are regularizing hyper-parameter; $P(I_{i}(x))$
    equals $1$ when the $I_{i}(x)$ comes from the labeled dataset $N$, and equals
    $0$ otherwise. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    (b), the predicted haze-free image $J_{pred}(x)$ is first obtained by the left
    network, and then the hazy image $I_{rec}(x)$ is reconstructed by the right network.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Liu et al. (Liu et al., [2021](#bib.bib93)) use a disentangled image dehazing
    network (DID-Net) and a disentangled-consistency mean-teacher network (DMT-Net)
    to combine labeled and unlabeled data. DID-Net is responsible for disentangling
    the hazy image into a haze-free image, the transmission map, and the global atmospheric
    light. DMT-Net is used to jointly exploit the labeled synthetic data and unlabeled
    real world data through disentangled consistency loss. The supervised loss consists
    of four terms: haze-free image prediction $L_{J}^{s}$, transmission map prediction
    $L_{t}^{s}$, atmospheric light prediction $L_{A}^{s}$, and hazy image reconstruction
    $L_{rec}$:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{J}^{s}=&#124;&#124;G_{J}-P_{J}&#124;&#124;_{1}+&#124;&#124;G_{J}-\hat{P}_{J}&#124;&#124;_{1},$
    |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{T}^{s}=&#124;&#124;G_{T}-P_{T}&#124;&#124;_{1}+&#124;&#124;G_{T}-\hat{P}_{T}&#124;&#124;_{1},$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{A}^{s}=&#124;&#124;G_{A}-P_{A}&#124;&#124;_{1}+&#124;&#124;G_{A}-\hat{P}_{A}&#124;&#124;_{1},$
    |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| (41) |  | $\displaystyle L_{rec}=&#124;&#124;I(x)-P_{I}&#124;&#124;_{1}+&#124;&#124;I(x)-\hat{P}_{I}&#124;&#124;_{1},$
    |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: 'where $G$ stands for ground-truth, $P$ is the prediction of the first stage,
    and $\hat{P}$ is the prediction of the second stage. The supervised loss $L^{s}(x)$
    is the weighted sum of the above four losses, that is $L^{s}(x)=L_{J}^{s}+\alpha_{1}L_{T}^{s}+\alpha_{2}L_{A}^{s}+\alpha_{3}L_{rec}$.
    For unlabeled data, consistency loss is used to constrain the teacher network
    $T$ and the student network $S$:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{J}^{c}=&#124;&#124;S_{J}-T_{J}&#124;&#124;_{1}+&#124;&#124;S_{\hat{J}}-T_{\hat{J}}&#124;&#124;_{1},$
    |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{T}^{c}=&#124;&#124;S_{T}-T_{T}&#124;&#124;_{1}+&#124;&#124;S_{\hat{T}}-T_{\hat{T}}&#124;&#124;_{1},$
    |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{A}^{c}=&#124;&#124;S_{A}-T_{A}&#124;&#124;_{1}+&#124;&#124;S_{\hat{A}}-T_{\hat{A}}&#124;&#124;_{1},$
    |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{A}^{c}=||S_{A}-T_{A}||_{1}+||S_{\hat{A}}-T_{\hat{A}}||_{1},$
    |  |'
- en: '| (42) |  | $\displaystyle L_{rec}^{c}=&#124;&#124;S_{I}-T_{I}&#124;&#124;_{1}+&#124;&#124;S_{\hat{I}}-T_{\hat{I}}&#124;&#124;_{1},$
    |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $\displaystyle L_{rec}^{c}=||S_{I}-T_{I}||_{1}+||S_{\hat{I}}-T_{\hat{I}}||_{1},$
    |  |'
- en: where the subscript $\hat{J}$, $\hat{T}$, $\hat{A}$ and $\hat{I}$ denote the
    results predicted in second stage. Thus, the loss for the unlabeled dataset is
    $L^{c}(y)=L_{J}^{c}+\alpha_{4}L_{T}^{c}+\alpha_{5}L_{A}^{c}+\alpha_{6}L_{rec}^{c}$.
    The final loss function of the semi-supervised framework consists of a supervised
    loss on the labeled dataset $N$ and a consistency loss on the unlabeled dataset
    $M$, that is $L_{total}=\sum_{x\in N}{L^{s}{(x)}}+\lambda\sum_{y\in M}{L^{c}{(y)}}$.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标 $\hat{J}$、$\hat{T}$、$\hat{A}$ 和 $\hat{I}$ 表示第二阶段预测的结果。因此，无标记数据集的损失为 $L^{c}(y)=L_{J}^{c}+\alpha_{4}L_{T}^{c}+\alpha_{5}L_{A}^{c}+\alpha_{6}L_{rec}^{c}$。半监督框架的最终损失函数由标记数据集
    $N$ 上的监督损失和无标记数据集 $M$ 上的一致性损失组成，即 $L_{total}=\sum_{x\in N}{L^{s}{(x)}}+\lambda\sum_{y\in
    M}{L^{c}{(y)}}$。
- en: CCDM (Zhang et al., [2020f](#bib.bib180)) designs a color-constrained dehazing
    model that can be extended to a semi-supervised framework, which is achieved by
    the reconstruction of hazy images, the smoothing loss of $t(x)$ and $A$, etc.
    Experiments (Liu et al., [2021](#bib.bib93); Zhang et al., [2020f](#bib.bib180);
    Chen et al., [2021c](#bib.bib22)) show that the reconstruction of hazy images
    can provide effective supervisory signals in an unsupervised manner, which is
    instructive for semi-supervised frameworks.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: CCDM (Zhang et al., [2020f](#bib.bib180)) 设计了一个颜色约束去雾模型，可以扩展到半监督框架，这通过雾霾图像的重建、$t(x)$
    和 $A$ 的平滑损失等实现。实验 (Liu et al., [2021](#bib.bib93); Zhang et al., [2020f](#bib.bib180);
    Chen et al., [2021c](#bib.bib22)) 表明，雾霾图像的重建可以以无监督的方式提供有效的监督信号，这对半监督框架具有指导意义。
- en: 4.3\. Two-branches Training
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 两分支训练
- en: 'SSID (Li et al., [2020a](#bib.bib79)) designs an end-to-end network that integrates
    a supervised learning branch and an unsupervised learning branch. The training
    process of SSID uses both the labeled dataset and the unlabeled dataset by the
    following process:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: SSID (Li et al., [2020a](#bib.bib79)) 设计了一个集成了监督学习分支和无监督学习分支的端到端网络。SSID 的训练过程使用了标记数据集和无标记数据集，具体过程如下：
- en: '| (43) |  | $J_{pred}(x)=G(I(x)),$ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $J_{pred}(x)=G(I(x)),$ |  |'
- en: where $G(\cdot)$ consists of a supervised part $G_{s}(\cdot)$ and an unsupervised
    part $G_{u}(\cdot)$. The supervised loss composed of L2 loss and perceptual loss
    is used to ensure that the predicted image $J_{pred}(x)$ and its corresponding
    ground truth image are as close as possible, which is the same as the supervised
    dehazing algorithm. A combination of total variation loss and dark channel loss
    is used for unsupervised training. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\.
    Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")(c), supervised training and unsupervised training
    are performed in a shared weight manner.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(\cdot)$ 由一个监督部分 $G_{s}(\cdot)$ 和一个无监督部分 $G_{u}(\cdot)$ 组成。监督损失由 L2 损失和感知损失组成，用于确保预测图像
    $J_{pred}(x)$ 与其对应的真实图像尽可能接近，这与监督去雾算法相同。无监督训练使用总变差损失和暗通道损失的组合。如图 [8](#S4.F8 "Figure
    8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")(c) 所示，监督训练和无监督训练以共享权重的方式进行。
- en: SSIDN (An et al., [2022](#bib.bib2)) also combines supervised and unsupervised
    training processes. Supervised training is used to learn the mapping from hazy
    to haze-free images. With dark channel prior and bright channel prior (Wang et al.,
    [2013](#bib.bib143)) guiding the training process, the unsupervised branch incorporates
    the estimation of the transmission map and atmospheric light.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: SSIDN (An et al., [2022](#bib.bib2)) 也结合了监督和无监督训练过程。监督训练用于学习从雾霾图像到无雾图像的映射。在暗通道先验和亮通道先验
    (Wang et al., [2013](#bib.bib143)) 指导训练过程的情况下，无监督分支结合了传输图和大气光的估计。
- en: 'The DAID (Shao et al., [2020](#bib.bib119)) adopts a domain adaptation model
    to jointly train multi-subnetworks. The $G_{S\to{R}}$ and $G_{R\to{S}}$ are the
    image translation modules used for the translation between the synthetic domain
    and the real domain, where $R$ and $S$ stand for real domain and synthetic domain,
    respectively. By using an image-level discriminator $D_{R}^{img}$ and a feature-level
    discriminator $D_{R}^{feat}$, the adversarial loss of the translation process
    can be calculated. In order to ensure that the content of images are maintained
    during the translation process, DAID uses cycle consistency loss to constrain
    the translation network. Furthermore, identity mapping loss is also used in the
    conversion process to restrict the identity of the image generation process in
    two domains. The training of the dehazing network $G_{R}$ is a combination of
    unsupervised and supervised processes. The supervised process minimizes the $L_{rm}$
    loss to make the dehazed image $J_{S\to{R}}$ closer to the corresponding haze-free
    image $Y_{S}$:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '| (44) |  | $L_{rm}=&#124;&#124;J_{S\to{R}}-Y_{S}&#124;&#124;_{2}^{2}.$ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: Then, the total variation loss and dark channel loss are used as unsupervised
    losses. For the training of the dehazing network $G_{S}$, there is also a combination
    of supervised loss and unsupervised loss. With the help of domain transformation
    and unsupervised loss, DAID can effectively reduce the gap between the synthetic
    domain and the real domain.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: As introduced above (Li et al., [2020a](#bib.bib79); An et al., [2022](#bib.bib2);
    Shao et al., [2020](#bib.bib119)), using supervised branch and unsupervised branch
    for joint training to build a semi-supervised framework can effectively alleviate
    the problem of domain shift.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Unsupervised Dehazing
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The supervised and semi-supervised dehazing methods have achieved excellent
    performance on public datasets. However, the training process requires paired
    data (i.e. hazy images and haze-free images / transmission maps), which are difficult
    to obtain in the real world. For outdoor scenes containing grass, water or moving
    objects, it is difficult to guarantee that two images taken under hazy and clear
    weather have exactly the same content. If the haze-free labels are not accurate
    enough, the accuracy of dehazed images will be reduced. Therefore,  (Engin et al.,
    [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al., [2020a](#bib.bib90);
    Wei et al., [2021](#bib.bib147)) explore dehazing algorithms in an unsupervised
    way.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a997349418cfbe573bc502ed67c9e046.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: (a) Between two domains      (b) Among multi-domains             (c) Without
    target               (d) Zero-shot
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Schematic diagram of the unsupervised dehazing algorithms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Unsupervised Domain Transfer
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the study of image style transfer and image-to-image translation, CycleGAN (Zhu
    et al., [2017](#bib.bib193)) is proposed, which provides a way for learning the
    bidirectional mapping functions between two domains. Inspired by CycleGAN, (Engin
    et al., [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al.,
    [2020a](#bib.bib90)) are designed for unsupervised transformation of hazy and
    haze-free / transmission domains. The Cycle-Dehaze (Engin et al., [2018](#bib.bib38))
    contains two generators $G$ and $F$, which are used to learn the mapping from
    hazy domain to haze-free domain and the reverse mapping, respectively. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") (a), the hazy and
    haze-free images are translated to each other by two generators. By sampling $x$
    and $y$ from the hazy domain $X$ and the haze-free domain $Y$, the Cycle-Dehaze
    uses the perceptual metric (denoted as $\psi(\cdot)$) to obtain the cyclic perceptual
    consistency loss:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '| (45) |  | $\displaystyle L_{cyc-p}=$ | $\displaystyle{&#124;&#124;\psi(x)-\psi(F(G(x)))}{&#124;&#124;}_{2}^{2}+{&#124;&#124;\psi(y)-\psi(G(F(y)))}{&#124;&#124;}_{2}^{2}.$
    |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: The overall loss function of Cycle-Dehaze is composed of cyclic perceptual consistency
    loss and Cycle-GAN’s loss function, which can alleviate the requirement of paired
    data. CDNet (Dudhane and Murala, [2019a](#bib.bib35)) also adopts a cycle-consistent
    adversarial approach for unsupervised dehazing network training. Unlike Cycle-Dehaze,
    CDNet embeds ASM into the network architecture for estimating transmission map,
    which enables it to acquire physical parameters while restoring haze-free images.
    E-CycleGAN (Liu et al., [2020a](#bib.bib90)) adds ASM and a priori statistical
    law to estimate atmospheric light on the basis of CycleGAN, which allows it to
    perform independent parameter estimation for sky regions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: USID (Huang et al., [2019](#bib.bib57)) introduces the concept of haze mask
    $H(x)$ and constant bias $\epsilon$, which can obtain the reformulated ASM as
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '| (46) |  | $J(x)=\frac{I(x)-A}{H(x)}+A+\epsilon.$ |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: By combining with cycle consistent loss, USID also removes the requirement of
    an explicit paired haze/depth data in an unsupervised multi-task learning manner.
    In order to decouple content and haze information, USID-DR (Liu, [2019](#bib.bib88))
    designs a content encoder and a haze encoder embedded in CycleGAN. This decoupling
    approach can enhance feature extraction and reconstruction during cycle consistent
    training. USID-DR proposes that making the output of the encoder conform to the
    Gaussian distribution by latent regression loss can improve the quality of hazy
    image synthesis process, thereby improving the overall dehazing performance. DCA-CycleGAN (Mo
    et al., [2022](#bib.bib99)) utilizes the dark channel to build the input and generates
    the attention for handling the nonhomogeneous haze. These CycleGAN-based methods
    demonstrate that unsupervised hazy and haze-free image domain transformation can
    achieve the same performance as supervised algorithms from both ASM-based and
    non-ASM-based perspectives.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合循环一致性损失，USID 还在无监督多任务学习的方式中去除了对显式配对雾霾/深度数据的要求。为了分离内容和雾霾信息，USID-DR（刘，[2019](#bib.bib88)）在
    CycleGAN 中设计了一个内容编码器和一个雾霾编码器。这种解耦方法可以增强在循环一致性训练过程中的特征提取和重建。USID-DR 提出，通过潜在回归损失使编码器的输出符合高斯分布，可以提高雾霾图像合成过程的质量，从而改善整体去雾性能。DCA-CycleGAN（Mo
    等，[2022](#bib.bib99)）利用暗通道构建输入并生成处理非均匀雾霾的注意力。这些基于 CycleGAN 的方法表明，无监督雾霾与无雾霾图像领域转换可以从
    ASM 基于和非 ASM 基于的角度实现与监督算法相同的性能。
- en: 'For the dehazing task, the haze density in hazy images can be very different.
    Most supervised, semi-supervised and unsupervised dehazing methods hold the view
    that hazy images and haze-free images should be treated as two domains, without
    considering the density among different examples. Recent work (Jin et al., [2020](#bib.bib62))
    proposes that it is an important issue to apply the haze density information to
    the unsupervised training process, thereby improving the generalization ability
    of the dehazing model to images with different densities of haze. An unsupervised
    conditional disentangle network, called UCDN (Jin et al., [2020](#bib.bib62)),
    is designed by incorporating conditional information into the training process
    of CycleGAN. Further, DHL-Dehaze (Cong et al., [2020](#bib.bib24)) analyzes multiple
    haze density levels of hazy images, and proposes that the difference in haze density
    should be fully utilized in the dehazing procedure. Compared to UCDN which contains
    four submodules, DHL-Dehaze has only two networks to train. The idea of DHL-Dehaze
    is based on the research of multi-domain image-to-image translation. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")(b), the images
    $I_{1}(x)$, $I_{2}(x)$ and $I_{3}(x)$ of different densities can be transformed
    to other domains, where $I_{3}(x)$ can be explained as the haze-free domain. The
    training of DHL-Dehaze consists of a adversarial process and a classification
    process. By sampling $X_{ori}=\{x_{ori}^{(1)},x_{ori}^{(2)},\ldots,x_{ori}^{(m)}\}$
    with corresponding density labels $L_{ori}=\{l_{ori}^{(1)},l_{ori}^{(2)},\ldots,l_{ori}^{(m)}\}$,
    the classification loss $L_{cls}^{ori}$ and adversarial loss $L_{dis}^{ori}$ in
    the source domain can be obtained by ([47](#S5.E47 "In 5.1\. Unsupervised Domain
    Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning")) and ([48](#S5.E48 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '| (47) |  | $L_{cls}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{ori}^{(i)}\mid
    x_{ori}^{(i)}),$ |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| (48) |  | $L_{dis}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{dis}(x_{ori}^{(i)}).$
    |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: 'In order to generate the images of the target domain, the labels of target
    domain $L_{tar}=\{l_{tar}^{(1)},l_{tar}^{(2)},\ldots,l_{tar}^{(m)}\}$ are required.
    DHL-Dehaze sends $X_{ori}$ and $L_{tar}$ into the multi-scale generator (MST)
    together, and obtains new images with the same attributes as $L_{tar}$:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '| (49) |  | $X_{tar}=MST(X_{ori},L_{tar}).$ |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: 'Then, the classification loss $L_{cls}^{tar}$ and adversarial loss $L_{dis}^{tar}$
    of the target domain image can be obtained by using ([50](#S5.E50 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")) and ([51](#S5.E51 "In 5.1\.
    Unsupervised Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '| (50) |  | $L_{cls}^{tar}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{tar}^{(i)}\mid
    x_{tar}^{(i)}),$ |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| (51) |  | $L_{dis}^{tar}=\frac{1}{m}\sum_{i=1}^{m}logD_{dis}(x_{tar}^{(i)}).$
    |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: It is worth noting that UCDN and DHL-Dehaze do not use the hazy and haze-free
    images of the same scene for supervision, but apply the density of haze as the
    supervisory information for the training process.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The training of unsupervised domain transformation algorithms is more difficult
    than that of supervised algorithms. The convergence of GAN-based domain transformation
    algorithms is difficult to determine, which may lead to over-enhanced images.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Learning without Haze-free Images
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process of CycleGAN-based methods and DHL-Dehaze does not require
    paired data, which has greatly reduced the difficulty of data collection. Further,
    Deep-DCP (Golts et al., [2020](#bib.bib44)) proposes to use only hazy image for
    training process. The strategy of domain translation dehazing algorithms is to
    learn the mapping relationship between hazy and haze-free / transmission domains,
    while the main idea of Deep-DCP is to minimize the DCP (He et al., [2010](#bib.bib52))
    energy function. According to statistical assumption, the transmission map can
    be estimated in an unsupervised way. Based on the estimated transmission map and
    soft matting, the energy function $E(t_{\theta},I(x))$ can be obtained, where
    $\theta$ is the parameters for tuning. Thus, the goal of training is to minimize
    the energy function:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '| (52) |  | $\theta^{*}=\mathop{\arg\min}_{\theta}[\frac{1}{N}\sum_{i=1}^{N}E(t_{\theta}(I_{i}(x)),I_{i}(x))].$
    |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(c), the
    network can automatically learn the mapping from hazy to haze-free images without
    requiring target domain labels.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Unsupervised Image Decomposition
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Double-DIP (Gandelsman et al., [2019](#bib.bib42)) proposes an unsupervised
    hierarchical decoupling framework based on the observation that the internal statistics
    of a mixed layer is more complex than the single layer that composes it. Suppose
    $Z$ is a linear sum of independent random variables $X$ and $Y$. From a statistical
    point of view, the entropy of $Z$ is larger than its independent components, that
    is, $H(Z)\geq\max{\{H(X),H(Y)\}}$. Based on this, Double-DIP proposes the loss
    for image layer decomposition:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '| (53) |  | $L=L_{reconst}+\alpha\cdot L_{excl}+\beta\cdot L_{reg},$ |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: where $L_{reconst}$ represents the reconstruction loss of the hazy image, $L_{excl}$
    is the exclusion loss between the two DIPs, and $L_{reg}$ is the regularization
    loss used to obtain the continuous and smooth transmission map.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Zero-Shot Learning for Dehazing
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data-driven unsupervised dehazing methods have achieved impressive performance.
    Unlike those models that require sufficient data to perform network training,
    ZID (Li et al., [2020b](#bib.bib72)) proposes a neural network dehazing process
    that only requires a single example. ZID has further reduced the dependence of
    the parameter learning process on data by combining the advantages of unsupervised
    learning and zero-shot learning. Three sub-networks $f_{J}(\cdot)$ (J-Net), $f_{T}(\cdot)$
    (T-Net) and $f_{A}(\cdot)$ (A-Net) are used to estimate the $J(x)$, $t(x)$ and
    $A$, respectively. By the reconstruction process, the hazy image $I(x)$ can be
    disentangled by minimizing $L_{rec}$ loss:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '| (54) |  | $L_{rec}=&#124;&#124;I_{rec}(x)-I(x)&#124;&#124;_{p},$ |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: 'where $I_{rec}(x)$ is reconstructed hazy image, and $p$ denotes $p$-norm. The
    disentangled atmospheric light $f_{A}(x)$ and Kullback-Leibler divergence are
    used to obtain the atmospheric light $A$ as shown in the following formula:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '| (55) |  | $\displaystyle L_{A}=$ | $\displaystyle L_{H}+L_{KL}=&#124;&#124;f_{A}(x)-A(x)&#124;&#124;_{p}+KL(N(u_{z},\delta_{z}^{2})&#124;&#124;N(0,I)),$
    |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: where $L_{H}$ is the loss for $f_{A}(x)$ and initial hint value $A(x)$ is automatically
    learned from data. It should be noted that $A(x)$ in ZID is not the same as $A$
    in ASM; $N(0,I)$ stands for Gaussian distribution; $z$ is learned from input $x$.
    The unsupervised channel loss $L_{J}$ used in J-Net is for the decomposing process
    of the haze-free image $J(x)$. The $L_{J}$ is calculated based on the dark channel,
    and the formula is $L_{J}=||\min_{c\in\{r,g,b\}}(J^{c}(y))||_{p}$, where $c$ denotes
    color channel and $y$ stands for local patch of the J-Net output. The purpose
    of $L_{reg}$ is to enhance the stability of the model and make $A$ and $t(x)$
    smooth. The overall loss function of ZID is
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '| (56) |  | $L=L_{rec}+L_{A}+L_{J}+L_{reg}.$ |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: YOLY (Li et al., [2021a](#bib.bib71)) uses three joint disentanglement subnetworks
    for clear image and physical parameter estimation, enabling unsupervised and untrained
    haze removal. As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(d),
    ZID and YOLY can learn the mapping from hazy to haze-free image using a single
    unlabeled example.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of the zero-shot algorithm is that the generalization ability
    is limited, and the network must be retrained for each unseen example to get good
    dehazing performance.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Experiment and Performance Analysis
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides quantitative and qualitative analysis of the dehazing
    performance of the baseline supervised, semi-supervised and unsupervised algorithms.
    The training examples used in the experiments are the indoor data ITS and outdoor
    data OTS from RESIDE (Li et al., [2019c](#bib.bib74)). In order to accurately
    compare the dehazed images and the real clear images, the indoor and outdoor images
    included in the SOTS provided by RESIDE are used as the test set. Aiming at ensuring
    the fairness and representativeness of the experimental results, 10 representative
    algorithms are selected for comparison:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'supervised: AOD-Net (Li et al., [2017a](#bib.bib73)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)), DMMFD (Deng et al., [2019](#bib.bib27)), FFA-Net (Qin et al.,
    [2020](#bib.bib109)), GCA-Net (Chen et al., [2019c](#bib.bib14)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), MSBDN (Dong et al., [2020b](#bib.bib29)).'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'semi-supervised: SSID (Li et al., [2020a](#bib.bib79)).'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'unsupervised: Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), ZID (Li et al.,
    [2020b](#bib.bib72)).'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The framework used for the algorithm implementation is PyTorch, and the running
    platform is NVIDIA Tesla V100 32 GB (2 GPUs). Except for the zero-shot ZID, all
    algorithms use a batch size of 8 in the training phase. For the ITS and OTS datasets,
    the training epochs are set according to (Qin et al., [2020](#bib.bib109)) and (Liu
    et al., [2019a](#bib.bib91)), respectively. For the supervised dehazing methods,
    four loss function strategies are adopted, including L1, L2, L1 + P, L2 + P, where
    P denotes the perceptual loss. For semi-supervised and unsupervised methods, the
    loss functions follow the settings in the respective papers. In the experiment,
    we first compared the convergence curves of different supervised algorithms when
    they were set to 0.001, 0.0005 or 0.0002, and selected the value with the best
    convergence effect as the best learning rate. After the training process, the
    loss values of all supervised models have stably converged. For semi-supervised
    and unsupervised algorithms, the learning rates used are those recommended in
    the corresponding papers.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to ensure as few interference factors as possible in the experiments,
    the following strategies are not used in the experiments: (1) pre-training, (2)
    dynamic learning rate adjustment, (3) larger batch size, and (4) data augmentation.
    Therefore, the quantitative results obtained in the following experiments may
    be a little lower than the best results provided in the papers corresponding to
    the various algorithms.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Tables [4](#S6.T4 "Table 4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5
    "Table 5 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") show the PSNR/SSIM
    values obtained on the indoor and outdoor test sets of various algorithms on RESIDE
    SOTS, where the highest values are shown in bold. It can be concluded that FFA-Net
    achieves the best performance on the indoor test set, with the highest PSNR and
    SSIM. The best performance among different algorithms on the outdoor test set
    is achieved by different methods. It can be also seen from Tables [4](#S6.T4 "Table
    4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5 "Table 5 ‣ 6\.
    Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") that the perceptual loss has a certain
    effect on the performance of the model, but it is not obvious.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6\. Experiment and Performance Analysis ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    and Figure [11](#S6.F11 "Figure 11 ‣ 6\. Experiment and Performance Analysis ‣
    A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    show the visual results achieved by the supervised, semi-supervised and unsupervised
    algorithms, respectively, where the loss function used by the supervised algorithms
    is the L1 loss. The visual results show that the dehazed images obtained by the
    supervised algorithms are closer to the real images in terms of color and detail
    than the semi-supervised/unsupervised algorithms.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Results on RESIDE SOTS indoor, where the SSIM and PNSR values are
    separated by the slash.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| AODNet | 0.839/19.375 | 0.841/19.454 | 0.851/19.576 | 0.839/19.388 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| 4kDehazing | 0.932/23.881 | 0.949/26.569 | 0.928/23.370 | 0.928/23.353 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| DMMDF | 0.959/30.585 | 0.960/30.383 | 0.961/30.631 | 0.961/30.725 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| FFA-Net | 0.983/32.730 | 0.983/32.536 | 0.978/32.224 | 0.978/32.178 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| GCA-Net | 0.924/25.044 | 0.930/25.638 | 0.917/24.716 | 0.909/24.714 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| GridDehazeNet | 0.962/25.671 | 0.962/25.655 | 0.935/22.940 | 0.940/23.052
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| MSBDN | 0.955/28.341 | 0.955/28.562 | 0.941/27.237 | 0.937/25.662 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| SSID | 0.814/20.959 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| Cycle-Dehaze | 0.810/18.880 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| ZID | 0.835/19.830 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: Table 5\. Results on RESIDE SOTS outdoor, where the SSIM and PNSR values are
    separated by the slash.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| AODNet | 0.913/23.613 | 0.917/23.683 | 0.912/23.253 | 0.916/23.488 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| 4kDehazing | 0.963/28.476 | 0.964/28.473 | 0.958/28.103 | 0.950/27.546 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| DMMDF | 0.905/25.805 | 0.963/30.237 | 0.963/30.535 | 0.965/30.682 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| FFA-Net | 0.921/27.126 | 0.925/27.299 | 0.913/28.176 | 0.920/28.441 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| GCA-Net | 0.953/27.784 | 0.949/27.559 | 0.946/27.20 | 0.943/27.273 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| GridDehazeNet | 0.964/28.296 | 0.964/28.388 | 0.963/27.807 | 0.963/27.766
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| MSBDN | 0.962/29.944 | 0.963/30.277 | 0.964/29.843 | 0.956/28.782 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| SSID | 0.840/20.905 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| Cycle-Dehaze | 0.861/20.347 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| ZID | 0.633/13.520 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/f2feb7aa286362078dfe8186f310925d.png)![Refer to caption](img/e418d85a6ae343188b08b02a3ad6a92c.png)![Refer
    to caption](img/2428c902fccbec1bcacf909a5e7bd4df.png)![Refer to caption](img/389fd6a26aa53b01a8883f7b6ec217ab.png)![Refer
    to caption](img/b1d6407d47e0a14538c9976fcdfbd68c.png)![Refer to caption](img/4594fcd3634f81c59bcd89ed6da37c86.png)![Refer
    to caption](img/b6f5f2c29d263145e069929d814bface.png)![Refer to caption](img/43af5737033d352a00d5d9036de2496d.png)![Refer
    to caption](img/98fee125a5ea6f50f70277d099bafde8.png)![Refer to caption](img/35e8cd57d7c7d47c8802ad55c3519e2d.png)![Refer
    to caption](img/c9f2c3d3419ab045ef4b7e8101ed2a12.png)![Refer to caption](img/dee2c61c33aa4a2634a4f2e0abcbd6ce.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0265fca5f17e3afe4bacdd124402b6ac.png) ![Refer to caption](img/637f18ddf1be133d9d9a6d45dd4cc022.png)
    ![Refer to caption](img/e30793f2ea4c8952e6aad7b332237c23.png) ![Refer to caption](img/8d16c4a1f9f7ae603e2295cc36996415.png)
    ![Refer to caption](img/0f645aef82073de109986af5c39e4cba.png) ![Refer to caption](img/99aebfe9185de93e7e66932d62d42898.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: '![Refer to caption](img/ad036f8379af4611d79a1147af6eaa15.png)![Refer to caption](img/53c7bd060ecb81cc8ee7992cf241b1d4.png)![Refer
    to caption](img/6d23ab8ae6c39bd64cd8340df126b260.png)![Refer to caption](img/8b4a6ffc7663471489aa68104c7f1056.png)![Refer
    to caption](img/e4b48e13428ee7136c32e0fbc905aebc.png)![Refer to caption](img/b4ec35306228437ec033cc0f53c76719.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10\. Visual results on RESIDE SOTS indoor.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: For a fair comparison of the computational speed of the baseline methods, the
    zero-shot-based ZID and high-resolution-based 4kDehazing are excluded. Fig. [12](#S6.F12
    "Figure 12 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the average
    time for each algorithm that run 1000 times with PyTorch framework. It can be
    seen that AODNet is the fastest, and can achieve real-time effects for inputs
    of different sizes. FFA-Net is the slowest, taking more than 0.3 seconds to process
    the $672\times 672$ input.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a4f08bf1d355f9968af362221046d1d.png)![Refer to caption](img/5b918adaccab1c195ed3dec9473afeec.png)![Refer
    to caption](img/08b5b66664070f079d16fb266494771e.png)![Refer to caption](img/9884719b91385e20926c3b32da2dce84.png)![Refer
    to caption](img/f0fba61952cd7d03bce779fcc518f669.png)![Refer to caption](img/0e1cffc4f3971a082e7d7c4f653c4c93.png)![Refer
    to caption](img/2f6c7b259446e234d2edb6ca5c54d084.png)![Refer to caption](img/9a7c55b7e8d9f4779a234ca3b9410640.png)![Refer
    to caption](img/f8b86157fa28cddb393ad089c7e207b5.png)![Refer to caption](img/e04635817fa8d55f6eb165a6e95fdfe6.png)![Refer
    to caption](img/6c0c8c5703ad99355a02a70f3975932f.png)![Refer to caption](img/bdac90bf8dac6d18140c54d8cf9ab913.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd980cba3ea42e600d042cd16e5b3e43.png) ![Refer to caption](img/839113d06d8afdc49672df5104101e1a.png)
    ![Refer to caption](img/cc48c167cfe455ef7c980f1e038e3a6f.png) ![Refer to caption](img/d54216a1933edd7e224448735e674f58.png)
    ![Refer to caption](img/2a23e72d85c231ac49545b13a7f24b3f.png) ![Refer to caption](img/e36b5bab9e12b91740e14c5fc89f7957.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: '![Refer to caption](img/0568101726b59880cc8237702f6897c3.png)![Refer to caption](img/f1df20ef1bda3e07de87344d5e2a0e92.png)![Refer
    to caption](img/c037f06c9e16f2dada648ff67e987a32.png)![Refer to caption](img/3530b94362a34dc9b8befdd019bc8518.png)![Refer
    to caption](img/7a74af0e6abdda5eddbced6819596ca5.png)![Refer to caption](img/6e96c861dce4ea57cad9321f7d7cb7f6.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Visual results on RESIDE SOTS outdoor.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24fcff6c5f47e30203db9e6bb2a5b858.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Running speed on different sizes of input.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Challenges and Opportunities
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For image dehazing task, the current supervised, semi-supervised and unsupervised
    methods have achieved good performance. However, there are problems that still
    exist and open issues that should be explored in the future research. Next, we
    will discuss these challenges.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. More Effective ASM
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current ASM has been proved to be suitable for describing the formation
    process of haze by many supervised, semi-supervised and unsupervised dehazing
    methods. However, recent work (Ju et al., [2021](#bib.bib65)) finds that the intrinsic
    limitation of ASM will cause a dim effect in the image after dehazing. By adding
    a new parameter, an enhanced ASM (EASM) (Ju et al., [2021](#bib.bib65)) is proposed.
    Improvements to ASM will have an important impact on the dehazing performance
    of existing methods that rely on ASM. Therefore, it is worth exploring a more
    accurate model of the haze formation process.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Shift between the Real Domain and Synthetic Domain
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current training process of dehazing models generally requires sufficient
    data. Since it is difficult to collect pairs of hazy and haze-free images in the
    real world, the synthesized data is needed by dehazing task. However, there are
    inherent differences between the synthesized hazy image and the real world hazy
    image. In order to solve the domain shift problem, the following three directions
    are worth exploring.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The haze synthesized based on ASM cannot completely simulate the formation of
    haze in the real world, so we can attempt to design a more realistic hazy image
    synthesis algorithm to compensate for the difference between domains.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By introducing domain adaptation, semi-supervised and unsupervised methods into
    network design, experiments show that these algorithms can perform well in real
    world dehazing task.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent work (MRFID/BeDDE) collected some real world paired data, but they did
    not contain as many examples as RESIDE. It is challenging to build a large-scale
    real world dataset that can be used for a large-capacity CNN-based dehazing model.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7.3\. Computational Efficiency and New Metrics
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dehazing models are often used as a preprocessing module for high-level
    computer vision tasks. For example, the lightweight AOD-Net is applied to object
    detection task. There are three issues that should be considered in the application
    of the dehazing model.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to help follow-up tasks, the dehazing model should ensure that the
    dehazed image is of high quality.
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inference speed of the model must be fast enough to meet the real-time requirement.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several end devices with small storage are sensitive to the number of parameters
    of the dehazing model.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, we need to balance quantitative performance, inference time and the
    number of parameters. High-quality dehazed image with high PSNR and SSIM are the
    main goals of the current research. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    discusses the computational efficiency of 14 models, which shows that several
    algorithms with excellent performance may be very time and memory consuming. Dehazing
    algorithms based on deep learning usually require the use of graphical computing
    units for model training and deployment. In real world applications, the forward
    inference speed of an algorithm is an important evaluation metric. 4kDehazing (Zheng
    et al., [2021](#bib.bib190)) explores fast dehazing of high-resolution input,
    which takes only $8$ ms ($125$ fps) to process a 4K ($3840\times 2160$) image
    on a single Titan RTX GPU. Future research can try to design a new evaluation
    metric that can comprehensively consider the dehazing quality, running speed and
    model size.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Perceptual Loss
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pre-trained model trained on a large-scale dataset can be used to calculate
    the perceptual loss. As shown in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\. Total Variation
    Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), many dehazing models use the
    perceptual loss obtained by VGG16 or VGG19 as a part of the overall loss to improve
    the quality of the final result. Thus, a natural question is, can the perceptual
    loss be the same as the L1/L2 loss as a general loss for image dehazing task?
    Alternatively, is there a more efficient and higher quality way to compute the
    perceptual loss? For example, is it a better solution to obtain perceptual loss
    by other pre-trained models? Currently, there is no comprehensive study on the
    relationship between perceptual loss and dehazing task.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. How Dehazing Methods Affect High-level Computer Vision Tasks
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many dehazing algorithms have been validated on high-level computer vision tasks,
    such as image segmentation and object detection. Experiments show that these dehazing
    methods can promote the performance of high-level computer vision tasks under
    hazy conditions. However, the recent study (Pei et al., [2018](#bib.bib108)) has
    shown that several dehazing algorithms may be ineffective for image classification,
    although their dehazing ability has been proven. The experimental results (Pei
    et al., [2018](#bib.bib108)) on synthetic and real hazy data show that several
    well-designed dehazing models have little positive effect on the performance of
    the classification model, and sometimes may reduce the classification accuracy
    to some extent.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Besides, domain adaptation and generalization methods have been proposed to
    deal with high-level computer vision tasks such as object detection (Wang et al.,
    [2022](#bib.bib139), [2021a](#bib.bib138)) and semantic segmentation (Zhang et al.,
    [2019b](#bib.bib173); Gao et al., [2021](#bib.bib43)) in bad weather conditions
    including haze, which can be treated as mitigating the side effect of haze in
    the feature space by aligning hazy image features with those clean ones. It will
    be an interesting research topic to investigate and reduce the influence of haze
    on high-level computer vision tasks both at image-level (i.e., dehazing) and feature-level
    (i.e., domain adaptation).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 7.6\. Prior Knowledge and Learning Model
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before deep learning was widely used in image dehazing task, image-based prior
    statistical knowledge was an important part for guiding the dehazing process.
    Now, extensive work has shown that deep learning techniques can effectively remove
    haze from images independently of physical model. At the same time, several recent
    studies have found that prior statistical knowledge can be used for semi-supervised (Li
    et al., [2020a](#bib.bib79)) and unsupervised (Golts et al., [2020](#bib.bib44))
    network training. Effective prior knowledge can reduce the model’s dependence
    on data to a certain extent and improve the generalization ability of the model.
    However, there is currently no evidence which statistical priors are always valid.
    Therefore, the general prior knowledge that can be used for the CNN-based dehazing
    algorithm is worth verifying.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey of deep learning-based dehazing research.
    First, the commonly used physical model, high-quality datasets, general loss functions,
    effective network modules and evaluation metrics are summarized. Then, supervised,
    semi-supervised and unsupervised dehazing studies are classified and analyzed
    from different technical perspectives. Moreover, quantitative and qualitative
    dehazing performance of various baselines are discussed. Finally, we discuss several
    valuable research directions and open issues.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  id: totrans-464
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work was supported in part by the grant of the National Science Foundation
    of China under Grant 62172090, 62172089; Alibaba Group through Alibaba Innovative
    Research Program; CAAI-Huawei MindSpore Open Fund. All correspondence should be
    directed to Xiaofeng Cong and Yuan Cao. Dr Jing Zhang is supported by ARC research
    project FL-170100117.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A What is not discussed in this survey
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We noticed that several papers’ formulation description and open source code
    may not be consistent. In particular, loss functions not provided in several papers
    are used in the training code. In this case, we use the loss functions provided
    in the paper as a guide, regardless of the code implementation. Further, several
    public papers that have not yet been published formally are not covered in this
    survey, such as those on arxiv, since their content may be changed in the future.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. (2022) Shunmin An, Xixia Huang, Le Wang, Linling Wang, and Zhangjing
    Zheng. 2022. Semi-Supervised image dehazing network. *The Visual Computer* 38,
    6 (2022), 2041–2055.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2016) Cosmin Ancuti, Codruta O Ancuti, and Christophe De Vleeschouwer.
    2016. D-hazy: A dataset to evaluate quantitatively dehazing algorithms. In *International
    Conference on Image Processing*. 2226–2230.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018a) Cosmin Ancuti, Codruta O Ancuti, and Radu Timofte. 2018a.
    Ntire 2018 challenge on image dehazing: Methods and results. In *Conference on
    Computer Vision and Pattern Recognition Workshops*. 891–901.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018b) Cosmin Ancuti, Codruta O Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018b. I-HAZE: a dehazing benchmark with real hazy and haze-free
    indoor images. In *International Conference on Advanced Concepts for Intelligent
    Vision Systems*. 620–631.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2019a) Codruta O Ancuti, Cosmin Ancuti, Mateu Sbert, and Radu
    Timofte. 2019a. Dense-Haze: A Benchmark for Image Dehazing with Dense-Haze and
    Haze-Free Images. In *International Conference on Image Processing*. 1014–1018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2020a) Codruta O. Ancuti, Cosmin Ancuti, and Radu Timofte. 2020a.
    NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1798–1805.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018c) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018c. O-HAZE: a dehazing benchmark with real hazy and haze-free
    outdoor images. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    754–762.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancuti et al. (2019b) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool,
    Lei Zhang, and Ming-Hsuan Yang. 2019b. NTIRE 2019 Image Dehazing Challenge Report.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 2241–2253.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancuti et al. (2020b) Codruta O Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu,
    and Radu Timofte. 2020b. NTIRE 2020 Challenge on NonHomogeneous Dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2029–2044.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Chaudhuri (2021) Sriparna Banerjee and Sheli Sinha Chaudhuri.
    2021. Nighttime Image-Dehazing: A Review and Quantitative Benchmarking. *Archives
    of Computational Methods in Engineering* 28 (2021), 2943–2975.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianco et al. (2019) Simone Bianco, Luigi Celona, Flavio Piccoli, and Raimondo
    Schettini. 2019. High-resolution single image dehazing using encoder-decoder architecture.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1927–1935.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2016) Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and Dacheng
    Tao. 2016. Dehazenet: An end-to-end system for single image haze removal. *IEEE
    Transactions on Image Processing* 25, 11 (2016), 5187–5198.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019c) Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Liheng
    Zhang, Dongdong Hou, Lu Yuan, and Gang Hua. 2019c. Gated Context Aggregation Network
    for Image Dehazing and Deraining. In *Winter Conference on Applications of Computer
    Vision*. 1375–1383.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Lai (2019) Rongsen Chen and Edmund M-K Lai. 2019. Convolutional Autoencoder
    For Single Image Dehazing.. In *International Conference on Image Processing*.
    4464–4468.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Shuxin Chen, Yizi Chen, Yanyun Qu, Jingying Huang, and Ming
    Hong. 2019a. Multi-scale adaptive dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2051–2059.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) Tianyi Chen, Jiahui Fu, Wentao Jiang, Chen Gao, and Si
    Liu. 2021a. SRKTDN: Applying Super Resolution Method to Dehazing Task. In *Conference
    on Computer Vision and Pattern Recognition*. 487–496.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Wei-Ting Chen, Jian-Jiun Ding, and Sy-Yen Kuo. 2019b. PMS-Net:
    Robust Haze Removal Based on Patch Map for Single Images. In *Conference on Computer
    Vision and Pattern Recognition*. 11673–11681.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Wei-Ting Chen, Hao-Yu Fang, Jian-Jiun Ding, and Sy-Yen Kuo.
    2020. PMHLD: patch map-based hybrid learning DehazeNet for single image haze removal.
    *IEEE Transactions on Image Processing* 29 (2020), 6773–6788.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019d) Xuesong Chen, Haihua Lu, Kaili Cheng, Yanbo Ma, Qiuhao Zhou,
    and Yong Zhao. 2019d. Sequentially refined spatial and channel-wise feature aggregation
    in encoder-decoder network for single image dehazing. In *International Conference
    on Image Processing*. 2776–2780.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021b) Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu. 2021b.
    PSD: Principled Synthetic-to-Real Dehazing Guided by Physical Priors. In *Conference
    on Computer Vision and Pattern Recognition*. 7180–7189.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021c) Zhihua Chen, Yu Zhou, Ping Li, Xiaoyu Chi, Lei Ma, and
    Bin Sheng. 2021c. DCNet: Dual-Task Cycle Network for End-to-End Image Dehazing.
    In *International Conference on Multimedia and Expo*. 1–6.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng and Zhao (2021) Lu Cheng and Li Zhao. 2021. Two-Stage Image Dehazing with
    Depth Information and Cross-Scale Non-Local Attention. In *International Conference
    on Big Data*. 3155–3162.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cong et al. (2020) Xiaofeng Cong, Jie Gui, Kai-Chao Miao, Jun Zhang, Bing Wang,
    and Peng Chen. 2020. Discrete Haze Level Dehazing Network. In *ACM International
    Conference on Multimedia*. 1828–1836.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Das and Dutta (2020) Sourya Dipta Das and Saikat Dutta. 2020. Fast deep multi-patch
    hierarchical network for nonhomogeneous image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 482–483.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Qili Deng, Ziling Huang, Chung-Chi Tsai, and Chia-Wen Lin.
    2020. Hardgan: A haze-aware representation distillation gan for single image dehazing.
    In *European Conference on Computer Vision*. 722–738.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2019) Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu,
    Qing Zhang, Jing Qin, and Pheng-Ann Heng. 2019. Deep multi-model fusion for single-image
    dehazing. In *International Conference on Computer Vision*. 2453–2462.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dharejo et al. (2021) Fayaz Ali Dharejo, Yuanchun Zhou, Farah Deeba, Munsif Ali
    Jatoi, Muhammad Ashfaq Khan, Ghulam Ali Mallah, Abdul Ghaffar, Muhammad Chhattal,
    Yi Du, and Xuezhi Wang. 2021. A deep hybrid neural network for single image dehazing
    via wavelet transform. *Optik* 231 (2021), 166462.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2020b) Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang,
    Fei Wang, and Ming-Hsuan Yang. 2020b. Multi-scale boosted dehazing network with
    dense feature fusion. In *Conference on Computer Vision and Pattern Recognition*.
    2157–2167.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2020c) Hang Dong, Xinyi Zhang, Yu Guo, and Fei Wang. 2020c. Deep
    multi-scale gabor wavelet network for image restoration. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2028–2032.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong and Pan (2020) Jiangxin Dong and Jinshan Pan. 2020. Physics-based feature
    dehazing networks. In *European Conference on Computer Vision*. 188–204.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2020a) Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, and Yu Qiao.
    2020a. FD-GAN: Generative Adversarial Networks with Fusion-Discriminator for Single
    Image Dehazing. In *AAAI Conference on Artificial Intelligence*. 10729–10736.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Li (2018) Yixin Du and Xin Li. 2018. Recursive deep residual learning
    for single image dehazing. In *Conference on Computer Vision and Pattern Recognition
    Workshops*. 730–737.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Li (2019) Yixin Du and Xin Li. 2019. Recursive image dehazing via perceptually
    optimized generative adversarial network (POGAN). In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 1824–1832.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane and Murala (2019a) Akshay Dudhane and Subrahmanyam Murala. 2019a. Cdnet:
    Single image de-hazing using unpaired adversarial training. In *Winter Conference
    on Applications of Computer Vision*. 1147–1155.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane and Murala (2019b) Akshay Dudhane and Subrahmanyam Murala. 2019b. RYF-Net:
    Deep fusion network for single image haze removal. *IEEE Transactions on Image
    Processing* 29 (2019), 628–640.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane et al. (2019) Akshay Dudhane, Harshjeet Singh Aulakh, and Subrahmanyam
    Murala. 2019. Ri-gan: An end-to-end network for single image haze removal. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 2014–2023.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engin et al. (2018) Deniz Engin, Anil Genç, and Hazim Kemal Ekenel. 2018. Cycle-dehaze:
    Enhanced cyclegan for single image dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 825–833.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2021) Zhengyun Fang, Ming Zhao, Zhengtao Yu, Meiyu Li, and Yong
    Yang. 2021. A guiding teaching and dual adversarial learning framework for a single
    image dehazing. *The Visual Computer* (2021), 1–13.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2021) Minghan Fu, Huan Liu, Yankun Yu, Jun Chen, and Keyan Wang.
    2021. DW-GAN: A Discrete Wavelet Transform GAN for NonHomogeneous Dehazing. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 203–212.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galdran et al. (2018) Adrian Galdran, Aitor Alvarez-Gila, Alessandro Bria, Javier
    Vazquez-Corral, and Marcelo Bertalmío. 2018. On the duality between retinex and
    image dehazing. In *Conference on Computer Vision and Pattern Recognition*. 8212–8221.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gandelsman et al. (2019) Yosef Gandelsman, Assaf Shocher, and Michal Irani.
    2019. “Double-DIP”: Unsupervised Image Decomposition via Coupled Deep-Image-Priors.
    In *Conference on Computer Vision and Pattern Recognition*. 11026–11035.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Li Gao, Jing Zhang, Lefei Zhang, and Dacheng Tao. 2021. Dsp:
    Dual soft-paste for unsupervised domain adaptive semantic segmentation. In *ACM
    International Conference on Multimedia*. 2825–2833.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Golts et al. (2020) Alona Golts, Daniel Freedman, and Michael Elad. 2020. Unsupervised
    Single Image Dehazing Using Dark Channel Prior Loss. *IEEE Transactions on Image
    Processing* 29 (2020), 2692–2701.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. 2021. Knowledge distillation: A survey. *International Journal of Computer
    Vision* 129, 6 (2021), 1789–1819.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2021) Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing
    Zhang, and Dacheng Tao. 2021. A Comprehensive Survey on Image Dehazing Based on
    Deep Learning. In *International Joint Conference on Artificial Intelligence*.
    4426–4433.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gui et al. (2022) Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping
    Ye. 2022. A review on generative adversarial networks: Algorithms, theory, and
    applications. *IEEE Transactions on Knowledge and Data Engineering* (2022).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019a) Tiantong Guo, Venkateswararao Cherukuri, and Vishal Monga.
    2019a. Dense ‘123’ color enhancement dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2131–2139.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019b) Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, and Vishal
    Monga. 2019b. Dense scene information estimation network for dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2122–2130.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Monga (2020) Tiantong Guo and Vishal Monga. 2020. Reinforced depth-aware
    deep learning for single image dehazing. In *International Conference on Acoustics,
    Speech and Signal Processing*. 8891–8895.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hambarde and Murala (2020) Praful Hambarde and Subrahmanyam Murala. 2020. S2dnet:
    Depth estimation from single image and sparse samples. *IEEE Transactions on Computational
    Imaging* 6 (2020), 806–817.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2010) Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Single image haze
    removal using dark channel prior. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence* 33, 12 (2010), 2341–2353.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Conference on Computer Vision
    and Pattern Recognition*. 770–778.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2019) Linyuan He, Junqiang Bai, and Meng Yang. 2019. Feature aggregation
    convolution network for haze removal. In *International Conference on Image Processing*.
    2806–2810.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Ming Hong, Yuan Xie, Cuihua Li, and Yanyun Qu. 2020. Distilling
    Image Dehazing With Heterogeneous Task Imitation. In *Conference on Computer Vision
    and Pattern Recognition*. 3462–3471.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. 2017. Densely connected convolutional networks. In *Conference on
    Computer Vision and Pattern Recognition*. 4700–4708.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Lu-Yao Huang, Jia-Li Yin, Bo-Hao Chen, and Shao-Zhen Ye.
    2019. Towards unsupervised single image dehazing with deep learning. In *International
    Conference on Image Processing*. 2741–2745.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2021) Pengcheng Huang, Li Zhao, Runhua Jiang, Tao Wang, and Xiaoqin
    Zhang. 2021. Self-filtering image dehazing with self-supporting module. *Neurocomputing*
    432 (2021), 57–69.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Yimin Huang, Yiyang Wang, and Zhixun Su. 2018. Single image
    dehazing via a joint deep modeling. In *International Conference on Image Processing*.
    2840–2844.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huynh-Thu and Ghanbari (2008) Quan Huynh-Thu and Mohammed Ghanbari. 2008. Scope
    of validity of PSNR in image/video quality assessment. *Electronics letters* 44,
    13 (2008), 800–801.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    2017. Image-to-image translation with conditional adversarial networks. In *Conference
    on Computer Vision and Pattern Recognition*. 1125–1134.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020) Yizhou Jin, Guangshuai Gao, Qingjie Liu, and Yunhong Wang.
    2020. Unsupervised conditional disentangle network for image dehazing. In *International
    Conference on Image Processing*. 963–967.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jo and Sim (2021) Eunsung Jo and Jae-Young Sim. 2021. Multi-Scale Selective
    Residual Learning for Non-Homogeneous Dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshop*. 507–515.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2016) Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
    Perceptual losses for real-time style transfer and super-resolution. In *European
    Conference on Computer Vision*. 694–711.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ju et al. (2021) Mingye Ju, Can Ding, Wenqi Ren, Yi Yang, Dengyin Zhang, and
    Y Jay Guo. 2021. Ide: Image dehazing and exposure using an enhanced atmospheric
    scattering model. *IEEE Transactions on Image Processing* 30 (2021), 2180–2192.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. In *Neural Information Processing Systems*. 18661–18673.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Guisik Kim, Sung Woo Park, and Junseok Kwon. 2021. Pixel-wise
    Wasserstein Autoencoder for Highly Generative Dehazing. *IEEE Transactions on
    Image Processing* 30 (2021), 5452–5462.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ledig et al. (2017) Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero,
    Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
    Zehan Wang, et al. 2017. Photo-realistic single image super-resolution using a
    generative adversarial network. In *Conference on Computer Vision and Pattern
    Recognition*. 4681–4690.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020a) Byeong-Uk Lee, Kyunghyun Lee, Jean Oh, and In So Kweon. 2020a.
    CNN-Based Simultaneous Dehazing and Depth Estimation. In *International Conference
    on Robotics and Automation*. 9722–9728.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020b) Yean-Wei Lee, Lai-Kuan Wong, and John See. 2020b. Image Dehazing
    With Contextualized Attentive U-NET. In *International Conference on Image Processing*.
    1068–1072.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi
    Zhou, and Xi Peng. 2021a. You only look yourself: Unsupervised and untrained single
    image dehazing neural network. *International Journal of Computer Vision* 129,
    5 (2021), 1754–1767.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi
    Zhou, and Xi Peng. 2020b. Zero-Shot Image Dehazing. *IEEE Transactions on Image
    Processing* 29 (2020), 8457–8466.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017a) Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan
    Feng. 2017a. AOD-Net: All-in-One Dehazing Network. In *International Conference
    on Computer Vision*. 4780–4788.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019c) Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun
    Zeng, and Zhangyang Wang. 2019c. Benchmarking Single-Image Dehazing and Beyond.
    *IEEE Transactions on Image Processing* 28, 1 (2019), 492–505.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu,
    and Runmin Cong. 2019a. PDR-Net: Perception-inspired single image dehazing network
    with refinement. *IEEE Transactions on Multimedia* 22, 3 (2019), 704–716.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Chongyi Li, Jichang Guo, Runmin Cong, Yanwei Pang, and Bo Wang.
    2016. Underwater image enhancement by dehazing with minimum information loss and
    histogram distribution prior. *IEEE Transactions on Image Processing* 25, 12 (2016),
    5664–5677.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021b) Hongyu Li, Jia Li, Dong Zhao, and Long Xu. 2021b. DehazeFlow:
    Multi-scale Conditional Flow Network for Single Image Dehazing. In *ACM International
    Conference on Multimedia*. 2577–2585.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020e) Hui Li, Qingbo Wu, King Ngi Ngan, Hongliang Li, and Fanman
    Meng. 2020e. Region adaptive two-shot network for single image dehazing. In *International
    Conference on Multimedia and Expo*. 1–6.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Lerenhan Li, Yunlong Dong, Wenqi Ren, Jinshan Pan, Changxin
    Gao, Nong Sang, and Ming-Hsuan Yang. 2020a. Semi-Supervised Image Dehazing. *IEEE
    Transactions on Image Processing* 29 (2020), 2766–2779.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021c) Pengyue Li, Jiandong Tian, Yandong Tang, Guolin Wang, and
    Chengdong Wu. 2021c. Deep Retinex Network for Single Image Dehazing. *IEEE Transactions
    on Image Processing* 30 (2021), 1100–1115.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020d) Runde Li, Jinshan Pan, Min He, Zechao Li, and Jinhui Tang.
    2020d. Task-oriented network for image dehazing. *IEEE Transactions on Image Processing*
    29 (2020), 6523–6534.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020c) Yuenan Li, Yuhang Liu, Qixin Yan, and Kuangshi Zhang. 2020c.
    Deep Dehazing Network With Latent Ensembling Architecture and Adversarial Learning.
    *IEEE Transactions on Image Processing* 30 (2020), 1354–1368.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Huijuan
    Fang, Chao Dong, and Yining Quan. 2019b. LAP-Net: Level-aware progressive network
    for image dehazing. In *International Conference on Computer Vision*. 3276–3285.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017b) Yu Li, Shaodi You, Michael S Brown, and Robby T Tan. 2017b.
    Haze visibility enhancement: A survey and quantitative benchmarking. *Computer
    Vision and Image Understanding* 165 (2017), 1–16.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2019) Xiao Liang, Runde Li, and Jinhui Tang. 2019. Selective Attention
    network for Image Dehazing and Deraining. In *ACM Multimedia Asia*. 1–6.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020b. Trident dehazing network. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 430–431.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2014) Lixiong Liu, Bao Liu, Hua Huang, and Alan Conrad Bovik. 2014.
    No-reference image quality assessment based on spatial and spectral entropies.
    *Signal processing: Image communication* 29, 8 (2014), 856–863.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu (2019) Qian Liu. 2019. Unsupervised Single Image Dehazing Via Disentangled
    Representation. In *International Conference on Video and Image Processing*. 106–111.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan
    Luo, and Lei Zhang. 2018. Learning aggregated transmission propagation networks
    for haze removal and beyond. *IEEE Transactions on Neural Networks and Learning
    Systems* 30, 10 (2018), 2973–2986.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Wei Liu, Xianxu Hou, Jiang Duan, and Guoping Qiu. 2020a.
    End-to-End Single Image Fog Removal Using Enhanced Cycle Consistent Adversarial
    Networks. *IEEE Transactions on Image Processing* 29 (2020), 7819–7833.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. 2019a.
    GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing. In *International
    Conference on Computer Vision*. 7313–7322.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Yang Liu, Jinshan Pan, Jimmy Ren, and Zhixun Su. 2019b. Learning
    deep priors for image dehazing. In *International Conference on Computer Vision*.
    2492–2500.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Ye Liu, Lei Zhu, Shunda Pei, Huazhu Fu, Jing Qin, Qing Zhang,
    Liang Wan, and Wei Feng. 2021. From Synthetic to Real: Image Dehazing Collaborating
    with Unlabeled Real Data. In *ACM International Conference on Multimedia*. 50–58.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCartney (1976) Earl J McCartney. 1976. Optics of the atmosphere: scattering
    by molecules and particles. *Physics Bulletin* (1976), 1–421.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehra et al. (2021) Aryan Mehra, Pratik Narang, and Murari Mandal. 2021. TheiaNet:
    Towards fast and inexpensive CNN design choices for image dehazing. *Journal of
    Visual Communication and Image Representation* 77 (2021), 103137.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. (2020) Aditya Mehta, Harsh Sinha, Pratik Narang, and Murari Mandal.
    2020. Hidegan: A hyperspectral-guided image dehazing gan. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 212–213.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metwaly et al. (2020) Kareem Metwaly, Xuelu Li, Tiantong Guo, and Vishal Monga.
    2020. Nonlocal channel attention for nonhomogeneous image dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 452–453.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min et al. (2019) Xiongkuo Min, Guangtao Zhai, Ke Gu, Yucheng Zhu, Jiantao Zhou,
    Guodong Guo, Xiaokang Yang, Xinping Guan, and Wenjun Zhang. 2019. Quality evaluation
    of image dehazing methods using synthetic hazy images. *IEEE Transactions on Multimedia*
    21, 9 (2019), 2319–2333.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo et al. (2022) Yaozong Mo, Chaofeng Li, Yuhui Zheng, and Xiaojun Wu. 2022.
    DCA-CycleGAN: Unsupervised Single Image Dehazing Using Dark Channel Attention
    Optimized CycleGAN. *Journal of Visual Communication and Image Representation*
    82 (2022), 103431.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mondal et al. (2018) Ranjan Mondal, Sanchayan Santra, and Bhabatosh Chanda.
    2018. Image dehazing by joint estimation of transmittance and airlight using bi-directional
    consistency loss minimized FCN. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 920–928.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morales et al. (2019) Peter Morales, Tzofi Klinghoffer, and Seung Jae Lee. 2019.
    Feature forwarding for efficient single image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2078–2085.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narasimhan and Nayar (2003) Srinivasa G Narasimhan and Shree K Nayar. 2003.
    Contrast restoration of weather degraded images. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 25, 6 (2003), 713–724.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayar and Narasimhan (1999) Shree K Nayar and Srinivasa G Narasimhan. 1999.
    Vision in bad weather. In *International Conference on Computer Vision*. 820–827.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. (2020) Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, and Xuelong
    Li. 2020. BidNet: Binocular Image Dehazing Without Explicit Disparity Estimation.
    In *Conference on Computer Vision and Pattern Recognition*. 5930–5939.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2018) Yanwei Pang, Jin Xie, and Xuelong Li. 2018. Visual haze removal
    by a unified generative adversarial network. *IEEE Transactions on Circuits and
    Systems for Video Technology* 29, 11 (2018), 3211–3221.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parihar et al. (2020) Anil Singh Parihar, Yash Kumar Gupta, Yash Singodia, Vibhu
    Singh, and Kavinder Singh. 2020. A comparative study of image dehazing algorithms.
    In *International Conference on Communication and Electronics Systems*. 766–771.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2020) Jaihyun Park, David K Han, and Hanseok Ko. 2020. Fusion of
    heterogeneous adversarial networks for single image dehazing. *IEEE Transactions
    on Image Processing* 29 (2020), 4721–4732.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2018) Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu, and Song Wang.
    2018. Does Haze Removal Help CNN-based Image Classification?. In *European Conference
    on Computer Vision*. 697–712.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2020) Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu
    Jia. 2020. FFA-Net: Feature Fusion Attention Network for Single Image Dehazing.
    In *AAAI Conference on Artificial Intelligence*. 11908–11915.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qu et al. (2019) Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced
    Pix2pix Dehazing Network. In *Conference on Computer Vision and Pattern Recognition*.
    8152–8160.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2016) Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and
    Ming-Hsuan Yang. 2016. Single image dehazing via multi-scale convolutional neural
    networks. In *European Conference on Computer Vision*. 154–169.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018a) Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao,
    Wei Liu, and Ming-Hsuan Yang. 2018a. Gated Fusion Network for Single Image Dehazing.
    In *Conference on Computer Vision and Pattern Recognition*. 3253–3261.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2020) Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, and Ming-Hsuan
    Yang. 2020. Single image dehazing via multi-scale convolutional neural networks
    with holistic edges. *International Journal of Computer Vision* 128, 1 (2020),
    240–259.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018b) Wenqi Ren, Jingang Zhang, Xiangyu Xu, Lin Ma, Xiaochun Cao,
    Gaofeng Meng, and Wei Liu. 2018b. Deep video dehazing with semantic segmentation.
    *IEEE Transactions on Image Processing* 28, 4 (2018), 1895–1908.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical Image Computing and Computer-assisted Intervention*. 234–241.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rudin et al. (1992) Leonid I Rudin, Stanley Osher, and Emad Fatemi. 1992. Nonlinear
    total variation based noise removal algorithms. *Physica D: nonlinear phenomena*
    60, 1-4 (1992), 259–268.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saad et al. (2012) Michele A Saad, Alan C Bovik, and Christophe Charrier. 2012.
    Blind image quality assessment: A natural scene statistics approach in the DCT
    domain. *IEEE transactions on Image Processing* 21, 8 (2012), 3339–3352.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sakaridis et al. (2018) Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc
    Van Gool. 2018. Model adaptation with synthetic and real data for semantic dense
    foggy scene understanding. In *European Conference on Computer Vision*. 687–704.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2020) Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and Nong
    Sang. 2020. Domain Adaptation for Image Dehazing. In *Conference on Computer Vision
    and Pattern Recognition*. 2805–2814.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2005) Gaurav Sharma, Wencheng Wu, and Edul N Dalal. 2005. The
    CIEDE2000 color-difference formula: Implementation notes, supplementary test data,
    and mathematical observations. *Color Research & Application: Endorsed by Inter-Society
    Color Council, etc* 30, 1 (2005), 21–30.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2020) Prasen Sharma, Priyankar Jain, and Arijit Sur. 2020. Scale-aware
    conditional generative adversarial network for image dehazing. In *Winter Conference
    on Applications of Computer Vision*. 2355–2365.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2022) Jiechao Sheng, Guoqiang Lv, Gang Du, Zi Wang, and Qibin
    Feng. 2022. Multi-scale residual attention network for single image dehazing.
    *Digital Signal Processing* 121 (2022), 103327.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shin et al. (2022) Joongchol Shin, Hasil Park, and Joonki Paik. 2022. Region-Based
    Dehazing via Dual-Supervised Triple-Convolutional Network. *IEEE Transactions
    on Multimedia* 24 (2022), 245–260.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shyam et al. (2021) Pranjay Shyam, Kuk-Jin Yoon, and Kyung-Soo Kim. 2021. Towards
    Domain Invariant Single Image Dehazing. In *AAAI Conference on Artificial Intelligence*.
    9657–9665.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. 2012. Indoor segmentation and support inference from rgbd images. In *European
    Conference on Computer Vision*. 746–760.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sim et al. (2018) Hyeonjun Sim, Sehwan Ki, Jae-Seok Choi, Soomin Seo, Saehun
    Kim, and Munchurl Kim. 2018. High-resolution image dehazing with respect to training
    losses and receptive field sizes. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 912–919.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. 2015. Very Deep
    Convolutional Networks for Large-Scale Image Recognition. In *International Conference
    on Learning Representations*. 1–14.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2020) Ayush Singh, Ajay Bhave, and Dilip K Prasad. 2020. Single
    image dehazing for a variety of haze scenarios using back projected pyramid network.
    In *European Conference on Computer Vision*. 166–181.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Kumar (2019) Dilbag Singh and Vijay Kumar. 2019. A comprehensive review
    of computational dehazing techniques. *Archives of Computational Methods in Engineering*
    26, 5 (2019), 1395–1413.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2021) Lexuan Sun, Xueliang Liu, Zhenzhen Hu, and Richang Hong.
    2021. WFN-PSC: weighted-fusion network with poly-scale convolution for image dehazing.
    In *ACM International Conference on Multimedia in Asia*. 1–7.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Conference on Computer Vision and Pattern
    Recognition*. 1–9.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Guiying Tang, Li Zhao, Runhua Jiang, and Xiaoqin Zhang. 2019.
    Single Image Dehazing via Lightweight Multi-scale Networks. In *International
    Conference on Big Data*. 5062–5069.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Anna Wang, Wenhui Wang, Jinglu Liu, and Nanhui Gu. 2018a.
    AIPNet: Image-to-image single image dehazing with atmospheric illumination prior.
    *IEEE Transactions on Image Processing* 28, 1 (2018), 381–393.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Cong Wang, Yuexian Zou, and Zehan Chen. 2020. ABC-NET: Avoiding
    Blocking Effect & Color Shift Network for Single Image Dehazing Via Restraining
    Transmission Bias. In *International Conference on Image Processing*. 1053–1057.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021c) Juan Wang, Chang Ding, Minghu Wu, Yuanyuan Liu, and Guanhai
    Chen. 2021c. Lightweight multiple scale-patch dehazing network for real-world
    hazy image. *KSII Transactions on Internet and Information Systems* 15, 12 (2021),
    4420–4438.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021d) Jixiao Wang, Chaofeng Li, and Shoukun Xu. 2021d. An ensemble
    multi-scale residual attention network (EMRA-net) for image Dehazing. *Multimedia
    Tools and Applications* (2021), 29299–29319.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. 2019.
    Distilling Object Detectors With Fine-Grained Feature Imitation. In *Conference
    on Computer Vision and Pattern Recognition*. 4928–4937.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun
    Zha, Yonggang Wen, and Dacheng Tao. 2021a. Exploring Sequence Feature Alignment
    for Domain Adaptive Detection Transformers. In *ACM International Conference on
    Multimedia*. 1730–1738.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wen Wang, Jing Zhang, Wei Zhai, Yang Cao, and Dacheng Tao.
    2022. Robust Object Detection via Adversarial Novel Style Exploration. *IEEE Transactions
    on Image Processing* 31 (2022), 1949–1962.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
    Chao Dong, Yu Qiao, and Chen Change Loy. 2018b. Esrgan: Enhanced super-resolution
    generative adversarial networks. In *European Conference on Computer Vision*.
    63–79.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Yang Wang, Yang Cao, Jing Zhang, Feng Wu, and Zheng-Jun
    Zha. 2021b. Leveraging Deep Statistics for Underwater Image Enhancement. *ACM
    Transactions on Multimedia Computing, Communications, and Applications* 17, 3s
    (2021), 1–20.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Yang Wang, Jing Zhang, Yang Cao, and Zengfu Wang. 2017. A
    deep CNN method for underwater image enhancement. In *IEEE International Conference
    on Image Processing*. 1382–1386.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2013) Yinting Wang, Shaojie Zhuo, Dapeng Tao, Jiajun Bu, and Na
    Li. 2013. Automatic local exposure correction using bright channel prior for under-exposed
    images. *Signal processing* 93, 11 (2013), 3227–3238.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2004) Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli,
    et al. 2004. Image quality assessment: from error visibility to structural similarity.
    *IEEE Transactions on Image Processing* 13, 4 (2004), 600–612.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2003) Zhou Wang, Eero P Simoncelli, and Alan C Bovik. 2003. Multiscale
    structural similarity for image quality assessment. In *Asilomar Conference on
    Signals, Systems & Computers*. 1398–1402.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2020) Haoran Wei, Qingbo Wu, Hui Li, King Ngi Ngan, Hongliang Li,
    and Fanman Meng. 2020. Single Image Dehazing via Artificial Multiple Shots and
    Multidimensional Context. In *International Conference on Image Processing*. 1023–1027.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2021) Pan Wei, Xin Wang, Lei Wang, and Ji Xiang. 2021. SIDGAN:
    Single Image Dehazing without Paired Supervision. In *International Conference
    on Pattern Recognition*. 2958–2965.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Haiyan Wu, Jing Liu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020. Knowledge transfer dehazing network for nonhomogeneous dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 478–479.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao,
    Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. 2021. Contrastive Learning for Compact
    Single Image Dehazing. In *Conference on Computer Vision and Pattern Recognition*.
    10551–10560.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2020) Jinsheng Xiao, Mengyao Shen, Junfeng Lei, Jinglong Zhou,
    Reinhard Klette, and HaiGang Sui. 2020. Single image dehazing based on learning
    of haze layers. *Neurocomputing* 389 (2020), 108–122.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Liangru Xie, Hao Wang, Zhuowei Wang, and Lianglun Cheng.
    2020. DHD-Net: A Novel Deep-Learning-based Dehazing Network. In *International
    Joint Conference on Neural Networks*. 1–7.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2015) Yong Xu, Jie Wen, Lunke Fei, and Zheng Zhang. 2015. Review
    of video and image defogging algorithms and related studies on image restoration
    and enhancement. *IEEE Access* 4 (2015), 165–188.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2020) Lan Yan, Wenbo Zheng, Chao Gou, and Fei-Yue Wang. 2020. Feature
    Aggregation Attention Network for Single Image Dehazing. In *International Conference
    on Image Processing*. 923–927.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Aiping Yang, Haixin Wang, Zhong Ji, Yanwei Pang, and Ling
    Shao. 2019. Dual-Path in Dual-Path Network for Single Image Dehazing. In *International
    Joint Conference on Artificial Intelligence*. 4627–4634.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Zhang (2022) Fei Yang and Qian Zhang. 2022. Depth aware image dehazing.
    *The Visual Computer* 38, 5 (2022), 1579–1587.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Fu (2019) Hao-Hsiang Yang and Yanwei Fu. 2019. Wavelet u-net and the
    chromatic adaptation transform for single image dehazing. In *International Conference
    on Image Processing*. 2736–2740.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Hao-Hsiang Yang, Chao-Han Huck Yang, and Yi-Chang James
    Tsai. 2020. Y-net: Multi-scale feature aggregation network with wavelet structure
    similarity loss function for single image dehazing. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2628–2632.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeh et al. (2019) Chia-Hung Yeh, Chih-Hsiang Huang, and Li-Wei Kang. 2019. Multi-scale
    deep residual learning-based single image haze removal via image decomposition.
    *IEEE Transactions on Image Processing* 29 (2019), 3153–3167.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2019) Jia-Li Yin, Yi-Chi Huang, Bo-Hao Chen, and Shao-Zhen Ye. 2019.
    Color transferred convolutional neural networks for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 11 (2019), 3957–3967.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2020) Shibai Yin, Yibin Wang, and Yee-Hong Yang. 2020. A novel image-dehazing
    network with a parallel attention block. *Pattern Recognition* 102 (2020), 107255.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2021) Shibai Yin, Xiaolong Yang, Yibin Wang, and Yee-Hong Yang.
    2021. Visual Attention Dehazing Network with Multi-level Features Refinement and
    Fusion. *Pattern Recognition* 118 (2021), 108021.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2020) Mingzhao Yu, Venkateswararao Cherukuri, Tiantong Guo, and Vishal
    Monga. 2020. Ensemble dehazing networks for non-homogeneous haze. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 450–451.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, and
    Keyan Wang. 2021. A Two-branch Neural Network for Non-homogeneous Dehazing via
    Ensemble Learning. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    193–202.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Patel (2018) He Zhang and Vishal M Patel. 2018. Densely connected
    pyramid dehazing network. In *Conference on Computer Vision and Pattern Recognition*.
    3194–3203.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018b) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2018b.
    Multi-scale single image dehazing using perceptual pyramid deep network. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 902–911.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2019a.
    Joint transmission map estimation and dehazing using deep networks. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 7 (2019), 1975–1986.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017a) Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, and Chang Wen Chen.
    2017a. Fast haze removal for nighttime image using maximum reflectance prior.
    In *Conference on Computer Vision and Pattern Recognition*. 7418–7426.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2014) Jing Zhang, Yang Cao, and Zengfu Wang. 2014. Nighttime haze
    removal based on a new imaging model. In *International Conference on Image Processing*.
    4557–4561.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Jing Zhang, Yang Cao, Zheng-Jun Zha, and Dacheng Tao. 2020a.
    Nighttime dehazing with a synthetic benchmark. In *ACM International Conference
    on Multimedia*. 2355–2363.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Jingang Zhang, Wenqi Ren, Shengdong Zhang, He Zhang, Yunfeng
    Nie, Zhe Xue, and Xiaochun Cao. 2022b. Hierarchical Density-Aware Dehazing Network.
    *IEEE Transactions on Cybernetics* 52, 10 (2022), 11187–11199.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Tao (2020) Jing Zhang and Dacheng Tao. 2020. FAMED-Net: A Fast and
    Accurate Multi-Scale End-to-End Dehazing Network. *IEEE Transactions on Image
    Processing* 29 (2020), 72–84.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Li (2021) Kuangshi Zhang and Yuenan Li. 2021. Single image dehazing
    via semi-supervised domain translation and architecture search. *IEEE Signal Processing
    Letters* 28 (2021), 2127–2131.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Qiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao. 2019b.
    Category anchor-guided unsupervised domain adaptation for semantic segmentation.
    In *Neural Information Processing Systems*. 433–443.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and He (2020) Shengdong Zhang and Fazhi He. 2020. DRCDN: learning deep
    residual convolutional dehazing networks. *The Visual Computer* 36 (2020), 1797–1808.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020b. NLDN:
    Non-local dehazing network for dense haze removal. *Neurocomputing* 410 (2020),
    363–373.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020c. Photo-realistic
    dehazing via contextual generative adversarial networks. *Machine Vision and Applications*
    31 (2020), 1–12.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020d) Shengdong Zhang, Fazhi He, Wenqi Ren, and Jian Yao. 2020d.
    Joint learning of image detail and transmission map for single image dehazing.
    *The Visual Computer* 36, 2 (2020), 305–316.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Shengdong Zhang, Wenqi Ren, Xin Tan, Zhi-Jie Wang, Yong
    Liu, Jingang Zhang, Xiaoqin Zhang, and Xiaochun Cao. 2022a. Semantic-aware dehazing
    network with adaptive feature fusion. *IEEE Transactions on Cybernetics* (2022).
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) Shengdong Zhang, Wenqi Ren, and Jian Yao. 2018a. Feed-Net:
    Fully End-to-End Dehazing. In *International Conference on Multimedia and Expo*.
    1–6.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020f) Shengdong Zhang, Yue Wu, Yuanjie Zhao, Zuomin Cheng, and
    Wenqi Ren. 2020f. Color-constrained dehazing model. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 870–871.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Xiaoqin Zhang, Runhua Jiang, Tao Wang, and Wenhan Luo.
    2021a. Single Image Dehazing via Dual-Path Recurrent Network. *IEEE Transactions
    on Image Processing* 30 (2021), 5211–5222.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022c) Xiaoqin Zhang, Jinxin Wang, Tao Wang, and Runhua Jiang.
    2022c. Hierarchical feature fusion with mixed convolution attention for single
    image dehazing. *IEEE Transactions on Circuits and Systems for Video Technology*
    32, 2 (2022), 510–522.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Xiaoqin Zhang, Tao Wang, Wenhan Luo, and Pengcheng Huang.
    2021b. Multi-level fusion and attention-guided CNN for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 31, 11 (2021), 4162–4173.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020e) Xiaoqin Zhang, Tao Wang, Jinxin Wang, Guiying Tang, and
    Li Zhao. 2020e. Pyramid Channel-based Feature Attention Network for image dehazing.
    *Computer Vision and Image Understanding* 197-198 (2020), 103003.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017b) Yanfu Zhang, Li Ding, and Gaurav Sharma. 2017b. HazeRD:
    An outdoor scene dataset and benchmark for single image dehazing. In *International
    Conference on Image Processing*. 3205–3209.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020g) Zhengxi Zhang, Liang Zhao, Yunan Liu, Shanshan Zhang, and
    Jian Yang. 2020g. Unified Density-Aware Image Dehazing and Object Detection in
    Real-World Hazy Scenes. In *Asian Conference on Computer Vision*. 119–135.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021a) Dong Zhao, Long Xu, Lin Ma, Jia Li, and Yihua Yan. 2021a.
    Pyramid Global Context Network for Image Dehazing. *IEEE Transactions on Circuits
    and Systems for Video Technology* 31, 8 (2021), 3037–3050.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Shiyu Zhao, Lin Zhang, Shuaiyi Huang, Ying Shen, and Shengjie
    Zhao. 2020. Dehazing Evaluation: Real-World Benchmark Datasets, Criteria, and
    Baselines. *IEEE Transactions on Image Processing* 29 (2020), 6947–6962.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021b) Shiyu Zhao, Lin Zhang, Ying Shen, and Yicong Zhou. 2021b.
    RefineDNet: A Weakly Supervised Refinement Framework for Single Image Dehazing.
    *IEEE Transactions on Image Processing* 30 (2021), 3391–3404.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Zhuoran Zheng, Wenqi Ren, Xiaochun Cao, Xiaobin Hu, Tao
    Wang, Fenglong Song, and Xiuyi Jia. 2021. Ultra-High-Definition Image Dehazing
    via Multi-Guided Bilateral Learning. In *Conference on Computer Vision and Pattern
    Recognition*. 16180–16189.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021) Hongyuan Zhu, Yi Cheng, Xi Peng, Joey Tianyi Zhou, Zhao Kang,
    Shijian Lu, Zhiwen Fang, Liyuan Li, and Joo-Hwee Lim. 2021. Single-image dehazing
    via compositional adversarial network. *IEEE Transactions on Cybernetics* 51,
    2 (2021), 829–838.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2018) Hongyuan Zhu, Xi Peng, Vijay Chandrasekhar, Liyuan Li, and
    Joo-Hwee Lim. 2018. DehazeGAN: When Image Dehazing Meets Differential Programming.
    In *International Joint Conference on Artificial Intelligence*. 1234–1240.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *International Conference on Computer Vision*. 2223–2232.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
