- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2106.03323] A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.03323](https://ar5iv.labs.arxiv.org/html/2106.03323)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jie Gui [guijie@seu.edu.cn](mailto:guijie@seu.edu.cn) School of Cyber Science
    and Engineering, Southeast University and Purple Mountain LaboratoriesNanjingJiangsuChina210000
    ,  Xiaofeng Cong School of Cyber Science and Engineering, Southeast UniversityNanjingChina
    [cxf_svip@163.com](mailto:cxf_svip@163.com) ,  Yuan Cao Ocean University of China
    QingdaoChina [cy8661@ouc.edu.cn](mailto:cy8661@ouc.edu.cn) ,  Wenqi Ren Institute
    of Information Engineering, Chinese Academy of SciencesBeijingChina [renwenqi@iie.ac.cn](mailto:renwenqi@iie.ac.cn)
    ,  Jun Zhang Anhui UniversityHefeiChina [wwwzhangjun@163.com](mailto:wwwzhangjun@163.com)
    ,  Jing Zhang The University of SydneySydneyAustralia [jing.zhang1@sydney.edu.au](mailto:jing.zhang1@sydney.edu.au)
    ,  Jiuxin Cao School of Cyber Science and Engineering, Southeast UniversityNanjingChina
     and  Dacheng Tao JD Explore Academy, China and The University of SydneySydneyAustralia
    [dacheng.tao@gmail.com](mailto:dacheng.tao@gmail.com)(2022)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the development of convolutional neural networks, hundreds of deep learning
    based dehazing methods have been proposed. In this paper, we provide a comprehensive
    survey on supervised, semi-supervised, and unsupervised single image dehazing.
    We first discuss the physical model, datasets, network modules, loss functions,
    and evaluation metrics that are commonly used. Then, the main contributions of
    various dehazing algorithms are categorized and summarized. Further, quantitative
    and qualitative experiments of various baseline methods are carried out. Finally,
    the unsolved issues and challenges that can inspire the future research are pointed
    out. A collection of useful dehazing materials is available at [https://github.com/Xiaofeng-life/AwesomeDehazing](https://github.com/Xiaofeng-life/AwesomeDehazing).
  prefs: []
  type: TYPE_NORMAL
- en: 'image dehazing, supervised, semi-supervised, unsupervised, atmospheric scattering
    model.^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†doi: XXXXXXX.XXXXXXX^†^†conference:
    Make sure to enter the correct conference title from your rights confirmation
    emai; June 03–05, 2018; Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Vision for robotics^†^†ccs: Computing methodologies Computational
    photography^†^†ccs: Computing methodologies Computer vision'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the absorption by floating particles contained in the hazy environment,
    the quality of the image captured by the camera will be reduced. The phenomenon
    of image quality degradation in hazy weather has a negative impact on photography
    work. The contrast of the image will decrease and the color will shift. Meantime,
    the texture and edge of objects in the scene will become blurred. As shown in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), there is an obvious difference
    between the pixel histograms of hazy and haze-free images. For computer vision
    tasks such as object detection and image segmentation, low-quality inputs can
    degrade the performance of the models trained on haze-free images.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, many researchers try to recover high-quality clear scenes from hazy
    images. Before deep learning was widely used in computer vision tasks, image dehazing
    algorithms had mainly relied on various prior assumptions (He et al., [2010](#bib.bib52))
    and atmospheric scattering model (ASM) (McCartney, [1976](#bib.bib94)). The processing
    flow of these statistical rule based methods has good interpretability. However,
    they may exhibit shortcomings when facing complex real world scenarios. For example,
    the well-known dark channel prior (He et al., [2010](#bib.bib52)) (DCP, best paper
    of CVPR 2009) cannot handle regions containing sky well.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by deep learning, (Ren et al., [2016](#bib.bib111); Cai et al., [2016](#bib.bib13);
    Ren et al., [2020](#bib.bib113)) combine ASM and convolutional neural network
    (CNN) to estimate the parameters of the ASM. Quantitative and qualitative experimental
    results show that deep learning can help the prediction of these parameters in
    a supervised way.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, (Qin et al., [2020](#bib.bib109); Liu et al., [2019a](#bib.bib91);
    Liang et al., [2019](#bib.bib85); Zhang et al., [2022c](#bib.bib182); Zheng et al.,
    [2021](#bib.bib190)) have demonstrated that end-to-end supervised dehazing networks
    can be implemented independently of the ASM. Thanks to the powerful feature extraction
    capability of CNN, these non-ASM-based dehazing algorithms can achieve comparable
    accuracy as ASM-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: ASM-based and non-ASM-based supervised algorithms have shown impressive performance.
    However, they often require synthetic paired images that are inconsistent with
    real world hazy images. Therefore, recent research focus on methods that are more
    suitable to the real world dehazing task. (Cong et al., [2020](#bib.bib24); Golts
    et al., [2020](#bib.bib44); Li et al., [2020b](#bib.bib72)) explore unsupervised
    algorithms that do not require synthetic data while other studies (Li et al.,
    [2020a](#bib.bib79); An et al., [2022](#bib.bib2); Chen et al., [2021b](#bib.bib21);
    Zhang and Li, [2021](#bib.bib172)) propose semi-supervised algorithms that exploit
    both synthetic paired data and real world unpaired data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/50395ece082c77eeeddb8770c5ca443b.png)![Refer to caption](img/3711e6faf7811490083dce87156b8618.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A clear image                                (c) A hazy image                               
    ![Refer to caption](img/ce29973e98b8d683a3a16a2a78041334.png) ![Refer to caption](img/cd6060c43bbf613b833cc7a28e19ae1b.png)
  prefs: []
  type: TYPE_NORMAL
- en: (b) Histogram for (a)                   (d) Histogram for (c)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Pixel histograms of clear (a) and hazy (c) images.
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid development in this area, hundreds of dehazing methods have been
    proposed. To inspire and guide the future research, a comprehensive survey is
    urgently needed. Some papers have attempted to partially review the recent development
    of the dehazing research. For example,  (Singh and Kumar, [2019](#bib.bib129);
    Li et al., [2017b](#bib.bib84); Xu et al., [2015](#bib.bib152)) gives a summary
    of the non-deep learning dehazing methods, including depth estimation, wavelet,
    enhancement, filtering, but lacks of research on recent CNN-based methods. Parihar
    et al. (Parihar et al., [2020](#bib.bib106)) provides a survey about supervised
    dehazing models, but it does not pay enough attention to the latest explorations
    of semi-supervised and unsupervised methods. Banerjee et al. (Banerjee and Chaudhuri,
    [2021](#bib.bib11)) introduce and group the existing nighttime image dehazing
    methods, however, the methods of daytime are rarely analyzed. Gui et al. (Gui
    et al., [2021](#bib.bib46)) briefly classify and analyze supervised and unsupervised
    algorithms, but does not summarize the various recently proposed semi-supervised
    methods. Unlike existing reviews, we give a comprehensive survey on the supervised,
    semi-supervised and unsupervised daytime dehazing models based on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce35310209484fc8e93d3ea80527fe22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Atmospheric Scattering Model (ASM), same as (Cai et al., [2016](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. A taxonomy of dehazing methods. Red number index represents ASM-based
    methods, and black number index represents non-ASM-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Key Idea | Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Learning of $t(x)$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DehazeNet (Cai et al., [2016](#bib.bib13)), ABC-Net (Wang et al., [2020](#bib.bib134)),
    MSCNN (Ren et al., [2016](#bib.bib111)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSCNN-HE (Ren et al., [2020](#bib.bib113)), SID-JMP (Huang et al., [2018](#bib.bib59)),
    LATPN (Liu et al., [2018](#bib.bib89)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Joint learning of $t(x)$ and $A$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DCPDN (Zhang and Patel, [2018](#bib.bib164)), DSIEN (Guo et al., [2019b](#bib.bib49)),
    LDPID (Liu et al., [2019b](#bib.bib92)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PMHLD (Chen et al., [2020](#bib.bib19)), HRGAN (Pang et al., [2018](#bib.bib105))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Non-explicitly embedded ASM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AOD-Net (Li et al., [2017a](#bib.bib73)), FAMED-Net (Zhang and Tao,
    [2020](#bib.bib171)), DehazeGAN (Zhu et al., [2018](#bib.bib192)), PFDN (Dong
    and Pan, [2020](#bib.bib31)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SI-DehazeGAN (Zhu et al., [2021](#bib.bib191)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Generative adversarial network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; EPDN (Qu et al., [2019](#bib.bib110)), PGC-UNet (Zhao et al., [2021a](#bib.bib187)),
    RI-GAN (Dudhane et al., [2019](#bib.bib37)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DHGAN (Sim et al., [2018](#bib.bib126)), SA-CGAN (Sharma et al., [2020](#bib.bib121))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Level-aware | LAP-Net (Li et al., [2019b](#bib.bib83)), HardGAN (Deng et al.,
    [2020](#bib.bib26)) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-function fusion | DMMFD (Deng et al., [2019](#bib.bib27)) |'
  prefs: []
  type: TYPE_TB
- en: '| Transformation and decomposition of input | GFN (Ren et al., [2018a](#bib.bib112)),
    MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)), DPDP-Net (Yang et al., [2019](#bib.bib154)),
    DIDH (Shyam et al., [2021](#bib.bib124)) |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge distillation | KDDN (Hong et al., [2020](#bib.bib55)), KTDN (Wu
    et al., [2020](#bib.bib148)), SRKTDN (Chen et al., [2021a](#bib.bib17)), DALF (Fang
    et al., [2021](#bib.bib39)) |'
  prefs: []
  type: TYPE_TB
- en: '| Transformation of colorspace | AIP-Net (Wang et al., [2018a](#bib.bib133)),
    MSRA-Net (Sheng et al., [2022](#bib.bib122)), TheiaNet (Mehra et al., [2021](#bib.bib95)),
    RYF-Net (Dudhane and Murala, [2019b](#bib.bib36)) |'
  prefs: []
  type: TYPE_TB
- en: '| Contrastive learning | AECR-Net  (Wu et al., [2021](#bib.bib149)) |'
  prefs: []
  type: TYPE_TB
- en: '| Non-deterministic output | pWAE (Kim et al., [2021](#bib.bib67)), DehazeFlow (Li
    et al., [2021b](#bib.bib77)) |'
  prefs: []
  type: TYPE_TB
- en: '| Retinex model | RDN (Li et al., [2021c](#bib.bib80)) |'
  prefs: []
  type: TYPE_TB
- en: '| Residual learning | GCA-Net(Chen et al., [2019c](#bib.bib14)), DRL (Du and
    Li, [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)), POGAN (Du
    and Li, [2019](#bib.bib34)) |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency domain |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Wavelet U-net  (Yang and Fu, [2019](#bib.bib156)), MsGWN (Dong et al.,
    [2020c](#bib.bib30)), EMRA-Net (Wang et al., [2021d](#bib.bib136)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TDN (Liu et al., [2020b](#bib.bib86)), DW-GAN (Fu et al., [2021](#bib.bib40))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Joint dehazing and depth estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SDDE (Lee et al., [2020a](#bib.bib69)), S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)), DDRL (Guo and Monga, [2020](#bib.bib50)), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DeAID (Yang and Zhang, [2022](#bib.bib155)), TSDCN-Net  (Cheng and Zhao,
    [2021](#bib.bib23)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Detection and segmentation with dehazing | LEAAL (Li et al., [2020c](#bib.bib82)),
    SDNet  (Zhang et al., [2022a](#bib.bib178)), UDnD (Zhang et al., [2020g](#bib.bib186))
    |'
  prefs: []
  type: TYPE_TB
- en: '| End-to-end CNN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu et al.,
    [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HFF (Zhang et al., [2022c](#bib.bib182)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Semi-supervised | Pretrain backbone and fine-tune | PSD (Chen et al., [2021b](#bib.bib21)),
    SSDT (Zhang and Li, [2021](#bib.bib172)) |'
  prefs: []
  type: TYPE_TB
- en: '| Disentangled and reconstruction | DCNet (Chen et al., [2021c](#bib.bib22)),
    FSR (Liu et al., [2021](#bib.bib93)), CCDM (Zhang et al., [2020f](#bib.bib180))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Two-branches training | DAID (Shao et al., [2020](#bib.bib119)), SSID (Li
    et al., [2020a](#bib.bib79)), SSIDN (An et al., [2022](#bib.bib2)) |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | Unsupervised domain translation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), CDNet (Dudhane and
    Murala, [2019a](#bib.bib35)), E-CycleGAN (Liu et al., [2020a](#bib.bib90)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; USID (Huang et al., [2019](#bib.bib57)), DCA-CycleGAN (Mo et al., [2022](#bib.bib99)),
    DHL (Cong et al., [2020](#bib.bib24)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning without haze-free images | Deep-DCP (Golts et al., [2020](#bib.bib44))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised image decomposition | Double-DIP (Gandelsman et al., [2019](#bib.bib42))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot learning | ZID (Li et al., [2020b](#bib.bib72)), YOLY (Li et al.,
    [2021a](#bib.bib71)) |'
  prefs: []
  type: TYPE_TB
- en: 1.1\. Scope and Goals of This Survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This survey does not cover all themes of dehazing research. We focus our attention
    on deep learning based algorithms that employ monocular daytime images. This means
    that we will not discuss in detail about non-deep learning dehazing, underwater
    dehazing (Wang et al., [2021b](#bib.bib141), [2017](#bib.bib142); Li et al., [2016](#bib.bib76)),
    video dehazing (Ren et al., [2018b](#bib.bib114)), hyperspectral dehazing (Mehta
    et al., [2020](#bib.bib96)), nighttime dehazing (Zhang et al., [2020a](#bib.bib169),
    [2017a](#bib.bib167), [2014](#bib.bib168)), binocular dehazing (Pang et al., [2020](#bib.bib104)),
    etc. Therefore, when we refer to “dehazing” in this paper, we usually mean deep
    learning based algorithms whose input data satisfies four conditions: single frame
    image, daytime, monocular, on the ground. In summary, there are three contributions
    to this survey as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonly used physical models, datasets, network modules, loss functions and
    evaluation metrics are summarized.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classification and introduction of supervised, semi-supervised, and unsupervised
    methods is presented.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the existing achievements and unsolved problems, the future research
    directions are prospected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.2\. A Guide for Reading This Survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") introduces physical model ASM, synthetic
    & generated & real world datasets, loss functions, basic modules commonly used
    in dehazing networks and evaluation metrics for various algorithms. Section [3](#S3
    "3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") provides a comprehensive discussion of supervised
    dehazing algorithms. A review of semi-supervised and unsupervised dehazing methods
    is in Section [4](#S4 "4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") and Section [5](#S5
    "5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), respectively. Section [6](#S6 "6\. Experiment
    and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") presents quantitative and qualitative experimental
    results for three categories of baseline algorithms. Section [7](#S7 "7\. Challenges
    and Opportunities ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing
    Based on Deep Learning") discusses the open issues of dehazing research.
  prefs: []
  type: TYPE_NORMAL
- en: A critical challenge for a logical and comprehensive review of dehazing research
    is how to properly classify existing methods. In the classification of Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), there are several items that need to be pointed
    out as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DCP is validated on the dehazing task, and the inference process utilizes
    ASM. Therefore, those methods that utilize DCP are considered to be ASM-based.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey treat the knowledge distillation based supervised dehazing network
    as a supervised algorithm rather than a weakly supervised/semi-supervised algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised dehazing methods using total variation loss or GAN loss are still
    classified as supervised.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here we give the notational conventions for this survey. Unless otherwise specified,
    all symbols have the following meanings: $I(x)$ means hazy image; $J(x)$ denotes
    haze-free image. $t(x)$ stands for transmission map; $x$ in $I(x)$, $J(x)$ and
    $t(x)$ is pixel location. The subscript “$rec$” denotes “reconstructed”, such
    as $I_{rec}(x)$ is the reconstructed hazy image. The subscript “$pred$” refers
    to the prediction output. According to (Gandelsman et al., [2019](#bib.bib42)),
    atmospheric light may be regarded as a constant $A$ or a non-uniform matrix $A(x)$.
    In this survey, atmospheric light is uniformly denoted as $A$. In addition, many
    papers give the proposed algorithm an abbreviated name, such as GFN (Ren et al.,
    [2018a](#bib.bib112)) (gated fusion network for single image dehazing). For readability,
    this survey uses abbreviations as references to these papers. For a small amount
    of papers that do not give a name for their algorithms, we designate the abbreviated
    name according to the title of the corresponding paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The commonalities of dehazing algorithms are mainly reflected in four aspects.
    First, the modeling of the network relies on physical model ASM or is completely
    based on neural networks. Second, the dataset used for training the network needs
    to contain transmission map, atmospheric light value, or paired supervision information.
    Third, the dehazing networks employ different kinds of basic modules. Fourth,
    different kinds of loss functions are used for training. Based on these four factors,
    the researchers designed a variety of effective dehazing algorithms. Thus, this
    section introduces ASM, datasets, loss functions, and network architecture modules.
    In addition, how to evaluate the dehazing results is also discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Modeling of the Dehazing Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Haze is a natural phenomenon that can be approximately explained by ASM. McCartney (McCartney,
    [1976](#bib.bib94)) first proposed the basic ASM to describe the principles of
    haze formation. Then, Narasimhan (Narasimhan and Nayar, [2003](#bib.bib102)) and
    Nayar (Nayar and Narasimhan, [1999](#bib.bib103)) extended and developed the ASM
    that is currently widely used. The ASM provides a reliable theoretical basis for
    the research of image dehazing. Its formula is
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $I(x)=J(x)t(x)+A(1-t(x)),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the pixel location and $A$ means the global atmospheric light.
    In different papers, $A$ may be referred to as airlight or ambient light. For
    ease of understanding, $A$ is noted as atmospheric light in this survey. For the
    dehazing methods based on ASM, $A$ is usually unknown. $I(x)$ stands for the hazy
    image and $J(x)$ denotes the clear scene image. For most dehazing models, $I(x)$
    is the input and $J(x)$ is the desired output. The $t(x)$ means the medium transmission
    map which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $t(x)=e^{-\beta{d(x)}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\beta$ and $d(x)$ stands for the atmosphere scattering parameter and
    the depth of $I(x)$, respectively. Thus, the $t(x)$ is determined by $d(x)$, which
    can be used for the synthesis of hazy image. If the $t(x)$ and $A$ can be estimated,
    the haze-free image $J(x)$ can be obtained by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $J(x)=\frac{I(x)-A(1-t(x))}{t(x)}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: The imaging principle of ASM is shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning").
    It can be seen that the light reaching the camera from the object is affected
    by the particles in the air. Some works use ASM to describe the formation process
    of haze, and the parameters included in the atmospheric scattering model are solved
    in an explicit or implicit way. As shown in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"),
    ASM has a profound impact on dehazing research, including supervised (Ren et al.,
    [2016](#bib.bib111); Cai et al., [2016](#bib.bib13); Li et al., [2017a](#bib.bib73);
    Zhang and Patel, [2018](#bib.bib164)), semi-supervised (Li et al., [2020a](#bib.bib79);
    Liu et al., [2021](#bib.bib93); Chen et al., [2021b](#bib.bib21)), and unsupervised
    algorithms (Li et al., [2021a](#bib.bib71); Golts et al., [2020](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Datasets for Dehazing Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 2\. Datasets for image dehazing task. Syn stands for synthetic hazy images.
    HG denotes the hazy images generated from a haze generator. Real means real world
    scenes. S&R denotes Syn&Real. I and O denotes indoor and outdoor, respectively.
    P and NP means pair and non-pair, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | type | Nums | I/O | P/NP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| D-HAZY (Ancuti et al., [2016](#bib.bib3)) | Syn | 1400+ | I | P |'
  prefs: []
  type: TYPE_TB
- en: '| HazeRD (Zhang et al., [2017b](#bib.bib185)) | Syn | 15 | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| I-HAZE (Ancuti et al., [2018b](#bib.bib5)) | HG | 35 | I | P |'
  prefs: []
  type: TYPE_TB
- en: '| O-HAZE (Ancuti et al., [2018c](#bib.bib8)) | HG | 45 | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| RESIDE (Li et al., [2019c](#bib.bib74)) | S&R | 10000+ | I&O | P&NP |'
  prefs: []
  type: TYPE_TB
- en: '| Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) | HG | 33 | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| NH-HAZE (Ancuti et al., [2020a](#bib.bib7)) | HG | 55 | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| MRFID (Liu et al., [2020a](#bib.bib90)) | Real | 200 | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| BeDDE (Zhao et al., [2020](#bib.bib188)) | Real | 200+ | O | P |'
  prefs: []
  type: TYPE_TB
- en: '| 4KID (Zheng et al., [2021](#bib.bib190)) | Syn | 10000 | O | P |'
  prefs: []
  type: TYPE_TB
- en: 'For computer vision tasks such as object detection, image segmentation, and
    image classification, accurate ground-truth labels can be obtained with careful
    annotation. However, sharp, accurate and pixel-wise labels (i.e., paired haze-free
    images) for hazy images in natural scenes are almost impossible to obtain. Currently,
    there are mainly two approaches for obtaining paired hazy and haze-free images.
    The first way is to obtain synthetic data with the help of ASM, such as the D-HAZY (Ancuti
    et al., [2016](#bib.bib3)), HazeRD (Zhang et al., [2017b](#bib.bib185)) and RESIDE (Li
    et al., [2019c](#bib.bib74)). By selecting different parameters for ASM, researchers
    can easily obtain hazy images with different haze densities. Four components are
    needed to synthesize a hazy image: a clear image, a depth map $d(x)$ corresponding
    to the content of the clear image, atmospheric light $A$ and atmosphere scattering
    parameter $\beta$. Thus, we can divide the synthetic dataset into two stages.
    In the first stage, clear images and corresponding depth maps are needed to be
    collected in pairs. In order to ensure that the synthesized haze is as close as
    possible to the real world haze, the depth information must be sufficiently accurate.
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the clear image and corresponding depth map in the NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)). In the second stage, atmospheric light $A$ and atmosphere
    scattering parameter $\beta$ are designated as fixed values or randomly selected.
    The commonly used D-HAZY (Ancuti et al., [2016](#bib.bib3)) dataset is synthesized
    when both $A$ and $\beta$ are $1$. Several researches choose different $A$ and
    $\beta$ in order to increase the diversity of the synthesized images and thus
    improve the generalization ability of the trained model. For example, MSCNN (Ren
    et al., [2016](#bib.bib111)) sets $A\in(0.7,1.0)$ and $\beta\in(0.5,1.5)$. Fig.
    [4](#S2.F4 "Figure 4 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the corresponding hazy image when $\beta$ takes $6$ different values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60136141ecd63dd2e71a51ede116fec1.png)![Refer to caption](img/e41d2dfc40739fce93d6721617f524dd.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A clear image       (b) Depth for the clear image
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. A clear image and corresponding depth map in NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/289611583db612323c711b7b83db8a21.png)![Refer to caption](img/f779c97fa911a7a57446c982c83b1f81.png)![Refer
    to caption](img/33c380ec6c011ec9104f9d6cd80d0ff5.png)![Refer to caption](img/09c95952de08c447a55508d1f2f46b53.png)![Refer
    to caption](img/57e98b1225188fd083f91867742a762a.png)![Refer to caption](img/316ebd886e9811388ed98d6cf6f33776.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Synthesized hazy images of different densities by setting different
    values for the atmosphere scattering parameter $\beta$ based on NYU-Depth dataset
    (Silberman et al., [2012](#bib.bib125)).
  prefs: []
  type: TYPE_NORMAL
- en: The second way is to generate the hazy image by using a haze generator, such
    as I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti et al., [2018c](#bib.bib8)),
    Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
    The well-known competition New Trends in Image Restoration and Enhance (NTIRE
    2018-2020) dehazing challenge  (Ancuti et al., [2018a](#bib.bib4), [2019b](#bib.bib9),
    [2020b](#bib.bib10)) are based on these generated datasets. Fig. [5](#S2.F5 "Figure
    5 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows four pairs
    of hazy and haze-free examples contained in the datasets simulated by the haze
    generator. The images in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2\. Datasets for Dehazing
    Task ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") (a) are from indoor scenes, while (b), (c) and
    (d) are all pictured at outdoor views. There are differences in the pattern of
    haze in these three outdoor datasets. The haze in (b) and (c) is evenly distributed
    throughout the entire image, while the haze in (d) are non-homogeneous in the
    whole scenes. In addition, the density of haze in (c) is significantly higher
    than that in (b) and (d). These datasets with different characteristics provide
    useful insights for the design of dehazing algorithms. For example, in order to
    remove the high density of haze in Dense-Haze, it is necessary to design dehazing
    models with stronger feature extraction and recovery capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a67f29b9c7336e6502f2312160584d5.png)![Refer to caption](img/e89637664c4362468ae836fd15460f09.png)![Refer
    to caption](img/23ee74b6da9b40e50dc696f12b44dcf8.png)![Refer to caption](img/12a9d3b98e907b5bbbbf7b77ddeff179.png)![Refer
    to caption](img/00c58a42576e10d7fc02f256cf87990e.png)![Refer to caption](img/5bec5c0b347541aa68e504234a21dc28.png)![Refer
    to caption](img/c976c8c9abad588ceb996b8f311bcee9.png)![Refer to caption](img/bce4a4be864141dde51642ecac05e4b9.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Indoor Haze             (b) Outdoor Haze         (c) Dense Haze      (d)
    Non-Homogeneous Haze
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. Examples from I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti
    et al., [2018c](#bib.bib8)), Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and
    NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of synthetic and generated haze is that it alleviates the
    difficulty during data acquisition. However, the hazy images synthesized based
    on ASM or generated by haze generator cannot perfectly simulate the formation
    process of real world haze. Therefore, there is an inherent difference between
    the synthetic and real world data. Several researches have noticed the problems
    of artificial data and tried to construct real world datasets, such as MRFID (Liu
    et al., [2020a](#bib.bib90)) and BeDDE (Zhao et al., [2020](#bib.bib188)). However,
    due to the high costs and difficulties of data collection, the current real world
    datasets do not contain enough examples as the synthetic dataset, like RESIDE (Li
    et al., [2019c](#bib.bib74)). To facilitate the comparison of different datasets,
    we summarize the characteristics of various datasets in Table [2](#S2.T2 "Table
    2 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Network Block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs are widely used in current deep learning based dehazing networks. Commonly
    adopted modules are standard convolution, dilated convolution, multi-scale fusion,
    feature pyramid, cross-layer connection and attention. Usually, multiple basic
    blocks are formed into a dehazing network. In order to facilitate the understanding
    of the principles of different dehazing algorithms, the basic blocks commonly
    used in network architectures are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Standard convolution: It is shown that using standard convolution in a sequential
    connection way to build neural networks is effective. Therefore, standard convolution
    are often used in dehazing models (Li et al., [2017a](#bib.bib73); Ren et al.,
    [2016](#bib.bib111); Sharma et al., [2020](#bib.bib121); Zhang et al., [2020e](#bib.bib184))
    together with other blocks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dilated convolution: Dilated convolution can increase the receptive field while
    keeping the size of the convolution kernel unchanged. Studies (Chen et al., [2019c](#bib.bib14);
    Zhang et al., [2020c](#bib.bib176); Zhang and He, [2020](#bib.bib174); Lee et al.,
    [2020b](#bib.bib70); Yan et al., [2020](#bib.bib153)) have shown that dilated
    convolution can improve the performance of global feature extraction. Moreover,
    fusing convolution layers with different dilation rates can extract features from
    different receptive fields.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-scale fusion: CNNs with multi-scale convolution kernels have been proven
    effective in extracting features in a variety of visual tasks (Szegedy et al.,
    [2015](#bib.bib131)). By using convolution kernels at different scales and fusing
    the extracted feature together, dehazing methods (Wang et al., [2018a](#bib.bib133);
    Tang et al., [2019](#bib.bib132); Dudhane et al., [2019](#bib.bib37); Wang et al.,
    [2020](#bib.bib134)) have demonstrated that the fusion strategy can obtain the
    multi-scale details that are useful for image restoration. In the process of feature
    fusion, a common way is to spatially concatenate or add output features obtained
    by convolution kernels of different sizes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature pyramid: In the research of digital image processing, the image pyramid
    can be used to obtain information of different resolutions. The dehazing network
    based on deep learning (Zhang et al., [2020e](#bib.bib184); Zhang and Patel, [2018](#bib.bib164);
    Zhang et al., [2018b](#bib.bib165); Singh et al., [2020](#bib.bib128); Zhao et al.,
    [2021a](#bib.bib187); Yin et al., [2020](#bib.bib160); Chen et al., [2019a](#bib.bib16))
    uses this strategy in the middle layer of the network to extract multiple scales
    of space and channel information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-layer connection: In order to enhance the information exchange between
    different layers and improve the feature extraction ability of the network, cross-layer
    connections are often used in CNNs. There are mainly three types of cross-layer
    connections used in dehazing networks, which are residual connection (Zhang et al.,
    [2020g](#bib.bib186); Qu et al., [2019](#bib.bib110); Hong et al., [2020](#bib.bib55);
    Liang et al., [2019](#bib.bib85); Chen et al., [2019d](#bib.bib20)) proposed by
    ResNet (He et al., [2016](#bib.bib53)), dense connection (Zhu et al., [2018](#bib.bib192);
    Zhang et al., [2022a](#bib.bib178); Dong et al., [2020a](#bib.bib32); Chen and
    Lai, [2019](#bib.bib15); Guo et al., [2019a](#bib.bib48); Li et al., [2019a](#bib.bib75))
    designed by DenseNet (Huang et al., [2017](#bib.bib56)), and skip connection (Zhao
    et al., [2021a](#bib.bib187); Dudhane et al., [2019](#bib.bib37); Yang and Zhang,
    [2022](#bib.bib155); Lee et al., [2020b](#bib.bib70)) inspired by U-Net (Ronneberger
    et al., [2015](#bib.bib115)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention in dehazing: The attention mechanism has been successfully applied
    in the research of natural language processing. Commonly used attention blocks
    in computer vision include channel attention and spatial attention. For the feature
    extraction and reconstruction process of 2D image, channel attention can emphasize
    the useful channels of the feature map. This unequal feature map processing strategy
    allows the model to focus more on effective feature information. The spatial attention
    mechanism focuses on the differences in the internal location regions of the feature
    map, such as the distribution of haze on the entire map. By embedding the attention
    module in the network, several dehazing methods (Liang et al., [2019](#bib.bib85);
    Chen et al., [2019d](#bib.bib20); Liu et al., [2019a](#bib.bib91); Qin et al.,
    [2020](#bib.bib109); Yin et al., [2020](#bib.bib160); Lee et al., [2020b](#bib.bib70);
    Yan et al., [2020](#bib.bib153); Dong et al., [2020c](#bib.bib30); Yan et al.,
    [2020](#bib.bib153); Zhang et al., [2020e](#bib.bib184); Yin et al., [2021](#bib.bib161);
    Metwaly et al., [2020](#bib.bib97); Wang et al., [2021d](#bib.bib136)) have achieved
    excellent dehazing performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.4\. Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section introduces the commonly adopted loss functions in supervised, semi-supervised
    and unsupervised dehazing models, which can be used for transmission map estimation,
    clear image prediction, hazy image reconstruction, atmospheric light regression,
    etc. Several algorithms use multiple losses in combination to obtain better dehazing
    performance. A detailed classification and summary of the loss functions used
    by different dehazing methods is presented in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\.
    Total Variation Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"). In the
    loss function introduced below, $X$ and $Y$ denote the predicted value and ground
    truth value, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1\. Fidelity Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The widely used pixel-wise loss functions for dehazing research are L1 loss
    and L2 loss, which are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $L1=&#124;&#124;X-Y&#124;&#124;_{1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (5) |  | $L2=&#124;&#124;X-Y&#124;&#124;_{2}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 2.4.2\. Perceptual Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The research on image super resolution (Ledig et al., [2017](#bib.bib68); Wang
    et al., [2018b](#bib.bib140)) and style transfer (Johnson et al., [2016](#bib.bib64))
    indicates that the attributes of the human visual system in the process of perceptual
    evaluation is not fully reflected by L1 or L2 loss. Meanwhile, L2 loss may lead
    to over-smooth outputs (Ledig et al., [2017](#bib.bib68)). Recent researches use
    pre-trained classification neural networks to calculate the perceptual loss in
    the feature space. The most commonly used pre-training model is VGG (Simonyan
    and Zisserman, [2015](#bib.bib127)), and some of its layers are used to calculate
    the distance between the predicted image and the reference image in the feature
    space, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $L_{per}(X,Y)=\sum_{i=1}^{N}&#124;&#124;\psi_{i}(X)-\psi_{i}(Y)&#124;&#124;_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $N$ represents the number of features selected for calculation; $i$ means
    the index of feature map; $\psi(\cdot)$ denotes the pretrained VGG. During the
    calculation of perceptual loss and network optimization, the parameters of VGG
    are always frozen.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3\. Structure Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a metric of the dehazing methods, Structural Similarity (SSIM) (Wang et al.,
    [2004](#bib.bib144)) is also used as a loss function in the optimization process.
    Studies (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162)) have
    shown that SSIM loss can improve the structural similarity during image restoration.
    MS-SSIM (Wang et al., [2003](#bib.bib145)) introduces multi-scale evaluation into
    SSIM, which is also used as a loss function by the dehazing algorithms, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $L_{ssim}=1-SSIM(X,Y),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (8) |  | $L_{msssim}=1-MSSSIM(X,Y).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 2.4.4\. Gradient Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gradient loss, also known as edge loss, is used to better restore the contour
    and edge information of the haze-free image. The edge extraction can be implemented
    as Laplacian operator, Canny operator, and so on. For example, SA-CGAN  (Sharma
    et al., [2020](#bib.bib121)) uses the Laplacian of Gaussian with standard deviation
    $\sigma$ to perform quadratic differentiation on the two-dimensional image $F$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\displaystyle L(m,n)$ | $\displaystyle=\bigtriangledown^{2}F(m,n)=\frac{\partial^{2}F}{\partial{m^{2}}}+\frac{\partial^{2}F}{\partial{n^{2}}}=-\frac{1}{\pi{\sigma^{4}}}[1-\frac{m^{2}+n^{2}}{2\sigma^{2}}]\exp{(-\frac{m^{2}+n^{2}}{2\sigma^{2}})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(m,n)$ means pixel location and $L(m,n)$ is calculated for both $X$ and
    $Y$, respectively. Then, regression objective functions such as L1 and L2 are
    used for the calculation of gradient loss.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.5\. Total Variation Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Total variation (TV) loss (Rudin et al., [1992](#bib.bib116)) can be used to
    smooth image and remove noise. The training objective is to minimize the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $L_{TV}=&#124;&#124;\partial_{m}{X}&#124;&#124;_{1}+&#124;&#124;\partial_{n}{X}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    It can be seen from the formula that the TV loss can be added to networks trained
    in an unsupervised manner without using ground-truth $Y$.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Loss functions for dehazing task
  prefs: []
  type: TYPE_NORMAL
- en: '| Loss Function | Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| L1 |  (Mondal et al., [2018](#bib.bib100); Chen et al., [2019b](#bib.bib18);
    Deng et al., [2019](#bib.bib27); Liang et al., [2019](#bib.bib85); Yin et al.,
    [2019](#bib.bib159); Dong and Pan, [2020](#bib.bib31); Chen et al., [2020](#bib.bib19);
    Yan et al., [2020](#bib.bib153); Qin et al., [2020](#bib.bib109); Hong et al.,
    [2020](#bib.bib55); Li et al., [2020d](#bib.bib81); Zhang et al., [2020b](#bib.bib175);
    Zhao et al., [2021a](#bib.bib187); Park et al., [2020](#bib.bib107); Shin et al.,
    [2022](#bib.bib123); Zhang et al., [2022a](#bib.bib178)) |'
  prefs: []
  type: TYPE_TB
- en: '| L2 |  (Li et al., [2017a](#bib.bib73); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2018](#bib.bib192); Zhang et al., [2018a](#bib.bib179); Guo et al.,
    [2019b](#bib.bib49); Morales et al., [2019](#bib.bib101); Chen et al., [2019d](#bib.bib20);
    Yang et al., [2019](#bib.bib154); Tang et al., [2019](#bib.bib132); Dong et al.,
    [2020b](#bib.bib29); Zhang et al., [2020d](#bib.bib177); Yin et al., [2020](#bib.bib160);
    Zhang et al., [2021b](#bib.bib183), [a](#bib.bib181), [2022c](#bib.bib182); Huang
    et al., [2021](#bib.bib58); Sheng et al., [2022](#bib.bib122)) |'
  prefs: []
  type: TYPE_TB
- en: '| SSIM |  (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162);
    Metwaly et al., [2020](#bib.bib97); Wei et al., [2020](#bib.bib146); Singh et al.,
    [2020](#bib.bib128); Li et al., [2020e](#bib.bib78); Jo and Sim, [2021](#bib.bib63);
    Shyam et al., [2021](#bib.bib124); Zhao et al., [2021a](#bib.bib187)) |'
  prefs: []
  type: TYPE_TB
- en: '| MS-SSIM |  (Sun et al., [2021](#bib.bib130); Guo et al., [2019a](#bib.bib48);
    Cong et al., [2020](#bib.bib24); Yu et al., [2021](#bib.bib163); Fu et al., [2021](#bib.bib40))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Perceptual |  (Sim et al., [2018](#bib.bib126); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2021](#bib.bib191); Zhang et al., [2022c](#bib.bib182); Wang et al.,
    [2021c](#bib.bib135); Deng et al., [2020](#bib.bib26); Liu et al., [2019a](#bib.bib91);
    Hong et al., [2020](#bib.bib55); Qu et al., [2019](#bib.bib110); Chen and Lai,
    [2019](#bib.bib15); Li et al., [2019a](#bib.bib75); Chen et al., [2019d](#bib.bib20);
    Singh et al., [2020](#bib.bib128); Dong et al., [2020a](#bib.bib32); Shyam et al.,
    [2021](#bib.bib124); Engin et al., [2018](#bib.bib38)) |'
  prefs: []
  type: TYPE_TB
- en: '| TV Loss |  (Das and Dutta, [2020](#bib.bib25); Li et al., [2020a](#bib.bib79);
    Shao et al., [2020](#bib.bib119); Huang et al., [2019](#bib.bib57); Wang et al.,
    [2021c](#bib.bib135); He et al., [2019](#bib.bib54)) |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient |  (Zhang and Patel, [2018](#bib.bib164); Zhang et al., [2019a](#bib.bib166),
    [2020b](#bib.bib175), [2020e](#bib.bib184); Yin et al., [2020](#bib.bib160); Zhang
    et al., [2022b](#bib.bib170); Li et al., [2021c](#bib.bib80); Dudhane et al.,
    [2019](#bib.bib37); Yin et al., [2021](#bib.bib161)) |'
  prefs: []
  type: TYPE_TB
- en: 2.5\. Image Quality Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the presence of haze, the saturation and contrast of the image are reduced,
    and color of the image is distorted by the uncertainty mode. To measure the difference
    between the dehazed image and the ground truth haze-free image, objective metrics
    are needed to evaluate the results obtained by various dehazing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most papers use Peak Signal-to-Noise Ratio (PSNR) (Huynh-Thu and Ghanbari,
    [2008](#bib.bib60)) and SSIM (Wang et al., [2004](#bib.bib144)) to evaluate the
    image quality after dehazing. The computation of PSNR needs to use the formula
    ([11](#S2.E11 "In 2.5\. Image Quality Metrics ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")) to obtain
    the mean square error (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $MSE=\frac{1}{H\times{W}}\sum_{i=1}^{H}\sum_{j=1}^{W}(X(i,j)-Y(i,j))^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $X$ and $Y$ respectively represent two images to be evaluated. $H$ and
    $W$ are their height and width, that is, the dimensionalities of $X$ and $Y$ should
    be strictly the same. The pixel position index of the image is represented by
    $i$ and $j$. Then, the PSNR can be obtained by logarithmic calculation as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $PSNR=10log_{10}[\frac{({2^{N}-1)^{2}}}{MSE}],$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $N$ equals $8$ for $8$-bit images. SSIM is based on the correlation between
    human visual perception and structural information, and its formula is defined
    as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $SSIM(X,Y)=\frac{{2u_{x}}{u_{y}}+C_{1}}{u_{x}^{2}+u_{y}^{2}+C_{1}}\frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mu_{x}$, $\mu_{y}$, $\sigma_{x}$ and $\sigma_{y}$ represent the mean
    and variance of $X$ and $Y$, respectively; $\sigma_{xy}$ is the covariance between
    two variables; $C_{1}$ and $C_{2}$ are constants used to ensure numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: Since haze can cause the color of scenes and objects to change, some works use
    CIEDE2000 (Sharma et al., [2005](#bib.bib120)) as an assessment of the degree
    of color shift. PSNR, SSIM and CIEDE belong to full-reference evaluation metrics,
    which means that a clear image corresponding to a hazy image must be used as a
    reference. However, real world pairs of hazy and haze-free images are difficult
    to keep exactly the same in content. For example, the lighting and objects in
    the scene may change before and after the haze appears. Therefore, in order to
    maintain the accuracy of the evaluation process, it is necessary to synthesize
    the corresponding hazy image with a clear image (Min et al., [2019](#bib.bib98)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the full-reference metric PSNR, SSIM and CIEDE, the recent works (Li
    et al., [2020c](#bib.bib82), [2019c](#bib.bib74)) utilizes the no-reference metrics
    SSEQ (Liu et al., [2014](#bib.bib87)) and BLIINDS-II (Saad et al., [2012](#bib.bib117))
    to evaluate dehazed images without ground truth. No-reference metric is of crucial
    value for real world dehazing evaluation. Nevertheless, the evaluation of current
    dehazing algorithms are usually conducted on datasets with pairs of hazy and haze-free
    images. Since the full-reference metric is more suitable for paired datasets,
    it is more widely used than the no-reference metric.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Supervised Dehazing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised dehazing models usually require different types of supervisory signals
    to guide the training process, such as transmission map, atmospheric light, haze-free
    image label, etc. Conceptually, supervised dehazing methods can be divided into
    ASM-based and non-ASM-based ones. However, there may be overlaps in this way,
    since both ASM-based and non-ASM-based algorithms may entangle with other computer
    vision tasks such as segmentation, detection, and depth estimation. Therefore,
    this section categorizes supervised algorithms according to their main contributions,
    so that the techniques that prove valuable for dehazing research are clearly observed.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Learning of $t(x)$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the ASM, the dehazing process can be divided into three parts:
    transmission map estimation, atmospheric light prediction, and haze-free image
    recovery. MSCNN (Ren et al., [2016](#bib.bib111)) proposes the following three
    steps for solving ASM: (1) use CNN to estimate the transmission map $t(x)$, (2)
    adopt statistical rules to predict atmospheric light $A$, and (3) solve $J(x)$
    by $t(x)$ and $A$ jointly. MSCNN adopts a multi-scale convolutional model for
    transmission map estimation and optimizes it with L2 loss. In addition, $A$ can
    be obtained by selecting $0.1\%$ darkest pixels in $t(x)$ corresponding the one
    with the highest intensity in $I(x)$ (He et al., [2010](#bib.bib52)). Thus the
    clear image $J(x)$ can be obtained by'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $J(x)=\frac{I(x)-A}{max{\{0.1,t(x)\}}}+A.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Different papers may use different statistical priors to estimate $A$, but the
    strategies they use for dehazing are similar to MSCNN. ABC-Net (Wang et al., [2020](#bib.bib134))
    uses the max pooling operation to obtain maximum value from each channel of $I(x)$.
    SID-JPM (Huang et al., [2018](#bib.bib59)) filters each channel of a RGB input
    image by a minimum filter kernel. Then the maximum value of each channel is used
    as the estimated $A$. LAPTN (Liu et al., [2018](#bib.bib89)) also applies the
    minimum filter together with the maximum filter for the prediction of $A$. These
    methods generally do not require atmospheric light annotations, but need paired
    of hazy images and transmission maps.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Joint Learning of $t(x)$ and $A$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using convolutional networks and statistical priors jointly to estimate
    the physical parameters of ASM, some works implement the prediction of physical
    parameters entirely through CNN. DCPDN (Zhang and Patel, [2018](#bib.bib164))
    estimates the transmission map through a pyramid densely connected encoder-decoder
    network, and uses a symmetric U-Net to predict the atmospheric light $A$. In order
    to improve the edge accuracy of the transmission map, DCPDN has designed a hybrid
    edge-preserving loss, which includes L2 loss, two-directional gradient loss, and
    feature edge loss.
  prefs: []
  type: TYPE_NORMAL
- en: DHD-Net (Xie et al., [2020](#bib.bib151)) designs a segmentation-based haze
    density estimation algorithm, which can segment dense haze areas and divide the
    global atmosphere light $A$ candidate areas. HRGAN (Pang et al., [2018](#bib.bib105))
    utilizes a multi-scale fused dilated convolutional network to predict $t(x)$,
    and employs a single-layer convolutional model to estimate $A$. PMHLD (Chen et al.,
    [2020](#bib.bib19)) uses a patch map generator and refine the network for transmission
    map estimation, and utilizes VGG-16 for atmospheric light estimation. It is worth
    noting that if using regression training to obtain $A$, the ground truth labels
    are generally required.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Non-explicitly Embedded ASM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ASM can be incorporated into CNN in a reformulated or embedded way. AOD-Net (Li
    et al., [2017a](#bib.bib73)) founds that the end-to-end neural network can still
    be used to solve the ASM without directly using ground truth $t(x)$ and $A$. According
    to the original ASM, the expression of $J(x)$ is
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $J(x)=\frac{1}{t(x)}I(x)-A\frac{1}{t(x)}+A.$ |  |'
  prefs: []
  type: TYPE_TB
- en: AOD-Net proposes $K(x)$, which has no actual physical meaning, as an intermediate
    parameter describing $t(x)$ and $A$. $K(x)$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $K(x)=\frac{\frac{1}{t(x)}(I(x)-A)+(A-b)}{I(x)-1},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $b$ equals $1$. According to the ASM theory, $J(x)$ can be uniquely determined
    by $K(x)$ as
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $J(x)=K(x)I(x)-K(x)+1.$ |  |'
  prefs: []
  type: TYPE_TB
- en: AOD-Net considers that the non-joint transmission map and atmospheric light
    prediction process may produce accumulated errors. Therefore, independent estimation
    of $K(x)$ can reduce the systematic error. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    extends this formulation in a multi-scale framework and utilizes fully point-wise
    convolutions to achieve fast and accurate dehazing performance. DehazeGAN (Zhu
    et al., [2018](#bib.bib192)) incorporates the idea of differentiable programming
    into the estimation process of $A$ and $t(x)$. Combined with a reformulated ASM,
    DehazeGAN also implements an end-to-end dehazing pipeline. PFDN (Dong and Pan,
    [2020](#bib.bib31)) embeds ASM in the network design and proposes a feature dehazing
    unit, which removes haze in a well-designed feature space rather in the raw image
    space. It is instructive that ASM can still help the dehazing task for non-explicit
    parameter estimation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fdddfd7395438914517d66d6a263617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Generative adversarial network for dehazing, where gt means ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Generative Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative adversarial networks have an important impact on dehazing research.
    In general, supervised dehazing networks that rely on paired data can use adversarial
    loss as an auxiliary supervisory signal. Adversarial loss (Gui et al., [2022](#bib.bib47))
    can be seen as two parts: the training objective of the generator is to generate
    images that the discriminator considers to be real. The optimization purpose of
    the discriminator is to distinguish the generated image from the real image contained
    in the dataset as possible as it could. For dehazing task, the effect of the adversarial
    loss is to make the generated image closer to the real one, which is beneficial
    for the optimization of the haze-free $J(x)$ and transmission map $t(x)$ (Zhang
    et al., [2019a](#bib.bib166)), as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Non-explicitly
    Embedded ASM ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by patchGAN (Isola et al., [2017](#bib.bib61)), which can better preserve
    high-frequency information, DH-GAN (Sim et al., [2018](#bib.bib126)), RI-GAN (Dudhane
    et al., [2019](#bib.bib37)) and DehazingGAN (Zhang et al., [2020c](#bib.bib176))
    use $N\times N$ patches instead of a single value as the output of the discriminator.
    Several works explore joint training mechanisms of multiple discriminators, such
    as EPDN (Qu et al., [2019](#bib.bib110)) and PGC-UNet (Zhao et al., [2021a](#bib.bib187)).
    Discriminator $D_{1}$ is used to guide the generator on a fine scale, while discriminator
    $D_{2}$ helps the generator to produce a global realistic output on a coarse scale.
    In order to realize the joint training of the two discriminators, EPDN downsamples
    the input image of $D_{1}$ by a factor of $2$ as the input of $D_{2}$. The adversarial
    loss is
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $L_{adv}=\min_{G}[\max_{D_{1},D_{2}}\sum_{k=1,2}\ell_{A}(G,D_{k})],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the form of adversarial loss $\ell_{A}$ is the same as the single GAN.
    The exploration of GAN brings plug-and-play tools to supervised algorithms. Since
    the training of the discriminator can be done in an unsupervised manner, the quality
    of the dehazed images can be improved without the requirement of extra labels.
    However, the training of GAN sometimes suffers from instability and non-convergence,
    which may bring certain additional difficulties to the training of the dehazing
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Level-aware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the scattering theory, the farther the scene is from the camera,
    the more aerosol particles pass through. This means that areas within a single
    hazy image that are farther from the camera have higher densities of haze. Therefore,
    LAP-Net (Li et al., [2019b](#bib.bib83)) proposes that the algorithm should consider
    the difference in haze density inside the image. Through multi-stage joint training,
    LAP-Net implements an easy-to-hard model that focuses on a specific haze density
    level by a stage-wise loss
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $\hat{t}^{s}(x)=\begin{cases}\mathcal{F}(I(x),\theta^{s}),\text{if
    }s=1,\\ \mathcal{F}(I(x),\theta^{s},\hat{t}^{s-1}(x)),\text{if }s>1,\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{F}$ represents the transmission map prediction network with
    parameter $\theta^{s}$ in stage $s$. In the first stage, the transmission map
    prediction network is responsible for estimating the case with mild haze. In the
    second and subsequent stages, the prediction result of the previous stage and
    the hazy image are used as joint input for processing higher haze density.
  prefs: []
  type: TYPE_NORMAL
- en: The density of haze may be related to conditions such as temperature, wind,
    altitude, and humidity. Thus, the formation of haze should be space-variant and
    non-homogeneous. Based on this observation, HardGAN (Deng et al., [2020](#bib.bib26))
    argues that estimating the transmission map for dehazing may be inaccurate. By
    encoding the atmospheric brightness as $1\times 1\times 2$ matrix $\gamma_{i}^{G}\&amp;\beta_{i}^{G}$
    and pixel-wise spatial information as $H\times W\times 2$ matrix $\gamma_{i}^{L}\&amp;\beta_{i}^{L}$
    for $i$-th channel of the input $x$, HardGAN designs the control function of atmospheric
    brightness $G_{i}$ and spatial information $L_{i}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G_{i}=\gamma_{i}^{G}\frac{x-\mu}{\sigma}+\beta_{i}^{G},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (20) |  | $\displaystyle L_{i}=\gamma_{i}^{L}\frac{x-\mu}{\sigma}+\beta_{i}^{L},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mu$ and $\sigma$ denote the mean and standard deviation for $x$, respectively.
    After obtaining $G_{i}$ and $L_{i}$, HardGAN uses a linear model to fuse them
    for recovering haze-free image
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $J_{pred_{i}}(x)=(1-HA_{i})*G_{i}+HA_{i}*L_{i},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $HA$ is calculated by the intermediate feature map of HardGAN via using
    instance normalization followed by a sigmoid layer, and $*$ denotes element-wise
    product.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Multi-function Fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DMMFD (Deng et al., [2019](#bib.bib27)) designs a layer separation and fusion
    model for improving learning ability, including reformulated ASM, multiplication,
    addition, exponentiation and logarithmic decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J_{0}(x)=\frac{I(x)-A_{0}\times(1-t_{0}(x))}{t_{0}(x)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle J_{1}(x)=I(x)\times R_{1}(x),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle J_{2}(x)=I(x)+R_{2}(x),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle J_{3}(x)=(I(x))^{R_{3}(x)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (22) |  | $\displaystyle J_{4}(x)=\log(1+I(x)\times R_{4}(x)),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $R_{i}(x)$ stands for layers in the network; $A_{0}$ and $t_{0}(x)$ are
    the atmospheric light and transmission map estimated by the feature extraction
    network, respectively. $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$, and $J_{4}{(x)}$
    can be used as four independent haze-layer separation models. It is based on the
    assumption that the input hazy image $I(x)$ can be separated into a haze-free
    layer $J(x)$ and another layer $H(x)$, denoted as $I(x)=\phi(J(x),H(x))$. The
    final dehazing result is obtained by weighted fusion of the intermediate outputs
    $J_{0}{(x)}$, $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$ and $J_{4}{(x)}$ by five
    learned attention maps $W_{0}$, $W_{1}$, $W_{2}$, $W_{3}$ and $W_{4}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\displaystyle J_{pred}(x)=$ | $\displaystyle W_{0}\times J_{0}{(x)}+W_{1}\times
    J_{1}{(x)}+W_{2}\times J_{2}{(x)}+W_{3}\times J_{3}{(x)}+W_{4}\times J_{4}{(x)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Through ablation studies, DMMFD has demonstrated that the fusion of multiple
    layers can improve the quality of the scene restoration process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Transformation and Decomposition of Input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GFN (Ren et al., [2018a](#bib.bib112)) proposes two observations on the influence
    of haze. First, under the influence of atmospheric light, the color of a hazy
    image may be distorted to some extent. Second, due to the existence of scattering
    and attenuation phenomena, the visibility of objects far away from the camera
    in the scene will be reduced. Therefore, GFN uses three enhancement strategies
    to process the original hazy image and use them as inputs to the dehazing network
    together. The white balanced input $I_{wb}(x)$ is obtained from the gray world
    assumption. The contrast enhanced input $I_{ce}(x)$ is composed of average luminance
    value $\widetilde{I}(x)$ and the control factor $\mu$, as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $I_{ce}{(x)}=\mu(I(x)-\widetilde{I}{(x)}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mu=2\cdot(0.5+\widetilde{I}(x))$. By using the the nonlinear gamma correction,
    the $I_{gc}{(x)}$ used to enhance the visibility of the $I(x)$ can be obtained
    by
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $I_{gc}{(x)}=\alpha{I(x)^{\gamma}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha=1$, and $\gamma=2.5$. The final dehazing image $J(x)$ is determined
    by the combination of three inputs, where $C_{wb}$, $C_{ce}$ and $C_{gc}$ are
    confidences map for fusion process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $J(x)=C_{wb}\circ{I_{wb}{(x)}}+C_{ce}\circ{I_{ce}{(x)}}+C_{gc}\circ{I_{gc}{(x)}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)) decomposes the hazy image
    into low frequency and high frequency as the base component $I_{base}{(x)}$ and
    the detail component $I_{detail}{(x)}$, respectively. The basic component can
    be thought as the main content of the image, while the high frequency component
    denotes the edge and texture. Therefore, the dehazed image can be obtained by
    the dehazing function $D(\cdot)$ of the basic component and the enhancement function
    $E(\cdot)$ of the detail component. The whole process can be represented by a
    linear model as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I(x)=I_{base}{(x)}+I_{detail}{(x)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (27) |  | $\displaystyle J(x)=D(I_{base}{(x)})+E(I_{detail}{(x)}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: DIDH (Shyam et al., [2021](#bib.bib124)) decomposes both the input hazy image
    $I(x)$ and the predicted haze-free image $J(x)$ to obtain $(I(x),LF(I(x)))$, $(I(x),HF(I(x)))$,
    $(J(x),LF(J(x)))$ and $J(x),HF(J(x))$, where $LF(\cdot)$ and $HF(\cdot)$ denotes
    Gaussian and Laplacian filter, respectively. By fusing the decomposed data with
    the pre-decomposition data as the input of the discriminator, DIDH can improve
    the quality of the image generated by the adversarial training process. With the
    help of the discriminators $D_{LF}$ and $D_{HF}$, the dehazing network can be
    optimized in an adversarial way.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with conventional data augmentation, such as rotation, horizontal or
    vertical mirror symmetry, and random cropping, the transformation and decomposition
    of the input is a more efficient strategy for the usage of hazy images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a81a177c411c6c06da3da995439ee4cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. A general knowledge distillation strategy used in KDDN (Hong et al.,
    [2020](#bib.bib55)), KTDN (Wu et al., [2020](#bib.bib148)), SRKTDN (Chen et al.,
    [2021a](#bib.bib17)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8\. Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation (Gou et al., [2021](#bib.bib45)) provides a strategy
    to transfer the knowledge learned by the teacher network to the student network,
    which has been applied in high-level computer vision tasks like object detection
    and image classification (Wang et al., [2019](#bib.bib137)). Recent work (Hong
    et al., [2020](#bib.bib55)) presents three challenges for applying knowledge distillation
    to the dehazing task. First, what kind of teacher task can help the dehazing task.
    Second, how the teacher network helps the dehazing network during training. Third,
    which similarity measure between teacher task and student task should be chosen.
    Fig. [7](#S3.F7 "Figure 7 ‣ 3.7\. Transformation and Decomposition of Input ‣
    3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") shows the knowledge distillation strategy adopted
    by various dehazing algorithms. Different methods may use different numbers and
    locations of output features to compute feature loss.
  prefs: []
  type: TYPE_NORMAL
- en: KDDN (Hong et al., [2020](#bib.bib55)) designs a process-oriented learning mechanism,
    where the teacher network $T$ is an auto-encoder for high-quality haze-free image
    reconstruction. When training the dehazing network, the teacher network assists
    in feature learning, and optimizes $L_{T}=||J(x)-T(J(x))||_{1}$. Therefore, the
    teacher task and the student task proposed by KDDN are two different tasks. In
    order to make full use of the feature information learned by the teacher network
    and help the training of the dehazing network. KDDN uses feature matching loss
    and haze density aware loss by a linear transformation function $g$, which are
    represented by ([28](#S3.E28 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")) and ([29](#S3.E29 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $L_{rm}=\sum_{(m,n)\in{C}}&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (29) |  | $L_{wrm}=\sum_{(m,n)\in{C}}\psi\times{&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $T^{m}$ represents the $m$-th layer of the teacher network, and the corresponding
    $S^{n}$ represents the $n$-th layer of the student network; $\psi$ is obtained
    by the normalization operation. KDDN can be trained without real transmission
    map, replaced by the residual between hazy and haze-free images.
  prefs: []
  type: TYPE_NORMAL
- en: KTDN (Wu et al., [2020](#bib.bib148)) is jointly trained using a teacher network
    and a dehazing network with the same structure. Through feature level loss, the
    prior knowledge possessed by the teacher network can be transferred to the dehazing
    network. SRKTDN (Chen et al., [2021a](#bib.bib17)) uses ResNet18 pre-trained on
    ImageNet (with classification layers removed) as the teacher network, transferring
    many statistical experiences to the Res2Net101 encoder for dehazing. DALF (Fang
    et al., [2021](#bib.bib39)) integrates dual adversarial training into the training
    process of knowledge distillation to improve the imitation ability of the student
    network to the teacher network. Applying knowledge distillation to dehazing networks
    provides a new and efficient way to introduce external prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9\. Transformation of Colorspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input data of the dehazing network are usually three channel color images
    in RGB mode. By calculating the mean square error of hazy and haze-free images
    in RGB space and YCrCb space on the Dense-Haze dataset, Bianco et al. (Bianco
    et al., [2019](#bib.bib12)) find that haze shows obvious numerical differences
    in the two spaces. The error values for the red, green and blue channels in RGB
    space are very close. However, in the YCrCb space, the error value of the luminance
    channel is significantly larger than that of the blue and red chroma components.
    AIP-Net (Wang et al., [2018a](#bib.bib133)) performs a similar error comparison
    of color space transformation on synthetic datasets and obtained the same conclusion
    as  (Bianco et al., [2019](#bib.bib12)). Quantitative results (Wang et al., [2018a](#bib.bib133);
    Bianco et al., [2019](#bib.bib12)) obtained by training the dehazing network in
    the YCrCb color space show that RGB space is not the only effective color mode
    for deep learning based dehazing methods. Furthermore, TheiaNet (Mehra et al.,
    [2021](#bib.bib95)) comprehensively analyzes the performance obtained by training
    the dehazing model in RGB, YCrCb, HSV and LAB color spaces. Experiments (Singh
    et al., [2020](#bib.bib128); Sheng et al., [2022](#bib.bib122); Chen et al., [2021a](#bib.bib17);
    Dudhane and Murala, [2019b](#bib.bib36)) show that converting images from RGB
    space to other color spaces for model training is an effective scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 3.10\. Contrastive Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the process of training a non-ASM-based supervised dehazing network, a common
    way is to use the hazy image as the input of the network and expect to obtain
    a clear image. In this process, the clear image is used as a positive example
    to guide the optimization of the network. By designing pairs of positive and negative
    examples, AECR-Net provides a new perspective for treating hazy and haze-free
    images. Specifically, the clear image $J(x)$ and the dehazed image $J_{pred}(x)$
    are taken as a positive sample pair, while the hazy image $I(x)$ and the dehazed
    image $J_{pred}{(x)}$ are taken as a negative sample pair. According to contrastive
    learning (Khosla et al., [2020](#bib.bib66)), $J_{pred}(x)$, $J(x)$ and $I(x)$
    can be regarded as anchor, positive and negative, respectively. For the pre-trained
    model $G(\cdot)$, the dehazing loss can be regarded as the sum of the reconstruction
    loss and the regularization term, as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\min{&#124;&#124;J(x)-\psi(I(x))&#124;&#124;_{1}}+\lambda\sum_{i=1}^{N}{\omega_{i}}\cdot\frac{&#124;&#124;G_{i}(J(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}}{&#124;&#124;G_{i}(I(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $G_{i}$ represents the output feature of the $i$-th layer of the pre-trained
    model; $\lambda$ is the weight ratio between the image reconstruction loss and
    the regularization term; $w_{i}$ is weight factor for feature output; $\phi$ is
    the dehazing network. AECR-Net provides a universal contrastive regularization
    strategy for existing methods without adding extra parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.11\. Non-deterministic Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dehazing methods based on deep learning usually set the optimization objective
    to obtain a single dehazed image. By introducing 2-dimensional latent tensors,
    pWAE (Kim et al., [2021](#bib.bib67)) can generate different styles of dehazed
    images, which extends the general training purpose. pWAE proposes dehazing latent
    space $z_{h}$ and style latent space $z_{s}$ for dehazing, and applies the mean
    function $\mu(\cdot)$ and standard deviation function $\sigma(\cdot)$ to perform
    the transformation of the space:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $z_{h\to{s}}=\sigma(z_{s})(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})})+\mu{(z_{s})}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: A natural question is how to adjust the magnitude of the spatial mapping to
    control the degree of style transformation. pWAE uses linear module $z_{h}^{s}=\alpha{z_{h\to{s}}}+(1-\alpha)z_{h}$
    to adjust the weight between the dehazed image and the style information. By controlling
    $\alpha$, different degrees of stylized dehazed images corresponding to the style
    image can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: DehazeFlow (Li et al., [2021b](#bib.bib77)) proposes that the dehazing task
    itself is ill-posedness, so it is unreliable to learn a model with deterministic
    one-to-one mapping. With a conditional normalizing flow network, DehazeFlow can
    compute the conditional distribution of clear images from a given hazy image.
    Therefore, it can learn a one-to-many mapping and obtain different dehazing results
    in the inference stage. The non-deterministic output (Kim et al., [2021](#bib.bib67);
    Li et al., [2021b](#bib.bib77)) brings interpretable flexibility to the dehazing
    algorithm. Since the visual evaluation criteria of individual human beings are
    inherently different, one-to-many dehazing mapping provides more options for photographic
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.12\. Retinex Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Non-deep learning based research (Galdran et al., [2018](#bib.bib41)) demonstrates
    the duality between Retinex and image dehazing. Apart from the already widely
    used ASM, recent work (Li et al., [2021c](#bib.bib80)) explores the combination
    of Retinex model and CNN for dehazing. Retinex theory proposes that the image
    can be regarded as the element-wise product of the reflectance image $R$ and the
    illumination map $L$. Assuming the reflectance $R$ is illumination invariant,
    the relationship between hazy and haze-free image can be modeled by a retinex-based
    decomposition model as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $I(x)=J(x)\ast\frac{L_{I}}{L_{J}}=J(x)\ast L_{r},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $*$ means the multiplication in an element-wise way. $L_{r}$ can be seen
    as absorbed and scattered light caused by haze, which is determined by the ratio
    of the hazy image illumination map $L_{I}$ and natural illumination map $L_{J}$.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with ASM, which has been proven to be reliable, Retinex has a more
    compact physical form and fewer parameters to be estimated, but is not widely
    combined with CNN for dehazing, yet.
  prefs: []
  type: TYPE_NORMAL
- en: 3.13\. Residual Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Rather than directly learning the mapping from hazy to haze-free image, several
    methods argue that the residual learning method can reduce the learning difficulty
    of the network. The dehazing methods based on residual learning is performed at
    the image level. GCANet (Chen et al., [2019c](#bib.bib14)) uses the residual {$J(x)-I(x)$}
    between haze-free and hazy images as the optimization objective. DRL (Du and Li,
    [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)) and POGAN (Du and
    Li, [2019](#bib.bib34)) believe that the way of residual learning is related to
    ASM, and the relationship can be obtained by reformulated ASM:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (33) |  | $\displaystyle I(x)$ | $\displaystyle=J(x)+(A-J(x))(1-t(x))=J(x)+r(x),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $r(x)=(A-J(x))(1-t(x))$ can be interpreted as a structural error term.
    By predicting nonlinear signal-dependent degradation $r(x)$, a clear image $J(x)=I(x)-r(x)$
    can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 3.14\. Frequency Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dehazing methods based on CNN usually use down-sampling and upsampling for
    feature extraction and clear image reconstruction, and this process less considers
    the frequency information contained in the image. There are currently two approaches
    to combine frequency analysis and dehazing networks. The first is to embed the
    frequency decomposition function into the convolutional network, and the second
    is to use frequency decomposition as a constraint for loss computation. For a
    given image, one low-frequency component and three high-frequency components can
    be obtained by wavelet decomposition. Wavelet U-net  (Yang and Fu, [2019](#bib.bib156))
    uses discrete wavelet transform (DWT) and inverse discrete wavelet transform (IDWT)
    to facilitate high quality restoration of edge information in a clear image. Through
    the 1D scaling function $\phi(\cdot)$ and the wavelet function $\psi(\cdot)$,
    the 2D wavelets low-low (LL), low-high (LH), high-low (HL) and high-high (HH)
    are calculated as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Phi_{LL}(m,n)=\phi(m)\phi(n),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Psi_{LH}(m,n)=\phi(m)\psi(n),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Psi_{HL}(m,n)=\psi(m)\phi(n),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (34) |  | $\displaystyle\Psi_{HH}(m,n)=\psi(m)\psi(n).$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    MsGWN (Dong et al., [2020c](#bib.bib30)) uses Gabor wavelet decomposition for
    feature extraction and reconstruction. By setting different degree values, the
    feature extraction module of MsGWN can obtain frequency information in different
    directions. EMRA-Net (Wang et al., [2021d](#bib.bib136)) uses Haar wavelet decomposition
    as a downsampling operation instead of nearest downsampling and strided-convolution
    to avoid the loss of image texture details. By embedding wavelet analysis into
    convolutional network or loss function, recent studies (Yang et al., [2020](#bib.bib157);
    Dharejo et al., [2021](#bib.bib28); Fu et al., [2021](#bib.bib40)) have shown
    that 2D wavelet transform can improve the recovery of high-frequency information
    in the wavelet domain. These works successfully apply wavelet theory and neural
    networks to dehazing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, TDN (Liu et al., [2020b](#bib.bib86)) introduces the Fast Fourier transform
    (FFT) loss as a constraint in the frequency domain. With the help of supervised
    training on amplitude and phase, the visual perceptual quality of images is improved
    without any additional computation in the inference stage.
  prefs: []
  type: TYPE_NORMAL
- en: 3.15\. Joint Dehazing and Depth Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scattering particles in the hazy environment will affect the accuracy of
    the depth information collected by the LiDAR equipment. S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)) proves that high-quality depth estimation algorithms can help
    the dehazing task. According to ASM, it can be known that the transmission map
    $t(x)$ and the depth map $d(x)$ have a negative exponential relationship $t(x)=e^{-\beta{d(x)}}$.
    Based on this physical dependency, SDDE (Lee et al., [2020a](#bib.bib69)) uses
    four decoders to integrate estimates of atmospheric light, clear image, transmission
    map, and depth map into an end-to-end training pipeline. In particular, SDDE proposes
    a depth-transmission consistency loss based on the observation that the standard
    deviation value ($std$) of the transmission map and the depth map pair should
    tend to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (35) |  | $L=&#124;&#124;std(\ln(t_{pred}{(x)}/d_{pred}{(x)}))&#124;&#124;_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $t_{pred}{(x)}$ and $d_{pred}{(x)}$ represent the predicted transmission
    map and depth map, respectively. Aiming at better dehazing, TSDCN-Net  (Cheng
    and Zhao, [2021](#bib.bib23)) designs a cascaded network for two-stage methods
    with depth information prediction. Quantitative experimental results (Yang and
    Zhang, [2022](#bib.bib155)) show that this joint estimation method can improve
    the accuracy of dehazing task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.16\. Segmentation and Detection with Dehazing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing experiments (Li et al., [2017a](#bib.bib73)) show that the existence
    of image haze may cause various problems in detection algorithms, such as missing
    targets, inaccurate localizations and unconfident category prediction. Recent
    work (Sakaridis et al., [2018](#bib.bib118)) has shown that haze also brings difficulties
    to semantic scene understanding. As a preprocessing module for high-level computer
    vision tasks, the dehazing process is usually separated from high-level computer
    vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Li et al. (Li et al., [2017a](#bib.bib73)) jointly optimize the object detection
    algorithm and AOD-Net, and the result proves that the dehazing algorithm can promote
    the detection task. LEAAL (Li et al., [2020c](#bib.bib82)) proposes that fine-tuning
    the parameters of the object detector during joint training may lead to a detector
    biased towards the haze-free images generated by the pretrained dehazing network.
    Different from the fine-tuning operation of Li et al. (Li et al., [2017a](#bib.bib73)),
    LEAAL uses object detection as an auxiliary task for dehazing and the parameters
    of the object detector are not fully updated during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: UDnD (Zhang et al., [2020g](#bib.bib186)) takes advantage of each other by jointly
    training a dehazing network and a dense-aware multi-domain object detector. The
    object detection network is trained by adopting the classification and localization
    terms used by Region Proposal Network and Region of Interest. The multi-task training
    approach used by UDnD can consider the reduced inter-domain gaps and the remained
    intra-domain gaps for different density levels.
  prefs: []
  type: TYPE_NORMAL
- en: Recent work has explored performing dehazing and high-level computer vision
    tasks simultaneously without using ASM. SDNet (Zhang et al., [2022a](#bib.bib178))
    combines semantic segmentation and dehazing into a unified framework in order
    to use semantic prior as a constraint for the optimization process. By embedding
    the predictions of the segmentation map into the dehazing network, SDNet performs
    a joint optimization of pixel-wise classification loss and regression loss. The
    classification loss is
  prefs: []
  type: TYPE_NORMAL
- en: '| (36) |  | $L_{sem}(s,s^{*})=-\frac{1}{P}\sum_{i}s_{i}^{*}{\log(s_{i})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $P$ is the total number of pixels; $s_{i}$ is the class prediction at
    position $i$; $s^{*}$ denotes the ground truth semantic annotation.
  prefs: []
  type: TYPE_NORMAL
- en: (Zhang et al., [2022a](#bib.bib178), [2020g](#bib.bib186); Li et al., [2020c](#bib.bib82))
    show that segmentation and detection can be performed by embedding with dehazing
    networks. The way of joint dehazing task and high-level vision task may reduce
    the computational load to a certain extent by sharing the learned features, which
    can expand the goal of dehazing research.
  prefs: []
  type: TYPE_NORMAL
- en: 3.17\. End-to-end CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The “end-to-end” CNN stands for the non-ASM-based supervised algorithms, which
    usually consist of well-designed neural networks that take a single hazy image
    as input and a haze-free image as output. Networks based on different ideas are
    adopted, which are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention mechanism: FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)), HFF (Zhang
    et al., [2022c](#bib.bib182)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoder-Decoder: CAE (Chen and Lai, [2019](#bib.bib15)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on dense block: 123-CEDH (Guo et al., [2019a](#bib.bib48)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'U-shaped structure: DSEU (Lee et al., [2020b](#bib.bib70)), MSBDN (Dong et al.,
    [2020b](#bib.bib29)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hierarchical network: DMHN (Das and Dutta, [2020](#bib.bib25)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fusion with bilateral grid learning: 4kDehazing (Zheng et al., [2021](#bib.bib190)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The end-to-end dehazing networks have an important impact on the entire dehazing
    field, proving that numerous deep learning models are beneficial to the dehazing
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Semi-supervised Dehazing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared with the research on supervised methods, semi-supervised dehazing (Zhao
    et al., [2021b](#bib.bib189); Chen et al., [2021b](#bib.bib21)) algorithms have
    received relatively less attention. An important advantage of semi-supervised
    methods is the ability to utilize both labeled and unlabeled datasets. Therefore,
    compared to the fully supervised dehazing model, the semi-supervised dehazing
    model can alleviate the requirement of paired hazy and haze-free images. For dehazing
    task, labeled datasets are usually synthetic or artificially generated, while
    unlabeled datasets usually contain real world hazy images. According to the analysis
    of the dataset in Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning"), there are inherent
    differences between synthetic data and real world data. Therefore, semi-supervised
    algorithms usually have the ability to mitigate the gaps between synthetic domain
    and the real domain.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the principles
    of various semi-supervised dehazing models, where the network is a schematic diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9dbf265897b637151ab61c1b6178b0e4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Pretrain and finetune              (b) Disentangled and reconstruction          
    (c) Two branches training
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8\. Semi-supervised dehazing methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Pretrain Backbone and Finetune
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PSD (Chen et al., [2021b](#bib.bib21)) proposes a domain adaptation method that
    can be extended by existing models. It first uses a powerful backbone network
    for pre-training to obtain a base model suitable for synthetic data. Then the
    fine-tuning of the real domain in an unsupervised manner is applied to the well-trained
    network, thereby improving the generalization ability of the model to real world
    hazy images. To achieve fine-tuning in the real domain, PSD combines DCP loss,
    Bright Channel Prior (BCP) (Wang et al., [2013](#bib.bib143)) loss and Contrast
    Limited Adaptive Histogram Equalization (CLAHE) loss together. BCP loss and CLAHE
    loss are
  prefs: []
  type: TYPE_NORMAL
- en: '| (37) |  | $L_{BCP}=&#124;&#124;t_{BCP}(x)-t_{pred}(x)&#124;&#124;_{1},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (38) |  | $L_{CLAHE}=&#124;&#124;I(x)-I_{CLAHE}(x)&#124;&#124;_{1},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $t_{BCP}$ represents the transmission map estimated by BCP and $t_{pred}$
    is the predicted output of the network. $I_{CLAHE}$ is the hazy image reconstructed
    by CLAHE and other physical parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'During fine-tuning, the model may forget the useful knowledge it learned during
    the pre-training phase. Therefore, PSD proposes a feature-level constraint, which
    is achieved by calculating the feature map difference between the fine-tuning
    network and the pre-trained network. By feeding the synthetic data and real data
    into the fine-tuning network and the pre-trained network, respectively, four feature
    maps can be obtained: $F_{syn}^{tune}$, $F_{syn}^{pre}$, $F_{real}^{tune}$, and
    $F_{real}^{pre}$. Then, the loss for preventing knowledge forgetting can be calculated
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '| (39) |  | $L_{lwf}=&#124;&#124;F_{syn}^{tune}-F_{syn}^{pre}&#124;&#124;_{1}+&#124;&#124;F_{real}^{tune}-F_{real}^{pre}&#124;&#124;_{1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(a), supervised
    pre-training is performed first, and then the dehazing network is fine-tuned in
    an unsupervised form.
  prefs: []
  type: TYPE_NORMAL
- en: SSDT (Zhang and Li, [2021](#bib.bib172)) uses the encoder-decoder network for
    pre-training in the form of domain translation. After pre-training, two encoders
    and one decoder are selected to obtain the holistic dehazing network $G(\cdot)$.
    Then, $G(\cdot)$ is fine-tuned with synthetic hazy images and real world hazy
    images. The two-stage method described above needs to ensure that the pre-training
    process can meet the accuracy requirements, otherwise it may bring accumulated
    errors to the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Disentangled and Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the perspective of dual learning, the dehazing task and the haze generation
    task can be able to assist each other. Based on this assumption, DCNet (Chen et al.,
    [2021c](#bib.bib22)) proposes a dual-task cycle network that jointly utilizes
    labeled dataset $N$ and unlabeled dataset $M$ by dehazing network $DN(\cdot)$
    and haze generation network $HGN(\cdot)$. The total loss is combine by dehazing
    loss $L_{DN}$ and reconstruction loss $L_{HGN}$, that is $L_{total}=L_{DN}+L_{HGN}$,
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\displaystyle L_{total}=$ | $\displaystyle\sum_{i=1}^{M+N}P(I_{i}(x))L(DN(I_{i}(x)),J_{i}(x),\epsilon_{1})+\lambda{L(HGN(DN(I_{i}(x))),I_{i}(x),\epsilon_{2})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon_{1}$ and $\epsilon_{2}$ are regularizing hyper-parameter; $P(I_{i}(x))$
    equals $1$ when the $I_{i}(x)$ comes from the labeled dataset $N$, and equals
    $0$ otherwise. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    (b), the predicted haze-free image $J_{pred}(x)$ is first obtained by the left
    network, and then the hazy image $I_{rec}(x)$ is reconstructed by the right network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Liu et al. (Liu et al., [2021](#bib.bib93)) use a disentangled image dehazing
    network (DID-Net) and a disentangled-consistency mean-teacher network (DMT-Net)
    to combine labeled and unlabeled data. DID-Net is responsible for disentangling
    the hazy image into a haze-free image, the transmission map, and the global atmospheric
    light. DMT-Net is used to jointly exploit the labeled synthetic data and unlabeled
    real world data through disentangled consistency loss. The supervised loss consists
    of four terms: haze-free image prediction $L_{J}^{s}$, transmission map prediction
    $L_{t}^{s}$, atmospheric light prediction $L_{A}^{s}$, and hazy image reconstruction
    $L_{rec}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{J}^{s}=&#124;&#124;G_{J}-P_{J}&#124;&#124;_{1}+&#124;&#124;G_{J}-\hat{P}_{J}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{T}^{s}=&#124;&#124;G_{T}-P_{T}&#124;&#124;_{1}+&#124;&#124;G_{T}-\hat{P}_{T}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{A}^{s}=&#124;&#124;G_{A}-P_{A}&#124;&#124;_{1}+&#124;&#124;G_{A}-\hat{P}_{A}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (41) |  | $\displaystyle L_{rec}=&#124;&#124;I(x)-P_{I}&#124;&#124;_{1}+&#124;&#124;I(x)-\hat{P}_{I}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $G$ stands for ground-truth, $P$ is the prediction of the first stage,
    and $\hat{P}$ is the prediction of the second stage. The supervised loss $L^{s}(x)$
    is the weighted sum of the above four losses, that is $L^{s}(x)=L_{J}^{s}+\alpha_{1}L_{T}^{s}+\alpha_{2}L_{A}^{s}+\alpha_{3}L_{rec}$.
    For unlabeled data, consistency loss is used to constrain the teacher network
    $T$ and the student network $S$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{J}^{c}=&#124;&#124;S_{J}-T_{J}&#124;&#124;_{1}+&#124;&#124;S_{\hat{J}}-T_{\hat{J}}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{T}^{c}=&#124;&#124;S_{T}-T_{T}&#124;&#124;_{1}+&#124;&#124;S_{\hat{T}}-T_{\hat{T}}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{A}^{c}=&#124;&#124;S_{A}-T_{A}&#124;&#124;_{1}+&#124;&#124;S_{\hat{A}}-T_{\hat{A}}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (42) |  | $\displaystyle L_{rec}^{c}=&#124;&#124;S_{I}-T_{I}&#124;&#124;_{1}+&#124;&#124;S_{\hat{I}}-T_{\hat{I}}&#124;&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the subscript $\hat{J}$, $\hat{T}$, $\hat{A}$ and $\hat{I}$ denote the
    results predicted in second stage. Thus, the loss for the unlabeled dataset is
    $L^{c}(y)=L_{J}^{c}+\alpha_{4}L_{T}^{c}+\alpha_{5}L_{A}^{c}+\alpha_{6}L_{rec}^{c}$.
    The final loss function of the semi-supervised framework consists of a supervised
    loss on the labeled dataset $N$ and a consistency loss on the unlabeled dataset
    $M$, that is $L_{total}=\sum_{x\in N}{L^{s}{(x)}}+\lambda\sum_{y\in M}{L^{c}{(y)}}$.
  prefs: []
  type: TYPE_NORMAL
- en: CCDM (Zhang et al., [2020f](#bib.bib180)) designs a color-constrained dehazing
    model that can be extended to a semi-supervised framework, which is achieved by
    the reconstruction of hazy images, the smoothing loss of $t(x)$ and $A$, etc.
    Experiments (Liu et al., [2021](#bib.bib93); Zhang et al., [2020f](#bib.bib180);
    Chen et al., [2021c](#bib.bib22)) show that the reconstruction of hazy images
    can provide effective supervisory signals in an unsupervised manner, which is
    instructive for semi-supervised frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Two-branches Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SSID (Li et al., [2020a](#bib.bib79)) designs an end-to-end network that integrates
    a supervised learning branch and an unsupervised learning branch. The training
    process of SSID uses both the labeled dataset and the unlabeled dataset by the
    following process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (43) |  | $J_{pred}(x)=G(I(x)),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $G(\cdot)$ consists of a supervised part $G_{s}(\cdot)$ and an unsupervised
    part $G_{u}(\cdot)$. The supervised loss composed of L2 loss and perceptual loss
    is used to ensure that the predicted image $J_{pred}(x)$ and its corresponding
    ground truth image are as close as possible, which is the same as the supervised
    dehazing algorithm. A combination of total variation loss and dark channel loss
    is used for unsupervised training. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\.
    Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")(c), supervised training and unsupervised training
    are performed in a shared weight manner.
  prefs: []
  type: TYPE_NORMAL
- en: SSIDN (An et al., [2022](#bib.bib2)) also combines supervised and unsupervised
    training processes. Supervised training is used to learn the mapping from hazy
    to haze-free images. With dark channel prior and bright channel prior (Wang et al.,
    [2013](#bib.bib143)) guiding the training process, the unsupervised branch incorporates
    the estimation of the transmission map and atmospheric light.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DAID (Shao et al., [2020](#bib.bib119)) adopts a domain adaptation model
    to jointly train multi-subnetworks. The $G_{S\to{R}}$ and $G_{R\to{S}}$ are the
    image translation modules used for the translation between the synthetic domain
    and the real domain, where $R$ and $S$ stand for real domain and synthetic domain,
    respectively. By using an image-level discriminator $D_{R}^{img}$ and a feature-level
    discriminator $D_{R}^{feat}$, the adversarial loss of the translation process
    can be calculated. In order to ensure that the content of images are maintained
    during the translation process, DAID uses cycle consistency loss to constrain
    the translation network. Furthermore, identity mapping loss is also used in the
    conversion process to restrict the identity of the image generation process in
    two domains. The training of the dehazing network $G_{R}$ is a combination of
    unsupervised and supervised processes. The supervised process minimizes the $L_{rm}$
    loss to make the dehazed image $J_{S\to{R}}$ closer to the corresponding haze-free
    image $Y_{S}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (44) |  | $L_{rm}=&#124;&#124;J_{S\to{R}}-Y_{S}&#124;&#124;_{2}^{2}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Then, the total variation loss and dark channel loss are used as unsupervised
    losses. For the training of the dehazing network $G_{S}$, there is also a combination
    of supervised loss and unsupervised loss. With the help of domain transformation
    and unsupervised loss, DAID can effectively reduce the gap between the synthetic
    domain and the real domain.
  prefs: []
  type: TYPE_NORMAL
- en: As introduced above (Li et al., [2020a](#bib.bib79); An et al., [2022](#bib.bib2);
    Shao et al., [2020](#bib.bib119)), using supervised branch and unsupervised branch
    for joint training to build a semi-supervised framework can effectively alleviate
    the problem of domain shift.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Unsupervised Dehazing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The supervised and semi-supervised dehazing methods have achieved excellent
    performance on public datasets. However, the training process requires paired
    data (i.e. hazy images and haze-free images / transmission maps), which are difficult
    to obtain in the real world. For outdoor scenes containing grass, water or moving
    objects, it is difficult to guarantee that two images taken under hazy and clear
    weather have exactly the same content. If the haze-free labels are not accurate
    enough, the accuracy of dehazed images will be reduced. Therefore,  (Engin et al.,
    [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al., [2020a](#bib.bib90);
    Wei et al., [2021](#bib.bib147)) explore dehazing algorithms in an unsupervised
    way.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a997349418cfbe573bc502ed67c9e046.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Between two domains      (b) Among multi-domains             (c) Without
    target               (d) Zero-shot
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Schematic diagram of the unsupervised dehazing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Unsupervised Domain Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the study of image style transfer and image-to-image translation, CycleGAN (Zhu
    et al., [2017](#bib.bib193)) is proposed, which provides a way for learning the
    bidirectional mapping functions between two domains. Inspired by CycleGAN, (Engin
    et al., [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al.,
    [2020a](#bib.bib90)) are designed for unsupervised transformation of hazy and
    haze-free / transmission domains. The Cycle-Dehaze (Engin et al., [2018](#bib.bib38))
    contains two generators $G$ and $F$, which are used to learn the mapping from
    hazy domain to haze-free domain and the reverse mapping, respectively. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") (a), the hazy and
    haze-free images are translated to each other by two generators. By sampling $x$
    and $y$ from the hazy domain $X$ and the haze-free domain $Y$, the Cycle-Dehaze
    uses the perceptual metric (denoted as $\psi(\cdot)$) to obtain the cyclic perceptual
    consistency loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (45) |  | $\displaystyle L_{cyc-p}=$ | $\displaystyle{&#124;&#124;\psi(x)-\psi(F(G(x)))}{&#124;&#124;}_{2}^{2}+{&#124;&#124;\psi(y)-\psi(G(F(y)))}{&#124;&#124;}_{2}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The overall loss function of Cycle-Dehaze is composed of cyclic perceptual consistency
    loss and Cycle-GAN’s loss function, which can alleviate the requirement of paired
    data. CDNet (Dudhane and Murala, [2019a](#bib.bib35)) also adopts a cycle-consistent
    adversarial approach for unsupervised dehazing network training. Unlike Cycle-Dehaze,
    CDNet embeds ASM into the network architecture for estimating transmission map,
    which enables it to acquire physical parameters while restoring haze-free images.
    E-CycleGAN (Liu et al., [2020a](#bib.bib90)) adds ASM and a priori statistical
    law to estimate atmospheric light on the basis of CycleGAN, which allows it to
    perform independent parameter estimation for sky regions.
  prefs: []
  type: TYPE_NORMAL
- en: USID (Huang et al., [2019](#bib.bib57)) introduces the concept of haze mask
    $H(x)$ and constant bias $\epsilon$, which can obtain the reformulated ASM as
  prefs: []
  type: TYPE_NORMAL
- en: '| (46) |  | $J(x)=\frac{I(x)-A}{H(x)}+A+\epsilon.$ |  |'
  prefs: []
  type: TYPE_TB
- en: By combining with cycle consistent loss, USID also removes the requirement of
    an explicit paired haze/depth data in an unsupervised multi-task learning manner.
    In order to decouple content and haze information, USID-DR (Liu, [2019](#bib.bib88))
    designs a content encoder and a haze encoder embedded in CycleGAN. This decoupling
    approach can enhance feature extraction and reconstruction during cycle consistent
    training. USID-DR proposes that making the output of the encoder conform to the
    Gaussian distribution by latent regression loss can improve the quality of hazy
    image synthesis process, thereby improving the overall dehazing performance. DCA-CycleGAN (Mo
    et al., [2022](#bib.bib99)) utilizes the dark channel to build the input and generates
    the attention for handling the nonhomogeneous haze. These CycleGAN-based methods
    demonstrate that unsupervised hazy and haze-free image domain transformation can
    achieve the same performance as supervised algorithms from both ASM-based and
    non-ASM-based perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the dehazing task, the haze density in hazy images can be very different.
    Most supervised, semi-supervised and unsupervised dehazing methods hold the view
    that hazy images and haze-free images should be treated as two domains, without
    considering the density among different examples. Recent work (Jin et al., [2020](#bib.bib62))
    proposes that it is an important issue to apply the haze density information to
    the unsupervised training process, thereby improving the generalization ability
    of the dehazing model to images with different densities of haze. An unsupervised
    conditional disentangle network, called UCDN (Jin et al., [2020](#bib.bib62)),
    is designed by incorporating conditional information into the training process
    of CycleGAN. Further, DHL-Dehaze (Cong et al., [2020](#bib.bib24)) analyzes multiple
    haze density levels of hazy images, and proposes that the difference in haze density
    should be fully utilized in the dehazing procedure. Compared to UCDN which contains
    four submodules, DHL-Dehaze has only two networks to train. The idea of DHL-Dehaze
    is based on the research of multi-domain image-to-image translation. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")(b), the images
    $I_{1}(x)$, $I_{2}(x)$ and $I_{3}(x)$ of different densities can be transformed
    to other domains, where $I_{3}(x)$ can be explained as the haze-free domain. The
    training of DHL-Dehaze consists of a adversarial process and a classification
    process. By sampling $X_{ori}=\{x_{ori}^{(1)},x_{ori}^{(2)},\ldots,x_{ori}^{(m)}\}$
    with corresponding density labels $L_{ori}=\{l_{ori}^{(1)},l_{ori}^{(2)},\ldots,l_{ori}^{(m)}\}$,
    the classification loss $L_{cls}^{ori}$ and adversarial loss $L_{dis}^{ori}$ in
    the source domain can be obtained by ([47](#S5.E47 "In 5.1\. Unsupervised Domain
    Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning")) and ([48](#S5.E48 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (47) |  | $L_{cls}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{ori}^{(i)}\mid
    x_{ori}^{(i)}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (48) |  | $L_{dis}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{dis}(x_{ori}^{(i)}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In order to generate the images of the target domain, the labels of target
    domain $L_{tar}=\{l_{tar}^{(1)},l_{tar}^{(2)},\ldots,l_{tar}^{(m)}\}$ are required.
    DHL-Dehaze sends $X_{ori}$ and $L_{tar}$ into the multi-scale generator (MST)
    together, and obtains new images with the same attributes as $L_{tar}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (49) |  | $X_{tar}=MST(X_{ori},L_{tar}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then, the classification loss $L_{cls}^{tar}$ and adversarial loss $L_{dis}^{tar}$
    of the target domain image can be obtained by using ([50](#S5.E50 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")) and ([51](#S5.E51 "In 5.1\.
    Unsupervised Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (50) |  | $L_{cls}^{tar}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{tar}^{(i)}\mid
    x_{tar}^{(i)}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (51) |  | $L_{dis}^{tar}=\frac{1}{m}\sum_{i=1}^{m}logD_{dis}(x_{tar}^{(i)}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: It is worth noting that UCDN and DHL-Dehaze do not use the hazy and haze-free
    images of the same scene for supervision, but apply the density of haze as the
    supervisory information for the training process.
  prefs: []
  type: TYPE_NORMAL
- en: The training of unsupervised domain transformation algorithms is more difficult
    than that of supervised algorithms. The convergence of GAN-based domain transformation
    algorithms is difficult to determine, which may lead to over-enhanced images.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Learning without Haze-free Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process of CycleGAN-based methods and DHL-Dehaze does not require
    paired data, which has greatly reduced the difficulty of data collection. Further,
    Deep-DCP (Golts et al., [2020](#bib.bib44)) proposes to use only hazy image for
    training process. The strategy of domain translation dehazing algorithms is to
    learn the mapping relationship between hazy and haze-free / transmission domains,
    while the main idea of Deep-DCP is to minimize the DCP (He et al., [2010](#bib.bib52))
    energy function. According to statistical assumption, the transmission map can
    be estimated in an unsupervised way. Based on the estimated transmission map and
    soft matting, the energy function $E(t_{\theta},I(x))$ can be obtained, where
    $\theta$ is the parameters for tuning. Thus, the goal of training is to minimize
    the energy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (52) |  | $\theta^{*}=\mathop{\arg\min}_{\theta}[\frac{1}{N}\sum_{i=1}^{N}E(t_{\theta}(I_{i}(x)),I_{i}(x))].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(c), the
    network can automatically learn the mapping from hazy to haze-free images without
    requiring target domain labels.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Unsupervised Image Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Double-DIP (Gandelsman et al., [2019](#bib.bib42)) proposes an unsupervised
    hierarchical decoupling framework based on the observation that the internal statistics
    of a mixed layer is more complex than the single layer that composes it. Suppose
    $Z$ is a linear sum of independent random variables $X$ and $Y$. From a statistical
    point of view, the entropy of $Z$ is larger than its independent components, that
    is, $H(Z)\geq\max{\{H(X),H(Y)\}}$. Based on this, Double-DIP proposes the loss
    for image layer decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (53) |  | $L=L_{reconst}+\alpha\cdot L_{excl}+\beta\cdot L_{reg},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $L_{reconst}$ represents the reconstruction loss of the hazy image, $L_{excl}$
    is the exclusion loss between the two DIPs, and $L_{reg}$ is the regularization
    loss used to obtain the continuous and smooth transmission map.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Zero-Shot Learning for Dehazing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data-driven unsupervised dehazing methods have achieved impressive performance.
    Unlike those models that require sufficient data to perform network training,
    ZID (Li et al., [2020b](#bib.bib72)) proposes a neural network dehazing process
    that only requires a single example. ZID has further reduced the dependence of
    the parameter learning process on data by combining the advantages of unsupervised
    learning and zero-shot learning. Three sub-networks $f_{J}(\cdot)$ (J-Net), $f_{T}(\cdot)$
    (T-Net) and $f_{A}(\cdot)$ (A-Net) are used to estimate the $J(x)$, $t(x)$ and
    $A$, respectively. By the reconstruction process, the hazy image $I(x)$ can be
    disentangled by minimizing $L_{rec}$ loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (54) |  | $L_{rec}=&#124;&#124;I_{rec}(x)-I(x)&#124;&#124;_{p},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $I_{rec}(x)$ is reconstructed hazy image, and $p$ denotes $p$-norm. The
    disentangled atmospheric light $f_{A}(x)$ and Kullback-Leibler divergence are
    used to obtain the atmospheric light $A$ as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (55) |  | $\displaystyle L_{A}=$ | $\displaystyle L_{H}+L_{KL}=&#124;&#124;f_{A}(x)-A(x)&#124;&#124;_{p}+KL(N(u_{z},\delta_{z}^{2})&#124;&#124;N(0,I)),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $L_{H}$ is the loss for $f_{A}(x)$ and initial hint value $A(x)$ is automatically
    learned from data. It should be noted that $A(x)$ in ZID is not the same as $A$
    in ASM; $N(0,I)$ stands for Gaussian distribution; $z$ is learned from input $x$.
    The unsupervised channel loss $L_{J}$ used in J-Net is for the decomposing process
    of the haze-free image $J(x)$. The $L_{J}$ is calculated based on the dark channel,
    and the formula is $L_{J}=||\min_{c\in\{r,g,b\}}(J^{c}(y))||_{p}$, where $c$ denotes
    color channel and $y$ stands for local patch of the J-Net output. The purpose
    of $L_{reg}$ is to enhance the stability of the model and make $A$ and $t(x)$
    smooth. The overall loss function of ZID is
  prefs: []
  type: TYPE_NORMAL
- en: '| (56) |  | $L=L_{rec}+L_{A}+L_{J}+L_{reg}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: YOLY (Li et al., [2021a](#bib.bib71)) uses three joint disentanglement subnetworks
    for clear image and physical parameter estimation, enabling unsupervised and untrained
    haze removal. As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(d),
    ZID and YOLY can learn the mapping from hazy to haze-free image using a single
    unlabeled example.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of the zero-shot algorithm is that the generalization ability
    is limited, and the network must be retrained for each unseen example to get good
    dehazing performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Experiment and Performance Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides quantitative and qualitative analysis of the dehazing
    performance of the baseline supervised, semi-supervised and unsupervised algorithms.
    The training examples used in the experiments are the indoor data ITS and outdoor
    data OTS from RESIDE (Li et al., [2019c](#bib.bib74)). In order to accurately
    compare the dehazed images and the real clear images, the indoor and outdoor images
    included in the SOTS provided by RESIDE are used as the test set. Aiming at ensuring
    the fairness and representativeness of the experimental results, 10 representative
    algorithms are selected for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'supervised: AOD-Net (Li et al., [2017a](#bib.bib73)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)), DMMFD (Deng et al., [2019](#bib.bib27)), FFA-Net (Qin et al.,
    [2020](#bib.bib109)), GCA-Net (Chen et al., [2019c](#bib.bib14)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), MSBDN (Dong et al., [2020b](#bib.bib29)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'semi-supervised: SSID (Li et al., [2020a](#bib.bib79)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'unsupervised: Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), ZID (Li et al.,
    [2020b](#bib.bib72)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The framework used for the algorithm implementation is PyTorch, and the running
    platform is NVIDIA Tesla V100 32 GB (2 GPUs). Except for the zero-shot ZID, all
    algorithms use a batch size of 8 in the training phase. For the ITS and OTS datasets,
    the training epochs are set according to (Qin et al., [2020](#bib.bib109)) and (Liu
    et al., [2019a](#bib.bib91)), respectively. For the supervised dehazing methods,
    four loss function strategies are adopted, including L1, L2, L1 + P, L2 + P, where
    P denotes the perceptual loss. For semi-supervised and unsupervised methods, the
    loss functions follow the settings in the respective papers. In the experiment,
    we first compared the convergence curves of different supervised algorithms when
    they were set to 0.001, 0.0005 or 0.0002, and selected the value with the best
    convergence effect as the best learning rate. After the training process, the
    loss values of all supervised models have stably converged. For semi-supervised
    and unsupervised algorithms, the learning rates used are those recommended in
    the corresponding papers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to ensure as few interference factors as possible in the experiments,
    the following strategies are not used in the experiments: (1) pre-training, (2)
    dynamic learning rate adjustment, (3) larger batch size, and (4) data augmentation.
    Therefore, the quantitative results obtained in the following experiments may
    be a little lower than the best results provided in the papers corresponding to
    the various algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Tables [4](#S6.T4 "Table 4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5
    "Table 5 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") show the PSNR/SSIM
    values obtained on the indoor and outdoor test sets of various algorithms on RESIDE
    SOTS, where the highest values are shown in bold. It can be concluded that FFA-Net
    achieves the best performance on the indoor test set, with the highest PSNR and
    SSIM. The best performance among different algorithms on the outdoor test set
    is achieved by different methods. It can be also seen from Tables [4](#S6.T4 "Table
    4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5 "Table 5 ‣ 6\.
    Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") that the perceptual loss has a certain
    effect on the performance of the model, but it is not obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6\. Experiment and Performance Analysis ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    and Figure [11](#S6.F11 "Figure 11 ‣ 6\. Experiment and Performance Analysis ‣
    A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    show the visual results achieved by the supervised, semi-supervised and unsupervised
    algorithms, respectively, where the loss function used by the supervised algorithms
    is the L1 loss. The visual results show that the dehazed images obtained by the
    supervised algorithms are closer to the real images in terms of color and detail
    than the semi-supervised/unsupervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Results on RESIDE SOTS indoor, where the SSIM and PNSR values are
    separated by the slash.
  prefs: []
  type: TYPE_NORMAL
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  prefs: []
  type: TYPE_TB
- en: '| AODNet | 0.839/19.375 | 0.841/19.454 | 0.851/19.576 | 0.839/19.388 |'
  prefs: []
  type: TYPE_TB
- en: '| 4kDehazing | 0.932/23.881 | 0.949/26.569 | 0.928/23.370 | 0.928/23.353 |'
  prefs: []
  type: TYPE_TB
- en: '| DMMDF | 0.959/30.585 | 0.960/30.383 | 0.961/30.631 | 0.961/30.725 |'
  prefs: []
  type: TYPE_TB
- en: '| FFA-Net | 0.983/32.730 | 0.983/32.536 | 0.978/32.224 | 0.978/32.178 |'
  prefs: []
  type: TYPE_TB
- en: '| GCA-Net | 0.924/25.044 | 0.930/25.638 | 0.917/24.716 | 0.909/24.714 |'
  prefs: []
  type: TYPE_TB
- en: '| GridDehazeNet | 0.962/25.671 | 0.962/25.655 | 0.935/22.940 | 0.940/23.052
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSBDN | 0.955/28.341 | 0.955/28.562 | 0.941/27.237 | 0.937/25.662 |'
  prefs: []
  type: TYPE_TB
- en: '| SSID | 0.814/20.959 |'
  prefs: []
  type: TYPE_TB
- en: '| Cycle-Dehaze | 0.810/18.880 |'
  prefs: []
  type: TYPE_TB
- en: '| ZID | 0.835/19.830 |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. Results on RESIDE SOTS outdoor, where the SSIM and PNSR values are
    separated by the slash.
  prefs: []
  type: TYPE_NORMAL
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  prefs: []
  type: TYPE_TB
- en: '| AODNet | 0.913/23.613 | 0.917/23.683 | 0.912/23.253 | 0.916/23.488 |'
  prefs: []
  type: TYPE_TB
- en: '| 4kDehazing | 0.963/28.476 | 0.964/28.473 | 0.958/28.103 | 0.950/27.546 |'
  prefs: []
  type: TYPE_TB
- en: '| DMMDF | 0.905/25.805 | 0.963/30.237 | 0.963/30.535 | 0.965/30.682 |'
  prefs: []
  type: TYPE_TB
- en: '| FFA-Net | 0.921/27.126 | 0.925/27.299 | 0.913/28.176 | 0.920/28.441 |'
  prefs: []
  type: TYPE_TB
- en: '| GCA-Net | 0.953/27.784 | 0.949/27.559 | 0.946/27.20 | 0.943/27.273 |'
  prefs: []
  type: TYPE_TB
- en: '| GridDehazeNet | 0.964/28.296 | 0.964/28.388 | 0.963/27.807 | 0.963/27.766
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSBDN | 0.962/29.944 | 0.963/30.277 | 0.964/29.843 | 0.956/28.782 |'
  prefs: []
  type: TYPE_TB
- en: '| SSID | 0.840/20.905 |'
  prefs: []
  type: TYPE_TB
- en: '| Cycle-Dehaze | 0.861/20.347 |'
  prefs: []
  type: TYPE_TB
- en: '| ZID | 0.633/13.520 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/f2feb7aa286362078dfe8186f310925d.png)![Refer to caption](img/e418d85a6ae343188b08b02a3ad6a92c.png)![Refer
    to caption](img/2428c902fccbec1bcacf909a5e7bd4df.png)![Refer to caption](img/389fd6a26aa53b01a8883f7b6ec217ab.png)![Refer
    to caption](img/b1d6407d47e0a14538c9976fcdfbd68c.png)![Refer to caption](img/4594fcd3634f81c59bcd89ed6da37c86.png)![Refer
    to caption](img/b6f5f2c29d263145e069929d814bface.png)![Refer to caption](img/43af5737033d352a00d5d9036de2496d.png)![Refer
    to caption](img/98fee125a5ea6f50f70277d099bafde8.png)![Refer to caption](img/35e8cd57d7c7d47c8802ad55c3519e2d.png)![Refer
    to caption](img/c9f2c3d3419ab045ef4b7e8101ed2a12.png)![Refer to caption](img/dee2c61c33aa4a2634a4f2e0abcbd6ce.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0265fca5f17e3afe4bacdd124402b6ac.png) ![Refer to caption](img/637f18ddf1be133d9d9a6d45dd4cc022.png)
    ![Refer to caption](img/e30793f2ea4c8952e6aad7b332237c23.png) ![Refer to caption](img/8d16c4a1f9f7ae603e2295cc36996415.png)
    ![Refer to caption](img/0f645aef82073de109986af5c39e4cba.png) ![Refer to caption](img/99aebfe9185de93e7e66932d62d42898.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Refer to caption](img/ad036f8379af4611d79a1147af6eaa15.png)![Refer to caption](img/53c7bd060ecb81cc8ee7992cf241b1d4.png)![Refer
    to caption](img/6d23ab8ae6c39bd64cd8340df126b260.png)![Refer to caption](img/8b4a6ffc7663471489aa68104c7f1056.png)![Refer
    to caption](img/e4b48e13428ee7136c32e0fbc905aebc.png)![Refer to caption](img/b4ec35306228437ec033cc0f53c76719.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10\. Visual results on RESIDE SOTS indoor.
  prefs: []
  type: TYPE_NORMAL
- en: For a fair comparison of the computational speed of the baseline methods, the
    zero-shot-based ZID and high-resolution-based 4kDehazing are excluded. Fig. [12](#S6.F12
    "Figure 12 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the average
    time for each algorithm that run 1000 times with PyTorch framework. It can be
    seen that AODNet is the fastest, and can achieve real-time effects for inputs
    of different sizes. FFA-Net is the slowest, taking more than 0.3 seconds to process
    the $672\times 672$ input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a4f08bf1d355f9968af362221046d1d.png)![Refer to caption](img/5b918adaccab1c195ed3dec9473afeec.png)![Refer
    to caption](img/08b5b66664070f079d16fb266494771e.png)![Refer to caption](img/9884719b91385e20926c3b32da2dce84.png)![Refer
    to caption](img/f0fba61952cd7d03bce779fcc518f669.png)![Refer to caption](img/0e1cffc4f3971a082e7d7c4f653c4c93.png)![Refer
    to caption](img/2f6c7b259446e234d2edb6ca5c54d084.png)![Refer to caption](img/9a7c55b7e8d9f4779a234ca3b9410640.png)![Refer
    to caption](img/f8b86157fa28cddb393ad089c7e207b5.png)![Refer to caption](img/e04635817fa8d55f6eb165a6e95fdfe6.png)![Refer
    to caption](img/6c0c8c5703ad99355a02a70f3975932f.png)![Refer to caption](img/bdac90bf8dac6d18140c54d8cf9ab913.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd980cba3ea42e600d042cd16e5b3e43.png) ![Refer to caption](img/839113d06d8afdc49672df5104101e1a.png)
    ![Refer to caption](img/cc48c167cfe455ef7c980f1e038e3a6f.png) ![Refer to caption](img/d54216a1933edd7e224448735e674f58.png)
    ![Refer to caption](img/2a23e72d85c231ac49545b13a7f24b3f.png) ![Refer to caption](img/e36b5bab9e12b91740e14c5fc89f7957.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Refer to caption](img/0568101726b59880cc8237702f6897c3.png)![Refer to caption](img/f1df20ef1bda3e07de87344d5e2a0e92.png)![Refer
    to caption](img/c037f06c9e16f2dada648ff67e987a32.png)![Refer to caption](img/3530b94362a34dc9b8befdd019bc8518.png)![Refer
    to caption](img/7a74af0e6abdda5eddbced6819596ca5.png)![Refer to caption](img/6e96c861dce4ea57cad9321f7d7cb7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Visual results on RESIDE SOTS outdoor.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24fcff6c5f47e30203db9e6bb2a5b858.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Running speed on different sizes of input.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Challenges and Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For image dehazing task, the current supervised, semi-supervised and unsupervised
    methods have achieved good performance. However, there are problems that still
    exist and open issues that should be explored in the future research. Next, we
    will discuss these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. More Effective ASM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current ASM has been proved to be suitable for describing the formation
    process of haze by many supervised, semi-supervised and unsupervised dehazing
    methods. However, recent work (Ju et al., [2021](#bib.bib65)) finds that the intrinsic
    limitation of ASM will cause a dim effect in the image after dehazing. By adding
    a new parameter, an enhanced ASM (EASM) (Ju et al., [2021](#bib.bib65)) is proposed.
    Improvements to ASM will have an important impact on the dehazing performance
    of existing methods that rely on ASM. Therefore, it is worth exploring a more
    accurate model of the haze formation process.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Shift between the Real Domain and Synthetic Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current training process of dehazing models generally requires sufficient
    data. Since it is difficult to collect pairs of hazy and haze-free images in the
    real world, the synthesized data is needed by dehazing task. However, there are
    inherent differences between the synthesized hazy image and the real world hazy
    image. In order to solve the domain shift problem, the following three directions
    are worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The haze synthesized based on ASM cannot completely simulate the formation of
    haze in the real world, so we can attempt to design a more realistic hazy image
    synthesis algorithm to compensate for the difference between domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By introducing domain adaptation, semi-supervised and unsupervised methods into
    network design, experiments show that these algorithms can perform well in real
    world dehazing task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent work (MRFID/BeDDE) collected some real world paired data, but they did
    not contain as many examples as RESIDE. It is challenging to build a large-scale
    real world dataset that can be used for a large-capacity CNN-based dehazing model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7.3\. Computational Efficiency and New Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dehazing models are often used as a preprocessing module for high-level
    computer vision tasks. For example, the lightweight AOD-Net is applied to object
    detection task. There are three issues that should be considered in the application
    of the dehazing model.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to help follow-up tasks, the dehazing model should ensure that the
    dehazed image is of high quality.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inference speed of the model must be fast enough to meet the real-time requirement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several end devices with small storage are sensitive to the number of parameters
    of the dehazing model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, we need to balance quantitative performance, inference time and the
    number of parameters. High-quality dehazed image with high PSNR and SSIM are the
    main goals of the current research. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    discusses the computational efficiency of 14 models, which shows that several
    algorithms with excellent performance may be very time and memory consuming. Dehazing
    algorithms based on deep learning usually require the use of graphical computing
    units for model training and deployment. In real world applications, the forward
    inference speed of an algorithm is an important evaluation metric. 4kDehazing (Zheng
    et al., [2021](#bib.bib190)) explores fast dehazing of high-resolution input,
    which takes only $8$ ms ($125$ fps) to process a 4K ($3840\times 2160$) image
    on a single Titan RTX GPU. Future research can try to design a new evaluation
    metric that can comprehensively consider the dehazing quality, running speed and
    model size.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Perceptual Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pre-trained model trained on a large-scale dataset can be used to calculate
    the perceptual loss. As shown in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\. Total Variation
    Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), many dehazing models use the
    perceptual loss obtained by VGG16 or VGG19 as a part of the overall loss to improve
    the quality of the final result. Thus, a natural question is, can the perceptual
    loss be the same as the L1/L2 loss as a general loss for image dehazing task?
    Alternatively, is there a more efficient and higher quality way to compute the
    perceptual loss? For example, is it a better solution to obtain perceptual loss
    by other pre-trained models? Currently, there is no comprehensive study on the
    relationship between perceptual loss and dehazing task.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. How Dehazing Methods Affect High-level Computer Vision Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many dehazing algorithms have been validated on high-level computer vision tasks,
    such as image segmentation and object detection. Experiments show that these dehazing
    methods can promote the performance of high-level computer vision tasks under
    hazy conditions. However, the recent study (Pei et al., [2018](#bib.bib108)) has
    shown that several dehazing algorithms may be ineffective for image classification,
    although their dehazing ability has been proven. The experimental results (Pei
    et al., [2018](#bib.bib108)) on synthetic and real hazy data show that several
    well-designed dehazing models have little positive effect on the performance of
    the classification model, and sometimes may reduce the classification accuracy
    to some extent.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, domain adaptation and generalization methods have been proposed to
    deal with high-level computer vision tasks such as object detection (Wang et al.,
    [2022](#bib.bib139), [2021a](#bib.bib138)) and semantic segmentation (Zhang et al.,
    [2019b](#bib.bib173); Gao et al., [2021](#bib.bib43)) in bad weather conditions
    including haze, which can be treated as mitigating the side effect of haze in
    the feature space by aligning hazy image features with those clean ones. It will
    be an interesting research topic to investigate and reduce the influence of haze
    on high-level computer vision tasks both at image-level (i.e., dehazing) and feature-level
    (i.e., domain adaptation).
  prefs: []
  type: TYPE_NORMAL
- en: 7.6\. Prior Knowledge and Learning Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before deep learning was widely used in image dehazing task, image-based prior
    statistical knowledge was an important part for guiding the dehazing process.
    Now, extensive work has shown that deep learning techniques can effectively remove
    haze from images independently of physical model. At the same time, several recent
    studies have found that prior statistical knowledge can be used for semi-supervised (Li
    et al., [2020a](#bib.bib79)) and unsupervised (Golts et al., [2020](#bib.bib44))
    network training. Effective prior knowledge can reduce the model’s dependence
    on data to a certain extent and improve the generalization ability of the model.
    However, there is currently no evidence which statistical priors are always valid.
    Therefore, the general prior knowledge that can be used for the CNN-based dehazing
    algorithm is worth verifying.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey of deep learning-based dehazing research.
    First, the commonly used physical model, high-quality datasets, general loss functions,
    effective network modules and evaluation metrics are summarized. Then, supervised,
    semi-supervised and unsupervised dehazing studies are classified and analyzed
    from different technical perspectives. Moreover, quantitative and qualitative
    dehazing performance of various baselines are discussed. Finally, we discuss several
    valuable research directions and open issues.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work was supported in part by the grant of the National Science Foundation
    of China under Grant 62172090, 62172089; Alibaba Group through Alibaba Innovative
    Research Program; CAAI-Huawei MindSpore Open Fund. All correspondence should be
    directed to Xiaofeng Cong and Yuan Cao. Dr Jing Zhang is supported by ARC research
    project FL-170100117.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A What is not discussed in this survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We noticed that several papers’ formulation description and open source code
    may not be consistent. In particular, loss functions not provided in several papers
    are used in the training code. In this case, we use the loss functions provided
    in the paper as a guide, regardless of the code implementation. Further, several
    public papers that have not yet been published formally are not covered in this
    survey, such as those on arxiv, since their content may be changed in the future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. (2022) Shunmin An, Xixia Huang, Le Wang, Linling Wang, and Zhangjing
    Zheng. 2022. Semi-Supervised image dehazing network. *The Visual Computer* 38,
    6 (2022), 2041–2055.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2016) Cosmin Ancuti, Codruta O Ancuti, and Christophe De Vleeschouwer.
    2016. D-hazy: A dataset to evaluate quantitatively dehazing algorithms. In *International
    Conference on Image Processing*. 2226–2230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018a) Cosmin Ancuti, Codruta O Ancuti, and Radu Timofte. 2018a.
    Ntire 2018 challenge on image dehazing: Methods and results. In *Conference on
    Computer Vision and Pattern Recognition Workshops*. 891–901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018b) Cosmin Ancuti, Codruta O Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018b. I-HAZE: a dehazing benchmark with real hazy and haze-free
    indoor images. In *International Conference on Advanced Concepts for Intelligent
    Vision Systems*. 620–631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2019a) Codruta O Ancuti, Cosmin Ancuti, Mateu Sbert, and Radu
    Timofte. 2019a. Dense-Haze: A Benchmark for Image Dehazing with Dense-Haze and
    Haze-Free Images. In *International Conference on Image Processing*. 1014–1018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2020a) Codruta O. Ancuti, Cosmin Ancuti, and Radu Timofte. 2020a.
    NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1798–1805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ancuti et al. (2018c) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018c. O-HAZE: a dehazing benchmark with real hazy and haze-free
    outdoor images. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    754–762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancuti et al. (2019b) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool,
    Lei Zhang, and Ming-Hsuan Yang. 2019b. NTIRE 2019 Image Dehazing Challenge Report.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 2241–2253.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancuti et al. (2020b) Codruta O Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu,
    and Radu Timofte. 2020b. NTIRE 2020 Challenge on NonHomogeneous Dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2029–2044.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Chaudhuri (2021) Sriparna Banerjee and Sheli Sinha Chaudhuri.
    2021. Nighttime Image-Dehazing: A Review and Quantitative Benchmarking. *Archives
    of Computational Methods in Engineering* 28 (2021), 2943–2975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianco et al. (2019) Simone Bianco, Luigi Celona, Flavio Piccoli, and Raimondo
    Schettini. 2019. High-resolution single image dehazing using encoder-decoder architecture.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1927–1935.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2016) Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and Dacheng
    Tao. 2016. Dehazenet: An end-to-end system for single image haze removal. *IEEE
    Transactions on Image Processing* 25, 11 (2016), 5187–5198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019c) Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Liheng
    Zhang, Dongdong Hou, Lu Yuan, and Gang Hua. 2019c. Gated Context Aggregation Network
    for Image Dehazing and Deraining. In *Winter Conference on Applications of Computer
    Vision*. 1375–1383.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Lai (2019) Rongsen Chen and Edmund M-K Lai. 2019. Convolutional Autoencoder
    For Single Image Dehazing.. In *International Conference on Image Processing*.
    4464–4468.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Shuxin Chen, Yizi Chen, Yanyun Qu, Jingying Huang, and Ming
    Hong. 2019a. Multi-scale adaptive dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2051–2059.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) Tianyi Chen, Jiahui Fu, Wentao Jiang, Chen Gao, and Si
    Liu. 2021a. SRKTDN: Applying Super Resolution Method to Dehazing Task. In *Conference
    on Computer Vision and Pattern Recognition*. 487–496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Wei-Ting Chen, Jian-Jiun Ding, and Sy-Yen Kuo. 2019b. PMS-Net:
    Robust Haze Removal Based on Patch Map for Single Images. In *Conference on Computer
    Vision and Pattern Recognition*. 11673–11681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Wei-Ting Chen, Hao-Yu Fang, Jian-Jiun Ding, and Sy-Yen Kuo.
    2020. PMHLD: patch map-based hybrid learning DehazeNet for single image haze removal.
    *IEEE Transactions on Image Processing* 29 (2020), 6773–6788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019d) Xuesong Chen, Haihua Lu, Kaili Cheng, Yanbo Ma, Qiuhao Zhou,
    and Yong Zhao. 2019d. Sequentially refined spatial and channel-wise feature aggregation
    in encoder-decoder network for single image dehazing. In *International Conference
    on Image Processing*. 2776–2780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021b) Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu. 2021b.
    PSD: Principled Synthetic-to-Real Dehazing Guided by Physical Priors. In *Conference
    on Computer Vision and Pattern Recognition*. 7180–7189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021c) Zhihua Chen, Yu Zhou, Ping Li, Xiaoyu Chi, Lei Ma, and
    Bin Sheng. 2021c. DCNet: Dual-Task Cycle Network for End-to-End Image Dehazing.
    In *International Conference on Multimedia and Expo*. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng and Zhao (2021) Lu Cheng and Li Zhao. 2021. Two-Stage Image Dehazing with
    Depth Information and Cross-Scale Non-Local Attention. In *International Conference
    on Big Data*. 3155–3162.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cong et al. (2020) Xiaofeng Cong, Jie Gui, Kai-Chao Miao, Jun Zhang, Bing Wang,
    and Peng Chen. 2020. Discrete Haze Level Dehazing Network. In *ACM International
    Conference on Multimedia*. 1828–1836.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Das and Dutta (2020) Sourya Dipta Das and Saikat Dutta. 2020. Fast deep multi-patch
    hierarchical network for nonhomogeneous image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 482–483.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2020) Qili Deng, Ziling Huang, Chung-Chi Tsai, and Chia-Wen Lin.
    2020. Hardgan: A haze-aware representation distillation gan for single image dehazing.
    In *European Conference on Computer Vision*. 722–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2019) Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu,
    Qing Zhang, Jing Qin, and Pheng-Ann Heng. 2019. Deep multi-model fusion for single-image
    dehazing. In *International Conference on Computer Vision*. 2453–2462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dharejo et al. (2021) Fayaz Ali Dharejo, Yuanchun Zhou, Farah Deeba, Munsif Ali
    Jatoi, Muhammad Ashfaq Khan, Ghulam Ali Mallah, Abdul Ghaffar, Muhammad Chhattal,
    Yi Du, and Xuezhi Wang. 2021. A deep hybrid neural network for single image dehazing
    via wavelet transform. *Optik* 231 (2021), 166462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2020b) Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang,
    Fei Wang, and Ming-Hsuan Yang. 2020b. Multi-scale boosted dehazing network with
    dense feature fusion. In *Conference on Computer Vision and Pattern Recognition*.
    2157–2167.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2020c) Hang Dong, Xinyi Zhang, Yu Guo, and Fei Wang. 2020c. Deep
    multi-scale gabor wavelet network for image restoration. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2028–2032.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong and Pan (2020) Jiangxin Dong and Jinshan Pan. 2020. Physics-based feature
    dehazing networks. In *European Conference on Computer Vision*. 188–204.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2020a) Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, and Yu Qiao.
    2020a. FD-GAN: Generative Adversarial Networks with Fusion-Discriminator for Single
    Image Dehazing. In *AAAI Conference on Artificial Intelligence*. 10729–10736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Li (2018) Yixin Du and Xin Li. 2018. Recursive deep residual learning
    for single image dehazing. In *Conference on Computer Vision and Pattern Recognition
    Workshops*. 730–737.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Li (2019) Yixin Du and Xin Li. 2019. Recursive image dehazing via perceptually
    optimized generative adversarial network (POGAN). In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 1824–1832.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane and Murala (2019a) Akshay Dudhane and Subrahmanyam Murala. 2019a. Cdnet:
    Single image de-hazing using unpaired adversarial training. In *Winter Conference
    on Applications of Computer Vision*. 1147–1155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane and Murala (2019b) Akshay Dudhane and Subrahmanyam Murala. 2019b. RYF-Net:
    Deep fusion network for single image haze removal. *IEEE Transactions on Image
    Processing* 29 (2019), 628–640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dudhane et al. (2019) Akshay Dudhane, Harshjeet Singh Aulakh, and Subrahmanyam
    Murala. 2019. Ri-gan: An end-to-end network for single image haze removal. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 2014–2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engin et al. (2018) Deniz Engin, Anil Genç, and Hazim Kemal Ekenel. 2018. Cycle-dehaze:
    Enhanced cyclegan for single image dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 825–833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2021) Zhengyun Fang, Ming Zhao, Zhengtao Yu, Meiyu Li, and Yong
    Yang. 2021. A guiding teaching and dual adversarial learning framework for a single
    image dehazing. *The Visual Computer* (2021), 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2021) Minghan Fu, Huan Liu, Yankun Yu, Jun Chen, and Keyan Wang.
    2021. DW-GAN: A Discrete Wavelet Transform GAN for NonHomogeneous Dehazing. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 203–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galdran et al. (2018) Adrian Galdran, Aitor Alvarez-Gila, Alessandro Bria, Javier
    Vazquez-Corral, and Marcelo Bertalmío. 2018. On the duality between retinex and
    image dehazing. In *Conference on Computer Vision and Pattern Recognition*. 8212–8221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gandelsman et al. (2019) Yosef Gandelsman, Assaf Shocher, and Michal Irani.
    2019. “Double-DIP”: Unsupervised Image Decomposition via Coupled Deep-Image-Priors.
    In *Conference on Computer Vision and Pattern Recognition*. 11026–11035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Li Gao, Jing Zhang, Lefei Zhang, and Dacheng Tao. 2021. Dsp:
    Dual soft-paste for unsupervised domain adaptive semantic segmentation. In *ACM
    International Conference on Multimedia*. 2825–2833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Golts et al. (2020) Alona Golts, Daniel Freedman, and Michael Elad. 2020. Unsupervised
    Single Image Dehazing Using Dark Channel Prior Loss. *IEEE Transactions on Image
    Processing* 29 (2020), 2692–2701.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. 2021. Knowledge distillation: A survey. *International Journal of Computer
    Vision* 129, 6 (2021), 1789–1819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2021) Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing
    Zhang, and Dacheng Tao. 2021. A Comprehensive Survey on Image Dehazing Based on
    Deep Learning. In *International Joint Conference on Artificial Intelligence*.
    4426–4433.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gui et al. (2022) Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping
    Ye. 2022. A review on generative adversarial networks: Algorithms, theory, and
    applications. *IEEE Transactions on Knowledge and Data Engineering* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019a) Tiantong Guo, Venkateswararao Cherukuri, and Vishal Monga.
    2019a. Dense ‘123’ color enhancement dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2131–2139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019b) Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, and Vishal
    Monga. 2019b. Dense scene information estimation network for dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2122–2130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Monga (2020) Tiantong Guo and Vishal Monga. 2020. Reinforced depth-aware
    deep learning for single image dehazing. In *International Conference on Acoustics,
    Speech and Signal Processing*. 8891–8895.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hambarde and Murala (2020) Praful Hambarde and Subrahmanyam Murala. 2020. S2dnet:
    Depth estimation from single image and sparse samples. *IEEE Transactions on Computational
    Imaging* 6 (2020), 806–817.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2010) Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Single image haze
    removal using dark channel prior. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence* 33, 12 (2010), 2341–2353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Conference on Computer Vision
    and Pattern Recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2019) Linyuan He, Junqiang Bai, and Meng Yang. 2019. Feature aggregation
    convolution network for haze removal. In *International Conference on Image Processing*.
    2806–2810.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Ming Hong, Yuan Xie, Cuihua Li, and Yanyun Qu. 2020. Distilling
    Image Dehazing With Heterogeneous Task Imitation. In *Conference on Computer Vision
    and Pattern Recognition*. 3462–3471.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. 2017. Densely connected convolutional networks. In *Conference on
    Computer Vision and Pattern Recognition*. 4700–4708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Lu-Yao Huang, Jia-Li Yin, Bo-Hao Chen, and Shao-Zhen Ye.
    2019. Towards unsupervised single image dehazing with deep learning. In *International
    Conference on Image Processing*. 2741–2745.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2021) Pengcheng Huang, Li Zhao, Runhua Jiang, Tao Wang, and Xiaoqin
    Zhang. 2021. Self-filtering image dehazing with self-supporting module. *Neurocomputing*
    432 (2021), 57–69.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Yimin Huang, Yiyang Wang, and Zhixun Su. 2018. Single image
    dehazing via a joint deep modeling. In *International Conference on Image Processing*.
    2840–2844.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huynh-Thu and Ghanbari (2008) Quan Huynh-Thu and Mohammed Ghanbari. 2008. Scope
    of validity of PSNR in image/video quality assessment. *Electronics letters* 44,
    13 (2008), 800–801.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    2017. Image-to-image translation with conditional adversarial networks. In *Conference
    on Computer Vision and Pattern Recognition*. 1125–1134.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020) Yizhou Jin, Guangshuai Gao, Qingjie Liu, and Yunhong Wang.
    2020. Unsupervised conditional disentangle network for image dehazing. In *International
    Conference on Image Processing*. 963–967.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jo and Sim (2021) Eunsung Jo and Jae-Young Sim. 2021. Multi-Scale Selective
    Residual Learning for Non-Homogeneous Dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshop*. 507–515.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2016) Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
    Perceptual losses for real-time style transfer and super-resolution. In *European
    Conference on Computer Vision*. 694–711.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ju et al. (2021) Mingye Ju, Can Ding, Wenqi Ren, Yi Yang, Dengyin Zhang, and
    Y Jay Guo. 2021. Ide: Image dehazing and exposure using an enhanced atmospheric
    scattering model. *IEEE Transactions on Image Processing* 30 (2021), 2180–2192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. In *Neural Information Processing Systems*. 18661–18673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Guisik Kim, Sung Woo Park, and Junseok Kwon. 2021. Pixel-wise
    Wasserstein Autoencoder for Highly Generative Dehazing. *IEEE Transactions on
    Image Processing* 30 (2021), 5452–5462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ledig et al. (2017) Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero,
    Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
    Zehan Wang, et al. 2017. Photo-realistic single image super-resolution using a
    generative adversarial network. In *Conference on Computer Vision and Pattern
    Recognition*. 4681–4690.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020a) Byeong-Uk Lee, Kyunghyun Lee, Jean Oh, and In So Kweon. 2020a.
    CNN-Based Simultaneous Dehazing and Depth Estimation. In *International Conference
    on Robotics and Automation*. 9722–9728.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020b) Yean-Wei Lee, Lai-Kuan Wong, and John See. 2020b. Image Dehazing
    With Contextualized Attentive U-NET. In *International Conference on Image Processing*.
    1068–1072.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi
    Zhou, and Xi Peng. 2021a. You only look yourself: Unsupervised and untrained single
    image dehazing neural network. *International Journal of Computer Vision* 129,
    5 (2021), 1754–1767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi
    Zhou, and Xi Peng. 2020b. Zero-Shot Image Dehazing. *IEEE Transactions on Image
    Processing* 29 (2020), 8457–8466.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017a) Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan
    Feng. 2017a. AOD-Net: All-in-One Dehazing Network. In *International Conference
    on Computer Vision*. 4780–4788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019c) Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun
    Zeng, and Zhangyang Wang. 2019c. Benchmarking Single-Image Dehazing and Beyond.
    *IEEE Transactions on Image Processing* 28, 1 (2019), 492–505.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu,
    and Runmin Cong. 2019a. PDR-Net: Perception-inspired single image dehazing network
    with refinement. *IEEE Transactions on Multimedia* 22, 3 (2019), 704–716.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Chongyi Li, Jichang Guo, Runmin Cong, Yanwei Pang, and Bo Wang.
    2016. Underwater image enhancement by dehazing with minimum information loss and
    histogram distribution prior. *IEEE Transactions on Image Processing* 25, 12 (2016),
    5664–5677.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021b) Hongyu Li, Jia Li, Dong Zhao, and Long Xu. 2021b. DehazeFlow:
    Multi-scale Conditional Flow Network for Single Image Dehazing. In *ACM International
    Conference on Multimedia*. 2577–2585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020e) Hui Li, Qingbo Wu, King Ngi Ngan, Hongliang Li, and Fanman
    Meng. 2020e. Region adaptive two-shot network for single image dehazing. In *International
    Conference on Multimedia and Expo*. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Lerenhan Li, Yunlong Dong, Wenqi Ren, Jinshan Pan, Changxin
    Gao, Nong Sang, and Ming-Hsuan Yang. 2020a. Semi-Supervised Image Dehazing. *IEEE
    Transactions on Image Processing* 29 (2020), 2766–2779.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021c) Pengyue Li, Jiandong Tian, Yandong Tang, Guolin Wang, and
    Chengdong Wu. 2021c. Deep Retinex Network for Single Image Dehazing. *IEEE Transactions
    on Image Processing* 30 (2021), 1100–1115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020d) Runde Li, Jinshan Pan, Min He, Zechao Li, and Jinhui Tang.
    2020d. Task-oriented network for image dehazing. *IEEE Transactions on Image Processing*
    29 (2020), 6523–6534.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020c) Yuenan Li, Yuhang Liu, Qixin Yan, and Kuangshi Zhang. 2020c.
    Deep Dehazing Network With Latent Ensembling Architecture and Adversarial Learning.
    *IEEE Transactions on Image Processing* 30 (2020), 1354–1368.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Huijuan
    Fang, Chao Dong, and Yining Quan. 2019b. LAP-Net: Level-aware progressive network
    for image dehazing. In *International Conference on Computer Vision*. 3276–3285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017b) Yu Li, Shaodi You, Michael S Brown, and Robby T Tan. 2017b.
    Haze visibility enhancement: A survey and quantitative benchmarking. *Computer
    Vision and Image Understanding* 165 (2017), 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2019) Xiao Liang, Runde Li, and Jinhui Tang. 2019. Selective Attention
    network for Image Dehazing and Deraining. In *ACM Multimedia Asia*. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020b. Trident dehazing network. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 430–431.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2014) Lixiong Liu, Bao Liu, Hua Huang, and Alan Conrad Bovik. 2014.
    No-reference image quality assessment based on spatial and spectral entropies.
    *Signal processing: Image communication* 29, 8 (2014), 856–863.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu (2019) Qian Liu. 2019. Unsupervised Single Image Dehazing Via Disentangled
    Representation. In *International Conference on Video and Image Processing*. 106–111.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan
    Luo, and Lei Zhang. 2018. Learning aggregated transmission propagation networks
    for haze removal and beyond. *IEEE Transactions on Neural Networks and Learning
    Systems* 30, 10 (2018), 2973–2986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Wei Liu, Xianxu Hou, Jiang Duan, and Guoping Qiu. 2020a.
    End-to-End Single Image Fog Removal Using Enhanced Cycle Consistent Adversarial
    Networks. *IEEE Transactions on Image Processing* 29 (2020), 7819–7833.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. 2019a.
    GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing. In *International
    Conference on Computer Vision*. 7313–7322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Yang Liu, Jinshan Pan, Jimmy Ren, and Zhixun Su. 2019b. Learning
    deep priors for image dehazing. In *International Conference on Computer Vision*.
    2492–2500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Ye Liu, Lei Zhu, Shunda Pei, Huazhu Fu, Jing Qin, Qing Zhang,
    Liang Wan, and Wei Feng. 2021. From Synthetic to Real: Image Dehazing Collaborating
    with Unlabeled Real Data. In *ACM International Conference on Multimedia*. 50–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCartney (1976) Earl J McCartney. 1976. Optics of the atmosphere: scattering
    by molecules and particles. *Physics Bulletin* (1976), 1–421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehra et al. (2021) Aryan Mehra, Pratik Narang, and Murari Mandal. 2021. TheiaNet:
    Towards fast and inexpensive CNN design choices for image dehazing. *Journal of
    Visual Communication and Image Representation* 77 (2021), 103137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. (2020) Aditya Mehta, Harsh Sinha, Pratik Narang, and Murari Mandal.
    2020. Hidegan: A hyperspectral-guided image dehazing gan. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 212–213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metwaly et al. (2020) Kareem Metwaly, Xuelu Li, Tiantong Guo, and Vishal Monga.
    2020. Nonlocal channel attention for nonhomogeneous image dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 452–453.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min et al. (2019) Xiongkuo Min, Guangtao Zhai, Ke Gu, Yucheng Zhu, Jiantao Zhou,
    Guodong Guo, Xiaokang Yang, Xinping Guan, and Wenjun Zhang. 2019. Quality evaluation
    of image dehazing methods using synthetic hazy images. *IEEE Transactions on Multimedia*
    21, 9 (2019), 2319–2333.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo et al. (2022) Yaozong Mo, Chaofeng Li, Yuhui Zheng, and Xiaojun Wu. 2022.
    DCA-CycleGAN: Unsupervised Single Image Dehazing Using Dark Channel Attention
    Optimized CycleGAN. *Journal of Visual Communication and Image Representation*
    82 (2022), 103431.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mondal et al. (2018) Ranjan Mondal, Sanchayan Santra, and Bhabatosh Chanda.
    2018. Image dehazing by joint estimation of transmittance and airlight using bi-directional
    consistency loss minimized FCN. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 920–928.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morales et al. (2019) Peter Morales, Tzofi Klinghoffer, and Seung Jae Lee. 2019.
    Feature forwarding for efficient single image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2078–2085.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narasimhan and Nayar (2003) Srinivasa G Narasimhan and Shree K Nayar. 2003.
    Contrast restoration of weather degraded images. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 25, 6 (2003), 713–724.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayar and Narasimhan (1999) Shree K Nayar and Srinivasa G Narasimhan. 1999.
    Vision in bad weather. In *International Conference on Computer Vision*. 820–827.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. (2020) Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, and Xuelong
    Li. 2020. BidNet: Binocular Image Dehazing Without Explicit Disparity Estimation.
    In *Conference on Computer Vision and Pattern Recognition*. 5930–5939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2018) Yanwei Pang, Jin Xie, and Xuelong Li. 2018. Visual haze removal
    by a unified generative adversarial network. *IEEE Transactions on Circuits and
    Systems for Video Technology* 29, 11 (2018), 3211–3221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parihar et al. (2020) Anil Singh Parihar, Yash Kumar Gupta, Yash Singodia, Vibhu
    Singh, and Kavinder Singh. 2020. A comparative study of image dehazing algorithms.
    In *International Conference on Communication and Electronics Systems*. 766–771.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2020) Jaihyun Park, David K Han, and Hanseok Ko. 2020. Fusion of
    heterogeneous adversarial networks for single image dehazing. *IEEE Transactions
    on Image Processing* 29 (2020), 4721–4732.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2018) Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu, and Song Wang.
    2018. Does Haze Removal Help CNN-based Image Classification?. In *European Conference
    on Computer Vision*. 697–712.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2020) Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu
    Jia. 2020. FFA-Net: Feature Fusion Attention Network for Single Image Dehazing.
    In *AAAI Conference on Artificial Intelligence*. 11908–11915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qu et al. (2019) Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced
    Pix2pix Dehazing Network. In *Conference on Computer Vision and Pattern Recognition*.
    8152–8160.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2016) Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and
    Ming-Hsuan Yang. 2016. Single image dehazing via multi-scale convolutional neural
    networks. In *European Conference on Computer Vision*. 154–169.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018a) Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao,
    Wei Liu, and Ming-Hsuan Yang. 2018a. Gated Fusion Network for Single Image Dehazing.
    In *Conference on Computer Vision and Pattern Recognition*. 3253–3261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2020) Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, and Ming-Hsuan
    Yang. 2020. Single image dehazing via multi-scale convolutional neural networks
    with holistic edges. *International Journal of Computer Vision* 128, 1 (2020),
    240–259.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018b) Wenqi Ren, Jingang Zhang, Xiangyu Xu, Lin Ma, Xiaochun Cao,
    Gaofeng Meng, and Wei Liu. 2018b. Deep video dehazing with semantic segmentation.
    *IEEE Transactions on Image Processing* 28, 4 (2018), 1895–1908.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical Image Computing and Computer-assisted Intervention*. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rudin et al. (1992) Leonid I Rudin, Stanley Osher, and Emad Fatemi. 1992. Nonlinear
    total variation based noise removal algorithms. *Physica D: nonlinear phenomena*
    60, 1-4 (1992), 259–268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saad et al. (2012) Michele A Saad, Alan C Bovik, and Christophe Charrier. 2012.
    Blind image quality assessment: A natural scene statistics approach in the DCT
    domain. *IEEE transactions on Image Processing* 21, 8 (2012), 3339–3352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sakaridis et al. (2018) Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc
    Van Gool. 2018. Model adaptation with synthetic and real data for semantic dense
    foggy scene understanding. In *European Conference on Computer Vision*. 687–704.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2020) Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and Nong
    Sang. 2020. Domain Adaptation for Image Dehazing. In *Conference on Computer Vision
    and Pattern Recognition*. 2805–2814.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2005) Gaurav Sharma, Wencheng Wu, and Edul N Dalal. 2005. The
    CIEDE2000 color-difference formula: Implementation notes, supplementary test data,
    and mathematical observations. *Color Research & Application: Endorsed by Inter-Society
    Color Council, etc* 30, 1 (2005), 21–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2020) Prasen Sharma, Priyankar Jain, and Arijit Sur. 2020. Scale-aware
    conditional generative adversarial network for image dehazing. In *Winter Conference
    on Applications of Computer Vision*. 2355–2365.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2022) Jiechao Sheng, Guoqiang Lv, Gang Du, Zi Wang, and Qibin
    Feng. 2022. Multi-scale residual attention network for single image dehazing.
    *Digital Signal Processing* 121 (2022), 103327.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shin et al. (2022) Joongchol Shin, Hasil Park, and Joonki Paik. 2022. Region-Based
    Dehazing via Dual-Supervised Triple-Convolutional Network. *IEEE Transactions
    on Multimedia* 24 (2022), 245–260.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shyam et al. (2021) Pranjay Shyam, Kuk-Jin Yoon, and Kyung-Soo Kim. 2021. Towards
    Domain Invariant Single Image Dehazing. In *AAAI Conference on Artificial Intelligence*.
    9657–9665.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. 2012. Indoor segmentation and support inference from rgbd images. In *European
    Conference on Computer Vision*. 746–760.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sim et al. (2018) Hyeonjun Sim, Sehwan Ki, Jae-Seok Choi, Soomin Seo, Saehun
    Kim, and Munchurl Kim. 2018. High-resolution image dehazing with respect to training
    losses and receptive field sizes. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 912–919.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. 2015. Very Deep
    Convolutional Networks for Large-Scale Image Recognition. In *International Conference
    on Learning Representations*. 1–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2020) Ayush Singh, Ajay Bhave, and Dilip K Prasad. 2020. Single
    image dehazing for a variety of haze scenarios using back projected pyramid network.
    In *European Conference on Computer Vision*. 166–181.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Kumar (2019) Dilbag Singh and Vijay Kumar. 2019. A comprehensive review
    of computational dehazing techniques. *Archives of Computational Methods in Engineering*
    26, 5 (2019), 1395–1413.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2021) Lexuan Sun, Xueliang Liu, Zhenzhen Hu, and Richang Hong.
    2021. WFN-PSC: weighted-fusion network with poly-scale convolution for image dehazing.
    In *ACM International Conference on Multimedia in Asia*. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Conference on Computer Vision and Pattern
    Recognition*. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Guiying Tang, Li Zhao, Runhua Jiang, and Xiaoqin Zhang. 2019.
    Single Image Dehazing via Lightweight Multi-scale Networks. In *International
    Conference on Big Data*. 5062–5069.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Anna Wang, Wenhui Wang, Jinglu Liu, and Nanhui Gu. 2018a.
    AIPNet: Image-to-image single image dehazing with atmospheric illumination prior.
    *IEEE Transactions on Image Processing* 28, 1 (2018), 381–393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Cong Wang, Yuexian Zou, and Zehan Chen. 2020. ABC-NET: Avoiding
    Blocking Effect & Color Shift Network for Single Image Dehazing Via Restraining
    Transmission Bias. In *International Conference on Image Processing*. 1053–1057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021c) Juan Wang, Chang Ding, Minghu Wu, Yuanyuan Liu, and Guanhai
    Chen. 2021c. Lightweight multiple scale-patch dehazing network for real-world
    hazy image. *KSII Transactions on Internet and Information Systems* 15, 12 (2021),
    4420–4438.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021d) Jixiao Wang, Chaofeng Li, and Shoukun Xu. 2021d. An ensemble
    multi-scale residual attention network (EMRA-net) for image Dehazing. *Multimedia
    Tools and Applications* (2021), 29299–29319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. 2019.
    Distilling Object Detectors With Fine-Grained Feature Imitation. In *Conference
    on Computer Vision and Pattern Recognition*. 4928–4937.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun
    Zha, Yonggang Wen, and Dacheng Tao. 2021a. Exploring Sequence Feature Alignment
    for Domain Adaptive Detection Transformers. In *ACM International Conference on
    Multimedia*. 1730–1738.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wen Wang, Jing Zhang, Wei Zhai, Yang Cao, and Dacheng Tao.
    2022. Robust Object Detection via Adversarial Novel Style Exploration. *IEEE Transactions
    on Image Processing* 31 (2022), 1949–1962.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
    Chao Dong, Yu Qiao, and Chen Change Loy. 2018b. Esrgan: Enhanced super-resolution
    generative adversarial networks. In *European Conference on Computer Vision*.
    63–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Yang Wang, Yang Cao, Jing Zhang, Feng Wu, and Zheng-Jun
    Zha. 2021b. Leveraging Deep Statistics for Underwater Image Enhancement. *ACM
    Transactions on Multimedia Computing, Communications, and Applications* 17, 3s
    (2021), 1–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Yang Wang, Jing Zhang, Yang Cao, and Zengfu Wang. 2017. A
    deep CNN method for underwater image enhancement. In *IEEE International Conference
    on Image Processing*. 1382–1386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2013) Yinting Wang, Shaojie Zhuo, Dapeng Tao, Jiajun Bu, and Na
    Li. 2013. Automatic local exposure correction using bright channel prior for under-exposed
    images. *Signal processing* 93, 11 (2013), 3227–3238.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2004) Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli,
    et al. 2004. Image quality assessment: from error visibility to structural similarity.
    *IEEE Transactions on Image Processing* 13, 4 (2004), 600–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2003) Zhou Wang, Eero P Simoncelli, and Alan C Bovik. 2003. Multiscale
    structural similarity for image quality assessment. In *Asilomar Conference on
    Signals, Systems & Computers*. 1398–1402.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2020) Haoran Wei, Qingbo Wu, Hui Li, King Ngi Ngan, Hongliang Li,
    and Fanman Meng. 2020. Single Image Dehazing via Artificial Multiple Shots and
    Multidimensional Context. In *International Conference on Image Processing*. 1023–1027.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2021) Pan Wei, Xin Wang, Lei Wang, and Ji Xiang. 2021. SIDGAN:
    Single Image Dehazing without Paired Supervision. In *International Conference
    on Pattern Recognition*. 2958–2965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Haiyan Wu, Jing Liu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020. Knowledge transfer dehazing network for nonhomogeneous dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 478–479.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao,
    Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. 2021. Contrastive Learning for Compact
    Single Image Dehazing. In *Conference on Computer Vision and Pattern Recognition*.
    10551–10560.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2020) Jinsheng Xiao, Mengyao Shen, Junfeng Lei, Jinglong Zhou,
    Reinhard Klette, and HaiGang Sui. 2020. Single image dehazing based on learning
    of haze layers. *Neurocomputing* 389 (2020), 108–122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Liangru Xie, Hao Wang, Zhuowei Wang, and Lianglun Cheng.
    2020. DHD-Net: A Novel Deep-Learning-based Dehazing Network. In *International
    Joint Conference on Neural Networks*. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2015) Yong Xu, Jie Wen, Lunke Fei, and Zheng Zhang. 2015. Review
    of video and image defogging algorithms and related studies on image restoration
    and enhancement. *IEEE Access* 4 (2015), 165–188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2020) Lan Yan, Wenbo Zheng, Chao Gou, and Fei-Yue Wang. 2020. Feature
    Aggregation Attention Network for Single Image Dehazing. In *International Conference
    on Image Processing*. 923–927.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Aiping Yang, Haixin Wang, Zhong Ji, Yanwei Pang, and Ling
    Shao. 2019. Dual-Path in Dual-Path Network for Single Image Dehazing. In *International
    Joint Conference on Artificial Intelligence*. 4627–4634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Zhang (2022) Fei Yang and Qian Zhang. 2022. Depth aware image dehazing.
    *The Visual Computer* 38, 5 (2022), 1579–1587.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Fu (2019) Hao-Hsiang Yang and Yanwei Fu. 2019. Wavelet u-net and the
    chromatic adaptation transform for single image dehazing. In *International Conference
    on Image Processing*. 2736–2740.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Hao-Hsiang Yang, Chao-Han Huck Yang, and Yi-Chang James
    Tsai. 2020. Y-net: Multi-scale feature aggregation network with wavelet structure
    similarity loss function for single image dehazing. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2628–2632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeh et al. (2019) Chia-Hung Yeh, Chih-Hsiang Huang, and Li-Wei Kang. 2019. Multi-scale
    deep residual learning-based single image haze removal via image decomposition.
    *IEEE Transactions on Image Processing* 29 (2019), 3153–3167.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2019) Jia-Li Yin, Yi-Chi Huang, Bo-Hao Chen, and Shao-Zhen Ye. 2019.
    Color transferred convolutional neural networks for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 11 (2019), 3957–3967.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2020) Shibai Yin, Yibin Wang, and Yee-Hong Yang. 2020. A novel image-dehazing
    network with a parallel attention block. *Pattern Recognition* 102 (2020), 107255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2021) Shibai Yin, Xiaolong Yang, Yibin Wang, and Yee-Hong Yang.
    2021. Visual Attention Dehazing Network with Multi-level Features Refinement and
    Fusion. *Pattern Recognition* 118 (2021), 108021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2020) Mingzhao Yu, Venkateswararao Cherukuri, Tiantong Guo, and Vishal
    Monga. 2020. Ensemble dehazing networks for non-homogeneous haze. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 450–451.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, and
    Keyan Wang. 2021. A Two-branch Neural Network for Non-homogeneous Dehazing via
    Ensemble Learning. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    193–202.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Patel (2018) He Zhang and Vishal M Patel. 2018. Densely connected
    pyramid dehazing network. In *Conference on Computer Vision and Pattern Recognition*.
    3194–3203.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018b) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2018b.
    Multi-scale single image dehazing using perceptual pyramid deep network. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 902–911.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2019a.
    Joint transmission map estimation and dehazing using deep networks. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 7 (2019), 1975–1986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017a) Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, and Chang Wen Chen.
    2017a. Fast haze removal for nighttime image using maximum reflectance prior.
    In *Conference on Computer Vision and Pattern Recognition*. 7418–7426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2014) Jing Zhang, Yang Cao, and Zengfu Wang. 2014. Nighttime haze
    removal based on a new imaging model. In *International Conference on Image Processing*.
    4557–4561.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Jing Zhang, Yang Cao, Zheng-Jun Zha, and Dacheng Tao. 2020a.
    Nighttime dehazing with a synthetic benchmark. In *ACM International Conference
    on Multimedia*. 2355–2363.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Jingang Zhang, Wenqi Ren, Shengdong Zhang, He Zhang, Yunfeng
    Nie, Zhe Xue, and Xiaochun Cao. 2022b. Hierarchical Density-Aware Dehazing Network.
    *IEEE Transactions on Cybernetics* 52, 10 (2022), 11187–11199.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Tao (2020) Jing Zhang and Dacheng Tao. 2020. FAMED-Net: A Fast and
    Accurate Multi-Scale End-to-End Dehazing Network. *IEEE Transactions on Image
    Processing* 29 (2020), 72–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Li (2021) Kuangshi Zhang and Yuenan Li. 2021. Single image dehazing
    via semi-supervised domain translation and architecture search. *IEEE Signal Processing
    Letters* 28 (2021), 2127–2131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Qiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao. 2019b.
    Category anchor-guided unsupervised domain adaptation for semantic segmentation.
    In *Neural Information Processing Systems*. 433–443.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and He (2020) Shengdong Zhang and Fazhi He. 2020. DRCDN: learning deep
    residual convolutional dehazing networks. *The Visual Computer* 36 (2020), 1797–1808.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020b. NLDN:
    Non-local dehazing network for dense haze removal. *Neurocomputing* 410 (2020),
    363–373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020c. Photo-realistic
    dehazing via contextual generative adversarial networks. *Machine Vision and Applications*
    31 (2020), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020d) Shengdong Zhang, Fazhi He, Wenqi Ren, and Jian Yao. 2020d.
    Joint learning of image detail and transmission map for single image dehazing.
    *The Visual Computer* 36, 2 (2020), 305–316.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Shengdong Zhang, Wenqi Ren, Xin Tan, Zhi-Jie Wang, Yong
    Liu, Jingang Zhang, Xiaoqin Zhang, and Xiaochun Cao. 2022a. Semantic-aware dehazing
    network with adaptive feature fusion. *IEEE Transactions on Cybernetics* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) Shengdong Zhang, Wenqi Ren, and Jian Yao. 2018a. Feed-Net:
    Fully End-to-End Dehazing. In *International Conference on Multimedia and Expo*.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020f) Shengdong Zhang, Yue Wu, Yuanjie Zhao, Zuomin Cheng, and
    Wenqi Ren. 2020f. Color-constrained dehazing model. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 870–871.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Xiaoqin Zhang, Runhua Jiang, Tao Wang, and Wenhan Luo.
    2021a. Single Image Dehazing via Dual-Path Recurrent Network. *IEEE Transactions
    on Image Processing* 30 (2021), 5211–5222.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022c) Xiaoqin Zhang, Jinxin Wang, Tao Wang, and Runhua Jiang.
    2022c. Hierarchical feature fusion with mixed convolution attention for single
    image dehazing. *IEEE Transactions on Circuits and Systems for Video Technology*
    32, 2 (2022), 510–522.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Xiaoqin Zhang, Tao Wang, Wenhan Luo, and Pengcheng Huang.
    2021b. Multi-level fusion and attention-guided CNN for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 31, 11 (2021), 4162–4173.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020e) Xiaoqin Zhang, Tao Wang, Jinxin Wang, Guiying Tang, and
    Li Zhao. 2020e. Pyramid Channel-based Feature Attention Network for image dehazing.
    *Computer Vision and Image Understanding* 197-198 (2020), 103003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017b) Yanfu Zhang, Li Ding, and Gaurav Sharma. 2017b. HazeRD:
    An outdoor scene dataset and benchmark for single image dehazing. In *International
    Conference on Image Processing*. 3205–3209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020g) Zhengxi Zhang, Liang Zhao, Yunan Liu, Shanshan Zhang, and
    Jian Yang. 2020g. Unified Density-Aware Image Dehazing and Object Detection in
    Real-World Hazy Scenes. In *Asian Conference on Computer Vision*. 119–135.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021a) Dong Zhao, Long Xu, Lin Ma, Jia Li, and Yihua Yan. 2021a.
    Pyramid Global Context Network for Image Dehazing. *IEEE Transactions on Circuits
    and Systems for Video Technology* 31, 8 (2021), 3037–3050.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Shiyu Zhao, Lin Zhang, Shuaiyi Huang, Ying Shen, and Shengjie
    Zhao. 2020. Dehazing Evaluation: Real-World Benchmark Datasets, Criteria, and
    Baselines. *IEEE Transactions on Image Processing* 29 (2020), 6947–6962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021b) Shiyu Zhao, Lin Zhang, Ying Shen, and Yicong Zhou. 2021b.
    RefineDNet: A Weakly Supervised Refinement Framework for Single Image Dehazing.
    *IEEE Transactions on Image Processing* 30 (2021), 3391–3404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Zhuoran Zheng, Wenqi Ren, Xiaochun Cao, Xiaobin Hu, Tao
    Wang, Fenglong Song, and Xiuyi Jia. 2021. Ultra-High-Definition Image Dehazing
    via Multi-Guided Bilateral Learning. In *Conference on Computer Vision and Pattern
    Recognition*. 16180–16189.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021) Hongyuan Zhu, Yi Cheng, Xi Peng, Joey Tianyi Zhou, Zhao Kang,
    Shijian Lu, Zhiwen Fang, Liyuan Li, and Joo-Hwee Lim. 2021. Single-image dehazing
    via compositional adversarial network. *IEEE Transactions on Cybernetics* 51,
    2 (2021), 829–838.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2018) Hongyuan Zhu, Xi Peng, Vijay Chandrasekhar, Liyuan Li, and
    Joo-Hwee Lim. 2018. DehazeGAN: When Image Dehazing Meets Differential Programming.
    In *International Joint Conference on Artificial Intelligence*. 1234–1240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *International Conference on Computer Vision*. 2223–2232.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
