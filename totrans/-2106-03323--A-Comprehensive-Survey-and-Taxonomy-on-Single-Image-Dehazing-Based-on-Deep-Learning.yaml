- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:54:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:54:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.03323] A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.03323] 《基于深度学习的单幅图像去雾的综合调查与分类》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.03323](https://ar5iv.labs.arxiv.org/html/2106.03323)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.03323](https://ar5iv.labs.arxiv.org/html/2106.03323)
- en: A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《基于深度学习的单幅图像去雾的综合调查与分类》
- en: Jie Gui [guijie@seu.edu.cn](mailto:guijie@seu.edu.cn) School of Cyber Science
    and Engineering, Southeast University and Purple Mountain LaboratoriesNanjingJiangsuChina210000
    ,  Xiaofeng Cong School of Cyber Science and Engineering, Southeast UniversityNanjingChina
    [cxf_svip@163.com](mailto:cxf_svip@163.com) ,  Yuan Cao Ocean University of China
    QingdaoChina [cy8661@ouc.edu.cn](mailto:cy8661@ouc.edu.cn) ,  Wenqi Ren Institute
    of Information Engineering, Chinese Academy of SciencesBeijingChina [renwenqi@iie.ac.cn](mailto:renwenqi@iie.ac.cn)
    ,  Jun Zhang Anhui UniversityHefeiChina [wwwzhangjun@163.com](mailto:wwwzhangjun@163.com)
    ,  Jing Zhang The University of SydneySydneyAustralia [jing.zhang1@sydney.edu.au](mailto:jing.zhang1@sydney.edu.au)
    ,  Jiuxin Cao School of Cyber Science and Engineering, Southeast UniversityNanjingChina
     and  Dacheng Tao JD Explore Academy, China and The University of SydneySydneyAustralia
    [dacheng.tao@gmail.com](mailto:dacheng.tao@gmail.com)(2022)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jie Gui [guijie@seu.edu.cn](mailto:guijie@seu.edu.cn) 东南大学网络科学与工程学院及紫金山实验室 南京
    江苏 中国 210000，Xiaofeng Cong 东南大学网络科学与工程学院 南京 中国 [cxf_svip@163.com](mailto:cxf_svip@163.com)，Yuan
    Cao 中国海洋大学 青岛 中国 [cy8661@ouc.edu.cn](mailto:cy8661@ouc.edu.cn)，Wenqi Ren 中国科学院信息工程研究所
    北京 中国 [renwenqi@iie.ac.cn](mailto:renwenqi@iie.ac.cn)，Jun Zhang 安徽大学 合肥 中国 [wwwzhangjun@163.com](mailto:wwwzhangjun@163.com)，Jing
    Zhang 悉尼大学 悉尼 澳大利亚 [jing.zhang1@sydney.edu.au](mailto:jing.zhang1@sydney.edu.au)，Jiuxin
    Cao 东南大学网络科学与工程学院 南京 中国 以及 Dacheng Tao 京东探索学院，中国和悉尼大学 悉尼 澳大利亚 [dacheng.tao@gmail.com](mailto:dacheng.tao@gmail.com)（2022）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: With the development of convolutional neural networks, hundreds of deep learning
    based dehazing methods have been proposed. In this paper, we provide a comprehensive
    survey on supervised, semi-supervised, and unsupervised single image dehazing.
    We first discuss the physical model, datasets, network modules, loss functions,
    and evaluation metrics that are commonly used. Then, the main contributions of
    various dehazing algorithms are categorized and summarized. Further, quantitative
    and qualitative experiments of various baseline methods are carried out. Finally,
    the unsolved issues and challenges that can inspire the future research are pointed
    out. A collection of useful dehazing materials is available at [https://github.com/Xiaofeng-life/AwesomeDehazing](https://github.com/Xiaofeng-life/AwesomeDehazing).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卷积神经网络的发展，已经提出了数百种基于深度学习的去雾方法。本文提供了关于监督学习、半监督学习和无监督学习单幅图像去雾的综合调查。我们首先讨论了常用的物理模型、数据集、网络模块、损失函数和评估指标。然后，对各种去雾算法的主要贡献进行了分类和总结。此外，还进行了各种基线方法的定量和定性实验。最后，指出了尚未解决的问题和挑战，这些问题和挑战可以激发未来的研究。更多有用的去雾材料可以在
    [https://github.com/Xiaofeng-life/AwesomeDehazing](https://github.com/Xiaofeng-life/AwesomeDehazing)
    获取。
- en: 'image dehazing, supervised, semi-supervised, unsupervised, atmospheric scattering
    model.^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†doi: XXXXXXX.XXXXXXX^†^†conference:
    Make sure to enter the correct conference title from your rights confirmation
    emai; June 03–05, 2018; Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Vision for robotics^†^†ccs: Computing methodologies Computational
    photography^†^†ccs: Computing methodologies Computer vision'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去雾，监督学习，半监督学习，无监督学习，大气散射模型。^†^†版权：acm版权^†^†期刊年份：2022^†^†doi：XXXXXXX.XXXXXXX^†^†会议：请确保输入您权利确认邮件中的正确会议标题；2018年6月03–05日；纽约伍德斯托克^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法
    机器人视觉^†^†ccs：计算方法 计算摄影^†^†ccs：计算方法 计算机视觉
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Due to the absorption by floating particles contained in the hazy environment,
    the quality of the image captured by the camera will be reduced. The phenomenon
    of image quality degradation in hazy weather has a negative impact on photography
    work. The contrast of the image will decrease and the color will shift. Meantime,
    the texture and edge of objects in the scene will become blurred. As shown in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), there is an obvious difference
    between the pixel histograms of hazy and haze-free images. For computer vision
    tasks such as object detection and image segmentation, low-quality inputs can
    degrade the performance of the models trained on haze-free images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雾霾环境中漂浮颗粒的吸收，摄像机拍摄的图像质量会降低。雾霾天气中的图像质量下降对摄影工作产生负面影响。图像的对比度会降低，颜色会偏移。同时，场景中物体的纹理和边缘会变得模糊。如图[1](#S1.F1
    "图 1 ‣ 1\. 介绍 ‣ 基于深度学习的单幅图像去雾的全面调查与分类")所示，雾霾图像和无雾图像的像素直方图存在明显差异。对于目标检测和图像分割等计算机视觉任务，低质量的输入会降低在无雾图像上训练的模型的性能。
- en: Therefore, many researchers try to recover high-quality clear scenes from hazy
    images. Before deep learning was widely used in computer vision tasks, image dehazing
    algorithms had mainly relied on various prior assumptions (He et al., [2010](#bib.bib52))
    and atmospheric scattering model (ASM) (McCartney, [1976](#bib.bib94)). The processing
    flow of these statistical rule based methods has good interpretability. However,
    they may exhibit shortcomings when facing complex real world scenarios. For example,
    the well-known dark channel prior (He et al., [2010](#bib.bib52)) (DCP, best paper
    of CVPR 2009) cannot handle regions containing sky well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多研究人员试图从雾霾图像中恢复高质量的清晰场景。在深度学习广泛应用于计算机视觉任务之前，图像去雾算法主要依赖于各种先验假设（He等，[2010](#bib.bib52)）和大气散射模型（ASM）（McCartney，[1976](#bib.bib94)）。这些基于统计规则的方法的处理流程具有良好的可解释性。然而，它们在面对复杂的真实世界场景时可能会显示出缺陷。例如，著名的暗通道先验（He等，[2010](#bib.bib52)）（DCP，CVPR
    2009最佳论文）无法很好地处理包含天空的区域。
- en: Inspired by deep learning, (Ren et al., [2016](#bib.bib111); Cai et al., [2016](#bib.bib13);
    Ren et al., [2020](#bib.bib113)) combine ASM and convolutional neural network
    (CNN) to estimate the parameters of the ASM. Quantitative and qualitative experimental
    results show that deep learning can help the prediction of these parameters in
    a supervised way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 受到深度学习的启发，（Ren等，[2016](#bib.bib111)；Cai等，[2016](#bib.bib13)；Ren等，[2020](#bib.bib113)）将ASM和卷积神经网络（CNN）结合起来，以估计ASM的参数。定量和定性实验结果表明，深度学习可以在监督方式下帮助预测这些参数。
- en: Following this, (Qin et al., [2020](#bib.bib109); Liu et al., [2019a](#bib.bib91);
    Liang et al., [2019](#bib.bib85); Zhang et al., [2022c](#bib.bib182); Zheng et al.,
    [2021](#bib.bib190)) have demonstrated that end-to-end supervised dehazing networks
    can be implemented independently of the ASM. Thanks to the powerful feature extraction
    capability of CNN, these non-ASM-based dehazing algorithms can achieve comparable
    accuracy as ASM-based algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 继而，（秦等，[2020](#bib.bib109)；刘等，[2019a](#bib.bib91)；梁等，[2019](#bib.bib85)；张等，[2022c](#bib.bib182)；郑等，[2021](#bib.bib190)）已经证明，端到端的监督去雾网络可以独立于ASM进行实现。得益于CNN强大的特征提取能力，这些非ASM基础的去雾算法可以达到与ASM基础算法相当的准确性。
- en: ASM-based and non-ASM-based supervised algorithms have shown impressive performance.
    However, they often require synthetic paired images that are inconsistent with
    real world hazy images. Therefore, recent research focus on methods that are more
    suitable to the real world dehazing task. (Cong et al., [2020](#bib.bib24); Golts
    et al., [2020](#bib.bib44); Li et al., [2020b](#bib.bib72)) explore unsupervised
    algorithms that do not require synthetic data while other studies (Li et al.,
    [2020a](#bib.bib79); An et al., [2022](#bib.bib2); Chen et al., [2021b](#bib.bib21);
    Zhang and Li, [2021](#bib.bib172)) propose semi-supervised algorithms that exploit
    both synthetic paired data and real world unpaired data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ASM的和非ASM基础的监督算法都表现出了令人印象深刻的性能。然而，它们通常需要与真实世界雾霾图像不一致的合成配对图像。因此，最近的研究重点是更加适合真实世界去雾任务的方法。（Cong等，[2020](#bib.bib24)；Golts等，[2020](#bib.bib44)；Li等，[2020b](#bib.bib72)）探索了不需要合成数据的无监督算法，而其他研究（Li等，[2020a](#bib.bib79)；An等，[2022](#bib.bib2)；Chen等，[2021b](#bib.bib21)；张和李，[2021](#bib.bib172)）提出了利用合成配对数据和真实世界无配对数据的半监督算法。
- en: '![Refer to caption](img/50395ece082c77eeeddb8770c5ca443b.png)![Refer to caption](img/3711e6faf7811490083dce87156b8618.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/50395ece082c77eeeddb8770c5ca443b.png)![参见图例](img/3711e6faf7811490083dce87156b8618.png)'
- en: (a) A clear image                                (c) A hazy image                               
    ![Refer to caption](img/ce29973e98b8d683a3a16a2a78041334.png) ![Refer to caption](img/cd6060c43bbf613b833cc7a28e19ae1b.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 清晰图像                                (c) 有雾图像                               
    ![参见图例](img/ce29973e98b8d683a3a16a2a78041334.png) ![参见图例](img/cd6060c43bbf613b833cc7a28e19ae1b.png)
- en: (b) Histogram for (a)                   (d) Histogram for (c)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (b) (a) 的直方图                   (d) (c) 的直方图
- en: Figure 1\. Pixel histograms of clear (a) and hazy (c) images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 清晰图像 (a) 和有雾图像 (c) 的像素直方图。
- en: With the rapid development in this area, hundreds of dehazing methods have been
    proposed. To inspire and guide the future research, a comprehensive survey is
    urgently needed. Some papers have attempted to partially review the recent development
    of the dehazing research. For example,  (Singh and Kumar, [2019](#bib.bib129);
    Li et al., [2017b](#bib.bib84); Xu et al., [2015](#bib.bib152)) gives a summary
    of the non-deep learning dehazing methods, including depth estimation, wavelet,
    enhancement, filtering, but lacks of research on recent CNN-based methods. Parihar
    et al. (Parihar et al., [2020](#bib.bib106)) provides a survey about supervised
    dehazing models, but it does not pay enough attention to the latest explorations
    of semi-supervised and unsupervised methods. Banerjee et al. (Banerjee and Chaudhuri,
    [2021](#bib.bib11)) introduce and group the existing nighttime image dehazing
    methods, however, the methods of daytime are rarely analyzed. Gui et al. (Gui
    et al., [2021](#bib.bib46)) briefly classify and analyze supervised and unsupervised
    algorithms, but does not summarize the various recently proposed semi-supervised
    methods. Unlike existing reviews, we give a comprehensive survey on the supervised,
    semi-supervised and unsupervised daytime dehazing models based on deep learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一领域的迅速发展，已经提出了数百种去雾方法。为了激发和指导未来的研究，迫切需要一项全面的调查。虽然有一些论文尝试部分回顾去雾研究的最新进展，但大多数研究仍有所欠缺。例如，(Singh
    and Kumar, [2019](#bib.bib129); Li et al., [2017b](#bib.bib84); Xu et al., [2015](#bib.bib152))
    总结了非深度学习的去雾方法，包括深度估计、小波、增强、滤波，但缺乏对近期基于 CNN 的方法的研究。Parihar et al. (Parihar et al.,
    [2020](#bib.bib106)) 提供了关于监督去雾模型的调查，但没有充分关注半监督和无监督方法的最新探索。Banerjee et al. (Banerjee
    and Chaudhuri, [2021](#bib.bib11)) 介绍并归类了现有的夜间图像去雾方法，但白天的去雾方法很少被分析。Gui et al.
    (Gui et al., [2021](#bib.bib46)) 简要分类和分析了监督与无监督算法，但未总结最近提出的各种半监督方法。与现有的综述不同，我们对基于深度学习的监督、半监督和无监督的白天去雾模型进行了全面的调查。
- en: '![Refer to caption](img/ce35310209484fc8e93d3ea80527fe22.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ce35310209484fc8e93d3ea80527fe22.png)'
- en: Figure 2\. Atmospheric Scattering Model (ASM), same as (Cai et al., [2016](#bib.bib13)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 大气散射模型（ASM），与 (Cai et al., [2016](#bib.bib13)) 相同。
- en: Table 1\. A taxonomy of dehazing methods. Red number index represents ASM-based
    methods, and black number index represents non-ASM-based methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 去雾方法的分类。红色编号索引代表基于 ASM 的方法，黑色编号索引代表非 ASM 基础的方法。
- en: '| Category | Key Idea | Methods |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 关键思想 | 方法 |'
- en: '| Supervised | Learning of $t(x)$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 监督学习 | $t(x)$ 的学习 |'
- en: '&#124; DehazeNet (Cai et al., [2016](#bib.bib13)), ABC-Net (Wang et al., [2020](#bib.bib134)),
    MSCNN (Ren et al., [2016](#bib.bib111)), &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DehazeNet (Cai et al., [2016](#bib.bib13)), ABC-Net (Wang et al., [2020](#bib.bib134)),
    MSCNN (Ren et al., [2016](#bib.bib111)), &#124;'
- en: '&#124; MSCNN-HE (Ren et al., [2020](#bib.bib113)), SID-JMP (Huang et al., [2018](#bib.bib59)),
    LATPN (Liu et al., [2018](#bib.bib89)) &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSCNN-HE (Ren et al., [2020](#bib.bib113)), SID-JMP (Huang et al., [2018](#bib.bib59)),
    LATPN (Liu et al., [2018](#bib.bib89)) &#124;'
- en: '|'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Joint learning of $t(x)$ and $A$ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $t(x)$ 和 $A$ 的联合学习 |'
- en: '&#124; DCPDN (Zhang and Patel, [2018](#bib.bib164)), DSIEN (Guo et al., [2019b](#bib.bib49)),
    LDPID (Liu et al., [2019b](#bib.bib92)), &#124;'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DCPDN (Zhang and Patel, [2018](#bib.bib164)), DSIEN (Guo et al., [2019b](#bib.bib49)),
    LDPID (Liu et al., [2019b](#bib.bib92)), &#124;'
- en: '&#124; PMHLD (Chen et al., [2020](#bib.bib19)), HRGAN (Pang et al., [2018](#bib.bib105))
    &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PMHLD (Chen et al., [2020](#bib.bib19)), HRGAN (Pang et al., [2018](#bib.bib105))
    &#124;'
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Non-explicitly embedded ASM |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 非显式嵌入 ASM |'
- en: '&#124; AOD-Net (Li et al., [2017a](#bib.bib73)), FAMED-Net (Zhang and Tao,
    [2020](#bib.bib171)), DehazeGAN (Zhu et al., [2018](#bib.bib192)), PFDN (Dong
    and Pan, [2020](#bib.bib31)), &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AOD-Net (Li et al., [2017a](#bib.bib73)), FAMED-Net (Zhang and Tao,
    [2020](#bib.bib171)), DehazeGAN (Zhu et al., [2018](#bib.bib192)), PFDN (Dong
    and Pan, [2020](#bib.bib31)), &#124;'
- en: '&#124; SI-DehazeGAN (Zhu et al., [2021](#bib.bib191)) &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SI-DehazeGAN (Zhu et al., [2021](#bib.bib191)) &#124;'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Generative adversarial network |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 生成对抗网络 |'
- en: '&#124; EPDN (Qu et al., [2019](#bib.bib110)), PGC-UNet (Zhao et al., [2021a](#bib.bib187)),
    RI-GAN (Dudhane et al., [2019](#bib.bib37)), &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EPDN (屈等，[2019](#bib.bib110)), PGC-UNet (赵等，[2021a](#bib.bib187)), RI-GAN (杜丹等，[2019](#bib.bib37)),
    &#124;'
- en: '&#124; DHGAN (Sim et al., [2018](#bib.bib126)), SA-CGAN (Sharma et al., [2020](#bib.bib121))
    &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DHGAN (沈等，[2018](#bib.bib126)), SA-CGAN (香玛等，[2020](#bib.bib121)) &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Level-aware | LAP-Net (Li et al., [2019b](#bib.bib83)), HardGAN (Deng et al.,
    [2020](#bib.bib26)) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 级别感知 | LAP-Net (李等，[2019b](#bib.bib83)), HardGAN (邓等，[2020](#bib.bib26))
    |'
- en: '| Multi-function fusion | DMMFD (Deng et al., [2019](#bib.bib27)) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 多功能融合 | DMMFD (邓等，[2019](#bib.bib27)) |'
- en: '| Transformation and decomposition of input | GFN (Ren et al., [2018a](#bib.bib112)),
    MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)), DPDP-Net (Yang et al., [2019](#bib.bib154)),
    DIDH (Shyam et al., [2021](#bib.bib124)) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 输入的变换与分解 | GFN (任等，[2018a](#bib.bib112)), MSRL-DehazeNet (叶等，[2019](#bib.bib158)),
    DPDP-Net (杨等，[2019](#bib.bib154)), DIDH (夏姆等，[2021](#bib.bib124)) |'
- en: '| Knowledge distillation | KDDN (Hong et al., [2020](#bib.bib55)), KTDN (Wu
    et al., [2020](#bib.bib148)), SRKTDN (Chen et al., [2021a](#bib.bib17)), DALF (Fang
    et al., [2021](#bib.bib39)) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 | KDDN (洪等，[2020](#bib.bib55)), KTDN (吴等，[2020](#bib.bib148)), SRKTDN (陈等，[2021a](#bib.bib17)),
    DALF (方等，[2021](#bib.bib39)) |'
- en: '| Transformation of colorspace | AIP-Net (Wang et al., [2018a](#bib.bib133)),
    MSRA-Net (Sheng et al., [2022](#bib.bib122)), TheiaNet (Mehra et al., [2021](#bib.bib95)),
    RYF-Net (Dudhane and Murala, [2019b](#bib.bib36)) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 颜色空间变换 | AIP-Net (王等，[2018a](#bib.bib133)), MSRA-Net (盛等，[2022](#bib.bib122)),
    TheiaNet (梅赫拉等，[2021](#bib.bib95)), RYF-Net (杜丹和穆拉拉，[2019b](#bib.bib36)) |'
- en: '| Contrastive learning | AECR-Net  (Wu et al., [2021](#bib.bib149)) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 对比学习 | AECR-Net  (吴等，[2021](#bib.bib149)) |'
- en: '| Non-deterministic output | pWAE (Kim et al., [2021](#bib.bib67)), DehazeFlow (Li
    et al., [2021b](#bib.bib77)) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 非确定性输出 | pWAE (金等，[2021](#bib.bib67)), DehazeFlow (李等，[2021b](#bib.bib77))
    |'
- en: '| Retinex model | RDN (Li et al., [2021c](#bib.bib80)) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Retinex模型 | RDN (李等，[2021c](#bib.bib80)) |'
- en: '| Residual learning | GCA-Net(Chen et al., [2019c](#bib.bib14)), DRL (Du and
    Li, [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)), POGAN (Du
    and Li, [2019](#bib.bib34)) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 残差学习 | GCA-Net(陈等，[2019c](#bib.bib14)), DRL (杜和李，[2018](#bib.bib33)), SID-HL (肖等，[2020](#bib.bib150)),
    POGAN (杜和李，[2019](#bib.bib34)) |'
- en: '| Frequency domain |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 频域 |'
- en: '&#124; Wavelet U-net  (Yang and Fu, [2019](#bib.bib156)), MsGWN (Dong et al.,
    [2020c](#bib.bib30)), EMRA-Net (Wang et al., [2021d](#bib.bib136)), &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Wavelet U-net  (杨和傅，[2019](#bib.bib156)), MsGWN (董等，[2020c](#bib.bib30)),
    EMRA-Net (王等，[2021d](#bib.bib136)), &#124;'
- en: '&#124; TDN (Liu et al., [2020b](#bib.bib86)), DW-GAN (Fu et al., [2021](#bib.bib40))
    &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TDN (刘等，[2020b](#bib.bib86)), DW-GAN (傅等，[2021](#bib.bib40)) &#124;'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Joint dehazing and depth estimation |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 联合去雾与深度估计 |'
- en: '&#124; SDDE (Lee et al., [2020a](#bib.bib69)), S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)), DDRL (Guo and Monga, [2020](#bib.bib50)), &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SDDE (李等，[2020a](#bib.bib69)), S2DNet (汉巴尔德和穆拉拉，[2020](#bib.bib51)),
    DDRL (郭和蒙戈，[2020](#bib.bib50)), &#124;'
- en: '&#124; DeAID (Yang and Zhang, [2022](#bib.bib155)), TSDCN-Net  (Cheng and Zhao,
    [2021](#bib.bib23)) &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeAID (杨和张，[2022](#bib.bib155)), TSDCN-Net  (程和赵，[2021](#bib.bib23))
    &#124;'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Detection and segmentation with dehazing | LEAAL (Li et al., [2020c](#bib.bib82)),
    SDNet  (Zhang et al., [2022a](#bib.bib178)), UDnD (Zhang et al., [2020g](#bib.bib186))
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 去雾检测与分割 | LEAAL (李等，[2020c](#bib.bib82)), SDNet  (张等，[2022a](#bib.bib178)),
    UDnD (张等，[2020g](#bib.bib186)) |'
- en: '| End-to-end CNN |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 端到端CNN |'
- en: '&#124; FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu et al.,
    [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)) &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FFA-Net (秦等，[2020](#bib.bib109)), GridDehazeNet (刘等，[2019a](#bib.bib91)),
    SAN (梁等，[2019](#bib.bib85)) &#124;'
- en: '&#124; HFF (Zhang et al., [2022c](#bib.bib182)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)) &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HFF (张等，[2022c](#bib.bib182)), 4kDehazing (郑等，[2021](#bib.bib190)) &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semi-supervised | Pretrain backbone and fine-tune | PSD (Chen et al., [2021b](#bib.bib21)),
    SSDT (Zhang and Li, [2021](#bib.bib172)) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 半监督 | 预训练骨干网络和微调 | PSD (陈等，[2021b](#bib.bib21)), SSDT (张和李，[2021](#bib.bib172))
    |'
- en: '| Disentangled and reconstruction | DCNet (Chen et al., [2021c](#bib.bib22)),
    FSR (Liu et al., [2021](#bib.bib93)), CCDM (Zhang et al., [2020f](#bib.bib180))
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 解耦与重建 | DCNet (陈等，[2021c](#bib.bib22)), FSR (刘等，[2021](#bib.bib93)), CCDM (张等，[2020f](#bib.bib180))
    |'
- en: '| Two-branches training | DAID (Shao et al., [2020](#bib.bib119)), SSID (Li
    et al., [2020a](#bib.bib79)), SSIDN (An et al., [2022](#bib.bib2)) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 两分支训练 | DAID (邵等，[2020](#bib.bib119)), SSID (李等，[2020a](#bib.bib79)), SSIDN (安等，[2022](#bib.bib2))
    |'
- en: '| Unsupervised | Unsupervised domain translation |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | 无监督领域转换 |'
- en: '&#124; Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), CDNet (Dudhane and
    Murala, [2019a](#bib.bib35)), E-CycleGAN (Liu et al., [2020a](#bib.bib90)) &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), CDNet (Dudhane and
    Murala, [2019a](#bib.bib35)), E-CycleGAN (Liu et al., [2020a](#bib.bib90)) &#124;'
- en: '&#124; USID (Huang et al., [2019](#bib.bib57)), DCA-CycleGAN (Mo et al., [2022](#bib.bib99)),
    DHL (Cong et al., [2020](#bib.bib24)) &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; USID (Huang et al., [2019](#bib.bib57)), DCA-CycleGAN (Mo et al., [2022](#bib.bib99)),
    DHL (Cong et al., [2020](#bib.bib24)) &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Learning without haze-free images | Deep-DCP (Golts et al., [2020](#bib.bib44))
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 无需去雾图像的学习 | Deep-DCP (Golts et al., [2020](#bib.bib44)) |'
- en: '| Unsupervised image decomposition | Double-DIP (Gandelsman et al., [2019](#bib.bib42))
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 无监督图像分解 | Double-DIP (Gandelsman et al., [2019](#bib.bib42)) |'
- en: '| Zero-shot learning | ZID (Li et al., [2020b](#bib.bib72)), YOLY (Li et al.,
    [2021a](#bib.bib71)) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 零样本学习 | ZID (Li et al., [2020b](#bib.bib72)), YOLY (Li et al., [2021a](#bib.bib71))
    |'
- en: 1.1\. Scope and Goals of This Survey
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 本调查的范围和目标
- en: 'This survey does not cover all themes of dehazing research. We focus our attention
    on deep learning based algorithms that employ monocular daytime images. This means
    that we will not discuss in detail about non-deep learning dehazing, underwater
    dehazing (Wang et al., [2021b](#bib.bib141), [2017](#bib.bib142); Li et al., [2016](#bib.bib76)),
    video dehazing (Ren et al., [2018b](#bib.bib114)), hyperspectral dehazing (Mehta
    et al., [2020](#bib.bib96)), nighttime dehazing (Zhang et al., [2020a](#bib.bib169),
    [2017a](#bib.bib167), [2014](#bib.bib168)), binocular dehazing (Pang et al., [2020](#bib.bib104)),
    etc. Therefore, when we refer to “dehazing” in this paper, we usually mean deep
    learning based algorithms whose input data satisfies four conditions: single frame
    image, daytime, monocular, on the ground. In summary, there are three contributions
    to this survey as follows.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查不涵盖所有去雾研究主题。我们重点关注基于深度学习的算法，这些算法使用单目白天图像。这意味着我们不会详细讨论非深度学习去雾、潜水去雾 (Wang et
    al., [2021b](#bib.bib141), [2017](#bib.bib142); Li et al., [2016](#bib.bib76))、视频去雾
    (Ren et al., [2018b](#bib.bib114))、高光谱去雾 (Mehta et al., [2020](#bib.bib96))、夜间去雾
    (Zhang et al., [2020a](#bib.bib169), [2017a](#bib.bib167), [2014](#bib.bib168))、双目去雾
    (Pang et al., [2020](#bib.bib104)) 等。因此，当我们在本文中提到“去雾”时，通常指基于深度学习的算法，其输入数据满足四个条件：单帧图像、白天、单目、地面。总之，本文的贡献有以下三点。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Commonly used physical models, datasets, network modules, loss functions and
    evaluation metrics are summarized.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常用的物理模型、数据集、网络模块、损失函数和评估指标已被总结。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A classification and introduction of supervised, semi-supervised, and unsupervised
    methods is presented.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍了监督、半监督和无监督方法的分类。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to the existing achievements and unsolved problems, the future research
    directions are prospected.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据现有成果和未解决的问题，展望了未来的研究方向。
- en: 1.2\. A Guide for Reading This Survey
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 阅读本调查的指南
- en: Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") introduces physical model ASM, synthetic
    & generated & real world datasets, loss functions, basic modules commonly used
    in dehazing networks and evaluation metrics for various algorithms. Section [3](#S3
    "3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") provides a comprehensive discussion of supervised
    dehazing algorithms. A review of semi-supervised and unsupervised dehazing methods
    is in Section [4](#S4 "4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") and Section [5](#S5
    "5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), respectively. Section [6](#S6 "6\. Experiment
    and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") presents quantitative and qualitative experimental
    results for three categories of baseline algorithms. Section [7](#S7 "7\. Challenges
    and Opportunities ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing
    Based on Deep Learning") discusses the open issues of dehazing research.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 部分[2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")介绍了物理模型ASM、合成与生成以及现实世界数据集、损失函数、去雾网络中常用的基本模块以及各种算法的评估指标。部分[3](#S3
    "3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")提供了对监督去雾算法的全面讨论。半监督和无监督去雾方法的综述分别在部分[4](#S4 "4\.
    Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")和[5](#S5 "5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")中进行。部分[6](#S6
    "6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")展示了三类基线算法的定量和定性实验结果。部分[7](#S7
    "7\. Challenges and Opportunities ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")讨论了去雾研究中的开放问题。
- en: A critical challenge for a logical and comprehensive review of dehazing research
    is how to properly classify existing methods. In the classification of Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning"), there are several items that need to be pointed
    out as follows.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对去雾研究进行逻辑性和全面性综述的一个关键挑战是如何正确分类现有方法。在表格[1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")的分类中，有几个项目需要指出如下。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The DCP is validated on the dehazing task, and the inference process utilizes
    ASM. Therefore, those methods that utilize DCP are considered to be ASM-based.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DCP在去雾任务上得到了验证，并且推断过程利用了ASM。因此，利用DCP的方法被认为是基于ASM的。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey treat the knowledge distillation based supervised dehazing network
    as a supervised algorithm rather than a weakly supervised/semi-supervised algorithm.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查将基于知识蒸馏的监督去雾网络视为一种监督算法，而不是弱监督/半监督算法。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Supervised dehazing methods using total variation loss or GAN loss are still
    classified as supervised.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用总变差损失或生成对抗网络（GAN）损失的监督去雾方法仍然被归类为监督学习方法。
- en: 'Here we give the notational conventions for this survey. Unless otherwise specified,
    all symbols have the following meanings: $I(x)$ means hazy image; $J(x)$ denotes
    haze-free image. $t(x)$ stands for transmission map; $x$ in $I(x)$, $J(x)$ and
    $t(x)$ is pixel location. The subscript “$rec$” denotes “reconstructed”, such
    as $I_{rec}(x)$ is the reconstructed hazy image. The subscript “$pred$” refers
    to the prediction output. According to (Gandelsman et al., [2019](#bib.bib42)),
    atmospheric light may be regarded as a constant $A$ or a non-uniform matrix $A(x)$.
    In this survey, atmospheric light is uniformly denoted as $A$. In addition, many
    papers give the proposed algorithm an abbreviated name, such as GFN (Ren et al.,
    [2018a](#bib.bib112)) (gated fusion network for single image dehazing). For readability,
    this survey uses abbreviations as references to these papers. For a small amount
    of papers that do not give a name for their algorithms, we designate the abbreviated
    name according to the title of the corresponding paper.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出本调查的符号约定。除非另有说明，所有符号均有如下含义：$I(x)$ 表示有雾图像；$J(x)$ 表示无雾图像。$t(x)$ 代表传输图；$x$
    在 $I(x)$、$J(x)$ 和 $t(x)$ 中为像素位置。下标“$rec$”表示“重建”，例如 $I_{rec}(x)$ 是重建的有雾图像。下标“$pred$”表示预测输出。根据
    (Gandelsman 等，[2019](#bib.bib42))，大气光可以视为常量 $A$ 或非均匀矩阵 $A(x)$。在本调查中，大气光统一记作 $A$。此外，许多论文为其提出的算法提供了缩写名称，例如
    GFN (Ren 等，[2018a](#bib.bib112))（用于单图像去雾的门控融合网络）。为了可读性，本调查使用这些缩写作为参考。对于少量没有为其算法命名的论文，我们根据相应论文的标题指定缩写名称。
- en: 2\. Related Work
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: The commonalities of dehazing algorithms are mainly reflected in four aspects.
    First, the modeling of the network relies on physical model ASM or is completely
    based on neural networks. Second, the dataset used for training the network needs
    to contain transmission map, atmospheric light value, or paired supervision information.
    Third, the dehazing networks employ different kinds of basic modules. Fourth,
    different kinds of loss functions are used for training. Based on these four factors,
    the researchers designed a variety of effective dehazing algorithms. Thus, this
    section introduces ASM, datasets, loss functions, and network architecture modules.
    In addition, how to evaluate the dehazing results is also discussed in this section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 去雾算法的共同点主要体现在四个方面。首先，网络建模依赖于物理模型 ASM 或完全基于神经网络。其次，用于训练网络的数据集需要包含传输图、大气光值或配对监督信息。第三，去雾网络采用不同类型的基本模块。第四，使用不同类型的损失函数进行训练。基于这四个因素，研究人员设计了多种有效的去雾算法。因此，本节介绍了
    ASM、数据集、损失函数和网络架构模块。此外，本节还讨论了如何评估去雾结果。
- en: 2.1\. Modeling of the Dehazing Process
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 去雾过程建模
- en: Haze is a natural phenomenon that can be approximately explained by ASM. McCartney (McCartney,
    [1976](#bib.bib94)) first proposed the basic ASM to describe the principles of
    haze formation. Then, Narasimhan (Narasimhan and Nayar, [2003](#bib.bib102)) and
    Nayar (Nayar and Narasimhan, [1999](#bib.bib103)) extended and developed the ASM
    that is currently widely used. The ASM provides a reliable theoretical basis for
    the research of image dehazing. Its formula is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 雾霾是一种自然现象，可以通过 ASM 近似解释。McCartney（McCartney，[1976](#bib.bib94)）首次提出了基本的 ASM
    以描述雾霾形成的原理。随后，Narasimhan（Narasimhan 和 Nayar，[2003](#bib.bib102)）和 Nayar（Nayar
    和 Narasimhan，[1999](#bib.bib103)）扩展和发展了目前广泛使用的 ASM。ASM 为图像去雾研究提供了可靠的理论基础。其公式为
- en: '| (1) |  | $I(x)=J(x)t(x)+A(1-t(x)),$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $I(x)=J(x)t(x)+A(1-t(x)),$ |  |'
- en: where $x$ is the pixel location and $A$ means the global atmospheric light.
    In different papers, $A$ may be referred to as airlight or ambient light. For
    ease of understanding, $A$ is noted as atmospheric light in this survey. For the
    dehazing methods based on ASM, $A$ is usually unknown. $I(x)$ stands for the hazy
    image and $J(x)$ denotes the clear scene image. For most dehazing models, $I(x)$
    is the input and $J(x)$ is the desired output. The $t(x)$ means the medium transmission
    map which is defined as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是像素位置，$A$ 表示全球大气光。在不同的论文中，$A$ 可能被称为气光或环境光。为了便于理解，在本调查中 $A$ 记作大气光。对于基于
    ASM 的去雾方法，$A$ 通常是未知的。$I(x)$ 代表有雾图像，$J(x)$ 表示清晰场景图像。对于大多数去雾模型，$I(x)$ 是输入，$J(x)$
    是期望输出。$t(x)$ 表示介质传输图，其定义为
- en: '| (2) |  | $t(x)=e^{-\beta{d(x)}},$ |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $t(x)=e^{-\beta{d(x)}},$ |  |'
- en: 'where $\beta$ and $d(x)$ stands for the atmosphere scattering parameter and
    the depth of $I(x)$, respectively. Thus, the $t(x)$ is determined by $d(x)$, which
    can be used for the synthesis of hazy image. If the $t(x)$ and $A$ can be estimated,
    the haze-free image $J(x)$ can be obtained by the following formula:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 和 $d(x)$ 分别表示大气散射参数和 $I(x)$ 的深度。因此，$t(x)$ 由 $d(x)$ 决定，可用于合成雾霾图像。如果可以估计
    $t(x)$ 和 $A$，则可以通过以下公式获得无雾图像 $J(x)$：
- en: '| (3) |  | $J(x)=\frac{I(x)-A(1-t(x))}{t(x)}.$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $J(x)=\frac{I(x)-A(1-t(x))}{t(x)}.$ |  |'
- en: The imaging principle of ASM is shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning").
    It can be seen that the light reaching the camera from the object is affected
    by the particles in the air. Some works use ASM to describe the formation process
    of haze, and the parameters included in the atmospheric scattering model are solved
    in an explicit or implicit way. As shown in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"),
    ASM has a profound impact on dehazing research, including supervised (Ren et al.,
    [2016](#bib.bib111); Cai et al., [2016](#bib.bib13); Li et al., [2017a](#bib.bib73);
    Zhang and Patel, [2018](#bib.bib164)), semi-supervised (Li et al., [2020a](#bib.bib79);
    Liu et al., [2021](#bib.bib93); Chen et al., [2021b](#bib.bib21)), and unsupervised
    algorithms (Li et al., [2021a](#bib.bib71); Golts et al., [2020](#bib.bib44)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ASM 的成像原理如图 [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") 所示。可以看到，从物体到达相机的光线受到空气中颗粒的影响。一些工作使用
    ASM 描述雾霾的形成过程，并以显式或隐式的方式求解大气散射模型中的参数。如表 [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    所示，ASM 对去雾研究有深远的影响，包括监督式 (Ren et al., [2016](#bib.bib111); Cai et al., [2016](#bib.bib13);
    Li et al., [2017a](#bib.bib73); Zhang and Patel, [2018](#bib.bib164))，半监督式 (Li
    et al., [2020a](#bib.bib79); Liu et al., [2021](#bib.bib93); Chen et al., [2021b](#bib.bib21))，以及无监督算法 (Li
    et al., [2021a](#bib.bib71); Golts et al., [2020](#bib.bib44))。
- en: 2.2\. Datasets for Dehazing Task
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 去雾任务的数据集
- en: Table 2\. Datasets for image dehazing task. Syn stands for synthetic hazy images.
    HG denotes the hazy images generated from a haze generator. Real means real world
    scenes. S&R denotes Syn&Real. I and O denotes indoor and outdoor, respectively.
    P and NP means pair and non-pair, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 图像去雾任务的数据集。Syn 代表合成的雾霾图像。HG 表示从雾霾生成器生成的雾霾图像。Real 代表真实世界场景。S&R 代表 Syn&Real。I
    和 O 分别表示室内和室外。P 和 NP 分别表示配对和非配对。
- en: '| Dataset | type | Nums | I/O | P/NP |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 数量 | I/O | P/NP |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| D-HAZY (Ancuti et al., [2016](#bib.bib3)) | Syn | 1400+ | I | P |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| D-HAZY (Ancuti et al., [2016](#bib.bib3)) | Syn | 1400+ | I | P |'
- en: '| HazeRD (Zhang et al., [2017b](#bib.bib185)) | Syn | 15 | O | P |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| HazeRD (Zhang et al., [2017b](#bib.bib185)) | Syn | 15 | O | P |'
- en: '| I-HAZE (Ancuti et al., [2018b](#bib.bib5)) | HG | 35 | I | P |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| I-HAZE (Ancuti et al., [2018b](#bib.bib5)) | HG | 35 | I | P |'
- en: '| O-HAZE (Ancuti et al., [2018c](#bib.bib8)) | HG | 45 | O | P |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| O-HAZE (Ancuti et al., [2018c](#bib.bib8)) | HG | 45 | O | P |'
- en: '| RESIDE (Li et al., [2019c](#bib.bib74)) | S&R | 10000+ | I&O | P&NP |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RESIDE (Li et al., [2019c](#bib.bib74)) | S&R | 10000+ | I&O | P&NP |'
- en: '| Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) | HG | 33 | O | P |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) | HG | 33 | O | P |'
- en: '| NH-HAZE (Ancuti et al., [2020a](#bib.bib7)) | HG | 55 | O | P |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| NH-HAZE (Ancuti et al., [2020a](#bib.bib7)) | HG | 55 | O | P |'
- en: '| MRFID (Liu et al., [2020a](#bib.bib90)) | Real | 200 | O | P |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MRFID (Liu et al., [2020a](#bib.bib90)) | Real | 200 | O | P |'
- en: '| BeDDE (Zhao et al., [2020](#bib.bib188)) | Real | 200+ | O | P |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| BeDDE (Zhao et al., [2020](#bib.bib188)) | Real | 200+ | O | P |'
- en: '| 4KID (Zheng et al., [2021](#bib.bib190)) | Syn | 10000 | O | P |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4KID (Zheng et al., [2021](#bib.bib190)) | Syn | 10000 | O | P |'
- en: 'For computer vision tasks such as object detection, image segmentation, and
    image classification, accurate ground-truth labels can be obtained with careful
    annotation. However, sharp, accurate and pixel-wise labels (i.e., paired haze-free
    images) for hazy images in natural scenes are almost impossible to obtain. Currently,
    there are mainly two approaches for obtaining paired hazy and haze-free images.
    The first way is to obtain synthetic data with the help of ASM, such as the D-HAZY (Ancuti
    et al., [2016](#bib.bib3)), HazeRD (Zhang et al., [2017b](#bib.bib185)) and RESIDE (Li
    et al., [2019c](#bib.bib74)). By selecting different parameters for ASM, researchers
    can easily obtain hazy images with different haze densities. Four components are
    needed to synthesize a hazy image: a clear image, a depth map $d(x)$ corresponding
    to the content of the clear image, atmospheric light $A$ and atmosphere scattering
    parameter $\beta$. Thus, we can divide the synthetic dataset into two stages.
    In the first stage, clear images and corresponding depth maps are needed to be
    collected in pairs. In order to ensure that the synthesized haze is as close as
    possible to the real world haze, the depth information must be sufficiently accurate.
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the clear image and corresponding depth map in the NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)). In the second stage, atmospheric light $A$ and atmosphere
    scattering parameter $\beta$ are designated as fixed values or randomly selected.
    The commonly used D-HAZY (Ancuti et al., [2016](#bib.bib3)) dataset is synthesized
    when both $A$ and $\beta$ are $1$. Several researches choose different $A$ and
    $\beta$ in order to increase the diversity of the synthesized images and thus
    improve the generalization ability of the trained model. For example, MSCNN (Ren
    et al., [2016](#bib.bib111)) sets $A\in(0.7,1.0)$ and $\beta\in(0.5,1.5)$. Fig.
    [4](#S2.F4 "Figure 4 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    shows the corresponding hazy image when $\beta$ takes $6$ different values.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算机视觉任务，如目标检测、图像分割和图像分类，准确的真实标签可以通过仔细标注获得。然而，对于自然场景中的雾霾图像，几乎不可能获得清晰、准确且逐像素的标签（即配对的无雾图像）。目前，主要有两种方法来获得配对的雾霾和无雾图像。第一种方法是借助ASM获得合成数据，如D-HAZY（Ancuti
    et al., [2016](#bib.bib3)）、HazeRD（Zhang et al., [2017b](#bib.bib185)）和RESIDE（Li
    et al., [2019c](#bib.bib74)）。通过选择不同的ASM参数，研究人员可以轻松获得具有不同雾霾密度的图像。合成雾霾图像需要四个组件：清晰图像、与清晰图像内容对应的深度图$d(x)$、大气光$A$和大气散射参数$\beta$。因此，我们可以将合成数据集分为两个阶段。在第一阶段，需要成对收集清晰图像及其对应的深度图。为了确保合成的雾霾尽可能接近现实世界的雾霾，深度信息必须足够准确。图[3](#S2.F3
    "Figure 3 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")展示了NYU-Depth
    数据集中清晰图像及其对应的深度图（Silberman et al., [2012](#bib.bib125)）。在第二阶段，大气光$A$和大气散射参数$\beta$被指定为固定值或随机选择。常用的D-HAZY（Ancuti
    et al., [2016](#bib.bib3)）数据集是在$A$和$\beta$均为$1$时合成的。若干研究选择不同的$A$和$\beta$以增加合成图像的多样性，从而提高训练模型的泛化能力。例如，MSCNN（Ren
    et al., [2016](#bib.bib111)）将$A\in(0.7,1.0)$和$\beta\in(0.5,1.5)$。图[4](#S2.F4 "Figure
    4 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")展示了当$\beta$取$6$个不同值时的对应雾霾图像。
- en: '![Refer to caption](img/60136141ecd63dd2e71a51ede116fec1.png)![Refer to caption](img/e41d2dfc40739fce93d6721617f524dd.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60136141ecd63dd2e71a51ede116fec1.png)![参见说明](img/e41d2dfc40739fce93d6721617f524dd.png)'
- en: (a) A clear image       (b) Depth for the clear image
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 清晰图像       (b) 清晰图像的深度
- en: Figure 3\. A clear image and corresponding depth map in NYU-Depth dataset (Silberman
    et al., [2012](#bib.bib125)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. NYU-Depth 数据集中清晰图像及对应的深度图（Silberman et al., [2012](#bib.bib125)）。
- en: '![Refer to caption](img/289611583db612323c711b7b83db8a21.png)![Refer to caption](img/f779c97fa911a7a57446c982c83b1f81.png)![Refer
    to caption](img/33c380ec6c011ec9104f9d6cd80d0ff5.png)![Refer to caption](img/09c95952de08c447a55508d1f2f46b53.png)![Refer
    to caption](img/57e98b1225188fd083f91867742a762a.png)![Refer to caption](img/316ebd886e9811388ed98d6cf6f33776.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/289611583db612323c711b7b83db8a21.png)![参见说明](img/f779c97fa911a7a57446c982c83b1f81.png)![参见说明](img/33c380ec6c011ec9104f9d6cd80d0ff5.png)![参见说明](img/09c95952de08c447a55508d1f2f46b53.png)![参见说明](img/57e98b1225188fd083f91867742a762a.png)![参见说明](img/316ebd886e9811388ed98d6cf6f33776.png)'
- en: Figure 4\. Synthesized hazy images of different densities by setting different
    values for the atmosphere scattering parameter $\beta$ based on NYU-Depth dataset
    (Silberman et al., [2012](#bib.bib125)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 通过设置不同的大气散射参数 $\beta$，基于 NYU-Depth 数据集 (Silberman 等人，[2012](#bib.bib125))
    合成的不同密度的雾霾图像。
- en: The second way is to generate the hazy image by using a haze generator, such
    as I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti et al., [2018c](#bib.bib8)),
    Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
    The well-known competition New Trends in Image Restoration and Enhance (NTIRE
    2018-2020) dehazing challenge  (Ancuti et al., [2018a](#bib.bib4), [2019b](#bib.bib9),
    [2020b](#bib.bib10)) are based on these generated datasets. Fig. [5](#S2.F5 "Figure
    5 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows four pairs
    of hazy and haze-free examples contained in the datasets simulated by the haze
    generator. The images in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2\. Datasets for Dehazing
    Task ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") (a) are from indoor scenes, while (b), (c) and
    (d) are all pictured at outdoor views. There are differences in the pattern of
    haze in these three outdoor datasets. The haze in (b) and (c) is evenly distributed
    throughout the entire image, while the haze in (d) are non-homogeneous in the
    whole scenes. In addition, the density of haze in (c) is significantly higher
    than that in (b) and (d). These datasets with different characteristics provide
    useful insights for the design of dehazing algorithms. For example, in order to
    remove the high density of haze in Dense-Haze, it is necessary to design dehazing
    models with stronger feature extraction and recovery capabilities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是使用雾生成器生成雾霾图像，例如 I-HAZE (Ancuti 等人，[2018b](#bib.bib5))、O-HAZE (Ancuti 等人，[2018c](#bib.bib8))、Dense-Haze
    (Ancuti 等人，[2019a](#bib.bib6)) 和 NH-HAZE (Ancuti 等人，[2020a](#bib.bib7))。著名的“新趋势图像修复与增强”（NTIRE
    2018-2020）去雾挑战赛 (Ancuti 等人，[2018a](#bib.bib4)、[2019b](#bib.bib9)、[2020b](#bib.bib10))
    基于这些生成的数据集。图 [5](#S2.F5 "图5 ‣ 2.2\. 去雾任务的数据集 ‣ 2\. 相关工作 ‣ 基于深度学习的单图像去雾的综合调查和分类")
    显示了四对包含在由雾生成器模拟的数据集中的雾霾和无雾霾示例。图 [5](#S2.F5 "图5 ‣ 2.2\. 去雾任务的数据集 ‣ 2\. 相关工作 ‣ 基于深度学习的单图像去雾的综合调查和分类")
    中的图像 (a) 来自室内场景，而 (b)、(c) 和 (d) 都是户外视图。这三种户外数据集的雾霾模式存在差异。图像 (b) 和 (c) 的雾霾均匀分布在整个图像中，而
    (d) 中的雾霾则在整个场景中不均匀。此外，图像 (c) 中的雾霾密度显著高于 (b) 和 (d)。这些具有不同特征的数据集为去雾算法的设计提供了有用的见解。例如，为了去除
    Dense-Haze 中的高密度雾霾，需要设计具有更强特征提取和恢复能力的去雾模型。
- en: '![Refer to caption](img/0a67f29b9c7336e6502f2312160584d5.png)![Refer to caption](img/e89637664c4362468ae836fd15460f09.png)![Refer
    to caption](img/23ee74b6da9b40e50dc696f12b44dcf8.png)![Refer to caption](img/12a9d3b98e907b5bbbbf7b77ddeff179.png)![Refer
    to caption](img/00c58a42576e10d7fc02f256cf87990e.png)![Refer to caption](img/5bec5c0b347541aa68e504234a21dc28.png)![Refer
    to caption](img/c976c8c9abad588ceb996b8f311bcee9.png)![Refer to caption](img/bce4a4be864141dde51642ecac05e4b9.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0a67f29b9c7336e6502f2312160584d5.png)![参见说明](img/e89637664c4362468ae836fd15460f09.png)![参见说明](img/23ee74b6da9b40e50dc696f12b44dcf8.png)![参见说明](img/12a9d3b98e907b5bbbbf7b77ddeff179.png)![参见说明](img/00c58a42576e10d7fc02f256cf87990e.png)![参见说明](img/5bec5c0b347541aa68e504234a21dc28.png)![参见说明](img/c976c8c9abad588ceb996b8f311bcee9.png)![参见说明](img/bce4a4be864141dde51642ecac05e4b9.png)'
- en: (a) Indoor Haze             (b) Outdoor Haze         (c) Dense Haze      (d)
    Non-Homogeneous Haze
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 室内雾霾             (b) 户外雾霾         (c) 密集雾霾      (d) 非均匀雾霾
- en: Figure 5\. Examples from I-HAZE (Ancuti et al., [2018b](#bib.bib5)), O-HAZE (Ancuti
    et al., [2018c](#bib.bib8)), Dense-Haze (Ancuti et al., [2019a](#bib.bib6)) and
    NH-HAZE (Ancuti et al., [2020a](#bib.bib7)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 例图来自 I-HAZE (Ancuti 等人，[2018b](#bib.bib5))、O-HAZE (Ancuti 等人，[2018c](#bib.bib8))、Dense-Haze
    (Ancuti 等人，[2019a](#bib.bib6)) 和 NH-HAZE (Ancuti 等人，[2020a](#bib.bib7))。
- en: The main advantage of synthetic and generated haze is that it alleviates the
    difficulty during data acquisition. However, the hazy images synthesized based
    on ASM or generated by haze generator cannot perfectly simulate the formation
    process of real world haze. Therefore, there is an inherent difference between
    the synthetic and real world data. Several researches have noticed the problems
    of artificial data and tried to construct real world datasets, such as MRFID (Liu
    et al., [2020a](#bib.bib90)) and BeDDE (Zhao et al., [2020](#bib.bib188)). However,
    due to the high costs and difficulties of data collection, the current real world
    datasets do not contain enough examples as the synthetic dataset, like RESIDE (Li
    et al., [2019c](#bib.bib74)). To facilitate the comparison of different datasets,
    we summarize the characteristics of various datasets in Table [2](#S2.T2 "Table
    2 ‣ 2.2\. Datasets for Dehazing Task ‣ 2\. Related Work ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning").
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 合成和生成的雾的主要优点是减轻了数据采集过程中的困难。然而，基于ASM合成的雾图像或由雾生成器生成的雾图像无法完美模拟现实世界雾的形成过程。因此，合成数据和真实世界数据之间存在固有差异。一些研究已注意到人工数据的问题，并尝试构建真实世界的数据集，如MRFID（Liu
    et al., [2020a](#bib.bib90)）和BeDDE（Zhao et al., [2020](#bib.bib188)）。然而，由于数据收集的高成本和困难，当前的真实世界数据集无法包含像合成数据集那样的足够多的例子，例如RESIDE（Li
    et al., [2019c](#bib.bib74)）。为了便于不同数据集的比较，我们在表[2](#S2.T2 "Table 2 ‣ 2.2. Datasets
    for Dehazing Task ‣ 2. Related Work ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")中总结了各种数据集的特点。
- en: 2.3\. Network Block
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 网络块
- en: CNNs are widely used in current deep learning based dehazing networks. Commonly
    adopted modules are standard convolution, dilated convolution, multi-scale fusion,
    feature pyramid, cross-layer connection and attention. Usually, multiple basic
    blocks are formed into a dehazing network. In order to facilitate the understanding
    of the principles of different dehazing algorithms, the basic blocks commonly
    used in network architectures are summarized as follows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs在当前基于深度学习的去雾网络中被广泛使用。常用的模块包括标准卷积、扩张卷积、多尺度融合、特征金字塔、跨层连接和注意力机制。通常，多个基本块被组成一个去雾网络。为了便于理解不同去雾算法的原理，总结了网络架构中常用的基本块如下。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Standard convolution: It is shown that using standard convolution in a sequential
    connection way to build neural networks is effective. Therefore, standard convolution
    are often used in dehazing models (Li et al., [2017a](#bib.bib73); Ren et al.,
    [2016](#bib.bib111); Sharma et al., [2020](#bib.bib121); Zhang et al., [2020e](#bib.bib184))
    together with other blocks.'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准卷积：研究表明，使用标准卷积以顺序连接的方式构建神经网络是有效的。因此，标准卷积通常与其他块一起用于去雾模型（Li et al., [2017a](#bib.bib73);
    Ren et al., [2016](#bib.bib111); Sharma et al., [2020](#bib.bib121); Zhang et
    al., [2020e](#bib.bib184)）。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dilated convolution: Dilated convolution can increase the receptive field while
    keeping the size of the convolution kernel unchanged. Studies (Chen et al., [2019c](#bib.bib14);
    Zhang et al., [2020c](#bib.bib176); Zhang and He, [2020](#bib.bib174); Lee et al.,
    [2020b](#bib.bib70); Yan et al., [2020](#bib.bib153)) have shown that dilated
    convolution can improve the performance of global feature extraction. Moreover,
    fusing convolution layers with different dilation rates can extract features from
    different receptive fields.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩张卷积：扩张卷积可以在保持卷积核大小不变的情况下增加感受野。研究（Chen et al., [2019c](#bib.bib14); Zhang et
    al., [2020c](#bib.bib176); Zhang and He, [2020](#bib.bib174); Lee et al., [2020b](#bib.bib70);
    Yan et al., [2020](#bib.bib153)）表明，扩张卷积可以提高全局特征提取的性能。此外，融合不同扩张率的卷积层可以从不同的感受野中提取特征。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-scale fusion: CNNs with multi-scale convolution kernels have been proven
    effective in extracting features in a variety of visual tasks (Szegedy et al.,
    [2015](#bib.bib131)). By using convolution kernels at different scales and fusing
    the extracted feature together, dehazing methods (Wang et al., [2018a](#bib.bib133);
    Tang et al., [2019](#bib.bib132); Dudhane et al., [2019](#bib.bib37); Wang et al.,
    [2020](#bib.bib134)) have demonstrated that the fusion strategy can obtain the
    multi-scale details that are useful for image restoration. In the process of feature
    fusion, a common way is to spatially concatenate or add output features obtained
    by convolution kernels of different sizes.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多尺度融合：具有多尺度卷积核的卷积神经网络（CNN）在各种视觉任务中的特征提取上已被证明有效（Szegedy 等人，[2015](#bib.bib131)）。通过使用不同尺度的卷积核并融合提取的特征，去雾方法（Wang
    等人，[2018a](#bib.bib133)；Tang 等人，[2019](#bib.bib132)；Dudhane 等人，[2019](#bib.bib37)；Wang
    等人，[2020](#bib.bib134)）已经证明融合策略可以获得对图像恢复有用的多尺度细节。在特征融合过程中，一种常见的方法是空间上连接或添加由不同大小卷积核获得的输出特征。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature pyramid: In the research of digital image processing, the image pyramid
    can be used to obtain information of different resolutions. The dehazing network
    based on deep learning (Zhang et al., [2020e](#bib.bib184); Zhang and Patel, [2018](#bib.bib164);
    Zhang et al., [2018b](#bib.bib165); Singh et al., [2020](#bib.bib128); Zhao et al.,
    [2021a](#bib.bib187); Yin et al., [2020](#bib.bib160); Chen et al., [2019a](#bib.bib16))
    uses this strategy in the middle layer of the network to extract multiple scales
    of space and channel information.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征金字塔：在数字图像处理的研究中，图像金字塔可以用来获取不同分辨率的信息。基于深度学习的去雾网络（Zhang 等人，[2020e](#bib.bib184)；Zhang
    和 Patel，[2018](#bib.bib164)；Zhang 等人，[2018b](#bib.bib165)；Singh 等人，[2020](#bib.bib128)；Zhao
    等人，[2021a](#bib.bib187)；Yin 等人，[2020](#bib.bib160)；Chen 等人，[2019a](#bib.bib16)）在网络的中间层使用了这种策略来提取多尺度的空间和通道信息。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-layer connection: In order to enhance the information exchange between
    different layers and improve the feature extraction ability of the network, cross-layer
    connections are often used in CNNs. There are mainly three types of cross-layer
    connections used in dehazing networks, which are residual connection (Zhang et al.,
    [2020g](#bib.bib186); Qu et al., [2019](#bib.bib110); Hong et al., [2020](#bib.bib55);
    Liang et al., [2019](#bib.bib85); Chen et al., [2019d](#bib.bib20)) proposed by
    ResNet (He et al., [2016](#bib.bib53)), dense connection (Zhu et al., [2018](#bib.bib192);
    Zhang et al., [2022a](#bib.bib178); Dong et al., [2020a](#bib.bib32); Chen and
    Lai, [2019](#bib.bib15); Guo et al., [2019a](#bib.bib48); Li et al., [2019a](#bib.bib75))
    designed by DenseNet (Huang et al., [2017](#bib.bib56)), and skip connection (Zhao
    et al., [2021a](#bib.bib187); Dudhane et al., [2019](#bib.bib37); Yang and Zhang,
    [2022](#bib.bib155); Lee et al., [2020b](#bib.bib70)) inspired by U-Net (Ronneberger
    et al., [2015](#bib.bib115)).'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨层连接：为了增强不同层之间的信息交换并提高网络的特征提取能力，卷积神经网络中常常使用跨层连接。在去雾网络中主要有三种跨层连接类型，分别是残差连接（Zhang
    等人，[2020g](#bib.bib186)；Qu 等人，[2019](#bib.bib110)；Hong 等人，[2020](#bib.bib55)；Liang
    等人，[2019](#bib.bib85)；Chen 等人，[2019d](#bib.bib20)）由 ResNet（He 等人，[2016](#bib.bib53)）提出，密集连接（Zhu
    等人，[2018](#bib.bib192)；Zhang 等人，[2022a](#bib.bib178)；Dong 等人，[2020a](#bib.bib32)；Chen
    和 Lai，[2019](#bib.bib15)；Guo 等人，[2019a](#bib.bib48)；Li 等人，[2019a](#bib.bib75)）由
    DenseNet（Huang 等人，[2017](#bib.bib56)）设计，以及跳跃连接（Zhao 等人，[2021a](#bib.bib187)；Dudhane
    等人，[2019](#bib.bib37)；Yang 和 Zhang，[2022](#bib.bib155)；Lee 等人，[2020b](#bib.bib70)）受到
    U-Net（Ronneberger 等人，[2015](#bib.bib115)）的启发。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention in dehazing: The attention mechanism has been successfully applied
    in the research of natural language processing. Commonly used attention blocks
    in computer vision include channel attention and spatial attention. For the feature
    extraction and reconstruction process of 2D image, channel attention can emphasize
    the useful channels of the feature map. This unequal feature map processing strategy
    allows the model to focus more on effective feature information. The spatial attention
    mechanism focuses on the differences in the internal location regions of the feature
    map, such as the distribution of haze on the entire map. By embedding the attention
    module in the network, several dehazing methods (Liang et al., [2019](#bib.bib85);
    Chen et al., [2019d](#bib.bib20); Liu et al., [2019a](#bib.bib91); Qin et al.,
    [2020](#bib.bib109); Yin et al., [2020](#bib.bib160); Lee et al., [2020b](#bib.bib70);
    Yan et al., [2020](#bib.bib153); Dong et al., [2020c](#bib.bib30); Yan et al.,
    [2020](#bib.bib153); Zhang et al., [2020e](#bib.bib184); Yin et al., [2021](#bib.bib161);
    Metwaly et al., [2020](#bib.bib97); Wang et al., [2021d](#bib.bib136)) have achieved
    excellent dehazing performance.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去雾中的注意力机制：注意力机制已经成功应用于自然语言处理的研究中。在计算机视觉中，常用的注意力模块包括通道注意力和空间注意力。在二维图像的特征提取和重建过程中，通道注意力可以强调特征图的有用通道。这种不均等的特征图处理策略使模型能够更加关注有效的特征信息。空间注意力机制则关注特征图内部位置区域的差异，如整个图像上的雾霾分布。通过在网络中嵌入注意力模块，一些去雾方法（Liang等，[2019](#bib.bib85)；Chen等，[2019d](#bib.bib20)；Liu等，[2019a](#bib.bib91)；Qin等，[2020](#bib.bib109)；Yin等，[2020](#bib.bib160)；Lee等，[2020b](#bib.bib70)；Yan等，[2020](#bib.bib153)；Dong等，[2020c](#bib.bib30)；Yan等，[2020](#bib.bib153)；Zhang等，[2020e](#bib.bib184)；Yin等，[2021](#bib.bib161)；Metwaly等，[2020](#bib.bib97)；Wang等，[2021d](#bib.bib136)）已取得了优异的去雾效果。
- en: 2.4\. Loss Function
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 损失函数
- en: This section introduces the commonly adopted loss functions in supervised, semi-supervised
    and unsupervised dehazing models, which can be used for transmission map estimation,
    clear image prediction, hazy image reconstruction, atmospheric light regression,
    etc. Several algorithms use multiple losses in combination to obtain better dehazing
    performance. A detailed classification and summary of the loss functions used
    by different dehazing methods is presented in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\.
    Total Variation Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"). In the
    loss function introduced below, $X$ and $Y$ denote the predicted value and ground
    truth value, respectively.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在监督式、半监督式和无监督式去雾模型中常用的损失函数，这些函数可用于传输图估计、清晰图像预测、雾霾图像重建、大气光回归等。一些算法通过组合多个损失函数来获得更好的去雾性能。不同去雾方法使用的损失函数的详细分类和总结见表[3](#S2.T3
    "表 3 ‣ 2.4.5\. 总变差损失 ‣ 2.4\. 损失函数 ‣ 2\. 相关工作 ‣ 基于深度学习的单幅图像去雾的全面调查和分类")。在下文介绍的损失函数中，$X$和$Y$分别表示预测值和真实值。
- en: 2.4.1\. Fidelity Loss
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 保真度损失
- en: 'The widely used pixel-wise loss functions for dehazing research are L1 loss
    and L2 loss, which are defined as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛用于去雾研究的逐像素损失函数有L1损失和L2损失，其定义如下：
- en: '| (4) |  | $L1=&#124;&#124;X-Y&#124;&#124;_{1}.$ |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $L1=&#124;&#124;X-Y&#124;&#124;_{1}.$ |  |'
- en: '| (5) |  | $L2=&#124;&#124;X-Y&#124;&#124;_{2}.$ |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $L2=&#124;&#124;X-Y&#124;&#124;_{2}.$ |  |'
- en: 2.4.2\. Perceptual Loss
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 感知损失
- en: The research on image super resolution (Ledig et al., [2017](#bib.bib68); Wang
    et al., [2018b](#bib.bib140)) and style transfer (Johnson et al., [2016](#bib.bib64))
    indicates that the attributes of the human visual system in the process of perceptual
    evaluation is not fully reflected by L1 or L2 loss. Meanwhile, L2 loss may lead
    to over-smooth outputs (Ledig et al., [2017](#bib.bib68)). Recent researches use
    pre-trained classification neural networks to calculate the perceptual loss in
    the feature space. The most commonly used pre-training model is VGG (Simonyan
    and Zisserman, [2015](#bib.bib127)), and some of its layers are used to calculate
    the distance between the predicted image and the reference image in the feature
    space, i.e.,
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像超分辨率的研究（Ledig 等，[2017](#bib.bib68)；Wang 等，[2018b](#bib.bib140)）和风格迁移（Johnson
    等，[2016](#bib.bib64)）表明，人类视觉系统在感知评估过程中的属性并不能完全通过 L1 或 L2 损失来反映。同时，L2 损失可能导致过度平滑的输出（Ledig
    等，[2017](#bib.bib68)）。最近的研究使用预训练的分类神经网络来计算特征空间中的感知损失。最常用的预训练模型是 VGG（Simonyan 和
    Zisserman，[2015](#bib.bib127)），其某些层被用来计算预测图像和参考图像在特征空间中的距离，即，
- en: '| (6) |  | $L_{per}(X,Y)=\sum_{i=1}^{N}&#124;&#124;\psi_{i}(X)-\psi_{i}(Y)&#124;&#124;_{2},$
    |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $L_{per}(X,Y)=\sum_{i=1}^{N}&#124;&#124;\psi_{i}(X)-\psi_{i}(Y)&#124;&#124;_{2},$
    |  |'
- en: where $N$ represents the number of features selected for calculation; $i$ means
    the index of feature map; $\psi(\cdot)$ denotes the pretrained VGG. During the
    calculation of perceptual loss and network optimization, the parameters of VGG
    are always frozen.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 代表用于计算的特征数量；$i$ 表示特征图的索引；$\psi(\cdot)$ 表示预训练的 VGG。在计算感知损失和网络优化过程中，VGG
    的参数始终被冻结。
- en: 2.4.3\. Structure Loss
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 结构损失
- en: As a metric of the dehazing methods, Structural Similarity (SSIM) (Wang et al.,
    [2004](#bib.bib144)) is also used as a loss function in the optimization process.
    Studies (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162)) have
    shown that SSIM loss can improve the structural similarity during image restoration.
    MS-SSIM (Wang et al., [2003](#bib.bib145)) introduces multi-scale evaluation into
    SSIM, which is also used as a loss function by the dehazing algorithms, i.e.,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 作为去雾方法的度量，结构相似性（SSIM）（Wang 等，[2004](#bib.bib144)）也在优化过程中用作损失函数。研究（Dong 等，[2020a](#bib.bib32)；Yu
    等，[2020](#bib.bib162)）表明，SSIM 损失可以在图像恢复过程中提高结构相似性。MS-SSIM（Wang 等，[2003](#bib.bib145)）将多尺度评估引入
    SSIM，这也被去雾算法用作损失函数，即，
- en: '| (7) |  | $L_{ssim}=1-SSIM(X,Y),$ |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $L_{ssim}=1-SSIM(X,Y),$ |  |'
- en: '| (8) |  | $L_{msssim}=1-MSSSIM(X,Y).$ |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $L_{msssim}=1-MSSSIM(X,Y).$ |  |'
- en: 2.4.4\. Gradient Loss
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4\. 梯度损失
- en: 'Gradient loss, also known as edge loss, is used to better restore the contour
    and edge information of the haze-free image. The edge extraction can be implemented
    as Laplacian operator, Canny operator, and so on. For example, SA-CGAN  (Sharma
    et al., [2020](#bib.bib121)) uses the Laplacian of Gaussian with standard deviation
    $\sigma$ to perform quadratic differentiation on the two-dimensional image $F$:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度损失，也称为边缘损失，用于更好地恢复无雾图像的轮廓和边缘信息。边缘提取可以通过拉普拉斯算子、Canny 算子等来实现。例如，SA-CGAN（Sharma
    等，[2020](#bib.bib121)）使用标准差为 $\sigma$ 的高斯拉普拉斯算子对二维图像 $F$ 进行二次微分：
- en: '| (9) |  | $\displaystyle L(m,n)$ | $\displaystyle=\bigtriangledown^{2}F(m,n)=\frac{\partial^{2}F}{\partial{m^{2}}}+\frac{\partial^{2}F}{\partial{n^{2}}}=-\frac{1}{\pi{\sigma^{4}}}[1-\frac{m^{2}+n^{2}}{2\sigma^{2}}]\exp{(-\frac{m^{2}+n^{2}}{2\sigma^{2}})},$
    |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle L(m,n)$ | $\displaystyle=\bigtriangledown^{2}F(m,n)=\frac{\partial^{2}F}{\partial{m^{2}}}+\frac{\partial^{2}F}{\partial{n^{2}}}=-\frac{1}{\pi{\sigma^{4}}}[1-\frac{m^{2}+n^{2}}{2\sigma^{2}}]\exp{(-\frac{m^{2}+n^{2}}{2\sigma^{2}})},$
    |  |'
- en: where $(m,n)$ means pixel location and $L(m,n)$ is calculated for both $X$ and
    $Y$, respectively. Then, regression objective functions such as L1 and L2 are
    used for the calculation of gradient loss.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(m,n)$ 表示像素位置，$L(m,n)$ 分别对 $X$ 和 $Y$ 进行计算。然后，L1 和 L2 等回归目标函数用于计算梯度损失。
- en: 2.4.5\. Total Variation Loss
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.5\. 总变差损失
- en: 'Total variation (TV) loss (Rudin et al., [1992](#bib.bib116)) can be used to
    smooth image and remove noise. The training objective is to minimize the following
    function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差（TV）损失（Rudin 等，[1992](#bib.bib116)）可以用来平滑图像并去除噪声。训练目标是最小化以下函数：
- en: '| (10) |  | $L_{TV}=&#124;&#124;\partial_{m}{X}&#124;&#124;_{1}+&#124;&#124;\partial_{n}{X}&#124;&#124;_{1},$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $L_{TV}=&#124;&#124;\partial_{m}{X}&#124;&#124;_{1}+&#124;&#124;\partial_{n}{X}&#124;&#124;_{1},$
    |  |'
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    It can be seen from the formula that the TV loss can be added to networks trained
    in an unsupervised manner without using ground-truth $Y$.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别表示水平和垂直坐标。从公式中可以看出，TV 损失可以添加到以无监督方式训练的网络中，而无需使用真实值 $Y$。
- en: Table 3\. Loss functions for dehazing task
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 去雾任务的损失函数
- en: '| Loss Function | Algorithms |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 | 算法 |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| L1 |  (Mondal et al., [2018](#bib.bib100); Chen et al., [2019b](#bib.bib18);
    Deng et al., [2019](#bib.bib27); Liang et al., [2019](#bib.bib85); Yin et al.,
    [2019](#bib.bib159); Dong and Pan, [2020](#bib.bib31); Chen et al., [2020](#bib.bib19);
    Yan et al., [2020](#bib.bib153); Qin et al., [2020](#bib.bib109); Hong et al.,
    [2020](#bib.bib55); Li et al., [2020d](#bib.bib81); Zhang et al., [2020b](#bib.bib175);
    Zhao et al., [2021a](#bib.bib187); Park et al., [2020](#bib.bib107); Shin et al.,
    [2022](#bib.bib123); Zhang et al., [2022a](#bib.bib178)) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| L1 |  (Mondal 等, [2018](#bib.bib100); Chen 等, [2019b](#bib.bib18); Deng 等,
    [2019](#bib.bib27); Liang 等, [2019](#bib.bib85); Yin 等, [2019](#bib.bib159); Dong
    和 Pan, [2020](#bib.bib31); Chen 等, [2020](#bib.bib19); Yan 等, [2020](#bib.bib153);
    Qin 等, [2020](#bib.bib109); Hong 等, [2020](#bib.bib55); Li 等, [2020d](#bib.bib81);
    Zhang 等, [2020b](#bib.bib175); Zhao 等, [2021a](#bib.bib187); Park 等, [2020](#bib.bib107);
    Shin 等, [2022](#bib.bib123); Zhang 等, [2022a](#bib.bib178)) |'
- en: '| L2 |  (Li et al., [2017a](#bib.bib73); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2018](#bib.bib192); Zhang et al., [2018a](#bib.bib179); Guo et al.,
    [2019b](#bib.bib49); Morales et al., [2019](#bib.bib101); Chen et al., [2019d](#bib.bib20);
    Yang et al., [2019](#bib.bib154); Tang et al., [2019](#bib.bib132); Dong et al.,
    [2020b](#bib.bib29); Zhang et al., [2020d](#bib.bib177); Yin et al., [2020](#bib.bib160);
    Zhang et al., [2021b](#bib.bib183), [a](#bib.bib181), [2022c](#bib.bib182); Huang
    et al., [2021](#bib.bib58); Sheng et al., [2022](#bib.bib122)) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| L2 |  (Li 等, [2017a](#bib.bib73); Pang 等, [2018](#bib.bib105); Zhu 等, [2018](#bib.bib192);
    Zhang 等, [2018a](#bib.bib179); Guo 等, [2019b](#bib.bib49); Morales 等, [2019](#bib.bib101);
    Chen 等, [2019d](#bib.bib20); Yang 等, [2019](#bib.bib154); Tang 等, [2019](#bib.bib132);
    Dong 等, [2020b](#bib.bib29); Zhang 等, [2020d](#bib.bib177); Yin 等, [2020](#bib.bib160);
    Zhang 等, [2021b](#bib.bib183), [a](#bib.bib181), [2022c](#bib.bib182); Huang 等,
    [2021](#bib.bib58); Sheng 等, [2022](#bib.bib122)) |'
- en: '| SSIM |  (Dong et al., [2020a](#bib.bib32); Yu et al., [2020](#bib.bib162);
    Metwaly et al., [2020](#bib.bib97); Wei et al., [2020](#bib.bib146); Singh et al.,
    [2020](#bib.bib128); Li et al., [2020e](#bib.bib78); Jo and Sim, [2021](#bib.bib63);
    Shyam et al., [2021](#bib.bib124); Zhao et al., [2021a](#bib.bib187)) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| SSIM |  (Dong 等, [2020a](#bib.bib32); Yu 等, [2020](#bib.bib162); Metwaly
    等, [2020](#bib.bib97); Wei 等, [2020](#bib.bib146); Singh 等, [2020](#bib.bib128);
    Li 等, [2020e](#bib.bib78); Jo 和 Sim, [2021](#bib.bib63); Shyam 等, [2021](#bib.bib124);
    Zhao 等, [2021a](#bib.bib187)) |'
- en: '| MS-SSIM |  (Sun et al., [2021](#bib.bib130); Guo et al., [2019a](#bib.bib48);
    Cong et al., [2020](#bib.bib24); Yu et al., [2021](#bib.bib163); Fu et al., [2021](#bib.bib40))
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MS-SSIM |  (Sun 等, [2021](#bib.bib130); Guo 等, [2019a](#bib.bib48); Cong
    等, [2020](#bib.bib24); Yu 等, [2021](#bib.bib163); Fu 等, [2021](#bib.bib40)) |'
- en: '| Perceptual |  (Sim et al., [2018](#bib.bib126); Pang et al., [2018](#bib.bib105);
    Zhu et al., [2021](#bib.bib191); Zhang et al., [2022c](#bib.bib182); Wang et al.,
    [2021c](#bib.bib135); Deng et al., [2020](#bib.bib26); Liu et al., [2019a](#bib.bib91);
    Hong et al., [2020](#bib.bib55); Qu et al., [2019](#bib.bib110); Chen and Lai,
    [2019](#bib.bib15); Li et al., [2019a](#bib.bib75); Chen et al., [2019d](#bib.bib20);
    Singh et al., [2020](#bib.bib128); Dong et al., [2020a](#bib.bib32); Shyam et al.,
    [2021](#bib.bib124); Engin et al., [2018](#bib.bib38)) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 感知损失 |  (Sim 等, [2018](#bib.bib126); Pang 等, [2018](#bib.bib105); Zhu 等,
    [2021](#bib.bib191); Zhang 等, [2022c](#bib.bib182); Wang 等, [2021c](#bib.bib135);
    Deng 等, [2020](#bib.bib26); Liu 等, [2019a](#bib.bib91); Hong 等, [2020](#bib.bib55);
    Qu 等, [2019](#bib.bib110); Chen 和 Lai, [2019](#bib.bib15); Li 等, [2019a](#bib.bib75);
    Chen 等, [2019d](#bib.bib20); Singh 等, [2020](#bib.bib128); Dong 等, [2020a](#bib.bib32);
    Shyam 等, [2021](#bib.bib124); Engin 等, [2018](#bib.bib38)) |'
- en: '| TV Loss |  (Das and Dutta, [2020](#bib.bib25); Li et al., [2020a](#bib.bib79);
    Shao et al., [2020](#bib.bib119); Huang et al., [2019](#bib.bib57); Wang et al.,
    [2021c](#bib.bib135); He et al., [2019](#bib.bib54)) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| TV 损失 |  (Das 和 Dutta, [2020](#bib.bib25); Li 等, [2020a](#bib.bib79); Shao
    等, [2020](#bib.bib119); Huang 等, [2019](#bib.bib57); Wang 等, [2021c](#bib.bib135);
    He 等, [2019](#bib.bib54)) |'
- en: '| Gradient |  (Zhang and Patel, [2018](#bib.bib164); Zhang et al., [2019a](#bib.bib166),
    [2020b](#bib.bib175), [2020e](#bib.bib184); Yin et al., [2020](#bib.bib160); Zhang
    et al., [2022b](#bib.bib170); Li et al., [2021c](#bib.bib80); Dudhane et al.,
    [2019](#bib.bib37); Yin et al., [2021](#bib.bib161)) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Gradient |  (Zhang和Patel，[2018](#bib.bib164)；Zhang等，[2019a](#bib.bib166)，[2020b](#bib.bib175)，[2020e](#bib.bib184)；Yin等，[2020](#bib.bib160)；Zhang等，[2022b](#bib.bib170)；Li等，[2021c](#bib.bib80)；Dudhane等，[2019](#bib.bib37)；Yin等，[2021](#bib.bib161))
    |'
- en: 2.5\. Image Quality Metrics
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 图像质量指标
- en: Due to the presence of haze, the saturation and contrast of the image are reduced,
    and color of the image is distorted by the uncertainty mode. To measure the difference
    between the dehazed image and the ground truth haze-free image, objective metrics
    are needed to evaluate the results obtained by various dehazing algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在雾霾，图像的饱和度和对比度降低，图像的颜色由于不确定性模式而失真。为了测量去雾图像与真实无雾图像之间的差异，需要使用客观指标来评估各种去雾算法得到的结果。
- en: 'Most papers use Peak Signal-to-Noise Ratio (PSNR) (Huynh-Thu and Ghanbari,
    [2008](#bib.bib60)) and SSIM (Wang et al., [2004](#bib.bib144)) to evaluate the
    image quality after dehazing. The computation of PSNR needs to use the formula
    ([11](#S2.E11 "In 2.5\. Image Quality Metrics ‣ 2\. Related Work ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")) to obtain
    the mean square error (MSE):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数论文使用峰值信噪比（PSNR） (Huynh-Thu和Ghanbari，[2008](#bib.bib60))和SSIM (Wang等，[2004](#bib.bib144))来评估去雾后的图像质量。计算PSNR需要使用公式（[11](#S2.E11
    "In 2.5\. Image Quality Metrics ‣ 2\. Related Work ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning")）来获得均方误差（MSE）：
- en: '| (11) |  | $MSE=\frac{1}{H\times{W}}\sum_{i=1}^{H}\sum_{j=1}^{W}(X(i,j)-Y(i,j))^{2},$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $MSE=\frac{1}{H\times{W}}\sum_{i=1}^{H}\sum_{j=1}^{W}(X(i,j)-Y(i,j))^{2},$
    |  |'
- en: 'where $X$ and $Y$ respectively represent two images to be evaluated. $H$ and
    $W$ are their height and width, that is, the dimensionalities of $X$ and $Y$ should
    be strictly the same. The pixel position index of the image is represented by
    $i$ and $j$. Then, the PSNR can be obtained by logarithmic calculation as following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$和$Y$分别表示要评估的两幅图像。$H$和$W$是它们的高度和宽度，即$X$和$Y$的维度应完全一致。图像的像素位置索引由$i$和$j$表示。然后，可以通过对数计算得到PSNR，如下所示：
- en: '| (12) |  | $PSNR=10log_{10}[\frac{({2^{N}-1)^{2}}}{MSE}],$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $PSNR=10log_{10}[\frac{({2^{N}-1)^{2}}}{MSE}],$ |  |'
- en: 'where $N$ equals $8$ for $8$-bit images. SSIM is based on the correlation between
    human visual perception and structural information, and its formula is defined
    as following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$等于$8$用于$8$位图像。SSIM基于人类视觉感知与结构信息之间的相关性，其公式定义如下：
- en: '| (13) |  | $SSIM(X,Y)=\frac{{2u_{x}}{u_{y}}+C_{1}}{u_{x}^{2}+u_{y}^{2}+C_{1}}\frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $SSIM(X,Y)=\frac{{2u_{x}}{u_{y}}+C_{1}}{u_{x}^{2}+u_{y}^{2}+C_{1}}\frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  |'
- en: where $\mu_{x}$, $\mu_{y}$, $\sigma_{x}$ and $\sigma_{y}$ represent the mean
    and variance of $X$ and $Y$, respectively; $\sigma_{xy}$ is the covariance between
    two variables; $C_{1}$ and $C_{2}$ are constants used to ensure numerical stability.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu_{x}$、$\mu_{y}$、$\sigma_{x}$和$\sigma_{y}$分别表示$X$和$Y$的均值和方差；$\sigma_{xy}$是两个变量之间的协方差；$C_{1}$和$C_{2}$是用于确保数值稳定性的常数。
- en: Since haze can cause the color of scenes and objects to change, some works use
    CIEDE2000 (Sharma et al., [2005](#bib.bib120)) as an assessment of the degree
    of color shift. PSNR, SSIM and CIEDE belong to full-reference evaluation metrics,
    which means that a clear image corresponding to a hazy image must be used as a
    reference. However, real world pairs of hazy and haze-free images are difficult
    to keep exactly the same in content. For example, the lighting and objects in
    the scene may change before and after the haze appears. Therefore, in order to
    maintain the accuracy of the evaluation process, it is necessary to synthesize
    the corresponding hazy image with a clear image (Min et al., [2019](#bib.bib98)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雾霾可能导致场景和物体的颜色发生变化，一些研究使用CIEDE2000 (Sharma等，[2005](#bib.bib120))作为颜色偏移度的评估。PSNR、SSIM和CIEDE属于全参考评估指标，这意味着必须使用与雾霾图像对应的清晰图像作为参考。然而，现实世界中的雾霾和无雾图像对很难在内容上完全相同。例如，场景中的光照和物体在雾霾出现前后可能发生变化。因此，为了保持评估过程的准确性，有必要合成对应的雾霾图像与清晰图像 (Min等，[2019](#bib.bib98))。
- en: In addition to the full-reference metric PSNR, SSIM and CIEDE, the recent works (Li
    et al., [2020c](#bib.bib82), [2019c](#bib.bib74)) utilizes the no-reference metrics
    SSEQ (Liu et al., [2014](#bib.bib87)) and BLIINDS-II (Saad et al., [2012](#bib.bib117))
    to evaluate dehazed images without ground truth. No-reference metric is of crucial
    value for real world dehazing evaluation. Nevertheless, the evaluation of current
    dehazing algorithms are usually conducted on datasets with pairs of hazy and haze-free
    images. Since the full-reference metric is more suitable for paired datasets,
    it is more widely used than the no-reference metric.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了全参考度量 PSNR、SSIM 和 CIEDE 外，近期的研究（Li et al., [2020c](#bib.bib82), [2019c](#bib.bib74)）还利用了无参考度量
    SSEQ (Liu et al., [2014](#bib.bib87)) 和 BLIINDS-II (Saad et al., [2012](#bib.bib117))
    来评估没有真实标签的去雾图像。无参考度量对于现实世界的去雾评估至关重要。然而，当前的去雾算法通常在包含雾霾图像和去雾图像配对的数据集上进行评估。由于全参考度量更适用于配对数据集，因此它比无参考度量使用得更广泛。
- en: 3\. Supervised Dehazing
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 有监督去雾
- en: Supervised dehazing models usually require different types of supervisory signals
    to guide the training process, such as transmission map, atmospheric light, haze-free
    image label, etc. Conceptually, supervised dehazing methods can be divided into
    ASM-based and non-ASM-based ones. However, there may be overlaps in this way,
    since both ASM-based and non-ASM-based algorithms may entangle with other computer
    vision tasks such as segmentation, detection, and depth estimation. Therefore,
    this section categorizes supervised algorithms according to their main contributions,
    so that the techniques that prove valuable for dehazing research are clearly observed.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督去雾模型通常需要不同类型的监督信号来指导训练过程，如传输图、大气光、去雾图像标签等。概念上，有监督去雾方法可以分为基于 ASM 的和非 ASM 基于的方法。然而，由于基于
    ASM 和非 ASM 的算法可能涉及其他计算机视觉任务，如分割、检测和深度估计，这种分类方法可能会有重叠。因此，本节根据主要贡献对有监督算法进行分类，以便清晰地观察对去雾研究有价值的技术。
- en: 3.1\. Learning of $t(x)$
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. $t(x)$ 的学习
- en: 'According to the ASM, the dehazing process can be divided into three parts:
    transmission map estimation, atmospheric light prediction, and haze-free image
    recovery. MSCNN (Ren et al., [2016](#bib.bib111)) proposes the following three
    steps for solving ASM: (1) use CNN to estimate the transmission map $t(x)$, (2)
    adopt statistical rules to predict atmospheric light $A$, and (3) solve $J(x)$
    by $t(x)$ and $A$ jointly. MSCNN adopts a multi-scale convolutional model for
    transmission map estimation and optimizes it with L2 loss. In addition, $A$ can
    be obtained by selecting $0.1\%$ darkest pixels in $t(x)$ corresponding the one
    with the highest intensity in $I(x)$ (He et al., [2010](#bib.bib52)). Thus the
    clear image $J(x)$ can be obtained by'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 ASM，去雾过程可以分为三个部分：传输图估计、大气光预测和去雾图像恢复。MSCNN (Ren et al., [2016](#bib.bib111))
    提出了以下三个步骤来解决 ASM： (1) 使用 CNN 估计传输图 $t(x)$，(2) 采用统计规则预测大气光 $A$，(3) 通过 $t(x)$ 和
    $A$ 共同解决 $J(x)$。MSCNN 采用多尺度卷积模型进行传输图估计，并使用 L2 损失优化。此外，可以通过选择 $t(x)$ 中 $0.1\%$
    最暗的像素来获得 $A$，这些像素对应于 $I(x)$ 中的最高强度（He et al., [2010](#bib.bib52)）。从而可以获得清晰图像 $J(x)$。
- en: '| (14) |  | $J(x)=\frac{I(x)-A}{max{\{0.1,t(x)\}}}+A.$ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $J(x)=\frac{I(x)-A}{max{\{0.1,t(x)\}}}+A.$ |  |'
- en: Different papers may use different statistical priors to estimate $A$, but the
    strategies they use for dehazing are similar to MSCNN. ABC-Net (Wang et al., [2020](#bib.bib134))
    uses the max pooling operation to obtain maximum value from each channel of $I(x)$.
    SID-JPM (Huang et al., [2018](#bib.bib59)) filters each channel of a RGB input
    image by a minimum filter kernel. Then the maximum value of each channel is used
    as the estimated $A$. LAPTN (Liu et al., [2018](#bib.bib89)) also applies the
    minimum filter together with the maximum filter for the prediction of $A$. These
    methods generally do not require atmospheric light annotations, but need paired
    of hazy images and transmission maps.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的论文可能使用不同的统计先验来估计 $A$，但它们用于去雾的策略与 MSCNN 相似。ABC-Net (Wang et al., [2020](#bib.bib134))
    使用最大池化操作从 $I(x)$ 的每个通道中获得最大值。SID-JPM (Huang et al., [2018](#bib.bib59)) 通过最小滤波器内核过滤
    RGB 输入图像的每个通道。然后，将每个通道的最大值用作估计的 $A$。LAPTN (Liu et al., [2018](#bib.bib89)) 也结合使用最小滤波器和最大滤波器来预测
    $A$。这些方法通常不需要大气光注释，但需要雾霾图像和传输图的配对。
- en: 3.2\. Joint Learning of $t(x)$ and $A$
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. $t(x)$ 和 $A$ 的联合学习
- en: Instead of using convolutional networks and statistical priors jointly to estimate
    the physical parameters of ASM, some works implement the prediction of physical
    parameters entirely through CNN. DCPDN (Zhang and Patel, [2018](#bib.bib164))
    estimates the transmission map through a pyramid densely connected encoder-decoder
    network, and uses a symmetric U-Net to predict the atmospheric light $A$. In order
    to improve the edge accuracy of the transmission map, DCPDN has designed a hybrid
    edge-preserving loss, which includes L2 loss, two-directional gradient loss, and
    feature edge loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究通过完全使用CNN来实现物理参数的预测，而不是将卷积网络和统计先验联合使用来估计ASM的物理参数。DCPDN (Zhang 和 Patel, [2018](#bib.bib164))
    通过一个金字塔密集连接的编码器-解码器网络估计传输图，并使用对称U-Net来预测大气光 $A$。为了提高传输图的边缘准确性，DCPDN设计了一种混合边缘保留损失，包括L2损失、双向梯度损失和特征边缘损失。
- en: DHD-Net (Xie et al., [2020](#bib.bib151)) designs a segmentation-based haze
    density estimation algorithm, which can segment dense haze areas and divide the
    global atmosphere light $A$ candidate areas. HRGAN (Pang et al., [2018](#bib.bib105))
    utilizes a multi-scale fused dilated convolutional network to predict $t(x)$,
    and employs a single-layer convolutional model to estimate $A$. PMHLD (Chen et al.,
    [2020](#bib.bib19)) uses a patch map generator and refine the network for transmission
    map estimation, and utilizes VGG-16 for atmospheric light estimation. It is worth
    noting that if using regression training to obtain $A$, the ground truth labels
    are generally required.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DHD-Net (Xie 等, [2020](#bib.bib151)) 设计了一种基于分割的雾霾密度估计算法，能够分割密集的雾霾区域并划分全球大气光
    $A$ 的候选区域。HRGAN (Pang 等, [2018](#bib.bib105)) 利用多尺度融合的扩张卷积网络来预测 $t(x)$，并使用单层卷积模型来估计
    $A$。PMHLD (Chen 等, [2020](#bib.bib19)) 使用补丁图生成器和细化网络来估计传输图，并利用VGG-16进行大气光估计。值得注意的是，如果使用回归训练来获得
    $A$，通常需要真实标签。
- en: 3.3\. Non-explicitly Embedded ASM
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 非显式嵌入ASM
- en: ASM can be incorporated into CNN in a reformulated or embedded way. AOD-Net (Li
    et al., [2017a](#bib.bib73)) founds that the end-to-end neural network can still
    be used to solve the ASM without directly using ground truth $t(x)$ and $A$. According
    to the original ASM, the expression of $J(x)$ is
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ASM 可以以重新公式化或嵌入的方式融入CNN。AOD-Net (Li 等, [2017a](#bib.bib73)) 发现端到端的神经网络仍然可以用来解决ASM，而不直接使用真实的
    $t(x)$ 和 $A$。根据原始ASM，$J(x)$ 的表达式为
- en: '| (15) |  | $J(x)=\frac{1}{t(x)}I(x)-A\frac{1}{t(x)}+A.$ |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $J(x)=\frac{1}{t(x)}I(x)-A\frac{1}{t(x)}+A.$ |  |'
- en: AOD-Net proposes $K(x)$, which has no actual physical meaning, as an intermediate
    parameter describing $t(x)$ and $A$. $K(x)$ is defined as
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AOD-Net 提出了 $K(x)$ 作为描述 $t(x)$ 和 $A$ 的中间参数，它没有实际的物理意义。$K(x)$ 被定义为
- en: '| (16) |  | $K(x)=\frac{\frac{1}{t(x)}(I(x)-A)+(A-b)}{I(x)-1},$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $K(x)=\frac{\frac{1}{t(x)}(I(x)-A)+(A-b)}{I(x)-1},$ |  |'
- en: where $b$ equals $1$. According to the ASM theory, $J(x)$ can be uniquely determined
    by $K(x)$ as
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 等于 $1$。根据ASM理论，$J(x)$ 可以通过 $K(x)$ 唯一确定，如下所示
- en: '| (17) |  | $J(x)=K(x)I(x)-K(x)+1.$ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $J(x)=K(x)I(x)-K(x)+1.$ |  |'
- en: AOD-Net considers that the non-joint transmission map and atmospheric light
    prediction process may produce accumulated errors. Therefore, independent estimation
    of $K(x)$ can reduce the systematic error. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    extends this formulation in a multi-scale framework and utilizes fully point-wise
    convolutions to achieve fast and accurate dehazing performance. DehazeGAN (Zhu
    et al., [2018](#bib.bib192)) incorporates the idea of differentiable programming
    into the estimation process of $A$ and $t(x)$. Combined with a reformulated ASM,
    DehazeGAN also implements an end-to-end dehazing pipeline. PFDN (Dong and Pan,
    [2020](#bib.bib31)) embeds ASM in the network design and proposes a feature dehazing
    unit, which removes haze in a well-designed feature space rather in the raw image
    space. It is instructive that ASM can still help the dehazing task for non-explicit
    parameter estimation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: AOD-Net 认为非联合的传输图和大气光预测过程可能会产生累积误差。因此，独立估计 $K(x)$ 可以减少系统误差。FAMED-Net (Zhang
    和 Tao, [2020](#bib.bib171)) 在多尺度框架中扩展了这一公式，并利用全点卷积实现快速准确的去雾性能。DehazeGAN (Zhu 等,
    [2018](#bib.bib192)) 将可微分编程的思想融入 $A$ 和 $t(x)$ 的估计过程中。结合重新公式化的ASM，DehazeGAN 还实现了端到端的去雾管道。PFDN
    (Dong 和 Pan, [2020](#bib.bib31)) 将ASM嵌入网络设计中，并提出了一种特征去雾单元，该单元在精心设计的特征空间中去除雾霾，而不是在原始图像空间中。值得指出的是，ASM仍然可以帮助非显式参数估计的去雾任务。
- en: '![Refer to caption](img/8fdddfd7395438914517d66d6a263617.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8fdddfd7395438914517d66d6a263617.png)'
- en: Figure 6\. Generative adversarial network for dehazing, where gt means ground
    truth.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 生成对抗网络用于去雾，其中 gt 表示真实值。
- en: 3.4\. Generative Adversarial Networks
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 生成对抗网络
- en: 'Generative adversarial networks have an important impact on dehazing research.
    In general, supervised dehazing networks that rely on paired data can use adversarial
    loss as an auxiliary supervisory signal. Adversarial loss (Gui et al., [2022](#bib.bib47))
    can be seen as two parts: the training objective of the generator is to generate
    images that the discriminator considers to be real. The optimization purpose of
    the discriminator is to distinguish the generated image from the real image contained
    in the dataset as possible as it could. For dehazing task, the effect of the adversarial
    loss is to make the generated image closer to the real one, which is beneficial
    for the optimization of the haze-free $J(x)$ and transmission map $t(x)$ (Zhang
    et al., [2019a](#bib.bib166)), as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Non-explicitly
    Embedded ASM ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning").'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络对去雾研究有着重要影响。一般来说，依赖配对数据的监督去雾网络可以将对抗损失作为辅助监督信号。对抗损失 (Gui et al., [2022](#bib.bib47))
    可以看作是两个部分：生成器的训练目标是生成被判别器认为真实的图像。判别器的优化目标是尽可能区分生成的图像与数据集中包含的真实图像。在去雾任务中，对抗损失的作用是使生成的图像更接近真实图像，这有利于优化无雾的
    $J(x)$ 和传输图 $t(x)$ (Zhang et al., [2019a](#bib.bib166))，如图 [6](#S3.F6 "Figure
    6 ‣ 3.3\. Non-explicitly Embedded ASM ‣ 3\. Supervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") 所示。
- en: Inspired by patchGAN (Isola et al., [2017](#bib.bib61)), which can better preserve
    high-frequency information, DH-GAN (Sim et al., [2018](#bib.bib126)), RI-GAN (Dudhane
    et al., [2019](#bib.bib37)) and DehazingGAN (Zhang et al., [2020c](#bib.bib176))
    use $N\times N$ patches instead of a single value as the output of the discriminator.
    Several works explore joint training mechanisms of multiple discriminators, such
    as EPDN (Qu et al., [2019](#bib.bib110)) and PGC-UNet (Zhao et al., [2021a](#bib.bib187)).
    Discriminator $D_{1}$ is used to guide the generator on a fine scale, while discriminator
    $D_{2}$ helps the generator to produce a global realistic output on a coarse scale.
    In order to realize the joint training of the two discriminators, EPDN downsamples
    the input image of $D_{1}$ by a factor of $2$ as the input of $D_{2}$. The adversarial
    loss is
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 受 patchGAN (Isola et al., [2017](#bib.bib61)) 启发，它可以更好地保留高频信息，DH-GAN (Sim et al.,
    [2018](#bib.bib126))、RI-GAN (Dudhane et al., [2019](#bib.bib37)) 和 DehazingGAN (Zhang
    et al., [2020c](#bib.bib176)) 使用 $N\times N$ 补丁而不是单一值作为判别器的输出。一些研究探讨了多个判别器的联合训练机制，例如
    EPDN (Qu et al., [2019](#bib.bib110)) 和 PGC-UNet (Zhao et al., [2021a](#bib.bib187))。判别器
    $D_{1}$ 用于在细尺度上引导生成器，而判别器 $D_{2}$ 帮助生成器在粗尺度上生成全局逼真的输出。为了实现两个判别器的联合训练，EPDN 将 $D_{1}$
    的输入图像下采样 $2$ 倍作为 $D_{2}$ 的输入。对抗损失为
- en: '| (18) |  | $L_{adv}=\min_{G}[\max_{D_{1},D_{2}}\sum_{k=1,2}\ell_{A}(G,D_{k})],$
    |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $L_{adv}=\min_{G}[\max_{D_{1},D_{2}}\sum_{k=1,2}\ell_{A}(G,D_{k})],$
    |  |'
- en: where the form of adversarial loss $\ell_{A}$ is the same as the single GAN.
    The exploration of GAN brings plug-and-play tools to supervised algorithms. Since
    the training of the discriminator can be done in an unsupervised manner, the quality
    of the dehazed images can be improved without the requirement of extra labels.
    However, the training of GAN sometimes suffers from instability and non-convergence,
    which may bring certain additional difficulties to the training of the dehazing
    network.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失 $\ell_{A}$ 的形式与单一 GAN 相同。GAN 的探索为监督算法带来了即插即用的工具。由于判别器的训练可以以无监督的方式进行，因此去雾图像的质量可以在无需额外标签的情况下得到改善。然而，GAN
    的训练有时会遭遇不稳定性和非收敛性，这可能为去雾网络的训练带来某些额外困难。
- en: 3.5\. Level-aware
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 级别感知
- en: According to the scattering theory, the farther the scene is from the camera,
    the more aerosol particles pass through. This means that areas within a single
    hazy image that are farther from the camera have higher densities of haze. Therefore,
    LAP-Net (Li et al., [2019b](#bib.bib83)) proposes that the algorithm should consider
    the difference in haze density inside the image. Through multi-stage joint training,
    LAP-Net implements an easy-to-hard model that focuses on a specific haze density
    level by a stage-wise loss
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 根据散射理论，场景距离相机越远，通过的气溶胶颗粒越多。这意味着在单一雾霾图像中，距离相机较远的区域具有更高的雾霾密度。因此，LAP-Net (Li et
    al., [2019b](#bib.bib83)) 提出算法应考虑图像内的雾霾密度差异。通过多阶段联合训练，LAP-Net 实现了一种由易到难的模型，通过阶段性损失关注特定的雾霾密度水平。
- en: '| (19) |  | $\hat{t}^{s}(x)=\begin{cases}\mathcal{F}(I(x),\theta^{s}),\text{if
    }s=1,\\ \mathcal{F}(I(x),\theta^{s},\hat{t}^{s-1}(x)),\text{if }s>1,\end{cases}$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\hat{t}^{s}(x)=\begin{cases}\mathcal{F}(I(x),\theta^{s}),\text{if
    }s=1,\\ \mathcal{F}(I(x),\theta^{s},\hat{t}^{s-1}(x)),\text{if }s>1,\end{cases}$
    |  |'
- en: where $\mathcal{F}$ represents the transmission map prediction network with
    parameter $\theta^{s}$ in stage $s$. In the first stage, the transmission map
    prediction network is responsible for estimating the case with mild haze. In the
    second and subsequent stages, the prediction result of the previous stage and
    the hazy image are used as joint input for processing higher haze density.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}$ 表示带有参数 $\theta^{s}$ 的传输图预测网络。在第一阶段，传输图预测网络负责估计轻度雾霾的情况。在第二阶段及后续阶段，使用上一阶段的预测结果和雾霾图像作为联合输入来处理更高密度的雾霾。
- en: The density of haze may be related to conditions such as temperature, wind,
    altitude, and humidity. Thus, the formation of haze should be space-variant and
    non-homogeneous. Based on this observation, HardGAN (Deng et al., [2020](#bib.bib26))
    argues that estimating the transmission map for dehazing may be inaccurate. By
    encoding the atmospheric brightness as $1\times 1\times 2$ matrix $\gamma_{i}^{G}\&amp;\beta_{i}^{G}$
    and pixel-wise spatial information as $H\times W\times 2$ matrix $\gamma_{i}^{L}\&amp;\beta_{i}^{L}$
    for $i$-th channel of the input $x$, HardGAN designs the control function of atmospheric
    brightness $G_{i}$ and spatial information $L_{i}$ as
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 雾的密度可能与温度、风速、海拔和湿度等条件有关。因此，雾的形成应该是空间变化的且不均匀的。基于这一观察，HardGAN (Deng et al., [2020](#bib.bib26))
    认为估计去雾的传输图可能不准确。通过将大气亮度编码为 $1\times 1\times 2$ 矩阵 $\gamma_{i}^{G}\&\beta_{i}^{G}$
    和逐像素空间信息编码为 $H\times W\times 2$ 矩阵 $\gamma_{i}^{L}\&\beta_{i}^{L}$，针对输入 $x$ 的
    $i$-th 通道，HardGAN 设计了大气亮度的控制函数 $G_{i}$ 和空间信息的控制函数 $L_{i}$ 如下：
- en: '|  | $\displaystyle G_{i}=\gamma_{i}^{G}\frac{x-\mu}{\sigma}+\beta_{i}^{G},$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{i}=\gamma_{i}^{G}\frac{x-\mu}{\sigma}+\beta_{i}^{G},$
    |  |'
- en: '| (20) |  | $\displaystyle L_{i}=\gamma_{i}^{L}\frac{x-\mu}{\sigma}+\beta_{i}^{L},$
    |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle L_{i}=\gamma_{i}^{L}\frac{x-\mu}{\sigma}+\beta_{i}^{L},$
    |  |'
- en: where $\mu$ and $\sigma$ denote the mean and standard deviation for $x$, respectively.
    After obtaining $G_{i}$ and $L_{i}$, HardGAN uses a linear model to fuse them
    for recovering haze-free image
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu$ 和 $\sigma$ 分别表示 $x$ 的均值和标准差。获取 $G_{i}$ 和 $L_{i}$ 后，HardGAN 使用线性模型将它们融合以恢复无雾图像。
- en: '| (21) |  | $J_{pred_{i}}(x)=(1-HA_{i})*G_{i}+HA_{i}*L_{i},$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $J_{pred_{i}}(x)=(1-HA_{i})*G_{i}+HA_{i}*L_{i},$ |  |'
- en: where $HA$ is calculated by the intermediate feature map of HardGAN via using
    instance normalization followed by a sigmoid layer, and $*$ denotes element-wise
    product.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $HA$ 是通过 HardGAN 的中间特征图计算的，使用实例归一化后跟随 sigmoid 层，而 $*$ 表示逐元素乘积。
- en: 3.6\. Multi-function Fusion
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 多功能融合
- en: 'DMMFD (Deng et al., [2019](#bib.bib27)) designs a layer separation and fusion
    model for improving learning ability, including reformulated ASM, multiplication,
    addition, exponentiation and logarithmic decomposition:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: DMMFD (Deng et al., [2019](#bib.bib27)) 设计了一种层分离和融合模型以提高学习能力，包括重新构造的 ASM、乘法、加法、指数和对数分解：
- en: '|  | $\displaystyle J_{0}(x)=\frac{I(x)-A_{0}\times(1-t_{0}(x))}{t_{0}(x)},$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{0}(x)=\frac{I(x)-A_{0}\times(1-t_{0}(x))}{t_{0}(x)},$
    |  |'
- en: '|  | $\displaystyle J_{1}(x)=I(x)\times R_{1}(x),$ |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{1}(x)=I(x)\times R_{1}(x),$ |  |'
- en: '|  | $\displaystyle J_{2}(x)=I(x)+R_{2}(x),$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{2}(x)=I(x)+R_{2}(x),$ |  |'
- en: '|  | $\displaystyle J_{3}(x)=(I(x))^{R_{3}(x)},$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{3}(x)=(I(x))^{R_{3}(x)},$ |  |'
- en: '| (22) |  | $\displaystyle J_{4}(x)=\log(1+I(x)\times R_{4}(x)),$ |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\displaystyle J_{4}(x)=\log(1+I(x)\times R_{4}(x)),$ |  |'
- en: where $R_{i}(x)$ stands for layers in the network; $A_{0}$ and $t_{0}(x)$ are
    the atmospheric light and transmission map estimated by the feature extraction
    network, respectively. $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$, and $J_{4}{(x)}$
    can be used as four independent haze-layer separation models. It is based on the
    assumption that the input hazy image $I(x)$ can be separated into a haze-free
    layer $J(x)$ and another layer $H(x)$, denoted as $I(x)=\phi(J(x),H(x))$. The
    final dehazing result is obtained by weighted fusion of the intermediate outputs
    $J_{0}{(x)}$, $J_{1}{(x)}$, $J_{2}{(x)}$, $J_{3}{(x)}$ and $J_{4}{(x)}$ by five
    learned attention maps $W_{0}$, $W_{1}$, $W_{2}$, $W_{3}$ and $W_{4}$ as
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R_{i}(x)$ 代表网络中的层；$A_{0}$ 和 $t_{0}(x)$ 分别是特征提取网络估计的大气光和传输图。$J_{1}{(x)}$、$J_{2}{(x)}$、$J_{3}{(x)}$
    和 $J_{4}{(x)}$ 可以作为四个独立的雾霾层分离模型。该模型基于这样的假设：输入的雾霾图像 $I(x)$ 可以被分离为无雾层 $J(x)$ 和另一层
    $H(x)$，表示为 $I(x)=\phi(J(x),H(x))$。最终的去雾结果通过五个学习到的注意力图 $W_{0}$、$W_{1}$、$W_{2}$、$W_{3}$
    和 $W_{4}$ 对中间输出 $J_{0}{(x)}$、$J_{1}{(x)}$、$J_{2}{(x)}$、$J_{3}{(x)}$ 和 $J_{4}{(x)}$
    进行加权融合得到。
- en: '| (23) |  | $\displaystyle J_{pred}(x)=$ | $\displaystyle W_{0}\times J_{0}{(x)}+W_{1}\times
    J_{1}{(x)}+W_{2}\times J_{2}{(x)}+W_{3}\times J_{3}{(x)}+W_{4}\times J_{4}{(x)}.$
    |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\displaystyle J_{pred}(x)=$ | $\displaystyle W_{0}\times J_{0}{(x)}+W_{1}\times
    J_{1}{(x)}+W_{2}\times J_{2}{(x)}+W_{3}\times J_{3}{(x)}+W_{4}\times J_{4}{(x)}.$
    |  |'
- en: Through ablation studies, DMMFD has demonstrated that the fusion of multiple
    layers can improve the quality of the scene restoration process.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消融研究，DMMFD 已经证明，多个层的融合可以改善场景恢复过程的质量。
- en: 3.7\. Transformation and Decomposition of Input
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 输入的变换和分解
- en: 'GFN (Ren et al., [2018a](#bib.bib112)) proposes two observations on the influence
    of haze. First, under the influence of atmospheric light, the color of a hazy
    image may be distorted to some extent. Second, due to the existence of scattering
    and attenuation phenomena, the visibility of objects far away from the camera
    in the scene will be reduced. Therefore, GFN uses three enhancement strategies
    to process the original hazy image and use them as inputs to the dehazing network
    together. The white balanced input $I_{wb}(x)$ is obtained from the gray world
    assumption. The contrast enhanced input $I_{ce}(x)$ is composed of average luminance
    value $\widetilde{I}(x)$ and the control factor $\mu$, as following:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: GFN (Ren et al., [2018a](#bib.bib112)) 提出了雾霾影响的两个观察。首先，在大气光的影响下，雾霾图像的颜色可能会有一定程度的扭曲。其次，由于存在散射和衰减现象，远离相机的场景中的物体可见性会降低。因此，GFN
    使用三种增强策略来处理原始雾霾图像，并将其作为输入与去雾网络一起使用。白平衡输入 $I_{wb}(x)$ 是从灰世界假设中获得的。对比度增强输入 $I_{ce}(x)$
    由平均亮度值 $\widetilde{I}(x)$ 和控制因子 $\mu$ 组成，如下所示：
- en: '| (24) |  | $I_{ce}{(x)}=\mu(I(x)-\widetilde{I}{(x)}),$ |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $I_{ce}{(x)}=\mu(I(x)-\widetilde{I}{(x)}),$ |  |'
- en: where $\mu=2\cdot(0.5+\widetilde{I}(x))$. By using the the nonlinear gamma correction,
    the $I_{gc}{(x)}$ used to enhance the visibility of the $I(x)$ can be obtained
    by
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu=2\cdot(0.5+\widetilde{I}(x))$。通过非线性伽玛校正，可以获得用于增强 $I(x)$ 可见性的 $I_{gc}(x)$，其计算公式为：
- en: '| (25) |  | $I_{gc}{(x)}=\alpha{I(x)^{\gamma}},$ |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $I_{gc}{(x)}=\alpha{I(x)^{\gamma}},$ |  |'
- en: 'where $\alpha=1$, and $\gamma=2.5$. The final dehazing image $J(x)$ is determined
    by the combination of three inputs, where $C_{wb}$, $C_{ce}$ and $C_{gc}$ are
    confidences map for fusion process:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha=1$，$\gamma=2.5$。最终的去雾图像 $J(x)$ 由三个输入的组合决定，其中 $C_{wb}$、$C_{ce}$ 和
    $C_{gc}$ 是融合过程的置信度图：
- en: '| (26) |  | $J(x)=C_{wb}\circ{I_{wb}{(x)}}+C_{ce}\circ{I_{ce}{(x)}}+C_{gc}\circ{I_{gc}{(x)}}.$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $J(x)=C_{wb}\circ{I_{wb}{(x)}}+C_{ce}\circ{I_{ce}{(x)}}+C_{gc}\circ{I_{gc}{(x)}}.$
    |  |'
- en: 'MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)) decomposes the hazy image
    into low frequency and high frequency as the base component $I_{base}{(x)}$ and
    the detail component $I_{detail}{(x)}$, respectively. The basic component can
    be thought as the main content of the image, while the high frequency component
    denotes the edge and texture. Therefore, the dehazed image can be obtained by
    the dehazing function $D(\cdot)$ of the basic component and the enhancement function
    $E(\cdot)$ of the detail component. The whole process can be represented by a
    linear model as following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: MSRL-DehazeNet (Yeh et al., [2019](#bib.bib158)) 将雾霾图像分解为低频和高频，分别作为基础组件 $I_{base}{(x)}$
    和细节组件 $I_{detail}{(x)}$。基本组件可以被看作图像的主要内容，而高频组件则表示边缘和纹理。因此，通过基础组件的去雾函数 $D(\cdot)$
    和细节组件的增强函数 $E(\cdot)$ 可以获得去雾图像。整个过程可以通过线性模型表示，如下所示：
- en: '|  | $\displaystyle I(x)=I_{base}{(x)}+I_{detail}{(x)},$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(x)=I_{base}{(x)}+I_{detail}{(x)},$ |  |'
- en: '| (27) |  | $\displaystyle J(x)=D(I_{base}{(x)})+E(I_{detail}{(x)}).$ |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\displaystyle J(x)=D(I_{base}{(x)})+E(I_{detail}{(x)}).$ |  |'
- en: DIDH (Shyam et al., [2021](#bib.bib124)) decomposes both the input hazy image
    $I(x)$ and the predicted haze-free image $J(x)$ to obtain $(I(x),LF(I(x)))$, $(I(x),HF(I(x)))$,
    $(J(x),LF(J(x)))$ and $J(x),HF(J(x))$, where $LF(\cdot)$ and $HF(\cdot)$ denotes
    Gaussian and Laplacian filter, respectively. By fusing the decomposed data with
    the pre-decomposition data as the input of the discriminator, DIDH can improve
    the quality of the image generated by the adversarial training process. With the
    help of the discriminators $D_{LF}$ and $D_{HF}$, the dehazing network can be
    optimized in an adversarial way.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: DIDH (Shyam 等，[2021](#bib.bib124)) 将输入的模糊图像 $I(x)$ 和预测的无雾图像 $J(x)$ 分解为 $(I(x),LF(I(x)))$、$(I(x),HF(I(x)))$、$(J(x),LF(J(x)))$
    和 $J(x),HF(J(x))$，其中 $LF(\cdot)$ 和 $HF(\cdot)$ 分别表示高斯滤波器和拉普拉斯滤波器。通过将分解后的数据与预分解数据融合作为判别器的输入，DIDH
    可以提高对抗训练过程中生成图像的质量。在判别器 $D_{LF}$ 和 $D_{HF}$ 的帮助下，去雾网络可以以对抗方式进行优化。
- en: Compared with conventional data augmentation, such as rotation, horizontal or
    vertical mirror symmetry, and random cropping, the transformation and decomposition
    of the input is a more efficient strategy for the usage of hazy images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的数据增强方法（如旋转、水平或垂直镜像对称、随机裁剪）相比，输入图像的变换和分解是一种更高效的模糊图像利用策略。
- en: '![Refer to caption](img/a81a177c411c6c06da3da995439ee4cd.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a81a177c411c6c06da3da995439ee4cd.png)'
- en: Figure 7\. A general knowledge distillation strategy used in KDDN (Hong et al.,
    [2020](#bib.bib55)), KTDN (Wu et al., [2020](#bib.bib148)), SRKTDN (Chen et al.,
    [2021a](#bib.bib17)), etc.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 一般知识蒸馏策略，应用于 KDDN (Hong 等，[2020](#bib.bib55))、KTDN (Wu 等，[2020](#bib.bib148))、SRKTDN
    (Chen 等，[2021a](#bib.bib17)) 等。
- en: 3.8\. Knowledge Distillation
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8\. 知识蒸馏
- en: Knowledge distillation (Gou et al., [2021](#bib.bib45)) provides a strategy
    to transfer the knowledge learned by the teacher network to the student network,
    which has been applied in high-level computer vision tasks like object detection
    and image classification (Wang et al., [2019](#bib.bib137)). Recent work (Hong
    et al., [2020](#bib.bib55)) presents three challenges for applying knowledge distillation
    to the dehazing task. First, what kind of teacher task can help the dehazing task.
    Second, how the teacher network helps the dehazing network during training. Third,
    which similarity measure between teacher task and student task should be chosen.
    Fig. [7](#S3.F7 "Figure 7 ‣ 3.7\. Transformation and Decomposition of Input ‣
    3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") shows the knowledge distillation strategy adopted
    by various dehazing algorithms. Different methods may use different numbers and
    locations of output features to compute feature loss.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏 (Gou 等，[2021](#bib.bib45)) 提供了一种将教师网络学到的知识转移到学生网络的策略，这已经应用于高层次计算机视觉任务，如目标检测和图像分类
    (Wang 等，[2019](#bib.bib137))。最近的工作 (Hong 等，[2020](#bib.bib55)) 提出了将知识蒸馏应用于去雾任务的三个挑战。首先，什么样的教师任务可以帮助去雾任务。第二，教师网络如何在训练过程中帮助去雾网络。第三，应该选择哪种教师任务与学生任务之间的相似度度量。图
    [7](#S3.F7 "图 7 ‣ 3.7\. 输入的变换和分解 ‣ 3\. 监督去雾 ‣ 基于深度学习的单图像去雾的全面调查和分类") 显示了各种去雾算法采用的知识蒸馏策略。不同的方法可能使用不同数量和位置的输出特征来计算特征损失。
- en: KDDN (Hong et al., [2020](#bib.bib55)) designs a process-oriented learning mechanism,
    where the teacher network $T$ is an auto-encoder for high-quality haze-free image
    reconstruction. When training the dehazing network, the teacher network assists
    in feature learning, and optimizes $L_{T}=||J(x)-T(J(x))||_{1}$. Therefore, the
    teacher task and the student task proposed by KDDN are two different tasks. In
    order to make full use of the feature information learned by the teacher network
    and help the training of the dehazing network. KDDN uses feature matching loss
    and haze density aware loss by a linear transformation function $g$, which are
    represented by ([28](#S3.E28 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")) and ([29](#S3.E29 "In 3.8\. Knowledge Distillation ‣ 3\. Supervised
    Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based
    on Deep Learning")), respectively.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: KDDN（Hong 等，[2020](#bib.bib55)）设计了一种面向过程的学习机制，其中教师网络 $T$ 是用于高质量无雾图像重建的自编码器。在训练去雾网络时，教师网络协助特征学习，并优化
    $L_{T}=||J(x)-T(J(x))||_{1}$。因此，KDDN 提出的教师任务和学生任务是两个不同的任务。为了充分利用教师网络学习到的特征信息并帮助去雾网络的训练，KDDN
    使用通过线性变换函数 $g$ 表示的特征匹配损失和雾密度感知损失，这些分别由 ([28](#S3.E28 "In 3.8\. Knowledge Distillation
    ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")) 和 ([29](#S3.E29 "In 3.8\. Knowledge Distillation
    ‣ 3\. Supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")) 表示。
- en: '| (28) |  | $L_{rm}=\sum_{(m,n)\in{C}}&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;,$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $L_{rm}=\sum_{(m,n)\in{C}}|T^{m}(J(x))-g(S^{n}(I(x)))|,$ |  |'
- en: '| (29) |  | $L_{wrm}=\sum_{(m,n)\in{C}}\psi\times{&#124;T^{m}(J(x))-g(S^{n}(I(x)))&#124;},$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $L_{wrm}=\sum_{(m,n)\in{C}}\psi\times{|T^{m}(J(x))-g(S^{n}(I(x)))|},$
    |  |'
- en: where $T^{m}$ represents the $m$-th layer of the teacher network, and the corresponding
    $S^{n}$ represents the $n$-th layer of the student network; $\psi$ is obtained
    by the normalization operation. KDDN can be trained without real transmission
    map, replaced by the residual between hazy and haze-free images.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T^{m}$ 表示教师网络的第 $m$ 层，相应的 $S^{n}$ 表示学生网络的第 $n$ 层；$\psi$ 是通过归一化操作获得的。KDDN
    可以在没有真实传输图的情况下进行训练，用雾图和无雾图之间的残差替代。
- en: KTDN (Wu et al., [2020](#bib.bib148)) is jointly trained using a teacher network
    and a dehazing network with the same structure. Through feature level loss, the
    prior knowledge possessed by the teacher network can be transferred to the dehazing
    network. SRKTDN (Chen et al., [2021a](#bib.bib17)) uses ResNet18 pre-trained on
    ImageNet (with classification layers removed) as the teacher network, transferring
    many statistical experiences to the Res2Net101 encoder for dehazing. DALF (Fang
    et al., [2021](#bib.bib39)) integrates dual adversarial training into the training
    process of knowledge distillation to improve the imitation ability of the student
    network to the teacher network. Applying knowledge distillation to dehazing networks
    provides a new and efficient way to introduce external prior knowledge.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: KTDN（Wu 等，[2020](#bib.bib148)）使用具有相同结构的教师网络和去雾网络进行联合训练。通过特征层级的损失，教师网络所拥有的先验知识可以转移到去雾网络中。SRKTDN（Chen
    等，[2021a](#bib.bib17)）使用在 ImageNet 上预训练的 ResNet18（去掉分类层）作为教师网络，将许多统计经验转移到 Res2Net101
    编码器中以进行去雾。DALF（Fang 等，[2021](#bib.bib39)）将双重对抗训练整合到知识蒸馏的训练过程中，以提高学生网络对教师网络的模仿能力。将知识蒸馏应用于去雾网络提供了一种引入外部先验知识的新方法和高效途径。
- en: 3.9\. Transformation of Colorspace
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9\. 颜色空间的转换
- en: The input data of the dehazing network are usually three channel color images
    in RGB mode. By calculating the mean square error of hazy and haze-free images
    in RGB space and YCrCb space on the Dense-Haze dataset, Bianco et al. (Bianco
    et al., [2019](#bib.bib12)) find that haze shows obvious numerical differences
    in the two spaces. The error values for the red, green and blue channels in RGB
    space are very close. However, in the YCrCb space, the error value of the luminance
    channel is significantly larger than that of the blue and red chroma components.
    AIP-Net (Wang et al., [2018a](#bib.bib133)) performs a similar error comparison
    of color space transformation on synthetic datasets and obtained the same conclusion
    as  (Bianco et al., [2019](#bib.bib12)). Quantitative results (Wang et al., [2018a](#bib.bib133);
    Bianco et al., [2019](#bib.bib12)) obtained by training the dehazing network in
    the YCrCb color space show that RGB space is not the only effective color mode
    for deep learning based dehazing methods. Furthermore, TheiaNet (Mehra et al.,
    [2021](#bib.bib95)) comprehensively analyzes the performance obtained by training
    the dehazing model in RGB, YCrCb, HSV and LAB color spaces. Experiments (Singh
    et al., [2020](#bib.bib128); Sheng et al., [2022](#bib.bib122); Chen et al., [2021a](#bib.bib17);
    Dudhane and Murala, [2019b](#bib.bib36)) show that converting images from RGB
    space to other color spaces for model training is an effective scheme.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 去雾网络的输入数据通常是RGB模式的三通道彩色图像。通过计算Dense-Haze数据集中RGB空间和YCrCb空间中雾霾图像和无雾图像的均方误差，Bianco等（Bianco
    et al., [2019](#bib.bib12)）发现雾霾在这两个空间中显示出明显的数值差异。RGB空间中红、绿、蓝通道的误差值非常接近。然而，在YCrCb空间中，亮度通道的误差值显著大于蓝色和红色色度分量的误差值。AIP-Net（Wang
    et al., [2018a](#bib.bib133)）对合成数据集中的色彩空间转换进行了类似的误差比较，并得出了与（Bianco et al., [2019](#bib.bib12)）相同的结论。通过在YCrCb色彩空间中训练去雾网络获得的定量结果（Wang
    et al., [2018a](#bib.bib133); Bianco et al., [2019](#bib.bib12)）显示，RGB空间并不是基于深度学习的去雾方法中唯一有效的颜色模式。此外，TheiaNet（Mehra
    et al., [2021](#bib.bib95)）综合分析了在RGB、YCrCb、HSV和LAB色彩空间中训练去雾模型所获得的性能。实验（Singh et
    al., [2020](#bib.bib128); Sheng et al., [2022](#bib.bib122); Chen et al., [2021a](#bib.bib17);
    Dudhane和Murala，[2019b](#bib.bib36)）表明，将图像从RGB空间转换到其他色彩空间进行模型训练是一种有效的方案。
- en: 3.10\. Contrastive Learning
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10\. 对比学习
- en: 'In the process of training a non-ASM-based supervised dehazing network, a common
    way is to use the hazy image as the input of the network and expect to obtain
    a clear image. In this process, the clear image is used as a positive example
    to guide the optimization of the network. By designing pairs of positive and negative
    examples, AECR-Net provides a new perspective for treating hazy and haze-free
    images. Specifically, the clear image $J(x)$ and the dehazed image $J_{pred}(x)$
    are taken as a positive sample pair, while the hazy image $I(x)$ and the dehazed
    image $J_{pred}{(x)}$ are taken as a negative sample pair. According to contrastive
    learning (Khosla et al., [2020](#bib.bib66)), $J_{pred}(x)$, $J(x)$ and $I(x)$
    can be regarded as anchor, positive and negative, respectively. For the pre-trained
    model $G(\cdot)$, the dehazing loss can be regarded as the sum of the reconstruction
    loss and the regularization term, as following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练基于非ASM的监督去雾网络的过程中，一种常见的方法是将雾霾图像作为网络的输入，期望得到清晰的图像。在这个过程中，清晰的图像作为正样本来指导网络的优化。通过设计正负样本对，AECR-Net为处理有雾和无雾图像提供了新的视角。具体来说，清晰图像$J(x)$和去雾图像$J_{pred}(x)$被视为一对正样本，而雾霾图像$I(x)$和去雾图像$J_{pred}{(x)}$被视为一对负样本。根据对比学习（Khosla等，[2020](#bib.bib66)），$J_{pred}(x)$、$J(x)$和$I(x)$可以分别视为锚点、正样本和负样本。对于预训练模型$G(\cdot)$，去雾损失可以看作是重建损失和正则化项的总和，如下所示：
- en: '| (30) |  | $\min{&#124;&#124;J(x)-\psi(I(x))&#124;&#124;_{1}}+\lambda\sum_{i=1}^{N}{\omega_{i}}\cdot\frac{&#124;&#124;G_{i}(J(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}}{&#124;&#124;G_{i}(I(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}},$
    |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\min{&#124;&#124;J(x)-\psi(I(x))&#124;&#124;_{1}}+\lambda\sum_{i=1}^{N}{\omega_{i}}\cdot\frac{&#124;&#124;G_{i}(J(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}}{&#124;&#124;G_{i}(I(x)),G_{i}(\phi(I(x)))&#124;&#124;_{1}},$
    |  |'
- en: where $G_{i}$ represents the output feature of the $i$-th layer of the pre-trained
    model; $\lambda$ is the weight ratio between the image reconstruction loss and
    the regularization term; $w_{i}$ is weight factor for feature output; $\phi$ is
    the dehazing network. AECR-Net provides a universal contrastive regularization
    strategy for existing methods without adding extra parameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{i}$ 代表预训练模型第 $i$ 层的输出特征；$\lambda$ 是图像重建损失和正则化项之间的权重比例；$w_{i}$ 是特征输出的权重因子；$\phi$
    是去雾网络。AECR-Net 为现有方法提供了一种通用的对比正则化策略，而无需增加额外的参数。
- en: 3.11\. Non-deterministic Output
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.11\. 非确定性输出
- en: 'Dehazing methods based on deep learning usually set the optimization objective
    to obtain a single dehazed image. By introducing 2-dimensional latent tensors,
    pWAE (Kim et al., [2021](#bib.bib67)) can generate different styles of dehazed
    images, which extends the general training purpose. pWAE proposes dehazing latent
    space $z_{h}$ and style latent space $z_{s}$ for dehazing, and applies the mean
    function $\mu(\cdot)$ and standard deviation function $\sigma(\cdot)$ to perform
    the transformation of the space:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的去雾方法通常将优化目标设定为获得单一的去雾图像。通过引入二维潜在张量，pWAE（Kim 等，[2021](#bib.bib67)）可以生成不同风格的去雾图像，从而扩展了通用训练目的。pWAE
    提出了去雾潜在空间 $z_{h}$ 和风格潜在空间 $z_{s}$，并应用均值函数 $\mu(\cdot)$ 和标准差函数 $\sigma(\cdot)$
    来执行空间转换：
- en: '| (31) |  | $z_{h\to{s}}=\sigma(z_{s})(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})})+\mu{(z_{s})}.$
    |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $z_{h\to{s}}=\sigma(z_{s})(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})})+\mu{(z_{s})}.$
    |  |'
- en: A natural question is how to adjust the magnitude of the spatial mapping to
    control the degree of style transformation. pWAE uses linear module $z_{h}^{s}=\alpha{z_{h\to{s}}}+(1-\alpha)z_{h}$
    to adjust the weight between the dehazed image and the style information. By controlling
    $\alpha$, different degrees of stylized dehazed images corresponding to the style
    image can be obtained.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是如何调整空间映射的幅度以控制风格转换的程度。pWAE 使用线性模块 $z_{h}^{s}=\alpha{z_{h\to{s}}}+(1-\alpha)z_{h}$
    来调整去雾图像与风格信息之间的权重。通过控制 $\alpha$，可以获得与风格图像对应的不同程度的风格化去雾图像。
- en: DehazeFlow (Li et al., [2021b](#bib.bib77)) proposes that the dehazing task
    itself is ill-posedness, so it is unreliable to learn a model with deterministic
    one-to-one mapping. With a conditional normalizing flow network, DehazeFlow can
    compute the conditional distribution of clear images from a given hazy image.
    Therefore, it can learn a one-to-many mapping and obtain different dehazing results
    in the inference stage. The non-deterministic output (Kim et al., [2021](#bib.bib67);
    Li et al., [2021b](#bib.bib77)) brings interpretable flexibility to the dehazing
    algorithm. Since the visual evaluation criteria of individual human beings are
    inherently different, one-to-many dehazing mapping provides more options for photographic
    work.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: DehazeFlow（Li 等，[2021b](#bib.bib77)）提出去雾任务本身是一个病态问题，因此学习一个确定性的一对一映射是不可靠的。通过条件归一化流网络，DehazeFlow
    可以从给定的有雾图像中计算清晰图像的条件分布。因此，它可以学习一对多的映射，并在推理阶段获得不同的去雾结果。非确定性输出（Kim 等，[2021](#bib.bib67)；Li
    等，[2021b](#bib.bib77)）为去雾算法带来了可解释的灵活性。由于个体人类的视觉评估标准本质上不同，一对多的去雾映射为摄影工作提供了更多的选择。
- en: 3.12\. Retinex Model
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.12\. Retinex 模型
- en: 'Non-deep learning based research (Galdran et al., [2018](#bib.bib41)) demonstrates
    the duality between Retinex and image dehazing. Apart from the already widely
    used ASM, recent work (Li et al., [2021c](#bib.bib80)) explores the combination
    of Retinex model and CNN for dehazing. Retinex theory proposes that the image
    can be regarded as the element-wise product of the reflectance image $R$ and the
    illumination map $L$. Assuming the reflectance $R$ is illumination invariant,
    the relationship between hazy and haze-free image can be modeled by a retinex-based
    decomposition model as following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 非深度学习研究（Galdran 等，[2018](#bib.bib41)）展示了 Retinex 和图像去雾之间的对偶性。除了已经广泛使用的 ASM，最近的工作（Li
    等，[2021c](#bib.bib80)）探索了 Retinex 模型与 CNN 结合进行去雾的可能性。Retinex 理论提出图像可以被视为反射图像 $R$
    和照明图 $L$ 的逐元素乘积。假设反射图像 $R$ 对照明不变，那么有雾图像和无雾图像之间的关系可以通过基于 Retinex 的分解模型进行建模，如下所示：
- en: '| (32) |  | $I(x)=J(x)\ast\frac{L_{I}}{L_{J}}=J(x)\ast L_{r},$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $I(x)=J(x)\ast\frac{L_{I}}{L_{J}}=J(x)\ast L_{r},$ |  |'
- en: where $*$ means the multiplication in an element-wise way. $L_{r}$ can be seen
    as absorbed and scattered light caused by haze, which is determined by the ratio
    of the hazy image illumination map $L_{I}$ and natural illumination map $L_{J}$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $*$ 表示逐元素的乘法。$L_{r}$ 可以被看作是由雾霾引起的吸收和散射光，这由有雾图像的光照图 $L_{I}$ 和自然光照图 $L_{J}$
    的比率决定。
- en: Compared with ASM, which has been proven to be reliable, Retinex has a more
    compact physical form and fewer parameters to be estimated, but is not widely
    combined with CNN for dehazing, yet.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与已经被证明可靠的ASM相比，Retinex具有更紧凑的物理形态和更少的参数需要估计，但尚未广泛与CNN结合用于去雾。
- en: 3.13\. Residual Learning
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.13\. 残差学习
- en: 'Rather than directly learning the mapping from hazy to haze-free image, several
    methods argue that the residual learning method can reduce the learning difficulty
    of the network. The dehazing methods based on residual learning is performed at
    the image level. GCANet (Chen et al., [2019c](#bib.bib14)) uses the residual {$J(x)-I(x)$}
    between haze-free and hazy images as the optimization objective. DRL (Du and Li,
    [2018](#bib.bib33)), SID-HL (Xiao et al., [2020](#bib.bib150)) and POGAN (Du and
    Li, [2019](#bib.bib34)) believe that the way of residual learning is related to
    ASM, and the relationship can be obtained by reformulated ASM:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与其直接学习从有雾到无雾图像的映射，几种方法认为残差学习方法可以减少网络的学习难度。基于残差学习的去雾方法在图像层面上进行。GCANet (Chen et
    al., [2019c](#bib.bib14)) 使用去雾图像和有雾图像之间的残差 {$J(x)-I(x)$} 作为优化目标。DRL (Du和Li, [2018](#bib.bib33))、SID-HL
    (Xiao et al., [2020](#bib.bib150)) 和POGAN (Du和Li, [2019](#bib.bib34)) 认为残差学习的方法与ASM相关，且可以通过重新表述ASM获得这种关系：
- en: '| (33) |  | $\displaystyle I(x)$ | $\displaystyle=J(x)+(A-J(x))(1-t(x))=J(x)+r(x),$
    |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle I(x)$ | $\displaystyle=J(x)+(A-J(x))(1-t(x))=J(x)+r(x),$
    |  |'
- en: where $r(x)=(A-J(x))(1-t(x))$ can be interpreted as a structural error term.
    By predicting nonlinear signal-dependent degradation $r(x)$, a clear image $J(x)=I(x)-r(x)$
    can be obtained.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r(x)=(A-J(x))(1-t(x))$ 可以解释为结构误差项。通过预测非线性信号依赖的退化 $r(x)$，可以得到清晰图像 $J(x)=I(x)-r(x)$。
- en: 3.14\. Frequency Domain
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.14\. 频率域
- en: The dehazing methods based on CNN usually use down-sampling and upsampling for
    feature extraction and clear image reconstruction, and this process less considers
    the frequency information contained in the image. There are currently two approaches
    to combine frequency analysis and dehazing networks. The first is to embed the
    frequency decomposition function into the convolutional network, and the second
    is to use frequency decomposition as a constraint for loss computation. For a
    given image, one low-frequency component and three high-frequency components can
    be obtained by wavelet decomposition. Wavelet U-net  (Yang and Fu, [2019](#bib.bib156))
    uses discrete wavelet transform (DWT) and inverse discrete wavelet transform (IDWT)
    to facilitate high quality restoration of edge information in a clear image. Through
    the 1D scaling function $\phi(\cdot)$ and the wavelet function $\psi(\cdot)$,
    the 2D wavelets low-low (LL), low-high (LH), high-low (HL) and high-high (HH)
    are calculated as follows
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的去雾方法通常使用下采样和上采样进行特征提取和清晰图像重建，这一过程较少考虑图像中的频率信息。目前有两种方法将频率分析与去雾网络结合。第一种是将频率分解函数嵌入卷积网络中，第二种是将频率分解作为损失计算的约束。对于给定图像，通过小波分解可以得到一个低频分量和三个高频分量。Wavelet
    U-net (Yang和Fu, [2019](#bib.bib156)) 使用离散小波变换（DWT）和逆离散小波变换（IDWT）以便高质量恢复清晰图像中的边缘信息。通过1D尺度函数$\phi(\cdot)$和小波函数$\psi(\cdot)$，计算二维小波低-低（LL）、低-高（LH）、高-低（HL）和高-高（HH）如下
- en: '|  | $\displaystyle\Phi_{LL}(m,n)=\phi(m)\phi(n),$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Phi_{LL}(m,n)=\phi(m)\phi(n),$ |  |'
- en: '|  | $\displaystyle\Psi_{LH}(m,n)=\phi(m)\psi(n),$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi_{LH}(m,n)=\phi(m)\psi(n),$ |  |'
- en: '|  | $\displaystyle\Psi_{HL}(m,n)=\psi(m)\phi(n),$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi_{HL}(m,n)=\psi(m)\phi(n),$ |  |'
- en: '| (34) |  | $\displaystyle\Psi_{HH}(m,n)=\psi(m)\psi(n).$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\Psi_{HH}(m,n)=\psi(m)\psi(n).$ |  |'
- en: where $m$ and $n$ represent the horizontal and vertical coordinates, respectively.
    MsGWN (Dong et al., [2020c](#bib.bib30)) uses Gabor wavelet decomposition for
    feature extraction and reconstruction. By setting different degree values, the
    feature extraction module of MsGWN can obtain frequency information in different
    directions. EMRA-Net (Wang et al., [2021d](#bib.bib136)) uses Haar wavelet decomposition
    as a downsampling operation instead of nearest downsampling and strided-convolution
    to avoid the loss of image texture details. By embedding wavelet analysis into
    convolutional network or loss function, recent studies (Yang et al., [2020](#bib.bib157);
    Dharejo et al., [2021](#bib.bib28); Fu et al., [2021](#bib.bib40)) have shown
    that 2D wavelet transform can improve the recovery of high-frequency information
    in the wavelet domain. These works successfully apply wavelet theory and neural
    networks to dehazing tasks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别表示水平和垂直坐标。MsGWN (Dong et al., [2020c](#bib.bib30)) 使用Gabor小波分解进行特征提取和重建。通过设置不同的角度值，MsGWN
    的特征提取模块可以获得不同方向的频率信息。EMRA-Net (Wang et al., [2021d](#bib.bib136)) 使用Haar小波分解作为下采样操作，而不是最近邻下采样和步幅卷积，以避免图像纹理细节的丢失。通过将小波分析嵌入卷积网络或损失函数，近期研究
    (Yang et al., [2020](#bib.bib157); Dharejo et al., [2021](#bib.bib28); Fu et al.,
    [2021](#bib.bib40)) 已经显示2D小波变换可以提高小波域高频信息的恢复。这些工作成功地将小波理论和神经网络应用于去雾任务。
- en: Besides, TDN (Liu et al., [2020b](#bib.bib86)) introduces the Fast Fourier transform
    (FFT) loss as a constraint in the frequency domain. With the help of supervised
    training on amplitude and phase, the visual perceptual quality of images is improved
    without any additional computation in the inference stage.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TDN (Liu et al., [2020b](#bib.bib86)) 在频域引入了快速傅里叶变换（FFT）损失作为约束。借助于对幅度和相位的监督训练，图像的视觉感知质量得到了提升，而推理阶段没有任何额外的计算。
- en: 3.15\. Joint Dehazing and Depth Estimation
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.15\. 联合去雾与深度估计
- en: 'The scattering particles in the hazy environment will affect the accuracy of
    the depth information collected by the LiDAR equipment. S2DNet (Hambarde and Murala,
    [2020](#bib.bib51)) proves that high-quality depth estimation algorithms can help
    the dehazing task. According to ASM, it can be known that the transmission map
    $t(x)$ and the depth map $d(x)$ have a negative exponential relationship $t(x)=e^{-\beta{d(x)}}$.
    Based on this physical dependency, SDDE (Lee et al., [2020a](#bib.bib69)) uses
    four decoders to integrate estimates of atmospheric light, clear image, transmission
    map, and depth map into an end-to-end training pipeline. In particular, SDDE proposes
    a depth-transmission consistency loss based on the observation that the standard
    deviation value ($std$) of the transmission map and the depth map pair should
    tend to zero:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在雾霾环境中的散射颗粒会影响LiDAR设备收集的深度信息的准确性。S2DNet (Hambarde and Murala, [2020](#bib.bib51))
    证明了高质量的深度估计算法可以帮助去雾任务。根据ASM，可以知道传输图 $t(x)$ 和深度图 $d(x)$ 具有负指数关系 $t(x)=e^{-\beta{d(x)}}$。基于这种物理依赖性，SDDE
    (Lee et al., [2020a](#bib.bib69)) 使用四个解码器将大气光、清晰图像、传输图和深度图的估计集成到端到端的训练流程中。特别地，SDDE
    提出了基于观察的深度-传输一致性损失，标准偏差值 ($std$) 的传输图和深度图对应该趋近于零：
- en: '| (35) |  | $L=&#124;&#124;std(\ln(t_{pred}{(x)}/d_{pred}{(x)}))&#124;&#124;_{2},$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $L=&#124;&#124;std(\ln(t_{pred}{(x)}/d_{pred}{(x)}))&#124;&#124;_{2},$
    |  |'
- en: where $t_{pred}{(x)}$ and $d_{pred}{(x)}$ represent the predicted transmission
    map and depth map, respectively. Aiming at better dehazing, TSDCN-Net  (Cheng
    and Zhao, [2021](#bib.bib23)) designs a cascaded network for two-stage methods
    with depth information prediction. Quantitative experimental results (Yang and
    Zhang, [2022](#bib.bib155)) show that this joint estimation method can improve
    the accuracy of dehazing task.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{pred}{(x)}$ 和 $d_{pred}{(x)}$ 分别表示预测的传输图和深度图。为了更好的去雾，TSDCN-Net (Cheng
    and Zhao, [2021](#bib.bib23)) 设计了一个级联网络用于具有深度信息预测的两阶段方法。定量实验结果 (Yang and Zhang,
    [2022](#bib.bib155)) 显示，这种联合估计方法可以提高去雾任务的准确性。
- en: 3.16\. Segmentation and Detection with Dehazing
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.16\. 分割与去雾检测
- en: Existing experiments (Li et al., [2017a](#bib.bib73)) show that the existence
    of image haze may cause various problems in detection algorithms, such as missing
    targets, inaccurate localizations and unconfident category prediction. Recent
    work (Sakaridis et al., [2018](#bib.bib118)) has shown that haze also brings difficulties
    to semantic scene understanding. As a preprocessing module for high-level computer
    vision tasks, the dehazing process is usually separated from high-level computer
    vision tasks.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现有实验（Li et al., [2017a](#bib.bib73)）表明，图像雾霾的存在可能导致检测算法出现各种问题，如目标遗漏、不准确的定位和不自信的类别预测。最近的工作（Sakaridis
    et al., [2018](#bib.bib118)）显示，雾霾还给语义场景理解带来了困难。作为高层计算机视觉任务的预处理模块，去雾过程通常与高层计算机视觉任务分开。
- en: Li et al. (Li et al., [2017a](#bib.bib73)) jointly optimize the object detection
    algorithm and AOD-Net, and the result proves that the dehazing algorithm can promote
    the detection task. LEAAL (Li et al., [2020c](#bib.bib82)) proposes that fine-tuning
    the parameters of the object detector during joint training may lead to a detector
    biased towards the haze-free images generated by the pretrained dehazing network.
    Different from the fine-tuning operation of Li et al. (Li et al., [2017a](#bib.bib73)),
    LEAAL uses object detection as an auxiliary task for dehazing and the parameters
    of the object detector are not fully updated during the training process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人（Li et al., [2017a](#bib.bib73)）联合优化了目标检测算法和 AOD-Net，结果证明去雾算法可以促进检测任务。LEAAL（Li
    et al., [2020c](#bib.bib82)）提出，在联合训练过程中微调目标检测器的参数可能导致检测器偏向于由预训练去雾网络生成的无雾图像。不同于
    Li 等人（Li et al., [2017a](#bib.bib73)）的微调操作，LEAAL 将目标检测作为去雾的辅助任务，在训练过程中目标检测器的参数不会被完全更新。
- en: UDnD (Zhang et al., [2020g](#bib.bib186)) takes advantage of each other by jointly
    training a dehazing network and a dense-aware multi-domain object detector. The
    object detection network is trained by adopting the classification and localization
    terms used by Region Proposal Network and Region of Interest. The multi-task training
    approach used by UDnD can consider the reduced inter-domain gaps and the remained
    intra-domain gaps for different density levels.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: UDnD（Zhang et al., [2020g](#bib.bib186)）通过联合训练去雾网络和密集感知多域目标检测器相互利用对方的优势。目标检测网络通过采用
    Region Proposal Network 和 Region of Interest 使用的分类和定位术语进行训练。UDnD 使用的多任务训练方法可以考虑不同密度级别的减少的领域间差距和剩余的领域内差距。
- en: Recent work has explored performing dehazing and high-level computer vision
    tasks simultaneously without using ASM. SDNet (Zhang et al., [2022a](#bib.bib178))
    combines semantic segmentation and dehazing into a unified framework in order
    to use semantic prior as a constraint for the optimization process. By embedding
    the predictions of the segmentation map into the dehazing network, SDNet performs
    a joint optimization of pixel-wise classification loss and regression loss. The
    classification loss is
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探讨了在不使用 ASM 的情况下同时执行去雾和高层计算机视觉任务。SDNet（Zhang et al., [2022a](#bib.bib178)）将语义分割和去雾结合成一个统一的框架，以利用语义先验作为优化过程的约束。通过将分割图的预测嵌入去雾网络，SDNet
    执行像素级分类损失和回归损失的联合优化。分类损失是
- en: '| (36) |  | $L_{sem}(s,s^{*})=-\frac{1}{P}\sum_{i}s_{i}^{*}{\log(s_{i})},$
    |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $L_{sem}(s,s^{*})=-\frac{1}{P}\sum_{i}s_{i}^{*}{\log(s_{i})},$
    |  |'
- en: where $P$ is the total number of pixels; $s_{i}$ is the class prediction at
    position $i$; $s^{*}$ denotes the ground truth semantic annotation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 是像素的总数；$s_{i}$ 是位置 $i$ 的类别预测；$s^{*}$ 表示真实的语义标注。
- en: (Zhang et al., [2022a](#bib.bib178), [2020g](#bib.bib186); Li et al., [2020c](#bib.bib82))
    show that segmentation and detection can be performed by embedding with dehazing
    networks. The way of joint dehazing task and high-level vision task may reduce
    the computational load to a certain extent by sharing the learned features, which
    can expand the goal of dehazing research.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (Zhang et al., [2022a](#bib.bib178), [2020g](#bib.bib186); Li et al., [2020c](#bib.bib82))
    表明，可以通过将去雾网络嵌入实现分割和检测。联合去雾任务和高层视觉任务的方式可能通过共享学习到的特征在一定程度上减少计算负担，这可以扩展去雾研究的目标。
- en: 3.17\. End-to-end CNN
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.17\. 端到端 CNN
- en: The “end-to-end” CNN stands for the non-ASM-based supervised algorithms, which
    usually consist of well-designed neural networks that take a single hazy image
    as input and a haze-free image as output. Networks based on different ideas are
    adopted, which are summarized as follows.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: “端到端” CNN 代表非 ASM 基于的监督算法，这些算法通常由设计良好的神经网络组成，该网络以单张有雾图像作为输入，输出无雾图像。采用了基于不同思想的网络，概述如下。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention mechanism: FFA-Net (Qin et al., [2020](#bib.bib109)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), SAN (Liang et al., [2019](#bib.bib85)), HFF (Zhang
    et al., [2022c](#bib.bib182)).'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力机制：FFA-Net（Qin 等，[2020](#bib.bib109)）、GridDehazeNet（Liu 等，[2019a](#bib.bib91)）、SAN（Liang
    等，[2019](#bib.bib85)）、HFF（Zhang 等，[2022c](#bib.bib182)）。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Encoder-Decoder: CAE (Chen and Lai, [2019](#bib.bib15)).'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器-解码器：CAE（Chen 和 Lai，[2019](#bib.bib15)）。
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Based on dense block: 123-CEDH (Guo et al., [2019a](#bib.bib48)).'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于密集块：123-CEDH（Guo 等，[2019a](#bib.bib48)）。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'U-shaped structure: DSEU (Lee et al., [2020b](#bib.bib70)), MSBDN (Dong et al.,
    [2020b](#bib.bib29)).'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: U 型结构：DSEU（Lee 等，[2020b](#bib.bib70)）、MSBDN（Dong 等，[2020b](#bib.bib29)）。
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical network: DMHN (Das and Dutta, [2020](#bib.bib25)).'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次网络：DMHN（Das 和 Dutta，[2020](#bib.bib25)）。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fusion with bilateral grid learning: 4kDehazing (Zheng et al., [2021](#bib.bib190)).'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双边网格学习融合：4kDehazing（Zheng 等，[2021](#bib.bib190)）。
- en: The end-to-end dehazing networks have an important impact on the entire dehazing
    field, proving that numerous deep learning models are beneficial to the dehazing
    task.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端去雾网络对整个去雾领域产生了重要影响，证明了众多深度学习模型对去雾任务是有益的。
- en: 4\. Semi-supervised Dehazing
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 半监督去雾
- en: Compared with the research on supervised methods, semi-supervised dehazing (Zhao
    et al., [2021b](#bib.bib189); Chen et al., [2021b](#bib.bib21)) algorithms have
    received relatively less attention. An important advantage of semi-supervised
    methods is the ability to utilize both labeled and unlabeled datasets. Therefore,
    compared to the fully supervised dehazing model, the semi-supervised dehazing
    model can alleviate the requirement of paired hazy and haze-free images. For dehazing
    task, labeled datasets are usually synthetic or artificially generated, while
    unlabeled datasets usually contain real world hazy images. According to the analysis
    of the dataset in Section [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning"), there are inherent
    differences between synthetic data and real world data. Therefore, semi-supervised
    algorithms usually have the ability to mitigate the gaps between synthetic domain
    and the real domain.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 与对监督方法的研究相比，半监督去雾（Zhao 等，[2021b](#bib.bib189); Chen 等，[2021b](#bib.bib21)）算法获得的关注相对较少。半监督方法的一个重要优势是能够利用标记数据集和未标记数据集。因此，与完全监督的去雾模型相比，半监督去雾模型可以缓解配对的有雾和无雾图像的需求。对于去雾任务，标记数据集通常是合成的或人工生成的，而未标记数据集通常包含真实世界的有雾图像。根据第
    [2](#S2 "2\. Related Work ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning") 节的数据集分析，合成数据和真实世界数据之间存在固有差异。因此，半监督算法通常具有减轻合成域和真实域之间差距的能力。
- en: Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the principles
    of various semi-supervised dehazing models, where the network is a schematic diagram.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") 展示了各种半监督去雾模型的原理，其中网络是示意图。
- en: '![Refer to caption](img/9dbf265897b637151ab61c1b6178b0e4.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9dbf265897b637151ab61c1b6178b0e4.png)'
- en: (a) Pretrain and finetune              (b) Disentangled and reconstruction          
    (c) Two branches training
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预训练和微调              (b) 解耦和重建           (c) 两分支训练
- en: Figure 8\. Semi-supervised dehazing methods.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 半监督去雾方法。
- en: 4.1\. Pretrain Backbone and Finetune
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 预训练骨干网和微调
- en: PSD (Chen et al., [2021b](#bib.bib21)) proposes a domain adaptation method that
    can be extended by existing models. It first uses a powerful backbone network
    for pre-training to obtain a base model suitable for synthetic data. Then the
    fine-tuning of the real domain in an unsupervised manner is applied to the well-trained
    network, thereby improving the generalization ability of the model to real world
    hazy images. To achieve fine-tuning in the real domain, PSD combines DCP loss,
    Bright Channel Prior (BCP) (Wang et al., [2013](#bib.bib143)) loss and Contrast
    Limited Adaptive Histogram Equalization (CLAHE) loss together. BCP loss and CLAHE
    loss are
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: PSD（Chen 等人, [2021b](#bib.bib21)）提出了一种可以通过现有模型扩展的领域适应方法。它首先使用强大的骨干网络进行预训练，以获得适用于合成数据的基础模型。然后，对真实领域进行无监督微调，从而提高模型对真实世界雾图像的泛化能力。为了在真实领域实现微调，PSD
    将 DCP 损失、亮通道先验（BCP）（Wang 等人, [2013](#bib.bib143)）损失和对比度限制自适应直方图均衡（CLAHE）损失结合在一起。BCP
    损失和 CLAHE 损失为
- en: '| (37) |  | $L_{BCP}=&#124;&#124;t_{BCP}(x)-t_{pred}(x)&#124;&#124;_{1},$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $L_{BCP}=&#124;&#124;t_{BCP}(x)-t_{pred}(x)&#124;&#124;_{1},$ |  |'
- en: '| (38) |  | $L_{CLAHE}=&#124;&#124;I(x)-I_{CLAHE}(x)&#124;&#124;_{1},$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $L_{CLAHE}=&#124;&#124;I(x)-I_{CLAHE}(x)&#124;&#124;_{1},$ |  |'
- en: where $t_{BCP}$ represents the transmission map estimated by BCP and $t_{pred}$
    is the predicted output of the network. $I_{CLAHE}$ is the hazy image reconstructed
    by CLAHE and other physical parameters.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{BCP}$ 表示 BCP 估计的传输图，$t_{pred}$ 是网络的预测输出。$I_{CLAHE}$ 是由 CLAHE 和其他物理参数重建的雾图像。
- en: 'During fine-tuning, the model may forget the useful knowledge it learned during
    the pre-training phase. Therefore, PSD proposes a feature-level constraint, which
    is achieved by calculating the feature map difference between the fine-tuning
    network and the pre-trained network. By feeding the synthetic data and real data
    into the fine-tuning network and the pre-trained network, respectively, four feature
    maps can be obtained: $F_{syn}^{tune}$, $F_{syn}^{pre}$, $F_{real}^{tune}$, and
    $F_{real}^{pre}$. Then, the loss for preventing knowledge forgetting can be calculated
    as'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，模型可能会忘记在预训练阶段学到的有用知识。因此，PSD 提出了特征级约束，通过计算微调网络与预训练网络之间的特征图差异来实现。通过分别将合成数据和真实数据输入微调网络和预训练网络，可以获得四个特征图：$F_{syn}^{tune}$、$F_{syn}^{pre}$、$F_{real}^{tune}$
    和 $F_{real}^{pre}$。然后，可以计算防止知识遗忘的损失：
- en: '| (39) |  | $L_{lwf}=&#124;&#124;F_{syn}^{tune}-F_{syn}^{pre}&#124;&#124;_{1}+&#124;&#124;F_{real}^{tune}-F_{real}^{pre}&#124;&#124;_{1}.$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $L_{lwf}=&#124;&#124;F_{syn}^{tune}-F_{syn}^{pre}&#124;&#124;_{1}+&#124;&#124;F_{real}^{tune}-F_{real}^{pre}&#124;&#124;_{1}.$
    |  |'
- en: As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(a), supervised
    pre-training is performed first, and then the dehazing network is fine-tuned in
    an unsupervised form.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")(a) 所示，首先进行监督预训练，然后在无监督形式下对去雾网络进行微调。
- en: SSDT (Zhang and Li, [2021](#bib.bib172)) uses the encoder-decoder network for
    pre-training in the form of domain translation. After pre-training, two encoders
    and one decoder are selected to obtain the holistic dehazing network $G(\cdot)$.
    Then, $G(\cdot)$ is fine-tuned with synthetic hazy images and real world hazy
    images. The two-stage method described above needs to ensure that the pre-training
    process can meet the accuracy requirements, otherwise it may bring accumulated
    errors to the fine-tuning process.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: SSDT（Zhang 和 Li, [2021](#bib.bib172)）使用编码器-解码器网络进行领域翻译形式的预训练。预训练后，选择两个编码器和一个解码器来获得整体去雾网络
    $G(\cdot)$。然后，$G(\cdot)$ 通过合成雾图像和真实世界雾图像进行微调。上述两阶段方法需要确保预训练过程能够满足准确性要求，否则可能会将累积误差带入微调过程。
- en: 4.2\. Disentangled and Reconstruction
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 解耦与重建
- en: From the perspective of dual learning, the dehazing task and the haze generation
    task can be able to assist each other. Based on this assumption, DCNet (Chen et al.,
    [2021c](#bib.bib22)) proposes a dual-task cycle network that jointly utilizes
    labeled dataset $N$ and unlabeled dataset $M$ by dehazing network $DN(\cdot)$
    and haze generation network $HGN(\cdot)$. The total loss is combine by dehazing
    loss $L_{DN}$ and reconstruction loss $L_{HGN}$, that is $L_{total}=L_{DN}+L_{HGN}$,
    as
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 从双重学习的角度来看，去雾任务和雾生成任务可以相互辅助。在此假设基础上，DCNet（Chen et al., [2021c](#bib.bib22)）提出了一种双任务循环网络，该网络通过去雾网络$DN(\cdot)$和雾生成网络$HGN(\cdot)$共同利用标记数据集$N$和未标记数据集$M$。总损失由去雾损失$L_{DN}$和重建损失$L_{HGN}$组成，即$L_{total}=L_{DN}+L_{HGN}$，如下所示：
- en: '| (40) |  | $\displaystyle L_{total}=$ | $\displaystyle\sum_{i=1}^{M+N}P(I_{i}(x))L(DN(I_{i}(x)),J_{i}(x),\epsilon_{1})+\lambda{L(HGN(DN(I_{i}(x))),I_{i}(x),\epsilon_{2})},$
    |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  | $\displaystyle L_{total}=$ | $\displaystyle\sum_{i=1}^{M+N}P(I_{i}(x))L(DN(I_{i}(x)),J_{i}(x),\epsilon_{1})+\lambda{L(HGN(DN(I_{i}(x))),I_{i}(x),\epsilon_{2})},$
    |  |'
- en: where $\epsilon_{1}$ and $\epsilon_{2}$ are regularizing hyper-parameter; $P(I_{i}(x))$
    equals $1$ when the $I_{i}(x)$ comes from the labeled dataset $N$, and equals
    $0$ otherwise. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    (b), the predicted haze-free image $J_{pred}(x)$ is first obtained by the left
    network, and then the hazy image $I_{rec}(x)$ is reconstructed by the right network.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\epsilon_{1}$和$\epsilon_{2}$是正则化超参数；当$I_{i}(x)$来自标记数据集$N$时，$P(I_{i}(x))$等于$1$，否则等于$0$。如图
    [8](#S4.F8 "Figure 8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning")（b）所示，首先通过左侧网络获得预测的无雾图像$J_{pred}(x)$，然后通过右侧网络重建雾霾图像$I_{rec}(x)$。
- en: 'Liu et al. (Liu et al., [2021](#bib.bib93)) use a disentangled image dehazing
    network (DID-Net) and a disentangled-consistency mean-teacher network (DMT-Net)
    to combine labeled and unlabeled data. DID-Net is responsible for disentangling
    the hazy image into a haze-free image, the transmission map, and the global atmospheric
    light. DMT-Net is used to jointly exploit the labeled synthetic data and unlabeled
    real world data through disentangled consistency loss. The supervised loss consists
    of four terms: haze-free image prediction $L_{J}^{s}$, transmission map prediction
    $L_{t}^{s}$, atmospheric light prediction $L_{A}^{s}$, and hazy image reconstruction
    $L_{rec}$:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人（Liu et al., [2021](#bib.bib93)）使用了一个解缠结图像去雾网络（DID-Net）和一个解缠结一致性均值教师网络（DMT-Net）来结合标记数据和未标记数据。DID-Net负责将雾霾图像解缠结为无雾图像、传输图和全局大气光。DMT-Net用于通过解缠结一致性损失共同利用标记的合成数据和未标记的真实世界数据。监督损失由四项组成：无雾图像预测$L_{J}^{s}$，传输图预测$L_{t}^{s}$，大气光预测$L_{A}^{s}$，以及雾霾图像重建$L_{rec}$：
- en: '|  | $\displaystyle L_{J}^{s}=&#124;&#124;G_{J}-P_{J}&#124;&#124;_{1}+&#124;&#124;G_{J}-\hat{P}_{J}&#124;&#124;_{1},$
    |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{J}^{s}=&#124;&#124;G_{J}-P_{J}&#124;&#124;_{1}+&#124;&#124;G_{J}-\hat{P}_{J}&#124;&#124;_{1},$
    |  |'
- en: '|  | $\displaystyle L_{T}^{s}=&#124;&#124;G_{T}-P_{T}&#124;&#124;_{1}+&#124;&#124;G_{T}-\hat{P}_{T}&#124;&#124;_{1},$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{T}^{s}=&#124;&#124;G_{T}-P_{T}&#124;&#124;_{1}+&#124;&#124;G_{T}-\hat{P}_{T}&#124;&#124;_{1},$
    |  |'
- en: '|  | $\displaystyle L_{A}^{s}=&#124;&#124;G_{A}-P_{A}&#124;&#124;_{1}+&#124;&#124;G_{A}-\hat{P}_{A}&#124;&#124;_{1},$
    |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{A}^{s}=&#124;&#124;G_{A}-P_{A}&#124;&#124;_{1}+&#124;&#124;G_{A}-\hat{P}_{A}&#124;&#124;_{1},$
    |  |'
- en: '| (41) |  | $\displaystyle L_{rec}=&#124;&#124;I(x)-P_{I}&#124;&#124;_{1}+&#124;&#124;I(x)-\hat{P}_{I}&#124;&#124;_{1},$
    |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| (41) |  | $\displaystyle L_{rec}=&#124;&#124;I(x)-P_{I}&#124;&#124;_{1}+&#124;&#124;I(x)-\hat{P}_{I}&#124;&#124;_{1},$
    |  |'
- en: 'where $G$ stands for ground-truth, $P$ is the prediction of the first stage,
    and $\hat{P}$ is the prediction of the second stage. The supervised loss $L^{s}(x)$
    is the weighted sum of the above four losses, that is $L^{s}(x)=L_{J}^{s}+\alpha_{1}L_{T}^{s}+\alpha_{2}L_{A}^{s}+\alpha_{3}L_{rec}$.
    For unlabeled data, consistency loss is used to constrain the teacher network
    $T$ and the student network $S$:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$G$代表真实值，$P$是第一阶段的预测，$\hat{P}$是第二阶段的预测。监督损失$L^{s}(x)$是上述四种损失的加权和，即$L^{s}(x)=L_{J}^{s}+\alpha_{1}L_{T}^{s}+\alpha_{2}L_{A}^{s}+\alpha_{3}L_{rec}$。对于未标记数据，使用一致性损失来约束教师网络$T$和学生网络$S$：
- en: '|  | $\displaystyle L_{J}^{c}=&#124;&#124;S_{J}-T_{J}&#124;&#124;_{1}+&#124;&#124;S_{\hat{J}}-T_{\hat{J}}&#124;&#124;_{1},$
    |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{J}^{c}=&#124;&#124;S_{J}-T_{J}&#124;&#124;_{1}+&#124;&#124;S_{\hat{J}}-T_{\hat{J}}&#124;&#124;_{1},$
    |  |'
- en: '|  | $\displaystyle L_{T}^{c}=&#124;&#124;S_{T}-T_{T}&#124;&#124;_{1}+&#124;&#124;S_{\hat{T}}-T_{\hat{T}}&#124;&#124;_{1},$
    |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{T}^{c}=&#124;&#124;S_{T}-T_{T}&#124;&#124;_{1}+&#124;&#124;S_{\hat{T}}-T_{\hat{T}}&#124;&#124;_{1},$
    |  |'
- en: '|  | $\displaystyle L_{A}^{c}=&#124;&#124;S_{A}-T_{A}&#124;&#124;_{1}+&#124;&#124;S_{\hat{A}}-T_{\hat{A}}&#124;&#124;_{1},$
    |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{A}^{c}=||S_{A}-T_{A}||_{1}+||S_{\hat{A}}-T_{\hat{A}}||_{1},$
    |  |'
- en: '| (42) |  | $\displaystyle L_{rec}^{c}=&#124;&#124;S_{I}-T_{I}&#124;&#124;_{1}+&#124;&#124;S_{\hat{I}}-T_{\hat{I}}&#124;&#124;_{1},$
    |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $\displaystyle L_{rec}^{c}=||S_{I}-T_{I}||_{1}+||S_{\hat{I}}-T_{\hat{I}}||_{1},$
    |  |'
- en: where the subscript $\hat{J}$, $\hat{T}$, $\hat{A}$ and $\hat{I}$ denote the
    results predicted in second stage. Thus, the loss for the unlabeled dataset is
    $L^{c}(y)=L_{J}^{c}+\alpha_{4}L_{T}^{c}+\alpha_{5}L_{A}^{c}+\alpha_{6}L_{rec}^{c}$.
    The final loss function of the semi-supervised framework consists of a supervised
    loss on the labeled dataset $N$ and a consistency loss on the unlabeled dataset
    $M$, that is $L_{total}=\sum_{x\in N}{L^{s}{(x)}}+\lambda\sum_{y\in M}{L^{c}{(y)}}$.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标 $\hat{J}$、$\hat{T}$、$\hat{A}$ 和 $\hat{I}$ 表示第二阶段预测的结果。因此，无标记数据集的损失为 $L^{c}(y)=L_{J}^{c}+\alpha_{4}L_{T}^{c}+\alpha_{5}L_{A}^{c}+\alpha_{6}L_{rec}^{c}$。半监督框架的最终损失函数由标记数据集
    $N$ 上的监督损失和无标记数据集 $M$ 上的一致性损失组成，即 $L_{total}=\sum_{x\in N}{L^{s}{(x)}}+\lambda\sum_{y\in
    M}{L^{c}{(y)}}$。
- en: CCDM (Zhang et al., [2020f](#bib.bib180)) designs a color-constrained dehazing
    model that can be extended to a semi-supervised framework, which is achieved by
    the reconstruction of hazy images, the smoothing loss of $t(x)$ and $A$, etc.
    Experiments (Liu et al., [2021](#bib.bib93); Zhang et al., [2020f](#bib.bib180);
    Chen et al., [2021c](#bib.bib22)) show that the reconstruction of hazy images
    can provide effective supervisory signals in an unsupervised manner, which is
    instructive for semi-supervised frameworks.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: CCDM (Zhang et al., [2020f](#bib.bib180)) 设计了一个颜色约束去雾模型，可以扩展到半监督框架，这通过雾霾图像的重建、$t(x)$
    和 $A$ 的平滑损失等实现。实验 (Liu et al., [2021](#bib.bib93); Zhang et al., [2020f](#bib.bib180);
    Chen et al., [2021c](#bib.bib22)) 表明，雾霾图像的重建可以以无监督的方式提供有效的监督信号，这对半监督框架具有指导意义。
- en: 4.3\. Two-branches Training
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 两分支训练
- en: 'SSID (Li et al., [2020a](#bib.bib79)) designs an end-to-end network that integrates
    a supervised learning branch and an unsupervised learning branch. The training
    process of SSID uses both the labeled dataset and the unlabeled dataset by the
    following process:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: SSID (Li et al., [2020a](#bib.bib79)) 设计了一个集成了监督学习分支和无监督学习分支的端到端网络。SSID 的训练过程使用了标记数据集和无标记数据集，具体过程如下：
- en: '| (43) |  | $J_{pred}(x)=G(I(x)),$ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $J_{pred}(x)=G(I(x)),$ |  |'
- en: where $G(\cdot)$ consists of a supervised part $G_{s}(\cdot)$ and an unsupervised
    part $G_{u}(\cdot)$. The supervised loss composed of L2 loss and perceptual loss
    is used to ensure that the predicted image $J_{pred}(x)$ and its corresponding
    ground truth image are as close as possible, which is the same as the supervised
    dehazing algorithm. A combination of total variation loss and dark channel loss
    is used for unsupervised training. As shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4\.
    Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single Image
    Dehazing Based on Deep Learning")(c), supervised training and unsupervised training
    are performed in a shared weight manner.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(\cdot)$ 由一个监督部分 $G_{s}(\cdot)$ 和一个无监督部分 $G_{u}(\cdot)$ 组成。监督损失由 L2 损失和感知损失组成，用于确保预测图像
    $J_{pred}(x)$ 与其对应的真实图像尽可能接近，这与监督去雾算法相同。无监督训练使用总变差损失和暗通道损失的组合。如图 [8](#S4.F8 "Figure
    8 ‣ 4\. Semi-supervised Dehazing ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning")(c) 所示，监督训练和无监督训练以共享权重的方式进行。
- en: SSIDN (An et al., [2022](#bib.bib2)) also combines supervised and unsupervised
    training processes. Supervised training is used to learn the mapping from hazy
    to haze-free images. With dark channel prior and bright channel prior (Wang et al.,
    [2013](#bib.bib143)) guiding the training process, the unsupervised branch incorporates
    the estimation of the transmission map and atmospheric light.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: SSIDN (An et al., [2022](#bib.bib2)) 也结合了监督和无监督训练过程。监督训练用于学习从雾霾图像到无雾图像的映射。在暗通道先验和亮通道先验
    (Wang et al., [2013](#bib.bib143)) 指导训练过程的情况下，无监督分支结合了传输图和大气光的估计。
- en: 'The DAID (Shao et al., [2020](#bib.bib119)) adopts a domain adaptation model
    to jointly train multi-subnetworks. The $G_{S\to{R}}$ and $G_{R\to{S}}$ are the
    image translation modules used for the translation between the synthetic domain
    and the real domain, where $R$ and $S$ stand for real domain and synthetic domain,
    respectively. By using an image-level discriminator $D_{R}^{img}$ and a feature-level
    discriminator $D_{R}^{feat}$, the adversarial loss of the translation process
    can be calculated. In order to ensure that the content of images are maintained
    during the translation process, DAID uses cycle consistency loss to constrain
    the translation network. Furthermore, identity mapping loss is also used in the
    conversion process to restrict the identity of the image generation process in
    two domains. The training of the dehazing network $G_{R}$ is a combination of
    unsupervised and supervised processes. The supervised process minimizes the $L_{rm}$
    loss to make the dehazed image $J_{S\to{R}}$ closer to the corresponding haze-free
    image $Y_{S}$:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: DAID（Shao等，[2020](#bib.bib119)）采用领域适应模型来联合训练多个子网络。$G_{S\to{R}}$ 和 $G_{R\to{S}}$
    是用于合成领域和真实领域之间转换的图像翻译模块，其中 $R$ 和 $S$ 分别代表真实领域和合成领域。通过使用图像级判别器 $D_{R}^{img}$ 和特征级判别器
    $D_{R}^{feat}$，可以计算转换过程的对抗损失。为了确保在转换过程中图像内容得到保留，DAID 使用循环一致性损失来约束转换网络。此外，在转换过程中还使用了身份映射损失，以限制两个领域中图像生成过程的身份。去雾网络
    $G_{R}$ 的训练是无监督和监督过程的结合。监督过程通过最小化 $L_{rm}$ 损失，使去雾图像 $J_{S\to{R}}$ 更接近对应的无雾霾图像
    $Y_{S}$：
- en: '| (44) |  | $L_{rm}=&#124;&#124;J_{S\to{R}}-Y_{S}&#124;&#124;_{2}^{2}.$ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  | $L_{rm}=&#124;&#124;J_{S\to{R}}-Y_{S}&#124;&#124;_{2}^{2}.$ |  |'
- en: Then, the total variation loss and dark channel loss are used as unsupervised
    losses. For the training of the dehazing network $G_{S}$, there is also a combination
    of supervised loss and unsupervised loss. With the help of domain transformation
    and unsupervised loss, DAID can effectively reduce the gap between the synthetic
    domain and the real domain.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，总变差损失和暗通道损失被用作无监督损失。在去雾网络 $G_{S}$ 的训练中，也结合了监督损失和无监督损失。借助领域转换和无监督损失，DAID 可以有效减少合成领域和真实领域之间的差距。
- en: As introduced above (Li et al., [2020a](#bib.bib79); An et al., [2022](#bib.bib2);
    Shao et al., [2020](#bib.bib119)), using supervised branch and unsupervised branch
    for joint training to build a semi-supervised framework can effectively alleviate
    the problem of domain shift.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述（Li等，[2020a](#bib.bib79)；An等，[2022](#bib.bib2)；Shao等，[2020](#bib.bib119)），使用监督分支和无监督分支进行联合训练来建立半监督框架，可以有效缓解领域偏移的问题。
- en: 5\. Unsupervised Dehazing
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 无监督去雾
- en: The supervised and semi-supervised dehazing methods have achieved excellent
    performance on public datasets. However, the training process requires paired
    data (i.e. hazy images and haze-free images / transmission maps), which are difficult
    to obtain in the real world. For outdoor scenes containing grass, water or moving
    objects, it is difficult to guarantee that two images taken under hazy and clear
    weather have exactly the same content. If the haze-free labels are not accurate
    enough, the accuracy of dehazed images will be reduced. Therefore,  (Engin et al.,
    [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al., [2020a](#bib.bib90);
    Wei et al., [2021](#bib.bib147)) explore dehazing algorithms in an unsupervised
    way.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 监督和半监督去雾方法在公共数据集上取得了优异的性能。然而，训练过程需要配对数据（即雾霾图像和无雾霾图像/传输图），这些数据在现实世界中难以获得。对于包含草地、水域或移动物体的户外场景，很难保证在雾霾和晴朗天气下拍摄的两张图像具有完全相同的内容。如果无雾霾标签不够准确，去雾图像的准确性将降低。因此，（Engin等，[2018](#bib.bib38)；Dudhane和Murala，[2019a](#bib.bib35)；Liu等，[2020a](#bib.bib90)；Wei等，[2021](#bib.bib147)）探索了无监督的去雾算法。
- en: '![Refer to caption](img/a997349418cfbe573bc502ed67c9e046.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a997349418cfbe573bc502ed67c9e046.png)'
- en: (a) Between two domains      (b) Among multi-domains             (c) Without
    target               (d) Zero-shot
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在两个领域之间      (b) 在多个领域之间             (c) 没有目标               (d) 零样本
- en: Figure 9\. Schematic diagram of the unsupervised dehazing algorithms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 无监督去雾算法示意图。
- en: 5.1\. Unsupervised Domain Transfer
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 无监督领域迁移
- en: 'In the study of image style transfer and image-to-image translation, CycleGAN (Zhu
    et al., [2017](#bib.bib193)) is proposed, which provides a way for learning the
    bidirectional mapping functions between two domains. Inspired by CycleGAN, (Engin
    et al., [2018](#bib.bib38); Dudhane and Murala, [2019a](#bib.bib35); Liu et al.,
    [2020a](#bib.bib90)) are designed for unsupervised transformation of hazy and
    haze-free / transmission domains. The Cycle-Dehaze (Engin et al., [2018](#bib.bib38))
    contains two generators $G$ and $F$, which are used to learn the mapping from
    hazy domain to haze-free domain and the reverse mapping, respectively. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") (a), the hazy and
    haze-free images are translated to each other by two generators. By sampling $x$
    and $y$ from the hazy domain $X$ and the haze-free domain $Y$, the Cycle-Dehaze
    uses the perceptual metric (denoted as $\psi(\cdot)$) to obtain the cyclic perceptual
    consistency loss:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像风格迁移和图像对图像转换的研究中，提出了 CycleGAN (Zhu et al., [2017](#bib.bib193))，它提供了一种学习两个领域之间双向映射函数的方法。受到
    CycleGAN 的启发，(Engin et al., [2018](#bib.bib38); Dudhane 和 Murala, [2019a](#bib.bib35);
    Liu et al., [2020a](#bib.bib90)) 被设计用于模糊和无雾/传输领域的无监督变换。Cycle-Dehaze (Engin et
    al., [2018](#bib.bib38)) 包含两个生成器 $G$ 和 $F$，分别用于学习从模糊领域到无雾领域的映射以及反向映射。如图 [9](#S5.F9
    "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning") (a) 所示，模糊图像和无雾图像通过两个生成器互相转换。通过从模糊领域
    $X$ 和无雾领域 $Y$ 中采样 $x$ 和 $y$，Cycle-Dehaze 使用感知度量（记作 $\psi(\cdot)$）来获得循环感知一致性损失：
- en: '| (45) |  | $\displaystyle L_{cyc-p}=$ | $\displaystyle{&#124;&#124;\psi(x)-\psi(F(G(x)))}{&#124;&#124;}_{2}^{2}+{&#124;&#124;\psi(y)-\psi(G(F(y)))}{&#124;&#124;}_{2}^{2}.$
    |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| (45) |  | $\displaystyle L_{cyc-p}=$ | $\displaystyle{&#124;&#124;\psi(x)-\psi(F(G(x)))}{&#124;&#124;}_{2}^{2}+{&#124;&#124;\psi(y)-\psi(G(F(y)))}{&#124;&#124;}_{2}^{2}.$
    |  |'
- en: The overall loss function of Cycle-Dehaze is composed of cyclic perceptual consistency
    loss and Cycle-GAN’s loss function, which can alleviate the requirement of paired
    data. CDNet (Dudhane and Murala, [2019a](#bib.bib35)) also adopts a cycle-consistent
    adversarial approach for unsupervised dehazing network training. Unlike Cycle-Dehaze,
    CDNet embeds ASM into the network architecture for estimating transmission map,
    which enables it to acquire physical parameters while restoring haze-free images.
    E-CycleGAN (Liu et al., [2020a](#bib.bib90)) adds ASM and a priori statistical
    law to estimate atmospheric light on the basis of CycleGAN, which allows it to
    perform independent parameter estimation for sky regions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Cycle-Dehaze 的整体损失函数由循环感知一致性损失和 Cycle-GAN 的损失函数组成，这可以缓解对配对数据的需求。CDNet (Dudhane
    和 Murala, [2019a](#bib.bib35)) 也采用了循环一致的对抗方法用于无监督去雾网络训练。与 Cycle-Dehaze 不同，CDNet
    将 ASM 嵌入到网络架构中以估计传输图，这使其在恢复无雾图像的同时能够获取物理参数。E-CycleGAN (Liu et al., [2020a](#bib.bib90))
    在 CycleGAN 的基础上增加了 ASM 和先验统计规律，以估计大气光，从而使其能够对天空区域进行独立的参数估计。
- en: USID (Huang et al., [2019](#bib.bib57)) introduces the concept of haze mask
    $H(x)$ and constant bias $\epsilon$, which can obtain the reformulated ASM as
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: USID (Huang et al., [2019](#bib.bib57)) 引入了雾霾掩模 $H(x)$ 和常数偏置 $\epsilon$ 的概念，这可以得到重新公式化的
    ASM，如下所示
- en: '| (46) |  | $J(x)=\frac{I(x)-A}{H(x)}+A+\epsilon.$ |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | $J(x)=\frac{I(x)-A}{H(x)}+A+\epsilon.$ |  |'
- en: By combining with cycle consistent loss, USID also removes the requirement of
    an explicit paired haze/depth data in an unsupervised multi-task learning manner.
    In order to decouple content and haze information, USID-DR (Liu, [2019](#bib.bib88))
    designs a content encoder and a haze encoder embedded in CycleGAN. This decoupling
    approach can enhance feature extraction and reconstruction during cycle consistent
    training. USID-DR proposes that making the output of the encoder conform to the
    Gaussian distribution by latent regression loss can improve the quality of hazy
    image synthesis process, thereby improving the overall dehazing performance. DCA-CycleGAN (Mo
    et al., [2022](#bib.bib99)) utilizes the dark channel to build the input and generates
    the attention for handling the nonhomogeneous haze. These CycleGAN-based methods
    demonstrate that unsupervised hazy and haze-free image domain transformation can
    achieve the same performance as supervised algorithms from both ASM-based and
    non-ASM-based perspectives.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合循环一致性损失，USID 还在无监督多任务学习的方式中去除了对显式配对雾霾/深度数据的要求。为了分离内容和雾霾信息，USID-DR（刘，[2019](#bib.bib88)）在
    CycleGAN 中设计了一个内容编码器和一个雾霾编码器。这种解耦方法可以增强在循环一致性训练过程中的特征提取和重建。USID-DR 提出，通过潜在回归损失使编码器的输出符合高斯分布，可以提高雾霾图像合成过程的质量，从而改善整体去雾性能。DCA-CycleGAN（Mo
    等，[2022](#bib.bib99)）利用暗通道构建输入并生成处理非均匀雾霾的注意力。这些基于 CycleGAN 的方法表明，无监督雾霾与无雾霾图像领域转换可以从
    ASM 基于和非 ASM 基于的角度实现与监督算法相同的性能。
- en: 'For the dehazing task, the haze density in hazy images can be very different.
    Most supervised, semi-supervised and unsupervised dehazing methods hold the view
    that hazy images and haze-free images should be treated as two domains, without
    considering the density among different examples. Recent work (Jin et al., [2020](#bib.bib62))
    proposes that it is an important issue to apply the haze density information to
    the unsupervised training process, thereby improving the generalization ability
    of the dehazing model to images with different densities of haze. An unsupervised
    conditional disentangle network, called UCDN (Jin et al., [2020](#bib.bib62)),
    is designed by incorporating conditional information into the training process
    of CycleGAN. Further, DHL-Dehaze (Cong et al., [2020](#bib.bib24)) analyzes multiple
    haze density levels of hazy images, and proposes that the difference in haze density
    should be fully utilized in the dehazing procedure. Compared to UCDN which contains
    four submodules, DHL-Dehaze has only two networks to train. The idea of DHL-Dehaze
    is based on the research of multi-domain image-to-image translation. As shown
    in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")(b), the images
    $I_{1}(x)$, $I_{2}(x)$ and $I_{3}(x)$ of different densities can be transformed
    to other domains, where $I_{3}(x)$ can be explained as the haze-free domain. The
    training of DHL-Dehaze consists of a adversarial process and a classification
    process. By sampling $X_{ori}=\{x_{ori}^{(1)},x_{ori}^{(2)},\ldots,x_{ori}^{(m)}\}$
    with corresponding density labels $L_{ori}=\{l_{ori}^{(1)},l_{ori}^{(2)},\ldots,l_{ori}^{(m)}\}$,
    the classification loss $L_{cls}^{ori}$ and adversarial loss $L_{dis}^{ori}$ in
    the source domain can be obtained by ([47](#S5.E47 "In 5.1\. Unsupervised Domain
    Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning")) and ([48](#S5.E48 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于去雾任务，雾图像中的雾密度可能非常不同。大多数监督、半监督和无监督去雾方法认为雾图像和无雾图像应被视为两个领域，而不考虑不同示例之间的密度。最近的研究（Jin
    et al., [2020](#bib.bib62)）提出，将雾密度信息应用于无监督训练过程是一个重要问题，从而提高去雾模型对不同密度雾图像的泛化能力。一个无监督条件解耦网络，称为UCDN（Jin
    et al., [2020](#bib.bib62)），通过将条件信息纳入CycleGAN的训练过程中来设计。此外，DHL-Dehaze（Cong et al.,
    [2020](#bib.bib24)）分析了雾图像的多个雾密度水平，并提出在去雾过程中应充分利用雾密度的差异。与包含四个子模块的UCDN相比，DHL-Dehaze仅有两个网络进行训练。DHL-Dehaze的理念基于多领域图像到图像转换的研究。如图[9](#S5.F9
    "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning")(b)所示，不同密度的图像$I_{1}(x)$、$I_{2}(x)$和$I_{3}(x)$可以转换到其他领域，其中$I_{3}(x)$可以解释为无雾领域。DHL-Dehaze的训练包括一个对抗过程和一个分类过程。通过采样$X_{ori}=\{x_{ori}^{(1)},x_{ori}^{(2)},\ldots,x_{ori}^{(m)}\}$及其相应的密度标签$L_{ori}=\{l_{ori}^{(1)},l_{ori}^{(2)},\ldots,l_{ori}^{(m)}\}$，可以通过([47](#S5.E47
    "In 5.1\. Unsupervised Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"))和([48](#S5.E48
    "In 5.1\. Unsupervised Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning"))得到源领域的分类损失$L_{cls}^{ori}$和对抗损失$L_{dis}^{ori}$：
- en: '| (47) |  | $L_{cls}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{ori}^{(i)}\mid
    x_{ori}^{(i)}),$ |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| (47) |  | $L_{cls}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{ori}^{(i)}\mid
    x_{ori}^{(i)}),$ |  |'
- en: '| (48) |  | $L_{dis}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{dis}(x_{ori}^{(i)}).$
    |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| (48) |  | $L_{dis}^{ori}=\frac{1}{m}\sum_{i=1}^{m}-logD_{dis}(x_{ori}^{(i)}).$
    |  |'
- en: 'In order to generate the images of the target domain, the labels of target
    domain $L_{tar}=\{l_{tar}^{(1)},l_{tar}^{(2)},\ldots,l_{tar}^{(m)}\}$ are required.
    DHL-Dehaze sends $X_{ori}$ and $L_{tar}$ into the multi-scale generator (MST)
    together, and obtains new images with the same attributes as $L_{tar}$:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成目标领域的图像，需要目标领域的标签$L_{tar}=\{l_{tar}^{(1)},l_{tar}^{(2)},\ldots,l_{tar}^{(m)}\}$。DHL-Dehaze将$X_{ori}$和$L_{tar}$一起送入多尺度生成器（MST），并获得与$L_{tar}$具有相同属性的新图像：
- en: '| (49) |  | $X_{tar}=MST(X_{ori},L_{tar}).$ |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| (49) |  | $X_{tar}=MST(X_{ori},L_{tar}).$ |  |'
- en: 'Then, the classification loss $L_{cls}^{tar}$ and adversarial loss $L_{dis}^{tar}$
    of the target domain image can be obtained by using ([50](#S5.E50 "In 5.1\. Unsupervised
    Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")) and ([51](#S5.E51 "In 5.1\.
    Unsupervised Domain Transfer ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning")):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用 ([50](#S5.E50 "在 5.1\. 无监督领域迁移 ‣ 5\. 无监督去雾 ‣ 基于深度学习的单幅图像去雾的综合调查和分类"))
    和 ([51](#S5.E51 "在 5.1\. 无监督领域迁移 ‣ 5\. 无监督去雾 ‣ 基于深度学习的单幅图像去雾的综合调查和分类")) 来获得目标领域图像的分类损失
    $L_{cls}^{tar}$ 和对抗损失 $L_{dis}^{tar}$：
- en: '| (50) |  | $L_{cls}^{tar}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{tar}^{(i)}\mid
    x_{tar}^{(i)}),$ |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| (50) |  | $L_{cls}^{tar}=\frac{1}{m}\sum_{i=1}^{m}-logD_{cls}(l_{tar}^{(i)}\mid
    x_{tar}^{(i)}),$ |  |'
- en: '| (51) |  | $L_{dis}^{tar}=\frac{1}{m}\sum_{i=1}^{m}logD_{dis}(x_{tar}^{(i)}).$
    |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| (51) |  | $L_{dis}^{tar}=\frac{1}{m}\sum_{i=1}^{m}logD_{dis}(x_{tar}^{(i)}).$
    |  |'
- en: It is worth noting that UCDN and DHL-Dehaze do not use the hazy and haze-free
    images of the same scene for supervision, but apply the density of haze as the
    supervisory information for the training process.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，UCDN 和 DHL-Dehaze 不使用同一场景的有雾和无雾图像进行监督，而是将雾的密度作为训练过程中的监督信息。
- en: The training of unsupervised domain transformation algorithms is more difficult
    than that of supervised algorithms. The convergence of GAN-based domain transformation
    algorithms is difficult to determine, which may lead to over-enhanced images.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督领域转换算法的训练比监督算法更困难。基于 GAN 的领域转换算法的收敛性难以确定，这可能导致图像过度增强。
- en: 5.2\. Learning without Haze-free Images
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 无需无雾图像的学习
- en: 'The training process of CycleGAN-based methods and DHL-Dehaze does not require
    paired data, which has greatly reduced the difficulty of data collection. Further,
    Deep-DCP (Golts et al., [2020](#bib.bib44)) proposes to use only hazy image for
    training process. The strategy of domain translation dehazing algorithms is to
    learn the mapping relationship between hazy and haze-free / transmission domains,
    while the main idea of Deep-DCP is to minimize the DCP (He et al., [2010](#bib.bib52))
    energy function. According to statistical assumption, the transmission map can
    be estimated in an unsupervised way. Based on the estimated transmission map and
    soft matting, the energy function $E(t_{\theta},I(x))$ can be obtained, where
    $\theta$ is the parameters for tuning. Thus, the goal of training is to minimize
    the energy function:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CycleGAN 的方法和 DHL-Dehaze 的训练过程不需要配对数据，这大大降低了数据收集的难度。此外，Deep-DCP (Golts 等，
    [2020](#bib.bib44)) 提出仅使用有雾图像进行训练。领域转换去雾算法的策略是学习有雾图像和无雾/透射领域之间的映射关系，而 Deep-DCP
    的主要思想是最小化 DCP (He 等， [2010](#bib.bib52)) 能量函数。根据统计假设，透射图可以以无监督的方式进行估计。基于估计的透射图和软分割，可以获得能量函数
    $E(t_{\theta},I(x))$，其中 $\theta$ 是调整的参数。因此，训练的目标是最小化能量函数：
- en: '| (52) |  | $\theta^{*}=\mathop{\arg\min}_{\theta}[\frac{1}{N}\sum_{i=1}^{N}E(t_{\theta}(I_{i}(x)),I_{i}(x))].$
    |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| (52) |  | $\theta^{*}=\mathop{\arg\min}_{\theta}[\frac{1}{N}\sum_{i=1}^{N}E(t_{\theta}(I_{i}(x)),I_{i}(x))].$
    |  |'
- en: As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(c), the
    network can automatically learn the mapping from hazy to haze-free images without
    requiring target domain labels.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [9](#S5.F9 "图 9 ‣ 5\. 无监督去雾 ‣ 基于深度学习的单幅图像去雾的综合调查和分类")(c) 所示，网络可以自动学习从有雾到无雾图像的映射，而无需目标领域标签。
- en: 5.3\. Unsupervised Image Decomposition
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 无监督图像分解
- en: 'Double-DIP (Gandelsman et al., [2019](#bib.bib42)) proposes an unsupervised
    hierarchical decoupling framework based on the observation that the internal statistics
    of a mixed layer is more complex than the single layer that composes it. Suppose
    $Z$ is a linear sum of independent random variables $X$ and $Y$. From a statistical
    point of view, the entropy of $Z$ is larger than its independent components, that
    is, $H(Z)\geq\max{\{H(X),H(Y)\}}$. Based on this, Double-DIP proposes the loss
    for image layer decomposition:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Double-DIP (Gandelsman 等， [2019](#bib.bib42)) 提出了一个无监督的层次解耦框架，基于混合层的内部统计比构成它的单层更复杂的观察。假设
    $Z$ 是独立随机变量 $X$ 和 $Y$ 的线性和。从统计角度来看，$Z$ 的熵大于其独立组件，即 $H(Z)\geq\max{\{H(X),H(Y)\}}$。基于此，Double-DIP
    提出了用于图像层分解的损失：
- en: '| (53) |  | $L=L_{reconst}+\alpha\cdot L_{excl}+\beta\cdot L_{reg},$ |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| (53) |  | $L=L_{reconst}+\alpha\cdot L_{excl}+\beta\cdot L_{reg},$ |  |'
- en: where $L_{reconst}$ represents the reconstruction loss of the hazy image, $L_{excl}$
    is the exclusion loss between the two DIPs, and $L_{reg}$ is the regularization
    loss used to obtain the continuous and smooth transmission map.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{reconst}$ 表示模糊图像的重建损失，$L_{excl}$ 是两个 DIP 之间的排除损失，$L_{reg}$ 是用于获取连续和平滑的传输图的正则化损失。
- en: 5.4\. Zero-Shot Learning for Dehazing
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 零样本学习去雾
- en: 'Data-driven unsupervised dehazing methods have achieved impressive performance.
    Unlike those models that require sufficient data to perform network training,
    ZID (Li et al., [2020b](#bib.bib72)) proposes a neural network dehazing process
    that only requires a single example. ZID has further reduced the dependence of
    the parameter learning process on data by combining the advantages of unsupervised
    learning and zero-shot learning. Three sub-networks $f_{J}(\cdot)$ (J-Net), $f_{T}(\cdot)$
    (T-Net) and $f_{A}(\cdot)$ (A-Net) are used to estimate the $J(x)$, $t(x)$ and
    $A$, respectively. By the reconstruction process, the hazy image $I(x)$ can be
    disentangled by minimizing $L_{rec}$ loss:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的无监督去雾方法已经取得了显著的成绩。不同于那些需要大量数据进行网络训练的模型，ZID（Li 等，[2020b](#bib.bib72)）提出了一种仅需单个示例的神经网络去雾过程。通过结合无监督学习和零样本学习的优势，ZID
    进一步减少了参数学习过程对数据的依赖。三个子网络 $f_{J}(\cdot)$（J-Net）、$f_{T}(\cdot)$（T-Net）和 $f_{A}(\cdot)$（A-Net）分别用于估计
    $J(x)$、$t(x)$ 和 $A$。通过重建过程，可以通过最小化 $L_{rec}$ 损失来解耦模糊图像 $I(x)$：
- en: '| (54) |  | $L_{rec}=&#124;&#124;I_{rec}(x)-I(x)&#124;&#124;_{p},$ |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| (54) |  | $L_{rec}=&#124;&#124;I_{rec}(x)-I(x)&#124;&#124;_{p},$ |  |'
- en: 'where $I_{rec}(x)$ is reconstructed hazy image, and $p$ denotes $p$-norm. The
    disentangled atmospheric light $f_{A}(x)$ and Kullback-Leibler divergence are
    used to obtain the atmospheric light $A$ as shown in the following formula:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{rec}(x)$ 是重建的模糊图像，$p$ 表示 $p$-范数。解耦的大气光 $f_{A}(x)$ 和 Kullback-Leibler
    散度用于获取大气光 $A$，如以下公式所示：
- en: '| (55) |  | $\displaystyle L_{A}=$ | $\displaystyle L_{H}+L_{KL}=&#124;&#124;f_{A}(x)-A(x)&#124;&#124;_{p}+KL(N(u_{z},\delta_{z}^{2})&#124;&#124;N(0,I)),$
    |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| (55) |  | $\displaystyle L_{A}=$ | $\displaystyle L_{H}+L_{KL}=&#124;&#124;f_{A}(x)-A(x)&#124;&#124;_{p}+KL(N(u_{z},\delta_{z}^{2})&#124;&#124;N(0,I)),$
    |  |'
- en: where $L_{H}$ is the loss for $f_{A}(x)$ and initial hint value $A(x)$ is automatically
    learned from data. It should be noted that $A(x)$ in ZID is not the same as $A$
    in ASM; $N(0,I)$ stands for Gaussian distribution; $z$ is learned from input $x$.
    The unsupervised channel loss $L_{J}$ used in J-Net is for the decomposing process
    of the haze-free image $J(x)$. The $L_{J}$ is calculated based on the dark channel,
    and the formula is $L_{J}=||\min_{c\in\{r,g,b\}}(J^{c}(y))||_{p}$, where $c$ denotes
    color channel and $y$ stands for local patch of the J-Net output. The purpose
    of $L_{reg}$ is to enhance the stability of the model and make $A$ and $t(x)$
    smooth. The overall loss function of ZID is
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{H}$ 是 $f_{A}(x)$ 的损失，初始提示值 $A(x)$ 从数据中自动学习。需要注意的是，ZID 中的 $A(x)$ 与 ASM
    中的 $A$ 不同；$N(0,I)$ 表示高斯分布；$z$ 是从输入 $x$ 中学习到的。J-Net 中使用的无监督通道损失 $L_{J}$ 用于无雾图像
    $J(x)$ 的分解过程。$L_{J}$ 基于暗通道计算，其公式为 $L_{J}=||\min_{c\in\{r,g,b\}}(J^{c}(y))||_{p}$，其中
    $c$ 表示颜色通道，$y$ 代表 J-Net 输出的局部补丁。$L_{reg}$ 的目的是增强模型的稳定性，使 $A$ 和 $t(x)$ 平滑。ZID 的总体损失函数为
- en: '| (56) |  | $L=L_{rec}+L_{A}+L_{J}+L_{reg}.$ |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| (56) |  | $L=L_{rec}+L_{A}+L_{J}+L_{reg}.$ |  |'
- en: YOLY (Li et al., [2021a](#bib.bib71)) uses three joint disentanglement subnetworks
    for clear image and physical parameter estimation, enabling unsupervised and untrained
    haze removal. As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5\. Unsupervised Dehazing
    ‣ A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")(d),
    ZID and YOLY can learn the mapping from hazy to haze-free image using a single
    unlabeled example.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: YOLY（Li 等，[2021a](#bib.bib71)）使用三个联合解耦子网络进行清晰图像和物理参数估计，实现了无监督和未训练的去雾。如图 [9](#S5.F9
    "Figure 9 ‣ 5\. Unsupervised Dehazing ‣ A Comprehensive Survey and Taxonomy on
    Single Image Dehazing Based on Deep Learning")(d) 所示，ZID 和 YOLY 可以使用单个未标记示例学习从模糊图像到无雾图像的映射。
- en: The limitation of the zero-shot algorithm is that the generalization ability
    is limited, and the network must be retrained for each unseen example to get good
    dehazing performance.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本算法的限制在于其泛化能力有限，网络必须为每个未见过的示例重新训练，以获得良好的去雾性能。
- en: 6\. Experiment and Performance Analysis
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 实验和性能分析
- en: 'This section provides quantitative and qualitative analysis of the dehazing
    performance of the baseline supervised, semi-supervised and unsupervised algorithms.
    The training examples used in the experiments are the indoor data ITS and outdoor
    data OTS from RESIDE (Li et al., [2019c](#bib.bib74)). In order to accurately
    compare the dehazed images and the real clear images, the indoor and outdoor images
    included in the SOTS provided by RESIDE are used as the test set. Aiming at ensuring
    the fairness and representativeness of the experimental results, 10 representative
    algorithms are selected for comparison:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了基线监督、半监督和无监督算法的去雾性能的定量和定性分析。实验中使用的训练样本是来自 RESIDE 的室内数据 ITS 和室外数据 OTS（Li
    et al., [2019c](#bib.bib74)）。为了准确比较去雾图像和真实清晰图像，使用了 RESIDE 提供的 SOTS 中的室内和室外图像作为测试集。为确保实验结果的公平性和代表性，选择了
    10 个具有代表性的算法进行比较：
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'supervised: AOD-Net (Li et al., [2017a](#bib.bib73)), 4kDehazing (Zheng et al.,
    [2021](#bib.bib190)), DMMFD (Deng et al., [2019](#bib.bib27)), FFA-Net (Qin et al.,
    [2020](#bib.bib109)), GCA-Net (Chen et al., [2019c](#bib.bib14)), GridDehazeNet (Liu
    et al., [2019a](#bib.bib91)), MSBDN (Dong et al., [2020b](#bib.bib29)).'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督：AOD-Net (Li et al., [2017a](#bib.bib73))，4kDehazing (Zheng et al., [2021](#bib.bib190))，DMMFD
    (Deng et al., [2019](#bib.bib27))，FFA-Net (Qin et al., [2020](#bib.bib109))，GCA-Net
    (Chen et al., [2019c](#bib.bib14))，GridDehazeNet (Liu et al., [2019a](#bib.bib91))，MSBDN
    (Dong et al., [2020b](#bib.bib29))。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'semi-supervised: SSID (Li et al., [2020a](#bib.bib79)).'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半监督：SSID (Li et al., [2020a](#bib.bib79))。
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'unsupervised: Cycle-Dehaze (Engin et al., [2018](#bib.bib38)), ZID (Li et al.,
    [2020b](#bib.bib72)).'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督：Cycle-Dehaze (Engin et al., [2018](#bib.bib38))，ZID (Li et al., [2020b](#bib.bib72))。
- en: The framework used for the algorithm implementation is PyTorch, and the running
    platform is NVIDIA Tesla V100 32 GB (2 GPUs). Except for the zero-shot ZID, all
    algorithms use a batch size of 8 in the training phase. For the ITS and OTS datasets,
    the training epochs are set according to (Qin et al., [2020](#bib.bib109)) and (Liu
    et al., [2019a](#bib.bib91)), respectively. For the supervised dehazing methods,
    four loss function strategies are adopted, including L1, L2, L1 + P, L2 + P, where
    P denotes the perceptual loss. For semi-supervised and unsupervised methods, the
    loss functions follow the settings in the respective papers. In the experiment,
    we first compared the convergence curves of different supervised algorithms when
    they were set to 0.001, 0.0005 or 0.0002, and selected the value with the best
    convergence effect as the best learning rate. After the training process, the
    loss values of all supervised models have stably converged. For semi-supervised
    and unsupervised algorithms, the learning rates used are those recommended in
    the corresponding papers.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 算法实现使用的框架是 PyTorch，运行平台为 NVIDIA Tesla V100 32 GB（2 个 GPU）。除零样本 ZID 外，所有算法在训练阶段均使用批量大小为
    8。对于 ITS 和 OTS 数据集，训练轮数分别按照 (Qin et al., [2020](#bib.bib109)) 和 (Liu et al., [2019a](#bib.bib91))
    设置。对于监督去雾方法，采用了四种损失函数策略，包括 L1、L2、L1 + P、L2 + P，其中 P 表示感知损失。对于半监督和无监督方法，损失函数遵循各自论文中的设置。在实验中，我们首先比较了不同监督算法在设置为
    0.001、0.0005 或 0.0002 时的收敛曲线，并选择了收敛效果最佳的值作为最佳学习率。训练过程结束后，所有监督模型的损失值均已稳定收敛。对于半监督和无监督算法，使用了相应论文推荐的学习率。
- en: 'In order to ensure as few interference factors as possible in the experiments,
    the following strategies are not used in the experiments: (1) pre-training, (2)
    dynamic learning rate adjustment, (3) larger batch size, and (4) data augmentation.
    Therefore, the quantitative results obtained in the following experiments may
    be a little lower than the best results provided in the papers corresponding to
    the various algorithms.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保实验中干扰因素尽可能少，实验中未使用以下策略：（1）预训练，（2）动态学习率调整，（3）更大批量大小，以及（4）数据增强。因此，以下实验中获得的定量结果可能略低于各算法论文中提供的最佳结果。
- en: Tables [4](#S6.T4 "Table 4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5
    "Table 5 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") show the PSNR/SSIM
    values obtained on the indoor and outdoor test sets of various algorithms on RESIDE
    SOTS, where the highest values are shown in bold. It can be concluded that FFA-Net
    achieves the best performance on the indoor test set, with the highest PSNR and
    SSIM. The best performance among different algorithms on the outdoor test set
    is achieved by different methods. It can be also seen from Tables [4](#S6.T4 "Table
    4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning") and [5](#S6.T5 "Table 5 ‣ 6\.
    Experiment and Performance Analysis ‣ A Comprehensive Survey and Taxonomy on Single
    Image Dehazing Based on Deep Learning") that the perceptual loss has a certain
    effect on the performance of the model, but it is not obvious.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S6.T4 "Table 4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") 和 [5](#S6.T5
    "Table 5 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") 显示了在 RESIDE SOTS 的室内和室外测试集上获得的
    PSNR/SSIM 值，其中最高值以 **粗体** 显示。可以得出结论，FFA-Net 在室内测试集上表现最佳，具有最高的 PSNR 和 SSIM。不同算法在室外测试集上的最佳表现由不同方法实现。从表
    [4](#S6.T4 "Table 4 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") 和 [5](#S6.T5
    "Table 5 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey and
    Taxonomy on Single Image Dehazing Based on Deep Learning") 也可以看出，感知损失对模型的性能有一定影响，但并不明显。
- en: Figure [10](#S6.F10 "Figure 10 ‣ 6\. Experiment and Performance Analysis ‣ A
    Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    and Figure [11](#S6.F11 "Figure 11 ‣ 6\. Experiment and Performance Analysis ‣
    A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning")
    show the visual results achieved by the supervised, semi-supervised and unsupervised
    algorithms, respectively, where the loss function used by the supervised algorithms
    is the L1 loss. The visual results show that the dehazed images obtained by the
    supervised algorithms are closer to the real images in terms of color and detail
    than the semi-supervised/unsupervised algorithms.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#S6.F10 "Figure 10 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive
    Survey and Taxonomy on Single Image Dehazing Based on Deep Learning") 和图 [11](#S6.F11
    "Figure 11 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") 显示了由监督、半监督和无监督算法分别获得的视觉结果，其中监督算法使用的损失函数是
    L1 损失。视觉结果显示，与半监督/无监督算法相比，监督算法得到的去雾图像在颜色和细节上更接近真实图像。
- en: Table 4\. Results on RESIDE SOTS indoor, where the SSIM and PNSR values are
    separated by the slash.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 在 RESIDE SOTS 室内的结果，其中 SSIM 和 PNSR 值用斜杠分隔。
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Config | L1 | L1 + P | L2 | L2 + P |'
- en: '| AODNet | 0.839/19.375 | 0.841/19.454 | 0.851/19.576 | 0.839/19.388 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| AODNet | 0.839/19.375 | 0.841/19.454 | 0.851/19.576 | 0.839/19.388 |'
- en: '| 4kDehazing | 0.932/23.881 | 0.949/26.569 | 0.928/23.370 | 0.928/23.353 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 4kDehazing | 0.932/23.881 | 0.949/26.569 | 0.928/23.370 | 0.928/23.353 |'
- en: '| DMMDF | 0.959/30.585 | 0.960/30.383 | 0.961/30.631 | 0.961/30.725 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| DMMDF | 0.959/30.585 | 0.960/30.383 | 0.961/30.631 | 0.961/30.725 |'
- en: '| FFA-Net | 0.983/32.730 | 0.983/32.536 | 0.978/32.224 | 0.978/32.178 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| FFA-Net | 0.983/32.730 | 0.983/32.536 | 0.978/32.224 | 0.978/32.178 |'
- en: '| GCA-Net | 0.924/25.044 | 0.930/25.638 | 0.917/24.716 | 0.909/24.714 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| GCA-Net | 0.924/25.044 | 0.930/25.638 | 0.917/24.716 | 0.909/24.714 |'
- en: '| GridDehazeNet | 0.962/25.671 | 0.962/25.655 | 0.935/22.940 | 0.940/23.052
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| GridDehazeNet | 0.962/25.671 | 0.962/25.655 | 0.935/22.940 | 0.940/23.052
    |'
- en: '| MSBDN | 0.955/28.341 | 0.955/28.562 | 0.941/27.237 | 0.937/25.662 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| MSBDN | 0.955/28.341 | 0.955/28.562 | 0.941/27.237 | 0.937/25.662 |'
- en: '| SSID | 0.814/20.959 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| SSID | 0.814/20.959 |'
- en: '| Cycle-Dehaze | 0.810/18.880 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| Cycle-Dehaze | 0.810/18.880 |'
- en: '| ZID | 0.835/19.830 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| ZID | 0.835/19.830 |'
- en: Table 5\. Results on RESIDE SOTS outdoor, where the SSIM and PNSR values are
    separated by the slash.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 在 RESIDE SOTS 室外的结果，其中 SSIM 和 PNSR 值用斜杠分隔。
- en: '| Config | L1 | L1 + P | L2 | L2 + P |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| Config | L1 | L1 + P | L2 | L2 + P |'
- en: '| AODNet | 0.913/23.613 | 0.917/23.683 | 0.912/23.253 | 0.916/23.488 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| AODNet | 0.913/23.613 | 0.917/23.683 | 0.912/23.253 | 0.916/23.488 |'
- en: '| 4kDehazing | 0.963/28.476 | 0.964/28.473 | 0.958/28.103 | 0.950/27.546 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 4kDehazing | 0.963/28.476 | 0.964/28.473 | 0.958/28.103 | 0.950/27.546 |'
- en: '| DMMDF | 0.905/25.805 | 0.963/30.237 | 0.963/30.535 | 0.965/30.682 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| DMMDF | 0.905/25.805 | 0.963/30.237 | 0.963/30.535 | 0.965/30.682 |'
- en: '| FFA-Net | 0.921/27.126 | 0.925/27.299 | 0.913/28.176 | 0.920/28.441 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| FFA-Net | 0.921/27.126 | 0.925/27.299 | 0.913/28.176 | 0.920/28.441 |'
- en: '| GCA-Net | 0.953/27.784 | 0.949/27.559 | 0.946/27.20 | 0.943/27.273 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| GCA-Net | 0.953/27.784 | 0.949/27.559 | 0.946/27.20 | 0.943/27.273 |'
- en: '| GridDehazeNet | 0.964/28.296 | 0.964/28.388 | 0.963/27.807 | 0.963/27.766
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| GridDehazeNet | 0.964/28.296 | 0.964/28.388 | 0.963/27.807 | 0.963/27.766
    |'
- en: '| MSBDN | 0.962/29.944 | 0.963/30.277 | 0.964/29.843 | 0.956/28.782 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| MSBDN | 0.962/29.944 | 0.963/30.277 | 0.964/29.843 | 0.956/28.782 |'
- en: '| SSID | 0.840/20.905 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| SSID | 0.840/20.905 |'
- en: '| Cycle-Dehaze | 0.861/20.347 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Cycle-Dehaze | 0.861/20.347 |'
- en: '| ZID | 0.633/13.520 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| ZID | 0.633/13.520 |'
- en: '![Refer to caption](img/f2feb7aa286362078dfe8186f310925d.png)![Refer to caption](img/e418d85a6ae343188b08b02a3ad6a92c.png)![Refer
    to caption](img/2428c902fccbec1bcacf909a5e7bd4df.png)![Refer to caption](img/389fd6a26aa53b01a8883f7b6ec217ab.png)![Refer
    to caption](img/b1d6407d47e0a14538c9976fcdfbd68c.png)![Refer to caption](img/4594fcd3634f81c59bcd89ed6da37c86.png)![Refer
    to caption](img/b6f5f2c29d263145e069929d814bface.png)![Refer to caption](img/43af5737033d352a00d5d9036de2496d.png)![Refer
    to caption](img/98fee125a5ea6f50f70277d099bafde8.png)![Refer to caption](img/35e8cd57d7c7d47c8802ad55c3519e2d.png)![Refer
    to caption](img/c9f2c3d3419ab045ef4b7e8101ed2a12.png)![Refer to caption](img/dee2c61c33aa4a2634a4f2e0abcbd6ce.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f2feb7aa286362078dfe8186f310925d.png)![参考说明](img/e418d85a6ae343188b08b02a3ad6a92c.png)![参考说明](img/2428c902fccbec1bcacf909a5e7bd4df.png)![参考说明](img/389fd6a26aa53b01a8883f7b6ec217ab.png)![参考说明](img/b1d6407d47e0a14538c9976fcdfbd68c.png)![参考说明](img/4594fcd3634f81c59bcd89ed6da37c86.png)![参考说明](img/b6f5f2c29d263145e069929d814bface.png)![参考说明](img/43af5737033d352a00d5d9036de2496d.png)![参考说明](img/98fee125a5ea6f50f70277d099bafde8.png)![参考说明](img/35e8cd57d7c7d47c8802ad55c3519e2d.png)![参考说明](img/c9f2c3d3419ab045ef4b7e8101ed2a12.png)![参考说明](img/dee2c61c33aa4a2634a4f2e0abcbd6ce.png)'
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 模糊       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
- en: '![Refer to caption](img/0265fca5f17e3afe4bacdd124402b6ac.png) ![Refer to caption](img/637f18ddf1be133d9d9a6d45dd4cc022.png)
    ![Refer to caption](img/e30793f2ea4c8952e6aad7b332237c23.png) ![Refer to caption](img/8d16c4a1f9f7ae603e2295cc36996415.png)
    ![Refer to caption](img/0f645aef82073de109986af5c39e4cba.png) ![Refer to caption](img/99aebfe9185de93e7e66932d62d42898.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0265fca5f17e3afe4bacdd124402b6ac.png) ![参考说明](img/637f18ddf1be133d9d9a6d45dd4cc022.png)
    ![参考说明](img/e30793f2ea4c8952e6aad7b332237c23.png) ![参考说明](img/8d16c4a1f9f7ae603e2295cc36996415.png)
    ![参考说明](img/0f645aef82073de109986af5c39e4cba.png) ![参考说明](img/99aebfe9185de93e7e66932d62d42898.png)'
- en: '![Refer to caption](img/ad036f8379af4611d79a1147af6eaa15.png)![Refer to caption](img/53c7bd060ecb81cc8ee7992cf241b1d4.png)![Refer
    to caption](img/6d23ab8ae6c39bd64cd8340df126b260.png)![Refer to caption](img/8b4a6ffc7663471489aa68104c7f1056.png)![Refer
    to caption](img/e4b48e13428ee7136c32e0fbc905aebc.png)![Refer to caption](img/b4ec35306228437ec033cc0f53c76719.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad036f8379af4611d79a1147af6eaa15.png)![参考说明](img/53c7bd060ecb81cc8ee7992cf241b1d4.png)![参考说明](img/6d23ab8ae6c39bd64cd8340df126b260.png)![参考说明](img/8b4a6ffc7663471489aa68104c7f1056.png)![参考说明](img/e4b48e13428ee7136c32e0fbc905aebc.png)![参考说明](img/b4ec35306228437ec033cc0f53c76719.png)'
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) 清晰
- en: Figure 10\. Visual results on RESIDE SOTS indoor.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 在 RESIDE SOTS 室内的视觉结果。
- en: For a fair comparison of the computational speed of the baseline methods, the
    zero-shot-based ZID and high-resolution-based 4kDehazing are excluded. Fig. [12](#S6.F12
    "Figure 12 ‣ 6\. Experiment and Performance Analysis ‣ A Comprehensive Survey
    and Taxonomy on Single Image Dehazing Based on Deep Learning") shows the average
    time for each algorithm that run 1000 times with PyTorch framework. It can be
    seen that AODNet is the fastest, and can achieve real-time effects for inputs
    of different sizes. FFA-Net is the slowest, taking more than 0.3 seconds to process
    the $672\times 672$ input.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较基准方法的计算速度，基于零-shot 的 ZID 和高分辨率的 4kDehazing 被排除在外。图 [12](#S6.F12 "图 12
    ‣ 6\. 实验和性能分析 ‣ 关于基于深度学习的单幅图像去雾的综合调查和分类") 显示了每个算法运行 1000 次的平均时间，使用 PyTorch 框架。可以看出，AODNet
    是最快的，能够实现对不同尺寸输入的实时效果。FFA-Net 最慢，处理 $672\times 672$ 的输入需要超过 0.3 秒。
- en: '![Refer to caption](img/5a4f08bf1d355f9968af362221046d1d.png)![Refer to caption](img/5b918adaccab1c195ed3dec9473afeec.png)![Refer
    to caption](img/08b5b66664070f079d16fb266494771e.png)![Refer to caption](img/9884719b91385e20926c3b32da2dce84.png)![Refer
    to caption](img/f0fba61952cd7d03bce779fcc518f669.png)![Refer to caption](img/0e1cffc4f3971a082e7d7c4f653c4c93.png)![Refer
    to caption](img/2f6c7b259446e234d2edb6ca5c54d084.png)![Refer to caption](img/9a7c55b7e8d9f4779a234ca3b9410640.png)![Refer
    to caption](img/f8b86157fa28cddb393ad089c7e207b5.png)![Refer to caption](img/e04635817fa8d55f6eb165a6e95fdfe6.png)![Refer
    to caption](img/6c0c8c5703ad99355a02a70f3975932f.png)![Refer to caption](img/bdac90bf8dac6d18140c54d8cf9ab913.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a4f08bf1d355f9968af362221046d1d.png)![参见说明](img/5b918adaccab1c195ed3dec9473afeec.png)![参见说明](img/08b5b66664070f079d16fb266494771e.png)![参见说明](img/9884719b91385e20926c3b32da2dce84.png)![参见说明](img/f0fba61952cd7d03bce779fcc518f669.png)![参见说明](img/0e1cffc4f3971a082e7d7c4f653c4c93.png)![参见说明](img/2f6c7b259446e234d2edb6ca5c54d084.png)![参见说明](img/9a7c55b7e8d9f4779a234ca3b9410640.png)![参见说明](img/f8b86157fa28cddb393ad089c7e207b5.png)![参见说明](img/e04635817fa8d55f6eb165a6e95fdfe6.png)![参见说明](img/6c0c8c5703ad99355a02a70f3975932f.png)![参见说明](img/bdac90bf8dac6d18140c54d8cf9ab913.png)'
- en: (a) hazy       (b) 4kDehazing       (c) AODNet       (d) DMMFD         (e) FFA-Net
          (f) GCA-Net
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 有雾       (b) 4k去雾       (c) AODNet       (d) DMMFD         (e) FFA-Net      
    (f) GCA-Net
- en: '![Refer to caption](img/bd980cba3ea42e600d042cd16e5b3e43.png) ![Refer to caption](img/839113d06d8afdc49672df5104101e1a.png)
    ![Refer to caption](img/cc48c167cfe455ef7c980f1e038e3a6f.png) ![Refer to caption](img/d54216a1933edd7e224448735e674f58.png)
    ![Refer to caption](img/2a23e72d85c231ac49545b13a7f24b3f.png) ![Refer to caption](img/e36b5bab9e12b91740e14c5fc89f7957.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd980cba3ea42e600d042cd16e5b3e43.png) ![参见说明](img/839113d06d8afdc49672df5104101e1a.png)
    ![参见说明](img/cc48c167cfe455ef7c980f1e038e3a6f.png) ![参见说明](img/d54216a1933edd7e224448735e674f58.png)
    ![参见说明](img/2a23e72d85c231ac49545b13a7f24b3f.png) ![参见说明](img/e36b5bab9e12b91740e14c5fc89f7957.png)'
- en: '![Refer to caption](img/0568101726b59880cc8237702f6897c3.png)![Refer to caption](img/f1df20ef1bda3e07de87344d5e2a0e92.png)![Refer
    to caption](img/c037f06c9e16f2dada648ff67e987a32.png)![Refer to caption](img/3530b94362a34dc9b8befdd019bc8518.png)![Refer
    to caption](img/7a74af0e6abdda5eddbced6819596ca5.png)![Refer to caption](img/6e96c861dce4ea57cad9321f7d7cb7f6.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0568101726b59880cc8237702f6897c3.png)![参见说明](img/f1df20ef1bda3e07de87344d5e2a0e92.png)![参见说明](img/c037f06c9e16f2dada648ff67e987a32.png)![参见说明](img/3530b94362a34dc9b8befdd019bc8518.png)![参见说明](img/7a74af0e6abdda5eddbced6819596ca5.png)![参见说明](img/6e96c861dce4ea57cad9321f7d7cb7f6.png)'
- en: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) clear
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: (g) GridDehazeNet   (h) MSBDN        (i) SSID        (j) Cycle-Dehaze       
    (k) ZID          (l) 清晰
- en: Figure 11\. Visual results on RESIDE SOTS outdoor.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. RESIDE SOTS 户外的视觉结果。
- en: '![Refer to caption](img/24fcff6c5f47e30203db9e6bb2a5b858.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24fcff6c5f47e30203db9e6bb2a5b858.png)'
- en: Figure 12\. Running speed on different sizes of input.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 不同输入大小下的运行速度。
- en: 7\. Challenges and Opportunities
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 挑战与机遇
- en: For image dehazing task, the current supervised, semi-supervised and unsupervised
    methods have achieved good performance. However, there are problems that still
    exist and open issues that should be explored in the future research. Next, we
    will discuss these challenges.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像去雾任务，目前的监督式、半监督式和无监督式方法已经取得了良好的性能。然而，仍然存在一些问题和未来研究中应探索的开放问题。接下来，我们将讨论这些挑战。
- en: 7.1\. More Effective ASM
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 更有效的 ASM
- en: The current ASM has been proved to be suitable for describing the formation
    process of haze by many supervised, semi-supervised and unsupervised dehazing
    methods. However, recent work (Ju et al., [2021](#bib.bib65)) finds that the intrinsic
    limitation of ASM will cause a dim effect in the image after dehazing. By adding
    a new parameter, an enhanced ASM (EASM) (Ju et al., [2021](#bib.bib65)) is proposed.
    Improvements to ASM will have an important impact on the dehazing performance
    of existing methods that rely on ASM. Therefore, it is worth exploring a more
    accurate model of the haze formation process.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的 ASM 已被证明适用于描述雾霾形成过程，通过许多监督式、半监督式和无监督式去雾方法。然而，近期工作 (Ju 等，[2021](#bib.bib65))
    发现 ASM 的固有限制会导致去雾后图像变暗。通过添加新参数，提出了一种增强的 ASM (EASM) (Ju 等，[2021](#bib.bib65))。对
    ASM 的改进将对依赖于 ASM 的现有去雾方法的去雾性能产生重要影响。因此，探索更准确的雾霾形成过程模型是值得的。
- en: 7.2\. Shift between the Real Domain and Synthetic Domain
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 真实域与合成域之间的转变
- en: The current training process of dehazing models generally requires sufficient
    data. Since it is difficult to collect pairs of hazy and haze-free images in the
    real world, the synthesized data is needed by dehazing task. However, there are
    inherent differences between the synthesized hazy image and the real world hazy
    image. In order to solve the domain shift problem, the following three directions
    are worth exploring.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 当前去雾模型的训练过程通常需要足够的数据。由于在真实世界中收集雾霾图像和无雾图像的配对数据较为困难，去雾任务需要合成数据。然而，合成雾霾图像与真实世界的雾霾图像之间存在固有差异。为了解决领域迁移问题，以下三个方向值得探索。
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The haze synthesized based on ASM cannot completely simulate the formation of
    haze in the real world, so we can attempt to design a more realistic hazy image
    synthesis algorithm to compensate for the difference between domains.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于ASM合成的雾霾无法完全模拟真实世界中的雾霾形成，因此我们可以尝试设计一种更现实的雾霾图像合成算法，以弥补领域之间的差异。
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: By introducing domain adaptation, semi-supervised and unsupervised methods into
    network design, experiments show that these algorithms can perform well in real
    world dehazing task.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将领域适应、半监督和无监督方法引入网络设计，实验表明这些算法在真实世界的去雾任务中表现良好。
- en: •
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recent work (MRFID/BeDDE) collected some real world paired data, but they did
    not contain as many examples as RESIDE. It is challenging to build a large-scale
    real world dataset that can be used for a large-capacity CNN-based dehazing model.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近的工作 (MRFID/BeDDE) 收集了一些真实世界的配对数据，但数量不如RESIDE多。构建一个大规模的真实世界数据集，以用于大容量CNN基础的去雾模型，具有挑战性。
- en: 7.3\. Computational Efficiency and New Metrics
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 计算效率和新指标
- en: The dehazing models are often used as a preprocessing module for high-level
    computer vision tasks. For example, the lightweight AOD-Net is applied to object
    detection task. There are three issues that should be considered in the application
    of the dehazing model.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 去雾模型通常作为高级计算机视觉任务的预处理模块使用。例如，轻量级的AOD-Net应用于物体检测任务。在去雾模型的应用中，有三个问题需要考虑。
- en: •
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In order to help follow-up tasks, the dehazing model should ensure that the
    dehazed image is of high quality.
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了帮助后续任务，去雾模型应确保去雾图像具有高质量。
- en: •
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The inference speed of the model must be fast enough to meet the real-time requirement.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的推理速度必须足够快，以满足实时要求。
- en: •
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Several end devices with small storage are sensitive to the number of parameters
    of the dehazing model.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几个存储小的终端设备对去雾模型的参数数量非常敏感。
- en: Therefore, we need to balance quantitative performance, inference time and the
    number of parameters. High-quality dehazed image with high PSNR and SSIM are the
    main goals of the current research. FAMED-Net (Zhang and Tao, [2020](#bib.bib171))
    discusses the computational efficiency of 14 models, which shows that several
    algorithms with excellent performance may be very time and memory consuming. Dehazing
    algorithms based on deep learning usually require the use of graphical computing
    units for model training and deployment. In real world applications, the forward
    inference speed of an algorithm is an important evaluation metric. 4kDehazing (Zheng
    et al., [2021](#bib.bib190)) explores fast dehazing of high-resolution input,
    which takes only $8$ ms ($125$ fps) to process a 4K ($3840\times 2160$) image
    on a single Titan RTX GPU. Future research can try to design a new evaluation
    metric that can comprehensively consider the dehazing quality, running speed and
    model size.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要平衡定量性能、推理时间和参数数量。高质量的去雾图像，具有高PSNR和SSIM，是当前研究的主要目标。FAMED-Net (Zhang 和
    Tao, [2020](#bib.bib171)) 讨论了14种模型的计算效率，表明一些性能优越的算法可能非常耗时和占用内存。基于深度学习的去雾算法通常需要使用图形计算单元进行模型训练和部署。在实际应用中，算法的前向推理速度是一个重要的评估指标。4kDehazing
    (Zheng 等, [2021](#bib.bib190)) 探索了高分辨率输入的快速去雾，在单个Titan RTX GPU上处理一张4K (3840×2160)
    图像仅需$8$毫秒 ($125$ fps)。未来的研究可以尝试设计一种新的评估指标，综合考虑去雾质量、运行速度和模型大小。
- en: 7.4\. Perceptual Loss
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 感知损失
- en: The pre-trained model trained on a large-scale dataset can be used to calculate
    the perceptual loss. As shown in Table [3](#S2.T3 "Table 3 ‣ 2.4.5\. Total Variation
    Loss ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning"), many dehazing models use the
    perceptual loss obtained by VGG16 or VGG19 as a part of the overall loss to improve
    the quality of the final result. Thus, a natural question is, can the perceptual
    loss be the same as the L1/L2 loss as a general loss for image dehazing task?
    Alternatively, is there a more efficient and higher quality way to compute the
    perceptual loss? For example, is it a better solution to obtain perceptual loss
    by other pre-trained models? Currently, there is no comprehensive study on the
    relationship between perceptual loss and dehazing task.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模数据集上预训练的模型可以用来计算感知损失。如表[3](#S2.T3 "Table 3 ‣ 2.4.5\. Total Variation Loss
    ‣ 2.4\. Loss Function ‣ 2\. Related Work ‣ A Comprehensive Survey and Taxonomy
    on Single Image Dehazing Based on Deep Learning")所示，许多去雾模型将VGG16或VGG19获得的感知损失作为整体损失的一部分，以提升最终结果的质量。因此，一个自然的问题是，感知损失能否与L1/L2损失相同，作为图像去雾任务的一般损失？或者，有没有更高效、更优质的计算感知损失的方法？例如，通过其他预训练模型获得感知损失是否是更好的解决方案？目前，还没有关于感知损失与去雾任务之间关系的综合研究。
- en: 7.5\. How Dehazing Methods Affect High-level Computer Vision Tasks
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 去雾方法对高级计算机视觉任务的影响
- en: Many dehazing algorithms have been validated on high-level computer vision tasks,
    such as image segmentation and object detection. Experiments show that these dehazing
    methods can promote the performance of high-level computer vision tasks under
    hazy conditions. However, the recent study (Pei et al., [2018](#bib.bib108)) has
    shown that several dehazing algorithms may be ineffective for image classification,
    although their dehazing ability has been proven. The experimental results (Pei
    et al., [2018](#bib.bib108)) on synthetic and real hazy data show that several
    well-designed dehazing models have little positive effect on the performance of
    the classification model, and sometimes may reduce the classification accuracy
    to some extent.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 许多去雾算法已在高级计算机视觉任务上进行了验证，如图像分割和目标检测。实验表明，这些去雾方法能够在雾霾条件下提升高级计算机视觉任务的性能。然而，最近的研究（Pei
    et al., [2018](#bib.bib108)）显示，尽管去雾能力已被证明，但一些去雾算法在图像分类方面可能效果不佳。实验结果（Pei et al.,
    [2018](#bib.bib108)）表明，在合成和真实雾霾数据上的一些精心设计的去雾模型对分类模型的性能几乎没有积极影响，有时甚至可能在某种程度上降低分类准确性。
- en: Besides, domain adaptation and generalization methods have been proposed to
    deal with high-level computer vision tasks such as object detection (Wang et al.,
    [2022](#bib.bib139), [2021a](#bib.bib138)) and semantic segmentation (Zhang et al.,
    [2019b](#bib.bib173); Gao et al., [2021](#bib.bib43)) in bad weather conditions
    including haze, which can be treated as mitigating the side effect of haze in
    the feature space by aligning hazy image features with those clean ones. It will
    be an interesting research topic to investigate and reduce the influence of haze
    on high-level computer vision tasks both at image-level (i.e., dehazing) and feature-level
    (i.e., domain adaptation).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还提出了领域适应和泛化方法来处理包括雾霾在内的恶劣天气条件下的高级计算机视觉任务，如目标检测（Wang et al., [2022](#bib.bib139),
    [2021a](#bib.bib138)）和语义分割（Zhang et al., [2019b](#bib.bib173); Gao et al., [2021](#bib.bib43)），这些方法可以视为通过将雾霾图像特征与干净图像特征对齐来减轻雾霾在特征空间中的副作用。研究并减少雾霾对高级计算机视觉任务在图像级（即去雾）和特征级（即领域适应）的影响将是一个有趣的研究课题。
- en: 7.6\. Prior Knowledge and Learning Model
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6\. 先验知识与学习模型
- en: Before deep learning was widely used in image dehazing task, image-based prior
    statistical knowledge was an important part for guiding the dehazing process.
    Now, extensive work has shown that deep learning techniques can effectively remove
    haze from images independently of physical model. At the same time, several recent
    studies have found that prior statistical knowledge can be used for semi-supervised (Li
    et al., [2020a](#bib.bib79)) and unsupervised (Golts et al., [2020](#bib.bib44))
    network training. Effective prior knowledge can reduce the model’s dependence
    on data to a certain extent and improve the generalization ability of the model.
    However, there is currently no evidence which statistical priors are always valid.
    Therefore, the general prior knowledge that can be used for the CNN-based dehazing
    algorithm is worth verifying.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习广泛应用于图像去雾任务之前，基于图像的先验统计知识是指导去雾过程的重要部分。现在，大量工作已经表明，深度学习技术可以有效地独立去除图像中的雾霾，而不依赖物理模型。同时，最近的一些研究发现，先验统计知识可以用于半监督（Li等，[2020a](#bib.bib79)）和无监督（Golts等，[2020](#bib.bib44)）网络训练。有效的先验知识可以在一定程度上减少模型对数据的依赖，提高模型的泛化能力。然而，目前没有证据表明哪些统计先验始终有效。因此，值得验证可用于基于CNN的去雾算法的通用先验知识。
- en: 8\. Conclusion
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: This paper provides a comprehensive survey of deep learning-based dehazing research.
    First, the commonly used physical model, high-quality datasets, general loss functions,
    effective network modules and evaluation metrics are summarized. Then, supervised,
    semi-supervised and unsupervised dehazing studies are classified and analyzed
    from different technical perspectives. Moreover, quantitative and qualitative
    dehazing performance of various baselines are discussed. Finally, we discuss several
    valuable research directions and open issues.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文提供了基于深度学习的去雾研究的全面调查。首先，总结了常用的物理模型、高质量数据集、通用损失函数、有效的网络模块和评估指标。然后，从不同的技术角度对有监督、半监督和无监督的去雾研究进行了分类和分析。此外，讨论了各种基准方法的定量和定性去雾性能。最后，我们讨论了几个有价值的研究方向和未解决的问题。
- en: Acknowledgements.
  id: totrans-464
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work was supported in part by the grant of the National Science Foundation
    of China under Grant 62172090, 62172089; Alibaba Group through Alibaba Innovative
    Research Program; CAAI-Huawei MindSpore Open Fund. All correspondence should be
    directed to Xiaofeng Cong and Yuan Cao. Dr Jing Zhang is supported by ARC research
    project FL-170100117.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了中国国家自然科学基金（资助编号62172090, 62172089）；阿里巴巴集团通过阿里巴巴创新研究计划；CAAI-Huawei MindSpore开放基金的资助。所有通信应直接联系Xiaofeng
    Cong和Yuan Cao。Jing Zhang博士由ARC研究项目FL-170100117资助。
- en: Appendix A What is not discussed in this survey
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 本调查中未讨论的内容
- en: We noticed that several papers’ formulation description and open source code
    may not be consistent. In particular, loss functions not provided in several papers
    are used in the training code. In this case, we use the loss functions provided
    in the paper as a guide, regardless of the code implementation. Further, several
    public papers that have not yet been published formally are not covered in this
    survey, such as those on arxiv, since their content may be changed in the future.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到几篇论文的公式描述和开源代码可能不一致。特别是，有些论文中未提供的损失函数被用于训练代码中。在这种情况下，我们以论文中提供的损失函数作为指南，而不考虑代码实现。此外，尚未正式发表的几篇公共论文，如arxiv上的论文，未在本调查中涵盖，因为它们的内容可能在未来发生变化。
- en: References
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: An et al. (2022) Shunmin An, Xixia Huang, Le Wang, Linling Wang, and Zhangjing
    Zheng. 2022. Semi-Supervised image dehazing network. *The Visual Computer* 38,
    6 (2022), 2041–2055.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An等（2022）Shunmin An, Xixia Huang, Le Wang, Linling Wang, 和 Zhangjing Zheng。2022。半监督图像去雾网络。*视觉计算机*
    38, 6 (2022), 2041–2055。
- en: 'Ancuti et al. (2016) Cosmin Ancuti, Codruta O Ancuti, and Christophe De Vleeschouwer.
    2016. D-hazy: A dataset to evaluate quantitatively dehazing algorithms. In *International
    Conference on Image Processing*. 2226–2230.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancuti等（2016）Cosmin Ancuti, Codruta O Ancuti, 和 Christophe De Vleeschouwer。2016。D-hazy：一个定量评估去雾算法的数据集。见于*国际图像处理会议*。2226–2230。
- en: 'Ancuti et al. (2018a) Cosmin Ancuti, Codruta O Ancuti, and Radu Timofte. 2018a.
    Ntire 2018 challenge on image dehazing: Methods and results. In *Conference on
    Computer Vision and Pattern Recognition Workshops*. 891–901.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancuti等（2018a）Cosmin Ancuti, Codruta O Ancuti, 和 Radu Timofte。2018a。Ntire 2018挑战：图像去雾的方法和结果。见于*计算机视觉与模式识别会议研讨会*。891–901。
- en: 'Ancuti et al. (2018b) Cosmin Ancuti, Codruta O Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018b. I-HAZE: a dehazing benchmark with real hazy and haze-free
    indoor images. In *International Conference on Advanced Concepts for Intelligent
    Vision Systems*. 620–631.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ancuti et al. (2018b) Cosmin Ancuti, Codruta O Ancuti, Radu Timofte, 和 Christophe
    De Vleeschouwer. 2018b. I-HAZE: 一个真实雾霾和无雾霾室内图像的去雾基准。在 *国际智能视觉系统先进概念会议*。620–631。'
- en: 'Ancuti et al. (2019a) Codruta O Ancuti, Cosmin Ancuti, Mateu Sbert, and Radu
    Timofte. 2019a. Dense-Haze: A Benchmark for Image Dehazing with Dense-Haze and
    Haze-Free Images. In *International Conference on Image Processing*. 1014–1018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ancuti et al. (2019a) Codruta O Ancuti, Cosmin Ancuti, Mateu Sbert, 和 Radu
    Timofte. 2019a. Dense-Haze: 一个用于图像去雾的基准数据集，包括密集雾霾和无雾霾图像。在 *国际图像处理会议*。1014–1018。'
- en: 'Ancuti et al. (2020a) Codruta O. Ancuti, Cosmin Ancuti, and Radu Timofte. 2020a.
    NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1798–1805.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ancuti et al. (2020a) Codruta O. Ancuti, Cosmin Ancuti, 和 Radu Timofte. 2020a.
    NH-HAZE: 一个非均匀雾霾和无雾霾图像的去雾基准。在 *计算机视觉与模式识别会议工作坊*。1798–1805。'
- en: 'Ancuti et al. (2018c) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe
    De Vleeschouwer. 2018c. O-HAZE: a dehazing benchmark with real hazy and haze-free
    outdoor images. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    754–762.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ancuti et al. (2018c) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, 和 Christophe
    De Vleeschouwer. 2018c. O-HAZE: 一个真实雾霾和无雾霾户外图像的去雾基准。在 *计算机视觉与模式识别会议工作坊*。754–762。'
- en: Ancuti et al. (2019b) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool,
    Lei Zhang, and Ming-Hsuan Yang. 2019b. NTIRE 2019 Image Dehazing Challenge Report.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 2241–2253.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancuti et al. (2019b) Codruta O Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van
    Gool, Lei Zhang, 和 Ming-Hsuan Yang. 2019b. NTIRE 2019图像去雾挑战报告。在 *计算机视觉与模式识别会议工作坊*。2241–2253。
- en: Ancuti et al. (2020b) Codruta O Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu,
    and Radu Timofte. 2020b. NTIRE 2020 Challenge on NonHomogeneous Dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2029–2044.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancuti et al. (2020b) Codruta O Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu,
    和 Radu Timofte. 2020b. NTIRE 2020挑战赛：非均匀去雾。在 *计算机视觉与模式识别会议工作坊*。2029–2044。
- en: 'Banerjee and Chaudhuri (2021) Sriparna Banerjee and Sheli Sinha Chaudhuri.
    2021. Nighttime Image-Dehazing: A Review and Quantitative Benchmarking. *Archives
    of Computational Methods in Engineering* 28 (2021), 2943–2975.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Chaudhuri (2021) Sriparna Banerjee 和 Sheli Sinha Chaudhuri. 2021.
    夜间图像去雾：综述与定量基准。*计算方法与工程档案* 28 (2021), 2943–2975。
- en: Bianco et al. (2019) Simone Bianco, Luigi Celona, Flavio Piccoli, and Raimondo
    Schettini. 2019. High-resolution single image dehazing using encoder-decoder architecture.
    In *Conference on Computer Vision and Pattern Recognition Workshops*. 1927–1935.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianco et al. (2019) Simone Bianco, Luigi Celona, Flavio Piccoli, 和 Raimondo
    Schettini. 2019. 高分辨率单图像去雾使用编码器-解码器架构。在 *计算机视觉与模式识别会议工作坊*。1927–1935。
- en: 'Cai et al. (2016) Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and Dacheng
    Tao. 2016. Dehazenet: An end-to-end system for single image haze removal. *IEEE
    Transactions on Image Processing* 25, 11 (2016), 5187–5198.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. (2016) Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, 和 Dacheng
    Tao. 2016. Dehazenet: 一个端到端的单图像去雾系统。*IEEE图像处理学报* 25, 11 (2016), 5187–5198。'
- en: Chen et al. (2019c) Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Liheng
    Zhang, Dongdong Hou, Lu Yuan, and Gang Hua. 2019c. Gated Context Aggregation Network
    for Image Dehazing and Deraining. In *Winter Conference on Applications of Computer
    Vision*. 1375–1383.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019c) Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Liheng
    Zhang, Dongdong Hou, Lu Yuan, 和 Gang Hua. 2019c. 门控上下文聚合网络用于图像去雾和去雨。在 *冬季计算机视觉应用会议*。1375–1383。
- en: Chen and Lai (2019) Rongsen Chen and Edmund M-K Lai. 2019. Convolutional Autoencoder
    For Single Image Dehazing.. In *International Conference on Image Processing*.
    4464–4468.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Lai (2019) Rongsen Chen 和 Edmund M-K Lai. 2019. 卷积自编码器用于单图像去雾。在 *国际图像处理会议*。4464–4468。
- en: Chen et al. (2019a) Shuxin Chen, Yizi Chen, Yanyun Qu, Jingying Huang, and Ming
    Hong. 2019a. Multi-scale adaptive dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2051–2059.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019a) Shuxin Chen, Yizi Chen, Yanyun Qu, Jingying Huang, 和 Ming
    Hong. 2019a. 多尺度自适应去雾网络。在 *计算机视觉与模式识别会议工作坊*。2051–2059。
- en: 'Chen et al. (2021a) Tianyi Chen, Jiahui Fu, Wentao Jiang, Chen Gao, and Si
    Liu. 2021a. SRKTDN: Applying Super Resolution Method to Dehazing Task. In *Conference
    on Computer Vision and Pattern Recognition*. 487–496.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021a) Tianyi Chen, Jiahui Fu, Wentao Jiang, Chen Gao, 和 Si Liu.
    2021a. SRKTDN：将超分辨率方法应用于去雾任务。发表于 *计算机视觉与模式识别会议*。487–496。
- en: 'Chen et al. (2019b) Wei-Ting Chen, Jian-Jiun Ding, and Sy-Yen Kuo. 2019b. PMS-Net:
    Robust Haze Removal Based on Patch Map for Single Images. In *Conference on Computer
    Vision and Pattern Recognition*. 11673–11681.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019b) Wei-Ting Chen, Jian-Jiun Ding, 和 Sy-Yen Kuo. 2019b. PMS-Net：基于补丁映射的鲁棒去雾算法。发表于
    *计算机视觉与模式识别会议*。11673–11681。
- en: 'Chen et al. (2020) Wei-Ting Chen, Hao-Yu Fang, Jian-Jiun Ding, and Sy-Yen Kuo.
    2020. PMHLD: patch map-based hybrid learning DehazeNet for single image haze removal.
    *IEEE Transactions on Image Processing* 29 (2020), 6773–6788.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Wei-Ting Chen, Hao-Yu Fang, Jian-Jiun Ding, 和 Sy-Yen Kuo.
    2020. PMHLD：基于补丁映射的混合学习去雾网络用于单图像去雾。*IEEE 图像处理汇刊* 29 (2020)，6773–6788。
- en: Chen et al. (2019d) Xuesong Chen, Haihua Lu, Kaili Cheng, Yanbo Ma, Qiuhao Zhou,
    and Yong Zhao. 2019d. Sequentially refined spatial and channel-wise feature aggregation
    in encoder-decoder network for single image dehazing. In *International Conference
    on Image Processing*. 2776–2780.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019d) Xuesong Chen, Haihua Lu, Kaili Cheng, Yanbo Ma, Qiuhao Zhou,
    和 Yong Zhao. 2019d. 编码器-解码器网络中顺序精炼的空间和通道特征聚合用于单图像去雾。发表于 *国际图像处理会议*。2776–2780。
- en: 'Chen et al. (2021b) Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu. 2021b.
    PSD: Principled Synthetic-to-Real Dehazing Guided by Physical Priors. In *Conference
    on Computer Vision and Pattern Recognition*. 7180–7189.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021b) Zeyuan Chen, Yangchao Wang, Yang Yang, 和 Dong Liu. 2021b.
    PSD：由物理先验引导的有原则的合成到真实去雾。发表于 *计算机视觉与模式识别会议*。7180–7189。
- en: 'Chen et al. (2021c) Zhihua Chen, Yu Zhou, Ping Li, Xiaoyu Chi, Lei Ma, and
    Bin Sheng. 2021c. DCNet: Dual-Task Cycle Network for End-to-End Image Dehazing.
    In *International Conference on Multimedia and Expo*. 1–6.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021c) Zhihua Chen, Yu Zhou, Ping Li, Xiaoyu Chi, Lei Ma, 和 Bin
    Sheng. 2021c. DCNet：用于端到端图像去雾的双任务循环网络。发表于 *国际多媒体与博览会*。1–6。
- en: Cheng and Zhao (2021) Lu Cheng and Li Zhao. 2021. Two-Stage Image Dehazing with
    Depth Information and Cross-Scale Non-Local Attention. In *International Conference
    on Big Data*. 3155–3162.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng and Zhao (2021) Lu Cheng 和 Li Zhao. 2021. 基于深度信息和跨尺度非局部注意力的两阶段图像去雾。发表于
    *国际大数据会议*。3155–3162。
- en: Cong et al. (2020) Xiaofeng Cong, Jie Gui, Kai-Chao Miao, Jun Zhang, Bing Wang,
    and Peng Chen. 2020. Discrete Haze Level Dehazing Network. In *ACM International
    Conference on Multimedia*. 1828–1836.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cong et al. (2020) Xiaofeng Cong, Jie Gui, Kai-Chao Miao, Jun Zhang, Bing Wang,
    和 Peng Chen. 2020. 离散雾霾水平去雾网络。发表于 *ACM 国际多媒体会议*。1828–1836。
- en: Das and Dutta (2020) Sourya Dipta Das and Saikat Dutta. 2020. Fast deep multi-patch
    hierarchical network for nonhomogeneous image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 482–483.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das and Dutta (2020) Sourya Dipta Das 和 Saikat Dutta. 2020. 快速深度多补丁分层网络用于非均匀图像去雾。发表于
    *计算机视觉与模式识别会议研讨会*。482–483。
- en: 'Deng et al. (2020) Qili Deng, Ziling Huang, Chung-Chi Tsai, and Chia-Wen Lin.
    2020. Hardgan: A haze-aware representation distillation gan for single image dehazing.
    In *European Conference on Computer Vision*. 722–738.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2020) Qili Deng, Ziling Huang, Chung-Chi Tsai, 和 Chia-Wen Lin.
    2020. Hardgan：用于单图像去雾的雾感知表示蒸馏 GAN。发表于 *欧洲计算机视觉会议*。722–738。
- en: Deng et al. (2019) Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu,
    Qing Zhang, Jing Qin, and Pheng-Ann Heng. 2019. Deep multi-model fusion for single-image
    dehazing. In *International Conference on Computer Vision*. 2453–2462.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2019) Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu,
    Qing Zhang, Jing Qin, 和 Pheng-Ann Heng. 2019. 基于深度多模型融合的单图像去雾。发表于 *国际计算机视觉会议*。2453–2462。
- en: Dharejo et al. (2021) Fayaz Ali Dharejo, Yuanchun Zhou, Farah Deeba, Munsif Ali
    Jatoi, Muhammad Ashfaq Khan, Ghulam Ali Mallah, Abdul Ghaffar, Muhammad Chhattal,
    Yi Du, and Xuezhi Wang. 2021. A deep hybrid neural network for single image dehazing
    via wavelet transform. *Optik* 231 (2021), 166462.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dharejo et al. (2021) Fayaz Ali Dharejo, Yuanchun Zhou, Farah Deeba, Munsif
    Ali Jatoi, Muhammad Ashfaq Khan, Ghulam Ali Mallah, Abdul Ghaffar, Muhammad Chhattal,
    Yi Du, 和 Xuezhi Wang. 2021. 基于小波变换的单图像去雾深度混合神经网络。*Optik* 231 (2021)，166462。
- en: Dong et al. (2020b) Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang,
    Fei Wang, and Ming-Hsuan Yang. 2020b. Multi-scale boosted dehazing network with
    dense feature fusion. In *Conference on Computer Vision and Pattern Recognition*.
    2157–2167.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2020b) Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang,
    Fei Wang, 和 Ming-Hsuan Yang. 2020b. 多尺度增强去雾网络与密集特征融合。发表于 *计算机视觉与模式识别会议*。2157–2167。
- en: Dong et al. (2020c) Hang Dong, Xinyi Zhang, Yu Guo, and Fei Wang. 2020c. Deep
    multi-scale gabor wavelet network for image restoration. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2028–2032.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2020c) 董航、张鑫怡、郭宇、王飞。2020c年。深度多尺度Gabor小波网络用于图像恢复。发表于*国际声学、语音与信号处理会议*。2028–2032。
- en: Dong and Pan (2020) Jiangxin Dong and Jinshan Pan. 2020. Physics-based feature
    dehazing networks. In *European Conference on Computer Vision*. 188–204.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong and Pan (2020) 董江欣和潘锦山。2020年。基于物理的特征去雾网络。发表于*欧洲计算机视觉会议*。188–204。
- en: 'Dong et al. (2020a) Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, and Yu Qiao.
    2020a. FD-GAN: Generative Adversarial Networks with Fusion-Discriminator for Single
    Image Dehazing. In *AAAI Conference on Artificial Intelligence*. 10729–10736.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2020a) 董宇、刘易浩、张赫、陈世锋、乔宇。2020a年。FD-GAN: 带有融合判别器的生成对抗网络用于单幅图像去雾。发表于*AAAI人工智能会议*。10729–10736。'
- en: Du and Li (2018) Yixin Du and Xin Li. 2018. Recursive deep residual learning
    for single image dehazing. In *Conference on Computer Vision and Pattern Recognition
    Workshops*. 730–737.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du and Li (2018) 杜毅新和李鑫。2018年。用于单幅图像去雾的递归深度残差学习。发表于*计算机视觉与模式识别会议研讨会*。730–737。
- en: Du and Li (2019) Yixin Du and Xin Li. 2019. Recursive image dehazing via perceptually
    optimized generative adversarial network (POGAN). In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 1824–1832.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du and Li (2019) 杜毅新和李鑫。2019年。通过感知优化生成对抗网络（POGAN）的递归图像去雾。发表于*计算机视觉与模式识别会议研讨会*。1824–1832。
- en: 'Dudhane and Murala (2019a) Akshay Dudhane and Subrahmanyam Murala. 2019a. Cdnet:
    Single image de-hazing using unpaired adversarial training. In *Winter Conference
    on Applications of Computer Vision*. 1147–1155.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dudhane and Murala (2019a) 阿克谢·杜丹和苏布拉赫曼亚姆·穆拉拉。2019a年。Cdnet: 使用未配对对抗训练的单幅图像去雾。发表于*计算机视觉应用冬季会议*。1147–1155。'
- en: 'Dudhane and Murala (2019b) Akshay Dudhane and Subrahmanyam Murala. 2019b. RYF-Net:
    Deep fusion network for single image haze removal. *IEEE Transactions on Image
    Processing* 29 (2019), 628–640.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dudhane and Murala (2019b) 阿克谢·杜丹和苏布拉赫曼亚姆·穆拉拉。2019b年。RYF-Net: 用于单幅图像雾霾去除的深度融合网络。*IEEE图像处理学报*
    29 (2019)，628–640。'
- en: 'Dudhane et al. (2019) Akshay Dudhane, Harshjeet Singh Aulakh, and Subrahmanyam
    Murala. 2019. Ri-gan: An end-to-end network for single image haze removal. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 2014–2023.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dudhane et al. (2019) 阿克谢·杜丹、哈希杰特·辛格·奥拉赫和苏布拉赫曼亚姆·穆拉拉。2019年。Ri-gan: 一种端到端的单幅图像雾霾去除网络。发表于*计算机视觉与模式识别会议研讨会*。2014–2023。'
- en: 'Engin et al. (2018) Deniz Engin, Anil Genç, and Hazim Kemal Ekenel. 2018. Cycle-dehaze:
    Enhanced cyclegan for single image dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshops*. 825–833.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Engin et al. (2018) 德尼兹·恩金、安尼尔·根奇、哈兹姆·凯马尔·埃肯尔。2018年。Cycle-dehaze: 增强的CycleGAN用于单幅图像去雾。发表于*计算机视觉与模式识别会议研讨会*。825–833。'
- en: Fang et al. (2021) Zhengyun Fang, Ming Zhao, Zhengtao Yu, Meiyu Li, and Yong
    Yang. 2021. A guiding teaching and dual adversarial learning framework for a single
    image dehazing. *The Visual Computer* (2021), 1–13.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021) 方征云、赵铭、余正涛、李美玉、杨勇。2021年。用于单幅图像去雾的引导教学与双对抗学习框架。*视觉计算* (2021)，1–13。
- en: 'Fu et al. (2021) Minghan Fu, Huan Liu, Yankun Yu, Jun Chen, and Keyan Wang.
    2021. DW-GAN: A Discrete Wavelet Transform GAN for NonHomogeneous Dehazing. In
    *Conference on Computer Vision and Pattern Recognition Workshops*. 203–212.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2021) 傅名汉、刘欢、俞延坤、陈俊、王科言。2021年。DW-GAN: 用于非均匀去雾的离散小波变换GAN。发表于*计算机视觉与模式识别会议研讨会*。203–212。'
- en: Galdran et al. (2018) Adrian Galdran, Aitor Alvarez-Gila, Alessandro Bria, Javier
    Vazquez-Corral, and Marcelo Bertalmío. 2018. On the duality between retinex and
    image dehazing. In *Conference on Computer Vision and Pattern Recognition*. 8212–8221.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galdran et al. (2018) 阿德里安·加尔德兰、阿伊托尔·阿尔瓦雷斯-吉拉、亚历山德罗·布里亚、哈维尔·巴斯克斯-科拉尔、马塞洛·贝塔尔米奥。2018年。关于Retinex与图像去雾之间的对偶性。发表于*计算机视觉与模式识别会议*。8212–8221。
- en: 'Gandelsman et al. (2019) Yosef Gandelsman, Assaf Shocher, and Michal Irani.
    2019. “Double-DIP”: Unsupervised Image Decomposition via Coupled Deep-Image-Priors.
    In *Conference on Computer Vision and Pattern Recognition*. 11026–11035.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gandelsman et al. (2019) 约瑟夫·甘德尔斯曼、阿萨夫·肖赫和米哈尔·伊拉尼。2019年。“Double-DIP”：通过耦合深度图像先验进行无监督图像分解。发表于*计算机视觉与模式识别会议*。11026–11035。
- en: 'Gao et al. (2021) Li Gao, Jing Zhang, Lefei Zhang, and Dacheng Tao. 2021. Dsp:
    Dual soft-paste for unsupervised domain adaptive semantic segmentation. In *ACM
    International Conference on Multimedia*. 2825–2833.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2021) 李高、张静、张乐飞、陶大成。2021年。Dsp: 用于无监督领域自适应语义分割的双软粘贴。发表于*ACM国际多媒体会议*。2825–2833。'
- en: Golts et al. (2020) Alona Golts, Daniel Freedman, and Michael Elad. 2020. Unsupervised
    Single Image Dehazing Using Dark Channel Prior Loss. *IEEE Transactions on Image
    Processing* 29 (2020), 2692–2701.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golts et al. (2020) Alona Golts, Daniel Freedman, 和 Michael Elad。2020。利用暗通道先验损失的无监督单图像去雾。*IEEE图像处理汇刊*
    29 (2020), 2692–2701。
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. 2021. Knowledge distillation: A survey. *International Journal of Computer
    Vision* 129, 6 (2021), 1789–1819.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, 和 Dacheng Tao。2021。知识蒸馏：综述。*计算机视觉国际期刊*
    129, 6 (2021), 1789–1819。
- en: Gui et al. (2021) Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing
    Zhang, and Dacheng Tao. 2021. A Comprehensive Survey on Image Dehazing Based on
    Deep Learning. In *International Joint Conference on Artificial Intelligence*.
    4426–4433.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui et al. (2021) Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing
    Zhang, 和 Dacheng Tao。2021。基于深度学习的图像去雾全面综述。发表于*国际人工智能联合会议*。4426–4433。
- en: 'Gui et al. (2022) Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping
    Ye. 2022. A review on generative adversarial networks: Algorithms, theory, and
    applications. *IEEE Transactions on Knowledge and Data Engineering* (2022).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui et al. (2022) Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, 和 Jieping
    Ye。2022。生成对抗网络综述：算法、理论及应用。*IEEE知识与数据工程汇刊* (2022)。
- en: Guo et al. (2019a) Tiantong Guo, Venkateswararao Cherukuri, and Vishal Monga.
    2019a. Dense ‘123’ color enhancement dehazing network. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2131–2139.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2019a) Tiantong Guo, Venkateswararao Cherukuri, 和 Vishal Monga。2019a。密集‘123’色彩增强去雾网络。发表于*计算机视觉与模式识别会议工作坊*。2131–2139。
- en: Guo et al. (2019b) Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, and Vishal
    Monga. 2019b. Dense scene information estimation network for dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 2122–2130.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2019b) Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, 和 Vishal
    Monga。2019b。用于去雾的密集场景信息估计网络。发表于*计算机视觉与模式识别会议工作坊*。2122–2130。
- en: Guo and Monga (2020) Tiantong Guo and Vishal Monga. 2020. Reinforced depth-aware
    deep learning for single image dehazing. In *International Conference on Acoustics,
    Speech and Signal Processing*. 8891–8895.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo and Monga (2020) Tiantong Guo 和 Vishal Monga。2020。强化深度感知深度学习用于单图像去雾。发表于*国际声学、语音与信号处理会议*。8891–8895。
- en: 'Hambarde and Murala (2020) Praful Hambarde and Subrahmanyam Murala. 2020. S2dnet:
    Depth estimation from single image and sparse samples. *IEEE Transactions on Computational
    Imaging* 6 (2020), 806–817.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hambarde and Murala (2020) Praful Hambarde 和 Subrahmanyam Murala。2020。S2dnet：从单张图像和稀疏样本中进行深度估计。*IEEE计算成像汇刊*
    6 (2020), 806–817。
- en: He et al. (2010) Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Single image haze
    removal using dark channel prior. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence* 33, 12 (2010), 2341–2353.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2010) Kaiming He, Jian Sun, 和 Xiaoou Tang。2010。利用暗通道先验进行单图像雾霾去除。*IEEE模式分析与机器智能汇刊*
    33, 12 (2010), 2341–2353。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Conference on Computer Vision
    and Pattern Recognition*. 770–778.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun。2016。用于图像识别的深度残差学习。发表于*计算机视觉与模式识别会议*。770–778。
- en: He et al. (2019) Linyuan He, Junqiang Bai, and Meng Yang. 2019. Feature aggregation
    convolution network for haze removal. In *International Conference on Image Processing*.
    2806–2810.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2019) Linyuan He, Junqiang Bai, 和 Meng Yang。2019。用于雾霾去除的特征聚合卷积网络。发表于*国际图像处理会议*。2806–2810。
- en: Hong et al. (2020) Ming Hong, Yuan Xie, Cuihua Li, and Yanyun Qu. 2020. Distilling
    Image Dehazing With Heterogeneous Task Imitation. In *Conference on Computer Vision
    and Pattern Recognition*. 3462–3471.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. (2020) Ming Hong, Yuan Xie, Cuihua Li, 和 Yanyun Qu。2020。通过异质任务模仿进行图像去雾蒸馏。发表于*计算机视觉与模式识别会议*。3462–3471。
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. 2017. Densely connected convolutional networks. In *Conference on
    Computer Vision and Pattern Recognition*. 4700–4708.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, 和 Kilian
    Q Weinberger。2017。密集连接卷积网络。发表于*计算机视觉与模式识别会议*。4700–4708。
- en: Huang et al. (2019) Lu-Yao Huang, Jia-Li Yin, Bo-Hao Chen, and Shao-Zhen Ye.
    2019. Towards unsupervised single image dehazing with deep learning. In *International
    Conference on Image Processing*. 2741–2745.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2019) Lu-Yao Huang, Jia-Li Yin, Bo-Hao Chen, 和 Shao-Zhen Ye。2019。基于深度学习的无监督单图像去雾。发表于*国际图像处理会议*。2741–2745。
- en: Huang et al. (2021) Pengcheng Huang, Li Zhao, Runhua Jiang, Tao Wang, and Xiaoqin
    Zhang. 2021. Self-filtering image dehazing with self-supporting module. *Neurocomputing*
    432 (2021), 57–69.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2021）Pengcheng Huang, Li Zhao, Runhua Jiang, Tao Wang 和 Xiaoqin Zhang。2021。具有自支持模块的自滤波图像去雾。*神经计算*
    432（2021），57–69。
- en: Huang et al. (2018) Yimin Huang, Yiyang Wang, and Zhixun Su. 2018. Single image
    dehazing via a joint deep modeling. In *International Conference on Image Processing*.
    2840–2844.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2018）Yimin Huang, Yiyang Wang 和 Zhixun Su。2018。通过联合深度建模进行单幅图像去雾。发表于*国际图像处理会议*。2840–2844。
- en: Huynh-Thu and Ghanbari (2008) Quan Huynh-Thu and Mohammed Ghanbari. 2008. Scope
    of validity of PSNR in image/video quality assessment. *Electronics letters* 44,
    13 (2008), 800–801.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huynh-Thu 和 Ghanbari（2008）Quan Huynh-Thu 和 Mohammed Ghanbari。2008。PSNR 在图像/视频质量评估中的有效范围。*电子信函*
    44, 13（2008），800–801。
- en: Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    2017. Image-to-image translation with conditional adversarial networks. In *Conference
    on Computer Vision and Pattern Recognition*. 1125–1134.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等（2017）Phillip Isola, Jun-Yan Zhu, Tinghui Zhou 和 Alexei A Efros。2017。使用条件对抗网络的图像到图像翻译。发表于*计算机视觉与模式识别会议*。1125–1134。
- en: Jin et al. (2020) Yizhou Jin, Guangshuai Gao, Qingjie Liu, and Yunhong Wang.
    2020. Unsupervised conditional disentangle network for image dehazing. In *International
    Conference on Image Processing*. 963–967.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等（2020）Yizhou Jin, Guangshuai Gao, Qingjie Liu 和 Yunhong Wang。2020。无监督条件解缠网用于图像去雾。发表于*国际图像处理会议*。963–967。
- en: Jo and Sim (2021) Eunsung Jo and Jae-Young Sim. 2021. Multi-Scale Selective
    Residual Learning for Non-Homogeneous Dehazing. In *Conference on Computer Vision
    and Pattern Recognition Workshop*. 507–515.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jo 和 Sim（2021）Eunsung Jo 和 Jae-Young Sim。2021。用于非均匀去雾的多尺度选择性残差学习。发表于*计算机视觉与模式识别会议研讨会*。507–515。
- en: Johnson et al. (2016) Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
    Perceptual losses for real-time style transfer and super-resolution. In *European
    Conference on Computer Vision*. 694–711.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等（2016）Justin Johnson, Alexandre Alahi 和 Li Fei-Fei。2016。用于实时风格转换和超分辨率的感知损失。发表于*欧洲计算机视觉会议*。694–711。
- en: 'Ju et al. (2021) Mingye Ju, Can Ding, Wenqi Ren, Yi Yang, Dengyin Zhang, and
    Y Jay Guo. 2021. Ide: Image dehazing and exposure using an enhanced atmospheric
    scattering model. *IEEE Transactions on Image Processing* 30 (2021), 2180–2192.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等（2021）Mingye Ju, Can Ding, Wenqi Ren, Yi Yang, Dengyin Zhang 和 Y Jay Guo。2021。IDE：使用增强的大气散射模型进行图像去雾和曝光。*IEEE
    图像处理汇刊* 30（2021），2180–2192。
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. In *Neural Information Processing Systems*. 18661–18673.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等（2020）Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong
    Tian, Phillip Isola, Aaron Maschinot, Ce Liu 和 Dilip Krishnan。2020。监督对比学习。发表于*神经信息处理系统*。18661–18673。
- en: Kim et al. (2021) Guisik Kim, Sung Woo Park, and Junseok Kwon. 2021. Pixel-wise
    Wasserstein Autoencoder for Highly Generative Dehazing. *IEEE Transactions on
    Image Processing* 30 (2021), 5452–5462.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2021）Guisik Kim, Sung Woo Park 和 Junseok Kwon。2021。用于高生成性去雾的像素级 Wasserstein
    自编码器。*IEEE 图像处理汇刊* 30（2021），5452–5462。
- en: Ledig et al. (2017) Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero,
    Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
    Zehan Wang, et al. 2017. Photo-realistic single image super-resolution using a
    generative adversarial network. In *Conference on Computer Vision and Pattern
    Recognition*. 4681–4690.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ledig 等（2017）Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew
    Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan
    Wang 等。2017。使用生成对抗网络的照片真实单幅图像超分辨率。发表于*计算机视觉与模式识别会议*。4681–4690。
- en: Lee et al. (2020a) Byeong-Uk Lee, Kyunghyun Lee, Jean Oh, and In So Kweon. 2020a.
    CNN-Based Simultaneous Dehazing and Depth Estimation. In *International Conference
    on Robotics and Automation*. 9722–9728.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2020a）Byeong-Uk Lee, Kyunghyun Lee, Jean Oh 和 In So Kweon。2020a。基于 CNN
    的同时去雾和深度估计。发表于*国际机器人与自动化会议*。9722–9728。
- en: Lee et al. (2020b) Yean-Wei Lee, Lai-Kuan Wong, and John See. 2020b. Image Dehazing
    With Contextualized Attentive U-NET. In *International Conference on Image Processing*.
    1068–1072.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2020b）Yean-Wei Lee, Lai-Kuan Wong 和 John See。2020b。具有上下文注意力 U-NET 的图像去雾。发表于*国际图像处理会议*。1068–1072。
- en: 'Li et al. (2021a) Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi
    Zhou, and Xi Peng. 2021a. You only look yourself: Unsupervised and untrained single
    image dehazing neural network. *International Journal of Computer Vision* 129,
    5 (2021), 1754–1767.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2021a） Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi
    Zhou, 和 Xi Peng. 2021a. 你只看自己: 无监督和未训练的单张图像去雾神经网络。*International Journal of Computer
    Vision* 129, 5 (2021), 1754–1767。'
- en: Li et al. (2020b) Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi
    Zhou, and Xi Peng. 2020b. Zero-Shot Image Dehazing. *IEEE Transactions on Image
    Processing* 29 (2020), 8457–8466.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020b） Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi
    Zhou, 和 Xi Peng. 2020b. 零-shot 图像去雾。*IEEE Transactions on Image Processing* 29
    (2020), 8457–8466。
- en: 'Li et al. (2017a) Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan
    Feng. 2017a. AOD-Net: All-in-One Dehazing Network. In *International Conference
    on Computer Vision*. 4780–4788.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2017a） Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, 和 Dan Feng.
    2017a. AOD-Net: 一体化去雾网络。在 *International Conference on Computer Vision*。4780–4788。'
- en: Li et al. (2019c) Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun
    Zeng, and Zhangyang Wang. 2019c. Benchmarking Single-Image Dehazing and Beyond.
    *IEEE Transactions on Image Processing* 28, 1 (2019), 492–505.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2019c） Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng,
    和 Zhangyang Wang. 2019c. 单张图像去雾及其超越的基准测试。*IEEE Transactions on Image Processing*
    28, 1 (2019), 492–505。
- en: 'Li et al. (2019a) Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu,
    and Runmin Cong. 2019a. PDR-Net: Perception-inspired single image dehazing network
    with refinement. *IEEE Transactions on Multimedia* 22, 3 (2019), 704–716.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2019a） Chongyi Li, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu, 和 Runmin
    Cong. 2019a. PDR-Net: 基于感知的单张图像去雾网络与优化。*IEEE Transactions on Multimedia* 22, 3
    (2019), 704–716。'
- en: Li et al. (2016) Chongyi Li, Jichang Guo, Runmin Cong, Yanwei Pang, and Bo Wang.
    2016. Underwater image enhancement by dehazing with minimum information loss and
    histogram distribution prior. *IEEE Transactions on Image Processing* 25, 12 (2016),
    5664–5677.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2016） Chongyi Li, Jichang Guo, Runmin Cong, Yanwei Pang, 和 Bo Wang. 2016.
    通过最小信息丢失和直方图分布先验进行水下图像增强去雾。*IEEE Transactions on Image Processing* 25, 12 (2016),
    5664–5677。
- en: 'Li et al. (2021b) Hongyu Li, Jia Li, Dong Zhao, and Long Xu. 2021b. DehazeFlow:
    Multi-scale Conditional Flow Network for Single Image Dehazing. In *ACM International
    Conference on Multimedia*. 2577–2585.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2021b） Hongyu Li, Jia Li, Dong Zhao, 和 Long Xu. 2021b. DehazeFlow: 用于单张图像去雾的多尺度条件流网络。在
    *ACM International Conference on Multimedia*。2577–2585。'
- en: Li et al. (2020e) Hui Li, Qingbo Wu, King Ngi Ngan, Hongliang Li, and Fanman
    Meng. 2020e. Region adaptive two-shot network for single image dehazing. In *International
    Conference on Multimedia and Expo*. 1–6.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020e） Hui Li, Qingbo Wu, King Ngi Ngan, Hongliang Li, 和 Fanman Meng.
    2020e. 面向单张图像去雾的区域自适应双重网络。在 *International Conference on Multimedia and Expo*。1–6。
- en: Li et al. (2020a) Lerenhan Li, Yunlong Dong, Wenqi Ren, Jinshan Pan, Changxin
    Gao, Nong Sang, and Ming-Hsuan Yang. 2020a. Semi-Supervised Image Dehazing. *IEEE
    Transactions on Image Processing* 29 (2020), 2766–2779.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020a） Lerenhan Li, Yunlong Dong, Wenqi Ren, Jinshan Pan, Changxin Gao,
    Nong Sang, 和 Ming-Hsuan Yang. 2020a. 半监督图像去雾。*IEEE Transactions on Image Processing*
    29 (2020), 2766–2779。
- en: Li et al. (2021c) Pengyue Li, Jiandong Tian, Yandong Tang, Guolin Wang, and
    Chengdong Wu. 2021c. Deep Retinex Network for Single Image Dehazing. *IEEE Transactions
    on Image Processing* 30 (2021), 1100–1115.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2021c） Pengyue Li, Jiandong Tian, Yandong Tang, Guolin Wang, 和 Chengdong
    Wu. 2021c. 用于单张图像去雾的深度 Retinex 网络。*IEEE Transactions on Image Processing* 30 (2021),
    1100–1115。
- en: Li et al. (2020d) Runde Li, Jinshan Pan, Min He, Zechao Li, and Jinhui Tang.
    2020d. Task-oriented network for image dehazing. *IEEE Transactions on Image Processing*
    29 (2020), 6523–6534.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020d） Runde Li, Jinshan Pan, Min He, Zechao Li, 和 Jinhui Tang. 2020d.
    面向任务的图像去雾网络。*IEEE Transactions on Image Processing* 29 (2020), 6523–6534。
- en: Li et al. (2020c) Yuenan Li, Yuhang Liu, Qixin Yan, and Kuangshi Zhang. 2020c.
    Deep Dehazing Network With Latent Ensembling Architecture and Adversarial Learning.
    *IEEE Transactions on Image Processing* 30 (2020), 1354–1368.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020c） Yuenan Li, Yuhang Liu, Qixin Yan, 和 Kuangshi Zhang. 2020c. 具有潜在集成结构和对抗学习的深度去雾网络。*IEEE
    Transactions on Image Processing* 30 (2020), 1354–1368。
- en: 'Li et al. (2019b) Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Huijuan
    Fang, Chao Dong, and Yining Quan. 2019b. LAP-Net: Level-aware progressive network
    for image dehazing. In *International Conference on Computer Vision*. 3276–3285.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2019b） Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Huijuan Fang,
    Chao Dong, 和 Yining Quan. 2019b. LAP-Net: 级别感知渐进网络用于图像去雾。在 *International Conference
    on Computer Vision*。3276–3285。'
- en: 'Li et al. (2017b) Yu Li, Shaodi You, Michael S Brown, and Robby T Tan. 2017b.
    Haze visibility enhancement: A survey and quantitative benchmarking. *Computer
    Vision and Image Understanding* 165 (2017), 1–16.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017b) Yu Li, Shaodi You, Michael S Brown, and Robby T Tan. 2017b.
    雾霾可见性增强：综述与定量基准测试。*计算机视觉与图像理解* 165 (2017), 1–16。
- en: Liang et al. (2019) Xiao Liang, Runde Li, and Jinhui Tang. 2019. Selective Attention
    network for Image Dehazing and Deraining. In *ACM Multimedia Asia*. 1–6.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2019) Xiao Liang, Runde Li, and Jinhui Tang. 2019. 用于图像去雾和去雨的选择性注意力网络。在
    *ACM Multimedia Asia*。1–6。
- en: Liu et al. (2020b) Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020b. Trident dehazing network. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 430–431.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020b. Trident去雾网络。在 *计算机视觉与模式识别会议工作坊*。430–431。
- en: 'Liu et al. (2014) Lixiong Liu, Bao Liu, Hua Huang, and Alan Conrad Bovik. 2014.
    No-reference image quality assessment based on spatial and spectral entropies.
    *Signal processing: Image communication* 29, 8 (2014), 856–863.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2014) Lixiong Liu, Bao Liu, Hua Huang, and Alan Conrad Bovik. 2014.
    基于空间和光谱熵的无参考图像质量评估。*信号处理：图像通信* 29, 8 (2014), 856–863。
- en: Liu (2019) Qian Liu. 2019. Unsupervised Single Image Dehazing Via Disentangled
    Representation. In *International Conference on Video and Image Processing*. 106–111.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu (2019) Qian Liu. 2019. 基于解耦表示的无监督单幅图像去雾。在 *国际视频与图像处理会议*。106–111。
- en: Liu et al. (2018) Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan
    Luo, and Lei Zhang. 2018. Learning aggregated transmission propagation networks
    for haze removal and beyond. *IEEE Transactions on Neural Networks and Learning
    Systems* 30, 10 (2018), 2973–2986.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan
    Luo, and Lei Zhang. 2018. 学习聚合传输传播网络以去除雾霾及其他。在 *IEEE神经网络与学习系统汇刊* 30, 10 (2018),
    2973–2986。
- en: Liu et al. (2020a) Wei Liu, Xianxu Hou, Jiang Duan, and Guoping Qiu. 2020a.
    End-to-End Single Image Fog Removal Using Enhanced Cycle Consistent Adversarial
    Networks. *IEEE Transactions on Image Processing* 29 (2020), 7819–7833.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) Wei Liu, Xianxu Hou, Jiang Duan, and Guoping Qiu. 2020a.
    基于增强循环一致对抗网络的端到端单幅图像去雾。*IEEE图像处理汇刊* 29 (2020), 7819–7833。
- en: 'Liu et al. (2019a) Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. 2019a.
    GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing. In *International
    Conference on Computer Vision*. 7313–7322.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019a) Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. 2019a.
    GridDehazeNet：基于注意力的多尺度网络用于图像去雾。在 *国际计算机视觉会议*。7313–7322。
- en: Liu et al. (2019b) Yang Liu, Jinshan Pan, Jimmy Ren, and Zhixun Su. 2019b. Learning
    deep priors for image dehazing. In *International Conference on Computer Vision*.
    2492–2500.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019b) Yang Liu, Jinshan Pan, Jimmy Ren, and Zhixun Su. 2019b. 学习图像去雾的深度先验。在
    *国际计算机视觉会议*。2492–2500。
- en: 'Liu et al. (2021) Ye Liu, Lei Zhu, Shunda Pei, Huazhu Fu, Jing Qin, Qing Zhang,
    Liang Wan, and Wei Feng. 2021. From Synthetic to Real: Image Dehazing Collaborating
    with Unlabeled Real Data. In *ACM International Conference on Multimedia*. 50–58.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Ye Liu, Lei Zhu, Shunda Pei, Huazhu Fu, Jing Qin, Qing Zhang,
    Liang Wan, and Wei Feng. 2021. 从合成到真实：利用未标记真实数据进行图像去雾。在 *ACM国际多媒体会议*。50–58。
- en: 'McCartney (1976) Earl J McCartney. 1976. Optics of the atmosphere: scattering
    by molecules and particles. *Physics Bulletin* (1976), 1–421.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCartney (1976) Earl J McCartney. 1976. 大气光学：分子和颗粒的散射。*物理学公报* (1976), 1–421。
- en: 'Mehra et al. (2021) Aryan Mehra, Pratik Narang, and Murari Mandal. 2021. TheiaNet:
    Towards fast and inexpensive CNN design choices for image dehazing. *Journal of
    Visual Communication and Image Representation* 77 (2021), 103137.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehra et al. (2021) Aryan Mehra, Pratik Narang, and Murari Mandal. 2021. TheiaNet：针对图像去雾的快速且经济的CNN设计选择。*视觉通信与图像表示杂志*
    77 (2021), 103137。
- en: 'Mehta et al. (2020) Aditya Mehta, Harsh Sinha, Pratik Narang, and Murari Mandal.
    2020. Hidegan: A hyperspectral-guided image dehazing gan. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 212–213.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2020) Aditya Mehta, Harsh Sinha, Pratik Narang, and Murari Mandal.
    2020. Hidegan：一种高光谱引导的图像去雾生成对抗网络。在 *计算机视觉与模式识别会议工作坊*。212–213。
- en: Metwaly et al. (2020) Kareem Metwaly, Xuelu Li, Tiantong Guo, and Vishal Monga.
    2020. Nonlocal channel attention for nonhomogeneous image dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 452–453.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metwaly et al. (2020) Kareem Metwaly, Xuelu Li, Tiantong Guo, and Vishal Monga.
    2020. 非局部通道注意力用于非均质图像去雾。在 *计算机视觉与模式识别会议工作坊*。452–453。
- en: Min et al. (2019) Xiongkuo Min, Guangtao Zhai, Ke Gu, Yucheng Zhu, Jiantao Zhou,
    Guodong Guo, Xiaokang Yang, Xinping Guan, and Wenjun Zhang. 2019. Quality evaluation
    of image dehazing methods using synthetic hazy images. *IEEE Transactions on Multimedia*
    21, 9 (2019), 2319–2333.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2019) Xiongkuo Min, Guangtao Zhai, Ke Gu, Yucheng Zhu, Jiantao Zhou,
    Guodong Guo, Xiaokang Yang, Xinping Guan, 和 Wenjun Zhang. 2019. 使用合成雾霾图像的图像去雾方法质量评估。*IEEE
    多媒体汇刊* 21, 9 (2019), 2319–2333。
- en: 'Mo et al. (2022) Yaozong Mo, Chaofeng Li, Yuhui Zheng, and Xiaojun Wu. 2022.
    DCA-CycleGAN: Unsupervised Single Image Dehazing Using Dark Channel Attention
    Optimized CycleGAN. *Journal of Visual Communication and Image Representation*
    82 (2022), 103431.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mo et al. (2022) Yaozong Mo, Chaofeng Li, Yuhui Zheng, 和 Xiaojun Wu. 2022.
    DCA-CycleGAN: 使用暗通道注意力优化 CycleGAN 的无监督单幅图像去雾。*视觉通信与图像表示期刊* 82 (2022), 103431。'
- en: Mondal et al. (2018) Ranjan Mondal, Sanchayan Santra, and Bhabatosh Chanda.
    2018. Image dehazing by joint estimation of transmittance and airlight using bi-directional
    consistency loss minimized FCN. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 920–928.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mondal et al. (2018) Ranjan Mondal, Sanchayan Santra, 和 Bhabatosh Chanda. 2018.
    通过双向一致性损失最小化 FCN 联合估计透射率和空气光来去雾。发表于 *计算机视觉与模式识别会议工作坊*。920–928。
- en: Morales et al. (2019) Peter Morales, Tzofi Klinghoffer, and Seung Jae Lee. 2019.
    Feature forwarding for efficient single image dehazing. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 2078–2085.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morales et al. (2019) Peter Morales, Tzofi Klinghoffer, 和 Seung Jae Lee. 2019.
    高效单幅图像去雾的特征转发。发表于 *计算机视觉与模式识别会议工作坊*。2078–2085。
- en: Narasimhan and Nayar (2003) Srinivasa G Narasimhan and Shree K Nayar. 2003.
    Contrast restoration of weather degraded images. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 25, 6 (2003), 713–724.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narasimhan and Nayar (2003) Srinivasa G Narasimhan 和 Shree K Nayar. 2003. 天气退化图像的对比度恢复。*IEEE
    模式分析与机器智能汇刊* 25, 6 (2003), 713–724。
- en: Nayar and Narasimhan (1999) Shree K Nayar and Srinivasa G Narasimhan. 1999.
    Vision in bad weather. In *International Conference on Computer Vision*. 820–827.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nayar and Narasimhan (1999) Shree K Nayar 和 Srinivasa G Narasimhan. 1999. 恶劣天气中的视觉。发表于
    *国际计算机视觉会议*。820–827。
- en: 'Pang et al. (2020) Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, and Xuelong
    Li. 2020. BidNet: Binocular Image Dehazing Without Explicit Disparity Estimation.
    In *Conference on Computer Vision and Pattern Recognition*. 5930–5939.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang et al. (2020) Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, 和 Xuelong Li.
    2020. BidNet: 无需显式视差估计的双目图像去雾。发表于 *计算机视觉与模式识别会议*。5930–5939。'
- en: Pang et al. (2018) Yanwei Pang, Jin Xie, and Xuelong Li. 2018. Visual haze removal
    by a unified generative adversarial network. *IEEE Transactions on Circuits and
    Systems for Video Technology* 29, 11 (2018), 3211–3221.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang et al. (2018) Yanwei Pang, Jin Xie, 和 Xuelong Li. 2018. 通过统一的生成对抗网络进行视觉雾霾去除。*IEEE
    视频技术电路与系统汇刊* 29, 11 (2018), 3211–3221。
- en: Parihar et al. (2020) Anil Singh Parihar, Yash Kumar Gupta, Yash Singodia, Vibhu
    Singh, and Kavinder Singh. 2020. A comparative study of image dehazing algorithms.
    In *International Conference on Communication and Electronics Systems*. 766–771.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parihar et al. (2020) Anil Singh Parihar, Yash Kumar Gupta, Yash Singodia, Vibhu
    Singh, 和 Kavinder Singh. 2020. 图像去雾算法的比较研究。发表于 *国际通信与电子系统会议*。766–771。
- en: Park et al. (2020) Jaihyun Park, David K Han, and Hanseok Ko. 2020. Fusion of
    heterogeneous adversarial networks for single image dehazing. *IEEE Transactions
    on Image Processing* 29 (2020), 4721–4732.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2020) Jaihyun Park, David K Han, 和 Hanseok Ko. 2020. 异构对抗网络的融合用于单幅图像去雾。*IEEE
    图像处理汇刊* 29 (2020), 4721–4732。
- en: Pei et al. (2018) Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu, and Song Wang.
    2018. Does Haze Removal Help CNN-based Image Classification?. In *European Conference
    on Computer Vision*. 697–712.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pei et al. (2018) Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu, 和 Song Wang.
    2018. 雾霾去除是否有助于基于 CNN 的图像分类？发表于 *欧洲计算机视觉会议*。697–712。
- en: 'Qin et al. (2020) Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu
    Jia. 2020. FFA-Net: Feature Fusion Attention Network for Single Image Dehazing.
    In *AAAI Conference on Artificial Intelligence*. 11908–11915.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2020) Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, 和 Huizhu
    Jia. 2020. FFA-Net: 特征融合注意力网络用于单幅图像去雾。发表于 *AAAI 人工智能会议*。11908–11915。'
- en: Qu et al. (2019) Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced
    Pix2pix Dehazing Network. In *Conference on Computer Vision and Pattern Recognition*.
    8152–8160.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu et al. (2019) Yanyun Qu, Yizi Chen, Jingying Huang, 和 Yuan Xie. 2019. 增强的
    Pix2pix 去雾网络。发表于 *计算机视觉与模式识别会议*。8152–8160。
- en: Ren et al. (2016) Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and
    Ming-Hsuan Yang. 2016. Single image dehazing via multi-scale convolutional neural
    networks. In *European Conference on Computer Vision*. 154–169.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等（2016）Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, 和 Ming-Hsuan
    Yang。2016。《通过多尺度卷积神经网络进行单幅图像去雾》。发表于*欧洲计算机视觉会议*。154–169。
- en: Ren et al. (2018a) Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao,
    Wei Liu, and Ming-Hsuan Yang. 2018a. Gated Fusion Network for Single Image Dehazing.
    In *Conference on Computer Vision and Pattern Recognition*. 3253–3261.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等（2018a）Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu,
    和 Ming-Hsuan Yang。2018a。《用于单幅图像去雾的门控融合网络》。发表于*计算机视觉与模式识别会议*。3253–3261。
- en: Ren et al. (2020) Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, and Ming-Hsuan
    Yang. 2020. Single image dehazing via multi-scale convolutional neural networks
    with holistic edges. *International Journal of Computer Vision* 128, 1 (2020),
    240–259.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等（2020）Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, 和 Ming-Hsuan Yang。2020。《通过多尺度卷积神经网络与整体边缘进行单幅图像去雾》。*计算机视觉国际期刊*
    128, 1 (2020), 240–259。
- en: Ren et al. (2018b) Wenqi Ren, Jingang Zhang, Xiangyu Xu, Lin Ma, Xiaochun Cao,
    Gaofeng Meng, and Wei Liu. 2018b. Deep video dehazing with semantic segmentation.
    *IEEE Transactions on Image Processing* 28, 4 (2018), 1895–1908.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等（2018b）Wenqi Ren, Jingang Zhang, Xiangyu Xu, Lin Ma, Xiaochun Cao, Gaofeng
    Meng, 和 Wei Liu。2018b。《通过语义分割进行深度视频去雾》。*IEEE图像处理学报* 28, 4 (2018), 1895–1908。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical Image Computing and Computer-assisted Intervention*. 234–241.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger等（2015）Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox。2015。《U-net：用于生物医学图像分割的卷积网络》。发表于*医学图像计算与计算机辅助干预国际会议*。234–241。
- en: 'Rudin et al. (1992) Leonid I Rudin, Stanley Osher, and Emad Fatemi. 1992. Nonlinear
    total variation based noise removal algorithms. *Physica D: nonlinear phenomena*
    60, 1-4 (1992), 259–268.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudin等（1992）Leonid I Rudin, Stanley Osher, 和 Emad Fatemi。1992。《基于非线性全变差的噪声去除算法》。*物理学D：非线性现象*
    60, 1-4 (1992), 259–268。
- en: 'Saad et al. (2012) Michele A Saad, Alan C Bovik, and Christophe Charrier. 2012.
    Blind image quality assessment: A natural scene statistics approach in the DCT
    domain. *IEEE transactions on Image Processing* 21, 8 (2012), 3339–3352.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saad等（2012）Michele A Saad, Alan C Bovik, 和 Christophe Charrier。2012。《盲图像质量评估：基于DCT域自然场景统计的方法》。*IEEE图像处理学报*
    21, 8 (2012), 3339–3352。
- en: Sakaridis et al. (2018) Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc
    Van Gool. 2018. Model adaptation with synthetic and real data for semantic dense
    foggy scene understanding. In *European Conference on Computer Vision*. 687–704.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaridis等（2018）Christos Sakaridis, Dengxin Dai, Simon Hecker, 和 Luc Van Gool。2018。《利用合成和真实数据进行模型适应，以实现语义密集雾霾场景理解》。发表于*欧洲计算机视觉会议*。687–704。
- en: Shao et al. (2020) Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and Nong
    Sang. 2020. Domain Adaptation for Image Dehazing. In *Conference on Computer Vision
    and Pattern Recognition*. 2805–2814.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao等（2020）Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, 和 Nong Sang。2020。《图像去雾的领域自适应》。发表于*计算机视觉与模式识别会议*。2805–2814。
- en: 'Sharma et al. (2005) Gaurav Sharma, Wencheng Wu, and Edul N Dalal. 2005. The
    CIEDE2000 color-difference formula: Implementation notes, supplementary test data,
    and mathematical observations. *Color Research & Application: Endorsed by Inter-Society
    Color Council, etc* 30, 1 (2005), 21–30.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma等（2005）Gaurav Sharma, Wencheng Wu, 和 Edul N Dalal。2005。《CIEDE2000色差公式：实施说明、补充测试数据和数学观察》。*色彩研究与应用：由国际色彩委员会等机构认可*
    30, 1 (2005), 21–30。
- en: Sharma et al. (2020) Prasen Sharma, Priyankar Jain, and Arijit Sur. 2020. Scale-aware
    conditional generative adversarial network for image dehazing. In *Winter Conference
    on Applications of Computer Vision*. 2355–2365.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma等（2020）Prasen Sharma, Priyankar Jain, 和 Arijit Sur。2020。《用于图像去雾的尺度感知条件生成对抗网络》。发表于*计算机视觉应用冬季会议*。2355–2365。
- en: Sheng et al. (2022) Jiechao Sheng, Guoqiang Lv, Gang Du, Zi Wang, and Qibin
    Feng. 2022. Multi-scale residual attention network for single image dehazing.
    *Digital Signal Processing* 121 (2022), 103327.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng等（2022）Jiechao Sheng, Guoqiang Lv, Gang Du, Zi Wang, 和 Qibin Feng。2022。《用于单幅图像去雾的多尺度残差注意力网络》。*数字信号处理*
    121 (2022), 103327。
- en: Shin et al. (2022) Joongchol Shin, Hasil Park, and Joonki Paik. 2022. Region-Based
    Dehazing via Dual-Supervised Triple-Convolutional Network. *IEEE Transactions
    on Multimedia* 24 (2022), 245–260.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin等（2022）Joongchol Shin, Hasil Park, 和 Joonki Paik。2022。《通过双重监督的三卷积网络进行区域基础去雾》。*IEEE多媒体学报*
    24 (2022), 245–260。
- en: Shyam et al. (2021) Pranjay Shyam, Kuk-Jin Yoon, and Kyung-Soo Kim. 2021. Towards
    Domain Invariant Single Image Dehazing. In *AAAI Conference on Artificial Intelligence*.
    9657–9665.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shyam等人（2021）Pranjay Shyam, Kuk-Jin Yoon, 和 Kyung-Soo Kim. 2021. 朝着领域不变的单幅图像去雾。在*AAAI人工智能会议*。9657–9665。
- en: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. 2012. Indoor segmentation and support inference from rgbd images. In *European
    Conference on Computer Vision*. 746–760.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman等人（2012）Nathan Silberman, Derek Hoiem, Pushmeet Kohli, 和 Rob Fergus.
    2012. 从rgbd图像进行室内分割与支持推断。在*欧洲计算机视觉会议*。746–760。
- en: Sim et al. (2018) Hyeonjun Sim, Sehwan Ki, Jae-Seok Choi, Soomin Seo, Saehun
    Kim, and Munchurl Kim. 2018. High-resolution image dehazing with respect to training
    losses and receptive field sizes. In *Conference on Computer Vision and Pattern
    Recognition Workshops*. 912–919.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sim等人（2018）Hyeonjun Sim, Sehwan Ki, Jae-Seok Choi, Soomin Seo, Saehun Kim, 和
    Munchurl Kim. 2018. 高分辨率图像去雾关于训练损失和感受野大小。在*计算机视觉与模式识别研讨会*。912–919。
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. 2015. Very Deep
    Convolutional Networks for Large-Scale Image Recognition. In *International Conference
    on Learning Representations*. 1–14.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2015）K. Simonyan 和 A. Zisserman. 2015. 用于大规模图像识别的深度卷积网络。在*国际学习表征会议*。1–14。
- en: Singh et al. (2020) Ayush Singh, Ajay Bhave, and Dilip K Prasad. 2020. Single
    image dehazing for a variety of haze scenarios using back projected pyramid network.
    In *European Conference on Computer Vision*. 166–181.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人（2020）Ayush Singh, Ajay Bhave, 和 Dilip K Prasad. 2020. 使用反向投影金字塔网络处理多种雾霾场景的单幅图像去雾。在*欧洲计算机视觉会议*。166–181。
- en: Singh and Kumar (2019) Dilbag Singh and Vijay Kumar. 2019. A comprehensive review
    of computational dehazing techniques. *Archives of Computational Methods in Engineering*
    26, 5 (2019), 1395–1413.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh和Kumar（2019）Dilbag Singh 和 Vijay Kumar. 2019. 计算去雾技术的综合综述。*计算方法工程档案* 26,
    5（2019），1395–1413。
- en: 'Sun et al. (2021) Lexuan Sun, Xueliang Liu, Zhenzhen Hu, and Richang Hong.
    2021. WFN-PSC: weighted-fusion network with poly-scale convolution for image dehazing.
    In *ACM International Conference on Multimedia in Asia*. 1–7.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等人（2021）Lexuan Sun, Xueliang Liu, Zhenzhen Hu, 和 Richang Hong. 2021. WFN-PSC:
    加权融合网络与多尺度卷积用于图像去雾。在*ACM国际多媒体会议（亚洲）*。1–7。'
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Conference on Computer Vision and Pattern
    Recognition*. 1–9.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等人（2015）Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott
    Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, 和 Andrew Rabinovich.
    2015. 深度卷积网络。在*计算机视觉与模式识别会议*。1–9。
- en: Tang et al. (2019) Guiying Tang, Li Zhao, Runhua Jiang, and Xiaoqin Zhang. 2019.
    Single Image Dehazing via Lightweight Multi-scale Networks. In *International
    Conference on Big Data*. 5062–5069.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2019）Guiying Tang, Li Zhao, Runhua Jiang, 和 Xiaoqin Zhang. 2019. 通过轻量级多尺度网络进行单幅图像去雾。在*国际大数据会议*。5062–5069。
- en: 'Wang et al. (2018a) Anna Wang, Wenhui Wang, Jinglu Liu, and Nanhui Gu. 2018a.
    AIPNet: Image-to-image single image dehazing with atmospheric illumination prior.
    *IEEE Transactions on Image Processing* 28, 1 (2018), 381–393.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2018a）Anna Wang, Wenhui Wang, Jinglu Liu, 和 Nanhui Gu. 2018a. AIPNet:
    图像到图像的单幅图像去雾与大气照明先验。*IEEE图像处理汇刊* 28, 1（2018），381–393。'
- en: 'Wang et al. (2020) Cong Wang, Yuexian Zou, and Zehan Chen. 2020. ABC-NET: Avoiding
    Blocking Effect & Color Shift Network for Single Image Dehazing Via Restraining
    Transmission Bias. In *International Conference on Image Processing*. 1053–1057.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2020）Cong Wang, Yuexian Zou, 和 Zehan Chen. 2020. ABC-NET: 避免阻塞效应与颜色偏移网络用于单幅图像去雾，通过限制传输偏差。在*国际图像处理会议*。1053–1057。'
- en: Wang et al. (2021c) Juan Wang, Chang Ding, Minghu Wu, Yuanyuan Liu, and Guanhai
    Chen. 2021c. Lightweight multiple scale-patch dehazing network for real-world
    hazy image. *KSII Transactions on Internet and Information Systems* 15, 12 (2021),
    4420–4438.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2021c）Juan Wang, Chang Ding, Minghu Wu, Yuanyuan Liu, 和 Guanhai Chen.
    2021c. 轻量级多尺度补丁去雾网络用于现实世界的雾霾图像。*KSII互联网与信息系统汇刊* 15, 12（2021），4420–4438。
- en: Wang et al. (2021d) Jixiao Wang, Chaofeng Li, and Shoukun Xu. 2021d. An ensemble
    multi-scale residual attention network (EMRA-net) for image Dehazing. *Multimedia
    Tools and Applications* (2021), 29299–29319.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2021d）Jixiao Wang, Chaofeng Li, 和 Shoukun Xu. 2021d. 一种集成多尺度残差注意网络（EMRA-net）用于图像去雾。*多媒体工具与应用*（2021），29299–29319。
- en: Wang et al. (2019) Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. 2019.
    Distilling Object Detectors With Fine-Grained Feature Imitation. In *Conference
    on Computer Vision and Pattern Recognition*. 4928–4937.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019）Tao Wang、Li Yuan、Xiaopeng Zhang 和 Jiashi Feng。2019年。《通过细粒度特征模仿提炼目标检测器》。发表于
    *Conference on Computer Vision and Pattern Recognition*。4928–4937。
- en: Wang et al. (2021a) Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun
    Zha, Yonggang Wen, and Dacheng Tao. 2021a. Exploring Sequence Feature Alignment
    for Domain Adaptive Detection Transformers. In *ACM International Conference on
    Multimedia*. 1730–1738.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2021a）Wen Wang、Yang Cao、Jing Zhang、Fengxiang He、Zheng-Jun Zha、Yonggang
    Wen 和 Dacheng Tao。2021a年。《探索序列特征对齐用于领域自适应检测变换器》。发表于 *ACM International Conference
    on Multimedia*。1730–1738。
- en: Wang et al. (2022) Wen Wang, Jing Zhang, Wei Zhai, Yang Cao, and Dacheng Tao.
    2022. Robust Object Detection via Adversarial Novel Style Exploration. *IEEE Transactions
    on Image Processing* 31 (2022), 1949–1962.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022）Wen Wang、Jing Zhang、Wei Zhai、Yang Cao 和 Dacheng Tao。2022年。《通过对抗新风格探索进行鲁棒目标检测》。*IEEE
    Transactions on Image Processing* 31（2022），1949–1962。
- en: 'Wang et al. (2018b) Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
    Chao Dong, Yu Qiao, and Chen Change Loy. 2018b. Esrgan: Enhanced super-resolution
    generative adversarial networks. In *European Conference on Computer Vision*.
    63–79.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2018b）Xintao Wang、Ke Yu、Shixiang Wu、Jinjin Gu、Yihao Liu、Chao Dong、Yu
    Qiao 和 Chen Change Loy。2018b年。《Esrgan: 增强超分辨率生成对抗网络》。发表于 *European Conference
    on Computer Vision*。63–79。'
- en: Wang et al. (2021b) Yang Wang, Yang Cao, Jing Zhang, Feng Wu, and Zheng-Jun
    Zha. 2021b. Leveraging Deep Statistics for Underwater Image Enhancement. *ACM
    Transactions on Multimedia Computing, Communications, and Applications* 17, 3s
    (2021), 1–20.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2021b）Yang Wang、Yang Cao、Jing Zhang、Feng Wu 和 Zheng-Jun Zha。2021b年。《利用深度统计进行水下图像增强》。*ACM
    Transactions on Multimedia Computing, Communications, and Applications* 17, 3s（2021），1–20。
- en: Wang et al. (2017) Yang Wang, Jing Zhang, Yang Cao, and Zengfu Wang. 2017. A
    deep CNN method for underwater image enhancement. In *IEEE International Conference
    on Image Processing*. 1382–1386.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2017）Yang Wang、Jing Zhang、Yang Cao 和 Zengfu Wang。2017年。《一种深度CNN方法用于水下图像增强》。发表于
    *IEEE International Conference on Image Processing*。1382–1386。
- en: Wang et al. (2013) Yinting Wang, Shaojie Zhuo, Dapeng Tao, Jiajun Bu, and Na
    Li. 2013. Automatic local exposure correction using bright channel prior for under-exposed
    images. *Signal processing* 93, 11 (2013), 3227–3238.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2013）Yinting Wang、Shaojie Zhuo、Dapeng Tao、Jiajun Bu 和 Na Li。2013年。《使用亮通道先验进行自动局部曝光校正以应对曝光不足的图像》。*Signal
    processing* 93, 11（2013），3227–3238。
- en: 'Wang et al. (2004) Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli,
    et al. 2004. Image quality assessment: from error visibility to structural similarity.
    *IEEE Transactions on Image Processing* 13, 4 (2004), 600–612.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2004）Zhou Wang、Alan C Bovik、Hamid R Sheikh、Eero P Simoncelli 等人。2004年。《图像质量评估：从错误可见性到结构相似性》。*IEEE
    Transactions on Image Processing* 13, 4（2004），600–612。
- en: Wang et al. (2003) Zhou Wang, Eero P Simoncelli, and Alan C Bovik. 2003. Multiscale
    structural similarity for image quality assessment. In *Asilomar Conference on
    Signals, Systems & Computers*. 1398–1402.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2003）周望、Eero P Simoncelli 和 Alan C Bovik。2003年。《多尺度结构相似性用于图像质量评估》。发表于
    *Asilomar Conference on Signals, Systems & Computers*。1398–1402。
- en: Wei et al. (2020) Haoran Wei, Qingbo Wu, Hui Li, King Ngi Ngan, Hongliang Li,
    and Fanman Meng. 2020. Single Image Dehazing via Artificial Multiple Shots and
    Multidimensional Context. In *International Conference on Image Processing*. 1023–1027.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2020）Haoran Wei、Qingbo Wu、Hui Li、King Ngi Ngan、Hongliang Li 和 Fanman
    Meng。2020年。《通过人工多次拍摄和多维上下文进行单图像去雾》。发表于 *International Conference on Image Processing*。1023–1027。
- en: 'Wei et al. (2021) Pan Wei, Xin Wang, Lei Wang, and Ji Xiang. 2021. SIDGAN:
    Single Image Dehazing without Paired Supervision. In *International Conference
    on Pattern Recognition*. 2958–2965.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人（2021）Pan Wei、Xin Wang、Lei Wang 和 Ji Xiang。2021年。《SIDGAN: 无需配对监督的单图像去雾》。发表于
    *International Conference on Pattern Recognition*。2958–2965。'
- en: Wu et al. (2020) Haiyan Wu, Jing Liu, Yuan Xie, Yanyun Qu, and Lizhuang Ma.
    2020. Knowledge transfer dehazing network for nonhomogeneous dehazing. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 478–479.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2020）Haiyan Wu、Jing Liu、Yuan Xie、Yanyun Qu 和 Lizhuang Ma。2020年。《用于非均匀去雾的知识转移去雾网络》。发表于
    *Conference on Computer Vision and Pattern Recognition Workshops*。478–479。
- en: Wu et al. (2021) Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao,
    Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. 2021. Contrastive Learning for Compact
    Single Image Dehazing. In *Conference on Computer Vision and Pattern Recognition*.
    10551–10560.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2021）Haiyan Wu、Yanyun Qu、Shaohui Lin、Jian Zhou、Ruizhi Qiao、Zhizhong Zhang、Yuan
    Xie 和 Lizhuang Ma。2021年。《用于紧凑单图像去雾的对比学习》。发表于 *Conference on Computer Vision and
    Pattern Recognition*。10551–10560。
- en: Xiao et al. (2020) Jinsheng Xiao, Mengyao Shen, Junfeng Lei, Jinglong Zhou,
    Reinhard Klette, and HaiGang Sui. 2020. Single image dehazing based on learning
    of haze layers. *Neurocomputing* 389 (2020), 108–122.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2020) Jinsheng Xiao, Mengyao Shen, Junfeng Lei, Jinglong Zhou,
    Reinhard Klette, 和 HaiGang Sui. 2020. 基于雾层学习的单幅图像去雾。*Neurocomputing* 389 (2020),
    108–122。
- en: 'Xie et al. (2020) Liangru Xie, Hao Wang, Zhuowei Wang, and Lianglun Cheng.
    2020. DHD-Net: A Novel Deep-Learning-based Dehazing Network. In *International
    Joint Conference on Neural Networks*. 1–7.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. (2020) Liangru Xie, Hao Wang, Zhuowei Wang, 和 Lianglun Cheng. 2020.
    DHD-Net: 一种新型深度学习去雾网络。在 *International Joint Conference on Neural Networks* 中。1–7。'
- en: Xu et al. (2015) Yong Xu, Jie Wen, Lunke Fei, and Zheng Zhang. 2015. Review
    of video and image defogging algorithms and related studies on image restoration
    and enhancement. *IEEE Access* 4 (2015), 165–188.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2015) Yong Xu, Jie Wen, Lunke Fei, 和 Zheng Zhang. 2015. 视频和图像去雾算法及图像恢复和增强相关研究的综述。*IEEE
    Access* 4 (2015), 165–188。
- en: Yan et al. (2020) Lan Yan, Wenbo Zheng, Chao Gou, and Fei-Yue Wang. 2020. Feature
    Aggregation Attention Network for Single Image Dehazing. In *International Conference
    on Image Processing*. 923–927.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. (2020) Lan Yan, Wenbo Zheng, Chao Gou, 和 Fei-Yue Wang. 2020. 单幅图像去雾的特征聚合注意力网络。在
    *International Conference on Image Processing* 中。923–927。
- en: Yang et al. (2019) Aiping Yang, Haixin Wang, Zhong Ji, Yanwei Pang, and Ling
    Shao. 2019. Dual-Path in Dual-Path Network for Single Image Dehazing. In *International
    Joint Conference on Artificial Intelligence*. 4627–4634.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Aiping Yang, Haixin Wang, Zhong Ji, Yanwei Pang, 和 Ling Shao.
    2019. 在 *International Joint Conference on Artificial Intelligence* 中的“双路径网络中的双路径”。4627–4634。
- en: Yang and Zhang (2022) Fei Yang and Qian Zhang. 2022. Depth aware image dehazing.
    *The Visual Computer* 38, 5 (2022), 1579–1587.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang and Zhang (2022) Fei Yang 和 Qian Zhang. 2022. 深度感知图像去雾。*The Visual Computer*
    38, 5 (2022), 1579–1587。
- en: Yang and Fu (2019) Hao-Hsiang Yang and Yanwei Fu. 2019. Wavelet u-net and the
    chromatic adaptation transform for single image dehazing. In *International Conference
    on Image Processing*. 2736–2740.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang and Fu (2019) Hao-Hsiang Yang 和 Yanwei Fu. 2019. 小波 u-net 和单幅图像去雾的色彩适配变换。在
    *International Conference on Image Processing* 中。2736–2740。
- en: 'Yang et al. (2020) Hao-Hsiang Yang, Chao-Han Huck Yang, and Yi-Chang James
    Tsai. 2020. Y-net: Multi-scale feature aggregation network with wavelet structure
    similarity loss function for single image dehazing. In *International Conference
    on Acoustics, Speech and Signal Processing*. 2628–2632.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2020) Hao-Hsiang Yang, Chao-Han Huck Yang, 和 Yi-Chang James Tsai.
    2020. Y-net: 具有小波结构相似性损失函数的多尺度特征聚合网络用于单幅图像去雾。在 *International Conference on Acoustics,
    Speech and Signal Processing* 中。2628–2632。'
- en: Yeh et al. (2019) Chia-Hung Yeh, Chih-Hsiang Huang, and Li-Wei Kang. 2019. Multi-scale
    deep residual learning-based single image haze removal via image decomposition.
    *IEEE Transactions on Image Processing* 29 (2019), 3153–3167.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeh et al. (2019) Chia-Hung Yeh, Chih-Hsiang Huang, 和 Li-Wei Kang. 2019. 基于多尺度深度残差学习的单幅图像雾去除通过图像分解。*IEEE
    Transactions on Image Processing* 29 (2019), 3153–3167。
- en: Yin et al. (2019) Jia-Li Yin, Yi-Chi Huang, Bo-Hao Chen, and Shao-Zhen Ye. 2019.
    Color transferred convolutional neural networks for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 11 (2019), 3957–3967.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2019) Jia-Li Yin, Yi-Chi Huang, Bo-Hao Chen, 和 Shao-Zhen Ye. 2019.
    用于图像去雾的颜色转移卷积神经网络。*IEEE Transactions on Circuits and Systems for Video Technology*
    30, 11 (2019), 3957–3967。
- en: Yin et al. (2020) Shibai Yin, Yibin Wang, and Yee-Hong Yang. 2020. A novel image-dehazing
    network with a parallel attention block. *Pattern Recognition* 102 (2020), 107255.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2020) Shibai Yin, Yibin Wang, 和 Yee-Hong Yang. 2020. 一种新颖的图像去雾网络，具有并行注意力块。*Pattern
    Recognition* 102 (2020), 107255。
- en: Yin et al. (2021) Shibai Yin, Xiaolong Yang, Yibin Wang, and Yee-Hong Yang.
    2021. Visual Attention Dehazing Network with Multi-level Features Refinement and
    Fusion. *Pattern Recognition* 118 (2021), 108021.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2021) Shibai Yin, Xiaolong Yang, Yibin Wang, 和 Yee-Hong Yang. 2021.
    具有多层特征精炼和融合的视觉注意力去雾网络。*Pattern Recognition* 118 (2021), 108021。
- en: Yu et al. (2020) Mingzhao Yu, Venkateswararao Cherukuri, Tiantong Guo, and Vishal
    Monga. 2020. Ensemble dehazing networks for non-homogeneous haze. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 450–451.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2020) Mingzhao Yu, Venkateswararao Cherukuri, Tiantong Guo, 和 Vishal
    Monga. 2020. 非均匀雾的集成去雾网络。在 *Conference on Computer Vision and Pattern Recognition
    Workshops* 中。450–451。
- en: Yu et al. (2021) Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, and
    Keyan Wang. 2021. A Two-branch Neural Network for Non-homogeneous Dehazing via
    Ensemble Learning. In *Conference on Computer Vision and Pattern Recognition Workshops*.
    193–202.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021) Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, 和 Keyan
    Wang. 2021. 一种通过集成学习进行非均匀去雾的双分支神经网络。在 *Conference on Computer Vision and Pattern
    Recognition Workshops* 中。193–202。
- en: Zhang and Patel (2018) He Zhang and Vishal M Patel. 2018. Densely connected
    pyramid dehazing network. In *Conference on Computer Vision and Pattern Recognition*.
    3194–3203.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Patel (2018) He Zhang 和 Vishal M Patel。2018。密集连接的金字塔去雾网络。见于 *计算机视觉与模式识别会议*。3194–3203。
- en: Zhang et al. (2018b) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2018b.
    Multi-scale single image dehazing using perceptual pyramid deep network. In *Conference
    on Computer Vision and Pattern Recognition Workshops*. 902–911.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018b) He Zhang、Vishwanath Sindagi 和 Vishal M Patel。2018b。使用感知金字塔深度网络的多尺度单图像去雾。见于
    *计算机视觉与模式识别会议研讨会*。902–911。
- en: Zhang et al. (2019a) He Zhang, Vishwanath Sindagi, and Vishal M Patel. 2019a.
    Joint transmission map estimation and dehazing using deep networks. *IEEE Transactions
    on Circuits and Systems for Video Technology* 30, 7 (2019), 1975–1986.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019a) He Zhang、Vishwanath Sindagi 和 Vishal M Patel。2019a。使用深度网络的联合透射图估计与去雾。*IEEE
    电路与系统视频技术学报* 30, 7 (2019)，1975–1986。
- en: Zhang et al. (2017a) Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, and Chang Wen Chen.
    2017a. Fast haze removal for nighttime image using maximum reflectance prior.
    In *Conference on Computer Vision and Pattern Recognition*. 7418–7426.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2017a) Jing Zhang、Yang Cao、Shuai Fang、Yu Kang 和 Chang Wen Chen。2017a。利用最大反射率先验进行夜间图像快速雾霾去除。见于
    *计算机视觉与模式识别会议*。7418–7426。
- en: Zhang et al. (2014) Jing Zhang, Yang Cao, and Zengfu Wang. 2014. Nighttime haze
    removal based on a new imaging model. In *International Conference on Image Processing*.
    4557–4561.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2014) Jing Zhang、Yang Cao 和 Zengfu Wang。2014。基于新成像模型的夜间雾霾去除。见于 *国际图像处理会议*。4557–4561。
- en: Zhang et al. (2020a) Jing Zhang, Yang Cao, Zheng-Jun Zha, and Dacheng Tao. 2020a.
    Nighttime dehazing with a synthetic benchmark. In *ACM International Conference
    on Multimedia*. 2355–2363.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020a) Jing Zhang、Yang Cao、Zheng-Jun Zha 和 Dacheng Tao。2020a。带有合成基准的夜间去雾。见于
    *ACM 国际多媒体会议*。2355–2363。
- en: Zhang et al. (2022b) Jingang Zhang, Wenqi Ren, Shengdong Zhang, He Zhang, Yunfeng
    Nie, Zhe Xue, and Xiaochun Cao. 2022b. Hierarchical Density-Aware Dehazing Network.
    *IEEE Transactions on Cybernetics* 52, 10 (2022), 11187–11199.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022b) Jingang Zhang、Wenqi Ren、Shengdong Zhang、He Zhang、Yunfeng Nie、Zhe
    Xue 和 Xiaochun Cao。2022b。层次化密度感知去雾网络。*IEEE 网络系统学报* 52, 10 (2022)，11187–11199。
- en: 'Zhang and Tao (2020) Jing Zhang and Dacheng Tao. 2020. FAMED-Net: A Fast and
    Accurate Multi-Scale End-to-End Dehazing Network. *IEEE Transactions on Image
    Processing* 29 (2020), 72–84.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Tao (2020) Jing Zhang 和 Dacheng Tao。2020。FAMED-Net：一种快速准确的多尺度端到端去雾网络。*IEEE
    图像处理学报* 29 (2020)，72–84。
- en: Zhang and Li (2021) Kuangshi Zhang and Yuenan Li. 2021. Single image dehazing
    via semi-supervised domain translation and architecture search. *IEEE Signal Processing
    Letters* 28 (2021), 2127–2131.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Li (2021) Kuangshi Zhang 和 Yuenan Li。2021。通过半监督领域转换和架构搜索的单幅图像去雾。*IEEE
    信号处理快报* 28 (2021)，2127–2131。
- en: Zhang et al. (2019b) Qiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao. 2019b.
    Category anchor-guided unsupervised domain adaptation for semantic segmentation.
    In *Neural Information Processing Systems*. 433–443.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019b) Qiming Zhang、Jing Zhang、Wei Liu 和 Dacheng Tao。2019b。类别锚点引导的无监督领域适应用于语义分割。见于
    *神经信息处理系统*。433–443。
- en: 'Zhang and He (2020) Shengdong Zhang and Fazhi He. 2020. DRCDN: learning deep
    residual convolutional dehazing networks. *The Visual Computer* 36 (2020), 1797–1808.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 He (2020) Shengdong Zhang 和 Fazhi He。2020。DRCDN：学习深度残差卷积去雾网络。*视觉计算*
    36 (2020)，1797–1808。
- en: 'Zhang et al. (2020b) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020b. NLDN:
    Non-local dehazing network for dense haze removal. *Neurocomputing* 410 (2020),
    363–373.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020b) Shengdong Zhang、Fazhi He 和 Wenqi Ren。2020b。NLDN：用于密集雾霾去除的非局部去雾网络。*神经计算*
    410 (2020)，363–373。
- en: Zhang et al. (2020c) Shengdong Zhang, Fazhi He, and Wenqi Ren. 2020c. Photo-realistic
    dehazing via contextual generative adversarial networks. *Machine Vision and Applications*
    31 (2020), 1–12.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020c) Shengdong Zhang、Fazhi He 和 Wenqi Ren。2020c。通过上下文生成对抗网络实现真实感去雾。*机器视觉与应用*
    31 (2020)，1–12。
- en: Zhang et al. (2020d) Shengdong Zhang, Fazhi He, Wenqi Ren, and Jian Yao. 2020d.
    Joint learning of image detail and transmission map for single image dehazing.
    *The Visual Computer* 36, 2 (2020), 305–316.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020d) Shengdong Zhang、Fazhi He、Wenqi Ren 和 Jian Yao。2020d。单幅图像去雾的图像细节与透射图联合学习。*视觉计算*
    36, 2 (2020)，305–316。
- en: Zhang et al. (2022a) Shengdong Zhang, Wenqi Ren, Xin Tan, Zhi-Jie Wang, Yong
    Liu, Jingang Zhang, Xiaoqin Zhang, and Xiaochun Cao. 2022a. Semantic-aware dehazing
    network with adaptive feature fusion. *IEEE Transactions on Cybernetics* (2022).
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2022a）Shengdong Zhang、Wenqi Ren、Xin Tan、Zhi-Jie Wang、Yong Liu、Jingang
    Zhang、Xiaoqin Zhang和Xiaochun Cao。2022a。《基于语义感知的去雾网络与自适应特征融合》。*IEEE控制论学报*（2022）。
- en: 'Zhang et al. (2018a) Shengdong Zhang, Wenqi Ren, and Jian Yao. 2018a. Feed-Net:
    Fully End-to-End Dehazing. In *International Conference on Multimedia and Expo*.
    1–6.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018a）Shengdong Zhang、Wenqi Ren和Jian Yao。2018a。《Feed-Net：完全端到端去雾》。在*国际多媒体与博览会会议*上，1–6。
- en: Zhang et al. (2020f) Shengdong Zhang, Yue Wu, Yuanjie Zhao, Zuomin Cheng, and
    Wenqi Ren. 2020f. Color-constrained dehazing model. In *Conference on Computer
    Vision and Pattern Recognition Workshops*. 870–871.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020f）Shengdong Zhang、Yue Wu、Yuanjie Zhao、Zuomin Cheng和Wenqi Ren。2020f。《颜色约束去雾模型》。在*计算机视觉与模式识别会议研讨会*上，870–871。
- en: Zhang et al. (2021a) Xiaoqin Zhang, Runhua Jiang, Tao Wang, and Wenhan Luo.
    2021a. Single Image Dehazing via Dual-Path Recurrent Network. *IEEE Transactions
    on Image Processing* 30 (2021), 5211–5222.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021a）Xiaoqin Zhang、Runhua Jiang、Tao Wang和Wenhan Luo。2021a。《通过双路径递归网络进行单张图像去雾》。*IEEE图像处理学报*
    30（2021），5211–5222。
- en: Zhang et al. (2022c) Xiaoqin Zhang, Jinxin Wang, Tao Wang, and Runhua Jiang.
    2022c. Hierarchical feature fusion with mixed convolution attention for single
    image dehazing. *IEEE Transactions on Circuits and Systems for Video Technology*
    32, 2 (2022), 510–522.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2022c）Xiaoqin Zhang、Jinxin Wang、Tao Wang和Runhua Jiang。2022c。《具有混合卷积注意的分层特征融合用于单张图像去雾》。*IEEE视频技术电路与系统学报*
    32，2（2022），510–522。
- en: Zhang et al. (2021b) Xiaoqin Zhang, Tao Wang, Wenhan Luo, and Pengcheng Huang.
    2021b. Multi-level fusion and attention-guided CNN for image dehazing. *IEEE Transactions
    on Circuits and Systems for Video Technology* 31, 11 (2021), 4162–4173.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021b）Xiaoqin Zhang、Tao Wang、Wenhan Luo和Pengcheng Huang。2021b。《多层次融合和注意力引导CNN用于图像去雾》。*IEEE视频技术电路与系统学报*
    31，11（2021），4162–4173。
- en: Zhang et al. (2020e) Xiaoqin Zhang, Tao Wang, Jinxin Wang, Guiying Tang, and
    Li Zhao. 2020e. Pyramid Channel-based Feature Attention Network for image dehazing.
    *Computer Vision and Image Understanding* 197-198 (2020), 103003.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020e）Xiaoqin Zhang、Tao Wang、Jinxin Wang、Guiying Tang和Li Zhao。2020e。《基于金字塔通道的特征注意网络用于图像去雾》。*计算机视觉与图像理解*
    197-198（2020），103003。
- en: 'Zhang et al. (2017b) Yanfu Zhang, Li Ding, and Gaurav Sharma. 2017b. HazeRD:
    An outdoor scene dataset and benchmark for single image dehazing. In *International
    Conference on Image Processing*. 3205–3209.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2017b）Yanfu Zhang、Li Ding和Gaurav Sharma。2017b。《HazeRD：用于单张图像去雾的户外场景数据集和基准》。在*国际图像处理会议*上，3205–3209。
- en: Zhang et al. (2020g) Zhengxi Zhang, Liang Zhao, Yunan Liu, Shanshan Zhang, and
    Jian Yang. 2020g. Unified Density-Aware Image Dehazing and Object Detection in
    Real-World Hazy Scenes. In *Asian Conference on Computer Vision*. 119–135.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020g）Zhengxi Zhang、Liang Zhao、Yunan Liu、Shanshan Zhang和Jian Yang。2020g。《在真实世界雾霾场景中的统一密度感知图像去雾和目标检测》。在*亚洲计算机视觉会议*上，119–135。
- en: Zhao et al. (2021a) Dong Zhao, Long Xu, Lin Ma, Jia Li, and Yihua Yan. 2021a.
    Pyramid Global Context Network for Image Dehazing. *IEEE Transactions on Circuits
    and Systems for Video Technology* 31, 8 (2021), 3037–3050.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2021a）Dong Zhao、Long Xu、Lin Ma、Jia Li和Yihua Yan。2021a。《用于图像去雾的金字塔全局上下文网络》。*IEEE视频技术电路与系统学报*
    31，8（2021），3037–3050。
- en: 'Zhao et al. (2020) Shiyu Zhao, Lin Zhang, Shuaiyi Huang, Ying Shen, and Shengjie
    Zhao. 2020. Dehazing Evaluation: Real-World Benchmark Datasets, Criteria, and
    Baselines. *IEEE Transactions on Image Processing* 29 (2020), 6947–6962.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2020）Shiyu Zhao、Lin Zhang、Shuaiyi Huang、Ying Shen和Shengjie Zhao。2020。《去雾评估：真实世界基准数据集、标准和基线》。*IEEE图像处理学报*
    29（2020），6947–6962。
- en: 'Zhao et al. (2021b) Shiyu Zhao, Lin Zhang, Ying Shen, and Yicong Zhou. 2021b.
    RefineDNet: A Weakly Supervised Refinement Framework for Single Image Dehazing.
    *IEEE Transactions on Image Processing* 30 (2021), 3391–3404.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2021b）Shiyu Zhao、Lin Zhang、Ying Shen和Yicong Zhou。2021b。《RefineDNet：一种弱监督的单张图像去雾精炼框架》。*IEEE图像处理学报*
    30（2021），3391–3404。
- en: Zheng et al. (2021) Zhuoran Zheng, Wenqi Ren, Xiaochun Cao, Xiaobin Hu, Tao
    Wang, Fenglong Song, and Xiuyi Jia. 2021. Ultra-High-Definition Image Dehazing
    via Multi-Guided Bilateral Learning. In *Conference on Computer Vision and Pattern
    Recognition*. 16180–16189.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等（2021）Zhuoran Zheng、Wenqi Ren、Xiaochun Cao、Xiaobin Hu、Tao Wang、Fenglong
    Song和Xiuyi Jia。2021。《通过多引导双边学习实现超高分辨率图像去雾》。在*计算机视觉与模式识别会议*上，16180–16189。
- en: Zhu et al. (2021) Hongyuan Zhu, Yi Cheng, Xi Peng, Joey Tianyi Zhou, Zhao Kang,
    Shijian Lu, Zhiwen Fang, Liyuan Li, and Joo-Hwee Lim. 2021. Single-image dehazing
    via compositional adversarial network. *IEEE Transactions on Cybernetics* 51,
    2 (2021), 829–838.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2021）Hongyuan Zhu、Yi Cheng、Xi Peng、Joey Tianyi Zhou、Zhao Kang、Shijian
    Lu、Zhiwen Fang、Liyuan Li 和 Joo-Hwee Lim。2021。《通过组成对抗网络进行单图像去雾》。*IEEE 网络学报* 51,
    2 (2021), 829–838。
- en: 'Zhu et al. (2018) Hongyuan Zhu, Xi Peng, Vijay Chandrasekhar, Liyuan Li, and
    Joo-Hwee Lim. 2018. DehazeGAN: When Image Dehazing Meets Differential Programming.
    In *International Joint Conference on Artificial Intelligence*. 1234–1240.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2018）Hongyuan Zhu、Xi Peng、Vijay Chandrasekhar、Liyuan Li 和 Joo-Hwee Lim。2018。《DehazeGAN:
    当图像去雾遇上微分编程》。发表于 *国际人工智能联合会议*。1234–1240。'
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *International Conference on Computer Vision*. 2223–2232.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2017）Jun-Yan Zhu、Taesung Park、Phillip Isola 和 Alexei A Efros。2017。《使用循环一致对抗网络的未配对图像到图像翻译》。发表于
    *计算机视觉国际会议*。2223–2232。
