- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:59:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.13120] Medical Image Segmentation Using Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.13120] 医学图像分割使用深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.13120](https://ar5iv.labs.arxiv.org/html/2009.13120)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.13120](https://ar5iv.labs.arxiv.org/html/2009.13120)
- en: 'Medical Image Segmentation Using Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医学图像分割使用深度学习：综述
- en: 'Risheng Wang, Tao Lei, Ruixia Cui, Bingtao Zhang, Hongying Meng and Asoke K.
    Nandi R. Wang and T. Lei are with the School of Electronic Information and Artificial
    Intelligence and the Shaanxi Joint Laboratory of Artificial Intelligence, Shaanxi
    University of Science and Technology, Xi’an 710021, China.R. Cui is with the ’Laboratory
    of Hepatobiliary Surgery, First Affiliated Hospital’ and ’National Engineering
    Laboratory of Big Data Algorithm and Analysis Technology Research’(Xi’an Jiaotong
    University), Xi’an, 710049, China.B. Zhang is with the School of Electronic and
    Information Engineering, Lanzhou Jiaotong University, Lanzhou 730070, China.H.
    Meng is with the Department of Electronic and Electrical Engineering, Brunel University
    London, Uxbridge UB8 3PH, U.K.A. K. Nandi is with the Department of Electronic
    and Electrical Engineering, Brunel University London, Uxbridge UB8 3PH, U.K.(Corresponding
    author: Tao Lei) (E-mail: leitao@sust.edu.cn)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王瑞生、雷涛、崔瑞霞、张冰涛、孟红英和阿索克·K·南迪，王瑞生和雷涛均来自陕西科技大学电子信息与人工智能学院以及陕西省人工智能联合实验室，中国西安710021。崔瑞霞在‘第一附属医院肝胆外科实验室’和‘大数据算法与分析技术研究国家工程实验室’（西安交通大学），中国西安710049。张冰涛在兰州交通大学电子与信息工程学院，中国兰州730070。孟红英在布鲁内尔大学电子与电气工程系，英国Uxbridge
    UB8 3PH。阿索克·K·南迪在布鲁内尔大学电子与电气工程系，英国Uxbridge UB8 3PH（通讯作者：雷涛）（电子邮件：leitao@sust.edu.cn）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has been widely used for medical image segmentation and a large
    number of papers has been presented recording the success of deep learning in
    the field. In this paper, we present a comprehensive thematic survey on medical
    image segmentation using deep learning techniques. This paper makes two original
    contributions. Firstly, compared to traditional surveys that directly divide literatures
    of deep learning on medical image segmentation into many groups and introduce
    literatures in detail for each group, we classify currently popular literatures
    according to a multi-level structure from coarse to fine. Secondly, this paper
    focuses on supervised and weakly supervised learning approaches, without including
    unsupervised approaches since they have been introduced in many old surveys and
    they are not popular currently. For supervised learning approaches, we analyze
    literatures in three aspects: the selection of backbone networks, the design of
    network blocks, and the improvement of loss functions. For weakly supervised learning
    approaches, we investigate literature according to data augmentation, transfer
    learning, and interactive segmentation, separately. Compared to existing surveys,
    this survey classifies the literatures very differently from before and is more
    convenient for readers to understand the relevant rationale and will guide them
    to think of appropriate improvements in medical image segmentation based on deep
    learning approaches.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已广泛应用于医学图像分割领域，许多论文记录了深度学习在该领域的成功。在本文中，我们对使用深度学习技术进行医学图像分割进行了全面的主题调查。本文有两个原创性贡献。首先，与传统的调查研究直接将医学图像分割中深度学习的文献分为多个组并详细介绍每组文献不同，我们根据从粗到细的多层次结构对当前流行的文献进行分类。其次，本文重点关注监督学习和弱监督学习方法，不包括无监督学习方法，因为它们在许多旧调查中已被介绍，并且目前不再流行。对于监督学习方法，我们从三个方面分析文献：骨干网络的选择、网络块的设计和损失函数的改进。对于弱监督学习方法，我们根据数据增强、迁移学习和交互式分割分别调查文献。与现有的调查相比，本调查对文献的分类方式有很大不同，更方便读者理解相关原理，并将指导他们在基于深度学习的方法进行医学图像分割时思考适当的改进。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: medical image segmentation, deep learning, supervised learning, weakly supervised
    learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割，深度学习，监督学习，弱监督学习。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Medical image segmentation aims to make anatomical or pathological structures
    changes in more clear in images; it often plays a key role in computer aided diagnosis
    and smart medicine due to the great improvement in diagnostic efficiency and accuracy.
    Popular medical image segmentation tasks include liver and liver-tumor segmentation
    [[1](#bib.bib1)] [[2](#bib.bib2)], brain and brain-tumor segmentation [[3](#bib.bib3)] [[4](#bib.bib4)],
    optic disc segmentation  [[5](#bib.bib5)] [[6](#bib.bib6)], cell segmentation [[7](#bib.bib7)] [[8](#bib.bib8)],
    lung segmentation, pulmonary nodules [[9](#bib.bib9)] [[10](#bib.bib10)], cardiac
    image segmentation [[11](#bib.bib11)] [[12](#bib.bib12)], etc. With the development
    and popularization of medical imaging equipments, X-ray, Computed Tomography (CT),
    Magnetic Resonance Imaging (MRI) and ultrasound have become four important image
    assisted means to help clinicians diagnose diseases, to evaluate prognopsis, and
    to plan operations in medical institutions. In practical applications, although
    these ways of imaging have advantages as well as disadvantages, they are useful
    for the medical examination of different parts of human body.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割旨在使解剖或病理结构在图像中更加清晰；由于在诊断效率和准确性上的巨大提升，它通常在计算机辅助诊断和智能医疗中发挥关键作用。常见的医学图像分割任务包括肝脏和肝肿瘤分割[[1](#bib.bib1)]
    [[2](#bib.bib2)]，大脑和脑肿瘤分割[[3](#bib.bib3)] [[4](#bib.bib4)]，视盘分割[[5](#bib.bib5)]
    [[6](#bib.bib6)]，细胞分割[[7](#bib.bib7)] [[8](#bib.bib8)]，肺部分割，肺结节[[9](#bib.bib9)]
    [[10](#bib.bib10)]，心脏图像分割[[11](#bib.bib11)] [[12](#bib.bib12)]等。随着医学影像设备的发展和普及，X射线、计算机断层扫描（CT）、磁共振成像（MRI）和超声已经成为帮助临床医生诊断疾病、评估预后和计划手术的四种重要图像辅助手段。在实际应用中，尽管这些成像方法各有优缺点，但它们对不同部位的医学检查依然具有重要作用。
- en: To help clinicians make accurate diagnosis, it is necessary to segment some
    crucial objects in medical images and extract features from segmented areas. Early
    approaches to medical image segmentation often depend on edge detection, template
    matching techniques, statistical shape models, active contours, and machine learning,
    etc. Zhao et al. [[13](#bib.bib13)] proposed a new mathematical morphology edge
    detection algorithm for lung CT images. Lalonde et al. [[14](#bib.bib14)] applied
    Hausdorff-based template matching to disc inspection, and Chen et al. [[15](#bib.bib15)]
    also employed template matching to perform ventricular segmentation in brain CT
    images. Tsai et al. [[16](#bib.bib16)] proposed a shape based approach using horizontal
    sets for 2D segmentation of cardiac MRI images and 3D segmentation of prostate
    MRI images. Li et al. [[17](#bib.bib17)] used the activity profile model to segment
    liver-tumors from abdominal CT images, while Li et al. [[18](#bib.bib18)] proposed
    a framework for medical body data segmentation by combining level sets and support
    vector machines (SVMs). Held et al. [[19](#bib.bib19)] applied Markov random fields
    (MRF) to brain MRI image segmentation. Although a large number of approaches have
    been reported and they are successful in certain circumstances, image segmentation
    is still one of the most challenging topics in the field of computer vision due
    to the difficulty of feature representation. In particular, it is more difficult
    to extract discriminating features from medical images than normal RGB images
    since the former often suffers from problems of blur, noise, low contrast, etc.
    Due to the rapid development of deep learning techniques [[20](#bib.bib20)], medical
    image segmentation will no longer require hand-crafted feature and convolutional
    neural networks (CNN) successfully achieve hierarchical feature representation
    of images, and thus become the hottest research topic in image processing and
    computer vision. As CNNs used for feature learning are insensitive to image noise,
    blur, contrast, etc., they provide excellent segmentation results for medical
    images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助临床医生做出准确诊断，有必要对医学图像中的一些关键对象进行分割，并从分割区域提取特征。早期的医学图像分割方法常常依赖于边缘检测、模板匹配技术、统计形状模型、活动轮廓和机器学习等。赵等人[[13](#bib.bib13)]提出了一种用于肺部CT图像的新数学形态学边缘检测算法。Lalonde等人[[14](#bib.bib14)]将基于Hausdorff的模板匹配应用于椎间盘检查，陈等人[[15](#bib.bib15)]也使用模板匹配进行脑部CT图像的脑室分割。Tsai等人[[16](#bib.bib16)]提出了一种基于形状的方法，利用水平集进行心脏MRI图像的2D分割和前列腺MRI图像的3D分割。李等人[[17](#bib.bib17)]使用活动轮廓模型对腹部CT图像中的肝脏肿瘤进行分割，而李等人[[18](#bib.bib18)]提出了一种结合水平集和支持向量机（SVM）的医学体数据分割框架。Held等人[[19](#bib.bib19)]将马尔可夫随机场（MRF）应用于脑部MRI图像分割。尽管已经有大量方法被报道，并且在某些情况下取得了成功，但由于特征表示的困难，图像分割仍然是计算机视觉领域最具挑战性的课题之一。特别是，从医学图像中提取区分特征比从普通RGB图像中提取要困难，因为前者常常面临模糊、噪声、低对比度等问题。由于深度学习技术的快速发展[[20](#bib.bib20)]，医学图像分割将不再需要手工设计的特征，卷积神经网络（CNN）成功实现了图像的层次特征表示，因此成为图像处理和计算机视觉领域最热门的研究课题。由于用于特征学习的CNN对图像噪声、模糊、对比度等不敏感，因此为医学图像提供了优秀的分割结果。
- en: It is worth mentioning that there are currently two categories of image segmentation
    tasks, semantic segmentation and instance segmentation. Image semantic segmentation
    is a pixel-level classification that assigns a corresponding category to each
    pixel in an image. Compared to semantic segmentation, the instance segmentation
    not only needs to achieve pixel-level classification, but also needs to distinguish
    instances on the basis of specific categories. In fact, there are few reports
    on instance segmentation in medical image segmentation since each organ or tissue
    is quite different. In this paper, we review the advances of deep learning techniques
    on medical image segmentation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，目前图像分割任务主要分为两类：语义分割和实例分割。图像语义分割是一种像素级分类方法，将每个像素分配到图像中的相应类别。与语义分割相比，实例分割不仅需要实现像素级分类，还需要在特定类别的基础上区分实例。实际上，由于每个器官或组织的差异性较大，关于医学图像实例分割的报道较少。本文回顾了深度学习技术在医学图像分割方面的进展。
- en: '![Refer to caption](img/78b0391c9c9f79a2bed8f5ca4ad729b9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78b0391c9c9f79a2bed8f5ca4ad729b9.png)'
- en: 'Figure 1: An overview of deep learning methods on medical image segmentation'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习方法在医学图像分割中的概述
- en: According to the number of labeled data, machine learning is often categorized
    into supervised learning, weakly supervised learning, and unsupervised learning.
    The advantage of supervised learning is that we can train models based on carefully
    labeled data, but it is difficult to obtain a large number of labeled data for
    medical images. On the contrary, labeled data are not required for unsupervised
    learning, but the difficulty of learning is increased. Weakly supervised learning
    is between the supervised and unsupervised learning since it only requires a small
    part of data labeled while most of data are unlabeled.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 根据标注数据的数量，机器学习通常被分类为监督学习、弱监督学习和无监督学习。监督学习的优势在于我们可以基于精心标注的数据来训练模型，但获取大量标注数据对于医学图像来说是困难的。相反，无监督学习不需要标注数据，但学习难度增加。弱监督学习介于监督学习和无监督学习之间，因为它只需要一小部分标注数据，而大多数数据未标注。
- en: Prior to the widespread application of deep learning, researchers had presented
    many approaches based on model-driven on medical image segmentation. Masood et
    al. [[21](#bib.bib21)], made a comprehensive summary of many model-driven techniques
    in medical image analysis, including image clustering, region growing, and random
    forest. In [[21](#bib.bib21)], authors summarized different segmentation approaches
    on medical images according to different mathematical models. Recently, only a
    few studies based on model-driven techniques were reported, but more and more
    studies based on data-driven were reported for medical image segmentation. In
    this paper, we mainly focus on the evolution and development of deep learning
    models on medical image segmentation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习广泛应用之前，研究人员已经提出了许多基于模型驱动的医学图像分割方法。Masood 等人[[21](#bib.bib21)]对许多医学图像分析中的模型驱动技术进行了全面总结，包括图像聚类、区域生长和随机森林。在[[21](#bib.bib21)]中，作者根据不同的数学模型总结了医学图像上的不同分割方法。最近，虽然基于模型驱动技术的研究报告较少，但基于数据驱动的研究却越来越多。在本文中，我们主要关注深度学习模型在医学图像分割中的演变和发展。
- en: In [[22](#bib.bib22)], Shen et al. presented a special review of the application
    of deep learning in medical image analysis. This review summarizes the progress
    of machine learning and deep learning in medical image registration, anatomy and
    cell structure detection, tissue segmentation, computer-aided disease diagnosis
    and prognopsis. Litjens et al. [[23](#bib.bib23)]reported a survey of deep learning
    methods, the survey covers the use of deep learning in image classification, object
    detection, segmentation, registration and other tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[22](#bib.bib22)]中，Shen 等人提出了一项关于深度学习在医学图像分析中的应用的特别综述。这项综述总结了机器学习和深度学习在医学图像配准、解剖学和细胞结构检测、组织分割、计算机辅助疾病诊断及预后中的进展。Litjens
    等人[[23](#bib.bib23)]报道了一项关于深度学习方法的调查，该调查涵盖了深度学习在图像分类、目标检测、分割、配准等任务中的应用。
- en: 'More recently, Taghanaki et al. [[24](#bib.bib24)]discussed the development
    of semantic and medical image segmentation; they categorized deep learning-based
    image segmentation solutions into six groups, i.e., deep architectural, data synthesis-based,
    loss function-based, sequenced models, weakly supervised, and multi-task methods.
    To develop a more complete survey on medical image segmentation, Seo et al. [[25](#bib.bib25)]reviewed
    classical machine learning algorithms such as Markov random fields, $k$-means
    clustering, random forest, and reviewed latest deep learning architectures such
    as the artificial neural networks (ANNs), the convolutional neural networks (CNNs),
    the recurrent neural networks (RNNs), etc. Tajbakhsh et al. [[26](#bib.bib26)]reviewed
    solutions of medical image segmentation with imperfect datasets, including two
    major dataset limitations: scarce annotations and weak annotations. All these
    surveys play an important role for the development of medical image segmentation
    techniques. Hesamian et al.  [[27](#bib.bib27)]reviewed on three aspects of approaches
    (network structures), training techniques, and challenges. The network structures
    section describes the main, popular network structures used for image segmentation.
    The training techniques section discusses the J Digit imaging technique used to
    train deep neural network models. The challenges section describes the various
    challenges associated with medical image segmentation using deep learning techniques.
    Meyer et al. [[28](#bib.bib28)]reviewed the advances in the application or potential
    application of deep learning to radiotherapy. Akkus et al. [[29](#bib.bib29)]provided
    an overview of current deep learning-based segmentation approaches for quantitative
    brain MRI images. Zhou et al. [[30](#bib.bib30)]focused on three typical types
    of weak supervision: incomplete supervision, inexact supervision and inaccurate
    supervision. Eelbode et al. [[31](#bib.bib31)]focus on evaluating and summarizing
    the optimization methods used in medical image segmentation tasks based primarily
    on Dice scores or Jaccard indices.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Taghanaki 等人 [[24](#bib.bib24)] 讨论了语义和医学图像分割的发展；他们将基于深度学习的图像分割解决方案分为六类，即深度架构、数据合成、损失函数、序列模型、弱监督和多任务方法。为了开发更全面的医学图像分割调查，Seo
    等人 [[25](#bib.bib25)] 回顾了经典的机器学习算法，如马尔可夫随机场、$k$-均值聚类、随机森林，并回顾了最新的深度学习架构，如人工神经网络（ANNs）、卷积神经网络（CNNs）、递归神经网络（RNNs）等。Tajbakhsh
    等人 [[26](#bib.bib26)] 回顾了具有不完美数据集的医学图像分割解决方案，包括两个主要的数据集限制：稀缺的标注和弱标注。所有这些调查在医学图像分割技术的发展中都发挥了重要作用。Hesamian
    等人 [[27](#bib.bib27)] 从三个方面回顾了方法（网络结构）、训练技术和挑战。网络结构部分描述了用于图像分割的主要流行网络结构。训练技术部分讨论了用于训练深度神经网络模型的
    J Digit 成像技术。挑战部分描述了使用深度学习技术进行医学图像分割的各种挑战。Meyer 等人 [[28](#bib.bib28)] 回顾了深度学习在放射治疗中的应用或潜在应用的进展。Akkus
    等人 [[29](#bib.bib29)] 提供了基于深度学习的定量脑部 MRI 图像分割方法的概述。Zhou 等人 [[30](#bib.bib30)]
    重点关注三种典型的弱监督类型：不完全监督、不准确监督和不精确监督。Eelbode 等人 [[31](#bib.bib31)] 重点评估和总结了主要基于 Dice
    分数或 Jaccard 指数的医学图像分割任务中使用的优化方法。
- en: 'Through studying the aforementioned surveys, researchers can learn the latest
    techniques of medical image segmentation, and then make more significant contributions
    for computer aided diagnoses and smart healthcare. However, these surveys suffer
    from two problems. One is that most of them chronologically summarize the development
    of medical image segmentation, and they thus ignore the technical branch of deep
    learning for medical image segmentation. The other problem is that these surveys
    only introduce related technical development but not focus on the task characteristics
    of medical image segmentation such as few-shot learning, imbalance learning, etc.,
    which limits the improvement of medical image segmentation based on task-driven.
    To address these two problems, we present a novel survey on medical image segmentation
    using deep learning. In this work, we make the following contributions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究上述调查，研究人员可以了解医学图像分割的最新技术，从而为计算机辅助诊断和智能医疗做出更重要的贡献。然而，这些调查存在两个问题。一是大多数调查按时间顺序总结了医学图像分割的发展，因此忽略了深度学习在医学图像分割中的技术分支。另一个问题是这些调查仅介绍了相关技术的发展，而没有关注医学图像分割的任务特征，如少样本学习、类别不平衡等，这限制了基于任务驱动的医学图像分割的改进。为了解决这两个问题，我们提出了一种基于深度学习的医学图像分割新型调查。在这项工作中，我们做出了以下贡献：
- en: 1\. We summarize the technical branch of deep learning for medical image segmentation
    from coarse to fine as shown in Fig. 1\. The summation includes two aspects of
    supervised learning and weakly supervised learning. The latest applications of
    neural architecture search (NAS), graph convolutional networks (GCN), multi-modality
    data fusion and medical transformer in medical image analysis are also discussed.
    Compared to the previous surveys, our survey follows conceptual developments and
    is believed to be clearer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 我们从粗到细总结了深度学习在医学图像分割领域的技术分支，如图1所示。总结包括有监督学习和弱监督学习两个方面。还讨论了神经架构搜索（NAS）、图卷积网络（GCN）、多模态数据融合和医学变压器在医学图像分析中的最新应用。与之前的调查相比，我们的调查遵循了概念发展的脉络，且被认为更为清晰。
- en: '2\. On supervised learning approaches we analyze literature from three aspects:
    the selection of backbone networks, the design of network blocks, and the improvement
    of loss functions. This classification method can help subsequent researchers
    to understand more deeply motivations and improvement strategies of medical image
    segmentation networks. For weakly supervised learning, we also review literatures
    from three aspects for processing few-shot data or class imbalanced data: data
    augmentation, transfer learning, and interactive segmentation. This organization
    is expected to be more conducive to researchers in finding innovations for improving
    the accuracy of medical image segmentation.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对于有监督学习方法，我们从三个方面分析了文献：骨干网络的选择、网络模块的设计以及损失函数的改进。这种分类方法可以帮助后续研究人员更深入地理解医学图像分割网络的动机和改进策略。对于弱监督学习，我们还从处理少样本数据或类别不平衡数据的三个方面回顾了文献：数据增强、迁移学习和交互分割。这种组织方式有望更有利于研究人员发现提升医学图像分割准确性的创新方法。
- en: 3\. In addition to reviewing comprehensively the development and application
    of deep learning in medical image segmentation, we also collect the currently
    common public medical image segmentation datasets. Finally, we discuss future
    research trends and directions in this field.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 除了全面回顾深度学习在医学图像分割中的发展和应用外，我们还收集了当前常见的公共医学图像分割数据集。最后，我们讨论了该领域未来的研究趋势和方向。
- en: The rest of this paper is organized as follows. In Section II, we review the
    development and evolution of supervised learning applied to medical images, including
    the selection of backbone network, the design of network blocks, and the improvement
    of loss function. In Section III, we introduce the application of unsupervised
    or weakly supervised methods in the field of medical image segmentation and analyze
    the commonly unsupervised or weakly supervised strategies for processing few-shot
    data or class imbalanced data. In Section IV, we briefly introduce some of the
    most advanced methods of medical image segmentation, including NAS, application
    of GCN, multi-modality data fusion, etc. In Section V, we collect the currently
    available public medical image segmentation datasets, and summarize limitations
    of current deep learning methods and future research directions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：在第二部分，我们回顾了应用于医学图像的监督学习的发展和演变，包括骨干网络的选择、网络模块的设计和损失函数的改进。在第三部分，我们介绍了无监督或弱监督方法在医学图像分割领域的应用，并分析了处理少量样本数据或类别不平衡数据的常见无监督或弱监督策略。在第四部分，我们简要介绍了一些最先进的医学图像分割方法，包括NAS、GCN的应用、多模态数据融合等。在第五部分，我们收集了当前可用的公共医学图像分割数据集，并总结了当前深度学习方法的局限性和未来研究方向。
- en: II Supervised learning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 监督学习
- en: For medical image segmentation tasks, supervised learning is the most popular
    method since these tasks usually require high accuracy. In this section, we focus
    on the review of improvements of neural network architectures. These improvements
    mainly include network backbones, network blocks and the design of loss functions.
    Fig. 2 shows an overview on the improvement of network architectures based on
    supervised learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学图像分割任务，监督学习是最受欢迎的方法，因为这些任务通常需要高精度。在本节中，我们将**深入探讨**神经网络架构的改进。这些改进主要包括网络骨干、网络模块和损失函数的设计。图2展示了基于监督学习的网络架构改进概述。
- en: '![Refer to caption](img/edcf36f690b2dd678c44494d1f0e106b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/edcf36f690b2dd678c44494d1f0e106b.png)'
- en: 'Figure 2: An overview of network architectures based on supervised learning.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于监督学习的网络架构概述。
- en: II-A Backbone Networks
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 网络骨干
- en: Image semantic segmentation aims to achieve pixel classification of an image.
    For this goal, researchers proposed the encoder-decoder structure that is one
    of the most popular end-to-end architectures, such as fully convolution network
    (FCN) [[32](#bib.bib32)], U-Net [[7](#bib.bib7)],Deeplab [[33](#bib.bib33)], etc.
    In these structures, an encoder is often used to extract image features while
    a decoder is often used to restore extracted features to the original image size
    and output the final segmentation results. Although the end-to-end structure is
    pragmatic for medical image segmentation, it reduces the interpretability of models.
    The first high-impact encoder-decoder structure, the U-Net proposed by Ronneberger
    et al. [[7](#bib.bib7)]has been widely used for medical image segmentation. Fig.
    3 shows the U-Net architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图像语义分割旨在实现图像的像素分类。为了这个目标，研究人员提出了编码器-解码器结构，这是一种最受欢迎的端到端架构之一，如全卷积网络（FCN）[[32](#bib.bib32)]、U-Net
    [[7](#bib.bib7)]、Deeplab [[33](#bib.bib33)]等。在这些结构中，编码器通常用于提取图像特征，而解码器通常用于将提取的特征恢复到原始图像大小并输出最终的分割结果。尽管端到端结构在医学图像分割中具有实用性，但它降低了模型的可解释性。第一个高影响力的编码器-解码器结构，即Ronneberger等人提出的U-Net
    [[7](#bib.bib7)]，已广泛用于医学图像分割。图3展示了U-Net架构。
- en: '*U-Net:* The U-Net solves problems of general CNN networks used for medical
    image segmentation, since it adopts a perfect symmetric structure and skip connection.
    Different from common image segmentation, medical images usually contain noise
    and show blurred boundaries. Therefore, it is very difficult to detect or recognize
    objects in medical images only depending on image low-level features. Meanwhile,
    it is also impossible to obtain accurate boundaries depending only on image semantic
    features due to the lack of image detail information. Whereas, the U-Net effectively
    fuses low-level and high-level image features by combining low-resolution and
    high-resolution feature maps through skip connections, which is a perfect solution
    for medical image segmentation tasks. Currently, the U-Net has become the benchmark
    for most medical image segmentation tasks and has inspired a lot of meaningful
    improvements.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*U-Net:* U-Net 解决了用于医学图像分割的一般 CNN 网络的问题，因为它采用了完美的对称结构和跳跃连接。与普通的图像分割不同，医学图像通常包含噪声并显示模糊的边界。因此，仅依靠图像的低级特征来检测或识别医学图像中的物体是非常困难的。同时，由于缺乏图像细节信息，仅依靠图像语义特征也无法获得准确的边界。而
    U-Net 通过跳跃连接将低分辨率和高分辨率特征图结合起来，有效地融合了低级和高级图像特征，这对于医学图像分割任务是一个完美的解决方案。目前，U-Net 已成为大多数医学图像分割任务的基准，并激发了许多有意义的改进。'
- en: '![Refer to caption](img/b0f582f4a4fe10fa6981441407edb5f8.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0f582f4a4fe10fa6981441407edb5f8.png)'
- en: 'Figure 3: The U-Net architecture [[7](#bib.bib7)].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：U-Net 架构 [[7](#bib.bib7)]。
- en: '*3D Net:* In practice, as most of medical data such as CT and MRI images exist
    in the form of 3D volume data, the use of 3D convolution kernels can better mine
    the high-dimensional spatial correlation of data. Motivated by this idea, Çiçek
    et al. [[34](#bib.bib34)]extended U-Net architecture to the application of 3D
    data, and proposed 3D U-Net that deals with 3D medical data directly. Due to the
    limitation of computational resources, the 3D U-Net only includes three down-sampling,
    which cannot effectively extract deep-layer image features leading to limited
    segmentation accuracy for medical images. In addition, Milletari et al. [[35](#bib.bib35)]proposed
    a similar architecture, V-Net, as shown in Fig. 4\. It is well known that residual
    connections can avoid vanishing gradient and accelerate network convergence, and
    it is thus easy to design deeper network structures that can provide better feature
    representation. Compared to 3D U-Net, V-Net employs residual connections to design
    a deeper network (4 down-samplings), and thus achieves higher performance. Similarly,
    by applying residual connections to 3D networks, Yu et al. [[36](#bib.bib36)]presented
    Voxresnet, Lee et al. [[37](#bib.bib37)]proposed 3DRUNet, and Xiao et al. [[38](#bib.bib38)]proposed
    Res-UNet. However, these 3D Networks encounter same problems of high computational
    cost and GPU memory usage due to a very large number of parameters.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*3D Net:* 实际上，由于大多数医学数据如 CT 和 MRI 图像以 3D 体积数据的形式存在，使用 3D 卷积核可以更好地挖掘数据的高维空间相关性。受此启发，Çiçek
    等人 [[34](#bib.bib34)]扩展了 U-Net 架构以应用于 3D 数据，提出了直接处理 3D 医学数据的 3D U-Net。由于计算资源的限制，3D
    U-Net 仅包含三个下采样层，这无法有效提取深层图像特征，导致医学图像的分割准确性有限。此外，Milletari 等人 [[35](#bib.bib35)]提出了类似的架构
    V-Net，如图 4 所示。众所周知，残差连接可以避免梯度消失并加速网络收敛，因此很容易设计出更深的网络结构，从而提供更好的特征表示。与 3D U-Net
    相比，V-Net 采用残差连接设计了一个更深的网络（4 个下采样），因此性能更高。同样，通过将残差连接应用于 3D 网络，Yu 等人 [[36](#bib.bib36)]提出了
    Voxresnet，Lee 等人 [[37](#bib.bib37)]提出了 3DRUNet，Xiao 等人 [[38](#bib.bib38)]提出了 Res-UNet。然而，这些
    3D 网络由于参数数量非常庞大，遇到了相同的高计算成本和 GPU 内存使用问题。'
- en: '![Refer to caption](img/fa53d9eff74dea95c87a783bbda718c2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa53d9eff74dea95c87a783bbda718c2.png)'
- en: 'Figure 4: The V -Net architecture [[35](#bib.bib35)].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：V-Net 架构 [[35](#bib.bib35)]。
- en: '*Recurrent Neural Network (RNN):* RNN is initially designed to deal with sequence
    problems. The long Short-Term Memory (LSTM) network [[39](#bib.bib39)]is one of
    the most popular RNNs. It can retain the gradient flow for a long time by introducing
    a self-loop. For medical image segmentation, RNN has been used to model the time
    dependence of image sequences. Alom et al. [[40](#bib.bib40)]proposed a medical
    image segmentation method that combines ResUNet with RNN. The method achieves
    feature accumulation of recursive residual convolutional layers, which improves
    feature representation for image segmentation tasks. Fig. 5 shows the recurrent
    residual convolutional unit. Gao et al. [[41](#bib.bib41)]joined LSTM and CNN
    to model the temporal relationship between different brain MRI slices to improve
    segmentation accuracy. Bai et al. [[42](#bib.bib42)]combined FCN with RNN to mine
    the spatiotemporal information for aortic sequence segmentation. Clearly, RNN
    can capture local and global spatial features of images by considering the context
    information relationship. However, in medical image segmentation, the capture
    of complete and valid temporal information requires good medical image quality
    (e.g. smaller slice thickness and pixel spacing). Therefore, the design of RNN
    is uncommon for improving the performance of medical image segmentation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环神经网络 (RNN)：* RNN 最初设计用于处理序列问题。长短期记忆网络 (LSTM) [[39](#bib.bib39)] 是最受欢迎的 RNN
    之一。通过引入自环，它可以长时间保持梯度流。对于医学图像分割，RNN 已被用于建模图像序列的时间依赖性。Alom 等人 [[40](#bib.bib40)]
    提出了将 ResUNet 与 RNN 结合的医学图像分割方法。该方法实现了递归残差卷积层的特征积累，从而改善了图像分割任务的特征表示。图 5 显示了循环残差卷积单元。Gao
    等人 [[41](#bib.bib41)] 将 LSTM 和 CNN 结合，建模不同脑 MRI 切片之间的时间关系，以提高分割精度。Bai 等人 [[42](#bib.bib42)]
    将 FCN 与 RNN 结合，以挖掘主动脉序列分割的时空信息。显然，RNN 可以通过考虑上下文信息关系来捕捉图像的局部和全局空间特征。然而，在医学图像分割中，捕捉完整有效的时间信息需要良好的医学图像质量（例如更小的切片厚度和像素间距）。因此，RNN
    的设计在提高医学图像分割性能方面并不常见。'
- en: '![Refer to caption](img/0dcc5b8a290c3ae32901ef7900a36b79.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0dcc5b8a290c3ae32901ef7900a36b79.png)'
- en: 'Figure 5: The recurrent residual convolutional unitalom2018recurrent.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 循环残差卷积单元2018recurrent。'
- en: '*Skip Connection:* Although the skip connection can fuse low-resolution and
    high-resolution information and thus improve feature representation, it suffers
    from the problem of the large semantic gap between low- and high-resolution features,
    leading to blurred feature maps. To improve skip connection, Ibtehaz et al. [[43](#bib.bib43)]proposed
    MultiResUNet including the Residual Path (ResPath), which makes the encoder features
    perform some additional convolution operations before fusing with the corresponding
    features in the decoder. Seo et al. [[44](#bib.bib44)] proposed mUNet and Chen
    et al. [[45](#bib.bib45)]proposed FED-Net. Both mU-Net and FED-Net add convolution
    operations to the skip connection to improve the performance of medical image
    segmentation.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*跳跃连接：* 尽管跳跃连接可以融合低分辨率和高分辨率的信息，从而改善特征表示，但它面临低分辨率和高分辨率特征之间的大语义差距问题，导致特征图模糊。为了改进跳跃连接，Ibtehaz
    等人 [[43](#bib.bib43)] 提出了包括 Residual Path (ResPath) 的 MultiResUNet，这使得编码器特征在与解码器中的相应特征融合之前，进行一些额外的卷积操作。Seo
    等人 [[44](#bib.bib44)] 提出了 mUNet，Chen 等人 [[45](#bib.bib45)] 提出了 FED-Net。mU-Net
    和 FED-Net 都在跳跃连接中添加了卷积操作，以提高医学图像分割的性能。'
- en: '*Cascade of 2D and 3D:* For image segmentation tasks, the cascade model often
    trains two or more models to improve segmentation accuracy. This method is especially
    popular in medical image segmentation. The cascade model can be broadly divided
    into three types of frameworks: coarse-fine segmentation, detection segmentation,
    and mixed segmentation. The first class is a coarse-fine segmentation framework
    that uses a cascade of two 2D networks for segmentation, where the first network
    performs coarse segmentation and then uses another network model to achieve fine
    segmentation based on the previous coarse segmentation results. Christ et al. [[46](#bib.bib46)]proposed
    a cascaded network for liver and liver-tumor segmentation. This network firstly
    uses a FCN to segment livers, and then uses previous liver segmentation results
    as the input of the second FCN for liver-tumor segmentation. Yuan et al. [[47](#bib.bib47)]first
    trained a simple convolutional-deconvolutional neural networks (CDNN) model (19-layer
    FCN) to provide rapid but coarse liver segmentation over the entire images of
    a CT volume, and then applied another CDNN (29-layer FCN) to the liver region
    for fine-grained liver segmentation. Finally, the liver segmentation region enhanced
    by histogram equalization is considered as an additional input to the third CDNN
    (29-layer CNN) for liver-tumor segmentation. Besides, other networks using the
    coarse-fine segmentation framework can be found in [[48](#bib.bib48)] [[49](#bib.bib49)] [[50](#bib.bib50)].
    At the same time, the detection segmentation framework is also popular. First,
    a network model such as R-CNN [[51](#bib.bib51)]or You-On-Look-Once (YOLO) [[52](#bib.bib52)]is
    used for target location identification, and then another network is used for
    further detailed segmentation based on previously coarse segmentation results.
    Al-Antari et al. [[53](#bib.bib53)]proposed a similar approach for breast mass
    detection, segmentation and classification from mammograms. In this work, the
    first step is to use the regional deep learning method YOLO for target detection,
    the second step is to input the detected targets into a newly designed full-resolution
    convolutional network (FrCN) for segmentation, and finally, a deep convolutional
    neural network is used to identify the masses and classify them as benign or malignant.
    Similarly, Tang et al. [[47](#bib.bib47)]used faster R-CNN [[54](#bib.bib54)]and
    Deeplab [[55](#bib.bib55)]cascades for localization segmentation of the liver.
    In addition, both Salehi et al. [[56](#bib.bib56)]and Yan et al. [[57](#bib.bib57)]proposed
    a kind of cascade networks for whole-brain MRI and high-resolution mammogram segmentation.
    This kind of cascade network can effectively extract richer multi-scale context
    information by using a posteriori probabilities generated by the first network
    than normal cascade networks.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*二维和三维级联:* 对于图像分割任务，级联模型通常训练两个或更多模型以提高分割准确性。这种方法在医学图像分割中尤其受欢迎。级联模型可以大致分为三种框架：粗细分割，检测分割和混合分割。第一类是粗细分割框架，它使用两个2D网络进行分割的级联，第一个网络进行粗分割，然后使用另一个网络模型根据前面的粗分割结果进行细分割。Christ等人[[46](#bib.bib46)]提出了一种用于肝脏和肝肿瘤分割的级联网络。该网络首先使用FCN对肝脏进行分割，然后使用前一次肝脏分割结果作为第二个FCN对肝肿瘤进行分割的输入。Yuan等人[[47](#bib.bib47)]首先训练了一个简单的卷积-反卷积神经网络（CDNN）模型（19层FCN），以在整个CT体积的图像上提供快速但粗糙的肝脏分割，然后在肝脏区域应用另一个CDNN（29层FCN）进行细粒度的肝脏分割。最后，由直方图均衡增强的肝脏分割区被视为第三个CDNN（29层CNN）进行肝肿瘤分割的额外输入。此外，还可以在[[48](#bib.bib48)] [[49](#bib.bib49)] [[50](#bib.bib50)]找到使用粗细分割框架的其他网络。同时，检测分割框架也很受欢迎。首先使用网络模型（如R-CNN[[51](#bib.bib51)]或You-Only-Look-Once（YOLO）[[52](#bib.bib52)]）进行目标定位识别，然后根据先前的粗分割结果使用另一个网络进行进一步的详细分割。Al-Antari等人[[53](#bib.bib53)]提出了一种类似的方法，用于从乳房X线照片中检测、分割和分类乳房肿块。在这项工作中，第一步是使用区域深度学习方法YOLO进行目标检测，第二步是将检测到的目标输入到新设计的全分辨率卷积网络（FrCN）进行分割，最后，使用深度卷积神经网络识别肿块并将其分类为良性或恶性。类似地，唐等人[[47](#bib.bib47)]使用更快的R-CNN[[54](#bib.bib54)]和Deeplab[[55](#bib.bib55)]级联进行肝脏的定位分割。此外，Salehi等人[[56](#bib.bib56)]和Yan等人[[57](#bib.bib57)]提出了一种级联网络用于整个脑MRI和高分辨率乳房X线照片的分割。这种级联网络可以通过利用第一个网络生成的后验概率有效地提取更丰富的多尺度上下文信息。'
- en: However, most of medical images are 3D volume data, but a 2D convolutional neural
    network cannot learn temporal information in the third dimension, and a 3D convolutional
    neural network often requires high computation cost and severes GPU memory consumption.
    Therefore some pseudo-3D segmentation methods have been proposed. Oda et al. [[58](#bib.bib58)]proposed
    a three-plane method of cascading three networks to segment the abdominal artery
    region effectively from the medical CT volume. Vu et al. [[59](#bib.bib59)]applied
    the overlay of adjacent slices as input to the central slice prediction, and then
    fed the obtained 2D feature map into a standard 2D network for model training.
    Although these pseudo-3D approaches can segment object from 3D volume data, they
    only obtain limited accuracy improvement due to the utilization of local temporal
    information. Compared to pseudo-3D networks, hybrid cascading 2D and 3D networks
    are more popular. Li et al. [[60](#bib.bib60)]proposed a hybrid densely connected
    U-Net (H-DenseUNet) for liver and liver-tumor segmentation. This method first
    employs a simple Resnet to obtain a rough liver segmentation result, utilizing
    the 2D DenseUNet to extract 2D image features effectively, then uses the 3D DenseUNet
    to extract 3D image features, and finally designs a hybrid feature fusion layer
    to jointly optimize 2D and 3D features. Although the H-DenseUNet reduces the complexity
    of models compared to an entire 3D network, the model is complex and it still
    suffers from a large number of parameters from 3D convolutions. For the problem,
    Zhang et al. [[61](#bib.bib61)]proposed a lightweight hybrid convolutional network
    (LW-HCN) with a similar structure to the H-DenseUNet, but the former requires
    fewer parameters and computational cost than the latter due to the design of the
    depthwise and spatiotemporal separate (DSTS) block and the use of 3D depth separable
    convolution. Similarly, Dey et al. [[62](#bib.bib62)]also designed a cascade of
    2D and 3D network for liver and liver-tumor segmentation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数医学图像是三维体积数据，但二维卷积神经网络无法学习第三维度的时间信息，而三维卷积神经网络通常需要高计算成本和巨大的 GPU 内存消耗。因此，提出了一些伪三维分割方法。Oda
    等人[[58](#bib.bib58)]提出了一种三平面方法，通过级联三个网络来有效地从医学 CT 体积图像中分割腹部动脉区域。Vu 等人[[59](#bib.bib59)]将相邻切片的叠加作为中心切片预测的输入，然后将获得的二维特征图输入到标准二维网络进行模型训练。虽然这些伪三维方法可以从三维体积数据中分割物体，但由于只利用了局部时间信息，因此只能获得有限的精度提升。与伪三维网络相比，混合级联二维和三维网络更受欢迎。Li
    等人[[60](#bib.bib60)]提出了一种用于肝脏和肝脏肿瘤分割的混合密集连接 U-Net（H-DenseUNet）。该方法首先使用简单的 Resnet
    获得粗略的肝脏分割结果，然后利用二维 DenseUNet 有效提取二维图像特征，再用三维 DenseUNet 提取三维图像特征，最后设计了一个混合特征融合层来共同优化二维和三维特征。虽然
    H-DenseUNet 相比整个三维网络减少了模型的复杂性，但该模型仍然复杂，并且由于三维卷积的参数量庞大而受到限制。对此问题，Zhang 等人[[61](#bib.bib61)]提出了一种轻量级混合卷积网络（LW-HCN），其结构类似于
    H-DenseUNet，但由于设计了深度分离空间时间（DSTS）块和使用了三维深度可分离卷积，前者所需的参数和计算成本更少。类似地，Dey 等人[[62](#bib.bib62)]也设计了一种用于肝脏和肝脏肿瘤分割的二维和三维网络级联结构。
- en: Obviously, among the three types of cascade networks mentioned above, the hybrid
    2D and 3D cascade network can effectively improve segmentation accuracy and reduce
    the learning burdens.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在上述三种级联网络中，混合的二维和三维级联网络可以有效提高分割精度并减轻学习负担。
- en: In contrast to the above cascade networks, Valanarasu et al. [[63](#bib.bib63)]proposed
    a complete cascade network namely KiU-Net to perform brain dissection segmentation.
    The performance of vanilla U-Net is greatly degraded when detecting smaller anatomical
    structures with fuzzy noise boundaries. To overcome this problem, authors designed
    a novel over-complete architecture Ki-Net, in which the spatial size of the intermediate
    layer is larger than that of the input data, and this is achieved by using an
    up-sampling layer after each conversion layer in the encoder. Thus the proposed
    Ki-Net possesses stronger edge capture capability compared to U-Net and finally
    it is cascaded with the vanilla U-Net to improve the overall segmentation accuracy.
    Since the KiU-Net can exploit both the low-level fine edges feature maps using
    Ki-Net and the high-level shape feature maps using U-Net, it not only improves
    segmentation accuracy but also achieves fast convergence for small anatomical
    landmarks and blurred noisy boundaries.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于上述级联网络，Valanarasu等人[[63](#bib.bib63)]提出了一种完整的级联网络，即KiU-Net，用于进行脑部解剖分割。原始U-Net在检测较小的解剖结构时，其性能显著下降，特别是在模糊噪声边界的情况下。为了克服这个问题，作者设计了一种新颖的过完美架构Ki-Net，其中中间层的空间大小大于输入数据的大小，这通过在编码器中的每个转换层后使用上采样层来实现。因此，所提出的Ki-Net相对于U-Net具有更强的边缘捕捉能力，并最终与原始U-Net级联，以提高整体分割精度。由于KiU-Net可以利用Ki-Net中的低级细边特征图和U-Net中的高级形状特征图，它不仅提高了分割精度，还实现了对小型解剖标志和模糊噪声边界的快速收敛。
- en: '*Others:*A generating adversarial networks (GAN) [[64](#bib.bib64)]has been
    widely used in many areas of computer vision. In its infancy, the GAN was often
    used for data augmentation by generating new samples, which would be reviewed
    in Section III, but later researchers discovered that the idea of generative confrontation
    could be used in almost any field, and was therefore also used for image segmentation.
    Since medical images usually show low contrast, blurred boundaries between different
    tissues or between tissues and lesions, and sparse medical image data with labels,
    U-Net-based segmentation methods using pixel loss to learn local and global relationships
    between pixels are not sufficient for medical image segmentation, and the use
    of generative adversarial networks is becoming a popular idea for improving image
    segmentation. Luc et al. [[65](#bib.bib65)]firstly applied the generative adversarial
    network to image segmentation, where the generative network is used for segmentation
    models and the adversarial network is trained as a classifier. Singh et al. [[66](#bib.bib66)]proposed
    a conditional generation adversarial network (cGAN) to segment breast tumors within
    the target area (ROI) in mammograms. The generative network learns to identify
    tumor regions and generates segmentation results, and the adversarial network
    learns to distinguish between ground truth and segmentation results from the generative
    network, thereby enforcing the generative network to obtain labels as realistic
    as possible. The cGAN works fine when the number of training samples is limited.
    Conze et al. [[67](#bib.bib67)]utilized cascaded pretrained convolutional encoder-decoders
    as generators of cGAN for abdominal multi-organ segmentation, and considered the
    adversarial network as a discriminator to enforces the model to create realistic
    organ delineations.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*其他：*生成对抗网络（GAN）[[64](#bib.bib64)]已广泛应用于计算机视觉的许多领域。在其早期阶段，GAN通常用于通过生成新样本来进行数据增强，这将在第三节中进行讨论，但后来研究人员发现，生成对抗的思想几乎可以应用于任何领域，因此也被用于图像分割。由于医学图像通常显示低对比度、不同组织或组织与病变之间的边界模糊，并且标注的医学图像数据稀少，基于U-Net的分割方法使用像素损失来学习像素之间的局部和全局关系，对于医学图像分割并不充分，因此使用生成对抗网络成为了改善图像分割的热门方法。Luc等人[[65](#bib.bib65)]首次将生成对抗网络应用于图像分割，其中生成网络用于分割模型，而对抗网络则被训练为分类器。Singh等人[[66](#bib.bib66)]提出了一种条件生成对抗网络（cGAN），用于在乳腺X光片中对目标区域（ROI）进行乳腺肿瘤分割。生成网络学习识别肿瘤区域并生成分割结果，对抗网络则学习区分生成网络的地面真实情况和分割结果，从而强制生成网络获得尽可能真实的标签。cGAN在训练样本有限的情况下效果良好。Conze等人[[67](#bib.bib67)]利用级联预训练卷积编码器-解码器作为cGAN的生成器，用于腹部多器官分割，并将对抗网络视为判别器，以强制模型创建真实的器官轮廓。'
- en: In addition, the incorporation of the prior knowledge about organ shape and
    position may be crucial for improving medical image segmentation effect, where
    images are corrupted and thus contain artefacts due to limitations of imaging
    techniques. However, there are few works about how to incorporate prior knowledge
    into CNN models. As one of the earliest studies in this field, Oktay et al. [[68](#bib.bib68)]proposed
    a novel and general method to combine a priori knowledge of shape and label structure
    into the anatomically constrained neural networks (ACNN) for medical image analysis
    tasks. In this way, the neural network training process can be constrained and
    guided to make more anatomical and meaningful predictions, especially in cases
    where input image data is not sufficiently informative or consistent enough (e.g.,
    missing object boundaries). Similarly, Boutillon et al. [[69](#bib.bib69)]incorporated
    anatomical priors into a conditional adversarial framework for scapula bone segmentation,
    combining shape priors with conditional neural networks to encourage models to
    follow global anatomical properties in terms of shape and position information,
    and to make segmentation results as accurate as possible. The above study shows
    that improved models can provide higher segmentation accuracy and they are more
    robust since priori knowledge constraints are employed in the training process
    of neural networks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结合有关器官形状和位置的先验知识可能对改善医学图像分割效果至关重要，因为图像可能受到成像技术限制而出现伪影。然而，关于如何将先验知识融入CNN模型的研究较少。作为该领域最早的研究之一，Oktay等人[[68](#bib.bib68)]提出了一种新颖且通用的方法，将形状和标签结构的先验知识结合到解剖约束神经网络（ACNN）中用于医学图像分析任务。通过这种方式，神经网络训练过程可以被约束和指导，以做出更具解剖学和意义的预测，特别是在输入图像数据不够信息丰富或一致（例如，缺失对象边界）的情况下。类似地，Boutillon等人[[69](#bib.bib69)]将解剖学先验知识融入到条件对抗框架中，用于肩胛骨分割，将形状先验与条件神经网络结合，鼓励模型在形状和位置方面遵循全局解剖学特性，并使分割结果尽可能准确。上述研究表明，改进的模型可以提供更高的分割准确性，并且由于在神经网络训练过程中采用了先验知识约束，它们更具鲁棒性。
- en: After proposing U-Net in [[7](#bib.bib7)], the encoder-decoder structure became
    the most popular structure in medical image segmentation. The design of the network
    backbone focuses on more efficient feature extraction in the encoder and feature
    recovery and fusion in the decoder to improve segmentation accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出U-Net之后[[7](#bib.bib7)]，编码器-解码器结构成为医学图像分割中最流行的结构。网络主干的设计着重于在编码器中更高效地提取特征，并在解码器中进行特征恢复和融合，以提高分割准确性。
- en: II-B Network Function Block
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 网络功能块
- en: II-B1 Dense Connection
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 密集连接
- en: Dense connection is often used to construct a kind of special convolution neural
    networks. For dense connection networks, the input of each layer comes from the
    output of all previous layers in the process of forward transmission. Inspired
    by the dense connection, Guan et al.  [[70](#bib.bib70)] proposed an improved
    U-Net by replacing each sub-block of U-Net with a form of dense connections as
    shown in Fig. 6\. Although the dense connection is helpful for obtaining richer
    image features, it often reduces the robustness of feature representation to a
    certain extent and increases the number of parameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接常用于构建一种特殊的卷积神经网络。对于密集连接网络，每一层的输入来自于前面所有层的输出。在密集连接的启发下，Guan等人[[70](#bib.bib70)]提出了一种改进的U-Net，通过用密集连接形式替换U-Net的每个子块，如图6所示。尽管密集连接有助于获取更丰富的图像特征，但它往往在一定程度上降低了特征表示的鲁棒性，并增加了参数数量。
- en: Zhou et al.  [[71](#bib.bib71)] connected all U-Net layers (from one to four)
    together as shown in Fig. 7\. The advantage of this structure is that it allows
    the network to learn automatically importance of features at different layers.
    Besides, the skip connection is redesigned so that features with different semantic
    scales can be aggregated in the decoder, resulting in a highly flexible feature
    fusion scheme. The disadvantage is that the number of parameters is increased
    due to the employment of dense connection. Therefore, a pruning method is integrated
    into model optimization to reduce the number of parameters. Meanwhile, the deep
    supervision  [[72](#bib.bib72)] is also employed to balance the decline of segmentation
    accuracy caused by the pruning. Although the dense connection is helpful for obtaining
    richer image features, it often reduces the robustness of feature representation
    to a certain extent and increases the number of parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人 [[71](#bib.bib71)] 将所有 U-Net 层（从一层到四层）连接在一起，如图 7 所示。这种结构的优点在于它允许网络自动学习不同层次特征的重要性。此外，跳跃连接被重新设计，使得具有不同语义尺度的特征可以在解码器中聚合，从而形成高度灵活的特征融合方案。缺点是由于密集连接的使用，参数数量增加。因此，将剪枝方法整合到模型优化中，以减少参数数量。同时，采用深度监督
    [[72](#bib.bib72)] 来平衡剪枝引起的分割精度下降。尽管密集连接有助于获得更丰富的图像特征，但它往往在一定程度上降低了特征表示的鲁棒性，并增加了参数数量。
- en: '![Refer to caption](img/06af7bbed9f93b96b4ec5e47ce0e3911.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/06af7bbed9f93b96b4ec5e47ce0e3911.png)'
- en: 'Figure 6: Dense connection architecture  [[70](#bib.bib70)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 密集连接结构 [[70](#bib.bib70)]。'
- en: '![Refer to caption](img/b77be5b31e16f672d99c152b8c184738.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b77be5b31e16f672d99c152b8c184738.png)'
- en: 'Figure 7: The U-Net++ architecture  [[71](#bib.bib71)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: U-Net++ 结构 [[71](#bib.bib71)]。'
- en: II-B2 Inception
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 Inception
- en: For CNNs, deep networks often give better performances than shallow ones, but
    they encounter some new problems such as vanishing gradient, the difficulty of
    network convergence, the requirement of large memory usage, etc. The inception
    structure overcomes these problems. It gives better performance by merging convolution
    kernels in parallel without increasing the depth of networks. This structure is
    able to extract richer image features using multi-scale convolution kernels, and
    to perform feature fusion to obtain better feature representation. Inspired by
    GoogleNet  [[73](#bib.bib73)] [[74](#bib.bib74)], Gu et al.  [[75](#bib.bib75)]
    proposed CE-Net by introducing the inception structure into medical image segmentation.
    The CE-Net adds atrous convolution to each parallel structure to extract features
    on a wide reception field, and adds $1\times 1$ convolution of feature maps, Fig.
    8 shows the architecture of the inception. However, the inception structure is
    complex leading to the difficulty of model modification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CNN，深层网络通常比浅层网络表现更好，但它们面临一些新问题，如梯度消失、网络收敛困难、大量内存需求等。Inception 结构克服了这些问题。通过在不增加网络深度的情况下并行合并卷积核，它提供了更好的性能。该结构能够利用多尺度卷积核提取更丰富的图像特征，并进行特征融合以获得更好的特征表示。受
    GoogleNet [[73](#bib.bib73)] [[74](#bib.bib74)] 启发，Gu 等人 [[75](#bib.bib75)] 提出了
    CE-Net，通过将 inception 结构引入医学图像分割。CE-Net 在每个并行结构中添加了空洞卷积，以在宽广的感受野上提取特征，并添加了 $1\times
    1$ 卷积特征图，图 8 显示了 Inception 的结构。然而，Inception 结构复杂，导致模型修改困难。
- en: '![Refer to caption](img/0d63751a865f891b1ba3699a47198ebb.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d63751a865f891b1ba3699a47198ebb.png)'
- en: 'Figure 8: The inception architecture  [[75](#bib.bib75)]. It contains four
    cascade branches with the gradual increment of the number of atrous convolution,
    from 1 to 1, 3, and 5, then the receptive field of each branch will be 3, 7, 9,
    and 19\. Therefore, the network can extract features from different scales.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: Inception 结构 [[75](#bib.bib75)]。它包含四个级联分支，每个分支的空洞卷积数量逐渐增加，从 1 到 1、3 和
    5，因此每个分支的感受野将分别为 3、7、9 和 19。这样，网络可以从不同尺度提取特征。'
- en: II-B3 Depth Separability
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 深度可分离性
- en: To improve the generalization capability of network models and to reduce the
    requirement of memory usage, many researchers focus on the study of lightweight
    networks for complex medical 3D volume data. Howard et. al.  [[76](#bib.bib76)]
    proposed MobileNet to decompose vanilla convolution into depthwise separable convolution
    and pointwise convolution. The number of vanilla convolution operation is usually
    ${D_{K}\times D_{K}\times M\times N}$, where ${M}$ is the dimension of the input
    feature maps, ${N}$ is the dimension of the output feature maps, ${D_{K}}$ is
    the size of the convolution kernels. However, the number of the channel convolution
    operation is ${D_{K}\times D_{K}\times 1\times M}$ and the point convolution is
    ${1\times 1\times M\times N}$. Compared to vanilla convolution, the computational
    cost of depthwise separable convolution is (1/$N$ + 1/${D_{K}^{2}}$) times than
    that of the vanilla convolution. Based on this, Sandler et al.  [[77](#bib.bib77)]
    proposed MobileNet-V2 that contains a novel layer module, the inverted residual
    with linear bottleneck. In this module, the input is a low-dimensional compressed
    representation which is first expanded to high dimension and then filtered with
    a lightweight depthwise convolution. Features are subsequently projected back
    to a low-dimensional representation with a linear convolution. It allows to significantly
    reduce the memory footprint needed during inference. By extending the depth separable
    convolution to the design of 3D networks, Lei et al.  [[78](#bib.bib78)] proposed
    a lightweight V-Net (LV-Net) with fewer operations than V-Net for liver segmentation.Besides,
    Zhang et al.  [[61](#bib.bib61)] and Huang et al.  [[79](#bib.bib79)] also proposed
    the application of depthwise separable convolutions to the segmentation of 3D
    medical volume data. Other related works for lightweight deep networks can be
    found in  [[80](#bib.bib80)] [[81](#bib.bib81)]. Depthwise separable convolution
    is an effective way to reduce the number of model parameters, but it may result
    in loss of accuracy in medical image segmentation, and thus other approaches (e.g.
    deep supervision) [[78](#bib.bib78)] need to be employed to improve segmentation
    accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高网络模型的泛化能力并减少内存使用的需求，许多研究者关注于针对复杂医疗三维体积数据的轻量级网络的研究。Howard 等人[[76](#bib.bib76)]
    提出了 MobileNet，将传统卷积分解为深度可分离卷积和点卷积。传统卷积的计算量通常为 ${D_{K}\times D_{K}\times M\times
    N}$，其中 ${M}$ 是输入特征图的维度，${N}$ 是输出特征图的维度，${D_{K}}$ 是卷积核的大小。然而，通道卷积的计算量为 ${D_{K}\times
    D_{K}\times 1\times M}$，点卷积的计算量为 ${1\times 1\times M\times N}$。与传统卷积相比，深度可分离卷积的计算成本是传统卷积的
    (1/$N$ + 1/${D_{K}^{2}}$) 倍。基于此，Sandler 等人[[77](#bib.bib77)] 提出了包含新型层模块的 MobileNet-V2，该模块为反向残差和线性瓶颈。在此模块中，输入是低维压缩表示，首先扩展到高维度，然后通过轻量级深度卷积进行滤波。特征随后通过线性卷积被投影回低维表示。这显著减少了推理过程中所需的内存占用。通过将深度可分离卷积扩展到三维网络的设计，Lei
    等人[[78](#bib.bib78)] 提出了比 V-Net 更少操作的轻量级 V-Net (LV-Net) 用于肝脏分割。此外，Zhang 等人[[61](#bib.bib61)]
    和 Huang 等人[[79](#bib.bib79)] 也提出了将深度可分离卷积应用于三维医疗体积数据分割的方案。有关轻量级深度网络的其他相关工作可以在[[80](#bib.bib80)]
    [[81](#bib.bib81)] 中找到。深度可分离卷积是减少模型参数数量的有效方法，但可能会导致医疗图像分割的精度损失，因此需要采用其他方法（例如深度监督）[[78](#bib.bib78)]
    来提高分割精度。
- en: II-B4 Attention Mechanism
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B4 注意力机制
- en: For neural networks, an attention block can selectively change input or assigns
    different weights to input variables according to different importance. In recent
    years, most of researches combining deep learning and visual attention mechanism
    have focused on using masks to form attention mechanisms. The principle of masks
    is to design a new layer that can identify key features from an image, through
    training and learning, and then let networks only focus on interesting areas of
    images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，注意力模块可以根据不同的重要性选择性地改变输入或为输入变量分配不同的权重。近年来，大多数将深度学习与视觉注意力机制相结合的研究都集中在使用掩码来形成注意力机制。掩码的原理是设计一个可以从图像中识别关键特征的新层，通过训练和学习，然后让网络只关注图像中感兴趣的区域。
- en: '*Local Spatial Attention:* The spatial attention block aims to calculate the
    feature importance of each pixel in space-domain and extract the key information
    of an image. Jaderberg et al.  [[82](#bib.bib82)] early proposed a spatial transformer
    network (ST-Net) for image classification by using spatial attention that transforms
    the spatial information of an original image into another space and retains the
    key information. Normal pooling is equivalent to the information merge that easily
    causes the loss of key information. For this problem, a block called spatial transformer
    is designed to extract key information of images by performing a spatial transformation.
    Inspired by this, Oktay et al.  [[83](#bib.bib83)] proposed attention U-Net. The
    improved U-Net uses an attention block to change the output of the encoder before
    fusing features from the encoder and the corresponding decoder. The attention
    block outputs a gating signal to control feature importance of pixels at different
    spatial positions. Fig. 9 shows the architecture. This block combines the Relu
    and sigmoid functions via $1\times 1$ convolution to generate a weight map that
    is corrected by multiplying features from the encoder.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*局部空间注意力：* 空间注意力模块旨在计算每个像素在空间域中的特征重要性，并提取图像的关键信息。Jaderberg 等人 [[82](#bib.bib82)]
    早期提出了一种空间变换网络 (ST-Net)，用于图像分类，通过使用空间注意力将原始图像的空间信息转换到另一个空间，并保留关键信息。普通池化相当于信息合并，容易导致关键信息丢失。为了解决这个问题，设计了一个称为空间变换器的模块，通过执行空间变换来提取图像的关键信息。受到此启发，Oktay
    等人 [[83](#bib.bib83)] 提出了注意力 U-Net。改进的 U-Net 使用注意力模块来改变编码器的输出，然后将编码器和相应解码器的特征进行融合。注意力模块输出一个门控信号，以控制不同空间位置像素的特征重要性。图
    9 显示了这种架构。该模块通过 $1\times 1$ 卷积结合 Relu 和 sigmoid 函数生成一个权重图，通过将特征从编码器中乘以这个权重图来进行修正。'
- en: '![Refer to caption](img/c2255277bf2f8d98c5527393e17dad27.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c2255277bf2f8d98c5527393e17dad27.png)'
- en: 'Figure 9: The attention block in the attention U-Net  [[83](#bib.bib83)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：注意力 U-Net 中的注意力模块 [[83](#bib.bib83)]。
- en: '*Channel Attention:* The channel attention block can achieve feature recalibration,
    which utilizes learned global information to emphasize selectively useful features
    and suppress useless features. Hu et al.  [[84](#bib.bib84)] proposed SE-Net that
    introduced the channel attention to the field of image analysis and won the ImageNet
    Challenge in 2017\. This method implements attention weighting on channels using
    three steps; Fig. 10 shows this architecture. The first is the squeezing operation,
    the global average pooling is performed on input features to obtain the $1\times
    1\times Channel$ feature map. The second is the excitation operation, where channel
    features are interacted to reduce the number of channels, and then the reduced
    channel features are reconstructed back to the number of channels. Finally the
    sigmoid function is employed to generate a feature weight map of $[0,1]$ that
    multiplies the scale back to the original input feature. Chen et al.  [[45](#bib.bib45)]
    proposed FED-Net that uses the SE block to achieve the feature channel attention.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*通道注意力：* 通道注意力模块可以实现特征的重新校准，利用学到的全局信息来选择性地强调有用的特征，并抑制无用的特征。Hu 等人 [[84](#bib.bib84)]
    提出了 SE-Net，将通道注意力引入图像分析领域，并在 2017 年赢得了 ImageNet 挑战赛。该方法通过三个步骤实现通道的注意力加权；图 10 显示了这种架构。第一个步骤是压缩操作，对输入特征进行全局平均池化，以获得
    $1\times 1\times Channel$ 特征图。第二个步骤是激励操作，其中通道特征被交互以减少通道数，然后将减少后的通道特征重建回原来的通道数。最后，使用
    sigmoid 函数生成一个 $[0,1]$ 的特征权重图，将缩放因子乘回原始输入特征。Chen 等人 [[45](#bib.bib45)] 提出了 FED-Net，利用
    SE 模块实现了特征通道注意力。'
- en: '![Refer to caption](img/9ac74b9f2796725bb292948e3ca2a92d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9ac74b9f2796725bb292948e3ca2a92d.png)'
- en: 'Figure 10: The channel attention in the SE-Net  [[84](#bib.bib84)].'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：SE-Net 中的通道注意力 [[84](#bib.bib84)]。
- en: '*Mixture Attention:* Spatial and channel attention mechanisms are two popular
    strategies for improving feature representation. However, the spatial attention
    ignores the difference of different channel information and treats each channel
    equally. On the contrary, the channel attention pools global information directly
    while ignoring local information in each channel, which is a relatively rough
    operation. Therefore, combining advantages of two attention mechanisms, researchers
    have designed many models based on a mixed domain attention block. Kaul et al.
     [[85](#bib.bib85)] proposed the focusNet using a mixture of spatial attention
    and channel attention for medical image segmentation, where the SE-Block is used
    for channel attention and a branch of spatial attention is designed. Besides,
    other related works can be found in  [[80](#bib.bib80)] [[81](#bib.bib81)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*混合注意力：* 空间和通道注意力机制是提高特征表示的两种流行策略。然而，空间注意力忽略了不同通道信息的差异，对每个通道一视同仁。相反，通道注意力直接汇聚全局信息，同时忽略了每个通道中的局部信息，这是一种相对粗略的操作。因此，为了结合这两种注意力机制的优点，研究人员设计了许多基于混合域注意力块的模型。Kaul等人
    [[85](#bib.bib85)] 提出了使用空间注意力和通道注意力混合的focusNet用于医学图像分割，其中SE-Block用于通道注意力，并设计了一个空间注意力分支。此外，其他相关工作可以在
    [[80](#bib.bib80)] [[81](#bib.bib81)] 中找到。'
- en: To improve the feature discriminant representation of networks, Wang et al.
     [[86](#bib.bib86)] embedded an attention block inside the central bottleneck
    between the contraction path and the expansion path of the U-Net, and proposed
    the ScleraSegNet. Furthermore, they compared the performance of channel attention,
    spatial attention, and different combinations of two attentions for medical image
    segmentations. They concluded that the channel-centric attention was the most
    effective in improving image segmentation performance. Based on this conclusion,
    they finally won the championship of the sclera segmentation benchmarking competition
    (SSBC2019).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高网络的特征判别表示，Wang等人 [[86](#bib.bib86)] 将一个注意力块嵌入到U-Net的收缩路径和扩展路径之间的中央瓶颈中，并提出了ScleraSegNet。此外，他们比较了通道注意力、空间注意力和两种注意力的不同组合在医学图像分割中的性能。他们得出结论，通道中心注意力在提高图像分割性能方面最为有效。基于这一结论，他们最终赢得了巩膜分割基准竞赛（SSBC2019）的冠军。
- en: Although those attention mechanisms mentioned above improve the final segmentation
    performance, they only perform an operation of local convolution. The operation
    focuses on the area of neighboring convolution kernels but misses the global information.
    In addition, the operation of down-sampling leads to the loss of spatial information,
    which is especially unfavorable for biomedical image segmentation. A basic solution
    is to extract long-distance information by stacking multiple layers, but this
    is low efficiency due to a large number of parameters and high computational cost.
    In the decoder, the up-sampling, the deconvolution, and the interpolation are
    also performd in the way of local convolution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述注意力机制提高了最终的分割性能，但它们仅执行局部卷积操作。该操作关注于相邻卷积核的区域，但遗漏了全局信息。此外，下采样操作导致空间信息的丧失，这对生物医学图像分割尤其不利。一个基本的解决方案是通过堆叠多层来提取长距离信息，但由于参数众多和计算成本高，效率较低。在解码器中，上采样、反卷积和插值也以局部卷积的方式执行。
- en: '*Non-local Attention:* Recently, Wang et al.  [[87](#bib.bib87)] proposed a
    Non-local U-Net to overcome the drawback of local convolution for medical image
    segmentation. The Non-local U-Net employs the self-attention mechanism and the
    global aggregation block to extract full image information during the parts of
    both up-sampling and down-sampling, which can improve the final segmentation accuracy.
    Fig. 11 shows the global aggregation block. The Non-local block is a general-purpose
    block that can be easily embedded in different convolutional neural networks to
    improve their performance.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*非局部注意力：* 最近，Wang等人 [[87](#bib.bib87)] 提出了一个非局部U-Net，以克服局部卷积在医学图像分割中的缺点。非局部U-Net采用自注意力机制和全局聚合块，在上采样和下采样的过程中提取整个图像的信息，这可以提高最终的分割准确性。图11展示了全局聚合块。非局部块是一个通用块，可以轻松嵌入到不同的卷积神经网络中以提高其性能。'
- en: It can be seen that the attention mechanism is effective for improving image
    segmentation accuracy. In fact, spatial attention looks for interesting target
    regions while channel attention looks for interesting features. The mixed attention
    mechanism can take advantages of both spaces and channels. However, compared with
    the non-local attention, the conventional attention mechanism lacks the ability
    of exploiting the associations between different targets and features, so CNNs
    based on non-local attention usually exhibit better performance than normal CNNs
    for image segmentation tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，注意力机制对于提高图像分割精度是有效的。实际上，空间注意力机制寻找感兴趣的目标区域，而通道注意力机制寻找感兴趣的特征。混合注意力机制可以同时利用空间和通道的优势。然而，与非局部注意力机制相比，传统注意力机制缺乏利用不同目标和特征之间关联的能力，因此基于非局部注意力的卷积神经网络（CNNs）通常在图像分割任务中表现优于普通CNNs。
- en: '![Refer to caption](img/2888c42d488e83569426953ce8c4abc9.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2888c42d488e83569426953ce8c4abc9.png)'
- en: 'Figure 11: The global aggregation block in the Non-Local U-Net  [[87](#bib.bib87)].'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：非局部 U-Net 中的全局聚合块 [[87](#bib.bib87)]。
- en: II-B5 Multi-scale Information Fusion
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B5 多尺度信息融合
- en: One of the challenges in medical image segmentation is a large range of scales
    among objects. For example, a tumor in the middle or late stage could be much
    larger than that in the early stage. The size of perceptive field roughly determines
    how much context information we can use. The general convolution or pooling only
    employs a single kernel, for instance, a $3\times 3$ kernel for convolution and
    a $2\times 2$ kernel for pooling.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割的一个挑战是对象之间的尺度差异范围很大。例如，晚期的肿瘤可能比早期的肿瘤大得多。感受野的大小大致决定了我们可以使用多少上下文信息。一般的卷积或池化只使用单一的核，例如，卷积的
    $3\times 3$ 核和池化的 $2\times 2$ 核。
- en: '*Pyramid Pooling:* The parallel operation of multi-scale pooling can effectively
    improve context information of networks, and thus extract richer semantic information.
    He et al.  [[88](#bib.bib88)] first proposed spatial pyramid pooling (SPP) to
    achieve multi-scale feature extraction. The SPP divides an image from the fine
    space to the coarse space, then gathers local features and extracts multi-scale
    features. Inspired by the SPP, a multi-scale information extraction block is designed
    and named residual multi-kernel pooling (RMP)  [[75](#bib.bib75)] that uses four
    pooling kernels with different sizes to encode global context information. However,
    the up-sampling operation in RMP cannot restore the loss of detail information
    due to pooling that usually enlarges the receptive field but reduces the image
    resolution.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*金字塔池化：* 多尺度池化的并行操作可以有效提升网络的上下文信息，从而提取更丰富的语义信息。He 等人 [[88](#bib.bib88)] 首次提出了空间金字塔池化（SPP）来实现多尺度特征提取。SPP
    将图像从细致空间划分到粗略空间，然后收集局部特征并提取多尺度特征。受 SPP 启发，设计了一个多尺度信息提取块，命名为残差多核池化（RMP） [[75](#bib.bib75)]，该块使用四个不同尺寸的池化核来编码全局上下文信息。然而，RMP
    中的上采样操作无法恢复由于池化而丢失的细节信息，这通常会放大感受野但降低图像分辨率。'
- en: '*Atrous Spatial Pyramid Pooling:* In order to reduce the loss of detail information
    caused by pooling operation, researchers proposed atrous convolution instead of
    the polling operation. Compared with the vanilla convolution, the atrous convolution
    can effectively enlarge the receptive field without increasing the number of parameters.
    Combining advantages of the atrous convolution and the SPP block, Chen et al.
     [[55](#bib.bib55)] proposed the atrous spatial pyramid pooling module (ASPP)
    to improve image segmentation results. The ASPP shows strong recognition capability
    on same objects with different scales. Similarly, Similarly, Lopez et al  [[89](#bib.bib89)]
    and Lei et al  [[90](#bib.bib90)] applied superposition of multi-scale atrous
    convolutions to brain tumor segmentation and liver tumor segmentation, respectively,
    which achieves a clear accuracy improvement.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*空洞空间金字塔池化：* 为了减少池化操作造成的细节信息丢失，研究人员提出了使用空洞卷积替代池化操作。与普通卷积相比，空洞卷积可以有效扩大感受野而不增加参数数量。结合空洞卷积和
    SPP 块的优点，Chen 等人 [[55](#bib.bib55)] 提出了空洞空间金字塔池化模块（ASPP），以改善图像分割结果。ASPP 对于不同尺度的同一对象表现出强大的识别能力。同样，Lopez
    等人 [[89](#bib.bib89)] 和 Lei 等人 [[90](#bib.bib90)] 将多尺度空洞卷积的叠加应用于脑肿瘤分割和肝肿瘤分割，分别取得了明显的精度提升。'
- en: However, the ASPP suffers from two serious problems for image segmentation.
    The first problem is the loss of local information as shown in Fig. 12, where
    we assume that the convolutional kernel is $3\times 3$ and the dilation rate is
    2 for three iterations. The second problem is that the information could be irrelevant
    across large distances. How to simultaneously handle the relationship between
    objects with different scales is important for designing a fine atrous convolutional
    network. In response to the above problems, Wang et al.  [[91](#bib.bib91)] designed
    an hybrid expansion convolution (HDC) networks. This structure uses a sawtooth
    wave-like heuristic to allocate the dilation rate, so that information from a
    wider pixel range can be accessed and thus the gridding effect is suppressed.
    In  [[91](#bib.bib91)], authors gave several atrous convolution sequences using
    variable dilation rate, e.g., [1,2,3], [3,4,5], [1,2,5], [5,9,17], and [1,2,5,9].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ASPP 在图像分割中存在两个严重问题。第一个问题是局部信息的丢失，如图 12 所示，我们假设卷积核为 $3\times 3$，膨胀率为 2，进行三次迭代。第二个问题是信息可能在较大距离上不相关。如何同时处理不同尺度的物体之间的关系，对于设计精细的空洞卷积网络至关重要。针对上述问题，Wang
    等人 [[91](#bib.bib91)] 设计了混合扩展卷积（HDC）网络。该结构使用锯齿波状启发式方法来分配膨胀率，从而可以访问更广泛的像素范围，抑制了网格效应。在
    [[91](#bib.bib91)] 中，作者给出了几种使用可变膨胀率的空洞卷积序列，例如，[1,2,3]、[3,4,5]、[1,2,5]、[5,9,17]
    和 [1,2,5,9]。
- en: '![Refer to caption](img/0d810ade8670965fac469de3281c409e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d810ade8670965fac469de3281c409e.png)'
- en: 'Figure 12: The gridding effect (the way of treating images as a chessboard
    causes the loss of information continuity).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 网格效应（将图像视为棋盘的处理方式导致信息连续性的丧失）。'
- en: '*Non-local and ASPP:* The atrous convolution can efficiently enlarge the receptive
    field to collect richer semantic information, but it causes the loss of detail
    information due to the gridding effect. Therefore, it is necessary to add constraints
    or establish pixel associations for improving the atrous convolution performance.
    Recently, Yang et al.  [[92](#bib.bib92)] proposed a combination block of ASPP
    and Non-local for the segmentation of human body parts, as shown in Fig. 13\.
    ASPP uses multiple parallel atrous convolutions with different scales to capture
    richer information, and the Non-local operation captures a wide range of dependencies.
    This combination possesses advantages of both ASPP and Non-local, and it has a
    good application prospect for medical image segmentation.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*非局部和 ASPP:* 空洞卷积可以有效地扩大感受野，以收集更丰富的语义信息，但由于网格效应，它会导致细节信息的丢失。因此，有必要添加约束或建立像素关联，以改善空洞卷积的性能。最近，Yang
    等人 [[92](#bib.bib92)] 提出了 ASRR 和非局部的组合模块用于人体部位的分割，如图 13 所示。ASPP 使用多个不同尺度的并行空洞卷积来捕获更丰富的信息，而非局部操作则捕获了广泛的依赖关系。这种组合具有
    ASPP 和非局部的优点，并且在医学图像分割中具有良好的应用前景。'
- en: '![Refer to caption](img/b27ecc5ffe8feaad578efccec96483fe.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b27ecc5ffe8feaad578efccec96483fe.png)'
- en: 'Figure 13: The combination of ASPP and Non-local architecture  [[92](#bib.bib92)].'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: ASPP 和非局部架构的组合 [[92](#bib.bib92)]。'
- en: The network function module is designed to perform more efficient feature fusion.
    When feature is usually extracted by the encoder, the feature is usually fused
    by the network function module to enhance the feature representation. Feature
    fusion is usually performed by fusing different scale information or performing
    a more efficient way of feature transfer. Then the feature is passed through the
    decoder to obtain a better segmentation result.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网络功能模块旨在实现更高效的特征融合。当特征通常由编码器提取时，网络功能模块通常会对特征进行融合以增强特征表示。特征融合通常通过融合不同尺度的信息或执行更高效的特征传递方式来实现。然后，特征通过解码器传递，以获得更好的分割结果。
- en: II-C Loss Function
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 损失函数
- en: In addition to improved segmentation speed and accuracy by designing network
    backbone and the function block, designing new loss functions also resulted in
    improvements in subsequent inference-time segmentation accuracy. Therefore, a
    great deal of work has been reported about the design of suitable loss functions
    for medical image segmentation tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过设计网络骨干和功能模块来提高分割速度和准确性外，设计新的损失函数也提高了后续推理时的分割准确性。因此，关于医学图像分割任务适用损失函数的设计已有大量研究报道。
- en: II-C1 Cross Entropy Loss
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 交叉熵损失
- en: For image segmentation tasks, the cross entropy is one of the most popular loss
    functions. The function compares pixel-wisely the predicted category vector with
    the real segmentation result vector. For the case of binary segmentation, let
    $P(Y=1)=p$ and $P(Y=0)=1-p$, then the prediction is given by the sigmoid function,
    where $P(\hat{Y}=1)=1/(1+e^{-x})=\hat{p}$ and $P(\hat{Y}=0)=1-1/(1+e^{-x})=1-\hat{p}$,
    $x$ is the output of neural networks. The cross entropy loss is defined as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分割任务，交叉熵是最常用的损失函数之一。该函数逐像素比较预测类别向量与真实分割结果向量。对于二分类分割情况，设 $P(Y=1)=p$ 和 $P(Y=0)=1-p$，则预测由
    sigmoid 函数给出，其中 $P(\hat{Y}=1)=1/(1+e^{-x})=\hat{p}$ 和 $P(\hat{Y}=0)=1-1/(1+e^{-x})=1-\hat{p}$，$x$
    是神经网络的输出。交叉熵损失定义为
- en: '|  | $\displaystyle CE(p,\hat{p})=-(plog(\hat{p})+(1-p)log(1-\hat{p})).$ |  |
    (1) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle CE(p,\hat{p})=-(plog(\hat{p})+(1-p)log(1-\hat{p})).$ |  |
    (1) |'
- en: II-C2 Weighted Cross Entropy Loss
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 加权交叉熵损失
- en: The cross entropy loss deals with each pixel of images equally, and thus outputs
    an average value, which ignores the class imbalance and leads to a problem that
    the loss function depends on the class including the maximal number of pixels.
    Therefore, the cross entropy loss often shows low performance for small target
    segmentation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失对图像的每个像素给予相同的处理，因此输出一个平均值，这忽略了类别不平衡的问题，并导致损失函数依赖于包含最大像素数的类别。因此，交叉熵损失在小目标分割中往往表现较差。
- en: To address the problem of class imbalance, Long et al.  [[32](#bib.bib32)] proposed
    weighted cross entropy loss (WCE) to counteract the class imbalance. For the case
    of binary segmentation, the weighted cross entropy loss is defined as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决类别不平衡的问题，Long 等人[[32](#bib.bib32)] 提出了加权交叉熵损失（WCE）来对抗类别不平衡。对于二分类分割情况，加权交叉熵损失定义为
- en: '|  | $\displaystyle WCE(p,\hat{p})=-(\beta plog(\hat{p})+(1-p)log(1-\hat{p})),$
    |  | (2) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle WCE(p,\hat{p})=-(\beta plog(\hat{p})+(1-p)log(1-\hat{p})),$
    |  | (2) |'
- en: where $\beta$ is used to tune the proportion of positive and negative samples,
    and it is an empirical value. If $\beta>1$, the number of false negatives will
    be decreased; on the contrary, the number of false positives will be decreased
    when $\beta<1$. In fact, the cross entropy is a special case of the weighted cross
    entropy when $\beta=1$. To adjust the weight of positive and negative samples
    simultaneously, we can use the balanced cross entropy (BCE) loss function that
    is defined as
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 用于调节正负样本的比例，它是一个经验值。如果 $\beta>1$，则假阴性数量会减少；相反，当 $\beta<1$ 时，假阳性数量会减少。实际上，交叉熵是在
    $\beta=1$ 时加权交叉熵的特殊情况。为了同时调整正负样本的权重，我们可以使用定义为
- en: '|  | $\displaystyle BCE(p,\hat{p})=-(\beta plog(\hat{p})+(1-\beta)(1-p)log(1-\hat{p})).$
    |  | (3) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle BCE(p,\hat{p})=-(\beta plog(\hat{p})+(1-\beta)(1-p)log(1-\hat{p})).$
    |  | (3) |'
- en: In  [[7](#bib.bib7)], Ronneberger et al. proposed U-Net in which the cross entropy
    loss function is improved by adding a distance function. The improved loss function
    is able to improve the learning capability of models for inter-class distance.
    The distance function is defined as
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[7](#bib.bib7)]中，Ronneberger 等人提出了 U-Net，在该模型中，通过添加距离函数改进了交叉熵损失函数。改进后的损失函数能够提高模型对类别间距离的学习能力。距离函数定义为
- en: '|  | $\displaystyle D(x)=\omega_{0}\texttimes e^{\frac{-(d_{1}(x)+d_{2}(x)^{2}}{2\sigma^{2}}},$
    |  | (4) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D(x)=\omega_{0}\texttimes e^{\frac{-(d_{1}(x)+d_{2}(x)^{2}}{2\sigma^{2}}},$
    |  | (4) |'
- en: where both $d_{1}(x)$ and $d_{2}(x)$ denote the distance between the pixel $x$
    and boundaries of the first two nearest cells. So the final loss function is defined
    as
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{1}(x)$ 和 $d_{2}(x)$ 分别表示像素 $x$ 到前两个最近细胞边界的距离。因此，最终的损失函数定义为
- en: '|  | $\displaystyle L=\ BCE\left(p,\hat{p}\right)+\ D(x).$ |  | (5) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L=\ BCE\left(p,\hat{p}\right)+\ D(x).$ |  | (5) |'
- en: II-C3 Dice Loss
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 Dice 损失
- en: The Dice is a popular performance metric for the evaluation of medical image
    segmentation. This metric is essentially a measure of overlap between a segmentation
    result and corresponding ground truth. The value of Dice ranges from 0 to 1\.
    “1” means the segmentation result completely overlaps with the real segmentation
    result. The calculation formula is defined as
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 系数是评估医学图像分割的一个常用性能指标。这个指标本质上是分割结果与相应真实值之间重叠程度的度量。Dice 的值范围从 0 到 1。"1" 表示分割结果完全与真实分割结果重合。计算公式定义为
- en: '|  | $\displaystyle Dice\left(A,B\right)=\frac{2\times\left&#124;A\cap B\right&#124;}{A+B}\times
    100\%,$ |  | (6) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Dice\left(A,B\right)=\frac{2\times\left|A\cap B\right|}{A+B}\times
    100\%,$ |  | (6) |'
- en: where $A$ is a predicted segmentation result and $B$ is a real segmentation
    result.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$A$是预测的分割结果，$B$是实际的分割结果。
- en: For 3D medical volume data segmentation, Milletari et al.  [[35](#bib.bib35)]
    proposed V-Net that employs the Dice loss
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于3D医学体积数据分割，Milletari等人[[35](#bib.bib35)]提出了V-Net，该网络使用Dice损失
- en: '|  | $\displaystyle DL(p,\hat{p})=1-\frac{2<p,\hat{p}>}{\left\&#124;p\right\&#124;_{1}+\left\&#124;p\right\&#124;_{2}},$
    |  | (7) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle DL(p,\hat{p})=1-\frac{2<p,\hat{p}>}{\left\|p\right\|_{1}+\left\|p\right\|_{2}},$
    |  | (7) |'
- en: where $<p,\hat{p}>$ represents the dot product of the ground truth of each channel
    and the prediction result matrix.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$<p,\hat{p}>$表示每个通道的真实值和预测结果矩阵的点积。
- en: It is worth noting that the Dice loss is suitable for uneven samples. However,
    the use of the Dice loss easily influences the back propagation and leads to a
    training difficulty. Besides, the Dice loss has a low robustness for different
    models such as mean surface distance or Hausdorff surface distance due to unbelievable
    gradient values. For example, the gradient of softmax function can be simplified
    to ($p-t$), where $t$ is the target value, and $p$ is the predicted value, but
    the value of dice loss is $2t^{2}$/${(p+t)}^{2}$. If values of $p$ and $t$ are
    too small, then the gradient value will change drastically leading to training
    difficulty.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Dice损失适用于不均匀样本。然而，使用Dice损失容易影响反向传播，导致训练困难。此外，由于梯度值不可靠，Dice损失对不同模型（如平均表面距离或Hausdorff表面距离）的鲁棒性较差。例如，softmax函数的梯度可以简化为($p-t$)，其中$t$是目标值，$p$是预测值，但Dice损失的值为$2t^{2}$/${(p+t)}^{2}$。如果$p$和$t$的值太小，梯度值会发生剧烈变化，从而导致训练困难。
- en: II-C4 Tversky Loss
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C4 Tversky损失
- en: Salehi et al.  [[93](#bib.bib93)] proposed the Tversky Loss (TL) that is a regularized
    version of Dice loss to control the contribution of both false positive and false
    negative to the loss function. The TL is defined as
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Salehi等人[[93](#bib.bib93)]提出了Tversky损失（TL），这是一种Dice损失的正则化版本，用于控制假阳性和假阴性对损失函数的贡献。TL定义为
- en: '|  | $\displaystyle TL(p,\hat{p})=\frac{p,\hat{p}}{p,\hat{p}+\beta(1-p,\hat{p})+(1-\beta)(p,1-\hat{p})},$
    |  | (8) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle TL(p,\hat{p})=\frac{p,\hat{p}}{p,\hat{p}+\beta(1-p,\hat{p})+(1-\beta)(p,1-\hat{p})},$
    |  | (8) |'
- en: where $p\in{0,1}$ and $0\leq\hat{p}\leq 1$. $p$ and $\hat{p}$ are the ground
    truth and predicted segmentation, respectively. TL is equivalent to (7) if $\beta=0.5$.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p\in{0,1}$且$0\leq\hat{p}\leq 1$。$p$和$\hat{p}$分别是真实值和预测分割。如果$\beta=0.5$，则TL等于（7）。
- en: II-C5 Generalized Dice Loss
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C5 广义Dice损失
- en: Although the Dice loss can solve the problem of class imbalance to a certain
    extent, it does not work for serious class imbalance. For instance, small targets
    suffer from prediction errors of some pixels, which easily causes a large change
    for Dice values. Sudre et al.  [[94](#bib.bib94)] proposed an Generalized Dice
    Loss (GDL), the GDL is defined as
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Dice损失在一定程度上可以解决类别不平衡的问题，但对于严重的类别不平衡情况效果不佳。例如，小目标的某些像素的预测误差可能导致Dice值发生大幅变化。Sudre等人[[94](#bib.bib94)]提出了一种广义Dice损失（GDL），GDL定义为
- en: '|  | $\displaystyle GDL\left(p,\hat{p}\right)=1-\frac{1}{m}\frac{2\sum_{j=1}^{m}{\omega_{j}\sum_{i=1}^{n}{p_{ij}{\hat{p}}_{ij}}}}{\sum_{j=1}^{m}{\omega_{j}\sum_{i=1}^{n}{(p_{ij}{+\hat{p}}_{ij})}}},$
    |  | (9) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle GDL\left(p,\hat{p}\right)=1-\frac{1}{m}\frac{2\sum_{j=1}^{m}{\omega_{j}\sum_{i=1}^{n}{p_{ij}{\hat{p}}_{ij}}}}{\sum_{j=1}^{m}{\omega_{j}\sum_{i=1}^{n}{(p_{ij}{+\hat{p}}_{ij})}}},$
    |  | (9) |'
- en: where the weight $\omega=\left[\omega_{1},\omega_{2},...,\omega_{m}\right]$
    is assigned to each class, and $\omega_{j}=1/{(\sum_{i=1}^{n}p_{ij})}^{2}$. The
    GDL is superior to the Dice loss since different areas have the similar contributions
    to the loss, and the GDL is more stable and robust during the training process.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，权重$\omega=\left[\omega_{1},\omega_{2},...,\omega_{m}\right]$被分配给每个类别，且$\omega_{j}=1/{(\sum_{i=1}^{n}p_{ij})}^{2}$。GDL优于Dice损失，因为不同区域对损失的贡献相似，并且GDL在训练过程中更加稳定和健壮。
- en: II-C6 Boundary Loss
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C6 边界损失
- en: To solve the problem of class imbalance, Kervadec et al.  [[95](#bib.bib95)]
    proposed a new boundary loss used for brain lesion segmentation. This loss function
    aims to minimize the distance between segmented boundaries and labeled boundaries.
    Authors conducted experiments on two imbalanced datasets with labels. The results
    show that the combination of the Dice loss and the boundary loss is superior to
    the single one. The composite loss is defined as
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决类别不平衡的问题，Kervadec 等人 [[95](#bib.bib95)] 提出了用于脑部病变分割的新边界损失。该损失函数旨在最小化分割边界与标注边界之间的距离。作者在两个标注的类别不平衡数据集上进行了实验。结果表明，Dice
    损失与边界损失的结合优于单独使用的损失。复合损失定义为
- en: '|  | $\displaystyle L=\alpha L_{GD}(\theta)+(1-\alpha)L_{B}(\theta),$ |  |
    (10) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L=\alpha L_{GD}(\theta)+(1-\alpha)L_{B}(\theta),$ |  |
    (10) |'
- en: where the first part is a regularized Dice Loss that is defined as
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一部分是正则化的 Dice 损失，定义为
- en: '|  | $\displaystyle L_{GD}\left(\theta\right)=1-2(\omega_{G}\sum_{p\epsilon\Omega}{g(p)s_{\theta}(p)}$
    |  | (11) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{GD}\left(\theta\right)=1-2(\omega_{G}\sum_{p\epsilon\Omega}{g(p)s_{\theta}(p)}$
    |  | (11) |'
- en: '|  | $\displaystyle+\omega_{B}\sum_{p\epsilon\Omega}{(1-g(p))(1-s_{\theta}(p))})/$
    |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+\omega_{B}\sum_{p\epsilon\Omega}{(1-g(p))(1-s_{\theta}(p))})/$
    |  |'
- en: '|  | $\displaystyle((\omega_{G}\sum_{p\epsilon\Omega}\left[g(p){+s}_{\theta}(p)\right]$
    |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle((\omega_{G}\sum_{p\epsilon\Omega}\left[g(p){+s}_{\theta}(p)\right]$
    |  |'
- en: '|  | $\displaystyle+\omega_{B}\sum_{p\epsilon\Omega}{(2-g(p)-s_{\theta}(p))})),$
    |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+\omega_{B}\sum_{p\epsilon\Omega}{(2-g(p)-s_{\theta}(p))})),$
    |  |'
- en: and the second part is the boundary loss that is defined as
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是边界损失，定义为
- en: '|  | $\displaystyle L_{B}\left(\theta\right)=\emptyset G(p)s_{\theta}(p),$
    |  | (12) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{B}\left(\theta\right)=\emptyset G(p)s_{\theta}(p),$
    |  | (12) |'
- en: where if $p\epsilon G$, then $\emptyset G\left(p\right)=-||p-z_{\vartheta G}(p)||$,
    otherwise $\emptyset G\left(p\right)=||p-z_{\vartheta G}(p)||$. Besides, $\sum_{\mathrm{\Omega}}{g(p)f(s_{\theta}(p))}$
    is used for the foreground and $\sum_{\mathrm{\Omega}}{(1-g(p))(1-f(s_{\theta}(p)))}$
    is used for the background. The $L_{GD}\left(\theta\right)$ weight is $\omega_{G}=1/{(\sum_{p\epsilon\Omega}{g(p)})}^{2}$
    and the $\omega_{B}=1/{(\sum_{p\epsilon\Omega}{(1-g(p))})}^{2}$. The $\mathrm{\Omega}$
    represents the pixel set in the entire spatial domain.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $p\epsilon G$，则 $\emptyset G\left(p\right)=-||p-z_{\vartheta G}(p)||$，否则
    $\emptyset G\left(p\right)=||p-z_{\vartheta G}(p)||$。此外，$\sum_{\mathrm{\Omega}}{g(p)f(s_{\theta}(p))}$
    用于前景，$\sum_{\mathrm{\Omega}}{(1-g(p))(1-f(s_{\theta}(p)))}$ 用于背景。$L_{GD}\left(\theta\right)$
    的权重为 $\omega_{G}=1/{(\sum_{p\epsilon\Omega}{g(p)})}^{2}$，$\omega_{B}=1/{(\sum_{p\epsilon\Omega}{(1-g(p))})}^{2}$。$\mathrm{\Omega}$
    代表整个空间域中的像素集。
- en: II-C7 Exponential Logarithmic Loss
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C7 指数对数损失
- en: In (9), the weighted dice loss is actually that the obtained dice value divides
    the sum of each label, which achieves a balance for objects with different scales.
    Therefore, by combining focal loss  [[96](#bib.bib96)] and dice loss, Wong et
    al.  [[97](#bib.bib97)] proposed the exponential logarithmic loss (EXP loss) used
    for brain segmentation to solve problem of serious class imbalance. With the introduction
    of the exponential form, the nonlinearity of the loss functions can be further
    controlled to improve the segmentation accuracy. The EXP loss function is defined
    as
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (9) 中，加权 Dice 损失实际上是将得到的 Dice 值除以每个标签的总和，从而实现对不同规模对象的平衡。因此，通过结合焦点损失 [[96](#bib.bib96)]
    和 Dice 损失，Wong 等人 [[97](#bib.bib97)] 提出了用于脑部分割的指数对数损失（EXP 损失），以解决严重的类别不平衡问题。引入指数形式后，损失函数的非线性可以进一步控制，从而提高分割精度。EXP
    损失函数定义为
- en: '|  | $\displaystyle L_{EXP}=\omega_{dice}\times L_{dice}+\omega_{cross}\times
    L_{cross},$ |  | (13) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{EXP}=\omega_{dice}\times L_{dice}+\omega_{cross}\times
    L_{cross},$ |  | (13) |'
- en: where two new parameter weights are denoted by $\omega_{dice}$ and $\omega_{cross}$,
    respectively. The $L_{dice}$ is an exponential log Dice loss, and the $L_{cross}$is
    a cross entropy loss
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中两个新参数权重分别表示为 $\omega_{dice}$ 和 $\omega_{cross$。$L_{dice}$ 是指数对数 Dice 损失，而
    $L_{cross}$ 是交叉熵损失
- en: '|  | $\displaystyle L_{dice}=E[{(-ln({Dice}_{i}))}^{\gamma_{Dice}}],$ |  |
    (14) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{dice}=E[{(-ln({Dice}_{i}))}^{\gamma_{Dice}}],$ |  |
    (14) |'
- en: '|  | $\displaystyle L_{cross}=E[{\omega_{l}(-ln(p_{l}(x)))}^{\gamma_{cross}}],$
    |  | (15) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{cross}=E[{\omega_{l}(-ln(p_{l}(x)))}^{\gamma_{cross}}],$
    |  | (15) |'
- en: and,
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 和，
- en: '|  | $\displaystyle{Dice}_{i}=\frac{2(\sum_{x}{\sigma_{il}(x)p_{i}(x)})+\varepsilon}{\sum_{x}{{(\sigma}_{il}(x)+p_{i}(x)})+\varepsilon},$
    |  | (16) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{Dice}_{i}=\frac{2(\sum_{x}{\sigma_{il}(x)p_{i}(x)})+\varepsilon}{\sum_{x}{{(\sigma}_{il}(x)+p_{i}(x)})+\varepsilon},$
    |  | (16) |'
- en: '|  | $\displaystyle\omega_{l}={(\frac{\sum_{k}f_{k}}{f_{l}})}^{0.5},$ |  |
    (17) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\omega_{l}={(\frac{\sum_{k}f_{k}}{f_{l}})}^{0.5},$ |  |
    (17) |'
- en: where $x$ is pixel position, $i$ is the label and $l$ is the ground-truth value
    at the position $x$. The $p_{i}(x)$ is the probability value outputted from the
    softmax.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是像素位置，$i$ 是标签，$l$ 是位置 $x$ 处的真实值。$p_{i}(x)$ 是从 softmax 输出的概率值。
- en: In (17), $f_{k}$ is the frequency of occurrence of the label $k$, this parameter
    can reduce the influence of more frequently seen labels. Both $\gamma_{Dice}$
    and $\gamma_{cross}$ are used to enhance the nonlinearity of the loss function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在（17）中，$f_{k}$ 是标签 $k$ 的出现频率，这个参数可以减少出现频率较高标签的影响。$\gamma_{Dice}$ 和 $\gamma_{cross}$
    都用于增强损失函数的非线性。
- en: II-C8 Loss Improvements
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C8 损失改进
- en: For medical image segmentation, the improvement of loss mainly focuses on the
    problem of segmentation of small objects in a large background (the problem of
    class imbalance). Chen et al.  [[98](#bib.bib98)] proposed a new loss function
    by applying traditional active contour energy minimization to convolutional neural
    networks, Li et al.  [[99](#bib.bib99)] proposed a new regularization term to
    improve the cross-entropy loss function, and Karimi et al.  [[100](#bib.bib100)]
    proposed a loss function based on Hausdorff distance (HD). Besides, there are
    still a lot of works  [[101](#bib.bib101)] [[102](#bib.bib102)] trying to deal
    with this problem by adding penalties to loss functions or changing the optimization
    strategy according to specific tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学图像分割，损失的改进主要集中在大背景下小物体的分割问题（类别不平衡问题）。Chen 等人 [[98](#bib.bib98)] 提出了通过将传统的活动轮廓能量最小化应用于卷积神经网络的新损失函数，Li
    等人 [[99](#bib.bib99)] 提出了一个新的正则化项来改进交叉熵损失函数，Karimi 等人 [[100](#bib.bib100)] 提出了基于
    Hausdorff 距离 (HD) 的损失函数。此外，还有许多工作 [[101](#bib.bib101)] [[102](#bib.bib102)] 试图通过向损失函数添加惩罚或根据具体任务改变优化策略来解决这个问题。
- en: In many medical image segmentation tasks, there are often only one or two targets
    in an image, and the pixel ratio of targets is sometimes small, which makes network
    training difficult. Therefore, to improve network training and segmentation accuracy,
    it is easier to focus on smaller targets by changing loss functions than to change
    the network structure. However, the design of loss functions is highly task-specific,
    so we need to analyze carefully task requirement, and then design reasonable and
    available loss functions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多医学图像分割任务中，图像中通常只有一两个目标，而且目标的像素比例有时较小，这使得网络训练变得困难。因此，为了提高网络训练和分割准确性，通过改变损失函数来关注较小的目标比改变网络结构更容易。然而，损失函数的设计高度依赖于任务，因此我们需要仔细分析任务需求，然后设计合理且可用的损失函数。
- en: II-C9 Deep supervision
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C9 深度监督
- en: In general, the increase of network depth can improve the feature representation
    of networks to some extent, but it simultaneously causes new problems such as
    vanishing gradient and gradient explosion. In order to train deep networks effectively,
    Lee et al.  [[72](#bib.bib72)] proposed Deeply-supervised nets (DSNs) by adding
    some auxiliary branching classifiers to some layers of the neural network. Dou
    et al.  [[103](#bib.bib103)] proposed a 3D DSN for heart and liver segmentation,
    which incorporates a 3D deep monitoring mechanism into a 3D full convolutional
    network for volume-to-volume learning and inference, eliminating redundant computation
    and reducing the risk of over-fitting in the case of limited training data. Similarly,
    Dou et al  [[104](#bib.bib104)] presented a method for fetal brain MRI cortical
    plate segmentation using a fully convolutional neural network architecture with
    deep supervision and residual connection, and obtained high segmentation accuracy
    for brain MRI cortical plate segmentation. In fact, deep supervision not only
    can constrain the discrimination and robustness of learned features at all stages,
    but also improves network training efficiency.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，增加网络深度可以在一定程度上改善网络的特征表示，但同时会导致梯度消失和梯度爆炸等新问题。为了有效训练深度网络，Lee 等人 [[72](#bib.bib72)]
    通过在神经网络的某些层中添加一些辅助分支分类器，提出了深度监督网络 (DSNs)。Dou 等人 [[103](#bib.bib103)] 提出了用于心脏和肝脏分割的
    3D DSN，将 3D 深度监控机制集成到 3D 全卷积网络中，用于体积到体积的学习和推断，消除了冗余计算，并在训练数据有限的情况下降低了过拟合的风险。同样，Dou
    等人 [[104](#bib.bib104)] 提出了使用具有深度监督和残差连接的全卷积神经网络架构进行胎儿大脑 MRI 皮层板分割的方法，并获得了高精度的脑部
    MRI 皮层板分割。实际上，深度监督不仅可以约束学习特征在各个阶段的判别力和鲁棒性，还提高了网络训练效率。
- en: III Weakly supervised learning
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 弱监督学习
- en: Although convolutional neural networks show strong adaptability for medical
    image segmentation, segmentation results seriously depend on high-quality labels.
    In fact, it is rare to build many datasets with high-quality labels, especially
    in the field of medical image analysis, since data acquisition and labeling often
    incur high costs. Therefore, a lot of studies on incomplete or imperfect datasets
    are reported. We summarize these studies as weakly supervised learning as shown
    in Fig. 14.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积神经网络在医学图像分割中表现出强大的适应性，但分割结果严重依赖高质量的标签。实际上，很少能建立许多具有高质量标签的数据集，特别是在医学图像分析领域，因为数据获取和标注通常会产生高成本。因此，许多关于不完整或不完美数据集的研究被报告出来。我们将这些研究总结为弱监督学习，如图14所示。
- en: '![Refer to caption](img/9d95b46908798683f8fef437c13badb5.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d95b46908798683f8fef437c13badb5.png)'
- en: 'Figure 14: The weakly supervised learning methods for medical image segmentation.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：医学图像分割的弱监督学习方法。
- en: III-A Data Augmentation
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据增强
- en: In the absence of largely labeled datasets, data augmentation is an effective
    solution to this problem. However, general data expansion methods produce images
    that are highly correlated with original images. Compared to common data augmentation
    approaches, GAN proposed by Goodfellow  [[64](#bib.bib64)] is currently a popular
    strategy for data augmentation since GAN overcomes the problem of reliance on
    original data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模标注数据集缺乏的情况下，数据增强是解决此问题的有效方法。然而，一般的数据扩展方法会生成与原始图像高度相关的图像。与常见的数据增强方法相比，由Goodfellow
    提出的 GAN [[64](#bib.bib64)] 目前是一种流行的数据增强策略，因为GAN克服了对原始数据依赖的问题。
- en: '*Traditional Methods:* General data augmentation methods include the improvement
    of image quality such as noise suppression, the change of image intensity such
    as brightness, saturation, and contrast, and the change of image layout such as
    rotation, distortion, and scaling, etc. Sirinukunwattana et al.  [[105](#bib.bib105)]
    utilized the Gaussian blur to achieve data enhancement, which is helpful for performing
    gland segmentation tasks in the colon tissue images. Dong et al.  [[106](#bib.bib106)]
    randomly used the brightness enhancement function in 3D MR images to enrich training
    data for brain tumor segmentation. Contrast enhancement is usually helpful when
    an image shows uneven intensity. Furthermore, Ronneberger et al.  [[7](#bib.bib7)]
    used random elastic deformation to perform data expansion on the original dataset.
    In fact, the most commonly method used for traditional data augmentation is parametric
    transformation (rotation, translation, shear, shift, flip, …). Since this kind
    of transformation is virtual without computational cost and the annotation on
    medical images is difficult, it is always performed before each training session.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*传统方法：* 一般的数据增强方法包括图像质量的改善，如噪声抑制，图像强度的变化，如亮度、饱和度和对比度，以及图像布局的变化，如旋转、畸变和缩放等。Sirinukunwattana
    等人 [[105](#bib.bib105)] 利用高斯模糊来实现数据增强，这对在结肠组织图像中执行腺体分割任务非常有帮助。Dong 等人 [[106](#bib.bib106)]
    随机使用了3D MR图像中的亮度增强功能，以丰富脑肿瘤分割的训练数据。对比度增强通常对图像强度不均匀时有帮助。此外，Ronneberger 等人 [[7](#bib.bib7)]
    使用随机弹性变形对原始数据集进行数据扩展。实际上，传统数据增强最常用的方法是参数变换（旋转、平移、剪切、位移、翻转等）。由于这种变换是虚拟的且没有计算成本，而医学图像的标注困难，因此通常在每次训练前进行。'
- en: '*Conditional Generative Adversarial Nets (cGAN):* In contrast to the use of
    cGAN for supervised learning introduced in Section II, this section focuses on
    the use of cGAN for data augmentation. An original GAN generator denoted by $G$
    can learn data distribution, but generated pictures are random, which means that
    the generation process of the $G$ is an unguided state. In contrast, cGAN adds
    a condition to the original GAN in order to guide the generation process of the
    $G$. Fig. 15 shows the architecture of cGAN. Guibas et al.  [[107](#bib.bib107)]
    proposed a network architecture composed of a GAN  [[64](#bib.bib64)] and a cGAN
     [[108](#bib.bib108)]. The random variables are input into the GAN leading to
    the generation of a synthetic image of fundus blood vessel label, then the generated
    label map is input into the conditional GAN to generate a real retinal fundi image.
    Finally, authors verified the authenticity of synthesized images by checking whether
    the classifier can distinguish a synthesized image from a real image. Mahapatra
    et al.  [[109](#bib.bib109)] used a cGAN to synthesize X-ray images with required
    abnormalities, this model considers abnormal X-ray images and lung segmentation
    labels as inputs, and then generates synthetic X-ray images with same diseases
    as input X-ray images. At the same time, the segmentated label is obtained. In
    addition, there are also some other works  [[110](#bib.bib110)] [[111](#bib.bib111)]
    using GAN or cGAN to generate images to achieve data enhancement. Although the
    image generated by cGAN has many defects, such as blurred boundary and low resolution,
    the cGAN provides a basic ideas for the later CycleGAN  [[112](#bib.bib112)] and
    StarGAN  [[113](#bib.bib113)] used for the conversion of image styles.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*条件生成对抗网络（cGAN）：* 与第二节中介绍的用于监督学习的 cGAN 使用不同，本节关注 cGAN 在数据增强中的应用。一个原始的 GAN 生成器记作
    $G$ 可以学习数据分布，但生成的图像是随机的，这意味着 $G$ 的生成过程是无指导的状态。相比之下，cGAN 向原始 GAN 添加了一个条件，以指导 $G$
    的生成过程。图 15 展示了 cGAN 的架构。Guibas 等人 [[107](#bib.bib107)] 提出了一个由 GAN [[64](#bib.bib64)]
    和 cGAN [[108](#bib.bib108)] 组成的网络架构。随机变量输入到 GAN 中，从而生成一个合成的眼底血管标签图像，然后将生成的标签图输入到条件
    GAN 中以生成真实的视网膜眼底图像。最后，作者通过检查分类器是否能区分合成图像和真实图像来验证合成图像的真实性。Mahapatra 等人 [[109](#bib.bib109)]
    使用 cGAN 合成具有所需异常的 X 光图像，该模型将异常 X 光图像和肺部分割标签作为输入，然后生成具有与输入 X 光图像相同疾病的合成 X 光图像。同时，还获得了分割标签。此外，还有一些其他工作
    [[110](#bib.bib110)] [[111](#bib.bib111)] 使用 GAN 或 cGAN 生成图像以实现数据增强。尽管 cGAN 生成的图像存在许多缺陷，如边界模糊和分辨率低，但
    cGAN 为后来的 CycleGAN [[112](#bib.bib112)] 和 StarGAN [[113](#bib.bib113)] 提供了基本思路，用于图像风格转换。'
- en: '![Refer to caption](img/72f5a130e58df1f93a6e399d8ad1549e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72f5a130e58df1f93a6e399d8ad1549e.png)'
- en: 'Figure 15: The cGAN architecture  [[108](#bib.bib108)].'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：cGAN 架构 [[108](#bib.bib108)]。
- en: III-B Transfer Learning
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 迁移学习
- en: By utilizing trained parameters of a model to initialize a new model, transfer
    learning can achieve fast model training for data with limited labels. One approach
    is to fine-tune the pre-trained model on ImageNet for the target medical image
    analysis task, while the other is to migrate the training for data from across
    domains.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用已训练模型的参数来初始化新模型，迁移学习可以实现对标记有限数据的快速模型训练。一种方法是对在 ImageNet 上预训练的模型进行微调以适应目标医学图像分析任务，另一种方法是迁移来自不同领域的数据的训练。
- en: '*Pre-trained Model:* Transfer learning is often used to solve the problem of
    limited data labeled in medical image analysis, and some researchers found that
    using pre-trained networks on natural images such as ImageNet as an encoder within
    a U-Net-like network and then performing fine-tuning on medical data can further
    improve the segmentation effect of medical images. Kalinin  [[114](#bib.bib114)]
    et al. considered the VGG-11, VGG-16, and ResNet-34 networks pre-trained on ImageNet
    as encoders of the U-shaped network to perform semantic segmentation of robotic
    instruments from wireless capsule endoscopic videos of vascular proliferative
    lesions and surgical procedures. Similarly, Conze et al.  [[115](#bib.bib115)]
    used VGG-11 pre-trained on ImageNet as the encoder of a segmentation network to
    perform shoulder muscle MRI segmentation. Experiments demonstrate that the pre-trained
    network is useful for improving segmentation accuracy. It can be concluded that
    a pre-trained model on ImageNet can learn some common underlying features that
    are required for both medical and natural images, thus retraining process is unnecessary
    while performing fine-tuning is useful for training models. However, the domain
    adaptive may be a problem when applying pre-trained models of natural scene images
    to medical image analysis tasks. Besides, popular transfer learning methods are
    hardly applicable to 3D medical image analysis because pre-trained models often
    rely on 2D image datasets. If the number of medical datasets with annotations
    is large enough, it is possible that the effect of pre-training is weak for improving
    model performance. In fact, the effect of a pre-trained model is unstable and
    it depends on segmentation datasets and tasks. Empirically, we can try to use
    the pre-trained model if it can improve segmentation accuracy, otherwise we need
    to consider designing new models.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练模型：* 迁移学习通常用于解决医学图像分析中标记数据有限的问题，一些研究人员发现，使用在自然图像（如ImageNet）上预训练的网络作为U-Net-like网络中的编码器，然后在医学数据上进行微调，可以进一步改善医学图像的分割效果。Kalinin
    [[114](#bib.bib114)] 等人将在ImageNet上预训练的VGG-11、VGG-16和ResNet-34网络作为U形网络的编码器，用于对血管增生病变和手术操作的无线胶囊内窥镜视频中的机器人仪器进行语义分割。同样，Conze
    等人 [[115](#bib.bib115)] 使用在ImageNet上预训练的VGG-11作为分割网络的编码器，用于肩部肌肉MRI分割。实验表明，预训练网络对提高分割准确率是有用的。可以得出结论，在ImageNet上预训练的模型可以学习一些医学图像和自然图像都需要的常见基础特征，因此在进行微调时无需重新训练模型。然而，将自然场景图像的预训练模型应用于医学图像分析任务时，领域适应可能会成为一个问题。此外，流行的迁移学习方法在3D医学图像分析中很难应用，因为预训练模型通常依赖于2D图像数据集。如果标注的医学数据集数量足够多，预训练的效果可能对提高模型性能的作用较弱。事实上，预训练模型的效果是不稳定的，取决于分割数据集和任务。根据经验，如果预训练模型可以提高分割准确率，我们可以尝试使用它，否则需要考虑设计新模型。'
- en: '*Domain Adaptation:*If the labels from the training target domain are not available,
    and we can only access the labels in other domains, then popular methods are to
    transfer the trained classifier on the source domain to the target domain without
    labeled data. CycleGAN is a cycle structure, and mainly composed of two generators
    and two discriminators. Fig. 16 shows the architecture of CycleGAN. First, an
    image in the X domain is transferred to the Y domain by a generator G, and then
    the output from the G is reconstructed back to the original image in the X domain
    by the generator F. On the contrary, the image in the Y domain is transferred
    to the X domain by the generator F, and then the output from the F is reconstructed
    back to the original image in the Y domain by the generator G. Both discriminator
    G and F play discriminating roles ensuring the style transfer of images. Huo et
    al.  [[116](#bib.bib116)] proposed a jointly optimized image synthesis and segmentation
    framework for the task of spleen segmentation in CT images using CycleGAN  [[112](#bib.bib112)].
    The framework achieves an image conversion from the marked source domain to the
    synthesized target domain. During training, synthesized target images are used
    to train the segmentation network. During the test process, a real image from
    the target domain is directly input into the trained segmentation network to obtain
    desired segmentation results. Chen et al.  [[117](#bib.bib117)] also adopted a
    similar method using segmentation labels of MR images to achieve the task of cardiac
    CT segmentation.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*领域适应：* 如果训练目标领域的标签不可用，而我们只能访问其他领域的标签，那么常用的方法是将训练好的源领域分类器转移到没有标签数据的目标领域。CycleGAN
    是一个循环结构，主要由两个生成器和两个判别器组成。图 16 显示了 CycleGAN 的架构。首先，通过生成器 G 将 X 域中的图像转换到 Y 域，然后
    G 的输出通过生成器 F 重建回 X 域中的原始图像。相反，通过生成器 F 将 Y 域中的图像转换到 X 域，然后 F 的输出通过生成器 G 重建回 Y 域中的原始图像。生成器
    G 和 F 都发挥了判别作用，确保图像的风格转换。霍等人 [[116](#bib.bib116)] 提出了一个联合优化的图像合成和分割框架，用于使用 CycleGAN
    进行 CT 图像中的脾脏分割任务 [[112](#bib.bib112)]。该框架实现了从标记的源领域到合成目标领域的图像转换。在训练过程中，使用合成的目标图像来训练分割网络。在测试过程中，将目标领域的真实图像直接输入到训练好的分割网络中，以获得所需的分割结果。陈等人
    [[117](#bib.bib117)] 也采用了类似的方法，使用 MR 图像的分割标签来实现心脏 CT 图像的分割任务。'
- en: '![Refer to caption](img/b45c6fe7f34c5d30fda34f3dd492869e.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/b45c6fe7f34c5d30fda34f3dd492869e.png)'
- en: 'Figure 16: The Cycle GAN architecture  [[112](#bib.bib112)].'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：Cycle GAN 架构 [[112](#bib.bib112)]。
- en: Chartsias et al.  [[118](#bib.bib118)] used the CycleGAN to generate corresponding
    MR images and labels from CT slices and myocardial segmentation labels, and then
    used synthetic MR and real MR images to train the myocardial segmentation model.
    This model obtains 15% improvement over the myocardial segmentation model trained
    on real MR images. Similarly, there are some other works that realize the image
    conversion between different domains through the CycleGAN and improve the performance
    of medical image segmentation  [[119](#bib.bib119)] [[120](#bib.bib120)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Chartsias 等人 [[118](#bib.bib118)] 使用 CycleGAN 从 CT 切片和心肌分割标签生成相应的 MR 图像及标签，然后使用合成
    MR 图像和真实 MR 图像来训练心肌分割模型。该模型在与真实 MR 图像训练的心肌分割模型相比，获得了 15% 的改进。同样，还有一些其他工作通过 CycleGAN
    实现了不同领域之间的图像转换，并改善了医学图像分割的性能 [[119](#bib.bib119)] [[120](#bib.bib120)]。
- en: III-C Interactive Segmentation
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 互动分割
- en: Manually drawing medical image segmentation labels is usually tedious and time-consuming,
    especially for the drawing of 3D volume data. Interactive segmentation allows
    clinicians to correct interactively the initially segmented image generated by
    a model to obtain more accurate segmentation. The key to effective interactive
    segmentations is that clinicians can use interactive methods such as mouse clicks
    and outline boxes to improve an initial segmentation result from a model. Then
    the model can update parameters and generate new segmentation images to obtain
    new feedback from the clinicians.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 手动绘制医学图像分割标签通常是繁琐且耗时的，尤其是对 3D 体积数据的绘制。互动分割允许临床医生交互地修正模型生成的初始分割图像，以获得更准确的分割。有效的互动分割的关键在于临床医生可以使用鼠标点击和轮廓框等互动方法来改进模型的初始分割结果。然后，模型可以更新参数并生成新的分割图像，以获得来自临床医生的新反馈。
- en: Wang et al.  [[121](#bib.bib121)] proposed the DeepIGeoS using the cascade of
    two CNNs for interactive segmentation of 2D and 3D medical images. The first CNN
    called P-Net outputs a coarse segmentation result. Based on this, users provide
    interactive points or short lines to mark wrong segmentation areas, and then use
    them as the input of the second CNN called R-Net to obtain corrected results.
    Experiments were conducted on two dimensional foetal MRI images and three-dimensional
    brain tumor images, and experimental results showed that compared with traditional
    interactive segmentation methods such as GraphCuts, RandomWalks and ITK-Snap,
    the DeepIGeoS greatly reduces the requirement for user interaction and reduces
    user time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[121](#bib.bib121)] 提出了 DeepIGeoS，使用两个 CNN 的级联来进行 2D 和 3D 医学图像的交互式分割。第一个
    CNN 称为 P-Net，输出粗略的分割结果。在此基础上，用户提供交互点或短线来标记错误的分割区域，然后将这些作为第二个 CNN（称为 R-Net）的输入，以获得修正的结果。实验在二维胎儿
    MRI 图像和三维脑肿瘤图像上进行，实验结果表明，与传统的交互式分割方法如 GraphCuts、RandomWalks 和 ITK-Snap 相比，DeepIGeoS
    大大减少了用户交互的需求并缩短了用户时间。
- en: Wang et al.  [[122](#bib.bib122)] proposed the BIFSeg that is similar to the
    principle of GrabCut  [[123](#bib.bib123)] [[124](#bib.bib124)]. Users first draw
    a bounding box, and the area inside the bounding box is considered as the input
    of CNN, then an initial result is obtained. After that, users perform an image-specific
    fine-tuning to make CNN provide better segmentation results. The GrabCut achieves
    image segmentation by learning a Gaussian mixture model (GMM) from images, while
    the BIFSeg learns a CNN from images. Usually CNN-based segmentation methods can
    only deal with objects that have appeared in the training set, which limits the
    flexibility of these methods, but the BIFSeg attempts to use a CNN to segment
    objects that have not been seen during training process. The process is equivalent
    to making the BIFSeg learn to extract the foreground part of the object from a
    bounding box. During the test, the CNN can better use the information in the specific
    image through an adaptive fine-tuning.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[122](#bib.bib122)] 提出了 BIFSeg，其原理类似于 GrabCut [[123](#bib.bib123)]
    [[124](#bib.bib124)]。用户首先绘制一个边界框，边界框内的区域被视为 CNN 的输入，然后得到初步结果。之后，用户进行图像特定的微调，以使
    CNN 提供更好的分割结果。GrabCut 通过从图像中学习高斯混合模型（GMM）来实现图像分割，而 BIFSeg 则从图像中学习 CNN。通常基于 CNN
    的分割方法只能处理训练集中出现的对象，这限制了这些方法的灵活性，但 BIFSeg 尝试使用 CNN 来分割训练过程中未见过的对象。这个过程相当于使 BIFSeg
    学会从边界框中提取对象的前景部分。在测试过程中，CNN 可以通过自适应微调更好地利用特定图像中的信息。
- en: Rupprecht et al.  [[125](#bib.bib125)] proposed a new interactive segmentation
    method named GM interacting that updates image segmentation results according
    to the input text from users. This method changes the output of the network by
    modifying the feature maps between an encoder and a decoder interactively. The
    category of areas is first set according to the response of users, then some guiding
    parameters including multiplication and offset coefficients are updated through
    back propagation, the feature map is finally changed resulting in updated segmentation
    results.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Rupprecht 等人 [[125](#bib.bib125)] 提出了一个新的交互式分割方法，称为 GM interacting，该方法根据用户输入的文本更新图像分割结果。该方法通过交互性地修改编码器和解码器之间的特征图来改变网络的输出。首先根据用户的反馈设置区域的类别，然后通过反向传播更新一些指导参数，包括乘法和偏移系数，最终改变特征图，从而得到更新的分割结果。
- en: The interactive image segmentation based on deep learning can reduce the number
    of user interactions and the user time, which shows broader application prospects.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的交互式图像分割可以减少用户交互次数和用户时间，这显示了更广泛的应用前景。
- en: III-D Others Works
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 其他工作
- en: Semi-supervised learning can use a small part of labeled data and any number
    of unlabeled data to train a model, and its loss function often consists of the
    sum of two loss functions. The first is a supervised loss function that is only
    related with labeled data. The second is an unsupervised loss function or regularization
    term that is related to both labeled and unlabeled data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习可以利用少量的标记数据和任意数量的未标记数据来训练模型，其损失函数通常由两个损失函数的总和组成。第一个是仅与标记数据相关的监督损失函数。第二个是与标记数据和未标记数据都相关的无监督损失函数或正则化项。
- en: Based on the idea of GAN, Zhang et al.  [[126](#bib.bib126)] proposed a semi-supervised
    learning framework based on the adversarial way between segmentation network and
    evaluation network. An image is fed into U-Net to generate a segmentation map,
    which is then stacked with the original image and presented to the evaluation
    network to obtain a segmentation score. During the training process, the segmentation
    network is optimized in two aspects, one is to minimize the segmentation loss
    of labeled images and the other is to make the evaluation network obtain high
    scores for unlabeled images. Besides, the evaluation network is updated to assign
    low scores to unmarked images but high scores to marked images. Due to this adversarial
    learning, the segmentation network obtains supervised signals from both labeled
    and unlabeled images. Thus, the semi-supervised learning framework achieves better
    segmentation effect in the gland segmentation task for histopathology images.
    Similarly, some other semi-supervised frameworks  [[127](#bib.bib127)] [[128](#bib.bib128)] [[99](#bib.bib99)] [[129](#bib.bib129)]
    are also proposed to optimize medical image segmentation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAN 的思想，张等人 [[126](#bib.bib126)] 提出了一个基于分割网络和评估网络对抗方式的半监督学习框架。图像被输入到 U-Net
    中生成分割图，然后与原始图像叠加，并呈现给评估网络以获得分割评分。在训练过程中，分割网络从两个方面进行优化，一是最小化标记图像的分割损失，另一个是让评估网络对未标记图像获得高分。此外，评估网络会被更新，以便给未标记图像分配低分，而给标记图像分配高分。由于这种对抗学习，分割网络从标记和未标记图像中获得监督信号。因此，半监督学习框架在组织切片图像的分割任务中取得了更好的分割效果。类似地，一些其他半监督框架
    [[127](#bib.bib127)] [[128](#bib.bib128)] [[99](#bib.bib99)] [[129](#bib.bib129)]
    也被提出用于优化医学图像分割。
- en: Accurate and robust segmentation of organs or lesions from medical images plays
    a vital role in many clinical applications, such as diagnosis and treatment planning.
    However, it is difficult for medical images to acquire the annotated data, as
    generating accurate annotations requires expertise and time. Weakly supervised
    segmentation methods learn image segmentation from border or image-level labels
    or from a small amount of annotated image data, rather than using a large number
    of pixel-level annotations, to obtain high-quality segmentation results. In fact,
    a small amount of annotated data and a large amount of unannotated data are more
    compatible with the real clinical situation. However, in practice, the performance
    of weakly supervised learning only provides rarely acceptable results for medical
    image segmentation tasks, especially for 3D medical images. Therefore, this is
    a direction worth exploring in the future.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从医学图像中准确且鲁棒地分割器官或病变在许多临床应用中发挥着至关重要的作用，如诊断和治疗规划。然而，医学图像很难获取标注数据，因为生成准确的标注需要专业知识和时间。弱监督分割方法从边界或图像级标签或少量标注图像数据中学习图像分割，而不是使用大量的像素级标注，以获得高质量的分割结果。实际上，少量标注数据和大量未标注数据更符合真实临床情况。然而，在实践中，弱监督学习的性能仅提供了在医学图像分割任务中很少可接受的结果，尤其是对于
    3D 医学图像。因此，这是一个未来值得探索的方向。
- en: IV Currently popular direction
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 当前热门方向
- en: IV-A Network Architecture Search
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 网络架构搜索
- en: 'Recently, the performance of convolutional neural network models has been continuously
    improved. Researchers have designed a large number of popular network architectures
    for specific tasks such as image classification, segmentation, reconstruction,
    etc. These architectures are often designed by industry experts or academics for
    months or even years, since the design of network architectures with excellent
    performance usually requires a great deal of domain knowledge. Therefore, the
    design process is time consuming and laborious for researchers without domain
    knowledge. So far, NAS  [[130](#bib.bib130)]has made significant progress in improving
    the accuracy of image classification. The NAS can be deemed to a subdomain of
    automatic machine learning  [[131](#bib.bib131)](AutoML) and has a strong overlap
    with hyperparametric optimization  [[132](#bib.bib132)]and meta learning  [[133](#bib.bib133)].
    Current research on NAS focuses on three aspects: search space, search strategy
    and performance estimation. The search space is a candidate collection of network
    structures to be searched. The search space is divided into a global search space
    that represents the search for the entire network structure, and a cell-based
    search space that searches only a few small structures that are assembled into
    a complete large network by the ways of stacking and stitching. The search strategy
    aims to find the optimal network structure as fast as possible in search spaces.
    Popular search strategies are often grouped into three categories, reinforcement-based
    learning, evolutionary algorithms, and gradients. Performance estimation strategy
    is the process of assessing how well the network structure performs on target
    datasets. For NAS techniques, researcher pay more attention to the improvement
    of search strategies since search space and performance estimation methods are
    usually rarely changed. Some improved CNN model based on NAS  [[134](#bib.bib134)] [[135](#bib.bib135)]have
    been proposed and applied to image segmentation.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，卷积神经网络模型的性能不断提升。研究人员为图像分类、分割、重建等特定任务设计了大量流行的网络架构。这些架构通常由行业专家或学术界人士设计，可能需要几个月甚至几年的时间，因为设计具有优秀性能的网络架构通常需要大量领域知识。因此，对于没有领域知识的研究人员而言，设计过程既耗时又费力。迄今为止，NAS
    [[130](#bib.bib130)] 在提高图像分类准确性方面取得了显著进展。NAS 可以被视为自动机器学习 [[131](#bib.bib131)]
    (AutoML) 的一个子领域，并与超参数优化 [[132](#bib.bib132)] 和元学习 [[133](#bib.bib133)] 有很大的重叠。目前关于
    NAS 的研究集中在三个方面：搜索空间、搜索策略和性能评估。搜索空间是待搜索的网络结构候选集合。搜索空间分为代表整个网络结构搜索的全局搜索空间，以及仅搜索少数小结构的基于单元的搜索空间，这些小结构通过堆叠和拼接的方式组成一个完整的大网络。搜索策略旨在尽可能快速地在搜索空间中找到最佳网络结构。常见的搜索策略通常分为三类：基于强化学习、进化算法和梯度方法。性能评估策略是评估网络结构在目标数据集上表现如何的过程。对于
    NAS 技术，研究人员更多地关注搜索策略的改进，因为搜索空间和性能评估方法通常变化较少。基于 NAS [[134](#bib.bib134)] [[135](#bib.bib135)]
    改进的 CNN 模型已被提出并应用于图像分割。
- en: Most current studies on deep learning in medical image segmentation depend on
    U-Net networks and makes some changes to the network structure according to different
    tasks, but in reality the non-network structure factors may be also important
    for improving segmentation effect. Isensee et al. [[136](#bib.bib136)] argued
    that too much manual adjustment on network structure could lead to over-fitting
    for a given dataset, and therefore proposed a medical image segmentation framework
    no-new-UNet (nnU-Net) that adapts itself to any new dataset. The nnUnet automatically
    adjusts all hyperparameters according to the properties of the given dataset without
    manual intervention. Therefore, the nnU-Net only relies on vanilla 2D UNet, 3D
    UNet, UNet cascade and a robust training scheme. It focuses on the stage of pre-processing
    (resampling and normalization), training (loss, optimizer settings, data augmentation),
    inference (patch-based strategies, test-time-augmentations integration, model
    integration, etc.), and post-processing (e.g., enhanced single pass domain). In
    practical applications, the improvements of network structure design usually depend
    on experiences without adequate interpretability theory support, Moreover, more
    complex network models indicate higher risk of over-fitting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当前关于医学图像分割的深度学习研究大多依赖于U-Net网络，并根据不同任务对网络结构进行一些修改，但实际上，非网络结构因素也可能对提高分割效果非常重要。Isensee等人[[136](#bib.bib136)]认为，过多的网络结构手动调整可能导致对特定数据集的过拟合，因此提出了一种能够自适应任何新数据集的医学图像分割框架no-new-UNet
    (nnU-Net)。nnU-Net根据给定数据集的特性自动调整所有超参数，无需手动干预。因此，nnU-Net仅依赖于基础的2D UNet、3D UNet、UNet级联以及一个稳健的训练方案。它专注于预处理阶段（重采样和标准化）、训练阶段（损失、优化器设置、数据增强）、推断阶段（基于补丁的策略、测试时增强集成、模型集成等）以及后处理阶段（例如，增强的单次域）。在实际应用中，网络结构设计的改进通常依赖于经验，而缺乏充分的可解释性理论支持。此外，更复杂的网络模型表示更高的过拟合风险。
- en: Weng et al  [[137](#bib.bib137)]first proposed a NAS-UNet for medical image
    segmentation. The NAS-UNet contains the same two cell architectures DownSC and
    UpSC. The difference between them is that the former performs a search on the
    U-shaped backbone to obtain DownSC and UpSC blocks. The NAS-UNet outperforms the
    U-Net and its variants, and its training time is close to that of U-Net, but with
    only 6% of the number of parameters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Weng等人[[137](#bib.bib137)]首次提出了一种用于医学图像分割的NAS-UNet。NAS-UNet包含相同的两个单元结构DownSC和UpSC。它们之间的区别在于，前者在U形骨干网络上执行搜索，以获得DownSC和UpSC块。NAS-UNet优于U-Net及其变体，其训练时间接近U-Net，但参数数量仅为U-Net的6%。
- en: To perform image segmentation in real time for high-resolution 2D images (e.g.
    CT, MRI and histopathology images), the study of compressed neural network models
    has become a popular direction in medical image segmentation. The application
    of NAS can effectively reduce the number of model parameters and achieves high
    segmentation performance. Although the performance of NAS is stunning,the fact
    of why particular architectures perform well can not be explained. Therefore,
    it is also important for future research to better understand the mechanisms which
    have a significant impact on performance and to explore whether these properties
    can be generalized to different tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为高分辨率2D图像（例如CT、MRI和组织病理图像）进行实时图像分割的研究中，压缩神经网络模型已经成为医学图像分割的热门方向。NAS的应用可以有效减少模型参数数量并实现高分割性能。尽管NAS的性能令人惊叹，但为什么某些特定架构表现良好的事实无法解释。因此，未来的研究也应着重于更好地理解对性能有重大影响的机制，并探索这些特性是否可以推广到不同的任务。
- en: IV-B Graph Convolutional Neural Network
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 图卷积神经网络
- en: The GCN  [[138](#bib.bib138)]is one of the powerful tools for the study of non-Euclidean
    domains. A graph is a data structure consisting of nodes and edges. The early
    graph neural networks (GNNs)  [[139](#bib.bib139)] mainly address strictly graphical
    problems such as the classification of molecular structures. In practice, the
    Euclidean spaces (e.g., images) or sequences (e.g., text), and many common scenes
    can be converted into graphs that can be modeled by using GCN techniques.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GCN  [[138](#bib.bib138)]是研究非欧几里得领域的强大工具之一。图是一种由节点和边组成的数据结构。早期的图神经网络（GNNs）[[139](#bib.bib139)]主要解决如分子结构分类等严格的图形问题。在实际应用中，欧几里得空间（例如，图像）或序列（例如，文本），以及许多常见场景可以被转换成图，通过使用GCN技术进行建模。
- en: Gao et al.  [[140](#bib.bib140)] designed a new graph pooling (gPool) and graph
    unpooling (gUnpool) operation based on GCN and proposed an encoder-decoder model
    namely graph U-Net. The graph U-Net achieves better performance than popular U-Nets
    by adding a small number of parameters. In contrast to traditional convolutional
    neural networks where deeper is better, the performance of the graph U-Net cannot
    be improved by increasing the depth of networks when the value of depth exceeds
    4\. However, the graph U-Net show stronger capability of feature encoding than
    popular U-Nets when the value of depth is smaller or equivalent to 4\. Yang et
    al.  [[141](#bib.bib141)] proposed the end-to-end conditional partial residual
    plot convolutional network CPR-GCN for automatic anatomical marking of coronary
    arteries. Authors showed that the GCN-based approach provided better performance
    and stronger robustness than traditional and recent depth learning based approaches.
    Results from these GCNs in medical image segmentations are promising, as the graph
    structure has high data representation efficiency and strong capability of feature
    encoding.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 高等学者们[[140](#bib.bib140)]设计了一种基于GCN的新图池化（gPool）和图反池化（gUnpool）操作，并提出了一种名为图U-Net的编码器-解码器模型。图U-Net通过增加少量参数，达到了比流行的U-Net更好的性能。与传统卷积神经网络中深度越深效果越好不同，当网络深度超过4时，图U-Net的性能不能通过增加网络深度来提高。然而，当深度值小于或等于4时，图U-Net显示出比流行U-Net更强的特征编码能力。杨等人[[141](#bib.bib141)]提出了用于冠状动脉自动解剖标记的端到端条件部分残差图卷积网络CPR-GCN。作者们展示了基于GCN的方法提供了比传统和近期深度学习方法更好的性能和更强的鲁棒性。这些GCN在医学图像分割中的结果是有希望的，因为图结构具有高数据表示效率和强特征编码能力。
- en: IV-C Interpretable Shape Attentive Neural Network
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 可解释的形状注意神经网络
- en: Currently, many deep learning algorithms tend to make judgments by using ”memorized”
    models that approximately fit to input data. As a result, these algorithms cannot
    be explained sufficiently and give convincing evidences for each specific prediction.
    Therefore, the study of the interpretability of deep neural networks is a hot
    topic at present.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，许多深度学习算法倾向于使用“记忆化”的模型来进行判断，这些模型大致适应输入数据。因此，这些算法不能充分解释，并为每个特定预测提供令人信服的证据。因此，深度神经网络的可解释性研究目前是一个热点话题。
- en: Sun et al.  [[142](#bib.bib142)] proposed the SAU-Net that focuses on the interpretability
    and the robustness of models. The proposed architecture attempts to address the
    problem of poor edge segmentation accuracy in medical images by using a secondary
    shape stream. Specially, the shape stream and the regular texture stream can capture
    rich shape-dependent information in parallel. Furthermore, both spatial and channel
    attention mechanism are used for the decoder to explain the learning capability
    of models at each resolution of U-Net. Finally, by extracting the learned shape
    and spatial attention maps, we can interpret the highly activated regions of each
    decoder block. The learned shape maps can be used to infer correct shapes of interesting
    categories learned by the model. The SAU-Net is able to learn robust shape features
    of objects via the gated shape stream, and is also more interpretable than previous
    works via built-in saliency maps using attention.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 孙等人[[142](#bib.bib142)]提出了SAU-Net，专注于模型的可解释性和鲁棒性。提出的架构试图通过使用辅助形状流解决医学图像中边缘分割精度差的问题。特别地，形状流和常规纹理流可以并行捕捉丰富的形状依赖信息。此外，解码器使用了空间和通道注意机制，以解释U-Net每个分辨率下模型的学习能力。最后，通过提取学习到的形状和空间注意图，我们可以解释每个解码器块的高激活区域。学习到的形状图可以用来推断模型学习到的有趣类别的正确形状。SAU-Net能够通过门控形状流学习物体的鲁棒形状特征，并且通过使用注意力的内置显著性图比以往的工作更具可解释性。
- en: Wickstrøm et al.  [[143](#bib.bib143)] explored the uncertainty and interpretability
    of semantic segmentation of colorectal polyps in convolutional neural networks,
    and the authors developed the central idea of guided back propagation  [[144](#bib.bib144)]
    for the interpretation of network gradients. By using back propagation, the gradient
    corresponding to each pixel in the input is obtained so that the features considered
    by the network can be visualized. In the process of back propagation, pixels with
    large and positive gradient values in an image should be paid more attention due
    to high importance while pixels with large and negative gradient values should
    be suppressed. If these negative gradients are included in the visualization of
    important pixels, they may result in noisy visualizations of descriptive features.
    To avoid creating noisy visualizations, the guide back propagation process changes
    the back propagation of the neural network so that the negative gradients are
    set to zero at each layer, thereby allowing only positive gradients to flow backwards
    through the network and highlight these pixels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Wickstrøm等人[[143](#bib.bib143)]探讨了卷积神经网络中结直肠息肉的语义分割的不确定性和可解释性，作者开发了引导反向传播的核心思想[[144](#bib.bib144)]来解释网络梯度。通过反向传播，可以获得输入中每个像素对应的梯度，从而可视化网络关注的特征。在反向传播过程中，图像中梯度值大且为正的像素应受到更多关注，因为这些像素很重要，而梯度值大且为负的像素应被抑制。如果将这些负梯度包含在重要像素的可视化中，可能会导致描述性特征的噪声可视化。为避免产生噪声可视化，引导反向传播过程通过在每一层将负梯度设为零，改变神经网络的反向传播方式，从而只允许正梯度在网络中反向传播，并突出这些像素。
- en: Medical image analysis is an aid to the clinical diagnosis, the clinicians wonder
    not only where the lesion is located at, but also the interpretability of results
    given by networks. Currently, the interpretation of medical image analysis is
    dominated by visualization methods such as attention and the class-activation-map
    (CAM). Therefore, the research on the interpretability of deep learning for medical
    image segmentation  [[145](#bib.bib145)] [[146](#bib.bib146)] [[147](#bib.bib147)] [[148](#bib.bib148)]
    will be a popular direction in future.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分析是临床诊断的辅助工具，临床医生不仅关心病变的具体位置，还关心网络给出的结果的可解释性。目前，医学图像分析的解释主要依赖于可视化方法，如注意力机制和类别激活图（CAM）。因此，未来深度学习在医学图像分割中的可解释性研究[[145](#bib.bib145)]
    [[146](#bib.bib146)] [[147](#bib.bib147)] [[148](#bib.bib148)]将成为一个热门方向。
- en: IV-D Multi-modality Data Fusion
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 多模态数据融合
- en: Multi-modality data fusion has been widely used in medical image analysis because
    it can provide richer object features that are helpful for improving object detection
    and segmentation results. Dou et al.  [[149](#bib.bib149)] proposed a novel multi-modal
    learning scheme for accurate segmentation of anatomical structures from unpaired
    CT and MRI images, and designed a new loss function using knowledge distillation
    to improve model training efficiency  [[150](#bib.bib150)]. More specifically,
    the normalization layer used for different modalities (i.e., CT and MRI) is implemented
    within separate variables, whereas the convolutional layer is constructed within
    shared variables. In each training iteration, samples for each modality are loaded
    separately and then forwarded to the shared convolutional and independent normalization
    layers, and finally the logarithms that can be used to calculate knowledge distillation
    losses will be obtained. Moeskops et al.  [[151](#bib.bib151)] investigated a
    question whether it is possible to train a single convolutional neural network
    (CNN) to perform same segmentation tasks on different-modality data. It is well
    known that CNNs show excellent performance for image feature encoding and based
    on this, the experiments in  [[151](#bib.bib151)] furthermore demonstrate that
    CNNs are also excellent for feature encoding of multi-modality data when they
    are used for the same tasks. Therefore, a single system can be used in clinical
    practice to automatically execute segmentation tasks on various modality data
    without extra task-specific training.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据融合在医学图像分析中被广泛应用，因为它可以提供更丰富的对象特征，有助于改善对象检测和分割结果。Dou等人[[149](#bib.bib149)]提出了一种新颖的多模态学习方案，用于从未配对的CT和MRI图像中准确分割解剖结构，并设计了一种新的损失函数，利用知识蒸馏来提高模型训练效率[[150](#bib.bib150)]。更具体地说，针对不同模态（即CT和MRI）的归一化层在独立的变量中实现，而卷积层则在共享变量中构建。在每次训练迭代中，为每种模态加载的样本分别传递到共享卷积层和独立归一化层，最后获得可以用于计算知识蒸馏损失的对数。Moeskops等人[[151](#bib.bib151)]探讨了是否可以训练一个单一的卷积神经网络（CNN）在不同模态数据上执行相同的分割任务。众所周知，CNN在图像特征编码方面表现优异，基于此[[151](#bib.bib151)]的实验进一步证明，当CNN用于相同任务时，它们在多模态数据的特征编码方面也表现出色。因此，在临床实践中可以使用单一系统自动执行各种模态数据上的分割任务，而无需额外的任务特定训练。
- en: 'More relevant literatures can be found in the review on multi-modal fusion
    for medical image segmentation using deep learning  [[152](#bib.bib152)]. In this
    review, authors classified fusion strategies into three categories: input-level
    fusion, layer-level fusion, and decision-level fusion. Although it is known that
    multi-modal fusion networks usually show better performance for segmentation tasks
    than unimodal networks, multi-model fusion causes some new problems such as how
    to design multi-modal networks to efficiently combine different modalities, how
    to exploit potential relationships between different modalities, how to integrate
    multiple information into segmentation networks to improve segmentation performance,
    etc. In addition, the integration of multi-modal data fusion into an effective
    single-parameter network can help simplify deployment and improve the usability
    of models in clinical practice.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 更多相关文献可以在关于使用深度学习进行医学图像分割的多模态融合综述中找到[[152](#bib.bib152)]。在这篇综述中，作者将融合策略分为三类：输入级融合、层级融合和决策级融合。虽然已知多模态融合网络在分割任务中通常比单模态网络表现更好，但多模态融合也带来了一些新问题，例如如何设计多模态网络以高效结合不同模态、如何利用不同模态之间的潜在关系、如何将多种信息整合到分割网络中以提高分割性能等。此外，将多模态数据融合整合到一个有效的单参数网络中可以帮助简化部署，并提高模型在临床实践中的可用性。
- en: V Discussion and outlook
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 讨论与展望
- en: V-A Medical Image Segmentation Datasets
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 医学图像分割数据集
- en: 'In order to help clinicians make accurate diagnoses, it is necessary to segment
    important organs, tissues or lesions from medical images with the aid of a computer
    and extract features from segmented objects. As a result, various medical image
    datasets and corresponding competitions have been launched to promote the development
    of computer-aided diagnosis techniques. In recent years, there has been a growing
    interest in developing more comprehensive computational anatomical models with
    the development of deep learning techniques, which has facilitated the development
    of multi-organ analysis models. The multi-organ segmentation approaches are different
    from traditional organ-specific strategies in that they incorporate relationships
    between different organs into models to represent more accurately the complex
    human anatomy. In the context of multiorgan analysis, brain and abdomen are the
    most popular in medical image analysis. Thus there are many datasets on the brain
    and abdomen such as BRATS  [[3](#bib.bib3)] [[153](#bib.bib153)] [[154](#bib.bib154)],
    ISLES  [[155](#bib.bib155)], KITS  [[156](#bib.bib156)], LITS  [[157](#bib.bib157)],
    CHAOS  [[158](#bib.bib158)], etc. There are two reasons for the emergence of large
    datasets: on the one hand, the rapid development of imaging techniques, increasingly
    higher resolution shows more detailed anatomical tissue, which provides a better
    reference for clinicians; on the other hand, with the development of deep learning
    techniques, a large number of training samples are necessary, so many research
    teams have collected many samples and annotated data to form datasets in order
    to train network models easily. In addition, stable organ structures in the abdomen
    (e.g., the liver, spleen, and kidneys) can provide constraints and contextual
    information for creating computational anatomical models of the abdomen. There
    are also a small number of public datasets on hippocampus and pelvic organs (e.g.,
    Colon  [[159](#bib.bib159)], and prostate  [[160](#bib.bib160)]). Indeed, the
    construction of more holistic and global anatomical models remains one of the
    greatest challenges and opportunities in future due to the lack of large datasets
    to characterize the complexity of the human anatomy. More discussions on multi-organ
    analysis and computational anatomical methods can be found in  [[161](#bib.bib161)].
    The review proposed by Cerrolaza et al.  [[161](#bib.bib161)] follows a methodology-based
    classification of different techniques that are available for the analysis of
    multi-organs and multi-anatomical structures, from techniques using point distribution
    models to the latest deep learning-based approaches.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助临床医生做出准确的诊断，有必要借助计算机对医学图像中的重要器官、组织或病变进行分割，并从分割后的对象中提取特征。因此，各种医学图像数据集和相关竞赛应运而生，以推动计算机辅助诊断技术的发展。近年来，随着深度学习技术的发展，对开发更全面的计算解剖模型的兴趣不断增长，这促进了多脏器分析模型的发展。与传统的器官特定策略不同，多脏器分割方法将不同器官之间的关系纳入模型中，以更准确地表示复杂的人体解剖结构。在多脏器分析的背景下，大脑和腹部是医学图像分析中最受欢迎的领域。因此，存在许多关于大脑和腹部的数据集，如
    BRATS [[3](#bib.bib3)] [[153](#bib.bib153)] [[154](#bib.bib154)]、ISLES [[155](#bib.bib155)]、KITS
    [[156](#bib.bib156)]、LITS [[157](#bib.bib157)]、CHAOS [[158](#bib.bib158)] 等。大型数据集的出现有两个原因：一方面，成像技术的快速发展使得分辨率越来越高，展示了更详细的解剖组织，这为临床医生提供了更好的参考；另一方面，随着深度学习技术的发展，大量的训练样本是必需的，因此许多研究团队收集了大量样本并进行了标注，以形成数据集，便于训练网络模型。此外，腹部中的稳定器官结构（如肝脏、脾脏和肾脏）可以为创建腹部计算解剖模型提供约束和背景信息。关于海马体和盆腔器官（如结肠
    [[159](#bib.bib159)] 和前列腺 [[160](#bib.bib160)]）也有少量公共数据集。实际上，由于缺乏大规模的数据集来描述人体解剖的复杂性，构建更全面和全球性的解剖模型仍然是未来面临的重大挑战和机遇。关于多脏器分析和计算解剖方法的更多讨论可以参考
    [[161](#bib.bib161)]。Cerrolaza 等人提出的综述 [[161](#bib.bib161)] 根据不同技术的分类方法，对多脏器和多解剖结构分析中可用的技术进行了分类，从使用点分布模型的技术到最新的基于深度学习的方法。
- en: '![Refer to caption](img/25eafabddd16bd1f9fb3ec632041d5af.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25eafabddd16bd1f9fb3ec632041d5af.png)'
- en: 'Figure 17: Some images of benchmark datasets.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: 一些基准数据集的图像。'
- en: 'TABLE I: Public datasets for medical segmentation.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 医学分割的公共数据集。'
- en: '| Objects | Dataset | URL |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 对象 | 数据集 | URL |'
- en: '| --- | --- | --- |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Liver | LiTS  [[157](#bib.bib157)] | https://competitions.codalab.org/competitions/17094
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 肝脏 | LiTS [[157](#bib.bib157)] | https://competitions.codalab.org/competitions/17094
    |'
- en: '| Sliver07  [[162](#bib.bib162)] | http://www.sliver07.org/ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Sliver07 [[162](#bib.bib162)] | http://www.sliver07.org/ |'
- en: '| 3Dircadb  [[163](#bib.bib163)] | https://www.ircad.fr/research/3dircadb/
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 3Dircadb [[163](#bib.bib163)] | https://www.ircad.fr/research/3dircadb/ |'
- en: '| Medical Segmentation Decathlon (MSD)  [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '|  | CHAOS  [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | CHAOS [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
- en: '| Pancreas | Medical Segmentation Decathlon (MSD)  [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 胰腺 | 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| NIH Pancreas  [[166](#bib.bib166)] | http://academictorrents.com/details/80ecfefcabede760cdbdf63e38986501f7becd49
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| NIH 胰腺 [[166](#bib.bib166)] | http://academictorrents.com/details/80ecfefcabede760cdbdf63e38986501f7becd49
    |'
- en: '| Colon | COLONOGRAPHY  [[159](#bib.bib159)] | https://wiki.cancerimagingarchive.net/display/Public/CT+COLONOGRAPHY#dc149b9170f54aa29e88f1119e25ba3e
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 结肠 | 结肠造影 [[159](#bib.bib159)] | https://wiki.cancerimagingarchive.net/display/Public/CT+COLONOGRAPHY#dc149b9170f54aa29e88f1119e25ba3e
    |'
- en: '| Medical Segmentation Decathlon (MSD)  [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Heart | AMRG Cardiac Atlas  [[167](#bib.bib167)] | http://www.cardiacatlas.org/studies/amrg-cardiac-atlas/
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 心脏 | AMRG 心脏图谱 [[167](#bib.bib167)] | http://www.cardiacatlas.org/studies/amrg-cardiac-atlas/
    |'
- en: '| Medical Segmentation Decathlon (MSD)  [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Lung | LIDC-IDRI  [[168](#bib.bib168)] | https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI#
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 肺部 | LIDC-IDRI [[168](#bib.bib168)] | https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI#
    |'
- en: '| VESSEL12  [[169](#bib.bib169)] | https://vessel12.grand-challenge.org/ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| VESSEL12 [[169](#bib.bib169)] | https://vessel12.grand-challenge.org/ |'
- en: '| Medical Segmentation Decathlon (MSD)  [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Prostate | PROMISE12  [[160](#bib.bib160)] | https://promise12.grand‐challenge.org/
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 前列腺 | PROMISE12 [[160](#bib.bib160)] | https://promise12.grand‐challenge.org/
    |'
- en: '| Medical Segmentation Decathlon (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Brain | OASIS  [[170](#bib.bib170)] | http://www.oasis-brains.org/ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 大脑 | OASIS [[170](#bib.bib170)] | http://www.oasis-brains.org/ |'
- en: '| BRATS  [[3](#bib.bib3)] [[153](#bib.bib153)] [[154](#bib.bib154)] | https://www.med.upenn.edu/sbia/brats2018/registration.html
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| BRATS [[3](#bib.bib3)] [[153](#bib.bib153)] [[154](#bib.bib154)] | https://www.med.upenn.edu/sbia/brats2018/registration.html
    |'
- en: '| ISLES  [[155](#bib.bib155)] | http://www.isles‐challenge.org/ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ISLES [[155](#bib.bib155)] | http://www.isles‐challenge.org/ |'
- en: '| mTOP  [[171](#bib.bib171)] | https://www.smir.ch/MTOP/Start2016 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| mTOP [[171](#bib.bib171)] | https://www.smir.ch/MTOP/Start2016 |'
- en: '|  | Medical Segmentation Decathlon (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Kidney | KITS  [[156](#bib.bib156)] | https://kits19.grand-challenge.org
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 肾脏 | KITS [[156](#bib.bib156)] | https://kits19.grand-challenge.org |'
- en: '| CHAOS  [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| CHAOS [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
- en: '| Spleen | Medical Segmentation Decathlon (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 脾脏 | 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| CHAOS  [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| CHAOS [[165](#bib.bib165)] | https://chaos.grand-challenge.org |'
- en: '| Hippocampus | Medical Segmentation Decathlon (MSD) [[164](#bib.bib164)] |
    http://medicaldecathlon.com/index.html |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 海马体 | 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Hepatic Vessel | Medical Segmentation Decathlon (MSD) [[164](#bib.bib164)]
    | http://medicaldecathlon.com/index.html |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 肝脏血管 | 医学分割十项全能 (MSD) [[164](#bib.bib164)] | http://medicaldecathlon.com/index.html
    |'
- en: '| Skin lesion | ISIC  [[172](#bib.bib172)] | https://challenge.isic-archive.com/data
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 皮肤病变 | ISIC [[172](#bib.bib172)] | https://challenge.isic-archive.com/data
    |'
- en: '| STARE | STARE [[173](#bib.bib173)] | https://cecas.clemson.edu/ ahoover/stare/
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| STARE | STARE [[173](#bib.bib173)] | https://cecas.clemson.edu/ ahoover/stare/
    |'
- en: '| Thyroid | TNSCUI [[174](#bib.bib174)] | https://tn-scui2020.grand-challenge.org/
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 甲状腺 | TNSCUI [[174](#bib.bib174)] | https://tn-scui2020.grand-challenge.org/
    |'
- en: There are many publicly available datasets for medical image segmentation, Table
    I provides a brief description and list of each dataset. As shown in Fig. 17,
    we also provide some images of benchmark datasets. In fact, there are more public
    datasets than in the list of Table I used for medical image segmentation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多公开可用的医学图像分割数据集，表I提供了每个数据集的简要描述和列表。如图17所示，我们还提供了一些基准数据集的图像。实际上，公开的医学图像分割数据集比表I中的列表更多。
- en: V-B Popular evaluation metrics
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 流行的评估指标
- en: In order to measure effectively the performance of medical image segmentation
    model, a large number of metrics have been proposed for evaluating the segmentation
    effectiveness. The evaluation of image segmentation performance relies on pixel
    quality, region quality and surface distance quality. In this section, we give
    some popular metrics for evaluating the performance of medical image segmentation.
    Pixel quality metrics include pixel accuracy (PA). Region quality metrics include
    Dice score, volume overlap error (VOE) and relative volume difference (RVD). Surface
    distance quality metrics include average symmetric surface distance (ASD) and
    maximum symmetric surface distance (MSD).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地衡量医学图像分割模型的性能，提出了大量的度量标准来评估分割效果。图像分割性能的评估依赖于像素质量、区域质量和表面距离质量。本节提供了一些用于评估医学图像分割性能的常见指标。像素质量指标包括像素准确率（PA）。区域质量指标包括Dice评分、体积重叠误差（VOE）和相对体积差异（RVD）。表面距离质量指标包括平均对称表面距离（ASD）和最大对称表面距离（MSD）。
- en: '*PA:* Pixel accuracy simply finds the ratio of pixels properly classified,
    divided by the total number of pixels. For $K+1$ classes ($K$ foreground classes
    and the background) pixel accuracy is defined as:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*PA:* 像素准确率简单地找到正确分类的像素与总像素数量的比例。对于 $K+1$ 个类别（$K$ 个前景类别和背景），像素准确率定义为：'
- en: '|  | $\displaystyle PA=\frac{\sum_{i=0}^{K}p_{ii}}{\sum_{i=0}^{K}\sum_{j=0}^{K}p_{ij}},$
    |  | (18) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle PA=\frac{\sum_{i=0}^{K}p_{ii}}{\sum_{i=0}^{K}\sum_{j=0}^{K}p_{ij}},$
    |  | (18) |'
- en: where $p_{ij}$ is the number of pixels of class $i$ predicted as belonging to
    class $j$.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{ij}$ 是类别 $i$ 预测为类别 $j$ 的像素数量。
- en: '*Dice score:* it is a popular metric for image segmentation (and is more commonly
    used in medical image analysis), which can be defined as twice the overlap area
    of predicted and ground-truth maps, divided by the total number of pixels in both
    images. The Dice score is defined as:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dice评分:* 是图像分割的流行指标（在医学图像分析中更常用），可以定义为预测图和真实图重叠区域的两倍，除以两个图像中像素的总数。Dice评分定义为：'
- en: '|  | $\displaystyle Dice=\frac{2\left&#124;A\cap B\right&#124;}{\left&#124;A\right&#124;+\left&#124;B\right&#124;},$
    |  | (19) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Dice=\frac{2\left\|A\cap B\right\|}{\left\|A\right\|+\left\|B\right\|},$
    |  | (19) |'
- en: where $A$ and $B$ denote the ground truth and the predicted segmentation maps,
    respectively.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 和 $B$ 分别表示真实值和预测分割图。
- en: '*VOE:* it is the complement of the Jaccard index, it is defined as:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*VOE:* 是Jaccard指数的补集，定义为：'
- en: '|  | $\displaystyle VOE\left(A,B\right)=1-\frac{\left&#124;A\cap B\right&#124;}{\left&#124;A\cup
    B\right&#124;}.$ |  | (20) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle VOE\left(A,B\right)=1-\frac{\left\|A\cap B\right\|}{\left\|A\cup
    B\right\|}.$ |  | (20) |'
- en: '*RVD:* it is an asymmetric measure defined as:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*RVD:* 是一种非对称度量，定义为：'
- en: '|  | $\displaystyle RVD\left(A,B\right)=\frac{\left&#124;B\right&#124;-\left&#124;A\right&#124;}{\left&#124;A\right&#124;}.$
    |  | (21) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle RVD\left(A,B\right)=\frac{\left\|B\right\|-\left\|A\right\|}{\left\|A\right\|}.$
    |  | (21) |'
- en: Surface distance metrics are a set of correlated measures of the distance between
    the surfaces of a reference and predicted lesion.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表面距离指标是一组相关的度量，用于测量参考和预测病变表面之间的距离。
- en: 'Let $S(A)$ denote the set of surface voxels of $A$. The shortest distance of
    an arbitrary voxel $v$ to $S(A)$ is defined as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $S(A)$ 表示 $A$ 的表面体素集合。任意体素 $v$ 到 $S(A)$ 的最短距离定义为：
- en: '|  | $\displaystyle d(v,S(A))=\mathop{min}\limits_{s_{A}\in S(A}(\left\&#124;v-s_{A}\right\&#124;),$
    |  | (22) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d(v,S(A))=\mathop{min}\limits_{s_{A}\in S(A}(\left\|\bullet-s_{A}\right\|),$
    |  | (22) |'
- en: where $\left\|\bullet\right\|$ denotes the Euclidean distance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left\|\bullet\right\|$ 表示欧几里得距离。
- en: '*ASD:* it is defined as:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*ASD:* 定义为：'
- en: '|  | $\displaystyle ASD(A,B)=\frac{1}{\left&#124;S(A)\right&#124;+\left&#124;S(B)\right&#124;}(\sum_{s_{A}\in
    S(A)}{d(s_{A},S(B))}$ |  | (23) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ASD(A,B)=\frac{1}{\left\|S(A)\right\|+\left\|S(B)\right\|}(\sum_{s_{A}\in
    S(A)}{d(s_{A},S(B))}$ |  | (23) |'
- en: '|  | $\displaystyle+\sum_{s_{B}\in S(B)}{d(s_{B},S(A))}).$ |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+\sum_{s_{B}\in S(B)}{d(s_{B},S(A))}).$ |  |'
- en: '*MSD:* it is also known as the Symmetric Hausdorff Distance, and is similar
    to $ASD$ except that the maximum distance that is taken instead of the average:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*MSD:* 它也被称为对称Hausdorff距离，类似于$ASD$，不同之处在于取最大距离而非平均距离：'
- en: '|  | $\displaystyle MSD(A,B)=max\{\mathop{max}\limits_{s_{A}\in S(A)}{d(s_{A},S(B))},$
    |  | (24) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MSD(A,B)=max\{\mathop{max}\limits_{s_{A}\in S(A)}{d(s_{A},S(B))},$
    |  | (24) |'
- en: '|  | $\displaystyle\mathop{max}\limits_{s_{B}\in S(B)}{d(s_{B},S(A))}\}.$ |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathop{max}\limits_{s_{B}\in S(B)}{d(s_{B},S(A))}\}.$ |  |'
- en: V-C Challenges and Future Scope
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 挑战与未来展望
- en: It has been proved that fully automated segmentation of medical images based
    on deep neural networks is very valuable. By reviewing the progress of deep learning
    in medical image segmentation, we have identified potential difficulties. Researchers
    successfully employed a variety of means to improve the accuracy of medical image
    segmentation. Whereas, only the improvement of accuracy cannot account for the
    performance of algorithms, especially in the field of medical image analysis,
    where problems of class imbalance, noise interference problems and serious consequences
    of missed tests must be considered. In the following subsections, we will analyze
    potential future research directions for medical image segmentation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，基于深度神经网络的医学图像全自动分割非常有价值。通过回顾深度学习在医学图像分割中的进展，我们识别出了一些潜在的困难。研究人员成功地采用了多种手段来提高医学图像分割的准确性。然而，仅仅提高准确性无法完全体现算法的性能，特别是在医学图像分析领域，还必须考虑类别不平衡、噪声干扰问题以及漏检的严重后果。在接下来的子节中，我们将分析医学图像分割的潜在未来研究方向。
- en: V-C1 Design of Network Architecture
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 网络架构设计
- en: In studies of medical image segmentation, the innovation of network structure
    design is most popular, as the improvement of network structure design shows clear
    effect and it is easily transferred to other tasks. Through reviewing classical
    models in recent years, we find that the basic framework of encoder-decoder U-shaped
    networks with long and short skipped connections has been widely used for medical
    image segmentation. The residual network (ResNet) and the densely connected network
    (DenseNet) have demonstrated the effect of deepening network depth and the effectiveness
    of residual structure on gradient propagation, respectively. Skip connections
    in deep networks can facilitate gradient propagation and thus reduce the risk
    of gradient dispersion leading to improved segmentation performance. Furthermore,
    the optimization of skipped connections will allow the model to extract richer
    features.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学图像分割研究中，网络结构设计的创新最为流行，因为网络结构设计的改进效果显著且容易转移到其他任务。通过回顾近年来的经典模型，我们发现带有长短跳跃连接的编码器-解码器U型网络的基本框架在医学图像分割中得到了广泛应用。残差网络（ResNet）和密集连接网络（DenseNet）分别展示了网络深度加深和残差结构对梯度传播的有效性。深度网络中的跳跃连接可以促进梯度传播，从而减少梯度消散的风险，提高分割性能。此外，跳跃连接的优化将使模型提取更丰富的特征。
- en: In addition, the design of the network module is worth exploring. Recently,
    spatial pyramid modules have been widely used in the field of semantic segmentation.
    The atrous convolution with fewer parameters allows for wider receptive fields,
    and the feature pyramid allows for features with different scales to be acquired.
    The development of spatial channel attention modules makes the process of neural
    network feature extraction more targeted, so the design of task-specific feature
    extraction network modules is also well worth investigating.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，网络模块的设计也是值得探索的。最近，空间金字塔模块在语义分割领域得到了广泛应用。具有更少参数的空洞卷积允许更宽的感受野，而特征金字塔允许获取不同尺度的特征。空间通道注意力模块的发展使神经网络特征提取过程更加有针对性，因此，任务特定特征提取网络模块的设计也值得进一步研究。
- en: The manual design of model structures requires rich experiences, so it is inevitable
    that NAS will gradually replace the manual design. However, it is difficult to
    search directly a large network due to memory and GPU limitations. Therefore,
    the future trend should be the combination of manual design and the use of NAS
    technology. First, a backbone network is designed manually, and then small network
    modules are searched by NAS before training.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 手动设计模型结构需要丰富的经验，因此NAS逐渐取代手动设计是不可避免的。然而，由于内存和GPU限制，直接搜索大型网络是困难的。因此，未来的趋势应是手动设计与NAS技术的结合。首先，手动设计一个骨干网络，然后在训练前通过NAS搜索小型网络模块。
- en: The design of different convolution operations is also a meaningful research
    direction, such as atrous convolution, deformable convolution, deep separable
    convolution, etc. Although these convolutions are all excellent for improving
    performance of models, they still belong to traditional convolutional categories.
    As a convolutional method of processing non-Euclidean data, the graph convolution
    goes beyond the traditional convolution and is valuable for medical data because
    the graph structure is more efficient and has a strong semantic feature encoding
    capability.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不同卷积操作的设计也是一个有意义的研究方向，例如膨胀卷积、可变形卷积、深度可分离卷积等。虽然这些卷积在提升模型性能方面都很出色，但它们仍属于传统卷积类别。作为处理非欧几里得数据的卷积方法，图卷积超越了传统卷积，并且对医学数据具有重要价值，因为图结构更有效且具有强大的语义特征编码能力。
- en: V-C2 Design of Loss Function
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 损失函数的设计
- en: In many medical image segmentation tasks, there are often only one or two targets
    in an image, and the pixel ratio of targets is sometimes small, which makes network
    training difficult. For this problem, it is easier to focus on smaller targets
    by changing loss functions than to change the network structure. However, the
    design of loss functions is highly task-specific, so we need to analyze carefully
    task requirement, and then design reasonable and available loss functions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多医学图像分割任务中，图像中通常只有一两个目标，而且目标的像素比例有时较小，这使得网络训练变得困难。对于这个问题，通过更改损失函数来关注较小的目标比更改网络结构更容易。然而，损失函数的设计高度依赖于任务，因此我们需要仔细分析任务需求，然后设计合理且可用的损失函数。
- en: In specific tasks of medical image segmentation, the use of classical cross-entropy
    loss functions combined with a specific regularization term or a specific loss
    function has become a popular trend. In addition, the use of domain knowledge
    or a priori knowledge as regular terms or the design of specific loss functions
    can yield better task-specific segmentation results for medical images. Another
    avenue is an automatic loss function (or regularization term) search based on
    NAS techniques.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学图像分割的具体任务中，结合特定正则化项或特定损失函数的经典交叉熵损失函数的使用已成为一种流行趋势。此外，利用领域知识或先验知识作为正则项或设计特定损失函数可以为医学图像提供更好的任务特定分割结果。另一个方向是基于NAS技术的自动损失函数（或正则项）搜索。
- en: V-C3 Transfer learning
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C3 迁移学习
- en: Medical imaging is usually accompanied by severe noise interference. Moreover,
    the data annotation of medical images is often more expensive than natural images.
    Therefore, medical image segmentation based on pre-trained deep learning models
    on natural images is a worthy direction for future research.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 医学成像通常伴随着严重的噪声干扰。此外，医学图像的数据标注通常比自然图像更昂贵。因此，基于在自然图像上预训练的深度学习模型的医学图像分割是未来研究的一个值得方向。
- en: In addition, transfer learning is an important way to achieve weakly supervised
    medical image segmentation. In fact, transfer learning is the use of existing
    knowledge to learn new knowledge, and it focuses on finding similarities between
    existing knowledge and new knowledge. Since most data or tasks are correlated,
    transfer learning allows us to share the model parameters (or knowledge learned
    by the model) with the new model in a way that speeds up the efficiency of model
    learning. Thus, transfer learning can solve the problem of insufficient labelling
    data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，迁移学习是实现弱监督医学图像分割的重要途径。实际上，迁移学习是利用现有知识学习新知识，并且侧重于寻找现有知识与新知识之间的相似性。由于大多数数据或任务是相关的，迁移学习允许我们以一种提高模型学习效率的方式与新模型共享模型参数（或模型学到的知识）。因此，迁移学习可以解决标注数据不足的问题。
- en: V-C4 Interactive Segmentation
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C4 交互式分割
- en: Although deep learning has achieved good results in many image segmentation
    tasks, the vast majority of related works have been with automatic segmentation
    methods. Many cases still require interactive segmentation methods, such as the
    annotation of radiotherapy targets, or when user correction is required because
    the automatic segmentation results are not good enough. In addition, training
    deep learning models often requires a large number of labeled images as the training
    datasets that can be done more efficiently with an interactive segmentation tool.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在许多图像分割任务中取得了良好效果，但绝大多数相关工作仍然使用自动分割方法。许多情况下仍然需要交互式分割方法，例如放射治疗靶点的标注，或当自动分割结果不够好时需要用户校正。此外，训练深度学习模型通常需要大量标注图像作为训练数据集，这可以通过交互式分割工具更高效地完成。
- en: Due to the superior performance of deep learning, the interactive image segmentation [[126](#bib.bib126)]
    based on deep learning can reduce the number of user interactions and the user
    time that shows broader application prospect.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习的优越性能，基于深度学习的交互式图像分割[[126](#bib.bib126)]可以减少用户交互的次数和用户时间，显示出更广泛的应用前景。
- en: V-C5 Graph Convolutional Neural Network
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C5 图卷积神经网络
- en: 'In general, convolution-based deep neural networks with translation invariance,
    rotation invariance, scale invariance, shared convolution kernels and fast automatic
    feature extraction have yielded remarkable results in the field of medical images.
    However, convolutional neural networks also have many limitations: they rely heavily
    on geometric priors and it is difficult to capture the intrinsic relationships
    between different objects using extracted local features, etc. GNN provides a
    powerful and intuitive modelling approach [[175](#bib.bib175)] to the problem
    of modelling non-Euclidean spaces. Taking the studied objects as nodes and the
    correlation or similarity between objects as edges, GNN is able to integrate non-Euclidean
    data and extract invisible relationships between objects by exploiting their intrinsic
    relationships, and it has been widely used in brain segmentation [[176](#bib.bib176)],
    vessel segmentation [[177](#bib.bib177)], prostate segmentation [[178](#bib.bib178)],
    coronary artery segmentation [[141](#bib.bib141)], etc.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，具有平移不变性、旋转不变性、尺度不变性、共享卷积核和快速自动特征提取的卷积神经网络在医学图像领域取得了显著成果。然而，卷积神经网络也存在许多局限性：它们高度依赖几何先验，并且难以通过提取的局部特征捕捉不同对象之间的内在关系等。GNN
    提供了一种强大且直观的建模方法[[175](#bib.bib175)]，用于建模非欧几里得空间。GNN 将研究对象视为节点，将对象之间的关联或相似性视为边，通过利用对象之间的内在关系，GNN
    能够整合非欧几里得数据并提取对象之间的隐性关系，已经广泛应用于脑部分割[[176](#bib.bib176)]、血管分割[[177](#bib.bib177)]、前列腺分割[[178](#bib.bib178)]、冠状动脉分割[[141](#bib.bib141)]等。
- en: V-C6 Medical Transformer
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C6 医学变换器
- en: In recent years, deep neural networks based on U-shaped structures and skip
    connection have been widely used in various medical imaging tasks. However, in
    despite of the fact of achieving excellent performance by CNNs, it is unable to
    learn global and long-range semantic information interactions well due to the
    limitations of convolutional operations. Recently, transformer-based architectures
    have become very popular that replaces the convolutional operator and use self-attention
    modules to compose entire encoder-decoder structures that can encode long-range
    dependencies. It has been a great success in the field of natural language processing.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于 U 形结构和跳跃连接的深度神经网络在各种医学影像任务中得到了广泛应用。然而，尽管 CNN 已经取得了出色的性能，但由于卷积操作的限制，它无法很好地学习全局和长距离的语义信息交互。最近，基于变换器的架构变得非常流行，这种架构替代了卷积操作符，并使用自注意力模块构建了完整的编码器-解码器结构，可以编码长距离依赖关系。这在自然语言处理领域取得了巨大的成功。
- en: Dosovitskiy et al. [[179](#bib.bib179)] proposed Vision Transformer (ViT) that
    is able to classify images directly using the Transformer. Recently, a large number
    of researches [[180](#bib.bib180)] [[181](#bib.bib181)] [[182](#bib.bib182)] [[183](#bib.bib183)]
    have applied the transformer to medical image segmentation. CNNs have a comparative
    advantage in extracting the underlying features. These low-level features form
    the key points, lines, and some basic image structures at the patch level. However,
    when we detect these basic visual elements, the higher-level visual semantic information
    is often more concerned with how these elements relate to each other to form an
    object, and how the spatial location of objects relates to each other to form
    the scene. At present, the transformer is more natural and effective in dealing
    with the relationships between these elements. However, if all the convolutional
    operators in CV tasks are replaced by Transformer, it may suffer from many problems,
    such as high computational cost and memory usage. From existing researches, the
    combination of Transformer and CNNs may lead to better results.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人[[179](#bib.bib179)] 提出了 Vision Transformer (ViT)，它能够直接使用 Transformer
    对图像进行分类。近年来，大量研究[[180](#bib.bib180)] [[181](#bib.bib181)] [[182](#bib.bib182)]
    [[183](#bib.bib183)] 将 Transformer 应用于医学图像分割。CNN 在提取底层特征方面具有比较优势。这些低级特征在补丁级别形成关键点、线条和一些基本的图像结构。然而，当我们检测这些基本视觉元素时，更高级的视觉语义信息通常更关注这些元素如何相互关联以形成一个对象，以及对象的空间位置如何相互关联以形成场景。目前，Transformer
    在处理这些元素之间的关系时更加自然和有效。然而，如果将 CV 任务中的所有卷积操作符都替换为 Transformer，可能会遇到许多问题，如高计算成本和内存使用。从现有研究来看，Transformer
    和 CNN 的结合可能会带来更好的结果。
- en: Acknowledgment
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was supported in part by Natural Science Basic Research Program of
    Shaanxi (Program No. 2021JC-47), in part by the National Natural Science Foundation
    of China under Grant 61871259, Grant 61861024, National Natural Science Foundation
    of China-Royal Society: Grant 61811530325 (IECnNSFCn170396, Royal Society, U.K.),
    in part by Key Research and Development Program of Shaanxi (Program No. 2021ZDLGY08-07),
    and in part by Shaanxi Joint Laboratory of Artificial Intelligence (Program No.
    2020SS-03).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了陕西省自然科学基础研究计划（项目编号2021JC-47）的支持，部分得到了中国国家自然科学基金资助（资助号61871259，61861024），中国国家自然科学基金-皇家学会：资助号61811530325（IECnNSFCn170396，皇家学会，英国）的支持，部分得到了陕西省重点研发计划（项目编号2021ZDLGY08-07）的支持，部分得到了陕西省人工智能联合实验室（项目编号2020SS-03）的支持。
- en: References
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] W. Li, “Automatic segmentation of liver tumor in ct images with deep convolutional
    neural networks,” *J. Comput. Commun.*, vol. 3, no. 11, pp. 146–151, 2015.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] W. Li，“使用深度卷积神经网络进行CT图像中的肝肿瘤自动分割，” *J. Comput. Commun.*，第3卷，第11期，页码146–151，2015年。'
- en: '[2] R. Vivanti, A. Ephrat, L. Joskowicz, O. Karaaslan, N. Lev-Cohain, and J. Sosna,
    “Automatic liver tumor segmentation in follow-up ct studies using convolutional
    neural networks,” vol. 2, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. Vivanti, A. Ephrat, L. Joskowicz, O. Karaaslan, N. Lev-Cohain, 和 J.
    Sosna，“使用卷积神经网络在后续CT研究中进行肝肿瘤自动分割，” 第2卷，2015年。'
- en: '[3] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest *et al.*, “The multimodal brain tumor
    image segmentation benchmark (brats),” *IEEE Trans. Med. Image.*, vol. 34, no. 10,
    pp. 1993–2024, 2014.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    Y. Burren, N. Porz, J. Slotboom, R. Wiest *等*，“多模态脑肿瘤图像分割基准（brats），” *IEEE Trans.
    Med. Image.*，第34卷，第10期，页码1993–2024，2014年。'
- en: '[4] V. Cherukuri, P. Ssenyonga, B. C. Warf, A. V. Kulkarni, V. Monga, and S. J.
    Schiff, “Learning based segmentation of ct brain images: application to postoperative
    hydrocephalic scans,” *IEEE Trans. Bio-Med. Eng.*, vol. 65, no. 8, pp. 1871–1884,
    2017.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] V. Cherukuri, P. Ssenyonga, B. C. Warf, A. V. Kulkarni, V. Monga, 和 S.
    J. Schiff，“基于学习的CT脑图像分割：应用于术后脑积水扫描，” *IEEE Trans. Bio-Med. Eng.*，第65卷，第8期，页码1871–1884，2017年。'
- en: '[5] J. Cheng, J. Liu, Y. Xu, F. Yin, D. W. K. Wong, N.-M. Tan, D. Tao, C.-Y.
    Cheng, T. Aung, and T. Y. Wong, “Superpixel classification based optic disc and
    optic cup segmentation for glaucoma screening,” *IEEE Trans. Med. Image.*, vol. 32,
    no. 6, pp. 1019–1032, 2013.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Cheng, J. Liu, Y. Xu, F. Yin, D. W. K. Wong, N.-M. Tan, D. Tao, C.-Y.
    Cheng, T. Aung, 和 T. Y. Wong，“基于超像素分类的视盘和视杯分割用于青光眼筛查，” *IEEE Trans. Med. Image.*，第32卷，第6期，页码1019–1032，2013年。'
- en: '[6] H. Fu, J. Cheng, Y. Xu, D. W. K. Wong, J. Liu, and X. Cao, “Joint optic
    disc and cup segmentation based on multi-label deep network and polar transformation,”
    *IEEE Trans. Med. Image.*, vol. 37, no. 7, pp. 1597–1605, 2018.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Fu, J. Cheng, Y. Xu, D. W. K. Wong, J. Liu, 和 X. Cao, “基于多标签深度网络和极坐标变换的视盘及视杯分割，”
    *IEEE Trans. Med. Image.*, 卷37, 号7, 页1597–1605, 2018。'
- en: '[7] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” *Proc. Int. Conf. Med. Image Comput. Comput.
    Assist. Intervent. (MICCAI)*, pp. 234–241, 2015.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] O. Ronneberger, P. Fischer, 和 T. Brox, “U-net：用于生物医学图像分割的卷积网络，” *Proc.
    Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)*, 页234–241,
    2015。'
- en: '[8] T.-H. Song, V. Sanchez, H. EIDaly, and N. M. Rajpoot, “Dual-channel active
    contour model for megakaryocytic cell segmentation in bone marrow trephine histology
    images,” *IEEE Trans. Bio-Med. Eng.*, vol. 64, no. 12, pp. 2913–2923, 2017.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T.-H. Song, V. Sanchez, H. EIDaly, 和 N. M. Rajpoot, “用于骨髓穿刺组织学图像中巨核细胞分割的双通道主动轮廓模型，”
    *IEEE Trans. Bio-Med. Eng.*, 卷64, 号12, 页2913–2923, 2017。'
- en: '[9] S. Wang, M. Zhou, Z. Liu, Z. Liu, D. Gu, Y. Zang, D. Dong, O. Gevaert,
    and J. Tian, “Central focused convolutional neural networks: Developing a data-driven
    model for lung nodule segmentation,” *Med. Image Anal.*, vol. 40, pp. 172–183,
    2017.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Wang, M. Zhou, Z. Liu, Z. Liu, D. Gu, Y. Zang, D. Dong, O. Gevaert,
    和 J. Tian, “中央聚焦卷积神经网络：开发用于肺结节分割的数据驱动模型，” *Med. Image Anal.*, 卷40, 页172–183, 2017。'
- en: '[10] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama,
    K. Imaizumi, and H. Fujita, “Multiplanar analysis for pulmonary nodule classification
    in ct images using deep convolutional neural network and generative adversarial
    networks,” *Int. J. Comput. Assist. Radiol Surg.*, vol. 15, no. 1, pp. 173–178,
    2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto, K. Saito, H. Toyama,
    K. Imaizumi, 和 H. Fujita, “使用深度卷积神经网络和生成对抗网络进行CT图像中肺结节分类的多平面分析，” *Int. J. Comput.
    Assist. Radiol Surg.*, 卷15, 号1, 页173–178, 2020。'
- en: '[11] F. Wu and X. Zhuang, “Cf distance: A new domain discrepancy metric and
    application to explicit domain adaptation for cross-modality cardiac image segmentation,”
    *IEEE Transactions on Medical Imaging*, vol. 39, no. 12, pp. 4274–4285, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] F. Wu 和 X. Zhuang, “Cf距离：一种新的领域差异度量及其在交叉模态心脏图像分割显式领域适配中的应用，” *IEEE Transactions
    on Medical Imaging*, 卷39, 号12, 页4274–4285, 2020。'
- en: '[12] C. Chen, C. Qin, H. Qiu, G. Tarroni, J. Duan, W. Bai, and D. Rueckert,
    “Deep learning for cardiac image segmentation: a review,” *Frontiers in Cardiovascular
    Medicine*, vol. 7, p. 25, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. Chen, C. Qin, H. Qiu, G. Tarroni, J. Duan, W. Bai, 和 D. Rueckert, “心脏影像分割的深度学习：综述，”
    *Frontiers in Cardiovascular Medicine*, 卷7, 页25, 2020。'
- en: '[13] Z. Yu-Qian, G. Wei-Hua, C. Zhen-Cheng, T. Jing-Tian, and L. Ling-Yun,
    “Medical images edge detection based on mathematical morphology,” *Proc. IEEE
    Eng. Med. Biol. Soc.*, pp. 6492–6495, 2006.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Yu-Qian, G. Wei-Hua, C. Zhen-Cheng, T. Jing-Tian, 和 L. Ling-Yun, “基于数学形态学的医学图像边缘检测，”
    *Proc. IEEE Eng. Med. Biol. Soc.*, 页6492–6495, 2006。'
- en: '[14] M. Lalonde, M. Beaulieu, and L. Gagnon, “Fast and robust optic disc detection
    using pyramidal decomposition and hausdorff-based template matching,” *IEEE Trans.
    Med. Image.*, vol. 20, no. 11, pp. 1193–1200, 2001.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Lalonde, M. Beaulieu, 和 L. Gagnon, “利用金字塔分解和基于Hausdorff的模板匹配进行快速而稳健的视盘检测，”
    *IEEE Trans. Med. Image.*, 卷20, 号11, 页1193–1200, 2001。'
- en: '[15] W. Chen, R. Smith, S.-Y. Ji, K. R. Ward, and K. Najarian, “Automated ventricular
    systems segmentation in brain ct images by combining low-level segmentation and
    high-level template matching,” *BMC Medical Inform. Decis. Mak.*, vol. 9, no. S1,
    p. S4, 2009.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W. Chen, R. Smith, S.-Y. Ji, K. R. Ward, 和 K. Najarian, “通过结合低级分割和高级模板匹配在脑CT图像中自动分割脑室系统，”
    *BMC Medical Inform. Decis. Mak.*, 卷9, 号S1, 页S4, 2009。'
- en: '[16] A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. E. Grimson,
    and A. Willsky, “A shape-based approach to the segmentation of medical imagery
    using level sets,” *IEEE Trans. Med. Imaging*, vol. 22, no. 2, pp. 137–154, 2003.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. E. Grimson,
    和 A. Willsky, “基于形状的方法用于利用水平集进行医学影像分割，” *IEEE Trans. Med. Imaging*, 卷22, 号2, 页137–154,
    2003。'
- en: '[17] C. Li, X. Wang, S. Eberl, M. Fulham, Y. Yin, J. Chen, and D. D. Feng,
    “A likelihood and local constraint level set model for liver tumor segmentation
    from ct volumes,” *IEEE Trans. Biomed. Eng.*, vol. 60, no. 10, pp. 2967–2977,
    2013.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Li, X. Wang, S. Eberl, M. Fulham, Y. Yin, J. Chen, 和 D. D. Feng, “用于从CT体积中分割肝肿瘤的似然和局部约束水平集模型，”
    *IEEE Trans. Biomed. Eng.*, 卷60, 号10, 页2967–2977, 2013。'
- en: '[18] S. Li, T. Fevens, and A. Krzyżak, “A svm-based framework for autonomous
    volumetric medical image segmentation using hierarchical and coupled level sets,”
    *Int. Congr. Series*, vol. 1268, pp. 207–212, 2004.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Li, T. Fevens, 和 A. Krzyżak，“基于SVM的自主体积医学图像分割框架，使用分层和耦合的水平集，” *国际大会系列*，第1268卷，第207–212页，2004年。'
- en: '[19] K. Held, E. R. Kops, B. J. Krause, W. M. Wells, R. Kikinis, and H.-W.
    Muller-Gartner, “Markov random field segmentation of brain mr images,” *IEEE Trans.
    Med. Imaging*, vol. 16, no. 6, pp. 878–886, 1997.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] K. Held, E. R. Kops, B. J. Krause, W. M. Wells, R. Kikinis, 和 H.-W. Muller-Gartner，“脑部MRI图像的马尔可夫随机场分割，”
    *IEEE医学成像汇刊*，第16卷，第6期，第878–886页，1997年。'
- en: '[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Commun. ACM*, vol. 60, no. 6, pp. 84–90,
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络的ImageNet分类，” *ACM通信*，第60卷，第6期，第84–90页，2017年。'
- en: '[21] S. Masood, M. Sharif, A. Masood, M. Yasmin, and M. Raza, “A survey on
    medical image segmentation,” *Curr. Med. Imaging Rev.*, vol. 11, no. 1, pp. 3–14,
    2015.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Masood, M. Sharif, A. Masood, M. Yasmin, 和 M. Raza，“医学图像分割的调查，” *当前医学成像评论*，第11卷，第1期，第3–14页，2015年。'
- en: '[22] D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,”
    *Ann. Rev. Biomed. Eng*, vol. 19, pp. 221–248, 2017.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. Shen, G. Wu, 和 H.-I. Suk，“医学图像分析中的深度学习，” *生物医学工程年评*，第19卷，第221–248页，2017年。'
- en: '[23] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Med. Image Anal.*, vol. 42, pp. 60–88, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, 和 C. I. Sánchez，“深度学习在医学图像分析中的调查，” *医学图像分析*，第42卷，第60–88页，2017年。'
- en: '[24] S. A. Taghanaki, K. Abhishek, J. P. Cohen, J. Cohen-Adad, and G. Hamarneh,
    “Deep semantic segmentation of natural and medical images: A review,” *Artif.
    Intell. Rev.*, pp. 1–42, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. A. Taghanaki, K. Abhishek, J. P. Cohen, J. Cohen-Adad, 和 G. Hamarneh，“自然与医学图像的深度语义分割：回顾，”
    *人工智能评论*，第1–42页，2020年。'
- en: '[25] H. Seo, M. Badiei Khuzani, V. Vasudevan, C. Huang, H. Ren, R. Xiao, X. Jia,
    and L. Xing, “Machine learning techniques for biomedical image segmentation: An
    overview of technical aspects and introduction to state-of-art applications,”
    *Med. Phys.*, vol. 47, no. 5, pp. e148–e167, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Seo, M. Badiei Khuzani, V. Vasudevan, C. Huang, H. Ren, R. Xiao, X.
    Jia, 和 L. Xing，“生物医学图像分割的机器学习技术：技术方面概述及最新应用介绍，” *医学物理*，第47卷，第5期，第e148–e167页，2020年。'
- en: '[26] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
    “Embracing imperfect datasets: A review of deep learning solutions for medical
    image segmentation,” *Med. Image Anal.*, p. 101693, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, 和 X. Ding，“接受不完美的数据集：深度学习在医学图像分割中的解决方案回顾，”
    *医学图像分析*，第101693页，2020年。'
- en: '[27] M. H. Hesamian, W. Jia, X. He, and P. Kennedy, “Deep learning techniques
    for medical image segmentation: Achievements and challenges,” *J. Digit. Imaging*,
    vol. 32, no. 4, pp. 582–596, 2019.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. H. Hesamian, W. Jia, X. He, 和 P. Kennedy，“医学图像分割的深度学习技术：成就与挑战，” *数字成像期刊*，第32卷，第4期，第582–596页，2019年。'
- en: '[28] P. Meyer, V. Noblet, C. Mazzara, and A. Lallement, “Survey on deep learning
    for radiotherapy,” *Comput. Biol. Med.*, vol. 98, pp. 126–146, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] P. Meyer, V. Noblet, C. Mazzara, 和 A. Lallement，“关于放射治疗的深度学习调查，” *计算生物医学*，第98卷，第126–146页，2018年。'
- en: '[29] Z. Akkus, A. Galimzianova, A. Hoogi, D. L. Rubin, and B. J. Erickson,
    “Deep learning for brain mri segmentation: state of the art and future directions,”
    *J. Digit. Imaging*, vol. 30, no. 4, pp. 449–459, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Akkus, A. Galimzianova, A. Hoogi, D. L. Rubin, 和 B. J. Erickson，“脑MRI分割的深度学习：现状与未来方向，”
    *数字成像期刊*，第30卷，第4期，第449–459页，2017年。'
- en: '[30] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    science review*, vol. 5, no. 1, pp. 44–53, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z.-H. Zhou，“弱监督学习简要介绍，” *国家科学评论*，第5卷，第1期，第44–53页，2018年。'
- en: '[31] T. Eelbode, J. Bertels, M. Berman, D. Vandermeulen, F. Maes, R. Bisschops,
    and M. B. Blaschko, “Optimization for medical image segmentation: theory and practice
    when evaluating with dice score or jaccard index,” *IEEE Trans. Med. Imaging*,
    vol. 39, no. 11, pp. 3679–3690, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Eelbode, J. Bertels, M. Berman, D. Vandermeulen, F. Maes, R. Bisschops,
    和 M. B. Blaschko，“医学图像分割的优化：使用dice评分或jaccard指数进行评估的理论与实践，” *IEEE医学成像汇刊*，第39卷，第11期，第3679–3690页，2020年。'
- en: '[32] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” *Proc. the IEEE Conf. Comput. Vis. Pattern Recognitit.
    (CVPR)*, pp. 3431–3440, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Long, E. Shelhamer, 和 T. Darrell，“用于语义分割的全卷积网络，” *IEEE计算机视觉与模式识别会议论文集（CVPR）*，第3431–3440页，2015年。'
- en: '[33] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L.-C. Chen, G. Papandreou, F. Schroff 和 H. Adam, “重新思考 atrous 卷积在语义图像分割中的应用，”
    *arXiv preprint arXiv:1706.05587*, 2017.'
- en: '[34] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
    “3d u-net: learning dense volumetric segmentation from sparse annotation,” *Proc.
    Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)*, pp. 424–432,
    2016.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox 和 O. Ronneberger, “3D
    U-Net: 从稀疏标注中学习密集体积分割，” *Proc. Int. Conf. Med. Image Comput. Comput. Assist. Intervent.
    (MICCAI)*, pp. 424–432, 2016.'
- en: '[35] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
    neural networks for volumetric medical image segmentation,” *Conf. 3D Vis. (3DV)*,
    pp. 565–571, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] F. Milletari, N. Navab 和 S.-A. Ahmadi, “V-net: 完全卷积神经网络用于体积医学图像分割，” *Conf.
    3D Vis. (3DV)*, pp. 565–571, 2016.'
- en: '[36] H. Chen, Q. Dou, L. Yu, and P.-A. Heng, “Voxresnet: Deep voxelwise residual
    networks for volumetric brain segmentation,” *arXiv preprint arXiv:1608.05895*,
    2016.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Chen, Q. Dou, L. Yu 和 P.-A. Heng, “Voxresnet: 深度体素残差网络用于体积脑部分割，” *arXiv
    preprint arXiv:1608.05895*, 2016.'
- en: '[37] K. Lee, J. Zung, P. Li, V. Jain, and H. S. Seung, “Superhuman accuracy
    on the snemi3d connectomics challenge,” *arXiv preprint arXiv:1706.00120*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] K. Lee, J. Zung, P. Li, V. Jain 和 H. S. Seung, “在 SNEMI3D 连接组学挑战中的超人准确度，”
    *arXiv preprint arXiv:1706.00120*, 2017.'
- en: '[38] X. Xiao, S. Lian, Z. Luo, and S. Li, “Weighted res-unet for high-quality
    retina vessel segmentation,” *Conf. Informa. Technol. Med. Educ. (ITME)*, pp.
    327–331, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. Xiao, S. Lian, Z. Luo 和 S. Li, “高质量视网膜血管分割的加权 res-unet,” *Conf. Informa.
    Technol. Med. Educ. (ITME)*, pp. 327–331, 2018.'
- en: '[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Comput.*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Hochreiter 和 J. Schmidhuber, “长短期记忆网络，” *Neural Comput.*, vol. 9, no.
    8, pp. 1735–1780, 1997.'
- en: '[40] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, “Recurrent
    residual convolutional neural network based on u-net (r2u-net) for medical image
    segmentation,” *arXiv preprint arXiv:1802.06955*, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha 和 V. K. Asari, “基于 U-Net
    的递归残差卷积神经网络（R2U-Net）用于医学图像分割，” *arXiv preprint arXiv:1802.06955*, 2018.'
- en: '[41] Y. Gao, J. M. Phillips, Y. Zheng, R. Min, P. T. Fletcher, and G. Gerig,
    “Fully convolutional structured lstm networks for joint 4d medical image segmentation,”
    *Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)*, pp. 1104–1108, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Y. Gao, J. M. Phillips, Y. Zheng, R. Min, P. T. Fletcher 和 G. Gerig, “用于联合
    4D 医学图像分割的完全卷积结构化 LSTM 网络，” *Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)*, pp.
    1104–1108, 2018.'
- en: '[42] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews, and D. Rueckert,
    “Recurrent neural networks for aortic image sequence segmentation with sparse
    annotations,” *Proc. Int. Conf. Med. Image Comput. Comput. Assist. Intervent.(MICCAI)*,
    pp. 586–594, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews 和 D. Rueckert,
    “用于稀疏标注的主动脉图像序列分割的递归神经网络，” *Proc. Int. Conf. Med. Image Comput. Comput. Assist.
    Intervent. (MICCAI)*, pp. 586–594, 2018.'
- en: '[43] N. Ibtehaz and M. S. Rahman, “Multiresunet: Rethinking the u-net architecture
    for multimodal biomedical image segmentation,” *Neural Netw.*, vol. 121, pp. 74–87,
    2020.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] N. Ibtehaz 和 M. S. Rahman, “Multiresunet: 重新思考 u-net 架构用于多模态生物医学图像分割，”
    *Neural Netw.*, vol. 121, pp. 74–87, 2020.'
- en: '[44] H. Seo, C. Huang, M. Bassenne, R. Xiao, and L. Xing, “Modified u-net (mu-net)
    with incorporation of object-dependent high level features for improved liver
    and liver-tumor segmentation in ct images,” *IEEE Trans. Med. Imag.*, vol. 39,
    no. 5, pp. 1316–1325, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Seo, C. Huang, M. Bassenne, R. Xiao 和 L. Xing, “修改的 U-Net (MU-Net)
    结合了对象依赖的高级特征，用于改进 CT 图像中的肝脏和肝脏肿瘤分割，” *IEEE Trans. Med. Imag.*, vol. 39, no. 5,
    pp. 1316–1325, 2019.'
- en: '[45] X. Chen, R. Zhang, and P. Yan, “Feature fusion encoder decoder network
    for automatic liver lesion segmentation,” *Proc. IEEE 16th Int. Symp. Biomed.
    Imag. (ISBI)*, pp. 430–433, 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] X. Chen, R. Zhang 和 P. Yan, “用于自动肝脏病变分割的特征融合编码解码网络，” *Proc. IEEE 16th
    Int. Symp. Biomed. Imag. (ISBI)*, pp. 430–433, 2019.'
- en: '[46] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
    P. Bilic, M. Rempfler, M. Armbruster, F. Hofmann, M. D’Anastasi *et al.*, “Automatic
    liver and lesion segmentation in ct using cascaded fully convolutional neural
    networks and 3d conditional random fields,” *Proc. Int. Conf. Med. Image Comput.
    Comput.-Assist. Intervent.*, pp. 415–423, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
    P. Bilic, M. Rempfler, M. Armbruster, F. Hofmann, M. D’Anastasi *等*, “使用级联完全卷积神经网络和
    3D 条件随机场进行 CT 中的自动肝脏和病变分割，” *Proc. Int. Conf. Med. Image Comput. Comput.-Assist.
    Intervent.*, pp. 415–423, 2016.'
- en: '[47] W. Tang, D. Zou, S. Yang, and J. Shi, “Dsl: Automatic liver segmentation
    with faster r-cnn and deeplab,” *Proc. Int. Conf. Artif. Neural Netw.*, pp. 137–147,
    2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Tang, D. Zou, S. Yang 和 J. Shi，“DSL: 使用更快的R-CNN和DeepLab的自动肝脏分割”，*Proc.
    Int. Conf. Artif. Neural Netw.*，第137–147页，2018年。'
- en: '[48] K. C. Kaluva, M. Khened, A. Kori, and G. Krishnamurthi, “2d-densely connected
    convolution neural networks for automatic liver and tumor segmentation,” *arXiv
    preprint arXiv:1802.02182*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. C. Kaluva, M. Khened, A. Kori 和 G. Krishnamurthi，“用于自动肝脏和肿瘤分割的2D密集连接卷积神经网络”，*arXiv
    preprint arXiv:1802.02182*，2018年。'
- en: '[49] X. Feng, C. Wang, S. Cheng, and L. Guo, “Automatic liver and tumor segmentation
    of ct based on cascaded u-net,” *Proc. Chin. Int. Syst. Conf.*, pp. 155–164, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Feng, C. Wang, S. Cheng 和 L. Guo，“基于级联U-Net的CT自动肝脏和肿瘤分割”，*Proc. Chin.
    Int. Syst. Conf.*，第155–164页，2019年。'
- en: '[50] A. A. Albishri, S. J. H. Shah, and Y. Lee, “Cu-net: Cascaded u-net model
    for automated liver and lesion segmentation and summarization,” *Proc. IEEE Int.
    Conf. Bioinform. Biomed. (BIBM)*, pp. 1416–1423, 2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. A. Albishri, S. J. H. Shah 和 Y. Lee，“Cu-Net: 级联U-Net模型用于自动肝脏和病变分割及总结”，*Proc.
    IEEE Int. Conf. Bioinform. Biomed. (BIBM)*，第1416–1423页，2019年。'
- en: '[51] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” *Proc. the
    IEEE Conf. on Comput. Vis. (ICCV)*, pp. 2961–2969, 2017.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. He, G. Gkioxari, P. Dollár 和 R. Girshick，“Mask R-CNN”，*Proc. the IEEE
    Conf. on Comput. Vis. (ICCV)*，第2961–2969页，2017年。'
- en: '[52] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed
    and accuracy of object detection,” *arXiv preprint arXiv:2004.10934*, 2020.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. Bochkovskiy, C.-Y. Wang 和 H.-Y. M. Liao，“Yolov4: 目标检测的最佳速度和精度”，*arXiv
    preprint arXiv:2004.10934*，2020年。'
- en: '[53] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han, and T.-S. Kim,
    “A fully integrated computer-aided diagnosis system for digital x-ray mammograms
    via deep learning detection, segmentation, and classification,” *Int. J. Med.
    Inform.*, vol. 117, pp. 44–54, 2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. A. Al-Antari, M. A. Al-Masni, M.-T. Choi, S.-M. Han 和 T.-S. Kim，“通过深度学习检测、分割和分类的数字X光乳腺图像的全功能计算机辅助诊断系统”，*Int.
    J. Med. Inform.*，第117卷，第44–54页，2018年。'
- en: '[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 39, no. 6, pp. 1137–1149, 2016.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Ren, K. He, R. Girshick 和 J. Sun，“Faster R-CNN: 实时目标检测的区域提议网络”，*IEEE
    Trans. Pattern Anal. Mach. Intell.*，第39卷，第6期，第1137–1149页，2016年。'
- en: '[55] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40,
    no. 4, pp. 834–848, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy 和 A. L. Yuille，“DeepLab:
    结合深度卷积网络、膨胀卷积和全连接CRFs的语义图像分割”，*IEEE Trans. Pattern Anal. Mach. Intell.*，第40卷，第4期，第834–848页，2017年。'
- en: '[56] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Auto-context convolutional
    neural network (auto-net) for brain extraction in magnetic resonance imaging,”
    *IEEE Trans. Med. Imaging*, vol. 36, no. 11, pp. 2319–2330, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. S. M. Salehi, D. Erdogmus 和 A. Gholipour，“用于磁共振成像中大脑提取的自适应卷积神经网络（auto-net）”，*IEEE
    Trans. Med. Imaging*，第36卷，第11期，第2319–2330页，2017年。'
- en: '[57] Y. Yan, P.-H. Conze, E. Decencière, M. Lamard, G. Quellec, B. Cochener,
    and G. Coatrieux, “Cascaded multi-scale convolutional encoder-decoders for breast
    mass segmentation in high-resolution mammograms,” *Annu. Int. Conf. IEEE. Eng.
    Med. Biol. Soc. (EMBC)*, pp. 6738–6741, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Yan, P.-H. Conze, E. Decencière, M. Lamard, G. Quellec, B. Cochener
    和 G. Coatrieux，“用于高分辨率乳腺X光片中乳腺肿块分割的级联多尺度卷积编码器-解码器”，*Annu. Int. Conf. IEEE. Eng.
    Med. Biol. Soc. (EMBC)*，第6738–6741页，2019年。'
- en: '[58] M. Oda, H. R. Roth, T. Kitasaka, K. Misawa, M. Fujiwara, and K. Mori,
    “Abdominal artery segmentation method from ct volumes using fully convolutional
    neural network,” *Int. J. Comput. Assist Radiol. Surg.*, vol. 14, no. 12, pp.
    2069–2081, 2019.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Oda, H. R. Roth, T. Kitasaka, K. Misawa, M. Fujiwara 和 K. Mori，“基于完全卷积神经网络的腹部动脉分割方法”，*Int.
    J. Comput. Assist Radiol. Surg.*，第14卷，第12期，第2069–2081页，2019年。'
- en: '[59] M. H. Vu, G. Grimbergen, T. Nyholm, and T. Löfstedt, “Evaluation of multi-slice
    inputs to convolutional neural networks for medical image segmentation,” *arXiv
    preprint arXiv:1912.09287*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. H. Vu, G. Grimbergen, T. Nyholm 和 T. Löfstedt，“对医学图像分割的卷积神经网络多切片输入的评估”，*arXiv
    preprint arXiv:1912.09287*，2019年。'
- en: '[60] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-denseunet:
    hybrid densely connected unet for liver and tumor segmentation from ct volumes,”
    *IEEE Trans. Med. Imag*, vol. 37, no. 12, pp. 2663–2674, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu 和 P.-A. Heng，“H-denseunet: 混合密集连接的Unet用于从CT图像中分割肝脏和肿瘤”，*IEEE
    Trans. Med. Imag*，第37卷，第12期，第2663–2674页，2018年。'
- en: '[61] J. Zhang, Y. Xie, P. Zhang, H. Chen, Y. Xia, and C. Shen, “Light-weight
    hybrid convolutional network for liver tumor segmentation.” *Int. Joint Conf.
    Artif. Intell. (IJCAI)*, pp. 4271–4277, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Zhang, Y. Xie, P. Zhang, H. Chen, Y. Xia 和 C. Shen, “轻量级混合卷积网络用于肝脏肿瘤分割，”
    *国际联合人工智能会议 (IJCAI)*, 第4271–4277页, 2019年。'
- en: '[62] R. Dey and Y. Hong, “Hybrid cascaded neural network for liver lesion segmentation,”
    *Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)*, pp. 1173–1177, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. Dey 和 Y. Hong, “用于肝脏病变分割的混合级联神经网络，” *IEEE 国际生物医学影像研讨会 (ISBI)*, 第1173–1177页,
    2020年。'
- en: '[63] J. M. J. Valanarasu, V. A. Sindagi, I. Hacihaliloglu, and V. M. Patel,
    “Kiu-net: Towards accurate segmentation of biomedical images using over-complete
    representations,” *roc. Int. Conf. Med. Image Comput. Comput. Assist. Intervent.(MICCAI)*,
    pp. 363–373, 2020.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. M. J. Valanarasu, V. A. Sindagi, I. Hacihaliloglu 和 V. M. Patel, “Kiu-net:
    通过超完整表示实现生物医学图像的准确分割，” *国际医学图像计算与计算机辅助干预会议 (MICCAI)*, 第363–373页, 2020年。'
- en: '[64] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *Adv. Neural Inform.
    Process. Syst.*, pp. 2672–2680, 2014.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio, “生成对抗网络，” *先进神经信息处理系统*, 第2672–2680页, 2014年。'
- en: '[65] P. Luc, C. Couprie, S. Chintala, and J. Verbeek, “Semantic segmentation
    using adversarial networks,” *arXiv preprint arXiv:1611.08408*, 2016.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. Luc, C. Couprie, S. Chintala 和 J. Verbeek, “使用对抗网络的语义分割，” *arXiv 预印本
    arXiv:1611.08408*, 2016年。'
- en: '[66] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *et al.*, “Breast tumor segmentation and
    shape classification in mammograms using generative adversarial and convolutional
    neural network,” *Expert Syst. Appl.*, vol. 139, p. 112855, 2020.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] V. K. Singh, H. A. Rashwan, S. Romani, F. Akram, N. Pandey, M. M. K. Sarker,
    A. Saleh, M. Arenas, M. Arquez, D. Puig *等*, “使用生成对抗和卷积神经网络的乳腺肿瘤分割与形状分类，” *专家系统应用*,
    卷139, 第112855页, 2020年。'
- en: '[67] P.-H. Conze, A. E. Kavur, E. C.-L. Gall, N. S. Gezer, Y. L. Meur, M. A.
    Selver, and F. Rousseau, “Abdominal multi-organ segmentation with cascaded convolutional
    and adversarial deep networks,” *arXiv preprint arXiv:2001.09521*, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P.-H. Conze, A. E. Kavur, E. C.-L. Gall, N. S. Gezer, Y. L. Meur, M. A.
    Selver 和 F. Rousseau, “腹部多脏器分割与级联卷积和对抗深度网络，” *arXiv 预印本 arXiv:2001.09521*, 2020年。'
- en: '[68] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,
    S. A. Cook, A. De Marvao, T. Dawes, D. P. O‘Regan *et al.*, “Anatomically constrained
    neural networks (acnns): application to cardiac image enhancement and segmentation,”
    *IEEE Trans. Med. Image.*, vol. 37, no. 2, pp. 384–395, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,
    S. A. Cook, A. De Marvao, T. Dawes, D. P. O‘Regan *等*, “解剖约束神经网络（ACNNs）：应用于心脏图像增强和分割，”
    *IEEE 医学影像学报*, 卷37, 第2期, 第384–395页, 2017年。'
- en: '[69] A. Boutillon, B. Borotikar, V. Burdin, and P.-H. Conze, “Combining shape
    priors with conditional adversarial networks for improved scapula segmentation
    in mr images,” *Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)*, pp. 1164–1167, 2020.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Boutillon, B. Borotikar, V. Burdin 和 P.-H. Conze, “结合形状先验和条件对抗网络以改进
    MR 图像中的肩胛骨分割，” *IEEE 国际生物医学影像研讨会 (ISBI)*, 第1164–1167页, 2020年。'
- en: '[70] S. Guan, A. A. Khan, S. Sikdar, and P. V. Chitnis, “Fully dense unet for
    2-d sparse photoacoustic tomography artifact removal,” *IEEE journal of biomedical
    and health informatics*, vol. 24, no. 2, pp. 568–576, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Guan, A. A. Khan, S. Sikdar 和 P. V. Chitnis, “用于 2D 稀疏光声断层扫描伪影去除的全密度
    Unet，” *IEEE 生物医学与健康信息学杂志*, 卷24, 第2期, 第568–576页, 2019年。'
- en: '[71] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: Redesigning
    skip connections to exploit multiscale features in image segmentation,” *IEEE
    Trans. Med. Imag.*, vol. 39, no. 6, pp. 1856–1867, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh 和 J. Liang, “Unet++：重新设计跳跃连接以利用图像分割中的多尺度特征，”
    *IEEE 医学影像学报*, 卷39, 第6期, 第1856–1867页, 2019年。'
- en: '[72] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
    nets,” *Artif. Intell. Statistics*, pp. 562–570, 2015.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang 和 Z. Tu, “深度监督网络，” *人工智能统计*,
    第562–570页, 2015年。'
- en: '[73] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” *Proc. the
    IEEE Conf. Comput. Vis. Pattern Recognitit.(CVPR)*, pp. 1–9, 2015.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich, “深入卷积网络的探索，” *IEEE 计算机视觉与模式识别会议 (CVPR)*, 第1–9页,
    2015年。'
- en: '[74] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” *Proc. the IEEE Conf. Comput.
    Vis. Pattern Recognitit.(CVPR)*, pp. 2818–2826, 2016.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna，“重新思考计算机视觉的Inception架构，”
    *IEEE计算机视觉与模式识别会议论文集 (CVPR)*，第2818–2826页，2016年。'
- en: '[75] Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y. Zhao, T. Zhang, S. Gao, and
    J. Liu, “Ce-net: Context encoder network for 2d medical image segmentation,” *IEEE
    Trans. Med. Imag.*, vol. 38, no. 10, pp. 2281–2292, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y. Zhao, T. Zhang, S. Gao, 和
    J. Liu，“Ce-net: 用于2D医学图像分割的上下文编码网络，” *IEEE医学影像学期刊*，第38卷，第10期，第2281–2292页，2019年。'
- en: '[76] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *arXiv preprint arXiv:1704.04861*, 2017.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam，“Mobilenets: 高效的卷积神经网络用于移动视觉应用，” *arXiv 预印本 arXiv:1704.04861*，2017年。'
- en: '[77] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 4510–4520.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L.-C. Chen，“Mobilenetv2:
    反向残差和线性瓶颈，” *IEEE计算机视觉与模式识别会议论文集*，2018年，第4510–4520页。'
- en: '[78] T. Lei, W. Zhou, Y. Zhang, R. Wang, H. Meng, and A. K. Nandi, “Lightweight
    v-net for liver segmentation,” *IEEE Int. Conf. Acoust. Speech Signal Process.
    (ICASSP)*, pp. 1379–1383, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Lei, W. Zhou, Y. Zhang, R. Wang, H. Meng, 和 A. K. Nandi，“轻量级V-Net用于肝脏分割，”
    *IEEE声学、语音与信号处理国际会议 (ICASSP)*，第1379–1383页，2020年。'
- en: '[79] C. Huang, H. Han, Q. Yao, S. Zhu, and S. K. Zhou, “3d u net: A 3d universal
    u-net for multi-domain medical image segmentation,” *Proc. Int. Conf. Med. Image
    Comput. Comput. Assist. Intervent. (MICCAI)*, pp. 291–299, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Huang, H. Han, Q. Yao, S. Zhu, 和 S. K. Zhou，“3d u net: 一个用于多领域医学图像分割的3D通用U-Net，”
    *医学图像计算与计算机辅助干预国际会议论文集 (MICCAI)*，第291–299页，2019年。'
- en: '[80] M. Paschali, S. Gasperini, A. G. Roy, M. Y.-S. Fang, and N. Navab, “3dq:
    Compact quantized neural networks for volumetric whole brain segmentation,” *arXiv
    preprint arXiv:1904.03110*, pp. 438–446, 2019.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Paschali, S. Gasperini, A. G. Roy, M. Y.-S. Fang, 和 N. Navab，“3dq:
    紧凑量化神经网络用于体积全脑分割，” *arXiv 预印本 arXiv:1904.03110*，第438–446页，2019年。'
- en: '[81] X. Xu, Q. Lu, L. Yang, S. Hu, D. Chen, Y. Hu, and Y. Shi, “Quantization
    of fully convolutional networks for accurate biomedical image segmentation,” *Proc.
    the IEEE Conf. Comput. Vis. Pattern Recognitit. (CVPR)*, pp. 8300–8308, 2018.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] X. Xu, Q. Lu, L. Yang, S. Hu, D. Chen, Y. Hu, 和 Y. Shi，“全卷积网络的量化用于准确的生物医学图像分割，”
    *IEEE计算机视觉与模式识别会议论文集 (CVPR)*，第8300–8308页，2018年。'
- en: '[82] M. Jaderberg, K. Simonyan, A. Zisserman *et al.*, “Spatial transformer
    networks,” *in Proc. Adv. Neural Inf. Process. Syst.*, pp. 2017–2025, 2015.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Jaderberg, K. Simonyan, A. Zisserman *等*，“空间变换网络，” *在先进神经信息处理系统会议论文集中*，第2017–2025页，2015年。'
- en: '[83] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
    K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz *et al.*, “Attention u-net: Learning
    where to look for the pancreas,” *arXiv preprint arXiv:1804.03999*, 2018.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
    K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz *等*，“Attention u-net: 学习在哪里寻找胰腺，”
    *arXiv 预印本 arXiv:1804.03999*，2018年。'
- en: '[84] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” *in Proc.
    IEEE Conf. Comput. Vis. Pattern Recogniti.*, pp. 7132–7141, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Hu, L. Shen, 和 G. Sun，“挤压与激励网络，” *在IEEE计算机视觉与模式识别会议论文集中*，第7132–7141页，2018年。'
- en: '[85] C. Kaul, S. Manandhar, and N. Pears, “Focusnet: An attention-based fully
    convolutional network for medical image segmentation,” *Proc. IEEE 16th Int. Symp.
    Biomed. Imag. (ISBI)*, pp. 455–458, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Kaul, S. Manandhar, 和 N. Pears，“Focusnet: 基于注意力的全卷积网络用于医学图像分割，” *IEEE第16届生物医学影像学国际研讨会
    (ISBI)*，第455–458页，2019年。'
- en: '[86] C. Wang, Y. He, Y. Liu, Z. He, R. He, and Z. Sun, “Sclerasegnet: an improved
    u-net model with attention for accurate sclera segmentation,” *Proc. IAPR Int.
    Conf. Biometrics*, pp. 1–8, 2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Wang, Y. He, Y. Liu, Z. He, R. He, 和 Z. Sun，“Sclerasegnet: 一种改进的U-Net模型，具有注意力机制用于准确的巩膜分割，”
    *IAPR生物识别国际会议论文集*，第1–8页，2019年。'
- en: '[87] Z. Wang, N. Zou, D. Shen, and S. Ji, “Non-local u-nets for biomedical
    image segmentation.” *Proc. AAAI Conf. Artif. Intell.*, pp. 6315–6322, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Z. Wang, N. Zou, D. Shen, 和 S. Ji，“用于生物医学图像分割的非局部U-Nets，” *AAAI人工智能会议论文集*，第6315–6322页，2020年。'
- en: '[88] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
    convolutional networks for visual recognition,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 37, no. 9, pp. 1904–1916, 2015.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度卷积网络中的空间金字塔池化用于视觉识别,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, vol. 37, no. 9, pp. 1904–1916, 2015。'
- en: '[89] M. M. Lopez and J. Ventura, “Dilated convolutions for brain tumor segmentation
    in mri scans,” *Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)
    wrokshop*, pp. 253–262, 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. M. Lopez 和 J. Ventura, “用于脑肿瘤分割的膨胀卷积在 mri 扫描中,” *Int. Conf. Med. Image
    Comput. Comput. Assist. Intervent. (MICCAI) workshop*, pp. 253–262, 2017。'
- en: '[90] T. Lei, R. Wang, Y. Zhang, Y. Wan, C. Liu, and A. K. Nandi, “Defed-net:
    Deformable encoder-decoder network for liver and liver tumor segmentation,” *IEEE
    Transactions on Radiation and Plasma Medical Sciences*, p. 10.1109/TRPMS.2021.3059780,
    2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] T. Lei, R. Wang, Y. Zhang, Y. Wan, C. Liu, 和 A. K. Nandi, “Defed-net:
    用于肝脏和肝脏肿瘤分割的可变形编码解码网络,” *IEEE Transactions on Radiation and Plasma Medical Sciences*,
    p. 10.1109/TRPMS.2021.3059780, 2021。'
- en: '[91] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell,
    “Understanding convolution for semantic segmentation,” *Proc. IEEE Winter Conf.
    Appl. Comput. Vis. (WACV)*, pp. 1451–1460, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, 和 G. Cottrell, “理解卷积用于语义分割,”
    *Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV)*, pp. 1451–1460, 2018。'
- en: '[92] L. Yang, Q. Song, Z. Wang, and M. Jiang, “Parsing r-cnn for instance-level
    human analysis,” *Proc. IEEE Conf. Comput. Vis.Pattern Recognit. (CVPR)*, pp.
    364–373, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Yang, Q. Song, Z. Wang, 和 M. Jiang, “用于实例级人体分析的解析 r-cnn,” *Proc. IEEE
    Conf. Comput. Vis.Pattern Recognit. (CVPR)*, pp. 364–373, 2019。'
- en: '[93] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function
    for image segmentation using 3d fully convolutional deep networks,” *Int. Workshop
    Mach. Learn. Med. Imag.*, pp. 379–387, 2017.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. S. M. Salehi, D. Erdogmus, 和 A. Gholipour, “用于图像分割的 Tversky 损失函数使用
    3d 完全卷积深度网络,” *Int. Workshop Mach. Learn. Med. Imag.*, pp. 379–387, 2017。'
- en: '[94] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. J. Cardoso, “Generalised
    dice overlap as a deep learning loss function for highly unbalanced segmentations.”   Springer,
    2017, pp. 240–248.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, 和 M. J. Cardoso, “广义
    dice 重叠作为深度学习损失函数用于高度不平衡分割。” Springer, 2017, pp. 240–248。'
- en: '[95] H. Kervadec, J. Bouchtiba, C. Desrosiers, E. Granger, J. Dolz, and I. B.
    Ayed, “Boundary loss for highly unbalanced segmentation,” *arXiv preprint arXiv:1812.07032*,
    pp. 285–296, 2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Kervadec, J. Bouchtiba, C. Desrosiers, E. Granger, J. Dolz, 和 I. B.
    Ayed, “边界损失用于高度不平衡的分割,” *arXiv preprint arXiv:1812.07032*, pp. 285–296, 2019。'
- en: '[96] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” *Proc. IEEE Int. Conf. Comput. Vis.*, pp. 2980–2988,
    2017.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T.-Y. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár, “用于密集目标检测的焦点损失,”
    *Proc. IEEE Int. Conf. Comput. Vis.*, pp. 2980–2988, 2017。'
- en: '[97] K. C. Wong, M. Moradi, H. Tang, and T. Syeda-Mahmood, “3d segmentation
    with exponential logarithmic loss for highly unbalanced object sizes,” *Proc.
    Int. Conf. Med. Image Comput. Comput.-Assist. Intervent.*, pp. 612–619, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. C. Wong, M. Moradi, H. Tang, 和 T. Syeda-Mahmood, “使用指数对数损失进行 3d 分割以应对高度不平衡的对象尺寸,”
    *Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent.*, pp. 612–619,
    2018。'
- en: '[98] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams,
    and Y. Zheng, “Learning active contour models for medical image segmentation,”
    *Proc. the IEEE Conf. Comput. Vis. Pattern Recognitit.*, pp. 11 632–11 640, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams,
    和 Y. Zheng, “学习主动轮廓模型用于医学图像分割,” *Proc. the IEEE Conf. Comput. Vis. Pattern Recognit.*,
    pp. 11 632–11 640, 2019。'
- en: '[99] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, and P.-A. Heng, “Transformation-consistent
    self-ensembling model for semisupervised medical image segmentation,” *arXiv preprint
    arXiv:1903.00348*, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, 和 P.-A. Heng, “变换一致的自我集成模型用于半监督医学图像分割,”
    *arXiv preprint arXiv:1903.00348*, 2020。'
- en: '[100] D. Karimi and S. E. Salcudean, “Reducing the hausdorff distance in medical
    image segmentation with convolutional neural networks,” *IEEE Trans. Med. Imag.*,
    vol. 39, no. 2, pp. 499–513, 2019.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Karimi 和 S. E. Salcudean, “使用卷积神经网络减少医学图像分割中的 Hausdorff 距离,” *IEEE
    Trans. Med. Imag.*, vol. 39, no. 2, pp. 499–513, 2019。'
- en: '[101] S. A. Taghanaki, Y. Zheng, S. K. Zhou, B. Georgescu, P. Sharma, D. Xu,
    D. Comaniciu, and G. Hamarneh, “Combo loss: Handling input and output imbalance
    in multi-organ segmentation,” *Comput. Med. Imag. Graph.*, vol. 75, pp. 24–33,
    2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. A. Taghanaki, Y. Zheng, S. K. Zhou, B. Georgescu, P. Sharma, D. Xu,
    D. Comaniciu, 和 G. Hamarneh, “组合损失: 处理多脏器分割中的输入和输出不平衡,” *Comput. Med. Imag. Graph.*,
    vol. 75, pp. 24–33, 2019。'
- en: '[102] F. Caliva, C. Iriondo, A. M. Martinez, S. Majumdar, and V. Pedoia, “Distance
    map loss penalty term for semantic segmentation,” *arXiv preprint arXiv:1908.03679*,
    2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] F. Caliva, C. Iriondo, A. M. Martinez, S. Majumdar, 和 V. Pedoia，“用于语义分割的距离图损失惩罚项，”
    *arXiv预印本 arXiv:1908.03679*，2019。'
- en: '[103] Q. Dou, L. Yu, H. Chen, Y. Jin, X. Yang, J. Qin, and P.-A. Heng, “3d
    deeply supervised network for automated segmentation of volumetric medical images,”
    *Med. Image Anal.*, vol. 41, pp. 40–54, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Q. Dou, L. Yu, H. Chen, Y. Jin, X. Yang, J. Qin, 和 P.-A. Heng，“用于自动分割体积医学图像的3D深度监督网络，”
    *医学图像分析*，第41卷，页码 40–54, 2017。'
- en: '[104] H. Dou, D. Karimi, C. K. Rollins, C. M. Ortinau, L. Vasung, C. Velasco-Annis,
    A. Ouaalam, X. Yang, D. Ni, and A. Gholipour, “A deep attentive convolutional
    neural network for automatic cortical plate segmentation in fetal mri,” *arXiv
    preprint arXiv:2004.12847*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Dou, D. Karimi, C. K. Rollins, C. M. Ortinau, L. Vasung, C. Velasco-Annis,
    A. Ouaalam, X. Yang, D. Ni, 和 A. Gholipour，“用于胎儿MRI的自动皮层板分割的深度注意卷积神经网络，” *arXiv预印本
    arXiv:2004.12847*，2020。'
- en: '[105] K. Sirinukunwattana, J. P. Pluim, H. Chen, X. Qi, P.-A. Heng, Y. B. Guo,
    L. Y. Wang, B. J. Matuszewski, E. Bruni, U. Sanchez *et al.*, “Gland segmentation
    in colon histology images: The glas challenge contest,” *Med. Image Anal.*, vol. 35,
    pp. 489–502, 2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. Sirinukunwattana, J. P. Pluim, H. Chen, X. Qi, P.-A. Heng, Y. B. Guo,
    L. Y. Wang, B. J. Matuszewski, E. Bruni, U. Sanchez *等*，“结肠组织学图像中的腺体分割：GLAS挑战赛，”
    *医学图像分析*，第35卷，页码 489–502, 2017。'
- en: '[106] H. Dong, G. Yang, F. Liu, Y. Mo, and Y. Guo, “Automatic brain tumor detection
    and segmentation using u-net based fully convolutional networks,” *Proc. Ann.
    Conf. Med. Image Underst. Anal. (MIUA)*, pp. 506–517, 2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] H. Dong, G. Yang, F. Liu, Y. Mo, 和 Y. Guo，“使用基于U-Net的全卷积网络进行自动脑肿瘤检测和分割，”
    *医学图像理解与分析年会（MIUA）会议录*，页码 506–517, 2017。'
- en: '[107] J. T. Guibas, T. S. Virdi, and P. S. Li, “Synthetic medical images from
    dual generative adversarial networks,” *arXiv preprint arXiv:1709.01872*, 2017.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] J. T. Guibas, T. S. Virdi, 和 P. S. Li，“来自双生成对抗网络的合成医学图像，” *arXiv预印本 arXiv:1709.01872*，2017。'
- en: '[108] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
    *arXiv preprint arXiv:1411.1784*, 2014.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. Mirza 和 S. Osindero，“条件生成对抗网络，” *arXiv预印本 arXiv:1411.1784*，2014。'
- en: '[109] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, and M. Reyes, “Efficient
    active learning for image classification and segmentation using a sample selection
    and conditional generative adversarial network,” *Proc. Int. Conf. Med. Image
    Comput. Comput. Assist. Intervent. (MICCAI)*, pp. 580–588, 2018.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, 和 M. Reyes，“利用样本选择和条件生成对抗网络进行图像分类和分割的高效主动学习，”
    *医学图像计算与计算机辅助手术国际会议（MICCAI）会议录*，页码 580–588, 2018。'
- en: '[110] H.-C. Shin, N. A. Tenenholtz, J. K. Rogers, C. G. Schwarz, M. L. Senjem,
    J. L. Gunter, K. P. Andriole, and M. Michalski, “Medical image synthesis for data
    augmentation and anonymization using generative adversarial networks,” *Proc.
    Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)*, pp. 1–11,
    2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H.-C. Shin, N. A. Tenenholtz, J. K. Rogers, C. G. Schwarz, M. L. Senjem,
    J. L. Gunter, K. P. Andriole, 和 M. Michalski，“使用生成对抗网络进行医学图像合成以用于数据增强和匿名化，” *医学图像计算与计算机辅助手术国际会议（MICCAI）会议录*，页码
    1–11, 2018。'
- en: '[111] D. Jin, Z. Xu, Y. Tang, A. P. Harrison, and D. J. Mollura, “Ct-realistic
    lung nodule simulation from 3d conditional generative adversarial networks for
    robust lung segmentation,” *Proc. Int. Conf. Med. Image Comput. Comput. Assist.
    Intervent. (MICCAI)*, pp. 732–740, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] D. Jin, Z. Xu, Y. Tang, A. P. Harrison, 和 D. J. Mollura，“基于3D条件生成对抗网络的CT真实感肺结节模拟用于稳健的肺部分
    segmentation，” *医学图像计算与计算机辅助手术国际会议（MICCAI）会议录*，页码 732–740, 2018。'
- en: '[112] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” *Proc. the IEEE Conf.
    on Comput. Vis. (ICCV)*, pp. 2223–2232, 2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros，“使用循环一致对抗网络的无配对图像到图像翻译，”
    *IEEE计算机视觉大会（ICCV）会议录*，页码 2223–2232, 2017。'
- en: '[113] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan: Unified
    generative adversarial networks for multi-domain image-to-image translation,”
    *Proc. the IEEE Conf. Comput. Vis. Pattern Recognitit. (CVPR)*, pp. 8789–8797,
    2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, 和 J. Choo，“Stargan：用于多领域图像到图像翻译的统一生成对抗网络，”
    *IEEE计算机视觉与模式识别大会（CVPR）会议录*，页码 8789–8797, 2018。'
- en: '[114] A. A. Kalinin, V. I. Iglovikov, A. Rakhlin, and A. A. Shvets, “Medical
    image segmentation using deep neural networks with pre-trained encoders.”   Springer,
    2020, pp. 39–52.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. A. Kalinin, V. I. Iglovikov, A. Rakhlin, 和 A. A. Shvets，“使用预训练编码器的深度神经网络进行医学图像分割。”
     Springer, 2020, 页码 39–52。'
- en: '[115] P.-H. Conze, S. Brochard, V. Burdin, F. T. Sheehan, and C. Pons, “Healthy
    versus pathological learning transferability in shoulder muscle mri segmentation
    using deep convolutional encoder-decoders,” *Comput. Med. Imaging Graph*, p. 101733,
    2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] P.-H. Conze, S. Brochard, V. Burdin, F. T. Sheehan 和 C. Pons，“健康与病理学习转移在肩部肌肉
    MRI 分割中的应用”，*计算机医学成像与图形*，第101733页，2020年。'
- en: '[116] Y. Huo, Z. Xu, S. Bao, A. Assad, R. G. Abramson, and B. A. Landman, “Adversarial
    synthesis learning enables segmentation without target modality ground truth,”
    *Proc. IEEE 15th Int. Symp. Biomed. Imag. (ISBI)*, pp. 1217–1220, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Huo, Z. Xu, S. Bao, A. Assad, R. G. Abramson 和 B. A. Landman，“对抗合成学习实现无目标模态地面实况分割”，*IEEE
    第15届生物医学成像国际研讨会 (ISBI)*，第1217–1220页，2018年。'
- en: '[117] C. Chen, Q. Dou, H. Chen, J. Qin, and P.-A. Heng, “Synergistic image
    and feature adaptation: Towards cross-modality domain adaptation for medical image
    segmentation,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 33, pp. 865–872, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Chen, Q. Dou, H. Chen, J. Qin 和 P.-A. Heng，“协同图像和特征适应：面向医学图像分割的跨模态领域适应”，*AAAI
    人工智能会议论文集*，第33卷，第865–872页，2019年。'
- en: '[118] A. Chartsias, T. Joyce, R. Dharmakumar, and S. A. Tsaftaris, “Adversarial
    image synthesis for unpaired multi-modal cardiac data,” *Int. Workshop Simul.
    Synth. Med. Imag*, pp. 3–13, 2017.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. Chartsias, T. Joyce, R. Dharmakumar 和 S. A. Tsaftaris，“针对未配对多模态心脏数据的对抗图像合成”，*国际医学图像模拟与合成研讨会*，第3–13页，2017年。'
- en: '[119] C. Zhao, A. Carass, J. Lee, Y. He, and J. L. Prince, “Whole brain segmentation
    and labeling from ct using synthetic mr images,” *Int. Workshop Mach. Learn. Med.
    Imag*, pp. 291–298, 2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Zhao, A. Carass, J. Lee, Y. He 和 J. L. Prince，“使用合成 MR 图像进行整体脑分割和标记”，*国际机器学习医学图像研讨会*，第291–298页，2017年。'
- en: '[120] V. V. Valindria, N. Pawlowski, M. Rajchl, I. Lavdas, E. O. Aboagye, A. G.
    Rockall, D. Rueckert, and B. Glocker, “Multi-modal learning from unpaired images:
    Application to multi-organ segmentation in ct and mri,” *Proc. IEEE Winter Conf.
    Appl. Comput. Vis. (WACV)*, pp. 547–556, 2018.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] V. V. Valindria, N. Pawlowski, M. Rajchl, I. Lavdas, E. O. Aboagye, A.
    G. Rockall, D. Rueckert 和 B. Glocker，“从未配对图像中学习多模态：应用于 CT 和 MRI 的多脏器分割”，*IEEE
    冬季计算机视觉应用会议 (WACV)*，第547–556页，2018年。'
- en: '[121] G. Wang, M. A. Zuluaga, W. Li, R. Pratt, P. A. Patel, M. Aertsen, T. Doel,
    A. L. David, J. Deprest, S. Ourselin *et al.*, “Deepigeos: a deep interactive
    geodesic framework for medical image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 41, no. 7, pp. 1559–1572, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] G. Wang, M. A. Zuluaga, W. Li, R. Pratt, P. A. Patel, M. Aertsen, T.
    Doel, A. L. David, J. Deprest, S. Ourselin *等*，“Deepigeos：一种用于医学图像分割的深度交互式测地框架”，*IEEE
    计算机视觉与模式识别汇刊*，第41卷，第7期，第1559–1572页，2018年。'
- en: '[122] G. Wang, W. Li, M. A. Zuluaga, R. Pratt, P. A. Patel, M. Aertsen, T. Doel,
    A. L. David, J. Deprest, S. Ourselin *et al.*, “Interactive medical image segmentation
    using deep learning with image-specific fine tuning,” *IEEE Trans. Med. Imag*,
    vol. 37, no. 7, pp. 1562–1573, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] G. Wang, W. Li, M. A. Zuluaga, R. Pratt, P. A. Patel, M. Aertsen, T.
    Doel, A. L. David, J. Deprest, S. Ourselin *等*，“使用深度学习和图像特定微调的交互式医学图像分割”，*IEEE
    医学成像汇刊*，第37卷，第7期，第1562–1573页，2018年。'
- en: '[123] Y. Y. Boykov and M.-P. Jolly, “Interactive graph cuts for optimal boundary
    & region segmentation of objects in nd images,” *Proc. the IEEE Conf. on Comput.
    Vis. (ICCV)*, vol. 1, pp. 105–112, 2001.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y. Y. Boykov 和 M.-P. Jolly，“用于 ND 图像中对象的最优边界和区域分割的交互式图割”，*IEEE 计算机视觉会议论文集
    (ICCV)*，第1卷，第105–112页，2001年。'
- en: '[124] C. Rother, V. Kolmogorov, and A. Blake, “” grabcut” interactive foreground
    extraction using iterated graph cuts,” *ACM Trans. Graph.*, vol. 23, no. 3, pp.
    309–314, 2004.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] C. Rother, V. Kolmogorov 和 A. Blake，“‘grabcut’ 迭代图割的交互式前景提取”，*ACM 图形学汇刊*，第23卷，第3期，第309–314页，2004年。'
- en: '[125] C. Rupprecht, I. Laina, N. Navab, G. D. Hager, and F. Tombari, “Guide
    me: Interacting with deep networks,” *Proc. Int. Conf. Med. Image Comput. Comput-Assist.
    Intervent*, pp. 8551–8561, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] C. Rupprecht, I. Laina, N. Navab, G. D. Hager 和 F. Tombari，“引导我：与深度网络交互”，*国际医学图像计算与计算机辅助干预会议*，第8551–8561页，2018年。'
- en: '[126] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting multimodal
    medical volumes with cycle-and shape-consistency generative adversarial network,”
    *Proc. IEEE Conf. Comput. Vis. Pattern Recogniti. (CVPR)*, pp. 9242–9251, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Zhang, L. Yang 和 Y. Zheng，“使用循环和形状一致性生成对抗网络翻译和分割多模态医学体积”，*IEEE 计算机视觉与模式识别会议论文集
    (CVPR)*，第9242–9251页，2018年。'
- en: '[127] C. Baur, S. Albarqouni, and N. Navab, “Semi-supervised deep learning
    for fully convolutional networks,” *Proc. Int. Conf. Med. Image Comput. Comput-Assist.
    Intervent.*, pp. 311–319, 2017.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] C. Baur, S. Albarqouni 和 N. Navab，“用于全卷积网络的半监督深度学习”，*国际医学图像计算与计算机辅助干预会议论文集*，页码
    311–319，2017年。'
- en: '[128] A. Chartsias, T. Joyce, G. Papanastasiou, S. Semple, M. Williams, D. Newby,
    R. Dharmakumar, and S. A. Tsaftaris, “Factorised spatial representation learning:
    Application in semi-supervised myocardial segmentation,” *Proc. Int. Conf. Med.
    Image Comput. Comput. Assist. Intervent. (MICCAI)*, pp. 490–498, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Chartsias, T. Joyce, G. Papanastasiou, S. Semple, M. Williams, D.
    Newby, R. Dharmakumar 和 S. A. Tsaftaris，“分解空间表示学习：在半监督心肌分割中的应用”，*国际医学图像计算与计算机辅助干预会议论文集
    (MICCAI)*，页码 490–498，2018年。'
- en: '[129] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca, “Data
    augmentation using learned transformations for one-shot medical image segmentation,”
    *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp. 8543–8553, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag 和 A. V. Dalca，“使用学习的变换进行数据增强以实现一次性医学图像分割”，*IEEE
    计算机视觉与模式识别会议论文集*，页码 8543–8553，2019年。'
- en: '[130] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
    A survey,” *arXiv preprint arXiv:1808.05377*, 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] T. Elsken, J. H. Metzen 和 F. Hutter，“神经网络结构搜索：综述”，*arXiv 预印本 arXiv:1808.05377*，2018年。'
- en: '[131] X. He, K. Zhao, and X. Chu, “Automl: A survey of the state-of-the-art,”
    *Knowledge-Based Systems*, p. 106622, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] X. He, K. Zhao 和 X. Chu，“Automl：最先进技术的综述”，*知识基础系统*，第106622页，2020年。'
- en: '[132] H. Ha, S. Rana, S. Gupta, T. Nguyen, S. Venkatesh *et al.*, “Bayesian
    optimization with unknown search space,” *Neural Inform. Process. Syst*, pp. 11 795–11 804,
    2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. Ha, S. Rana, S. Gupta, T. Nguyen, S. Venkatesh *等*，“具有未知搜索空间的贝叶斯优化”，*神经信息处理系统*，页码
    11,795–11,804，2019年。'
- en: '[133] J. Vanschoren, “Meta-learning: A survey,” *arXiv preprint arXiv:1810.03548*,
    2018.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Vanschoren，“元学习：综述”，*arXiv 预印本 arXiv:1810.03548*，2018年。'
- en: '[134] L.-C. Chen, M. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff, H. Adam,
    and J. Shlens, “Searching for efficient multi-scale architectures for dense image
    prediction,” *Neural Inform. Process. Syst*, vol. 31, pp. 8699–8710, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] L.-C. Chen, M. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff, H.
    Adam 和 J. Shlens，“寻找用于密集图像预测的高效多尺度架构”，*神经信息处理系统*，第31卷，页码 8699–8710，2018年。'
- en: '[135] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei,
    “Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation,”
    *Proc. the IEEE Conf. Comput. Vis. Pattern Recognitit. (CVPR)*, pp. 82–92, 2019.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille 和 L. Fei-Fei，“Auto-deeplab：用于语义图像分割的分层神经网络结构搜索”，*IEEE
    计算机视觉与模式识别会议论文集*，页码 82–92，2019年。'
- en: '[136] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl,
    J. Wasserthal, G. Koehler, T. Norajitra, S. Wirkert *et al.*, “nnu-net: Self-adapting
    framework for u-net-based medical image segmentation,” *arXiv preprint arXiv:1809.10486*,
    2018.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl,
    J. Wasserthal, G. Koehler, T. Norajitra, S. Wirkert *等*，“nnu-net：用于 u-net 基医学图像分割的自适应框架”，*arXiv
    预印本 arXiv:1809.10486*，2018年。'
- en: '[137] Y. Weng, T. Zhou, Y. Li, and X. Qiu, “Nas-unet: Neural architecture search
    for medical image segmentation,” *IEEE Access*, vol. 7, pp. 44 247–44 257, 2019.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Y. Weng, T. Zhou, Y. Li 和 X. Qiu，“Nas-unet：医学图像分割的神经网络结构搜索”，*IEEE Access*，第7卷，页码
    44,247–44,257，2019年。'
- en: '[138] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive
    survey on graph neural networks,” *IEEE Trans. Neural. Netw. Learn Syst*, 2020.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang 和 S. Y. Philip，“关于图神经网络的综合调查”，*IEEE
    交易：神经网络与学习系统*，2020年。'
- en: '[139] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Trans. Neural Netw*, vol. 20, no. 1, pp.
    61–80, 2008.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner 和 G. Monfardini，“图神经网络模型”，*IEEE
    交易：神经网络*，第20卷，第1期，页码 61–80，2008年。'
- en: '[140] H. Gao and S. Ji, “Graph u-nets,” *Nature*, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. Gao 和 S. Ji，“图 u-net”，*自然*，2019年。'
- en: '[141] H. Yang, X. Zhen, Y. Chi, L. Zhang, and X.-S. Hua, “Cpr-gcn: Conditional
    partial-residual graph convolutional network in automated anatomical labeling
    of coronary arteries,” *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pp. 3803–3811, 2020.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Yang, X. Zhen, Y. Chi, L. Zhang 和 X.-S. Hua，“cpr-gcn：用于自动化冠状动脉标注的条件部分残差图卷积网络”，*IEEE/CVF
    计算机视觉与模式识别会议论文集*，页码 3803–3811，2020年。'
- en: '[142] J. Sun, F. Darbeha, M. Zaidi, and B. Wang, “Saunet: Shape attentive u-net
    for interpretable medical image segmentation,” *arXiv preprint arXiv:2001.07645*,
    2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Sun, F. Darbeha, M. Zaidi 和 B. Wang，“Saunet：形状注意力U-Net用于可解释的医学图像分割”，*arXiv
    preprint arXiv:2001.07645*，2020年。'
- en: '[143] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, “Uncertainty and interpretability
    in convolutional neural networks for semantic segmentation of colorectal polyps,”
    *Med. Image Anal.*, vol. 60, p. 101619, 2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] K. Wickstrøm, M. Kampffmeyer 和 R. Jenssen，“卷积神经网络在结直肠息肉的语义分割中的不确定性和可解释性”，*Med.
    Image Anal.*，第60卷，第101619页，2020年。'
- en: '[144] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving
    for simplicity: The all convolutional net,” *arXiv preprint arXiv:1412.6806*,
    2014.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. T. Springenberg, A. Dosovitskiy, T. Brox 和 M. Riedmiller，“追求简单性：全卷积网络”，*arXiv
    preprint arXiv:1412.6806*，2014年。'
- en: '[145] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang, “Diagnose
    like a radiologist: Attention guided convolutional neural network for thorax disease
    classification,” *arXiv preprint arXiv:1801.09927*, 2018.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng 和 Y. Yang，“像放射科医生一样诊断：基于注意力的卷积神经网络用于胸部疾病分类”，*arXiv
    preprint arXiv:1801.09927*，2018年。'
- en: '[146] Z. Tang, K. V. Chuang, C. DeCarli, L.-W. Jin, L. Beckett, M. J. Keiser,
    and B. N. Dugger, “Interpretable classification of alzheimer’s disease pathologies
    with a convolutional neural network pipeline,” *Nat. Commun*, vol. 10, no. 1,
    pp. 1–14, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Tang, K. V. Chuang, C. DeCarli, L.-W. Jin, L. Beckett, M. J. Keiser
    和 B. N. Dugger，“使用卷积神经网络管道对阿尔茨海默病病理进行可解释分类”，*Nat. Commun*，第10卷，第1期，第1–14页，2019年。'
- en: '[147] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    *Proc. the IEEE Conf. on Comput. Vis. (ICCV)*, pp. 618–626, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh 和 D. Batra，“Grad-CAM：通过基于梯度的定位从深度网络获得的可视化解释”，*Proc.
    the IEEE Conf. on Comput. Vis. (ICCV)*，第618–626页，2017年。'
- en: '[148] Z. Zhang, P. Chen, M. McGough, F. Xing, C. Wang, M. Bui, Y. Xie, M. Sapkota,
    L. Cui, J. Dhillon *et al.*, “Pathologist-level interpretable whole-slide cancer
    diagnosis with deep learning,” *Nat. Mach. Intell*, vol. 1, no. 5, pp. 236–245,
    2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Z. Zhang, P. Chen, M. McGough, F. Xing, C. Wang, M. Bui, Y. Xie, M. Sapkota,
    L. Cui, J. Dhillon *等*，“利用深度学习进行病理学家级可解释的全切片癌症诊断”，*Nat. Mach. Intell*，第1卷，第5期，第236–245页，2019年。'
- en: '[149] Q. Dou, Q. Liu, P. A. Heng, and B. Glocker, “Unpaired multi-modal segmentation
    via knowledge distillation,” *IEEE Trans. Med. Imaging*, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Q. Dou, Q. Liu, P. A. Heng 和 B. Glocker，“通过知识蒸馏进行未配对的多模态分割”，*IEEE Trans.
    Med. Imaging*，2020年。'
- en: '[150] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] G. Hinton, O. Vinyals 和 J. Dean，“蒸馏神经网络中的知识”，*arXiv preprint arXiv:1503.02531*，2015年。'
- en: '[151] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T. Leiner,
    M. A. Viergever, and I. Išgum, “Deep learning for multi-task medical image segmentation
    in multiple modalities,” *Proc. Int. Conf. Med. Image Comput. Comput. Assist.
    Intervent. (MICCAI)*, pp. 478–486, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] P. Moeskops, J. M. Wolterink, B. H. van der Velden, K. G. Gilhuijs, T.
    Leiner, M. A. Viergever 和 I. Išgum，“多任务医学图像分割的深度学习：多模态”，*Proc. Int. Conf. Med.
    Image Comput. Comput. Assist. Intervent. (MICCAI)*，第478–486页，2016年。'
- en: '[152] T. Zhou, S. Ruan, and S. Canu, “A review: Deep learning for medical image
    segmentation using multi-modality fusion,” *Array*, vol. 3, p. 100004, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] T. Zhou, S. Ruan 和 S. Canu，“综述：使用多模态融合进行医学图像分割的深度学习”，*Array*，第3卷，第100004页，2019年。'
- en: '[153] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. S. Kirby,
    J. B. Freymann, K. Farahani, and C. Davatzikos, “Advancing the cancer genome atlas
    glioma mri collections with expert segmentation labels and radiomic features,”
    *Nat. Scient. Data*, vol. 4, p. 170117, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. S. Kirby,
    J. B. Freymann, K. Farahani 和 C. Davatzikos，“通过专家分割标签和放射组学特征推进癌症基因组图谱胶质瘤MRI数据集”，*Nat.
    Scient. Data*，第4卷，第170117页，2017年。'
- en: '[154] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T.
    Shinohara, C. Berger, S. M. Ha, M. Rozycki *et al.*, “Identifying the best machine
    learning algorithms for brain tumor segmentation, progression assessment, and
    overall survival prediction in the brats challenge,” *arXiv preprint arXiv:1811.02629*,
    2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T.
    Shinohara, C. Berger, S. M. Ha, M. Rozycki *等*，“在BRATS挑战中识别最佳的机器学习算法用于脑肿瘤分割、进展评估和总体生存预测”，*arXiv
    preprint arXiv:1811.02629*，2018年。'
- en: '[155] O. Maier, B. H. Menze, J. von der Gablentz, L. Häni, M. P. Heinrich,
    M. Liebrand, S. Winzeck, A. Basit, P. Bentley, L. Chen *et al.*, “Isles 2015-a
    public evaluation benchmark for ischemic stroke lesion segmentation from multispectral
    mri,” *Med. Image Anal.*, vol. 35, pp. 250–269, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] O. Maier, B. H. Menze, J. von der Gablentz, L. Häni, M. P. Heinrich,
    M. Liebrand, S. Winzeck, A. Basit, P. Bentley, L. Chen *等*，“Isles 2015——多光谱MRI中缺血性中风病灶分割的公共评估基准”，*医学图像分析*，第35卷，第250-269页，2017年。'
- en: '[156] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak,
    J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich *et al.*, “The kits19 challenge
    data: 300 kidney tumor cases with clinical context, ct semantic segmentations,
    and surgical outcomes,” *arXiv preprint arXiv:1904.00445*, 2019.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak,
    J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich *等*，“kits19挑战数据：300例肾脏肿瘤病例，附临床背景、ct语义分割和手术结果”，*arXiv预印本
    arXiv:1904.00445*，2019年。'
- en: '[157] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.
    Fu, X. Han, P.-A. Heng, J. Hesser *et al.*, “The liver tumor segmentation benchmark
    (lits),” *arXiv preprint arXiv:1901.04056*, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.
    Fu, X. Han, P.-A. Heng, J. Hesser *等*，“肝脏肿瘤分割基准（LITS）”，*arXiv预印本 arXiv:1901.04056*，2019年。'
- en: '[158] A. E. Kavur, M. A. Selver, O. Dicle, M. Barıs, and N. S. Gezer, “Chaos-combined
    (ct-mr) healthy abdominal organ segmentation challenge data,” 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. E. Kavur, M. A. Selver, O. Dicle, M. Barıs, 和 N. S. Gezer，“Chaos-combined（ct-mr）健康腹部器官分割挑战数据”，2019年。'
- en: '[159] W. B. T. N. J. K. M. W. K. Smith, K. Clark, “Data from ct_colonography,”
    *https://doi.org/10.7937/K9/TCIA.2015.NWTESAY1*, 2015.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] W. B. T. N. J. K. M. W. K. Smith, K. Clark，“来自ct_colonography的数据”，*https://doi.org/10.7937/K9/TCIA.2015.NWTESAY1*，2015年。'
- en: '[160] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang *et al.*, “Evaluation of prostate
    segmentation algorithms for mri: the promise12 challenge,” *Med. Image Anal.*,
    vol. 18, no. 2, pp. 359–373, 2014.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken,
    G. Vincent, G. Guillard, N. Birbeck, J. Zhang *等*，“前列腺分割算法的评估：Promise12挑战”，*医学图像分析*，第18卷，第2期，第359-373页，2014年。'
- en: '[161] J. J. Cerrolaza, M. L. Picazo, L. Humbert, Y. Sato, D. Rueckert, M. Á. G.
    Ballester, and M. G. Linguraru, “Computational anatomy for multi-organ analysis
    in medical imaging: A review,” *Med. Image Anal.*, vol. 56, pp. 44–67, 2019.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] J. J. Cerrolaza, M. L. Picazo, L. Humbert, Y. Sato, D. Rueckert, M. Á.
    G. Ballester, 和 M. G. Linguraru，“医学成像中的多器官分析计算解剖学：综述”，*医学图像分析*，第56卷，第44-67页，2019年。'
- en: '[162] M. A. S. T. Heimann, B. V. Ginneken, “Segmentation of the liver 2007
    (sliver07),” *http://www.sliver07.org/*, 2007.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. A. S. T. Heimann, B. V. Ginneken，“肝脏分割2007（sliver07）”，*http://www.sliver07.org/*，2007年。'
- en: '[163] I. France, “3dircadb, 3d image reconstruction for comparison of algorithm
    database,” 2016.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] I. France，“3dircadb，算法数据库的3D图像重建”，2016年。'
- en: '[164] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,
    A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze *et al.*, “A large annotated
    medical image dataset for the development and evaluation of segmentation algorithms,”
    *arXiv preprint arXiv:1902.09063*, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van
    Ginneken, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze *等*，“用于分割算法开发和评估的大型标注医学图像数据集”，*arXiv预印本
    arXiv:1902.09063*，2019年。'
- en: '[165] A. E. Kavur, N. S. Gezer, M. Barış, P.-H. Conze, V. Groza, D. D. Pham,
    S. Chatterjee, P. Ernst, S. Özkan, B. Baydar *et al.*, “Chaos challenge–combined
    (ct-mr) healthy abdominal organ segmentation,” *arXiv preprint arXiv:2001.06535*,
    2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] A. E. Kavur, N. S. Gezer, M. Barış, P.-H. Conze, V. Groza, D. D. Pham,
    S. Chatterjee, P. Ernst, S. Özkan, B. Baydar *等*，“Chaos挑战——组合（ct-mr）健康腹部器官分割”，*arXiv预印本
    arXiv:2001.06535*，2020年。'
- en: '[166] E. B. T. L. L. J. L. R. M. S. H. R. Roth, A. Farag, “Nih pancreas-ct
    dataset,” *http://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU*, 2015.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] E. B. T. L. L. J. L. R. M. S. H. R. Roth, A. Farag，“Nih pancreas-ct数据集”，*http://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU*，2015年。'
- en: '[167] A. Suinesiaputra, P. Medrano-Gracia, B. R. Cowan, and A. A. Young, “Big
    heart data: advancing health informatics through data sharing in cardiovascular
    imaging,” *IEEE J. Biomed. Health Inform.*, vol. 19, no. 4, pp. 1283–1290, 2014.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Suinesiaputra, P. Medrano-Gracia, B. R. Cowan, 和 A. A. Young，“大数据：通过数据共享推进心血管成像中的健康信息学”，*IEEE生物医学健康信息学期刊*，第19卷，第4期，第1283-1290页，2014年。'
- en: '[168] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *et al.*, “The
    lung image database consortium (lidc) and image database resource initiative (idri):
    a completed reference database of lung nodules on ct scans,” *Medical physics*,
    vol. 38, no. 2, pp. 915–931, 2011.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer,
    A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman *等*，“肺部影像数据库联盟（lidc）和影像数据库资源倡议（idri）：一个完整的CT扫描肺结节参考数据库”，*医学物理*，第38卷，第2期，第915–931页，2011年。'
- en: '[169] I. S. Topkaya, H. Erdogan, and F. Porikli, “Counting people by clustering
    person detector outputs,” *2014 11th IEEE International Conference on Advanced
    Video and Signal Based Surveillance (AVSS)*, pp. 313–318, 2014.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] I. S. Topkaya, H. Erdogan, 和 F. Porikli, “通过聚类人员检测器输出进行人员计数”，*2014年第11届IEEE国际高级视频与信号监控会议
    (AVSS)*，pp. 313–318, 2014年。'
- en: '[170] P. J. LaMontagne, T. L. Benzinger, J. C. Morris, S. Keefe, R. Hornbeck,
    C. Xiong, E. Grant, J. Hassenstab, K. Moulder, A. Vlassenko *et al.*, “Oasis-3:
    longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and
    alzheimer disease,” *medRxiv*, 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] P. J. LaMontagne, T. L. Benzinger, J. C. Morris, S. Keefe, R. Hornbeck,
    C. Xiong, E. Grant, J. Hassenstab, K. Moulder, A. Vlassenko *等*，“Oasis-3：用于正常衰老和阿尔茨海默病的纵向神经影像、临床和认知数据集”，*medRxiv*，2019年。'
- en: '[171] M. Kistler, S. Bonaretti, M. Pfahrer, R. Niklaus, and P. Büchler, “The
    virtual skeleton database: an open access repository for biomedical research and
    collaboration,” *J. Med. Internet Res.*, vol. 15, no. 11, p. e245, 2013.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. Kistler, S. Bonaretti, M. Pfahrer, R. Niklaus, 和 P. Büchler, “虚拟骨骼数据库：一个开放访问的生物医学研究和合作库”，*J.
    Med. Internet Res.*，第15卷，第11期，第e245页，2013年。'
- en: '[172] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W.
    Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler *et al.*, “Skin lesion analysis
    toward melanoma detection: A challenge at the 2017 international symposium on
    biomedical imaging (isbi), hosted by the international skin imaging collaboration
    (isic),” *2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)*,
    pp. 168–172, 2018.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S.
    W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler *等*，“皮肤病变分析以检测黑色素瘤：2017年国际生物医学影像学研讨会
    (ISBI)的挑战，由国际皮肤影像合作组织 (ISIC) 主办”，*2018 IEEE第15届国际生物医学影像学研讨会 (ISBI 2018)*，pp. 168–172,
    2018年。'
- en: '[173] A. Hoover, V. Kouznetsova, and M. Goldbaum, “Locating blood vessels in
    retinal images by piecewise threshold probing of a matched filter response,” *IEEE
    Transactions on Medical imaging*, vol. 19, no. 3, pp. 203–210, 2000.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Hoover, V. Kouznetsova, 和 M. Goldbaum, “通过分段阈值探测匹配滤波响应定位视网膜图像中的血管”，*IEEE医学影像学交易*，第19卷，第3期，第203–210页，2000年。'
- en: '[174] Y. Zhang, H. Lai, and W. Yang, “Cascade unet and ch-unet for thyroid
    nodule segmentation and benign and malignant classification,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*.   Springer,
    2020, pp. 129–134.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Y. Zhang, H. Lai, 和 W. Yang, “用于甲状腺结节分割和良恶性分类的Cascade unet和ch-unet”，发表于*国际医学图像计算与计算机辅助干预会议*。
    Springer, 2020, pp. 129–134。'
- en: '[175] B. Zhang, J. Xiao, J. Jiao, Y. Wei, and Y. Zhao, “Affinity attention
    graph neural network for weakly supervised semantic segmentation,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] B. Zhang, J. Xiao, J. Jiao, Y. Wei, 和 Y. Zhao, “用于弱监督语义分割的亲和注意力图神经网络”，*IEEE模式分析与机器智能交易*，2021年。'
- en: '[176] B. Yang, H. Pan, J. Yu, K. Han, and Y. Wang, “Classification of medical
    images with synergic graph convolutional networks,” *2019 IEEE 35th International
    Conference on Data Engineering Workshops (ICDEW)*, pp. 253–258, 2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] B. Yang, H. Pan, J. Yu, K. Han, 和 Y. Wang, “通过协同图卷积网络对医学图像进行分类”，*2019
    IEEE第35届数据工程国际会议研讨会 (ICDEW)*，pp. 253–258, 2019年。'
- en: '[177] S. Y. Shin, S. Lee, I. D. Yun, and K. M. Lee, “Deep vessel segmentation
    by learning graphical connectivity,” *Medical image analysis*, vol. 58, p. 101556,
    2019.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Y. Shin, S. Lee, I. D. Yun, 和 K. M. Lee, “通过学习图形连接进行深度血管分割”，*医学图像分析*，第58卷，第101556页，2019年。'
- en: '[178] Z. Tian, X. Li, Y. Zheng, Z. Chen, Z. Shi, L. Liu, and B. Fei, “Graph-convolutional-network-based
    interactive prostate segmentation in mr images,” *Medical physics*, vol. 47, no. 9,
    pp. 4164–4176, 2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Z. Tian, X. Li, Y. Zheng, Z. Chen, Z. Shi, L. Liu, 和 B. Fei, “基于图卷积网络的MR图像中前列腺交互分割”，*医学物理*，第47卷，第9期，第4164–4176页，2020年。'
- en: '[179] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像值 16x16 个词：用于大规模图像识别的变换器”，*arXiv
    预印本 arXiv:2010.11929*，2020年。'
- en: '[180] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
    and Y. Zhou, “Transunet: Transformers make strong encoders for medical image segmentation,”
    *arXiv preprint arXiv:2102.04306*, 2021.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
    和 Y. Zhou，“Transunet：变换器使医疗图像分割变得更强”，*arXiv 预印本 arXiv:2102.04306*，2021年。'
- en: '[181] Y. Gao, M. Zhou, and D. Metaxas, “Utnet: A hybrid transformer architecture
    for medical image segmentation,” *arXiv preprint arXiv:2107.00781*, 2021.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Gao, M. Zhou, 和 D. Metaxas，“Utnet：一种用于医疗图像分割的混合变换器架构”，*arXiv 预印本 arXiv:2107.00781*，2021年。'
- en: '[182] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical
    transformer: Gated axial-attention for medical image segmentation,” *arXiv preprint
    arXiv:2102.10662*, 2021.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, 和 V. M. Patel，“医疗变换器：用于医疗图像分割的门控轴向注意力”，*arXiv
    预印本 arXiv:2102.10662*，2021年。'
- en: '[183] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, “Swin-unet:
    Unet-like pure transformer for medical image segmentation,” *arXiv preprint arXiv:2105.05537*,
    2021.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, 和 M. Wang，“Swin-unet：类似
    Unet 的纯变换器用于医疗图像分割”，*arXiv 预印本 arXiv:2105.05537*，2021年。'
