- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:39:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.07348] A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.07348] 关于基于深度学习的单目航天器姿态估计的调查：现状、局限性与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.07348](https://ar5iv.labs.arxiv.org/html/2305.07348)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.07348](https://ar5iv.labs.arxiv.org/html/2305.07348)
- en: 1]organization=Interdisciplinary Centre for Security, Reliability and Trust
    (SnT),addressline=University of Luxembourg, city=Luxembourg, postcode=L-1855,state=Luxembourg,country=Luxembourg
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 1]组织=卢森堡大学跨学科安全、可靠性与信任中心（SnT），地址=卢森堡大学，城市=卢森堡，邮政编码=L-1855，州=卢森堡，国家=卢森堡
- en: 'A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于基于深度学习的单目航天器姿态估计的调查：现状、局限性与展望
- en: Leo Pauly leo.pauly@uni.lu    Wassim Rharbaoui wassim.rharbaoui@uni.lu    Carl
    Shneider carl.shneider@uni.lu    Arunkumar Rathinam arunkumar.rathinam@uni.lu
       Vincent Gaudillière vincent.gaudilliere@uni.lu    Djamila Aouada djamila.aouada@uni.lu
    [
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Leo Pauly leo.pauly@uni.lu    Wassim Rharbaoui wassim.rharbaoui@uni.lu    Carl
    Shneider carl.shneider@uni.lu    Arunkumar Rathinam arunkumar.rathinam@uni.lu
       Vincent Gaudillière vincent.gaudilliere@uni.lu    Djamila Aouada djamila.aouada@uni.lu
    [
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Estimating the pose of an uncooperative spacecraft is an important computer
    vision problem for enabling the deployment of automatic vision-based systems in
    orbit, with applications ranging from on-orbit servicing to space debris removal.
    Following the general trend in computer vision, more and more works have been
    focusing on leveraging Deep Learning (DL) methods to address this problem. However
    and despite promising research-stage results, major challenges preventing the
    use of such methods in real-life missions still stand in the way. In particular,
    the deployment of such computation-intensive algorithms is still under-investigated,
    while the performance drop when training on synthetic and testing on real images
    remains to mitigate. The primary goal of this survey is to describe the current
    DL-based methods for spacecraft pose estimation in a comprehensive manner. The
    secondary goal is to help define the limitations towards the effective deployment
    of DL-based spacecraft pose estimation solutions for reliable autonomous vision-based
    applications. To this end, the survey first summarises the existing algorithms
    according to two approaches: hybrid modular pipelines and direct end-to-end regression
    methods. A comparison of algorithms is presented not only in terms of pose accuracy
    but also with a focus on network architectures and models’ sizes keeping potential
    deployment in mind. Then, current monocular spacecraft pose estimation datasets
    used to train and test these methods are discussed. The data generation methods:
    simulators and testbeds, the domain gap and the performance drop between synthetically
    generated and lab/space collected images and the potential solutions are also
    discussed. Finally, the paper presents open research questions and future directions
    in the field, drawing parallels with other computer vision applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 估计非合作航天器的姿态是一个重要的计算机视觉问题，旨在实现基于自动视觉系统的轨道部署，应用范围从轨道服务到太空垃圾清除。随着计算机视觉领域的一般趋势，越来越多的研究工作开始利用深度学习（DL）方法来解决这一问题。然而，尽管有前景的研究阶段结果，阻碍这些方法在实际任务中使用的主要挑战仍然存在。特别是，这些计算密集型算法的部署仍然未得到充分研究，同时在合成图像上训练而在真实图像上测试时性能下降的问题依然存在。本文的主要目标是全面描述当前基于深度学习的航天器姿态估计方法。次要目标是帮助定义在有效部署基于深度学习的航天器姿态估计解决方案以实现可靠的自主视觉应用过程中存在的局限性。为此，调查首先根据两种方法总结了现有算法：混合模块化管道和直接端到端回归方法。对算法的比较不仅在姿态精度方面进行，还关注网络架构和模型大小，以考虑潜在的部署。接着，讨论了用于训练和测试这些方法的当前单目航天器姿态估计数据集。数据生成方法：模拟器和测试平台、领域差距以及合成生成图像与实验室/太空采集图像之间的性能下降及其潜在解决方案也进行了讨论。最后，论文提出了该领域的开放研究问题和未来方向，并与其他计算机视觉应用进行了对比。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Spacecraft Pose Estimation, Algorithms, Deep Learning, Datasets, Simulators
    and Testbeds, Domain adaptation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 航天器姿态估计、算法、深度学习、数据集、模拟器和测试平台、领域适应。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/4150aa09f199bac768bf1dacdb1313eb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4150aa09f199bac768bf1dacdb1313eb.png)'
- en: 'Figure 1: Spacecraft pose estimation is the problem of finding the relative
    position ($t_{BC}$) and orientation ($R_{BC}$) of the target spacecraft reference
    frame (B) shown in red, with respect to the camera reference frame (C) shown in
    blue, mounted on a chaser spacecraft.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：航天器姿态估计是确定目标航天器参考框架（B）相对于安装在追踪航天器上的相机参考框架（C）的相对位置（$t_{BC}$）和方向（$R_{BC}$）的问题，目标框架在图中以红色表示，相机框架以蓝色表示。
- en: In recent years, the number of satellites launched into orbit has increased
    rapidly, aided by lower launch costs and minimal entry barriers, making space
    more accessible than ever before [[65](#bib.bib65), [178](#bib.bib178)]. Each
    space mission has a unique set of goals that influences the satellite’s size,
    functions, and intended lifetime. In most mission scenarios, the satellites launched
    into orbit will last for the entire mission life-cycle, and at the end of life,
    they are either moved to the graveyard orbit or left to re-enter the Earth’s atmosphere.
    However, a few space missions may encounter anomalies or malfunctions before their
    full life span. These malfunctioned satellites may become non-cooperative and
    threaten existing space infrastructure. To tackle such scenarios, the demand for
    orbital missions targeting On-Orbit Servicing (OOS) and Active Debris Removal
    (ADR) has steadily increased, as OOS and ADR are considered key spaceflight capabilities
    for the next decade. OOS is defined as the process of inspection, maintenance,
    and repair of a system as an in-space operation. Commercial OOS missions aim to
    perform various functions, including providing life extension, maintaining the
    spacecraft, rescuing and recovering satellites from deployment failures and assisting
    astronauts with extravehicular activities [[73](#bib.bib73), [83](#bib.bib83)].
    ADR is the process of removing obsolete space objects (such as satellites, rocket
    bodies, or fragments of spacecraft) through an external disposal method, thus
    minimizing the build-up of unnecessary objects and lowering the probability of
    on-orbit collisions that can fuel a “collision cascade” [[176](#bib.bib176), [97](#bib.bib97)].
    Several technology demonstration missions, including PROBA-3 by the European Space
    Agency (ESA) [[89](#bib.bib89)], PRISMA by OHB Sweden [[156](#bib.bib156)], and
    commercial missions such as MEV-1 by Northrop Grumman [[129](#bib.bib129)], had
    been carried out successfully in recent years. Future missions such as Clearspace-1
    by ESA and Clearspace [[11](#bib.bib11)] are already in preparation to demonstrate
    ADR in 2026.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由于发射成本降低和进入门槛减少，发射到轨道上的卫星数量急剧增加，使得太空比以往任何时候都更易于接触 [[65](#bib.bib65), [178](#bib.bib178)]。每个太空任务都有一套独特的目标，这些目标会影响卫星的大小、功能和预期寿命。在大多数任务场景中，发射到轨道上的卫星会持续整个任务生命周期，而在寿命结束时，它们要么被转移到墓地轨道，要么被留在地球大气层重新进入。然而，一些太空任务可能在其完整寿命之前遇到异常或故障。这些故障的卫星可能变得不合作，并威胁到现有的太空基础设施。为了应对这种情况，对针对轨道在轨服务（OOS）和主动碎片清除（ADR）的轨道任务的需求稳步增加，因为OOS和ADR被认为是下一个十年的关键太空飞行能力。OOS定义为对系统进行检查、维护和修理的过程，作为一种在太空操作。商业OOS任务旨在执行各种功能，包括提供寿命延长、维护航天器、从部署失败中拯救和恢复卫星，并协助宇航员进行舱外活动
    [[73](#bib.bib73), [83](#bib.bib83)]。ADR是通过外部处置方法移除过时的太空物体（如卫星、火箭部件或航天器碎片）的过程，从而减少不必要物体的积累，并降低可能引发“碰撞级联”的在轨碰撞的概率
    [[176](#bib.bib176), [97](#bib.bib97)]。近年来，包括欧洲航天局（ESA）的PROBA-3 [[89](#bib.bib89)]、OHB
    Sweden的PRISMA [[156](#bib.bib156)] 和诺斯罗普·格鲁曼的MEV-1 [[129](#bib.bib129)] 在内的多个技术演示任务已成功实施。未来任务如ESA的Clearspace-1
    和Clearspace [[11](#bib.bib11)] 已在准备中，以展示2026年的ADR。
- en: An important aspect of OOS and ADR missions is that it requires rendezvous and
    proximity operations near the target before performing mission-specific operations.
    To perform any rendezvous operations, it is essential to know the target spacecraft’s
    position and orientation (i.e. pose), allowing the relative navigation algorithms
    to generate real-time trajectories onboard the spacecraft. Several sensor options
    are available to perform inference and observation of the target spacecraft state,
    including Monocular RGB/Greyscale Cameras, Stereo Cameras, Thermal cameras, Range
    Detection and Ranging (RADAR), Light Detection and Ranging (LIDAR), etc. Monocular
    cameras are widely preferred over other active sensors (like LIDARs and RADARs)
    due to their relative simplicity, small size, weight, power requirements, and
    ability to be easily integrated into a wide range of spacecraft configurations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: OOS和ADR任务的一个重要方面是，在执行任务特定操作之前，需要在目标附近进行会合和接近操作。为了执行任何会合操作，必须了解目标航天器的位置和方向（即姿态），以使相对导航算法能够在航天器上生成实时轨迹。有多种传感器选项可用于推断和观察目标航天器状态，包括单目RGB/灰度相机、立体相机、热成像相机、距离检测和测距（雷达）、光学测距（激光雷达）等。由于其相对简单、小巧、轻便、功耗低，并且能够轻松集成到各种航天器配置中，单目相机比其他主动传感器（如激光雷达和雷达）更为受欢迎。
- en: 'Recovering the relative pose between a camera and an observed object from a
    single image is a fundamental computer vision problem [[96](#bib.bib96), [107](#bib.bib107),
    [158](#bib.bib158)]. Given an image and the corresponding intrinsic camera parameters,
    the relative pose estimation problem involves estimating the relative transformation,
    i.e. translation and rotation, between the camera and the target object. The location
    of the object in the camera reference frame is specified by $\mathit{t}\in\mathbb{R}^{3}$,
    and its orientation is most often represented by a quaternion $\mathit{q}=(q_{0},q_{1},q_{2},q_{3})\in\mathbb{R}^{4}$.
    The relative orientation (rotation) can also be represented using standard 3D
    rotation representations such as rotation matrix or Euler’s Angles [[61](#bib.bib61)].
    In [Figure 1](#S1.F1 "In 1 Introduction ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects"), a simple
    illustration of the spacecraft pose estimation problem is presented, where axes
    $x_{C},y_{C},z_{C}$ represent the camera reference frame mounted on the chaser
    (C) spacecraft and $x_{B},y_{B},z_{B}$ represent the target spacecraft’s body
    (B) reference frame. Spacecraft pose estimation is the problem of finding the
    relative position ($t_{BC}$) and orientation ($R_{BC}$) of the reference frame
    of a target spacecraft with respect to the reference frame of a camera mounted
    on a chaser spacecraft, using a single image from a monocular camera.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从单张图像中恢复相机与观察对象之间的相对姿态是一个基础的计算机视觉问题[[96](#bib.bib96), [107](#bib.bib107), [158](#bib.bib158)]。给定图像及其对应的相机内参，相对姿态估计问题涉及估计相机与目标对象之间的相对变换，即平移和旋转。对象在相机参考系中的位置由$\mathit{t}\in\mathbb{R}^{3}$指定，其方向通常由四元数$\mathit{q}=(q_{0},q_{1},q_{2},q_{3})\in\mathbb{R}^{4}$表示。相对方向（旋转）也可以使用标准的3D旋转表示方法，如旋转矩阵或欧拉角[[61](#bib.bib61)]。在[图1](#S1.F1
    "在 1 引言 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性与前景")中，展示了航天器姿态估计问题的一个简单示意图，其中$x_{C},y_{C},z_{C}$表示安装在追逐者（C）航天器上的相机参考系，$x_{B},y_{B},z_{B}$表示目标航天器的主体（B）参考系。航天器姿态估计是指在单目相机拍摄的图像基础上，找到目标航天器参考系相对于安装在追逐者航天器上的相机参考系的相对位置($t_{BC}$)和方向($R_{BC}$)的问题。
- en: In the last decade, vision-based spacecraft pose estimation has utilized hand-engineered
    features described using feature descriptors and detected using feature detectors
    to detect these features in the 2D images and to finally use their 3D correspondences
    to find the relative pose [[66](#bib.bib66), [30](#bib.bib30)]. Although the use
    of feature correspondences between the detected features in the 2D image and 3D
    feature locations, together with perspective transformation, aids in pose solution
    convergence, the features are not robust to harsh lighting conditions encountered
    in space. The feature-based approaches perform poorly in variable illumination
    conditions, low signal-to-noise ratio, and high contrast characteristics encountered
    in space imagery. This results in a poor estimation of the target state in many
    scenarios. Spacecraft pose estimation before the evolution of deep learning algorithms
    has been summarised in [[20](#bib.bib20), [105](#bib.bib105)]. With their gain
    in popularity and exponential growth, Deep Learning (DL)-based approaches have
    prompted many new developments in recent years. According to the findings of the
    recent ESA’s Spacecraft Pose Estimation Challenges [[71](#bib.bib71), [112](#bib.bib112)],
    DL-based methods have been the preferred option for tackling the problem of uncooperative
    spacecraft pose estimation. However, investigated DL-based approaches still heavily
    rely on annotated data that are cumbersome to obtain. While synthetic data generation
    and laboratory data acquisition have been identified as the most tractable way
    to train and test such algorithms, the performance drops significantly on the
    test image domain compared to the train image domain, such problem being known
    as the domain gap [[168](#bib.bib168)]. Dedicated strategies have therefore to
    be investigated to mitigate it. In addition, the laboratory conditions under which
    test images are acquired still differ from space-borne conditions, adding another
    level of domain discrepancy that is yet to be addressed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，基于视觉的航天器姿态估计利用了通过特征描述符描述并通过特征检测器检测的手工工程特征，以在2D图像中检测这些特征，并最终使用其3D对应关系来找到相对姿态[[66](#bib.bib66),
    [30](#bib.bib30)]。尽管在2D图像中检测到的特征与3D特征位置之间的特征对应关系，加上透视变换，有助于姿态解的收敛，但这些特征在太空中遇到的恶劣光照条件下并不稳健。基于特征的方法在光照条件变化大、信噪比低和高对比度特征的太空图像中表现不佳。这导致在许多情况下对目标状态的估计不准确。在深度学习算法发展之前的航天器姿态估计总结见[[20](#bib.bib20),
    [105](#bib.bib105)]。随着其受欢迎程度的提升和指数级增长，基于深度学习（DL）的方法在近年来催生了许多新的发展。根据最近ESA的航天器姿态估计挑战的发现[[71](#bib.bib71),
    [112](#bib.bib112)]，基于DL的方法已成为解决非合作航天器姿态估计问题的首选。然而，研究中的DL方法仍然严重依赖难以获得的标注数据。虽然合成数据生成和实验室数据采集被确定为训练和测试这些算法的最可行方式，但与训练图像域相比，测试图像域上的性能显著下降，这种问题被称为域间差距[[168](#bib.bib168)]。因此，需要研究专门的策略来减轻这一问题。此外，测试图像获取的实验室条件仍与太空条件有所不同，增加了尚待解决的另一个域间差异。
- en: A recent survey on the DL-based approaches for spacecraft relative navigation
    [[151](#bib.bib151)] provides a general narrative across different use cases,
    including spacecraft pose estimation. In this survey, we focus on monocular pose
    estimation of non-cooperative targets using DL approaches and review the latest
    developments in the field. In addition, we conduct a comparison between the two
    main types of approaches and assess the still unmet needs that would enable the
    deployment of DL-based algorithms in real space missions. Furthermore, we explore
    the fundamental counterpart of any DL-based algorithm that is the data. We review
    the existing datasets, generation engines and testbed facilities. We also analyse
    the current validation procedure that consists in testing on laboratory-acquired
    images algorithms trained on synthetic data, after discussing the methods proposed
    to address this domain gap. Finally, we provide the reader with prospects on research
    directions that could help making the leap to the deployment of reliable DL-based
    spacecraft pose estimation algorithms for autonomous in-orbit operations. Note
    that we mainly considered the works published until Dec 2022 for this survey.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项关于基于深度学习（DL）的方法在航天器相对导航中的调查[[151](#bib.bib151)]提供了不同使用案例的一般叙述，包括航天器姿态估计。在这项调查中，我们重点关注使用DL方法对非合作目标进行单目姿态估计，并回顾了该领域的最新进展。此外，我们对两种主要类型的方法进行了比较，并评估了尚未满足的需求，这些需求将使DL算法能够在实际太空任务中部署。进一步地，我们探讨了任何DL算法的基本组成部分——数据。我们回顾了现有的数据集、生成引擎和测试平台。我们还分析了当前的验证程序，该程序包括在合成数据上训练的算法在实验室获取的图像上的测试，并讨论了解决这一领域差距的方法。最后，我们向读者提供了关于研究方向的展望，这些方向可能有助于实现可靠的DL基础航天器姿态估计算法在自主轨道操作中的部署。请注意，我们主要考虑了截至2022年12月发布的相关工作。
- en: 'The following sections are organized as follows. Section [2](#S2 "2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects") provides a comprehensive survey of the two
    main types of DL-based algorithms for spacecraft pose estimation, before highlighting
    their limitations. Section [3](#S3 "3 Datasets ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")
    presents the datasets, generation engines and testbed facilities. It also presents
    the main existing methods to address the domain gap between synthetic and laboratory
    images, and discuss the underlying validation procedure. Section [4](#S4 "4 Future
    Research Directions ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose
    Estimation: Current State, Limitations and Prospects") discusses open research
    problems and future directions and finally, Section [5](#S5 "5 Conclusions ‣ A
    Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State,
    Limitations and Prospects") concludes the survey.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '以下各节组织如下。第[2](#S2 "2 Algorithms ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")节全面调查了两种主要的基于DL的航天器姿态估计算法，并突出了它们的局限性。第[3](#S3
    "3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")节介绍了数据集、生成引擎和测试平台，并提出了主要现有的方法来解决合成图像和实验室图像之间的领域差距，讨论了基础的验证程序。第[4](#S4
    "4 Future Research Directions ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")节讨论了开放的研究问题和未来方向，最后，第[5](#S5
    "5 Conclusions ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")节总结了调查结果。'
- en: <svg   height="353.05" overflow="visible" version="1.1" width="1044.66"><g transform="translate(0,353.05)
    matrix(1 0 0 -1 0 0) translate(179.53,0) translate(0,131.27)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -43.8 -37.36)" fill="#000000"
    stroke="#000000"><foreignobject width="87.6" height="74.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Spacecraft |
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="353.05" overflow="visible" version="1.1" width="1044.66"><g transform="translate(0,353.05)
    matrix(1 0 0 -1 0 0) translate(179.53,0) translate(0,131.27)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -43.8 -37.36)" fill="#000000"
    stroke="#000000"><foreignobject width="87.6" height="74.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 航天器 |
- en: '| Pose |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 姿态 |'
- en: '| Estimation |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 41.21
    61.07)" fill="#000000" stroke="#000000"><foreignobject width="75.07" height="74.72"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Hybrid |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 估计 |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 41.21 61.07)"
    fill="#000000" stroke="#000000"><foreignobject width="75.07" height="74.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 混合 |'
- en: '| Modular |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 模块化 |'
- en: '| Approach |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 68.89
    140.2)" fill="#000000" stroke="#000000"><foreignobject width="791.36" height="78.09"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| - Multi-stage detectors
    [[23](#bib.bib23), [58](#bib.bib58), [126](#bib.bib126)] |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 68.89 140.2)"
    fill="#000000" stroke="#000000"><foreignobject width="791.36" height="78.09" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| - 多阶段检测器 [[23](#bib.bib23), [58](#bib.bib58),
    [126](#bib.bib126)] |'
- en: '| - Single-stage detectors [[114](#bib.bib114), [60](#bib.bib60), [120](#bib.bib120),
    [12](#bib.bib12), [29](#bib.bib29), [92](#bib.bib92), [81](#bib.bib81), [172](#bib.bib172)]
    |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 135.59 48.61)" fill="#000000"
    stroke="#000000"><foreignobject width="594.95" height="99.63" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| - Regression of keypoint locations [[58](#bib.bib58),
    [115](#bib.bib115), [92](#bib.bib92)] |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| - 单阶段检测器 [[114](#bib.bib114), [60](#bib.bib60), [120](#bib.bib120), [12](#bib.bib12),
    [29](#bib.bib29), [92](#bib.bib92), [81](#bib.bib81), [172](#bib.bib172)] |</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 135.59 48.61)" fill="#000000" stroke="#000000"><foreignobject
    width="594.95" height="99.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    - 关键点位置回归 [[58](#bib.bib58), [115](#bib.bib115), [92](#bib.bib92)] |'
- en: '| - Segmentation-driven approach [[41](#bib.bib41), [57](#bib.bib57), [75](#bib.bib75)]
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| - 基于分割的方法 [[41](#bib.bib41), [57](#bib.bib57), [75](#bib.bib75)] |'
- en: '| - Heatmap prediction [[23](#bib.bib23), [126](#bib.bib126), [60](#bib.bib60),
    [120](#bib.bib120), [29](#bib.bib29)] |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| - 热图预测 [[23](#bib.bib23), [126](#bib.bib126), [60](#bib.bib60), [120](#bib.bib120),
    [29](#bib.bib29)] |'
- en: '| - Bounding box prediction [[81](#bib.bib81), [172](#bib.bib172)] |</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 112.16 -17.36)" fill="#000000" stroke="#000000"><foreignobject
    width="728.44" height="78.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    - PnP solver [[23](#bib.bib23), [41](#bib.bib41), [115](#bib.bib115), [60](#bib.bib60),
    [120](#bib.bib120), [58](#bib.bib58), [126](#bib.bib126), [12](#bib.bib12), [57](#bib.bib57),
    [92](#bib.bib92), [81](#bib.bib81), [172](#bib.bib172)] |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| - 边界框预测 [[81](#bib.bib81), [172](#bib.bib172)] |</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 112.16 -17.36)" fill="#000000" stroke="#000000"><foreignobject
    width="728.44" height="78.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    - PnP求解器 [[23](#bib.bib23), [41](#bib.bib41), [115](#bib.bib115), [60](#bib.bib60),
    [120](#bib.bib120), [58](#bib.bib58), [126](#bib.bib126), [12](#bib.bib12), [57](#bib.bib57),
    [92](#bib.bib92), [81](#bib.bib81), [172](#bib.bib172)] |'
- en: '| - Learning-based method [[75](#bib.bib75)] |</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 174.18 152.26)" fill="#000000" stroke="#000000"><foreignobject width="84.71"
    height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Spacecraft
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| - 基于学习的方法 [[75](#bib.bib75)] |</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 174.18 152.26)" fill="#000000" stroke="#000000"><foreignobject width="84.71"
    height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| 航天器 |'
- en: '| localisation |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0
    176.98 73.52)" fill="#000000" stroke="#000000"><foreignobject width="79.1" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Keypoint |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 定位 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 176.98 73.52)"
    fill="#000000" stroke="#000000"><foreignobject width="79.1" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 关键点 |'
- en: '| Prediction |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 168.26
    -5.22)" fill="#000000" stroke="#000000"><foreignobject width="96.55" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Pose |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 预测 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 168.26 -5.22)"
    fill="#000000" stroke="#000000"><foreignobject width="96.55" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 姿态 |'
- en: '| Computation |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -174.08
    -124.99)" fill="#000000" stroke="#000000"><foreignobject width="702.5" height="101.76"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Direct End- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 计算 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -174.08 -124.99)"
    fill="#000000" stroke="#000000"><foreignobject width="702.5" height="101.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 直接终端- |'
- en: '| to-end |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 到终端 |'
- en: '| Approach[[145](#bib.bib145), [146](#bib.bib146), [110](#bib.bib110), [124](#bib.bib124),
    [121](#bib.bib121), [38](#bib.bib38), [59](#bib.bib59), [119](#bib.bib119)] |</foreignobject></g></g></g></svg>'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '| 方法[[145](#bib.bib145), [146](#bib.bib146), [110](#bib.bib110), [124](#bib.bib124),
    [121](#bib.bib121), [38](#bib.bib38), [59](#bib.bib59), [119](#bib.bib119)] |</foreignobject></g></g></g></svg>'
- en: 'Figure 2: Tree diagram of spacecraft pose estimation algorithms reviewed in
    this paper. Blue boxes show the two different categories of approaches: hybrid
    modular and direct end-to-end. The yellow boxes and the sub-branches (grey boxes)
    show the separate stages and the different methods used at each stage, respectively,
    of the hybrid modular approach.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：航天器姿态估计算法的树状图，回顾了本文中的算法。蓝色框显示了两种不同类别的方法：混合模块化和直接端到端。黄色框和子分支（灰色框）分别展示了混合模块化方法中的各个阶段和每个阶段使用的不同方法。
- en: 2 Algorithms
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 算法
- en: 'The use of DL has had significant implications in developing computer vision
    algorithms over the last decade  [[166](#bib.bib166)],  [[22](#bib.bib22)], improving
    their performance and robustness for applications such as image classification
    [[173](#bib.bib173)], segmentation [[99](#bib.bib99)], and object tracking [[26](#bib.bib26)].
    Following this trend, the proposals of DL-based spacecraft pose estimation algorithms
    have outnumbered [[151](#bib.bib151)],  [[20](#bib.bib20)] the classical feature-engineering-based
    methods [[32](#bib.bib32), [148](#bib.bib148), [87](#bib.bib87), [147](#bib.bib147),
    [135](#bib.bib135), [19](#bib.bib19)] in recent years. [Figure 2](#S1.F2 "In 1
    Introduction ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects") presents an overview tree diagram of
    the algorithms reviewed in this survey and [Figure 3](#S2.F3 "In 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects") shows their branching into different approaches.
    DL-based spacecraft pose estimation algorithms broadly fall under two categories:
    1) Hybrid modular approaches, and 2) Direct end-to-end approaches.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年里，深度学习（DL）的使用对计算机视觉算法的发展产生了重大影响[[166](#bib.bib166)], [[22](#bib.bib22)]，提升了其在图像分类[[173](#bib.bib173)]、分割[[99](#bib.bib99)]和目标跟踪[[26](#bib.bib26)]等应用中的性能和鲁棒性。跟随这一趋势，基于DL的航天器姿态估计算法的提案数量已经超过了[[151](#bib.bib151)],
    [[20](#bib.bib20)]传统的特征工程方法[[32](#bib.bib32), [148](#bib.bib148), [87](#bib.bib87),
    [147](#bib.bib147), [135](#bib.bib135), [19](#bib.bib19)]。 [图 2](#S1.F2 "在 1 介绍
    ‣ 深度学习单目航天器姿态估计的调查：现状、局限性和前景") 展示了本调查中回顾的算法的概述树状图，[图 3](#S2.F3 "在 2 算法 ‣ 深度学习单目航天器姿态估计的调查：现状、局限性和前景")
    显示了这些算法分支为不同的方法。基于DL的航天器姿态估计算法大致分为两类：1）混合模块化方法和2）直接端到端方法。
- en: 'Hybrid modular approaches (see [Figure 4](#S2.F4 "In 2 Algorithms ‣ A Survey
    on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations
    and Prospects")-A) combine multiple DL models and classical computer vision methods
    for spacecraft pose estimation. On the other hand, direct end-to-end approaches
    (see [Figure 4](#S2.F4 "In 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")-B) only
    use a single DL model for pose estimation, trained end-to-end. Each of these approaches
    are discussed in detail ([Section 2.1](#S2.SS1 "2.1 Hybrid Modular Approaches
    ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects") and [Section 2.2](#S2.SS2 "2.2 Direct
    End-to-end Approaches ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")), with
    a comparative analysis ([Section 2.3](#S2.SS3 "2.3 Algorithm Comparison ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects")) and a discussion on limitations ([Section 2.4](#S2.SS4
    "2.4 Limitations ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")) below.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模块化方法（见 [图4](#S2.F4 "在2种算法 ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景")-A）结合了多个深度学习模型和经典计算机视觉方法用于航天器姿态估计。另一方面，直接端到端方法（见
    [图4](#S2.F4 "在2种算法 ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景")-B）仅使用单一的深度学习模型进行姿态估计，经过端到端训练。这些方法的详细讨论在([第2.1节](#S2.SS1
    "2.1 混合模块化方法 ‣ 2种算法 ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景")和 [第2.2节](#S2.SS2 "2.2 直接端到端方法
    ‣ 2种算法 ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景"))，并有比较分析 ([第2.3节](#S2.SS3 "2.3 算法比较 ‣ 2种算法
    ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景")) 和对限制的讨论 ([第2.4节](#S2.SS4 "2.4 限制 ‣ 2种算法 ‣ 基于深度学习的单目航天器姿态估计：现状、限制与前景"))。
- en: '![Refer to caption](img/860f03ae12ea3d00b3c7e86f597302ff.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/860f03ae12ea3d00b3c7e86f597302ff.png)'
- en: 'Figure 3: Distribution of algorithms surveyed in this paper'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：本文调查的算法分布
- en: '![Refer to caption](img/8523f610504f13498bbaefaf22e30b2b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8523f610504f13498bbaefaf22e30b2b.png)'
- en: 'Figure 4: Illustration of different approaches for spacecraft pose estimation.
    A) Direct end-to-end approaches which use deep learning. B) Hybrid modular approaches
    which consist of three steps: object detection/localisation, keypoint regression,
    and pose computation. The first two steps use deep learning and the third step
    uses a classical algorithm which performs outlier removal necessary for the PnP
    solver and finally pose refinement.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：航天器姿态估计不同方法的示意图。A) 使用深度学习的直接端到端方法。B) 包含三步的混合模块化方法：目标检测/定位、关键点回归和姿态计算。前两步使用深度学习，第三步使用经典算法进行必要的异常值移除，以适应PnP求解器，最后进行姿态优化。
- en: '![Refer to caption](img/4cd717bf2ff9576b6dc5990d9c3d7c92.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4cd717bf2ff9576b6dc5990d9c3d7c92.png)'
- en: 'Figure 5: Hybrid modular approach for spacecraft pose estimation. The spacecraft
    localisation stage is outlined in blue, the keypoint prediction stage is in red
    and the pose computation stage is shown in green. Spacecraft image from the SPARK2
    dataset is used for illustration [[127](#bib.bib127)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：航天器姿态估计的混合模块化方法。航天器定位阶段以蓝色标出，关键点预测阶段为红色，姿态计算阶段为绿色。航天器图像来自SPARK2数据集，用于说明 [[127](#bib.bib127)]。
- en: 2.1 Hybrid Modular Approaches
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 混合模块化方法
- en: 'This survey defines hybrid approaches as those using a combination of DL models
    and classical computer vision methods for spacecraft pose estimation. The hybrid
    algorithms have three common stages (see [Figure 5](#S2.F5 "In 2 Algorithms ‣
    A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects")): (1) spacecraft localisation for detecting
    and cropping the spacecraft region in the image, (2) keypoint prediction for predicting
    2D keypoints locations of pre-defined 3D keypoints inside cropped regions and
    (3) pose computation for computing the pose from these 2D-3D correspondences.
    The following subsections describe each of these stages in detail.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述定义混合方法为那些结合使用DL模型和经典计算机视觉方法进行航天器姿态估计的算法。混合算法有三个共同阶段（见[图5](#S2.F5 "在2 算法 ‣
    基于深度学习的单目航天器姿态估计综述：当前状态、局限性与前景")）：（1）航天器定位，用于检测和裁剪图像中的航天器区域，（2）关键点预测，用于预测裁剪区域内预定义的3D关键点的2D关键点位置，（3）姿态计算，用于从这些2D-3D对应关系中计算姿态。以下小节将详细描述每个阶段。
- en: 2.1.1 Spacecraft Localisation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 航天器定位
- en: 'The spacecraft object size in the image varies considerably with changes in
    the relative distance between the chaser and target spacecraft as illustrated
    in [Figure 6](#S2.F6 "In 2.1.1 Spacecraft Localisation ‣ 2.1 Hybrid Modular Approaches
    ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects"). This scale variance affects the performance
    of the pose estimation algorithm [[71](#bib.bib71)]. The spacecraft localisation
    stage uses a DL object detection framework to detect the spacecraft by predicting
    bounding boxes around the object (spacecraft). These bounding boxes are then used
    to crop out the region of interest (RoI) in the image containing the spacecraft.
    The extracted RoI is then processed for pose estimation in the subsequent stages.
    Based on literature [[64](#bib.bib64)], DL-based object detectors for spacecraft
    localisation can be classified into two categories:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中航天器的目标大小随着追踪器和目标航天器之间相对距离的变化而显著变化，如[图6](#S2.F6 "在2.1.1 航天器定位 ‣ 2.1 混合模块化方法
    ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计综述：当前状态、局限性与前景")所示。这种尺度变化影响了姿态估计算法的性能[[71](#bib.bib71)]。航天器定位阶段使用深度学习（DL）目标检测框架，通过预测围绕对象（航天器）的边界框来检测航天器。这些边界框随后用于裁剪出图像中包含航天器的感兴趣区域（RoI）。提取的RoI随后在后续阶段处理中用于姿态估计。根据文献[[64](#bib.bib64)]，用于航天器定位的DL目标检测器可以分为两类：
- en: '![Refer to caption](img/61d40d512db253fbe27c33ab8020dbaa.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/61d40d512db253fbe27c33ab8020dbaa.png)'
- en: 'Figure 6: Illustrating variations in spacecraft size in captured images. The
    bounding boxes predicted by an object detector are shown in green. These images
    are taken from the SPARK2 [[127](#bib.bib127)] dataset, and show the Proba-2 spacecraft
    class.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：展示了捕获图像中航天器大小的变化。目标检测器预测的边界框显示为绿色。这些图像取自SPARK2 [[127](#bib.bib127)] 数据集，展示了Proba-2航天器类别。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-stage object detectors
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多阶段目标检测器
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Single-stage object detectors
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单阶段目标检测器
- en: 'Multi-stage object detectors: In these detectors object detection proceeds
    in multiple stages. The first stage generates region proposals, i.e. image areas
    with a higher probability of containing objects to be detected. These region proposals
    are then refined and classified in the second stage. Detectors of this kind generally
    provide highly accurate detections. However, due to their multi-stage nature,
    they suffer from longer image processing times (high latency) and higher number
    of parameters making them resource-intensive. This can be particularly detrimental
    in resource-constrained scenarios such as those encountered in space. Faster R-CNN [[133](#bib.bib133)]
    and Mask R-CNN [[47](#bib.bib47)] are the commonly used multi-stage object detectors
    for spacecraft localisation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段目标检测器：在这些检测器中，目标检测分为多个阶段进行。第一阶段生成区域提案，即包含目标可能性较高的图像区域。这些区域提案在第二阶段中进行细化和分类。这类检测器通常提供高度准确的检测结果。然而，由于其多阶段特性，它们在图像处理时间（高延迟）和参数数量上消耗较大，资源需求高。这在资源受限的场景中，特别是空间环境中可能会造成不利影响。Faster
    R-CNN [[133](#bib.bib133)] 和 Mask R-CNN [[47](#bib.bib47)] 是用于航天器定位的常用多阶段目标检测器。
- en: 'Single-stage object detectors: These detectors, on the other hand, are lightweight
    detectors with a reduced number of parameters and have lower latency for real-time
    detection. YOLO [[130](#bib.bib130)] (and its derivatives), SSD [[88](#bib.bib88)],
    and MobileDet [[182](#bib.bib182)] are the single-stage detectors applied in the
    different spacecraft pose estimation algorithms reviewed this survey.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 单阶段目标检测器：这些检测器是轻量级检测器，参数数量减少，并且具有较低的实时检测延迟。YOLO [[130](#bib.bib130)]（及其衍生版本）、SSD [[88](#bib.bib88)]
    和 MobileDet [[182](#bib.bib182)] 是在本调查中回顾的不同航天器姿态估计算法中应用的单阶段检测器。
- en: Several other object detectors have also been proposed in the wider computer
    vision literature, which can be applied for spacecraft localisation. Zaidi et
    al. [[185](#bib.bib185)] and Zou et al. [[186](#bib.bib186)] presented detailed
    surveys on different classes of object detectors and their characteristics. The
    modular nature of the hybrid approaches makes it easier to replace object detectors
    in the pose estimation algorithms based on criteria such as the number of parameters,
    resource utilisation, latency and real-time inference.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的计算机视觉文献中，还提出了其他几种目标检测器，这些检测器可以用于航天器定位。Zaidi 等人 [[185](#bib.bib185)] 和 Zou
    等人 [[186](#bib.bib186)] 详细回顾了不同类别目标检测器及其特性。混合方法的模块化特性使得根据参数数量、资源利用率、延迟和实时推理等标准更容易替换姿态估计算法中的目标检测器。
- en: 2.1.2 Keypoint Prediction
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 关键点预测
- en: '![Refer to caption](img/db9dbf4c50250aedca303ec671e0a17f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/db9dbf4c50250aedca303ec671e0a17f.png)'
- en: 'Figure 7: (A) Keypoint heatmap prediction with a ResNet-UNet architecture [[29](#bib.bib29)]
    (B) YOLO-like CNN detector with a heatmap regression subnetwork [[60](#bib.bib60)]
    (C) Keypoint prediction is formulated as a keypoint bounding box detection problem [[81](#bib.bib81)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7： (A) 使用 ResNet-UNet 架构进行的关键点热图预测 [[29](#bib.bib29)] (B) 带有热图回归子网络的类似 YOLO
    的 CNN 检测器 [[60](#bib.bib60)] (C) 将关键点预测形式化为关键点边界框检测问题 [[81](#bib.bib81)]。
- en: 'In this stage, the 2D projections of a set of predefined 3D keypoints are predicted
    from the cropped regions containing the spacecraft using a DL model (see [Figure 5](#S2.F5
    "In 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")). The 3D keypoints are generally defined
    by the CAD model of the spacecraft. If the CAD model is not available, multiview
    triangulation (as in [[24](#bib.bib24)] [[123](#bib.bib123)] [[60](#bib.bib60)])
    or Structure from Motion (SfM) techniques [[46](#bib.bib46)] can be used for reconstructing
    a wireframe 3D model of the spacecraft containing the 3D keypoints.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，使用深度学习模型从包含航天器的裁剪区域中预测一组预定义 3D 关键点的 2D 投影（见 [图 5](#S2.F5 "在 2 个算法中 ‣ 基于深度学习的单目航天器姿态估计调查：当前状态、局限性和前景")）。3D
    关键点通常由航天器的 CAD 模型定义。如果 CAD 模型不可用，可以使用多视角三角测量（如 [[24](#bib.bib24)] [[123](#bib.bib123)] [[60](#bib.bib60)]）或运动结构
    (SfM) 技术 [[46](#bib.bib46)] 来重建包含 3D 关键点的航天器线框 3D 模型。
- en: 'Regression of keypoint locations: A common method for predicting keypoints
    is to directly regress the keypoint locations. Huan et al. [[58](#bib.bib58)]
    uses a CNN regression model with an HRNet[[169](#bib.bib169)] backbone for directly
    regressing the 2D keypoint locations as a $1\times 1\times 2M$ vector, where $M$
    is the number of keypoints. Park et al. [[115](#bib.bib115)] uses a YOLOv2[[131](#bib.bib131)]
    based architecture with a MobileNetv2[[142](#bib.bib142)] backbone with only 5.64M
    parameters for regressing keypoints. The lightweight nature of the model makes
    it suitable for deployment in space hardware or edge devices. Similarly, Lotti
    et al. [[92](#bib.bib92)] also propose a deployable CNN regression model for keypoint
    regression with EfficientNet-Lite backbone [[161](#bib.bib161)], which is obtained
    by removing operations not well supported for mobile applications (deployment)
    from the original EfficientNets[[159](#bib.bib159)].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点位置回归：预测关键点的常见方法是直接回归关键点位置。Huan 等人 [[58](#bib.bib58)] 使用一个带有 HRNet[[169](#bib.bib169)]
    主干的 CNN 回归模型，直接回归 2D 关键点位置为 $1\times 1\times 2M$ 向量，其中 $M$ 是关键点的数量。Park 等人 [[115](#bib.bib115)]
    使用基于 YOLOv2[[131](#bib.bib131)] 的架构，配有仅 5.64M 参数的 MobileNetv2[[142](#bib.bib142)]
    主干，用于回归关键点。模型的轻量化特性使其适合在航天硬件或边缘设备中部署。类似地，Lotti 等人 [[92](#bib.bib92)] 也提出了一种可部署的
    CNN 回归模型用于关键点回归，主干为 EfficientNet-Lite [[161](#bib.bib161)]，该模型通过移除原始 EfficientNets[[159](#bib.bib159)]
    中不适合移动应用（部署）的操作来获得。
- en: 'Segmentation-driven approach: Algorithms in [[41](#bib.bib41)],[[57](#bib.bib57)]
    and [[75](#bib.bib75)] follow the segmentation-driven approach from Hu et al. [[56](#bib.bib56)]
    for regressing the keypoint locations, with a dual-headed (segmentation and regression)
    network architecture and a shared backbone. The input image is divided into a
    grid and the segmentation head separates the foreground grid cells (containing
    the spacecraft) from the background. The regression head predicts the location
    of each keypoint as an offset from the centre of each of the grid cells. Only
    the predictions from foreground (spacecraft) grid cells contribute to the prediction
    of the keypoint location, making predictions more accurate. Additionally, [[75](#bib.bib75)]
    also presents different variants of the keypoint prediction model with a lower
    number of parameters making it suitable for deployment in space hardware. The
    model with the lowest number of parameters achieving sufficient keypoint prediction
    accuracy uses a MobileNetv3[[53](#bib.bib53)] backbone that has only 7.8M parameters.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分割驱动的方法：在[[41](#bib.bib41)]、[[57](#bib.bib57)]和[[75](#bib.bib75)]中，算法遵循Hu等人[[56](#bib.bib56)]提出的分割驱动方法来回归关键点位置，采用双头（分割和回归）网络结构和共享的骨干网络。输入图像被划分成网格，分割头将前景网格单元（包含航天器）与背景分离。回归头将每个关键点的位置预测为相对于每个网格单元中心的偏移量。只有来自前景（航天器）网格单元的预测会贡献于关键点位置的预测，从而提高预测的准确性。此外，[[75](#bib.bib75)]还提出了不同变体的关键点预测模型，这些变体具有较少的参数，使其适合用于空间硬件的部署。具有最低参数数量且能够实现足够关键点预测精度的模型使用了只有7.8M参数的MobileNetv3[[53](#bib.bib53)]骨干网络。
- en: 'Heatmap prediction: Another method for keypoint prediction is to regress the
    heatmaps encoding probability of the keypoint locations. The pixel coordinates
    are then obtained by extracting locations with the highest probability from these
    heatmaps [[23](#bib.bib23)] [[126](#bib.bib126)] [[60](#bib.bib60)] [[120](#bib.bib120)] [[29](#bib.bib29)].
    The ground truth heatmaps are generated as 2D normal distributions with means
    equal to the ground truth keypoint locations and unit standard deviations. HRNet [[169](#bib.bib169)]
    network architecture and its derivative, the HigherHRNet [[25](#bib.bib25)], is
    used extensively for heatmap predictions in different algorithms. HRNet architectures
    maintain high-resolution feature maps throughout the network making it suitable
    for heatmap prediction tasks. UNet [[138](#bib.bib138)] architecture is also used
    for predicting keypoint heatmaps [[29](#bib.bib29)] (see [Figure 7](#S2.F7 "In
    2.1.2 Keypoint Prediction ‣ 2.1 Hybrid Modular Approaches ‣ 2 Algorithms ‣ A Survey
    on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations
    and Prospects")-A). Originally developed for image segmentation, UNet architecture
    consists of a sequence of downsampling layers (contracting path) that captures
    relevant semantic information. This is followed by symmetrical upsampling layers
    (expanding path) for precise location predictions. The use of skip connections
    in the architecture preserves spatial information during downsampling and subsequent
    upsampling. Huo et al. [[60](#bib.bib60)] presented a lightweight hybrid architecture
    for keypoint prediction combining a YOLO-like CNN spacecraft detector with a heatmap
    regression subnetwork (see [Figure 7](#S2.F7 "In 2.1.2 Keypoint Prediction ‣ 2.1
    Hybrid Modular Approaches ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")-B). Sharing
    the backbone network architecture between the object detection and the keypoint
    prediction brings down the total number of parameters to $\sim$.89M, making it
    suitable to deploy in resource-constrained space systems.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 热图预测：另一种关键点预测的方法是回归编码关键点位置概率的热图。通过从这些热图
- en: 'Bounding box prediction: Recently, Li et al. [[81](#bib.bib81)] formulated
    keypoint prediction as a keypoint bounding box detection problem. Instead of predicting
    the keypoint locations or heatmaps, the enclosing bounding boxes over the keypoints
    are predicted along with the confidence scores. Authors used CSPDarknet [[13](#bib.bib13)]
    CNN backbone with a Feature Pyramid Network (FPN) [[85](#bib.bib85)] for multi-scale
    feature extraction, followed by a detection head for the keypoint bounding box
    detection (see [Figure 7](#S2.F7 "In 2.1.2 Keypoint Prediction ‣ 2.1 Hybrid Modular
    Approaches ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")-C). A similar method
    is also used in [[172](#bib.bib172)]. Here, a counterfactual analysis [[117](#bib.bib117)]
    framework is used to generate the FPN, which is then fed to the keypoint detector.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框预测：最近，Li 等人[[81](#bib.bib81)] 将关键点预测公式化为关键点边界框检测问题。不是预测关键点位置或热图，而是预测关键点上的包围边界框及其置信度分数。作者使用了
    CSPDarknet [[13](#bib.bib13)] CNN 主干网络，结合特征金字塔网络（FPN）[[85](#bib.bib85)] 进行多尺度特征提取，然后通过检测头进行关键点边界框检测（见
    [图 7](#S2.F7 "在 2.1.2 关键点预测 ‣ 2.1 混合模块方法 ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性和前景")-C）。类似的方法也用于
    [[172](#bib.bib172)]。这里使用了反事实分析 [[117](#bib.bib117)] 框架生成 FPN，然后将其输入关键点检测器。
- en: 2.1.3 Pose Computation
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 姿态计算
- en: The final stage is to compute the spacecraft pose using the 2D keypoints (from
    the keypoint prediction stage) and the corresponding pre-defined 3D points [[94](#bib.bib94)].
    One important step in the pose computation process is to remove the wrongly predicted
    keypoints, referred to as outliers, since the Perspective-$n$-Point (P$n$P) [[35](#bib.bib35)]
    solvers are sensitive to the presence of outliers. The RANdom SAmple Consensus
    (RANSAC) [[153](#bib.bib153)] algorithm is commonly used for removing outliers.
    IterativePnP [[104](#bib.bib104)] and EPnP[[80](#bib.bib80)] are the two solvers
    extensively used in the different hybrid algorithms. Recently, Legrand et.al [[75](#bib.bib75)]
    replaced the P$n$P solver with a Multi-Layer Perceptron (MLP) network architecture,
    the Pose Inference Network (PIN) [[55](#bib.bib55)], for regressing the pose from
    the predicted keypoints. This makes pose computation differentiable and it can
    be trained with a pose loss function. In the final step, the estimated pose is
    further refined by optimising a geometrical loss function [[67](#bib.bib67)] such
    as the keypoint reprojection error [[23](#bib.bib23)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最终阶段是使用 2D 关键点（来自关键点预测阶段）和相应的预定义 3D 点 [[94](#bib.bib94)] 计算航天器姿态。姿态计算过程中的一个重要步骤是去除错误预测的关键点，称为异常值，因为透视-$n$-点（P$n$P）[[35](#bib.bib35)]
    求解器对异常值非常敏感。RANdom SAmple Consensus (RANSAC) [[153](#bib.bib153)] 算法通常用于去除异常值。IterativePnP
    [[104](#bib.bib104)] 和 EPnP [[80](#bib.bib80)] 是在不同混合算法中广泛使用的两个求解器。最近，Legrand
    等人 [[75](#bib.bib75)] 将 P$n$P 求解器替换为多层感知机（MLP）网络架构，即姿态推断网络（PIN）[[55](#bib.bib55)]，用于从预测的关键点回归姿态。这使得姿态计算具有可微性，并且可以使用姿态损失函数进行训练。在最后一步中，通过优化几何损失函数
    [[67](#bib.bib67)]（如关键点重投影误差 [[23](#bib.bib23)]）进一步细化估计的姿态。
- en: 2.2 Direct End-to-end Approaches
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 直接端到端方法
- en: '![Refer to caption](img/2523d29e877480747732ab4f0f812cd2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2523d29e877480747732ab4f0f812cd2.png)'
- en: 'Figure 8: Network architecture used in [[119](#bib.bib119)]. A GoogLeNet [[157](#bib.bib157)]
    based CNN architecture is used to regress the 7D pose vector $[x,y,z,q_{0},q_{1},q_{2},q_{3}]$.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 用于 [[119](#bib.bib119)] 的网络架构。使用基于 GoogLeNet [[157](#bib.bib157)] 的 CNN
    架构来回归 7D 姿态向量 $[x,y,z,q_{0},q_{1},q_{2},q_{3}]$。'
- en: In this survey, direct approaches refer to the use of only one DL model in an
    end-to-end manner for regressing the spacecraft pose directly from the images
    without relying on intermediate stages. The models are trained using loss functions
    calculated from the pose error. Unlike hybrid algorithms, the approach does not
    require any additional information like camera parameters or a 3D model of the
    spacecraft apart from the ground truth pose labels. The camera parameters are
    intrinsically learned by the models during the training process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，直接方法指的是仅使用一个深度学习模型以端到端的方式直接从图像中回归航天器姿态，而不依赖于中间阶段。这些模型使用姿态误差计算的损失函数进行训练。与混合算法不同，这种方法不需要除了真实姿态标签之外的任何额外信息，如相机参数或航天器的
    3D 模型。相机参数在训练过程中由模型内在学习。
- en: 'Phisannupawong et al. [[119](#bib.bib119)] proposed a GoogLeNet-based [[157](#bib.bib157)]
    CNN architecture for regressing the 7D pose vector representing position and orientation
    quaternion (see [Figure 8](#S2.F8 "In 2.2 Direct End-to-end Approaches ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects")). The network was trained using different loss
    functions, an exponential loss function and a weighted Euclidean-based loss function.
    The experimental results show that the network offers better performance when
    trained with the latter. However, directly regressing the orientation using a
    norm-based loss of unit quaternions fails to achieve higher accuracies and results
    in a larger error margin [[124](#bib.bib124)]. This is mainly due to the loss
    function’s inability to represent the actual angular distance of any orientation
    representation.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Phisannupawong等人[[119](#bib.bib119)] 提出了基于GoogLeNet的[[157](#bib.bib157)] CNN架构，用于回归7D姿态向量，表示位置和方向四元数（参见[图8](#S2.F8
    "在2.2 直接端到端方法 ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性和前景")）。该网络使用不同的损失函数进行训练，包括指数损失函数和加权欧几里得损失函数。实验结果表明，使用后者进行训练时网络性能更佳。然而，直接使用基于单位四元数的范数损失回归方向未能实现更高的准确度，导致较大的误差范围[[124](#bib.bib124)]。这主要是因为损失函数无法表示任何方向表示的实际角度距离。
- en: 'Sharma et al. [[145](#bib.bib145)] proposed discretising the pose space itself
    into pose classification labels by quantising along four degrees of freedom as
    illustrated in [Figure 9](#S2.F9 "In 2.2 Direct End-to-end Approaches ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects"). Two degrees of freedom controlling the position
    of the camera (w.r.t. to the spacecraft) along the surface of the enclosing sphere,
    one degree of freedom denoting the rotation of the camera along the bore-sight
    angle and one degree of freedom determined by the distance of the camera from
    the spacecraft. An AlexNet-based [[74](#bib.bib74)] CNN network is used for classifying
    the spacecraft images into these discretised pose label classes, trained with
    a Softmax loss function [[171](#bib.bib171)]. However, this is constrained by
    the total number of pose class labels to be learned. A larger number of pose labels
    will need an equivalent number of neurons in the final softmax layer, increasing
    model size considerably. Also, the method provides an initial guess and requires
    further refinement to produce more accurate pose estimations.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma等人[[145](#bib.bib145)] 提出了将姿态空间本身离散化为姿态分类标签，通过沿四个自由度进行量化，如[图9](#S2.F9
    "在2.2 直接端到端方法 ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性和前景")所示。两个自由度控制相机（相对于航天器）在包围球表面上的位置，一个自由度表示相机沿视轴角度的旋转，另一个自由度由相机与航天器的距离决定。使用基于AlexNet的[[74](#bib.bib74)]
    CNN网络将航天器图像分类到这些离散的姿态标签类别中，训练使用Softmax损失函数[[171](#bib.bib171)]。然而，这受到需要学习的姿态分类标签总数的限制。更多的姿态标签将需要等量的神经元在最终的softmax层中，从而显著增加模型大小。此外，该方法提供了初步估计，并需要进一步的细化以产生更准确的姿态估计。
- en: '![Refer to caption](img/74cdb5ea07e790109d3fd2c9ad1e19ec.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/74cdb5ea07e790109d3fd2c9ad1e19ec.png)'
- en: 'Figure 9: Illustration of pose space discretisation along four degrees of freedom
    used in [[145](#bib.bib145)]. Two degrees of freedom controlling the position
    of the camera on the enclosing sphere, one degree of freedom from the rotation
    of the camera along the bore-sight direction and one degree of freedom from the
    distance of the camera to the spacecraft.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在[[145](#bib.bib145)]中使用的四个自由度下的姿态空间离散化示意图。两个自由度控制相机在包围球上的位置，一个自由度来自相机沿视轴方向的旋转，另一个自由度来自相机与航天器的距离。
- en: 'To overcome these limitations, Sharma et al. [[146](#bib.bib146)] later presented
    Spacecraft Pose Network (SPN), a model with a five-layer CNN backbone followed
    by three different sub-branches (see [Figure 10](#S2.F10 "In 2.2 Direct End-to-end
    Approaches ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")). The first branch
    localises the spacecraft in the input image and returns the bounding box. The
    second branch classifies the target orientation in terms of a probability distribution
    of discrete classes. It minimises a standard cross entropy loss for a set of closest
    orientation labels. Finally, the third branch takes the candidate orientation
    class labels obtained from the previous branch and minimises another cross-entropy
    loss to yield the relative weighting of each orientation class. The final refined
    attitude is obtained via quaternion averaging with respect to the computed weights,
    which represents a soft classification approach. The position is then estimated
    from the constraints imposed by the detected bounding box and the estimated orientation,
    using the Gauss–Newton optimisation algorithm [[100](#bib.bib100)].'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些局限性，Sharma 等人 [[146](#bib.bib146)] 后来提出了航天器姿态网络 (SPN)，这是一个具有五层 CNN 主干的模型，后跟三个不同的子分支（见
    [图 10](#S2.F10 "在 2.2 直接端到端方法 ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景)）。第一个分支在输入图像中定位航天器并返回边界框。第二个分支根据离散类别的概率分布对目标方向进行分类。它最小化一组最接近方向标签的标准交叉熵损失。最后，第三个分支接受从前一个分支获得的候选方向类别标签，并最小化另一个交叉熵损失，以得出每个方向类别的相对权重。最终的精炼姿态通过对计算权重进行四元数平均来获得，这代表了一种软分类方法。然后，通过使用
    Gauss-Newton 优化算法 [[100](#bib.bib100)] 从检测到的边界框和估计的方向施加的约束来估计位置。
- en: Similar network architecture is also used in [[59](#bib.bib59)]. A ResNet50
    model [[48](#bib.bib48)] with a Squeeze-and-Excitation (SE) module [[54](#bib.bib54)]
    is used as the base CNN network for feature extraction. The first sub-network,
    the attitude-prediction-subnetwork, estimates the orientation by soft classification
    and error quaternion regression. The second pose regression sub-network, predicts
    the position of the spacecraft by direct regression. Finally, the object detection
    sub-network detects the spacecraft by predicting the enclosing bounding box. The
    bounding box is used to validate the position and orientation prediction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的网络架构也被用于 [[59](#bib.bib59)]。一个带有 Squeeze-and-Excitation (SE) 模块 [[54](#bib.bib54)]的
    ResNet50 模型 [[48](#bib.bib48)] 被用作特征提取的基础 CNN 网络。第一个子网络，即姿态预测子网络，通过软分类和误差四元数回归来估计方向。第二个姿态回归子网络，通过直接回归来预测航天器的位置。最后，物体检测子网络通过预测包围边界框来检测航天器。边界框用于验证位置和方向的预测。
- en: '![Refer to caption](img/f47b13e10a6ebd71c34ccb5e046d5e48.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f47b13e10a6ebd71c34ccb5e046d5e48.png)'
- en: 'Figure 10: Network architecture used for spacecraft pose estimation in [[146](#bib.bib146)].
    Branch 1 localises the spacecraft outputting the bounding box, branch 2 predicts
    the probability distribution for orientation classification and branch 3 regresses
    the weights for each orientation class.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 用于航天器姿态估计的网络架构如 [[146](#bib.bib146)]所示。分支 1 定位航天器并输出边界框，分支 2 预测方向分类的概率分布，分支
    3 回归每个方向类别的权重。'
- en: 'Proença et al. [[124](#bib.bib124)] propose URSONet, a ResNet-based backbone
    architecture followed by two separate branches for the estimation of the position
    and orientation (see [Figure 11](#S2.F11 "In 2.2 Direct End-to-end Approaches
    ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")). The position estimation was carried
    out through a simple regression branch with two fully connected layers while minimising
    the relative error in the loss function. A continuous orientation estimation via
    classification with soft-assignment coding was proposed for orientation estimation.
    Each ground truth label is encoded as a Gaussian random variable in the orientation
    discrete output space. The network was then trained to output the probability
    mass function corresponding to the actual orientation. Poss et al. [[121](#bib.bib121)]
    presented Mobile-URSONet, a mobile-friendly deployable lightweight version of
    the URSONet. The ResNet backbone was replaced with a MobileNetv2 [[142](#bib.bib142)]
    model, and the number of fully connected layers in the sub-branches was reduced
    to one (from two). It reduced the number of parameters to a range of 2.2M to 7.4M,
    13 times smaller than the URSONet. Moreover, this was achieved without a considerable
    degradation in performance.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Proença 等人 [[124](#bib.bib124)] 提出了 URSONet，一种基于 ResNet 的骨干网络架构，随后是两个独立的分支用于位置和方向的估计（见
    [图 11](#S2.F11 "在 2.2 直接端到端方法 ‣ 2 算法 ‣ 深度学习单目航天器姿态估计的调查：当前状态、局限性和前景")）。位置估计通过一个具有两个全连接层的简单回归分支来进行，同时最小化损失函数中的相对误差。方向估计则提出了通过软分配编码的分类进行连续方向估计。每个地面真实标签在方向离散输出空间中被编码为一个高斯随机变量。然后，网络被训练以输出对应实际方向的概率质量函数。Poss
    等人 [[121](#bib.bib121)] 提出了 Mobile-URSONet，这是一种适合移动部署的轻量化 URSONet 版本。ResNet 骨干被替换为
    MobileNetv2 [[142](#bib.bib142)] 模型，子分支中的全连接层数量减少到一个（从两个减少）。这将参数数量减少到 2.2M 到 7.4M
    范围，比 URSONet 小了 13 倍。此外，这一改进在性能上没有明显的下降。
- en: 'Recently, Park et al. [[110](#bib.bib110)] presented SPNv2, improving on the
    original SPN [[146](#bib.bib146)] for addressing the domain gap problem. SPNv2
    has a multi-scale multi-task network architecture with a shared feature extractor
    following the EfficientPose [[16](#bib.bib16)] network, which is based on the
    EfficientDet[[160](#bib.bib160)] feature encoder comprised of an EfficientNet[[159](#bib.bib159)]
    backbone and a Bi-directional FPN (BiFPN) [[160](#bib.bib160)] for multi-scale
    feature fusion. This is followed by multiple prediction heads for each of the
    tasks learned: binary classification of spacecraft presence, bounding box prediction,
    target position and orientation estimation, keypoint heatmap regression and pixel-wise
    binary segmentation of the spacecraft foreground. The results show that joint
    multi-task learning helps in domain generalisation by preventing the shared feature
    extractor from learning task-specific features. The authors also propose an online
    domain refinement (ODR) using target domain images (without labels) to be performed
    on board spacecraft. The ODR fine-tunes SPNv2 on the target images by minimising
    the Shannon entropy [[144](#bib.bib144)] on the segmentation task prediction head.
    The paper also presents different variants of the algorithm by changing the number
    of parameters in the EfficientNet backbone. The smallest variant with 3.8M parameters
    has comparable performance to the best-performing variant with 52.5M parameters
    on the SPEED+ synthetic dataset.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Park 等人 [[110](#bib.bib110)] 提出了 SPNv2，改进了原始 SPN [[146](#bib.bib146)] 以解决领域间差距问题。SPNv2
    具有一个多尺度多任务网络架构，带有一个共享特征提取器，继承自 EfficientPose [[16](#bib.bib16)] 网络，该网络基于 EfficientDet
    [[160](#bib.bib160)] 特征编码器，包括 EfficientNet [[159](#bib.bib159)] 骨干和一个双向 FPN (BiFPN)
    [[160](#bib.bib160)] 用于多尺度特征融合。随后是多个预测头，用于学习的每个任务：航天器存在的二分类、边界框预测、目标位置和方向估计、关键点热图回归以及航天器前景的逐像素二分类分割。结果表明，联合多任务学习通过防止共享特征提取器学习任务特定特征，帮助领域泛化。作者还提出了一种在线领域细化
    (ODR) 方法，使用目标领域图像（无标签）在航天器上进行。ODR 通过最小化 Shannon 熵 [[144](#bib.bib144)] 在分割任务预测头上对
    SPNv2 进行细化。论文还展示了通过改变 EfficientNet 骨干中的参数数量而产生的不同算法变体。具有 3.8M 参数的最小变体在 SPEED+
    合成数据集上的性能与具有 52.5M 参数的最佳变体相当。
- en: '![Refer to caption](img/4e6acd506b3f7d5439840350bab55130.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e6acd506b3f7d5439840350bab55130.png)'
- en: 'Figure 11: Direct end-to-end approach for spacecraft pose estimation. The position
    is regressed directly and the orientation is obtained with soft classification [[124](#bib.bib124)]'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 用于航天器姿态估计的直接端到端方法。位置直接回归，方向通过软分类获得 [[124](#bib.bib124)]'
- en: .
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '![Refer to caption](img/26a9c28c28fcb8c8ff543c05fe60382a.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/26a9c28c28fcb8c8ff543c05fe60382a.png)'
- en: 'Figure 12: LSPnet architecture for spacecraft pose estimation [[38](#bib.bib38)]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 用于航天器姿态估计的 LSPnet 架构 [[38](#bib.bib38)]'
- en: .
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Garcia et al. [[38](#bib.bib38)] presented a network architecture with two
    CNN modules: the translation and orientation modules, for pose estimation (see
    [Figure 12](#S2.F12 "In 2.2 Direct End-to-end Approaches ‣ 2 Algorithms ‣ A Survey
    on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations
    and Prospects")). The translation module has a UNet architecture [[138](#bib.bib138)]
    for predicting the 3D position $[x,y,z]$ of the target (from the intermediate
    feature embedding layer) and the 2D spacecraft location in the image $[u,v]$ (from
    the final heatmap output). This is then used to generate the enclosing bounding
    box for the spacecraft and the RoI is cropped out. The orientation module with
    a CNN regression network predicts the spacecraft orientation $[q_{0},q_{1},q_{2},q_{3}]$
    from the cropped RoI.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Garcia 等人 [[38](#bib.bib38)] 提出了一个具有两个 CNN 模块的网络架构：平移模块和方向模块，用于姿态估计（见 [图 12](#S2.F12
    "在 2.2 直接端到端方法 ‣ 2 算法 ‣ 深度学习单目航天器姿态估计的调查：当前状态、局限性和前景")）。平移模块采用 UNet 架构 [[138](#bib.bib138)]
    预测目标的 3D 位置 $[x,y,z]$（来自中间特征嵌入层）以及图像中的 2D 航天器位置 $[u,v]$（来自最终热图输出）。然后，这些信息用于生成航天器的包围框，并裁剪出
    RoI。方向模块使用 CNN 回归网络从裁剪出的 RoI 中预测航天器的方向 $[q_{0},q_{1},q_{2},q_{3}]$。
- en: Finally, Musallam et al. evaluated their state-of-the-art absolute pose regression
    network E-PoseNet [[103](#bib.bib103)] on the SPEED dataset. The model is based
    on the PoseNet architecture [[69](#bib.bib69)], where the backbone is replaced
    by a SE(2)-equivariant ResNet18 backbone [[175](#bib.bib175)]. The equivariant
    features encode more geometric information about the input image. Moreover, equivariance
    to planar transformations constrains the network in a way that can aid generalization,
    especially due to the weights sharing. Finally, the rotation-equivariant ResNet
    shows a significant reduction in model size compared to the regular ResNet architecture,
    to obtain the same feature size.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Musallam 等人对其最先进的绝对姿态回归网络 E-PoseNet [[103](#bib.bib103)] 在 SPEED 数据集上进行了评估。该模型基于
    PoseNet 架构 [[69](#bib.bib69)]，其中主干网络被 SE(2)-等变的 ResNet18 主干网络 [[175](#bib.bib175)]
    替代。等变特征编码了更多关于输入图像的几何信息。此外，平面变换的等变性以一种有助于泛化的方式约束了网络，特别是由于权重共享。最后，与常规 ResNet 架构相比，旋转等变的
    ResNet 在模型尺寸上显示出显著减少，以获得相同的特征尺寸。
- en: 2.3 Algorithm Comparison
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 算法比较
- en: '![Refer to caption](img/8065828170ee71d2f827a28de53b0ef7.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/8065828170ee71d2f827a28de53b0ef7.png)'
- en: (a)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/3d75c01ac1d386f35388c12bdd4c2c03.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/3d75c01ac1d386f35388c12bdd4c2c03.png)'
- en: (b)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 13: Comparison of pose estimation algorithms in terms of number of parameters
    versus (a) position error and (b) orientation error.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 姿态估计算法的参数数量与 (a) 位置误差和 (b) 方向误差的比较。'
- en: 'In this section, different spacecraft pose estimation algorithms are compared.
    LABEL:table:hybrid_approach_review and [Table 2](#S2.T2 "In 2.4.3 Robustness to
    Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")
    summarise different hybrid and direct algorithms, respectively, with a comparison
    of DL models used, the total number of parameters, and the pose accuracy. The
    performance of the pose estimation algorithm is expressed in terms of the mean
    position and orientation errors. The position error is calculated as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，不同的航天器姿态估计算法进行了比较。LABEL:table:hybrid_approach_review 和 [表 2](#S2.T2 "在
    2.4.3 对光照条件的鲁棒性 ‣ 2.4 局限性 ‣ 2 算法 ‣ 深度学习单目航天器姿态估计的调查：当前状态、局限性和前景") 分别总结了不同的混合算法和直接算法，并比较了所用的深度学习模型、总参数数量和姿态精度。姿态估计算法的性能以均值位置和方向误差来表示。位置误差计算公式为：
- en: '|  | $E_{t}=&#124;&#124;t_{\text{predicted}}-t_{\text{groundtruth}}&#124;&#124;_{2}$
    |  | (1) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{t}=&#124;&#124;t_{\text{predicted}}-t_{\text{groundtruth}}&#124;&#124;_{2}$
    |  | (1) |'
- en: 'and the orientation error is calculated as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方向误差计算公式为：
- en: '|  | $E_{R}=2*\mathrm{arccos}\left(&#124;<q_{\text{predicted}},q_{\text{groundtruth}}>&#124;\right)$
    |  | (2) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{R}=2*\mathrm{arccos}\left(&#124;<q_{\text{predicted}},q_{\text{groundtruth}}>&#124;\right)$
    |  | (2) |'
- en: where, $t_{\text{predicted}}$, $t_{\text{groundtruth}}$ are the predicted and
    the ground truth translation vectors and $q_{\text{predicted}}$, $q_{\text{groundtruth}}$
    are the predicted and the ground truth rotation quaternions respectively. $|<,>|$
    indicates the absolute value of the vector dot product and $\|_{2}$ is the Euclidean
    norm. The mean position and orientation error values on the SPEED [[71](#bib.bib71)]
    synthetic test set are reported where available [[71](#bib.bib71)]. In other cases,
    the error values on the corresponding synthetic published dataset are reported.
    Similarly, in many instances, authors do not report the total number of parameters
    in their algorithms. In such cases, an approximate number of parameters is estimated
    based on the known backbone models and frameworks used. This survey is the first
    attempt to compare different DL-based spacecraft pose estimation algorithms in
    terms of performance reported on different datasets and the number of model parameters
    with available information in the literature.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$t_{\text{predicted}}$ 和 $t_{\text{groundtruth}}$ 分别是预测的和真实的位移向量，$q_{\text{predicted}}$
    和 $q_{\text{groundtruth}}$ 分别是预测的和真实的旋转四元数。$|<,>|$ 表示向量点积的绝对值，$\|_{2}$ 是欧几里得范数。在SPEED [[71](#bib.bib71)]
    合成测试集上报告了均值位置和方向误差值，具体情况见[[71](#bib.bib71)]。在其他情况下，报告了相应合成发布数据集上的误差值。类似地，在许多情况下，作者未报告其算法中的参数总数。在这种情况下，基于已知的主干模型和框架估算了参数的大致数量。本调查是首次尝试比较不同的基于深度学习的航天器姿态估计算法，在不同数据集上的性能和文献中可用信息的模型参数数量方面。
- en: 'A key aspect of the spacecraft pose estimation algorithms is the deployment
    on edge devices for their use in space. Unlike the commonly used resource-abundant
    workstations, computing resources are scarce in space systems. Hence, deploying
    large DL models that have a very large number of parameters is difficult. On the
    other hand, using smaller DL models with a lower number of parameters leads to
    a drop in performance. Thus, a trade-off is needed between the use of large, high-performing
    models and smaller, deployable models. Based on LABEL:table:hybrid_approach_review
    and [Table 2](#S2.T2 "In 2.4.3 Robustness to Illumination Conditions ‣ 2.4 Limitations
    ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects"), [Figure 13(b)](#S2.F13.sf2 "In Figure
    13 ‣ 2.3 Algorithm Comparison ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")
    shows this trade-off by plotting the algorithm performance against the total number
    of model parameters. The results show that the algorithms [[93](#bib.bib93), [120](#bib.bib120),
    [81](#bib.bib81)] and the SLAB Baseline [[115](#bib.bib115)] provide a good trade-off
    in terms of the performance and the number of parameters.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '航天器姿态估计算法的一个关键方面是部署到边缘设备上以便在太空中使用。与常用的资源丰富的工作站不同，太空系统中的计算资源非常有限。因此，部署具有大量参数的大型深度学习模型是困难的。另一方面，使用参数较少的小型深度学习模型会导致性能下降。因此，需要在大型高性能模型和较小的可部署模型之间进行权衡。根据
    LABEL:table:hybrid_approach_review 和 [Table 2](#S2.T2 "In 2.4.3 Robustness to
    Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")，[Figure 13(b)](#S2.F13.sf2
    "In Figure 13 ‣ 2.3 Algorithm Comparison ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")
    通过绘制算法性能与模型参数总数的关系图展示了这种权衡。结果表明，算法 [[93](#bib.bib93), [120](#bib.bib120), [81](#bib.bib81)]
    和 SLAB 基线 [[115](#bib.bib115)] 在性能和参数数量方面提供了良好的权衡。'
- en: 'Another factor of comparison for algorithms is the modular nature of the approaches
    themselves. The hybrid algorithms are built by integrating three components: spacecraft
    localisation, keypoint regression and pose computation. This helps to work and
    improve each stage of the algorithms in isolation. For example, changes in the
    camera model can be incorporated into the pose computation stage without retraining
    the localisation and keypoint regression models. This provides more flexibility
    in building the algorithms for different pose estimation applications. By contrast,
    the direct algorithms comprise only a single DL model trained end-to-end. The
    entire model has to be retrained to incorporate changes such as changes in camera
    parameters.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 算法比较的另一个因素是方法本身的模块化特性。混合算法通过集成三个组件构建：航天器定位、关键点回归和姿态计算。这有助于在隔离的情况下对每个阶段进行优化和改进。例如，相机模型的变化可以被纳入姿态计算阶段，而无需重新训练定位和关键点回归模型。这为构建适用于不同姿态估计应用的算法提供了更多灵活性。相比之下，直接算法仅包括一个端到端训练的DL模型。要纳入如相机参数变化等变更，必须重新训练整个模型。
- en: In terms of performance comparison between the approaches, analysis of the top-10
    methods from the first edition of ESA Kelvin Satellite Pose Estimation Challenge
    (KSPEC’19) [[71](#bib.bib71)] show that the hybrid approaches perform comparatively
    better than the direct approaches. The hybrid and direct algorithms have mean
    position errors of $0.0083\pm 0.0269$ m and $0.0328\pm 0.0430$ m and mean orientation
    errors of $1.31\pm 2.24\degree$ and $9.76\pm 18.51\degree$, respectively. Analysis
    of the recently concluded second edition of the same challenge (KSPEC’21) [[112](#bib.bib112)]
    also gives similar indications. Winning algorithms on both streams of the challenge
    used the hybrid approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 就方法之间的性能比较而言，ESA Kelvin卫星姿态估计挑战赛（KSPEC’19）第一届的前10名方法的分析 [[71](#bib.bib71)] 显示，混合方法的表现相较于直接方法更为优越。混合和直接算法的均值位置误差分别为$0.0083\pm
    0.0269$ m和$0.0328\pm 0.0430$ m，均值姿态误差分别为$1.31\pm 2.24\degree$和$9.76\pm 18.51\degree$。对同一挑战赛（KSPEC’21）第二届的分析
    [[112](#bib.bib112)] 也显示出类似的趋势。挑战赛中两个赛道的获胜算法都使用了混合方法。
- en: 2.4 Limitations
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 限制
- en: Recently, several promising algorithms have been developed for DL-based spacecraft
    pose estimation using both the hybrid and the direct approaches. However, these
    algorithms still have several limitations that need to be considered and have
    room for further improvement. This section highlights these limitations with discussions
    on each topic.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，开发了几种有前景的DL基础航天器姿态估计算法，使用了混合和直接两种方法。然而，这些算法仍存在若干需要考虑的局限性，并且有进一步改进的空间。本节重点讨论这些局限性。
- en: 2.4.1 Deployability
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 可部署性
- en: Deployability is a key aspect of any space algorithm. Despite the recent progress
    in spacecraft pose estimation algorithm development, the deployment remains an
    important open research question. The limitations of current algorithms in terms
    of deployability refer to the challenges of implementing these algorithms in real-world
    space missions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可部署性是任何空间算法的关键方面。尽管在航天器姿态估计算法开发方面取得了近期进展，但部署仍然是一个重要的开放研究问题。当前算法在可部署性方面的局限性指的是在实际空间任务中实施这些算法所面临的挑战。
- en: Among the current research works, only a small fraction of the developed algorithms
    are tested and evaluated on edge systems for space deployment [[93](#bib.bib93),
    [29](#bib.bib29), [172](#bib.bib172)]. Also, authors rarely report factors effecting
    algorithm deployability such as latency, inference time, memory requirements,
    power consumption and computational cost. These missing details are important
    to understand the deployability of a model [[45](#bib.bib45), [4](#bib.bib4)],
    on resource-constrained environment such in a space system with limited computational
    capabilities.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在目前的研究工作中，只有一小部分开发的算法在空间部署的边缘系统上进行了测试和评估 [[93](#bib.bib93), [29](#bib.bib29),
    [172](#bib.bib172)]。此外，作者们很少报告影响算法可部署性的因素，如延迟、推理时间、内存要求、功耗和计算成本。这些缺失的细节对于理解模型在资源受限环境（如具有有限计算能力的空间系统）中的可部署性是至关重要的
    [[45](#bib.bib45), [4](#bib.bib4)]。
- en: 'Another limitation is the extensive use of off-the-shelf DL models and frameworks
    (refer to LABEL:table:hybrid_approach_review and [Table 2](#S2.T2 "In 2.4.3 Robustness
    to Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms ‣ A Survey on Deep
    Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations
    and Prospects")). While these off-the-shelf models work well on a workstation,
    they may not be suitable for space deployment due to several reasons. Primarily,
    these models are designed to work on systems with abundant resources and are computationally
    expensive, requiring significant processing power and memory. Secondly, these
    models (or certain DL layers) may not be supported [[180](#bib.bib180)] by the
    AI accelerators used in current space systems like FPGA-based [[36](#bib.bib36),
    [77](#bib.bib77)] accelerators. Hence it is required to build algorithms with
    architectures specifically customised for space applications and hardware.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是大量使用现成的深度学习模型和框架（参见 LABEL:table:hybrid_approach_review 和 [表 2](#S2.T2
    "在 2.4.3 光照条件的鲁棒性 ‣ 2.4 限制 ‣ 2 算法 ‣ 关于基于深度学习的单目航天器姿态估计的调查：当前状态、限制和前景")）。虽然这些现成的模型在工作站上表现良好，但由于多个原因，它们可能不适合用于空间部署。首先，这些模型设计用于资源丰富的系统，计算成本高，需要大量的处理能力和内存。其次，这些模型（或某些深度学习层）可能不被当前空间系统中使用的
    AI 加速器支持，如基于 FPGA 的 [[36](#bib.bib36), [77](#bib.bib77)] 加速器。因此，需要建立专门为空间应用和硬件定制的算法架构。
- en: 2.4.2 Explainability
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 可解释性
- en: Explainability refers to the ability to understand how an algorithm arrives
    at its predictions, and it is an essential factor in building trust and ensuring
    safety in critical applications such as space missions. This makes error analysis
    and troubleshooting easier. A key limitation of the current DL-based spacecraft
    pose estimation algorithms is their lack of explainability. In the direct approach,
    the black-box nature of DL models in general [[2](#bib.bib2)] makes interpreting
    the errors and failures very difficult. Comparably, the hybrid approach tackles
    the spacecraft pose estimation problem in stages, providing better interpretability.
    However, these algorithms still lack capabilities such as reasoning[[82](#bib.bib82)]
    or modelling the uncertainty between the input data and the predictions made [[167](#bib.bib167)]
    .
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性指的是理解算法如何得出预测结果的能力，这是建立信任和确保关键应用（如空间任务）安全的重要因素。这使得错误分析和故障排除变得更加容易。目前基于深度学习的航天器姿态估计算法的一个关键限制是其缺乏可解释性。在直接方法中，深度学习模型的黑箱特性
    [[2](#bib.bib2)] 使得解释错误和故障非常困难。相比之下，混合方法分阶段处理航天器姿态估计问题，提供了更好的可解释性。然而，这些算法仍然缺乏诸如推理
    [[82](#bib.bib82)] 或建模输入数据与预测结果之间的不确定性 [[167](#bib.bib167)] 等能力。
- en: 2.4.3 Robustness to Illumination Conditions
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 光照条件的鲁棒性
- en: '![Refer to caption](img/e3533cf84b500e82c061d73ebdc21542.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e3533cf84b500e82c061d73ebdc21542.png)'
- en: 'Figure 14: . Visualization of the worst 3 predictions made by stream-1 winning
    method of the KSPEC’21 challenge on lightbox (top-row) and sunlamp (bottom-row)
    images [[112](#bib.bib112)]. These results show considerable drop in accuracy
    of estimated poses (shown in green) under extreme lighting conditions, highlighting
    a important limitation of vision-based spacecraft pose estimation algorithms.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：KSPEC’21 挑战中 stream-1 胜出方法在光箱（上排）和太阳灯（下排）图像上的最差 3 次预测的可视化 [[112](#bib.bib112)]。这些结果显示，在极端光照条件下，姿态估计的准确性（显示为绿色）显著下降，突显了基于视觉的航天器姿态估计算法的一个重要限制。
- en: 'Monocular vision-based algorithms are in general sensitive to changes in lighting
    conditions. This can affect the accuracy and robustness of the pose estimation,
    especially in the dynamic illumination conditions in space. For example, shadows,
    reflections and sun glare can all create visual noise and make it difficult to
    identify and track features on the spacecraft. Analysis of the results (see [Figure 14](#S2.F14
    "In 2.4.3 Robustness to Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects")) from the latest edition of KSPEC (KSPEC’21) [[112](#bib.bib112)]
    shows that even the best vision-based spacecraft pose estimation algorithms performs
    poorly on images with extreme lighting conditions.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '基于单目视觉的算法通常对光照条件的变化敏感。这可能会影响姿态估计的准确性和鲁棒性，特别是在太空中动态光照条件下。例如，阴影、反射和太阳眩光都可能产生视觉噪声，使得识别和跟踪航天器上的特征变得困难。最新版本的KSPEC（KSPEC’21）[[112](#bib.bib112)]的结果（见[图14](#S2.F14
    "In 2.4.3 Robustness to Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects")）分析表明，即使是最好的基于视觉的航天器姿态估计算法在极端光照条件下表现也不佳。'
- en: 'Overcoming these limitations will require continued research and development
    in areas including algorithm design, evaluation protocols on edge devices, sensor
    technology and modelling of environmental factors. [Section 4](#S4 "4 Future Research
    Directions ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects") outlines future directions of research
    in spacecraft pose estimation algorithm development to address these challenges.
    Finally, any DL-based algorithm development cannot be separated from the question
    of the datasets, both for training and validating the algorithms. The next section
    ([Section 3](#S3 "3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")) presents a detailed
    discussion of spacecraft pose estimation datasets ([Section 3.1](#S3.SS1 "3.1
    Summary of Datasets, Simulators & Testbeds ‣ 3 Datasets ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects"))
    with a focus on the domain gap problem ([Section 3.2](#S3.SS2 "3.2 Bridging the
    Domain Gap ‣ 3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")) and a discussion
    on their limitations ([Section 3.3](#S3.SS3 "3.3 Limitations ‣ 3 Datasets ‣ A
    Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State,
    Limitations and Prospects")).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '克服这些限制将需要在包括算法设计、边缘设备上的评估协议、传感器技术和环境因素建模等领域持续研究和开发。[第4节](#S4 "4 Future Research
    Directions ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")概述了航天器姿态估计算法开发的未来研究方向，以应对这些挑战。最后，任何基于深度学习的算法开发都无法离开数据集的问题，这包括训练和验证算法的需要。下一节（[第3节](#S3
    "3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")）详细讨论了航天器姿态估计数据集（[第3.1节](#S3.SS1 "3.1
    Summary of Datasets, Simulators & Testbeds ‣ 3 Datasets ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")），重点讨论了领域差距问题（[第3.2节](#S3.SS2
    "3.2 Bridging the Domain Gap ‣ 3 Datasets ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")）以及它们的局限性（[第3.3节](#S3.SS3
    "3.3 Limitations ‣ 3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")）。'
- en: 'Table 1: Summary of the hybrid algorithms for spacecraft pose estimation. Details
    of the object detector and keypoint prediction models (including the estimated
    number of parameters) and the pose computation methods used are provided. The
    mean position and orientation error values on the SPEED synthetic test set are
    reported where available. In cases where the number of parameters is not reported
    by the authors, estimated values based on the known backbone models and frameworks
    are given. Additionally, the links to the publicly available algorithms are included
    in [Section A.1](#A1.SS1 "A.1 Publicly available algorithm implementations ‣ Appendix
    A Additional Information ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects").'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 航天器姿态估计混合算法的总结。提供了物体检测器和关键点预测模型的详细信息（包括估计的参数数量）以及使用的姿态计算方法。报告了 SPEED 合成测试集上的平均位置和方向误差值（如有）。在作者未报告参数数量的情况下，提供了基于已知主干模型和框架的估计值。此外，公开可用算法的链接包含在
    [A.1 节](#A1.SS1 "A.1 公开可用算法实现 ‣ 附录 A 附加信息 ‣ 关于基于深度学习的单目航天器姿态估计的调查：现状、局限性和前景")
    中。'
- en: '| Ref | Object Detector | Parameters  (millions) | Keypoint Prediction | Parameters  (millions)
    | Total Parameters  (millions) | Pose Computation | Mean position error (E[t])
              (m) | Mean       orientation error (E[R])           (deg) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 物体检测器 | 参数（百万） | 关键点预测 | 参数（百万） | 总参数（百万） | 姿态计算 | 平均位置误差 (E[t])          
    (m) | 平均方向误差 (E[R])           (deg) |'
- en: '| UniAdelaide [[23](#bib.bib23)] | Faster-RCNN [[133](#bib.bib133)] with HRNet-W18-C[[169](#bib.bib169)]
    as the backbone | $\sim$21.3* [[155](#bib.bib155)] | Pose-HRNet-W32 [[154](#bib.bib154)]
    | $\sim$28.5 [[154](#bib.bib154)] | $\sim$49.8 (176.2 [[92](#bib.bib92)]) | PnP
    + RANSAC refined with a geometric loss optimized using SA-LMPE optimiser | 0.0320^+
    | 0.4100^+ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| UniAdelaide [[23](#bib.bib23)] | 以 HRNet-W18-C[[169](#bib.bib169)] 作为主干的
    Faster-RCNN [[133](#bib.bib133)] | $\sim$21.3* [[155](#bib.bib155)] | Pose-HRNet-W32 [[154](#bib.bib154)]
    | $\sim$28.5 [[154](#bib.bib154)] | $\sim$49.8 (176.2 [[92](#bib.bib92)]) | 通过
    SA-LMPE 优化器优化的几何损失细化的 PnP + RANSAC | 0.0320^+ | 0.4100^+ |'
- en: '| EPFL_cvlab [[41](#bib.bib41)] | Not applied | -NA- | Yolov3 [[132](#bib.bib132)]
    with DarkNet-53[[132](#bib.bib132)] as the backbone followed by a segmentation
    and regression decoder branchers | $\sim$59.1[[90](#bib.bib90)] | $\sim$59.1 (89.2 [[92](#bib.bib92)])
    | EPnP [[80](#bib.bib80)] + RANSAC | 0.0730^+ | 0.9100^+ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| EPFL_cvlab [[41](#bib.bib41)] | 未应用 | -NA- | 以 DarkNet-53[[132](#bib.bib132)]
    作为主干的 Yolov3 [[132](#bib.bib132)] 后接分割和回归解码器分支 | $\sim$59.1[[90](#bib.bib90)]
    | $\sim$59.1 (89.2 [[92](#bib.bib92)]) | EPnP [[80](#bib.bib80)] + RANSAC | 0.0730^+
    | 0.9100^+ |'
- en: '| SLAB Baseline [[115](#bib.bib115)] | YOLOv3 [[132](#bib.bib132)] with MobileNetV2[[142](#bib.bib142)]
    as the backbone | 5.53 | YOLOv2 [[131](#bib.bib131)] with MobileNetV2[[142](#bib.bib142)]
    as the backbone | 5.64 | 11.2 | EPnP | 0.2090^+ | 2.6200^+ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SLAB 基线 [[115](#bib.bib115)] | 以 MobileNetV2[[142](#bib.bib142)] 作为主干的 YOLOv3 [[132](#bib.bib132)]
    | 5.53 | 以 MobileNetV2[[142](#bib.bib142)] 作为主干的 YOLOv2 [[131](#bib.bib131)] |
    5.64 | 11.2 | EPnP | 0.2090^+ | 2.6200^+ |'
- en: '| Huo et al. [[60](#bib.bib60)] | Tiny-YOLOv3 [[132](#bib.bib132)] architecture**
    with a detection subnetwork | -NA- | Tiny-YOLOv3 [[132](#bib.bib132)] architecture**
    with a regression subnetwork | -NA- | $\sim$0.89 | PnP+RANSAC refined with a Log-cosh
    geometric loss optimized by Levenberg-Marquardt solver [[101](#bib.bib101)] |
    0.0320 | 0.6812 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Huo 等 [[60](#bib.bib60)] | 具有检测子网络的 Tiny-YOLOv3 [[132](#bib.bib132)] 架构**
    | -NA- | 具有回归子网络的 Tiny-YOLOv3 [[132](#bib.bib132)] 架构** | -NA- | $\sim$0.89 |
    通过 Levenberg-Marquardt 求解器优化的 Log-cosh 几何损失细化的 PnP+RANSAC [[101](#bib.bib101)]
    | 0.0320 | 0.6812 |'
- en: '| Piazza et al. [[120](#bib.bib120)] | YOLOv5 | 7.5 | HRNet32 [[154](#bib.bib154)]
    | $\sim$28.6* [[25](#bib.bib25)] | $\sim$36.1 | EPnP refined with a geometric
    loss optimised by Levenberg-Marquardt solver |  0.1036 | 2.2400 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Piazza 等 [[120](#bib.bib120)] | YOLOv5 | 7.5 | HRNet32 [[154](#bib.bib154)]
    | $\sim$28.6* [[25](#bib.bib25)] | $\sim$36.1 | 通过 Levenberg-Marquardt 求解器优化的几何损失细化的
    EPnP | 0.1036 | 2.2400 |'
- en: '| Huan et al. [[58](#bib.bib58)] | Cascade Mask R-CNN [[17](#bib.bib17)] with
    HRNet as backbone | -NA- | HRNet [[154](#bib.bib154)] | $\sim$28.5 to $\sim$63.6 [[154](#bib.bib154)]
    | -NA- | EPnP refined with a Huber style geometric loss optimised as non-linear
    least-squares problem | 0.1823 | 2.8723 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Huan 等 [[58](#bib.bib58)] | 以 HRNet 为主干的 Cascade Mask R-CNN [[17](#bib.bib17)]
    | -NA- | HRNet [[154](#bib.bib154)] | $\sim$28.5 到 $\sim$63.6 [[154](#bib.bib154)]
    | -NA- | 通过 Huber 风格几何损失优化的 EPnP 作为非线性最小二乘问题 | 0.1823 | 2.8723 |'
- en: '| STAR LAB keypoint method [[126](#bib.bib126)] | Faster-RCNN [[133](#bib.bib133)]
    with RestNet50[[48](#bib.bib48)] backbone | $\sim$23.9* [[78](#bib.bib78)] | HigherHRNet [[25](#bib.bib25)]
    with HRNet-W32[[154](#bib.bib154)] as the backbone | $\sim$28.6* [[25](#bib.bib25)]
    | $\sim$54.2 | PnP + RANSAC | 0.3000 (URSO-OrViS dataset) | 4.9000 (URSO-OrViS
    dataset) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| STAR LAB 关键点方法 [[126](#bib.bib126)] | Faster-RCNN [[133](#bib.bib133)]，以
    RestNet50[[48](#bib.bib48)] 作为骨干 | $\sim$23.9* [[78](#bib.bib78)] | 采用 HRNet-W32[[154](#bib.bib154)]
    作为骨干的 HigherHRNet [[25](#bib.bib25)] | $\sim$28.6* [[25](#bib.bib25)] | $\sim$54.2
    | PnP + RANSAC | 0.3000 (URSO-OrViS 数据集) | 4.9000 (URSO-OrViS 数据集) |'
- en: '| Black et al. [[12](#bib.bib12)] | SSD [[88](#bib.bib88)] MobileNetV2[[142](#bib.bib142)]
    | -NA- | MobilePose [[52](#bib.bib52)] architecture with MobileNetV2[[142](#bib.bib142)]
    as backbone | -NA- | 6.9 | EPnP + RANSAC | 1.0800 (Cygnus dataset) | 6.4500 (Cygnus
    dataset) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Black 等人 [[12](#bib.bib12)] | SSD [[88](#bib.bib88)] MobileNetV2[[142](#bib.bib142)]
    | -NA- | 采用 MobileNetV2[[142](#bib.bib142)] 作为骨干的 MobilePose [[52](#bib.bib52)]
    架构 | -NA- | 6.9 | EPnP + RANSAC | 1.0800 (Cygnus 数据集) | 6.4500 (Cygnus 数据集) |'
- en: '| Wide-Depth-Range [[57](#bib.bib57)] | Not applied | -NA- | FPN [[85](#bib.bib85)]
    architecture with DarkNet-53[[132](#bib.bib132)] as the backbone | 51.5 | 51.5
    | PnP + RANAC with and without a pose refinement strategy | -NA- | -NA- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Wide-Depth-Range [[57](#bib.bib57)] | 未应用 | -NA- | 采用 DarkNet-53[[132](#bib.bib132)]
    作为骨干的 FPN [[85](#bib.bib85)] 架构 | 51.5 | 51.5 | PnP + RANAC，包含和不包含姿态细化策略 | -NA-
    | -NA- |'
- en: '| Cosmas et al. [[29](#bib.bib29)]† | YOLOv3 [[132](#bib.bib132)] | $\sim$59.1* [[91](#bib.bib91)]
    | ResNet34-UNet [[48](#bib.bib48), [138](#bib.bib138)] architecture | $\sim$21.5* [[79](#bib.bib79)]
    | $\sim$80.6 | -NA- | -NA- | -NA- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Cosmas 等人 [[29](#bib.bib29)]† | YOLOv3 [[132](#bib.bib132)] | $\sim$59.1* [[91](#bib.bib91)]
    | ResNet34-UNet [[48](#bib.bib48), [138](#bib.bib138)] 架构 | $\sim$21.5* [[79](#bib.bib79)]
    | $\sim$80.6 | -NA- | -NA- | -NA- |'
- en: '| Lotti et al. [[92](#bib.bib92)]† | MobileDet [[182](#bib.bib182)] | 3.3 |
    Regression head with an EfficientNet-Lite [[159](#bib.bib159)] backbone | -NA-
    | 15.4 | EPnP + RANSAC optimised by Levenberg-Marquardt solver | 0.0340 | 0.5200
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Lotti 等人 [[92](#bib.bib92)]† | MobileDet [[182](#bib.bib182)] | 3.3 | 使用
    EfficientNet-Lite [[159](#bib.bib159)] 骨干的回归头 | -NA- | 15.4 | 通过 Levenberg-Marquardt
    解算器优化的 EPnP + RANSAC | 0.0340 | 0.5200 |'
- en: '| Kecen et al. [[81](#bib.bib81)]† | YOLOX-Tiny [[40](#bib.bib40)] | $\sim$5.06 [[40](#bib.bib40)]
    | FPN [[85](#bib.bib85)] architecture with CSPDarknet53[[13](#bib.bib13)] as the
    backbone | $\sim$27.6 [[13](#bib.bib13)] | $\sim$32.66 | EPnP | 0.0049 | 0.0129
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Kecen 等人 [[81](#bib.bib81)]† | YOLOX-Tiny [[40](#bib.bib40)] | $\sim$5.06 [[40](#bib.bib40)]
    | 采用 CSPDarknet53[[13](#bib.bib13)] 作为骨干的 FPN [[85](#bib.bib85)] 架构 | $\sim$27.6 [[13](#bib.bib13)]
    | $\sim$32.66 | EPnP | 0.0049 | 0.0129 |'
- en: '| CA-SpaceNet [[172](#bib.bib172)] | Not used | -NA- | Keypoint prediction
    head having three FPNs [[85](#bib.bib85)] with two DarkNet-53[[132](#bib.bib132)]
    networks as the backbones | -NA- | 51.29 M † | PnP | -NA- | -NA- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| CA-SpaceNet [[172](#bib.bib172)] | 未使用 | -NA- | 具有两个 DarkNet-53[[132](#bib.bib132)]
    网络作为骨干的三个 FPN [[85](#bib.bib85)] 的关键点预测头 | -NA- | 51.29 M † | PnP | -NA- | -NA-
    |'
- en: '| Legrand et al. [[75](#bib.bib75)]† | An ideal object detector assumed | -NA-
    | DarkNet-53 [[132](#bib.bib132)] pre-trained on Linemod[[49](#bib.bib49)] with
    two decoding heads - a segmentation head and a regression head | 71.2 | -NA- |
    PIN architecture [[55](#bib.bib55)] consists of an MLP that aggregates local features
    per keypoint into a single representation | 0.201 | 4.687 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Legrand 等人 [[75](#bib.bib75)]† | 假设为理想的物体检测器 | -NA- | 预训练于 Linemod[[49](#bib.bib49)]
    的 DarkNet-53 [[132](#bib.bib132)]，具有两个解码头 - 分割头和回归头 | 71.2 | -NA- | PIN 架构 [[55](#bib.bib55)]
    包含一个 MLP，将每个关键点的局部特征聚合成一个单一表示 | 0.201 | 4.687 |'
- en: ^+ Results from KSPEC first edition [[71](#bib.bib71)]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^+ 来自 KSPEC 第一版的结果 [[71](#bib.bib71)]
- en: '**Backbone shared between the object detector and the keypoint prediction model'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**物体检测器和关键点预测模型共享骨干网络**'
- en: †Best performing variant considered
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: †考虑的最佳表现变体
- en: 'Table 2: Summary of direct end-to-end algorithms for spacecraft pose estimation.
    Details of the network architectures used, along with an estimated number of parameters,
    are presented. The error values on the SPEED synthetic test set are reported where
    available. In cases, the number of parameters is not reported by the authors,
    an estimated number of parameters based on the backbone models used are given.
    Additionally, the links to the publicly available algorithms are included in [Section A.1](#A1.SS1
    "A.1 Publicly available algorithm implementations ‣ Appendix A Additional Information
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：航天器姿态估计的直接端到端算法总结。提供了使用的网络架构的详细信息及估计的参数数量。报告了 SPEED 合成测试集上的误差值（如果可用）。在作者未报告参数数量的情况下，给出了基于所用骨干模型的估计参数数量。此外，[A.1
    节](#A1.SS1 "A.1 公开可用的算法实现 ‣ 附录 A 附加信息 ‣ 关于基于深度学习的单目航天器姿态估计的调查：现状、局限性和前景") 包括了公开可用算法的链接。
- en: '| Reference | Model architecture | Parameters      (millions) | Mean position
    error (E[t])       (m) | Mean rotation error (E[R])       (deg) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 模型架构 | 参数      （百万） | 平均位置误差 (E[t])       （米） | 平均旋转误差 (E[R])      
    （度） |'
- en: '| Sharma et al. [[145](#bib.bib145)]† | AlexNet [[74](#bib.bib74)] with half
    as many kernels per layer as the original AlexNet architecture, with the last
    fully connected layer containing as many neurons as the number of pose labels
    | $\sim$20.8 | 0.83 (Imitation-25 dataset) | 14.35 (Imitation-25 dataset) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Sharma 等人 [[145](#bib.bib145)]† | AlexNet [[74](#bib.bib74)]，每层的卷积核数为原始 AlexNet
    架构的一半，最后一层全连接层的神经元数与姿态标签数相同 | $\sim$20.8 | 0.83 (Imitation-25 数据集) | 14.35 (Imitation-25
    数据集) |'
- en: '| SPN [[146](#bib.bib146)] | A 5-layer CNN with 3 sub-branches for bounding
    box classification and regression, relative orientation classification and relative
    orientation weights regression. | -NA- | 0.7832 | 8.4254 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SPN [[146](#bib.bib146)] | 5 层 CNN，具有 3 个子分支用于边界框分类和回归、相对方向分类和相对方向权重回归。 |
    -NA- | 0.7832 | 8.4254 |'
- en: '| SPNv2[[110](#bib.bib110)]† | Bi-directional Feature Pyramid Network (BiFPN) [[160](#bib.bib160)]
    with EfficientNet[[159](#bib.bib159)] backbone and with multi-task head networks
    shared by the features at all scales. | 52.5 | 0.031 (SPEED+) | 0.885 (SPEED+)
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| SPNv2 [[110](#bib.bib110)]† | 双向特征金字塔网络 (BiFPN) [[160](#bib.bib160)]，具有 EfficientNet
    [[159](#bib.bib159)] 骨干网，并具有由所有尺度上的特征共享的多任务头网络。 | 52.5 | 0.031 (SPEED+) | 0.885
    (SPEED+) |'
- en: '| URSONet [[124](#bib.bib124)] | ResNet18, ResNet34, ResNet50, ResNet101 [[48](#bib.bib48)]
    base networks with 2 sub-branch networks for position regression and probabilistic
    orientation estimation via soft classification. | $\sim$11.4 to $\sim$42.8 [[78](#bib.bib78)]
    ($\sim$500**) | 0.1450^+ | 2.4900^+ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| URSONet [[124](#bib.bib124)] | ResNet18、ResNet34、ResNet50、ResNet101 [[48](#bib.bib48)]
    基础网络，具有两个子分支网络，用于位置回归和通过软分类的概率方向估计。 | $\sim$11.4 至 $\sim$42.8 [[78](#bib.bib78)]
    ($\sim$500**) | 0.1450^+ | 2.4900^+ |'
- en: '| Mobile-URSONet [[121](#bib.bib121)]† | MobileNet-v2 [[142](#bib.bib142)]
    based network, pre-trained on ImageNet[[31](#bib.bib31)], with 2 sub-branches
    for position regression and probabilistic orientation estimation via soft classification.
    | 7.4 | 0.5600 | 6.2900 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Mobile-URSONet [[121](#bib.bib121)]† | 基于 MobileNet-v2 [[142](#bib.bib142)]
    的网络，在 ImageNet [[31](#bib.bib31)] 上预训练，具有两个子分支用于位置回归和通过软分类的概率方向估计。 | 7.4 | 0.5600
    | 6.2900 |'
- en: '| LSPnet [[38](#bib.bib38)] | ResNet50 [[48](#bib.bib48)] base architecture
    for position regression followed by an up-sampling CNN for object localisation
    and a second ResNet50 for orientation regression. | $\sim$47.8 [[78](#bib.bib78)]
    | 0.4560 | 13.9600 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| LSPnet [[38](#bib.bib38)] | ResNet50 [[48](#bib.bib48)] 基础架构用于位置回归，后跟一个上采样
    CNN 用于目标定位，以及第二个 ResNet50 用于方向回归。 | $\sim$47.8 [[78](#bib.bib78)] | 0.4560 | 13.9600
    |'
- en: '| Huang et al. [[59](#bib.bib59)] | ResNet50 [[48](#bib.bib48)] base network
    with 3 sub-branch networks for object detection, position regression and orientating
    soft classification. | $\sim$23.9 [[78](#bib.bib78)] | 0.1715 (URSO-OrViS datast)
    | 4.3820 (URSO-OrViS dataset) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等人 [[59](#bib.bib59)] | ResNet50 [[48](#bib.bib48)] 基础网络，具有 3 个子分支网络，用于目标检测、位置回归和方向软分类。
    | $\sim$23.9 [[78](#bib.bib78)] | 0.1715 (URSO-OrViS 数据集) | 4.3820 (URSO-OrViS
    数据集) |'
- en: '| Phisannupawong et al. [[119](#bib.bib119)] | A modified version of GoogLeNet [[157](#bib.bib157)]
    that forms a general pose estimation model as implemented in PoseNet[[70](#bib.bib70)].
    The softmax classifiers in the original GoogLeNet were replaced with affine regressors
    and each fully connected layer was modified to output a 7D pose vector. | $\sim$7.0
    | 1.1915^# (URSO-OrViS dataset) | 13.7043^# (URSO-OrViS dataset) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Phisannupawong et al. [[119](#bib.bib119)] | 一个修改版的 GoogLeNet [[157](#bib.bib157)]，作为通用姿态估计模型实现于
    PoseNet[[70](#bib.bib70)]。原始 GoogLeNet 中的 softmax 分类器被仿射回归器替代，每个全连接层被修改为输出 7D
    姿态向量。 | $\sim$7.0 | 1.1915^# (URSO-OrViS 数据集) | 13.7043^# (URSO-OrViS 数据集) |'
- en: '| E-PoseNet [[103](#bib.bib103)] | PoseNet architecture [[69](#bib.bib69)]
    with SE(2)-equivariant ResNet18 backbone[[175](#bib.bib175)]. | 14.1 | 0.1806
    | 2.3073 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| E-PoseNet [[103](#bib.bib103)] | PoseNet 架构 [[69](#bib.bib69)] 采用 SE(2)-等变
    ResNet18 主干[[175](#bib.bib175)]。 | 14.1 | 0.1806 | 2.3073 |'
- en: †Details of the best-performing variant reported
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: †报告的最佳表现变体的详细信息
- en: ^+Results from KSPEC first edition [[71](#bib.bib71)].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ^+来自 KSPEC 第一版的结果 [[71](#bib.bib71)]。
- en: '**Number of parameters in the best performing ensemble of models reported by
    the authors'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者报告的最佳表现模型集的参数数量**'
- en: ^#Median values reported
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^#报告的中位数值
- en: 'Table 3: Review of recent spacecraft pose estimation datasets, sorted by year.
    The Syn/Lab/Space column is, the number of synthetic, lab and space-borne images
    in the dataset, respectively. The Spacecraft column specifies the spacecraft used
    in the dataset. The resolution column corresponds to the width x height of the
    images, in pixels. The I column indicates if the images are RGB (C) or grey-scale
    (G). The Range column indicates the distance between the camera and the spacecraft.
    The Tools column is a list of the rendering software used to generate the synthetic
    data. Additionally, the links to the publicly available datasets are included
    in [Section A.2](#A1.SS2 "A.2 Links to the publicly available datasets ‣ Appendix
    A Additional Information ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3：最近的航天器姿态估计数据集回顾，按年份排序。Syn/Lab/Space 列表示数据集中合成的、实验室的和空间中的图像数量。Spacecraft
    列指定数据集中使用的航天器。分辨率列对应于图像的宽 x 高，单位为像素。I 列指示图像是 RGB (C) 还是灰度 (G)。Range 列指示相机与航天器之间的距离。Tools
    列是生成合成数据所用渲染软件的列表。此外，公开数据集的链接包含在[Section A.2](#A1.SS2 "A.2 Links to the publicly
    available datasets ‣ Appendix A Additional Information ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")。'
- en: '| Dataset | Year | Syn/Lab/Space | Spacecraft | Resolution | I | Range | Tools
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | Syn/Lab/Space | 航天器 | 分辨率 | I | 范围 | 工具 |'
- en: '| SHIRT [[109](#bib.bib109)] | 2022 | 5k/5k/- | Tango | 1920x1200 | G | $\leq$8m
    | OpenGL |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SHIRT [[109](#bib.bib109)] | 2022 | 5k/5k/- | Tango | 1920x1200 | G | $\leq$8m
    | OpenGL |'
- en: '| SPARK2-Stream2 [[127](#bib.bib127)] | 2022 | 30k / 900 / - | Proba-2 | 1440
    x 1080 | C | [1.5m,10m] | Blender |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SPARK2-Stream2 [[127](#bib.bib127)] | 2022 | 30k / 900 / - | Proba-2 | 1440
    x 1080 | C | [1.5m,10m] | Blender |'
- en: '| COSMO [[93](#bib.bib93)] | 2022 | 15k / - / - | COSMO-SkyMed | 1920 x 1200
    | C | [36m,70m] | Blender |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| COSMO [[93](#bib.bib93)] | 2022 | 15k / - / - | COSMO-SkyMed | 1920 x 1200
    | C | [36m,70m] | Blender |'
- en: '| SwissCube [[57](#bib.bib57)] | 2021 | 50k / - / - | SwissCube | 1024 x 1024
    | C | [0.1m, 1m] | Mitsuba 2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| SwissCube [[57](#bib.bib57)] | 2021 | 50k / - / - | SwissCube | 1024 x 1024
    | C | [0.1m, 1m] | Mitsuba 2 |'
- en: '| SPEED+ [[113](#bib.bib113)] | 2021 | 60k / 10k / - | Tango | 1920 × 1200
    | G | $\leq$ 10m | OpenGL |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| SPEED+ [[113](#bib.bib113)] | 2021 | 60k / 10k / - | Tango | 1920 × 1200
    | G | $\leq$ 10m | OpenGL |'
- en: '| Cygnus [[12](#bib.bib12)] | 2021 | 20k / - / 540 | Cygnus | 1024 × 1024 |
    C | [35m,75m] | Blender |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Cygnus [[12](#bib.bib12)] | 2021 | 20k / - / 540 | Cygnus | 1024 × 1024 |
    C | [35m,75m] | Blender |'
- en: '| SPEED [[71](#bib.bib71)] | 2020 | 15k / 305 /- | Tango | 1920 × 1200 | G
    | [3m,40.5m] | OpenGL |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SPEED [[71](#bib.bib71)] | 2020 | 15k / 305 /- | Tango | 1920 × 1200 | G
    | [3m,40.5m] | OpenGL |'
- en: '| URSO [[124](#bib.bib124)] | 2019 | 15k / - / - | Dragon, Soyuz | 1080 x 960
    | C | [10m,40m] | Unreal Engine 4 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| URSO [[124](#bib.bib124)] | 2019 | 15k / - / - | Dragon, Soyuz | 1080 x 960
    | C | [10m,40m] | Unreal Engine 4 |'
- en: '| PRISMA12K [[115](#bib.bib115)] | 2019 | 12k / - / - | Tango | 752 x 580 |
    G | - | OpenGL |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| PRISMA12K [[115](#bib.bib115)] | 2019 | 12k / - / - | Tango | 752 x 580 |
    G | - | OpenGL |'
- en: '| PRISMA12K-TR [[115](#bib.bib115)] | 2019 | 12k / - / - | Tango | 752 x 580
    | G | - | OpenGL |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| PRISMA12K-TR [[115](#bib.bib115)] | 2019 | 12k / - / - | Tango | 752 x 580
    | G | - | OpenGL |'
- en: '| Sharma et. al. [[145](#bib.bib145)] | 2018 | 500k / - / - | Tango | 227 x
    227 | C | [3m,12m] | OpenGL |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Sharma et. al. [[145](#bib.bib145)] | 2018 | 500k / - / - | Tango | 227 x
    227 | C | [3m,12m] | OpenGL |'
- en: 3 Datasets
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个数据集
- en: The use of DL models in spacecraft pose estimation necessitates proper training
    to achieve the robust performance demanded by space applications. The quality
    of the datasets is likely equally influential in DL model performance compared
    to designing an effective DL algorithm to reach the intended performance. Large
    datasets [[84](#bib.bib84), [140](#bib.bib140)] with a wide range of application
    scenarios are usually considered to train DL models, which helps them generalise
    well for unseen scenarios. Though DL algorithms are evolving towards few-shot
    [[152](#bib.bib152)] and zero-shot [[18](#bib.bib18)] learning, solving 6 DoF
    pose prediction problems with high accuracy still depends on large datasets with
    images spanning a wide range of scenarios [[134](#bib.bib134), [179](#bib.bib179)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在航天器姿态估计中使用深度学习模型需要适当的训练，以实现空间应用所要求的强大性能。数据集的质量可能与设计有效的深度学习算法以达到预期性能一样，对深度学习模型的性能有影响。通常，使用包含各种应用场景的大型数据集[[84](#bib.bib84),
    [140](#bib.bib140)]来训练深度学习模型，这有助于它们在未见过的场景中良好地泛化。尽管深度学习算法正在向少样本[[152](#bib.bib152)]和零样本[[18](#bib.bib18)]学习发展，但高精度解决6自由度姿态预测问题仍然依赖于包含各种场景的大型数据集[[134](#bib.bib134),
    [179](#bib.bib179)]。
- en: Currently, there is a lack of publicly available space-borne image datasets.
    This limits the application of DL models and their validation to specific targets
    where actual space-borne images are available and to a limited range of operation
    scenarios. To overcome this limitation, image rendering tools are the preferred
    way to generate realistic space-borne images and testbeds are considered for on-ground
    validation. The rendering tools help generate thousands of images for a wide range
    of targets with annotations for any user-defined applications such as object detection,
    semantic segmentation and 6 DoF pose estimation. These generation tools also provide
    a lot of flexibility to adapt parameters such as camera models, orbital lighting
    conditions, etc., depending on the final use-case application.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，缺乏公开的空间载荷图像数据集。这限制了深度学习模型的应用和验证，仅适用于实际空间图像可用的特定目标以及有限的操作场景。为了克服这一限制，图像渲染工具是生成逼真的空间载荷图像的首选方式，测试平台则被视为地面验证的方法。渲染工具帮助生成成千上万张图像，涵盖各种目标，并带有注释，适用于任何用户定义的应用，如物体检测、语义分割和6自由度姿态估计。这些生成工具还提供了很多灵活性，可以根据最终的使用案例调整参数，如相机模型、轨道照明条件等。
- en: Spacecraft pose estimation algorithms are usually part of vision-based navigation
    systems and are validated in a dedicated testbed facility that can simulate the
    orbital relative motion using robotic arms [[116](#bib.bib116), [108](#bib.bib108)]
    or air-bearing [[141](#bib.bib141)] platforms under realistic space lighting conditions.
    The target mock-up used in such facilities will be scaled or original depending
    on various factors, including the size of the facility, mock-up size, application
    scenario, etc. While synthetic imagery can be mass-produced to address any requirements,
    the images produced from testbed scenarios are limited to a certain extent. It
    includes the Earth in the background, the accurate position of the sun, earth’s
    albedo; such characteristics differentiate the lab/testbed imagery from the actual
    space imagery.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 航天器姿态估计算法通常是基于视觉的导航系统的一部分，并在能够模拟轨道相对运动的专用测试平台上验证，如使用机器人臂[[116](#bib.bib116),
    [108](#bib.bib108)]或气垫[[141](#bib.bib141)]平台，模拟逼真的空间照明条件。在这些设施中使用的目标模型会根据各种因素，如设施的大小、模型的大小、应用场景等，进行缩放或保持原样。虽然合成图像可以大规模生产以满足任何需求，但测试平台场景生成的图像在某种程度上是有限的。它包括背景中的地球、太阳的准确位置、地球的反射率；这些特征使实验室/测试平台的图像与实际空间图像有所不同。
- en: From the above discussion, it is evident that the spacecraft pose estimation
    deals with images from three domains (i.e., synthetic, lab and actual space imagery)
    during the development, testing/validation and deployment phases. It is the nature
    of the DL models to overfit the model to the features specific to the training
    domain, and this challenge is well-known in the literature as domain gap [[34](#bib.bib34),
    [170](#bib.bib170)] problem. So, the algorithms need to consider the aspect of
    domain generalisation from the data viewpoint to improve the algorithm’s performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述讨论可以看出，航天器姿态估计涉及在开发、测试/验证和部署阶段处理来自三个领域（即合成、实验室和实际空间图像）的图像。DL模型容易对训练领域特定的特征进行过拟合，这一挑战在文献中被称为领域间隔[[34](#bib.bib34),
    [170](#bib.bib170)]问题。因此，算法需要从数据的角度考虑领域泛化，以提升算法的最终性能。
- en: '![Refer to caption](img/5276e30977cdb110ab327ab8b270dd5a.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5276e30977cdb110ab327ab8b270dd5a.png)'
- en: (a)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/8268777ac3436edf93bf69d77a9411d1.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8268777ac3436edf93bf69d77a9411d1.png)'
- en: (b)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 15: (a) SnT Zero-G Lab at the University of Luxembourg [[116](#bib.bib116)]
    (b) TRON facility at Stanford University [[108](#bib.bib108)]'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图15: (a) 卢森堡大学的SnT零重力实验室 [[116](#bib.bib116)] (b) 斯坦福大学的TRON设施 [[108](#bib.bib108)]'
- en: 3.1 Summary of Datasets, Simulators & Testbeds
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集、模拟器与测试平台概述
- en: This section provides a summary of the spacecraft pose estimation datasets,
    simulators and rendering tools for synthetic image generation, and testbeds for
    validation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了航天器姿态估计数据集、模拟器和用于合成图像生成的渲染工具，以及验证的测试平台。
- en: 'Datasets: [Table 3](#S2.T3 "In 2.4.3 Robustness to Illumination Conditions
    ‣ 2.4 Limitations ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects") summarises the properties
    of the major spacecraft pose estimation datasets. The properties of the datasets
    include the number of images, the target spacecraft model, image resolution, annotations
    and the rendering tools used for the synthetic image generation. The number of
    images in the currently available spacecraft pose estimation datasets is between
    $10^{4}$ and $10^{5}$. This is relatively low compared to some typical datasets
    used for other machine learning tasks such as image classification and object
    detection. The COCO [[86](#bib.bib86)] dataset, one of the standard datasets used
    for object detection, contains ~300k images. ImageNet dataset [[31](#bib.bib31)]
    primarily used for classification contains ~14M images. Similarly, YCB dataset
    [[179](#bib.bib179)], a recent generic dataset for 6 DoF pose estimation, has
    ~133k images.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集: [表3](#S2.T3 "在2.4.3对光照条件的鲁棒性 ‣ 2.4 局限性 ‣ 2 算法 ‣ 基于深度学习的单目航天器姿态估计综述: 现状、局限性与展望")总结了主要航天器姿态估计数据集的属性。这些属性包括图像数量、目标航天器模型、图像分辨率、注释和用于合成图像生成的渲染工具。目前可用的航天器姿态估计数据集中的图像数量在$10^{4}$到$10^{5}$之间。这与用于其他机器学习任务（如图像分类和目标检测）的典型数据集相比相对较少。COCO
    [[86](#bib.bib86)] 数据集，作为目标检测的标准数据集之一，包含约30万张图像。ImageNet数据集 [[31](#bib.bib31)]
    主要用于分类，包含约1400万张图像。同样，YCB数据集 [[179](#bib.bib179)]，一个近期用于6自由度姿态估计的通用数据集，包含约13.3万张图像。'
- en: The target spacecraft model used in the datasets also plays a vital role in
    determining the dataset characteristics. For example, a smaller target size will
    lead to a smaller operation range and vice-versa. The TANGO satellite [[33](#bib.bib33)]
    model used in the multiple datasets [[113](#bib.bib113), [109](#bib.bib109), [71](#bib.bib71),
    [115](#bib.bib115), [115](#bib.bib115), [145](#bib.bib145)] has a coarse dimension
    of 80×75×32cm will lead to the operation range of ~10m. However, for Soyuz or
    Cygnus models in other datasets increases the operating range to 40 ~ 80 m. Similar
    constraints will apply to testbed data as well. A 1:1 mockup scale of TANGO spacecraft
    in SPEED+ [[113](#bib.bib113)] leads to a lower range in the lab-generated images
    due to the size constraint of the facility. Usually, a scaled mock-up is considered
    a solution to increase the validation range in the testbed scenarios.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中使用的目标航天器模型在确定数据集特性中也起着至关重要的作用。例如，较小的目标尺寸将导致操作范围变小，反之亦然。多数据集中使用的TANGO卫星[[33](#bib.bib33)]模型，其粗略尺寸为80×75×32cm，将导致操作范围约为10m。然而，对于其他数据集中使用的Soyuz或Cygnus模型，操作范围将增加到40
    ~ 80m。类似的限制也适用于测试台数据。在SPEED+ [[113](#bib.bib113)]中，TANGO航天器的1:1模拟模型由于设施的尺寸限制，会导致实验室生成图像的范围较低。通常，缩放的模型被认为是增加测试台场景验证范围的解决方案。
- en: 'The level of annotations may vary for different datasets; for spacecraft pose
    estimation applications, each image in the dataset must be appropriately annotated
    with corresponding relative 6DoF pose labels. All the datasets mentioned in [Table 3](#S2.T3
    "In 2.4.3 Robustness to Illumination Conditions ‣ 2.4 Limitations ‣ 2 Algorithms
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects") are adequately annotated with 6DoF pose labels.
    However, the hybrid algorithm approach discussed in [Section 2.1](#S2.SS1 "2.1
    Hybrid Modular Approaches ‣ 2 Algorithms ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects") demands
    secondary annotations such as the bounding boxes and the keypoints. To recover
    the secondary annotations from pose labels, it is necessary to have 3D information
    on the edges or vertices of the target. Even for the standard datasets such as
    SPEED [[71](#bib.bib71)] and SPEED+ [[113](#bib.bib113)], the only way to use
    a hybrid approach is to recover the 3D locations of interested keypoints is via
    the 3D reconstruction methods [[23](#bib.bib23)]. These recovered keypoints will
    be used to construct secondary annotations such as bounding boxes, keypoints,
    segmentation masks and even ellipse heatmap annotations [[39](#bib.bib39)]. The
    lack of secondary annotations can be an issue for multi-task learning approaches
    where the annotations (such as segmentation masks) could be used to define auxiliary
    tasks intended to prevent learning domain-specific features to improve generalisation
    [[110](#bib.bib110)]. Several learning-based approaches are evolving to generate
    secondary annotation to address the label scarcity, such as depth estimation using
    a single image depth estimator [[98](#bib.bib98)] and an image segmentation technique [[184](#bib.bib184)].
    Some self-supervised approaches are evolving as an alternate way to get bounding
    box annotations for a single target in the image [[174](#bib.bib174)]. Though
    these approaches aid annotations, they cannot replace the properly calibrated
    annotations recorded during synthetic data generation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注释的级别可能因数据集而异；对于航天器姿态估计应用，数据集中的每张图像必须用相应的6自由度姿态标签进行适当注释。[表3](#S2.T3 "在2.4.3对光照条件的鲁棒性
    ‣ 2.4局限性 ‣ 2算法 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景")中提到的所有数据集均已充分注释了6DoF姿态标签。然而，[第2.1节](#S2.SS1
    "2.1混合模块化方法 ‣ 2算法 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景")中讨论的混合算法方法要求附加注释，如边界框和关键点。为了从姿态标签中恢复附加注释，需要对目标的边缘或顶点有3D信息。即使对于标准数据集如SPEED
    [[71](#bib.bib71)] 和 SPEED+ [[113](#bib.bib113)]，使用混合方法的唯一方式是通过3D重建方法 [[23](#bib.bib23)]
    恢复感兴趣关键点的3D位置。这些恢复的关键点将用于构建附加注释，如边界框、关键点、分割掩膜甚至椭圆热图注释 [[39](#bib.bib39)]。缺乏附加注释可能成为多任务学习方法中的问题，其中注释（如分割掩膜）可以用于定义辅助任务，以防止学习领域特定的特征，从而提高泛化能力
    [[110](#bib.bib110)]。几种基于学习的方法正在发展，以生成附加注释以解决标签稀缺问题，如使用单张图像深度估计器的深度估计 [[98](#bib.bib98)]
    和图像分割技术 [[184](#bib.bib184)]。一些自监督方法正作为获取图像中单个目标边界框注释的替代方法 [[174](#bib.bib174)]。虽然这些方法有助于注释，但它们不能替代在合成数据生成过程中记录的经过适当校准的注释。
- en: 'Simulators and Rendering Tools: Computer graphics allow us to create realistic
    images of objects based on high-quality textures using ray tracing. Ray Tracing
    techniques mimic how light interacts with the real world and rely on evaluating
    and simulating the path of view lines from the observer camera to objects in the
    field of view. This simulation enables the calculation of the light intensity
    of associated pixels. Several efforts were made towards creating simulators for
    space applications. Realistic image simulation tools were used in previous missions
    to aid vision-based navigation in space/planetary environments (such as the Lunar
    environment, Asteroid surface), and it includes the PANGU (Planet and Asteroid
    Natural scene Generation Utility) [[95](#bib.bib95)] and the SurRender [[14](#bib.bib14)]
    by Airbus. The University of Dundee has developed the PANGU simulation tool, which
    generates realistic, high-quality, synthetic surface images of planets and asteroids.
    PANGU uses a custom GPU-based renderer to render the scene. Airbus’s Surrender
    can be used in two modes of image rendering, ray tracing and OpenGL[[149](#bib.bib149)].
    It can produce physically accurate images providing the known irradiance (each
    pixel contains an irradiance value expressed in W/m2). Other general rendering
    tools such as Blender [[93](#bib.bib93)], Unreal Engine [[124](#bib.bib124)],
    and Mitsuba [[57](#bib.bib57)] were also used to generate synthetic images. The
    main issue with these tools is that they are designed for general usage and are
    not customised for space imagery. A brief comparison of rendering tools for synthetic
    imagery was provided in [[128](#bib.bib128)]. Recently, efforts [[71](#bib.bib71)],
    [[5](#bib.bib5)] have been made toward developing simulation tools specific for
    the purpose of synthetic image generation for spacecraft pose estimation. SPEED
    and SPEED+ images are obtained using the Optical Simulator [[8](#bib.bib8)], based
    on an OpenGL rendering pipeline. The images from SPEED and SPEED+ are validated
    against the real images of TANGO spacecraft from the PRISMA mission using histogram
    comparison [[71](#bib.bib71)]. However, to our knowledge, no tool can be considered
    a de facto standard to generate space imagery for spacecraft pose estimation.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器和渲染工具：计算机图形学使我们能够基于高质量纹理使用**光线追踪**创建对象的逼真图像。光线追踪技术模拟了光线如何与现实世界互动，并依赖于评估和模拟从观察者摄像头到视野中物体的视线路径。这种模拟使得可以计算相关像素的光强度。针对太空应用的模拟器开发已做出了多项努力。在以往的任务中，逼真图像模拟工具被用来辅助空间/行星环境中的基于视觉的导航（如月球环境、陨石表面），其中包括由Airbus开发的PANGU（行星和小行星自然场景生成工具）[[95](#bib.bib95)]和SurRender
    [[14](#bib.bib14)]。邓迪大学开发了PANGU模拟工具，它生成行星和小行星的逼真、高质量的合成表面图像。PANGU使用定制的基于GPU的渲染器来渲染场景。Airbus的Surrender可以在两种图像渲染模式下使用，即光线追踪和OpenGL[[149](#bib.bib149)]。它可以生成物理上准确的图像，提供已知的辐照度（每个像素包含以W/m2表示的辐照度值）。其他通用渲染工具，如Blender
    [[93](#bib.bib93)]、Unreal Engine [[124](#bib.bib124)]和Mitsuba [[57](#bib.bib57)]，也被用来生成合成图像。这些工具的主要问题在于它们是为通用使用设计的，并未针对太空图像进行定制。对合成图像渲染工具的简要比较见[[128](#bib.bib128)]。最近，已有努力[[71](#bib.bib71)]、[[5](#bib.bib5)]致力于开发专门用于生成空间飞行器姿态估计的合成图像的模拟工具。SPEED和SPEED+图像是使用光学模拟器[[8](#bib.bib8)]获得的，基于OpenGL渲染管道。SPEED和SPEED+的图像通过直方图比较[[71](#bib.bib71)]与PRISMA任务中的TANGO空间飞行器的真实图像进行验证。然而，据我们所知，目前尚无工具被认为是生成空间图像以进行空间飞行器姿态估计的**事实标准**。
- en: 'Testbeds: In spacecraft pose estimation, collecting images from space for training
    and evaluating algorithms is extremely difficult and expensive. Laboratory testbeds
    (see [Figure 15](#S3.F15 "In 3 Datasets ‣ A Survey on Deep Learning-Based Monocular
    Spacecraft Pose Estimation: Current State, Limitations and Prospects")) are considered
    as an alternative to replicate relative motion and orbital lighting conditions.
    [Table 4](#S3.T4 "In 3.1 Summary of Datasets, Simulators & Testbeds ‣ 3 Datasets
    ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current
    State, Limitations and Prospects"), summarises different laboratory testbed facilities
    based on their size, manipulation capabilities, tracking systems, perception sensors
    and orbital motion simulations. Some of the SoTA testbed includes The Robotic
    Testbed for Rendezvous and Optical Navigation (TRON) at Stanford’s Space Rendezvous
    Laboratory (SLAB) [[108](#bib.bib108)], STAR Lab at the University of Surrey [[128](#bib.bib128)],
    SnT Zero-G Lab at the University of Luxembourg [[116](#bib.bib116)], GMV Platform-Art [[28](#bib.bib28),
    [42](#bib.bib42)], German Aerospace Center European Proximity Operations Simulator
    2.0 (DLR EPOS) [[10](#bib.bib10)] and European Space Agency’s GNC Rendezvous,
    Approach and Landing Simulator (GRALS) [[21](#bib.bib21)]. These testbeds generally
    have robotic manipulators to carry the payloads. The payloads can be different
    target spacecraft mock-up models or mounted cameras mimicking a chaser. Different
    lighting equipment has been used for simulating space conditions. For instance,
    in SPEED+ [[113](#bib.bib113)], the images collected in a sunlamp setup replicate
    the sun’s bright light, and those collected with a lightbox setup emulate the
    diffused light of the earth’s albedo, respectively. Motion capture systems are
    extensively used to collect pose labels based on the reflective markers attached
    to the target and cameras. However, these motion capture systems should be carefully
    calibrated [[108](#bib.bib108)] to generate accurate ground truth data, which
    can be tedious and time-consuming.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 测试平台：在航天器姿态估计中，从太空收集图像以训练和评估算法是极其困难且昂贵的。实验室测试平台（参见[图15](#S3.F15 "在3个数据集中 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性和前景的调查")）被视为一种替代方案，以复制相对运动和轨道光照条件。[表4](#S3.T4
    "在3.1 数据集、模拟器和测试平台的总结 ‣ 3个数据集 ‣ 基于深度学习的单目航天器姿态估计：当前状态、局限性和前景的调查") 汇总了不同实验室测试平台的大小、操控能力、跟踪系统、感知传感器和轨道运动模拟。某些最先进的测试平台包括斯坦福大学太空对接实验室（SLAB）的对接和光学导航机器人测试平台（TRON）[[108](#bib.bib108)]，萨里大学的STAR
    Lab[[128](#bib.bib128)]，卢森堡大学的SnT零重力实验室[[116](#bib.bib116)]，GMV Platform-Art[[28](#bib.bib28),
    [42](#bib.bib42)]，德国航空航天中心欧洲接近操作模拟器2.0（DLR EPOS）[[10](#bib.bib10)]和欧洲航天局的GNC对接、接近和着陆模拟器（GRALS）[[21](#bib.bib21)]。这些测试平台通常配备机器人操控器来携带有效载荷。有效载荷可以是不同的目标航天器模型或模拟追踪器的安装相机。不同的照明设备被用于模拟太空条件。例如，在SPEED+[[113](#bib.bib113)]中，使用太阳灯设置收集的图像复制了太阳的强光，而使用光箱设置收集的图像模拟了地球反照率的漫射光。运动捕捉系统被广泛使用，以基于附加在目标和相机上的反射标记收集姿态标签。然而，这些运动捕捉系统应该被仔细校准[[108](#bib.bib108)]以生成准确的地面真实数据，这可能是繁琐且耗时的。
- en: 'The next section discusses the major issue with the current spacecraft pose
    estimation datasets: the domain gap between synthetic data used for training and
    the real laboratory/ space-borne images used for testing/ validating and deploying
    the DL-based algorithms.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节讨论了当前航天器姿态估计数据集的主要问题：用于训练的合成数据与用于测试/验证和部署基于深度学习的算法的实际实验室/太空图像之间的领域差距。
- en: 'Table 4: Summary of different laboratory testbed facilities for evaluating
    spacecraft pose estimation algorithms'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：用于评估航天器姿态估计算法的不同实验室测试平台汇总
- en: '| Facility | STAR Lab [[128](#bib.bib128)] | TRON [[108](#bib.bib108)] | SnT
    Zero-G Lab [[116](#bib.bib116)] | GMV Platform-Art^© [[28](#bib.bib28)][[42](#bib.bib42)]
    | DLR EPOS 2.0 [[10](#bib.bib10)] | GRALS [[21](#bib.bib21)] |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 设施 | STAR Lab [[128](#bib.bib128)] | TRON [[108](#bib.bib108)] | SnT Zero-G
    Lab [[116](#bib.bib116)] | GMV Platform-Art^© [[28](#bib.bib28)][[42](#bib.bib42)]
    | DLR EPOS 2.0 [[10](#bib.bib10)] | GRALS [[21](#bib.bib21)] |'
- en: '| Illumination | • Forza 500 LED spotlight | •  LED panels (for diffused light)
    • Metal-halide arc lamp (for sunlight) | •  Godox SL-60 LED Video Light • Aputure
    LS 600d Pro | • Numerically controlled Sun emulator | • Osram ARRI Max 12/18 (with
    a 12 kW hydrargyrum medium-arc iodide lamp) | •  Dimmable, uniform and collimated
    light source |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 照明 | • Forza 500 LED聚光灯 | • LED面板（用于散射光） • 金属卤化物弧灯（用于阳光） | • Godox SL-60
    LED视频灯 • Aputure LS 600d Pro | • 数控太阳模拟器 | • Osram ARRI Max 12/18（配有12 kW汞中弧碘化物灯）
    | • 可调光、均匀且准直的光源 |'
- en: '| Perception Sensor | • FLIR Blackfly (monocular camera) • 2D/3D LIDAR • Intel
    RealSense D435i (RGBD camera) | • Point Grey Grasshopper 3 (monocular camera)
    | • FLIR Blackfly (monocular camera) • Prophesee EKv4 (event camera) • Intel RealSense
    D435i (RGBD camera) | • Optical navigation camera • Industrial laser sensor • A
    set of GPS-like pseudolites | • Prosilica GC-655M (CCD camera) • PMDtec Camcube
    3.0 (PMD camera) • Bluetechnix Argos3D-IRS1020 DLR Prototype (PMD LiDAR) | • Prosilica
    GC2450 (monocular camera) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 感知传感器 | • FLIR Blackfly（单目相机） • 2D/3D LIDAR • Intel RealSense D435i（RGBD相机）
    | • Point Grey Grasshopper 3（单目相机） | • FLIR Blackfly（单目相机） • Prophesee EKv4（事件相机）
    • Intel RealSense D435i（RGBD相机） | • 光学导航相机 • 工业激光传感器 • 一套类似GPS的伪卫星 | • Prosilica
    GC-655M（CCD相机） • PMDtec Camcube 3.0（PMD相机） • Bluetechnix Argos3D-IRS1020 DLR原型（PMD
    LiDAR） | • Prosilica GC2450（单目相机） |'
- en: '| Manipulator (Robotic Arms) | • UR5 | • KUKA | • UR10e | • Mitsubishi PA10-6CE
    • KUKA KR150-2 | • KUKA KR100HA • KUKA KR240-2 | • KUKA • UR5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 操作臂（机器人臂） | • UR5 | • KUKA | • UR10e | • 三菱 PA10-6CE • KUKA KR150-2 | • KUKA
    KR100HA • KUKA KR240-2 | • KUKA • UR5 |'
- en: '| Tracking System | Qualisys | Vicon | OptiTrack | Model-based tracking algorithm
    based on virtual visual servoing & Kanade-Lucas- Tomasi (KLT) feature tracker
    algorithm | VIsion BAsed NAvigation Sensor System (VIBANASS) | VICON |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 追踪系统 | Qualisys | Vicon | OptiTrack | 基于虚拟视觉伺服和Kanade-Lucas-Tomasi (KLT)特征跟踪算法的模型跟踪算法
    | 基于视觉的导航传感器系统（VIBANASS） | VICON |'
- en: '| Background Material | Black background curtains | Light-absorbing black commando
    curtains | Blind made of non-reflective black textile from inside and outside
    | Black curtains fully covering the walls and ceiling | Black curtain and a black
    wrapping of one of the robots made of Molton material | Black background curtains
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 背景材料 | 黑色背景幕布 | 吸光黑色指挥幕布 | 内外均为非反射黑色纺织品制成的百叶窗 | 完全覆盖墙壁和天花板的黑色帘子 | 用Molton材料制成的黑色帘子和其中一个机器人的黑色包裹
    | 黑色背景幕布 |'
- en: '| Simulated Operations | • Proximity | • Rendezvous • Proximity | • Proximity
    • Rendezvous • Orbital maintenance operations | • Rendezvous • Proximity | • Rendezvous
    • Docking/berthing • Proximity | • Rendezvous • Proximity |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 模拟操作 | • 近距离 | • 对接 • 近距离 | • 近距离 • 对接 • 轨道维护操作 | • 对接 • 近距离 | • 对接 • 近距离
    |'
- en: '| Dimensions (WxLxH) | 3mx2mx2.5m | 8m×3m×3m (simulation room) and 6m (track)
    | 5m×3m×2.3m | 15m | 25m (track) | 4m |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸（WxLxH） | 3m×2m×2.5m | 8m×3m×3m（模拟室）和6m（轨道） | 5m×3m×2.3m | 15m | 25m（轨道）
    | 4m |'
- en: '| ROS [[125](#bib.bib125)] Supported | Yes | -NA- | Yes | No | No | No |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 支持ROS [[125](#bib.bib125)] | 是 | -NA- | 是 | 否 | 否 | 否 |'
- en: 3.2 Bridging the Domain Gap
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 弥合领域间隙
- en: 'Any DL-based algorithm trained on a synthetic dataset is likely to suffer from
    a performance drop when tested on real images (whether acquired within a ground-based
    laboratory or in space), which is referred to as the domain gap [[9](#bib.bib9)]
    problem. Following the related computer vision terminology, the training dataset
    arises from a source domain while the test dataset belongs to the target domain.
    More subtly, a domain gap persists even when the real source and target datasets
    are acquired under different (laboratory and space) environmental conditions  [[165](#bib.bib165)].
    To ensure the reliable performance of DL-based spacecraft pose estimation algorithms
    in real-world space missions, it is, therefore, crucial to bridge the domain gap.
    Several methods have been used in spacecraft pose estimation literature for this
    purpose. These methods are classified into two categories:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 任何基于深度学习的算法在合成数据集上训练后，在真实图像（无论是在地面实验室还是在太空中获取的）上的测试表现都可能会下降，这被称为领域间隙问题[[9](#bib.bib9)]。根据相关计算机视觉术语，训练数据集来自源领域，而测试数据集属于目标领域。更微妙的是，即使真实的源数据集和目标数据集在不同的（实验室和太空）环境条件下获取，领域间隙仍然存在[[165](#bib.bib165)]。因此，为了确保基于深度学习的航天器姿态估计算法在实际太空任务中的可靠性能，弥合领域间隙至关重要。在航天器姿态估计文献中已经使用了几种方法来实现这一目标。这些方法被分为两类：
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data level methods: Expanding or adding diversity to the training data by applying
    different techniques to alter the images, such as 1) data augmentation [[110](#bib.bib110)]
    or 2) domain randomization [[115](#bib.bib115)]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据级方法：通过应用不同的技术来改变图像，从而扩展或增加训练数据的多样性，例如 1) 数据增强[[110](#bib.bib110)] 或 2) 领域随机化[[115](#bib.bib115)]
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Algorithm level methods: Adapting the learning procedure of the model by using
    different techniques, such as 3) multi-task learning [[111](#bib.bib111)] or 4)
    adversarial learning [[113](#bib.bib113)], to make the features extracted from
    images as less domain-dependent as possible'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法级方法：通过使用不同的技术来调整模型的学习过程，例如 3) 多任务学习[[111](#bib.bib111)] 或 4) 对抗学习[[113](#bib.bib113)]，使从图像中提取的特征尽可能减少领域依赖性
- en: 3.2.1 Data Augmentation
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 数据增强
- en: '![Refer to caption](img/98f5c1d23886a5715900498c8c8883b9.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98f5c1d23886a5715900498c8c8883b9.png)'
- en: 'Figure 16: Illustration of different data augmentation methods used on the
    same reference image taken from SPARK2 [[127](#bib.bib127)] dataset. Images A,
    B and C show examples of pixel-wise augmentation methods and images D, E and F
    show the application of spatial augmentation methods. The captions refer to the
    corresponding functions used by the Albumentations Python library.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：展示了在相同参考图像上使用的不同数据增强方法的示例，这些图像来自 SPARK2 [[127](#bib.bib127)] 数据集。图像 A、B
    和 C 展示了像素级增强方法的示例，图像 D、E 和 F 展示了空间增强方法的应用。说明文字指的是 Albumentations Python 库使用的相应功能。
- en: 'This involves artificially creating additional training data by applying various
    transformations to the existing data [[102](#bib.bib102)]. This is done to increase
    the size and variations of the training set and to make the model more robust
    to unseen variations in the input data, i.e. to improve the generalisation to
    unseen domains. Data augmentation techniques used in spacecraft pose estimation
    algorithms can be further categorised into:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及通过对现有数据应用各种变换来人工创建额外的训练数据[[102](#bib.bib102)]。这样做是为了增加训练集的大小和多样性，并使模型对输入数据中未见过的变化更具鲁棒性，即提高对未见领域的泛化能力。航天器姿态估计算法中使用的数据增强技术可以进一步分类为：
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pixel-wise data augmentations such as blurring, noising or changing the image
    contrast
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像素级数据增强如模糊、噪声或改变图像对比度
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Spatial-level data augmentations such as rotation or scaling
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间级数据增强如旋转或缩放
- en: 'The main difference between the two categories is their effect on the pose
    labels. The pixel-level augmentations only affect the input image, whereas the
    spatial-level augmentations require modifying both the input image as well as
    the pose label, which can be difficult. Figure [16](#S3.F16 "Figure 16 ‣ 3.2.1
    Data Augmentation ‣ 3.2 Bridging the Domain Gap ‣ 3 Datasets ‣ A Survey on Deep
    Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations
    and Prospects") illustrates different data augmentation techniques (pixel and
    spatial-level) applied to a reference image of PROBA-2 spacecraft from the SPARK2
    [[127](#bib.bib127)] dataset. Finally, even though data augmentation generally
    helps with the domain gap problem, there can be instances when applying data augmentation
    can be counter-productive. For example, the Random Erase augmentation used by
    Park et.al  [[110](#bib.bib110)] is shown to cause a drop in the pose estimation
    performance. Consequently, finding the best set of augmentations for a given context
    is a hard task in itself [[118](#bib.bib118)]. [Table 5](#S3.T5 "In 3.2.1 Data
    Augmentation ‣ 3.2 Bridging the Domain Gap ‣ 3 Datasets ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects")
    provides a summary of data augmentation methods used in spacecraft pose estimation
    algorithms surveyed in this paper.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类别的主要区别在于它们对姿态标签的影响。像素级增强仅影响输入图像，而空间级增强需要同时修改输入图像和姿态标签，这可能很困难。[图 16](#S3.F16
    "图 16 ‣ 3.2.1 数据增强 ‣ 3.2 缩小领域差距 ‣ 3 数据集 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景") 展示了应用于
    PROBA-2 航天器的参考图像的不同数据增强技术（像素级和空间级）。最终，尽管数据增强通常有助于解决领域差距问题，但也可能会出现数据增强适得其反的情况。例如，Park
    等人使用的随机擦除增强[[110](#bib.bib110)] 被证明会导致姿态估计性能下降。因此，为特定上下文找到最佳的增强集合本身就是一项艰巨的任务[[118](#bib.bib118)]。[表
    5](#S3.T5 "在 3.2.1 数据增强 ‣ 3.2 缩小领域差距 ‣ 3 数据集 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景") 提供了本文调查中使用的航天器姿态估计算法的数据增强方法的总结。
- en: 'Table 5: Datasets used and data augmentations applied with different pose estimation
    algorithms'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同姿态估计算法使用的数据集和数据增强
- en: '| Algorithm | Datasets Used | Data Augmentations Applied |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 使用的数据集 | 应用的数据增强 |'
- en: '| EPFL_cvlab [[41](#bib.bib41)] | SPEED | Rotation, addition of random noise,
    zooming and cropping |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| EPFL_cvlab [[41](#bib.bib41)] | SPEED | 旋转，添加随机噪声，缩放和裁剪 |'
- en: '| SLAB Baseline [[115](#bib.bib115)] | SPEED, PRISMA12K, PRISMA25 | Random
    variations in brightness and contrast, random flipping, rotation at 90 degree
    intervals and addition of random Gaussian noise. Also, RoI enlargement and RoI
    shifting are applied specifically for object detector training. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| SLAB基线 [[115](#bib.bib115)] | SPEED, PRISMA12K, PRISMA25 | 亮度和对比度的随机变化，随机翻转，90度间隔旋转以及添加随机高斯噪声。同时，为了物体检测器训练，应用了RoI扩展和RoI平移。
    |'
- en: '| STAR LAB keypoint method [[126](#bib.bib126)] | SPEED, URSO-OrViS | Rotation,
    translation, coarse dropout, addition of Gaussian noise, random brightness and
    contrast variations applied for training keypoint prediction network |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| STAR LAB关键点方法 [[126](#bib.bib126)] | SPEED, URSO-OrViS | 旋转、平移、粗略丢弃、添加高斯噪声、随机亮度和对比度变化，用于训练关键点预测网络
    |'
- en: '| Black et al. [[12](#bib.bib12)] | SPEED, Cygnus | Randomised flips, 90 degree
    rotations and crops applied for object detector training. Random translation and
    expansions, random flips, 90 degree rotations, brightness, contrast, and saturation
    augmentations applied for keypoint prediction training. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Black et al. [[12](#bib.bib12)] | SPEED, Cygnus | 随机翻转、90度旋转和裁剪应用于物体检测器训练。随机平移和扩展、随机翻转、90度旋转、亮度、对比度和饱和度增强应用于关键点预测训练。
    |'
- en: '| Wide-Depth-Range [[57](#bib.bib57)] | SPEED, SwissCube | Random shift, scale
    and rotation |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Wide-Depth-Range [[57](#bib.bib57)] | SPEED, SwissCube | 随机位移、缩放和旋转 |'
- en: '| LSPnet [[38](#bib.bib38)] | SPEED | Centre data augmentation |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| LSPnet [[38](#bib.bib38)] | SPEED | 中心数据增强 |'
- en: '| URSONet [[124](#bib.bib124)] | SPEED, URSO-OrViS | Change in image exposure
    and contrast, addition of Additive White Gaussian (AWG) noise, blurring and drop
    out of patches, random camera orientation perturbations and random plane rotations
    (only for SPEED dataset) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| URSONet [[124](#bib.bib124)] | SPEED, URSO-OrViS | 图像曝光和对比度的变化，添加加性白高斯（AWG）噪声，模糊和丢弃补丁，随机相机方向扰动和随机平面旋转（仅适用于SPEED数据集）
    |'
- en: '| Mobile-URSONet [[121](#bib.bib121)] | SPEED | Random rotation of the camera
    across the roll axis with a maximum magnitude of 25 degrees, Gaussian blur, random
    changes to brightness, contrast, saturation, and hue |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Mobile-URSONet [[121](#bib.bib121)] | SPEED | 在滚动轴上进行随机旋转，最大幅度为25度，高斯模糊，随机变化亮度、对比度、饱和度和色调
    |'
- en: '| Huang et al. [[59](#bib.bib59)] | SPEED, URSO | Change in image exposure
    and contrast, addition of AWG noise, blurring and drop out patches, random camera
    orientation perturbations and random plane rotations (only for SPEED) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. [[59](#bib.bib59)] | SPEED, URSO | 图像曝光和对比度的变化，添加AWG噪声，模糊和丢弃补丁，随机相机方向扰动和随机平面旋转（仅适用于SPEED）
    |'
- en: '| Lotti et al. [[92](#bib.bib92)] | SPEED, CPD | Random image rotations, bounding
    box enlargement and shifts, random brightness, and contrast adjustments |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Lotti et al. [[92](#bib.bib92)] | SPEED, CPD | 随机图像旋转、边界框扩展和位移、随机亮度和对比度调整
    |'
- en: '| Kecen et al. [[81](#bib.bib81)] | SPEED | Same as SLAB Baseline |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Kecen et al. [[81](#bib.bib81)] | SPEED | 与SLAB基线相同 |'
- en: '| SPNv2 [[110](#bib.bib110)] | SPEED+ | Style augmentation via neural style
    transfer, brightness and contrast, random erase, sun flare, blur (motion blur,
    median blur, glass blur), noise (Gaussian noise, ISO noise) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SPNv2 [[110](#bib.bib110)] | SPEED+ | 通过神经风格迁移、亮度和对比度、随机擦除、日光晕影、模糊（运动模糊、中值模糊、玻璃模糊）、噪声（高斯噪声、ISO噪声）进行样式增强
    |'
- en: '| Sharma et al. [[145](#bib.bib145)] | PRISMA (Imitation-25) | Horizontal reflection,
    addition of zero mean white Gaussian noise |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Sharma et al. [[145](#bib.bib145)] | PRISMA (Imitation-25) | 水平反射，添加均值为零的白噪声
    |'
- en: '| CA-SpaceNet [[172](#bib.bib172)] | SPEED, SwissCube | Random shift, scale,
    and rotation |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| CA-SpaceNet [[172](#bib.bib172)] | SPEED, SwissCube | 随机位移、缩放和旋转 |'
- en: '| Legrand et al. [[75](#bib.bib75)] | SPEED | Random variations in brightness
    and contrast, Gaussian noise augmentations, random rotations, and random background
    data augmentation |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Legrand et al. [[75](#bib.bib75)] | SPEED | 亮度和对比度的随机变化，高斯噪声增强，随机旋转和随机背景数据增强
    |'
- en: 3.2.2 Domain Randomization
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 领域随机化
- en: 'The goal is to help the model generalise by training it on a set of sufficiently
    randomized source data so that the target domain appears as just another randomization
    to the model [[163](#bib.bib163)]. Hence, the expectation is that the model will
    be less prone to the domain gap [[164](#bib.bib164)]. An example of domain randomization
    in the context of spacecraft pose estimation is provided in [[115](#bib.bib115)],
    where the spacecraft texture is randomized using the Neural Style Transfer (NST)
    technique presented in [[62](#bib.bib62)]. Domain randomization can be seen as
    a particular case of data augmentation: one does not search for a set of augmentations
    relevant to a context, but for a sufficiently varied set of augmentations that
    will make the actual scene appear as just another variation.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过在一组充分随机化的源数据上训练模型来帮助模型进行泛化，从而使目标领域对模型来说只不过是另一种随机化 [[163](#bib.bib163)]。因此，期望模型对领域差距的敏感性降低
    [[164](#bib.bib164)]。在航天器姿态估计的背景下，领域随机化的一个示例见 [[115](#bib.bib115)]，其中使用了 [[62](#bib.bib62)]
    中介绍的神经风格迁移（NST）技术对航天器纹理进行随机化。领域随机化可以看作是数据增强的一种特殊情况：它并不是寻找与特定背景相关的增强集合，而是寻找一个足够多样化的增强集合，使实际场景看起来只是另一种变体。
- en: 3.2.3 Multi-Task Learning
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 多任务学习
- en: 'In this approach, a single DL model is trained to perform multiple related
    tasks (a primary task and several secondary/ auxiliary tasks) simultaneously.
    The assumption here is that the model will generalise better on the primary task
    (spacecraft pose estimation in this context) by being less prone to the noise
    induced by the primary task [[139](#bib.bib139)]. The most common way of implementing
    multi-task learning is to have a shared backbone architecture extracting features
    and feeding these features to the task-specific layers [[110](#bib.bib110)] (see
    [Figure 17](#S3.F17 "In 3.2.3 Multi-Task Learning ‣ 3.2 Bridging the Domain Gap
    ‣ 3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation:
    Current State, Limitations and Prospects")). Here EfficientPose [[16](#bib.bib16)]
    network architecture is modified with the addition of two heads: one for the segmentation
    of the spacecraft and one to compute the 2D heatmaps associated with pre-designated
    keypoints on the spacecraft. The results show that when the model is trained with
    different head configurations, the best performance is reached when all the task
    heads are enabled, thereby showing the effectiveness of multi-task learning. However,
    the authors show that all the heads do not contribute to the same extent; the
    segmentation head only improves the performance slightly. This highlights one
    of the key challenges in multi-task learning: identifying the correct set of secondary
    tasks that is relevant for a particular primary task [[150](#bib.bib150)].'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，单一的深度学习模型被训练来同时执行多个相关任务（一个主要任务和若干次要/辅助任务）。这里的假设是，通过减少主任务引起的噪声，模型在主要任务（在这个背景下是航天器姿态估计）上会有更好的泛化。实现多任务学习的最常见方式是使用共享的骨干网络结构提取特征，并将这些特征传递给任务特定的层
    [[110](#bib.bib110)]（见 [图 17](#S3.F17 "在 3.2.3 多任务学习 ‣ 3.2 缩小领域差距 ‣ 3 数据集 ‣ 基于深度学习的单目航天器姿态估计：现状、局限性和前景综述")）。在这里，EfficientPose
    [[16](#bib.bib16)] 网络架构被修改，增加了两个头：一个用于航天器的分割，另一个用于计算与航天器上预定关键点相关的 2D 热图。结果表明，当模型用不同的头配置进行训练时，当所有任务头都启用时性能最佳，从而显示了多任务学习的有效性。然而，作者表明并不是所有的头都对性能贡献相同；分割头仅稍微改善了性能。这突出了多任务学习中的一个关键挑战：识别与特定主要任务相关的正确次要任务集合
    [[150](#bib.bib150)]。
- en: '![Refer to caption](img/3346f3c421fcccd359bb98339032eb98.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3346f3c421fcccd359bb98339032eb98.png)'
- en: 'Figure 17: A model architecture used for multi-task learning, where some layers
    are shared between all tasks, and some layers are dedicated to specific tasks.
    Adopted from [[110](#bib.bib110)].'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：用于多任务学习的模型架构，其中一些层在所有任务之间共享，某些层专门用于特定任务。摘自 [[110](#bib.bib110)]。
- en: 3.2.4 Domain-Adversarial Learning
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 领域对抗学习
- en: This technique [[37](#bib.bib37)] is applied to spacecraft pose estimation [[113](#bib.bib113)]
    to bridge the domain gap. The goal here is to help the model learn features that
    are domain-invariant but discriminative with respect to the pose estimation task.
    A domain classifier, whose purpose is to discriminate between the source and the
    target domain, is attached to the model and its loss function maximised over the
    learning phase. The underlying idea of this method is that the less this classifier
    can distinguish between the source and the target domain, the more domain-invariant
    the model becomes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术[[37](#bib.bib37)]被应用于航天器姿态估计[[113](#bib.bib113)]以缩小领域差距。这里的目标是帮助模型学习那些对姿态估计任务具有领域不变性但具有辨别性的特征。一个领域分类器被附加到模型上，其目的是区分源领域和目标领域，并在学习阶段最大化其损失函数。这种方法的基本思想是，该分类器越无法区分源领域和目标领域，模型就越具有领域不变性。
- en: 3.3 Limitations
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 限制
- en: Current datasets and evaluation procedures are still insufficient to enable
    the deployment of DL-based spacecraft pose estimation algorithms in space missions.
    We identify key limitations below.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的数据集和评估程序仍不足以使基于深度学习的航天器姿态估计算法能够在空间任务中部署。我们在下面识别了关键的限制。
- en: 3.3.1 Realism of Synthetically Generated Datasets
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 合成数据集的逼真度
- en: One factor increasing the domain gap is the realism of the synthetic images
    used to train the models. The question of rendering realistic images is a hard
    topic in the context of space because it involves simulating the behaviour of
    light and its interaction with various materials and surfaces in space. The lack
    of reference points and the absence of an atmosphere make it difficult to create
    realistic lighting and shading effects. To achieve a realistic depiction of space,
    computer graphics techniques need to be tailored specifically to the unique properties
    of space environments. Therefore, the question of how to render more realistic
    synthetic space images is a challenging and open research topic.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 增加领域差距的一个因素是用于训练模型的合成图像的逼真度。在空间环境中渲染逼真图像是一个困难的问题，因为它涉及到模拟光线的行为及其与空间中各种材料和表面的互动。缺乏参考点和缺乏大气使得创建逼真的光照和阴影效果变得困难。要实现对空间的逼真描绘，计算机图形技术需要专门针对空间环境的独特属性进行调整。因此，如何渲染更逼真的合成空间图像是一个具有挑战性且开放的研究课题。
- en: 3.3.2 Algorithm Evaluation
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 算法评估
- en: While several attempts have been made to mitigate the domain gap between synthetic
    and laboratory images, there persists a one-order-of-magnitude difference between
    the best pose scores in the 2019 (synthetic test images) and 2021 (laboratory
    test images) editions of ESA’s Satellite Pose Estimation Challenge [[71](#bib.bib71),
    [113](#bib.bib113)]. Moreover, ensuring that an algorithm trained on synthetic
    images (source domain) performs well on laboratory images (target domain) does
    not guarantee that the performance level will be maintained for space images,
    mainly as a result of the domain gap between the two environments.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有多次尝试来减轻合成图像与实验室图像之间的领域差距，但2019年（合成测试图像）和2021年（实验室测试图像）ESA卫星姿态估计挑战赛中的最佳姿态得分之间仍存在一个数量级的差异[[71](#bib.bib71),
    [113](#bib.bib113)]。此外，确保一个在合成图像（源领域）上训练的算法在实验室图像（目标领域）上表现良好，并不保证其在空间图像上的性能水平会保持，主要是由于两个环境之间的领域差距。
- en: 4 Future Research Directions
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 未来研究方向
- en: Despite the recent progress in spacecraft pose estimation, there is room for
    improvement in algorithm development and data generation (or collection). This
    section summarises open research questions and possible future directions for
    the field.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在航天器姿态估计方面取得了近期进展，但在算法开发和数据生成（或收集）方面仍有改进空间。本节总结了开放的研究问题和该领域可能的未来方向。
- en: 4.1 Deployability of Algorithms
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 算法的可部署性
- en: The end goal of developing spacecraft pose estimation algorithms is to deploy
    them in space-borne hardware with limited resources. However, most existing algorithms
    are tested on workstations and large server clusters and very limited evaluations
    have been conducted on edge systems with FPGA [[36](#bib.bib36), [77](#bib.bib77)]
    or GPU [[72](#bib.bib72), [122](#bib.bib122), [15](#bib.bib15)]-based AI accelerators
    for space applications. In this context, this survey has made an effort to perform
    a trade-off comparison between the number of parameters (which can be a measure
    of resource consumption in the deployed hardware) in the DL models used and the
    algorithm performance. However, the lack of relevant information reported makes
    this difficult. In future works, it would be valuable for authors to report additional
    metrics such as size, number of FLOPS and latency, which are suitable measures
    for estimating the deployability of algorithms.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 开发航天器姿态估计算法的最终目标是将其部署到资源有限的空间硬件中。然而，大多数现有算法在工作站和大型服务器集群上进行测试，对基于FPGA [[36](#bib.bib36),
    [77](#bib.bib77)] 或GPU [[72](#bib.bib72), [122](#bib.bib122), [15](#bib.bib15)]
    的边缘系统的评估非常有限。在这种背景下，本调查努力在所使用的深度学习模型中的参数数量（可以作为部署硬件资源消耗的度量）和算法性能之间进行权衡比较。然而，缺乏相关信息的报告使得这一点变得困难。在未来的工作中，作者报告更多的指标如大小、FLOPS数量和延迟将会是估算算法可部署性的重要参考。
- en: Another future direction is to develop novel DL models specifically suited for
    edge AI accelerators. Unlike commonplace Nvidia GPUs, AI accelerators for space
    systems support only a limited number of network layers and operations [[181](#bib.bib181)].
    DL models with unsupported layers will have difficulty to work on such devices.
    Techniques like Neural Architecture Search (NAS) [[177](#bib.bib177)] can be used
    for developing efficient DL models which are deployable in space systems.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个未来方向是开发专门适合边缘AI加速器的全新深度学习模型。与常见的Nvidia GPU不同，空间系统的AI加速器仅支持有限数量的网络层和操作 [[181](#bib.bib181)]。具有不支持层的深度学习模型在这些设备上将难以工作。像**神经架构搜索（NAS）**
    [[177](#bib.bib177)] 这样的技术可以用于开发在空间系统中可部署的高效深度学习模型。
- en: 4.2 Explainability of Algorithms
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 算法的可解释性
- en: In real-world applications, the explainability of algorithms is a key factor
    in determining their reliability and trustworthiness. Especially in safety-critical
    applications like in space, it is important to know why and how a decision / prediction
    was made. However, the black-box nature of DL models makes them weak for interpreting
    their inference processes and final results. This makes explainability difficult
    in DL-based spacecraft pose estimation, especially for direct end-to-end algorithms.
    Recently, eXplainable-AI (XAI) [[44](#bib.bib44)] has become a hot research topic,
    with new methods developed [[3](#bib.bib3), [183](#bib.bib183)]. Several of these
    proposed methods, like Bayesian deep learning [[68](#bib.bib68)] or conformal
    inference methods  [[143](#bib.bib143), [1](#bib.bib1), [162](#bib.bib162)] can
    be applied to spacecraft pose estimation improving their explainability, which
    are interesting directions for future research.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，算法的可解释性是确定其可靠性和可信度的关键因素。特别是在像太空这样的安全关键应用中，了解决策/预测的原因和过程非常重要。然而，深度学习模型的黑箱性质使其在解释推理过程和最终结果方面较弱。这使得在基于深度学习的航天器姿态估计中，尤其是直接端到端算法中，可解释性变得困难。最近，**可解释人工智能（XAI）**
    [[44](#bib.bib44)] 成为了一个热门研究主题，开发了新的方法 [[3](#bib.bib3), [183](#bib.bib183)]。其中一些提议的方法，如贝叶斯深度学习
    [[68](#bib.bib68)] 或符合推断方法 [[143](#bib.bib143), [1](#bib.bib1), [162](#bib.bib162)]
    可以应用于航天器姿态估计，提高其可解释性，这些都是未来研究的有趣方向。
- en: 4.3 Multi-Modal Spacecraft Pose Estimation
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 多模态航天器姿态估计
- en: Most existing methods focus on visible-range images only. However, visible cameras
    are likely to suffer from difficult acquisition conditions in space (e.g., low
    light, overexposure). Therefore, other sensor such as thermal and time-of-flight
    of event cameras need to be considered in order to extend the operational range
    of classical computer vision methods. Till now, only a few works have investigated
    multi-modality for spacecraft pose estimation [[63](#bib.bib63), [136](#bib.bib136),
    [51](#bib.bib51), [137](#bib.bib137)] which is a direction of interest for the
    future of vision-based navigation in space.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数现有方法仅关注可见光范围的图像。然而，可见光相机在空间中很可能会遇到困难的采集条件（例如，低光照、过度曝光）。因此，需要考虑其他传感器，如热成像和时间飞行事件相机，以扩展经典计算机视觉方法的操作范围。到目前为止，仅有少数研究探讨了多模态在航天器姿态估计中的应用[[63](#bib.bib63)、[136](#bib.bib136)、[51](#bib.bib51)、[137](#bib.bib137)]，这是未来基于视觉的空间导航中的一个有趣方向。
- en: 4.4 Generation of More Realistic Synthetic Data
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 生成更为真实的合成数据
- en: 'As mentioned in section [3](#S3 "3 Datasets ‣ A Survey on Deep Learning-Based
    Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects"),
    the main issue with the application of machine learning to space applications
    is the lack of data. Moreover, the ubiquitous resort to synthetic data is the
    source of the current domain gap problem faced in the literature. Addressing this
    problem could be done through a deep analysis of the rendering engines’ images
    compared to actual space imagery. The results of this analysis could serve as
    the starting point for developing a rendering engine dedicated to generating realistic
    data for model training. To the best of our knowledge, PANGU [[95](#bib.bib95)]
    is the only initiative on this track to date. Another approach for simulation-to-real,
    is to introduce a physics informed layer into a deep learning system, as for example
    in [[76](#bib.bib76)]. This may induce invariance to lighting conditions in images
    of satellites that result from complex lighting and shadowing conditions for satellites
    orbiting the Earth, such as from reflections from the satellite itself, from the
    Earth’s surface, and from the moon’s surface.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '正如[3](#S3 "3 Datasets ‣ A Survey on Deep Learning-Based Monocular Spacecraft
    Pose Estimation: Current State, Limitations and Prospects")节中提到的，将机器学习应用于空间应用的主要问题是数据的缺乏。此外，普遍依赖合成数据是当前文献中面临的领域差距问题的根源。解决这个问题可以通过对渲染引擎图像与实际空间图像的深度分析来完成。这种分析的结果可以作为开发专门用于生成真实数据的渲染引擎的起点。据我们所知，PANGU[[95](#bib.bib95)]是目前唯一一个在这方面的项目。另一种模拟到现实的方式是向深度学习系统中引入一个物理信息层，例如在[[76](#bib.bib76)]中。这可能会使得卫星图像对光照条件具有不变性，而这些条件来自地球上复杂的光照和阴影效果，例如来自卫星自身、地球表面和月球表面的反射。'
- en: 4.5 Domain Adaptation
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 领域适应
- en: One of the main obstacles to the deployment of DL-based pose estimation methods
    in space is the performance gap when the models are trained on synthetic images
    and tested on real ones. The second edition of the ESA Pose Estimation Challenge
    [[113](#bib.bib113)] was specifically designed to address this challenge, with
    one synthetic training and two lab test datasets. Winning methods [[112](#bib.bib112)]
    have taken advantage of dedicated learning approaches, such as self-supervised,
    multi-task or adversarial learning. Together with the generation of more realistic
    synthetic datasets for training, domain adaptation is likely to receive much interest
    in the coming years to overcome the domain gap problem.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将基于深度学习的姿态估计方法应用于空间中的主要障碍之一是，当模型在合成图像上训练并在真实图像上测试时的性能差距。ESA姿态估计挑战赛的第二版[[113](#bib.bib113)]专门设计来解决这一挑战，包含一个合成训练数据集和两个实验室测试数据集。获胜的方法[[112](#bib.bib112)]利用了专门的学习方法，如自监督、多任务或对抗学习。随着生成更为真实的合成数据集用于训练，领域适应可能会在未来几年受到广泛关注，以克服领域差距问题。
- en: 4.6 Beyond Target-Specific Spacecraft Pose Estimation
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 超越特定目标的航天器姿态估计
- en: Current algorithms estimate the pose of a single type of spacecraft at a time.
    For every additional spacecraft, a new dataset has to be generated and the algorithm
    needs to be retrained. However, with the increasing number of spacecraft launched
    yearly, a natural way forward is to develop more generic algorithms that are not
    restricted to a particular spacecraft model. Especially in applications such as
    debris removal, the original spacecraft structure can disintegrate into geometrical
    shapes not seen by the algorithm during training. Generic 6D pose estimation methods
    for unseen objects [[43](#bib.bib43)] [[106](#bib.bib106)] can be exploited to
    develop spacecraft-agnostic pose estimation algorithms.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当前算法一次估计一种类型的航天器的姿态。对于每增加一个航天器，需要生成新的数据集并重新训练算法。然而，随着每年发射的航天器数量的增加，前进的自然方式是开发更通用的算法，不受限于特定的航天器模型。特别是在诸如碎片清除等应用中，原始航天器结构可能会解体成在训练过程中未见过的几何形状。可以利用通用的
    6D 姿态估计方法 [[43](#bib.bib43)] [[106](#bib.bib106)] 来开发与航天器无关的姿态估计算法。
- en: 4.7 Multi-Frame Spacecraft Pose Estimation
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 多帧航天器姿态估计
- en: Multi-frame spacecraft pose estimation refers to determining the spacecraft
    pose using consecutive images, thereby leveraging temporal information. Current
    spacecraft pose estimation algorithms consider each image frame in isolation and
    the pose is estimated from information extracted from this single image frame.
    However, in space, pose estimation algorithms are commonly used in applications
    such as autonomous navigation, where a sequence of consecutive images (trajectories)
    is available. Hence using temporal information is key to higher pose estimation
    accuracy and generating temporally consistent poses. Datasets like SPARK2 [[127](#bib.bib127)]
    already provide pose estimation data as trajectories. In this direction, recently
    proposed ChiNet [[137](#bib.bib137)] have used Long Short-Term Memory (LSTM) [[50](#bib.bib50)]
    units in modelling sequences of data for estimating the spacecraft pose. However,
    there is a rich history of video-based 6 DoF pose estimation methods leveraging
    temporal information in general computer vision [[7](#bib.bib7), [6](#bib.bib6),
    [27](#bib.bib27)]. In future, these methods can be in-cooperated into spacecraft
    pose estimation, especially for applications such as spacecraft related navigation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 多帧航天器姿态估计是指利用连续图像来确定航天器的姿态，从而利用时间信息。目前的航天器姿态估计算法将每帧图像视为孤立的，通过从单一图像帧中提取的信息来估计姿态。然而，在太空中，姿态估计算法通常用于自主导航等应用，其中可以获得一系列连续图像（轨迹）。因此，使用时间信息是提高姿态估计精度和生成时间一致姿态的关键。像
    SPARK2 [[127](#bib.bib127)] 这样的数据集已经提供了作为轨迹的姿态估计数据。在这方面，最近提出的 ChiNet [[137](#bib.bib137)]
    利用了长短期记忆（LSTM）[[50](#bib.bib50)] 单元来建模数据序列以估计航天器姿态。然而，在一般计算机视觉领域中，基于视频的 6 自由度姿态估计方法利用时间信息已有丰富的历史
    [[7](#bib.bib7), [6](#bib.bib6), [27](#bib.bib27)]。未来，这些方法可以被纳入航天器姿态估计中，特别是用于航天器相关导航等应用。
- en: 5 Conclusions
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'Monocular vision-based spacecraft pose estimation has seen considerable progress
    with the use of DL in recent years. However, there are still fundamental concerns
    that need to be addressed before these algorithms are deployed in actual space
    scenarios. This survey highlights these limitations, both in terms of algorithms
    design and datasets used for training and validation/testing. With this aim, the
    survey first summarised the existing algorithms according to two common approaches:
    hybrid modular and direct end-to-end regression approaches. Algorithms were compared
    in terms of performance as well as the size of the network architectures used
    to help understand their deployability. Then the spacecraft pose estimation datasets
    available for training and validating/testing these methods were discussed. The
    data generation methods, simulators and testbeds, and strategies used to bridge
    the domain gap problem were also discussed in detail. Finally, the survey provides
    future research directions to address these limitations and to develop spacecraft
    pose estimation algorithms deployable in real space missions.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单目视觉的航天器姿态估计近年来通过深度学习取得了显著进展。然而，在这些算法应用于实际空间场景之前，仍然存在一些基本问题需要解决。本综述突出了这些局限性，包括算法设计和用于训练及验证/测试的数据集。为此，综述首先根据两种常见方法总结了现有算法：混合模块和直接端到端回归方法。对算法的性能及所使用的网络架构规模进行了比较，以帮助理解它们的可部署性。然后讨论了用于训练和验证/测试这些方法的航天器姿态估计数据集。还详细讨论了数据生成方法、模拟器和测试平台，以及用于解决领域差距问题的策略。最后，综述提供了未来研究方向，以解决这些局限性并开发可在实际空间任务中应用的航天器姿态估计算法。
- en: Appendix A Additional Information
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 额外信息
- en: A.1 Publicly available algorithm implementations
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 公开的算法实现
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/BoChenYS/satellite-pose-estimation](https://github.com/BoChenYS/satellite-pose-estimation) [[23](#bib.bib23)]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/BoChenYS/satellite-pose-estimation](https://github.com/BoChenYS/satellite-pose-estimation) [[23](#bib.bib23)]'
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf](https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf) [[41](#bib.bib41)]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf](https://indico.esa.int/event/319/attachments/3561/4754/pose_gerard_segmentation.pdf) [[41](#bib.bib41)]'
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/cvlab-epfl/wide-depth-range-pose](https://github.com/cvlab-epfl/wide-depth-range-pose) [[57](#bib.bib57)]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/cvlab-epfl/wide-depth-range-pose](https://github.com/cvlab-epfl/wide-depth-range-pose) [[57](#bib.bib57)]'
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/tpark94/speedplusbaseline](https://github.com/tpark94/speedplusbaseline) [[115](#bib.bib115)]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/tpark94/speedplusbaseline](https://github.com/tpark94/speedplusbaseline) [[115](#bib.bib115)]'
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/pedropro/UrsoNet](https://github.com/pedropro/UrsoNet) [[124](#bib.bib124)]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/pedropro/UrsoNet](https://github.com/pedropro/UrsoNet) [[124](#bib.bib124)]'
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/possoj/Mobile-URSONet](https://github.com/possoj/Mobile-URSONet) [[121](#bib.bib121)]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/possoj/Mobile-URSONet](https://github.com/possoj/Mobile-URSONet) [[121](#bib.bib121)]'
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/tpark94/spnv2](https://github.com/tpark94/spnv2) [[110](#bib.bib110)]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/tpark94/spnv2](https://github.com/tpark94/spnv2) [[110](#bib.bib110)]'
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[https://github.com/Shunli-Wang/CA-SpaceNet](https://github.com/Shunli-Wang/CA-SpaceNet) [[172](#bib.bib172)]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/Shunli-Wang/CA-SpaceNet](https://github.com/Shunli-Wang/CA-SpaceNet) [[172](#bib.bib172)]'
- en: A.2 Links to the publicly available datasets
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 公开数据集链接
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SHIRT: [https://taehajeffpark.com/shirt/](https://taehajeffpark.com/shirt/)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SHIRT: [https://taehajeffpark.com/shirt/](https://taehajeffpark.com/shirt/)'
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPARK2022: [https://cvi2.uni.lu/spark2022/registration/](https://cvi2.uni.lu/spark2022/registration/)'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SPARK2022: [https://cvi2.uni.lu/spark2022/registration/](https://cvi2.uni.lu/spark2022/registration/)'
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SwissCube: [https://github.com/cvlab-epfl/wide-depth-range-pose](https://github.com/cvlab-epfl/wide-depth-range-pose)'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SwissCube: [https://github.com/cvlab-epfl/wide-depth-range-pose](https://github.com/cvlab-epfl/wide-depth-range-pose)'
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPEED+: [https://zenodo.org/record/5588480](https://zenodo.org/record/5588480)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SPEED+: [https://zenodo.org/record/5588480](https://zenodo.org/record/5588480)'
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPEED: [https://zenodo.org/record/6327547](https://zenodo.org/record/6327547)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SPEED: [https://zenodo.org/record/6327547](https://zenodo.org/record/6327547)'
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'URSO-OrViS: [https://zenodo.org/record/3279632](https://zenodo.org/record/3279632)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'URSO-OrViS: [https://zenodo.org/record/3279632](https://zenodo.org/record/3279632)'
- en: References
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Angelopoulos and Bates [2021] Angelopoulos, A.N., Bates, S., 2021. A gentle
    introduction to conformal prediction and distribution-free uncertainty quantification.
    arXiv preprint arXiv:2107.07511 .
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angelopoulos 和 Bates [2021] Angelopoulos, A.N., Bates, S., 2021. 对保形预测和无分布不确定性量化的温和介绍。arXiv
    预印本 arXiv:2107.07511。
- en: 'Azodi et al. [2020] Azodi, C.B., Tang, J., Shiu, S.H., 2020. Opening the black
    box: interpretable machine learning for geneticists. Trends in genetics 36, 442–455.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azodi等人 [2020] Azodi, C.B., Tang, J., Shiu, S.H., 2020. 打开黑箱：遗传学家的可解释机器学习。遗传学趋势
    36, 442–455。
- en: 'Bai et al. [2021] Bai, X., Wang, X., Liu, X., Liu, Q., Song, J., Sebe, N.,
    Kim, B., 2021. Explainable deep learning for efficient and robust pattern recognition:
    A survey of recent developments. Pattern Recognition 120, 108102.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等人 [2021] Bai, X., Wang, X., Liu, X., Liu, Q., Song, J., Sebe, N., Kim, B.,
    2021. 可解释深度学习用于高效且鲁棒的模式识别：近期发展的综述。模式识别 120, 108102。
- en: 'Baller et al. [2021] Baller, S.P., Jindal, A., Chadha, M., Gerndt, M., 2021.
    Deepedgebench: Benchmarking deep neural networks on edge devices, in: 2021 IEEE
    International Conference on Cloud Engineering (IC2E), IEEE. pp. 20–30.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baller等人 [2021] Baller, S.P., Jindal, A., Chadha, M., Gerndt, M., 2021. Deepedgebench：在边缘设备上基准测试深度神经网络，见：2021
    IEEE云工程国际会议 (IC2E)，IEEE。pp. 20–30。
- en: 'Bechini et al. [2022] Bechini, M., Lunghi, P., Lavagna, M., et al., 2022. Spacecraft
    pose estimation via monocular image processing: Dataset generation and validation,
    in: 9th European Conference for Aerospace Sciences (EUCASS 2022), pp. 1–15.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bechini等人 [2022] Bechini, M., Lunghi, P., Lavagna, M., 等, 2022. 通过单目图像处理进行航天器姿态估计：数据集生成与验证，见：第9届欧洲航空航天科学会议
    (EUCASS 2022)，pp. 1–15。
- en: Beedu et al. [2022] Beedu, A., Alamri, H., Essa, I., 2022. Video based object
    6d pose estimation using transformers. arXiv preprint arXiv:2210.13540 .
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beedu等人 [2022] Beedu, A., Alamri, H., Essa, I., 2022. 基于视频的物体6D姿态估计使用变换器。arXiv预印本
    arXiv:2210.13540。
- en: 'Beedu et al. [2021] Beedu, A., Ren, Z., Agrawal, V., Essa, I., 2021. Videopose:
    Estimating 6d object pose from videos. arXiv preprint arXiv:2111.10677 .'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beedu等人 [2021] Beedu, A., Ren, Z., Agrawal, V., Essa, I., 2021. Videopose：从视频中估计6D物体姿态。arXiv预印本
    arXiv:2111.10677。
- en: Beierle and D’Amico [2019] Beierle, C., D’Amico, S., 2019. Variable-magnification
    optical stimulator for training and validation of spaceborne vision-based navigation.
    Journal of Spacecraft and Rockets 56, 1060–1072.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beierle和D’Amico [2019] Beierle, C., D’Amico, S., 2019. 变倍光学刺激器用于训练和验证空间视导导航。航天器与火箭期刊
    56, 1060–1072。
- en: Ben-David et al. [2006] Ben-David, S., Blitzer, J., Crammer, K., Pereira, F.,
    2006. Analysis of representations for domain adaptation. Advances in neural information
    processing systems 19.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-David等人 [2006] Ben-David, S., Blitzer, J., Crammer, K., Pereira, F., 2006.
    领域适应的表示分析。神经信息处理系统进展 19。
- en: Benninghoff et al. [2017] Benninghoff, H., Rems, F., Risse, E.A., Mietner, C.,
    2017. European proximity operations simulator 2.0 (epos) - a robotic-based rendezvous
    and docking simulator. Journal of large-scale research facilities JLSRF 3, 107.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benninghoff等人 [2017] Benninghoff, H., Rems, F., Risse, E.A., Mietner, C., 2017.
    欧洲接近操作模拟器2.0 (EPOS) - 一种基于机器人对接和对接模拟器。大型研究设施期刊 JLSRF 3, 107。
- en: 'Biesbroek et al. [2021] Biesbroek, R., Aziz, S., Wolahan, A., Cipolla, S.f.,
    Richard-Noca, M., Piguet, L., 2021. The clearspace-1 mission: Esa and clearspace
    team up to remove debris, in: Proc. 8th Eur. Conf. Sp. Debris, pp. 1–3.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biesbroek等人 [2021] Biesbroek, R., Aziz, S., Wolahan, A., Cipolla, S.f., Richard-Noca,
    M., Piguet, L., 2021. Clearspace-1任务：Esa和Clearspace团队联手移除碎片，见：第8届欧洲空间碎片会议，pp.
    1–3。
- en: Black et al. [2021] Black, K., Shankar, S., Fonseka, D., Deutsch, J., Dhir,
    A., Akella, M.R., 2021. Real-time, flight-ready, non-cooperative spacecraft pose
    estimation using monocular imagery. arXiv preprint arXiv:2101.09553 .
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Black等人 [2021] Black, K., Shankar, S., Fonseka, D., Deutsch, J., Dhir, A., Akella,
    M.R., 2021. 实时飞行准备的非合作航天器姿态估计，使用单目图像。arXiv预印本 arXiv:2101.09553。
- en: 'Bochkovskiy et al. [2020] Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020.
    Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934
    .'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bochkovskiy等人 [2020] Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020. Yolov4：物体检测的最佳速度和精度。arXiv预印本
    arXiv:2004.10934。
- en: Brochard et al. [2018] Brochard, R., Lebreton, J., Robin, C., Kanani, K., Jonniaux,
    G., Masson, A., Despré, N., Berjaoui, A., 2018. Scientific image rendering for
    space scenes with the surrender software. arXiv preprint arXiv:1810.01423 .
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brochard等人 [2018] Brochard, R., Lebreton, J., Robin, C., Kanani, K., Jonniaux,
    G., Masson, A., Despré, N., Berjaoui, A., 2018. 使用Surrender软件进行空间场景的科学图像渲染。arXiv预印本
    arXiv:1810.01423。
- en: Bruhn et al. [2020] Bruhn, F.C., Tsog, N., Kunkel, F., Flordal, O., Troxel,
    I., 2020. Enabling radiation tolerant heterogeneous gpu-based onboard data processing
    in space. CEAS Space Journal 12, 551–564.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruhn等人 [2020] Bruhn, F.C., Tsog, N., Kunkel, F., Flordal, O., Troxel, I., 2020.
    实现辐射容忍的异构GPU基于空间的板载数据处理。CEAS空间期刊 12, 551–564。
- en: 'Bukschat and Vetter [2020] Bukschat, Y., Vetter, M., 2020. Efficientpose: An
    efficient, accurate and scalable end-to-end 6d multi object pose estimation approach.
    arXiv preprint arXiv:2011.04307 .'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bukschat 和 Vetter [2020] Bukschat, Y., Vetter, M., 2020. Efficientpose：一种高效、准确且可扩展的端到端
    6D 多目标姿态估计方法。arXiv 预印本 arXiv:2011.04307。
- en: 'Cai and Vasconcelos [2018] Cai, Z., Vasconcelos, N., 2018. Cascade r-cnn: Delving
    into high quality object detection, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 6154–6162.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 和 Vasconcelos [2018] Cai, Z., Vasconcelos, N., 2018. Cascade r-cnn：深入探讨高质量物体检测，在：IEEE
    计算机视觉与模式识别会议论文集，页码 6154–6162。
- en: 'Cao et al. [2020] Cao, W., Zhou, C., Wu, Y., Ming, Z., Xu, Z., Zhang, J., 2020.
    Research progress of zero-shot learning beyond computer vision, in: Algorithms
    and Architectures for Parallel Processing: 20th International Conference, ICA3PP
    2020, New York City, NY, USA, October 2–4, 2020, Proceedings, Part II 20, Springer.
    pp. 538–551.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 [2020] Cao, W., Zhou, C., Wu, Y., Ming, Z., Xu, Z., Zhang, J., 2020. 超越计算机视觉的零样本学习研究进展，在：并行处理算法与架构：第
    20 届国际会议，ICA3PP 2020，美国纽约市，2020 年 10 月 2–4 日，会议论文集，第二部分 20，Springer，页码 538–551。
- en: 'Capuano et al. [2019] Capuano, V., Alimo, S.R., Ho, A.Q., Chung, S.J., 2019.
    Robust features extraction for on-board monocular-based spacecraft pose acquisition,
    in: AIAA Scitech 2019 Forum, p. 2005.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capuano 等 [2019] Capuano, V., Alimo, S.R., Ho, A.Q., Chung, S.J., 2019. 针对航天器姿态获取的单目特征提取方法，在：AIAA
    Scitech 2019 论坛，页码 2005。
- en: Cassinis et al. [2019] Cassinis, L.P., Fonod, R., Gill, E., 2019. Review of
    the robustness and applicability of monocular pose estimation systems for relative
    navigation with an uncooperative spacecraft. Progress in Aerospace Sciences 110,
    100548.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassinis 等 [2019] Cassinis, L.P., Fonod, R., Gill, E., 2019. 单目姿态估计系统在非合作航天器相对导航中的鲁棒性和适用性综述。航空航天科学进展
    110, 100548。
- en: 'Cassinis et al. [2021] Cassinis, L.P., Menicucci, A., Gill, E., Ahrns, I.,
    Fernandez, J.G., 2021. On-ground validation of a cnn-based monocular pose estimation
    system for uncooperative spacecraft, in: 8th European Conference on Space Debris.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassinis 等 [2021] Cassinis, L.P., Menicucci, A., Gill, E., Ahrns, I., Fernandez,
    J.G., 2021. 基于 CNN 的单目姿态估计系统在非合作航天器上的地面验证，在：第八届欧洲空间碎片会议。
- en: 'Chai et al. [2021] Chai, J., Zeng, H., Li, A., Ngai, E.W., 2021. Deep learning
    in computer vision: A critical review of emerging techniques and application scenarios.
    Machine Learning with Applications 6, 100134.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai 等 [2021] Chai, J., Zeng, H., Li, A., Ngai, E.W., 2021. 计算机视觉中的深度学习：新兴技术和应用场景的关键回顾。应用机器学习
    6, 100134。
- en: 'Chen et al. [2019a] Chen, B., Cao, J., Parra, A., Chin, T.J., 2019a. Satellite
    pose estimation with deep landmark regression and nonlinear pose refinement, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)
    Workshops.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2019a] Chen, B., Cao, J., Parra, A., Chin, T.J., 2019a. 基于深度标志回归和非线性姿态精化的卫星姿态估计，在：IEEE/CVF
    国际计算机视觉会议 (ICCV) 论文集。
- en: 'Chen et al. [2019b] Chen, B., Cao, J., Parra, A., Chin, T.J., 2019b. Satellite
    pose estimation with deep landmark regression and nonlinear pose refinement, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 0–0.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2019b] Chen, B., Cao, J., Parra, A., Chin, T.J., 2019b. 基于深度标志回归和非线性姿态精化的卫星姿态估计，在：IEEE/CVF
    国际计算机视觉会议论文集，页码 0–0。
- en: 'Cheng et al. [2020] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang,
    L., 2020. Higherhrnet: Scale-aware representation learning for bottom-up human
    pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 5386–5395.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 [2020] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang, L.,
    2020. Higherhrnet：面向自下而上的人体姿态估计的尺度感知表示学习，在：IEEE/CVF 计算机视觉与模式识别会议论文集，页码 5386–5395。
- en: 'Ciaparrone et al. [2020] Ciaparrone, G., Sánchez, F.L., Tabik, S., Troiano,
    L., Tagliaferri, R., Herrera, F., 2020. Deep learning in video multi-object tracking:
    A survey. Neurocomputing 381, 61–88.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciaparrone 等 [2020] Ciaparrone, G., Sánchez, F.L., Tabik, S., Troiano, L., Tagliaferri,
    R., Herrera, F., 2020. 视频多目标跟踪中的深度学习：综述。神经计算 381, 61–88。
- en: 'Clark et al. [2017] Clark, R., Wang, S., Markham, A., Trigoni, N., Wen, H.,
    2017. Vidloc: A deep spatio-temporal model for 6-dof video-clip relocalization,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 6856–6864.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 [2017] Clark, R., Wang, S., Markham, A., Trigoni, N., Wen, H., 2017.
    Vidloc：用于 6 自由度视频片段重定位的深度时空模型，在：IEEE 计算机视觉与模式识别会议论文集，页码 6856–6864。
- en: 'Colmenarejo et al. [2019] Colmenarejo, P., Graziano, M., Novelli, G., Mora,
    D., Serra, P., Tomassini, A., Seweryn, K., Prisco, G., Fernandez, J.G., 2019.
    On ground validation of debris removal technologies. Acta Astronautica 158, 206–219.
    URL: [https://www.sciencedirect.com/science/article/pii/S0094576517312845](https://www.sciencedirect.com/science/article/pii/S0094576517312845),
    doi:[https://doi.org/10.1016/j.actaastro.2018.01.026](https:/doi.org/https://doi.org/10.1016/j.actaastro.2018.01.026).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Colmenarejo 等 [2019] Colmenarejo, P., Graziano, M., Novelli, G., Mora, D.,
    Serra, P., Tomassini, A., Seweryn, K., Prisco, G., Fernandez, J.G., 2019. 垃圾清除技术的地面验证。《宇航学报》158,
    206–219. URL: [https://www.sciencedirect.com/science/article/pii/S0094576517312845](https://www.sciencedirect.com/science/article/pii/S0094576517312845),
    doi:[https://doi.org/10.1016/j.actaastro.2018.01.026](https:/doi.org/https://doi.org/10.1016/j.actaastro.2018.01.026)。'
- en: Cosmas and Kenichi [2020] Cosmas, K., Kenichi, A., 2020. Utilization of fpga
    for onboard inference of landmark localization in cnn-based spacecraft pose estimation.
    Aerospace 7, 159.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cosmas 和 Kenichi [2020] Cosmas, K., Kenichi, A., 2020. FPGA 在基于 CNN 的航天器姿态估计中用于地标定位的推断。航空航天
    7, 159。
- en: 'D’ et al. [2014] D’, S., Amico, N., Benn, M., Jørgensen, J.L., 2014. Pose estimation
    of an uncooperative spacecraft from actual space imagery. International Journal
    of Space Science and Engineering 2, 171. URL: [http://www.inderscience.com/link.php?id=60600](http://www.inderscience.com/link.php?id=60600),
    doi:[10.1504/IJSPACESE.2014.060600](https:/doi.org/10.1504/IJSPACESE.2014.060600).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'D’ 等 [2014] D’, S., Amico, N., Benn, M., Jørgensen, J.L., 2014. 从实际太空图像中估计不合作航天器的姿态。《国际空间科学与工程杂志》2,
    171. URL: [http://www.inderscience.com/link.php?id=60600](http://www.inderscience.com/link.php?id=60600),
    doi:[10.1504/IJSPACESE.2014.060600](https:/doi.org/10.1504/IJSPACESE.2014.060600)。'
- en: 'Deng et al. [2009] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei,
    L., 2009. Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference
    on computer vision and pattern recognition, Ieee. pp. 248–255.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 [2009] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.,
    2009. ImageNet：一个大规模层次图像数据库，见：2009 IEEE 计算机视觉与模式识别大会，IEEE. 第 248–255 页。
- en: D’Amico et al. [2014] D’Amico, S., Benn, M., Jørgensen, J.L., 2014. Pose estimation
    of an uncooperative spacecraft from actual space imagery. International Journal
    of Space Science and Engineering 5 2, 171–189.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Amico 等 [2014] D’Amico, S., Benn, M., Jørgensen, J.L., 2014. 从实际太空图像中估计不合作航天器的姿态。《国际空间科学与工程杂志》5
    2, 171–189。
- en: '[33] (ESA), E.S.A., 2010. Prisma’s tango and mango satellites. [https://www.esa.int/ESA_Multimedia/Images/2010/10/Prisma_s_Tango_and_Mango_satellites](https://www.esa.int/ESA_Multimedia/Images/2010/10/Prisma_s_Tango_and_Mango_satellites).
    Accessed: 05-April-2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] (ESA), E.S.A., 2010. Prisma 的 Tango 和 Mango 卫星。 [https://www.esa.int/ESA_Multimedia/Images/2010/10/Prisma_s_Tango_and_Mango_satellites](https://www.esa.int/ESA_Multimedia/Images/2010/10/Prisma_s_Tango_and_Mango_satellites)。访问时间：2023年4月5日。'
- en: 'Fang et al. [2022] Fang, Y., Yap, P.T., Lin, W., Zhu, H., Liu, M., 2022. Source-free
    unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265 .'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等 [2022] Fang, Y., Yap, P.T., Lin, W., Zhu, H., Liu, M., 2022. 无源无监督领域适应：综述。arXiv
    预印本 arXiv:2301.00265。
- en: 'Fischler and Bolles [1981] Fischler, M.A., Bolles, R.C., 1981. Random sample
    consensus: a paradigm for model fitting with applications to image analysis and
    automated cartography. Communications of the ACM 24, 381–395.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischler 和 Bolles [1981] Fischler, M.A., Bolles, R.C., 1981. 随机采样一致性：一个用于模型拟合的范例及其在图像分析和自动制图中的应用。《计算机协会通讯》24,
    381–395。
- en: 'Furano et al. [2020] Furano, G., Meoni, G., Dunne, A., Moloney, D., Ferlet-Cavrois,
    V., Tavoularis, A., Byrne, J., Buckley, L., Psarakis, M., Voss, K.O., et al.,
    2020. Towards the use of artificial intelligence on the edge in space systems:
    Challenges and opportunities. IEEE Aerospace and Electronic Systems Magazine 35,
    44–56.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furano 等 [2020] Furano, G., Meoni, G., Dunne, A., Moloney, D., Ferlet-Cavrois,
    V., Tavoularis, A., Byrne, J., Buckley, L., Psarakis, M., Voss, K.O., 等，2020.
    在空间系统边缘使用人工智能：挑战与机遇。IEEE 航空航天与电子系统杂志 35, 44–56。
- en: Ganin et al. [2016] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle,
    H., Laviolette, F., Marchand, M., Lempitsky, V., 2016. Domain-adversarial training
    of neural networks. The journal of machine learning research 17, 2096–2030.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganin 等 [2016] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle,
    H., Laviolette, F., Marchand, M., Lempitsky, V., 2016. 神经网络的领域对抗训练。《机器学习研究杂志》17,
    2096–2030。
- en: 'Garcia et al. [2021] Garcia, A., Musallam, M.A., Gaudilliere, V., Ghorbel,
    E., Al Ismaeil, K., Perez, M., Aouada, D., 2021. Lspnet: A 2d localization-oriented
    spacecraft pose estimation neural network, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 2048–2056.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia et al. [2021] Garcia, A., Musallam, M.A., Gaudilliere, V., Ghorbel, E.,
    Al Ismaeil, K., Perez, M., Aouada, D., 2021. Lspnet：一种面向2D定位的航天器姿态估计神经网络，载于：IEEE/CVF计算机视觉与模式识别会议论文集，pp.
    2048–2056。
- en: Gaudillière et al. [2023] Gaudillière, V., Pauly, L., Rathinam, A., Sanchez,
    A.G., Musallam, M.A., Aouada, D., 2023. 3d-aware object localization using gaussian
    implicit occupancy function. arXiv preprint arXiv:2303.02058 .
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaudillière et al. [2023] Gaudillière, V., Pauly, L., Rathinam, A., Sanchez,
    A.G., Musallam, M.A., Aouada, D., 2023. 使用高斯隐式占用函数的三维感知物体定位。arXiv预印本 arXiv:2303.02058。
- en: 'Ge et al. [2021] Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. Yolox: Exceeding
    yolo series in 2021. arXiv preprint arXiv:2107.08430 .'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2021] Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. Yolox：超越YOLO系列的2021年版本。arXiv预印本
    arXiv:2107.08430。
- en: 'Gerard [2019] Gerard, K., 2019. Segmentation-Driven Satellite Pose Estimation.
    Technical Report. Technical Report. EPFL. Available online at: https://indico.
    esa. int/event ….'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerard [2019] Gerard, K., 2019. 基于分割的卫星姿态估计。技术报告。EPFL。在线获取：https://indico.esa.int/event….
- en: GMV [2018] GMV, 2018. platform-art. [https://satsearch.co/services/gmv-platform-art-for-satellite-orbit-simulation](https://satsearch.co/services/gmv-platform-art-for-satellite-orbit-simulation).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMV [2018] GMV, 2018. platform-art。 [https://satsearch.co/services/gmv-platform-art-for-satellite-orbit-simulation](https://satsearch.co/services/gmv-platform-art-for-satellite-orbit-simulation)。
- en: 'Gou et al. [2022] Gou, M., Pan, H., Fang, H.S., Liu, Z., Lu, C., Tan, P., 2022.
    Unseen object 6d pose estimation: a benchmark and baselines. arXiv preprint arXiv:2206.11808
    .'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou et al. [2022] Gou, M., Pan, H., Fang, H.S., Liu, Z., Lu, C., Tan, P., 2022.
    未见物体的6D姿态估计：基准与基线。arXiv预印本 arXiv:2206.11808。
- en: Gunning et al. [2019] Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf,
    S., Yang, G.Z., 2019. Xai—explainable artificial intelligence. Science robotics
    4, eaay7120.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunning et al. [2019] Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf,
    S., Yang, G.Z., 2019. Xai——可解释的人工智能。《科学机器人》，4，eaay7120。
- en: 'Hadidi et al. [2019] Hadidi, R., Cao, J., Xie, Y., Asgari, B., Krishna, T.,
    Kim, H., 2019. Characterizing the deployment of deep neural networks on commercial
    edge devices, in: 2019 IEEE International Symposium on Workload Characterization
    (IISWC), IEEE. pp. 35–48.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadidi et al. [2019] Hadidi, R., Cao, J., Xie, Y., Asgari, B., Krishna, T.,
    Kim, H., 2019. 描述深度神经网络在商业边缘设备上的部署，载于：2019 IEEE国际工作负载特征研讨会（IISWC），IEEE。pp. 35–48。
- en: Hartley and Zisserman [2003] Hartley, R., Zisserman, A., 2003. Multiple view
    geometry in computer vision. Cambridge university press.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hartley and Zisserman [2003] Hartley, R., Zisserman, A., 2003. 计算机视觉中的多视角几何。剑桥大学出版社。
- en: 'He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    r-cnn, in: Proceedings of the IEEE international conference on computer vision,
    pp. 2961–2969.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    R-CNN，载于：IEEE国际计算机视觉会议论文集，pp. 2961–2969。
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 770–778.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. 图像识别的深度残差学习，载于：IEEE计算机视觉与模式识别会议论文集，pp.
    770–778。
- en: 'Hinterstoißer et al. [2012] Hinterstoißer, S., Lepetit, V., Ilic, S., Holzer,
    S., Bradski, G.R., Konolige, K., Navab, N., 2012. Model based training, detection
    and pose estimation of texture-less 3d objects in heavily cluttered scenes, in:
    Asian Conference on Computer Vision.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinterstoißer et al. [2012] Hinterstoißer, S., Lepetit, V., Ilic, S., Holzer,
    S., Bradski, G.R., Konolige, K., Navab, N., 2012. 基于模型的训练、检测和无纹理三维物体在严重杂乱场景中的姿态估计，载于：亚洲计算机视觉会议。
- en: 'Hochreiter and Schmidhuber [1997] Hochreiter, S., Schmidhuber, J., 1997. Long
    short-term memory. Neural Comput. 9, 1735–1780. URL: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735),
    doi:[10.1162/neco.1997.9.8.1735](https:/doi.org/10.1162/neco.1997.9.8.1735).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber [1997] Hochreiter, S., Schmidhuber, J., 1997. 长短期记忆。《神经计算》，9，1735–1780。网址：[https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)，doi：[10.1162/neco.1997.9.8.1735](https:/doi.org/10.1162/neco.1997.9.8.1735)。
- en: 'Hogan et al. [2021] Hogan, M., Rondao, D., Aouf, N., Dubois-Matra, O., 2021.
    Using convolutional neural networks for relative pose estimation of a non-cooperative
    spacecraft with thermal infrared imagery. CoRR abs/2105.13789. URL: [https://arxiv.org/abs/2105.13789](https://arxiv.org/abs/2105.13789),
    [arXiv:2105.13789](http://arxiv.org/abs/2105.13789).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hogan 等人 [2021] Hogan, M., Rondao, D., Aouf, N., Dubois-Matra, O., 2021. 使用卷积神经网络进行非合作航天器的相对姿态估计，基于热红外图像。CoRR
    abs/2105.13789。网址: [https://arxiv.org/abs/2105.13789](https://arxiv.org/abs/2105.13789),
    [arXiv:2105.13789](http://arxiv.org/abs/2105.13789)。'
- en: 'Hou et al. [2020] Hou, T., Ahmadyan, A., Zhang, L., Wei, J., Grundmann, M.,
    2020. Mobilepose: Real-time pose estimation for unseen objects with weak shape
    supervision. arXiv preprint arXiv:2003.03522 .'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等人 [2020] Hou, T., Ahmadyan, A., Zhang, L., Wei, J., Grundmann, M., 2020.
    Mobilepose：针对未见物体的实时姿态估计，具有弱形状监督。arXiv预印本 arXiv:2003.03522。
- en: 'Howard et al. [2019] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B.,
    Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al., 2019. Searching for
    mobilenetv3, in: Proceedings of the IEEE/CVF international conference on computer
    vision, pp. 1314--1324.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等人 [2019] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan,
    M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., 等, 2019. 寻找Mobilenetv3，发表于：IEEE/CVF国际计算机视觉会议论文集，第1314--1324页。
- en: 'Hu et al. [2018] Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 7132--7141.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2018] Hu, J., Shen, L., Sun, G., 2018. 压缩与激励网络，发表于：IEEE计算机视觉与模式识别会议论文集，第7132--7141页。
- en: 'Hu et al. [2020] Hu, Y., Fua, P., Wang, W., Salzmann, M., 2020. Single-stage
    6d object pose estimation, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, pp. 2930--2939.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2020] Hu, Y., Fua, P., Wang, W., Salzmann, M., 2020. 单阶段6D物体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第2930--2939页。
- en: 'Hu et al. [2019] Hu, Y., Hugonot, J., Fua, P., Salzmann, M., 2019. Segmentation-driven
    6d object pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 3385--3394.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2019] Hu, Y., Hugonot, J., Fua, P., Salzmann, M., 2019. 基于分割的6D物体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第3385--3394页。
- en: 'Hu et al. [2021] Hu, Y., Speierer, S., Jakob, W., Fua, P., Salzmann, M., 2021.
    Wide-depth-range 6d object pose estimation in space, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 15870--15879.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Hu, Y., Speierer, S., Jakob, W., Fua, P., Salzmann, M., 2021. 宽深度范围的6D物体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第15870--15879页。
- en: 'Huan et al. [2020] Huan, W., Liu, M., Hu, Q., 2020. Pose estimation for non-cooperative
    spacecraft based on deep learning, in: 2020 39th Chinese Control Conference (CCC),
    IEEE. pp. 3339--3343.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huan 等人 [2020] Huan, W., Liu, M., Hu, Q., 2020. 基于深度学习的非合作航天器姿态估计，发表于：2020年第39届中国控制会议（CCC），IEEE，第3339--3343页。
- en: Huang et al. [2021] Huang, H., Zhao, G., Gu, D., Bo, Y., 2021. Non-model-based
    monocular pose estimation network for uncooperative spacecraft using convolutional
    neural network. IEEE Sensors Journal 21, 24579--24590.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2021] Huang, H., Zhao, G., Gu, D., Bo, Y., 2021. 基于卷积神经网络的非模型单目姿态估计网络，针对非合作航天器。IEEE传感器期刊
    21, 24579--24590。
- en: Huo et al. [2020] Huo, Y., Li, Z., Zhang, F., 2020. Fast and accurate spacecraft
    pose estimation from single shot space imagery using box reliability and keypoints
    existence judgments. IEEE Access 8, 216283--216297.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huo 等人 [2020] Huo, Y., Li, Z., Zhang, F., 2020. 从单张空间图像中快速准确地估计航天器姿态，使用框可靠性和关键点存在判断。IEEE
    Access 8, 216283--216297。
- en: 'Huynh [2009] Huynh, D.Q., 2009. Metrics for 3d rotations: Comparison and analysis.
    Journal of Mathematical Imaging and Vision 35, 155--164.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huynh [2009] Huynh, D.Q., 2009. 3D旋转度量：比较与分析。数学成像与视觉期刊 35, 155--164。
- en: 'Jackson et al. [2018] Jackson, P.T.G., Abarghouei, A.A., Bonner, S., Breckon,
    T.P., Obara, B., 2018. Style augmentation: Data augmentation via style randomization.
    CoRR abs/1809.05375. URL: [http://arxiv.org/abs/1809.05375](http://arxiv.org/abs/1809.05375),
    [arXiv:1809.05375](http://arxiv.org/abs/1809.05375).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jackson 等人 [2018] Jackson, P.T.G., Abarghouei, A.A., Bonner, S., Breckon, T.P.,
    Obara, B., 2018. 风格增强：通过风格随机化的数据增强。CoRR abs/1809.05375。网址: [http://arxiv.org/abs/1809.05375](http://arxiv.org/abs/1809.05375),
    [arXiv:1809.05375](http://arxiv.org/abs/1809.05375)。'
- en: 'Jawaid et al. [2022] Jawaid, M., Elms, E., Latif, Y., Chin, T.J., 2022. Towards
    bridging the space domain gap for satellite pose estimation using event sensing.
    URL: [https://arxiv.org/abs/2209.11945](https://arxiv.org/abs/2209.11945), doi:[10.48550/ARXIV.2209.11945](https:/doi.org/10.48550/ARXIV.2209.11945).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jawaid等人[2022] Jawaid, M., Elms, E., Latif, Y., Chin, T.J., 2022. 为卫星姿态估计缩小空间领域差距的努力。网址：[https://arxiv.org/abs/2209.11945](https://arxiv.org/abs/2209.11945)，doi：[10.48550/ARXIV.2209.11945](https://doi.org/10.48550/ARXIV.2209.11945)。
- en: Jiao et al. [2019] Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z.,
    Qu, R., 2019. A survey of deep learning-based object detection. IEEE access 7,
    128837--128868.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao等人[2019] Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z., Qu, R.,
    2019. 基于深度学习的目标检测综述。《IEEE Access》7，128837--128868。
- en: Jones [2018] Jones, H., 2018. The recent large reduction in space launch cost,
    48th International Conference on Environmental Systems.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones [2018] Jones, H., 2018. 最近空间发射成本的大幅下降，第48届国际环境系统会议。
- en: 'Kelsey et al. [2006] Kelsey, J., Byrne, J., Cosgrove, M., Seereeram, S., Mehra,
    R., 2006. Vision-based relative pose estimation for autonomous rendezvous and
    docking, in: 2006 IEEE Aerospace Conference, pp. 20 pp.--. doi:[10.1109/AERO.2006.1655916](https:/doi.org/10.1109/AERO.2006.1655916).'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kelsey等人[2006] Kelsey, J., Byrne, J., Cosgrove, M., Seereeram, S., Mehra, R.,
    2006. 基于视觉的相对姿态估计用于自主对接和停靠，见：2006年IEEE航天会议，第20页。doi：[10.1109/AERO.2006.1655916](https://doi.org/10.1109/AERO.2006.1655916)。
- en: 'Kendall and Cipolla [2017] Kendall, A., Cipolla, R., 2017. Geometric loss functions
    for camera pose regression with deep learning, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 5974--5983.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall和Cipolla [2017] Kendall, A., Cipolla, R., 2017. 基于深度学习的相机姿态回归的几何损失函数，见：IEEE计算机视觉与模式识别会议论文集，第5974--5983页。
- en: Kendall and Gal [2017] Kendall, A., Gal, Y., 2017. What uncertainties do we
    need in bayesian deep learning for computer vision? Advances in neural information
    processing systems 30.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall和Gal [2017] Kendall, A., Gal, Y., 2017. 对于计算机视觉，贝叶斯深度学习需要哪些不确定性？《神经信息处理系统进展》30。
- en: 'Kendall et al. [2015] Kendall, A., Grimes, M., Cipolla, R., 2015. Posenet:
    A convolutional network for real-time 6-dof camera relocalization, in: Proceedings
    of the IEEE International Conference on Computer Vision (ICCV).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall等人[2015] Kendall, A., Grimes, M., Cipolla, R., 2015. PoseNet：一种用于实时6自由度相机重新定位的卷积网络，见：IEEE国际计算机视觉会议（ICCV）论文集。
- en: 'Kendall et al. [2015] Kendall, A., Grimes, M., Cipolla, R., 2015. PoseNet:
    A Convolutional Network for Real-Time 6-DOF Camera Relocalization. arXiv e-prints
    , arXiv:1505.07427[arXiv:1505.07427](http://arxiv.org/abs/1505.07427).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall等人[2015] Kendall, A., Grimes, M., Cipolla, R., 2015. PoseNet：一种用于实时6自由度相机重新定位的卷积网络。arXiv电子预印本，arXiv:1505.07427[arXiv:1505.07427](http://arxiv.org/abs/1505.07427)。
- en: 'Kisantal et al. [2020] Kisantal, M., Sharma, S., Park, T.H., Izzo, D., Märtens,
    M., D’Amico, S., 2020. Satellite pose estimation challenge: Dataset, competition
    design, and results. IEEE Transactions on Aerospace and Electronic Systems 56,
    4083--4098.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kisantal等人[2020] Kisantal, M., Sharma, S., Park, T.H., Izzo, D., Märtens, M.,
    D’Amico, S., 2020. 卫星姿态估计挑战：数据集、竞赛设计及结果。《IEEE航天与电子系统汇刊》56，4083--4098。
- en: 'Kosmidis et al. [2020] Kosmidis, L., Rodriguez, I., Jover, Á., Alcaide, S.,
    Lachaize, J., Abella, J., Notebaert, O., Cazorla, F.J., Steenari, D., 2020. Gpu4s:
    Embedded gpus in space-latest project updates. Microprocessors and Microsystems
    77, 103143.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosmidis等人[2020] Kosmidis, L., Rodriguez, I., Jover, Á., Alcaide, S., Lachaize,
    J., Abella, J., Notebaert, O., Cazorla, F.J., Steenari, D., 2020. GPU4S：嵌入式GPU在太空中的最新项目更新。《微处理器与微系统》77，103143。
- en: 'Kreisel [2002] Kreisel, J., 2002. On-orbit servicing of satellites (oos): its
    potential market & impact, in: proceedings of 7th ESA Workshop on Advanced Space
    Technologies for Robotics and Automation ‘ASTRA.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreisel [2002] Kreisel, J., 2002. 卫星在轨服务（OOS）：其潜在市场与影响，见：第七届ESA高级空间技术研讨会‘ASTRA’论文集。
- en: Krizhevsky et al. [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks. Communications
    of the ACM 60, 84 -- 90.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等人[2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. 使用深度卷积神经网络进行Imagenet分类。《ACM通讯》60，84
    -- 90。
- en: '[75] Legrand, A., Detry, R., De Vleeschouwer, C., . End-to-end neural estimation
    of spacecraft pose with intermediate detection of keypoints .'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Legrand, A., Detry, R., De Vleeschouwer, C., . 端到端的神经网络航天器姿态估计与关键点的中间检测。'
- en: 'Lengyel et al. [2021] Lengyel, A., Garg, S., Milford, M., van Gemert, J.C.,
    2021. Zero-shot day-night domain adaptation with a physics prior, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 4399--4409.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lengyel 等人 [2021] Lengyel, A., Garg, S., Milford, M., van Gemert, J.C., 2021.
    带有物理先验的零样本日夜领域适应, 在：IEEE/CVF 国际计算机视觉会议论文集, pp. 4399--4409。
- en: 'Leon et al. [2022] Leon, V., Lentaris, G., Soudris, D., Vellas, S., Bernou,
    M., 2022. Towards employing fpga and asip acceleration to enable onboard ai/ml
    in space applications, in: 2022 IFIP/IEEE 30th International Conference on Very
    Large Scale Integration (VLSI-SoC), IEEE. pp. 1--4.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leon 等人 [2022] Leon, V., Lentaris, G., Soudris, D., Vellas, S., Bernou, M.,
    2022. 旨在利用 FPGA 和 ASIP 加速实现空间应用中的板载 AI/ML, 在：2022 IFIP/IEEE 第三十届国际超大规模集成会议 (VLSI-SoC),
    IEEE. pp. 1--4。
- en: Leong et al. [2020a] Leong, M.C., Prasad, D.K., Lee, Y.T., Lin, F., 2020a. Semi-cnn
    architecture for effective spatio-temporal learning in action recognition. Applied
    Sciences 10, 557.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leong 等人 [2020a] Leong, M.C., Prasad, D.K., Lee, Y.T., Lin, F., 2020a. 用于动作识别的有效时空学习的半卷积神经网络架构。应用科学
    10, 557。
- en: Leong et al. [2020b] Leong, M.C., Prasad, D.K., Lee, Y.T., Lin, F., 2020b. Semi-cnn
    architecture for effective spatio-temporal learning in action recognition. Applied
    Sciences 10, 557.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leong 等人 [2020b] Leong, M.C., Prasad, D.K., Lee, Y.T., Lin, F., 2020b. 用于动作识别的有效时空学习的半卷积神经网络架构。应用科学
    10, 557。
- en: 'Lepetit et al. [2009] Lepetit, V., Moreno-Noguer, F., Fua, P., 2009. Epnp:
    An accurate o (n) solution to the pnp problem. International journal of computer
    vision 81, 155.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lepetit 等人 [2009] Lepetit, V., Moreno-Noguer, F., Fua, P., 2009. Epnp: 一种准确的
    O (N) 解决方案用于 PNP 问题。国际计算机视觉期刊 81, 155。'
- en: 'Li et al. [2022] Li, K., Zhang, H., Hu, C., 2022. Learning-based pose estimation
    of non-cooperative spacecrafts with uncertainty prediction. Aerospace 9. URL:
    [https://www.mdpi.com/2226-4310/9/10/592](https://www.mdpi.com/2226-4310/9/10/592),
    doi:[10.3390/aerospace9100592](https:/doi.org/10.3390/aerospace9100592).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2022] Li, K., Zhang, H., Hu, C., 2022. 基于学习的非合作航天器姿态估计及不确定性预测。航空航天 9.
    URL: [https://www.mdpi.com/2226-4310/9/10/592](https://www.mdpi.com/2226-4310/9/10/592),
    doi:[10.3390/aerospace9100592](https:/doi.org/10.3390/aerospace9100592)。'
- en: 'Li et al. [2018] Li, O., Liu, H., Chen, C., Rudin, C., 2018. Deep learning
    for case-based reasoning through prototypes: A neural network that explains its
    predictions, in: Proceedings of the AAAI Conference on Artificial Intelligence.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018] Li, O., Liu, H., Chen, C., Rudin, C., 2018. 通过原型的深度学习案例推理：一种能够解释其预测的神经网络,
    在：AAAI 人工智能会议论文集。
- en: 'Li et al. [2019] Li, W.J., Cheng, D.Y., Liu, X.G., Wang, Y.B., Shi, W.H., Tang,
    Z.X., Gao, F., Zeng, F.M., Chai, H.Y., Luo, W.B., et al., 2019. On-orbit service
    (oos) of spacecraft: A review of engineering developments. Progress in Aerospace
    Sciences 108, 32--120.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2019] Li, W.J., Cheng, D.Y., Liu, X.G., Wang, Y.B., Shi, W.H., Tang,
    Z.X., Gao, F., Zeng, F.M., Chai, H.Y., Luo, W.B., 等, 2019. 航天器在轨服务（OOS）：工程进展回顾。航空航天科学进展
    108, 32--120。
- en: 'Lin et al. [2014a] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014a. Microsoft COCO: common objects
    in context, in: Fleet, D.J., Pajdla, T., Schiele, B., Tuytelaars, T. (Eds.), Computer
    Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September
    6-12, 2014, Proceedings, Part V, Springer. pp. 740--755. URL: [https://doi.org/10.1007/978-3-319-10602-1_48](https://doi.org/10.1007/978-3-319-10602-1_48),
    doi:[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2014a] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan,
    D., Dollár, P., Zitnick, C.L., 2014a. Microsoft COCO: 语境中的常见物体, 在：Fleet, D.J.,
    Pajdla, T., Schiele, B., Tuytelaars, T.（编辑）, 计算机视觉 - ECCV 2014 - 第十三届欧洲会议, 苏黎世,
    瑞士, 2014年9月6-12日, 论文集, 第五部分, Springer. pp. 740--755. URL: [https://doi.org/10.1007/978-3-319-10602-1_48](https://doi.org/10.1007/978-3-319-10602-1_48),
    doi:[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48)。'
- en: 'Lin et al. [2017] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017. Feature pyramid networks for object detection, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2117--2125.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2017] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie,
    S., 2017. 用于目标检测的特征金字塔网络, 在：IEEE 计算机视觉与模式识别会议论文集, pp. 2117--2125。
- en: 'Lin et al. [2014b] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014b. Microsoft coco: Common objects
    in context, in: Computer Vision--ECCV 2014: 13th European Conference, Zurich,
    Switzerland, September 6-12, 2014, Proceedings, Part V 13, Springer. pp. 740--755.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2014b] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan,
    D., Dollár, P., Zitnick, C.L., 2014b. Microsoft COCO: 语境中的常见物体, 在：计算机视觉--ECCV
    2014: 第十三届欧洲会议, 苏黎世, 瑞士, 2014年9月6-12日, 论文集, 第五部分 13, Springer. pp. 740--755。'
- en: Liu and Hu [2014] Liu, C., Hu, W., 2014. Relative pose estimation for cylinder-shaped
    spacecrafts using single image. IEEE Transactions on Aerospace and Electronic
    Systems 50, 3036--3056.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Hu [2014] Liu, C., Hu, W., 2014. 使用单图像进行圆柱形航天器的相对姿态估计。IEEE 航空航天与电子系统交易
    50，3036--3056。
- en: 'Liu et al. [2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016. Ssd: Single shot multibox detector, in: European conference
    on computer vision, Springer. pp. 21--37.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu,
    C.Y., Berg, A.C., 2016. SSD：单次检测多框检测器，收录于：欧洲计算机视觉会议，Springer，第21--37页。
- en: 'Llorente et al. [2013] Llorente, J.S., Agenjo, A., Carrascosa, C., de Negueruela,
    C., Mestreau-Garreau, A., Cropp, A., Santovincenzo, A., 2013. Proba-3: Precise
    formation flying demonstration mission. Acta Astronautica 82, 38--46.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llorente et al. [2013] Llorente, J.S., Agenjo, A., Carrascosa, C., de Negueruela,
    C., Mestreau-Garreau, A., Cropp, A., Santovincenzo, A., 2013. Proba-3：精确编队飞行演示任务。宇航学报
    82，38--46。
- en: 'Long et al. [2020a] Long, X., Deng, K., Wang, G., Zhang, Y., Dang, Q., Gao,
    Y., Shen, H., Ren, J., Han, S., Ding, E., et al., 2020a. Pp-yolo: An effective
    and efficient implementation of object detector. arXiv preprint arXiv:2007.12099
    .'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. [2020a] Long, X., Deng, K., Wang, G., Zhang, Y., Dang, Q., Gao,
    Y., Shen, H., Ren, J., Han, S., Ding, E., 等，2020a. Pp-yolo：一种有效且高效的物体检测器实现。arXiv
    预印本 arXiv:2007.12099。
- en: 'Long et al. [2020b] Long, X., Deng, K., Wang, G., Zhang, Y., Dang, Q., Gao,
    Y., Shen, H., Ren, J., Han, S., Ding, E., et al., 2020b. Pp-yolo: An effective
    and efficient implementation of object detector. arXiv preprint arXiv:2007.12099
    .'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. [2020b] Long, X., Deng, K., Wang, G., Zhang, Y., Dang, Q., Gao,
    Y., Shen, H., Ren, J., Han, S., Ding, E., 等，2020b. Pp-yolo：一种有效且高效的物体检测器实现。arXiv
    预印本 arXiv:2007.12099。
- en: Lotti et al. [2022] Lotti, A., Modenini, D., Tortora, P., Saponara, M., Perino,
    M.A., 2022. Deep Learning for Real Time Satellite Pose Estimation on Low Power
    Edge TPU. arXiv e-prints , arXiv:2204.03296[arXiv:2204.03296](http://arxiv.org/abs/2204.03296).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lotti et al. [2022] Lotti, A., Modenini, D., Tortora, P., Saponara, M., Perino,
    M.A., 2022. 低功耗边缘 TPU 上的实时卫星姿态估计的深度学习。arXiv 电子版，arXiv:2204.03296[arXiv:2204.03296](http://arxiv.org/abs/2204.03296)。
- en: Lotti et al. [2022] Lotti, A., Modenini, D., Tortora, P., Saponara, M., Perino,
    M.A., 2022. Deep learning for real time satellite pose estimation on low power
    edge tpu. arXiv preprint arXiv:2204.03296 .
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lotti et al. [2022] Lotti, A., Modenini, D., Tortora, P., Saponara, M., Perino,
    M.A., 2022. 低功耗边缘 TPU 上的实时卫星姿态估计的深度学习。arXiv 预印本 arXiv:2204.03296。
- en: 'Marchand et al. [2015] Marchand, E., Uchiyama, H., Spindler, F., 2015. Pose
    estimation for augmented reality: a hands-on survey. IEEE transactions on visualization
    and computer graphics 22, 2633--2651.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marchand et al. [2015] Marchand, E., Uchiyama, H., Spindler, F., 2015. 增强现实中的姿态估计：实践调查。IEEE
    可视化与计算机图形学交易 22，2633--2651。
- en: 'Martin et al. [2019] Martin, I., Dunstan, M., Gestido, M.S., 2019. Planetary
    surface image generation for testing future space missions with pangu, in: 2nd
    RPI Space Imaging Workshop, Sensing, Estimation, and Automation Laboratory.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martin et al. [2019] Martin, I., Dunstan, M., Gestido, M.S., 2019. 用 pangu 进行未来太空任务的行星表面图像生成，收录于：第2届
    RPI 太空成像研讨会，传感、估计与自动化实验室。
- en: 'Marullo et al. [2022] Marullo, G., Tanzi, L., Piazzolla, P., Vezzetti, E.,
    2022. 6d object position estimation from 2d images: a literature review. Multimedia
    Tools and Applications , 1--39.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marullo et al. [2022] Marullo, G., Tanzi, L., Piazzolla, P., Vezzetti, E., 2022.
    从 2D 图像中估计 6D 物体位置：文献综述。多媒体工具与应用，1--39。
- en: May [2021] May, C., 2021. Triggers and effects of an active debris removal market.
    The Aerospace Corporation, Center for Space Policy and Strategy, Tech. Rep , 2021--01.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: May [2021] May, C., 2021. 主动碎片移除市场的触发因素和影响。航空航天公司，太空政策与战略中心，技术报告，2021--01。
- en: 'Mertan et al. [2022] Mertan, A., Duff, D.J., Unal, G., 2022. Single image depth
    estimation: An overview. Digital Signal Processing , 103441.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mertan et al. [2022] Mertan, A., Duff, D.J., Unal, G., 2022. 单图像深度估计：概述。数字信号处理，103441。
- en: 'Minaee et al. [2021] Minaee, S., Boykov, Y.Y., Porikli, F., Plaza, A.J., Kehtarnavaz,
    N., Terzopoulos, D., 2021. Image segmentation using deep learning: A survey. IEEE
    transactions on pattern analysis and machine intelligence .'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee et al. [2021] Minaee, S., Boykov, Y.Y., Porikli, F., Plaza, A.J., Kehtarnavaz,
    N., Terzopoulos, D., 2021. 使用深度学习的图像分割：综述。IEEE 模式分析与机器智能交易。
- en: Mittelhammer et al. [2000] Mittelhammer, R.C., Judge, G.G., Miller, D.J., 2000.
    Econometric foundations. Cambridge University Press.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mittelhammer et al. [2000] Mittelhammer, R.C., Judge, G.G., Miller, D.J., 2000.
    计量经济学基础。剑桥大学出版社。
- en: 'Moré [1978] Moré, J.J., 1978. The levenberg-marquardt algorithm: implementation
    and theory, in: Numerical analysis. Springer, pp. 105--116.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moré [1978] Moré, J.J., 1978. Levenberg-Marquardt 算法：实现与理论，收录于：数值分析。Springer，第105--116页。
- en: 'Mumuni and Mumuni [2022] Mumuni, A., Mumuni, F., 2022. Data augmentation: A
    comprehensive survey of modern approaches. Array 16, 100258. URL: [https://www.sciencedirect.com/science/article/pii/S2590005622000911](https://www.sciencedirect.com/science/article/pii/S2590005622000911),
    doi:[https://doi.org/10.1016/j.array.2022.100258](https:/doi.org/https://doi.org/10.1016/j.array.2022.100258).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mumuni 和 Mumuni [2022] Mumuni, A., Mumuni, F., 2022. 数据增强：现代方法的综合调查。《Array》16,
    100258。URL: [https://www.sciencedirect.com/science/article/pii/S2590005622000911](https://www.sciencedirect.com/science/article/pii/S2590005622000911)，doi:[https://doi.org/10.1016/j.array.2022.100258](https://doi.org/10.1016/j.array.2022.100258)。'
- en: 'Musallam et al. [2022] Musallam, M.A., Gaudillière, V., del Castillo, M.O.,
    Al Ismaeil, K., Aouada, D., 2022. Leveraging equivariant features for absolute
    pose regression, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 6876--6886.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Musallam 等人 [2022] Musallam, M.A., Gaudillière, V., del Castillo, M.O., Al Ismaeil,
    K., Aouada, D., 2022. 利用等变特征进行绝对姿态回归，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，第6876--6886页。
- en: '[104] OpenCV, . Perspective-n-point (pnp) pose computation. URL: [https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html](https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] OpenCV. Perspective-n-point (pnp) 姿态计算。URL: [https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html](https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html)。'
- en: Opromolla et al. [2017] Opromolla, R., Fasano, G., Rufino, G., Grassi, M., 2017.
    A review of cooperative and uncooperative spacecraft pose determination techniques
    for close-proximity operations. Progress in Aerospace Sciences 93, 53--72.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Opromolla 等人 [2017] Opromolla, R., Fasano, G., Rufino, G., Grassi, M., 2017.
    合作与非合作航天器姿态确定技术的综述。航空航天科学进展 93, 53--72。
- en: 'Park et al. [2020] Park, K., Mousavian, A., Xiang, Y., Fox, D., 2020. Latentfusion:
    End-to-end differentiable reconstruction and rendering for unseen object pose
    estimation, in: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, pp. 10710--10719.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2020] Park, K., Mousavian, A., Xiang, Y., Fox, D., 2020. Latentfusion：用于未见物体姿态估计的端到端可微重建和渲染，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，第10710--10719页。
- en: 'Park et al. [2019a] Park, K., Patten, T., Vincze, M., 2019a. Pix2pose: Pixel-wise
    coordinate regression of objects for 6d pose estimation, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 7668--7677.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2019a] Park, K., Patten, T., Vincze, M., 2019a. Pix2pose：用于6D姿态估计的像素级坐标回归，发表于：IEEE/CVF
    国际计算机视觉会议论文集，第7668--7677页。
- en: 'Park et al. [2021] Park, T.H., Bosse, J., D’Amico, S., 2021. Robotic testbed
    for rendezvous and optical navigation: Multi-source calibration and machine learning
    use cases. arXiv preprint arXiv:2108.05529 .'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2021] Park, T.H., Bosse, J., D’Amico, S., 2021. 用于会合和光学导航的机器人测试平台：多源标定和机器学习应用。arXiv
    预印本 arXiv:2108.05529。
- en: Park and D’Amico [2022a] Park, T.H., D’Amico, S., 2022a. Adaptive neural network-based
    unscented kalman filter for spacecraft pose tracking at rendezvous. arXiv preprint
    arXiv:2206.03796 .
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 和 D’Amico [2022a] Park, T.H., D’Amico, S., 2022a. 基于自适应神经网络的无味卡尔曼滤波器用于航天器会合跟踪。arXiv
    预印本 arXiv:2206.03796。
- en: Park and D’Amico [2022b] Park, T.H., D’Amico, S., 2022b. Robust multi-task learning
    and online refinement for spacecraft pose estimation across domain gap. arXiv
    preprint arXiv:2203.04275 .
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 和 D’Amico [2022b] Park, T.H., D’Amico, S., 2022b. 适用于跨领域差距的航天器姿态估计的稳健多任务学习和在线优化。arXiv
    预印本 arXiv:2203.04275。
- en: 'Park and D’Amico [2023] Park, T.H., D’Amico, S., 2023. Robust multi-task learning
    and online refinement for spacecraft pose estimation across domain gap. Advances
    in Space Research URL: [https://www.sciencedirect.com/science/article/pii/S0273117723002284](https://www.sciencedirect.com/science/article/pii/S0273117723002284),
    doi:[https://doi.org/10.1016/j.asr.2023.03.036](https:/doi.org/https://doi.org/10.1016/j.asr.2023.03.036).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 和 D’Amico [2023] Park, T.H., D’Amico, S., 2023. 适用于跨领域差距的航天器姿态估计的稳健多任务学习和在线优化。《空间研究进展》
    URL: [https://www.sciencedirect.com/science/article/pii/S0273117723002284](https://www.sciencedirect.com/science/article/pii/S0273117723002284)，doi:[https://doi.org/10.1016/j.asr.2023.03.036](https://doi.org/10.1016/j.asr.2023.03.036)。'
- en: 'Park et al. [2023a] Park, T.H., Märtens, M., Jawaid, M., Wang, Z., Chen, B.,
    Chin, T.J., Izzo, D., D’Amico, S., 2023a. Satellite pose estimation competition
    2021: Results and analyses. Acta Astronautica .'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2023a] Park, T.H., Märtens, M., Jawaid, M., Wang, Z., Chen, B., Chin,
    T.J., Izzo, D., D’Amico, S., 2023a. 卫星姿态估计竞赛2021：结果与分析。《航天学报》。
- en: 'Park et al. [2023b] Park, T.H., Märtens, M., Jawaid, M., Wang, Z., Chen, B.,
    Chin, T.J., Izzo, D., D’Amico, S., 2023b. Satellite pose estimation competition
    2021: Results and analyses. Acta Astronautica 204, 640--665. URL: [https://www.sciencedirect.com/science/article/pii/S0094576523000048](https://www.sciencedirect.com/science/article/pii/S0094576523000048),
    doi:[https://doi.org/10.1016/j.actaastro.2023.01.002](https:/doi.org/https://doi.org/10.1016/j.actaastro.2023.01.002).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2023b] Park, T.H., Märtens, M., Jawaid, M., Wang, Z., Chen, B., Chin,
    T.J., Izzo, D., D’Amico, S., 2023b. 卫星姿态估计竞赛 2021：结果与分析。Acta Astronautica 204,
    640--665。网址：[https://www.sciencedirect.com/science/article/pii/S0094576523000048](https://www.sciencedirect.com/science/article/pii/S0094576523000048)，doi：[https://doi.org/10.1016/j.actaastro.2023.01.002](https://doi.org/10.1016/j.actaastro.2023.01.002)。
- en: Park et al. [2019b] Park, T.H., Sharma, S., D’Amico, S., 2019b. Towards robust
    learning-based pose estimation of noncooperative spacecraft. arXiv preprint arXiv:1909.00392
    .
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2019b] Park, T.H., Sharma, S., D’Amico, S., 2019b. 朝着强健的基于学习的非合作航天器姿态估计。arXiv
    预印本 arXiv:1909.00392。
- en: Park et al. [2019c] Park, T.H., Sharma, S., D’Amico, S., 2019c. Towards robust
    learning-based pose estimation of noncooperative spacecraft. arXiv preprint arXiv:1909.00392
    .
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2019c] Park, T.H., Sharma, S., D’Amico, S., 2019c. 朝着强健的基于学习的非合作航天器姿态估计。arXiv
    预印本 arXiv:1909.00392。
- en: Pauly et al. [2022] Pauly, L., Jamrozik, M.L., Del Castillo, M.O., Borgue, O.,
    Singh, I.P., Makhdoomi, M.R., Christidi-Loumpasefski, O.O., Gaudilliere, V., Martinez,
    C., Rathinam, A., et al., 2022. Lessons from a space lab--an image acquisition
    perspective. arXiv preprint arXiv:2208.08865 .
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pauly 等 [2022] Pauly, L., Jamrozik, M.L., Del Castillo, M.O., Borgue, O., Singh,
    I.P., Makhdoomi, M.R., Christidi-Loumpasefski, O.O., Gaudilliere, V., Martinez,
    C., Rathinam, A., 等，2022. 从空间实验室中汲取的经验——图像采集的视角。arXiv 预印本 arXiv:2208.08865。
- en: 'Pearl and Mackenzie [2018] Pearl, J., Mackenzie, D., 2018. The book of why:
    the new science of cause and effect. Basic books.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearl 和 Mackenzie [2018] Pearl, J., Mackenzie, D., 2018. 《为何之书：因果关系的新科学》。Basic
    books。
- en: 'Peng et al. [2018] Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D.,
    2018. Jointly optimize data augmentation and network training: Adversarial data
    augmentation in human pose estimation, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 2226--2234.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 [2018] Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D., 2018.
    联合优化数据增强和网络训练：在人类姿态估计中的对抗性数据增强，发表于：IEEE 计算机视觉与模式识别会议论文集，页码 2226--2234。
- en: Phisannupawong et al. [2020] Phisannupawong, T., Kamsing, P., Torteeka, P.,
    Channumsin, S., Sawangwit, U., Hematulin, W., Jarawan, T., Somjit, T., Yooyen,
    S., Delahaye, D., et al., 2020. Vision-based spacecraft pose estimation via a
    deep convolutional neural network for noncooperative docking operations. Aerospace
    7, 126.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phisannupawong 等 [2020] Phisannupawong, T., Kamsing, P., Torteeka, P., Channumsin,
    S., Sawangwit, U., Hematulin, W., Jarawan, T., Somjit, T., Yooyen, S., Delahaye,
    D., 等，2020. 基于视觉的航天器姿态估计通过深度卷积神经网络用于非合作对接操作。Aerospace 7, 126。
- en: 'Piazza et al. [2021] Piazza, M., Maestrini, M., Di Lizia, P., et al., 2021.
    Deep learning-based monocular relative pose estimation of uncooperative spacecraft,
    in: 8th European Conference on Space Debris, ESA/ESOC, ESA. pp. 1--13.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piazza 等 [2021] Piazza, M., Maestrini, M., Di Lizia, P., 等，2021. 基于深度学习的单目相对姿态估计非合作航天器，发表于：第八届欧洲空间碎片会议，ESA/ESOC，ESA。页码
    1--13。
- en: 'Posso et al. [2022] Posso, J., Bois, G., Savaria, Y., 2022. Mobile-ursonet:
    an embeddable neural network for onboard spacecraft pose estimation. arXiv preprint
    arXiv:2205.02065 .'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Posso 等 [2022] Posso, J., Bois, G., Savaria, Y., 2022. Mobile-ursonet：一种嵌入式神经网络用于航天器姿态估计。arXiv
    预印本 arXiv:2205.02065。
- en: Powell et al. [2018] Powell, W., Campola, M., Sheets, T., Davidson, A., Welsh,
    S., 2018. Commercial Off-The-Shelf GPU Qualification for Space Applications. Technical
    Report.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powell 等 [2018] Powell, W., Campola, M., Sheets, T., Davidson, A., Welsh, S.,
    2018. 商用现成 GPU 在太空应用中的认证。技术报告。
- en: 'Price and Yoshida [2021] Price, A., Yoshida, K., 2021. A monocular pose estimation
    case study: The hayabusa2 minerva-ii2 deployment, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 1992--2001.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Price 和 Yoshida [2021] Price, A., Yoshida, K., 2021. 单目姿态估计案例研究：hayabusa2 minerva-ii2
    部署，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，页码 1992--2001。
- en: 'Proença and Gao [2020] Proença, P.F., Gao, Y., 2020. Deep learning for spacecraft
    pose estimation from photorealistic rendering, in: 2020 IEEE International Conference
    on Robotics and Automation (ICRA), IEEE. pp. 6007--6013.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proença 和 Gao [2020] Proença, P.F., Gao, Y., 2020. 从逼真渲染中进行航天器姿态估计的深度学习，发表于：2020
    IEEE 国际机器人与自动化会议 (ICRA)，IEEE。页码 6007--6013。
- en: 'Quigley et al. [2009] Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote,
    T., Leibs, J., Wheeler, R., Ng, A.Y., et al., 2009. Ros: an open-source robot
    operating system, in: ICRA workshop on open source software, Kobe, Japan. p. 5.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quigley 等 [2009] Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T.,
    Leibs, J., Wheeler, R., Ng, A.Y., 等, 2009. Ros: 一个开源机器人操作系统, 载于: ICRA 开源软件研讨会,
    神户, 日本. 第 5 页。'
- en: 'Rathinam and Gao [2020] Rathinam, A., Gao, Y., 2020. On-orbit relative navigation
    near a known target using monocular vision and convolutional neural networks for
    pose estimation, in: International Symposium on Artificial Intelligence, Robotics
    and Automation in Space (iSAIRAS), Virutal Conference (Pasadena, CA:), pp. 1--6.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rathinam 和 Gao [2020] Rathinam, A., Gao, Y., 2020. 使用单目视觉和卷积神经网络进行已知目标附近的在轨相对导航,
    载于: 国际人工智能、机器人与空间自动化研讨会 (iSAIRAS), 虚拟会议 (帕萨迪纳，加州：), 第 1--6 页。'
- en: 'Rathinam et al. [2022] Rathinam, A., Gaudilliere, V., Mohamed Ali, M.A., Ortiz
    Del Castillo, M., Pauly, L., Aouada, D., 2022. SPARK 2022 Dataset : Spacecraft
    Detection and Trajectory Estimation. URL: [https://doi.org/10.5281/zenodo.6599762](https://doi.org/10.5281/zenodo.6599762),
    doi:[10.5281/zenodo.6599762](https:/doi.org/10.5281/zenodo.6599762).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rathinam 等 [2022] Rathinam, A., Gaudilliere, V., Mohamed Ali, M.A., Ortiz Del
    Castillo, M., Pauly, L., Aouada, D., 2022. SPARK 2022 数据集：航天器检测和轨迹估计。网址: [https://doi.org/10.5281/zenodo.6599762](https://doi.org/10.5281/zenodo.6599762)，doi:
    [10.5281/zenodo.6599762](https:/doi.org/10.5281/zenodo.6599762)。'
- en: 'Rathinam et al. [2021] Rathinam, A., Hao, Z., Gao, Y., 2021. Autonomous visual
    navigation for spacecraft on-orbit operations, in: Space Robotics and Autonomous
    Systems: Technologies, advances and applications. Institution of Engineering and
    Technology, pp. 125--157. URL: [https://doi.org/10.1049/PBCE131E_ch5](https://doi.org/10.1049/PBCE131E_ch5).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rathinam 等 [2021] Rathinam, A., Hao, Z., Gao, Y., 2021. 用于航天器在轨操作的自主视觉导航, 载于:
    空间机器人和自主系统：技术、进展与应用. 工程技术学会, 第 125--157 页。网址: [https://doi.org/10.1049/PBCE131E_ch5](https://doi.org/10.1049/PBCE131E_ch5)。'
- en: 'Redd [2020] Redd, N.T., 2020. Bringing satellites back from the dead: Mission
    extension vehicles give defunct spacecraft a new lease on life - [news]. IEEE
    Spectrum 57, 6--7. doi:[10.1109/MSPEC.2020.9150540](https:/doi.org/10.1109/MSPEC.2020.9150540).'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redd [2020] Redd, N.T., 2020. 让卫星重生：任务扩展车辆为失效航天器带来新生 - [新闻]。IEEE Spectrum 57,
    6--7。doi: [10.1109/MSPEC.2020.9150540](https:/doi.org/10.1109/MSPEC.2020.9150540)。'
- en: 'Redmon et al. [2016] Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You only look once: Unified, real-time object detection, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 779--788.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 等 [2016] Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. 你只需看一次：统一的实时目标检测,
    载于: IEEE 计算机视觉与模式识别会议论文集, 第 779--788 页。'
- en: 'Redmon and Farhadi [2017] Redmon, J., Farhadi, A., 2017. Yolo9000: better,
    faster, stronger, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, pp. 7263--7271.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 和 Farhadi [2017] Redmon, J., Farhadi, A., 2017. Yolo9000: 更好、更快、更强,
    载于: IEEE 计算机视觉与模式识别会议论文集, 第 7263--7271 页。'
- en: 'Redmon and Farhadi [2018] Redmon, J., Farhadi, A., 2018. Yolov3: An incremental
    improvement. arXiv preprint arXiv:1804.02767 .'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 和 Farhadi [2018] Redmon, J., Farhadi, A., 2018. Yolov3: 一项渐进式改进。arXiv
    预印本 arXiv:1804.02767。'
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks. Advances in
    neural information processing systems 28.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等 [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: 向实时目标检测迈进，使用区域提议网络。神经信息处理系统进展
    28。'
- en: 'Rennie et al. [2016] Rennie, C., Shome, R., Bekris, K.E., Souza, A.F.D., 2016.
    A dataset for improved rgbd-based object detection and pose estimation for warehouse
    pick-and-place. IEEE Robotics Autom. Lett. 1, 1179--1185. URL: [https://doi.org/10.1109/LRA.2016.2532924](https://doi.org/10.1109/LRA.2016.2532924),
    doi:[10.1109/LRA.2016.2532924](https:/doi.org/10.1109/LRA.2016.2532924).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rennie 等 [2016] Rennie, C., Shome, R., Bekris, K.E., Souza, A.F.D., 2016. 用于改进基于
    RGBD 的目标检测和姿态估计的仓库数据集。IEEE 机器人自动化通讯. 1, 1179--1185。网址: [https://doi.org/10.1109/LRA.2016.2532924](https://doi.org/10.1109/LRA.2016.2532924)，doi:
    [10.1109/LRA.2016.2532924](https:/doi.org/10.1109/LRA.2016.2532924)。'
- en: 'Rondao and Aouf [2018] Rondao, D., Aouf, N., 2018. Multi-view monocular pose
    estimation for spacecraft relative navigation, in: 2018 AIAA Guidance, Navigation,
    and Control Conference, p. 2100.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rondao 和 Aouf [2018] Rondao, D., Aouf, N., 2018. 用于航天器相对导航的多视角单目姿态估计, 载于: 2018
    AIAA 指导、导航和控制会议, 第 2100 页。'
- en: 'Rondao et al. [2021] Rondao, D., Aouf, N., Richardson, M.A., 2021. Chinet:
    Deep recurrent convolutional learning for multimodal spacecraft pose estimation.
    CoRR abs/2108.10282. URL: [https://arxiv.org/abs/2108.10282](https://arxiv.org/abs/2108.10282),
    [arXiv:2108.10282](http://arxiv.org/abs/2108.10282).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rondao 等 [2021] Rondao, D., Aouf, N., Richardson, M.A., 2021. Chinet: 深度递归卷积学习用于多模态航天器姿态估计。CoRR
    abs/2108.10282。网址：[https://arxiv.org/abs/2108.10282](https://arxiv.org/abs/2108.10282)，[arXiv:2108.10282](http://arxiv.org/abs/2108.10282)。'
- en: 'Rondao et al. [2022] Rondao, D., Aouf, N., Richardson, M.A., 2022. Chinet:
    Deep recurrent convolutional learning for multimodal spacecraft pose estimation.
    IEEE Transactions on Aerospace and Electronic Systems .'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rondao 等 [2022] Rondao, D., Aouf, N., Richardson, M.A., 2022. Chinet: 深度递归卷积学习用于多模态航天器姿态估计。IEEE
    航空航天与电子系统学报。'
- en: 'Ronneberger et al. [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
    Convolutional networks for biomedical image segmentation, in: International Conference
    on Medical image computing and computer-assisted intervention, Springer. pp. 234--241.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等 [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: 用于生物医学图像分割的卷积网络，见：国际医学图像计算与计算机辅助干预会议，Springer。第234--241页。'
- en: Ruder [2017] Ruder, S., 2017. An overview of multi-task learning in deep neural
    networks. arXiv preprint arXiv:1706.05098 .
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder [2017] Ruder, S., 2017. 深度神经网络中的多任务学习概述。arXiv 预印本 arXiv:1706.05098。
- en: 'Russakovsky et al. [2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C.,
    Fei-Fei, L., 2015. Imagenet large scale visual recognition challenge. Int. J.
    Comput. Vis. 115, 211--252. URL: [https://doi.org/10.1007/s11263-015-0816-y](https://doi.org/10.1007/s11263-015-0816-y),
    doi:[10.1007/s11263-015-0816-y](https:/doi.org/10.1007/s11263-015-0816-y).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky 等 [2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C.,
    Fei-Fei, L., 2015. ImageNet 大规模视觉识别挑战。计算机视觉国际期刊 115, 211--252。网址：[https://doi.org/10.1007/s11263-015-0816-y](https://doi.org/10.1007/s11263-015-0816-y)，doi：[10.1007/s11263-015-0816-y](https://doi.org/10.1007/s11263-015-0816-y)。
- en: 'Sabatini et al. [2015] Sabatini, M., Palmerini, G.B., Gasbarri, P., 2015. A
    testbed for visual based navigation and control during space rendezvous operations.
    Acta Astronautica 117, 184--196. URL: [https://www.sciencedirect.com/science/article/pii/S0094576515003070](https://www.sciencedirect.com/science/article/pii/S0094576515003070),
    doi:[https://doi.org/10.1016/j.actaastro.2015.07.026](https:/doi.org/https://doi.org/10.1016/j.actaastro.2015.07.026).'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabatini 等 [2015] Sabatini, M., Palmerini, G.B., Gasbarri, P., 2015. 用于视觉导航和控制的测试平台，特别是在空间对接操作期间。宇航学报
    117, 184--196。网址：[https://www.sciencedirect.com/science/article/pii/S0094576515003070](https://www.sciencedirect.com/science/article/pii/S0094576515003070)，doi：[https://doi.org/10.1016/j.actaastro.2015.07.026](https://doi.org/10.1016/j.actaastro.2015.07.026)。
- en: 'Sandler et al. [2018] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen,
    L.C., 2018. Mobilenetv2: Inverted residuals and linear bottlenecks, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 4510--4520.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler 等 [2018] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.,
    2018. Mobilenetv2: 反向残差和线性瓶颈，见：IEEE 计算机视觉与模式识别会议论文集，第4510--4520页。'
- en: Shafer and Vovk [2008] Shafer, G., Vovk, V., 2008. A tutorial on conformal prediction.
    Journal of Machine Learning Research 9.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafer 和 Vovk [2008] Shafer, G., Vovk, V., 2008. 关于保形预测的教程。机器学习研究期刊 9。
- en: Shannon [1948] Shannon, C.E., 1948. A mathematical theory of communication.
    The Bell system technical journal 27, 379--423.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon [1948] Shannon, C.E., 1948. 通信的数学理论。贝尔系统技术期刊 27, 379--423。
- en: 'Sharma et al. [2018a] Sharma, S., Beierle, C., D’Amico, S., 2018a. Pose estimation
    for non-cooperative spacecraft rendezvous using convolutional neural networks,
    in: 2018 IEEE Aerospace Conference, IEEE. pp. 1--12.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等 [2018a] Sharma, S., Beierle, C., D’Amico, S., 2018a. 使用卷积神经网络进行非合作性航天器对接的姿态估计，见：2018
    IEEE 航空会议，IEEE。第1--12页。
- en: Sharma and D’Amico [2019] Sharma, S., D’Amico, S., 2019. Pose estimation for
    non-cooperative rendezvous using neural networks. arXiv preprint arXiv:1906.09868
    .
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 和 D’Amico [2019] Sharma, S., D’Amico, S., 2019. 使用神经网络进行非合作性对接的姿态估计。arXiv
    预印本 arXiv:1906.09868。
- en: Sharma et al. [2018b] Sharma, S., Ventura, J., D’Amico, S., 2018b. Robust model-based
    monocular pose initialization for noncooperative spacecraft rendezvous. Journal
    of Spacecraft and Rockets 55, 1414--1429.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等 [2018b] Sharma, S., Ventura, J., D’Amico, S., 2018b. 强健的基于模型的单目姿态初始化用于非合作性航天器对接。航天器与火箭学报
    55, 1414--1429。
- en: 'Shi et al. [2016] Shi, J., Ulrich, S., Ruel, S., 2016. Spacecraft pose estimation
    using a monocular camera, in: 67th International Astronautical Congress, Guadalajara.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. [2016] Shi, J., Ulrich, S., Ruel, S., 2016. 使用单目相机进行航天器姿态估计，见：第67届国际宇航大会，瓜达拉哈拉。
- en: 'Shreiner et al. [2009] Shreiner, D., Group, B.T.K.O.A.W., et al., 2009. OpenGL
    programming guide: the official guide to learning OpenGL, versions 3.0 and 3.1.
    Pearson Education.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shreiner et al. [2009] Shreiner, D., Group, B.T.K.O.A.W., 等，2009. OpenGL编程指南：学习OpenGL的官方指南，版本3.0和3.1。Pearson
    Education。
- en: Shui et al. [2019] Shui, C., Abbasi, M., Robitaille, L.É., Wang, B., Gagné,
    C., 2019. A principled approach for learning task similarity in multitask learning.
    arXiv preprint arXiv:1903.09109 .
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shui et al. [2019] Shui, C., Abbasi, M., Robitaille, L.É., Wang, B., Gagné,
    C., 2019. 在多任务学习中学习任务相似性的原则性方法。arXiv预印本arXiv:1903.09109。
- en: 'Song et al. [2022a] Song, J., Rondao, D., Aouf, N., 2022a. Deep learning-based
    spacecraft relative navigation methods: A survey. Acta Astronautica 191, 22--40.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2022a] Song, J., Rondao, D., Aouf, N., 2022a. 基于深度学习的航天器相对导航方法：综述。《宇航学报》191，22--40页。
- en: 'Song et al. [2022b] Song, Y., Wang, T., Mondal, S.K., Sahoo, J.P., 2022b. A
    comprehensive survey of few-shot learning: Evolution, applications, challenges,
    and opportunities. arXiv preprint arXiv:2205.06743 .'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2022b] Song, Y., Wang, T., Mondal, S.K., Sahoo, J.P., 2022b. 少样本学习的全面综述：演变、应用、挑战与机遇。arXiv预印本arXiv:2205.06743。
- en: 'Strutz [2011] Strutz, T., 2011. Data fitting and uncertainty: A practical introduction
    to weighted least squares and beyond. Springer.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strutz [2011] Strutz, T., 2011. 数据拟合与不确定性：加权最小二乘法及其扩展的实用介绍。Springer。
- en: 'Sun et al. [2019a] Sun, K., Xiao, B., Liu, D., Wang, J., 2019a. Deep high-resolution
    representation learning for human pose estimation, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pp. 5693--5703.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2019a] Sun, K., Xiao, B., Liu, D., Wang, J., 2019a. 用于人体姿态估计的深度高分辨率表示学习，见：IEEE/CVF计算机视觉与模式识别会议论文集，第5693--5703页。
- en: Sun et al. [2019b] Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D.,
    Mu, Y., Wang, X., Liu, W., Wang, J., 2019b. High-resolution representations for
    labeling pixels and regions. arXiv preprint arXiv:1904.04514 .
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2019b] Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D.,
    Mu, Y., Wang, X., Liu, W., Wang, J., 2019b. 用于标记像素和区域的高分辨率表示。arXiv预印本arXiv:1904.04514。
- en: '[156] Sweden, O., . Prisma. [https://www.ohb-sweden.se/space-missions/prisma](https://www.ohb-sweden.se/space-missions/prisma).
    Accessed: April 5, 2023.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] 瑞典，O.，Prisma。[https://www.ohb-sweden.se/space-missions/prisma](https://www.ohb-sweden.se/space-missions/prisma)。访问时间：2023年4月5日。'
- en: 'Szegedy et al. [2015] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with
    convolutions, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1--9.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. [2015] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. 通过卷积深入研究，见：IEEE计算机视觉与模式识别会议论文集，第1--9页。
- en: 'Szeliski [2022] Szeliski, R., 2022. Computer vision: algorithms and applications.
    Springer Nature.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szeliski [2022] Szeliski, R., 2022. 计算机视觉：算法与应用。Springer Nature。
- en: 'Tan and Le [2019] Tan, M., Le, Q., 2019. Efficientnet: Rethinking model scaling
    for convolutional neural networks, in: International conference on machine learning,
    PMLR. pp. 6105--6114.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan and Le [2019] Tan, M., Le, Q., 2019. Efficientnet: 重新思考卷积神经网络的模型缩放，见：国际机器学习会议，PMLR，第6105--6114页。'
- en: 'Tan et al. [2020] Tan, M., Pang, R., Le, Q.V., 2020. Efficientdet: Scalable
    and efficient object detection, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, pp. 10781--10790.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. [2020] Tan, M., Pang, R., Le, Q.V., 2020. Efficientdet: 可扩展且高效的物体检测，见：IEEE/CVF计算机视觉与模式识别会议论文集，第10781--10790页。'
- en: '[161] Tensorflow, . Tpu/models/official/efficientnet/lite at master · tensorflow/tpu.
    URL: [https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Tensorflow，Tpu/models/official/efficientnet/lite at master · tensorflow/tpu。网址：[https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite)。'
- en: 'Tibshirani et al. [2019] Tibshirani, R.J., Foygel Barber, R., Candes, E., Ramdas,
    A., 2019. Conformal prediction under covariate shift, in: Wallach, H., Larochelle,
    H., Beygelzimer, A., d''Alché-Buc, F., Fox, E., Garnett, R. (Eds.), Advances in
    Neural Information Processing Systems, Curran Associates, Inc. URL: [https://proceedings.neurips.cc/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf).'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tibshirani et al. [2019] Tibshirani, R.J., Foygel Barber, R., Candes, E., Ramdas,
    A., 2019年。在协变量偏移下的符合预测，见：Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc,
    F., Fox, E., Garnett, R. (编辑)，《神经信息处理系统进展》，Curran Associates, Inc. 网址：[https://proceedings.neurips.cc/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf)。
- en: Tobin et al. [2017a] Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W.,
    Abbeel, P., 2017a. Domain randomization for transferring deep neural networks
    from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS) , 23--30.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tobin et al. [2017a] Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W.,
    Abbeel, P., 2017a年。从模拟到现实世界的深度神经网络领域随机化。2017 IEEE/RSJ 国际智能机器人与系统会议（IROS），第23--30页。
- en: 'Tobin et al. [2017b] Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba,
    W., Abbeel, P., 2017b. Domain randomization for transferring deep neural networks
    from simulation to the real world, in: 2017 IEEE/RSJ international conference
    on intelligent robots and systems (IROS), IEEE. pp. 23--30.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tobin et al. [2017b] Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W.,
    Abbeel, P., 2017b年。从模拟到现实世界的深度神经网络领域随机化，见：2017 IEEE/RSJ 国际智能机器人与系统会议（IROS），IEEE。第23--30页。
- en: Toft et al. [2020] Toft, C., Maddern, W., Torii, A., Hammarstrand, L., Stenborg,
    E., Safari, D., Okutomi, M., Pollefeys, M., Sivic, J., Pajdla, T., et al., 2020.
    Long-term visual localization revisited. IEEE Transactions on Pattern Analysis
    and Machine Intelligence .
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toft et al. [2020] Toft, C., Maddern, W., Torii, A., Hammarstrand, L., Stenborg,
    E., Safari, D., Okutomi, M., Pollefeys, M., Sivic, J., Pajdla, T., 等，2020年。长期视觉定位重访。IEEE
    模式分析与机器智能学报。
- en: 'Voulodimos et al. [2018] Voulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis,
    E., 2018. Deep learning for computer vision: A brief review. Computational intelligence
    and neuroscience 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voulodimos et al. [2018] Voulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis,
    E., 2018年。计算机视觉中的深度学习：简要综述。计算智能与神经科学 2018。
- en: Wang and Yeung [2020] Wang, H., Yeung, D.Y., 2020. A survey on bayesian deep
    learning. ACM computing surveys (csur) 53, 1--37.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Yeung [2020] Wang, H., Yeung, D.Y., 2020年。贝叶斯深度学习综述。ACM 计算调查（csur）
    53, 1--37。
- en: 'Wang et al. [2022a] Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W.,
    Chen, Y., Zeng, W., Yu, P., 2022a. Generalizing to unseen domains: A survey on
    domain generalization. IEEE Transactions on Knowledge and Data Engineering .'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022a] Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W.,
    Chen, Y., Zeng, W., Yu, P., 2022a年。泛化到未见领域：领域泛化的综述。IEEE 知识与数据工程学报。
- en: Wang et al. [2020] Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao,
    Y., Liu, D., Mu, Y., Tan, M., Wang, X., et al., 2020. Deep high-resolution representation
    learning for visual recognition. IEEE transactions on pattern analysis and machine
    intelligence 43, 3349--3364.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao,
    Y., Liu, D., Mu, Y., Tan, M., Wang, X., 等，2020年。深度高分辨率表征学习用于视觉识别。IEEE 计算机视觉与模式识别学报
    43, 3349--3364。
- en: 'Wang and Deng [2018] Wang, M., Deng, W., 2018. Deep visual domain adaptation:
    A survey. Neurocomputing 312, 135--153.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Deng [2018] Wang, M., Deng, W., 2018年。深度视觉领域适应：综述。神经计算 312, 135--153。
- en: Wang et al. [2022b] Wang, Q., Ma, Y., Zhao, K., Tian, Y., 2022b. A comprehensive
    survey of loss functions in machine learning. Annals of Data Science 9, 187--212.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022b] Wang, Q., Ma, Y., Zhao, K., Tian, Y., 2022b年。机器学习中的损失函数综合调查。数据科学年鉴
    9, 187--212。
- en: 'Wang et al. [2022] Wang, S., Wang, S., Jiao, B., Yang, D., Su, L., Zhai, P.,
    Chen, C., Zhang, L., 2022. CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation
    in Space. arXiv e-prints , arXiv:2207.07869[arXiv:2207.07869](http://arxiv.org/abs/2207.07869).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022] Wang, S., Wang, S., Jiao, B., Yang, D., Su, L., Zhai, P.,
    Chen, C., Zhang, L., 2022年。CA-SpaceNet：空间中6D姿态估计的反事实分析。arXiv 电子预印本，arXiv:2207.07869[arXiv:2207.07869](http://arxiv.org/abs/2207.07869)。
- en: 'Wang et al. [2019] Wang, W., Yang, Y., Wang, X., Wang, W., Li, J., 2019. Development
    of convolutional neural network and its application in image classification: a
    survey. Optical Engineering 58, 040901--040901.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2019] Wang, W., Yang, Y., Wang, X., Wang, W., Li, J., 2019年。卷积神经网络的发展及其在图像分类中的应用：综述。光学工程
    58, 040901--040901。
- en: 'Wang et al. [2022] Wang, Y., Shen, X., Hu, S.X., Yuan, Y., Crowley, J.L., Vaufreydaz,
    D., 2022. Self-supervised transformers for unsupervised object discovery using
    normalized cut, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 14543--14553.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] Wang, Y., Shen, X., Hu, S.X., Yuan, Y., Crowley, J.L., Vaufreydaz,
    D., 2022. 自监督变换器用于利用归一化切割进行无监督目标发现，见：IEEE/CVF 计算机视觉与模式识别会议论文集，第 14543--14553 页。
- en: 'Weiler and Cesa [2019] Weiler, M., Cesa, G., 2019. General e(2)-equivariant
    steerable cnns, in: Wallach, H., Larochelle, H., Beygelzimer, A., d''Alché-Buc,
    F., Fox, E., Garnett, R. (Eds.), Advances in Neural Information Processing Systems.
    URL: [https://proceedings.neurips.cc/paper_files/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weiler 和 Cesa [2019] Weiler, M., Cesa, G., 2019. 一般 e(2)-等变可调卷积神经网络，见：Wallach,
    H., Larochelle, H., Beygelzimer, A., d''Alché-Buc, F., Fox, E., Garnett, R. (编)，《神经信息处理系统进展》。URL:
    [https://proceedings.neurips.cc/paper_files/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf)。'
- en: 'Wijayatunga et al. [2023] Wijayatunga, M.C., Armellin, R., Holt, H., Pirovano,
    L., Lidtke, A.A., 2023. Design and guidance of a multi-active debris removal mission.
    Astrodynamics URL: [https://link.springer.com/10.1007/s42064-023-0159-3](https://link.springer.com/10.1007/s42064-023-0159-3),
    doi:[10.1007/s42064-023-0159-3](https:/doi.org/10.1007/s42064-023-0159-3).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wijayatunga 等人 [2023] Wijayatunga, M.C., Armellin, R., Holt, H., Pirovano,
    L., Lidtke, A.A., 2023. 多活跃碎片移除任务的设计与指导。航天动力学 URL: [https://link.springer.com/10.1007/s42064-023-0159-3](https://link.springer.com/10.1007/s42064-023-0159-3),
    doi:[10.1007/s42064-023-0159-3](https:/doi.org/10.1007/s42064-023-0159-3)。'
- en: Wistuba et al. [2019] Wistuba, M., Rawat, A., Pedapati, T., 2019. A survey on
    neural architecture search. arXiv preprint arXiv:1905.01392 .
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wistuba 等人 [2019] Wistuba, M., Rawat, A., Pedapati, T., 2019. 神经网络架构搜索的综述。arXiv
    预印本 arXiv:1905.01392。
- en: 'Witze [2023] Witze, A., 2023. 2022 was a record year for space launches. URL:
    [https://www.nature.com/articles/d41586-023-00048-7](https://www.nature.com/articles/d41586-023-00048-7).'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Witze [2023] Witze, A., 2023. 2022 年是太空发射的创纪录之年。URL: [https://www.nature.com/articles/d41586-023-00048-7](https://www.nature.com/articles/d41586-023-00048-7)。'
- en: 'Xiang et al. [2018] Xiang, Y., Schmidt, T., Narayanan, V., Fox, D., 2018. Posecnn:
    A convolutional neural network for 6d object pose estimation in cluttered scenes,
    in: Kress-Gazit, H., Srinivasa, S.S., Howard, T., Atanasov, N. (Eds.), Robotics:
    Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania,
    USA, June 26-30, 2018. URL: [http://www.roboticsproceedings.org/rss14/p19.html](http://www.roboticsproceedings.org/rss14/p19.html),
    doi:[10.15607/RSS.2018.XIV.019](https:/doi.org/10.15607/RSS.2018.XIV.019).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiang 等人 [2018] Xiang, Y., Schmidt, T., Narayanan, V., Fox, D., 2018. Posecnn：一种用于在混杂场景中进行
    6D 目标姿态估计的卷积神经网络，见：Kress-Gazit, H., Srinivasa, S.S., Howard, T., Atanasov, N.
    (编)，《机器人学：科学与系统 XIV》，卡内基梅隆大学，美国宾夕法尼亚州匹兹堡，2018年6月26-30日。URL: [http://www.roboticsproceedings.org/rss14/p19.html](http://www.roboticsproceedings.org/rss14/p19.html),
    doi:[10.15607/RSS.2018.XIV.019](https:/doi.org/10.15607/RSS.2018.XIV.019)。'
- en: 'Xilinx [2022] Xilinx, 2022. Product guide: Dpuczdx8g for zynq ultrascale+ mpsocs.
    URL: [https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/dpu/v4_0/pg338-dpu.pdf](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/dpu/v4_0/pg338-dpu.pdf).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xilinx [2022] Xilinx, 2022. 产品指南：用于 Zynq Ultrascale+ MPSOC 的 Dpuczdx8g。URL:
    [https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/dpu/v4_0/pg338-dpu.pdf](https://www.xilinx.com/content/dam/xilinx/support/documents/ip_documentation/dpu/v4_0/pg338-dpu.pdf)。'
- en: Xilinx [15 Jun 2022] Xilinx, A., 15 Jun 2022. Vitis ai user guide. [https://www.xilinx.com/content/dam/xilinx/support/documents/sw_manuals/vitis_ai/2_5/ug1414-vitis-ai.pdf](https://www.xilinx.com/content/dam/xilinx/support/documents/sw_manuals/vitis_ai/2_5/ug1414-vitis-ai.pdf).
    [Online; accessed 30-Jan-2023].
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xilinx [2022年6月15日] Xilinx, A., 2022年6月15日. Vitis AI 用户指南。 [https://www.xilinx.com/content/dam/xilinx/support/documents/sw_manuals/vitis_ai/2_5/ug1414-vitis-ai.pdf](https://www.xilinx.com/content/dam/xilinx/support/documents/sw_manuals/vitis_ai/2_5/ug1414-vitis-ai.pdf)。[在线;
    访问日期：2023年1月30日]。
- en: 'Xiong et al. [2021] Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang,
    Y., Kindermans, P.J., Tan, M., Singh, V., Chen, B., 2021. Mobiledets: Searching
    for object detection architectures for mobile accelerators, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3825--3834.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人 [2021] Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang, Y.,
    Kindermans, P.J., Tan, M., Singh, V., Chen, B., 2021. Mobiledets：为移动加速器搜索目标检测架构，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 3825--3834 页。
- en: 'Xu et al. [2019] Xu, F., Uszkoreit, H., Du, Y., Fan, W., Zhao, D., Zhu, J.,
    2019. Explainable ai: A brief survey on history, research areas, approaches and
    challenges, in: Natural Language Processing and Chinese Computing: 8th CCF International
    Conference, NLPCC 2019, Dunhuang, China, October 9--14, 2019, Proceedings, Part
    II 8, Springer. pp. 563--574.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人 [2019] 徐飞、乌斯科雷特、杜宇、范伟、赵东、朱军，2019。可解释的人工智能：关于历史、研究领域、方法和挑战的简要调查，见：自然语言处理与中文计算：第八届中国计算机学会国际会议，NLPCC
    2019，敦煌，中国，2019年10月9日至14日，会议录，第II部分8，施普林格。第563--574页。
- en: Yuheng and Hao [2017] Yuheng, S., Hao, Y., 2017. Image segmentation algorithms
    overview. arXiv preprint arXiv:1707.02051 .
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余恒和郝耀 [2017] 余恒、郝耀，2017。图像分割算法概述。arXiv预印本 arXiv:1707.02051。
- en: Zaidi et al. [2022] Zaidi, S.S.A., Ansari, M.S., Aslam, A., Kanwal, N., Asghar,
    M., Lee, B., 2022. A survey of modern deep learning based object detection models.
    Digital Signal Processing , 103514.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎伊迪等人 [2022] 扎伊迪、S.S.A.、安萨里、M.S.、阿斯拉姆、A.、坎瓦尔、N.、阿斯ghar、M.、李博，2022。现代深度学习基础的目标检测模型综述。数字信号处理，103514。
- en: 'Zou et al. [2019] Zou, Z., Shi, Z., Guo, Y., Ye, J., 2019. Object detection
    in 20 years: A survey. arXiv preprint arXiv:1905.05055 .'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邹等人 [2019] 邹忠、石哲、郭燕、叶静，2019。20年来的目标检测：综述。arXiv预印本 arXiv:1905.05055。
