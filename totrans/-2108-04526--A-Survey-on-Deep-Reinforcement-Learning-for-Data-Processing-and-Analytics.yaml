- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:52:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:52:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.04526] A Survey on Deep Reinforcement Learning for Data Processing and
    Analytics'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.04526] 关于深度强化学习在数据处理和分析中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04526](https://ar5iv.labs.arxiv.org/html/2108.04526)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04526](https://ar5iv.labs.arxiv.org/html/2108.04526)
- en: A Survey on Deep Reinforcement Learning for Data Processing and Analytics
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度强化学习在数据处理和分析中的调查
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Data processing and analytics are fundamental and pervasive. Algorithms play
    a vital role in data processing and analytics where many algorithm designs have
    incorporated heuristics and general rules from human knowledge and experience
    to improve their effectiveness. Recently, reinforcement learning, deep reinforcement
    learning (DRL) in particular, is increasingly explored and exploited in many areas
    because it can learn better strategies in complicated environments it is interacting
    with than statically designed algorithms. Motivated by this trend, we provide
    a comprehensive review of recent works focusing on utilizing DRL to improve data
    processing and analytics. First, we present an introduction to key concepts, theories,
    and methods in DRL. Next, we discuss DRL deployment on database systems, facilitating
    data processing and analytics in various aspects, including data organization,
    scheduling, tuning, and indexing. Then, we survey the application of DRL in data
    processing and analytics, ranging from data preparation, natural language processing
    to healthcare, fintech, etc. Finally, we discuss important open challenges and
    future research directions of using DRL in data processing and analytics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理和分析是基础且普遍的。算法在数据处理和分析中扮演着重要角色，许多算法设计已融入了人类知识和经验中的启发式和通用规则，以提高其效果。近年来，强化学习，特别是深度强化学习（DRL），在许多领域得到越来越多的探索和利用，因为它能在与复杂环境互动中学习比静态设计算法更好的策略。受到这一趋势的激励，我们提供了一项关于利用DRL改善数据处理和分析的最新工作的全面综述。首先，我们介绍了DRL中的关键概念、理论和方法。接下来，我们讨论了DRL在数据库系统上的部署，促进数据处理和分析的各个方面，包括数据组织、调度、调优和索引。然后，我们调查了DRL在数据处理和分析中的应用，从数据准备、自然语言处理到医疗保健、金融科技等。最后，我们讨论了使用DRL进行数据处理和分析的重要开放挑战及未来研究方向。
- en: A Survey on Deep Reinforcement Learning for Data Processing and Analytics
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度强化学习在数据处理和分析中的调查
- en: Qingpeng Cai^(∗†), Can Cui^(∗†), Yiyuan Xiong^(∗†)^†^†^∗These authors have contributed
    equally to this work, and M. Zhang is the contact author., Wei Wang^†,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Qingpeng Cai^(∗†), Can Cui^(∗†), Yiyuan Xiong^(∗†)^†^†^∗这些作者对本研究贡献相同，M. Zhang
    是通讯作者。, Wei Wang^†,
- en: Zhongle Xie^§, Meihui Zhang^‡
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Zhongle Xie^§, Meihui Zhang^‡
- en: ^†National University of Singapore    ^§ Zhejiang University
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^†新加坡国立大学    ^§ 浙江大学
- en: ^‡Beijing Institute of Techonology
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^‡北京理工大学
- en: '{qingpeng, cuican, yiyuan, wangwei}@comp.nus.edu.sg   xiezl@zju.edu.cn   meihui_zhang@bit.edu.cn'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '{qingpeng, cuican, yiyuan, wangwei}@comp.nus.edu.sg   xiezl@zju.edu.cn   meihui_zhang@bit.edu.cn'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the age of big data, data processing and analytics are fundamental, ubiquitous,
    and crucial to many organizations which undertake a digitalization journey to
    improve and transform their businesses and operations. Data analytics typically
    entails other key operations such as data acquisition, data cleansing, data integration,
    modeling, etc., before insights could be extracted. Big data can unleash significant
    value creation across many sectors such as healthcare and retail [[56](#bib.bib56)].
    However, the complexity of data (e.g., high volume, high velocity, and high variety)
    presents many challenges in data analytics and hence renders the difficulty in
    drawing meaningful insights. To tackle the challenge and facilitate the data processing
    and analytics efficiently and effectively, a large number of algorithms and techniques
    have been designed and numerous learning systems have also been developed by researchers
    and practitioners such as Spark MLlib [[63](#bib.bib63)], and Rafiki [[106](#bib.bib106)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据时代，数据处理和分析是基础的、普遍存在的，并且对许多正在进行数字化转型以改善和改造业务及运营的组织至关重要。数据分析通常包括数据获取、数据清洗、数据集成、建模等其他关键操作，之后才能提取见解。大数据可以在许多领域（如医疗保健和零售）释放出显著的价值[[56](#bib.bib56)]。然而，数据的复杂性（例如高容量、高速度和高多样性）在数据分析中提出了许多挑战，因此提取有意义的见解变得困难。为了应对挑战并高效、有效地促进数据处理和分析，研究人员和从业者设计了大量的算法和技术，并开发了许多学习系统，如Spark
    MLlib[[63](#bib.bib63)]和Rafiki[[106](#bib.bib106)]。
- en: To support fast data processing and accurate data analytics, a huge number of
    algorithms rely on rules that are developed based on human knowledge and experience.
    For example, shortest-job-first is a scheduling algorithm that chooses the job
    with the smallest execution time for the next execution. However, without fully
    exploiting characteristics of the workload, it can achieve inferior performance
    compared to a learning-based scheduling algorithm [[58](#bib.bib58)]. Another
    example is packet classification in computer networking which matches a packet
    to a rule from a set of rules. One solution is to construct the decision tree
    using hand-tuned heuristics for classification. Specifically, the heuristics are
    designed for a particular set of rules and thus may not work well for other workloads
    with different characteristics [[47](#bib.bib47)]. We observe three limitations
    of existing algorithms [[97](#bib.bib97), [46](#bib.bib46)]. First, the algorithms
    are suboptimal. Useful information such as data distribution could be overlooked
    or not fully exploited by the rules. Second, the algorithm lacks adaptivity. Algorithms
    designed for a specific workload cannot perform well in another different workload.
    Third, the algorithm design is a time-consuming process. Developers have to spend
    much time trying a lot of rules to find one that empirically works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持快速数据处理和准确的数据分析，大量算法依赖于基于人类知识和经验开发的规则。例如，最短作业优先是一种调度算法，它选择下一个执行时间最短的作业。然而，如果没有充分利用负载的特征，它的性能可能会比基于学习的调度算法差[[58](#bib.bib58)]。另一个例子是计算机网络中的数据包分类，它将数据包匹配到规则集合中的规则之一。一种解决方案是使用手动调整的启发式方法构建决策树进行分类。具体而言，这些启发式方法是为特定规则集合设计的，因此可能不适用于具有不同特征的其他负载[[47](#bib.bib47)]。我们观察到现有算法的三个局限性[[97](#bib.bib97),
    [46](#bib.bib46)]。首先，算法是次优的。像数据分布这样的有用信息可能被忽略或未被充分利用。其次，算法缺乏适应性。为特定负载设计的算法在其他不同负载中可能表现不好。第三，算法设计是一个耗时的过程。开发人员必须花费大量时间尝试许多规则以找到一个经验上有效的规则。
- en: 'Learning-based algorithms have also been studied for data processing and analytics.
    Two types of learning methods are often used: supervised learning and reinforcement
    learning. They achieve better performance by direct optimization of the performance
    objective. Supervised learning typically requires a rich set of high-quality labeled
    training data, which could be hard and challenging to acquire. For example, configuration
    tuning is important to optimize the overall performance of a database management
    system (DBMS)[[44](#bib.bib44)]. There could be hundreds of tuning knobs that
    are correlated in discrete and continuous space. Furthermore, diverse database
    instances, query workloads, hardware characteristics render data collection infeasible,
    especially in the cloud environment. Compared to supervised learning, reinforcement
    learning shows good performance because it adopts a trial-and-error search and
    requires fewer training samples to find good configuration for cloud databases [[123](#bib.bib123)].
    Another specific example would be query optimization in query processing. Database
    system optimizers are tasked to find the best execution plan for a query to reduce
    its query cost. Traditional optimizers typically enumerate many candidate plans
    and use a cost model to find the plan with minimal cost. The optimization process
    could be slow and inaccurate [[42](#bib.bib42)]. Without relying on an inaccurate
    cost model, deep reinforcement learning (DRL) methods improve the execution plan
    (e.g., changing the table join orders) by interacting with the database[[61](#bib.bib61),
    [37](#bib.bib37)]. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on
    Deep Reinforcement Learning for Data Processing and Analytics") provides a typical
    workflow for query optimization using DRL. When the query is sent to the agent
    (i.e., DRL optimizer), it produces a state vector via conducting featurization
    on essential information, such as the accessed relations and tables. Taking the
    state as the input, the agent employs neural networks to produce the probability
    distribution of an action set, where the action set could contain all possible
    join operations as potential actions. Each action denotes a partial join plan
    on a pair of tables, and the state will be updated once an action is taken. After
    taking possible actions, a complete plan is generated, which is then executed
    by a DBMS to get the reward. In this query optimization problem, the reward can
    be calculated by the real latency. During the training process with the reward
    signal, the agent can improve the policy and produce a better join ordering with
    a higher reward (i.e., less latency).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的算法也已被用于数据处理和分析。通常使用两种学习方法：**监督学习**和**强化学习**。它们通过直接优化性能目标来实现更好的性能。**监督学习**通常需要大量高质量的标注训练数据，这可能很难获取。例如，配置调优对于优化数据库管理系统（DBMS）的整体性能非常重要[[44](#bib.bib44)]。可能有数百个调优参数，这些参数在离散和连续空间中是相关的。此外，数据库实例、多样的查询工作负载、硬件特性使得数据收集变得不可行，尤其是在云环境中。与**监督学习**相比，**强化学习**表现出良好的性能，因为它采用了试错搜索，并且需要较少的训练样本来找到云数据库的良好配置[[123](#bib.bib123)]。另一个具体的例子是查询处理中的查询优化。数据库系统优化器的任务是找到查询的最佳执行计划，以减少查询成本。传统的优化器通常会枚举许多候选计划，并使用成本模型来找到成本最低的计划。优化过程可能会很慢且不准确[[42](#bib.bib42)]。在不依赖于不准确的成本模型的情况下，**深度强化学习（DRL）**方法通过与数据库交互来改进执行计划（例如，改变表的连接顺序）[[61](#bib.bib61),
    [37](#bib.bib37)]。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics")展示了使用**DRL**进行查询优化的典型工作流程。当查询被发送到代理（即**DRL**优化器）时，它通过对基本信息（如访问的关系和表）进行特征化生成状态向量。以状态作为输入，代理使用神经网络生成动作集合的概率分布，其中动作集合可能包含所有可能的连接操作作为潜在动作。每个动作表示对一对表的部分连接计划，并且一旦采取了动作，状态将被更新。在采取可能的动作后，将生成一个完整的计划，然后由**DBMS**执行以获得奖励。在这个查询优化问题中，奖励可以通过实际延迟计算。在训练过程中，通过奖励信号，代理可以改进策略，并生成具有更高奖励（即更少延迟）的更好连接顺序。
- en: '![Refer to caption](img/90259969a325225ecd657f75199786c3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/90259969a325225ecd657f75199786c3.png)'
- en: 'Figure 1: The Workflow of DRL for Query Optimization. A, B, C and D are four
    tables.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：**DRL**进行查询优化的工作流程。A、B、C和D是四个表。
- en: 'Reinforcement learning (RL) [[89](#bib.bib89)] focuses on learning to make
    intelligent actions in an environment. The RL algorithm works on the basis of
    exploration and exploitation to improve itself with feedback from the environment.
    In the past decades, RL has achieved tremendous improvements in both theoretical
    and technical aspects [[86](#bib.bib86), [89](#bib.bib89)]. Notably, DRL incorporates
    deep learning (DL) techniques to handle complex unstructured data and has been
    designed to learn from historical data and self-exploration to solve notoriously
    hard and large-scale problems (e.g., AlphaGo[[85](#bib.bib85)]). In recent years,
    researchers from different communities have proposed DRL solutions to address
    issues in data processing and analytics[[116](#bib.bib116), [58](#bib.bib58),
    [52](#bib.bib52)]. We categorize existing works using DRL from two perspectives:
    system and application. From the system’s perspective, we focus on fundamental
    research topics ranging from general ones, such as scheduling, to system-specific
    ones, such as query optimization in databases. We shall also emphasize how it
    is formulated in the Markov Decision Process and discuss how the problem can be
    solved by DRL more effectively compared to traditional methods. Many techniques
    such as sampling and simulation are adopted to improve DRL training efficiency
    because workload execution and data collection in the real system could be time-consuming [[31](#bib.bib31)].
    From the application’s perspective, we shall cover various key applications in
    both data processing and data analytics to provide a comprehensive understanding
    of the DRL’s usability and adaptivity. Many domains are transformed by the adoption
    of DRL, which helps to learn domain-specific knowledge about the applications.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）[[89](#bib.bib89)] 侧重于学习如何在环境中做出智能决策。RL 算法基于探索和利用的原则，通过环境反馈来提升自身。在过去几十年中，RL
    在理论和技术方面都取得了巨大的进展[[86](#bib.bib86), [89](#bib.bib89)]。特别地，深度强化学习（DRL）结合了深度学习（DL）技术，以处理复杂的非结构化数据，并被设计用来从历史数据和自我探索中学习，以解决那些著名的难题和大规模问题（例如，AlphaGo[[85](#bib.bib85)]）。近年来，来自不同领域的研究人员提出了DRL解决方案，以应对数据处理和分析中的问题[[116](#bib.bib116),
    [58](#bib.bib58), [52](#bib.bib52)]。我们从两个角度对现有的DRL研究进行分类：系统和应用。从系统的角度来看，我们关注从一般性的研究主题（如调度）到系统特定的研究主题（如数据库中的查询优化）等基础研究课题。我们还将重点讨论其在马尔可夫决策过程中的表述方式，并讨论DRL如何比传统方法更有效地解决问题。许多技术（如采样和仿真）被采用以提高DRL训练效率，因为在真实系统中执行工作负载和收集数据可能是非常耗时的[[31](#bib.bib31)]。从应用的角度来看，我们将涵盖数据处理和数据分析中的各种关键应用，以提供对DRL的可用性和适应性的全面理解。许多领域通过采用DRL而发生了转变，这有助于学习关于应用程序的特定领域知识。
- en: In this survey, we aim at providing a broad and systematic review of recent
    advancements in employing DRL in solving data systems, data processing and analytics
    issues. In Section 2, we introduce the key concepts, theories, and techniques
    in RL to lay the foundations. To gain a deeper understanding of DRL, readers could
    refer to the recently published book [[13](#bib.bib13)], which covers selected
    DRL research topics and applications with detailed illustrations. In Section 3,
    we review the latest important research works on using DRL for system optimization
    to support data processing and analytics. We cover fundamental topics such as
    data organization, scheduling, system tuning, index, query optimization, and cache
    management. In Section 4, we discuss using DRL for applications in data processing
    and analytics ranging from data preparation, natural language interaction to various
    real-world applications such as healthcare, fintech, E-commerce, etc. In Section
    5, we highlight various open challenges and potential research problems. We conclude
    in Section 6\. This survey focuses on recent advancements in exploring RL for
    data processing and analytics that spurs great interest, especially in the database
    and data mining community. There are survey papers discussing DRL for other domains.
    We refer readers to the survey of DRL for healthcare in [[118](#bib.bib118)],
    communications and networking in [[54](#bib.bib54)], and RL explainability in
    [[76](#bib.bib76)]. Another work[[107](#bib.bib107)] discusses how deep learning
    can be used to optimize database system design, and vice versa. In this paper,
    we use "DRL" and "RL" interchangeably.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在提供关于在数据系统、数据处理和分析中应用深度强化学习（DRL）的最新进展的广泛和系统的综述。在第2节中，我们介绍了强化学习中的关键概念、理论和技术，为基础奠定基础。为了更深入地了解DRL，读者可以参考最近出版的书籍
    [[13](#bib.bib13)]，该书详细阐述了选定的DRL研究主题和应用。在第3节中，我们回顾了使用DRL进行系统优化以支持数据处理和分析的最新重要研究成果。我们涵盖了基本主题，如数据组织、调度、系统调优、索引、查询优化和缓存管理。在第4节中，我们讨论了DRL在数据处理和分析中的应用，包括数据准备、自然语言交互以及各种现实世界应用，如医疗保健、金融科技、电子商务等。在第5节中，我们突出了各种开放挑战和潜在的研究问题。我们在第6节中总结。本文调查关注于探索RL在数据处理和分析中的最新进展，特别是在数据库和数据挖掘社区中引发了极大兴趣。还有讨论DRL在其他领域的调查论文。我们建议读者参考
    [[118](#bib.bib118)] 关于医疗保健的DRL调查，[[54](#bib.bib54)] 关于通信和网络的DRL调查，以及 [[76](#bib.bib76)]
    关于RL解释性的调查。另一项工作[[107](#bib.bib107)] 讨论了深度学习如何用于优化数据库系统设计，反之亦然。本文中，我们将“DRL”和“RL”互换使用。
- en: 2 Theoretical Foundation and Algorithms of Reinforcement Learning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 强化学习的理论基础和算法
- en: RL is targeted to solve the sequential decision making problem and the goal
    is to take actions with maximum expected rewards. In detail, the agent follows
    a policy to make a series of decisions (i.e. taking actions) in different states
    of the environment, and the sequence of the states and the actions form a trajectory.
    To estimate whether the policy is good or not, each decision under the policy
    will be evaluated by the accumulated rewards through the trajectory. After evaluating
    the policy from the trajectories, the agent next improves the policy by increasing
    the probabilities of making decisions with greater expected rewards. By repeating
    these steps, the agent can improve the policy through trial-and-error until the
    policy reaches the optimal, and such a sequential decision-making process is modeled
    via Markov Decision Process (MDP).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的目标是解决序列决策问题，其目的是采取最大期望奖励的行动。具体来说，智能体遵循一个策略在环境的不同状态下做出一系列决策（即采取行动），这些状态和行动的序列形成一个轨迹。为了评估策略的好坏，策略下的每个决策将通过轨迹中的累积奖励进行评估。评估策略后，智能体会通过提高做出高期望奖励决策的概率来改进策略。通过重复这些步骤，智能体可以通过试错法改进策略，直到策略达到最优，这样的序列决策过程通过马尔可夫决策过程（MDP）进行建模。
- en: 2.1 Markov Decision Process
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 马尔可夫决策过程
- en: Mathematically, MDP, shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    A Survey on Deep Reinforcement Learning for Data Processing and Analytics"), is
    a stochastic control process $\mathcal{M}$ defined by a tuple with 5 elements,
    $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma)$, which
    are explained as follows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，MDP，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") 所示，是一个由五个元素组成的随机控制过程 $\mathcal{M}$，定义为一个五元组
    $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma)$，其含义如下。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'State $\mathcal{S}$: $\mathcal{S}$ is the space for states that denote different
    situations in the environment and $s_{t}\in\mathcal{S}$ denotes the state of the
    situation at the time $t$.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '状态 $\mathcal{S}$: $\mathcal{S}$ 是表示环境中不同情况的状态空间，$s_{t}\in\mathcal{S}$ 表示时间
    $t$ 时的状态。'
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Action $\mathcal{A}$: $\mathcal{A}$ is the space for actions that the agent
    can take; the actions can either be discrete or continuous, and $a_{t}\in\mathcal{A}$
    denotes the action taken at the time $t$.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '行动 $\mathcal{A}$: $\mathcal{A}$ 是智能体可以采取的行动空间；这些行动可以是离散的或连续的，$a_{t}\in\mathcal{A}$
    表示在时间 $t$ 采取的行动。'
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reward function $\mathcal{R}(s_{t},a_{t})$: It denotes the immediate reward
    of the action $a_{t}$ taken under the state $s_{t}$.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '奖励函数 $\mathcal{R}(s_{t},a_{t})$: 它表示在状态 $s_{t}$ 下采取行动 $a_{t}$ 的即时奖励。'
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transition function $\mathcal{P}(s_{t+1}=s^{\prime}|s_{t}=s,a_{t}=a)$: It denotes
    the probability of transition to the state $s^{\prime}$ at the time $t+1$ given
    the current state $s$ and the taken action $a$ at the time $t$.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '转移函数 $\mathcal{P}(s_{t+1}=s^{\prime}|s_{t}=s,a_{t}=a)$: 它表示在时间 $t+1$ 转移到状态
    $s^{\prime}$ 的概率，给定当前状态 $s$ 和在时间 $t$ 采取的行动 $a$。'
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Discount factor $\gamma\in[0,1]$: The total rewards of a certain action consist
    of both immediate rewards and future rewards, and the $\gamma$ quantifies how
    much importance we give for future rewards.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '折扣因子 $\gamma\in[0,1]$: 某个行动的总奖励包括即时奖励和未来奖励，而 $\gamma$ 量化了我们对未来奖励的重要性。'
- en: We take the query optimization problem demonstrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Reinforcement Learning for Data Processing
    and Analytics") to help explain the five components of the MDP. In this example,
    the state is expressed as a state vector, which summarizes the information of
    relations and tables that are assessed by the query $q$. In each state, the RL
    agent produces a probability distribution over all potential actions where each
    action denotes a partial join plan on a pair of tables. After repeating these
    two processes, it reaches a terminal state where the final join ordering is generated
    for an agent to execute, and all actions’ target rewards are measured by the actual
    performance (i.e., latency) or a cost model. As for the transition function, the
    transitions of the states are always deterministic in both this problem and most
    of the other DB problems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用图示[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics)中展示的查询优化问题来帮助解释 MDP 的五个组成部分。在这个例子中，状态表示为状态向量，它总结了查询
    $q$ 评估的关系和表的信息。在每个状态下，强化学习智能体生成一个概率分布，覆盖所有潜在的行动，其中每个行动表示一对表的部分连接计划。在重复这两个过程后，它达到一个终态，生成最终的连接顺序供智能体执行，并且所有行动的目标奖励通过实际性能（即延迟）或成本模型来衡量。至于转移函数，这个问题和大多数其他数据库问题中的状态转移都是确定性的。
- en: 'In RL, we aim to train the agent with a good policy $\pi$ that is a mapping
    function from state to action. Through the policy, the agent can take a series
    of actions that will result in continuous changes in the states, and the sequence
    of the states and the actions following the policy $\pi$ form a trajectory $\tau=(s_{0},a_{0},s_{1},a_{1},...)$.
    From each $\tau$, we can evaluate the effect of each action by the accumulated
    rewards $\mathcal{G}$, and it consists of the immediate reward of this action
    and the discounted rewards of its following actions in the trajectory. The total
    result $\mathcal{G}$ for the action $a_{t}$ is as follows: $\mathcal{G}(\tau)=\sum_{t=0}\gamma^{t}r_{t}$,
    where $\gamma$ quantifies how much importance we give for future rewards. With
    a bigger $\gamma$, the RL agent will be more likely to take any action that may
    have a less immediate reward at the current time but has a greater future reward
    in expectation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们的目标是训练一个好的策略 $\pi$，它是从状态到行动的映射函数。通过这个策略，智能体可以采取一系列行动，这将导致状态的持续变化，并且遵循策略
    $\pi$ 的状态和行动序列形成一个轨迹 $\tau=(s_{0},a_{0},s_{1},a_{1},...)$。从每个 $\tau$ 中，我们可以通过累积奖励
    $\mathcal{G}$ 来评估每个行动的效果，它包括此行动的即时奖励和轨迹中后续行动的折扣奖励。行动 $a_{t}$ 的总结果 $\mathcal{G}$
    如下：$\mathcal{G}(\tau)=\sum_{t=0}\gamma^{t}r_{t}$，其中 $\gamma$ 量化了我们对未来奖励的重要性。更大的
    $\gamma$ 会使得强化学习智能体更可能采取任何当前可能即时奖励较少但期望未来奖励更大的行动。
- en: 'RL continuously evaluates the policy $\pi$ and improves it until it reaches
    the optimal policy $\pi^{*}=\arg\max_{(\tau\sim\pi)}\mathcal{G}(\tau)$ where the
    agent always takes actions that maximize the expected return. To evaluate the
    policy $\pi$, RL algorithms estimate how good or bad it is for a state and a state-action
    pair by the function $\mathcal{V}$ and function $\mathcal{Q}$ respectively. Both
    of these two value functions are calculated according to the discounted return
    $\mathcal{G}$ in expectation which can be written as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）持续评估策略$\pi$并改进它，直到达到最优策略$\pi^{*}=\arg\max_{(\tau\sim\pi)}\mathcal{G}(\tau)$，在这种情况下，智能体总是采取最大化预期回报的行动。为了评估策略$\pi$，RL算法通过函数$\mathcal{V}$和函数$\mathcal{Q}$分别估计状态和状态-动作对的好坏。这两个价值函数都是根据折扣回报$\mathcal{G}$的期望值计算的，可以写成：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s]$
    |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s]$
    |  | (1) |'
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s,a_{0}=a]$
    |  | (2) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s,a_{0}=a]$
    |  | (2) |'
- en: 'These two value functions have a close association where the $\mathcal{V}^{\pi}(s_{t})$
    is the expectation of the function $\mathcal{Q}$ of all possible actions under
    the state $s_{t}$ according to the policy $\pi$, and the $\mathcal{Q}^{\pi}(s_{t},a_{t})$
    is the combination of the immediate reward of the action $a_{t}$ and the expectation
    of all possible states’ values after taking the action $a_{t}$ under the state
    $s_{t}$. Hence, we have:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个价值函数之间有密切的关联，其中$\mathcal{V}^{\pi}(s_{t})$是根据策略$\pi$在状态$s_{t}$下所有可能动作的函数$\mathcal{Q}$的期望值，而$\mathcal{Q}^{\pi}(s_{t},a_{t})$是动作$a_{t}$的即时奖励和在状态$s_{t}$下采取动作$a_{t}$后所有可能状态值的期望值的组合。因此，我们有：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=$ | $\displaystyle\sum_{a\in\mathcal{A}}\pi(a&#124;s)\mathcal{Q}^{\pi}(s,a)$
    |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=$ | $\displaystyle\sum_{a\in\mathcal{A}}\pi(a&#124;s)\mathcal{Q}^{\pi}(s,a)$
    |  | (3) |'
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)$ | $\displaystyle=$ | $\displaystyle
    R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})$
    |  | (4) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)$ | $\displaystyle=$ | $\displaystyle
    R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})$
    |  | (4) |'
- en: 'Given a policy $\pi$, we can evaluate its value functions by Bellman equations [[89](#bib.bib89)]
    which utilize the recursive relationships of these value functions. Formally,
    Bellman equations deduce the relationships between a given state (i.e. function
    $\mathcal{V}$) or a given state-action pair (i.e. function $\mathcal{Q}$) and
    its successors which can be written as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个策略$\pi$，我们可以通过贝尔曼方程[[89](#bib.bib89)]来评估其价值函数，这些方程利用了这些价值函数的递归关系。正式地，贝尔曼方程推导出给定状态（即函数$\mathcal{V}$）或给定状态-动作对（即函数$\mathcal{Q}$）与其后继状态之间的关系，可以写成：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=\sum_{a_{t}\in\mathcal{A}}\pi(a&#124;s)[R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})]$
    |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=\sum_{a_{t}\in\mathcal{A}}\pi(a&#124;s)[R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})]$
    |  | (5) |'
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)[R(s,a)+\gamma\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}&#124;s^{\prime})\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})]$
    |  | (6) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)[R(s,a)+\gamma\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}&#124;s^{\prime})\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})]$
    |  | (6) |'
- en: 'By iterating the Bellman equations, we can easily obtain the value functions
    for a policy, and to compare policies, we define that the policy $\pi$ is better
    than $\pi^{\prime}$ if the function $\mathcal{V}$ according to the $\pi$ is no
    less than the function $\mathcal{V}$ according to the $\pi^{\prime}$ for all states,
    that is $\mathcal{V}^{\pi}(s)\geq\mathcal{V}^{\pi^{\prime}}(s),\forall s$. It
    has been proven in [[89](#bib.bib89)] that the existence of the optimal policy
    $\pi^{*}$ is guaranteed in the MDP problem, where $\mathcal{V}^{*}(s)=\max_{\pi}\mathcal{V}^{\pi}(s)$
    and $\mathcal{Q}^{*}(s)=\max_{\pi}\mathcal{Q}^{\pi}(s)$. These two functions are
    defined as the optimal function $\mathcal{V}$ and the optimal function $\mathcal{Q}$.
    We can obtain the optimal policy $\pi^{*}$ by maximizing over the $\mathcal{Q}^{*}(\pi)$
    which can be written as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代贝尔曼方程，我们可以轻松获得策略的价值函数，并且为了比较策略，我们定义策略 $\pi$ 比 $\pi^{\prime}$ 更优，如果 $\pi$
    对应的函数 $\mathcal{V}$ 对所有状态都不低于 $\pi^{\prime}$ 对应的函数 $\mathcal{V}$，即 $\mathcal{V}^{\pi}(s)\geq\mathcal{V}^{\pi^{\prime}}(s),\forall
    s$。在[[89](#bib.bib89)]中已证明，在MDP问题中最优策略 $\pi^{*}$ 的存在是有保障的，其中 $\mathcal{V}^{*}(s)=\max_{\pi}\mathcal{V}^{\pi}(s)$
    和 $\mathcal{Q}^{*}(s)=\max_{\pi}\mathcal{Q}^{\pi}(s)$。这两个函数被定义为最优函数 $\mathcal{V}$
    和最优函数 $\mathcal{Q}$。我们可以通过最大化 $\mathcal{Q}^{*}(\pi)$ 来获得最优策略 $\pi^{*}$，其表示为：
- en: '|  | $\displaystyle\pi^{*}(a&#124;s)=\arg\max\mathcal{Q}^{*}(s,a)$ |  | (7)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi^{*}(a&#124;s)=\arg\max\mathcal{Q}^{*}(s,a)$ |  | (7)
    |'
- en: To improve the policy, we apply the Bellman optimality equations [[89](#bib.bib89)]
    to update value functions by taking the action with maximum value instead of trying
    all possible actions. To facilitate the optimization of the policy, many RL techniques
    are proposed from different perspectives, and Figure [2](#S2.F2 "Figure 2 ‣ 2.1
    Markov Decision Process ‣ 2 Theoretical Foundation and Algorithms of Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Data Processing and Analytics")
    provides a diagram outlining the broad categorization of these techniques, illustrating
    how these techniques can be applied.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进策略，我们应用贝尔曼最优性方程[[89](#bib.bib89)]通过采取具有最大值的动作来更新价值函数，而不是尝试所有可能的动作。为了促进策略的优化，许多RL技术从不同角度提出，图
    [2](#S2.F2 "图 2 ‣ 2.1 马尔可夫决策过程 ‣ 2 强化学习的理论基础和算法 ‣ 深度强化学习在数据处理和分析中的调查") 提供了一个图示，概述了这些技术的广泛分类，展示了这些技术的应用方式。
- en: '![Refer to caption](img/3bde90649a892e748ee969c0a0cb66f9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3bde90649a892e748ee969c0a0cb66f9.png)'
- en: 'Figure 2: Broad categorization of RL techniques.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RL技术的广泛分类。
- en: 2.2 Basic Techniques
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基本技术
- en: 'Based on the representation of MDP elements, basic techniques can be categorized
    into two classes: model-based method and model-free method. The main difference
    is whether the agent has access to model the environment, i.e. whether the agent
    knows the transition function and the reward function. These two functions are
    already known in the model-based method where Dynamic Programming (DP)[[6](#bib.bib6)]
    and Alpha-Zero [[86](#bib.bib86)] are the classical methods which have achieved
    significant results in numerous applications. In these methods, agents are allowed
    to think ahead and plan future actions with known effects on the environment.
    Besides, an agent can learn the optimal policy from the planned experience which
    results in high sample efficiency.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于MDP元素的表示，基本技术可以分为两类：基于模型的方法和无模型的方法。主要区别在于代理是否可以访问环境模型，即代理是否知道转移函数和奖励函数。在基于模型的方法中，这两个函数已经是已知的，其中动态规划（DP）[[6](#bib.bib6)]和Alpha-Zero
    [[86](#bib.bib86)]是经典方法，这些方法在许多应用中取得了显著成果。在这些方法中，代理被允许预先思考并计划对环境的未来作用。此外，代理可以从计划的经验中学习最优策略，从而实现高样本效率。
- en: 'In many RL problems, the reward and the transition function are typically unknown
    due to the complicated environment and its intricate inherent mechanism. For example,
    as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Reinforcement Learning for Data Processing and Analytics"), we are unable to obtain
    the actual latency as the reward in the joint query optimization example. Besides,
    in the stochastic job scheduling problem [[59](#bib.bib59)], it is also impossible
    to directly model the transition function because of the randomness of the job
    arrivals in the practical scenarios. Hence, in these problems, agents usually
    employ model-free methods that can purely learn the policy from the experience
    gained during the interaction with the environment. Model-free methods can mainly
    be classified into two categories, namely the value-based method and the policy-based
    method. In the value-based method, the RL algorithm learns the optimal policy
    by maximizing the value functions. There are two main approaches in estimating
    the value functions that are Mento-Carlo (MC) methods and Temporal difference
    (TD) methods. MC methods calculate the $\mathcal{V}(s)$ by directly applying its
    definition, that is Equation [1](#S2.E1 "In 2.1 Markov Decision Process ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"). MC methods can directly update the
    value functions once they get a new trajectory $\tau$ as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多强化学习问题中，由于环境复杂及其内在机制复杂，奖励和转移函数通常是未知的。例如，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Reinforcement Learning for Data Processing and Analytics")所示，我们无法获得实际延迟作为联合查询优化示例中的奖励。此外，在随机作业调度问题[[59](#bib.bib59)]中，由于实际场景中作业到达的随机性，也无法直接建模转移函数。因此，在这些问题中，代理通常采用无模型的方法，这些方法可以纯粹从与环境交互过程中获得的经验中学习策略。无模型的方法主要可以分为两类，即基于价值的方法和基于策略的方法。在基于价值的方法中，强化学习算法通过最大化价值函数来学习最优策略。估计价值函数的主要方法有两种，分别是蒙特卡罗（MC）方法和时序差分（TD）方法。MC
    方法通过直接应用其定义来计算 $\mathcal{V}(s)$，即方程[1](#S2.E1 "In 2.1 Markov Decision Process
    ‣ 2 Theoretical Foundation and Algorithms of Reinforcement Learning ‣ A Survey
    on Deep Reinforcement Learning for Data Processing and Analytics")。MC 方法一旦获得新的轨迹
    $\tau$ 就可以直接更新价值函数，如下所示：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(\mathcal{G}_{\tau\sim\pi}(\tau&#124;s_{0}=s)-\mathcal{V}^{\pi}(s))$
    |  | (8) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(\mathcal{G}_{\tau\sim\pi}(\tau&#124;s_{0}=s)-\mathcal{V}^{\pi}(s))$
    |  | (8) |'
- en: 'where $\alpha\in[0,1)$ denotes the learning rate which controls the rate of
    updating the policy with new experiences. However, it has an obvious drawback
    that a complete trajectory requires the agent to reach a terminal state, while
    it is not practical in some applications, such as online systems. Different from
    MC methods, the TD method builds on the recursive relationship of value functions,
    and hence, can learn from the incomplete trajectory. Mathematically, the update
    of TD methods can be written as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha\in[0,1)$ 表示学习率，控制用新经验更新策略的速率。然而，它有一个明显的缺点，即完整的轨迹要求代理达到终止状态，而在一些应用中，如在线系统，这并不实际。与
    MC 方法不同，TD 方法基于价值函数的递归关系，因此可以从不完整的轨迹中学习。从数学上讲，TD 方法的更新可以写成：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(R(s,a)+\gamma\mathcal{V}^{\pi}(s^{\prime})-\mathcal{V}^{\pi}(s))$
    |  | (9) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(R(s,a)+\gamma\mathcal{V}^{\pi}(s^{\prime})-\mathcal{V}^{\pi}(s))$
    |  | (9) |'
- en: However, there is bias when estimating the function $\mathcal{V}$ with TD methods
    because they learn from the recursive relationship. To reduce the bias, TD methods
    can extend the length of the incomplete trajectories and update the function $\mathcal{V}$
    by thinking more steps ahead, which is called $n$-steps TD methods. As $n$ grows
    to the length of whole trajectories, MC methods can be regarded as a special case
    of TD methods where function $\mathcal{V}$ is an unbiased estimate. On the other
    side of the coin, as the length $n$ increases, the variance of the trajectory
    also increases. In addition to the above consideration, TD-based methods are more
    efficient and require less storage and computation, thus they are more popular
    among RL algorithms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用 TD 方法估计函数 $\mathcal{V}$ 时存在偏差，因为它们是通过递归关系进行学习的。为了减少偏差，TD 方法可以扩展不完整轨迹的长度，通过考虑更多的步骤来更新函数
    $\mathcal{V}$，这被称为 $n$ 步 TD 方法。随着 $n$ 增长到整个轨迹的长度，MC 方法可以被视为 TD 方法的一个特例，其中函数 $\mathcal{V}$
    是无偏估计。另一方面，随着长度 $n$ 的增加，轨迹的方差也会增加。除了上述考虑之外，基于 TD 的方法更高效，所需的存储和计算也更少，因此在 RL 算法中更受欢迎。
- en: 'In value-based methods, we can obtain the optimal policy by acting greedily
    via Equation [7](#S2.E7 "In 2.1 Markov Decision Process ‣ 2 Theoretical Foundation
    and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Data Processing and Analytics"). The update of the function $\mathcal{Q}$
    with TD methods is similar to the update of the function $\mathcal{V}$, and is
    as follows: $\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\mathcal{Q}^{\pi^{\prime}}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$
    where the agent follows the policy $\pi$ to take actions and follows the policy
    $\pi^{\prime}$ to maximize the function $\mathcal{Q}$. If the two policies are
    the same, that is $\pi^{\prime}=\pi$, we call such RL algorithms the on-policy
    methods where the SARSA[[77](#bib.bib77)] is the representative method. In addition,
    other policies can also be used in $\pi^{\prime}$. For example, in Q-learning[[109](#bib.bib109)],
    the agent applies the greedy policy and updates the function $\mathcal{Q}$ with
    the maximum value in its successor. Its update formula can be written as: $\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$.
    Both value-based methods can work well without the model of the environment, and
    Q-learning directly learns the optimal policy, whilst SARSA learns a near-optimal
    policy during exploring. Theoretically, Q-learning should converge quicker than
    SARSA, but its generated samples have a high variance which may suffer from the
    problems of converging.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于值的方法中，我们可以通过方程 [7](#S2.E7 "In 2.1 Markov Decision Process ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") 通过贪心策略获得最优策略。TD 方法中函数 $\mathcal{Q}$
    的更新类似于函数 $\mathcal{V}$ 的更新，具体如下：$\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\mathcal{Q}^{\pi^{\prime}}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$，其中代理根据策略
    $\pi$ 执行动作，并根据策略 $\pi^{\prime}$ 来最大化函数 $\mathcal{Q}$。如果这两个策略相同，即 $\pi^{\prime}=\pi$，我们称这种
    RL 算法为在线策略方法，其中 SARSA[[77](#bib.bib77)] 是代表方法。此外，其他策略也可以在 $\pi^{\prime}$ 中使用。例如，在
    Q-learning[[109](#bib.bib109)] 中，代理应用贪心策略并通过其后继中的最大值来更新函数 $\mathcal{Q}$。其更新公式可以写作：$\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$。这两种基于值的方法在没有环境模型的情况下都能很好地工作，而
    Q-learning 直接学习最优策略，而 SARSA 在探索过程中学习接近最优的策略。从理论上讲，Q-learning 应该比 SARSA 更快收敛，但其生成的样本具有较高的方差，可能会面临收敛问题。
- en: 'In RL, storage and computation costs are very high when there is a huge number
    of states or actions. To overcome this problem, DRL, as a branch of RL, adopts
    Deep Neural Network (DNN) to replace tabular representations with neural networks.
    For function $\mathcal{V}$, DNN takes the state $s$ as input and outputs its state
    value $\mathcal{V}_{\theta}(s)\approx\mathcal{V}^{\pi}(s)$ where the $\theta$
    denotes the parameter in the DNN. When comes to function $\mathcal{Q}$, It takes
    the combination of the state $s$ and the action $a$ as input and outputs the value
    of the state-action pair $\mathcal{Q}_{\theta}(s,a)\approx\mathcal{Q}^{\pi}(s,a)$,
    As for the neural networks, we can optimize them by applying the techniques that
    are widely used in deep learning (e.g. gradient descent). Deep Q-learning network
    (DQN) [[65](#bib.bib65)], as a representative method in DRL, combines the DNN
    with Q-learning and its loss function is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，当状态或动作数量庞大时，存储和计算成本非常高。为了解决这个问题，深度强化学习（DRL）作为RL的一个分支，采用深度神经网络（DNN）来用神经网络替代表格表示。对于函数$\mathcal{V}$，DNN将状态$s$作为输入，并输出其状态值$\mathcal{V}_{\theta}(s)\approx\mathcal{V}^{\pi}(s)$，其中$\theta$表示DNN中的参数。对于函数$\mathcal{Q}$，它将状态$s$和动作$a$的组合作为输入，并输出状态-动作对的值$\mathcal{Q}_{\theta}(s,a)\approx\mathcal{Q}^{\pi}(s,a)$。至于神经网络，我们可以通过应用在深度学习中广泛使用的技术（例如梯度下降）来优化它们。深度Q学习网络（DQN）[[65](#bib.bib65)]，作为DRL中的一种代表方法，将DNN与Q学习相结合，其损失函数如下：
- en: '|  | $\mathcal{L}_{w}=\mathbf{E}_{\mathcal{D}}[(R(s,a)+\gamma\max_{a^{*}\in\mathcal{A}}\mathcal{Q}_{w}(s^{\prime},a^{*})-\mathcal{Q}_{w}(s,a))^{2}]$
    |  | (10) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{w}=\mathbf{E}_{\mathcal{D}}[(R(s,a)+\gamma\max_{a^{*}\in\mathcal{A}}\mathcal{Q}_{w}(s^{\prime},a^{*})-\mathcal{Q}_{w}(s,a))^{2}]$
    |  | (10) |'
- en: where $\mathcal{D}$ denotes the experience replay which accumulates the generated
    samples and can stabilize the training process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{D}$表示经验回放，累积生成的样本并可以稳定训练过程。
- en: 'Policy-based methods are another branch of the model-free RL algorithm that
    have a clear representation of the policy $\pi(a|s)$, and they can tackle several
    challenges that are encountered in value-based methods. For example, when the
    action space is continuous, value-based methods need to discretize the action
    which could increase the dimensionality of the problem, and memory and computation
    consumption. Value-based methods learn a deterministic policy that generates the
    action given a state through an optimal function $\mathcal{Q}$ (i.e. $\pi(s)=a$).
    However, for policy-based methods, they can learn a stochastic policy (i.e. $\pi_{\theta}(a_{i}|s)=p_{i},\sum_{i}p_{i}=1$)
    as the optimal policy, where $p_{i}$ denotes the probability of taking the action
    $a_{i}$ given a state $s$, and $\theta$ denotes the parameters where neural networks
    can be used to approximate the policy. Policy Gradient [[90](#bib.bib90)] method
    is one of the main policy-based methods which can tackle the aforementioned challenges.
    Its goal is to optimize the parameters $\theta$ by using the gradient ascent method,
    and the target can be denoted in a generalized expression:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的方法是模型自由RL算法的另一个分支，它们对策略$\pi(a|s)$有明确的表示，并且可以解决在基于价值的方法中遇到的几个挑战。例如，当动作空间是连续的时，基于价值的方法需要离散化动作，这可能会增加问题的维度，以及内存和计算消耗。基于价值的方法学习一个确定性的策略，通过一个最优函数$\mathcal{Q}$生成给定状态下的动作（即$\pi(s)=a$）。然而，对于基于策略的方法，它们可以学习一个随机策略（即$\pi_{\theta}(a_{i}|s)=p_{i},\sum_{i}p_{i}=1$）作为最优策略，其中$p_{i}$表示在状态$s$下采取动作$a_{i}$的概率，而$\theta$表示参数，其中神经网络可以用来逼近策略。策略梯度[[90](#bib.bib90)]方法是主要的基于策略的方法之一，它可以解决上述挑战。其目标是通过使用梯度上升法优化参数$\theta$，并且目标可以用一个广义表达式表示：
- en: '|  | $\nabla_{\theta}J(\theta)=\mathbf{E}_{\tau\sim\pi_{\theta}}[R(\tau)\nabla_{\pi_{\theta}}\log_{\pi_{\theta}}(a&#124;s)]$
    |  | (11) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\theta}J(\theta)=\mathbf{E}_{\tau\sim\pi_{\theta}}[R(\tau)\nabla_{\pi_{\theta}}\log_{\pi_{\theta}}(a&#124;s)]$
    |  | (11) |'
- en: The specific proof process can refer to [[89](#bib.bib89)]. Sampling via the
    MC methods, we will get the entire trajectories to improve the policy for the
    policy-based methods.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的证明过程可以参考[[89](#bib.bib89)]。通过MC方法进行采样，我们可以获得整个轨迹，以改善基于策略的方法的策略。
- en: After training, the action with higher rewards in expectation will have a higher
    probability to be chosen and vice versa. As for the continuous action, The optimal
    policy learned from the Policy Gradient is stochastic which still needs to be
    sampled to get the action. However, the stochastic policy still requires lots
    of samples to train the model when the search space is huge. Deterministic Policy
    Gradient (DPG) [[87](#bib.bib87)], as an extension of the Policy Gradient, overcomes
    this problem by using a stochastic policy to perform sampling while applying deterministic
    policy to output the action which demands relatively fewer samples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，期望奖励较高的动作将有更高的概率被选择，反之亦然。对于连续动作，从策略梯度学到的最优策略是随机的，仍然需要采样来获得动作。然而，当搜索空间巨大时，随机策略仍然需要大量样本来训练模型。确定性策略梯度（DPG）[[87](#bib.bib87)]
    作为策略梯度的扩展，通过使用随机策略进行采样，同时应用确定性策略输出动作，从而克服了这一问题，减少了所需样本量。
- en: 'Both value-based methods and policy-based methods have their strengths and
    weaknesses, but they are not contradictory to each other. Actor-Critic (AC) method,
    as the integration of both methods, divides the model into two parts: actor and
    critic. The actor part selects the action based on the parameterized policy and
    the critic part concentrates on evaluating the value functions. Different from
    previous approaches, AC evaluates the advantage function $\mathcal{A}^{\pi}(s,a)=\mathcal{Q}^{\pi}(s,a)-\mathcal{V}^{\pi}(s)$
    which reflects the relative advantage of a certain action $a$ to the average value
    of all actions. The introduction of the value functions also allows AC to update
    by step through the TD method, and the incorporation of the policy-based methods
    makes AC be suitable for continuous actions. However, the combination of the two
    methods also makes the AC method more difficult to converge. Moreover, Deep Deterministic
    Policy Gradient (DDPG) [[49](#bib.bib49)], as an extension of the AC, absorbs
    the advanced techniques from the DQN and the DPG which enables DDPG to learn the
    policy more efficiently.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的方法和基于策略的方法各有优缺点，但它们并不相互矛盾。Actor-Critic (AC) 方法作为两者的融合，将模型分为两个部分：actor 和
    critic。actor 部分基于参数化的策略选择动作，而 critic 部分专注于评估价值函数。与之前的方法不同，AC 评估优势函数 $\mathcal{A}^{\pi}(s,a)=\mathcal{Q}^{\pi}(s,a)-\mathcal{V}^{\pi}(s)$，它反映了某个动作
    $a$ 相对于所有动作的平均值的相对优势。价值函数的引入还允许 AC 通过 TD 方法逐步更新，并且政策方法的结合使 AC 适用于连续动作。然而，两种方法的结合也使得
    AC 方法更难以收敛。此外，Deep Deterministic Policy Gradient (DDPG) [[49](#bib.bib49)] 作为
    AC 的扩展，吸收了 DQN 和 DPG 的先进技术，使 DDPG 能够更有效地学习策略。
- en: 'In all the above-mentioned methods, there always exists a trade-off between
    exploring the unknown situation and exploiting with learned knowledge. On the
    one hand, exploiting the learned knowledge can help the model converge quicker,
    but it always leads the model into a local optimal rather than a globally optimal.
    On the other hand, exploring unknown situations can find some new and better solutions,
    but always being in the exploring process causes the model hard to converge. To
    balance these two processes, researchers have been devoting much energy to finding
    a good heuristics strategy, such as $\epsilon-greedy$ strategy, Boltzmann exploration
    (Softmax exploration), upper confidence bound (UCB) algorithm [[2](#bib.bib2)],
    Thompson sampling [[92](#bib.bib92)], and so on. Here, we consider the $\epsilon-greedy$,
    a widely used exploration strategy, as an example. $\epsilon-greedy$ typically
    selects the action with the maximal Q value to exploit the learned experience
    while occasionally selecting an action evenly at random to explore unknown cases.
    $\epsilon-greedy$ exploration strategy with $m$ actions can be denoted as follow:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述所有方法中，总是存在探索未知情况和利用已学知识之间的权衡。一方面，利用已学知识可以帮助模型更快收敛，但它总是将模型引向局部最优而非全局最优。另一方面，探索未知情况可以找到一些新的、更好的解决方案，但始终处于探索过程中的模型难以收敛。为了平衡这两个过程，研究人员一直在致力于寻找良好的启发式策略，例如
    $\epsilon$-贪婪策略、Boltzmann 探索（Softmax 探索）、上置信界（UCB）算法 [[2](#bib.bib2)]、Thompson
    采样 [[92](#bib.bib92)] 等。在这里，我们以 $\epsilon$-贪婪策略为例，$\epsilon$-贪婪通常选择具有最大 Q 值的动作来利用已学经验，同时偶尔随机选择一个动作以探索未知情况。具有
    $m$ 个动作的 $\epsilon$-贪婪探索策略可以表示如下：
- en: '|  | $\pi(a&#124;s)=\left\{\begin{array}[]{ll}\epsilon/m+(1-\epsilon)&amp;a^{*}=arg\max_{a\in\mathcal{A}}\mathcal{Q}(s,a),\\
    \epsilon/m&amp;a\not=a^{*}.\end{array}\right.$ |  | (12) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi(a&#124;s)=\left\{\begin{array}[]{ll}\epsilon/m+(1-\epsilon)&amp;a^{*}=arg\max_{a\in\mathcal{A}}\mathcal{Q}(s,a),\\
    \epsilon/m&amp;a\not=a^{*}.\end{array}\right.$ |  | (12) |'
- en: $\epsilon\in[0,1)$ is an exploration factor. The agent is more likely to select
    the action at random when the $\epsilon$ is closer to 1, and the $\epsilon$ will
    be continuously reduced during the training process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $\epsilon\in[0,1)$ 是探索因子。当 $\epsilon$ 趋近于1时，智能体更可能随机选择动作，而 $\epsilon$ 将在训练过程中持续减少。
- en: 2.3 Advanced Techniques
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 高级技术
- en: 'This section mainly discusses some advanced techniques in RL which focus on
    efficiently using the limited samples and building sophisticated model structures
    for better representation and optimization. According to the different improvements,
    they can be broadly classified into two parts: data sampling and model efficiency.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要讨论了强化学习中的一些高级技术，这些技术集中在有效利用有限样本和构建复杂模型结构以获得更好的表示和优化。根据不同的改进，这些技术大致可以分为两部分：数据采样和模型效率。
- en: 2.3.1 Data Sampling
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 数据采样
- en: Data sampling is one of the most important concerns in training the DRL in data
    processing and analytics. In most applications, the sample generation process
    costs a great amount of time and computation resources. For example, a sample
    may refer to an execution run for workload and repartitioning for the database,
    which can take about 40 minutes[[31](#bib.bib31)]. Hence, to train the model with
    limited samples, we need to increase data utilization and reduce data correlation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采样是训练DRL时数据处理和分析中最重要的关注点之一。在大多数应用中，样本生成过程耗费大量时间和计算资源。例如，一个样本可能指的是工作负载的执行运行和数据库的重分区，这可能需要大约40分钟[[31](#bib.bib31)]。因此，为了用有限的样本训练模型，我们需要提高数据利用率并减少数据相关性。
- en: 'Data utilization: Most DRL algorithms train the optimal policy and sample data
    at the same time. Instead of dropping samples after being trained, experience
    replay[[50](#bib.bib50)] accumulates the samples in a big table where samples
    are randomly selected during the learning phase. With this mechanism, samples
    will have a higher utilization rate and a lower variance, and hence, it can stabilize
    the training process and accelerate the training convergence. Samples after several
    iterations may differ from the current policy, and hence, Growing-batch [[40](#bib.bib40)]
    can continuously refresh the table and replace these outdated samples. In addition,
    samples that are far away from the current policy should be paid more attention
    and Prioritized Experience Replay[[79](#bib.bib79)] uses TD error as the priority
    to measure the sample importance, and hence, focus more on learning the samples
    with high errors. In a nutshell, with the experience replay, DRL cannot only stable
    the learning phase but also efficiently optimize the policy with fewer samples.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据利用：大多数深度强化学习（DRL）算法同时训练最优策略和采样数据。与训练后丢弃样本不同，经验回放[[50](#bib.bib50)]将样本积累在一个大表中，在学习阶段期间随机选择样本。通过这种机制，样本的利用率更高，方差更低，从而可以稳定训练过程并加快训练收敛。经过多次迭代后的样本可能与当前策略不同，因此，Growing-batch
    [[40](#bib.bib40)]可以持续刷新表格并替换这些过时的样本。此外，应更加关注那些远离当前策略的样本，而优先经验回放[[79](#bib.bib79)]使用TD误差作为优先级来衡量样本的重要性，从而更多地关注高误差的样本。总之，通过经验回放，DRL不仅可以稳定学习阶段，还可以通过更少的样本高效优化策略。
- en: 'Data correlation: Strong correlation of training data is another concern that
    may lead the agent to learn a sub-optimal solution instead of the globally optimal
    one. Apart from the experience replay, the mechanism of the distributed environments
    is another research direction to alleviate this problem. For example, the asynchronous
    advantage actor-critic (A3C) [[64](#bib.bib64)] and Distributed PPO (DPPO) [[27](#bib.bib27)]
    apply multi-threads to build multiple individual environments where multiple agents
    take actions in parallel, and the update is calculated periodically and separately
    which can accelerate the sampling process and reduce the data correlation.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据相关性：训练数据的强相关性是另一个可能导致智能体学习到次优解而非全局最优解的问题。除了经验回放外，分布式环境的机制是缓解此问题的另一个研究方向。例如，异步优势演员-评论家（A3C）[[64](#bib.bib64)]
    和 分布式PPO（DPPO）[[27](#bib.bib27)]应用多线程来构建多个独立环境，在这些环境中，多个智能体并行采取行动，更新会周期性且分别地计算，从而可以加速采样过程并减少数据相关性。
- en: 2.3.2 Model Efficiency
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 模型效率
- en: RL model with better efficiency is the major driving force of the development
    of RL, and there are many researchers improving it from three major aspects, namely
    policy, reward function, and value function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更高效的 RL 模型是 RL 发展的主要推动力，许多研究者从三个主要方面进行改进，即政策、奖励函数和价值函数。
- en: 'Policy: The policy-related techniques focus on stably and effectively learning
    a comprehensive policy, and the advanced techniques to efficiently learn the policy
    can be classified into three parts in detail, which are policy exploration, policy
    representation, and policy optimization.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 政策：与政策相关的技术集中于稳定和有效地学习全面的政策，高级技术有效地学习政策可以详细分类为三部分，即政策探索、政策表示和政策优化。
- en: 'a) Policy exploration: Its target is to explore as many actions as possible
    during the training process in case the policy will be trapped into the local
    optimal. For example, entropy regularisation [[64](#bib.bib64)] adds the entropy
    of the actions’ probabilities into the loss item which can sufficiently explore
    the actions. Besides, adding noise to the action is another research direction
    to increase the randomness into policy exploration. The DDPG applies an Ornstein–Uhlenbeck
    process [[96](#bib.bib96)] to generate temporal noise $\mathcal{N}$ which are
    directly injected into policy. Noisy-Net [[22](#bib.bib22)] incorporates the noise
    into the parameters of neural networks which is easy to implement, and it shows
    a better performance than the $\epsilon-greedy$ and entropy regularisation methods.
    Further, Plappert et al. [[75](#bib.bib75)] investigate an effective way to combine
    the parameter space noise to enrich the exploratory behaviors which can benefit
    both on-policy methods and off-policy methods.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: a) 政策探索：其目标是在训练过程中尽可能多地探索各种动作，以防政策陷入局部最优。例如，熵正则化 [[64](#bib.bib64)] 将动作概率的熵添加到损失项中，这可以充分探索动作。此外，向动作中添加噪声是另一个研究方向，用于增加政策探索的随机性。DDPG
    应用 Ornstein–Uhlenbeck 过程 [[96](#bib.bib96)] 生成时间噪声 $\mathcal{N}$，这些噪声直接注入政策中。Noisy-Net
    [[22](#bib.bib22)] 将噪声融入神经网络的参数中，这种方法易于实现，且表现优于 $\epsilon-greedy$ 和熵正则化方法。此外，Plappert
    等人 [[75](#bib.bib75)] 研究了一种有效的方式，将参数空间噪声结合起来，以丰富探索行为，这对政策方法和离政策方法都有好处。
- en: 'b) Policy representation: The states in some RL problems are in a huge dimension
    which causes challenges during the training. To approximate a better policy, a
    branch of DRL models improve the policy representation by absorbing convolutional
    neural networks (CNN) into DQN to analyze the data, such as Dueling DQN [[108](#bib.bib108)],
    DRQN [[26](#bib.bib26)], and so on. In addition, DRQN also incorporates the LSTM
    structure to increase the capacity of the policy which is able to capture the
    temporal information, such as speed, direction.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: b) 政策表示：一些 RL 问题中的状态具有巨大的维度，这在训练过程中带来了挑战。为了近似更好的政策，一些 DRL 模型通过将卷积神经网络（CNN）引入
    DQN 来改进政策表示，例如 Dueling DQN [[108](#bib.bib108)]、DRQN [[26](#bib.bib26)] 等。此外，DRQN
    还结合了 LSTM 结构，以增加政策的容量，能够捕捉时间信息，例如速度、方向。
- en: 'c) Policy optimization: The update of the value functions following the Equation
    [5](#S2.E5 "In 2.1 Markov Decision Process ‣ 2 Theoretical Foundation and Algorithms
    of Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning for Data Processing
    and Analytics") and [6](#S2.E6 "In 2.1 Markov Decision Process ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") tends to overestimate the value functions
    and introduce a bias because they learn estimates from the estimates. Mnih et
    al.[[66](#bib.bib66)] separate the two estimation process by using two same Q-networks
    which can reduce the correlation of two estimation processes and hence, stabilize
    the course of training. However, the action with the maximum Q-value may differ
    between two Q-networks which will be hard for convergence. and Double DQN (DDQN)
    [[99](#bib.bib99)] alleviate the issue by disaggregating the step of selecting
    the action and calculating the max Q-value.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 策略优化：根据方程[5](#S2.E5 "In 2.1 Markov Decision Process ‣ 2 Theoretical Foundation
    and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Data Processing and Analytics")和[6](#S2.E6 "In 2.1 Markov Decision Process
    ‣ 2 Theoretical Foundation and Algorithms of Reinforcement Learning ‣ A Survey
    on Deep Reinforcement Learning for Data Processing and Analytics")更新的价值函数倾向于高估价值函数，并引入偏差，因为它们从估计中学习。Mnih等人[[66](#bib.bib66)]通过使用两个相同的Q网络将两个估计过程分开，从而减少两个估计过程的相关性，从而稳定训练过程。然而，具有最大Q值的动作可能在两个Q网络之间有所不同，这将使收敛变得困难。双重DQN（DDQN）[[99](#bib.bib99)]通过将选择动作和计算最大Q值的步骤分开来缓解这个问题。
- en: 'When we apply the policy-based RL methods, the learning rate of the policy
    plays an essential role in achieving superior performance. A higher learning rate
    can always maximize the improvement on a policy by step, but it also causes the
    instability of the learning phase. Hence, The Trust Region Policy Optimization
    (TRPO) [[80](#bib.bib80)] builds constraints on the old policy and new policy
    via KL divergence to control the change of the policy in an acceptable range.
    With this constraint, TRPO can iteratively optimize the policy via a surrogate
    objective function which can monotonically improve policies. However, the design
    of the KL constraint makes it hard to be trained, and Proximal Policy Optimization
    (PPO) [[81](#bib.bib81)] simplifies the constraint through two ways: adding it
    into the objective function, designing a clipping function to control the update
    rate. Empirically, PPO methods are much simpler to implement and are able to perform
    at least as well as TRPO.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用基于策略的强化学习方法时，策略的学习率在实现优越性能中起着至关重要的作用。较高的学习率可以始终最大化策略的逐步改进，但也会导致学习阶段的不稳定性。因此，信任域策略优化（TRPO）[[80](#bib.bib80)]通过KL散度对旧策略和新策略施加约束，以控制策略变化在可接受的范围内。有了这个约束，TRPO可以通过一个可以单调改善策略的代理目标函数来迭代优化策略。然而，KL约束的设计使得训练变得困难，临近策略优化（PPO）[[81](#bib.bib81)]通过两种方式简化了约束：将其添加到目标函数中，并设计了一个裁剪函数来控制更新速率。经验上，PPO方法实现起来要简单得多，并且至少能够达到与TRPO相当的性能。
- en: 'Reward: Reward function as one of the key components in the MDP plays an essential
    role in the RL. In some specific problems, the agent has to achieve multiple goals
    which may have some relationships. For example, the robot can only get out through
    the door only if it has already found the key. To tackle this challenge, Hierarchical
    DQN [[38](#bib.bib38)] proposes two levels of hierarchical RL (HRL) models to
    repeatedly select a new goal and achieve the chosen goal. However, there is a
    limitation that the goal needs to be manually predefined which may be unknown
    or unmeasurable in some environments, such as the market and the effect of a drug.
    To overcome it, Inverse RL (IRL) [[68](#bib.bib68)] learns the rewards function
    from the given experts’ demonstrations (i.e. the handcraft trajectories), but
    the agent in IRL can only prioritize the entire trajectories over others. It will
    cause a shift when the agent comes to a state that never appears before, and Generative
    Adversarial Imitation Learning (GAIL) [[32](#bib.bib32)], as an imitation learning
    algorithm, applies adversarial training methods to generate fake samples and is
    able to learn the expert’s policy explicitly and directly.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励：奖励函数作为马尔可夫决策过程（MDP）中的关键组成部分，在强化学习（RL）中扮演着重要角色。在一些特定问题中，代理需要实现多个目标，这些目标可能存在某种关系。例如，机器人只有在找到钥匙后才能通过门。为解决这一挑战，分层
    DQN [[38](#bib.bib38)] 提出了两个层级的分层 RL (HRL) 模型，以重复选择新的目标并实现选定目标。然而，其局限性在于目标需要手动预定义，这在一些环境中可能未知或不可测量，如市场和药物效果。为了克服这个问题，逆强化学习（IRL）
    [[68](#bib.bib68)] 从给定的专家演示（即手工制作的轨迹）中学习奖励函数，但 IRL 中的代理只能优先考虑整个轨迹而非其他轨迹。当代理进入一个从未出现过的状态时，这将导致偏移。生成对抗模仿学习（GAIL）
    [[32](#bib.bib32)] 作为一种模仿学习算法，应用对抗训练方法生成虚假样本，并能够明确直接地学习专家的策略。
- en: 'Value: As we have mentioned earlier, the tabular representation of the value
    functions has several limitations which can be alleviated via DRL. Different from
    directly taking the state-action pair as the input to calculate the Q-function,
    Dueling DQN [[108](#bib.bib108)] estimates its value by approximating two separate
    parts that are the state-values and the advantage values, and hence, can distinguish
    whether the value is brought by the state or the action.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 价值：如我们之前提到的，价值函数的表格表示法有若干局限，这些局限可以通过深度强化学习（DRL）来缓解。不同于直接将状态-动作对作为输入来计算 Q 函数，Dueling
    DQN [[108](#bib.bib108)] 通过近似两个独立的部分来估计其价值，即状态价值和优势价值，因此能够区分价值是由状态还是动作带来的。
- en: The aforementioned advanced algorithms and techniques improve and enhance the
    DRL from different perspectives, which makes DRL-based algorithms be a promising
    way to improve data processing and analytics. We observe that problems with the
    following characteristics may be amenable to DRL-based optimization. First, problems
    are incredibly complex and difficult. The system and application involve a complicated
    operational environment (e.g., large-scale, high-dimensional states) and internal
    implementation mechanisms, which is hard to construct a white-box model accurately.
    DRL can process complex data and learn from experience generated from interacting,
    which is naturally suitable for data processing and analytics where many kinds
    of data exist and are processed frequently. Second, the optimization objectives
    can be represented and calculated easily as the reward because the RL agent improves
    itself towards maximizing the rewards and rewards could be computed a lot of times
    during training. Third, the environment can be well described as MDP. DRL has
    been shown to solve MDP with theoretical guarantees and empirical results. Thus,
    problems involving sequential decision making such as planning, scheduling, structure
    generation (e.g., tree, graph), and searching could be expressed as MDP and a
    good fit for DRL. Fourth, collecting required labels of data massively is hard.
    Compared to supervised learning, DRL can utilize data efficiently to gain good
    performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述高级算法和技术从不同角度改善和增强了DRL，这使得基于DRL的算法成为提高数据处理和分析的有前途的方法。我们观察到具有以下特点的问题可能适合DRL优化。首先，问题极其复杂且困难。系统和应用涉及复杂的操作环境（例如，大规模、高维状态）和内部实现机制，很难准确构建白盒模型。DRL可以处理复杂数据并通过交互产生经验进行学习，这自然适用于存在多种数据并频繁处理的数据处理和分析。其次，优化目标可以作为奖励轻松表示和计算，因为RL代理在训练过程中通过最大化奖励自我改进，奖励可以计算很多次。第三，环境可以很好地描述为MDP。DRL已被证明能够解决MDP，并且具有理论保证和实证结果。因此，涉及顺序决策的问题，如规划、调度、结构生成（例如，树、图）和搜索，可以表示为MDP，并且非常适合DRL。第四，大规模收集所需的数据标签很困难。与监督学习相比，DRL可以有效利用数据以获得良好的性能。
- en: 3 Data System Optimizations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据系统优化
- en: DRL learns knowledge about the system by interacting with it and optimizes the
    system. In this section, we focus on several fundamental aspects with regards
    to system optimization in data processing and analytics including data organization,
    scheduling, tuning, indexing, query optimization, and cache management. We discuss
    how each problem is formulated in MDP by defining three key elements (action,
    state, and reward) in the system and solved by DRL. Generally, the states are
    defined by some key characteristics of the system. The actions are possible decisions
    (e.g., system configuration), that affect the system performance and the reward
    is calculated based on the performance metrics (e.g. throughput, latency). Table
    1 presents a summary of representative works and the estimated dimension ranges
    on the state and action space of each work are added as signals on the DRL training
    difficulty. As a comparison, OpenAI Five[[8](#bib.bib8)], a Dota-playing AI, observes
    the state as 20,000 numbers representing useful game information and about 1,000
    valid actions (like ordering a hero to move to a location) for per hero. Dota
    is a real-time strategy game between two teams of five players where each player
    controls a character called a “hero”.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: DRL通过与系统交互来学习系统知识并优化系统。在本节中，我们重点讨论数据处理和分析中系统优化的几个基本方面，包括数据组织、调度、调整、索引、查询优化和缓存管理。我们讨论了如何通过定义系统中的三个关键元素（动作、状态和奖励）将每个问题表述为MDP，并由DRL解决。通常，状态由系统的一些关键特征定义。动作是可能的决策（例如，系统配置），影响系统性能，而奖励则基于性能指标（例如，吞吐量、延迟）进行计算。表1展示了代表性工作的总结，每项工作的状态和动作空间的估计维度范围作为DRL训练难度的信号。作为对比，OpenAI
    Five[[8](#bib.bib8)]，一个玩Dota的AI，将状态观察为20,000个数字，代表有用的游戏信息，每个英雄约有1,000个有效动作（例如，命令英雄移动到某个位置）。Dota是一个实时战略游戏，由两个五人团队进行，每个玩家控制一个称为“英雄”的角色。
- en: 3.1 Data Organization
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据组织
- en: 3.1.1 Data Partitioning
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 数据分区
- en: Effective data partitioning strategy is essential to accelerate data processing
    and analytics by skipping irrelevant data for a given query. It is challenging
    as many factors need to be considered, including the workload and data characteristics,
    hardware profiles, and system implementation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的数据分区策略对于加速数据处理和分析至关重要，因为它通过跳过给定查询的不相关数据来实现。这是一项具有挑战性的任务，因为需要考虑许多因素，包括工作负载和数据特性、硬件配置以及系统实现。
- en: In data analytics systems, data is split into blocks in main memory or secondary
    storage, which are accessed by relevant queries. A query may fetch many blocks
    redundantly and, therefore, an effective block layout avoids reading unnecessary
    data and reduces the number of block accesses, thereby improving the system performance.
    Yang et al.[[116](#bib.bib116)] propose a framework called the qd-tree that partitions
    data into blocks using DRL over the analytical workload. The qd-tree resembles
    the classic k-d tree and describes the partition of multi-dimensional data space
    where each internal node splits data using a particular predicate and represents
    a subspace. The data in the leaf node is assigned to the same block. In the MDP,
    each state is a node representing the subspace of the whole data and featured
    as the concatenation of range and category predicates. After the agent takes an
    action to generate two child nodes, two new states will be produced and explored
    later. The available action set is the predicates parsed from workload queries.
    The reward is computed by the normalized number of skipped blocks over all queries.
    They do not execute queries and a sampling technique is used to estimate the reward
    efficiently. The formulation of using DRL to learn a tree is similar to NeuroCuts[[47](#bib.bib47)]
    that learns a tree for packet classification. However, the qd-tree may not support
    a complex workload containing user-defined functions (UDFs) queries.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析系统中，数据被分割成块存储在主内存或辅助存储中，这些块会被相关查询访问。一个查询可能会冗余地提取许多块，因此，有效的块布局可以避免读取不必要的数据，减少块访问次数，从而提高系统性能。杨等人[[116](#bib.bib116)]提出了一种称为qd-tree的框架，该框架通过在分析负载上使用DRL将数据分区成块。qd-tree类似于经典的k-d树，描述了多维数据空间的划分，其中每个内部节点使用特定谓词分割数据，并表示一个子空间。叶节点中的数据被分配到同一个块中。在MDP中，每个状态是一个节点，代表整个数据的子空间，并以范围和类别谓词的串联形式呈现。代理在采取行动生成两个子节点后，将产生两个新状态，并在稍后进行探索。可用的动作集是从工作负载查询中解析出的谓词。奖励通过所有查询中跳过块的归一化数量来计算。他们不执行查询，而是使用采样技术来高效地估计奖励。使用DRL学习树的公式类似于NeuroCuts[[47](#bib.bib47)]，后者学习用于数据包分类的树。然而，qd-tree可能不支持包含用户定义函数（UDFs）查询的复杂工作负载。
- en: 'Horizontal partitioning in the database chooses attributes of large tables
    and splits them across multiple machines to improve the performance of analytical
    workloads. The design relies on either the experience of database administrators
    (DBAs) or cost models that are often inaccurate[[42](#bib.bib42)] to predict the
    runtime for different partitions. Data collection is too challenging and costly
    to train the accurate supervised learning model in the cloud environment. Hilprecht
    et al.[[31](#bib.bib31)] learn to partition using DRL on analytical workloads
    in cloud databases, on the fact that DRL is able to efficiently navigate the partition
    search and requires less training data. In the MDP, the state consists of two
    parts. The database part encodes whether a table is replicated, an attribute is
    used for partitioning, and which tables are co-partitioned. The workload part
    incorporates normalized frequencies of representative queries. Supported actions
    are: partitioning a table using an attribute, replicating a table, and changing
    tables co-partition. The reward is the negative of the runtime for the workload.
    One challenge is that the cost of database partitioning is high during training.
    To alleviate the problem, the agent is trained in the simulation environment and
    is further refined in the real environment by estimating the rewards using sampling.
    One limitation is that it may not support new queries well because only the frequency
    features of queries are considered. Durand et al. in [[17](#bib.bib17), [18](#bib.bib18)]
    utilize DRL to improve vertical partitioning that optimizes the physical table
    layout. They show that the DQN algorithm can easily work for a single workload
    with one table but is hard to generalize to random workloads.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库中的水平分区选择大表的属性，并将其拆分到多个机器上，以提高分析工作负载的性能。该设计依赖于数据库管理员（DBA）的经验或通常不准确的成本模型[[42](#bib.bib42)]来预测不同分区的运行时间。在云环境中，数据收集过于复杂和昂贵，难以训练出准确的监督学习模型。Hilprecht
    等人[[31](#bib.bib31)] 使用深度强化学习（DRL）在云数据库中的分析工作负载上进行分区学习，因为 DRL 能够高效地导航分区搜索，并且需要较少的训练数据。在马尔可夫决策过程（MDP）中，状态包括两个部分。数据库部分编码了一个表是否被复制、一个属性是否用于分区以及哪些表是共同分区的。工作负载部分包含了代表性查询的归一化频率。支持的操作包括：使用属性对表进行分区、复制表和更改表的共同分区。奖励是工作负载运行时间的负值。一个挑战是数据库分区的成本在训练过程中较高。为了解决这个问题，代理在模拟环境中进行训练，并通过采样估算奖励在真实环境中进一步优化。一个限制是由于只考虑了查询的频率特征，它可能无法很好地支持新查询。Durand
    等人[[17](#bib.bib17), [18](#bib.bib18)] 利用 DRL 改进垂直分区，以优化物理表布局。他们展示了 DQN 算法可以很容易地用于一个表的单一工作负载，但难以推广到随机工作负载。
- en: 'For UDFs analytics workloads on unstructured data, partitioning is more challenging
    where UDFs could express complex computations and functional dependency is unavailable
    in the unstructured data. Zou et al.[[127](#bib.bib127)] propose the Lachesis
    system to provide automatic partitioning for non-relational data analytics. Lachesis
    translates UDFs to graph-based intermediate representations (IR) and identifies
    partition candidates based on the subgraph of IR as a two-terminal graph. Lachesis
    adopts DRL to learn to choose the optimal candidate. The state incorporates features
    for each partition extracted from historical workflows: frequency, the execution
    interval, time of the most recent run, complexity, selectivity, key distribution,
    number, and size of co-partition. In addition, the state also incorporates other
    features such as hardware configurations. The action is to select one partition
    candidate. The reward is the throughput speedup compared to the average throughput
    of the historical executions of applications. To reduce the training time, the
    reward is derived from historical latency statistics without partitioning the
    data when running the applications. One limitation is that Lachesis largely depends
    on historical statistics to design the state and calculate the reward, which could
    lead to poor performance when the statistics are inadequate.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非结构化数据上的UDF分析工作负载，分区更具挑战性，因为UDF可能表达复杂的计算，而非结构化数据中没有功能依赖。Zou等人[[127](#bib.bib127)]提出了Lachesis系统，以提供对非关系数据分析的自动分区。Lachesis将UDF转换为基于图的中间表示（IR），并根据IR的子图作为双端图来识别分区候选。Lachesis采用DRL来学习选择最佳候选。状态包括从历史工作流中提取的每个分区的特征：频率、执行间隔、最近运行时间、复杂性、选择性、键分布、协同分区的数量和大小。此外，状态还包括其他特征，如硬件配置。动作是选择一个分区候选。奖励是与应用程序历史执行的平均吞吐量相比的吞吐量加速。为了减少训练时间，奖励是从历史延迟统计中得出的，在运行应用程序时无需对数据进行分区。一个局限性是Lachesis在设计状态和计算奖励时很大程度上依赖于历史统计，这可能会导致统计数据不足时性能较差。
- en: 3.1.2 Data Compression
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 数据压缩
- en: Data compression is widely employed to save storage space. The effectiveness
    of a compression scheme however relies on the data types and patterns. In time-series
    data, the pattern can change over time and a fixed compression scheme may not
    work well for the entire duration. Yu et al.[[120](#bib.bib120)] propose a two-level
    compression framework, where a scheme space is constructed by extracting global
    features at the top level and a compression schema is selected for each point
    at the bottom level. The proposed AMMMO framework incorporates compression primitives
    and the control parameters, which define the compression scheme space. Due to
    the fact that the enumeration is computationally infeasible, the framework proposes
    to adopt DRL to find the compression scheme. The agent takes a block that consists
    of 32 data points with the compressed header and data segment, timestamps, and
    metrics value as the state. The action is to select a scheme from compression
    scheme space and then the compression ratio is computed as the reward. The limitation
    is that the method may not work for other data types like images and videos.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩被广泛使用以节省存储空间。然而，压缩方案的有效性依赖于数据类型和模式。在时间序列数据中，模式可能随时间变化，固定的压缩方案可能不适用于整个期间。Yu等人[[120](#bib.bib120)]提出了一个两级压缩框架，其中通过在顶层提取全局特征构建方案空间，并在底层为每个点选择一个压缩方案。所提出的AMMMO框架包括压缩原语和控制参数，这些定义了压缩方案空间。由于枚举在计算上不可行，该框架建议采用DRL来寻找压缩方案。代理将一个包含32个数据点的块（包括压缩头和数据段、时间戳和度量值）作为状态。动作是从压缩方案空间中选择一个方案，然后计算压缩比作为奖励。局限性在于该方法可能不适用于其他数据类型，如图像和视频。
- en: 'Table 1: Representative Works using DRL for Data System Optimizations. D(X)
    denotes the approximate dimension of X space.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用DRL进行数据系统优化的代表性工作。D(X)表示X空间的近似维度。
- en: '| Domain | Work | Algorithm | D(State) | D(Action) | DRL-based Approach | Open
    Source |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 工作 | 算法 | D(状态) | D(动作) | 基于DRL的方法 | 开源 |'
- en: '| Data organization | Analytical system data partition[[116](#bib.bib116)]
    | PPO | 10 - 100 | 100 - 1000 | Exploit workload patterns and Generate the tree
    | NO |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 数据组织 | 分析系统数据分区[[116](#bib.bib116)] | PPO | 10 - 100 | 100 - 1000 | 利用工作负载模式并生成树
    | 否 |'
- en: '|  | Database horizontal partition [[31](#bib.bib31)] | DQN | 100 | 10 | Navigate
    the partition search efficiently | NO |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据库水平分区 [[31](#bib.bib31)] | DQN | 100 | 10 | 高效导航分区搜索 | NO |'
- en: '|  | UDF-centric workload data partition [[127](#bib.bib127)] | A3C | 10 |
    1-10 | Exploit the features of partition and search | YES |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | UDF-中心工作负载数据分区 [[127](#bib.bib127)] | A3C | 10 | 1-10 | 利用分区和搜索的特性 | YES
    |'
- en: '|  | Time series data compression [[120](#bib.bib120)] | PG | 100 | 10 | Search
    parameters interactively | NO |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 时间序列数据压缩 [[120](#bib.bib120)] | PG | 100 | 10 | 交互式搜索参数 | NO |'
- en: '| Scheduling | Distributed job processing [[58](#bib.bib58)] | PG | 100 | 10
    | Exploit the job dependencies and learn schedule decision | YES |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 调度 | 分布式作业处理 [[58](#bib.bib58)] | PG | 100 | 10 | 利用作业依赖关系并学习调度决策 | YES |'
- en: '|  | Distributed stream data [[45](#bib.bib45)] | DDPG | 100 | 10-100 | Learn
    schedule decision | NO |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 分布式流数据 [[45](#bib.bib45)] | DDPG | 100 | 10-100 | 学习调度决策 | NO |'
- en: '| Tuning | Database configuration [[123](#bib.bib123)] [[44](#bib.bib44)] |
    DDPG | 100 | 10 | Search configuration parameters interactively | YES |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 调优 | 数据库配置 [[123](#bib.bib123)] [[44](#bib.bib44)] | DDPG | 100 | 10 | 交互式搜索配置参数
    | YES |'
- en: '| Index | Index Selection [[84](#bib.bib84)] | CEM | 100 | 10 | Search the
    index interactively | NO |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 索引 | 索引选择 [[84](#bib.bib84)] | CEM | 100 | 10 | 交互式搜索索引 | NO |'
- en: '|  | R-tree construction [[24](#bib.bib24)] | DQN | 10-100 | 10 | Learn to
    generate the tree | NO |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | R树构建 [[24](#bib.bib24)] | DQN | 10-100 | 10 | 学习生成树 | NO |'
- en: '| Query Optimization | Join order selection [[61](#bib.bib61), [37](#bib.bib37),
    [119](#bib.bib119), [29](#bib.bib29)] | PG, DQN, … | 10-100 | 1-10 | Learn to
    decide the join order | Only [[29](#bib.bib29)] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 查询优化 | 连接顺序选择 [[61](#bib.bib61), [37](#bib.bib37), [119](#bib.bib119), [29](#bib.bib29)]
    | PG, DQN, … | 10-100 | 1-10 | 学习决定连接顺序 | 仅 [[29](#bib.bib29)] |'
- en: '| Cache Management | View Materialization [[121](#bib.bib121)] | DQN | 100
    | 10 | Model the problem as IIP and solve | NO |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 缓存管理 | 视图物化 [[121](#bib.bib121)] | DQN | 100 | 10 | 将问题建模为IIP并解决 | NO |'
- en: 3.2 Scheduling
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 调度
- en: Scheduling is a critical component in data processing and analytics systems
    to ensure that resources are well utilized. Job scheduling in a distributed computing
    cluster faces many challenging factors such as workload (e.g., job dependencies,
    sizes, priority), data locality, and hardware characteristics. Existing algorithms
    using general heuristics such as shortest-job-first do not utilize these factors
    well and fail to yield top performance. To this end, Mao et al.[[58](#bib.bib58)]
    propose Decima to learn to schedule jobs with dependent stages using DRL for data
    processing clusters and improve the job completion time. In the data processing
    systems such as Hive[[93](#bib.bib93)], Pig[[70](#bib.bib70)], Spark-SQL[[1](#bib.bib1)],
    jobs could have up to hundreds of stages and many stages run in parallel, which
    are represented as directed acyclic graphs (DAGs) where the nodes are the execution
    stages and each edge represents the dependency. To handle parallelism and dependencies
    in job DAGs, Decima first applies graph neural network (GNN) to extract features
    as the state instead of manually designing them while achieving scalability. Three
    types of feature embeddings are generated. Node embedding captures information
    about the node and its children including the number of remaining tasks, busy
    and available executors, duration, and locality of executors. Job embedding aggregates
    all node embeddings in the job and cluster embedding combines job embeddings.
    To balance possible large action space and long action sequences, The action determines
    the job stage to be scheduled next and the parallelism limit of executors. The
    reward is based on the average job completion time. To train effectively in a
    job streaming environment, Decima gradually increases the length of training jobs
    to conduct curriculum learning[[7](#bib.bib7)]. The variance reduction technique[[59](#bib.bib59)]
    is applied to handle stochastic job arrivals for robustness. However, we note
    that Decima is non-preemptive and does not re-schedule for higher priority jobs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 调度是数据处理和分析系统中的一个关键组成部分，以确保资源得到合理利用。分布式计算集群中的作业调度面临许多挑战因素，例如工作负载（如作业依赖、大小、优先级）、数据本地性和硬件特性。现有的算法使用诸如最短作业优先等通用启发式方法，没有很好地利用这些因素，导致性能不佳。为此，Mao等人[[58](#bib.bib58)]提出了Decima，利用深度强化学习（DRL）来学习调度具有依赖阶段的作业，以改善数据处理集群的作业完成时间。在数据处理系统中，如Hive[[93](#bib.bib93)]、Pig[[70](#bib.bib70)]、Spark-SQL[[1](#bib.bib1)]，作业可能有多达数百个阶段，许多阶段并行运行，这些都表示为有向无环图（DAGs），其中节点是执行阶段，每条边表示依赖关系。为了处理作业DAG中的并行性和依赖性，Decima首先应用图神经网络（GNN）来提取特征作为状态，而不是手动设计它们，同时实现可扩展性。生成了三种类型的特征嵌入。节点嵌入捕获关于节点及其子节点的信息，包括剩余任务数、忙碌和可用的执行器、持续时间以及执行器的本地性。作业嵌入汇总了作业中的所有节点嵌入，而集群嵌入则结合了作业嵌入。为了平衡可能的大行动空间和长的行动序列，行动决定了下一个要调度的作业阶段和执行器的并行限制。奖励基于平均作业完成时间。为了在作业流环境中有效训练，Decima逐渐增加训练作业的长度以进行课程学习[[7](#bib.bib7)]。应用了方差减少技术[[59](#bib.bib59)]来处理随机作业到达以提高鲁棒性。然而，我们注意到Decima是非抢占性的，并且不会为更高优先级的作业重新调度。
- en: 'In distributed stream data processing, streams of continuous data are processed
    at scale in a real-time manner. The scheduling algorithm assigns workers to process
    data where each worker uses many threads to process data tuples and aims to minimize
    average data tuple processing time. Li et al.[[45](#bib.bib45)] design a scheduling
    algorithm using DRL for distributed stream data processing, which learns to assign
    tuples to work threads. The state consists of the scheduling plan (e.g., the current
    assignment of workers) and the workload information (e.g., tuple arrival rate).
    The action is to assign threads to machines. The reward is the negative tuple
    processing time on average. The work shows that DQN does not work well because
    the action space is large and applies DDPG to train the actor-critic based agent
    instead. To find a good action, the proposed method looks for k nearest neighbors
    of the action that the actor network outputs and selects the neighbor with the
    highest value that the critic network outputs. The algorithm is implemented on
    Apache Storm and evaluated with representative applications: log stream processing,
    continuous queries, and word count.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式流数据处理领域，连续数据流以实时方式在规模上进行处理。调度算法将工作线程分配给数据处理任务，每个工作线程使用多个线程来处理数据元组，旨在最小化平均数据元组处理时间。Li等人[[45](#bib.bib45)]设计了一种使用深度强化学习（DRL）的调度算法，用于分布式流数据处理，该算法学习将元组分配给工作线程。状态包括调度计划（例如，当前的工作线程分配）和负载信息（例如，元组到达率）。行动是将线程分配给机器。奖励是平均负的元组处理时间。研究表明，DQN表现不佳，因为动作空间过大，因此改用DDPG训练基于演员-评论员的代理。为了找到好的行动，提出的方法寻找演员网络输出的动作的k个最近邻，并选择评论员网络输出值最高的邻居。该算法在Apache
    Storm上实现，并通过代表性应用进行评估：日志流处理、连续查询和词频统计。
- en: Many works have been recently proposed to improve scheduling using DRL[[122](#bib.bib122),
    [35](#bib.bib35)]. Query scheduling determines the execution order of queries,
    which has a great influence on query performance and resource utilization in the
    database system. SmartQueue[[122](#bib.bib122)] improves query scheduling by leveraging
    overlapping data access among queries and learns to improve cache hits using DRL.
    In addition, Tim et al.[[35](#bib.bib35)] design a scheduling system in SageDB
    using RL techniques. Other works using RL for scheduling include Bayesian RL for
    scheduling in heterogeneous clusters[[3](#bib.bib3)], operation scheduling in
    devices[[23](#bib.bib23)], application container scheduling in clusters[[102](#bib.bib102)],
    etc.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出了许多改进调度的工作，使用了DRL[[122](#bib.bib122)，[35](#bib.bib35)]。查询调度决定了查询的执行顺序，这对数据库系统中的查询性能和资源利用率有着重要影响。SmartQueue[[122](#bib.bib122)]通过利用查询之间的数据访问重叠来改进查询调度，并通过DRL学习提高缓存命中率。此外，Tim等人[[35](#bib.bib35)]在SageDB中设计了一种使用强化学习（RL）技术的调度系统。其他使用RL进行调度的工作包括异构集群中的贝叶斯RL调度[[3](#bib.bib3)]、设备中的操作调度[[23](#bib.bib23)]、集群中的应用容器调度[[102](#bib.bib102)]等。
- en: 3.3 Tuning
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 调优
- en: Tuning the configuration of data processing and analytic systems plays a key
    role to improve system performance. The task is challenging because up to hundreds
    of parameters and complex relations between them could exist. Furthermore, other
    factors such as hardware and workload also impact the performance. Existing works
    often employ search-based or supervised learning methods. The former takes much
    time to get an acceptable configuration and the latter such as OtterTune[[98](#bib.bib98)]
    needs large high-quality data that is non-trivial to obtain in practice. Zhang
    et al.[[123](#bib.bib123)] design a cloud database tuning system CDBTune using
    DRL to find the best parameter in high-dimensional configuration space. The CDBTune
    formulates MDP as follows. The state is represented by the internal metrics (e.g.,
    buffer size, pages read). The action is to increase or decrease the knob values.
    The reward is the performance difference between two states, which is calculated
    using throughput and latency. CDBTune takes several hours on offline training
    in simulation and online training in the real environment. Compared to OtterTune,
    CDBTune eases the burden of collecting large training data sets. In the experiments,
    CDBTune is shown to outperform DBA experts and OtterTune and improve tuning efficiency
    under 6 different workloads on four databases. One limitation of the approach
    is that the workload information is ignored and thus it may not perform well when
    the query workload is changed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 调整数据处理和分析系统的配置在提高系统性能中起着关键作用。这个任务很具挑战性，因为可能存在多达数百个参数以及它们之间的复杂关系。此外，硬件和工作负载等其他因素也会影响性能。现有的工作通常采用基于搜索或监督学习的方法。前者需要较长时间才能获得可接受的配置，而后者如
    OtterTune[[98](#bib.bib98)] 则需要大量高质量的数据，这在实践中不易获得。Zhang 等人[[123](#bib.bib123)]
    设计了一个使用 DRL 的云数据库调优系统 CDBTune，以在高维配置空间中找到最佳参数。CDBTune 将 MDP 公式化如下。状态由内部指标（例如，缓冲区大小、读取页面）表示。动作是增加或减少控制参数值。奖励是两个状态之间的性能差异，通过吞吐量和延迟计算。CDBTune
    在离线训练模拟和实际环境中的在线训练上需要几个小时。与 OtterTune 相比，CDBTune 减轻了收集大量训练数据集的负担。在实验中，CDBTune
    被证明优于 DBA 专家和 OtterTune，并在四个数据库的 6 种不同工作负载下提高了调优效率。该方法的一个局限性是忽略了工作负载信息，因此当查询工作负载发生变化时，可能表现不佳。
- en: To address the issue, Li et al.[[44](#bib.bib44)] propose QTune that considers
    query information to tune the database using DRL. First, Qtune extracts features
    from SQL query including types (e.g., insert, delete), tables, and operation (e.g.,
    scan, hash join) costs estimated by the database engine. The columns attributes
    and operations like selection conditions in the query are ignored. Subsequently,
    Qtune trains a DNN model to predict the difference of statistics (e.g., updated
    tuples, the number of committed transactions) in the state after executing the
    queries in the workload and updates the state using it. The action and reward
    design are similar to CDBTune. Additionally, QTune supports three levels of tuning
    granularity for balancing throughput and latency. For query-level, QTune inputs
    query vector and tries to find good knobs for each query. For workload-level,
    vectors for all queries are merged and used. For cluster-level, QTune employs
    a clustering method based on deep learning to classify queries and merge queries
    into clusters. One drawback of QTune is that the query featurization could lose
    key information such as query attributes (i.e., columns) and hurt the performance
    especially when the cost estimation is inaccurate. The prediction model for state
    changes is trained alone and needs accurate training data. An end-to-end training
    framework is therefore essential and a good direction to undertake.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Li 等人[[44](#bib.bib44)] 提出了 QTune，它利用 DRL 考虑查询信息来调优数据库。首先，Qtune 从 SQL
    查询中提取特征，包括类型（例如，插入、删除）、表以及数据库引擎估算的操作（例如，扫描、哈希连接）成本。查询中的列属性和操作如选择条件被忽略。随后，Qtune
    训练一个 DNN 模型来预测在执行工作负载中的查询后，状态中统计信息（例如，更新的元组、提交事务的数量）的差异，并使用这些信息更新状态。动作和奖励设计类似于
    CDBTune。此外，QTune 支持三种调优粒度级别，以平衡吞吐量和延迟。对于查询级别，QTune 输入查询向量，并尝试为每个查询找到合适的控制参数。对于工作负载级别，合并所有查询的向量并使用。对于集群级别，QTune
    使用基于深度学习的聚类方法来分类查询并将查询合并为簇。QTune 的一个缺点是查询特征化可能会丢失关键的信息，如查询属性（即列），并且在成本估算不准确时可能会影响性能。因此，准确的训练数据对于训练状态变化预测模型至关重要，因此端到端的训练框架是一个必要且值得探索的方向。
- en: 3.4 Indexing
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 索引
- en: 3.4.1 Database Index Selection
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 数据库索引选择
- en: Database index selection considers which attributes to create an index to maximize
    query performance. Sharma et al.[[84](#bib.bib84)] show how DRL can be used to
    recommend an index based on a given workload. The state encodes selectivity values
    for workload queries and columns in the database schema and current column indexes.
    The action is to create an index on a column. The reward is the improvement compared
    to the baseline without indexes. The experiments show that the approach can perform
    as well or better as having indexes on all columns. Sadri et al.[[78](#bib.bib78)]
    utilize DRL to select the index for a cluster database where both query processing
    and load balancing are considered. Welborn et al.[[110](#bib.bib110)] optimize
    the action space design by introducing task-specific knowledge for index selection
    tasks in the database. However, these works only consider the situation where
    single-column indexes are built. Lan et al.[[39](#bib.bib39)] propose both single-attribute
    and multi-attribute indexes selection using DRL. Five rules are proposed to reduce
    the action and state space, which help the agent learn effective strategy easier.
    The method uses what-if caller[[9](#bib.bib9)] to get the cost of queries under
    specific index configurations without building indexes physically. These works
    conduct basic experiments with small and simple datasets. Extensive and large-scale
    experiments using real datasets are therefore needed to benchmark these methods
    to ensure that they can scale well.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库索引选择考虑哪些属性需要创建索引以最大化查询性能。Sharma 等人[[84](#bib.bib84)] 展示了如何使用深度强化学习（DRL）根据给定的工作负载推荐索引。状态编码了工作负载查询和数据库模式中列的选择性值及当前列索引。动作是在某一列上创建索引。奖励是相较于没有索引的基线的改进。实验表明，该方法的性能与对所有列创建索引相当或更好。Sadri
    等人[[78](#bib.bib78)] 利用 DRL 选择集群数据库中的索引，同时考虑查询处理和负载均衡。Welborn 等人[[110](#bib.bib110)]
    通过为数据库中的索引选择任务引入任务特定知识来优化动作空间设计。然而，这些工作仅考虑了单列索引的情况。Lan 等人[[39](#bib.bib39)] 提出了使用
    DRL 进行单属性和多属性索引选择。提出了五条规则来减少动作和状态空间，这有助于代理更容易地学习有效策略。该方法使用 what-if caller[[9](#bib.bib9)]
    在特定索引配置下获取查询的成本，而无需实际构建索引。这些工作进行的实验都是基础的、使用小型简单数据集的。因此，需要使用真实数据集进行广泛的大规模实验，以基准测试这些方法，以确保它们能够良好扩展。
- en: 3.4.2 Index Structure Construction
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 索引结构构建
- en: The learned index is proposed recently as an alternative index to replace the
    B^+-Tree and bloom filter by viewing indexes as models and using deep learning
    models to act as indexes[[36](#bib.bib36)]. DRL can enhance the traditional indexes
    instead of replacing them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的学习索引作为替代索引被建议用来替换 B^+-树和布隆过滤器，通过将索引视为模型，并使用深度学习模型作为索引[[36](#bib.bib36)]。DRL
    可以增强传统索引，而不是替代它们。
- en: Hierarchical structures such as the B^+-tree and R-tree are important indexing
    mechanisms to locate data of interest efficiently without scanning a large portion
    of the database. Compared to the single dimensional counterpart, the R-tree is
    more complex to optimize due to bounding box efficiency and multi-path traversals.
    Earlier conventional approaches use heuristics to determine these two operations
    (i.e. choosing the insertion subtree and splitting an overflowing node) during
    the construction of the R-tree[[71](#bib.bib71)]. Gu et al.[[24](#bib.bib24)]
    propose to use DRL to replace heuristics to construct the R-tree and propose the
    RLR-tree. The approach models two operations ChooseSubtree and Split as two MDPs
    respectively and combines them to generate an R-Tree. For ChooseSubtree, the state
    is represented as the concatenation of the four features (i.e., area, perimeter,
    overlap, occupancy rate) of each selected child node. More features are evaluated
    but do not improve the performance in the reported experiments. The action is
    to select a node to insert from top-k child nodes in terms of the increase of
    area. The reward is the performance improvement from the RLR-tree. For Split MDP,
    the state is the areas and perimeters of the two nodes created by all top-k splits
    in the ascending order of total area. The action is to choose one split rule from
    k rules and the reward is similar to that of ChooseSubtree. The two agents are
    trained alternately. As expected, the optimizations render the RLR-tree improved
    performance in range and KNN queries.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 层级结构如B^+-树和R-tree是重要的索引机制，可以有效地定位感兴趣的数据，而无需扫描大量数据库。与单维度的对应结构相比，R-tree因边界框效率和多路径遍历而更难优化。早期的传统方法使用启发式算法来确定这两个操作（即选择插入子树和拆分溢出的节点）在构建R-tree的过程中[[71](#bib.bib71)]。Gu等人[[24](#bib.bib24)]提议使用DRL来替代启发式算法构建R-tree，并提出了RLR-tree。这种方法将两个操作ChooseSubtree和Split分别建模为两个MDP，并将它们结合起来生成一个R-Tree。对于ChooseSubtree，状态表示为每个选定子节点的四个特征（即面积、周长、重叠度、占用率）的串联。评估了更多特征，但在报告的实验中没有提高性能。动作是从前k个子节点中选择一个节点进行插入，以增加面积。奖励是RLR-tree的性能改进。对于Split
    MDP，状态是通过所有前k个拆分生成的两个节点的面积和周长，按总面积的升序排列。动作是从k个规则中选择一个拆分规则，奖励类似于ChooseSubtree。两个代理交替训练。正如预期的那样，这些优化使RLR-tree在范围查询和KNN查询中表现更佳。
- en: Graphs can be used as effective indexes to accelerate nearest neighbors search[[55](#bib.bib55),
    [15](#bib.bib15)]. Existing graph construction methods generally propose different
    rules to generate graphs, which cannot provide adaptivity for different workloads[[104](#bib.bib104)].
    Baranchuk et al.[[5](#bib.bib5)] employ DRL to optimize the graph for nearest
    neighbors search. The approach learns the probabilities of edges in the graph
    and tries to maximize the search efficiency. It considers the initial graph and
    the search algorithm as the state. The action is to keep an edge or not. The reward
    is the performance for search. It chooses the TRPO[[80](#bib.bib80)] algorithm
    to train. The reported experimental results show that the agent can refine state-of-the-art
    graphs and achieve better performance. However, this approach does not learn to
    explore and add new edges to the initial graph that may affect the performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以作为有效的索引来加速最近邻搜索[[55](#bib.bib55), [15](#bib.bib15)]。现有的图构建方法通常提出不同的规则来生成图，但不能为不同的工作负载提供适应性[[104](#bib.bib104)]。Baranchuk等人[[5](#bib.bib5)]使用DRL来优化图以进行最近邻搜索。这种方法学习图中边的概率，并试图最大化搜索效率。它将初始图和搜索算法视为状态。动作是保留边或不保留。奖励是搜索的性能。它选择TRPO[[80](#bib.bib80)]算法进行训练。报告的实验结果表明，代理能够细化最先进的图并实现更好的性能。然而，这种方法未能学习探索和添加可能影响性能的新边。
- en: Searching and constructing a new index structure is another line of interesting
    research [[33](#bib.bib33)]. Inspired by Neural Architecture Search (NAS)[[126](#bib.bib126)],
    Wu et al.[[112](#bib.bib112)] propose an RNN-based neural index search (NIS) framework
    that employs DRL to search the index structures and parameters given the workload.
    NIS can generate tree-like index structures layer by layer via formalizing abstract
    ordered blocks and unordered blocks, which can provide a well-designed search
    space. The keys in the ordered block are sorted in ascending order, and the skip
    list or B^+-Tree can be used. The keys in the unordered block are partitioned
    using customized functions and the hash bucket can be used. Overall, the whole
    learning process is similar to that of NAS.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索和构建新的索引结构是另一条有趣的研究方向[[33](#bib.bib33)]。受神经架构搜索（NAS）[[126](#bib.bib126)]的启发，吴等人[[112](#bib.bib112)]提出了一种基于RNN的神经索引搜索（NIS）框架，该框架利用DRL在给定工作负载的情况下搜索索引结构和参数。NIS可以通过形式化抽象有序块和无序块逐层生成树状索引结构，这可以提供一个设计良好的搜索空间。有序块中的键按升序排序，可以使用跳表或B^+-树。无序块中的键通过自定义函数进行分区，可以使用哈希桶。总体而言，整个学习过程类似于NAS。
- en: 3.5 Query Optimization
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 查询优化
- en: Query optimization aims to find the most efficient way to execute queries in
    database management systems. There are many different plans to access the query
    data that can have a large processing time variance from seconds to hours. The
    performance of a query plan is determined mostly by the table join orders. Traditionally,
    query optimizers use certain heuristics combined with dynamic programming to enumerate
    possible efficient execution plans and evaluate them using cost models that could
    produce large errors[[42](#bib.bib42)]. Marcus et al.[[61](#bib.bib61)] propose
    Rejoin that applies DRL to learn to select better join orders utilizing past experience.
    The state encodes join tree structure and join predicates. The action is to combine
    two subtrees, where each subtree represents an input relation to join. The reward
    is assigned based on the cost model in the optimizer. The experiments show that
    ReJOIN can match or outperform the optimizer in PostgreSQL. Compared to ReJoin,
    DQ[[37](#bib.bib37)] presents an extensible featurization scheme for state representation
    and improves the training efficiency using the DQN[[65](#bib.bib65)] algorithm.
    Heitz et al.[[29](#bib.bib29)] compare different RL algorithms including DQN[[65](#bib.bib65)],
    DDQN[[49](#bib.bib49)], and PPO[[81](#bib.bib81)] for join order optimization
    and use a symmetric matrix to represent the state instead of vector. Yu et al.[[119](#bib.bib119)]
    introduce a graph neural network (GNN) with DRL for join order selection that
    replaces fixed-length hand-tuned vector in Rejoin[[61](#bib.bib61)] and DQ[[37](#bib.bib37)]
    with learned scalable GNN representation and better captures and distinguishes
    the join tree structure information. These works mainly differ in encoding what
    information and how to encode them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 查询优化旨在寻找在数据库管理系统中执行查询的最有效方式。针对查询数据有许多不同的访问计划，这些计划的处理时间差异可以从秒到小时不等。查询计划的性能主要取决于表连接顺序。传统上，查询优化器使用某些启发式方法结合动态规划来枚举可能的高效执行计划，并使用成本模型进行评估，这些模型可能会产生较大的误差[[42](#bib.bib42)]。Marcus等人[[61](#bib.bib61)]提出了Rejoin，它应用DRL来学习选择更好的连接顺序，利用过去的经验。状态编码连接树结构和连接谓词。动作是合并两个子树，其中每个子树表示一个输入关系。奖励基于优化器中的成本模型分配。实验表明，ReJOIN可以与PostgreSQL中的优化器匹敌或超越。与ReJoin相比，DQ[[37](#bib.bib37)]提出了一种可扩展的特征化方案用于状态表示，并使用DQN[[65](#bib.bib65)]算法提高了训练效率。Heitz等人[[29](#bib.bib29)]比较了包括DQN[[65](#bib.bib65)]、DDQN[[49](#bib.bib49)]和PPO[[81](#bib.bib81)]在内的不同RL算法进行连接顺序优化，并使用对称矩阵来表示状态，而不是向量。Yu等人[[119](#bib.bib119)]引入了一个结合DRL的图神经网络（GNN）用于连接顺序选择，用学到的可扩展GNN表示替代了Rejoin[[61](#bib.bib61)]和DQ[[37](#bib.bib37)]中的固定长度手动调整向量，并更好地捕捉和区分连接树结构信息。这些工作主要在于编码哪些信息以及如何编码它们。
- en: Instead of learning from past query executions, Trummer et al.[[94](#bib.bib94)]
    propose SkinnerDB to learn from the current query execution status to optimize
    the remaining execution of a query using RL. Specifically, SkinnerDB breaks the
    query execution into many small time intervals (e.g., tens to thousands of slices
    per second) and processes the query adaptively. At the beginning of each time
    interval, the RL agent chooses the join order and measures the execution progress.
    SkinnerDB adopts a similar adaptive query processing strategy in Eddies[[95](#bib.bib95)]
    and uses the UCT algorithm[[34](#bib.bib34)], which provides formal guarantees
    that the difference is bounded between the rewards obtained by the agent and those
    by optimal choices. The reward is calculated by the progress for the current interval.
    A tailored execution engine is designed to fully exploit the learning strategy
    with tuple representations and specialized multi-way join algorithms. SkinnerDB
    offers several advantages. First, it is inherently robust to query distribution
    changes because its execution only depends on the current query. Second, it relies
    on less assumption and information (e.g., cardinality models) than traditional
    optimizers and thus is more suitable for the complicated environment where cardinality
    is hard to estimate. Third, it predicts the optimal join order based on real performance.
    However, it may introduce overhead caused by join order switching.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Trummer 等人[[94](#bib.bib94)]提出了SkinnerDB，通过从当前查询执行状态中学习，使用强化学习优化查询的剩余执行。具体而言，SkinnerDB将查询执行拆分为许多小的时间间隔（例如，每秒几十到上千个切片），并对查询进行自适应处理。在每个时间间隔的开始，强化学习代理选择连接顺序并测量执行进度。SkinnerDB采用了类似于Eddies[[95](#bib.bib95)]的自适应查询处理策略，并使用UCT算法[[34](#bib.bib34)]，该算法提供了形式化的保证，代理获得的奖励与最佳选择之间的差异是有界的。奖励通过当前间隔的进度进行计算。设计了一种量身定制的执行引擎，以充分利用带有元组表示的学习策略和专门的多路连接算法。SkinnerDB提供了几个优势。首先，它对查询分布变化具有内在的鲁棒性，因为其执行仅依赖于当前查询。其次，它比传统优化器依赖的假设和信息（例如基数模型）更少，因此更适合基数难以估计的复杂环境。第三，它基于实际性能预测最佳连接顺序。然而，它可能引入由于连接顺序切换带来的开销。
- en: Learning-based methods that have been proposed to replace traditional query
    optimizers often incur a great deal of training overhead because they have to
    learn from scratch. To mitigate the problem, Bao [[60](#bib.bib60)] (the Bandit
    optimizer)) is designed to take advantage of the existing query optimizers. Specifically,
    Bao learns to choose the best plan from the query plan candidates provided by
    available optimizers by passing different flags or hints to them. Bao transforms
    query plan trees into vectors and adopts a tree convolutional neural network to
    identify patterns in the tree. Then it formulates the choosing task as a contextual
    multi-armed bandit problem and uses Thompson sampling[[92](#bib.bib92)] to solve
    it. Bao is a hybrid solution for query optimization. It achieves good training
    time and is robust to changes in workload [[60](#bib.bib60)].
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的基于学习的方法通常需要较高的训练开销，因为它们必须从零开始学习。为了缓解这一问题，Bao [[60](#bib.bib60)]（Bandit优化器）被设计为利用现有的查询优化器。具体而言，Bao通过向现有优化器传递不同的标志或提示，学习从查询计划候选中选择最佳计划。Bao将查询计划树转换为向量，并采用树卷积神经网络来识别树中的模式。然后，它将选择任务表述为一个上下文多臂赌博问题，并使用Thompson采样[[92](#bib.bib92)]来解决。Bao是一个查询优化的混合解决方案。它在训练时间上表现良好，并且对工作负载的变化具有鲁棒性[[60](#bib.bib60)]。
- en: 'Table 2: Methods of query optimization.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 查询优化的方法。'
- en: '| Method | Techniques | Training | Workload Adaptivity |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 技术 | 训练 | 工作负载适应性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Rejoin[[61](#bib.bib61)], DQ[[37](#bib.bib37)] | learn from execution experience
    | High | Low |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Rejoin[[61](#bib.bib61)], DQ[[37](#bib.bib37)] | 从执行经验中学习 | 高 | 低 |'
- en: '| SkinnerDB [[94](#bib.bib94)] | learn from current execution status | Medium
    | Medium |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SkinnerDB [[94](#bib.bib94)] | 从当前执行状态中学习 | 中等 | 中等 |'
- en: '| Bao[[60](#bib.bib60)] | learn to choose existing optimizers | Low | High
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Bao[[60](#bib.bib60)] | 学习选择现有优化器 | 低 | 高 |'
- en: 3.6 Cache Management
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 缓存管理
- en: 3.6.1 View Materialization
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 视图物化
- en: View materialization is the process of deciding which view, i.e., results of
    query or subquery, to cache. In database systems, a view is represented as a table
    and other queries could be accelerated by reading this table instead of accessing
    the original tables. There is an overhead of materializing and maintaining the
    view when the original table is updated. Existing methods are based on heuristics,
    which either rely on simple Least-Recently-Used rule or cost-model based approaches[[74](#bib.bib74)].
    The performance of these approaches is limited because feedback from the historical
    performance of view materialization is not incorporated. Liang et al.[[48](#bib.bib48)]
    implement Deep Q-Materialization (DQM) system that leverages DRL to improve the
    view materialization process in the OLAP system. First, DQM analyzes SQL queries
    to find candidate views for the current query. Second, it trains a DRL agent to
    select from the set of candidates. Third, it uses an eviction policy to delete
    the materialized views. In the MDP, the state encodes view state and workload
    information. The action is to create the view or do nothing. The reward is calculated
    by the query time improvement minus amortized creation cost. Additionally, the
    eviction policy is based on credit and it evicts the materialized view with the
    lowest score.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 视图物化是决定缓存哪个视图（即查询或子查询的结果）的过程。在数据库系统中，视图被表示为表格，其他查询可以通过读取该表来加速，而不是访问原始表。原始表更新时，物化和维护视图会有一定的开销。现有方法基于启发式规则，这些规则要么依赖简单的最近最少使用（Least-Recently-Used）规则，要么基于成本模型的方法[[74](#bib.bib74)]。这些方法的性能受到限制，因为它们没有纳入视图物化的历史性能反馈。梁等人[[48](#bib.bib48)]实现了深度
    Q-物化（DQM）系统，该系统利用深度强化学习（DRL）来改进OLAP系统中的视图物化过程。首先，DQM分析SQL查询以寻找当前查询的候选视图。其次，它训练一个DRL代理，从候选集中过滤选择。第三，它使用驱逐策略删除已物化的视图。在马尔可夫决策过程（MDP）中，状态编码视图状态和工作负载信息。动作是创建视图或什么都不做。奖励是通过查询时间改进减去摊销创建成本来计算的。此外，驱逐策略基于信用，驱逐得分最低的已物化视图。
- en: Yuan et al.[[121](#bib.bib121)] present a different way that use DRL to automate
    view generation and select the most beneficial subqueries to materialize. First,
    the approach uses a DNN to estimate the benefits of a materialized view where
    features from tables, queries, and view plans are extracted. Then the approach
    models selection as an Integer Linear Programming (IIP) problem and introduce
    an iterative optimization method to figure it out. However, the method cannot
    guarantee convergence. To address the issue, the problem is formulated as the
    MDP. The state encodes the subqueries that are selected to materialize and status
    if queries use these materialized views. The action is to choose the subquery
    to materialize or not. The reward is the difference between benefit changes of
    two states. Both cost estimation and view selection models are trained offline
    using the actual cost of queries and benefits. Then the cost estimation model
    is used for the online recommendation for view materialization. Performance study
    shows its good performance; However, it lacks a comparison with DQM.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 袁等人[[121](#bib.bib121)]提出了一种不同的方法，使用DRL来自动生成视图，并选择最有利的子查询进行物化。首先，该方法使用深度神经网络（DNN）来估计物化视图的收益，从中提取表格、查询和视图计划的特征。然后，该方法将选择建模为整数线性规划（ILP）问题，并引入迭代优化方法来解决。然而，该方法无法保证收敛。为了解决这个问题，该问题被公式化为MDP。状态编码为选择进行物化的子查询以及查询是否使用这些物化视图。动作是选择是否物化子查询。奖励是两个状态收益变化的差异。成本估计和视图选择模型均使用查询的实际成本和收益进行离线训练。然后，成本估计模型用于视图物化的在线推荐。性能研究表明其表现良好；然而，它缺乏与DQM的比较。
- en: 3.6.2 Storage
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 存储
- en: Cache management impacts the performance of computer systems with hierarchical
    hardware structures directly. Generally, a caching policy considers which objects
    to cache, to evict when the cache is full to maximize the object hit rate in the
    cache. In many systems, the optimal caching policy depends on workload characteristics.
    Phoebe[[111](#bib.bib111)] is the RL-based framework for cache management for
    storage models. The state encodes the information from a preceding fixed-length
    sequence of accesses where for each access, nine features are extracted including
    data block address, data block address delta, frequency, reuse distance, penultimate
    reuse distance, average reuse distance, frequency in the sliding window, the number
    of cache misses, and a priority value. The action is to set a priority value ranging
    within $[-1,1]$ to the data. The reward is computed from if the cache is hit or
    missed and values are 1 and -1 respectively. It applies the DDPG algorithm to
    train the agent. Periodical training is employed to amortize training costs in
    online training. In network systems, one issue is that the reward delay is very
    long in systems with a large cache, i.e., CDN cache can host up to millions of
    objects. Wang et al.[[100](#bib.bib100)] propose a subsampling technique by hashing
    the objects to mitigate the issue when applying RL on caching systems.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存管理直接影响具有层次硬件结构的计算机系统的性能。一般来说，缓存策略考虑缓存哪些对象，当缓存满时驱逐哪些对象，以最大化缓存中的命中率。在许多系统中，最优缓存策略依赖于工作负载特征。Phoebe[[111](#bib.bib111)]是一个基于RL的缓存管理框架，用于存储模型。状态编码了来自前一个固定长度访问序列的信息，每个访问提取了九个特征，包括数据块地址、数据块地址增量、频率、重用距离、倒数第二次重用距离、平均重用距离、滑动窗口中的频率、缓存未命中的次数和优先级值。动作是为数据设置一个范围在$[-1,1]$内的优先级值。奖励从缓存是否命中计算，值分别为1和-1。它应用DDPG算法训练代理。周期性训练用于摊销在线训练的成本。在网络系统中，一个问题是大缓存系统中的奖励延迟非常长，即CDN缓存可以托管多达数百万个对象。Wang等[[100](#bib.bib100)]提出了一种通过对对象进行哈希的子采样技术，以缓解在缓存系统上应用RL时的问题。
- en: 4 Data Analytics Applications
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据分析应用
- en: 'Table 3: Representative works for RL applications. D(X) denotes the approximate
    dimension of X space.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: RL应用的代表性工作。D(X)表示X空间的大致维度。'
- en: '| Domain | Work | Algorithm | D(State) | D(Action) | DRL-based Approach |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 工作 | 算法 | D(State) | D(Action) | 基于DRL的方法 |'
- en: '| Data processing | Entity matching[[11](#bib.bib11), [20](#bib.bib20)] | PG
    | 100 - 1000 | 100 - 1000 | Select target entity from the candidate entities |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 数据处理 | 实体匹配[[11](#bib.bib11), [20](#bib.bib20)] | PG | 100 - 1000 | 100 -
    1000 | 从候选实体中选择目标实体 |'
- en: '| application | Database interaction with natural language [[125](#bib.bib125),
    [14](#bib.bib14)] | PG | 100 - 1000 | 100 - 1000 | Learn to generate the query
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 用自然语言进行数据库交互[[125](#bib.bib125), [14](#bib.bib14)] | PG | 100 - 1000
    | 100 - 1000 | 学习生成查询 |'
- en: '|  | Feature engineering [[52](#bib.bib52)] | DQN | 100 | 1-10 | Select features
    and model feature correlations in states |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | 特征工程[[52](#bib.bib52)] | DQN | 100 | 1-10 | 选择特征并建模状态下的特征相关性 |'
- en: '|  | Exploratory data analysis [[4](#bib.bib4)] | A3C | 10-100 | 100000 | Learn
    to query a dataset for key characteristics |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | 探索性数据分析[[4](#bib.bib4)] | A3C | 10-100 | 100000 | 学习查询数据集的关键特征 |'
- en: '|  | Abnormal detection [[69](#bib.bib69)] | IRL | 1-10 | 1-10 | Learn the
    reward function for normal sequences |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | 异常检测[[69](#bib.bib69)] | IRL | 1-10 | 1-10 | 学习正常序列的奖励函数 |'
- en: '|  | AutoML pipeline generation [[28](#bib.bib28)] | DQN | 10 | 100 | Learn
    to select modules of a pipeline |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | AutoML管道生成[[28](#bib.bib28)] | DQN | 10 | 100 | 学习选择管道模块 |'
- en: '| Healthcare | Treatment recommendation [[103](#bib.bib103)] | DDPG | 10 |
    100-1000 | Select treatment from candidate treatments |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 医疗保健 | 治疗推荐[[103](#bib.bib103)] | DDPG | 10 | 100-1000 | 从候选治疗中选择治疗方案 |'
- en: '|  | Diagnostic inference [[51](#bib.bib51)] | DQN | 100-1000 | 1-10 | Learn
    diagnostic decision |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 诊断推理[[51](#bib.bib51)] | DQN | 100-1000 | 1-10 | 学习诊断决策 |'
- en: '|  | Hospital resource allocation [[19](#bib.bib19)] | DDPG | 100 | 1000-10000
    | Learn resource scheduling |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | 医院资源分配[[19](#bib.bib19)] | DDPG | 100 | 1000-10000 | 学习资源调度 |'
- en: '| Fintech | Portfolio optimization [[12](#bib.bib12)] | Q-Learning | 100 |
    100 | Select the portfolio weights for stocks |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 金融科技 | 投资组合优化[[12](#bib.bib12)] | Q-Learning | 100 | 100 | 选择股票的投资组合权重 |'
- en: '|  | Trading [[115](#bib.bib115), [114](#bib.bib114)] | IRL | 1-10 | 10 | Learn
    the reward function of trading behaviors |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 交易[[115](#bib.bib115), [114](#bib.bib114)] | IRL | 1-10 | 10 | 学习交易行为的奖励函数
    |'
- en: '|  | Fraud detection [[114](#bib.bib114)] | IRL | 100 | 10-100 | Learn the
    reward function of trading behaviors |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | 欺诈检测 [[114](#bib.bib114)] | IRL | 100 | 10-100 | 学习交易行为的奖励函数 |'
- en: '| E- | Online advertising [[124](#bib.bib124)] | DQN | 1-10 | 1-10 | Learn
    to schedule the advertisements |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| E- | 在线广告 [[124](#bib.bib124)] | DQN | 1-10 | 1-10 | 学习调度广告 |'
- en: '| Commerce | Online recommendation [[10](#bib.bib10)] | DQN | 100 | 10000 |
    Learn to schedule recommendations |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 商业 | 在线推荐 [[10](#bib.bib10)] | DQN | 100 | 10000 | 学习调度推荐 |'
- en: '|  | Search results aggregation [[91](#bib.bib91)] | DQN | 10-100 | 10-100
    | Learn to schedule search results |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 搜索结果聚合 [[91](#bib.bib91)] | DQN | 10-100 | 10-100 | 学习调度搜索结果 |'
- en: '| Others | User profiling [[105](#bib.bib105)] | DQN | 100-1000 | 1000-10000
    | Select users’ next activities by modeling spatial semantics |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 用户画像 [[105](#bib.bib105)] | DQN | 100-1000 | 1000-10000 | 通过建模空间语义选择用户的下一步活动
    |'
- en: '|  | Spammer detection [[16](#bib.bib16)] | PG | 100 | 100 | Search for the
    detector by interacting with spammers |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 垃圾邮件检测 [[16](#bib.bib16)] | PG | 100 | 100 | 通过与垃圾邮件发送者互动来寻找检测器 |'
- en: '|  | Transportation [[83](#bib.bib83)] | PG | 1000-10000 | 1000 | Learn to
    schedule transportation |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 交通运输 [[83](#bib.bib83)] | PG | 1000-10000 | 1000 | 学习调度交通运输 |'
- en: In this section, we shall discuss DRL applications from the perspective of data
    processing and data analytics. These two categories of DRL applications form indispensable
    parts of a pipeline, in which data processing provides a better basis for data
    analytics. In addition, these two categories share some overlapping topics, making
    these topics mutually motivating and stimulating. We have summarized the technical
    comparisons of different applications in Table [3](#S4.T3 "Table 3 ‣ 4 Data Analytics
    Applications ‣ A Survey on Deep Reinforcement Learning for Data Processing and
    Analytics"). We shall first discuss DRL applications in data preparation and then
    in data analytics.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从数据处理和数据分析的角度讨论深度强化学习（DRL）的应用。这两类DRL应用构成了一个管道中不可或缺的部分，其中数据处理为数据分析提供了更好的基础。此外，这两类应用共享一些重叠的主题，使这些主题相互激励和刺激。我们在表
    [3](#S4.T3 "Table 3 ‣ 4 Data Analytics Applications ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") 中总结了不同应用的技术比较。我们将首先讨论数据准备中的DRL应用，然后讨论数据分析中的DRL应用。
- en: 4.1 Data Preparation
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据准备
- en: 4.1.1 Entity Matching
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 实体匹配
- en: Entity matching is a data cleaning task that aligns different mentions of the
    same entity in the context. Clark et al. [[11](#bib.bib11)] identify the issue
    that the heuristic loss function cannot effectively optimize the evaluation metric
    $B^{3}$, and propose using reinforcement learning to directly optimize the metric.
    The problem is formulated as a sequential decision problem where each action is
    performed on one mention of a document. The action maps the mention to an entity
    in the database at each step by a mention ranking model. Then the reward is calculated
    using the evaluation metric $B^{3}$. This work originally proposes scaling each
    action’s weight by measuring its impact on the final reward since each action
    is independent. However, this work does not consider the global relations between
    entities. Fang et al. [[20](#bib.bib20)] propose a reinforcement learning framework
    based on the fact that an easier entity will create a better context for the subsequent
    entity matching. Specifically, both local and global representations of entity
    mentions are modeled and a learned policy network is devised to choose from the
    next action (i.e., which entity to recognize). However, the selection of the easier
    entity to learn the context could be less powerful than context modeling with
    more recent techniques in NLP such as the transformer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 实体匹配是一项数据清洗任务，用于在上下文中对齐相同实体的不同提及。Clark 等人 [[11](#bib.bib11)] 识别出启发式损失函数无法有效优化评估指标
    $B^{3}$ 的问题，并提出使用强化学习直接优化该指标。该问题被表述为一个序贯决策问题，其中每个动作都是在文档的一个提及上执行的。该动作通过提及排名模型在每一步将提及映射到数据库中的一个实体。然后使用评估指标
    $B^{3}$ 计算奖励。这项工作最初提出通过测量每个动作对最终奖励的影响来缩放每个动作的权重，因为每个动作是独立的。然而，这项工作并未考虑实体之间的全局关系。Fang
    等人 [[20](#bib.bib20)] 提出了一个基于强化学习的框架，基于一个事实：更容易的实体会为后续的实体匹配创建更好的上下文。具体来说，模型化了实体提及的局部和全局表示，并设计了一个学习的策略网络来选择下一步动作（即，识别哪个实体）。然而，选择更容易的实体来学习上下文可能不如使用NLP中的更先进技术（如变压器）进行上下文建模更为有效。
- en: 4.1.2 Database Interaction With Natural Language
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 数据库与自然语言的交互
- en: To facilitate query formulation for relational databases, there have been efforts
    in generating SQL queries from various other means that do not require knowledge
    of SQL and schema. Zhong et al. [[125](#bib.bib125)] propose to generate SQL from
    a natural language using Reinforcement Learning. For queries formed by a natural
    language, the model Seq2SQL will learn a policy transforming the queries into
    SQL queries. The transformed queries will then be executed in the database system
    to get results. The results will be compared with the ground truth to generate
    RL rewards. Earlier work [[14](#bib.bib14)] using generic autoencoder model for
    semantic parsing with Softmax as the final layer may generate unnecessarily large
    output spaces for SQL query generation tasks. Thus the structure of SQL is used
    to prune the output space of query generating and policy-based reinforcement learning
    to optimize the part which cannot be optimized by cross-entropy. However, RL is
    observed to have limited performance enhancement by [[113](#bib.bib113)] due to
    unnecessary modeling of query serialization.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便关系型数据库的查询构造，已经有一些工作致力于从各种其他方式生成 SQL 查询，这些方式不需要了解 SQL 和模式。钟等人[[125](#bib.bib125)]
    提出了使用强化学习从自然语言生成 SQL。对于由自然语言构造的查询，模型 Seq2SQL 将学习一种将查询转化为 SQL 查询的策略。然后，转化后的查询将在数据库系统中执行以获取结果。这些结果将与真实结果进行比较，以生成
    RL 奖励。早期的工作[[14](#bib.bib14)] 使用通用的自编码器模型进行语义解析，并以 Softmax 作为最终层，可能会为 SQL 查询生成任务产生不必要的大输出空间。因此，利用
    SQL 的结构来修剪查询生成的输出空间，并基于策略的强化学习来优化那些无法通过交叉熵优化的部分。然而，[[113](#bib.bib113)] 观察到 RL
    由于不必要的查询序列化建模，其性能提升有限。
- en: Efficiently querying a database of documents is a promising data processing
    application. Karthik et al. [[67](#bib.bib67)] propose collecting evidence from
    external sources of documents to boost extraction accuracy to original sources
    where data might be scarce. The problem is formulated as an MDP problem, where
    each step the agent needs to decide if current extracted articles are accepted
    and stop querying, or these articles are rejected and more relevant articles are
    queried. Both data reconciliation (from original sources) and data retrieval (from
    external sources) are represented as states. Extraction accuracy and penalties
    for extra retrieval actions are reflected in the reward function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 高效查询文档数据库是一个有前途的数据处理应用。Karthik 等人[[67](#bib.bib67)] 提出了从外部文档来源收集证据，以提高对数据可能稀缺的原始来源的提取准确性。这个问题被表述为一个
    MDP 问题，其中每一步代理需要决定是否接受当前提取的文章并停止查询，或者这些文章被拒绝并且查询更多相关的文章。数据对账（来自原始来源）和数据检索（来自外部来源）被表示为状态。提取准确性和额外检索操作的惩罚在奖励函数中体现。
- en: 4.1.3 Feature Engineering
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 特征工程
- en: 'Feature engineering can be formulated as a single-agent reinforcement learning
    problem to search for an optimal subset of features in a large space: the agent
    selects one feature at each action step. The state is the current feature subspace.
    A reward is assigned to the agent based on the predictive performance of the current
    features subset. Liu et al. [[52](#bib.bib52)] propose a method to reformulate
    feature engineering as a multi-agent reinforcement learning problem. The multi-agent
    RL formulation reduces the large action space of a single agent since now each
    of the agents has a smaller action space for one feature selection. However, this
    formulation also brings challenges: interactions between agents, representation
    of the environment, and selection of samples. Three technical methods in [[52](#bib.bib52)]
    have been proposed to tackle them respectively: adding inter-feature information
    to reward formulation, using meta statistics, and deep learning methods to learn
    the representation of the environment, and Gaussian mixture to independently determine
    samples. However, although this formulation reduces the action space, the trade-off
    is using more computing resources to support more agents’ learning. Also, the
    method is difficult to scale to a large feature space.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以被表述为单一智能体强化学习问题，以在大空间中搜索最佳特征子集：智能体在每一步动作中选择一个特征。状态是当前的特征子空间。根据当前特征子集的预测性能为智能体分配奖励。刘等人[[52](#bib.bib52)]提出了一种将特征工程重新表述为多智能体强化学习问题的方法。多智能体RL（强化学习）表述减少了单一智能体的大动作空间，因为现在每个智能体对于特征选择有一个更小的动作空间。然而，这种表述也带来了挑战：智能体之间的交互、环境的表征和样本的选择。在[[52](#bib.bib52)]中提出了三种技术方法来分别解决这些问题：将特征间信息添加到奖励表述中，使用元统计和深度学习方法来学习环境的表征，以及使用高斯混合模型来独立确定样本。然而，尽管这种表述减少了动作空间，但代价是需要更多的计算资源来支持更多智能体的学习。此外，该方法难以扩展到大型特征空间。
- en: 4.1.4 Exploratory Data Analysis
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 探索性数据分析
- en: Exploratory data analysis (EDA) is useful for users to understand the characteristics
    of a new dataset. In [[4](#bib.bib4)], the problem is formulated as a MDP. The
    action space is the combination of a finite set of operators and their corresponding
    parameters to query a dataset. The result of a query shows the characteristics
    of the dataset. The characteristics are modeled as the state, which is represented
    by descriptive statistics and recent operators. The reward signal measures the
    interestingness, diversity, and coherency of the characteristics by an episode
    of EDA operations. DRL is applied to the non-differential signals and discrete
    states in MDP. However, challenges arise when applying deep reinforcement learning
    given a large number of possible actions as parameterized operations (i.e., for
    each type of operation, the corresponding possible action is the Cartesian product
    of all parameters’ possible values). In [[4](#bib.bib4)], a two-fold layer architecture
    is proposed to replace a global softmax layer into two local layers, which effectively
    reduces the intractable large numbers of actions. However, the global interactions
    of operations and attributes are not considered.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）对于用户理解新数据集的特征非常有用。在[[4](#bib.bib4)]中，问题被表述为MDP（马尔科夫决策过程）。动作空间是有限操作集及其相应参数的组合，用于查询数据集。查询的结果展示了数据集的特征。这些特征被建模为状态，由描述性统计和最新操作表示。奖励信号通过一系列EDA操作来衡量特征的趣味性、多样性和连贯性。DRL（深度强化学习）应用于MDP中的非差分信号和离散状态。然而，当给定大量可能的操作作为参数化操作（即每种操作的对应可能动作是所有参数可能值的笛卡尔积）时，应用深度强化学习会面临挑战。在[[4](#bib.bib4)]中，提出了一种双层架构，将全局softmax层替换为两个局部层，从而有效减少了难以处理的大量动作。然而，这种方法并未考虑操作和属性的全局交互。
- en: 4.1.5 Abnormal Detection
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 异常检测
- en: Abnormal detection is important for high-stake applications such as healthcare
    (e.g., predicting patients’ status) and fintech (e.g., financial crime). Based
    on the assumptions, there are two approaches to this problem. One approach models
    the dynamics in the unlabeled datasets as a sequential decision process where
    the agent performs an action on each observation. Oh et al. [[69](#bib.bib69)]
    propose to use IRL to learn a reward function and a Bayesian network to estimate
    a confidence score for a potential abnormal observation. To achieve this, the
    prior distribution of the reward function is assumed. Then a reward function is
    sampled from the distribution to determine the sample generating policy, which
    generates sample background trajectories. As explained by the reward part of Section
    [2.3.2](#S2.SS3.SSS2 "2.3.2 Model Efficiency ‣ 2.3 Advanced Techniques ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"), experts’ trajectories are observed.
    With these experts’ trajectories and sample background trajectories, the parameters
    of the reward function are updated and thus the policy is improved. The sequence
    of actions is the input into the neural network. This network is trained to learn
    the normal pattern of a targeted agent and to predict if the next observation
    is abnormal or not. However, this approach relies too much on mining unlabeled
    datasets and ignores the labeled dataset. To address this issue, another approach
    also uses DRL but focus on the Exploit-Explore trade-off on both unlabeled and
    labeled dataset. Pang et al. [[73](#bib.bib73)] propose a DRL model with a sampling
    function to select data instances from both the unlabeled and labeled dataset.
    This sampling function helps the DRL model to exploit the scarce but useful labeled
    anomaly data instances and to explore the large unlabeled dataset for novel anomaly
    data instances. Thus, more anomaly data instances are selected to train the DRL
    model with better model capacity.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测在高风险应用中非常重要，例如医疗保健（例如，预测患者的状态）和金融科技（例如，金融犯罪）。根据假设，这个问题有两种方法。一种方法将未标记数据集中的动态建模为一个序列决策过程，其中代理在每次观察时执行一个动作。Oh
    等人 [[69](#bib.bib69)] 提议使用 IRL 学习奖励函数，并使用贝叶斯网络来估计潜在异常观察的置信度分数。为此，假设奖励函数的先验分布。然后从该分布中抽样奖励函数，以确定样本生成策略，该策略生成样本背景轨迹。正如第
    [2.3.2](#S2.SS3.SSS2 "2.3.2 Model Efficiency ‣ 2.3 Advanced Techniques ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") 节奖励部分所解释的，专家轨迹被观察。通过这些专家轨迹和样本背景轨迹，更新奖励函数的参数，从而改进策略。动作序列作为输入进入神经网络。该网络经过训练，以学习目标代理的正常模式，并预测下一个观察是否异常。然而，这种方法过于依赖挖掘未标记数据集，而忽视了标记数据集。为了解决这个问题，另一种方法也使用
    DRL，但关注于未标记和标记数据集上的利用-探索权衡。Pang 等人 [[73](#bib.bib73)] 提出了一种 DRL 模型，具有从未标记和标记数据集中选择数据实例的采样功能。该采样功能帮助
    DRL 模型利用稀缺但有用的标记异常数据实例，并探索大量未标记数据集中的新颖异常数据实例。因此，选择更多异常数据实例来训练 DRL 模型，从而提升模型能力。
- en: 4.1.6 AutoML Pipeline Generation
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 AutoML 流水线生成
- en: Pipeline generation includes generating all data processing and analytics steps
    or modules to perform ML tasks. Heffetz et al. [[28](#bib.bib28)] propose a grid-world
    to represent all possible families of each step of a data pipeline as cells and
    connect all possible cells as a graph. Subsequently, a hierarchical method is
    used to reduce the space of all actions and represent all actions by layers of
    clusters. Finally, the state representations are inputs to the value sub-network
    in a DQN network, and action representations are inputs to evaluate the advantage-to-average
    sub-network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线生成包括生成所有数据处理和分析步骤或模块以执行机器学习任务。Heffetz 等人 [[28](#bib.bib28)] 提出使用网格世界来表示数据流水线每一步的所有可能的家族作为单元，并将所有可能的单元连接成图形。随后，采用分层方法来减少所有操作的空间，并通过簇的层次来表示所有操作。最后，状态表示作为输入进入
    DQN 网络中的价值子网络，而动作表示作为输入用于评估优势与平均子网络。
- en: 4.2 Healthcare
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 医疗保健
- en: 'Healthcare analytics has gained increasing attention in tandem with the advancement
    of healthcare treatment and availability of medical data and computational capacity
    [[41](#bib.bib41)]. Naturally, a great amount of effort has been spent on applying
    DRL to healthcare. As before, implementing DRL-based models in healthcare requires
    the understanding of the application context and defining the key elements of
    MDP. However, differences occur in the approaches to learning better decisions:
    learning the motivation of expert decisions by IRL, learning better decisions
    without an expert by interacting with an environment or interacting with an environment
    with expert decisions as supervising signals.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 随着医疗治疗的进步以及医疗数据和计算能力的增加，医疗分析受到了越来越多的关注 [[41](#bib.bib41)]。自然，大量的努力被投入到将DRL应用于医疗领域。与以往一样，在医疗领域实施基于DRL的模型需要理解应用背景并定义MDP的关键要素。然而，在学习更好决策的方法上存在差异：通过IRL学习专家决策的动机、通过与环境互动学习更好的决策，或通过将专家决策作为监督信号与环境互动。
- en: 4.2.1 Treatment Recommendation
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 治疗推荐
- en: Treatment recommendation systems are designed to assist doctors to make better
    decisions based on electronic health records. However, the doctors’ prescriptions
    are not ground truth but valuable suggestions for high stake medical cases. The
    ground truth is the delayed condition of the patients. Thus model predictions
    must not deviate from the doctors’ judgments too much, and not use those judgments
    as true labels. To tackle this challenge, Wang et al. [[103](#bib.bib103)] propose
    an architecture to combine supervised learning and reinforcement learning. This
    model reduces the inconsistency between indicator signals learned from doctor’s
    prescriptions via supervised learning and evaluation signals learned from the
    long-term outcome of patients via reinforcement learning. In the formulated MDP,
    the domain expert makes a decision based on an unknown policy. The goal is to
    learn a policy that simultaneously reduces the difference between the chosen action
    of the agent and the expert’s decision and to maximize the weighted sum of discounted
    rewards.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 治疗推荐系统旨在帮助医生基于电子健康记录做出更好的决策。然而，医生的处方不是绝对真相，而是针对高风险医疗案例的宝贵建议。真正的真相是患者的延迟状况。因此，模型的预测不能过于偏离医生的判断，也不能将这些判断作为真实标签。为了解决这个问题，Wang
    等人 [[103](#bib.bib103)] 提出了一个结合监督学习和强化学习的架构。该模型减少了通过监督学习从医生处方中学习到的指标信号与通过强化学习从患者长期结果中学习到的评估信号之间的不一致。在制定的MDP中，领域专家基于未知策略做出决策。目标是学习一个策略，既减少代理选择的行动与专家决策之间的差异，又最大化折扣奖励的加权和。
- en: 4.2.2 Diagnostic Inference
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 诊断推断
- en: Using DRL to perform diagnosis can provide a second opinion in high-intensity
    diagnosis from historical medical records to reduce diagnostic errors. Ling et
    al. [[51](#bib.bib51)] propose modeling the integration of external evidence to
    capture diagnostic concept as a MDP. The objective is to find the optimal policy
    function. The inputs are case narratives and the outputs are improved concepts
    and inferred diagnoses. The states are a set of measures over the similarity of
    current concepts and externally extracted concepts. The actions are whether to
    accept (part of) the extracted concepts from external evidence. The environments
    are the top extracted case narratives from Wikipedia as the document pool for
    concepts extraction and a knowledge base for evaluating the intermediate results
    for current best concepts. The rewards are evaluated based on an external knowledge
    base mapping from the concepts to the diagnoses. The whole process is modeled
    by DQN. At each step, narrative cases and evidence are extracted, which provide
    the initial concepts and external concepts. The state representing the agent’s
    confidence in the learned concept is duly calculated. Then the state is sent to
    the DQN agent to estimate the reward to model the long-run accuracy of the learned
    concept by the agent. Iteratively, the model converges with better concepts and
    diagnoses.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DRL 进行诊断可以通过历史医疗记录提供第二意见，从而减少诊断错误。Ling 等人 [[51](#bib.bib51)] 提出将外部证据的整合建模为
    MDP，以捕捉诊断概念。目标是找到最优的策略函数。输入是病例叙述，输出是改进的概念和推断的诊断。状态是一组衡量当前概念与外部提取概念相似度的度量。行动是是否接受（部分）来自外部证据的提取概念。环境是来自维基百科的顶级提取病例叙述作为概念提取的文档库，以及用于评估当前最佳概念的知识库。奖励是基于将概念映射到诊断的外部知识库进行评估的。整个过程通过
    DQN 建模。在每一步，提取病例和证据，提供初始概念和外部概念。代表代理对学习概念信心的状态被适当地计算。然后将状态发送到 DQN 代理，以估计奖励以建模代理学习概念的长期准确性。通过迭代，模型逐渐收敛到更好的概念和诊断。
- en: 4.2.3 Hospital Resource Allocation
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 医院资源分配
- en: Allocating limited hospital resources is the key to providing timely treatment
    for patients. In [[19](#bib.bib19)], the problem is formulated as a classification
    problem where the patients’ features are given and the target is to predict the
    location of admissions. The RL framework uses a student network to solve the classification
    problem. The weights of the student network are used as states, which are fed
    into a teacher network to generate actions to select which batch of data to train
    the student network. The accuracy of the classification is used as the reward.
    This method provides a view on the resource allocation problem from a curriculum
    learning perspective. However, the temporal information of the data samples is
    not considered but it could affect resource allocation since some hours during
    a day could have fewer patients than the others.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 分配有限的医院资源是为患者提供及时治疗的关键。在[[19](#bib.bib19)]中，这个问题被表述为一个分类问题，其中给出了患者的特征，目标是预测入院的位置。RL
    框架使用一个学生网络来解决分类问题。学生网络的权重被用作状态，并输入到教师网络中以生成行动，选择训练学生网络的数据批次。分类的准确性被用作奖励。这种方法从课程学习的角度提供了对资源分配问题的看法。然而，数据样本的时间信息没有被考虑，但它可能影响资源分配，因为一天中的某些小时可能会有比其他时间段更少的患者。
- en: 4.3 Fintech
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 金融科技
- en: Reinforcement learning has wide applications in the finance domain. Firstly,
    reinforcement learning has brought new perspectives to let the finance research
    community revisit many classic financial research topics. For example, traditional
    financial research topics such as option pricing that are typically solved by
    the classic Black–Scholes model can be steered through with a data-driven insight
    by reinforcement learning [[25](#bib.bib25)]. Secondly, portfolio optimization,
    typically formulated as a stochastic optimal control problem, can be addressed
    by reinforcement learning. Finally, the agents are financial market participants
    with different intentions. Reward functions can be learned to model these intentions,
    and hence, make better decisions as illustrated in Figure [3](#S4.F3 "Figure 3
    ‣ 4.3 Fintech ‣ 4 Data Analytics Applications ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"). We refer readers with further interest
    in finance to [[62](#bib.bib62)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在金融领域有广泛的应用。首先，强化学习为金融研究社区提供了新的视角，让他们重新审视许多经典的金融研究课题。例如，传统的金融研究课题如期权定价，通常由经典的布莱克–舒尔斯模型解决，可以通过强化学习的基于数据的洞察来引导[[25](#bib.bib25)]。其次，投资组合优化通常被表述为随机最优控制问题，可以通过强化学习来解决。最后，代理是具有不同意图的金融市场参与者。奖励函数可以学习以建模这些意图，从而做出更好的决策，如图[3](#S4.F3
    "图 3 ‣ 4.3 金融科技 ‣ 4 数据分析应用 ‣ 关于深度强化学习在数据处理和分析中的应用的调查")所示。我们建议对金融领域有进一步兴趣的读者参考[[62](#bib.bib62)]。
- en: '![Refer to caption](img/a1487672fbf27deae7079fc83e442543.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a1487672fbf27deae7079fc83e442543.png)'
- en: 'Figure 3: DRL in fintech applications.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 金融科技应用中的深度强化学习（DRL）。'
- en: 4.3.1 Dynamic Portfolio Optimization
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 动态投资组合优化
- en: The portfolio optimization problem is challenging because of the high scale
    of the dimensionality and the high noise-to-signal ratio nature of stock price
    data. The latter problem of noisy observation can cause uncertainty in a learned
    policy. Therefore, [[12](#bib.bib12)] proposes a novel model structure based on
    the Q-learning to handle noisy data and to scale to high dimensionality. The quadratic
    form of reward function is shown to have a semi-analytic solution that is computationally
    efficient. In the problem formulation, the agent’s actions are represented as
    the changes in the assets at each time step. The states are the concatenation
    of market signals and the agent’s holding assets. This method enhances Q-learning
    by introducing an entropy term measuring the noise in the data. This term acts
    as a regularization term forcing the learned policy to be close to a reference
    policy that is modeled by a Gaussian distribution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 投资组合优化问题具有挑战性，因为高维度的规模和股票价格数据的高噪声信号比。噪声观测的问题会导致学到的策略的不确定性。因此，[[12](#bib.bib12)]提出了一种基于Q学习的新型模型结构来处理噪声数据并扩展到高维度。奖励函数的二次形式被证明具有计算效率高的半解析解。在问题的表述中，代理的行动被表示为每个时间步资产的变化。状态是市场信号和代理持有资产的拼接。这种方法通过引入一个测量数据噪声的熵项来增强Q学习。这个熵项作为一个正则化项，迫使学到的策略接近由高斯分布建模的参考策略。
- en: 4.3.2 Algorithm Trading Strategy Identification
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 算法交易策略识别
- en: Identification of algorithm trading strategies from historical trades is important
    in fraud detection and maintaining a healthy financial environment. [[114](#bib.bib114)]
    proposes using IRL to learn the reward function behind the trading behaviors.
    The problem is formulated as an Inverse Markov Decision Process (IMDP). The states
    are the differences between the volumes of bid orders and ask orders, which are
    discretized into three intervals based on the values of the volumes. The actions
    are the limit and market order discretized into 10 intervals each by their values.
    The prior distribution of the reward function is a Gaussian Process parameterized
    by $\theta$. Given $\theta$, the approximation of the posterior distribution of
    reward is performed by maximum a posteriori (MAP). This step would give a MAP
    estimated value of the reward. $\theta$ is optimized by a log-likelihood function
    on the posterior of observations. The optimization process can be proved to be
    convex which guarantees the global minimum. The learned features are then used
    to identify and classify trading strategies in the financial markets.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史交易中识别算法交易策略对于欺诈检测和维护健康的金融环境至关重要。[[114](#bib.bib114)] 提出使用 IRL 来学习交易行为背后的奖励函数。问题被构造为逆马尔可夫决策过程（IMDP）。状态是买入订单和卖出订单的量之间的差异，这些差异根据量的值被离散化为三个区间。动作是限价订单和市场订单，每个动作按其值离散化为
    10 个区间。奖励函数的先验分布是由 $\theta$ 参数化的高斯过程。给定 $\theta$，奖励的后验分布的近似是通过最大后验概率（MAP）进行的。这一步会给出奖励的
    MAP 估计值。$\theta$ 通过对观察值后验的对数似然函数进行优化。优化过程可以证明是凸的，这保证了全局最小值。然后使用学习到的特征来识别和分类金融市场中的交易策略。
- en: 4.3.3 Sentiment-based Trading
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 基于情绪的交易
- en: 'One of the main predictors in stock trading is sentiment, which drives the
    demand of bid orders and asks orders. Sentiment scores are often represented by
    unstructured text data such as news or twitters. [[115](#bib.bib115)] proposes
    treating the sentiment as the aggregated action of all the market participants,
    which has the advantage of simplifying the modeling of the numerous market participants.
    Specifically, the sentiment scores are categorized into three intervals: high,
    medium, and low as the action spaces. Compared to previous works, the proposed
    method can model the dependency between the sentiment and the market state by
    the policy function. This method is based on Gaussian Inverse Reinforcement Learning
    [[43](#bib.bib43)] similar to [[114](#bib.bib114)] as discussed at the beginning
    of Section [4.3](#S4.SS3 "4.3 Fintech ‣ 4 Data Analytics Applications ‣ A Survey
    on Deep Reinforcement Learning for Data Processing and Analytics"), which is effective
    at dealing with uncertainty in the stock environment. This method provides a method
    for modeling market sentiments. However, as IRL faces the challenge of non-uniqueness
    of reward [[13](#bib.bib13)] of one agent’s actions, the method does not address
    how aggregated actions of multiple market participants can infer a unique reward
    function.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 股票交易中的主要预测因素之一是情绪，它驱动买入订单和卖出订单的需求。情绪分数通常通过如新闻或推特等非结构化文本数据表示。[[115](#bib.bib115)]
    提出将情绪视为所有市场参与者的聚合行动，这种方法的优点是简化了众多市场参与者的建模。具体而言，情绪分数被分类为三个区间：高、中、低，作为行动空间。与之前的研究相比，提出的方法可以通过策略函数建模情绪与市场状态之间的依赖关系。这种方法基于高斯逆强化学习
    [[43](#bib.bib43)]，类似于 [[114](#bib.bib114)]，如第 [4.3](#S4.SS3 "4.3 Fintech ‣ 4
    Data Analytics Applications ‣ A Survey on Deep Reinforcement Learning for Data
    Processing and Analytics") 节开头所讨论的，这对于处理股票环境中的不确定性是有效的。这种方法为建模市场情绪提供了一种方法。然而，由于
    IRL 面临奖励的非唯一性 [[13](#bib.bib13)]，该方法没有解决多个市场参与者的聚合行动如何推断唯一奖励函数的问题。
- en: 4.4 E-Commerce
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 电子商务
- en: 4.4.1 Online Advertising
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 在线广告
- en: 'With the increasing digitalization of businesses, sales and competition for
    market shares have moved online in tandem. As a result, online advertising has
    been increasing in its presence and importance and exploiting RL in various aspects.
    One of the topics in online advertising, bidding optimization, can be formulated
    as a sequential decision problem: the advertiser is required to have strategic
    proposals with bidding keywords sequentially to maximize the overall profit. In [[124](#bib.bib124)],
    the issue of using static transitional probability to model dynamic environments
    is identified and a new DRL model is proposed to exploit the pattern discovered
    from dynamic environments.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随着企业数字化程度的提高，销售和市场份额竞争也同步转移到线上。因此，在线广告在其存在和重要性方面不断增加，并在各个方面利用 RL。在线广告的一个话题，竞价优化，可以被表述为一个序列决策问题：广告商需要对竞价关键词进行战略性提案，以最大化总体利润。在[[124](#bib.bib124)]中，识别了使用静态转移概率建模动态环境的问题，并提出了一种新的
    DRL 模型来利用从动态环境中发现的模式。
- en: Including but not limited to advertising, Feng et al. [[21](#bib.bib21)] propose
    to consider the whole picture of multiple ranking tasks that occurred in the sequence
    of user’s queries. A new multi-agent reinforcement learning model is proposed
    to enable multiple agents to partially observe inputs and choose actions through
    their own actor networks. The agents communicate through a centralized critic
    model to optimize a shared objective. This allows different ranking algorithms
    to reconcile with each other when taking their own actions and consider the contextual
    information.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 包括但不限于广告，冯等人[[21](#bib.bib21)]建议考虑用户查询序列中发生的多个排名任务的整体情况。提出了一种新的多智能体强化学习模型，使多个智能体能够部分观察输入，并通过各自的行动者网络选择动作。智能体通过集中式批评模型进行通信，以优化共享目标。这使得不同的排名算法在执行各自动作时能够相互协调，并考虑上下文信息。
- en: 4.4.2 Online Recommendation
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 在线推荐
- en: The problem of an unstabilized reward function arises because of the dynamic
    environment in the online recommendation. For example, user preference is modeled
    as the reward in DRL and it changes unexpectedly when a special discount happens
    for some products. In [[10](#bib.bib10)], a random stratified sampling method
    is proposed to calculate the optimal way of stratifying by allocating more samples
    to the strata with more weighted variance. Then the replay sampling is improved
    to consider key attributes of customers (e.g., gender, age, etc.), which are less
    volatile in the dynamic environment. This allows the modeling of reward function
    based on sampling from a pool with a longer horizon, thus reducing the bias in
    the estimation of the reward function. Lastly, the dynamic environment poses a
    challenge in setting an optimal policy used in regretting. A new method in [[10](#bib.bib10)]
    is proposed to train an offline model to calculate a real-time reward for a subset
    of customers to approximate a reference policy, that is used as an offset in the
    reward recalibration to stabilize the performance of the DRL algorithm.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 未稳定奖励函数的问题出现是因为在线推荐中的动态环境。例如，用户偏好在 DRL 中被建模为奖励，当某些产品发生特殊折扣时，这种偏好会意外变化。在[[10](#bib.bib10)]中，提出了一种随机分层抽样方法，通过将更多样本分配给具有更多加权方差的层来计算最佳分层方式。然后，重放抽样得到改进，考虑了顾客的关键属性（如性别、年龄等），这些属性在动态环境中变化较小。这允许基于来自较长时间范围池的抽样来建模奖励函数，从而减少奖励函数估计中的偏差。最后，动态环境在设定用于遗憾的最佳策略时提出了挑战。在[[10](#bib.bib10)]中，提出了一种新方法来训练离线模型，以计算子集顾客的实时奖励，以逼近参考策略，并作为奖励再校准中的偏移量，用于稳定
    DRL 算法的性能。
- en: 4.4.3 Search Results Aggregation
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 搜索结果聚合
- en: Aggregating useful search results in online shopping search is important to
    improve the shopping experience. However, the challenge of aggregating heterogeneous
    data sources is often encountered. The heterogeneous data sources in online shopping
    are different product categories such as a shoe brand group or a particular topic
    group, each of which is a ranking system. A new model in [[91](#bib.bib91)] is
    proposed to decompose the task into two sub-tasks. The first one is to select
    a data source for the current page of search results based on historical users’
    clicks on previous pages. Learning to select the correct data source for each
    page is a sequential decision-making problem. The second sub-task is to fill the
    sequence of a page by selecting the best source from the candidate sources. However,
    the items from different sources cannot be directly compared because of their
    heterogeneous nature. The problem is solved by formulating the sub-task as an
    RL task to let an agent fill up the sequence. However, one limitation of this
    method is that lacking full annotations of item relevance scores may constrain
    the model’s performance on various scenarios [[91](#bib.bib91)].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线购物搜索中汇总有用的搜索结果对于改善购物体验至关重要。然而，常常会遇到汇总异质数据源的挑战。在线购物中的异质数据源包括不同的产品类别，如鞋子品牌组或特定主题组，每个类别都是一个排序系统。在[[91](#bib.bib91)]中提出了一种新模型，将任务分解为两个子任务。第一个子任务是根据历史用户在以前页面上的点击选择当前页面的搜索结果数据源。学习为每个页面选择正确的数据源是一个序列决策问题。第二个子任务是通过从候选数据源中选择最佳来源来填充页面的序列。然而，由于不同来源的项目具有异质性，不能直接进行比较。通过将子任务公式化为
    RL 任务来解决问题，让代理填充序列。然而，这种方法的一个局限性是缺乏完整的项目相关性评分注释，这可能限制模型在各种场景中的表现[[91](#bib.bib91)]。
- en: 4.5 Other Applications
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 其他应用
- en: DRL has been applied to various other applications. These DRL methods are often
    used with a knowledge graph, confounders, or game theory to model application-specific
    dynamics. These methods are not only well motivated from their respective applications
    but also general enough to be applied in other applications. However, these methods
    often fail to be evaluated by experiments in other applications.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 已应用于各种其他应用。这些 DRL 方法通常与知识图谱、混杂因素或博弈论一起使用，以建模特定应用的动态。这些方法不仅在各自的应用中有很好的动机，而且足够通用，可以应用于其他应用。然而，这些方法往往在其他应用中的实验评估上失败。
- en: The problem of mobile user profiling aims to identify user profiles to provide
    personalized services. In [[105](#bib.bib105)], the action is the selection of
    a place of visit. The environment is comprised of all users and a knowledge graph
    learning the semantic connections between the spatial entities. The knowledge
    graph is updated once a user’s new activity is performed and then affects the
    agent’s prediction. The state is the embedding of a user and the knowledge graph
    for the current time step. The reward is determined by several metrics measuring
    the similarity between the predicted spatial entities and the ground truth. This
    method considers the spatial semantics of entities but does not consider how the
    change of a user’s key attributes (e.g., career) will affect activity prediction
    and policy learning, which could cause instability in policy updating.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 移动用户画像问题旨在识别用户档案以提供个性化服务。在[[105](#bib.bib105)]中，动作是选择访问的地点。环境包括所有用户和一个学习空间实体语义连接的知识图谱。知识图谱在用户执行新活动时会更新，并影响代理的预测。状态是用户的嵌入表示和当前时间步骤的知识图谱。奖励由几个指标确定，这些指标衡量预测的空间实体与真实情况之间的相似性。这种方法考虑了实体的空间语义，但没有考虑用户关键属性（例如职业）的变化如何影响活动预测和政策学习，这可能会导致政策更新的不稳定。
- en: In the transportation system, drivers often get recommendations and provide
    feedback in return to improve the service. However, the recommendation often fails
    when drivers make decisions in a complex environment. To address this issue, in
    [[83](#bib.bib83)] a new method is proposed to model hidden causal factors, called
    confounders, in a complex environment. Specifically, the framework in [[32](#bib.bib32)]
    is extended to include the confounders. First, all three elements (i.e., policy
    agent, environment, confounder) are treated as agents. The effect of a confounder
    is modeled as the policy of the hidden agent, which takes the observation and
    action of the policy agent as inputs and performs an action. The environment in
    turn takes the action based on inputs of the hidden agent’s action and the policy
    agent’s action and observation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在交通系统中，司机经常会收到建议并提供反馈以改进服务。然而，当司机在复杂环境中做出决策时，这些建议往往失败。为了解决这个问题，在[[83](#bib.bib83)]中提出了一种新的方法，用于建模复杂环境中的隐藏因果因素，即混杂因素。具体来说，[[32](#bib.bib32)]中的框架被扩展以包含这些混杂因素。首先，所有三个元素（即策略代理、环境、混杂因素）都被视为代理。混杂因素的影响被建模为隐藏代理的策略，该策略以政策代理的观察和动作作为输入并执行一个动作。环境则根据隐藏代理的动作和政策代理的动作及观察作为输入来执行动作。
- en: The problem of spammer detection aims to detect spam generating strategies.
    The challenge is that the detectors only detect easier spams while missing spams
    with strategies. In [[16](#bib.bib16)], the problem is formulated as two agents
    counteracting each other. One agent is the spammer, whose policy is to maintain
    a distribution of spam strategies and the action is to sample from the distribution.
    Another agent is the detector, whose state is the detection results after a spam
    attack and the action is to identify the spam. The rewards of two agents are measured
    by winning or losing revenue manipulation, respectively. The limitation of this
    method is that there is no guarantee for equilibrium.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件检测问题旨在检测生成垃圾邮件的策略。挑战在于检测器只检测较简单的垃圾邮件，而遗漏了具有策略的垃圾邮件。在[[16](#bib.bib16)]中，该问题被公式化为两个相互对抗的代理。一个代理是垃圾邮件发送者，其策略是保持垃圾邮件策略的分布，行动是从分布中抽样。另一个代理是检测器，其状态是垃圾邮件攻击后的检测结果，行动是识别垃圾邮件。这两个代理的奖励分别通过赢得或失去收入操控来衡量。该方法的限制是无法保证均衡。
- en: 5 Open Challenges and Future Directions
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 开放挑战与未来方向
- en: RL approaches provide strong alternatives to traditional heuristics or supervised
    learning-based algorithms. However, many challenges remain to be addressed to
    make RL a practical solution in the context of data processing and analytics.
    We also foresee many important future research directions to be developed.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: RL 方法为传统启发式或基于监督学习的算法提供了强有力的替代方案。然而，许多挑战仍待解决，以使 RL 成为数据处理和分析中的实际解决方案。我们也预见到许多重要的未来研究方向有待发展。
- en: 5.1 Open Challenges For System Optimization
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 系统优化中的开放挑战
- en: 5.1.1 MDP Formulation and Lack of Justification
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 MDP 公式化与缺乏理由说明
- en: The design of MDP impacts the performance and efficiency of the RL algorithm
    greatly. The state should satisfy Markov property that its representation contains
    enough relevant information for the RL agent to make the optimal decision. It
    should summarize the environment compactly because a complicated state design
    will cause more training and inference costs. The action space should be designed
    carefully to balance learning performance and computational complexity. The reward
    definition directly affects the optimization direction and the system performance.
    Additionally, the process of reward calculation can involve costly data collection
    and computation in the data systems optimization. Currently, many works rely on
    experimental exploration and experience to formulate MDP while some works exploit
    domain knowledge to improve the MDP formulation by injecting task-specific knowledge
    into action space[[110](#bib.bib110)]. Generally, MDP can influence computational
    complexity, data required, and algorithm performance. Unfortunately, many works
    lack ablation studies of their MDP formulations and do not justify the design
    in a convincing manner. Therefore, automation of MDP formulation remains an open
    problem.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 的设计对 RL 算法的性能和效率有很大影响。状态应该满足马尔可夫性质，其表示必须包含足够的相关信息，以便 RL 代理能够做出最佳决策。状态应简洁地总结环境，因为复杂的状态设计会导致更多的训练和推理成本。动作空间应精心设计，以平衡学习性能和计算复杂性。奖励定义直接影响优化方向和系统性能。此外，奖励计算过程可能涉及在数据系统优化中成本高昂的数据收集和计算。目前，许多研究依赖实验探索和经验来制定
    MDP，而一些研究则利用领域知识，通过将任务特定的知识注入动作空间来改进 MDP 的制定[[110](#bib.bib110)]。一般来说，MDP 可以影响计算复杂性、所需数据和算法性能。不幸的是，许多研究缺乏对其
    MDP 制定的消融研究，也未以令人信服的方式证明设计。因此，MDP 制定的自动化仍然是一个未解决的问题。
- en: 5.1.2 RL Algorithm and Technique Selection
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 RL 算法和技术选择
- en: RL algorithms and techniques have different tradeoffs and assumptions. Value-based
    DRL algorithms like DQN are not stable and guaranteed convergence. Policy-based
    DRL algorithms like TRPO and PPO are often not efficient. Model-based DRL algorithms
    do not guarantee that a better model can result in a better policy. Value-based
    methods assume full observability while policy-based ones assume episodic learning.
    Off-policy algorithms are usually more efficient than on-policy algorithms in
    terms of sample efficiency. One example is that DQ[[37](#bib.bib37)] uses off-policy
    deep Q-learning to increase data efficiency and reduce the number of training
    queries needed. Training efficiency can be a big concern for DRL-based system
    optimization, especially when the workload of the system could change dramatically
    and the model needs to be retrained frequently. Generally, RL algorithms and techniques
    selection affect the training efficiency and effectiveness greatly.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: RL 算法和技术具有不同的权衡和假设。基于值的 DRL 算法，如 DQN，通常不稳定且无法保证收敛。基于策略的 DRL 算法，如 TRPO 和 PPO，通常效率较低。基于模型的
    DRL 算法无法保证更好的模型会导致更好的策略。基于值的方法假设完全可观察，而基于策略的方法假设是逐步学习。离策略算法通常比在策略算法在样本效率方面更有效。例如，DQ[[37](#bib.bib37)]
    使用离策略深度 Q 学习来提高数据效率，减少所需的训练查询次数。训练效率可能是 DRL 系统优化中的一个大问题，特别是当系统的工作负载可能发生剧烈变化且模型需要频繁重新训练时。一般来说，RL
    算法和技术的选择对训练效率和效果有很大影响。
- en: 5.1.3 Integration with Existing Systems
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 与现有系统的集成
- en: Integrating RL-based methods into the real system more naturally and seamlessly
    faces many challenges. The RL agent has to be evolved when the system environment
    changes (e.g., workload) and the performance is degraded. We need to design new
    model management mechanisms to monitor, maintain, and upgrade the models. Furthermore,
    we find that the RL-based solutions can be lightweight or intrusive. The lightweight
    approach in which the RL agent is not designed as a component of the system, e.g.
    using RL to generate the qd-tree[[116](#bib.bib116)], is easier to integrate into
    the system because it does not change the architecture of the system dramatically.
    In contrast, the intrusive approach such as using RL models for join order optimization[[61](#bib.bib61)]
    is deeply embedded in the system and hence may need a redesign and optimization
    of the original system architecture to support model inference efficiently. SageDB[[35](#bib.bib35)]
    proposes to learn various database system components by integrating RL and other
    ML techniques. Nevertheless, the proposed model-driven database system is yet
    to be fully implemented and benchmarked. It is likely that the data system architecture
    needs to be overhauled or significantly amended in order to graft data-driven
    RL solutions into the data system seamlessly to yield an overall performance gain.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 将基于RL的方法更自然、无缝地整合到实际系统中面临许多挑战。当系统环境（例如工作负载）发生变化并且性能下降时，RL代理必须进行演变。我们需要设计新的模型管理机制来监控、维护和升级模型。此外，我们发现基于RL的解决方案可以是轻量级的或侵入性的。轻量级的方法中，RL代理不是作为系统的组成部分进行设计，例如使用RL生成qd-tree[[116](#bib.bib116)]，因为它不会显著改变系统的架构，从而更容易集成到系统中。相比之下，像使用RL模型进行连接顺序优化[[61](#bib.bib61)]的侵入性方法深嵌于系统中，因此可能需要重新设计和优化原始系统架构，以高效支持模型推理。SageDB[[35](#bib.bib35)]提出通过整合RL和其他机器学习技术来学习各种数据库系统组件。然而，所提出的模型驱动数据库系统尚未完全实现和基准测试。数据系统架构可能需要彻底改革或显著修改，以便将数据驱动的RL解决方案无缝植入数据系统，从而实现整体性能的提升。
- en: 5.1.4 Reproducibility and Benchmark
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 可重复性和基准测试
- en: In the data system optimization problem, RL algorithms are not easy to be reproduced
    due to many factors such as lacking open source codes, workload, historic statistics
    used, and the unstable performance of RL algorithms. The landscape of problems
    in system optimization is vast and diverse. It could prevent fair comparison and
    optimization for future research works and deployments in practice. Lacking benchmarks
    is another challenge to evaluate these RL approaches. The benchmarks are therefore
    to provide standardized environments and evaluation metrics to conduct experiments
    with different RL approaches. There are some efforts to mitigate the issue. For
    example, Park[[57](#bib.bib57)] is an open platform for researchers to conduct
    experiments with RL. However, it only provides a basic interface and lacks system
    specifications. There is much room to improve with regards to the reproducibility
    and benchmark in order to promote the development and adoption of RL-based methods[[30](#bib.bib30)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据系统优化问题中，由于缺乏开源代码、工作负载、历史统计数据以及RL算法的性能不稳定等诸多因素，RL算法不容易被复制。系统优化中的问题范围广泛且多样，这可能阻碍了对未来研究工作和实际部署的公平比较和优化。缺乏基准测试是评估这些RL方法的另一个挑战。因此，基准测试旨在提供标准化的环境和评估指标，以进行不同RL方法的实验。已有一些努力来缓解这一问题。例如，Park[[57](#bib.bib57)]是一个开放平台供研究人员进行RL实验。然而，它仅提供基本接口，缺乏系统规格。为了促进RL方法的开发和采用，在可重复性和基准测试方面还有很大的改进空间[[30](#bib.bib30)]。
- en: 5.2 Open Challenges For Applications
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 应用领域的开放挑战
- en: 5.2.1 Lack of Adaptability
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 适应性不足
- en: There is a lack of adaptability for methods on a single component of a data
    pipeline to the whole. For example, many works focus on data cleaning tasks such
    as entity matching. However, little works have shown their efficiency in deploying
    their model in an end-to-end data pipeline. These works treat the tasks isolatedly
    from other tasks in the pipeline, thereby limiting the pipeline’s performance.
    In healthcare, each method is applied in different steps of the whole treatment
    process, without being integrated and evaluated as one pipeline. One possible
    direction could be considering DRL as a module in the data pipeline optimization.
    However, data pipeline optimization has been focusing on models simpler than DRL
    to enable fast pipeline evaluation [[53](#bib.bib53)]. How to efficiently incorporate
    DRL into the data pipeline optimization remains a challenge.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据管道单一组件的方法缺乏对整体的适应性。例如，许多研究集中在数据清理任务如实体匹配上。然而，少有研究展示了它们在端到端数据管道中的效率。这些工作将任务与管道中的其他任务孤立对待，从而限制了管道的性能。在医疗保健中，每种方法应用于整个治疗过程中的不同步骤，而未被整合和评估为一个管道。一个可能的方向是将DRL视为数据管道优化中的一个模块。然而，数据管道优化一直专注于比DRL更简单的模型，以便于快速管道评估[[53](#bib.bib53)]。如何有效地将DRL融入数据管道优化仍然是一个挑战。
- en: 5.2.2 Difficulty in Comparison with Different Applications
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 与不同应用比较的困难
- en: To date, most works with generalized contributions are only evaluated domain-specifically.
    Research questions are often formulated in their own platform as in E-Commerce.
    This presents difficulty in evaluating the methods for different environments.
    For example, the confounders modeling hidden causal factors in [[83](#bib.bib83)]
    can also contribute to DRL modeling in E-commerce. This is because modeling customers’
    interests are always subject to changing environments and a new environment may
    contain hidden causal factors. For example, consumers are more willing to buy
    relevant products for certain situations such as Covid-19\. Thus a general DRL
    method is yet to show the robustness and effectiveness under the environment of
    different applications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，大多数具有通用贡献的工作仅在特定领域进行评估。研究问题通常在其自身平台上进行制定，例如电子商务。这使得在不同环境中评估方法变得困难。例如，[[83](#bib.bib83)]中隐藏因果因素的混淆建模也可以贡献于电子商务中的DRL建模。这是因为建模客户兴趣总是受到环境变化的影响，而新环境可能包含隐藏的因果因素。例如，消费者更愿意为特定情况如Covid-19购买相关产品。因此，一种通用的DRL方法尚未在不同应用环境中展示其鲁棒性和有效性。
- en: 5.2.3 Lack of Prediction in Multi-modality
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 多模态中的预测缺失
- en: In healthcare and finance, multiple sources of data bring different perspectives.
    For example in healthcare, electronic health records, image scans, and medical
    tests can provide different features for accurate prediction. In addition, these
    sources of data with different sample frequencies provide contextual information
    for modeling a patient’s visits to the hospital or symptom development. However,
    most innovations in healthcare focus on one particular source of data. How to
    integrate the contextual information with multi-modality effectively remains an
    unsolved difficult problem.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健和金融领域，多个数据来源带来不同的视角。例如在医疗保健中，电子健康记录、图像扫描和医学测试可以提供不同的特征以便于准确预测。此外，这些具有不同采样频率的数据源为建模患者的医院就诊或症状发展提供了上下文信息。然而，大多数医疗保健创新集中在某一特定的数据源上。如何有效地将上下文信息与多模态整合仍然是一个未解决的难题。
- en: 5.2.4 Injecting Domain Knowledge in Experience Replay
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 在经验重放中注入领域知识
- en: In high-stake applications such as healthcare and finance, injecting domain
    knowledge can make decision making in RL more robust and explainable. One possible
    way is to inject the knowledge of human beings’ experience into an agent’s experience
    pool as a prior distribution for the policy. For example, in dynamic portfolio
    optimization, a portfolio manager could have a large source of experience for
    risk management and profit optimization. Such experience could be useful for warming
    up the agent’s exploration in the search space. Some works have shown positive
    effects of domain knowledge injection on selecting important experiences (i.e.,
    transition samples) [[79](#bib.bib79)]. Notwithstanding, it remains a big challenge
    to inject useful and relevant knowledge from the experience into the agent’s experience
    pool.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗和金融等高风险应用中，注入领域知识可以使强化学习中的决策过程更加稳健且具有解释性。一个可能的方式是将人类经验的知识注入到代理的经验池中，作为策略的先验分布。例如，在动态投资组合优化中，投资组合经理可能会有大量的风险管理和利润优化的经验。这些经验对于提升代理在搜索空间中的探索具有帮助。一些研究已经展示了领域知识注入在选择重要经验（即，转移样本）方面的积极效果[[79](#bib.bib79)]。尽管如此，从经验中注入有用和相关的知识到代理的经验池中仍然是一个巨大的挑战。
- en: 5.3 Future Research Directions
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 未来研究方向
- en: 5.3.1 Data Structure Design
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 数据结构设计
- en: DRL provides an alternative way to find good data structures through feedback
    instead of designing them based on human knowledge and experience, e.g., decision
    tree[[47](#bib.bib47)] and the qd-tree[[116](#bib.bib116)]. These trees are optimized
    better because they are learned by interacting with the environment. DRL has also
    been effective in graph designs (e.g., molecular graph[[117](#bib.bib117)]). However,
    large-scale graph generation using DRL is difficult and daunting because it involves
    a huge search space. Generating other important structures using DRL remains to
    be explored. Idreos et al.[[33](#bib.bib33)] propose a Data Alchemist that learns
    to synthesize data structures by DRL and other techniques including Genetic Algorithms
    and Bayesian Optimization. In summary, DRL has a role in the design of more efficient
    data structures by interacting and learning from the environment. These indexes
    have to be adaptive to different data distributions and workloads.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 提供了一种通过反馈寻找良好数据结构的替代方式，而不是基于人类知识和经验设计数据结构，例如，决策树[[47](#bib.bib47)]和 qd-tree[[116](#bib.bib116)]。这些树通过与环境交互学习，从而得到了更好的优化。DRL
    在图形设计（例如，分子图[[117](#bib.bib117)]）方面也表现出了有效性。然而，使用 DRL 进行大规模图形生成是困难且令人生畏的，因为它涉及一个巨大的搜索空间。使用
    DRL 生成其他重要结构仍有待探索。Idreos 等人[[33](#bib.bib33)]提出了一种数据炼金术士，通过 DRL 和其他技术，包括遗传算法和贝叶斯优化，来学习合成数据结构。总之，DRL
    在通过与环境交互和学习来设计更高效的数据结构方面发挥着作用。这些索引必须适应不同的数据分布和工作负载。
- en: 5.3.2 Interpretability
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 可解释性
- en: The underlying logic behind the DRL agent is still unknown. In high-risk application
    areas such as healthcare, the adoption of DRL will be a big issue in the case
    that these approaches make wrong decisions and people do not know why it happens
    due to lack of interpretability. Many techniques have been proposed to mitigate
    the issue and provide interpretability[[76](#bib.bib76)]. However, they neglect
    domain knowledge from related fields and applications and the explanations are
    not effective to human users. To instill confidence in the deployment of DRL-based
    systems in practice, interpretability is an important component and we should
    avoid treating DRL solutions as black boxes especially in critical applications.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 代理的基本逻辑仍然未知。在医疗等高风险应用领域，如果这些方法做出了错误决策而人们由于缺乏可解释性而不知道原因，那么采用 DRL 将是一个大问题。许多技术已经被提出以减轻这一问题并提供可解释性[[76](#bib.bib76)]。然而，它们忽略了相关领域和应用中的领域知识，解释对人类用户并不有效。为了增强对
    DRL 基于系统在实践中部署的信心，可解释性是一个重要组成部分，我们应避免将 DRL 解决方案视为黑箱，特别是在关键应用中。
- en: 5.3.3 Robustness by Causal Reasoning
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 通过因果推理的鲁棒性
- en: Modeling real-world applications by DRL inevitably suffers from the problem
    of distribution changes. The real world has independent physical mechanisms that
    can be seen as different modules. For example, an image is subjected to the light
    of the environment. Given the modular property, a structural type of modeling
    focusing on factorizing the causal mechanisms can extract the invariant causal
    mechanisms and show robustness cross distribution changes [[82](#bib.bib82)].
    One research direction towards DRL robust decision making is to perform sampling
    from past actions from a causal perspective. Given the invariance property of
    causal mechanisms, past actions can be reused by capturing the invariant mechanisms
    in a changing environment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 用 DRL 建模现实世界应用不可避免地会遇到分布变化的问题。现实世界有独立的物理机制，可以视为不同的模块。例如，图像受到环境光的影响。鉴于模块化特性，关注因果机制的结构化建模可以提取不变的因果机制，并在分布变化中展示鲁棒性[[82](#bib.bib82)]。朝向
    DRL 鲁棒决策的一个研究方向是从因果视角进行过去动作的采样。鉴于因果机制的不变性，可以通过捕捉变化环境中的不变机制来重用过去的动作。
- en: 5.3.4 Extension to Other Domains
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4 扩展到其他领域
- en: Beyond existing works, many classic problems in the data system and analytics
    could potentially be solved by DRL. For example, Polyjuice[[101](#bib.bib101)]
    learns the concurrency control algorithm for a given workload by defining fine-grained
    actions and states in the context of concurrency control. Though they use an evolutionary
    algorithm to learn and outperform a simple DRL baseline, we believe that there
    are huge potentials to further improve DRL for niche applications. Hence, we expect
    that more problems will be explored and solved with DRL in various domains in
    the near future.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了现有的研究，数据系统和分析中的许多经典问题可能通过深度强化学习（DRL）得到解决。例如，Polyjuice[[101](#bib.bib101)]
    通过在并发控制的背景下定义细粒度的动作和状态来学习给定工作负载的并发控制算法。尽管他们使用了一种进化算法来学习并超越了简单的 DRL 基准，我们相信还有巨大的潜力可以进一步提升
    DRL 在特定应用中的表现。因此，我们预计未来在各个领域将会有更多的问题被探索并通过 DRL 解决。
- en: 5.3.5 Towards Intelligent and Autonomous Databases
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.5 朝向智能和自主数据库
- en: Although DRL algorithms could provide breakthrough performance on many tasks
    than traditional methods, many issues need to be addressed towards intelligent
    and autonomous databases. First, database schema could be updated and DRL models
    trained on the previous snapshots may not work. DRL algorithms need to tackle
    generalization[[72](#bib.bib72)]. Second, it would be so costly and infeasible
    to train models from scratch for each scenario and setting. Transfer learning
    from existing models could be a potential way to ease the workload greatly. Third,
    we have to choose appropriate DRL algorithms automatically, in the same spirit
    as AutoML. Fourth, current DBMS systems were designed without considering much
    about the learning mechanism. A radically new DBMS design may be proposed based
    on the learning-centric architecture. To support intelligent and autonomous database
    systems, DRL models intelligent behaviors and may provide a solid basis for achieving
    artificial general intelligence based on reward maximization and trial-and-error
    experience[[88](#bib.bib88)].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DRL 算法可以在许多任务上提供比传统方法更突破的性能，但在智能和自主数据库方面仍然需要解决许多问题。首先，数据库模式可能会更新，基于之前快照训练的
    DRL 模型可能不再有效。DRL 算法需要解决泛化问题[[72](#bib.bib72)]。其次，对于每种场景和设置，从头训练模型既昂贵又不可行。利用现有模型的迁移学习可能是大大减轻工作负担的一种潜在方式。第三，我们必须像
    AutoML 一样自动选择合适的 DRL 算法。第四，当前的 DBMS 系统在设计时并未充分考虑学习机制。基于学习中心架构，可能需要提出一种全新的 DBMS
    设计。为了支持智能和自主的数据库系统，DRL 模型具有智能行为，并可能为基于奖励最大化和试错经验实现人工通用智能提供坚实基础[[88](#bib.bib88)]。
- en: 6 Conclusions
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this survey, we present a comprehensive review of recent advances in utilizing
    DRL in data processing and analytics. The DRL agent could learn to understand
    and solve various tasks with the right incentives. First, we introduce basic foundations
    and practical techniques in DRL. Next, we survey and review DRL for data processing
    and analytics from two perspectives, systems and applications. We cover a large
    number of topics ranging from fundamental problems in system areas such as tuning
    and scheduling to important applications such as healthcare and fintech. Finally,
    we discuss key challenges and future directions for applying DRL in data processing
    and analytics. We hope the survey would serve as a basis for research and development
    in this emerging area, and better integration of DRL techniques into data processing
    pipelines and stacks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调研中，我们呈现了近期在数据处理和分析中利用深度强化学习的进展的综合回顾。深度强化学习代理可以通过正确的激励学习理解和解决各种任务。首先，我们介绍了深度强化学习的基本基础和实用技术。接下来，我们从系统和应用两个角度对深度强化学习在数据处理和分析中的应用进行调查和回顾。我们涵盖了大量主题，从系统领域的基本问题如调优和调度到重要应用如医疗保健和金融科技。最后，我们讨论了在数据处理和分析中应用深度强化学习的关键挑战和未来方向。我们希望这次调研能够成为该新兴领域研究和发展的基础，并促进深度强化学习技术更好地集成到数据处理管道和堆栈中。
- en: References
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng,
    T. Kaftan, M. J. Franklin, A. Ghodsi, et al. Spark sql: Relational data processing
    in spark. In ACM SIGMOD, pages 1383–1394, 2015.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng,
    T. Kaftan, M. J. Franklin, A. Ghodsi 等. Spark sql: Spark 中的关系数据处理。ACM SIGMOD,
    页码 1383–1394, 2015。'
- en: '[2] P. Auer. Using confidence bounds for exploitation-exploration trade-offs.
    Journal of Machine Learning Research, 3(Nov):397–422, 2002.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. Auer. 使用置信界限进行利用-探索权衡。机器学习研究期刊, 3(Nov):397–422, 2002。'
- en: '[3] S. Banerjee, S. Jha, Z. Kalbarczyk, and R. Iyer. Inductive-bias-driven
    reinforcement learning for efficient schedules in heterogeneous clusters. In ICML,
    pages 629–641\. PMLR, 2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. Banerjee, S. Jha, Z. Kalbarczyk 和 R. Iyer. 基于归纳偏置的强化学习在异构集群中高效调度的应用。ICML,
    页码 629–641\. PMLR, 2020。'
- en: '[4] O. Bar El, T. Milo, and A. Somech. Automatically generating data exploration
    sessions using deep reinforcement learning. In ACM SIGMOD, pages 1527–1537, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] O. Bar El, T. Milo 和 A. Somech. 使用深度强化学习自动生成数据探索会话。ACM SIGMOD, 页码 1527–1537,
    2020。'
- en: '[5] D. Baranchuk and A. Babenko. Towards similarity graphs constructed by deep
    reinforcement learning. arXiv preprint arXiv:1911.12122, 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Baranchuk 和 A. Babenko. 基于深度强化学习构建的相似性图。arXiv 预印本 arXiv:1911.12122,
    2019。'
- en: '[6] R. Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Bellman. 动态规划。科学, 153(3731):34–37, 1966。'
- en: '[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning.
    In ICML, pages 41–48, 2009.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Bengio, J. Louradour, R. Collobert 和 J. Weston. 课程学习。ICML, 页码 41–48,
    2009。'
- en: '[8] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi,
    Q. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement
    learning. arXiv preprint arXiv:1912.06680, 2019.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D.
    Farhi, Q. Fischer, S. Hashme, C. Hesse 等. 使用大规模深度强化学习的 Dota 2。arXiv 预印本 arXiv:1912.06680,
    2019。'
- en: '[9] S. Chaudhuri and V. Narasayya. Autoadmin “what-if” index analysis utility.
    ACM SIGMOD Record, 27(2):367–378, 1998.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Chaudhuri 和 V. Narasayya. Autoadmin “what-if” 索引分析工具。ACM SIGMOD Record,
    27(2):367–378, 1998。'
- en: '[10] S.-Y. Chen, Y. Yu, Q. Da, J. Tan, H.-K. Huang, and H.-H. Tang. Stabilizing
    reinforcement learning in dynamic environment with application to online recommendation.
    In ACM SIGKDD, pages 1187–1196, 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S.-Y. Chen, Y. Yu, Q. Da, J. Tan, H.-K. Huang 和 H.-H. Tang. 在动态环境中稳定强化学习及其在在线推荐中的应用。ACM
    SIGKDD, 页码 1187–1196, 2018。'
- en: '[11] K. Clark and C. D. Manning. Deep reinforcement learning for mention-ranking
    coreference models. arXiv preprint arXiv:1609.08667, 2016.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. Clark 和 C. D. Manning. 用于提及排序核心ference模型的深度强化学习。arXiv 预印本 arXiv:1609.08667,
    2016。'
- en: '[12] M. Dixon and I. Halperin. G-learner and girl: Goal based wealth management
    with reinforcement learning. arXiv preprint arXiv:2002.10990, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Dixon 和 I. Halperin. G-learner 和 girl: 基于目标的财富管理与强化学习。arXiv 预印本 arXiv:2002.10990,
    2020。'
- en: '[13] H. Dong, H. Dong, Z. Ding, S. Zhang, and Chang. Deep Reinforcement Learning.
    Springer, 2020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] H. Dong, H. Dong, Z. Ding, S. Zhang 和 Chang. 深度强化学习。Springer, 2020。'
- en: '[14] L. Dong and M. Lapata. Language to logical form with neural attention.
    arXiv preprint arXiv:1601.01280, 2016.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. Dong 和 M. Lapata. 使用神经注意力的语言到逻辑形式。arXiv 预印本 arXiv:1601.01280, 2016。'
- en: '[15] W. Dong, C. Moses, and K. Li. Efficient k-nearest neighbor graph construction
    for generic similarity measures. In WWW, pages 577–586, 2011.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W. Dong, C. Moses 和 K. Li. 用于通用相似性度量的高效k最近邻图构建. 在WWW，页码 577–586, 2011.'
- en: '[16] Y. Dou, G. Ma, P. S. Yu, and S. Xie. Robust spammer detection by nash
    reinforcement learning. In ACM SIGKDD, pages 924–933, 2020.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Dou, G. Ma, P. S. Yu 和 S. Xie. 通过纳什强化学习进行鲁棒的垃圾邮件检测. 在ACM SIGKDD，页码
    924–933, 2020.'
- en: '[17] G. C. Durand, M. Pinnecke, R. Piriyev, M. Mohsen, D. Broneske, G. Saake,
    M. S. Sekeran, F. Rodriguez, and L. Balami. Gridformation: towards self-driven
    online data partitioning using reinforcement learning. In Proceedings of the First
    International Workshop on Exploiting Artificial Intelligence Techniques for Data
    Management, pages 1–7, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. C. Durand, M. Pinnecke, R. Piriyev, M. Mohsen, D. Broneske, G. Saake,
    M. S. Sekeran, F. Rodriguez 和 L. Balami. Gridformation: 朝向自驱动在线数据分区的强化学习. 在《第一届国际人工智能技术数据管理研讨会论文集》，页码
    1–7, 2018.'
- en: '[18] G. C. Durand, R. Piriyev, M. Pinnecke, D. Broneske, B. Gurumurthy, and
    G. Saake. Automated vertical partitioning with deep reinforcement learning. In
    European Conference on Advances in Databases and Information Systems, pages 126–134\.
    Springer, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. C. Durand, R. Piriyev, M. Pinnecke, D. Broneske, B. Gurumurthy 和 G.
    Saake. 使用深度强化学习的自动化垂直分区. 在《数据库与信息系统进展欧洲会议》，页码 126–134. Springer, 2019.'
- en: '[19] R. El-Bouri, D. Eyre, P. Watkinson, T. Zhu, and D. Clifton. Student-teacher
    curriculum learning via reinforcement learning: Predicting hospital inpatient
    admission location. In ICML, pages 2848–2857, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] R. El-Bouri, D. Eyre, P. Watkinson, T. Zhu 和 D. Clifton. 通过强化学习的学生-教师课程学习:
    预测医院住院位置. 在ICML，页码 2848–2857, 2020.'
- en: '[20] Z. Fang, Y. Cao, Q. Li, D. Zhang, Z. Zhang, and Y. Liu. Joint entity linking
    with deep reinforcement learning. In WWW, pages 438–447, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Z. Fang, Y. Cao, Q. Li, D. Zhang, Z. Zhang 和 Y. Liu. 使用深度强化学习的联合实体链接.
    在WWW，页码 438–447, 2019.'
- en: '[21] J. Feng, H. Li, M. Huang, S. Liu, W. Ou, Z. Wang, and X. Zhu. Learning
    to collaborate: Multi-scenario ranking via multi-agent reinforcement learning.
    In WWW, pages 1939–1948, 2018.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Feng, H. Li, M. Huang, S. Liu, W. Ou, Z. Wang 和 X. Zhu. 学习合作: 通过多智能体强化学习进行多场景排名.
    在WWW，页码 1939–1948, 2018.'
- en: '[22] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih,
    R. Munos, D. Hassabis, O. Pietquin, et al. Noisy networks for exploration. arXiv
    preprint arXiv:1706.10295, 2017.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V.
    Mnih, R. Munos, D. Hassabis, O. Pietquin 等. 用于探索的噪声网络. arXiv预印本 arXiv:1706.10295,
    2017.'
- en: '[23] Y. Gao, L. Chen, and B. Li. Spotlight: Optimizing device placement for
    training deep neural networks. In ICML, pages 1676–1684\. PMLR, 2018.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Gao, L. Chen 和 B. Li. Spotlight: 优化深度神经网络训练的设备放置. 在ICML，页码 1676–1684.
    PMLR, 2018.'
- en: '[24] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and S. Wang. The rlr-tree:
    A reinforcement learning based r-tree for spatial data. arXiv preprint arXiv:2103.04541,
    2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang 和 S. Wang. RLR树: 基于强化学习的空间数据R树.
    arXiv预印本 arXiv:2103.04541, 2021.'
- en: '[25] I. Halperin. The qlbs q-learner goes nuqlear: fitted q iteration, inverse
    rl, and option portfolios. Quantitative Finance, 19(9):1543–1553, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] I. Halperin. QLBS Q-learner变为核武器: 拟合Q迭代、逆向RL和选项组合. 《定量金融》，19(9):1543–1553,
    2019.'
- en: '[26] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable
    mdps. arXiv preprint arXiv:1507.06527, 2015.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Hausknecht 和 P. Stone. 深度递归Q学习用于部分可观察MDPs. arXiv预印本 arXiv:1507.06527,
    2015.'
- en: '[27] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez,
    Z. Wang, S. Eslami, et al. Emergence of locomotion behaviours in rich environments.
    arXiv preprint arXiv:1707.02286, 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T.
    Erez, Z. Wang, S. Eslami 等. 在丰富环境中出现的运动行为. arXiv预印本 arXiv:1707.02286, 2017.'
- en: '[28] Y. Heffetz, R. Vainshtein, G. Katz, and L. Rokach. Deepline: Automl tool
    for pipelines generation using deep reinforcement learning and hierarchical actions
    filtering. In ACM SIGMOD, pages 2103–2113, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Heffetz, R. Vainshtein, G. Katz 和 L. Rokach. Deepline: 使用深度强化学习和分层动作过滤的自动化工具生成管道.
    在ACM SIGMOD，页码 2103–2113, 2020.'
- en: '[29] J. Heitz and K. Stockinger. Join query optimization with deep reinforcement
    learning algorithms. arXiv preprint arXiv:1911.11689, 2019.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Heitz 和 K. Stockinger. 使用深度强化学习算法的联接查询优化. arXiv预印本 arXiv:1911.11689,
    2019.'
- en: '[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger.
    Deep reinforcement learning that matters. In AAAI, volume 32, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup 和 D. Meger. 重要的深度强化学习.
    在AAAI，第32卷, 2018.'
- en: '[31] B. Hilprecht, C. Binnig, and U. Röhm. Learning a partitioning advisor
    for cloud databases. In ACM SIGMOD, pages 143–157, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Hilprecht, C. Binnig, 和 U. Röhm. 为云数据库学习分区顾问。在 ACM SIGMOD, 页码 143–157,
    2020。'
- en: '[32] J. Ho and S. Ermon. Generative adversarial imitation learning. arXiv preprint
    arXiv:1606.03476, 2016.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Ho 和 S. Ermon. 生成对抗模仿学习。arXiv 预印本 arXiv:1606.03476, 2016。'
- en: '[33] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin, A. Wasay, B. Hentschel,
    M. Kester, N. Dayan, D. Guo, M. Kang, et al. Learning data structure alchemy.
    TCDE, 42(2), 2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin, A. Wasay, B. Hentschel,
    M. Kester, N. Dayan, D. Guo, M. Kang, 等. 学习数据结构炼金术。TCDE, 42(2), 2019。'
- en: '[34] L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In European
    conference on machine learning, pages 282–293. Springer, 2006.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] L. Kocsis 和 C. Szepesvári. 基于赌博的蒙特卡洛规划。在欧洲机器学习会议, 页码 282–293. Springer,
    2006。'
- en: '[35] T. Kraska, M. Alizadeh, A. Beutel, H. Chi, A. Kristo, G. Leclerc, S. Madden,
    H. Mao, and V. Nathan. Sagedb: A learned database system. In CIDR, 2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Kraska, M. Alizadeh, A. Beutel, H. Chi, A. Kristo, G. Leclerc, S. Madden,
    H. Mao, 和 V. Nathan. Sagedb：一个学习型数据库系统。在 CIDR, 2019。'
- en: '[36] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for
    learned index structures. In ACM SIGMOD, pages 489–504, 2018.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Kraska, A. Beutel, E. H. Chi, J. Dean, 和 N. Polyzotis. 学习的索引结构的案例。ACM
    SIGMOD, 页码 489–504, 2018。'
- en: '[37] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning
    to optimize join queries with deep reinforcement learning. arXiv preprint arXiv:1808.03196,
    2018.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, 和 I. Stoica. 使用深度强化学习优化连接查询。arXiv
    预印本 arXiv:1808.03196, 2018。'
- en: '[38] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum. Hierarchical
    deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.
    arXiv preprint arXiv:1604.06057, 2016.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, 和 J. B. Tenenbaum. 分层深度强化学习：整合时间抽象与内在动机。arXiv
    预印本 arXiv:1604.06057, 2016。'
- en: '[39] H. Lan, Z. Bao, and Y. Peng. An index advisor using deep reinforcement
    learning. In CIKM, pages 2105–2108, 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H. Lan, Z. Bao, 和 Y. Peng. 使用深度强化学习的索引顾问。在 CIKM, 页码 2105–2108, 2020。'
- en: '[40] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In
    Reinforcement learning, pages 45–73\. Springer, 2012.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Lange, T. Gabel, 和 M. Riedmiller. 批量强化学习。在《强化学习》，页码 45–73. Springer,
    2012。'
- en: '[41] C. Lee, Z. Luo, K. Y. Ngiam, M. Zhang, K. Zheng, G. Chen, B. Ooi, and
    J. Yip. Big healthcare data analytics: Challenges and applications. In Handbook
    of Large-Scale Distributed Computing in Smart Healthcare, 2017.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] C. Lee, Z. Luo, K. Y. Ngiam, M. Zhang, K. Zheng, G. Chen, B. Ooi, 和 J.
    Yip. 大型医疗数据分析：挑战与应用。在《智能医疗大规模分布式计算手册》，2017。'
- en: '[42] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann.
    How good are query optimizers, really? VLDB, 9(3):204–215, 2015.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, 和 T. Neumann. 查询优化器到底有多好？VLDB,
    9(3):204–215, 2015。'
- en: '[43] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement
    learning with gaussian processes. NIPS, 24:19–27, 2011.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Levine, Z. Popovic, 和 V. Koltun. 使用高斯过程的非线性逆强化学习。NIPS, 24:19–27, 2011。'
- en: '[44] G. Li, X. Zhou, S. Li, and B. Gao. Qtune: A query-aware database tuning
    system with deep reinforcement learning. VLDB, 12(12):2118–2130, 2019.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] G. Li, X. Zhou, S. Li, 和 B. Gao. Qtune：一个基于深度强化学习的查询感知数据库调优系统。VLDB, 12(12):2118–2130,
    2019。'
- en: '[45] T. Li, Z. Xu, J. Tang, and Y. Wang. Model-free control for distributed
    stream data processing using deep reinforcement learning. VLDB, 11, 2018.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Li, Z. Xu, J. Tang, 和 Y. Wang. 基于深度强化学习的分布式流数据处理的无模型控制。VLDB, 11, 2018。'
- en: '[46] W. Li, X. Li, H. Li, and G. Xie. Cutsplit: A decision-tree combining cutting
    and splitting for scalable packet classification. In IEEE INFOCOM 2018-IEEE Conference
    on Computer Communications, pages 2645–2653\. IEEE, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] W. Li, X. Li, H. Li, 和 G. Xie. Cutsplit：一个结合剪切和分割的决策树，用于可扩展的数据包分类。IEEE
    INFOCOM 2018-IEEE 计算机通信会议, 页码 2645–2653. IEEE, 2018。'
- en: '[47] E. Liang, H. Zhu, X. Jin, and I. Stoica. Neural packet classification.
    In ACM SIGCOMM, pages 256–269\. 2019.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] E. Liang, H. Zhu, X. Jin, 和 I. Stoica. 神经网络数据包分类。ACM SIGCOMM, 页码 256–269.
    2019。'
- en: '[48] X. Liang, A. J. Elmore, and S. Krishnan. Opportunistic view materialization
    with deep reinforcement learning. arXiv preprint arXiv:1903.01363, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Liang, A. J. Elmore, 和 S. Krishnan. 使用深度强化学习的机会视图物化。arXiv 预印本 arXiv:1903.01363,
    2019。'
- en: '[49] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning.
    arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra. 使用深度强化学习的连续控制。arXiv 预印本 arXiv:1509.02971, 2015。'
- en: '[50] L.-J. Lin. Self-improving reactive agents based on reinforcement learning,
    planning and teaching. Machine learning, 8(3-4):293–321, 1992.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] L.-J. Lin. 基于强化学习、规划和教学的自我改进反应性智能体。《机器学习》，8(3-4):293–321, 1992。'
- en: '[51] Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, and O. Farri.
    Diagnostic inferencing via improving clinical concept extraction with deep reinforcement
    learning: A preliminary study. In MLHC, 2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, 和 O. Farri.
    通过改进临床概念提取的诊断推理与深度强化学习：初步研究。在 MLHC, 2017。'
- en: '[52] K. Liu, Y. Fu, P. Wang, L. Wu, R. Bo, and X. Li. Automating feature subspace
    exploration via multi-agent reinforcement learning. In ACM SIGKDD, pages 207–215,
    2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] K. Liu, Y. Fu, P. Wang, L. Wu, R. Bo, 和 X. Li. 通过多智能体强化学习自动化特征子空间探索。在
    ACM SIGKDD 上，页码 207–215, 2019。'
- en: '[53] Z. Luo, S. H. Yeung, M. Zhang, K. Zheng, L. Zhu, G. Chen, F. Fan, Q. Lin,
    K. Y. Ngiam, and B. Chin Ooi. Mlcask: Efficient management of component evolution
    in collaborative data analytics pipelines. In ICDE, pages 1655–1666, 2021.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Luo, S. H. Yeung, M. Zhang, K. Zheng, L. Zhu, G. Chen, F. Fan, Q. Lin,
    K. Y. Ngiam, 和 B. Chin Ooi. Mlcask: 在协作数据分析管道中高效管理组件演化。在 ICDE 上，页码 1655–1666,
    2021。'
- en: '[54] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim. Applications of deep reinforcement learning in communications and networking:
    A survey. IEEE Communications Surveys & Tutorials, 21(4):3133–3174, 2019.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, 和
    D. I. Kim. 深度强化学习在通信和网络中的应用：综述。《IEEE 通信调查与教程》，21(4):3133–3174, 2019。'
- en: '[55] Y. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest
    neighbor search using hierarchical navigable small icde graphs. PAMI, 42(4):824–836,
    2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. A. Malkov 和 D. A. Yashunin. 使用层次导航小图进行高效且稳健的近似最近邻搜索。PAMI, 42(4):824–836,
    2018。'
- en: '[56] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, A. Hung Byers,
    et al. Big data: The next frontier for innovation, competition, and productivity.
    McKinsey Global Institute, 2011.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, A. Hung
    Byers 等. 大数据：创新、竞争和生产力的下一个前沿。麦肯锡全球研究院，2011。'
- en: '[57] H. Mao, P. Negi, A. Narayan, H. Wang, J. Yang, H. Wang, R. Marcus, R. Addanki,
    M. Khani Shirkoohi, S. He, et al. Park: An open platform for learning-augmented
    computer systems. NIPS, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Mao, P. Negi, A. Narayan, H. Wang, J. Yang, H. Wang, R. Marcus, R.
    Addanki, M. Khani Shirkoohi, S. He 等. Park: 一个用于学习增强计算机系统的开放平台。NIPS, 2019。'
- en: '[58] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, and M. Alizadeh.
    Learning scheduling algorithms for data processing clusters. In ACM SIGCOMM, pages
    270–288\. 2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, 和 M. Alizadeh.
    为数据处理集群学习调度算法。在 ACM SIGCOMM 上，页码 270–288, 2019。'
- en: '[59] H. Mao, S. B. Venkatakrishnan, M. Schwarzkopf, and M. Alizadeh. Variance
    reduction for reinforcement learning in input-driven environments. In ICLR, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Mao, S. B. Venkatakrishnan, M. Schwarzkopf, 和 M. Alizadeh. 在输入驱动环境中用于强化学习的方差减少。在
    ICLR, 2019。'
- en: '[60] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao:
    Making learned query optimization practical. In ACM SIGMOD, pages 1275–1288, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, 和 T. Kraska. Bao:
    使学习的查询优化变得实用。在 ACM SIGMOD 上，页码 1275–1288, 2021。'
- en: '[61] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join order
    enumeration. In Proceedings of the First International Workshop on Exploiting
    Artificial Intelligence Techniques for Data Management, pages 1–4, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. Marcus 和 O. Papaemmanouil. 用于联接顺序枚举的深度强化学习。在第一次国际人工智能技术数据管理研讨会上，页码
    1–4, 2018。'
- en: '[62] F. D. Matthew, H. Igor, and B. Paul. Machine learning in finance: From
    theory to practice, 2021.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] F. D. Matthew, H. Igor, 和 B. Paul. 金融中的机器学习：从理论到实践, 2021。'
- en: '[63] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman,
    D. Tsai, M. Amde, S. Owen, et al. Mllib: Machine learning in apache spark. The
    Journal of Machine Learning Research, 17(1):1235–1241, 2016.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J.
    Freeman, D. Tsai, M. Amde, S. Owen 等. Mllib: Apache Spark 中的机器学习。《机器学习研究期刊》，17(1):1235–1241,
    2016。'
- en: '[64] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML,
    pages 1928–1937\. PMLR, 2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu. 深度强化学习的异步方法。在 ICML 上，页码 1928–1937, PMLR, 2016。'
- en: '[65] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
    arXiv:1312.5602, 2013.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    和 M. Riedmiller. 用深度强化学习玩 Atari 游戏。arXiv 预印本 arXiv:1312.5602, 2013。'
- en: '[66] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control
    through deep reinforcement learning. nature, 518(7540):529–533, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski，等。通过深度强化学习实现的人类级控制。nature，518(7540)：529–533，2015
    年。'
- en: '[67] K. Narasimhan, A. Yala, and R. Barzilay. Improving information extraction
    by acquiring external evidence with reinforcement learning. arXiv preprint arXiv:1603.07954,
    2016.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] K. Narasimhan, A. Yala 和 R. Barzilay. 通过强化学习获取外部证据来改进信息提取。arXiv 预印本 arXiv:1603.07954，2016
    年。'
- en: '[68] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning.
    In ICML, volume 1, page 2, 2000.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Y. Ng, S. J. Russell, 等。用于逆强化学习的算法。在 ICML，第 1 卷，页码 2，2000 年。'
- en: '[69] M.-h. Oh and G. Iyengar. Sequential anomaly detection using inverse reinforcement
    learning. In ACM SIGMOD, pages 1480–1490, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M.-h. Oh 和 G. Iyengar. 使用逆强化学习进行顺序异常检测。在 ACM SIGMOD，页码 1480–1490，2019
    年。'
- en: '[70] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin:
    a not-so-foreign language for data processing. In ACM SIGMOD, pages 1099–1110,
    2008.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Olston, B. Reed, U. Srivastava, R. Kumar 和 A. Tomkins. Pig latin：一个并不陌生的数据处理语言。在
    ACM SIGMOD，页码 1099–1110，2008 年。'
- en: '[71] B. C. Ooi, R. Sacks-Davis, and J. Han. Indexing in spatial databases.
    Unpublished/Technical Papers, 1993.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. C. Ooi, R. Sacks-Davis 和 J. Han. 空间数据库中的索引。未发表/技术论文，1993 年。'
- en: '[72] C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song. Assessing
    generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282,
    2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun 和 D. Song. 评估深度强化学习的泛化能力。arXiv
    预印本 arXiv:1810.12282，2018 年。'
- en: '[73] G. Pang, A. van den Hengel, C. Shen, and L. Cao. Toward deep supervised
    anomaly detection: Reinforcement learning from partially labeled anomaly data.
    In ACM SIGKDD, pages 1298–1308, 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] G. Pang, A. van den Hengel, C. Shen 和 L. Cao. 朝向深度监督异常检测：从部分标记的异常数据中学习强化学习。在
    ACM SIGKDD，页码 1298–1308，2021 年。'
- en: '[74] L. L. Perez and C. M. Jermaine. History-aware query optimization with
    materialized intermediate views. In ICDE, pages 520–531, 2014.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. L. Perez 和 C. M. Jermaine. 历史感知查询优化与素材化中间视图。在 ICDE，页码 520–531，2014
    年。'
- en: '[75] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen,
    T. Asfour, P. Abbeel, and M. Andrychowicz. Parameter space noise for exploration.
    arXiv preprint arXiv:1706.01905, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen,
    T. Asfour, P. Abbeel 和 M. Andrychowicz. 用于探索的参数空间噪声。arXiv 预印本 arXiv:1706.01905，2017
    年。'
- en: '[76] E. Puiutta and E. M. Veith. Explainable reinforcement learning: A survey.
    In CD-MAKE, pages 77–95\. Springer, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] E. Puiutta 和 E. M. Veith. 可解释的强化学习：一项调查。在 CD-MAKE，页码 77–95。Springer，2020
    年。'
- en: '[77] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist
    systems, volume 37. University of Cambridge, Department of Engineering Cambridge,
    UK, 1994.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] G. A. Rummery 和 M. Niranjan. 在线 Q-learning 使用 connectionist 系统，第 37 卷。剑桥大学，剑桥工程学系，1994
    年。'
- en: '[78] Z. Sadri, L. Gruenwald, and E. Lead. Drlindex: deep reinforcement learning
    index advisor for a cluster database. In Proceedings of the 24th Symposium on
    International Database Engineering & Applications, pages 1–8, 2020.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. Sadri, L. Gruenwald 和 E. Lead. Drlindex：用于集群数据库的深度强化学习指数顾问。在第 24 届国际数据库工程和应用研讨会论文集，页码
    1–8，2020 年。'
- en: '[79] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience
    replay. arXiv preprint arXiv:1511.05952, 2015.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Schaul, J. Quan, I. Antonoglou 和 D. Silver. 优先经验重播。arXiv 预印本 arXiv:1511.05952，2015
    年。'
- en: '[80] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region
    policy optimization. In ICML, pages 1889–1897\. PMLR, 2015.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz. 信任区域策略优化。在 ICML，页码
    1889–1897。PMLR，2015 年。'
- en: '[81] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal
    policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov. 近端策略优化算法。arXiv
    预印本 arXiv:1707.06347，2017 年。'
- en: '[82] B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal,
    and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE,
    109(5):612–634, 2021.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal
    和 Y. Bengio. 朝向因果表示学习。IEEE 论文集，109(5)：612–634，2021 年。'
- en: '[83] W. Shang, Y. Yu, Q. Li, Z. Qin, Y. Meng, and J. Ye. Environment reconstruction
    with hidden confounders for reinforcement learning based recommendation. In ACM
    SIGKDD, pages 566–576, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] W. Shang, Y. Yu, Q. Li, Z. Qin, Y. Meng 和 J. Ye. 用于基于强化学习的推荐的隐含混杂因素的环境重建。在
    ACM SIGKDD，页码 566–576，2019 年。'
- en: '[84] A. Sharma, F. M. Schuhknecht, and J. Dittrich. The case for automatic
    database administration using deep reinforcement learning. arXiv preprint arXiv:1801.05643,
    2018.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. 沙尔马，F. M. 舒赫克内赫特 和 J. 迪特里希。使用深度强化学习进行自动化数据库管理的案例。arXiv预印本 arXiv:1801.05643，2018年。'
- en: '[85] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering
    the game of go with deep neural networks and tree search. nature, 529(7587):484–489,
    2016.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] D. 银，A. 黄，C. J. 马迪森，A. 盖兹，L. 西弗，G. 范·登·德里谢赫，J. 施里特维瑟，I. 安托诺格鲁，V. 潘内谢尔瓦姆，M.
    兰克托 等。利用深度神经网络和树搜索掌握围棋游戏。自然，529(7587):484–489，2016年。'
- en: '[86] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
    M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning
    algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144,
    2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. 银，T. 休伯特，J. 施里特维瑟，I. 安托诺格鲁，M. 莱，A. 盖兹，M. 兰克托，L. 西弗，D. 库马兰，T. 格雷佩尔 等。一个通用的强化学习算法，通过自我对弈掌握国际象棋，将棋和围棋。科学，362(6419):1140–1144，2018年。'
- en: '[87] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller.
    Deterministic policy gradient algorithms. In ICML, pages 387–395\. PMLR, 2014.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. 银，G. 莱弗，N. 希斯，T. 德格里斯，D. 维尔斯特拉 和 M. 里德米勒。确定性策略梯度算法。在ICML，第387–395页。PMLR，2014年。'
- en: '[88] D. Silver, S. Singh, D. Precup, and R. S. Sutton. Reward is enough. Artificial
    Intelligence, page 103535, 2021.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. 银，S. 辛格，D. 普雷库普，和 R. S. 萨顿。奖励足够了。人工智能，第103535页，2021年。'
- en: '[89] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. S. 萨顿 和 A. G. 巴托。强化学习：导论。MIT出版社，2018年。'
- en: '[90] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient
    methods for reinforcement learning with function approximation. In NIPS, pages
    1057–1063, 2000.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] R. S. 萨顿，D. A. 麦卡利斯特，S. P. 辛格 和 Y. 曼索尔。用于函数逼近的策略梯度方法。NIPS，第1057–1063页，2000年。'
- en: '[91] R. Takanobu, T. Zhuang, M. Huang, J. Feng, H. Tang, and B. Zheng. Aggregating
    e-commerce search results from heterogeneous sources via hierarchical reinforcement
    learning. In WWW, pages 1771–1781, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. 高信，T. 庄，M. 黄，J. 风，H. 唐 和 B. 郑。通过层次化强化学习从异构来源聚合电子商务搜索结果。在WWW，第1771–1781页，2019年。'
- en: '[92] W. R. Thompson. On the likelihood that one unknown probability exceeds
    another in view of the evidence of two samples. Biometrika, 25:285–294, 1933.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] W. R. 汤普森。基于两个样本的证据，一个未知概率超过另一个的可能性。生物计量学，25:285–294，1933年。'
- en: '[93] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
    P. Wyckoff, and R. Murthy. Hive: a warehousing solution over a map-reduce framework.
    VLDB, 2(2):1626–1629, 2009.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. 塔苏，J. S. 萨尔马，N. 贾因，Z. 邵，P. 查卡，S. 安东尼，H. 刘，P. 威科夫 和 R. 穆尔提。Hive：基于Map-Reduce框架的数据仓储解决方案。VLDB，2(2):1626–1629，2009年。'
- en: '[94] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis.
    Skinnerdb: Regret-bounded query evaluation via reinforcement learning. In ACM
    SIGMOD, pages 1153–1170, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] I. 特鲁默，J. 王，D. 马拉姆，S. 摩斯利，S. 乔 和 J. 安托纳卡基斯。Skinnerdb：通过强化学习实现的有悔界限查询评估。在ACM
    SIGMOD，第1153–1170页，2019年。'
- en: '[95] K. Tzoumas, T. Sellis, and C. S. Jensen. A reinforcement learning approach
    for adaptive query processing. History, 2008.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] K. 佐马斯，T. 塞利斯 和 C. S. 詹森。一种用于自适应查询处理的强化学习方法。历史，2008年。'
- en: '[96] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion.
    Physical review, 36:823, 1930.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] G. E. 乌伦贝克 和 L. S. 奥恩斯坦。关于布朗运动的理论。物理评论，36:823，1930年。'
- en: '[97] B. Vamanan, G. Voskuilen, and T. Vijaykumar. Efficuts: Optimizing packet
    classification for memory and throughput. ACM SIGCOMM, 40(4):207–218, 2010.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] B. 瓦马南，G. 沃斯库伦 和 T. 维贾伊库马尔。Efficuts：优化内存和吞吐量的包分类。ACM SIGCOMM，40(4):207–218，2010年。'
- en: '[98] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic database
    management system tuning through large-scale machine learning. In ACM SIGMOD,
    pages 1009–1024, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] D. 范·阿肯，A. 帕夫洛，G. J. 戈登 和 B. 张。通过大规模机器学习进行自动化数据库管理系统调优。在ACM SIGMOD，第1009–1024页，2017年。'
- en: '[99] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with
    double q-learning. In AAAI, volume 30, 2016.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. 范·哈斯尔特，A. 盖兹 和 D. 银。结合双重Q学习的深度强化学习。在AAAI，第30卷，2016年。'
- en: '[100] H. Wang, H. He, M. Alizadeh, and H. Mao. Learning caching policies with
    subsampling. In NeurIPS Machine Learning for Systems Workshop, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. 王，H. 何，M. 阿里扎德 和 H. 毛。通过子采样学习缓存策略。NeurIPS机器学习系统研讨会，2019年。'
- en: '[101] J. Wang, D. Ding, H. Wang, C. Christensen, Z. Wang, H. Chen, and J. Li.
    Polyjuice: High-performance transactions via learned concurrency control. In OSDI,
    pages 198–216, July 2021.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] J. 王，D. 丁，H. 王，C. 克里斯滕森，Z. 王，H. 陈，和 J. 李。Polyjuice：通过学习的并发控制实现高性能事务。在OSDI，第198–216页，2021年7月。'
- en: '[102] L. Wang, Q. Weng, W. Wang, C. Chen, and B. Li. Metis: learning to schedule
    long-running applications in shared container clusters at scale. In SC20, pages
    1–17\. IEEE, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] L. Wang, Q. Weng, W. Wang, C. Chen, 和 B. Li. Metis：在共享容器集群中大规模调度长期运行应用的学习。发表于SC20，第1–17页。IEEE，2020年。'
- en: '[103] L. Wang, W. Zhang, X. He, and H. Zha. Supervised reinforcement learning
    with recurrent neural network for dynamic treatment recommendation. In ACM SIGKDD,
    pages 2447–2456, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Wang, W. Zhang, X. He, 和 H. Zha. 使用递归神经网络的监督强化学习用于动态治疗推荐。发表于ACM SIGKDD，第2447–2456页，2018年。'
- en: '[104] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental
    comparison of graph-based approximate nearest neighbor search. arXiv preprint
    arXiv:2101.12631, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Wang, X. Xu, Q. Yue, 和 Y. Wang. 基于图的近似最近邻搜索的综合调查和实验比较。arXiv预印本 arXiv:2101.12631，2021年。'
- en: '[105] P. Wang, K. Liu, L. Jiang, X. Li, and Y. Fu. Incremental mobile user
    profiling: Reinforcement learning with spatial knowledge graph for modeling event
    streams. In ACM SIGKDD, pages 853–861, 2020.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] P. Wang, K. Liu, L. Jiang, X. Li, 和 Y. Fu. 增量移动用户画像：结合空间知识图的强化学习用于建模事件流。发表于ACM
    SIGKDD，第853–861页，2020年。'
- en: '[106] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J. Shao,
    and M. Reyad. Rafiki: machine learning as an analytics service system. VLDB, 12(2):128–140,
    2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J.
    Shao, 和 M. Reyad. Rafiki：作为分析服务系统的机器学习。VLDB，12(2):128–140，2018年。'
- en: '[107] W. Wang, M. Zhang, G. Chen, H. Jagadish, B. C. Ooi, and K.-L. Tan. Database
    meets deep learning: Challenges and opportunities. ACM SIGMOD Record, 45(2):17–22,
    2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] W. Wang, M. Zhang, G. Chen, H. Jagadish, B. C. Ooi, 和 K.-L. Tan. 数据库遇上深度学习：挑战与机遇。ACM
    SIGMOD记录，45(2):17–22，2016年。'
- en: '[108] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas.
    Dueling network architectures for deep reinforcement learning. In ICML, pages
    1995–2003\. PMLR, 2016.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, 和 N. Freitas.
    深度强化学习的对抗网络架构。发表于ICML，第1995–2003页。PMLR，2016年。'
- en: '[109] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292,
    1992.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. J. Watkins 和 P. Dayan. Q学习。机器学习，8(3-4):279–292，1992年。'
- en: '[110] J. Welborn, M. Schaarschmidt, and E. Yoneki. Learning index selection
    with structured action spaces. arXiv preprint arXiv:1909.07440, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Welborn, M. Schaarschmidt, 和 E. Yoneki. 具有结构化动作空间的索引选择学习。arXiv预印本
    arXiv:1909.07440，2019年。'
- en: '[111] N. Wu and P. Li. Phoebe: Reuse-aware online caching with reinforcement
    learning for emerging storage models. arXiv preprint arXiv:2011.07160, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] N. Wu 和 P. Li. Phoebe：使用强化学习进行在线缓存以适应新兴存储模型。arXiv预印本 arXiv:2011.07160，2020年。'
- en: '[112] S. Wu, X. Yu, X. Feng, F. Li, W. Cao, and G. Chen. Progressive neural
    index search for database system. arXiv preprint arXiv:1912.07001, 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. Wu, X. Yu, X. Feng, F. Li, W. Cao, 和 G. Chen. 数据库系统的渐进神经索引搜索。arXiv预印本
    arXiv:1912.07001，2019年。'
- en: '[113] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries from
    natural language without reinforcement learning. arXiv preprint arXiv:1711.04436,
    2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. Xu, C. Liu, 和 D. Song. Sqlnet：从自然语言生成结构化查询，无需强化学习。arXiv预印本 arXiv:1711.04436，2017年。'
- en: '[114] S. Y. Yang, Q. Qiao, P. A. Beling, W. T. Scherer, and A. A. Kirilenko.
    Gaussian process-based algorithmic trading strategy identification. Quantitative
    Finance, 15(10):1683–1703, 2015.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Y. Yang, Q. Qiao, P. A. Beling, W. T. Scherer, 和 A. A. Kirilenko.
    基于高斯过程的算法交易策略识别。定量金融，15(10):1683–1703，2015年。'
- en: '[115] S. Y. Yang, Y. Yu, and S. Almahdi. An investor sentiment reward-based
    trading system using gaussian inverse reinforcement learning algorithm. Expert
    Systems with Applications, 114:388–401, 2018.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] S. Y. Yang, Y. Yu, 和 S. Almahdi. 使用高斯逆强化学习算法的投资者情绪奖励交易系统。专家系统应用，114:388–401，2018年。'
- en: '[116] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li, U. F. Minhas, P.-Å.
    Larson, D. Kossmann, and R. Acharya. Qd-tree: Learning data layouts for big data
    analytics. In ACM SIGMOD, pages 193–208, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li, U. F. Minhas, P.-Å.
    Larson, D. Kossmann, 和 R. Acharya. Qd-tree：为大数据分析学习数据布局。发表于ACM SIGMOD，第193–208页，2020年。'
- en: '[117] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional
    policy network for goal-directed molecular graph generation. In NIPS, pages 6412–6422,
    2018.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. You, B. Liu, R. Ying, V. Pande, 和 J. Leskovec. 用于目标导向分子图生成的图卷积策略网络。发表于NIPS，第6412–6422页，2018年。'
- en: '[118] C. Yu, J. Liu, and S. Nemati. Reinforcement learning in healthcare: A
    survey. arXiv preprint, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] C. Yu, J. Liu, 和 S. Nemati. 医疗保健中的强化学习：一项调查。arXiv预印本，2019年。'
- en: '[119] X. Yu, G. Li, C. Chai, and N. Tang. Reinforcement learning with tree-lstm
    for join order selection. In ICDE, pages 1297–1308\. IEEE, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. Yu, G. Li, C. Chai, 和 N. Tang. 使用树状LSTM的强化学习用于联接顺序选择。发表于ICDE，第1297–1308页。IEEE，2020年。'
- en: '[120] X. Yu, Y. Peng, F. Li, S. Wang, X. Shen, H. Mai, and Y. Xie. Two-level
    data compression using machine learning in time series database. In ICDE, pages
    1333–1344\. IEEE, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] X. Yu, Y. Peng, F. Li, S. Wang, X. Shen, H. Mai, 和 Y. Xie。基于机器学习的时间序列数据库双层数据压缩。在
    ICDE 会议上，页面 1333–1344。IEEE，2020年。'
- en: '[121] H. Yuan, G. Li, L. Feng, J. Sun, and Y. Han. Automatic view generation
    with deep learning and reinforcement learning. In ICDE, pages 1501–1512\. IEEE,
    2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] H. Yuan, G. Li, L. Feng, J. Sun, 和 Y. Han。利用深度学习和强化学习自动生成视图。在 ICDE 会议上，页面
    1501–1512。IEEE，2020年。'
- en: '[122] C. Zhang, R. Marcus, A. Kleiman, and O. Papaemmanouil. Buffer pool aware
    query scheduling via deep reinforcement learning. arXiv preprint arXiv:2007.10568,
    2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Zhang, R. Marcus, A. Kleiman, 和 O. Papaemmanouil。通过深度强化学习的缓冲池感知查询调度。arXiv
    预印本 arXiv:2007.10568，2020年。'
- en: '[123] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang,
    T. Cheng, L. Liu, et al. An end-to-end automatic cloud database tuning system
    using deep reinforcement learning. In ACM SIGMOD, pages 415–432, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang,
    T. Cheng, L. Liu, 等。使用深度强化学习的端到端自动云数据库调优系统。在 ACM SIGMOD 会议上，页面 415–432，2019年。'
- en: '[124] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He. Deep reinforcement learning
    for sponsored search real-time bidding. In ACM SIGKDD, pages 1021–1030, 2018.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Zhao, G. Qiu, Z. Guan, W. Zhao, 和 X. He。深度强化学习在赞助搜索实时竞标中的应用。在 ACM
    SIGKDD 会议上，页面 1021–1030，2018年。'
- en: '[125] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries
    from natural language using reinforcement learning. arXiv preprint, 2017.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] V. Zhong, C. Xiong, 和 R. Socher。Seq2sql：使用强化学习从自然语言生成结构化查询。arXiv 预印本，2017年。'
- en: '[126] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning.
    arXiv preprint arXiv:1611.01578, 2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] B. Zoph 和 Q. V. Le。使用强化学习的神经网络架构搜索。arXiv 预印本 arXiv:1611.01578，2016年。'
- en: '[127] J. Zou, P. Barhate, A. Das, A. Iyengar, B. Yuan, D. Jankov, and C. Jermaine.
    Lachesis: Automated generation of persistent partitionings for big data applications.
    VLDB, 14(8), 2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Zou, P. Barhate, A. Das, A. Iyengar, B. Yuan, D. Jankov, 和 C. Jermaine。Lachesis：大数据应用的持久分区自动生成。VLDB,
    14(8), 2021年。'
