- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:52:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2108.04526] A Survey on Deep Reinforcement Learning for Data Processing and
    Analytics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04526](https://ar5iv.labs.arxiv.org/html/2108.04526)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Reinforcement Learning for Data Processing and Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data processing and analytics are fundamental and pervasive. Algorithms play
    a vital role in data processing and analytics where many algorithm designs have
    incorporated heuristics and general rules from human knowledge and experience
    to improve their effectiveness. Recently, reinforcement learning, deep reinforcement
    learning (DRL) in particular, is increasingly explored and exploited in many areas
    because it can learn better strategies in complicated environments it is interacting
    with than statically designed algorithms. Motivated by this trend, we provide
    a comprehensive review of recent works focusing on utilizing DRL to improve data
    processing and analytics. First, we present an introduction to key concepts, theories,
    and methods in DRL. Next, we discuss DRL deployment on database systems, facilitating
    data processing and analytics in various aspects, including data organization,
    scheduling, tuning, and indexing. Then, we survey the application of DRL in data
    processing and analytics, ranging from data preparation, natural language processing
    to healthcare, fintech, etc. Finally, we discuss important open challenges and
    future research directions of using DRL in data processing and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Deep Reinforcement Learning for Data Processing and Analytics
  prefs: []
  type: TYPE_NORMAL
- en: Qingpeng Cai^(∗†), Can Cui^(∗†), Yiyuan Xiong^(∗†)^†^†^∗These authors have contributed
    equally to this work, and M. Zhang is the contact author., Wei Wang^†,
  prefs: []
  type: TYPE_NORMAL
- en: Zhongle Xie^§, Meihui Zhang^‡
  prefs: []
  type: TYPE_NORMAL
- en: ^†National University of Singapore    ^§ Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: ^‡Beijing Institute of Techonology
  prefs: []
  type: TYPE_NORMAL
- en: '{qingpeng, cuican, yiyuan, wangwei}@comp.nus.edu.sg   xiezl@zju.edu.cn   meihui_zhang@bit.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the age of big data, data processing and analytics are fundamental, ubiquitous,
    and crucial to many organizations which undertake a digitalization journey to
    improve and transform their businesses and operations. Data analytics typically
    entails other key operations such as data acquisition, data cleansing, data integration,
    modeling, etc., before insights could be extracted. Big data can unleash significant
    value creation across many sectors such as healthcare and retail [[56](#bib.bib56)].
    However, the complexity of data (e.g., high volume, high velocity, and high variety)
    presents many challenges in data analytics and hence renders the difficulty in
    drawing meaningful insights. To tackle the challenge and facilitate the data processing
    and analytics efficiently and effectively, a large number of algorithms and techniques
    have been designed and numerous learning systems have also been developed by researchers
    and practitioners such as Spark MLlib [[63](#bib.bib63)], and Rafiki [[106](#bib.bib106)].
  prefs: []
  type: TYPE_NORMAL
- en: To support fast data processing and accurate data analytics, a huge number of
    algorithms rely on rules that are developed based on human knowledge and experience.
    For example, shortest-job-first is a scheduling algorithm that chooses the job
    with the smallest execution time for the next execution. However, without fully
    exploiting characteristics of the workload, it can achieve inferior performance
    compared to a learning-based scheduling algorithm [[58](#bib.bib58)]. Another
    example is packet classification in computer networking which matches a packet
    to a rule from a set of rules. One solution is to construct the decision tree
    using hand-tuned heuristics for classification. Specifically, the heuristics are
    designed for a particular set of rules and thus may not work well for other workloads
    with different characteristics [[47](#bib.bib47)]. We observe three limitations
    of existing algorithms [[97](#bib.bib97), [46](#bib.bib46)]. First, the algorithms
    are suboptimal. Useful information such as data distribution could be overlooked
    or not fully exploited by the rules. Second, the algorithm lacks adaptivity. Algorithms
    designed for a specific workload cannot perform well in another different workload.
    Third, the algorithm design is a time-consuming process. Developers have to spend
    much time trying a lot of rules to find one that empirically works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning-based algorithms have also been studied for data processing and analytics.
    Two types of learning methods are often used: supervised learning and reinforcement
    learning. They achieve better performance by direct optimization of the performance
    objective. Supervised learning typically requires a rich set of high-quality labeled
    training data, which could be hard and challenging to acquire. For example, configuration
    tuning is important to optimize the overall performance of a database management
    system (DBMS)[[44](#bib.bib44)]. There could be hundreds of tuning knobs that
    are correlated in discrete and continuous space. Furthermore, diverse database
    instances, query workloads, hardware characteristics render data collection infeasible,
    especially in the cloud environment. Compared to supervised learning, reinforcement
    learning shows good performance because it adopts a trial-and-error search and
    requires fewer training samples to find good configuration for cloud databases [[123](#bib.bib123)].
    Another specific example would be query optimization in query processing. Database
    system optimizers are tasked to find the best execution plan for a query to reduce
    its query cost. Traditional optimizers typically enumerate many candidate plans
    and use a cost model to find the plan with minimal cost. The optimization process
    could be slow and inaccurate [[42](#bib.bib42)]. Without relying on an inaccurate
    cost model, deep reinforcement learning (DRL) methods improve the execution plan
    (e.g., changing the table join orders) by interacting with the database[[61](#bib.bib61),
    [37](#bib.bib37)]. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on
    Deep Reinforcement Learning for Data Processing and Analytics") provides a typical
    workflow for query optimization using DRL. When the query is sent to the agent
    (i.e., DRL optimizer), it produces a state vector via conducting featurization
    on essential information, such as the accessed relations and tables. Taking the
    state as the input, the agent employs neural networks to produce the probability
    distribution of an action set, where the action set could contain all possible
    join operations as potential actions. Each action denotes a partial join plan
    on a pair of tables, and the state will be updated once an action is taken. After
    taking possible actions, a complete plan is generated, which is then executed
    by a DBMS to get the reward. In this query optimization problem, the reward can
    be calculated by the real latency. During the training process with the reward
    signal, the agent can improve the policy and produce a better join ordering with
    a higher reward (i.e., less latency).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90259969a325225ecd657f75199786c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Workflow of DRL for Query Optimization. A, B, C and D are four
    tables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement learning (RL) [[89](#bib.bib89)] focuses on learning to make
    intelligent actions in an environment. The RL algorithm works on the basis of
    exploration and exploitation to improve itself with feedback from the environment.
    In the past decades, RL has achieved tremendous improvements in both theoretical
    and technical aspects [[86](#bib.bib86), [89](#bib.bib89)]. Notably, DRL incorporates
    deep learning (DL) techniques to handle complex unstructured data and has been
    designed to learn from historical data and self-exploration to solve notoriously
    hard and large-scale problems (e.g., AlphaGo[[85](#bib.bib85)]). In recent years,
    researchers from different communities have proposed DRL solutions to address
    issues in data processing and analytics[[116](#bib.bib116), [58](#bib.bib58),
    [52](#bib.bib52)]. We categorize existing works using DRL from two perspectives:
    system and application. From the system’s perspective, we focus on fundamental
    research topics ranging from general ones, such as scheduling, to system-specific
    ones, such as query optimization in databases. We shall also emphasize how it
    is formulated in the Markov Decision Process and discuss how the problem can be
    solved by DRL more effectively compared to traditional methods. Many techniques
    such as sampling and simulation are adopted to improve DRL training efficiency
    because workload execution and data collection in the real system could be time-consuming [[31](#bib.bib31)].
    From the application’s perspective, we shall cover various key applications in
    both data processing and data analytics to provide a comprehensive understanding
    of the DRL’s usability and adaptivity. Many domains are transformed by the adoption
    of DRL, which helps to learn domain-specific knowledge about the applications.'
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we aim at providing a broad and systematic review of recent
    advancements in employing DRL in solving data systems, data processing and analytics
    issues. In Section 2, we introduce the key concepts, theories, and techniques
    in RL to lay the foundations. To gain a deeper understanding of DRL, readers could
    refer to the recently published book [[13](#bib.bib13)], which covers selected
    DRL research topics and applications with detailed illustrations. In Section 3,
    we review the latest important research works on using DRL for system optimization
    to support data processing and analytics. We cover fundamental topics such as
    data organization, scheduling, system tuning, index, query optimization, and cache
    management. In Section 4, we discuss using DRL for applications in data processing
    and analytics ranging from data preparation, natural language interaction to various
    real-world applications such as healthcare, fintech, E-commerce, etc. In Section
    5, we highlight various open challenges and potential research problems. We conclude
    in Section 6\. This survey focuses on recent advancements in exploring RL for
    data processing and analytics that spurs great interest, especially in the database
    and data mining community. There are survey papers discussing DRL for other domains.
    We refer readers to the survey of DRL for healthcare in [[118](#bib.bib118)],
    communications and networking in [[54](#bib.bib54)], and RL explainability in
    [[76](#bib.bib76)]. Another work[[107](#bib.bib107)] discusses how deep learning
    can be used to optimize database system design, and vice versa. In this paper,
    we use "DRL" and "RL" interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Theoretical Foundation and Algorithms of Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RL is targeted to solve the sequential decision making problem and the goal
    is to take actions with maximum expected rewards. In detail, the agent follows
    a policy to make a series of decisions (i.e. taking actions) in different states
    of the environment, and the sequence of the states and the actions form a trajectory.
    To estimate whether the policy is good or not, each decision under the policy
    will be evaluated by the accumulated rewards through the trajectory. After evaluating
    the policy from the trajectories, the agent next improves the policy by increasing
    the probabilities of making decisions with greater expected rewards. By repeating
    these steps, the agent can improve the policy through trial-and-error until the
    policy reaches the optimal, and such a sequential decision-making process is modeled
    via Markov Decision Process (MDP).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Markov Decision Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mathematically, MDP, shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    A Survey on Deep Reinforcement Learning for Data Processing and Analytics"), is
    a stochastic control process $\mathcal{M}$ defined by a tuple with 5 elements,
    $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma)$, which
    are explained as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State $\mathcal{S}$: $\mathcal{S}$ is the space for states that denote different
    situations in the environment and $s_{t}\in\mathcal{S}$ denotes the state of the
    situation at the time $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action $\mathcal{A}$: $\mathcal{A}$ is the space for actions that the agent
    can take; the actions can either be discrete or continuous, and $a_{t}\in\mathcal{A}$
    denotes the action taken at the time $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function $\mathcal{R}(s_{t},a_{t})$: It denotes the immediate reward
    of the action $a_{t}$ taken under the state $s_{t}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transition function $\mathcal{P}(s_{t+1}=s^{\prime}|s_{t}=s,a_{t}=a)$: It denotes
    the probability of transition to the state $s^{\prime}$ at the time $t+1$ given
    the current state $s$ and the taken action $a$ at the time $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discount factor $\gamma\in[0,1]$: The total rewards of a certain action consist
    of both immediate rewards and future rewards, and the $\gamma$ quantifies how
    much importance we give for future rewards.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We take the query optimization problem demonstrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Reinforcement Learning for Data Processing
    and Analytics") to help explain the five components of the MDP. In this example,
    the state is expressed as a state vector, which summarizes the information of
    relations and tables that are assessed by the query $q$. In each state, the RL
    agent produces a probability distribution over all potential actions where each
    action denotes a partial join plan on a pair of tables. After repeating these
    two processes, it reaches a terminal state where the final join ordering is generated
    for an agent to execute, and all actions’ target rewards are measured by the actual
    performance (i.e., latency) or a cost model. As for the transition function, the
    transitions of the states are always deterministic in both this problem and most
    of the other DB problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In RL, we aim to train the agent with a good policy $\pi$ that is a mapping
    function from state to action. Through the policy, the agent can take a series
    of actions that will result in continuous changes in the states, and the sequence
    of the states and the actions following the policy $\pi$ form a trajectory $\tau=(s_{0},a_{0},s_{1},a_{1},...)$.
    From each $\tau$, we can evaluate the effect of each action by the accumulated
    rewards $\mathcal{G}$, and it consists of the immediate reward of this action
    and the discounted rewards of its following actions in the trajectory. The total
    result $\mathcal{G}$ for the action $a_{t}$ is as follows: $\mathcal{G}(\tau)=\sum_{t=0}\gamma^{t}r_{t}$,
    where $\gamma$ quantifies how much importance we give for future rewards. With
    a bigger $\gamma$, the RL agent will be more likely to take any action that may
    have a less immediate reward at the current time but has a greater future reward
    in expectation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RL continuously evaluates the policy $\pi$ and improves it until it reaches
    the optimal policy $\pi^{*}=\arg\max_{(\tau\sim\pi)}\mathcal{G}(\tau)$ where the
    agent always takes actions that maximize the expected return. To evaluate the
    policy $\pi$, RL algorithms estimate how good or bad it is for a state and a state-action
    pair by the function $\mathcal{V}$ and function $\mathcal{Q}$ respectively. Both
    of these two value functions are calculated according to the discounted return
    $\mathcal{G}$ in expectation which can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s]$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=E_{\tau\sim\pi}[\mathcal{G}(\tau)&#124;s_{0}=s,a_{0}=a]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'These two value functions have a close association where the $\mathcal{V}^{\pi}(s_{t})$
    is the expectation of the function $\mathcal{Q}$ of all possible actions under
    the state $s_{t}$ according to the policy $\pi$, and the $\mathcal{Q}^{\pi}(s_{t},a_{t})$
    is the combination of the immediate reward of the action $a_{t}$ and the expectation
    of all possible states’ values after taking the action $a_{t}$ under the state
    $s_{t}$. Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=$ | $\displaystyle\sum_{a\in\mathcal{A}}\pi(a&#124;s)\mathcal{Q}^{\pi}(s,a)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)$ | $\displaystyle=$ | $\displaystyle
    R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Given a policy $\pi$, we can evaluate its value functions by Bellman equations [[89](#bib.bib89)]
    which utilize the recursive relationships of these value functions. Formally,
    Bellman equations deduce the relationships between a given state (i.e. function
    $\mathcal{V}$) or a given state-action pair (i.e. function $\mathcal{Q}$) and
    its successors which can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)=\sum_{a_{t}\in\mathcal{A}}\pi(a&#124;s)[R(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)\mathcal{V}^{\pi}(s^{\prime})]$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{Q}^{\pi}(s,a)=\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}(s^{\prime}&#124;s,a)[R(s,a)+\gamma\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}&#124;s^{\prime})\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})]$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'By iterating the Bellman equations, we can easily obtain the value functions
    for a policy, and to compare policies, we define that the policy $\pi$ is better
    than $\pi^{\prime}$ if the function $\mathcal{V}$ according to the $\pi$ is no
    less than the function $\mathcal{V}$ according to the $\pi^{\prime}$ for all states,
    that is $\mathcal{V}^{\pi}(s)\geq\mathcal{V}^{\pi^{\prime}}(s),\forall s$. It
    has been proven in [[89](#bib.bib89)] that the existence of the optimal policy
    $\pi^{*}$ is guaranteed in the MDP problem, where $\mathcal{V}^{*}(s)=\max_{\pi}\mathcal{V}^{\pi}(s)$
    and $\mathcal{Q}^{*}(s)=\max_{\pi}\mathcal{Q}^{\pi}(s)$. These two functions are
    defined as the optimal function $\mathcal{V}$ and the optimal function $\mathcal{Q}$.
    We can obtain the optimal policy $\pi^{*}$ by maximizing over the $\mathcal{Q}^{*}(\pi)$
    which can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi^{*}(a&#124;s)=\arg\max\mathcal{Q}^{*}(s,a)$ |  | (7)
    |'
  prefs: []
  type: TYPE_TB
- en: To improve the policy, we apply the Bellman optimality equations [[89](#bib.bib89)]
    to update value functions by taking the action with maximum value instead of trying
    all possible actions. To facilitate the optimization of the policy, many RL techniques
    are proposed from different perspectives, and Figure [2](#S2.F2 "Figure 2 ‣ 2.1
    Markov Decision Process ‣ 2 Theoretical Foundation and Algorithms of Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Data Processing and Analytics")
    provides a diagram outlining the broad categorization of these techniques, illustrating
    how these techniques can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3bde90649a892e748ee969c0a0cb66f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Broad categorization of RL techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Basic Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the representation of MDP elements, basic techniques can be categorized
    into two classes: model-based method and model-free method. The main difference
    is whether the agent has access to model the environment, i.e. whether the agent
    knows the transition function and the reward function. These two functions are
    already known in the model-based method where Dynamic Programming (DP)[[6](#bib.bib6)]
    and Alpha-Zero [[86](#bib.bib86)] are the classical methods which have achieved
    significant results in numerous applications. In these methods, agents are allowed
    to think ahead and plan future actions with known effects on the environment.
    Besides, an agent can learn the optimal policy from the planned experience which
    results in high sample efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In many RL problems, the reward and the transition function are typically unknown
    due to the complicated environment and its intricate inherent mechanism. For example,
    as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Reinforcement Learning for Data Processing and Analytics"), we are unable to obtain
    the actual latency as the reward in the joint query optimization example. Besides,
    in the stochastic job scheduling problem [[59](#bib.bib59)], it is also impossible
    to directly model the transition function because of the randomness of the job
    arrivals in the practical scenarios. Hence, in these problems, agents usually
    employ model-free methods that can purely learn the policy from the experience
    gained during the interaction with the environment. Model-free methods can mainly
    be classified into two categories, namely the value-based method and the policy-based
    method. In the value-based method, the RL algorithm learns the optimal policy
    by maximizing the value functions. There are two main approaches in estimating
    the value functions that are Mento-Carlo (MC) methods and Temporal difference
    (TD) methods. MC methods calculate the $\mathcal{V}(s)$ by directly applying its
    definition, that is Equation [1](#S2.E1 "In 2.1 Markov Decision Process ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"). MC methods can directly update the
    value functions once they get a new trajectory $\tau$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(\mathcal{G}_{\tau\sim\pi}(\tau&#124;s_{0}=s)-\mathcal{V}^{\pi}(s))$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha\in[0,1)$ denotes the learning rate which controls the rate of
    updating the policy with new experiences. However, it has an obvious drawback
    that a complete trajectory requires the agent to reach a terminal state, while
    it is not practical in some applications, such as online systems. Different from
    MC methods, the TD method builds on the recursive relationship of value functions,
    and hence, can learn from the incomplete trajectory. Mathematically, the update
    of TD methods can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)\leftarrow\mathcal{V}^{\pi}(s)+\alpha(R(s,a)+\gamma\mathcal{V}^{\pi}(s^{\prime})-\mathcal{V}^{\pi}(s))$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: However, there is bias when estimating the function $\mathcal{V}$ with TD methods
    because they learn from the recursive relationship. To reduce the bias, TD methods
    can extend the length of the incomplete trajectories and update the function $\mathcal{V}$
    by thinking more steps ahead, which is called $n$-steps TD methods. As $n$ grows
    to the length of whole trajectories, MC methods can be regarded as a special case
    of TD methods where function $\mathcal{V}$ is an unbiased estimate. On the other
    side of the coin, as the length $n$ increases, the variance of the trajectory
    also increases. In addition to the above consideration, TD-based methods are more
    efficient and require less storage and computation, thus they are more popular
    among RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In value-based methods, we can obtain the optimal policy by acting greedily
    via Equation [7](#S2.E7 "In 2.1 Markov Decision Process ‣ 2 Theoretical Foundation
    and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Data Processing and Analytics"). The update of the function $\mathcal{Q}$
    with TD methods is similar to the update of the function $\mathcal{V}$, and is
    as follows: $\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\mathcal{Q}^{\pi^{\prime}}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$
    where the agent follows the policy $\pi$ to take actions and follows the policy
    $\pi^{\prime}$ to maximize the function $\mathcal{Q}$. If the two policies are
    the same, that is $\pi^{\prime}=\pi$, we call such RL algorithms the on-policy
    methods where the SARSA[[77](#bib.bib77)] is the representative method. In addition,
    other policies can also be used in $\pi^{\prime}$. For example, in Q-learning[[109](#bib.bib109)],
    the agent applies the greedy policy and updates the function $\mathcal{Q}$ with
    the maximum value in its successor. Its update formula can be written as: $\mathcal{Q}^{\pi}(s,a)\leftarrow\mathcal{Q}^{\pi}(s,a)+\alpha(R(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}^{\pi}(s^{\prime},a^{\prime})-\mathcal{Q}^{\pi}(s,a))$.
    Both value-based methods can work well without the model of the environment, and
    Q-learning directly learns the optimal policy, whilst SARSA learns a near-optimal
    policy during exploring. Theoretically, Q-learning should converge quicker than
    SARSA, but its generated samples have a high variance which may suffer from the
    problems of converging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In RL, storage and computation costs are very high when there is a huge number
    of states or actions. To overcome this problem, DRL, as a branch of RL, adopts
    Deep Neural Network (DNN) to replace tabular representations with neural networks.
    For function $\mathcal{V}$, DNN takes the state $s$ as input and outputs its state
    value $\mathcal{V}_{\theta}(s)\approx\mathcal{V}^{\pi}(s)$ where the $\theta$
    denotes the parameter in the DNN. When comes to function $\mathcal{Q}$, It takes
    the combination of the state $s$ and the action $a$ as input and outputs the value
    of the state-action pair $\mathcal{Q}_{\theta}(s,a)\approx\mathcal{Q}^{\pi}(s,a)$,
    As for the neural networks, we can optimize them by applying the techniques that
    are widely used in deep learning (e.g. gradient descent). Deep Q-learning network
    (DQN) [[65](#bib.bib65)], as a representative method in DRL, combines the DNN
    with Q-learning and its loss function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{w}=\mathbf{E}_{\mathcal{D}}[(R(s,a)+\gamma\max_{a^{*}\in\mathcal{A}}\mathcal{Q}_{w}(s^{\prime},a^{*})-\mathcal{Q}_{w}(s,a))^{2}]$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{D}$ denotes the experience replay which accumulates the generated
    samples and can stabilize the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy-based methods are another branch of the model-free RL algorithm that
    have a clear representation of the policy $\pi(a|s)$, and they can tackle several
    challenges that are encountered in value-based methods. For example, when the
    action space is continuous, value-based methods need to discretize the action
    which could increase the dimensionality of the problem, and memory and computation
    consumption. Value-based methods learn a deterministic policy that generates the
    action given a state through an optimal function $\mathcal{Q}$ (i.e. $\pi(s)=a$).
    However, for policy-based methods, they can learn a stochastic policy (i.e. $\pi_{\theta}(a_{i}|s)=p_{i},\sum_{i}p_{i}=1$)
    as the optimal policy, where $p_{i}$ denotes the probability of taking the action
    $a_{i}$ given a state $s$, and $\theta$ denotes the parameters where neural networks
    can be used to approximate the policy. Policy Gradient [[90](#bib.bib90)] method
    is one of the main policy-based methods which can tackle the aforementioned challenges.
    Its goal is to optimize the parameters $\theta$ by using the gradient ascent method,
    and the target can be denoted in a generalized expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla_{\theta}J(\theta)=\mathbf{E}_{\tau\sim\pi_{\theta}}[R(\tau)\nabla_{\pi_{\theta}}\log_{\pi_{\theta}}(a&#124;s)]$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: The specific proof process can refer to [[89](#bib.bib89)]. Sampling via the
    MC methods, we will get the entire trajectories to improve the policy for the
    policy-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: After training, the action with higher rewards in expectation will have a higher
    probability to be chosen and vice versa. As for the continuous action, The optimal
    policy learned from the Policy Gradient is stochastic which still needs to be
    sampled to get the action. However, the stochastic policy still requires lots
    of samples to train the model when the search space is huge. Deterministic Policy
    Gradient (DPG) [[87](#bib.bib87)], as an extension of the Policy Gradient, overcomes
    this problem by using a stochastic policy to perform sampling while applying deterministic
    policy to output the action which demands relatively fewer samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both value-based methods and policy-based methods have their strengths and
    weaknesses, but they are not contradictory to each other. Actor-Critic (AC) method,
    as the integration of both methods, divides the model into two parts: actor and
    critic. The actor part selects the action based on the parameterized policy and
    the critic part concentrates on evaluating the value functions. Different from
    previous approaches, AC evaluates the advantage function $\mathcal{A}^{\pi}(s,a)=\mathcal{Q}^{\pi}(s,a)-\mathcal{V}^{\pi}(s)$
    which reflects the relative advantage of a certain action $a$ to the average value
    of all actions. The introduction of the value functions also allows AC to update
    by step through the TD method, and the incorporation of the policy-based methods
    makes AC be suitable for continuous actions. However, the combination of the two
    methods also makes the AC method more difficult to converge. Moreover, Deep Deterministic
    Policy Gradient (DDPG) [[49](#bib.bib49)], as an extension of the AC, absorbs
    the advanced techniques from the DQN and the DPG which enables DDPG to learn the
    policy more efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all the above-mentioned methods, there always exists a trade-off between
    exploring the unknown situation and exploiting with learned knowledge. On the
    one hand, exploiting the learned knowledge can help the model converge quicker,
    but it always leads the model into a local optimal rather than a globally optimal.
    On the other hand, exploring unknown situations can find some new and better solutions,
    but always being in the exploring process causes the model hard to converge. To
    balance these two processes, researchers have been devoting much energy to finding
    a good heuristics strategy, such as $\epsilon-greedy$ strategy, Boltzmann exploration
    (Softmax exploration), upper confidence bound (UCB) algorithm [[2](#bib.bib2)],
    Thompson sampling [[92](#bib.bib92)], and so on. Here, we consider the $\epsilon-greedy$,
    a widely used exploration strategy, as an example. $\epsilon-greedy$ typically
    selects the action with the maximal Q value to exploit the learned experience
    while occasionally selecting an action evenly at random to explore unknown cases.
    $\epsilon-greedy$ exploration strategy with $m$ actions can be denoted as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi(a&#124;s)=\left\{\begin{array}[]{ll}\epsilon/m+(1-\epsilon)&amp;a^{*}=arg\max_{a\in\mathcal{A}}\mathcal{Q}(s,a),\\
    \epsilon/m&amp;a\not=a^{*}.\end{array}\right.$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: $\epsilon\in[0,1)$ is an exploration factor. The agent is more likely to select
    the action at random when the $\epsilon$ is closer to 1, and the $\epsilon$ will
    be continuously reduced during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Advanced Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section mainly discusses some advanced techniques in RL which focus on
    efficiently using the limited samples and building sophisticated model structures
    for better representation and optimization. According to the different improvements,
    they can be broadly classified into two parts: data sampling and model efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Data Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data sampling is one of the most important concerns in training the DRL in data
    processing and analytics. In most applications, the sample generation process
    costs a great amount of time and computation resources. For example, a sample
    may refer to an execution run for workload and repartitioning for the database,
    which can take about 40 minutes[[31](#bib.bib31)]. Hence, to train the model with
    limited samples, we need to increase data utilization and reduce data correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data utilization: Most DRL algorithms train the optimal policy and sample data
    at the same time. Instead of dropping samples after being trained, experience
    replay[[50](#bib.bib50)] accumulates the samples in a big table where samples
    are randomly selected during the learning phase. With this mechanism, samples
    will have a higher utilization rate and a lower variance, and hence, it can stabilize
    the training process and accelerate the training convergence. Samples after several
    iterations may differ from the current policy, and hence, Growing-batch [[40](#bib.bib40)]
    can continuously refresh the table and replace these outdated samples. In addition,
    samples that are far away from the current policy should be paid more attention
    and Prioritized Experience Replay[[79](#bib.bib79)] uses TD error as the priority
    to measure the sample importance, and hence, focus more on learning the samples
    with high errors. In a nutshell, with the experience replay, DRL cannot only stable
    the learning phase but also efficiently optimize the policy with fewer samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data correlation: Strong correlation of training data is another concern that
    may lead the agent to learn a sub-optimal solution instead of the globally optimal
    one. Apart from the experience replay, the mechanism of the distributed environments
    is another research direction to alleviate this problem. For example, the asynchronous
    advantage actor-critic (A3C) [[64](#bib.bib64)] and Distributed PPO (DPPO) [[27](#bib.bib27)]
    apply multi-threads to build multiple individual environments where multiple agents
    take actions in parallel, and the update is calculated periodically and separately
    which can accelerate the sampling process and reduce the data correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Model Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RL model with better efficiency is the major driving force of the development
    of RL, and there are many researchers improving it from three major aspects, namely
    policy, reward function, and value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy: The policy-related techniques focus on stably and effectively learning
    a comprehensive policy, and the advanced techniques to efficiently learn the policy
    can be classified into three parts in detail, which are policy exploration, policy
    representation, and policy optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) Policy exploration: Its target is to explore as many actions as possible
    during the training process in case the policy will be trapped into the local
    optimal. For example, entropy regularisation [[64](#bib.bib64)] adds the entropy
    of the actions’ probabilities into the loss item which can sufficiently explore
    the actions. Besides, adding noise to the action is another research direction
    to increase the randomness into policy exploration. The DDPG applies an Ornstein–Uhlenbeck
    process [[96](#bib.bib96)] to generate temporal noise $\mathcal{N}$ which are
    directly injected into policy. Noisy-Net [[22](#bib.bib22)] incorporates the noise
    into the parameters of neural networks which is easy to implement, and it shows
    a better performance than the $\epsilon-greedy$ and entropy regularisation methods.
    Further, Plappert et al. [[75](#bib.bib75)] investigate an effective way to combine
    the parameter space noise to enrich the exploratory behaviors which can benefit
    both on-policy methods and off-policy methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'b) Policy representation: The states in some RL problems are in a huge dimension
    which causes challenges during the training. To approximate a better policy, a
    branch of DRL models improve the policy representation by absorbing convolutional
    neural networks (CNN) into DQN to analyze the data, such as Dueling DQN [[108](#bib.bib108)],
    DRQN [[26](#bib.bib26)], and so on. In addition, DRQN also incorporates the LSTM
    structure to increase the capacity of the policy which is able to capture the
    temporal information, such as speed, direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'c) Policy optimization: The update of the value functions following the Equation
    [5](#S2.E5 "In 2.1 Markov Decision Process ‣ 2 Theoretical Foundation and Algorithms
    of Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning for Data Processing
    and Analytics") and [6](#S2.E6 "In 2.1 Markov Decision Process ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics") tends to overestimate the value functions
    and introduce a bias because they learn estimates from the estimates. Mnih et
    al.[[66](#bib.bib66)] separate the two estimation process by using two same Q-networks
    which can reduce the correlation of two estimation processes and hence, stabilize
    the course of training. However, the action with the maximum Q-value may differ
    between two Q-networks which will be hard for convergence. and Double DQN (DDQN)
    [[99](#bib.bib99)] alleviate the issue by disaggregating the step of selecting
    the action and calculating the max Q-value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we apply the policy-based RL methods, the learning rate of the policy
    plays an essential role in achieving superior performance. A higher learning rate
    can always maximize the improvement on a policy by step, but it also causes the
    instability of the learning phase. Hence, The Trust Region Policy Optimization
    (TRPO) [[80](#bib.bib80)] builds constraints on the old policy and new policy
    via KL divergence to control the change of the policy in an acceptable range.
    With this constraint, TRPO can iteratively optimize the policy via a surrogate
    objective function which can monotonically improve policies. However, the design
    of the KL constraint makes it hard to be trained, and Proximal Policy Optimization
    (PPO) [[81](#bib.bib81)] simplifies the constraint through two ways: adding it
    into the objective function, designing a clipping function to control the update
    rate. Empirically, PPO methods are much simpler to implement and are able to perform
    at least as well as TRPO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reward: Reward function as one of the key components in the MDP plays an essential
    role in the RL. In some specific problems, the agent has to achieve multiple goals
    which may have some relationships. For example, the robot can only get out through
    the door only if it has already found the key. To tackle this challenge, Hierarchical
    DQN [[38](#bib.bib38)] proposes two levels of hierarchical RL (HRL) models to
    repeatedly select a new goal and achieve the chosen goal. However, there is a
    limitation that the goal needs to be manually predefined which may be unknown
    or unmeasurable in some environments, such as the market and the effect of a drug.
    To overcome it, Inverse RL (IRL) [[68](#bib.bib68)] learns the rewards function
    from the given experts’ demonstrations (i.e. the handcraft trajectories), but
    the agent in IRL can only prioritize the entire trajectories over others. It will
    cause a shift when the agent comes to a state that never appears before, and Generative
    Adversarial Imitation Learning (GAIL) [[32](#bib.bib32)], as an imitation learning
    algorithm, applies adversarial training methods to generate fake samples and is
    able to learn the expert’s policy explicitly and directly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Value: As we have mentioned earlier, the tabular representation of the value
    functions has several limitations which can be alleviated via DRL. Different from
    directly taking the state-action pair as the input to calculate the Q-function,
    Dueling DQN [[108](#bib.bib108)] estimates its value by approximating two separate
    parts that are the state-values and the advantage values, and hence, can distinguish
    whether the value is brought by the state or the action.'
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned advanced algorithms and techniques improve and enhance the
    DRL from different perspectives, which makes DRL-based algorithms be a promising
    way to improve data processing and analytics. We observe that problems with the
    following characteristics may be amenable to DRL-based optimization. First, problems
    are incredibly complex and difficult. The system and application involve a complicated
    operational environment (e.g., large-scale, high-dimensional states) and internal
    implementation mechanisms, which is hard to construct a white-box model accurately.
    DRL can process complex data and learn from experience generated from interacting,
    which is naturally suitable for data processing and analytics where many kinds
    of data exist and are processed frequently. Second, the optimization objectives
    can be represented and calculated easily as the reward because the RL agent improves
    itself towards maximizing the rewards and rewards could be computed a lot of times
    during training. Third, the environment can be well described as MDP. DRL has
    been shown to solve MDP with theoretical guarantees and empirical results. Thus,
    problems involving sequential decision making such as planning, scheduling, structure
    generation (e.g., tree, graph), and searching could be expressed as MDP and a
    good fit for DRL. Fourth, collecting required labels of data massively is hard.
    Compared to supervised learning, DRL can utilize data efficiently to gain good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Data System Optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DRL learns knowledge about the system by interacting with it and optimizes the
    system. In this section, we focus on several fundamental aspects with regards
    to system optimization in data processing and analytics including data organization,
    scheduling, tuning, indexing, query optimization, and cache management. We discuss
    how each problem is formulated in MDP by defining three key elements (action,
    state, and reward) in the system and solved by DRL. Generally, the states are
    defined by some key characteristics of the system. The actions are possible decisions
    (e.g., system configuration), that affect the system performance and the reward
    is calculated based on the performance metrics (e.g. throughput, latency). Table
    1 presents a summary of representative works and the estimated dimension ranges
    on the state and action space of each work are added as signals on the DRL training
    difficulty. As a comparison, OpenAI Five[[8](#bib.bib8)], a Dota-playing AI, observes
    the state as 20,000 numbers representing useful game information and about 1,000
    valid actions (like ordering a hero to move to a location) for per hero. Dota
    is a real-time strategy game between two teams of five players where each player
    controls a character called a “hero”.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Data Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Data Partitioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Effective data partitioning strategy is essential to accelerate data processing
    and analytics by skipping irrelevant data for a given query. It is challenging
    as many factors need to be considered, including the workload and data characteristics,
    hardware profiles, and system implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In data analytics systems, data is split into blocks in main memory or secondary
    storage, which are accessed by relevant queries. A query may fetch many blocks
    redundantly and, therefore, an effective block layout avoids reading unnecessary
    data and reduces the number of block accesses, thereby improving the system performance.
    Yang et al.[[116](#bib.bib116)] propose a framework called the qd-tree that partitions
    data into blocks using DRL over the analytical workload. The qd-tree resembles
    the classic k-d tree and describes the partition of multi-dimensional data space
    where each internal node splits data using a particular predicate and represents
    a subspace. The data in the leaf node is assigned to the same block. In the MDP,
    each state is a node representing the subspace of the whole data and featured
    as the concatenation of range and category predicates. After the agent takes an
    action to generate two child nodes, two new states will be produced and explored
    later. The available action set is the predicates parsed from workload queries.
    The reward is computed by the normalized number of skipped blocks over all queries.
    They do not execute queries and a sampling technique is used to estimate the reward
    efficiently. The formulation of using DRL to learn a tree is similar to NeuroCuts[[47](#bib.bib47)]
    that learns a tree for packet classification. However, the qd-tree may not support
    a complex workload containing user-defined functions (UDFs) queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Horizontal partitioning in the database chooses attributes of large tables
    and splits them across multiple machines to improve the performance of analytical
    workloads. The design relies on either the experience of database administrators
    (DBAs) or cost models that are often inaccurate[[42](#bib.bib42)] to predict the
    runtime for different partitions. Data collection is too challenging and costly
    to train the accurate supervised learning model in the cloud environment. Hilprecht
    et al.[[31](#bib.bib31)] learn to partition using DRL on analytical workloads
    in cloud databases, on the fact that DRL is able to efficiently navigate the partition
    search and requires less training data. In the MDP, the state consists of two
    parts. The database part encodes whether a table is replicated, an attribute is
    used for partitioning, and which tables are co-partitioned. The workload part
    incorporates normalized frequencies of representative queries. Supported actions
    are: partitioning a table using an attribute, replicating a table, and changing
    tables co-partition. The reward is the negative of the runtime for the workload.
    One challenge is that the cost of database partitioning is high during training.
    To alleviate the problem, the agent is trained in the simulation environment and
    is further refined in the real environment by estimating the rewards using sampling.
    One limitation is that it may not support new queries well because only the frequency
    features of queries are considered. Durand et al. in [[17](#bib.bib17), [18](#bib.bib18)]
    utilize DRL to improve vertical partitioning that optimizes the physical table
    layout. They show that the DQN algorithm can easily work for a single workload
    with one table but is hard to generalize to random workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For UDFs analytics workloads on unstructured data, partitioning is more challenging
    where UDFs could express complex computations and functional dependency is unavailable
    in the unstructured data. Zou et al.[[127](#bib.bib127)] propose the Lachesis
    system to provide automatic partitioning for non-relational data analytics. Lachesis
    translates UDFs to graph-based intermediate representations (IR) and identifies
    partition candidates based on the subgraph of IR as a two-terminal graph. Lachesis
    adopts DRL to learn to choose the optimal candidate. The state incorporates features
    for each partition extracted from historical workflows: frequency, the execution
    interval, time of the most recent run, complexity, selectivity, key distribution,
    number, and size of co-partition. In addition, the state also incorporates other
    features such as hardware configurations. The action is to select one partition
    candidate. The reward is the throughput speedup compared to the average throughput
    of the historical executions of applications. To reduce the training time, the
    reward is derived from historical latency statistics without partitioning the
    data when running the applications. One limitation is that Lachesis largely depends
    on historical statistics to design the state and calculate the reward, which could
    lead to poor performance when the statistics are inadequate.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Data Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data compression is widely employed to save storage space. The effectiveness
    of a compression scheme however relies on the data types and patterns. In time-series
    data, the pattern can change over time and a fixed compression scheme may not
    work well for the entire duration. Yu et al.[[120](#bib.bib120)] propose a two-level
    compression framework, where a scheme space is constructed by extracting global
    features at the top level and a compression schema is selected for each point
    at the bottom level. The proposed AMMMO framework incorporates compression primitives
    and the control parameters, which define the compression scheme space. Due to
    the fact that the enumeration is computationally infeasible, the framework proposes
    to adopt DRL to find the compression scheme. The agent takes a block that consists
    of 32 data points with the compressed header and data segment, timestamps, and
    metrics value as the state. The action is to select a scheme from compression
    scheme space and then the compression ratio is computed as the reward. The limitation
    is that the method may not work for other data types like images and videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Representative Works using DRL for Data System Optimizations. D(X)
    denotes the approximate dimension of X space.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain | Work | Algorithm | D(State) | D(Action) | DRL-based Approach | Open
    Source |'
  prefs: []
  type: TYPE_TB
- en: '| Data organization | Analytical system data partition[[116](#bib.bib116)]
    | PPO | 10 - 100 | 100 - 1000 | Exploit workload patterns and Generate the tree
    | NO |'
  prefs: []
  type: TYPE_TB
- en: '|  | Database horizontal partition [[31](#bib.bib31)] | DQN | 100 | 10 | Navigate
    the partition search efficiently | NO |'
  prefs: []
  type: TYPE_TB
- en: '|  | UDF-centric workload data partition [[127](#bib.bib127)] | A3C | 10 |
    1-10 | Exploit the features of partition and search | YES |'
  prefs: []
  type: TYPE_TB
- en: '|  | Time series data compression [[120](#bib.bib120)] | PG | 100 | 10 | Search
    parameters interactively | NO |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling | Distributed job processing [[58](#bib.bib58)] | PG | 100 | 10
    | Exploit the job dependencies and learn schedule decision | YES |'
  prefs: []
  type: TYPE_TB
- en: '|  | Distributed stream data [[45](#bib.bib45)] | DDPG | 100 | 10-100 | Learn
    schedule decision | NO |'
  prefs: []
  type: TYPE_TB
- en: '| Tuning | Database configuration [[123](#bib.bib123)] [[44](#bib.bib44)] |
    DDPG | 100 | 10 | Search configuration parameters interactively | YES |'
  prefs: []
  type: TYPE_TB
- en: '| Index | Index Selection [[84](#bib.bib84)] | CEM | 100 | 10 | Search the
    index interactively | NO |'
  prefs: []
  type: TYPE_TB
- en: '|  | R-tree construction [[24](#bib.bib24)] | DQN | 10-100 | 10 | Learn to
    generate the tree | NO |'
  prefs: []
  type: TYPE_TB
- en: '| Query Optimization | Join order selection [[61](#bib.bib61), [37](#bib.bib37),
    [119](#bib.bib119), [29](#bib.bib29)] | PG, DQN, … | 10-100 | 1-10 | Learn to
    decide the join order | Only [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '| Cache Management | View Materialization [[121](#bib.bib121)] | DQN | 100
    | 10 | Model the problem as IIP and solve | NO |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scheduling is a critical component in data processing and analytics systems
    to ensure that resources are well utilized. Job scheduling in a distributed computing
    cluster faces many challenging factors such as workload (e.g., job dependencies,
    sizes, priority), data locality, and hardware characteristics. Existing algorithms
    using general heuristics such as shortest-job-first do not utilize these factors
    well and fail to yield top performance. To this end, Mao et al.[[58](#bib.bib58)]
    propose Decima to learn to schedule jobs with dependent stages using DRL for data
    processing clusters and improve the job completion time. In the data processing
    systems such as Hive[[93](#bib.bib93)], Pig[[70](#bib.bib70)], Spark-SQL[[1](#bib.bib1)],
    jobs could have up to hundreds of stages and many stages run in parallel, which
    are represented as directed acyclic graphs (DAGs) where the nodes are the execution
    stages and each edge represents the dependency. To handle parallelism and dependencies
    in job DAGs, Decima first applies graph neural network (GNN) to extract features
    as the state instead of manually designing them while achieving scalability. Three
    types of feature embeddings are generated. Node embedding captures information
    about the node and its children including the number of remaining tasks, busy
    and available executors, duration, and locality of executors. Job embedding aggregates
    all node embeddings in the job and cluster embedding combines job embeddings.
    To balance possible large action space and long action sequences, The action determines
    the job stage to be scheduled next and the parallelism limit of executors. The
    reward is based on the average job completion time. To train effectively in a
    job streaming environment, Decima gradually increases the length of training jobs
    to conduct curriculum learning[[7](#bib.bib7)]. The variance reduction technique[[59](#bib.bib59)]
    is applied to handle stochastic job arrivals for robustness. However, we note
    that Decima is non-preemptive and does not re-schedule for higher priority jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In distributed stream data processing, streams of continuous data are processed
    at scale in a real-time manner. The scheduling algorithm assigns workers to process
    data where each worker uses many threads to process data tuples and aims to minimize
    average data tuple processing time. Li et al.[[45](#bib.bib45)] design a scheduling
    algorithm using DRL for distributed stream data processing, which learns to assign
    tuples to work threads. The state consists of the scheduling plan (e.g., the current
    assignment of workers) and the workload information (e.g., tuple arrival rate).
    The action is to assign threads to machines. The reward is the negative tuple
    processing time on average. The work shows that DQN does not work well because
    the action space is large and applies DDPG to train the actor-critic based agent
    instead. To find a good action, the proposed method looks for k nearest neighbors
    of the action that the actor network outputs and selects the neighbor with the
    highest value that the critic network outputs. The algorithm is implemented on
    Apache Storm and evaluated with representative applications: log stream processing,
    continuous queries, and word count.'
  prefs: []
  type: TYPE_NORMAL
- en: Many works have been recently proposed to improve scheduling using DRL[[122](#bib.bib122),
    [35](#bib.bib35)]. Query scheduling determines the execution order of queries,
    which has a great influence on query performance and resource utilization in the
    database system. SmartQueue[[122](#bib.bib122)] improves query scheduling by leveraging
    overlapping data access among queries and learns to improve cache hits using DRL.
    In addition, Tim et al.[[35](#bib.bib35)] design a scheduling system in SageDB
    using RL techniques. Other works using RL for scheduling include Bayesian RL for
    scheduling in heterogeneous clusters[[3](#bib.bib3)], operation scheduling in
    devices[[23](#bib.bib23)], application container scheduling in clusters[[102](#bib.bib102)],
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tuning the configuration of data processing and analytic systems plays a key
    role to improve system performance. The task is challenging because up to hundreds
    of parameters and complex relations between them could exist. Furthermore, other
    factors such as hardware and workload also impact the performance. Existing works
    often employ search-based or supervised learning methods. The former takes much
    time to get an acceptable configuration and the latter such as OtterTune[[98](#bib.bib98)]
    needs large high-quality data that is non-trivial to obtain in practice. Zhang
    et al.[[123](#bib.bib123)] design a cloud database tuning system CDBTune using
    DRL to find the best parameter in high-dimensional configuration space. The CDBTune
    formulates MDP as follows. The state is represented by the internal metrics (e.g.,
    buffer size, pages read). The action is to increase or decrease the knob values.
    The reward is the performance difference between two states, which is calculated
    using throughput and latency. CDBTune takes several hours on offline training
    in simulation and online training in the real environment. Compared to OtterTune,
    CDBTune eases the burden of collecting large training data sets. In the experiments,
    CDBTune is shown to outperform DBA experts and OtterTune and improve tuning efficiency
    under 6 different workloads on four databases. One limitation of the approach
    is that the workload information is ignored and thus it may not perform well when
    the query workload is changed.
  prefs: []
  type: TYPE_NORMAL
- en: To address the issue, Li et al.[[44](#bib.bib44)] propose QTune that considers
    query information to tune the database using DRL. First, Qtune extracts features
    from SQL query including types (e.g., insert, delete), tables, and operation (e.g.,
    scan, hash join) costs estimated by the database engine. The columns attributes
    and operations like selection conditions in the query are ignored. Subsequently,
    Qtune trains a DNN model to predict the difference of statistics (e.g., updated
    tuples, the number of committed transactions) in the state after executing the
    queries in the workload and updates the state using it. The action and reward
    design are similar to CDBTune. Additionally, QTune supports three levels of tuning
    granularity for balancing throughput and latency. For query-level, QTune inputs
    query vector and tries to find good knobs for each query. For workload-level,
    vectors for all queries are merged and used. For cluster-level, QTune employs
    a clustering method based on deep learning to classify queries and merge queries
    into clusters. One drawback of QTune is that the query featurization could lose
    key information such as query attributes (i.e., columns) and hurt the performance
    especially when the cost estimation is inaccurate. The prediction model for state
    changes is trained alone and needs accurate training data. An end-to-end training
    framework is therefore essential and a good direction to undertake.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Indexing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.4.1 Database Index Selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Database index selection considers which attributes to create an index to maximize
    query performance. Sharma et al.[[84](#bib.bib84)] show how DRL can be used to
    recommend an index based on a given workload. The state encodes selectivity values
    for workload queries and columns in the database schema and current column indexes.
    The action is to create an index on a column. The reward is the improvement compared
    to the baseline without indexes. The experiments show that the approach can perform
    as well or better as having indexes on all columns. Sadri et al.[[78](#bib.bib78)]
    utilize DRL to select the index for a cluster database where both query processing
    and load balancing are considered. Welborn et al.[[110](#bib.bib110)] optimize
    the action space design by introducing task-specific knowledge for index selection
    tasks in the database. However, these works only consider the situation where
    single-column indexes are built. Lan et al.[[39](#bib.bib39)] propose both single-attribute
    and multi-attribute indexes selection using DRL. Five rules are proposed to reduce
    the action and state space, which help the agent learn effective strategy easier.
    The method uses what-if caller[[9](#bib.bib9)] to get the cost of queries under
    specific index configurations without building indexes physically. These works
    conduct basic experiments with small and simple datasets. Extensive and large-scale
    experiments using real datasets are therefore needed to benchmark these methods
    to ensure that they can scale well.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Index Structure Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The learned index is proposed recently as an alternative index to replace the
    B^+-Tree and bloom filter by viewing indexes as models and using deep learning
    models to act as indexes[[36](#bib.bib36)]. DRL can enhance the traditional indexes
    instead of replacing them.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical structures such as the B^+-tree and R-tree are important indexing
    mechanisms to locate data of interest efficiently without scanning a large portion
    of the database. Compared to the single dimensional counterpart, the R-tree is
    more complex to optimize due to bounding box efficiency and multi-path traversals.
    Earlier conventional approaches use heuristics to determine these two operations
    (i.e. choosing the insertion subtree and splitting an overflowing node) during
    the construction of the R-tree[[71](#bib.bib71)]. Gu et al.[[24](#bib.bib24)]
    propose to use DRL to replace heuristics to construct the R-tree and propose the
    RLR-tree. The approach models two operations ChooseSubtree and Split as two MDPs
    respectively and combines them to generate an R-Tree. For ChooseSubtree, the state
    is represented as the concatenation of the four features (i.e., area, perimeter,
    overlap, occupancy rate) of each selected child node. More features are evaluated
    but do not improve the performance in the reported experiments. The action is
    to select a node to insert from top-k child nodes in terms of the increase of
    area. The reward is the performance improvement from the RLR-tree. For Split MDP,
    the state is the areas and perimeters of the two nodes created by all top-k splits
    in the ascending order of total area. The action is to choose one split rule from
    k rules and the reward is similar to that of ChooseSubtree. The two agents are
    trained alternately. As expected, the optimizations render the RLR-tree improved
    performance in range and KNN queries.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs can be used as effective indexes to accelerate nearest neighbors search[[55](#bib.bib55),
    [15](#bib.bib15)]. Existing graph construction methods generally propose different
    rules to generate graphs, which cannot provide adaptivity for different workloads[[104](#bib.bib104)].
    Baranchuk et al.[[5](#bib.bib5)] employ DRL to optimize the graph for nearest
    neighbors search. The approach learns the probabilities of edges in the graph
    and tries to maximize the search efficiency. It considers the initial graph and
    the search algorithm as the state. The action is to keep an edge or not. The reward
    is the performance for search. It chooses the TRPO[[80](#bib.bib80)] algorithm
    to train. The reported experimental results show that the agent can refine state-of-the-art
    graphs and achieve better performance. However, this approach does not learn to
    explore and add new edges to the initial graph that may affect the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Searching and constructing a new index structure is another line of interesting
    research [[33](#bib.bib33)]. Inspired by Neural Architecture Search (NAS)[[126](#bib.bib126)],
    Wu et al.[[112](#bib.bib112)] propose an RNN-based neural index search (NIS) framework
    that employs DRL to search the index structures and parameters given the workload.
    NIS can generate tree-like index structures layer by layer via formalizing abstract
    ordered blocks and unordered blocks, which can provide a well-designed search
    space. The keys in the ordered block are sorted in ascending order, and the skip
    list or B^+-Tree can be used. The keys in the unordered block are partitioned
    using customized functions and the hash bucket can be used. Overall, the whole
    learning process is similar to that of NAS.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Query Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Query optimization aims to find the most efficient way to execute queries in
    database management systems. There are many different plans to access the query
    data that can have a large processing time variance from seconds to hours. The
    performance of a query plan is determined mostly by the table join orders. Traditionally,
    query optimizers use certain heuristics combined with dynamic programming to enumerate
    possible efficient execution plans and evaluate them using cost models that could
    produce large errors[[42](#bib.bib42)]. Marcus et al.[[61](#bib.bib61)] propose
    Rejoin that applies DRL to learn to select better join orders utilizing past experience.
    The state encodes join tree structure and join predicates. The action is to combine
    two subtrees, where each subtree represents an input relation to join. The reward
    is assigned based on the cost model in the optimizer. The experiments show that
    ReJOIN can match or outperform the optimizer in PostgreSQL. Compared to ReJoin,
    DQ[[37](#bib.bib37)] presents an extensible featurization scheme for state representation
    and improves the training efficiency using the DQN[[65](#bib.bib65)] algorithm.
    Heitz et al.[[29](#bib.bib29)] compare different RL algorithms including DQN[[65](#bib.bib65)],
    DDQN[[49](#bib.bib49)], and PPO[[81](#bib.bib81)] for join order optimization
    and use a symmetric matrix to represent the state instead of vector. Yu et al.[[119](#bib.bib119)]
    introduce a graph neural network (GNN) with DRL for join order selection that
    replaces fixed-length hand-tuned vector in Rejoin[[61](#bib.bib61)] and DQ[[37](#bib.bib37)]
    with learned scalable GNN representation and better captures and distinguishes
    the join tree structure information. These works mainly differ in encoding what
    information and how to encode them.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of learning from past query executions, Trummer et al.[[94](#bib.bib94)]
    propose SkinnerDB to learn from the current query execution status to optimize
    the remaining execution of a query using RL. Specifically, SkinnerDB breaks the
    query execution into many small time intervals (e.g., tens to thousands of slices
    per second) and processes the query adaptively. At the beginning of each time
    interval, the RL agent chooses the join order and measures the execution progress.
    SkinnerDB adopts a similar adaptive query processing strategy in Eddies[[95](#bib.bib95)]
    and uses the UCT algorithm[[34](#bib.bib34)], which provides formal guarantees
    that the difference is bounded between the rewards obtained by the agent and those
    by optimal choices. The reward is calculated by the progress for the current interval.
    A tailored execution engine is designed to fully exploit the learning strategy
    with tuple representations and specialized multi-way join algorithms. SkinnerDB
    offers several advantages. First, it is inherently robust to query distribution
    changes because its execution only depends on the current query. Second, it relies
    on less assumption and information (e.g., cardinality models) than traditional
    optimizers and thus is more suitable for the complicated environment where cardinality
    is hard to estimate. Third, it predicts the optimal join order based on real performance.
    However, it may introduce overhead caused by join order switching.
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based methods that have been proposed to replace traditional query
    optimizers often incur a great deal of training overhead because they have to
    learn from scratch. To mitigate the problem, Bao [[60](#bib.bib60)] (the Bandit
    optimizer)) is designed to take advantage of the existing query optimizers. Specifically,
    Bao learns to choose the best plan from the query plan candidates provided by
    available optimizers by passing different flags or hints to them. Bao transforms
    query plan trees into vectors and adopts a tree convolutional neural network to
    identify patterns in the tree. Then it formulates the choosing task as a contextual
    multi-armed bandit problem and uses Thompson sampling[[92](#bib.bib92)] to solve
    it. Bao is a hybrid solution for query optimization. It achieves good training
    time and is robust to changes in workload [[60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Methods of query optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Techniques | Training | Workload Adaptivity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rejoin[[61](#bib.bib61)], DQ[[37](#bib.bib37)] | learn from execution experience
    | High | Low |'
  prefs: []
  type: TYPE_TB
- en: '| SkinnerDB [[94](#bib.bib94)] | learn from current execution status | Medium
    | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Bao[[60](#bib.bib60)] | learn to choose existing optimizers | Low | High
    |'
  prefs: []
  type: TYPE_TB
- en: 3.6 Cache Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.6.1 View Materialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: View materialization is the process of deciding which view, i.e., results of
    query or subquery, to cache. In database systems, a view is represented as a table
    and other queries could be accelerated by reading this table instead of accessing
    the original tables. There is an overhead of materializing and maintaining the
    view when the original table is updated. Existing methods are based on heuristics,
    which either rely on simple Least-Recently-Used rule or cost-model based approaches[[74](#bib.bib74)].
    The performance of these approaches is limited because feedback from the historical
    performance of view materialization is not incorporated. Liang et al.[[48](#bib.bib48)]
    implement Deep Q-Materialization (DQM) system that leverages DRL to improve the
    view materialization process in the OLAP system. First, DQM analyzes SQL queries
    to find candidate views for the current query. Second, it trains a DRL agent to
    select from the set of candidates. Third, it uses an eviction policy to delete
    the materialized views. In the MDP, the state encodes view state and workload
    information. The action is to create the view or do nothing. The reward is calculated
    by the query time improvement minus amortized creation cost. Additionally, the
    eviction policy is based on credit and it evicts the materialized view with the
    lowest score.
  prefs: []
  type: TYPE_NORMAL
- en: Yuan et al.[[121](#bib.bib121)] present a different way that use DRL to automate
    view generation and select the most beneficial subqueries to materialize. First,
    the approach uses a DNN to estimate the benefits of a materialized view where
    features from tables, queries, and view plans are extracted. Then the approach
    models selection as an Integer Linear Programming (IIP) problem and introduce
    an iterative optimization method to figure it out. However, the method cannot
    guarantee convergence. To address the issue, the problem is formulated as the
    MDP. The state encodes the subqueries that are selected to materialize and status
    if queries use these materialized views. The action is to choose the subquery
    to materialize or not. The reward is the difference between benefit changes of
    two states. Both cost estimation and view selection models are trained offline
    using the actual cost of queries and benefits. Then the cost estimation model
    is used for the online recommendation for view materialization. Performance study
    shows its good performance; However, it lacks a comparison with DQM.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cache management impacts the performance of computer systems with hierarchical
    hardware structures directly. Generally, a caching policy considers which objects
    to cache, to evict when the cache is full to maximize the object hit rate in the
    cache. In many systems, the optimal caching policy depends on workload characteristics.
    Phoebe[[111](#bib.bib111)] is the RL-based framework for cache management for
    storage models. The state encodes the information from a preceding fixed-length
    sequence of accesses where for each access, nine features are extracted including
    data block address, data block address delta, frequency, reuse distance, penultimate
    reuse distance, average reuse distance, frequency in the sliding window, the number
    of cache misses, and a priority value. The action is to set a priority value ranging
    within $[-1,1]$ to the data. The reward is computed from if the cache is hit or
    missed and values are 1 and -1 respectively. It applies the DDPG algorithm to
    train the agent. Periodical training is employed to amortize training costs in
    online training. In network systems, one issue is that the reward delay is very
    long in systems with a large cache, i.e., CDN cache can host up to millions of
    objects. Wang et al.[[100](#bib.bib100)] propose a subsampling technique by hashing
    the objects to mitigate the issue when applying RL on caching systems.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Data Analytics Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 3: Representative works for RL applications. D(X) denotes the approximate
    dimension of X space.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain | Work | Algorithm | D(State) | D(Action) | DRL-based Approach |'
  prefs: []
  type: TYPE_TB
- en: '| Data processing | Entity matching[[11](#bib.bib11), [20](#bib.bib20)] | PG
    | 100 - 1000 | 100 - 1000 | Select target entity from the candidate entities |'
  prefs: []
  type: TYPE_TB
- en: '| application | Database interaction with natural language [[125](#bib.bib125),
    [14](#bib.bib14)] | PG | 100 - 1000 | 100 - 1000 | Learn to generate the query
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Feature engineering [[52](#bib.bib52)] | DQN | 100 | 1-10 | Select features
    and model feature correlations in states |'
  prefs: []
  type: TYPE_TB
- en: '|  | Exploratory data analysis [[4](#bib.bib4)] | A3C | 10-100 | 100000 | Learn
    to query a dataset for key characteristics |'
  prefs: []
  type: TYPE_TB
- en: '|  | Abnormal detection [[69](#bib.bib69)] | IRL | 1-10 | 1-10 | Learn the
    reward function for normal sequences |'
  prefs: []
  type: TYPE_TB
- en: '|  | AutoML pipeline generation [[28](#bib.bib28)] | DQN | 10 | 100 | Learn
    to select modules of a pipeline |'
  prefs: []
  type: TYPE_TB
- en: '| Healthcare | Treatment recommendation [[103](#bib.bib103)] | DDPG | 10 |
    100-1000 | Select treatment from candidate treatments |'
  prefs: []
  type: TYPE_TB
- en: '|  | Diagnostic inference [[51](#bib.bib51)] | DQN | 100-1000 | 1-10 | Learn
    diagnostic decision |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hospital resource allocation [[19](#bib.bib19)] | DDPG | 100 | 1000-10000
    | Learn resource scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| Fintech | Portfolio optimization [[12](#bib.bib12)] | Q-Learning | 100 |
    100 | Select the portfolio weights for stocks |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trading [[115](#bib.bib115), [114](#bib.bib114)] | IRL | 1-10 | 10 | Learn
    the reward function of trading behaviors |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fraud detection [[114](#bib.bib114)] | IRL | 100 | 10-100 | Learn the
    reward function of trading behaviors |'
  prefs: []
  type: TYPE_TB
- en: '| E- | Online advertising [[124](#bib.bib124)] | DQN | 1-10 | 1-10 | Learn
    to schedule the advertisements |'
  prefs: []
  type: TYPE_TB
- en: '| Commerce | Online recommendation [[10](#bib.bib10)] | DQN | 100 | 10000 |
    Learn to schedule recommendations |'
  prefs: []
  type: TYPE_TB
- en: '|  | Search results aggregation [[91](#bib.bib91)] | DQN | 10-100 | 10-100
    | Learn to schedule search results |'
  prefs: []
  type: TYPE_TB
- en: '| Others | User profiling [[105](#bib.bib105)] | DQN | 100-1000 | 1000-10000
    | Select users’ next activities by modeling spatial semantics |'
  prefs: []
  type: TYPE_TB
- en: '|  | Spammer detection [[16](#bib.bib16)] | PG | 100 | 100 | Search for the
    detector by interacting with spammers |'
  prefs: []
  type: TYPE_TB
- en: '|  | Transportation [[83](#bib.bib83)] | PG | 1000-10000 | 1000 | Learn to
    schedule transportation |'
  prefs: []
  type: TYPE_TB
- en: In this section, we shall discuss DRL applications from the perspective of data
    processing and data analytics. These two categories of DRL applications form indispensable
    parts of a pipeline, in which data processing provides a better basis for data
    analytics. In addition, these two categories share some overlapping topics, making
    these topics mutually motivating and stimulating. We have summarized the technical
    comparisons of different applications in Table [3](#S4.T3 "Table 3 ‣ 4 Data Analytics
    Applications ‣ A Survey on Deep Reinforcement Learning for Data Processing and
    Analytics"). We shall first discuss DRL applications in data preparation and then
    in data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Data Preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Entity Matching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Entity matching is a data cleaning task that aligns different mentions of the
    same entity in the context. Clark et al. [[11](#bib.bib11)] identify the issue
    that the heuristic loss function cannot effectively optimize the evaluation metric
    $B^{3}$, and propose using reinforcement learning to directly optimize the metric.
    The problem is formulated as a sequential decision problem where each action is
    performed on one mention of a document. The action maps the mention to an entity
    in the database at each step by a mention ranking model. Then the reward is calculated
    using the evaluation metric $B^{3}$. This work originally proposes scaling each
    action’s weight by measuring its impact on the final reward since each action
    is independent. However, this work does not consider the global relations between
    entities. Fang et al. [[20](#bib.bib20)] propose a reinforcement learning framework
    based on the fact that an easier entity will create a better context for the subsequent
    entity matching. Specifically, both local and global representations of entity
    mentions are modeled and a learned policy network is devised to choose from the
    next action (i.e., which entity to recognize). However, the selection of the easier
    entity to learn the context could be less powerful than context modeling with
    more recent techniques in NLP such as the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Database Interaction With Natural Language
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To facilitate query formulation for relational databases, there have been efforts
    in generating SQL queries from various other means that do not require knowledge
    of SQL and schema. Zhong et al. [[125](#bib.bib125)] propose to generate SQL from
    a natural language using Reinforcement Learning. For queries formed by a natural
    language, the model Seq2SQL will learn a policy transforming the queries into
    SQL queries. The transformed queries will then be executed in the database system
    to get results. The results will be compared with the ground truth to generate
    RL rewards. Earlier work [[14](#bib.bib14)] using generic autoencoder model for
    semantic parsing with Softmax as the final layer may generate unnecessarily large
    output spaces for SQL query generation tasks. Thus the structure of SQL is used
    to prune the output space of query generating and policy-based reinforcement learning
    to optimize the part which cannot be optimized by cross-entropy. However, RL is
    observed to have limited performance enhancement by [[113](#bib.bib113)] due to
    unnecessary modeling of query serialization.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently querying a database of documents is a promising data processing
    application. Karthik et al. [[67](#bib.bib67)] propose collecting evidence from
    external sources of documents to boost extraction accuracy to original sources
    where data might be scarce. The problem is formulated as an MDP problem, where
    each step the agent needs to decide if current extracted articles are accepted
    and stop querying, or these articles are rejected and more relevant articles are
    queried. Both data reconciliation (from original sources) and data retrieval (from
    external sources) are represented as states. Extraction accuracy and penalties
    for extra retrieval actions are reflected in the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Feature Engineering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Feature engineering can be formulated as a single-agent reinforcement learning
    problem to search for an optimal subset of features in a large space: the agent
    selects one feature at each action step. The state is the current feature subspace.
    A reward is assigned to the agent based on the predictive performance of the current
    features subset. Liu et al. [[52](#bib.bib52)] propose a method to reformulate
    feature engineering as a multi-agent reinforcement learning problem. The multi-agent
    RL formulation reduces the large action space of a single agent since now each
    of the agents has a smaller action space for one feature selection. However, this
    formulation also brings challenges: interactions between agents, representation
    of the environment, and selection of samples. Three technical methods in [[52](#bib.bib52)]
    have been proposed to tackle them respectively: adding inter-feature information
    to reward formulation, using meta statistics, and deep learning methods to learn
    the representation of the environment, and Gaussian mixture to independently determine
    samples. However, although this formulation reduces the action space, the trade-off
    is using more computing resources to support more agents’ learning. Also, the
    method is difficult to scale to a large feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Exploratory Data Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA) is useful for users to understand the characteristics
    of a new dataset. In [[4](#bib.bib4)], the problem is formulated as a MDP. The
    action space is the combination of a finite set of operators and their corresponding
    parameters to query a dataset. The result of a query shows the characteristics
    of the dataset. The characteristics are modeled as the state, which is represented
    by descriptive statistics and recent operators. The reward signal measures the
    interestingness, diversity, and coherency of the characteristics by an episode
    of EDA operations. DRL is applied to the non-differential signals and discrete
    states in MDP. However, challenges arise when applying deep reinforcement learning
    given a large number of possible actions as parameterized operations (i.e., for
    each type of operation, the corresponding possible action is the Cartesian product
    of all parameters’ possible values). In [[4](#bib.bib4)], a two-fold layer architecture
    is proposed to replace a global softmax layer into two local layers, which effectively
    reduces the intractable large numbers of actions. However, the global interactions
    of operations and attributes are not considered.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Abnormal Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Abnormal detection is important for high-stake applications such as healthcare
    (e.g., predicting patients’ status) and fintech (e.g., financial crime). Based
    on the assumptions, there are two approaches to this problem. One approach models
    the dynamics in the unlabeled datasets as a sequential decision process where
    the agent performs an action on each observation. Oh et al. [[69](#bib.bib69)]
    propose to use IRL to learn a reward function and a Bayesian network to estimate
    a confidence score for a potential abnormal observation. To achieve this, the
    prior distribution of the reward function is assumed. Then a reward function is
    sampled from the distribution to determine the sample generating policy, which
    generates sample background trajectories. As explained by the reward part of Section
    [2.3.2](#S2.SS3.SSS2 "2.3.2 Model Efficiency ‣ 2.3 Advanced Techniques ‣ 2 Theoretical
    Foundation and Algorithms of Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"), experts’ trajectories are observed.
    With these experts’ trajectories and sample background trajectories, the parameters
    of the reward function are updated and thus the policy is improved. The sequence
    of actions is the input into the neural network. This network is trained to learn
    the normal pattern of a targeted agent and to predict if the next observation
    is abnormal or not. However, this approach relies too much on mining unlabeled
    datasets and ignores the labeled dataset. To address this issue, another approach
    also uses DRL but focus on the Exploit-Explore trade-off on both unlabeled and
    labeled dataset. Pang et al. [[73](#bib.bib73)] propose a DRL model with a sampling
    function to select data instances from both the unlabeled and labeled dataset.
    This sampling function helps the DRL model to exploit the scarce but useful labeled
    anomaly data instances and to explore the large unlabeled dataset for novel anomaly
    data instances. Thus, more anomaly data instances are selected to train the DRL
    model with better model capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 AutoML Pipeline Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pipeline generation includes generating all data processing and analytics steps
    or modules to perform ML tasks. Heffetz et al. [[28](#bib.bib28)] propose a grid-world
    to represent all possible families of each step of a data pipeline as cells and
    connect all possible cells as a graph. Subsequently, a hierarchical method is
    used to reduce the space of all actions and represent all actions by layers of
    clusters. Finally, the state representations are inputs to the value sub-network
    in a DQN network, and action representations are inputs to evaluate the advantage-to-average
    sub-network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Healthcare
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Healthcare analytics has gained increasing attention in tandem with the advancement
    of healthcare treatment and availability of medical data and computational capacity
    [[41](#bib.bib41)]. Naturally, a great amount of effort has been spent on applying
    DRL to healthcare. As before, implementing DRL-based models in healthcare requires
    the understanding of the application context and defining the key elements of
    MDP. However, differences occur in the approaches to learning better decisions:
    learning the motivation of expert decisions by IRL, learning better decisions
    without an expert by interacting with an environment or interacting with an environment
    with expert decisions as supervising signals.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Treatment Recommendation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Treatment recommendation systems are designed to assist doctors to make better
    decisions based on electronic health records. However, the doctors’ prescriptions
    are not ground truth but valuable suggestions for high stake medical cases. The
    ground truth is the delayed condition of the patients. Thus model predictions
    must not deviate from the doctors’ judgments too much, and not use those judgments
    as true labels. To tackle this challenge, Wang et al. [[103](#bib.bib103)] propose
    an architecture to combine supervised learning and reinforcement learning. This
    model reduces the inconsistency between indicator signals learned from doctor’s
    prescriptions via supervised learning and evaluation signals learned from the
    long-term outcome of patients via reinforcement learning. In the formulated MDP,
    the domain expert makes a decision based on an unknown policy. The goal is to
    learn a policy that simultaneously reduces the difference between the chosen action
    of the agent and the expert’s decision and to maximize the weighted sum of discounted
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Diagnostic Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using DRL to perform diagnosis can provide a second opinion in high-intensity
    diagnosis from historical medical records to reduce diagnostic errors. Ling et
    al. [[51](#bib.bib51)] propose modeling the integration of external evidence to
    capture diagnostic concept as a MDP. The objective is to find the optimal policy
    function. The inputs are case narratives and the outputs are improved concepts
    and inferred diagnoses. The states are a set of measures over the similarity of
    current concepts and externally extracted concepts. The actions are whether to
    accept (part of) the extracted concepts from external evidence. The environments
    are the top extracted case narratives from Wikipedia as the document pool for
    concepts extraction and a knowledge base for evaluating the intermediate results
    for current best concepts. The rewards are evaluated based on an external knowledge
    base mapping from the concepts to the diagnoses. The whole process is modeled
    by DQN. At each step, narrative cases and evidence are extracted, which provide
    the initial concepts and external concepts. The state representing the agent’s
    confidence in the learned concept is duly calculated. Then the state is sent to
    the DQN agent to estimate the reward to model the long-run accuracy of the learned
    concept by the agent. Iteratively, the model converges with better concepts and
    diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Hospital Resource Allocation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Allocating limited hospital resources is the key to providing timely treatment
    for patients. In [[19](#bib.bib19)], the problem is formulated as a classification
    problem where the patients’ features are given and the target is to predict the
    location of admissions. The RL framework uses a student network to solve the classification
    problem. The weights of the student network are used as states, which are fed
    into a teacher network to generate actions to select which batch of data to train
    the student network. The accuracy of the classification is used as the reward.
    This method provides a view on the resource allocation problem from a curriculum
    learning perspective. However, the temporal information of the data samples is
    not considered but it could affect resource allocation since some hours during
    a day could have fewer patients than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Fintech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning has wide applications in the finance domain. Firstly,
    reinforcement learning has brought new perspectives to let the finance research
    community revisit many classic financial research topics. For example, traditional
    financial research topics such as option pricing that are typically solved by
    the classic Black–Scholes model can be steered through with a data-driven insight
    by reinforcement learning [[25](#bib.bib25)]. Secondly, portfolio optimization,
    typically formulated as a stochastic optimal control problem, can be addressed
    by reinforcement learning. Finally, the agents are financial market participants
    with different intentions. Reward functions can be learned to model these intentions,
    and hence, make better decisions as illustrated in Figure [3](#S4.F3 "Figure 3
    ‣ 4.3 Fintech ‣ 4 Data Analytics Applications ‣ A Survey on Deep Reinforcement
    Learning for Data Processing and Analytics"). We refer readers with further interest
    in finance to [[62](#bib.bib62)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1487672fbf27deae7079fc83e442543.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: DRL in fintech applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Dynamic Portfolio Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The portfolio optimization problem is challenging because of the high scale
    of the dimensionality and the high noise-to-signal ratio nature of stock price
    data. The latter problem of noisy observation can cause uncertainty in a learned
    policy. Therefore, [[12](#bib.bib12)] proposes a novel model structure based on
    the Q-learning to handle noisy data and to scale to high dimensionality. The quadratic
    form of reward function is shown to have a semi-analytic solution that is computationally
    efficient. In the problem formulation, the agent’s actions are represented as
    the changes in the assets at each time step. The states are the concatenation
    of market signals and the agent’s holding assets. This method enhances Q-learning
    by introducing an entropy term measuring the noise in the data. This term acts
    as a regularization term forcing the learned policy to be close to a reference
    policy that is modeled by a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Algorithm Trading Strategy Identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Identification of algorithm trading strategies from historical trades is important
    in fraud detection and maintaining a healthy financial environment. [[114](#bib.bib114)]
    proposes using IRL to learn the reward function behind the trading behaviors.
    The problem is formulated as an Inverse Markov Decision Process (IMDP). The states
    are the differences between the volumes of bid orders and ask orders, which are
    discretized into three intervals based on the values of the volumes. The actions
    are the limit and market order discretized into 10 intervals each by their values.
    The prior distribution of the reward function is a Gaussian Process parameterized
    by $\theta$. Given $\theta$, the approximation of the posterior distribution of
    reward is performed by maximum a posteriori (MAP). This step would give a MAP
    estimated value of the reward. $\theta$ is optimized by a log-likelihood function
    on the posterior of observations. The optimization process can be proved to be
    convex which guarantees the global minimum. The learned features are then used
    to identify and classify trading strategies in the financial markets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Sentiment-based Trading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the main predictors in stock trading is sentiment, which drives the
    demand of bid orders and asks orders. Sentiment scores are often represented by
    unstructured text data such as news or twitters. [[115](#bib.bib115)] proposes
    treating the sentiment as the aggregated action of all the market participants,
    which has the advantage of simplifying the modeling of the numerous market participants.
    Specifically, the sentiment scores are categorized into three intervals: high,
    medium, and low as the action spaces. Compared to previous works, the proposed
    method can model the dependency between the sentiment and the market state by
    the policy function. This method is based on Gaussian Inverse Reinforcement Learning
    [[43](#bib.bib43)] similar to [[114](#bib.bib114)] as discussed at the beginning
    of Section [4.3](#S4.SS3 "4.3 Fintech ‣ 4 Data Analytics Applications ‣ A Survey
    on Deep Reinforcement Learning for Data Processing and Analytics"), which is effective
    at dealing with uncertainty in the stock environment. This method provides a method
    for modeling market sentiments. However, as IRL faces the challenge of non-uniqueness
    of reward [[13](#bib.bib13)] of one agent’s actions, the method does not address
    how aggregated actions of multiple market participants can infer a unique reward
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 E-Commerce
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.4.1 Online Advertising
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the increasing digitalization of businesses, sales and competition for
    market shares have moved online in tandem. As a result, online advertising has
    been increasing in its presence and importance and exploiting RL in various aspects.
    One of the topics in online advertising, bidding optimization, can be formulated
    as a sequential decision problem: the advertiser is required to have strategic
    proposals with bidding keywords sequentially to maximize the overall profit. In [[124](#bib.bib124)],
    the issue of using static transitional probability to model dynamic environments
    is identified and a new DRL model is proposed to exploit the pattern discovered
    from dynamic environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Including but not limited to advertising, Feng et al. [[21](#bib.bib21)] propose
    to consider the whole picture of multiple ranking tasks that occurred in the sequence
    of user’s queries. A new multi-agent reinforcement learning model is proposed
    to enable multiple agents to partially observe inputs and choose actions through
    their own actor networks. The agents communicate through a centralized critic
    model to optimize a shared objective. This allows different ranking algorithms
    to reconcile with each other when taking their own actions and consider the contextual
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Online Recommendation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The problem of an unstabilized reward function arises because of the dynamic
    environment in the online recommendation. For example, user preference is modeled
    as the reward in DRL and it changes unexpectedly when a special discount happens
    for some products. In [[10](#bib.bib10)], a random stratified sampling method
    is proposed to calculate the optimal way of stratifying by allocating more samples
    to the strata with more weighted variance. Then the replay sampling is improved
    to consider key attributes of customers (e.g., gender, age, etc.), which are less
    volatile in the dynamic environment. This allows the modeling of reward function
    based on sampling from a pool with a longer horizon, thus reducing the bias in
    the estimation of the reward function. Lastly, the dynamic environment poses a
    challenge in setting an optimal policy used in regretting. A new method in [[10](#bib.bib10)]
    is proposed to train an offline model to calculate a real-time reward for a subset
    of customers to approximate a reference policy, that is used as an offset in the
    reward recalibration to stabilize the performance of the DRL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Search Results Aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Aggregating useful search results in online shopping search is important to
    improve the shopping experience. However, the challenge of aggregating heterogeneous
    data sources is often encountered. The heterogeneous data sources in online shopping
    are different product categories such as a shoe brand group or a particular topic
    group, each of which is a ranking system. A new model in [[91](#bib.bib91)] is
    proposed to decompose the task into two sub-tasks. The first one is to select
    a data source for the current page of search results based on historical users’
    clicks on previous pages. Learning to select the correct data source for each
    page is a sequential decision-making problem. The second sub-task is to fill the
    sequence of a page by selecting the best source from the candidate sources. However,
    the items from different sources cannot be directly compared because of their
    heterogeneous nature. The problem is solved by formulating the sub-task as an
    RL task to let an agent fill up the sequence. However, one limitation of this
    method is that lacking full annotations of item relevance scores may constrain
    the model’s performance on various scenarios [[91](#bib.bib91)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Other Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DRL has been applied to various other applications. These DRL methods are often
    used with a knowledge graph, confounders, or game theory to model application-specific
    dynamics. These methods are not only well motivated from their respective applications
    but also general enough to be applied in other applications. However, these methods
    often fail to be evaluated by experiments in other applications.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of mobile user profiling aims to identify user profiles to provide
    personalized services. In [[105](#bib.bib105)], the action is the selection of
    a place of visit. The environment is comprised of all users and a knowledge graph
    learning the semantic connections between the spatial entities. The knowledge
    graph is updated once a user’s new activity is performed and then affects the
    agent’s prediction. The state is the embedding of a user and the knowledge graph
    for the current time step. The reward is determined by several metrics measuring
    the similarity between the predicted spatial entities and the ground truth. This
    method considers the spatial semantics of entities but does not consider how the
    change of a user’s key attributes (e.g., career) will affect activity prediction
    and policy learning, which could cause instability in policy updating.
  prefs: []
  type: TYPE_NORMAL
- en: In the transportation system, drivers often get recommendations and provide
    feedback in return to improve the service. However, the recommendation often fails
    when drivers make decisions in a complex environment. To address this issue, in
    [[83](#bib.bib83)] a new method is proposed to model hidden causal factors, called
    confounders, in a complex environment. Specifically, the framework in [[32](#bib.bib32)]
    is extended to include the confounders. First, all three elements (i.e., policy
    agent, environment, confounder) are treated as agents. The effect of a confounder
    is modeled as the policy of the hidden agent, which takes the observation and
    action of the policy agent as inputs and performs an action. The environment in
    turn takes the action based on inputs of the hidden agent’s action and the policy
    agent’s action and observation.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of spammer detection aims to detect spam generating strategies.
    The challenge is that the detectors only detect easier spams while missing spams
    with strategies. In [[16](#bib.bib16)], the problem is formulated as two agents
    counteracting each other. One agent is the spammer, whose policy is to maintain
    a distribution of spam strategies and the action is to sample from the distribution.
    Another agent is the detector, whose state is the detection results after a spam
    attack and the action is to identify the spam. The rewards of two agents are measured
    by winning or losing revenue manipulation, respectively. The limitation of this
    method is that there is no guarantee for equilibrium.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Open Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RL approaches provide strong alternatives to traditional heuristics or supervised
    learning-based algorithms. However, many challenges remain to be addressed to
    make RL a practical solution in the context of data processing and analytics.
    We also foresee many important future research directions to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Open Challenges For System Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 MDP Formulation and Lack of Justification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The design of MDP impacts the performance and efficiency of the RL algorithm
    greatly. The state should satisfy Markov property that its representation contains
    enough relevant information for the RL agent to make the optimal decision. It
    should summarize the environment compactly because a complicated state design
    will cause more training and inference costs. The action space should be designed
    carefully to balance learning performance and computational complexity. The reward
    definition directly affects the optimization direction and the system performance.
    Additionally, the process of reward calculation can involve costly data collection
    and computation in the data systems optimization. Currently, many works rely on
    experimental exploration and experience to formulate MDP while some works exploit
    domain knowledge to improve the MDP formulation by injecting task-specific knowledge
    into action space[[110](#bib.bib110)]. Generally, MDP can influence computational
    complexity, data required, and algorithm performance. Unfortunately, many works
    lack ablation studies of their MDP formulations and do not justify the design
    in a convincing manner. Therefore, automation of MDP formulation remains an open
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 RL Algorithm and Technique Selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RL algorithms and techniques have different tradeoffs and assumptions. Value-based
    DRL algorithms like DQN are not stable and guaranteed convergence. Policy-based
    DRL algorithms like TRPO and PPO are often not efficient. Model-based DRL algorithms
    do not guarantee that a better model can result in a better policy. Value-based
    methods assume full observability while policy-based ones assume episodic learning.
    Off-policy algorithms are usually more efficient than on-policy algorithms in
    terms of sample efficiency. One example is that DQ[[37](#bib.bib37)] uses off-policy
    deep Q-learning to increase data efficiency and reduce the number of training
    queries needed. Training efficiency can be a big concern for DRL-based system
    optimization, especially when the workload of the system could change dramatically
    and the model needs to be retrained frequently. Generally, RL algorithms and techniques
    selection affect the training efficiency and effectiveness greatly.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Integration with Existing Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Integrating RL-based methods into the real system more naturally and seamlessly
    faces many challenges. The RL agent has to be evolved when the system environment
    changes (e.g., workload) and the performance is degraded. We need to design new
    model management mechanisms to monitor, maintain, and upgrade the models. Furthermore,
    we find that the RL-based solutions can be lightweight or intrusive. The lightweight
    approach in which the RL agent is not designed as a component of the system, e.g.
    using RL to generate the qd-tree[[116](#bib.bib116)], is easier to integrate into
    the system because it does not change the architecture of the system dramatically.
    In contrast, the intrusive approach such as using RL models for join order optimization[[61](#bib.bib61)]
    is deeply embedded in the system and hence may need a redesign and optimization
    of the original system architecture to support model inference efficiently. SageDB[[35](#bib.bib35)]
    proposes to learn various database system components by integrating RL and other
    ML techniques. Nevertheless, the proposed model-driven database system is yet
    to be fully implemented and benchmarked. It is likely that the data system architecture
    needs to be overhauled or significantly amended in order to graft data-driven
    RL solutions into the data system seamlessly to yield an overall performance gain.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 Reproducibility and Benchmark
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the data system optimization problem, RL algorithms are not easy to be reproduced
    due to many factors such as lacking open source codes, workload, historic statistics
    used, and the unstable performance of RL algorithms. The landscape of problems
    in system optimization is vast and diverse. It could prevent fair comparison and
    optimization for future research works and deployments in practice. Lacking benchmarks
    is another challenge to evaluate these RL approaches. The benchmarks are therefore
    to provide standardized environments and evaluation metrics to conduct experiments
    with different RL approaches. There are some efforts to mitigate the issue. For
    example, Park[[57](#bib.bib57)] is an open platform for researchers to conduct
    experiments with RL. However, it only provides a basic interface and lacks system
    specifications. There is much room to improve with regards to the reproducibility
    and benchmark in order to promote the development and adoption of RL-based methods[[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Open Challenges For Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1 Lack of Adaptability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a lack of adaptability for methods on a single component of a data
    pipeline to the whole. For example, many works focus on data cleaning tasks such
    as entity matching. However, little works have shown their efficiency in deploying
    their model in an end-to-end data pipeline. These works treat the tasks isolatedly
    from other tasks in the pipeline, thereby limiting the pipeline’s performance.
    In healthcare, each method is applied in different steps of the whole treatment
    process, without being integrated and evaluated as one pipeline. One possible
    direction could be considering DRL as a module in the data pipeline optimization.
    However, data pipeline optimization has been focusing on models simpler than DRL
    to enable fast pipeline evaluation [[53](#bib.bib53)]. How to efficiently incorporate
    DRL into the data pipeline optimization remains a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Difficulty in Comparison with Different Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To date, most works with generalized contributions are only evaluated domain-specifically.
    Research questions are often formulated in their own platform as in E-Commerce.
    This presents difficulty in evaluating the methods for different environments.
    For example, the confounders modeling hidden causal factors in [[83](#bib.bib83)]
    can also contribute to DRL modeling in E-commerce. This is because modeling customers’
    interests are always subject to changing environments and a new environment may
    contain hidden causal factors. For example, consumers are more willing to buy
    relevant products for certain situations such as Covid-19\. Thus a general DRL
    method is yet to show the robustness and effectiveness under the environment of
    different applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Lack of Prediction in Multi-modality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In healthcare and finance, multiple sources of data bring different perspectives.
    For example in healthcare, electronic health records, image scans, and medical
    tests can provide different features for accurate prediction. In addition, these
    sources of data with different sample frequencies provide contextual information
    for modeling a patient’s visits to the hospital or symptom development. However,
    most innovations in healthcare focus on one particular source of data. How to
    integrate the contextual information with multi-modality effectively remains an
    unsolved difficult problem.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Injecting Domain Knowledge in Experience Replay
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In high-stake applications such as healthcare and finance, injecting domain
    knowledge can make decision making in RL more robust and explainable. One possible
    way is to inject the knowledge of human beings’ experience into an agent’s experience
    pool as a prior distribution for the policy. For example, in dynamic portfolio
    optimization, a portfolio manager could have a large source of experience for
    risk management and profit optimization. Such experience could be useful for warming
    up the agent’s exploration in the search space. Some works have shown positive
    effects of domain knowledge injection on selecting important experiences (i.e.,
    transition samples) [[79](#bib.bib79)]. Notwithstanding, it remains a big challenge
    to inject useful and relevant knowledge from the experience into the agent’s experience
    pool.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Future Research Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.3.1 Data Structure Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DRL provides an alternative way to find good data structures through feedback
    instead of designing them based on human knowledge and experience, e.g., decision
    tree[[47](#bib.bib47)] and the qd-tree[[116](#bib.bib116)]. These trees are optimized
    better because they are learned by interacting with the environment. DRL has also
    been effective in graph designs (e.g., molecular graph[[117](#bib.bib117)]). However,
    large-scale graph generation using DRL is difficult and daunting because it involves
    a huge search space. Generating other important structures using DRL remains to
    be explored. Idreos et al.[[33](#bib.bib33)] propose a Data Alchemist that learns
    to synthesize data structures by DRL and other techniques including Genetic Algorithms
    and Bayesian Optimization. In summary, DRL has a role in the design of more efficient
    data structures by interacting and learning from the environment. These indexes
    have to be adaptive to different data distributions and workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Interpretability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The underlying logic behind the DRL agent is still unknown. In high-risk application
    areas such as healthcare, the adoption of DRL will be a big issue in the case
    that these approaches make wrong decisions and people do not know why it happens
    due to lack of interpretability. Many techniques have been proposed to mitigate
    the issue and provide interpretability[[76](#bib.bib76)]. However, they neglect
    domain knowledge from related fields and applications and the explanations are
    not effective to human users. To instill confidence in the deployment of DRL-based
    systems in practice, interpretability is an important component and we should
    avoid treating DRL solutions as black boxes especially in critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Robustness by Causal Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modeling real-world applications by DRL inevitably suffers from the problem
    of distribution changes. The real world has independent physical mechanisms that
    can be seen as different modules. For example, an image is subjected to the light
    of the environment. Given the modular property, a structural type of modeling
    focusing on factorizing the causal mechanisms can extract the invariant causal
    mechanisms and show robustness cross distribution changes [[82](#bib.bib82)].
    One research direction towards DRL robust decision making is to perform sampling
    from past actions from a causal perspective. Given the invariance property of
    causal mechanisms, past actions can be reused by capturing the invariant mechanisms
    in a changing environment.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Extension to Other Domains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beyond existing works, many classic problems in the data system and analytics
    could potentially be solved by DRL. For example, Polyjuice[[101](#bib.bib101)]
    learns the concurrency control algorithm for a given workload by defining fine-grained
    actions and states in the context of concurrency control. Though they use an evolutionary
    algorithm to learn and outperform a simple DRL baseline, we believe that there
    are huge potentials to further improve DRL for niche applications. Hence, we expect
    that more problems will be explored and solved with DRL in various domains in
    the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.5 Towards Intelligent and Autonomous Databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although DRL algorithms could provide breakthrough performance on many tasks
    than traditional methods, many issues need to be addressed towards intelligent
    and autonomous databases. First, database schema could be updated and DRL models
    trained on the previous snapshots may not work. DRL algorithms need to tackle
    generalization[[72](#bib.bib72)]. Second, it would be so costly and infeasible
    to train models from scratch for each scenario and setting. Transfer learning
    from existing models could be a potential way to ease the workload greatly. Third,
    we have to choose appropriate DRL algorithms automatically, in the same spirit
    as AutoML. Fourth, current DBMS systems were designed without considering much
    about the learning mechanism. A radically new DBMS design may be proposed based
    on the learning-centric architecture. To support intelligent and autonomous database
    systems, DRL models intelligent behaviors and may provide a solid basis for achieving
    artificial general intelligence based on reward maximization and trial-and-error
    experience[[88](#bib.bib88)].
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we present a comprehensive review of recent advances in utilizing
    DRL in data processing and analytics. The DRL agent could learn to understand
    and solve various tasks with the right incentives. First, we introduce basic foundations
    and practical techniques in DRL. Next, we survey and review DRL for data processing
    and analytics from two perspectives, systems and applications. We cover a large
    number of topics ranging from fundamental problems in system areas such as tuning
    and scheduling to important applications such as healthcare and fintech. Finally,
    we discuss key challenges and future directions for applying DRL in data processing
    and analytics. We hope the survey would serve as a basis for research and development
    in this emerging area, and better integration of DRL techniques into data processing
    pipelines and stacks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng,
    T. Kaftan, M. J. Franklin, A. Ghodsi, et al. Spark sql: Relational data processing
    in spark. In ACM SIGMOD, pages 1383–1394, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Auer. Using confidence bounds for exploitation-exploration trade-offs.
    Journal of Machine Learning Research, 3(Nov):397–422, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Banerjee, S. Jha, Z. Kalbarczyk, and R. Iyer. Inductive-bias-driven
    reinforcement learning for efficient schedules in heterogeneous clusters. In ICML,
    pages 629–641\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] O. Bar El, T. Milo, and A. Somech. Automatically generating data exploration
    sessions using deep reinforcement learning. In ACM SIGMOD, pages 1527–1537, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Baranchuk and A. Babenko. Towards similarity graphs constructed by deep
    reinforcement learning. arXiv preprint arXiv:1911.12122, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning.
    In ICML, pages 41–48, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi,
    Q. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement
    learning. arXiv preprint arXiv:1912.06680, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Chaudhuri and V. Narasayya. Autoadmin “what-if” index analysis utility.
    ACM SIGMOD Record, 27(2):367–378, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S.-Y. Chen, Y. Yu, Q. Da, J. Tan, H.-K. Huang, and H.-H. Tang. Stabilizing
    reinforcement learning in dynamic environment with application to online recommendation.
    In ACM SIGKDD, pages 1187–1196, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. Clark and C. D. Manning. Deep reinforcement learning for mention-ranking
    coreference models. arXiv preprint arXiv:1609.08667, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Dixon and I. Halperin. G-learner and girl: Goal based wealth management
    with reinforcement learning. arXiv preprint arXiv:2002.10990, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] H. Dong, H. Dong, Z. Ding, S. Zhang, and Chang. Deep Reinforcement Learning.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. Dong and M. Lapata. Language to logical form with neural attention.
    arXiv preprint arXiv:1601.01280, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] W. Dong, C. Moses, and K. Li. Efficient k-nearest neighbor graph construction
    for generic similarity measures. In WWW, pages 577–586, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. Dou, G. Ma, P. S. Yu, and S. Xie. Robust spammer detection by nash
    reinforcement learning. In ACM SIGKDD, pages 924–933, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] G. C. Durand, M. Pinnecke, R. Piriyev, M. Mohsen, D. Broneske, G. Saake,
    M. S. Sekeran, F. Rodriguez, and L. Balami. Gridformation: towards self-driven
    online data partitioning using reinforcement learning. In Proceedings of the First
    International Workshop on Exploiting Artificial Intelligence Techniques for Data
    Management, pages 1–7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] G. C. Durand, R. Piriyev, M. Pinnecke, D. Broneske, B. Gurumurthy, and
    G. Saake. Automated vertical partitioning with deep reinforcement learning. In
    European Conference on Advances in Databases and Information Systems, pages 126–134\.
    Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. El-Bouri, D. Eyre, P. Watkinson, T. Zhu, and D. Clifton. Student-teacher
    curriculum learning via reinforcement learning: Predicting hospital inpatient
    admission location. In ICML, pages 2848–2857, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Z. Fang, Y. Cao, Q. Li, D. Zhang, Z. Zhang, and Y. Liu. Joint entity linking
    with deep reinforcement learning. In WWW, pages 438–447, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Feng, H. Li, M. Huang, S. Liu, W. Ou, Z. Wang, and X. Zhu. Learning
    to collaborate: Multi-scenario ranking via multi-agent reinforcement learning.
    In WWW, pages 1939–1948, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih,
    R. Munos, D. Hassabis, O. Pietquin, et al. Noisy networks for exploration. arXiv
    preprint arXiv:1706.10295, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Gao, L. Chen, and B. Li. Spotlight: Optimizing device placement for
    training deep neural networks. In ICML, pages 1676–1684\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and S. Wang. The rlr-tree:
    A reinforcement learning based r-tree for spatial data. arXiv preprint arXiv:2103.04541,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] I. Halperin. The qlbs q-learner goes nuqlear: fitted q iteration, inverse
    rl, and option portfolios. Quantitative Finance, 19(9):1543–1553, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable
    mdps. arXiv preprint arXiv:1507.06527, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez,
    Z. Wang, S. Eslami, et al. Emergence of locomotion behaviours in rich environments.
    arXiv preprint arXiv:1707.02286, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Heffetz, R. Vainshtein, G. Katz, and L. Rokach. Deepline: Automl tool
    for pipelines generation using deep reinforcement learning and hierarchical actions
    filtering. In ACM SIGMOD, pages 2103–2113, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Heitz and K. Stockinger. Join query optimization with deep reinforcement
    learning algorithms. arXiv preprint arXiv:1911.11689, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger.
    Deep reinforcement learning that matters. In AAAI, volume 32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] B. Hilprecht, C. Binnig, and U. Röhm. Learning a partitioning advisor
    for cloud databases. In ACM SIGMOD, pages 143–157, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Ho and S. Ermon. Generative adversarial imitation learning. arXiv preprint
    arXiv:1606.03476, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Idreos, K. Zoumpatianos, S. Chatterjee, W. Qin, A. Wasay, B. Hentschel,
    M. Kester, N. Dayan, D. Guo, M. Kang, et al. Learning data structure alchemy.
    TCDE, 42(2), 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In European
    conference on machine learning, pages 282–293. Springer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. Kraska, M. Alizadeh, A. Beutel, H. Chi, A. Kristo, G. Leclerc, S. Madden,
    H. Mao, and V. Nathan. Sagedb: A learned database system. In CIDR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for
    learned index structures. In ACM SIGMOD, pages 489–504, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning
    to optimize join queries with deep reinforcement learning. arXiv preprint arXiv:1808.03196,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum. Hierarchical
    deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.
    arXiv preprint arXiv:1604.06057, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H. Lan, Z. Bao, and Y. Peng. An index advisor using deep reinforcement
    learning. In CIKM, pages 2105–2108, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In
    Reinforcement learning, pages 45–73\. Springer, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] C. Lee, Z. Luo, K. Y. Ngiam, M. Zhang, K. Zheng, G. Chen, B. Ooi, and
    J. Yip. Big healthcare data analytics: Challenges and applications. In Handbook
    of Large-Scale Distributed Computing in Smart Healthcare, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neumann.
    How good are query optimizers, really? VLDB, 9(3):204–215, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement
    learning with gaussian processes. NIPS, 24:19–27, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] G. Li, X. Zhou, S. Li, and B. Gao. Qtune: A query-aware database tuning
    system with deep reinforcement learning. VLDB, 12(12):2118–2130, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. Li, Z. Xu, J. Tang, and Y. Wang. Model-free control for distributed
    stream data processing using deep reinforcement learning. VLDB, 11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] W. Li, X. Li, H. Li, and G. Xie. Cutsplit: A decision-tree combining cutting
    and splitting for scalable packet classification. In IEEE INFOCOM 2018-IEEE Conference
    on Computer Communications, pages 2645–2653\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] E. Liang, H. Zhu, X. Jin, and I. Stoica. Neural packet classification.
    In ACM SIGCOMM, pages 256–269\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] X. Liang, A. J. Elmore, and S. Krishnan. Opportunistic view materialization
    with deep reinforcement learning. arXiv preprint arXiv:1903.01363, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning.
    arXiv preprint arXiv:1509.02971, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] L.-J. Lin. Self-improving reactive agents based on reinforcement learning,
    planning and teaching. Machine learning, 8(3-4):293–321, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, and O. Farri.
    Diagnostic inferencing via improving clinical concept extraction with deep reinforcement
    learning: A preliminary study. In MLHC, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] K. Liu, Y. Fu, P. Wang, L. Wu, R. Bo, and X. Li. Automating feature subspace
    exploration via multi-agent reinforcement learning. In ACM SIGKDD, pages 207–215,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Z. Luo, S. H. Yeung, M. Zhang, K. Zheng, L. Zhu, G. Chen, F. Fan, Q. Lin,
    K. Y. Ngiam, and B. Chin Ooi. Mlcask: Efficient management of component evolution
    in collaborative data analytics pipelines. In ICDE, pages 1655–1666, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim. Applications of deep reinforcement learning in communications and networking:
    A survey. IEEE Communications Surveys & Tutorials, 21(4):3133–3174, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest
    neighbor search using hierarchical navigable small icde graphs. PAMI, 42(4):824–836,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, A. Hung Byers,
    et al. Big data: The next frontier for innovation, competition, and productivity.
    McKinsey Global Institute, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Mao, P. Negi, A. Narayan, H. Wang, J. Yang, H. Wang, R. Marcus, R. Addanki,
    M. Khani Shirkoohi, S. He, et al. Park: An open platform for learning-augmented
    computer systems. NIPS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, and M. Alizadeh.
    Learning scheduling algorithms for data processing clusters. In ACM SIGCOMM, pages
    270–288\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] H. Mao, S. B. Venkatakrishnan, M. Schwarzkopf, and M. Alizadeh. Variance
    reduction for reinforcement learning in input-driven environments. In ICLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] R. Marcus, P. Negi, H. Mao, N. Tatbul, M. Alizadeh, and T. Kraska. Bao:
    Making learned query optimization practical. In ACM SIGMOD, pages 1275–1288, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join order
    enumeration. In Proceedings of the First International Workshop on Exploiting
    Artificial Intelligence Techniques for Data Management, pages 1–4, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] F. D. Matthew, H. Igor, and B. Paul. Machine learning in finance: From
    theory to practice, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman,
    D. Tsai, M. Amde, S. Owen, et al. Mllib: Machine learning in apache spark. The
    Journal of Machine Learning Research, 17(1):1235–1241, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML,
    pages 1928–1937\. PMLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
    arXiv:1312.5602, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control
    through deep reinforcement learning. nature, 518(7540):529–533, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] K. Narasimhan, A. Yala, and R. Barzilay. Improving information extraction
    by acquiring external evidence with reinforcement learning. arXiv preprint arXiv:1603.07954,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning.
    In ICML, volume 1, page 2, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M.-h. Oh and G. Iyengar. Sequential anomaly detection using inverse reinforcement
    learning. In ACM SIGMOD, pages 1480–1490, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig latin:
    a not-so-foreign language for data processing. In ACM SIGMOD, pages 1099–1110,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B. C. Ooi, R. Sacks-Davis, and J. Han. Indexing in spatial databases.
    Unpublished/Technical Papers, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song. Assessing
    generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] G. Pang, A. van den Hengel, C. Shen, and L. Cao. Toward deep supervised
    anomaly detection: Reinforcement learning from partially labeled anomaly data.
    In ACM SIGKDD, pages 1298–1308, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. L. Perez and C. M. Jermaine. History-aware query optimization with
    materialized intermediate views. In ICDE, pages 520–531, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen,
    T. Asfour, P. Abbeel, and M. Andrychowicz. Parameter space noise for exploration.
    arXiv preprint arXiv:1706.01905, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] E. Puiutta and E. M. Veith. Explainable reinforcement learning: A survey.
    In CD-MAKE, pages 77–95\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist
    systems, volume 37. University of Cambridge, Department of Engineering Cambridge,
    UK, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Sadri, L. Gruenwald, and E. Lead. Drlindex: deep reinforcement learning
    index advisor for a cluster database. In Proceedings of the 24th Symposium on
    International Database Engineering & Applications, pages 1–8, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience
    replay. arXiv preprint arXiv:1511.05952, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region
    policy optimization. In ICML, pages 1889–1897\. PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal
    policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal,
    and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE,
    109(5):612–634, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] W. Shang, Y. Yu, Q. Li, Z. Qin, Y. Meng, and J. Ye. Environment reconstruction
    with hidden confounders for reinforcement learning based recommendation. In ACM
    SIGKDD, pages 566–576, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Sharma, F. M. Schuhknecht, and J. Dittrich. The case for automatic
    database administration using deep reinforcement learning. arXiv preprint arXiv:1801.05643,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering
    the game of go with deep neural networks and tree search. nature, 529(7587):484–489,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
    M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning
    algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller.
    Deterministic policy gradient algorithms. In ICML, pages 387–395\. PMLR, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] D. Silver, S. Singh, D. Precup, and R. S. Sutton. Reward is enough. Artificial
    Intelligence, page 103535, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient
    methods for reinforcement learning with function approximation. In NIPS, pages
    1057–1063, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] R. Takanobu, T. Zhuang, M. Huang, J. Feng, H. Tang, and B. Zheng. Aggregating
    e-commerce search results from heterogeneous sources via hierarchical reinforcement
    learning. In WWW, pages 1771–1781, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. R. Thompson. On the likelihood that one unknown probability exceeds
    another in view of the evidence of two samples. Biometrika, 25:285–294, 1933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
    P. Wyckoff, and R. Murthy. Hive: a warehousing solution over a map-reduce framework.
    VLDB, 2(2):1626–1629, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] I. Trummer, J. Wang, D. Maram, S. Moseley, S. Jo, and J. Antonakakis.
    Skinnerdb: Regret-bounded query evaluation via reinforcement learning. In ACM
    SIGMOD, pages 1153–1170, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] K. Tzoumas, T. Sellis, and C. S. Jensen. A reinforcement learning approach
    for adaptive query processing. History, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion.
    Physical review, 36:823, 1930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Vamanan, G. Voskuilen, and T. Vijaykumar. Efficuts: Optimizing packet
    classification for memory and throughput. ACM SIGCOMM, 40(4):207–218, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang. Automatic database
    management system tuning through large-scale machine learning. In ACM SIGMOD,
    pages 1009–1024, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with
    double q-learning. In AAAI, volume 30, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Wang, H. He, M. Alizadeh, and H. Mao. Learning caching policies with
    subsampling. In NeurIPS Machine Learning for Systems Workshop, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] J. Wang, D. Ding, H. Wang, C. Christensen, Z. Wang, H. Chen, and J. Li.
    Polyjuice: High-performance transactions via learned concurrency control. In OSDI,
    pages 198–216, July 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] L. Wang, Q. Weng, W. Wang, C. Chen, and B. Li. Metis: learning to schedule
    long-running applications in shared container clusters at scale. In SC20, pages
    1–17\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Wang, W. Zhang, X. He, and H. Zha. Supervised reinforcement learning
    with recurrent neural network for dynamic treatment recommendation. In ACM SIGKDD,
    pages 2447–2456, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental
    comparison of graph-based approximate nearest neighbor search. arXiv preprint
    arXiv:2101.12631, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] P. Wang, K. Liu, L. Jiang, X. Li, and Y. Fu. Incremental mobile user
    profiling: Reinforcement learning with spatial knowledge graph for modeling event
    streams. In ACM SIGKDD, pages 853–861, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J. Shao,
    and M. Reyad. Rafiki: machine learning as an analytics service system. VLDB, 12(2):128–140,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. Wang, M. Zhang, G. Chen, H. Jagadish, B. C. Ooi, and K.-L. Tan. Database
    meets deep learning: Challenges and opportunities. ACM SIGMOD Record, 45(2):17–22,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas.
    Dueling network architectures for deep reinforcement learning. In ICML, pages
    1995–2003\. PMLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] J. Welborn, M. Schaarschmidt, and E. Yoneki. Learning index selection
    with structured action spaces. arXiv preprint arXiv:1909.07440, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] N. Wu and P. Li. Phoebe: Reuse-aware online caching with reinforcement
    learning for emerging storage models. arXiv preprint arXiv:2011.07160, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Wu, X. Yu, X. Feng, F. Li, W. Cao, and G. Chen. Progressive neural
    index search for database system. arXiv preprint arXiv:1912.07001, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries from
    natural language without reinforcement learning. arXiv preprint arXiv:1711.04436,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Y. Yang, Q. Qiao, P. A. Beling, W. T. Scherer, and A. A. Kirilenko.
    Gaussian process-based algorithmic trading strategy identification. Quantitative
    Finance, 15(10):1683–1703, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Y. Yang, Y. Yu, and S. Almahdi. An investor sentiment reward-based
    trading system using gaussian inverse reinforcement learning algorithm. Expert
    Systems with Applications, 114:388–401, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li, U. F. Minhas, P.-Å.
    Larson, D. Kossmann, and R. Acharya. Qd-tree: Learning data layouts for big data
    analytics. In ACM SIGMOD, pages 193–208, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional
    policy network for goal-directed molecular graph generation. In NIPS, pages 6412–6422,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] C. Yu, J. Liu, and S. Nemati. Reinforcement learning in healthcare: A
    survey. arXiv preprint, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] X. Yu, G. Li, C. Chai, and N. Tang. Reinforcement learning with tree-lstm
    for join order selection. In ICDE, pages 1297–1308\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] X. Yu, Y. Peng, F. Li, S. Wang, X. Shen, H. Mai, and Y. Xie. Two-level
    data compression using machine learning in time series database. In ICDE, pages
    1333–1344\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. Yuan, G. Li, L. Feng, J. Sun, and Y. Han. Automatic view generation
    with deep learning and reinforcement learning. In ICDE, pages 1501–1512\. IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Zhang, R. Marcus, A. Kleiman, and O. Papaemmanouil. Buffer pool aware
    query scheduling via deep reinforcement learning. arXiv preprint arXiv:2007.10568,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang,
    T. Cheng, L. Liu, et al. An end-to-end automatic cloud database tuning system
    using deep reinforcement learning. In ACM SIGMOD, pages 415–432, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He. Deep reinforcement learning
    for sponsored search real-time bidding. In ACM SIGKDD, pages 1021–1030, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries
    from natural language using reinforcement learning. arXiv preprint, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning.
    arXiv preprint arXiv:1611.01578, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Zou, P. Barhate, A. Das, A. Iyengar, B. Yuan, D. Jankov, and C. Jermaine.
    Lachesis: Automated generation of persistent partitionings for big data applications.
    VLDB, 14(8), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
