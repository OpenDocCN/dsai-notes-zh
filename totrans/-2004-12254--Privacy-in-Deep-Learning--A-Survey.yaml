- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:01:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.12254] Privacy in Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.12254] 深度学习中的隐私：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12254](https://ar5iv.labs.arxiv.org/html/2004.12254)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.12254](https://ar5iv.labs.arxiv.org/html/2004.12254)
- en: 'Privacy in Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的隐私：综述
- en: Fatemehsadat Mireshghallah¹, Mohammadkazem Taram¹, Praneeth Vepakomma²,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Fatemehsadat Mireshghallah¹, Mohammadkazem Taram¹, Praneeth Vepakomma²,
- en: Abhishek Singh², Ramesh Raskar², Hadi Esmaeilzadeh¹,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Abhishek Singh², Ramesh Raskar², Hadi Esmaeilzadeh¹,
- en: ¹ University of California San Diego, ² Massachusetts Institute of Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 加州大学圣地亚哥分校，² 麻省理工学院
- en: '{fatemeh, mtaram, hadi}@ucsd.edu,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{fatemeh, mtaram, hadi}@ucsd.edu,'
- en: '{vepakom, abhi24, raskar}@mit.edu'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{vepakom, abhi24, raskar}@mit.edu'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ever-growing advances of deep learning in many areas including vision, recommendation
    systems, natural language processing, etc., have led to the adoption of Deep Neural
    Networks (DNNs) in production systems. The availability of large datasets and
    high computational power are the main contributors to these advances. The datasets
    are usually crowdsourced and may contain sensitive information. This poses serious
    privacy concerns as this data can be misused or leaked through various vulnerabilities.
    Even if the cloud provider and the communication link is trusted, there are still
    threats of inference attacks where an attacker could speculate properties of the
    data used for training, or find the underlying model architecture and parameters.
    In this survey, we review the privacy concerns brought by deep learning, and the
    mitigating techniques introduced to tackle these issues. We also show that there
    is a gap in the literature regarding test-time inference privacy, and propose
    possible future research directions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多领域的持续进步，包括视觉、推荐系统、自然语言处理等，已经导致深度神经网络（DNNs）在生产系统中的广泛应用。大型数据集和高计算能力是这些进步的主要推动因素。这些数据集通常是众包获得的，可能包含敏感信息。这带来了严重的隐私问题，因为这些数据可能被滥用或通过各种漏洞泄露。即使云服务提供商和通信链路是可信的，仍然存在推断攻击的威胁，攻击者可能推测用于训练的数据的特性，或找到基础模型的架构和参数。在本综述中，我们回顾了深度学习带来的隐私问题以及为解决这些问题而提出的缓解技术。我们还指出了关于测试时推断隐私的文献中的空白，并提出了可能的未来研究方向。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The success of Deep Neural Networks (DNNs) in various fields including vision,
    medicine, recommendation systems, natural language processing, etc., has resulted
    in their deployment in numerous production systems [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]. In the world of medicine, learning is used to
    find patterns in patient histories and to recognize abnormalities in medical imaging
    which help with disease diagnosis and prognosis. The use of machine learning in
    healthcare can compromise patient privacy, for instance by exposing the patient’s
    genetic markers, as shown by Fredrikson et al. [[5](#bib.bib5)]. Deep learning
    is also widely used in finance for predicting prices or creating portfolios, among
    many other applications. In these cases, usually, an entity trains its own model
    and the model parameters are considered confidential. Being able to find or infer
    them is considered a breach of privacy [[6](#bib.bib6)]. Ease of access to large
    datasets and high computational power (GPUs and TPUs) have paved the way for the
    aforementioned advances. These datasets are usually crowdsourced and might contain
    sensitive information. This poses serious privacy concerns, as neural networks
    are used in different aspects of our lives [[7](#bib.bib7), [5](#bib.bib5), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）在视觉、医学、推荐系统、自然语言处理等各个领域的成功，导致它们在众多生产系统中的部署 [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]。在医学领域，学习用于发现患者历史中的模式，并识别医学影像中的异常，这有助于疾病的诊断和预后。正如
    Fredrikson 等人所示，机器学习在医疗保健中的使用可能会泄露患者的基因标记，从而危及患者隐私 [[5](#bib.bib5)]。深度学习还被广泛应用于金融领域，用于预测价格或创建投资组合等众多应用。在这些情况下，通常一个实体会训练自己的模型，模型参数被视为机密。能够发现或推断这些参数被认为是隐私的泄露 [[6](#bib.bib6)]。对大型数据集和高计算能力（GPUs
    和 TPUs）的轻松访问为上述进展铺平了道路。这些数据集通常是众包获得的，可能包含敏感信息。这带来了严重的隐私问题，因为神经网络被用于我们生活的不同方面 [[7](#bib.bib7),
    [5](#bib.bib5), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]。
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Direct Information Exposure ‣ 2 Existing
    Threats ‣ Privacy in Deep Learning: A Survey") shows a classification of possible
    threats to deep learning. One threat is the direct intentional or unintentional
    exposure of sensitive information, through untrusted data curator, communication
    link, or cloud [[13](#bib.bib13), [14](#bib.bib14)]. This information can be the
    training data, inference queries or model parameters or hyperparameters. If we
    assume that information cannot be attained directly, there is still the threat
    of information exposure through inference, indirectly. Membership inference attacks [[15](#bib.bib15)]
    can infer whether a given data instance was part of the training process of a
    model. Model inversion and attribute inference attacks can infer sensitive features
    about a data instance, from observed predictions of a trained model, and other
    non-sensitive features of that data instance [[16](#bib.bib16), [17](#bib.bib17)].
    Some attacks are targeted towards stealing information about a deployed model,
    such as its architecture [[18](#bib.bib18)], trained parameters [[19](#bib.bib19)]
    or a general property of the data it was trained on, for instance, if the images
    used for training were all taken outdoor [[20](#bib.bib20)].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S2.F1 "Figure 1 ‣ 2.1 Direct Information Exposure ‣ 2 Existing Threats
    ‣ Privacy in Deep Learning: A Survey") 展示了可能对深度学习构成威胁的分类。其中一种威胁是通过不可信的数据策展人、通信链路或云 [[13](#bib.bib13),
    [14](#bib.bib14)]，直接有意或无意地暴露敏感信息。这些信息可以是训练数据、推理查询或模型参数或超参数。如果我们假设信息不能直接获得，仍然存在通过推理间接暴露信息的威胁。成员推理攻击 [[15](#bib.bib15)]
    可以推断某个数据实例是否是模型训练过程的一部分。模型反演和属性推断攻击可以从训练模型的观察预测中推断数据实例的敏感特征以及该数据实例的其他非敏感特征 [[16](#bib.bib16),
    [17](#bib.bib17)]。一些攻击则针对窃取关于已部署模型的信息，例如其架构 [[18](#bib.bib18)]、训练参数 [[19](#bib.bib19)]
    或其训练数据的一般属性，例如训练图像是否全部取自户外 [[20](#bib.bib20)]。'
- en: There is a myriad of methods proposed to tackle these threats. The majority
    of these methods focus on the data aggregation/dataset publishing and training
    stages of deep learning. We classify these methods into three classes. The first
    class of methods focuses on sanitizing the data and trying to remove sensitive
    information from it while maintaining the statistical trends [[21](#bib.bib21),
    [22](#bib.bib22)]. The second class focuses on making the DNN training phase private
    and protecting the data used for training [[23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]. The last
    class, of which there is only a handful of works, attempts to protect the privacy
    of the test-time inference phase by protecting the input data (request) that the
    user sends to a deployed DNN [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些威胁，已经提出了大量方法。这些方法大多数集中在数据聚合/数据集发布和深度学习训练阶段。我们将这些方法分为三类。第一类方法关注于清理数据，尝试在保持统计趋势的同时移除敏感信息 [[21](#bib.bib21),
    [22](#bib.bib22)]。第二类方法关注于使DNN训练阶段保密，并保护用于训练的数据 [[23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]。最后一类方法，只有少数几项，试图通过保护用户发送给已部署DNN的输入数据（请求）来保护测试时推理阶段的隐私 [[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)]。
- en: In this paper, we first briefly discuss existing attacks and privacy threats
    against deep learning. Then, we focus on the existing privacy-preserving methods
    for deep learning and demonstrate that there is a gap in the literature regarding
    test-time inference privacy. There are few other security vulnerabilities which
    can be exploited in a deep learning model such as adversarial attacks [[32](#bib.bib32)],
    data poisoning [[33](#bib.bib33)]. This work focuses only on privacy specific
    vulnerability and other such attacks are out of scope of this paper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先简要讨论了针对深度学习的现有攻击和隐私威胁。然后，我们重点介绍了现有的深度学习隐私保护方法，并展示了文献中关于测试时推理隐私的空白。还有一些可以在深度学习模型中利用的其他安全漏洞，例如对抗性攻击 [[32](#bib.bib32)]，数据中毒 [[33](#bib.bib33)]。这项工作仅关注隐私特定的脆弱性，其他此类攻击超出了本文的范围。
- en: 2 Existing Threats
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 现有威胁
- en: 'In this section, we map the space of existing threats against privacy in deep
    learning and machine learning in general. While the focus of this survey is privacy-preserving
    techniques, we provide a brief summary of attacks to better situate the need for
    privacy protection. Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Direct Information Exposure
    ‣ 2 Existing Threats ‣ Privacy in Deep Learning: A Survey") shows the landscape
    of these threats, which we have divided into two main categories of direct and
    indirect information exposure hazards. Direct threats are those where the attacker
    can gain access to the actual information. In indirect attacks, however, the attacker
    tries to infer or guess the information and does not have access to the actual
    information.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们映射了深度学习和一般机器学习中现有隐私威胁的空间。虽然本调查的重点是隐私保护技术，但我们提供了攻击的简要总结，以更好地说明隐私保护的必要性。图 [1](#S2.F1
    "Figure 1 ‣ 2.1 Direct Information Exposure ‣ 2 Existing Threats ‣ Privacy in
    Deep Learning: A Survey") 显示了这些威胁的全貌，我们将其分为直接和间接信息暴露的两大类。直接威胁是指攻击者可以直接访问实际信息。而在间接攻击中，攻击者试图推测或猜测信息，而无法访问实际信息。'
- en: 2.1 Direct Information Exposure
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 直接信息暴露
- en: Direct intentional or unintentional data breaches can occur in many different
    settings and are not limited to machine learning. Dataset breaches through data
    curators or entities housing the data can be caused unintentionally by hackers,
    malware, virus, or social engineering, by tricking individuals into handing over
    sensitive data to adversaries [[13](#bib.bib13)]. A study by Intel Security [[34](#bib.bib34)]
    demonstrated that employees are responsible for 43% of data leakage, half of which
    is believed to be unintentional. A malicious party can exploit a system’s backdoor
    to bypass a server’s authentication mechanism and gain direct access to sensitive
    datasets, or sensitive parameters and models [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)]. The recent hacking of Equifax, for instance, exploited a vulnerability
    in the Apache Struts software, which was used by Equifax [[35](#bib.bib35)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 直接的有意或无意的数据泄露可能发生在许多不同的环境中，并不仅限于机器学习。数据集通过数据管理者或存储数据的实体发生泄露，可能是由于黑客、恶意软件、病毒或社会工程学的无意行为，这些行为通过欺骗个人将敏感数据交给对手 [[13](#bib.bib13)]。英特尔安全的研究 [[34](#bib.bib34)]
    表明，员工对数据泄露负责的比例为43%，其中一半被认为是无意的。恶意方可以利用系统的后门绕过服务器的认证机制，直接访问敏感数据集，或敏感参数和模型 [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)]。例如，近期对Equifax的黑客攻击利用了Apache Struts软件中的一个漏洞，该软件被Equifax使用 [[35](#bib.bib35)]。
- en: Data sharing by transmitting confidential data without proper encryption is
    an example of data exposure through communication link [[38](#bib.bib38)]. Kaspersky
    Labs reported in 2018 that they found four million Android apps that were sending
    unencrypted user profile data to advertisers’ servers [[39](#bib.bib39)]. Private
    data can also be exposed through the cloud service that receives it to run a process
    on it, for instance, Machine Learning as a Service (MLaaS). Some of these services
    do not clarify what happens to the data once the process is finished, nor do they
    even mention that they are sending the user’s data to the cloud, and not processing
    it locally [[8](#bib.bib8)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过未经适当加密传输机密数据来进行数据共享是数据通过通信链路暴露的一个例子 [[38](#bib.bib38)]。卡巴斯基实验室在2018年报告称，他们发现了四百万个将未加密的用户资料数据发送到广告商服务器的Android应用程序 [[39](#bib.bib39)]。私密数据也可能通过接收数据并在其上运行过程的云服务被暴露，例如机器学习即服务（MLaaS）。一些这些服务没有说明数据处理完毕后的去向，也没有提到他们将用户的数据发送到云端，而不是在本地处理 [[8](#bib.bib8)]。
- en: 'Table 1: Properties of some notable attacks against machine learning privacy.
    MIA denotes Model Inversion Attack in the table.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：一些显著针对机器学习隐私的攻击特性。MIA 表示表中的模型逆转攻击。
- en: '| Attack | Membership | Model | Hyperparam | Parameter | Property | Access
    | Access |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 会员 | 模型 | 超参数 | 参数 | 属性 | 访问 | 访问 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | Inference | Inversion | Inference | Inference | Inference | to Model |
    to Output |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | 推断 | 逆转 | 推断 | 推断 | 推断 | 到模型 | 到输出 |'
- en: '| Membership Inference [[15](#bib.bib15)] | ● | ○ | ○ | ○ | ○ | Blackbox |
    Logits |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 会员推断 [[15](#bib.bib15)] | ● | ○ | ○ | ○ | ○ | 黑箱 | 逻辑回归 |'
- en: '| Measuring Membership Privacy[[40](#bib.bib40)] | ● | ○ | ○ | ○ | ○ | Blackbox
    | Logits |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 测量会员隐私 [[40](#bib.bib40)] | ● | ○ | ○ | ○ | ○ | 黑箱 | 逻辑回归 |'
- en: '| ML-Leaks[[41](#bib.bib41)] | ● | ○ | ○ | ○ | ○ | Blackbox | Logits |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ML-Leaks [[41](#bib.bib41)] | ● | ○ | ○ | ○ | ○ | 黑箱 | 逻辑回归 |'
- en: '| The Natural Auditor [[42](#bib.bib42)] | ● | ○ | ○ | ○ | ○ | Blackbox | Label
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 自然审计者 [[42](#bib.bib42)] | ● | ○ | ○ | ○ | ○ | 黑箱 | 标签 |'
- en: '| LOGAN [[43](#bib.bib43)] | ● | ○ | ○ | ○ | ○ | Both | Logits |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| LOGAN [[43](#bib.bib43)] | ● | ○ | ○ | ○ | ○ | 两者 | Logits |'
- en: '| Data Provenance [[44](#bib.bib44)] | ● | ○ | ○ | ○ | ○ | Blackbox | Logits
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 数据来源 [[44](#bib.bib44)] | ● | ○ | ○ | ○ | ○ | 黑箱 | Logits |'
- en: '| Privacy Risk in ML [[17](#bib.bib17)] | ● | ● | ○ | ○ | ○ | Whitebox | Logits+Auxilary
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习中的隐私风险 [[17](#bib.bib17)] | ● | ● | ○ | ○ | ○ | 白箱 | Logits+附加信息 |'
- en: '| Fredrikson et al. [[5](#bib.bib5)] | ○ | ● | ○ | ○ | ○ | Blackbox | Logits
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Fredrikson 等 [[5](#bib.bib5)] | ○ | ● | ○ | ○ | ○ | 黑箱 | Logits |'
- en: '| MIA w/ Confidence Values [[16](#bib.bib16)] | ○ | ● | ○ | ○ | ○ | Both |
    Logits |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 带置信度值的 MIA [[16](#bib.bib16)] | ○ | ● | ○ | ○ | ○ | 两者 | Logits |'
- en: '| Adversarial NN Inversion [[45](#bib.bib45)] | ○ | ● | ○ | ○ | ○ | Blackbox
    | Logits |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性神经网络逆向 [[45](#bib.bib45)] | ○ | ● | ○ | ○ | ○ | 黑箱 | Logits |'
- en: '| Updates-Leak [[46](#bib.bib46)] | ○ | ● | ○ | ○ | ○ | Blackbox | Logits |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 更新泄漏 [[46](#bib.bib46)] | ○ | ● | ○ | ○ | ○ | 黑箱 | Logits |'
- en: '| Collaborative Inference MIA [[47](#bib.bib47)] | ○ | ● | ○ | ○ | ○ | Both
    | Logits |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 协作推断 MIA [[47](#bib.bib47)] | ○ | ● | ○ | ○ | ○ | 两者 | Logits |'
- en: '| The Secret Sharer [[48](#bib.bib48)] | ○ | ○ | ○ | ○ | ● | Blackbox | Logits
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 秘密分享者 [[48](#bib.bib48)] | ○ | ○ | ○ | ○ | ● | 黑箱 | Logits |'
- en: '| Property Inference on FCNNs [[20](#bib.bib20)] | ○ | ○ | ○ | ○ | ● | Whitebox
    | Logits |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| FCNNs 的属性推断 [[20](#bib.bib20)] | ○ | ○ | ○ | ○ | ● | 白箱 | Logits |'
- en: '| Hacking Smart Machines w [[49](#bib.bib49)] | ○ | ○ | ○ | ○ | ● | Whitebox
    | Logits |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 入侵智能机器 [[49](#bib.bib49)] | ○ | ○ | ○ | ○ | ● | 白箱 | Logits |'
- en: '| Cache Telepathy [[18](#bib.bib18)] | ○ | ○ | ○ | ● | ○ | Blackbox | Logits
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 缓存心灵感应 [[18](#bib.bib18)] | ○ | ○ | ○ | ● | ○ | 黑箱 | Logits |'
- en: '| Stealing Hyperparameters [[50](#bib.bib50)] | ○ | ○ | ○ | ● | ○ | Blackbox
    | Logits |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 偷窃超参数 [[50](#bib.bib50)] | ○ | ○ | ○ | ● | ○ | 黑箱 | Logits |'
- en: '| Stealing ML Models [[19](#bib.bib19)] | ○ | ○ | ● | ● | ○ | Blackbox | Label
    | ![Refer to caption](img/478f15d3f996391ed818748abfb2c3a3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| 偷窃机器学习模型 [[19](#bib.bib19)] | ○ | ○ | ● | ● | ○ | 黑箱 | 标签 | ![参考说明](img/478f15d3f996391ed818748abfb2c3a3.png)'
- en: 'Figure 1: Categorization of existing threats against deep learning'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 现有深度学习威胁的分类'
- en: 2.2 Indirect (Inferred) Information Exposure
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 间接（推断）信息暴露
- en: 'As shown in figure [1](#S2.F1 "Figure 1 ‣ 2.1 Direct Information Exposure ‣
    2 Existing Threats ‣ Privacy in Deep Learning: A Survey"), we categorize indirect
    attacks into 5 main groups of membership inference, model inversion, hyperparameter
    inference, parameter inference, and property inference attacks. Table [1](#S2.T1
    "Table 1 ‣ 2.1 Direct Information Exposure ‣ 2 Existing Threats ‣ Privacy in Deep
    Learning: A Survey") shows a summary of different attacks and their properties.
    The “Access to Model” column determines whether the attack needs white-box or
    black-box access to model to successfully mount. White-box access assumes access
    to the full target model, whereas black-box assumes only query access to the model,
    without knowledge on the architecture or parameters of the target model. The last
    column shows whether the attacker needs access to the output confidence values
    of the model (the probabilities, logits), or whether only the predicted labels
    suffice.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1](#S2.F1 "图 1 ‣ 2.1 直接信息暴露 ‣ 2 现有威胁 ‣ 深度学习中的隐私：调查") 所示，我们将间接攻击分为 5 大类：成员推断、模型逆向、超参数推断、参数推断和属性推断攻击。表
    [1](#S2.T1 "表 1 ‣ 2.1 直接信息暴露 ‣ 2 现有威胁 ‣ 深度学习中的隐私：调查") 显示了不同攻击及其属性的总结。“模型访问”列决定了攻击是否需要白箱或黑箱访问模型才能成功实施。白箱访问假定可以访问完整的目标模型，而黑箱访问仅假定可以查询模型，但不了解目标模型的架构或参数。最后一列显示了攻击者是否需要访问模型的输出置信度值（概率，logits），或者仅预测标签是否足够。
- en: 2.2.1 Membership Inference
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 成员推断
- en: 'Given a data instance and (black-box or white-box) access to a pre-trained
    target model, a membership inference attack speculates whether or not the given
    data instance has contributed to the training step of the target model. Shokri
    et al. [[15](#bib.bib15)] propose the first membership inference attack on machine
    learning where they consider an attacker who has black-box query access to the
    target model and can obtain confidence scores (probability vector) for the queried
    input. The attacker uses this confidence score to deduce the participation of
    given data in training. They first train shadow models on a labeled dataset that
    can be generated using three methods: model inversion attack (we will see next),
    statistics-based synthesis (through assumptions about the underlying distribution
    of training set), or noisy real data. Using these shadow models, the attacker
    trains an “attack model” that distinguishes the participation of a data instance
    in the training set of the shadow models. Lastly, for the main inference attack,
    the attacker makes queries to the target deployed model to receive confidence
    scores for each given input data instance and infers whether or not the input
    was part of the target training data. This attack is built on the assumption that
    if a record was used in the training of a model, it would yield a higher confidence
    score, than a record which was not seen before by the model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据实例和对预训练目标模型的（黑箱或白箱）访问，成员推断攻击会推测给定的数据实例是否参与了目标模型的训练步骤。Shokri 等人[[15](#bib.bib15)]
    提出了首个机器学习中的成员推断攻击，他们考虑了一个攻击者，该攻击者具有对目标模型的黑箱查询访问权限，并且可以获得查询输入的置信度评分（概率向量）。攻击者使用这个置信度评分来推断给定数据在训练中的参与情况。他们首先在标记数据集上训练影子模型，该数据集可以通过三种方法生成：模型反演攻击（我们将接下来讨论）、基于统计的合成（通过对训练集的底层分布的假设）或带噪声的真实数据。利用这些影子模型，攻击者训练一个“攻击模型”，以区分数据实例是否参与了影子模型的训练集。最后，对于主要的推断攻击，攻击者对目标部署模型进行查询，以接收每个给定输入数据实例的置信度评分，并推断输入是否属于目标训练数据的一部分。这种攻击建立在这样的假设上：如果一个记录用于模型的训练，它将产生比模型以前未见过的记录更高的置信度评分。
- en: Some studies  [[51](#bib.bib51), [52](#bib.bib52), [17](#bib.bib17)] attribute
    membership inference attacks to the generalization gap, the over-fitting of the
    model, and data memorization capabilities of neural networks. Deep neural networks
    have been shown to memorize the training data [[53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55)], rather than learning the latent properties of the data, which
    means they often tend to over-fit to the training data. Long et al.  [[40](#bib.bib40)]
    propose an approach which more accurately tests the membership of a given instance.
    They train the shadow models with and without this given instance, and then at
    inference time the attacker tests to see if the instance was used for training
    the target model, similar to Shokri et al.’s approach. More recently, Salem et
    al. [[41](#bib.bib41)] propose a more generic attack that could relax the main
    requirements in previous attacks (such as using multiple shadow models, knowledge
    of the target model structure, and having a dataset from the same distribution
    as the target model’s training data), and show that such attacks are also applicable
    at a lower cost, without significantly degrading their effectiveness.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[51](#bib.bib51), [52](#bib.bib52), [17](#bib.bib17)] 将成员推断攻击归因于泛化差距、模型的过拟合和神经网络的数据记忆能力。已经证明深度神经网络会记住训练数据[[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)]，而不是学习数据的潜在属性，这意味着它们往往倾向于过度拟合训练数据。Long 等人[[40](#bib.bib40)]
    提出了一种更准确测试给定实例成员身份的方法。他们在有和没有这个给定实例的情况下训练影子模型，然后在推断时，攻击者测试实例是否用于训练目标模型，这与 Shokri
    等人的方法类似。最近，Salem 等人[[41](#bib.bib41)] 提出了一个更通用的攻击方法，这种方法可以放宽之前攻击中的主要要求（例如使用多个影子模型、了解目标模型结构，以及拥有与目标模型训练数据相同分布的数据集），并显示这些攻击在成本较低的情况下仍然有效，而不会显著降低其效果。
- en: Membership inference attacks do not always need access to the confidence values
    (logits) of the target model, as shown by Song & Shmatikov in a recent attack [[42](#bib.bib42)],
    which can detect with very few queries to a model if a particular user’s texts
    were used to train it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 成员推断攻击并不总是需要访问目标模型的置信值（logits），正如宋和施马季科夫在最近的攻击中所示[[42](#bib.bib42)]，该攻击可以通过对模型进行非常少量的查询来检测特定用户的文本是否用于训练模型。
- en: Yeom et al [[17](#bib.bib17)] suggest a membership inference attack for cases
    where the attacker can have white-box access to the target model and know the
    average training loss of the model. In this attack, for an input record, the attacker
    evaluates the loss of the model and if the loss is smaller than a threshold (the
    average loss on the training set), the input record is deemed part of the training
    set. Membership inference attacks can also be applied to Generative Adversarial
    Networks (GANs), as shown by Hayes et al. [[43](#bib.bib43)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Yeom等人 [[17](#bib.bib17)] 提出了一种成员推断攻击，适用于攻击者可以获得目标模型的白箱访问权限并知道模型的平均训练损失的情况。在这种攻击中，对于一个输入记录，攻击者评估模型的损失，如果损失小于阈值（训练集上的平均损失），则该输入记录被认为是训练集的一部分。成员推断攻击也可以应用于生成对抗网络（GANs），如Hayes等人 [[43](#bib.bib43)]
    所示。
- en: '![Refer to caption](img/9717f32e8fdc63b6e4eb8b450023e215.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9717f32e8fdc63b6e4eb8b450023e215.png)'
- en: 'Figure 2: The image on the left was recovered using the model inversion attack
    of Fredrikson et al. [[16](#bib.bib16)]. The image on the right shows an image
    from the training set. The attacker is given only the person’s name and access
    to a facial recognition system that returns a class confidence score [[16](#bib.bib16)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：左侧的图像是使用Fredrikson等人的模型反演攻击恢复的 [[16](#bib.bib16)]。右侧的图像显示了训练集中的一张图像。攻击者仅获得了个人的姓名以及访问一个返回分类置信度分数的面部识别系统的权限 [[16](#bib.bib16)]。
- en: 2.2.2 Model Inversion and Attribute Inference
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 模型反演和属性推断
- en: Model inversion and attribute inference attacks are against attribute privacy,
    where an adversary tries to infer sensitive attributes of given data instances
    from a released model and the instance’s non-sensitive attributes [[56](#bib.bib56)].
    The most prominent of these attacks is against a publicly-released linear regression
    model, where Fredrikson et al. [[5](#bib.bib5)] invert the model of a medicine
    (Warfarin) dosage prediction task. They recover genomic information about the
    patient, based on the model output and several other non-sensitive attributes
    (e.g., height, age, weight). This attack can be applied with only black-box API
    access to the target model. Fredrikson et al. formalize this attack as maximizing
    the posterior probability estimate of the sensitive attribute. In other words,
    the attacker assumes that features $f_{1}$ to $f_{d-1}$, of the $f_{d}$ features
    of each data instance are non-sensitive. the attacker then tries to maximize the
    posterior probability of feature $f_{d}$, given the nonsensitive features of $f_{1}$
    to $f_{d-1}$, and the model output.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反演和属性推断攻击针对属性隐私，攻击者尝试从发布的模型和实例的非敏感属性中推断给定数据实例的敏感属性 [[56](#bib.bib56)]。这些攻击中最显著的是针对公开发布的线性回归模型，其中Fredrikson等人 [[5](#bib.bib5)]
    反演了药物（华法林）剂量预测任务的模型。他们基于模型输出和其他若干非敏感属性（如身高、年龄、体重）恢复了患者的基因组信息。这种攻击只需对目标模型进行黑箱API访问即可应用。Fredrikson等人将此攻击形式化为最大化敏感属性的后验概率估计。换句话说，攻击者假设每个数据实例的特征$f_{1}$到$f_{d-1}$是非敏感的。然后，攻击者试图最大化特征$f_{d}$的后验概率，给定特征$f_{1}$到$f_{d-1}$的非敏感特征以及模型输出。
- en: 'In another work, given white-box access to a neural network, Fredrikson et
    al. [[16](#bib.bib16)] show that they could extract instances of training data,
    from observed model predictions. Figure [2](#S2.F2 "Figure 2 ‣ 2.2.1 Membership
    Inference ‣ 2.2 Indirect (Inferred) Information Exposure ‣ 2 Existing Threats
    ‣ Privacy in Deep Learning: A Survey") shows a recovered face image that is similar
    to the input image and was reconstructed by utilizing the confidence score of
    the target model. Yeom et al. [[17](#bib.bib17)] also propose an attribute inference
    attack, built upon the same principle used for their membership inference attack,
    mentioned in Section [2.2.1](#S2.SS2.SSS1 "2.2.1 Membership Inference ‣ 2.2 Indirect
    (Inferred) Information Exposure ‣ 2 Existing Threats ‣ Privacy in Deep Learning:
    A Survey"). The attacker evaluates the model’s loss on the input instance for
    different values of the sensitive attribute and infers the value that yields a
    loss value similar to that outputted by the original data, as the sensitive value.
    Salem et al. [[46](#bib.bib46)] suggest a model inversion attack on online-learning,
    using a generative adversarial network and based on the difference between a model,
    before and after a gradient update. In the same direction, Brockschmidt et al. [[57](#bib.bib57)]
    demonstrate the information leakage of updates to language models. More recently,
    He et al. [[47](#bib.bib47)] propose a new set of attacks to compromise the privacy
    of test-time inference queries, in collaborative deep learning systems where a
    DNN is split and distributed to different participants. This scheme is called
    split learning [[58](#bib.bib58)], which is discussed in Section [4](#S4 "4 Privacy-Enhancing
    Execution Models and Environments ‣ Privacy in Deep Learning: A Survey"). They
    demonstrate that with their attack, one malicious participant can recover an arbitrary
    input fed into this system, even with no access to other participants’ data or
    computations.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中，Fredrikson 等人 [[16](#bib.bib16)] 证明，在获得神经网络的白盒访问权限的情况下，他们可以从观察到的模型预测中提取训练数据实例。图
    [2](#S2.F2 "图 2 ‣ 2.2.1 会员推断 ‣ 2.2 间接（推断的）信息曝光 ‣ 2 现有威胁 ‣ 深度学习中的隐私：调查") 显示了一个恢复的面部图像，该图像与输入图像相似，通过利用目标模型的置信度分数进行重建。Yeom
    等人 [[17](#bib.bib17)] 还提出了一种属性推断攻击，该攻击基于与其会员推断攻击相同的原理，如第 [2.2.1](#S2.SS2.SSS1
    "2.2.1 会员推断 ‣ 2.2 间接（推断的）信息曝光 ‣ 2 现有威胁 ‣ 深度学习中的隐私：调查") 节中提到的。攻击者评估模型在输入实例上的损失，对于不同的敏感属性值，并推断出产生与原始数据输出相似的损失值的敏感值。Salem
    等人 [[46](#bib.bib46)] 建议对在线学习进行模型反演攻击，使用生成对抗网络，并基于梯度更新前后模型之间的差异。在相同方向上，Brockschmidt
    等人 [[57](#bib.bib57)] 演示了语言模型更新的信息泄露。最近，He 等人 [[47](#bib.bib47)] 提出了一套新的攻击方法，以破坏测试时推断查询的隐私，在协作深度学习系统中，其中一个
    DNN 被拆分并分发给不同的参与者。这一方案被称为拆分学习 [[58](#bib.bib58)]，在第 [4](#S4 "4 隐私增强执行模型和环境 ‣ 深度学习中的隐私：调查")
    节中讨论。他们展示了，通过他们的攻击，一个恶意参与者可以恢复输入系统中的任意输入，即使没有访问其他参与者的数据或计算。
- en: '2.2.3 Model Stealing: Hyperparameter and Parameter Inference'
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 模型窃取：超参数和参数推断
- en: Trained models are considered intellectual properties of their owners and can
    be considered confidential in many cases [[6](#bib.bib6)], therefore extracting
    the model can be considered a privacy breach. Apart from this, as discussed earlier,
    DNNs are shown to memorize information about their training data, therefore exposing
    the model parameters could lead to exposure of training data. A model stealing
    attack is meant to recover the model parameters via black-box access to the target
    model. Tramer et al. [[19](#bib.bib19)] devise an attack that finds parameters
    of a model given the observation of its predictions (confidence values). Their
    attack tries to find parameters of the model through equation solving, based on
    pairs of input-outputs. This attack cannot be mounted in a setting where the confidence
    values are not provided. Hyperparameter stealing attacks try to find the hyperparameters
    used during the model training, such as the regularization coefficient  [[50](#bib.bib50)]
    or model architecture [[18](#bib.bib18)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型被视为其所有者的知识产权，并且在许多情况下可以被视为机密[[6](#bib.bib6)]，因此提取模型可能被视为隐私侵犯。除此之外，正如前面讨论的那样，深度神经网络（DNN）被证明会记住关于其训练数据的信息，因此暴露模型参数可能会导致训练数据的暴露。模型盗窃攻击旨在通过对目标模型的黑箱访问来恢复模型参数。Tramer
    等人[[19](#bib.bib19)]设计了一种攻击方法，该方法通过观察模型的预测（置信值）来找到模型的参数。他们的攻击尝试通过方程求解来找到模型参数，基于输入-输出对。该攻击无法在未提供置信值的情况下进行。超参数盗窃攻击试图找到在模型训练期间使用的超参数，例如正则化系数[[50](#bib.bib50)]或模型架构[[18](#bib.bib18)]。
- en: 2.2.4 Property Inference
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 属性推断
- en: This class of attacks tries to infer specific patterns of information from the
    target model. An example of these attacks is the memorization attack that aims
    to find sensitive patterns in the training data of a target model [[48](#bib.bib48)].
    These attacks have been mounted on Hidden Markov Models (HMM) and Support Vector
    Machines (SVM)  [[49](#bib.bib49)] and neural networks [[20](#bib.bib20)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这类攻击试图从目标模型中推断特定的信息模式。这些攻击的一个例子是记忆攻击，它旨在发现目标模型训练数据中的敏感模式[[48](#bib.bib48)]。这些攻击已针对隐马尔可夫模型（HMM）、支持向量机（SVM）[[49](#bib.bib49)]和神经网络[[20](#bib.bib20)]进行过。
- en: 3 Privacy-Preserving Mechanisms
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 隐私保护机制
- en: 'In this section, we review the literature of privacy-preserving mechanisms
    for deep learning and machine learning in general. Figure [3](#S3.F3 "Figure 3
    ‣ 3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning:
    A Survey") shows our classification of the landscape of this field. We divide
    the literature into three main groups. The first is private data aggregation methods,
    which aim at collecting data and forming datasets, while preserving the privacy
    of the contributors [[21](#bib.bib21), [22](#bib.bib22)]. The second group, which
    is comprised of a large body of work focuses on devising mechanisms that make
    the training process of models private so that sensitive information about the
    participants of the training dataset would not be exposed. Finally, the last group
    aims at the test-time inference phase of deep learning. It tries to protect the
    privacy of users of deployed models, who send their data to a trained model for
    having a given inference service carried out.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们回顾了深度学习和机器学习的一般隐私保护机制的文献。图[3](#S3.F3 "Figure 3 ‣ 3.2 Training Phase
    ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning: A Survey")展示了我们对这一领域的分类。我们将文献分为三个主要组别。第一组是私人数据聚合方法，旨在收集数据并形成数据集，同时保护贡献者的隐私[[21](#bib.bib21),
    [22](#bib.bib22)]。第二组包括大量的研究工作，重点是设计机制使得模型的训练过程保持私密，从而不暴露训练数据集中参与者的敏感信息。最后，第三组关注深度学习的测试推断阶段。它试图保护已部署模型用户的隐私，这些用户将其数据发送给训练好的模型以进行特定的推断服务。'
- en: 3.1 Data Aggregation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据聚合
- en: Here, we introduce the most prominent data privacy-preserving mechanisms. Not
    all these methods are applied to deep learning, but we briefly discuss them for
    the sake of comprehensiveness. These methods can be broadly divided into two groups
    of context-free privacy and context-aware. Context-free privacy solutions, such
    as differential privacy, are unaware of the specific context or the purpose that
    the data will be used for. Whereas context-aware privacy solutions, such as information-theoretic
    privacy, are aware of the context where the data is going to be used, and can
    achieve an improved privacy-utility tradeoff [[59](#bib.bib59)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍最突出的数据隐私保护机制。并非所有这些方法都应用于深度学习，但为了全面性，我们将简要讨论它们。这些方法可以大致分为两组：无上下文隐私和上下文感知隐私。无上下文隐私解决方案，例如差分隐私，对数据将被使用的具体上下文或目的并不知晓。而上下文感知隐私解决方案，例如信息论隐私，能够了解数据将被使用的上下文，并能够实现改进的隐私-效用权衡 [[59](#bib.bib59)]。
- en: 3.1.1 Naive Data Anonymization
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 简单数据匿名化
- en: What we mean by naive anonymization in this survey is the removal of identifiers
    from data, such as the names, addresses, and full postcodes of the participants,
    to protect privacy. This method was used for protecting patients while processing
    medical data and has been shown to fail on many occasions [[60](#bib.bib60), [21](#bib.bib21),
    [61](#bib.bib61)]. Perhaps the most prominent failure is the Netflix prize case,
    where Narayanan & Shmatikov apply their de-anonymization technique to the Netflix
    Prize dataset. This dataset contains anonymous movie ratings of 500,000 subscribers
    of Netflix. They showed that an adversary with auxiliary knowledge (from the publicly
    available Internet Movie Database records) about individual subscribers can easily
    identify the user and uncover potentially sensitive information [[60](#bib.bib60)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中所指的简单匿名化是指从数据中移除标识符，例如参与者的姓名、地址和完整邮政编码，以保护隐私。这种方法曾用于保护患者在处理医疗数据时的隐私，但已被证明在许多情况下失败 [[60](#bib.bib60),
    [21](#bib.bib21), [61](#bib.bib61)]。也许最突出的失败案例是Netflix奖的情况，其中Narayanan & Shmatikov将他们的去匿名化技术应用于Netflix奖数据集。该数据集包含500,000名Netflix订阅者的匿名电影评分。他们展示了一个具有辅助知识的对手（来自公开的互联网电影数据库记录）可以轻易识别用户并揭示潜在的敏感信息 [[60](#bib.bib60)]。
- en: 3.1.2 K-Anonymity
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 K-匿名性
- en: A dataset has k-anonymity property if each participant’s information cannot
    be distinguished from at least $k-1$ other participants whose information is in
    the dataset [[21](#bib.bib21)]. K-anonymity means that for any given combination
    of attributes that are available to the adversary (these attributes are called
    quasi-identifiers), there are at least k rows with the exact same set of attributes.
    K-anonymity has the objective of impeding re-identification. However, k-anonymization
    has been shown to perform poorly on the anonymization of high-dimensional datasets [[62](#bib.bib62)].
    This has led to privacy notions such as l-diversity [[63](#bib.bib63)] and t-closeness [[64](#bib.bib64)],
    which are out of the scope of this survey.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据集具有k-匿名性特性，如果每个参与者的信息不能与数据集中至少$k-1$个其他参与者的信息区分开来 [[21](#bib.bib21)]。K-匿名性意味着，对于任何对对手可用的属性组合（这些属性被称为准标识符），至少有k行具有完全相同的属性集。K-匿名性的目标是阻止重新识别。然而，k-匿名化在高维数据集的匿名化上表现较差 [[62](#bib.bib62)]。这导致了如l-多样性 [[63](#bib.bib63)]和t-接近性 [[64](#bib.bib64)]等隐私概念，这些概念超出了本调查的范围。
- en: 3.1.3 Differential Privacy
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 差分隐私
- en: Definition 3.1.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.1。
- en: '$\epsilon$-Differential Privacy ($\epsilon$-DP). For $\epsilon\geq 0$, an algorithm
    $A$ satisfies $\epsilon$-DP [[65](#bib.bib65), [22](#bib.bib22)] if and only if
    for any pair of datasets $D$ and $D^{\prime}$ that differ in only one element:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $\epsilon$-差分隐私 ($\epsilon$-DP)。对于$\epsilon\geq 0$，一个算法$A$满足$\epsilon$-DP [[65](#bib.bib65),
    [22](#bib.bib22)] 当且仅当对于任意一对仅在一个元素上有所不同的数据集$D$和$D^{\prime}$：
- en: '|  | $\mathcal{P}[A(D)=t]\leq e^{\epsilon}\mathcal{P}[A(D^{\prime})=t]\;\;\;\forall
    t$ |  | (1) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}[A(D)=t]\leq e^{\epsilon}\mathcal{P}[A(D^{\prime})=t]\;\;\;\forall
    t$ |  | (1) |'
- en: where, $\mathcal{P}[A(D)=t]$ denotes the probability that the algorithm $A$
    outputs $t$. In this setup, the quantity $ln\frac{\mathcal{P}[A(D)=t]}{\mathcal{P}[A(D^{\prime})=t]}$
    is named the privacy loss. DP tries to approximate the effect of an individual
    opting out of contributing to the dataset, by ensuring that any effect due to
    the inclusion of one’s data is small. One of the widely used DP mechanisms when
    dealing with numerical data is the Laplace mechanism.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{P}[A(D)=t]$ 表示算法 $A$ 输出 $t$ 的概率。在这种设置下，量 $ln\frac{\mathcal{P}[A(D)=t]}{\mathcal{P}[A(D^{\prime})=t]}$
    被称为隐私损失。DP 试图通过确保因包含一个人的数据而产生的效果很小，来近似个体选择退出数据集的效果。当处理数值数据时，广泛使用的 DP 机制之一是拉普拉斯机制。
- en: Definition 3.2.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.2。
- en: Laplace Mechanism. [[22](#bib.bib22)] Given a target function $f$ and a fixed
    $\epsilon\geq 0$, the randomizing algorithm $A_{f}(D)=f(D)+x$ where $x$ is a perturbation
    random variable drawn from a Laplace distribution $Lap(\mu,\frac{\Delta_{f}}{\epsilon})$,
    is called the Laplace Mechanism and is $\epsilon$-DP. Here, $\Delta_{f}$ is the
    global sensitivity of function $f$, and is defined as $\Delta_{f}=\sup|f(D)-f(D^{\prime})|$
    over all the dataset pairs $(D,D^{\prime})$ that differ in only one element. Finding
    this sensitivity is not always trivial, specifically if the function $f$ is a
    deep neural network, or even a number of layers of it [[66](#bib.bib66)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯机制 [[22](#bib.bib22)] 给定目标函数 $f$ 和固定的 $\epsilon\geq 0$，随机化算法 $A_{f}(D)=f(D)+x$
    其中 $x$ 是从拉普拉斯分布 $Lap(\mu,\frac{\Delta_{f}}{\epsilon})$ 中抽取的扰动随机变量，被称为拉普拉斯机制，并且是
    $\epsilon$-DP。在这里，$\Delta_{f}$ 是函数 $f$ 的全局敏感度，定义为 $\Delta_{f}=\sup|f(D)-f(D^{\prime})|$，对于所有在仅一个元素上有所不同的数据集对
    $(D,D^{\prime})$。找到这种敏感度并不总是简单的，特别是当函数 $f$ 是深度神经网络，甚至是其多个层时 [[66](#bib.bib66)]。
- en: 'Differential privacy satisfies a composition property that states when two
    mechanisms with privacy budgets $\epsilon_{1}$ and $\epsilon_{2}$ are applied
    to the same datasets, together they use a privacy budget of $\epsilon_{1}$ + $\epsilon_{2}$.
    As such, composing multiple differentially private mechanisms consumes a linearly
    increasing privacy budget. It has been shown that tighter privacy bound for composition
    can be reached, so that the privacy budget decreases sub-linearly [[67](#bib.bib67),
    [68](#bib.bib68)]. There are multiple variants of the conventional $\epsilon$-differential
    privacy which have been proposed to provide a tighter analysis of the privacy
    budget under composition. One of them is differential privacy with Advanced Composition
    (AC) [[69](#bib.bib69)], which allows an additive leakage probability parameter
    $\delta$ to the right-hand side of Equation [1](#S3.E1 "In Definition 3.1\. ‣
    3.1.3 Differential Privacy ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving Mechanisms
    ‣ Privacy in Deep Learning: A Survey").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私满足一种组合属性，该属性表明，当两个隐私预算为 $\epsilon_{1}$ 和 $\epsilon_{2}$ 的机制应用于相同的数据集时，它们总共使用的隐私预算为
    $\epsilon_{1}$ + $\epsilon_{2}$。因此，组合多个差分隐私机制会消耗线性增长的隐私预算。已经证明可以达到更紧的隐私界限，以使隐私预算以次线性方式减少
    [[67](#bib.bib67), [68](#bib.bib68)]。已有多种传统的 $\epsilon$-差分隐私变体被提出，以提供更紧的隐私预算分析。其中之一是具有高级组合（AC）的差分隐私
    [[69](#bib.bib69)]，它允许在方程 [1](#S3.E1 "在定义 3.1\. ‣ 3.1.3 差分隐私 ‣ 3.1 数据聚合 ‣ 3 隐私保护机制
    ‣ 深度学习中的隐私：调查") 的右侧添加泄漏概率参数 $\delta$。
- en: Differential privacy can also be achieved without the need to trust a centralized
    server by having each participant apply a differentially private randomization
    to their data themselves, before sharing it. This model is named the local model
    of differential privacy, and the method “randomized response” is shown to be locally
    differentially private [[70](#bib.bib70)]. Local differential privacy has been
    deployed on many systems for gathering statistics privately [[71](#bib.bib71),
    [72](#bib.bib72)]. For instance, Google uses a technique named RAPPOR [[71](#bib.bib71)]
    to allow web browser developers to privately collect usage statistics. A large
    body of Differentially private mechanisms has been proposed for various applications [[73](#bib.bib73),
    [24](#bib.bib24), [74](#bib.bib74)]. Triastcyn & Faltings present a technique
    that generates synthetic datasets that still have statistical properties of the
    real data while providing differential privacy guarantees with respect to this
    data [[75](#bib.bib75)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 微分隐私也可以通过让每个参与者在共享数据之前对其数据应用微分隐私随机化来实现，而不需要信任一个集中服务器。这个模型被称为局部微分隐私模型，方法“随机响应”被证明是局部微分隐私的[[70](#bib.bib70)]。局部微分隐私已被部署在许多系统中，用于私密地收集统计数据[[71](#bib.bib71),
    [72](#bib.bib72)]。例如，Google 使用一种名为 RAPPOR 的技术[[71](#bib.bib71)]，允许网页浏览器开发者私密地收集使用统计数据。已经提出了大量的微分隐私机制用于各种应用[[73](#bib.bib73),
    [24](#bib.bib24), [74](#bib.bib74)]。Triastcyn 和 Faltings 提出了一种生成合成数据集的技术，这些数据集在提供微分隐私保证的同时仍具有真实数据的统计属性[[75](#bib.bib75)]。
- en: 'A generalized version of differential privacy called Pufferfish was proposed
    by [[76](#bib.bib76)]. The Pufferfish framework can be used to create new privacy
    definitions tailored for specific applications [[77](#bib.bib77)], such as Census
    data release. Another framework that is also an adaptation of differential privacy
    for location obfuscation is dubbed geo-indistinguishablity [[78](#bib.bib78)].
    Geo-indistinguishablity relaxes differential privacy’s guarantees by introducing
    a distance metric, $d$, which is multiplied by $\epsilon$ in Equation [1](#S3.E1
    "In Definition 3.1\. ‣ 3.1.3 Differential Privacy ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey"). This distance permits adjusting
    the obfuscation such that the probability of being obfuscated to a closer point
    (being mapped to a point closer to the real value) is higher than being obfuscated
    to a far point.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为 Pufferfish 的微分隐私广义版本由[[76](#bib.bib76)]提出。Pufferfish 框架可以用于创建针对特定应用的新的隐私定义[[77](#bib.bib77)]，例如普查数据发布。另一个也是对微分隐私进行位置模糊处理的框架称为
    geo-indistinguishablity[[78](#bib.bib78)]。Geo-indistinguishablity 通过引入距离度量 $d$（在公式
    [1](#S3.E1 "在定义 3.1. ‣ 3.1.3 微分隐私 ‣ 3.1 数据聚合 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：一项调查") 中乘以
    $\epsilon$）来放宽微分隐私的保证。这种距离允许调整模糊处理，使得被模糊到更近点（映射到接近真实值的点）的概率高于被模糊到远点的概率。
- en: 3.1.4 Semantic Security and Encryption
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 语义安全与加密
- en: 'Semantic security [[79](#bib.bib79)] (computationally secure) is a standard
    privacy requirement of encryption schemes which states that the advantage (a measure
    of how successfully an adversary can attack a cryptographic algorithm) of an adversary
    with background information should be cryptographically small. Semantic security
    is theoretically possible to break but it is infeasible to do so by any known
    practical means [[80](#bib.bib80)]. Secure Multiparty Computation (SMC), which
    we discuss in Section [3.2](#S3.SS2 "3.2 Training Phase ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey"), is based on semantic security
    definition [[81](#bib.bib81)].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 语义安全[[79](#bib.bib79)]（计算上安全）是加密方案的标准隐私要求，它声明具有背景信息的对手的优势（衡量对手攻击加密算法成功的程度）应该是加密学上微小的。虽然理论上语义安全是可能被突破的，但通过任何已知的实际手段这样做是不可行的[[80](#bib.bib80)]。安全多方计算（SMC），我们在第
    [3.2](#S3.SS2 "3.2 训练阶段 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：一项调查") 节中讨论，基于语义安全定义[[81](#bib.bib81)]。
- en: 3.1.5 Information-Theoretic Privacy
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 信息论隐私
- en: Information-theoretic privacy is a context-aware privacy solution. Context-aware
    solutions explicitly model the dataset statistics, unlike context-free solutions
    that assume worst-case dataset statistics and adversaries. There is a body of
    work studying information-theoretic based methods for both privacy and fairness,
    where privacy and fairness are provided through information degradation, through
    obfuscation or adversarial learning and demonstrated by mutual information reduction
     [[82](#bib.bib82), [83](#bib.bib83), [59](#bib.bib59), [84](#bib.bib84), [85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90),
    [91](#bib.bib91)]. Huang et al. introduce a context-aware privacy framework called
    generative adversarial privacy (GAP), which leverages generative adversarial networks
    (GANs) to generate privatized datasets. Their scheme comprises of a sanitizer
    that tries to remove private attributes, and an adversary that tries to infer
    them [[59](#bib.bib59)]. They show that the privacy mechanisms learned from data
    (in a generative adversarial fashion) match the theoretically optimal ones.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 信息论隐私是一种上下文感知的隐私解决方案。上下文感知解决方案明确地对数据集统计信息进行建模，而非像上下文无关解决方案那样假设最坏情况下的数据集统计信息和对手情况。有一些研究工作研究了基于信息论的隐私和公平方法，其中隐私和公平是通过信息降级、模糊化或对抗性学习来提供，并通过互信息减少来加以论证 [[82](#bib.bib82),
    [83](#bib.bib83), [59](#bib.bib59), [84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)]。黄等人引入了一种称为生成对抗隐私（GAP）的上下文感知隐私框架，利用生成对抗网络（GANs）生成私有数据集。他们的方案包括一个试图移除私有属性的清洗器，以及一个试图推断私有属性的对手 [[59](#bib.bib59)]。他们表明，从数据中学习的隐私机制（以生成对抗方式）与理论上的最佳机制相匹配。
- en: 3.2 Training Phase
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练阶段
- en: 'Table 2: Categorization of some notable privacy-preserving mechanisms for training.
    In the table, the following abbreviations have been used: ERM for Empirical Risk
    Minimization, GM for Generative Model, AE for Auto Encoder, LIR for Linear Regression,
    LOR for Logistic Regression, LM for Linear Means, FLD for Fisher’s Linear Discriminant,
    NB for Naive Bayes and RF for Random Forest.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：一些值得注意的隐私保护机制的分类。在表中，使用了以下缩写：ERM 代表经验风险最小化，GM 代表生成模型，AE 代表自动编码器，LIR 代表线性回归，LOR
    代表逻辑回归，LM 代表线性均值，FLD 代表费舍尔线性判别，NB 代表朴素贝叶斯，RF 代表随机森林。
- en: '| Method | DP | SMC | HE | Dataset(s) | Task |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | DP | SMC | HE | 数据集 | 任务 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DPSGD [[24](#bib.bib24)] | ● | ○ | ○ | MNIST, CIFAR-10 | Image Classification
    w/ DNN |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| DPSGD [[24](#bib.bib24)] | ● | ○ | ○ | MNIST, CIFAR-10 | 使用 DNN 进行图像分类 |'
- en: '| DP LSTM [[92](#bib.bib92)] | ● | ○ | ○ | Reddit Posts | Language Model w/
    LSTMs |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| DP LSTM [[92](#bib.bib92)] | ● | ○ | ○ | Reddit 帖子 | 使用 LSTM 进行语言模型 |'
- en: '| DP LOR [[93](#bib.bib93)] | ● | ○ | ○ | Artificial Data | Logistic Regression
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| DP LOR [[93](#bib.bib93)] | ● | ○ | ○ | 人工数据 | 逻辑回归 |'
- en: '| DP ERM [[94](#bib.bib94)] | ● | ○ | ○ | Adult, KDD-99 | Classification w/
    ERM |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| DP ERM [[94](#bib.bib94)] | ● | ○ | ○ | Adult, KDD-99 | 使用 ERM 进行分类 |'
- en: '| DP GAN [[95](#bib.bib95)] | ● | ○ | ○ | MNIST, MIMIC-III | Data Generation
    w/ GAN |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DP GAN [[95](#bib.bib95)] | ● | ○ | ○ | MNIST, MIMIC-III | 使用 GAN 进行数据生成
    |'
- en: '| DP GM [[96](#bib.bib96)] | ● | ○ | ○ | MNIST, CDR, TRANSIT | Data Generation
    w/ GM |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| DP GM [[96](#bib.bib96)] | ● | ○ | ○ | MNIST, CDR, TRANSIT | 使用 GM 进行数据生成
    |'
- en: '| DP AE [[97](#bib.bib97)] | ● | ○ | ○ | Health Social Network Data | Behaviour
    Prediction w/ AE |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DP AE [[97](#bib.bib97)] | ● | ○ | ○ | 健康社交网络数据 | 使用 AE 进行行为预测 |'
- en: '| DP Belief Network[[98](#bib.bib98)] | ● | ○ | ○ | YesiWell, MNIST | Classification
    w/ DNN |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| DP 信念网络[[98](#bib.bib98)] | ● | ○ | ○ | YesiWell, MNIST | 使用 DNN 进行分类 |'
- en: '| Adaptive Laplace Mechanism[[99](#bib.bib99)] | ● | ○ | ○ | MNIST, CIFAR-10
    | Image Classification w/ DNN |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 自适应拉普拉斯机制 [[99](#bib.bib99)] | ● | ○ | ○ | MNIST, CIFAR-10 | 使用 DNN 进行图像分类
    |'
- en: '| PATE [[25](#bib.bib25)] | ● | ○ | ○ | MNIST, SVHN | Image Classification
    w/ DNN |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 使用 PATE 进行图像分类 [[25](#bib.bib25)] | ● | ○ | ○ | MNIST, SVHN | 使用 DNN 进行图像分类
    |'
- en: '| Scalable Learning w/ PATE [[100](#bib.bib100)] | ● | ○ | ○ | MNIST, SVHN,
    Adult, Glyph | Image Classification w/ DNN |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 使用 PATE 进行可伸缩学习 [[100](#bib.bib100)] | ● | ○ | ○ | MNIST, SVHN, Adult, Glyph
    | 使用 DNN 进行图像分类 |'
- en: '| DP Ensemble [[26](#bib.bib26)] | ● | ○ | ○ | KDD-99, UCI-HAR, URLs | Classification
    w/ ERM |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| DP Ensemble [[26](#bib.bib26)] | ● | ○ | ○ | KDD-99, UCI-HAR, URLs | 使用 ERM
    进行分类 |'
- en: '| SecProbe [[101](#bib.bib101)] | ● | ○ | ○ | US, MNIST, SVHN | Regress. &
    Class. w/ DNN |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SecProbe [[101](#bib.bib101)] | ● | ○ | ○ | US, MNIST, SVHN | 使用 DNN 进行回归和分类
    |'
- en: '| Distributed DP [[102](#bib.bib102)] | ● | ○ | ○ | eICU, TCGA | Classification
    w/ DNN |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 分布式 DP [[102](#bib.bib102)] | ● | ○ | ○ | eICU, TCGA | 使用 DNN 进行分类 |'
- en: '| DP model publishing [[103](#bib.bib103)] | ● | ○ | ○ | MNIST, CIFAR | Image
    Classification w/ DNN |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| DP模型发布 [[103](#bib.bib103)] | ● | ○ | ○ | MNIST, CIFAR | 使用DNN的图像分类 |'
- en: '| DP federated learning [[104](#bib.bib104)] | ● | ○ | ○ | MNIST | Image Classification
    w/ DNN |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| DP联邦学习 [[104](#bib.bib104)] | ● | ○ | ○ | MNIST | 使用DNN的图像分类 |'
- en: '| ScalarDP, PrivUnit [[105](#bib.bib105)] | ● | ○ | ○ | MNIST, CIFAR | Image
    Classification w/ DNN |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ScalarDP, PrivUnit [[105](#bib.bib105)] | ● | ○ | ○ | MNIST, CIFAR | 使用DNN的图像分类
    |'
- en: '| DSSGD [[23](#bib.bib23)] | ● | ○ | ○ | MNIST, SVHN | Image Classification
    w/ DNN |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| DSSGD [[23](#bib.bib23)] | ● | ○ | ○ | MNIST, SVHN | 使用DNN的图像分类 |'
- en: '| Private Collaborative NN [[106](#bib.bib106)] | ● | ● | ○ | MNIST | Image
    Classification w/ DNN |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Private Collaborative NN [[106](#bib.bib106)] | ● | ● | ○ | MNIST | 使用DNN的图像分类
    |'
- en: '| Secure Aggregation for ML [[107](#bib.bib107)] | ○ | ● | ○ | - | Federated
    Learning |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Secure Aggregation for ML [[107](#bib.bib107)] | ○ | ● | ○ | - | 联邦学习 |'
- en: '| QUOTIENT [[108](#bib.bib108)] | ○ | ● | ○ | MNIST, Thyroid, Credit | Classification
    w/ DNN |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| QUOTIENT [[108](#bib.bib108)] | ○ | ● | ○ | MNIST, 甲状腺, 信用 | 使用DNN的分类 |'
- en: '| SecureNN [[109](#bib.bib109)] | ○ | ● | ○ | MNIST | Image Classification
    w/ DNN |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SecureNN [[109](#bib.bib109)] | ○ | ● | ○ | MNIST | 使用DNN的图像分类 |'
- en: '| ABY3 [[110](#bib.bib110)] | ○ | ● | ○ | MNIST | LIR, LOR, NN |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ABY3 [[110](#bib.bib110)] | ○ | ● | ○ | MNIST | LIR, LOR, NN |'
- en: '| Trident [[111](#bib.bib111)] | ○ | ● | ○ | MNIST, Boston Housing | LIR, LOR,
    NN |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Trident [[111](#bib.bib111)] | ○ | ● | ○ | MNIST, 波士顿房价 | LIR, LOR, NN |'
- en: '| SecureML [[112](#bib.bib112)] | ○ | ● | ● | MNIST, Gisette, Arcene | LIR,
    LOR, NN |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SecureML [[112](#bib.bib112)] | ○ | ● | ● | MNIST, Gisette, Arcene | LIR,
    LOR, NN |'
- en: '| Deep Learning w/ AHE [[113](#bib.bib113)] | ○ | ○ | ● | MNIST | Image Classification
    w/ DNN |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Deep Learning w/ AHE [[113](#bib.bib113)] | ○ | ○ | ● | MNIST | 使用DNN的图像分类
    |'
- en: '| ML Confidential [[114](#bib.bib114)] | ○ | ○ | ● | Wisconsin Breast Cancer
    | LM, FLD |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ML Confidential [[114](#bib.bib114)] | ○ | ○ | ● | 威斯康星乳腺癌 | LM, FLD |'
- en: '| Encrypted Statistical ML [[115](#bib.bib115)] | ○ | ○ | ● | 20 datasets from
    UCI ML | LOR, NB, RF |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Encrypted Statistical ML [[115](#bib.bib115)] | ○ | ○ | ● | 来自UCI ML的20个数据集
    | LOR, NB, RF |'
- en: '| CryptoDL [[27](#bib.bib27)] | ○ | ○ | ● | MNIST, CIFAR-10 | Image Classification
    w/ DNN |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CryptoDL [[27](#bib.bib27)] | ○ | ○ | ● | MNIST, CIFAR-10 | 使用DNN的图像分类 |'
- en: '| DPHE [[116](#bib.bib116)] | ○ | ○ | ● | Caltech101/256, CelebA | Image Classification
    w/ SVM |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DPHE [[116](#bib.bib116)] | ○ | ○ | ● | Caltech101/256, CelebA | 使用SVM的图像分类
    |'
- en: '![Refer to caption](img/24d9ef525b023d78e16d767b1436f28d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24d9ef525b023d78e16d767b1436f28d.png)'
- en: 'Figure 3: Categorization of privacy-preserving schemes for deep learning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 隐私保护方案在深度学习中的分类。'
- en: 'The literature surrounding private training of deep learning, and machine learning
    can be categorized based on the guarantee that these methods provide, which is
    most commonly either based on differential privacy (Section [3.1.3](#S3.SS1.SSS3
    "3.1.3 Differential Privacy ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving Mechanisms
    ‣ Privacy in Deep Learning: A Survey")) or semantic security and encryption (Section [3.1.4](#S3.SS1.SSS4
    "3.1.4 Semantic Security and Encryption ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey")). Privacy using encryption is
    achieved by doing computation over encrypted data. The two most common methods
    for this are Homomorphic Encryption (HE) and Secure Multi-Party Computation (SMC).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '关于深度学习和机器学习的私有训练的文献可以根据这些方法提供的保障进行分类，这些保障通常基于差分隐私（第[3.1.3节](#S3.SS1.SSS3 "3.1.3
    Differential Privacy ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving Mechanisms
    ‣ Privacy in Deep Learning: A Survey")）或语义安全和加密（第[3.1.4节](#S3.SS1.SSS4 "3.1.4
    Semantic Security and Encryption ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey")）。使用加密的隐私保护是通过对加密数据进行计算实现的。最常见的两种方法是同态加密（HE）和安全多方计算（SMC）。'
- en: Homomorphic Encryption (HE). HE [[117](#bib.bib117)] allows computation over
    encrypted data. A client can send their data, in an encrypted format, to a server
    and the server can compute over this data without decrypting it, and then send
    a ciphertext (encrypted result) to the client for decryption. HE is extremely
    compute-intensive and is therefore not yet deployed in many production systems [[118](#bib.bib118),
    [119](#bib.bib119)].
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密（HE）。HE [[117](#bib.bib117)] 允许对加密数据进行计算。客户端可以将其数据以加密格式发送到服务器，服务器可以在不解密的情况下对数据进行计算，然后将密文（加密结果）发送回客户端进行解密。HE计算密集型，因此尚未在许多生产系统中部署 [[118](#bib.bib118),
    [119](#bib.bib119)]。
- en: Secure Multi-Party Computation (SMC). SMC attempts at designing a network of
    computing parties (not all of which the user necessarily has to trust) that carry
    out a given computation and makes sure no data leaks. Each party in this network
    has access to only an encrypted part of the data. SMC ensures that as long as
    the owner of the data trusts at least one of the computing systems in the network,
    their input data remain secret. Simple functions can easily be computed using
    this scheme. Arbitrarily complex function computations can also be supported,
    but with an often prohibitive computational cost [[119](#bib.bib119)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 安全多方计算（SMC）。SMC 试图设计一个计算方网络（其中并非所有计算方都必须被用户信任），以执行给定的计算，并确保数据不会泄漏。网络中的每个方仅能访问数据的加密部分。SMC
    确保只要数据的拥有者信任网络中的至少一个计算系统，他们的输入数据将保持秘密。使用此方案可以轻松计算简单函数。也可以支持任意复杂的函数计算，但通常会有较高的计算成本 [[119](#bib.bib119)]。
- en: 'In this survey, we divided the literature of private training into three groups
    of methods that employ: 1) Differential Privacy (DP), 2) Homomorphic Encryption
    (HE) and 3) Secure Multi-Party Computation (SMC). Table [2](#S3.T2 "Table 2 ‣
    3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning:
    A Survey") shows this categorization for the literature we discuss in this section.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项调查中，我们将私人训练的文献分为三组方法：1）差分隐私（DP），2）同态加密（HE）和 3）安全多方计算（SMC）。表 [2](#S3.T2 "Table
    2 ‣ 3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning:
    A Survey") 展示了我们在本节中讨论的文献的这一分类。'
- en: 3.2.1 Differential Privacy
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 差分隐私
- en: '![Refer to caption](img/faded9920da43df1b9f5491bc7cbac8b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/faded9920da43df1b9f5491bc7cbac8b.png)'
- en: 'Figure 4: Overview of how a deep learning framework works and how differential
    privacy can be applied to different parts of the pipeline.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：深度学习框架的工作概述以及差分隐私如何应用于管道的不同部分。
- en: 'This section briefly discusses methods for modifying deep learning algorithms
    to satisfy differential privacy. Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Differential
    Privacy ‣ 3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep
    Learning: A Survey") shows an overview of a deep learning framework. As can be
    seen, the randomization required for differential privacy (or the privacy-preserving
    noise) can be inserted in five places: to the input, to the loss/objective function,
    to the gradient updates, to the output (the optimized parameters of the trained
    model) and to the labels [[25](#bib.bib25)].'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '本节简要讨论了修改深度学习算法以满足差分隐私的方法。图 [4](#S3.F4 "Figure 4 ‣ 3.2.1 Differential Privacy
    ‣ 3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning:
    A Survey") 展示了深度学习框架的概览。如图所示，满足差分隐私（或隐私保护噪声）所需的随机化可以插入五个位置：输入、损失/目标函数、梯度更新、输出（训练模型的优化参数）和标签 [[25](#bib.bib25)]。'
- en: 'Input perturbations can be considered equivalent to using a sanitized dataset
    (discussed in Section [3.1](#S3.SS1 "3.1 Data Aggregation ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey")) for training. objective function
    perturbation and output perturbation are explored for machine learning tasks with
    convex objective functions. For instance in the case of logistic regression, Chaudhuri
    et al. prove that objective perturbation requires sampling noise in the scale
    of $\frac{2}{n\epsilon}$, and output perturbation requires sampling noise in the
    scale of $\frac{2}{n\lambda\epsilon}$, where $n$ is the number of samples and
    $\lambda$ is the regularization coefficient [[94](#bib.bib94)]. More recently,
    Iyengar et al [[120](#bib.bib120)] propose a more practical and general objective
    perturbation approach, and benchmark it using high-dimensional real-world data.
    In deep learning tasks, due to the non-convexity of the objective function, calculating
    the sensitivity of the function (which is needed to determine the intensity of
    the added noise) becomes non-trivial. One solution is replacing the non-convex
    function with an approximate convex polynomial function  [[97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99)] and then using objective function perturbation. This approximation
    limits the capabilities and the utility that a conventional DNN would have. Given
    discussed limitations, gradient perturbation is the approach that is widely used
    for private training in deep learning. Applying perturbations on the gradients
    requires the gradient norms to be bounded, and since in deep learning tasks the
    gradient could be unbounded, clipping is usually used to alleviate this issue.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输入扰动可以被视为使用一个已清理的数据集（见第[3.1节](#S3.SS1 "3.1 数据聚合 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：调查")）进行训练的等效方式。目标函数扰动和输出扰动在具有凸目标函数的机器学习任务中被探索。例如，在逻辑回归的情况下，Chaudhuri
    等人证明目标扰动需要在$\frac{2}{n\epsilon}$尺度上进行采样噪声，而输出扰动需要在$\frac{2}{n\lambda\epsilon}$尺度上进行采样噪声，其中$n$是样本数量，$\lambda$是正则化系数[[94](#bib.bib94)]。最近，Iyengar
    等人[[120](#bib.bib120)]提出了一种更实用和通用的目标扰动方法，并使用高维真实数据进行了基准测试。在深度学习任务中，由于目标函数的非凸性，计算函数的敏感性（这对于确定添加噪声的强度是必要的）变得不那么简单。一种解决方案是用近似的凸多项式函数[[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]来替代非凸函数，然后使用目标函数扰动。这种近似限制了传统DNN的能力和效用。鉴于讨论的限制，梯度扰动是深度学习中广泛使用的隐私保护训练方法。对梯度应用扰动需要将梯度范数限制在一定范围内，由于在深度学习任务中梯度可能是无界的，因此通常使用剪切来缓解这个问题。
- en: 'Shokri et al. showed that deep neural networks can be trained in a distributed
    manner and with perturbed parameters to achieve privacy [[23](#bib.bib23)], but
    their implementation requires $\epsilon$ proportional to the size of the target
    model, which can be in the order of couple millions. Abadi et al. [[24](#bib.bib24)]
    propose a mechanism dubbed the “moments accountant (MA)”, for bounding the cumulative
    privacy budget of sequentially applied differentially private algorithms, over
    deep neural networks. The moments accountant uses the moment generating function
    of the privacy loss random variable to keep track of a bound on the privacy loss
    during composition. MA operates in three steps: first, it calculates the moment
    generating functions for the algorithms $A_{1}$, $A_{2}$,.., which are the randomizing
    algorithms. It then composes the moments together through a composition theorem,
    and finally, finds the best leakage parameter ($\delta$) for a given privacy budget
    of $\epsilon$. The moments accountant is widely used in different DP mechanisms
    for private deep learning. Papernot et al. use MA to aid bounding the privacy
    budget for their teacher ensemble method that uses noisy voting and label perturbation [[25](#bib.bib25),
    [100](#bib.bib100)]. MA is also employed by the works [[102](#bib.bib102), [103](#bib.bib103),
    [104](#bib.bib104), [95](#bib.bib95), [96](#bib.bib96), [105](#bib.bib105)] all
    of which use perturbed gradients.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Shokri 等人展示了深度神经网络可以以分布式方式和使用扰动参数进行训练以实现隐私保护 [[23](#bib.bib23)]，但他们的实现需要与目标模型大小成正比的$\epsilon$，这个大小可能达到几百万。Abadi
    等人 [[24](#bib.bib24)] 提出了一个称为“时刻会计师（MA）”的机制，用于界定在深度神经网络上顺序应用不同差分隐私算法的累计隐私预算。时刻会计师利用隐私损失随机变量的矩生成函数来跟踪组合过程中隐私损失的界限。MA
    分三个步骤操作：首先，计算算法 $A_{1}$、$A_{2}$、.. 的矩生成函数，这些算法是随机化算法。然后通过组合定理将这些矩组合在一起，最后找到给定隐私预算
    $\epsilon$ 的最佳泄漏参数 ($\delta$)。时刻会计师在各种用于私密深度学习的差分隐私机制中得到广泛应用。Papernot 等人使用 MA
    来帮助界定其教师集成方法的隐私预算，该方法使用了噪声投票和标签扰动 [[25](#bib.bib25), [100](#bib.bib100)]。MA 也被应用于
    [[102](#bib.bib102), [103](#bib.bib103), [104](#bib.bib104), [95](#bib.bib95),
    [96](#bib.bib96), [105](#bib.bib105)] 等工作中，这些工作都使用了扰动梯度。
- en: More recently, Bu et al. apply the Gaussian Differential Privacy (GDP) notion
    introduced by Dong et al. [[121](#bib.bib121)] to deep learning [[122](#bib.bib122)]
    to achieve a more refined analysis of neural network training, compared to that
    of Abadi et al. [[24](#bib.bib24)]. They analyze the privacy budget exhaustion
    of private DNN training using Adam optimizer, without the need of developing sophisticated
    techniques such as the moments accountant. They demonstrate that GDP allows for
    a new privacy analysis that improves on the moments accountant analysis and provides
    better guarantees (i.e. lower $\epsilon$ values).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Bu 等人将 Dong 等人提出的高斯差分隐私（GDP）概念 [[121](#bib.bib121)] 应用于深度学习 [[122](#bib.bib122)]，以实现对神经网络训练的更精细分析，与
    Abadi 等人 [[24](#bib.bib24)] 的分析相比。他们分析了使用 Adam 优化器的私密 DNN 训练中的隐私预算消耗，无需开发复杂的技术，如时刻会计师。他们展示了
    GDP 允许一种新的隐私分析，这种分析改进了时刻会计师的分析，并提供了更好的保障（即更低的 $\epsilon$ 值）。
- en: Inherently, applying differential privacy to deep learning yields loss of utility
    due to the addition of noise and clipping. Bagdasaryan et al. have demonstrated
    that this loss in utility is disparate across different sub-groups of the population,
    with different sizes [[123](#bib.bib123)]. They experimentally show that sub-groups
    with less training samples (less representation) lose more accuracy, compared
    to well-represented groups, i.e. the poor get poorer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，将差分隐私应用于深度学习会由于噪声和剪辑的添加而导致效用损失。Bagdasaryan 等人已证明，这种效用损失在不同的人群子组之间存在差异，且不同子组的规模各异
    [[123](#bib.bib123)]。他们通过实验表明，训练样本较少（表现较差）的子组比样本充分的子组损失更多的准确性，即穷者愈穷。
- en: There is a body of work that tries to experimentally measure and audit the privacy
    brought by differentially private learning algorithms [[124](#bib.bib124), [125](#bib.bib125)].
    Jagielski et al. [[124](#bib.bib124)] investigate whether DP-SGD offers better
    privacy in practice than what is guaranteed by its analysis, using data poisoning
    attacks. Jayaraman et al [[125](#bib.bib125)] apply membership and attribute inference
    attacks on multiple differentially private machine learning and deep learning
    algorithms, and compare their performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作尝试实验性地测量和审计不同ially private 学习算法带来的隐私 [[124](#bib.bib124), [125](#bib.bib125)]。Jagielski
    等人 [[124](#bib.bib124)] 研究了 DP-SGD 是否在实际中提供了比其分析所保证的更好的隐私，使用了数据投毒攻击。Jayaraman
    等人 [[125](#bib.bib125)] 对多个差分隐私机器学习和深度学习算法进行成员资格和属性推断攻击，并比较它们的性能。
- en: 3.2.2 Homomorphic Encryption
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 同态加密
- en: There are only a handful of works that exploit solely homomorphic encryption
    for private training of machine learning models [[114](#bib.bib114), [115](#bib.bib115),
    [27](#bib.bib27)]. Graepel et al. use a Somewhat HE (SHE) scheme to train Linear
    Means (LM) and Fisher’s Linear Discriminate (FLD) classifiers [[114](#bib.bib114)].
    HE algorithms have some limitations in terms of the functions they can compute
    (for instance they cannot implement non-linearities). For that reason, Graepel
    et al. propose division-free algorithms and focus on simple classifiers and not
    complex algorithms such as neural networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 只有少数工作完全利用同态加密进行机器学习模型的私人训练 [[114](#bib.bib114), [115](#bib.bib115), [27](#bib.bib27)]。Graepel
    等人使用了一种某种同态加密 (SHE) 方案来训练线性均值 (LM) 和 Fisher 线性判别 (FLD) 分类器 [[114](#bib.bib114)]。同态加密算法在它们能计算的函数方面存在一些限制（例如，它们无法实现非线性）。因此，Graepel
    等人提出了无除法算法，并专注于简单的分类器，而不是像神经网络这样的复杂算法。
- en: Hesamifard et al. [[27](#bib.bib27)] try to exploit HE for deep learning tasks.
    They introduce methods for approximating the most commonly used neural network
    activation functions (ReLU, Sigmoid, and Tanh) with low degree polynomials. This
    is a crucial step for designing efficient homomorphic encryption schemes. They
    then train convolutional neural networks with those approximate polynomial functions
    and finally, implement convolutional neural networks over encrypted data and measure
    the performance of the models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Hesamifard 等人 [[27](#bib.bib27)] 试图利用同态加密进行深度学习任务。他们引入了用低度多项式近似最常用的神经网络激活函数（ReLU、Sigmoid
    和 Tanh）的方法。这是设计高效同态加密方案的关键步骤。然后，他们用这些近似的多项式函数训练卷积神经网络，并最终在加密数据上实现卷积神经网络，测量模型的性能。
- en: 3.2.3 Secure Multi-Party Computation (SMC)
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 安全多方计算 (SMC)
- en: A trend in research on private and secure computation consists of designing
    custom protocols for applications such as linear and logistic regression  [[112](#bib.bib112)]
    and neural network training and inference [[112](#bib.bib112), [108](#bib.bib108),
    [126](#bib.bib126)]. These methods usually target settings where different datasets
    from different places are set to train a model together, or where computation
    is off-loaded to a group of computing servers that do not collude with each other.
    SMC requires that all participants be online at all times, which requires a significant
    amount of communication [[127](#bib.bib127)]. Mohassel & Zhang proposed SecureML
    which is a privacy-preserving stochastic gradient descent-based method to privately
    train machine learning algorithms such as linear regression, logistic regression
    and neural networks in multiparty computation settings. SecureML uses secret sharing
    to achieve privacy during training. In a more recent work [[110](#bib.bib110)],
    Mohassel et al design protocols for secure three-party training of DNNs with a
    majority of honest parties. Agrawal et al. propose QUOTIENT [[108](#bib.bib108)]
    where their goal is to design an optimization algorithm alongside a secure computation
    protocol customized for it, instead of a conventional approach which is using
    encryption on top of existing optimization algorithms.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 研究私密和安全计算的一个趋势是为应用程序设计定制协议，例如线性回归和逻辑回归 [[112](#bib.bib112)] 以及神经网络训练和推断 [[112](#bib.bib112),
    [108](#bib.bib108), [126](#bib.bib126)]。这些方法通常针对的是不同地点的不同数据集共同训练模型，或者将计算任务委托给不相互勾结的计算服务器群体。SMC
    需要所有参与者始终在线，这需要大量的通信 [[127](#bib.bib127)]。Mohassel 和 Zhang 提出了 SecureML，这是一种基于随机梯度下降的隐私保护方法，用于在多方计算环境中私密地训练机器学习算法，例如线性回归、逻辑回归和神经网络。SecureML
    使用秘密共享在训练过程中实现隐私保护。在最近的一项工作中 [[110](#bib.bib110)]，Mohassel 等人设计了安全的三方 DNN 训练协议，确保大多数方诚实。Agrawal
    等人提出了 QUOTIENT [[108](#bib.bib108)]，他们的目标是设计一个优化算法及其定制的安全计算协议，而不是使用传统的在现有优化算法上加密的方法。
- en: 3.3 Inference Phase
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 推断阶段
- en: 'Table 3: Categorization of some notable privacy-preserving mechanisms for inference.
    In this table, NB is short for Naive Bayes, and DT is short for Decision Tree.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：一些显著隐私保护机制的分类。在此表中，NB 是 Naive Bayes 的缩写，DT 是 Decision Tree 的缩写。
- en: '| Method | DP | SMC | HE | IT | Dataset(s) | Task |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | DP | SMC | HE | IT | 数据集 | 任务 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| ARDEN [[128](#bib.bib128)] | ● | ○ | ○ | ○ | MNIST, CIFAR-10, SVHN | Image
    Classification w/ DNN |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ARDEN [[128](#bib.bib128)] | ● | ○ | ○ | ○ | MNIST, CIFAR-10, SVHN | 使用 DNN
    进行图像分类 |'
- en: '| Cryptonets [[129](#bib.bib129)] | ○ | ○ | ● | ○ | MNIST | Image Classification
    w/ DNN |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Cryptonets [[129](#bib.bib129)] | ○ | ○ | ● | ○ | MNIST | 使用 DNN 进行图像分类 |'
- en: '| Private Classification [[130](#bib.bib130)] | ○ | ○ | ● | ○ | MNIST | Image
    Classification w/ DNN |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Private Classification [[130](#bib.bib130)] | ○ | ○ | ● | ○ | MNIST | 使用
    DNN 进行图像分类 |'
- en: '| TAPAS [[131](#bib.bib131)] | ○ | ○ | ● | ○ | MNIST, Faces, Cancer, Diabetes
    | Image Classification w/ DNN |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| TAPAS [[131](#bib.bib131)] | ○ | ○ | ● | ○ | MNIST, Faces, Cancer, Diabetes
    | 使用 DNN 进行图像分类 |'
- en: '| FHE–DiNN [[132](#bib.bib132)] | ○ | ○ | ● | ○ | MNIST | Image Classification
    w/ DNN |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| FHE–DiNN [[132](#bib.bib132)] | ○ | ○ | ● | ○ | MNIST | 使用 DNN 进行图像分类 |'
- en: '| Face Match [[133](#bib.bib133)] | ○ | ○ | ● | ○ | LFW, IJB-A, IJB-B, CASIA
    | Face recognition with CNNs |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Face Match [[133](#bib.bib133)] | ○ | ○ | ● | ○ | LFW, IJB-A, IJB-B, CASIA
    | 使用 CNN 进行人脸识别 |'
- en: '| Cheetah [[134](#bib.bib134)] | ○ | ○ | ● | ○ | MNIST, Imagenet | Image Classification
    w/ DNN |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Cheetah [[134](#bib.bib134)] | ○ | ○ | ● | ○ | MNIST, Imagenet | 使用 DNN 进行图像分类
    |'
- en: '| EPIC [[119](#bib.bib119)] | ○ | ● | ○ | ○ | CIFAR-10, MIT, Caltech | Image
    Classification w/ DNN |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| EPIC [[119](#bib.bib119)] | ○ | ● | ○ | ○ | CIFAR-10, MIT, Caltech | 使用 DNN
    进行图像分类 |'
- en: '| DeepSecure [[135](#bib.bib135)] | ○ | ● | ○ | ○ | MNIST, UCI-HAR | Classification
    w/ DNN |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DeepSecure [[135](#bib.bib135)] | ○ | ● | ○ | ○ | MNIST, UCI-HAR | 使用 DNN
    进行分类 |'
- en: '| XONN [[118](#bib.bib118)] | ○ | ● | ○ | ○ | MNIST, CIFAR-10 | Image Classification
    w/ DNN |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| XONN [[118](#bib.bib118)] | ○ | ● | ○ | ○ | MNIST, CIFAR-10 | 使用 DNN 进行图像分类
    |'
- en: '| Chameleon [[136](#bib.bib136)] | ○ | ● | ○ | ○ | MNIST, Credit Approval |
    Classification w/ DNN and SVM |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Chameleon [[136](#bib.bib136)] | ○ | ● | ○ | ○ | MNIST, Credit Approval |
    使用 DNN 和 SVM 进行分类 |'
- en: '| CRYPTFLOW [[137](#bib.bib137)] | ○ | ● | ○ | ○ | MNIST,CIFAR, ImageNet |
    Classification w/ DNN |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CRYPTFLOW [[137](#bib.bib137)] | ○ | ● | ○ | ○ | MNIST, CIFAR, ImageNet |
    使用 DNN 进行分类 |'
- en: '| Classification over Encrypted Data[[138](#bib.bib138)] | ○ | ● | ● | ○ |
    Wisconsin Breast Cancer | Classification w/ NB, DT |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Classification over Encrypted Data[[138](#bib.bib138)] | ○ | ● | ● | ○ |
    Wisconsin Breast Cancer | 使用 NB、DT 进行分类 |'
- en: '| MiniONN [[139](#bib.bib139)] | ○ | ● | ● | ○ | MNIST, CIFAR-10 | Image Classification
    w/ DNN |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| MiniONN [[139](#bib.bib139)] | ○ | ● | ● | ○ | MNIST, CIFAR-10 | 基于 DNN 的图像分类
    |'
- en: '| GAZELLE [[140](#bib.bib140)] | ○ | ● | ● | ○ | MNIST, CIFAR-10 | Image Classification
    w/ DNN |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GAZELLE [[140](#bib.bib140)] | ○ | ● | ● | ○ | MNIST, CIFAR-10 | 基于 DNN 的图像分类
    |'
- en: '| DELPHI [[141](#bib.bib141)] | ○ | ● | ● | ○ | CIFAR-10, CIFAR-100 | Image
    Classification w/ DNN |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DELPHI [[141](#bib.bib141)] | ○ | ● | ● | ○ | CIFAR-10, CIFAR-100 | 基于 DNN
    的图像分类 |'
- en: '| Shredder [[31](#bib.bib31)] | ○ | ○ | ○ | ● | SVHN, VGG-Face, ImageNet |
    Classification w/ DNN |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Shredder [[31](#bib.bib31)] | ○ | ○ | ○ | ● | SVHN, VGG-Face, ImageNet |
    基于 DNN 的分类 |'
- en: '| Sensor Data Obfuscation [[142](#bib.bib142)] | ○ | ○ | ○ | ● | Iphone 6s
    Accelerometer Data | Activity Recognition w/ DNN |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 传感器数据混淆 [[142](#bib.bib142)] | ○ | ○ | ○ | ● | Iphone 6s 加速度计数据 | 基于 DNN
    的活动识别 |'
- en: '| Olympus [[143](#bib.bib143)] | ○ | ○ | ○ | ● | Driving images | Activity
    Recognition w/ DNN |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Olympus [[143](#bib.bib143)] | ○ | ○ | ○ | ● | 驾驶图像 | 基于 DNN 的活动识别 |'
- en: '| DPFE [[29](#bib.bib29)] | ○ | ○ | ○ | ● | CelebA | Image Classification w/
    DNN |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DPFE [[29](#bib.bib29)] | ○ | ○ | ○ | ● | CelebA | 基于 DNN 的图像分类 |'
- en: '| Cloak [[144](#bib.bib144)] | ○ | ○ | ○ | ● | CIFAR-100, CelebA, UTKFace |
    Image Classification w/ DNN |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Cloak [[144](#bib.bib144)] | ○ | ○ | ○ | ● | CIFAR-100, CelebA, UTKFace |
    基于 DNN 的图像分类 |'
- en: 'As shown in Table [3](#S3.T3 "Table 3 ‣ 3.3 Inference Phase ‣ 3 Privacy-Preserving
    Mechanisms ‣ Privacy in Deep Learning: A Survey") there are fewer works in the
    field of inference privacy, compared to training. Inference privacy targets systems
    that are deployed to offer Inference-as-a-Service. In these cases, the deployed
    system is assumed to be trained and is not to learn anything new from the data
    provided by the user. It is only supposed to carry out its designated inference
    task. The categorization of literature for inference privacy is similar to training,
    except that there is one extra group here, named Information-Theoretic (IT) privacy.
    The works in this group usually offer information-theoretic mathematical or empirical
    evidence of how their methods operate and help privacy. These works are based
    on the context-aware privacy definition of Section [3.1.5](#S3.SS1.SSS5 "3.1.5
    Information-Theoretic Privacy ‣ 3.1 Data Aggregation ‣ 3 Privacy-Preserving Mechanisms
    ‣ Privacy in Deep Learning: A Survey"), and they aim at decreasing the information
    content in the data sent to the service provider for inference so that there is
    only as much information in the input as needed for the service and not more.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [3](#S3.T3 "Table 3 ‣ 3.3 Inference Phase ‣ 3 Privacy-Preserving Mechanisms
    ‣ Privacy in Deep Learning: A Survey") 所示，与训练领域相比，推理隐私领域的工作较少。推理隐私针对的是部署用于提供推理即服务的系统。在这些情况下，假定部署的系统已完成训练，并且不应从用户提供的数据中学习任何新内容。它仅应执行指定的推理任务。推理隐私的文献分类与训练类似，只是多了一个额外的组，称为信息理论（IT）隐私。这个组的工作通常提供信息理论的数学或实证证据，说明他们的方法如何运作并帮助隐私。这些工作基于第
    [3.1.5](#S3.SS1.SSS5 "3.1.5 Information-Theoretic Privacy ‣ 3.1 Data Aggregation
    ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning: A Survey") 节的上下文感知隐私定义，旨在减少发送到服务提供商进行推理的数据中的信息内容，以便输入中的信息仅限于服务所需的内容，而不多。 '
- en: 'One notable difference between training and inference privacy is the difference
    in the amount of literature on different categories. There seems to be a trend
    of using differential privacy for training, and encryption methods (HE and SMC)
    for inference. One underlying reason could be computational complexity and implementation.
    Encryption methods, specifically homomorphic encryption, are shown to be at least
    two orders of magnitude slower than conventional execution [[140](#bib.bib140)].
    That’s why adopting them for training will increase training time significantly.
    Also, as mentioned in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Homomorphic Encryption
    ‣ 3.2 Training Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning:
    A Survey"), due to approximating non-linear functions, the capabilities of neural
    networks in terms of performance become limited during training on encrypted data.
    For inference, however, adopting encryption is more trivial, since the model is
    already trained. Employing differential privacy, and noise addition, however,
    is less trivial for inference, since it could damage the accuracy of the trained
    model, if not done meticulously. Below we delve deeper into the literature of
    each category.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 训练隐私和推理隐私之间一个显著的区别是不同类别的文献量差异。似乎存在一种趋势，即将差分隐私用于训练，将加密方法（HE 和 SMC）用于推理。一个潜在的原因可能是计算复杂性和实现。加密方法，特别是同态加密，显示出至少比传统执行慢两个数量级
    [[140](#bib.bib140)]。这就是为什么将它们用于训练会显著增加训练时间。此外，如第 [3.2.2](#S3.SS2.SSS2 "3.2.2
    同态加密 ‣ 3.2 训练阶段 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：综述") 节所述，由于近似非线性函数，神经网络在处理加密数据时的性能能力变得有限。然而，对于推理来说，采用加密则较为简单，因为模型已经经过训练。然而，采用差分隐私和噪声添加对于推理来说则不那么简单，因为如果处理不当，可能会损害训练模型的准确性。下面我们将更深入地探讨每一类别的文献。
- en: 3.3.1 Differential Privacy
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 差分隐私
- en: There are very few works using differential privacy for inference. The main
    reason is that differential privacy offers a worst-case guarantee which requires
    high-intensity noise (noise with high standard deviation) to be applied to all
    the segments of the input. This inherently causes performance degradation on pre-trained
    networks. Wang et al. [[128](#bib.bib128)] propose Arden, a data nullification
    and differentially private noise injection mechanism for inference. Arden partitions
    the DNN across edge device and the cloud. A simple data transformation is performed
    on the mobile device, while the computation heavy and complex inference relies
    on the cloud data center. Arden uses data nullification, and noise injection to
    make different queries indistinguishable so that the privacy of the clients is
    preserved. The proposed scheme requires noisy retraining of the entire network,
    with noise injected at different layers. Since it is complicated to calculate
    the global sensitivity at each layer of the neural network, the input to the noise
    injection layer is clipped to the largest possible value created by a member of
    the training set, on the trained network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用差分隐私进行推理的研究非常少。主要原因是差分隐私提供了最坏情况下的保障，这要求对所有输入片段施加高强度的噪声（标准差大的噪声）。这本质上导致了预训练网络的性能下降。Wang
    等人 [[128](#bib.bib128)] 提出了 Arden，这是一种数据无效化和差分隐私噪声注入机制，用于推理。Arden 将 DNN 分割到边缘设备和云端。移动设备上执行简单的数据转换，而计算繁重且复杂的推理则依赖于云数据中心。Arden
    使用数据无效化和噪声注入来使不同查询不可区分，从而保护客户的隐私。该方案需要对整个网络进行有噪声的重新训练，并在不同层注入噪声。由于计算神经网络每一层的全局灵敏度很复杂，因此输入到噪声注入层的数据被剪裁为训练集中成员创建的最大值。
- en: 3.3.2 Homomorphic Encryption
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 同态加密
- en: CryptoNets is one of the first works in HE inference [[129](#bib.bib129)]. Dowlin
    et al. present a method for converting a trained neural network into an encrypted
    one, named a CryptoNet. This allows the clients of an inference service to send
    their data in an encrypted format and receive the result, without their data being
    decrypted. CryptoNets allows the use of SIMD (Single Instruction Multiple Data)
    operations, which increase the throughput of the deployed system. However, for
    single queries, the latency of this scheme is still high.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CryptoNets 是同态加密推理领域的开创性工作之一 [[129](#bib.bib129)]。Dowlin 等人提出了一种将训练好的神经网络转换为加密网络的方法，称为
    CryptoNet。这使得推理服务的客户端可以以加密格式发送数据并接收结果，而无需解密数据。CryptoNets 允许使用 SIMD（单指令多数据）操作，这提高了系统的吞吐量。然而，对于单个查询，这种方案的延迟仍然较高。
- en: Chabanne et al. [[130](#bib.bib130)] approximate the ReLu non-linear activation
    function using low-degree polynomials and provide a normalization layer before
    the activation function, which offers high accuracy. However, they do not show
    results on the latency of their method. More recently, Juvekar et al. propose
    GAZELLE [[140](#bib.bib140)], a system with lower latency (compared to prior work)
    for secure and private neural network inference. GAZELLE combines homomorphic
    encryption with traditional two-party computation techniques (such as garbled
    circuits). With the help of its homomorphic linear algebra kernels, which map
    neural network operations to optimized homomorphic matrix-vector multiplication
    and convolutions, GAZELLE is shown to be three orders of magnitude faster than
    CryptoNets. Sanyal et al. leverage binarized neural networks to speed-up their
    HE inference method. They claim that unlike CryptoNets which only protects the
    data, their proposed scheme can protect the privacy of the model as well.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Chabanne 等人 [[130](#bib.bib130)] 使用低阶多项式近似 ReLu 非线性激活函数，并在激活函数之前提供了归一化层，确保了高准确率。然而，他们没有展示其方法的延迟结果。最近，Juvekar
    等人提出了 GAZELLE [[140](#bib.bib140)]，这是一个具有较低延迟（相较于之前的工作）的安全私密神经网络推理系统。GAZELLE 结合了同态加密与传统的双方计算技术（如混淆电路）。借助其同态线性代数内核，将神经网络操作映射到优化的同态矩阵-向量乘法和卷积中，GAZELLE
    的速度比 CryptoNets 快了三个数量级。Sanyal 等人利用二值化神经网络加速他们的 HE 推理方法。他们声称，与仅保护数据的 CryptoNets
    不同，他们提出的方案也能保护模型的隐私。
- en: 3.3.3 Secure Multi-Party Computation (SMC)
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 安全多方计算（SMC）
- en: Liu et al. propose MiniONN [[139](#bib.bib139)], which uses additively homomorphic
    encryption (AHE) in a preprocessing step, unlike GAZELLE which uses AHE to speed
    up linear algebra directly. MiniONN demonstrates a significant performance improvement
    compared to CryptoNets, without loss of accuracy. However, it is only a two-party
    computation scheme and does not support computation over multiple parties. Riazi
    et al. introduce Chameleon, a two-party computation framework whose vector dot
    product of signed fixed-point numbers improves the efficiency of prediction in
    classification methods based upon heavy matrix multiplications. Chameleon achieves
    a 4.2$\times$ latency improvement over MiniONN. Most of the efforts in the field
    of SMC for deep learning are focused on speeding up the computation, as demonstrated
    above, and also by  [[118](#bib.bib118)],  [[119](#bib.bib119)],  [[135](#bib.bib135)].
    The accuracy loss of the aforementioned methods, compared to their pre-trained
    models is negligible (less than 1%).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人提出了 MiniONN [[139](#bib.bib139)]，在预处理步骤中使用加法同态加密（AHE），不同于直接利用 AHE 加速线性代数的
    GAZELLE。MiniONN 显示出相较于 CryptoNets 的显著性能提升，同时没有精度损失。然而，它只是一个双方计算方案，不支持多方计算。Riazi
    等人介绍了 Chameleon，一个双方计算框架，其签名定点数的向量点积提高了基于重矩阵乘法的分类方法的预测效率。Chameleon 比 MiniONN 的延迟提高了
    4.2$\times$。如上所示，深度学习领域的 SMC 大多集中在加速计算方面，也可以参考 [[118](#bib.bib118)]、[[119](#bib.bib119)]、[[135](#bib.bib135)]。上述方法的准确性损失与其预训练模型相比微不足道（不到
    1%）。
- en: 3.3.4 Information Theoretic Privacy
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 信息论隐私
- en: Privacy-preserving schemes that rely on information-theoretic approaches usually
    assume a non-sensitive task, the task that the service is supposed to execute
    and try to degrade any excessive information in the input data that is not needed
    for the main inference task [[142](#bib.bib142), [145](#bib.bib145), [143](#bib.bib143),
    [29](#bib.bib29), [31](#bib.bib31), [146](#bib.bib146)].  [[142](#bib.bib142),
    [143](#bib.bib143), [145](#bib.bib145)] propose anynymization schemes for protecting
    temporal sensory data through obfuscation. Malekzadeh et al. [[142](#bib.bib142)]
    propose a multi-objective loss function for training deep autoencoders to extract
    and obfuscate user identity-related information, while preserving the utility
    of the sensor data. The training process regulates the encoder to disregard user-identifiable
    patterns and tunes the decoder to shape the output independently of users in the
    training set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于信息理论方法的隐私保护方案通常假设一个非敏感任务，即服务应该执行的任务，并尝试减少输入数据中对主要推断任务不必要的多余信息[[142](#bib.bib142),
    [145](#bib.bib145), [143](#bib.bib143), [29](#bib.bib29), [31](#bib.bib31), [146](#bib.bib146)]。[[142](#bib.bib142),
    [143](#bib.bib143), [145](#bib.bib145)] 提出了通过混淆保护时间敏感数据的匿名化方案。Malekzadeh 等人[[142](#bib.bib142)]
    提出了一个多目标损失函数，用于训练深度自编码器以提取和混淆与用户身份相关的信息，同时保持传感器数据的实用性。训练过程会调节编码器以忽略用户可识别的模式，并调整解码器，使输出不依赖于训练集中的用户。
- en: Another body of work [[31](#bib.bib31), [147](#bib.bib147), [29](#bib.bib29),
    [144](#bib.bib144)] propose such schemes for computer vision tasks and preserving
    privacy of images. Osia et al. propose Deep Private Feature Extraction (DPFE) [[29](#bib.bib29)]
    which aims at obfuscating input images to hinder the classification of given sensitive
    (private) labels, by modifying the network topology and re-training all the model
    parameters. DPFE partitions the network in two partitions, first partition to
    be deployed on the edge and the second on the cloud. It also modifies the network
    architecture by adding an auto-encoder in the middle and then re-training the
    entire network with its loss function. The encoder part of the auto-encoder is
    deployed on the edge device, and the decoder is deployed on the server. The auto-encoder
    aims to reduce the dimensions of the sent data which decreases the communication
    cost, alongside decreasing the amount of information that is sent, which helps
    the privacy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一部分工作[[31](#bib.bib31), [147](#bib.bib147), [29](#bib.bib29), [144](#bib.bib144)]
    提出了用于计算机视觉任务和图像隐私保护的方案。Osia 等人提出了深度私人特征提取（DPFE）[[29](#bib.bib29)]，其目的是通过修改网络拓扑结构和重新训练所有模型参数，来混淆输入图像以阻碍对给定敏感（私人）标签的分类。DPFE
    将网络分为两个部分，第一部分部署在边缘，第二部分部署在云端。它还通过在中间添加一个自编码器来修改网络架构，然后用其损失函数重新训练整个网络。自编码器的编码器部分部署在边缘设备上，而解码器部署在服务器上。自编码器旨在减少发送数据的维度，从而降低通信成本，并减少发送的信息量，这有助于保护隐私。
- en: 'DPFE’s loss function can be seen in Equation [2](#S3.E2 "In 3.3.4 Information
    Theoretic Privacy ‣ 3.3 Inference Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy
    in Deep Learning: A Survey"). It is composed of three terms, first, the cross-entropy
    loss for a classification problem consisting of M classes ($y_{o,c}$ indicates
    whether the observation $o$ belongs to class $c$ and $p_{o,c}$ is the probability
    given by the network for the observation to belong to class $c$). This term aims
    at maintaining accuracy. Second, a term that tries to decrease the distance between
    intermediate activations of inputs with different private labels, and a final
    term which tries to increase the distance between intermediate activations of
    inputs with the same private label. $\gamma$ is a constant which depends on the
    number of dimensions and the training data, it is used as a normalization factor.
    $k$ is also a constant which depends on the training data. $i$ and $j$ are iterators
    over the main batch and a random batch, respectively and $Y$ is the private label
    for that batch member.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DPFE的损失函数可以在方程 [2](#S3.E2 "在3.3.4 信息论隐私 ‣ 3.3 推断阶段 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：综述")
    中看到。它由三部分组成：首先，是用于分类问题的交叉熵损失，涉及M类（$y_{o,c}$表示观测值$o$是否属于类$c$，$p_{o,c}$是网络为观测值属于类$c$提供的概率）。这一项旨在保持准确性。其次，是一个旨在减少具有不同隐私标签的输入之间中间激活的距离的项，以及一个最终项，旨在增加具有相同隐私标签的输入之间中间激活的距离。$\gamma$是一个依赖于维度数量和训练数据的常量，用作归一化因子。$k$也是一个依赖于训练数据的常量。$i$和$j$分别是主批次和随机批次的迭代器，而$Y$是该批次成员的隐私标签。
- en: '|  | $\begin{split}-\sum_{c=1}^{M}y_{o,c}log(p_{o,c})+\gamma(\sum_{(i,j):Y_{i}\neq
    Y_{j}}&#124;&#124;a^{\prime}_{i}-a^{\prime}_{j}&#124;&#124;_{2}\\ +\sum_{(i,j):Y_{i}=Y_{j}}(k-&#124;&#124;a^{\prime}_{i}-a^{\prime}_{j}&#124;&#124;_{2}))\end{split}$
    |  | (2) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}-\sum_{c=1}^{M}y_{o,c}log(p_{o,c})+\gamma(\sum_{(i,j):Y_{i}\neq
    Y_{j}}&#124;&#124;a^{\prime}_{i}-a^{\prime}_{j}&#124;&#124;_{2}\\ +\sum_{(i,j):Y_{i}=Y_{j}}(k-&#124;&#124;a^{\prime}_{i}-a^{\prime}_{j}&#124;&#124;_{2}))\end{split}$
    |  | (2) |'
- en: DPFE retrains the given neural network and the auto-encoder with this loss function.
    The training can be seen as an attempt to create clustered representations of
    data, where the inputs with the same private labels go in different clusters,
    and inputs with different labels are pushed to the same cluster, to mislead any
    adversary who tries to infer the private labels. Given its loss function, DPFE
    cannot operate without the private labels. Therefore, if there is a setting in
    which no sensitive labels are provided, DPFE cannot be used. After training, for
    each inference request, a randomly generated noise is added to the intermediate
    results on the fly. This noise is not there to achieve differential privacy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: DPFE使用该损失函数对给定的神经网络和自编码器进行再训练。训练可以视为尝试创建数据的聚类表示，其中具有相同隐私标签的输入进入不同的聚类，而具有不同标签的输入被推向相同的聚类，以误导任何试图推断隐私标签的对手。鉴于其损失函数，DPFE在没有隐私标签的情况下无法操作。因此，如果存在没有提供敏感标签的设置，则无法使用DPFE。训练后，对于每个推断请求，都会随机生成噪声并动态添加到中间结果中。此噪声并不是为了实现差分隐私。
- en: More recently, Mireshghallah et al. suggested Shredder [[31](#bib.bib31)], a
    framework that without altering the topology or the weights of a pre-trained network,
    heuristically learns additive noise distributions that reduce the information
    content of communicated data while incurring minimal loss to the inference accuracy.
    Shredder’s approach also consists of cutting the neural network and executing
    a part of it on the edge device, similar to DPFE. This approach has been shown
    to decrease the overall execution time in some cases [[31](#bib.bib31)], compared
    to running the entire neural network on the cloud, since the communication takes
    the bulk of time and sensing intermediate representations can sometimes save on
    the communication since there are fewer dimensions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Mireshghallah 等人建议了Shredder [[31](#bib.bib31)]，这是一个在不改变预训练网络的拓扑结构或权重的情况下，启发式地学习附加噪声分布的框架，这些分布减少了传输数据的信息内容，同时对推断准确性造成的损失最小。Shredder的方法还包括剪切神经网络并在边缘设备上执行其中的一部分，类似于DPFE。与在云端运行整个神经网络相比，这种方法在某些情况下被证明能减少总体执行时间
    [[31](#bib.bib31)]，因为通信占用了大量时间，而感知中间表示有时可以节省通信，因为维度更少。
- en: 'Shredder initializes a noise tensor, with the same dimension as the intermediate
    activation, by sampling from a Laplace distribution with location of $0$, and
    scale of $b$, which is a hyperparameter. Then, using the loss function shown in
    Equation [3](#S3.E3 "In 3.3.4 Information Theoretic Privacy ‣ 3.3 Inference Phase
    ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy in Deep Learning: A Survey"), it tries
    to maintain the accuracy of the model (first term), while increasing the amount
    of additive noise (second term). $\lambda$ is a knob that provides an accuracy-privacy
    trade-off.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Shredder 通过从位置为 $0$、尺度为 $b$ 的拉普拉斯分布中采样来初始化噪声张量，其维度与中间激活相同，$b$ 是一个超参数。然后，使用方程
    [3](#S3.E3 "在 3.3.4 信息理论隐私 ‣ 3.3 推理阶段 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：综述") 中显示的损失函数，它试图保持模型的准确性（第一项），同时增加附加噪声的量（第二项）。$\lambda$
    是一个提供准确性-隐私权衡的控制参数。
- en: '|  | $-\sum_{c=1}^{M}y_{o,c}log(p_{o,c})-\lambda\sum_{i=1}^{N}{&#124;n_{i}&#124;}$
    |  | (3) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $-\sum_{c=1}^{M}y_{o,c}log(p_{o,c})-\lambda\sum_{i=1}^{N}{&#124;n_{i}&#124;}$
    |  | (3) |'
- en: Once the training is terminated, a Laplace distribution is fit to the trained
    tensor, and the parameters of that distribution, alongside the order of the elements,
    are saved. A collection of these distributions are gathered. During inference,
    noise is sampled from one of the saved distributions and re-ordered to match the
    saved order. This noise tensor is then added to the intermediate representation,
    before being sent to the cloud.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练结束，会将拉普拉斯分布拟合到训练后的张量上，并保存该分布的参数以及元素的顺序。这些分布被收集起来。在推理过程中，从保存的分布中采样噪声并重新排序以匹配保存的顺序。然后将这个噪声张量添加到中间表示中，然后再发送到云端。
- en: Both DPFE and Shredder empirically demonstrate a reduction in the number of
    mutual information bits between the original data and the sent intermediate representation.
    DPFE can only be effective if the user knows what s/he wants to protect against,
    whereas Shredder offers a more general approach that tries to obliterate any information
    that is irrelevant to the primary task. Empirical evaluations showed that Shredder
    can in average loose more mutual information, compared to DPFE. However, in the
    task of inferring private labels, DPFE performs slightly better by causing a higher
    misclassification rate for the adversary since it has access to the private labels
    during training time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DPFE 和 Shredder 都通过实验证明了原始数据和发送的中间表示之间的互信息比的减少。DPFE 只有在用户知道自己想要保护什么时才能有效，而 Shredder
    提供了一种更为通用的方法，试图消除与主要任务无关的任何信息。实证评估显示，相比于 DPFE，Shredder 平均可以丢失更多的互信息。然而，在推断私人标签的任务中，由于
    DPFE 在训练时可以访问私人标签，它通过使对手的误分类率更高，表现略好。
- en: 'More recently, Mireshghallah et al. propose a non-intrusive interpretable approach
    dubbed Cloak, in which there is no need to change/retrain the network parameters,
    nor partition it. This work attempts to explain the decision making process of
    deep neural networks (DNNs) by separating the subset of input features that are
    essential and unessential for the decisions made by the DNN during test-time inference.
    This separation is made possible through the adoption of information-theoretic
    bounds for different set of features. After identifying the essential subset,
    Cloak suppresses the rest of the features using learned values, and only sends
    the essential ones. In this respect, Cloak offers an interpretable privacy-preserving
    mechanism. An example of representations produced by Cloak and the conducive/non-conducive
    feature separation can be seen in Figure [5](#S3.F5 "Figure 5 ‣ 3.3.4 Information
    Theoretic Privacy ‣ 3.3 Inference Phase ‣ 3 Privacy-Preserving Mechanisms ‣ Privacy
    in Deep Learning: A Survey").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Mireshghallah 等人提出了一种名为 Cloak 的非侵入性可解释方法，该方法无需更改/重新训练网络参数，也无需对其进行分割。这项工作试图通过分离在测试时推理过程中对
    DNN 决策至关重要和不重要的输入特征子集来解释深度神经网络（DNN）的决策过程。这种分离是通过采用不同特征集合的信息理论界限来实现的。在识别出关键子集后，Cloak
    使用学习到的值抑制其余特征，并仅发送关键特征。在这方面，Cloak 提供了一种可解释的隐私保护机制。Cloak 生成的表示和有利/不利特征分离的示例如图 [5](#S3.F5
    "图 5 ‣ 3.3.4 信息理论隐私 ‣ 3.3 推理阶段 ‣ 3 隐私保护机制 ‣ 深度学习中的隐私：综述") 中所示。
- en: '![Refer to caption](img/3aa0f36225ec10b5ec18676b33b0e281.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3aa0f36225ec10b5ec18676b33b0e281.png)'
- en: 'Figure 5: Cloak’s discovered features for target DNN classifiers (VGG-16) for
    black-hair color, eyeglasses, gender, and smile detection. The colored features
    are conducive to the task. The 3 sets of features depicted for each task correspond
    to different suppression ratios (SR). AL denotes the range of accuracy loss imposed
    by the suppression.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Cloak为目标DNN分类器（VGG-16）发现的特征，用于黑发颜色、眼镜、性别和笑容检测。彩色特征有助于任务。每个任务描绘的3组特征对应不同的抑制比（SR）。AL表示抑制施加的准确度损失范围。
- en: 4 Privacy-Enhancing Execution Models and Environments
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 隐私增强执行模型和环境
- en: Apart from privacy-preserving schemes which are methods that directly optimize
    for a given definition of privacy, there are given execution models and environments
    that help enhance privacy and are not by themselves privacy-preserving. In this
    section, we will briefly discuss federated learning, split learning and trusted
    execution environments, which have been used to enhance privacy. These methods
    are usually accompanied by privacy-preserving schemes from the previous section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了隐私保护方案，这些方案是直接优化特定隐私定义的方法，还有一些执行模型和环境有助于增强隐私，但本身并非隐私保护的。在本节中，我们将简要讨论联邦学习、分裂学习和可信执行环境，这些方法已被用于增强隐私。这些方法通常伴随前一节中的隐私保护方案。
- en: 4.1 Federated Learning
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 联邦学习
- en: '![Refer to caption](img/8ddbdc78609dd4ae525bee1c92cb020a.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/8ddbdc78609dd4ae525bee1c92cb020a.png)'
- en: 'Figure 6: The workflow of federated learning model [[127](#bib.bib127)].'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：联邦学习模型的工作流程[[127](#bib.bib127)]。
- en: Federated learning (FL) is a machine learning setting where many clients collaboratively
    train a model under the administration of a central server while keeping the training
    data local. Federated learning is built on the principles of focused collection
    and data minimization which can alleviate the privacy risks of centralized machine
    learning [[127](#bib.bib127)].
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）是一种机器学习设置，多个客户端在中央服务器的管理下协作训练模型，同时保持训练数据本地。联邦学习建立在集中收集和数据最小化的原则上，这可以缓解集中式机器学习的隐私风险[[127](#bib.bib127)]。
- en: 'The workflow of federated learning can be seen in Figure [6](#S4.F6 "Figure
    6 ‣ 4.1 Federated Learning ‣ 4 Privacy-Enhancing Execution Models and Environments
    ‣ Privacy in Deep Learning: A Survey"). This workflow is broken into six stages [[127](#bib.bib127)]:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '联邦学习的工作流程如图[6](#S4.F6 "Figure 6 ‣ 4.1 Federated Learning ‣ 4 Privacy-Enhancing
    Execution Models and Environments ‣ Privacy in Deep Learning: A Survey")所示。这个工作流程分为六个阶段[[127](#bib.bib127)]：'
- en: '1.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Problem identification: The problem that is to be solved using federated learning
    should first be defined.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题识别：首先需要定义要使用联邦学习解决的问题。
- en: '2.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Client instrumentation: The clients can be instructed to save the data needed
    for training. For example, the applications running on the edge devices might
    need to locally save some metadata (e.g. user interaction data) alongside the
    main data (for instance text messages).'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 客户端工具：可以指示客户端保存训练所需的数据。例如，运行在边缘设备上的应用程序可能需要本地保存一些元数据（例如用户交互数据）以及主要数据（例如文本消息）。
- en: '3.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Simulation prototyping (optional): The engineer who is deploying the system
    might need to prototype different architectures and try different hyperparameters
    in a federated learning simulation.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模拟原型（可选）：部署系统的工程师可能需要在联邦学习模拟中原型化不同的架构并尝试不同的超参数。
- en: '4.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Federated model training: Multiple federated training tasks are initiated which
    train different variations of the model or use different optimization hyperparameters.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦模型训练：启动多个联邦训练任务，这些任务训练模型的不同变体或使用不同的优化超参数。
- en: '5.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Model evaluation: When the tasks are done with the training phase (usually
    after a few days), the models are analyzed and evaluated, either on standard centralized
    datasets or on local client data.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型评估：当任务完成训练阶段（通常在几天后）时，模型会进行分析和评估，无论是在标准的集中式数据集上还是在本地客户端数据上。
- en: '6.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Deployment: When the analysis is finished and a model is selected, the launch
    process is initiated. This process consists of live A/B testing, manual quality
    assurance, and a staged roll-out.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署：当分析完成并选择了模型后，启动发布过程。该过程包括实时A/B测试、手动质量保证和分阶段推出。
- en: Federated learning is being widely used with SMC and differential Privacy [[127](#bib.bib127),
    [107](#bib.bib107), [148](#bib.bib148)].Bonawitz et al. apply Secure aggregation
    to privately combine the outputs of local machine learning on user devices in
    the federated learning setup, to update a global model. Secure aggregation refers
    to the computation of a sum in a multiparty setting, where no party reveals its
    update in the clear, even to the aggregator. When Secure Aggregation is added
    to Federated Learning, the aggregation of model updates is performed by a virtual
    incorruptible third party induced by secure multiparty communication. With this
    setup, the cloud provider learns only the aggregated model update. There are also
    bodies of work that consider shuffling of user data, so as to hide the origin
    of each data item.The works of Cheu et al., and Balle et al. have proposed secure
    aggregation protocols that satisfy differential privacy guarantees in the shuffle
    model [[149](#bib.bib149), [150](#bib.bib150)]. More recent work [[151](#bib.bib151)]
    mitigates the incurred error and communication overheads in shuffle model. More
    in-depth details of federated learning workflow and integration is out of the
    scope of this survey.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习广泛应用于SMC和差分隐私[[127](#bib.bib127), [107](#bib.bib107), [148](#bib.bib148)]。Bonawitz等人应用安全聚合来私密地组合用户设备上本地机器学习的输出，以更新全球模型。安全聚合指的是在多方环境中计算总和，其中没有任何一方将其更新明文显示，即使是给聚合者。将安全聚合添加到联邦学习中时，模型更新的聚合由通过安全多方通信引导的虚拟不可篡改第三方执行。使用这种设置，云提供商只了解聚合后的模型更新。还有一些研究考虑了用户数据的混洗，以隐藏每个数据项的来源。Cheu等人和Balle等人提出了在混洗模型中满足差分隐私保证的安全聚合协议[[149](#bib.bib149),
    [150](#bib.bib150)]。最近的工作[[151](#bib.bib151)]减少了混洗模型中产生的误差和通信开销。关于联邦学习工作流程和集成的更深入细节超出了本调查的范围。
- en: 4.2 Split Learning
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 分割学习
- en: '![Refer to caption](img/0fb15fe2595fb4d9a7361960df428612.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0fb15fe2595fb4d9a7361960df428612.png)'
- en: (a) Vanilla split learning
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始分割学习
- en: '![Refer to caption](img/7712c1501696467db22f722a4e2e5800.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7712c1501696467db22f722a4e2e5800.png)'
- en: (b) Boomerang split learning
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 回旋镖分割学习
- en: 'Figure 7: The vanilla configuration of split learning where raw data is not
    shared between client and server, and boomerang (U-shaped) configuration where
    neither raw data nor the labels are shared between client and server [[152](#bib.bib152)].'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：原始的分割学习配置，其中原始数据在客户端和服务器之间不共享，以及回旋镖（U形）配置，其中原始数据和标签都不在客户端和服务器之间共享[[152](#bib.bib152)]。
- en: 'Split-learning is an execution model where the neural network is split, between
    the client and the server [[58](#bib.bib58)]. This is very similar to the neural
    network partitioning described in Shredder [[31](#bib.bib31)] and DPFE [[29](#bib.bib29)].
    Vanilla split learning is formed by each client computing the forward pass through
    a deep neural network up to a specific layer, called the cut layer. The outputs
    of the cut layer, referred to as smashed data, are sent from the edge device to
    another entity (either the server or another client), which completes the rest
    of the computation. With this execution scheme, a round of forward pass is computed
    without sharing raw data. The gradients can then be backpropagated from the server
    to the cut layer in a similar fashion. The gradients at the cut layer are transferred
    back to the clients, where the rest of the backpropagation is completed. In this
    fashion, the training or inference is done without having clients directly access
    each other’s raw data. An instantiation of this setup where labels are also not
    shared along with raw data is shown in Figure [7](#S4.F7 "Figure 7 ‣ 4.2 Split
    Learning ‣ 4 Privacy-Enhancing Execution Models and Environments ‣ Privacy in
    Deep Learning: A Survey").'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 分割学习是一种执行模型，其中神经网络在客户端和服务器之间分割[[58](#bib.bib58)]。这与Shredder[[31](#bib.bib31)]和DPFE[[29](#bib.bib29)]中描述的神经网络分区非常相似。原始的分割学习是由每个客户端计算通过深度神经网络到特定层（称为切割层）的前向传递。切割层的输出，称为破碎数据，从边缘设备发送到另一个实体（可以是服务器或另一客户端），该实体完成其余计算。通过这种执行方案，一轮前向传递在不共享原始数据的情况下计算完成。梯度然后可以以类似方式从服务器回传到切割层。切割层的梯度被转移回客户端，剩余的反向传播在客户端完成。通过这种方式，训练或推理在客户端无需直接访问彼此的原始数据即可完成。一个实例，其中标签也未与原始数据一起共享，如图[7](#S4.F7
    "图 7 ‣ 4.2 分割学习 ‣ 4 隐私增强执行模型和环境 ‣ 深度学习中的隐私：一项调查")所示。
- en: 4.3 Trusted Execution Environments (TEEs)
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 受信执行环境（TEEs）
- en: Trusted execution environments, also referred to as secure enclaves, provide
    opportunities to move parts of decentralized learning or inference processes into
    a trusted environment in the cloud, whose code can be attested and verified. Recently,
    Mo et al. have suggested a framework that uses an edge device’s Trusted Execution
    Environment (TEE) in conjunction with model partitioning to limit the attack surface
    against DNNs [[153](#bib.bib153)]. TEEs can provide integrity and confidentiality
    during execution. TEEs have been deployed in many forms, including Intel’s SGX-enabled
    CPUs [[154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)],
    Arm’s TrustZone [[158](#bib.bib158)]. This execution model, however, requires
    the users to send their data to an enclave running on remote servers which allows
    the remote server to have access to the raw data and as the new breaches in hardware [[37](#bib.bib37),
    [36](#bib.bib36), [159](#bib.bib159), [160](#bib.bib160), [161](#bib.bib161),
    [162](#bib.bib162)] show, the access can lead to comprised privacy.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The surge in the use of machine learning is due to the growth in data and compute.
    The data mostly comes from people [[7](#bib.bib7)] and includes an abundance of
    sensitive information. This work tries to provide a comprehensive and systematic
    summary of the efforts made to protect privacy of users in deep learning settings.
    We find an apparent disparity in the number of efforts between data aggregation,
    training, and inference phases. In particular, little attention has been made
    to privacy of the users during inference phase.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] S. Laine, T. Karras, T. Aila, A. Herva, S. Saito, R. Yu, H. Li, and J. Lehtinen,
    “Production-level facial performance capture using deep convolutional neural networks,”
    in Proceedings of the ACM SIGGRAPH / Eurographics Symposium on Computer Animation,
    SCA ’17, (New York, NY, USA), Association for Computing Machinery, 2017.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] V. Këpuska and G. Bohouta, “Next-generation of virtual personal assistants
    (microsoft cortana, apple siri, amazon alexa and google home),” 2018 IEEE 8th
    Annual Computing and Communication Workshop and Conference (CCWC), pp. 99–103,
    2018.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] G. A. Kaissis, M. R. Makowski, D. Rückert, and R. F. Braren, “Secure, privacy-preserving
    and federated machine learning in medical imaging,” Nature Machine Intelligence,
    pp. 1–7, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Singh, H. Sikka, S. Kotti, and A. Trask, “Benchmarking differentially
    private residual networks for medical imagery,” arXiv preprint arXiv:2005.13099,
    2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, “Privacy
    in pharmacogenetics: An end-to-end case study of personalized warfarin dosing,”
    in Proceedings of the 23rd USENIX Conference on Security Symposium, SEC’14, (USA),
    p. 17–32, USENIX Association, 2014.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. J. Bolton and D. J. Hand, “Statistical fraud detection: A review,” Statist.
    Sci., vol. 17, pp. 235–255, 08 2002.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. A. Thompson and C. Warzel, “The privacy project: Twelve million phones,
    one dataset, zero privacy,” 2019. online accessed February 2020 [https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html](https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Warzel, “The privacy project: Faceapp shows we care about privacy but
    don’t understand it,” 2019. online accessed February 2020 [https://www.nytimes.com/2019/07/18/opinion/faceapp-privacy.html](https://www.nytimes.com/2019/07/18/opinion/faceapp-privacy.html).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Newcomb, “Facebook data harvesting scandal widens to 87 million people,”
    2018. online accessed February 2020 [https://www.nbcnews.com/tech/tech-news/facebook-data-harvesting-scandal-widens-87-million-people-n862771](https://www.nbcnews.com/tech/tech-news/facebook-data-harvesting-scandal-widens-87-million-people-n862771).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Frome, G. Cheung, A. Abdulkader, M. Zennaro, B. Wu, A. Bissacco, H. Adam,
    H. Neven, and L. Vincent, “Large-scale privacy protection in google street view,”
    2009 IEEE 12th International Conference on Computer Vision, pp. 2373–2380, 2009.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Schiff, M. Meingast, D. K. Mulligan, S. S. Sastry, and K. Goldberg,
    “Respectful cameras: Detecting visual markers in real-time to address privacy
    concerns,” in Protecting Privacy in Video Surveillance, 2009.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Senior, S. Pankanti, A. Hampapur, L. Brown, Ying-Li Tian, A. Ekin,
    J. Connell, Chiao Fe Shu, and M. Lu, “Enabling video privacy through computer
    vision,” IEEE Security Privacy, vol. 3, no. 3, pp. 50–57, 2005.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] L. Cheng, F. Liu, and D. D. Yao, “Enterprise data breach: causes, challenges,
    prevention, and future directions,” WIREs Data Mining and Knowledge Discovery,
    vol. 7, no. 5, p. e1211, 2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] T. Armerding, “The 18 biggest data breaches of the 21st century,” 2018.
    online accessed February 2020 [https://www.csoonline.com/article/2130877/the-biggest-data-breaches-of-the-21st-century.html](https://www.csoonline.com/article/2130877/the-biggest-data-breaches-of-the-21st-century.html).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] R. Shokri, M. Stronati, and V. Shmatikov, “Membership inference attacks
    against machine learning models,” CoRR, vol. abs/1610.05820, 2016.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
    exploit confidence information and basic countermeasures,” in Proceedings of the
    22nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, (New
    York, NY, USA), p. 1322–1333, Association for Computing Machinery, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in machine
    learning: Analyzing the connection to overfitting,” in 2018 IEEE 31st Computer
    Security Foundations Symposium (CSF), pp. 268–282, July 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Yan, C. W. Fletcher, and J. Torrellas, “Cache telepathy: Leveraging
    shared resource attacks to learn dnn architectures,” ArXiv, vol. abs/1808.04761,
    2018.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
    machine learning models via prediction apis,” CoRR, vol. abs/1609.02943, 2016.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property inference
    attacks on fully connected neural networks using permutation invariant representations,”
    in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications
    Security, CCS ’18, (New York, NY, USA), p. 619–633, Association for Computing
    Machinery, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] L. Sweeney, “k-anonymity: A model for protecting privacy,” International
    Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 10, no. 05,
    pp. 557–570, 2002.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
    sensitivity in private data analysis,” in Proceedings of the Third Conference
    on Theory of Cryptography, TCC’06, (Berlin, Heidelberg), pp. 265–284, Springer-Verlag,
    2006.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in Proceedings
    of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS
    ’15, (New York, NY, USA), p. 1310–1321, Association for Computing Machinery, 2015.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
    and L. Zhang, “Deep learning with differential privacy,” in Proceedings of the
    2016 ACM SIGSAC Conference on Computer and Communications Security, CCS ’16, (New
    York, NY, USA), p. 308–318, Association for Computing Machinery, 2016.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Papernot, M. Abadi, Úlfar Erlingsson, I. Goodfellow, and K. Talwar,
    “Semi-supervised knowledge transfer for deep learning from private training data,”
    2016.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Hamm, P. Cao, and M. Belkin, “Learning privately from multiparty data,”
    CoRR, vol. abs/1602.03552, 2016.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] E. Hesamifard, H. Takabi, and M. Ghasemi, “Cryptodl: Deep neural networks
    over encrypted data,” CoRR, vol. abs/1711.05189, 2017.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. Zhang, Z. He, and R. B. Lee, “Privacy-preserving machine learning through
    data obfuscation,” ArXiv, vol. abs/1807.01860, 2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. A. Ossia, A. Taheri, A. S. Shamsabadi, K. Katevas, H. Haddadi, and
    H. R. Rabiee, “Deep private-feature extraction,” IEEE Transactions on Knowledge
    and Data Engineering, vol. 32, pp. 54–66, 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] N. Dowlin, R. Gilad-Bachrach, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing,
    “Cryptonets: Applying neural networks to encrypted data with high throughput and
    accuracy,” in Proceedings of the 33rd International Conference on International
    Conference on Machine Learning - Volume 48, ICML’16, pp. 201–210, JMLR.org, 2016.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] F. Mireshghallah, M. Taram, A. Jalali, D. Tullsen, and H. Esmaeilzadeh,
    “Shredder: Learning noise distributions to protect inference privacy,” in Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems, ASPLOS ’20, (New York, NY, USA), Association
    for Computing Machinery, March 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay,
    “Adversarial attacks and defences: A survey,” CoRR, vol. abs/1810.00069, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
    vector machines,” in Proceedings of the 29th International Coference on International
    Conference on Machine Learning, ICML’12, (Madison, WI, USA), p. 1467–1474, Omnipress,
    2012.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] I. Security, “Data exfiltration study: actors, tactics, and detection
    (2015),” 2015.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. Warzel, “Chinese hacking is alarming. so are data brokers.,” 2020.
    online accessed February 2020 [https://www.nytimes.com/2020/02/10/opinion/equifax-breach-china-hacking.html](https://www.nytimes.com/2020/02/10/opinion/equifax-breach-china-hacking.html).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Lipp, M. Schwarz, D. Gruss, T. Prescher, W. Haas, A. Fogh, J. Horn,
    S. Mangard, P. Kocher, D. Genkin, Y. Yarom, and M. Hamburg, “Meltdown: Reading
    kernel memory from user space,” in 27th USENIX Security Symposium (USENIX Security
    18), 2018.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] P. Kocher, J. Horn, A. Fogh, , D. Genkin, D. Gruss, W. Haas, M. Hamburg,
    M. Lipp, S. Mangard, T. Prescher, M. Schwarz, and Y. Yarom, “Spectre attacks:
    Exploiting speculative execution,” in 40th IEEE Symposium on Security and Privacy
    (S&P’19), 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Whitten and J. D. Tygar, “Why johnny can’t encrypt: A usability evaluation
    of pgp 5.0,” in Proceedings of the 8th Conference on USENIX Security Symposium
    - Volume 8, SSYM’99, (USA), p. 14, USENIX Association, 1999.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Unuchek, “Leaking ads – is user data truly secure?,” April 2018. online
    accessed February 2020 [https://published-prd.lanyonevents.com/published/rsaus18/sessionsFiles/8161/ASEC-T08-Leaking-Ads-Is-User-Data-Truly-Secure.pdf](https://published-prd.lanyonevents.com/published/rsaus18/sessionsFiles/8161/ASEC-T08-Leaking-Ads-Is-User-Data-Truly-Secure.pdf).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Y. Long, V. Bindschaedler, and C. A. Gunter, “Towards measuring membership
    privacy,” ArXiv, vol. abs/1712.09136, 2017.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes, “Ml-leaks: Model
    and data independent membership inference attacks and defenses on machine learning
    models,” ArXiv, vol. abs/1806.01246, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] C. Song and V. Shmatikov, “The natural auditor: How to tell if someone
    used your words to train their model,” ArXiv, vol. abs/1811.00513, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro, “LOGAN: evaluating
    privacy leakage of generative models using generative adversarial networks,” CoRR,
    vol. abs/1705.07663, 2017.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C. Song and V. Shmatikov, “Auditing data provenance in text-generation
    models,” in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining, KDD ’19, (New York, NY, USA), p. 196–206, Association
    for Computing Machinery, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Z. Yang, E.-C. Chang, and Z. Liang, “Adversarial neural network inversion
    via auxiliary knowledge alignment,” ArXiv, vol. abs/1902.08552, 2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Salem, A. Bhattacharyya, M. Backes, M. Fritz, and Y. Zhang, “Updates-leak:
    Data set inference and reconstruction attacks in online learning,” CoRR, vol. abs/1904.01067,
    2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Z. He, T. Zhang, and R. B. Lee, “Model inversion attacks against collaborative
    inference,” in Proceedings of the 35th Annual Computer Security Applications Conference,
    ACSAC ’19, (New York, NY, USA), p. 148–162, Association for Computing Machinery,
    2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] N. Carlini, C. Liu, J. Kos, Ú. Erlingsson, and D. Song, “The secret sharer:
    Measuring unintended neural network memorization & extracting secrets,” CoRR,
    vol. abs/1802.08232, 2018.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] G. Ateniese, G. Felici, L. V. Mancini, A. Spognardi, A. Villani, and D. Vitali,
    “Hacking smart machines with smarter ones: How to extract meaningful data from
    machine learning classifiers,” CoRR, vol. abs/1306.4447, 2013.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine learning,”
    2018 IEEE Symposium on Security and Privacy (SP), pp. 36–52, 2018.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Sablayrolles, M. Douze, Y. Ollivier, C. Schmid, and H. Jégou, “White-box
    vs black-box: Bayes optimal strategies for membership inference,” 2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei, “Towards demystifying
    membership inference attacks,” ArXiv, vol. abs/1807.09173, 2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] D. Arpit, S. Jastrzundefinedbski, N. Ballas, D. Krueger, E. Bengio, M. S.
    Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio, and et al., “A closer
    look at memorization in deep networks,” in Proceedings of the 34th International
    Conference on Machine Learning - Volume 70, ICML’17, p. 233–242, JMLR.org, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] C. R. Meehan, K. Chaudhuri, and S. Dasgupta, “A non-parametric test to
    detect data-copying in generative models,” ArXiv, vol. abs/2004.05675, 2020.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Izzo, M. A. Smart, K. Chaudhuri, and J. Zou, “Approximate data deletion
    from machine learning models: Algorithms and evaluations,” ArXiv, vol. abs/2002.10077,
    2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, “A methodology for formalizing
    model-inversion attacks,” in 2016 IEEE 29th Computer Security Foundations Symposium
    (CSF), pp. 355–370, June 2016.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Brockschmidt, B. Köpf, O. Ohrimenko, A. Paverd, V. Rühle, S. Tople,
    L. Wutschitz, and S. Zanella-Béguelin, “Analyzing information leakage of updates
    to natural language models,” 2020.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] O. Gupta and R. Raskar, “Distributed learning of deep neural network over
    multiple agents,” J. Netw. Comput. Appl., vol. 116, pp. 1–8, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Huang, P. Kairouz, X. Chen, L. Sankar, and R. Rajagopal, “Context-aware
    generative adversarial privacy,” CoRR, vol. abs/1710.09549, 2017.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Narayanan and V. Shmatikov, “Robust de-anonymization of large sparse
    datasets,” in 2008 IEEE Symposium on Security and Privacy (sp 2008), pp. 111–125,
    May 2008.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V.
    Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig, “Resolving individuals
    contributing trace amounts of dna to highly complex mixtures using high-density
    snp genotyping microarrays,” PLoS genetics, vol. 4, no. 8, 2008.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] C. C. Aggarwal, “On k-anonymity and the curse of dimensionality,” in Proceedings
    of the 31st International Conference on Very Large Data Bases, VLDB ’05, p. 901–909,
    VLDB Endowment, 2005.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam, “L-diversity:
    Privacy beyond k-anonymity,” ACM Trans. Knowl. Discov. Data, vol. 1, p. 3–es,
    Mar. 2007.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] N. Li, T. Li, and S. Venkatasubramanian, “t-closeness: Privacy beyond
    k-anonymity and l-diversity,” in 2007 IEEE 23rd International Conference on Data
    Engineering, pp. 106–115, IEEE, 2007.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, “Our data,
    ourselves: Privacy via distributed noise generation,” in Proceedings of the 24th
    Annual International Conference on The Theory and Applications of Cryptographic
    Techniques, EUROCRYPT’06, (Berlin, Heidelberg), pp. 486–503, Springer-Verlag,
    2006.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    robustness to adversarial examples with differential privacy,” 2019 IEEE Symposium
    on Security and Privacy (SP), pp. 656–672, 2018.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. Dwork, G. Rothblum, and S. Vadhan, “Boosting and differential privacy,”
    in Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science
    (FOCS ‘10), (Las Vegas, NV), p. 51–60, IEEE, IEEE, 23–26 October 2010.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Oh and P. Viswanath, “The composition theorem for differential privacy,”
    CoRR, vol. abs/1311.0776, 2013.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Dwork and A. Roth, “The algorithmic foundations of differential privacy,”
    Found. Trends Theor. Comput. Sci., vol. 9, pp. 211–407, Aug. 2014.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] P. Kairouz, S. Oh, and P. Viswanath, “Extremal mechanisms for local differential
    privacy,” CoRR, vol. abs/1407.1338, 2014.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Ú. Erlingsson, A. Korolova, and V. Pihur, “RAPPOR: randomized aggregatable
    privacy-preserving ordinal response,” CoRR, vol. abs/1407.6981, 2014.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Geumlek and K. Chaudhuri, “Profile-based privacy for locally private
    computations,” 2019 IEEE International Symposium on Information Theory (ISIT),
    pp. 537–541, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Gopi, P. Gulhane, J. Kulkarni, J. H. Shen, M. Shokouhi, and S. Yekhanin,
    “Differentially private set union,” ArXiv, vol. abs/2002.09745, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
    private recurrent language models,” arXiv preprint arXiv:1710.06963, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. Triastcyn and B. Faltings, “Generating differentially private datasets
    using gans,” CoRR, vol. abs/1803.03148, 2018.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Kifer and A. Machanavajjhala, “Pufferfish: A framework for mathematical
    privacy definitions,” ACM Trans. Database Syst., vol. 39, Jan. 2014.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. Wang, S. Song, and K. Chaudhuri, “Privacy-preserving analysis of correlated
    data,” CoRR, vol. abs/1603.03977, 2016.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi,
    “Geo-indistinguishability: Differential privacy for location-based systems,” in
    Proceedings of the 2013 ACM SIGSAC Conference on Computer and Communications Security,
    CCS ’13, (New York, NY, USA), p. 901–914, Association for Computing Machinery,
    2013.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Goldwasser and S. Micali, “Probabilistic encryption,” Journal of computer
    and system sciences, vol. 28, no. 2, pp. 270–299, 1984.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] K. Nissim and A. Wood, “Is privacy privacy?,” Philosophical Transactions
    of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 376,
    no. 2128, p. 20170358, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Liu, L. Xiong, and J. Luo, “Semantic security: Privacy definitions
    revisited.,” Trans. Data Privacy, vol. 6, no. 3, pp. 185–198, 2013.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Diaz, H. Wang, F. P. Calmon, and L. Sankar, “On the robustness of information-theoretic
    privacy measures and mechanisms,” CoRR, vol. abs/1811.06057, 2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Pinceti, O. Kosut, and L. Sankar, “Data-driven generation of synthetic
    load datasets preserving spatio-temporal features,” in 2019 IEEE Power Energy
    Society General Meeting (PESGM), pp. 1–5, Aug 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] P. Kairouz, J. Liao, C. Huang, and L. Sankar, “Censored and fair universal
    representations using generative adversarial models,” 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] D. P. Varodayan and A. Khisti, “Smart meter privacy using a rechargeable
    battery: Minimizing the rate of information leakage,” 2011 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1932–1935,
    2011.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. L. Hsu, S. Asoodeh, and F. du Pin Calmon, “Obfuscation via information
    density estimation,” ArXiv, vol. abs/1910.08109, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Li, J. Guo, H. Yang, and Y. Chen, “Deepobfuscator: Adversarial training
    framework for privacy-preserving image classification,” ArXiv, vol. abs/1909.04126,
    2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. Mirjalili, S. Raschka, and A. Ross, “Flowsan: Privacy-enhancing semi-adversarial
    networks to confound arbitrary face-based gender classifiers,” IEEE Access, vol. 7,
    pp. 99735–99745, 2019.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] P. C. Roy and V. N. Boddeti, “Mitigating information leakage in image
    representations: A maximum entropy approach,” 2019 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2581–2589, 2019.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Z. Wu, Z. Wang, Z. Wang, and H. Jin, “Towards privacy-preserving visual
    recognition via adversarial training: A pilot study,” ArXiv, vol. abs/1807.08379,
    2018.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C. Xu, J. Ren, D. Zhang, Y. Zhang, Z. Qin, and K. Ren, “Ganobfuscator:
    Mitigating information leakage under gan via differential privacy,” IEEE Transactions
    on Information Forensics and Security, vol. 14, pp. 2358–2371, 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
    private language models without losing accuracy,” CoRR, vol. abs/1710.06963, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] K. Chaudhuri and C. Monteleoni, “Privacy-preserving logistic regression,”
    in Advances in Neural Information Processing Systems 21 (D. Koller, D. Schuurmans,
    Y. Bengio, and L. Bottou, eds.), pp. 289–296, Curran Associates, Inc., 2009.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private
    empirical risk minimization,” J. Mach. Learn. Res., vol. 12, p. 1069–1109, July
    2011.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] L. Xie, K. Lin, S. Wang, F. Wang, and J. Zhou, “Differentially private
    generative adversarial network,” CoRR, vol. abs/1802.06739, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] G. Ács, L. Melis, C. Castelluccia, and E. D. Cristofaro, “Differentially
    private mixture of generative neural networks,” CoRR, vol. abs/1709.04514, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] N. Phan, Y. Wang, X. Wu, and D. Dou, “Differential privacy preservation
    for deep auto-encoders: An application of human behavior prediction,” in Proceedings
    of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, p. 1309–1316,
    AAAI Press, 2016.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] N. Phan, X. Wu, and D. Dou, “Preserving differential privacy in convolutional
    deep belief networks,” Mach. Learn., vol. 106, p. 1681–1704, Oct. 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] N. Phan, X. Wu, H. Hu, and D. Dou, “Adaptive laplace mechanism: Differential
    privacy preservation in deep learning,” CoRR, vol. abs/1709.05750, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and Úlfar
    Erlingsson, “Scalable private learning with pate,” 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Zhao, Y. Zhang, Q. Wang, Y. Chen, C. Wang, and Q. Zou, “Privacy-preserving
    collaborative deep learning with irregular participants,” CoRR, vol. abs/1812.10113,
    2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] B. K. Beaulieu-Jones, W. Yuan, S. G. Finlayson, and Z. S. Wu, “Privacy-preserving
    distributed deep learning for clinical data,” CoRR, vol. abs/1812.01484, 2018.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Yu, L. Liu, C. Pu, M. E. Gursoy, and S. Truex, “Differentially private
    model publishing for deep learning,” CoRR, vol. abs/1904.02200, 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
    learning: A client level perspective,” CoRR, vol. abs/1712.07557, 2017.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Bhowmick, J. C. Duchi, J. Freudiger, G. Kapoor, and R. Rogers, “Protection
    against reconstruction and its applications in private federated learning,” ArXiv,
    vol. abs/1812.00984, 2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Chase, R. Gilad-Bachrach, K. Laine, K. E. Lauter, and P. Rindal, “Private
    collaborative neural network learning,” IACR Cryptology ePrint Archive, vol. 2017,
    p. 762, 2017.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
    D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for privacy-preserving
    machine learning,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer
    and Communications Security, CCS ’17, (New York, NY, USA), p. 1175–1191, Association
    for Computing Machinery, 2017.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] N. Agrawal, A. S. Shamsabadi, M. J. Kusner, and A. Gascón, “QUOTIENT:
    two-party secure neural network training and prediction,” CoRR, vol. abs/1907.03372,
    2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Wagh, D. Gupta, and N. Chandran, “Securenn: 3-party secure computation
    for neural network training,” Proceedings on Privacy Enhancing Technologies, vol. 2019,
    pp. 26 – 49, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] P. Mohassel and P. Rindal, “Aby3: A mixed protocol framework for machine
    learning,” in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications
    Security, CCS ’18, (New York, NY, USA), p. 35–52, Association for Computing Machinery,
    2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] R. Rachuri and A. Suresh, “Trident: Efficient 4pc framework for privacy
    preserving machine learning,” IACR Cryptol. ePrint Arch., vol. 2019, p. 1315,
    2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-preserving
    machine learning,” in 2017 IEEE Symposium on Security and Privacy (SP), pp. 19–38,
    May 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacy-preserving
    deep learning via additively homomorphic encryption,” IEEE Transactions on Information
    Forensics and Security, vol. 13, pp. 1333–1345, May 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] T. Graepel, K. Lauter, and M. Naehrig, “Ml confidential: Machine learning
    on encrypted data,” in Information Security and Cryptology – ICISC 2012 (T. Kwon,
    M.-K. Lee, and D. Kwon, eds.), (Berlin, Heidelberg), pp. 1–21, Springer Berlin
    Heidelberg, 2013.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] L. J. M. Aslett, P. M. Esperança, and C. C. Holmes, “Encrypted statistical
    machine learning: new privacy preserving methods,” 2015.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Yonetani, V. N. Boddeti, K. M. Kitani, and Y. Sato, “Privacy-preserving
    visual learning using doubly permuted homomorphic encryption,” 2017 IEEE International
    Conference on Computer Vision (ICCV), pp. 2059–2069, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] C. Gentry, “Fully homomorphic encryption using ideal lattices,” in In
    Proc. STOC, pp. 169–178, 2009.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. S. Riazi, M. Samragh, H. Chen, K. Laine, K. Lauter, and F. Koushanfar,
    “Xonn: Xnor-based oblivious deep neural network inference,” in Proceedings of
    the 28th USENIX Conference on Security Symposium, SEC’19, (USA), p. 1501–1518,
    USENIX Association, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] E. Makri, D. Rotaru, N. P. Smart, and F. Vercauteren, “Epic: Efficient
    private image classification (or: Learning from the masters),” in CT-RSA, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. Iyengar, J. P. Near, D. Song, O. Thakkar, A. Thakurta, and L. Wang,
    “Towards practical differentially private convex optimization,” in 2019 IEEE Symposium
    on Security and Privacy (SP), pp. 299–316, IEEE, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Dong, A. Roth, and W. Su, “Gaussian differential privacy,” ArXiv,
    vol. abs/1905.02383, 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Bu, J. Dong, Q. Long, and W. Su, “Deep learning with gaussian differential
    privacy,” ArXiv, vol. abs/1911.11607, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] E. Bagdasaryan, O. Poursaeed, and V. Shmatikov, “Differential privacy
    has disparate impact on model accuracy,” in Advances in Neural Information Processing
    Systems, pp. 15479–15488, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] M. Jagielski, J. Ullman, and A. Oprea, “Auditing differentially private
    machine learning: How private is private sgd?,” ArXiv, vol. abs/2006.07709, 2020.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] B. Jayaraman and D. Evans, “Evaluating differentially private machine
    learning in practice,” in 28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
    Security 19), pp. 1895–1912, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] A. S. Shamsabadi, A. Gascón, H. Haddadi, and A. Cavallaro, “Privedge:
    From local to distributed private training and prediction,” ArXiv, vol. abs/2004.05574,
    2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, R. G. L. D’Oliveira, S. E. Rouayheb,
    D. Evans, J. Gardner, Z. A. Garrett, A. Gascón, B. Ghazi, P. B. Gibbons, M. Gruteser,
    Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi,
    G. Joshi, M. Khodak, J. Konecný, A. Korolova, F. Koushanfar, O. Koyejo, T. Lepoint,
    Y. Liu, P. Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh, M. Raykova, H. Qi, D. Ramage,
    R. Raskar, D. X. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tramèr,
    P. Vepakomma, J. Wang, L. Xiong, Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao,
    “Advances and open problems in federated learning,” ArXiv, vol. abs/1912.04977,
    2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. Wang, J. Zhang, W. Bao, X. Zhu, B. Cao, and P. S. Yu, “Not just privacy,”
    Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining, Jul 2018.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] N. Dowlin, R. Gilad-Bachrach, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing,
    “Cryptonets: Applying neural networks to encrypted data with high throughput and
    accuracy,” Tech. Rep. MSR-TR-2016-3, February 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] H. Chabanne, A. de Wargny, J. Milgram, C. Morel, and E. Prouff, “Privacy-preserving
    classification on deep neural network,” IACR Cryptology ePrint Archive, vol. 2017,
    p. 35, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. Sanyal, M. J. Kusner, A. Gascón, and V. Kanade, “TAPAS: tricks to
    accelerate (encrypted) prediction as a service,” CoRR, vol. abs/1806.03461, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] F. Bourse, M. Minelli, M. Minihold, and P. Paillier, “Fast homomorphic
    evaluation of deep discretized neural networks,” in CRYPTO, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] V. N. Boddeti, “Secure face matching using fully homomorphic encryption,”
    ArXiv, vol. abs/1805.00577, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B. Reagen, W. Choi, Y. Ko, V. Lee, G.-Y. Wei, H.-H. S. Lee, and D. Brooks,
    “Cheetah: Optimizing and accelerating homomorphic encryption for private inference,”
    2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] B. D. Rouhani, M. S. Riazi, and F. Koushanfar, “Deepsecure: Scalable
    provably-secure deep learning,” CoRR, vol. abs/1705.08963, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider,
    and F. Koushanfar, “Chameleon: A hybrid secure computation framework for machine
    learning applications,” CoRR, vol. abs/1801.03239, 2018.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] N. Kumar, M. Rathee, N. Chandran, D. Gupta, A. Rastogi, and R. Sharma,
    “Cryptflow: Secure tensorflow inference,” 2020 IEEE Symposium on Security and
    Privacy (SP), pp. 336–353, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] R. Bost, R. A. Popa, S. Tu, and S. Goldwasser, “Machine learning classification
    over encrypted data,” IACR Cryptology ePrint Archive, vol. 2014, p. 331, 2014.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Liu, M. Juuti, Y. Lu, and N. Asokan, “Oblivious neural network predictions
    via minionn transformations,” in Proceedings of the 2017 ACM SIGSAC Conference
    on Computer and Communications Security, CCS ’17, (New York, NY, USA), p. 619–631,
    Association for Computing Machinery, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan, “Gazelle: A low latency
    framework for secure neural network inference,” in Proceedings of the 27th USENIX
    Conference on Security Symposium, SEC’18, (USA), p. 1651–1668, USENIX Association,
    2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] P. Mishra, R. T. Lehmkuhl, A. Srinivasan, W. Zheng, and R. A. Popa, “Delphi:
    A cryptographic inference service for neural networks,” IACR Cryptology ePrint
    Archive, vol. 2020, p. 50, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M. Malekzadeh, R. G. Clegg, A. Cavallaro, and H. Haddadi, “Mobile sensor
    data anonymization,” in Proceedings of the International Conference on Internet
    of Things Design and Implementation, IoTDI ’19, (New York, NY, USA), p. 49–58,
    Association for Computing Machinery, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] N. Raval, A. Machanavajjhala, and J. Pan, “Olympus: sensor privacy through
    utility aware obfuscation,” Proceedings on Privacy Enhancing Technologies, vol. 2019,
    no. 1, pp. 5–25, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] F. Mireshghallah, M. Taram, A. Jalali, A. T. Elthakeb, D. M. Tullsen,
    and H. Esmaeilzadeh, “A principled approach to learning stochastic representations
    for privacy in deep neural inference,” ArXiv, vol. abs/2003.12154, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Malekzadeh, R. G. Clegg, A. Cavallaro, and H. Haddadi, “Protecting
    sensory data against sensitive inferences,” in Proceedings of the 1st Workshop
    on Privacy by Design in Distributed Systems, W-P2DS’18, (New York, NY, USA), Association
    for Computing Machinery, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. Malekzadeh, R. G. Clegg, A. Cavallaro, and H. Haddadi, “Privacy and
    utility preserving sensor-data transformations,” Pervasive and Mobile Computing,
    p. 101132, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. A. Osia, A. S. Shamsabadi, S. Sajadmanesh, A. Taheri, K. Katevas,
    H. R. Rabiee, N. D. Lane, and H. Haddadi, “A hybrid deep learning architecture
    for privacy-preserving mobile analytics,” IEEE Internet of Things Journal, pp. 1–1,
    2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] B. Hitaj, G. Ateniese, and F. Pérez-Cruz, “Deep models under the gan:
    Information leakage from collaborative deep learning,” in CCS ’17, 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] A. Cheu, A. D. Smith, J. Ullman, D. Zeber, and M. Zhilyaev, “Distributed
    differential privacy via shuffling,” IACR Cryptology ePrint Archive, vol. 2019,
    p. 245, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] B. Balle, J. Bell, A. Gascón, and K. Nissim, “The privacy blanket of
    the shuffle model,” in CRYPTO, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] B. Ghazi, R. Pagh, and A. Velingker, “Scalable and differentially private
    distributed aggregation in the shuffled model,” ArXiv, vol. abs/1906.08320, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar, “Split learning for
    health: Distributed deep learning without sharing raw patient data,” ArXiv, vol. abs/1812.00564,
    2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] F. Mo, A. S. Shamsabadi, K. Katevas, S. Demetriou, I. Leontiadis, A. Cavallaro,
    and H. Haddadi, “Darknetz: Towards model privacy at the edge using trusted execution
    environments,” ArXiv, vol. abs/2004.05703, 2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] V. Costan and S. Devadas, “Intel sgx explained,” IACR Cryptology ePrint
    Archive, vol. 2016, p. 86, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] K. G. Narra, Z. Lin, Y. Wang, K. Balasubramaniam, and M. Annavaram, “Privacy-preserving
    inference in machine learning services using trusted execution environments,”
    ArXiv, vol. abs/1912.03485, 2019.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] H. Hashemi, Y. Wang, and M. Annavaram, “Darknight: A data privacy scheme
    for training and inference of deep neural networks,” ArXiv, vol. abs/2006.01300,
    2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] F. Tramer and D. Boneh, “Slalom: Fast, verifiable and private execution
    of neural networks in trusted hardware,” in International Conference on Learning
    Representations, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. inc., “Arm trustzone technology.” online accessed February 2020 [https://developer.arm.com/ip-products/security-ip/trustzone](https://developer.arm.com/ip-products/security-ip/trustzone).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] O. Weisse, J. Van Bulck, M. Minkin, D. Genkin, B. Kasikci, F. Piessens,
    M. Silberstein, R. Strackx, T. F. Wenisch, and Y. Yarom, “Foreshadow-NG: Breaking
    the virtual memory abstraction with transient out-of-order execution,” Technical
    report, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] C. Canella, D. Genkin, L. Giner, D. Gruss, M. Lipp, M. Minkin, D. Moghimi,
    F. Piessens, M. Schwarz, B. Sunar, J. Van Bulck, and Y. Yarom, “Fallout: Leaking
    data on meltdown-resistant cpus,” in Proceedings of the ACM SIGSAC Conference
    on Computer and Communications Security (CCS), ACM, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Taram, A. Venkat, and D. M. Tullsen, “Packet chasing: Spying on network
    packets over a cache side-channel,” ArXiv, vol. abs/1909.04841, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] M. Taram, A. Venkat, and D. Tullsen, “Context-sensitive fencing: Securing
    speculative execution via microcode customization,” in Proceedings of the Twenty-Fourth
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, ASPLOS ’19, (New York, NY, USA), p. 395–410, Association for
    Computing Machinery, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
