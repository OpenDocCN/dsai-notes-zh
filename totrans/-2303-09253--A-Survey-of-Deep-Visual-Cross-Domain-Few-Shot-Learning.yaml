- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:40:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.09253] A Survey of Deep Visual Cross-Domain Few-Shot Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.09253] 深度视觉跨领域少样本学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.09253](https://ar5iv.labs.arxiv.org/html/2303.09253)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.09253](https://ar5iv.labs.arxiv.org/html/2303.09253)
- en: '[orcid=0000-0002-3941-8952]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0002-3941-8952]'
- en: \cormark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \cormark
- en: '[1]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: \cormark
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \cormark
- en: '[1]'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: \credit
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Conceptualization of this study, Methodology, Software
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的概念化、方法论、软件
- en: \credit
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \credit
- en: Data curation, Writing - Original draft preparation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据策划，撰写 - 原稿准备
- en: A Survey of Deep Visual Cross-Domain Few-Shot Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度视觉跨领域少样本学习调查
- en: Wenjian Wang wangwj@emails.bjut.edu.cn    Lijuan Duan ljduan@bjut.edu.cn   
    Yuxi Wang yuxiwang93@gmail.com    Junsong Fan junsong.fan@ia.ac.cn    Zhi Gong
    gongzhi97@emails.bjut.edu.cn    Zhaoxiang Zhang zhaoxiang.zhang@ia.ac.cn Faculty
    of Information Technology, Beijing University of Technology, China Beijing Key
    Laboratory of Trusted Computing, Beijing, China National Engineering Laboratory
    for Key Technologies of Information Security Level Protection, Beijing, China
    Centre for Artificial Intelligence and Robotics, (HKISI_CAS) Institute of Automation,
    Chinese Academy of Sciences (NLPR, CASIA, UCAS)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Wenjian Wang wangwj@emails.bjut.edu.cn    Lijuan Duan ljduan@bjut.edu.cn   
    Yuxi Wang yuxiwang93@gmail.com    Junsong Fan junsong.fan@ia.ac.cn    Zhi Gong
    gongzhi97@emails.bjut.edu.cn    Zhaoxiang Zhang zhaoxiang.zhang@ia.ac.cn 北京工业大学信息技术学院，中国
    北京市可信计算重点实验室，中国 北京市信息安全等级保护关键技术国家工程实验室，中国 人工智能与机器人中心，（HKISI_CAS）自动化研究所，中国科学院（NLPR,
    CASIA, UCAS）
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Few-Shot transfer learning has become a major focus of research as it allows
    recognition of new classes with limited labeled data. While it is assumed that
    train and test data have the same data distribution, this is often not the case
    in real-world applications. This leads to decreased model transfer effects when
    the new class distribution differs significantly from the learned classes. Research
    into Cross-Domain Few-Shot (CDFS) has emerged to address this issue, forming a
    more challenging and realistic setting. In this survey, we provide a detailed
    taxonomy of CDFS from the problem setting and corresponding solutions view. We
    summarise the existing CDFS network architectures and discuss the solution ideas
    for each direction the taxonomy indicates. Furthermore, we introduce various CDFS
    downstream applications and outline classification, detection, and segmentation
    benchmarks and corresponding standards for evaluation. We also discuss the challenges
    of CDFS research and explore potential directions for future investigation. Through
    this review, we aim to provide comprehensive guidance on CDFS research, enabling
    researchers to gain insight into the state-of-the-art while allowing them to build
    upon existing solutions to develop their own CDFS models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本迁移学习已经成为研究的主要焦点，因为它允许在有限的标记数据下识别新类别。虽然假设训练数据和测试数据具有相同的数据分布，但在实际应用中情况常常并非如此。这导致当新类别的分布与学习的类别显著不同时，模型迁移效果降低。为解决这一问题，跨领域少样本（CDFS）的研究应运而生，形成了一个更具挑战性和现实性的环境。在本调查中，我们从问题设置和相应解决方案的视角提供了CDFS的详细分类。我们总结了现有的CDFS网络架构，并讨论了分类中每个方向的解决思路。此外，我们介绍了各种CDFS下游应用，并概述了分类、检测和分割基准以及相应的评估标准。我们还讨论了CDFS研究的挑战，并探索了未来研究的潜在方向。通过这次审查，我们旨在为CDFS研究提供全面的指导，帮助研究人员深入了解最新技术，同时允许他们在现有解决方案的基础上开发自己的CDFS模型。
- en: 'keywords:'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Few-Shot Learning \sepCross-Domain \sepSurvey
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习 \sep 跨领域 \sep 调查
- en: 1 INTRODUCTION
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Strong computing power has boosted the development of deep learning, thereby
    aiding computer vision in making considerable progress. Research fields such as
    image classification, object detection, and semantic segmentation continue to
    shape the development of computer vision in innovative ways. Current deep models
    require a large volume of annotated data for training, but this data is typically
    expensive and labor-intensive. Few labeled data are in certain fields to enable
    the model to recognize new categories, triggering the need for few-shot learning
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. Although few-shot learning assumes
    that the training and testing data come from the same domain, domain shift is
    commonly seen in real-world scenarios [[4](#bib.bib4)]. In these contexts, cross-domain
    few-shot learning (CDFS) offers a promising solution to the few-shot learning
    problem by simultaneously addressing both domain shift and data scarcity [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的计算能力促进了深度学习的发展，从而帮助计算机视觉取得了显著进展。图像分类、目标检测和语义分割等研究领域继续以创新的方式塑造计算机视觉的发展。当前的深度模型需要大量标注数据进行训练，但这些数据通常价格昂贵且劳动密集。在某些领域，少量标注数据使模型能够识别新类别，从而引发了少样本学习的需求[[1](#bib.bib1)、[2](#bib.bib2)、[3](#bib.bib3)]。尽管少样本学习假设训练和测试数据来自同一领域，但在现实世界中，领域转移是常见的现象[[4](#bib.bib4)]。在这些背景下，跨领域少样本学习（CDFS）通过同时解决领域转移和数据稀缺问题，为少样本学习提供了有前景的解决方案[[5](#bib.bib5)、[6](#bib.bib6)、[7](#bib.bib7)]。
- en: 'This paper reviews and categorizes the current research in CDFS based on problem
    sets, solutions, and applications. Regarding problem setting, we identify two
    main models for CDFS research with multiple data sources: single- and multiple-model
    (where the latter involves training a model for each source and subsequently aggregating
    them). For single-source modeling, sub-categorizations are achieved based on whether
    the target domain data is accessible (through supervised/unsupervised means) or
    forbidden. We also comprehensively introduce the popular CDFS classification benchmark
    datasets and highlight the unified transfer effects across datasets.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于问题集、解决方案和应用对当前CDFS研究进行了回顾和分类。在问题设置方面，我们识别了两种主要的CDFS研究模型，涉及多个数据源：单模型和多模型（后者涉及为每个数据源训练一个模型并随后进行聚合）。对于单数据源建模，根据目标领域数据是否可获取（通过监督/无监督手段）或禁止，进行了子分类。我们还全面介绍了流行的CDFS分类基准数据集，并突出了数据集之间的统一迁移效果。
- en: 'Regarding solution, we cover four main approaches: Image Augmentation, Feature
    Augmentation, Decoupling, and Fine-tuning. Image and Feature Augmentation rely
    on the mutual image/feature transformation to enrich data distribution. At the
    same time, Decoupling Based Methods distinguish domain-irrelevant from domain-specific
    features, and Fine-tuning uses meta-learning and distribution alignment to extract
    transferable features. We further discuss its application to various fields and
    identify potential future research directions.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于解决方案，我们涵盖了四种主要方法：图像增强、特征增强、解耦和微调。图像增强和特征增强依赖于图像/特征的相互转换来丰富数据分布。同时，基于解耦的方法区分领域无关特征和领域特定特征，而微调则通过元学习和分布对齐来提取可迁移特征。我们进一步讨论了这些方法在各个领域的应用，并确定了潜在的未来研究方向。
- en: 'The contributions of this article are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献总结如下：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We summarized the cross-domain research in few-shot learning. The existing solutions
    are summarized in detail according to the different problem settings.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了少样本学习中的跨领域研究。根据不同的问题设置，详细总结了现有的解决方案。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For different CDFS solutions, we give a detailed sorting and classification
    from features, images, and network architecture. We also summarize the application
    of CDFS in different scenarios.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于不同的CDFS解决方案，我们从特征、图像和网络架构等方面进行了详细的整理和分类。我们还总结了CDFS在不同场景中的应用。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We gave the benchmark of the current CDFS, providing a unified reference precision
    for subsequent research. We provide the future direction of CDFS in terms of challenges,
    technical solutions, and applications.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了当前CDFS的基准，为后续研究提供了统一的参考精度。我们提供了CDFS在挑战、技术解决方案和应用方面的未来方向。
- en: '![Refer to caption](img/765d668f3fa1d9226c7342cdf0b0805a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/765d668f3fa1d9226c7342cdf0b0805a.png)'
- en: 'Figure 1: The related researches on few-shot learning increased from 2017 to
    2022.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：2017年至2022年间少样本学习相关研究的增加情况。
- en: 2 CONCEPTS AND PRELIMINARIES
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 概念与初步研究
- en: 2.1 Few-Shot Learning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 少样本学习
- en: Few-Shot Learning (FSL) is a transfer learning technique designed to learn novel
    classes from limited annotated labels [[8](#bib.bib8)]. In FSL, two assumptions
    must be met to ensure its successful application. Firstly, the classes between
    the train and finetune process must be distinct; there must be no class intersection
    between the two processes, i.e., $C_{b}\cap C_{n}=\varnothing$. Secondly, the
    annotation labels for each class in the finetune process must be limited (or even
    only have one annotation label). Specifically, there are two datasets $D_{b}$
    and $D_{n}$, where the classes $C_{b}\in D_{b}$ have sufficient labels and $C_{n}\in
    D_{n}$ only have limited labels. The FSL model aims to train on dataset $D_{b}$,
    then apply a few annotation labels to finetune novel classes of dataset $D_{n}$
    during the test process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习（FSL）是一种迁移学习技术，旨在从有限的标注标签中学习新类别[[8](#bib.bib8)]。在FSL中，必须满足两个假设以确保其成功应用。首先，训练和微调过程中的类别必须是不同的；这两个过程之间不得有类别交集，即$C_{b}\cap
    C_{n}=\varnothing$。其次，微调过程中每个类别的标注标签必须有限（甚至可能只有一个标注标签）。具体来说，存在两个数据集$D_{b}$和$D_{n}$，其中$D_{b}$中的类别$C_{b}$有足够的标签，而$D_{n}$中的类别$C_{n}$只有有限的标签。FSL模型旨在对数据集$D_{b}$进行训练，然后在测试过程中应用少量标注标签来微调数据集$D_{n}$中的新类别。
- en: 2.2 Few-Shot Domain Adaptation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 少样本领域适应
- en: 'Few-Shot Domain Adaptation (FSDA) is a powerful combination of Few-Shot and
    Domain Adaptation, making Few-Shot learning more difficult. Unlike standard Few-Shot
    Learning, which assumes a shared data distribution between the training dataset
    $D_{b}$ and finetuning dataset $D_{n}$, FSDA identifies a domain gap between the
    two datasets. Two kinds of FSDA: the supervised kind, explored in [[9](#bib.bib9)],
    and the unsupervised kind, explored in [[10](#bib.bib10)]. This research seeks
    to bridge the difference between the two domains and fully use the limited annotations
    available. The goal of FSDA is thus to align two data domains while utilizing
    the limited annotation data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本领域适应（FSDA）是少样本学习和领域适应的强大结合，使得少样本学习更加困难。与标准的少样本学习不同，标准的少样本学习假设训练数据集$D_{b}$和微调数据集$D_{n}$之间具有共享的数据分布，而FSDA识别了两个数据集之间的领域差距。FSDA有两种类型：监督型，探讨于[[9](#bib.bib9)]，和非监督型，探讨于[[10](#bib.bib10)]。这项研究旨在弥合两个领域之间的差异，并充分利用有限的标注数据。因此，FSDA的目标是对齐两个数据领域，同时利用有限的标注数据。
- en: 2.3 Cross-Domain Few-Shot
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 跨领域少样本学习
- en: Cross-domain few-shot (CDFS) learning, a recently emerged field in few-shot
    learning, assumes there is a significant domain gap between source and target
    domains, making the task more challenging compared to the traditional few-shot
    domain adaptation (FSDA) approach [[5](#bib.bib5)]. As shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 INTRODUCTION ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"),
    the interest in CDFS research has grown steadily from 2020 to 2022\. The following
    sections provide a detailed overview of the problem sets, solutions, and applications
    of CDFS. Section [3](#S3 "3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") summarizes the problem divisions of CDFS, Section
    [4](#S4 "4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning") reviews the different solutions proposed for CDFS, Section [5](#S5
    "5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") provides
    the benchmark, Section [6](#S6 "6 Application OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") provides various applications of CDFS, and Section
    [7](#S7 "7 FUTURE DIRECTION ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")
    outlines directions for future research.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域少样本（CDFS）学习是少样本学习中一个新兴的领域，假设源领域和目标领域之间存在显著的领域差距，使得任务相比于传统的少样本领域适应（FSDA）方法更具挑战性[[5](#bib.bib5)]。如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 深度视觉跨领域少样本学习综述")所示，CDFS研究的兴趣从2020年到2022年稳步增长。接下来的章节提供了CDFS问题集、解决方案和应用的详细概述。第[3](#S3
    "3 CDFS的不同设置 ‣ 深度视觉跨领域少样本学习综述")节总结了CDFS的问题分类，第[4](#S4 "4 CDFS的不同解决方案 ‣ 深度视觉跨领域少样本学习综述")节回顾了为CDFS提出的不同解决方案，第[5](#S5
    "5 基准 ‣ 深度视觉跨领域少样本学习综述")节提供了基准，第[6](#S6 "6 CDFS的应用 ‣ 深度视觉跨领域少样本学习综述")节提供了CDFS的各种应用，第[7](#S7
    "7 未来方向 ‣ 深度视觉跨领域少样本学习综述")节概述了未来研究的方向。
- en: 2.4 Domain Adaptation & Generalization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 领域适应与泛化
- en: Domain Adaptation (DA) requires a model to be trained on a source domain and
    tested on a target domain with different data distribution. This can be done in
    a supervised [[11](#bib.bib11), [12](#bib.bib12)] or unsupervised manner [[13](#bib.bib13),
    [14](#bib.bib14)], assuming the same classes are present in both domains. DA focuses
    on aligning the distributions of two domains [[15](#bib.bib15)]. In an unsupervised
    setting, the source model trains the feature extractor using the data of the target
    domain, thus accomplishing domain alignment. In contrast, Domain Generalization
    (DG) is more complex and valuable than DA [[16](#bib.bib16)], as the target domain
    data is not accessible during training [[17](#bib.bib17)]. After completing source
    domain training, the model must be tested directly on the target domain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 域适应（DA）要求模型在源域上训练，并在数据分布不同的目标域上测试。这可以在有监督[[11](#bib.bib11), [12](#bib.bib12)]或无监督[[13](#bib.bib13),
    [14](#bib.bib14)]的方式下进行，假设两个领域中都有相同的类别。DA着重于对齐两个领域的分布[[15](#bib.bib15)]。在无监督环境下，源模型使用目标领域的数据训练特征提取器，从而完成领域对齐。相比之下，领域泛化（DG）比DA更复杂且更有价值[[16](#bib.bib16)]，因为在训练期间无法访问目标领域数据[[17](#bib.bib17)]。在完成源领域训练后，模型必须直接在目标领域进行测试。
- en: 2.5 Meta-Learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 元学习
- en: Humans constantly summarize experiences during the learning process. When faced
    with a novel environment, humans can draw upon acquired knowledge to adapt quickly
    to new tasks. Meta-Learning, or "learning to learn," is an approach that collects
    experiences across various tasks to provide valuable information for transfer
    learning [[18](#bib.bib18)]. As a form of meta-learning, task learning encompasses
    various applications, including classification problems, regression, and mixed
    tasks [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]. Meta-learning aims
    to distill knowledge, which can be manifested in various network architectures.
    For example, Reptile [[22](#bib.bib22)], a model-agnostic meta-learning algorithm,
    utilizes the prediction of model parameters as its knowledge representation. Alternatively,
    DMML [[23](#bib.bib23)] and OFMA [[24](#bib.bib24)] encode knowledge via an embedding
    function and an initialization parameter, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在学习过程中不断总结经验。当面对新的环境时，人类可以利用已有的知识迅速适应新任务。元学习，即“学习如何学习”，是一种通过收集各种任务的经验来为迁移学习提供有价值信息的方法[[18](#bib.bib18)]。作为元学习的一种形式，任务学习涵盖了各种应用，包括分类问题、回归和混合任务[[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)]。元学习旨在提炼知识，这可以通过各种网络架构体现。例如，Reptile [[22](#bib.bib22)]，一种模型无关的元学习算法，利用模型参数的预测作为其知识表示。另一种选择是DMML
    [[23](#bib.bib23)]和OFMA [[24](#bib.bib24)]，它们分别通过嵌入函数和初始化参数来编码知识。
- en: 'Table 1: The different related researches. DA and DG transfer source knowledge
    to target domain, but DA have the accessible of target data. FSL assume that only
    have limited fine-tuning data and have no class intersection between training
    and testing data. CDFS and FSDA consider the few-shot learning and domain shift
    together, but FSDA have the target domain accessible and have the same domain
    class.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 各种相关研究。DA 和 DG 将源知识迁移到目标领域，但 DA 可以访问目标数据。FSL 假设只有有限的微调数据，并且训练和测试数据之间没有类别交集。CDFS
    和 FSDA 同时考虑了少样本学习和领域转移，但 FSDA 可以访问目标领域并且具有相同的领域类别。'
- en: '| Research | Training Domain | Test Domain | Domain Class Intersection | Fine-tuning
    Size | Test Data Training |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 研究 | 训练领域 | 测试领域 | 领域类别交集 | 微调大小 | 测试数据训练 |'
- en: '| Cross-Domain Few-Shot | $D_{s}$ | $D_{t}$ | No | Few | Forbidden |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域少样本学习 | $D_{s}$ | $D_{t}$ | 无 | 少 | 禁止 |'
- en: '| Few-Shot Domain Adaptation | $D_{s}$ | $D_{t}$ | Same | Few | Accessible
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 少样本领域适应 | $D_{s}$ | $D_{t}$ | 相同 | 少 | 可访问 |'
- en: '| Few-Shot Learning | $D_{s}$ | $D_{s}$ | No | Few | Inexistent |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 少样本学习 | $D_{s}$ | $D_{s}$ | 无 | 少 | 不存在 |'
- en: '| Domain Adaptation | $D_{s}$ | $D_{t}$ | Same | Sufficient | Accessible |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 领域适应 | $D_{s}$ | $D_{t}$ | 相同 | 足够 | 可访问 |'
- en: '| Domain Generalization | $D_{s}$ | $D_{t}$ | Same | Inexistent | Forbidden
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 领域泛化 | $D_{s}$ | $D_{t}$ | 相同 | 不存在 | 禁止 |'
- en: <svg   height="670.42" overflow="visible" version="1.1" width="937.78"><g transform="translate(0,670.42)
    matrix(1 0 0 -1 0 0) translate(404.74,0) translate(0,538.92)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -399.85 -273.63)" fill="#000000"
    stroke="#000000"><foreignobject width="12.3" height="114.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Few-Shot Learning</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 -359.14 -132.18)" fill="#000000" stroke="#000000"><foreignobject width="9.61"
    height="146.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Cross-Domain
    Few-Shot</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -360.48 -484.6)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="181.8" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Few-Shot Domain-Adaptation</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 30.87)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="95.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Source</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -310.07 -99.43)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="80.75" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Single Source</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 -310.07 -230.98)" fill="#000000" stroke="#000000"><foreignobject width="9.61"
    height="68.26" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Benchmark</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 -368.64)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="107.35" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised
    FSDA</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -310.07 -534.03)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="123.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Unsupervised FSDA</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -243.27 114.65)" fill="#000000" stroke="#000000"><foreignobject width="93.4"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Model</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -238.51 35.91)"
    fill="#000000" stroke="#000000"><foreignobject width="83.48" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Single Models</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -270.7 -23.14)" fill="#000000" stroke="#000000"><foreignobject width="157.59"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Target Domain
    Accessible</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -270.7 -101.88)"
    fill="#000000" stroke="#000000"><foreignobject width="158.43" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Target Domain Forbidden</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -183.01)" fill="#000000" stroke="#000000"><foreignobject
    width="602.28" height="251.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSCD-FSL
    [[6](#bib.bib6)],Meta-Dataset [[25](#bib.bib25)],PATNet [[26](#bib.bib26)],MoF-SOD
    [[27](#bib.bib27)],U-CDFSL [[28](#bib.bib28)], H-CDFSL [[29](#bib.bib29)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -93.54 116.73)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)], CosML [[31](#bib.bib31)],URL [[32](#bib.bib32)] <g fill="#FFFFB3"><path
    d="M -98.15 10.54 h 343.87 v 57.65 h -343.87 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -93.54 53.9)" fill="#000000" stroke="#000000"><foreignobject width="334.65"
    height="48.43" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MDLCC [[33](#bib.bib33)],LFT
    [[7](#bib.bib7)],TSA [[34](#bib.bib34)],CDNet [[35](#bib.bib35)],ISS [[36](#bib.bib36)],DAML
    [[37](#bib.bib37)]</foreignobject></g> <g fill="#FFFFB3"><path d="M -98.15 -56.81
    h 343.87 v 74.26 h -343.87 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    -93.54 3.15)" fill="#000000" stroke="#000000"><foreignobject width="334.65" height="65.03"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)],
    DDN [[39](#bib.bib39)],DAFL [[40](#bib.bib40)],OA-FSUI2IT [[41](#bib.bib41)],
    ME-D2N [[42](#bib.bib42)],TGDM [[43](#bib.bib43)],AcroFOD [[44](#bib.bib44)],CDSC-FSL
    [[45](#bib.bib45)],WULD [[46](#bib.bib46)],CutMix [[47](#bib.bib47)]</foreignobject></g>
    <g fill="#FFFFB3"><path d="M -98.15 -160.46 h 343.87 v 124.07 h -343.87 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.54 -50.69)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="114.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CMT
    [[48](#bib.bib48)], ProtoTransfer [[49](#bib.bib49)],RMP [[50](#bib.bib50)],NSA
    [[51](#bib.bib51)],ATA [[52](#bib.bib52)],DCM+SS [[53](#bib.bib53)],TL-SS [[54](#bib.bib54)],RDC
    [[55](#bib.bib55)],RD [[5](#bib.bib5)],AFA [[56](#bib.bib56)],LCCS [[57](#bib.bib57)],MemREIN
    [[58](#bib.bib58)],Drop-CDFS [[59](#bib.bib59)],TMH [[60](#bib.bib60)],FTE [[61](#bib.bib61)],
    SB-MTL [[62](#bib.bib62)],STCDFS [[63](#bib.bib63)],Wave-SAN [[64](#bib.bib64)],ReFine
    [[65](#bib.bib65)]</foreignobject></g> <g stroke-width="0.8pt"><path d="M -117.56
    118.11 L -99.41 118.11" style="fill:none"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -99.41 118.11)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -117.56 39.37 L -99.41 39.37" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 39.37)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -108.23 -19.69 L -99.41 -19.69" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 -19.69)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -108.53 -98.43 L -99.41 -98.43" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 -98.43)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    fill="#FFFFB3"><path d="M -275.31 -327.87 h 808.08 v 25.83 h -808.08 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -317.04)" fill="#000000" stroke="#000000"><foreignobject
    width="798.86" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FSADA
    [[9](#bib.bib9)],APT [[66](#bib.bib66)],PixDA [[67](#bib.bib67)]</foreignobject></g><g
    fill="#FFFFB3"><path d="M -275.31 -492.97 h 791.86 v 41.05 h -791.86 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -466.91)" fill="#000000" stroke="#000000"><foreignobject
    width="783.02" height="123.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FUDA
    [[10](#bib.bib10)],IMSE [[68](#bib.bib68)],S3T [[69](#bib.bib69)],OUCD [[70](#bib.bib70)],MixUp-OT
    [[71](#bib.bib71)]</foreignobject></g><g stroke-width="0.8pt"><path d="M -292.88
    -314.96 L -276.57 -314.96" style="fill:none"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -276.57 -314.96)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -292.88 -472.44 L -276.57 -472.44" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -276.57 -472.44)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="670.42" overflow="visible" version="1.1" width="937.78"><g transform="translate(0,670.42)
    matrix(1 0 0 -1 0 0) translate(404.74,0) translate(0,538.92)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -399.85 -273.63)" fill="#000000"
    stroke="#000000"><foreignobject width="12.3" height="114.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">少样本学习</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 -359.14 -132.18)" fill="#000000" stroke="#000000"><foreignobject width="9.61"
    height="146.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">跨领域少样本</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -360.48 -484.6)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="181.8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">少样本领域自适应</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 30.87)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="95.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">多源</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 -99.43)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="80.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">单源</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 -230.98)" fill="#000000" stroke="#000000"><foreignobject
    width="9.61" height="68.26" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基准</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 -368.64)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="107.35" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">监督下的
    FSDA</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -310.07 -534.03)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="123.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">无监督 FSDA</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -243.27 114.65)" fill="#000000" stroke="#000000"><foreignobject width="93.4"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">多模型</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -238.51 35.91)" fill="#000000" stroke="#000000"><foreignobject
    width="83.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">单模型</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -23.14)" fill="#000000" stroke="#000000"><foreignobject
    width="157.59" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">目标领域可访问</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -101.88)" fill="#000000" stroke="#000000"><foreignobject
    width="158.43" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">目标领域禁用</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -183.01)" fill="#000000" stroke="#000000"><foreignobject
    width="602.28" height="251.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSCD-FSL
    [[6](#bib.bib6)],Meta-Dataset [[25](#bib.bib25)],PATNet [[26](#bib.bib26)],MoF-SOD
    [[27](#bib.bib27)],U-CDFSL [[28](#bib.bib28)], H-CDFSL [[29](#bib.bib29)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -93.54 116.73)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)], CosML [[31](#bib.bib31)],URL [[32](#bib.bib32)] <g fill="#FFFFB3"><path
    d="M -98.15 10.54 h 343.87 v 57.65 h -343.87 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -93.54 53.9)" fill="#000000" stroke="#000000"><foreignobject width="334.65"
    height="48.43" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MDLCC [[33](#bib.bib33)],LFT
    [[7](#bib.bib7)],TSA [[34](#bib.bib34)],CDNet [[35](#bib.bib35)],ISS [[36](#bib.bib36)],DAML
    [[37](#bib.bib37)]</foreignobject></g> <g fill="#FFFFB3"><path d="M -98.15 -56.81
    h 343.87 v 74.26 h -343.87 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    -93.54 3.15)" fill="#000000" stroke="#000000"><foreignobject width="334.65" height="65.03"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)],
    DDN [[39](#bib.bib39)],DAFL [[40](#bib.bib40)],OA-FSUI2IT [[41](#bib.bib41)],
    ME-D2N [[42](#bib.bib42)],TGDM [[43](#bib.bib43)],AcroFOD [[44](#bib.bib44)],CDSC-FSL
    [[45](#bib.bib45)],WULD [[46](#bib.bib46)],CutMix [[47](#bib.bib47)]</foreignobject></g>
    <g fill="#FFFFB3"><path d="M -98.15 -160.46 h 343.87 v 124.07 h -343.87 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.54 -50.69)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="114.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CMT
    [[48](#bib.bib48)], ProtoTransfer [[49](#bib.bib49)],RMP [[50](#bib.bib50)],NSA
    [[51](#bib.bib51)],ATA [[52](#bib.bib52)],DCM+SS [[53](#bib.bib53)],TL-SS [[54](#bib.bib54)],RDC
    [[55](#bib.bib55)],RD [[5](#bib.bib5)],AFA [[56](#bib.bib56)],LCCS [[57](#
- en: 'Figure 2: Taxonomy of CDFS. For domain researches of CDFS, we split the Cross-Domain
    Few-Shot into Multiple Source and Single Source setting. For Multiple Source,
    we divide the research into Multiple Models and Single Model according to the
    models used. For Single Source, we divide the research into Target Domain Accessible
    and Target Domain Forbidden according to whether the target domain can access.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: CDFS 的分类。对于 CDFS 的领域研究，我们将跨域少样本分为多源和单源设置。对于多源，我们根据使用的模型将研究分为多模型和单模型。对于单源，我们根据目标领域是否可访问将研究分为目标领域可访问和目标领域禁止。'
- en: 3 DIFFERENT SETTING OF CDFS
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CDFS 的不同设置
- en: '![Refer to caption](img/2ba12d4d353eb38e0d85a2597eac60f2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2ba12d4d353eb38e0d85a2597eac60f2.png)'
- en: 'Figure 3: Different setting of CDFS. For the CDFS research, there are Multiple
    Source and Single Source setting.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: CDFS 的不同设置。对于 CDFS 研究，存在多源和单源设置。'
- en: 'We group recent CDFS research into three categories: Multiple Source, Single
    Source, and Benchmark. Multiple Source research includes two solutions: multiple
    models, and a single model. Single Source can be divided into two subcategories,
    depending on whether the target domain is accessible or forbidden, as depicted
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"). We have also provided a summary of benchmarks
    for CDFS in various research fields.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最近的 CDFS 研究分为三类：多源、多模型和基准。多源研究包括两个解决方案：多个模型和单个模型。单源可以分为两种子类别，取决于目标领域是否可访问，如图
    [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning") 所示。我们还提供了在各个研究领域中 CDFS 基准的总结。
- en: 3.1 Multiple Source CDFS
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多源 CDFS
- en: Different domains can provide essential data distributions for deep model learning,
    making it intuitive to collect multiple source domain data to address the CDFS
    problem. Specifically, researchers first collect several source domain datasets
    with disjoint classes. They then employ specific training strategies to capture
    domain information and combine the results into a final transfer model. Typically,
    these domains span various scenarios, such as CropDisease [[72](#bib.bib72)],
    EuroSAT [[73](#bib.bib73)], and ChestX [[74](#bib.bib74)]. The learning strategies
    of Multiple Source CDFS can be divided into Multiple-Model and Single-Model approaches,
    as depicted in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不同领域可以为深度模型学习提供重要的数据分布，这使得收集多个源领域的数据以解决 CDFS 问题变得直观。具体来说，研究人员首先收集几个具有不重叠类别的源领域数据集。然后，他们采用特定的训练策略来捕捉领域信息，并将结果结合成最终的迁移模型。通常，这些领域涵盖了各种场景，例如
    CropDisease [[72](#bib.bib72)]、EuroSAT [[73](#bib.bib73)] 和 ChestX [[74](#bib.bib74)]。多源
    CDFS 的学习策略可以分为多模型和单模型方法，如图 [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") 所示。
- en: 3.1.1 Multiple-Model For Multiple Source
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 多模型的多源研究
- en: The domain will have its specific model for the Multiple-Model Methods. The
    learning process contains the training and aggregation stage. Each domain optimizes
    the domain-specific model in the training stage, then combines all the models
    into a single model, which will serve as the final transfer model in the aggregation
    stage, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A
    Survey of Deep Visual Cross-Domain Few-Shot Learning") left part. The training
    stage gives the model diverse feature distribution or highly adaptable parameters.
    The subsequent aggregation stage focuses on feature reuse, which aims to achieve
    the best transfer effect.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多模型方法，领域将有其特定的模型。学习过程包括训练阶段和聚合阶段。每个领域在训练阶段优化领域特定模型，然后将所有模型合并为一个模型，该模型将在聚合阶段作为最终的迁移模型，如图
    [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning") 左侧部分所示。训练阶段为模型提供多样化的特征分布或高度适应的参数。随后的聚合阶段专注于特征重用，旨在实现最佳的迁移效果。
- en: '![Refer to caption](img/477b2138b25ca9b470d9c18b73c32bfd.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/477b2138b25ca9b470d9c18b73c32bfd.png)'
- en: 'Figure 4: The URL [[32](#bib.bib32)] use the knowledge distillation to collect
    multiple source information.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 网址 [[32](#bib.bib32)] 使用知识蒸馏技术来收集多个源信息。'
- en: The URL [[32](#bib.bib32)] is a typical model based on Multiple-Model Methods,
    as shown the Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Multiple-Model For Multiple Source
    ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). The framework first trains $K$ teacher network
    $\{f_{\phi_{1}^{*}},f_{\phi_{2}^{*}}...f_{\phi_{K}^{*}}\}$ for $K$ domains on
    the training stage, then use the student network $f_{\phi}$ to distill the domain
    information on the aggregation stage. The student network $f_{\phi}$ can observe
    all the source domain datasets, but the label comes from the teacher networks’
    output. URL [[32](#bib.bib32)] focuses on aggregation learning and aims to learn
    a single set of universal representations. For the aggregation, use the Domain-specific
    adaptors $\{A_{\theta_{1}},A_{\theta_{2}}...A_{\theta_{K}}\}$ and Classifiers
    $\{h_{\psi_{1}},h_{\psi_{2}}...h_{\psi_{K}}\}$ to match the teacher networks’
    output. URL [[32](#bib.bib32)] has a fixed computational cost regardless of the
    number of domains at inference unlike them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网址[[32](#bib.bib32)]是一个基于多模型方法的典型模型，如图[4](#S3.F4 "Figure 4 ‣ 3.1.1 Multiple-Model
    For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning")所示。该框架首先在训练阶段为$K$个领域训练一组教师网络$\{f_{\phi_{1}^{*}},f_{\phi_{2}^{*}}...f_{\phi_{K}^{*}}\}$，然后在聚合阶段使用学生网络$f_{\phi}$来提取领域信息。学生网络$f_{\phi}$可以观察所有源领域数据集，但标签来自教师网络的输出。网址[[32](#bib.bib32)]专注于聚合学习，旨在学习一组通用表示。对于聚合，使用领域特定适配器$\{A_{\theta_{1}},A_{\theta_{2}}...A_{\theta_{K}}\}$和分类器$\{h_{\psi_{1}},h_{\psi_{2}}...h_{\psi_{K}}\}$来匹配教师网络的输出。网址[[32](#bib.bib32)]在推理时具有固定的计算成本，不受领域数量的影响。
- en: '![Refer to caption](img/a44e49eff2454b450ec8edf499694ac5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a44e49eff2454b450ec8edf499694ac5.png)'
- en: 'Figure 5: The SRF [[30](#bib.bib30)] focus on the more relevant domain feature,
    use multi-domain feature bank to automatically select the most relevant representations.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：SRF [[30](#bib.bib30)]专注于更相关的领域特征，使用多领域特征库自动选择最相关的表示。
- en: SRF [[30](#bib.bib30)] consider that more relevant domain features should be
    concerned, and irrelevant domain should be inhibited on the aggregation stage.
    The framework uses a set of learnable parameters $\lambda$ to select the source
    domain feature. First, SRF [[30](#bib.bib30)] train a set of $K$ feature extractors
    and obtain a multi-domain feature representation, consisting of feature blocks
    with different semantics, as shown Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.1 Multiple-Model
    For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning") left part. Then given
    a few-shot task, select only the relevant feature blocks from the multi-domain
    representation by optimizing masking parameters $\lambda$ on the support set.
    SRF [[30](#bib.bib30)] shows that a simple feature selection mechanism can replace
    feature adaptation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SRF [[30](#bib.bib30)]考虑到应关注更相关的领域特征，并在聚合阶段抑制不相关的领域。该框架使用一组可学习参数$\lambda$来选择源领域特征。首先，SRF
    [[30](#bib.bib30)]训练一组$K$个特征提取器，并获得一个包含不同语义特征块的多领域特征表示，如图[5](#S3.F5 "Figure 5
    ‣ 3.1.1 Multiple-Model For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT
    SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")左侧所示。然后在给定少样本任务的情况下，通过优化支持集上的掩码参数$\lambda$来仅选择相关的特征块。SRF
    [[30](#bib.bib30)]表明，一个简单的特征选择机制可以替代特征适配。
- en: '![Refer to caption](img/b30710796279eae14d40f20fcced1552.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b30710796279eae14d40f20fcced1552.png)'
- en: 'Figure 6: The CosML [[31](#bib.bib31)] first trains a set of meta-learners,
    one for each training domain, to learn prior knowledge (i.e., meta-parameters)
    specific to each domain. The domain-specific meta-learners are then combined in
    the parameter space, by taking a weighted average of their meta-parameters, which
    is used as the initialization parameters of a task network that is quickly adapted
    to novel few-shot classification tasks in an unseen domain.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：CosML [[31](#bib.bib31)]首先训练一组元学习器，每个训练领域一个，以学习特定于每个领域的先验知识（即元参数）。然后在参数空间中将领域特定的元学习器结合起来，通过加权平均它们的元参数，作为任务网络的初始化参数，该网络能够快速适应在未见领域中的新少样本分类任务。
- en: Compared with feature level transfer, CosML [[31](#bib.bib31)] focuses on the
    parameter aggregation, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.1 Multiple-Model
    For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning"). For CosML [[31](#bib.bib31)],
    it first get parameters $\theta_{D_{k}}$ for each domain. $\theta_{D_{k}}$ have
    a certain transfer ability to the new domain, but they lack communication. For
    the final model parameter $\theta$, CosML [[31](#bib.bib31)] combine all domain
    parameters $\theta_{D_{k}}$ with a domain weight $\alpha_{D_{k}}$ which can reflect
    the domain’s importance, with Eq. [1](#S3.E1 "In 3.1.1 Multiple-Model For Multiple
    Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与特征层级迁移相比，CosML [[31](#bib.bib31)] 关注于参数聚合，如图 [6](#S3.F6 "图 6 ‣ 3.1.1 多模型处理多个源
    ‣ 3.1 多源 CDFS ‣ 3 不同设置的 CDFS ‣ 深度视觉跨域小样本学习综述") 所示。对于 CosML [[31](#bib.bib31)]，它首先获取每个领域的参数
    $\theta_{D_{k}}$。$\theta_{D_{k}}$ 对新领域具有一定的迁移能力，但缺乏沟通。对于最终模型参数 $\theta$，CosML
    [[31](#bib.bib31)] 将所有领域参数 $\theta_{D_{k}}$ 与反映领域重要性的领域权重 $\alpha_{D_{k}}$ 结合起来，如公式
    [1](#S3.E1 "在 3.1.1 多模型处理多个源 ‣ 3.1 多源 CDFS ‣ 3 不同设置的 CDFS ‣ 深度视觉跨域小样本学习综述")。
- en: '|  | $\begin{split}\theta=\alpha_{D_{1}}\theta_{D_{1}}+\alpha_{D_{2}}\theta_{D_{2}}+...+\alpha_{D_{k}}\theta_{D_{k}}\end{split}$
    |  | (1) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\theta=\alpha_{D_{1}}\theta_{D_{1}}+\alpha_{D_{2}}\theta_{D_{2}}+...+\alpha_{D_{k}}\theta_{D_{k}}\end{split}$
    |  | (1) |'
- en: 3.1.2 Single-Model For Multiple Source
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 单模型处理多个源
- en: Single-Model Methods apply a unified model to learn the multiple domain feature.
    The Single-Model Methods can train all the source domain data, as shown in Fig.
    [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning") right part. Compared with Multiple-Models Methods, the Single-Model
    Methods have fewer calculation and omit the multimodel’s aggregation stage. For
    getting high adaptability parameters, DAML [[37](#bib.bib37)] focuses on the optimization
    strategy and learns to adapt the model to novel classes in both seen and unseen
    domains by data sampled from multiple domains.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 单模型方法应用统一模型来学习多个领域特征。单模型方法可以训练所有源领域数据，如图 [3](#S3.F3 "图 3 ‣ 3 不同设置的 CDFS ‣ 深度视觉跨域小样本学习综述")
    右侧所示。与多模型方法相比，单模型方法计算量较少，并省略了多模型的聚合阶段。为了获得高适应性的参数，DAML [[37](#bib.bib37)] 关注优化策略，并通过来自多个领域的数据学习以适应新类别。
- en: LFT [[7](#bib.bib7)] and TSA [[34](#bib.bib34)] propose to attach feature transformation
    or adapters directly to a pre-trained model. The multiple source domain collectively
    trains these modules, improving the model’s transfer ability.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LFT [[7](#bib.bib7)] 和 TSA [[34](#bib.bib34)] 提出了将特征变换或适配器直接附加到预训练模型上的方法。多个源领域共同训练这些模块，从而提高模型的迁移能力。
- en: '![Refer to caption](img/3d3b899b34d3ccdea45c831126c1e813.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3d3b899b34d3ccdea45c831126c1e813.png)'
- en: 'Figure 7: The CDNet [[35](#bib.bib35)] learns the ability of learn-to-decompose
    that can be easily adapted to identify unseen compound expressions.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: CDNet [[35](#bib.bib35)] 学习了能够轻松适应识别未见过的复合表达式的学习分解能力。'
- en: The domain-specific information may be confused as the single model faces more
    than one source domain. To relieve this problem, CDNet [[35](#bib.bib35)] decomposes
    domain information from a given feature and extracts a domain-independent expression,
    as shown in the Fig. [7](#S3.F7 "Figure 7 ‣ 3.1.2 Single-Model For Multiple Source
    ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). CDNet [[35](#bib.bib35)] stacks serial LD (Learn-to-Decompose)
    module, which output the class feature, and SD (Sequential Decomposition) module
    to decompose the domain-agnostic and domain-specific feature. Similarly, MDLCC
    [[33](#bib.bib33)] through the channel re-weighting module to decompose the different
    domain features. After the training, the model only needs to finetune the re-weighting
    module with fewer parameter changes to adapt to the new domain.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当单个模型面临多个源领域时，领域特定的信息可能会被混淆。为了解决这个问题，CDNet [[35](#bib.bib35)] 从给定特征中分解领域信息并提取领域独立的表达，如图
    [7](#S3.F7 "Figure 7 ‣ 3.1.2 Single-Model For Multiple Source ‣ 3.1 Multiple Source
    CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning") 所示。CDNet [[35](#bib.bib35)] 堆叠了串行 LD（Learn-to-Decompose）模块，该模块输出类特征，以及
    SD（Sequential Decomposition）模块，用于分解领域无关和领域特定特征。类似地，MDLCC [[33](#bib.bib33)] 通过通道重加权模块分解不同领域的特征。训练后，模型仅需微调重加权模块，调整的参数更少，以适应新领域。
- en: To relieve the domain labels’ cost, ISS [[36](#bib.bib36)] uses one source domain
    with labels and several other unlabeled domain datasets to train a single model.
    The labeled domain input will be mix-enhanced with other unlabeled domain data.
    This can expand the labeled domain data distribution with other domains’ stylization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低领域标签的成本，ISS [[36](#bib.bib36)] 使用一个带标签的源领域和其他几个未标记的领域数据集来训练一个模型。带标签的领域输入将与其他未标记的领域数据混合增强。这可以通过其他领域的风格化扩展带标签的领域数据分布。
- en: 3.1.3 Discussion and Summary
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 讨论与总结
- en: 'Multi-source cross-domain few-shot learning uses multi-source data to improve
    the model’s scalability. In order to optimize the learning process, the decision
    should analyze the efficiency, accuracy, and scalability. To this end, there are
    two different methods: the multi-model method (which requires communication between
    sources) and the single-model method (which directly processes source data and
    is easier to set). By appropriately using these two methods and putting enough
    energy into data collection, we can significantly improve the results of multi-source
    cross-domain few-shot learning. In addition, structured data sources, collection,
    and processing methods can reduce the labor-intensive nature of multi-source few-shot
    learning and may significantly improve the results.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多源跨领域少量样本学习利用多源数据来提高模型的可扩展性。为了优化学习过程，决策应分析效率、准确性和可扩展性。为此，有两种不同的方法：多模型方法（需要源之间的通信）和单模型方法（直接处理源数据，设置更为简单）。通过适当地使用这两种方法并投入足够的精力进行数据收集，我们可以显著提高多源跨领域少量样本学习的结果。此外，结构化的数据源、收集和处理方法可以减少多源少量样本学习的劳动强度，并可能显著提高结果。
- en: 3.2 Single Source CDFS
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 单源CDFS
- en: 'The standard few-shot learning setting requires limited annotation, and thus
    the use of multiple domains to train a model is not always feasible. Seeking a
    more realistic and manageable solution, a single source cross-domain few-shot
    (CDFS) method has been proposed, which uses just one source domain to train the
    model. This single source CDFS includes two settings: Target Domain Accessible
    and Target Domain Forbidden.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的少量样本学习设置需要有限的注释，因此使用多个领域来训练模型并不总是可行。为了寻求更现实且可管理的解决方案，提出了一种单源跨领域少量样本（CDFS）方法，该方法仅使用一个源领域来训练模型。这种单源CDFS包括两个设置：目标领域可访问和目标领域禁用。
- en: 'The standard few-shot learning setting requires limited annotation and thus
    makes it difficult to employ multiple domains to train a model. A single-source
    method can be used to provide a more realistic and manageable solution, which
    uses only one source domain to train the model. This single-source CDFS includes
    two settings: Target Domain Accessible and Target Domain Forbidden. In the former,
    the target domain is available during training time, which allows the model adapts
    to the target domain. The target domain remains inaccessible in the latter, so
    the proposed models must be able to generalize across domains.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的少样本学习设置要求有限的注释，因此很难使用多个领域来训练模型。可以使用单源方法提供更现实和可管理的解决方案，仅使用一个源领域来训练模型。这个单源
    CDFS 包括两种设置：目标领域可访问和目标领域禁止。在前者中，目标领域在训练时可用，这允许模型适应目标领域。而在后者中，目标领域保持不可访问，因此提出的模型必须能够跨领域进行泛化。
- en: 3.2.1 Target Domain Accessible For Single Source
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 单源领域可访问性
- en: For the Single Source CDFS, using just one domain data to solve the cross-domain
    and few-shot problem is challenging. To address this, Target Domain Accessible
    methods assume that some target domain data can be accessed either via supervised
    learning methods [[40](#bib.bib40), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]
    or unsupervised ones [[47](#bib.bib47), [38](#bib.bib38), [75](#bib.bib75), [41](#bib.bib41),
    [39](#bib.bib39), [46](#bib.bib46)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单源 CDFS，仅使用一个领域数据解决跨领域和少样本问题是具有挑战性的。为此，目标领域可访问方法假设可以通过监督学习方法 [[40](#bib.bib40),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)] 或无监督方法 [[47](#bib.bib47),
    [38](#bib.bib38), [75](#bib.bib75), [41](#bib.bib41), [39](#bib.bib39), [46](#bib.bib46)]
    访问一些目标领域数据。
- en: Target Supervised Methods draw on a few target domain labeled data for model
    training. However, the dataset’s size is limited, resulting in an overfitting
    risk. Conversely, over-adaption and misleading can be caused by overly amplified
    target samples. To address this, AcroFOD [[44](#bib.bib44)] utilizes an adaptive
    optimization strategy that selects augmented data that is more similar to target
    samples rather than simply increasing the data amount. ME-D2N [[42](#bib.bib42)]
    proposes a decompose module for knowledge distillation between two networks as
    domain experts to combat this.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 目标监督方法借助少量目标领域标记数据进行模型训练。然而，数据集的大小有限，这可能导致过拟合风险。相反，目标样本的过度放大可能会导致过度适应和误导。为了解决这个问题，AcroFOD
    [[44](#bib.bib44)] 采用了一种自适应优化策略，选择与目标样本更相似的增强数据，而不仅仅是增加数据量。ME-D2N [[42](#bib.bib42)]
    提出了一个用于知识蒸馏的解耦模块，在两个网络之间作为领域专家来应对这一问题。
- en: '![Refer to caption](img/5f1cc2dd2d44c9f1edfd1d61e351aedb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5f1cc2dd2d44c9f1edfd1d61e351aedb.png)'
- en: 'Figure 8: The TGDM [[43](#bib.bib43)] mix framework. Use the mixed intermediate
    domain reduce the domain gap.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: TGDM [[43](#bib.bib43)] 混合框架。使用混合中间领域来缩小领域差距。'
- en: The domain gap is a critical issue for CDFS, so TGDM [[43](#bib.bib43)] design
    an intermediate domain generated by mixing images in the source and the target
    domain, as shown in the Fig. [8](#S3.F8 "Figure 8 ‣ 3.2.1 Target Domain Accessible
    For Single Source ‣ 3.2 Single Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning"). The model calculates $L_{S}$,
    $L_{T}$, and $L_{Mix}$ three loss focus on the source, target, and intermediate
    domain optimized. Experiments show that the intermediate domain can effectively
    relieve the domain gap influence.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 领域差距是 CDFS 的关键问题，因此 TGDM [[43](#bib.bib43)] 设计了一个通过混合源领域和目标领域图像生成的中间领域，如图 [8](#S3.F8
    "图 8 ‣ 3.2.1 单源领域可访问性 ‣ 3.2 单源 CDFS ‣ 3 不同的 CDFS 设置 ‣ 深度视觉跨领域少样本学习综述") 所示。模型计算了
    $L_{S}$、$L_{T}$ 和 $L_{Mix}$ 三个损失，关注于源领域、目标领域和中间领域的优化。实验表明，中间领域可以有效缓解领域差距的影响。
- en: Target Unsupervised Methods are often favored due to having sufficient target
    domain data compared to supervised methods, but data annotation can be lacking.
    Meta-FDMixup [[38](#bib.bib38)], and Generalized Meta-FDMixup [[75](#bib.bib75)]
    use the target domain’s unlabeled data to mix it with source training data. This
    provides class and domain labels and requires the model to learn domain-irrelevant
    and domain-specific features. OA-FSUI2IT [[41](#bib.bib41)] utilizes its target’s
    unlabeled data to transform source data and generate content consistent with the
    source while leaving the style to match the target–to train the network subsequently.
    DDN [[39](#bib.bib39)] is yet another approach, in which a Dynamic Distillation
    Network is trained on target unlabeled data, approximate predictions from weakly-augmented
    versions of the same images from a teacher network are used to impose consistency
    regularization that matches strongly augmented versions of those same images from
    a student. Lastly, CDSC-FSL [[45](#bib.bib45)] proposes a new setting focusing
    on the domain gap between support and query set. Here, contrast learning is utilized
    to align the same classes from two domains; however, it is important to note that
    CDSC-FSL [[45](#bib.bib45)] requires source and target domains to have the same
    classes. Improvements to existing target unsupervised methods are therefore necessary
    to enhance transfer accuracy and effectively address any domain disparities.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 目标无监督方法通常受到青睐，因为与监督方法相比，目标领域数据较为充足，但数据标注可能不足。Meta-FDMixup [[38](#bib.bib38)]
    和 Generalized Meta-FDMixup [[75](#bib.bib75)] 使用目标领域的未标注数据将其与源训练数据混合。这提供了类别和领域标签，并要求模型学习领域无关和领域特定的特征。OA-FSUI2IT
    [[41](#bib.bib41)] 利用其目标的未标注数据来转换源数据，并生成与源一致的内容，同时使风格匹配目标，以便随后训练网络。DDN [[39](#bib.bib39)]
    是另一种方法，其中在目标未标注数据上训练动态蒸馏网络，使用来自教师网络的弱增强版本的相同图像的近似预测来施加一致性正则化，从而匹配来自学生的强增强版本的相同图像。最后，CDSC-FSL
    [[45](#bib.bib45)] 提出了一个新的设置，专注于支持集和查询集之间的领域差距。在这里，利用对比学习对齐来自两个领域的相同类别；然而，需要注意的是，CDSC-FSL
    [[45](#bib.bib45)] 要求源领域和目标领域具有相同的类别。因此，现有目标无监督方法的改进是必要的，以提高转移准确性并有效解决领域差异。
- en: 3.2.2 Target Domain Forbidden For Single Source
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 单一来源的目标领域禁止
- en: 'Target Domain Forbidden CDFS is the most challenging cross-domain few-shot
    learning task, which assumes that only a single source domain is available for
    training and the target domain data is forbidden. Recent research [[48](#bib.bib48),
    [50](#bib.bib50), [51](#bib.bib51), [55](#bib.bib55), [52](#bib.bib52), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [64](#bib.bib64), [65](#bib.bib65)]
    aims to develop networks with better adaptability by training with just one source
    domain. Two main strategies have been proposed: Normalization Methods and Self-Supervised
    Methods.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目标领域禁止 CDFS 是最具挑战性的跨领域少样本学习任务，它假设只有一个源领域可用于训练，而目标领域数据被禁止。近期研究 [[48](#bib.bib48),
    [50](#bib.bib50), [51](#bib.bib51), [55](#bib.bib55), [52](#bib.bib52), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [64](#bib.bib64), [65](#bib.bib65)]
    旨在通过仅用一个源领域进行训练，开发出具有更好适应性的网络。提出了两种主要策略：归一化方法和自监督方法。
- en: Normalization Methods mainly aim to improve model generation by introducing
    various normalization techniques to reduce the impact of domain differences. For
    instance, DCM+SS [[53](#bib.bib53)] uses normalization to correct the bias caused
    by the large difference between the source and the target domain. AFA [[56](#bib.bib56)]
    introduces adversarial learning to improve feature diversity with normalization.
    LCCS [[57](#bib.bib57)] proposes to learn the parameters of BatchNorm from the
    source domain to mitigate the domain gap. RD [[5](#bib.bib5)] uses normalization
    to mix instance features and a learnable memory to transfer the source domain
    information to the target domain. MemREIN [[58](#bib.bib58)] combines instance
    normalization with a Memory Bank to recover discriminant features.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化方法主要通过引入各种归一化技术来减少领域差异的影响，从而提高模型生成能力。例如，DCM+SS [[53](#bib.bib53)] 使用归一化来修正源领域和目标领域之间的巨大差异引起的偏差。AFA
    [[56](#bib.bib56)] 引入对抗学习，通过归一化来提高特征多样性。LCCS [[57](#bib.bib57)] 提出了从源领域学习 BatchNorm
    的参数以减轻领域差距。RD [[5](#bib.bib5)] 使用归一化来混合实例特征和可学习的记忆，以将源领域信息转移到目标领域。MemREIN [[58](#bib.bib58)]
    结合实例归一化和记忆库来恢复判别特征。
- en: Self-Supervised Methods leverage images’ semantic consistency to expand the
    source model’s input and contrastive learning approaches to improve model generation
    further. ProtoTransfer [[49](#bib.bib49)] uses self-supervision to train an embedding
    function, leading to fast adaptation to new domains. STCDFS [[63](#bib.bib63)]
    divides the domain adaptation task into inner and outer tasks. The model first
    self-supervises the inner task (through image rotation or background swapping)
    and then solves the few-shot learning problem in the outer task. TL-SS [[54](#bib.bib54)]
    follows the episodic training approach, advocating for task-level self-supervision
    to handle the domain discrepancy problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 自我监督方法利用图像的语义一致性来扩展源模型的输入，并使用对比学习方法进一步提高模型生成能力。ProtoTransfer [[49](#bib.bib49)]
    使用自我监督训练嵌入函数，从而快速适应新领域。STCDFS [[63](#bib.bib63)] 将领域适应任务分为内部任务和外部任务。模型首先通过图像旋转或背景交换自我监督内部任务，然后在外部任务中解决少样本学习问题。TL-SS
    [[54](#bib.bib54)] 采用情境训练方法，提倡在任务层面进行自我监督以处理领域差异问题。
- en: 3.2.3 Discussion and Summary
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 讨论与总结
- en: The single-source solution proposes that a single-source domain model is sufficient
    for training, implying fewer requirements from different domains than multi-source
    approaches. However, this raises the question of whether the target domain data
    can be reached. If accessible, both supervised and unsupervised access are possible
    options. If forbidden, self-supervision can be adopted to enrich data distribution.
    Single-source cross-domain few-shot learning is one of the most pervasive studies
    in this area, and venturing into target-domain data prohibition is the most exciting
    challenge here. In the following section, we will examine solutions to this issue.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 单源解决方案提出，单一来源的领域模型足以进行训练，这意味着比多源方法对不同领域的要求更少。然而，这引发了目标领域数据是否可以获取的问题。如果可以获取，监督和非监督访问都是可能的选项。如果禁止访问，可以采用自我监督来丰富数据分布。单源跨领域少样本学习是该领域最广泛研究的课题之一，而探索目标领域数据禁用的挑战是这里最令人兴奋的挑战。在接下来的部分中，我们将讨论解决此问题的方法。
- en: 4 DIFFERENT SOLUTION OF CDFS
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种不同的 CDFS 解决方案
- en: <svg   height="426.3" overflow="visible" version="1.1" width="740.01"><g transform="translate(0,426.3)
    matrix(1 0 0 -1 0 0) translate(118.11,0) translate(0,426.3)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -97.94 -117.29)" fill="#000000"
    stroke="#000000"><foreignobject width="82.78" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Self-Supervised</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -97.47 -27.08)" fill="#000000" stroke="#000000"><foreignobject width="318.67"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -62.51)" fill="#000000" stroke="#000000"><foreignobject width="314.45" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDSC-FSL [[45](#bib.bib45)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -97.94)" fill="#000000" stroke="#000000"><foreignobject
    width="272.74" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WULD[[46](#bib.bib46)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -82.2)" fill="#000000" stroke="#000000"><foreignobject
    width="274.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">STCDFS
    [[63](#bib.bib63)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -42.83)" fill="#000000" stroke="#000000"><foreignobject width="352.04" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ProtoTransfer [[49](#bib.bib49)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 20.18 -117.22)" fill="#000000" stroke="#000000"><foreignobject
    width="58.46" height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image
    Mix</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 15.91 -27.08)" fill="#000000"
    stroke="#000000"><foreignobject width="251.41" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="294" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CutMix
    [[47](#bib.bib47)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 24.57
    -66.45)" fill="#000000" stroke="#000000"><foreignobject width="254.39" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TGDM [[43](#bib.bib43)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -86.13)" fill="#000000" stroke="#000000"><foreignobject
    width="277.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AcroFOD
    [[44](#bib.bib44)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 20.18
    -170.42)" fill="#000000" stroke="#000000"><foreignobject width="49.24" height="8.51"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Generate</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -101.41 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="251.58" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NSA
    [[51](#bib.bib51)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -34.48
    -157)" fill="#000000" stroke="#000000"><foreignobject width="252.76" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ISS [[36](#bib.bib36)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="318.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.47
    -87)" fill="#000000" stroke="#000000"><foreignobject width="129.03" height="8.65"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature Transformation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.93 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="263.56" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.93
    -46.76)" fill="#000000" stroke="#000000"><foreignobject width="249.64" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AFA [[56](#bib.bib56)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.93 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="223.97" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FTE[[61](#bib.bib61)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="277.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wave-SAN[[64](#bib.bib64)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="282.54" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DFTL[[76](#bib.bib76)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="278.2" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LFT
    [[7](#bib.bib7)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 331.66
    -66.45)" fill="#000000" stroke="#000000"><foreignobject width="251.03" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TSA [[34](#bib.bib34)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 331.66 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="255.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RD
    [[5](#bib.bib5)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.47
    -140.91)" fill="#000000" stroke="#000000"><foreignobject width="124.08" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Knowledge Distillation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.93 -117.63)" fill="#000000" stroke="#000000"><foreignobject
    width="232.12" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">URL
    [[32](#bib.bib32)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 248.98
    -117.63)" fill="#000000" stroke="#000000"><foreignobject width="254.18" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DDN [[39](#bib.bib39)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 319.85 -117.63)" fill="#000000" stroke="#000000"><foreignobject
    width="253.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ME-D2N[[42](#bib.bib42)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 182.05 -168.81)" fill="#000000" stroke="#000000"><foreignobject
    width="327.51" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">EGCDFS[[77](#bib.bib77)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 300.16 -168.81)" fill="#000000" stroke="#000000"><foreignobject
    width="232.62" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Drop-CDFS
    [[59](#bib.bib59)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -251.49)" fill="#000000" stroke="#000000"><foreignobject width="251.41" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -290.86)" fill="#000000" stroke="#000000"><foreignobject
    width="248.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RMP[[50](#bib.bib50)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -330.23)" fill="#000000" stroke="#000000"><foreignobject
    width="258.33" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ME-D2N
    [[42](#bib.bib42)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -369.6)" fill="#000000" stroke="#000000"><foreignobject width="298.69" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DAFL[[40](#bib.bib40)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.26 -251.49)" fill="#000000" stroke="#000000"><foreignobject
    width="315.71" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDNet
    [[35](#bib.bib35)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 44.26
    -290.86)" fill="#000000" stroke="#000000"><foreignobject width="278.12" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MemREIN[[58](#bib.bib58)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 12.76 -330.23)" fill="#000000" stroke="#000000"><foreignobject
    width="334.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">G-Meta-FDMixup
    [[75](#bib.bib75)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 44.26
    -369.6)" fill="#000000" stroke="#000000"><foreignobject width="281.78" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wave-SAN [[64](#bib.bib64)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.47 -286.58)" fill="#000000" stroke="#000000"><foreignobject
    width="112.36" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Distribution
    Aligned</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 189.93 -243.61)"
    fill="#000000" stroke="#000000"><foreignobject width="265.44" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CDTF [[78](#bib.bib78)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 268.67 -243.61)" fill="#000000" stroke="#000000"><foreignobject
    width="268.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CMT
    [[48](#bib.bib48)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 339.53
    -243.61)" fill="#000000" stroke="#000000"><foreignobject width="229.31" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RDC [[55](#bib.bib55)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.93 -263.3)" fill="#000000" stroke="#000000"><foreignobject
    width="266.25" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCM+SS
    [[53](#bib.bib53)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 272.61
    -263.3)" fill="#000000" stroke="#000000"><foreignobject width="272.74" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ATA[[52](#bib.bib52)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 339.53 -263.3)" fill="#000000" stroke="#000000"><foreignobject
    width="256.79" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LCCS[[57](#bib.bib57)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 327.26 -360.59)" fill="#000000" stroke="#000000"><foreignobject
    width="75.79" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-learning</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 260.79 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="262.06" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TL-SS
    [[54](#bib.bib54)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 260.79
    -342.04)" fill="#000000" stroke="#000000"><foreignobject width="299.73" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DAML [[37](#bib.bib37)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -361.72)" fill="#000000" stroke="#000000"><foreignobject
    width="235.11" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">L2F[[79](#bib.bib79)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 339.53 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="277.43" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CosML
    [[31](#bib.bib31)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 339.53
    -342.04)" fill="#000000" stroke="#000000"><foreignobject width="253.72" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SB-MTL[[62](#bib.bib62)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="247.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDFSL[[80](#bib.bib80)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -345.97)" fill="#000000" stroke="#000000"><foreignobject
    width="280.43" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PATNet
    [[26](#bib.bib26)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 176.15
    -387.31)" fill="#000000" stroke="#000000"><foreignobject width="295.61" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TACDFSL[[81](#bib.bib81)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -365.66)" fill="#000000" stroke="#000000"><foreignobject
    width="239.5" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ReFine
    [[65](#bib.bib65)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 260.79
    -387.31)" fill="#000000" stroke="#000000"><foreignobject width="212.4" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TMH [[60](#bib.bib60)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 323.79 -387.31)" fill="#000000" stroke="#000000"><foreignobject
    width="293.5" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MDLCC
    [[33](#bib.bib33)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -53.24
    -197.06)" fill="#000000" stroke="#000000"><foreignobject width="147.83" height="14.76"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image Enhancement</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 222.35 -198.68)" fill="#000000" stroke="#000000"><foreignobject
    width="158.71" height="11.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature
    Enhancement</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -25.69 -417.54)"
    fill="#000000" stroke="#000000"><foreignobject width="80.95" height="14.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Decoupling</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.91 -417.44)" fill="#000000" stroke="#000000"><foreignobject width="84.64"
    height="14.57" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fine-tuning</foreignobject></g></g></svg>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="426.3" overflow="visible" version="1.1" width="740.01"><g transform="translate(0,426.3)
    matrix(1 0 0 -1 0 0) translate(118.11,0) translate(0,426.3)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -97.94 -117.29)" fill="#000000"
    stroke="#000000"><foreignobject width="82.78" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">自监督</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -97.47 -27.08)" fill="#000000" stroke="#000000"><foreignobject width="318.67"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -62.51)" fill="#000000" stroke="#000000"><foreignobject width="314.45" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDSC-FSL [[45](#bib.bib45)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -97.94)" fill="#000000" stroke="#000000"><foreignobject
    width="272.74" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WULD[[46](#bib.bib46)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -82.2)" fill="#000000" stroke="#000000"><foreignobject
    width="274.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">STCDFS
    [[63](#bib.bib63)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -42.83)" fill="#000000" stroke="#000000"><foreignobject width="352.04" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ProtoTransfer [[49](#bib.bib49)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 20.18 -117.22)" fill="#000000" stroke="#000000"><foreignobject
    width="58.46" height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图像混合</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 15.91 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="251.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup
    [[38](#bib.bib38)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 24.57
    -46.76)" fill="#000000" stroke="#000000"><foreignobject width="294" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CutMix [[47](#bib.bib47)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="254.39" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TGDM
    [[43](#bib.bib43)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 24.57
    -86.13)" fill="#000000" stroke="#000000"><foreignobject width="277.72" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AcroFOD [[44](#bib.bib44)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 20.18 -170.42)" fill="#000000" stroke="#000000"><foreignobject
    width="49.24" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">生成</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -101.41 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="251.58" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NSA
    [[51](#bib.bib51)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -34.48
    -157)" fill="#000000" stroke="#000000"><foreignobject width="252.76" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ISS [[36](#bib.bib36)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="318.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.47
    -87)" fill="#000000" stroke="#000000"><foreignobject width="129.03" height="8.65"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">特征变换</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.93 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="263.56" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.93
    -46.76)" fill="#000000" stroke="#000000"><foreignobject width="249.64" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AFA [[56](#bib.bib56)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.93 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="223.97" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FTE[[61](#bib.bib61)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="277.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wave-SAN[[64](#bib.bib64)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="282.54" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DFTL[[76](#bib.bib76)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -66.45)" fill="#000000" stroke="#000000"><foreign
- en: 'Figure 9: Different solution of CDFS. Image Enhancement directly mix the image
    and generate the new image. Feature Enhancement use feature-wise transformation
    and knowledge distillation to achieve enhancement. Finetuning focus on the adaptability
    feature training. Decoupling solutions expect to get the domain-irrelevant and
    domain-specific feature.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：CDFS 的不同解决方案。图像增强直接混合图像并生成新图像。特征增强使用特征级变换和知识蒸馏来实现增强。微调关注适应性特征训练。解耦解决方案期望获得与领域无关和领域特定的特征。
- en: 'We divide the solution of CDFS into four categories: Feature-Enhancement, Image-Enhancement,
    Decompose, and Finetuning (see Fig. [9](#S4.F9 "Figure 9 ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")). The Feature-Enhancement
    Based methods seek to improve the model’s performance through expanded feature
    diversity. In contrast, the Image-Enhancement Based methods leverage the usual
    mixing or pasting techniques to generate additional images. Decompose Based methods
    consider both domain-specific and domain-agnostic features, and finally, Finetuning
    Based methods strive to obtain highly adaptable features to the new domain. We
    further discuss these solutions, each in its dedicated section ([4.1](#S4.SS1
    "4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"), [4.2](#S4.SS2 "4.2 Image-Enhancement
    Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning"), [4.3](#S4.SS3 "4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"), and [4.4](#S4.SS4
    "4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning")), below.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 CDFS 的解决方案分为四类：特征增强、图像增强、解耦和微调（见图 [9](#S4.F9 "Figure 9 ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")）。基于特征增强的方法通过扩展特征多样性来提升模型的性能。相比之下，基于图像增强的方法利用常见的混合或粘贴技术生成额外的图像。解耦基的方法考虑领域特定和领域无关的特征，最后，基于微调的方法力求获得高度适应新领域的特征。我们将在下文的各自章节中进一步讨论这些解决方案，[4.1](#S4.SS1
    "4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning")、[4.2](#S4.SS2 "4.2 Image-Enhancement Based
    ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning")、[4.3](#S4.SS3 "4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning") 和 [4.4](#S4.SS4 "4.4
    Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning")。
- en: 4.1 Feature-Enhancement Based
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于特征增强
- en: After training in the base classes, the few-shot model must learn the novel
    classes in a new domain with limited annotation. In such cases, the feature quality
    directly influences the transfer effect. To address this, the CDFS researchers
    adopt feature transformation, as illustrated in Fig. [10](#S4.F10 "Figure 10 ‣
    4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"), or knowledge distillation to enhance
    the feature quality.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础类训练之后，少样本模型必须在有限标注的新领域中学习新类。在这种情况下，特征质量直接影响转移效果。为了解决这个问题，CDFS 的研究人员采用特征变换，如图
    [10](#S4.F10 "Figure 10 ‣ 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") 所示，或通过知识蒸馏来提高特征质量。
- en: '![Refer to caption](img/c8d195165252ba9376f9ca2bc0644806.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8d195165252ba9376f9ca2bc0644806.png)'
- en: 'Figure 10: The different feature transformation in the CDFS. FWT [[7](#bib.bib7)],
    DFTL[[76](#bib.bib76)], AFA [[56](#bib.bib56)] and SRF [[30](#bib.bib30)] use
    the learnable $\lambda$ and $\beta$ to achieve knowledge transfer, and each domain
    data have a FT-layer in the SRF [[30](#bib.bib30)]. Wave-SAN [[64](#bib.bib64)]
    and RD [[5](#bib.bib5)] mix the feature distribution to restructure the feature.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：CDFS 中不同的特征变换。FWT [[7](#bib.bib7)]、DFTL [[76](#bib.bib76)]、AFA [[56](#bib.bib56)]
    和 SRF [[30](#bib.bib30)] 使用可学习的 $\lambda$ 和 $\beta$ 来实现知识转移，每个领域数据在 SRF [[30](#bib.bib30)]
    中都有一个 FT 层。Wave-SAN [[64](#bib.bib64)] 和 RD [[5](#bib.bib5)] 混合特征分布以重构特征。
- en: 'Feature Transformation Enhancement mainly changes the feature distribution,
    which can make the feature easily transfer [[64](#bib.bib64), [61](#bib.bib61)].
    The normalized strategy adds noise or prior distribution into the original feature.
    The training process contains normalization and reconstructing features using
    Eq. [2](#S4.E2 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 特征变换增强主要改变特征分布，这可以使特征容易转移 [[64](#bib.bib64), [61](#bib.bib61)]。归一化策略向原始特征中添加噪声或先验分布。训练过程包含使用
    Eq. [2](#S4.E2 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") 进行归一化和重构特征：
- en: '|  | $\begin{split}&amp;f_{b}^{norm}=\dfrac{f_{b}-\mu_{b}}{v_{b}}\\ &amp;f_{b}^{reco}=\lambda
    f_{b}^{norm}+\beta\end{split}$ |  | (2) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;f_{b}^{norm}=\dfrac{f_{b}-\mu_{b}}{v_{b}}\\ &amp;f_{b}^{reco}=\lambda
    f_{b}^{norm}+\beta\end{split}$ |  | (2) |'
- en: where $f_{b}$,$\mu_{b}$, and $v_{b}$ are backbone features, channel means and
    channel variance. After the feature normalization, use $\lambda$ and $\beta$ to
    reconstruct the backbone feature. If $\lambda$ and $\beta$ in Eq. [2](#S4.E2 "In
    4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning") from learnable parameters, the feature-wise
    transformer focuses on the feature distribution improvement. FWT [[7](#bib.bib7)]
    insert a feature-wise transformation layer after the batch normalization layer
    in the feature encoder, which can effectively augment the intermediate feature.
    FWT [[7](#bib.bib7)] will remove the feature-wise transformation layer after the
    training. And DFTL[[76](#bib.bib76)] propose a diversified feature transformation
    that parallels several feature transformers. The final layer averages the FT-layer
    output and gets the final prediction. The single-source network also can use feature
    transformation to simulate unseen domain distribution. In the AFA [[56](#bib.bib56)]
    network, the domain discriminator is learned by recognizing the augmented features
    (unseen domain) from the original ones (seen domain). In summary, FWT [[7](#bib.bib7)],
    DFTL[[76](#bib.bib76)] and AFA [[56](#bib.bib56)] use the learnable $\lambda$
    and $\beta$ to achieve knowledge transfer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{b}$、$\mu_{b}$ 和 $v_{b}$ 分别是主干特征、通道均值和通道方差。在特征归一化后，使用 $\lambda$ 和 $\beta$
    来重构主干特征。如果 Eq. [2](#S4.E2 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") 中的 $\lambda$
    和 $\beta$ 是可学习参数，特征级变换器会专注于特征分布的改善。FWT [[7](#bib.bib7)] 在特征编码器的批归一化层之后插入特征级变换层，这可以有效地增强中间特征。FWT
    [[7](#bib.bib7)] 在训练后会去除特征级变换层。而 DFTL [[76](#bib.bib76)] 提出了一个多样化的特征变换，平行使用多个特征变换器。最终层对
    FT 层的输出进行平均并获得最终预测。单源网络也可以使用特征变换来模拟未见领域分布。在 AFA [[56](#bib.bib56)] 网络中，通过识别来自原始（已见领域）和增强（未见领域）特征来学习领域判别器。总之，FWT
    [[7](#bib.bib7)]、DFTL [[76](#bib.bib76)] 和 AFA [[56](#bib.bib56)] 使用可学习的 $\lambda$
    和 $\beta$ 来实现知识迁移。
- en: The $\lambda$ and $\beta$ could come from other instance’s normalized features.
    Wave-SAN [[64](#bib.bib64)] augment source images by swapping the styles of their
    low-frequency components with each other. Wave-SAN [[64](#bib.bib64)] follows
    the replacing strategy and completely restructures the original feature with another
    instance feature’s distribution. This may lead to fluctuation, especially if the
    datasets have a domain shift. The mixing strategy can serve as an improved version
    compared, where the network uses the mixed tensor to restructure the feature,
    as the Eq. [3](#S4.E3 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda$ 和 $\beta$ 可以来自其他实例的归一化特征。Wave-SAN [[64](#bib.bib64)] 通过交换低频分量的风格来增强源图像。Wave-SAN
    [[64](#bib.bib64)] 遵循替换策略，并完全重构原始特征的分布。特别是如果数据集存在领域迁移，这可能导致波动。混合策略可以作为改进版本，网络使用混合张量来重构特征，如
    Eq. [3](#S4.E3 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") 所示。
- en: '|  | $\begin{split}&amp;\lambda^{mix}=\alpha\mu_{b}+(1-\alpha)\mu_{o},\beta^{mix}=\alpha
    v_{b}+(1-\alpha)v_{o},\\ &amp;f_{b}^{reco}=\lambda^{mix}f_{b}^{norm}+\beta^{mix}\end{split}$
    |  | (3) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\lambda^{mix}=\alpha\mu_{b}+(1-\alpha)\mu_{o},\beta^{mix}=\alpha
    v_{b}+(1-\alpha)v_{o},\\ &amp;f_{b}^{reco}=\lambda^{mix}f_{b}^{norm}+\beta^{mix}\end{split}$
    |  | (3) |'
- en: where $\alpha$ controls the retention ratio of original features, $\mu_{o}$
    and $v_{o}$ are channel mean and variance from other image. Specifically, RD [[5](#bib.bib5)]
    uses a Memory-Bank to mix the instance feature, which means the $\mu_{o}$ and
    $v_{o}$ in Eq. [3](#S4.E3 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") from the Memory-Bank
    which collect different source stylization information, the Memory-Bank can load
    the source knowledge into the target domain which can directly relive the domain
    shift. For the multiple-source network, SRF [[30](#bib.bib30)] append different
    FT-layer for each domain. Using a parametric network family to obtain multi-domain
    representations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$控制原始特征的保留比率，$\mu_{o}$和$v_{o}$是来自其他图像的通道均值和方差。具体来说，RD [[5](#bib.bib5)]使用记忆库来混合实例特征，这意味着记忆库中的$\mu_{o}$和$v_{o}$来自于收集不同源样式信息的记忆库，记忆库可以将源知识加载到目标领域，从而直接缓解领域迁移。对于多源网络，SRF
    [[30](#bib.bib30)]为每个领域附加不同的FT层。使用参数化网络家族以获得多领域表示。
- en: Knowledge Distillation Enhancement train the teacher and student networks. The
    student network’s learning target is to make the feature more robust than the
    teacher network. URL [[32](#bib.bib32)] train one teacher network for each domain
    data, then use a student network to distill the domain knowledge from each teacher
    network. DDN [[39](#bib.bib39)] learns a teacher network with weak augmented target
    data, and the target domain’s knowledge will be distilled from the teacher to
    the final transfer student network. The distillation process also can load target
    domain distribution into the source training process. Similarly, ME-D2N [[42](#bib.bib42)]
    also uses Knowledge Distillation to decompose the source and target domain, which
    can effectively improve the CDFS accuracy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏增强训练教师网络和学生网络。学生网络的学习目标是使特征比教师网络更具鲁棒性。URL [[32](#bib.bib32)]为每个领域数据训练一个教师网络，然后使用学生网络从每个教师网络中提取领域知识。DDN
    [[39](#bib.bib39)]在弱增强目标数据上学习教师网络，目标领域的知识将从教师网络蒸馏到最终的转移学生网络。蒸馏过程还可以将目标领域分布加载到源训练过程中。同样，ME-D2N
    [[42](#bib.bib42)]也使用知识蒸馏来分解源领域和目标领域，从而有效提高CDFS的准确性。
- en: 4.2 Image-Enhancement Based
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于图像增强
- en: Image enhancement-based methods directly enhance the image data, as shown in
    Fig. [11](#S4.F11 "Figure 11 ‣ 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). Compared with
    feature enhancement-based methods, it is simpler and changes the image directly.
    The self-supervised method transforms the original image to generate new labels
    for model training, providing another perspective for model observation data.
    At the same time, Mix-Paste methods mix the different images to enrich data distribution.
    The generation strategy is another enhancement strategy, which generates new images
    through the encoder-decoder and inputs them into the network to improve the data
    diversity.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像增强的方法直接增强图像数据，如图[11](#S4.F11 "图11 ‣ 4.2 基于图像增强 ‣ 4 DIFFERENT SOLUTION OF
    CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")所示。与基于特征增强的方法相比，它更简单，并直接改变图像。自监督方法将原始图像转换为生成新的标签用于模型训练，为模型观察数据提供另一种视角。同时，Mix-Paste方法混合不同的图像以丰富数据分布。生成策略是另一种增强策略，通过编码器-解码器生成新图像，并将其输入网络以提高数据多样性。
- en: '![Refer to caption](img/97a2eb688849676a966897c5b778c172.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/97a2eb688849676a966897c5b778c172.png)'
- en: 'Figure 11: Image Enhancement Based. Self-supervised generate the new labels
    for model training, and generate methods use the consistency loss to constrain
    the model, Mix-Paste methods mix the different image which can effectively improve
    the data diversity.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：基于图像增强。自监督生成新的标签用于模型训练，生成方法使用一致性损失来约束模型，Mix-Paste方法混合不同的图像，有效提高数据多样性。
- en: Self-Supervised Image-Enhancement generates the new instance through images’
    semantic consistency. It is practical to improve data quality because this process
    is label-free [[49](#bib.bib49)]. OA-FSUI2IT [[41](#bib.bib41)] assume that the
    image semantic is translation irrelevant. Applying flipping, translation, and
    linear scaling on the image in the training process will not change the image
    content. The deep model should obtain a similar prediction. WULD [[46](#bib.bib46)]
    train the few-shot model with the labeled and unlabeled data. The model must recognize
    the labeled classes and predict the rotated degrees in the labeled and unlabeled
    data. As the auxiliary task, image rotation can improve the effectiveness of the
    main task. Similar to WULD [[46](#bib.bib46)], STCDFS [[63](#bib.bib63)] also
    uses rotation to improve the few-shot model effect. It designs the inner task
    in the few-shot support set. The model first predicted the image rotation degrees
    and labeled classes. The model gets the class prototypes with the original support
    image and finally uses the prototypes to recognize the query set. In the source
    unlabeled CDFS, ProtoTransfer [[49](#bib.bib49)] first transforms the unlabeled
    batch. Each sample will generate $|Q|$ transform samples. The transformed feature
    needs to be concentrated around the original sample. Other original samples in
    the batch are regarded as negative, and the original sample is regarded as positive.
    Finally, in the target domain, the extracted image features will serve as the
    classifier weight for fine-tuning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督图像增强通过图像的语义一致性生成新的实例。这一过程不依赖标签，实际有效地提高数据质量 [[49](#bib.bib49)]。OA-FSUI2IT
    [[41](#bib.bib41)] 假设图像语义与平移无关。在训练过程中对图像进行翻转、平移和线性缩放不会改变图像内容。深度模型应该得到相似的预测。WULD
    [[46](#bib.bib46)] 使用标记和未标记的数据训练少样本模型。模型必须识别标记的类别，并预测标记和未标记数据中的旋转角度。作为辅助任务，图像旋转可以提高主任务的效果。类似于
    WULD [[46](#bib.bib46)]，STCDFS [[63](#bib.bib63)] 也利用旋转来提升少样本模型的效果。它在少样本支持集内设计了内部任务。模型首先预测图像的旋转角度和标记类别。模型利用原始支持图像获得类别原型，最后使用原型来识别查询集。在源未标记的
    CDFS 中，ProtoTransfer [[49](#bib.bib49)] 首先对未标记的批次进行转换。每个样本会生成 $|Q|$ 个转换样本。转换后的特征需要集中在原始样本附近。批次中的其他原始样本被视为负样本，而原始样本被视为正样本。最后，在目标领域，提取的图像特征将作为分类器权重进行微调。
- en: Mix-Paste Image-Enhancement [[47](#bib.bib47)] is the most simple methods which
    cut few target data and past into the source data, this help the model learn the
    target domain knowledge.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Mix-Paste 图像增强 [[47](#bib.bib47)] 是最简单的方法之一，它将少量目标数据裁剪后粘贴到源数据中，这有助于模型学习目标领域的知识。
- en: '![Refer to caption](img/1e095326db5eb7e288347dad896428fc.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1e095326db5eb7e288347dad896428fc.png)'
- en: 'Figure 12: The different mix methods proposed by AcroFOD [[44](#bib.bib44)].'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：AcroFOD 提出的不同混合方法 [[44](#bib.bib44)]。
- en: And AcroFOD [[44](#bib.bib44)] propose two different mix methods to improve
    the effect of few-shot object detection as shown in Fig. [12](#S4.F12 "Figure
    12 ‣ 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning"). The left Image-level Domain-aware
    Augmentation generates the new image with several domain instances. And the right
    Box-level Domain-aware Augmentation first cuts the foreground part of the instance,
    then pastes the corresponding box into another instance. Moreover, Meta-FDMixup
    [[38](#bib.bib38)] uses the super-parameter $\lambda$ to control the source and
    target ratio in the mixed image. After mixing, there will be source, target, and
    mixed three types of images. Due to the target labels being accessible, the source-mix
    pairs and target-mix pairs will run the few-shot classification and calculate
    the loss. The source and target mixing can effectively improve the data distribution
    and get more target data to fit the model. Compare the stiffly paste methods,
    TGDM [[43](#bib.bib43)] aim to learn a dynamic mix ratio via input the validation
    loss to a generation network, in other words, the $\lambda$ in the Meta-FDMixup
    [[38](#bib.bib38)] is learnable.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: AcroFOD [[44](#bib.bib44)] 提出了两种不同的混合方法以改善少样本对象检测的效果，如图 [12](#S4.F12 "Figure
    12 ‣ 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning") 所示。左侧的图像级领域感知增强生成具有多个领域实例的新图像。而右侧的框级领域感知增强首先切割实例的前景部分，然后将相应的框粘贴到另一个实例中。此外，Meta-FDMixup
    [[38](#bib.bib38)] 使用超参数 $\lambda$ 来控制混合图像中的源和目标比例。混合后，将生成源图像、目标图像和混合图像三种类型。由于目标标签是可访问的，源混合对和目标混合对将进行少样本分类并计算损失。源和目标混合可以有效改善数据分布并获取更多目标数据以适应模型。与僵硬粘贴方法相比，TGDM
    [[43](#bib.bib43)] 旨在通过将验证损失输入生成网络来学习动态混合比例，换句话说，Meta-FDMixup [[38](#bib.bib38)]
    中的 $\lambda$ 是可学习的。
- en: Generate Image-Enhancement generate a new image with the Encoder-Decoder method.
    NSAE [[51](#bib.bib51)] propose to take reconstructed images from auto-encoder
    as noisy inputs and let the model further predict their labels. In the target
    testing stage, first performs reconstruction task on the novel dataset and then
    the encoder is fine-tuned for classification. Compare with NSAE [[51](#bib.bib51)]
    vallina encoder-decoder, ISS [[36](#bib.bib36)] combines the feature transformation
    and generates it to solve the CDFS problem. The labeled and unlabelled domain
    features are mixed in the training stage. Then the mixed features are decoded
    to generate images, which are constrained by content perception loss and style
    loss to ensure that the semantic information is consistent with labeling, and
    the style is consistent with unlabelled. Finally, the enhanced dataset is used
    for few-shot training and transferred to the new domain. Meanwhile, OA-FSUI2IT
    [[41](#bib.bib41)] focuses on object detection and proposes an image-translation
    module that generates the source content consistency and target style consistency
    image. ISS [[36](#bib.bib36)] and OA-FSUI2IT [[41](#bib.bib41)] have similar inspiration.
    Specifically, Perceptual loss measures the perceptual similarity of two samples,
    shown in the Eq. [4](#S4.E4 "In 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像增强使用编码器-解码器方法生成新图像。NSAE [[51](#bib.bib51)] 提出将自编码器重建的图像作为噪声输入，并让模型进一步预测其标签。在目标测试阶段，首先在新数据集上执行重建任务，然后对编码器进行微调以进行分类。与NSAE
    [[51](#bib.bib51)] 的原始编码器-解码器相比，ISS [[36](#bib.bib36)] 结合了特征变换并生成它以解决CDFS问题。在训练阶段，标记的和未标记的领域特征被混合。然后，混合特征被解码生成图像，这些图像受到内容感知损失和风格损失的约束，以确保语义信息与标签一致，风格与未标记一致。最后，增强的数据集用于少样本训练并转移到新领域。同时，OA-FSUI2IT
    [[41](#bib.bib41)] 关注对象检测，并提出了一种图像翻译模块，该模块生成源内容一致性和目标风格一致性的图像。ISS [[36](#bib.bib36)]
    和OA-FSUI2IT [[41](#bib.bib41)] 有类似的灵感。具体而言，感知损失度量两个样本的感知相似性，如公式 [4](#S4.E4 "In
    4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning") 所示。
- en: '|  | $\begin{split}\mathcal{L}_{percep}=\sum_{l\in\mathcal{S}}&#124;&#124;f_{l}(x)-f_{l}(\hat{x})&#124;&#124;_{2}^{2}\end{split}$
    |  | (4) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{percep}=\sum_{l\in\mathcal{S}}&#124;&#124;f_{l}(x)-f_{l}(\hat{x})&#124;&#124;_{2}^{2}\end{split}$
    |  | (4) |'
- en: where $f_{l}$ denotes feature from the converged network $\mathcal{S}$, $\mathcal{L}_{percep}$
    require the input $x$ and generated image $\hat{x}$ share the same content or
    semantic. Style loss measures the differences between covariances of the feature
    maps, which can reduce the stylisation from different images, as shown in Eq.
    [5](#S4.E5 "In 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A
    Survey of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f_{l}$表示来自收敛网络$\mathcal{S}$的特征，$\mathcal{L}_{percep}$要求输入$x$和生成图像$\hat{x}$共享相同的内容或语义。风格损失度量特征图的协方差之间的差异，这可以减少来自不同图像的风格化，如方程[5](#S4.E5
    "在 4.2 图像增强基础上 ‣ 4 CDFS 的不同解决方案 ‣ 深度视觉跨域小样本学习综述")所示。
- en: '|  | $\begin{split}\mathcal{L}_{style}=&#124;&#124;G_{j}(x)-G_{j}(\hat{x})&#124;&#124;_{F}^{2}\end{split}$
    |  | (5) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{style}=&#124;&#124;G_{j}(x)-G_{j}(\hat{x})&#124;&#124;_{F}^{2}\end{split}$
    |  | (5) |'
- en: where $G_{j}$ get the gram matrix of $j$-th layer. And OA-FSUI2IT [[41](#bib.bib41)]
    additional use adversarial networks to improve the generation effect. Generate
    Image-Enhancement is the specific self-supervised version, but it generate new
    image compare the single image rotation and mask.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$G_{j}$获取第$j$层的Gram矩阵。OA-FSUI2IT [[41](#bib.bib41)] 额外使用对抗网络来改善生成效果。生成图像增强是特定的自监督版本，但它生成的新图像与单一图像旋转和遮罩进行比较。
- en: 4.3 Decompose Based
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基于分解
- en: '![Refer to caption](img/1af0e09d76eb3dc4975014d5db09d859.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1af0e09d76eb3dc4975014d5db09d859.png)'
- en: 'Figure 13: Different decompose methods. Sample based methods extract different
    feature mean and variance, then sample the specific feature. Path based methods
    design several forward path for feature passing, and each path decompose different
    feature. Parameter based methods learn the decompose parameter, e.g., $\alpha$
    and use the it to filter the feature.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 不同的分解方法。基于样本的方法提取不同的特征均值和方差，然后对特定特征进行采样。基于路径的方法设计多个前向路径用于特征传递，每条路径分解不同的特征。基于参数的方法学习分解参数，例如$\alpha$，并使用它来滤波特征。'
- en: Methods think different features should be decomposed, as shown in Fig. [13](#S4.F13
    "Figure 13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning"). Due to the data of CDFS from a different
    domain, the mixed feature space disturbs the model effect. To decompose these
    features, researchers [[35](#bib.bib35), [40](#bib.bib40), [58](#bib.bib58)] except
    to get different feature expressions. E.g., The domain-specific and domain-irrelevant
    features, The irrelevant feature mainly collects the domain-agnostic discriminate
    information, which means the feature has excellent transfer ability across the
    domain. As a complement, the irrelevant feature can provide the related domain
    information. The specific and irrelevant features adapt to each other through
    adversarial learning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 方法认为不同的特征应该被分解，如图[13](#S4.F13 "图 13 ‣ 4.3 基于分解 ‣ 4 CDFS 的不同解决方案 ‣ 深度视觉跨域小样本学习综述")所示。由于CDFS的数据来自不同的领域，混合特征空间干扰了模型效果。为了分解这些特征，研究人员[[35](#bib.bib35),
    [40](#bib.bib40), [58](#bib.bib58)]除了获得不同的特征表达。比如，领域特定特征和领域无关特征，无关特征主要收集领域无关的判别信息，这意味着该特征具有很好的跨域迁移能力。作为补充，无关特征可以提供相关的领域信息。特定特征和无关特征通过对抗学习相互适应。
- en: Meta-FDMixup serious [[38](#bib.bib38), [75](#bib.bib75)] design a disentangle
    module which generates the mean and variance of irrelevant and specific features
    as shown in the Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT
    SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") left
    part, then use the corresponding mean and variance to sample the domain feature.
    The disentangle module is inspired by VAE (Variation Auto-Encoder), and lies in
    the best learning to disentangle the domain-specific and domain-irrelevant features,
    thus alleviating the domain shift problem in CDFS.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Meta-FDMixup严谨地[[38](#bib.bib38), [75](#bib.bib75)]设计了一个分解模块，该模块生成如图[13](#S4.F13
    "图 13 ‣ 4.3 基于分解 ‣ 4 CDFS 的不同解决方案 ‣ 深度视觉跨域小样本学习综述")左侧部分所示的无关特征和特定特征的均值和方差，然后使用相应的均值和方差来采样领域特征。分解模块受到VAE（变分自编码器）的启发，旨在最佳地学习分解领域特定特征和领域无关特征，从而缓解CDFS中的领域迁移问题。
- en: Different from the Sample base methods, as shown in the Fig. [13](#S4.F13 "Figure
    13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") middle part, Path-based use several forward paths
    to learn the different feature expressions, such as ME-D2N [[42](#bib.bib42)]
    extra learn domain-specific gate forward path which can assign each filter to
    only one specific domain in a learnable way. The structure of Wave-SAN [[64](#bib.bib64)]
    contains stand and style-augmented forward. And Wave-SAN [[64](#bib.bib64)] aims
    to enable the decomposition of visual representations into low-frequency components,
    such as shape and style, and high-frequency components, e.g., texture. The various
    feature produced required to share the same semantics can effectively improve
    the feature quality of CDFS.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与样本基方法不同，如图[13](#S4.F13 "图 13 ‣ 4.3 基于分解 ‣ 4 CDFS 的不同解决方案 ‣ 深度视觉跨域小样本学习概述")中部所示，基于路径的方法使用多个前向路径来学习不同的特征表达，例如
    ME-D2N [[42](#bib.bib42)] 额外学习领域特定的门控前向路径，该路径可以以可学习的方式将每个滤波器分配给一个特定领域。Wave-SAN
    [[64](#bib.bib64)] 的结构包含标准和风格增强的前向路径。Wave-SAN [[64](#bib.bib64)] 旨在实现视觉表征的分解，分为低频组件，如形状和风格，以及高频组件，如纹理。产生的各种特征需要共享相同的语义，可以有效提高
    CDFS 的特征质量。
- en: The Parameter methods use the learnable tensor to filter screen the feature,
    as shown in the right part of Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Decompose Based
    ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning"). CDNet [[35](#bib.bib35)] propose a serial framework. Each step uses
    the Learn-to-Decompose (LD) Module to filter the domain feature and complete the
    feature decomposition. A MemREIN [[58](#bib.bib58)] focuses on the channel decomposition,
    the discriminative feature will be registered into the memory bank.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数方法使用可学习的张量来筛选特征，如图[13](#S4.F13 "图 13 ‣ 4.3 基于分解 ‣ 4 CDFS 的不同解决方案 ‣ 深度视觉跨域小样本学习概述")右侧部分所示。CDNet
    [[35](#bib.bib35)] 提出了一个串行框架。每一步都使用 Learn-to-Decompose (LD) 模块来筛选域特征并完成特征分解。MemREIN
    [[58](#bib.bib58)] 专注于通道分解，判别特征将被注册到内存库中。
- en: Besides these domain decomposing methods, researchers also study the expression
    at the class level. RMP [[50](#bib.bib50)] considers that the mid-level feature
    is more meaningful for the transferring between distant domains. Class mid-level
    features must predict the class discriminates features to supply the mid-level
    features. As the middle feature lacks the discrimination, RMP [[50](#bib.bib50)]
    first extract the class feature, then use the features of other classes to reconstruct
    the current class, and finally use the existing features to subtract the reconstructed
    features in the cosine space to obtain the class discriminate features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些领域分解方法，研究人员还研究了类级别的表达。RMP [[50](#bib.bib50)] 认为中层特征对于远程领域之间的迁移更有意义。类中层特征必须预测类区分特征以提供中层特征。由于中层特征缺乏区分性，RMP
    [[50](#bib.bib50)] 首先提取类特征，然后使用其他类的特征重构当前类，最后在余弦空间中使用现有特征减去重构特征，以获得类区分特征。
- en: 4.4 Fine-tuning Based
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于微调
- en: Fine-tuning Based methods focus on the feature transfer [[33](#bib.bib33)] and
    parameters adaptability [[65](#bib.bib65)]. Fine-tuning aims to get a robust feature
    with high transfer ability [[81](#bib.bib81)], expect to transform domain-specific
    features into irrelevant metric spaces, which will reduce the adverse effects
    of domain shift. Since the metric space is invariant, it is easier for downstream
    segmentation modules to predict in such a stable space. We group Fine-tuning Based
    methods into Meta-Learning Methods, Distribution Aligned Methods.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于微调的方法关注于特征迁移 [[33](#bib.bib33)] 和参数适应性 [[65](#bib.bib65)]。微调的目的是获得具有高迁移能力的鲁棒特征
    [[81](#bib.bib81)]，期望将特定领域的特征转换为无关的度量空间，这将减少领域偏移的负面影响。由于度量空间是不变的，下游分割模块在这种稳定空间中进行预测更容易。我们将基于微调的方法分为元学习方法和分布对齐方法。
- en: 4.4.1 Meta-Learning Fine-tuning
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 元学习微调
- en: '![Refer to caption](img/bfbd204cae47100bd409449817c9d551.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bfbd204cae47100bd409449817c9d551.png)'
- en: 'Figure 14: The parameter updating strategy of the meta-generation network,
    where $\theta$ is the fast-weight and $\phi$ is the slow-weight.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：元生成网络的参数更新策略，其中 $\theta$ 是快权重，$\phi$ 是慢权重。
- en: The core idea of Meta-Learning is ’learn to learn,’ which means the optimized
    target is how to collect the experience faced by the new task. Compared with traditional
    training methods, the meta-learning process has fast-weights and slow-weights.
    The fast-weight solves the specific task problem. The slow-weight needs to collect
    experience, as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.4.2 Distribution Aligned
    Fine-tuning ‣ 4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习的核心思想是“学会学习”，这意味着优化目标是如何收集面对新任务的经验。与传统训练方法相比，元学习过程具有快权重和慢权重。快权重解决特定任务问题。慢权重则需要收集经验，如图
    [15](#S4.F15 "图 15 ‣ 4.4.2 分布对齐微调 ‣ 4.4 微调基于 ‣ 4 不同的 CDFS 解决方案 ‣ 深度视觉跨领域少样本学习综述")
    所示。
- en: The key design of meta-learning is task sampling. DAML [[37](#bib.bib37)] sample
    the tasks from multiple domains, the model learns domain-agnostic initial parameters,
    which would adapt to novel classes in unseen domains during meta-testing. And
    CosML [[31](#bib.bib31)] directly combines multiple domain parameters as the transfer
    initialization, its training process on each separate domain compared with DAML
    [[37](#bib.bib37)]. And except for the slow and fast optimized weight, TL-SS [[54](#bib.bib54)]
    uses a Weight Generator to predict the parameters of a high-level network, which
    require the model to generate proper parameters and enables the encoder to flexibly
    to any unseen tasks. For parameters initialization in the meta-learning, forcibly
    sharing an initialization can lead to conflicts among tasks and the compromised
    (undesired by tasks) location on the optimization landscape, thereby hindering
    task adaptation. L2F [[79](#bib.bib79)] propose task-and-layer-wise attenuation
    on the compromised initialization to reduce its adverse influence on task adaptation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习的关键设计是任务采样。DAML [[37](#bib.bib37)] 从多个领域中采样任务，模型学习领域无关的初始参数，这些参数在元测试过程中能够适应未见领域中的新类别。而
    CosML [[31](#bib.bib31)] 直接将多个领域参数结合为转移初始化，其在每个独立领域的训练过程与 DAML [[37](#bib.bib37)]
    进行比较。除了慢优化权重和快优化权重之外，TL-SS [[54](#bib.bib54)] 使用权重生成器来预测高级网络的参数，这要求模型生成合适的参数，并使编码器能够灵活适应任何未见任务。在元学习中的参数初始化中，强制共享初始化可能导致任务之间的冲突以及在优化景观上不理想的位置，从而阻碍任务适应。L2F
    [[79](#bib.bib79)] 提出了对不理想初始化进行任务和层级的衰减，以减少其对任务适应的负面影响。
- en: 4.4.2 Distribution Aligned Fine-tuning
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 分布对齐微调
- en: '![Refer to caption](img/8566c3bf9e6bbd9e12932ff3bdb176de.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8566c3bf9e6bbd9e12932ff3bdb176de.png)'
- en: 'Figure 15: Different scenario have disunion distribution, which may reflect
    in the input image or feature level(left part). After Distribution Align, optimize
    the model could in a unified space, and have better transfer ability(right part).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：不同场景具有不一致的分布，这可能体现在输入图像或特征级别（左侧）。经过分布对齐后，优化模型可以在统一空间中进行，并具有更好的迁移能力（右侧）。
- en: The distribution between the source and target domains in the CDFS is different,
    as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.4.2 Distribution Aligned Fine-tuning
    ‣ 4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). To address the influence of this difference,
    CMT [[48](#bib.bib48)] proposed mechanism transfer, a meta-distributional scenario
    in which the data-generating mechanism is invariant across domains. CMT [[48](#bib.bib48)]
    enabled domain adaptation among disparate distributions without relying on parametric
    assumptions. To prevent the pre-trained representation from being biased towards
    the source domain, RDC [[55](#bib.bib55)] constructed a non-linear subspace to
    minimize task-irrelevant features while also retaining more transferrable discriminative
    information through a hyperbolic tangent transformation. In other words, the source
    data was aligned between the original space and the non-linear subspace, resulting
    in more transferable discriminant information. CDTF [[78](#bib.bib78)] focused
    on aligning the support and query sets and proposed to transductive fine-tune
    the base model on a set of query images under the few-shot setting. The core idea
    was to implicitly guide query image segmentation using support labels.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: CDFS 中源领域和目标领域之间的分布不同，如图 [15](#S4.F15 "Figure 15 ‣ 4.4.2 Distribution Aligned
    Fine-tuning ‣ 4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning") 所示。为了应对这种差异的影响，CMT [[48](#bib.bib48)]
    提出了机制转移，这是一种在领域间数据生成机制不变的元分布场景。CMT [[48](#bib.bib48)] 使得在不同分布之间进行领域适应，而不依赖于参数假设。为了防止预训练表示对源领域的偏见，RDC
    [[55](#bib.bib55)] 构建了一个非线性子空间，以最小化与任务无关的特征，同时通过双曲正切变换保留更多可转移的判别信息。换句话说，源数据在原始空间和非线性子空间之间对齐，从而获得更多可转移的判别信息。CDTF
    [[78](#bib.bib78)] 侧重于对齐支持集和查询集，并提出在少样本设置下对一组查询图像进行传递性微调的基础模型。核心思想是通过支持标签隐式地指导查询图像的分割。
- en: As the base unit of meta-learning, the task distribution directly influences
    the model’s effectiveness. ATA [[52](#bib.bib52)] sought to improve the robustness
    of the inductive bias through task augmentation. The network adaptively generates
    ’challenging’ tasks using different inductive biases. Task-level augmentation
    can increase the diversity of training tasks and improve the model’s robustness
    under domain shift. At the feature level, researchers [[57](#bib.bib57)] attempted
    to transfer the source distributions into the target domain.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 作为元学习的基本单元，任务分布直接影响模型的效果。ATA [[52](#bib.bib52)] 试图通过任务增强提高归纳偏差的鲁棒性。网络通过不同的归纳偏差自适应地生成“具有挑战性”的任务。任务级增强可以增加训练任务的多样性，提高模型在领域转移下的鲁棒性。在特征层面，研究人员
    [[57](#bib.bib57)] 尝试将源分布转移到目标领域。
- en: 5 BENCHMARK
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基准
- en: 5.1 CDFS Classification.
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 CDFS 分类。
- en: BSCD-FSL [[6](#bib.bib6)] propose the Broader Study of Cross-Domain Few-Shot
    Learning (BSCD-FSL) benchmark, consisting of image data from diverse image acquisition
    methods. There are mini-ImageNet [[82](#bib.bib82)], CropDiease [[72](#bib.bib72)],
    EuroSAT [[73](#bib.bib73)], ISIC [[83](#bib.bib83)], ChestX [[74](#bib.bib74)]
    datasets in BSCD-FSL benchmark. Since the mini-ImageNet [[82](#bib.bib82)] has
    100 classes where each class has 600 images, BSCD-FSL selects it as the source
    domain and transfers the model knowledge to the rest domains.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: BSCD-FSL [[6](#bib.bib6)] 提出了跨领域少样本学习（BSCD-FSL）基准，其中包含来自多种图像获取方法的图像数据。BSCD-FSL基准包含mini-ImageNet
    [[82](#bib.bib82)]、CropDiease [[72](#bib.bib72)]、EuroSAT [[73](#bib.bib73)]、ISIC
    [[83](#bib.bib83)]、ChestX [[74](#bib.bib74)] 数据集。由于mini-ImageNet [[82](#bib.bib82)]
    具有100个类别，每个类别有600张图像，BSCD-FSL选择它作为源领域，并将模型知识转移到其他领域。
- en: Meanwhile, LFT [[7](#bib.bib7)] proposes a CDFS benchmark that contains five
    datasets. LFT design the leave-one-out experiments setting, which selects one
    domain from the CUB [[84](#bib.bib84)], Cars [[85](#bib.bib85)], Places [[86](#bib.bib86)],
    and Plantae [[87](#bib.bib87)] as the unseen domain for the evaluation, the mini-ImageNet
    [[82](#bib.bib82)] and the remaining domains serve as the seen domains for training
    the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，LFT [[7](#bib.bib7)] 提出了一个包含五个数据集的CDFS基准。LFT 设计了留一域实验设置，从CUB [[84](#bib.bib84)]、Cars
    [[85](#bib.bib85)]、Places [[86](#bib.bib86)] 和 Plantae [[87](#bib.bib87)] 中选择一个领域作为评估的未见领域，mini-ImageNet
    [[82](#bib.bib82)] 和其余领域作为训练模型的已见领域。
- en: For cross-domain few-shot datasets, Meta-Dataset [[25](#bib.bib25)] proposes
    a new benchmark for training and evaluating large-scale models, consists of diverse
    datasets and presents more realistic tasks. Meta-Dataset is divided by domain.
    That is, there is no cross-category between datasets. We show the benchmark used
    for all the CDFS classification methods in Tab. [2](#S5.T2 "Table 2 ‣ 5.1 CDFS
    Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning") and Tab. [3](#S5.T3 "Table 3 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). The most frequent
    experiment is the 5-way 1-shot setting. We summary the 5-way 1-shot transfer effect
    in the Tab. [2](#S5.T2 "Table 2 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning") and the 5-way k-shot transfer
    effect in the Tab. [3](#S5.T3 "Table 3 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跨领域少样本数据集，Meta-Dataset [[25](#bib.bib25)] 提出了一个新的基准，用于训练和评估大规模模型，包含多样的数据集，并呈现更现实的任务。Meta-Dataset
    按领域划分，即数据集之间没有跨类别。我们展示了所有 CDFS 分类方法使用的基准，如表 [2](#S5.T2 "表 2 ‣ 5.1 CDFS 分类。 ‣ 5
    基准 ‣ 深度视觉跨领域少样本学习调查") 和表 [3](#S5.T3 "表 3 ‣ 5.1 CDFS 分类。 ‣ 5 基准 ‣ 深度视觉跨领域少样本学习调查")。最常见的实验是
    5-way 1-shot 设置。我们总结了表 [2](#S5.T2 "表 2 ‣ 5.1 CDFS 分类。 ‣ 5 基准 ‣ 深度视觉跨领域少样本学习调查")
    中的 5-way 1-shot 转移效果，以及表 [3](#S5.T3 "表 3 ‣ 5.1 CDFS 分类。 ‣ 5 基准 ‣ 深度视觉跨领域少样本学习调查")
    中的 5-way k-shot 转移效果。
- en: 'Table 2: The CDFS classification result of 5-way 1-shot. ^† denotes the leave-one-out
    experiments, ^∗ denotes the target data is accessible, ^T denotes the transductive
    methods.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：5-way 1-shot 的 CDFS 分类结果。^† 表示留一法实验，^∗ 表示目标数据可用，^T 表示传递方法。
- en: '| 5-way 1-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 5-way 1-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
- en: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 35.89[±0.5] | 30.77[±0.5] | 49.86[±0.8]
    | 32.70[±0.6] | - | - | - | - |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 35.89[±0.5] | 30.77[±0.5] | 49.86[±0.8]
    | 32.70[±0.6] | - | - | - | - |'
- en: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 36.61[±0.5] | 29.82[±0.4] | 51.07[±0.7]
    | 34.48[±0.5] | - | - | - | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 36.61[±0.5] | 29.82[±0.4] | 51.07[±0.7]
    | 34.48[±0.5] | - | - | - | - |'
- en: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 42.44[±0.8] | 29.11[±0.6] | 48.64[±0.9]
    | 33.17[±0.6] | - | - | - | - |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 42.44[±0.8] | 29.11[±0.6] | 48.64[±0.9]
    | 33.17[±0.6] | - | - | - | - |'
- en: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 44.07[±0.8] | 28.63[±0.6] | 50.68[±0.8]
    | 33.14[±0.6] | - | - | - | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 44.07[±0.8] | 28.63[±0.6] | 50.68[±0.8]
    | 33.14[±0.6] | - | - | - | - |'
- en: '| GNN[[88](#bib.bib88)][(ICLR18)] | 45.69[±0.7] | 31.79[±0.5] | 53.10[±0.8]
    | 35.60[±0.6] | - | - | - | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GNN[[88](#bib.bib88)][(ICLR18)] | 45.69[±0.7] | 31.79[±0.5] | 53.10[±0.8]
    | 35.60[±0.6] | - | - | - | - |'
- en: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 47.47[±0.8] | 31.61[±0.5] | 55.77[±0.8]
    | 35.95[±0.6] | - | - | - | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 47.47[±0.8] | 31.61[±0.5] | 55.77[±0.8]
    | 35.95[±0.6] | - | - | - | - |'
- en: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 42.44[±0.4] | 29.65[±0.3] | 50.59[±0.5]
    | 34.80[±0.4] | - | - | - | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 42.44[±0.4] | 29.65[±0.3] | 50.59[±0.5]
    | 34.80[±0.4] | - | - | - | - |'
- en: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 42.88[±0.5] | 29.61[±0.4] | 53.07[±0.6]
    | 34.54[±0.4] | - | - | - | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 42.88[±0.5] | 29.61[±0.4] | 53.07[±0.6]
    | 34.54[±0.4] | - | - | - | - |'
- en: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 46.23[±0.4] | 32.66[±0.5] | 56.96[±0.5]
    | 38.23[±0.5] | - | - | - | - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 46.23[±0.4] | 32.66[±0.5] | 56.96[±0.5]
    | 38.23[±0.5] | - | - | - | - |'
- en: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 48.35[±0.5] | 32.35[±0.4] | 61.60[±0.6]
    | 38.48[±0.4] | - | - | - | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 48.35[±0.5] | 32.35[±0.4] | 61.60[±0.6]
    | 38.48[±0.4] | - | - | - | - |'
- en: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 48.29[±0.5] | 32.78[±0.4] | 54.83[±0.6]
    | 37.49[±0.4] | - | - | - | - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 48.29[±0.5] | 32.78[±0.4] | 54.83[±0.6]
    | 37.49[±0.4] | - | - | - | - |'
- en: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 51.51[±0.8] | 34.12 [±0.6] | 56.31 [±0.8]
    | 42.09 [±0.7] | - | - | - | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 51.51[±0.8] | 34.12 [±0.6] | 56.31 [±0.8]
    | 42.09 [±0.7] | - | - | - | - |'
- en: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 46.89[±0.5] | 47.74[±0.6] | 53.96[±0.6]
    | 30.93[±0.5] | - | - | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 46.89[±0.5] | 47.74[±0.6] | 53.96[±0.6]
    | 30.93[±0.5] | - | - | - | - |'
- en: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 52.04 [±0.7] | 34.84 [±0.6] | 57.57 [±0.8]
    | 42.05 [±0.7] | - | - | - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 52.04 [±0.7] | 34.84 [±0.6] | 57.57 [±0.8]
    | 42.05 [±0.7] | - | - | - | - |'
- en: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 46.15[±0.7] | 33.54 [±0.6] | 51.81[±0.7]
    | 39.97[±0.6] | - | - | - | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 46.15[±0.7] | 33.54 [±0.6] | 51.81[±0.7]
    | 39.97[±0.6] | - | - | - | - |'
- en: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 54.26 [±0.6] | 37.55 [±0.5] | 59.98
    [±0.6] | 45.69 [±0.6] | - | - | - | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 54.26 [±0.6] | 37.55 [±0.5] | 59.98
    [±0.6] | 45.69 [±0.6] | - | - | - | - |'
- en: '| MN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 46.37 [±0.5] | 35.65 [±0.5] | 54.92
    [±0.6] | 38.82 [±0.5] | - | - | - | - |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 46.37 [±0.5] | 35.65 [±0.5] | 54.92
    [±0.6] | 38.82 [±0.5] | - | - | - | - |'
- en: '| RN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 52.02 [±0.5] | 36.38 [±0.4] | 54.82
    [±0.6] | 36.74 [±0.5] | - | - | - | - |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| RN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 52.02 [±0.5] | 36.38 [±0.4] | 54.82
    [±0.6] | 36.74 [±0.5] | - | - | - | - |'
- en: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 43.10 [±0.4] | 32.34 [±0.3] | 50.53 [±0.5]
    | 33.19 [±0.3] | 63.29[±0.4] | 57.36[±0.30] | 32.09[±0.3] | 22.28[±0.2] |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 43.10 [±0.4] | 32.34 [±0.3] | 50.53 [±0.5]
    | 33.19 [±0.3] | 63.29[±0.4] | 57.36[±0.30] | 32.09[±0.3] | 22.28[±0.2] |'
- en: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 68.93[±0.8] | 64.14[±0.82]
    | 35.30[±0.59] | 22.48[±0.4] |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 68.93[±0.8] | 64.14[±0.82]
    | 35.30[±0.59] | 22.48[±0.4] |'
- en: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 50.25[±0.7] | 33.55[±0.6] | 57.75[±0.8]
    | 40.71[±0.7] | 70.80[±1.0] | 69.64[±1.0] | 33.35[±0.7] | 22.93[±0.5] |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 50.25[±0.7] | 33.55[±0.6] | 57.75[±0.8]
    | 40.71[±0.7] | 70.80[±1.0] | 69.64[±1.0] | 33.35[±0.7] | 22.93[±0.5] |'
- en: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 50.33[±0.7] | 32.69[±0.6] | 57.84[±0.8]
    | 38.25[±0.6] | 69.65[±1.0] | 65.50[±1.1] | 33.09[±0.7] | 22.39[±0.5] |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 50.33[±0.7] | 32.69[±0.6] | 57.84[±0.8]
    | 38.25[±0.6] | 69.65[±1.0] | 65.50[±1.1] | 33.09[±0.7] | 22.39[±0.5] |'
- en: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 41.02[±0.4] | 33.52[±0.4] | 54.66[±0.5]
    | 37.60[±0.4] | 60.71[±0.5] | 61.28[±0.5] | 32.32[±0.3] | 22.11[±0.2] |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 41.02[±0.4] | 33.52[±0.4] | 54.66[±0.5]
    | 37.60[±0.4] | 60.71[±0.5] | 61.28[±0.5] | 32.32[±0.3] | 22.11[±0.2] |'
- en: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 46.86[±0.5] | 34.25[±0.4] | 54.04[±0.6]
    | 36.76[±0.4] | 67.61[±0.5] | 63.12[±0.5] | 33.21[±0.3] | 22.92[±0.2] |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 46.86[±0.5] | 34.25[±0.4] | 54.04[±0.6]
    | 36.76[±0.4] | 67.61[±0.5] | 63.12[±0.5] | 33.21[±0.3] | 22.92[±0.2] |'
- en: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 50.85[±0.4] | 38.43[±0.4] | 60.29[±0.5]
    | 40.27[±0.4] | 72.44[±0.6] | 66.17[±0.4] | 34.25[±0.4] | 21.69[±0.1] |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 50.85[±0.4] | 38.43[±0.4] | 60.29[±0.5]
    | 40.27[±0.4] | 72.44[±0.6] | 66.17[±0.4] | 34.25[±0.4] | 21.69[±0.1] |'
- en: '| RDC[[55](#bib.bib55)][(CVPR22)] | 48.68[±0.5] | 38.26[±0.5] | 59.53[±0.5]
    | 42.29[±0.5] | 79.72[±0.5] | 65.58[±0.5] | 32.33[±0.3] | 22.77[±0.2] |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| RDC[[55](#bib.bib55)][(CVPR22)] | 48.68[±0.5] | 38.26[±0.5] | 59.53[±0.5]
    | 42.29[±0.5] | 79.72[±0.5] | 65.58[±0.5] | 32.33[±0.3] | 22.77[±0.2] |'
- en: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 51.20[±0.5] | 39.13[±0.5] | 61.50[±0.6]
    | 44.33[±0.6] | 86.33[±0.5] | 71.57[±0.5] | 35.84[±0.4] | 22.27[±0.2] |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 51.20[±0.5] | 39.13[±0.5] | 61.50[±0.6]
    | 44.33[±0.6] | 86.33[±0.5] | 71.57[±0.5] | 35.84[±0.4] | 22.27[±0.2] |'
- en: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 43.02[±0.4] | 31.79[±0.3] | 51.16[±0.5]
    | 33.72[±0.3] | 61.17[±0.5] | 55.69[±0.5] | 31.13[±0.3] | 22.14[±0.2] |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 43.02[±0.4] | 31.79[±0.3] | 51.16[±0.5]
    | 33.72[±0.3] | 61.17[±0.5] | 55.69[±0.5] | 31.13[±0.3] | 22.14[±0.2] |'
- en: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 45.00[±0.5] | 33.61[±0.4] | 53.57[±0.5]
    | 34.42[±0.4] | 67.47[±0.5] | 61.35[±0.5] | 33.21[±0.4] | 22.10[±0.2] |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 45.00[±0.5] | 33.61[±0.4] | 53.57[±0.5]
    | 34.42[±0.4] | 67.47[±0.5] | 61.35[±0.5] | 33.21[±0.4] | 22.10[±0.2] |'
- en: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 50.26[±0.5] | 34.18[±0.4] | 57.03[±0.5]
    | 39.83[±0.4] | 77.82[±0.5] | 65.94[±0.5] | 34.70[±0.4] | 21.67[±0.2] |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 50.26[±0.5] | 34.18[±0.4] | 57.03[±0.5]
    | 39.83[±0.4] | 77.82[±0.5] | 65.94[±0.5] | 34.70[±0.4] | 21.67[±0.2] |'
- en: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 65.05[±0.8] | 49.53[±0.8]
    | 60.36[±0.9] | 52.89[±0.8] | - | - | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 65.05[±0.8] | 49.53[±0.8]
    | 60.36[±0.9] | 52.89[±0.8] | - | - | - | - |'
- en: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 64.80[±0.3] | 50.70[±0.2]
    | 61.88[±0.3] | 52.39[±0.3] | - | - | - | - |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 64.80[±0.3] | 50.70[±0.2]
    | 61.88[±0.3] | 52.39[±0.3] | - | - | - | - |'
- en: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 63.24[±0.8] | 51.31[±0.8]
    | 58.22[±0.8] | 51.03[±0.8] | - | - | - | - |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 63.24[±0.8] | 51.31[±0.8]
    | 58.22[±0.8] | 51.03[±0.8] | - | - | - | - |'
- en: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 63.85[±0.4] | 53.10[±0.4] | 59.39[±0.4]
    | 51.28[±0.4] | 70.21[±0.4] | 91.07[±0.4] | 70.90[±0.6] | 53.07[±0.4] |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 63.85[±0.4] | 53.10[±0.4] | 59.39[±0.4]
    | 51.28[±0.4] | 70.21[±0.4] | 91.07[±0.4] | 70.90[±0.6] | 53.07[±0.4] |'
- en: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 82.14[±0.8] | 73.14[±0.8]
    | 34.66[±0.6] | 23.38[±0.4] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 82.14[±0.8] | 73.14[±0.8]
    | 34.66[±0.6] | 23.38[±0.4] |'
- en: 'Table 3: The CDFS classification result of 5-way k-shot. ^† denotes the leave-one-out
    experiments, ^∗ denotes the target data is accessible, ^T denotes the transductive
    methods.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 3: The CDFS classification result of 5-way k-shot. ^† denotes the leave-one-out
    experiments, ^∗ denotes the target data is accessible, ^T denotes the transductive
    methods.'
- en: '| 5-way 5-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 5-way 5-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
- en: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 51.37[±0.8] | 38.99[±0.6] | 63.16[±0.8]
    | 46.53[±0.9] | - | - | - | - |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 51.37[±0.8] | 38.99[±0.6] | 63.16[±0.8]
    | 46.53[±0.9] | - | - | - | - |'
- en: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 55.23[±0.8] | 41.24[±0.6] | 64.55[±0.8]
    | 41.69[±0.6] | - | - | - | - |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 55.23[±0.8] | 41.24[±0.6] | 64.55[±0.8]
    | 41.69[±0.6] | - | - | - | - |'
- en: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 57.77[±0.7] | 37.33[±0.7] | 63.32[±0.8]
    | 44.00[±0.6] | - | - | - | - |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 57.77[±0.7] | 37.33[±0.7] | 63.32[±0.8]
    | 44.00[±0.6] | - | - | - | - |'
- en: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 59.46[±0.7] | 39.91[±0.7] | 66.28[±0.7]
    | 45.08[±0.6] | - | - | - | - |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 59.46[±0.7] | 39.91[±0.7] | 66.28[±0.7]
    | 45.08[±0.6] | - | - | - | - |'
- en: '| GNN[[88](#bib.bib88)][(ICLR18)] | 62.25[±0.6] | 44.28[±0.6] | 70.84[±0.7]
    | 52.53[±0.6] | - | - | - | - |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GNN[[88](#bib.bib88)][(ICLR18)] | 62.25[±0.6] | 44.28[±0.6] | 70.84[±0.7]
    | 52.53[±0.6] | - | - | - | - |'
- en: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 66.98[±0.7] | 44.90[±0.6] | 73.94[±0.7]
    | 53.85[±0.6] | - | - | - | - |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 66.98[±0.7] | 44.90[±0.6] | 73.94[±0.7]
    | 53.85[±0.6] | - | - | - | - |'
- en: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 59.30[±0.4] | 39.19[±0.4] | 66.90[±0.4]
    | 48.09[±0.4] | - | - | - | - |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 59.30[±0.4] | 39.19[±0.4] | 66.90[±0.4]
    | 48.09[±0.4] | - | - | - | - |'
- en: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 59.22[±0.4] | 38.31[±0.4] | 68.25[±0.4]
    | 47.67[±0.4] | - | - | - | - |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 59.22[±0.4] | 38.31[±0.4] | 68.25[±0.4]
    | 47.67[±0.4] | - | - | - | - |'
- en: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 66.58[±0.3] | 43.86[±0.4] | 74.91[±0.4]
    | 53.25[±0.4] | - | - | - | - |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 66.58[±0.3] | 43.86[±0.4] | 74.91[±0.4]
    | 53.25[±0.4] | - | - | - | - |'
- en: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 66.57[±0.4] | 42.57[±0.4] | 76.90[±0.4]
    | 51.63[±0.4] | - | - | - | - |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 66.57[±0.4] | 42.57[±0.4] | 76.90[±0.4]
    | 51.63[±0.4] | - | - | - | - |'
- en: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 64.44[±0.5] | 46.20[±0.5] | 74.45[±0.5]
    | 54.46[±0.5] | - | - | - | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 64.44[±0.5] | 46.20[±0.5] | 74.45[±0.5]
    | 54.46[±0.5] | - | - | - | - |'
- en: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 73.11[±0.7] | 49.88[±0.7] | 77.05[±0.7]
    | 58.84[±0.7] | - | - | - | - |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 73.11[±0.7] | 49.88[±0.7] | 77.05[±0.7]
    | 58.84[±0.7] | - | - | - | - |'
- en: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 66.15[±0.6] | 60.17[±0.6] | 88.08[±0.5]
    | 42.96[±0.6] | - | - | - | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 66.15[±0.6] | 60.17[±0.6] | 88.08[±0.5]
    | 42.96[±0.6] | - | - | - | - |'
- en: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 73.94[±0.7] | 50.63[±0.7] | 76.68[±0.6]
    | 62.14[±0.7] | - | - | - | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 73.94[±0.7] | 50.63[±0.7] | 76.68[±0.6]
    | 62.14[±0.7] | - | - | - | - |'
- en: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 69.75[±0.7] | 49.55[±0.7] | 69.38[±0.7]
    | 58.76[±0.6] | - | - | - | - |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 69.75[±0.7] | 49.55[±0.7] | 69.38[±0.7]
    | 58.76[±0.6] | - | - | - | - |'
- en: '| DFTL[[76](#bib.bib76)][(ICAICA21)] | 69.35[±0.7] | 47.91[±0.6] | 69.12[±0.8]
    | 58.12[±0.7] | - | - | - | - |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| DFTL[[76](#bib.bib76)][(ICAICA21)] | 69.35[±0.7] | 47.91[±0.6] | 69.12[±0.8]
    | 58.12[±0.7] | - | - | - | - |'
- en: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 77.54[±0.6] | 56.78[±0.6] | 78.84[±0.6]
    | 65.44[±0.6] | - | - | - | - |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 77.54[±0.6] | 56.78[±0.6] | 78.84[±0.6]
    | 65.44[±0.6] | - | - | - | - |'
- en: '| MNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 67.31[±0.5] | 47.36[±0.5] | 68.14[±0.6]
    | 52.28[±0.5] | - | - | - | - |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| MNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 67.31[±0.5] | 47.36[±0.5] | 68.14[±0.6]
    | 52.28[±0.5] | - | - | - | - |'
- en: '| RNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 68.39[±0.5] | 46.92[±0.5] | 69.87[±0.5]
    | 58.64[±0.5] | - | - | - | - |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| RNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 68.39[±0.5] | 46.92[±0.5] | 69.87[±0.5]
    | 58.64[±0.5] | - | - | - | - |'
- en: '| MAP[[80](#bib.bib80)][(Arxiv)] | 67.92[±1.1] | 51.64[±1.2] | 75.94[±1.0]
    | 58.45[±1.2] | 90.29[±1.6] | 82.76[±2.0] | 47.85[±2.0] | 24.79[±1.2] |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MAP[[80](#bib.bib80)][(Arxiv)] | 67.92[±1.1] | 51.64[±1.2] | 75.94[±1.0]
    | 58.45[±1.2] | 90.29[±1.6] | 82.76[±2.0] | 47.85[±2.0] | 24.79[±1.2] |'
- en: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 62.94[±0.4] | 43.26[±0.4] | 66.74[±0.4]
    | 46.92[±0.3] | 78.62[±0.4] | 75.84[±0.4] | 44.42[±0.3] | 24.79[±0.2] |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 62.94[±0.4] | 43.26[±0.4] | 66.74[±0.4]
    | 46.92[±0.3] | 78.62[±0.4] | 75.84[±0.4] | 44.42[±0.3] | 24.79[±0.2] |'
- en: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 59.46[±0.4] | 46.13[±0.4] | 68.87[±0.4]
    | 52.43[±0.4] | 80.07[±0.4] | 69.63[±0.5] | 39.88[±0.3] | 23.18[±0.2] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 59.46[±0.4] | 46.13[±0.4] | 68.87[±0.4]
    | 52.43[±0.4] | 80.07[±0.4] | 69.63[±0.5] | 39.88[±0.3] | 23.18[±0.2] |'
- en: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 68.25[±0.5] | 49.28[±0.5] | 76.21[±0.5]
    | 54.26[±0.4] | 88.06[±0.3] | 85.58[±0.4] | 46.01[±0.4] | 25.02[±0.2] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 68.25[±0.5] | 49.28[±0.5] | 76.21[±0.5]
    | 54.26[±0.4] | 88.06[±0.3] | 85.58[±0.4] | 46.01[±0.4] | 25.02[±0.2] |'
- en: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 65.86[±0.4] | 47.89[±0.4] | 72.81[±0.4]
    | 55.67[±0.4] | 85.69[±0.4] | 80.12[±0.4] | 46.29[±0.3] | 23.47[±0.2] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 65.86[±0.4] | 47.89[±0.4] | 72.81[±0.4]
    | 55.67[±0.4] | 85.69[±0.4] | 80.12[±0.4] | 46.29[±0.3] | 23.47[±0.2] |'
- en: '| RDC[[55](#bib.bib55)][(CVPR22)] | 64.36[±0.4] | 52.15[±0.4] | 73.24[±0.4]
    | 57.50[±0.4] | 88.90[±0.3] | 77.15[±0.4] | 41.28[±0.3] | 25.91[±0.2] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| RDC[[55](#bib.bib55)][(CVPR22)] | 64.36[±0.4] | 52.15[±0.4] | 73.24[±0.4]
    | 57.50[±0.4] | 88.90[±0.3] | 77.15[±0.4] | 41.28[±0.3] | 25.91[±0.2] |'
- en: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 67.77[±0.4] | 53.75[±0.5] | 74.65[±0.4]
    | 60.63[±0.4] | 93.55[±0.3] | 84.67[±0.3] | 49.06[±0.3] | 25.48[±0.2] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 67.77[±0.4] | 53.75[±0.5] | 74.65[±0.4]
    | 60.63[±0.4] | 93.55[±0.3] | 84.67[±0.3] | 49.06[±0.3] | 25.48[±0.2] |'
- en: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 59.36[±0.4] | 42.95[±0.4] | 66.90[±0.4]
    | 45.32[±0.3] | 78.20[±0.4] | 71.02[±0.4] | 40.38[±0.3] | 24.43[±0.2] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 59.36[±0.4] | 42.95[±0.4] | 66.90[±0.4]
    | 45.32[±0.3] | 78.20[±0.4] | 71.02[±0.4] | 40.38[±0.3] | 24.43[±0.2] |'
- en: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 66.22[±0.5] | 49.14[±0.4] | 75.48[±0.4]
    | 52.69[±0.4] | 90.59[±0.3] | 83.75[±0.4] | 44.91[±0.4] | 24.32[±0.4] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 66.22[±0.5] | 49.14[±0.4] | 75.48[±0.4]
    | 52.69[±0.4] | 90.59[±0.3] | 83.75[±0.4] | 44.91[±0.4] | 24.32[±0.4] |'
- en: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 65.31[±0.4] | 46.95[±0.4] | 72.12[±0.4]
    | 55.08[±0.4] | 88.15[±0.5] | 79.47[±0.3] | 45.83[±0.3] | 23.60[±0.2] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 65.31[±0.4] | 46.95[±0.4] | 72.12[±0.4]
    | 55.08[±0.4] | 88.15[±0.5] | 79.47[±0.3] | 45.83[±0.3] | 23.60[±0.2] |'
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 76.00[±0.7] | 61.11[±0.8] | 73.40[±0.7]
    | 65.66[±0.8] | 96.09[±0.4] | 87.53[±0.5] | 56.85[±0.7] | 28.73[±0.5] |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 76.00[±0.7] | 61.11[±0.8] | 73.40[±0.7]
    | 65.66[±0.8] | 96.09[±0.4] | 87.53[±0.5] | 56.85[±0.7] | 28.73[±0.5] |'
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 93.42[±0.6] | 85.19[±0.7]
    | 45.39[±0.7] | 25.32[±0.5] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 93.42[±0.6] | 85.19[±0.7]
    | 45.39[±0.7] | 25.32[±0.5] |'
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 95.28[±0.4] | 85.34[±0.6]
    | 53.84[±0.7] | 27.98[±0.5] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 95.28[±0.4] | 85.34[±0.6]
    | 53.84[±0.7] | 27.98[±0.5] |'
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 96.59[±0.3] | 88.13[±0.5]
    | 57.40[±0.7] | 29.72[±0.5] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 96.59[±0.3] | 88.13[±0.5]
    | 57.40[±0.7] | 29.72[±0.5] |'
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 96.01[±0.4] | 85.93[±0.7]
    | 50.68[±0.8] | 25.99[±0.5] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 96.01[±0.4] | 85.93[±0.7]
    | 50.68[±0.8] | 25.99[±0.5] |'
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 88.09[±0.6] | 81.10[±0.6]
    | 43.74[±0.6] | 26.80[±0.5] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 88.09[±0.6] | 81.10[±0.6]
    | 43.74[±0.6] | 26.80[±0.5] |'
- en: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 90.75[±0.5] | 82.36[±0.6]
    | 51.68[±0.6] | 26.76[±0.4] |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 90.75[±0.5] | 82.36[±0.6]
    | 51.68[±0.6] | 26.76[±0.4] |'
- en: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 70.31[±0.7] | 46.11[±0.7] | 76.88[±0.6]
    | 57.72[±0.6] | 89.70[±0.6] | 85.22[±0.7] | 44.93[±0.7] | 25.63[±0.5] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 70.31[±0.7] | 46.11[±0.7] | 76.88[±0.6]
    | 57.72[±0.6] | 89.70[±0.6] | 85.22[±0.7] | 44.93[±0.7] | 25.63[±0.5] |'
- en: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 71.16[±0.7] | 47.78[±0.7] | 78.19[±0.6]
    | 57.85[±0.7] | 91.23[±0.5] | 84.84[±0.7] | 46.00[±0.7] | 25.27[±0.5] |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 71.16[±0.7] | 47.78[±0.7] | 78.19[±0.6]
    | 57.85[±0.7] | 91.23[±0.5] | 84.84[±0.7] | 46.00[±0.7] | 25.27[±0.5] |'
- en: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 83.17[±0.6] | 69.17[±0.7]
    | 80.45[±0.6] | 72.87[±0.7] | - | - | - | - |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 83.17[±0.6] | 69.17[±0.7]
    | 80.45[±0.6] | 72.87[±0.7] | - | - | - | - |'
- en: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 84.21[±0.2] | 70.99[±0.2]
    | 81.62[±0.2] | 71.78[±0.2] | - | - | - | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 84.21[±0.2] | 70.99[±0.2]
    | 81.62[±0.2] | 71.78[±0.2] | - | - | - | - |'
- en: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 79.46[±0.6] | 66.52[±0.7]
    | 78.92[±0.6] | 69.22[±0.7] | - | - | - | - |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 79.46[±0.6] | 66.52[±0.7]
    | 78.92[±0.6] | 69.22[±0.7] | - | - | - | - |'
- en: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 80.49[±0.3] | 71.80[±0.3] | 78.80[±0.3]
    | 69.45[±0.3] | 87.32[±0.2] | 95.87[±0.2] | 84.07[±0.4] | 55.37[±0.4] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 80.49[±0.3] | 71.80[±0.3] | 78.80[±0.3]
    | 69.45[±0.3] | 87.32[±0.2] | 95.87[±0.2] | 84.07[±0.4] | 55.37[±0.4] |'
- en: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 95.54[±0.4] | 89.07[±0.5]
    | 49.36[±0.6] | 28.31[±0.5] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 95.54[±0.4] | 89.07[±0.5]
    | 49.36[±0.6] | 28.31[±0.5] |'
- en: '| 5-way 20-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 5-way 20-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.16[±0.1] | 94.72[±0.3]
    | 68.09[±0.6] | 38.34[±0.5] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.16[±0.1] | 94.72[±0.3]
    | 68.09[±0.6] | 38.34[±0.5] |'
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 98.51[±0.2] | 92.42[±0.4]
    | 65.43[±0.6] | 37.11[±0.5] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 98.51[±0.2] | 92.42[±0.4]
    | 65.43[±0.6] | 37.11[±0.5] |'
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.19[±0.1] | 95.18[±0.4]
    | 68.58[±0.7] | 33.47[±0.5] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.19[±0.1] | 95.18[±0.4]
    | 68.58[±0.7] | 33.47[±0.5] |'
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 94.95[±0.3] | 88.54[±0.5]
    | 54.61[±0.5] | 32.90[±0.5] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 94.95[±0.3] | 88.54[±0.5]
    | 54.61[±0.5] | 32.90[±0.5] |'
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 91.08[±0.4] | 85.04[±0.5] | 83.00[±0.6]
    | 81.54[±0.6] | 99.20[±0.1] | 94.21[±0.3] | 67.45[±0.6] | 36.14[±0.5] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 91.08[±0.4] | 85.04[±0.5] | 83.00[±0.6]
    | 81.54[±0.6] | 99.20[±0.1] | 94.21[±0.3] | 67.45[±0.6] | 36.14[±0.5] |'
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.49[±0.4] | 87.87[±0.5]
    | 53.15[±0.6] | 29.17[±0.5] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.49[±0.4] | 87.87[±0.5]
    | 53.15[±0.6] | 29.17[±0.5] |'
- en: '| 5-way 50-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 5-way 50-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.73[±0.1] | 96.89[±0.2]
    | 74.08[±0.6] | 44.43[±0.6] |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.73[±0.1] | 96.89[±0.2]
    | 74.08[±0.6] | 44.43[±0.6] |'
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 99.28[±0.1] | 95.63[±0.3]
    | 71.29[±0.8] | 43.43[±0.7] |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 99.28[±0.1] | 95.63[±0.3]
    | 71.29[±0.8] | 43.43[±0.7] |'
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.75[±0.1] | 97.73[±0.3]
    | 75.55[±0.6] | 38.37[±0.6] |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.75[±0.1] | 97.73[±0.3]
    | 75.55[±0.6] | 38.37[±0.6] |'
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 96.27[±0.3] | 91.40[±0.4]
    | 60.86[±0.5] | 37.05[±0.5] |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 96.27[±0.3] | 91.40[±0.4]
    | 60.86[±0.5] | 37.05[±0.5] |'
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 95.41[±0.5] |  | 86.53[±0.8] | 85.99[±0.7]
    | 99.70[±0.1] | 96.50[±0.3] | 73.00[±0.6] | 41.80[±0.7] |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 95.41[±0.5] |  | 86.53[±0.8] | 85.99[±0.7]
    | 99.70[±0.1] | 96.50[±0.3] | 73.00[±0.6] | 41.80[±0.7] |'
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.88[±0.4] | 89.07[±0.4]
    | 56.68[±0.6] | 31.75[±0.5] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.88[±0.4] | 89.07[±0.4]
    | 56.68[±0.6] | 31.75[±0.5] |'
- en: CDFS Object-Detection. MoF-SOD [[27](#bib.bib27)] propose a Multi-domain Few-Shot
    Object Detection benchmark consisting of 10 datasets from a wide range of domains
    to evaluate few-shot object-detection algorithms, as shown in the Fig. [16](#S5.F16
    "Figure 16 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). Empirical results show several keys actors that
    have yet to be explored in previous works. Under the proposed benchmark, MoF-SOD
    [[27](#bib.bib27)] conducted extensive experiments on the impact of freezing parameters,
    different architectures, and different pre-training datasets.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: CDFS 对象检测。MoF-SOD [[27](#bib.bib27)] 提出了一个包含来自广泛领域的 10 个数据集的多领域少样本对象检测基准，以评估少样本对象检测算法，如图
    [16](#S5.F16 "图 16 ‣ 5.1 CDFS 分类。 ‣ 5 基准 ‣ 深度视觉跨领域少样本学习调查") 所示。实证结果显示了许多在以前的工作中尚未探索的关键因素。在提出的基准下，MoF-SOD
    [[27](#bib.bib27)] 对冻结参数、不同架构和不同预训练数据集的影响进行了广泛实验。
- en: '![Refer to caption](img/aaf9e56f9bb9c348a9a0f13b6def5ed5.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aaf9e56f9bb9c348a9a0f13b6def5ed5.png)'
- en: 'Figure 16: Sample images in the proposed MoF-SOD [[27](#bib.bib27)] benchmark.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 提出的 MoF-SOD [[27](#bib.bib27)] 基准中的样本图像。'
- en: CDFS Segmentation. PATNet [[26](#bib.bib26)] proposes a cross-domain few-shot
    segmentation benchmark. Evaluate both representative few-shot segmentation and
    transfer learning-based methods on the proposed benchmark. And propose a novel
    Pyramid-Anchor-Transformation based few-shot segmentation network, in which domain-specific
    features are transformed into domain-agnostic ones for downstream segmentation
    modules to fast adapt to unseen domains. And RD [[5](#bib.bib5)] also proposes
    a cross-domain few-shot segmentation task using different public datasets to validate
    the model effect. As the most famous datasets, we summarize the segmentation transfer
    effect from COCO-$2-^{i}$ to PASCAL-$5^{i}$ as shown in Tab. [4](#S5.T4 "Table
    4 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning").
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: CDFS 分割。PATNet [[26](#bib.bib26)] 提出了一个跨领域少样本分割基准。在提出的基准上评估了代表性的少样本分割和基于迁移学习的方法。并提出了一种基于金字塔锚点变换的少样本分割网络，其中领域特定特征被转化为领域无关特征，以便下游分割模块能够快速适应未见领域。RD
    [[5](#bib.bib5)] 还提出了一个使用不同公共数据集的跨领域少样本分割任务，以验证模型效果。作为最著名的数据集，我们总结了从 COCO-$2-^{i}$
    到 PASCAL-$5^{i}$ 的分割迁移效果，如表 [4](#S5.T4 "表 4 ‣ 5.1 CDFS 分类。 ‣ 5 基准 ‣ 深度视觉跨领域少样本学习调查")
    所示。
- en: 'Table 4: Cross-domain few-shot semantic segmentation results on COCO-$20^{i}$
    to PASCAL-$5^{i}$ task.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: COCO-$20^{i}$ 到 PASCAL-$5^{i}$ 任务的跨领域少样本语义分割结果。'
- en: '| COCO-$20^{i}$ to PASCAL-$5^{i}$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| COCO-$20^{i}$ 到 PASCAL-$5^{i}$ |'
- en: '| Backbone | Method | 1-shot | 5-shot |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Backbone | 方法 | 1-shot | 5-shot |'
- en: '| split0 | split1 | split2 | split3 | Mean | split0 | split1 | split2 | split3
    | Mean |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| split0 | split1 | split2 | split3 | 平均 | split0 | split1 | split2 | split3
    | 平均 |'
- en: '| ResNet50 | RPMMs[[90](#bib.bib90)][(ECCV20)] | 36.3 | 55.0 | 52.5 | 54.6
    | 49.6 | 40.2 | 58.0 | 55.2 | 61.8 | 53.8 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| ResNet50 | RPMMs[[90](#bib.bib90)][(ECCV20)] | 36.3 | 55.0 | 52.5 | 54.6
    | 49.6 | 40.2 | 58.0 | 55.2 | 61.8 | 53.8 |'
- en: '| RePRI[[91](#bib.bib91)][(CVPR21)] | 52.4 | 64.3 | 65.3 | 71.5 | 63.3 | 57.0
    | 68.0 | 70.4 | 76.2 | 67.9 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| RePRI[[91](#bib.bib91)][(CVPR21)] | 52.4 | 64.3 | 65.3 | 71.5 | 63.3 | 57.0
    | 68.0 | 70.4 | 76.2 | 67.9 |'
- en: '| ASGNet[[92](#bib.bib92)][(CVPR21)] | 42.5 | 58.7 | 65.5 | 63.0 | 57.4 | 53.7
    | 69.8 | 67.1 | 75.9 | 66.6 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| ASGNet[[92](#bib.bib92)][(CVPR21)] | 42.5 | 58.7 | 65.5 | 63.0 | 57.4 | 53.7
    | 69.8 | 67.1 | 75.9 | 66.6 |'
- en: '| PFENet[[93](#bib.bib93)][(TPAMI)] | - | - | - | - | 60.8 | - | - | - | -
    | 61.9 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| PFENet[[93](#bib.bib93)][(TPAMI)] | - | - | - | - | 60.8 | - | - | - | -
    | 61.9 |'
- en: '| CWT[[94](#bib.bib94)][(ICCV21)] | 53.5 | 59.2 | 60.2 | 64.9 | 59.4 | 60.3
    | 65.8 | 67.1 | 72.8 | 66.5 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| CWT[[94](#bib.bib94)][(ICCV21)] | 53.5 | 59.2 | 60.2 | 64.9 | 59.4 | 60.3
    | 65.8 | 67.1 | 72.8 | 66.5 |'
- en: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 48.7 | 61.5 | 63.0 | 72.8 | 61.5 | 58.2
    | 65.9 | 71.8 | 77.9 | 68.4 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 48.7 | 61.5 | 63.0 | 72.8 | 61.5 | 58.2
    | 65.9 | 71.8 | 77.9 | 68.4 |'
- en: '| RD[[5](#bib.bib5)][(CVPR22)] | 57.4 | 62.2 | 68.0 | 74.8 | 65.6 | 65.7 |
    69.2 | 70.8 | 75.0 | 70.1 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| RD[[5](#bib.bib5)][(CVPR22)] | 57.4 | 62.2 | 68.0 | 74.8 | 65.6 | 65.7 |
    69.2 | 70.8 | 75.0 | 70.1 |'
- en: '| ResNet101 | SCL[[96](#bib.bib96)][(CVPR21)] | 43.1 | 60.3 | 66.1 | 68.1 |
    59.4 | 43.3 | 61.2 | 66.5 | 70.4 | 60.3 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| ResNet101 | SCL[[96](#bib.bib96)][(CVPR21)] | 43.1 | 60.3 | 66.1 | 68.1 |
    59.4 | 43.3 | 61.2 | 66.5 | 70.4 | 60.3 |'
- en: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 46.3 | 64.7 | 67.7 | 74.2 | 63.2 | 59.1
    | 69.0 | 73.4 | 78.7 | 70.0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 46.3 | 64.7 | 67.7 | 74.2 | 63.2 | 59.1
    | 69.0 | 73.4 | 78.7 | 70.0 |'
- en: '| RD[[5](#bib.bib5)][(CVPR22)] | 59.4 | 64.3 | 70.8 | 72.0 | 66.6 | 67.2 |
    72.7 | 72.0 | 78.9 | 72.7 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| RD[[5](#bib.bib5)][(CVPR22)] | 59.4 | 64.3 | 70.8 | 72.0 | 66.6 | 67.2 |
    72.7 | 72.0 | 78.9 | 72.7 |'
- en: 6 Application OF CDFS
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 CDFS 的应用
- en: The CDFS focuses on the domain problem of few-shot. It has been used in various
    applications, as shown in Fig. [17](#S6.F17 "Figure 17 ‣ 6 Application OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). We summary the different
    CDFS methods for object detection and segmentation. For the other applications,
    we also provide a detailed survey.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: CDFS 关注少样本的领域问题。它已在各种应用中得到使用，如图 [17](#S6.F17 "图 17 ‣ 6 CDFS 应用 ‣ 深度视觉跨领域少样本学习调查")
    所示。我们总结了用于物体检测和分割的不同 CDFS 方法。对于其他应用，我们也提供了详细的调查。
- en: '![Refer to caption](img/b33afdf48d281db3a115fa0a1e3e5efb.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b33afdf48d281db3a115fa0a1e3e5efb.png)'
- en: 'Figure 17: The Cross-Domain Few-shot related applications.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：跨领域少样本相关应用。
- en: 6.1 CDFS for Object Detection
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 CDFS 在物体检测中的应用
- en: '![Refer to caption](img/c0c1953f61fb5b4b4dd648a7f29487a0.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c0c1953f61fb5b4b4dd648a7f29487a0.png)'
- en: 'Figure 18: AcroFOD [[44](#bib.bib44)] address the task of cross-domain few-shot
    object detection. Top: Existing feature-aligning based methods fail to extract
    discriminative features within limited labeled data in the target domain. Bottom:
    Our method filters (thick black dotted line) source data that is far away from
    the target domain.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：AcroFOD [[44](#bib.bib44)] 解决了跨领域少样本物体检测任务。上图：现有的特征对齐方法在目标领域的有限标注数据中无法提取出判别性特征。下图：我们的方法筛选（粗黑虚线）远离目标领域的源数据。
- en: For CDFS object detection, image enhancement is still the most direct choice.
    CutMix [[47](#bib.bib47)] directly pastes data from the target domain to enhance
    feature diversity. However, due to the scarcity of target domain data, direct
    image mixing will bring unnecessary noise impact. To alleviate this situation,
    AcroFOD [[44](#bib.bib44)] proposes a filtering mechanism to filter the enhanced
    image, as shown in Fig. [18](#S6.F18 "Figure 18 ‣ 6.1 CDFS for Object Detection
    ‣ 6 Application OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
    In the training process, the model continuously measures the distance between
    the enhanced and target features, retaining the most similar top-k. Finally, the
    enhanced features are used for object detection learning. The generation strategy
    can still enrich the feature distribution. For this, OA-FSUI2IT [[41](#bib.bib41)]
    uses the unlabeled data of the target domain to generate new style images for
    model training. The style of the generated images is consistent with the target
    domain, and the content is consistent with the source domain. Domain adaptive
    training ensures the reproducibility of the learned content, improves the interpretability
    of the model, and self-supervised content consistency improves the model generalization.
    CDTL [[97](#bib.bib97)] is one of the first to study the problem of few-shot SAR
    image ship detection, which has great practical value but is less studied than
    the classification problem of SAR images under few-shot conditions. CDTL [[97](#bib.bib97)]
    design a two-stage few-shot SAR ship detection model based on transfer learning
    using the similarity information of images of optical ships.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '对于CDFS目标检测，图像增强仍然是最直接的选择。CutMix [[47](#bib.bib47)] 直接将目标领域的数据粘贴到图像中以增强特征多样性。然而，由于目标领域数据的稀缺，直接图像混合会带来不必要的噪声影响。为了缓解这种情况，AcroFOD
    [[44](#bib.bib44)] 提出了一个过滤机制来过滤增强后的图像，如图[18](#S6.F18 "Figure 18 ‣ 6.1 CDFS for
    Object Detection ‣ 6 Application OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning")所示。在训练过程中，模型不断测量增强特征与目标特征之间的距离，保留最相似的前k个特征。最终，增强特征被用于目标检测学习。生成策略仍然可以丰富特征分布。为此，OA-FSUI2IT
    [[41](#bib.bib41)] 使用目标领域的未标记数据生成新的风格图像用于模型训练。生成图像的风格与目标领域一致，内容与源领域一致。领域自适应训练确保了学习内容的可重复性，提高了模型的可解释性，自监督内容一致性提高了模型的泛化能力。CDTL
    [[97](#bib.bib97)] 是首批研究少样本SAR图像船舶检测问题的工作之一，这具有很大的实际价值，但相比于少样本条件下SAR图像分类问题，研究较少。CDTL
    [[97](#bib.bib97)] 设计了一个基于迁移学习的两阶段少样本SAR船舶检测模型，利用光学船舶图像的相似性信息。 '
- en: 6.2 CDFS for Segmentation
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 CDFS用于分割
- en: 'For CDFS segmentation, RD [[5](#bib.bib5)] proposes a domain enhancement strategy
    of memory mechanism. During training, source domain data continuously stores domain-style
    information in memory. During testing, source information stored in memory is
    loaded into target domain feature enhancement. RD [[5](#bib.bib5)] can directly
    reduce the domain difference and has been verified on typically partitioned datasets.
    For the task of semantic segmentation in autonomous driving applications, PixDA
    [[67](#bib.bib67)] propose a novel pixel-by-pixel domain adversarial loss following
    three criteria: (i) align the source and the target domain for each pixel, (ii)
    avoid negative transfer on the correctly represented pixels, and (iii) regularize
    the training of infrequent classes to avoid overfitting. CDTF [[78](#bib.bib78)]
    through aligning the support and query prototypes to achieve the cross-domain
    few-shot segmentation. By aligning query and support prototypes with an uncertainty-aware
    contrastive loss, and using a supervised cross-entropy loss and an unsupervised
    boundary loss as regularizations, CDTF [[78](#bib.bib78)] could generalize the
    base model to the target domain without additional labels. CD-FSS [[98](#bib.bib98)]
    propose a cross-domain few-shot segmentation framework, which enables the model
    to leverage the learning ability obtained from the natural domain to facilitate
    rare-disease skin lesion segmentation with limited data of common diseases.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CDFS分割，RD [[5](#bib.bib5)]提出了一种记忆机制的领域增强策略。在训练过程中，源领域数据不断将领域样式信息存储在记忆中。在测试过程中，将存储在记忆中的源信息加载到目标领域特征增强中。RD
    [[5](#bib.bib5)]可以直接减少领域差异，并已在典型划分数据集上验证。对于自动驾驶应用中的语义分割任务，PixDA [[67](#bib.bib67)]提出了一种新颖的逐像素领域对抗损失，遵循三个标准：（i）对每个像素对齐源领域和目标领域，（ii）避免在正确表示的像素上进行负迁移，（iii）正则化不常见类别的训练以避免过拟合。CDTF
    [[78](#bib.bib78)]通过对齐支持和查询原型实现跨领域少样本分割。通过用不确定性感知对比损失对齐查询和支持原型，并使用监督交叉熵损失和无监督边界损失作为正则化，CDTF
    [[78](#bib.bib78)]能够将基础模型推广到目标领域，而无需额外标签。CD-FSS [[98](#bib.bib98)]提出了一种跨领域少样本分割框架，使模型能够利用从自然领域获得的学习能力来促进对有限常见疾病数据的稀有疾病皮损分割。
- en: 6.3 CDFS for Other Application
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 CDFS在其他应用中的应用
- en: Cross-Domain Few-shot Hyperspectral Image Classification [[99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)] draw the research
    attention recently, the model focus on learning of local and global image context
    due to the special image data. The related hyperspectral framework is similar
    to the traditional few-shot research. SAR-FS [[103](#bib.bib103)] developed an
    algorithm to transfer knowledge from EO domains to SAR domains to eliminate the
    need for huge labeled data points in the SAR domains.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域少样本高光谱图像分类[[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)]最近引起了研究关注，该模型关注于由于特殊图像数据的局部和全球图像上下文的学习。相关的高光谱框架类似于传统的少样本研究。SAR-FS
    [[103](#bib.bib103)]开发了一种算法，将知识从EO领域转移到SAR领域，从而消除在SAR领域中对大量标记数据点的需求。
- en: Researchers [[104](#bib.bib104), [105](#bib.bib105)] carried out a series of
    research and discussion on fault diagnosis, focusing on the impact of meta-learning
    on cross-domain fault diagnosis. FSPR [[106](#bib.bib106)] first formulated a
    reidentification scenario as a cross-domain few-shot problem and discussed the
    difference between conventional and unsupervised re-ID. And introduce a reweighting
    instance method based on the function (ReWIF) to guide the training procedure
    of the re-ID model. In other related fields [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)], CDFS has gradually begun to receive attention,
    promoting the development of various research subfields.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员[[104](#bib.bib104), [105](#bib.bib105)]进行了一系列关于故障诊断的研究和讨论，重点关注元学习对跨领域故障诊断的影响。FSPR
    [[106](#bib.bib106)]首次将重新识别场景表述为一个跨领域少样本问题，并讨论了传统和无监督重识别之间的区别。并引入了一种基于函数的重新加权实例方法（ReWIF）来指导重识别模型的训练过程。在其他相关领域[[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)]，CDFS逐渐受到关注，推动了各研究子领域的发展。
- en: 7 FUTURE DIRECTION
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来方向
- en: 7.1 Future Challenges
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 未来挑战
- en: CDFS focuses on learning in different domains and largely solves the domain
    shift challenge. With limited labeled data, current research focuses on near-domain
    transfer, neglecting the more challenging distant-domain transfer. Transference
    of models from a natural scene to a proprietary domain remains difficult. For
    CDFS of multiple sources, the goal of the devised learning scheme is to achieve
    Domain Adaptation and Generalization. Even the lack of sufficient labeled data
    in the new CDFS domain may inhibit the transfer effect, reducing the effectiveness
    of multiple sources in this domain. Thus, fine-tuning variably distributed data
    among multiple sources is essential to maximize the transfer effect and ensure
    successful CDFS.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CDFS专注于不同领域的学习，并在很大程度上解决了领域转移挑战。由于标注数据有限，目前的研究主要关注近领域迁移，忽视了更具挑战性的远领域迁移。从自然场景到专有领域的模型迁移仍然困难重重。对于多源CDFS，设计的学习方案的目标是实现领域适应和泛化。即便在新的CDFS领域中标注数据不足，也可能抑制迁移效果，降低多个来源在该领域的有效性。因此，细化在多个来源之间分布不均的数据对最大化迁移效果并确保成功的CDFS至关重要。
- en: 7.2 Future Techniques
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 未来技术
- en: Recent researchers have sought to use image and feature enhancement to address
    the CDFS problem, which requires a model to adapt quickly to a new environment
    while retaining rapid learning ability. However, successfully combining the insufficient
    annotation and domain shift issue continues to be challenging. To further develop
    solutions to the CDFS problem, researchers should consider the importance of adapting
    high parameters and utilizing domain-invariant information.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究人员试图使用图像和特征增强来解决CDFS问题，该问题要求模型能够快速适应新环境，同时保持快速学习能力。然而，成功结合不足的标注和领域转移问题仍然是一个挑战。为了进一步开发CDFS问题的解决方案，研究人员应考虑适应高参数和利用领域不变信息的重要性。
- en: 7.2.1 Multiple Source Meta-Learning
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 多源元学习
- en: In the future, meta-learning is poised to be a key tool for addressing domain
    shifts, especially in the context of few-shot learning. The model can successfully
    manage complex scenarios by leveraging its high adaptability, especially regarding
    domain migration. As such, upcoming work in cross-domain few-shot (CDFS) should
    focus more on meta-learning in a multi-source setting [[111](#bib.bib111)]. Exploiting
    the abundant data distributions available should maximize the utility of meta-learning
    and preserve its original intention. Additionally, research should be conducted
    to combine memory mechanisms with meta-learning; such a union of the two would
    bring straightforward benefits to the CDFS domain. Memory can be key in connecting
    domains without compromising performance; meanwhile, meta-learning provides highly
    customizable parameters.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，元学习有望成为解决领域转移的关键工具，特别是在少样本学习的背景下。模型可以通过利用其高适应性成功应对复杂的场景，特别是在领域迁移方面。因此，跨领域少样本（CDFS）的未来工作应更多关注多源环境中的元学习[[111](#bib.bib111)]。充分利用丰富的数据分布应最大限度地发挥元学习的效用，并保持其原始意图。此外，应开展将记忆机制与元学习结合的研究；这种结合将为CDFS领域带来直接的好处。记忆可以在不妨碍性能的情况下连接领域，同时元学习提供高度可定制的参数。
- en: 7.2.2 Domain invariant information
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 领域不变信息
- en: When faced with single-domain CDFS, domain-invariant information is invaluable
    for successful zero-shot learning [[112](#bib.bib112), [113](#bib.bib113)]. Specifically,
    semantic information can often be used to evidence relationships between different
    categories. Because visual differences do not hamper it, they can then be readily
    employed as supplementary information for learning. In the context of FSL, several
    studies have explored the intersection of semantic modalities and vision, often
    using the former to facilitate the knowledge migration between categories [[114](#bib.bib114),
    [115](#bib.bib115)]. When shared category information can be successfully converted
    into a different modality, it can offer valuable guidance in cross-domain FSL
    scenarios [[116](#bib.bib116)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 面对单领域CDFS时，领域不变信息对于成功的零样本学习至关重要[[112](#bib.bib112), [113](#bib.bib113)]。具体来说，语义信息通常可以用来证明不同类别之间的关系。由于视觉差异不会阻碍它，因此可以作为学习的补充信息加以利用。在FSL的背景下，几项研究探讨了语义模态和视觉的交集，通常利用前者来促进类别之间的知识迁移[[114](#bib.bib114),
    [115](#bib.bib115)]。当共享的类别信息能够成功转换为不同的模态时，它可以为跨领域FSL场景提供有价值的指导[[116](#bib.bib116)]。
- en: 7.3 Future Applications
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 未来应用
- en: 7.3.1 Open-Set Few-Shot Learning
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 开放集少样本学习
- en: The softmax of the deep model assigns all samples in the test phase to a fixed
    category, which is different from actual expectations as it cannot identify unseen
    classes [[117](#bib.bib117)]. To address this limitation, open-set models are
    more suitable compared with close-set models as they require the model to make
    a corresponding response to the unseen class rather than a mechanical one. Cross
    Domain Few-Shot (CDFS) transfer of the model to the target domain allows for identifying
    unseen classes, albeit requiring a few fine-tuning annotations when doing so [[118](#bib.bib118),
    [119](#bib.bib119)]. However, this approach is only effective when classes in
    the source and target domains can be delineated, raising an important question
    about identifying these unlabeled classes for complex scenarios where all classes
    need to be labeled. Open-set few-shot learning can help to meet this challenge.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型的softmax将测试阶段的所有样本分配到固定类别，这与实际期望不同，因为它无法识别未见过的类别[[117](#bib.bib117)]。为了解决这一限制，相比于闭集模型，开放集模型更为合适，因为它们要求模型对未见过的类别做出相应的反应，而不是机械地处理。模型的跨领域少样本（CDFS）迁移到目标领域可以识别未见过的类别，但在此过程中需要一些微调注释[[118](#bib.bib118),
    [119](#bib.bib119)]。然而，这种方法仅在源领域和目标领域的类别可以界定时有效，这提出了一个重要问题：在所有类别都需要标记的复杂场景中如何识别这些未标记的类别。开放集少样本学习可以帮助应对这一挑战。
- en: 7.3.2 Incremental Few-Shot Learning
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 增量式少样本学习
- en: Incremental few-shot learning requires the model to recognize new classes without
    forgetting the learned classes [[120](#bib.bib120), [121](#bib.bib121)]. Typical
    few-shot models only focus on new classes, and base classes must be relearned
    to complete recognition. Data dependency results in the models being effective
    in limited scenarios and cannot increase classes. For future CDFS, the model will
    constantly learn new domain information during the generalization process. In
    order to identify the different domain classes, the model needs to learn and save
    the data of the corresponding domain constantly. Binding data to the model anytime
    does not conform to the original intention of few-shot learning. Therefore, studying
    incremental few-shot learning can promote CDFS to a certain extent. If we can
    eliminate the dependence on data domains, CDFS will be more practical [[122](#bib.bib122),
    [123](#bib.bib123)].
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 增量式少样本学习要求模型在不遗忘已学类别的情况下识别新类别[[120](#bib.bib120), [121](#bib.bib121)]。典型的少样本模型只关注新类别，基础类别必须重新学习以完成识别。数据依赖性导致模型在有限场景下有效，且无法增加类别。对于未来的CDFS，模型将在泛化过程中不断学习新的领域信息。为了识别不同领域的类别，模型需要不断学习和保存对应领域的数据。随时将数据绑定到模型上并不符合少样本学习的初衷。因此，研究增量式少样本学习可以在一定程度上促进CDFS的发展。如果我们能消除对数据领域的依赖，CDFS将变得更加实用[[122](#bib.bib122),
    [123](#bib.bib123)]。
- en: 8 Conclusion
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We have provided a comprehensive overview of recent cross-domain few-shot learning
    (CDFS) research. Our analysis considered existing solutions and research issues
    while comparing performance indices between different studies. Furthermore, we
    discussed the broad applications of CDFS and its implications for future research.
    Our review will serve as a valuable reference guide and provide theoretical support
    for advancing the field of CDFS. Our survey reveals that cross-domain few-shot
    learning is gradually becoming an increasingly popular research topic and has
    received extensive attention due to its potential to alleviate the domain shift
    problem in AI applications. The current solutions to the problem span various
    approaches, each with its advantages and limitations. As the research field is
    still in its infancy, future work should focus on extending existing or new methods
    to improve the performance of cross-domain few-shot learning systems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了对近期跨领域少样本学习（CDFS）研究的全面概述。我们的分析考虑了现有解决方案和研究问题，同时比较了不同研究之间的性能指标。此外，我们讨论了CDFS的广泛应用及其对未来研究的影响。我们的综述将作为一个有价值的参考指南，并为推动CDFS领域的发展提供理论支持。我们的调查显示，跨领域少样本学习正逐渐成为一个越来越受欢迎的研究主题，并因其在AI应用中缓解领域偏移问题的潜力而受到广泛关注。目前解决该问题的方法涵盖了多种途径，每种方法都有其优缺点。由于研究领域仍处于起步阶段，未来的工作应着重于扩展现有或新方法，以提升跨领域少样本学习系统的性能。
- en: 9 Acknowledgements
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: This work was supported in part by the National Natural Science Foundation of
    China (No.62176009), the Major Project for New Generation of AI (No.2018AAA0100400),
    the National Natural Science Foundation of China (No. 61836014, No. U21B2042,
    No. 62072457, No. 62006231).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了中国国家自然科学基金（编号62176009）、新一代人工智能重大项目（编号2018AAA0100400）、中国国家自然科学基金（编号61836014、U21B2042、62072457、62006231）的资助。
- en: References
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Snell, K. Swersky, R. S. Zemel, Prototypical networks for few-shot learning,
    ArXiv abs/1703.05175.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Snell, K. Swersky, R. S. Zemel, 原型网络用于少样本学习，ArXiv abs/1703.05175。'
- en: '[2] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    Learning to compare: Relation network for few-shot learning, 2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (2017) 1199–1208.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    学习比较：少样本学习的关系网络，2018 IEEE/CVF计算机视觉与模式识别会议（2017）1199–1208。'
- en: '[3] O. Vinyals, C. Blundell, T. P. Lillicrap, K. Kavukcuoglu, D. Wierstra,
    Matching networks for one shot learning, in: Proc. NIPS, 2016.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] O. Vinyals, C. Blundell, T. P. Lillicrap, K. Kavukcuoglu, D. Wierstra,
    匹配网络用于单次学习，载于：NIPS会议，2016年。'
- en: '[4] W. Wang, L. Duan, Q. En, B. Zhang, F. Liang, Tpsn: Transformer-based multi-prototype
    search network for few-shot semantic segmentation, Comput. Electr. Eng. 103 (2022)
    108326.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] W. Wang, L. Duan, Q. En, B. Zhang, F. Liang, Tpsn: 基于变压器的多原型搜索网络用于少样本语义分割，计算机与电子工程学报103（2022）108326。'
- en: '[5] W. Wang, L. Duan, Y. Wang, Q. En, J. Fan, Z. Zhang, Remember the difference:
    Cross-domain few-shot semantic segmentation via meta-memory transfer, 2022 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 7055–7064.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] W. Wang, L. Duan, Y. Wang, Q. En, J. Fan, Z. Zhang, 记住差异：通过元记忆转移进行跨领域少样本语义分割，2022
    IEEE/CVF计算机视觉与模式识别会议（CVPR）（2022）7055–7064。'
- en: '[6] Y. Guo, N. C. F. Codella, L. Karlinsky, J. Codella, J. R. Smith, K. Saenko,
    T. S. Rosing, R. S. Feris, A broader study of cross-domain few-shot learning,
    in: European Conference on Computer Vision, 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. Guo, N. C. F. Codella, L. Karlinsky, J. Codella, J. R. Smith, K. Saenko,
    T. S. Rosing, R. S. Feris, 更广泛的跨领域少样本学习研究，载于：欧洲计算机视觉会议，2019年。'
- en: '[7] H.-Y. Tseng, H.-Y. Lee, J.-B. Huang, M.-H. Yang, Cross-domain few-shot
    classification via learned feature-wise transformation, in: International Conference
    on Learning Representations, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H.-Y. Tseng, H.-Y. Lee, J.-B. Huang, M.-H. Yang, 通过学习的特征变换进行跨领域少样本分类，载于：国际学习表征会议，2020年。'
- en: '[8] Y. Wang, Q. Yao, J. T.-Y. Kwok, L. M. shuan Ni, Generalizing from a few
    examples: A survey on few-shot learning, arXiv: Learning.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Wang, Q. Yao, J. T.-Y. Kwok, L. M. shuan Ni, 从少数例子中概括：少样本学习的综述，arXiv:
    Learning。'
- en: '[9] S. Motiian, Q. Jones, S. M. Iranmanesh, G. Doretto, Few-shot adversarial
    domain adaptation, in: NIPS, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Motiian, Q. Jones, S. M. Iranmanesh, G. Doretto, 少样本对抗领域自适应，载于：NIPS，2017年。'
- en: '[10] X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, A. S. Vincentelli,
    Prototypical cross-domain self-supervised learning for few-shot unsupervised domain
    adaptation, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 13829–13839.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, A. S. Vincentelli,
    原型式跨领域自监督学习用于少样本无监督领域自适应，2021 IEEE/CVF计算机视觉与模式识别会议（CVPR）（2021）13829–13839。'
- en: '[11] K. Saito, D. Kim, S. Sclaroff, T. Darrell, K. Saenko, Semi-supervised
    domain adaptation via minimax entropy, 2019 IEEE/CVF International Conference
    on Computer Vision (ICCV) (2019) 8049–8057.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. Saito, D. Kim, S. Sclaroff, T. Darrell, K. Saenko, 通过最小最大熵进行半监督领域自适应，2019
    IEEE/CVF国际计算机视觉会议（ICCV）（2019）8049–8057。'
- en: '[12] S. Chen, X. Jia, J. He, Y. Shi, J. Liu, Semi-supervised domain adaptation
    based on dual-level domain mixing for semantic segmentation, 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR) (2021) 11013–11022.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Chen, X. Jia, J. He, Y. Shi, J. Liu, 基于双层领域混合的半监督领域自适应用于语义分割，2021 IEEE/CVF计算机视觉与模式识别会议（CVPR）（2021）11013–11022。'
- en: '[13] J. Liang, D. Hu, Y. Wang, R. He, J. Feng, Source data-absent unsupervised
    domain adaptation through hypothesis transfer and labeling transfer, IEEE Transactions
    on Pattern Analysis and Machine Intelligence 44 (2020) 8602–8617.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Liang, D. Hu, Y. Wang, R. He, J. Feng, 缺少源数据的无监督领域自适应通过假设转移和标注转移，IEEE模式分析与机器智能学报44（2020）8602–8617。'
- en: '[14] T. Xu, W. Chen, P. Wang, F. Wang, H. Li, R. Jin, Cdtrans: Cross-domain
    transformer for unsupervised domain adaptation, ArXiv abs/2109.06165.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. Xu, W. Chen, P. Wang, F. Wang, H. Li, R. Jin, Cdtrans: 用于无监督领域自适应的跨领域变压器，ArXiv
    abs/2109.06165。'
- en: '[15] X. Liu, Z. Guo, S. Li, F. Xing, J. J. You, C.-C. J. Kuo, G. E. Fakhri,
    J. Woo, Adversarial unsupervised domain adaptation with conditional and label
    shift: Infer, align and iterate, 2021 IEEE/CVF International Conference on Computer
    Vision (ICCV) (2021) 10347–10356.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] X. Liu, Z. Guo, S. Li, F. Xing, J. J. You, C.-C. J. Kuo, G. E. Fakhri,
    J. Woo, 对抗性无监督领域适应：条件和标签偏移：推断、对齐和迭代，2021 IEEE/CVF 国际计算机视觉会议 (ICCV) (2021) 10347–10356。'
- en: '[16] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, Generalizing to unseen domains:
    A survey on domain generalization, in: International Joint Conference on Artificial
    Intelligence, 2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, 泛化到未见领域：领域泛化综述，发表于：国际人工智能联合会议，2021年。'
- en: '[17] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, IEEE transactions on pattern analysis and machine intelligence PP.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, 领域泛化：综述，IEEE 模式分析与机器智能学报
    PP。'
- en: '[18] S. Thrun, L. Y. Pratt, Learning to learn: Introduction and overview, in:
    Learning to Learn, 1998.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Thrun, L. Y. Pratt, 学习如何学习：介绍与概述，发表于：学习如何学习，1998年。'
- en: '[19] Q. Sun, Y. Liu, Z. Chen, T.-S. Chua, B. Schiele, Meta-transfer learning
    through hard tasks, IEEE Transactions on Pattern Analysis and Machine Intelligence
    44 (2019) 1443–1456.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Q. Sun, Y. Liu, Z. Chen, T.-S. Chua, B. Schiele, 通过困难任务进行元转移学习，IEEE 模式分析与机器智能学报
    44 (2019) 1443–1456。'
- en: '[20] A. Achille, M. Lam, R. Tewari, A. Ravichandran, S. Maji, C. C. Fowlkes,
    S. Soatto, P. Perona, Task2vec: Task embedding for meta-learning, 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV) (2019) 6429–6438.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Achille, M. Lam, R. Tewari, A. Ravichandran, S. Maji, C. C. Fowlkes,
    S. Soatto, P. Perona, Task2vec: 用于元学习的任务嵌入，2019 IEEE/CVF 国际计算机视觉会议 (ICCV) (2019)
    6429–6438。'
- en: '[21] R. Vuorio, S.-H. Sun, H. Hu, J. J. Lim, Multimodal model-agnostic meta-learning
    via task-aware modulation, in: Neural Information Processing Systems, 2019.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. Vuorio, S.-H. Sun, H. Hu, J. J. Lim, 通过任务感知调制的多模态模型无关元学习，发表于：神经信息处理系统，2019年。'
- en: '[22] A. Nichol, J. Schulman, Reptile: a scalable metalearning algorithm, arXiv:
    Learning.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Nichol, J. Schulman, Reptile: 一种可扩展的元学习算法，arXiv: 学习。'
- en: '[23] G. Chen, T. Zhang, J. Lu, J. Zhou, Deep meta metric learning, 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV) (2019) 9546–9555.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] G. Chen, T. Zhang, J. Lu, J. Zhou, 深度元度量学习，2019 IEEE/CVF 国际计算机视觉会议 (ICCV)
    (2019) 9546–9555。'
- en: '[24] A. Nichol, J. Achiam, J. Schulman, On first-order meta-learning algorithms,
    ArXiv abs/1803.02999.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Nichol, J. Achiam, J. Schulman, 关于一阶元学习算法，ArXiv abs/1803.02999。'
- en: '[25] E. Triantafillou, T. L. Zhu, V. Dumoulin, P. Lamblin, K. Xu, R. Goroshin,
    C. Gelada, K. Swersky, P.-A. Manzagol, H. Larochelle, Meta-dataset: A dataset
    of datasets for learning to learn from few examples, ArXiv abs/1903.03096.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] E. Triantafillou, T. L. Zhu, V. Dumoulin, P. Lamblin, K. Xu, R. Goroshin,
    C. Gelada, K. Swersky, P.-A. Manzagol, H. Larochelle, Meta-dataset: 一个用于从少量样本中学习的集合数据集，ArXiv
    abs/1903.03096。'
- en: '[26] S. Lei, X. Zhang, J. He, F. Chen, B. Du, C.-T. Lu, Cross-domain few-shot
    semantic segmentation, in: European Conference on Computer Vision, 2022.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Lei, X. Zhang, J. He, F. Chen, B. Du, C.-T. Lu, 跨领域少样本语义分割，发表于：欧洲计算机视觉会议，2022年。'
- en: '[27] K. Lee, H. Yang, S. Chakraborty, Z. Cai, G. Swaminathan, A. Ravichandran,
    O. Dabeer, Rethinking few-shot object detection on a multi-domain benchmark, ArXiv
    abs/2207.11169.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. Lee, H. Yang, S. Chakraborty, Z. Cai, G. Swaminathan, A. Ravichandran,
    O. Dabeer, 在多领域基准上重新思考少样本目标检测，ArXiv abs/2207.11169。'
- en: '[28] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, Understanding cross-domain
    few-shot learning based on domain similarity and few-shot difficulty, 2022.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, 基于领域相似性和少样本难度理解跨领域少样本学习，2022年。'
- en: '[29] Y. Zhang, Y. Zheng, X. Xu, J. Wang, How well do self-supervised methods
    perform in cross-domain few-shot learning?, ArXiv abs/2202.09014.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Zhang, Y. Zheng, X. Xu, J. Wang, 自监督方法在跨领域少样本学习中的表现如何？，ArXiv abs/2202.09014。'
- en: '[30] N. Dvornik, C. Schmid, J. Mairal, Selecting relevant features from a multi-domain
    representation for few-shot classification, in: European Conference on Computer
    Vision, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] N. Dvornik, C. Schmid, J. Mairal, 从多领域表示中选择相关特征用于少样本分类，发表于：欧洲计算机视觉会议，2020年。'
- en: '[31] S. Peng, W. Song, M. Ester, Combining domain-specific meta-learners in
    the parameter space for cross-domain few-shot classification, ArXiv abs/2011.00179.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Peng, W. Song, M. Ester, 在参数空间中结合领域特定的元学习器用于跨领域少样本分类，ArXiv abs/2011.00179。'
- en: '[32] W.-H. Li, X. Liu, H. Bilen, Universal representation learning from multiple
    domains for few-shot classification, 2021 IEEE/CVF International Conference on
    Computer Vision (ICCV) (2021) 9506–9515.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W.-H. Li, X. Liu, H. Bilen, 从多个领域学习通用表示用于少样本分类，2021年IEEE/CVF计算机视觉国际会议（ICCV）（2021）9506–9515。'
- en: '[33] J. Xiao, S. Gu, L. Zhang, Multi-domain learning for accurate and few-shot
    color constancy, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 3255–3264.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Xiao, S. Gu, L. Zhang, 精确和少样本色彩一致性的多领域学习，2020年IEEE/CVF计算机视觉与模式识别会议（CVPR）（2020）3255–3264。'
- en: '[34] W.-H. Li, X. Liu, H. Bilen, Cross-domain few-shot learning with task-specific
    adapters, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2022) 7151–7160.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W.-H. Li, X. Liu, H. Bilen, 带有任务特定适配器的跨领域少样本学习，2022年IEEE/CVF计算机视觉与模式识别会议（CVPR）（2022）7151–7160。'
- en: '[35] X. Zou, Y. Yan, J. Xue, S. Chen, H. Wang, Learn-to-decompose: Cascaded
    decomposition network for cross-domain few-shot facial expression recognition,
    in: European Conference on Computer Vision, 2022.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] X. Zou, Y. Yan, J. Xue, S. Chen, H. Wang, Learn-to-decompose: 用于跨领域少样本面部表情识别的级联分解网络，欧洲计算机视觉会议，2022年。'
- en: '[36] H. Xu, L. Liu, Cross-domain few-shot classification via inter-source stylization,
    ArXiv abs/2208.08015.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Xu, L. Liu, 通过源间风格化进行跨领域少样本分类，ArXiv abs/2208.08015。'
- en: '[37] W.-Y. Lee, J.-Y. Wang, Y. Wang, Domain-agnostic meta-learning for cross-domain
    few-shot classification, ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP) (2022) 1715–1719.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] W.-Y. Lee, J.-Y. Wang, Y. Wang, 跨领域少样本分类的领域无关元学习，ICASSP 2022 - 2022年IEEE国际声学、语音与信号处理会议（ICASSP）（2022）1715–1719。'
- en: '[38] Y. Fu, Y. Fu, Y.-G. Jiang, Meta-fdmixup: Cross-domain few-shot learning
    guided by labeled target data, arXiv preprint arXiv:2107.11978.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Fu, Y. Fu, Y.-G. Jiang, Meta-fdmixup: 由标记目标数据引导的跨领域少样本学习，arXiv预印本 arXiv:2107.11978。'
- en: '[39] A. Islam, C.-F. Chen, R. Panda, L. Karlinsky, R. S. Feris, R. J. Radke,
    Dynamic distillation network for cross-domain few-shot recognition with unlabeled
    data, in: Neural Information Processing Systems, 2021.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Islam, C.-F. Chen, R. Panda, L. Karlinsky, R. S. Feris, R. J. Radke,
    动态蒸馏网络用于带有未标记数据的跨领域少样本识别，神经信息处理系统，2021年。'
- en: '[40] A. Zhao, M. Ding, Z. Lu, T. Xiang, Y. Niu, J. Guan, J. rong Wen, P. Luo,
    Domain-adaptive few-shot learning, 2021 IEEE Winter Conference on Applications
    of Computer Vision (WACV) (2020) 1389–1398.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Zhao, M. Ding, Z. Lu, T. Xiang, Y. Niu, J. Guan, J. rong Wen, P. Luo,
    领域自适应少样本学习，2021年IEEE冬季计算机视觉应用会议（WACV）（2020）1389–1398。'
- en: '[41] L. Zhao, Y. Meng, L. Xu, Oa-fsui2it: A novel few-shot cross domain object
    detection framework with object-aware few-shot unsupervised image-to-image translation,
    in: AAAI, 2022.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] L. Zhao, Y. Meng, L. Xu, Oa-fsui2it: 一种新颖的少样本跨领域目标检测框架，具有对象感知的少样本无监督图像到图像转换，AAAI，2022年。'
- en: '[42] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Me-d2n: Multi-expert domain
    decompositional network for cross-domain few-shot learning, Proceedings of the
    30th ACM International Conference on Multimedia.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Me-d2n: 跨领域少样本学习的多专家领域分解网络，
    第30届ACM国际多媒体会议论文集。'
- en: '[43] L. Zhuo, Y. Fu, J. Chen, Y. Cao, Y.-G. Jiang, Tgdm: Target guided dynamic
    mixup for cross-domain few-shot learning, Proceedings of the 30th ACM International
    Conference on Multimedia.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] L. Zhuo, Y. Fu, J. Chen, Y. Cao, Y.-G. Jiang, Tgdm: 用于跨领域少样本学习的目标引导动态混合，
    第30届ACM国际多媒体会议论文集。'
- en: '[44] Y. Gao, L. Yang, Y. Huang, S. Xie, S. Li, W. Zheng, Acrofod: An adaptive
    method for cross-domain few-shot object detection, in: European Conference on
    Computer Vision, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Gao, L. Yang, Y. Huang, S. Xie, S. Li, W. Zheng, Acrofod: 一种用于跨领域少样本目标检测的自适应方法，欧洲计算机视觉会议，2022年。'
- en: '[45] W. Chen, Z. Zhang, W. Wang, L. Wang, Z. Wang, T. Tan, Cross-domain cross-set
    few-shot learning via learning compact and aligned representations, in: European
    Conference on Computer Vision, 2022.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] W. Chen, Z. Zhang, W. Wang, L. Wang, Z. Wang, T. Tan, 通过学习紧凑且对齐的表示进行跨领域跨集合少样本学习，欧洲计算机视觉会议，2022年。'
- en: '[46] F. Yao, Cross-domain few-shot learning with unlabelled data, ArXiv abs/2101.07899.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] F. Yao, 跨领域少样本学习与未标记数据，ArXiv abs/2101.07899。'
- en: '[47] Y. Nakamura, Y. Ishii, Y. Maruyama, T. Yamashita, Few-shot adaptive object
    detection with cross-domain cutmix, ArXiv abs/2208.14586.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Nakamura, Y. Ishii, Y. Maruyama, T. Yamashita, 带有跨领域CutMix的少样本自适应目标检测，ArXiv
    abs/2208.14586。'
- en: '[48] T. Teshima, I. Sato, M. Sugiyama, Few-shot domain adaptation by causal
    mechanism transfer, in: International Conference on Machine Learning, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Teshima, I. Sato, M. Sugiyama, 通过因果机制转移进行小样本领域适应, 见：国际机器学习大会, 2020。'
- en: '[49] C. Medina, A. Devos, M. Grossglauser, Self-supervised prototypical transfer
    learning for few-shot classification, ArXiv abs/2006.11325.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Medina, A. Devos, M. Grossglauser, 自监督原型迁移学习用于小样本分类, ArXiv abs/2006.11325。'
- en: '[50] Y. Zou, S. Zhang, J. Yu, Y. Tian, J. M. F. Moura, Revisiting mid-level
    patterns for cross-domain few-shot recognition, Proceedings of the 29th ACM International
    Conference on Multimedia.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Zou, S. Zhang, J. Yu, Y. Tian, J. M. F. Moura, 重新审视跨域小样本识别中的中级模式, 第29届ACM国际多媒体大会论文集。'
- en: '[51] H. Liang, Q. Zhang, P. Dai, J. Lu, Boosting the generalization capability
    in cross-domain few-shot learning via noise-enhanced supervised autoencoder, 2021
    IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 9404–9414.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Liang, Q. Zhang, P. Dai, J. Lu, 通过噪声增强的监督自编码器提升跨域小样本学习的泛化能力, 2021 IEEE/CVF
    国际计算机视觉大会 (ICCV) (2021) 9404–9414。'
- en: '[52] H. Wang, Z. Deng, Cross-domain few-shot classification via adversarial
    task augmentation, in: International Joint Conference on Artificial Intelligence,
    2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Wang, Z. Deng, 跨域小样本分类通过对抗任务增强, 见：国际人工智能联合会议, 2021。'
- en: '[53] R. Tao, H. Zhang, Y. Zheng, M. Savvides, Powering finetuning in few-shot
    learning: Domain-agnostic bias reduction with selected sampling, in: AAAI, 2022.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] R. Tao, H. Zhang, Y. Zheng, M. Savvides, 提升小样本学习中的微调效果：通过选择采样减少领域无关偏差,
    见：AAAI, 2022。'
- en: '[54] W. Yuan, Z. Zhang, C. Wang, H. Song, Y. Xie, L. Ma, Task-level self-supervision
    for cross-domain few-shot learning, in: AAAI, 2022.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Yuan, Z. Zhang, C. Wang, H. Song, Y. Xie, L. Ma, 跨域小样本学习的任务级自监督, 见：AAAI,
    2022。'
- en: '[55] P. Li, S. Gong, Y. Fu, C. Wang, Ranking distance calibration for cross-domain
    few-shot learning, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 9089–9098.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] P. Li, S. Gong, Y. Fu, C. Wang, 跨域小样本学习的排名距离校准, 2022 IEEE/CVF 计算机视觉与模式识别大会
    (CVPR) (2021) 9089–9098。'
- en: '[56] Y. Hu, A. J. Ma, Adversarial feature augmentation for cross-domain few-shot
    classification, in: ECCV, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Hu, A. J. Ma, 对抗特征增强用于跨域小样本分类, 见：ECCV, 2022。'
- en: '[57] W. Zhang, L. Shen, W. Zhang, C.-S. Foo, Few-shot adaptation of pre-trained
    networks for domain shift, in: IJCAI, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] W. Zhang, L. Shen, W. Zhang, C.-S. Foo, 预训练网络的领域适应小样本学习, 见：IJCAI, 2022。'
- en: '[58] Y. Xu, L. Wang, Y. Wang, C. Qin, Y. Zhang, Y. R. Fu, Memrein: Rein the
    domain shift for cross-domain few-shot learning, in: IJCAI, 2022.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Xu, L. Wang, Y. Wang, C. Qin, Y. Zhang, Y. R. Fu, Memrein: 控制跨域小样本学习中的领域偏移,
    见：IJCAI, 2022。'
- en: '[59] P.-C. Tu, H.-K. K. Pao, A dropout style model augmentation for cross domain
    few-shot learning, 2021 IEEE International Conference on Big Data (Big Data) (2021)
    1138–1147.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] P.-C. Tu, H.-K. K. Pao, 一种用于跨域小样本学习的 dropout 风格模型增强, 2021 IEEE 国际大数据大会
    (Big Data) (2021) 1138–1147。'
- en: '[60] J. Jiang, Z. Li, Y. Guo, J. Ye, A transductive multi-head model for cross-domain
    few-shot learning, ArXiv abs/2006.11384.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Jiang, Z. Li, Y. Guo, J. Ye, 用于跨域小样本学习的传导多头模型, ArXiv abs/2006.11384。'
- en: '[61] B. Liu, Z. Zhao, Z. Li, J. Jiang, Y. Guo, J. Ye, Feature transformation
    ensemble model with batch spectral regularization for cross-domain few-shot classification,
    ArXiv abs/2005.08463.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] B. Liu, Z. Zhao, Z. Li, J. Jiang, Y. Guo, J. Ye, 带批量谱正则化的特征变换集成模型用于跨域小样本分类,
    ArXiv abs/2005.08463。'
- en: '[62] J. Cai, B. Y. Cai, S. M. Shen, Sb-mtl: Score-based meta transfer-learning
    for cross-domain few-shot learning, ArXiv abs/2012.01784.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Cai, B. Y. Cai, S. M. Shen, Sb-mtl: 基于评分的元迁移学习用于跨域小样本学习, ArXiv abs/2012.01784。'
- en: '[63] X. Liu, Z. Ji, Y. Pang, Z. Zhang, Self-taught cross-domain few-shot learning
    with weakly supervised object localization and task-decomposition, ArXiv abs/2109.01302.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Liu, Z. Ji, Y. Pang, Z. Zhang, 自学的跨域小样本学习与弱监督目标定位和任务分解, ArXiv abs/2109.01302。'
- en: '[64] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Wave-san: Wavelet based style
    augmentation network for cross-domain few-shot learning, ArXiv abs/2203.07656.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Wave-san: 基于小波的风格增强网络用于跨域小样本学习,
    ArXiv abs/2203.07656。'
- en: '[65] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, Refine: Re-randomization
    before fine-tuning for cross-domain few-shot learning, Proceedings of the 31st
    ACM International Conference on Information & Knowledge Management.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, Refine: 微调前的再随机化用于跨域小样本学习,
    第31届ACM国际信息与知识管理大会论文集。'
- en: '[66] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. Ting,
    R. S. M. Goh, Few-shot domain adaptation with polymorphic transformers, ArXiv
    abs/2107.04805.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. Ting,
    R. S. M. Goh, 具有多态变换器的少样本领域适应，ArXiv abs/2107.04805。'
- en: '[67] A. Tavera, F. Cermelli, C. Masone, B. Caputo, Pixel-by-pixel cross-domain
    alignment for few-shot semantic segmentation, 2022 IEEE/CVF Winter Conference
    on Applications of Computer Vision (WACV) (2021) 1959–1968.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. Tavera, F. Cermelli, C. Masone, B. Caputo, 像素级跨领域对齐的少样本语义分割，2022 IEEE/CVF
    冬季计算机视觉应用会议（WACV）（2021）1959–1968。'
- en: '[68] S. Huang, W. Yang, L. Wang, L. Zhou, M. Yang, Few-shot unsupervised domain
    adaptation with image-to-class sparse similarity encoding, Proceedings of the
    29th ACM International Conference on Multimedia.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Huang, W. Yang, L. Wang, L. Zhou, M. Yang, 图像到类别稀疏相似性编码的少样本无监督领域适应，第29届ACM国际多媒体会议论文集。'
- en: '[69] C. P. Phoo, B. Hariharan, Self-training for few-shot transfer across extreme
    task differences, ArXiv abs/2010.07734.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. P. Phoo, B. Hariharan, 针对极端任务差异的少样本迁移自训练，ArXiv abs/2010.07734。'
- en: '[70] A. D’Innocente, F. C. Borlino, S. Bucci, B. Caputo, T. Tommasi, One-shot
    unsupervised cross-domain detection, in: European Conference on Computer Vision,
    2020.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. D’Innocente, F. C. Borlino, S. Bucci, B. Caputo, T. Tommasi, 一次性无监督跨领域检测，见：欧洲计算机视觉大会，2020。'
- en: '[71] B. Xu, Z. Zeng, C. Lian, Z. Ding, Few-shot domain adaptation via mixup
    optimal transport, IEEE Transactions on Image Processing 31 (2022) 2518–2528.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. Xu, Z. Zeng, C. Lian, Z. Ding, 通过混合最优传输进行少样本领域适应，IEEE 图像处理学报 31（2022）2518–2528。'
- en: '[72] S. P. Mohanty, D. P. Hughes, M. Salathé, Using deep learning for image-based
    plant disease detection, Frontiers in Plant Science 7.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. P. Mohanty, D. P. Hughes, M. Salathé, 使用深度学习进行基于图像的植物疾病检测，植物科学前沿 7。'
- en: '[73] P. Helber, B. Bischke, A. R. Dengel, D. Borth, Eurosat: A novel dataset
    and deep learning benchmark for land use and land cover classification, IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing 12 (2019)
    2217–2226.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] P. Helber, B. Bischke, A. R. Dengel, D. Borth, Eurosat：用于土地利用和土地覆盖分类的新数据集及深度学习基准，IEEE
    应用地球观测与遥感精选主题期刊 12（2019）2217–2226。'
- en: '[74] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases, 2017 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR) (2017) 3462–3471.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8：医院级胸部X射线数据库及弱监督分类和常见胸部疾病定位基准，2017
    IEEE 计算机视觉与模式识别会议（CVPR）（2017）3462–3471。'
- en: '[75] Y. Fu, Y. Fu, J. Chen, Y.-G. Jiang, Generalized meta-fdmixup: Cross-domain
    few-shot learning guided by labeled target data, IEEE Transactions on Image Processing
    31 (2022) 7078–7090.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Fu, Y. Fu, J. Chen, Y.-G. Jiang, 泛化元-fdmixup：以标记目标数据指导的跨领域少样本学习，IEEE
    图像处理学报 31（2022）7078–7090。'
- en: '[76] L. Yalan, W. Jijie, Cross-domain few-shot classification through diversified
    feature transformation layers, 2021 IEEE International Conference on Artificial
    Intelligence and Computer Applications (ICAICA) (2021) 549–555.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] L. Yalan, W. Jijie, 通过多样化特征变换层进行跨领域少样本分类，2021 IEEE 国际人工智能与计算机应用大会（ICAICA）（2021）549–555。'
- en: '[77] J. Sun, S. Lapuschkin, W. Samek, Y. Zhao, N.-M. Cheung, A. Binder, Explanation-guided
    training for cross-domain few-shot classification, 2020 25th International Conference
    on Pattern Recognition (ICPR) (2020) 7609–7616.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Sun, S. Lapuschkin, W. Samek, Y. Zhao, N.-M. Cheung, A. Binder, 以解释为指导的跨领域少样本分类训练，2020年第25届国际模式识别大会（ICPR）（2020）7609–7616。'
- en: '[78] Y. Lu, X. Wu, Z. Wu, S. Wang, Cross-domain few-shot segmentation with
    transductive fine-tuning, ArXiv abs/2211.14745.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Lu, X. Wu, Z. Wu, S. Wang, 通过传导微调进行跨领域少样本分割，ArXiv abs/2211.14745。'
- en: '[79] S. Baik, J. Oh, S. Hong, K. M. Lee, Learning to forget for meta-learning
    via task-and-layer-wise attenuation, IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44 (2021) 7718–7730.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Baik, J. Oh, S. Hong, K. M. Lee, 通过任务和层级衰减学习遗忘进行元学习，IEEE 模式分析与机器智能学报
    44（2021）7718–7730。'
- en: '[80] X. Lin, M. Ye, Y. Gong, G. T. Buracas, N. Basiou, A. Divakaran, Y. Yao,
    Modular adaptation for cross-domain few-shot learning, ArXiv abs/2104.00619.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Lin, M. Ye, Y. Gong, G. T. Buracas, N. Basiou, A. Divakaran, Y. Yao,
    跨领域少样本学习的模块化适应，ArXiv abs/2104.00619。'
- en: '[81] Q. Zhang, Y. Jiang, Z. Wen, Tacdfsl: Task adaptive cross domain few-shot
    learning, Symmetry 14 (2022) 1097.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Q. Zhang, Y. Jiang, Z. Wen, Tacdfsl：任务自适应跨领域少样本学习，Symmetry 14（2022）1097。'
- en: '[82] S. Ravi, H. Larochelle, Optimization as a model for few-shot learning,
    in: International Conference on Learning Representations, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. 拉维, H. 拉罗谢尔, 将优化作为少样本学习的模型，见：国际表示学习会议, 2016。'
- en: '[83] P. Tschandl, C. Rosendahl, H. Kittler, The ham10000 dataset, a large collection
    of multi-source dermatoscopic images of common pigmented skin lesions, Scientific
    Data 5.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] P. 茨汉德尔, C. 罗森达尔, H. 基特勒, HAM10000 数据集：一个包含多源皮肤镜图像的大型常见色素性皮肤病变集合，《科学数据》5。'
- en: '[84] C. Wah, S. Branson, P. Welinder, P. Perona, S. J. Belongie, The caltech-ucsd
    birds-200-2011 dataset, 2011.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] C. 瓦赫, S. 布兰森, P. 韦林德, P. 佩罗纳, S. J. 贝隆吉, Caltech-UCSD 鸟类 200-2011 数据集，2011。'
- en: '[85] J. Krause, M. Stark, J. Deng, L. Fei-Fei, 3d object representations for
    fine-grained categorization, 2013 IEEE International Conference on Computer Vision
    Workshops (2013) 554–561.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. 克劳斯, M. 斯塔克, J. 邓, L. 费伊-费伊, 用于细粒度分类的三维物体表示，2013 IEEE国际计算机视觉会议工作坊 (2013)
    554–561。'
- en: '[86] B. Zhou, À. Lapedriza, A. Khosla, A. Oliva, A. Torralba, Places: A 10
    million image database for scene recognition, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 40 (2018) 1452–1464.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] B. 周, À. 拉佩德里扎, A. 科斯拉, A. 奥利瓦, A. 托拉尔巴, Places: 用于场景识别的千万图像数据库，《IEEE
    模式分析与机器智能》40 (2018) 1452–1464。'
- en: '[87] G. V. Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, S. J. Belongie, The inaturalist species classification and detection
    dataset, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2017)
    8769–8778.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] G. V. 霍恩, O. M. 奥达, Y. 宋, Y. 崔, C. 孙, A. 谢泼德, H. 亚当, P. 佩罗纳, S. J. 贝隆吉,
    iNaturalist 物种分类和检测数据集，2018 IEEE/CVF 计算机视觉与模式识别会议 (2017) 8769–8778。'
- en: '[88] V. G. Satorras, J. Bruna, Few-shot learning with graph neural networks,
    ArXiv abs/1711.04043.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. G. 萨托拉斯, J. 布鲁纳, 使用图神经网络进行少样本学习，ArXiv abs/1711.04043。'
- en: '[89] Y. Chen, Y. Zheng, Z. Xu, T. Tang, Z. Tang, J. Chen, Y. Liu, Cross-domain
    few-shot classification based on lightweight res2net and flexible gnn, Knowl.
    Based Syst. 247 (2022) 108623.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. 陈, Y. 郑, Z. 许, T. 唐, Z. 唐, J. 陈, Y. 刘, 基于轻量级 res2net 和灵活 GNN 的跨领域少样本分类，《知识基系统》247
    (2022) 108623。'
- en: '[90] B. Yang, C. Liu, B. Li, J. Jiao, Q. Ye, Prototype mixture models for few-shot
    semantic segmentation, in: Proc. ECCV, Vol. abs/2008.03898, 2020.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] B. 杨, C. 刘, B. 李, J. 焦, Q. 叶, 用于少样本语义分割的原型混合模型，见：Proc. ECCV, Vol. abs/2008.03898,
    2020。'
- en: '[91] M. Boudiaf, H. Kervadec, I. M. Ziko, P. Piantanida, I. B. Ayed, J. Dolz,
    Few-shot segmentation without meta-learning: A good transductive inference is
    all you need?, in: Proc. CVPR, 2021, pp. 13974–13983.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. 布迪亚夫, H. 凯尔瓦代克, I. M. 齐科, P. 皮安塔尼达, I. B. 阿耶德, J. 多尔兹, 无需元学习的少样本分割：你只需要一个好的推断吗？，见：Proc.
    CVPR, 2021, pp. 13974–13983。'
- en: '[92] G. Li, V. Jampani, L. Sevilla-Lara, D. Sun, J. Kim, J. Kim, Adaptive prototype
    learning and allocation for few-shot segmentation, in: Proc. CVPR, 2021, pp. 8330–8339.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] G. 李, V. 詹帕尼, L. 塞维拉-拉拉, D. 孙, J. 金, J. 金, 用于少样本分割的自适应原型学习和分配，见：Proc.
    CVPR, 2021, pp. 8330–8339。'
- en: '[93] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, J. Jia, Prior guided feature
    enrichment network for few-shot segmentation, IEEE transactions on pattern analysis
    and machine intelligence PP.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. 田, H. 赵, M. 舒, Z. 杨, R. 李, J. 贾, 基于先验引导的特征增强网络用于少样本分割，《IEEE 模式分析与机器智能》PP。'
- en: '[94] Z. lu, S. He, X. Zhu, L. Zhang, Y.-Z. Song, T. Xiang, Simpler is better:
    Few-shot semantic segmentation with classifier weight transformer, in: Proc. ICCV,
    Vol. abs/2108.03032, 2021.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. 陆, S. 何, X. 朱, L. 张, Y.-Z. 宋, T. 项, 更简单更好：通过分类器权重变换器进行少样本语义分割，见：Proc.
    ICCV, Vol. abs/2108.03032, 2021。'
- en: '[95] J. Min, D. Kang, M. Cho, Hypercorrelation squeeze for few-shot segmentation,
    in: Proc. ICCV, Vol. abs/2104.01538, 2021.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. 闵, D. 康, M. 卓, 用于少样本分割的超相关挤压，见：Proc. ICCV, Vol. abs/2104.01538, 2021。'
- en: '[96] B. Zhang, J. Xiao, T. Qin, Self-guided and cross-guided learning for few-shot
    segmentation, in: Proc. CVPR, 2021, pp. 8308–8317.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] B. 张, J. 萧, T. 秦, 自我引导和交叉引导学习用于少样本分割，见：Proc. CVPR, 2021, pp. 8308–8317。'
- en: '[97] X. Wang, H. Zhou, Z. Chen, J. Bai, J. Ren, J. Shi, Few-shot sar ship image
    detection using two-stage cross-domain transfer learning, IGARSS 2022 - 2022 IEEE
    International Geoscience and Remote Sensing Symposium (2022) 2195–2198.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. 王, H. 周, Z. 陈, J. 白, J. 任, J. 石, 使用双阶段跨领域迁移学习的少样本 SAR 船只图像检测，IGARSS
    2022 - 2022 IEEE 国际地球科学与遥感研讨会 (2022) 2195–2198。'
- en: '[98] Y. Wang, Z. Xu, J. Tian, J. Luo, Z. Shi, Y. Zhang, J. Fan, Z. He, Cross-domain
    few-shot learning for rare-disease skin lesion segmentation, ICASSP 2022 - 2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
    (2022) 1086–1090.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Wang, Z. Xu, J. Tian, J. Luo, Z. Shi, Y. Zhang, J. Fan, Z. He, 跨域少样本学习用于稀有疾病皮肤病变分割，ICASSP
    2022 - 2022 IEEE 国际声学、语音和信号处理会议（ICASSP） (2022) 1086–1090。'
- en: '[99] Y. Zhang, W. Li, M. Zhang, S. Wang, R. Tao, Q. Du, Graph information aggregation
    cross-domain few-shot learning for hyperspectral image classification., IEEE transactions
    on neural networks and learning systems PP.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Zhang, W. Li, M. Zhang, S. Wang, R. Tao, Q. Du, 图信息聚合跨域少样本学习用于高光谱图像分类，IEEE
    神经网络与学习系统学报 PP。'
- en: '[100] Z. Li, M. Liu, Y. Chen, Y. Xu, W. Li, Q. Du, Deep cross-domain few-shot
    learning for hyperspectral image classification, IEEE Transactions on Geoscience
    and Remote Sensing 60 (2021) 1–18.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Li, M. Liu, Y. Chen, Y. Xu, W. Li, Q. Du, 深度跨域少样本学习用于高光谱图像分类，IEEE
    地球科学与遥感学报 60 (2021) 1–18。'
- en: '[101] Y. Zhang, W. Li, M. Zhang, R. Tao, Dual graph cross-domain few-shot learning
    for hyperspectral image classification, ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) (2022) 3573–3577.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Zhang, W. Li, M. Zhang, R. Tao, 双图跨域少样本学习用于高光谱图像分类，ICASSP 2022 - 2022
    IEEE 国际声学、语音和信号处理会议（ICASSP） (2022) 3573–3577。'
- en: '[102] B. Wang, Y. Xu, Z. Wu, T. Zhan, Z. Wei, Spatial–spectral local domain
    adaption for cross domain few shot hyperspectral images classification, IEEE Transactions
    on Geoscience and Remote Sensing 60 (2022) 1–15.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] B. Wang, Y. Xu, Z. Wu, T. Zhan, Z. Wei, 空间–光谱局部领域适应用于跨域少样本高光谱图像分类，IEEE
    地球科学与遥感学报 60 (2022) 1–15。'
- en: '[103] M. Rostami, S. Kolouri, E. Eaton, K. Kim, Sar image classification using
    few-shot cross-domain transfer learning, 2019 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW) (2019) 907–915.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Rostami, S. Kolouri, E. Eaton, K. Kim, 使用少样本跨域迁移学习进行 SAR 图像分类，2019
    IEEE/CVF 计算机视觉与模式识别会议研讨会（CVPRW） (2019) 907–915。'
- en: '[104] T. Tang, C. Qiu, T. Yang, W. Jingwei, M. Chen, J. Wu, L. Wang, A novel
    lightweight relation network for cross-domain few-shot fault diagnosis, SSRN Electronic
    Journal.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Tang, C. Qiu, T. Yang, W. Jingwei, M. Chen, J. Wu, L. Wang, 一种用于跨域少样本故障诊断的新型轻量关系网络，SSRN
    电子期刊。'
- en: '[105] Y. Feng, J. Chen, J. Xie, T. Zhang, H. Lv, T. Pan, Meta-learning as a
    promising approach for few-shot cross-domain fault diagnosis: Algorithms, applications,
    and prospects, Knowl. Based Syst. 235 (2021) 107646.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Feng, J. Chen, J. Xie, T. Zhang, H. Lv, T. Pan, 元学习作为一种有前景的方法用于少样本跨域故障诊断：算法、应用和前景，Knowl.
    Based Syst. 235 (2021) 107646。'
- en: '[106] X. Dai, C. Yang, B. Liu, H. gang Gong, X. Yuan, An easy way to deploy
    the re-id system on intelligent city: Cross-domain few-shot person reidentification,
    Mobile Information Systems.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. Dai, C. Yang, B. Liu, H. gang Gong, X. Yuan, 在智能城市上部署重新识别系统的简易方法：跨域少样本人员重新识别，移动信息系统。'
- en: '[107] H. Hou, S. Bi, L. Zheng, X. Lin, Z. Quan, Sample-efficient cross-domain
    wifi indoor crowd counting via few-shot learning, 2022 31st Wireless and Optical
    Communications Conference (WOCC) (2022) 132–137.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Hou, S. Bi, L. Zheng, X. Lin, Z. Quan, 样本高效跨域 WiFi 室内人群计数通过少样本学习，2022
    第31届无线与光学通信大会（WOCC） (2022) 132–137。'
- en: '[108] G. Sun, Z. Liu, L. Wen, J. Shi, C. Xu, Anomaly crossing: A new method
    for video anomaly detection as cross-domain few-shot learning, ArXiv abs/2112.06320.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] G. Sun, Z. Liu, L. Wen, J. Shi, C. Xu, 异常穿越：一种用于视频异常检测的新方法，作为跨域少样本学习，ArXiv
    abs/2112.06320。'
- en: '[109] X. Zhou, Y. Mu, Google helps youtube: Learning few-shot video classification
    from historic tasks and cross-domain sample transfer, Proceedings of the 2020
    International Conference on Multimedia Retrieval.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] X. Zhou, Y. Mu, 谷歌帮助 YouTube：从历史任务和跨域样本转移中学习少样本视频分类，2020 年国际多媒体检索会议论文集。'
- en: '[110] K. Wu, J. Tan, C. Liu, Cross-domain few-shot learning approach for lithium-ion
    battery surface defects classification using an improved siamese network, IEEE
    Sensors Journal 22 (2022) 11847–11856.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. Wu, J. Tan, C. Liu, 使用改进的孪生网络的跨域少样本学习方法用于锂离子电池表面缺陷分类，IEEE 传感器学报 22
    (2022) 11847–11856。'
- en: '[111] S. Qiu, C. Zhu, W. Zhou, Meta self-learning for multi-source domain adaptation:
    A benchmark, 2021 IEEE/CVF International Conference on Computer Vision Workshops
    (ICCVW) (2021) 1592–1601.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Qiu, C. Zhu, W. Zhou, 元自学习用于多源领域适应：一个基准，2021 IEEE/CVF 计算机视觉国际会议研讨会（ICCVW）
    (2021) 1592–1601。'
- en: '[112] S. Min, H. Yao, H. Xie, C. Wang, Z. Zha, Y. Zhang, Domain-aware visual
    bias eliminating for generalized zero-shot learning, 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR) (2020) 12661–12670.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. Min, H. Yao, H. Xie, C. Wang, Z. Zha, Y. Zhang, 领域感知视觉偏差消除用于广义零样本学习，2020
    IEEE/CVF 计算机视觉与模式识别大会 (CVPR) (2020) 12661–12670。'
- en: '[113] S. Min, H. Yao, H. Xie, Z. Zha, Y. Zhang, Domain-specific embedding network
    for zero-shot recognition, Proceedings of the 27th ACM International Conference
    on Multimedia.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] S. Min, H. Yao, H. Xie, Z. Zha, Y. Zhang, 特定领域嵌入网络用于零样本识别，发表于第27届ACM国际多媒体大会。'
- en: '[114] S. Wang, J. Yue, J. Liu, Q. Tian, M. Wang, Large-scale few-shot learning
    via multi-modal knowledge discovery, in: European Conference on Computer Vision,
    2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Wang, J. Yue, J. Liu, Q. Tian, M. Wang, 通过多模态知识发现的大规模小样本学习，发表于：欧洲计算机视觉会议，2020年。'
- en: '[115] Z. Peng, Z. Li, J. Zhang, Y. Li, G.-J. Qi, J. Tang, Few-shot image recognition
    with knowledge transfer, 2019 IEEE/CVF International Conference on Computer Vision
    (ICCV) (2019) 441–449.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Peng, Z. Li, J. Zhang, Y. Li, G.-J. Qi, J. Tang, 知识迁移的小样本图像识别，2019
    IEEE/CVF 国际计算机视觉大会 (ICCV) (2019) 441–449。'
- en: '[116] J. Guan, M. Zhang, Z. Lu, Large-scale cross-domain few-shot learning,
    in: Asian Conference on Computer Vision, 2020.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Guan, M. Zhang, Z. Lu, 大规模跨域小样本学习，发表于：亚洲计算机视觉会议，2020年。'
- en: '[117] M. Salehi, H. Mirzaei, D. Hendrycks, Y. Li, M. H. Rohban, M. Sabokrou,
    A unified survey on anomaly, novelty, open-set, and out-of-distribution detection:
    Solutions and future challenges, ArXiv abs/2110.14051.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Salehi, H. Mirzaei, D. Hendrycks, Y. Li, M. H. Rohban, M. Sabokrou,
    关于异常、novelty、开放集和分布外检测的统一调查：解决方案与未来挑战，ArXiv abs/2110.14051。'
- en: '[118] B. Liu, H. Kang, H. Li, G. Hua, N. Vasconcelos, Few-shot open-set recognition
    using meta-learning, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 8795–8804.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] B. Liu, H. Kang, H. Li, G. Hua, N. Vasconcelos, 利用元学习的小样本开放集识别，2020 IEEE/CVF
    计算机视觉与模式识别大会 (CVPR) (2020) 8795–8804。'
- en: '[119] M. Jeong, S. Choi, C. Kim, Few-shot open-set recognition by transformation
    consistency, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 12561–12570.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Jeong, S. Choi, C. Kim, 通过变换一致性的小样本开放集识别，2021 IEEE/CVF 计算机视觉与模式识别大会
    (CVPR) (2021) 12561–12570。'
- en: '[120] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, Y. Gong, Few-shot class-incremental
    learning, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 12180–12189.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, Y. Gong, 小样本类别增量学习，2020 IEEE/CVF
    计算机视觉与模式识别大会 (CVPR) (2020) 12180–12189。'
- en: '[121] K. Zhu, Y. Cao, W. Zhai, J. Cheng, Z. Zha, Self-promoted prototype refinement
    for few-shot class-incremental learning, 2021 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR) (2021) 6797–6806.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] K. Zhu, Y. Cao, W. Zhai, J. Cheng, Z. Zha, 自我促成的原型细化用于小样本类别增量学习，2021
    IEEE/CVF 计算机视觉与模式识别大会 (CVPR) (2021) 6797–6806。'
- en: '[122] D. A. Ganea, B. Boom, R. Poppe, Incremental few-shot instance segmentation,
    2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)
    1185–1194.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] D. A. Ganea, B. Boom, R. Poppe, 增量式小样本实例分割，2021 IEEE/CVF 计算机视觉与模式识别大会
    (CVPR) (2021) 1185–1194。'
- en: '[123] K. D. M. Nguyen, S. Todorovic, ifs-rcnn: An incremental few-shot instance
    segmenter, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2022) 7000–7009.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. D. M. Nguyen, S. Todorovic, ifs-rcnn: 一种增量式小样本实例分割器，2022 IEEE/CVF
    计算机视觉与模式识别大会 (CVPR) (2022) 7000–7009。'
- en: \bio
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: ./bio/wwj.jpg Wenjian Wang Wenjian Wang received an M.S degree in computer science
    from North University of China. He is currently studying for a Ph.D. degree at
    the Faculty of Information Technology, Beijing University of Technology, China.
    His current research interests include Image processing, machine learning, and
    computer vision.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/wwj.jpg 温健 王 温健获得了中国北方大学计算机科学硕士学位。他目前在中国北京工业大学信息技术学院攻读博士学位。他的当前研究兴趣包括图像处理、机器学习和计算机视觉。
- en: \endbio
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: ./bio/dlj.PNG Lijuan Duan received the B.Sc. and M.Sc. degrees in computer science
    from Zhengzhou University of Technology, Zhengzhou, China, in 1995 and 1998, respectively.
    She received the Ph.D. degree in computer science from the Institute of Computing
    Technology, Chinese Academy of Sciences, Beijing, in 2003\. She is currently a
    Professor at the Faculty of Information Technology, Beijing University of Technology,
    China. Her research interests include Artificial Intelligence, Image Processing,
    Computer Vision and Information Security. She has published more than 70 research
    articles in refereed journals and proceedings on artificial intelligent, image
    processing and computer vision.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/dlj.PNG**段莉娟**于1995年和1998年分别从郑州大学获得计算机科学学士和硕士学位。她于2003年从中国科学院计算技术研究所获得计算机科学博士学位。她目前是北京工业大学信息技术学院的教授。她的研究兴趣包括人工智能、图像处理、计算机视觉和信息安全。她在人工智能、图像处理和计算机视觉领域的同行评审期刊和会议上发表了70多篇研究文章。
- en: \endbio\bio
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio\bio
- en: ./bio/yuxi_wang.pdfYuxi Wang received the Bachelor degree from North Eastern
    University, China, in 2016, and the PhD degree from University of Chinese Academy
    of Sciences (UCAS), Institute of Automation, Chinese Academy of Sciences (CASIA),
    in January 2022\. He is now an assistant professor in the Center for Artificial
    Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation,
    Chinese Academy of Science (HKISI-CAS). His research interests include transfer
    learning, domain adaptation and computer vision.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/yuxi_wang.pdf**王宇曦**于2016年从中国东北大学获得学士学位，并于2022年1月从中国科学院自动化研究所（CASIA）中国科学院大学（UCAS）获得博士学位。他现在是香港科学与创新研究院（HKISI-CAS）人工智能与机器人中心（CAIR）的助理教授。他的研究兴趣包括迁移学习、领域适应和计算机视觉。
- en: \endbio
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: ./bio/fjs.jpgJunsong Fan received his bachelor’s degree from the School of Automation
    Science and Electrical Engineering, Beihang University, in 2016\. Then, he joined
    the Center of Research on Intelligent Perception and Computing, National Laboratory
    of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences,
    under the supervision of Professor Tieniu Tan and Zhaoxiang Zhang, and received
    his Ph.D. degree in 2022\. He is now an assistant professor in the Centre for
    Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation,
    Chinese Academy of Sciences. His research interests include computer vision and
    machine learning, label-free learning, and visual perception in open worlds.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/fjs.jpg**范俊松**于2016年从北航大学自动化科学与电气工程学院获得学士学位。随后，他在中国科学院自动化研究所模式识别国家重点实验室的智能感知与计算研究中心，师从谭天宇教授和张兆翔教授，并于2022年获得博士学位。他现在是香港科学与创新研究院人工智能与机器人中心的助理教授。他的研究兴趣包括计算机视觉和机器学习、无标签学习以及开放世界中的视觉感知。
- en: \endbio\bio
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio\bio
- en: ./bio/gz.PNGZhi Gong Zhi Gong (Student Member,IEEE) received the B.E. degree
    incomputer science and technology from Shandong Normal University,Shandong,China,in2019\.
    He is currently pursuing the Ph.D. degree in computer science and technology from
    the Beijing University of Technology,Beijing,China. His research interests include
    computer vision and image processing.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/gz.PNG**龚智**（学生会员，IEEE）于2019年从山东师范大学获得计算机科学与技术学士学位。他目前正在北京工业大学攻读计算机科学与技术博士学位。他的研究兴趣包括计算机视觉和图像处理。
- en: \endbio
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
- en: \bio
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: \bio
- en: ./bio/zhaoxiangzhang.pdfZhaoxiang Zhang received his bachelor’s degree from
    the Department of Electronic Science and Technology in the University of Science
    and Technology of China (USTC) in 2004\. After that, he was a Ph.D. candidate
    under the supervision of Professor Tieniu Tan in the National Laboratory of Pattern
    Recognition, Institute of Automation, Chinese Academy of Sciences, where he received
    his Ph.D. degree in 2009\. In October 2009, he joined the School of Computer Science
    and Engineering, Beihang University, as an Assistant Professor (2009-2011), an
    Associate Professor (2012-2015) and the Vise-Director of the Department of Computer
    application technology (2014-2015). In July 2015, he returned to the Institute
    of Automation, Chinese Academy of Sciences. He is now a full Professor in the
    Center for Research on Intelligent Perception and Computing (CRIPAC) and the National
    Laboratory of Pattern Recognition (NLPR). His research interests include Computer
    Vision, Pattern Recognition, and Machine Learning. Recently, he specifically focuses
    on deep learning models, biologically-inspired visual computing and human-like
    learning, and their applications on human analysis and scene understanding. He
    has published more than 200 papers in international journals and conferences,
    including reputable international journals such as JMLR, IEEE TIP, IEEE TNN, IEEE
    TCSVT, IEEE TIFS and top level international conferences like CVPR, ICCV, NIPS,
    ECCV, AAAI, IJCAI and ACM MM. He is serving or has served as the Associated Editor
    of IEEE T-CSVT, Patten Recognition, Neurocomputing, and Frontiers of Computer
    Science. He has served as the Area Chair, Senior PC of international conferences
    like CVPR, NIPS, ICML, AAAI, IJCAI and ACM MM. He is a Senior Member of IEEE,
    a Distinguished Member of CCF, and a Distinguished member of CAAI.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ./bio/zhaoxiangzhang.pdf赵向张于2004年获得中国科学技术大学（USTC）电子科学与技术系的学士学位。之后，他在中国科学院自动化研究所模式识别国家实验室，师从**谭铁牛**教授，成为博士研究生，并于2009年获得博士学位。2009年10月，他加入了北京航空航天大学计算机科学与工程学院，担任助理教授（2009-2011）、副教授（2012-2015）以及计算机应用技术系副主任（2014-2015）。2015年7月，他回到中国科学院自动化研究所。目前，他是智能感知与计算研究中心（CRIPAC）和模式识别国家实验室（NLPR）的**教授**。他的研究兴趣包括计算机视觉、模式识别和机器学习。最近，他特别关注深度学习模型、生物启发的视觉计算和类人学习，以及这些技术在人类分析和场景理解中的应用。他在国际期刊和会议上发表了200多篇论文，包括JMLR、IEEE
    TIP、IEEE TNN、IEEE TCSVT、IEEE TIFS等知名国际期刊，以及CVPR、ICCV、NIPS、ECCV、AAAI、IJCAI和ACM
    MM等顶级国际会议。他担任或曾担任IEEE T-CSVT、《模式识别》、《神经计算》和《计算机科学前沿》的副编辑，并在CVPR、NIPS、ICML、AAAI、IJCAI和ACM
    MM等国际会议上担任领域主席、高级程序委员会成员。他是IEEE的高级会员、CCF的杰出会员和CAAI的杰出会员。
- en: \endbio
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: \endbio
