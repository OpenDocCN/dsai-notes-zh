- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2303.09253] A Survey of Deep Visual Cross-Domain Few-Shot Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.09253](https://ar5iv.labs.arxiv.org/html/2303.09253)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[orcid=0000-0002-3941-8952]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Conceptualization of this study, Methodology, Software
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Data curation, Writing - Original draft preparation
  prefs: []
  type: TYPE_NORMAL
- en: A Survey of Deep Visual Cross-Domain Few-Shot Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wenjian Wang wangwj@emails.bjut.edu.cn    Lijuan Duan ljduan@bjut.edu.cn   
    Yuxi Wang yuxiwang93@gmail.com    Junsong Fan junsong.fan@ia.ac.cn    Zhi Gong
    gongzhi97@emails.bjut.edu.cn    Zhaoxiang Zhang zhaoxiang.zhang@ia.ac.cn Faculty
    of Information Technology, Beijing University of Technology, China Beijing Key
    Laboratory of Trusted Computing, Beijing, China National Engineering Laboratory
    for Key Technologies of Information Security Level Protection, Beijing, China
    Centre for Artificial Intelligence and Robotics, (HKISI_CAS) Institute of Automation,
    Chinese Academy of Sciences (NLPR, CASIA, UCAS)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Few-Shot transfer learning has become a major focus of research as it allows
    recognition of new classes with limited labeled data. While it is assumed that
    train and test data have the same data distribution, this is often not the case
    in real-world applications. This leads to decreased model transfer effects when
    the new class distribution differs significantly from the learned classes. Research
    into Cross-Domain Few-Shot (CDFS) has emerged to address this issue, forming a
    more challenging and realistic setting. In this survey, we provide a detailed
    taxonomy of CDFS from the problem setting and corresponding solutions view. We
    summarise the existing CDFS network architectures and discuss the solution ideas
    for each direction the taxonomy indicates. Furthermore, we introduce various CDFS
    downstream applications and outline classification, detection, and segmentation
    benchmarks and corresponding standards for evaluation. We also discuss the challenges
    of CDFS research and explore potential directions for future investigation. Through
    this review, we aim to provide comprehensive guidance on CDFS research, enabling
    researchers to gain insight into the state-of-the-art while allowing them to build
    upon existing solutions to develop their own CDFS models.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Few-Shot Learning \sepCross-Domain \sepSurvey
  prefs: []
  type: TYPE_NORMAL
- en: 1 INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Strong computing power has boosted the development of deep learning, thereby
    aiding computer vision in making considerable progress. Research fields such as
    image classification, object detection, and semantic segmentation continue to
    shape the development of computer vision in innovative ways. Current deep models
    require a large volume of annotated data for training, but this data is typically
    expensive and labor-intensive. Few labeled data are in certain fields to enable
    the model to recognize new categories, triggering the need for few-shot learning
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. Although few-shot learning assumes
    that the training and testing data come from the same domain, domain shift is
    commonly seen in real-world scenarios [[4](#bib.bib4)]. In these contexts, cross-domain
    few-shot learning (CDFS) offers a promising solution to the few-shot learning
    problem by simultaneously addressing both domain shift and data scarcity [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper reviews and categorizes the current research in CDFS based on problem
    sets, solutions, and applications. Regarding problem setting, we identify two
    main models for CDFS research with multiple data sources: single- and multiple-model
    (where the latter involves training a model for each source and subsequently aggregating
    them). For single-source modeling, sub-categorizations are achieved based on whether
    the target domain data is accessible (through supervised/unsupervised means) or
    forbidden. We also comprehensively introduce the popular CDFS classification benchmark
    datasets and highlight the unified transfer effects across datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding solution, we cover four main approaches: Image Augmentation, Feature
    Augmentation, Decoupling, and Fine-tuning. Image and Feature Augmentation rely
    on the mutual image/feature transformation to enrich data distribution. At the
    same time, Decoupling Based Methods distinguish domain-irrelevant from domain-specific
    features, and Fine-tuning uses meta-learning and distribution alignment to extract
    transferable features. We further discuss its application to various fields and
    identify potential future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of this article are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We summarized the cross-domain research in few-shot learning. The existing solutions
    are summarized in detail according to the different problem settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For different CDFS solutions, we give a detailed sorting and classification
    from features, images, and network architecture. We also summarize the application
    of CDFS in different scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We gave the benchmark of the current CDFS, providing a unified reference precision
    for subsequent research. We provide the future direction of CDFS in terms of challenges,
    technical solutions, and applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/765d668f3fa1d9226c7342cdf0b0805a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The related researches on few-shot learning increased from 2017 to
    2022.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 CONCEPTS AND PRELIMINARIES
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Few-Shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-Shot Learning (FSL) is a transfer learning technique designed to learn novel
    classes from limited annotated labels [[8](#bib.bib8)]. In FSL, two assumptions
    must be met to ensure its successful application. Firstly, the classes between
    the train and finetune process must be distinct; there must be no class intersection
    between the two processes, i.e., $C_{b}\cap C_{n}=\varnothing$. Secondly, the
    annotation labels for each class in the finetune process must be limited (or even
    only have one annotation label). Specifically, there are two datasets $D_{b}$
    and $D_{n}$, where the classes $C_{b}\in D_{b}$ have sufficient labels and $C_{n}\in
    D_{n}$ only have limited labels. The FSL model aims to train on dataset $D_{b}$,
    then apply a few annotation labels to finetune novel classes of dataset $D_{n}$
    during the test process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Few-Shot Domain Adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Few-Shot Domain Adaptation (FSDA) is a powerful combination of Few-Shot and
    Domain Adaptation, making Few-Shot learning more difficult. Unlike standard Few-Shot
    Learning, which assumes a shared data distribution between the training dataset
    $D_{b}$ and finetuning dataset $D_{n}$, FSDA identifies a domain gap between the
    two datasets. Two kinds of FSDA: the supervised kind, explored in [[9](#bib.bib9)],
    and the unsupervised kind, explored in [[10](#bib.bib10)]. This research seeks
    to bridge the difference between the two domains and fully use the limited annotations
    available. The goal of FSDA is thus to align two data domains while utilizing
    the limited annotation data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Cross-Domain Few-Shot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-domain few-shot (CDFS) learning, a recently emerged field in few-shot
    learning, assumes there is a significant domain gap between source and target
    domains, making the task more challenging compared to the traditional few-shot
    domain adaptation (FSDA) approach [[5](#bib.bib5)]. As shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 INTRODUCTION ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"),
    the interest in CDFS research has grown steadily from 2020 to 2022\. The following
    sections provide a detailed overview of the problem sets, solutions, and applications
    of CDFS. Section [3](#S3 "3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") summarizes the problem divisions of CDFS, Section
    [4](#S4 "4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning") reviews the different solutions proposed for CDFS, Section [5](#S5
    "5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") provides
    the benchmark, Section [6](#S6 "6 Application OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") provides various applications of CDFS, and Section
    [7](#S7 "7 FUTURE DIRECTION ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")
    outlines directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Domain Adaptation & Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Domain Adaptation (DA) requires a model to be trained on a source domain and
    tested on a target domain with different data distribution. This can be done in
    a supervised [[11](#bib.bib11), [12](#bib.bib12)] or unsupervised manner [[13](#bib.bib13),
    [14](#bib.bib14)], assuming the same classes are present in both domains. DA focuses
    on aligning the distributions of two domains [[15](#bib.bib15)]. In an unsupervised
    setting, the source model trains the feature extractor using the data of the target
    domain, thus accomplishing domain alignment. In contrast, Domain Generalization
    (DG) is more complex and valuable than DA [[16](#bib.bib16)], as the target domain
    data is not accessible during training [[17](#bib.bib17)]. After completing source
    domain training, the model must be tested directly on the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Meta-Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Humans constantly summarize experiences during the learning process. When faced
    with a novel environment, humans can draw upon acquired knowledge to adapt quickly
    to new tasks. Meta-Learning, or "learning to learn," is an approach that collects
    experiences across various tasks to provide valuable information for transfer
    learning [[18](#bib.bib18)]. As a form of meta-learning, task learning encompasses
    various applications, including classification problems, regression, and mixed
    tasks [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]. Meta-learning aims
    to distill knowledge, which can be manifested in various network architectures.
    For example, Reptile [[22](#bib.bib22)], a model-agnostic meta-learning algorithm,
    utilizes the prediction of model parameters as its knowledge representation. Alternatively,
    DMML [[23](#bib.bib23)] and OFMA [[24](#bib.bib24)] encode knowledge via an embedding
    function and an initialization parameter, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The different related researches. DA and DG transfer source knowledge
    to target domain, but DA have the accessible of target data. FSL assume that only
    have limited fine-tuning data and have no class intersection between training
    and testing data. CDFS and FSDA consider the few-shot learning and domain shift
    together, but FSDA have the target domain accessible and have the same domain
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Research | Training Domain | Test Domain | Domain Class Intersection | Fine-tuning
    Size | Test Data Training |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-Domain Few-Shot | $D_{s}$ | $D_{t}$ | No | Few | Forbidden |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Domain Adaptation | $D_{s}$ | $D_{t}$ | Same | Few | Accessible
    |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Learning | $D_{s}$ | $D_{s}$ | No | Few | Inexistent |'
  prefs: []
  type: TYPE_TB
- en: '| Domain Adaptation | $D_{s}$ | $D_{t}$ | Same | Sufficient | Accessible |'
  prefs: []
  type: TYPE_TB
- en: '| Domain Generalization | $D_{s}$ | $D_{t}$ | Same | Inexistent | Forbidden
    |'
  prefs: []
  type: TYPE_TB
- en: <svg id="S2.F2.1.pic1" class="ltx_picture ltx_centering" height="670.42" overflow="visible"
    version="1.1" width="937.78"><g transform="translate(0,670.42) matrix(1 0 0 -1
    0 0) translate(404.74,0) translate(0,538.92)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -399.85 -273.63)" fill="#000000"
    stroke="#000000"><foreignobject width="12.3" height="114.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Few-Shot Learning</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 -359.14 -132.18)" fill="#000000" stroke="#000000"><foreignobject width="9.61"
    height="146.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Cross-Domain
    Few-Shot</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -360.48 -484.6)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="181.8" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Few-Shot Domain-Adaptation</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 30.87)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="95.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Source</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -310.07 -99.43)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="80.75" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Single Source</foreignobject></g>  <g transform="matrix(1.0
    0.0 0.0 1.0 -310.07 -230.98)" fill="#000000" stroke="#000000"><foreignobject width="9.61"
    height="68.26" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Benchmark</foreignobject></g>  <g
    transform="matrix(1.0 0.0 0.0 1.0 -310.07 -368.64)" fill="#000000" stroke="#000000"><foreignobject
    width="12.3" height="107.35" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised
    FSDA</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 -310.07 -534.03)"
    fill="#000000" stroke="#000000"><foreignobject width="12.3" height="123.19" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Unsupervised FSDA</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -243.27 114.65)" fill="#000000" stroke="#000000"><foreignobject width="93.4"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Model</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -238.51 35.91)"
    fill="#000000" stroke="#000000"><foreignobject width="83.48" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Single Models</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -270.7 -23.14)" fill="#000000" stroke="#000000"><foreignobject width="157.59"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Target Domain
    Accessible</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -270.7 -101.88)"
    fill="#000000" stroke="#000000"><foreignobject width="158.43" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Target Domain Forbidden</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -183.01)" fill="#000000" stroke="#000000"><foreignobject
    width="602.28" height="251.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSCD-FSL
    [[6](#bib.bib6)],Meta-Dataset [[25](#bib.bib25)],PATNet [[26](#bib.bib26)],MoF-SOD
    [[27](#bib.bib27)],U-CDFSL [[28](#bib.bib28)], H-CDFSL [[29](#bib.bib29)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -93.54 116.73)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)], CosML [[31](#bib.bib31)],URL [[32](#bib.bib32)] <g fill="#FFFFB3"><path
    d="M -98.15 10.54 h 343.87 v 57.65 h -343.87 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -93.54 53.9)" fill="#000000" stroke="#000000"><foreignobject width="334.65"
    height="48.43" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MDLCC [[33](#bib.bib33)],LFT
    [[7](#bib.bib7)],TSA [[34](#bib.bib34)],CDNet [[35](#bib.bib35)],ISS [[36](#bib.bib36)],DAML
    [[37](#bib.bib37)]</foreignobject></g> <g fill="#FFFFB3"><path d="M -98.15 -56.81
    h 343.87 v 74.26 h -343.87 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    -93.54 3.15)" fill="#000000" stroke="#000000"><foreignobject width="334.65" height="65.03"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)],
    DDN [[39](#bib.bib39)],DAFL [[40](#bib.bib40)],OA-FSUI2IT [[41](#bib.bib41)],
    ME-D2N [[42](#bib.bib42)],TGDM [[43](#bib.bib43)],AcroFOD [[44](#bib.bib44)],CDSC-FSL
    [[45](#bib.bib45)],WULD [[46](#bib.bib46)],CutMix [[47](#bib.bib47)]</foreignobject></g>
    <g fill="#FFFFB3"><path d="M -98.15 -160.46 h 343.87 v 124.07 h -343.87 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.54 -50.69)" fill="#000000" stroke="#000000"><foreignobject
    width="334.65" height="114.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CMT
    [[48](#bib.bib48)], ProtoTransfer [[49](#bib.bib49)],RMP [[50](#bib.bib50)],NSA
    [[51](#bib.bib51)],ATA [[52](#bib.bib52)],DCM+SS [[53](#bib.bib53)],TL-SS [[54](#bib.bib54)],RDC
    [[55](#bib.bib55)],RD [[5](#bib.bib5)],AFA [[56](#bib.bib56)],LCCS [[57](#bib.bib57)],MemREIN
    [[58](#bib.bib58)],Drop-CDFS [[59](#bib.bib59)],TMH [[60](#bib.bib60)],FTE [[61](#bib.bib61)],
    SB-MTL [[62](#bib.bib62)],STCDFS [[63](#bib.bib63)],Wave-SAN [[64](#bib.bib64)],ReFine
    [[65](#bib.bib65)]</foreignobject></g> <g stroke-width="0.8pt"><path d="M -117.56
    118.11 L -99.41 118.11" style="fill:none"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -99.41 118.11)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -117.56 39.37 L -99.41 39.37" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 39.37)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -108.23 -19.69 L -99.41 -19.69" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 -19.69)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -108.53 -98.43 L -99.41 -98.43" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.41 -98.43)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    fill="#FFFFB3"><path d="M -275.31 -327.87 h 808.08 v 25.83 h -808.08 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -317.04)" fill="#000000" stroke="#000000"><foreignobject
    width="798.86" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FSADA
    [[9](#bib.bib9)],APT [[66](#bib.bib66)],PixDA [[67](#bib.bib67)]</foreignobject></g><g
    fill="#FFFFB3"><path d="M -275.31 -492.97 h 791.86 v 41.05 h -791.86 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -270.7 -466.91)" fill="#000000" stroke="#000000"><foreignobject
    width="783.02" height="123.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FUDA
    [[10](#bib.bib10)],IMSE [[68](#bib.bib68)],S3T [[69](#bib.bib69)],OUCD [[70](#bib.bib70)],MixUp-OT
    [[71](#bib.bib71)]</foreignobject></g><g stroke-width="0.8pt"><path d="M -292.88
    -314.96 L -276.57 -314.96" style="fill:none"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -276.57 -314.96)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g
    stroke-width="0.8pt"><path d="M -292.88 -472.44 L -276.57 -472.44" style="fill:none"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -276.57 -472.44)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M
    -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Taxonomy of CDFS. For domain researches of CDFS, we split the Cross-Domain
    Few-Shot into Multiple Source and Single Source setting. For Multiple Source,
    we divide the research into Multiple Models and Single Model according to the
    models used. For Single Source, we divide the research into Target Domain Accessible
    and Target Domain Forbidden according to whether the target domain can access.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 DIFFERENT SETTING OF CDFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ba12d4d353eb38e0d85a2597eac60f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Different setting of CDFS. For the CDFS research, there are Multiple
    Source and Single Source setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We group recent CDFS research into three categories: Multiple Source, Single
    Source, and Benchmark. Multiple Source research includes two solutions: multiple
    models, and a single model. Single Source can be divided into two subcategories,
    depending on whether the target domain is accessible or forbidden, as depicted
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"). We have also provided a summary of benchmarks
    for CDFS in various research fields.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Multiple Source CDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different domains can provide essential data distributions for deep model learning,
    making it intuitive to collect multiple source domain data to address the CDFS
    problem. Specifically, researchers first collect several source domain datasets
    with disjoint classes. They then employ specific training strategies to capture
    domain information and combine the results into a final transfer model. Typically,
    these domains span various scenarios, such as CropDisease [[72](#bib.bib72)],
    EuroSAT [[73](#bib.bib73)], and ChestX [[74](#bib.bib74)]. The learning strategies
    of Multiple Source CDFS can be divided into Multiple-Model and Single-Model approaches,
    as depicted in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Multiple-Model For Multiple Source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The domain will have its specific model for the Multiple-Model Methods. The
    learning process contains the training and aggregation stage. Each domain optimizes
    the domain-specific model in the training stage, then combines all the models
    into a single model, which will serve as the final transfer model in the aggregation
    stage, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A
    Survey of Deep Visual Cross-Domain Few-Shot Learning") left part. The training
    stage gives the model diverse feature distribution or highly adaptable parameters.
    The subsequent aggregation stage focuses on feature reuse, which aims to achieve
    the best transfer effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/477b2138b25ca9b470d9c18b73c32bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The URL [[32](#bib.bib32)] use the knowledge distillation to collect
    multiple source information.'
  prefs: []
  type: TYPE_NORMAL
- en: The URL [[32](#bib.bib32)] is a typical model based on Multiple-Model Methods,
    as shown the Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Multiple-Model For Multiple Source
    ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). The framework first trains $K$ teacher network
    $\{f_{\phi_{1}^{*}},f_{\phi_{2}^{*}}...f_{\phi_{K}^{*}}\}$ for $K$ domains on
    the training stage, then use the student network $f_{\phi}$ to distill the domain
    information on the aggregation stage. The student network $f_{\phi}$ can observe
    all the source domain datasets, but the label comes from the teacher networks’
    output. URL [[32](#bib.bib32)] focuses on aggregation learning and aims to learn
    a single set of universal representations. For the aggregation, use the Domain-specific
    adaptors $\{A_{\theta_{1}},A_{\theta_{2}}...A_{\theta_{K}}\}$ and Classifiers
    $\{h_{\psi_{1}},h_{\psi_{2}}...h_{\psi_{K}}\}$ to match the teacher networks’
    output. URL [[32](#bib.bib32)] has a fixed computational cost regardless of the
    number of domains at inference unlike them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a44e49eff2454b450ec8edf499694ac5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The SRF [[30](#bib.bib30)] focus on the more relevant domain feature,
    use multi-domain feature bank to automatically select the most relevant representations.'
  prefs: []
  type: TYPE_NORMAL
- en: SRF [[30](#bib.bib30)] consider that more relevant domain features should be
    concerned, and irrelevant domain should be inhibited on the aggregation stage.
    The framework uses a set of learnable parameters $\lambda$ to select the source
    domain feature. First, SRF [[30](#bib.bib30)] train a set of $K$ feature extractors
    and obtain a multi-domain feature representation, consisting of feature blocks
    with different semantics, as shown Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.1 Multiple-Model
    For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning") left part. Then given
    a few-shot task, select only the relevant feature blocks from the multi-domain
    representation by optimizing masking parameters $\lambda$ on the support set.
    SRF [[30](#bib.bib30)] shows that a simple feature selection mechanism can replace
    feature adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b30710796279eae14d40f20fcced1552.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The CosML [[31](#bib.bib31)] first trains a set of meta-learners,
    one for each training domain, to learn prior knowledge (i.e., meta-parameters)
    specific to each domain. The domain-specific meta-learners are then combined in
    the parameter space, by taking a weighted average of their meta-parameters, which
    is used as the initialization parameters of a task network that is quickly adapted
    to novel few-shot classification tasks in an unseen domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared with feature level transfer, CosML [[31](#bib.bib31)] focuses on the
    parameter aggregation, as shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.1 Multiple-Model
    For Multiple Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣
    A Survey of Deep Visual Cross-Domain Few-Shot Learning"). For CosML [[31](#bib.bib31)],
    it first get parameters $\theta_{D_{k}}$ for each domain. $\theta_{D_{k}}$ have
    a certain transfer ability to the new domain, but they lack communication. For
    the final model parameter $\theta$, CosML [[31](#bib.bib31)] combine all domain
    parameters $\theta_{D_{k}}$ with a domain weight $\alpha_{D_{k}}$ which can reflect
    the domain’s importance, with Eq. [1](#S3.E1 "In 3.1.1 Multiple-Model For Multiple
    Source ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\theta=\alpha_{D_{1}}\theta_{D_{1}}+\alpha_{D_{2}}\theta_{D_{2}}+...+\alpha_{D_{k}}\theta_{D_{k}}\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 3.1.2 Single-Model For Multiple Source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-Model Methods apply a unified model to learn the multiple domain feature.
    The Single-Model Methods can train all the source domain data, as shown in Fig.
    [3](#S3.F3 "Figure 3 ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning") right part. Compared with Multiple-Models Methods, the Single-Model
    Methods have fewer calculation and omit the multimodel’s aggregation stage. For
    getting high adaptability parameters, DAML [[37](#bib.bib37)] focuses on the optimization
    strategy and learns to adapt the model to novel classes in both seen and unseen
    domains by data sampled from multiple domains.
  prefs: []
  type: TYPE_NORMAL
- en: LFT [[7](#bib.bib7)] and TSA [[34](#bib.bib34)] propose to attach feature transformation
    or adapters directly to a pre-trained model. The multiple source domain collectively
    trains these modules, improving the model’s transfer ability.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d3b899b34d3ccdea45c831126c1e813.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The CDNet [[35](#bib.bib35)] learns the ability of learn-to-decompose
    that can be easily adapted to identify unseen compound expressions.'
  prefs: []
  type: TYPE_NORMAL
- en: The domain-specific information may be confused as the single model faces more
    than one source domain. To relieve this problem, CDNet [[35](#bib.bib35)] decomposes
    domain information from a given feature and extracts a domain-independent expression,
    as shown in the Fig. [7](#S3.F7 "Figure 7 ‣ 3.1.2 Single-Model For Multiple Source
    ‣ 3.1 Multiple Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). CDNet [[35](#bib.bib35)] stacks serial LD (Learn-to-Decompose)
    module, which output the class feature, and SD (Sequential Decomposition) module
    to decompose the domain-agnostic and domain-specific feature. Similarly, MDLCC
    [[33](#bib.bib33)] through the channel re-weighting module to decompose the different
    domain features. After the training, the model only needs to finetune the re-weighting
    module with fewer parameter changes to adapt to the new domain.
  prefs: []
  type: TYPE_NORMAL
- en: To relieve the domain labels’ cost, ISS [[36](#bib.bib36)] uses one source domain
    with labels and several other unlabeled domain datasets to train a single model.
    The labeled domain input will be mix-enhanced with other unlabeled domain data.
    This can expand the labeled domain data distribution with other domains’ stylization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Discussion and Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-source cross-domain few-shot learning uses multi-source data to improve
    the model’s scalability. In order to optimize the learning process, the decision
    should analyze the efficiency, accuracy, and scalability. To this end, there are
    two different methods: the multi-model method (which requires communication between
    sources) and the single-model method (which directly processes source data and
    is easier to set). By appropriately using these two methods and putting enough
    energy into data collection, we can significantly improve the results of multi-source
    cross-domain few-shot learning. In addition, structured data sources, collection,
    and processing methods can reduce the labor-intensive nature of multi-source few-shot
    learning and may significantly improve the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Single Source CDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The standard few-shot learning setting requires limited annotation, and thus
    the use of multiple domains to train a model is not always feasible. Seeking a
    more realistic and manageable solution, a single source cross-domain few-shot
    (CDFS) method has been proposed, which uses just one source domain to train the
    model. This single source CDFS includes two settings: Target Domain Accessible
    and Target Domain Forbidden.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard few-shot learning setting requires limited annotation and thus
    makes it difficult to employ multiple domains to train a model. A single-source
    method can be used to provide a more realistic and manageable solution, which
    uses only one source domain to train the model. This single-source CDFS includes
    two settings: Target Domain Accessible and Target Domain Forbidden. In the former,
    the target domain is available during training time, which allows the model adapts
    to the target domain. The target domain remains inaccessible in the latter, so
    the proposed models must be able to generalize across domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Target Domain Accessible For Single Source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the Single Source CDFS, using just one domain data to solve the cross-domain
    and few-shot problem is challenging. To address this, Target Domain Accessible
    methods assume that some target domain data can be accessed either via supervised
    learning methods [[40](#bib.bib40), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]
    or unsupervised ones [[47](#bib.bib47), [38](#bib.bib38), [75](#bib.bib75), [41](#bib.bib41),
    [39](#bib.bib39), [46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: Target Supervised Methods draw on a few target domain labeled data for model
    training. However, the dataset’s size is limited, resulting in an overfitting
    risk. Conversely, over-adaption and misleading can be caused by overly amplified
    target samples. To address this, AcroFOD [[44](#bib.bib44)] utilizes an adaptive
    optimization strategy that selects augmented data that is more similar to target
    samples rather than simply increasing the data amount. ME-D2N [[42](#bib.bib42)]
    proposes a decompose module for knowledge distillation between two networks as
    domain experts to combat this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f1cc2dd2d44c9f1edfd1d61e351aedb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The TGDM [[43](#bib.bib43)] mix framework. Use the mixed intermediate
    domain reduce the domain gap.'
  prefs: []
  type: TYPE_NORMAL
- en: The domain gap is a critical issue for CDFS, so TGDM [[43](#bib.bib43)] design
    an intermediate domain generated by mixing images in the source and the target
    domain, as shown in the Fig. [8](#S3.F8 "Figure 8 ‣ 3.2.1 Target Domain Accessible
    For Single Source ‣ 3.2 Single Source CDFS ‣ 3 DIFFERENT SETTING OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning"). The model calculates $L_{S}$,
    $L_{T}$, and $L_{Mix}$ three loss focus on the source, target, and intermediate
    domain optimized. Experiments show that the intermediate domain can effectively
    relieve the domain gap influence.
  prefs: []
  type: TYPE_NORMAL
- en: Target Unsupervised Methods are often favored due to having sufficient target
    domain data compared to supervised methods, but data annotation can be lacking.
    Meta-FDMixup [[38](#bib.bib38)], and Generalized Meta-FDMixup [[75](#bib.bib75)]
    use the target domain’s unlabeled data to mix it with source training data. This
    provides class and domain labels and requires the model to learn domain-irrelevant
    and domain-specific features. OA-FSUI2IT [[41](#bib.bib41)] utilizes its target’s
    unlabeled data to transform source data and generate content consistent with the
    source while leaving the style to match the target–to train the network subsequently.
    DDN [[39](#bib.bib39)] is yet another approach, in which a Dynamic Distillation
    Network is trained on target unlabeled data, approximate predictions from weakly-augmented
    versions of the same images from a teacher network are used to impose consistency
    regularization that matches strongly augmented versions of those same images from
    a student. Lastly, CDSC-FSL [[45](#bib.bib45)] proposes a new setting focusing
    on the domain gap between support and query set. Here, contrast learning is utilized
    to align the same classes from two domains; however, it is important to note that
    CDSC-FSL [[45](#bib.bib45)] requires source and target domains to have the same
    classes. Improvements to existing target unsupervised methods are therefore necessary
    to enhance transfer accuracy and effectively address any domain disparities.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Target Domain Forbidden For Single Source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Target Domain Forbidden CDFS is the most challenging cross-domain few-shot
    learning task, which assumes that only a single source domain is available for
    training and the target domain data is forbidden. Recent research [[48](#bib.bib48),
    [50](#bib.bib50), [51](#bib.bib51), [55](#bib.bib55), [52](#bib.bib52), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [64](#bib.bib64), [65](#bib.bib65)]
    aims to develop networks with better adaptability by training with just one source
    domain. Two main strategies have been proposed: Normalization Methods and Self-Supervised
    Methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization Methods mainly aim to improve model generation by introducing
    various normalization techniques to reduce the impact of domain differences. For
    instance, DCM+SS [[53](#bib.bib53)] uses normalization to correct the bias caused
    by the large difference between the source and the target domain. AFA [[56](#bib.bib56)]
    introduces adversarial learning to improve feature diversity with normalization.
    LCCS [[57](#bib.bib57)] proposes to learn the parameters of BatchNorm from the
    source domain to mitigate the domain gap. RD [[5](#bib.bib5)] uses normalization
    to mix instance features and a learnable memory to transfer the source domain
    information to the target domain. MemREIN [[58](#bib.bib58)] combines instance
    normalization with a Memory Bank to recover discriminant features.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Supervised Methods leverage images’ semantic consistency to expand the
    source model’s input and contrastive learning approaches to improve model generation
    further. ProtoTransfer [[49](#bib.bib49)] uses self-supervision to train an embedding
    function, leading to fast adaptation to new domains. STCDFS [[63](#bib.bib63)]
    divides the domain adaptation task into inner and outer tasks. The model first
    self-supervises the inner task (through image rotation or background swapping)
    and then solves the few-shot learning problem in the outer task. TL-SS [[54](#bib.bib54)]
    follows the episodic training approach, advocating for task-level self-supervision
    to handle the domain discrepancy problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Discussion and Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The single-source solution proposes that a single-source domain model is sufficient
    for training, implying fewer requirements from different domains than multi-source
    approaches. However, this raises the question of whether the target domain data
    can be reached. If accessible, both supervised and unsupervised access are possible
    options. If forbidden, self-supervision can be adopted to enrich data distribution.
    Single-source cross-domain few-shot learning is one of the most pervasive studies
    in this area, and venturing into target-domain data prohibition is the most exciting
    challenge here. In the following section, we will examine solutions to this issue.
  prefs: []
  type: TYPE_NORMAL
- en: 4 DIFFERENT SOLUTION OF CDFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <svg id="S4.F9.pic1" class="ltx_picture ltx_centering" height="426.3" overflow="visible"
    version="1.1" width="740.01"><g transform="translate(0,426.3) matrix(1 0 0 -1
    0 0) translate(118.11,0) translate(0,426.3)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -97.94 -117.29)" fill="#000000" stroke="#000000"><foreignobject
    width="82.78" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Self-Supervised</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -97.47 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="318.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -62.51)" fill="#000000" stroke="#000000"><foreignobject width="314.45" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDSC-FSL [[45](#bib.bib45)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -97.94)" fill="#000000" stroke="#000000"><foreignobject
    width="272.74" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WULD[[46](#bib.bib46)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -82.2)" fill="#000000" stroke="#000000"><foreignobject
    width="274.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">STCDFS
    [[63](#bib.bib63)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -42.83)" fill="#000000" stroke="#000000"><foreignobject width="352.04" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ProtoTransfer [[49](#bib.bib49)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 20.18 -117.22)" fill="#000000" stroke="#000000"><foreignobject
    width="58.46" height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image
    Mix</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 15.91 -27.08)" fill="#000000"
    stroke="#000000"><foreignobject width="251.41" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="294" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CutMix
    [[47](#bib.bib47)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 24.57
    -66.45)" fill="#000000" stroke="#000000"><foreignobject width="254.39" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TGDM [[43](#bib.bib43)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -86.13)" fill="#000000" stroke="#000000"><foreignobject
    width="277.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AcroFOD
    [[44](#bib.bib44)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 20.18
    -170.42)" fill="#000000" stroke="#000000"><foreignobject width="49.24" height="8.51"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Generate</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -101.41 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="251.58" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NSA
    [[51](#bib.bib51)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -34.48
    -157)" fill="#000000" stroke="#000000"><foreignobject width="252.76" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ISS [[36](#bib.bib36)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 24.57 -157)" fill="#000000" stroke="#000000"><foreignobject
    width="318.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OA-FSUI2IT
    [[41](#bib.bib41)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.47
    -87)" fill="#000000" stroke="#000000"><foreignobject width="129.03" height="8.65"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature Transformation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.93 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="263.56" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SRF
    [[30](#bib.bib30)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.93
    -46.76)" fill="#000000" stroke="#000000"><foreignobject width="249.64" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AFA [[56](#bib.bib56)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.93 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="223.97" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FTE[[61](#bib.bib61)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -27.08)" fill="#000000" stroke="#000000"><foreignobject
    width="277.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wave-SAN[[64](#bib.bib64)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="282.54" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DFTL[[76](#bib.bib76)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -66.45)" fill="#000000" stroke="#000000"><foreignobject
    width="278.2" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LFT
    [[7](#bib.bib7)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 331.66
    -66.45)" fill="#000000" stroke="#000000"><foreignobject width="251.03" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TSA [[34](#bib.bib34)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 331.66 -46.76)" fill="#000000" stroke="#000000"><foreignobject
    width="255.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RD
    [[5](#bib.bib5)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 189.47
    -140.91)" fill="#000000" stroke="#000000"><foreignobject width="124.08" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Knowledge Distillation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 189.93 -117.63)" fill="#000000" stroke="#000000"><foreignobject
    width="232.12" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">URL
    [[32](#bib.bib32)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 248.98
    -117.63)" fill="#000000" stroke="#000000"><foreignobject width="254.18" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DDN [[39](#bib.bib39)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 319.85 -117.63)" fill="#000000" stroke="#000000"><foreignobject
    width="253.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ME-D2N[[42](#bib.bib42)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 182.05 -168.81)" fill="#000000" stroke="#000000"><foreignobject
    width="327.51" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">EGCDFS[[77](#bib.bib77)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 300.16 -168.81)" fill="#000000" stroke="#000000"><foreignobject
    width="232.62" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Drop-CDFS
    [[59](#bib.bib59)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -251.49)" fill="#000000" stroke="#000000"><foreignobject width="251.41" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-FDMixup [[38](#bib.bib38)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -290.86)" fill="#000000" stroke="#000000"><foreignobject
    width="248.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RMP[[50](#bib.bib50)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -97.47 -330.23)" fill="#000000" stroke="#000000"><foreignobject
    width="258.33" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ME-D2N
    [[42](#bib.bib42)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -97.47
    -369.6)" fill="#000000" stroke="#000000"><foreignobject width="298.69" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DAFL[[40](#bib.bib40)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.26 -251.49)" fill="#000000" stroke="#000000"><foreignobject
    width="315.71" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDNet
    [[35](#bib.bib35)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 44.26
    -290.86)" fill="#000000" stroke="#000000"><foreignobject width="278.12" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MemREIN[[58](#bib.bib58)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 12.76 -330.23)" fill="#000000" stroke="#000000"><foreignobject
    width="334.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">G-Meta-FDMixup
    [[75](#bib.bib75)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 44.26
    -369.6)" fill="#000000" stroke="#000000"><foreignobject width="281.78" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wave-SAN [[64](#bib.bib64)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.47 -286.58)" fill="#000000" stroke="#000000"><foreignobject
    width="112.36" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Distribution
    Aligned</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 189.93 -243.61)"
    fill="#000000" stroke="#000000"><foreignobject width="265.44" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CDTF [[78](#bib.bib78)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 268.67 -243.61)" fill="#000000" stroke="#000000"><foreignobject
    width="268.09" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CMT
    [[48](#bib.bib48)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 339.53
    -243.61)" fill="#000000" stroke="#000000"><foreignobject width="229.31" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RDC [[55](#bib.bib55)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 189.93 -263.3)" fill="#000000" stroke="#000000"><foreignobject
    width="266.25" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCM+SS
    [[53](#bib.bib53)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 272.61
    -263.3)" fill="#000000" stroke="#000000"><foreignobject width="272.74" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ATA[[52](#bib.bib52)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 339.53 -263.3)" fill="#000000" stroke="#000000"><foreignobject
    width="256.79" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">LCCS[[57](#bib.bib57)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 327.26 -360.59)" fill="#000000" stroke="#000000"><foreignobject
    width="75.79" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta-learning</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 260.79 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="262.06" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TL-SS
    [[54](#bib.bib54)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 260.79
    -342.04)" fill="#000000" stroke="#000000"><foreignobject width="299.73" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DAML [[37](#bib.bib37)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 260.79 -361.72)" fill="#000000" stroke="#000000"><foreignobject
    width="235.11" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">L2F[[79](#bib.bib79)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 339.53 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="277.43" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CosML
    [[31](#bib.bib31)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 339.53
    -342.04)" fill="#000000" stroke="#000000"><foreignobject width="253.72" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SB-MTL[[62](#bib.bib62)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -322.35)" fill="#000000" stroke="#000000"><foreignobject
    width="247.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CDFSL[[80](#bib.bib80)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -345.97)" fill="#000000" stroke="#000000"><foreignobject
    width="280.43" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PATNet
    [[26](#bib.bib26)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 176.15
    -387.31)" fill="#000000" stroke="#000000"><foreignobject width="295.61" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TACDFSL[[81](#bib.bib81)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 176.15 -365.66)" fill="#000000" stroke="#000000"><foreignobject
    width="239.5" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ReFine
    [[65](#bib.bib65)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 260.79
    -387.31)" fill="#000000" stroke="#000000"><foreignobject width="212.4" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TMH [[60](#bib.bib60)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 323.79 -387.31)" fill="#000000" stroke="#000000"><foreignobject
    width="293.5" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MDLCC
    [[33](#bib.bib33)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -53.24
    -197.06)" fill="#000000" stroke="#000000"><foreignobject width="147.83" height="14.76"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image Enhancement</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 222.35 -198.68)" fill="#000000" stroke="#000000"><foreignobject
    width="158.71" height="11.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature
    Enhancement</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -25.69 -417.54)"
    fill="#000000" stroke="#000000"><foreignobject width="80.95" height="14.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Decoupling</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.91 -417.44)" fill="#000000" stroke="#000000"><foreignobject width="84.64"
    height="14.57" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fine-tuning</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Different solution of CDFS. Image Enhancement directly mix the image
    and generate the new image. Feature Enhancement use feature-wise transformation
    and knowledge distillation to achieve enhancement. Finetuning focus on the adaptability
    feature training. Decoupling solutions expect to get the domain-irrelevant and
    domain-specific feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We divide the solution of CDFS into four categories: Feature-Enhancement, Image-Enhancement,
    Decompose, and Finetuning (see Fig. [9](#S4.F9 "Figure 9 ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning")). The Feature-Enhancement
    Based methods seek to improve the model’s performance through expanded feature
    diversity. In contrast, the Image-Enhancement Based methods leverage the usual
    mixing or pasting techniques to generate additional images. Decompose Based methods
    consider both domain-specific and domain-agnostic features, and finally, Finetuning
    Based methods strive to obtain highly adaptable features to the new domain. We
    further discuss these solutions, each in its dedicated section ([4.1](#S4.SS1
    "4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"), [4.2](#S4.SS2 "4.2 Image-Enhancement
    Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning"), [4.3](#S4.SS3 "4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"), and [4.4](#S4.SS4
    "4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning")), below.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Feature-Enhancement Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After training in the base classes, the few-shot model must learn the novel
    classes in a new domain with limited annotation. In such cases, the feature quality
    directly influences the transfer effect. To address this, the CDFS researchers
    adopt feature transformation, as illustrated in Fig. [10](#S4.F10 "Figure 10 ‣
    4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning"), or knowledge distillation to enhance
    the feature quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8d195165252ba9376f9ca2bc0644806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The different feature transformation in the CDFS. FWT [[7](#bib.bib7)],
    DFTL[[76](#bib.bib76)], AFA [[56](#bib.bib56)] and SRF [[30](#bib.bib30)] use
    the learnable $\lambda$ and $\beta$ to achieve knowledge transfer, and each domain
    data have a FT-layer in the SRF [[30](#bib.bib30)]. Wave-SAN [[64](#bib.bib64)]
    and RD [[5](#bib.bib5)] mix the feature distribution to restructure the feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature Transformation Enhancement mainly changes the feature distribution,
    which can make the feature easily transfer [[64](#bib.bib64), [61](#bib.bib61)].
    The normalized strategy adds noise or prior distribution into the original feature.
    The training process contains normalization and reconstructing features using
    Eq. [2](#S4.E2 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;f_{b}^{norm}=\dfrac{f_{b}-\mu_{b}}{v_{b}}\\ &amp;f_{b}^{reco}=\lambda
    f_{b}^{norm}+\beta\end{split}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{b}$,$\mu_{b}$, and $v_{b}$ are backbone features, channel means and
    channel variance. After the feature normalization, use $\lambda$ and $\beta$ to
    reconstruct the backbone feature. If $\lambda$ and $\beta$ in Eq. [2](#S4.E2 "In
    4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep
    Visual Cross-Domain Few-Shot Learning") from learnable parameters, the feature-wise
    transformer focuses on the feature distribution improvement. FWT [[7](#bib.bib7)]
    insert a feature-wise transformation layer after the batch normalization layer
    in the feature encoder, which can effectively augment the intermediate feature.
    FWT [[7](#bib.bib7)] will remove the feature-wise transformation layer after the
    training. And DFTL[[76](#bib.bib76)] propose a diversified feature transformation
    that parallels several feature transformers. The final layer averages the FT-layer
    output and gets the final prediction. The single-source network also can use feature
    transformation to simulate unseen domain distribution. In the AFA [[56](#bib.bib56)]
    network, the domain discriminator is learned by recognizing the augmented features
    (unseen domain) from the original ones (seen domain). In summary, FWT [[7](#bib.bib7)],
    DFTL[[76](#bib.bib76)] and AFA [[56](#bib.bib56)] use the learnable $\lambda$
    and $\beta$ to achieve knowledge transfer.
  prefs: []
  type: TYPE_NORMAL
- en: The $\lambda$ and $\beta$ could come from other instance’s normalized features.
    Wave-SAN [[64](#bib.bib64)] augment source images by swapping the styles of their
    low-frequency components with each other. Wave-SAN [[64](#bib.bib64)] follows
    the replacing strategy and completely restructures the original feature with another
    instance feature’s distribution. This may lead to fluctuation, especially if the
    datasets have a domain shift. The mixing strategy can serve as an improved version
    compared, where the network uses the mixed tensor to restructure the feature,
    as the Eq. [3](#S4.E3 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;\lambda^{mix}=\alpha\mu_{b}+(1-\alpha)\mu_{o},\beta^{mix}=\alpha
    v_{b}+(1-\alpha)v_{o},\\ &amp;f_{b}^{reco}=\lambda^{mix}f_{b}^{norm}+\beta^{mix}\end{split}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ controls the retention ratio of original features, $\mu_{o}$
    and $v_{o}$ are channel mean and variance from other image. Specifically, RD [[5](#bib.bib5)]
    uses a Memory-Bank to mix the instance feature, which means the $\mu_{o}$ and
    $v_{o}$ in Eq. [3](#S4.E3 "In 4.1 Feature-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") from the Memory-Bank
    which collect different source stylization information, the Memory-Bank can load
    the source knowledge into the target domain which can directly relive the domain
    shift. For the multiple-source network, SRF [[30](#bib.bib30)] append different
    FT-layer for each domain. Using a parametric network family to obtain multi-domain
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation Enhancement train the teacher and student networks. The
    student network’s learning target is to make the feature more robust than the
    teacher network. URL [[32](#bib.bib32)] train one teacher network for each domain
    data, then use a student network to distill the domain knowledge from each teacher
    network. DDN [[39](#bib.bib39)] learns a teacher network with weak augmented target
    data, and the target domain’s knowledge will be distilled from the teacher to
    the final transfer student network. The distillation process also can load target
    domain distribution into the source training process. Similarly, ME-D2N [[42](#bib.bib42)]
    also uses Knowledge Distillation to decompose the source and target domain, which
    can effectively improve the CDFS accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Image-Enhancement Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image enhancement-based methods directly enhance the image data, as shown in
    Fig. [11](#S4.F11 "Figure 11 ‣ 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). Compared with
    feature enhancement-based methods, it is simpler and changes the image directly.
    The self-supervised method transforms the original image to generate new labels
    for model training, providing another perspective for model observation data.
    At the same time, Mix-Paste methods mix the different images to enrich data distribution.
    The generation strategy is another enhancement strategy, which generates new images
    through the encoder-decoder and inputs them into the network to improve the data
    diversity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/97a2eb688849676a966897c5b778c172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Image Enhancement Based. Self-supervised generate the new labels
    for model training, and generate methods use the consistency loss to constrain
    the model, Mix-Paste methods mix the different image which can effectively improve
    the data diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Supervised Image-Enhancement generates the new instance through images’
    semantic consistency. It is practical to improve data quality because this process
    is label-free [[49](#bib.bib49)]. OA-FSUI2IT [[41](#bib.bib41)] assume that the
    image semantic is translation irrelevant. Applying flipping, translation, and
    linear scaling on the image in the training process will not change the image
    content. The deep model should obtain a similar prediction. WULD [[46](#bib.bib46)]
    train the few-shot model with the labeled and unlabeled data. The model must recognize
    the labeled classes and predict the rotated degrees in the labeled and unlabeled
    data. As the auxiliary task, image rotation can improve the effectiveness of the
    main task. Similar to WULD [[46](#bib.bib46)], STCDFS [[63](#bib.bib63)] also
    uses rotation to improve the few-shot model effect. It designs the inner task
    in the few-shot support set. The model first predicted the image rotation degrees
    and labeled classes. The model gets the class prototypes with the original support
    image and finally uses the prototypes to recognize the query set. In the source
    unlabeled CDFS, ProtoTransfer [[49](#bib.bib49)] first transforms the unlabeled
    batch. Each sample will generate $|Q|$ transform samples. The transformed feature
    needs to be concentrated around the original sample. Other original samples in
    the batch are regarded as negative, and the original sample is regarded as positive.
    Finally, in the target domain, the extracted image features will serve as the
    classifier weight for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Mix-Paste Image-Enhancement [[47](#bib.bib47)] is the most simple methods which
    cut few target data and past into the source data, this help the model learn the
    target domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e095326db5eb7e288347dad896428fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The different mix methods proposed by AcroFOD [[44](#bib.bib44)].'
  prefs: []
  type: TYPE_NORMAL
- en: And AcroFOD [[44](#bib.bib44)] propose two different mix methods to improve
    the effect of few-shot object detection as shown in Fig. [12](#S4.F12 "Figure
    12 ‣ 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning"). The left Image-level Domain-aware
    Augmentation generates the new image with several domain instances. And the right
    Box-level Domain-aware Augmentation first cuts the foreground part of the instance,
    then pastes the corresponding box into another instance. Moreover, Meta-FDMixup
    [[38](#bib.bib38)] uses the super-parameter $\lambda$ to control the source and
    target ratio in the mixed image. After mixing, there will be source, target, and
    mixed three types of images. Due to the target labels being accessible, the source-mix
    pairs and target-mix pairs will run the few-shot classification and calculate
    the loss. The source and target mixing can effectively improve the data distribution
    and get more target data to fit the model. Compare the stiffly paste methods,
    TGDM [[43](#bib.bib43)] aim to learn a dynamic mix ratio via input the validation
    loss to a generation network, in other words, the $\lambda$ in the Meta-FDMixup
    [[38](#bib.bib38)] is learnable.
  prefs: []
  type: TYPE_NORMAL
- en: Generate Image-Enhancement generate a new image with the Encoder-Decoder method.
    NSAE [[51](#bib.bib51)] propose to take reconstructed images from auto-encoder
    as noisy inputs and let the model further predict their labels. In the target
    testing stage, first performs reconstruction task on the novel dataset and then
    the encoder is fine-tuned for classification. Compare with NSAE [[51](#bib.bib51)]
    vallina encoder-decoder, ISS [[36](#bib.bib36)] combines the feature transformation
    and generates it to solve the CDFS problem. The labeled and unlabelled domain
    features are mixed in the training stage. Then the mixed features are decoded
    to generate images, which are constrained by content perception loss and style
    loss to ensure that the semantic information is consistent with labeling, and
    the style is consistent with unlabelled. Finally, the enhanced dataset is used
    for few-shot training and transferred to the new domain. Meanwhile, OA-FSUI2IT
    [[41](#bib.bib41)] focuses on object detection and proposes an image-translation
    module that generates the source content consistency and target style consistency
    image. ISS [[36](#bib.bib36)] and OA-FSUI2IT [[41](#bib.bib41)] have similar inspiration.
    Specifically, Perceptual loss measures the perceptual similarity of two samples,
    shown in the Eq. [4](#S4.E4 "In 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION
    OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{percep}=\sum_{l\in\mathcal{S}}&#124;&#124;f_{l}(x)-f_{l}(\hat{x})&#124;&#124;_{2}^{2}\end{split}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{l}$ denotes feature from the converged network $\mathcal{S}$, $\mathcal{L}_{percep}$
    require the input $x$ and generated image $\hat{x}$ share the same content or
    semantic. Style loss measures the differences between covariances of the feature
    maps, which can reduce the stylisation from different images, as shown in Eq.
    [5](#S4.E5 "In 4.2 Image-Enhancement Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A
    Survey of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{style}=&#124;&#124;G_{j}(x)-G_{j}(\hat{x})&#124;&#124;_{F}^{2}\end{split}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $G_{j}$ get the gram matrix of $j$-th layer. And OA-FSUI2IT [[41](#bib.bib41)]
    additional use adversarial networks to improve the generation effect. Generate
    Image-Enhancement is the specific self-supervised version, but it generate new
    image compare the single image rotation and mask.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Decompose Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1af0e09d76eb3dc4975014d5db09d859.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Different decompose methods. Sample based methods extract different
    feature mean and variance, then sample the specific feature. Path based methods
    design several forward path for feature passing, and each path decompose different
    feature. Parameter based methods learn the decompose parameter, e.g., $\alpha$
    and use the it to filter the feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Methods think different features should be decomposed, as shown in Fig. [13](#S4.F13
    "Figure 13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of
    Deep Visual Cross-Domain Few-Shot Learning"). Due to the data of CDFS from a different
    domain, the mixed feature space disturbs the model effect. To decompose these
    features, researchers [[35](#bib.bib35), [40](#bib.bib40), [58](#bib.bib58)] except
    to get different feature expressions. E.g., The domain-specific and domain-irrelevant
    features, The irrelevant feature mainly collects the domain-agnostic discriminate
    information, which means the feature has excellent transfer ability across the
    domain. As a complement, the irrelevant feature can provide the related domain
    information. The specific and irrelevant features adapt to each other through
    adversarial learning.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-FDMixup serious [[38](#bib.bib38), [75](#bib.bib75)] design a disentangle
    module which generates the mean and variance of irrelevant and specific features
    as shown in the Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT
    SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning") left
    part, then use the corresponding mean and variance to sample the domain feature.
    The disentangle module is inspired by VAE (Variation Auto-Encoder), and lies in
    the best learning to disentangle the domain-specific and domain-irrelevant features,
    thus alleviating the domain shift problem in CDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the Sample base methods, as shown in the Fig. [13](#S4.F13 "Figure
    13 ‣ 4.3 Decompose Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning") middle part, Path-based use several forward paths
    to learn the different feature expressions, such as ME-D2N [[42](#bib.bib42)]
    extra learn domain-specific gate forward path which can assign each filter to
    only one specific domain in a learnable way. The structure of Wave-SAN [[64](#bib.bib64)]
    contains stand and style-augmented forward. And Wave-SAN [[64](#bib.bib64)] aims
    to enable the decomposition of visual representations into low-frequency components,
    such as shape and style, and high-frequency components, e.g., texture. The various
    feature produced required to share the same semantics can effectively improve
    the feature quality of CDFS.
  prefs: []
  type: TYPE_NORMAL
- en: The Parameter methods use the learnable tensor to filter screen the feature,
    as shown in the right part of Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Decompose Based
    ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning"). CDNet [[35](#bib.bib35)] propose a serial framework. Each step uses
    the Learn-to-Decompose (LD) Module to filter the domain feature and complete the
    feature decomposition. A MemREIN [[58](#bib.bib58)] focuses on the channel decomposition,
    the discriminative feature will be registered into the memory bank.
  prefs: []
  type: TYPE_NORMAL
- en: Besides these domain decomposing methods, researchers also study the expression
    at the class level. RMP [[50](#bib.bib50)] considers that the mid-level feature
    is more meaningful for the transferring between distant domains. Class mid-level
    features must predict the class discriminates features to supply the mid-level
    features. As the middle feature lacks the discrimination, RMP [[50](#bib.bib50)]
    first extract the class feature, then use the features of other classes to reconstruct
    the current class, and finally use the existing features to subtract the reconstructed
    features in the cosine space to obtain the class discriminate features.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Fine-tuning Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning Based methods focus on the feature transfer [[33](#bib.bib33)] and
    parameters adaptability [[65](#bib.bib65)]. Fine-tuning aims to get a robust feature
    with high transfer ability [[81](#bib.bib81)], expect to transform domain-specific
    features into irrelevant metric spaces, which will reduce the adverse effects
    of domain shift. Since the metric space is invariant, it is easier for downstream
    segmentation modules to predict in such a stable space. We group Fine-tuning Based
    methods into Meta-Learning Methods, Distribution Aligned Methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Meta-Learning Fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfbd204cae47100bd409449817c9d551.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The parameter updating strategy of the meta-generation network,
    where $\theta$ is the fast-weight and $\phi$ is the slow-weight.'
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of Meta-Learning is ’learn to learn,’ which means the optimized
    target is how to collect the experience faced by the new task. Compared with traditional
    training methods, the meta-learning process has fast-weights and slow-weights.
    The fast-weight solves the specific task problem. The slow-weight needs to collect
    experience, as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.4.2 Distribution Aligned
    Fine-tuning ‣ 4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: The key design of meta-learning is task sampling. DAML [[37](#bib.bib37)] sample
    the tasks from multiple domains, the model learns domain-agnostic initial parameters,
    which would adapt to novel classes in unseen domains during meta-testing. And
    CosML [[31](#bib.bib31)] directly combines multiple domain parameters as the transfer
    initialization, its training process on each separate domain compared with DAML
    [[37](#bib.bib37)]. And except for the slow and fast optimized weight, TL-SS [[54](#bib.bib54)]
    uses a Weight Generator to predict the parameters of a high-level network, which
    require the model to generate proper parameters and enables the encoder to flexibly
    to any unseen tasks. For parameters initialization in the meta-learning, forcibly
    sharing an initialization can lead to conflicts among tasks and the compromised
    (undesired by tasks) location on the optimization landscape, thereby hindering
    task adaptation. L2F [[79](#bib.bib79)] propose task-and-layer-wise attenuation
    on the compromised initialization to reduce its adverse influence on task adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Distribution Aligned Fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8566c3bf9e6bbd9e12932ff3bdb176de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Different scenario have disunion distribution, which may reflect
    in the input image or feature level(left part). After Distribution Align, optimize
    the model could in a unified space, and have better transfer ability(right part).'
  prefs: []
  type: TYPE_NORMAL
- en: The distribution between the source and target domains in the CDFS is different,
    as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.4.2 Distribution Aligned Fine-tuning
    ‣ 4.4 Fine-tuning Based ‣ 4 DIFFERENT SOLUTION OF CDFS ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). To address the influence of this difference,
    CMT [[48](#bib.bib48)] proposed mechanism transfer, a meta-distributional scenario
    in which the data-generating mechanism is invariant across domains. CMT [[48](#bib.bib48)]
    enabled domain adaptation among disparate distributions without relying on parametric
    assumptions. To prevent the pre-trained representation from being biased towards
    the source domain, RDC [[55](#bib.bib55)] constructed a non-linear subspace to
    minimize task-irrelevant features while also retaining more transferrable discriminative
    information through a hyperbolic tangent transformation. In other words, the source
    data was aligned between the original space and the non-linear subspace, resulting
    in more transferable discriminant information. CDTF [[78](#bib.bib78)] focused
    on aligning the support and query sets and proposed to transductive fine-tune
    the base model on a set of query images under the few-shot setting. The core idea
    was to implicitly guide query image segmentation using support labels.
  prefs: []
  type: TYPE_NORMAL
- en: As the base unit of meta-learning, the task distribution directly influences
    the model’s effectiveness. ATA [[52](#bib.bib52)] sought to improve the robustness
    of the inductive bias through task augmentation. The network adaptively generates
    ’challenging’ tasks using different inductive biases. Task-level augmentation
    can increase the diversity of training tasks and improve the model’s robustness
    under domain shift. At the feature level, researchers [[57](#bib.bib57)] attempted
    to transfer the source distributions into the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: 5 BENCHMARK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 CDFS Classification.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BSCD-FSL [[6](#bib.bib6)] propose the Broader Study of Cross-Domain Few-Shot
    Learning (BSCD-FSL) benchmark, consisting of image data from diverse image acquisition
    methods. There are mini-ImageNet [[82](#bib.bib82)], CropDiease [[72](#bib.bib72)],
    EuroSAT [[73](#bib.bib73)], ISIC [[83](#bib.bib83)], ChestX [[74](#bib.bib74)]
    datasets in BSCD-FSL benchmark. Since the mini-ImageNet [[82](#bib.bib82)] has
    100 classes where each class has 600 images, BSCD-FSL selects it as the source
    domain and transfers the model knowledge to the rest domains.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, LFT [[7](#bib.bib7)] proposes a CDFS benchmark that contains five
    datasets. LFT design the leave-one-out experiments setting, which selects one
    domain from the CUB [[84](#bib.bib84)], Cars [[85](#bib.bib85)], Places [[86](#bib.bib86)],
    and Plantae [[87](#bib.bib87)] as the unseen domain for the evaluation, the mini-ImageNet
    [[82](#bib.bib82)] and the remaining domains serve as the seen domains for training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: For cross-domain few-shot datasets, Meta-Dataset [[25](#bib.bib25)] proposes
    a new benchmark for training and evaluating large-scale models, consists of diverse
    datasets and presents more realistic tasks. Meta-Dataset is divided by domain.
    That is, there is no cross-category between datasets. We show the benchmark used
    for all the CDFS classification methods in Tab. [2](#S5.T2 "Table 2 ‣ 5.1 CDFS
    Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain Few-Shot
    Learning") and Tab. [3](#S5.T3 "Table 3 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). The most frequent
    experiment is the 5-way 1-shot setting. We summary the 5-way 1-shot transfer effect
    in the Tab. [2](#S5.T2 "Table 2 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey
    of Deep Visual Cross-Domain Few-Shot Learning") and the 5-way k-shot transfer
    effect in the Tab. [3](#S5.T3 "Table 3 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The CDFS classification result of 5-way 1-shot. ^† denotes the leave-one-out
    experiments, ^∗ denotes the target data is accessible, ^T denotes the transductive
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5-way 1-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
  prefs: []
  type: TYPE_TB
- en: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 35.89[±0.5] | 30.77[±0.5] | 49.86[±0.8]
    | 32.70[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 36.61[±0.5] | 29.82[±0.4] | 51.07[±0.7]
    | 34.48[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 42.44[±0.8] | 29.11[±0.6] | 48.64[±0.9]
    | 33.17[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 44.07[±0.8] | 28.63[±0.6] | 50.68[±0.8]
    | 33.14[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN[[88](#bib.bib88)][(ICLR18)] | 45.69[±0.7] | 31.79[±0.5] | 53.10[±0.8]
    | 35.60[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 47.47[±0.8] | 31.61[±0.5] | 55.77[±0.8]
    | 35.95[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 42.44[±0.4] | 29.65[±0.3] | 50.59[±0.5]
    | 34.80[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 42.88[±0.5] | 29.61[±0.4] | 53.07[±0.6]
    | 34.54[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 46.23[±0.4] | 32.66[±0.5] | 56.96[±0.5]
    | 38.23[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 48.35[±0.5] | 32.35[±0.4] | 61.60[±0.6]
    | 38.48[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 48.29[±0.5] | 32.78[±0.4] | 54.83[±0.6]
    | 37.49[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 51.51[±0.8] | 34.12 [±0.6] | 56.31 [±0.8]
    | 42.09 [±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 46.89[±0.5] | 47.74[±0.6] | 53.96[±0.6]
    | 30.93[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 52.04 [±0.7] | 34.84 [±0.6] | 57.57 [±0.8]
    | 42.05 [±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 46.15[±0.7] | 33.54 [±0.6] | 51.81[±0.7]
    | 39.97[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 54.26 [±0.6] | 37.55 [±0.5] | 59.98
    [±0.6] | 45.69 [±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 46.37 [±0.5] | 35.65 [±0.5] | 54.92
    [±0.6] | 38.82 [±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 52.02 [±0.5] | 36.38 [±0.4] | 54.82
    [±0.6] | 36.74 [±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 43.10 [±0.4] | 32.34 [±0.3] | 50.53 [±0.5]
    | 33.19 [±0.3] | 63.29[±0.4] | 57.36[±0.30] | 32.09[±0.3] | 22.28[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 68.93[±0.8] | 64.14[±0.82]
    | 35.30[±0.59] | 22.48[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 50.25[±0.7] | 33.55[±0.6] | 57.75[±0.8]
    | 40.71[±0.7] | 70.80[±1.0] | 69.64[±1.0] | 33.35[±0.7] | 22.93[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 50.33[±0.7] | 32.69[±0.6] | 57.84[±0.8]
    | 38.25[±0.6] | 69.65[±1.0] | 65.50[±1.1] | 33.09[±0.7] | 22.39[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 41.02[±0.4] | 33.52[±0.4] | 54.66[±0.5]
    | 37.60[±0.4] | 60.71[±0.5] | 61.28[±0.5] | 32.32[±0.3] | 22.11[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 46.86[±0.5] | 34.25[±0.4] | 54.04[±0.6]
    | 36.76[±0.4] | 67.61[±0.5] | 63.12[±0.5] | 33.21[±0.3] | 22.92[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 50.85[±0.4] | 38.43[±0.4] | 60.29[±0.5]
    | 40.27[±0.4] | 72.44[±0.6] | 66.17[±0.4] | 34.25[±0.4] | 21.69[±0.1] |'
  prefs: []
  type: TYPE_TB
- en: '| RDC[[55](#bib.bib55)][(CVPR22)] | 48.68[±0.5] | 38.26[±0.5] | 59.53[±0.5]
    | 42.29[±0.5] | 79.72[±0.5] | 65.58[±0.5] | 32.33[±0.3] | 22.77[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 51.20[±0.5] | 39.13[±0.5] | 61.50[±0.6]
    | 44.33[±0.6] | 86.33[±0.5] | 71.57[±0.5] | 35.84[±0.4] | 22.27[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 43.02[±0.4] | 31.79[±0.3] | 51.16[±0.5]
    | 33.72[±0.3] | 61.17[±0.5] | 55.69[±0.5] | 31.13[±0.3] | 22.14[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 45.00[±0.5] | 33.61[±0.4] | 53.57[±0.5]
    | 34.42[±0.4] | 67.47[±0.5] | 61.35[±0.5] | 33.21[±0.4] | 22.10[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 50.26[±0.5] | 34.18[±0.4] | 57.03[±0.5]
    | 39.83[±0.4] | 77.82[±0.5] | 65.94[±0.5] | 34.70[±0.4] | 21.67[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 65.05[±0.8] | 49.53[±0.8]
    | 60.36[±0.9] | 52.89[±0.8] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 64.80[±0.3] | 50.70[±0.2]
    | 61.88[±0.3] | 52.39[±0.3] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 63.24[±0.8] | 51.31[±0.8]
    | 58.22[±0.8] | 51.03[±0.8] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 63.85[±0.4] | 53.10[±0.4] | 59.39[±0.4]
    | 51.28[±0.4] | 70.21[±0.4] | 91.07[±0.4] | 70.90[±0.6] | 53.07[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 82.14[±0.8] | 73.14[±0.8]
    | 34.66[±0.6] | 23.38[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The CDFS classification result of 5-way k-shot. ^† denotes the leave-one-out
    experiments, ^∗ denotes the target data is accessible, ^T denotes the transductive
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5-way 5-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT | ISIC
    | ChestX |'
  prefs: []
  type: TYPE_TB
- en: '| MatchingNet[[3](#bib.bib3)][(NIPS16)] | 51.37[±0.8] | 38.99[±0.6] | 63.16[±0.8]
    | 46.53[±0.9] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MN+FT[[7](#bib.bib7)][(ICLR20)] | 55.23[±0.8] | 41.24[±0.6] | 64.55[±0.8]
    | 41.69[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RelationNet[[2](#bib.bib2)][(CVPR18)] | 57.77[±0.7] | 37.33[±0.7] | 63.32[±0.8]
    | 44.00[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+FT[[7](#bib.bib7)][(ICLR20)] | 59.46[±0.7] | 39.91[±0.7] | 66.28[±0.7]
    | 45.08[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN[[88](#bib.bib88)][(ICLR18)] | 62.25[±0.6] | 44.28[±0.6] | 70.84[±0.7]
    | 52.53[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+FT[[7](#bib.bib7)][(ICLR20)] | 66.98[±0.7] | 44.90[±0.6] | 73.94[±0.7]
    | 53.85[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+LRP[[77](#bib.bib77)][(ICPR20)] | 59.30[±0.4] | 39.19[±0.4] | 66.90[±0.4]
    | 48.09[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 59.22[±0.4] | 38.31[±0.4] | 68.25[±0.4]
    | 47.67[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CAN+LRP[[77](#bib.bib77)][(ICPR20)] | 66.58[±0.3] | 43.86[±0.4] | 74.91[±0.4]
    | 53.25[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CAN+LRP^T[[77](#bib.bib77)][(ICPR20)] | 66.57[±0.4] | 42.57[±0.4] | 76.90[±0.4]
    | 51.63[±0.4] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+LRP[[77](#bib.bib77)][(ICPR20)] | 64.44[±0.5] | 46.20[±0.5] | 74.45[±0.5]
    | 54.46[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+FT^†[[7](#bib.bib7)][(ICLR20)] | 73.11[±0.7] | 49.88[±0.7] | 77.05[±0.7]
    | 58.84[±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CosML^†[[31](#bib.bib31)][(Arxiv)] | 66.15[±0.6] | 60.17[±0.6] | 88.08[±0.5]
    | 42.96[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LRFG^†[[89](#bib.bib89)][(KBS22)] | 73.94[±0.7] | 50.63[±0.7] | 76.68[±0.6]
    | 62.14[±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DFTL^†[[76](#bib.bib76)][(ICAICA21)] | 69.75[±0.7] | 49.55[±0.7] | 69.38[±0.7]
    | 58.76[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DFTL[[76](#bib.bib76)][(ICAICA21)] | 69.35[±0.7] | 47.91[±0.6] | 69.12[±0.8]
    | 58.12[±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+MR^†[[58](#bib.bib58)][(IJCAI22)] | 77.54[±0.6] | 56.78[±0.6] | 78.84[±0.6]
    | 65.44[±0.6] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 67.31[±0.5] | 47.36[±0.5] | 68.14[±0.6]
    | 52.28[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RNet+MR^†[[58](#bib.bib58)][(IJCAI22)] | 68.39[±0.5] | 46.92[±0.5] | 69.87[±0.5]
    | 58.64[±0.5] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MAP[[80](#bib.bib80)][(Arxiv)] | 67.92[±1.1] | 51.64[±1.2] | 75.94[±1.0]
    | 58.45[±1.2] | 90.29[±1.6] | 82.76[±2.0] | 47.85[±2.0] | 24.79[±1.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RN+ST[[63](#bib.bib63)][(Arxiv)] | 62.94[±0.4] | 43.26[±0.4] | 66.74[±0.4]
    | 46.92[±0.3] | 78.62[±0.4] | 75.84[±0.4] | 44.42[±0.3] | 24.79[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| MN+AFA[[56](#bib.bib56)][(ECCV22)] | 59.46[±0.4] | 46.13[±0.4] | 68.87[±0.4]
    | 52.43[±0.4] | 80.07[±0.4] | 69.63[±0.5] | 39.88[±0.3] | 23.18[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+AFA[[56](#bib.bib56)][(ECCV22)] | 68.25[±0.5] | 49.28[±0.5] | 76.21[±0.5]
    | 54.26[±0.4] | 88.06[±0.3] | 85.58[±0.4] | 46.01[±0.4] | 25.02[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| TPN+AFA[[56](#bib.bib56)][(ECCV22)] | 65.86[±0.4] | 47.89[±0.4] | 72.81[±0.4]
    | 55.67[±0.4] | 85.69[±0.4] | 80.12[±0.4] | 46.29[±0.3] | 23.47[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RDC[[55](#bib.bib55)][(CVPR22)] | 64.36[±0.4] | 52.15[±0.4] | 73.24[±0.4]
    | 57.50[±0.4] | 88.90[±0.3] | 77.15[±0.4] | 41.28[±0.3] | 25.91[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RDC-FT[[55](#bib.bib55)][(CVPR22)] | 67.77[±0.4] | 53.75[±0.5] | 74.65[±0.4]
    | 60.63[±0.4] | 93.55[±0.3] | 84.67[±0.3] | 49.06[±0.3] | 25.48[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| RN+ATA[[52](#bib.bib52)][(IJCAI21)] | 59.36[±0.4] | 42.95[±0.4] | 66.90[±0.4]
    | 45.32[±0.3] | 78.20[±0.4] | 71.02[±0.4] | 40.38[±0.3] | 24.43[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+ATA[[52](#bib.bib52)][(IJCAI21)] | 66.22[±0.5] | 49.14[±0.4] | 75.48[±0.4]
    | 52.69[±0.4] | 90.59[±0.3] | 83.75[±0.4] | 44.91[±0.4] | 24.32[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: '| TPN+ATA[[52](#bib.bib52)][(IJCAI21)] | 65.31[±0.4] | 46.95[±0.4] | 72.12[±0.4]
    | 55.08[±0.4] | 88.15[±0.5] | 79.47[±0.3] | 45.83[±0.3] | 23.60[±0.2] |'
  prefs: []
  type: TYPE_TB
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 76.00[±0.7] | 61.11[±0.8] | 73.40[±0.7]
    | 65.66[±0.8] | 96.09[±0.4] | 87.53[±0.5] | 56.85[±0.7] | 28.73[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 93.42[±0.6] | 85.19[±0.7]
    | 45.39[±0.7] | 25.32[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 95.28[±0.4] | 85.34[±0.6]
    | 53.84[±0.7] | 27.98[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 96.59[±0.3] | 88.13[±0.5]
    | 57.40[±0.7] | 29.72[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 96.01[±0.4] | 85.93[±0.7]
    | 50.68[±0.8] | 25.99[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 88.09[±0.6] | 81.10[±0.6]
    | 43.74[±0.6] | 26.80[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| ReFine[[65](#bib.bib65)][(CIKM22)] | - | - | - | - | 90.75[±0.5] | 82.36[±0.6]
    | 51.68[±0.6] | 26.76[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: '| GNN+WS[[64](#bib.bib64)][(Arxiv)] | 70.31[±0.7] | 46.11[±0.7] | 76.88[±0.6]
    | 57.72[±0.6] | 89.70[±0.6] | 85.22[±0.7] | 44.93[±0.7] | 25.63[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| FWT+WS[[64](#bib.bib64)][(Arxiv)] | 71.16[±0.7] | 47.78[±0.7] | 78.19[±0.6]
    | 57.85[±0.7] | 91.23[±0.5] | 84.84[±0.7] | 46.00[±0.7] | 25.27[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| ME-D2N^∗[[42](#bib.bib42)]${}_{(\mathrm{ACM\,MM22)}}$ | 83.17[±0.6] | 69.17[±0.7]
    | 80.45[±0.6] | 72.87[±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TGDM^∗[[43](#bib.bib43)]${}_{(\mathrm{ACM\,MM22)}}$ | 84.21[±0.2] | 70.99[±0.2]
    | 81.62[±0.2] | 71.78[±0.2] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| M-FDM^∗[[38](#bib.bib38)]${}_{(\mathrm{ACM\,MM21)}}$ | 79.46[±0.6] | 66.52[±0.7]
    | 78.92[±0.6] | 69.22[±0.7] | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GM-FDM^∗[[75](#bib.bib75)][(TIP22)] | 80.49[±0.3] | 71.80[±0.3] | 78.80[±0.3]
    | 69.45[±0.3] | 87.32[±0.2] | 95.87[±0.2] | 84.07[±0.4] | 55.37[±0.4] |'
  prefs: []
  type: TYPE_TB
- en: '| DDN^∗[[39](#bib.bib39)][(NIPS21)] | - | - | - | - | 95.54[±0.4] | 89.07[±0.5]
    | 49.36[±0.6] | 28.31[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| 5-way 20-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
  prefs: []
  type: TYPE_TB
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.16[±0.1] | 94.72[±0.3]
    | 68.09[±0.6] | 38.34[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 98.51[±0.2] | 92.42[±0.4]
    | 65.43[±0.6] | 37.11[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.19[±0.1] | 95.18[±0.4]
    | 68.58[±0.7] | 33.47[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 94.95[±0.3] | 88.54[±0.5]
    | 54.61[±0.5] | 32.90[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 91.08[±0.4] | 85.04[±0.5] | 83.00[±0.6]
    | 81.54[±0.6] | 99.20[±0.1] | 94.21[±0.3] | 67.45[±0.6] | 36.14[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.49[±0.4] | 87.87[±0.5]
    | 53.15[±0.6] | 29.17[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| 5-way 50-shot | CUB | Cars | Places | Plantae | CropDiseases | EuroSAT |
    ISIC | ChestX |'
  prefs: []
  type: TYPE_TB
- en: '| BSR[[61](#bib.bib61)][(Arxiv)] | - | - | - | - | 99.73[±0.1] | 96.89[±0.2]
    | 74.08[±0.6] | 44.43[±0.6] |'
  prefs: []
  type: TYPE_TB
- en: '| TMHFS[[60](#bib.bib60)][(Arxiv)] | - | - | - | - | 99.28[±0.1] | 95.63[±0.3]
    | 71.29[±0.8] | 43.43[±0.7] |'
  prefs: []
  type: TYPE_TB
- en: '| SB-MTL[[62](#bib.bib62)][(Arxiv)] | - | - | - | - | 99.75[±0.1] | 97.73[±0.3]
    | 75.55[±0.6] | 38.37[±0.6] |'
  prefs: []
  type: TYPE_TB
- en: '| SSP[[29](#bib.bib29)][(Arxiv)] | - | - | - | - | 96.27[±0.3] | 91.40[±0.4]
    | 60.86[±0.5] | 37.05[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: '| NSAE[[51](#bib.bib51)][(ICCV21)] | 95.41[±0.5] |  | 86.53[±0.8] | 85.99[±0.7]
    | 99.70[±0.1] | 96.50[±0.3] | 73.00[±0.6] | 41.80[±0.7] |'
  prefs: []
  type: TYPE_TB
- en: '| TACDFSL[[81](#bib.bib81)][(SB22)] | - | - | - | - | 95.88[±0.4] | 89.07[±0.4]
    | 56.68[±0.6] | 31.75[±0.5] |'
  prefs: []
  type: TYPE_TB
- en: CDFS Object-Detection. MoF-SOD [[27](#bib.bib27)] propose a Multi-domain Few-Shot
    Object Detection benchmark consisting of 10 datasets from a wide range of domains
    to evaluate few-shot object-detection algorithms, as shown in the Fig. [16](#S5.F16
    "Figure 16 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual
    Cross-Domain Few-Shot Learning"). Empirical results show several keys actors that
    have yet to be explored in previous works. Under the proposed benchmark, MoF-SOD
    [[27](#bib.bib27)] conducted extensive experiments on the impact of freezing parameters,
    different architectures, and different pre-training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aaf9e56f9bb9c348a9a0f13b6def5ed5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Sample images in the proposed MoF-SOD [[27](#bib.bib27)] benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: CDFS Segmentation. PATNet [[26](#bib.bib26)] proposes a cross-domain few-shot
    segmentation benchmark. Evaluate both representative few-shot segmentation and
    transfer learning-based methods on the proposed benchmark. And propose a novel
    Pyramid-Anchor-Transformation based few-shot segmentation network, in which domain-specific
    features are transformed into domain-agnostic ones for downstream segmentation
    modules to fast adapt to unseen domains. And RD [[5](#bib.bib5)] also proposes
    a cross-domain few-shot segmentation task using different public datasets to validate
    the model effect. As the most famous datasets, we summarize the segmentation transfer
    effect from COCO-$2-^{i}$ to PASCAL-$5^{i}$ as shown in Tab. [4](#S5.T4 "Table
    4 ‣ 5.1 CDFS Classification. ‣ 5 BENCHMARK ‣ A Survey of Deep Visual Cross-Domain
    Few-Shot Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Cross-domain few-shot semantic segmentation results on COCO-$20^{i}$
    to PASCAL-$5^{i}$ task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| COCO-$20^{i}$ to PASCAL-$5^{i}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Backbone | Method | 1-shot | 5-shot |'
  prefs: []
  type: TYPE_TB
- en: '| split0 | split1 | split2 | split3 | Mean | split0 | split1 | split2 | split3
    | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet50 | RPMMs[[90](#bib.bib90)][(ECCV20)] | 36.3 | 55.0 | 52.5 | 54.6
    | 49.6 | 40.2 | 58.0 | 55.2 | 61.8 | 53.8 |'
  prefs: []
  type: TYPE_TB
- en: '| RePRI[[91](#bib.bib91)][(CVPR21)] | 52.4 | 64.3 | 65.3 | 71.5 | 63.3 | 57.0
    | 68.0 | 70.4 | 76.2 | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ASGNet[[92](#bib.bib92)][(CVPR21)] | 42.5 | 58.7 | 65.5 | 63.0 | 57.4 | 53.7
    | 69.8 | 67.1 | 75.9 | 66.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PFENet[[93](#bib.bib93)][(TPAMI)] | - | - | - | - | 60.8 | - | - | - | -
    | 61.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CWT[[94](#bib.bib94)][(ICCV21)] | 53.5 | 59.2 | 60.2 | 64.9 | 59.4 | 60.3
    | 65.8 | 67.1 | 72.8 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 48.7 | 61.5 | 63.0 | 72.8 | 61.5 | 58.2
    | 65.9 | 71.8 | 77.9 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| RD[[5](#bib.bib5)][(CVPR22)] | 57.4 | 62.2 | 68.0 | 74.8 | 65.6 | 65.7 |
    69.2 | 70.8 | 75.0 | 70.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet101 | SCL[[96](#bib.bib96)][(CVPR21)] | 43.1 | 60.3 | 66.1 | 68.1 |
    59.4 | 43.3 | 61.2 | 66.5 | 70.4 | 60.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HSNet[[95](#bib.bib95)][(ICCV21)] | 46.3 | 64.7 | 67.7 | 74.2 | 63.2 | 59.1
    | 69.0 | 73.4 | 78.7 | 70.0 |'
  prefs: []
  type: TYPE_TB
- en: '| RD[[5](#bib.bib5)][(CVPR22)] | 59.4 | 64.3 | 70.8 | 72.0 | 66.6 | 67.2 |
    72.7 | 72.0 | 78.9 | 72.7 |'
  prefs: []
  type: TYPE_TB
- en: 6 Application OF CDFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CDFS focuses on the domain problem of few-shot. It has been used in various
    applications, as shown in Fig. [17](#S6.F17 "Figure 17 ‣ 6 Application OF CDFS
    ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning"). We summary the different
    CDFS methods for object detection and segmentation. For the other applications,
    we also provide a detailed survey.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b33afdf48d281db3a115fa0a1e3e5efb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The Cross-Domain Few-shot related applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 CDFS for Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0c1953f61fb5b4b4dd648a7f29487a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: AcroFOD [[44](#bib.bib44)] address the task of cross-domain few-shot
    object detection. Top: Existing feature-aligning based methods fail to extract
    discriminative features within limited labeled data in the target domain. Bottom:
    Our method filters (thick black dotted line) source data that is far away from
    the target domain.'
  prefs: []
  type: TYPE_NORMAL
- en: For CDFS object detection, image enhancement is still the most direct choice.
    CutMix [[47](#bib.bib47)] directly pastes data from the target domain to enhance
    feature diversity. However, due to the scarcity of target domain data, direct
    image mixing will bring unnecessary noise impact. To alleviate this situation,
    AcroFOD [[44](#bib.bib44)] proposes a filtering mechanism to filter the enhanced
    image, as shown in Fig. [18](#S6.F18 "Figure 18 ‣ 6.1 CDFS for Object Detection
    ‣ 6 Application OF CDFS ‣ A Survey of Deep Visual Cross-Domain Few-Shot Learning").
    In the training process, the model continuously measures the distance between
    the enhanced and target features, retaining the most similar top-k. Finally, the
    enhanced features are used for object detection learning. The generation strategy
    can still enrich the feature distribution. For this, OA-FSUI2IT [[41](#bib.bib41)]
    uses the unlabeled data of the target domain to generate new style images for
    model training. The style of the generated images is consistent with the target
    domain, and the content is consistent with the source domain. Domain adaptive
    training ensures the reproducibility of the learned content, improves the interpretability
    of the model, and self-supervised content consistency improves the model generalization.
    CDTL [[97](#bib.bib97)] is one of the first to study the problem of few-shot SAR
    image ship detection, which has great practical value but is less studied than
    the classification problem of SAR images under few-shot conditions. CDTL [[97](#bib.bib97)]
    design a two-stage few-shot SAR ship detection model based on transfer learning
    using the similarity information of images of optical ships.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 CDFS for Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For CDFS segmentation, RD [[5](#bib.bib5)] proposes a domain enhancement strategy
    of memory mechanism. During training, source domain data continuously stores domain-style
    information in memory. During testing, source information stored in memory is
    loaded into target domain feature enhancement. RD [[5](#bib.bib5)] can directly
    reduce the domain difference and has been verified on typically partitioned datasets.
    For the task of semantic segmentation in autonomous driving applications, PixDA
    [[67](#bib.bib67)] propose a novel pixel-by-pixel domain adversarial loss following
    three criteria: (i) align the source and the target domain for each pixel, (ii)
    avoid negative transfer on the correctly represented pixels, and (iii) regularize
    the training of infrequent classes to avoid overfitting. CDTF [[78](#bib.bib78)]
    through aligning the support and query prototypes to achieve the cross-domain
    few-shot segmentation. By aligning query and support prototypes with an uncertainty-aware
    contrastive loss, and using a supervised cross-entropy loss and an unsupervised
    boundary loss as regularizations, CDTF [[78](#bib.bib78)] could generalize the
    base model to the target domain without additional labels. CD-FSS [[98](#bib.bib98)]
    propose a cross-domain few-shot segmentation framework, which enables the model
    to leverage the learning ability obtained from the natural domain to facilitate
    rare-disease skin lesion segmentation with limited data of common diseases.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 CDFS for Other Application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-Domain Few-shot Hyperspectral Image Classification [[99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)] draw the research
    attention recently, the model focus on learning of local and global image context
    due to the special image data. The related hyperspectral framework is similar
    to the traditional few-shot research. SAR-FS [[103](#bib.bib103)] developed an
    algorithm to transfer knowledge from EO domains to SAR domains to eliminate the
    need for huge labeled data points in the SAR domains.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers [[104](#bib.bib104), [105](#bib.bib105)] carried out a series of
    research and discussion on fault diagnosis, focusing on the impact of meta-learning
    on cross-domain fault diagnosis. FSPR [[106](#bib.bib106)] first formulated a
    reidentification scenario as a cross-domain few-shot problem and discussed the
    difference between conventional and unsupervised re-ID. And introduce a reweighting
    instance method based on the function (ReWIF) to guide the training procedure
    of the re-ID model. In other related fields [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)], CDFS has gradually begun to receive attention,
    promoting the development of various research subfields.
  prefs: []
  type: TYPE_NORMAL
- en: 7 FUTURE DIRECTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Future Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CDFS focuses on learning in different domains and largely solves the domain
    shift challenge. With limited labeled data, current research focuses on near-domain
    transfer, neglecting the more challenging distant-domain transfer. Transference
    of models from a natural scene to a proprietary domain remains difficult. For
    CDFS of multiple sources, the goal of the devised learning scheme is to achieve
    Domain Adaptation and Generalization. Even the lack of sufficient labeled data
    in the new CDFS domain may inhibit the transfer effect, reducing the effectiveness
    of multiple sources in this domain. Thus, fine-tuning variably distributed data
    among multiple sources is essential to maximize the transfer effect and ensure
    successful CDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Future Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent researchers have sought to use image and feature enhancement to address
    the CDFS problem, which requires a model to adapt quickly to a new environment
    while retaining rapid learning ability. However, successfully combining the insufficient
    annotation and domain shift issue continues to be challenging. To further develop
    solutions to the CDFS problem, researchers should consider the importance of adapting
    high parameters and utilizing domain-invariant information.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Multiple Source Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the future, meta-learning is poised to be a key tool for addressing domain
    shifts, especially in the context of few-shot learning. The model can successfully
    manage complex scenarios by leveraging its high adaptability, especially regarding
    domain migration. As such, upcoming work in cross-domain few-shot (CDFS) should
    focus more on meta-learning in a multi-source setting [[111](#bib.bib111)]. Exploiting
    the abundant data distributions available should maximize the utility of meta-learning
    and preserve its original intention. Additionally, research should be conducted
    to combine memory mechanisms with meta-learning; such a union of the two would
    bring straightforward benefits to the CDFS domain. Memory can be key in connecting
    domains without compromising performance; meanwhile, meta-learning provides highly
    customizable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Domain invariant information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When faced with single-domain CDFS, domain-invariant information is invaluable
    for successful zero-shot learning [[112](#bib.bib112), [113](#bib.bib113)]. Specifically,
    semantic information can often be used to evidence relationships between different
    categories. Because visual differences do not hamper it, they can then be readily
    employed as supplementary information for learning. In the context of FSL, several
    studies have explored the intersection of semantic modalities and vision, often
    using the former to facilitate the knowledge migration between categories [[114](#bib.bib114),
    [115](#bib.bib115)]. When shared category information can be successfully converted
    into a different modality, it can offer valuable guidance in cross-domain FSL
    scenarios [[116](#bib.bib116)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Future Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 7.3.1 Open-Set Few-Shot Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The softmax of the deep model assigns all samples in the test phase to a fixed
    category, which is different from actual expectations as it cannot identify unseen
    classes [[117](#bib.bib117)]. To address this limitation, open-set models are
    more suitable compared with close-set models as they require the model to make
    a corresponding response to the unseen class rather than a mechanical one. Cross
    Domain Few-Shot (CDFS) transfer of the model to the target domain allows for identifying
    unseen classes, albeit requiring a few fine-tuning annotations when doing so [[118](#bib.bib118),
    [119](#bib.bib119)]. However, this approach is only effective when classes in
    the source and target domains can be delineated, raising an important question
    about identifying these unlabeled classes for complex scenarios where all classes
    need to be labeled. Open-set few-shot learning can help to meet this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Incremental Few-Shot Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Incremental few-shot learning requires the model to recognize new classes without
    forgetting the learned classes [[120](#bib.bib120), [121](#bib.bib121)]. Typical
    few-shot models only focus on new classes, and base classes must be relearned
    to complete recognition. Data dependency results in the models being effective
    in limited scenarios and cannot increase classes. For future CDFS, the model will
    constantly learn new domain information during the generalization process. In
    order to identify the different domain classes, the model needs to learn and save
    the data of the corresponding domain constantly. Binding data to the model anytime
    does not conform to the original intention of few-shot learning. Therefore, studying
    incremental few-shot learning can promote CDFS to a certain extent. If we can
    eliminate the dependence on data domains, CDFS will be more practical [[122](#bib.bib122),
    [123](#bib.bib123)].
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have provided a comprehensive overview of recent cross-domain few-shot learning
    (CDFS) research. Our analysis considered existing solutions and research issues
    while comparing performance indices between different studies. Furthermore, we
    discussed the broad applications of CDFS and its implications for future research.
    Our review will serve as a valuable reference guide and provide theoretical support
    for advancing the field of CDFS. Our survey reveals that cross-domain few-shot
    learning is gradually becoming an increasingly popular research topic and has
    received extensive attention due to its potential to alleviate the domain shift
    problem in AI applications. The current solutions to the problem span various
    approaches, each with its advantages and limitations. As the research field is
    still in its infancy, future work should focus on extending existing or new methods
    to improve the performance of cross-domain few-shot learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by the National Natural Science Foundation of
    China (No.62176009), the Major Project for New Generation of AI (No.2018AAA0100400),
    the National Natural Science Foundation of China (No. 61836014, No. U21B2042,
    No. 62072457, No. 62006231).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Snell, K. Swersky, R. S. Zemel, Prototypical networks for few-shot learning,
    ArXiv abs/1703.05175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    Learning to compare: Relation network for few-shot learning, 2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (2017) 1199–1208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] O. Vinyals, C. Blundell, T. P. Lillicrap, K. Kavukcuoglu, D. Wierstra,
    Matching networks for one shot learning, in: Proc. NIPS, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] W. Wang, L. Duan, Q. En, B. Zhang, F. Liang, Tpsn: Transformer-based multi-prototype
    search network for few-shot semantic segmentation, Comput. Electr. Eng. 103 (2022)
    108326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] W. Wang, L. Duan, Y. Wang, Q. En, J. Fan, Z. Zhang, Remember the difference:
    Cross-domain few-shot semantic segmentation via meta-memory transfer, 2022 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 7055–7064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Y. Guo, N. C. F. Codella, L. Karlinsky, J. Codella, J. R. Smith, K. Saenko,
    T. S. Rosing, R. S. Feris, A broader study of cross-domain few-shot learning,
    in: European Conference on Computer Vision, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] H.-Y. Tseng, H.-Y. Lee, J.-B. Huang, M.-H. Yang, Cross-domain few-shot
    classification via learned feature-wise transformation, in: International Conference
    on Learning Representations, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Y. Wang, Q. Yao, J. T.-Y. Kwok, L. M. shuan Ni, Generalizing from a few
    examples: A survey on few-shot learning, arXiv: Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Motiian, Q. Jones, S. M. Iranmanesh, G. Doretto, Few-shot adversarial
    domain adaptation, in: NIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, A. S. Vincentelli,
    Prototypical cross-domain self-supervised learning for few-shot unsupervised domain
    adaptation, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 13829–13839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. Saito, D. Kim, S. Sclaroff, T. Darrell, K. Saenko, Semi-supervised
    domain adaptation via minimax entropy, 2019 IEEE/CVF International Conference
    on Computer Vision (ICCV) (2019) 8049–8057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Chen, X. Jia, J. He, Y. Shi, J. Liu, Semi-supervised domain adaptation
    based on dual-level domain mixing for semantic segmentation, 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR) (2021) 11013–11022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Liang, D. Hu, Y. Wang, R. He, J. Feng, Source data-absent unsupervised
    domain adaptation through hypothesis transfer and labeling transfer, IEEE Transactions
    on Pattern Analysis and Machine Intelligence 44 (2020) 8602–8617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] T. Xu, W. Chen, P. Wang, F. Wang, H. Li, R. Jin, Cdtrans: Cross-domain
    transformer for unsupervised domain adaptation, ArXiv abs/2109.06165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] X. Liu, Z. Guo, S. Li, F. Xing, J. J. You, C.-C. J. Kuo, G. E. Fakhri,
    J. Woo, Adversarial unsupervised domain adaptation with conditional and label
    shift: Infer, align and iterate, 2021 IEEE/CVF International Conference on Computer
    Vision (ICCV) (2021) 10347–10356.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, Generalizing to unseen domains:
    A survey on domain generalization, in: International Joint Conference on Artificial
    Intelligence, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain generalization:
    A survey, IEEE transactions on pattern analysis and machine intelligence PP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Thrun, L. Y. Pratt, Learning to learn: Introduction and overview, in:
    Learning to Learn, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Q. Sun, Y. Liu, Z. Chen, T.-S. Chua, B. Schiele, Meta-transfer learning
    through hard tasks, IEEE Transactions on Pattern Analysis and Machine Intelligence
    44 (2019) 1443–1456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Achille, M. Lam, R. Tewari, A. Ravichandran, S. Maji, C. C. Fowlkes,
    S. Soatto, P. Perona, Task2vec: Task embedding for meta-learning, 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV) (2019) 6429–6438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Vuorio, S.-H. Sun, H. Hu, J. J. Lim, Multimodal model-agnostic meta-learning
    via task-aware modulation, in: Neural Information Processing Systems, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Nichol, J. Schulman, Reptile: a scalable metalearning algorithm, arXiv:
    Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] G. Chen, T. Zhang, J. Lu, J. Zhou, Deep meta metric learning, 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV) (2019) 9546–9555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Nichol, J. Achiam, J. Schulman, On first-order meta-learning algorithms,
    ArXiv abs/1803.02999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] E. Triantafillou, T. L. Zhu, V. Dumoulin, P. Lamblin, K. Xu, R. Goroshin,
    C. Gelada, K. Swersky, P.-A. Manzagol, H. Larochelle, Meta-dataset: A dataset
    of datasets for learning to learn from few examples, ArXiv abs/1903.03096.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Lei, X. Zhang, J. He, F. Chen, B. Du, C.-T. Lu, Cross-domain few-shot
    semantic segmentation, in: European Conference on Computer Vision, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] K. Lee, H. Yang, S. Chakraborty, Z. Cai, G. Swaminathan, A. Ravichandran,
    O. Dabeer, Rethinking few-shot object detection on a multi-domain benchmark, ArXiv
    abs/2207.11169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, Understanding cross-domain
    few-shot learning based on domain similarity and few-shot difficulty, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Zhang, Y. Zheng, X. Xu, J. Wang, How well do self-supervised methods
    perform in cross-domain few-shot learning?, ArXiv abs/2202.09014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] N. Dvornik, C. Schmid, J. Mairal, Selecting relevant features from a multi-domain
    representation for few-shot classification, in: European Conference on Computer
    Vision, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Peng, W. Song, M. Ester, Combining domain-specific meta-learners in
    the parameter space for cross-domain few-shot classification, ArXiv abs/2011.00179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] W.-H. Li, X. Liu, H. Bilen, Universal representation learning from multiple
    domains for few-shot classification, 2021 IEEE/CVF International Conference on
    Computer Vision (ICCV) (2021) 9506–9515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Xiao, S. Gu, L. Zhang, Multi-domain learning for accurate and few-shot
    color constancy, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 3255–3264.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W.-H. Li, X. Liu, H. Bilen, Cross-domain few-shot learning with task-specific
    adapters, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2022) 7151–7160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] X. Zou, Y. Yan, J. Xue, S. Chen, H. Wang, Learn-to-decompose: Cascaded
    decomposition network for cross-domain few-shot facial expression recognition,
    in: European Conference on Computer Vision, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] H. Xu, L. Liu, Cross-domain few-shot classification via inter-source stylization,
    ArXiv abs/2208.08015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] W.-Y. Lee, J.-Y. Wang, Y. Wang, Domain-agnostic meta-learning for cross-domain
    few-shot classification, ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP) (2022) 1715–1719.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Fu, Y. Fu, Y.-G. Jiang, Meta-fdmixup: Cross-domain few-shot learning
    guided by labeled target data, arXiv preprint arXiv:2107.11978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Islam, C.-F. Chen, R. Panda, L. Karlinsky, R. S. Feris, R. J. Radke,
    Dynamic distillation network for cross-domain few-shot recognition with unlabeled
    data, in: Neural Information Processing Systems, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Zhao, M. Ding, Z. Lu, T. Xiang, Y. Niu, J. Guan, J. rong Wen, P. Luo,
    Domain-adaptive few-shot learning, 2021 IEEE Winter Conference on Applications
    of Computer Vision (WACV) (2020) 1389–1398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. Zhao, Y. Meng, L. Xu, Oa-fsui2it: A novel few-shot cross domain object
    detection framework with object-aware few-shot unsupervised image-to-image translation,
    in: AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Me-d2n: Multi-expert domain
    decompositional network for cross-domain few-shot learning, Proceedings of the
    30th ACM International Conference on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] L. Zhuo, Y. Fu, J. Chen, Y. Cao, Y.-G. Jiang, Tgdm: Target guided dynamic
    mixup for cross-domain few-shot learning, Proceedings of the 30th ACM International
    Conference on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Gao, L. Yang, Y. Huang, S. Xie, S. Li, W. Zheng, Acrofod: An adaptive
    method for cross-domain few-shot object detection, in: European Conference on
    Computer Vision, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] W. Chen, Z. Zhang, W. Wang, L. Wang, Z. Wang, T. Tan, Cross-domain cross-set
    few-shot learning via learning compact and aligned representations, in: European
    Conference on Computer Vision, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] F. Yao, Cross-domain few-shot learning with unlabelled data, ArXiv abs/2101.07899.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Nakamura, Y. Ishii, Y. Maruyama, T. Yamashita, Few-shot adaptive object
    detection with cross-domain cutmix, ArXiv abs/2208.14586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Teshima, I. Sato, M. Sugiyama, Few-shot domain adaptation by causal
    mechanism transfer, in: International Conference on Machine Learning, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Medina, A. Devos, M. Grossglauser, Self-supervised prototypical transfer
    learning for few-shot classification, ArXiv abs/2006.11325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. Zou, S. Zhang, J. Yu, Y. Tian, J. M. F. Moura, Revisiting mid-level
    patterns for cross-domain few-shot recognition, Proceedings of the 29th ACM International
    Conference on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] H. Liang, Q. Zhang, P. Dai, J. Lu, Boosting the generalization capability
    in cross-domain few-shot learning via noise-enhanced supervised autoencoder, 2021
    IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 9404–9414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Wang, Z. Deng, Cross-domain few-shot classification via adversarial
    task augmentation, in: International Joint Conference on Artificial Intelligence,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R. Tao, H. Zhang, Y. Zheng, M. Savvides, Powering finetuning in few-shot
    learning: Domain-agnostic bias reduction with selected sampling, in: AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. Yuan, Z. Zhang, C. Wang, H. Song, Y. Xie, L. Ma, Task-level self-supervision
    for cross-domain few-shot learning, in: AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] P. Li, S. Gong, Y. Fu, C. Wang, Ranking distance calibration for cross-domain
    few-shot learning, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 9089–9098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Hu, A. J. Ma, Adversarial feature augmentation for cross-domain few-shot
    classification, in: ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] W. Zhang, L. Shen, W. Zhang, C.-S. Foo, Few-shot adaptation of pre-trained
    networks for domain shift, in: IJCAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Xu, L. Wang, Y. Wang, C. Qin, Y. Zhang, Y. R. Fu, Memrein: Rein the
    domain shift for cross-domain few-shot learning, in: IJCAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] P.-C. Tu, H.-K. K. Pao, A dropout style model augmentation for cross domain
    few-shot learning, 2021 IEEE International Conference on Big Data (Big Data) (2021)
    1138–1147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Jiang, Z. Li, Y. Guo, J. Ye, A transductive multi-head model for cross-domain
    few-shot learning, ArXiv abs/2006.11384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B. Liu, Z. Zhao, Z. Li, J. Jiang, Y. Guo, J. Ye, Feature transformation
    ensemble model with batch spectral regularization for cross-domain few-shot classification,
    ArXiv abs/2005.08463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Cai, B. Y. Cai, S. M. Shen, Sb-mtl: Score-based meta transfer-learning
    for cross-domain few-shot learning, ArXiv abs/2012.01784.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] X. Liu, Z. Ji, Y. Pang, Z. Zhang, Self-taught cross-domain few-shot learning
    with weakly supervised object localization and task-decomposition, ArXiv abs/2109.01302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Fu, Y. Xie, Y. Fu, J. Chen, Y.-G. Jiang, Wave-san: Wavelet based style
    augmentation network for cross-domain few-shot learning, ArXiv abs/2203.07656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] J. Oh, S. Kim, N. Ho, J.-H. Kim, H. Song, S.-Y. Yun, Refine: Re-randomization
    before fine-tuning for cross-domain few-shot learning, Proceedings of the 31st
    ACM International Conference on Information & Knowledge Management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. Ting,
    R. S. M. Goh, Few-shot domain adaptation with polymorphic transformers, ArXiv
    abs/2107.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Tavera, F. Cermelli, C. Masone, B. Caputo, Pixel-by-pixel cross-domain
    alignment for few-shot semantic segmentation, 2022 IEEE/CVF Winter Conference
    on Applications of Computer Vision (WACV) (2021) 1959–1968.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Huang, W. Yang, L. Wang, L. Zhou, M. Yang, Few-shot unsupervised domain
    adaptation with image-to-class sparse similarity encoding, Proceedings of the
    29th ACM International Conference on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. P. Phoo, B. Hariharan, Self-training for few-shot transfer across extreme
    task differences, ArXiv abs/2010.07734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. D’Innocente, F. C. Borlino, S. Bucci, B. Caputo, T. Tommasi, One-shot
    unsupervised cross-domain detection, in: European Conference on Computer Vision,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B. Xu, Z. Zeng, C. Lian, Z. Ding, Few-shot domain adaptation via mixup
    optimal transport, IEEE Transactions on Image Processing 31 (2022) 2518–2528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. P. Mohanty, D. P. Hughes, M. Salathé, Using deep learning for image-based
    plant disease detection, Frontiers in Plant Science 7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] P. Helber, B. Bischke, A. R. Dengel, D. Borth, Eurosat: A novel dataset
    and deep learning benchmark for land use and land cover classification, IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing 12 (2019)
    2217–2226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8:
    Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification
    and localization of common thorax diseases, 2017 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR) (2017) 3462–3471.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Fu, Y. Fu, J. Chen, Y.-G. Jiang, Generalized meta-fdmixup: Cross-domain
    few-shot learning guided by labeled target data, IEEE Transactions on Image Processing
    31 (2022) 7078–7090.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] L. Yalan, W. Jijie, Cross-domain few-shot classification through diversified
    feature transformation layers, 2021 IEEE International Conference on Artificial
    Intelligence and Computer Applications (ICAICA) (2021) 549–555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Sun, S. Lapuschkin, W. Samek, Y. Zhao, N.-M. Cheung, A. Binder, Explanation-guided
    training for cross-domain few-shot classification, 2020 25th International Conference
    on Pattern Recognition (ICPR) (2020) 7609–7616.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Lu, X. Wu, Z. Wu, S. Wang, Cross-domain few-shot segmentation with
    transductive fine-tuning, ArXiv abs/2211.14745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Baik, J. Oh, S. Hong, K. M. Lee, Learning to forget for meta-learning
    via task-and-layer-wise attenuation, IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44 (2021) 7718–7730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] X. Lin, M. Ye, Y. Gong, G. T. Buracas, N. Basiou, A. Divakaran, Y. Yao,
    Modular adaptation for cross-domain few-shot learning, ArXiv abs/2104.00619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Q. Zhang, Y. Jiang, Z. Wen, Tacdfsl: Task adaptive cross domain few-shot
    learning, Symmetry 14 (2022) 1097.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Ravi, H. Larochelle, Optimization as a model for few-shot learning,
    in: International Conference on Learning Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] P. Tschandl, C. Rosendahl, H. Kittler, The ham10000 dataset, a large collection
    of multi-source dermatoscopic images of common pigmented skin lesions, Scientific
    Data 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] C. Wah, S. Branson, P. Welinder, P. Perona, S. J. Belongie, The caltech-ucsd
    birds-200-2011 dataset, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Krause, M. Stark, J. Deng, L. Fei-Fei, 3d object representations for
    fine-grained categorization, 2013 IEEE International Conference on Computer Vision
    Workshops (2013) 554–561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] B. Zhou, À. Lapedriza, A. Khosla, A. Oliva, A. Torralba, Places: A 10
    million image database for scene recognition, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 40 (2018) 1452–1464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] G. V. Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, S. J. Belongie, The inaturalist species classification and detection
    dataset, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2017)
    8769–8778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. G. Satorras, J. Bruna, Few-shot learning with graph neural networks,
    ArXiv abs/1711.04043.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Chen, Y. Zheng, Z. Xu, T. Tang, Z. Tang, J. Chen, Y. Liu, Cross-domain
    few-shot classification based on lightweight res2net and flexible gnn, Knowl.
    Based Syst. 247 (2022) 108623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Yang, C. Liu, B. Li, J. Jiao, Q. Ye, Prototype mixture models for few-shot
    semantic segmentation, in: Proc. ECCV, Vol. abs/2008.03898, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Boudiaf, H. Kervadec, I. M. Ziko, P. Piantanida, I. B. Ayed, J. Dolz,
    Few-shot segmentation without meta-learning: A good transductive inference is
    all you need?, in: Proc. CVPR, 2021, pp. 13974–13983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] G. Li, V. Jampani, L. Sevilla-Lara, D. Sun, J. Kim, J. Kim, Adaptive prototype
    learning and allocation for few-shot segmentation, in: Proc. CVPR, 2021, pp. 8330–8339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, J. Jia, Prior guided feature
    enrichment network for few-shot segmentation, IEEE transactions on pattern analysis
    and machine intelligence PP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. lu, S. He, X. Zhu, L. Zhang, Y.-Z. Song, T. Xiang, Simpler is better:
    Few-shot semantic segmentation with classifier weight transformer, in: Proc. ICCV,
    Vol. abs/2108.03032, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Min, D. Kang, M. Cho, Hypercorrelation squeeze for few-shot segmentation,
    in: Proc. ICCV, Vol. abs/2104.01538, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] B. Zhang, J. Xiao, T. Qin, Self-guided and cross-guided learning for few-shot
    segmentation, in: Proc. CVPR, 2021, pp. 8308–8317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] X. Wang, H. Zhou, Z. Chen, J. Bai, J. Ren, J. Shi, Few-shot sar ship image
    detection using two-stage cross-domain transfer learning, IGARSS 2022 - 2022 IEEE
    International Geoscience and Remote Sensing Symposium (2022) 2195–2198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Wang, Z. Xu, J. Tian, J. Luo, Z. Shi, Y. Zhang, J. Fan, Z. He, Cross-domain
    few-shot learning for rare-disease skin lesion segmentation, ICASSP 2022 - 2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
    (2022) 1086–1090.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Zhang, W. Li, M. Zhang, S. Wang, R. Tao, Q. Du, Graph information aggregation
    cross-domain few-shot learning for hyperspectral image classification., IEEE transactions
    on neural networks and learning systems PP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Z. Li, M. Liu, Y. Chen, Y. Xu, W. Li, Q. Du, Deep cross-domain few-shot
    learning for hyperspectral image classification, IEEE Transactions on Geoscience
    and Remote Sensing 60 (2021) 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Zhang, W. Li, M. Zhang, R. Tao, Dual graph cross-domain few-shot learning
    for hyperspectral image classification, ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) (2022) 3573–3577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] B. Wang, Y. Xu, Z. Wu, T. Zhan, Z. Wei, Spatial–spectral local domain
    adaption for cross domain few shot hyperspectral images classification, IEEE Transactions
    on Geoscience and Remote Sensing 60 (2022) 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] M. Rostami, S. Kolouri, E. Eaton, K. Kim, Sar image classification using
    few-shot cross-domain transfer learning, 2019 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW) (2019) 907–915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Tang, C. Qiu, T. Yang, W. Jingwei, M. Chen, J. Wu, L. Wang, A novel
    lightweight relation network for cross-domain few-shot fault diagnosis, SSRN Electronic
    Journal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Feng, J. Chen, J. Xie, T. Zhang, H. Lv, T. Pan, Meta-learning as a
    promising approach for few-shot cross-domain fault diagnosis: Algorithms, applications,
    and prospects, Knowl. Based Syst. 235 (2021) 107646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] X. Dai, C. Yang, B. Liu, H. gang Gong, X. Yuan, An easy way to deploy
    the re-id system on intelligent city: Cross-domain few-shot person reidentification,
    Mobile Information Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] H. Hou, S. Bi, L. Zheng, X. Lin, Z. Quan, Sample-efficient cross-domain
    wifi indoor crowd counting via few-shot learning, 2022 31st Wireless and Optical
    Communications Conference (WOCC) (2022) 132–137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] G. Sun, Z. Liu, L. Wen, J. Shi, C. Xu, Anomaly crossing: A new method
    for video anomaly detection as cross-domain few-shot learning, ArXiv abs/2112.06320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] X. Zhou, Y. Mu, Google helps youtube: Learning few-shot video classification
    from historic tasks and cross-domain sample transfer, Proceedings of the 2020
    International Conference on Multimedia Retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. Wu, J. Tan, C. Liu, Cross-domain few-shot learning approach for lithium-ion
    battery surface defects classification using an improved siamese network, IEEE
    Sensors Journal 22 (2022) 11847–11856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Qiu, C. Zhu, W. Zhou, Meta self-learning for multi-source domain adaptation:
    A benchmark, 2021 IEEE/CVF International Conference on Computer Vision Workshops
    (ICCVW) (2021) 1592–1601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Min, H. Yao, H. Xie, C. Wang, Z. Zha, Y. Zhang, Domain-aware visual
    bias eliminating for generalized zero-shot learning, 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR) (2020) 12661–12670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. Min, H. Yao, H. Xie, Z. Zha, Y. Zhang, Domain-specific embedding network
    for zero-shot recognition, Proceedings of the 27th ACM International Conference
    on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Wang, J. Yue, J. Liu, Q. Tian, M. Wang, Large-scale few-shot learning
    via multi-modal knowledge discovery, in: European Conference on Computer Vision,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Z. Peng, Z. Li, J. Zhang, Y. Li, G.-J. Qi, J. Tang, Few-shot image recognition
    with knowledge transfer, 2019 IEEE/CVF International Conference on Computer Vision
    (ICCV) (2019) 441–449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. Guan, M. Zhang, Z. Lu, Large-scale cross-domain few-shot learning,
    in: Asian Conference on Computer Vision, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. Salehi, H. Mirzaei, D. Hendrycks, Y. Li, M. H. Rohban, M. Sabokrou,
    A unified survey on anomaly, novelty, open-set, and out-of-distribution detection:
    Solutions and future challenges, ArXiv abs/2110.14051.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] B. Liu, H. Kang, H. Li, G. Hua, N. Vasconcelos, Few-shot open-set recognition
    using meta-learning, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 8795–8804.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Jeong, S. Choi, C. Kim, Few-shot open-set recognition by transformation
    consistency, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2021) 12561–12570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, Y. Gong, Few-shot class-incremental
    learning, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2020) 12180–12189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] K. Zhu, Y. Cao, W. Zhai, J. Cheng, Z. Zha, Self-promoted prototype refinement
    for few-shot class-incremental learning, 2021 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR) (2021) 6797–6806.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] D. A. Ganea, B. Boom, R. Poppe, Incremental few-shot instance segmentation,
    2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)
    1185–1194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] K. D. M. Nguyen, S. Todorovic, ifs-rcnn: An incremental few-shot instance
    segmenter, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR) (2022) 7000–7009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/wwj.jpg Wenjian Wang Wenjian Wang received an M.S degree in computer science
    from North University of China. He is currently studying for a Ph.D. degree at
    the Faculty of Information Technology, Beijing University of Technology, China.
    His current research interests include Image processing, machine learning, and
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio
  prefs: []
  type: TYPE_NORMAL
- en: \bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/dlj.PNG Lijuan Duan received the B.Sc. and M.Sc. degrees in computer science
    from Zhengzhou University of Technology, Zhengzhou, China, in 1995 and 1998, respectively.
    She received the Ph.D. degree in computer science from the Institute of Computing
    Technology, Chinese Academy of Sciences, Beijing, in 2003\. She is currently a
    Professor at the Faculty of Information Technology, Beijing University of Technology,
    China. Her research interests include Artificial Intelligence, Image Processing,
    Computer Vision and Information Security. She has published more than 70 research
    articles in refereed journals and proceedings on artificial intelligent, image
    processing and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio\bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/yuxi_wang.pdfYuxi Wang received the Bachelor degree from North Eastern
    University, China, in 2016, and the PhD degree from University of Chinese Academy
    of Sciences (UCAS), Institute of Automation, Chinese Academy of Sciences (CASIA),
    in January 2022\. He is now an assistant professor in the Center for Artificial
    Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation,
    Chinese Academy of Science (HKISI-CAS). His research interests include transfer
    learning, domain adaptation and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio
  prefs: []
  type: TYPE_NORMAL
- en: \bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/fjs.jpgJunsong Fan received his bachelor’s degree from the School of Automation
    Science and Electrical Engineering, Beihang University, in 2016\. Then, he joined
    the Center of Research on Intelligent Perception and Computing, National Laboratory
    of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences,
    under the supervision of Professor Tieniu Tan and Zhaoxiang Zhang, and received
    his Ph.D. degree in 2022\. He is now an assistant professor in the Centre for
    Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation,
    Chinese Academy of Sciences. His research interests include computer vision and
    machine learning, label-free learning, and visual perception in open worlds.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio\bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/gz.PNGZhi Gong Zhi Gong (Student Member,IEEE) received the B.E. degree
    incomputer science and technology from Shandong Normal University,Shandong,China,in2019\.
    He is currently pursuing the Ph.D. degree in computer science and technology from
    the Beijing University of Technology,Beijing,China. His research interests include
    computer vision and image processing.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio
  prefs: []
  type: TYPE_NORMAL
- en: \bio
  prefs: []
  type: TYPE_NORMAL
- en: ./bio/zhaoxiangzhang.pdfZhaoxiang Zhang received his bachelor’s degree from
    the Department of Electronic Science and Technology in the University of Science
    and Technology of China (USTC) in 2004\. After that, he was a Ph.D. candidate
    under the supervision of Professor Tieniu Tan in the National Laboratory of Pattern
    Recognition, Institute of Automation, Chinese Academy of Sciences, where he received
    his Ph.D. degree in 2009\. In October 2009, he joined the School of Computer Science
    and Engineering, Beihang University, as an Assistant Professor (2009-2011), an
    Associate Professor (2012-2015) and the Vise-Director of the Department of Computer
    application technology (2014-2015). In July 2015, he returned to the Institute
    of Automation, Chinese Academy of Sciences. He is now a full Professor in the
    Center for Research on Intelligent Perception and Computing (CRIPAC) and the National
    Laboratory of Pattern Recognition (NLPR). His research interests include Computer
    Vision, Pattern Recognition, and Machine Learning. Recently, he specifically focuses
    on deep learning models, biologically-inspired visual computing and human-like
    learning, and their applications on human analysis and scene understanding. He
    has published more than 200 papers in international journals and conferences,
    including reputable international journals such as JMLR, IEEE TIP, IEEE TNN, IEEE
    TCSVT, IEEE TIFS and top level international conferences like CVPR, ICCV, NIPS,
    ECCV, AAAI, IJCAI and ACM MM. He is serving or has served as the Associated Editor
    of IEEE T-CSVT, Patten Recognition, Neurocomputing, and Frontiers of Computer
    Science. He has served as the Area Chair, Senior PC of international conferences
    like CVPR, NIPS, ICML, AAAI, IJCAI and ACM MM. He is a Senior Member of IEEE,
    a Distinguished Member of CCF, and a Distinguished member of CAAI.
  prefs: []
  type: TYPE_NORMAL
- en: \endbio
  prefs: []
  type: TYPE_NORMAL
