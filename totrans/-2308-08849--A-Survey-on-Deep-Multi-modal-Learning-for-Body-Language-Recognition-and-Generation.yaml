- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2308.08849] A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.08849](https://ar5iv.labs.arxiv.org/html/2308.08849)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Li Liu,, Lufei Gao, Wentao Lei, Fengji Ma, Xiaotian Lin, Jinting Wang Li Liu,
    Wentao Lei, Fengji Ma, Xiaotian Lin, and Jinting Wang are with the Hong Kong University
    of Science and Technology (Guangzhou), Guangzhou 511458, China. E-mail: avrillliu@hkust-gz.edu.cn.Lufei
    Gao is with the Shenzhen Research Institute of Big Data, Shenzhen, China.All authors
    are equal contribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Body language (BL) refers to the non-verbal communication expressed through
    physical movements, gestures, facial expressions, and postures. It is a form of
    communication that conveys information, emotions, attitudes, and intentions without
    the use of spoken or written words. It plays a crucial role in interpersonal interactions
    and can complement or even override verbal communication. Deep multi-modal learning
    techniques have shown promise in understanding and analyzing these diverse aspects
    of BL, which often incorporate multiple modalities. The survey explores recent
    advances in deep multi-modal learning, emphasizing their applications to BL generation
    and recognition. Several common BLs are considered i.e., Sign Language (SL), Cued
    Speech (CS), Co-speech (CoS), and Talking Head (TH), and we have conducted an
    analysis and established the connections among these four BL for the first time.
    Their generation and recognition often involve multi-modal approaches, for example,
    multi-modal feature representation, multi-modal fusion, and multi-modal joint
    learning will be introduced. Benchmark datasets for BL research are well collected
    and organized, along with the evaluation of state-of-the-art (SOTA) methods on
    these datasets. The survey highlights challenges such as limited labeled data,
    multi-modal learning, and the need for domain adaptation to generalize models
    to unseen speakers or languages. Future research directions are presented, including
    exploring self-supervised learning techniques, integrating contextual information
    from other modalities, and exploiting large-scale pre-trained multi-modal models.
    Real-world applications and user-centric evaluations are emphasized to drive practical
    adoption. In summary, this survey paper provides a comprehensive understanding
    of deep multi-modal learning for various BL generations and recognitions for the
    first time. By analyzing advancements, challenges, and future directions, it serves
    as a valuable resource for researchers and practitioners in advancing this field.
    In addition, we maintain a continuously updated paper list for deep multi-modal
    learning for BL recognition and generation: https://github.com/wentaoL86/awesome-body-language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Multi-modal Learning, Body Language, Sign Language, Cued Speech, Co-speech,
    Talking Head, Recognition and Generation.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Body language (BL), as a vital component of non-verbal communication, holds
    great significance in facilitating effective communication and enhancing social
    interactions. The ability to analyze and understand BL has various applications,
    ranging from BL recognition and generation to digital human interaction and assistive
    technologies. Understanding BL often necessitates the incorporation of multiple
    modalities. Deep multi-modal learning, which combines visual, audio and text modalities
    have emerged as a promising approach to enhancing the accuracy and robustness
    of intelligent BL multi-modal conversion systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab2aa85738386556e1710801db1f1b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Examples of Cued Speech, Sign Language, Co-speech and Talking Head,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we primarily focus on four typical BLs and use them as examples
    to review and analyze the multi-modal BL recognition and generation. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation") presents a simple diagram for the four types of BLs,
    i.e., Cued Speech (CS) [[1](#bib.bib1)], Sign Language (SL) [[2](#bib.bib2)],
    Co-speech (CoS) [[3](#bib.bib3)] and Talking Head (TH) [[4](#bib.bib4)]. In this
    field, there have been numerous previous works, which have made significant progress.
    However, despite the progress made in deep multi-modal learning for BL generation
    and recognition, several challenges and open research questions remain, such as
    the multi-modal learning of different types of data modalities, the scarcity of
    labeled datasets, representing fine-grained cues, modeling temporal dynamics,
    and limited computational resources. These challenges need to be addressed in
    multi-modal BL recognition and generation to further advance the field and make
    applications in human-computer interaction (HCI), social robotics, and affective
    computing more effective, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/636d32181f3350c5e1c56643e55a017d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The architecture of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: Organization of This Survey. In this survey, we first introduce four typical
    variants of BL and establish the connections between these four types in Section
    2\. Then, We organize and present various types of datasets for BL recognition
    and generation, along with evaluation metrics in Section 3\. In Sections 4 and
    5, we provide detailed reviews of the BL recognition and generation of CS, SL,
    CoS and TH, respectively. Furthermore, in Section 6, we give a detailed analysis
    of the challenges for these types of BL. Finally, we discuss and conclude this
    survey by proposing multiple research directions that need to be studied. The
    architecture of this survey is visualized in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The structured taxonomy of the existing BL research and some representative works
    are shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: The number of existing reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | SL | CS | CoS | TH | LR | SL+CS | LR+TH | Total |'
  prefs: []
  type: TYPE_TB
- en: '| R | 5 | 1 | 0 | 0 | 5 | 1 | 0 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| G | 4 | 0 | 1 | 3 | 0 | 0 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| R&G | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 10 | 1 | 1 | 3 | 5 | 1 | 1 | 22 |'
  prefs: []
  type: TYPE_TB
- en: 'The corresponding terms for the abbreviations are as follows: R – Recognition;
    G – Generation; SL – Sign Language; CS – Cued Speech; CoS – Co-speech; TH – Talking
    Head; LR – Lip Reading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Differences from Existing Reviews. Table [I](#S1.T1 "TABLE I ‣ 1 Introduction
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    presents the number of review articles related to BL recognition and generation
    in the relevant field. While there are already 22 existing surveys, the differences
    between our survey and these prior works can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scope. Existing reviews on BL [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]
    have only focused on specific subtasks within the field. For BL recognition, the
    reviews [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [5](#bib.bib5), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]
    concentrate on SL recognition. Regarding BL generation, [[6](#bib.bib6)] only
    explores CoS generation and [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]
    delves into TH generation. [[20](#bib.bib20)] integrates subtasks: LR recognition
    and TH generation. Unlike the reviews mentioned earlier, this paper focuses on
    two primary tasks: recognition and generation. Each task is expanded to incorporate
    four different types of BL: SL, CS, CoS, and TH. As far as we know, this is the
    first to encompass all four types of BL along with their corresponding recognition
    and generation tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeline. This survey highlights the latest advances, major challenges and deep
    learning (DL)-based multi-modal approaches in the aforementioned research areas
    from 2017 to the present. Please note that we will consistently update the repository
    we maintain with the latest developments. It is expected that this study will
    facilitate knowledge accumulation and the creation of deep multi-modal BL methods,
    providing readers, researchers, and practitioners with a roadmap to guide future
    direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, this survey provides a thorough examination of the progress made
    in deep multi-modal learning techniques for automatic BL recognition and generation.
    It also outlines the road ahead for future research in this area. The objective
    is to offer researchers and practitioners a consolidated understanding of the
    field, covering the foundational principles, multi-modal fusion methods, DL architectures,
    benchmark datasets, challenges, and potential directions.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S1.F3.1.pic1" class="ltx_picture" height="729.8" overflow="visible"
    version="1.1" width="1070.89"><g transform="translate(0,729.8) matrix(1 0 0 -1
    0 0) translate(530.51,0) translate(0,324.13)"><g stroke="#000000" fill="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -69.19 -0.83)" fill="#000000"
    stroke="#000000"><foreignobject width="138.37" height="17.71" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Body Language <g stroke="#80B3CC"
    fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -16.01 8.41)"><path d="M 233.44
    -103.78 C 233.44 -74.12 209.4 -50.09 179.75 -50.09 C 150.1 -50.09 126.06 -74.12
    126.06 -103.78 C 126.06 -133.43 150.1 -157.47 179.75 -157.47 C 209.4 -157.47 233.44
    -133.43 233.44 -103.78 Z M 179.75 -103.78" style="stroke:none"></path></g><g fill="#80B3CC"
    stroke="#80B3CC"><path d="M 233.44 -103.78 C 233.44 -74.12 209.4 -50.09 179.75
    -50.09 C 150.1 -50.09 126.06 -74.12 126.06 -103.78 C 126.06 -133.43 150.1 -157.47
    179.75 -157.47 C 209.4 -157.47 233.44 -133.43 233.44 -103.78 Z M 179.75 -103.78"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 126.48 -106.08)" fill="#000000" stroke="#000000"><foreignobject
    width="106.55" height="14.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Body Language Generation</foreignobject></g> <clippath id="pgfcp9"><path
    d="M 68.19 -39.37 M 77.54 -13.67 C 74.33 -31.9 64.79 -48.42 50.61 -60.32 C 65.69
    -47.67 82.89 -53.28 99.94 -63.12 L 104.64 -54.99 C 87.59 -45.14 74.13 -33.06 77.54
    -13.67 Z M 112.35 -59.44 L 104.64 -54.99 L 99.94 -63.12 L 107.65 -67.58 Z M 99.94
    -63.12 M 145.24 -62.65 C 135.57 -70.76 129.07 -82.02 126.87 -94.46 C 129.21 -81.24
    119.28 -74.29 107.65 -67.58 L 112.35 -59.44 C 123.98 -66.15 134.96 -71.28 145.24
    -62.65 Z"></path></clippath><g clip-path="url(#pgfcp9)"><g transform="matrix(1.0
    0.0 0.0 1.0 68.19 -39.37)"><g fill="#CCCCCC"><path d="M 40.57 67.51 L -28.82 107.57
    L -107.57 -28.83 L -38.18 -68.89 Z M -107.57 -28.83" style="stroke:none"></path></g><g
    fill="#80B3CC"><path d="M 103.24 31.33 L 150.93 3.79 L 72.18 -132.61 L 24.49 -105.07
    Z M 72.18 -132.61" style="stroke:none"><g transform="matrix(0.46848 -0.27048 0.567
    0.98207 32.53 -18.78)"><defs><lineargradient id="pgfsh10"></lineargradient></defs></g></path></g></g></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -30.39 0.11)"><path
    d="M 377.03 -20.76 C 377.03 8.78 353.08 32.73 323.55 32.73 C 294.01 32.73 270.07
    8.78 270.07 -20.76 C 270.07 -50.29 294.01 -74.24 323.55 -74.24 C 353.08 -74.24
    377.03 -50.29 377.03 -20.76 Z M 323.55 -20.76" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 377.03 -20.76 C 377.03 8.78 353.08
    32.73 323.55 32.73 C 294.01 32.73 270.07 8.78 270.07 -20.76 C 270.07 -50.29 294.01
    -74.24 323.55 -74.24 C 353.08 -74.24 377.03 -50.29 377.03 -20.76 Z M 323.55 -20.76"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 270.28 -24.29)" fill="#000000" stroke="#000000"><foreignobject
    width="106.55" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Sign Language production</foreignobject></g> <g fill="#80B3CC"><path
    d="M 226.25 -76.93 M 214.26 -62.65 C 223.93 -70.76 230.43 -82.02 232.63 -94.45
    C 230.29 -81.24 240.21 -74.27 251.84 -67.56 L 247.16 -59.46 C 235.53 -66.17 224.54
    -71.28 214.26 -62.65 Z M 251.74 -56.81 L 247.16 -59.46 L 251.84 -67.56 L 256.42
    -64.92 Z M 251.84 -67.56 M 270.88 -30.04 C 273.06 -42.42 279.54 -53.64 289.17
    -61.72 C 278.93 -53.13 267.99 -58.23 256.42 -64.92 L 251.74 -56.81 C 263.32 -50.13
    273.2 -43.21 270.88 -30.04 Z" style="stroke:none"></path></g><g stroke="#80B3CC"
    fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -30.39 16.72)"><path d="M 377.03
    -186.8 C 377.03 -157.26 353.08 -133.32 323.55 -133.32 C 294.01 -133.32 270.07
    -157.26 270.07 -186.8 C 270.07 -216.34 294.01 -240.28 323.55 -240.28 C 353.08
    -240.28 377.03 -216.34 377.03 -186.8 Z M 323.55 -186.8" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 377.03 -186.8 C 377.03 -157.26 353.08
    -133.32 323.55 -133.32 C 294.01 -133.32 270.07 -157.26 270.07 -186.8 C 270.07
    -216.34 294.01 -240.28 323.55 -240.28 C 353.08 -240.28 377.03 -216.34 377.03 -186.8
    Z M 323.55 -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 270.28 -190.34)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 226.25 -130.62 M 232.63 -113.1 C 230.43 -125.53 223.93
    -136.79 214.26 -144.91 C 224.54 -136.28 235.53 -141.39 247.16 -148.1 L 251.84
    -139.99 C 240.21 -133.28 230.29 -126.32 232.63 -113.1 Z M 256.42 -142.64 L 251.84
    -139.99 L 247.16 -148.1 L 251.74 -150.74 Z M 247.16 -148.1 M 289.17 -145.83 C
    279.54 -153.91 273.06 -165.13 270.88 -177.51 C 273.2 -164.35 263.32 -157.43 251.74
    -150.74 L 256.42 -142.64 C 267.99 -149.32 278.93 -154.43 289.17 -145.83 Z" style="stroke:none"></path></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -16.01 25.02)"><path
    d="M 233.23 -269.82 C 233.23 -240.28 209.29 -216.34 179.75 -216.34 C 150.21 -216.34
    126.27 -240.28 126.27 -269.82 C 126.27 -299.36 150.21 -323.3 179.75 -323.3 C 209.29
    -323.3 233.23 -299.36 233.23 -269.82 Z M 179.75 -269.82" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 233.23 -269.82 C 233.23 -240.28 209.29
    -216.34 179.75 -216.34 C 150.21 -216.34 126.27 -240.28 126.27 -269.82 C 126.27
    -299.36 150.21 -323.3 179.75 -323.3 C 209.29 -323.3 233.23 -299.36 233.23 -269.82
    Z M 179.75 -269.82"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 126.48 -273.36)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Talking Head Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 179.75 -157.47 M 198.11 -154.23 C 186.25 -158.55
    173.25 -158.55 161.38 -154.23 C 174 -158.82 175.07 -170.89 175.07 -184.32 L 184.43
    -184.32 C 184.43 -170.89 185.5 -158.82 198.11 -154.23 Z M 184.43 -189.6 L 184.43
    -184.32 L 175.07 -184.32 L 175.07 -189.6 Z M 175.07 -184.32 M 198.04 -219.57 C
    186.23 -215.27 173.27 -215.27 161.46 -219.57 C 174.02 -214.99 175.07 -202.97 175.07
    -189.6 L 184.43 -189.6 C 184.43 -202.97 185.48 -214.99 198.04 -219.57 Z" style="stroke:none"></path></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -1.63 16.72)"><path
    d="M 89.43 -186.8 C 89.43 -157.26 65.49 -133.32 35.95 -133.32 C 6.41 -133.32 -17.53
    -157.26 -17.53 -186.8 C -17.53 -216.34 6.41 -240.28 35.95 -240.28 C 65.49 -240.28
    89.43 -216.34 89.43 -186.8 Z M 35.95 -186.8" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 89.43 -186.8 C 89.43 -157.26 65.49
    -133.32 35.95 -133.32 C 6.41 -133.32 -17.53 -157.26 -17.53 -186.8 C -17.53 -216.34
    6.41 -240.28 35.95 -240.28 C 65.49 -240.28 89.43 -216.34 89.43 -186.8 Z M 35.95
    -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -17.32 -190.34)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Co-speech Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 133.25 -130.62 M 145.24 -144.91 C 135.57 -136.79
    129.06 -125.53 126.87 -113.1 C 129.2 -126.32 119.29 -133.28 107.66 -139.99 L 112.34
    -148.1 C 123.97 -141.39 134.95 -136.28 145.24 -144.91 Z M 107.76 -150.74 L 112.34
    -148.1 L 107.66 -139.99 L 103.08 -142.64 Z M 107.66 -139.99 M 88.62 -177.51 C
    86.43 -165.13 79.96 -153.91 70.32 -145.83 C 80.57 -154.43 91.5 -149.32 103.08
    -142.64 L 107.76 -150.74 C 96.18 -157.43 86.29 -164.35 88.62 -177.51 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 1.97 -22.72)"><path
    d="M 53.69 207.56 C 53.69 237.21 29.65 261.25 0 261.25 C -29.65 261.25 -53.69
    237.21 -53.69 207.56 C -53.69 177.9 -29.65 153.86 0 153.86 C 29.65 153.86 53.69
    177.9 53.69 207.56 Z M 0 207.56" style="stroke:none"></path></g><g fill="#66C2A3"
    stroke="#66C2A3"><path d="M 53.69 207.56 C 53.69 237.21 29.65 261.25 0 261.25
    C -29.65 261.25 -53.69 237.21 -53.69 207.56 C -53.69 177.9 -29.65 153.86 0 153.86
    C 29.65 153.86 53.69 177.9 53.69 207.56 Z M 0 207.56"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -53.27 205.25)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="14.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Typical
    Body Language</foreignobject></g> <clippath id="pgfcp11"><path d="M 0 78.74 M
    -26.93 73.99 C -9.54 80.32 9.54 80.32 26.93 73.99 C 8.43 80.72 4.7 98.43 4.7 118.11
    L -4.7 118.11 C -4.7 98.43 -8.43 80.72 -26.93 73.99 Z M -4.7 127.02 L -4.7 118.11
    L 4.7 118.11 L 4.7 127.02 Z M 4.7 118.11 M -18.36 157.1 C -6.5 152.78 6.5 152.78
    18.36 157.1 C 5.75 152.51 4.7 140.44 4.7 127.02 L -4.7 127.02 C -4.7 140.44 -5.75
    152.51 -18.36 157.1 Z"></path></clippath><g clip-path="url(#pgfcp11)"><g transform="matrix(1.0
    0.0 0.0 1.0 0 78.74)"><g fill="#CCCCCC"><path d="M -78.75 1.38 L -78.75 -78.74
    L 78.75 -78.74 L 78.75 1.38 Z M 78.75 -78.74" style="stroke:none"></path></g><g
    fill="#66C2A3"><path d="M -78.75 73.74 L -78.75 128.82 L 78.75 128.82 L 78.75
    73.74 Z M 78.75 128.82" style="stroke:none"><g transform="matrix(0.0 0.54095 -1.134
    0.0 0 37.57)"><defs><lineargradient id="pgfsh12"></lineargradient></defs></g></path></g></g></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 18.57 -22.72)"><path
    d="M -112.56 207.56 C -112.56 237.09 -136.51 261.04 -166.04 261.04 C -195.58 261.04
    -219.53 237.09 -219.53 207.56 C -219.53 178.02 -195.58 154.07 -166.04 154.07 C
    -136.51 154.07 -112.56 178.02 -112.56 207.56 Z M -166.04 207.56" style="stroke:none"></path></g><g
    fill="#66C2A3" stroke="#66C2A3"><path d="M -112.56 207.56 C -112.56 237.09 -136.51
    261.04 -166.04 261.04 C -195.58 261.04 -219.53 237.09 -219.53 207.56 C -219.53
    178.02 -195.58 154.07 -166.04 154.07 C -136.51 154.07 -112.56 178.02 -112.56 207.56
    Z M -166.04 207.56"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -219.32 203.94)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Sign Language</foreignobject></g>
    <g fill="#66C2A3"><path d="M -53.69 207.56 M -50.45 189.19 C -54.77 201.05 -54.77
    214.06 -50.45 225.92 C -55.04 213.31 -67.11 212.23 -80.54 212.23 L -80.54 202.88
    C -67.11 202.88 -55.04 201.8 -50.45 189.19 Z M -85.82 202.88 L -80.54 202.88 L
    -80.54 212.23 L -85.82 212.23 Z M -80.54 212.23 M -115.79 189.26 C -111.49 201.08
    -111.49 214.03 -115.79 225.85 C -111.22 213.28 -99.19 212.23 -85.82 212.23 L -85.82
    202.88 C -99.19 202.88 -111.22 201.83 -115.79 189.26 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 10.27 -37.1)"><path
    d="M -29.54 351.35 C -29.54 380.89 -53.48 404.84 -83.02 404.84 C -112.56 404.84
    -136.5 380.89 -136.5 351.35 C -136.5 321.82 -112.56 297.87 -83.02 297.87 C -53.48
    297.87 -29.54 321.82 -29.54 351.35 Z M -83.02 351.35" style="stroke:none"></path></g><g
    fill="#66C2A3" stroke="#66C2A3"><path d="M -29.54 351.35 C -29.54 380.89 -53.48
    404.84 -83.02 404.84 C -112.56 404.84 -136.5 380.89 -136.5 351.35 C -136.5 321.82
    -112.56 297.87 -83.02 297.87 C -53.48 297.87 -29.54 321.82 -29.54 351.35 Z M -83.02
    351.35"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -136.29 347.82)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech</foreignobject></g>
    <g fill="#66C2A3"><path d="M -26.85 254.05 M -41.13 242.07 C -33.02 251.74 -21.75
    258.24 -9.32 260.43 C -22.54 258.1 -29.5 268.02 -36.22 279.64 L -44.32 274.96
    C -37.61 263.34 -32.5 252.35 -41.13 242.07 Z M -46.97 279.54 L -44.32 274.96 L
    -36.22 279.64 L -38.86 284.22 Z M -36.22 279.64 M -73.74 298.69 C -61.35 300.87
    -50.14 307.35 -42.05 316.98 C -50.65 306.74 -45.55 295.8 -38.86 284.22 L -46.97
    279.54 C -53.65 291.12 -60.57 301.01 -73.74 298.69 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -6.34 -37.1)"><path
    d="M 136.5 351.35 C 136.5 380.89 112.56 404.84 83.02 404.84 C 53.48 404.84 29.54
    380.89 29.54 351.35 C 29.54 321.82 53.48 297.87 83.02 297.87 C 112.56 297.87 136.5
    321.82 136.5 351.35 Z M 83.02 351.35" style="stroke:none"></path></g><g fill="#66C2A3"
    stroke="#66C2A3"><path d="M 136.5 351.35 C 136.5 380.89 112.56 404.84 83.02 404.84
    C 53.48 404.84 29.54 380.89 29.54 351.35 C 29.54 321.82 53.48 297.87 83.02 297.87
    C 112.56 297.87 136.5 321.82 136.5 351.35 Z M 83.02 351.35"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 29.75 347.82)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Co-speech</foreignobject></g>
    <g fill="#66C2A3"><path d="M 26.85 254.05 M 9.32 260.43 C 21.75 258.24 33.02 251.74
    41.13 242.07 C 32.5 252.35 37.61 263.34 44.32 274.96 L 36.22 279.64 C 29.5 268.02
    22.54 258.1 9.32 260.43 Z M 38.86 284.22 L 36.22 279.64 L 44.32 274.96 L 46.97
    279.54 Z M 44.32 274.96 M 42.05 316.98 C 50.14 307.35 61.35 300.87 73.74 298.69
    C 60.57 301.01 53.65 291.12 46.97 279.54 L 38.86 284.22 C 45.55 295.8 50.65 306.74
    42.05 316.98 Z" style="stroke:none"></path></g><g stroke="#66C2A3" fill="#808080"
    transform="matrix(1.1 0.0 0.0 1.1 -14.64 -22.72)"><path d="M 219.53 207.56 C 219.53
    237.09 195.58 261.04 166.04 261.04 C 136.51 261.04 112.56 237.09 112.56 207.56
    C 112.56 178.02 136.51 154.07 166.04 154.07 C 195.58 154.07 219.53 178.02 219.53
    207.56 Z M 166.04 207.56" style="stroke:none"></path></g><g fill="#66C2A3" stroke="#66C2A3"><path
    d="M 219.53 207.56 C 219.53 237.09 195.58 261.04 166.04 261.04 C 136.51 261.04
    112.56 237.09 112.56 207.56 C 112.56 178.02 136.51 154.07 166.04 154.07 C 195.58
    154.07 219.53 178.02 219.53 207.56 Z M 166.04 207.56"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 112.77 204.02)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Talking
    Head</foreignobject></g> <g fill="#66C2A3"><path d="M 53.69 207.56 M 50.45 225.92
    C 54.77 214.06 54.77 201.05 50.45 189.19 C 55.04 201.8 67.11 202.88 80.54 202.88
    L 80.54 212.23 C 67.11 212.23 55.04 213.31 50.45 225.92 Z M 80.54 202.88 h 5.28
    v 9.36 h -5.28 Z M 115.79 225.85 C 111.49 214.03 111.49 201.08 115.79 189.26 C
    111.22 201.83 99.19 202.88 85.82 202.88 L 85.82 212.23 C 99.19 212.23 111.22 213.28
    115.79 225.85 Z" style="stroke:none"></path></g><g stroke="#FFD166" fill="#808080"
    transform="matrix(1.1 0.0 0.0 1.1 19.94 8.41)"><path d="M -126.06 -103.78 C -126.06
    -74.12 -150.1 -50.09 -179.75 -50.09 C -209.4 -50.09 -233.44 -74.12 -233.44 -103.78
    C -233.44 -133.43 -209.4 -157.47 -179.75 -157.47 C -150.1 -157.47 -126.06 -133.43
    -126.06 -103.78 Z M -179.75 -103.78" style="stroke:none"></path></g><g fill="#FFD166"
    stroke="#FFD166"><path d="M -126.06 -103.78 C -126.06 -74.12 -150.1 -50.09 -179.75
    -50.09 C -209.4 -50.09 -233.44 -74.12 -233.44 -103.78 C -233.44 -133.43 -209.4
    -157.47 -179.75 -157.47 C -150.1 -157.47 -126.06 -133.43 -126.06 -103.78 Z M -179.75
    -103.78"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -233.02 -106.08)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="14.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Body Language Recognition</foreignobject></g>
    <clippath id="pgfcp13"><path d="M -68.19 -39.37 M -50.61 -60.32 C -64.79 -48.42
    -74.33 -31.9 -77.54 -13.67 C -74.13 -33.06 -87.59 -45.14 -104.64 -54.99 L -99.94
    -63.12 C -82.89 -53.28 -65.69 -47.67 -50.61 -60.32 Z M -107.65 -67.58 L -99.94
    -63.12 L -104.64 -54.99 L -112.35 -59.44 Z M -104.64 -54.99 M -126.87 -94.46 C
    -129.07 -82.02 -135.57 -70.76 -145.24 -62.65 C -134.96 -71.28 -123.98 -66.15 -112.35
    -59.44 L -107.65 -67.58 C -119.28 -74.29 -129.21 -81.24 -126.87 -94.46 Z"></path></clippath><g
    clip-path="url(#pgfcp13)"><g transform="matrix(1.0 0.0 0.0 1.0 -68.19 -39.37)"><g
    fill="#CCCCCC"><path d="M 38.18 -68.89 L 107.57 -28.83 L 28.82 107.57 L -40.57
    67.51 Z M 28.82 107.57" style="stroke:none"></path></g><g fill="#FFD166"><path
    d="M -24.49 -105.07 L -72.18 -132.61 L -150.93 3.79 L -103.24 31.33 Z M -150.93
    3.79" style="stroke:none"><g transform="matrix(-0.46848 -0.27048 0.567 -0.98207
    -32.53 -18.78)"><defs><lineargradient id="pgfsh14"></lineargradient></defs></g></path></g></g></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 19.94 25.02)"><path
    d="M -126.27 -269.82 C -126.27 -240.28 -150.21 -216.34 -179.75 -216.34 C -209.29
    -216.34 -233.23 -240.28 -233.23 -269.82 C -233.23 -299.36 -209.29 -323.3 -179.75
    -323.3 C -150.21 -323.3 -126.27 -299.36 -126.27 -269.82 Z M -179.75 -269.82" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -126.27 -269.82 C -126.27 -240.28 -150.21
    -216.34 -179.75 -216.34 C -209.29 -216.34 -233.23 -240.28 -233.23 -269.82 C -233.23
    -299.36 -209.29 -323.3 -179.75 -323.3 C -150.21 -323.3 -126.27 -299.36 -126.27
    -269.82 Z M -179.75 -269.82"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -233.02
    -273.36)" fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Lip reading
    recognition</foreignobject></g> <g fill="#FFD166"><path d="M -179.75 -157.47 M
    -161.38 -154.23 C -173.25 -158.55 -186.25 -158.55 -198.11 -154.23 C -185.5 -158.82
    -184.43 -170.89 -184.43 -184.32 L -175.07 -184.32 C -175.07 -170.89 -174 -158.82
    -161.38 -154.23 Z M -175.07 -189.6 L -175.07 -184.32 L -184.43 -184.32 L -184.43
    -189.6 Z M -184.43 -184.32 M -161.46 -219.57 C -173.27 -215.27 -186.23 -215.27
    -198.04 -219.57 C -185.48 -214.99 -184.43 -202.97 -184.43 -189.6 L -175.07 -189.6
    C -175.07 -202.97 -174.02 -214.99 -161.46 -219.57 Z" style="stroke:none"></path></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 34.32 16.72)"><path
    d="M -270.07 -186.8 C -270.07 -157.26 -294.01 -133.32 -323.55 -133.32 C -353.08
    -133.32 -377.03 -157.26 -377.03 -186.8 C -377.03 -216.34 -353.08 -240.28 -323.55
    -240.28 C -294.01 -240.28 -270.07 -216.34 -270.07 -186.8 Z M -323.55 -186.8" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -270.07 -186.8 C -270.07 -157.26 -294.01
    -133.32 -323.55 -133.32 C -353.08 -133.32 -377.03 -157.26 -377.03 -186.8 C -377.03
    -216.34 -353.08 -240.28 -323.55 -240.28 C -294.01 -240.28 -270.07 -216.34 -270.07
    -186.8 Z M -323.55 -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -376.82
    -190.34)" fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech
    recognition</foreignobject></g> <g fill="#FFD166"><path d="M -226.25 -130.62 M
    -214.26 -144.91 C -223.93 -136.79 -230.43 -125.53 -232.63 -113.1 C -230.29 -126.32
    -240.21 -133.28 -251.84 -139.99 L -247.16 -148.1 C -235.53 -141.39 -224.54 -136.28
    -214.26 -144.91 Z M -251.74 -150.74 L -247.16 -148.1 L -251.84 -139.99 L -256.42
    -142.64 Z M -251.84 -139.99 M -270.88 -177.51 C -273.06 -165.13 -279.54 -153.91
    -289.17 -145.83 C -278.93 -154.43 -267.99 -149.32 -256.42 -142.64 L -251.74 -150.74
    C -263.32 -157.43 -273.2 -164.35 -270.88 -177.51 Z" style="stroke:none"></path></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 34.32 0.11)"><path
    d="M -270.07 -20.76 C -270.07 8.78 -294.01 32.73 -323.55 32.73 C -353.08 32.73
    -377.03 8.78 -377.03 -20.76 C -377.03 -50.29 -353.08 -74.24 -323.55 -74.24 C -294.01
    -74.24 -270.07 -50.29 -270.07 -20.76 Z M -323.55 -20.76" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -270.07 -20.76 C -270.07 8.78 -294.01
    32.73 -323.55 32.73 C -353.08 32.73 -377.03 8.78 -377.03 -20.76 C -377.03 -50.29
    -353.08 -74.24 -323.55 -74.24 C -294.01 -74.24 -270.07 -50.29 -270.07 -20.76 Z
    M -323.55 -20.76"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -376.82 -24.37)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Sign Language recognition</foreignobject></g>
    <g fill="#FFD166"><path d="M -226.25 -76.93 M -232.63 -94.45 C -230.43 -82.02
    -223.93 -70.76 -214.26 -62.65 C -224.54 -71.28 -235.53 -66.17 -247.16 -59.46 L
    -251.84 -67.56 C -240.21 -74.27 -230.29 -81.24 -232.63 -94.45 Z M -256.42 -64.92
    L -251.84 -67.56 L -247.16 -59.46 L -251.74 -56.81 Z M -247.16 -59.46 M -289.17
    -61.72 C -279.54 -53.64 -273.06 -42.42 -270.88 -30.04 C -273.2 -43.21 -263.32
    -50.13 -251.74 -56.81 L -256.42 -64.92 C -267.99 -58.23 -278.93 -53.13 -289.17
    -61.72 Z" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g
    fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -44.13 -0.21)"><path d="M 533.8
    8.36 L 388.03 8.36 C 384.98 8.36 382.5 5.88 382.5 2.83 L 382.5 -37.89 C 382.5
    -40.95 384.98 -43.43 388.03 -43.43 L 533.8 -43.43 C 536.86 -43.43 539.34 -40.95
    539.34 -37.89 L 539.34 2.83 C 539.34 5.88 536.86 8.36 533.8 8.36 Z M 382.5 -43.43"
    style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M 533.8 8.36 L 388.03 8.36 C 384.98 8.36 382.5 5.88 382.5 2.83 L 382.5 -37.89
    C 382.5 -40.95 384.98 -43.43 388.03 -43.43 L 533.8 -43.43 C 536.86 -43.43 539.34
    -40.95 539.34 -37.89 L 539.34 2.83 C 539.34 5.88 536.86 8.36 533.8 8.36 Z M 382.5
    -43.43"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 387.62 -3.05)" fill="#000000"
    stroke="#000000"><foreignobject width="225.54" height="63.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Text2sign[[21](#bib.bib21)] HLSTM[[22](#bib.bib22)]
    ESN[[23](#bib.bib23)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -1.63
    26.08)"><path d="M 102.99 -247.58 L -31.09 -247.58 C -34.15 -247.58 -36.62 -250.06
    -36.62 -253.11 L -36.62 -307.68 C -36.62 -310.74 -34.15 -313.22 -31.09 -313.22
    L 102.99 -313.22 C 106.04 -313.22 108.52 -310.74 108.52 -307.68 L 108.52 -253.11
    C 108.52 -250.06 106.04 -247.58 102.99 -247.58 Z M -36.62 -313.22" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 102.99 -247.58
    L -31.09 -247.58 C -34.15 -247.58 -36.62 -250.06 -36.62 -253.11 L -36.62 -307.68
    C -36.62 -310.74 -34.15 -313.22 -31.09 -313.22 L 102.99 -313.22 C 106.04 -313.22
    108.52 -310.74 108.52 -307.68 L 108.52 -253.11 C 108.52 -250.06 106.04 -247.58
    102.99 -247.58 Z M -36.62 -313.22"></path></g><g transform="matrix(0.65 0.0 0.0
    0.65 -31.51 -258.99)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="85.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DiffGAN[[24](#bib.bib24)]
    RG[[25](#bib.bib25)] SEEG[[26](#bib.bib26)] HA2G[[27](#bib.bib27)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 -43.73 17.06)"><path d="M 523.94 -171.29 L 389.86 -171.29 C 386.8
    -171.29 384.33 -173.77 384.33 -176.83 L 384.33 -203.69 C 384.33 -206.75 386.8
    -209.23 389.86 -209.23 L 523.94 -209.23 C 526.99 -209.23 529.47 -206.75 529.47
    -203.69 L 529.47 -176.83 C 529.47 -173.77 526.99 -171.29 523.94 -171.29 Z M 384.33
    -209.23" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M 523.94 -171.29 L 389.86 -171.29 C 386.8 -171.29 384.33 -173.77 384.33 -176.83
    L 384.33 -203.69 C 384.33 -206.75 386.8 -209.23 389.86 -209.23 L 523.94 -209.23
    C 526.99 -209.23 529.47 -206.75 529.47 -203.69 L 529.47 -176.83 C 529.47 -173.77
    526.99 -171.29 523.94 -171.29 Z M 384.33 -209.23"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 389.44 -182.7)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Paul Duchnowski[[28](#bib.bib28)]
    Gérard Bailly[[29](#bib.bib29)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -29.35
    26.75)"><path d="M 380.14 -254.3 L 246.06 -254.3 C 243.01 -254.3 240.53 -256.78
    240.53 -259.83 L 240.53 -314.4 C 240.53 -317.46 243.01 -319.94 246.06 -319.94
    L 380.14 -319.94 C 383.2 -319.94 385.67 -317.46 385.67 -314.4 L 385.67 -259.83
    C 385.67 -256.78 383.2 -254.3 380.14 -254.3 Z M 240.53 -319.94" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 380.14 -254.3
    L 246.06 -254.3 C 243.01 -254.3 240.53 -256.78 240.53 -259.83 L 240.53 -314.4
    C 240.53 -317.46 243.01 -319.94 246.06 -319.94 L 380.14 -319.94 C 383.2 -319.94
    385.67 -317.46 385.67 -314.4 L 385.67 -259.83 C 385.67 -256.78 383.2 -254.3 380.14
    -254.3 Z M 240.53 -319.94"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 245.65
    -265.71)" fill="#000000" stroke="#000000"><foreignobject width="207.56" height="85.24"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wav2lip [[30](#bib.bib30)]
    Audio2head[[31](#bib.bib31)] AD-NERF[[32](#bib.bib32)] DiffTalk[[33](#bib.bib33)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 33.28 26.75)"><path d="M -246.06 -254.3 L -380.14 -254.3 C -383.2
    -254.3 -385.67 -256.78 -385.67 -259.83 L -385.67 -314.4 C -385.67 -317.46 -383.2
    -319.94 -380.14 -319.94 L -246.06 -319.94 C -243.01 -319.94 -240.53 -317.46 -240.53
    -314.4 L -240.53 -259.83 C -240.53 -256.78 -243.01 -254.3 -246.06 -254.3 Z M -385.67
    -319.94" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M -246.06 -254.3 L -380.14 -254.3 C -383.2 -254.3 -385.67 -256.78 -385.67 -259.83
    L -385.67 -314.4 C -385.67 -317.46 -383.2 -319.94 -380.14 -319.94 L -246.06 -319.94
    C -243.01 -319.94 -240.53 -317.46 -240.53 -314.4 L -240.53 -259.83 C -240.53 -256.78
    -243.01 -254.3 -246.06 -254.3 Z M -385.67 -319.94"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 -380.56 -265.71)" fill="#000000" stroke="#000000"><foreignobject
    width="207.56" height="85.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rule-based[[34](#bib.bib34)]
    LBP[[35](#bib.bib35)] SDF[[36](#bib.bib36)] PTSLP[[37](#bib.bib37)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 47.66 17.06)"><path d="M -389.86 -157.44 L -523.94 -157.44 C -526.99
    -157.44 -529.47 -159.92 -529.47 -162.97 L -529.47 -217.54 C -529.47 -220.6 -526.99
    -223.08 -523.94 -223.08 L -389.86 -223.08 C -386.8 -223.08 -384.33 -220.6 -384.33
    -217.54 L -384.33 -162.97 C -384.33 -159.92 -386.8 -157.44 -389.86 -157.44 Z M
    -529.47 -223.08" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF"
    stroke-width="1.5pt"><path d="M -389.86 -157.44 L -523.94 -157.44 C -526.99 -157.44
    -529.47 -159.92 -529.47 -162.97 L -529.47 -217.54 C -529.47 -220.6 -526.99 -223.08
    -523.94 -223.08 L -389.86 -223.08 C -386.8 -223.08 -384.33 -220.6 -384.33 -217.54
    L -384.33 -162.97 C -384.33 -159.92 -386.8 -157.44 -389.86 -157.44 Z M -529.47
    -223.08"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 -524.35 -168.85)" fill="#000000"
    stroke="#000000"><foreignobject width="207.56" height="85.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Syn[[38](#bib.bib38)] MMFSL[[39](#bib.bib39)]
    Re-Syn[[40](#bib.bib40)] CMML[[41](#bib.bib41)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 47.66 0.46)"><path d="M -389.86 8.6 L -523.94 8.6 C -526.99 8.6 -529.47
    6.13 -529.47 3.07 L -529.47 -51.5 C -529.47 -54.56 -526.99 -57.03 -523.94 -57.03
    L -389.86 -57.03 C -386.8 -57.03 -384.33 -54.56 -384.33 -51.5 L -384.33 3.07 C
    -384.33 6.13 -386.8 8.6 -389.86 8.6 Z M -529.47 -57.03" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M -389.86 8.6 L
    -523.94 8.6 C -526.99 8.6 -529.47 6.13 -529.47 3.07 L -529.47 -51.5 C -529.47
    -54.56 -526.99 -57.03 -523.94 -57.03 L -389.86 -57.03 C -386.8 -57.03 -384.33
    -54.56 -384.33 -51.5 L -384.33 3.07 C -384.33 6.13 -386.8 8.6 -389.86 8.6 Z M
    -529.47 -57.03"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 -524.35 -2.81)"
    fill="#000000" stroke="#000000"><foreignobject width="207.56" height="85.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DTW[[42](#bib.bib42)] HMMs[[43](#bib.bib43)]
    FCN[[44](#bib.bib44)] RL[[45](#bib.bib45)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 23.6 -36.76)"><path d="M -149.34 366.86 L -283.41 366.86 C -286.47
    366.86 -288.95 364.39 -288.95 361.33 L -288.95 334.46 C -288.95 331.4 -286.47
    328.93 -283.41 328.93 L -149.34 328.93 C -146.28 328.93 -143.8 331.4 -143.8 334.46
    L -143.8 361.33 C -143.8 364.39 -146.28 366.86 -149.34 366.86 Z M -288.95 328.93"
    style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M -149.34 366.86 L -283.41 366.86 C -286.47 366.86 -288.95 364.39 -288.95 361.33
    L -288.95 334.46 C -288.95 331.4 -286.47 328.93 -283.41 328.93 L -149.34 328.93
    C -146.28 328.93 -143.8 331.4 -143.8 334.46 L -143.8 361.33 C -143.8 364.39 -146.28
    366.86 -149.34 366.86 Z M -288.95 328.93"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 -283.83 355.45)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CS[[1](#bib.bib1)]
    MCCS[[46](#bib.bib46)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 31.91
    -22.38)"><path d="M -232.36 223.06 L -366.43 223.06 C -369.49 223.06 -371.97 220.59
    -371.97 217.53 L -371.97 190.66 C -371.97 187.61 -369.49 185.13 -366.43 185.13
    L -232.36 185.13 C -229.3 185.13 -226.82 187.61 -226.82 190.66 L -226.82 217.53
    C -226.82 220.59 -229.3 223.06 -232.36 223.06 Z M -371.97 185.13" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M -232.36 223.06
    L -366.43 223.06 C -369.49 223.06 -371.97 220.59 -371.97 217.53 L -371.97 190.66
    C -371.97 187.61 -369.49 185.13 -366.43 185.13 L -232.36 185.13 C -229.3 185.13
    -226.82 187.61 -226.82 190.66 L -226.82 217.53 C -226.82 220.59 -229.3 223.06
    -232.36 223.06 Z M -371.97 185.13"></path></g><g transform="matrix(0.65 0.0 0.0
    0.65 -366.85 211.65)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SLreview[[2](#bib.bib2)]
    UnorgSign[[47](#bib.bib47)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -19.67
    -36.76)"><path d="M 283.41 373.79 L 149.34 373.79 C 146.28 373.79 143.8 371.31
    143.8 368.25 L 143.8 327.54 C 143.8 324.48 146.28 322 149.34 322 L 283.41 322
    C 286.47 322 288.95 324.48 288.95 327.54 L 288.95 368.25 C 288.95 371.31 286.47
    373.79 283.41 373.79 Z M 143.8 322" style="stroke:none"></path></g><g stroke="#000000"
    fill="#FFFFFF" stroke-width="1.5pt"><path d="M 283.41 373.79 L 149.34 373.79 C
    146.28 373.79 143.8 371.31 143.8 368.25 L 143.8 327.54 C 143.8 324.48 146.28 322
    149.34 322 L 283.41 322 C 286.47 322 288.95 324.48 288.95 327.54 L 288.95 368.25
    C 288.95 371.31 286.47 373.79 283.41 373.79 Z M 143.8 322"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 148.92 362.38)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="63.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoSreview[[6](#bib.bib6)]
    CoSreview[[3](#bib.bib3)] SE[[48](#bib.bib48)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 -27.98 -22.38)"><path d="M 366.43 229.99 L 232.36 229.99 C 229.3 229.99
    226.82 227.51 226.82 224.46 L 226.82 183.74 C 226.82 180.68 229.3 178.2 232.36
    178.2 L 366.43 178.2 C 369.49 178.2 371.97 180.68 371.97 183.74 L 371.97 224.46
    C 371.97 227.51 369.49 229.99 366.43 229.99 Z M 226.82 178.2" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 366.43 229.99
    L 232.36 229.99 C 229.3 229.99 226.82 227.51 226.82 224.46 L 226.82 183.74 C 226.82
    180.68 229.3 178.2 232.36 178.2 L 366.43 178.2 C 369.49 178.2 371.97 180.68 371.97
    183.74 L 371.97 224.46 C 371.97 227.51 369.49 229.99 366.43 229.99 Z M 226.82
    178.2"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 231.94 218.58)" fill="#000000"
    stroke="#000000"><foreignobject width="207.56" height="63.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">THreview[[4](#bib.bib4)] THE[[49](#bib.bib49)]
    VHTHG[[50](#bib.bib50)]</foreignobject></g></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Structured taxonomy of the existing BL research which includes three
    genres. Only several representative methods of each category are demonstrated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f2f7967da9b444908ba912a5c099e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Element compositions of four typical body language cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Typical Body Language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BL through which humans convey information usually involves five aspects, i.e.,
    gestures, facial expressions, lip reading (LR), head pose and postures. In this
    survey, we refer to these five aspects as the basic elements of BL.
  prefs: []
  type: TYPE_NORMAL
- en: Gestures refer to the use of hand movements to convey meaning. People communicate
    through actions such as waving, pointing, or gesturing with their hands. Additionally,
    facial expressions play a crucial role as a basic element of BL. Humans express
    emotions and intentions by altering the facial muscles around the eyes, eyebrows,
    mouth, etc. Another fundamental element is LR, which involves interpreting speech
    by observing the movements of the lips and mouth. Furthermore, head pose, including
    tilting or turning the head, can also convey information related to attention,
    interest, or specific desires. Lastly, postures, such as standing, sitting, or
    body leaning, contribute to conveying emotional states and social intentions within
    BL.
  prefs: []
  type: TYPE_NORMAL
- en: It is common for BL cases to consist of two or more of these modalities. As
    shown in Figure [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"), we listed four typical
    BL cases that are discussed in this survey, and each of them can be regarded as
    a composition of the basic BL elements. In this section, we will provide a comprehensive
    overview of these four BL cases, including their concepts, significance, and the
    challenges that exist in their corresponding recognition or generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Sign Language
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SL is categorized as a natural language commonly used in deaf communities [[2](#bib.bib2)].
    Based on data from the World Federation of the Deaf, the worldwide population
    of the deaf is estimated to be around 72 million, with over 80% living in developing
    nations[[47](#bib.bib47)]. Over 300 different SLs are used by these individuals,
    each having its own distinct vocabulary and grammar. SL is also known as a visual
    language which is generally composed of several visual partials, such as gestures,
    facial expressions, head pose and body postures. Specifically, six basic parameters
    are listed as the basic components of SL in [[51](#bib.bib51)], i.e., hand shape,
    orientation, movement, location, mouth shape and eyebrow movements. Taking an
    overall perspective into account, we regard gestures, facial expressions, and
    head poses as the primary visual modalities in SL.
  prefs: []
  type: TYPE_NORMAL
- en: SL is the major communication tool for the deaf, yet it is difficult to be mastered.
    In order to eliminate communication barriers, it is of great significance to develop
    technologies for automatic SL processing, including SL recognition (SLR) that
    extracts words or utterances by capturing and analyzing image or video sequences
    of the SL data, SL generation (SLG) that generates visualizable SL animations
    from input with semantic meaning and SL translation that translates the extracted
    information to another signed or spoken language [[52](#bib.bib52)][[53](#bib.bib53)].
    This survey mainly focuses on the literature review of SLR and SLG in order to
    deeply understand the important issues and difficulties in the field of SL processing.
  prefs: []
  type: TYPE_NORMAL
- en: As a highly dynamic and multi-modal visual language, SL involves a combination
    of multiple visual elements that have complementary semantics. Therefore, extracting
    and fusing high-dimensional features from different modalities effectively is
    an important task. Deep multi-modal learning techniques play a pivotal role in
    addressing these challenges and advancing the field of SL processing. By combining
    visual and spatial information from video or depth sensors with linguistic cues,
    these approaches have shown promising results in improving the accuracy and naturalness
    of SLR and SLG systems.
  prefs: []
  type: TYPE_NORMAL
- en: In Section [4.1](#S4.SS1 "4.1 Sign Language Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") and [5.1](#S5.SS1 "5.1 Sign Language Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we investigate the recent research advancements
    and techniques in deep multi-modal learning specifically for SLR and SLG. Besides,
    we delve into the challenges that are associated with these tasks and emphasize
    the potential applications and future directions in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Cued Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CS is a visual communication system proposed by Cornett [[1](#bib.bib1)] to
    enhance speech perception for individuals with hearing loss. CS uses a set of
    hand shapes and positions, named cues, to code the phonemes such as consonants
    and vowels. Figure [5](#S2.F5 "Figure 5 ‣ 2.2 Cued Speech ‣ 2 Typical Body Language
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    presents the chart for Mandarin Chinese CS (MCCS) [[46](#bib.bib46), [54](#bib.bib54)].
    Gestures in CS functions as a complement to lip-reading, visualizing the phonetic
    details that can be observed from the mouth movements to remove ambiguities caused
    by lip-reading alone. As a clear and unambiguous visual counterpart to the auditory
    information in spoken language, CS enables individuals with hearing loss to better
    understand and distinguish speech sounds, facilitating their language acquisition,
    spoken capabilities, reading skills, and overall communication abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66fe9c6984825888f54fc36da048b0cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The corresponding table between hand shapes and hand positions in
    Mandarin Chinese CS for vowels and consonants, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: CS has currently been adapted to approximately 65 languages and dialects globally,
    including English, French, Chinese, etc. [[55](#bib.bib55)]. Recently, there has
    been growing interest in developing technologies for automatic recognition and
    generation in the CS research field [[56](#bib.bib56), [57](#bib.bib57)]. These
    technologies aim to enhance accessibility for people who primarily use CS for
    communication. For instance, utilizing automatic CS recognition (ACSR), people
    can effortlessly transcribe gestures and lip-reading into corresponding spoken
    language at a phonemic level [[58](#bib.bib58), [59](#bib.bib59)]. In the opposite
    direction, a digital agent equipped with automatic CS generation (ACSG) can convert
    spoken input into authentic CS expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To process CS data efficiently, it is crucial to effectively extract information
    from two modalities: hand and lip movement. However, this task poses challenges
    in several aspects. Firstly, there exists an inherent asynchrony phenomenon when
    the human brain processes speech with gestures [[40](#bib.bib40)]; secondly, recognizing
    or generating appropriate hand shapes and lip movements entails tackling fine-grained
    image processing problems [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)].
    Consequently, deep multi-modal learning techniques have emerged as a prominent
    research trend to uncover the interplay between gestures and LR, aiming to achieve
    high-performance ACSR or ACSG systems.'
  prefs: []
  type: TYPE_NORMAL
- en: In Section [4.2](#S4.SS2 "4.2 Cued Speech Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") and [5.2](#S5.SS2 "5.2 Cued Speech Generation ‣ 5 Automatic Body
    Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we will discuss some SOTA methods to solve the problems
    lying in CS processing. We explore the challenges and opportunities in leveraging
    deep multi-modal learning techniques in this new research area, aiming to enhance
    the accessibility and inclusivity of communication for individuals who rely on
    CS.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Co-speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CoS refers to the non-verbal behaviors and signals that accompany and complement
    spoken language during communication [[6](#bib.bib6)]. It encompasses various
    visual cues, such as gestures, body postures, and facial expressions such as eye
    gaze and blinking, which can be used in conjunction with speech to convey additional
    information and meaning [[63](#bib.bib63)][[64](#bib.bib64)]. CoS gestures contribute
    substantially to the overall comprehension and interpretation of spoken language
    [[65](#bib.bib65)]. They serve as contextual cues, accentuate salient points,
    convey emotional states, and facilitate social interactions [[66](#bib.bib66)].
  prefs: []
  type: TYPE_NORMAL
- en: With the development of AI agents technologies, there has been extensive research
    exploration in CoS generation or synthesis to give AI agents such as digital humans
    more expressive and realistic BL [[67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69),
    [70](#bib.bib70), [27](#bib.bib27)]. The primary objective of this task is to
    generate a sequence of human BL by utilizing speech audio and transcripts as input,
    enhancing the performance of human-machine interaction systems. On the other hand,
    most existing gesture recognition methods primarily focus on recognizing specific
    types of gestures [[71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)],
    overlooking their connections with other modalities such as speech.
  prefs: []
  type: TYPE_NORMAL
- en: CoS signals not only play a crucial role in enhancing the clarity, expressiveness
    and emotional content of verbal communication, but also capture the rich communicative
    context, and reveal the speaker’s social identity and cultural affiliation [[48](#bib.bib48)].
    Therefore, it is a growing trend towards exploring multi-modal approaches that
    take into account both the visual information from gestures and the accompanying
    speech signals, which allows for more comprehensive and accurate analysis in the
    areas such as emotion recognition and dialogue understanding [[75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: In Section [4.3](#S4.SS3 "4.3 Co-speech Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we provide a brief overview of the work on automatic CoS recognition.
    Due to its limited application scenarios, research in this field is relatively
    scarce. In Section [5.3](#S5.SS3 "5.3 Co-speech Generation ‣ 5 Automatic Body
    Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we review the SOTA techniques and advancements in
    deep multi-modal learning for CoS generation, highlighting the potential applications
    and future research directions in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Multi-Modal Body Language Datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Name | Year | Scale | Modal | Language | Link |'
  prefs: []
  type: TYPE_TB
- en: '| Sign Language | Dicta-Sign [[76](#bib.bib76)] | 2008 | $\sim$1k | Video-Text
    | English | [Link](https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/) |'
  prefs: []
  type: TYPE_TB
- en: '| PHOENIX-Weather[[77](#bib.bib77)] | 2012 | $\sim$3k | Video-Text | Germany
    | [Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/) |'
  prefs: []
  type: TYPE_TB
- en: '| ASLLVD[[78](#bib.bib78)] | 2012 | $\sim$3K | Video-Text | English | [Link](https://www.bu.edu/asllrp/av/dai-asllvd.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SIGNUM [[79](#bib.bib79)] | 2013 | $\sim$33K | Video-Text | Germany | [Link](https://www.phonetik.uni-muenchen.de/forschung/Bas/SIGNUM/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DEVISIGN[[80](#bib.bib80)] | 2014 | $\sim$24k | Video-Text | Chinese | [Link](http://vipl.ict.ac.cn/homepage/ksl/data_ch.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASL-LEX 1.0[[81](#bib.bib81)] | 2017 | $\sim$1K | Video-Text | English |
    [Link](https://asl-lex.org/download.html) |'
  prefs: []
  type: TYPE_TB
- en: '| PHOENIX14T[[82](#bib.bib82)] | 2018 | $\sim$68K | Video-Text | Germany |
    [Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMLR[[83](#bib.bib83)] | 2019 | $\sim$102K | Image-Text | Chinese | [Link](https://www.vipazoo.cn/CMLR.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| KETI[[84](#bib.bib84)] | 2019 | $\sim$15K | Video-Text | Korean | Not Available
    |'
  prefs: []
  type: TYPE_TB
- en: '| GSL[[85](#bib.bib85)] | 2020 | $\sim$3K | Video-Text | Greek | [Link](https://zenodo.org/record/3941811#.ZHb2LXZBxD8)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASL-LEX 2.0[[86](#bib.bib86)] | 2021 | $\sim$ 10K | Video-Text-Depth | English
    | [Link](https://asl-lex.org/download.html) |'
  prefs: []
  type: TYPE_TB
- en: '| How2sign[[87](#bib.bib87)] | 2021 | $\sim$35K | Video-Text-Skelton(2D)-Depth
    | English | [Link](https://how2sign.github.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| Slovo[[88](#bib.bib88)] | 2023 | $\sim$20K | Video-Text | Russian | [Link](https://github.com/hukenovs/slovo)
    |'
  prefs: []
  type: TYPE_TB
- en: '| AASL[[89](#bib.bib89)] | 2023 | $\sim$8K | Image-Text | Arabic | [Link](https://www.kaggle.com/datasets/muhammadalbrham/rgb-arabic-alphabets-sign-language-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASL-27C[[83](#bib.bib83)] | 2023 | $\sim$23K | Image-Text | English | [Link](https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cued Speech | FCS[[90](#bib.bib90)] | 2018 | $\sim$13k | Video-Text-Audio
    | French | [Link](https://zenodo.org/record/5554849) |'
  prefs: []
  type: TYPE_TB
- en: '| BEC[[59](#bib.bib59)] | 2019 | $\sim$3k | Video-Text-Audio | English | [Link](https://zenodo.org/record/3464212)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PCSC [[91](#bib.bib91)] | 2020 | 20 (P) | Video-Text-Audio | Polish | [Link](https://phonbank.talkbank.org/access/Clinical/PCSC.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CLeLfPC[[92](#bib.bib92)] | 2022 | 350 | Video-Text-Audio | French | [Link](https://www.ortolang.fr/market/corpora/clelfpc)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MCCS-2023[[41](#bib.bib41)] | 2023 | $\sim$132k | Video-Text-Audio-Skelton(2,3D)
    | Chinese | [Link](https://mccs-2023.github.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| Co-speech | Trinity[[67](#bib.bib67)] | 2018 | 224(Min) | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://trinityspeechgesture.scss.tcd.ie/) |'
  prefs: []
  type: TYPE_TB
- en: '| TED-Gesture[[68](#bib.bib68)] | 2019 | $\sim$252k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](https://github.com/youngwoo-yoon/youtube-gesture-dataset) |'
  prefs: []
  type: TYPE_TB
- en: '| Talking With Hands [[69](#bib.bib69)] | 2019 | 200 | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://github.com/facebookresearch/TalkingWithHands32M) |'
  prefs: []
  type: TYPE_TB
- en: '| Speech2Gesture[[70](#bib.bib70)] | 2019 | $\sim$60k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](http://people.eecs.berkeley.edu/~shiry/speech2gesture/) |'
  prefs: []
  type: TYPE_TB
- en: '| TED-Expressive[[27](#bib.bib27)] | 2022 | $\sim$252k | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://github.com/alvinliu0/HA2G) |'
  prefs: []
  type: TYPE_TB
- en: '| Talking Head | GRID [[93](#bib.bib93)] | 2006 | $\sim$34k | Video-Text |
    English | [Link](https://spandh.dcs.shef.ac.uk/gridcorpus/) |'
  prefs: []
  type: TYPE_TB
- en: '| eNTERFACE [[94](#bib.bib94)] | 2006 | $\sim$1k | Video-Text-Audio | Multiple
    | [Link](http://www.enterface.net/enterface05) |'
  prefs: []
  type: TYPE_TB
- en: '| MIRACL-VC1 [[95](#bib.bib95)] | 2014 | $\sim$3k | Video-Text-Depth | English
    | [Link](https://sites.google.com/site/achrafbenhamadou/-datasets/miracl-vc1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CREMA-D[[96](#bib.bib96)] | 2015 | $\sim$7k | Video-Text-Audio | English
    | [Link](https://github.com/CheyneyComputerScience/CREMA-D) |'
  prefs: []
  type: TYPE_TB
- en: '| TCD-TIMIT [[96](#bib.bib96)] | 2015 | $\sim$7k | Video-Text-Audio | English
    | [Link](https://sigmedia.tcd.ie/TCDTIMIT/) |'
  prefs: []
  type: TYPE_TB
- en: '| MODALITY [[97](#bib.bib97)] | 2015 | $\sim$6k | Video-Text-Audio | English
    | [Link](http://www.modality-corpus.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| LRW [[98](#bib.bib98)] | 2016 | $\sim$539k | Video-Text | English | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSP-IMPROV [[99](#bib.bib99)] | 2016 | $\sim$ 1K | Video-Text-Audio | English
    | [Link](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ObamaSet[[100](#bib.bib100)] | 2017 | $\sim$1k | Video-Text-Audio | English
    | [Link](https://github.com/supasorn/synthesizing_obama_network_training) |'
  prefs: []
  type: TYPE_TB
- en: '| VoxCeleb1[[101](#bib.bib101)] | 2017 | $\sim$22k | Video-Text-Audio | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html) |'
  prefs: []
  type: TYPE_TB
- en: '| VoxCeleb2[[102](#bib.bib102)] | 2018 | $\sim$146k | Video-Text-Audio | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html) |'
  prefs: []
  type: TYPE_TB
- en: '| LRS2 [[103](#bib.bib103)] | 2018 | $\sim$96k | Video-Text | English | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LRS3-TED [[104](#bib.bib104)] | 2018 | $\sim$119k | Video-Text | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/) |'
  prefs: []
  type: TYPE_TB
- en: '| RAVDESS [[105](#bib.bib105)] | 2018 | $\sim$1k | Video-Text-Audio | English
    | [Link](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MELD [[106](#bib.bib106)] | 2018 | $\sim$13k | Video-Text-Audio | English
    | [Link](https://affective-meld.github.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| AVSpeech [[107](#bib.bib107)] | 2018 | $\sim$150k | Video-Audio | Multiple
    | [Link](http://looking-to-listen.github.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| VOCASET[[108](#bib.bib108)] | 2019 | 480 | Video-Text-Audio-3DFace | English
    | [Link](https://voca.is.tue.mpg.de/) |'
  prefs: []
  type: TYPE_TB
- en: '| LRW-1000 [[109](#bib.bib109)] | 2019 | $\sim$718K | Video-Text | Chinese
    | [Link](https://vipl.ict.ac.cn/resources/databases/201810/t20181017_32714.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FaceForensics++[[110](#bib.bib110)] | 2019 | $\sim$1k | Video-Text-Audio
    | English | [Link](https://github.com/ondyari/FaceForensics) |'
  prefs: []
  type: TYPE_TB
- en: '| MEAD[[111](#bib.bib111)] | 2020 | $\sim$281k | Video-Text-Audio | English
    | [Link](https://wywu.github.io/projects/MEAD/MEAD.html) |'
  prefs: []
  type: TYPE_TB
- en: '| HDTF[[112](#bib.bib112)] | 2021 | $\sim$10k | Video-Text-Audio | English
    | [Link](https://github.com/MRzzm/HDTF) |'
  prefs: []
  type: TYPE_TB
- en: '| AnimeCeleb [[113](#bib.bib113)] | 2022 | $\sim$2.4M | Video-Text-Audio-3DFace
    | English | [Link](https://github.com/kangyeolk/AnimeCeleb) |'
  prefs: []
  type: TYPE_TB
- en: '| VLRDT [[114](#bib.bib114)] | 2022 | $\sim$2k | Video-Text | Turkish | [Link](https://data.mendeley.com/datasets/4t8vs4dr4v/1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| KoEBA [[115](#bib.bib115)] | 2023 | 104(P) | Video-Text-Audio | Korea | [Link](https://github.com/deepbrainai-research/koeba)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Others | AV Letters [[116](#bib.bib116)] | 2002 | $\sim$19k | Video-Text
    | English | [Link](http://www.ee.surrey.ac.uk/Projects/LILiR/datasets/avletters1/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| AV Digits [[117](#bib.bib117)] | 2002 | $\sim$5k | Video-Text | English |
    [Link](https://ibug-avs.eu/) |'
  prefs: []
  type: TYPE_TB
- en: '| Aoyama Gakuin [[118](#bib.bib118)] | 2017 | $\sim$1k | Videos-Text-Audio-Skelton(2D)
    | Japanese | Not Available |'
  prefs: []
  type: TYPE_TB
- en: '| P2PSTORY[[119](#bib.bib119)] | 2018 | $\sim$13k | Video-Text-Audio | Multiple
    | [Link](https://www.media.mit.edu/projects/p2pstory/overview/) |'
  prefs: []
  type: TYPE_TB
- en: '| AMASS[[120](#bib.bib120)] | 2019 | $\sim$18k | video-text-Skelton(3D) | English
    | [Link](https://amass.is.tue.mpg.de/download.php) |'
  prefs: []
  type: TYPE_TB
- en: '| BoLD[[121](#bib.bib121)] | 2020 | $\sim$10k | Video-Text-Audio-Skelton(3D)
    | English | [Link](https://cydar.ist.psu.edu/emotionchallenge/index.php) |'
  prefs: []
  type: TYPE_TB
- en: '| PATS [[122](#bib.bib122)] | 2020 | $\sim$84k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](https://chahuja.com/pats/) |'
  prefs: []
  type: TYPE_TB
- en: '| BABEL[[123](#bib.bib123)] | 2021 | $\sim$28k | video-text-Skelton(3D) | English
    | [Link](https://babel.is.tue.mpg.de/data.html) |'
  prefs: []
  type: TYPE_TB
- en: '| HumanML3D[[124](#bib.bib124)] | 2022 | $\sim$15k | video-text-Skelton | English
    | [Link](https://github.com/EricGuo5513/HumanML3D) |'
  prefs: []
  type: TYPE_TB
- en: '| BEAT[[125](#bib.bib125)] | 2023 | $\sim$3k | Video-Text-Audio-Skelton(3D)
    | Multiple | [Link](https://pantomatrix.github.io/BEAT-Dataset/index.html) |'
  prefs: []
  type: TYPE_TB
- en: 2.4 Talking Head
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TH refers to a virtual or digital representation of the human face or head,
    typically used in multimedia applications, computer graphics, and HCI. It is usually
    an animated character that appears on a screen and can simulate various facial
    expressions, head actions, and speech with synchronized lip movements [[17](#bib.bib17),
    [126](#bib.bib126), [127](#bib.bib127)]. TH aims to enhance user experiences in
    various applications, from virtual assistants to entertainment platforms, by providing
    interactive and immersive communication interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: In 2003, visual text-to-speech (VTTS) that generates talking faces driven by
    a speech synthesizer has been proposed for HCI systems [[128](#bib.bib128)]. Speech
    synthesis techniques are used to convert text input into synthesized speech, allowing
    the virtual character to speak. Facial animation algorithms are employed to animate
    the virtual character’s facial movements, including lip synchronization with the
    generated speech. These algorithms analyze the phonetic information in the speech
    and map it onto corresponding facial movements. Additionally, sophisticated computer
    graphics techniques are utilized to generate realistic textures, lighting, and
    shading for the virtual character, enhancing its visual appearance[[129](#bib.bib129)].
    Previous approaches to TH generation faced many limitations and were unable to
    achieve high-quality and realistic results due to constraints like limited data
    availability and computing power.
  prefs: []
  type: TYPE_NORMAL
- en: TH generation needs to fuse and synchronize information from different modalities
    to ensure consistency and coherence between the animation, sound, and text of
    the character. This involves the alignment, fusion, and synchronization of each
    modal to produce a more uniform response. In recent years, DL and multi-modal
    neural networks advance the performance of TH generation from multiple perspectives
    [[19](#bib.bib19), [50](#bib.bib50)]. By using multi-modal or cross-modal techniques
    based on a large amount of data, TH generation can integrate user input from different
    sources and interact in a more natural and realistic way. This enables multi-modal
    HCI systems to better understand user intentions, generate responses accordingly,
    and provide a more immersive and personalized interactive experience.
  prefs: []
  type: TYPE_NORMAL
- en: In Section [4.4](#S4.SS4 "4.4 Talking Head Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we give a brief review of TH recognition. In Section [5.4](#S5.SS4
    "5.4 Talking Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey
    on Deep Multi-modal Learning for Body Language Recognition and Generation"), we
    explore the applications and advancements in deep multi-modal learning for TH
    generation. We discuss the challenges associated with creating realistic and expressive
    virtual characters, including the synthesis of natural-sounding speech and the
    accurate representation of facial expressions. We review the SOTA techniques and
    highlight the potential future developments in this field, aiming to improve the
    realism and interactivity of THs in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Body Language Dataset and Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Body Language Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets have played a crucial role in the entire history of BL research, serving
    as a common foundation not only for measuring and comparing the performance of
    competing algorithms but also for driving the field toward increasingly complex
    and challenging problems. Particularly in recent years, DL techniques have brought
    significant success to BL research, with a substantial amount of annotated data
    being key to this success. The availability of large-scale image collections through
    the internet has made it possible to construct comprehensive datasets. Additionally,
    the availability of multi-modal data has provided richer information for related
    tasks, opening up new possibilities for future BL recognition and generation research.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we have collected and presented relevant datasets pertaining
    to BL tasks. As shown in Table [II](#S2.T2 "TABLE II ‣ 2.3 Co-speech ‣ 2 Typical
    Body Language ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we have categorized them into five types based on data format
    and task purposes: CS, SL, CoS, TH, and others (Here “others” means these datasets
    are multi-modal BL datasets but are not designed for these four tasks). We have
    introduced the relevant information about these datasets, including publication
    year, dataset scale, available modalities, and the languages used in the datasets.
    Moreover, we have provided official links to these datasets to facilitate easier
    access for researchers. Please note that we measure the dataset scale based on
    the number of video clips/sequences. For datasets that do not provide these numbers,
    we provide the duration of the videos in minutes (represented as ”Min”) or the
    number of performers (represented as ”P”). Some examples of BL datasets are shown
    in Figure [6](#S3.F6 "Figure 6 ‣ 3.1 Body Language Datasets ‣ 3 Body Language
    Dataset and Evaluation Metrics ‣ A Survey on Deep Multi-modal Learning for Body
    Language Recognition and Generation") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3eae72b0580e0b909ebb3292e3499d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Some examples of BL datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: We present the distribution of dataset languages in Figure [7](#S3.F7 "Figure
    7 ‣ 3.1 Body Language Datasets ‣ 3 Body Language Dataset and Evaluation Metrics
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The chart shows the related datasets are primarily English datasets, but it also
    includes datasets in other languages like English datasets, Chinese datasets,
    and German datasets. This illustrates that current BL research is predominantly
    focused on English, but there is also growing importance placed on cross-cultural
    and multilingual datasets. Another problem is the difference in the format and
    standards of BL datasets. Different datasets may have varying storage formats,
    recording requirements, and model standards.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56ca6b7321fed85c539d60f50b100934.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The Distribution of language used in BL datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 BL Generation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to evaluate the performance of BL gesture generation methods, we summarize
    the main gesture generation metrics and show these generation metrics, and corresponding
    calculation formulas in Table [VI](#S4.T6 "TABLE VI ‣ 4.4 Talking Head Recognition
    ‣ 4 Automatic Body Language Recognition ‣ A Survey on Deep Multi-modal Learning
    for Body Language Recognition and Generation"). A total of seven metrics are introduced
    for evaluation, namely PCK[[130](#bib.bib130)], FGD[[131](#bib.bib131)], MAE[[132](#bib.bib132)],
    STD[[132](#bib.bib132)], PMB[[25](#bib.bib25)], MAJE[[131](#bib.bib131)], and
    MAD[[131](#bib.bib131)]. Percentage of Correct Keypoints (PCK) assesses the accuracy
    of generated motion by comparing keypoints with actual motion. A predicted keypoint
    is considered correct if it falls within a specified threshold of the actual keypoints.
    Mean Absolute Error (MAE) quantifies the average difference between standardized
    coordinate values of generated and actual keypoints. Standard Deviation (STD)
    represents the variability or distribution of keypoints from their mean position
    after standardization. Fréchet Gesture Distance (FGD) measures dissimilarity between
    the distributions of latent features in generated and ground truth gestures, incorporating
    both location and spread. Percentage of Matched Beats (PMB) considers a motion
    beat matched if its temporal distance to an audio beat is below a threshold. Mean
    Absolute Joint Error (MAJE) calculates average errors between generated and ground
    truth joint positions across all time steps and joints. Mean Absolute Difference
    (MAD) computes average differences in joint accelerations, considering magnitude
    and direction. These criteria provide comprehensive insights into the accuracy,
    similarity, and alignment between generated and ground truth motion data.
  prefs: []
  type: TYPE_NORMAL
- en: The TH generation results can be evaluated quantitatively from multiple perspectives.
    Evaluation metrics include identity-preserving metrics, audio-visual synchronization
    metrics, image quality-preserving metrics, expression metrics, and eye-blinking
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/82c926449e7a8efe6eba8df636a5ee83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The milestones of Datasets and Methods for BL recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Automatic Body Language Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will introduce the recognition for the four BL variants, with a particular
    focus on the application expansion and innovation of multi-modal learning. In
    Figure [8](#S3.F8 "Figure 8 ‣ 3.2 BL Generation Metrics ‣ 3 Body Language Dataset
    and Evaluation Metrics ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we present a summary of some representative works
    for BL recognition.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bf7f060f190f74e9452554bf6599f71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The comprehensive methods of SL recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Sign Language Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SLR aims to utilize a classifier to recognize SL glosses from video streams.
    It can be classified into two types in general according to the content of the
    SL: continuous SL recognition and isolated SL recognition. In this paper, we focus
    on continuous SL recognition (CSLR), in which the feature encoder module first
    extracts semantic representations from the sign video, and then the sequential
    module performs the mapping from the extracted semantics to the text sequence.
    In addition, some training strategies have been investigated for sufficient training.
    The comprehensive approaches are presented in Figure [9](#S4.F9 "Figure 9 ‣ 4
    Automatic Body Language Recognition ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Encoder. Since the hand acts a dominant role in the expression of SL,
    it has evolved over the past three decades, and we can divide these methods into
    the two following types.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handcraft-based Method. In the early research, handcrafted features are used
    to extract the hand motion, shape and orientation, such as HOG[[77](#bib.bib77),
    [133](#bib.bib133)], Grassmann covariance matrix (GCM)[[134](#bib.bib134)] and
    SIFT[[135](#bib.bib135)]. However, these methods require manual feature extraction
    and cannot directly be applied to different gestures, which means that different
    gestures necessitate distinct feature extraction approaches, resulting in a substantial
    amount of work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN-based Method. With the development of DL, CNNs[[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)] generally replace the handcraft-based
    methods, becoming the most powerful feature extractor for SLR. Many researchers
    try to explore the reasonable CNN-based architecture to directly extract discriminative
    visual features from the video sequence. Specifically, exist works used 2D-CNN-TCN[[140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [44](#bib.bib44)] and 3D-CNN[[140](#bib.bib140),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148)] as the backbone to extract spatial-temporal
    discriminative cues. For instance, IAN[[143](#bib.bib143)] utilizes 3D-ResNet[[138](#bib.bib138)]
    for visual representation. DNF[[149](#bib.bib149)] subtly designs 2D-CNN with
    the 1D temporal convolution, which has become one of the mainstream baseline methods.
    Although CNN-based methods can effectively capture spatial features in gesture
    images, they are limited in handling the temporal dynamics of gestures directly,
    and 3D-CNN-based methods involve significant computational overhead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sequential Module. There exist three representative approaches[[150](#bib.bib150),
    [151](#bib.bib151)] for CSLR. In the early research, HMM[[152](#bib.bib152), [153](#bib.bib153),
    [72](#bib.bib72), [154](#bib.bib154)] is used to learn the correspondence between
    the visual representation and sign gloss sequence. However, gesture actions of
    SLR often have long-term dependencies, and HMM struggles to capture such complex
    sequential patterns. Additionally, HMM does not consider the alignment between
    input and output modalities. To this end, the RNN-based methods with CTC loss[[155](#bib.bib155),
    [156](#bib.bib156), [141](#bib.bib141), [143](#bib.bib143), [157](#bib.bib157)]
    are developed for CSLR to replace the HMM model, which improves the model’s ability
    to handle data with incomplete alignments, but the ability to model global information
    is still limited. Therefore, to better understand the semantic relationship of
    the entire sign language sequence, encoder-encoder[[22](#bib.bib22), [158](#bib.bib158),
    [159](#bib.bib159)] has become a commonly used sequential framework. For instance,
    Guo et al.[[22](#bib.bib22)] utilizes the encoder-decoder framework with hierarchical
    deep recurrent fusion to merge cues from RGB and skeleton modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Training Strategy. For sufficient training, some optimization strategies are
    widely used, with the most prominent being CTC[[160](#bib.bib160), [156](#bib.bib156),
    [161](#bib.bib161)] and Iterative Training[[141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [161](#bib.bib161)] strategies. On top of these two strategies,
    Pu et al. [[142](#bib.bib142)] introduce a cross-modality constraint called CMA
    to aid the training. Hao et al. [[160](#bib.bib160)] propose a three-stage optimization
    approach, which improves the recognition performance but it is time-consuming.
    Recently, Min et al. [[156](#bib.bib156)] further present two auxiliary constraints
    over the frame-level probability distributions, making the entire model end-to-end
    trainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The timeline of some representative works for BL recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Year | Ref | Feature Extraction | Sequence Model | Learning Paradigm
    | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| SL | 2019 | Pei et al. [[162](#bib.bib162)] | 3D-ResNet | BGRU | CTC | Phoenix-2014
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | Pei et al. [[163](#bib.bib163)] | 3D-ResNet | Transformer | Reinforcement
    Learning | Phoenix-2014 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | Cui et al. [[141](#bib.bib141)] | CNN | RNN | Iterative Training
    | Phoenix-2014 and SIGNUM |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | Niu et al. [[164](#bib.bib164)] | CNN | Transformer | CTC | Phoenix-2014
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | SAFI [[151](#bib.bib151)] | 2D-CNN *plus* 1D-CNN | SAN | ACE *plus*
    CTC | Phoenix-weather and SIGNUM |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | Koishybay et al. [[165](#bib.bib165)] | 2D-CNN *plus* 1D-CNN |
    RNN | Iterative GR *plus* CTC | Phoenix-weather and SIGNUM |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | SLRGAN [[166](#bib.bib166)] | CNN | BiLSTM | GAN | Phoenix-weather,
    CSL and GSL |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | Chen et al. [[167](#bib.bib167)] | S3D | BLC | CTC *plus* Self-distillation
    | Phoenix-2014 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | Zhou et al. [[161](#bib.bib161)] | SMC | BiLSTM *plus* SA-LSTM
    | CTC *plus* Keypoint Regression | PHOENIX-2014, CSL and PHOENIX-2014-T |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | Hu et al. [[168](#bib.bib168)] | 2D-CNN | 1D-CNN *plus* BiLSTM
    | SSTM *plus* TSEM | PHOENIX-2014, PHOENIX-2014-T, CSL and CSL-Daily |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | Zheng et al. [[169](#bib.bib169)] | CNN | VAE | CTC *plus* Contrastive
    Alignment Loss | PHOENIX-2014 and PHOENIX-2014-T |'
  prefs: []
  type: TYPE_TB
- en: '| CS | 2018 | Liu et al. [[90](#bib.bib90)] | CNN | HMM | - | French CS |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | Papadimitriou et al. [[170](#bib.bib170)] | 2D-CNN *plus* 3D-CNN
    | Attention-based CNN | - | French and British English CS |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | Liu et al. [[40](#bib.bib40)] | CNN | MSHMM | HPM | French and
    British English CS |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | Wang et al. [[58](#bib.bib58)] | CNN *plus* ANN | BiLSTM *plus*
    FC | Cross-Modal Knowledge Distillation | French and British English CS |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | Sankar et al. [[171](#bib.bib171)] | Bi-GRU | Bi-GRU | CTC | CSF18
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | Liu et al. [[41](#bib.bib41)] | ResNet-18 | Transformer | Cross-Modal
    Mutual Learning | Chinese, French, and British English CS |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: The timeline of SL generation works.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Year | Ref | Input Modality | Framework | Dataset | Description |'
  prefs: []
  type: TYPE_TB
- en: '| SL | 2011 | kippet et al. [[172](#bib.bib172)] | RCB video | EMBR | ViSiCAST
    | A gloss-centric tool is proposed to enable the comparison of avatars with human
    signers. But it is necessary to incorporate non-manual features. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2016 | John et al. [[173](#bib.bib173)] | RGB video | Segmental framework
    | Own dataset | This approach achieves automatic realism in generated images with
    low complexity, but it requires positioning the shoulder and torso. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2016 | Sign3D[[174](#bib.bib174)] | RGB video | Heterogeneous Database
    | Own dataset | This approach guarantees sign avatars that are easily understood
    and widely accepted by viewers, but it is restricted to a limited set of sign
    phrases. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2018 | HLSTM[[22](#bib.bib22)] | RCB video | LSTM | Own dataset | This
    approach shows robustness in effectively aligning the word order with visual content
    in sentences. Nevertheless, a limitation arises when generalizing it to new datasets.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | Text2Sign[[21](#bib.bib21)] | Text | Transformer | PHOENIX14T |
    It demonstrates robustness in handling the dynamic length of the output sequence.
    However, It did not incorporate nonmanual information. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | Zelinka et al. [[175](#bib.bib175)] | Text | CNN | Crech news |
    This method is robust to missing part, but face expression is not included. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | ESN[[23](#bib.bib23)] | Text | GAN | PHOENIX14T | It shows Robustness
    to non-manual feature generation. But the genrated signs are not realistic . |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | Necati et al. [[176](#bib.bib176)] | Text | Transformers | PHOENIX14T
    | It does not need the gloss information, but the model is complex |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | Saunders et al. [[177](#bib.bib177)] | Text | GAN | PHOENIX14T
    | Robust to manual feature generation. The generated signs are not realistic.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | DSM. [[178](#bib.bib178)] | Gloss | Transformer | PHOENIX14T |
    This work improves the prosody in generated Sign Languages by modeling intensification
    in a data-driven manner. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | SignGAN. [[179](#bib.bib179)] | Text | FS-Net | meineDGS | It tackles
    large-scale SLP by learning to co-articulate between dictionary signs and improves
    the temporal alignment of interpolated dictionary signs to continuous signing
    sequences |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | PoseVQ-Diffusion. [[180](#bib.bib180)] | Gloss | CodeUnet | PHOENIX14T
    | It proposes a vector quantized diffusion method for conditional pose sequences
    generation and develops a novel sequential k-nearest-neighbors method to predict
    the variable lengths of pose sequences for corresponding gloss sequences |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: The timeline of Co-speech and Cued Speech generation works.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Year | Ref | Input Modality | Framework | Dataset | Description |'
  prefs: []
  type: TYPE_TB
- en: '| CoS | 2015 | DCNF[[181](#bib.bib181)] | Text | FC network | DIAC | This work
    integrated speech text, prosody, and part-of-speech tags to generate co-verbal
    gestures using a combination of FC networks and a Conditional Random Field (CRF).
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | S2G[[70](#bib.bib70)] | RGB video | CNN | S2G | This work presents
    a method for generating gestures with audio speech, utilizing cross-modal translation
    and training on unlabeled videos. But it relies on noisy pseudo ground truth for
    training |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2020 | StyleGestures [[182](#bib.bib182)] | RCB video | LSTM | Trinity
    | It achieves natural variations without manual annotation and allows control
    over gesture style while maintaining perceived naturalness. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | A2G[[183](#bib.bib183)] | Text | CVAE | Trinity | This work employed
    a CVAE to generate diverse gestures from speech input and involved a one-to-many
    mapping of speech-to-gesture. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2021 | Text2Gestures[[184](#bib.bib184)] | Text | Transformer | MPI-EBEDB
    | Their approach employed Transformer-based encoders and decoders to generate
    sequential joint positions based on the text and previous pose. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | ZeroEGGS[[185](#bib.bib185)] | Text | Variational Framework | Own
    dataset | A VAE-based framework is utilized to generate style-controllable CoS
    gestures and allowed for the generation of stylized gestures by conditioning on
    a zero-shot motion example |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | DiffGAN[[24](#bib.bib24)] | Text | Diffusion Model | PATS | An
    adversarial domain-adaptation approach is proposed to personalize the gestures
    of a speaker |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2022 | RG[[25](#bib.bib25)] | Trinity and TED | QVAE | PHOENIX14T | This
    work introduces a novel CoS gesture synthesis method that effectively captures
    both rhythm and semantics. |'
  prefs: []
  type: TYPE_TB
- en: '| CS | 1998 | Paul et al. [[28](#bib.bib28)] | Text | Template | Own dataset
    | Relying on manually selected keywords, low-context sentences, and pre-defined
    gesture templates. Its limitations included constrained expressiveness and increased
    manual effort. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2008 | Gérard et al. [[29](#bib.bib29)] | RGB video | Template | Own dataset
    | A post-processing algorithm was introduced to fine-tune synthesized hand gestures
    by addressing rotation, translation, and adaptation to new images. However, it
    relies on prior knowledge for adapting to new images. |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Cued Speech Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic lip reading is a crucial component of ACSR. Therefore, we will first
    introduce the research progress in automatic lip-reading and then review ACSR.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Automatic Lip Reading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Advances in DL have led to a promising performance in lip-reading methods. Generally,
    DL-based lip-reading methods consist of two main parts, one is the extraction
    of visual feature information, and the other is the classification of sequence
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Extraction. Traditional studies use pixel-based[[186](#bib.bib186)],
    shape-based[[187](#bib.bib187), [188](#bib.bib188)], and hybrid-based[[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]
    approaches to extract the visual feature. However, these methods are not only
    sensitive to image illumination change, lip deformation, and rotation but also
    cannot extract automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, DL has gradually become the mainstream research method in lip visual
    feature extraction, which can be divided into four categories. First, 2D-CNN-based
    methods are used[[194](#bib.bib194), [195](#bib.bib195)], which solves the problem
    of automatic feature extraction, but it can only process single-frame images and
    has a weak ability to process continuous frames, ignoring the spatio-temporal
    correlation between continuous frames. Then, 3D-CNN-based methods have received
    extensive attention[[196](#bib.bib196), [197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199)]. Although this method can solve the problem of spatio-temporal
    correlation of continuous frames, it loses the extraction of fine-grained feature
    information by 2D convolution to a certain extent. According to the aforementioned
    issues, the hybrid methods [[200](#bib.bib200), [201](#bib.bib201), [202](#bib.bib202)]
    of 2D-CNN and 3D-CNN are also introduced to solve the problem of spatio-temporal
    feature extraction and local fine-grained feature extraction simultaneously. This
    method utilizes 3D-CNN to extract spatio-temporal information and then directly
    accesses 2D-CNN to extract fine-grained local information. However, it still affects
    the time information of feature coding to some extent. For that purpose, some
    other neural networks have gradually become a popular choice for lip visual feature
    extraction, such as Autoencoder model[[203](#bib.bib203), [204](#bib.bib204),
    [205](#bib.bib205), [206](#bib.bib206)].
  prefs: []
  type: TYPE_NORMAL
- en: Recognition Modeling. So far, there have been many works viewing lip reading
    as a sequence-to-sequence task and using sequence-based methods to deal with it,
    such as RNN, LSTM, and Transformer. It divides the feature representations extracted
    from the feature extractor into equal time steps, feeding each of them sequentially
    to the classification layer. For instance, [[207](#bib.bib207), [199](#bib.bib199),
    [208](#bib.bib208), [209](#bib.bib209), [202](#bib.bib202), [210](#bib.bib210)]
    utilize Long-Short Term Memory (LSTM) networks and Gated Recurrent Unit (GRU)
    to capture both global and local temporal information. Considering that Temporal
    Convolutional Network (TCN) has the advantage of faster converging speed with
    longer temporal memory than LSTM or RNN models, it is also widely used in this
    task. For example, Bai et al.[[211](#bib.bib211)] first propose a simple yet effective
    TCN architecture, indicating that TCN can become a reasonable alternative to RNN
    as a sequential model. Following this work, Martinez et al.[[212](#bib.bib212)]
    further demonstrate that multi-scale TCN can outperform RNN in lip reading isolated
    words. However, these methods are relatively weak in modeling long-term dependencies
    and cannot directly capture long-term dependencies in sequences. Therefore, a
    new trend in the use of Transformer[[213](#bib.bib213)] for lip-reading tasks
    has emerged[[214](#bib.bib214), [37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: Although the aforementioned methods achieve promising performance, they cannot
    solve the problem of inconsistency between the input and the output modality for
    lip reading. For that purpose, many advanced works are further developed in recent
    years, such as attention mechanisms[[215](#bib.bib215), [98](#bib.bib98), [197](#bib.bib197),
    [214](#bib.bib214), [216](#bib.bib216), [217](#bib.bib217), [218](#bib.bib218)]
    and contrastive learning[[219](#bib.bib219)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Automatic Cued Speech Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The literature on ACSR can be classified into three main categories: Multimodal
    Feature Extraction, Multimodal Fusion, and ACSR Modeling. We discuss them separately
    in this section and review the representative works of CS in Table [III](#S4.T3
    "TABLE III ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Feature Extraction. In the literature, there are several popular
    methods for CS feature extraction (i.e., lips, hand position and hand shape).
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional Method. It uses artificial markings to record lips and hands from
    video images[[220](#bib.bib220), [221](#bib.bib221)]. For example, Burger et al.[[222](#bib.bib222)]
    let the speaker wear black gloves to obtain accurate hand segmentation, while
    Noureddine et al.[[40](#bib.bib40)] placed blue marks on the speaker’s fingers
    to obtain the coordinates of the fingers. However, both the speaker’s clothing
    color and the background color can affect the accuracy of the hand segmentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN-based Method. Recently, some CNN-based methods are utilized to get rid of
    artificial markings. For example, the CNN model is used in [[90](#bib.bib90),
    [40](#bib.bib40), [170](#bib.bib170)] to extract visual features from the regions
    of the lip and hand. On the basis of using the CNN model for the feature extraction
    of lips and hand shape, Liu et al. [[40](#bib.bib40)] further adopt the artificial
    neural network (ANN) to process the hand position feature. However, although CNN-based
    methods do not require artificial marks, their performances are limited by data
    scarcity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other DL-based Method. Considering the data-hungry problem for multi-modal,
    some researchers try to introduce some advanced methods to solve this issue. For
    instance, Wang et al. [[58](#bib.bib58)] use lips, hand shape, and hand position
    to pre-train multi-modal feature extractor, using it for feature extraction of
    ACSR task. In addition, in another of their work [[206](#bib.bib206)], the three-stage
    multi-modal feature extraction model based on self-supervised contrastive learning
    and self-attention mechanism is proposed to model spatial and temporal features
    of CS hand shape, lips, and hand position.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multi-modal Fusion. Most existing works in ACSR tend to direct concatenate the
    multi-modal feature flows, letting the model learn such features implicitly [[221](#bib.bib221),
    [206](#bib.bib206), [90](#bib.bib90), [170](#bib.bib170)]. For instance, [[221](#bib.bib221),
    [206](#bib.bib206)] utilize artificial marks to obtain regions of interest (ROIs)
    and directly concatenated features of lip and hand. MSHMM[[90](#bib.bib90)] merges
    different features by giving weights for different CS modalities. However, to
    the best of our knowledge, a critical issue in ACSR is the asynchrony between
    hand and lip articulations [[40](#bib.bib40), [57](#bib.bib57), [223](#bib.bib223)],
    while these researches mainly assume lip-hand movements are synchronous by default,
    ignoring the asynchronous issue.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to tackle asynchronous modalities in the ACSR task, Liu et al. [[40](#bib.bib40)]
    propose to utilize the re-synchronization method to align the hand and lips features,
    which is realized by introducing the prior knowledge of the hand position and
    hand shape. Nevertheless, since the acquisition of prior knowledge depends on
    speakers and specific datasets, it is difficult to directly apply it to other
    languages. For that purpose, Liu et al. [[41](#bib.bib41)] further propose a Transformer-based
    cross-modal mutual learning framework for multi-modal feature fusion. The framework
    captures linguistic information by constructing a modality-invariant shared representation
    and uses this linguistic information to guide cross-modal information alignment.
    Recently, [[224](#bib.bib224)] proposes a novel Federated CS recognition (FedCSR)
    framework to train an model of CS recognition in the decentralized data scenario.
    Particularly, they design a mutual knowledge distillation fusion mechanism to
    maintain cross-modal semantic consistency of the CS multi-modalities, which learning
    a unified feature space for both speech and visual feature.
  prefs: []
  type: TYPE_NORMAL
- en: ACSR Modeling. ACSR aims to transcript visual cues of speech to text. In the
    early research, traditional statistical methods are used, which map sequences
    of hand-crafted features to phonemes using statistical models, such as HMM [[221](#bib.bib221),
    [220](#bib.bib220)] and HMM-GMM[[90](#bib.bib90), [40](#bib.bib40)]. However,
    such methods only consider the relationships between the current state and the
    previous one, which means that longer contextual information cannot be captured.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, traditional DL-based methods (i.e., CNN-based, LSTM-based) have
    been developed to alleviate the aforementioned problem. For instance, Sankar et
    al.[[39](#bib.bib39)] propose a novel RNN model trained with a Connectionist Temporal
    Classification (CTC) loss [[225](#bib.bib225)]. Papatimitriou et al.[[170](#bib.bib170)]
    propose a fully convolutional model with a time-depth separable block and attention-based
    decoder. However, such traditional DL-based methods still cannot capture long-time
    dependencies well, while it would be desirable to capture global dependency [[213](#bib.bib213)]
    over dynamic longer because of the context relationships of phonemes in long-time
    CS videos. For that purpose, Transformer-based methods [[170](#bib.bib170)] receive
    a lot of attention on the ACSR task in recent years. This kind of method achieves
    promising performance on the ACSR task, but it still requires powerful computing
    resources and a large dataset for training and parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, considering the existing corpus for ACSR is limited, some advanced
    methods such as cross-modal knowledge distillation method [[58](#bib.bib58)] and
    contrastive learning method [[206](#bib.bib206)], are also introduced to this
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Co-speech Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the existing research on CoS mainly focuses on the generation of CoS
    gestures, some scholars have shown that recognizing emotional expressions in CoS
    are crucial to this generation task. For example, Bock et al. [[226](#bib.bib226)]
    is the first to use the EmoGes corpus for emotion recognition in CoS gesture generation.
    Bhattacharya et al. [[227](#bib.bib227)] proposed to leverage the Mel-frequency
    cepstral coefficients and the text transcript computed from the input speech in
    separate encoders in our generator to learn the desired sentiments and the associated
    affective cues.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Talking Head Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the development of the TH generation still has a long way to go, the focus
    of recent studies is not on TH recognition. TH recognition is primarily treated
    as an evaluation metric for TH generation algorithms. However, humans have the
    ability to detect and identify a person from their face, even when there are changes
    in gender or facial expressions. However, it is difficult to build an automatic
    face recognition system. Therefore, the focus of TH recognition is primarily on
    capturing the essential facial attributes of the target speaker rather than full-fledged
    recognition of the speaker’s identity. In the work proposed by Wen et al. [[228](#bib.bib228)],
    they classified the face identity to assess the performance of voice-based face
    reconstruction for known subjects. For unknown subjects, they used a gender classifier
    to evaluate the gender of the generated faces. Additionally, the feature distance,
    such as Cosine, $L_{1}$, and $L_{2}$ distances, between the target face and the
    generated face can be calculated to measure the accuracy of the generated face.
    To achieve this, a pre-trained face recognition model like FaceNet [[229](#bib.bib229)]
    or ArcFace [[230](#bib.bib230)] is employed as a feature extractor. The landmark
    distance (LMD) can also be measured as the disparity between the generated face
    and the real-world target face images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77bfdf899a0c43b3a2ec69b957685183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The milestones of Datasets and Methods for BL generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Metrics for gesture generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Calculation Formula |'
  prefs: []
  type: TYPE_TB
- en: '| PCK [[130](#bib.bib130)] | $\text{PCK}=\frac{1}{N}\sum_{i=1}^{N}\textbf{1}(d_{i}\leq\tau){N}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| FGD [[131](#bib.bib131)] | $\text{FGD}=\max_{\pi}\left(\frac{1}{T}\sum_{t=1}^{T}d(g_{t}^{*},g_{\pi(t)})\right)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAE [[132](#bib.bib132)] | $\text{MAE}=\frac{1}{T}\sum_{t=1}^{T}\left&#124;g_{t}-g_{t}^{*}\right&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: '| STD [[132](#bib.bib132)] | $\text{STD}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(g_{i}-\bar{g})^{2}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| PMB [[25](#bib.bib25)] | $\operatorname{PMB}=\frac{1}{N_{m}}\sum_{i=1}^{N_{m}}\sum_{j=1}^{N_{a}}\textbf{1}\left[\left\&#124;\bm{b}_{i}^{m}-\bm{b}_{j}^{a}\right\&#124;_{1}<\delta\right]$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAJE [[131](#bib.bib131)] | $\text{MAJE}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;g_{t}^{n}-g_{t}^{*n}\right&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAD [[131](#bib.bib131)] | $\text{MAD}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;a_{t}^{n}-a_{t}^{*n}\right&#124;_{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: 'The corresponding meanings for the alphabet are as follow: $N$ – The number
    of samples; $d_{i}$ – The distance of generated points and ground truth; $\tau$
    – The thresohold of PCK; $T$ – the number of generated frames; $g_{t}$ – the generated
    gestures; ${g_{t}}^{\*}$ – the ground truth; $b_{i}$ – The key frame corresponding
    to the beat. $a_{i}$ – The movement aceleration of generated gestures.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Automatic Body Language Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The gesture generation task aims to generate a continuous sequence of gestures
    (i.e., face, head, and hand) using multi-modal inputs (e.g., gloss, speech, and
    text). In this section, we present the related works on gesture language generation
    and review the development timeline of gesture language generation applications,
    such as CS, SL, CoS gesture generations, and TH Generation, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Sign Language Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the very beginning, we first present the difference between SL, CoS, and
    CS in Figure [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"). SL generation has been
    studied for a long time. In this part, we mainly discuss the DL-based research
    on SL generation. For other SL generation methods, please refer to [[5](#bib.bib5)].
    In Table [IV](#S4.T4 "TABLE IV ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body
    Language Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we present a summary of the details of the related
    SL generation works.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Feature Extraction. As a special visual language, the inputs of
    the SL gesture generation task are not only text and speech but also SL Gloss.
    It is a marking system for recording SL words and phrases, usually using written
    symbols and short descriptions to represent gestures, mouth movements as well
    as other non-gesture features. SL Gloss is suitable for recording the content
    of SL in written form to facilitate learners to learn and understand SL expressions.
    Previous work [[177](#bib.bib177), [231](#bib.bib231)] first converts spoken language
    to gloss and then uses gloss as input to extract features to generate SL gestures.
    Some work [[175](#bib.bib175)] use spoken language words and their characters
    as input to extract the word embedding of text, then the text features were used
    for gesture generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Methods. For the SL generation task, there are several popular DL-based
    methods: 1) Neural Machine Translation (NMT) method, which [[82](#bib.bib82),
    [22](#bib.bib22), [21](#bib.bib21)] views the SL generation as a translation task.
    It uses the neural machine translation model to process SL text input, which can
    handle the output SL sequence of dynamic length but needs to solve problems such
    as domain adaptation. 2) Motion Graph method [[21](#bib.bib21)] uses motion graphics
    technology to construct a directed graph from motion capture data and generate
    SL. This method can handle the continuity of SL, but it requires large scales
    of data and another challenge is the scalability and computational complexity
    of the graph to select the best transitions. 3) Conditional generation methods
    such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAEs)
    are also employed to generate SL videos. A hybrid model, including a VAE and GAN
    combination, has been proposed for the generation of people performing SL [[232](#bib.bib232),
    [233](#bib.bib233)]. However, the problems such as model complexity and video
    quality need to be solved. 4) Other methods. In addition to the previous work,
    some research tries to introduce novel transformer-based model architectures for
    SLP. For example, [[231](#bib.bib231)] proposes a Progressive Transformers to
    generate continuous sign sequences from spoken language sentences. [[234](#bib.bib234)]
    combines a transformer with a Mixture Density Network (MDN) to manage the translation
    from text to skeletal pose. Although these works have brought performance improvements,
    the cost of the model complexity cannot be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the SL generation has made some progress, some challenges in the
    CSL generation are still unsolved, i.e., 1) The SL relies on facial expression
    to identify the specific meaning and avoid ambiguity. But few works consider facial
    expressions. 2) The scale of the SL gestures library is very large. According
    to the official Chinese SL dictionary, there are about 5600 kinds of frequently
    used SL gestures. Most of the dataset only covers a small portion of all gestures,
    for example, [[235](#bib.bib235)] builds a CSL dataset with 500 categories. The
    huge number of gestures brings a huge cost for the DL-based models to construct
    the mapping relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5272d329189c4744511282ed8ce002f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The overall framework of the conversion between CS and text/audio.
    Direction 1 means CS to text/audio recognition, and direction 2 means text/audio
    to CS gesture generation. The first direction aims to recognize text or audio
    to make normal hearing better understand the hearing-impaired people, and the
    second direction can help the hearing-impaired to visually understand normal-hearing
    people.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Cued Speech Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a lip-hand aided system, CS requires generating both lip and hand gestures
    simultaneously. Therefore, it is very important to extract multi-modal features
    such as speech features and text features. Among them, speech features have a
    strong correlation with lip movement. At the same time, text features play an
    important role in determining hand shape and position according to the coding
    system. As depicted in Figure [11](#S5.F11 "Figure 11 ‣ 5.1 Sign Language Generation
    ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning
    for Body Language Recognition and Generation"), the generation of multi-modal
    CS hand gestures from audio-text is a crucial component of the CS conversion system.
    Previous studies in the literature have made limited initial attempts at CS gesture
    generation, which is mainly from two perspectives of multi-modal feature extraction
    and generation methods. Since the related work is relatively small, we incorporate
    the summary of the related CS generation works with the CoS In Table [V](#S4.T5
    "TABLE V ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Feature Extraction. For CS generation, the feature includes continuous
    lip shape and hand shape movements. [[28](#bib.bib28)] used specific manually
    selected keywords, along with low-context sentences [[236](#bib.bib236)] as a
    feature, and pre-defined corresponding manual templates for hand gestures. CS
    recognition was performed, followed by the mapping of recognized text to the hand
    templates. However, this approach heavily relied on manual designs, which not
    only constrained the expressiveness of CS gestures but also increased the amount
    of manual effort required.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Method. To the best of our knowledge, there is still a lack of research
    on end-to-end deep learning-based CS gesture generation. Only [[28](#bib.bib28)]
    proposed a post-processing algorithm to adjust synthesized hand gestures, involving
    correction of hand rotation and translation, as well as adaptation of the algorithm
    to new images. Nevertheless, this method requires prior human knowledge to adapt
    the algorithm to new images, leading to limited robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Co-speech Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The milestones of CoS generation in recent years are presented in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4 Talking Head Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The upper part is related datasets and the lower part is the algorithm. The target
    of CoS gesture generation is to generate a sequence of body movements based on
    the corresponding audio input. It has been widely used in virtual character animation,
    especially in virtual speech and advertising. We divided it into three stages
    based on performance and popularity, Which are rule/statistical-based methods,
    DL-based methods, and Diffusion-based methods. In Table [V](#S4.T5 "TABLE V ‣
    4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition ‣ A Survey
    on Deep Multi-modal Learning for Body Language Recognition and Generation"), we
    present a summary of the details of the related SL generation works.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Feature Extraction. In the CoS gesture generation task, the data
    of different modalities such as text and speech contain semantic and rhythmic
    information. How to extract and fuse these features to get a better representation
    is an important topic. [[131](#bib.bib131)] uses a tri-modal encoder to encode
    text, speech, and person IDs separately, and then perform feature fusion, sampling
    from the fused feature space to complete the generation task. [[237](#bib.bib237)]
    separately models speech and text information. Instead of directly fusing at the
    feature level, it establishes two pipelines to model the dynamic and semantic
    information of the gesture motion, so as to generate accurate and rhythmic gesture
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Model. Numerous endeavors have been made in the process of choosing
    the generative model for CoS gesture generation task. In the early research, rule-based
    approaches [[238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240)] were used,
    which required the manual construction of a gesture library and the development
    of rules mapping from spoken language to gestures in the library. These methods
    had limited flexibility and required expert knowledge, but it is easier to be
    interpreted and were effective at handling semantic gestures. Then, statistical-based
    methods[[241](#bib.bib241)] replaced the manually written rules with traditional
    statistical models (e.g., HMMs) trained on a dataset but still required the high-cost
    manual construction of a gesture library. In recent years, DL-based end-to-end
    approaches [[25](#bib.bib25), [242](#bib.bib242)] have been developed, which use
    raw “speech-gesture” datasets such as Trinity and TED [[67](#bib.bib67), [68](#bib.bib68)]
    to train deep neural networks for end-to-end gesture generation. These methods
    have reduced system complexity and produced more natural and fluid gestures, but
    they cannot guarantee the accuracy of generated rhythmic and semantic gestures.
    Meanwhile, most CoS research works do not consider the generation of the whole
    body, which also limits its expressiveness. Recently, diffusion models [[243](#bib.bib243)]
    have emerged as powerful deep generative models. Zhu et al. [[244](#bib.bib244)]
    introduced a novel diffusion-based framework called DiffGesture, which effectively
    captures the associations between audio and gestures and maintains temporal coherence
    to generate high-quality CoS gestures. However, the diffusion-based method has
    limitations in terms of training cost and the need for multiple steps to achieve
    satisfactory results, which hinders its real-time application in CoS gesture generation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Talking Head Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TH generation has become an emerging research topic in recent years. As shown
    in Figure [12](#S5.F12 "Figure 12 ‣ 5.4 Talking Head Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), talking face generation from an audio clip or dynamic
    TH generation from a target image and an audio clip are two fundamental research
    problems. The problems’ solutions are essential to enabling a wide range of practical
    applications: (a) Entertainment: Generating virtual characters with realistic
    expressions and voice output can be applied to virtual reality games, special
    effects in movies, and other fields, to enhance user experience; (b) Virtual assistants:
    Generating virtual assistants with natural language voice and facial expressions
    can be used in customer service, robot assistants, and other scenarios to improve
    natural language interaction experience; (c) Human-machine interaction: Generating
    virtual characters with realistic expressions and voice output can be used for
    virtual meetings, remote education, and other scenarios to improve human-machine
    interaction effectiveness. (d) Healthcare: Generating virtual doctors with natural
    speak voices and facial expressions can be used in telemedicine, psychotherapy,
    and other scenarios to improve service quality and user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17199def72011828923f2c26c1b54878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The two basic problems of speech-to-face generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Speech-to-face Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a strong connection between speech and face attributes, such as age,
    gender, and the shape of the mouth, which directly affect the mechanics of speech
    generation [[245](#bib.bib245)]. Additionally, properties of speech such as language,
    accent, speed, and pronunciation are frequently shared among various nationalities
    and cultures. These properties can consequently manifest as standard physical
    facial features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Extraction. For speech and face feature extraction, Oh et al. [[246](#bib.bib246)]
    employs a trained face recognition network [[247](#bib.bib247)] to obtain face
    embedding, and a voice encoder that takes a complex spectrogram of speech as input
    and output speech features. Duarte et al. [[248](#bib.bib248)] design a speech
    encoder modified from SEGAN discriminator [[249](#bib.bib249)] to learn audio
    embedding. Similarly, Wen et al. [[228](#bib.bib228)] develops a voice embedding
    network consisting of six convolution layers to learn speech features. A voice
    encoder included voice activity detection and V-net is used in Fang et al. [[250](#bib.bib250)]
    to output audio embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Generation Model. Oh et al. [[246](#bib.bib246)] employ a pre-trained face decoder
    [[251](#bib.bib251)] to reconstruct the face image. Motivated by the success of
    GAN [[252](#bib.bib252)] in generation images with high quality. Duarte et al.
    [[248](#bib.bib248)] developed a conditional GAN called WavPix that is able to
    generate face images directly from the speech. For better identity matching, Wen
    et al. [[228](#bib.bib228)] introduced the second discriminator to verify the
    identity of face image output. Considering that emotional expression is a key
    face attribute of a realistic face image, Fang et al. [[250](#bib.bib250)] applied
    two classifiers to measure identity and emotion semantic relevance in generating.
    In [[253](#bib.bib253)], a Face-based Residual Personalized Speech Synthesis Model
    (FR-PSS) containing a speech encoder, a speech synthesizer and a face encoder
    is designed for PSS.
  prefs: []
  type: TYPE_NORMAL
- en: 'These aforementioned methods can generate face images from speech, however,
    the authenticity and accuracy of the reconstructed face image still need to be
    improved: (a) Explicit cross-modal correlation learning is vital for identity
    information preservation, which is not explored in the previous methods. (b) The
    face images synthesized by the GAN-based or CNN-based generator lack details and
    authenticity.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations Metrics. For speech-to-face generation methods, identity information
    preservation is the key factor, therefore, quantitative metrics related to identity
    consistency are used to evaluate the performance, which includes landmark distance,
    feature distance, and face attributes evaluation. Landmark distance is to calculate
    the distance of landmark (LMD) of generated face image and true face, where the
    landmark is achieved by Dlib [[254](#bib.bib254)] pre-trained DL methods such
    as FaceNet [[229](#bib.bib229)]. Feature distances are Cosine, $L_{2}$, and $L_{1}$
    distances calculated between the feature of the true face and generated face.
    The face attributes are generally evaluated by attribution recognition accuracy
    like gender recognition, identity recognition, and face retrieval. The quality
    of generated face images is also important for speech-to-face generation, Fréchet
    Inception Distance (FID), and Inception score (IS) are two common metrics to evaluate
    performance. Those abovementioned metrics are highlighted in Table [VII](#S5.T7
    "TABLE VII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5
    Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Summary of quantitative metrics of Speech-to-face generation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics’ degree | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Identity preservation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LDM [[246](#bib.bib246)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cosine, $L_{2}$, $L_{1}$ [[246](#bib.bib246)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face retrieval [[246](#bib.bib246)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Identity recognition [[228](#bib.bib228)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gender classification [[228](#bib.bib228)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image quality | IS [[250](#bib.bib250)], FID [[250](#bib.bib250)] |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Summary of recent studies related to Talking Head generation. The
    following aspects are concluded: the network architecture for image synthesis
    and driving source; the methods work for a specific target or arbitrary identity;
    the audio feature is synchronized with lip motions or not; the ability to generate
    personalized attributes, and if any intermediate face models are used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Methods | Year | Driving source | Target | Audio features | Personalized
    | Face model |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | Chen et al. [[255](#bib.bib255)] | 2018 | Audio | Arbitrary | Sync
    | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[256](#bib.bib256)] | 2019 | Audio | Arbitrary | Sync | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[257](#bib.bib257)] | 2019 | Audio | Arbitrary | Sync | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| ATVG [[258](#bib.bib258)] | 2019 | Audio | Arbitrary | not sync | No | 2D
    landmarks |'
  prefs: []
  type: TYPE_TB
- en: '| Vougioukas et al. [[259](#bib.bib259)] | 2019 | Audio | Arbitrary | Sync
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye blinks, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eyebrow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| Kefalas et al . [[260](#bib.bib260)] | 2020 | Audio | Arbitrary | No sync
    | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Sinha et al. [[261](#bib.bib261)] | 2020 | Audio | Arbitrary | No Sync |
    Eye blinking | No |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[262](#bib.bib262)] | 2020 | Audio | Arbitrary | Sync | Head
    pose | 2D landmark |'
  prefs: []
  type: TYPE_TB
- en: '| Wav2lip [[30](#bib.bib30)] | 2020 | Audio | Arbitrary | Sync | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Eskimez et al. [[263](#bib.bib263)] | 2020 | Audio | Arbitrary | Sync | No
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Yi et al. [[264](#bib.bib264)] | 2020 | Video | Specific | Not sync | Head
    pose | 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[265](#bib.bib265)] | 2020 | Video | Arbitrary | Not sync |
    Head pose | 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Mittal et al. [[266](#bib.bib266)] | 2021 | Audio | Arbitrary | Not sync
    | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| MEAD [[111](#bib.bib111)] | 2020 | Audio | Arbitrary | Not sync | Emotion
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[267](#bib.bib267)] | 2021 | Audio | Arbitrary | No sync | No
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| FACIAL [[268](#bib.bib268)] | 2021 | Video | Arbitrary | Not sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eye blinking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[112](#bib.bib112)] | 2021 | Audio | Arbitrary | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eyebrow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Si et al. [[269](#bib.bib269)] | 2021 | Audio | Arbitrary | No sync | Emotion
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[270](#bib.bib270)] | 2021 | Audio | Arbitrary | Sync | No |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| PC-AVS [[271](#bib.bib271)] | 2021 | Video | Arbitrary | Sync | Head pose
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| GC-VAT [[272](#bib.bib272)] | 2022 | Video | Arbitrary | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; expression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[273](#bib.bib273)] | 2022 | Audio | Arbitrary | Sync | Head
    pose | No |'
  prefs: []
  type: TYPE_TB
- en: '| EAMM [[274](#bib.bib274)] | 2022 | Video | Arbitrary | No sync | Emotion
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| SPACE [[275](#bib.bib275)] | 2022 | Audio | Arbitrary | No sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; emotion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2D landmark |'
  prefs: []
  type: TYPE_TB
- en: '| DIRFA [[276](#bib.bib276)] | 2023 | Audio | Arbitrary | Sync | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| DisCoHead [[115](#bib.bib115)] | 2023 | Video | Arbitrary | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eye blinking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eyebrow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| OPT [[277](#bib.bib277)] | 2023 | Audio | Arbitrary | No sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; expression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[278](#bib.bib278)] | 2023 | Audio | Abitrary | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; expression,gaze, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eye blinking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[279](#bib.bib279)] | 2023 | Audio | Abitrary | No sync | No
    | No |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: Summary of recent studies related to Talking Head generation. The
    following aspects are concluded: The network architecture for image synthesis;
    Driving source; The methods work for a specific target or arbitrary identity;
    The audio feature is synchronized with lip motions or not; The ability to generate
    personalized attributes, and if any intermediate face models are used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Methods | Year | Driving source | Target | Audio features | Personalized
    | Face model |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | X2Face [[280](#bib.bib280)] | 2018 | Audio, video | Arbitrary | Sync
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; expression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| Jamaludin et al. [[281](#bib.bib281)] | 2019 | Audio | Arbitrary | Sync |
    No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Wen et al. [[282](#bib.bib282)] | 2020 | Video, audio | Arbitrary | No sync
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; expression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| LipSync3D [[283](#bib.bib283)] | 2021 | Video | Specific | No sync | No |
    3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| Audio2head [[31](#bib.bib31)] | 2021 | Audio | Arbitrary | Sync | Head pose
    | 2D landmark |'
  prefs: []
  type: TYPE_TB
- en: '| Lu et al. [[284](#bib.bib284)] | 2021 | Audio | Specific | No sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eyebrow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | Bigioi et al. [[285](#bib.bib285)] | 2022 | Video, audio | Arbitrary
    | No sync | Head pose | 2D landmark. |'
  prefs: []
  type: TYPE_TB
- en: '| VAE | SadTalker [[286](#bib.bib286)] | 2023 | Audio | Abitrary | No sync
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head pose, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; eye blinking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| NeRF | AD-NeRF [[32](#bib.bib32)] | 2021 | Audio | Specific | No sync | No
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| DFA-NERF [[287](#bib.bib287)] | 2022 | Video | specific | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye blinking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; head pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| No |'
  prefs: []
  type: TYPE_TB
- en: '| DFRF [[288](#bib.bib288)] | 2022 | Audio | Arbitrary | No sync | No | 3DMM
    |'
  prefs: []
  type: TYPE_TB
- en: '| SSP-NeRF [[289](#bib.bib289)] | 2022 | Video | Arbitrary | No sync | No |
    3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| DM | Yu et al. [[290](#bib.bib290)] | 2022 | Audio | Arbitrary | Sync | Facial
    motion | No |'
  prefs: []
  type: TYPE_TB
- en: '| Zhua et al. [[291](#bib.bib291)] | 2023 | Video | Arbitrary | Sync |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye blinking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; head pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DMM |'
  prefs: []
  type: TYPE_TB
- en: '| DiffTalk [[33](#bib.bib33)] | 2023 | Audio | Arbitrary | Sync | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[292](#bib.bib292)] | 2023 | Audio, text | Arbitrary | No sync
    | Emotion | 3DMM |'
  prefs: []
  type: TYPE_TB
- en: 5.4.2 Talking Head Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a target face image and a speech clip, TH generation aims at synthesizing
    a sequence of target face images where the lip motion, head pose, and facial expressions
    are synchronized with the audio. Significantly different from the speech-to-face
    generation task, which extracts the identity of the speaker from the given speech,
    the TH generation task focuses on the content of the speech.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Feature Extraction. A VGG-M network pre-trained on the VGG Face
    dataset [[293](#bib.bib293)] is employed in [[281](#bib.bib281)] to learn face
    features and a speech encoder modified from VGG-M is used to learn speech embedding.
    Three temporal encoders are used to extract representations of the speaker’s identity,
    the audio segment, and the facial expressions, and a polynomial fusion layer is
    designed to generate a joint representation of the three encodings [[260](#bib.bib260)].
    Differently, Mittal et al. [[266](#bib.bib266)] develop a VAE to disentangle the
    phonetic content, emotional tone, and other factors into different representations
    solely from the input audio signal. To effectively disentangle each motion factor
    and achieve fine-grained controllable TH generation, Wang et al. [[278](#bib.bib278)]
    propose a progressive disentangled representation strategy by separating the factors
    in a coarse-to-fine manner, where we first extract unified motion feature from
    the driving signal, and then isolate each fine-grained motion from the unified
    feature. A pre-trained audio-to-AU module is employed in [[270](#bib.bib270)]
    to extract the speech-related AU information from speech.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal Learning. For TH video generation, speech-synchronized lip movement,
    facial expressions, and head pose generation are key factors. Therefore, in the
    training stage, audio-visual cross-modal correlation learning is necessary for
    the consistency of these facial movements in a sequence. Chen et al. [[255](#bib.bib255)]
    propose an audio-visual correlation loss to synchronize lip changes and speech
    changes in a video regarding that variation along the temporal axis between two
    modalities are more likely correlated, specifically, the cosine similarity loss
    is used to maximize the correlation between the derivative of audio feature and
    visual variations. For joint audio-visual representation learning, Zhou et al.
    [[257](#bib.bib257)] enforces the audio features and visual features to share
    a classifier so that they can share the same distribution, additionally, a contrastive
    loss is employed to close the paired audio and visual features. Eskimez et al.
    [[263](#bib.bib263)] designs a pair discriminator to improve the synchronization
    between the mouth shape and the input speech in the generated video. Zhu et al.
    [[267](#bib.bib267)] introduces the theory of mutual information neural estimation
    in talking face generation task to learn the cross-modal coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Generation Model. The development of DL-based methods including CNN, RNN, GAN,
    Variational Autoencoder (VAE), Neural Radiance Fields (NeRF), and diffusion model
    (DM) have been explored in recent years. We compare the difference among them
    in Table [VIII](#S5.T8 "TABLE VIII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking
    Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation") and Table [IX](#S5.T9
    "TABLE IX ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5
    Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").
  prefs: []
  type: TYPE_NORMAL
- en: The GAN-based methods are the mainstream for TH generation, in particular, because
    of their ability to synthesize data before the stronger generator DM emerged.
    In Table [VIII](#S5.T8 "TABLE VIII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking
    Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"), we briefly list the recent
    works related to TH generation based on the GAN framework. Chen et al. [[255](#bib.bib255)]
    proposes a three-stream GAN to generate speech-synchronized lip video. Wang et
    al. [[262](#bib.bib262)] uses the GAN base network with an attentional mechanism
    to identify features related to head information. Zhang et al. [[268](#bib.bib268)]
    designs a FACIAL-GAN to encoder explicit and implicit attribute information for
    talking face video generation with audio-synchronized lip motion, personalized
    and natural head motion, and realistic eye blinks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to GAN-based approaches, inspired by the NeRF [[294](#bib.bib294)],
    Guo et al. [[32](#bib.bib32)] develops the audio-driven NeRF (AD-NeRF) model for
    TH synthesis, in which an implicit neural scene representation function is learned
    to map audio features to dynamic neural radiation fields for speaker face rendering.
    However, AD-NeRF often suffers from head and torso separation during the rendering
    stage. Therefore, a semantic-aware speaker portrait NeRF (SSP-NeRF) is proposed
    by Liu et al. [[289](#bib.bib289)]. They employ the semantic awareness of speech
    to address the problem of incongruity between local dynamics and global torso.
    The problem of slow rendering speed can not be ignored. To improve the real-time
    performance, Yao et al. [[287](#bib.bib287)] proposes a NeRF method that takes
    lip movement features and personalized attributes as two disentangled conditions,
    where lip movements are directly predicted from the audio inputs to achieve lip-synchronized
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Probabilistic Models (DM) have shown strong ability in various generation
    tasks [[295](#bib.bib295), [296](#bib.bib296)]. Zhua et al. [[291](#bib.bib291)]
    proposes an audio-driven diffusion model for TH video generation, in which the
    lip motion features are aligned with the TH by contrastive learning. Yu et al.
    [[290](#bib.bib290)] proposes audio-to-visual diffusion prior trained on top of
    the mapping between audio and disentangled non-lip facial representations to semantically
    match the input audio while still maintaining both the photo-realism of audio-lip
    synchronization and the overall naturalness. Shen et al. [[33](#bib.bib33)] employs
    the emerging powerful diffusion models and model the TH generation as an audio-driven
    temporally coherent denoising process (DiffTalk). Xu et al. [[292](#bib.bib292)]
    first represents the emotion in the text prompt, which could inherit rich semantics
    from the CLIP, allowing flexible and generalized emotion control.
  prefs: []
  type: TYPE_NORMAL
- en: 'For better facial appearance transfer, intermediate faces such as 2D landmarks
    or 3DMM are widely used in TH generation. Figure. [13](#S5.F13 "Figure 13 ‣ 5.4.2
    Talking Head Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic Body Language
    Generation ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") illustrates a simplified pipeline of the TH generation methods
    based on intermediate face, which mainly consists of two steps: low-dimensional
    driving source data are mapped into facial parameters; then rendering network
    is used to convert the learned facial parameters into high-dimensional video output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6838197f3fa0e06978982ea581322746.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The typical pipeline of TH generation methods based on the intermediate
    face.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics. Various perspectives reveal that the generated text-to-speech
    (TTS) output lacks the authenticity of human speech: (a) The target individual’s
    face should match that of the synthetic video’s speaker, (b) The generated speaker’s
    mouth should synchronize the audio, (c) The produced TH video should be of a good
    caliber, (d) The expression of the speaker in the generated video should be natural
    and match the emotion of the audio, and (e) Eye blinking should be expected when
    talking. Thus, the quantitative metrics of TH generation can be classified from
    these five views, as shown in Table [X](#S5.T10 "TABLE X ‣ 5.4.2 Talking Head
    Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic Body Language Generation
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Summary of quantitative metrics of Talking Head Generation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics’ degree | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Identity-preserving |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR, SSIM [[297](#bib.bib297)], FID, LMD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LPIPS [[298](#bib.bib298)], CSIM, IS, ACD [[299](#bib.bib299)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Audio-visual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; synchronization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AV Conf, AV Off [[300](#bib.bib300)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WER [[259](#bib.bib259)], LMD[m] [[272](#bib.bib272)], [[299](#bib.bib299)],
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sync[conf] [[272](#bib.bib272)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LRSD, LRA [[98](#bib.bib98)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image quality preserving | CPBD [[301](#bib.bib301)], FDBM [[302](#bib.bib302)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Expression | Classification accuracy [[303](#bib.bib303)] |'
  prefs: []
  type: TYPE_TB
- en: '| Eye blinking |'
  prefs: []
  type: TYPE_TB
- en: '&#124; EAR [[304](#bib.bib304)], Blink rate, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Blink median duration [[268](#bib.bib268)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Audio Input Pre-processing. Most of the TH generation works are audio signal
    driven. Here, we will introduce how previous work has dealt with speech signals
    in this field. In general, the audio waveform is resampled at 16KHz, and then
    the audio feature is computed [[305](#bib.bib305)]. Spectrogram, MFCC, and Fbank
    are the three mostly used audio features. Fang et al. [[250](#bib.bib250)] performs
    an ablation experiment on these three audio features, and they found that Fbank
    achieved the best performance, while the Spectrogram performed the worst FID.
    The reasons they guessed that Spectrogram contained much redundant information,
    MFCC discarded some related information, and Fbank kept balance. However, MFCC
    is used the most in the talking face generation.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Challenges of BL Recognition and Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The existing BL recognition and generation methods have not been capable of
    meeting real-world requirements under exposure to various challenges. In order
    to fully demonstrate the typical challenges of BL recognition and generation in
    the field of BL, we elaborate in detail on SL, CS, and TH from three aspects:
    subtasks challenges, datasets challenges and evaluation metrics challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Subtasks Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To more fully illustrate the challenges of BL recognition and generation tasks,
    we split each major task into three subtasks, i.e., Lip reading, SL recognition,
    and CS recognition. From the perspective of task definition, the TH task itself
    is more focused on the generation process. Moreover, limited by the development
    of the existing TH generation, it is difficult for researchers to capture the
    basic facial attributes of the target speaker. The existing studies lack an exploration
    of TH recognition, so the challenge of TH recognition is not included in the discussion
    of subtask challenges in this survey. Current research on CoS predominantly concentrates
    on CoS gesture generation. While some studies have demonstrated a positive impact
    on the CoS Generation task, the majority of recent works do not prioritize CoS
    Recognition as a primary focus. So the challenge of CoS Recognition is not included
    in the discussion of subtask challenges in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of BL recognition tasks are mainly due to the efficiency of the
    cross-modal feature fusion, and the specific challenges of each subtask are as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lip Reading. There are two primary challenges in automatic lip reading: intra-class
    difference and inter-class similarity. The former is hindered by factors such
    as speech emotion, speed, gender, age, skin color, and speech habits, making it
    difficult to distinguish variations within the same word category. Additionally,
    the semantic disparities between words used in different contexts significantly
    impact lip reading. The latter challenge stems from the abundance of word categories,
    leading to challenges in visually distinguishing similar-looking words belonging
    to different classes. Addressing these challenges is crucial for improving the
    accuracy and effectiveness of lip reading recognition systems, which have valuable
    implications for aiding communication for individuals with hearing impairments
    and advancing the field’s applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign Language Recognition. SL recognition encounters significant challenges
    arising from the pronounced variations in gestures, which seriously impede its
    accuracy. Moreover, factors like hand shape, illumination conditions, and resolution
    play pivotal roles in limiting SL recognition performance. Additionally, occlusion,
    including self-occlusion between fingers and occlusion between hands and other
    body parts, adversely affects feature fusion, becoming a key influencing factor
    in SL recognition. Another pressing challenge is the development of a real-time
    multilingual SL recognition system. Addressing these complexities is essential
    to advance the field and improve the efficiency and inclusivity of SL recognition
    technologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cued Speech Recognition. The primary obstacle in CS recognition is the hand
    preceding phenomenon [[41](#bib.bib41)], where the hand movements often occur
    faster than the corresponding lip movements, anticipating the next phoneme. This
    phenomenon hampers the efficiency of lip and hand feature fusion in CS recognition.
    Besides, due to variations in individual CS coding habits and styles, adaptability
    in multi-cuer scenarios is also a challenge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The challenge of BL generation mainly stems from the stability and quality of
    the generated gesture, and the specific challenges of each subtask are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cued Speech Generation. In conclusion, the CS generation faces several challenges
    that need to be addressed for the development of effective systems. The lack of
    large-scale annotated datasets, the complexity of modeling CS gestures, and the
    need for accurate asynchronous alignment between cued signs and spoken words are
    key challenges. Additionally, integrating audio and visual modalities and achieving
    generalization to new speakers and languages are important considerations. Overcoming
    these challenges through advancements in modeling ability, multi-modal fusion,
    and the availability of diverse datasets will contribute to the improvement of
    CS generation systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign Language Generation. In the realm of SL production, numerous obstacles
    warrant attention, chief among them being domain adaptation and model collapse.
    The former obstacle arises from the inherent variations in word styles and meanings
    across different languages, necessitating effective adaptation strategies. Furthermore,
    a noteworthy challenge lies in the limited proficiency of generating uncommon
    and unseen words, hindering the overall performance of the system. Moreover, the
    persisting issues of model collapse, non-convergence, and instability within generative
    models further compound the complexities faced in Sign Language production. Addressing
    these multifaceted challenges is crucial for advancing the SOTA in this domain
    and facilitating more reliable and robust SL generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Co-speech Generation. The generation process of CoS encounters challenges due
    to the presence of highly idiosyncratic and non-periodic spontaneous gestures.
    The accurate capture of finger motion poses difficulties, resulting in the manifestation
    of idiosyncratic gestures. Furthermore, the non-periodic nature of gestures arises
    from the substantial variation in gesture behavior.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talking Head Generation. TH generation confronts two primary challenges: information
    coupling and diversity targets. The former encompasses the synchronization of
    multiple facial elements, such as head posture, facial expression, lip movement,
    and background motion, while also addressing the “uncanny valley effect” [[306](#bib.bib306)],
    a phenomenon common in face generation where generated faces appear almost human-like
    but lack true realism, leading to discomfort. The latter challenge pertains to
    harmonizing temporal resolution and speech features across diverse data modalities,
    along with the complexity of defining visual quality as a clear training objective.
    Overcoming these challenges is crucial for advancing the field and achieving a
    more realistic and visually coherent TH generation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6.2 Datasets Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current datasets for SL recognition and generation encounter significant
    limitations due to the high costs associated with data collection and manual annotation.
    This results in datasets with small-scale and weak annotations, hindering the
    progress of BL-related tasks. To create BL datasets, collaboration between language
    experts and native speakers is essential, further adding to the complexities and
    expenses involved. A potential solution to address these challenges is to explore
    self-supervised learning using unlabeled BL data [[307](#bib.bib307)], which could
    alleviate the need for extensive manual annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, privacy protection poses another hurdle, as some large BL datasets [[308](#bib.bib308),
    [309](#bib.bib309)] are not publicly accessible. In light of the high costs and
    privacy concerns, a viable approach is to leverage existing wild online videos
    to collect the necessary BL data. Similar to the training datasets used for Contrastive
    Language-Image Pre-training (CLIP) [[310](#bib.bib310)] and DALL-E [[311](#bib.bib311)],
    employing very large datasets can enhance the generalization capabilities of BL
    recognition and generation models.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the dataset challenges, the high costs associated with collecting
    and annotating 3D data contribute to the scarcity of large-scale 3D BL datasets.
    Consequently, the development of 3D BL Generation faces significant obstacles
    in understanding and processing 3D BL data effectively. Overcoming these challenges
    is essential to advance the field of BL recognition and generation, allowing for
    more efficient and accurate communication support for individuals with hearing
    impairments.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Evaluation Metrics Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary nature of the BL recognition task lies in its classification essence,
    where simple and efficient classification accuracy serves as the prevalent evaluation
    metric. However, this paper shifts its focus to the BL generation task and the
    challenges it poses in terms of evaluation metrics. Subjective metrics utilized
    in the BL generation task prove to be costly, time-consuming and lack scalability.
    Metrics like human likeness and gesture appropriateness, although valuable, suffer
    from non-replicability and instability issues. On the other hand, objective metrics
    such as PSNR, SSIM, FID [[312](#bib.bib312)] and LRSD [[17](#bib.bib17)] offer
    advantages over subjective ones but come with limitations in assessing the similarity
    between gesture and speech, as well as the semantic appropriateness of gestures.
    Notably, unlike subjective metrics that evaluate human likeness, the existing
    literature rarely quantifies objective metrics measuring gesture diversity or
    various motion appropriateness aspects. These challenges highlight the need for
    robust and comprehensive evaluation metrics in the BL generation domain to ensure
    an accurate and meaningful assessment of generated Sign Language outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Through an extensive summary and analysis of the existing literature, this
    survey offers the following discussions and new insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The integration of large-scale multi-modal BL datasets and the establishment
    of a unified low-loss data format are key factors in advancing BL recognition
    and generation tasks. By collecting extensive datasets from diverse online videos,
    we can enhance the generalization and robustness of BL recognition and generation
    models for real-world scenarios. Additionally, the adoption of a unified data
    standard and adaptable conversion method allows for the seamless integration of
    different datasets and facilitates collaboration among researchers. This promotes
    interoperability between models, enabling efficient sharing and utilization of
    resources within the research community.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recently, large-scale pre-training models such as ChatGPT have achieved outstanding
    performance in various visual-linguistic cross-modal tasks. For instance, CLIP
    and various variations of the multi-modal CLIP model have emerged. However, they
    have the following drawbacks: a) they might not deeply connect different types
    of data as effectively as specialized models; b) they demand in terms of computing
    power due to their size. c) This model might not allow fine-tuning for specific
    tasks and could struggle with specialized knowledge; d) it needs a lot of diverse
    data to work well and could be hard to interpret. To this end, how to build a
    large-scale multi-modal model for BL recognition and generation is a promising
    topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Besides, it was found that the ability of existing large-scale pre-training
    models to learn fine-grain features still needs to be improved [[313](#bib.bib313)].
    In BL, fine-grained feature learning is essential, For example, hand positions
    and lip movements in CS and CoS needed to be accurately recognized and generated
    to ensure clarity and avoid ambiguity. Therefore, fine-grained BL recognition
    and generation is a feasible direction to improve their performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The multi-modal models in the task of BL recognition and generation are very
    susceptible to the perturbations (attacks) of different modalities, resulting
    in serious performance degradation. How to pre-train a robust and secure multimodal
    large-scale model for BL recognition and generation is an urgent problem to be
    solved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An essential requirement for BL recognition and generation systems is real-time
    capability, especially for multilingual and multiple-speakers scenarios. Creating
    a real-time system is vital to cater to the needs of both the deaf and speaking
    communities. However, existing audio-visual datasets are predominantly monolingual,
    with English being the most commonly represented language. In practical applications,
    multilingual communication is often necessary, highlighting the need for diverse
    datasets. Additionally, current methods for BL recognition and generation are
    often limited to specific target identities, as different speakers exhibit significant
    variations in appearance and habits. Overcoming these challenges is crucial to
    develop adaptable and effective real-time BL systems that accommodate various
    languages and diverse speakers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey has delved into the realm of deep multi-modal learning for automatic
    BL recognition and generation, shedding light on its potential and challenges.
    This survey focuses on four classical BL variants, i.e., Sign Language, Cued Speech,
    Co-speech, and Talking Head. Through a meticulous examination of various modalities,
    including visual, auditory, and textual data, and their integration, we have explored
    the intricacies of capturing and interpreting these four BL. By reviewing SOTA
    methodologies, such as feature fusion, representation learning, recognition, and
    generation methods, we have uncovered the strengths and limitations of current
    approaches. The significance of datasets and benchmarks in facilitating research
    progress was also emphasized, with a focus on annotation methodologies and evaluation
    metrics. Despite the progress, challenges persist, demanding the creation of diverse
    datasets, addressing limited labeled data, enhancing model interpretability, and
    ensuring robustness across environments and cultural contexts. Looking ahead,
    the future holds promises of more sophisticated architectures and training strategies,
    harnessing the complementary nature of multi-modal data and leveraging advancements
    in multi-modal learning, large-scale pre-trained model, self-supervised learning,
    and reinforcement learning. As this research area evolves, it is poised to revolutionize
    human-human and human-machine interactions, fostering natural and effective communication
    across domains.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. O. Cornett, “Cued speech,” *American annals of the deaf*, vol. 112,
    no. 1, pp. 3–13, 1967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Joksimoski, E. Zdravevski, P. Lameski, I. M. Pires, F. J. Melero, T. P.
    Martinez, N. M. Garcia, M. Mihajlov, I. Chorbev, and V. Trajkovik, “Technological
    solutions for sign language recognition: A scoping review of research trends,
    challenges, and opportunities,” *IEEE Access*, vol. 10, pp. 40 979–40 998, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. Liu, Q. Wu, H. Zhou, Y. Du, W. Wu, D. Lin, and Z. Liu, “Audio-driven
    co-speech gesture video generation,” *Advances in Neural Information Processing
    Systems (NIPS)*, vol. 35, pp. 21 386–21 399, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] B. Zhang, C. Qi, P. Zhang, B. Zhang, H. Wu, D. Chen, Q. Chen, Y. Wang,
    and F. Wen, “Metaportrait: Identity-preserving talking head generation with fast
    personalized adaptation,” in *Proc. IEEE/CVF-CVRP*, 2023, p. 22096–22105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] R. Rastgoo, K. Kiani, S. Escalera, and M. Sabokrou, “Sign language production:
    A review,” in *Proc. IEEE/CVF-CVRP*, 2021, pp. 3451–3461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Nyatsanga, T. Kucherenko, C. Ahuja, G. E. Henter, and M. Neff, “A comprehensive
    review of data-driven co-speech gesture generation,” in *Computer Graphics Forum*,
    vol. 42, no. 2.   Wiley Online Library, 2023, pp. 569–596.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] R. Rastgoo, K. Kiani, and S. Escalera, “Sign language recognition: A deep
    survey,” *Expert Systems with Applications*, vol. 164, p. 113794, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Fernandez-Lopez and F. M. Sukno, “Survey on automatic lip-reading in
    the era of deep learning,” *Image and Vision Computing*, vol. 78, pp. 53–72, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Fenghour, D. Chen, K. Guo, B. Li, and P. Xiao, “Deep learning-based
    automated lip-reading: A survey,” *IEEE Access*, vol. 9, pp. 121 184–121 205,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Chand, P. Jain, A. Mathur, S. Raj, and P. Kanikar, “Survey on visual
    speech recognition using deep learning techniques,” in *Proc. IEEE-CSCITA*, 2023,
    pp. 72–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Bhaskar, T. Thasleema, and R. Rajesh, “A survey on different visual
    speech recognition techniques,” in *Data Analytics and Learning (DAL)*, 2018,
    pp. 307–316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] N. Radha, A. Shahina *et al.*, “A survey on visual speech recognition
    approaches,” in *Proc. IEEE-ICAIS*, 2021, pp. 934–939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] O. Koller, “Quantitative survey of the state of the art in sign language
    recognition,” *arXiv preprint arXiv:2008.09918*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] I. Adeyanju, O. Bello, and M. Adegboye, “Machine learning methods for
    sign language recognition: A critical review and analysis,” *Intelligent Systems
    with Applications*, vol. 12, p. 200056, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] I. Papastratis, C. Chatzikonstantinou, D. Konstantinidis, K. Dimitropoulos,
    and P. Daras, “Artificial intelligence technologies for sign language,” *Sensors*,
    vol. 21, no. 17, p. 5843, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. M. Madhiarasan, P. Roy, and P. Pratim, “A comprehensive review of sign
    language recognition: Different types, modalities, and datasets,” *arXiv preprint
    arXiv:2204.03328*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] L. Chen, G. Cui, Z. Kou, H. Zheng, and C. Xu, “What comprises a good talking-head
    video generation?: A survey and benchmark,” *arXiv preprint arXiv:2005.03201*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Sha, W. Zhang, T. Shen, Z. Li, and T. Mei, “Deep person generation:
    A survey from the perspective of face, pose, and cloth synthesis,” *ACM Computing
    Surveys*, vol. 55, no. 12, pp. 1–37, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. Zhen, W. Song, Q. He, J. Cao, L. Shi, and J. Luo, “Human-computer interaction
    system: A survey of talking-head generation,” *Electronics*, vol. 12, no. 1, p.
    218, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. Sheng, G. Kuang, L. Bai, C. Hou, Y. Guo, X. Xu, M. Pietikäinen, and
    L. Liu, “Deep learning for visual speech analysis: A survey,” *arXiv preprint
    arXiv:2205.10839*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Stoll, N. C. Camgoz, S. Hadfield, and R. Bowden, “Text2sign: Towards
    sign language production using neural machine translation and generative adversarial
    networks,” *International Journal of Computer Vision*, vol. 128, pp. 891–908,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. Guo, W. Zhou, H. Li, and M. Wang, “Hierarchical lstm for sign language
    translation,” in *Proc. Conf AAAI Artif. Intell.*, vol. 32, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Saunders, N. C. Camgoz, and R. Bowden, “Everybody sign now: Translating
    spoken language to photo realistic sign language video,” *arXiv preprint arXiv:2011.09846*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. Ahuja, D. W. Lee, and L.-P. Morency, “Low-resource adaptation for personalized
    co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*, June 2022, pp. 20 566–20 576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Ao, Q. Gao, Y. Lou, B. Chen, and L. Liu, “Rhythmic gesticulator: Rhythm-aware
    co-speech gesture synthesis with hierarchical neural embeddings,” *ACM Transactions
    on Graphics (TOG)*, vol. 41, no. 6, p. 1–19, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Liang, Q. Feng, L. Zhu, L. Hu, P. Pan, and Y. Yang, “Seeg: Semantic
    energized co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*, June 2022,
    pp. 10 473–10 482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai,
    and B. Zhou, “Learning hierarchical cross-modal association for co-speech gesture
    generation,” in *Proc. IEEE/CVF-CVPR*, 2022, pp. 10 462–10 472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] P. Duchnowski, L. D. Braida, D. Lum, M. Sexton, J. Krause, and S. Banthia,
    “Automatic generation of cued speech for the deaf: status and outlook,” in *International
    Conference on Auditory-Visual Speech Processing (AVSP)*, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G. Bailly, Y. Fang, F. Elisei, and D. Beautemps, “Retargeting cued speech
    hand gestures for different talking heads and speakers,” in *Retargeting cued
    speech hand gestures for different talking heads and speakers*, September 2008,
    p. 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. KR, M. Rudrabha, P. Namboodir, and C. Jawahar, “A lip sync expert is
    all you need for speech to lip generation in the wild,” in *Proc. ACM MM*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Wang, L. Li, Y. Ding, C. Fan, and X. Yu, “Audio2head: Audio-driven
    one-shot talking-head generation with natural head motion,” in *Proc. IJCAI*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, and J. Zhang, “Ad-nerf:
    Audio driven neural radiance fields for talking head synthesis,” in *Proc. IEEE/CVF-ICCV*,
    2021, pp. 5784–5794.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Shen, W. Zhao, Z. Meng, W. Li, Z. Zhu, J. Zhou, and J. Lu, “Difftalk:
    Crafting diffusion models for generalized audio-driven portraits animation,” in
    *Proc. IEEE/CVF-CVPR*, 2023, pp. 1982–1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] P. Lucey, G. Potamianos, and S. Sridharan, “Patch-based analysis of visual
    speech from multiple views,” in *International Conference on Auditory-Visual Speech
    Processing (AVSP)*.   AVISA, 2008, pp. 69–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. Zhou, G. Zhao, and M. Pietikäinen, “Towards a practical lipreading
    system,” in *Proc. IEEE/CVF-CVPR*, 2011, pp. 137–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] P. Wu, H. Liu, X. Li, T. Fan, and X. Zhang, “A novel lip descriptor for
    audio-visual keyword spotting based on adaptive decision fusion,” *IEEE Transactions
    on Multimedia*, vol. 18, no. 3, pp. 326–338, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] P. Ma, S. Petridis, and M. Pantic, “End-to-end audio-visual speech recognition
    with conformers,” in *Proc. IEEE-ICASSP*, 2021, pp. 7613–7617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] L. Liu, G. Feng, D. Beautemps, and X.-P. Zhang, “A novel resynchronization
    procedure for hand-lips fusion applied to continuous french cued speech recognition,”
    in *Proc. IEEE-EUSIPCO*, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] K. Papadimitriou, M. Parelli, G. Sapountzaki, G. Pavlakos, P. Maragos,
    and G. Potamianos, “Multimodal fusion and sequence learning for cued speech recognition
    from videos,” in *International Conference on Human-Computer Interaction*, 2021,
    pp. 277–290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. Liu, G. Feng, B. Denis, and X.-P. Zhang, “Re-synchronization using
    the hand preceding model for multi-modal fusion in automatic continuous cued speech
    recognition,” *IEEE Transactions on Multimedia*, vol. 23, pp. 292–305, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. Liu and L. Liu, “Cross-modal mutual learning for cued speech recognition,”
    in *Proc. IEEE-ICASSP*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Zhang, W. Zhou, and H. Li, “A threshold-based HMM-DTW approach for
    continuous sign language recognition,” in *Proceedings of International Conference
    on Internet Multimedia Computing and Service*.   Association for Computing Machinery,
    2014, pp. 237–240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Yang, J. Tao, and Z. Ye, “Continuous sign language recognition using
    level building based on fast hidden markov model,” *Pattern Recognition Letters*,
    vol. 78, pp. 28–35, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] K. L. Cheng, Z. Yang, Q. Chen, and Y.-W. Tai, “Fully convolutional networks
    for continuous sign language recognition,” in *Proc. ECCV*, 2020, pp. 697–714.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Wei, J. Zhao, W. Zhou, and H. Li, “Semantic boundary detection with
    reinforcement learning for continuous sign language recognition,” *IEEE Transactions
    on Circuits and Systems for Video Technology*, vol. 31, no. 3, pp. 1138–1149,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L. Liu and G. Feng, “A pilot study on mandarin chinese cued speech,” *American
    Annals of the Deaf*, vol. 164, no. 4, pp. 496–518, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] “Sign languages unite us!” un.org, 2022\. [Online]. Available: [https://www.un.org/en/observances/sign-languages-day](https://www.un.org/en/observances/sign-languages-day)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Z. Pan, X. Qian, and H. Li, “Speaker extraction with co-speech gestures
    cue,” *IEEE Signal Processing Letters*, vol. 29, pp. 1467–1471, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Sondermann and M. Merkt, “Like it or learn from it: Effects of talking
    heads in educational videos,” *Computers & Education*, vol. 193, p. 104675, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] W. Song, Q. He, and G. Chen, “Virtual human talking-head generation,”
    in *Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine
    Learning*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] D. Kothadiya, C. Bhatt, K. Sapariya, K. Patel, A.-B. Gil-González, and
    J. M. Corchado, “Deepsign: Sign language detection and recognition using deep
    learning,” *Electronics*, vol. 11, no. 11, p. 1780, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. De Coster, D. Shterionov, M. Van Herreweghe, and J. Dambre, “Machine
    translation from signed to spoken languages: State of the art and challenges,”
    *Universal Access in the Information Society*, pp. 1–27, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] N. K. Kahlon and W. Singh, “Machine translation from text to sign language:
    a systematic review,” *Universal Access in the Information Society*, vol. 22,
    no. 1, pp. 1–35, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] L. Liu, G. Feng, X. Ren, and X. Ma, “Objective hand complexity comparison
    between two mandarin chinese cued speech systems,” in *Proc. IEEE-ISCSLP*, 2022,
    p. 215–219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] “Find your cued language,” cuedspeech.org. [Online]. Available: [https://cuedspeech.org/learn/find-your-cued-language/](https://cuedspeech.org/learn/find-your-cued-language/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] L. Liu, “Modeling for continuous cued speech recognition in french using
    advanced machine learning methods,” Ph.D. dissertation, Universite Grenoble Alpes,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] L. Liu, G. Feng, and D. Beautemps, “Automatic temporal segmentation of
    hand movements for hand positions recognition in french cued speech,” in *Proc.
    IEEE-ICASSP*, 2018, pp. 3061–3065.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Wang, Z. Tang, X. Li, M. Yu, Q. Fang, and L. Liu, “Cross-modal knowledge
    distillation method for automatic cued speech recognition,” in *Proc. Interspeech*,
    2021, p. 2986–2990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] L. Liu, J. Li, G. Feng, and X.-P. S. Zhang, “Automatic detection of the
    temporal segmentation of hand movements in british english cued speech.” in *Proc.
    Interspeech*, 2019, pp. 2285–2289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. J. Park, M. Kim, J. Hong, J. Choi, and Y. M. Ro, “Synctalkface: Talking
    face generation with precise lip-syncing via audio-lip memory,” in *Proc. Conf
    AAAI Artif. Intell.*, vol. 36, no. 2, 2022, pp. 2062–2070.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Liu, G. Feng, and D. Beautemps, “Inner lips parameter estimation based
    on adaptive ellipse model,” in *International Conference on Auditory-Visual Speech
    Processing (AVSP)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] ——, “Automatic dynamic template tracking of inner lips based on clnf,”
    in *Proc. IEEE-ICASSP*, 2017, p. 5130–5134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. W. Alibali, M. Bassok, K. O. Solomon, S. E. Syc, and S. Goldin-Meadow,
    “Illuminating mental representations through speech and gesture,” *Psychological
    Science*, vol. 10, no. 4, pp. 327–333, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Kang and B. Tversky, “From hands to minds: Gestures promote understanding,”
    *Cognitive Research: Principles and Implications*, vol. 1, no. 1, pp. 1–15, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Kendon, “Do gestures communicate? a review,” *Research on language
    and social interaction*, vol. 27, no. 3, pp. 175–200, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] K. Adam, *Gesture: Visible action as utterance*.   Cambridge University
    Press, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Y. Ferstl and R. McDonnell, “Investigating the use of recurrent motion
    modelling for speech gesture generation,” in *Proc. ACM IVA*, 2018, pp. 93–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Y. Yoon, W.-R. Ko, M. Jang, J. Lee, J. Kim, and G. Lee, “Robots learn
    social skills: End-to-end learning of co-speech gesture generation for humanoid
    robots,” in *Proc. IEEE-International Conference in Robotics and Automation (ICRA)*,
    2019, pp. 4303–4309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Yoon, P. Wolfert, T. Kucherenko, C. Viegas, T. Nikolov, M. Tsakov,
    and G. E. Henter, “The genea challenge 2022: A large evaluation of data-driven
    co-speech gesture generation,” in *Proc. ACM-International Conference on Multimodal
    Interaction*, 2022, pp. 736–747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik, “Learning
    individual styles of conversational gesture,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 3497–3506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] R. Poppe, “A survey on vision-based human action recognition,” *Image
    and vision computing*, vol. 28, no. 6, pp. 976–990, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao, J. Dambre, and
    J.-M. Odobez, “Deep dynamic neural networks for multimodal gesture segmentation
    and recognition,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 38, no. 8, pp. 1583–1597, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Wan, Y. Zhao, S. Zhou, I. Guyon, S. Escalera, and S. Z. Li, “Chalearn
    looking at people rgb-d isolated and continuous datasets for gesture recognition,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition
    workshops*, 2016, pp. 56–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Materzynska, G. Berger, I. Bax, and R. Memisevic, “The jester dataset:
    A large-scale video dataset of human gestures,” in *Proceedings of the IEEE/CVF
    international conference on computer vision workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] H. Zeng, X. Wang, Y. Wang, A. Wu, T.-C. Pong, and H. Qu, “Gesturelens:
    Visual analysis of gestures in presentation videos,” *IEEE Transactions on Visualization
    and Computer Graphics*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] E. Efthimiou, S.-E. Fotinea, T. Hanke, J. Glauert, R. Bowden, A. Braffort,
    C. Collet, P. Maragos, and F. Goudenove, “Dicta-sign: sign language recognition,
    generation and modelling with application in deaf communication,” in *LREC*.   European
    Language Resources Association (ELRA), 2010, pp. 80–83.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] O. Koller, J. Forster, and H. Ney, “Continuous sign language recognition:
    Towards large vocabulary statistical recognition systems handling multiple signers,”
    *Computer Vision and Image Understanding*, vol. 141, pp. 108–125, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] C. Neidle, A. Thangali, and S. Sclaroff, “Challenges in development of
    the american sign language lexicon video dataset (asllvd) corpus,” in *LREC*.   Citeseer,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] U. v. Agris and K.-F. Kraiss, “Signum database: Video corpus for signer-independent
    continuous sign language recognition,” in *LREC*.   European Language Resources
    Association (ELRA), 2010, pp. 243–246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Lin, X. Chai, Y. Zhou, and X. Chen, “Curve matching from the view of
    manifold for sign language recognition,” in *ACCV Workshops*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] N. K. Caselli, Z. S. Sehyr, A. M. Cohen-Goldberg, and K. Emmorey, “Asl-lex:
    A lexical database of american sign language,” *Behavior research methods*, vol. 49,
    pp. 784–801, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N. C. Camgoz, S. Hadfield, O. Koller, H. Ney, and R. Bowden, “Neural sign
    language translation,” in *Proc. IEEE-CVPR*, 2018, pp. 7784–7793.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Mavi and Z. Dikle, “A new 27 class sign language dataset collected
    from 173 individuals,” *arXiv preprint arXiv:2203.03859*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S.-K. Ko, C. J. Kim, H. Jung, and C. Cho, “Neural sign language translation
    based on human keypoint estimation,” *Applied sciences*, vol. 9, no. 13, p. 2683,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] N. Adaloglou, T. Chatzis, I. Papastratis, A. Stergioulas, G. T. Papadopoulos,
    V. Zacharopoulou, G. J. Xydopoulos, K. Atzakas, D. Papazachariou, and P. Daras,
    “A comprehensive study on deep learning-based methods for sign language recognition,”
    *IEEE Transactions on Multimedia*, vol. 24, pp. 1750–1762, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Z. S. Sehyr, N. Caselli, A. M. Cohen-Goldberg, and K. Emmorey, “The asl-lex
    2.0 project: A database of lexical and phonological properties for 2,723 signs
    in american sign language,” *The Journal of Deaf Studies and Deaf Education*,
    vol. 26, no. 2, pp. 263–277, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Duarte, S. Palaskar, L. Ventura, D. Ghadiyaram, K. DeHaan, F. Metze,
    J. Torres, and X. Giro-i Nieto, “How2sign: a large-scale multimodal dataset for
    continuous american sign language,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 2735–2744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] A. Kapitanov, K. Kvanchiani, A. Nagaev, and E. Petrova, “Slovo: Russian
    sign language dataset,” *arXiv preprint arXiv:2305.14527*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Al-Barham, A. Alsharkawi, M. Al-Yaman, M. Al-Fetyani, A. Elnagar, A. A.
    SaAleek, and M. Al-Odat, “Rgb arabic alphabets sign language dataset,” *arXiv
    preprint arXiv:2301.11932*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] L. Liu, H. Thomas, G. Feng, and B. Denis, “Visual recognition of continuous
    cued speech using a tandem cnn-hmm approach.” in *Proc. Interspeech*, 2018, pp.
    2643–2647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] T. A., “VOT and durational properties of selected segments in the speech
    of deaf and normally hearing children,” *Studia Phonetica Posnaniensia*, vol. 8,
    pp. 111–142, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] B. Bigi, M. Zimmermann, and C. André, “Clelfpc: a large open multi-speaker
    corpus of french cued speech,” in *LREC*, 2022, pp. 987–994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “The grid audio-visual
    speech corpus (1.0) [data set],” in *Zenodo*.   Zenodo, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The enterface’05 audio-visual
    emotion database,” in *Proc. IEEE-22nd international conference on data engineering
    workshops*, 2006, pp. 8–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. Rekik, A. Ben-Hamadou, and W. Mahdi, “An adaptive approach for lip-reading
    using image and depth data,” *Multimedia Tools and Applications*, vol. 75, pp.
    8609–8636, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma,
    “Crema-d: Crowd-sourced emotional multimodal actors dataset,” *IEEE Transactions
    on Affective Computing*, vol. 5, no. 4, pp. 377–390, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Czyzewski, B. Kostek, P. Bratoszewski, J. Kotus, and M. Szykulski,
    “An audio-visual corpus for multimodal automatic speech recognition,” *Journal
    of Intelligent Information Systems*, vol. 49, pp. 167–192, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. S. Chung and A. Zisserman, “Lip reading in the wild,” *Proc. ACCV*,
    pp. 87–103, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi, and
    E. M. Provost, “MSP-IMPROV: An acted corpus of dyadic interactions to study emotion
    perception,” *IEEE Transactions on Affective Computing*, vol. 8, no. 1, pp. 67–80,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman, “Synthesizing
    obama: learning lip sync from audio,” *ACM Transactions on Graphics (ToG)*, vol. 36,
    no. 4, pp. 1–13, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker
    identification dataset,” *Telephony*, vol. 3, pp. 33–039, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,”
    *Proc. Interspeech*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep
    audio-visual speech recognition,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 44, no. 12, pp. 8717–8727, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Afouras, J. S. Chung, and A. Zisserman, “LRS3-TED: a large-scale dataset
    for visual speech recognition,” *arXiv preprint arXiv:1809.00496*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. R. Livingstone and F. A. Russo, “The ryerson audio-visual database
    of emotional speech and song (ravdess): A dynamic, multimodal set of facial and
    vocal expressions in north american english,” *PloS one*, vol. 13, no. 5, p. e0196391,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea,
    “Meld: A multimodal multi-party dataset for emotion recognition in conversations,”
    *arXiv preprint arXiv:1810.02508*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T.
    Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: a speaker-independent
    audio-visual model for speech separation,” *ACM Transactions on Graphics (TOG)*,
    vol. 37, no. 4, pp. 1–11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, “Capture,
    learning, and synthesis of 3d speaking styles,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 10 101–10 111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan,
    and X. Chen, “LRW-1000: A naturally-distributed large-scale benchmark for lip
    reading in the wild,” in *Proceedings of 14th IEEE international conference on
    automatic face & gesture recognition*, 2019, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner,
    “Faceforensics++: Learning to detect manipulated facial images,” in *Proc. IEEE/CVF-ICCV*,
    2019, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, and
    C. C. Loy, “Mead: A large-scale audio-visual dataset for emotional talking-face
    generation,” in *Proc. ECCV*, 2020, pp. 700–717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Zhang, L. Li, Y. Ding, and C. Fan, “Flow-guided one-shot talking face
    generation with a high-resolution audio-visual dataset,” in *Proc. IEEE/CVF-CVPR*,
    2021, pp. 3661–3670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Kim, S. Park, J. Lee, S. Chung, J. Lee, and J. Choo, “AnimeCeleb:
    Large-scale animation celebheads dataset for head reenactment,” in *Proc. ECCV*,
    2022, pp. 414–430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. Berkol, T. Tümer-Sivri, N. Pervan-Akman, M. Çolak, and H. Erdem, “Visual
    lip reading dataset in turkish,” *Data*, vol. 8, no. 1, p. 15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] G. Hwang, S. Hong, S. Lee, S. Park, and G. Chae, “DisCoHead: Audio-and-video-driven
    talking head generation by disentangled control of head pose and facial expressions,”
    in *Proc. IEEE-ICASSP*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] I. Matthews, T. F. Cootes, J. A. Bangham, S. Cox, and R. Harvey, “Extraction
    of visual features for lipreading,” *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*, vol. 24, no. 2, pp. 198–213, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Petridis, J. Shen, D. Cetin, and M. Pantic, “Visual-only recognition
    of normal, whispered and silent speech,” in *Proc. IEEE-ICASSP*, 2018, pp. 6219–6223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] K. Takeuchi, S. Kubota, K. Suzuki, D. Hasegawa, and H. Sakuta, “Creating
    a gesture-speech dataset for speech-based automatic gesture generation,” *Communications
    in Computer and Information Science*, pp. 198–202, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] N. Singh, J. J. Lee, I. Grover, and C. Breazeal, “P2pstory: dataset of
    children as storytellers and listeners in peer-to-peer interactions,” in *Proceedings
    of the CHI Conference on Human Factors in Computing Systems*, 2018, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black,
    “AMASS: Archive of motion capture as surface shapes,” in *Proc. IEEE/CVF-ICCV*,
    2019, pp. 5442–5451.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, and J. Z. Wang, “ARBEE:
    Towards automated recognition of bodily expression of emotion in the wild,” *International
    journal of computer vision*, vol. 128, pp. 1–25, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Ahuja, D. W. Lee, R. Ishii, and L.-P. Morency, “No gestures left behind:
    Learning relationships between spoken language and freeform gestures,” in *Findings
    of the Association for Computational Linguistics: EMNLP*, 2020, pp. 1884–1895.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. R. Punnakkal, A. Chandrasekaran, N. Athanasiou, A. Quiros-Ramirez,
    and M. J. Black, “Babel: Bodies, action and behavior with english labels,” in
    *Proc. IEEE/CVF-CVPR*, 2021, pp. 722–731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng, “Generating
    diverse and natural 3d human motions from text,” in *Proc. IEEE/CVF-CVPR*, 2022,
    pp. 5152–5161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] H. Liu, Z. Zhu, N. Iwamoto, Y. Peng, Z. Li, Y. Zhou, E. Bozkurt, and
    B. Zheng, “Beat: A large-scale semantic and emotional multi-modal dataset for
    conversational gestures synthesis,” in *Proc. ECCV*, 2022, pp. 612–630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Wang, Y. Zhao, L. Liu, T. Xu, Q. Li, and S. Li, “Emotional talking
    head generation based on memory-sharing and attention-augmented networks,” *arXiv
    preprint arXiv:2306.03594*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Wang, Y. Zhao, H. Fan, T. Xu, Q. Li, S. Li, and L. Liu, “Memory-augmented
    contrastive learning for talking head generation,” in *Proc. IEEE-ICASSP*, 2023,
    p. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] E. Cosatto, J. Ostermann, H. P. Graf, and J. Schroeter, “Lifelike talking
    faces for interactive services,” *Proceedings of the IEEE*, vol. 91, no. 9, pp.
    1406–1429, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] O. Gambino, A. Augello, A. Caronia, G. Pilato, R. Pirrone, and S. Gaglio,
    “Virtual conversation with a real talking head,” in *Proc. IEEE-Conference on
    Human System Interactions*, 2008, pp. 263–268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. Yang and D. Ramanan, “Articulated human detection with flexible mixtures
    of parts,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 35,
    no. 12, pp. 2878–2890, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Yoon, B. Cha, J.-H. Lee, M. Jang, J. Lee, J. Kim, and G. Lee, “Speech
    gesture generation from the trimodal context of text, audio, and speaker identity,”
    *ACM Transactions on Graphics (TOG)*, vol. 39, no. 6, pp. 1–16, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] E. Asakawa, N. Kaneko, D. Hasegawa, and S. Shirakawa, “Evaluation of
    text-to-gesture generation model using convolutional neural network,” *Neural
    Networks*, vol. 151, pp. 365–375, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] P. Buehler, A. Zisserman, and M. Everingham, “Learning sign language
    by watching tv (using weakly aligned subtitles),” in *Proc. IEEE-CVPR*, 2009,
    pp. 2961–2968.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] H. Wang, X. Chai, and X. Chen, “A novel sign language recognition framework
    using hierarchical grassmann covariance matrix,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 11, pp. 2806–2814, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] T. Pfister, J. Charles, and A. Zisserman, “Large-scale learning of sign
    language by watching tv (using co-occurrences).” in *Proc. BMVC*.   British Machine
    Vision Association, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE-CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *Proc. IEEE-CVPR*, 2017, pp. 6299–6308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation
    with pseudo-3d residual networks,” in *Proc. IEEE-ICCV*, 2017, pp. 5533–5541.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Z. Qiu, T. Yao, C.-W. Ngo, X. Tian, and T. Mei, “Learning spatio-temporal
    representation with local and global diffusion,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 12 056–12 065.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] H. Hu, J. Pu, W. Zhou, and H. Li, “Collaborative multilingual continuous
    sign language recognition: A unified framework,” *IEEE Transactions on Multimedia*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] R. Cui, H. Liu, and C. Zhang, “A deep neural framework for continuous
    sign language recognition by iterative training,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 7, pp. 1880–1891, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. Pu, W. Zhou, H. Hu, and H. Li, “Boosting continuous sign language
    recognition via cross modality augmentation,” in *Proc. ACM MM*, 2020, pp. 1497–1505.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Pu, W. Zhou, and H. Li, “Iterative alignment network for continuous
    sign language recognition,” in *Proc. IEEE/CVF-CVPR*, 2019, pp. 4165–4174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Huang, W. Zhou, Q. Zhang, H. Li, and W. Li, “Video-based sign language
    recognition without temporal segmentation,” in *Proc. Conf AAAI Artif. Intell.*,
    vol. 32, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] H. Hu, W. Zhou, J. Pu, and H. Li, “Global-local enhancement network for
    nmf-aware sign language recognition,” *ACM transactions on multimedia computing,
    communications, and applications (TOMM)*, vol. 17, no. 3, pp. 1–19, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] C. Wei, W. Zhou, J. Pu, and H. Li, “Deep grammatical multi-classifier
    for continuous sign language recognition,” in *International Conference on Multimedia
    Big Data (BigMM)*, 2019, pp. 435–442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] D. Guo, S. Wang, Q. Tian, and M. Wang, “Dense temporal convolution network
    for sign language translation.” in *Proc.IJCAI*, 2019, pp. 744–750.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] H. Zhou, W. Zhou, and H. Li, “Dynamic pseudo label decoding for continuous
    sign language recognition,” in *International conference on multimedia and expo
    (ICME)*, 2019, pp. 1282–1287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. NadeemHashmi, H. Gupta, D. Mittal, K. Kumar, A. Nanda, and S. Gupta,
    “A lip reading model using cnn with batch normalization,” in *Proc. IEEE-11th
    international conference on contemporary computing (IC3)*, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] F. B. Slimane and M. Bouguessa, “Context matters: Self-attention for
    sign language recognition,” in *International Conference on Pattern Recognition
    (ICPR)*, 2021, pp. 7884–7891.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] M. Zhou, M. Ng, Z. Cai, and K. C. Cheung, “Self-attention-based fully-inception
    networks for continuous sign language recognition,” in *24th European Conference
    on Artificial Intelligence*, 2020, pp. 2832–2839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] O. Koller, N. C. Camgoz, H. Ney, and R. Bowden, “Weakly supervised learning
    with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language
    videos,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 42,
    no. 9, pp. 2306–2320, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] O. Koller, S. Zargaran, and H. Ney, “Re-sign: Re-aligned end-to-end sequence
    modelling with deep recurrent CNN-HMMs,” in *Proc. IEEE-CVPR*, 2017, pp. 4297–4305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] O. Koller, S. Zargaran, H. Ney, and R. Bowden, “Deep sign: Enabling robust
    statistical continuous sign language recognition via hybrid cnn-hmms,” *International
    Journal of Computer Vision*, vol. 126, pp. 1311–1325, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] R. Cui, H. Liu, and C. Zhang, “Recurrent convolutional neural networks
    for continuous sign language recognition by staged optimization,” in *Proc. IEEE-CVPR*,
    2017, pp. 7361–7369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Min, A. Hao, X. Chai, and X. Chen, “Visual alignment constraint for
    continuous sign language recognition,” in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 542–11 551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] N. Cihan Camgoz, S. Hadfield, O. Koller, and R. Bowden, “Subunets: End-to-end
    hand shape and continuous sign language recognition,” in *Proc. IEEE-ICCV*, 2017,
    pp. 3056–3065.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] D. Guo, W. Zhou, A. Li, H. Li, and M. Wang, “Hierarchical recurrent deep
    fusion using adaptive clip summarization for sign language translation,” *IEEE
    Transactions on Image Processing*, vol. 29, pp. 1575–1590, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Li, L. Gao, R. Han, L. Wan, and W. Feng, “Key action and joint ctc-attention
    based sign language recognition,” in *IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2020, pp. 2348–2352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Hao, Y. Min, and X. Chen, “Self-mutual distillation learning for continuous
    sign language recognition,” in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 303–11 312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] H. Zhou, W. Zhou, Y. Zhou, and H. Li, “Spatial-temporal multi-cue network
    for sign language recognition and translation,” *IEEE Transactions on Multimedia*,
    vol. 24, pp. 768–779, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] X. Pei, D. Guo, and Y. Zhao, “Continuous sign language recognition based
    on pseudo-supervised learning,” in *Proceedings of the 2nd Workshop on Multimedia
    for Accessible Human Computer Interfaces*, 2019, pp. 33–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z. Zhang, J. Pu, L. Zhuang, W. Zhou, and H. Li, “Continuous sign language
    recognition via reinforcement learning,” in *Proc. IEEE-ICIP*.   IEEE, 2019, pp.
    285–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Z. Niu and B. Mak, “Stochastic fine-grained labeling of multi-state sign
    glosses for continuous sign language recognition,” in *Proc. ECCV*, 2020, pp.
    172–186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] K. Koishybay, M. Mukushev, and A. Sandygulova, “Continuous sign language
    recognition with iterative spatiotemporal fine-tuning,” in *Proc. IEEE-ICPR*,
    2021, pp. 10 211–10 218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] I. Papastratis, K. Dimitropoulos, and P. Daras, “Continuous sign language
    recognition through a context-aware generative adversarial network,” *Sensors*,
    vol. 21, no. 7, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Y. Chen, R. Zuo, F. Wei, Y. Wu, S. Liu, and B. Mak, “Two-stream network
    for sign language recognition and translation,” *Advances in Neural Information
    Processing Systems (NIPS)*, vol. 35, pp. 17 043–17 056, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Hu, L. Gao, Z. Liu, and W. Feng, “Self-emphasizing network for continuous
    sign language recognition,” in *Proc. Conf AAAI Artif. Intell.*, vol. 37, no. 1,
    2023, pp. 854–862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. Zheng, Y. Wang, C. Tan, S. Li, G. Wang, J. Xia, Y. Chen, and S. Z.
    Li, “Cvt-slr: Contrastive visual-textual transformation for sign language recognition
    with variational alignment,” in *Proc. IEEE/CVF-CVPR*, 2023, pp. 23 141–23 150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] K. Papadimitriou and G. Potamianos, “A fully convolutional sequence learning
    approach for cued speech recognition from videos,” in *Proc. IEEE-EUSIPCO*, 2021,
    pp. 326–330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Sankar, D. Beautemps, and T. Hueber, “Multistream neural architectures
    for cued speech recognition using a pre-trained visual feature extractor and constrained
    ctc decoding,” in *Proc. IEEE-ICASSP*, 2022, pp. 8477–8481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Kipp, A. Heloir, and Q. Nguyen, “Sign language avatars: Animation
    and comprehensibility,” in *Intelligent Virtual Agents*.   Springer Berlin Heidelberg,
    2011, pp. 113–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] J. McDonald, R. Wolfe, J. Schnepp, J. Hochgesang, D. G. Jamrozik, M. Stumbo,
    L. Berke, M. Bialek, and F. Thomas, “An automated technique for real-time production
    of lifelike animations of american sign language,” *Universal Access in the Information
    Society*, vol. 15, pp. 551–566, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Gibet, F. Lefebvre-Albaret, L. Hamon, R. Brun, and A. Turki, “Interactive
    editing in french sign language dedicated to virtual signers: Requirements and
    challenges,” *Universal Access in the Information Society*, vol. 15, pp. 525–539,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Zelinka and J. Kanis, “Neural sign language synthesis: Words are our
    glosses,” in *Proc. IEEE/CVF-WACV*, March 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] N. C. Camgoz, O. Koller, S. Hadfield, and R. Bowden, “Multi-channel transformers
    for multi-articulatory sign language translation,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] B. Saunders, N. C. Camgöz, and R. Bowden, “Adversarial training for multi-channel
    sign language production,” in *The 31st British Machine Vision Virtual Conference*.   British
    Machine Vision Association, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] M. Inan, Y. Zhong, S. Hassan, L. Quandt, and M. Alikhani, “Modeling intensification
    for sign language generation: A computational approach,” in *Findings of the Association
    for Computational Linguistics*, 2022, pp. 2897–2911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] B. Saunders, N. C. Camgoz, and R. Bowden, “Signing at scale: Learning
    to co-articulate signs for large-scale photo-realistic sign language production,”
    in *Proc. IEEE/CVF-CVPR*, 2022, pp. 5141–5151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] P. Xie, Q. Zhang, Z. Li, H. Tang, Y. Du, and X. Hu, “Vector quantized
    diffusion model with codeunet for text-to-sign pose sequences generation,” *arXiv
    preprint arXiv:2208.09141*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] C.-C. Chiu, L.-P. Morency, and S. Marsella, “Predicting co-verbal gestures:
    A deep and temporal modeling approach,” in *Proc. Intelligent Virtual Agents*.   Springer
    International Publishing, 2015, pp. 152–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] S. Alexanderson, G. E. Henter, T. Kucherenko, and J. Beskow, “Style-controllable
    speech-driven gesture synthesis using normalising flows,” in *Computer Graphics
    Forum*, vol. 39, no. 2.   Wiley Online Library, 2020, pp. 487–496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] J. Li, D. Kang, W. Pei, X. Zhe, Y. Zhang, Z. He, and L. Bao, “Audio2gestures:
    Generating diverse gestures from speech audio with conditional variational autoencoders,”
    in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 293–11 302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] U. Bhattacharya, N. Rewkowski, A. Banerjee, P. Guhan, A. Bera, and D. Manocha,
    “Text2gestures: A transformer-based network for generating emotive body gestures
    for virtual agents,” in *IEEE virtual reality and 3D user interfaces (VR)*, 2021,
    pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] S. Ghorbani, Y. Ferstl, and M.-A. Carbonneau, “Exemplar-based stylized
    gesture generation from speech: An entry to the GENEA challenge 2022,” in *Proc.
    ACM-International Conference on Multimodal Interaction*, 2022, pp. 778–783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] M. Li and Y.-m. Cheung, “A novel motion based lip feature extraction
    for lip-reading,” in *Proc. IEEE-International Conference on Computational Intelligence
    and Security*, vol. 1, 2008, pp. 361–365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. Alizadeh, R. Boostani, and V. Asadpour, “Lip feature extraction and
    reduction for hmm-based visual speech recognition systems,” in *Pro. IEEE-9th
    International Conference on Signal Processing*, 2008, pp. 561–564.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] X. Ma, L. Yan, and Q. Zhong, “Lip feature extraction based on improved
    jumping-snake model,” in *Proc. IEEE-35th Chinese Control Conference (CCC)*, 2016,
    pp. 6928–6933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Y. Lan, B.-J. Theobald, and R. Harvey, “View independent computer lip-reading,”
    in *Proc. IEEE-International Conference on Multimedia and Expo*, 2012, pp. 432–437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] T. Watanabe, K. Katsurada, and Y. Kanazawa, “Lip reading from multi view
    facial images using 3D-AAM,” in *Proc. ACCV*.   Springer Verlag, 2017, pp. 303–316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] L. Liu, G. Feng, and D. Beautemps, “Inner lips feature extraction based
    on clnf with hybrid dynamic template for cued speech,” *EURASIP Journal on Image
    and Video Processing*, vol. 2017, p. 1–15, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] ——, “Extraction automatique de contour de levre a partir du modele clnf,”
    in *JEP-TALN-RECITAL 2016-conference conjointe 31e Journees d’Etudes sur la Parole,
    23e Traitement Automatique des Langues Naturelles, 18e Rencontre des Etudiants
    Chercheurs en Informatique pour le Traitement Automatique des Langues*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] J. Wang, T. Wu, S. Wang, M. Yu, Q. Fang, J. Zhang, and L. Liu, “Three-dimensional
    lip motion network for text-independent speaker recognition,” in *Proc. IEEE-ICPR*,
    2021, p. 3380–3387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] A. Garg, J. Noyola, and S. Bagadia, “Lip reading using cnn and lstm,”
    *Technical report, Stanford University, CS231 n project report*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] D. Lee, J. Lee, and K.-E. Kim, “Multi-view automatic lip-reading using
    neural network,” in *Proc. ACCV Workshop on Multi-view Lip-reading Challenges*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] I. Fung and B. Mak, “End-to-end low-resource lip-reading with maxout
    CNN and LSTM,” in *Proc. IEEE-ICASSP*, 2018, pp. 2511–2515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] K. Xu, D. Li, N. Cassimatis, and X. Wang, “LCANet: End-to-end lipreading
    with cascaded attention-CTC,” in *Proc. IEEE-FG*, 2018, pp. 548–555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] P. Wiriyathammabhum, “Spotfast networks with memory augmented lateral
    transformers for lipreading,” in *International Conference on Neural Information
    Processing*, 2020, pp. 554–561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] X. Weng and K. Kitani, “Learning spatio-temporal features with two-stream
    deep 3d cnns for lipreading,” *arXiv preprint arXiv:1905.02540*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] D. Feng, S. Yang, and S. Shan, “An efficient software for building lip
    reading models without pains,” in *Proc. IEEE-ICMEW*, 2021, pp. 1–2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] B. Xu, C. Lu, Y. Guo, and J. Wang, “Discriminative multi-modality speech
    recognition,” in *Proc. IEEE/CVF-CVPR*, 2020, pp. 14 433–14 442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] M. Luo, S. Yang, S. Shan, and X. Chen, “Pseudo-convolutional policy gradient
    for sequence-to-sequence lip-reading,” in *Proc. IEEE-FG*, 2020, pp. 273–280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] J. Gehring, Y. Miao, F. Metze, and A. Waibel, “Extracting deep bottleneck
    features using stacked auto-encoders,” in *Proc. IEEE-ICASSP*, 2013, pp. 3377–3381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata, “Audio-visual
    speech recognition using deep learning,” *Applied intelligence*, vol. 42, pp.
    722–737, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] S. Petridis and M. Pantic, “Deep complementary bottleneck features for
    visual speech recognition,” in *Proc. IEEE-ICASSP*, 2016, pp. 2304–2308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] J. Wang, N. Gu, M. Yu, X. Li, Q. Fang, and L. Liu, “An attention self-supervised
    contrastive learning based three-stage model for hand shape feature representation
    in cued speech,” *arXiv preprint arXiv:2106.14016*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] M. Wand, J. Schmidhuber, and N. T. Vu, “Investigations on end-to-end
    audiovisual fusion,” in *Proc. IEEE-ICASSP*, 2018, pp. 3041–3045.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Y. Zhang, S. Yang, J. Xiao, S. Shan, and X. Chen, “Can we read speech
    beyond the lips? rethinking roi selection for deep visual speech recognition,”
    in *Proc. IEEE-FG*, 2020, pp. 356–363.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] J. Xiao, S. Yang, Y. Zhang, S. Shan, and X. Chen, “Deformation flow based
    two-stream network for lip reading,” in *Proc. IEEE-FG*, 2020, pp. 364–370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] X. Zhao, S. Yang, S. Shan, and X. Chen, “Mutual information maximization
    for effective lip reading,” in *Proc. IEEE-FG*, 2020, pp. 420–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv preprint arXiv:1803.01271*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] B. Martinez, P. Ma, S. Petridis, and M. Pantic, “Lipreading using temporal
    convolutional networks,” in *Proc. IEEE-ICASSP*, 2020, pp. 6319–6323.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] V. Ashish, S. Noam, P. Niki, U. Jakob, J. Llion, G. A. N, K. Łukasz,
    and P. Illia, “Attention is all you need,” in *Advances in Neural Information
    Processing Systems (NIPS)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] T. Afouras, J. S. Chung, and A. Zisserman, “Deep lip reading: a comparison
    of models and an online application,” *arXiv preprint arXiv:1806.06053*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] J. Son Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip reading sentences
    in the wild,” in *Proc. IEEE-CVPR*, 2017, pp. 6447–6456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Y. Lu and H. Li, “Automatic lip-reading system based on deep convolutional
    neural network and attention-based long short-term memory,” *Applied Sciences*,
    vol. 9, no. 8, p. 1599, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] P. Zhou, W. Yang, W. Chen, Y. Wang, and J. Jia, “Modality attention for
    end-to-end audio-visual speech recognition,” in *Proc. IEEE-ICASSP*, 2019, pp.
    6565–6569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] X. Zhang, H. Gong, X. Dai, F. Yang, N. Liu, and M. Liu, “Understanding
    pictograph with facial features: End-to-end sentence-level lip reading of chinese,”
    in *Proc. Conf AAAI Artif. Intell.*, vol. 33, no. 01, 2019, pp. 9211–9218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] A. Torfi, S. M. Iranmanesh, N. Nasrabadi, and J. Dawson, “3d convolutional
    neural networks for cross audio-visual matching recognition,” *IEEE Access*, vol. 5,
    pp. 22 081–22 091, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] P. Heracleous, D. Beautemps, and N. Aboutabit, “Cued speech automatic
    recognition in normal-hearing and deaf subjects,” *Speech Communication*, vol. 52,
    no. 6, pp. 504–512, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] P. Heracleous, D. Beautemps, and N. Hagita, “Continuous phoneme recognition
    in cued speech for french,” in *Proc. IEEE-EUSIPCO*, 2012, pp. 2090–2093.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] T. Burger, A. Caplier, and S. Mancini, “Cued speech hand gestures recognition
    tool,” in *Proc. IEEE-EUSIPCO*, 2005, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] L. Gao, S. Huang, and L. Liu, “A novel interpretable and generalizable
    re-synchronization model for cued speech based on a multi-cuer corpus,” *arXiv
    preprint arXiv:2306.02596*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Y. Zhang, L. Liu, and L. Liu, “Cuing without sharing: A federated cued
    speech recognition framework via mutual knowledge distillation,” *arXiv preprint
    arXiv:2308.03432*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist
    temporal classification: labelling unsegmented sequence data with recurrent neural
    networks,” in *Proc. ICML*, 2006, pp. 369–376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] R. Boeck, K. Bergmann, and P. Jaecks, “Disposition recognition from spontaneous
    speech towards a combination with co-speech gestures,” in *Proceedings of the
    2nd International Workshop on Multimodal Analyses enabling Artificial Agents in
    Human-Machine Interaction*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] U. Bhattacharya, E. Childs, N. Rewkowski, and D. Manocha, “Speech2affectivegestures:
    Synthesizing co-speech gestures with generative adversarial affective expression
    learning,” in *Proc. ACM MM*, 2021, pp. 2027–2036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Y. Wen, B. Raj, and R. Singh, “Face reconstruction from voice using generative
    adversarial networks,” *Advances in Neural Information Processing Systems (NIPS)*,
    vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *Proc. IEEE-CVPR*, 2015, pp. 815–823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” in *Proc. IEEE/CVF-CVPR*, 2019, pp. 4690–4699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] B. Saunders, N. C. Camgoz, and R. Bowden, “Progressive transformers for
    end-to-end sign language production,” in *Proc. ECCV*, 2020, pp. 687–705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] S. Stoll, N. C. Camgöz, S. Hadfield, and R. Bowden, “Sign language production
    using neural machine translation and generative adversarial networks,” in *Proc.
    BMVC*.   British Machine Vision Association, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] N. Vasani, P. Autee, S. Kalyani, and R. Karani, “Generation of indian
    sign language by sentence processing and generative adversarial networks,” in
    *Proc. IEEE-ICISS*, 2020, pp. 1250–1255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] L. Ventura, A. Duarte, and X. Giró-i Nieto, “Can everybody sign now?
    exploring sign language video generation from 2d poses,” *arXiv preprint arXiv:2012.10941*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Q. Xiao, M. Qin, and Y. Yin, “Skeleton-based chinese sign language recognition
    and generation for bidirectional communication between deaf and hearing people,”
    *Neural networks*, vol. 125, pp. 41–55, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] E. Rothauser, “Ieee recommended practice for speech quality measurements,”
    *IEEE Transactions on Audio and Electroacoustics*, vol. 17, no. 3, pp. 225–246,
    1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] J. Kim, J. Kim, and S. Choi, “Flame: Free-form language-based motion
    synthesis & editing,” in *Proc. Conf AAAI Artif. Intell.*, vol. 37, no. 7, 2023,
    pp. 8255–8263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] J. Cassell, H. H. Vilhjálmsson, and T. Bickmore, “Beat: the behavior
    expression animation toolkit,” in *Proc. ACM SIGGRAPH*, 2001, pp. 477–486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] J. Cassell, C. Pelachaud, N. Badler, M. Steedman, B. Achorn, T. Becket,
    B. Douville, S. Prevost, and M. Stone, “Animated conversation: rule-based generation
    of facial expression, gesture & spoken intonation for multiple conversational
    agents,” in *Proc. ACM SIGGRAPH*, 1994, pp. 413–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] P. Wagner, Z. Malisz, and S. Kopp, “Gesture and speech in interaction:
    An overview,” *Speech Communication*, vol. 57, pp. 209–232, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] S. Levine, P. Krähenbühl, S. Thrun, and V. Koltun, “Gesture controllers,”
    *ACM Transactions on Graphics (TOG)*, vol. 29, no. 4, pp. 1–11, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] S. Qian, Z. Tu, Y. Zhi, W. Liu, and S. Gao, “Speech drives templates:
    Co-speech gesture synthesis with learned templates,” in *Proc. IEEE/CVF-ICCV*,
    2021, pp. 11 077–11 086.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] P. Dhariwal and A. Q. Nichol, “Diffusion models beat GANs on image synthesis,”
    in *Advances in Neural Information Processing Systems (NIPS)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] L. Zhu, X. Liu, X. Liu, R. Qian, Z. Liu, and L. Yu, “Taming diffusion
    models for audio-driven co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*,
    2023, pp. 10 544–10 553.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] H. Teager and S. Teager, “Evidence for nonlinear sound production mechanisms
    in the vocal tract,” *Speech production and speech modelling*, pp. 241–261, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik, “Speech2face: Learning the face behind a voice,” in *Proc. IEEE/CVF-CVPR*,
    2019, pp. 7539–7548.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] O. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,” in
    *Proc. BMVC*.   British Machine Vision Association, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] A. C. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador,
    E. Mohedano, K. McGuinness, J. Torres, and X. Giro-i Nieto, “Wav2pix: Speech-conditioned
    face generation using generative adversarial networks.” in *Proc. IEEE-ICASSP*,
    2019, pp. 8633–8637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] S. Pascual, A. Bonafonte, and J. Serrà, “SEGAN: Speech enhancement generative
    adversarial network,” *Proc. Interspeech*, pp. 3642–3646, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Z. Fang, Z. Liu, T. Liu, C.-C. Hung, J. Xiao, and G. Feng, “Facial expression
    gan for voice-driven face generation,” *The Visual Computer*, pp. 1–14, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and W. T. Freeman,
    “Synthesizing normalized faces from facial identity features,” in *Proc. IEEE-CVPR*,
    2017, pp. 3703–3712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] J. Wang, Z. Wang, X. Hu, X. Li, Q. Fang, and L. Liu, “Residual-guided
    personalized speech synthesis based on face image,” in *Proc. IEEE-ICASSP*, 2022,
    p. 4743–4747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] D. E. King, “Dlib-ml: A machine learning toolkit,” *The Journal of Machine
    Learning Research*, vol. 10, pp. 1755–1758, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu, “Lip movements generation
    at a glance,” in *Proc. ECCV*, 2018, pp. 520–535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Y. Song, J. Zhu, D. Li, A. Wang, and H. Qi, “Talking face generation
    by conditional recurrent adversarial network,” in *Proc. IJCAI*, 2019, pp. 919–925.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation
    by adversarially disentangled audio-visual representation,” in *Proc. Conf AAAI
    Artif. Intell.*, vol. 33, no. 01, 2019, pp. 9299–9306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] L. Chen, R. K. Maddox, Z. Duan, and C. Xu, “Hierarchical cross-modal
    talking face generation with dynamic pixel-wise loss,” in *Proc. IEEE/CVF-CVPR*,
    2019, pp. 7832–7841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven
    realistic facial animation with temporal GANs.” in *CVPR Workshops*, 2019, pp.
    37–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] T. Kefalas, K. Vougioukas, Y. Panagakis, S. Petridis, J. Kossaifi, and
    M. Pantic, “Speech-driven facial animation using polynomial fusion of features,”
    in *Proc. IEEE-ICASSP*, 2020, pp. 3487–3491.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] S. Sinha, S. Biswas, and B. Bhowmick, “Identity-preserving realistic
    talking face generation,” in *Proc. IEEE-IJCNN*, 2020, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] W. Wang, Y. Wang, J. Sun, Q. Liu, J. Liang, and T. Li, “Speech driven
    talking head generation via attentional landmarks based representation.” in *Proc.
    Interspeech*, 2020, pp. 1326–1330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] S. E. Eskimez, R. K. Maddox, C. Xu, and Z. Duan, “End-to-end generation
    of talking faces from noisy speech,” in *Proc. IEEE-ICASSP*, 2020, pp. 1948–1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] R. Yi, Z. Ye, J. Zhang, H. Bao, and Y.-J. Liu, “Audio-driven talking
    face video generation with learning-based personalized head pose,” *arXiv preprint
    arXiv:2002.10137*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] L. Chen, G. Cui, C. Liu, Z. Li, Z. Kou, Y. Xu, and C. Xu, “Talking-head
    generation with rhythmic head motion,” in *Proc. ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] G. Mittal and B. Wang, “Animating face using disentangled audio representations,”
    in *Proc. IEEE/CVF-WACV*, 2020, pp. 3290–3298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] H. Zhu, H. Huang, Y. Li, A. Zheng, and R. He, “Arbitrary talking face
    generation via attentional audio-visual coherence learning,” in *Proc. IJCAI*,
    2021, pp. 2362–2368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] C. Zhang, Y. Zhao, Y. Huang, M. Zeng, S. Ni, M. Budagavi, and X. Guo,
    “Facial: Synthesizing dynamic talking face with implicit attribute learning,”
    in *Proc. IEEE/CVF-ICCV*, 2021, pp. 3867–3876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, and J. Xiao, “Speech2video:
    Cross-modal distillation for speech to video generation,” *arXiv preprint arXiv:2107.04806*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] S. Chen, Z. Liu, J. Liu, Z. Yan, and L. Wang, “Talking head generation
    with audio and speech related facial action units,” *arXiv preprint arXiv:2110.09951*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, and Z. Liu, “Pose-controllable
    talking face generation by implicitly modularized audio-visual representation,”
    in *Proc. IEEE/CVF-CVPR*, 2021, pp. 4176–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] B. Liang, Y. Pan, Z. Guo, H. Zhou, Z. Hong, X. Han, J. Han, J. Liu, E. Ding,
    and J. Wang, “Expressive talking head generation with granular audio-visual control,”
    in *Proc. IEEE/CVF-CVPR*, 2022, pp. 3387–3396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] S. Wang, L. Li, Y. Ding, and X. Yu, “One-shot talking face generation
    from single-speaker audio-visual correlation learning,” in *Proc. Conf AAAI Artif.
    Intell.*, vol. 36, no. 3, 2022, pp. 2531–2539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] X. Ji, H. Zhou, K. Wang, Q. Wu, W. Wu, F. Xu, and X. Cao, “Eamm: One-shot
    emotional talking face via audio-based emotion-aware motion model,” in *Proc.
    ACM SIGGRAPH*, 2022, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] S. Gururani, A. Mallya, T.-C. Wang, R. Valle, and M.-Y. Liu, “Spacex:
    Speech-driven portrait animation with controllable expression,” *arXiv preprint
    arXiv:2211.09809*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] R. Wu, Y. Yu, F. Zhan, J. Zhang, X. Zhang, and S. Lu, “Audio-driven talking
    face generation with diverse yet realistic facial animations,” *arXiv preprint
    arXiv:2304.08945*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] J. Liu, X. Wang, X. Fu, Y. Chai, C. Yu, J. Dai, and J. Han, “Opt: One-shot
    pose-controllable talking head generation,” in *Proc. IEEE-ICASSP*, 2023, pp.
    1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] D. Wang, Y. Deng, Z. Yin, H.-Y. Shum, and B. Wang, “Progressive disentangled
    representation learning for fine-grained controllable talking head synthesis,”
    in *Proc. IEEE/CVF-CVPR*, 2023, pp. 17 979–17 989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] L. Zhang, Q. Chen, and Z. Liu, “Talking head generation for media interaction
    system with feature disentanglement,” in *Proc. IEEE-ICPADS*.   IEEE, 2023, pp.
    403–410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling
    face generation using images, audio, and pose codes,” in *Proc. ECCV*, 2018, pp.
    670–686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] A. Jamaludin, J. S. Chung, and A. Zisserman, “You said that?: Synthesising
    talking faces from audio,” *International Journal of Computer Vision*, vol. 127,
    pp. 1767–1779, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] X. Wen, M. Wang, C. Richardt, Z.-Y. Chen, and S.-M. Hu, “Photorealistic
    audio-driven video portraits,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 26, no. 12, pp. 3457–3466, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, and C. Bregler, “Lipsync3d:
    Data-efficient learning of personalized 3d talking faces from video using pose
    and lighting normalization,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 2755–2764.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Y. Lu, J. Chai, and X. Cao, “Live speech portraits: real-time photorealistic
    talking-head animation,” *ACM Transactions on Graphics (TOG)*, vol. 40, no. 6,
    pp. 1–17, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] D. Bigioi, H. Jordan, R. Jain, R. McDonnell, and P. Corcoran, “Pose-aware
    speech driven facial landmark animation pipeline for automated dubbing,” *IEEE
    Access*, vol. 10, pp. 133 357–133 369, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] W. Zhang, X. Cun, X. Wang, Y. Zhang, X. Shen, Y. Guo, Y. Shan, and F. Wang,
    “Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven
    single image talking face animation,” in *Proc. IEEE/CVF-CVPR*, 2023, pp. 8652–8661.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] S. Yao, R. Zhong, Y. Yan, G. Zhai, and X. Yang, “Dfa-nerf: personalized
    talking head generation via disentangled face attributes neural rendering,” *arXiv
    preprint arXiv:2201.00791*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] S. Shen, W. Li, Z. Zhu, Y. Duan, J. Zhou, and J. Lu, “Learning dynamic
    facial radiance fields for few-shot talking head synthesis,” in *Proc. ECCV*,
    2022, pp. 666–682.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] X. Liu, Y. Xu, Q. Wu, H. Zhou, W. Wu, and B. Zhou, “Semantic-aware implicit
    neural audio-driven video portrait generation,” in *Proc. ECCV*, 2022, pp. 106–125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Z. Yu, Z. Yin, D. Zhou, D. Wang, F. Wong, and B. Wang, “Talking head
    generation with probabilistic audio-to-visual diffusion priors,” *arXiv preprint
    arXiv:2212.04248*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Y. Zhua, C. Zhanga, Q. Liub, and X. Zhoub, “Audio-driven talking head
    video generation with diffusion model,” in *Proc. IEEE-ICASSP*.   IEEE, 2023,
    pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] C. Xu, S. Zhu, J. Zhu, T. Huang, J. Zhang, Y. Tai, and Y. Liu, “Multimodal-driven
    talking face generation via a unified diffusion-based generator.” *CoRR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: delving deep into convolutional nets,” in *Proc. BMVC*.   British
    Machine Vision Association, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy,
    and D. Duckworth, “Nerf in the wild: Neural radiance fields for unconstrained
    photo collections,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 7210–7219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, and Y. Ren, “Prodiff: Progressive
    fast diffusion model for high-quality text-to-speech,” in *Proc. ACM MM*, 2022,
    pp. 2595–2605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet,
    and M. Norouzi, “Palette: Image-to-image diffusion models,” in *Proc. ACM SIGGRAPH*,
    2022, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proc. IEEE-CVPR*,
    2018, pp. 586–595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decomposing
    motion and content for video generation,” in *Proc. IEEE-CVPR*, 2018, pp. 1526–1535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] J. Chung and A. Zisserman, “Out of time: automated lip sync in the wild,”
    in *Proc. ACCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] N. D. Narvekar and L. J. Karam, “A no-reference perceptual image sharpness
    metric based on a cumulative probability of blur detection,” in *Proc. IEEE-QoMEX*,
    2009, pp. 87–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] K. De and V. Masilamani, “Image sharpness measure for blurred images
    in frequency domain,” *Procedia Engineering*, vol. 64, pp. 149–158, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] D. Zeng, S. Zhao, J. Zhang, H. Liu, and K. Li, “Expression-tailored talking
    face generation with adaptive cross-modal weighting,” *Neurocomputing*, vol. 511,
    pp. 117–130, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] K. Vougioukas, S. Petridis, and M. Pantic, “Realistic speech-driven facial
    animation with GANs,” *International Journal of Computer Vision*, vol. 128, pp.
    1398–1413, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] J. Wang, J. Liu, L. Zhao, S. Wang, R. Yu, and L. Liu, “Acoustic-to-articulatory
    inversion based on speech decomposition and auxiliary feature,” in *Proc. IEEE-ICASSP*,
    2022, p. 4808–4812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] M. Mori, K. MacDorman, and N. Kageki, “The uncanny valley [from the field],”
    *IEEE Robotics and Automation Magazine*, vol. 19, pp. 98–100, 06 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] C. Sheng, M. Pietikäinen, Q. Tian, and L. Liu, “Cross-modal self-supervised
    learning for lip reading: When contrastive learning meets adversarial training,”
    in *Proc. ACM MM*, 2021, pp. 2456–2464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Y. Mroueh, E. Marcheret, and V. Goel, “Deep multimodal learning for audio-visual
    speech recognition,” in *Proc. IEEE-ICASSP*, 2015, pp. 2130–2134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu,
    H. Liao, H. Sak, K. Rao, L. Bennett *et al.*, “Large-scale visual speech recognition,”
    *Proc. Interspeech*, pp. 4135–4139, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *Proc. ICML*, 2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
    and I. Sutskever, “Zero-shot text-to-image generation,” in *Proc. ICML*, 2021,
    pp. 8821–8831.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “GANs trained by a two time-scale update rule converge to a local nash equilibrium,”
    *Proc. Advances in neural information processing systems (NIPS)*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] E. Mu, K. M. Lewis, A. V. Dalca, and J. Guttag, “Generating image-specific
    text improves fine-grained image classification,” *arXiv preprint arXiv:2307.11315*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
