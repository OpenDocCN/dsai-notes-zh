- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:53:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:53:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2107.01153] A Survey on Deep Learning Technique for Video Segmentation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2107.01153] 深度学习技术在视频分割中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.01153](https://ar5iv.labs.arxiv.org/html/2107.01153)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2107.01153](https://ar5iv.labs.arxiv.org/html/2107.01153)
- en: A Survey on Deep Learning Technique for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术在视频分割中的调查
- en: Video Segmentation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分割
- en: Tianfei Zhou, Fatih Porikli, , David J. Crandall, ,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Tianfei Zhou, Fatih Porikli, David J. Crandall
- en: 'Luc Van Gool, Wenguan Wang T. Zhou and L. Van Gool are with ETH Zurich. (Email:
    ztfei.debug@gmail.com, vangool@vision.ee.ethz.ch) F. Porikli is with the School
    of Computer Science, Australian National University. (Email: fatih.porikli@anu.edu.au)
    D. Crandall is with the Luddy School of Informatics, Computing, and Engineering,
    Indiana University. (Email: djcran@indiana.edu) W. Wang is with ReLER Lab, Australian
    Artificial Intelligence Institute, University of Technology Sydney (Email: wenguanwang.ai@gmail.com)
    Corresponding author: Wenguan Wang'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Luc Van Gool、Wenguan Wang、T. Zhou 和 L. Van Gool 来自 ETH Zurich。（电子邮件：ztfei.debug@gmail.com，vangool@vision.ee.ethz.ch）F.
    Porikli 来自澳大利亚国立大学计算机科学学院。（电子邮件：fatih.porikli@anu.edu.au）D. Crandall 来自印第安纳大学
    Luddy 信息学、计算和工程学院。（电子邮件：djcran@indiana.edu）W. Wang 来自澳大利亚人工智能研究所 ReLER 实验室、悉尼科技大学（电子邮件：wenguanwang.ai@gmail.com）通讯作者：Wenguan
    Wang
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Video segmentation—partitioning video frames into multiple segments or objects—plays
    a critical role in a broad range of practical applications, from enhancing visual
    effects in movie, to understanding scenes in autonomous driving, to creating virtual
    background in video conferencing. Recently, with the renaissance of connectionism
    in computer vision, there has been an influx of deep learning based approaches
    for video segmentation that have delivered compelling performance. In this survey,
    we comprehensively review two basic lines of research — generic object segmentation
    (of unknown categories) in videos, and video semantic segmentation — by introducing
    their respective task settings, background concepts, perceived need, development
    history, and main challenges. We also offer a detailed overview of representative
    literature on both methods and datasets. We further benchmark the reviewed methods
    on several well-known datasets. Finally, we point out open issues in this field,
    and suggest opportunities for further research. We also provide a public website
    to continuously track developments in this fast advancing field: [https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分割——将视频帧划分为多个片段或对象——在各种实际应用中起着关键作用，从增强电影中的视觉效果，到理解自动驾驶中的场景，再到在视频会议中创建虚拟背景。最近，随着计算机视觉中联结主义的复兴，出现了大量基于深度学习的视频分割方法，展现了引人注目的性能。在这项调查中，我们全面回顾了两条基本研究方向——视频中的通用对象分割（未知类别）和视频语义分割——通过介绍它们各自的任务设置、背景概念、需求、发展历史和主要挑战。我们还详细概述了这两种方法和数据集的代表性文献。我们进一步在多个著名数据集上基准测试了所回顾的方法。最后，我们指出了该领域中的开放问题，并提出了进一步研究的机会。我们还提供了一个公共网站，以持续跟踪这一快速发展的领域：[https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey)。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Video Segmentation, Video Object Segmentation, Video Semantic Segmentation,
    Deep Learning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分割、视频对象分割、视频语义分割、深度学习
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Video segmentation — identifying the key objects with some specific properties
    or semantics in a video scene — is a fundamental and challenging problem in computer
    vision, with numerous potential applications including autonomous driving, robotics,
    automated surveillance, social media, augmented reality, movie production, and
    video conferencing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分割——在视频场景中识别具有特定属性或语义的关键对象——是计算机视觉中的一个基础而具有挑战性的问题，具有众多潜在应用，包括自动驾驶、机器人技术、自动监控、社交媒体、增强现实、电影制作和视频会议。
- en: The problem has been addressed using various traditional computer vision and
    machine learning techniques, including hand-crafted features (*e.g*., histogram
    statistics, optical flow, *etc*.), heuristic prior knowledge (*e.g*., visual attention
    mechanism${}_{\!}$ [[1](#bib.bib1)], motion boundaries${}_{\!}$ [[2](#bib.bib2)],
    *etc*.), low/mid-level visual representations (*e.g*., super-voxel${}_{\!}$ [[3](#bib.bib3)],
    trajectory${}_{\!}$ [[4](#bib.bib4)], object proposal${}_{\!}$ [[5](#bib.bib5)],
    *etc*.), and classical machine learning models (*e.g*., clustering${}_{\!}$ [[6](#bib.bib6)],
    graph models${}_{\!}$ [[7](#bib.bib7)], random walks${}_{\!}$ [[8](#bib.bib8)],
    support vector machines${}_{\!}$ [[9](#bib.bib9)], random decision forests${}_{\!}$ [[10](#bib.bib10)],
    markov random fields${}_{\!}$ [[11](#bib.bib11)], conditional random fields${}_{\!}$ [[12](#bib.bib12)],
    *etc*.). Recently, deep neural networks, and Fully Convolutional Networks (FCNs)${}_{\!}$ [[13](#bib.bib13)]
    in particular, have led to remarkable advances in video segmentation. These deep
    learning-based video segmentation algorithms are significantly more accurate (and
    sometimes even more efficient) than traditional approaches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一问题已通过各种传统计算机视觉和机器学习技术得到解决，包括手工特征（*例如*，直方图统计，光流，*等*），启发式先验知识（*例如*，视觉注意机制${}_{\!}$ [[1](#bib.bib1)]，运动边界${}_{\!}$ [[2](#bib.bib2)]，*等*），低/中级视觉表示（*例如*，超体素${}_{\!}$ [[3](#bib.bib3)]，轨迹${}_{\!}$ [[4](#bib.bib4)]，对象提议${}_{\!}$ [[5](#bib.bib5)]，*等*），以及经典机器学习模型（*例如*，聚类${}_{\!}$ [[6](#bib.bib6)]，图模型${}_{\!}$ [[7](#bib.bib7)]，随机游走${}_{\!}$ [[8](#bib.bib8)]，支持向量机${}_{\!}$ [[9](#bib.bib9)]，随机决策森林${}_{\!}$ [[10](#bib.bib10)]，马尔可夫随机场${}_{\!}$ [[11](#bib.bib11)]，条件随机场${}_{\!}$ [[12](#bib.bib12)]，*等*）。近年来，深度神经网络，尤其是全卷积网络（FCNs）${}_{\!}$ [[13](#bib.bib13)]，在视频分割方面取得了显著的进展。这些基于深度学习的视频分割算法比传统方法显著更准确（有时甚至更高效）。
- en: '![Refer to caption](img/c07128cf20291102b1493d01ead0d206.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/c07128cf20291102b1493d01ead0d206.png)'
- en: 'Figure 1: Video segmentation tasks reviewed in this survey: (a) object-level
    automatic video object segmentation (object-level AVOS), (b) instance-level automatic
    video object segmentation (instance-level AVOS), (c) semi-automatic video object
    segmentation (SVOS), (d) interactive video object segmentation (IVOS), (e) language-guided
    video object segmentation (LVOS), (f) video semantic segmentation (VSS), (g) video
    instance segmentation (VIS), and (h) video panoptic segmentation (VPS).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本调查中回顾的视频分割任务：(a) 对象级自动视频对象分割（对象级 AVOS），(b) 实例级自动视频对象分割（实例级 AVOS），(c) 半自动视频对象分割（SVOS），(d)
    交互式视频对象分割（IVOS），(e) 语言引导的视频对象分割（LVOS），(f) 视频语义分割（VSS），(g) 视频实例分割（VIS），和 (h) 视频全景分割（VPS）。
- en: '![Refer to caption](img/5475b71002204b582bbf219fccfcaf2d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/5475b71002204b582bbf219fccfcaf2d.png)'
- en: <svg version="1.1" width="191.1" height="9.69" overflow="visible"><g transform="translate(0,9.69)
    scale(1,-1)"><g transform="translate(-680.78,303.03)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[1](#S1
    "1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-613.67,303.03)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2](#S2
    "2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-481.53,303.03)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3](#S3
    "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-386.74,315.48)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1](#S3.SS1
    "3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-387.44,290.58)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2](#S3.SS2
    "3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-291.27,302.34)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[4](#S4
    "4 Video Segmentation Datasets ‣ A Survey on Deep Learning Technique for Video
    Segmentation")</foreignobject></g></text></g><g transform="translate(-190.95,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[5](#S5
    "5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-107.24,302.34)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[6](#S6
    "6 Future Research Directions ‣ A Survey on Deep Learning Technique for Video
    Segmentation")</foreignobject></g></text></g><g transform="translate(-48.43,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[7](#S7
    "7 CONCLUSION ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-620.59,141.14)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.1](#S2.SS1
    "2.1 Problem Formulation and Taxonomy ‣ 2 Background ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-624.74,109.31)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.2](#S2.SS2
    "2.2 History and Terminology ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-637.89,77.49)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.3](#S2.SS3
    "2.3 Related Research Areas ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-488.45,224.16)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.1](#S3.SS1.SSS1
    "3.1.1 Automatic Video Object Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS
    Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-471.15,124.53)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.2](#S3.SS1.SSS2
    "3.1.2 Semi-automatic Video Object Segmentation (SVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-488.45,59.5)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.3](#S3.SS1.SSS3
    "3.1.3 Interactive Video Object Segmentation (IVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-471.15,17.99)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.4](#S3.SS1.SSS4
    "3.1.4 Language-guided Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-186.11,135.6)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.1](#S3.SS2.SSS1
    "3.2.1 (Instance-agnostic) Video Semantic Segmentation (VSS) ‣ 3.2 Deep Learning-based
    VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-187.49,69.19)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.2](#S3.SS2.SSS2
    "3.2.2 Video Instance Segmentation (VIS) ‣ 3.2 Deep Learning-based VSS Models
    ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-186.11,8.3)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.3](#S3.SS2.SSS3
    "3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep Learning-based VSS Models
    ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g></g></svg>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="191.1" height="9.69" overflow="visible"><g transform="translate(0,9.69)
    scale(1,-1)"><g transform="translate(-680.78,303.03)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[1](#S1
    "1 引言 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-613.67,303.03)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2](#S2
    "2 背景 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-481.53,303.03)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3](#S3
    "3 基于深度学习的视频分割 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-386.74,315.48)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1](#S3.SS1
    "3.1 基于深度学习的视频目标分割模型 ‣ 3 基于深度学习的视频分割 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g
    transform="translate(-387.44,290.58)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2](#S3.SS2
    "3.2 基于深度学习的视频语义分割模型 ‣ 3 基于深度学习的视频分割 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g
    transform="translate(-291.27,302.34)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[4](#S4
    "4 视频分割数据集 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-190.95,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[5](#S5
    "5 性能比较 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-107.24,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[6](#S6
    "6 未来研究方向 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-48.43,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[7](#S7
    "7 结论 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-620.59,141.14)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.1](#S2.SS1
    "2.1 问题表述和分类 ‣ 2 背景 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-624.74,109.31)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.2](#S2.SS2
    "2.2 历史和术语 ‣ 2 背景 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-637.89,77.49)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.3](#S2.SS3
    "2.3 相关研究领域 ‣ 2 背景 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g transform="translate(-488.45,224.16)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.1](#S3.SS1.SSS1
    "3.1.1 自动视频目标分割 (AVOS) ‣ 3.1 基于深度学习的视频目标分割模型 ‣ 3 基于深度学习的视频分割 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g
    transform="translate(-471.15,124.53)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.2](#S3.SS1.SSS2
    "3.1.2 半自动视频目标分割 (SVOS) ‣ 3.1 基于深度学习的视频目标分割模型 ‣ 3 基于深度学习的视频分割 ‣ 深度学习技术在视频分割中的调查")</foreignobject></g></text></g><g
    transform="translate(-488.45,59.5)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013
- en: 'Figure 2: Overview of this survey.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查的概述。
- en: With the rapid advance of this field, there is a huge body of new literature
    being produced. However, most existing surveys predate the modern deep learning
    era${}_{\!}$ [[14](#bib.bib14), [15](#bib.bib15)], and often take a narrow view,
    such as focusing only on video foreground/background segmentation${}_{\!}$ [[16](#bib.bib16),
    [17](#bib.bib17)]. In this paper, we offer a state-of-the-art review that addresses
    the wide area${}_{\!}$ of${}_{\!}$ video${}_{\!}$ segmentation,${}_{\!}$ especially${}_{\!}$
    to${}_{\!}$ help${}_{\!}$ new${}_{\!}$ researchers${}_{\!}$ enter this rapidly-developing
    field. We systematically introduce recent advances in video segmentation, spanning
    from task formulation to taxonomy, from algorithms to datasets, and from unsolved
    issues to future research directions. We cover crucial aspects including task
    categories (*i.e*., foreground/background separation vs semantic segmentation),
    inference modes (*i.e*., automatic, semi-automatic, and interactive), and learning
    paradigms (*i.e*., supervised, unsupervised, and weakly supervised), and we try
    to clarify terminology (*e.g*., background subtraction, motion segmentation, *etc*.).
    We hope that this survey helps accelerate progress in this field.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一领域的快速发展，大量新文献不断涌现。然而，大多数现有的调查研究早于现代深度学习时代${}_{\!}$ [[14](#bib.bib14), [15](#bib.bib15)]，并且通常视角狭窄，例如仅关注视频前景/背景分割${}_{\!}$ [[16](#bib.bib16),
    [17](#bib.bib17)]。在本文中，我们提供了一份**最先进**的综述，涵盖了视频${}_{\!}$分割的广泛领域${}_{\!}$，特别是为了帮助新${}_{\!}$研究者${}_{\!}$进入这个快速发展的领域。我们系统地介绍了视频分割的最新进展，从任务制定到分类法，从算法到数据集，从未解决的问题到未来的研究方向。我们涵盖了关键方面，包括任务类别（*即*，前景/背景分离与语义分割）、推理模式（*即*，自动、半自动和交互式）以及学习范式（*即*，监督、无监督和弱监督），并且尝试澄清术语（*例如*，背景减除、运动分割，*等等*）。我们希望这项调查能加速这一领域的进步。
- en: 'This survey mainly focuses on recent progress in two major branches of video
    segmentation, namely video object segmentation (Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(a-e))
    and video semantic segmentation (Fig.${}_{\!}$ [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning Technique for Video Segmentation")(f-h)),${}_{\!}$
    which${}_{\!}$ are${}_{\!}$ further${}_{\!}$ divided${}_{\!}$ into${}_{\!}$ eight${}_{\!}$
    sub-fields. Even after restricting our focus to deep learning-based video segmentation,
    there are still hundreds of papers in this fast-growing field. We select influential
    work published in prestigious journals and conferences. We also include some non-deep
    learning video segmentation models and relevant literature in other areas, *e.g*.,
    visual tracking, to give necessary background. Moreover, in order to promote the
    development of this field, we provide an accompanying webpage which catalogs algorithms
    and datasets addressing video segmentation: [https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查主要集中在视频分割的两个主要分支的近期进展，即视频目标分割（图[1](#S1.F1 "图1 ‣ 1 引言 ‣ 视频分割的深度学习技术综述")(a-e)）和视频语义分割（图${}_{\!}$ [1](#S1.F1
    "图1 ‣ 1 引言 ‣ 视频分割的深度学习技术综述")(f-h)），${}_{\!}$这两个分支进一步细分为八个子领域。即使将焦点限制在基于深度学习的视频分割上，这一快速发展的领域仍有数百篇论文。我们选择了在权威期刊和会议上发表的有影响力的工作。我们还包括了一些非深度学习的视频分割模型和其他相关领域的文献，*例如*，视觉跟踪，以提供必要的背景。此外，为了促进这一领域的发展，我们提供了一个附属网页，
    catalog 算法和数据集，解决视频分割问题：[https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey)。
- en: Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique
    for Video Segmentation") shows the structure of this survey. Section §[2](#S2
    "2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation") gives
    some brief background on taxonomy, terminology, study history, and related research
    areas. We review representative papers on deep learning algorithms and video segmentation
    datasets in §[3](#S3 "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep
    Learning Technique for Video Segmentation") and §[4](#S4 "4 Video Segmentation
    Datasets ‣ A Survey on Deep Learning Technique for Video Segmentation"), respectively.
    Section §[5](#S5 "5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation") conducts performance evaluation and analysis, while §[6](#S6
    "6 Future Research Directions ‣ A Survey on Deep Learning Technique for Video
    Segmentation") raises open questions and directions. Finally, we make concluding
    remarks in §[7](#S7 "7 CONCLUSION ‣ A Survey on Deep Learning Technique for Video
    Segmentation").
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique
    for Video Segmentation")显示了本调查的结构。第§[2](#S2 "2 Background ‣ A Survey on Deep Learning
    Technique for Video Segmentation")节提供了分类、术语、研究历史和相关研究领域的一些简要背景。我们在§[3](#S3 "3
    Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for
    Video Segmentation")和§[4](#S4 "4 Video Segmentation Datasets ‣ A Survey on Deep
    Learning Technique for Video Segmentation")中分别回顾了关于深度学习算法和视频分割数据集的代表性论文。第§[5](#S5
    "5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")节进行性能评估和分析，而第§[6](#S6
    "6 Future Research Directions ‣ A Survey on Deep Learning Technique for Video
    Segmentation")节提出了开放性问题和研究方向。最后，我们在第§[7](#S7 "7 CONCLUSION ‣ A Survey on Deep
    Learning Technique for Video Segmentation")节做出结论性总结。
- en: 2 Background
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: In this section, we first formalize the task, categorize research directions,
    and discuss key challenges and driving factors in §[2.1](#S2.SS1 "2.1 Problem
    Formulation and Taxonomy ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation"). Then, §[2.2](#S2.SS2 "2.2 History and Terminology ‣
    2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation") offers
    a brief historical background covering early work and foundations, and §[2.3](#S2.SS3
    "2.3 Related Research Areas ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation") establishes linkages with relevant fields.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先形式化任务，分类研究方向，并讨论§[2.1](#S2.SS1 "2.1 Problem Formulation and Taxonomy
    ‣ 2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")中的关键挑战和驱动因素。接着，§[2.2](#S2.SS2
    "2.2 History and Terminology ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation") 提供了涵盖早期工作和基础的简要历史背景，而§[2.3](#S2.SS3 "2.3 Related Research
    Areas ‣ 2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")则建立了与相关领域的联系。
- en: 2.1 Problem Formulation and Taxonomy
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题表述与分类
- en: Formally, let $\mathcal{X}$ and $\mathcal{Y}$ denote the input space and output
    segmentation space, respectively. Deep learning-based video segmentation solutions
    generally seek to learn an ideal video-to-segment mapping $f^{*\!}:\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，设 $\mathcal{X}$ 和 $\mathcal{Y}$ 分别表示输入空间和输出分割空间。基于深度学习的视频分割解决方案通常寻求学习一个理想的视频到分割的映射
    $f^{*\!}:\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$。
- en: 2.1.1 Video Segmentation Category
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 视频分割类别
- en: 'According to how the output space $\mathcal{Y}$ is defined, video segmentation
    can be broadly categorized into two classes: video object (foreground/background)
    segmentation, and video semantic segmentation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出空间 $\mathcal{Y}$ 的定义，视频分割大致可以分为两类：视频目标（前景/背景）分割和视频语义分割。
- en: $\bullet$ Video Foreground/Background Segmentation (Video Object Segmentation,
    VOS). VOS is the classic video segmentation setting and refers to segmenting dominant
    objects (of unknown categories). In this case, $\bm{\mathcal{Y}}$ is a binary,
    foreground/background segmentation space. VOS is typically used in video analysis
    and editing application scenarios, such as object removal in movie editing, content-based
    video coding, and virtual background creation in video conferencing. It typically
    is not concerned with the exact semantic categories of the segmented objects.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频前景/背景分割（视频目标分割，VOS）。VOS 是经典的视频分割设置，指的是对主要对象（类别未知）进行分割。在这种情况下，$\bm{\mathcal{Y}}$
    是一个二进制的前景/背景分割空间。VOS 通常用于视频分析和编辑应用场景，如电影编辑中的物体移除、基于内容的视频编码和视频会议中的虚拟背景创建。它通常不关心被分割对象的确切语义类别。
- en: $\bullet$ Video Semantic Segmentation (VSS). As a direct extension of image
    semantic segmentation to the spatio-temporal domain, VSS aims to extract objects
    within predefined semantic categories (*e.g*., car, building, pedestrian, road)
    from videos. Thus, $\bm{\mathcal{Y}}$ corresponds to a multi-class, semantic parsing
    space. VSS serves as a perception foundation for many application fields, such
    as robot sensing, human-machine interaction, and autonomous driving, which require
    high-level understanding of the physical environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频语义分割（VSS）。作为图像语义分割在时空领域的直接扩展，VSS 旨在从视频中提取预定义语义类别（*例如*，汽车、建筑、人行道、道路）中的物体。因此，$\bm{\mathcal{Y}}$
    对应于多类别的语义解析空间。VSS 作为许多应用领域的感知基础，如机器人感知、人机交互和自动驾驶，这些领域需要对物理环境有高级的理解。
- en: Remark. VOS and VSS share some common challenges, such as fast motion and object
    occlusion. However, due to differences in application scenarios, many challenges
    are different. For instance, VOS often focuses on human created media, which often
    have large camera motion, deformation, and appearance changes. VSS instead often
    focuses on applications like autonomous driving, which requires a good trade off
    between accuracy and latency, accurate detection of small objects, model parallelization,
    and cross-domain generalization ability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。VOS 和 VSS 面临一些共同的挑战，如快速运动和物体遮挡。然而，由于应用场景的不同，许多挑战也是不同的。例如，VOS 通常关注人工创建的媒体，这些媒体通常有大的摄像机运动、形变和外观变化。而
    VSS 则通常关注诸如自动驾驶等应用，这些应用需要在准确性和延迟之间做出良好的权衡，精确检测小物体，模型并行化，以及跨领域的泛化能力。
- en: 2.1.2 Inference Modes for Video Segmentation
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 视频分割的推理模式
- en: 'VOS methods can be further classified into three types: automatic, semi-automatic,
    and interactive, according to how much human intervention is involved during inference.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: VOS 方法可以进一步分为三类：自动、半自动和交互式，具体取决于推理过程中涉及的人工干预程度。
- en: $\bullet$ Automatic Video Object Segmentation (AVOS). AVOS, or unsupervised
    video segmentation or zero-shot video segmentation, performs VOS in an automatic
    manner, without any manual initialization (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning Technique for Video Segmentation")(a-b)). The input
    space $\bm{\mathcal{X}}$ refers to the video domain $\bm{\mathcal{V}}$ only. AVOS
    is suitable for video analysis but not for video editing that requires segmenting
    arbitrary objects or their parts flexibly; a typical application is virtual background
    creation in video conferencing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 自动视频对象分割（AVOS）。AVOS，或无监督视频分割或零样本视频分割，以自动方式执行 VOS，无需任何手动初始化（图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 视频分割深度学习技术调查")(a-b)）。输入空间 $\bm{\mathcal{X}}$ 仅指视频领域 $\bm{\mathcal{V}}$。AVOS
    适用于视频分析，但不适用于需要灵活分割任意物体或其部分的视频编辑；一个典型的应用是视频会议中的虚拟背景创建。
- en: $\bullet$ Semi-automatic Video Object Segmentation (SVOS). SVOS, also known
    as semi-supervised video segmentation or one-shot video segmentation [[18](#bib.bib18)],
    involves limited human inspection (typically provided in the first frame) to specify
    the desired objects (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on
    Deep Learning Technique for Video Segmentation")(c)). For SVOS, $\bm{\mathcal{X}}_{\!}\!=_{\!}\!\bm{\mathcal{V}}\!\times\!\bm{\mathcal{M}}$,
    where $\bm{\mathcal{V}}_{\!}$ indicates the video space and $\bm{\mathcal{M}}$
    refers to human input. Typically the human input is an object mask in the first
    video frame, in which case SVOS is also called pixel-wise tracking or mask propagation.
    Other forms of human input include bounding boxes and scribbles [[8](#bib.bib8)].
    From this perspective, language-guided video object segmentation (LVOS) is a sub-branch
    of SVOS, in which the human input is given as linguistic descriptions about the
    desired objects (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Learning Technique for Video Segmentation")(e)). Compared to AVOS, SVOS is more
    flexible in defining target objects, but requires human input. SVOS is typically
    applied in a user-friendly setting (without specialized equipment), such as video
    content creation in mobile phones. One of the core challenges in SVOS is how to
    fully utilize target information from limited human intervention.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 半自动视频对象分割（SVOS）。SVOS，也称为半监督视频分割或单次视频分割 [[18](#bib.bib18)]，涉及有限的人类检查（通常提供在第一帧）以指定所需对象（图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 关于视频分割的深度学习技术综述")(c)）。对于 SVOS，$\bm{\mathcal{X}}_{\!}\!=_{\!}\!\bm{\mathcal{V}}\!\times\!\bm{\mathcal{M}}$，其中
    $\bm{\mathcal{V}}_{\!}$ 表示视频空间，$\bm{\mathcal{M}}$ 指代人为输入。通常，人为输入是第一帧中的对象掩码，此时
    SVOS 也称为像素级跟踪或掩码传播。其他形式的人为输入包括边界框和涂鸦 [[8](#bib.bib8)]。从这个角度来看，语言引导的视频对象分割（LVOS）是
    SVOS 的一个子分支，其中人为输入以对所需对象的语言描述给出（图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 关于视频分割的深度学习技术综述")(e)）。与
    AVOS 相比，SVOS 在定义目标对象方面更具灵活性，但需要人为输入。SVOS 通常应用于用户友好的环境（没有专业设备），如手机视频内容创作。SVOS 的核心挑战之一是如何充分利用来自有限人工干预的目标信息。
- en: '$\bullet$ Interactive Video Object Segmentation (IVOS). SVOS models are designed
    to operate automatically once the target has been identified, while systems for
    IVOS incorporate user guidance throughout the analysis process (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(d)).
    IVOS can obtain high-quality segments and works well for computer-generated imagery
    and video post-production, where tedious human supervision is possible. IVOS is
    also studied in the graphics community as video cutout. The input space $\bm{\mathcal{X}}$
    for IVOS is $\bm{\mathcal{V}}\!\times\!\bm{\mathcal{S}}$, where $\bm{\mathcal{S}}$
    typically refers to human scribbling. Key challenges include: 1) allowing users
    to easily specify segmentation constraints; 2) incorporating human specified constraints
    into the segmentation algorithm; and 3) giving quick response to the constraints.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 交互式视频对象分割（IVOS）。SVOS 模型设计为在目标确定后自动运行，而 IVOS 系统在整个分析过程中结合用户指导（图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 关于视频分割的深度学习技术综述")(d)）。IVOS 可以获得高质量的分割效果，并且在计算机生成的图像和视频后期制作中表现良好，其中可能进行繁琐的人类监督。IVOS
    在图形学社区中也作为视频剪裁进行研究。IVOS 的输入空间 $\bm{\mathcal{X}}$ 是 $\bm{\mathcal{V}}\!\times\!\bm{\mathcal{S}}$，其中
    $\bm{\mathcal{S}}$ 通常指人为涂鸦。主要挑战包括：1）允许用户轻松指定分割约束；2）将人类指定的约束融入分割算法；3）对约束做出快速响应。
- en: In contrast to VOS, VSS methods typically work in an automatic mode (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(f-h)),
    *i.e*., $\bm{\mathcal{X}}\!\equiv\!\bm{\mathcal{V}}$. Only a few early methods
    address the semi-automatic setting, called label propagation [[19](#bib.bib19)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于 VOS，VSS 方法通常以自动模式运行（图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 关于视频分割的深度学习技术综述")(f-h)），*即*，$\bm{\mathcal{X}}\!\equiv\!\bm{\mathcal{V}}$。只有少数早期方法涉及半自动设置，称为标签传播
    [[19](#bib.bib19)]。
- en: Remark. The terms “unsupervised” and “semi-supervised” are conventionally used
    in VOS to specify the amount of human interaction involved during inference. But
    they are easily confused with “unsupervised${}_{\!}$ learning”${}_{\!}$ and${}_{\!}$
    “semi-supervised${}_{\!}$ learning.”${}_{\!}$ We${}_{\!}$ urge${}_{\!}$ the${}_{\!}$
    community${}_{\!}$ to${}_{\!}$ replace${}_{\!}$ these${}_{\!}$ ambiguous${}_{\!}$
    terms with “automatic” and “semi-automatic.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。术语“无监督”和“半监督”通常用于 VOS 中以指定推断过程中涉及的人类互动量。但这些术语容易与“无监督${}_{\!}$学习”${}_{\!}$和${}_{\!}$“半监督${}_{\!}$学习”${}_{\!}$混淆。我们${}_{\!}$敦促${}_{\!}$社区${}_{\!}$用“自动化”和“半自动化”替换这些模糊的术语。
- en: 2.1.3 Learning Paradigms for Video Segmentation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 视频分割的学习范式
- en: 'Deep learning-based video segmentation models can be grouped into three categories
    according to the learning strategy they use to approximate $f^{*}$: supervised,
    unsupervised, and weakly supervised.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的视频分割模型可以根据用于逼近 $f^{*}$ 的学习策略分为三类：监督、无监督和弱监督。
- en: $\bullet$ Supervised Learning Methods. Modern video segmentation models are
    typically learned in a fully supervised manner, requiring $N$ input training samples
    and their desired outputs $y_{n}\!\!:=\!f^{*\!}(x_{n})$, where $\{(x_{n},y_{n})\}_{n\!}\!\subset$$\mathcal{X}$$\times$$\mathcal{Y}$.
    The standard method for evaluating learning outcomes follows an empirical risk/loss
    minimization formulation:¹¹1We omit the regularization term for brevity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 监督学习方法。现代视频分割模型通常采用完全监督的方式进行学习，需要 $N$ 个输入训练样本及其期望输出 $y_{n}\!\!:=\!f^{*\!}(x_{n})$，其中
    $\{(x_{n},y_{n})\}_{n\!}\!\subset$$\mathcal{X}$$\times$$\mathcal{Y}$。评估学习结果的标准方法遵循经验风险/损失最小化公式¹¹1为了简洁，我们省略了正则化项。
- en: '|  | $\small\tilde{f}\in\mathop{\arg\min}_{f\in\bm{\mathcal{F}}}\frac{1}{N}\sum\nolimits_{n}\varepsilon(f(x_{n}),z(x_{n})),\vspace{-3pt}$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\tilde{f}\in\mathop{\arg\min}_{f\in\bm{\mathcal{F}}}\frac{1}{N}\sum\nolimits_{n}\varepsilon(f(x_{n}),z(x_{n})),\vspace{-3pt}$
    |  |'
- en: where $\mathcal{F}$ denotes the hypothesis (solution) space, and $\varepsilon_{\!\!}:$
    $\bm{\mathcal{X}}_{\!}\times\bm{\mathcal{Y}}_{\!}\mapsto\!\mathbb{R}$ is an error
    function that evaluates the estimate $f(x_{n})$ against video segmentation related
    prior knowledge $z(x_{n})\!\!\in$$\mathcal{Z}$. To make $\tilde{f}$ a good approximation
    of $f^{*\!}$, current supervised video segmentation methods directly use the desired
    output $y_{n}$, *i.e*., $z(x_{n})\!\!:=\!\!f^{*\!}(x_{n})$, as the prior knowledge,
    with${}_{\!}$ the${}_{\!}$ price${}_{\!}$ of${}_{\!}$ requiring${}_{\!}$ vast${}_{\!}$
    amounts${}_{\!}$ of${}_{\!}$ well-labeled${}_{\!}$ data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}$ 表示假设（解决方案）空间，$\varepsilon_{\!\!}:$ $\bm{\mathcal{X}}_{\!}\times\bm{\mathcal{Y}}_{\!}\mapsto\!\mathbb{R}$
    是一个误差函数，用于评估估计 $f(x_{n})$ 相对于视频分割相关的先验知识 $z(x_{n})\!\!\in$$\mathcal{Z}$。为了使 $\tilde{f}$
    成为 $f^{*\!}$ 的良好逼近，目前的监督视频分割方法直接使用期望输出 $y_{n}$，*即*，$z(x_{n})\!\!:=\!\!f^{*\!}(x_{n})$，作为先验知识，代价是需要大量的标注良好的数据。
- en: $\bullet$ Unsupervised (Self-supervised) Learning Methods. When only data samples
    $\{x_{n}\}_{n\!\!}\!\subset$$\mathcal{X}$ are given, the problem of approximating
    $f^{*\!}$ is known as unsupervised learning. Unsupervised learning includes fully
    unsupervised learning methods in which the methods do not need any labels at all,
    as well as self-supervised learning methods in which networks are explicitly trained
    with automatically-generated pseudo labels without any human annotations [[20](#bib.bib20)].
    Almost all existing unsupervised learning-based video segmentation models are
    self-supervised learning methods, where the prior knowledge $\mathcal{Z}$ refers
    to pseudo labels derived from intrinsic properties of video data (*e.g*., cross-frame
    consistency). We thus use “unsupervised learning” and “self-supervised learning”
    interchangeably.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 无监督（自监督）学习方法。当仅给出数据样本 $\{x_{n}\}_{n\!\!}\!\subset$$\mathcal{X}$ 时，逼近
    $f^{*\!}$ 的问题称为无监督学习。无监督学习包括完全无监督学习方法，其中方法完全不需要标签，以及自监督学习方法，其中网络通过自动生成的伪标签进行明确训练，而无需人工注释
    [[20](#bib.bib20)]。几乎所有现有的基于无监督学习的视频分割模型都是自监督学习方法，其中先验知识 $\mathcal{Z}$ 指的是从视频数据的内在属性中获得的伪标签（*例如*，跨帧一致性）。因此，我们将“无监督学习”和“自监督学习”互换使用。
- en: $\bullet$ Weakly-Supervised Learning Methods. In this case, $\mathcal{Z}$ is
    typically a more easily-annotated domain, such as tags, bounding boxes, or scribbles,
    and $f^{*}$ is approximated using a finite number of samples from $\mathcal{X}$$\times$$\mathcal{Z}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 弱监督学习方法。在这种情况下，$\mathcal{Z}$通常是一个更易于标注的领域，如标签、边界框或涂鸦，$f^{*}$通过从$\mathcal{X}$$\times$$\mathcal{Z}$中获取有限数量的样本来进行近似。
- en: Remark. So far, deep supervised learning-based methods are dominant in the field
    of video segmentation. However, exploring the task in an unsupervised or weakly
    supervised setting is more appealing, not only because it alleviates the annotation
    burden of $\mathcal{Y}$, but because it inspires an in-depth${}_{\!}$ understanding${}_{\!}$
    of${}_{\!}$ the${}_{\!}$ nature${}_{\!}$ of${}_{\!}$ the${}_{\!}$ task${}_{\!}$
    by${}_{\!}$ exploring${}_{\!}$ $\mathcal{Z}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。迄今为止，基于深度监督学习的方法在视频分割领域占据主导地位。然而，在无监督或弱监督环境下探索这一任务更具吸引力，这不仅因为它减轻了$\mathcal{Y}$的标注负担，还因为它通过探索$\mathcal{Z}$激发了对任务本质的深入理解。
- en: 2.2 History and Terminology
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 历史与术语
- en: Digital image segmentation has been studied for at least 50 years, starting
    with the Roberts operator [[21](#bib.bib21)] for identifying object boundaries.
    Since then, numerous algorithms for image segmentation have been proposed, and
    many are extended to the video domain. The field of video segmentation has evolved
    quickly and undergone great change.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像分割研究已至少有50年的历史，最初使用Roberts算子[[21](#bib.bib21)]来识别物体边界。从那时起，提出了许多图像分割算法，许多算法也扩展到了视频领域。视频分割领域发展迅速，经历了巨大的变化。
- en: Earlier attempts focus on video over-segmentation, *i.e*., partitioning a video
    into space-time homogeneous, perceptually distinct-regions. Typical approaches
    include hierarchical video segmentation [[7](#bib.bib7)], temporal superpixel [[22](#bib.bib22)],
    and super-voxels [[3](#bib.bib3)], based on the discontinuity and similarity of
    pixel intensities in a particular location, *i.e*., separating pixels according
    to abrupt changes in intensity or grouping pixels with similar intensity together.
    These methods are instructive for early stage video preprocessing, but cannot
    solve the problem of object-level pattern modeling, as they do not provide any
    principled approach to flatten the hierarchical video decomposition into a binary
    segmentation [[9](#bib.bib9), [2](#bib.bib2)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的尝试集中在视频过度分割，即将视频分割为时空同质、感知上不同的区域。典型的方法包括基于像素强度在特定位置的不连续性和相似性的分层视频分割[[7](#bib.bib7)]、时间超像素[[22](#bib.bib22)]和超体素[[3](#bib.bib3)]，即根据强度的急剧变化分离像素或将具有相似强度的像素分组。这些方法对于早期的视频预处理很有指导意义，但无法解决对象级模式建模的问题，因为它们没有提供任何有原则的方法来将分层视频分解展平为二进制分割[[9](#bib.bib9),
    [2](#bib.bib2)]。
- en: To extract foreground objects from video sequences, background subtraction techniques
    emerged beginning in the late 70s [[23](#bib.bib23)], and became popular following
    the work of [[24](#bib.bib24)]. They assume that the background is known a priori,
    and that the camera is stationary​ [[25](#bib.bib25), [26](#bib.bib26)] or undergoes
    a predictable, parametric 2D​ [[27](#bib.bib27)] or 3D motion with 3D parallax​ [[28](#bib.bib28)].
    These geometry-based methods fit well for specific application scenarios such
    as surveillance systems [[26](#bib.bib26), [9](#bib.bib9)], but they are sensitive
    to model selection (2D or 3D), and cannot handle non-rigid camera movements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从视频序列中提取前景对象，背景减除技术从70年代末开始出现，并在[[24](#bib.bib24)]的工作后变得流行。这些技术假设背景是已知的，并且摄像机是静止的[[25](#bib.bib25),
    [26](#bib.bib26)]，或者经历可预测的参数化二维[[27](#bib.bib27)]或三维运动，具有三维视差[[28](#bib.bib28)]。这些基于几何的方法适用于特定的应用场景，例如监控系统[[26](#bib.bib26),
    [9](#bib.bib9)]，但对模型选择（二维或三维）很敏感，无法处理非刚性摄像机运动。
- en: Another group of video segmentation solutions tackled the task of motion segmentation,
    *i.e*., finding objects in motion. Background subtraction can also be viewed as
    a specific case of motion segmentation. However, most motion segmentation models
    are built upon motion analysis [[29](#bib.bib29), [30](#bib.bib30)], factorization​ [[31](#bib.bib31)],
    and/or statistical​ [[32](#bib.bib32)] techniques that comprehensively model the
    characteristics of moving scenes without prior knowledge of camera motion. Among
    the big family of motion segmentation algorithms, trajectory segmentation attained
    particular attention [[4](#bib.bib4), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)]. Trajectories are generated through tracking points over multiple
    frames and can represent long-term motion patterns, serving as an informative
    cue for segmentation. Though impressive, motion-based methods heavily rely on
    the accuracy of optical flow estimation and can fail when different parts of an
    object exhibit heterogeneous motions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组视频分割解决方案处理了运动分割任务，即**寻找运动中的物体**。背景减除也可以视为运动分割的一个特定案例。然而，大多数运动分割模型建立在运动分析[[29](#bib.bib29),
    [30](#bib.bib30)]、因式分解[[31](#bib.bib31)]和/或统计[[32](#bib.bib32)]技术上，这些技术综合建模了移动场景的特征，而无需先验的摄像机运动知识。在众多运动分割算法中，**轨迹分割**受到特别关注[[4](#bib.bib4),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]。轨迹是通过跟踪多个帧中的点生成的，并且可以表示长期运动模式，作为分割的有用线索。尽管令人印象深刻，基于运动的方法严重依赖光流估计的准确性，并且当物体的不同部分表现出异质运动时可能会失败。
- en: To overcome these limitations, the task of extracting generic objects from unconstrained
    video sequences, *i.e*., AVOS, has drawn increasing research interest [[37](#bib.bib37)].
    Several methods [[5](#bib.bib5), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]
    explored object hypotheses or proposals [[41](#bib.bib41)] as middle-level object
    representations. They generate a large number of object candidates in every frame
    and cast the task of segmenting video objects as an object region selection problem.
    The main drawbacks of the proposal-based algorithms are the high computational
    cost [[17](#bib.bib17)] and complicated object inference schemes. Some others
    explored heuristic hypotheses such as visual attention [[1](#bib.bib1)] and motion
    boundary [[2](#bib.bib2)], but easily fail in scenarios where the heuristic assumptions
    do not hold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，从不受约束的视频序列中提取通用物体的任务，即**AVOS**，引起了越来越多的研究兴趣[[37](#bib.bib37)]。一些方法[[5](#bib.bib5),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]探索了物体假设或提议[[41](#bib.bib41)]作为中级物体表示。它们在每一帧中生成大量物体候选，并将视频物体的分割任务视为物体区域选择问题。基于提议的算法的主要缺点是高计算成本[[17](#bib.bib17)]和复杂的物体推理方案。其他一些方法探索了启发式假设，如视觉注意力[[1](#bib.bib1)]和运动边界[[2](#bib.bib2)]，但在启发式假设不成立的场景中容易失败。
- en: 'As argued earlier, an alternative to the above unattended solutions is to incorporate
    human-marked initialization, *i.e*., SVOS. Older SVOS methods often rely on optical
    flow [[42](#bib.bib42), [8](#bib.bib8), [43](#bib.bib43), [44](#bib.bib44)] and
    share a similar spirit with object tracking [[45](#bib.bib45), [46](#bib.bib46)].
    In addition, some pioneering IVOS methods were proposed to address high-quality
    video segmentation under extensive human guidance, including rotoscoping [[47](#bib.bib47),
    [48](#bib.bib48)], scribble [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [8](#bib.bib8)], contour [[53](#bib.bib53)], and points [[54](#bib.bib54)].
    Significant engineering is typically needed to allow IVOS systems to operate at
    interactive speeds. In short, SVOS and IVOS pay for the improved flexibility and
    accuracy: they are infeasible at large scale due to their human-in-the-loop nature.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，上述无人值守解决方案的替代方案是引入人工标记初始化，即**SVOS**。较早的**SVOS**方法通常依赖光流[[42](#bib.bib42),
    [8](#bib.bib8), [43](#bib.bib43), [44](#bib.bib44)]，并与物体跟踪[[45](#bib.bib45), [46](#bib.bib46)]有类似的精神。此外，一些开创性的**IVOS**方法被提出以解决在广泛人工指导下的高质量视频分割问题，包括**rotoscoping**[[47](#bib.bib47),
    [48](#bib.bib48)]、**scribble**[[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [8](#bib.bib8)]、**contour**[[53](#bib.bib53)]和**points**[[54](#bib.bib54)]。通常需要大量工程工作来使**IVOS**系统以交互速度运行。简而言之，**SVOS**和**IVOS**为了提高灵活性和准确性而付出了代价：由于其人机交互的特性，它们在大规模应用中不可行。
- en: In the pre-deep learning era, relatively few papers${}_{\!}$ [[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [12](#bib.bib12)] considered
    VSS due to the complexity of the task. The approaches typically relied on supervised
    classifiers such as SVMs and video over-segmentation techniques.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代之前，由于任务的复杂性，考虑VSS的论文相对较少${}_{\!}$ [[55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [12](#bib.bib12)]。这些方法通常依赖于监督分类器，如支持向量机（SVMs）和视频过分割技术。
- en: Overall, traditional approaches for video segmentation, though giving interesting
    results, are constrained by hand-crafted features and heavy engineering. But deep
    learning brought the performance of video segmentation to a new level, as we will
    review in §[3](#S3 "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep
    Learning Technique for Video Segmentation").
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，虽然传统的视频分割方法提供了有趣的结果，但它们受到手工特征和繁重工程的限制。但是，深度学习将视频分割的性能提升到了一个新水平，正如我们将在§[3](#S3
    "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")中回顾的那样。
- en: 2.3 Related Research Areas
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 相关研究领域
- en: There are several research fields closely related to video segmentation, which
    we now briefly describe.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个研究领域与视频分割密切相关，我们现在简要描述一下。
- en: $\bullet$ Visual Tracking. To infer the location of a target object over time,
    current tracking methods usually assume that the target is determined by a bounding
    box in the first frame [[59](#bib.bib59)]. However, in more general tracking scenarios,
    and in particular the cases studied in early tracking methods, diverse object
    representations are explored [[60](#bib.bib60)], including centroids, skeletons,
    and contours. Some video segmentation techniques, such as background subtraction,
    are also merged into older trackers [[61](#bib.bib61), [62](#bib.bib62)]. Hence,
    visual tracking and video segmentation encounter some common challenges (*e.g*.,
    object/camera motion, appearance change, occlusion, *etc*.), fostering their mutual
    collaboration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视觉跟踪。为了推断目标物体随时间的位置信息，目前的跟踪方法通常假设目标在第一帧中由一个边界框确定[[59](#bib.bib59)]。然而，在更一般的跟踪场景中，特别是在早期跟踪方法研究的情况下，探索了多样的对象表示[[60](#bib.bib60)]，包括质心、骨架和轮廓。一些视频分割技术，如背景减除，也被合并到旧的跟踪器中[[61](#bib.bib61),
    [62](#bib.bib62)]。因此，视觉跟踪和视频分割面临一些共同的挑战（*例如*，对象/相机运动、外观变化、遮挡，*等等*），促进了它们的相互协作。
- en: $\bullet$ Image Semantic Segmentation. The success of end-to-end image semantic
    segmentation [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)] has sparked
    the rapid development of VSS. Rather than directly applying image semantic segmentation
    techniques frame by frame, recent VSS systems explore temporal continuity to increase
    both accuracy and efficiency. Nevertheless, image semantic segmentation techniques
    continue to serve as a foundation for advancing segmentation in video.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 图像语义分割。端到端图像语义分割[[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]的成功激发了VSS的快速发展。与其逐帧直接应用图像语义分割技术，近期的VSS系统探索了时间连续性，以提高准确性和效率。然而，图像语义分割技术仍然作为推进视频分割的基础。
- en: 'TABLE I: Summary of essential characteristics for reviewed AVOS methods (§[3.1.1](#S3.SS1.SSS1
    "3.1.1 Automatic Video Object Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS
    Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 总结了已审查AVOS方法的基本特征（§[3.1.1](#S3.SS1.SSS1 "3.1.1 Automatic Video Object
    Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based
    Video Segmentation ‣ A Survey on Deep Learning Technique for Video Segmentation")）。'
- en: 'Instance: instance- or object-level segmentation; Flow: if optical flow is
    used.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实例：实例或对象级分割；流：如果使用光流。
- en: '|   Year | Method | Pub. | Core Architecture | Instance | Flow | Training Dataset
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 核心架构 | 实例 | 流 | 训练数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2017 | FSEG [[66](#bib.bib66)] | CVPR | Two-Stream FCN | Object | ✓ | ImageNet
    VID [[67](#bib.bib67)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | FSEG [[66](#bib.bib66)] | CVPR | 双流FCN | 对象 | ✓ | ImageNet VID [[67](#bib.bib67)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| SFL [[68](#bib.bib68)] | ICCV | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SFL [[68](#bib.bib68)] | ICCV | 双流FCN | 对象 | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| LVO [[69](#bib.bib69)] | ICCV | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LVO [[69](#bib.bib69)] | ICCV | 双流FCN | 对象 | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| LMP [[70](#bib.bib70)] | ICCV | FCN | Object | ✓ | FT3D [[71](#bib.bib71)]
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LMP [[70](#bib.bib70)] | ICCV | FCN | 对象 | ✓ | FT3D [[71](#bib.bib71)] |'
- en: '| NRF [[72](#bib.bib72)] | ICCV | FCN | Object | ✓ | Youtube-Objects [[73](#bib.bib73)]
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| NRF [[72](#bib.bib72)] | ICCV | FCN | 物体 | ✓ | Youtube-Objects [[73](#bib.bib73)]
    |'
- en: '| 2018 | IST [[74](#bib.bib74)] | CVPR | FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | IST [[74](#bib.bib74)] | CVPR | FCN | 物体 | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| FGRNE [[75](#bib.bib75)] | CVPR | FCN + RNN | Object |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| FGRNE [[75](#bib.bib75)] | CVPR | FCN + RNN | 物体 |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
- en: '| MBN [[77](#bib.bib77)] | ECCV | FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| MBN [[77](#bib.bib77)] | ECCV | FCN | 物体 | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| PDB [[78](#bib.bib78)] | ECCV | RNN | Object |  | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| PDB [[78](#bib.bib78)] | ECCV | RNN | 物体 |  | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| MOT [[79](#bib.bib79)] | ICRA | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| MOT [[79](#bib.bib79)] | ICRA | 双流 FCN | 物体 | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| 2019 | RVOS [[80](#bib.bib80)] | CVPR | RNN | Instance |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | RVOS [[80](#bib.bib80)] | CVPR | RNN | 实例 |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| COSNet [[83](#bib.bib83)] | CVPR | Siamese FCN + Co-attention | Object |  |
    MSRA10K [[84](#bib.bib84)] + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| COSNet [[83](#bib.bib83)] | CVPR | 孪生 FCN + 共注意力 | 物体 |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| UMOD [[86](#bib.bib86)] | CVPR | Adversarial Network | Object | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| UMOD [[86](#bib.bib86)] | CVPR | 对抗网络 | 物体 | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
- en: '| AGS [[87](#bib.bib87)] | CVPR | FCN | Object |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + DUT [[85](#bib.bib85)] + PASCAL-S [[88](#bib.bib88)]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| AGS [[87](#bib.bib87)] | CVPR | FCN | 物体 |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + DUT [[85](#bib.bib85)] + PASCAL-S [[88](#bib.bib88)]
    |'
- en: '| AGNN [[89](#bib.bib89)] | ICCV | FCN + GNN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AGNN [[89](#bib.bib89)] | ICCV | FCN + GNN | 物体 |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| MGA [[90](#bib.bib90)] | ICCV | Two-Stream FCN | Object | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MGA [[90](#bib.bib90)] | ICCV | 双流 FCN | 物体 | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
- en: '| AnDiff [[92](#bib.bib92)] | ICCV | Siamese FCN + Co-attention | Object |  |
    DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| AnDiff [[92](#bib.bib92)] | ICCV | 孪生 FCN + 共注意力 | 物体 |  | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| LSMO [[93](#bib.bib93)] | IJCV | Two-Stream FCN | Object | ✓ | FT3D [[71](#bib.bib71)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LSMO [[93](#bib.bib93)] | IJCV | 双流 FCN | 物体 | ✓ | FT3D [[71](#bib.bib71)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| 2020 | MATNet [[94](#bib.bib94)] | AAAI | Two-Stream FCN | Object | ✓ | Youtube-VOS [[95](#bib.bib95)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | MATNet [[94](#bib.bib94)] | AAAI | 双流 FCN | 物体 | ✓ | Youtube-VOS [[95](#bib.bib95)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| PyramidCSA [[96](#bib.bib96)] | AAAI | Siamese FCN + Co-attention | Object
    |  | DUTS [[91](#bib.bib91)] + DAVIS[16] [[17](#bib.bib17)] + DAVSOD [[97](#bib.bib97)]
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| PyramidCSA [[96](#bib.bib96)] | AAAI | 孪生 FCN + 共注意力 | 物体 |  | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] + DAVSOD [[97](#bib.bib97)] |'
- en: '| MuG [[98](#bib.bib98)] | CVPR | FCN | Object |  | OxUvA [[99](#bib.bib99)]
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| MuG [[98](#bib.bib98)] | CVPR | FCN | 物体 |  | OxUvA [[99](#bib.bib99)] |'
- en: '| EGMN [[100](#bib.bib100)] | ECCV | FCN + Episodic Memory | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| EGMN [[100](#bib.bib100)] | ECCV | FCN + 情景记忆 | 物体 |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| WCSNet [[101](#bib.bib101)] | ECCV | Siamese FCN | Object |  | SALICON [[102](#bib.bib102)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DUTS [[91](#bib.bib91)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WCSNet [[101](#bib.bib101)] | ECCV | 孪生 FCN | 物体 |  | SALICON [[102](#bib.bib102)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DUTS [[91](#bib.bib91)] + DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| DFNet [[104](#bib.bib104)] | ECCV | Siamese FCN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| DFNet [[104](#bib.bib104)] | ECCV | 孪生 FCN | 物体 |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| 2021 | F2Net [[105](#bib.bib105)] | AAAI | Siamese FCN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | F2Net [[105](#bib.bib105)] | AAAI | 孪生 FCN | 物体 |  | MSRA10K [[84](#bib.bib84)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| TODA [[106](#bib.bib106)] | CVPR | Siamese FCN | Instance |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| TODA [[106](#bib.bib106)] | CVPR | 孪生 FCN | 实例 |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| RTNet [[107](#bib.bib107)] | CVPR | Two-Stream FCN | Object | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| RTNet [[107](#bib.bib107)] | CVPR | 双流 FCN | 物体 | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| DyStab [[108](#bib.bib108)] | CVPR | Adversarial Network | Object | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DyStab [[108](#bib.bib108)] | CVPR | 对抗网络 | Object | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
- en: '| MotionGrouping [[109](#bib.bib109)] | ICCV | Transformer | Object | ✓ | DAVIS16 [[17](#bib.bib17)]/SegTrackV2 [[76](#bib.bib76)]/FBMS59 [[110](#bib.bib110)]/MoCA [[111](#bib.bib111)]
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MotionGrouping [[109](#bib.bib109)] | ICCV | Transformer | Object | ✓ | DAVIS16 [[17](#bib.bib17)]/SegTrackV2 [[76](#bib.bib76)]/FBMS59 [[110](#bib.bib110)]/MoCA [[111](#bib.bib111)]
    |'
- en: $\bullet$ Video Object Detection. To generalize object detection in the video
    domain [[112](#bib.bib112)], video object detectors incorporate temporal cues
    over the box- or feature- level. There are many key technical steps and challenges,
    such as object proposal generation, temporal information aggregation, and cross-frame
    object association, that are shared between video object detection and (instance-level)
    video segmentation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频对象检测。为了在视频领域推广对象检测 [[112](#bib.bib112)]，视频对象检测器在框或特征级别上结合时间线索。视频对象检测和（实例级别）视频分割之间有许多关键技术步骤和挑战，如对象提议生成、时间信息聚合和跨帧对象关联。
- en: 3 Deep Learning-based Video Segmentation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的视频分割
- en: 3.1 Deep Learning-based VOS Models
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于深度学习的 VOS 模型
- en: 'VOS extracts generic foreground objects from video sequences with no concern
    for semantic category recognition. Based on how much human intervention is involved
    in inference, VOS models can be divided into three classes (§[2.1.2](#S2.SS1.SSS2
    "2.1.2 Inference Modes for Video Segmentation ‣ 2.1 Problem Formulation and Taxonomy
    ‣ 2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")):
    automatic (AVOS, §[3.1.1](#S3.SS1.SSS1 "3.1.1 Automatic Video Object Segmentation
    (AVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), semi-automatic
    (SVOS, §[3.1.2](#S3.SS1.SSS2 "3.1.2 Semi-automatic Video Object Segmentation (SVOS)
    ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), and interactive
    (IVOS, §[3.1.3](#S3.SS1.SSS3 "3.1.3 Interactive Video Object Segmentation (IVOS)
    ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")). Moreover, although
    language-guided video object segmentation (LVOS) falls in the broader category
    of SVOS, LVOS methods are reviewed alone (§[3.1.4](#S3.SS1.SSS4 "3.1.4 Language-guided
    Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep
    Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for Video
    Segmentation")), due to the specific multi-modal task setup.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: VOS 从视频序列中提取通用的前景对象，而不考虑语义类别识别。根据推断过程中涉及的人类干预程度，VOS 模型可以分为三类 (§[2.1.2](#S2.SS1.SSS2
    "2.1.2 Inference Modes for Video Segmentation ‣ 2.1 Problem Formulation and Taxonomy
    ‣ 2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation"))：自动（AVOS，§[3.1.1](#S3.SS1.SSS1
    "3.1.1 Automatic Video Object Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS
    Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")）、半自动（SVOS，§[3.1.2](#S3.SS1.SSS2 "3.1.2 Semi-automatic
    Video Object Segmentation (SVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep
    Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for Video
    Segmentation")）和交互式（IVOS，§[3.1.3](#S3.SS1.SSS3 "3.1.3 Interactive Video Object
    Segmentation (IVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based
    Video Segmentation ‣ A Survey on Deep Learning Technique for Video Segmentation")）。此外，尽管语言引导的视频对象分割（LVOS）属于
    SVOS 的更广泛类别，但由于其特定的多模态任务设置，LVOS 方法被单独审视 (§[3.1.4](#S3.SS1.SSS4 "3.1.4 Language-guided
    Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep
    Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for Video
    Segmentation")）。
- en: 3.1.1 Automatic Video Object Segmentation (AVOS)
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 自动视频对象分割（AVOS）
- en: Instead of using heuristic priors and hand-crafted features to automatically
    execute VOS, modern AVOS methods learn generic video object patterns in a data-driven
    fashion. We group landmark efforts based on their key techniques.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 AVOS 方法不再使用启发式先验和手工特征来自动执行 VOS，而是以数据驱动的方式学习通用的视频对象模式。我们根据其关键技术对重要工作进行了分组。
- en: $\bullet$ Deep Learning Module based Methods. In 2015, Fragkiadaki *et al*. [[113](#bib.bib113)]
    made an early effort that learns a multi-layer perceptron to rank proposal segments
    and infer foreground objects. In 2016, Tsai *et al*. [[43](#bib.bib43)] proposed
    a joint optimization framework for AVOS and optical flow estimation with a naïve
    use of deep features from a pre-trained classification network. Later methods [[72](#bib.bib72),
    [70](#bib.bib70)] learn FCNs to predict initial, pixel-level foreground estimates
    from frame images [[72](#bib.bib72), [114](#bib.bib114)] or optical flow fields [[70](#bib.bib70)],
    while several post-processing steps are still needed. Basically, these primitive
    solutions largely rely on traditional AVOS techniques; the learning ability of
    neural networks is under-explored.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于深度学习模块的方法。2015 年，Fragkiadaki *等* [[113](#bib.bib113)] 进行了早期尝试，学习多层感知器以对提议段进行排序并推断前景物体。2016
    年，Tsai *等* [[43](#bib.bib43)] 提出了一个用于 AVOS 和光流估计的联合优化框架，采用了来自预训练分类网络的深度特征。后来的方法
    [[72](#bib.bib72), [70](#bib.bib70)] 学习 FCNs 从帧图像 [[72](#bib.bib72), [114](#bib.bib114)]
    或光流场 [[70](#bib.bib70)] 预测初步的像素级前景估计，但仍需要若干后处理步骤。基本上，这些原始解决方案在很大程度上依赖于传统的 AVOS
    技术；神经网络的学习能力尚未得到充分探索。
- en: $\bullet$ Pixel Instance Embedding based Methods. A group of AVOS models has
    been developed to make use of stronger deep learning descriptors [[74](#bib.bib74),
    [77](#bib.bib77)] – instance embeddings – learned from image instance segmentation
    data [[115](#bib.bib115)]. They first generate pixel-wise instance embeddings,
    and select representative embeddings which are clustered into foreground and background.
    Finally, the labels of the sampled embeddings are propagated to the other ones.
    The clustering and propagation can be achieved without video specific supervision.
    Though using fewer annotations, these methods suffer from a fragmented and complicated
    pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于像素实例嵌入的方法。一组 AVOS 模型已经被开发出来，以利用从图像实例分割数据中学习到的更强大的深度学习描述符 [[74](#bib.bib74),
    [77](#bib.bib77)] —— 实例嵌入。它们首先生成逐像素实例嵌入，并选择代表性嵌入，这些嵌入被聚类为前景和背景。最后，将采样的嵌入标签传播到其他嵌入上。聚类和传播可以在没有视频特定监督的情况下完成。尽管使用了较少的标注，这些方法仍然受到管道碎片化和复杂化的影响。
- en: $\bullet$ End-to-end Methods with Short-term Information Encoding. End-to-end
    model designs became the mainstream in this field. For example, convolutional
    recurrent neural networks (RNNs) were used to learn spatial and temporal visual
    patterns jointly [[78](#bib.bib78), [89](#bib.bib89)]. Another big family is built
    upon two-stream networks [[66](#bib.bib66), [68](#bib.bib68), [90](#bib.bib90),
    [75](#bib.bib75), [93](#bib.bib93), [69](#bib.bib69), [94](#bib.bib94)], wherein
    two parallel streams are built to extract features from raw image and optical
    flow, which are further fused for segmentation prediction. Two-stream methods
    make explicit use of appearance and motion cues, at the cost of optical flow computation
    and vast learnable parameters. These end-to-end methods improve accuracy and show
    the advantages of applying neural networks to this task. However, they only consider
    local content within very limited time span; they stack appearance and/or motion
    information from a few successive frames as input, ignoring relations among distant
    frames. Although RNNs are usually adopted, their internal hidden memory creates
    the inherent limits in modeling longer-term dependencies [[116](#bib.bib116)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 具有短期信息编码的端到端方法。端到端模型设计已成为该领域的主流。例如，卷积递归神经网络（RNNs）被用来共同学习空间和时间视觉模式
    [[78](#bib.bib78), [89](#bib.bib89)]。另一个重要家族是基于双流网络构建的 [[66](#bib.bib66), [68](#bib.bib68),
    [90](#bib.bib90), [75](#bib.bib75), [93](#bib.bib93), [69](#bib.bib69), [94](#bib.bib94)]，其中两个并行流用于从原始图像和光流中提取特征，这些特征进一步融合用于分割预测。双流方法明确利用了外观和运动线索，但需要计算光流和大量可学习参数。这些端到端方法提高了准确性，并展示了将神经网络应用于此任务的优势。然而，它们仅考虑非常有限时间跨度内的局部内容；它们将几帧连续图像的外观和/或运动信息堆叠作为输入，忽略了远帧之间的关系。尽管通常采用
    RNNs，但其内部隐藏记忆在建模长期依赖性方面存在固有限制 [[116](#bib.bib116)]。
- en: 'TABLE II: Summary of essential characteristics for reviewed SVOS methods (§[3.1.2](#S3.SS1.SSS2
    "3.1.2 Semi-automatic Video Object Segmentation (SVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")). Flow: if optical flow is used.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE II: 被审核的SVOS方法（§[3.1.2](#S3.SS1.SSS2 "3.1.2 Semi-automatic Video Object
    Segmentation (SVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based
    Video Segmentation ‣ A Survey on Deep Learning Technique for Video Segmentation"))的主要特征总结。流：如果使用了光流。'
- en: '|   Year | Method | Pub. | Core Architecture | Flow | Technical Feature | Training
    Dataset |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发布期刊 | 核心架构 | 流 | 技术特征 | 训练数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2017 | OSVOS [[18](#bib.bib18)] | CVPR | FCN |  | Online Fine-tuning | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | OSVOS [[18](#bib.bib18)] | CVPR | FCN |  | 在线微调 | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| MaskTrack [[117](#bib.bib117)] | CVPR | FCN | ✓ | Propagation-based | ECSSD [[118](#bib.bib118)]
    + MSRA10K [[84](#bib.bib84)] + PASCAL-S [[88](#bib.bib88)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MaskTrack [[117](#bib.bib117)] | CVPR | FCN | ✓ | 基于传播 | ECSSD [[118](#bib.bib118)]
    + MSRA10K [[84](#bib.bib84)] + PASCAL-S [[88](#bib.bib88)] + DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| CTN [[119](#bib.bib119)] | CVPR | FCN | ✓ | Propagation-based | PASCAL VOC
    2012 [[103](#bib.bib103)] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CTN [[119](#bib.bib119)] | CVPR | FCN | ✓ | 基于传播 | PASCAL VOC 2012 [[103](#bib.bib103)]
    |'
- en: '| VPN [[120](#bib.bib120)] | CVPR | Bilateral Network |  | Propagation-based
    | DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| VPN [[120](#bib.bib120)] | CVPR | 双边网络 |  | 基于传播 | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| PLM [[121](#bib.bib121)] | CVPR | Siamese FCN |  | Matching-based | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| PLM [[121](#bib.bib121)] | CVPR | Siamese FCN |  | 基于匹配 | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| OnAVOS [[122](#bib.bib122)] | BMVC | FCN |  | Online Fine-tuning | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + DAVIS [[17](#bib.bib17)]
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| OnAVOS [[122](#bib.bib122)] | BMVC | FCN |  | 在线微调 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + DAVIS [[17](#bib.bib17)] |'
- en: '| Lucid [[124](#bib.bib124)] | IJCV | Two-Stream FCN | ✓ | Propagation-based
    | DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Lucid [[124](#bib.bib124)] | IJCV | 双流FCN | ✓ | 基于传播 | DAVIS[16] [[17](#bib.bib17)]
    |'
- en: '| 2018 | CINM [[125](#bib.bib125)] | CVPR | Spatio-temporal MRF | ✓ | Propagation-based
    | DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | CINM [[125](#bib.bib125)] | CVPR | 时空MRF | ✓ | 基于传播 | DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| FAVOS [[126](#bib.bib126)] | CVPR | FCN |  | Propagation-based | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| FAVOS [[126](#bib.bib126)] | CVPR | FCN |  | 基于传播 | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| RGMP [[127](#bib.bib127)] | CVPR | Siamese FCN |  | Propagation-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + ECSSD [[118](#bib.bib118)] + MSRA10K [[84](#bib.bib84)]
    + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| RGMP [[127](#bib.bib127)] | CVPR | Siamese FCN |  | 基于传播 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + ECSSD [[118](#bib.bib118)] + MSRA10K [[84](#bib.bib84)] + DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| OSMN [[128](#bib.bib128)] | CVPR | FCN + Meta Learning |  | Online Fine-tuning
    | ImageNet VID [[67](#bib.bib67)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| OSMN [[128](#bib.bib128)] | CVPR | FCN + 元学习 |  | 在线微调 | ImageNet VID [[67](#bib.bib67)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| MONet [[129](#bib.bib129)] | CVPR | FCN | ✓ | Online Fine-tuning | PASCAL
    VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MONet [[129](#bib.bib129)] | CVPR | FCN | ✓ | 在线微调 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| CRN [[130](#bib.bib130)] | CVPR | FCN + Active Contour | ✓ | Propagation-based
    | PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| CRN [[130](#bib.bib130)] | CVPR | FCN + 主动轮廓 | ✓ | 基于传播 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| RCAL [[131](#bib.bib131)] | CVPR | FCN + RL |  | Propagation-based | MSRA10K [[84](#bib.bib84)]
    + PASCAL-S + SOD + ECSSD [[118](#bib.bib118)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| RCAL [[131](#bib.bib131)] | CVPR | FCN + RL |  | 基于传播 | MSRA10K [[84](#bib.bib84)]
    + PASCAL-S + SOD + ECSSD [[118](#bib.bib118)] + DAVIS[16] [[17](#bib.bib17)] |'
- en: '| OSVOS-S [[132](#bib.bib132)] | PAMI | FCN |  | Online Fine-tuning | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| OSVOS-S [[132](#bib.bib132)] | PAMI | FCN |  | 在线微调 | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| Videomatch [[133](#bib.bib133)] | ECCV | Siamese FCN |  | Matching-based
    | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Videomatch [[133](#bib.bib133)] | ECCV | Siamese FCN |  | 基于匹配 | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| Dyenet [[134](#bib.bib134)] | ECCV | Re-ID |  | Propagation-based | DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Dyenet [[134](#bib.bib134)] | ECCV | 重新识别 |  | 基于传播 | DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| LSE [[135](#bib.bib135)] | ECCV | FCN |  | Propagation-based | PASCAL VOC
    2012 [[103](#bib.bib103)] |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LSE [[135](#bib.bib135)] | ECCV | FCN |  | 基于传播 | PASCAL VOC 2012 [[103](#bib.bib103)]
    |'
- en: '| Colorization [[136](#bib.bib136)] | ECCV | Siamese FCN |  | Unsupervised
    Learning | Kinetics [[137](#bib.bib137)] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Colorization [[136](#bib.bib136)] | ECCV | Siamese FCN |  | 无监督学习 | Kinetics [[137](#bib.bib137)]
    |'
- en: '| 2019 | MVOS [[138](#bib.bib138)] | PAMI | Siamese FCN + Meta Learning |  |
    Online Fine-tuning | PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | MVOS [[138](#bib.bib138)] | PAMI | Siamese FCN + 元学习 |  | 在线微调 | PASCAL
    VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| FEELVOS [[139](#bib.bib139)] | CVPR | FCN |  | Matching-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| FEELVOS [[139](#bib.bib139)] | CVPR | FCN |  | 基于匹配 | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| MHP-VOS [[140](#bib.bib140)] | CVPR | Graph Optimization |  | Propagation-based
    | COCO [[123](#bib.bib123)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| MHP-VOS [[140](#bib.bib140)] | CVPR | 图优化 |  | 基于传播 | COCO [[123](#bib.bib123)]
    + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)] |'
- en: '| AGSS [[141](#bib.bib141)] | CVPR | FCN | ✓ | Propagation-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AGSS [[141](#bib.bib141)] | CVPR | FCN | ✓ | 基于传播 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| AGAME [[142](#bib.bib142)] | CVPR | FCN |  | Propagation-based | MSRA10K [[84](#bib.bib84)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| AGAME [[142](#bib.bib142)] | CVPR | FCN |  | 基于传播 | MSRA10K [[84](#bib.bib84)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| SiamMask [[143](#bib.bib143)] | CVPR | Siamese FCN |  | Box-Initialization
    | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SiamMask [[143](#bib.bib143)] | CVPR | Siamese FCN |  | 框初始化 | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| RVOS [[80](#bib.bib80)] | CVPR | RNN |  | Propagation-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| RVOS [[80](#bib.bib80)] | CVPR | RNN |  | 基于传播 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| BubbleNet [[144](#bib.bib144)] | CVPR | Siamese Network |  | Bubble Sorting
    | DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| BubbleNet [[144](#bib.bib144)] | CVPR | Siamese 网络 |  | 冒泡排序 | DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| RANet [[145](#bib.bib145)] | ICCV | Siamese FCN |  | Matching-based | MSRA10K [[84](#bib.bib84)]
    + ECSSD [[118](#bib.bib118)]+ HKU-IS [[146](#bib.bib146)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| RANet [[145](#bib.bib145)] | ICCV | Siamese FCN |  | 基于匹配 | MSRA10K [[84](#bib.bib84)]
    + ECSSD [[118](#bib.bib118)]+ HKU-IS [[146](#bib.bib146)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| DMM-Net [[147](#bib.bib147)] | ICCV | Mask R-CNN |  | Differentiable Matching
    | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| DMM-Net [[147](#bib.bib147)] | ICCV | Mask R-CNN |  | 可微匹配 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| DTN [[148](#bib.bib148)] | ICCV | FCN | ✓ | Propagation-based | COCO [[123](#bib.bib123)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| DTN [[148](#bib.bib148)] | ICCV | FCN | ✓ | 基于传播 | COCO [[123](#bib.bib123)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16]/DAVIS[17] [[81](#bib.bib81)]
    |'
- en: '| STM [[149](#bib.bib149)] | ICCV | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| STM [[149](#bib.bib149)] | ICCV | 记忆网络 |  | 基于匹配 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| TimeCycle [[150](#bib.bib150)] | ECCV | Siamese FCN |  | Unsupervised Learning
    | VLOG [[151](#bib.bib151)] |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| TimeCycle [[150](#bib.bib150)] | ECCV | Siamese FCN |  | 无监督学习 | VLOG [[151](#bib.bib151)]
    |'
- en: '|  | UVC [[152](#bib.bib152)] | NeurIPS | Siamese FCN |  | Unsupervised Learning
    | COCO [[123](#bib.bib123)] + Kinetics [[137](#bib.bib137)] |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | UVC [[152](#bib.bib152)] | NeurIPS | Siamese FCN |  | 无监督学习 | COCO [[123](#bib.bib123)]
    + Kinetics [[137](#bib.bib137)] |'
- en: '| 2020 | e-OSVOS [[153](#bib.bib153)] | NeurIPS | Mask R-CNN + Meta Learning
    |  | Online Fine-tuning | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | e-OSVOS [[153](#bib.bib153)] | NeurIPS | Mask R-CNN + 元学习 |  | 在线微调
    | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| AFB-URR [[154](#bib.bib154)] | NeurIPS | Memory Network |  | Matching-based
    | PASCAL VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| AFB-URR [[154](#bib.bib154)] | NeurIPS | 记忆网络 |  | 基于匹配 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| Fasttan [[155](#bib.bib155)] | CVPR | Faster R-CNN |  | Propagation-based
    | COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Fasttan [[155](#bib.bib155)] | CVPR | Faster R-CNN |  | 基于传播 | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] |'
- en: '| Fasttmu [[156](#bib.bib156)] | CVPR | FCN + RL |  | Box-Initialization |
    PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Fasttmu [[156](#bib.bib156)] | CVPR | FCN + RL |  | Box-Initialization |
    PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)] |'
- en: '| SAT [[157](#bib.bib157)] | CVPR | FCN + RL |  | Propagation-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| SAT [[157](#bib.bib157)] | CVPR | FCN + RL |  | 基于传播 | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| FRTM-VOS [[158](#bib.bib158)] | CVPR | FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| FRTM-VOS [[158](#bib.bib158)] | CVPR | FCN |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| TVOS [[159](#bib.bib159)] | CVPR | FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| TVOS [[159](#bib.bib159)] | CVPR | FCN |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| MuG [[98](#bib.bib98)] | CVPR | Siamese FCN |  | Unsupervised Learning |
    OxUvA [[99](#bib.bib99)] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| MuG [[98](#bib.bib98)] | CVPR | Siamese FCN |  | 无监督学习 | OxUvA [[99](#bib.bib99)]
    |'
- en: '| MAST [[160](#bib.bib160)] | CVPR | Memory Network |  | Unsupervised Learning
    | OxUvA [[99](#bib.bib99)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| MAST [[160](#bib.bib160)] | CVPR | 记忆网络 |  | 无监督学习 | OxUvA [[99](#bib.bib99)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| GCNet [[161](#bib.bib161)] | ECCV | Memory Network |  | Matching-based |
    MSRA10K [[84](#bib.bib84)] + ECSSD [[118](#bib.bib118)] + HKU-IS [[146](#bib.bib146)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| GCNet [[161](#bib.bib161)] | ECCV | 记忆网络 |  | 基于匹配 | MSRA10K [[84](#bib.bib84)]
    + ECSSD [[118](#bib.bib118)] + HKU-IS [[146](#bib.bib146)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| KMN [[162](#bib.bib162)] | ECCV | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| KMN [[162](#bib.bib162)] | ECCV | 记忆网络 |  | 基于匹配 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| CFBI [[163](#bib.bib163)] | ECCV | FCN |  | Matching-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CFBI [[163](#bib.bib163)] | ECCV | FCN |  | 基于匹配 | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
- en: '| LWL [[164](#bib.bib164)] | ECCV | Siamese FCN + Meta Learning |  | Matching-based
    | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LWL [[164](#bib.bib164)] | ECCV | Siamese FCN + 元学习 |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| MSN [[165](#bib.bib165)] | ECCV | Memory Network |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| MSN [[165](#bib.bib165)] | ECCV | 记忆网络 |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| EGMN [[100](#bib.bib100)] | ECCV | Memory Network |  | Matching-based | MSRA10K [[84](#bib.bib84)]
    + COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| EGMN [[100](#bib.bib100)] | ECCV | 记忆网络 |  | 基于匹配 | MSRA10K [[84](#bib.bib84)]
    + COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| STM-Cycle [[166](#bib.bib166)] | NeurIPS | Memory Network |  | Matching-based
    | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| STM-Cycle [[166](#bib.bib166)] | NeurIPS | 记忆网络 |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| 2021 | QMA [[167](#bib.bib167)] | AAAI | Memory Network |  | Box-Initialization
    | DUT [[85](#bib.bib85)] + HKU-IS [[146](#bib.bib146)] + MSRA10K [[84](#bib.bib84)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | QMA [[167](#bib.bib167)] | AAAI | 记忆网络 |  | Box-Initialization | DUT [[85](#bib.bib85)]
    + HKU-IS [[146](#bib.bib146)] + MSRA10K [[84](#bib.bib84)] + YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| SwiftNet [[168](#bib.bib168)] | CVPR | Memory Network |  | Matching-based
    | COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| SwiftNet [[168](#bib.bib168)] | CVPR | 记忆网络 |  | 基于匹配 | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
- en: '| G-FRTM [[169](#bib.bib169)] | CVPR | FCN + RL |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| G-FRTM [[169](#bib.bib169)] | CVPR | FCN + RL |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| SST [[170](#bib.bib170)] | CVPR | Transformer |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| SST [[170](#bib.bib170)] | CVPR | Transformer |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| GIEL [[171](#bib.bib171)] | CVPR | Siamese FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| GIEL [[171](#bib.bib171)] | CVPR | Siamese FCN |  | 基于匹配 | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
- en: '| LCM [[172](#bib.bib172)] | CVPR | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LCM [[172](#bib.bib172)] | CVPR | 记忆网络 |  | 基于匹配 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| RMNet [[173](#bib.bib173)] | CVPR | Memory Network | ✓ | Matching-based |
    PASCAL VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| RMNet [[173](#bib.bib173)] | CVPR | 记忆网络 | ✓ | 基于匹配 | PASCAL VOC 2012 [[103](#bib.bib103)]
    + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
- en: '| CRW [[174](#bib.bib174)] | NeurIPS | FCN |  | Unsupervised Learning | Kinetics
    [[137](#bib.bib137)] |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| CRW [[174](#bib.bib174)] | NeurIPS | FCN |  | 无监督学习 | Kinetics [[137](#bib.bib137)]
    |'
- en: $\bullet$ End-to-end Methods with Long-term Context Encoding. Current leading
    AVOS models use global context over long time spans. In a seminal work [[83](#bib.bib83)],
    Lu *et al*. proposed a Siamese architecture-based model that extracts features
    for arbitrary frame pairs and captures cross-frame context by calculating pixel-wise
    feature correlations. During inference, for each test frame, context from several
    other frames (within the same video) is aggregated to locate objects. A contemporary
    work [[92](#bib.bib92)] exploited a similar idea but only used the first frame
    as reference. Several papers [[101](#bib.bib101), [107](#bib.bib107)] extended
    [[83](#bib.bib83)] by making better use of information from multiple frames [[89](#bib.bib89),
    [175](#bib.bib175), [176](#bib.bib176)], encoding spatial context [[105](#bib.bib105)],
    and incorporating temporal consistency to improve representation power and computation
    efficiency [[96](#bib.bib96), [104](#bib.bib104)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 端到端方法与长期上下文编码。当前领先的AVOS模型利用长期的全局上下文。在一项开创性工作 [[83](#bib.bib83)]中，Lu
    *et al*. 提出了基于Siamese架构的模型，该模型为任意帧对提取特征，并通过计算像素级特征相关性来捕捉跨帧上下文。在推理过程中，对于每一帧测试图像，聚合来自其他几帧（在同一视频内）的上下文以定位目标。另一项现代工作 [[92](#bib.bib92)]利用了类似的思想，但仅使用第一帧作为参考。一些论文 [[101](#bib.bib101),
    [107](#bib.bib107)] 通过更好地利用来自多帧的信息 [[89](#bib.bib89), [175](#bib.bib175), [176](#bib.bib176)]、编码空间上下文 [[105](#bib.bib105)]
    和结合时间一致性来提升表示能力和计算效率 [[96](#bib.bib96), [104](#bib.bib104)]。
- en: $\bullet$ ${}_{\!}$Un-/Weakly-supervised${}_{\!}$ based${}_{\!}$ Methods. Only
    a handful of methods learn to perform AVOS from unlabeled or weakly labeled data.
    In [[87](#bib.bib87)], static image salient object segmentation and dynamic eye
    fixation data, which are more easily acquired compared with video segmentation
    data, are used to learn video generic object patterns. In [[98](#bib.bib98)],
    visual patterns are learned through exploring several intrinsic properties of
    video data at multiple granularities, *i.e*., intra-frame saliency, short-term
    visual coherence, long-range semantic correspondence, and video-level discriminativeness.
    In [[86](#bib.bib86)], an adversarial contextual model is developed to segment
    moving objects without any manual annotation, achieved by minimizing the mutual
    information between the motions of an object and its context. This method is further
    enhanced in [[108](#bib.bib108)] by adopting a bootstrapping strategy and enforcing
    temporal consistency. In [[109](#bib.bib109)], motion is exclusively exploited
    to discover moving objects, and a Transformer-based model is designed and trained
    by self-supervised flow reconstruction using unlabeled video data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ${}_{\!}$无监督/弱监督${}_{\!}$ 基于${}_{\!}$ 方法。仅有少数方法能够从未标注或弱标注的数据中学习进行AVOS。在 [[87](#bib.bib87)]中，使用静态图像显著目标分割和动态眼动数据（相对于视频分割数据更容易获取）来学习视频通用目标模式。在 [[98](#bib.bib98)]中，通过探索视频数据的多个内在属性，如帧内显著性、短期视觉一致性、长距离语义对应性和视频级区分性来学习视觉模式。在 [[86](#bib.bib86)]中，开发了一种对抗性上下文模型来分割移动目标，方法是通过最小化目标及其上下文运动之间的互信息来实现的。该方法在 [[108](#bib.bib108)]中通过采用自举策略和强制时间一致性得到进一步提升。在 [[109](#bib.bib109)]中，专门利用运动来发现移动对象，并设计了一种基于Transformer的模型，通过自监督流重建使用未标注的视频数据进行训练。
- en: '$\bullet$ Instance-level AVOS Methods. Instance-level AVOS, also referred as
    multi-object unsupervised video segmentation, was introduced with the launch of
    the DAVIS[19] challenge [[177](#bib.bib177)]. This task setting is more challenging
    as it requires not only separating the foreground objects from the background,
    but also discriminating different object instances. To tackle this task, current
    solutions typically work in a top-down fashion, *i.e*., generating object candidates
    for each frames, and associating instances over different frames. In an early
    attempt [[80](#bib.bib80)], Ventura *et al*. delivered a recurrent network-based
    model that consists of a spatial LSTM for per-frame instance discovery and a temporal
    LSTM for cross-frame instance association. This method features an elegant model
    design, while its representation ability is too weak to enumerate all the object
    instances and to capture complex interactions between instances over the temporal
    domain. Thus later methods [[178](#bib.bib178), [179](#bib.bib179), [175](#bib.bib175)]
    strengthen the two-step pipeline through: i) employing image instance segmentation
    models (*e.g*., Mask R-CNN [[180](#bib.bib180)]) to detect object candidates,
    and ii) leveraging tracking/re-identification techniques and manually designed
    rules for instance association. Foreground/background AVOS techniques [[89](#bib.bib89),
    [83](#bib.bib83)] are also used to filter out nonsalient candidates [[179](#bib.bib179),
    [175](#bib.bib175)]. More recent methods, *e.g*., [[106](#bib.bib106)], generate
    object candidates first and obtain corresponding tracklets via advanced SVOS techniques.
    Overall, current instance-level AVOS models follow the classic tracking-by-detection
    paradigm, involving several ad-hoc designs. There is still considerable room for
    further improvement in accuracy and efficiency.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 实例级 AVOS 方法。实例级 AVOS，也称为多对象无监督视频分割，随着 DAVIS[19] 挑战的推出而引入[[177](#bib.bib177)]。此任务设置更具挑战性，因为它不仅要求将前景对象与背景分离，还要区分不同的对象实例。为了解决这个问题，目前的解决方案通常采用自上而下的方式，*即*，为每帧生成对象候选，并在不同帧之间关联实例。在早期的尝试中[[80](#bib.bib80)]，Ventura
    *et al* 提出了一个基于递归网络的模型，该模型包括一个用于每帧实例发现的空间 LSTM 和一个用于跨帧实例关联的时间 LSTM。这个方法具有优雅的模型设计，但其表示能力过于薄弱，无法枚举所有对象实例并捕捉实例在时间域上的复杂交互。因此，后来的方法[[178](#bib.bib178),
    [179](#bib.bib179), [175](#bib.bib175)]通过以下方式加强了这一步骤：i) 使用图像实例分割模型（*例如*，Mask R-CNN
    [[180](#bib.bib180)]）来检测对象候选，ii) 利用跟踪/重识别技术和手动设计的规则进行实例关联。前景/背景 AVOS 技术[[89](#bib.bib89),
    [83](#bib.bib83)] 也用于过滤不显著的候选[[179](#bib.bib179), [175](#bib.bib175)]。更近期的方法，*例如*，[[106](#bib.bib106)]，首先生成对象候选，并通过先进的
    SVOS 技术获得相应的跟踪段。总体而言，当前的实例级 AVOS 模型遵循经典的基于检测的跟踪范式，涉及多个专门设计。准确性和效率仍有相当大的提升空间。
- en: 3.1.2 Semi-automatic Video Object Segmentation (SVOS)
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 半自动视频对象分割 (SVOS)
- en: Deep learning-based SVOS methods mainly focus on the first-frame mask propagation
    setting. They are categorized by their utilization of the test-time provided object
    masks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的 SVOS 方法主要集中在第一帧掩码传播设置上。它们根据对测试时提供的对象掩码的利用情况进行分类。
- en: $\bullet$ Online Fine-tuning based Methods. Following the one-
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在线微调方法。接下来是一个
- en: 'shot principle, this family of methods [[18](#bib.bib18), [132](#bib.bib132),
    [129](#bib.bib129), [138](#bib.bib138)] trains a segmentation model separately
    on each given object mask in an online fashion. Fine-tuning methods essentially
    exploit the transfer learning capabilities of neural networks and often follow
    a two-step training procedure: i) offline pre-training: learn general segmentation
    features from images and video sequences, and ii) online fine-tuning: learn target-specific
    representations from test-time supervision. The idea of fine-tuning was first
    introduced in [[18](#bib.bib18)], where only the initial image-mask pair is used
    for training an online, one-shot, but merely appearance-based FCN model. Then,
    in [[122](#bib.bib122)], more pixel samples in the unlabeled frames are mined
    as online training samples to better adapt to further changes over time. As [[18](#bib.bib18),
    [122](#bib.bib122)] have no notion of individual objects, [[132](#bib.bib132)]
    further incorporates instance segmentation models (*e.g*., Mask R-CNN [[180](#bib.bib180)])
    during inference. While elegant through their simplicity, fine-tuning methods
    have several weaknesses: i) pre-training is fixed and not optimized for subsequent
    fine-tuning, ii) hyperparameters of online fine-tuning are often excessively hand-crafted
    and fail to generalize between test cases, iii) the common existing fine-tuning
    setups suffer from high test runtimes (up to 1,000 training iterations per segmented
    object online [[18](#bib.bib18)]). The root cause is that these approaches choose
    to encode all the target-related cues (*i.e*., appearance, mask) into network
    parameters implicitly. Towards efficient and automated fine-tuning, some recent
    methods [[128](#bib.bib128), [138](#bib.bib138), [153](#bib.bib153)] turn to meta
    learning techniques, *i.e*., optimize the fine-tuning policies [[138](#bib.bib138),
    [153](#bib.bib153)] (*e.g*., generic model initialization, learning rates, *etc*.)
    or even directly modify network weights [[128](#bib.bib128)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 射击原理，这类方法[[18](#bib.bib18)、[132](#bib.bib132)、[129](#bib.bib129)、[138](#bib.bib138)]在线训练一个分割模型，每次处理一个给定的物体掩膜。微调方法本质上利用了神经网络的迁移学习能力，并且通常遵循两步训练程序：i)
    离线预训练：从图像和视频序列中学习一般分割特征，ii) 在线微调：从测试时监督中学习目标特定的表示。微调的想法最早在[[18](#bib.bib18)]中引入，其中仅使用初始的图像-掩膜对来训练一个在线的一次性、但仅仅是基于外观的FCN模型。然后，在[[122](#bib.bib122)]中，更多的像素样本在未标记的帧中被挖掘作为在线训练样本，以更好地适应随时间变化。由于[[18](#bib.bib18)、[122](#bib.bib122)]没有个体对象的概念，[[132](#bib.bib132)]在推断期间进一步引入实例分割模型（*例如*，Mask
    R-CNN [[180](#bib.bib180)]）。虽然由于其简洁性而优雅，但微调方法存在几个弱点：i) 预训练是固定的，并且没有针对随后的微调进行优化，ii)
    在线微调的超参数通常过于手工制作，无法在测试案例之间进行泛化，iii) 现有的微调设置通常存在较高的测试运行时间（每个分割物体在线最多1,000次训练迭代[[18](#bib.bib18)]）。根本原因是这些方法选择将所有目标相关的线索（*即*，外观、掩膜）隐式编码到网络参数中。为了实现高效和自动化的微调，一些最近的方法[[128](#bib.bib128)、[138](#bib.bib138)、[153](#bib.bib153)]转向元学习技术，*即*，优化微调策略[[138](#bib.bib138)、[153](#bib.bib153)]（*例如*，通用模型初始化、学习率、*等等*）甚至直接修改网络权重[[128](#bib.bib128)]。
- en: $\bullet$ Propagation-based Methods. Two recent lines of research
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于传播的方法。最近的两个研究方向
- en: – built upon mask propagation and template matching techniques respectively
    – try to refrain from the online optimization to deliver compact, end-to-end SVOS
    solutions. In particular, propagation-based methods use the previous frame mask
    to infer the current mask${}_{\!}$ [[117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120)].
    For example, Jampani *et al*. [[120](#bib.bib120)] propose a bilateral network
    for long-range video-adaptive mask propagation. Perazzi *et al*. [[117](#bib.bib117)]
    approach SVOS by employing a modified FCN, where the previous frame mask is considered
    as an extra input channel. Follow-up work adopts optical flow guided mask alignment [[129](#bib.bib129)],
    heavy first-frame data augmentation${}_{\!}$ [[124](#bib.bib124)], and multi-step
    segmentation refinement${}_{\!}$ [[130](#bib.bib130)]. Others apply
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: – 分别基于掩膜传播和模板匹配技术 – 尽量避免在线优化，以提供紧凑的端到端SVOS解决方案。特别地，基于传播的方法使用前一帧的掩膜来推断当前掩膜${}_{\!}$
    [[117](#bib.bib117)、[119](#bib.bib119)、[120](#bib.bib120)]。例如，Jampani *et al*
    [[120](#bib.bib120)] 提出了一个双边网络，用于长范围的视频自适应掩膜传播。Perazzi *et al* [[117](#bib.bib117)]
    通过采用修改后的FCN来处理SVOS，其中前一帧的掩膜被视为额外的输入通道。后续工作采用了光流引导的掩膜对齐[[129](#bib.bib129)]、重的数据增强${}_{\!}$
    [[124](#bib.bib124)]，以及多步分割精细化${}_{\!}$ [[130](#bib.bib130)]。其他方法应用
- en: re-identification to retrieve missing objects after prolonged occlusions${}_{\!}$ [[134](#bib.bib134)],
    design a reinforcement learning agent that tackles${}_{\!}$ SVOS as${}_{\!}$ a${}_{\!}$
    conditional${}_{\!}$ decision-making${}_{\!}$ process${}_{\!}$ [[131](#bib.bib131)],
    or propagate masks in a spatiotemporal MRF model to improve temporal coherency [[125](#bib.bib125)].
    Some researchers leverage location-aware embeddings to sharpen the feature${}_{\!}$ [[135](#bib.bib135)],
    or directly learn sequence-to-sequence mask propagation [[95](#bib.bib95)]. Advanced
    tracking techniques are also exploited in [[126](#bib.bib126), [140](#bib.bib140),
    [157](#bib.bib157), [155](#bib.bib155)]. Propagation-based methods are found to
    easily suffer from error accumulation due to occlusions and drifts during mask
    propagation. Conditioning propagation on the initial frame-mask pair${}_{\!}$ [[127](#bib.bib127),
    [141](#bib.bib141), [148](#bib.bib148)] seems a feasible${}_{\!}$ solution${}_{\!}$
    to${}_{\!}$ this.${}_{\!}$ Although${}_{\!}$ target-specific${}_{\!}$ mask${}_{\!}$
    is${}_{\!}$ exp-
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重新识别以检索长时间遮挡后的丢失物体[[134](#bib.bib134)]，设计一个强化学习代理来处理SVOS作为条件决策过程[[131](#bib.bib131)]，或在时空MRF模型中传播掩模以提高时间连贯性[[125](#bib.bib125)]。一些研究人员利用位置感知嵌入来锐化特征[[135](#bib.bib135)]，或直接学习序列到序列的掩模传播[[95](#bib.bib95)]。先进的跟踪技术也被应用于[[126](#bib.bib126),
    [140](#bib.bib140), [157](#bib.bib157), [155](#bib.bib155)]。基于传播的方法容易因遮挡和掩模传播过程中的漂移而导致误差积累。将传播条件化为初始帧掩模对[[127](#bib.bib127),
    [141](#bib.bib141), [148](#bib.bib148)]似乎是一个可行的解决方案。虽然目标特定掩模是exp-
- en: licitly encoded into the segmentation network, making up for${}_{\!}$ the${}_{\!}$
    deficiencies${}_{\!}$ of${}_{\!}$ fine-tuning${}_{\!}$ methods${}_{\!}$ to${}_{\!}$
    a${}_{\!}$ certain${}_{\!}$ extent,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 合法地编码进分割网络中，弥补了细调方法在一定程度上的不足，
- en: propagation-based${}_{\!}$ methods still embed object appearance into hidden
    network weights. Clearly, such implicit target-appearance modeling strategy hurts
    flexibility and adaptivity (while${}_{\!}$ [[142](#bib.bib142)] is an exception
    – a generative model of target
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基于传播的方法仍然将对象外观嵌入到隐藏的网络权重中。显然，这种隐式目标外观建模策略会影响灵活性和适应性（虽然[[142](#bib.bib142)]是一个例外——目标生成模型
- en: and background is explicitly built to aid mask propagation).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 背景明确地构建以帮助掩模传播）。
- en: 'tionally considers first-frame annotations, but easily fails in challenging
    scenes without human feedback. Moreover, the first-frame annotations are typically
    detailed masks, which are tedious to acquire: 79 seconds per instance for coarse
    polygon annotations of COCO [[123](#bib.bib123)], and much more for higher quality.
    Thus performing  VOS  in  the  interactive'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在互动传播中，通常考虑第一帧注释，但在没有人工反馈的挑战性场景中容易失败。此外，第一帧注释通常是详细的掩模，获取起来很繁琐：对于COCO的粗略多边形注释，每实例79秒，而高质量的则需要更多时间。因此，在交互式传播中执行VOS
- en: 'TABLE III: Summary of essential characteristics for reviewed IVOS methods (§[3.1.3](#S3.SS1.SSS3
    "3.1.3 Interactive Video Object Segmentation (IVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：回顾的IVOS方法的基本特征总结（§[3.1.3](#S3.SS1.SSS3 "3.1.3 Interactive Video Object
    Segmentation (IVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based
    Video Segmentation ‣ A Survey on Deep Learning Technique for Video Segmentation")）。
- en: '[t] Year Method Pub. Core Architecture Technical Feature Training Dataset 2017
    IIW [[181](#bib.bib181)] - FCN Interaction-Propagation PASCAL VOC 2012 [[103](#bib.bib103)]
    2018 BFVOS [[182](#bib.bib182)] CVPR FCN Pixel-wise Retrieval DAVIS[16] [[17](#bib.bib17)]
    2019 IVS [[183](#bib.bib183)] CVPR FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    2020 MANet [[184](#bib.bib184)] CVPR Siamese FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]
    ATNet [[185](#bib.bib185)] ECCV FCN Interaction-Propagation SBD + DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    ScribbleBox [[186](#bib.bib186)] ECCV GCN Interaction-Propagation COCO [[123](#bib.bib123)]
    + ImageNet VID [[67](#bib.bib67)] + YouTube-VOS [[95](#bib.bib95)] 2021 IVOS-W [[187](#bib.bib187)]
    CVPR FCN + RL Keyframe Selection DAVIS[17] [[81](#bib.bib81)] GIS [[188](#bib.bib188)]
    CVPR FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    MiVOS [[189](#bib.bib189)] CVPR Memory Network Interaction-Propagation BL30K [[189](#bib.bib189)]+DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)]'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[t] 年份 方法 期刊 核心架构 技术特点 训练数据集 2017 IIW [[181](#bib.bib181)] - FCN 交互传播 PASCAL
    VOC 2012 [[103](#bib.bib103)] 2018 BFVOS [[182](#bib.bib182)] CVPR FCN 像素级检索 DAVIS[16] [[17](#bib.bib17)]
    2019 IVS [[183](#bib.bib183)] CVPR FCN 交互传播 DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    2020 MANet [[184](#bib.bib184)] CVPR 孪生 FCN 交互传播 DAVIS[17] [[81](#bib.bib81)]
    ATNet [[185](#bib.bib185)] ECCV FCN 交互传播 SBD + DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    ScribbleBox [[186](#bib.bib186)] ECCV GCN 交互传播 COCO [[123](#bib.bib123)] + ImageNet
    VID [[67](#bib.bib67)] + YouTube-VOS [[95](#bib.bib95)] 2021 IVOS-W [[187](#bib.bib187)]
    CVPR FCN + RL 关键帧选择 DAVIS[17] [[81](#bib.bib81)] GIS [[188](#bib.bib188)] CVPR
    FCN 交互传播 DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)] MiVOS [[189](#bib.bib189)]
    CVPR 记忆网络 交互传播 BL30K [[189](#bib.bib189)]+DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]'
- en: $\bullet$${}_{\!}$ Matching-based${}_{\!}$ Methods.${}_{\!}$ This type of methods,
    might the most promising SVOS solution so far, constructs an embedding space to
    memorize the initial object embeddings, and classifies each pixel’s label according
    to their similarities to the target object in the embedding space. Thus the initial
    object appearance is explicitly modeled, and test-time fine-tuning is not needed.
    The earliest effort in this direction can be tracked back to${}_{\!}$ [[121](#bib.bib121)].
    Inspired by the advance in visual tracking [[190](#bib.bib190)], Yoon *et al*. [[121](#bib.bib121)]
    proposed a Siamese network to perform pixel-level matching between the first frame
    and
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$${}_{\!}$ 基于匹配的${}_{\!}$ 方法。${}_{\!}$ 这种方法可能是迄今为止最有前景的 SVOS 解决方案，通过构建一个嵌入空间来记住初始对象的嵌入，并根据像素在嵌入空间中与目标对象的相似性来对每个像素的标签进行分类。因此，初始对象的外观被显式建模，测试时不需要微调。这方面的最早努力可以追溯到${}_{\!}$ [[121](#bib.bib121)]。受到视觉跟踪进展的启发 [[190](#bib.bib190)]，Yoon
    *et al*. [[121](#bib.bib121)] 提出了一个孪生网络，以在第一帧和即将到来的帧之间进行像素级匹配。
- en: 'upcoming frames. Later, [[126](#bib.bib126)] proposed to learn an embedding
    space from the first-frame supervision and pose VOS as a task of pixel retrieval:
    pixels are simply their respective nearest neighbors in the learned embedding
    space. The idea of [[121](#bib.bib121)] is also explored in [[133](#bib.bib133)],
    while it computes two ma-'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，[[126](#bib.bib126)] 提出了从第一帧监督中学习嵌入空间，并将 VOS 视为像素检索任务：像素在学习的嵌入空间中简单地是它们各自的最近邻。[[121](#bib.bib121)]
    的想法也在 [[133](#bib.bib133)] 中得到了探索，虽然它计算了两个 ma-
- en: tching maps for each upcoming frame, with respect to the foreground and background
    annotated in the first frame. In [[139](#bib.bib139)], pixel-level similarities
    computed from the first frame and from the previous frame are used as a guide
    to segment succeeding frames. Later, many matching-based solutions were proposed [[145](#bib.bib145),
    [191](#bib.bib191)], perhaps most notably Oh *et al*., who propose a space-time
    memory (STM) model to explicitly store previously computed segmentation information
    in an external memory [[149](#bib.bib149)]. The memory facilitates learning the
    evolution of objects over time and allows for comprehensive use of past segmentation
    cues even over long period of time. Almost all current top-leading SVOS solutions [[159](#bib.bib159),
    [163](#bib.bib163)] are built upon STM; they improve the target adaption ability [[158](#bib.bib158),
    [100](#bib.bib100), [164](#bib.bib164)], incorporate local temporal continuity [[162](#bib.bib162),
    [173](#bib.bib173), [172](#bib.bib172)], explore instance-aware cues [[171](#bib.bib171)],
    and develop more efficient memory designs [[165](#bib.bib165), [161](#bib.bib161),
    [154](#bib.bib154), [168](#bib.bib168)]. Recently, [[170](#bib.bib170)] introduced
    a Transformer [[192](#bib.bib192)] based model, which performs matching-like computation
    through attending over a history of multiple frames. In general, matching-based
    solutions enjoy the advantage of flexible and differentiable model design as well
    as long-term correspondence modeling. On the other hand, feature matching relies
    on a powerful and generic feature embedding, which may limit its performance in
    challenging scenarios.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对每一帧进行匹配，参考在第一帧中标注的前景和背景。在[[139](#bib.bib139)]中，从第一帧和前一帧计算的像素级相似性被用作分割后续帧的指南。后来，提出了许多基于匹配的解决方案[[145](#bib.bib145),
    [191](#bib.bib191)]，其中最著名的可能是*Oh et al*。他们提出了一种时空记忆（STM）模型，用于在外部存储器中显式地存储以前计算的分割信息[[149](#bib.bib149)]。这种记忆有助于学习物体随时间的演变，并允许在较长时间内全面使用过去的分割线索。几乎所有当前领先的SVOS解决方案[[159](#bib.bib159),
    [163](#bib.bib163)]都是建立在STM之上的；它们提高了目标适应能力[[158](#bib.bib158), [100](#bib.bib100),
    [164](#bib.bib164)]，结合了局部时间连续性[[162](#bib.bib162), [173](#bib.bib173), [172](#bib.bib172)]，探索了实例感知线索[[171](#bib.bib171)]，并开发了更高效的记忆设计[[165](#bib.bib165),
    [161](#bib.bib161), [154](#bib.bib154), [168](#bib.bib168)]。最近，[[170](#bib.bib170)]介绍了一种基于Transformer[[192](#bib.bib192)]的模型，通过对多个帧的历史进行关注来执行类似匹配的计算。总体而言，基于匹配的解决方案具有灵活且可微的模型设计以及长期对应建模的优势。另一方面，特征匹配依赖于强大且通用的特征嵌入，这可能限制其在挑战性场景中的表现。
- en: It is also worth mentioning that, as an effective technique for target-specific
    model learning, online learning is applied by many propagation [[117](#bib.bib117),
    [130](#bib.bib130), [125](#bib.bib125), [95](#bib.bib95), [140](#bib.bib140)]
    and matching [[121](#bib.bib121), [145](#bib.bib145), [134](#bib.bib134)] methods
    to boost performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，作为一种有效的目标特定模型学习技术，在线学习被许多传播[[117](#bib.bib117), [130](#bib.bib130),
    [125](#bib.bib125), [95](#bib.bib95), [140](#bib.bib140)]和匹配[[121](#bib.bib121),
    [145](#bib.bib145), [134](#bib.bib134)]方法应用，以提升性能。
- en: $\bullet$ Box-initialization based Methods. As pixel-wise annotations are time-consuming
    or even impractical to acquire in realistic scenes, some work has considered the
    situation where the first-frame annotation is provided in the form of a bounding
    box. Specifically, in [[143](#bib.bib143)], Siamese trackers are augmented with
    a mask prediction branch. In [[156](#bib.bib156)], reinforcement learning is introduced
    to make decisions for target updating and matching. Later, in [[167](#bib.bib167)],
    an outside memory is utilized to build a stronger Siamese track-segmenter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于框初始化的方法。由于像素级标注在现实场景中耗时或甚至不切实际，因此一些研究考虑了第一帧标注以边界框形式提供的情况。具体而言，在[[143](#bib.bib143)]中，Siamese跟踪器通过增加一个掩码预测分支来增强。在[[156](#bib.bib156)]中，引入了强化学习来为目标更新和匹配做决策。后来，在[[167](#bib.bib167)]中，利用外部记忆构建了更强大的Siamese跟踪分割器。
- en: $\bullet$ Un-/Weakly-supervised based Methods. To alleviate the demand for large-scale,
    pixel-wise annotated training samples, several un-/weakly-supervised learning-based
    SVOS solutions were recently developed. They are typically built as${}_{\!}$ a${}_{\!}$
    reconstruction${}_{\!}$ scheme${}_{\!}$ (*i.e*.,${}_{\!}$ each${}_{\!}$ pixel${}_{\!}$
    from${}_{\!}$ a${}_{\!}$ ‘query’ frame is reconstructed by finding and assembling
    related pixels from adjacent frame(s)) [[136](#bib.bib136), [160](#bib.bib160),
    [193](#bib.bib193)], and/or adopt a cycle-consistent tracking paradigm (*i.e*.,
    pixels/patches are encouraged to fall into the same location after one cycle of
    forward and backward tracking) [[150](#bib.bib150), [152](#bib.bib152), [98](#bib.bib98),
    [174](#bib.bib174)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 无监督/弱监督方法。为了减少对大规模像素级注释训练样本的需求，最近开发了几种无监督/弱监督学习基础的 SVOS 解决方案。这些解决方案通常建立在重建方案上（*即*，从一个‘查询’帧重建每一个像素，通过查找并组合来自相邻帧的相关像素）[[136](#bib.bib136),
    [160](#bib.bib160), [193](#bib.bib193)]，和/或采用循环一致性跟踪范式（*即*，鼓励像素/补丁在经过一次前向和后向跟踪后落到相同的位置）[[150](#bib.bib150),
    [152](#bib.bib152), [98](#bib.bib98), [174](#bib.bib174)]。
- en: $\bullet$ Other Specific Methods. Other papers make specific contributions that
    deserve a separate look. In [[147](#bib.bib147)], Zeng *et al*. extract mask proposals
    per frame and formulate the matching between object templates and proposals in
    a differentiable manner. Instead of using only the first frame annotation, [[144](#bib.bib144)]
    learns to select the best frame from the whole video for user interaction, so
    as to boost mask propagation. In [[166](#bib.bib166)], Li *et al*. introduce a
    forward-backward data flow based cycle consistency mechanism to improve both traditional
    SVOS training and offline inference protocols, through mitigating the error propagation
    problem. To accelerate processing speed, a dynamic network [[169](#bib.bib169)]
    is proposed to selectively allocate computation source for each frame according
    to the similarity to the previous frame.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 其他特定方法。其他文献做出了具体贡献，值得单独关注。在 [[147](#bib.bib147)] 中，曾*等*提取了每一帧的掩码提案，并以可微分的方式制定了对象模板与提案之间的匹配。与仅使用第一帧注释不同，[[144](#bib.bib144)]
    学会从整个视频中选择最佳帧进行用户交互，以促进掩码传播。在 [[166](#bib.bib166)] 中，李*等*引入了一种基于前向-后向数据流的循环一致性机制，通过缓解误差传播问题，改进了传统的
    SVOS 训练和离线推理协议。为了加速处理速度，[[169](#bib.bib169)] 提出了一个动态网络，根据与前一帧的相似度选择性地分配计算资源给每一帧。
- en: 3.1.3 Interactive Video Object Segmentation (IVOS)
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 互动视频对象分割（IVOS）
- en: AVOS, without any human involvement, loses flexibility in segmenting arbitrary
    objects of user interest. SVOS addi- setting has gained increasing attention.
    Unlike classic models [[48](#bib.bib48), [49](#bib.bib49), [52](#bib.bib52)] requiring
    extensive and professional user intervention, recent deep learning-based IVOS
    solutions usually work with multiple rounds of scribble supervision, to minimize
    the user’s effort. In this scenario [[194](#bib.bib194)], the user draws scribbles
    on a selected frame and an algorithm computes the segmentation maps for all video
    frames in a batch process. For refinement, user intervention and segmentation
    are repeated. This round-based interaction [[183](#bib.bib183)] is useful for
    consumer-level applications and rapid prototyping for professional usage, where
    efficiency is the main concern. One can control the segmentation quality at the
    expense of time, as more rounds of interaction will provide better results.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: AVOS 在没有任何人工干预的情况下，在分割用户感兴趣的任意对象时失去了灵活性。SVOS 添加设置越来越受到关注。与经典模型 [[48](#bib.bib48),
    [49](#bib.bib49), [52](#bib.bib52)] 需要大量专业用户干预不同，最近基于深度学习的 IVOS 解决方案通常通过多轮涂鸦监督来减少用户的努力。在这种情况下
    [[194](#bib.bib194)]，用户在选择的帧上绘制涂鸦，算法计算批处理过程中所有视频帧的分割图。为了精细化，用户干预和分割会被重复进行。这种轮回式的交互
    [[183](#bib.bib183)] 对于消费者级应用和专业用途中快速原型设计非常有用，其中效率是主要关注点。用户可以在时间成本的代价下控制分割质量，因为更多的交互轮次将提供更好的结果。
- en: '$\bullet$ Interaction-propagation based Methods. The majority of current studies [[188](#bib.bib188),
    [189](#bib.bib189)] follow an interaction-propagation scheme. In the preliminary
    attempt [[181](#bib.bib181)], IVOS is achieved by a simple combination of two
    separate modules: an interactive image segmentation model [[195](#bib.bib195)]
    for producing segmentation based on user annotations; and a SVOS model [[18](#bib.bib18)]
    for propagating masks from the user-annotated frames to the others. Later, [[183](#bib.bib183)]
    devised a more compact solution, with also two modules for interaction and propagation,
    respectively. However, the two modules are internally connected through intermediate
    feature exchanging, and also externally connected, *i.e*., each of them is conditioned
    on the other’s output. In [[185](#bib.bib185)], a similar model design is also
    adopted, however, the propagation part is specifically designed to address both
    local mask tracking (over adjacent frames) and global propagation (among distant
    frames), respectively. However, these techniques [[181](#bib.bib181), [185](#bib.bib185)]
    have to start a new feed-forward computation in each interaction round, making
    them inefficient as the number of rounds grows. A more efficient solution was
    developed in [[184](#bib.bib184)]. The critical idea is to build a common encoder
    for discriminative pixel embedding learning, upon which two small network branches
    are added for interactive segmentation and mask propagation, respectively. Thus
    the model extracts pixel embeddings for all frames only once (in the first round).
    In the following rounds, the feed-forward computation is only made within the
    two shallow branches.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于交互传播的方法。目前大多数研究[[188](#bib.bib188), [189](#bib.bib189)]遵循交互传播方案。在初步尝试中[[181](#bib.bib181)]，通过简单组合两个独立模块实现IVOS：一个基于用户标注的交互图像分割模型[[195](#bib.bib195)]；和一个SVOS模型[[18](#bib.bib18)]，用于将掩码从用户标注的帧传播到其他帧。后来，[[183](#bib.bib183)]设计了一个更紧凑的解决方案，同样有两个模块分别用于交互和传播。然而，这两个模块通过中间特征交换在内部连接，也在外部连接，*即*，每个模块都依赖于另一个模块的输出。在[[185](#bib.bib185)]中，也采用了类似的模型设计，但传播部分专门设计用来处理局部掩码跟踪（在相邻帧之间）和全局传播（在远程帧之间）。然而，这些技术[[181](#bib.bib181),
    [185](#bib.bib185)]必须在每轮交互中启动新的前馈计算，随着轮数的增加，这使得它们效率低下。[[184](#bib.bib184)]中开发了一种更高效的解决方案。关键思想是构建一个用于区分像素嵌入学习的通用编码器，在其基础上添加两个小网络分支，分别用于交互分割和掩码传播。因此，模型只需在第一次轮次中为所有帧提取像素嵌入。在后续轮次中，前馈计算仅在两个浅层分支内进行。
- en: '$\bullet$ Other Methods. Chen *et al*. [[182](#bib.bib182)] propose a pixel
    embedding learning-based model, applicable to both SVOS and IVOS. With a similar
    idea of [[126](#bib.bib126)], IVOS is formulated as a pixel-wise retrieval problem,
    *i.e*., transferring labels to each pixel according to its nearest reference pixel.
    This model supports different kinds of user input, such as masks, clicks and scribbles,
    and can provide immediate feedback after user interaction. In [[186](#bib.bib186)],
    an interactive annotation tool is proposed for VOS. The annotation has two phases:
    annotating objects with tracked boxes, and labeling masks inside these tracks.
    Box tracks are annotated efficiently by approximating the trajectory using a parametric
    curve with a small number of control points which the annotator can interactively
    correct. Segmentation masks are corrected via scribbles which are propagated through
    time. In [[187](#bib.bib187)], a reinforcement learning framework is exploited
    to automatically determine the most valuable frame for interaction.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 其他方法。陈*等*[[182](#bib.bib182)] 提出了一个基于像素嵌入学习的模型，适用于SVOS和IVOS。与[[126](#bib.bib126)]类似，该模型将IVOS表述为一个像素级的检索问题，*即*，根据每个像素的最近参考像素转移标签。该模型支持不同类型的用户输入，如掩码、点击和涂鸦，并且能够在用户互动后提供即时反馈。在[[186](#bib.bib186)]中，提出了一种用于VOS的交互式标注工具。标注分为两个阶段：用跟踪框标注对象，以及在这些跟踪框内标注掩码。通过使用参数曲线近似轨迹来有效标注框轨迹，注释者可以交互地进行修正。分割掩码通过时间传播的涂鸦进行修正。在[[187](#bib.bib187)]中，利用强化学习框架自动确定最有价值的交互帧。
- en: 'TABLE IV: ​​Summary of characteristics for reviewed LVOS methods (§[3.1.4](#S3.SS1.SSS4
    "3.1.4 Language-guided Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).​​​​'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 综述 LVOS 方法的特点 (§[3.1.4](#S3.SS1.SSS4 "3.1.4 Language-guided Video Object
    Segmentation (LVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based
    Video Segmentation ‣ A Survey on Deep Learning Technique for Video Segmentation"))。'
- en: '|   Year | Method | Pub. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 |'
- en: '&#124; Visual + Language &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉 + 语言 &#124;'
- en: '&#124; Encoder &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器 &#124;'
- en: '| Technical Feature | Training Dataset |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 技术特征 | 训练数据集 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2018 | A2DS [[196](#bib.bib196)] | CVPR | I3D + CNN | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | A2DS [[196](#bib.bib196)] | CVPR | I3D + CNN | 动态卷积 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| LangVOS [[197](#bib.bib197)] | ACCV | CNN + CNN | Cross-modal Att. | DAVIS[17]-RVOS [[196](#bib.bib196)]
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| LangVOS [[197](#bib.bib197)] | ACCV | CNN + CNN | 跨模态注意力 | DAVIS[17]-RVOS [[196](#bib.bib196)]
    |'
- en: '| 2019 | AAN [[198](#bib.bib198)] | ICCV | I3D + CNN | Cross-modal Att. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | AAN [[198](#bib.bib198)] | ICCV | I3D + CNN | 跨模态注意力 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| 2020 | CDNet [[199](#bib.bib199)] | AAAI | I3D + GRU | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CDNet [[199](#bib.bib199)] | AAAI | I3D + GRU | 动态卷积 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| PolarRPE [[200](#bib.bib200)] | IJCAI | I3D + LSTM | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| PolarRPE [[200](#bib.bib200)] | IJCAI | I3D + LSTM | 动态卷积 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| VT-Capsule [[201](#bib.bib201)] | CVPR | I3D + CNN | Capsule Routing | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| VT-Capsule [[201](#bib.bib201)] | CVPR | I3D + CNN | 胶囊路由 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| URVOS [[202](#bib.bib202)] | ECCV | CNN + MLP | Cross-modal Att. | Refer-YouTube-VOS [[202](#bib.bib202)]
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| URVOS [[202](#bib.bib202)] | ECCV | CNN + MLP | 跨模态注意力 | Refer-YouTube-VOS [[202](#bib.bib202)]
    |'
- en: '| 2021 | CST [[203](#bib.bib203)] | CVPR | I3D + GRU | Cross-modal Att. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | CST [[203](#bib.bib203)] | CVPR | I3D + GRU | 跨模态注意力 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: '| CMSANet [[204](#bib.bib204)] | PAMI | CNN + Word embed. | Cross-modal Att.
    | A2D Sentences [[196](#bib.bib196)] |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| CMSANet [[204](#bib.bib204)] | PAMI | CNN + 词嵌入 | 跨模态注意力 | A2D Sentences [[196](#bib.bib196)]
    |'
- en: 'TABLE V: Summary of essential characteristics for reviewed VSS methods (§[3.2](#S3.SS2
    "3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")). Flow indicates
    whether optical flow is used.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 综述 VSS 方法的基本特征 (§[3.2](#S3.SS2 "3.2 Deep Learning-based VSS Models ‣ 3
    Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for
    Video Segmentation"))。流动指示是否使用了光流。'
- en: '|   Year | Method | Pub. | Seg. Level | Core Architecture | Flow | Technical
    Feature | Training Dataset |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 分割层级 | 核心架构 | 流动 | 技术特征 | 训练数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2016 | Clockwork [[205](#bib.bib205)] | ECCV | Semantic | FCN | ✓ | Faster
    Segmentation | Cityscapes [[206](#bib.bib206)]/YouTube-Objects [[73](#bib.bib73)]
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Clockwork [[205](#bib.bib205)] | ECCV | 语义 | FCN | ✓ | 更快的分割 | Cityscapes [[206](#bib.bib206)]/YouTube-Objects [[73](#bib.bib73)]
    |'
- en: '| FSO [[207](#bib.bib207)] | CVPR | Semantic | FCN + Dense CRF | ✓ | Temporal
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| FSO [[207](#bib.bib207)] | CVPR | 语义 | FCN + Dense CRF | ✓ | 时间特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| JFS [[209](#bib.bib209)] | ECCV | Semantic | FCN | ✓ | Temporal Feature Aggregation
    | KITTI MOTS [[210](#bib.bib210)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| JFS [[209](#bib.bib209)] | ECCV | 语义 | FCN | ✓ | 时间特征聚合 | KITTI MOTS [[210](#bib.bib210)]
    |'
- en: '| 2017 | BANet [[211](#bib.bib211)] | CVPR | Semantic | FCN + LSTM |  | Keyframe
    Selection | CamVid [[208](#bib.bib208)]/KITTI |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | BANet [[211](#bib.bib211)] | CVPR | 语义 | FCN + LSTM |  | 关键帧选择 | CamVid [[208](#bib.bib208)]/KITTI
    |'
- en: '| PEARL [[212](#bib.bib212)] | ICCV | Semantic | FCN | ✓ | Flow-guided Feature
    Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PEARL [[212](#bib.bib212)] | ICCV | 语义 | FCN | ✓ | 流指导特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| NetWarp [[213](#bib.bib213)] | ICCV | Semantic | Siamese FCN | ✓ | Flow-guided
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| NetWarp [[213](#bib.bib213)] | ICCV | 语义 | Siamese FCN | ✓ | 流指导特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| DFF [[214](#bib.bib214)] | ICCV | Semantic | FCN |  | Flow-guided Feature
    Aggregation | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| DFF [[214](#bib.bib214)] | ICCV | 语义 | FCN |  | 流指导特征聚合 | Cityscapes [[206](#bib.bib206)]
    |'
- en: '| BBF [[215](#bib.bib215)] | ICCV | Semantic | Two-Stream FCN | ✓ | Weakly-Supervised
    Learning | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| BBF [[215](#bib.bib215)] | ICCV | 语义 | 双流 FCN | ✓ | 弱监督学习 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| 2018 | GRFP [[216](#bib.bib216)] | CVPR | Semantic | FCN + GRU | ✓ | Temporal
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | GRFP [[216](#bib.bib216)] | CVPR | 语义 | FCN + GRU | ✓ | 时间特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| LVS [[217](#bib.bib217)] | CVPR | Semantic | FCN |  | Keyframe Selection
    | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LVS [[217](#bib.bib217)] | CVPR | 语义 | FCN |  | 关键帧选择 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| DVSN [[218](#bib.bib218)] | CVPR | Semantic | FCN+RL | ✓ | Keyframe Selection
    | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| DVSN [[218](#bib.bib218)] | CVPR | 语义 | FCN+RL | ✓ | 关键帧选择 | Cityscapes [[206](#bib.bib206)]
    |'
- en: '| EUVS [[219](#bib.bib219)] | ECCV | Semantic | Bayesian CNN | ✓ | Flow-guided
    Feature Aggregation | CamVid [[208](#bib.bib208)] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| EUVS [[219](#bib.bib219)] | ECCV | 语义 | 贝叶斯 CNN | ✓ | 流引导特征聚合 | CamVid [[208](#bib.bib208)]
    |'
- en: '| GCRF [[220](#bib.bib220)] | CVPR | Semantic | FCN+CRF | ✓ | Gaussian CRF
    | CamVid [[208](#bib.bib208)] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| GCRF [[220](#bib.bib220)] | CVPR | 语义 | FCN+CRF | ✓ | 高斯 CRF | CamVid [[208](#bib.bib208)]
    |'
- en: '| 2019 | Accel [[221](#bib.bib221)] | CVPR | Semantic | FCN | ✓ | Keyframe
    Selection | KITTI |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Accel [[221](#bib.bib221)] | CVPR | 语义 | FCN | ✓ | 关键帧选择 | KITTI |'
- en: '| SSeg [[222](#bib.bib222)] | CVPR | Semantic | FCN |  | Weakly-Supervised
    Learning | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SSeg [[222](#bib.bib222)] | CVPR | 语义 | FCN |  | 弱监督学习 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| MOTS [[210](#bib.bib210)] | CVPR | Instance | Mask R-CNN |  | Tracking by
    Detection | KITTI MOTS [[210](#bib.bib210)] /MOTSChallenge [[210](#bib.bib210)]
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| MOTS [[210](#bib.bib210)] | CVPR | 实例 | Mask R-CNN |  | 通过检测跟踪 | KITTI MOTS [[210](#bib.bib210)]
    /MOTSChallenge [[210](#bib.bib210)] |'
- en: '| MaskTrack R-CNN [[82](#bib.bib82)] | ICCV | Instance | Mask R-CNN |  | Tracking
    by Detection | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| MaskTrack R-CNN [[82](#bib.bib82)] | ICCV | 实例 | Mask R-CNN |  | 通过检测跟踪 |
    YouTube-VIS [[82](#bib.bib82)] |'
- en: '| 2020 | EFC [[223](#bib.bib223)] | AAAI | Semantic | FCN | ✓ | Temporal Feature
    Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | EFC [[223](#bib.bib223)] | AAAI | 语义 | FCN | ✓ | 时间特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| TDNet [[224](#bib.bib224)] | CVPR | Semantic | Memory Network |  | Attention-based
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]/NYUDv2 [[225](#bib.bib225)]
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| TDNet [[224](#bib.bib224)] | CVPR | 语义 | 记忆网络 |  | 基于注意力的特征聚合 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]/NYUDv2 [[225](#bib.bib225)]
    |'
- en: '| MaskProp [[226](#bib.bib226)] | CVPR | Instance | Mask R-CNN |  | Instance
    Feature Propagation | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| MaskProp [[226](#bib.bib226)] | CVPR | 实例 | Mask R-CNN |  | 实例特征传播 | YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| VPS [[227](#bib.bib227)] | CVPR | Panoptic | Mask R-CNN |  | Spatio-Temporal
    Feature Alignment | VIPER-VPS [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)]
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| VPS [[227](#bib.bib227)] | CVPR | 全景 | Mask R-CNN |  | 时空特征对齐 | VIPER-VPS [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)]
    |'
- en: '| MOTSNet [[228](#bib.bib228)] | CVPR | Instance | Mask R-CNN |  | Unsupervised
    Learning | KITTI MOTS [[210](#bib.bib210)] /BDD100K [[229](#bib.bib229)] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| MOTSNet [[228](#bib.bib228)] | CVPR | 实例 | Mask R-CNN |  | 无监督学习 | KITTI
    MOTS [[210](#bib.bib210)] /BDD100K [[229](#bib.bib229)] |'
- en: '| MVAE [[230](#bib.bib230)] | CVPR | Instance | Mask R-CNN+VAE |  | Variational
    Inference | KITTI MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| MVAE [[230](#bib.bib230)] | CVPR | 实例 | Mask R-CNN+VAE |  | 变分推断 | KITTI
    MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)] |'
- en: '| ETC [[231](#bib.bib231)] | ECCV | Semantic | FCN + KD | ✓ | Knowledge Distillation
    | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ETC [[231](#bib.bib231)] | ECCV | 语义 | FCN + KD | ✓ | 知识蒸馏 | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
- en: '| Sipmask [[232](#bib.bib232)] | ECCV | Instance | FCOS |  | Single-Stage Segmentation
    | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Sipmask [[232](#bib.bib232)] | ECCV | 实例 | FCOS |  | 单阶段分割 | YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| STEm-Seg [[233](#bib.bib233)] | ECCV | Instance | FCN |  | Spatio-Temporal
    Embedding Learning | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]/KITTI-MOTS [[210](#bib.bib210)]
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| STEm-Seg [[233](#bib.bib233)] | ECCV | 实例 | FCN |  | 时空嵌入学习 | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]/KITTI-MOTS [[210](#bib.bib210)]
    |'
- en: '| Naive-Student [[234](#bib.bib234)] | ECCV | Semantic | FCN+KD |  | Semi-Supervised
    Learning | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Naive-Student [[234](#bib.bib234)] | ECCV | 语义 | FCN+KD |  | 半监督学习 | Cityscapes [[206](#bib.bib206)]
    |'
- en: '| 2021 | CompFeat [[235](#bib.bib235)] | AAAI | Instance | Mask R-CNN |  |
    Spatio-Temporal Feature Alignment | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | CompFeat [[235](#bib.bib235)] | AAAI | 实例 | Mask R-CNN |  | 空间-时间特征对齐
    | YouTube-VIS [[82](#bib.bib82)] |'
- en: '| TraDeS [[236](#bib.bib236)] | CVPR | Instance | Siamese FCN |  | Tracking
    by Detection | MOT/nuScenes/KITTI MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| TraDeS [[236](#bib.bib236)] | CVPR | 实例 | Siamese FCN |  | 通过检测进行跟踪 | MOT/nuScenes/KITTI
    MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)] |'
- en: '| SG-Net [[237](#bib.bib237)] | CVPR | Instance | FCOS |  | Single-Stage Segmentation
    | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| SG-Net [[237](#bib.bib237)] | CVPR | 实例 | FCOS |  | 单阶段分割 | YouTube-VIS [[82](#bib.bib82)]
    |'
- en: '| VisTR [[238](#bib.bib238)] | CVPR | Instance | Transformer |  | Transformer-based
    Segmentation | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| VisTR [[238](#bib.bib238)] | CVPR | 实例 | Transformer |  | 基于 Transformer
    的分割 | YouTube-VIS [[82](#bib.bib82)] |'
- en: '| SSDE [[239](#bib.bib239)] | CVPR | Semantic | FCN |  | Semi-Supervised Learning
    | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| SSDE [[239](#bib.bib239)] | CVPR | 语义 | FCN |  | 半监督学习 | Cityscapes [[206](#bib.bib206)]
    |'
- en: '| SiamTrack [[240](#bib.bib240)] | CVPR | Panoptic | Siamese FCN |  | Supervised
    Contrastive Learning | VIPER-VPS [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)]
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SiamTrack [[240](#bib.bib240)] | CVPR | 全景 | Siamese FCN |  | 监督对比学习 | VIPER-VPS
    [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)] |'
- en: '| ViP-DeepLab [[241](#bib.bib241)] | CVPR | Panoptic | FCN |  | Depth-Aware
    Panoptic Segmentation | Cityscapes-VPS [[227](#bib.bib227)] |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| ViP-DeepLab [[241](#bib.bib241)] | CVPR | 全景 | FCN |  | 深度感知全景分割 | Cityscapes-VPS
    [[227](#bib.bib227)] |'
- en: '| fIRN [[242](#bib.bib242)] | CVPR | Instance | Mask R-CNN | ✓ | Weakly-Supervised
    Learning | YouTube-VIS [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| fIRN [[242](#bib.bib242)] | CVPR | 实例 | Mask R-CNN | ✓ | 弱监督学习 | YouTube-VIS
    [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
- en: '| SemiTrack [[243](#bib.bib243)] | CVPR | Instance | SOLO |  | Semi-Supervised
    Learning | YouTube-VIS [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| SemiTrack [[243](#bib.bib243)] | CVPR | 实例 | SOLO |  | 半监督学习 | YouTube-VIS
    [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
- en: '| Propose-Reduce [[244](#bib.bib244)] | ICCV | Instance | Mask R-CNN |  | Propose
    and Reduce | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Propose-Reduce [[244](#bib.bib244)] | ICCV | 实例 | Mask R-CNN |  | 提议与减少 |
    DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)] |'
- en: '| CrossVIS [[245](#bib.bib245)] | ICCV | Instance | FCN |  | Dynamic Convolution
    | YouTube-VIS [[82](#bib.bib82)]/OVIS [[246](#bib.bib246)] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CrossVIS [[245](#bib.bib245)] | ICCV | 实例 | FCN |  | 动态卷积 | YouTube-VIS [[82](#bib.bib82)]/OVIS
    [[246](#bib.bib246)] |'
- en: 3.1.4 Language-guided${}_{\!}$ Video${}_{\!}$ Object${}_{\!}$ Segmentation${}_{\!}$
    (LVOS)${}_{\!\!\!}$
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 语言引导${}_{\!}$ 视频${}_{\!}$ 目标${}_{\!}$ 分割${}_{\!}$ (LVOS)${}_{\!\!\!}$
- en: LVOS is an emerging area, dating back to 2018 [[196](#bib.bib196), [197](#bib.bib197)].
    Although there have already existed some efforts [[247](#bib.bib247)] in the intersection
    of language and video understanding, none of them addresses pixel-level video-language
    reasoning. Most efforts in LVOS are made around the theme of efficient alignment
    between visual and linguistic modalities. According to the multi-modal information
    fusion strategy, existing models can be divided into three groups.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: LVOS 是一个新兴领域，始于 2018 年[[196](#bib.bib196), [197](#bib.bib197)]。尽管在语言和视频理解的交叉领域已有一些努力[[247](#bib.bib247)]，但没有一个解决像素级别的视频语言推理。LVOS
    领域的大多数工作都围绕视觉和语言模态之间的高效对齐展开。根据多模态信息融合策略，现有模型可以分为三组。
- en: $\bullet$ Dynamic Convolution-based Methods. The first initiate was proposed
    in​ [[196](#bib.bib196)] that applies dynamic networks​ [[248](#bib.bib248)] for
    visual-language relation modeling. Specifically, convolution filters, dynamically
    generated from linguistic query, are used to adaptively transform visual features
    into desired segments. In the same line of work, [[199](#bib.bib199), [200](#bib.bib200)]
    incorporate spatial context into filter generation. However, as indicated by [[198](#bib.bib198)],
    linguistic variation of input description may greatly impact sentence representation
    and subsequently make dynamic filters unstable, causing inaccurate segmentation.
    For example, “car in blue is parked on the grass” and “blue car standing on the
    grass” have the same meaning but different generated filters, leading to poor
    performance.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于动态卷积的方法。最早的尝试是在[[196](#bib.bib196)]中提出的，使用动态网络[[248](#bib.bib248)]进行视觉-语言关系建模。具体而言，动态生成的卷积滤波器来自语言查询，用于自适应地将视觉特征转化为期望的分割。在相同的研究方向中，[[199](#bib.bib199),
    [200](#bib.bib200)]将空间上下文纳入滤波器生成。然而，如[[198](#bib.bib198)]所示，输入描述的语言变化可能对句子表示产生重大影响，从而使动态滤波器不稳定，导致分割不准确。例如，“蓝色的车停在草地上”和“停在草地上的蓝色车”具有相同的意思，但生成的滤波器不同，导致性能较差。
- en: $\bullet$ Capsule Routing-based Methods. In [[201](#bib.bib201)], both video
    and textual inputs are encoded through capsules [[249](#bib.bib249)], which are
    considered effective in modeling visual/textual entities. Then, dynamic routing
    is applied over the video and text capsules for visual-textual information integration.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于胶囊路由的方法。在[[201](#bib.bib201)]中，视频和文本输入通过胶囊进行编码[[249](#bib.bib249)]，这些胶囊被认为在建模视觉/文本实体方面有效。然后，对视频和文本胶囊应用动态路由进行视觉-文本信息整合。
- en: $\bullet$ Attention-based Methods. Neural attention technique is
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于注意力的方法。神经注意力技术是
- en: also widely adopted in the filed of LVOS${}_{\!}$ [[197](#bib.bib197), [250](#bib.bib250),
    [202](#bib.bib202), [204](#bib.bib204), [251](#bib.bib251)], for fully capturing
    global visual/textual context. In [[198](#bib.bib198)], vision-guided language
    attention and language-guided vision attention were developed to capture visual-textual
    correlations. In [[203](#bib.bib203)], two different attentions are learned to
    ground spatial and temporal relevant linguistic cues to static and dynamic visual
    embeddings, respectively.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 也广泛应用于LVOS${}_{\!}$领域[[197](#bib.bib197), [250](#bib.bib250), [202](#bib.bib202),
    [204](#bib.bib204), [251](#bib.bib251)]，以全面捕捉全球视觉/文本上下文。在[[198](#bib.bib198)]中，开发了视觉引导的语言注意力和语言引导的视觉注意力，以捕捉视觉-文本相关性。在[[203](#bib.bib203)]中，学习了两种不同的注意力机制，分别用于将空间和时间相关的语言线索与静态和动态视觉嵌入对接。
- en: 3.2 Deep Learning-based VSS Models
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于深度学习的VSS模型
- en: Video semantic segmentation aims to group pixels with different semantics (*e.g*.,
    category or instance membership), where different semantics result in different
    types of segmentation tasks, such as (instance-agnostic) video semantic segmentation
    (VSS, §[3.2.1](#S3.SS2.SSS1 "3.2.1 (Instance-agnostic) Video Semantic Segmentation
    (VSS) ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), video instance
    segmentation (VIS, §[3.2.2](#S3.SS2.SSS2 "3.2.2 Video Instance Segmentation (VIS)
    ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")) and video panoptic
    segmentation (VPS, §[3.2.3](#S3.SS2.SSS3 "3.2.3 Video Panoptic Segmentation (VPS)
    ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 视频语义分割旨在将具有不同语义的像素进行分组（*例如*，类别或实例归属），不同的语义会导致不同类型的分割任务，例如（与实例无关的）视频语义分割（VSS，§[3.2.1](#S3.SS2.SSS1
    "3.2.1 (与实例无关的)视频语义分割 (VSS) ‣ 3.2 基于深度学习的VSS模型 ‣ 3 基于深度学习的视频分割 ‣ 视频分割的深度学习技术综述")）、视频实例分割（VIS，§[3.2.2](#S3.SS2.SSS2
    "3.2.2 视频实例分割 (VIS) ‣ 3.2 基于深度学习的VSS模型 ‣ 3 基于深度学习的视频分割 ‣ 视频分割的深度学习技术综述")）和视频全景分割（VPS，§[3.2.3](#S3.SS2.SSS3
    "3.2.3 视频全景分割 (VPS) ‣ 3.2 基于深度学习的VSS模型 ‣ 3 基于深度学习的视频分割 ‣ 视频分割的深度学习技术综述")）。
- en: 3.2.1 ${}_{\!\!\!}$(Instance-agnostic)${}_{\!}$ Video${}_{\!}$ Semantic${}_{\!}$
    Segmentation${}_{\!}$ (VSS)${}_{\!\!\!\!\!\!\!}$
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 ${}_{\!\!\!}$(与实例无关的)${}_{\!}$ 视频${}_{\!}$ 语义${}_{\!}$ 分割${}_{\!}$ (VSS)${}_{\!\!\!\!\!\!\!}$
- en: Extending the success of deep learning-based image semantic segmentation techniques
    to the video domain has become a research focus in computer vision recently. To
    achieve this, the most straightforward strategy is the naïve application of an
    image semantic segmentation model in a frame-by-frame manner. But this strategy
    completely ignores temporal continuity and coherence cues provided in videos.
    To make better use of temporal information, research efforts in this field are
    mainly made along two lines.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习基础的图像语义分割技术扩展到视频领域已成为计算机视觉领域的研究重点。要实现这一点，最直接的策略是以逐帧的方式应用图像语义分割模型。但这种策略完全忽略了视频中提供的时间连续性和一致性线索。为了更好地利用时间信息，该领域的研究主要沿着两个方向进行。
- en: $\bullet$ Efforts towards More Accurate Segmentation. A major stream of methods
    exploits cross-frame relations to boost the prediction accuracy. They typically
    first apply the very same segmentation algorithms to each frame independently.
    Then they add extra modules on top, *e.g*., optical flow-guided feature aggregation [[213](#bib.bib213),
    [212](#bib.bib212), [219](#bib.bib219)], and sequential network based temporal
    information propagation [[216](#bib.bib216)], to gather multi-frame context and
    get better results. For example, in some pioneer work [[209](#bib.bib209), [207](#bib.bib207)],
    after performing static semantic segmentation for each frame individually, optical${}_{\!}$
    flow${}_{\!}$ [[209](#bib.bib209)]${}_{\!}$ or${}_{\!}$ 3D${}_{\!}$ CRF${}_{\!}$ [[207](#bib.bib207)]${}_{\!}$
    based${}_{\!}$ post${}_{\!}$ processing is applied for gaining temporally consistent
    segments. Later, [[220](#bib.bib220)] jointly learns CNN-based per-frame segmentation
    and CRF-based spatio-temporal reasoning. In [[213](#bib.bib213)], features warped
    from previous frames with optical flow are combined with the current frame features
    for prediction. These methods require additional feature aggregation modules,
    which increase the computational costs during the inference. Recently, [[223](#bib.bib223)]
    proposes to only incorporate flow-guided temporal consistency into the training
    phase, without bringing any extra inference cost. But its processing speed is
    still bounded to the adopted per-frame segmentation algorithms, as all features
    must be recomputed at each frame. For these methods, the utility in time-sensitive
    application areas, such as mobile and autonomous driving, is limited.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$  更准确的分割努力。一个主要的研究方向是利用跨帧关系来提高预测准确性。它们通常首先对每一帧独立应用相同的分割算法。然后，添加额外的模块，如*例如*，光流引导的特征聚合
    [[213](#bib.bib213), [212](#bib.bib212), [219](#bib.bib219)]，以及基于序列网络的时间信息传播 [[216](#bib.bib216)]，以收集多帧上下文并获得更好的结果。例如，在一些先驱工作
    [[209](#bib.bib209), [207](#bib.bib207)] 中，在对每一帧分别进行静态语义分割后，应用基于光流 [[209](#bib.bib209)]
    或 3D CRF [[207](#bib.bib207)] 的后处理，以获得时间一致的分段。后来，[[220](#bib.bib220)] 联合学习了基于CNN的逐帧分割和基于CRF的时空推理。在
    [[213](#bib.bib213)] 中，利用光流从之前的帧中变形的特征与当前帧特征进行结合预测。这些方法需要额外的特征聚合模块，从而增加了推理过程中的计算成本。最近，[[223](#bib.bib223)]
    提出了将流引导的时间一致性仅纳入训练阶段，而不增加额外的推理成本。但其处理速度仍受限于所采用的逐帧分割算法，因为所有特征必须在每一帧中重新计算。对于这些方法，其在时间敏感应用领域（如移动设备和自动驾驶）的实用性有限。
- en: $\bullet$ Efforts towards Faster Segmentation. Yet another complementary line
    of work tries to leverage temporal information to accelerate computation. They
    approximate the expensive per-frame forward pass with cheaper alternatives, *i.e*.,
    reusing the features in neighbouring frames. In [[205](#bib.bib205)], parts of
    segmentation networks are adaptively executed across frames, thus reducing the
    computation cost. Later methods use keyframes to avoid processing of each frame,
    and then propagate the outputs or the feature maps to other frames. For instance, [[214](#bib.bib214)]
    employs optical flow to warp the features between the keyframe and non-key frames.
    Adaptive keyframe selection is later exploited in [[211](#bib.bib211), [218](#bib.bib218)],
    further enhanced by adaptive feature propagation [[217](#bib.bib217)]. In [[221](#bib.bib221)],
    Jain *et al*. use a large, strong model to predict the keyframe and use a compact
    one in non-key frames. Keyframe-based methods have different computational loads
    between keyframes and non-key frames, causing high maximum latency and unbalanced
    occupation of computation resources that may decrease system efficiency [[224](#bib.bib224)].
    Additionally, the spatial misalignment of other frames with respect to the keyframes
    is challenging to compensate for and often leads to different quantity results
    between keyframes and non-key frames. In [[231](#bib.bib231)], a temporal consistency
    guided knowledge distillation technique is proposed to train a compact network,
    which is applied to all frames. In [[224](#bib.bib224)], several weight-sharing
    sub-networks are distributed over sequential frames, whose extracted shallow features
    are composed for final segmentation. This trend of methods indeed speeds up inference,
    but still with the cost of reduced accuracy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 对更快分割的努力。另一条补充性的工作线试图利用时间信息来加速计算。他们用更便宜的替代方法来近似昂贵的每帧前向传递，例如重用邻近帧中的特征。在[[205](#bib.bib205)]中，分割网络的部分内容在帧之间自适应执行，从而降低计算成本。后来的方法使用关键帧来避免处理每一帧，然后将输出或特征图传播到其他帧。例如，[[214](#bib.bib214)]使用光流在关键帧和非关键帧之间进行特征变形。自适应关键帧选择随后在[[211](#bib.bib211),
    [218](#bib.bib218)]中得到了应用，并通过自适应特征传播[[217](#bib.bib217)]进一步增强。在[[221](#bib.bib221)]中，Jain
    *et al* 使用一个大型强模型来预测关键帧，并在非关键帧中使用一个紧凑型模型。基于关键帧的方法在关键帧和非关键帧之间具有不同的计算负载，导致最大延迟高和计算资源占用不平衡，这可能降低系统效率[[224](#bib.bib224)]。此外，其他帧相对于关键帧的空间错位很难补偿，并且通常会导致关键帧和非关键帧之间的数量结果不同。在[[231](#bib.bib231)]中，提出了一种时间一致性指导的知识蒸馏技术，用于训练紧凑型网络，该网络应用于所有帧。在[[224](#bib.bib224)]中，多个权重共享子网络分布在连续帧上，其提取的浅层特征被组合以进行最终分割。这种方法的趋势确实加快了推理速度，但仍以降低准确性为代价。
- en: $\bullet$ Semi-/Weakly-supervised based Methods. Away from these main battlefields,
    some researchers made efforts to learn VSS under annotation efficient settings.
    In [[215](#bib.bib215)], classifier heatmaps are used to learn VSS from image
    tags only. [[222](#bib.bib222), [234](#bib.bib234)] use both labeled and unlabeled
    video frames to learn VSS. They propagate annotations from labeled frames to other
    unlabeled, neighboring frames [[222](#bib.bib222)], or alternatively train teacher
    and student networks with groundtruth annotations and iteratively generated pseudo
    labels [[234](#bib.bib234)].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 半监督/弱监督方法。在这些主要领域之外，一些研究人员努力在标注高效的设置下学习 VSS。在[[215](#bib.bib215)]中，使用分类器热图仅从图像标签中学习
    VSS。[[222](#bib.bib222), [234](#bib.bib234)] 使用有标签和无标签的视频帧来学习 VSS。他们将标注从有标签帧传播到其他无标签的邻近帧[[222](#bib.bib222)]，或者交替训练带有真实标注的教师和学生网络以及迭代生成的伪标签[[234](#bib.bib234)]。
- en: 3.2.2 Video Instance Segmentation (VIS)
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 视频实例分割 (VIS)
- en: 'In 2019, Yang *et al*. extended image instance segmentation to the video domain [[82](#bib.bib82)],
    which requires simultaneous detection, segmentation and tracking of instances
    in videos. This task is also known as multi-object tracking and segmentation (MOTS) [[210](#bib.bib210)].
    Based on the patterns of generating instance sequences, existing frameworks can
    be roughly categorized into four paradigms: i) track-detect, ii) clip-match, iii)
    propose-reduce, iv) segment-as-a-whole. Track-detect methods detect and segment
    instances for each individual frame, followed by frame-by-frame instance tracking [[82](#bib.bib82),
    [210](#bib.bib210), [235](#bib.bib235), [232](#bib.bib232), [230](#bib.bib230),
    [237](#bib.bib237), [252](#bib.bib252), [228](#bib.bib228), [236](#bib.bib236)].
    For example, in [[82](#bib.bib82), [210](#bib.bib210), [253](#bib.bib253)], Mask
    R-CNN [[180](#bib.bib180)] is adapted for VIS/MOTS by adding a tracking branch
    for cross-frame instance association. Alternatively, [[237](#bib.bib237)] models
    spatial attention to describe instances, tackling the task from a novel single-stage
    yet elegant perspective. Clip-match methods divide an entire video into multiple
    overlapped clips, and perform VIS independently for each clip through mask propagation [[226](#bib.bib226)]
    or spatial-temporal embedding [[233](#bib.bib233)]. Final instance sequences are
    generated by merging neighboring clips. Both of the two paradigms need two independent
    steps to generate a complete sequence. They both generate multiple incomplete
    sequences (*i.e*., frames or clips) from a video, and merge (or complete) them
    by tracking/matching at the second stage. Intuitively, these paradigms are vulnerable
    to error accumulation in the process of merging sequences, especially when occlusion
    or fast motion exists. To address these limitations, a propose-reduce paradigm
    is proposed in [[244](#bib.bib244)]. It first samples several key frames and obtains
    instance sequences by propagating the instance segmentation results from each
    key frame to the entire video. Then, the redundant sequence proposals of the same
    instances are removed. This paradigm not only discards the step of merging incomplete
    sequences, but also achieves robust results considering multiple key frames. However,
    these three types of methods still need complex heuristic rules to associate instances
    and/or multiple steps to generate instance sequences. The segment-as-a-whole paradigm [[238](#bib.bib238)]
    is more elegant; it poses the task as a direct sequence prediction problem using
    Transformer [[192](#bib.bib192)].'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年，杨*等*人将图像实例分割扩展到视频领域[[82](#bib.bib82)]，这要求同时在视频中进行检测、分割和跟踪实例。这项任务也被称为多目标跟踪和分割（MOTS）[[210](#bib.bib210)]。根据生成实例序列的模式，现有框架大致可以分为四种范式：i)
    跟踪-检测，ii) 剪辑-匹配，iii) 提议-减少，iv) 整体-分割。跟踪-检测方法对每一帧进行实例检测和分割，然后进行逐帧实例跟踪[[82](#bib.bib82),
    [210](#bib.bib210), [235](#bib.bib235), [232](#bib.bib232), [230](#bib.bib230),
    [237](#bib.bib237), [252](#bib.bib252), [228](#bib.bib228), [236](#bib.bib236)]。例如，在[[82](#bib.bib82),
    [210](#bib.bib210), [253](#bib.bib253)]中，Mask R-CNN[[180](#bib.bib180)]通过为跨帧实例关联添加跟踪分支来适应VIS/MOTS。或者，[[237](#bib.bib237)]模型使用空间注意力来描述实例，从一种新颖的单阶段而优雅的视角处理任务。剪辑-匹配方法将整个视频分割成多个重叠的剪辑，并通过掩膜传播[[226](#bib.bib226)]或时空嵌入[[233](#bib.bib233)]对每个剪辑独立执行VIS。最终实例序列通过合并相邻的剪辑生成。这两种范式都需要两个独立的步骤来生成完整的序列。它们都从视频中生成多个不完整的序列（*即*，帧或剪辑），然后通过跟踪/匹配在第二阶段合并（或完成）它们。直观上，这些范式在合并序列的过程中容易出现错误积累，尤其是在存在遮挡或快速运动时。为了解决这些限制，[[244](#bib.bib244)]提出了一种提议-减少范式。它首先采样几个关键帧，并通过将每个关键帧的实例分割结果传播到整个视频中来获取实例序列。然后，去除相同实例的冗余序列提议。该范式不仅省略了合并不完整序列的步骤，而且考虑多个关键帧实现了鲁棒的结果。然而，这三种类型的方法仍然需要复杂的启发式规则来关联实例和/或多个步骤来生成实例序列。整体-分割范式[[238](#bib.bib238)]更为优雅；它将任务直接作为序列预测问题，使用Transformer[[192](#bib.bib192)]。
- en: Almost all VIS models are built upon fully supervised learning, while [[242](#bib.bib242),
    [243](#bib.bib243)] are the exceptions. Specifically, in [[242](#bib.bib242)],
    motion and temporal consistency cues are leveraged to generate pseudo-labels from
    tag labeled videos for weakly supervised VIS learning. In [[243](#bib.bib243)],
    a semi-supervised embedding learning approach is proposed to learn VIS from pixel-wise
    annotated images and unlabeled videos.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的VIS模型都是建立在完全监督学习的基础上，[[242](#bib.bib242)] 和 [[243](#bib.bib243)] 是例外。具体来说，在[[242](#bib.bib242)]中，利用运动和时间一致性线索从标签视频中生成伪标签，以进行弱监督的VIS学习。在[[243](#bib.bib243)]中，提出了一种半监督嵌入学习方法，从像素级标注图像和未标记视频中学习VIS。
- en: 3.2.3 Video Panoptic Segmentation (VPS)
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 视频全景分割（VPS）
- en: Very recently, Kim *et al*. extended image panoptic segmentation to the video
    domain [[227](#bib.bib227)], which aims at a holistic segmentation of all foreground
    instance tracklets and background regions, and assigning a semantic label to each
    video pixel. They adapt an image panoptic segmentation model [[254](#bib.bib254)]
    for VPS, by adding two modules for temporal feature fusion and cross-frame instance
    association, respectively. Later, temporal correspondence was explored in [[240](#bib.bib240)]
    through learning coarse segment-level and fine pixel-level matching. Qiao *et
    al*. [[241](#bib.bib241)] propose to learn monocular depth estimation and video
    panoptic segmentation jointly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Kim *et al* 扩展了图像全景分割到视频领域[[227](#bib.bib227)]，旨在对所有前景实例轨迹和背景区域进行全面分割，并为每个视频像素分配语义标签。他们将图像全景分割模型[[254](#bib.bib254)]
    适配用于VPS，通过添加两个模块分别用于时间特征融合和跨帧实例关联。后来，[[240](#bib.bib240)]通过学习粗略的段级匹配和细致的像素级匹配探讨了时间对应性。Qiao
    *et al* [[241](#bib.bib241)] 提出联合学习单目深度估计和视频全景分割。
- en: '![Refer to caption](img/d478b97d2d5c97072cc59b470d19496b.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d478b97d2d5c97072cc59b470d19496b.png)'
- en: <svg version="1.1" width="713.07" height="73.34" overflow="visible"><g transform="translate(0,73.34)
    scale(1,-1)"><g transform="translate(-341.77,121.07)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="50%">Youtube-Objects <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[73](#bib.bib73)]</foreignobject></g></text></g><g
    transform="translate(-265.67,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">FBMS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{59}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[36](#bib.bib36)]</foreignobject></g></text></g><g
    transform="translate(-204.1,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{16}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[17](#bib.bib17)]</foreignobject></g></text></g><g
    transform="translate(-130.07,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{17}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[81](#bib.bib81)]</foreignobject></g></text></g><g
    transform="translate(-65.03,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VOS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[95](#bib.bib95)]</foreignobject></g></text></g><g
    transform="translate(-339.01,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">A2D Sentence <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-265.67,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">J-HMDB-S <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-207.56,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[17]</foreignobject></g>-RVOS <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[197](#bib.bib197)]</foreignobject></g></text></g><g
    transform="translate(-134.22,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">Refer-Youtube-VOS​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[202](#bib.bib202)]</foreignobject></g></text></g><g
    transform="translate(-49.81,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CamVid <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[208](#bib.bib208)]</foreignobject></g></text></g><g
    transform="translate(-337.62,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CityScapes <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[206](#bib.bib206)]</foreignobject></g></text></g><g
    transform="translate(-278.12,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">NYUDv2​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[225](#bib.bib225)]</foreignobject></g></text></g><g
    transform="translate(-220.01,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">VSPW <g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[255](#bib.bib255)]</foreignobject></g></text></g><g
    transform="translate(-167.43,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VIS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[82](#bib.bib82)]</foreignobject></g></text></g><g
    transform="translate(-83.02,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">KITTI MOTS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]</foreignobject></g></text></g><g
    transform="translate(-345.93,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">MOTSChallenge​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]</foreignobject></g></text></g><g
    transform="translate(-267.75,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">BDD100K <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[229](#bib.bib229)]</foreignobject></g></text></g><g
    transform="translate(-193.72,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">OVIS <g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[246](#bib.bib246)]</foreignobject></g></text></g><g
    transform="translate(-135.6,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">VIPER-VPS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[227](#bib.bib227)]</foreignobject></g></text></g><g
    transform="translate(-69.19,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">Cityscapes-VPS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[227](#bib.bib227)]</foreignobject></g></text></g></g></svg>
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="713.07" height="73.34" overflow="visible"><g transform="translate(0,73.34)
    scale(1,-1)"><g transform="translate(-341.77,121.07)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="50%">Youtube-Objects <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[73](#bib.bib73)]</foreignobject></g></text></g><g
    transform="translate(-265.67,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">FBMS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{59}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[36](#bib.bib36)]</foreignobject></g></text></g><g
    transform="translate(-204.1,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{16}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[17](#bib.bib17)]</foreignobject></g></text></g><g
    transform="translate(-130.07,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{17}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[81](#bib.bib81)]</foreignobject></g></text></g><g
    transform="translate(-65.03,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VOS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[95](#bib.bib95)]</foreignobject></g></text></g><g
    transform="translate(-339.01,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">A2D Sentence <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-265.67,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">J-HMDB-S <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-207.56,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[17]</foreignobject></g>-RVOS <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[197](#bib.bib197)]</foreignobject></g></text></g><g
    transform="translate(-134.22,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">Refer-Youtube-VOS​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[202](#bib.bib202)]</foreignobject></g></text></g><g
    transform="translate(-49.81,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CamVid <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[208](#bib.bib208)]</foreignobject></g></text></g><g
    transform="translate(-337.62,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CityScapes <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[206](#bib.bib206)]</foreignobject></g></text></g><g
    transform="translate(-278.12,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">NYUDv2​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[225](#bib.bib225)]</foreignobject></g></text></g><g
    transform="translate(-220.01,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">VSPW <g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[255](#bib.bib255)]</foreignobject></g></text></g><g
    transform="translate(-167.43,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VIS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[82](#bib.bib82)]</foreignobject></g></text></g><g
    transform="translate(-83.02,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">KITTI MOTS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]</foreignobject></g></text></g><g
    transform="translate(-345.93,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">MOTSChallenge​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]
- en: 'Figure 3: Example frames from twenty famous video segmentation benchmark datasets.
    The ground-truth segmentation annotation is overlaid.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 来自二十个著名视频分割基准数据集的示例帧。真值分割标注被覆盖在上面。'
- en: 'TABLE VI: Statistics of representative video segmentation datasets. See §[4.1](#S4.SS1
    "4.1 VOS Datasets ‣ 4 Video Segmentation Datasets ‣ A Survey on Deep Learning
    Technique for Video Segmentation") and §[4.2](#S4.SS2 "4.2 VSS Datasets ‣ 4 Video
    Segmentation Datasets ‣ A Survey on Deep Learning Technique for Video Segmentation")
    for more detailed descriptions.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 代表性视频分割数据集的统计信息。有关详细描述，请参见 §[4.1](#S4.SS1 "4.1 VOS 数据集 ‣ 4 视频分割数据集 ‣
    深度学习技术在视频分割中的应用") 和 §[4.2](#S4.SS2 "4.2 VSS 数据集 ‣ 4 视频分割数据集 ‣ 深度学习技术在视频分割中的应用")。'
- en: '|   Dataset | Year | Pub. | #Video | #Train/Val/Test/Dev | Annotation | Purpose
    | #Class | Synthetic |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|   数据集 | 年份 | 发表 | 视频数量 | 训练/验证/测试/开发 | 注释 | 目的 | 类别数量 | 合成 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Youtube-Objects [[73](#bib.bib73)] | 2012 | CVPR | 1,407 (126) | -/-/-/-
    | Object-level AVOS, SVOS | Generic | 10 |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Youtube-Objects [[73](#bib.bib73)] | 2012 | CVPR | 1,407 (126) | -/-/-/-
    | 对象级 AVOS, SVOS | 通用 | 10 |  |'
- en: '| FBMS[59] ​  [[36](#bib.bib36)] | 2014 | PAMI | 59 | 29/30/-/- | Object-level
    AVOS, SVOS | Generic | - |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| FBMS[59] ​  [[36](#bib.bib36)] | 2014 | PAMI | 59 | 29/30/-/- | 对象级 AVOS,
    SVOS | 通用 | - |  |'
- en: '| DAVIS[16] ​  [[17](#bib.bib17)] | 2016 | CVPR | 50 | 30/20/-/- | Object-level
    AVOS, SVOS | Generic | - |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| DAVIS[16] ​  [[17](#bib.bib17)] | 2016 | CVPR | 50 | 30/20/-/- | 对象级 AVOS,
    SVOS | 通用 | - |  |'
- en: '| DAVIS[17] [[81](#bib.bib81)] | 2017 | - | 150 | 60/30/30/30 | Instance-level
    AVOS, SVOS, IVOS | Generic | - |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| DAVIS[17] [[81](#bib.bib81)] | 2017 | - | 150 | 60/30/30/30 | 实例级 AVOS, SVOS,
    IVOS | 通用 | - |  |'
- en: '| YouTube-VOS [[95](#bib.bib95)] | 2018 | - | 4,519 | 3,471/507/541/- | SVOS
    | Generic | 94 |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| YouTube-VOS [[95](#bib.bib95)] | 2018 | - | 4,519 | 3,471/507/541/- | SVOS
    | 通用 | 94 |  |'
- en: '| A2D Sentence [[196](#bib.bib196)] | 2018 | CVPR | 3,782 | 3,017/737/-/- |
    LVOS | Human-centric | - |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| A2D Sentence [[196](#bib.bib196)] | 2018 | CVPR | 3,782 | 3,017/737/-/- |
    LVOS | 以人为中心 | - |  |'
- en: '| J-HMDB Sentence [[196](#bib.bib196)] | 2018 | CVPR | 928 | -/-/-/- | LVOS
    | Human-centric | - |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| J-HMDB Sentence [[196](#bib.bib196)] | 2018 | CVPR | 928 | -/-/-/- | LVOS
    | 以人为中心 | - |  |'
- en: '| DAVIS[17]-RVOS [[197](#bib.bib197)] | 2018 | ACCV | 90 | 60/30/-/- | LVOS
    | Generic | - |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| DAVIS[17]-RVOS [[197](#bib.bib197)] | 2018 | ACCV | 90 | 60/30/-/- | LVOS
    | 通用 | - |  |'
- en: '| Refer-Youtube-VOS [[202](#bib.bib202)] | 2020 | ECCV | 3,975 | 3,471/507/-/-
    | LVOS | Generic | - |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Refer-Youtube-VOS [[202](#bib.bib202)] | 2020 | ECCV | 3,975 | 3,471/507/-/-
    | LVOS | 通用 | - |  |'
- en: '| CamVid [[208](#bib.bib208)] | 2009 | PRL | 4 | (frame: 467/100/233/-) | VSS
    | Urban | 11 |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| CamVid [[208](#bib.bib208)] | 2009 | PRL | 4 | (帧: 467/100/233/-) | VSS |
    城市 | 11 |  |'
- en: '| CityScapes [[206](#bib.bib206)] | 2016 | CVPR | 5,000 | 2,975/500/1,525 |
    VSS | Urban | 19 |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| CityScapes [[206](#bib.bib206)] | 2016 | CVPR | 5,000 | 2,975/500/1,525 |
    VSS | 城市 | 19 |  |'
- en: '| NYUDv2 [[225](#bib.bib225)] | 2012 | ECCV | 518 | (frame: 795/654/-/-) |
    VSS | Indoor | 40 |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| NYUDv2 [[225](#bib.bib225)] | 2012 | ECCV | 518 | (帧: 795/654/-/-) | VSS
    | 室内 | 40 |  |'
- en: '| VSPW [[255](#bib.bib255)] | 2021 | CVPR | 3,536 | 2,806/343/387/- | VSS |
    Generic | 124 |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| VSPW [[255](#bib.bib255)] | 2021 | CVPR | 3,536 | 2,806/343/387/- | VSS |
    通用 | 124 |  |'
- en: '| YouTube-VIS [[82](#bib.bib82)] | 2019 | ICCV | 3,859 | 2,985/421/453/- |
    VIS | Generic | 40 |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| YouTube-VIS [[82](#bib.bib82)] | 2019 | ICCV | 3,859 | 2,985/421/453/- |
    VIS | 通用 | 40 |  |'
- en: '| KITTI MOTS [[210](#bib.bib210)] | 2019 | CVPR | 21 | 12/9/-/- | VIS | Urban
    | 2 |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| KITTI MOTS [[210](#bib.bib210)] | 2019 | CVPR | 21 | 12/9/-/- | VIS | 城市
    | 2 |  |'
- en: '| MOTSChallenge [[210](#bib.bib210)] | 2019 | CVPR | 4 | -/-/-/- | VIS | Urban
    | 1 |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| MOTSChallenge [[210](#bib.bib210)] | 2019 | CVPR | 4 | -/-/-/- | VIS | 城市
    | 1 |  |'
- en: '| BDD100K [[229](#bib.bib229)] | 2020 | ECCV | 100,000 | 7,000/1,000/2,000/-
    | VSS, VIS | Driving | 40 (VSS), 8 (VIS) |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| BDD100K [[229](#bib.bib229)] | 2020 | ECCV | 100,000 | 7,000/1,000/2,000/-
    | VSS, VIS | 驾驶 | 40 (VSS), 8 (VIS) |  |'
- en: '| OVIS [[246](#bib.bib246)] | 2021 | - | 901 | 607/140/154/- | VIS | Generic
    | 25 |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| OVIS [[246](#bib.bib246)] | 2021 | - | 901 | 607/140/154/- | VIS | 通用 | 25
    |  |'
- en: '| VIPER-VPS [[227](#bib.bib227)] | 2020 | CVPR | 124 | (frame: 134K/50K/70K/-)
    | VPS | Urban | 23 | ✓ |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| VIPER-VPS [[227](#bib.bib227)] | 2020 | CVPR | 124 | (帧: 134K/50K/70K/-)
    | VPS | 城市 | 23 | ✓ |'
- en: '| Cityscapes-VPS [[227](#bib.bib227)] | 2020 | CVPR | 500 | 400/100/-/- | VPS
    | Urban | 19 |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes-VPS [[227](#bib.bib227)] | 2020 | CVPR | 500 | 400/100/-/- | VPS
    | 城市 | 19 |  |'
- en: 4 Video Segmentation Datasets
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 视频分割数据集
- en: Several datasets have been proposed for video segmentation over the past decades.
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep
    Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey
    on Deep Learning Technique for Video Segmentation") shows example frames from
    twenty commonly used datasets. We summarize their essential features in Table [VI](#S3.T6
    "TABLE VI ‣ 3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep Learning-based
    VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation") and give detailed review below.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，提出了几种视频分割数据集。图[3](#S3.F3 "Figure 3 ‣ 3.2.3 Video Panoptic Segmentation
    (VPS) ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")展示了二十个常用数据集的示例帧。我们在表[VI](#S3.T6
    "TABLE VI ‣ 3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep Learning-based
    VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")中总结了它们的主要特征，并在下文中进行详细审查。
- en: 4.1 VOS Datasets
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 VOS 数据集
- en: 4.1.1 AVOS/SVOS/IVOS Datasets
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 AVOS/SVOS/IVOS 数据集
- en: $\bullet$ Youtube-Objects is a large dataset of $1,\!407$ videos collected from
    155 web videos belonging to 10 object categories (*e.g*., dog, cat, plane, *etc*.).
    VOS models typically test the generalization ability on a subset [[256](#bib.bib256)]
    having totally 126 shots with $20,\!647$ frames that provides coarse pixel-level
    fore-/background annotations on every 10^(th) frames.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Youtube-Objects 是一个大数据集，包含$1,\!407$个视频，来自155个网络视频，涉及10个物体类别（*例如*，狗、猫、飞机、*等等*）。VOS模型通常在一个子集上测试泛化能力，该子集[[256](#bib.bib256)]总共有126个镜头和$20,\!647$帧，每10^(th)帧提供粗略的像素级前景/背景标注。
- en: $\bullet$ FBMS[59]​ [[36](#bib.bib36)] consists of 59 video sequences with $13,\!860$
    frames in total. However, only 720 frames are annotated for fore-/background separation.
    The dataset is split into 29 and 30 sequences for training and evaluation, respectively.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FBMS[59]​ [[36](#bib.bib36)] 包含59个视频序列，总共$13,\!860$帧。然而，仅有720帧进行了前景/背景分离的标注。数据集被分为29个训练序列和30个评估序列。
- en: $\bullet$ DAVIS[16]​ [[17](#bib.bib17)] has 50 videos (30 for train set and
    20 for val set) with $3,\!455$ frames in total. For each frame, in addition to
    high-quality fore-/background segmentation annotation, a set of attributes (*e.g*.,
    deformation, occlusion, motion blur, *etc*.) are also provided to highlight the
    main challenges.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DAVIS[16]​ [[17](#bib.bib17)] 具有50个视频（30个用于训练集和20个用于验证集），总共$3,\!455$帧。每帧除了高质量的前景/背景分割标注外，还提供了一系列属性（*例如*，变形、遮挡、运动模糊、*等等*），以突出主要挑战。
- en: $\bullet$ DAVIS[17]​ [[81](#bib.bib81)] contains 150 videos, *i.e*., 60/30/30/30
    videos for train/val/test-dev/test-challenge sets. Its train and val sets are
    extended from the respective sets in DAVIS[16]. There are 10,459 frames in total.
    DAVIS[17] provides instance-level annotations to support SVOS. Then, DAVIS[18]
    challenge​ [[194](#bib.bib194)] provides scribble annotations to support IVOS.
    Moreover, as the original annotations of DAVIS[17] are biased towards the SVOS
    scenario, DAVIS[19] challenge​ [[177](#bib.bib177)] re-annotates val and test-dev
    sets of DAVIS[17] to support AVOS.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DAVIS[17]​ [[81](#bib.bib81)] 包含150个视频，即60/30/30/30个视频分别用于训练/验证/测试-dev/测试-challenge集。其训练集和验证集扩展自DAVIS[16]中的相应集合。总共有10,459帧。DAVIS[17]提供实例级标注以支持SVOS。然后，DAVIS[18]挑战​
    [[194](#bib.bib194)] 提供涂鸦标注以支持IVOS。此外，由于DAVIS[17]的原始标注偏向SVOS场景，DAVIS[19]挑战​ [[177](#bib.bib177)]
    重新标注了DAVIS[17]的验证集和测试-dev集以支持AVOS。
- en: $\bullet$ YouTube-VOS​ [[95](#bib.bib95)] is a large-scale dataset, which is
    split into a train ($3,\!471$ videos), val (507 videos), and test (541 videos)
    set, in its newest 2019 version. Instance-level precise annotations are provided
    every five frames in a 30FPS frame rate. There are 94 object categories (*e.g*.,
    person, snake, *etc*.) in total, of which 26 are unseen in train set.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ YouTube-VOS​ [[95](#bib.bib95)] 是一个大规模的数据集，最新版本2019年中分为训练集（$3,\!471$个视频）、验证集（507个视频）和测试集（541个视频）。在30FPS帧率下，每五帧提供一次实例级精确标注。总共有94个物体类别（*例如*，人、蛇、*等等*），其中26个在训练集中未出现。
- en: Remark. Youtube-Objects, FBMS[59] and DAVIS[16] are used
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。使用了Youtube-Objects、FBMS[59]和DAVIS[16]。
- en: for instance-agnostic AVOS and SVOS evaluation. DAVIS[17] is unique in comprehensive
    annotations for instance-level AVOS, SVOS as well as IVOS, but its scale is relatively
    small. YouTube-VOS is the largest one but only supports SVOS benchmarking. There
    also exist some other VOS datasets, such as SegTrack[V1] [[45](#bib.bib45)] and
    SegTrack[V2] [[76](#bib.bib76)], but they were less used recently, due to the
    limited scale and difficulty.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对实例无关的 AVOS 和 SVOS 评估。DAVIS[17] 在实例级 AVOS、SVOS 以及 IVOS 的全面注释方面独具特色，但其规模相对较小。YouTube-VOS
    是最大的，但仅支持 SVOS 基准测试。还有其他一些 VOS 数据集，例如 SegTrack[V1] [[45](#bib.bib45)] 和 SegTrack[V2]
    [[76](#bib.bib76)]，但由于规模和难度有限，它们最近使用较少。
- en: 4.1.2 LVOS Datasets
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 LVOS 数据集
- en: $\bullet$ A2D Sentence​ [[196](#bib.bib196)] augments A2D [[257](#bib.bib257)]
    with phrases. It contains $3,\!782$ videos, with $8$ action classes performed
    by $7$ actors. In each video, $3$ to $5$ frames are provided with segmentation
    masks. It contains $6,\!655$ sentences describing actors and their actions. The
    dataset is split into $3,\!017$/$737$ for train/test, and $28$ unlabeled videos
    are ignored [[198](#bib.bib198)].
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ A2D Sentence​ [[196](#bib.bib196)] 通过短语扩展了 A2D [[257](#bib.bib257)]。它包含
    $3,\!782$ 个视频，其中 $7$ 个演员执行 $8$ 个动作类别。在每个视频中，提供 $3$ 到 $5$ 帧，并附有分割掩膜。数据集包含 $6,\!655$
    个描述演员及其动作的句子。数据集被分为 $3,\!017$/$737$ 用于训练/测试，$28$ 个未标注的视频被忽略 [[198](#bib.bib198)]。
- en: $\bullet$ J-HMDB Sentence​ [[196](#bib.bib196)] is built upon J-HMDB [[258](#bib.bib258)].
    It is comprised of $928$ short videos with $928$ corresponding sentences describing
    $21$ different action categories.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ J-HMDB Sentence​ [[196](#bib.bib196)] 基于 J-HMDB [[258](#bib.bib258)]
    构建。它包含 $928$ 个短视频和 $928$ 个相应句子，描述了 $21$ 种不同的动作类别。
- en: '$\bullet$ DAVIS[17]-RVOS​ [[197](#bib.bib197)] extends DAVIS[17] by collecting
    referring expressions for the annotated objects. 90 videos from train and val
    sets are annotated with more than 1,500 referring expressions. They provide two
    types of annotations, which describe the highlighted object: 1) based on the entire
    video (*i.e*., full-video expression) and 2) using only the first frame of the
    video (*i.e*., first-frame expression).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DAVIS[17]-RVOS​ [[197](#bib.bib197)] 通过收集标注对象的引用表达来扩展 DAVIS[17]。训练和验证集中的
    90 个视频被标注了超过 1,500 个引用表达。它们提供了两种类型的注释，用于描述突出的对象：1) 基于整个视频（*即*，全视频表达）和 2) 仅使用视频的第一帧（*即*，第一帧表达）。
- en: $\bullet$ Refer-Youtube-VOS​ [[202](#bib.bib202)] includes 3,975 videos from
    YouTube-VOS​ [[95](#bib.bib95)], with 27,899 language descriptions of target objects.
    Similar to DAVIS[17]-RVOS [[197](#bib.bib197)], both full-video and first-frame
    expression annotations are provided.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Refer-Youtube-VOS​ [[202](#bib.bib202)] 包含来自 YouTube-VOS​ [[95](#bib.bib95)]
    的 3,975 个视频，以及 27,899 个目标对象的语言描述。类似于 DAVIS[17]-RVOS [[197](#bib.bib197)]，提供了全视频和第一帧表达的注释。
- en: Remark. To date, A2D Sentence and J-HMDB Sentence are the main test-beds. However,
    the phrases are not produced with the aim of reference, but description, and limited
    to only a few object categories corresponding to the dominant ‘actors’ performing
    a salient ‘action’ [[202](#bib.bib202)]. But newly introduced DAVIS[17]-RVOS and
    Refer-Youtube-VOS show improved difficulties in both visual and linguistic modalities.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：截至目前，A2D Sentence 和 J-HMDB Sentence 是主要的测试平台。然而，这些短语的产生并非为了参考，而是为了描述，并且仅限于与主要‘演员’执行显著‘动作’相关的少数几个对象类别[[202](#bib.bib202)]。但新引入的
    DAVIS[17]-RVOS 和 Refer-Youtube-VOS 显示在视觉和语言模态上都提高了难度。
- en: 4.2 VSS Datasets
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 VSS 数据集
- en: $\bullet$ CamVid​ [[208](#bib.bib208)] is composed of 4 urban scene videos with
    11-class pixelwise annotations. Each video is annotated every 30 frames. The annotated
    frames are usually grouped into 467/100/233 for train/val/test [[207](#bib.bib207)].
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CamVid​ [[208](#bib.bib208)] 由 4 个城市场景视频组成，具有 11 类像素级注释。每个视频每 30 帧进行一次标注。标注的帧通常分为
    467/100/233 用于训练/验证/测试 [[207](#bib.bib207)]。
- en: $\bullet$ CityScapes${}_{\!~{}}$ [[206](#bib.bib206)]${}_{\!~{}}$ is${}_{\!~{}}$
    a${}_{\!~{}}$ large-scale${}_{\!~{}}$ VSS${}_{\!~{}}$ dataset${}_{\!~{}}$ for${}_{\!~{}}$
    street
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CityScapes${}_{\!~{}}$ [[206](#bib.bib206)]${}_{\!~{}}$ 是${}_{\!~{}}$
    一个大规模${}_{\!~{}}$ VSS${}_{\!~{}}$ 数据集${}_{\!~{}}$，用于${}_{\!~{}}$ 街道
- en: views. It has 2,975/500/1,525 snippets for train/val/ test, captured at 17FPS.
    Each snippet contains 30 frames, and only the 20^(th) frame is densely labelled
    with 19 semantic classes. 20,000 coarsely annotated frames are also provided.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: views。它有 2,975/500/1,525 个片段用于训练/验证/测试，以 17FPS 捕捉。每个片段包含 30 帧，仅第 20^(th) 帧密集标注了
    19 个语义类别。还提供了 20,000 帧粗略标注。
- en: $\bullet$ NYUDv2​ [[225](#bib.bib225)] contains 518 indoor RGB-D videos with
    high-quality ground-truths (every 10^(th) video frame is labeled). There are 795
    training frames and 654 testing frames being rectified and annotated with 40-class
    semantic labels.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ NYUDv2​ [[225](#bib.bib225)] 包含 518 个室内 RGB-D 视频，具有高质量的真实标注（每第 10
    帧视频都进行了标注）。有 795 个训练帧和 654 个测试帧被校正并用 40 类语义标签注释。
- en: $\bullet$ VSPW​ [[255](#bib.bib255)] is a recently proposed large-scale VSS
    dataset. It addresses video scene parsing in the wild by considering diverse scenarios.
    It consists of 3,536 videos, and provides pixel-level annotations for 124 categories
    at 15FPS. The train/val/test sets contain 2,806/343/387 videos with 198,244/24,502/28,887
    frames, respectively.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ VSPW​ [[255](#bib.bib255)] 是一个最近提出的大规模 VSS 数据集。它通过考虑多种场景来处理野外的视频场景解析。数据集包含
    3,536 个视频，并提供 124 个类别的像素级注释，帧率为 15FPS。训练/验证/测试集分别包含 2,806/343/387 个视频，帧数为 198,244/24,502/28,887。
- en: $\bullet$ YouTube-VIS​ [[82](#bib.bib82)] is built upon YouTube-VOS [[95](#bib.bib95)]
    with instance-level annotations. Its newest 2021 version has 3,859 videos (2,985/421/453
    for train/val/test) with 40 semantic categories. It provides 232K high-quality
    annotations for 8,171 unique video instances.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ YouTube-VIS​ [[82](#bib.bib82)] 是在 YouTube-VOS [[95](#bib.bib95)]
    的基础上构建的，具有实例级别的注释。其最新的 2021 版本包含 3,859 个视频（2,985/421/453 用于训练/验证/测试），并涵盖了 40 个语义类别。它提供了
    232K 高质量注释，涵盖 8,171 个独特的视频实例。
- en: $\bullet$ KITTI MOTS​ [[210](#bib.bib210)] extends the 21 training sequences
    of KITTI tracking dataset [[259](#bib.bib259)] with VIS annotations – 12 for training
    and 9 for validation, respectively. The dataset contains 8,008 frames with a resolution
    of $375\times 1242$, 26,899 annotated cars and 11,420 annotated pedestrians.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ KITTI MOTS​ [[210](#bib.bib210)] 扩展了 KITTI 跟踪数据集 [[259](#bib.bib259)]
    的 21 个训练序列，添加了 VIS 注释——分别为 12 个用于训练和 9 个用于验证。数据集包含 8,008 帧，分辨率为 $375\times 1242$，标注了
    26,899 辆汽车和 11,420 名行人。
- en: $\bullet$ MOTSChallenge​ [[210](#bib.bib210)] annotates 4 of 7 training sequences
    of MOTChallenge[2017] [[260](#bib.bib260)]. It has 2,862 frames with 26,894 annotated
    pedestrians and presents many occlusion cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MOTSChallenge​ [[210](#bib.bib210)] 标注了 MOTChallenge[2017] [[260](#bib.bib260)]
    的 7 个训练序列中的 4 个。它包含 2,862 帧，标注了 26,894 名行人，并呈现了许多遮挡情况。
- en: $\bullet$ BDD100K​ [[229](#bib.bib229)] is a large-scale dataset with 100K driving
    videos (40 seconds and 30FPS each) and supports various tasks, including VSS and
    VIS. For VSS, 7,000/1,000/2,000 frames are densely labelled with 40 semantic classes
    for train/val/test. For VIS, 90 videos with 8 semantic categories are annotated
    by 129K instance masks – 60 training videos, 10 validation videos, and 20 testing
    videos.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BDD100K​ [[229](#bib.bib229)] 是一个大规模的数据集，包含 100K 个驾驶视频（每个视频 40 秒，30FPS），支持各种任务，包括
    VSS 和 VIS。对于 VSS，7,000/1,000/2,000 帧被密集标注为 40 个语义类别，用于训练/验证/测试。对于 VIS，有 90 个视频，涵盖
    8 个语义类别，由 129K 实例掩码注释——其中 60 个训练视频、10 个验证视频和 20 个测试视频。
- en: $\bullet$ OVIS​ [[246](#bib.bib246)] is a new challenging VIS dataset, where
    object occlusions usually occur. It has 901 videos and 296K high-quality instance
    masks for 25 semantic categories. It is split into 607 training, 140 validation
    and 154 test videos.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ OVIS​ [[246](#bib.bib246)] 是一个新的挑战性 VIS 数据集，通常出现对象遮挡现象。它包含 901 个视频和
    296K 高质量的实例掩码，涵盖 25 个语义类别。它被划分为 607 个训练视频、140 个验证视频和 154 个测试视频。
- en: $\bullet$ VIPER-VPS​ [[227](#bib.bib227)] re-organizes VIPER [[261](#bib.bib261)]
    into the video panoptic format. VIPER, extracted from the GTA-V game engine, has
    annotations of semantic and instance segmentations for 10 thing and 13 stuff classes
    on 254K frames of ego-centric driving scenes at $1080\!\times\!1920$ resolution.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ VIPER-VPS​ [[227](#bib.bib227)] 将 VIPER [[261](#bib.bib261)] 重新组织为视频全景格式。VIPER
    从 GTA-V 游戏引擎中提取，具有 10 个事物类别和 13 个物品类别的语义和实例分割注释，覆盖 254K 帧自视角驾驶场景，分辨率为 $1080\!\times\!1920$。
- en: $\bullet$ Cityscapes-VPS​ [[227](#bib.bib227)] is built upon CityScapes​ [[206](#bib.bib206)].
    Dense panoptic annotations for 8 thing and 11 stuff classes for 500 snippets in
    Cityscapes val set are provided every five frames and temporally consistent instance
    ids to the thing objects are also given, leading to 3000 annotated frames in total.
    These videos are split into 400/100 for train/val.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Cityscapes-VPS​ [[227](#bib.bib227)] 基于 CityScapes​ [[206](#bib.bib206)]。为
    Cityscapes 验证集中的 500 个片段提供了 8 个事物类别和 11 个物品类别的密集全景注释，每五帧提供一次，并且提供了时间一致的实例 ID，总共有
    3,000 帧的注释。这些视频被划分为 400 个训练视频和 100 个验证视频。
- en: Remark. CamVid, CityScapes, NYUDv2, and VSPW are built for VSS benchmarking.
    YouTube-VIS, OVIS, KITTI MOTS, and MOTSChallenge are VIS datasets, but the diversity
    of the last two are limited. BDD100K has both VSS and VIS annotations. VIPER-VPS
    and Cityscapes-VPS are aware of VPS evaluation, but VIPER-VPS is a synthesized
    dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：CamVid、CityScapes、NYUDv2 和 VSPW 用于 VSS 基准测试。YouTube-VIS、OVIS、KITTI MOTS 和
    MOTSChallenge 是 VIS 数据集，但后两个数据集的多样性有限。BDD100K 具有 VSS 和 VIS 注释。VIPER-VPS 和 Cityscapes-VPS
    关注 VPS 评估，但 VIPER-VPS 是一个合成数据集。
- en: 5 Performance Comparison
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 性能比较
- en: Next we tabulate the performance of previously discussed algorithms. For each
    of the reviewed fields, the most widely used dataset is selected for performance
    benchmarking. The performance scores are gathered from the original articles,
    unless specified. For the running speed, we obtain the FPS for most methods by
    running their codes on a RTX 2080Ti GPU. For a small set of methods whose implementations
    are not well organized or publicly available, we directly borrow the values from
    the corresponding papers. Despite this, it is essential to remark the difficulty
    when comparing runtime. As different methods are with different code bases and
    levels of optimization, it is hard to make completely fair runtime comparison [[262](#bib.bib262),
    [17](#bib.bib17), [63](#bib.bib63)]; the values are only provided for reference.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们列出了之前讨论的算法的性能。对于每个审查过的领域，选择最广泛使用的数据集进行性能基准测试。性能评分来自原始文章，除非另有说明。对于运行速度，我们通过在
    RTX 2080Ti GPU 上运行其代码来获取大多数方法的 FPS。对于一小部分实现不够规范或公开的的方法，我们直接从相应的论文中借用数值。尽管如此，比较运行时间时的困难是显而易见的。由于不同方法具有不同的代码库和优化级别，完全公平地比较运行时间非常困难[[262](#bib.bib262),
    [17](#bib.bib17), [63](#bib.bib63)]；这些数值仅供参考。
- en: 5.1 Object-level AVOS Performance Benchmarking
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 对象级 AVOS 性能基准测试
- en: 5.1.1 Evaluation Metrics
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 评估指标
- en: 'Presently, three metrics are frequently used [[17](#bib.bib17)] to measure
    how object-level AVOS methods perform on this task:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，三种度量标准经常被使用[[17](#bib.bib17)] 来衡量对象级 AVOS 方法在该任务上的表现：
- en: 'TABLE VII: Quantitative object-level AVOS results on DAVIS[16] [[17](#bib.bib17)]
    val (§[5.1.2](#S5.SS1.SSS2 "5.1.2 Results ‣ 5.1 Object-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")) in terms of region similarity $\mathcal{J}$, boundary
    accuracy $\mathcal{F}$ and time stability $\mathcal{T}$. We also report the recall
    and the decay performance over time for both $\mathcal{J}$ and $\mathcal{F}$.
    (FPS denotes frames per second. ^†: FPS is borrowed from the original paper. The
    three best scores are marked in red, blue, and green, respectively. These notes
    also apply to the other tables.)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：在 DAVIS[16] [[17](#bib.bib17)] val (§[5.1.2](#S5.SS1.SSS2 "5.1.2 Results
    ‣ 5.1 Object-level AVOS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation")) 上的定量对象级 AVOS 结果，涉及区域相似度
    $\mathcal{J}$、边界准确度 $\mathcal{F}$ 和时间稳定性 $\mathcal{T}$。我们还报告了 $\mathcal{J}$ 和
    $\mathcal{F}$ 的召回率和随时间的衰减性能。（FPS 表示每秒帧数。^†：FPS 值来源于原始论文。三种最佳得分分别用红色、蓝色和绿色标出。这些说明也适用于其他表格。）
- en: '|   | $\mathcal{J}$ | $\mathcal{F}$ | $\mathcal{T}$ |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|   | $\mathcal{J}$ | $\mathcal{F}$ | $\mathcal{T}$ |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Method | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | mean$\uparrow$
    | recall$\uparrow$ | decay$\downarrow$ | mean$\downarrow$ | FPS$\uparrow$ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | mean$\uparrow$
    | recall$\uparrow$ | decay$\downarrow$ | mean$\downarrow$ | FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MuG [[98](#bib.bib98)] | 58.0 | 65.3 | 2.0 | 51.5 | 53.2 | 2.1 | 30.1 | 2.5
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| MuG [[98](#bib.bib98)] | 58.0 | 65.3 | 2.0 | 51.5 | 53.2 | 2.1 | 30.1 | 2.5
    |'
- en: '| SFL [[68](#bib.bib68)] | 67.4 | 81.4 | 6.2 | 66.7 | 77.1 | 5.1 | 28.2 | 3.3
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| SFL [[68](#bib.bib68)] | 67.4 | 81.4 | 6.2 | 66.7 | 77.1 | 5.1 | 28.2 | 3.3
    |'
- en: '| MotionGrouping [[109](#bib.bib109)] | 68.3 | - | - | 67.6 | - | - | - | 83.3
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| MotionGrouping [[109](#bib.bib109)] | 68.3 | - | - | 67.6 | - | - | - | 83.3
    |'
- en: '| LVO [[69](#bib.bib69)] | 75.9 | 89.1 | 0.0 | 72.1 | 83.4 | 1.3 | 26.5 | 13.5
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| LVO [[69](#bib.bib69)] | 75.9 | 89.1 | 0.0 | 72.1 | 83.4 | 1.3 | 26.5 | 13.5
    |'
- en: '| LMP [[70](#bib.bib70)] | 70.0 | 85.0 | 1.3 | 65.9 | 79.2 | 2.5 | 57.2 | 18.3
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| LMP [[70](#bib.bib70)] | 70.0 | 85.0 | 1.3 | 65.9 | 79.2 | 2.5 | 57.2 | 18.3
    |'
- en: '| FSEG [[66](#bib.bib66)] | 70.7 | 83.0 | 1.5 | 65.3 | 73.8 | 1.8 | 32.8 |
    7.2 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| FSEG [[66](#bib.bib66)] | 70.7 | 83.0 | 1.5 | 65.3 | 73.8 | 1.8 | 32.8 |
    7.2 |'
- en: '| PDB [[78](#bib.bib78)] | 77.2 | 93.1 | 0.9 | 74.5 | 84.4 | -0.2 | 29.1 |
    1.4 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| PDB [[78](#bib.bib78)] | 77.2 | 93.1 | 0.9 | 74.5 | 84.4 | -0.2 | 29.1 |
    1.4 |'
- en: '| MOT [[79](#bib.bib79)] | 77.2 | 87.8 | 5.0 | 77.4 | 84.4 | 3.3 | 27.9 | 1.0
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| MOT [[79](#bib.bib79)] | 77.2 | 87.8 | 5.0 | 77.4 | 84.4 | 3.3 | 27.9 | 1.0
    |'
- en: '| LSMO [[93](#bib.bib93)] | 78.2 | 91.1 | 4.1 | 75.9 | 84.7 | 3.5 | 21.2 |
    0.4 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| LSMO [[93](#bib.bib93)] | 78.2 | 91.1 | 4.1 | 75.9 | 84.7 | 3.5 | 21.2 |
    0.4 |'
- en: '| IST [[74](#bib.bib74)] | 78.5 | - | - | 75.5 | - | - | - | - |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| IST [[74](#bib.bib74)] | 78.5 | - | - | 75.5 | - | - | - | - |'
- en: '| AGS [[87](#bib.bib87)] | 79.7 | 89.1 | 1.9 | 77.4 | 85.8 | 0.0 | 26.7 | 1.7
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| AGS [[87](#bib.bib87)] | 79.7 | 89.1 | 1.9 | 77.4 | 85.8 | 0.0 | 26.7 | 1.7
    |'
- en: '| MBN [[77](#bib.bib77)] | 80.4 | 93.2 | 4.8 | 78.5 | 88.6 | 4.4 | 27.8 | 1.0
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| MBN [[77](#bib.bib77)] | 80.4 | 93.2 | 4.8 | 78.5 | 88.6 | 4.4 | 27.8 | 1.0
    |'
- en: '| COSNet [[83](#bib.bib83)] | 80.5 | 93.1 | 4.4 | 79.4 | 89.5 | 5.0 | 18.4
    | 2.2 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| COSNet [[83](#bib.bib83)] | 80.5 | 93.1 | 4.4 | 79.4 | 89.5 | 5.0 | 18.4
    | 2.2 |'
- en: '| AGNN [[89](#bib.bib89)] | 81.3 | 93.1 | 0.0 | 79.7 | 88.5 | 5.1 | 33.7 |
    1.9 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| AGNN [[89](#bib.bib89)] | 81.3 | 93.1 | 0.0 | 79.7 | 88.5 | 5.1 | 33.7 |
    1.9 |'
- en: '| MGA [[90](#bib.bib90)] | 81.4 | - | - | 81.0 | - | - | - | 1.1 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| MGA [[90](#bib.bib90)] | 81.4 | - | - | 81.0 | - | - | - | 1.1 |'
- en: '| AnDiff [[92](#bib.bib92)] | 81.7 | 90.9 | 2.2 | 80.5 | 85.1 | 0.6 | 21.4
    | 2.8 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| AnDiff [[92](#bib.bib92)] | 81.7 | 90.9 | 2.2 | 80.5 | 85.1 | 0.6 | 21.4
    | 2.8 |'
- en: '| PyramidCSA [[96](#bib.bib96)] | 78.1 | 90.1 | - | 78.5 | 88.2 | - | - | ^†110
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| PyramidCSA [[96](#bib.bib96)] | 78.1 | 90.1 | - | 78.5 | 88.2 | - | - | ^†110
    |'
- en: '| WCSNet [[101](#bib.bib101)] | 82.2 | - | - | 80.7 | - | - | - | 25 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| WCSNet [[101](#bib.bib101)] | 82.2 | - | - | 80.7 | - | - | - | 25 |'
- en: '| MATNet [[94](#bib.bib94)] | 82.4 | 94.5 | 5.5 | 80.7 | 90.2 | 4.5 | 21.6
    | 1.3 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| MATNet [[94](#bib.bib94)] | 82.4 | 94.5 | 5.5 | 80.7 | 90.2 | 4.5 | 21.6
    | 1.3 |'
- en: '| EGMN [[100](#bib.bib100)] | 82.5 | 94.3 | 4.2 | 81.2 | 90.3 | 5.6 | 19.8
    | 5.0 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| EGMN [[100](#bib.bib100)] | 82.5 | 94.3 | 4.2 | 81.2 | 90.3 | 5.6 | 19.8
    | 5.0 |'
- en: '| DFNet [[104](#bib.bib104)] | 83.4 | - | - | 81.8 | - | - | 15.9 | ^†3.6 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| DFNet [[104](#bib.bib104)] | 83.4 | - | - | 81.8 | - | - | 15.9 | ^†3.6 |'
- en: '| F2Net [[105](#bib.bib105)] | 83.1 | 95.7 | 0.0 | 84.4 | 92.3 | 0.8 | 20.9
    | ^†10 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| F2Net [[105](#bib.bib105)] | 83.1 | 95.7 | 0.0 | 84.4 | 92.3 | 0.8 | 20.9
    | ^†10 |'
- en: '| RTNet [[107](#bib.bib107)] | 85.6 | 96.1 | 0.5 | 84.7 | 93.8 | 0.9 | 19.9
    | 6.7 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| RTNet [[107](#bib.bib107)] | 85.6 | 96.1 | 0.5 | 84.7 | 93.8 | 0.9 | 19.9
    | 6.7 |'
- en: 'TABLE 8: Quantitative instance-level AVOS results on DAVIS[17] [[81](#bib.bib81)]
    val (§[5.2.2](#S5.SS2.SSS2 "5.2.2 Results ‣ 5.2 Instance-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")) in terms of region similarity $\mathcal{J}$ and boundary
    accuracy $\mathcal{F}$.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在 DAVIS[17] [[81](#bib.bib81)] 验证集上的定量实例级 AVOS 结果（§[5.2.2](#S5.SS2.SSS2
    "5.2.2 Results ‣ 5.2 Instance-level AVOS Performance Benchmarking ‣ 5 Performance
    Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")），以区域相似度
    $\mathcal{J}$ 和边界准确率 $\mathcal{F}$ 为依据。
- en: '|   | $\mathcal{J}\!\&amp;\mathcal{F}$ | $\mathcal{J}$ | $\mathcal{F}$ |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|   | $\mathcal{J}\!\&amp;\mathcal{F}$ | $\mathcal{J}$ | $\mathcal{F}$ |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Method | mean$\uparrow$ | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$
    | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | FPS$\uparrow$ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | mean$\uparrow$ | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$
    | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PDB [[78](#bib.bib78)] | 55.1 | 53.2 | 58.9 | 4.9 | 57.0 | 60.2 | 6.8 | 0.7
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| PDB [[78](#bib.bib78)] | 55.1 | 53.2 | 58.9 | 4.9 | 57.0 | 60.2 | 6.8 | 0.7
    |'
- en: '| RVOS [[80](#bib.bib80)] | 41.2 | 36.8 | 40.2 | 0.5 | 45.7 | 46.4 | 1.7 |
    14.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| RVOS [[80](#bib.bib80)] | 41.2 | 36.8 | 40.2 | 0.5 | 45.7 | 46.4 | 1.7 |
    14.3 |'
- en: '| AGS [[87](#bib.bib87)] | 57.5 | 55.5 | 61.6 | 7.0 | 59.5 | 62.8 | 9.0 | 1.1
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| AGS [[87](#bib.bib87)] | 57.5 | 55.5 | 61.6 | 7.0 | 59.5 | 62.8 | 9.0 | 1.1
    |'
- en: '| AGNN [[175](#bib.bib175)] | 61.1 | 58.9 | 65.7 | 11.7 | 63.2 | 67.1 | 14.3
    | 0.9 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| AGNN [[175](#bib.bib175)] | 61.1 | 58.9 | 65.7 | 11.7 | 63.2 | 67.1 | 14.3
    | 0.9 |'
- en: '| STEm-Seg [[233](#bib.bib233)] | 64.7 | 61.5 | 70.4 | -4.0 | 67.8 | 75.5 |
    1.2 | 9.3 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| STEm-Seg [[233](#bib.bib233)] | 64.7 | 61.5 | 70.4 | -4.0 | 67.8 | 75.5 |
    1.2 | 9.3 |'
- en: '| UnOVOST [[178](#bib.bib178)] | 67.9 | 66.4 | 76.4 | -0.2 | 69.3 | 76.9 |
    0.0 | ^†1.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| UnOVOST [[178](#bib.bib178)] | 67.9 | 66.4 | 76.4 | -0.2 | 69.3 | 76.9 |
    0.0 | ^†1.0 |'
- en: '| TODA [[106](#bib.bib106)] | 65.0 | 63.7 | 71.9 | 6.9 | 66.2 | 73.1 | 9.4
    | 9.1 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| TODA [[106](#bib.bib106)] | 65.0 | 63.7 | 71.9 | 6.9 | 66.2 | 73.1 | 9.4
    | 9.1 |'
- en: '$\bullet$ Region Jaccard $\mathcal{J}$ is calculated by the intersection-over-union
    (IoU) between the segmentation results ${\hat{Y}}\!\in\!\{0,1\}^{w\times h}$ and
    the ground-truth ${Y}\!\in\!\{0,1\}^{w\times h}$: $\mathcal{J}={|\hat{Y}\cap Y}|/|{\hat{Y}\cup
    Y|}$, which computes the number of pixels of the intersection between $\hat{Y}$
    and ${Y}$, and divides it by the size of the union.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 区域 Jaccard $\mathcal{J}$ 是通过分割结果 ${\hat{Y}}\!\in\!\{0,1\}^{w\times
    h}$ 和真实值 ${Y}\!\in\!\{0,1\}^{w\times h}$ 之间的交并比（IoU）计算的：$\mathcal{J}={|\hat{Y}\cap
    Y}|/|{\hat{Y}\cup Y|}$，它计算了 $\hat{Y}$ 和 ${Y}$ 交集的像素数量，并将其除以并集的大小。
- en: '$\bullet$ Boundary Accuracy $\mathcal{F}$ is the harmonic mean of the boundary
    precision $\text{P}_{c}$ and recall $\text{R}_{c}$. The value of $\mathcal{F}$
    reflects how well the segment contours $c(\hat{Y})$ match the ground-truth contours
    $c(Y)$. Usually, the value of $\text{P}_{c}$ and $\text{R}_{c}$ can be computed
    via bipartite graph matching [[263](#bib.bib263)], then the boundary accuracy
    $\mathcal{F}$ can be computed as: $\mathcal{F}={2\text{P}_{c}\text{R}_{c}}/({\text{P}_{c}+\text{R}_{c}})$.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 边界准确度 $\mathcal{F}$ 是边界精度 $\text{P}_{c}$ 和召回率 $\text{R}_{c}$ 的调和平均值。$\mathcal{F}$
    的值反映了分割轮廓 $c(\hat{Y})$ 与真实轮廓 $c(Y)$ 的匹配程度。通常，$\text{P}_{c}$ 和 $\text{R}_{c}$ 的值可以通过二分图匹配 [[263](#bib.bib263)]
    来计算，然后可以通过以下公式计算边界准确度 $\mathcal{F}$：$\mathcal{F}={2\text{P}_{c}\text{R}_{c}}/({\text{P}_{c}+\text{R}_{c}})$。
- en: $\bullet$ Temporal Stability $\mathcal{T}$ is informative of the stability of
    segments. It is computed as the pixel-level cost of matching two successive segmentation
    boundaries. The match is achieved by minimizing the shape context descriptor [[264](#bib.bib264)]
    distances between matched points while preserving the order in which the points
    are present in the boundary polygon. Note that $\mathcal{T}$ will compensate motion
    and small deformations, but not penalize inaccuracies of the contours [[17](#bib.bib17)].
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 时间稳定性 $\mathcal{T}$ 能够提供片段稳定性的相关信息。它通过匹配两个连续分割边界的像素级成本来计算。匹配是通过最小化形状上下文描述符 [[264](#bib.bib264)]
    在匹配点之间的距离，同时保持点在边界多边形中的顺序。请注意，$\mathcal{T}$ 将补偿运动和小的变形，但不会惩罚轮廓的准确性 [[17](#bib.bib17)]。
- en: 'TABLE 10: Quantitative IVOS results on DAVIS[17] [[81](#bib.bib81)] val (§[5.4.2](#S5.SS4.SSS2
    "5.4.2 Results ‣ 5.4 IVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation")) in terms of AUC
    and $\mathcal{J}$@60.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：在 DAVIS[17] [[81](#bib.bib81)] 验证集上定量 IVOS 结果 (§[5.4.2](#S5.SS4.SSS2 "5.4.2
    Results ‣ 5.4 IVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation"))，以 AUC 和 $\mathcal{J}$@60
    为衡量标准。
- en: '|   Method | AUC $\uparrow$ | $\mathcal{J}$@60 $\uparrow$ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | AUC $\uparrow$ | $\mathcal{J}$@60 $\uparrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| IVS [[183](#bib.bib183)] | 69.1 | 73.4 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| IVS [[183](#bib.bib183)] | 69.1 | 73.4 |'
- en: '| MANet [[184](#bib.bib184)] | 74.9 | 76.1 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| MANet [[184](#bib.bib184)] | 74.9 | 76.1 |'
- en: '| IVOS-W [[187](#bib.bib187)] | 74.1 | - |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| IVOS-W [[187](#bib.bib187)] | 74.1 | - |'
- en: '| ATNet [[185](#bib.bib185)] | 77.1 | 79.0 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| ATNet [[185](#bib.bib185)] | 77.1 | 79.0 |'
- en: '| GIS [[188](#bib.bib188)] | 82.0 | 82.9 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| GIS [[188](#bib.bib188)] | 82.0 | 82.9 |'
- en: '| MiVOS [[189](#bib.bib189)] | 84.9 | 85.4 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| MiVOS [[189](#bib.bib189)] | 84.9 | 85.4 |'
- en: 'TABLE 11: Quantitative LVOS results on A2D Sentence [[196](#bib.bib196)] test
    (§[5.5.2](#S5.SS5.SSS2 "5.5.2 Results ‣ 5.5 LVOS Performance Benchmarking ‣ 5
    Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：在 A2D Sentence [[196](#bib.bib196)] 测试集上的定量 LVOS 结果 (§[5.5.2](#S5.SS5.SSS2
    "5.5.2 Results ‣ 5.5 LVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation"))
- en: in terms of Precision@$K$, mAP and IoU.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以 Precision@$K$、mAP 和 IoU 为衡量标准。
- en: '|   | Overlap | mAP$\uparrow$ | IoU |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|   | 重叠 | mAP$\uparrow$ | IoU |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Method | P@0.5$\uparrow$ | P@0.6$\uparrow$ | P@0.7$\uparrow$ | P@0.8$\uparrow$
    | P@0.9$\uparrow$ | 0.5:0.95 | overall$\uparrow$ | mean$\uparrow$ | FPS$\uparrow$
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | P@0.5$\uparrow$ | P@0.6$\uparrow$ | P@0.7$\uparrow$ | P@0.8$\uparrow$
    | P@0.9$\uparrow$ | 0.5:0.95 | 总体$\uparrow$ | 平均$\uparrow$ | FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| A2DS [[196](#bib.bib196)] | 50.0 | 37.6 | 23.1 | 9.4 | 0.4 | 21.5 | 55.1
    | 42.6 | - |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| A2DS [[196](#bib.bib196)] | 50.0 | 37.6 | 23.1 | 9.4 | 0.4 | 21.5 | 55.1
    | 42.6 | - |'
- en: '| CMSANet [[204](#bib.bib204)] | 46.7 | 38.5 | 27.9 | 13.6 | 1.7 | 25.3 | 61.8
    | 43.2 | 6.5 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| CMSANet [[204](#bib.bib204)] | 46.7 | 38.5 | 27.9 | 13.6 | 1.7 | 25.3 | 61.8
    | 43.2 | 6.5 |'
- en: '| AAN [[198](#bib.bib198)] | 55.7 | 45.9 | 31.9 | 16.0 | 2.0 | 27.4 | 60.1
    | 49.0 | 8.6 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| AAN [[198](#bib.bib198)] | 55.7 | 45.9 | 31.9 | 16.0 | 2.0 | 27.4 | 60.1
    | 49.0 | 8.6 |'
- en: '| VT-Capsule [[201](#bib.bib201)] | 52.6 | 45.0 | 34.5 | 20.7 | 3.6 | 30.3
    | 56.8 | 46.0 | - |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| VT-Capsule [[201](#bib.bib201)] | 52.6 | 45.0 | 34.5 | 20.7 | 3.6 | 30.3
    | 56.8 | 46.0 | - |'
- en: '| CDNet [[199](#bib.bib199)] | 60.7 | 52.5 | 40.5 | 23.5 | 4.5 | 33.3 | 62.3
    | 53.1 | 7.2 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| CDNet [[199](#bib.bib199)] | 60.7 | 52.5 | 40.5 | 23.5 | 4.5 | 33.3 | 62.3
    | 53.1 | 7.2 |'
- en: '| PolarRPE [[200](#bib.bib200)] | 63.4 | 57.9 | 48.3 | 32.2 | 8.3 | 38.8 |
    66.1 | 52.9 | 5.4 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| PolarRPE [[200](#bib.bib200)] | 63.4 | 57.9 | 48.3 | 32.2 | 8.3 | 38.8 |
    66.1 | 52.9 | 5.4 |'
- en: '| CST [[203](#bib.bib203)] | 65.4 | 58.9 | 49.7 | 33.3 | 9.1 | 39.9 | 66.2
    | 56.1 | 8.1 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| CST [[203](#bib.bib203)] | 65.4 | 58.9 | 49.7 | 33.3 | 9.1 | 39.9 | 66.2
    | 56.1 | 8.1 |'
- en: 5.1.2 Results
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 结果
- en: We select DAVIS[16]​ [[17](#bib.bib17)], the most widely used dataset in AVOS,
    for performance benchmarking. Table [VII](#S5.T7 "TABLE VII ‣ 5.1.1 Evaluation
    Metrics ‣ 5.1 Object-level AVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation") presents the results
    of those reviewed AVOS methods DAVIS[16] val set. The current best solution, RTNet [[107](#bib.bib107)],
    reaches 85.6 region similarity $\mathcal{J}$, significantly outperforming the
    earlier deep learning-based methods, such as SFL [[68](#bib.bib68)], proposed
    in 2017.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择DAVIS[16]​[[17](#bib.bib17)]，这是AVOS中使用最广泛的数据集，用于性能基准。表[VII](#S5.T7 "表VII
    ‣ 5.1.1 评估指标 ‣ 5.1 对象级AVOS性能基准 ‣ 5 性能比较 ‣ 深度学习技术在视频分割中的调查")展示了那些评估的AVOS方法在DAVIS[16]
    val集上的结果。目前最佳方案RTNet[[107](#bib.bib107)]在区域相似性$\mathcal{J}$上达到了85.6，显著超越了早期的基于深度学习的方法，如2017年提出的SFL[[68](#bib.bib68)]。
- en: 5.2 Instance-level AVOS Performance Benchmarking
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实例级AVOS性能基准
- en: 5.2.1 Evaluation Metrics
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 评估指标
- en: In instance-level AVOS setting, region Jaccard $\mathcal{J}$, boundary accuracy
    $\mathcal{F}$, and $\mathcal{J}\&amp;\mathcal{F}$ – the mean of $\mathcal{J}$
    and $\mathcal{F}$ – are used for evaluation [[177](#bib.bib177)]. Each of the
    annotated object tracklets will be matched with one of predicted tracklets according
    to $\mathcal{J}\&amp;\mathcal{F}$, using bipartite graph matching. For a certain
    criterion, the final score will be computed between each ground-truth object and
    its optimal assignment.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例级AVOS设置中，使用区域Jaccard $\mathcal{J}$、边界准确率$\mathcal{F}$以及$\mathcal{J}\&amp;\mathcal{F}$（$\mathcal{J}$和$\mathcal{F}$的均值）进行评估[[177](#bib.bib177)]。每个标注对象跟踪片段将根据$\mathcal{J}\&amp;\mathcal{F}$与预测跟踪片段匹配，使用二分图匹配。对于某一标准，最终得分将计算每个真实对象与其最佳分配之间的分数。
- en: 5.2.2 Results
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 结果
- en: Regarding instance-level AVOS, we take into account DAVIS[17] [[81](#bib.bib81)]
    in which the vast majority of methods are evaluated. From Table [8](#S5.T8 "TABLE
    8 ‣ 5.1.1 Evaluation Metrics ‣ 5.1 Object-level AVOS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    we can find that UnOVOST [[178](#bib.bib178)] is the top scorer, with 67.9 $\mathcal{J}$
    at the time of this writing.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 关于实例级AVOS，我们考虑DAVIS[17]​[[81](#bib.bib81)]，其中大多数方法都进行了评估。从表[8](#S5.T8 "表8 ‣
    5.1.1 评估指标 ‣ 5.1 对象级AVOS性能基准 ‣ 5 性能比较 ‣ 深度学习技术在视频分割中的调查")中我们可以发现，UnOVOST[[178](#bib.bib178)]是目前得分最高的，当前$\mathcal{J}$为67.9。
- en: 5.3 SVOS Performance Benchmarking
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 SVOS性能基准
- en: 5.3.1 Evaluation Metrics
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 评估指标
- en: Region Jaccard $\mathcal{J}$, boundary accuracy $\mathcal{F}$, and $\mathcal{J}\&amp;\mathcal{F}$
    are also widely adopted for SVOS performance evaluation [[194](#bib.bib194)].
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 区域Jaccard $\mathcal{J}$、边界准确率$\mathcal{F}$以及$\mathcal{J}\&amp;\mathcal{F}$也被广泛应用于SVOS性能评估[[194](#bib.bib194)]。
- en: 'TABLE 9: Quantitative SVOS results on DAVIS[17] [[81](#bib.bib81)] val (§[5.3.2](#S5.SS3.SSS2
    "5.3.2 Results ‣ 5.3 SVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：DAVIS[17]上的定量SVOS结果[[81](#bib.bib81)] val (§[5.3.2](#S5.SS3.SSS2 "5.3.2 结果
    ‣ 5.3 SVOS性能基准 ‣ 5 性能比较 ‣ 深度学习技术在视频分割中的调查"))
- en: in terms of region similarity $\mathcal{J}$ and boundary accuracy $\mathcal{F}$.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在区域相似性$\mathcal{J}$和边界准确率$\mathcal{F}$方面。
- en: '|   Method |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{J}$ &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{J}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{F}$ &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{F}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '| FPS$\uparrow$ | Method |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| FPS$\uparrow$ | 方法 |'
- en: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{J}$ &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{J}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{F}$ &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{F}$ &#124;'
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值$\uparrow$ &#124;'
- en: '| FPS$\uparrow$ |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| OnAVOS [[122](#bib.bib122)] | 67.9 | 64.5 | 70.5 | 0.08 | STM [[149](#bib.bib149)]
    | 81.8 | 79.2 | 84.3 | 6.3 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| OnAVOS [[122](#bib.bib122)] | 67.9 | 64.5 | 70.5 | 0.08 | STM [[149](#bib.bib149)]
    | 81.8 | 79.2 | 84.3 | 6.3 |'
- en: '| OSVOS [[18](#bib.bib18)] | 60.3 | 56.7 | 63.9 | 0.22 | e-OSVOS [[153](#bib.bib153)]
    | 77.2 | 74.4 | 80.0 | 0.5 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| OSVOS [[18](#bib.bib18)] | 60.3 | 56.7 | 63.9 | 0.22 | e-OSVOS [[153](#bib.bib153)]
    | 77.2 | 74.4 | 80.0 | 0.5 |'
- en: '| CINM [[125](#bib.bib125)] | 67.5 | 64.5 | 70.5 | ^†0.01 | AFB-URR [[154](#bib.bib154)]
    | 74.6 | 73.0 | 76.1 | 3.8 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| CINM [[125](#bib.bib125)] | 67.5 | 64.5 | 70.5 | ^†0.01 | AFB-URR [[154](#bib.bib154)]
    | 74.6 | 73.0 | 76.1 | 3.8 |'
- en: '| FAVOS [[126](#bib.bib126)] | 58.2 | 54.6 | 61.8 | 0.56 | Fasttan [[155](#bib.bib155)]
    | 75.9 | 72.3 | 79.4 | ^†7 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| FAVOS [[126](#bib.bib126)] | 58.2 | 54.6 | 61.8 | 0.56 | Fasttan [[155](#bib.bib155)]
    | 75.9 | 72.3 | 79.4 | ^†7 |'
- en: '| MAST [[160](#bib.bib160)] | 65.5 | 63.3 | 67.6 | 5.1 | STM-Cycle [[166](#bib.bib166)]
    | 71.7 | 68.7 | 74.7 | 38 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| MAST [[160](#bib.bib160)] | 65.5 | 63.3 | 67.6 | 5.1 | STM-Cycle [[166](#bib.bib166)]
    | 71.7 | 68.7 | 74.7 | 38 |'
- en: '| CRW [[174](#bib.bib174)] | 67.6 | 64.5 | 70.6 | 7.3 | QMA [[167](#bib.bib167)]
    | 71.9 | - | - | 6.3 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| CRW [[174](#bib.bib174)] | 67.6 | 64.5 | 70.6 | 7.3 | QMA [[167](#bib.bib167)]
    | 71.9 | - | - | 6.3 |'
- en: '| RGMP [[127](#bib.bib127)] | 66.7 | 64.8 | 68.6 | 7.7 | Fasttmu [[156](#bib.bib156)]
    | 70.6 | 69.1 | 72.1 | 9.7 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| RGMP [[127](#bib.bib127)] | 66.7 | 64.8 | 68.6 | 7.7 | Fasttmu [[156](#bib.bib156)]
    | 70.6 | 69.1 | 72.1 | 9.7 |'
- en: '| OSMN [[128](#bib.bib128)] | 54.8 | 52.5 | 57.1 | 7.7 | SAT [[157](#bib.bib157)]
    | 72.3 | 68.6 | 76.0 | ^†39 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| OSMN [[128](#bib.bib128)] | 54.8 | 52.5 | 57.1 | 7.7 | SAT [[157](#bib.bib157)]
    | 72.3 | 68.6 | 76.0 | ^†39 |'
- en: '| OSVOS-S [[132](#bib.bib132)] | 68.0 | 64.7 | 71.3 | 0.22 | TVOS [[159](#bib.bib159)]
    | 72.3 | 69.9 | 74.7 | ^†37 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| OSVOS-S [[132](#bib.bib132)] | 68.0 | 64.7 | 71.3 | 0.22 | TVOS [[159](#bib.bib159)]
    | 72.3 | 69.9 | 74.7 | ^†37 |'
- en: '| Videomatch [[133](#bib.bib133)] | 61.4 | - | - | ^†0.38 | GCNet [[161](#bib.bib161)]
    | 71.4 | 69.3 | 73.5 | ^†25 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| Videomatch [[133](#bib.bib133)] | 61.4 | - | - | ^†0.38 | GCNet [[161](#bib.bib161)]
    | 71.4 | 69.3 | 73.5 | ^†25 |'
- en: '| Dyenet [[134](#bib.bib134)] | 69.1 | 67.3 | 71.0 | 2.4 | KMN [[162](#bib.bib162)]
    | 76.0 | 74.2 | 77.8 | 8.3 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Dyenet [[134](#bib.bib134)] | 69.1 | 67.3 | 71.0 | 2.4 | KMN [[162](#bib.bib162)]
    | 76.0 | 74.2 | 77.8 | 8.3 |'
- en: '| MVOS [[138](#bib.bib138)] | 59.2 | 56.3 | 62.1 | 1.5 | CFBI [[163](#bib.bib163)]
    | 81.9 | 79.3 | 84.5 | 2.2 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| MVOS [[138](#bib.bib138)] | 59.2 | 56.3 | 62.1 | 1.5 | CFBI [[163](#bib.bib163)]
    | 81.9 | 79.3 | 84.5 | 2.2 |'
- en: '| FEELVOS [[139](#bib.bib139)] | 71.5 | 69.1 | 74.0 | 2.2 | LWL [[164](#bib.bib164)]
    | 70.8 | 68.2 | 73.5 | 15.6 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| FEELVOS [[139](#bib.bib139)] | 71.5 | 69.1 | 74.0 | 2.2 | LWL [[164](#bib.bib164)]
    | 70.8 | 68.2 | 73.5 | 15.6 |'
- en: '| MHP-VOS [[140](#bib.bib140)] | 75.3 | 71.8 | 78.8 | ^†0.01 | MSN [[165](#bib.bib165)]
    | 74.1 | 71.4 | 76.8 | $\dagger$10 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| MHP-VOS [[140](#bib.bib140)] | 75.3 | 71.8 | 78.8 | ^†0.01 | MSN [[165](#bib.bib165)]
    | 74.1 | 71.4 | 76.8 | $\dagger$10 |'
- en: '| AGSS [[141](#bib.bib141)] | 67.4 | 64.9 | 69.9 | ^†10 | EGMN [[100](#bib.bib100)]
    | 82.8 | 80.0 | 85.2 | 5.0 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| AGSS [[141](#bib.bib141)] | 67.4 | 64.9 | 69.9 | ^†10 | EGMN [[100](#bib.bib100)]
    | 82.8 | 80.0 | 85.2 | 5.0 |'
- en: '| AGAME [[142](#bib.bib142)] | 70.0 | 67.2 | 72.7 | ^†14 | SwiftNet [[168](#bib.bib168)]
    | 81.1 | 78.3 | 83.9 | ^†25 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| AGAME [[142](#bib.bib142)] | 70.0 | 67.2 | 72.7 | ^†14 | SwiftNet [[168](#bib.bib168)]
    | 81.1 | 78.3 | 83.9 | ^†25 |'
- en: '| SiamMask [[143](#bib.bib143)] | 56.4 | 64.3 | 58.5 | ^†35 | G-FRTM [[169](#bib.bib169)]
    | 76.4 | - | - | ^†18.2 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| SiamMask [[143](#bib.bib143)] | 56.4 | 64.3 | 58.5 | ^†35 | G-FRTM [[169](#bib.bib169)]
    | 76.4 | - | - | ^†18.2 |'
- en: '| RVOS [[80](#bib.bib80)] | 60.6 | 57.5 | 63.6 | 0.56 | SST [[170](#bib.bib170)]
    | 82.5 | 79.9 | 85.1 | - |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| RVOS [[80](#bib.bib80)] | 60.6 | 57.5 | 63.6 | 0.56 | SST [[170](#bib.bib170)]
    | 82.5 | 79.9 | 85.1 | - |'
- en: '| RANet [[145](#bib.bib145)] | 65.7 | 63.2 | 68.2 | ^†30 | GIEL [[171](#bib.bib171)]
    | 82.7 | 80.2 | 85.3 | 6.7 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| RANet [[145](#bib.bib145)] | 65.7 | 63.2 | 68.2 | ^†30 | GIEL [[171](#bib.bib171)]
    | 82.7 | 80.2 | 85.3 | 6.7 |'
- en: '| DMM-Net [[147](#bib.bib147)] | 70.7 | 68.1 | 73.3 | 0.37 | LCM [[172](#bib.bib172)]
    | 83.5 | 80.5 | 86.5 | 8.5 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| DMM-Net [[147](#bib.bib147)] | 70.7 | 68.1 | 73.3 | 0.37 | LCM [[172](#bib.bib172)]
    | 83.5 | 80.5 | 86.5 | 8.5 |'
- en: '| DTN [[148](#bib.bib148)] | 67.4 | 64.2 | 70.6 | 14.3 | RMNet [[173](#bib.bib173)]
    | 83.5 | 81.0 | 86.0 | ^†11.9 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| DTN [[148](#bib.bib148)] | 67.4 | 64.2 | 70.6 | 14.3 | RMNet [[173](#bib.bib173)]
    | 83.5 | 81.0 | 86.0 | ^†11.9 |'
- en: 5.3.2 Results
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 结果
- en: DAVIS[17] [[81](#bib.bib81)] is also one of the most important SVOS dataset.
    Table [9](#S5.T11 "TABLE 9 ‣ 5.3.1 Evaluation Metrics ‣ 5.3 SVOS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    shows the results of recent SVOS methods on DAVIS[17] val set. In this case, all
    the top-leading solutions, such as EGMN [[100](#bib.bib100)], LCM [[172](#bib.bib172)],
    and RMNet [[173](#bib.bib173)], are built upon the memory augmented architecture
    – STM [[149](#bib.bib149)].
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: DAVIS[17] [[81](#bib.bib81)] 也是最重要的SVOS数据集之一。表[9](#S5.T11 "TABLE 9 ‣ 5.3.1 Evaluation
    Metrics ‣ 5.3 SVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation")展示了最近SVOS方法在DAVIS[17]验证集上的结果。在这种情况下，所有领先的解决方案，如EGMN
    [[100](#bib.bib100)]、LCM [[172](#bib.bib172)] 和 RMNet [[173](#bib.bib173)]，都是基于记忆增强架构–
    STM [[149](#bib.bib149)]。
- en: 5.4 IVOS Performance Benchmarking
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 IVOS性能基准
- en: 5.4.1 Evaluation Metrics
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 评估指标
- en: Area under the curve (AUC) and Jaccard at 60 seconds ($\mathcal{J}$@60s) are
    two widely used IVOS evaluation criteria [[194](#bib.bib194)].
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）和60秒时的Jaccard ($\mathcal{J}$@60s) 是两种广泛使用的IVOS评估标准 [[194](#bib.bib194)]。
- en: $\bullet$ AUC is designed to measure the overall accuracy of the evaluation.
    It is computed over the plot Time vs Jaccard. Each sample in the plot is computed
    considering the average time and the average Jaccard for a certain interaction.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ AUC旨在衡量评估的整体准确性。它是通过绘制时间与Jaccard的图表来计算的。图表中的每个样本都是根据某一交互的平均时间和平均Jaccard值来计算的。
- en: $\bullet$ $\mathcal{J}$@60 measures the accuracy with a limited time budget
    (60 seconds). It is achieved by interpolating the Time vs Jaccard plot at 60 seconds.
    This evaluates which quality an IVOS method can obtain in 60 seconds.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ $\mathcal{J}$@60衡量在有限时间预算（60秒）下的准确性。它是通过在60秒处插值时间与Jaccard图表来实现的。这评估了IVOS方法在60秒内可以获得的质量。
- en: 5.4.2 Results
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 结果
- en: DAVIS[17] [[81](#bib.bib81)] is also widely used for IVOS performance benchmarking.
    Results summarized in Table [5.1.1](#S5.SS1.SSS1 "5.1.1 Evaluation Metrics ‣ 5.1
    Object-level AVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation") show that the method proposed
    by Cheng *et al*. [[189](#bib.bib189)] is the top one.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: DAVIS[17] [[81](#bib.bib81)] 也被广泛用于IVOS性能基准测试。总结的结果在表[5.1.1](#S5.SS1.SSS1 "5.1.1
    Evaluation Metrics ‣ 5.1 Object-level AVOS Performance Benchmarking ‣ 5 Performance
    Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")中显示，程*等*[[189](#bib.bib189)]提出的方法是最好的。
- en: 5.5 LVOS Performance Benchmarking
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 LVOS性能基准测试
- en: 5.5.1 Evaluation Metrics
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 评估指标
- en: As [[196](#bib.bib196)], overall IoU, mean IoU and precision are adopted.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如[[196](#bib.bib196)]所述，采用整体IoU、均值IoU和精确度。
- en: 'TABLE 12: Quantitative VSS results on Cityscapes [[206](#bib.bib206)] val (§[5.6.2](#S5.SS6.SSS2
    "5.6.2 Results ‣ 5.6 VSS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation")) in terms of IoU${}_{\text{class}}$
    and IoU${}_{\text{category}}$ (Max Latency: maximum per-frame time cost).'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: Cityscapes [[206](#bib.bib206)] 上的定量VSS结果 (§[5.6.2](#S5.SS6.SSS2 "5.6.2
    Results ‣ 5.6 VSS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation"))，按IoU${}_{\text{class}}$和IoU${}_{\text{category}}$（最大延迟：每帧最大时间成本）分类。'
- en: '|   Method | IoU${}_{\text{class}}$$\uparrow$ | IoU${}_{\text{category}}$ $\uparrow$
    | FPS$\uparrow$ | Max Latency (ms)$\downarrow$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | IoU${}_{\text{class}}$$\uparrow$ | IoU${}_{\text{category}}$ $\uparrow$
    | FPS$\uparrow$ | 最大延迟 (ms)$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Clockwork [[205](#bib.bib205)] | 66.4 | 88.6 | 6.4 | 198 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Clockwork [[205](#bib.bib205)] | 66.4 | 88.6 | 6.4 | 198 |'
- en: '| DFF [[214](#bib.bib214)] | 69.2 | 88.9 | 5.6 | 575 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| DFF [[214](#bib.bib214)] | 69.2 | 88.9 | 5.6 | 575 |'
- en: '| PEARL [[212](#bib.bib212)] | 75.4 | 89.2 | 1.3 | 800 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| PEARL [[212](#bib.bib212)] | 75.4 | 89.2 | 1.3 | 800 |'
- en: '| NetWarp [[213](#bib.bib213)] | 80.5 | 91.0 | - | - |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| NetWarp [[213](#bib.bib213)] | 80.5 | 91.0 | - | - |'
- en: '| DVSN [[218](#bib.bib218)] | 70.3 | - | ^†19.8 | - |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| DVSN [[218](#bib.bib218)] | 70.3 | - | ^†19.8 | - |'
- en: '| LVS [[217](#bib.bib217)] | 76.8 | 89.8 | 5.8 | 380 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| LVS [[217](#bib.bib217)] | 76.8 | 89.8 | 5.8 | 380 |'
- en: '| GRFP [[216](#bib.bib216)] | 80.6 | 90.8 | 3.9 | 255 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| GRFP [[216](#bib.bib216)] | 80.6 | 90.8 | 3.9 | 255 |'
- en: '| Accel [[221](#bib.bib221)] | 75.5 | - | ^†1.1 | - |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Accel [[221](#bib.bib221)] | 75.5 | - | ^†1.1 | - |'
- en: '| VPLR [[222](#bib.bib222)] | 81.4 | - | ^†5.9 | - |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| VPLR [[222](#bib.bib222)] | 81.4 | - | ^†5.9 | - |'
- en: '| TDNet [[224](#bib.bib224)] | 79.9 | 90.1 | 5.6 | 178 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| TDNet [[224](#bib.bib224)] | 79.9 | 90.1 | 5.6 | 178 |'
- en: '| EFC [[223](#bib.bib223)] | 83.5 | 92.2 | ^†16.7 | - |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| EFC [[223](#bib.bib223)] | 83.5 | 92.2 | ^†16.7 | - |'
- en: '| Lukas [[239](#bib.bib239)] | 71.2 | - | ^†1.9 | - |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| Lukas [[239](#bib.bib239)] | 71.2 | - | ^†1.9 | - |'
- en: 'TABLE 14: Quantitative VPS results on Cityscapes-VPS [[227](#bib.bib227)] (§[5.8.2](#S5.SS8.SSS2
    "5.8.2 Results ‣ 5.8 VPS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation")) test in term of
    VPQ. Each cell shows VPQ^k / VPQ^k-Thing / VPQ^k-Stuff.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: Cityscapes-VPS [[227](#bib.bib227)] 上的定量VPS结果 (§[5.8.2](#S5.SS8.SSS2
    "5.8.2 Results ‣ 5.8 VPS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation"))，按VPQ进行测试。每个单元格显示VPQ^k
    / VPQ^k-Thing / VPQ^k-Stuff。'
- en: '|   | Temporal window size |  |  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|   | 时间窗口大小 |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | $k=0$$\uparrow$ | $k=5$$\uparrow$ | $k=10$$\uparrow$ | $k=15$$\uparrow$
    | VPQ$\uparrow$ | FPS$\uparrow$ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $k=0$$\uparrow$ | $k=5$$\uparrow$ | $k=10$$\uparrow$ | $k=15$$\uparrow$
    | VPQ$\uparrow$ | FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| VPS [[227](#bib.bib227)] | 64.2 / 59.0 / 67.7 | 57.9 / 46.5 / 65.1 | 54.8
    / 41.1 / 63.4 | 52.6 / 36.5 / 62.9 | 57.4 / 45.8 / 64.8 | 1.3 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| VPS [[227](#bib.bib227)] | 64.2 / 59.0 / 67.7 | 57.9 / 46.5 / 65.1 | 54.8
    / 41.1 / 63.4 | 52.6 / 36.5 / 62.9 | 57.4 / 45.8 / 64.8 | 1.3 |'
- en: '| SiamTrack [[240](#bib.bib240)] | 63.8 / 59.4 / 66.6 | 58.2 / 47.2 / 65.9
    | 56.0 / 43.2 / 64.4 | 54.7 / 40.2 / 63.2 | 57.8 / 47.5 / 65.0 | 4.5 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| SiamTrack [[240](#bib.bib240)] | 63.8 / 59.4 / 66.6 | 58.2 / 47.2 / 65.9
    | 56.0 / 43.2 / 64.4 | 54.7 / 40.2 / 63.2 | 57.8 / 47.5 / 65.0 | 4.5 |'
- en: '| ViP-DeepLab [[241](#bib.bib241)] | 68.9 / 61.6 / 73.5 | 62.9 / 51.0 / 70.5
    | 59.9 / 46.0 / 68.8 | 58.2 / 42.1 / 68.4 | 62.5 / 50.2 / 70.3 | 10.0 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| ViP-DeepLab [[241](#bib.bib241)] | 68.9 / 61.6 / 73.5 | 62.9 / 51.0 / 70.5
    | 59.9 / 46.0 / 68.8 | 58.2 / 42.1 / 68.4 | 62.5 / 50.2 / 70.3 | 10.0 |'
- en: '$\bullet$ IoU: overall IoU is computed as total intersection area of all test
    data over the total union area, while mean IoU refers to average over IoU of each
    test sample.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ IoU：总体IoU计算为所有测试数据的总交集面积与总并集面积的比值，而mean IoU指的是每个测试样本的IoU的平均值。
- en: '$\bullet$ Precision: Precision@$K$ is computed as the percentage of test samples
    whose IoU scores are higher than a threshold $K$. Precision at five thresholds
    ranging from 0.5 to 0.9 and'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 精确度：Precision@$K$ 计算为IoU分数高于阈值$K$的测试样本的百分比。五个阈值范围从0.5到0.9的精确度
- en: mean${}_{\!}$ average${}_{\!}$ precision${}_{\!}$ (mAP)${}_{\!}$ over${}_{\!}$
    0.5:0.05:0.95${}_{\!}$ are${}_{\!}$ reported.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 报告了mean${}_{\!}$ average${}_{\!}$ precision${}_{\!}$ (mAP)${}_{\!}$ 在0.5:0.05:0.95${}_{\!}$上的结果。
- en: 5.5.2 Results
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 结果
- en: A2D Sentence [[196](#bib.bib196)] is arguably the most popular dataset in
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: A2D Sentence [[196](#bib.bib196)] 可以说是最受欢迎的数据集之一
- en: LVOS. Table [5.1.1](#S5.SS1.SSS1 "5.1.1 Evaluation Metrics ‣ 5.1 Object-level
    AVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning
    Technique for Video Segmentation") gives the results of six recent methods on
    A2D Sentence test set. It shows clear improvement trend from the first LVOS model [[196](#bib.bib196)]
    proposed in 2018, to recent complicated solution [[203](#bib.bib203)]. For runtime
    comparison, all the methods are tested on a video clip of 16 frames with resolution
    $512\!\times\!512$ and a textual sequence of 20 words.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: LVOS。表[5.1.1](#S5.SS1.SSS1 "5.1.1 Evaluation Metrics ‣ 5.1 Object-level AVOS
    Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning
    Technique for Video Segmentation")给出了六种最新方法在A2D Sentence测试集上的结果。从2018年提出的第一个LVOS模型[[196](#bib.bib196)]到近期复杂的解决方案[[203](#bib.bib203)]，显示出明显的改进趋势。在运行时间比较中，所有方法都在分辨率为$512\!\times\!512$的16帧视频片段和20词的文本序列上进行了测试。
- en: 5.6 VSS Performance Benchmarking
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 VSS性能基准测试
- en: 5.6.1 Evaluation Metrics
  id: totrans-487
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.1 评价指标
- en: IoU metric is the most widely used metric in VSS. Moreover, in Cityscapes [[206](#bib.bib206)]
    – the gold-standard benchmark dataset in this field, two IoU scores, IoU${}_{\text{category}}$
    and IoU${}_{\text{class}}$, defined over two semantic granularities, are reported.
    Here, ‘category’ refers to high-level semantic categories (*e.g*., vehicle, human),
    while ‘class’ indicates more fine-grained semantic classes (*e.g*., car, bicycle,
    person, rider). In total, [[206](#bib.bib206)] considers $19$ classes, which are
    further grouped into $8$ categories.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: IoU指标是VSS中最广泛使用的指标。此外，在Cityscapes[[206](#bib.bib206)]——该领域的黄金标准基准数据集中，报告了两个IoU分数，即IoU${}_{\text{category}}$和IoU${}_{\text{class}}$，这两个分数定义了两个语义粒度。这里，“category”指高层次的语义类别（*例如*，车辆，人），而“class”表示更细粒度的语义类别（*例如*，汽车，自行车，人，骑手）。总共有[[206](#bib.bib206)]考虑了$19$个类别，这些类别进一步分为$8$个类别。
- en: 5.6.2 Results
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.2 结果
- en: Table [12](#S5.T12 "TABLE 12 ‣ 5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation") summarizes the results of eleven VSS approaches on Cityscapes [[206](#bib.bib206)]
    val set. As seen, EFC [[223](#bib.bib223)] performs the best currently, with $83.5\%$
    in terms of IoU${}_{\text{class}}$.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 表[12](#S5.T12 "TABLE 12 ‣ 5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")总结了十一种VSS方法在Cityscapes[[206](#bib.bib206)]验证集上的结果。如所示，目前EFC[[223](#bib.bib223)]表现最佳，其IoU${}_{\text{class}}$为$83.5\%$。
- en: 5.7 VIS Performance Benchmarking
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 VIS性能基准测试
- en: 5.7.1 Evaluation Metrics
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1 评价指标
- en: As in [[82](#bib.bib82)], precision and recall metrics are used for VIS performance
    evaluation. Precision at IoU thresholds 0.5 and 0.75, as well as mean average
    precision (mAP) over 0.50:0.05:0.95 are reported. Recall@$N$ is defined as the
    maximum recall given $N$ segmented instances per video. These two metrics are
    first evaluated per category and then averaged over the category set. The IoU
    metric is similar to region Jaccard $\mathcal{J}$ used in instance-level AVOS
    (§[5.2.1](#S5.SS2.SSS1 "5.2.1 Evaluation Metrics ‣ 5.2 Instance-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[[82](#bib.bib82)]中，精确度和召回率度量用于VIS性能评估。报告了IoU阈值为0.5和0.75的精确度，以及0.50:0.05:0.95范围内的平均精确度（mAP）。Recall@$N$
    定义为在每个视频中给定 $N$ 个分割实例时的最大召回率。这两个度量首先按类别进行评估，然后在类别集上求平均。IoU度量类似于实例级Jaccard $\mathcal{J}$，用于实例级AVOS（§[5.2.1](#S5.SS2.SSS1
    "5.2.1 Evaluation Metrics ‣ 5.2 Instance-level AVOS Performance Benchmarking ‣
    5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")）。
- en: 5.7.2 Results
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2 结果
- en: Table [13](#S5.T14 "TABLE 13 ‣ 5.8.2 Results ‣ 5.8 VPS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    gathers VIS results for on YouTube-VIS [[82](#bib.bib82)] val set, showing that
    Transformer-based architecture, *i.e*., VisTR [[238](#bib.bib238)], and redundant
    sequence proposal based solution Propose-Reduce [[244](#bib.bib244)], greatly
    improve the state-of-the-art.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 表[13](#S5.T14 "TABLE 13 ‣ 5.8.2 Results ‣ 5.8 VPS Performance Benchmarking ‣
    5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    汇集了在YouTube-VIS [[82](#bib.bib82)] 验证集上的VIS结果，显示基于Transformer的架构，即VisTR [[238](#bib.bib238)]，以及基于冗余序列提议的解决方案Propose-Reduce
    [[244](#bib.bib244)]，极大地提高了现有技术水平。
- en: 5.8 VPS Performance Benchmarking
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8 VPS 性能基准测试
- en: 5.8.1 Evaluation Metrics
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.8.1 评估指标
- en: In [[227](#bib.bib227)], the panoptic quality (PQ) metric used in image panoptic
    segmentation is modified as video panoptic quality (VPQ) to adapt to video panoptic
    segmentation.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[227](#bib.bib227)]中，用于图像全景分割的全景质量（PQ）度量被修改为视频全景质量（VPQ），以适应视频全景分割。
- en: $\bullet$${}_{\!}$ VPQ:${}_{\!}$ Given${}_{\!}$ a${}_{\!}$ snippet${}_{\!}$
    $V^{t:t+k\!}$ with${}_{\!}$ time${}_{\!}$ window${}_{\!}$ $k$,${}_{\!}$ true${}_{\!}$ po-${}_{\!}$
    sitive (TP) is defined by $\text{TP}\!=\!\{(u,\hat{u})_{\!}\!\in\!U_{\!}\!\times\!\hat{U}_{\!}\!:_{\!}\text{IoU}(u,\hat{u})\!>\!0.5\}$
    where $U$ and $\hat{U}$ are the set of the ground-truth and predicted tubes, respectively.
    False Positives (FP) and False Negatives (FN) are defined accordingly. After accumulating
    TP[c], FP[c], and FN[c] on all the clips with window size $k$ and class $c$, we
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$${}_{\!}$ VPQ：${}_{\!}$ 给定一个${}_{\!}$ 时段${}_{\!}$ $V^{t:t+k\!}$，时间窗口为${}_{\!}$
    $k$，${}_{\!}$ 真正的${}_{\!}$ 正例（TP）定义为 $\text{TP}\!=\!\{(u,\hat{u})_{\!}\!\in\!U_{\!}\!\times\!\hat{U}_{\!}\!:_{\!}\text{IoU}(u,\hat{u})\!>\!0.5\}$
    其中 $U$ 和 $\hat{U}$ 分别是地面真值和预测管道的集合。假阳性（FP）和假阴性（FN）也相应定义。在所有窗口大小为 $k$ 和类别 $c$ 的片段上累积
    TP[c]、FP[c] 和 FN[c] 后，我们
- en: define:${}_{\!}$ $\text{VPQ}^{k}_{\!}\!=_{\!}\!\frac{1}{N_{\text{class}}}\!\sum_{c}\!\frac{\sum_{(u,\hat{u})\in\text{TP}_{c}}\!\text{IoU}(u,\hat{u})}{|\text{TP}_{c}|+\frac{1}{2}|\text{FP}_{c}|+\frac{1}{2}|\text{FN}_{c}|}$.${}_{\!}$
    When${}_{\!}$ $k\!=\!1$,${}_{\!}$ VPQ¹
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：${}_{\!}$ $\text{VPQ}^{k}_{\!}\!=_{\!}\!\frac{1}{N_{\text{class}}}\!\sum_{c}\!\frac{\sum_{(u,\hat{u})\in\text{TP}_{c}}\!\text{IoU}(u,\hat{u})}{|\text{TP}_{c}|+\frac{1}{2}|\text{FP}_{c}|+\frac{1}{2}|\text{FN}_{c}|}$.${}_{\!}$
    当${}_{\!}$ $k\!=\!1$ 时，${}_{\!}$ VPQ¹
- en: is equivalent to PQ. For evaluation, $\text{VPQ}^{k}$ is reported over $k\!\in\!\{0,5,10,15\}$
    and finally, $\text{VPQ}\!=\!\frac{1}{4}\sum_{k\in\{0,5,10,15\}\!}\text{VPQ}^{k}$.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 与PQ等效。为了评估，$\text{VPQ}^{k}$ 在 $k\!\in\!\{0,5,10,15\}$ 上报告，最终，$\text{VPQ}\!=\!\frac{1}{4}\sum_{k\in\{0,5,10,15\}\!}\text{VPQ}^{k}$。
- en: 5.8.2 Results
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.8.2 结果
- en: ​Cityscapes-VPS​ [[227](#bib.bib227)]${}_{\!}$ is${}_{\!}$ chosen${}_{\!}$ for${}_{\!}$
    testing${}_{\!}$ VPS${}_{\!}$ methods.${}_{\!}$ As${}_{\!}$ shown${}_{\!}$ in${}_{\!}$
    Table​ [14](#S5.T13 "TABLE 14 ‣ 5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation"), ViP-DeepLab​ [[241](#bib.bib241)]${}_{\!}$ is${}_{\!}$
    the${}_{\!}$ top${}_{\!}$ one.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ​Cityscapes-VPS​ [[227](#bib.bib227)]${}_{\!}$ 被${}_{\!}$ 选为${}_{\!}$ 测试${}_{\!}$
    VPS${}_{\!}$ 方法的${}_{\!}$ 数据集。${}_{\!}$ 如${}_{\!}$ 表​ [14](#S5.T13 "TABLE 14 ‣
    5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation") 所示，ViP-DeepLab​
    [[241](#bib.bib241)]${}_{\!}$ 是${}_{\!}$ 最佳的${}_{\!}$ 方案。
- en: 'TABLE 13: Quantitative VIS results on YouTube-VIS [[82](#bib.bib82)] the val
    (§[5.7.2](#S5.SS7.SSS2 "5.7.2 Results ‣ 5.7 VIS Performance Benchmarking ‣ 5 Performance
    Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '表13: YouTube-VIS上的定量VIS结果[[82](#bib.bib82)]的val（§[5.7.2](#S5.SS7.SSS2 "5.7.2
    Results ‣ 5.7 VIS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation")）'
- en: in terms of Precision@$K$, mAP, Recall@$N$ and IoU.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 以Precision@$K$, mAP, Recall@$N$和IoU为指标。
- en: '|   Method | P@0.5$\uparrow$ | P@0.75$\uparrow$ | R@1$\uparrow$ | R@10$\uparrow$
    |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | P@0.5$\uparrow$ | P@0.75$\uparrow$ | R@1$\uparrow$ | R@10$\uparrow$
    |'
- en: '&#124; mAP$\uparrow$ &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; mAP$\uparrow$ &#124;'
- en: '&#124; 0.5:0.95 &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.5:0.95 &#124;'
- en: '| FPS$\uparrow$ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| FPS$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| fIRN [[242](#bib.bib242)] | 27.2 | 6.2 | 12.3 | 13.6 | 10.5 | 3 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| fIRN [[242](#bib.bib242)] | 27.2 | 6.2 | 12.3 | 13.6 | 10.5 | 3 |'
- en: '| MaskTrack R-CNN [[82](#bib.bib82)] | 51.1 | 32.6 | 31.0 | 35.5 | 30.3 | 20
    |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| MaskTrack R-CNN [[82](#bib.bib82)] | 51.1 | 32.6 | 31.0 | 35.5 | 30.3 | 20
    |'
- en: '| Sipmask [[232](#bib.bib232)] | 53.0 | 33.3 | 33.5 | 38.9 | 32.5 | 24 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| Sipmask [[232](#bib.bib232)] | 53.0 | 33.3 | 33.5 | 38.9 | 32.5 | 24 |'
- en: '| STEm-Seg [[233](#bib.bib233)] | 55.8 | 37.9 | 34.4 | 41.6 | 34.6 | 9 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| STEm-Seg [[233](#bib.bib233)] | 55.8 | 37.9 | 34.4 | 41.6 | 34.6 | 9 |'
- en: '| CrossVIS [[245](#bib.bib245)] | 57.3 | 39.7 | 36.0 | 42.0 | 36.6 | 36 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| CrossVIS [[245](#bib.bib245)] | 57.3 | 39.7 | 36.0 | 42.0 | 36.6 | 36 |'
- en: '| SemiTrack [[243](#bib.bib243)] | 61.1 | 39.8 | 36.9 | 44.5 | 38.3 | 10 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| SemiTrack [[243](#bib.bib243)] | 61.1 | 39.8 | 36.9 | 44.5 | 38.3 | 10 |'
- en: '| MaskProp [[226](#bib.bib226)] | - | 45.6 | - | - | 42.5 | ^†1 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| MaskProp [[226](#bib.bib226)] | - | 45.6 | - | - | 42.5 | ^†1 |'
- en: '| CompFeat [[235](#bib.bib235)] | 56.0 | 38.6 | 33.1 | 40.3 | 35.3 | ^†17 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| CompFeat [[235](#bib.bib235)] | 56.0 | 38.6 | 33.1 | 40.3 | 35.3 | ^†17 |'
- en: '| TraDeS [[236](#bib.bib236)] | 52.6 | 32.8 | 29.1 | 36.6 | 32.6 | 26 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| TraDeS [[236](#bib.bib236)] | 52.6 | 32.8 | 29.1 | 36.6 | 32.6 | 26 |'
- en: '| SG-Net [[237](#bib.bib237)] | 57.1 | 39.6 | 35.9 | 43.0 | 36.3 | 20 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| SG-Net [[237](#bib.bib237)] | 57.1 | 39.6 | 35.9 | 43.0 | 36.3 | 20 |'
- en: '| VisTR [[238](#bib.bib238)] | 64.0 | 45.0 | 38.3 | 44.9 | 40.1 | 58 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| VisTR [[238](#bib.bib238)] | 64.0 | 45.0 | 38.3 | 44.9 | 40.1 | 58 |'
- en: '| Propose-Reduce [[244](#bib.bib244)] | 71.6 | 51.8 | 46.3 | 56.0 | 47.6 |
    2 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Propose-Reduce [[244](#bib.bib244)] | 71.6 | 51.8 | 46.3 | 56.0 | 47.6 |
    2 |'
- en: 5.9 Summary
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.9 总结
- en: From the results, we can draw several conclusions. The most important of them
    is related to reproducibility. Across different video segmentation areas, many
    methods do not describe the setup for the experimentation or do not provide the
    source code for implementation. Some of them even do not release segmentation
    masks. Moreover, different methods use various datasets and backbone models. These
    make fair comparison impossible and hurt reproducibility.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以得出几个结论。其中最重要的是与可重现性有关。在不同的视频分割领域，许多方法并未描述实验设置，也没有提供实现的源代码。其中一些甚至没有发布分割掩模。此外，不同的方法使用不同的数据集和骨干模型。这使得公正的比较成为不可能，且损害了可重现性。
- en: Another important fact discovered thanks to this study is the lack of information
    about execution time and memory use. Many methods particularly in the fields of
    AVOS, LVOS, and VPS, do not report execution time and almost no paper reports
    memory use. This void is due to the fact that many methods focus only on accuracy
    without any concern about running time efficiency or memory requirements. However,
    in many application scenarios, such as mobile devices and self-driving cars, computational
    power and memory are typically limited. As benchmark datasets and challenges serve
    as a main driven factor behind the fast evolution of segmentation techniques,
    we encourage organizers of future video segmentation datasets to give this kind
    of metrics its deserved importance in benchmarking.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这项研究，发现了另一个重要事实，即对执行时间和内存使用的信息缺乏。许多方法，特别是在AVOS、LVOS和VPS领域，都没有报告执行时间，几乎没有论文报告内存使用。这种空白是因为许多方法只专注于准确性，而对运行时间效率或内存需求不关心。然而，在许多应用场景中，如移动设备和自动驾驶汽车，计算能力和内存通常是有限的。由于基准数据集和挑战对分割技术的快速发展起着主要推动作用，我们鼓励未来视频分割数据集的组织者将这种指标作为基准的重要因素。
- en: Finally, performance on some extensively studied video segmentation datasets,
    such as DAVIS[16] [[17](#bib.bib17)] in AVOS, DAVIS[17] [[81](#bib.bib81)] in
    SVOS, A2D Sentence [[196](#bib.bib196)] in LVOS, have nearly reached saturation.
    Though some new datasets are proposed recently and claim huge space for performance
    improvement, the dataset collectors just gather more challenging samples, without
    necessarily figuring out which exact challenges have and have not been solved.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在一些广泛研究的视频分割数据集上的表现，如AVOS中的DAVIS[16] [[17](#bib.bib17)]，SVOS中的DAVIS[17] [[81](#bib.bib81)]，LVOS中的A2D
    Sentence [[196](#bib.bib196)]，已经接近饱和。尽管最近提出了一些新数据集，并声称存在巨大的性能提升空间，但数据集收集者只是收集了更多具有挑战性的样本，而未必确定具体解决了哪些挑战。
- en: 6 Future Research Directions
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来研究方向
- en: Based on the reviewed research, we list several future research directions that
    we believe should be pursued.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 基于已审阅的研究，我们列出了几个我们认为应当追求的未来研究方向。
- en: '$\bullet$ Long-Term Video Segmentation: Long-term video segmentation is much
    closer to practical applications, such as video editing. However, as the sequences
    in existing datasets often span several seconds, the performance of VOS models
    over long video sequences (*e.g*., at the minute level) are still unexamined.
    Bringing VOS into the long-term setting will unlock new research lines, and put
    forward higher demand of the re-detection capability of VOS models.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 长期视频分割：长期视频分割与实际应用（如视频编辑）更为接近。然而，由于现有数据集中的序列往往跨越几秒钟，VOS模型在长视频序列（*例如*，以分钟级别）上的表现仍未得到检验。将VOS引入长期设置将开启新的研究方向，并对VOS模型的重新检测能力提出更高的要求。
- en: '$\bullet$ Open World Video Segmentation: Despite the obvious dynamic and open
    nature of the world, current VSS algorithms are typically developed in a closed-world
    paradigm, where all the object categories are known as a prior. These algorithms
    are often brittle once exposed to the realistic complexity of the open world,
    where they are unable to efficiently adapt and robustly generalize to unseen categories.
    For example, practical deployments of VSS systems in robotics, self-driving cars,
    and surveillance cannot afford to have complete knowledge on what classes to expect
    at inference time, while being trained in-house. This calls for smarter VSS systems,
    with a strong capability to identify unknown categories in their environments
    [[265](#bib.bib265)].'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 开放世界视频分割：尽管世界显然具有动态和开放的特性，当前的VSS算法通常是在一个闭世界范式下开发的，其中所有对象类别都是已知的。这些算法一旦暴露于开放世界的现实复杂性中，往往会变得脆弱，无法有效适应和稳健地推广到未见过的类别。例如，VSS系统在机器人、自驾车和监控中的实际部署无法在推断时完全了解期望的类别，而是在内部进行训练。这就需要更智能的VSS系统，具备强大的能力在环境中识别未知类别[[265](#bib.bib265)]。
- en: '$\bullet$ Cooperation across Different Video Segmentation Sub-fields: VOS and
    VSS face many common challenges, *e.g*., object occlusion, deformation, and fast
    motion. Moreover, there are no precedents for modeling these tasks in a unified
    framework. Thus we call for closer collaboration across different video segmentation
    sub-fields.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 不同视频分割子领域的合作：VOS和VSS面临许多共同的挑战，*例如*，目标遮挡、形变和快速运动。此外，目前尚无统一框架来建模这些任务。因此，我们呼吁在不同的视频分割子领域之间进行更紧密的合作。
- en: '$\bullet$ Annotation-Efficient Video Segmentation Solutions: Though great advances
    have been achieved in various videos segmentation tasks, current top-leading algorithms
    are built on fully-supervised deep learning techniques, requiring a huge amount
    of annotated data. Though semi-supervised, weakly supervised and unsupervised
    alternatives were explored in some literature, annotation-efficient solutions
    receive far less attention and typically show weak performance, compared with
    the fully supervised ones. As the high temporal correlations in video data can
    provide additional cues for supervision, exploring existing annotation-efficient
    techniques in static semantic segmentation in the area of video segmentation is
    an appealing direction.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 注释高效的视频分割解决方案：尽管在各种视频分割任务中取得了重大进展，但当前领先的算法仍基于完全监督的深度学习技术，需要大量标注数据。虽然在一些文献中探索了半监督、弱监督和无监督的替代方法，但注释高效的解决方案受到的关注远远少于完全监督的方法，并且通常表现较弱。由于视频数据中的高时间相关性可以提供额外的监督线索，在视频分割领域探索现有的注释高效技术是一个有吸引力的方向。
- en: '$\bullet$ Adaptive Computation: It is widely recognized that there exist high
    correlations among video frames. Though such data redundancy and continuity are
    exploited to reduce the computation cost in VSS, almost all current video segmentation
    models are fixed feed-forward structures or work alternatively between heavy and
    light-weight modes. We expect more flexible segmentation model designs towards
    more efficient and adaptive computation [[266](#bib.bib266)], which allows network
    architecture change on-the-fly – selectively activating part of the network in
    an input-dependent fashion.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 自适应计算：广泛认为视频帧之间存在高度相关性。尽管这种数据冗余和连续性被利用来减少VSS中的计算成本，但几乎所有当前的视频分割模型都是固定的前馈结构，或者在重型和轻型模式之间交替工作。我们期望设计出更多灵活的分割模型，以实现更高效和自适应的计算[[266](#bib.bib266)]，这允许网络架构动态变化——根据输入选择性地激活网络的部分。
- en: '$\bullet$ Neural Architecture Search: Video segmentation models are typically
    built upon hand-designed architectures, which may be suboptimal for capturing
    the nature of video data and limit the best possible performance. Using neural
    architecture search techniques to automate the design of video segmentation networks
    is a promising direction.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 神经架构搜索：视频分割模型通常建立在手工设计的架构基础上，这可能对于捕捉视频数据的特性并限制最佳性能。使用神经架构搜索技术来自动设计视频分割网络是一个有前景的方向。
- en: 7 CONCLUSION
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: To${}_{\!}$ our${}_{\!}$ knowledge,${}_{\!}$ this${}_{\!}$ is${}_{\!}$ the${}_{\!}$
    first${}_{\!}$ survey${}_{\!}$ to${}_{\!}$ comprehensively
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这项调查是首次全面回顾视频分割领域的最新进展。
- en: review recent progress in video segmentation. We provided the reader with the
    necessary background knowledge and summarized more than 150 deep learning models
    according to various criteria, including task settings, technique contributions,
    and learning strategies. We also presented a structured survey of 20 widely used
    video segmentation datasets and benchmarking results on 7 most widely-used ones.
    We discussed the results and provided insight into the shape of future research
    directions and open problems in the field. In conclusion, video segmentation has
    achieved notable progress thanks to the striking development of deep learning
    techniques, but several challenges still lie ahead.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为读者提供了必要的背景知识，并根据任务设置、技术贡献和学习策略等各种标准总结了超过150个深度学习模型。我们还展示了20个广泛使用的视频分割数据集的结构化调查，并对7个最常用的数据集进行了基准测试结果的展示。我们讨论了这些结果，并对未来研究方向和领域中的开放问题提供了见解。总之，视频分割由于深度学习技术的显著发展取得了显著进展，但仍面临若干挑战。
- en: References
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] W. Wang, J. Shen, R. Yang, and F. Porikli, “Saliency-aware video object
    segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 1, pp.
    20–33, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] W. Wang, J. Shen, R. Yang 和 F. Porikli，“显著性感知的视频对象分割，” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 40, no. 1, 第20–33页, 2017。'
- en: '[2] A. Papazoglou and V. Ferrari, “Fast object segmentation in unconstrained
    video,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 1777–1784.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Papazoglou 和 V. Ferrari，“在无约束视频中的快速对象分割，” *IEEE Int. Conf. Comput. Vis.*,
    2013，第1777–1784页。'
- en: '[3] C. Xu and J. J. Corso, “Evaluation of super-voxel methods for early video
    processing,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2012, pp. 1202–1209.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. Xu 和 J. J. Corso，“超体素方法在早期视频处理中的评估，” *IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2012，第1202–1209页。'
- en: '[4] T. Brox and J. Malik, “Object segmentation by long term analysis of point
    trajectories,” in *Proc. Eur. Conf. Comput. Vis.*, 2010, pp. 282–295.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Brox 和 J. Malik，“通过长期点轨迹分析进行对象分割，” *Eur. Conf. Comput. Vis.*, 2010，第282–295页。'
- en: '[5] Y. J. Lee, J. Kim, and K. Grauman, “Key-segments for video object segmentation,”
    in *Proc. IEEE Int. Conf. Comput. Vis.*, 2011, pp. 1995–2002.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Y. J. Lee, J. Kim 和 K. Grauman，“视频对象分割的关键片段，” *IEEE Int. Conf. Comput.
    Vis.*, 2011，第1995–2002页。'
- en: '[6] C.-P. Yu, H. Le, G. Zelinsky, and D. Samaras, “Efficient video segmentation
    using parametric graph partitioning,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2015, pp. 3155–3163.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C.-P. Yu, H. Le, G. Zelinsky 和 D. Samaras，“使用参数图分割的高效视频分割，” *IEEE Int.
    Conf. Comput. Vis.*, 2015，第3155–3163页。'
- en: '[7] M. Grundmann, V. Kwatra, M. Han, and I. Essa, “Efficient hierarchical graph-based
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010,
    pp. 2141–2148.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Grundmann, V. Kwatra, M. Han 和 I. Essa，“高效的分层图形视频分割，” *IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2010，第2141–2148页。'
- en: '[8] N. Shankar Nagaraja, F. R. Schmidt, and T. Brox, “Video segmentation with
    just a few strokes,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3235–3243.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] N. Shankar Nagaraja, F. R. Schmidt 和 T. Brox，“仅用几笔进行视频分割”，发表于*IEEE计算机视觉国际会议论文集*，2015年，第3235–3243页。'
- en: '[9] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung, “Fully connected
    object proposals for video segmentation,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2015, pp. 3227–3234.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] F. Perazzi, O. Wang, M. Gross 和 A. Sorkine-Hornung，“用于视频分割的全连接对象提议”，发表于*IEEE计算机视觉国际会议论文集*，2015年，第3227–3234页。'
- en: '[10] V. Badrinarayanan, I. Budvytis, and R. Cipolla, “Semi-supervised video
    segmentation using tree structured graphical models,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 35, no. 11, pp. 2751–2764, 2013.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] V. Badrinarayanan, I. Budvytis 和 R. Cipolla，“使用树结构图模型的半监督视频分割”，*IEEE模式分析与机器智能学报*，第35卷，第11期，第2751–2764页，2013年。'
- en: '[11] W.-D. Jang and C.-S. Kim, “Streaming video segmentation via short-term
    hierarchical segmentation and frame-by-frame markov random field optimization,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 599–615.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] W.-D. Jang 和 C.-S. Kim，“通过短期层次分割和逐帧马尔可夫随机场优化进行流视频分割”，发表于*欧洲计算机视觉会议论文集*，2016年，第599–615页。'
- en: '[12] B. Liu and X. He, “Multiclass semantic video segmentation with object-level
    active inference,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015,
    pp. 4286–4294.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] B. Liu 和 X. He，“具有对象级主动推断的多类语义视频分割”，发表于*IEEE计算机视觉与模式识别会议论文集*，2015年，第4286–4294页。'
- en: '[13] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 3431–3440.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Long, E. Shelhamer 和 T. Darrell，“用于语义分割的全卷积网络”，发表于*IEEE计算机视觉与模式识别会议论文集*，2015年，第3431–3440页。'
- en: '[14] D. M. Thounaojam, A. Trivedi, K. M. Singh, and S. Roy, “A survey on video
    segmentation,” in *Intelligent computing, networking, and informatics*, 2014,
    pp. 903–912.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. M. Thounaojam, A. Trivedi, K. M. Singh 和 S. Roy，“视频分割的综述”，发表于*智能计算、网络和信息学*，2014年，第903–912页。'
- en: '[15] Y.-J. Zhang, “An overview of image and video segmentation in the last
    40 years,” *Advances in Image and Video Segmentation*, pp. 1–16, 2006.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y.-J. Zhang，“过去40年图像和视频分割的概述”，*图像和视频分割进展*，第1–16页，2006年。'
- en: '[16] R. Yao, G. Lin, S. Xia, J. Zhao, and Y. Zhou, “Video object segmentation
    and tracking: A survey,” *ACM Transactions on Intelligent Systems and Technology*,
    vol. 11, no. 4, pp. 1–47, 2020.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Yao, G. Lin, S. Xia, J. Zhao 和 Y. Zhou，“视频对象分割与跟踪：综述”，*ACM智能系统与技术学报*，第11卷，第4期，第1–47页，2020年。'
- en: '[17] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung,
    “A benchmark dataset and evaluation methodology for video object segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 724–732.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross 和 A. Sorkine-Hornung，“视频对象分割的基准数据集和评估方法”，发表于*IEEE计算机视觉与模式识别会议论文集*，2016年，第724–732页。'
- en: '[18] S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-Taixé, D. Cremers, and
    L. V. Gool, “One-shot video object segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 5320–5329.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-Taixé, D. Cremers 和 L.
    V. Gool，“一次性视频对象分割”，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第5320–5329页。'
- en: '[19] V. Badrinarayanan, F. Galasso, and R. Cipolla, “Label propagation in video
    sequences,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010, pp. 3265–3272.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. Badrinarayanan, F. Galasso 和 R. Cipolla，“视频序列中的标签传播”，发表于*IEEE计算机视觉与模式识别会议论文集*，2010年，第3265–3272页。'
- en: '[20] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 43,
    no. 11, pp. 4037–4058, 2020.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. Jing 和 Y. Tian，“使用深度神经网络进行自监督视觉特征学习：综述”，*IEEE模式分析与机器智能学报*，第43卷，第11期，第4037–4058页，2020年。'
- en: '[21] L. G. Roberts, “Machine perception of three-dimensional solids,” *Optical
    and Electro-Optical Information Processing*, 1965.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] L. G. Roberts，“三维固体的机器感知”，*光学与电子光学信息处理*，1965年。'
- en: '[22] J. Chang, D. Wei, and J. W. Fisher, “A video representation using temporal
    superpixels,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp.
    2051–2058.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Chang, D. Wei 和 J. W. Fisher，“使用时间超像素的视频表示”，发表于*IEEE计算机视觉与模式识别会议论文集*，2013年，第2051–2058页。'
- en: '[23] R. Jain and H.-H. Nagel, “On the analysis of accumulative difference pictures
    from image sequences of real world scenes,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    no. 2, pp. 206–214, 1979.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Jain 和 H.-H. Nagel，“对真实场景图像序列的累积差异图分析”，*IEEE模式分析与机器智能学报*，第2期，第206–214页，1979年。'
- en: '[24] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder:
    Real-time tracking of the human body,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 19, no. 7, pp. 780–785, 1997.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. R. Wren, A. Azarbayejani, T. Darrell 和 A. P. Pentland, “Pfinder：人体的实时跟踪，”*IEEE模式分析与机器智能汇刊*，第19卷，第7期，第780–785页，1997年。'
- en: '[25] A. Criminisi, G. Cross, A. Blake, and V. Kolmogorov, “Bilayer segmentation
    of live video,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2006, pp.
    53–60.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Criminisi, G. Cross, A. Blake 和 V. Kolmogorov, “实时视频的双层分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2006年，第53–60页。'
- en: '[26] S. Brutzer, B. Höferlin, and G. Heidemann, “Evaluation of background subtraction
    techniques for video surveillance,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2011, pp. 1937–1944.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Brutzer, B. Höferlin 和 G. Heidemann, “视频监控背景减除技术的评估，”发表于*IEEE计算机视觉与模式识别会议论文集*，2011年，第1937–1944页。'
- en: '[27] E. Hayman and J.-O. Eklundh, “Statistical background subtraction for a
    mobile observer,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2003, pp. 67–67.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] E. Hayman 和 J.-O. Eklundh, “针对移动观察者的统计背景减除，”发表于*IEEE国际计算机视觉会议论文集*，2003年，第67–67页。'
- en: '[28] M. Irani and P. Anandan, “A unified approach to moving object detection
    in 2d and 3d scenes,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 20, no. 6,
    pp. 577–589, 1998.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Irani 和 P. Anandan, “二维和三维场景中移动目标检测的统一方法，”*IEEE模式分析与机器智能汇刊*，第20卷，第6期，第577–589页，1998年。'
- en: '[29] J. Y. Wang and E. H. Adelson, “Layered representation for motion analysis,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 1993, pp. 361–366.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Y. Wang 和 E. H. Adelson, “用于运动分析的分层表示，”发表于*IEEE计算机视觉与模式识别会议论文集*，1993年，第361–366页。'
- en: '[30] H. S. Sawhney and S. Ayer, “Compact representations of videos through
    dominant and multiple motion estimation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 18, no. 8, pp. 814–830, 1996.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. S. Sawhney 和 S. Ayer, “通过主导和多重运动估计进行视频的紧凑表示，”*IEEE模式分析与机器智能汇刊*，第18卷，第8期，第814–830页，1996年。'
- en: '[31] J. Costeira and T. Kanade, “A multi-body factorization method for motion
    analysis,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 1995, pp. 1071–1076.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Costeira 和 T. Kanade, “一种用于运动分析的多体分解方法，”发表于*IEEE国际计算机视觉会议论文集*，1995年，第1071–1076页。'
- en: '[32] D. Cremers and S. Soatto, “Motion competition: A variational approach
    to piecewise parametric motion segmentation,” *Int. J. Comput. Vis.*, vol. 62,
    no. 3, pp. 249–265, 2005.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Cremers 和 S. Soatto, “运动竞争：一种变分方法用于分段参数运动分割，”*国际计算机视觉期刊*，第62卷，第3期，第249–265页，2005年。'
- en: '[33] P. Ochs and T. Brox, “Object segmentation in video: A hierarchical variational
    approach for turning point trajectories into dense regions,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2011, pp. 1583–1590.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Ochs 和 T. Brox, “视频中的对象分割：一种层次变分方法用于将转折点轨迹转化为密集区域，”发表于*IEEE国际计算机视觉会议论文集*，2011年，第1583–1590页。'
- en: '[34] K. Fragkiadaki, G. Zhang, and J. Shi, “Video segmentation by tracing discontinuities
    in a trajectory embedding,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 1846–1853.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] K. Fragkiadaki, G. Zhang 和 J. Shi, “通过追踪轨迹嵌入中的不连续性进行视频分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2012年，第1846–1853页。'
- en: '[35] M. Keuper, B. Andres, and T. Brox, “Motion trajectory segmentation via
    minimum cost multicuts,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3271–3279.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Keuper, B. Andres 和 T. Brox, “通过最小成本多重割进行运动轨迹分割，”发表于*IEEE国际计算机视觉会议论文集*，2015年，第3271–3279页。'
- en: '[36] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by long
    term video analysis,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 36, no. 6,
    pp. 1187–1200, 2014.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. Ochs, J. Malik 和 T. Brox, “通过长期视频分析进行移动对象的分割，”*IEEE模式分析与机器智能汇刊*，第36卷，第6期，第1187–1200页，2014年。'
- en: '[37] A. Faktor and M. Irani, “Video segmentation by non-local consensus voting,”
    in *Proc. British Mach. Vis. Conf.*, 2014.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Faktor 和 M. Irani, “通过非局部共识投票进行视频分割，”发表于*英国机器视觉会议论文集*，2014年。'
- en: '[38] T. Ma and L. J. Latecki, “Maximum weight cliques with mutex constraints
    for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 670–677.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Ma 和 L. J. Latecki, “具有互斥约束的最大权重团用于视频目标分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2012年，第670–677页。'
- en: '[39] D. Zhang, O. Javed, and M. Shah, “Video object segmentation through spatially
    accurate and temporally dense extraction of primary object regions,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 628–635.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. Zhang, O. Javed 和 M. Shah, “通过空间准确和时间密集的主要对象区域提取进行视频目标分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2013年，第628–635页。'
- en: '[40] F. Xiao and Y. Jae Lee, “Track and segment: An iterative unsupervised
    approach for video object proposals,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2016, pp. 933–942.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] F. Xiao 和 Y. Jae Lee，“跟踪与分割: 一种迭代的无监督视频对象提议方法，”在 *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*，2016年，第933–942页。'
- en: '[41] I. Endres and D. Hoiem, “Category independent object proposals,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2010, pp. 575–588.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] I. Endres 和 D. Hoiem，“类别独立的对象提议，”在 *Proc. Eur. Conf. Comput. Vis.*，2010年，第575–588页。'
- en: '[42] S. Avinash Ramakanth and R. Venkatesh Babu, “SeamSeg: Video object segmentation
    using patch seams,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014,
    pp. 376–383.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Avinash Ramakanth 和 R. Venkatesh Babu，“SeamSeg: 使用补丁缝隙进行视频对象分割，”在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2014年，第376–383页。'
- en: '[43] Y.-H. Tsai, M.-H. Yang, and M. J. Black, “Video segmentation via object
    flow,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 3899–3908.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y.-H. Tsai, M.-H. Yang, 和 M. J. Black，“通过对象流进行视频分割，”在 *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*，2016年，第3899–3908页。'
- en: '[44] W. Wang, J. Shen, F. Porikli, and R. Yang, “Semi-supervised video object
    segmentation with super-trajectories,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 41, no. 4, pp. 985–998, 2018.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] W. Wang, J. Shen, F. Porikli, 和 R. Yang，“半监督视频对象分割与超轨迹，”*IEEE Trans. Pattern
    Anal. Mach. Intell.*，第41卷，第4期，第985–998页，2018年。'
- en: '[45] D. Tsai, M. Flagg, and J. M. Rehg, “Motion coherent tracking using multi-label
    MRF optimization,” in *Proc. British Mach. Vis. Conf.*, 2010, pp. 190–202.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] D. Tsai, M. Flagg, 和 J. M. Rehg，“使用多标签 MRF 优化的运动一致跟踪，”在 *Proc. British
    Mach. Vis. Conf.*，2010年，第190–202页。'
- en: '[46] L. Wen, D. Du, Z. Lei, S. Z. Li, and M.-H. Yang, “JOTS: Joint online tracking
    and segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015,
    pp. 2226–2234.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] L. Wen, D. Du, Z. Lei, S. Z. Li, 和 M.-H. Yang，“JOTS: 联合在线跟踪和分割，”在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2015年，第2226–2234页。'
- en: '[47] A. Agarwala, A. Hertzmann, D. H. Salesin, and S. M. Seitz, “Keyframe-based
    tracking for rotoscoping and animation,” *ACM Tran. Graphics*, vol. 23, no. 3,
    pp. 584–591, 2004.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Agarwala, A. Hertzmann, D. H. Salesin, 和 S. M. Seitz，“基于关键帧的跟踪用于转描和动画，”*ACM
    Tran. Graphics*，第23卷，第3期，第584–591页，2004年。'
- en: '[48] W. Li, F. Viola, J. Starck, G. J. Brostow, and N. D. Campbell, “Roto++
    accelerating professional rotoscoping using shape manifolds,” *ACM Tran. Graphics*,
    vol. 35, no. 4, pp. 1–15, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] W. Li, F. Viola, J. Starck, G. J. Brostow, 和 N. D. Campbell，“Roto++ 加速专业转描使用形状流形，”*ACM
    Tran. Graphics*，第35卷，第4期，第1–15页，2016年。'
- en: '[49] X. Bai, J. Wang, D. Simons, and G. Sapiro, “Video SnapCut: robust video
    object cutout using localized classifiers,” *ACM Tran. Graphics*, vol. 28, no. 3,
    p. 70, 2009.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Bai, J. Wang, D. Simons, 和 G. Sapiro，“视频 SnapCut: 使用局部分类器的鲁棒视频对象抠图，”*ACM
    Tran. Graphics*，第28卷，第3期，第70页，2009年。'
- en: '[50] A. Criminisi, T. Sharp, C. Rother, and P. Pérez, “Geodesic image and video
    editing,” *ACM Tran. Graphics*, vol. 29, no. 5, pp. 134–1, 2010.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Criminisi, T. Sharp, C. Rother, 和 P. Pérez，“测地线图像和视频编辑，”*ACM Tran.
    Graphics*，第29卷，第5期，第134–1页，2010年。'
- en: '[51] F. Zhong, X. Qin, Q. Peng, and X. Meng, “Discontinuity-aware video object
    cutout,” *ACM Tran. Graphics*, vol. 31, no. 6, p. 175, 2012.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] F. Zhong, X. Qin, Q. Peng, 和 X. Meng，“关注不连续性的视频对象抠图，”*ACM Tran. Graphics*，第31卷，第6期，第175页，2012年。'
- en: '[52] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen, “JumpCut: non-successive
    mask transfer and interpolation for video cutout.” *ACM Tran. Graphics*, vol. 34,
    no. 6, pp. 195–1, 2015.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, 和 B. Chen，“JumpCut: 非连续遮罩转移与视频抠图插值。”
    *ACM Tran. Graphics*，第34卷，第6期，第195–1页，2015年。'
- en: '[53] Y. Lu, X. Bai, L. Shapiro, and J. Wang, “Coherent parametric contours
    for interactive video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2016, pp. 642–650.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Lu, X. Bai, L. Shapiro, 和 J. Wang，“用于交互式视频对象分割的连贯参数轮廓，”在 *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2016年，第642–650页。'
- en: '[54] W. Wang, J. Shen, and F. Porikli, “Selective video object cutout,” *IEEE
    Trans. Image Process.*, vol. 26, no. 12, pp. 5645–5655, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Wang, J. Shen, 和 F. Porikli，“选择性视频对象抠图，”*IEEE Trans. Image Process.*，第26卷，第12期，第5645–5655页，2017年。'
- en: '[55] A. Jain, S. Chatterjee, and R. Vidal, “Coarse-to-fine semantic video segmentation
    using supervoxel trees,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 1865–1872.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Jain, S. Chatterjee, 和 R. Vidal，“使用超体素树的粗到细语义视频分割，”在 *Proc. IEEE Int.
    Conf. Comput. Vis.*，2013年，第1865–1872页。'
- en: '[56] A. Kae, B. Marlin, and E. Learned-Miller, “The shape-time random field
    for semantic video labeling,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 272–279.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Kae, B. Marlin, 和 E. Learned-Miller，“用于语义视频标注的形状时间随机场，”在 *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2014年，第272–279页。'
- en: '[57] K. Tang, R. Sukthankar, J. Yagnik, and L. Fei-Fei, “Discriminative segment
    annotation in weakly labeled video,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2013, pp. 2483–2490.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Tang, R. Sukthankar, J. Yagnik, 和 L. Fei-Fei，“在弱标签视频中的区分性分割标注”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2013年，页码2483–2490。'
- en: '[58] X. Liu, D. Tao, M. Song, Y. Ruan, C. Chen, and J. Bu, “Weakly supervised
    multiclass video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 57–64.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] X. Liu, D. Tao, M. Song, Y. Ruan, C. Chen, 和 J. Bu，“弱监督多类视频分割”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2014年，页码57–64。'
- en: '[59] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and
    M. Shah, “Visual tracking: An experimental survey,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 36, no. 7, pp. 1442–1468, 2013.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, 和
    M. Shah，“视觉跟踪：实验性综述”，*IEEE模式分析与机器智能汇刊*，第36卷，第7期，页码1442–1468，2013年。'
- en: '[60] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,” *ACM Computing
    Surveys*, vol. 38, no. 4, pp. 1–45, 2006.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Yilmaz, O. Javed, 和 M. Shah，“目标跟踪：综述”，*ACM计算调查*，第38卷，第4期，页码1–45，2006年。'
- en: '[61] C. Bibby and I. Reid, “Robust real-time visual tracking using pixel-wise
    posteriors,” in *Proc. Eur. Conf. Comput. Vis.*, 2008, pp. 831–844.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. Bibby 和 I. Reid，“使用像素级后验进行鲁棒的实时视觉跟踪”，发表于 *欧洲计算机视觉会议论文集*，2008年，页码831–844。'
- en: '[62] X. Ren and J. Malik, “Tracking as repeated figure/ground segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2007, pp. 1–8.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. Ren 和 J. Malik，“将跟踪视作重复的前景/背景分割”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2007年，页码1–8。'
- en: '[63] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez-Gonzalez,
    and J. Garcia-Rodriguez, “A survey on deep learning techniques for image and video
    semantic segmentation,” *Applied Soft Computing*, vol. 70, pp. 41–65, 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P.
    Martinez-Gonzalez, 和 J. Garcia-Rodriguez，“深度学习技术在图像和视频语义分割中的综述”，*应用软计算*，第70卷，页码41–65，2018年。'
- en: '[64] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, and L. Van Gool, “Exploring
    cross-image pixel contrast for semantic segmentation,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2021, pp. 7303–7313.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, 和 L. Van Gool，“探索跨图像像素对比以进行语义分割”，发表于
    *IEEE国际计算机视觉会议论文集*，2021年，页码7303–7313。'
- en: '[65] T. Zhou, W. Wang, E. Konukoglu, and L. Van Gool, “Rethinking semantic
    segmentation: A prototype view,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2022, pp. 2582–2593.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] T. Zhou, W. Wang, E. Konukoglu, 和 L. Van Gool，“重新思考语义分割：一种原型视角”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2022年，页码2582–2593。'
- en: '[66] S. D. Jain, B. Xiong, and K. Grauman, “Fusionseg: Learning to combine
    motion and appearance for fully automatic segmention of generic objects in videos,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 686–695.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. D. Jain, B. Xiong, 和 K. Grauman，“Fusionseg：学习结合运动和外观以实现视频中通用对象的完全自动分割”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2017年，页码686–695。'
- en: '[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large
    Scale Visual Recognition Challenge,” *Int. J. Comput. Vis.*, vol. 115, no. 3,
    pp. 211–252, 2015.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 L. Fei-Fei，“ImageNet大规模视觉识别挑战”，*国际计算机视觉杂志*，第115卷，第3期，页码211–252，2015年。'
- en: '[68] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang, “Segflow: Joint learning
    for video object segmentation and optical flow,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2017, pp. 686–695.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Cheng, Y.-H. Tsai, S. Wang, 和 M.-H. Yang，“Segflow：视频目标分割与光流的联合学习”，发表于
    *IEEE国际计算机视觉会议论文集*，2017年，页码686–695。'
- en: '[69] P. Tokmakov, K. Alahari, and C. Schmid, “Learning video object segmentation
    with visual memory,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 4491–4500.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] P. Tokmakov, K. Alahari, 和 C. Schmid，“利用视觉记忆学习视频目标分割”，发表于 *IEEE国际计算机视觉会议论文集*，2017年，页码4491–4500。'
- en: '[70] ——, “Learning motion patterns in videos,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 531–539.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] ——，“学习视频中的运动模式”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页码531–539。'
- en: '[71] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, pp. 4040–4048.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    和 T. Brox，“一个大型数据集用于训练卷积网络以进行视差、光流和场景流估计”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2016年，页码4040–4048。'
- en: '[72] J. Li, A. Zheng, X. Chen, and B. Zhou, “Primary video object segmentation
    via complementary cnns and neighborhood reversible flow,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2017, pp. 1417–1425.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Li, A. Zheng, X. Chen, 和 B. Zhou, “通过互补的 CNN 和邻域可逆流进行初步视频对象分割，” 见于
    *IEEE 国际计算机视觉会议论文集*，2017年，第1417–1425页。'
- en: '[73] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Ferrari, “Learning
    object class detectors from weakly annotated video,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2012, pp. 3282–3289.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Prest, C. Leistner, J. Civera, C. Schmid, 和 V. Ferrari, “从弱标注视频中学习对象类别检测器，”
    见于 *IEEE 计算机视觉与模式识别会议论文集*，2012年，第3282–3289页。'
- en: '[74] S. Li, B. Seybold, A. Vorobyov, A. Fathi, Q. Huang, and C.-C. Jay Kuo,
    “Instance embedding transfer to unsupervised video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 6526–6535.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. Li, B. Seybold, A. Vorobyov, A. Fathi, Q. Huang, 和 C.-C. Jay Kuo, “实例嵌入迁移到无监督视频对象分割，”
    见于 *IEEE 计算机视觉与模式识别会议论文集*，2018年，第6526–6535页。'
- en: '[75] G. Li, Y. Xie, T. Wei, K. Wang, and L. Lin, “Flow guided recurrent neural
    encoder for video salient object detection,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2018, pp. 3243–3252.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Li, Y. Xie, T. Wei, K. Wang, 和 L. Lin, “流引导递归神经编码器用于视频显著对象检测，” 见于 *IEEE
    计算机视觉与模式识别会议论文集*，2018年，第3243–3252页。'
- en: '[76] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg, “Video segmentation
    by tracking many figure-ground segments,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2013, pp. 2192–2199.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] F. Li, T. Kim, A. Humayun, D. Tsai, 和 J. M. Rehg, “通过跟踪多个图形-背景段进行视频分割，”
    见于 *IEEE 国际计算机视觉会议论文集*，2013年，第2192–2199页。'
- en: '[77] S. Li, B. Seybold, A. Vorobyov, X. Lei, and C.-C. Jay Kuo, “Unsupervised
    video object segmentation with motion-based bilateral networks,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2018, pp. 215–231.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Li, B. Seybold, A. Vorobyov, X. Lei, 和 C.-C. Jay Kuo, “基于运动的双边网络的无监督视频对象分割，”
    见于 *欧洲计算机视觉会议论文集*，2018年，第215–231页。'
- en: '[78] H. Song, W. Wang, S. Zhao, J. Shen, and K.-M. Lam, “Pyramid dilated deeper
    convlstm for video salient object detection,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018, pp. 744–760.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] H. Song, W. Wang, S. Zhao, J. Shen, 和 K.-M. Lam, “金字塔扩张深层 ConvLSTM 用于视频显著对象检测，”
    见于 *欧洲计算机视觉会议论文集*，2018年，第744–760页。'
- en: '[79] M. Siam, C. Jiang, S. Lu, L. Petrich, M. Gamal, M. Elhoseiny, and M. Jagersand,
    “Video segmentation using teacher-student adaptation in a human robot interaction
    (hri) setting,” in *International Conference on Robotics and Automation*, 2019,
    pp. 50–56.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Siam, C. Jiang, S. Lu, L. Petrich, M. Gamal, M. Elhoseiny, 和 M. Jagersand,
    “在人机交互（HRI）环境中使用教师-学生适应的视频分割，” 见于 *国际机器人与自动化会议*，2019年，第50–56页。'
- en: '[80] C. Ventura, M. Bellver, A. Girbau, A. Salvador, F. Marques, and X. Giro-i
    Nieto, “Rvos: End-to-end recurrent network for video object segmentation,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 5277–5286.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. Ventura, M. Bellver, A. Girbau, A. Salvador, F. Marques, 和 X. Giro-i
    Nieto, “RVOS：端到端递归网络用于视频对象分割，” 见于 *IEEE 计算机视觉与模式识别会议论文集*，2019年，第5277–5286页。'
- en: '[81] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung,
    and L. Van Gool, “The 2017 davis challenge on video object segmentation,” *arXiv
    preprint arXiv:1704.00675*, 2017.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung,
    和 L. Van Gool, “2017 年 DAVIS 挑战赛视频对象分割，” *arXiv 预印本 arXiv:1704.00675*，2017年。'
- en: '[82] L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2019, pp. 5188–5197.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] L. Yang, Y. Fan, 和 N. Xu, “视频实例分割，” 见于 *IEEE 国际计算机视觉会议论文集*，2019年，第5188–5197页。'
- en: '[83] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and F. Porikli, “See more, know
    more: Unsupervised video object segmentation with co-attention siamese networks,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 3623–3632.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, 和 F. Porikli, “看得更多，知道更多：利用协同注意力孪生网络进行无监督视频对象分割，”
    见于 *IEEE 计算机视觉与模式识别会议论文集*，2019年，第3623–3632页。'
- en: '[84] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global
    contrast based salient region detection,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 37, no. 3, pp. 569–582, 2015.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, 和 S.-M. Hu, “基于全局对比度的显著区域检测，”
    *IEEE 模式分析与机器智能期刊*，第37卷，第3期，第569–582页，2015年。'
- en: '[85] C. Yang, L. Zhang, H. Lu, X. Ruan, and M. Yang, “Saliency detection via
    graph-based manifold ranking,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2013, pp. 3166–3173.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Yang, L. Zhang, H. Lu, X. Ruan, 和 M. Yang, “通过基于图的流形排序进行显著性检测，” 见于
    *IEEE 计算机视觉与模式识别会议论文集*，2013年，第3166–3173页。'
- en: '[86] Y. Yang, A. Loquercio, D. Scaramuzza, and S. Soatto, “Unsupervised moving
    object detection via contextual information separation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 879–888.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Yang, A. Loquercio, D. Scaramuzza, 和 S. Soatto, “通过上下文信息分离进行无监督的移动物体检测，”发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第879–888页。'
- en: '[87] W. Wang, H. Song, S. Zhao, J. Shen, S. Zhao, S. C. Hoi, and H. Ling, “Learning
    unsupervised video object segmentation through visual attention,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 3064–3074.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] W. Wang, H. Song, S. Zhao, J. Shen, S. Zhao, S. C. Hoi, 和 H. Ling, “通过视觉注意力学习无监督视频物体分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第3064–3074页。'
- en: '[88] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of
    salient object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 280–287.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Li, X. Hou, C. Koch, J. M. Rehg, 和 A. L. Yuille, “显著物体分割的秘密，”发表于*IEEE计算机视觉与模式识别会议论文集*，2014年，第280–287页。'
- en: '[89] W. Wang, X. Lu, J. Shen, D. J. Crandall, and L. Shao, “Zero-shot video
    object segmentation via attentive graph neural networks,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2019, pp. 9236–9245.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. Wang, X. Lu, J. Shen, D. J. Crandall, 和 L. Shao, “通过注意力图神经网络进行零样本视频物体分割，”发表于*IEEE国际计算机视觉会议论文集*，2019年，第9236–9245页。'
- en: '[90] H. Li, G. Chen, G. Li, and Y. Yu, “Motion guided attention for video salient
    object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp. 7274–7283.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] H. Li, G. Chen, G. Li, 和 Y. Yu, “用于视频显著物体检测的运动引导注意力，”发表于*IEEE国际计算机视觉会议论文集*，2019年，第7274–7283页。'
- en: '[91] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan, “Learning
    to detect salient objects with image-level supervision,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, pp. 136–145.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, 和 X. Ruan, “通过图像级监督学习检测显著物体，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第136–145页。'
- en: '[92] Z. Yang, Q. Wang, L. Bertinetto, W. Hu, S. Bai, and P. H. Torr, “Anchor
    diffusion for unsupervised video object segmentation,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2019, pp. 931–940.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Z. Yang, Q. Wang, L. Bertinetto, W. Hu, S. Bai, 和 P. H. Torr, “用于无监督视频物体分割的锚点扩散，”发表于*IEEE国际计算机视觉会议论文集*，2019年，第931–940页。'
- en: '[93] P. Tokmakov, C. Schmid, and K. Alahari, “Learning to segment moving objects,”
    *Int. J. Comput. Vis.*, vol. 127, no. 3, pp. 282–301, 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] P. Tokmakov, C. Schmid, 和 K. Alahari, “学习分割移动物体，”*国际计算机视觉期刊*，第127卷，第3期，第282–301页，2019年。'
- en: '[94] T. Zhou, S. Wang, Y. Zhou, Y. Yao, J. Li, and L. Shao, “Motion-attentive
    transition for zero-shot video object segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2020, pp. 13 066–13 073.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] T. Zhou, S. Wang, Y. Zhou, Y. Yao, J. Li, 和 L. Shao, “针对零样本视频物体分割的运动关注转移，”发表于*AAAI人工智能会议*，2020年，第13,066–13,073页。'
- en: '[95] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, “Youtube-vos:
    A large-scale video object segmentation benchmark,” *arXiv preprint arXiv:1809.03327*,
    2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, 和 T. Huang, “Youtube-vos：大规模视频物体分割基准，”*arXiv预印本arXiv:1809.03327*，2018年。'
- en: '[96] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, and S.-P. Lu, “Pyramid constrained
    self-attention network for fast video salient object detection,” in *AAAI Conference
    on Artificial Intelligence*, 2020, pp. 10 869–10 876.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, 和 S.-P. Lu, “用于快速视频显著物体检测的金字塔约束自注意力网络，”发表于*AAAI人工智能会议*，2020年，第10,869–10,876页。'
- en: '[97] D.-P. Fan, W. Wang, M.-M. Cheng, and J. Shen, “Shifting more attention
    to video salient object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, pp. 8554–8564.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] D.-P. Fan, W. Wang, M.-M. Cheng, 和 J. Shen, “将更多关注转移到视频显著物体检测上，”发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第8554–8564页。'
- en: '[98] X. Lu, W. Wang, J. Shen, Y.-W. Tai, D. J. Crandall, and S. C. Hoi, “Learning
    video object segmentation from unlabeled videos,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 8960–8970.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Lu, W. Wang, J. Shen, Y.-W. Tai, D. J. Crandall, 和 S. C. Hoi, “从未标注的视频中学习视频物体分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2020年，第8960–8970页。'
- en: '[99] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, and E. Gavves, “Long-term tracking in the wild: A benchmark,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 670–685.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, 和 E. Gavves, “野外长期跟踪：一个基准测试，”发表于*欧洲计算机视觉会议论文集*，2018年，第670–685页。'
- en: '[100] X. Lu, W. Wang, M. Danelljan, T. Zhou, J. Shen, and L. Van Gool, “Video
    object segmentation with episodic graph memory networks,” in *Proc. Eur. Conf.
    Comput. Vis.*, 2020, pp. 661–679.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Lu, W. Wang, M. Danelljan, T. Zhou, J. Shen, 和 L. Van Gool, “带有情节图记忆网络的视频物体分割，”发表于*欧洲计算机视觉会议论文集*，2020年，第661–679页。'
- en: '[101] L. Zhang, J. Zhang, Z. Lin, R. Mech, H. Lu, and Y. He, “Unsupervised
    video object segmentation with joint hotspot tracking,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 490–506.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. Zhang, J. Zhang, Z. Lin, R. Mech, H. Lu 和 Y. He，“通过联合热点跟踪的无监督视频对象分割”，在*Proc.
    Eur. Conf. Comput. Vis.*，2020年，页码490–506。'
- en: '[102] M. Jiang, S. Huang, J. Duan, and Q. Zhao, “Salicon: Saliency in context,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 1072–1080.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Jiang, S. Huang, J. Duan 和 Q. Zhao，“Salicon：上下文中的显著性”，在*Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2015年，页码1072–1080。'
- en: '[103] M. Everingham, S. M. A. Eslami, L. J. V. Gool, C. K. I. Williams, J. M.
    Winn, and A. Zisserman, “The pascal visual object classes challenge: A retrospective,”
    *Int. J. Comput. Vis.*, vol. 111, no. 1, pp. 98–136, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Everingham, S. M. A. Eslami, L. J. V. Gool, C. K. I. Williams, J.
    M. Winn 和 A. Zisserman，“Pascal视觉对象类别挑战：回顾”，*Int. J. Comput. Vis.*, 第111卷，第1期，页码98–136，2015年。'
- en: '[104] M. Zhen, S. Li, L. Zhou, J. Shang, H. Feng, T. Fang, and L. Quan, “Learning
    discriminative feature with crf for unsupervised video object segmentation,” in
    *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 445–462.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Zhen, S. Li, L. Zhou, J. Shang, H. Feng, T. Fang 和 L. Quan，“通过CRF学习具有区分性的特征用于无监督视频对象分割”，在*Proc.
    Eur. Conf. Comput. Vis.*，2020年，页码445–462。'
- en: '[105] D. Liu, D. Yu, C. Wang, and P. Zhou, “F2net: Learning to focus on the
    foreground for unsupervised video object segmentation,” in *AAAI Conference on
    Artificial Intelligence*, 2021, pp. 2109–2117.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] D. Liu, D. Yu, C. Wang 和 P. Zhou，“F2net：学习关注前景以进行无监督视频对象分割”，在*AAAI人工智能会议*，2021年，页码2109–2117。'
- en: '[106] T. Zhou, J. Li, X. Li, and L. Shao, “Target-aware object discovery and
    association for unsupervised video multi-object segmentation,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 6985–6994.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] T. Zhou, J. Li, X. Li 和 L. Shao，“针对无监督视频多对象分割的目标感知对象发现与关联”，在*Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2021年，页码6985–6994。'
- en: '[107] S. Ren, W. Liu, Y. Liu, H. Chen, G. Han, and S. He, “Reciprocal transformations
    for unsupervised video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 15 455–15 464.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. Ren, W. Liu, Y. Liu, H. Chen, G. Han 和 S. He，“用于无监督视频对象分割的互惠变换”，在*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2021年，页码15,455–15,464。'
- en: '[108] Y. Yang, B. Lai, and S. Soatto, “Dystab: Unsupervised object segmentation
    via dynamic-static bootstrapping,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2021, pp. 2826–2836.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Yang, B. Lai 和 S. Soatto，“Dystab：通过动态-静态自举的无监督对象分割”，在*Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*，2021年，页码2826–2836。'
- en: '[109] C. Yang, H. Lamdouar, E. Lu, A. Zisserman, and W. Xie, “Self-supervised
    video object segmentation by motion grouping,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, pp. 7177–7188.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Yang, H. Lamdouar, E. Lu, A. Zisserman 和 W. Xie，“通过运动分组的自监督视频对象分割”，在*Proc.
    IEEE Int. Conf. Comput. Vis.*，2021年，页码7177–7188。'
- en: '[110] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by long
    term video analysis,” vol. 36, no. 6, pp. 1187–1200, 2013.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] P. Ochs, J. Malik 和 T. Brox，“通过长期视频分析分割移动对象”，第36卷，第6期，页码1187–1200，2013年。'
- en: '[111] H. Lamdouar, C. Yang, W. Xie, and A. Zisserman, “Betrayed by motion:
    Camouflaged object discovery via motion segmentation,” in *Asian Conference on
    Computer Vision*, 2020, pp. 488–503.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] H. Lamdouar, C. Yang, W. Xie 和 A. Zisserman，“被运动背叛：通过运动分割发现伪装对象”，在*亚洲计算机视觉会议*，2020年，页码488–503。'
- en: '[112] L. Jiao, R. Zhang, F. Liu, S. Yang, B. Hou, L. Li, and X. Tang, “New
    generation deep learning for video object detection: A survey,” *IEEE Trans. Neural
    Netw. Learning Sys.*, 2021.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Jiao, R. Zhang, F. Liu, S. Yang, B. Hou, L. Li 和 X. Tang，“新一代深度学习视频对象检测：综述”，*IEEE
    Trans. Neural Netw. Learning Sys.*，2021年。'
- en: '[113] K. Fragkiadaki, P. Arbelaez, P. Felsen, and J. Malik, “Learning to segment
    moving objects in videos,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 4083–4090.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. Fragkiadaki, P. Arbelaez, P. Felsen 和 J. Malik，“学习分割视频中的移动对象”，在*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2015年，页码4083–4090。'
- en: '[114] W. Wang, J. Shen, and L. Shao, “Video salient object detection via fully
    convolutional networks,” *IEEE Trans. Image Process.*, vol. 27, no. 1, pp. 38–49,
    2017.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] W. Wang, J. Shen 和 L. Shao，“通过全卷积网络的视频显著性对象检测”，*IEEE Trans. Image Process.*,
    第27卷，第1期，页码38–49，2017年。'
- en: '[115] A. Fathi, Z. Wojna, V. Rathod, P. Wang, H. O. Song, S. Guadarrama, and
    K. P. Murphy, “Semantic instance segmentation via deep metric learning,” *arXiv
    preprint arXiv:1703.10277*, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Fathi, Z. Wojna, V. Rathod, P. Wang, H. O. Song, S. Guadarrama 和 K.
    P. Murphy，“通过深度度量学习的语义实例分割”，*arXiv预印本 arXiv:1703.10277*，2017年。'
- en: '[116] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end memory
    networks,” in *Proc. Advances Neural Inf. Process. Syst*, 2015, pp. 2440–2448.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. Sukhbaatar, A. Szlam, J. Weston 和 R. Fergus，“端到端记忆网络”，在*Proc. Advances
    Neural Inf. Process. Syst*，2015年，页码2440–2448。'
- en: '[117] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. Sorkine-Hornung,
    “Learning video object segmentation from static images,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, pp. 3491–3500.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, 和 A. Sorkine-Hornung,
    “从静态图像中学习视频物体分割，” 发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    第3491–3500页。'
- en: '[118] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 1155–1162.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Q. Yan, L. Xu, J. Shi, 和 J. Jia, “分层显著性检测，” 发表在 *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2013, 第1155–1162页。'
- en: '[119] W.-D. Jang and C.-S. Kim, “Online video object segmentation via convolutional
    trident network,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 5849–5858.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] W.-D. Jang 和 C.-S. Kim, “通过卷积三叉神经网络进行在线视频物体分割，” 发表在 *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, 第5849–5858页。'
- en: '[120] V. Jampani, R. Gadde, and P. V. Gehler, “Video propagation networks,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 3154–3164.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] V. Jampani, R. Gadde, 和 P. V. Gehler, “视频传播网络，” 发表在 *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, 第3154–3164页。'
- en: '[121] J. S. Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and I. S. Kweon, “Pixel-level
    matching for video object segmentation using convolutional neural networks,” in
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2186–2195.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. S. Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, 和 I. S. Kweon, “使用卷积神经网络的像素级匹配进行视频物体分割，”
    发表在 *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 第2186–2195页。'
- en: '[122] P. Voigtlaender and B. Leibe, “Online adaptation of convolutional neural
    networks for video object segmentation,” in *Proc. British Mach. Vis. Conf.*,
    2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Voigtlaender 和 B. Leibe, “用于视频物体分割的卷积神经网络的在线适应，” 发表在 *Proc. British
    Mach. Vis. Conf.*, 2017。'
- en: '[123] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2014, pp. 740–755.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár, 和 C. L. Zitnick, “微软 COCO: 上下文中的常见物体，” 发表在 *Proc. Eur. Conf. Comput. Vis.*,
    2014, 第740–755页。'
- en: '[124] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele, “Lucid data
    dreaming for video object segmentation,” *Int. J. Comput. Vis.*, vol. 127, no. 9,
    pp. 1175–1197, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] A. Khoreva, R. Benenson, E. Ilg, T. Brox, 和 B. Schiele, “清晰数据梦境用于视频物体分割，”
    *Int. J. Comput. Vis.*, 卷127, 第9期, 第1175–1197页, 2019。'
- en: '[125] L. Bao, B. Wu, and W. Liu, “CNN in MRF: Video object segmentation via
    inference in a CNN-based higher-order spatio-temporal MRF,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2018, pp. 5977–5986.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. Bao, B. Wu, 和 W. Liu, “在MRF中使用CNN: 通过在基于CNN的高阶时空MRF中进行推理实现视频物体分割，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第5977–5986页。'
- en: '[126] J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang, “Fast and
    accurate online video object segmentation via tracking parts,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 7415–7424.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, 和 M.-H. Yang, “通过跟踪部分实现快速且准确的在线视频物体分割，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第7415–7424页。'
- en: '[127] S. Wug Oh, J.-Y. Lee, K. Sunkavalli, and S. Joo Kim, “Fast video object
    segmentation by reference-guided mask propagation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 7376–7385.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Wug Oh, J.-Y. Lee, K. Sunkavalli, 和 S. Joo Kim, “通过参考引导的掩膜传播实现快速视频物体分割，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第7376–7385页。'
- en: '[128] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos, “Efficient
    video object segmentation via network modulation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 6499–6507.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] L. Yang, Y. Wang, X. Xiong, J. Yang, 和 A. K. Katsaggelos, “通过网络调制实现高效的视频物体分割，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第6499–6507页。'
- en: '[129] H. Xiao, J. Feng, G. Lin, Y. Liu, and M. Zhang, “Monet: Deep motion exploitation
    for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 1140–1148.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] H. Xiao, J. Feng, G. Lin, Y. Liu, 和 M. Zhang, “Monet: 深度运动利用用于视频物体分割，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第1140–1148页。'
- en: '[130] P. Hu, G. Wang, X. Kong, J. Kuen, and Y.-P. Tan, “Motion-guided cascaded
    refinement network for video object segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 1400–1409.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] P. Hu, G. Wang, X. Kong, J. Kuen, 和 Y.-P. Tan, “用于视频物体分割的运动引导级联精化网络，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第1400–1409页。'
- en: '[131] J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang, “Reinforcement cutting-agent
    learning for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 9080–9089.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Han, L. Yang, D. Zhang, X. Chang, 和 X. Liang, “用于视频物体分割的强化学习切割代理，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 第9080–9089页。'
- en: '[132] K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixé, D. Cremers,
    and L. Van Gool, “Video object segmentation without temporal information,” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, vol. 41, no. 6, pp. 1515–1530, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixé, D.
    Cremers, 和 L. Van Gool, “无时间信息的视频对象分割，” *IEEE模式分析与机器智能汇刊*，第41卷，第6期，第1515–1530页，2018年。'
- en: '[133] Y.-T. Hu, J.-B. Huang, and A. G. Schwing, “Videomatch: Matching based
    video object segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 56–73.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Y.-T. Hu, J.-B. Huang, 和 A. G. Schwing, “Videomatch：基于匹配的视频对象分割，” *欧洲计算机视觉会议论文集*，2018年，第56–73页。'
- en: '[134] X. Li and C. Change Loy, “Video object segmentation with joint re-identification
    and attention-aware mask propagation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018,
    pp. 93–110.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Li 和 C. Change Loy, “结合重识别和注意力感知掩码传播的视频对象分割，” *欧洲计算机视觉会议论文集*，2018年，第93–110页。'
- en: '[135] H. Ci, C. Wang, and Y. Wang, “Video object segmentation by learning location-sensitive
    embeddings,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 524–539.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] H. Ci, C. Wang, 和 Y. Wang, “通过学习位置敏感嵌入的视频对象分割，” *欧洲计算机视觉会议论文集*，2018年，第524–539页。'
- en: '[136] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in *Proc. Eur. Conf. Comput. Vis.*, 2018,
    pp. 391–408.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, 和 K. Murphy, “通过为视频上色来进行跟踪，”
    *欧洲计算机视觉会议论文集*，2018年，第391–408页。'
- en: '[137] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv preprint arXiv:1705.06950*, 2017.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *等*，“Kinetics人类动作视频数据集，” *arXiv预印本 arXiv:1705.06950*，2017年。'
- en: '[138] H. Xiao, B. Kang, Y. Liu, M. Zhang, and J. Feng, “Online meta adaptation
    for fast video object segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 42, no. 5, pp. 1205–1217, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] H. Xiao, B. Kang, Y. Liu, M. Zhang, 和 J. Feng, “用于快速视频对象分割的在线元适应，” *IEEE模式分析与机器智能汇刊*，第42卷，第5期，第1205–1217页，2019年。'
- en: '[139] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, and L.-C. Chen,
    “Feelvos: Fast end-to-end embedding learning for video object segmentation,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 9481–9490.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, 和 L.-C. Chen,
    “Feelvos：快速端到端嵌入学习的视频对象分割，” *IEEE计算机视觉与模式识别会议论文集*，2019年，第9481–9490页。'
- en: '[140] S. Xu, D. Liu, L. Bao, W. Liu, and P. Zhou, “Mhp-vos: Multiple hypotheses
    propagation for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, pp. 314–323.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Xu, D. Liu, L. Bao, W. Liu, 和 P. Zhou, “Mhp-vos：视频对象分割的多重假设传播，” *IEEE计算机视觉与模式识别会议论文集*，2019年，第314–323页。'
- en: '[141] H. Lin, X. Qi, and J. Jia, “Agss-vos: Attention guided single-shot video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019,
    pp. 3949–3957.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Lin, X. Qi, 和 J. Jia, “Agss-vos：注意力引导的单次视频对象分割，” *IEEE计算机视觉与模式识别会议论文集*，2019年，第3949–3957页。'
- en: '[142] J. Johnander, M. Danelljan, E. Brissman, F. S. Khan, and M. Felsberg,
    “A generative appearance model for end-to-end video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 8953–8962.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Johnander, M. Danelljan, E. Brissman, F. S. Khan, 和 M. Felsberg, “一种用于端到端视频对象分割的生成外观模型，”
    *IEEE计算机视觉与模式识别会议论文集*，2019年，第8953–8962页。'
- en: '[143] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. Torr, “Fast online
    object tracking and segmentation: A unifying approach,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 1328–1338.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, 和 P. H. Torr, “快速在线对象跟踪与分割：一种统一方法，”
    *IEEE计算机视觉与模式识别会议论文集*，2019年，第1328–1338页。'
- en: '[144] B. A. Griffin and J. J. Corso, “Bubblenets: Learning to select the guidance
    frame in video object segmentation by deep sorting frames,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 8914–8923.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] B. A. Griffin 和 J. J. Corso, “Bubblenets：通过深度排序帧学习选择视频对象分割中的指导帧，” *IEEE计算机视觉与模式识别会议论文集*，2019年，第8914–8923页。'
- en: '[145] Z. Wang, J. Xu, L. Liu, F. Zhu, and L. Shao, “Ranet: Ranking attention
    network for fast video object segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019, pp. 3978–3987.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Z. Wang, J. Xu, L. Liu, F. Zhu, 和 L. Shao, “Ranet：用于快速视频对象分割的排名注意力网络，”
    *IEEE国际计算机视觉会议论文集*，2019年，第3978–3987页。'
- en: '[146] G. Li and Y. Yu, “Visual saliency based on multiscale deep features,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 5455–5463.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] G. Li 和 Y. Yu, “基于多尺度深度特征的视觉显著性，” *IEEE计算机视觉与模式识别会议论文集*，2015年，第5455–5463页。'
- en: '[147] X. Zeng, R. Liao, L. Gu, Y. Xiong, S. Fidler, and R. Urtasun, “Dmm-net:
    Differentiable mask-matching network for video object segmentation,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2019, pp. 3929–3938.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Zeng, R. Liao, L. Gu, Y. Xiong, S. Fidler, 和 R. Urtasun，“Dmm-net:
    可微分的掩模匹配网络用于视频目标分割，” 见 *IEEE国际计算机视觉会议论文集*，2019年，页码3929–3938。'
- en: '[148] L. Zhang, Z. Lin, J. Zhang, H. Lu, and Y. He, “Fast video object segmentation
    via dynamic targeting network,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    pp. 5582–5591.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] L. Zhang, Z. Lin, J. Zhang, H. Lu, 和 Y. He，“通过动态目标网络进行快速视频目标分割，” 见 *IEEE国际计算机视觉会议论文集*，2019年，页码5582–5591。'
- en: '[149] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, “Video object segmentation
    using space-time memory networks,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    pp. 9226–9235.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. W. Oh, J.-Y. Lee, N. Xu, 和 S. J. Kim，“使用时空记忆网络的视频目标分割，” 见 *IEEE国际计算机视觉会议论文集*，2019年，页码9226–9235。'
- en: '[150] X. Wang, A. Jabri, and A. A. Efros, “Learning correspondence from the
    cycle-consistency of time,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, pp. 2566–2576.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] X. Wang, A. Jabri, 和 A. A. Efros，“从时间的一致性学习对应关系，” 见 *IEEE计算机视觉与模式识别会议论文集*，2019年，页码2566–2576。'
- en: '[151] D. F. Fouhey, W.-c. Kuo, A. A. Efros, and J. Malik, “From lifestyle vlogs
    to everyday interactions,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 4991–5000.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] D. F. Fouhey, W.-c. Kuo, A. A. Efros, 和 J. Malik，“从生活方式视频到日常互动，” 见 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码4991–5000。'
- en: '[152] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” in *Proc. Advances Neural
    Inf. Process. Syst*, 2019, pp. 318–328.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, 和 M.-H. Yang，“用于时间对应的联合任务自监督学习，”
    见 *神经信息处理系统进展*，2019年，页码318–328。'
- en: '[153] T. Meinhardt and L. Leal-Taixé, “Make one-shot video object segmentation
    efficient again,” in *Proc. Advances Neural Inf. Process. Syst*, 2020, pp. 10 607–10 619.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] T. Meinhardt 和 L. Leal-Taixé，“让一次性视频目标分割再次高效，” 见 *神经信息处理系统进展*，2020年，页码10 607–10 619。'
- en: '[154] Y. Liang, X. Li, N. Jafari, and Q. Chen, “Video object segmentation with
    adaptive feature bank and uncertain-region refinement,” in *Proc. Advances Neural
    Inf. Process. Syst*, 2020, pp. 3430–3441.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Y. Liang, X. Li, N. Jafari, 和 Q. Chen，“具有自适应特征库和不确定区域细化的视频目标分割，” 见 *神经信息处理系统进展*，2020年，页码3430–3441。'
- en: '[155] X. Huang, J. Xu, Y.-W. Tai, and C.-K. Tang, “Fast video object segmentation
    with temporal aggregation network and dynamic template matching,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 8879–8889.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] X. Huang, J. Xu, Y.-W. Tai, 和 C.-K. Tang，“具有时间聚合网络和动态模板匹配的快速视频目标分割，”
    见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码8879–8889。'
- en: '[156] M. Sun, J. Xiao, E. G. Lim, B. Zhang, and Y. Zhao, “Fast template matching
    and update for video object tracking and segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 10 791–10 799.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] M. Sun, J. Xiao, E. G. Lim, B. Zhang, 和 Y. Zhao，“用于视频目标跟踪和分割的快速模板匹配与更新，”
    见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码10 791–10 799。'
- en: '[157] X. Chen, Z. Li, Y. Yuan, G. Yu, J. Shen, and D. Qi, “State-aware tracker
    for real-time video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2020, pp. 9384–9393.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] X. Chen, Z. Li, Y. Yuan, G. Yu, J. Shen, 和 D. Qi，“用于实时视频目标分割的状态感知跟踪器，”
    见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码9384–9393。'
- en: '[158] A. Robinson, F. J. Lawin, M. Danelljan, F. S. Khan, and M. Felsberg,
    “Learning fast and robust target models for video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 7406–7415.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. Robinson, F. J. Lawin, M. Danelljan, F. S. Khan, 和 M. Felsberg，“为视频目标分割学习快速而稳健的目标模型，”
    见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码7406–7415。'
- en: '[159] Y. Zhang, Z. Wu, H. Peng, and S. Lin, “A transductive approach for video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020,
    pp. 6949–6958.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Y. Zhang, Z. Wu, H. Peng, 和 S. Lin，“一种视频目标分割的传导方法，” 见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码6949–6958。'
- en: '[160] Z. Lai, E. Lu, and W. Xie, “Mast: A memory-augmented self-supervised
    tracker,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 6479–6488.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Lai, E. Lu, 和 W. Xie，“Mast: 一种内存增强自监督跟踪器，” 见 *IEEE计算机视觉与模式识别会议论文集*，2020年，页码6479–6488。'
- en: '[161] Y. Li, Z. Shen, and Y. Shan, “Fast video object segmentation using the
    global context module,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 735–750.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Y. Li, Z. Shen, 和 Y. Shan，“使用全局上下文模块的快速视频目标分割，” 见 *欧洲计算机视觉会议论文集*，2020年，页码735–750。'
- en: '[162] H. Seong, J. Hyun, and E. Kim, “Kernelized memory network for video object
    segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 629–645.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] H. Seong, J. Hyun, 和 E. Kim，“用于视频目标分割的核化记忆网络，” 见 *欧洲计算机视觉会议论文集*，2020年，页码629–645。'
- en: '[163] Z. Yang, Y. Wei, and Y. Yang, “Collaborative video object segmentation
    by foreground-background integration,” in *Proc. Eur. Conf. Comput. Vis.*, 2020,
    pp. 332–348.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Z. Yang, Y. Wei, 和 Y. Yang，“通过前景-背景集成进行协作视频目标分割，” *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 332–348。'
- en: '[164] G. Bhat, F. J. Lawin, M. Danelljan, A. Robinson, M. Felsberg, L. Van Gool,
    and R. Timofte, “Learning what to learn for video object segmentation,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2020, pp. 777–794.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] G. Bhat, F. J. Lawin, M. Danelljan, A. Robinson, M. Felsberg, L. Van
    Gool, 和 R. Timofte，“学习视频目标分割的学习内容，” *Proc. Eur. Conf. Comput. Vis.*, 2020, pp.
    777–794。'
- en: '[165] R. Wu, H. Lin, X. Qi, and J. Jia, “Memory selection network for video
    propagation,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 175–190.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] R. Wu, H. Lin, X. Qi, 和 J. Jia，“视频传播的记忆选择网络，” *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 175–190。'
- en: '[166] Y. Li, N. Xu, J. Peng, J. See, and W. Lin, “Delving into the cyclic mechanism
    in semi-supervised video object segmentation,” in *Proc. Advances Neural Inf.
    Process. Syst*, 2020, pp. 1218–1228.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Li, N. Xu, J. Peng, J. See, 和 W. Lin，“深入探讨半监督视频目标分割中的循环机制，” *Proc.
    Advances Neural Inf. Process. Syst*, 2020, pp. 1218–1228。'
- en: '[167] F. Lin, H. Xie, Y. Li, and Y. Zhang, “Query-memory re-aggregation for
    weakly-supervised video object segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2021, pp. 2038–2046.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] F. Lin, H. Xie, Y. Li, 和 Y. Zhang，“弱监督视频目标分割的查询-记忆再聚合，” *AAAI Conference
    on Artificial Intelligence*, 2021, pp. 2038–2046。'
- en: '[168] H. Wang, X. Jiang, H. Ren, Y. Hu, and S. Bai, “Swiftnet: Real-time video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021,
    pp. 1296–1305.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. Wang, X. Jiang, H. Ren, Y. Hu, 和 S. Bai，“Swiftnet: 实时视频目标分割，” *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 1296–1305。'
- en: '[169] H. Park, J. Yoo, S. Jeong, G. Venkatesh, and N. Kwak, “Learning dynamic
    network using a reuse gate function in semi-supervised video object segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 8405–8414.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] H. Park, J. Yoo, S. Jeong, G. Venkatesh, 和 N. Kwak，“在半监督视频目标分割中使用重用门函数学习动态网络，”
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 8405–8414。'
- en: '[170] B. Duke, A. Ahmed, C. Wolf, P. Aarabi, and G. W. Taylor, “Sstvos: Sparse
    spatiotemporal transformers for video object segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2021, pp. 5912–5921.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] B. Duke, A. Ahmed, C. Wolf, P. Aarabi, 和 G. W. Taylor，“Sstvos: 稀疏时空变换器用于视频目标分割，”
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 5912–5921。'
- en: '[171] W. Ge, X. Lu, and J. Shen, “Video object segmentation using global and
    instance embedding learning,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2021, pp. 16 836–16 845.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] W. Ge, X. Lu, 和 J. Shen，“利用全局和实例嵌入学习进行视频目标分割，” *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2021, pp. 16 836–16 845。'
- en: '[172] L. Hu, P. Zhang, B. Zhang, P. Pan, Y. Xu, and R. Jin, “Learning position
    and target consistency for memory-based video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 4144–4154.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] L. Hu, P. Zhang, B. Zhang, P. Pan, Y. Xu, 和 R. Jin，“基于记忆的视频目标分割中的位置和目标一致性学习，”
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 4144–4154。'
- en: '[173] H. Xie, H. Yao, S. Zhou, S. Zhang, and W. Sun, “Efficient regional memory
    network for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 1286–1295.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] H. Xie, H. Yao, S. Zhou, S. Zhang, 和 W. Sun，“高效的区域记忆网络用于视频目标分割，” *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 1286–1295。'
- en: '[174] A. Jabri, A. Owens, and A. Efros, “Space-time correspondence as a contrastive
    random walk,” in *Proc. Advances Neural Inf. Process. Syst*, 2020, pp. 19 545–19 560.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] A. Jabri, A. Owens, 和 A. Efros，“时空对应作为对比随机游走，” *Proc. Advances Neural
    Inf. Process. Syst*, 2020, pp. 19 545–19 560。'
- en: '[175] X. Lu, W. Wang, J. Shen, D. Crandall, and J. Luo, “Zero-shot video object
    segmentation with co-attention siamese networks,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 44, no. 04, pp. 2228–2242, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] X. Lu, W. Wang, J. Shen, D. Crandall, 和 J. Luo，“使用共同关注的孪生网络进行零样本视频目标分割，”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 44, no. 04, pp. 2228–2242, 2020。'
- en: '[176] X. Lu, W. Wang, J. Shen, D. Crandall, and L. Van Gool, “Segmenting objects
    from relational visual data,” *IEEE Trans. Pattern Anal. Mach. Intell.*, 2021.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] X. Lu, W. Wang, J. Shen, D. Crandall, 和 L. Van Gool，“从关系视觉数据中分割对象，” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, 2021。'
- en: '[177] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and
    L. Van Gool, “The 2019 davis challenge on vos: Unsupervised multi-object segmentation,”
    *arXiv:1905.00737*, 2019.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, 和 L.
    Van Gool，“2019 DAVIS挑战赛：无监督多对象分割，” *arXiv:1905.00737*, 2019。'
- en: '[178] J. Luiten, I. E. Zulfikar, and B. Leibe, “Unovost: Unsupervised offline
    video object segmentation and tracking,” in *Proc. IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 2000–2009.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] J. Luiten, I. E. Zulfikar, 和 B. Leibe, “Unovost：无监督离线视频目标分割与跟踪，” 见于 *IEEE冬季计算机视觉应用会议论文集*，2020年，页2000–2009。'
- en: '[179] W. Wang, J. Shen, X. Lu, S. C. Hoi, and H. Ling, “Paying attention to
    video object pattern understanding,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 43, no. 7, pp. 2413–2428, 2020.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] W. Wang, J. Shen, X. Lu, S. C. Hoi, 和 H. Ling, “关注视频目标模式理解，” *IEEE模式分析与机器智能汇刊*，第43卷，第7期，页2413–2428，2020年。'
- en: '[180] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2961–2969.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick, “Mask R-CNN，” 见于 *IEEE国际计算机视觉会议论文集*，2017年，页2961–2969。'
- en: '[181] A. Benard and M. Gygli, “Interactive video object segmentation in the
    wild,” *arXiv preprint arXiv:1801.00269*, 2017.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] A. Benard 和 M. Gygli, “野外的交互式视频目标分割，” *arXiv预印本 arXiv:1801.00269*，2017年。'
- en: '[182] Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool, “Blazingly fast video
    object segmentation with pixel-wise metric learning,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 1189–1198.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Y. Chen, J. Pont-Tuset, A. Montes, 和 L. Van Gool, “通过像素级度量学习进行极快的视频目标分割，”
    见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页1189–1198。'
- en: '[183] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, “Fast user-guided video object
    segmentation by interaction-and-propagation networks,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 5247–5256.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. W. Oh, J.-Y. Lee, N. Xu, 和 S. J. Kim, “通过交互和传播网络进行快速用户引导视频目标分割，” 见于
    *IEEE计算机视觉与模式识别会议论文集*，2019年，页5247–5256。'
- en: '[184] J. Miao, Y. Wei, and Y. Yang, “Memory aggregation networks for efficient
    interactive video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2020, pp. 10 366–10 375.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Miao, Y. Wei, 和 Y. Yang, “用于高效交互式视频目标分割的记忆聚合网络，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2020年，页10 366–10 375。'
- en: '[185] Y. Heo, Y. J. Koh, and C.-S. Kim, “Interactive video object segmentation
    using global and local transfer modules,” in *Proc. Eur. Conf. Comput. Vis.*,
    2020, pp. 297–313.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Y. Heo, Y. J. Koh, 和 C.-S. Kim, “使用全局和局部转移模块的交互式视频目标分割，” 见于 *欧洲计算机视觉会议论文集*，2020年，页297–313。'
- en: '[186] B. Chen, H. Ling, X. Zeng, G. Jun, Z. Xu, and S. Fidler, “Scribblebox:
    Interactive annotation framework for video object segmentation,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2020, pp. 293–310.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] B. Chen, H. Ling, X. Zeng, G. Jun, Z. Xu, 和 S. Fidler, “Scribblebox：用于视频目标分割的交互式标注框架，”
    见于 *欧洲计算机视觉会议论文集*，2020年，页293–310。'
- en: '[187] Z. Yin, J. Zheng, W. Luo, S. Qian, H. Zhang, and S. Gao, “Learning to
    recommend frame for interactive video object segmentation in the wild,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 15 445–15 454.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Yin, J. Zheng, W. Luo, S. Qian, H. Zhang, 和 S. Gao, “学习推荐框架以用于野外的交互式视频目标分割，”
    见于 *IEEE计算机视觉与模式识别会议论文集*，2021年，页15 445–15 454。'
- en: '[188] Y. Heo, Y. J. Koh, and C.-S. Kim, “Guided interactive video object segmentation
    using reliability-based attention maps,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 7322–7330.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Y. Heo, Y. J. Koh, 和 C.-S. Kim, “基于可靠性注意力图的引导交互式视频目标分割，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2021年，页7322–7330。'
- en: '[189] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, “Modular interactive video object
    segmentation: Interaction-to-mask, propagation and difference-aware fusion,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 5559–5568.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] H. K. Cheng, Y.-W. Tai, 和 C.-K. Tang, “模块化交互式视频目标分割：从交互到掩模、传播和差异感知融合，”
    见于 *IEEE计算机视觉与模式识别会议论文集*，2021年，页5559–5568。'
- en: '[190] L. Wang, W. Ouyang, X. Wang, and H. Lu, “Visual tracking with fully convolutional
    networks,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3119–3127.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] L. Wang, W. Ouyang, X. Wang, 和 H. Lu, “使用全卷积网络的视觉跟踪，” 见于 *IEEE国际计算机视觉会议论文集*，2015年，页3119–3127。'
- en: '[191] K. Duarte, Y. S. Rawat, and M. Shah, “Capsulevos: Semi-supervised video
    object segmentation using capsule routing,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019, pp. 8480–8489.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] K. Duarte, Y. S. Rawat, 和 M. Shah, “Capsulevos：使用胶囊路由的半监督视频目标分割，” 见于
    *IEEE国际计算机视觉会议论文集*，2019年，页8480–8489。'
- en: '[192] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Proc. Advances
    Neural Inf. Process. Syst*, 2017.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin, “注意力机制是你所需要的一切，” 见于 *神经信息处理系统进展会议论文集*，2017年。'
- en: '[193] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2022, pp. 8719–8730.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, 和 Y. Yang，“自监督对应学习的局部感知视频间和视频内重建”，发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2022年，第8719–8730页。'
- en: '[194] S. Caelles, A. Montes, K.-K. Maninis, Y. Chen, L. Van Gool, F. Perazzi,
    and J. Pont-Tuset, “The 2018 davis challenge on video object segmentation,” *arXiv
    preprint arXiv:1803.00557*, 2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. Caelles, A. Montes, K.-K. Maninis, Y. Chen, L. Van Gool, F. Perazzi,
    和 J. Pont-Tuset，“2018年 Davis 挑战视频对象分割”，*arXiv preprint arXiv:1803.00557*，2018年。'
- en: '[195] N. Xu, B. Price, S. Cohen, J. Yang, and T. S. Huang, “Deep interactive
    object selection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 373–381.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] N. Xu, B. Price, S. Cohen, J. Yang, 和 T. S. Huang，“深度交互式对象选择”，发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2016年，第373–381页。'
- en: '[196] K. Gavrilyuk, A. Ghodrati, Z. Li, and C. G. Snoek, “Actor and action
    video segmentation from a sentence,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 5958–5966.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] K. Gavrilyuk, A. Ghodrati, Z. Li, 和 C. G. Snoek，“从句子中进行演员和动作视频分割”，发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2018年，第5958–5966页。'
- en: '[197] A. Khoreva, A. Rohrbach, and B. Schiele, “Video object segmentation with
    language referring expressions,” in *Asian Conference on Computer Vision*, 2018,
    pp. 123–141.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. Khoreva, A. Rohrbach, 和 B. Schiele，“带有语言指代表达的视频对象分割”，发表于*Asian Conference
    on Computer Vision*，2018年，第123–141页。'
- en: '[198] H. Wang, C. Deng, J. Yan, and D. Tao, “Asymmetric cross-guided attention
    network for actor and action video segmentation from natural language query,”
    in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp. 3939–3948.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] H. Wang, C. Deng, J. Yan, 和 D. Tao，“用于从自然语言查询中进行演员和动作视频分割的非对称跨引导注意力网络”，发表于*Proc.
    IEEE Int. Conf. Comput. Vis.*，2019年，第3939–3948页。'
- en: '[199] H. Wang, C. Deng, F. Ma, and Y. Yang, “Context modulated dynamic networks
    for actor and action video segmentation with language queries,” in *AAAI Conference
    on Artificial Intelligence*, 2020, pp. 12 152–12 159.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] H. Wang, C. Deng, F. Ma, 和 Y. Yang，“用于演员和动作视频分割的上下文调制动态网络”，发表于*AAAI Conference
    on Artificial Intelligence*，2020年，第12 152–12 159页。'
- en: '[200] K. Ning, L. Xie, F. Wu, and Q. Tian, “Polar relative positional encoding
    for video-language segmentation,” in *International Joint Conferences on Artificial
    Intelligence*, 2020, pp. 948–954.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] K. Ning, L. Xie, F. Wu, 和 Q. Tian，“用于视频语言分割的极坐标相对位置编码”，发表于*International
    Joint Conferences on Artificial Intelligence*，2020年，第948–954页。'
- en: '[201] B. McIntosh, K. Duarte, Y. S. Rawat, and M. Shah, “Visual-textual capsule
    routing for text-based video segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2020, pp. 9942–9951.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] B. McIntosh, K. Duarte, Y. S. Rawat, 和 M. Shah，“用于基于文本的视频分割的视觉-文本胶囊路由”，发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2020年，第9942–9951页。'
- en: '[202] S. Seo, J.-Y. Lee, and B. Han, “Urvos: Unified referring video object
    segmentation network with a large-scale benchmark,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 208–223.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] S. Seo, J.-Y. Lee, 和 B. Han，“Urvos: 统一的指代视频对象分割网络与大规模基准”，发表于*Proc. Eur.
    Conf. Comput. Vis.*，2020年，第208–223页。'
- en: '[203] T. Hui, S. Huang, S. Liu, Z. Ding, G. Li, W. Wang, J. Han, and F. Wang,
    “Collaborative spatial-temporal modeling for language-queried video actor segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 4187–4196.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] T. Hui, S. Huang, S. Liu, Z. Ding, G. Li, W. Wang, J. Han, 和 F. Wang，“用于语言查询的视频演员分割的协作时空建模”，发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2021年，第4187–4196页。'
- en: '[204] L. Ye, M. Rochan, Z. Liu, X. Zhang, and Y. Wang, “Referring segmentation
    in images and videos with cross-modal self-attention network,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, 2021.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] L. Ye, M. Rochan, Z. Liu, X. Zhang, 和 Y. Wang，“使用跨模态自注意力网络的图像和视频中的指代分割”，*IEEE
    Trans. Pattern Anal. Mach. Intell.*，2021年。'
- en: '[205] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell, “Clockwork convnets
    for video semantic segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp.
    852–868.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] E. Shelhamer, K. Rakelly, J. Hoffman, 和 T. Darrell，“用于视频语义分割的 Clockwork
    convnets”，发表于*Proc. Eur. Conf. Comput. Vis.*，2016年，第852–868页。'
- en: '[206] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 3213–3223.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, 和 B. Schiele，“用于语义城市场景理解的 cityscapes 数据集”，发表于*Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*，2016年，第3213–3223页。'
- en: '[207] A. Kundu, V. Vineet, and V. Koltun, “Feature space optimization for semantic
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 3168–3175.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] A. Kundu, V. Vineet, 和 V. Koltun，“语义视频分割的特征空间优化，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016年，第3168–3175页。'
- en: '[208] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes
    in video: A high-definition ground truth database,” *Pattern Recognition Letters*,
    vol. 30, no. 2, pp. 88–97, 2009.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] G. J. Brostow, J. Fauqueur, 和 R. Cipolla，“视频中的语义对象类别：高清地面真值数据库，”*模式识别快报*，第30卷，第2期，第88–97页，2009年。'
- en: '[209] J. Hur and S. Roth, “Joint optical flow and temporally consistent semantic
    segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 163–177.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. Hur 和 S. Roth，“联合光流与时间一致的语义分割，”发表于*欧洲计算机视觉会议论文集*，2016年，第163–177页。'
- en: '[210] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    and B. Leibe, “Mots: Multi-object tracking and segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 7942–7951.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    和 B. Leibe，“MOTS：多目标跟踪与分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第7942–7951页。'
- en: '[211] B. Mahasseni, S. Todorovic, and A. Fern, “Budget-aware deep semantic
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 1029–1038.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] B. Mahasseni, S. Todorovic, 和 A. Fern，“预算感知深度语义视频分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第1029–1038页。'
- en: '[212] X. Jin, X. Li, H. Xiao, X. Shen, Z. Lin, J. Yang, Y. Chen, J. Dong, L. Liu,
    Z. Jie *et al.*, “Video scene parsing with predictive feature learning,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2017, pp. 5580–5588.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] X. Jin, X. Li, H. Xiao, X. Shen, Z. Lin, J. Yang, Y. Chen, J. Dong, L.
    Liu, Z. Jie *等*，“通过预测特征学习的视频场景解析，”发表于*IEEE国际计算机视觉会议论文集*，2017年，第5580–5588页。'
- en: '[213] R. Gadde, V. Jampani, and P. V. Gehler, “Semantic video cnns through
    representation warping,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 4453–4462.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] R. Gadde, V. Jampani, 和 P. V. Gehler，“通过表示变形的语义视频CNNs，”发表于*IEEE国际计算机视觉会议论文集*，2017年，第4453–4462页。'
- en: '[214] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature flow for
    video recognition,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 2349–2358.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] X. Zhu, Y. Xiong, J. Dai, L. Yuan, 和 Y. Wei，“用于视频识别的深度特征流，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第2349–2358页。'
- en: '[215] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, and J. M.
    Alvarez, “Bringing background into the foreground: Making all classes equal in
    weakly-supervised video semantic segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2017, pp. 2125–2135.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, 和 J. M. Alvarez，“将背景带入前景：在弱监督视频语义分割中使所有类别平等，”发表于*IEEE国际计算机视觉会议论文集*，2017年，第2125–2135页。'
- en: '[216] D. Nilsson and C. Sminchisescu, “Semantic video segmentation by gated
    recurrent flow propagation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 6819–6828.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] D. Nilsson 和 C. Sminchisescu，“通过门控递归流传播的语义视频分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，第6819–6828页。'
- en: '[217] Y. Li, J. Shi, and D. Lin, “Low-latency video semantic segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 5997–6005.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Y. Li, J. Shi, 和 D. Lin，“低延迟视频语义分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，第5997–6005页。'
- en: '[218] Y.-S. Xu, T.-J. Fu, H.-K. Yang, and C.-Y. Lee, “Dynamic video segmentation
    network,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 6556–6565.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y.-S. Xu, T.-J. Fu, H.-K. Yang, 和 C.-Y. Lee，“动态视频分割网络，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，第6556–6565页。'
- en: '[219] P.-Y. Huang, W.-T. Hsu, C.-Y. Chiu, T.-F. Wu, and M. Sun, “Efficient
    uncertainty estimation for semantic segmentation in videos,” in *Proc. Eur. Conf.
    Comput. Vis.*, 2018, pp. 520–535.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] P.-Y. Huang, W.-T. Hsu, C.-Y. Chiu, T.-F. Wu, 和 M. Sun，“视频语义分割的高效不确定性估计，”发表于*欧洲计算机视觉会议论文集*，2018年，第520–535页。'
- en: '[220] S. Chandra, C. Couprie, and I. Kokkinos, “Deep spatio-temporal random
    fields for efficient video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 8915–8924.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] S. Chandra, C. Couprie, 和 I. Kokkinos，“用于高效视频分割的深度时空随机场，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，第8915–8924页。'
- en: '[221] S. Jain, X. Wang, and J. E. Gonzalez, “Accel: A corrective fusion network
    for efficient semantic segmentation on video,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, pp. 8866–8875.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] S. Jain, X. Wang, 和 J. E. Gonzalez，“Accel：一种用于高效视频语义分割的校正融合网络，”发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第8866–8875页。'
- en: '[222] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, and B. Catanzaro,
    “Improving semantic segmentation via video propagation and label relaxation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 8856–8865.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, 和 B. Catanzaro，“通过视频传播和标签松弛改善语义分割”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2019年，第8856–8865页。'
- en: '[223] M. Ding, Z. Wang, B. Zhou, J. Shi, Z. Lu, and P. Luo, “Every frame counts:
    joint learning of video segmentation and optical flow,” in *AAAI Conference on
    Artificial Intelligence*, 2020, pp. 10 713–10 720.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] M. Ding, Z. Wang, B. Zhou, J. Shi, Z. Lu, 和 P. Luo，“每帧都重要：视频分割与光流的联合学习”，发表于
    *人工智能学会会议*，2020年，第10 713–10 720页。'
- en: '[224] P. Hu, F. Caba, O. Wang, Z. Lin, S. Sclaroff, and F. Perazzi, “Temporally
    distributed networks for fast video semantic segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2020, pp. 8818–8827.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] P. Hu, F. Caba, O. Wang, Z. Lin, S. Sclaroff, 和 F. Perazzi，“用于快速视频语义分割的时间分布网络”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2020年，第8818–8827页。'
- en: '[225] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” in *Proc. Eur. Conf. Comput. Vis.*, 2012,
    pp. 746–760.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus，“基于rgbd图像的室内分割和支撑推断”，发表于
    *欧洲计算机视觉会议论文集*，2012年，第746–760页。'
- en: '[226] G. Bertasius and L. Torresani, “Classifying, segmenting, and tracking
    object instances in video with mask propagation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 9739–9748.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] G. Bertasius 和 L. Torresani，“通过掩模传播对视频中的物体实例进行分类、分割和跟踪”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2020年，第9739–9748页。'
- en: '[227] D. Kim, S. Woo, J.-Y. Lee, and I. S. Kweon, “Video panoptic segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 9859–9868.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] D. Kim, S. Woo, J.-Y. Lee, 和 I. S. Kweon，“视频全景分割”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2020年，第9859–9868页。'
- en: '[228] L. Porzi, M. Hofinger, I. Ruiz, J. Serrat, S. R. Bulo, and P. Kontschieder,
    “Learning multi-object tracking and segmentation from automatic annotations,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 6846–6855.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] L. Porzi, M. Hofinger, I. Ruiz, J. Serrat, S. R. Bulo, 和 P. Kontschieder，“从自动标注中学习多目标跟踪和分割”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2020年，第6846–6855页。'
- en: '[229] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell,
    “Bdd100k: A diverse driving dataset for heterogeneous multitask learning,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 2636–2645.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, 和 T.
    Darrell，“Bdd100k：用于异构多任务学习的多样化驾驶数据集”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2020年，第2636–2645页。'
- en: '[230] C.-C. Lin, Y. Hung, R. Feris, and L. He, “Video instance segmentation
    tracking with a modified vae architecture,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2020, pp. 13 147–13 157.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] C.-C. Lin, Y. Hung, R. Feris, 和 L. He，“具有修改的vae架构的视频实例分割跟踪”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2020年，第13 147–13 157页。'
- en: '[231] Y. Liu, C. Shen, C. Yu, and J. Wang, “Efficient semantic video segmentation
    with per-frame inference,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 352–368.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Y. Liu, C. Shen, C. Yu, 和 J. Wang，“通过逐帧推断实现高效的语义视频分割”，发表于 *欧洲计算机视觉会议论文集*，2020年，第352–368页。'
- en: '[232] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y. Pang, and L. Shao,
    “Sipmask: Spatial information preservation for fast image and video instance segmentation,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 1–18.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y. Pang, 和 L. Shao，“Sipmask：用于快速图像和视频实例分割的空间信息保留”，发表于
    *欧洲计算机视觉会议论文集*，2020年，第1–18页。'
- en: '[233] A. Athar, S. Mahadevan, A. Os̆ep, L. Leal-Taixé, and B. Leibe, “Stem-seg:
    Spatio-temporal embeddings for instance segmentation in videos,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2020, pp. 158–177.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] A. Athar, S. Mahadevan, A. Os̆ep, L. Leal-Taixé, 和 B. Leibe，“Stem-seg：视频中实例分割的时空嵌入”，发表于
    *欧洲计算机视觉会议论文集*，2020年，第158–177页。'
- en: '[234] L.-C. Chen, R. G. Lopes, B. Cheng, M. D. Collins, E. D. Cubuk, B. Zoph,
    H. Adam, and J. Shlens, “Naive-student: Leveraging semi-supervised learning in
    video sequences for urban scene segmentation,” in *Proc. Eur. Conf. Comput. Vis.*,
    2020, pp. 695–714.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] L.-C. Chen, R. G. Lopes, B. Cheng, M. D. Collins, E. D. Cubuk, B. Zoph,
    H. Adam, 和 J. Shlens，“Naive-student：利用半监督学习进行城市场景分割”，发表于 *欧洲计算机视觉会议论文集*，2020年，第695–714页。'
- en: '[235] Y. Fu, L. Yang, D. Liu, T. S. Huang, and H. Shi, “Compfeat: Comprehensive
    feature aggregation for video instance segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2021, pp. 1361–1369.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Y. Fu, L. Yang, D. Liu, T. S. Huang, 和 H. Shi，“Compfeat：用于视频实例分割的综合特征聚合”，发表于
    *人工智能学会会议*，2021年，第1361–1369页。'
- en: '[236] J. Wu, J. Cao, L. Song, Y. Wang, M. Yang, and J. Yuan, “Track to detect
    and segment: An online multi-object tracker,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 12 352–12 361.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] J. Wu, J. Cao, L. Song, Y. Wang, M. Yang, 和 J. Yuan，“跟踪以检测和分割：一种在线多目标跟踪器，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 12 352–12 361。'
- en: '[237] D. Liu, Y. Cui, W. Tan, and Y. Chen, “Sg-net: Spatial granularity network
    for one-stage video instance segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 9816–9825.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] D. Liu, Y. Cui, W. Tan, 和 Y. Chen，“Sg-net: 一阶段视频实例分割的空间粒度网络，” *IEEE计算机视觉与模式识别会议论文集*，2021年，页码
    9816–9825。'
- en: '[238] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia, “End-to-end
    video instance segmentation with transformers,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 8741–8750.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, 和 H. Xia，“基于变压器的端到端视频实例分割，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 8741–8750。'
- en: '[239] L. Hoyer, D. Dai, Y. Chen, A. Koring, S. Saha, and L. Van Gool, “Three
    ways to improve semantic segmentation with self-supervised depth estimation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 11 130–11 140.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] L. Hoyer, D. Dai, Y. Chen, A. Koring, S. Saha, 和 L. Van Gool，“通过自监督深度估计改善语义分割的三种方法，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 11 130–11 140。'
- en: '[240] S. Woo, D. Kim, J.-Y. Lee, and I. S. Kweon, “Learning to associate every
    segment for video panoptic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 2705–2714.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] S. Woo, D. Kim, J.-Y. Lee, 和 I. S. Kweon，“学习关联每个分割用于视频全景分割，” *IEEE计算机视觉与模式识别会议论文集*，2021年，页码
    2705–2714。'
- en: '[241] S. Qiao, Y. Zhu, H. Adam, A. Yuille, and L.-C. Chen, “Vip-deeplab: Learning
    visual perception with depth-aware video panoptic segmentation,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 3997–4008.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] S. Qiao, Y. Zhu, H. Adam, A. Yuille, 和 L.-C. Chen，“Vip-deeplab: 通过深度感知的视频全景分割学习视觉感知，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 3997–4008。'
- en: '[242] Q. Liu, V. Ramanathan, D. Mahajan, A. Yuille, and Z. Yang, “Weakly supervised
    instance segmentation for videos with temporal mask consistency,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 13 968–13 978.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Q. Liu, V. Ramanathan, D. Mahajan, A. Yuille, 和 Z. Yang，“具有时间掩膜一致性的弱监督实例分割，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 13 968–13 978。'
- en: '[243] Y. Fu, S. Liu, U. Iqbal, S. De Mello, H. Shi, and J. Kautz, “Learning
    to track instances without video annotations,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 8680–8689.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Y. Fu, S. Liu, U. Iqbal, S. De Mello, H. Shi, 和 J. Kautz，“学习在没有视频注释的情况下跟踪实例，”
    *IEEE计算机视觉与模式识别会议论文集*，2021年，页码 8680–8689。'
- en: '[244] H. Lin, R. Wu, S. Liu, J. Lu, and J. Jia, “Video instance segmentation
    with a propose-reduce paradigm,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2021,
    pp. 1739–1748.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] H. Lin, R. Wu, S. Liu, J. Lu, 和 J. Jia，“具有提出-减少范式的视频实例分割，” *IEEE国际计算机视觉会议论文集*，2021年，页码
    1739–1748。'
- en: '[245] S. Yang, Y. Fang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, and W. Liu,
    “Crossover learning for fast online video instance segmentation,” in *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2021, pp. 8043–8052.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] S. Yang, Y. Fang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, 和 W. Liu，“快速在线视频实例分割的交叉学习，”
    *IEEE国际计算机视觉会议论文集*，2021年，页码 8043–8052。'
- en: '[246] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille,
    P. Torr, and S. Bai, “Occluded video instance segmentation,” *arXiv preprint arXiv:2102.01558*,
    2021.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille,
    P. Torr, 和 S. Bai， “遮挡视频实例分割，” *arXiv 预印本 arXiv:2102.01558*，2021年。'
- en: '[247] J. Gao, C. Sun, Z. Yang, and R. Nevatia, “Tall: Temporal activity localization
    via language query,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 5267–5275.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] J. Gao, C. Sun, Z. Yang, 和 R. Nevatia，“Tall: 通过语言查询进行时间活动定位，” *IEEE国际计算机视觉会议论文集*，2017年，页码
    5267–5275。'
- en: '[248] Z. Li, R. Tao, E. Gavves, C. G. Snoek, and A. W. Smeulders, “Tracking
    by natural language specification,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2017, pp. 6495–6503.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] Z. Li, R. Tao, E. Gavves, C. G. Snoek, 和 A. W. Smeulders，“通过自然语言规范进行跟踪，”
    *IEEE计算机视觉与模式识别会议论文集*，2017年，页码 6495–6503。'
- en: '[249] G. E. Hinton, S. Sabour, and N. Frosst, “Matrix capsules with em routing,”
    in *Proc. Int. Conf. Learn. Representations*, 2018.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] G. E. Hinton, S. Sabour, 和 N. Frosst，“矩阵胶囊与EM路由，” *国际学习表征会议论文集*，2018年。'
- en: '[250] S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, “Cross-modal progressive
    comprehension for referring segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2021.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, 和 G. Li，“用于参考分割的跨模态渐进理解，” *IEEE模式分析与机器智能学报*，2021年。'
- en: '[251] C. Liang, W. Wang, T. Zhou, J. Miao, Y. Luo, and Y. Yang, “Local-global
    context aware transformer for language-guided video segmentation,” *arXiv preprint
    arXiv:2203.09773*, 2022.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] C. Liang, W. Wang, T. Zhou, J. Miao, Y. Luo 和 Y. Yang, “语言引导的视频分割的局部-全局上下文感知变换器，”
    *arXiv preprint arXiv:2203.09773*, 2022年。'
- en: '[252] A. Hu, A. Kendall, and R. Cipolla, “Learning a spatio-temporal embedding
    for video instance segmentation,” in *Proc. Int. Conf. Learn. Representations*,
    2019.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] A. Hu, A. Kendall 和 R. Cipolla, “学习视频实例分割的时空嵌入，” 发表在 *Proc. Int. Conf.
    Learn. Representations*, 2019年。'
- en: '[253] Y. Cui, L. Yan, Z. Cao, and D. Liu, “Tf-blender: Temporal feature blender
    for video object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2021, pp.
    8138–8147.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Y. Cui, L. Yan, Z. Cao 和 D. Liu, “Tf-blender: 视频物体检测的时间特征混合器，” 发表在 *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2021年, 页码 8138–8147。'
- en: '[254] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun,
    “Upsnet: A unified panoptic segmentation network,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 8818–8826.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer 和 R. Urtasun, “Upsnet:
    一个统一的全景分割网络，” 发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019年, 页码
    8818–8826。'
- en: '[255] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li, and Y. Yang, “Vspw: A large-scale
    dataset for video scene parsing in the wild,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 4133–4143.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li 和 Y. Yang, “Vspw: 一个大规模的野外视频场景解析数据集，”
    发表在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021年, 页码 4133–4143。'
- en: '[256] S. D. Jain and K. Grauman, “Supervoxel-consistent foreground propagation
    in video,” in *Proc. Eur. Conf. Comput. Vis.*, 2014, pp. 656–671.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] S. D. Jain 和 K. Grauman, “视频中的超像素一致前景传播，” 发表在 *Proc. Eur. Conf. Comput.
    Vis.*, 2014年, 页码 656–671。'
- en: '[257] C. Xu, S.-H. Hsieh, C. Xiong, and J. J. Corso, “Can humans fly? action
    understanding with multiple classes of actors,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2015, pp. 2264–2273.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] C. Xu, S.-H. Hsieh, C. Xiong 和 J. J. Corso, “人类能飞吗？多类别演员的动作理解，” 发表在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015年, 页码 2264–2273。'
- en: '[258] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 3192–3199.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] H. Jhuang, J. Gall, S. Zuffi, C. Schmid 和 M. J. Black, “朝向理解动作识别，” 发表在
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2013年, 页码 3192–3199。'
- en: '[259] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 3354–3361.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] A. Geiger, P. Lenz 和 R. Urtasun, “我们准备好自动驾驶了吗？KITTI视觉基准套件，” 发表在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2012年, 页码 3354–3361。'
- en: '[260] A. Milan, L. Leal-Taixé, I. Reid, S. Roth, and K. Schindler, “Mot16:
    A benchmark for multi-object tracking,” *arXiv preprint arXiv:1603.00831*, 2016.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] A. Milan, L. Leal-Taixé, I. Reid, S. Roth 和 K. Schindler, “Mot16: 多物体跟踪基准，”
    *arXiv preprint arXiv:1603.00831*, 2016年。'
- en: '[261] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2232–2241.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] S. R. Richter, Z. Hayder 和 V. Koltun, “为基准测试而玩，” 发表在 *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2017年, 页码 2232–2241。'
- en: '[262] N. Märki, F. Perazzi, O. Wang, and A. Sorkine-Hornung, “Bilateral space
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 743–751.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] N. Märki, F. Perazzi, O. Wang 和 A. Sorkine-Hornung, “双边空间视频分割，” 发表在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016年, 页码 743–751。'
- en: '[263] D. R. Martin, C. C. Fowlkes, and J. Malik, “Learning to detect natural
    image boundaries using local brightness, color, and texture cues,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, vol. 26, no. 5, pp. 530–549, 2004.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] D. R. Martin, C. C. Fowlkes 和 J. Malik, “学习使用局部亮度、颜色和纹理线索来检测自然图像边界，”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, 第26卷，第5期, 页码 530–549, 2004年。'
- en: '[264] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object recognition
    using shape contexts,” *IEEE Trans. Pattern Anal. Mach. Intell.*, no. 4, pp. 509–522,
    2002.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] S. Belongie, J. Malik 和 J. Puzicha, “使用形状上下文进行形状匹配和物体识别，” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 第4期, 页码 509–522, 2002年。'
- en: '[265] W. Wang, M. Feiszli, H. Wang, and D. Tran, “Unidentified video objects:
    A benchmark for dense, open-world segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, pp. 10 776–10 785.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] W. Wang, M. Feiszli, H. Wang 和 D. Tran, “未识别的视频物体: 稠密开放世界分割的基准，” 发表在
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2021年, 页码 10 776–10 785。'
- en: '[266] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional computation
    in neural networks for faster models,” in *Proc. Int. Conf. Learn. Representations*,
    2016.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] E. Bengio, P.-L. Bacon, J. Pineau 和 D. Precup, “神经网络中的条件计算以加快模型速度，” 发表在
    *Proc. Int. Conf. Learn. Representations*, 2016年。'
