- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:53:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2107.01153] A Survey on Deep Learning Technique for Video Segmentation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.01153](https://ar5iv.labs.arxiv.org/html/2107.01153)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Learning Technique for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video Segmentation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Tianfei Zhou, Fatih Porikli, , David J. Crandall, ,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Luc Van Gool, Wenguan Wang T. Zhou and L. Van Gool are with ETH Zurich. (Email:
    ztfei.debug@gmail.com, vangool@vision.ee.ethz.ch) F. Porikli is with the School
    of Computer Science, Australian National University. (Email: fatih.porikli@anu.edu.au)
    D. Crandall is with the Luddy School of Informatics, Computing, and Engineering,
    Indiana University. (Email: djcran@indiana.edu) W. Wang is with ReLER Lab, Australian
    Artificial Intelligence Institute, University of Technology Sydney (Email: wenguanwang.ai@gmail.com)
    Corresponding author: Wenguan Wang'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Video segmentation—partitioning video frames into multiple segments or objects—plays
    a critical role in a broad range of practical applications, from enhancing visual
    effects in movie, to understanding scenes in autonomous driving, to creating virtual
    background in video conferencing. Recently, with the renaissance of connectionism
    in computer vision, there has been an influx of deep learning based approaches
    for video segmentation that have delivered compelling performance. In this survey,
    we comprehensively review two basic lines of research — generic object segmentation
    (of unknown categories) in videos, and video semantic segmentation — by introducing
    their respective task settings, background concepts, perceived need, development
    history, and main challenges. We also offer a detailed overview of representative
    literature on both methods and datasets. We further benchmark the reviewed methods
    on several well-known datasets. Finally, we point out open issues in this field,
    and suggest opportunities for further research. We also provide a public website
    to continuously track developments in this fast advancing field: [https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Video Segmentation, Video Object Segmentation, Video Semantic Segmentation,
    Deep Learning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Video segmentation — identifying the key objects with some specific properties
    or semantics in a video scene — is a fundamental and challenging problem in computer
    vision, with numerous potential applications including autonomous driving, robotics,
    automated surveillance, social media, augmented reality, movie production, and
    video conferencing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The problem has been addressed using various traditional computer vision and
    machine learning techniques, including hand-crafted features (*e.g*., histogram
    statistics, optical flow, *etc*.), heuristic prior knowledge (*e.g*., visual attention
    mechanism${}_{\!}$ [[1](#bib.bib1)], motion boundaries${}_{\!}$ [[2](#bib.bib2)],
    *etc*.), low/mid-level visual representations (*e.g*., super-voxel${}_{\!}$ [[3](#bib.bib3)],
    trajectory${}_{\!}$ [[4](#bib.bib4)], object proposal${}_{\!}$ [[5](#bib.bib5)],
    *etc*.), and classical machine learning models (*e.g*., clustering${}_{\!}$ [[6](#bib.bib6)],
    graph models${}_{\!}$ [[7](#bib.bib7)], random walks${}_{\!}$ [[8](#bib.bib8)],
    support vector machines${}_{\!}$ [[9](#bib.bib9)], random decision forests${}_{\!}$ [[10](#bib.bib10)],
    markov random fields${}_{\!}$ [[11](#bib.bib11)], conditional random fields${}_{\!}$ [[12](#bib.bib12)],
    *etc*.). Recently, deep neural networks, and Fully Convolutional Networks (FCNs)${}_{\!}$ [[13](#bib.bib13)]
    in particular, have led to remarkable advances in video segmentation. These deep
    learning-based video segmentation algorithms are significantly more accurate (and
    sometimes even more efficient) than traditional approaches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c07128cf20291102b1493d01ead0d206.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Video segmentation tasks reviewed in this survey: (a) object-level
    automatic video object segmentation (object-level AVOS), (b) instance-level automatic
    video object segmentation (instance-level AVOS), (c) semi-automatic video object
    segmentation (SVOS), (d) interactive video object segmentation (IVOS), (e) language-guided
    video object segmentation (LVOS), (f) video semantic segmentation (VSS), (g) video
    instance segmentation (VIS), and (h) video panoptic segmentation (VPS).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5475b71002204b582bbf219fccfcaf2d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: <svg version="1.1" width="191.1" height="9.69" overflow="visible"><g transform="translate(0,9.69)
    scale(1,-1)"><g transform="translate(-680.78,303.03)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[1](#S1
    "1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-613.67,303.03)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2](#S2
    "2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-481.53,303.03)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3](#S3
    "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-386.74,315.48)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1](#S3.SS1
    "3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-387.44,290.58)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2](#S3.SS2
    "3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-291.27,302.34)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[4](#S4
    "4 Video Segmentation Datasets ‣ A Survey on Deep Learning Technique for Video
    Segmentation")</foreignobject></g></text></g><g transform="translate(-190.95,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[5](#S5
    "5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-107.24,302.34)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="70%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[6](#S6
    "6 Future Research Directions ‣ A Survey on Deep Learning Technique for Video
    Segmentation")</foreignobject></g></text></g><g transform="translate(-48.43,302.34)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="70%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[7](#S7
    "7 CONCLUSION ‣ A Survey on Deep Learning Technique for Video Segmentation")</foreignobject></g></text></g><g
    transform="translate(-620.59,141.14)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">§<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.1](#S2.SS1
    "2.1 Problem Formulation and Taxonomy ‣ 2 Background ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-624.74,109.31)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.2](#S2.SS2
    "2.2 History and Terminology ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-637.89,77.49)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[2.3](#S2.SS3
    "2.3 Related Research Areas ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-488.45,224.16)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.1](#S3.SS1.SSS1
    "3.1.1 Automatic Video Object Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS
    Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-471.15,124.53)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.2](#S3.SS1.SSS2
    "3.1.2 Semi-automatic Video Object Segmentation (SVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-488.45,59.5)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.3](#S3.SS1.SSS3
    "3.1.3 Interactive Video Object Segmentation (IVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-471.15,17.99)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.1.4](#S3.SS1.SSS4
    "3.1.4 Language-guided Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-186.11,135.6)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.1](#S3.SS2.SSS1
    "3.2.1 (Instance-agnostic) Video Semantic Segmentation (VSS) ‣ 3.2 Deep Learning-based
    VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-187.49,69.19)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.2](#S3.SS2.SSS2
    "3.2.2 Video Instance Segmentation (VIS) ‣ 3.2 Deep Learning-based VSS Models
    ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g><g transform="translate(-186.11,8.3)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">§<g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[3.2.3](#S3.SS2.SSS3
    "3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep Learning-based VSS Models
    ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique
    for Video Segmentation")</foreignobject></g></text></g></g></svg>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Overview of this survey.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid advance of this field, there is a huge body of new literature
    being produced. However, most existing surveys predate the modern deep learning
    era${}_{\!}$ [[14](#bib.bib14), [15](#bib.bib15)], and often take a narrow view,
    such as focusing only on video foreground/background segmentation${}_{\!}$ [[16](#bib.bib16),
    [17](#bib.bib17)]. In this paper, we offer a state-of-the-art review that addresses
    the wide area${}_{\!}$ of${}_{\!}$ video${}_{\!}$ segmentation,${}_{\!}$ especially${}_{\!}$
    to${}_{\!}$ help${}_{\!}$ new${}_{\!}$ researchers${}_{\!}$ enter this rapidly-developing
    field. We systematically introduce recent advances in video segmentation, spanning
    from task formulation to taxonomy, from algorithms to datasets, and from unsolved
    issues to future research directions. We cover crucial aspects including task
    categories (*i.e*., foreground/background separation vs semantic segmentation),
    inference modes (*i.e*., automatic, semi-automatic, and interactive), and learning
    paradigms (*i.e*., supervised, unsupervised, and weakly supervised), and we try
    to clarify terminology (*e.g*., background subtraction, motion segmentation, *etc*.).
    We hope that this survey helps accelerate progress in this field.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey mainly focuses on recent progress in two major branches of video
    segmentation, namely video object segmentation (Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(a-e))
    and video semantic segmentation (Fig.${}_{\!}$ [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning Technique for Video Segmentation")(f-h)),${}_{\!}$
    which${}_{\!}$ are${}_{\!}$ further${}_{\!}$ divided${}_{\!}$ into${}_{\!}$ eight${}_{\!}$
    sub-fields. Even after restricting our focus to deep learning-based video segmentation,
    there are still hundreds of papers in this fast-growing field. We select influential
    work published in prestigious journals and conferences. We also include some non-deep
    learning video segmentation models and relevant literature in other areas, *e.g*.,
    visual tracking, to give necessary background. Moreover, in order to promote the
    development of this field, we provide an accompanying webpage which catalogs algorithms
    and datasets addressing video segmentation: [https://github.com/tfzhou/VS-Survey](https://github.com/tfzhou/VS-Survey).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique
    for Video Segmentation") shows the structure of this survey. Section §[2](#S2
    "2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation") gives
    some brief background on taxonomy, terminology, study history, and related research
    areas. We review representative papers on deep learning algorithms and video segmentation
    datasets in §[3](#S3 "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep
    Learning Technique for Video Segmentation") and §[4](#S4 "4 Video Segmentation
    Datasets ‣ A Survey on Deep Learning Technique for Video Segmentation"), respectively.
    Section §[5](#S5 "5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation") conducts performance evaluation and analysis, while §[6](#S6
    "6 Future Research Directions ‣ A Survey on Deep Learning Technique for Video
    Segmentation") raises open questions and directions. Finally, we make concluding
    remarks in §[7](#S7 "7 CONCLUSION ‣ A Survey on Deep Learning Technique for Video
    Segmentation").
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first formalize the task, categorize research directions,
    and discuss key challenges and driving factors in §[2.1](#S2.SS1 "2.1 Problem
    Formulation and Taxonomy ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation"). Then, §[2.2](#S2.SS2 "2.2 History and Terminology ‣
    2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation") offers
    a brief historical background covering early work and foundations, and §[2.3](#S2.SS3
    "2.3 Related Research Areas ‣ 2 Background ‣ A Survey on Deep Learning Technique
    for Video Segmentation") establishes linkages with relevant fields.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Problem Formulation and Taxonomy
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formally, let $\mathcal{X}$ and $\mathcal{Y}$ denote the input space and output
    segmentation space, respectively. Deep learning-based video segmentation solutions
    generally seek to learn an ideal video-to-segment mapping $f^{*\!}:\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Video Segmentation Category
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to how the output space $\mathcal{Y}$ is defined, video segmentation
    can be broadly categorized into two classes: video object (foreground/background)
    segmentation, and video semantic segmentation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Video Foreground/Background Segmentation (Video Object Segmentation,
    VOS). VOS is the classic video segmentation setting and refers to segmenting dominant
    objects (of unknown categories). In this case, $\bm{\mathcal{Y}}$ is a binary,
    foreground/background segmentation space. VOS is typically used in video analysis
    and editing application scenarios, such as object removal in movie editing, content-based
    video coding, and virtual background creation in video conferencing. It typically
    is not concerned with the exact semantic categories of the segmented objects.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Video Semantic Segmentation (VSS). As a direct extension of image
    semantic segmentation to the spatio-temporal domain, VSS aims to extract objects
    within predefined semantic categories (*e.g*., car, building, pedestrian, road)
    from videos. Thus, $\bm{\mathcal{Y}}$ corresponds to a multi-class, semantic parsing
    space. VSS serves as a perception foundation for many application fields, such
    as robot sensing, human-machine interaction, and autonomous driving, which require
    high-level understanding of the physical environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频语义分割（VSS）。作为图像语义分割在时空领域的直接扩展，VSS 旨在从视频中提取预定义语义类别（*例如*，汽车、建筑、人行道、道路）中的物体。因此，$\bm{\mathcal{Y}}$
    对应于多类别的语义解析空间。VSS 作为许多应用领域的感知基础，如机器人感知、人机交互和自动驾驶，这些领域需要对物理环境有高级的理解。
- en: Remark. VOS and VSS share some common challenges, such as fast motion and object
    occlusion. However, due to differences in application scenarios, many challenges
    are different. For instance, VOS often focuses on human created media, which often
    have large camera motion, deformation, and appearance changes. VSS instead often
    focuses on applications like autonomous driving, which requires a good trade off
    between accuracy and latency, accurate detection of small objects, model parallelization,
    and cross-domain generalization ability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。VOS 和 VSS 面临一些共同的挑战，如快速运动和物体遮挡。然而，由于应用场景的不同，许多挑战也是不同的。例如，VOS 通常关注人工创建的媒体，这些媒体通常有大的摄像机运动、形变和外观变化。而
    VSS 则通常关注诸如自动驾驶等应用，这些应用需要在准确性和延迟之间做出良好的权衡，精确检测小物体，模型并行化，以及跨领域的泛化能力。
- en: 2.1.2 Inference Modes for Video Segmentation
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 视频分割的推理模式
- en: 'VOS methods can be further classified into three types: automatic, semi-automatic,
    and interactive, according to how much human intervention is involved during inference.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: VOS 方法可以进一步分为三类：自动、半自动和交互式，具体取决于推理过程中涉及的人工干预程度。
- en: $\bullet$ Automatic Video Object Segmentation (AVOS). AVOS, or unsupervised
    video segmentation or zero-shot video segmentation, performs VOS in an automatic
    manner, without any manual initialization (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning Technique for Video Segmentation")(a-b)). The input
    space $\bm{\mathcal{X}}$ refers to the video domain $\bm{\mathcal{V}}$ only. AVOS
    is suitable for video analysis but not for video editing that requires segmenting
    arbitrary objects or their parts flexibly; a typical application is virtual background
    creation in video conferencing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 自动视频对象分割（AVOS）。AVOS，或无监督视频分割或零样本视频分割，以自动方式执行 VOS，无需任何手动初始化（图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 视频分割深度学习技术调查")(a-b)）。输入空间 $\bm{\mathcal{X}}$ 仅指视频领域 $\bm{\mathcal{V}}$。AVOS
    适用于视频分析，但不适用于需要灵活分割任意物体或其部分的视频编辑；一个典型的应用是视频会议中的虚拟背景创建。
- en: $\bullet$ Semi-automatic Video Object Segmentation (SVOS). SVOS, also known
    as semi-supervised video segmentation or one-shot video segmentation [[18](#bib.bib18)],
    involves limited human inspection (typically provided in the first frame) to specify
    the desired objects (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on
    Deep Learning Technique for Video Segmentation")(c)). For SVOS, $\bm{\mathcal{X}}_{\!}\!=_{\!}\!\bm{\mathcal{V}}\!\times\!\bm{\mathcal{M}}$,
    where $\bm{\mathcal{V}}_{\!}$ indicates the video space and $\bm{\mathcal{M}}$
    refers to human input. Typically the human input is an object mask in the first
    video frame, in which case SVOS is also called pixel-wise tracking or mask propagation.
    Other forms of human input include bounding boxes and scribbles [[8](#bib.bib8)].
    From this perspective, language-guided video object segmentation (LVOS) is a sub-branch
    of SVOS, in which the human input is given as linguistic descriptions about the
    desired objects (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep
    Learning Technique for Video Segmentation")(e)). Compared to AVOS, SVOS is more
    flexible in defining target objects, but requires human input. SVOS is typically
    applied in a user-friendly setting (without specialized equipment), such as video
    content creation in mobile phones. One of the core challenges in SVOS is how to
    fully utilize target information from limited human intervention.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Interactive Video Object Segmentation (IVOS). SVOS models are designed
    to operate automatically once the target has been identified, while systems for
    IVOS incorporate user guidance throughout the analysis process (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(d)).
    IVOS can obtain high-quality segments and works well for computer-generated imagery
    and video post-production, where tedious human supervision is possible. IVOS is
    also studied in the graphics community as video cutout. The input space $\bm{\mathcal{X}}$
    for IVOS is $\bm{\mathcal{V}}\!\times\!\bm{\mathcal{S}}$, where $\bm{\mathcal{S}}$
    typically refers to human scribbling. Key challenges include: 1) allowing users
    to easily specify segmentation constraints; 2) incorporating human specified constraints
    into the segmentation algorithm; and 3) giving quick response to the constraints.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to VOS, VSS methods typically work in an automatic mode (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Technique for Video Segmentation")(f-h)),
    *i.e*., $\bm{\mathcal{X}}\!\equiv\!\bm{\mathcal{V}}$. Only a few early methods
    address the semi-automatic setting, called label propagation [[19](#bib.bib19)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Remark. The terms “unsupervised” and “semi-supervised” are conventionally used
    in VOS to specify the amount of human interaction involved during inference. But
    they are easily confused with “unsupervised${}_{\!}$ learning”${}_{\!}$ and${}_{\!}$
    “semi-supervised${}_{\!}$ learning.”${}_{\!}$ We${}_{\!}$ urge${}_{\!}$ the${}_{\!}$
    community${}_{\!}$ to${}_{\!}$ replace${}_{\!}$ these${}_{\!}$ ambiguous${}_{\!}$
    terms with “automatic” and “semi-automatic.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Learning Paradigms for Video Segmentation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning-based video segmentation models can be grouped into three categories
    according to the learning strategy they use to approximate $f^{*}$: supervised,
    unsupervised, and weakly supervised.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Supervised Learning Methods. Modern video segmentation models are
    typically learned in a fully supervised manner, requiring $N$ input training samples
    and their desired outputs $y_{n}\!\!:=\!f^{*\!}(x_{n})$, where $\{(x_{n},y_{n})\}_{n\!}\!\subset$$\mathcal{X}$$\times$$\mathcal{Y}$.
    The standard method for evaluating learning outcomes follows an empirical risk/loss
    minimization formulation:¹¹1We omit the regularization term for brevity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\tilde{f}\in\mathop{\arg\min}_{f\in\bm{\mathcal{F}}}\frac{1}{N}\sum\nolimits_{n}\varepsilon(f(x_{n}),z(x_{n})),\vspace{-3pt}$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{F}$ denotes the hypothesis (solution) space, and $\varepsilon_{\!\!}:$
    $\bm{\mathcal{X}}_{\!}\times\bm{\mathcal{Y}}_{\!}\mapsto\!\mathbb{R}$ is an error
    function that evaluates the estimate $f(x_{n})$ against video segmentation related
    prior knowledge $z(x_{n})\!\!\in$$\mathcal{Z}$. To make $\tilde{f}$ a good approximation
    of $f^{*\!}$, current supervised video segmentation methods directly use the desired
    output $y_{n}$, *i.e*., $z(x_{n})\!\!:=\!\!f^{*\!}(x_{n})$, as the prior knowledge,
    with${}_{\!}$ the${}_{\!}$ price${}_{\!}$ of${}_{\!}$ requiring${}_{\!}$ vast${}_{\!}$
    amounts${}_{\!}$ of${}_{\!}$ well-labeled${}_{\!}$ data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Unsupervised (Self-supervised) Learning Methods. When only data samples
    $\{x_{n}\}_{n\!\!}\!\subset$$\mathcal{X}$ are given, the problem of approximating
    $f^{*\!}$ is known as unsupervised learning. Unsupervised learning includes fully
    unsupervised learning methods in which the methods do not need any labels at all,
    as well as self-supervised learning methods in which networks are explicitly trained
    with automatically-generated pseudo labels without any human annotations [[20](#bib.bib20)].
    Almost all existing unsupervised learning-based video segmentation models are
    self-supervised learning methods, where the prior knowledge $\mathcal{Z}$ refers
    to pseudo labels derived from intrinsic properties of video data (*e.g*., cross-frame
    consistency). We thus use “unsupervised learning” and “self-supervised learning”
    interchangeably.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Weakly-Supervised Learning Methods. In this case, $\mathcal{Z}$ is
    typically a more easily-annotated domain, such as tags, bounding boxes, or scribbles,
    and $f^{*}$ is approximated using a finite number of samples from $\mathcal{X}$$\times$$\mathcal{Z}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Remark. So far, deep supervised learning-based methods are dominant in the field
    of video segmentation. However, exploring the task in an unsupervised or weakly
    supervised setting is more appealing, not only because it alleviates the annotation
    burden of $\mathcal{Y}$, but because it inspires an in-depth${}_{\!}$ understanding${}_{\!}$
    of${}_{\!}$ the${}_{\!}$ nature${}_{\!}$ of${}_{\!}$ the${}_{\!}$ task${}_{\!}$
    by${}_{\!}$ exploring${}_{\!}$ $\mathcal{Z}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 History and Terminology
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Digital image segmentation has been studied for at least 50 years, starting
    with the Roberts operator [[21](#bib.bib21)] for identifying object boundaries.
    Since then, numerous algorithms for image segmentation have been proposed, and
    many are extended to the video domain. The field of video segmentation has evolved
    quickly and undergone great change.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Earlier attempts focus on video over-segmentation, *i.e*., partitioning a video
    into space-time homogeneous, perceptually distinct-regions. Typical approaches
    include hierarchical video segmentation [[7](#bib.bib7)], temporal superpixel [[22](#bib.bib22)],
    and super-voxels [[3](#bib.bib3)], based on the discontinuity and similarity of
    pixel intensities in a particular location, *i.e*., separating pixels according
    to abrupt changes in intensity or grouping pixels with similar intensity together.
    These methods are instructive for early stage video preprocessing, but cannot
    solve the problem of object-level pattern modeling, as they do not provide any
    principled approach to flatten the hierarchical video decomposition into a binary
    segmentation [[9](#bib.bib9), [2](#bib.bib2)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: To extract foreground objects from video sequences, background subtraction techniques
    emerged beginning in the late 70s [[23](#bib.bib23)], and became popular following
    the work of [[24](#bib.bib24)]. They assume that the background is known a priori,
    and that the camera is stationary​ [[25](#bib.bib25), [26](#bib.bib26)] or undergoes
    a predictable, parametric 2D​ [[27](#bib.bib27)] or 3D motion with 3D parallax​ [[28](#bib.bib28)].
    These geometry-based methods fit well for specific application scenarios such
    as surveillance systems [[26](#bib.bib26), [9](#bib.bib9)], but they are sensitive
    to model selection (2D or 3D), and cannot handle non-rigid camera movements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Another group of video segmentation solutions tackled the task of motion segmentation,
    *i.e*., finding objects in motion. Background subtraction can also be viewed as
    a specific case of motion segmentation. However, most motion segmentation models
    are built upon motion analysis [[29](#bib.bib29), [30](#bib.bib30)], factorization​ [[31](#bib.bib31)],
    and/or statistical​ [[32](#bib.bib32)] techniques that comprehensively model the
    characteristics of moving scenes without prior knowledge of camera motion. Among
    the big family of motion segmentation algorithms, trajectory segmentation attained
    particular attention [[4](#bib.bib4), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)]. Trajectories are generated through tracking points over multiple
    frames and can represent long-term motion patterns, serving as an informative
    cue for segmentation. Though impressive, motion-based methods heavily rely on
    the accuracy of optical flow estimation and can fail when different parts of an
    object exhibit heterogeneous motions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, the task of extracting generic objects from unconstrained
    video sequences, *i.e*., AVOS, has drawn increasing research interest [[37](#bib.bib37)].
    Several methods [[5](#bib.bib5), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]
    explored object hypotheses or proposals [[41](#bib.bib41)] as middle-level object
    representations. They generate a large number of object candidates in every frame
    and cast the task of segmenting video objects as an object region selection problem.
    The main drawbacks of the proposal-based algorithms are the high computational
    cost [[17](#bib.bib17)] and complicated object inference schemes. Some others
    explored heuristic hypotheses such as visual attention [[1](#bib.bib1)] and motion
    boundary [[2](#bib.bib2)], but easily fail in scenarios where the heuristic assumptions
    do not hold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'As argued earlier, an alternative to the above unattended solutions is to incorporate
    human-marked initialization, *i.e*., SVOS. Older SVOS methods often rely on optical
    flow [[42](#bib.bib42), [8](#bib.bib8), [43](#bib.bib43), [44](#bib.bib44)] and
    share a similar spirit with object tracking [[45](#bib.bib45), [46](#bib.bib46)].
    In addition, some pioneering IVOS methods were proposed to address high-quality
    video segmentation under extensive human guidance, including rotoscoping [[47](#bib.bib47),
    [48](#bib.bib48)], scribble [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [8](#bib.bib8)], contour [[53](#bib.bib53)], and points [[54](#bib.bib54)].
    Significant engineering is typically needed to allow IVOS systems to operate at
    interactive speeds. In short, SVOS and IVOS pay for the improved flexibility and
    accuracy: they are infeasible at large scale due to their human-in-the-loop nature.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In the pre-deep learning era, relatively few papers${}_{\!}$ [[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [12](#bib.bib12)] considered
    VSS due to the complexity of the task. The approaches typically relied on supervised
    classifiers such as SVMs and video over-segmentation techniques.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Overall, traditional approaches for video segmentation, though giving interesting
    results, are constrained by hand-crafted features and heavy engineering. But deep
    learning brought the performance of video segmentation to a new level, as we will
    review in §[3](#S3 "3 Deep Learning-based Video Segmentation ‣ A Survey on Deep
    Learning Technique for Video Segmentation").
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Related Research Areas
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several research fields closely related to video segmentation, which
    we now briefly describe.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Visual Tracking. To infer the location of a target object over time,
    current tracking methods usually assume that the target is determined by a bounding
    box in the first frame [[59](#bib.bib59)]. However, in more general tracking scenarios,
    and in particular the cases studied in early tracking methods, diverse object
    representations are explored [[60](#bib.bib60)], including centroids, skeletons,
    and contours. Some video segmentation techniques, such as background subtraction,
    are also merged into older trackers [[61](#bib.bib61), [62](#bib.bib62)]. Hence,
    visual tracking and video segmentation encounter some common challenges (*e.g*.,
    object/camera motion, appearance change, occlusion, *etc*.), fostering their mutual
    collaboration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Image Semantic Segmentation. The success of end-to-end image semantic
    segmentation [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)] has sparked
    the rapid development of VSS. Rather than directly applying image semantic segmentation
    techniques frame by frame, recent VSS systems explore temporal continuity to increase
    both accuracy and efficiency. Nevertheless, image semantic segmentation techniques
    continue to serve as a foundation for advancing segmentation in video.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of essential characteristics for reviewed AVOS methods (§[3.1.1](#S3.SS1.SSS1
    "3.1.1 Automatic Video Object Segmentation (AVOS) ‣ 3.1 Deep Learning-based VOS
    Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance: instance- or object-level segmentation; Flow: if optical flow is
    used.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Core Architecture | Instance | Flow | Training Dataset
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| 2017 | FSEG [[66](#bib.bib66)] | CVPR | Two-Stream FCN | Object | ✓ | ImageNet
    VID [[67](#bib.bib67)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| SFL [[68](#bib.bib68)] | ICCV | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| LVO [[69](#bib.bib69)] | ICCV | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| LMP [[70](#bib.bib70)] | ICCV | FCN | Object | ✓ | FT3D [[71](#bib.bib71)]
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| NRF [[72](#bib.bib72)] | ICCV | FCN | Object | ✓ | Youtube-Objects [[73](#bib.bib73)]
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 2018 | IST [[74](#bib.bib74)] | CVPR | FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| FGRNE [[75](#bib.bib75)] | CVPR | FCN + RNN | Object |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| MBN [[77](#bib.bib77)] | ECCV | FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| PDB [[78](#bib.bib78)] | ECCV | RNN | Object |  | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| MOT [[79](#bib.bib79)] | ICRA | Two-Stream FCN | Object | ✓ | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| 2019 | RVOS [[80](#bib.bib80)] | CVPR | RNN | Instance |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| COSNet [[83](#bib.bib83)] | CVPR | Siamese FCN + Co-attention | Object |  |
    MSRA10K [[84](#bib.bib84)] + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| UMOD [[86](#bib.bib86)] | CVPR | Adversarial Network | Object | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| AGS [[87](#bib.bib87)] | CVPR | FCN | Object |  | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + DUT [[85](#bib.bib85)] + PASCAL-S [[88](#bib.bib88)]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| AGNN [[89](#bib.bib89)] | ICCV | FCN + GNN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| MGA [[90](#bib.bib90)] | ICCV | Two-Stream FCN | Object | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| AnDiff [[92](#bib.bib92)] | ICCV | Siamese FCN + Co-attention | Object |  |
    DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| LSMO [[93](#bib.bib93)] | IJCV | Two-Stream FCN | Object | ✓ | FT3D [[71](#bib.bib71)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MATNet [[94](#bib.bib94)] | AAAI | Two-Stream FCN | Object | ✓ | Youtube-VOS [[95](#bib.bib95)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| PyramidCSA [[96](#bib.bib96)] | AAAI | Siamese FCN + Co-attention | Object
    |  | DUTS [[91](#bib.bib91)] + DAVIS[16] [[17](#bib.bib17)] + DAVSOD [[97](#bib.bib97)]
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| MuG [[98](#bib.bib98)] | CVPR | FCN | Object |  | OxUvA [[99](#bib.bib99)]
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| EGMN [[100](#bib.bib100)] | ECCV | FCN + Episodic Memory | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| WCSNet [[101](#bib.bib101)] | ECCV | Siamese FCN | Object |  | SALICON [[102](#bib.bib102)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DUTS [[91](#bib.bib91)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| DFNet [[104](#bib.bib104)] | ECCV | Siamese FCN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DUT [[85](#bib.bib85)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| 2021 | F2Net [[105](#bib.bib105)] | AAAI | Siamese FCN | Object |  | MSRA10K [[84](#bib.bib84)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| TODA [[106](#bib.bib106)] | CVPR | Siamese FCN | Instance |  | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| RTNet [[107](#bib.bib107)] | CVPR | Two-Stream FCN | Object | ✓ | DUTS [[91](#bib.bib91)]
    + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| DyStab [[108](#bib.bib108)] | CVPR | Adversarial Network | Object | ✓ | SegTrackV2 [[76](#bib.bib76)]
    + DAVIS[16] [[17](#bib.bib17)] + FBMS [[36](#bib.bib36)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| MotionGrouping [[109](#bib.bib109)] | ICCV | Transformer | Object | ✓ | DAVIS16 [[17](#bib.bib17)]/SegTrackV2 [[76](#bib.bib76)]/FBMS59 [[110](#bib.bib110)]/MoCA [[111](#bib.bib111)]
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: $\bullet$ Video Object Detection. To generalize object detection in the video
    domain [[112](#bib.bib112)], video object detectors incorporate temporal cues
    over the box- or feature- level. There are many key technical steps and challenges,
    such as object proposal generation, temporal information aggregation, and cross-frame
    object association, that are shared between video object detection and (instance-level)
    video segmentation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning-based Video Segmentation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Deep Learning-based VOS Models
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'VOS extracts generic foreground objects from video sequences with no concern
    for semantic category recognition. Based on how much human intervention is involved
    in inference, VOS models can be divided into three classes (§[2.1.2](#S2.SS1.SSS2
    "2.1.2 Inference Modes for Video Segmentation ‣ 2.1 Problem Formulation and Taxonomy
    ‣ 2 Background ‣ A Survey on Deep Learning Technique for Video Segmentation")):
    automatic (AVOS, §[3.1.1](#S3.SS1.SSS1 "3.1.1 Automatic Video Object Segmentation
    (AVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), semi-automatic
    (SVOS, §[3.1.2](#S3.SS1.SSS2 "3.1.2 Semi-automatic Video Object Segmentation (SVOS)
    ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), and interactive
    (IVOS, §[3.1.3](#S3.SS1.SSS3 "3.1.3 Interactive Video Object Segmentation (IVOS)
    ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")). Moreover, although
    language-guided video object segmentation (LVOS) falls in the broader category
    of SVOS, LVOS methods are reviewed alone (§[3.1.4](#S3.SS1.SSS4 "3.1.4 Language-guided
    Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based VOS Models ‣ 3 Deep
    Learning-based Video Segmentation ‣ A Survey on Deep Learning Technique for Video
    Segmentation")), due to the specific multi-modal task setup.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Automatic Video Object Segmentation (AVOS)
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of using heuristic priors and hand-crafted features to automatically
    execute VOS, modern AVOS methods learn generic video object patterns in a data-driven
    fashion. We group landmark efforts based on their key techniques.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Deep Learning Module based Methods. In 2015, Fragkiadaki *et al*. [[113](#bib.bib113)]
    made an early effort that learns a multi-layer perceptron to rank proposal segments
    and infer foreground objects. In 2016, Tsai *et al*. [[43](#bib.bib43)] proposed
    a joint optimization framework for AVOS and optical flow estimation with a naïve
    use of deep features from a pre-trained classification network. Later methods [[72](#bib.bib72),
    [70](#bib.bib70)] learn FCNs to predict initial, pixel-level foreground estimates
    from frame images [[72](#bib.bib72), [114](#bib.bib114)] or optical flow fields [[70](#bib.bib70)],
    while several post-processing steps are still needed. Basically, these primitive
    solutions largely rely on traditional AVOS techniques; the learning ability of
    neural networks is under-explored.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Pixel Instance Embedding based Methods. A group of AVOS models has
    been developed to make use of stronger deep learning descriptors [[74](#bib.bib74),
    [77](#bib.bib77)] – instance embeddings – learned from image instance segmentation
    data [[115](#bib.bib115)]. They first generate pixel-wise instance embeddings,
    and select representative embeddings which are clustered into foreground and background.
    Finally, the labels of the sampled embeddings are propagated to the other ones.
    The clustering and propagation can be achieved without video specific supervision.
    Though using fewer annotations, these methods suffer from a fragmented and complicated
    pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ End-to-end Methods with Short-term Information Encoding. End-to-end
    model designs became the mainstream in this field. For example, convolutional
    recurrent neural networks (RNNs) were used to learn spatial and temporal visual
    patterns jointly [[78](#bib.bib78), [89](#bib.bib89)]. Another big family is built
    upon two-stream networks [[66](#bib.bib66), [68](#bib.bib68), [90](#bib.bib90),
    [75](#bib.bib75), [93](#bib.bib93), [69](#bib.bib69), [94](#bib.bib94)], wherein
    two parallel streams are built to extract features from raw image and optical
    flow, which are further fused for segmentation prediction. Two-stream methods
    make explicit use of appearance and motion cues, at the cost of optical flow computation
    and vast learnable parameters. These end-to-end methods improve accuracy and show
    the advantages of applying neural networks to this task. However, they only consider
    local content within very limited time span; they stack appearance and/or motion
    information from a few successive frames as input, ignoring relations among distant
    frames. Although RNNs are usually adopted, their internal hidden memory creates
    the inherent limits in modeling longer-term dependencies [[116](#bib.bib116)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of essential characteristics for reviewed SVOS methods (§[3.1.2](#S3.SS1.SSS2
    "3.1.2 Semi-automatic Video Object Segmentation (SVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")). Flow: if optical flow is used.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Core Architecture | Flow | Technical Feature | Training
    Dataset |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| 2017 | OSVOS [[18](#bib.bib18)] | CVPR | FCN |  | Online Fine-tuning | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| MaskTrack [[117](#bib.bib117)] | CVPR | FCN | ✓ | Propagation-based | ECSSD [[118](#bib.bib118)]
    + MSRA10K [[84](#bib.bib84)] + PASCAL-S [[88](#bib.bib88)] + DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| CTN [[119](#bib.bib119)] | CVPR | FCN | ✓ | Propagation-based | PASCAL VOC
    2012 [[103](#bib.bib103)] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| VPN [[120](#bib.bib120)] | CVPR | Bilateral Network |  | Propagation-based
    | DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| PLM [[121](#bib.bib121)] | CVPR | Siamese FCN |  | Matching-based | DAVIS[16] [[17](#bib.bib17)]
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| OnAVOS [[122](#bib.bib122)] | BMVC | FCN |  | Online Fine-tuning | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + DAVIS [[17](#bib.bib17)]
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| Lucid [[124](#bib.bib124)] | IJCV | Two-Stream FCN | ✓ | Propagation-based
    | DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CINM [[125](#bib.bib125)] | CVPR | Spatio-temporal MRF | ✓ | Propagation-based
    | DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| FAVOS [[126](#bib.bib126)] | CVPR | FCN |  | Propagation-based | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| RGMP [[127](#bib.bib127)] | CVPR | Siamese FCN |  | Propagation-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + ECSSD [[118](#bib.bib118)] + MSRA10K [[84](#bib.bib84)]
    + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| OSMN [[128](#bib.bib128)] | CVPR | FCN + Meta Learning |  | Online Fine-tuning
    | ImageNet VID [[67](#bib.bib67)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| MONet [[129](#bib.bib129)] | CVPR | FCN | ✓ | Online Fine-tuning | PASCAL
    VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| CRN [[130](#bib.bib130)] | CVPR | FCN + Active Contour | ✓ | Propagation-based
    | PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| RCAL [[131](#bib.bib131)] | CVPR | FCN + RL |  | Propagation-based | MSRA10K [[84](#bib.bib84)]
    + PASCAL-S + SOD + ECSSD [[118](#bib.bib118)] + DAVIS[16] [[17](#bib.bib17)] |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| OSVOS-S [[132](#bib.bib132)] | PAMI | FCN |  | Online Fine-tuning | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Videomatch [[133](#bib.bib133)] | ECCV | Siamese FCN |  | Matching-based
    | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Dyenet [[134](#bib.bib134)] | ECCV | Re-ID |  | Propagation-based | DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| LSE [[135](#bib.bib135)] | ECCV | FCN |  | Propagation-based | PASCAL VOC
    2012 [[103](#bib.bib103)] |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Colorization [[136](#bib.bib136)] | ECCV | Siamese FCN |  | Unsupervised
    Learning | Kinetics [[137](#bib.bib137)] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| 2019 | MVOS [[138](#bib.bib138)] | PAMI | Siamese FCN + Meta Learning |  |
    Online Fine-tuning | PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| FEELVOS [[139](#bib.bib139)] | CVPR | FCN |  | Matching-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| MHP-VOS [[140](#bib.bib140)] | CVPR | Graph Optimization |  | Propagation-based
    | COCO [[123](#bib.bib123)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| AGSS [[141](#bib.bib141)] | CVPR | FCN | ✓ | Propagation-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| AGAME [[142](#bib.bib142)] | CVPR | FCN |  | Propagation-based | MSRA10K [[84](#bib.bib84)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| SiamMask [[143](#bib.bib143)] | CVPR | Siamese FCN |  | Box-Initialization
    | DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| RVOS [[80](#bib.bib80)] | CVPR | RNN |  | Propagation-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| BubbleNet [[144](#bib.bib144)] | CVPR | Siamese Network |  | Bubble Sorting
    | DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| RANet [[145](#bib.bib145)] | ICCV | Siamese FCN |  | Matching-based | MSRA10K [[84](#bib.bib84)]
    + ECSSD [[118](#bib.bib118)]+ HKU-IS [[146](#bib.bib146)] + DAVIS[16] [[17](#bib.bib17)]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| DMM-Net [[147](#bib.bib147)] | ICCV | Mask R-CNN |  | Differentiable Matching
    | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| DTN [[148](#bib.bib148)] | ICCV | FCN | ✓ | Propagation-based | COCO [[123](#bib.bib123)]
    + PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[16]/DAVIS[17] [[81](#bib.bib81)]
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| STM [[149](#bib.bib149)] | ICCV | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| TimeCycle [[150](#bib.bib150)] | ECCV | Siamese FCN |  | Unsupervised Learning
    | VLOG [[151](#bib.bib151)] |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '|  | UVC [[152](#bib.bib152)] | NeurIPS | Siamese FCN |  | Unsupervised Learning
    | COCO [[123](#bib.bib123)] + Kinetics [[137](#bib.bib137)] |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 2020 | e-OSVOS [[153](#bib.bib153)] | NeurIPS | Mask R-CNN + Meta Learning
    |  | Online Fine-tuning | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| AFB-URR [[154](#bib.bib154)] | NeurIPS | Memory Network |  | Matching-based
    | PASCAL VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| Fasttan [[155](#bib.bib155)] | CVPR | Faster R-CNN |  | Propagation-based
    | COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Fasttmu [[156](#bib.bib156)] | CVPR | FCN + RL |  | Box-Initialization |
    PASCAL VOC 2012 [[103](#bib.bib103)] + DAVIS[17] [[81](#bib.bib81)] |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| SAT [[157](#bib.bib157)] | CVPR | FCN + RL |  | Propagation-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| FRTM-VOS [[158](#bib.bib158)] | CVPR | FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| TVOS [[159](#bib.bib159)] | CVPR | FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| MuG [[98](#bib.bib98)] | CVPR | Siamese FCN |  | Unsupervised Learning |
    OxUvA [[99](#bib.bib99)] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| MAST [[160](#bib.bib160)] | CVPR | Memory Network |  | Unsupervised Learning
    | OxUvA [[99](#bib.bib99)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| GCNet [[161](#bib.bib161)] | ECCV | Memory Network |  | Matching-based |
    MSRA10K [[84](#bib.bib84)] + ECSSD [[118](#bib.bib118)] + HKU-IS [[146](#bib.bib146)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| KMN [[162](#bib.bib162)] | ECCV | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| CFBI [[163](#bib.bib163)] | ECCV | FCN |  | Matching-based | COCO [[123](#bib.bib123)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| LWL [[164](#bib.bib164)] | ECCV | Siamese FCN + Meta Learning |  | Matching-based
    | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| MSN [[165](#bib.bib165)] | ECCV | Memory Network |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| EGMN [[100](#bib.bib100)] | ECCV | Memory Network |  | Matching-based | MSRA10K [[84](#bib.bib84)]
    + COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| STM-Cycle [[166](#bib.bib166)] | NeurIPS | Memory Network |  | Matching-based
    | DAVIS[17] [[81](#bib.bib81)] + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| 2021 | QMA [[167](#bib.bib167)] | AAAI | Memory Network |  | Box-Initialization
    | DUT [[85](#bib.bib85)] + HKU-IS [[146](#bib.bib146)] + MSRA10K [[84](#bib.bib84)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| SwiftNet [[168](#bib.bib168)] | CVPR | Memory Network |  | Matching-based
    | COCO [[123](#bib.bib123)] + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)]
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| G-FRTM [[169](#bib.bib169)] | CVPR | FCN + RL |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| SST [[170](#bib.bib170)] | CVPR | Transformer |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| GIEL [[171](#bib.bib171)] | CVPR | Siamese FCN |  | Matching-based | DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| LCM [[172](#bib.bib172)] | CVPR | Memory Network |  | Matching-based | PASCAL
    VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| RMNet [[173](#bib.bib173)] | CVPR | Memory Network | ✓ | Matching-based |
    PASCAL VOC 2012 [[103](#bib.bib103)] + COCO [[123](#bib.bib123)] + ECSSD [[118](#bib.bib118)]
    + DAVIS[17] [[81](#bib.bib81)]/YouTube-VOS [[95](#bib.bib95)] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| CRW [[174](#bib.bib174)] | NeurIPS | FCN |  | Unsupervised Learning | Kinetics
    [[137](#bib.bib137)] |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: $\bullet$ End-to-end Methods with Long-term Context Encoding. Current leading
    AVOS models use global context over long time spans. In a seminal work [[83](#bib.bib83)],
    Lu *et al*. proposed a Siamese architecture-based model that extracts features
    for arbitrary frame pairs and captures cross-frame context by calculating pixel-wise
    feature correlations. During inference, for each test frame, context from several
    other frames (within the same video) is aggregated to locate objects. A contemporary
    work [[92](#bib.bib92)] exploited a similar idea but only used the first frame
    as reference. Several papers [[101](#bib.bib101), [107](#bib.bib107)] extended
    [[83](#bib.bib83)] by making better use of information from multiple frames [[89](#bib.bib89),
    [175](#bib.bib175), [176](#bib.bib176)], encoding spatial context [[105](#bib.bib105)],
    and incorporating temporal consistency to improve representation power and computation
    efficiency [[96](#bib.bib96), [104](#bib.bib104)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ ${}_{\!}$Un-/Weakly-supervised${}_{\!}$ based${}_{\!}$ Methods. Only
    a handful of methods learn to perform AVOS from unlabeled or weakly labeled data.
    In [[87](#bib.bib87)], static image salient object segmentation and dynamic eye
    fixation data, which are more easily acquired compared with video segmentation
    data, are used to learn video generic object patterns. In [[98](#bib.bib98)],
    visual patterns are learned through exploring several intrinsic properties of
    video data at multiple granularities, *i.e*., intra-frame saliency, short-term
    visual coherence, long-range semantic correspondence, and video-level discriminativeness.
    In [[86](#bib.bib86)], an adversarial contextual model is developed to segment
    moving objects without any manual annotation, achieved by minimizing the mutual
    information between the motions of an object and its context. This method is further
    enhanced in [[108](#bib.bib108)] by adopting a bootstrapping strategy and enforcing
    temporal consistency. In [[109](#bib.bib109)], motion is exclusively exploited
    to discover moving objects, and a Transformer-based model is designed and trained
    by self-supervised flow reconstruction using unlabeled video data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Instance-level AVOS Methods. Instance-level AVOS, also referred as
    multi-object unsupervised video segmentation, was introduced with the launch of
    the DAVIS[19] challenge [[177](#bib.bib177)]. This task setting is more challenging
    as it requires not only separating the foreground objects from the background,
    but also discriminating different object instances. To tackle this task, current
    solutions typically work in a top-down fashion, *i.e*., generating object candidates
    for each frames, and associating instances over different frames. In an early
    attempt [[80](#bib.bib80)], Ventura *et al*. delivered a recurrent network-based
    model that consists of a spatial LSTM for per-frame instance discovery and a temporal
    LSTM for cross-frame instance association. This method features an elegant model
    design, while its representation ability is too weak to enumerate all the object
    instances and to capture complex interactions between instances over the temporal
    domain. Thus later methods [[178](#bib.bib178), [179](#bib.bib179), [175](#bib.bib175)]
    strengthen the two-step pipeline through: i) employing image instance segmentation
    models (*e.g*., Mask R-CNN [[180](#bib.bib180)]) to detect object candidates,
    and ii) leveraging tracking/re-identification techniques and manually designed
    rules for instance association. Foreground/background AVOS techniques [[89](#bib.bib89),
    [83](#bib.bib83)] are also used to filter out nonsalient candidates [[179](#bib.bib179),
    [175](#bib.bib175)]. More recent methods, *e.g*., [[106](#bib.bib106)], generate
    object candidates first and obtain corresponding tracklets via advanced SVOS techniques.
    Overall, current instance-level AVOS models follow the classic tracking-by-detection
    paradigm, involving several ad-hoc designs. There is still considerable room for
    further improvement in accuracy and efficiency.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Semi-automatic Video Object Segmentation (SVOS)
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning-based SVOS methods mainly focus on the first-frame mask propagation
    setting. They are categorized by their utilization of the test-time provided object
    masks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Online Fine-tuning based Methods. Following the one-
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'shot principle, this family of methods [[18](#bib.bib18), [132](#bib.bib132),
    [129](#bib.bib129), [138](#bib.bib138)] trains a segmentation model separately
    on each given object mask in an online fashion. Fine-tuning methods essentially
    exploit the transfer learning capabilities of neural networks and often follow
    a two-step training procedure: i) offline pre-training: learn general segmentation
    features from images and video sequences, and ii) online fine-tuning: learn target-specific
    representations from test-time supervision. The idea of fine-tuning was first
    introduced in [[18](#bib.bib18)], where only the initial image-mask pair is used
    for training an online, one-shot, but merely appearance-based FCN model. Then,
    in [[122](#bib.bib122)], more pixel samples in the unlabeled frames are mined
    as online training samples to better adapt to further changes over time. As [[18](#bib.bib18),
    [122](#bib.bib122)] have no notion of individual objects, [[132](#bib.bib132)]
    further incorporates instance segmentation models (*e.g*., Mask R-CNN [[180](#bib.bib180)])
    during inference. While elegant through their simplicity, fine-tuning methods
    have several weaknesses: i) pre-training is fixed and not optimized for subsequent
    fine-tuning, ii) hyperparameters of online fine-tuning are often excessively hand-crafted
    and fail to generalize between test cases, iii) the common existing fine-tuning
    setups suffer from high test runtimes (up to 1,000 training iterations per segmented
    object online [[18](#bib.bib18)]). The root cause is that these approaches choose
    to encode all the target-related cues (*i.e*., appearance, mask) into network
    parameters implicitly. Towards efficient and automated fine-tuning, some recent
    methods [[128](#bib.bib128), [138](#bib.bib138), [153](#bib.bib153)] turn to meta
    learning techniques, *i.e*., optimize the fine-tuning policies [[138](#bib.bib138),
    [153](#bib.bib153)] (*e.g*., generic model initialization, learning rates, *etc*.)
    or even directly modify network weights [[128](#bib.bib128)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Propagation-based Methods. Two recent lines of research
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: – built upon mask propagation and template matching techniques respectively
    – try to refrain from the online optimization to deliver compact, end-to-end SVOS
    solutions. In particular, propagation-based methods use the previous frame mask
    to infer the current mask${}_{\!}$ [[117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120)].
    For example, Jampani *et al*. [[120](#bib.bib120)] propose a bilateral network
    for long-range video-adaptive mask propagation. Perazzi *et al*. [[117](#bib.bib117)]
    approach SVOS by employing a modified FCN, where the previous frame mask is considered
    as an extra input channel. Follow-up work adopts optical flow guided mask alignment [[129](#bib.bib129)],
    heavy first-frame data augmentation${}_{\!}$ [[124](#bib.bib124)], and multi-step
    segmentation refinement${}_{\!}$ [[130](#bib.bib130)]. Others apply
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: re-identification to retrieve missing objects after prolonged occlusions${}_{\!}$ [[134](#bib.bib134)],
    design a reinforcement learning agent that tackles${}_{\!}$ SVOS as${}_{\!}$ a${}_{\!}$
    conditional${}_{\!}$ decision-making${}_{\!}$ process${}_{\!}$ [[131](#bib.bib131)],
    or propagate masks in a spatiotemporal MRF model to improve temporal coherency [[125](#bib.bib125)].
    Some researchers leverage location-aware embeddings to sharpen the feature${}_{\!}$ [[135](#bib.bib135)],
    or directly learn sequence-to-sequence mask propagation [[95](#bib.bib95)]. Advanced
    tracking techniques are also exploited in [[126](#bib.bib126), [140](#bib.bib140),
    [157](#bib.bib157), [155](#bib.bib155)]. Propagation-based methods are found to
    easily suffer from error accumulation due to occlusions and drifts during mask
    propagation. Conditioning propagation on the initial frame-mask pair${}_{\!}$ [[127](#bib.bib127),
    [141](#bib.bib141), [148](#bib.bib148)] seems a feasible${}_{\!}$ solution${}_{\!}$
    to${}_{\!}$ this.${}_{\!}$ Although${}_{\!}$ target-specific${}_{\!}$ mask${}_{\!}$
    is${}_{\!}$ exp-
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: licitly encoded into the segmentation network, making up for${}_{\!}$ the${}_{\!}$
    deficiencies${}_{\!}$ of${}_{\!}$ fine-tuning${}_{\!}$ methods${}_{\!}$ to${}_{\!}$
    a${}_{\!}$ certain${}_{\!}$ extent,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: propagation-based${}_{\!}$ methods still embed object appearance into hidden
    network weights. Clearly, such implicit target-appearance modeling strategy hurts
    flexibility and adaptivity (while${}_{\!}$ [[142](#bib.bib142)] is an exception
    – a generative model of target
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: and background is explicitly built to aid mask propagation).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'tionally considers first-frame annotations, but easily fails in challenging
    scenes without human feedback. Moreover, the first-frame annotations are typically
    detailed masks, which are tedious to acquire: 79 seconds per instance for coarse
    polygon annotations of COCO [[123](#bib.bib123)], and much more for higher quality.
    Thus performing  VOS  in  the  interactive'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of essential characteristics for reviewed IVOS methods (§[3.1.3](#S3.SS1.SSS3
    "3.1.3 Interactive Video Object Segmentation (IVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[t] Year Method Pub. Core Architecture Technical Feature Training Dataset 2017
    IIW [[181](#bib.bib181)] - FCN Interaction-Propagation PASCAL VOC 2012 [[103](#bib.bib103)]
    2018 BFVOS [[182](#bib.bib182)] CVPR FCN Pixel-wise Retrieval DAVIS[16] [[17](#bib.bib17)]
    2019 IVS [[183](#bib.bib183)] CVPR FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    2020 MANet [[184](#bib.bib184)] CVPR Siamese FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]
    ATNet [[185](#bib.bib185)] ECCV FCN Interaction-Propagation SBD + DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    ScribbleBox [[186](#bib.bib186)] ECCV GCN Interaction-Propagation COCO [[123](#bib.bib123)]
    + ImageNet VID [[67](#bib.bib67)] + YouTube-VOS [[95](#bib.bib95)] 2021 IVOS-W [[187](#bib.bib187)]
    CVPR FCN + RL Keyframe Selection DAVIS[17] [[81](#bib.bib81)] GIS [[188](#bib.bib188)]
    CVPR FCN Interaction-Propagation DAVIS[17] [[81](#bib.bib81)]+YouTube-VOS [[95](#bib.bib95)]
    MiVOS [[189](#bib.bib189)] CVPR Memory Network Interaction-Propagation BL30K [[189](#bib.bib189)]+DAVIS[17] [[81](#bib.bib81)]
    + YouTube-VOS [[95](#bib.bib95)]'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$${}_{\!}$ Matching-based${}_{\!}$ Methods.${}_{\!}$ This type of methods,
    might the most promising SVOS solution so far, constructs an embedding space to
    memorize the initial object embeddings, and classifies each pixel’s label according
    to their similarities to the target object in the embedding space. Thus the initial
    object appearance is explicitly modeled, and test-time fine-tuning is not needed.
    The earliest effort in this direction can be tracked back to${}_{\!}$ [[121](#bib.bib121)].
    Inspired by the advance in visual tracking [[190](#bib.bib190)], Yoon *et al*. [[121](#bib.bib121)]
    proposed a Siamese network to perform pixel-level matching between the first frame
    and
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'upcoming frames. Later, [[126](#bib.bib126)] proposed to learn an embedding
    space from the first-frame supervision and pose VOS as a task of pixel retrieval:
    pixels are simply their respective nearest neighbors in the learned embedding
    space. The idea of [[121](#bib.bib121)] is also explored in [[133](#bib.bib133)],
    while it computes two ma-'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: tching maps for each upcoming frame, with respect to the foreground and background
    annotated in the first frame. In [[139](#bib.bib139)], pixel-level similarities
    computed from the first frame and from the previous frame are used as a guide
    to segment succeeding frames. Later, many matching-based solutions were proposed [[145](#bib.bib145),
    [191](#bib.bib191)], perhaps most notably Oh *et al*., who propose a space-time
    memory (STM) model to explicitly store previously computed segmentation information
    in an external memory [[149](#bib.bib149)]. The memory facilitates learning the
    evolution of objects over time and allows for comprehensive use of past segmentation
    cues even over long period of time. Almost all current top-leading SVOS solutions [[159](#bib.bib159),
    [163](#bib.bib163)] are built upon STM; they improve the target adaption ability [[158](#bib.bib158),
    [100](#bib.bib100), [164](#bib.bib164)], incorporate local temporal continuity [[162](#bib.bib162),
    [173](#bib.bib173), [172](#bib.bib172)], explore instance-aware cues [[171](#bib.bib171)],
    and develop more efficient memory designs [[165](#bib.bib165), [161](#bib.bib161),
    [154](#bib.bib154), [168](#bib.bib168)]. Recently, [[170](#bib.bib170)] introduced
    a Transformer [[192](#bib.bib192)] based model, which performs matching-like computation
    through attending over a history of multiple frames. In general, matching-based
    solutions enjoy the advantage of flexible and differentiable model design as well
    as long-term correspondence modeling. On the other hand, feature matching relies
    on a powerful and generic feature embedding, which may limit its performance in
    challenging scenarios.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth mentioning that, as an effective technique for target-specific
    model learning, online learning is applied by many propagation [[117](#bib.bib117),
    [130](#bib.bib130), [125](#bib.bib125), [95](#bib.bib95), [140](#bib.bib140)]
    and matching [[121](#bib.bib121), [145](#bib.bib145), [134](#bib.bib134)] methods
    to boost performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Box-initialization based Methods. As pixel-wise annotations are time-consuming
    or even impractical to acquire in realistic scenes, some work has considered the
    situation where the first-frame annotation is provided in the form of a bounding
    box. Specifically, in [[143](#bib.bib143)], Siamese trackers are augmented with
    a mask prediction branch. In [[156](#bib.bib156)], reinforcement learning is introduced
    to make decisions for target updating and matching. Later, in [[167](#bib.bib167)],
    an outside memory is utilized to build a stronger Siamese track-segmenter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Un-/Weakly-supervised based Methods. To alleviate the demand for large-scale,
    pixel-wise annotated training samples, several un-/weakly-supervised learning-based
    SVOS solutions were recently developed. They are typically built as${}_{\!}$ a${}_{\!}$
    reconstruction${}_{\!}$ scheme${}_{\!}$ (*i.e*.,${}_{\!}$ each${}_{\!}$ pixel${}_{\!}$
    from${}_{\!}$ a${}_{\!}$ ‘query’ frame is reconstructed by finding and assembling
    related pixels from adjacent frame(s)) [[136](#bib.bib136), [160](#bib.bib160),
    [193](#bib.bib193)], and/or adopt a cycle-consistent tracking paradigm (*i.e*.,
    pixels/patches are encouraged to fall into the same location after one cycle of
    forward and backward tracking) [[150](#bib.bib150), [152](#bib.bib152), [98](#bib.bib98),
    [174](#bib.bib174)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Other Specific Methods. Other papers make specific contributions that
    deserve a separate look. In [[147](#bib.bib147)], Zeng *et al*. extract mask proposals
    per frame and formulate the matching between object templates and proposals in
    a differentiable manner. Instead of using only the first frame annotation, [[144](#bib.bib144)]
    learns to select the best frame from the whole video for user interaction, so
    as to boost mask propagation. In [[166](#bib.bib166)], Li *et al*. introduce a
    forward-backward data flow based cycle consistency mechanism to improve both traditional
    SVOS training and offline inference protocols, through mitigating the error propagation
    problem. To accelerate processing speed, a dynamic network [[169](#bib.bib169)]
    is proposed to selectively allocate computation source for each frame according
    to the similarity to the previous frame.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Interactive Video Object Segmentation (IVOS)
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AVOS, without any human involvement, loses flexibility in segmenting arbitrary
    objects of user interest. SVOS addi- setting has gained increasing attention.
    Unlike classic models [[48](#bib.bib48), [49](#bib.bib49), [52](#bib.bib52)] requiring
    extensive and professional user intervention, recent deep learning-based IVOS
    solutions usually work with multiple rounds of scribble supervision, to minimize
    the user’s effort. In this scenario [[194](#bib.bib194)], the user draws scribbles
    on a selected frame and an algorithm computes the segmentation maps for all video
    frames in a batch process. For refinement, user intervention and segmentation
    are repeated. This round-based interaction [[183](#bib.bib183)] is useful for
    consumer-level applications and rapid prototyping for professional usage, where
    efficiency is the main concern. One can control the segmentation quality at the
    expense of time, as more rounds of interaction will provide better results.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Interaction-propagation based Methods. The majority of current studies [[188](#bib.bib188),
    [189](#bib.bib189)] follow an interaction-propagation scheme. In the preliminary
    attempt [[181](#bib.bib181)], IVOS is achieved by a simple combination of two
    separate modules: an interactive image segmentation model [[195](#bib.bib195)]
    for producing segmentation based on user annotations; and a SVOS model [[18](#bib.bib18)]
    for propagating masks from the user-annotated frames to the others. Later, [[183](#bib.bib183)]
    devised a more compact solution, with also two modules for interaction and propagation,
    respectively. However, the two modules are internally connected through intermediate
    feature exchanging, and also externally connected, *i.e*., each of them is conditioned
    on the other’s output. In [[185](#bib.bib185)], a similar model design is also
    adopted, however, the propagation part is specifically designed to address both
    local mask tracking (over adjacent frames) and global propagation (among distant
    frames), respectively. However, these techniques [[181](#bib.bib181), [185](#bib.bib185)]
    have to start a new feed-forward computation in each interaction round, making
    them inefficient as the number of rounds grows. A more efficient solution was
    developed in [[184](#bib.bib184)]. The critical idea is to build a common encoder
    for discriminative pixel embedding learning, upon which two small network branches
    are added for interactive segmentation and mask propagation, respectively. Thus
    the model extracts pixel embeddings for all frames only once (in the first round).
    In the following rounds, the feed-forward computation is only made within the
    two shallow branches.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Other Methods. Chen *et al*. [[182](#bib.bib182)] propose a pixel
    embedding learning-based model, applicable to both SVOS and IVOS. With a similar
    idea of [[126](#bib.bib126)], IVOS is formulated as a pixel-wise retrieval problem,
    *i.e*., transferring labels to each pixel according to its nearest reference pixel.
    This model supports different kinds of user input, such as masks, clicks and scribbles,
    and can provide immediate feedback after user interaction. In [[186](#bib.bib186)],
    an interactive annotation tool is proposed for VOS. The annotation has two phases:
    annotating objects with tracked boxes, and labeling masks inside these tracks.
    Box tracks are annotated efficiently by approximating the trajectory using a parametric
    curve with a small number of control points which the annotator can interactively
    correct. Segmentation masks are corrected via scribbles which are propagated through
    time. In [[187](#bib.bib187)], a reinforcement learning framework is exploited
    to automatically determine the most valuable frame for interaction.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: ​​Summary of characteristics for reviewed LVOS methods (§[3.1.4](#S3.SS1.SSS4
    "3.1.4 Language-guided Video Object Segmentation (LVOS) ‣ 3.1 Deep Learning-based
    VOS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation")).​​​​'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '&#124; Visual + Language &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Encoder &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '| Technical Feature | Training Dataset |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| 2018 | A2DS [[196](#bib.bib196)] | CVPR | I3D + CNN | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| LangVOS [[197](#bib.bib197)] | ACCV | CNN + CNN | Cross-modal Att. | DAVIS[17]-RVOS [[196](#bib.bib196)]
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| 2019 | AAN [[198](#bib.bib198)] | ICCV | I3D + CNN | Cross-modal Att. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CDNet [[199](#bib.bib199)] | AAAI | I3D + GRU | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| PolarRPE [[200](#bib.bib200)] | IJCAI | I3D + LSTM | Dynamic Conv. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| VT-Capsule [[201](#bib.bib201)] | CVPR | I3D + CNN | Capsule Routing | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| URVOS [[202](#bib.bib202)] | ECCV | CNN + MLP | Cross-modal Att. | Refer-YouTube-VOS [[202](#bib.bib202)]
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CST [[203](#bib.bib203)] | CVPR | I3D + GRU | Cross-modal Att. | A2D
    Sentences [[196](#bib.bib196)] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| CMSANet [[204](#bib.bib204)] | PAMI | CNN + Word embed. | Cross-modal Att.
    | A2D Sentences [[196](#bib.bib196)] |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Summary of essential characteristics for reviewed VSS methods (§[3.2](#S3.SS2
    "3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")). Flow indicates
    whether optical flow is used.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Seg. Level | Core Architecture | Flow | Technical
    Feature | Training Dataset |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Clockwork [[205](#bib.bib205)] | ECCV | Semantic | FCN | ✓ | Faster
    Segmentation | Cityscapes [[206](#bib.bib206)]/YouTube-Objects [[73](#bib.bib73)]
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| FSO [[207](#bib.bib207)] | CVPR | Semantic | FCN + Dense CRF | ✓ | Temporal
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| JFS [[209](#bib.bib209)] | ECCV | Semantic | FCN | ✓ | Temporal Feature Aggregation
    | KITTI MOTS [[210](#bib.bib210)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| 2017 | BANet [[211](#bib.bib211)] | CVPR | Semantic | FCN + LSTM |  | Keyframe
    Selection | CamVid [[208](#bib.bib208)]/KITTI |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| PEARL [[212](#bib.bib212)] | ICCV | Semantic | FCN | ✓ | Flow-guided Feature
    Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| NetWarp [[213](#bib.bib213)] | ICCV | Semantic | Siamese FCN | ✓ | Flow-guided
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| DFF [[214](#bib.bib214)] | ICCV | Semantic | FCN |  | Flow-guided Feature
    Aggregation | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| BBF [[215](#bib.bib215)] | ICCV | Semantic | Two-Stream FCN | ✓ | Weakly-Supervised
    Learning | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| 2018 | GRFP [[216](#bib.bib216)] | CVPR | Semantic | FCN + GRU | ✓ | Temporal
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| LVS [[217](#bib.bib217)] | CVPR | Semantic | FCN |  | Keyframe Selection
    | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| DVSN [[218](#bib.bib218)] | CVPR | Semantic | FCN+RL | ✓ | Keyframe Selection
    | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| EUVS [[219](#bib.bib219)] | ECCV | Semantic | Bayesian CNN | ✓ | Flow-guided
    Feature Aggregation | CamVid [[208](#bib.bib208)] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| GCRF [[220](#bib.bib220)] | CVPR | Semantic | FCN+CRF | ✓ | Gaussian CRF
    | CamVid [[208](#bib.bib208)] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Accel [[221](#bib.bib221)] | CVPR | Semantic | FCN | ✓ | Keyframe
    Selection | KITTI |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| SSeg [[222](#bib.bib222)] | CVPR | Semantic | FCN |  | Weakly-Supervised
    Learning | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| MOTS [[210](#bib.bib210)] | CVPR | Instance | Mask R-CNN |  | Tracking by
    Detection | KITTI MOTS [[210](#bib.bib210)] /MOTSChallenge [[210](#bib.bib210)]
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| MaskTrack R-CNN [[82](#bib.bib82)] | ICCV | Instance | Mask R-CNN |  | Tracking
    by Detection | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| 2020 | EFC [[223](#bib.bib223)] | AAAI | Semantic | FCN | ✓ | Temporal Feature
    Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| TDNet [[224](#bib.bib224)] | CVPR | Semantic | Memory Network |  | Attention-based
    Feature Aggregation | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)]/NYUDv2 [[225](#bib.bib225)]
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| MaskProp [[226](#bib.bib226)] | CVPR | Instance | Mask R-CNN |  | Instance
    Feature Propagation | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| VPS [[227](#bib.bib227)] | CVPR | Panoptic | Mask R-CNN |  | Spatio-Temporal
    Feature Alignment | VIPER-VPS [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)]
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| MOTSNet [[228](#bib.bib228)] | CVPR | Instance | Mask R-CNN |  | Unsupervised
    Learning | KITTI MOTS [[210](#bib.bib210)] /BDD100K [[229](#bib.bib229)] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| MVAE [[230](#bib.bib230)] | CVPR | Instance | Mask R-CNN+VAE |  | Variational
    Inference | KITTI MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| ETC [[231](#bib.bib231)] | ECCV | Semantic | FCN + KD | ✓ | Knowledge Distillation
    | Cityscapes [[206](#bib.bib206)]/CamVid [[208](#bib.bib208)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| Sipmask [[232](#bib.bib232)] | ECCV | Instance | FCOS |  | Single-Stage Segmentation
    | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| STEm-Seg [[233](#bib.bib233)] | ECCV | Instance | FCN |  | Spatio-Temporal
    Embedding Learning | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)]/KITTI-MOTS [[210](#bib.bib210)]
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| Naive-Student [[234](#bib.bib234)] | ECCV | Semantic | FCN+KD |  | Semi-Supervised
    Learning | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CompFeat [[235](#bib.bib235)] | AAAI | Instance | Mask R-CNN |  |
    Spatio-Temporal Feature Alignment | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| TraDeS [[236](#bib.bib236)] | CVPR | Instance | Siamese FCN |  | Tracking
    by Detection | MOT/nuScenes/KITTI MOTS [[210](#bib.bib210)] /YouTube-VIS [[82](#bib.bib82)]
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| SG-Net [[237](#bib.bib237)] | CVPR | Instance | FCOS |  | Single-Stage Segmentation
    | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| VisTR [[238](#bib.bib238)] | CVPR | Instance | Transformer |  | Transformer-based
    Segmentation | YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| SSDE [[239](#bib.bib239)] | CVPR | Semantic | FCN |  | Semi-Supervised Learning
    | Cityscapes [[206](#bib.bib206)] |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| SiamTrack [[240](#bib.bib240)] | CVPR | Panoptic | Siamese FCN |  | Supervised
    Contrastive Learning | VIPER-VPS [[227](#bib.bib227)]/Cityscapes-VPS [[227](#bib.bib227)]
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| ViP-DeepLab [[241](#bib.bib241)] | CVPR | Panoptic | FCN |  | Depth-Aware
    Panoptic Segmentation | Cityscapes-VPS [[227](#bib.bib227)] |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| fIRN [[242](#bib.bib242)] | CVPR | Instance | Mask R-CNN | ✓ | Weakly-Supervised
    Learning | YouTube-VIS [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| SemiTrack [[243](#bib.bib243)] | CVPR | Instance | SOLO |  | Semi-Supervised
    Learning | YouTube-VIS [[82](#bib.bib82)]/Cityscapes [[206](#bib.bib206)] |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| Propose-Reduce [[244](#bib.bib244)] | ICCV | Instance | Mask R-CNN |  | Propose
    and Reduce | DAVIS[17] [[81](#bib.bib81)]/YouTube-VIS [[82](#bib.bib82)] |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| CrossVIS [[245](#bib.bib245)] | ICCV | Instance | FCN |  | Dynamic Convolution
    | YouTube-VIS [[82](#bib.bib82)]/OVIS [[246](#bib.bib246)] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: 3.1.4 Language-guided${}_{\!}$ Video${}_{\!}$ Object${}_{\!}$ Segmentation${}_{\!}$
    (LVOS)${}_{\!\!\!}$
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LVOS is an emerging area, dating back to 2018 [[196](#bib.bib196), [197](#bib.bib197)].
    Although there have already existed some efforts [[247](#bib.bib247)] in the intersection
    of language and video understanding, none of them addresses pixel-level video-language
    reasoning. Most efforts in LVOS are made around the theme of efficient alignment
    between visual and linguistic modalities. According to the multi-modal information
    fusion strategy, existing models can be divided into three groups.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Dynamic Convolution-based Methods. The first initiate was proposed
    in​ [[196](#bib.bib196)] that applies dynamic networks​ [[248](#bib.bib248)] for
    visual-language relation modeling. Specifically, convolution filters, dynamically
    generated from linguistic query, are used to adaptively transform visual features
    into desired segments. In the same line of work, [[199](#bib.bib199), [200](#bib.bib200)]
    incorporate spatial context into filter generation. However, as indicated by [[198](#bib.bib198)],
    linguistic variation of input description may greatly impact sentence representation
    and subsequently make dynamic filters unstable, causing inaccurate segmentation.
    For example, “car in blue is parked on the grass” and “blue car standing on the
    grass” have the same meaning but different generated filters, leading to poor
    performance.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Capsule Routing-based Methods. In [[201](#bib.bib201)], both video
    and textual inputs are encoded through capsules [[249](#bib.bib249)], which are
    considered effective in modeling visual/textual entities. Then, dynamic routing
    is applied over the video and text capsules for visual-textual information integration.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Attention-based Methods. Neural attention technique is
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: also widely adopted in the filed of LVOS${}_{\!}$ [[197](#bib.bib197), [250](#bib.bib250),
    [202](#bib.bib202), [204](#bib.bib204), [251](#bib.bib251)], for fully capturing
    global visual/textual context. In [[198](#bib.bib198)], vision-guided language
    attention and language-guided vision attention were developed to capture visual-textual
    correlations. In [[203](#bib.bib203)], two different attentions are learned to
    ground spatial and temporal relevant linguistic cues to static and dynamic visual
    embeddings, respectively.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Deep Learning-based VSS Models
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Video semantic segmentation aims to group pixels with different semantics (*e.g*.,
    category or instance membership), where different semantics result in different
    types of segmentation tasks, such as (instance-agnostic) video semantic segmentation
    (VSS, §[3.2.1](#S3.SS2.SSS1 "3.2.1 (Instance-agnostic) Video Semantic Segmentation
    (VSS) ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")), video instance
    segmentation (VIS, §[3.2.2](#S3.SS2.SSS2 "3.2.2 Video Instance Segmentation (VIS)
    ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")) and video panoptic
    segmentation (VPS, §[3.2.3](#S3.SS2.SSS3 "3.2.3 Video Panoptic Segmentation (VPS)
    ‣ 3.2 Deep Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation
    ‣ A Survey on Deep Learning Technique for Video Segmentation")).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 ${}_{\!\!\!}$(Instance-agnostic)${}_{\!}$ Video${}_{\!}$ Semantic${}_{\!}$
    Segmentation${}_{\!}$ (VSS)${}_{\!\!\!\!\!\!\!}$
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extending the success of deep learning-based image semantic segmentation techniques
    to the video domain has become a research focus in computer vision recently. To
    achieve this, the most straightforward strategy is the naïve application of an
    image semantic segmentation model in a frame-by-frame manner. But this strategy
    completely ignores temporal continuity and coherence cues provided in videos.
    To make better use of temporal information, research efforts in this field are
    mainly made along two lines.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Efforts towards More Accurate Segmentation. A major stream of methods
    exploits cross-frame relations to boost the prediction accuracy. They typically
    first apply the very same segmentation algorithms to each frame independently.
    Then they add extra modules on top, *e.g*., optical flow-guided feature aggregation [[213](#bib.bib213),
    [212](#bib.bib212), [219](#bib.bib219)], and sequential network based temporal
    information propagation [[216](#bib.bib216)], to gather multi-frame context and
    get better results. For example, in some pioneer work [[209](#bib.bib209), [207](#bib.bib207)],
    after performing static semantic segmentation for each frame individually, optical${}_{\!}$
    flow${}_{\!}$ [[209](#bib.bib209)]${}_{\!}$ or${}_{\!}$ 3D${}_{\!}$ CRF${}_{\!}$ [[207](#bib.bib207)]${}_{\!}$
    based${}_{\!}$ post${}_{\!}$ processing is applied for gaining temporally consistent
    segments. Later, [[220](#bib.bib220)] jointly learns CNN-based per-frame segmentation
    and CRF-based spatio-temporal reasoning. In [[213](#bib.bib213)], features warped
    from previous frames with optical flow are combined with the current frame features
    for prediction. These methods require additional feature aggregation modules,
    which increase the computational costs during the inference. Recently, [[223](#bib.bib223)]
    proposes to only incorporate flow-guided temporal consistency into the training
    phase, without bringing any extra inference cost. But its processing speed is
    still bounded to the adopted per-frame segmentation algorithms, as all features
    must be recomputed at each frame. For these methods, the utility in time-sensitive
    application areas, such as mobile and autonomous driving, is limited.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Efforts towards Faster Segmentation. Yet another complementary line
    of work tries to leverage temporal information to accelerate computation. They
    approximate the expensive per-frame forward pass with cheaper alternatives, *i.e*.,
    reusing the features in neighbouring frames. In [[205](#bib.bib205)], parts of
    segmentation networks are adaptively executed across frames, thus reducing the
    computation cost. Later methods use keyframes to avoid processing of each frame,
    and then propagate the outputs or the feature maps to other frames. For instance, [[214](#bib.bib214)]
    employs optical flow to warp the features between the keyframe and non-key frames.
    Adaptive keyframe selection is later exploited in [[211](#bib.bib211), [218](#bib.bib218)],
    further enhanced by adaptive feature propagation [[217](#bib.bib217)]. In [[221](#bib.bib221)],
    Jain *et al*. use a large, strong model to predict the keyframe and use a compact
    one in non-key frames. Keyframe-based methods have different computational loads
    between keyframes and non-key frames, causing high maximum latency and unbalanced
    occupation of computation resources that may decrease system efficiency [[224](#bib.bib224)].
    Additionally, the spatial misalignment of other frames with respect to the keyframes
    is challenging to compensate for and often leads to different quantity results
    between keyframes and non-key frames. In [[231](#bib.bib231)], a temporal consistency
    guided knowledge distillation technique is proposed to train a compact network,
    which is applied to all frames. In [[224](#bib.bib224)], several weight-sharing
    sub-networks are distributed over sequential frames, whose extracted shallow features
    are composed for final segmentation. This trend of methods indeed speeds up inference,
    but still with the cost of reduced accuracy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Semi-/Weakly-supervised based Methods. Away from these main battlefields,
    some researchers made efforts to learn VSS under annotation efficient settings.
    In [[215](#bib.bib215)], classifier heatmaps are used to learn VSS from image
    tags only. [[222](#bib.bib222), [234](#bib.bib234)] use both labeled and unlabeled
    video frames to learn VSS. They propagate annotations from labeled frames to other
    unlabeled, neighboring frames [[222](#bib.bib222)], or alternatively train teacher
    and student networks with groundtruth annotations and iteratively generated pseudo
    labels [[234](#bib.bib234)].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Video Instance Segmentation (VIS)
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2019, Yang *et al*. extended image instance segmentation to the video domain [[82](#bib.bib82)],
    which requires simultaneous detection, segmentation and tracking of instances
    in videos. This task is also known as multi-object tracking and segmentation (MOTS) [[210](#bib.bib210)].
    Based on the patterns of generating instance sequences, existing frameworks can
    be roughly categorized into four paradigms: i) track-detect, ii) clip-match, iii)
    propose-reduce, iv) segment-as-a-whole. Track-detect methods detect and segment
    instances for each individual frame, followed by frame-by-frame instance tracking [[82](#bib.bib82),
    [210](#bib.bib210), [235](#bib.bib235), [232](#bib.bib232), [230](#bib.bib230),
    [237](#bib.bib237), [252](#bib.bib252), [228](#bib.bib228), [236](#bib.bib236)].
    For example, in [[82](#bib.bib82), [210](#bib.bib210), [253](#bib.bib253)], Mask
    R-CNN [[180](#bib.bib180)] is adapted for VIS/MOTS by adding a tracking branch
    for cross-frame instance association. Alternatively, [[237](#bib.bib237)] models
    spatial attention to describe instances, tackling the task from a novel single-stage
    yet elegant perspective. Clip-match methods divide an entire video into multiple
    overlapped clips, and perform VIS independently for each clip through mask propagation [[226](#bib.bib226)]
    or spatial-temporal embedding [[233](#bib.bib233)]. Final instance sequences are
    generated by merging neighboring clips. Both of the two paradigms need two independent
    steps to generate a complete sequence. They both generate multiple incomplete
    sequences (*i.e*., frames or clips) from a video, and merge (or complete) them
    by tracking/matching at the second stage. Intuitively, these paradigms are vulnerable
    to error accumulation in the process of merging sequences, especially when occlusion
    or fast motion exists. To address these limitations, a propose-reduce paradigm
    is proposed in [[244](#bib.bib244)]. It first samples several key frames and obtains
    instance sequences by propagating the instance segmentation results from each
    key frame to the entire video. Then, the redundant sequence proposals of the same
    instances are removed. This paradigm not only discards the step of merging incomplete
    sequences, but also achieves robust results considering multiple key frames. However,
    these three types of methods still need complex heuristic rules to associate instances
    and/or multiple steps to generate instance sequences. The segment-as-a-whole paradigm [[238](#bib.bib238)]
    is more elegant; it poses the task as a direct sequence prediction problem using
    Transformer [[192](#bib.bib192)].'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Almost all VIS models are built upon fully supervised learning, while [[242](#bib.bib242),
    [243](#bib.bib243)] are the exceptions. Specifically, in [[242](#bib.bib242)],
    motion and temporal consistency cues are leveraged to generate pseudo-labels from
    tag labeled videos for weakly supervised VIS learning. In [[243](#bib.bib243)],
    a semi-supervised embedding learning approach is proposed to learn VIS from pixel-wise
    annotated images and unlabeled videos.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Video Panoptic Segmentation (VPS)
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Very recently, Kim *et al*. extended image panoptic segmentation to the video
    domain [[227](#bib.bib227)], which aims at a holistic segmentation of all foreground
    instance tracklets and background regions, and assigning a semantic label to each
    video pixel. They adapt an image panoptic segmentation model [[254](#bib.bib254)]
    for VPS, by adding two modules for temporal feature fusion and cross-frame instance
    association, respectively. Later, temporal correspondence was explored in [[240](#bib.bib240)]
    through learning coarse segment-level and fine pixel-level matching. Qiao *et
    al*. [[241](#bib.bib241)] propose to learn monocular depth estimation and video
    panoptic segmentation jointly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d478b97d2d5c97072cc59b470d19496b.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: <svg version="1.1" width="713.07" height="73.34" overflow="visible"><g transform="translate(0,73.34)
    scale(1,-1)"><g transform="translate(-341.77,121.07)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-size="50%">Youtube-Objects <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[73](#bib.bib73)]</foreignobject></g></text></g><g
    transform="translate(-265.67,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">FBMS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{59}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[36](#bib.bib36)]</foreignobject></g></text></g><g
    transform="translate(-204.1,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{16}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[17](#bib.bib17)]</foreignobject></g></text></g><g
    transform="translate(-130.07,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">${}_{17}\!$</foreignobject></g> <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[81](#bib.bib81)]</foreignobject></g></text></g><g
    transform="translate(-65.03,121.07)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VOS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[95](#bib.bib95)]</foreignobject></g></text></g><g
    transform="translate(-339.01,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">A2D Sentence <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-265.67,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">J-HMDB-S <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[196](#bib.bib196)]</foreignobject></g></text></g><g
    transform="translate(-207.56,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">DAVIS<g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[17]</foreignobject></g>-RVOS <g
    transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject width="1.383700013837"
    height="1.383700013837" overflow="visible">[[197](#bib.bib197)]</foreignobject></g></text></g><g
    transform="translate(-134.22,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">Refer-Youtube-VOS​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[202](#bib.bib202)]</foreignobject></g></text></g><g
    transform="translate(-49.81,76.1)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CamVid <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[208](#bib.bib208)]</foreignobject></g></text></g><g
    transform="translate(-337.62,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">CityScapes <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[206](#bib.bib206)]</foreignobject></g></text></g><g
    transform="translate(-278.12,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">NYUDv2​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[225](#bib.bib225)]</foreignobject></g></text></g><g
    transform="translate(-220.01,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">VSPW <g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[255](#bib.bib255)]</foreignobject></g></text></g><g
    transform="translate(-167.43,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">YouTube-VIS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[82](#bib.bib82)]</foreignobject></g></text></g><g
    transform="translate(-83.02,41.51)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">KITTI MOTS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]</foreignobject></g></text></g><g
    transform="translate(-345.93,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">MOTSChallenge​ <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[210](#bib.bib210)]</foreignobject></g></text></g><g
    transform="translate(-267.75,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">BDD100K <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[229](#bib.bib229)]</foreignobject></g></text></g><g
    transform="translate(-193.72,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">OVIS <g transform="translate(0,1.383700013837) scale(1,
    -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[246](#bib.bib246)]</foreignobject></g></text></g><g
    transform="translate(-135.6,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">VIPER-VPS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[227](#bib.bib227)]</foreignobject></g></text></g><g
    transform="translate(-69.19,2.63)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="50%">Cityscapes-VPS <g transform="translate(0,1.383700013837)
    scale(1, -1)"><foreignobject width="1.383700013837" height="1.383700013837" overflow="visible">[[227](#bib.bib227)]</foreignobject></g></text></g></g></svg>
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Example frames from twenty famous video segmentation benchmark datasets.
    The ground-truth segmentation annotation is overlaid.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Statistics of representative video segmentation datasets. See §[4.1](#S4.SS1
    "4.1 VOS Datasets ‣ 4 Video Segmentation Datasets ‣ A Survey on Deep Learning
    Technique for Video Segmentation") and §[4.2](#S4.SS2 "4.2 VSS Datasets ‣ 4 Video
    Segmentation Datasets ‣ A Survey on Deep Learning Technique for Video Segmentation")
    for more detailed descriptions.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '|   Dataset | Year | Pub. | #Video | #Train/Val/Test/Dev | Annotation | Purpose
    | #Class | Synthetic |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Youtube-Objects [[73](#bib.bib73)] | 2012 | CVPR | 1,407 (126) | -/-/-/-
    | Object-level AVOS, SVOS | Generic | 10 |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| FBMS[59] ​  [[36](#bib.bib36)] | 2014 | PAMI | 59 | 29/30/-/- | Object-level
    AVOS, SVOS | Generic | - |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| DAVIS[16] ​  [[17](#bib.bib17)] | 2016 | CVPR | 50 | 30/20/-/- | Object-level
    AVOS, SVOS | Generic | - |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| DAVIS[17] [[81](#bib.bib81)] | 2017 | - | 150 | 60/30/30/30 | Instance-level
    AVOS, SVOS, IVOS | Generic | - |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| YouTube-VOS [[95](#bib.bib95)] | 2018 | - | 4,519 | 3,471/507/541/- | SVOS
    | Generic | 94 |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| A2D Sentence [[196](#bib.bib196)] | 2018 | CVPR | 3,782 | 3,017/737/-/- |
    LVOS | Human-centric | - |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| J-HMDB Sentence [[196](#bib.bib196)] | 2018 | CVPR | 928 | -/-/-/- | LVOS
    | Human-centric | - |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| DAVIS[17]-RVOS [[197](#bib.bib197)] | 2018 | ACCV | 90 | 60/30/-/- | LVOS
    | Generic | - |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| Refer-Youtube-VOS [[202](#bib.bib202)] | 2020 | ECCV | 3,975 | 3,471/507/-/-
    | LVOS | Generic | - |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| CamVid [[208](#bib.bib208)] | 2009 | PRL | 4 | (frame: 467/100/233/-) | VSS
    | Urban | 11 |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| CityScapes [[206](#bib.bib206)] | 2016 | CVPR | 5,000 | 2,975/500/1,525 |
    VSS | Urban | 19 |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| NYUDv2 [[225](#bib.bib225)] | 2012 | ECCV | 518 | (frame: 795/654/-/-) |
    VSS | Indoor | 40 |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| VSPW [[255](#bib.bib255)] | 2021 | CVPR | 3,536 | 2,806/343/387/- | VSS |
    Generic | 124 |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| YouTube-VIS [[82](#bib.bib82)] | 2019 | ICCV | 3,859 | 2,985/421/453/- |
    VIS | Generic | 40 |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| KITTI MOTS [[210](#bib.bib210)] | 2019 | CVPR | 21 | 12/9/-/- | VIS | Urban
    | 2 |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| MOTSChallenge [[210](#bib.bib210)] | 2019 | CVPR | 4 | -/-/-/- | VIS | Urban
    | 1 |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| BDD100K [[229](#bib.bib229)] | 2020 | ECCV | 100,000 | 7,000/1,000/2,000/-
    | VSS, VIS | Driving | 40 (VSS), 8 (VIS) |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| OVIS [[246](#bib.bib246)] | 2021 | - | 901 | 607/140/154/- | VIS | Generic
    | 25 |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| VIPER-VPS [[227](#bib.bib227)] | 2020 | CVPR | 124 | (frame: 134K/50K/70K/-)
    | VPS | Urban | 23 | ✓ |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| Cityscapes-VPS [[227](#bib.bib227)] | 2020 | CVPR | 500 | 400/100/-/- | VPS
    | Urban | 19 |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: 4 Video Segmentation Datasets
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several datasets have been proposed for video segmentation over the past decades.
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep
    Learning-based VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey
    on Deep Learning Technique for Video Segmentation") shows example frames from
    twenty commonly used datasets. We summarize their essential features in Table [VI](#S3.T6
    "TABLE VI ‣ 3.2.3 Video Panoptic Segmentation (VPS) ‣ 3.2 Deep Learning-based
    VSS Models ‣ 3 Deep Learning-based Video Segmentation ‣ A Survey on Deep Learning
    Technique for Video Segmentation") and give detailed review below.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 VOS Datasets
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 AVOS/SVOS/IVOS Datasets
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: $\bullet$ Youtube-Objects is a large dataset of $1,\!407$ videos collected from
    155 web videos belonging to 10 object categories (*e.g*., dog, cat, plane, *etc*.).
    VOS models typically test the generalization ability on a subset [[256](#bib.bib256)]
    having totally 126 shots with $20,\!647$ frames that provides coarse pixel-level
    fore-/background annotations on every 10^(th) frames.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ FBMS[59]​ [[36](#bib.bib36)] consists of 59 video sequences with $13,\!860$
    frames in total. However, only 720 frames are annotated for fore-/background separation.
    The dataset is split into 29 and 30 sequences for training and evaluation, respectively.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ DAVIS[16]​ [[17](#bib.bib17)] has 50 videos (30 for train set and
    20 for val set) with $3,\!455$ frames in total. For each frame, in addition to
    high-quality fore-/background segmentation annotation, a set of attributes (*e.g*.,
    deformation, occlusion, motion blur, *etc*.) are also provided to highlight the
    main challenges.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ DAVIS[17]​ [[81](#bib.bib81)] contains 150 videos, *i.e*., 60/30/30/30
    videos for train/val/test-dev/test-challenge sets. Its train and val sets are
    extended from the respective sets in DAVIS[16]. There are 10,459 frames in total.
    DAVIS[17] provides instance-level annotations to support SVOS. Then, DAVIS[18]
    challenge​ [[194](#bib.bib194)] provides scribble annotations to support IVOS.
    Moreover, as the original annotations of DAVIS[17] are biased towards the SVOS
    scenario, DAVIS[19] challenge​ [[177](#bib.bib177)] re-annotates val and test-dev
    sets of DAVIS[17] to support AVOS.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ YouTube-VOS​ [[95](#bib.bib95)] is a large-scale dataset, which is
    split into a train ($3,\!471$ videos), val (507 videos), and test (541 videos)
    set, in its newest 2019 version. Instance-level precise annotations are provided
    every five frames in a 30FPS frame rate. There are 94 object categories (*e.g*.,
    person, snake, *etc*.) in total, of which 26 are unseen in train set.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Remark. Youtube-Objects, FBMS[59] and DAVIS[16] are used
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: for instance-agnostic AVOS and SVOS evaluation. DAVIS[17] is unique in comprehensive
    annotations for instance-level AVOS, SVOS as well as IVOS, but its scale is relatively
    small. YouTube-VOS is the largest one but only supports SVOS benchmarking. There
    also exist some other VOS datasets, such as SegTrack[V1] [[45](#bib.bib45)] and
    SegTrack[V2] [[76](#bib.bib76)], but they were less used recently, due to the
    limited scale and difficulty.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 LVOS Datasets
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: $\bullet$ A2D Sentence​ [[196](#bib.bib196)] augments A2D [[257](#bib.bib257)]
    with phrases. It contains $3,\!782$ videos, with $8$ action classes performed
    by $7$ actors. In each video, $3$ to $5$ frames are provided with segmentation
    masks. It contains $6,\!655$ sentences describing actors and their actions. The
    dataset is split into $3,\!017$/$737$ for train/test, and $28$ unlabeled videos
    are ignored [[198](#bib.bib198)].
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ J-HMDB Sentence​ [[196](#bib.bib196)] is built upon J-HMDB [[258](#bib.bib258)].
    It is comprised of $928$ short videos with $928$ corresponding sentences describing
    $21$ different action categories.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ DAVIS[17]-RVOS​ [[197](#bib.bib197)] extends DAVIS[17] by collecting
    referring expressions for the annotated objects. 90 videos from train and val
    sets are annotated with more than 1,500 referring expressions. They provide two
    types of annotations, which describe the highlighted object: 1) based on the entire
    video (*i.e*., full-video expression) and 2) using only the first frame of the
    video (*i.e*., first-frame expression).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Refer-Youtube-VOS​ [[202](#bib.bib202)] includes 3,975 videos from
    YouTube-VOS​ [[95](#bib.bib95)], with 27,899 language descriptions of target objects.
    Similar to DAVIS[17]-RVOS [[197](#bib.bib197)], both full-video and first-frame
    expression annotations are provided.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Remark. To date, A2D Sentence and J-HMDB Sentence are the main test-beds. However,
    the phrases are not produced with the aim of reference, but description, and limited
    to only a few object categories corresponding to the dominant ‘actors’ performing
    a salient ‘action’ [[202](#bib.bib202)]. But newly introduced DAVIS[17]-RVOS and
    Refer-Youtube-VOS show improved difficulties in both visual and linguistic modalities.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 VSS Datasets
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ CamVid​ [[208](#bib.bib208)] is composed of 4 urban scene videos with
    11-class pixelwise annotations. Each video is annotated every 30 frames. The annotated
    frames are usually grouped into 467/100/233 for train/val/test [[207](#bib.bib207)].
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ CityScapes${}_{\!~{}}$ [[206](#bib.bib206)]${}_{\!~{}}$ is${}_{\!~{}}$
    a${}_{\!~{}}$ large-scale${}_{\!~{}}$ VSS${}_{\!~{}}$ dataset${}_{\!~{}}$ for${}_{\!~{}}$
    street
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: views. It has 2,975/500/1,525 snippets for train/val/ test, captured at 17FPS.
    Each snippet contains 30 frames, and only the 20^(th) frame is densely labelled
    with 19 semantic classes. 20,000 coarsely annotated frames are also provided.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ NYUDv2​ [[225](#bib.bib225)] contains 518 indoor RGB-D videos with
    high-quality ground-truths (every 10^(th) video frame is labeled). There are 795
    training frames and 654 testing frames being rectified and annotated with 40-class
    semantic labels.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ VSPW​ [[255](#bib.bib255)] is a recently proposed large-scale VSS
    dataset. It addresses video scene parsing in the wild by considering diverse scenarios.
    It consists of 3,536 videos, and provides pixel-level annotations for 124 categories
    at 15FPS. The train/val/test sets contain 2,806/343/387 videos with 198,244/24,502/28,887
    frames, respectively.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ YouTube-VIS​ [[82](#bib.bib82)] is built upon YouTube-VOS [[95](#bib.bib95)]
    with instance-level annotations. Its newest 2021 version has 3,859 videos (2,985/421/453
    for train/val/test) with 40 semantic categories. It provides 232K high-quality
    annotations for 8,171 unique video instances.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ KITTI MOTS​ [[210](#bib.bib210)] extends the 21 training sequences
    of KITTI tracking dataset [[259](#bib.bib259)] with VIS annotations – 12 for training
    and 9 for validation, respectively. The dataset contains 8,008 frames with a resolution
    of $375\times 1242$, 26,899 annotated cars and 11,420 annotated pedestrians.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ MOTSChallenge​ [[210](#bib.bib210)] annotates 4 of 7 training sequences
    of MOTChallenge[2017] [[260](#bib.bib260)]. It has 2,862 frames with 26,894 annotated
    pedestrians and presents many occlusion cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ BDD100K​ [[229](#bib.bib229)] is a large-scale dataset with 100K driving
    videos (40 seconds and 30FPS each) and supports various tasks, including VSS and
    VIS. For VSS, 7,000/1,000/2,000 frames are densely labelled with 40 semantic classes
    for train/val/test. For VIS, 90 videos with 8 semantic categories are annotated
    by 129K instance masks – 60 training videos, 10 validation videos, and 20 testing
    videos.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ OVIS​ [[246](#bib.bib246)] is a new challenging VIS dataset, where
    object occlusions usually occur. It has 901 videos and 296K high-quality instance
    masks for 25 semantic categories. It is split into 607 training, 140 validation
    and 154 test videos.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ VIPER-VPS​ [[227](#bib.bib227)] re-organizes VIPER [[261](#bib.bib261)]
    into the video panoptic format. VIPER, extracted from the GTA-V game engine, has
    annotations of semantic and instance segmentations for 10 thing and 13 stuff classes
    on 254K frames of ego-centric driving scenes at $1080\!\times\!1920$ resolution.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Cityscapes-VPS​ [[227](#bib.bib227)] is built upon CityScapes​ [[206](#bib.bib206)].
    Dense panoptic annotations for 8 thing and 11 stuff classes for 500 snippets in
    Cityscapes val set are provided every five frames and temporally consistent instance
    ids to the thing objects are also given, leading to 3000 annotated frames in total.
    These videos are split into 400/100 for train/val.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Remark. CamVid, CityScapes, NYUDv2, and VSPW are built for VSS benchmarking.
    YouTube-VIS, OVIS, KITTI MOTS, and MOTSChallenge are VIS datasets, but the diversity
    of the last two are limited. BDD100K has both VSS and VIS annotations. VIPER-VPS
    and Cityscapes-VPS are aware of VPS evaluation, but VIPER-VPS is a synthesized
    dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 5 Performance Comparison
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next we tabulate the performance of previously discussed algorithms. For each
    of the reviewed fields, the most widely used dataset is selected for performance
    benchmarking. The performance scores are gathered from the original articles,
    unless specified. For the running speed, we obtain the FPS for most methods by
    running their codes on a RTX 2080Ti GPU. For a small set of methods whose implementations
    are not well organized or publicly available, we directly borrow the values from
    the corresponding papers. Despite this, it is essential to remark the difficulty
    when comparing runtime. As different methods are with different code bases and
    levels of optimization, it is hard to make completely fair runtime comparison [[262](#bib.bib262),
    [17](#bib.bib17), [63](#bib.bib63)]; the values are only provided for reference.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Object-level AVOS Performance Benchmarking
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Evaluation Metrics
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Presently, three metrics are frequently used [[17](#bib.bib17)] to measure
    how object-level AVOS methods perform on this task:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Quantitative object-level AVOS results on DAVIS[16] [[17](#bib.bib17)]
    val (§[5.1.2](#S5.SS1.SSS2 "5.1.2 Results ‣ 5.1 Object-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")) in terms of region similarity $\mathcal{J}$, boundary
    accuracy $\mathcal{F}$ and time stability $\mathcal{T}$. We also report the recall
    and the decay performance over time for both $\mathcal{J}$ and $\mathcal{F}$.
    (FPS denotes frames per second. ^†: FPS is borrowed from the original paper. The
    three best scores are marked in red, blue, and green, respectively. These notes
    also apply to the other tables.)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '|   | $\mathcal{J}$ | $\mathcal{F}$ | $\mathcal{T}$ |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Method | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | mean$\uparrow$
    | recall$\uparrow$ | decay$\downarrow$ | mean$\downarrow$ | FPS$\uparrow$ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| MuG [[98](#bib.bib98)] | 58.0 | 65.3 | 2.0 | 51.5 | 53.2 | 2.1 | 30.1 | 2.5
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| SFL [[68](#bib.bib68)] | 67.4 | 81.4 | 6.2 | 66.7 | 77.1 | 5.1 | 28.2 | 3.3
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| MotionGrouping [[109](#bib.bib109)] | 68.3 | - | - | 67.6 | - | - | - | 83.3
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| LVO [[69](#bib.bib69)] | 75.9 | 89.1 | 0.0 | 72.1 | 83.4 | 1.3 | 26.5 | 13.5
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| LMP [[70](#bib.bib70)] | 70.0 | 85.0 | 1.3 | 65.9 | 79.2 | 2.5 | 57.2 | 18.3
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| FSEG [[66](#bib.bib66)] | 70.7 | 83.0 | 1.5 | 65.3 | 73.8 | 1.8 | 32.8 |
    7.2 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| PDB [[78](#bib.bib78)] | 77.2 | 93.1 | 0.9 | 74.5 | 84.4 | -0.2 | 29.1 |
    1.4 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| MOT [[79](#bib.bib79)] | 77.2 | 87.8 | 5.0 | 77.4 | 84.4 | 3.3 | 27.9 | 1.0
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| LSMO [[93](#bib.bib93)] | 78.2 | 91.1 | 4.1 | 75.9 | 84.7 | 3.5 | 21.2 |
    0.4 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| IST [[74](#bib.bib74)] | 78.5 | - | - | 75.5 | - | - | - | - |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| AGS [[87](#bib.bib87)] | 79.7 | 89.1 | 1.9 | 77.4 | 85.8 | 0.0 | 26.7 | 1.7
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| MBN [[77](#bib.bib77)] | 80.4 | 93.2 | 4.8 | 78.5 | 88.6 | 4.4 | 27.8 | 1.0
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| COSNet [[83](#bib.bib83)] | 80.5 | 93.1 | 4.4 | 79.4 | 89.5 | 5.0 | 18.4
    | 2.2 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| AGNN [[89](#bib.bib89)] | 81.3 | 93.1 | 0.0 | 79.7 | 88.5 | 5.1 | 33.7 |
    1.9 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| MGA [[90](#bib.bib90)] | 81.4 | - | - | 81.0 | - | - | - | 1.1 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| AnDiff [[92](#bib.bib92)] | 81.7 | 90.9 | 2.2 | 80.5 | 85.1 | 0.6 | 21.4
    | 2.8 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| PyramidCSA [[96](#bib.bib96)] | 78.1 | 90.1 | - | 78.5 | 88.2 | - | - | ^†110
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| WCSNet [[101](#bib.bib101)] | 82.2 | - | - | 80.7 | - | - | - | 25 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| MATNet [[94](#bib.bib94)] | 82.4 | 94.5 | 5.5 | 80.7 | 90.2 | 4.5 | 21.6
    | 1.3 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| EGMN [[100](#bib.bib100)] | 82.5 | 94.3 | 4.2 | 81.2 | 90.3 | 5.6 | 19.8
    | 5.0 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| DFNet [[104](#bib.bib104)] | 83.4 | - | - | 81.8 | - | - | 15.9 | ^†3.6 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| F2Net [[105](#bib.bib105)] | 83.1 | 95.7 | 0.0 | 84.4 | 92.3 | 0.8 | 20.9
    | ^†10 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| RTNet [[107](#bib.bib107)] | 85.6 | 96.1 | 0.5 | 84.7 | 93.8 | 0.9 | 19.9
    | 6.7 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: 'TABLE 8: Quantitative instance-level AVOS results on DAVIS[17] [[81](#bib.bib81)]
    val (§[5.2.2](#S5.SS2.SSS2 "5.2.2 Results ‣ 5.2 Instance-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")) in terms of region similarity $\mathcal{J}$ and boundary
    accuracy $\mathcal{F}$.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '|   | $\mathcal{J}\!\&amp;\mathcal{F}$ | $\mathcal{J}$ | $\mathcal{F}$ |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Method | mean$\uparrow$ | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$
    | mean$\uparrow$ | recall$\uparrow$ | decay$\downarrow$ | FPS$\uparrow$ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| PDB [[78](#bib.bib78)] | 55.1 | 53.2 | 58.9 | 4.9 | 57.0 | 60.2 | 6.8 | 0.7
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| RVOS [[80](#bib.bib80)] | 41.2 | 36.8 | 40.2 | 0.5 | 45.7 | 46.4 | 1.7 |
    14.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| AGS [[87](#bib.bib87)] | 57.5 | 55.5 | 61.6 | 7.0 | 59.5 | 62.8 | 9.0 | 1.1
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| AGNN [[175](#bib.bib175)] | 61.1 | 58.9 | 65.7 | 11.7 | 63.2 | 67.1 | 14.3
    | 0.9 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| STEm-Seg [[233](#bib.bib233)] | 64.7 | 61.5 | 70.4 | -4.0 | 67.8 | 75.5 |
    1.2 | 9.3 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| UnOVOST [[178](#bib.bib178)] | 67.9 | 66.4 | 76.4 | -0.2 | 69.3 | 76.9 |
    0.0 | ^†1.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| TODA [[106](#bib.bib106)] | 65.0 | 63.7 | 71.9 | 6.9 | 66.2 | 73.1 | 9.4
    | 9.1 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Region Jaccard $\mathcal{J}$ is calculated by the intersection-over-union
    (IoU) between the segmentation results ${\hat{Y}}\!\in\!\{0,1\}^{w\times h}$ and
    the ground-truth ${Y}\!\in\!\{0,1\}^{w\times h}$: $\mathcal{J}={|\hat{Y}\cap Y}|/|{\hat{Y}\cup
    Y|}$, which computes the number of pixels of the intersection between $\hat{Y}$
    and ${Y}$, and divides it by the size of the union.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Boundary Accuracy $\mathcal{F}$ is the harmonic mean of the boundary
    precision $\text{P}_{c}$ and recall $\text{R}_{c}$. The value of $\mathcal{F}$
    reflects how well the segment contours $c(\hat{Y})$ match the ground-truth contours
    $c(Y)$. Usually, the value of $\text{P}_{c}$ and $\text{R}_{c}$ can be computed
    via bipartite graph matching [[263](#bib.bib263)], then the boundary accuracy
    $\mathcal{F}$ can be computed as: $\mathcal{F}={2\text{P}_{c}\text{R}_{c}}/({\text{P}_{c}+\text{R}_{c}})$.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Temporal Stability $\mathcal{T}$ is informative of the stability of
    segments. It is computed as the pixel-level cost of matching two successive segmentation
    boundaries. The match is achieved by minimizing the shape context descriptor [[264](#bib.bib264)]
    distances between matched points while preserving the order in which the points
    are present in the boundary polygon. Note that $\mathcal{T}$ will compensate motion
    and small deformations, but not penalize inaccuracies of the contours [[17](#bib.bib17)].
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 10: Quantitative IVOS results on DAVIS[17] [[81](#bib.bib81)] val (§[5.4.2](#S5.SS4.SSS2
    "5.4.2 Results ‣ 5.4 IVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation")) in terms of AUC
    and $\mathcal{J}$@60.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | AUC $\uparrow$ | $\mathcal{J}$@60 $\uparrow$ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| IVS [[183](#bib.bib183)] | 69.1 | 73.4 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| MANet [[184](#bib.bib184)] | 74.9 | 76.1 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| IVOS-W [[187](#bib.bib187)] | 74.1 | - |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| ATNet [[185](#bib.bib185)] | 77.1 | 79.0 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| GIS [[188](#bib.bib188)] | 82.0 | 82.9 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| MiVOS [[189](#bib.bib189)] | 84.9 | 85.4 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: 'TABLE 11: Quantitative LVOS results on A2D Sentence [[196](#bib.bib196)] test
    (§[5.5.2](#S5.SS5.SSS2 "5.5.2 Results ‣ 5.5 LVOS Performance Benchmarking ‣ 5
    Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: in terms of Precision@$K$, mAP and IoU.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Overlap | mAP$\uparrow$ | IoU |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Method | P@0.5$\uparrow$ | P@0.6$\uparrow$ | P@0.7$\uparrow$ | P@0.8$\uparrow$
    | P@0.9$\uparrow$ | 0.5:0.95 | overall$\uparrow$ | mean$\uparrow$ | FPS$\uparrow$
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| A2DS [[196](#bib.bib196)] | 50.0 | 37.6 | 23.1 | 9.4 | 0.4 | 21.5 | 55.1
    | 42.6 | - |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| CMSANet [[204](#bib.bib204)] | 46.7 | 38.5 | 27.9 | 13.6 | 1.7 | 25.3 | 61.8
    | 43.2 | 6.5 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| AAN [[198](#bib.bib198)] | 55.7 | 45.9 | 31.9 | 16.0 | 2.0 | 27.4 | 60.1
    | 49.0 | 8.6 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| VT-Capsule [[201](#bib.bib201)] | 52.6 | 45.0 | 34.5 | 20.7 | 3.6 | 30.3
    | 56.8 | 46.0 | - |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| CDNet [[199](#bib.bib199)] | 60.7 | 52.5 | 40.5 | 23.5 | 4.5 | 33.3 | 62.3
    | 53.1 | 7.2 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| PolarRPE [[200](#bib.bib200)] | 63.4 | 57.9 | 48.3 | 32.2 | 8.3 | 38.8 |
    66.1 | 52.9 | 5.4 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| CST [[203](#bib.bib203)] | 65.4 | 58.9 | 49.7 | 33.3 | 9.1 | 39.9 | 66.2
    | 56.1 | 8.1 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: 5.1.2 Results
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We select DAVIS[16]​ [[17](#bib.bib17)], the most widely used dataset in AVOS,
    for performance benchmarking. Table [VII](#S5.T7 "TABLE VII ‣ 5.1.1 Evaluation
    Metrics ‣ 5.1 Object-level AVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation") presents the results
    of those reviewed AVOS methods DAVIS[16] val set. The current best solution, RTNet [[107](#bib.bib107)],
    reaches 85.6 region similarity $\mathcal{J}$, significantly outperforming the
    earlier deep learning-based methods, such as SFL [[68](#bib.bib68)], proposed
    in 2017.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Instance-level AVOS Performance Benchmarking
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1 Evaluation Metrics
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In instance-level AVOS setting, region Jaccard $\mathcal{J}$, boundary accuracy
    $\mathcal{F}$, and $\mathcal{J}\&amp;\mathcal{F}$ – the mean of $\mathcal{J}$
    and $\mathcal{F}$ – are used for evaluation [[177](#bib.bib177)]. Each of the
    annotated object tracklets will be matched with one of predicted tracklets according
    to $\mathcal{J}\&amp;\mathcal{F}$, using bipartite graph matching. For a certain
    criterion, the final score will be computed between each ground-truth object and
    its optimal assignment.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Results
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regarding instance-level AVOS, we take into account DAVIS[17] [[81](#bib.bib81)]
    in which the vast majority of methods are evaluated. From Table [8](#S5.T8 "TABLE
    8 ‣ 5.1.1 Evaluation Metrics ‣ 5.1 Object-level AVOS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    we can find that UnOVOST [[178](#bib.bib178)] is the top scorer, with 67.9 $\mathcal{J}$
    at the time of this writing.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 SVOS Performance Benchmarking
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.3.1 Evaluation Metrics
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Region Jaccard $\mathcal{J}$, boundary accuracy $\mathcal{F}$, and $\mathcal{J}\&amp;\mathcal{F}$
    are also widely adopted for SVOS performance evaluation [[194](#bib.bib194)].
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 9: Quantitative SVOS results on DAVIS[17] [[81](#bib.bib81)] val (§[5.3.2](#S5.SS3.SSS2
    "5.3.2 Results ‣ 5.3 SVOS Performance Benchmarking ‣ 5 Performance Comparison
    ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: in terms of region similarity $\mathcal{J}$ and boundary accuracy $\mathcal{F}$.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{J}$ &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{F}$ &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '| FPS$\uparrow$ | Method |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '&#124; $\mathcal{J}\!\&amp;\mathcal{F}$ &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{J}$ &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{F}$ &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mean$\uparrow$ &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '| FPS$\uparrow$ |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| OnAVOS [[122](#bib.bib122)] | 67.9 | 64.5 | 70.5 | 0.08 | STM [[149](#bib.bib149)]
    | 81.8 | 79.2 | 84.3 | 6.3 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| OSVOS [[18](#bib.bib18)] | 60.3 | 56.7 | 63.9 | 0.22 | e-OSVOS [[153](#bib.bib153)]
    | 77.2 | 74.4 | 80.0 | 0.5 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| CINM [[125](#bib.bib125)] | 67.5 | 64.5 | 70.5 | ^†0.01 | AFB-URR [[154](#bib.bib154)]
    | 74.6 | 73.0 | 76.1 | 3.8 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| FAVOS [[126](#bib.bib126)] | 58.2 | 54.6 | 61.8 | 0.56 | Fasttan [[155](#bib.bib155)]
    | 75.9 | 72.3 | 79.4 | ^†7 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| MAST [[160](#bib.bib160)] | 65.5 | 63.3 | 67.6 | 5.1 | STM-Cycle [[166](#bib.bib166)]
    | 71.7 | 68.7 | 74.7 | 38 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| CRW [[174](#bib.bib174)] | 67.6 | 64.5 | 70.6 | 7.3 | QMA [[167](#bib.bib167)]
    | 71.9 | - | - | 6.3 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| RGMP [[127](#bib.bib127)] | 66.7 | 64.8 | 68.6 | 7.7 | Fasttmu [[156](#bib.bib156)]
    | 70.6 | 69.1 | 72.1 | 9.7 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| OSMN [[128](#bib.bib128)] | 54.8 | 52.5 | 57.1 | 7.7 | SAT [[157](#bib.bib157)]
    | 72.3 | 68.6 | 76.0 | ^†39 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| OSVOS-S [[132](#bib.bib132)] | 68.0 | 64.7 | 71.3 | 0.22 | TVOS [[159](#bib.bib159)]
    | 72.3 | 69.9 | 74.7 | ^†37 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Videomatch [[133](#bib.bib133)] | 61.4 | - | - | ^†0.38 | GCNet [[161](#bib.bib161)]
    | 71.4 | 69.3 | 73.5 | ^†25 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| Dyenet [[134](#bib.bib134)] | 69.1 | 67.3 | 71.0 | 2.4 | KMN [[162](#bib.bib162)]
    | 76.0 | 74.2 | 77.8 | 8.3 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| MVOS [[138](#bib.bib138)] | 59.2 | 56.3 | 62.1 | 1.5 | CFBI [[163](#bib.bib163)]
    | 81.9 | 79.3 | 84.5 | 2.2 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| FEELVOS [[139](#bib.bib139)] | 71.5 | 69.1 | 74.0 | 2.2 | LWL [[164](#bib.bib164)]
    | 70.8 | 68.2 | 73.5 | 15.6 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| MHP-VOS [[140](#bib.bib140)] | 75.3 | 71.8 | 78.8 | ^†0.01 | MSN [[165](#bib.bib165)]
    | 74.1 | 71.4 | 76.8 | $\dagger$10 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| AGSS [[141](#bib.bib141)] | 67.4 | 64.9 | 69.9 | ^†10 | EGMN [[100](#bib.bib100)]
    | 82.8 | 80.0 | 85.2 | 5.0 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| AGAME [[142](#bib.bib142)] | 70.0 | 67.2 | 72.7 | ^†14 | SwiftNet [[168](#bib.bib168)]
    | 81.1 | 78.3 | 83.9 | ^†25 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| SiamMask [[143](#bib.bib143)] | 56.4 | 64.3 | 58.5 | ^†35 | G-FRTM [[169](#bib.bib169)]
    | 76.4 | - | - | ^†18.2 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| RVOS [[80](#bib.bib80)] | 60.6 | 57.5 | 63.6 | 0.56 | SST [[170](#bib.bib170)]
    | 82.5 | 79.9 | 85.1 | - |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| RANet [[145](#bib.bib145)] | 65.7 | 63.2 | 68.2 | ^†30 | GIEL [[171](#bib.bib171)]
    | 82.7 | 80.2 | 85.3 | 6.7 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| DMM-Net [[147](#bib.bib147)] | 70.7 | 68.1 | 73.3 | 0.37 | LCM [[172](#bib.bib172)]
    | 83.5 | 80.5 | 86.5 | 8.5 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| DTN [[148](#bib.bib148)] | 67.4 | 64.2 | 70.6 | 14.3 | RMNet [[173](#bib.bib173)]
    | 83.5 | 81.0 | 86.0 | ^†11.9 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: 5.3.2 Results
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DAVIS[17] [[81](#bib.bib81)] is also one of the most important SVOS dataset.
    Table [9](#S5.T11 "TABLE 9 ‣ 5.3.1 Evaluation Metrics ‣ 5.3 SVOS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    shows the results of recent SVOS methods on DAVIS[17] val set. In this case, all
    the top-leading solutions, such as EGMN [[100](#bib.bib100)], LCM [[172](#bib.bib172)],
    and RMNet [[173](#bib.bib173)], are built upon the memory augmented architecture
    – STM [[149](#bib.bib149)].
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 IVOS Performance Benchmarking
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.4.1 Evaluation Metrics
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Area under the curve (AUC) and Jaccard at 60 seconds ($\mathcal{J}$@60s) are
    two widely used IVOS evaluation criteria [[194](#bib.bib194)].
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ AUC is designed to measure the overall accuracy of the evaluation.
    It is computed over the plot Time vs Jaccard. Each sample in the plot is computed
    considering the average time and the average Jaccard for a certain interaction.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ $\mathcal{J}$@60 measures the accuracy with a limited time budget
    (60 seconds). It is achieved by interpolating the Time vs Jaccard plot at 60 seconds.
    This evaluates which quality an IVOS method can obtain in 60 seconds.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Results
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DAVIS[17] [[81](#bib.bib81)] is also widely used for IVOS performance benchmarking.
    Results summarized in Table [5.1.1](#S5.SS1.SSS1 "5.1.1 Evaluation Metrics ‣ 5.1
    Object-level AVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey
    on Deep Learning Technique for Video Segmentation") show that the method proposed
    by Cheng *et al*. [[189](#bib.bib189)] is the top one.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 LVOS Performance Benchmarking
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.5.1 Evaluation Metrics
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As [[196](#bib.bib196)], overall IoU, mean IoU and precision are adopted.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 12: Quantitative VSS results on Cityscapes [[206](#bib.bib206)] val (§[5.6.2](#S5.SS6.SSS2
    "5.6.2 Results ‣ 5.6 VSS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation")) in terms of IoU${}_{\text{class}}$
    and IoU${}_{\text{category}}$ (Max Latency: maximum per-frame time cost).'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | IoU${}_{\text{class}}$$\uparrow$ | IoU${}_{\text{category}}$ $\uparrow$
    | FPS$\uparrow$ | Max Latency (ms)$\downarrow$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| Clockwork [[205](#bib.bib205)] | 66.4 | 88.6 | 6.4 | 198 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| DFF [[214](#bib.bib214)] | 69.2 | 88.9 | 5.6 | 575 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| PEARL [[212](#bib.bib212)] | 75.4 | 89.2 | 1.3 | 800 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| NetWarp [[213](#bib.bib213)] | 80.5 | 91.0 | - | - |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| DVSN [[218](#bib.bib218)] | 70.3 | - | ^†19.8 | - |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| LVS [[217](#bib.bib217)] | 76.8 | 89.8 | 5.8 | 380 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| GRFP [[216](#bib.bib216)] | 80.6 | 90.8 | 3.9 | 255 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| Accel [[221](#bib.bib221)] | 75.5 | - | ^†1.1 | - |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| VPLR [[222](#bib.bib222)] | 81.4 | - | ^†5.9 | - |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| TDNet [[224](#bib.bib224)] | 79.9 | 90.1 | 5.6 | 178 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| EFC [[223](#bib.bib223)] | 83.5 | 92.2 | ^†16.7 | - |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| Lukas [[239](#bib.bib239)] | 71.2 | - | ^†1.9 | - |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: 'TABLE 14: Quantitative VPS results on Cityscapes-VPS [[227](#bib.bib227)] (§[5.8.2](#S5.SS8.SSS2
    "5.8.2 Results ‣ 5.8 VPS Performance Benchmarking ‣ 5 Performance Comparison ‣
    A Survey on Deep Learning Technique for Video Segmentation")) test in term of
    VPQ. Each cell shows VPQ^k / VPQ^k-Thing / VPQ^k-Stuff.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Temporal window size |  |  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| Method | $k=0$$\uparrow$ | $k=5$$\uparrow$ | $k=10$$\uparrow$ | $k=15$$\uparrow$
    | VPQ$\uparrow$ | FPS$\uparrow$ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| VPS [[227](#bib.bib227)] | 64.2 / 59.0 / 67.7 | 57.9 / 46.5 / 65.1 | 54.8
    / 41.1 / 63.4 | 52.6 / 36.5 / 62.9 | 57.4 / 45.8 / 64.8 | 1.3 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| SiamTrack [[240](#bib.bib240)] | 63.8 / 59.4 / 66.6 | 58.2 / 47.2 / 65.9
    | 56.0 / 43.2 / 64.4 | 54.7 / 40.2 / 63.2 | 57.8 / 47.5 / 65.0 | 4.5 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| ViP-DeepLab [[241](#bib.bib241)] | 68.9 / 61.6 / 73.5 | 62.9 / 51.0 / 70.5
    | 59.9 / 46.0 / 68.8 | 58.2 / 42.1 / 68.4 | 62.5 / 50.2 / 70.3 | 10.0 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ IoU: overall IoU is computed as total intersection area of all test
    data over the total union area, while mean IoU refers to average over IoU of each
    test sample.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Precision: Precision@$K$ is computed as the percentage of test samples
    whose IoU scores are higher than a threshold $K$. Precision at five thresholds
    ranging from 0.5 to 0.9 and'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: mean${}_{\!}$ average${}_{\!}$ precision${}_{\!}$ (mAP)${}_{\!}$ over${}_{\!}$
    0.5:0.05:0.95${}_{\!}$ are${}_{\!}$ reported.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Results
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A2D Sentence [[196](#bib.bib196)] is arguably the most popular dataset in
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: LVOS. Table [5.1.1](#S5.SS1.SSS1 "5.1.1 Evaluation Metrics ‣ 5.1 Object-level
    AVOS Performance Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning
    Technique for Video Segmentation") gives the results of six recent methods on
    A2D Sentence test set. It shows clear improvement trend from the first LVOS model [[196](#bib.bib196)]
    proposed in 2018, to recent complicated solution [[203](#bib.bib203)]. For runtime
    comparison, all the methods are tested on a video clip of 16 frames with resolution
    $512\!\times\!512$ and a textual sequence of 20 words.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 VSS Performance Benchmarking
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.6.1 Evaluation Metrics
  id: totrans-487
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: IoU metric is the most widely used metric in VSS. Moreover, in Cityscapes [[206](#bib.bib206)]
    – the gold-standard benchmark dataset in this field, two IoU scores, IoU${}_{\text{category}}$
    and IoU${}_{\text{class}}$, defined over two semantic granularities, are reported.
    Here, ‘category’ refers to high-level semantic categories (*e.g*., vehicle, human),
    while ‘class’ indicates more fine-grained semantic classes (*e.g*., car, bicycle,
    person, rider). In total, [[206](#bib.bib206)] considers $19$ classes, which are
    further grouped into $8$ categories.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.2 Results
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [12](#S5.T12 "TABLE 12 ‣ 5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation") summarizes the results of eleven VSS approaches on Cityscapes [[206](#bib.bib206)]
    val set. As seen, EFC [[223](#bib.bib223)] performs the best currently, with $83.5\%$
    in terms of IoU${}_{\text{class}}$.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 VIS Performance Benchmarking
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.7.1 Evaluation Metrics
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As in [[82](#bib.bib82)], precision and recall metrics are used for VIS performance
    evaluation. Precision at IoU thresholds 0.5 and 0.75, as well as mean average
    precision (mAP) over 0.50:0.05:0.95 are reported. Recall@$N$ is defined as the
    maximum recall given $N$ segmented instances per video. These two metrics are
    first evaluated per category and then averaged over the category set. The IoU
    metric is similar to region Jaccard $\mathcal{J}$ used in instance-level AVOS
    (§[5.2.1](#S5.SS2.SSS1 "5.2.1 Evaluation Metrics ‣ 5.2 Instance-level AVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation")).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.2 Results
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [13](#S5.T14 "TABLE 13 ‣ 5.8.2 Results ‣ 5.8 VPS Performance Benchmarking
    ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation")
    gathers VIS results for on YouTube-VIS [[82](#bib.bib82)] val set, showing that
    Transformer-based architecture, *i.e*., VisTR [[238](#bib.bib238)], and redundant
    sequence proposal based solution Propose-Reduce [[244](#bib.bib244)], greatly
    improve the state-of-the-art.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 VPS Performance Benchmarking
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.8.1 Evaluation Metrics
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[227](#bib.bib227)], the panoptic quality (PQ) metric used in image panoptic
    segmentation is modified as video panoptic quality (VPQ) to adapt to video panoptic
    segmentation.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$${}_{\!}$ VPQ:${}_{\!}$ Given${}_{\!}$ a${}_{\!}$ snippet${}_{\!}$
    $V^{t:t+k\!}$ with${}_{\!}$ time${}_{\!}$ window${}_{\!}$ $k$,${}_{\!}$ true${}_{\!}$ po-${}_{\!}$
    sitive (TP) is defined by $\text{TP}\!=\!\{(u,\hat{u})_{\!}\!\in\!U_{\!}\!\times\!\hat{U}_{\!}\!:_{\!}\text{IoU}(u,\hat{u})\!>\!0.5\}$
    where $U$ and $\hat{U}$ are the set of the ground-truth and predicted tubes, respectively.
    False Positives (FP) and False Negatives (FN) are defined accordingly. After accumulating
    TP[c], FP[c], and FN[c] on all the clips with window size $k$ and class $c$, we
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: define:${}_{\!}$ $\text{VPQ}^{k}_{\!}\!=_{\!}\!\frac{1}{N_{\text{class}}}\!\sum_{c}\!\frac{\sum_{(u,\hat{u})\in\text{TP}_{c}}\!\text{IoU}(u,\hat{u})}{|\text{TP}_{c}|+\frac{1}{2}|\text{FP}_{c}|+\frac{1}{2}|\text{FN}_{c}|}$.${}_{\!}$
    When${}_{\!}$ $k\!=\!1$,${}_{\!}$ VPQ¹
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: is equivalent to PQ. For evaluation, $\text{VPQ}^{k}$ is reported over $k\!\in\!\{0,5,10,15\}$
    and finally, $\text{VPQ}\!=\!\frac{1}{4}\sum_{k\in\{0,5,10,15\}\!}\text{VPQ}^{k}$.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 5.8.2 Results
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ​Cityscapes-VPS​ [[227](#bib.bib227)]${}_{\!}$ is${}_{\!}$ chosen${}_{\!}$ for${}_{\!}$
    testing${}_{\!}$ VPS${}_{\!}$ methods.${}_{\!}$ As${}_{\!}$ shown${}_{\!}$ in${}_{\!}$
    Table​ [14](#S5.T13 "TABLE 14 ‣ 5.5.1 Evaluation Metrics ‣ 5.5 LVOS Performance
    Benchmarking ‣ 5 Performance Comparison ‣ A Survey on Deep Learning Technique
    for Video Segmentation"), ViP-DeepLab​ [[241](#bib.bib241)]${}_{\!}$ is${}_{\!}$
    the${}_{\!}$ top${}_{\!}$ one.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 13: Quantitative VIS results on YouTube-VIS [[82](#bib.bib82)] the val
    (§[5.7.2](#S5.SS7.SSS2 "5.7.2 Results ‣ 5.7 VIS Performance Benchmarking ‣ 5 Performance
    Comparison ‣ A Survey on Deep Learning Technique for Video Segmentation"))'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: in terms of Precision@$K$, mAP, Recall@$N$ and IoU.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | P@0.5$\uparrow$ | P@0.75$\uparrow$ | R@1$\uparrow$ | R@10$\uparrow$
    |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '&#124; mAP$\uparrow$ &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5:0.95 &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '| FPS$\uparrow$ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| fIRN [[242](#bib.bib242)] | 27.2 | 6.2 | 12.3 | 13.6 | 10.5 | 3 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| MaskTrack R-CNN [[82](#bib.bib82)] | 51.1 | 32.6 | 31.0 | 35.5 | 30.3 | 20
    |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| Sipmask [[232](#bib.bib232)] | 53.0 | 33.3 | 33.5 | 38.9 | 32.5 | 24 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| STEm-Seg [[233](#bib.bib233)] | 55.8 | 37.9 | 34.4 | 41.6 | 34.6 | 9 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| CrossVIS [[245](#bib.bib245)] | 57.3 | 39.7 | 36.0 | 42.0 | 36.6 | 36 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| SemiTrack [[243](#bib.bib243)] | 61.1 | 39.8 | 36.9 | 44.5 | 38.3 | 10 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| MaskProp [[226](#bib.bib226)] | - | 45.6 | - | - | 42.5 | ^†1 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| CompFeat [[235](#bib.bib235)] | 56.0 | 38.6 | 33.1 | 40.3 | 35.3 | ^†17 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| TraDeS [[236](#bib.bib236)] | 52.6 | 32.8 | 29.1 | 36.6 | 32.6 | 26 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| SG-Net [[237](#bib.bib237)] | 57.1 | 39.6 | 35.9 | 43.0 | 36.3 | 20 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| VisTR [[238](#bib.bib238)] | 64.0 | 45.0 | 38.3 | 44.9 | 40.1 | 58 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| Propose-Reduce [[244](#bib.bib244)] | 71.6 | 51.8 | 46.3 | 56.0 | 47.6 |
    2 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: 5.9 Summary
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the results, we can draw several conclusions. The most important of them
    is related to reproducibility. Across different video segmentation areas, many
    methods do not describe the setup for the experimentation or do not provide the
    source code for implementation. Some of them even do not release segmentation
    masks. Moreover, different methods use various datasets and backbone models. These
    make fair comparison impossible and hurt reproducibility.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Another important fact discovered thanks to this study is the lack of information
    about execution time and memory use. Many methods particularly in the fields of
    AVOS, LVOS, and VPS, do not report execution time and almost no paper reports
    memory use. This void is due to the fact that many methods focus only on accuracy
    without any concern about running time efficiency or memory requirements. However,
    in many application scenarios, such as mobile devices and self-driving cars, computational
    power and memory are typically limited. As benchmark datasets and challenges serve
    as a main driven factor behind the fast evolution of segmentation techniques,
    we encourage organizers of future video segmentation datasets to give this kind
    of metrics its deserved importance in benchmarking.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: Finally, performance on some extensively studied video segmentation datasets,
    such as DAVIS[16] [[17](#bib.bib17)] in AVOS, DAVIS[17] [[81](#bib.bib81)] in
    SVOS, A2D Sentence [[196](#bib.bib196)] in LVOS, have nearly reached saturation.
    Though some new datasets are proposed recently and claim huge space for performance
    improvement, the dataset collectors just gather more challenging samples, without
    necessarily figuring out which exact challenges have and have not been solved.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Research Directions
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the reviewed research, we list several future research directions that
    we believe should be pursued.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Long-Term Video Segmentation: Long-term video segmentation is much
    closer to practical applications, such as video editing. However, as the sequences
    in existing datasets often span several seconds, the performance of VOS models
    over long video sequences (*e.g*., at the minute level) are still unexamined.
    Bringing VOS into the long-term setting will unlock new research lines, and put
    forward higher demand of the re-detection capability of VOS models.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Open World Video Segmentation: Despite the obvious dynamic and open
    nature of the world, current VSS algorithms are typically developed in a closed-world
    paradigm, where all the object categories are known as a prior. These algorithms
    are often brittle once exposed to the realistic complexity of the open world,
    where they are unable to efficiently adapt and robustly generalize to unseen categories.
    For example, practical deployments of VSS systems in robotics, self-driving cars,
    and surveillance cannot afford to have complete knowledge on what classes to expect
    at inference time, while being trained in-house. This calls for smarter VSS systems,
    with a strong capability to identify unknown categories in their environments
    [[265](#bib.bib265)].'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Cooperation across Different Video Segmentation Sub-fields: VOS and
    VSS face many common challenges, *e.g*., object occlusion, deformation, and fast
    motion. Moreover, there are no precedents for modeling these tasks in a unified
    framework. Thus we call for closer collaboration across different video segmentation
    sub-fields.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Annotation-Efficient Video Segmentation Solutions: Though great advances
    have been achieved in various videos segmentation tasks, current top-leading algorithms
    are built on fully-supervised deep learning techniques, requiring a huge amount
    of annotated data. Though semi-supervised, weakly supervised and unsupervised
    alternatives were explored in some literature, annotation-efficient solutions
    receive far less attention and typically show weak performance, compared with
    the fully supervised ones. As the high temporal correlations in video data can
    provide additional cues for supervision, exploring existing annotation-efficient
    techniques in static semantic segmentation in the area of video segmentation is
    an appealing direction.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Adaptive Computation: It is widely recognized that there exist high
    correlations among video frames. Though such data redundancy and continuity are
    exploited to reduce the computation cost in VSS, almost all current video segmentation
    models are fixed feed-forward structures or work alternatively between heavy and
    light-weight modes. We expect more flexible segmentation model designs towards
    more efficient and adaptive computation [[266](#bib.bib266)], which allows network
    architecture change on-the-fly – selectively activating part of the network in
    an input-dependent fashion.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Neural Architecture Search: Video segmentation models are typically
    built upon hand-designed architectures, which may be suboptimal for capturing
    the nature of video data and limit the best possible performance. Using neural
    architecture search techniques to automate the design of video segmentation networks
    is a promising direction.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 7 CONCLUSION
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To${}_{\!}$ our${}_{\!}$ knowledge,${}_{\!}$ this${}_{\!}$ is${}_{\!}$ the${}_{\!}$
    first${}_{\!}$ survey${}_{\!}$ to${}_{\!}$ comprehensively
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: review recent progress in video segmentation. We provided the reader with the
    necessary background knowledge and summarized more than 150 deep learning models
    according to various criteria, including task settings, technique contributions,
    and learning strategies. We also presented a structured survey of 20 widely used
    video segmentation datasets and benchmarking results on 7 most widely-used ones.
    We discussed the results and provided insight into the shape of future research
    directions and open problems in the field. In conclusion, video segmentation has
    achieved notable progress thanks to the striking development of deep learning
    techniques, but several challenges still lie ahead.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] W. Wang, J. Shen, R. Yang, and F. Porikli, “Saliency-aware video object
    segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 1, pp.
    20–33, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Papazoglou and V. Ferrari, “Fast object segmentation in unconstrained
    video,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 1777–1784.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. Xu and J. J. Corso, “Evaluation of super-voxel methods for early video
    processing,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2012, pp. 1202–1209.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] T. Brox and J. Malik, “Object segmentation by long term analysis of point
    trajectories,” in *Proc. Eur. Conf. Comput. Vis.*, 2010, pp. 282–295.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Y. J. Lee, J. Kim, and K. Grauman, “Key-segments for video object segmentation,”
    in *Proc. IEEE Int. Conf. Comput. Vis.*, 2011, pp. 1995–2002.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C.-P. Yu, H. Le, G. Zelinsky, and D. Samaras, “Efficient video segmentation
    using parametric graph partitioning,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2015, pp. 3155–3163.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Grundmann, V. Kwatra, M. Han, and I. Essa, “Efficient hierarchical graph-based
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010,
    pp. 2141–2148.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. Shankar Nagaraja, F. R. Schmidt, and T. Brox, “Video segmentation with
    just a few strokes,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3235–3243.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung, “Fully connected
    object proposals for video segmentation,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2015, pp. 3227–3234.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] V. Badrinarayanan, I. Budvytis, and R. Cipolla, “Semi-supervised video
    segmentation using tree structured graphical models,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 35, no. 11, pp. 2751–2764, 2013.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] W.-D. Jang and C.-S. Kim, “Streaming video segmentation via short-term
    hierarchical segmentation and frame-by-frame markov random field optimization,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 599–615.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Liu and X. He, “Multiclass semantic video segmentation with object-level
    active inference,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015,
    pp. 4286–4294.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 3431–3440.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. M. Thounaojam, A. Trivedi, K. M. Singh, and S. Roy, “A survey on video
    segmentation,” in *Intelligent computing, networking, and informatics*, 2014,
    pp. 903–912.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y.-J. Zhang, “An overview of image and video segmentation in the last
    40 years,” *Advances in Image and Video Segmentation*, pp. 1–16, 2006.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Yao, G. Lin, S. Xia, J. Zhao, and Y. Zhou, “Video object segmentation
    and tracking: A survey,” *ACM Transactions on Intelligent Systems and Technology*,
    vol. 11, no. 4, pp. 1–47, 2020.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung,
    “A benchmark dataset and evaluation methodology for video object segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 724–732.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-Taixé, D. Cremers, and
    L. V. Gool, “One-shot video object segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 5320–5329.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. Badrinarayanan, F. Galasso, and R. Cipolla, “Label propagation in video
    sequences,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010, pp. 3265–3272.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 43,
    no. 11, pp. 4037–4058, 2020.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] L. G. Roberts, “Machine perception of three-dimensional solids,” *Optical
    and Electro-Optical Information Processing*, 1965.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Chang, D. Wei, and J. W. Fisher, “A video representation using temporal
    superpixels,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp.
    2051–2058.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] R. Jain and H.-H. Nagel, “On the analysis of accumulative difference pictures
    from image sequences of real world scenes,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    no. 2, pp. 206–214, 1979.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder:
    Real-time tracking of the human body,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 19, no. 7, pp. 780–785, 1997.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Criminisi, G. Cross, A. Blake, and V. Kolmogorov, “Bilayer segmentation
    of live video,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2006, pp.
    53–60.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Brutzer, B. Höferlin, and G. Heidemann, “Evaluation of background subtraction
    techniques for video surveillance,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2011, pp. 1937–1944.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] E. Hayman and J.-O. Eklundh, “Statistical background subtraction for a
    mobile observer,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2003, pp. 67–67.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. Irani and P. Anandan, “A unified approach to moving object detection
    in 2d and 3d scenes,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 20, no. 6,
    pp. 577–589, 1998.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Y. Wang and E. H. Adelson, “Layered representation for motion analysis,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 1993, pp. 361–366.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. S. Sawhney and S. Ayer, “Compact representations of videos through
    dominant and multiple motion estimation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 18, no. 8, pp. 814–830, 1996.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Costeira and T. Kanade, “A multi-body factorization method for motion
    analysis,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 1995, pp. 1071–1076.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Cremers and S. Soatto, “Motion competition: A variational approach
    to piecewise parametric motion segmentation,” *Int. J. Comput. Vis.*, vol. 62,
    no. 3, pp. 249–265, 2005.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Ochs and T. Brox, “Object segmentation in video: A hierarchical variational
    approach for turning point trajectories into dense regions,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2011, pp. 1583–1590.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] K. Fragkiadaki, G. Zhang, and J. Shi, “Video segmentation by tracing discontinuities
    in a trajectory embedding,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 1846–1853.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Keuper, B. Andres, and T. Brox, “Motion trajectory segmentation via
    minimum cost multicuts,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3271–3279.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by long
    term video analysis,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 36, no. 6,
    pp. 1187–1200, 2014.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] A. Faktor and M. Irani, “Video segmentation by non-local consensus voting,”
    in *Proc. British Mach. Vis. Conf.*, 2014.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. Ma and L. J. Latecki, “Maximum weight cliques with mutex constraints
    for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 670–677.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. Zhang, O. Javed, and M. Shah, “Video object segmentation through spatially
    accurate and temporally dense extraction of primary object regions,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 628–635.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] F. Xiao and Y. Jae Lee, “Track and segment: An iterative unsupervised
    approach for video object proposals,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2016, pp. 933–942.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] I. Endres and D. Hoiem, “Category independent object proposals,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2010, pp. 575–588.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Avinash Ramakanth and R. Venkatesh Babu, “SeamSeg: Video object segmentation
    using patch seams,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014,
    pp. 376–383.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y.-H. Tsai, M.-H. Yang, and M. J. Black, “Video segmentation via object
    flow,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 3899–3908.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W. Wang, J. Shen, F. Porikli, and R. Yang, “Semi-supervised video object
    segmentation with super-trajectories,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 41, no. 4, pp. 985–998, 2018.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] D. Tsai, M. Flagg, and J. M. Rehg, “Motion coherent tracking using multi-label
    MRF optimization,” in *Proc. British Mach. Vis. Conf.*, 2010, pp. 190–202.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] L. Wen, D. Du, Z. Lei, S. Z. Li, and M.-H. Yang, “JOTS: Joint online tracking
    and segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015,
    pp. 2226–2234.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Agarwala, A. Hertzmann, D. H. Salesin, and S. M. Seitz, “Keyframe-based
    tracking for rotoscoping and animation,” *ACM Tran. Graphics*, vol. 23, no. 3,
    pp. 584–591, 2004.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Li, F. Viola, J. Starck, G. J. Brostow, and N. D. Campbell, “Roto++
    accelerating professional rotoscoping using shape manifolds,” *ACM Tran. Graphics*,
    vol. 35, no. 4, pp. 1–15, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Bai, J. Wang, D. Simons, and G. Sapiro, “Video SnapCut: robust video
    object cutout using localized classifiers,” *ACM Tran. Graphics*, vol. 28, no. 3,
    p. 70, 2009.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Criminisi, T. Sharp, C. Rother, and P. Pérez, “Geodesic image and video
    editing,” *ACM Tran. Graphics*, vol. 29, no. 5, pp. 134–1, 2010.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] F. Zhong, X. Qin, Q. Peng, and X. Meng, “Discontinuity-aware video object
    cutout,” *ACM Tran. Graphics*, vol. 31, no. 6, p. 175, 2012.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen, “JumpCut: non-successive
    mask transfer and interpolation for video cutout.” *ACM Tran. Graphics*, vol. 34,
    no. 6, pp. 195–1, 2015.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Lu, X. Bai, L. Shapiro, and J. Wang, “Coherent parametric contours
    for interactive video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2016, pp. 642–650.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. Wang, J. Shen, and F. Porikli, “Selective video object cutout,” *IEEE
    Trans. Image Process.*, vol. 26, no. 12, pp. 5645–5655, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Jain, S. Chatterjee, and R. Vidal, “Coarse-to-fine semantic video segmentation
    using supervoxel trees,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 1865–1872.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Kae, B. Marlin, and E. Learned-Miller, “The shape-time random field
    for semantic video labeling,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 272–279.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Tang, R. Sukthankar, J. Yagnik, and L. Fei-Fei, “Discriminative segment
    annotation in weakly labeled video,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2013, pp. 2483–2490.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Liu, D. Tao, M. Song, Y. Ruan, C. Chen, and J. Bu, “Weakly supervised
    multiclass video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 57–64.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and
    M. Shah, “Visual tracking: An experimental survey,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 36, no. 7, pp. 1442–1468, 2013.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,” *ACM Computing
    Surveys*, vol. 38, no. 4, pp. 1–45, 2006.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. Bibby and I. Reid, “Robust real-time visual tracking using pixel-wise
    posteriors,” in *Proc. Eur. Conf. Comput. Vis.*, 2008, pp. 831–844.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] X. Ren and J. Malik, “Tracking as repeated figure/ground segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2007, pp. 1–8.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, P. Martinez-Gonzalez,
    and J. Garcia-Rodriguez, “A survey on deep learning techniques for image and video
    semantic segmentation,” *Applied Soft Computing*, vol. 70, pp. 41–65, 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, and L. Van Gool, “Exploring
    cross-image pixel contrast for semantic segmentation,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2021, pp. 7303–7313.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] T. Zhou, W. Wang, E. Konukoglu, and L. Van Gool, “Rethinking semantic
    segmentation: A prototype view,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2022, pp. 2582–2593.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. D. Jain, B. Xiong, and K. Grauman, “Fusionseg: Learning to combine
    motion and appearance for fully automatic segmention of generic objects in videos,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 686–695.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large
    Scale Visual Recognition Challenge,” *Int. J. Comput. Vis.*, vol. 115, no. 3,
    pp. 211–252, 2015.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang, “Segflow: Joint learning
    for video object segmentation and optical flow,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2017, pp. 686–695.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] P. Tokmakov, K. Alahari, and C. Schmid, “Learning video object segmentation
    with visual memory,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 4491–4500.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] ——, “Learning motion patterns in videos,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 531–539.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, pp. 4040–4048.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Li, A. Zheng, X. Chen, and B. Zhou, “Primary video object segmentation
    via complementary cnns and neighborhood reversible flow,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2017, pp. 1417–1425.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Ferrari, “Learning
    object class detectors from weakly annotated video,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2012, pp. 3282–3289.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] S. Li, B. Seybold, A. Vorobyov, A. Fathi, Q. Huang, and C.-C. Jay Kuo,
    “Instance embedding transfer to unsupervised video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 6526–6535.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Li, Y. Xie, T. Wei, K. Wang, and L. Lin, “Flow guided recurrent neural
    encoder for video salient object detection,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2018, pp. 3243–3252.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg, “Video segmentation
    by tracking many figure-ground segments,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2013, pp. 2192–2199.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Li, B. Seybold, A. Vorobyov, X. Lei, and C.-C. Jay Kuo, “Unsupervised
    video object segmentation with motion-based bilateral networks,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2018, pp. 215–231.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] H. Song, W. Wang, S. Zhao, J. Shen, and K.-M. Lam, “Pyramid dilated deeper
    convlstm for video salient object detection,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018, pp. 744–760.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Siam, C. Jiang, S. Lu, L. Petrich, M. Gamal, M. Elhoseiny, and M. Jagersand,
    “Video segmentation using teacher-student adaptation in a human robot interaction
    (hri) setting,” in *International Conference on Robotics and Automation*, 2019,
    pp. 50–56.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. Ventura, M. Bellver, A. Girbau, A. Salvador, F. Marques, and X. Giro-i
    Nieto, “Rvos: End-to-end recurrent network for video object segmentation,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 5277–5286.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung,
    and L. Van Gool, “The 2017 davis challenge on video object segmentation,” *arXiv
    preprint arXiv:1704.00675*, 2017.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2019, pp. 5188–5197.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and F. Porikli, “See more, know
    more: Unsupervised video object segmentation with co-attention siamese networks,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 3623–3632.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global
    contrast based salient region detection,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 37, no. 3, pp. 569–582, 2015.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Yang, L. Zhang, H. Lu, X. Ruan, and M. Yang, “Saliency detection via
    graph-based manifold ranking,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2013, pp. 3166–3173.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Yang, A. Loquercio, D. Scaramuzza, and S. Soatto, “Unsupervised moving
    object detection via contextual information separation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 879–888.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] W. Wang, H. Song, S. Zhao, J. Shen, S. Zhao, S. C. Hoi, and H. Ling, “Learning
    unsupervised video object segmentation through visual attention,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 3064–3074.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of
    salient object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 280–287.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] W. Wang, X. Lu, J. Shen, D. J. Crandall, and L. Shao, “Zero-shot video
    object segmentation via attentive graph neural networks,” in *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2019, pp. 9236–9245.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] H. Li, G. Chen, G. Li, and Y. Yu, “Motion guided attention for video salient
    object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp. 7274–7283.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan, “Learning
    to detect salient objects with image-level supervision,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, pp. 136–145.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Yang, Q. Wang, L. Bertinetto, W. Hu, S. Bai, and P. H. Torr, “Anchor
    diffusion for unsupervised video object segmentation,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2019, pp. 931–940.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] P. Tokmakov, C. Schmid, and K. Alahari, “Learning to segment moving objects,”
    *Int. J. Comput. Vis.*, vol. 127, no. 3, pp. 282–301, 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] T. Zhou, S. Wang, Y. Zhou, Y. Yao, J. Li, and L. Shao, “Motion-attentive
    transition for zero-shot video object segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2020, pp. 13 066–13 073.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, “Youtube-vos:
    A large-scale video object segmentation benchmark,” *arXiv preprint arXiv:1809.03327*,
    2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, and S.-P. Lu, “Pyramid constrained
    self-attention network for fast video salient object detection,” in *AAAI Conference
    on Artificial Intelligence*, 2020, pp. 10 869–10 876.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] D.-P. Fan, W. Wang, M.-M. Cheng, and J. Shen, “Shifting more attention
    to video salient object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, pp. 8554–8564.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Lu, W. Wang, J. Shen, Y.-W. Tai, D. J. Crandall, and S. C. Hoi, “Learning
    video object segmentation from unlabeled videos,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 8960–8970.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, and E. Gavves, “Long-term tracking in the wild: A benchmark,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 670–685.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Lu, W. Wang, M. Danelljan, T. Zhou, J. Shen, and L. Van Gool, “Video
    object segmentation with episodic graph memory networks,” in *Proc. Eur. Conf.
    Comput. Vis.*, 2020, pp. 661–679.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Zhang, J. Zhang, Z. Lin, R. Mech, H. Lu, and Y. He, “Unsupervised
    video object segmentation with joint hotspot tracking,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 490–506.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Jiang, S. Huang, J. Duan, and Q. Zhao, “Salicon: Saliency in context,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 1072–1080.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] M. Everingham, S. M. A. Eslami, L. J. V. Gool, C. K. I. Williams, J. M.
    Winn, and A. Zisserman, “The pascal visual object classes challenge: A retrospective,”
    *Int. J. Comput. Vis.*, vol. 111, no. 1, pp. 98–136, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] M. Zhen, S. Li, L. Zhou, J. Shang, H. Feng, T. Fang, and L. Quan, “Learning
    discriminative feature with crf for unsupervised video object segmentation,” in
    *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 445–462.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] D. Liu, D. Yu, C. Wang, and P. Zhou, “F2net: Learning to focus on the
    foreground for unsupervised video object segmentation,” in *AAAI Conference on
    Artificial Intelligence*, 2021, pp. 2109–2117.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Zhou, J. Li, X. Li, and L. Shao, “Target-aware object discovery and
    association for unsupervised video multi-object segmentation,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 6985–6994.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] S. Ren, W. Liu, Y. Liu, H. Chen, G. Han, and S. He, “Reciprocal transformations
    for unsupervised video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 15 455–15 464.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Yang, B. Lai, and S. Soatto, “Dystab: Unsupervised object segmentation
    via dynamic-static bootstrapping,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2021, pp. 2826–2836.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Yang, H. Lamdouar, E. Lu, A. Zisserman, and W. Xie, “Self-supervised
    video object segmentation by motion grouping,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, pp. 7177–7188.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by long
    term video analysis,” vol. 36, no. 6, pp. 1187–1200, 2013.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] H. Lamdouar, C. Yang, W. Xie, and A. Zisserman, “Betrayed by motion:
    Camouflaged object discovery via motion segmentation,” in *Asian Conference on
    Computer Vision*, 2020, pp. 488–503.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] L. Jiao, R. Zhang, F. Liu, S. Yang, B. Hou, L. Li, and X. Tang, “New
    generation deep learning for video object detection: A survey,” *IEEE Trans. Neural
    Netw. Learning Sys.*, 2021.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Fragkiadaki, P. Arbelaez, P. Felsen, and J. Malik, “Learning to segment
    moving objects in videos,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 4083–4090.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] W. Wang, J. Shen, and L. Shao, “Video salient object detection via fully
    convolutional networks,” *IEEE Trans. Image Process.*, vol. 27, no. 1, pp. 38–49,
    2017.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Fathi, Z. Wojna, V. Rathod, P. Wang, H. O. Song, S. Guadarrama, and
    K. P. Murphy, “Semantic instance segmentation via deep metric learning,” *arXiv
    preprint arXiv:1703.10277*, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end memory
    networks,” in *Proc. Advances Neural Inf. Process. Syst*, 2015, pp. 2440–2448.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. Sorkine-Hornung,
    “Learning video object segmentation from static images,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, pp. 3491–3500.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 1155–1162.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] W.-D. Jang and C.-S. Kim, “Online video object segmentation via convolutional
    trident network,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 5849–5858.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] V. Jampani, R. Gadde, and P. V. Gehler, “Video propagation networks,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 3154–3164.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. S. Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and I. S. Kweon, “Pixel-level
    matching for video object segmentation using convolutional neural networks,” in
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2186–2195.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Voigtlaender and B. Leibe, “Online adaptation of convolutional neural
    networks for video object segmentation,” in *Proc. British Mach. Vis. Conf.*,
    2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2014, pp. 740–755.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele, “Lucid data
    dreaming for video object segmentation,” *Int. J. Comput. Vis.*, vol. 127, no. 9,
    pp. 1175–1197, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] L. Bao, B. Wu, and W. Liu, “CNN in MRF: Video object segmentation via
    inference in a CNN-based higher-order spatio-temporal MRF,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2018, pp. 5977–5986.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang, “Fast and
    accurate online video object segmentation via tracking parts,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 7415–7424.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Wug Oh, J.-Y. Lee, K. Sunkavalli, and S. Joo Kim, “Fast video object
    segmentation by reference-guided mask propagation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 7376–7385.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos, “Efficient
    video object segmentation via network modulation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 6499–6507.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Xiao, J. Feng, G. Lin, Y. Liu, and M. Zhang, “Monet: Deep motion exploitation
    for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 1140–1148.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] P. Hu, G. Wang, X. Kong, J. Kuen, and Y.-P. Tan, “Motion-guided cascaded
    refinement network for video object segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 1400–1409.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang, “Reinforcement cutting-agent
    learning for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 9080–9089.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixé, D. Cremers,
    and L. Van Gool, “Video object segmentation without temporal information,” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, vol. 41, no. 6, pp. 1515–1530, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Y.-T. Hu, J.-B. Huang, and A. G. Schwing, “Videomatch: Matching based
    video object segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 56–73.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] X. Li and C. Change Loy, “Video object segmentation with joint re-identification
    and attention-aware mask propagation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018,
    pp. 93–110.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Ci, C. Wang, and Y. Wang, “Video object segmentation by learning location-sensitive
    embeddings,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 524–539.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in *Proc. Eur. Conf. Comput. Vis.*, 2018,
    pp. 391–408.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv preprint arXiv:1705.06950*, 2017.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] H. Xiao, B. Kang, Y. Liu, M. Zhang, and J. Feng, “Online meta adaptation
    for fast video object segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 42, no. 5, pp. 1205–1217, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, and L.-C. Chen,
    “Feelvos: Fast end-to-end embedding learning for video object segmentation,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 9481–9490.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Xu, D. Liu, L. Bao, W. Liu, and P. Zhou, “Mhp-vos: Multiple hypotheses
    propagation for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, pp. 314–323.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Lin, X. Qi, and J. Jia, “Agss-vos: Attention guided single-shot video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019,
    pp. 3949–3957.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. Johnander, M. Danelljan, E. Brissman, F. S. Khan, and M. Felsberg,
    “A generative appearance model for end-to-end video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 8953–8962.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. Torr, “Fast online
    object tracking and segmentation: A unifying approach,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 1328–1338.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] B. A. Griffin and J. J. Corso, “Bubblenets: Learning to select the guidance
    frame in video object segmentation by deep sorting frames,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 8914–8923.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Z. Wang, J. Xu, L. Liu, F. Zhu, and L. Shao, “Ranet: Ranking attention
    network for fast video object segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019, pp. 3978–3987.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] G. Li and Y. Yu, “Visual saliency based on multiscale deep features,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 5455–5463.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] X. Zeng, R. Liao, L. Gu, Y. Xiong, S. Fidler, and R. Urtasun, “Dmm-net:
    Differentiable mask-matching network for video object segmentation,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2019, pp. 3929–3938.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] L. Zhang, Z. Lin, J. Zhang, H. Lu, and Y. He, “Fast video object segmentation
    via dynamic targeting network,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    pp. 5582–5591.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, “Video object segmentation
    using space-time memory networks,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    pp. 9226–9235.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] X. Wang, A. Jabri, and A. A. Efros, “Learning correspondence from the
    cycle-consistency of time,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, pp. 2566–2576.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] D. F. Fouhey, W.-c. Kuo, A. A. Efros, and J. Malik, “From lifestyle vlogs
    to everyday interactions,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 4991–5000.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” in *Proc. Advances Neural
    Inf. Process. Syst*, 2019, pp. 318–328.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] T. Meinhardt and L. Leal-Taixé, “Make one-shot video object segmentation
    efficient again,” in *Proc. Advances Neural Inf. Process. Syst*, 2020, pp. 10 607–10 619.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Liang, X. Li, N. Jafari, and Q. Chen, “Video object segmentation with
    adaptive feature bank and uncertain-region refinement,” in *Proc. Advances Neural
    Inf. Process. Syst*, 2020, pp. 3430–3441.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] X. Huang, J. Xu, Y.-W. Tai, and C.-K. Tang, “Fast video object segmentation
    with temporal aggregation network and dynamic template matching,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 8879–8889.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Sun, J. Xiao, E. G. Lim, B. Zhang, and Y. Zhao, “Fast template matching
    and update for video object tracking and segmentation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 10 791–10 799.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] X. Chen, Z. Li, Y. Yuan, G. Yu, J. Shen, and D. Qi, “State-aware tracker
    for real-time video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2020, pp. 9384–9393.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. Robinson, F. J. Lawin, M. Danelljan, F. S. Khan, and M. Felsberg,
    “Learning fast and robust target models for video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 7406–7415.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Y. Zhang, Z. Wu, H. Peng, and S. Lin, “A transductive approach for video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020,
    pp. 6949–6958.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Lai, E. Lu, and W. Xie, “Mast: A memory-augmented self-supervised
    tracker,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 6479–6488.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Y. Li, Z. Shen, and Y. Shan, “Fast video object segmentation using the
    global context module,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 735–750.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] H. Seong, J. Hyun, and E. Kim, “Kernelized memory network for video object
    segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 629–645.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z. Yang, Y. Wei, and Y. Yang, “Collaborative video object segmentation
    by foreground-background integration,” in *Proc. Eur. Conf. Comput. Vis.*, 2020,
    pp. 332–348.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] G. Bhat, F. J. Lawin, M. Danelljan, A. Robinson, M. Felsberg, L. Van Gool,
    and R. Timofte, “Learning what to learn for video object segmentation,” in *Proc.
    Eur. Conf. Comput. Vis.*, 2020, pp. 777–794.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] R. Wu, H. Lin, X. Qi, and J. Jia, “Memory selection network for video
    propagation,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 175–190.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Li, N. Xu, J. Peng, J. See, and W. Lin, “Delving into the cyclic mechanism
    in semi-supervised video object segmentation,” in *Proc. Advances Neural Inf.
    Process. Syst*, 2020, pp. 1218–1228.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] F. Lin, H. Xie, Y. Li, and Y. Zhang, “Query-memory re-aggregation for
    weakly-supervised video object segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2021, pp. 2038–2046.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. Wang, X. Jiang, H. Ren, Y. Hu, and S. Bai, “Swiftnet: Real-time video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021,
    pp. 1296–1305.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Park, J. Yoo, S. Jeong, G. Venkatesh, and N. Kwak, “Learning dynamic
    network using a reuse gate function in semi-supervised video object segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 8405–8414.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] B. Duke, A. Ahmed, C. Wolf, P. Aarabi, and G. W. Taylor, “Sstvos: Sparse
    spatiotemporal transformers for video object segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2021, pp. 5912–5921.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] W. Ge, X. Lu, and J. Shen, “Video object segmentation using global and
    instance embedding learning,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2021, pp. 16 836–16 845.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] L. Hu, P. Zhang, B. Zhang, P. Pan, Y. Xu, and R. Jin, “Learning position
    and target consistency for memory-based video object segmentation,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 4144–4154.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] H. Xie, H. Yao, S. Zhou, S. Zhang, and W. Sun, “Efficient regional memory
    network for video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 1286–1295.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] A. Jabri, A. Owens, and A. Efros, “Space-time correspondence as a contrastive
    random walk,” in *Proc. Advances Neural Inf. Process. Syst*, 2020, pp. 19 545–19 560.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] X. Lu, W. Wang, J. Shen, D. Crandall, and J. Luo, “Zero-shot video object
    segmentation with co-attention siamese networks,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 44, no. 04, pp. 2228–2242, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] X. Lu, W. Wang, J. Shen, D. Crandall, and L. Van Gool, “Segmenting objects
    from relational visual data,” *IEEE Trans. Pattern Anal. Mach. Intell.*, 2021.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and
    L. Van Gool, “The 2019 davis challenge on vos: Unsupervised multi-object segmentation,”
    *arXiv:1905.00737*, 2019.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Luiten, I. E. Zulfikar, and B. Leibe, “Unovost: Unsupervised offline
    video object segmentation and tracking,” in *Proc. IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 2000–2009.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] W. Wang, J. Shen, X. Lu, S. C. Hoi, and H. Ling, “Paying attention to
    video object pattern understanding,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 43, no. 7, pp. 2413–2428, 2020.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2961–2969.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] A. Benard and M. Gygli, “Interactive video object segmentation in the
    wild,” *arXiv preprint arXiv:1801.00269*, 2017.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool, “Blazingly fast video
    object segmentation with pixel-wise metric learning,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 1189–1198.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, “Fast user-guided video object
    segmentation by interaction-and-propagation networks,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 5247–5256.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Miao, Y. Wei, and Y. Yang, “Memory aggregation networks for efficient
    interactive video object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2020, pp. 10 366–10 375.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Y. Heo, Y. J. Koh, and C.-S. Kim, “Interactive video object segmentation
    using global and local transfer modules,” in *Proc. Eur. Conf. Comput. Vis.*,
    2020, pp. 297–313.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] B. Chen, H. Ling, X. Zeng, G. Jun, Z. Xu, and S. Fidler, “Scribblebox:
    Interactive annotation framework for video object segmentation,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2020, pp. 293–310.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Z. Yin, J. Zheng, W. Luo, S. Qian, H. Zhang, and S. Gao, “Learning to
    recommend frame for interactive video object segmentation in the wild,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 15 445–15 454.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Y. Heo, Y. J. Koh, and C.-S. Kim, “Guided interactive video object segmentation
    using reliability-based attention maps,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 7322–7330.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, “Modular interactive video object
    segmentation: Interaction-to-mask, propagation and difference-aware fusion,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 5559–5568.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] L. Wang, W. Ouyang, X. Wang, and H. Lu, “Visual tracking with fully convolutional
    networks,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, pp. 3119–3127.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] K. Duarte, Y. S. Rawat, and M. Shah, “Capsulevos: Semi-supervised video
    object segmentation using capsule routing,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019, pp. 8480–8489.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Proc. Advances
    Neural Inf. Process. Syst*, 2017.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2022, pp. 8719–8730.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Caelles, A. Montes, K.-K. Maninis, Y. Chen, L. Van Gool, F. Perazzi,
    and J. Pont-Tuset, “The 2018 davis challenge on video object segmentation,” *arXiv
    preprint arXiv:1803.00557*, 2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] N. Xu, B. Price, S. Cohen, J. Yang, and T. S. Huang, “Deep interactive
    object selection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 373–381.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] K. Gavrilyuk, A. Ghodrati, Z. Li, and C. G. Snoek, “Actor and action
    video segmentation from a sentence,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 5958–5966.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] A. Khoreva, A. Rohrbach, and B. Schiele, “Video object segmentation with
    language referring expressions,” in *Asian Conference on Computer Vision*, 2018,
    pp. 123–141.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] H. Wang, C. Deng, J. Yan, and D. Tao, “Asymmetric cross-guided attention
    network for actor and action video segmentation from natural language query,”
    in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp. 3939–3948.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] H. Wang, C. Deng, F. Ma, and Y. Yang, “Context modulated dynamic networks
    for actor and action video segmentation with language queries,” in *AAAI Conference
    on Artificial Intelligence*, 2020, pp. 12 152–12 159.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Ning, L. Xie, F. Wu, and Q. Tian, “Polar relative positional encoding
    for video-language segmentation,” in *International Joint Conferences on Artificial
    Intelligence*, 2020, pp. 948–954.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] B. McIntosh, K. Duarte, Y. S. Rawat, and M. Shah, “Visual-textual capsule
    routing for text-based video segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2020, pp. 9942–9951.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] S. Seo, J.-Y. Lee, and B. Han, “Urvos: Unified referring video object
    segmentation network with a large-scale benchmark,” in *Proc. Eur. Conf. Comput.
    Vis.*, 2020, pp. 208–223.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] T. Hui, S. Huang, S. Liu, Z. Ding, G. Li, W. Wang, J. Han, and F. Wang,
    “Collaborative spatial-temporal modeling for language-queried video actor segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 4187–4196.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] L. Ye, M. Rochan, Z. Liu, X. Zhang, and Y. Wang, “Referring segmentation
    in images and videos with cross-modal self-attention network,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, 2021.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell, “Clockwork convnets
    for video semantic segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp.
    852–868.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 3213–3223.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] A. Kundu, V. Vineet, and V. Koltun, “Feature space optimization for semantic
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 3168–3175.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes
    in video: A high-definition ground truth database,” *Pattern Recognition Letters*,
    vol. 30, no. 2, pp. 88–97, 2009.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] J. Hur and S. Roth, “Joint optical flow and temporally consistent semantic
    segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 163–177.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    and B. Leibe, “Mots: Multi-object tracking and segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2019, pp. 7942–7951.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] B. Mahasseni, S. Todorovic, and A. Fern, “Budget-aware deep semantic
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 1029–1038.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] X. Jin, X. Li, H. Xiao, X. Shen, Z. Lin, J. Yang, Y. Chen, J. Dong, L. Liu,
    Z. Jie *et al.*, “Video scene parsing with predictive feature learning,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2017, pp. 5580–5588.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] R. Gadde, V. Jampani, and P. V. Gehler, “Semantic video cnns through
    representation warping,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 4453–4462.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature flow for
    video recognition,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    pp. 2349–2358.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, and J. M.
    Alvarez, “Bringing background into the foreground: Making all classes equal in
    weakly-supervised video semantic segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2017, pp. 2125–2135.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] D. Nilsson and C. Sminchisescu, “Semantic video segmentation by gated
    recurrent flow propagation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 6819–6828.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Y. Li, J. Shi, and D. Lin, “Low-latency video semantic segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 5997–6005.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Y.-S. Xu, T.-J. Fu, H.-K. Yang, and C.-Y. Lee, “Dynamic video segmentation
    network,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 6556–6565.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] P.-Y. Huang, W.-T. Hsu, C.-Y. Chiu, T.-F. Wu, and M. Sun, “Efficient
    uncertainty estimation for semantic segmentation in videos,” in *Proc. Eur. Conf.
    Comput. Vis.*, 2018, pp. 520–535.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] S. Chandra, C. Couprie, and I. Kokkinos, “Deep spatio-temporal random
    fields for efficient video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2018, pp. 8915–8924.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] S. Jain, X. Wang, and J. E. Gonzalez, “Accel: A corrective fusion network
    for efficient semantic segmentation on video,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, pp. 8866–8875.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, and B. Catanzaro,
    “Improving semantic segmentation via video propagation and label relaxation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 8856–8865.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] M. Ding, Z. Wang, B. Zhou, J. Shi, Z. Lu, and P. Luo, “Every frame counts:
    joint learning of video segmentation and optical flow,” in *AAAI Conference on
    Artificial Intelligence*, 2020, pp. 10 713–10 720.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] P. Hu, F. Caba, O. Wang, Z. Lin, S. Sclaroff, and F. Perazzi, “Temporally
    distributed networks for fast video semantic segmentation,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2020, pp. 8818–8827.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” in *Proc. Eur. Conf. Comput. Vis.*, 2012,
    pp. 746–760.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] G. Bertasius and L. Torresani, “Classifying, segmenting, and tracking
    object instances in video with mask propagation,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2020, pp. 9739–9748.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] D. Kim, S. Woo, J.-Y. Lee, and I. S. Kweon, “Video panoptic segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 9859–9868.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] L. Porzi, M. Hofinger, I. Ruiz, J. Serrat, S. R. Bulo, and P. Kontschieder,
    “Learning multi-object tracking and segmentation from automatic annotations,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 6846–6855.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell,
    “Bdd100k: A diverse driving dataset for heterogeneous multitask learning,” in
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, pp. 2636–2645.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] C.-C. Lin, Y. Hung, R. Feris, and L. He, “Video instance segmentation
    tracking with a modified vae architecture,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2020, pp. 13 147–13 157.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Liu, C. Shen, C. Yu, and J. Wang, “Efficient semantic video segmentation
    with per-frame inference,” in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 352–368.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y. Pang, and L. Shao,
    “Sipmask: Spatial information preservation for fast image and video instance segmentation,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2020, pp. 1–18.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] A. Athar, S. Mahadevan, A. Os̆ep, L. Leal-Taixé, and B. Leibe, “Stem-seg:
    Spatio-temporal embeddings for instance segmentation in videos,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2020, pp. 158–177.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] L.-C. Chen, R. G. Lopes, B. Cheng, M. D. Collins, E. D. Cubuk, B. Zoph,
    H. Adam, and J. Shlens, “Naive-student: Leveraging semi-supervised learning in
    video sequences for urban scene segmentation,” in *Proc. Eur. Conf. Comput. Vis.*,
    2020, pp. 695–714.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Y. Fu, L. Yang, D. Liu, T. S. Huang, and H. Shi, “Compfeat: Comprehensive
    feature aggregation for video instance segmentation,” in *AAAI Conference on Artificial
    Intelligence*, 2021, pp. 1361–1369.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Wu, J. Cao, L. Song, Y. Wang, M. Yang, and J. Yuan, “Track to detect
    and segment: An online multi-object tracker,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 12 352–12 361.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] D. Liu, Y. Cui, W. Tan, and Y. Chen, “Sg-net: Spatial granularity network
    for one-stage video instance segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 9816–9825.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia, “End-to-end
    video instance segmentation with transformers,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 8741–8750.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] L. Hoyer, D. Dai, Y. Chen, A. Koring, S. Saha, and L. Van Gool, “Three
    ways to improve semantic segmentation with self-supervised depth estimation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 11 130–11 140.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] S. Woo, D. Kim, J.-Y. Lee, and I. S. Kweon, “Learning to associate every
    segment for video panoptic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, pp. 2705–2714.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] S. Qiao, Y. Zhu, H. Adam, A. Yuille, and L.-C. Chen, “Vip-deeplab: Learning
    visual perception with depth-aware video panoptic segmentation,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 3997–4008.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Q. Liu, V. Ramanathan, D. Mahajan, A. Yuille, and Z. Yang, “Weakly supervised
    instance segmentation for videos with temporal mask consistency,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2021, pp. 13 968–13 978.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Y. Fu, S. Liu, U. Iqbal, S. De Mello, H. Shi, and J. Kautz, “Learning
    to track instances without video annotations,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 8680–8689.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] H. Lin, R. Wu, S. Liu, J. Lu, and J. Jia, “Video instance segmentation
    with a propose-reduce paradigm,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2021,
    pp. 1739–1748.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] S. Yang, Y. Fang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, and W. Liu,
    “Crossover learning for fast online video instance segmentation,” in *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2021, pp. 8043–8052.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille,
    P. Torr, and S. Bai, “Occluded video instance segmentation,” *arXiv preprint arXiv:2102.01558*,
    2021.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] J. Gao, C. Sun, Z. Yang, and R. Nevatia, “Tall: Temporal activity localization
    via language query,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 5267–5275.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Z. Li, R. Tao, E. Gavves, C. G. Snoek, and A. W. Smeulders, “Tracking
    by natural language specification,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2017, pp. 6495–6503.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] G. E. Hinton, S. Sabour, and N. Frosst, “Matrix capsules with em routing,”
    in *Proc. Int. Conf. Learn. Representations*, 2018.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, “Cross-modal progressive
    comprehension for referring segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2021.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] C. Liang, W. Wang, T. Zhou, J. Miao, Y. Luo, and Y. Yang, “Local-global
    context aware transformer for language-guided video segmentation,” *arXiv preprint
    arXiv:2203.09773*, 2022.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] A. Hu, A. Kendall, and R. Cipolla, “Learning a spatio-temporal embedding
    for video instance segmentation,” in *Proc. Int. Conf. Learn. Representations*,
    2019.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Y. Cui, L. Yan, Z. Cao, and D. Liu, “Tf-blender: Temporal feature blender
    for video object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2021, pp.
    8138–8147.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun,
    “Upsnet: A unified panoptic segmentation network,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 8818–8826.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li, and Y. Yang, “Vspw: A large-scale
    dataset for video scene parsing in the wild,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2021, pp. 4133–4143.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] S. D. Jain and K. Grauman, “Supervoxel-consistent foreground propagation
    in video,” in *Proc. Eur. Conf. Comput. Vis.*, 2014, pp. 656–671.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] C. Xu, S.-H. Hsieh, C. Xiong, and J. J. Corso, “Can humans fly? action
    understanding with multiple classes of actors,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2015, pp. 2264–2273.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, pp. 3192–3199.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2012, pp. 3354–3361.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] A. Milan, L. Leal-Taixé, I. Reid, S. Roth, and K. Schindler, “Mot16:
    A benchmark for multi-object tracking,” *arXiv preprint arXiv:1603.00831*, 2016.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in
    *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, pp. 2232–2241.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] N. Märki, F. Perazzi, O. Wang, and A. Sorkine-Hornung, “Bilateral space
    video segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 743–751.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] D. R. Martin, C. C. Fowlkes, and J. Malik, “Learning to detect natural
    image boundaries using local brightness, color, and texture cues,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, vol. 26, no. 5, pp. 530–549, 2004.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object recognition
    using shape contexts,” *IEEE Trans. Pattern Anal. Mach. Intell.*, no. 4, pp. 509–522,
    2002.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] W. Wang, M. Feiszli, H. Wang, and D. Tran, “Unidentified video objects:
    A benchmark for dense, open-world segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, pp. 10 776–10 785.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional computation
    in neural networks for faster models,” in *Proc. Int. Conf. Learn. Representations*,
    2016.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
