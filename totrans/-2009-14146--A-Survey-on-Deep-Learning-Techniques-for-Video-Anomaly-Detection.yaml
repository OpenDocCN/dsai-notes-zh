- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.14146] A Survey on Deep Learning Techniques for Video Anomaly Detection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.14146] 深度学习技术在视频异常检测中的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.14146](https://ar5iv.labs.arxiv.org/html/2009.14146)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.14146](https://ar5iv.labs.arxiv.org/html/2009.14146)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: Jessie James P. Suarez ²²institutetext: Computer Vision and
    Machine Intelligence Group'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：Jessie James P. Suarez ²²机构文本：计算机视觉与机器智能组
- en: University of the Philippines, Diliman
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 菲律宾大学，迪利曼
- en: '²²email: jpsuarez@up.edu.ph ³³institutetext: Prospero C. Naval, Jr ⁴⁴institutetext:
    Computer Vision and Machine Intelligence Group'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²²电子邮件：jpsuarez@up.edu.ph ³³机构文本：Prospero C. Naval, Jr ⁴⁴机构文本：计算机视觉与机器智能组
- en: University of the Philippines, Diliman
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 菲律宾大学，迪利曼
- en: '⁴⁴email: pcnaval@dcs.upd.edu.ph'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴⁴电子邮件：pcnaval@dcs.upd.edu.ph
- en: A Survey on Deep Learning Techniques for Video Anomaly Detection
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术在视频异常检测中的综述
- en: 'Jessie James P. Suarez    Prospero C. Naval    Jr(Received: date / Accepted:
    date)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Jessie James P. Suarez    Prospero C. Naval    Jr（接收日期：日期 / 接受日期：日期）
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Anomaly detection in videos is a problem that has been studied for more than
    a decade. This area has piqued the interest of researchers due to its wide applicability.
    Because of this, there has been a wide array of approaches that have been proposed
    throughout the years and these approaches range from statistical-based approaches
    to machine learning-based approaches. Numerous surveys have already been conducted
    on this area but this paper focuses on providing an overview on the recent advances
    in the field of anomaly detection using Deep Learning. Deep Learning has been
    applied successfully in many fields of artificial intelligence such as computer
    vision, natural language processing and more. This survey, however, focuses on
    how Deep Learning has improved and provided more insights to the area of video
    anomaly detection. This paper provides a categorization of the different Deep
    Learning approaches with respect to their objectives. Additionally, it also discusses
    the commonly used datasets along with the common evaluation metrics. Afterwards,
    a discussion synthesizing all of the recent approaches is made to provide direction
    and possible areas for future research.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频异常检测是一个研究了十多年的问题。由于其广泛的适用性，这个领域引起了研究人员的兴趣。因此，近年来提出了多种方法，这些方法包括基于统计的方法和基于机器学习的方法。尽管已有许多综述对这一领域进行了探讨，但本文着重于提供有关深度学习在视频异常检测领域的最新进展的概述。深度学习在计算机视觉、自然语言处理等多个人工智能领域中取得了成功应用。然而，本文重点讨论了深度学习如何改进并提供了视频异常检测领域的更多见解。本文提供了不同深度学习方法的分类，依据其目标进行分类。此外，本文还讨论了常用的数据集及常见的评估指标。随后，综合讨论了所有最新方法，以提供未来研究的方向和可能领域。
- en: 'Keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: video understanding video processing anomaly detection deep learning computer
    vision
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 视频理解 视频处理 异常检测 深度学习 计算机视觉
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Surveillance videos have been increasingly present in various establishments
    in order to monitor human activity and prevent crime from happening. It goes without
    saying that there needs to be someone behind watching the videos and signaling
    an alert whenever something different from normal is happening. However, these
    events do not happen very often and that most of the time, the person monitoring
    these videos would see nothing out of the ordinary (Sultani et al., [2018](#bib.bib45)).
    These unusual events can be thought of as anomalies which can be defined as patterns
    that do not conform to what is considered normal. The task of finding these nonconforming
    patterns is called anomaly detection (Chandola et al., [2009](#bib.bib6)). Because
    of this, researchers have been trying to create a robust anomaly detection algorithms
    that can automate the process of monitoring and detection of unusual events in
    surveillance videos. An example of a simple anomaly case can be seen in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Video Anomaly
    Detection") where the normal regions are denoted by $N$ and anomalies are those
    denoted by $O$. As seen in the figure, anomalies tend to clearly lie outside what
    is normal. However, these anomalies can, in fact, be close to normality which
    is illustrated by $O_{2}$
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监控视频在各种机构中越来越普遍，以便监控人类活动并防止犯罪发生。不言而喻，必须有专人观看这些视频，并在出现异常情况时发出警报。然而，这些事件并不常见，大多数时候，监控视频的人不会看到任何异常情况（Sultani
    et al., [2018](#bib.bib45)）。这些异常事件可以被视为异常模式，即与正常情况不符的模式。寻找这些不符合模式的任务称为异常检测（Chandola
    et al., [2009](#bib.bib6)）。因此，研究人员一直在尝试创建强大的异常检测算法，以自动化监控和检测监控视频中异常事件的过程。一个简单的异常情况可以参见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for
    Video Anomaly Detection")，其中正常区域用 $N$ 表示，异常区域用 $O$ 表示。如图所示，异常往往明显地位于正常区域之外。然而，这些异常实际上可能接近正常，如
    $O_{2}$ 所示。
- en: '![Refer to caption](img/17a9670492d71b2ac2715c3e73ecaa2e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17a9670492d71b2ac2715c3e73ecaa2e.png)'
- en: 'Figure 1: Simple Anomaly Case by Chandola et. al. [2009](#bib.bib6)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：由 Chandola 等人 [2009](#bib.bib6) 提出的简单异常情况
- en: 'Anomaly detection is a challenging task due to number of reasons: first, the
    definition of an anomaly may vary from one context to another (Medel and Savakis,
    [2016](#bib.bib31); Sabokrou et al., [2017](#bib.bib41)). Second, the different
    possibilities of what constitute an anomaly might be are boundless (Luo et al.,
    [2019](#bib.bib27)). Third, anomalous data points, especially with real-world
    data, tend to lie closely to what might be defined as normal (Vu et al., [2017](#bib.bib49)).
    Lastly, extracting robust features from the data even if anomalies seldom appear
    (Ribeiro et al., [2018](#bib.bib38)). The mentioned list does not entirely capture
    all of the possible reasons which make the problem hard but these main points
    are what researchers have been considering for the past years when proposing new
    solutions to the problem.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是一个具有挑战性的任务，原因有很多：首先，异常的定义可能因上下文而异（Medel and Savakis, [2016](#bib.bib31);
    Sabokrou et al., [2017](#bib.bib41)）。其次，异常的可能性是无限的（Luo et al., [2019](#bib.bib27)）。第三，异常数据点，尤其是实际数据，往往与可能被定义为正常的情况接近（Vu
    et al., [2017](#bib.bib49)）。最后，从数据中提取鲁棒特征，即使异常很少出现，也很困难（Ribeiro et al., [2018](#bib.bib38)）。上述列表并未完全涵盖所有使问题复杂的原因，但这些主要问题是研究人员在过去几年中提出新解决方案时所考虑的重点。
- en: Around a decade ago, most of the researchers have focused on trajectory-based
    anomaly detection (Jiang et al., [2011](#bib.bib14); Calderara et al., [2011](#bib.bib5);
    Tung et al., [2010](#bib.bib47); Li et al., [2013](#bib.bib19)). The main idea
    is if the objects of interest are not following the learned normal trajectories,
    the video will be tagged as an anomaly. However, one major drawback of this approach
    is occlusion since the approach heavily relies on continuously monitoring the
    objects of interest (Sabokrou et al., [2017](#bib.bib41); Narasimhan and S., [2018](#bib.bib34)).
    Due to these drawbacks, there was an emphasis on using low-level features for
    feature extraction instead (Sabokrou et al., [2017](#bib.bib41)). These approaches
    based on low-level features rely on the use of appearance, motion, and texture
    features (Mehran et al., [2009](#bib.bib32); Li et al., [2014](#bib.bib20); Zhang
    et al., [2016](#bib.bib54); Wang et al., [2018](#bib.bib50); Kim and Grauman,
    [2009](#bib.bib16); Benezeth et al., [2009](#bib.bib3)). Various representations
    have been used in order to represent these aspects of the video such as in the
    approach of Mehran et al. ([2009](#bib.bib32)) where they used social force maps
    to model motion of the crowds. Similarly, pixel-motion properties were used by
    Benezeth et al. ([2009](#bib.bib3)) to model behavior. Meanwhile, Kim and Grauman
    ([2009](#bib.bib16)) made use of optical flows which are then used as inputs to
    the mixture of probabilistic principal component analysis (MPPCA) model, thus,
    creating a more compact feature representation. However, features based on motion
    are not enough which is why there were proposed approaches that make use of both.
    An example is the approach of Li et al. ([2014](#bib.bib20)) where their approach
    makes use of mixture of dynamic textures (MDTs) that utilize temporal normalcy
    and discriminant saliency detectors to model spatial normalcy. Likewise, Zhang
    et al. ([2016](#bib.bib54)) used support vector data description for spatial features
    and optical flow for motion features. In contrast, Wang et al. ([2018](#bib.bib50))
    used spatially localized histogram of optical flows and uniform local gradient
    pattern-based optical flows. Most of these techniques and methods, specifically
    on these ”traditional” approaches, have been discussed in great detail in the
    works of Kaur et al. ([2018](#bib.bib15)); Li and min Cai ([2016](#bib.bib21));
    Popoola and Wang ([2012](#bib.bib36)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大约十年前，大多数研究人员关注的是基于轨迹的异常检测（Jiang et al., [2011](#bib.bib14); Calderara et al.,
    [2011](#bib.bib5); Tung et al., [2010](#bib.bib47); Li et al., [2013](#bib.bib19)）。主要思想是如果感兴趣的物体没有遵循学习到的正常轨迹，则视频会被标记为异常。然而，这种方法的一个主要缺陷是遮挡，因为该方法严重依赖于对感兴趣物体的持续监控（Sabokrou
    et al., [2017](#bib.bib41); Narasimhan and S., [2018](#bib.bib34)）。由于这些缺陷，重点转向了使用低级特征进行特征提取（Sabokrou
    et al., [2017](#bib.bib41)）。这些基于低级特征的方法依赖于外观、运动和纹理特征（Mehran et al., [2009](#bib.bib32);
    Li et al., [2014](#bib.bib20); Zhang et al., [2016](#bib.bib54); Wang et al.,
    [2018](#bib.bib50); Kim and Grauman, [2009](#bib.bib16); Benezeth et al., [2009](#bib.bib3)）。为了表示视频的这些方面，使用了各种表示方法，例如Mehran
    et al. ([2009](#bib.bib32))的方法中，他们使用社会力图来建模人群的运动。同样，Benezeth et al. ([2009](#bib.bib3))利用像素运动属性来建模行为。同时，Kim
    and Grauman ([2009](#bib.bib16))使用了光流，然后将其作为概率主成分分析（MPPCA）模型的输入，从而创建了更紧凑的特征表示。然而，基于运动的特征不够，因此提出了同时利用两者的方法。例如，Li
    et al. ([2014](#bib.bib20))的方法使用了动态纹理混合（MDTs），利用时间正常性和判别显著性检测器来建模空间正常性。同样，Zhang
    et al. ([2016](#bib.bib54))使用了支持向量数据描述来处理空间特征，而光流用于运动特征。相比之下，Wang et al. ([2018](#bib.bib50))使用了空间局部光流直方图和均匀局部梯度模式基础的光流。这些技术和方法，特别是这些“传统”方法，在Kaur
    et al. ([2018](#bib.bib15)); Li and min Cai ([2016](#bib.bib21)); Popoola and
    Wang ([2012](#bib.bib36))的工作中进行了详细讨论。
- en: Despite the proven success of these traditional approaches on benchmark datasets,
    they are still ineffective when used in a different domain. Furthermore, they
    are unable to adapt to anomalies that they have never seen before (Hu et al.,
    [2016](#bib.bib12); Medel and Savakis, [2016](#bib.bib31)). For these reasons,
    recent works have mostly explored the use of Deep Neural Networks for the task
    of anomaly detection. These neural networks automatically learn useful and discriminant
    features on their own which removes the hassle of creating handcrafting features
    (Krizhevsky et al., [2017](#bib.bib17)). This also makes it more adaptive when
    used on different domains. Deep learning was proven to be effective for a variety
    of computer vision tasks such as feature extraction in images (Yan et al., [2016](#bib.bib53)),
    image classification (Krizhevsky et al., [2017](#bib.bib17)), object detection
    (Zoph et al., [2018](#bib.bib58)), video analysis (Mei and Zhang, [2017](#bib.bib33)),
    face detection (Lopes et al., [2017](#bib.bib25)), visual question answering (Malinowski
    et al., [2017](#bib.bib29)) and many other tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些传统方法在基准数据集上已证明成功，但在不同领域中仍然效果不佳。此外，它们无法适应之前未见过的异常（Hu et al., [2016](#bib.bib12);
    Medel and Savakis, [2016](#bib.bib31)）。因此，近期的研究大多探讨了使用深度神经网络进行异常检测。这些神经网络能够自动学习有用且具有区分性的特征，从而省去了手动创建特征的麻烦（Krizhevsky
    et al., [2017](#bib.bib17)）。这也使得它在不同领域的适应性更强。深度学习已被证明在各种计算机视觉任务中有效，如图像中的特征提取（Yan
    et al., [2016](#bib.bib53)），图像分类（Krizhevsky et al., [2017](#bib.bib17)），目标检测（Zoph
    et al., [2018](#bib.bib58)），视频分析（Mei and Zhang, [2017](#bib.bib33)），人脸检测（Lopes
    et al., [2017](#bib.bib25)），视觉问答（Malinowski et al., [2017](#bib.bib29)）以及其他许多任务。
- en: As mentioned previously, there are existing works that have discussed various
    anomaly detection methods for videos (Kaur et al., [2018](#bib.bib15); Li and
    min Cai, [2016](#bib.bib21); Popoola and Wang, [2012](#bib.bib36)). However, due
    to the recent traction in the use of deep learning techniques on this field, the
    goal of this paper is to provide a closer look into these deep learning techniques.
    This entails providing organization as to how the approaches are related to one
    another, the rationale as to why these methods have been proposed, and summarizing
    the conclusions which they have presented in a clear manner. In addition, it would
    also be necessary to discuss datasets and evaluation metrics which have mostly
    been used by these approaches. It would also be insightful to determine how these
    datasets and metrics would scale well when dealing with real-world anomaly detection.
    Different researchers have created different environmental setups making some
    of them incomparable. Thus, the performances of the approaches discussed will
    not be included to avoid confusion and misinterpretation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，已有研究讨论了视频异常检测的各种方法（Kaur et al., [2018](#bib.bib15); Li and min Cai, [2016](#bib.bib21);
    Popoola and Wang, [2012](#bib.bib36)）。然而，由于深度学习技术在这一领域的近期关注，本文的目标是更详细地审视这些深度学习技术。这包括组织这些方法之间的关系，解释为何提出这些方法，并清晰地总结它们所提出的结论。此外，还需要讨论这些方法主要使用的数据集和评估指标。了解这些数据集和指标在处理实际异常检测时的扩展性也是必要的。不同研究者创建了不同的环境设置，使一些方法无法比较。因此，为避免混淆和误解，将不包括讨论中涉及的方法的性能。
- en: 'The paper is organized as follows: the first section serves as an introduction
    to the survey. Second, deep learning anomaly detection techniques will be discussed
    in detail. Third, the mostly used datasets will be tackled. Fourth, the commonly
    used evaluation metrics will be presented. Fifth, a section for discussion is
    allocated to synthesize all of the approaches and datasets mentioned. Lastly,
    the concluding remarks coupled with recommendations as to what directions this
    area of research could possibly go.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的组织结构如下：第一部分为调查的介绍。第二部分将详细讨论深度学习异常检测技术。第三部分将讨论最常用的数据集。第四部分将介绍常用的评估指标。第五部分将分配一个讨论部分，以综合所有提到的方法和数据集。最后，将给出结论性意见，并提出关于该研究领域可能发展的方向的建议。
- en: 2 Deep Learning in Anomaly Detection for Videos
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习在视频异常检测中的应用
- en: 'Deep learning techniques mostly focus on creating new architectures or crafting
    components that can be suitable for a specific problem. Since deep learning methods
    have been successful in a number of varied use cases (Yan et al., [2016](#bib.bib53);
    Krizhevsky et al., [2017](#bib.bib17); Zoph et al., [2018](#bib.bib58); Mei and
    Zhang, [2017](#bib.bib33); Lopes et al., [2017](#bib.bib25); Malinowski et al.,
    [2017](#bib.bib29)), most of these networks or architectures might be similar
    to each other. An example of which would be with Krizhevsky et al. ([2017](#bib.bib17))
    where they used Convolutional Neural Networks for image classification. However,
    almost the same network is also used for face recognition (Lopes et al., [2017](#bib.bib25)).
    Because of this, the presented categories below would group these approaches specifically
    with respect to their final objectives instead of network architecture or learning
    strategy. Examples of these include using reconstruction error or providing an
    anomaly score. In line with this, there are four (4) identified categories namely:
    using reconstruction error or reconstruction-based methods, framing the problem
    as a classification problem, predicting future frames, and computing for an anomaly
    score. A quick summary of all these techniques are provided in Table [1](#S2.T1
    "Table 1 ‣ 2.3 Using Classifiers ‣ 2 Deep Learning in Anomaly Detection for Videos
    ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术主要集中在创建新的架构或设计适合特定问题的组件上。由于深度学习方法在许多不同的应用案例中取得了成功（Yan et al., [2016](#bib.bib53);
    Krizhevsky et al., [2017](#bib.bib17); Zoph et al., [2018](#bib.bib58); Mei and
    Zhang, [2017](#bib.bib33); Lopes et al., [2017](#bib.bib25); Malinowski et al.,
    [2017](#bib.bib29)），这些网络或架构可能彼此相似。例如，Krizhevsky et al. ([2017](#bib.bib17))使用卷积神经网络进行图像分类。然而，几乎相同的网络也用于人脸识别（Lopes
    et al., [2017](#bib.bib25)）。因此，下面介绍的类别将根据最终目标而非网络架构或学习策略来具体分组这些方法。其中包括使用重建误差或提供异常分数。与此一致，识别出了四（4）个类别，即：使用重建误差或基于重建的方法，将问题框定为分类问题，预测未来帧，以及计算异常分数。所有这些技术的快速总结见表
    [1](#S2.T1 "Table 1 ‣ 2.3 Using Classifiers ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")。
- en: 2.1 Using Reconstruction Error
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 使用重建误差
- en: Reconstruction error has already been used in various traditional anomaly detection
    techniques (Popoola and Wang, [2012](#bib.bib36)). The basic assumption of using
    reconstruction error is that the reconstruction error for normal samples would
    be lower since they are closer to the training data. On the other hand, the reconstruction
    error is assumed or expected to be higher for samples which are not normal (Gong
    et al., [2019](#bib.bib8); Sabokrou et al., [2016](#bib.bib40)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差已经在各种传统异常检测技术中使用（Popoola and Wang, [2012](#bib.bib36)）。使用重建误差的基本假设是正常样本的重建误差较低，因为它们更接近训练数据。另一方面，对于不正常的样本，重建误差被假设或期望较高（Gong
    et al., [2019](#bib.bib8); Sabokrou et al., [2016](#bib.bib40)）。
- en: More formally, let $x$ be a video segment or video frame and let $g$ be a neural
    network that reconstructs $x$. The reconstruction error can be defined as a function
    $f$ such that is computes for error between $x$ (the original input), and $g(x)$
    which is the reconstruction Eqn [1](#S2.E1 "In 2.1 Using Reconstruction Error
    ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection"). This concept has been extended recently
    by making use of deep learning techniques to reconstruct various scenes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，设$x$为视频片段或视频帧，$g$为重建$x$的神经网络。重建误差可以定义为函数$f$，用于计算$x$（原始输入）和$g(x)$（重建）的误差。如公式
    [1](#S2.E1 "In 2.1 Using Reconstruction Error ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")所示。这个概念最近通过使用深度学习技术来重建各种场景得到了扩展。
- en: '|  | $e=f(x,g(x))$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $e=f(x,g(x))$ |  | (1) |'
- en: Different from usual feedforward networks, one type of neural network that is
    able to reconstruct input data is called an autoencoder. The autoencoder is a
    neural network that has the capability to encode an input into a more compact
    representation while retaining important and discriminative features. It also
    has the ability to decode this particular encoding back to its original form (Baldi,
    [2011](#bib.bib2)). A visual schematic of an autoencoder is shown in Fig. [2](#S2.F2
    "Figure 2 ‣ 2.1 Using Reconstruction Error ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
    where the diagram illustrates a simple architecture of an encoder where the left-hand
    side is the input to the autoencoder $X$, the middle portion is the encoded representation
    (sometimes called the latent vector or code) of $X$, and the right-hand side is
    the decoded encoding called $X^{\prime}$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的前馈网络不同，一种能够重建输入数据的神经网络称为自编码器。自编码器是一种能够将输入编码成更紧凑表示的神经网络，同时保留重要且具有区分性的特征。它还具有将这种特定编码解码回原始形式的能力（Baldi，[2011](#bib.bib2)）。图[2](#S2.F2
    "图2 ‣ 2.1 使用重建误差 ‣ 2 深度学习在视频异常检测中的应用 ‣ 视频异常检测的深度学习技术综述")展示了自编码器的视觉示意图，其中图示了编码器的简单架构，左侧是自编码器的输入$X$，中间部分是$X$的编码表示（有时称为潜在向量或代码），右侧是称为$X^{\prime}$的解码编码。
- en: '![Refer to caption](img/a5bc362b5c5386500e5d198db10f1b8a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a5bc362b5c5386500e5d198db10f1b8a.png)'
- en: 'Figure 2: Autoencoder Diagram by Michaela Massi ([2019](#bib.bib30)) via Wikimedia
    Commons'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Michaela Massi ([2019](#bib.bib30)) 通过Wikimedia Commons的自编码器图
- en: Most approaches whose goal of using reconstruction error as a means to identify
    anomalies base their method on autoencoders. One such method is introduced by
    Hasan et al. ([2016](#bib.bib10)) where they posited that in comparison to sparse
    coding, the objective function of an autoencoder is more efficient. They have
    also said that it is able to preserve spatio-temporal information while encoding
    dynamics. Their approach made use of combining 2D convolutions to autoencoders
    wherein the 2D convolutions take as input specific raw video segments. Conventionally,
    inputs to a Convolutional Neural Network is a 2D image having the third channel
    as the color channel (Krizhevsky et al., [2017](#bib.bib17)). However, in their
    approach, the third dimension is instead composed of stacked grayscale frames,
    allowing the model to encode both spatial and some temporal information for reconstruction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数以重建误差作为识别异常手段的方法都基于自编码器。其中一种方法由Hasan等人（[2016](#bib.bib10)）提出，他们认为与稀疏编码相比，自编码器的目标函数更高效。他们还表示，自编码器能够在编码动态时保留时空信息。他们的方法结合了2D卷积与自编码器，其中2D卷积以特定的原始视频片段作为输入。传统上，卷积神经网络的输入是一个二维图像，其第三通道是颜色通道（Krizhevsky等，[2017](#bib.bib17)）。然而，在他们的方法中，第三维度由堆叠的灰度帧组成，从而使模型能够编码空间和部分时间信息以进行重建。
- en: Similarly, the work of Medel and Savakis ([2016](#bib.bib31)) also framed the
    problem as a reconstruction problem. The approach makes use of a convolutional
    long short-term memory wherein the Long Short-Term Memory (LSTM) Network is a
    type of neural network that is capable of learning long-term dependencies of the
    data (Hochreiter and Schmidhuber, [1997](#bib.bib11)). Despite not being explicitly
    an autoencoder, their approach also makes use of an encoder-decoder sturture.
    Given an input sequence of video frames, the convolutional long short-term memory
    extracts relevant features along the spatial and temporal dimension in such a
    way that the last time step is used as the encoding. The decoder unravels the
    encoding and then reconstructs the frames which can then be used to compute the
    reconstruction error for anomaly detection.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Medel和Savakis的工作（[2016](#bib.bib31)）也将问题框定为重建问题。该方法利用卷积长短期记忆，其中长短期记忆（LSTM）网络是一种能够学习数据长期依赖关系的神经网络（Hochreiter和Schmidhuber，[1997](#bib.bib11)）。尽管不是显式的自编码器，他们的方法也采用了编码器-解码器结构。给定一个视频帧输入序列，卷积长短期记忆在空间和时间维度上提取相关特征，使得最后一个时间步被用作编码。解码器解开编码并重建帧，然后可以用来计算重建误差以进行异常检测。
- en: The proposed approach of Ribeiro et al. ([2018](#bib.bib38)) closely resembles
    that of Hasan et al. ([2016](#bib.bib10)). The main difference is that the low-level
    features such as optical flow and edges are used as inputs alongside the raw frames.
    In addition, they have also presented how these features affect the convolutional
    autoencoder with regard to detecting anomalies.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Ribeiro 等人（[2018](#bib.bib38)）提出的方法与 Hasan 等人（[2016](#bib.bib10)）的方法非常相似。主要区别在于，低级特征如光流和边缘被用作与原始帧一起的输入。此外，他们还展示了这些特征如何影响卷积自编码器在检测异常方面的表现。
- en: 'Another method was proposed by Sabokrou et al. ([2016](#bib.bib40)) where they
    have used two different autoencoders for the task: one is a regular autoencoder
    and the other is a sparse autoencoder. A sparse autoencoder is an autoencoder
    but has an additional sparsity penalty. This penalty encourages fewer neurons
    to activate. This constraint allows the network to learn relevant information
    without reducing the number of nodes in the hidden layers. Their approach involves
    two steps, the first step is to compute the sparsity value from cubic patches
    of the videos, if it is below a specific sparsity threshold, another set of patches
    are extracted around that patch for reconstruction.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Sabokrou 等人（[2016](#bib.bib40)）提出了另一种方法，他们为任务使用了两种不同的自编码器：一种是常规自编码器，另一种是稀疏自编码器。稀疏自编码器是一种自编码器，但具有额外的稀疏性惩罚。这个惩罚鼓励更少的神经元激活。这种约束使网络能够学习相关信息，而不减少隐藏层中的节点数量。他们的方法包括两个步骤，第一个步骤是从视频的立方体补丁中计算稀疏性值，如果低于特定的稀疏性阈值，则在该补丁周围提取另一组补丁进行重建。
- en: According to Zhao et al. ([2017](#bib.bib55)), the approach of Hasan et al.
    ([2016](#bib.bib10)) which makes use of temporal cuboids by stacking frames in
    the third dimension, does not necessarily retain the temporal information. Based
    on their work, a reason for this is that 2D convolutions operate on the frames
    spatially. Putting this in the perspective of the approach of Hasan et al. ([2016](#bib.bib10)),
    the third channel is represented along each of the channels of the first feature
    map which rarely preserves temporal information. To solve this, Zhao et al. ([2017](#bib.bib55))
    proposed the use of 3D convolutions as a means to retain temporal information
    during the convolution process. Since it is data intensive, they have also applied
    data augmentation to increase their samples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Zhao 等人（[2017](#bib.bib55)）的研究，Hasan 等人（[2016](#bib.bib10)）利用时间立方体通过在第三维度上堆叠帧的方法，并不一定能保留时间信息。根据他们的研究，原因是
    2D 卷积在空间上对帧进行操作。将这一点放在 Hasan 等人（[2016](#bib.bib10)）的方法的视角下，第三通道沿着第一个特征图的每个通道表示，这很少保留时间信息。为了解决这个问题，Zhao
    等人（[2017](#bib.bib55)）提出使用 3D 卷积来保留卷积过程中的时间信息。由于数据密集，他们还应用了数据增强以增加样本。
- en: As claimed in the work of Zhou et al. ([2019](#bib.bib56)), one weakness of
    the approach of Medel and Savakis ([2016](#bib.bib31)) is that spatial and temporal
    aspects of the inputs are encoded separately by the convolutions and the long
    short-term memory. This implies a broken relationship between the two during the
    encoding process. Furthermore, it was also stated by Zhou et al. ([2019](#bib.bib56))
    that the approach proposed by Medel and Savakis ([2016](#bib.bib31)) was not able
    to make use of existing pre-trained networks. These networks have shown remarkably
    improved performances once it has been applied to other domains. Hence, their
    proposed method makes use of a feature learning subnetwork that combines motion
    and appearance features into an image. Afterwards, it is then used as an input
    to a pretrained network for feature extraction. Moreover, they have proposed a
    novel subnetwork called sparse coding to network (SC2Net) to compute for the sparsity
    loss and reconstruction loss from the extracted features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人（[2019](#bib.bib56)）在他们的工作中声称，Medel 和 Savakis（[2016](#bib.bib31)）的方法的一个弱点是空间和时间方面的输入被卷积和长短期记忆分别编码。这意味着在编码过程中两者之间的关系被破坏。此外，Zhou
    等人（[2019](#bib.bib56)）还指出，Medel 和 Savakis（[2016](#bib.bib31)）提出的方法无法利用现有的预训练网络。这些网络在应用于其他领域时显示出显著的性能提升。因此，他们提出的方法利用了一个特征学习子网络，将运动和外观特征结合到图像中。随后，这些图像被用作预训练网络的输入进行特征提取。此外，他们还提出了一种新的子网络，称为稀疏编码到网络（SC2Net），用于计算从提取特征中获得的稀疏性损失和重建损失。
- en: Among all of the approaches, Gong et al. ([2019](#bib.bib8)) have posited that
    most of the works on reconstruction generally assume that the anomalous instances
    will have a high reconstruction error. Based on these works, this assumption does
    not necessarily hold true mainly because there might be instances where an autoencoder
    is able to generalize well. This poses a problem since it might accurately reconstruct
    anomalous instances as well. To mitigate this problem, they have introduced a
    new autoencoder which has the capability to store encodings into memory. The main
    difference from previous approaches is that instead of directly feeding the encoding
    to the decoder, the encoding is treated as a query. This query is expected to
    return closest normal patterns in memory which is instead used for decoding. In
    the event that an anomaly is to be reconstructed, it would have a high reconstruction
    error because the memory only has normal memory items.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，Gong et al. ([2019](#bib.bib8)) 提出了大多数重建工作通常假设异常实例将具有高重建误差。基于这些工作，这种假设并不一定成立，主要是因为可能存在自编码器能够很好地进行泛化的情况。这就导致了一个问题，因为它也可能准确地重建异常实例。为了缓解这个问题，他们引入了一种新型自编码器，该自编码器具有将编码存储到内存中的能力。与以往方法的主要区别在于，不是直接将编码送入解码器，而是将编码视为查询。此查询预期返回内存中最接近的正常模式，这些模式将用于解码。如果要重建异常，它将具有高重建误差，因为内存中仅包含正常内存项。
- en: 2.2 Using Future Frame Prediction
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 使用未来帧预测
- en: A different perspective on the problem was presented by Liu et al. ([2018](#bib.bib24)).
    They support the claim of Gong et al. ([2019](#bib.bib8)) stating that autoencoders
    might also accurately reconstruct anomalous frames. Since anomalies can be viewed
    as events that do not conform with certain expectations, Liu et al. ([2018](#bib.bib24))
    suggested a frame prediction approach might be a more natural way to view the
    problem. Mathematically speaking, given $x_{t}$ which is the video segment or
    frame $x$ at time $t$, future frame prediction can be expressed as a function
    $h$ predicting the next segment as shown in Eqn [2](#S2.E2 "In 2.2 Using Future
    Frame Prediction ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Liu et al. ([2018](#bib.bib24)) 提出了对问题的不同看法。他们支持 Gong et al. ([2019](#bib.bib8))
    的观点，即自编码器也可能准确地重建异常帧。由于异常可以视为不符合某些期望的事件，Liu et al. ([2018](#bib.bib24)) 建议帧预测方法可能是更自然的视角。数学上，给定
    $x_{t}$，即时间 $t$ 的视频片段或帧 $x$，未来帧预测可以表示为函数 $h$ 预测下一个片段，如公式 [2](#S2.E2 "In 2.2 Using
    Future Frame Prediction ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A
    Survey on Deep Learning Techniques for Video Anomaly Detection") 所示。
- en: '|  | $x_{t+1}=h(x_{t})$ |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t+1}=h(x_{t})$ |  | (2) |'
- en: In deep learning, there is a specific type of neural network is used for generating
    new data with the same statistics as the training data. This network which is
    called generative adversarial network (GAN) (Goodfellow et al., [2014](#bib.bib9)).
    This architecture has two main (2) parts. The first one is a generator whose job
    is to mimic the original data distribution. Meanwhile, the second network is called
    a discriminator that gives a probability of whether or not the input is coming
    from the generator.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，有一种特定类型的神经网络用于生成具有与训练数据相同统计特性的全新数据。这种网络被称为生成对抗网络（GAN）（Goodfellow et al.,
    [2014](#bib.bib9)）。这种架构主要有两个部分。第一个部分是生成器，其任务是模仿原始数据分布。与此同时，第二个网络被称为判别器，用于判断输入是否来自生成器。
- en: The approach of Liu et al. ([2018](#bib.bib24)) made use of a generator-discriminator
    structure, likened to that of a generative adversarial network. They used the
    U-Net architecture (Ronneberger et al., [2015](#bib.bib39)) for future frame prediction
    as the generator because of its exemplary performance in image-to-image translation.
    While the discriminator at the end of the network determines whether or not the
    predicted frame is anomalous.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Liu et al. ([2018](#bib.bib24)) 的方法利用了类似于生成对抗网络的生成器-判别器结构。他们使用了 U-Net 架构（Ronneberger
    et al., [2015](#bib.bib39)）作为生成器进行未来帧预测，因为它在图像到图像翻译中表现出色。而网络末端的判别器用于确定预测的帧是否异常。
- en: Some works on reconstruction also have the capability for predicting future
    frames such as in the work of Hasan et al. ([2016](#bib.bib10)). Their approach
    has the ability to encode both spatial and temporal aspects of the video by allowing
    the autoencoder to learn it from a sequence of video segments (discussed in more
    detail in Section [2.1](#S2.SS1 "2.1 Using Reconstruction Error ‣ 2 Deep Learning
    in Anomaly Detection for Videos ‣ A Survey on Deep Learning Techniques for Video
    Anomaly Detection")). It is because of this exact same reason that it can also
    predict future and past frames given a center frame. Based on their methodology,
    by padding the center frame with zero values, their model can extrapolate the
    near future and near past of the center frame.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重建工作的研究还具有预测未来帧的能力，例如Hasan等人的工作（[2016](#bib.bib10)）。他们的方法能够通过允许自编码器从一系列视频片段中学习来编码视频的空间和时间方面（在[2.1](#S2.SS1
    "2.1 Using Reconstruction Error ‣ 2 Deep Learning in Anomaly Detection for Videos
    ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")节中有更详细的讨论）。正因为如此，它也能在给定中心帧的情况下预测未来和过去的帧。基于他们的方法，通过用零值填充中心帧，他们的模型可以推断出中心帧的近期未来和近期过去。
- en: Moreover, some of the previous works actually leverage future frame prediction
    in the process of reconstructing the current frame. An example of this is the
    work of Zhao et al. ([2017](#bib.bib55)) where their network learns the future
    frames along with the task of reconstruction in a different branch of the network.
    Similarly, Medel and Savakis ([2016](#bib.bib31)) also has a separate branch in
    parallel that learns how to predict the future. Despite their similarities, they
    both have big differences as to how future frame prediction is used. Medel and
    Savakis ([2016](#bib.bib31)) makes use of future frame prediction to identify
    interest points within the video. On the contrary, in the approach of Zhao et al.
    ([2017](#bib.bib55)), the future frame is actually included in the computation
    of the loss to guide the network to extract temporal features. In addition, it
    is also included in the reconstruction score which combines the prediction loss
    and the reconstruction loss.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些先前的工作实际上在重建当前帧的过程中利用了未来帧预测。例如，Zhao等人（[2017](#bib.bib55)）的工作中，他们的网络在网络的不同分支中学习未来帧以及重建任务。类似地，Medel和Savakis（[2016](#bib.bib31)）也有一个并行的分支学习如何预测未来。尽管它们有相似之处，但在未来帧预测的使用方式上存在很大差异。Medel和Savakis（[2016](#bib.bib31)）利用未来帧预测来识别视频中的兴趣点。相反，在Zhao等人（[2017](#bib.bib55)）的方法中，未来帧实际上被包含在损失计算中，以指导网络提取时间特征。此外，它还被包含在结合预测损失和重建损失的重建评分中。
- en: 2.3 Using Classifiers
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用分类器
- en: Despite the sophisticated methods that rely mainly on reconstruction loss and
    future frame prediction, there are also still a handful of approaches that cast
    the problem as a classification problem. The classification problem can be viewed
    as a function $j$ that takes as its input a frame or video segment $x$ whose output
    $y$ is a class or category as seen in Eqn [3](#S2.E3 "In 2.3 Using Classifiers
    ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection").
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多主要依赖重建损失和未来帧预测的复杂方法，但仍然有一些方法将问题视为分类问题。分类问题可以被看作是一个函数 $j$，其输入为帧或视频片段 $x$，输出
    $y$ 是一个类或类别，如方程[3](#S2.E3 "In 2.3 Using Classifiers ‣ 2 Deep Learning in Anomaly
    Detection for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly
    Detection")所示。
- en: '|  | $y=j(x),y\in\mathbb{R}$ |  | (3) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=j(x),y\in\mathbb{R}$ |  | (3) |'
- en: Because of imbalanced datasets, these methods focus mostly on how to create
    compact, efficient, and robust features. The approach of Sabokrou et al. ([2017](#bib.bib41))
    tries to solve this problem by proposing a competitive cascade of deep neural
    networks. The cascade is composed of two stages where the first stage is a small
    stack of autoencoders which hierarchically models the normality of the video patches.
    The other one is a Convolutional Neural Network which takes as input video patches
    that the autoencoders could not handle and would need further probing. The classifier
    used for the approach is a Gaussian Classifier.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集不平衡，这些方法主要关注如何创建紧凑、高效和鲁棒的特征。Sabokrou等人（[2017](#bib.bib41)）通过提出一种竞争级联的深度神经网络来解决这个问题。级联由两个阶段组成，第一个阶段是一个小型自编码器堆栈，逐级建模视频片段的正常性。另一个阶段是卷积神经网络，它处理自编码器无法处理并需要进一步探测的视频片段。该方法使用的分类器是高斯分类器。
- en: 'Table 1: Summary of Methods and Contributions'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 方法及贡献总结'
- en: '| Year | Author | Type | Main Contribution |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 作者 | 类型 | 主要贡献 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 2016 | Medel et. al |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Medel 等 |'
- en: '&#124; Reconstruction &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建 &#124;'
- en: '&#124; & Future Frame &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 未来帧 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Convolutional Long &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积长 &#124;'
- en: '&#124; Short-Term Memory &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 短期记忆 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2016 | Hasan et. al. | Reconstruction |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Hasan 等 | 重建 |'
- en: '&#124; Fully 2D Convolutional &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全 2D 卷积 &#124;'
- en: '&#124; Autoencoder &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2016 | Sabokrou et. al. | Reconstruction |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Sabokrou 等 | 重建 |'
- en: '&#124; Sparse Autoencoder + &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稀疏自编码器 + &#124;'
- en: '&#124; Autoencoder &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2016 | Hu et. al. | Scoring |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Hu 等 | 评分 |'
- en: '&#124; Deep Neural Network + &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度神经网络 + &#124;'
- en: '&#124; Slow Feature Analysis &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 慢特征分析 &#124;'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2017 | Narasimhan et. al. | Classification |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Narasimhan 等 | 分类 |'
- en: '&#124; Sparse Denoising &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稀疏去噪 &#124;'
- en: '&#124; Autoencoders &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2017 | Sabokrou et. al. | Classification |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Sabokrou 等 | 分类 |'
- en: '&#124; Cascade of Deep &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度级联 &#124;'
- en: '&#124; Convolutional Neural &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积神经 &#124;'
- en: '&#124; Networks + &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 + &#124;'
- en: '&#124; Autoencoders &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2017 | Zhao et. al. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Zhao 等 |'
- en: '&#124; Reconstruction &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建 &#124;'
- en: '&#124; & Future Frame &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 未来帧 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Spatiotemporal &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时空 &#124;'
- en: '&#124; Autoencoder &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2018 | Sabokrou et. al. | Classification | Deep-Anomaly |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Sabokrou 等 | 分类 | 深度异常 |'
- en: '| 2018 | Sultani et. al. | Scoring |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Sultani 等 | 评分 |'
- en: '&#124; Multiple-Instance &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多实例 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2018 | Ribeiro et. al. | Reconstruction |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Ribeiro 等 | 重建 |'
- en: '&#124; Low-level Features + &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低级特征 + &#124;'
- en: '&#124; 2D Convolutional &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2D 卷积 &#124;'
- en: '&#124; Autoencoder &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2018 | Liu et. al. | Future Frame |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Liu 等 | 未来帧 |'
- en: '&#124; Future Frame using &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未来帧使用 &#124;'
- en: '&#124; U-Net &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net &#124;'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Landi et. al. | Scoring |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Landi 等 | 评分 |'
- en: '&#124; Localization before &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 定位之前 &#124;'
- en: '&#124; Feature Extraction &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征提取 &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Sabzailan et. al. | Scoring |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Sabzailan 等 | 评分 |'
- en: '&#124; Traditional + Deep &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传统 + 深度 &#124;'
- en: '&#124; Learning Features &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习特征 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Zhu et. al. | Scoring |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Zhu 等 | 评分 |'
- en: '&#124; Optical Flow as inputs &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光流作为输入 &#124;'
- en: '&#124; to Multiple-Instance &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 到多实例 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Zhou et. al. | Reconstruction |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Zhou 等 | 重建 |'
- en: '&#124; AnomalyNet: a unified &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AnomalyNet: 统一的 &#124;'
- en: '&#124; approach &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Gong et. al. | Reconstruction |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Gong 等 | 重建 |'
- en: '&#124; Autoencoder + memory &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 + 记忆 &#124;'
- en: '&#124; module + attention-based &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模块 + 基于注意力的 &#124;'
- en: '&#124; addressing &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解决 &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Lin. et. al. | Scoring |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Lin 等 | 评分 |'
- en: '&#124; Multiple-Instance &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多实例 &#124;'
- en: '&#124; Learning + Social Force &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 + 社会力 &#124;'
- en: '&#124; Maps &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 映射 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Santos et. al. | Classification |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Santos 等 | 分类 |'
- en: '&#124; Transfer Learning + &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移学习 + &#124;'
- en: '&#124; Transfer Component &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迁移组件 &#124;'
- en: '&#124; Analysis &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分析 &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Luo et. al. | Scoring |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Luo 等 | 评分 |'
- en: '&#124; Sparse Coding-inspired &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稀疏编码启发的 &#124;'
- en: '&#124; Deep Neural Network &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度神经网络 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Ionescu et. al. | Classification |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Ionescu 等 | 分类 |'
- en: '&#124; Object-Centric &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以对象为中心的 &#124;'
- en: '&#124; Convolutional &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '&#124; Autoencoders &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 | Xu et. al. | Classification |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Xu 等 | 分类 |'
- en: '&#124; Adaptive Intra-Frame &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应帧内 &#124;'
- en: '&#124; Classification Network &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类网络 &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2020 | Fan et. al. | Scoring |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Fan 等 | 评分 |'
- en: '&#124; Gaussian Mixture &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高斯混合 &#124;'
- en: '&#124; Fully Convolutional &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全卷积 &#124;'
- en: '&#124; Variational Autoencoders &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变分自编码器 &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: On the other hand, Narasimhan and S. ([2018](#bib.bib34)) proposed a method
    that makes use of local and global descriptors whose aim is utilize both spatial
    and temporal domains. For local features, they made use of an image similarity
    metric on the video cubic patches to represent the temporal and spatial features.
    Meanwhile, the global features are represented by the latent vector of the trained
    autoencoders. After creating both local and global features, it is then fed to
    an autoencoder which selects important features that are discriminative enough
    for anomaly detection. Finally, these features are fed into Gaussian classifiers
    separately for local and global descriptors and then combined to detect anomalies.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Narasimhan 和 S.（[2018](#bib.bib34)）提出了一种利用局部和全局描述符的方法，旨在利用空间和时间域。对于局部特征，他们使用了视频立方体补丁上的图像相似性度量来表示时间和空间特征。同时，全局特征由训练后的自编码器的潜在向量表示。在创建了局部和全局特征后，这些特征被输入到自编码器中，自编码器选择了足够具有判别力的重要特征用于异常检测。最后，这些特征分别输入到高斯分类器中用于局部和全局描述符，然后将结果结合起来以检测异常。
- en: 'Most of the above mentioned methods, even those in the previous sections, make
    use of Convolutional Neural Networks. However, Sabokrou et al. ([2018](#bib.bib42))
    has mentioned problems with regard to using these networks, one of which is that
    these networks are too inefficient for patch-based methods. Examples of approaches
    that made use of patches are as follows: Narasimhan and S. ([2018](#bib.bib34));
    Sabokrou et al. ([2016](#bib.bib40)); Sabzalian et al. ([2019](#bib.bib43)); Sabokrou
    et al. ([2018](#bib.bib42)); Medel and Savakis ([2016](#bib.bib31)). For this
    reason, they have proposed a possible solution to the problem which makes use
    of the discriminative power of a pre-trained model without having to tweak it
    Sabokrou et al. ([2018](#bib.bib42)). More specifically, they use the intermediate
    layer to generate the features that will be fed to a Gaussian Classifier. In the
    event that a low confidence is generated by the classifier, it is sent to another
    convolutional layer on top of the best intermediate layer for further probing.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述大多数方法，甚至包括前面的部分，都利用了卷积神经网络。然而，Sabokrou 等人（[2018](#bib.bib42)）提到了使用这些网络的问题，其中之一是这些网络对于基于补丁的方法效率过低。使用补丁的方法的示例如下：Narasimhan
    和 S.（[2018](#bib.bib34)）；Sabokrou 等人（[2016](#bib.bib40)）；Sabzalian 等人（[2019](#bib.bib43)）；Sabokrou
    等人（[2018](#bib.bib42)）；Medel 和 Savakis（[2016](#bib.bib31)）。因此，他们提出了一种可能的解决方案，该方案利用了预训练模型的判别能力，而无需调整模型（Sabokrou
    等人，[2018](#bib.bib42)）。更具体来说，他们使用中间层生成特征，并将其输入到高斯分类器中。如果分类器生成了低置信度的结果，这些结果将被送往最佳中间层之上的另一个卷积层进行进一步探测。
- en: Similar to Sabokrou et al. ([2018](#bib.bib42)), the proposed approach of [dos
    Santos] et al. ([2019](#bib.bib44)) took advantage of the available pre-trained
    models. They have investigated the generalization of feature spaces of Convolutional
    Neural Networks without requiring additional labels. In their experiments, they
    used transfer component analysis (Pan et al., [2011](#bib.bib35)) which attempts
    to learn a certain subspace that is shared by different domains. They have concluded
    that generalization through different domains.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Sabokrou 等人（[2018](#bib.bib42)），[dos Santos] 等人（[2019](#bib.bib44)）提出的方法利用了现有的预训练模型。他们研究了卷积神经网络特征空间的泛化，而无需额外的标签。在他们的实验中，他们使用了迁移成分分析（Pan
    等人，[2011](#bib.bib35)），该方法试图学习一个不同领域共享的子空间。他们得出结论，通过不同领域的泛化。
- en: Most of the methods mentioned previously make use of extracting either global
    or local features without taking the objects of interest into account. The approach
    of Ionescu et al. ([2019](#bib.bib13)) makes use of a single-shot detector (SSD)
    (Liu et al., [2016](#bib.bib23)) on each frame of the video. After isolating the
    objects, a convolutional autoencoder is used to learn deep unsupervised features
    thereby allowing the algorithm to focus on the objects in the scene. Furthermore,
    they have instead casted the problem of anomaly detection as a multi-class classification
    problem rather than an unbalanced binary classification problem or a one-class
    problem. To generate the artificial classes, they have used clustering on the
    set of features generated by the convolutional autoencoder where each cluster
    represents a different type of normality. A one-versus-rest classifier is trained
    which discriminates between the clusters. If the highest classification score
    is negative, meaning the sample does not belong to any cluster, it is tagged as
    anomalous.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的大多数方法都利用了提取全局或局部特征，但没有考虑感兴趣的对象。Ionescu 等人（[2019](#bib.bib13)）的方法在视频的每一帧上使用了单次检测器（SSD）（Liu
    等人，[2016](#bib.bib23)）。在隔离对象后，使用卷积自编码器学习深度无监督特征，从而使算法能够专注于场景中的对象。此外，他们将异常检测问题重新定义为多类分类问题，而不是不平衡的二分类问题或单类问题。为了生成人工类别，他们对卷积自编码器生成的特征集进行了聚类，每个聚类表示不同类型的正常性。训练了一个一对多分类器来区分这些聚类。如果最高分类分数为负，意味着样本不属于任何聚类，则标记为异常。
- en: Similar to Ionescu et al. ([2019](#bib.bib13)), Xu et al. ([2020](#bib.bib52))
    also framed the problem as a multi-class classification problem as opposed to
    either a one-class or a binary classification problem. In line with this, they
    also took note of the fact that most of the previous approaches were able to effectively
    identify subregions representations of anomalies. However, for most of the approaches,
    there is a wide array of inputs and outputs such as optical flows, patches, or
    gradients. This inspired the approach of Xu et al. ([2020](#bib.bib52)) which
    tries to unify all of these approach by creating a network called the adaptive
    intraframe classification network that takes the raw inputs, computes for motion
    and appearance features, and determines whether or not the sample is anomalous.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Ionescu 等人（[2019](#bib.bib13)）类似，Xu 等人（[2020](#bib.bib52)）也将问题框定为多类分类问题，而非单类或二分类问题。与此一致的是，他们还注意到，大多数先前的方法能够有效识别异常的子区域表示。然而，对于大多数方法，输入和输出的种类繁多，如光流、图像块或梯度。这启发了
    Xu 等人（[2020](#bib.bib52)）的方法，他们尝试通过创建一个名为自适应帧内分类网络的网络来统一所有这些方法，该网络接受原始输入，计算运动和外观特征，并确定样本是否异常。
- en: 2.4 Using Scoring Methods
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 使用评分方法
- en: Some researchers have instead, framed the problem as a regression problem wherein
    the goal is to provide an anomaly score which will then be used as a means to
    determine whether or not a video segment or a frame is anomalous (Landi et al.,
    [2019](#bib.bib18); Sultani et al., [2018](#bib.bib45)). The scoring methods can
    be viewed as a function $k$ such that it takes a video segment or frame $x$ as
    its input. It outputs a real number $z$ representing the anomaly score as seen
    in Eqn [4](#S2.E4 "In 2.4 Using Scoring Methods ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究者则将问题框定为回归问题，其目标是提供异常分数，然后用作确定视频片段或帧是否异常的依据（Landi 等人，[2019](#bib.bib18)；Sultani
    等人，[2018](#bib.bib45)）。评分方法可以视为一个函数 $k$，它以视频片段或帧 $x$ 作为输入。它输出一个实数 $z$ 代表异常分数，如公式
    [4](#S2.E4 "In 2.4 Using Scoring Methods ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
    所示。
- en: '|  | $z=k(x),z\in\mathbb{R}$ |  | (4) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $z=k(x),z\in\mathbb{R}$ |  | (4) |'
- en: The proposed approach of Hu et al. ([2016](#bib.bib12)), makes use of their
    novel sum squared derivative to score the features generated by their approach.
    This basically determines if the sequence of frames is anomalous. Prior to their
    scoring method, they combined both deep learning and slow feature analysis (Wiskott
    and Sejnowski, [2002](#bib.bib51)) in order to learn semantic-level representations
    given raw video frames. It is also worth noting that their approach has an online
    variant, thereby making their approach adaptive.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Hu 等人 ([2016](#bib.bib12)) 提出的方案利用其新颖的平方导数和来对其方法生成的特征进行评分。这基本上确定了帧序列是否异常。在他们的评分方法之前，他们结合了深度学习和慢特征分析（Wiskott
    和 Sejnowski，[2002](#bib.bib51)），以便在给定原始视频帧的情况下学习语义级别的表示。值得注意的是，他们的方法还有一个在线变体，从而使他们的方法具有适应性。
- en: The approach of Sultani et al. ([2018](#bib.bib45)) made use of a multiple instance
    learning to identify anomalies in video segments based on weakly-labelled videos
    (labels are on a video-level and not frame-level). Their approach uses C3D, a
    3D Convolutional Neural Network that learns spatiotemporal features by exposing
    the model to large-scale video datasets (Tran et al., [2015](#bib.bib46)). These
    spatiotemporal features are then fed to fully connected layers for generating
    the anomaly score. The backpropagation of the error is guided by the principle
    of multiple instance learning, allowing the model to learn anomalous segments
    despite having weak labels. This idea was taken up by Zhu and Newsam ([2019](#bib.bib57))
    where, instead of using C3D, they made use of computing for the optical flows
    which are then fed to a temporal augmented network. Their proposed approach also
    makes use of an attention mechanism (Vaswani et al., [2017](#bib.bib48)) that
    allows the network to identify which features are important to look at. Similarly,
    Lin et al. ([2019](#bib.bib22)) also built upon this idea where they proposed
    a dual-branch network that incorporates motion into the initial network introduced
    by Sultani et al. ([2018](#bib.bib45)). The approach of Lin et al. ([2019](#bib.bib22))
    adapts the same network of Sultani et al. ([2018](#bib.bib45)) as the first branch
    with a modification wherein an attention module (Vaswani et al., [2017](#bib.bib48))
    was added after the feature extraction layer. The second branch is similar in
    structure as the first branch except that it takes as an input social force maps
    (Mehran et al., [2009](#bib.bib32)) computed from the raw images to represent
    motion.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Sultani 等人 ([2018](#bib.bib45)) 的方法利用了多实例学习来识别基于弱标签视频（标签在视频级别而非帧级别）的异常情况。他们的方法使用了
    C3D，这是一种三维卷积神经网络，通过将模型暴露于大规模视频数据集（Tran 等人，[2015](#bib.bib46)）来学习时空特征。这些时空特征随后被送入全连接层以生成异常分数。误差的反向传播受到多实例学习原理的指导，使得模型能够在标签较弱的情况下学习异常片段。这个思想被
    Zhu 和 Newsam ([2019](#bib.bib57)) 继承，他们不使用 C3D，而是计算光流，并将其送入一个时间增强网络。他们提出的方法还利用了一种注意机制（Vaswani
    等人，[2017](#bib.bib48)），使网络能够识别哪些特征重要。类似地，Lin 等人 ([2019](#bib.bib22)) 也在这个思想基础上进行扩展，他们提出了一个双分支网络，将运动信息引入到
    Sultani 等人 ([2018](#bib.bib45)) 提出的初始网络中。Lin 等人 ([2019](#bib.bib22)) 的方法将 Sultani
    等人 ([2018](#bib.bib45)) 的网络作为第一个分支，并在特征提取层之后添加了一个注意模块（Vaswani 等人，[2017](#bib.bib48)）。第二个分支的结构与第一个分支类似，只是它以从原始图像计算得到的社会力图（Mehran
    等人，[2009](#bib.bib32)）作为输入来表示运动。
- en: Meanwhile, the approach of Sabzalian et al. ([2019](#bib.bib43)) makes full
    use of the effectiveness of traditional and deep learning features for anomaly
    detection. Their proposed approach starts by identifying the foreground of the
    video by using optical flows. Once the regions of interest have been identified,
    a pre-trained Convolutional Neural Network is used to extract features alongside
    computing for traditional features like histogram of gradients and histogram of
    optical flows. These three features are combined by making use of an iteratively
    weighted nonnegative matrix factorization method (Sabzalian et al., [2019](#bib.bib43)).
    Afterwards, the features are clustered and the discrimination of whether or not
    the sample is an anomaly will be done via a voting system.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，Sabzalian 等人（[2019](#bib.bib43)）的方法充分利用了传统和深度学习特征在异常检测中的有效性。他们提出的方法首先通过使用光流来识别视频的前景。一旦确定了感兴趣的区域，使用预训练的卷积神经网络提取特征，同时计算传统特征，如梯度直方图和光流直方图。这三种特征通过迭代加权的非负矩阵分解方法（Sabzalian
    等人，[2019](#bib.bib43)）进行结合。之后，这些特征被聚类，样本是否为异常的判别将通过投票系统完成。
- en: Aside from framing the problem as a regression problem, Landi et al. ([2019](#bib.bib18))
    proposed to make use of locality when computing for the anomaly score. The approach
    is similar to that of Sultani et al. ([2018](#bib.bib45)) except that their approach
    extracts a tube from the video which in a way localizes and adjusts the level
    of granularity when extracting features. From their experiments, they have shown
    that locality or, more specifically, zoning in on one region where the anomalous
    event takes place actually helps the method to accurately compute anomaly scores.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将问题框架化为回归问题，Landi 等人（[2019](#bib.bib18)）建议在计算异常分数时利用局部性。这种方法类似于 Sultani 等人（[2018](#bib.bib45)）的方法，只不过他们的方法从视频中提取一个管道，从而在提取特征时在某种程度上局部化并调整粒度水平。通过实验，他们展示了局部性，或者更具体地说，集中在发生异常事件的一个区域，实际上有助于该方法准确计算异常分数。
- en: Sparse coding for anomaly detection is an approach that learns a dictionary
    which attempts to encodes all normal events (Lu et al., [2013](#bib.bib26)). By
    revisiting sparse coding, Luo et al. ([2019](#bib.bib27)) proposed temporally-coherent
    sparse coding to model the coherence between neighboring events for normal frames.
    These temporal features are then combined with spatial features learn from pre-trained
    networks across different scales for a normality score. Note that the features
    extracted pass through a Stacked Recurrent Neural Network autoencoder to generate
    the final features for scoring.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏编码用于异常检测是一种学习字典的方式，它试图对所有正常事件进行编码（Lu 等人，[2013](#bib.bib26)）。通过重新审视稀疏编码，Luo
    等人（[2019](#bib.bib27)）提出了时间一致性稀疏编码，以建模正常帧之间相邻事件的连贯性。这些时间特征随后与从不同尺度的预训练网络中学习到的空间特征结合，用于生成正常性分数。请注意，提取的特征通过堆叠递归神经网络自编码器传递，以生成用于评分的最终特征。
- en: Past works demonstrated the effectiveness of autoencoders and that normal samples
    can be associated with at least one Gaussian Mixture Model. Because of this, Fan
    et al. ([2020](#bib.bib7)) proposed an end-to-end neural network called the Gaussian
    Mixture Fully Convolutional Variational Autoencoder to model anomalies and to
    predict them. Their model is trained on image and dynamic flow patches wherein
    both of them are separately fed into different networks. This basically captures
    separate motion and appearance features. Afterwards, joint probabilities are used
    to detect both appearance and motion anomalies via a sample energy-based method.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的研究展示了自编码器的有效性，并且正常样本可以与至少一个高斯混合模型相关联。基于此，Fan 等人（[2020](#bib.bib7)）提出了一种称为高斯混合全卷积变分自编码器的端到端神经网络，用于建模和预测异常。他们的模型在图像和动态流补丁上进行训练，其中这两者分别输入到不同的网络中。这基本上捕获了单独的运动和外观特征。随后，联合概率用于通过样本基于能量的方法检测外观和运动异常。
- en: 3 Existing Benchmark Datasets
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 现有基准数据集
- en: This section discusses in detail the publicly available datasets for the task
    of anomaly detection. There are a few papers which have created their own datasets
    but most of the works have tried to at least use one benchmark dataset in order
    to evaluate the performance of their proposed approaches with respect to previously
    published works. A summary presenting a high-level view of all of the different
    datasets included in this subsection can be seen in Table [2](#S3.T2 "Table 2
    ‣ 3 Existing Benchmark Datasets ‣ A Survey on Deep Learning Techniques for Video
    Anomaly Detection"). Note that the dataset links are added as footnotes for reference.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细讨论了用于异常检测任务的公开数据集。虽然有一些论文创建了自己的数据集，但大多数工作至少使用了一个基准数据集，以便与之前发布的工作相比评估其提出的方法的性能。所有不同数据集的高层次概述可以在表
    [2](#S3.T2 "表 2 ‣ 3 现有基准数据集 ‣ 深度学习技术用于视频异常检测的调查") 中查看。请注意，数据集链接作为脚注添加以供参考。
- en: 'Table 2: Overview of Benchmark Datasets'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基准数据集概览
- en: '| Dataset | Frames | Scene | Labels | Resolution | Anomalies |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 帧数 | 场景 | 标签 | 分辨率 | 异常 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UCSD &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UCSD &#124;'
- en: '&#124; Ped1 &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Ped1 &#124;'
- en: '| 14,000 | Single |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 14,000 | 单一 |'
- en: '&#124; Spatial & &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 & &#124;'
- en: '&#124; Temporal &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间上的 &#124;'
- en: '| 238$\times$158 | biker, cart, etc |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 238$\times$158 | 自行车手，手推车等 |'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UCSD &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UCSD &#124;'
- en: '&#124; Ped2 &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Ped2 &#124;'
- en: '| 4,560 | Single |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 4,560 | 单一 |'
- en: '&#124; Spatial & &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 & &#124;'
- en: '&#124; Temporal &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间上的 &#124;'
- en: '| 360$\times$240 | biker, cart, etc |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 360$\times$240 | 自行车手，手推车等 |'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UMN &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UMN &#124;'
- en: '&#124; Lawn &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 草坪 &#124;'
- en: '| 1,450 | Single | Temporal | 320$\times$240 | escape panic |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1,450 | 单一 | 时间上的 | 320$\times$240 | 逃生恐慌 |'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UMN &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UMN &#124;'
- en: '&#124; Indoor &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内 &#124;'
- en: '| 4,415 | Single | Temporal | 320$\times$240 | escape panic |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 4,415 | 单一 | 时间上的 | 320$\times$240 | 逃生恐慌 |'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; UMN &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UMN &#124;'
- en: '&#124; Plaza &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广场 &#124;'
- en: '| 2,145 | Single | Temporal | 320$\times$240 | escape panic |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 2,145 | 单一 | 时间上的 | 320$\times$240 | 逃生恐慌 |'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CUHK &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CUHK &#124;'
- en: '&#124; Avenue &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大道 &#124;'
- en: '| 30,652 | Single |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 30,652 | 单一 |'
- en: '&#124; Spatial & &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 & &#124;'
- en: '&#124; Temporal &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间上的 &#124;'
- en: '| 640$\times$360 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 640$\times$360 |'
- en: '&#124; loitering, running, &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 徘徊，奔跑， &#124;'
- en: '&#124; throwing objects &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 投掷物品 &#124;'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Subway &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 地铁 &#124;'
- en: '&#124; Entrance &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 入口 &#124;'
- en: '| 72,401 | Single | Temporal | 512$\times$384 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 72,401 | 单一 | 时间上的 | 512$\times$384 |'
- en: '&#124; avoiding payment, &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 避免付款， &#124;'
- en: '&#124; wrong direction &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误方向 &#124;'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Subway &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 地铁 &#124;'
- en: '&#124; Exit &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 退出 &#124;'
- en: '| 136,524 | Single | Temporal | 512$\times$384 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 136,524 | 单一 | 时间上的 | 512$\times$384 |'
- en: '&#124; avoiding payment, &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 避免付款， &#124;'
- en: '&#124; wrong direction &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误方向 &#124;'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Shanghai &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上海 &#124;'
- en: '&#124; Tech &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 技术 &#124;'
- en: '| 317,398 | Multi |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 317,398 | 多 |'
- en: '&#124; Spatial & &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 & &#124;'
- en: '&#124; Temporal &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间上的 &#124;'
- en: '| 856$\times$480 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 856$\times$480 |'
- en: '&#124; chasing, brawling &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 追逐，打架 &#124;'
- en: '&#124; sudden motion, etc &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 突发运动等 &#124;'
- en: '|'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| UCF-Crime | $\sim$13.8M | Multi |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| UCF-Crime | $\sim$13.8M | 多 |'
- en: '&#124; Video-level &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频级别 &#124;'
- en: '&#124; & Temporal &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 时间上的 &#124;'
- en: '| 320$\times$240 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 320$\times$240 |'
- en: '&#124; assault, burglary, &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 袭击，盗窃， &#124;'
- en: '&#124; robbery, etc &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 抢劫等 &#124;'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Street &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 街道 &#124;'
- en: '&#124; Scene &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 场景 &#124;'
- en: '| 203,257 | Single |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 203,257 | 单一 |'
- en: '&#124; Spatial & &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 & &#124;'
- en: '&#124; Temporal &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间上的 &#124;'
- en: '| 1280$\times$720 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 1280$\times$720 |'
- en: '&#124; jaywalking, &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 乱穿马路， &#124;'
- en: '&#124; person exits car, etc &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人员离开车辆等 &#124;'
- en: '|'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.1 The UCSD Pedestrian Dataset
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 UCSD 行人数据集
- en: The UCSD Pedestrian dataset¹¹1http://www.svcl.ucsd.edu/projects/anomaly/dataset.html
    was created by Mahadevan et al. ([2010](#bib.bib28)) for the purpose of evaluating
    their approach on anomaly detection. The dataset contains videos overlooking pedestrian
    walkways taken by a stationary camera at 10 frames per second that is mounted
    at an elevation. In this dataset, anomalous events are either due to non-pedestrian
    entities in walkways or anomalous pedestrian motion. Some anomalous examples include
    bikers, skaters, cats, and the like. The dataset has two (2) subsets where each
    subset corresponds to a particular scene. The first scene includes people walking
    to and from the camera’s angle while the second has people walking parallel to
    the camera plane. An example of the anomalies can be seen in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.1 The UCSD Pedestrian Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection")
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: UCSD 行人数据集¹¹1http://www.svcl.ucsd.edu/projects/anomaly/dataset.html 是 Mahadevan
    等人（[2010](#bib.bib28)）创建的，用于评估他们在异常检测上的方法。该数据集包含由固定摄像机拍摄的俯视行人走道的视频，摄像机以每秒 10 帧的速度拍摄，并安装在一定的高度上。在这个数据集中，异常事件要么是由于走道中的非行人实体，要么是由于异常的行人运动。一些异常示例包括骑自行车的人、滑板者、猫等。数据集有两个（2）子集，每个子集对应一个特定的场景。第一个场景包括从摄像机角度看走来走去的人，而第二个场景则有与摄像机平面平行行走的人。异常示例见图
    [3](#S3.F3 "Figure 3 ‣ 3.1 The UCSD Pedestrian Dataset ‣ 3 Existing Benchmark
    Datasets ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
- en: '![Refer to caption](img/2ad031dd973931eb35550ea58d03c3b3.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2ad031dd973931eb35550ea58d03c3b3.png)'
- en: (a)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/ef103d7fcf19e42998ed37b90b7461b4.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef103d7fcf19e42998ed37b90b7461b4.png)'
- en: (b)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 3: USCD Pedestrian Example Anomalies'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：USCD 行人示例异常
- en: The first subset called Peds1 contains 34 training clips and 36 testing clips
    having a resolution of 234 $\times$ 159\. Meanwhile, the second subset called
    Peds2 contains 16 clips for training and 14 clips for testing having a resolution
    of 360 $\times$ 240\. In general, there are around 3,400 frames with anomalies
    present while the normal frames are around 5,500\. Both subsets have a frame-level
    ground truth and a pixel-level ground truth.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个子集称为 Peds1，包含 34 个训练片段和 36 个测试片段，分辨率为 234 $\times$ 159。与此同时，第二个子集称为 Peds2，包含
    16 个训练片段和 14 个测试片段，分辨率为 360 $\times$ 240。总体来说，大约有 3,400 帧存在异常，而正常帧大约为 5,500 帧。两个子集都有帧级和像素级的真实标签。
- en: 3.2 The UMN Dataset
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 UMN 数据集
- en: The UMN dataset²²2http://mha.cs.umn.edu has a total of 11 clips containing three
    (3) different scenes, specifically, a lawn scene, and indoor scene, and a plaza
    scene (Hu et al., [2016](#bib.bib12)). These video clips were captured at 30 frames
    per second using a stationary camera that has no significant illumination changes.
    The resolution of the captured video clips is at 320 $\times$ 240\. With respect
    to the number of frames, all in all there are 7,740 frames where 1,450, 4,415,
    and 2,145 belong to lawn, indoor, and plaza scenes, respectively.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: UMN 数据集²²2http://mha.cs.umn.edu 总共有 11 个视频片段，包含三个（3）不同的场景，具体为草坪场景、室内场景和广场场景（Hu
    et al., [2016](#bib.bib12)）。这些视频片段以每秒 30 帧的速度使用固定摄像机拍摄，且没有显著的光照变化。捕获的视频片段的分辨率为
    320 $\times$ 240。至于帧的数量，总共有 7,740 帧，其中 1,450 帧、4,415 帧和 2,145 帧分别属于草坪、室内和广场场景。
- en: '![Refer to caption](img/df6c0a9afb650505318e4005bb6d7b86.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/df6c0a9afb650505318e4005bb6d7b86.png)'
- en: (a)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/33d65d0317f819f1e0a5c139c727af81.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/33d65d0317f819f1e0a5c139c727af81.png)'
- en: (b)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/ce3944b246c11122e128655a54f2d471.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce3944b246c11122e128655a54f2d471.png)'
- en: (c)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 4: UMN Dataset Examples'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：UMN 数据集示例
- en: In this dataset, the particular anomaly that happens is when the people run
    to escape or when they panic. The sequences generally start with normal behavior
    where an escape panic behavior ensues. Sample frames from the dataset are shown
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 The UMN Dataset ‣ 3 Existing Benchmark Datasets
    ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，特定的异常情况发生在人员跑步逃离或感到惊慌时。序列通常从正常行为开始，然后出现逃离惊慌的行为。数据集中的样本帧如图 [4](#S3.F4
    "Figure 4 ‣ 3.2 The UMN Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey on
    Deep Learning Techniques for Video Anomaly Detection") 所示。
- en: 3.3 The CUHK Avenue Dataset
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 CUHK Avenue 数据集
- en: Along with their proposed approach, Lu et al. ([2013](#bib.bib26)) also created
    a dataset called the CUHK Avenue dataset³³3http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html
    containing 16 videos for training and 21 videos for testing which includes 15,328
    training frames and 15,324 testing frames with a resolution of 640 $\times$ 360\.
    Furthermore, the dataset contains 47 different anomalies which include loitering,
    running, and throwing objects.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 除了他们提出的方法，Lu et al. ([2013](#bib.bib26)) 还创建了一个名为 CUHK Avenue 数据集³³3http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html
    的数据集，其中包含16个训练视频和21个测试视频，包括15,328个训练帧和15,324个测试帧，分辨率为640 $\times$ 360。此外，数据集包含47种不同的异常，包括徘徊、跑步和抛掷物体。
- en: '![Refer to caption](img/d0d09cc4ac814513ab7aa804ee5faaf7.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d0d09cc4ac814513ab7aa804ee5faaf7.png)'
- en: (a)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/c5cd44e8919bf3ea2b0b087ab181afac.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c5cd44e8919bf3ea2b0b087ab181afac.png)'
- en: (b)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 5: CUHK Avenue Dataset'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：CUHK Avenue 数据集
- en: However, compared to the other datasets which have stationary cameras, the avenue
    dataset may have differences in camera angle and position. In addition, each of
    the videos is around 1 to 2 minutes long. Some example anomalies are shown in
    Fig. [5](#S3.F5 "Figure 5 ‣ 3.3 The CUHK Avenue Dataset ‣ 3 Existing Benchmark
    Datasets ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
    where there is a running man on the left-hand side of the figure and the other
    image contains an anomalous action where paper is scattered around the area.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与其他拥有固定摄像机的数据集相比，Avenue 数据集可能在摄像机角度和位置上有所不同。此外，每个视频的时长约为1到2分钟。图[5](#S3.F5
    "图5 ‣ 3.3 CUHK Avenue 数据集 ‣ 3 现有基准数据集 ‣ 关于视频异常检测的深度学习技术综述")中展示了一些例外情况，其中一个图像的左侧有一个跑步的人，而另一个图像包含了纸张散落在区域内的异常动作。
- en: 3.4 The Subway Dataset
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 地铁数据集
- en: The Subway Dataset⁴⁴4http://vision.eecs.yorku.ca/research/anomalous-behaviour-data/.
    This link only contains the Subway Exit (Adam et al., [2008](#bib.bib1)) contains
    two types of videos namely the ”exit gate” and ”entrace gate” videos. All in all,
    the videos are around two (2) hours long with a resolution of 512 $\times$ 384.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 地铁数据集⁴⁴4http://vision.eecs.yorku.ca/research/anomalous-behaviour-data/。此链接仅包含地铁出口
    (Adam et al., [2008](#bib.bib1))，包括两种类型的视频，即“出口门”和“入口门”视频。总的来说，视频时长约为两个小时，分辨率为512
    $\times$ 384。
- en: '![Refer to caption](img/b1ada5015d4ad88dd3e18d95477d91d5.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1ada5015d4ad88dd3e18d95477d91d5.png)'
- en: (a)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/a5c15868b900e69fe56ec9674872bc84.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a5c15868b900e69fe56ec9674872bc84.png)'
- en: (b)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 6: Subway Dataset'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：地铁数据集
- en: The exit gate video has 136,524 frames while the entrance gate video has 72,401
    frames (Liu et al., [2018](#bib.bib24)). In both scenarios, abnormality may include
    avoiding payment or walking in the wrong direction as the crowd. Comparing it
    to other datasets, the anomalies present in this dataset are relatively low (Sabokrou
    et al., [2018](#bib.bib42)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 出口门视频包含136,524帧，而入口门视频包含72,401帧 (Liu et al., [2018](#bib.bib24))。在这两种情况下，异常可能包括避开付款或在人群中走错方向。与其他数据集相比，该数据集中的异常相对较少
    (Sabokrou et al., [2018](#bib.bib42))。
- en: 3.5 The ShanghaiTech Campus Dataset
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 上海科技大学校园数据集
- en: The ShanghaiTech Campus⁵⁵5https://svip-lab.github.io/dataset/campus_dataset.html
    dataset (Liu et al., [2018](#bib.bib24)) was proposed due to the lack of scene
    diversity from pre-existing benchmark datasets. Compared to previous datasets,
    the ShanghaiTech dataset has a larger number of videos having 330 training videos
    and 107 testing videos which consists of 13 different scenes and a large amount
    of varying anomaly types. The resolution of the videos in this dataset is at 856
    $\times$ 480.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 上海科技大学校园数据集⁵⁵5https://svip-lab.github.io/dataset/campus_dataset.html (Liu et
    al., [2018](#bib.bib24)) 的提出是由于现有基准数据集中场景多样性的缺乏。与之前的数据集相比，上海科技大学数据集包含330个训练视频和107个测试视频，涵盖了13种不同场景和大量不同类型的异常。该数据集中的视频分辨率为856
    $\times$ 480。
- en: '![Refer to caption](img/ad93480f3a7e26438d40783f79e7545b.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad93480f3a7e26438d40783f79e7545b.png)'
- en: (a)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/f3e20acfb8a5772b50f7a88f3749cc9e.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f3e20acfb8a5772b50f7a88f3749cc9e.png)'
- en: (b)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 7: ShanghaiTech Campus'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：上海科技大学校园
- en: An example is shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 The ShanghaiTech Campus
    Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey on Deep Learning Techniques
    for Video Anomaly Detection") where the left image is the normal image with students
    walking while the right image contains the anomaly where there is a biker. Furthermore,
    there are also anomalies which are cause by sudden motion such as chasing and
    brawling. These types of anomalies are not included in datasets such was UCSD
    Pedestrian, CUHK Avenue, UMN Dataset, and Subway Dataset.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7](#S3.F7 "图 7 ‣ 3.5 上海科技大学校园数据集 ‣ 3 现有基准数据集 ‣ 深度学习技术在视频异常检测中的应用综述") 所示，左侧图像为正常图像，显示学生在走动，而右侧图像包含异常情况，有一名骑自行车的人。此外，还有由突然运动引起的异常，如追逐和打斗。这些类型的异常未包含在
    UCSD 行人、CUHK 大道、UMN 数据集和地铁数据集中。
- en: 3.6 The UCF-Crime Dataset
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 UCF-Crime 数据集
- en: Due to the previous datasets being relatively small in size, the UCF-Crime Dataset⁶⁶6https://webpages.uncc.edu/cchen62/dataset.html
    was created by Sultani et al. ([2018](#bib.bib45)). This dataset contains 13 real-world
    anomalies namely accidents, burglary, explosion, fighting, robbery, shooting,
    stealing, shoplifting, and vandalism. Compared with previous datasets which were
    manually collected, this dataset was taken from Youtube⁷⁷7www.youtube.com and
    LiveLeak⁸⁸8www.liveleak.com using relevant text queries. These text queries are
    not limited to English, other languages (using Google Translate) were also used
    for searching. Overall, there are 950 untrimmed real-world surveillance videos
    and 950 normal videos garnering a total of 1,900 videos in the dataset. Note that
    the entire dataset has around 128 hours worth of data having a resolution of 240
    $\times$ 320.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前的数据集相对较小，UCF-Crime 数据集⁶⁶6https://webpages.uncc.edu/cchen62/dataset.html
    是由 Sultani 等人 ([2018](#bib.bib45)) 创建的。该数据集包含 13 种现实世界的异常情况，即事故、盗窃、爆炸、打斗、抢劫、枪击、偷窃、商店盗窃和破坏行为。与之前的手动收集的数据集相比，这个数据集是从
    Youtube⁷⁷7www.youtube.com 和 LiveLeak⁸⁸8www.liveleak.com 使用相关的文本查询获取的。这些文本查询不限于英语，也使用了其他语言（通过
    Google 翻译）进行搜索。总的来说，该数据集包含 950 个未经裁剪的现实世界监控视频和 950 个正常视频，总计 1,900 个视频。请注意，整个数据集的时长约为
    128 小时，分辨率为 240 $\times$ 320。
- en: '![Refer to caption](img/1812c508290febd0b1383bc84c091eee.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1812c508290febd0b1383bc84c091eee.png)'
- en: (a)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/973e9bce7b95f9d056f114bc17f996c2.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/973e9bce7b95f9d056f114bc17f996c2.png)'
- en: (b)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 8: UCF-Crime Dataset'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: UCF-Crime 数据集'
- en: The dataset is already divided into training and test sets for uniformity. The
    training set consists of 810 anomalous videos while having 800 normal videos while
    the testing set has 150 normal and 140 anomalous videos. Despite being split into
    different datasets, all 13 anomalies are present in both sets lying at various
    locations in the video.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已经被划分为训练集和测试集，以保持一致性。训练集包含 810 个异常视频和 800 个正常视频，而测试集包含 150 个正常视频和 140 个异常视频。尽管被划分为不同的数据集，但所有
    13 种异常在两个数据集中都有出现，且位于视频的不同位置。
- en: 3.7 The Street Scene Dataset
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 Street Scene 数据集
- en: One of the recently published datasets, the Street Scene dataset⁹⁹9http://www.merl.com/demos/video-anomaly-detection
    (Ramachandra and Jones, [2020](#bib.bib37)) was created to solve the existing
    problems that the older datasets were facing which is to have more realistic anomalies
    and to have a greater variety with respect to the types of anomalies that are
    present. In Street Scene, there are 46 training video sequences and 35 testing
    sequences. These videos are taken from a stationary USB camera which views a two-lane
    street that has pedestrian sidewalks and bike lanes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发布的一个数据集，Street Scene 数据集⁹⁹9http://www.merl.com/demos/video-anomaly-detection
    (Ramachandra 和 Jones, [2020](#bib.bib37)) 被创建来解决旧数据集中存在的问题，即拥有更真实的异常和更大种类的异常类型。在
    Street Scene 中，有 46 个训练视频序列和 35 个测试序列。这些视频来自一个固定的 USB 摄像头，摄像头视角覆盖了一个具有行人天桥和自行车道的双车道街道。
- en: '![Refer to caption](img/1b1c110f663b4b56f2274024e4e3d2b0.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b1c110f663b4b56f2274024e4e3d2b0.png)'
- en: (a)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/7815ca248b2e9daf65490485f854a316.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7815ca248b2e9daf65490485f854a316.png)'
- en: (b)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 9: Street Scene'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 街景'
- en: Example normal and anomaly in the Street Scene dataset are shown in Fig. [9](#S3.F9
    "Figure 9 ‣ 3.7 The Street Scene Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection"). The left-hand side
    of the figure shows a person jaywalking which is an anommaly in the dataset while
    the right figure shows a normal scene. There are a total of 17 different anomaly
    types in the dataset namely jaywalking, biker outside lane, loitering, dog on
    sidewalk, car outside lane, biker on sidewalk, pedestrian reverses direction,
    and so on.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S3.F9 "图9 ‣ 3.7 街景数据集 ‣ 3 现有基准数据集 ‣ 深度学习技术在视频异常检测中的应用")展示了街景数据集中正常和异常的例子。图的左侧展示了一名过马路的行人，这在数据集中被视为异常，而右侧的图展示了一个正常场景。数据集中总共有17种不同的异常类型，如：过马路、骑车人越过车道、徘徊、在人行道上的狗、车越过车道、骑车人在
    人行道上、行人反向行走等。
- en: 4 Evaluation Metrics
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估指标
- en: This section briefly discusses the mostly used evaluation metrics by the papers
    that have been presented in this paper. Most of the works have followed the metrics
    introduced by Li et al. ([2014](#bib.bib20)) where there are two (2) different
    criteria. The first one is a frame-level criterion where a frame is considered
    anomalous if at least one of its pixels are tagged as anomalous. To evaluate using
    the frame-level criterion, the temporal labels are used to determine metrics true
    positives and false positives. The second one is a pixel-level criterion where
    if at least 40% of the anomalous pixels are detected, the frame is considered
    to be anomalous. For both criterion, the area under the curve (AUC) of the receiver
    operating characteristic curve (ROC) is computed to measure the final performance
    of the models. Given a classification model having different thresholds, the receiver
    operating characteristic curve (ROC) illustrates the performance of the model.
    The true positive rate and false positive rates defined in Equations [5](#S4.E5
    "In 4 Evaluation Metrics ‣ A Survey on Deep Learning Techniques for Video Anomaly
    Detection") and [6](#S4.E6 "In 4 Evaluation Metrics ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection") are the parameters of the said curve
    (Bradley, [1997](#bib.bib4)).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要讨论了本文中提出的论文所使用的常见评估指标。大多数研究遵循了Li等人（[2014](#bib.bib20)）提出的指标，其中有两个不同的标准。第一个是帧级标准，如果帧中至少一个像素被标记为异常，则该帧被视为异常。使用帧级标准进行评估时，使用时间标签来确定真正例和假正例的指标。第二个是像素级标准，如果至少40%的异常像素被检测到，则该帧被视为异常。对于这两种标准，都计算接收者操作特征曲线（ROC）的曲线下面积（AUC）来衡量模型的最终性能。给定具有不同阈值的分类模型，接收者操作特征曲线（ROC）展示了模型的性能。公式[5](#S4.E5
    "在4 评估指标 ‣ 深度学习技术在视频异常检测中的应用")和[6](#S4.E6 "在4 评估指标 ‣ 深度学习技术在视频异常检测中的应用")中定义的真正例率和假正例率是该曲线的参数（Bradley，[1997](#bib.bib4)）。
- en: '|  | $\mbox{True Positive Rate}=\frac{\mbox{True Positives}}{\mbox{True Positives}+\mbox{False
    Negatives}}$ |  | (5) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{真正例率}=\frac{\mbox{真正例}}{\mbox{真正例}+\mbox{假负例}}$ |  | (5) |'
- en: '|  | $\mbox{False Positive Rate}=\frac{\mbox{False Positives}}{\mbox{False
    Positives}+\mbox{True Negatives}}$ |  | (6) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{假正例率}=\frac{\mbox{假正例}}{\mbox{假正例}+\mbox{真负例}}$ |  | (6) |'
- en: Basically, the ROC is a plot such that the true positive rate is on the y-axis
    and the false positive rate is on the x-axis. The values for each point in the
    plot is taken from different classification thresholds. The area under curve (AUC)
    of the ROC is used as a measure to determine how the good the model is performing.
    A higher value for the AUC of the ROC signifies that the model is performing well.
    The strengths of this metric include threshold-invariance and scale-invariance.
    It is scale-invariant because it does not look at the absolute values of the predictions
    and looks at how well the predictions are ranked. Meanwhile, it is also threshold-invariant
    since it measures the performance without considering the threshold chosen for
    classification. However, its strengths are also its weaknesses such as the scale-invariance
    of the metric might not be suited if well-calibrated probabilities are desired.
    Moreover, it is not suited for optimizing on metrics such as false positives in
    specific use cases since it expresses them as an aggregated value. Additionally,
    an equal error rate (EER) is computed alongside the receiver operating characteristic
    curve. The equal error rate computes for the percentage of misclassified frames
    when the false positive rate is equal to the miss rate. More specifically, it
    is when the $\mbox{False Positive Rate}=1-\mbox{True Positive Rate}$ for the frame-level
    criterion while it is $1-\mbox{EER}$ for the pixel-level criterion (Li et al.,
    [2014](#bib.bib20)).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，ROC 是一个图，其中真正阳性率位于 y 轴上，假阳性率位于 x 轴上。图中每个点的值来自不同的分类阈值。ROC 曲线下的面积（AUC）用作衡量模型性能的指标。AUC
    的值越高，表示模型表现越好。该指标的优点包括阈值不变性和尺度不变性。它是尺度不变的，因为它不考虑预测的绝对值，而是关注预测的排名情况。同时，它也是阈值不变的，因为它测量模型的性能而不考虑选择的分类阈值。然而，它的优点也是它的缺点，例如尺度不变性在需要良好校准概率时可能不适用。此外，由于它将假阳性值作为聚合值来表示，因此不适合在特定使用情况下对假阳性等指标进行优化。此外，还计算了与接收操作特征曲线一起的等错误率（EER）。等错误率计算在假阳性率等于错过率时的误分类帧的百分比。更具体地说，当帧级标准下的
    $\mbox{False Positive Rate}=1-\mbox{True Positive Rate}$ 时，而在像素级标准下为 $1-\mbox{EER}$（Li
    et al., [2014](#bib.bib20)）。
- en: There are problems in both of these metrics as mentioned in the work of Ramachandra
    and Jones ([2020](#bib.bib37)). They have pointed out that in the frame-level
    criterion, an algorithm could still be considered correct even if the anomalous
    pixel doesn’t necessarily overlap with the spatial region as to where the event
    is happening. Additionally, the pixel-level criterion does not take into account
    predictions that do not overlap with the ground truth. This prompted Ramachandra
    and Jones ([2020](#bib.bib37)) to propose new evaluation metrics alongside their
    recently published dataset. They have proposed to use track-based detection criterion
    and region-based detection criterion which they claim is similar to object tracking
    and object detection metrics. The track-based detection criterion measures the
    false positive regions per frame against the track-based detection rate (TBDR)
    which is defined in Equations [7](#S4.E7 "In 4 Evaluation Metrics ‣ A Survey on
    Deep Learning Techniques for Video Anomaly Detection") and [8](#S4.E8 "In 4 Evaluation
    Metrics ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Ramachandra 和 Jones ([2020](#bib.bib37)) 的研究中提到，这些指标存在问题。他们指出，在帧级标准下，即使异常像素并不一定与事件发生的空间区域重叠，算法仍然可能被认为是正确的。此外，像素级标准没有考虑那些与实际情况不重叠的预测。这促使
    Ramachandra 和 Jones ([2020](#bib.bib37)) 提出了新的评估指标，并发布了他们最近的 dataset。他们提议使用基于轨迹的检测标准和基于区域的检测标准，这些标准类似于目标跟踪和目标检测指标。基于轨迹的检测标准测量每帧的假阳性区域与基于轨迹的检测率（TBDR），TBDR
    的定义见于公式 [7](#S4.E7 "在 4 评估指标 ‣ 深度学习技术在视频异常检测中的调查") 和 [8](#S4.E8 "在 4 评估指标 ‣ 深度学习技术在视频异常检测中的调查")。
- en: '|  | $\mbox{TBDR}=\frac{\mbox{number of anomalous tracks detected}}{\mbox{total
    number of anomalous tracks}}$ |  | (7) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{TBDR}=\frac{\mbox{检测到的异常轨迹数量}}{\mbox{异常轨迹的总数量}}$ |  | (7) |'
- en: '|  | $\mbox{FPR}=\frac{\mbox{total false positive regions}}{\mbox{total frames}}$
    |  | (8) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{FPR}=\frac{\mbox{总假阳性区域}}{\mbox{总帧数}}$ |  | (8) |'
- en: Meanwhile, the region-based detection criterion measures the false positive
    regions per frame against the region-based detection rate (RBDR) across all testing
    frames. Correctly detected anomalous regions in frames are identified similar
    to the track-based detection criterion. The definition of RBDR is shown in Equation
    [9](#S4.E9 "In 4 Evaluation Metrics ‣ A Survey on Deep Learning Techniques for
    Video Anomaly Detection")
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，基于区域的检测标准衡量每帧中的假阳性区域与所有测试帧中的基于区域的检测率（RBDR）。在帧中正确检测到的异常区域与基于轨迹的检测标准类似地进行识别。RBDR
    的定义如公式 [9](#S4.E9 "在 4 评估指标 ‣ 深度学习技术在视频异常检测中的应用调查") 所示。
- en: '|  | $\mbox{RBDR}=\frac{\mbox{number of anomalous regions detected}}{\mbox{total
    number of anomalous regions}}$ |  | (9) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{RBDR}=\frac{\mbox{检测到的异常区域数量}}{\mbox{异常区域的总数量}}$ |  | (9) |'
- en: Note that anomalous tracks are correctly identified if the ground truth has
    an intersection over union (IoU) above a threshold $\alpha$ with the detections.
    Similarly, anomalous regions in the frame is considered correctly identified if
    the ground truth has an IoU of above a threshold $\beta$ with the corresponding
    detected regions.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果地面真相与检测结果的交并比（IoU）高于阈值 $\alpha$，则异常轨迹被正确识别。同样，如果地面真相与相应检测区域的 IoU 高于阈值 $\beta$，则帧中的异常区域被认为是正确识别的。
- en: 5 Discussion
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Based on the different methodologies discussed in this paper, it is evident
    that anomaly detection is indeed a hard task. Several deep learning methods ranging
    from simple architectures to complex unified approaches have been proposed by
    different researchers. By categorizing the different approaches together into
    groups such as reconstruction error, future frame prediction, using classifiers,
    and scoring, a paradigm has been introduced on how to view anomaly detection approaches.
    Moreover, the variety of the type of approaches present also goes to show that
    researchers have been exploring different ways and thinking out of the box to
    determine anomalous events mainly because of its difficulty.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 基于本文讨论的不同方法论，显而易见，异常检测确实是一项艰巨的任务。不同研究者提出了从简单架构到复杂统一方法的多种深度学习方法。通过将不同的方法分类为重建误差、未来帧预测、使用分类器和评分等组别，提出了一种查看异常检测方法的范式。此外，存在的多种方法类型也表明，研究人员在探索不同的方法并跳出框框，以确定异常事件，这主要是因为异常检测的困难。
- en: One common theme from all of the papers is that most of them still are careful
    about taking into account several aspects of human action such as appearance and
    motion. Representations may differ such as the work of Lin et al. ([2019](#bib.bib22))
    which uses social force maps while Xu et al. ([2020](#bib.bib52)) uses optical
    flows but the main idea remains the same. This points the research community to
    a direction that appearance and motion play a big part in detecting anomalies.
    More so, that even in deep learning approaches (which is supposed to automatically
    learn discriminative features), researchers still make use of these features or
    concepts to guide the network and make it look properly at these specific variables.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 所有论文中的一个共同主题是，大多数论文仍然小心考虑人类动作的多个方面，如外观和运动。表现形式可能有所不同，如 Lin 等人 ([2019](#bib.bib22))
    使用社会力图，而 Xu 等人 ([2020](#bib.bib52)) 使用光流，但主要思想保持不变。这指引研究界一个方向，即外观和运动在检测异常中起着重要作用。更进一步，即使在深度学习方法中（这些方法应该自动学习区分特征），研究人员仍然利用这些特征或概念来指导网络，确保它适当地关注这些特定变量。
- en: Recent papers have started to think of creating end-to-end deep learning solutions
    and unified architectures rather than making use of separate components in a traditional
    pipeline. This is important as well because end-to-end deep learning solutions
    are easily deployable in real-life, making the research more accessible and more
    usable than it is now. However, end-to-end deep learning solutions require a lot
    of data which might be a problem for older datasets such as the UCSD Pedestrian
    or UMN Dataset but large scale datasets have been proposed by Sultani et al. ([2018](#bib.bib45));
    Ramachandra and Jones ([2020](#bib.bib37)); Liu et al. ([2018](#bib.bib24)) to
    help solve this problem. Yet, an important issue to also consider as well is that
    video data is very laborious to annotate and collect which one of the main reasons
    why there haven’t been as much large scale datasets published yet despite having
    tons of data publicly available in video sharing sites. This stresses the importance
    of making use of unsupervised or weakly-supervised approaches in tackling this
    problem.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的论文开始考虑创建端到端的深度学习解决方案和统一架构，而不是使用传统管道中的单独组件。这一点也很重要，因为端到端的深度学习解决方案在现实生活中更易于部署，使研究变得比现在更加可及和可用。然而，端到端的深度学习解决方案需要大量的数据，这可能对像UCSD行人数据集或UMN数据集这样的旧数据集构成问题，但Sultani等人（[2018](#bib.bib45)）、Ramachandra和Jones（[2020](#bib.bib37)）、Liu等人（[2018](#bib.bib24)）提出了大规模数据集来帮助解决这个问题。然而，另一个重要问题是，视频数据的标注和收集非常费力，这是为什么尽管视频共享网站上有大量公开数据，但尚未发布更多大规模数据集的主要原因之一。这强调了在解决这个问题时使用无监督或弱监督方法的重要性。
- en: With regards to evaluation, as presented by Ramachandra and Jones ([2020](#bib.bib37)),
    the current evaluation metrics using the frame-level criterion and pixel-level
    criterion might not be representative of the performance of the model due to the
    reasons stated in their work. Hence, there might be a need to have more robust
    evaluation metrics which would be more effective irrespective of the type of new
    datasets that might be published in the future. Future evaluation metrics must
    consider providing better ways to assess spatial aspects of future methodologies
    since it is important to know which part of the frames cause the anomalies. This
    in turn, allows faster and better inference to what is happening should the approaches
    be deployed in real life.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估，正如Ramachandra和Jones（[2020](#bib.bib37)）所述，目前使用帧级标准和像素级标准的评估指标可能无法代表模型的性能，因此可能需要更稳健的评估指标，这些指标将更有效，无论未来发布什么类型的新数据集。未来的评估指标必须考虑提供更好的方法来评估未来方法的空间方面，因为了解帧的哪一部分导致异常是很重要的。这反过来允许在实际部署方法时更快和更好地推断正在发生的情况。
- en: Looking from a different perspective, results have become better over time because
    methods by various researchers, have successfully managed to incorporate spatial
    and temporal information to their models, thereby achieving excellent results.
    Yet, for real-life anomalous events, it is more than spatial and temporal information,
    there also needs to be context added to make the models more robust. As seen from
    the different definitions of different authors, the very definition of what an
    anomaly is also vary from one context to another. One possible way to achieve
    this is to slowly pivot the research area towards larger datasets and datasets
    captured from real-life videos and real-life scenarios. Furthermore, borrowing
    concepts such as attention or transformers from different fields might also be
    helpful to achieve this goal.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同的角度来看，随着时间的推移，结果变得更好是因为各种研究人员的方法成功地将空间和时间信息整合到他们的模型中，从而取得了优异的结果。然而，对于现实生活中的异常事件，仅仅依靠空间和时间信息是不够的，还需要添加上下文以使模型更为稳健。从不同作者的定义可以看出，对于什么是异常的定义也因背景而异。实现这一目标的一种可能方法是逐步将研究领域转向更大的数据集，以及从现实生活视频和实际场景中捕获的数据集。此外，从不同领域借用注意力机制或变换器等概念也可能有助于实现这一目标。
- en: 6 Conclusions
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper has provided an overview of the recent advances in anomaly detection
    for videos specifically using deep learning techniques. Four types of categories
    of current approaches have been introduced with respect to the final step in identifying
    anomalies such as using reconstruction error, predicting future frames, using
    classification, or using scoring. These categories show the diversity of the approaches
    and it also is a testament to the difficulty of the problem as it forces researchers
    and practitioners alike to think out of the box to find better solutions to the
    problem.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了视频异常检测中使用深度学习技术的最新进展。介绍了当前方法的四种类别，涉及识别异常的最终步骤，如使用重建误差、预测未来帧、使用分类或使用评分。这些类别展示了方法的多样性，也证明了问题的难度，因为它迫使研究人员和从业者跳出框框，寻找更好的解决方案。
- en: In addition, this paper has also presented the different commonly used datasets
    along with important details such as the video resolution and example anomalies
    found within the respective datasets. Over time, it can be seen that the datasets
    are gradually increasing in size and are also becoming closer to real-life scenarios.
    However, there is still an issue of manually annotating these videos. Approaches
    that leverage weakly-supervised or unsupervised learning should be explored more
    in the hopes that it might also be able to automatically annotate videos once
    they learn from a small sample.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文还介绍了不同的常用数据集以及重要细节，如视频分辨率和在各自数据集中发现的示例异常。随着时间的推移，可以看到数据集的规模逐渐增大，并且越来越接近现实场景。然而，手动标注这些视频仍然是一个问题。应更多探索利用弱监督或无监督学习的方法，希望这些方法能够在从小样本中学习后自动标注视频。
- en: Future areas of research might include adding context since most of the works
    have been successful in modelling both motion and appearance, studying the recently
    published large-scale datasets, creating end-to-end deep learning frameworks,
    and focusing more on approaches that require little to no supervision.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究领域可能包括添加上下文，因为大多数工作在建模运动和外观方面已经取得了成功，研究最近发布的大规模数据集，创建端到端深度学习框架，并更多关注那些几乎不需要监督的方法。
- en: References
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adam et al. (2008) Adam A, Rivlin E, Shimshoni I, Reinitz D (2008) Robust real-time
    unusual event detection using multiple fixed-location monitors. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 30(3):555–560
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam 等（2008）Adam A, Rivlin E, Shimshoni I, Reinitz D (2008) 使用多个固定位置监视器进行鲁棒实时异常事件检测。IEEE模式分析与机器智能学报
    30(3):555–560
- en: 'Baldi (2011) Baldi P (2011) Autoencoders, unsupervised learning and deep architectures.
    In: Proceedings of the 2011 International Conference on Unsupervised and Transfer
    Learning Workshop - Volume 27, JMLR.org, UTLW’11, p 37–50'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baldi (2011) Baldi P (2011) 自编码器、无监督学习和深度架构。发表于：2011国际无监督学习与迁移学习研讨会论文集 - 第27卷，JMLR.org，UTLW’11，第37–50页
- en: 'Benezeth et al. (2009) Benezeth Y, Jodoin PM, Saligrama V, Rosenberger C (2009)
    Abnormal events detection based on spatio-temporal co-occurences. In: 2009 IEEE
    Conference on Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206686,
    URL https://doi.org/10.1109/cvpr.2009.5206686'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benezeth 等（2009）Benezeth Y, Jodoin PM, Saligrama V, Rosenberger C (2009) 基于时空共现的异常事件检测。发表于：2009
    IEEE计算机视觉与模式识别会议，IEEE，DOI 10.1109/cvpr.2009.5206686，网址 [https://doi.org/10.1109/cvpr.2009.5206686](https://doi.org/10.1109/cvpr.2009.5206686)
- en: Bradley (1997) Bradley AP (1997) The use of the area under the roc curve in
    the evaluation of machine learning algorithms. Pattern Recognition 30(7):1145–1159,
    DOI 10.1016/s0031-3203(96)00142-2
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley (1997) Bradley AP (1997) 使用ROC曲线下的面积来评估机器学习算法。模式识别 30(7):1145–1159，DOI
    10.1016/s0031-3203(96)00142-2
- en: Calderara et al. (2011) Calderara S, Heinemann U, Prati A, Cucchiara R, Tishby
    N (2011) Detecting anomalies in people’s trajectories using spectral graph analysis.
    URL https://www.sciencedirect.com/science/article/pii/S1077314211000919
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calderara 等（2011）Calderara S, Heinemann U, Prati A, Cucchiara R, Tishby N (2011)
    使用谱图分析检测人员轨迹中的异常。网址 [https://www.sciencedirect.com/science/article/pii/S1077314211000919](https://www.sciencedirect.com/science/article/pii/S1077314211000919)
- en: 'Chandola et al. (2009) Chandola V, Banerjee A, Kumar V (2009) Anomaly detection:
    A survey. ACM Comput Surv 41(3), DOI 10.1145/1541880.1541882, URL https://doi.org/10.1145/1541880.1541882'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandola 等（2009）Chandola V, Banerjee A, Kumar V (2009) 异常检测：综述。ACM计算机调查 41(3)，DOI
    10.1145/1541880.1541882，网址 [https://doi.org/10.1145/1541880.1541882](https://doi.org/10.1145/1541880.1541882)
- en: Fan et al. (2020) Fan Y, Wen G, Li D, Qiu S, Levine MD, Xiao F (2020) Video
    anomaly detection and localization via gaussian mixture fully convolutional variational
    autoencoder. Computer Vision and Image Understanding 195:102920, DOI https://doi.org/10.1016/j.cviu.2020.102920,
    URL http://www.sciencedirect.com/science/article/pii/S1077314218302674
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等人 (2020) Fan Y, Wen G, Li D, Qiu S, Levine MD, Xiao F (2020) 通过高斯混合全卷积变分自编码器进行视频异常检测和定位。计算机视觉与图像理解
    195:102920, DOI https://doi.org/10.1016/j.cviu.2020.102920, URL http://www.sciencedirect.com/science/article/pii/S1077314218302674
- en: 'Gong et al. (2019) Gong D, Liu L, Le V, Saha B, Mansour M, Venkatesh S, Hengel
    A (2019) Memorizing normality to detect anomaly: Memory-augmented deep autoencoder
    for unsupervised anomaly detection. 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition pp 1705–1714, DOI 10.1109/ICCV.2019.00179'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong等人 (2019) Gong D, Liu L, Le V, Saha B, Mansour M, Venkatesh S, Hengel A
    (2019) 通过记忆正常性检测异常：用于无监督异常检测的记忆增强深度自编码器。2019 IEEE/CVF计算机视觉与模式识别会议 pp 1705–1714,
    DOI 10.1109/ICCV.2019.00179
- en: 'Goodfellow et al. (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: Proceedings
    of the 27th International Conference on Neural Information Processing Systems
    - Volume 2, MIT Press, Cambridge, MA, USA, NIPS’14, p 2672–2680'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等人 (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) 生成对抗网络。在：第27届国际神经信息处理系统会议 - 第二卷论文集，MIT
    Press，Cambridge, MA, USA, NIPS’14, p 2672–2680
- en: Hasan et al. (2016) Hasan M, Choi J, Neumann J, Roy-Chowdhury AK, Davis LS (2016)
    Learning temporal regularity in video sequences. 2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR) pp 733–742
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan等人 (2016) Hasan M, Choi J, Neumann J, Roy-Chowdhury AK, Davis LS (2016)
    学习视频序列中的时间规律。2016 IEEE计算机视觉与模式识别会议 (CVPR) pp 733–742
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural Comput 9(8):1735–1780, DOI 10.1162/neco.1997.9.8.1735, URL https://doi.org/10.1162/neco.1997.9.8.1735
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter和Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) 长短期记忆。神经计算
    9(8):1735–1780, DOI 10.1162/neco.1997.9.8.1735, URL https://doi.org/10.1162/neco.1997.9.8.1735
- en: Hu et al. (2016) Hu X, Hu S, Huang Y, Zhang H, Wu H (2016) Video anomaly detection
    using deep incremental slow feature analysis network. IET Computer Vision 10(4):258–265
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人 (2016) Hu X, Hu S, Huang Y, Zhang H, Wu H (2016) 使用深度增量慢特征分析网络的视频异常检测。IET计算机视觉
    10(4):258–265
- en: Ionescu et al. (2019) Ionescu RT, Khan F, Georgescu M, Shao L (2019) Object-centric
    auto-encoders and dummy anomalies for abnormal event detection in video. 2019
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp 7834–7843,
    DOI 10.1109/CVPR.2019.00803
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ionescu等人 (2019) Ionescu RT, Khan F, Georgescu M, Shao L (2019) 面向对象的自编码器和虚拟异常用于视频中的异常事件检测。2019
    IEEE/CVF计算机视觉与模式识别会议 (CVPR) pp 7834–7843, DOI 10.1109/CVPR.2019.00803
- en: Jiang et al. (2011) Jiang F, Yuan J, Tsaftaris SA, Katsaggelos AK (2011) Anomalous
    video event detection using spatiotemporal context. URL https://dl.acm.org/doi/10.1016/j.cviu.2010.10.008
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人 (2011) Jiang F, Yuan J, Tsaftaris SA, Katsaggelos AK (2011) 使用时空上下文检测异常视频事件。URL
    https://dl.acm.org/doi/10.1016/j.cviu.2010.10.008
- en: 'Kaur et al. (2018) Kaur P, Gangadharappa M, Gautam S (2018) An overview of
    anomaly detection in video surveillance. In: 2018 International Conference on
    Advances in Computing, Communication Control and Networking (ICACCCN), IEEE, DOI 10.1109/icacccn.2018.8748454,
    URL https://doi.org/10.1109/icacccn.2018.8748454'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur等人 (2018) Kaur P, Gangadharappa M, Gautam S (2018) 视频监控中的异常检测概述。在：2018年计算、通信控制与网络国际会议
    (ICACCCN)，IEEE, DOI 10.1109/icacccn.2018.8748454, URL https://doi.org/10.1109/icacccn.2018.8748454
- en: 'Kim and Grauman (2009) Kim J, Grauman K (2009) Observe locally, infer globally:
    A space-time MRF for detecting abnormal activities with incremental updates. In:
    2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206569,
    URL https://doi.org/10.1109/cvpr.2009.5206569'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim和Grauman (2009) Kim J, Grauman K (2009) 局部观察，全球推断：用于检测异常活动的时空马尔可夫随机场及其增量更新。在：2009
    IEEE计算机视觉与模式识别会议，IEEE, DOI 10.1109/cvpr.2009.5206569, URL https://doi.org/10.1109/cvpr.2009.5206569
- en: Krizhevsky et al. (2017) Krizhevsky A, Sutskever I, Hinton GE (2017) Imagenet
    classification with deep convolutional neural networks. Commun ACM 60(6):84–90,
    DOI 10.1145/3065386, URL https://doi.org/10.1145/3065386
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等人 (2017) Krizhevsky A, Sutskever I, Hinton GE (2017) 使用深度卷积神经网络进行Imagenet分类。通讯ACM
    60(6):84–90, DOI 10.1145/3065386, URL https://doi.org/10.1145/3065386
- en: Landi et al. (2019) Landi F, Snoek CGM, Cucchiara R (2019) Anomaly locality
    in video surveillance. ArXiv abs/1901.10364
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landi等人 (2019) Landi F, Snoek CGM, Cucchiara R (2019) 视频监控中的异常局部性。ArXiv abs/1901.10364
- en: Li et al. (2013) Li C, Han Z, Ye Q, Jiao J (2013) Visual abnormal behavior detection
    based on trajectory sparse reconstruction analysis. URL https://www.sciencedirect.com/science/article/abs/pii/S0925231213000179
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2013) Li C, Han Z, Ye Q, Jiao J (2013) 基于轨迹稀疏重建分析的视觉异常行为检测。网址 https://www.sciencedirect.com/science/article/abs/pii/S0925231213000179
- en: Li et al. (2014) Li W, Mahadevan V, Vasconcelos N (2014) Anomaly detection and
    localization in crowded scenes. IEEE Transactions on Pattern Analysis and Machine
    Intelligence 36(1):18–32, DOI 10.1109/tpami.2013.111, URL https://doi.org/10.1109/tpami.2013.111
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2014) Li W, Mahadevan V, Vasconcelos N (2014) 拥挤场景中的异常检测与定位。IEEE
    计算机视觉与模式识别学报 36(1):18–32，DOI 10.1109/tpami.2013.111，网址 https://doi.org/10.1109/tpami.2013.111
- en: 'Li and min Cai (2016) Li X, min Cai Z (2016) Anomaly detection techniques in
    surveillance videos. In: 2016 9th International Congress on Image and Signal Processing,
    BioMedical Engineering and Informatics (CISP-BMEI), IEEE, DOI 10.1109/cisp-bmei.2016.7852681,
    URL https://doi.org/10.1109/cisp-bmei.2016.7852681'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and min Cai (2016) Li X, min Cai Z (2016) 监控视频中的异常检测技术。在：2016年第9届国际图像与信号处理、生物医学工程与信息学会议（CISP-BMEI），IEEE，DOI
    10.1109/cisp-bmei.2016.7852681，网址 https://doi.org/10.1109/cisp-bmei.2016.7852681
- en: 'Lin et al. (2019) Lin S, Yang H, Tang X, Shi T, Chen L (2019) Social mil: Interaction-aware
    for crowd anomaly detection. In: 2019 16th IEEE International Conference on Advanced
    Video and Signal Based Surveillance (AVSS), pp 1–8'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2019) Lin S, Yang H, Tang X, Shi T, Chen L (2019) 社会MIL：面向人群异常检测的互动感知。2019年第16届IEEE国际高级视频与信号监控会议（AVSS），第1–8页
- en: 'Liu et al. (2016) Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg
    AC (2016) SSD: Single shot MultiBox detector. In: Computer Vision – ECCV 2016,
    Springer International Publishing, pp 21–37, DOI 10.1007/978-3-319-46448-0_2,
    URL https://doi.org/10.1007/978-3-319-46448-0_2'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016) Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg
    AC (2016) SSD：单次多框检测器。在：计算机视觉–ECCV 2016，Springer 国际出版，第21–37页，DOI 10.1007/978-3-319-46448-0_2，网址
    https://doi.org/10.1007/978-3-319-46448-0_2
- en: Liu et al. (2018) Liu W, Luo W, Lian D, Gao S (2018) Future frame prediction
    for anomaly detection - a new baseline. 2018 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition pp 6536–6545
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Liu W, Luo W, Lian D, Gao S (2018) 异常检测的未来帧预测——一个新的基准。2018
    IEEE/CVF 计算机视觉与模式识别会议，第6536–6545页
- en: Lopes et al. (2017) Lopes AT, de Aguiar E, De Souza AF, Oliveira-Santos T (2017)
    Facial expression recognition with convolutional neural networks. Pattern Recogn
    61(C):610–628, DOI 10.1016/j.patcog.2016.07.026, URL https://doi.org/10.1016/j.patcog.2016.07.026
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lopes et al. (2017) Lopes AT, de Aguiar E, De Souza AF, Oliveira-Santos T (2017)
    使用卷积神经网络进行面部表情识别。模式识别 61(C):610–628，DOI 10.1016/j.patcog.2016.07.026，网址 https://doi.org/10.1016/j.patcog.2016.07.026
- en: 'Lu et al. (2013) Lu C, Shi J, Jia J (2013) Abnormal event detection at 150
    fps in matlab. In: 2013 IEEE International Conference on Computer Vision, pp 2720–2727'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2013) Lu C, Shi J, Jia J (2013) 在150 fps下的异常事件检测。2013 IEEE 国际计算机视觉会议，第2720–2727页
- en: Luo et al. (2019) Luo W, Liu W, Lian D, Tang J, Duan L, Peng X, Gao S (2019)
    Video anomaly detection with sparse coding inspired deep neural networks. IEEE
    Transactions on Pattern Analysis and Machine Intelligence pp 1–1
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2019) Luo W, Liu W, Lian D, Tang J, Duan L, Peng X, Gao S (2019)
    使用稀疏编码启发的深度神经网络进行视频异常检测。IEEE 计算机视觉与模式识别学报，第1–1页
- en: 'Mahadevan et al. (2010) Mahadevan V, LI WX, Bhalodia V, Vasconcelos N (2010)
    Anomaly detection in crowded scenes. In: Proceedings of IEEE Conference on Computer
    Vision and Pattern Recognition, pp 1975–1981'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahadevan et al. (2010) Mahadevan V, LI WX, Bhalodia V, Vasconcelos N (2010)
    拥挤场景中的异常检测。在：IEEE 计算机视觉与模式识别会议论文集，第1975–1981页
- en: 'Malinowski et al. (2017) Malinowski M, Rohrbach M, Fritz M (2017) Ask your
    neurons: A deep learning approach to visual question answering. Int J Comput Vision
    125(1–3):110–135, DOI 10.1007/s11263-017-1038-2, URL https://doi.org/10.1007/s11263-017-1038-2'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinowski et al. (2017) Malinowski M, Rohrbach M, Fritz M (2017) 询问你的神经元：一种深度学习视觉问答的方法。国际计算机视觉杂志
    125(1–3):110–135，DOI 10.1007/s11263-017-1038-2，网址 https://doi.org/10.1007/s11263-017-1038-2
- en: Massi (2019) Massi M (2019) Wikimedia Commons. URL https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Massi (2019) Massi M (2019) 维基媒体公用资源。网址 https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png
- en: Medel and Savakis (2016) Medel JR, Savakis AE (2016) Anomaly detection in video
    using predictive convolutional long short-term memory networks. CoRR abs/1612.00390,
    URL http://arxiv.org/abs/1612.00390, 1612.00390
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Medel and Savakis (2016) Medel JR, Savakis AE (2016) 使用预测卷积长短期记忆网络的视频异常检测。CoRR
    abs/1612.00390，网址 http://arxiv.org/abs/1612.00390，1612.00390
- en: 'Mehran et al. (2009) Mehran R, Oyama A, Shah M (2009) Abnormal crowd behavior
    detection using social force model. In: 2009 IEEE Conference on Computer Vision
    and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206641, URL https://doi.org/10.1109/cvpr.2009.5206641'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehran 等人 (2009) Mehran R, Oyama A, Shah M (2009) 使用社会力模型的异常人群行为检测。载于：2009年IEEE计算机视觉与模式识别会议，IEEE,
    DOI 10.1109/cvpr.2009.5206641, URL https://doi.org/10.1109/cvpr.2009.5206641
- en: 'Mei and Zhang (2017) Mei T, Zhang C (2017) Deep learning for intelligent video
    analysis. In: Proceedings of the 25th ACM International Conference on Multimedia,
    Association for Computing Machinery, New York, NY, USA, MM ’17, p 1955–1956, DOI 10.1145/3123266.3130141,
    URL https://doi.org/10.1145/3123266.3130141'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei 和 Zhang (2017) Mei T, Zhang C (2017) 用于智能视频分析的深度学习。载于：第25届ACM国际多媒体会议论文集，计算机协会，纽约，NY，USA,
    MM ’17, 页 1955–1956, DOI 10.1145/3123266.3130141, URL https://doi.org/10.1145/3123266.3130141
- en: Narasimhan and S. (2018) Narasimhan MG, S SK (2018) Dynamic video anomaly detection
    and localization using sparse denoising autoencoders. Multimedia Tools Appl 77(11):13173–13195,
    DOI 10.1007/s11042-017-4940-2, URL https://doi.org/10.1007/s11042-017-4940-2
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narasimhan 和 S. (2018) Narasimhan MG, S SK (2018) 使用稀疏去噪自编码器的动态视频异常检测与定位。多媒体工具与应用
    77(11):13173–13195, DOI 10.1007/s11042-017-4940-2, URL https://doi.org/10.1007/s11042-017-4940-2
- en: Pan et al. (2011) Pan SJ, Tsang IW, Kwok JT, Yang Q (2011) Domain adaptation
    via transfer component analysis. IEEE Transactions on Neural Networks 22(2):199–210
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 (2011) Pan SJ, Tsang IW, Kwok JT, Yang Q (2011) 通过转移分量分析的领域适应。IEEE神经网络汇刊
    22(2):199–210
- en: Popoola and Wang (2012) Popoola OP, Wang K (2012) Video-based abnormal human
    behavior recognition—a review. IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews) 42(6):865–878
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popoola 和 Wang (2012) Popoola OP, Wang K (2012) 基于视频的异常人类行为识别——综述。IEEE系统、人类与控制论汇刊C部分（应用与综述）42(6):865–878
- en: 'Ramachandra and Jones (2020) Ramachandra B, Jones MJ (2020) Street scene: A
    new dataset and evaluation protocol for video anomaly detection. In: 2020 IEEE
    Winter Conference on Applications of Computer Vision (WACV), IEEE, DOI 10.1109/wacv45572.2020.9093457,
    URL https://doi.org/10.1109/wacv45572.2020.9093457'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandra 和 Jones (2020) Ramachandra B, Jones MJ (2020) 街景：用于视频异常检测的新数据集和评估协议。载于：2020年IEEE冬季计算机视觉应用会议（WACV），IEEE,
    DOI 10.1109/wacv45572.2020.9093457, URL https://doi.org/10.1109/wacv45572.2020.9093457
- en: Ribeiro et al. (2018) Ribeiro M, Lazzaretti AE, Lopes HS (2018) A study of deep
    convolutional auto-encoders for anomaly detection in videos. Pattern Recognition
    Letters 105:13 – 22, DOI https://doi.org/10.1016/j.patrec.2017.07.016, URL http://www.sciencedirect.com/science/article/pii/S0167865517302489,
    machine Learning and Applications in Artificial Intelligence
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等人 (2018) Ribeiro M, Lazzaretti AE, Lopes HS (2018) 对视频中异常检测的深度卷积自编码器的研究。模式识别通讯
    105:13 – 22, DOI https://doi.org/10.1016/j.patrec.2017.07.016, URL http://www.sciencedirect.com/science/article/pii/S0167865517302489,
    机器学习与人工智能应用
- en: 'Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional
    networks for biomedical image segmentation. In: Lecture Notes in Computer Science,
    Springer International Publishing, pp 234–241, DOI 10.1007/978-3-319-24574-4_28,
    URL https://doi.org/10.1007/978-3-319-24574-4_28'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等人 (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: 用于生物医学图像分割的卷积网络。载于：计算机科学讲义，Springer国际出版公司,
    页 234–241, DOI 10.1007/978-3-319-24574-4_28, URL https://doi.org/10.1007/978-3-319-24574-4_28'
- en: Sabokrou et al. (2016) Sabokrou M, Fathy M, Hoseini M (2016) Video anomaly detection
    and localisation based on the sparsity and reconstruction error of auto-encoder.
    Electronics Letters 52:1122–1124
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabokrou 等人 (2016) Sabokrou M, Fathy M, Hoseini M (2016) 基于自编码器的稀疏性和重建误差的视频异常检测与定位。电子快报
    52:1122–1124
- en: 'Sabokrou et al. (2017) Sabokrou M, Fayyaz M, Fathy M, Klette R (2017) Deep-cascade:
    Cascading 3d deep neural networks for fast anomaly detection and localization
    in crowded scenes. IEEE Transactions on Image Processing 26(4):1992–2004'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sabokrou 等人 (2017) Sabokrou M, Fayyaz M, Fathy M, Klette R (2017) Deep-cascade:
    用于快速异常检测和定位的3D深度神经网络级联。IEEE图像处理汇刊 26(4):1992–2004'
- en: 'Sabokrou et al. (2018) Sabokrou M, Fayyaz M, Fathy M, Moayed Z, Klette R (2018)
    Deep-anomaly: Fully convolutional neural network for fast anomaly detection in
    crowded scenes. Computer Vision and Image Understanding 172:88 – 97, DOI https://doi.org/10.1016/j.cviu.2018.02.006,
    URL http://www.sciencedirect.com/science/article/pii/S1077314218300249'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sabokrou 等人 (2018) Sabokrou M, Fayyaz M, Fathy M, Moayed Z, Klette R (2018)
    Deep-anomaly: 用于快速异常检测的全卷积神经网络。计算机视觉与图像理解 172:88 – 97, DOI https://doi.org/10.1016/j.cviu.2018.02.006,
    URL http://www.sciencedirect.com/science/article/pii/S1077314218300249'
- en: 'Sabzalian et al. (2019) Sabzalian B, Marvi H, Ahmadyfard A (2019) Deep and
    sparse features for anomaly detection and localization in video. In: 2019 4th
    International Conference on Pattern Recognition and Image Analysis (IPRIA), pp
    173–178'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabzalian等（2019）Sabzalian B, Marvi H, Ahmadyfard A（2019）在视频中进行异常检测和定位的深度和稀疏特征。载于：2019年第4届国际模式识别与图像分析会议（IPRIA），第173–178页。
- en: '[dos Santos] et al. (2019) [dos Santos] FP, Ribeiro LS, Ponti MA (2019) Generalization
    of feature embeddings transferred from different video anomaly detection domains.
    Journal of Visual Communication and Image Representation 60:407 – 416, DOI https://doi.org/10.1016/j.jvcir.2019.02.035,
    URL http://www.sciencedirect.com/science/article/pii/S1047320319300926'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[dos Santos]等（2019）[dos Santos] FP, Ribeiro LS, Ponti MA（2019）从不同视频异常检测领域转移的特征嵌入的泛化。视觉通信与图像表示杂志60:407–416，DOI
    https://doi.org/10.1016/j.jvcir.2019.02.035，URL http://www.sciencedirect.com/science/article/pii/S1047320319300926'
- en: 'Sultani et al. (2018) Sultani W, Chen C, Shah M (2018) Real-world anomaly detection
    in surveillance videos. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp 6479–6488'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sultani等（2018）Sultani W, Chen C, Shah M（2018）监控视频中的现实世界异常检测。载于：2018年IEEE/CVF计算机视觉与模式识别会议，第6479–6488页。
- en: 'Tran et al. (2015) Tran D, Bourdev L, Fergus R, Torresani L, Paluri M (2015)
    Learning spatiotemporal features with 3d convolutional networks. In: 2015 IEEE
    International Conference on Computer Vision (ICCV), pp 4489–4497'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等（2015）Tran D, Bourdev L, Fergus R, Torresani L, Paluri M（2015）使用3D卷积网络学习时空特征。载于：2015年IEEE国际计算机视觉会议（ICCV），第4489–4497页。
- en: Tung et al. (2010) Tung F, Zelek JS, Clausi DA (2010) Goal-based trajectory
    analysis for unusual behaviour detection in intelligent surveillance. URL https://www.sciencedirect.com/science/article/abs/pii/S026288561000154X
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tung等（2010）Tung F, Zelek JS, Clausi DA（2010）基于目标的轨迹分析用于智能监控中的异常行为检测。URL https://www.sciencedirect.com/science/article/abs/pii/S026288561000154X
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser u, Polosukhin I (2017) Attention is all you need. In: Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    Curran Associates Inc., Red Hook, NY, USA, NIPS’17, p 6000–6010'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等（2017）Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN,
    Kaiser u, Polosukhin I（2017）注意力机制即是你所需要的一切。载于：第31届国际神经信息处理系统会议论文集，Curran Associates
    Inc.，Red Hook, NY, USA，NIPS’17，第6000–6010页。
- en: Vu et al. (2017) Vu H, Phung D, Nguyen TD, Trevors A, Venkatesh S (2017) Energy-based
    models for video anomaly detection. 1708.05211
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vu等（2017）Vu H, Phung D, Nguyen TD, Trevors A, Venkatesh S（2017）基于能量的模型用于视频异常检测。1708.05211
- en: Wang et al. (2018) Wang S, Zhu E, Yin J, Porikli F (2018) Video anomaly detection
    and localization by local motion based joint video representation and OCELM. Neurocomputing
    277:161–175, DOI 10.1016/j.neucom.2016.08.156, URL https://doi.org/10.1016/j.neucom.2016.08.156
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018）Wang S, Zhu E, Yin J, Porikli F（2018）通过基于局部运动的联合视频表示和OCELM进行视频异常检测和定位。神经计算277:161–175，DOI
    10.1016/j.neucom.2016.08.156，URL https://doi.org/10.1016/j.neucom.2016.08.156
- en: 'Wiskott and Sejnowski (2002) Wiskott L, Sejnowski TJ (2002) Slow feature analysis:
    Unsupervised learning of invariances. Neural Computation 14(4):715–770, DOI 10.1162/089976602317318938,
    URL https://doi.org/10.1162/089976602317318938'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiskott和Sejnowski（2002）Wiskott L, Sejnowski TJ（2002）慢特征分析：无监督学习的不变性。神经计算14(4):715–770，DOI
    10.1162/089976602317318938，URL https://doi.org/10.1162/089976602317318938
- en: Xu et al. (2020) Xu K, Sun T, Jiang X (2020) Video anomaly detection and localization
    based on an adaptive intra-frame classification network. IEEE Transactions on
    Multimedia 22(2):394–406
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2020）Xu K, Sun T, Jiang X（2020）基于自适应帧内分类网络的视频异常检测与定位。IEEE多媒体学报22(2):394–406
- en: 'Yan et al. (2016) Yan H, Liu X, Hong R (2016) Image classification via fusing
    the latent deep cnn feature. In: Proceedings of the International Conference on
    Internet Multimedia Computing and Service, Association for Computing Machinery,
    New York, NY, USA, ICIMCS’16, p 110–113, DOI 10.1145/3007669.3007706, URL https://doi.org/10.1145/3007669.3007706'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan等（2016）Yan H, Liu X, Hong R（2016）通过融合潜在深度CNN特征进行图像分类。载于：国际互联网多媒体计算与服务会议论文集，计算机协会，纽约，NY，美国，ICIMCS’16，第110–113页，DOI
    10.1145/3007669.3007706，URL https://doi.org/10.1145/3007669.3007706
- en: Zhang et al. (2016) Zhang Y, Lu H, Zhang L, Ruan X (2016) Combining motion and
    appearance cues for anomaly detection. Pattern Recognition 51:443–452, DOI 10.1016/j.patcog.2015.09.005,
    URL https://doi.org/10.1016/j.patcog.2015.09.005
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2016）Zhang Y, Lu H, Zhang L, Ruan X（2016）结合运动和外观线索进行异常检测。模式识别51:443–452，DOI
    10.1016/j.patcog.2015.09.005，URL https://doi.org/10.1016/j.patcog.2015.09.005
- en: 'Zhao et al. (2017) Zhao Y, Deng B, Shen C, Liu Y, Lu H, Hua XS (2017) Spatio-temporal
    autoencoder for video anomaly detection. In: Proceedings of the 25th ACM International
    Conference on Multimedia, Association for Computing Machinery, New York, NY, USA,
    MM ’17, p 1933–1941, DOI 10.1145/3123266.3123451, URL https://doi.org/10.1145/3123266.3123451'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2017）Zhao Y, Deng B, Shen C, Liu Y, Lu H, Hua XS（2017）用于视频异常检测的时空自编码器。发表于：第25届ACM国际多媒体会议论文集，计算机协会，纽约，NY，美国，MM
    ’17，第1933–1941页，DOI 10.1145/3123266.3123451，网址 [https://doi.org/10.1145/3123266.3123451](https://doi.org/10.1145/3123266.3123451)
- en: 'Zhou et al. (2019) Zhou JT, Du J, Zhu H, Peng X, Liu Y, Goh RSM (2019) Anomalynet:
    An anomaly detection network for video surveillance. IEEE Transactions on Information
    Forensics and Security 14(10):2537–2550'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2019）Zhou JT, Du J, Zhu H, Peng X, Liu Y, Goh RSM（2019）Anomalynet：一种用于视频监控的异常检测网络。IEEE信息取证与安全学报
    14(10):2537–2550
- en: Zhu and Newsam (2019) Zhu Y, Newsam S (2019) Motion-aware feature for improved
    video anomaly detection. British Machine Vision Conference
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu和Newsam（2019）Zhu Y, Newsam S（2019）运动感知特征用于改进视频异常检测。英国机器视觉会议
- en: 'Zoph et al. (2018) Zoph B, Vasudevan V, Shlens J, Le QV (2018) Learning transferable
    architectures for scalable image recognition. In: 2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2018.00907, URL
    https://doi.org/10.1109/cvpr.2018.00907'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph等（2018）Zoph B, Vasudevan V, Shlens J, Le QV（2018）为可扩展图像识别学习可迁移架构。发表于：2018
    IEEE/CVF计算机视觉与模式识别会议，IEEE，DOI 10.1109/cvpr.2018.00907，网址 [https://doi.org/10.1109/cvpr.2018.00907](https://doi.org/10.1109/cvpr.2018.00907)
