- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:59:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2009.14146] A Survey on Deep Learning Techniques for Video Anomaly Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.14146](https://ar5iv.labs.arxiv.org/html/2009.14146)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹institutetext: Jessie James P. Suarez ²²institutetext: Computer Vision and
    Machine Intelligence Group'
  prefs: []
  type: TYPE_NORMAL
- en: University of the Philippines, Diliman
  prefs: []
  type: TYPE_NORMAL
- en: '²²email: jpsuarez@up.edu.ph ³³institutetext: Prospero C. Naval, Jr ⁴⁴institutetext:
    Computer Vision and Machine Intelligence Group'
  prefs: []
  type: TYPE_NORMAL
- en: University of the Philippines, Diliman
  prefs: []
  type: TYPE_NORMAL
- en: '⁴⁴email: pcnaval@dcs.upd.edu.ph'
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Deep Learning Techniques for Video Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jessie James P. Suarez    Prospero C. Naval    Jr(Received: date / Accepted:
    date)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Anomaly detection in videos is a problem that has been studied for more than
    a decade. This area has piqued the interest of researchers due to its wide applicability.
    Because of this, there has been a wide array of approaches that have been proposed
    throughout the years and these approaches range from statistical-based approaches
    to machine learning-based approaches. Numerous surveys have already been conducted
    on this area but this paper focuses on providing an overview on the recent advances
    in the field of anomaly detection using Deep Learning. Deep Learning has been
    applied successfully in many fields of artificial intelligence such as computer
    vision, natural language processing and more. This survey, however, focuses on
    how Deep Learning has improved and provided more insights to the area of video
    anomaly detection. This paper provides a categorization of the different Deep
    Learning approaches with respect to their objectives. Additionally, it also discusses
    the commonly used datasets along with the common evaluation metrics. Afterwards,
    a discussion synthesizing all of the recent approaches is made to provide direction
    and possible areas for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: video understanding video processing anomaly detection deep learning computer
    vision
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Surveillance videos have been increasingly present in various establishments
    in order to monitor human activity and prevent crime from happening. It goes without
    saying that there needs to be someone behind watching the videos and signaling
    an alert whenever something different from normal is happening. However, these
    events do not happen very often and that most of the time, the person monitoring
    these videos would see nothing out of the ordinary (Sultani et al., [2018](#bib.bib45)).
    These unusual events can be thought of as anomalies which can be defined as patterns
    that do not conform to what is considered normal. The task of finding these nonconforming
    patterns is called anomaly detection (Chandola et al., [2009](#bib.bib6)). Because
    of this, researchers have been trying to create a robust anomaly detection algorithms
    that can automate the process of monitoring and detection of unusual events in
    surveillance videos. An example of a simple anomaly case can be seen in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Video Anomaly
    Detection") where the normal regions are denoted by $N$ and anomalies are those
    denoted by $O$. As seen in the figure, anomalies tend to clearly lie outside what
    is normal. However, these anomalies can, in fact, be close to normality which
    is illustrated by $O_{2}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17a9670492d71b2ac2715c3e73ecaa2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Simple Anomaly Case by Chandola et. al. [2009](#bib.bib6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Anomaly detection is a challenging task due to number of reasons: first, the
    definition of an anomaly may vary from one context to another (Medel and Savakis,
    [2016](#bib.bib31); Sabokrou et al., [2017](#bib.bib41)). Second, the different
    possibilities of what constitute an anomaly might be are boundless (Luo et al.,
    [2019](#bib.bib27)). Third, anomalous data points, especially with real-world
    data, tend to lie closely to what might be defined as normal (Vu et al., [2017](#bib.bib49)).
    Lastly, extracting robust features from the data even if anomalies seldom appear
    (Ribeiro et al., [2018](#bib.bib38)). The mentioned list does not entirely capture
    all of the possible reasons which make the problem hard but these main points
    are what researchers have been considering for the past years when proposing new
    solutions to the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Around a decade ago, most of the researchers have focused on trajectory-based
    anomaly detection (Jiang et al., [2011](#bib.bib14); Calderara et al., [2011](#bib.bib5);
    Tung et al., [2010](#bib.bib47); Li et al., [2013](#bib.bib19)). The main idea
    is if the objects of interest are not following the learned normal trajectories,
    the video will be tagged as an anomaly. However, one major drawback of this approach
    is occlusion since the approach heavily relies on continuously monitoring the
    objects of interest (Sabokrou et al., [2017](#bib.bib41); Narasimhan and S., [2018](#bib.bib34)).
    Due to these drawbacks, there was an emphasis on using low-level features for
    feature extraction instead (Sabokrou et al., [2017](#bib.bib41)). These approaches
    based on low-level features rely on the use of appearance, motion, and texture
    features (Mehran et al., [2009](#bib.bib32); Li et al., [2014](#bib.bib20); Zhang
    et al., [2016](#bib.bib54); Wang et al., [2018](#bib.bib50); Kim and Grauman,
    [2009](#bib.bib16); Benezeth et al., [2009](#bib.bib3)). Various representations
    have been used in order to represent these aspects of the video such as in the
    approach of Mehran et al. ([2009](#bib.bib32)) where they used social force maps
    to model motion of the crowds. Similarly, pixel-motion properties were used by
    Benezeth et al. ([2009](#bib.bib3)) to model behavior. Meanwhile, Kim and Grauman
    ([2009](#bib.bib16)) made use of optical flows which are then used as inputs to
    the mixture of probabilistic principal component analysis (MPPCA) model, thus,
    creating a more compact feature representation. However, features based on motion
    are not enough which is why there were proposed approaches that make use of both.
    An example is the approach of Li et al. ([2014](#bib.bib20)) where their approach
    makes use of mixture of dynamic textures (MDTs) that utilize temporal normalcy
    and discriminant saliency detectors to model spatial normalcy. Likewise, Zhang
    et al. ([2016](#bib.bib54)) used support vector data description for spatial features
    and optical flow for motion features. In contrast, Wang et al. ([2018](#bib.bib50))
    used spatially localized histogram of optical flows and uniform local gradient
    pattern-based optical flows. Most of these techniques and methods, specifically
    on these ”traditional” approaches, have been discussed in great detail in the
    works of Kaur et al. ([2018](#bib.bib15)); Li and min Cai ([2016](#bib.bib21));
    Popoola and Wang ([2012](#bib.bib36)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the proven success of these traditional approaches on benchmark datasets,
    they are still ineffective when used in a different domain. Furthermore, they
    are unable to adapt to anomalies that they have never seen before (Hu et al.,
    [2016](#bib.bib12); Medel and Savakis, [2016](#bib.bib31)). For these reasons,
    recent works have mostly explored the use of Deep Neural Networks for the task
    of anomaly detection. These neural networks automatically learn useful and discriminant
    features on their own which removes the hassle of creating handcrafting features
    (Krizhevsky et al., [2017](#bib.bib17)). This also makes it more adaptive when
    used on different domains. Deep learning was proven to be effective for a variety
    of computer vision tasks such as feature extraction in images (Yan et al., [2016](#bib.bib53)),
    image classification (Krizhevsky et al., [2017](#bib.bib17)), object detection
    (Zoph et al., [2018](#bib.bib58)), video analysis (Mei and Zhang, [2017](#bib.bib33)),
    face detection (Lopes et al., [2017](#bib.bib25)), visual question answering (Malinowski
    et al., [2017](#bib.bib29)) and many other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, there are existing works that have discussed various
    anomaly detection methods for videos (Kaur et al., [2018](#bib.bib15); Li and
    min Cai, [2016](#bib.bib21); Popoola and Wang, [2012](#bib.bib36)). However, due
    to the recent traction in the use of deep learning techniques on this field, the
    goal of this paper is to provide a closer look into these deep learning techniques.
    This entails providing organization as to how the approaches are related to one
    another, the rationale as to why these methods have been proposed, and summarizing
    the conclusions which they have presented in a clear manner. In addition, it would
    also be necessary to discuss datasets and evaluation metrics which have mostly
    been used by these approaches. It would also be insightful to determine how these
    datasets and metrics would scale well when dealing with real-world anomaly detection.
    Different researchers have created different environmental setups making some
    of them incomparable. Thus, the performances of the approaches discussed will
    not be included to avoid confusion and misinterpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper is organized as follows: the first section serves as an introduction
    to the survey. Second, deep learning anomaly detection techniques will be discussed
    in detail. Third, the mostly used datasets will be tackled. Fourth, the commonly
    used evaluation metrics will be presented. Fifth, a section for discussion is
    allocated to synthesize all of the approaches and datasets mentioned. Lastly,
    the concluding remarks coupled with recommendations as to what directions this
    area of research could possibly go.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning in Anomaly Detection for Videos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning techniques mostly focus on creating new architectures or crafting
    components that can be suitable for a specific problem. Since deep learning methods
    have been successful in a number of varied use cases (Yan et al., [2016](#bib.bib53);
    Krizhevsky et al., [2017](#bib.bib17); Zoph et al., [2018](#bib.bib58); Mei and
    Zhang, [2017](#bib.bib33); Lopes et al., [2017](#bib.bib25); Malinowski et al.,
    [2017](#bib.bib29)), most of these networks or architectures might be similar
    to each other. An example of which would be with Krizhevsky et al. ([2017](#bib.bib17))
    where they used Convolutional Neural Networks for image classification. However,
    almost the same network is also used for face recognition (Lopes et al., [2017](#bib.bib25)).
    Because of this, the presented categories below would group these approaches specifically
    with respect to their final objectives instead of network architecture or learning
    strategy. Examples of these include using reconstruction error or providing an
    anomaly score. In line with this, there are four (4) identified categories namely:
    using reconstruction error or reconstruction-based methods, framing the problem
    as a classification problem, predicting future frames, and computing for an anomaly
    score. A quick summary of all these techniques are provided in Table [1](#S2.T1
    "Table 1 ‣ 2.3 Using Classifiers ‣ 2 Deep Learning in Anomaly Detection for Videos
    ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Using Reconstruction Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reconstruction error has already been used in various traditional anomaly detection
    techniques (Popoola and Wang, [2012](#bib.bib36)). The basic assumption of using
    reconstruction error is that the reconstruction error for normal samples would
    be lower since they are closer to the training data. On the other hand, the reconstruction
    error is assumed or expected to be higher for samples which are not normal (Gong
    et al., [2019](#bib.bib8); Sabokrou et al., [2016](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: More formally, let $x$ be a video segment or video frame and let $g$ be a neural
    network that reconstructs $x$. The reconstruction error can be defined as a function
    $f$ such that is computes for error between $x$ (the original input), and $g(x)$
    which is the reconstruction Eqn [1](#S2.E1 "In 2.1 Using Reconstruction Error
    ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection"). This concept has been extended recently
    by making use of deep learning techniques to reconstruct various scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $e=f(x,g(x))$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Different from usual feedforward networks, one type of neural network that is
    able to reconstruct input data is called an autoencoder. The autoencoder is a
    neural network that has the capability to encode an input into a more compact
    representation while retaining important and discriminative features. It also
    has the ability to decode this particular encoding back to its original form (Baldi,
    [2011](#bib.bib2)). A visual schematic of an autoencoder is shown in Fig. [2](#S2.F2
    "Figure 2 ‣ 2.1 Using Reconstruction Error ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
    where the diagram illustrates a simple architecture of an encoder where the left-hand
    side is the input to the autoencoder $X$, the middle portion is the encoded representation
    (sometimes called the latent vector or code) of $X$, and the right-hand side is
    the decoded encoding called $X^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5bc362b5c5386500e5d198db10f1b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Autoencoder Diagram by Michaela Massi ([2019](#bib.bib30)) via Wikimedia
    Commons'
  prefs: []
  type: TYPE_NORMAL
- en: Most approaches whose goal of using reconstruction error as a means to identify
    anomalies base their method on autoencoders. One such method is introduced by
    Hasan et al. ([2016](#bib.bib10)) where they posited that in comparison to sparse
    coding, the objective function of an autoencoder is more efficient. They have
    also said that it is able to preserve spatio-temporal information while encoding
    dynamics. Their approach made use of combining 2D convolutions to autoencoders
    wherein the 2D convolutions take as input specific raw video segments. Conventionally,
    inputs to a Convolutional Neural Network is a 2D image having the third channel
    as the color channel (Krizhevsky et al., [2017](#bib.bib17)). However, in their
    approach, the third dimension is instead composed of stacked grayscale frames,
    allowing the model to encode both spatial and some temporal information for reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the work of Medel and Savakis ([2016](#bib.bib31)) also framed the
    problem as a reconstruction problem. The approach makes use of a convolutional
    long short-term memory wherein the Long Short-Term Memory (LSTM) Network is a
    type of neural network that is capable of learning long-term dependencies of the
    data (Hochreiter and Schmidhuber, [1997](#bib.bib11)). Despite not being explicitly
    an autoencoder, their approach also makes use of an encoder-decoder sturture.
    Given an input sequence of video frames, the convolutional long short-term memory
    extracts relevant features along the spatial and temporal dimension in such a
    way that the last time step is used as the encoding. The decoder unravels the
    encoding and then reconstructs the frames which can then be used to compute the
    reconstruction error for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed approach of Ribeiro et al. ([2018](#bib.bib38)) closely resembles
    that of Hasan et al. ([2016](#bib.bib10)). The main difference is that the low-level
    features such as optical flow and edges are used as inputs alongside the raw frames.
    In addition, they have also presented how these features affect the convolutional
    autoencoder with regard to detecting anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method was proposed by Sabokrou et al. ([2016](#bib.bib40)) where they
    have used two different autoencoders for the task: one is a regular autoencoder
    and the other is a sparse autoencoder. A sparse autoencoder is an autoencoder
    but has an additional sparsity penalty. This penalty encourages fewer neurons
    to activate. This constraint allows the network to learn relevant information
    without reducing the number of nodes in the hidden layers. Their approach involves
    two steps, the first step is to compute the sparsity value from cubic patches
    of the videos, if it is below a specific sparsity threshold, another set of patches
    are extracted around that patch for reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: According to Zhao et al. ([2017](#bib.bib55)), the approach of Hasan et al.
    ([2016](#bib.bib10)) which makes use of temporal cuboids by stacking frames in
    the third dimension, does not necessarily retain the temporal information. Based
    on their work, a reason for this is that 2D convolutions operate on the frames
    spatially. Putting this in the perspective of the approach of Hasan et al. ([2016](#bib.bib10)),
    the third channel is represented along each of the channels of the first feature
    map which rarely preserves temporal information. To solve this, Zhao et al. ([2017](#bib.bib55))
    proposed the use of 3D convolutions as a means to retain temporal information
    during the convolution process. Since it is data intensive, they have also applied
    data augmentation to increase their samples.
  prefs: []
  type: TYPE_NORMAL
- en: As claimed in the work of Zhou et al. ([2019](#bib.bib56)), one weakness of
    the approach of Medel and Savakis ([2016](#bib.bib31)) is that spatial and temporal
    aspects of the inputs are encoded separately by the convolutions and the long
    short-term memory. This implies a broken relationship between the two during the
    encoding process. Furthermore, it was also stated by Zhou et al. ([2019](#bib.bib56))
    that the approach proposed by Medel and Savakis ([2016](#bib.bib31)) was not able
    to make use of existing pre-trained networks. These networks have shown remarkably
    improved performances once it has been applied to other domains. Hence, their
    proposed method makes use of a feature learning subnetwork that combines motion
    and appearance features into an image. Afterwards, it is then used as an input
    to a pretrained network for feature extraction. Moreover, they have proposed a
    novel subnetwork called sparse coding to network (SC2Net) to compute for the sparsity
    loss and reconstruction loss from the extracted features.
  prefs: []
  type: TYPE_NORMAL
- en: Among all of the approaches, Gong et al. ([2019](#bib.bib8)) have posited that
    most of the works on reconstruction generally assume that the anomalous instances
    will have a high reconstruction error. Based on these works, this assumption does
    not necessarily hold true mainly because there might be instances where an autoencoder
    is able to generalize well. This poses a problem since it might accurately reconstruct
    anomalous instances as well. To mitigate this problem, they have introduced a
    new autoencoder which has the capability to store encodings into memory. The main
    difference from previous approaches is that instead of directly feeding the encoding
    to the decoder, the encoding is treated as a query. This query is expected to
    return closest normal patterns in memory which is instead used for decoding. In
    the event that an anomaly is to be reconstructed, it would have a high reconstruction
    error because the memory only has normal memory items.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Using Future Frame Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A different perspective on the problem was presented by Liu et al. ([2018](#bib.bib24)).
    They support the claim of Gong et al. ([2019](#bib.bib8)) stating that autoencoders
    might also accurately reconstruct anomalous frames. Since anomalies can be viewed
    as events that do not conform with certain expectations, Liu et al. ([2018](#bib.bib24))
    suggested a frame prediction approach might be a more natural way to view the
    problem. Mathematically speaking, given $x_{t}$ which is the video segment or
    frame $x$ at time $t$, future frame prediction can be expressed as a function
    $h$ predicting the next segment as shown in Eqn [2](#S2.E2 "In 2.2 Using Future
    Frame Prediction ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t+1}=h(x_{t})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In deep learning, there is a specific type of neural network is used for generating
    new data with the same statistics as the training data. This network which is
    called generative adversarial network (GAN) (Goodfellow et al., [2014](#bib.bib9)).
    This architecture has two main (2) parts. The first one is a generator whose job
    is to mimic the original data distribution. Meanwhile, the second network is called
    a discriminator that gives a probability of whether or not the input is coming
    from the generator.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of Liu et al. ([2018](#bib.bib24)) made use of a generator-discriminator
    structure, likened to that of a generative adversarial network. They used the
    U-Net architecture (Ronneberger et al., [2015](#bib.bib39)) for future frame prediction
    as the generator because of its exemplary performance in image-to-image translation.
    While the discriminator at the end of the network determines whether or not the
    predicted frame is anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: Some works on reconstruction also have the capability for predicting future
    frames such as in the work of Hasan et al. ([2016](#bib.bib10)). Their approach
    has the ability to encode both spatial and temporal aspects of the video by allowing
    the autoencoder to learn it from a sequence of video segments (discussed in more
    detail in Section [2.1](#S2.SS1 "2.1 Using Reconstruction Error ‣ 2 Deep Learning
    in Anomaly Detection for Videos ‣ A Survey on Deep Learning Techniques for Video
    Anomaly Detection")). It is because of this exact same reason that it can also
    predict future and past frames given a center frame. Based on their methodology,
    by padding the center frame with zero values, their model can extrapolate the
    near future and near past of the center frame.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, some of the previous works actually leverage future frame prediction
    in the process of reconstructing the current frame. An example of this is the
    work of Zhao et al. ([2017](#bib.bib55)) where their network learns the future
    frames along with the task of reconstruction in a different branch of the network.
    Similarly, Medel and Savakis ([2016](#bib.bib31)) also has a separate branch in
    parallel that learns how to predict the future. Despite their similarities, they
    both have big differences as to how future frame prediction is used. Medel and
    Savakis ([2016](#bib.bib31)) makes use of future frame prediction to identify
    interest points within the video. On the contrary, in the approach of Zhao et al.
    ([2017](#bib.bib55)), the future frame is actually included in the computation
    of the loss to guide the network to extract temporal features. In addition, it
    is also included in the reconstruction score which combines the prediction loss
    and the reconstruction loss.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Using Classifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the sophisticated methods that rely mainly on reconstruction loss and
    future frame prediction, there are also still a handful of approaches that cast
    the problem as a classification problem. The classification problem can be viewed
    as a function $j$ that takes as its input a frame or video segment $x$ whose output
    $y$ is a class or category as seen in Eqn [3](#S2.E3 "In 2.3 Using Classifiers
    ‣ 2 Deep Learning in Anomaly Detection for Videos ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=j(x),y\in\mathbb{R}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Because of imbalanced datasets, these methods focus mostly on how to create
    compact, efficient, and robust features. The approach of Sabokrou et al. ([2017](#bib.bib41))
    tries to solve this problem by proposing a competitive cascade of deep neural
    networks. The cascade is composed of two stages where the first stage is a small
    stack of autoencoders which hierarchically models the normality of the video patches.
    The other one is a Convolutional Neural Network which takes as input video patches
    that the autoencoders could not handle and would need further probing. The classifier
    used for the approach is a Gaussian Classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of Methods and Contributions'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Author | Type | Main Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Medel et. al |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reconstruction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; & Future Frame &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Convolutional Long &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Short-Term Memory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | Hasan et. al. | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fully 2D Convolutional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | Sabokrou et. al. | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sparse Autoencoder + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | Hu et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Neural Network + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Slow Feature Analysis &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | Narasimhan et. al. | Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sparse Denoising &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoders &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | Sabokrou et. al. | Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Cascade of Deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Convolutional Neural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Networks + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoders &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | Zhao et. al. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reconstruction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; & Future Frame &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spatiotemporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | Sabokrou et. al. | Classification | Deep-Anomaly |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Sultani et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Multiple-Instance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | Ribeiro et. al. | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Low-level Features + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2D Convolutional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | Liu et. al. | Future Frame |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Future Frame using &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Landi et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Localization before &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature Extraction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Sabzailan et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Traditional + Deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning Features &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Zhu et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Optical Flow as inputs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to Multiple-Instance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Zhou et. al. | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AnomalyNet: a unified &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; approach &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Gong et. al. | Reconstruction |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Autoencoder + memory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; module + attention-based &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; addressing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Lin. et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Multiple-Instance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning + Social Force &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Maps &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Santos et. al. | Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Transfer Learning + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transfer Component &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Analysis &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Luo et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Sparse Coding-inspired &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Deep Neural Network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Ionescu et. al. | Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Object-Centric &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Convolutional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autoencoders &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | Xu et. al. | Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Adaptive Intra-Frame &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification Network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 | Fan et. al. | Scoring |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Gaussian Mixture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully Convolutional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Variational Autoencoders &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Narasimhan and S. ([2018](#bib.bib34)) proposed a method
    that makes use of local and global descriptors whose aim is utilize both spatial
    and temporal domains. For local features, they made use of an image similarity
    metric on the video cubic patches to represent the temporal and spatial features.
    Meanwhile, the global features are represented by the latent vector of the trained
    autoencoders. After creating both local and global features, it is then fed to
    an autoencoder which selects important features that are discriminative enough
    for anomaly detection. Finally, these features are fed into Gaussian classifiers
    separately for local and global descriptors and then combined to detect anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the above mentioned methods, even those in the previous sections, make
    use of Convolutional Neural Networks. However, Sabokrou et al. ([2018](#bib.bib42))
    has mentioned problems with regard to using these networks, one of which is that
    these networks are too inefficient for patch-based methods. Examples of approaches
    that made use of patches are as follows: Narasimhan and S. ([2018](#bib.bib34));
    Sabokrou et al. ([2016](#bib.bib40)); Sabzalian et al. ([2019](#bib.bib43)); Sabokrou
    et al. ([2018](#bib.bib42)); Medel and Savakis ([2016](#bib.bib31)). For this
    reason, they have proposed a possible solution to the problem which makes use
    of the discriminative power of a pre-trained model without having to tweak it
    Sabokrou et al. ([2018](#bib.bib42)). More specifically, they use the intermediate
    layer to generate the features that will be fed to a Gaussian Classifier. In the
    event that a low confidence is generated by the classifier, it is sent to another
    convolutional layer on top of the best intermediate layer for further probing.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Sabokrou et al. ([2018](#bib.bib42)), the proposed approach of [dos
    Santos] et al. ([2019](#bib.bib44)) took advantage of the available pre-trained
    models. They have investigated the generalization of feature spaces of Convolutional
    Neural Networks without requiring additional labels. In their experiments, they
    used transfer component analysis (Pan et al., [2011](#bib.bib35)) which attempts
    to learn a certain subspace that is shared by different domains. They have concluded
    that generalization through different domains.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the methods mentioned previously make use of extracting either global
    or local features without taking the objects of interest into account. The approach
    of Ionescu et al. ([2019](#bib.bib13)) makes use of a single-shot detector (SSD)
    (Liu et al., [2016](#bib.bib23)) on each frame of the video. After isolating the
    objects, a convolutional autoencoder is used to learn deep unsupervised features
    thereby allowing the algorithm to focus on the objects in the scene. Furthermore,
    they have instead casted the problem of anomaly detection as a multi-class classification
    problem rather than an unbalanced binary classification problem or a one-class
    problem. To generate the artificial classes, they have used clustering on the
    set of features generated by the convolutional autoencoder where each cluster
    represents a different type of normality. A one-versus-rest classifier is trained
    which discriminates between the clusters. If the highest classification score
    is negative, meaning the sample does not belong to any cluster, it is tagged as
    anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Ionescu et al. ([2019](#bib.bib13)), Xu et al. ([2020](#bib.bib52))
    also framed the problem as a multi-class classification problem as opposed to
    either a one-class or a binary classification problem. In line with this, they
    also took note of the fact that most of the previous approaches were able to effectively
    identify subregions representations of anomalies. However, for most of the approaches,
    there is a wide array of inputs and outputs such as optical flows, patches, or
    gradients. This inspired the approach of Xu et al. ([2020](#bib.bib52)) which
    tries to unify all of these approach by creating a network called the adaptive
    intraframe classification network that takes the raw inputs, computes for motion
    and appearance features, and determines whether or not the sample is anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Using Scoring Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some researchers have instead, framed the problem as a regression problem wherein
    the goal is to provide an anomaly score which will then be used as a means to
    determine whether or not a video segment or a frame is anomalous (Landi et al.,
    [2019](#bib.bib18); Sultani et al., [2018](#bib.bib45)). The scoring methods can
    be viewed as a function $k$ such that it takes a video segment or frame $x$ as
    its input. It outputs a real number $z$ representing the anomaly score as seen
    in Eqn [4](#S2.E4 "In 2.4 Using Scoring Methods ‣ 2 Deep Learning in Anomaly Detection
    for Videos ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z=k(x),z\in\mathbb{R}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: The proposed approach of Hu et al. ([2016](#bib.bib12)), makes use of their
    novel sum squared derivative to score the features generated by their approach.
    This basically determines if the sequence of frames is anomalous. Prior to their
    scoring method, they combined both deep learning and slow feature analysis (Wiskott
    and Sejnowski, [2002](#bib.bib51)) in order to learn semantic-level representations
    given raw video frames. It is also worth noting that their approach has an online
    variant, thereby making their approach adaptive.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of Sultani et al. ([2018](#bib.bib45)) made use of a multiple instance
    learning to identify anomalies in video segments based on weakly-labelled videos
    (labels are on a video-level and not frame-level). Their approach uses C3D, a
    3D Convolutional Neural Network that learns spatiotemporal features by exposing
    the model to large-scale video datasets (Tran et al., [2015](#bib.bib46)). These
    spatiotemporal features are then fed to fully connected layers for generating
    the anomaly score. The backpropagation of the error is guided by the principle
    of multiple instance learning, allowing the model to learn anomalous segments
    despite having weak labels. This idea was taken up by Zhu and Newsam ([2019](#bib.bib57))
    where, instead of using C3D, they made use of computing for the optical flows
    which are then fed to a temporal augmented network. Their proposed approach also
    makes use of an attention mechanism (Vaswani et al., [2017](#bib.bib48)) that
    allows the network to identify which features are important to look at. Similarly,
    Lin et al. ([2019](#bib.bib22)) also built upon this idea where they proposed
    a dual-branch network that incorporates motion into the initial network introduced
    by Sultani et al. ([2018](#bib.bib45)). The approach of Lin et al. ([2019](#bib.bib22))
    adapts the same network of Sultani et al. ([2018](#bib.bib45)) as the first branch
    with a modification wherein an attention module (Vaswani et al., [2017](#bib.bib48))
    was added after the feature extraction layer. The second branch is similar in
    structure as the first branch except that it takes as an input social force maps
    (Mehran et al., [2009](#bib.bib32)) computed from the raw images to represent
    motion.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the approach of Sabzalian et al. ([2019](#bib.bib43)) makes full
    use of the effectiveness of traditional and deep learning features for anomaly
    detection. Their proposed approach starts by identifying the foreground of the
    video by using optical flows. Once the regions of interest have been identified,
    a pre-trained Convolutional Neural Network is used to extract features alongside
    computing for traditional features like histogram of gradients and histogram of
    optical flows. These three features are combined by making use of an iteratively
    weighted nonnegative matrix factorization method (Sabzalian et al., [2019](#bib.bib43)).
    Afterwards, the features are clustered and the discrimination of whether or not
    the sample is an anomaly will be done via a voting system.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from framing the problem as a regression problem, Landi et al. ([2019](#bib.bib18))
    proposed to make use of locality when computing for the anomaly score. The approach
    is similar to that of Sultani et al. ([2018](#bib.bib45)) except that their approach
    extracts a tube from the video which in a way localizes and adjusts the level
    of granularity when extracting features. From their experiments, they have shown
    that locality or, more specifically, zoning in on one region where the anomalous
    event takes place actually helps the method to accurately compute anomaly scores.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse coding for anomaly detection is an approach that learns a dictionary
    which attempts to encodes all normal events (Lu et al., [2013](#bib.bib26)). By
    revisiting sparse coding, Luo et al. ([2019](#bib.bib27)) proposed temporally-coherent
    sparse coding to model the coherence between neighboring events for normal frames.
    These temporal features are then combined with spatial features learn from pre-trained
    networks across different scales for a normality score. Note that the features
    extracted pass through a Stacked Recurrent Neural Network autoencoder to generate
    the final features for scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Past works demonstrated the effectiveness of autoencoders and that normal samples
    can be associated with at least one Gaussian Mixture Model. Because of this, Fan
    et al. ([2020](#bib.bib7)) proposed an end-to-end neural network called the Gaussian
    Mixture Fully Convolutional Variational Autoencoder to model anomalies and to
    predict them. Their model is trained on image and dynamic flow patches wherein
    both of them are separately fed into different networks. This basically captures
    separate motion and appearance features. Afterwards, joint probabilities are used
    to detect both appearance and motion anomalies via a sample energy-based method.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Existing Benchmark Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses in detail the publicly available datasets for the task
    of anomaly detection. There are a few papers which have created their own datasets
    but most of the works have tried to at least use one benchmark dataset in order
    to evaluate the performance of their proposed approaches with respect to previously
    published works. A summary presenting a high-level view of all of the different
    datasets included in this subsection can be seen in Table [2](#S3.T2 "Table 2
    ‣ 3 Existing Benchmark Datasets ‣ A Survey on Deep Learning Techniques for Video
    Anomaly Detection"). Note that the dataset links are added as footnotes for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Overview of Benchmark Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Frames | Scene | Labels | Resolution | Anomalies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UCSD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ped1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 14,000 | Single |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Spatial & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 238$\times$158 | biker, cart, etc |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UCSD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ped2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 4,560 | Single |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Spatial & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 360$\times$240 | biker, cart, etc |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UMN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lawn &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1,450 | Single | Temporal | 320$\times$240 | escape panic |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UMN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Indoor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 4,415 | Single | Temporal | 320$\times$240 | escape panic |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UMN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Plaza &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2,145 | Single | Temporal | 320$\times$240 | escape panic |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CUHK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Avenue &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 30,652 | Single |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Spatial & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 640$\times$360 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; loitering, running, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; throwing objects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Subway &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Entrance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 72,401 | Single | Temporal | 512$\times$384 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; avoiding payment, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; wrong direction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Subway &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Exit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 136,524 | Single | Temporal | 512$\times$384 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; avoiding payment, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; wrong direction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Shanghai &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 317,398 | Multi |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Spatial & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 856$\times$480 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; chasing, brawling &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sudden motion, etc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| UCF-Crime | $\sim$13.8M | Multi |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Video-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; & Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 320$\times$240 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; assault, burglary, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; robbery, etc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Street &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Scene &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 203,257 | Single |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Spatial & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1280$\times$720 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; jaywalking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; person exits car, etc &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The UCSD Pedestrian Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UCSD Pedestrian dataset¹¹1http://www.svcl.ucsd.edu/projects/anomaly/dataset.html
    was created by Mahadevan et al. ([2010](#bib.bib28)) for the purpose of evaluating
    their approach on anomaly detection. The dataset contains videos overlooking pedestrian
    walkways taken by a stationary camera at 10 frames per second that is mounted
    at an elevation. In this dataset, anomalous events are either due to non-pedestrian
    entities in walkways or anomalous pedestrian motion. Some anomalous examples include
    bikers, skaters, cats, and the like. The dataset has two (2) subsets where each
    subset corresponds to a particular scene. The first scene includes people walking
    to and from the camera’s angle while the second has people walking parallel to
    the camera plane. An example of the anomalies can be seen in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.1 The UCSD Pedestrian Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection")
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ad031dd973931eb35550ea58d03c3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef103d7fcf19e42998ed37b90b7461b4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: USCD Pedestrian Example Anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: The first subset called Peds1 contains 34 training clips and 36 testing clips
    having a resolution of 234 $\times$ 159\. Meanwhile, the second subset called
    Peds2 contains 16 clips for training and 14 clips for testing having a resolution
    of 360 $\times$ 240\. In general, there are around 3,400 frames with anomalies
    present while the normal frames are around 5,500\. Both subsets have a frame-level
    ground truth and a pixel-level ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The UMN Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UMN dataset²²2http://mha.cs.umn.edu has a total of 11 clips containing three
    (3) different scenes, specifically, a lawn scene, and indoor scene, and a plaza
    scene (Hu et al., [2016](#bib.bib12)). These video clips were captured at 30 frames
    per second using a stationary camera that has no significant illumination changes.
    The resolution of the captured video clips is at 320 $\times$ 240\. With respect
    to the number of frames, all in all there are 7,740 frames where 1,450, 4,415,
    and 2,145 belong to lawn, indoor, and plaza scenes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df6c0a9afb650505318e4005bb6d7b86.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33d65d0317f819f1e0a5c139c727af81.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce3944b246c11122e128655a54f2d471.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: UMN Dataset Examples'
  prefs: []
  type: TYPE_NORMAL
- en: In this dataset, the particular anomaly that happens is when the people run
    to escape or when they panic. The sequences generally start with normal behavior
    where an escape panic behavior ensues. Sample frames from the dataset are shown
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 The UMN Dataset ‣ 3 Existing Benchmark Datasets
    ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 The CUHK Avenue Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with their proposed approach, Lu et al. ([2013](#bib.bib26)) also created
    a dataset called the CUHK Avenue dataset³³3http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html
    containing 16 videos for training and 21 videos for testing which includes 15,328
    training frames and 15,324 testing frames with a resolution of 640 $\times$ 360\.
    Furthermore, the dataset contains 47 different anomalies which include loitering,
    running, and throwing objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0d09cc4ac814513ab7aa804ee5faaf7.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5cd44e8919bf3ea2b0b087ab181afac.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: CUHK Avenue Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: However, compared to the other datasets which have stationary cameras, the avenue
    dataset may have differences in camera angle and position. In addition, each of
    the videos is around 1 to 2 minutes long. Some example anomalies are shown in
    Fig. [5](#S3.F5 "Figure 5 ‣ 3.3 The CUHK Avenue Dataset ‣ 3 Existing Benchmark
    Datasets ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection")
    where there is a running man on the left-hand side of the figure and the other
    image contains an anomalous action where paper is scattered around the area.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 The Subway Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Subway Dataset⁴⁴4http://vision.eecs.yorku.ca/research/anomalous-behaviour-data/.
    This link only contains the Subway Exit (Adam et al., [2008](#bib.bib1)) contains
    two types of videos namely the ”exit gate” and ”entrace gate” videos. All in all,
    the videos are around two (2) hours long with a resolution of 512 $\times$ 384.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1ada5015d4ad88dd3e18d95477d91d5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5c15868b900e69fe56ec9674872bc84.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Subway Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The exit gate video has 136,524 frames while the entrance gate video has 72,401
    frames (Liu et al., [2018](#bib.bib24)). In both scenarios, abnormality may include
    avoiding payment or walking in the wrong direction as the crowd. Comparing it
    to other datasets, the anomalies present in this dataset are relatively low (Sabokrou
    et al., [2018](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 The ShanghaiTech Campus Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ShanghaiTech Campus⁵⁵5https://svip-lab.github.io/dataset/campus_dataset.html
    dataset (Liu et al., [2018](#bib.bib24)) was proposed due to the lack of scene
    diversity from pre-existing benchmark datasets. Compared to previous datasets,
    the ShanghaiTech dataset has a larger number of videos having 330 training videos
    and 107 testing videos which consists of 13 different scenes and a large amount
    of varying anomaly types. The resolution of the videos in this dataset is at 856
    $\times$ 480.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad93480f3a7e26438d40783f79e7545b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3e20acfb8a5772b50f7a88f3749cc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: ShanghaiTech Campus'
  prefs: []
  type: TYPE_NORMAL
- en: An example is shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 The ShanghaiTech Campus
    Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey on Deep Learning Techniques
    for Video Anomaly Detection") where the left image is the normal image with students
    walking while the right image contains the anomaly where there is a biker. Furthermore,
    there are also anomalies which are cause by sudden motion such as chasing and
    brawling. These types of anomalies are not included in datasets such was UCSD
    Pedestrian, CUHK Avenue, UMN Dataset, and Subway Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 The UCF-Crime Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the previous datasets being relatively small in size, the UCF-Crime Dataset⁶⁶6https://webpages.uncc.edu/cchen62/dataset.html
    was created by Sultani et al. ([2018](#bib.bib45)). This dataset contains 13 real-world
    anomalies namely accidents, burglary, explosion, fighting, robbery, shooting,
    stealing, shoplifting, and vandalism. Compared with previous datasets which were
    manually collected, this dataset was taken from Youtube⁷⁷7www.youtube.com and
    LiveLeak⁸⁸8www.liveleak.com using relevant text queries. These text queries are
    not limited to English, other languages (using Google Translate) were also used
    for searching. Overall, there are 950 untrimmed real-world surveillance videos
    and 950 normal videos garnering a total of 1,900 videos in the dataset. Note that
    the entire dataset has around 128 hours worth of data having a resolution of 240
    $\times$ 320.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1812c508290febd0b1383bc84c091eee.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/973e9bce7b95f9d056f114bc17f996c2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: UCF-Crime Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is already divided into training and test sets for uniformity. The
    training set consists of 810 anomalous videos while having 800 normal videos while
    the testing set has 150 normal and 140 anomalous videos. Despite being split into
    different datasets, all 13 anomalies are present in both sets lying at various
    locations in the video.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 The Street Scene Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the recently published datasets, the Street Scene dataset⁹⁹9http://www.merl.com/demos/video-anomaly-detection
    (Ramachandra and Jones, [2020](#bib.bib37)) was created to solve the existing
    problems that the older datasets were facing which is to have more realistic anomalies
    and to have a greater variety with respect to the types of anomalies that are
    present. In Street Scene, there are 46 training video sequences and 35 testing
    sequences. These videos are taken from a stationary USB camera which views a two-lane
    street that has pedestrian sidewalks and bike lanes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b1c110f663b4b56f2274024e4e3d2b0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7815ca248b2e9daf65490485f854a316.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Street Scene'
  prefs: []
  type: TYPE_NORMAL
- en: Example normal and anomaly in the Street Scene dataset are shown in Fig. [9](#S3.F9
    "Figure 9 ‣ 3.7 The Street Scene Dataset ‣ 3 Existing Benchmark Datasets ‣ A Survey
    on Deep Learning Techniques for Video Anomaly Detection"). The left-hand side
    of the figure shows a person jaywalking which is an anommaly in the dataset while
    the right figure shows a normal scene. There are a total of 17 different anomaly
    types in the dataset namely jaywalking, biker outside lane, loitering, dog on
    sidewalk, car outside lane, biker on sidewalk, pedestrian reverses direction,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section briefly discusses the mostly used evaluation metrics by the papers
    that have been presented in this paper. Most of the works have followed the metrics
    introduced by Li et al. ([2014](#bib.bib20)) where there are two (2) different
    criteria. The first one is a frame-level criterion where a frame is considered
    anomalous if at least one of its pixels are tagged as anomalous. To evaluate using
    the frame-level criterion, the temporal labels are used to determine metrics true
    positives and false positives. The second one is a pixel-level criterion where
    if at least 40% of the anomalous pixels are detected, the frame is considered
    to be anomalous. For both criterion, the area under the curve (AUC) of the receiver
    operating characteristic curve (ROC) is computed to measure the final performance
    of the models. Given a classification model having different thresholds, the receiver
    operating characteristic curve (ROC) illustrates the performance of the model.
    The true positive rate and false positive rates defined in Equations [5](#S4.E5
    "In 4 Evaluation Metrics ‣ A Survey on Deep Learning Techniques for Video Anomaly
    Detection") and [6](#S4.E6 "In 4 Evaluation Metrics ‣ A Survey on Deep Learning
    Techniques for Video Anomaly Detection") are the parameters of the said curve
    (Bradley, [1997](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mbox{True Positive Rate}=\frac{\mbox{True Positives}}{\mbox{True Positives}+\mbox{False
    Negatives}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mbox{False Positive Rate}=\frac{\mbox{False Positives}}{\mbox{False
    Positives}+\mbox{True Negatives}}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Basically, the ROC is a plot such that the true positive rate is on the y-axis
    and the false positive rate is on the x-axis. The values for each point in the
    plot is taken from different classification thresholds. The area under curve (AUC)
    of the ROC is used as a measure to determine how the good the model is performing.
    A higher value for the AUC of the ROC signifies that the model is performing well.
    The strengths of this metric include threshold-invariance and scale-invariance.
    It is scale-invariant because it does not look at the absolute values of the predictions
    and looks at how well the predictions are ranked. Meanwhile, it is also threshold-invariant
    since it measures the performance without considering the threshold chosen for
    classification. However, its strengths are also its weaknesses such as the scale-invariance
    of the metric might not be suited if well-calibrated probabilities are desired.
    Moreover, it is not suited for optimizing on metrics such as false positives in
    specific use cases since it expresses them as an aggregated value. Additionally,
    an equal error rate (EER) is computed alongside the receiver operating characteristic
    curve. The equal error rate computes for the percentage of misclassified frames
    when the false positive rate is equal to the miss rate. More specifically, it
    is when the $\mbox{False Positive Rate}=1-\mbox{True Positive Rate}$ for the frame-level
    criterion while it is $1-\mbox{EER}$ for the pixel-level criterion (Li et al.,
    [2014](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: There are problems in both of these metrics as mentioned in the work of Ramachandra
    and Jones ([2020](#bib.bib37)). They have pointed out that in the frame-level
    criterion, an algorithm could still be considered correct even if the anomalous
    pixel doesn’t necessarily overlap with the spatial region as to where the event
    is happening. Additionally, the pixel-level criterion does not take into account
    predictions that do not overlap with the ground truth. This prompted Ramachandra
    and Jones ([2020](#bib.bib37)) to propose new evaluation metrics alongside their
    recently published dataset. They have proposed to use track-based detection criterion
    and region-based detection criterion which they claim is similar to object tracking
    and object detection metrics. The track-based detection criterion measures the
    false positive regions per frame against the track-based detection rate (TBDR)
    which is defined in Equations [7](#S4.E7 "In 4 Evaluation Metrics ‣ A Survey on
    Deep Learning Techniques for Video Anomaly Detection") and [8](#S4.E8 "In 4 Evaluation
    Metrics ‣ A Survey on Deep Learning Techniques for Video Anomaly Detection").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mbox{TBDR}=\frac{\mbox{number of anomalous tracks detected}}{\mbox{total
    number of anomalous tracks}}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mbox{FPR}=\frac{\mbox{total false positive regions}}{\mbox{total frames}}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Meanwhile, the region-based detection criterion measures the false positive
    regions per frame against the region-based detection rate (RBDR) across all testing
    frames. Correctly detected anomalous regions in frames are identified similar
    to the track-based detection criterion. The definition of RBDR is shown in Equation
    [9](#S4.E9 "In 4 Evaluation Metrics ‣ A Survey on Deep Learning Techniques for
    Video Anomaly Detection")
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mbox{RBDR}=\frac{\mbox{number of anomalous regions detected}}{\mbox{total
    number of anomalous regions}}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Note that anomalous tracks are correctly identified if the ground truth has
    an intersection over union (IoU) above a threshold $\alpha$ with the detections.
    Similarly, anomalous regions in the frame is considered correctly identified if
    the ground truth has an IoU of above a threshold $\beta$ with the corresponding
    detected regions.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the different methodologies discussed in this paper, it is evident
    that anomaly detection is indeed a hard task. Several deep learning methods ranging
    from simple architectures to complex unified approaches have been proposed by
    different researchers. By categorizing the different approaches together into
    groups such as reconstruction error, future frame prediction, using classifiers,
    and scoring, a paradigm has been introduced on how to view anomaly detection approaches.
    Moreover, the variety of the type of approaches present also goes to show that
    researchers have been exploring different ways and thinking out of the box to
    determine anomalous events mainly because of its difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: One common theme from all of the papers is that most of them still are careful
    about taking into account several aspects of human action such as appearance and
    motion. Representations may differ such as the work of Lin et al. ([2019](#bib.bib22))
    which uses social force maps while Xu et al. ([2020](#bib.bib52)) uses optical
    flows but the main idea remains the same. This points the research community to
    a direction that appearance and motion play a big part in detecting anomalies.
    More so, that even in deep learning approaches (which is supposed to automatically
    learn discriminative features), researchers still make use of these features or
    concepts to guide the network and make it look properly at these specific variables.
  prefs: []
  type: TYPE_NORMAL
- en: Recent papers have started to think of creating end-to-end deep learning solutions
    and unified architectures rather than making use of separate components in a traditional
    pipeline. This is important as well because end-to-end deep learning solutions
    are easily deployable in real-life, making the research more accessible and more
    usable than it is now. However, end-to-end deep learning solutions require a lot
    of data which might be a problem for older datasets such as the UCSD Pedestrian
    or UMN Dataset but large scale datasets have been proposed by Sultani et al. ([2018](#bib.bib45));
    Ramachandra and Jones ([2020](#bib.bib37)); Liu et al. ([2018](#bib.bib24)) to
    help solve this problem. Yet, an important issue to also consider as well is that
    video data is very laborious to annotate and collect which one of the main reasons
    why there haven’t been as much large scale datasets published yet despite having
    tons of data publicly available in video sharing sites. This stresses the importance
    of making use of unsupervised or weakly-supervised approaches in tackling this
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: With regards to evaluation, as presented by Ramachandra and Jones ([2020](#bib.bib37)),
    the current evaluation metrics using the frame-level criterion and pixel-level
    criterion might not be representative of the performance of the model due to the
    reasons stated in their work. Hence, there might be a need to have more robust
    evaluation metrics which would be more effective irrespective of the type of new
    datasets that might be published in the future. Future evaluation metrics must
    consider providing better ways to assess spatial aspects of future methodologies
    since it is important to know which part of the frames cause the anomalies. This
    in turn, allows faster and better inference to what is happening should the approaches
    be deployed in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Looking from a different perspective, results have become better over time because
    methods by various researchers, have successfully managed to incorporate spatial
    and temporal information to their models, thereby achieving excellent results.
    Yet, for real-life anomalous events, it is more than spatial and temporal information,
    there also needs to be context added to make the models more robust. As seen from
    the different definitions of different authors, the very definition of what an
    anomaly is also vary from one context to another. One possible way to achieve
    this is to slowly pivot the research area towards larger datasets and datasets
    captured from real-life videos and real-life scenarios. Furthermore, borrowing
    concepts such as attention or transformers from different fields might also be
    helpful to achieve this goal.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper has provided an overview of the recent advances in anomaly detection
    for videos specifically using deep learning techniques. Four types of categories
    of current approaches have been introduced with respect to the final step in identifying
    anomalies such as using reconstruction error, predicting future frames, using
    classification, or using scoring. These categories show the diversity of the approaches
    and it also is a testament to the difficulty of the problem as it forces researchers
    and practitioners alike to think out of the box to find better solutions to the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, this paper has also presented the different commonly used datasets
    along with important details such as the video resolution and example anomalies
    found within the respective datasets. Over time, it can be seen that the datasets
    are gradually increasing in size and are also becoming closer to real-life scenarios.
    However, there is still an issue of manually annotating these videos. Approaches
    that leverage weakly-supervised or unsupervised learning should be explored more
    in the hopes that it might also be able to automatically annotate videos once
    they learn from a small sample.
  prefs: []
  type: TYPE_NORMAL
- en: Future areas of research might include adding context since most of the works
    have been successful in modelling both motion and appearance, studying the recently
    published large-scale datasets, creating end-to-end deep learning frameworks,
    and focusing more on approaches that require little to no supervision.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adam et al. (2008) Adam A, Rivlin E, Shimshoni I, Reinitz D (2008) Robust real-time
    unusual event detection using multiple fixed-location monitors. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 30(3):555–560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baldi (2011) Baldi P (2011) Autoencoders, unsupervised learning and deep architectures.
    In: Proceedings of the 2011 International Conference on Unsupervised and Transfer
    Learning Workshop - Volume 27, JMLR.org, UTLW’11, p 37–50'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benezeth et al. (2009) Benezeth Y, Jodoin PM, Saligrama V, Rosenberger C (2009)
    Abnormal events detection based on spatio-temporal co-occurences. In: 2009 IEEE
    Conference on Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206686,
    URL https://doi.org/10.1109/cvpr.2009.5206686'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradley (1997) Bradley AP (1997) The use of the area under the roc curve in
    the evaluation of machine learning algorithms. Pattern Recognition 30(7):1145–1159,
    DOI 10.1016/s0031-3203(96)00142-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calderara et al. (2011) Calderara S, Heinemann U, Prati A, Cucchiara R, Tishby
    N (2011) Detecting anomalies in people’s trajectories using spectral graph analysis.
    URL https://www.sciencedirect.com/science/article/pii/S1077314211000919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chandola et al. (2009) Chandola V, Banerjee A, Kumar V (2009) Anomaly detection:
    A survey. ACM Comput Surv 41(3), DOI 10.1145/1541880.1541882, URL https://doi.org/10.1145/1541880.1541882'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Fan Y, Wen G, Li D, Qiu S, Levine MD, Xiao F (2020) Video
    anomaly detection and localization via gaussian mixture fully convolutional variational
    autoencoder. Computer Vision and Image Understanding 195:102920, DOI https://doi.org/10.1016/j.cviu.2020.102920,
    URL http://www.sciencedirect.com/science/article/pii/S1077314218302674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. (2019) Gong D, Liu L, Le V, Saha B, Mansour M, Venkatesh S, Hengel
    A (2019) Memorizing normality to detect anomaly: Memory-augmented deep autoencoder
    for unsupervised anomaly detection. 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition pp 1705–1714, DOI 10.1109/ICCV.2019.00179'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: Proceedings
    of the 27th International Conference on Neural Information Processing Systems
    - Volume 2, MIT Press, Cambridge, MA, USA, NIPS’14, p 2672–2680'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hasan et al. (2016) Hasan M, Choi J, Neumann J, Roy-Chowdhury AK, Davis LS (2016)
    Learning temporal regularity in video sequences. 2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR) pp 733–742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural Comput 9(8):1735–1780, DOI 10.1162/neco.1997.9.8.1735, URL https://doi.org/10.1162/neco.1997.9.8.1735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2016) Hu X, Hu S, Huang Y, Zhang H, Wu H (2016) Video anomaly detection
    using deep incremental slow feature analysis network. IET Computer Vision 10(4):258–265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ionescu et al. (2019) Ionescu RT, Khan F, Georgescu M, Shao L (2019) Object-centric
    auto-encoders and dummy anomalies for abnormal event detection in video. 2019
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp 7834–7843,
    DOI 10.1109/CVPR.2019.00803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2011) Jiang F, Yuan J, Tsaftaris SA, Katsaggelos AK (2011) Anomalous
    video event detection using spatiotemporal context. URL https://dl.acm.org/doi/10.1016/j.cviu.2010.10.008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaur et al. (2018) Kaur P, Gangadharappa M, Gautam S (2018) An overview of
    anomaly detection in video surveillance. In: 2018 International Conference on
    Advances in Computing, Communication Control and Networking (ICACCCN), IEEE, DOI 10.1109/icacccn.2018.8748454,
    URL https://doi.org/10.1109/icacccn.2018.8748454'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim and Grauman (2009) Kim J, Grauman K (2009) Observe locally, infer globally:
    A space-time MRF for detecting abnormal activities with incremental updates. In:
    2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206569,
    URL https://doi.org/10.1109/cvpr.2009.5206569'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2017) Krizhevsky A, Sutskever I, Hinton GE (2017) Imagenet
    classification with deep convolutional neural networks. Commun ACM 60(6):84–90,
    DOI 10.1145/3065386, URL https://doi.org/10.1145/3065386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landi et al. (2019) Landi F, Snoek CGM, Cucchiara R (2019) Anomaly locality
    in video surveillance. ArXiv abs/1901.10364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2013) Li C, Han Z, Ye Q, Jiao J (2013) Visual abnormal behavior detection
    based on trajectory sparse reconstruction analysis. URL https://www.sciencedirect.com/science/article/abs/pii/S0925231213000179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2014) Li W, Mahadevan V, Vasconcelos N (2014) Anomaly detection and
    localization in crowded scenes. IEEE Transactions on Pattern Analysis and Machine
    Intelligence 36(1):18–32, DOI 10.1109/tpami.2013.111, URL https://doi.org/10.1109/tpami.2013.111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and min Cai (2016) Li X, min Cai Z (2016) Anomaly detection techniques in
    surveillance videos. In: 2016 9th International Congress on Image and Signal Processing,
    BioMedical Engineering and Informatics (CISP-BMEI), IEEE, DOI 10.1109/cisp-bmei.2016.7852681,
    URL https://doi.org/10.1109/cisp-bmei.2016.7852681'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2019) Lin S, Yang H, Tang X, Shi T, Chen L (2019) Social mil: Interaction-aware
    for crowd anomaly detection. In: 2019 16th IEEE International Conference on Advanced
    Video and Signal Based Surveillance (AVSS), pp 1–8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg
    AC (2016) SSD: Single shot MultiBox detector. In: Computer Vision – ECCV 2016,
    Springer International Publishing, pp 21–37, DOI 10.1007/978-3-319-46448-0_2,
    URL https://doi.org/10.1007/978-3-319-46448-0_2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu W, Luo W, Lian D, Gao S (2018) Future frame prediction
    for anomaly detection - a new baseline. 2018 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition pp 6536–6545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lopes et al. (2017) Lopes AT, de Aguiar E, De Souza AF, Oliveira-Santos T (2017)
    Facial expression recognition with convolutional neural networks. Pattern Recogn
    61(C):610–628, DOI 10.1016/j.patcog.2016.07.026, URL https://doi.org/10.1016/j.patcog.2016.07.026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2013) Lu C, Shi J, Jia J (2013) Abnormal event detection at 150
    fps in matlab. In: 2013 IEEE International Conference on Computer Vision, pp 2720–2727'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2019) Luo W, Liu W, Lian D, Tang J, Duan L, Peng X, Gao S (2019)
    Video anomaly detection with sparse coding inspired deep neural networks. IEEE
    Transactions on Pattern Analysis and Machine Intelligence pp 1–1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahadevan et al. (2010) Mahadevan V, LI WX, Bhalodia V, Vasconcelos N (2010)
    Anomaly detection in crowded scenes. In: Proceedings of IEEE Conference on Computer
    Vision and Pattern Recognition, pp 1975–1981'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malinowski et al. (2017) Malinowski M, Rohrbach M, Fritz M (2017) Ask your
    neurons: A deep learning approach to visual question answering. Int J Comput Vision
    125(1–3):110–135, DOI 10.1007/s11263-017-1038-2, URL https://doi.org/10.1007/s11263-017-1038-2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massi (2019) Massi M (2019) Wikimedia Commons. URL https://commons.wikimedia.org/wiki/File:Autoencoder_schema.png
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medel and Savakis (2016) Medel JR, Savakis AE (2016) Anomaly detection in video
    using predictive convolutional long short-term memory networks. CoRR abs/1612.00390,
    URL http://arxiv.org/abs/1612.00390, 1612.00390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehran et al. (2009) Mehran R, Oyama A, Shah M (2009) Abnormal crowd behavior
    detection using social force model. In: 2009 IEEE Conference on Computer Vision
    and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2009.5206641, URL https://doi.org/10.1109/cvpr.2009.5206641'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mei and Zhang (2017) Mei T, Zhang C (2017) Deep learning for intelligent video
    analysis. In: Proceedings of the 25th ACM International Conference on Multimedia,
    Association for Computing Machinery, New York, NY, USA, MM ’17, p 1955–1956, DOI 10.1145/3123266.3130141,
    URL https://doi.org/10.1145/3123266.3130141'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narasimhan and S. (2018) Narasimhan MG, S SK (2018) Dynamic video anomaly detection
    and localization using sparse denoising autoencoders. Multimedia Tools Appl 77(11):13173–13195,
    DOI 10.1007/s11042-017-4940-2, URL https://doi.org/10.1007/s11042-017-4940-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2011) Pan SJ, Tsang IW, Kwok JT, Yang Q (2011) Domain adaptation
    via transfer component analysis. IEEE Transactions on Neural Networks 22(2):199–210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popoola and Wang (2012) Popoola OP, Wang K (2012) Video-based abnormal human
    behavior recognition—a review. IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews) 42(6):865–878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramachandra and Jones (2020) Ramachandra B, Jones MJ (2020) Street scene: A
    new dataset and evaluation protocol for video anomaly detection. In: 2020 IEEE
    Winter Conference on Applications of Computer Vision (WACV), IEEE, DOI 10.1109/wacv45572.2020.9093457,
    URL https://doi.org/10.1109/wacv45572.2020.9093457'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro et al. (2018) Ribeiro M, Lazzaretti AE, Lopes HS (2018) A study of deep
    convolutional auto-encoders for anomaly detection in videos. Pattern Recognition
    Letters 105:13 – 22, DOI https://doi.org/10.1016/j.patrec.2017.07.016, URL http://www.sciencedirect.com/science/article/pii/S0167865517302489,
    machine Learning and Applications in Artificial Intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional
    networks for biomedical image segmentation. In: Lecture Notes in Computer Science,
    Springer International Publishing, pp 234–241, DOI 10.1007/978-3-319-24574-4_28,
    URL https://doi.org/10.1007/978-3-319-24574-4_28'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabokrou et al. (2016) Sabokrou M, Fathy M, Hoseini M (2016) Video anomaly detection
    and localisation based on the sparsity and reconstruction error of auto-encoder.
    Electronics Letters 52:1122–1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sabokrou et al. (2017) Sabokrou M, Fayyaz M, Fathy M, Klette R (2017) Deep-cascade:
    Cascading 3d deep neural networks for fast anomaly detection and localization
    in crowded scenes. IEEE Transactions on Image Processing 26(4):1992–2004'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sabokrou et al. (2018) Sabokrou M, Fayyaz M, Fathy M, Moayed Z, Klette R (2018)
    Deep-anomaly: Fully convolutional neural network for fast anomaly detection in
    crowded scenes. Computer Vision and Image Understanding 172:88 – 97, DOI https://doi.org/10.1016/j.cviu.2018.02.006,
    URL http://www.sciencedirect.com/science/article/pii/S1077314218300249'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sabzalian et al. (2019) Sabzalian B, Marvi H, Ahmadyfard A (2019) Deep and
    sparse features for anomaly detection and localization in video. In: 2019 4th
    International Conference on Pattern Recognition and Image Analysis (IPRIA), pp
    173–178'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[dos Santos] et al. (2019) [dos Santos] FP, Ribeiro LS, Ponti MA (2019) Generalization
    of feature embeddings transferred from different video anomaly detection domains.
    Journal of Visual Communication and Image Representation 60:407 – 416, DOI https://doi.org/10.1016/j.jvcir.2019.02.035,
    URL http://www.sciencedirect.com/science/article/pii/S1047320319300926'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sultani et al. (2018) Sultani W, Chen C, Shah M (2018) Real-world anomaly detection
    in surveillance videos. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp 6479–6488'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2015) Tran D, Bourdev L, Fergus R, Torresani L, Paluri M (2015)
    Learning spatiotemporal features with 3d convolutional networks. In: 2015 IEEE
    International Conference on Computer Vision (ICCV), pp 4489–4497'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tung et al. (2010) Tung F, Zelek JS, Clausi DA (2010) Goal-based trajectory
    analysis for unusual behaviour detection in intelligent surveillance. URL https://www.sciencedirect.com/science/article/abs/pii/S026288561000154X
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser u, Polosukhin I (2017) Attention is all you need. In: Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    Curran Associates Inc., Red Hook, NY, USA, NIPS’17, p 6000–6010'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vu et al. (2017) Vu H, Phung D, Nguyen TD, Trevors A, Venkatesh S (2017) Energy-based
    models for video anomaly detection. 1708.05211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Wang S, Zhu E, Yin J, Porikli F (2018) Video anomaly detection
    and localization by local motion based joint video representation and OCELM. Neurocomputing
    277:161–175, DOI 10.1016/j.neucom.2016.08.156, URL https://doi.org/10.1016/j.neucom.2016.08.156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wiskott and Sejnowski (2002) Wiskott L, Sejnowski TJ (2002) Slow feature analysis:
    Unsupervised learning of invariances. Neural Computation 14(4):715–770, DOI 10.1162/089976602317318938,
    URL https://doi.org/10.1162/089976602317318938'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Xu K, Sun T, Jiang X (2020) Video anomaly detection and localization
    based on an adaptive intra-frame classification network. IEEE Transactions on
    Multimedia 22(2):394–406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2016) Yan H, Liu X, Hong R (2016) Image classification via fusing
    the latent deep cnn feature. In: Proceedings of the International Conference on
    Internet Multimedia Computing and Service, Association for Computing Machinery,
    New York, NY, USA, ICIMCS’16, p 110–113, DOI 10.1145/3007669.3007706, URL https://doi.org/10.1145/3007669.3007706'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Zhang Y, Lu H, Zhang L, Ruan X (2016) Combining motion and
    appearance cues for anomaly detection. Pattern Recognition 51:443–452, DOI 10.1016/j.patcog.2015.09.005,
    URL https://doi.org/10.1016/j.patcog.2015.09.005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2017) Zhao Y, Deng B, Shen C, Liu Y, Lu H, Hua XS (2017) Spatio-temporal
    autoencoder for video anomaly detection. In: Proceedings of the 25th ACM International
    Conference on Multimedia, Association for Computing Machinery, New York, NY, USA,
    MM ’17, p 1933–1941, DOI 10.1145/3123266.3123451, URL https://doi.org/10.1145/3123266.3123451'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Zhou JT, Du J, Zhu H, Peng X, Liu Y, Goh RSM (2019) Anomalynet:
    An anomaly detection network for video surveillance. IEEE Transactions on Information
    Forensics and Security 14(10):2537–2550'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Newsam (2019) Zhu Y, Newsam S (2019) Motion-aware feature for improved
    video anomaly detection. British Machine Vision Conference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zoph et al. (2018) Zoph B, Vasudevan V, Shlens J, Le QV (2018) Learning transferable
    architectures for scalable image recognition. In: 2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, IEEE, DOI 10.1109/cvpr.2018.00907, URL
    https://doi.org/10.1109/cvpr.2018.00907'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
