- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.04766] Explainable Deep Learning Methods in Medical Image Classification:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.04766] 医学图像分类中的可解释深度学习方法：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.04766](https://ar5iv.labs.arxiv.org/html/2205.04766)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.04766](https://ar5iv.labs.arxiv.org/html/2205.04766)
- en: 'Explainable Deep Learning Methods in Medical Image Classification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医学图像分类中的可解释深度学习方法：综述
- en: Cristiano Patrício [cristiano.patricio@ubi.pt](mailto:cristiano.patricio@ubi.pt)
    ,  João C. Neves [jcneves@di.ubi.pt](mailto:jcneves@di.ubi.pt) University of Beira
    Interior and NOVA LINCSCovilhãPortugal6201-001  and  Luís F. Teixeira [luisft@fe.up.pt](mailto:luisft@fe.up.pt)
    University of Porto and INESC TECPortoPortugal4200-465(2023)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Cristiano Patrício [cristiano.patricio@ubi.pt](mailto:cristiano.patricio@ubi.pt)
    ， João C. Neves [jcneves@di.ubi.pt](mailto:jcneves@di.ubi.pt) 比拉内尔大学和NOVA LINCS科维良葡萄牙6201-001
    以及 Luís F. Teixeira [luisft@fe.up.pt](mailto:luisft@fe.up.pt) 波尔图大学和INESC TEC波尔图葡萄牙4200-465（2023）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The remarkable success of deep learning has prompted interest in its application
    to medical imaging diagnosis. Even though state-of-the-art deep learning models
    have achieved human-level accuracy on the classification of different types of
    medical data, these models are hardly adopted in clinical workflows, mainly due
    to their lack of interpretability. The black-box-ness of deep learning models
    has raised the need for devising strategies to explain the decision process of
    these models, leading to the creation of the topic of eXplainable Artificial Intelligence
    (XAI). In this context, we provide a thorough survey of XAI applied to medical
    imaging diagnosis, including visual, textual, example-based and concept-based
    explanation methods. Moreover, this work reviews the existing medical imaging
    datasets and the existing metrics for evaluating the quality of the explanations.
    In addition, we include a performance comparison among a set of report generation-based
    methods. Finally, the major challenges in applying XAI to medical imaging and
    the future research directions on the topic are also discussed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的显著成功激发了对其在医学影像诊断中应用的兴趣。尽管最先进的深度学习模型在分类不同类型的医学数据方面达到了人类水平的准确性，但这些模型在临床工作流中的应用仍然很少，主要原因在于其缺乏可解释性。深度学习模型的黑箱特性引发了制定解释这些模型决策过程的策略的需求，从而催生了可解释人工智能（XAI）这一话题。在这一背景下，我们提供了关于应用于医学影像诊断的XAI的全面综述，包括视觉、文本、基于示例和基于概念的解释方法。此外，本文还回顾了现有的医学影像数据集和评估解释质量的现有指标。此外，我们还包括了一组基于报告生成的方法的性能比较。最后，还讨论了将XAI应用于医学影像的主要挑战以及未来的研究方向。
- en: 'Explainable AI, Explainability, Interpretability, Deep Learning, Medical Image
    Analysis^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi: 10.1145/nnnnnnn.nnnnnnn^†^†ccs:
    Applied computing Health care information systems'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能、可解释性、解释能力、深度学习、医学图像分析^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：10.1145/nnnnnnn.nnnnnnn^†^†ccs：应用计算
    健康护理信息系统
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The progress made on the last decade in the field of artificial intelligence
    (AI) has supported a dramatic increase in the accuracy of most computer vision
    applications. Medical image analysis is one of the applications where the progress
    made assured human-level accuracy on the classification of different types of
    medical data (e.g., chest X-rays ([91](#bib.bib91)), corneal images ([167](#bib.bib167))).
    However, and in spite of these advances, automated medical imaging is seldom adopted
    in clinical practice. According to Zachary Lipton ([78](#bib.bib78)), the explanation
    to this apparent paradox is straightforward, doctors will never trust the decision
    of an algorithm without understanding its decision process. This fact has raised
    the need for producing strategies capable of explaining the decision process of
    AI algorithms, leading subsequently to the creation of a novel research topic
    named as eXplainable Artificial Intelligence (XAI). According to DARPA ([47](#bib.bib47)),
    XAI aims to “produce more explainable models, while maintaining a high level of
    learning performance (prediction accuracy); and enable human users to understand,
    appropriately, trust, and effectively manage the emerging generation of artificially
    intelligent partners”. In spite of its general applicability, XAI is particularly
    important in high-stake decisions, such as clinical workflow, where the consequences
    of a wrong decision could lead to human deaths. This is also evidenced by European
    Union’s General Data Protection Regulation (GDPR) law, which requires an explanation
    of the decision-making process of the algorithm, thus improving its transparency
    before it can be used for patientcare ([42](#bib.bib42)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，人工智能（AI）领域取得的进展支持了大多数计算机视觉应用的准确性显著提高。医学图像分析是其中一个进展显著的应用领域，它在分类不同类型的医学数据（例如，胸部
    X 光片 ([91](#bib.bib91))，角膜图像 ([167](#bib.bib167)））方面达到了人类水平的准确性。然而，尽管有这些进展，自动化医学影像在临床实践中的应用仍然很少。根据扎卡里·利普顿 ([78](#bib.bib78))，这一明显矛盾的解释很简单，医生永远不会信任算法的决定，除非他们理解其决策过程。这一事实引发了对能够解释
    AI 算法决策过程的策略的需求，随后催生了一个新的研究主题——可解释人工智能（XAI）。根据 DARPA ([47](#bib.bib47))，XAI 旨在“产生更多可解释的模型，同时保持高水平的学习性能（预测准确性）；并使人类用户能够理解、适当地信任和有效地管理新一代人工智能伙伴”。尽管其具有普遍适用性，但
    XAI 在高风险决策中尤为重要，例如临床工作流程，其中错误决策的后果可能导致人员死亡。这一点也得到了欧盟通用数据保护条例（GDPR）法律的支持，该法律要求对算法的决策过程进行解释，从而在算法用于患者护理之前提高其透明度
    ([42](#bib.bib42))。
- en: Considering this, it is of utmost importance to invest in the research of novel
    strategies to improve the interpretability of deep learning methods before being
    possible to deploy them into clinical practice. During the last years, the research
    on this topic has focused primarily on devising methods for indirectly analysing
    the decision process of pre-built models. These methods operate either by analysing
    the impact of specific regions of the input images on the final prediction (perturbation-based
    methods ([113](#bib.bib113); [88](#bib.bib88)) and occlusion-based methods ([174](#bib.bib174)))
    or inspecting the network activations (saliency methods ([128](#bib.bib128); [177](#bib.bib177))).
    The fact that these methods can be applied to arbitrary network architectures
    without requiring an additional customization of the model has supported their
    popularity in the early days of XAI. However, it has been recently shown that
    post-hoc strategies suffer from several drawbacks regarding the significance of
    the explanations ([118](#bib.bib118); [3](#bib.bib3)). As a consequence, researchers
    have focused their attention in the design of models/architectures capable of
    explaining their decision process per se. Inherently interpretable models are
    believed to be particularly useful in medical imaging ([118](#bib.bib118)), justifying
    the recent growth in the number of medical imaging works focusing on this paradigm
    rather the traditional post-hoc strategies ([61](#bib.bib61); [163](#bib.bib163)).
    In spite of the recent popularity of inherently interpretable models, the existing
    surveys on the interpretability of deep learning applied to medical imaging have
    not comprehensively reviewed the progress done in this novel research trend. Also,
    the significant increase in the number of works focused on the interpretation
    of the decision process of deep learning applied to medical imaging justifies
    the need for an updated review over the most recent methods not covered by the
    last surveys on the topic. Moreover, the particular challenges of medical imaging
    analysis, including image complexity (anatomical structures, organs, and artifacts
    are often harder to identify compared to general images), data availability, and
    the misclassification risk, emphasize the need for a dedicated survey on interpretability
    applied to medical imaging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，在将深度学习方法部署到临床实践之前，投资于研究改善深度学习方法可解释性的创新策略至关重要。在过去几年中，这一主题的研究主要集中在制定间接分析预构建模型决策过程的方法。这些方法通过分析输入图像特定区域对最终预测的影响（基于扰动的方法 ([113](#bib.bib113);
    [88](#bib.bib88)) 和基于遮挡的方法 ([174](#bib.bib174))) 或检查网络激活（显著性方法 ([128](#bib.bib128);
    [177](#bib.bib177))) 来操作。这些方法可以应用于任意网络架构，而无需对模型进行额外的定制，这支持了它们在XAI早期阶段的流行。然而，最近已显示出后期策略在解释的重要性方面存在一些缺陷 ([118](#bib.bib118);
    [3](#bib.bib3))。因此，研究人员将注意力集中在设计能够自我解释其决策过程的模型/架构上。固有可解释模型被认为在医学成像中尤其有用 ([118](#bib.bib118))，这也解释了最近在这一范式下而非传统后期策略中的医学成像工作的增长。尽管固有可解释模型最近受到欢迎，但现有的关于应用于医学成像的深度学习可解释性的调查尚未全面回顾这一新兴研究趋势中的进展。此外，针对深度学习在医学成像中应用的决策过程的研究数量显著增加，说明了对最新方法进行更新审查的必要性，这些方法未被最后的主题调查覆盖。此外，医学成像分析的特定挑战，包括图像复杂性（与一般图像相比，解剖结构、器官和伪影往往更难识别）、数据可用性和误分类风险，强调了对医学成像应用的可解释性进行专门调查的必要性。
- en: 'To address these concerns, we comprehensively review the recent advances on
    explainable deep learning applied to medical diagnosis. In particular, this survey
    provides the following contributions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们全面回顾了应用于医学诊断的可解释深度学习的最新进展。特别是，本调查提供了以下贡献：
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a review of the recent surveys on the topic of interpretable deep learning in
    medical imaging, including the major conclusions derived from each work, as well
    as a comparative analysis to our survey;
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于可解释深度学习在医学成像中的最新调查的回顾，包括从每项工作中得出的主要结论，以及与我们的调查的比较分析；
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: an exhaustive list of the datasets commonly used in the study of interpretability
    of deep learning approaches applied to medical imaging;
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习方法应用于医学成像研究中常用的数据集的详尽列表；
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a comprehensive review of the state-of-the-art interpretable medical imaging
    approaches, covering both post-hoc and inherently interpretable models;
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对最先进的可解释医学影像方法进行全面审查，包括事后和本质上可解释的模型。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a complete description of the metrics commonly used for benchmarking interpretability
    methods either for visual or textual explanations;
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常用于基准测试可解释性方法的指标的完整描述，无论是可视的还是文本的解释。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a benchmark of interpretable medical imaging approaches regarding the quality
    of the textual explanations;
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于文本解释质量的可解释医学影像方法的基准测试。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the future research directions on the topic of interpretable deep learning in
    medical imaging.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在医学影像可解释深度学习主题的未来研究方向。
- en: 2\. Related Surveys
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 关联调查
- en: Explaining the decisions of deep learning models has been an active area of
    research, with various methods and algorithms proposed in the literature in the
    last years. The rapid pace of development of XAI has raised the need for comprehensive
    overviews of the advances in the state-of-the-art, and in most cases, the analysis
    of specific domains, due to the vast number of works published in the last years.
    Accordingly, in this section, we provide a critical analysis of the existing surveys
    in deep learning applied to medical imaging (section 2.1), with a particular focus
    on explainable approaches (section 2.2), and we compare the surveys analyzed with
    our survey (section 2.3).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 解释深度学习模型的决策已经是近年来研究的一个活跃领域，文献中提出了各种方法和算法。XAI的快速发展引发了对最新技术发展的全面概述的需求，大多数情况下，需要分析特定领域，因为近年来发表的作品数量庞大。因此，在本节中，我们对应用于医学成像的深度学习的现有调查进行了批判性分析（第2.1节），特别关注可解释方法（第2.2节），并将我们的调查与已分析的调查进行了比较（第2.3节）。
- en: 2.1\. Deep Learning in Medical Image Analysis
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 医学图像分析中的深度学习
- en: The advent of deep learning significantly changed the field of Computer Vision,
    where handcrafted feature extraction was replaced by end-to-end learning using
    Convolutional Neural Networks (CNNs). This new paradigm emerged in 2012 through
    the seminal work of Krizhevsky et al. ([66](#bib.bib66)), but it was not immediately
    incorporated by all the applications of Computer Vision. Litjens et al. ([80](#bib.bib80))
    were the first to review the advances on medical image analysis fostered by the
    advent of deep learning, where is clear that the use of CNNs in medical imaging
    research has only become the standard approach in 2017, being clearly preferred
    over traditional handcrafted feature extraction for most of the anatomical regions.
    Based on the works reviewed, the authors concluded that data preprocessing and
    data augmentation techniques were essential to obtain superior results, and that
    the combination of medical images with text data (medical reports) could improve
    the image classification accuracy ([125](#bib.bib125)). Despite the relevance
    of this survey at the time, the rapid advances in the field of deep learning occurring
    in the last 5 years have made this work outdated, since the major conclusions
    of the survey are currently common sense, and novel deep learning models are currently
    being used in the medical imaging domain.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的出现显著改变了计算机视觉领域，其中手工制作的特征提取被卷积神经网络（CNNs）的端到端学习所取代。这一新范式始于2012年Krizhevsky等人的开创性工作（[66](#bib.bib66)），但并没有立即被所有的计算机视觉应用所应用。Litjens等人（[80](#bib.bib80)）首先总结了深度学习引发的医学图像分析的进展，清楚地表明在2017年，CNN在医学成像研究中只有成为标准方法，被明显地优先于传统的手工制作特征提取，适用于大多数解剖区域。根据综述的作品，作者得出结论，数据预处理和数据增强技术对于获得更好的结果至关重要，并且医学图像与文本数据（医学报告）的组合可以提高图像分类的准确性（[125](#bib.bib125)）。尽管当时这项调查的相关性很大，但是过去5年深度学习领域的快速发展使得该工作已经过时，因为调查的主要结论目前已经司空见惯，新的深度学习模型目前正在医学成像领域得到应用。
- en: Considering this, Rehman et al. ([112](#bib.bib112)) provided an updated overview
    of the advances on deep learning applied to medical image analysis. The survey
    was divided over different pattern recognition tasks (image classification, segmentation,
    image registration). Regarding the image classification task, the authors suggested
    using generative models to perform data augmentation to improve the results. The
    survey also gave future research directions to overcome the most common challenges
    identified by Litjens et al. ([80](#bib.bib80)). The use of techniques such as
    transfer learning and synthetic data generation were suggested to address these
    challenges while improving the generalization capability of the developed strategies.
    Nevertheless, the authors concluded that the non-availability of large-scale annotated
    datasets remains one of the major challenges in medical imaging, which is impacting
    the performance of the deep learning models due to data overfitting and bias issues.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，Rehman 等人 ([112](#bib.bib112)) 提供了有关应用于医学图像分析的深度学习进展的更新概述。该综述被分为不同的模式识别任务（图像分类、分割、图像配准）。关于图像分类任务，作者建议使用生成模型进行数据增强以提高结果。该综述还给出了未来研究方向，以克服
    Litjens 等人 ([80](#bib.bib80)) 识别出的最常见挑战。建议使用如迁移学习和合成数据生成等技术来应对这些挑战，同时提升所开发策略的泛化能力。然而，作者总结道，大规模注释数据集的不可用仍然是医学成像领域的主要挑战之一，这影响了深度学习模型的性能，导致数据过拟合和偏差问题。
- en: 2.2\. Interpretable Deep Learning in Medical Imaging
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 可解释的深度学习在医学成像中的应用
- en: The works of Litjens et al. ([80](#bib.bib80)) and Rehman et al. ([112](#bib.bib112))
    show that in the last years the use of deep learning has greatly improved the
    performance of medical imaging analysis algorithms, allowing also to create a
    myriad of approaches for the different image modalities and recognition tasks.
    Nevertheless, this contrasts with the adoption of these algorithms by clinicians
    who refuse to rely on decisions that they do not understand ([78](#bib.bib78)).
    In fact, as foresaw by Litjens et al. ([80](#bib.bib80)), the importance of designing
    interpretable models for medical imaging has been growing in the last years and
    is nowadays one of the major challenges in medical imaging. The following paragraphs
    describe the different surveys focused on reviewing the recent advances on interpretable
    deep learning applied to medical imaging. Additionally, the major conclusions
    derived from each work and a brief comparison to our survey are also outlined.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Litjens 等人 ([80](#bib.bib80)) 和 Rehman 等人 ([112](#bib.bib112)) 的研究表明，近年来深度学习的使用极大地提高了医学成像分析算法的性能，也使得为不同的图像模态和识别任务创建了大量的方法。然而，这与临床医生拒绝依赖他们不了解的决策形成了对比 ([78](#bib.bib78))。实际上，正如
    Litjens 等人 ([80](#bib.bib80)) 预见的那样，设计可解释模型在医学成像中的重要性在近年来不断增长，现在已成为医学成像中的主要挑战之一。接下来的段落描述了专注于回顾可解释深度学习在医学成像中应用的不同综述。此外，还概述了每项工作的主要结论及与我们综述的简要比较。
- en: 2.2.1\. General Reviews
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 综合评论
- en: 'Tjoa and Guan ([146](#bib.bib146)) provide a general overview of machine learning
    and deep learning interpretability methods with an emphasis on its application
    to medical field. The authors consider two types of interpretability: (i) perceptive
    interpretability, where the saliency methods are included, and (ii) interpretability
    by mathematical structures, which include mathematical formulations that can analyze
    the patterns in data. Although a significant number of works per image modality
    are covered, the survey lacks a comparison between the reviewed works. Also, the
    survey of Tjoa and Guan is more suitable for technical oriented readers, which
    is corroborated by a poor intuitive categorization for the medical community.
    Most works were discussed based on their mathematical foundation instead of describing
    the rationale behind the proposed method. As major conclusions, Tjoa and Guan
    state that combining visual and textual explanations is the most promising modality
    for conveying the explanations of medical imaging analysis algorithms, and that
    these algorithms should always be considered as a complementary aid/support to
    clinicians, who should be responsible for the final decision.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Tjoa 和 Guan ([146](#bib.bib146)) 提供了机器学习和深度学习解释性方法的一般概述，重点介绍其在医学领域的应用。作者考虑了两种解释性：
    (i) 感知解释性，其中包括显著性方法，和 (ii) 通过数学结构的解释性，其中包括能够分析数据模式的数学公式。虽然涵盖了大量图像模态的研究，但该调查缺乏对所评审工作的比较。此外，Tjoa
    和 Guan 的调查更适合技术导向的读者，这一点通过对医学界的直观分类较差得到证实。大多数工作是基于其数学基础进行讨论的，而不是描述所提出方法背后的理由。作为主要结论，Tjoa
    和 Guan 指出，将视觉和文本解释结合起来是传达医学影像分析算法解释的最有前景的方式，并且这些算法应始终作为对临床医生的补充援助/支持，最终决策应由临床医生负责。
- en: Singh et al. ([134](#bib.bib134)) propose a review of the works related to the
    explainability of deep learning models in the context of medical imaging. The
    methods are broadly divided in two major categories (attribution-based and non
    attribution-based) with respect to their capability of determining the contribution
    of an input feature to the target output. Both categories are reviewed by describing
    works applied to the different image modalities of medical data. Nevertheless,
    the survey focuses primarily on the attribution-based category, providing a superficial
    discussion of existing methods on the different categories of non-attribution
    methods, where inherently interpretable approaches are included. Based on the
    works reviewed, the authors conclude that leveraging patient record data and images
    can be an exciting research direction to push forward the performance of deep
    learning in medical imaging. When compared to our survey, the work of Singh et
    al. lacks a comprehensive analysis of inherently interpretable approaches, an
    analysis of the available medical imaging datasets, and the benchmarking of the
    most prominent reviewed methods.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Singh 等 ([134](#bib.bib134)) 提出了一个关于医学影像中深度学习模型可解释性的工作的综述。这些方法大致分为两大类（基于归因的和非基于归因的），根据它们确定输入特征对目标输出的贡献的能力。两类方法通过描述应用于不同医学数据图像模态的工作进行回顾。然而，该调查主要集中在基于归因的类别上，对于非基于归因的方法的现有方法的讨论较为肤浅，其中包括内在可解释的方法。基于回顾的工作，作者总结出利用患者记录数据和图像可以成为推动深度学习在医学影像中表现的一个令人兴奋的研究方向。与我们的调查相比，Singh
    等的工作缺乏对内在可解释方法的全面分析，对现有医学影像数据集的分析，以及对最突出评审方法的基准测试。
- en: Recently, Salahuddin et al. ([120](#bib.bib120)) review a set of interpretability
    methods which are grouped into nine different categories based on the type of
    explanations generated. They also discuss the problem of evaluating explanations
    and describe a set of evaluation strategies adopted to quantitatively and qualitatively
    measure the explanations’ quality. Similarly to the other surveys, the authors
    also emphasize the importance of involving clinicians in designing and validating
    interpretability models to ensure the utility of the generated explanations. For
    the future perspectives, Salahuddin et al. claim that case-based and concept-learning
    models are promising interpretability models for being inherently interpretable
    and achieving similar performance to black-box CNNs. Despite being one of the
    most complete surveys on the topic, it lacks the description of most relevant
    datasets of the field as well as the benchmarking of most prominent approaches
    reviewed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Salahuddin 等人 ([120](#bib.bib120)) 对一组可解释性方法进行了评审，这些方法根据生成的解释类型被分为九个不同的类别。他们还讨论了评估解释的问题，并描述了一套用于定量和定性测量解释质量的评估策略。与其他调查类似，作者们也强调了在设计和验证可解释性模型时涉及临床医生的重要性，以确保生成解释的实用性。对于未来的展望，Salahuddin
    等人声称，基于案例和概念学习的模型是有前景的可解释性模型，因为它们本质上是可解释的，并且能实现与黑箱 CNN 相似的性能。尽管这是该主题最完整的调查之一，但它缺乏对该领域大多数相关数据集的描述，以及对评审的最突出方法的基准测试。
- en: 2.2.2\. Specific Image Modality Reviews
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 特定图像模式评审
- en: 'In contrast to the above-mentioned works, the work of Pocevičiūtė et al. ([109](#bib.bib109))
    is focused on a particular image modality. The XAI techniques devised for digital
    pathology are reviewed with respect to three criteria: 1) what is going to be
    explained (e.g., model predictions, predictions uncertainty); 2) explanation modality;
    3) how the explanations are derived (e.g., perturbation-based strategies, interpretable
    network design). The authors point out the importance of developing a toolbox
    for objectively measuring the quality of explanations, as the lack of an evaluation
    framework remains an open problem in the XAI field. Additionally, the authors
    state that the use of counterfactual examples can enhance the interpretability
    of the methods. When compared to our survey, this work has disregarded the textual
    explanation modality, focusing solely on visual explanations, either by visual
    examples or saliency maps.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述工作相比，Pocevičiūtė 等人 ([109](#bib.bib109)) 的工作专注于特定的图像模式。针对数字病理学的 XAI 技术根据三个标准进行了评审：1）将要解释的内容（例如，模型预测、预测不确定性）；2）解释模式；3）解释的推导方式（例如，基于扰动的策略、可解释网络设计）。作者指出了开发一个用于客观测量解释质量的工具箱的重要性，因为缺乏评估框架仍然是
    XAI 领域中的一个开放问题。此外，作者表示，使用反事实示例可以增强方法的可解释性。与我们的调查相比，这项工作忽略了文本解释模式，仅关注于视觉解释，无论是通过视觉示例还是显著性图。
- en: Gulum et al. ([46](#bib.bib46)) produce a review of the visual explainability
    techniques applied to cancer detection from Magnetic Resonance Imaging (MRI) scans.
    Contrary to the work of Pocevičiūtė et al. ([109](#bib.bib109)), Gulum et al.
    discuss the strategies used for measuring the quality of explanations, but they
    only consider one metric to quantitatively evaluate the explanations. They also
    emphasize that there is a lack of studies that assess the explanation methods
    based on human evaluation. As future directions, Gulum et al. highlight the need
    for developing inherently interpretable approaches, as opposed to the traditional
    post-hoc strategy. Finally, the authors proposed the use of uncertainty estimation
    associated with model predictions to perceive how a model is confident in making
    a prediction. Despite its relevance, the work of Gulum et al. is specific to a
    particular image modality (MRI) and target disease (cancer).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Gulum 等人 ([46](#bib.bib46)) 对应用于癌症检测的磁共振成像 (MRI) 扫描的视觉解释技术进行了评审。与 Pocevičiūtė
    等人 ([109](#bib.bib109)) 的工作相反，Gulum 等人讨论了用于衡量解释质量的策略，但他们仅考虑了一种度量标准来定量评估解释。他们还强调，缺乏基于人类评估的解释方法研究。作为未来的方向，Gulum
    等人强调了开发本质上可解释的方法的必要性，而不是传统的事后策略。最后，作者提出了使用与模型预测相关的不确定性估计来感知模型在进行预测时的信心。尽管其相关性很高，但
    Gulum 等人的工作仅针对特定的图像模式（MRI）和目标疾病（癌症）。
- en: 2.2.3\. Specific Explanation Modality Reviews
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 特定解释模式评审
- en: 'While visual explanations are usually the primary option for explaining the
    model decisions, these strategies can be unreliable since they often highlight
    regions regardless of the class of interest ([118](#bib.bib118)). This has fostered
    the research on textual explanations, and in the particular case of medical imaging
    led researchers to devise approaches capable of producing different types of textual
    explanations: 1) textual concepts and 2) textual reports. The recent developments
    on this topic have been covered in the surveys of Messina et al. ([96](#bib.bib96))
    and Ayesha et al. ([9](#bib.bib9)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然视觉解释通常是解释模型决策的主要选择，但这些策略可能不可靠，因为它们经常突出显示区域，而不考虑兴趣类别 ([118](#bib.bib118))。这促使了对文本解释的研究，特别是在医学成像的情况下，促使研究人员设计了能够产生不同类型文本解释的方法：1）文本概念和2）文本报告。Messina等人 ([96](#bib.bib96))
    和Ayesha等人 ([9](#bib.bib9)) 的调查涵盖了这一主题的最新进展。
- en: 'In ([96](#bib.bib96)), a thorough overview of the current state-of-the-art
    on automatic report generation from medical images is provided. The authors review
    40 papers with respect to four dimensions: datasets used, model design, explainability
    and evaluation metrics. Furthermore, a benchmark of most relevant approaches is
    provided with respect to the performance in terms of NLP metrics on IU Chest X-ray
    dataset. Based on the works analysed, the authors identify the following challenges
    and future research directions: 1) the validation of the obtained explanations
    by clinicians is impractical, being necessary to create automatic metrics positively
    correlated with the clinicians opinion; 2) most research has concentrated on chest
    X-rays, mainly due to the availability of public data; 3) supervised learning
    may not be the most adequate strategy for medical report generation learning,
    whereas reinforcement learning seems a more reasonable training paradigm to explore.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([96](#bib.bib96)) 中，提供了有关从医学图像生成自动报告的当前最先进技术的全面概述。作者回顾了40篇论文，涉及四个维度：使用的数据集、模型设计、可解释性和评估指标。此外，还提供了最相关方法的基准，基于IU胸部X光数据集的NLP指标性能。根据分析的工作，作者确定了以下挑战和未来研究方向：1）获得的解释由临床医生验证不切实际，需要创建与临床医生意见正相关的自动指标；2）大多数研究集中在胸部X光上，主要由于公共数据的可用性；3）监督学习可能不是医学报告生成学习的最合适策略，而强化学习似乎是一个更合理的训练范式。
- en: Similarly to the work of Messina et al., Ayesha et al. ([9](#bib.bib9)) presents
    a detailed survey of the existing automatic caption generation methods for medical
    images. The most used datasets and the evaluation metrics are also discussed.
    An extensive study is done around the most significant works under the various
    deep learning-based medical imaging caption generation methods, namely encoder-decoder
    based, retrieval-based, attention-based, concepts detection-based, and patient’s
    metadata-based. Additionally, a comparative analysis of the performance of the
    methods reviewed is provided. Finally, Ayesha et al. suggest some future research
    directions to deal with the main open issues in medical imaging. They point out
    the lack of large-scale annotated datasets as the major limitation in the medical
    imaging field, where the data is scarce and often mislabelled. Also, they claim
    that the lack of a suitable evaluation metric to assess the generated caption
    remains an open problem since the evaluation of the generated text is still based
    on standard NLP metrics, such as BLEU score, ROUGE, METEOR, and CIDEr. As a final
    remark, Ayesha et al. also indicate the importance of having a model capable of
    detecting multiple diseases simultaneously.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Messina等人的工作，Ayesha等人 ([9](#bib.bib9)) 对现有的医学图像自动生成字幕方法进行了详细的调查。最常用的数据集和评估指标也被讨论。对各种基于深度学习的医学图像字幕生成方法进行了广泛的研究，包括编码器-解码器基、检索基、注意力基、概念检测基和患者元数据基。此外，还提供了对审查方法性能的比较分析。最后，Ayesha等人建议了一些未来的研究方向，以解决医学成像中的主要开放问题。他们指出，缺乏大规模标注数据集是医学成像领域的主要限制，在该领域数据稀缺且经常被错误标注。此外，他们声称缺乏适当的评估指标来评估生成的字幕仍然是一个悬而未决的问题，因为对生成文本的评估仍基于标准的NLP指标，如BLEU分数、ROUGE、METEOR和CIDEr。最后，Ayesha等人还指出了拥有一个能够同时检测多种疾病的模型的重要性。
- en: Table 1\. Comparative analysis between the surveys on the topic of explainable
    deep learning applied to medical imaging. Our survey is the first to comprehensively
    review the advances on the topic regarding the different explanation modalities
    and the explanation processes. Also, it analyses the most relevant datasets on
    the the field, as well as their use for the development of explainable approaches.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 关于应用于医学成像的可解释深度学习主题的综述之间的比较分析。我们的综述是首个全面回顾该主题的不同解释模式和解释过程的进展的综述。同时，它分析了该领域最相关的数据集，以及这些数据集在可解释方法开发中的应用。
- en: '| Survey | Year | Explanations | Model Type | Medical Imaging | Benchmarking
    Performance |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 年份 | 解释 | 模型类型 | 医学成像 | 基准性能 |'
- en: '| Visual | Textual | Post-hoc | In-model | Datasets |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 视觉 | 文本 | 事后解释 | 内部模型 | 数据集 |'
- en: '| Pocevičiūtė et al. ([109](#bib.bib109)) | 2020 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Pocevičiūtė 等 ([109](#bib.bib109)) | 2020 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Tjoa and Guan ([146](#bib.bib146)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Tjoa 和 Guan ([146](#bib.bib146)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Singh et al. ([134](#bib.bib134)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Singh 等 ([134](#bib.bib134)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Gulum et al. ([46](#bib.bib46)) | 2021 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Gulum 等 ([46](#bib.bib46)) | 2021 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Ayesha et al. ([9](#bib.bib9)) | 2021 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Ayesha 等 ([9](#bib.bib9)) | 2021 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
- en: '| Salahuddin et al. ([120](#bib.bib120)) | 2022 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Salahuddin 等 ([120](#bib.bib120)) | 2022 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Messina et al. ([96](#bib.bib96)) | 2022 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Messina 等 ([96](#bib.bib96)) | 2022 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
- en: '| This survey | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 本综述 | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 2.3\. Discussion
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 讨论
- en: 'Despite the significant contributions of each reviewed survey, few of them
    have described the most important datasets for medical imaging. Moreover, most
    surveys focused on particular aspects of interpretability, such as visual or textual
    approaches, and few works have comprehensively reviewed inherently interpretable
    models devised for medical image analysis. Another problem was the lack of a performance
    comparison among the reviewed methods. Accordingly, this survey covers these limitations
    by providing a broader overview of the current state-of-the-art XAI applied to
    medical diagnosis, including uni and multimodal approaches, followed by the most
    important medical imaging datasets and a comparative analysis of the models performance
    using standard evaluation metrics. In addition, this survey explores a contemporary
    trend and an under-exploited category of inherently interpretable models, specifically
    concept-based learning approaches. As delineated in subsequent sections, these
    approaches are advantageous for medical diagnosis as they provide explanations
    in the context of high-level concepts that align with the knowledge of the physicians
    and promote the interaction between physicians and AI through model intervention.
    Table [1](#S2.T1 "Table 1 ‣ 2.2.3\. Specific Explanation Modality Reviews ‣ 2.2\.
    Interpretable Deep Learning in Medical Imaging ‣ 2\. Related Surveys ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey") summarizes the
    major differences between the reviewed surveys and our work (This survey).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每项综述都有显著贡献，但很少有综述描述了医学成像领域最重要的数据集。此外，大多数综述集中在解释性特定方面，如视觉或文本方法，少数工作全面回顾了为医学图像分析制定的固有可解释模型。另一个问题是缺乏对审阅方法之间性能的比较。因此，本综述通过提供对当前最先进的
    XAI 在医学诊断中的应用的更广泛概述，包括单模态和多模态方法，涵盖了这些局限性，并紧接着介绍了最重要的医学成像数据集，以及使用标准评估指标的模型性能比较。此外，本综述还探索了一个当代趋势和一个未充分利用的固有可解释模型类别，特别是基于概念的学习方法。如后续章节所述，这些方法对医学诊断具有优势，因为它们提供了与医生知识对齐的高层次概念背景中的解释，并通过模型干预促进医生与
    AI 之间的互动。表 [1](#S2.T1 "表 1 ‣ 2.2.3\. 特定解释模式综述 ‣ 2.2\. 医学成像中的可解释深度学习 ‣ 2\. 相关综述
    ‣ 医学图像分类中的可解释深度学习方法：综述") 总结了所审阅的综述与我们的工作（本综述）之间的主要差异。
- en: '![Refer to caption](img/87b8a354e32ab9681eb98e9f590606f3.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/87b8a354e32ab9681eb98e9f590606f3.png)'
- en: Figure 1\. Proposed categorization of the XAI methods taxonomy. The proposed
    categorization was inspired by the various taxonomies presented in the reviewed
    papers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 提议的 XAI 方法分类法。提议的分类法受到审阅论文中提出的各种分类法的启发。
- en: 3\. Background in XAI
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. XAI 背景
- en: From a historical perspective, the problem of explaining expert systems has
    its origin in the mid-80s ([99](#bib.bib99)), but the term XAI was only introduced
    in 2004 by Van et al. ([153](#bib.bib153)). Nevertheless, XAI has only gained
    prominence when deep learning dominated AI, and the first sign of this interest
    was demonstrated by the launching, in 2015, of the Explainable AI program by DARPA,
    whose primary goal was producing more explainable models to increase their understanding
    and transparency, leading to greater trust by users. Later, the European Union
    (EU) ([42](#bib.bib42)) introduced legislation about the “right to algorithmic
    explanation” which provided citizens with the right to receive an explanation
    for algorithmic decisions obtained from personal data. Considering this, researchers
    shifted their efforts towards the creation of interpretable models rather than
    simply focusing on accuracy, leading to an exponential increase in the popularity
    and interest on XAI, whose number of works on the topic has rapidly increased
    in the last years.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史角度来看，解释专家系统的问题起源于 80 年代中期（[99](#bib.bib99)），但 XAI 这一术语直到 2004 年由 Van 等人提出（[153](#bib.bib153)）。然而，XAI
    直到深度学习主导 AI 时才获得关注，2015 年 DARPA 启动的可解释 AI 计划是这种兴趣的初步迹象，该计划的主要目标是产生更可解释的模型，以增加对其理解和透明度，从而增强用户的信任。后来，欧盟（EU）（[42](#bib.bib42)）引入了有关“算法解释权”的立法，赋予公民对从个人数据获得的算法决策提出解释的权利。考虑到这一点，研究人员将努力转向创建可解释模型，而不仅仅关注准确性，从而导致
    XAI 的流行和兴趣呈指数增长，该主题的研究数量在近年来迅速增加。
- en: 'This section provides a general overview of the taxonomy of XAI methods, the
    description of seminal XAI methods, as well as the existing frameworks providing
    implementations of these methods (Table [4](#A1.T4 "Table 4 ‣ A.1\. Intepretability
    Frameworks ‣ Appendix A Appendix ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey") in appendix [A.1](#A1.SS1 "A.1\. Intepretability
    Frameworks ‣ Appendix A Appendix ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey")).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了 XAI 方法的分类概述、开创性 XAI 方法的描述，以及提供这些方法实现的现有框架（见附录 [A.1](#A1.SS1 "A.1\. 可解释性框架
    ‣ 附录 A 附录 ‣ 医疗图像分类中的可解释深度学习方法：综述") 中的表 [4](#A1.T4 "表 4 ‣ A.1\. 可解释性框架 ‣ 附录 A 附录
    ‣ 医疗图像分类中的可解释深度学习方法：综述")）。
- en: 3.1\. XAI Methods Taxonomy
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. XAI 方法分类
- en: 'Based on the reviewed literature, XAI methods can be categorized according
    to three criteria: (i) Model-Agnostic versus Model-Specific; (ii) Global Interpretability
    versus Local Interpretability; and (iii) Post-hoc versus Intrinsic. Figure [1](#S2.F1
    "Figure 1 ‣ 2.3\. Discussion ‣ 2\. Related Surveys ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey") illustrates the general taxonomy
    of the XAI methods, and each category is detailed in the following paragraphs.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据审查的文献，XAI 方法可以根据以下三种标准进行分类：（i）模型无关与模型特定；（ii）全局可解释性与局部可解释性；以及（iii）事后解释与内在解释。图
    [1](#S2.F1 "图 1 ‣ 2.3\. 讨论 ‣ 2\. 相关调查 ‣ 医疗图像分类中的可解释深度学习方法：综述") 说明了 XAI 方法的一般分类，每个类别将在以下段落中详细介绍。
- en: Model-Agnostic versus Model-Specific
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型无关与模型特定
- en: A distinguishing factor between interpretability approaches is their comprehensiveness
    regarding the models they can be applied to. Model-agnostic methods can be used
    to explain arbitrary models, not being limited to a specific model architecture.
    Conversely, model-specific methods are restricted to a specific model architecture,
    meaning that these methods require access to the model’s internal information.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性方法的一个区分因素是它们对模型的适用范围。模型无关方法可以用于解释任意模型，不限于特定的模型架构。相反，模型特定方法受限于特定的模型架构，这意味着这些方法需要访问模型的内部信息。
- en: Global Interpretability versus Local Interpretability
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 全局可解释性与局部可解释性
- en: The type of explanations provided by XAI methods can be broadly divided into
    global and local whether the explanations provide insights about the model functioning
    for the general data distribution or for a specific data sample, respectively.
    Global interpretability methods explain which patterns in the data, i.e., class
    features, contributed the most to the model’s prediction. These explanations can
    reveal critical reasoning about what the model is learning. On the other hand,
    local interpretability methods seek to explain why a model performs a specific
    prediction for a single input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: XAI方法提供的解释类型大致分为全局和局部，全局解释提供关于模型在总体数据分布中的功能的见解，而局部解释则针对特定数据样本提供见解。全局可解释性方法解释了数据中的哪些模式，即类别特征，对模型预测的贡献最大。这些解释可以揭示模型学习的关键推理。另一方面，局部可解释性方法旨在解释模型为何对单个输入进行特定预测。
- en: Post-hoc versus Intrinsic
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后分析与内在分析
- en: This criterion distinguishes the methods with respect to whether the explanation
    mechanism lies in the internal architecture of the model (intrinsic) or if it
    is applied after the learning/development of the model (post-hoc). Post-hoc methods
    usually operate by perturbing parts of the data so that they can understand the
    contribution of different features in the model prediction, or by analytically
    determining the contribution of different features to the model prediction. On
    the other hand, intrinsic models, also known as in-model approaches or inherently
    interpretable models, are self-explainable since they are designed to produce
    human-understandable representations from the internal model features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一标准区分了解释机制是存在于模型的内部架构中（内在的）还是在模型的学习/开发之后应用的（事后的）。事后方法通常通过扰动数据的部分来了解不同特征对模型预测的贡献，或通过分析确定不同特征对模型预测的贡献。另一方面，内在模型，也称为模型内部方法或固有可解释模型，是自解释的，因为它们被设计成从模型的内部特征中生成人类可理解的表示。
- en: Explanation Modality
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解释模式
- en: 'Explanation modality refers to the type of explanation provided by each interpretability
    method. Among the reviewed methods, the explanation can be provided in the form
    of saliency maps (Explanation by Feature Attribution), semantic descriptions (Explanation
    by Text), similar examples (Explanation by Examples), or using high-level concepts
    (Explanation by Concepts). In Chapter [5](#S5 "5\. XAI Methods in Medical Diagnosis
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey"),
    we used this categorization to discuss the reviewed methods.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '解释模式指的是每种可解释性方法提供的解释类型。在审查的方法中，解释可以以显著性图（特征归因解释）、语义描述（文本解释）、类似例子（示例解释）或使用高级概念（概念解释）的形式提供。在第[5](#S5
    "5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey")章中，我们使用了这种分类方法来讨论所审查的方法。'
- en: 3.2\. Classical XAI Methods
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 经典XAI方法
- en: The first attempts to explain deep learning models relied on the post-hoc analysis
    of the models. In spite of the criticism that post-hoc approaches have been recently
    subjected to ([118](#bib.bib118)), they are still being used in many domains of
    medical imaging, and their understanding is important to explain the advances
    on the topic of interpretable deep learning. As such, the following sections briefly
    describe the most popular XAI algorithms according to the two major categories
    of post-hoc analysis.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 解释深度学习模型的初步尝试依赖于对模型的事后分析。尽管事后方法最近遭受了批评（[118](#bib.bib118)），但它们在许多医学成像领域仍被使用，并且对解释可解释深度学习的进展很重要。因此，以下部分简要描述了根据事后分析的两个主要类别最流行的XAI算法。
- en: 3.2.1\. Perturbation-based methods
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 基于扰动的方法
- en: The rationale behind perturbation-based methods is to perceive how a perturbation
    in the input affects the model’s prediction. Examples of perturbation-based methods
    are LIME ([113](#bib.bib113)) and SHAP ([88](#bib.bib88)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扰动的方法的基本原理是感知输入中的扰动如何影响模型的预测。基于扰动的方法的例子包括LIME（[113](#bib.bib113)）和SHAP（[88](#bib.bib88)）。
- en: LIME
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LIME
- en: 'LIME ([113](#bib.bib113)) stands for Local Interpretable Model-agnostic Explanations.
    As the name suggests, it can explain any black-box model, and according to the
    XAI taxonomy is a post-hoc, model-agnostic method providing local explanations.
    The intuition behind LIME is to approximate the complex model (black-box model)
    locally with an interpretable model, usually denoted as local surrogate model.
    Thus, an individual instance is explained locally using a simple interpretable
    model around the prediction, such as linear models or decision trees. Figure [2](#S3.F2
    "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based methods ‣ 3.2\. Classical XAI Methods
    ‣ 3\. Background in XAI ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey")a) provides an intuitive illustration of the overall functioning of
    LIME.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'LIME（[113](#bib.bib113)）代表局部可解释模型无关解释。顾名思义，它可以解释任何黑箱模型，根据XAI分类法，它是一种事后模型无关的方法，提供局部解释。LIME的直觉是通过一个可解释模型局部近似复杂模型（黑箱模型），通常称为局部替代模型。因此，使用一个简单的可解释模型在预测周围局部解释个体实例，例如线性模型或决策树。图[2](#S3.F2
    "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based methods ‣ 3.2\. Classical XAI Methods
    ‣ 3\. Background in XAI ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey")a) 提供了LIME整体功能的直观说明。'
- en: In order to approximate the model prediction locally, a new dataset consisting
    of perturbed samples conditioned on their proximity to the instance being explained
    is used to fit the interpretable model. The labels for those perturbed samples
    are obtained through the complex model. In the case of tabular data, the perturbed
    instances are sampled around the instance being explained, by randomly changing
    the feature values in order to obtain samples both in the vicinity and far away
    from the instance being explained. Analogously, when LIME is applied to the image
    classification problem, the image being explained is first segmented into superpixels,
    which are groups of pixels in the image sharing common characteristics, such as
    colour and intensity. Then, the perturbed versions of the original data are obtained
    by randomly masking out a subset of superpixels, resulting in an image with occluded
    patches. The new dataset used to fit the interpretable model consists of perturbed
    versions of the image being explained, and the superpixels with the highest positive
    coefficients in the interpretable model suggest they largely contributed to the
    prediction. Thus, they will be selected as part of the interpretable representation
    that is simply a binary vector indicating the presence or absence of those superpixels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在局部范围内近似模型预测，使用了一个新的数据集，该数据集由受扰动的样本组成，这些样本依据其与正在解释的实例的接近程度进行条件化。通过复杂模型获得这些扰动样本的标签。在表格数据的情况下，扰动实例在正在解释的实例周围进行采样，通过随机改变特征值来获得既接近又远离正在解释的实例的样本。类似地，当LIME应用于图像分类问题时，首先将正在解释的图像分割成超像素，即图像中具有共同特征（如颜色和强度）的像素组。然后，通过随机遮挡一部分超像素来获得原始数据的扰动版本，从而得到一个有遮挡区域的图像。用于拟合可解释模型的新数据集由正在解释的图像的扰动版本组成，而在可解释模型中具有最高正系数的超像素表明它们对预测贡献很大。因此，它们将被选为可解释表示的一部分，该表示仅为一个二进制向量，指示这些超像素的存在或缺失。
- en: SHAP
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SHAP
- en: 'SHAP ([88](#bib.bib88)) was inspired on the Shapley values from the cooperative
    game theory ([130](#bib.bib130)) and operates by determining the average contribution
    of a feature value to the model prediction using all combinations of the features
    powerset. As an example, given the task of predicting the risk of stroke based
    on age, gender and Body Mass Index (BMI), the SHAP explanations for a particular
    prediction are given in terms of the contribution of each feature. This contribution
    is determined from the change observed in model prediction when using the $2^{n}$
    combinations from the features powerset, where the missing features are replaced
    by random values. Figure [2](#S3.F2 "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based
    methods ‣ 3.2\. Classical XAI Methods ‣ 3\. Background in XAI ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey")b) illustrates the
    above-described example. Similarly to LIME, SHAP is a local model-agnostic interpretation
    method that can be applied to both tabular and image data. In the case of tabular
    data, the explanation is given in the form of importance values to each feature.
    In the case of image data, it follows a similar procedure to the LIME, by calculating
    the Shapley values for all possible combinations between superpixels. Several
    variations of SHAP method were proposed to approximate Shapley values in a more
    efficient way, namely KernelSHAP, DeepSHAP and TreeSHAP ([87](#bib.bib87)).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'SHAP ([88](#bib.bib88))受合作博弈论中的沙普利值启发([130](#bib.bib130))，通过确定特征值对模型预测的平均贡献来操作，使用特征幂集的所有组合。例如，给定基于年龄、性别和身体质量指数(BMI)来预测中风风险的任务，对特定预测的SHAP解释以每个特征的贡献来说明。这一贡献来自于使用$2^{n}$的特征幂集组合观察到的模型预测变化确定，其中缺失的特征被随机值替换。图[2](#S3.F2
    "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based methods ‣ 3.2\. Classical XAI Methods
    ‣ 3\. Background in XAI ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey")b)说明了上述例子。与LIME类似，SHAP是一种局部的模型无关解释方法，可应用于表格数据和图像数据。对于表格数据，解释以每个特征的重要性值的形式给出。对于图像数据，它遵循与LIME类似的流程，通过计算超像素之间所有可能的组合的沙普利值。提出了SHAP方法的几种变体，以更高效地逼近沙普利值，包括KernelSHAP、DeepSHAP和TreeSHAP ([87](#bib.bib87))。'
- en: <svg   height="213.38" overflow="visible" version="1.1" width="614.13"><g transform="translate(0,213.38)
    matrix(1 0 0 -1 0 0) translate(160.02,0) translate(0,121.77)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -155.41 -84)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="168" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/a7b63c723119b71caca81b9091e4f485.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 150.5 -87)" fill="#000000" stroke="#000000"><foreignobject
    width="299" height="174" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/7e1cf444120dc676ca47dc03989c702d.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -113.7)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -113.7)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="213.38" overflow="visible" version="1.1" width="614.13"><g transform="translate(0,213.38)
    matrix(1 0 0 -1 0 0) translate(160.02,0) translate(0,121.77)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -155.41 -84)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="168" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![参见说明](img/a7b63c723119b71caca81b9091e4f485.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 150.5 -87)" fill="#000000" stroke="#000000"><foreignobject
    width="299" height="174" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![参见说明](img/7e1cf444120dc676ca47dc03989c702d.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 9.16 -113.7)" fill="#000000" stroke="#000000"><foreignobject
    width="17.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -113.7)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
- en: Figure 2\. (a) LIME. The black curved line represents a decision boundary learned
    by the complex black-box model. LIME explains a new test sample (dashed circle),
    by fitting an interpretable model (represented by a green dashed line) to the
    variations of the test sample (orange circles), which are generated by randomly
    perturbing the test sample features. The fitted model allows to perceive the contribution
    of each feature for classifying that specific test sample. (b) SHAP. The predicted
    risk of stroke of a classification model for a 13 years-old female person, a body
    mass index of 29.1 and an average glucose level of 76.55 was “No Stroke”. As evidenced
    by the bar plot, which provides the Shapley values for each feature, “age” was
    the feature with a higher impact on the prediction of “No Stroke”, followed by
    the BMI and average glucose level features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. (a) LIME。黑色曲线代表由复杂黑箱模型学习的决策边界。LIME通过将一个可解释的模型（由绿色虚线表示）拟合到测试样本的变化（橙色圆圈），从而解释新的测试样本（虚线圆圈）。这些变化是通过随机扰动测试样本特征生成的。拟合的模型使我们能够感知每个特征对分类特定测试样本的贡献。(b)
    SHAP。对于一个13岁女性，体质指数为29.1，平均葡萄糖水平为76.55的分类模型预测的中风风险是“无中风”。通过条形图可以看到，每个特征的Shapley值，其中“年龄”对“无中风”预测的影响最大，其次是BMI和平均葡萄糖水平特征。
- en: 3.2.2\. Saliency
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 显著性
- en: Saliency maps are one of the most popular techniques to explain the decisions
    of a model. Saliency methods produce visual explanation maps representing the
    importance of image pixels to the model classification.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图是解释模型决策的最流行技术之一。显著性方法生成视觉解释图，表示图像像素对模型分类的重要性。
- en: Class Activation Mapping (CAM) ([177](#bib.bib177)) is a seminal saliency method
    which allowed the generation of a saliency map using a linear combination of the
    output of the last Global Average Pooling (GAP) layer of the network. Despite
    being a seminal contribution, CAM can only be applied to architectures following
    a specific pattern. To address this problem, Selvaraju et al. ([128](#bib.bib128))
    proposed the Gradient-weighted Class Activation Mapping (Grad-CAM) ([128](#bib.bib128))
    that uses the gradient information of the target class with respect to the input
    image to produce a class-discriminative localization map that acts as a visual
    explanation for the model’s prediction. Grad-CAM is, therefore, a generalization
    of CAM. Alternatively, SmoothGrad ([139](#bib.bib139)) is another gradient-based
    explanation method whose core idea is to attenuate the noise of the explanations
    provided by gradient-based techniques. The rationale behind SmoothGrad is to sample
    multiple images from the input image by adding noise to it. Then, the sensitivity
    maps are computed for each sampled image. The final map is the average of the
    sensitivity maps.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 类激活映射（CAM）([177](#bib.bib177)) 是一种开创性的显著性方法，它通过使用网络最后一层全局平均池化（GAP）层输出的线性组合来生成显著性图。尽管CAM是一个开创性的贡献，但它仅适用于遵循特定模式的架构。为了解决这个问题，Selvaraju等人([128](#bib.bib128))
    提出了梯度加权类激活映射（Grad-CAM）([128](#bib.bib128))，它利用目标类相对于输入图像的梯度信息来生成一个类别区分的定位图，从而作为模型预测的视觉解释。因此，Grad-CAM是CAM的一个推广。另一方面，SmoothGrad([139](#bib.bib139))
    是另一种基于梯度的解释方法，其核心思想是减弱梯度基方法提供的解释噪声。SmoothGrad的原理是通过向输入图像添加噪声来采样多个图像。然后，为每个采样图像计算敏感度图。最终的图是敏感度图的平均值。
- en: The Integrated Gradients (IG) ([144](#bib.bib144)) is an attribution method
    that relies on generating a set of images between the baseline and the original
    image using linear interpolation. These interpolated images are minor changes
    in the feature space between the baseline and input image and consistently increase
    with each interpolated image’s intensity. Calculating the gradients per feature
    (pixels) makes it possible to measure the correlation between changes to a feature
    and changes in the model’s predictions. The pixels with a high score are the ones
    that contributed the most to the prediction. The Layer-wise Relevance Propagation
    (LRP) ([10](#bib.bib10)) is an alternative solution to the use of gradients, where
    the decision function is decomposed into the relevance score of each neuron in
    the network. The output is propagated backwards through the model to determine
    the relevance score of the input, allowing to produce an importance heatmap of
    image pixels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Integrated Gradients (IG) ([144](#bib.bib144)) 是一种归因方法，它通过线性插值生成一组基线和原始图像之间的图像。这些插值图像在特征空间中产生细微变化，并且每个插值图像的强度一致增加。通过计算每个特征（像素）的梯度，可以测量特征变化与模型预测变化之间的相关性。得分较高的像素是对预测贡献最大的像素。Layer-wise
    Relevance Propagation (LRP) ([10](#bib.bib10)) 是一种替代梯度的解决方案，它将决策函数分解为网络中每个神经元的相关性分数。输出通过模型向后传播，以确定输入的相关性分数，从而生成图像像素的重要性热图。
- en: The main advantage of saliency methods is the reduced computational cost when
    compared to perturbation-based methods. However, different studies argue that
    the explanations provided by gradient-based methods can be ambiguous and unreliable,
    as well as sensitive to adversarial perturbations ([3](#bib.bib3); [40](#bib.bib40)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性方法的主要优势在于与基于扰动的方法相比，计算成本降低。然而，不同的研究认为，梯度方法提供的解释可能模糊和不可靠，并且对对抗性扰动敏感 ([3](#bib.bib3);
    [40](#bib.bib40))。
- en: 4\. Datasets
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 数据集
- en: 'Among the reviewed literature, a set of 25 publicly available medical imaging
    datasets were considered based on the reviewed papers to provide a thorough overview
    of the existing medical imaging databases. Table [3](#S7.T3 "Table 3 ‣ 7\. Performance
    Comparison ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") presents the main characteristics of the selected datasets, grouped
    by image type.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查的文献中，基于审查的论文，考虑了25个公开的医学影像数据集，以提供对现有医学影像数据库的全面概述。表[3](#S7.T3 "表 3 ‣ 7\. 性能比较
    ‣ 医学图像分类中的可解释深度学习方法：综述")展示了所选数据集的主要特征，按图像类型分组。
- en: Table 2\. Medical Imaging Datasets. The datasets marked with a “*” have reports
    written in Spanish. The datasets marked with a “**” have reports written in Portuguese.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 医学影像数据集。标有“*”的数据集报告以西班牙语书写。标有“**”的数据集报告以葡萄牙语书写。
- en: '| Dataset | Image Type | Year | No. Images | Notes |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 图像类型 | 年份 | 图像数量 | 备注 |'
- en: '| IU Chest X-Ray ([32](#bib.bib32)) | Chest X-ray | 2016 | 7,470 | Includes
    reports |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| IU Chest X-Ray ([32](#bib.bib32)) | 胸部X光 | 2016 | 7,470 | 包含报告 |'
- en: '| Chest X-Ray14 ([159](#bib.bib159)) | Chest X-ray | 2017 | 112,120 | Multiple
    labels |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Chest X-Ray14 ([159](#bib.bib159)) | 胸部X光 | 2017 | 112,120 | 多标签 |'
- en: '| CheXpert ([54](#bib.bib54)) | Chest X-ray | 2019 | 224,316 | Multiple labels
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CheXpert ([54](#bib.bib54)) | 胸部X光 | 2019 | 224,316 | 多标签 |'
- en: '| MIMIC-CXR ([56](#bib.bib56)) | Chest X-ray | 2019 | 377,110 | Includes reports
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-CXR ([56](#bib.bib56)) | 胸部X光 | 2019 | 377,110 | 包含报告 |'
- en: '| PadChest^∗ ([19](#bib.bib19)) | Chest X-ray | 2020 | 160,868 | Includes reports
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| PadChest^∗ ([19](#bib.bib19)) | 胸部X光 | 2020 | 160,868 | 包含报告 |'
- en: '| VinDr-CXR ([102](#bib.bib102)) | Chest X-ray | 2020 | 18,000 | Multiple labels
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| VinDr-CXR ([102](#bib.bib102)) | 胸部X光 | 2020 | 18,000 | 多标签 |'
- en: '| COVIDx ([156](#bib.bib156)) | Chest X-ray | 2020 | 13,975 | Multiclass |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| COVIDx ([156](#bib.bib156)) | 胸部X光 | 2020 | 13,975 | 多类别 |'
- en: '| Inbreast^(∗∗) ([100](#bib.bib100)) | Mammography X-ray | 2012 | 410 | Includes
    reports |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Inbreast^(∗∗) ([100](#bib.bib100)) | 乳腺X光 | 2012 | 410 | 包含报告 |'
- en: '| CBIS-DDSM ([73](#bib.bib73)) | Mammography X-ray | 2017 | 2,620 | Multiclass
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CBIS-DDSM ([73](#bib.bib73)) | 乳腺X光 | 2017 | 2,620 | 多类别 |'
- en: '| VinDr-SpineXR ([103](#bib.bib103)) | Spinal X-ray | 2021 | 10,469 | Multiple
    labels/BBox |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| VinDr-SpineXR ([103](#bib.bib103)) | 脊柱X光 | 2021 | 10,469 | 多标签/边界框 |'
- en: '| Knee Osteoarthirtis ([101](#bib.bib101)) | Knee X-ray | 2006 | 8,894 | Multiclass
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 膝关节骨关节炎 ([101](#bib.bib101)) | 膝关节X光 | 2006 | 8,894 | 多类别 |'
- en: '| PH² ([95](#bib.bib95)) | Dermatoscopic Images | 2013 | 200 | Multiple labels/Lesion
    segment. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| PH² ([95](#bib.bib95)) | 皮肤镜图像 | 2013 | 200 | 多标签/病变分割 |'
- en: '| HAM10000 ([149](#bib.bib149)) | Dermatoscopic Images | 2018 | 10,015 | Multiclass/Lesion
    segment. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| HAM10000 ([149](#bib.bib149)) | 皮肤镜图像 | 2018 | 10,015 | 多类别/病变分割 |'
- en: '| SKINL2 ([31](#bib.bib31)) | Dermatoscopic Images | 2019 | 376 | Multiclass
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SKINL2 ([31](#bib.bib31)) | 皮肤镜图像 | 2019 | 376 | 多类别 |'
- en: '| Derm7pt ([57](#bib.bib57)) | Dermatoscopic Images | 2019 | 2,000 | Multiclass
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Derm7pt ([57](#bib.bib57)) | 皮肤镜图像 | 2019 | 2,000 | 多类别 |'
- en: '| ISIC 2020 ([117](#bib.bib117)) | Dermatoscopic Images | 2020 | 33,126 | Multiple
    labels |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ISIC 2020 ([117](#bib.bib117)) | 皮肤镜图像 | 2020 | 33,126 | 多标签 |'
- en: '| SkinCon ([29](#bib.bib29)) | Dermatoscopic Images | 2022 | 3,230 | Concept
    annotations |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SkinCon ([29](#bib.bib29)) | 皮肤镜图像 | 2022 | 3,230 | 概念注释 |'
- en: '| BreakHis ([141](#bib.bib141)) | Microscopy Images | 2015 | 9,109 | Multiclass
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| BreakHis ([141](#bib.bib141)) | 显微镜图像 | 2015 | 9,109 | 多类别 |'
- en: '| Camelyon17 ([79](#bib.bib79)) | Microscopy Images | 2018 | 1,000 | Multiclass
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Camelyon17 ([79](#bib.bib79)) | 显微镜图像 | 2018 | 1,000 | 多类别 |'
- en: '| Databiox ([17](#bib.bib17)) | Microscopy Images | 2020 | 922 | Multiclass
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Databiox ([17](#bib.bib17)) | 显微镜图像 | 2020 | 922 | 多类别 |'
- en: '| BCIDR^((priv)) ([176](#bib.bib176)) | Microscopy Images | 2017 | 5,000 |
    Includes reports |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| BCIDR^((priv)) ([176](#bib.bib176)) | 显微镜图像 | 2017 | 5,000 | 包含报告 |'
- en: '| APTOS ([140](#bib.bib140)) | Retina Images | 2019 | 5,590 | Multiclass |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| APTOS ([140](#bib.bib140)) | 视网膜图像 | 2019 | 5,590 | 多类别 |'
- en: '| LIDC-IDRI ([8](#bib.bib8)) | CT scans | 2011 | 1,018 | Includes annotations
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LIDC-IDRI ([8](#bib.bib8)) | CT 扫描 | 2011 | 1,018 | 包含注释 |'
- en: '| COV-CTR ([75](#bib.bib75)) | CT scans | 2023 | 728 | Includes reports |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| COV-CTR ([75](#bib.bib75)) | CT 扫描 | 2023 | 728 | 包含报告 |'
- en: '| PEIR ([55](#bib.bib55)) | Photographs | 2017 | 33,648 | Includes reports
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| PEIR ([55](#bib.bib55)) | 照片 | 2017 | 33,648 | 包含报告 |'
- en: '| ROCO ([107](#bib.bib107)) | Multimodal | 2018 | 81,825 | Includes reports/UMLS
    Concepts |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ROCO ([107](#bib.bib107)) | 多模态 | 2018 | 81,825 | 包含报告/UMLS 概念 |'
- en: 4.1\. Chest X-ray
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 胸部 X 光
- en: With respect to X-ray imaging modality, IU Chest X-ray ([32](#bib.bib32)), Chest
    X-ray 14 ([159](#bib.bib159)), CheXpert ([54](#bib.bib54)), MIMIC-CXR ([56](#bib.bib56)),
    PadChest ([19](#bib.bib19)), COVIDx ([156](#bib.bib156)), and VinDr-CXR ([102](#bib.bib102))
    datasets pertain to the chest anatomical region. Moreover, IU Chest X-Ray, MIMIC-CXR,
    and PadChest datasets include free-text radiology reports. The reports are written
    in English, except for the PadChest dataset, whose reports are written in Spanish.
    MIMIC-CXR and CheXpert are the largest databases, composed by $377,110$ and $224,316$
    radiographs, respectively. CheXpert does not provide the raw free-text reports,
    but it provides an automated rule-based labeller for extracting keywords from
    medical reports conforming to the Fleischner Society’s recommended glossary ([49](#bib.bib49)).
    This tool was also used in the MIMIC-CXR dataset to extract the labels from the
    radiology reports. UI Chest X-Ray ([32](#bib.bib32)) comprises $7,470$ chest X-ray
    images jointly with $3,955$ free-text reports, being the most used dataset in
    the literature. The VineDr-CXR consists of $18,000$ chest X-ray images annotated
    with $22$ findings (local labels) and $6$ diagnosis (global labels). The local
    labels are inferred from the “findings” section in the radiology reports. In contrast,
    the global labels come from “impressions” section and indicate suspected diseases,
    such as “Pneumonia”, “Tuberculosis”, “Lung Tumor”, “Chronic obstructive pulmonary
    disease”, “Other diseases”, and “No Findings”. Additionally, each “finding” is
    annotated on the X-ray image with a bounding box. Finally, COVIDx ([156](#bib.bib156))
    dataset comprises 13,975 chest X-ray images across $13,870$ patient cases, distributed
    by “Normal”, “Pneumonia” and “COVID-19” cases.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 X 射线成像模式，IU 胸部 X 射线 ([32](#bib.bib32))、Chest X-ray 14 ([159](#bib.bib159))、CheXpert
    ([54](#bib.bib54))、MIMIC-CXR ([56](#bib.bib56))、PadChest ([19](#bib.bib19))、COVIDx
    ([156](#bib.bib156)) 和 VinDr-CXR ([102](#bib.bib102)) 数据集涉及胸部解剖区域。此外，IU 胸部 X 射线、MIMIC-CXR
    和 PadChest 数据集包含自由文本放射学报告。报告用英语书写，PadChest 数据集的报告用西班牙语书写。MIMIC-CXR 和 CheXpert
    是最大的数据库，分别包含 $377,110$ 和 $224,316$ 张 X 射线影像。CheXpert 不提供原始自由文本报告，但提供了一个基于规则的自动标注器，用于从符合
    Fleischner 协会推荐词汇表 ([49](#bib.bib49)) 的医学报告中提取关键词。这个工具也在 MIMIC-CXR 数据集中用于从放射学报告中提取标签。UI
    胸部 X 射线 ([32](#bib.bib32)) 包含 $7,470$ 张胸部 X 射线影像和 $3,955$ 张自由文本报告，是文献中使用最多的数据集。VineDr-CXR
    包含 $18,000$ 张胸部 X 射线影像，标注了 $22$ 个发现（局部标签）和 $6$ 个诊断（全局标签）。局部标签是从放射学报告的“发现”部分推断出来的。相反，全局标签来自“印象”部分，表示怀疑的疾病，如“肺炎”、“结核病”、“肺肿瘤”、“慢性阻塞性肺疾病”、“其他疾病”和“无发现”。此外，每个“发现”都在
    X 射线影像上用边界框标注。最后，COVIDx ([156](#bib.bib156)) 数据集包含 13,975 张胸部 X 射线影像，覆盖 $13,870$
    个患者病例，按“正常”、“肺炎”和“COVID-19”病例分布。
- en: It is worth noting that the majority of the chest X-ray dataset’s labels were
    extracted using an automatic rule-based labeler, such as the CheXpert NLP tool ([54](#bib.bib54)).
    However, relying on these automated tools can pose several issues concerning the
    quality of the labels. Consequently, the authors of the VinDr-CXR ([102](#bib.bib102))
    dataset provided only radiologist-level annotations in both training and test
    sets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大多数胸部 X 射线数据集的标签是使用自动规则标注器（如 CheXpert NLP 工具 ([54](#bib.bib54))）提取的。然而，依赖这些自动工具可能会对标签的质量造成一些问题。因此，VinDr-CXR
    ([102](#bib.bib102)) 数据集的作者仅提供了训练集和测试集中的放射科医师级注释。
- en: 4.2\. Other X-ray modalities
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 其他 X 射线模式
- en: In the same segment of the X-ray imaging, and similarly to the VinDr-CXR ([102](#bib.bib102))
    dataset approach, VinDr-SpinalXR ([103](#bib.bib103)) is a recent dataset comprising
    $10,469$ spine X-ray images manually annotated by an experienced radiologist with
    bounding-boxes around abnormal findings in $13$ categories. The Knee Osteoarthritis ([101](#bib.bib101))
    dataset contains 8,894 knee X-rays for both knee joint detection and knee Kellgren
    and Lawrence grading ([59](#bib.bib59)), whose value ranges from 0 to 4, according
    to the level of severity. Regarding the mammography datasets, Inbreast ([100](#bib.bib100))
    consists of $410$ mammography X-rays along with 115 radiology reports written
    in Portuguese. Similarly, CBIS-DDSM ([73](#bib.bib73)) dataset is composed of
    $2,620$ mammography scans distributed into “Normal”, “Benign”, and “Malignant”
    cases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一段 X 射线影像中，与 VinDr-CXR ([102](#bib.bib102)) 数据集的方法类似，VinDr-SpinalXR ([103](#bib.bib103))
    是一个最近的数据集，包括 $10,469$ 张脊柱 X 射线图像，这些图像由经验丰富的放射科医生手动标注，围绕 $13$ 类异常发现绘制了边界框。膝关节骨关节炎 ([101](#bib.bib101))
    数据集包含 8,894 张膝关节 X 射线图像，用于膝关节检测以及膝关节 Kellgren 和 Lawrence 分级 ([59](#bib.bib59))，分级范围从
    0 到 4，根据严重程度的不同。关于乳腺 X 射线数据集，Inbreast ([100](#bib.bib100)) 包含 $410$ 张乳腺 X 射线图像以及
    115 份用葡萄牙语撰写的放射学报告。类似地，CBIS-DDSM ([73](#bib.bib73)) 数据集由 $2,620$ 张乳腺扫描图像组成，分为“正常”、“良性”和“恶性”病例。
- en: 4.3\. Dermatoscopy
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 皮肤镜检查
- en: In the scope of dermatology, the ISIC 2020 ([117](#bib.bib117)) dataset consists
    of $33,126$ skin lesion images of different categorizations (malignant, melanoma,
    keratosis, etc), and it is part of the International Skin Imaging Collaboration,
    which promotes annual challenges to enhance the diagnosis of malignant skin lesions
    in dermoscopy images. The HAM10000 ([149](#bib.bib149)) dataset consists of $10,015$
    dermatoscopic images distributed into diagnostic categories in the realm of pigmented
    lesions. The PH² ([95](#bib.bib95)) dataset comprises $200$ dermoscopic images
    of melanocytic lesions, including common nevi, atypical nevi, and melanoma. Moreover,
    the PH² database includes medical segmentation of the lesions, clinical and histological
    diagnosis, and the assessment of several dermoscopic criteria. The Derm7pt ([57](#bib.bib57))
    is another dermoscopic image dataset with over $2,000$ images annotated following
    the 7-point melanoma checklist criteria. Finally, the SKINL2 ([31](#bib.bib31))
    database consists of a total of $376$ light fields of skin lesions manually annotated
    under eight categories, based on the type of skin lesion and using the correspondent
    International Classification of Diseases (ICD) code. SkinCon ([29](#bib.bib29))
    includes 3,230 images from the Fitzpatrick 17k skin disease dataset ([45](#bib.bib45))
    annotated with 48 clinical concepts.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在皮肤科范围内，ISIC 2020 ([117](#bib.bib117)) 数据集包括 $33,126$ 张不同分类（恶性、黑色素瘤、角化症等）的皮肤病变图像，是国际皮肤影像合作组织的一部分，该组织每年举办挑战赛，以提升皮肤镜图像中恶性皮肤病变的诊断水平。HAM10000 ([149](#bib.bib149))
    数据集包括 $10,015$ 张皮肤镜图像，按色素病变领域的诊断类别进行分布。PH² ([95](#bib.bib95)) 数据集包括 $200$ 张黑色素病变的皮肤镜图像，包括普通痣、非典型痣和黑色素瘤。此外，PH²
    数据库包含病变的医学分割、临床和组织学诊断，以及对若干皮肤镜标准的评估。Derm7pt ([57](#bib.bib57)) 是另一个皮肤镜图像数据集，包含超过
    $2,000$ 张图像，按照 7 点黑色素瘤检查表标准进行了标注。最后，SKINL2 ([31](#bib.bib31)) 数据库包括 $376$ 张皮肤病变的光场图像，这些图像根据皮肤病变类型和对应的国际疾病分类（ICD）代码，手动标注为八个类别。SkinCon ([29](#bib.bib29))
    包含来自 Fitzpatrick 17k 皮肤疾病数据集 ([45](#bib.bib45)) 的 3,230 张图像，标注了 48 个临床概念。
- en: 4.4\. Microscopy
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 显微镜检查
- en: With regard to datasets composed of microscopy images, the BreakHis ([141](#bib.bib141))
    dataset comprises $9,109$ microscopic images of breast tumour tissue distributed
    in various magnifying factors (40x, 100x, 200x, and 400x) and categorized into
    “benign” and “malignant” tumours. The Camelyon 17 ([79](#bib.bib79)) dataset consists
    of $1,000$ annotated whole-slide images (WSI) of lymph nodes. It is part of a
    challenge whose primary goal is the classification of breast cancer metastases
    in WSI of histological lymph node sections. Similarly, Databiox  ([17](#bib.bib17))
    dataset comprises $922$ histopathological microscopy images with four levels of
    magnification (4x, 10x, 20x, and 40x) for the task of Invasive Ductal Carcinoma
    (IDC) grading using three grades of IDC.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于由显微镜图像组成的数据集，BreakHis ([141](#bib.bib141)) 数据集包括 $9,109$ 张乳腺肿瘤组织的显微图像，这些图像分布在不同的放大倍数（40x、100x、200x
    和 400x）下，并被分类为“良性”和“恶性”肿瘤。Camelyon 17 ([79](#bib.bib79)) 数据集包含 $1,000$ 张带注释的淋巴结全切片图像（WSI）。这是一个挑战赛的一部分，其主要目标是对淋巴结组织学切片中的乳腺癌转移进行分类。同样，Databiox
    ([17](#bib.bib17)) 数据集包含 $922$ 张病理显微镜图像，具有四个放大级别（4x、10x、20x 和 40x），用于侵袭性导管癌（IDC）分级任务，IDC
    分为三个等级。
- en: 4.5\. Others
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 其他
- en: For blindness detection purposes, the APTOS ([140](#bib.bib140)) dataset provides
    $5,590$ images of the retina taken using fundus photography. A clinician annotated
    each image according to the severity of diabetic retinopathy on a scale of 0 (Diabetic
    Retinopathy) to 4 (Proliferative Diabetic Retinopathy). In the scope of COVID-19,
    COVID-19 CT Reports (COV-CTR) ([75](#bib.bib75)) is a dataset composed of 728
    CT scans paired with Chinese and English reports.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于盲点检测目的，APTOS ([140](#bib.bib140)) 数据集提供 $5,590$ 张使用眼底摄影拍摄的视网膜图像。临床医生根据糖尿病视网膜病变的严重程度对每张图像进行了标注，评分范围从
    0（糖尿病视网膜病变）到 4（增殖性糖尿病视网膜病变）。在 COVID-19 的背景下，COVID-19 CT Reports (COV-CTR) ([75](#bib.bib75))
    是一个数据集，由 728 张 CT 扫描图像及其对应的中文和英文报告组成。
- en: Regarding databases with multiple imaging modalities, the Pathology Education
    Informational Resource (PEIR) is a multidisciplinary public access image database
    intended for medical education. The PEIR database consists of $33,648$ images
    and the respective descriptions from different sub-classes of PEIR albums (PEIR
    Pathology, PEIR Radiology, and PEIR Slice). Similarly, the Radiology Objects in
    COntext (ROCO) ([107](#bib.bib107)) dataset is a multimodal image dataset consisting
    of $81,825$ radiology images divided into CT, MRI, X-ray Ultrasound, and Mammography.
    Each image is accompanied by the corresponding caption and the annotated Unified
    Medical Language System (UMLS) concepts.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关于多模态成像的数据库，病理教育信息资源（PEIR）是一个多学科的公共访问图像数据库，旨在用于医学教育。PEIR 数据库包含 $33,648$ 张图像及其相应的描述，来自不同子类的
    PEIR 相册（PEIR Pathology、PEIR Radiology 和 PEIR Slice）。类似地，放射学对象在上下文（ROCO） ([107](#bib.bib107))
    数据集是一个多模态图像数据集，包含 $81,825$ 张放射学图像，分为 CT、MRI、X 射线、超声和乳腺摄影。每张图像都附有相应的说明和标注的统一医学语言系统（UMLS）概念。
- en: 4.6\. Discussion
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 讨论
- en: 'According to Table [2](#S4.T2 "Table 2 ‣ 4\. Datasets ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), Chest X-ray is the most popular
    imaging modality regarding simultaneously the number of datasets and their scale.
    Large-scale datasets have the advantage that they can be used to train a model
    from scratch. However, the automatic labeling process of some examples could compromise
    the reliability of the model since some class labels are not human verified and
    can be mislabeled. On the other hand, as evidenced by the number of images per
    dataset, the scarcity of large-scale datasets concerning other imaging modalities
    is noticed, hindering the emergence of specialized task-specific models due to
    the limited training data. The majority of the medical imaging datasets have been
    gathered primarily for use in classification or segmentation tasks, where the
    labeling of class and mask annotations are sufficient for their intended purpose.
    Nevertheless, when these sets are utilized for interpretability purposes, their
    annotations may prove inadequate for a comprehensive qualitative evaluation. Nonetheless,
    some datasets, such as SkinCon and others, have been created explicitly with interpretability
    in mind and contain appropriate annotations to facilitate such evaluations. Accordingly,
    it is particularly important that researchers consider interpretability issues
    when building novel medical datasets, ensuring that appropriate annotations are
    included to further enable comprehensive and informative analyses of the data.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表[2](#S4.T2 "表2 ‣ 4. 数据集 ‣ 医学图像分类中的可解释深度学习方法：综述")，胸部X光是同时拥有最多数据集和最大规模的成像模态。大规模数据集的优点在于它们可以用来从头开始训练模型。然而，一些例子的自动标记过程可能会影响模型的可靠性，因为某些类别标签未经人工验证，可能会被错误标记。另一方面，从每个数据集的图像数量来看，与其他成像模态相比，大规模数据集的稀缺性是显而易见的，这阻碍了专业任务特定模型的出现，因为训练数据有限。大多数医疗成像数据集主要用于分类或分割任务，其中类别和掩码注释的标记对于其预期目的已经足够。然而，当这些数据集用于解释性目的时，它们的注释可能不适合进行全面的定性评估。尽管如此，一些数据集，如SkinCon等，已明确创建以便于解释，并包含适当的注释以促进此类评估。因此，研究人员在构建新的医疗数据集时特别重要的是要考虑解释性问题，确保包含适当的注释，以进一步促进对数据的全面和有信息量的分析。
- en: 5\. XAI Methods in Medical Diagnosis
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 医疗诊断中的XAI方法
- en: 'As aforementioned, deep learning models must confer transparency and trustworthiness
    when deployed in real-world scenarios. Moreover, this requirement becomes particularly
    relevant in clinical practice, where a less informed decision can put patient’s
    lives at risk. Among the reviewed literature, several methods have been proposed
    to confer interpretability in the deep learning methods applied to medical diagnosis.
    The following sections summarize and categorize the most relevant works in the
    scope of explainable models applied to medical diagnosis. Furthermore, we give
    particular attention to the inherently interpretable neural networks and their
    applicability to medical imaging. We categorize the methods according to the explanation
    modality: (i) Explanation by Feature Attribution, (ii) Explanation by Text, (iii)
    Explanation by Examples, (iv) Explanation by Concepts, and (v) Other Approaches;
    inspired by the taxonomy presented in ([97](#bib.bib97)). The list of the reviewed
    methods categorized by the interpretability method employed, image modality, and
    the dataset is provided in Table [5](#A1.T5 "Table 5 ‣ A.2\. Methods ‣ Appendix
    A Appendix ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") in appendix [A.2](#A1.SS2 "A.2\. Methods ‣ Appendix A Appendix ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey")¹¹1An interactive
    version of the Table 5 is provided in this [link](https://github.com/CristianoPatricio/Explainable-Deep-Learning-Methods-in-Medical-Image-Classification-A-Survey)..'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，深度学习模型在实际应用中必须具备透明性和可信度。此外，这一要求在临床实践中尤为重要，因为不够充分的信息可能会危及患者的生命。在回顾的文献中，已经提出了几种方法来赋予深度学习方法以可解释性，这些方法应用于医疗诊断。以下部分总结并分类了在医疗诊断中应用的可解释模型中最相关的工作。此外，我们特别关注于本质上可解释的神经网络及其在医学成像中的适用性。我们根据解释方式对方法进行分类：（i）特征归因解释，（ii）文本解释，（iii）示例解释，（iv）概念解释，以及（v）其他方法；灵感来自于 ([97](#bib.bib97))
    中提出的分类法。表 [5](#A1.T5 "Table 5 ‣ A.2\. Methods ‣ Appendix A Appendix ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey") 中提供了按照可解释性方法、图像模式和数据集分类的回顾方法列表，见附录
    [A.2](#A1.SS2 "A.2\. Methods ‣ Appendix A Appendix ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey")¹¹1。表 5 的互动版本可通过此 [链接](https://github.com/CristianoPatricio/Explainable-Deep-Learning-Methods-in-Medical-Image-Classification-A-Survey)
    查看。'
- en: <svg   height="193.22" overflow="visible" version="1.1" width="611.13"><g transform="translate(0,193.22)
    matrix(1 0 0 -1 0 0) translate(160.02,0) translate(0,100.11)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -155.41 -88.5)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="177" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/7190cb54a5e4ed377341cddf9967e7ff.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 153.5 -88)" fill="#000000" stroke="#000000"><foreignobject
    width="293" height="176" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/194d17549768fabd785cfcd596630683.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -92.04)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -92.04)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见标题](img/7190cb54a5e4ed377341cddf9967e7ff.png) ![参见标题](img/194d17549768fabd785cfcd596630683.png)
    (a) (b)'
- en: Figure 3\. (a) Perturbation-based methods. The input image is randomly perturbed
    by turning on and off certain pixels, resulting in an image with occluded parts
    (Perturbed Examples in the figure). Then, the perturbed image is fed to the classification
    model and the prediction confidence is exploited to determine the regions that
    contributed positively to the class prediction. Those regions will be considered
    to obtain the final explanation map (Explanation in the figure). (b) Saliency
    methods. The input image is fed to the classification model to obtain a class
    prediction. Then, the gradient is calculated for the score of the class concerning
    the feature maps of the last convolutional layer. After calculating the importance
    of the feature map regarding the predicted class, they are weighted with each
    of respective weight, followed by a ReLU operation to obtain the final saliency
    map.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. (a) 基于扰动的方法。通过开启和关闭某些像素来随机扰动输入图像，得到一个包含遮挡部分的图像（图中的扰动示例）。然后，将扰动后的图像输入分类模型，利用预测置信度来确定对类别预测做出积极贡献的区域。这些区域将被用来获得最终的解释图（图中的解释）。(b)
    显著性方法。将输入图像输入分类模型以获得类别预测。然后，计算类别相对于最后一层卷积特征图的分数梯度。在计算特征图对预测类别的重要性之后，将其与各自的权重加权，接着进行
    ReLU 操作，以获得最终的显著性图。
- en: 5.1\. Explanation by Feature Attribution
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 特征归因解释
- en: 'Feature attribution methods indicate how much each input feature contributed
    to the final model prediction. These methods can work on tabular data or image
    data by depicting feature importance scores in a bar chart or using a saliency
    map, respectively. Among the existing feature attribution approaches in the literature,
    we categorize them into (i) perturbation-based methods and (ii) saliency methods,
    emphasizing their application to medical image analysis. A schematic diagram illustrating
    the general pipeline of these methods is shown in Figure [3](#S5.F3 "Figure 3
    ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in
    Medical Image Classification: A Survey").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因方法指明了每个输入特征对最终模型预测的贡献。这些方法可以处理表格数据或图像数据，通过条形图描绘特征重要性分数，或使用显著性图。现有的特征归因方法中，我们将其分为
    (i) 基于扰动的方法和 (ii) 显著性方法，重点介绍它们在医学图像分析中的应用。图 [3](#S5.F3 "图 3 ‣ 5\. XAI 方法在医学诊断中的应用
    ‣ 医学图像分类中的可解释深度学习方法：综述") 展示了这些方法的一般流程图。
- en: 5.1.1\. Perturbation-based methods
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 基于扰动的方法
- en: 'As previously described in section [3.2.1](#S3.SS2.SSS1 "3.2.1\. Perturbation-based
    methods ‣ 3.2\. Classical XAI Methods ‣ 3\. Background in XAI ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey"), perturbation-based
    methods aim to perform a modification in the input data to perceive how it affects
    the model’s prediction. Popular examples of perturbation-based methods are LIME ([113](#bib.bib113))
    and SHAP ([88](#bib.bib88)). Regarding the use of perturbation methods to explain
    the prediction of medical diagnosis algorithms, Malhi et al. ([92](#bib.bib92))
    applied the LIME method to explain the decisions of a classifier to detect bleeding
    in gastral endoscopy images. Similarly, Punn et al. ([110](#bib.bib110)) applied
    the LIME technique to explain the predictions of various state-of-the-art deep
    learning models used to classify pulmonary diseases in chest X-ray images. Magesh
    et al. ([90](#bib.bib90)) also used LIME to justify the decisions of a Parkison’s
    disease classifier model. In the context of melanoma detection, Young et al. ([171](#bib.bib171))
    used Kernel SHAP ([88](#bib.bib88)) interpretability method to investigate its
    reliability in providing explanations for a melanoma classifier from dermoscopy
    images. They concluded that the interpretability strategy highlighted irrelevant
    features to the final prediction. The authors also conducted sanity checks on
    the interpretability methods, which confirmed that these methods often produce
    different explanations for models with similar performance. This can be explained
    by the fact that the model can learn some spurious correlations, causing interpretability
    methods to give exaggerated importance to those spurious regions highlighted on
    the produced saliency maps. In the same context, Wang et al. ([158](#bib.bib158))
    proposed a multimodal CNN for skin lesion diagnosis, which considered patient
    metadata and the skin lesion images. To analyze the contribution of each feature
    regarding the patient metadata, they adopted SHAP. Similarly, Eitel et al. ([35](#bib.bib35))
    relied on an occlusion-based interpretability method ([174](#bib.bib174)) and
    investigated its robustness for the task of Alzheimer’s disease classification.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[3.2.1](#S3.SS2.SSS1 "3.2.1\. 基于扰动的方法 ‣ 3.2\. 经典的解释性人工智能方法 ‣ 3\. 解释性人工智能背景
    ‣ 医学图像分类中的可解释深度学习方法：综述")部分所述，基于扰动的方法旨在对输入数据进行修改，以观察这如何影响模型的预测。常见的基于扰动的方法包括LIME ([113](#bib.bib113))和SHAP ([88](#bib.bib88))。关于使用扰动方法来解释医学诊断算法的预测，Malhi等人 ([92](#bib.bib92))应用了LIME方法来解释分类器在胃镜图像中检测出出血的决策。类似地，Punn等人 ([110](#bib.bib110))应用了LIME技术来解释用于分类胸部X光图像中肺部疾病的各种最先进深度学习模型的预测。Magesh等人 ([90](#bib.bib90))也使用了LIME来证明帕金森病分类器模型的决策。在黑色素瘤检测的背景下，Young等人 ([171](#bib.bib171))使用了Kernel
    SHAP ([88](#bib.bib88))解释性方法来调查其在提供黑色素瘤分类器解释中的可靠性。他们得出结论，解释性策略突出了与最终预测无关的特征。作者还对解释性方法进行了合理性检查，结果确认这些方法通常会对性能相似的模型给出不同的解释。这可以通过模型可能学习到一些虚假的关联来解释，导致解释性方法对在生成的显著性图上突出显示的虚假区域赋予了夸大的重要性。在相同的背景下，Wang等人 ([158](#bib.bib158))提出了一种用于皮肤病变诊断的多模态CNN，该模型考虑了患者的元数据和皮肤病变图像。为了分析每个特征在患者元数据中的贡献，他们采用了SHAP。类似地，Eitel等人 ([35](#bib.bib35))依赖于基于遮挡的解释性方法 ([174](#bib.bib174))并调查了其在阿尔茨海默病分类任务中的鲁棒性。
- en: RISE was proposed by Petsiuk et al. ([108](#bib.bib108)), consisting also of
    a post-hoc model-agnostic method for explaining the predictions of a black-box
    model through the generation of a saliency map indicating the important pixels
    to the model’s prediction. The core idea is to probe the model with a set of perturbed
    images from the input image via random masking to perceive the model’s response
    as important regions of the image are randomly occluded. The final saliency map
    is generated as a linear combination of the generated masks weighted with the
    output probabilities predicted by the model. For evaluation, the authors proposed
    two novel metrics, namely deletion and insertion, based on the removal and insertion
    of important pixels in the image to perceive the increase or the decrease of the
    model’s performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: RISE 由 Petsiuk 等人 ([108](#bib.bib108)) 提出，也是一种后验模型不可知的方法，用于通过生成显著性图来解释黑箱模型的预测，该显著性图指示模型预测的重要像素。其核心思想是通过随机掩膜对输入图像进行一组扰动图像的探测，以感知模型的响应，因为图像的重要区域会被随机遮挡。最终显著性图作为生成的掩膜的线性组合生成，这些掩膜按模型预测的输出概率加权。为了评估，作者提出了两个新颖的度量指标，即删除和插入，基于图像中重要像素的移除和插入，以感知模型性能的增加或减少。
- en: 5.1.2\. Saliency Methods
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 显著性方法
- en: Saliency methods allow to produce a saliency map in which each pixel is assigned
    a value that represents its relevance to the prediction of a certain class. Popular
    techniques are CAM ([177](#bib.bib177)), Grad-CAM ([128](#bib.bib128)), DeepLIFT ([131](#bib.bib131))
    and Integrated Gradients ([144](#bib.bib144)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Saliency 方法可以生成一个显著性图，其中每个像素被分配一个值，表示其对某一类别预测的相关性。流行的技术包括 CAM ([177](#bib.bib177))、Grad-CAM ([128](#bib.bib128))、DeepLIFT ([131](#bib.bib131))
    和 Integrated Gradients ([144](#bib.bib144))。
- en: Rajpurkar et al. ([111](#bib.bib111)) proposed the CheXNeXt model to detect
    pulmonary pathologies and used CAM to identify the locations on the chest radiograph
    that contributed most to the final model prediction.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Rajpurkar 等人 ([111](#bib.bib111)) 提出了 CheXNeXt 模型用于检测肺部病理，并使用 CAM 确定胸部 X 光片上对最终模型预测贡献最大的区域。
- en: In the context of detecting COVID-19 from chest radiographs, Lin et al. ([77](#bib.bib77))
    used Grad-CAM and Guided Grad-CAM to investigate the regions that the model considered
    more discriminative. The produced heatmaps showed that when no preprocessing is
    used, the CNN tends to concentrate on non-lung areas (e.g., spine, heart, background),
    deemed irrelevant for the classification decision. However, when a masking process
    is used to highlight only the lung’s area, the produced heatmaps highlight only
    the relevant regions since the CNN attention is limited to the critical area for
    detecting pulmonary diseases (lung’s area). Following the same procedures, Lopatina
    et al. ([84](#bib.bib84)) used DeepLIFT attribution algorithm to investigate the
    decisions of a multiple sclerosis classification model, and Sayres et al. ([124](#bib.bib124))
    used Integrated Gradients to provide explanations for the task of predicting diabetic
    retinopathy from retinal fundus images.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在从胸部 X 光片中检测 COVID-19 的背景下，Lin 等人 ([77](#bib.bib77)) 使用了 Grad-CAM 和 Guided Grad-CAM
    来研究模型认为更具区分性的区域。生成的热图显示，当未使用预处理时，CNN 倾向于集中于非肺部区域（例如脊柱、心脏、背景），这些区域被认为与分类决策无关。然而，当使用掩膜过程仅突出肺部区域时，生成的热图仅突出相关区域，因为
    CNN 注意力限制在检测肺部疾病（肺部区域）的关键区域。按照相同的程序，Lopatina 等人 ([84](#bib.bib84)) 使用 DeepLIFT
    归因算法研究多发性硬化分类模型的决策，Sayres 等人 ([124](#bib.bib124)) 使用 Integrated Gradients 为从视网膜眼底图像预测糖尿病视网膜病变的任务提供解释。
- en: In contrast to the previous approaches, Rio-Torto et al. ([115](#bib.bib115))
    proposed an in-model joint architecture composed of an explainer and a classifier
    to produce visual explanations for the predicted class labels. The explainer consists
    of an encoder-decoder network based on U-Net, and the classifier is based on VGG-16\.
    Since the classifier is trained using the explanations provided by the explainer,
    the classifier focuses only on the relevant regions of the image containing the
    class. The qualitative assessment of the provided explanations was carried out
    by using state-of-the-art explainability methods provided by the Captum library ([65](#bib.bib65)).
    Additionally, a quantitative analysis was also provided in terms of accuracy,
    average precision, AUROC, AOPC ([121](#bib.bib121)) and the proposed POMPOM metric.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，Rio-Torto 等人（[115](#bib.bib115)）提出了一种包含解释器和分类器的模型内联合架构，以生成对预测类别标签的视觉解释。解释器由基于
    U-Net 的编码器-解码器网络组成，分类器则基于 VGG-16。由于分类器是利用解释器提供的解释进行训练的，因此分类器只关注图像中包含类别的相关区域。提供的解释的定性评估是使用
    Captum 库提供的最先进的可解释性方法进行的（[65](#bib.bib65)）。此外，还进行了准确率、平均精度、AUROC、AOPC（[121](#bib.bib121)）和提出的
    POMPOM 指标等方面的定量分析。
- en: 5.1.3\. Discussion
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3. 讨论
- en: Despite the simplicity of the feature attribution methods and their applicability
    to a wide range of approaches, these methods may often produce ambiguous explanations,
    difficulting their qualitative evaluation. Furthermore, preprocessing techniques
    were required for some methods to generate more plausible explanations ([77](#bib.bib77)).
    Thus, researchers began exploring other modalities, such as textual explanations,
    and it was discovered that textual explanations were indeed valid explanations
    and, in some cases, preferred over visual explanations since they are inherently
    understandable by humans  ([152](#bib.bib152)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管特征归因方法简单且适用于广泛的方式，这些方法仍然可能产生模糊的解释，使其定性评估变得困难。此外，某些方法需要预处理技术以生成更可信的解释（[77](#bib.bib77)）。因此，研究人员开始探索其他方式，例如文本解释，发现文本解释确实是有效的解释，并且在某些情况下，比视觉解释更受欢迎，因为它们本质上是易于理解的（[152](#bib.bib152)）。
- en: 5.2\. Explanation by Text
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 通过文本进行解释
- en: 'The use of semantic descriptions became another way of explaining the model
    decisions since most of the clinicians prefer textual explanations compared to
    visual explanations solely, and the combination of textual and visual explanations
    over either alone  ([39](#bib.bib39)). In general, providing textual explanations
    can be built on three paradigms: (i) image captioning, (ii) image captioning with
    visual explanation, and (iii) concept attribution ([152](#bib.bib152)). Figure
    [4](#S5.F4 "Figure 4 ‣ 5.2\. Explanation by Text ‣ 5\. XAI Methods in Medical
    Diagnosis ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") depicts the general scheme adopted by most works to generate a textual
    description based on the visual features of the input image.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语义描述成为解释模型决策的另一种方式，因为大多数临床医生更倾向于文本解释而非单纯的视觉解释，文本和视觉解释的结合优于单独使用其中之一（[39](#bib.bib39)）。通常，提供文本解释可以基于三种范式：（i）图像字幕生成，（ii）结合视觉解释的图像字幕生成，以及（iii）概念归因（[152](#bib.bib152)）。图
    [4](#S5.F4 "图 4 ‣ 5.2. 文本解释 ‣ 5. XAI 方法在医学诊断中的应用 ‣ 医学图像分类中的可解释深度学习方法：综述") 描绘了大多数工作采用的基于输入图像的视觉特征生成文本描述的通用方案。
- en: '![Refer to caption](img/46f0589ce7d87d94d63e6f50803ddf0f.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46f0589ce7d87d94d63e6f50803ddf0f.png)'
- en: Figure 4\. Explanation by textual descriptions. The typical architecture for
    obtaining a textual description from image data combines an image embedding model
    (e.g., CNN) for extracting the features from the input image and a language model
    (e.g., LSTM) for generating the word sentences. The attention module can be inserted
    between those two models to guide the language model to focus only on relevant
    regions of the input image to improve the generation of the word sentences.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 通过文本描述进行解释。获取图像数据的文本描述的典型架构结合了一个图像嵌入模型（例如，CNN）用于从输入图像中提取特征，以及一个语言模型（例如，LSTM）用于生成单词句子。注意力模块可以插入这两个模型之间，以引导语言模型仅关注输入图像的相关区域，从而提高单词句子的生成效果。
- en: 5.2.1\. Image Captioning
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1. 图像字幕生成
- en: 'The task of generating a textual description for explaining a model decision
    can be viewed as an extension of the image captioning problem, commonly treated
    in Natural Language Processing (NLP). Indeed, the vast majority of works that
    aim to generate a textual description for a given input image follow the classical
    strategy of combining a CNN for extracting the visual features with an RNN (e.g.,
    LSTM) to generate the word sentences. Based on this paradigm, Sun et al. ([143](#bib.bib143))
    developed a joint framework to generate sequences of words to provide a textual
    explanation for the task of diagnosing malignant tumors from breast mammography.
    Similarly, Singh et al. ([137](#bib.bib137)) built on an encoder-decoder framework
    composed of a CNN and a stacked Long Short-Term Memory (LSTM) for automatically
    generating radiology reports from chest X-rays. Regarding image captioning with
    visual explanation, Zhang et al. ([176](#bib.bib176)) proposed a multimodal approach,
    dubbed MDNet, composed of an image embedding model and a language model, that
    can generate diagnostic reports, retrieve images by symptom descriptions, and
    visualize network attention. The MDNet model was evaluated on a dataset (BCIDR)
    containing histopathological images of bladder cancer. Furthermore, MDNet inspired
    several approaches. For example, Jing et al. ([55](#bib.bib55)) proposed a multi-task
    learning framework with a co-attention mechanism to guide the generation of text
    according to the localized regions containing abnormalities. They showed that
    a hierarchical LSTM model performs better in generating long text reports. Subsequently,
    Wang et al. ([160](#bib.bib160)) proposed TieNet, which makes use of attention
    modules to extract the most important information from chest X-ray images and
    also use their diagnostic reports in order to guide the model to produce more
    coherent reports. Similarly, Barata et al. ([12](#bib.bib12)) proposed a hierarchical
    classification model that uses attention modules, including channel and spatial
    attention, to identify relevant regions in the skin lesions and subsequently guide
    further the LSTM attending at different locations whilst conferring more transparency
    to the network. Lee et al. ([72](#bib.bib72)) also explained the decisions of
    a breast masses classifier using both visual and textual explanations, based on
    a CNN-RNN architecture. In the same fashion, Gale et al. ([39](#bib.bib39)) proposed
    a model-agnostic interpretable method based on an RNN to produce textual explanations
    for the decisions of deep learning classifiers. Furthermore, they developed a
    visual attention mechanism in charge of highlighting the relevant regions for
    classifying hip fractures in pelvic X-rays. Yin et al. ([170](#bib.bib170)) also
    used attention mechanisms to attend to the regions at sentence level. They proposed
    the Hierarchical Recurrent Neural Network (HRNN) model, composed of two-level
    LSTMs: a word RNN and a sentence RNN. The sentence RNN produces the topic vectors
    whereas the word RNN receives the output of the sentence RNN and infers the words
    that constitute the final report. Moreover, they introduced a matching mechanism
    to map the topic vectors and the sentences into a jointly semantic space that
    minimizes a contrastive loss. Similar to the work of Yin et al. ([170](#bib.bib170)),
    the model proposed by Liu et al. ([83](#bib.bib83)) generates topics from images
    and then completes sentences from these topics. When compared to  ([170](#bib.bib170)),
    this work allows the generation of more coherent report generation due to the
    use of a fine-tuning process that uses reinforcement learning via CIDEr.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 生成用于解释模型决策的文本描述任务可以看作是图像描述问题的扩展，这通常在自然语言处理（NLP）中处理。确实，大多数旨在为给定输入图像生成文本描述的工作遵循了经典的策略，即结合使用卷积神经网络（CNN）提取视觉特征与递归神经网络（RNN）（例如，LSTM）生成单词句子。基于这一范式，Sun
    等人（[143](#bib.bib143)）开发了一个联合框架，用于生成单词序列，以提供乳腺X光图像中恶性肿瘤诊断任务的文本解释。类似地，Singh 等人（[137](#bib.bib137)）在一个由CNN和堆叠长短期记忆（LSTM）组成的编码器-解码器框架上进行了构建，以自动生成胸部X光图像的放射学报告。关于具有视觉解释的图像描述，Zhang
    等人（[176](#bib.bib176)）提出了一种多模态方法，称为MDNet，由图像嵌入模型和语言模型组成，可以生成诊断报告，通过症状描述检索图像，并可视化网络注意力。MDNet模型在包含膀胱癌组织病理图像的数据集（BCIDR）上进行了评估。此外，MDNet还启发了几种方法。例如，Jing
    等人（[55](#bib.bib55)）提出了一种多任务学习框架，带有共同注意机制，以指导根据包含异常的局部区域生成文本。他们展示了分层LSTM模型在生成长文本报告时的表现更佳。随后，Wang
    等人（[160](#bib.bib160)）提出了TieNet，该模型利用注意模块从胸部X光图像中提取最重要的信息，并利用其诊断报告来指导模型生成更连贯的报告。类似地，Barata
    等人（[12](#bib.bib12)）提出了一种分层分类模型，使用包括通道和空间注意在内的注意模块，以识别皮肤病变中的相关区域，并进一步指导LSTM在不同位置的注意，同时为网络提供更多的透明度。Lee
    等人（[72](#bib.bib72)）还使用基于CNN-RNN架构的视觉和文本解释来解释乳腺肿块分类器的决策。以相同的方式，Gale 等人（[39](#bib.bib39)）提出了一种基于RNN的模型无关解释方法，用于为深度学习分类器的决策生成文本解释。此外，他们开发了一种视觉注意机制，负责突出显示分类骨盆X光图像中髋部骨折的相关区域。Yin
    等人（[170](#bib.bib170)）也使用注意机制在句子级别关注区域。他们提出了分层递归神经网络（HRNN）模型，由两个级别的LSTM组成：一个单词RNN和一个句子RNN。句子RNN生成主题向量，而单词RNN接收句子RNN的输出并推断构成最终报告的单词。此外，他们引入了一种匹配机制，将主题向量和句子映射到一个共同的语义空间中，以最小化对比损失。类似于Yin
    等人（[170](#bib.bib170)）的工作，Liu 等人（[83](#bib.bib83)）提出的模型从图像中生成主题，然后从这些主题完成句子。与（[170](#bib.bib170)）相比，这项工作允许生成更连贯的报告生成，因为它使用了通过CIDEr进行的强化学习的微调过程。
- en: A more disruptive approach was introduced by Li et al. ([74](#bib.bib74)) that
    proposed the Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) consisting
    of a retrieval policy module and a generation module. The retrieval policy module
    is responsible for deciding whether sentences are obtained from a generation module
    or retrieved from the template database, which is composed of a set of template
    sentences. Moreover, the retrieval policy and generation modules are updated via
    reinforcement learning, guided by sentence-level and word-level rewards using
    the CIDEr.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人（[74](#bib.bib74)）提出了一种更具颠覆性的方案，提出了包含检索策略模块和生成模块的混合检索-生成强化代理（HRGR-Agent）。检索策略模块负责决定句子是从生成模块获取还是从由一组模板句子组成的模板数据库中检索。此外，检索策略和生成模块通过强化学习进行更新，通过使用CIDEr的句子级和词级奖励进行指导。
- en: In contrast to the previous approaches, Chen et al. ([25](#bib.bib25)) exploited
    the Transformer ([154](#bib.bib154)) architecture, where they incorporated two
    memory modules into the decoder. These modules are responsible for memorizing
    textual patterns and assisting the decoder of the Transformer in generating radiology
    reports containing relevant information associated with chest X-ray images. The
    same tendency is reflected in recent works ([81](#bib.bib81); [82](#bib.bib82);
    [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)), which employ transformer-based
    models with additional custom modules to better capture the relevant features
    of input images, leading to an improved performance in the task of radiological
    report generation. Recently, Selivanov et al. ([127](#bib.bib127)) introduced
    a novel image captioning architecture that combines two language models, incorporating
    image-attention (SAT) ([166](#bib.bib166)) and text-attention (GPT-3) ([18](#bib.bib18)),
    resulting in an outstanding performance compared to the previous methods. For
    a more comprehensive analysis of the use of Transformers in medical imaging, we
    refer the reader to the survey of Shamshad et al. ([129](#bib.bib129)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，Chen等人（[25](#bib.bib25)）利用了Transformer（[154](#bib.bib154)）架构，他们在解码器中加入了两个记忆模块。这些模块负责记忆文本模式，并协助Transformer的解码器生成包含与胸部X光图像相关信息的放射学报告。最近的研究（[81](#bib.bib81);
    [82](#bib.bib82); [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)）也反映了相同的趋势，这些研究采用基于变压器的模型，并添加了自定义模块，以更好地捕捉输入图像的相关特征，从而在放射学报告生成任务中取得了更好的性能。最近，Selivanov等人（[127](#bib.bib127)）介绍了一种新颖的图像标注架构，结合了两个语言模型，融入了图像注意力（SAT）（[166](#bib.bib166)）和文本注意力（GPT-3）（[18](#bib.bib18)），与以前的方法相比，表现突出。有关Transformer在医学成像中应用的更全面分析，请参见Shamshad等人（[129](#bib.bib129)）的调查。
- en: 5.2.2\. Concept Attribution
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 概念归因
- en: 'The idea behind concept-based attribution is learning human-defined concepts
    from the internal activations of a CNN. The use of concepts to provide global
    explanations was proposed by Kim et al.  ([60](#bib.bib60)) with the introduction
    of the Concept Activation Vectors (CAVs), which provide explanations in terms
    of human-understandable concepts that are typically related to parts of the image.
    Kim et al. also proposed Testing with CAVs (TCAV), which enable to quantify the
    importance of a user-defined concept to a classification result. Figure [5](#S5.F5
    "Figure 5 ‣ 5.2.2\. Concept Attribution ‣ 5.2\. Explanation by Text ‣ 5\. XAI
    Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in Medical Image
    Classification: A Survey") illustrates the typical pipeline of concept-based attribution
    methods. In the context of medical imaging, the term “microaneurysm” can be viewed
    as a concept, being possible to be identified by humans in fundus imaging, and
    which denote the presence of diabetic retinopathy  ([152](#bib.bib152)). In the
    same line of research, Graziani et al. ([44](#bib.bib44)) proposed a framework
    for concept-based attribution to generate explanations for CNN decisions of a
    breast histopathology classifier. They built on TCAV by incorporating Regression
    Concept Vectors (RCV), which provide continuous-values measures of a concept instead
    of solely indicating its presence or absence. This is particularly useful in the
    medical domain since a value indicating, for example, tumor size is more informative
    than a binary value indicating its presence or absence. Graziani et al. also concluded
    that the learning of concepts by an intermediate layer of a CNN could be improved
    by removing spatial dependencies of the convolutional layers and introducing L2
    norm regularization in the regression problem. Recently, Lucieri et al. ([85](#bib.bib85))
    introduced ExAID, a framework that provides multimodal concept-based explanations
    for the task of melanoma classification. Their framework relied on CAVs for concept
    identification and used the TCAV method to estimate the influence of a specific
    concept on the decision. The authors provided textual explanations in the form
    of template phrases by only replacing the identified concepts and their importance
    to the prediction in the phrase structure. In order to localize the regions of
    the learned concepts in the latent space of the trained classifier, the authors
    used the Concept Localization Maps ([86](#bib.bib86)) which uses perturbation-based
    concept localization to generate a saliency map highlighting the relevant regions
    with respect to the learned concepts, thus providing visual explanations.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概念的归因的核心思想是从CNN的内部激活中学习人类定义的概念。Kim等人提出了使用概念提供全局解释的方案（[60](#bib.bib60)），引入了概念激活向量（CAVs），这些向量以通常与图像部分相关的人类可理解的概念来提供解释。Kim等人还提出了使用CAVs进行测试（TCAV），可以量化用户定义的概念对分类结果的重要性。图[5](#S5.F5
    "图5 ‣ 5.2.2\. 概念归因 ‣ 5.2\. 文本解释 ‣ 5\. 医学诊断中的XAI方法 ‣ 医学图像分类中的可解释深度学习方法：综述")展示了基于概念的归因方法的典型流程。在医学影像的背景下，“微动脉瘤”可以被视为一个概念，可以在人眼底图像中被识别，且表示糖尿病视网膜病变的存在（[152](#bib.bib152)）。在相同的研究方向上，Graziani等人（[44](#bib.bib44)）提出了一种基于概念的归因框架，用于生成乳腺组织病理分类器的CNN决策解释。他们在TCAV的基础上，结合了回归概念向量（RCV），提供概念的连续值度量，而不仅仅是指示其存在或不存在。这在医学领域尤为有用，因为例如肿瘤大小的值比表示其存在或不存在的二进制值更具信息量。Graziani等人还得出结论，通过去除卷积层的空间依赖性和在回归问题中引入L2范数正则化，可以改进CNN中间层对概念的学习。最近，Lucieri等人（[85](#bib.bib85)）介绍了ExAID，一个为黑色素瘤分类任务提供多模态概念解释的框架。他们的框架依赖于CAVs进行概念识别，并使用TCAV方法估计特定概念对决策的影响。作者通过仅替换识别出的概念及其对预测的重要性来提供模板短语形式的文本解释。为了在训练分类器的潜在空间中定位学习到的概念区域，作者使用了概念定位图（[86](#bib.bib86)），该图通过基于扰动的概念定位生成显著性图，突出显示与学习到的概念相关的区域，从而提供可视化解释。
- en: '![Refer to caption](img/8e4f3c71e91e903a8e675b8d96f51fff.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e4f3c71e91e903a8e675b8d96f51fff.png)'
- en: Figure 5\. Explanation by concept attribution. In the first phase, the human-defined
    concepts (Regular Dots and Globules, Atypical Pigment Network, Typical Pigment
    Network, Streaks, Irregular Dots and Globules, and Blue-Whitish Veil) are modeled
    as numeric features, following the CAV technique. Then, after the input image
    passes through a classifier model, the class prediction is globally explained
    based on the importance of each concept to the final prediction.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 概念归因解释。在第一阶段，人为定义的概念（常规点和小球、非典型色素网络、典型色素网络、条纹、不规则点和小球，以及蓝白色面纱）被建模为数值特征，遵循
    CAV 技术。然后，在输入图像通过分类器模型后，类预测会根据每个概念对最终预测的重要性进行全球解释。
- en: 5.2.3\. Discussion
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 讨论
- en: In summary, except for the work of Chen et al. ([25](#bib.bib25)), all the explanation
    text-based approaches mentioned above rely on RNN architectures to generate text
    descriptions towards providing a more human-interpretable explanation for a model
    decision. However, as stated by Pascanu et al. ([105](#bib.bib105)), RNN-based
    approaches, such as LSTM, have some limitations in generating long text reports.
    On the contrary, concept-based attribution methods provide a more objective and
    human-understandable way of interpreting classification decisions. However, the
    main limitation of these methods is the need for manual annotations of the concept
    examples, which may be impractical for specific medical image modalities, increasing
    the need for involving clinicians in the annotation tasks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，除了 Chen 等人的工作 ([25](#bib.bib25))，上述所有基于文本的解释方法都依赖 RNN 架构生成文本描述，以提供对模型决策的更人性化解释。然而，正如
    Pascanu 等人所述 ([105](#bib.bib105))，基于 RNN 的方法，如 LSTM，在生成长文本报告方面存在一些限制。相反，基于概念的归因方法提供了更客观和人类易懂的分类决策解释方式。然而，这些方法的主要限制是需要对概念示例进行人工标注，这对于特定医疗图像模态可能不切实际，从而增加了临床医生参与标注任务的需求。
- en: 5.3\. Explanation by Examples
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 示例解释
- en: 'The type of methods that explain a model decision by selecting a set of similar
    examples are dubbed example-based explanation methods. Apart from explaining algorithm
    decisions, this strategy is also commonly used between clinicians to explain the
    rationale behind their decision process. Below, we categorize example-based explanation
    methods into the following categories: (i) Case-Based Reasoning, (ii) Counterfactual
    Explanations, and (iii) Propotypes.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择一组相似示例来解释模型决策的方法被称为基于示例的解释方法。除了解释算法决策外，这一策略在临床医生之间也常用于解释其决策过程的理由。下面，我们将基于示例的解释方法分类为以下几类：（i）基于案例的推理，（ii）反事实解释，以及（iii）原型。
- en: 5.3.1\. Case-Based Reasoning
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 基于案例的推理
- en: 'Case-Based Reasoning (CBR) and Content-Based Image Retrieval (CBIR) are example-based
    explanation methods that aim to search a database for visually similar entries
    to a specific query image. The general scheme for implementing a CBR system using
    DNNs is depicted in Figure [6](#S5.F6 "Figure 6 ‣ 5.3.1\. Case-Based Reasoning
    ‣ 5.3\. Explanation by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey")a. Although the
    idea of using CBIR systems in clinical settings is not novel ([4](#bib.bib4)),
    there has been renewed interest in CBIR approaches as a way to provide explainability
    to deep learning methods for medical diagnosis ([26](#bib.bib26); [48](#bib.bib48);
    [6](#bib.bib6)).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基于案例的推理（CBR）和基于内容的图像检索（CBIR）是基于示例的解释方法，旨在搜索数据库中与特定查询图像视觉上相似的条目。使用 DNN 实现 CBR
    系统的一般方案如图 [6](#S5.F6 "图 6 ‣ 5.3.1\. 基于案例的推理 ‣ 5.3\. 示例解释 ‣ 5\. 医疗诊断中的 XAI 方法 ‣
    医疗图像分类中的可解释深度学习方法：综述")a 所示。尽管在临床环境中使用 CBIR 系统的想法并不新颖 ([4](#bib.bib4))，但对 CBIR
    方法的兴趣重新燃起，因为它们为深度学习医疗诊断方法提供了可解释性 ([26](#bib.bib26); [48](#bib.bib48); [6](#bib.bib6))。
- en: <svg   height="200.78" overflow="visible" version="1.1" width="586.6"><g transform="translate(0,200.78)
    matrix(1 0 0 -1 0 0) translate(161.99,0) translate(0,109.96)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -157.37 -85.5)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="171" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/a13dfd21b302b7cbc02aa03fb0d29fd0.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 180 -83)" fill="#000000" stroke="#000000"><foreignobject
    width="240" height="166" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/239036c54a031057caa680f9967a0015.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -101.88)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -101.88)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. (a) Explanation by Case-Based Reasoning. The feature vector corresponding
    to the input image is compared against the feature vectors of the images in the
    catalogue using a distance metric, such as L2 distance. Finally, the images are
    retrieved from the catalogue ranked by their similarity to the input image. (b)
    Explanation by Counterfactual Examples. To explain the prediction made by a classifier,
    the input image is modified in a controlled way, typically by using a generative
    model (e.g., GAN) to shift the original class, i.e., from normal to abnormal or
    vice-versa. Thus, the counterfactual example intends to explain the prediction
    by showing that the image was classified as ”abnormal” because it is not ”normal”,
    as perceived by the absence of tissue inflammation (white spots) in the generated
    counterfactual example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Barnett et al. ([14](#bib.bib14)) introduced a novel interpretable
    AI algorithm (IAIA-BL) for classifying breast masses using CBR. The model provided
    both a prediction of malignancy and its explanation by using known medical features
    (mass margins). Given an image region to analyze, the algorithm compared that
    region with a set of previous similar cases (image patches) using euclidean distance.
    The similarity score was then used to provide the mass margin scores for each
    medical feature, and those scores were then used to predict the malignancy score
    (benign or malignant). The model was trained using a fine-annotation loss penalizing
    activations of medically irrelevant regions on the data. The authors also introduced
    an interpretable evaluation metric, namely Activation Precision, to quantify the
    proportion of relevant information from the “relevant region” used to classify
    the mass margin regarding the radiologist annotations. The experimental results
    showed that the IAIA-BL achieved comparable performance to black-box models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Barnett 等人 ([14](#bib.bib14)) 提出了一个新型的可解释 AI 算法（IAIA-BL），用于使用 CBR 对乳腺肿块进行分类。该模型不仅提供了恶性肿块的预测，还通过使用已知的医学特征（肿块边缘）提供了解释。给定一个待分析的图像区域，该算法通过欧氏距离将该区域与一组先前类似的病例（图像补丁）进行比较。相似性分数然后被用来提供每个医学特征的肿块边缘分数，这些分数随后被用来预测恶性评分（良性或恶性）。该模型使用了一种细化注释损失来惩罚数据中医学不相关区域的激活。作者还引入了一种可解释的评价指标，即激活精度，用于量化“相关区域”中用于分类肿块边缘的相关信息比例，相对于放射科医师的注释。实验结果表明，IAIA-BL
    达到了与黑箱模型相当的性能。
- en: A different approach was presented by Tschandl et al. ([148](#bib.bib148)),
    in which they compared the predictions of the ResNet-50 softmax classifier with
    the diagnostic accuracy obtained by using CBIR. Contrary to Barnett et al. ([14](#bib.bib14)),
    Tschandl et al. measured the cosine similarity between two feature vectors to
    retrieve the most similar images to the image query. The results showed that the
    diagnostic accuracy obtained through CBIR is comparable to the performance of
    a softmax classifier leading Tschandl et al. to claim that CBIR can replace traditional
    softmax classifiers to improve diagnostic interpretability in a clinical workflow.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Tschandl 等人 ([148](#bib.bib148)) 提出了另一种方法，在这种方法中，他们比较了 ResNet-50 softmax 分类器的预测结果与使用
    CBIR 获得的诊断准确率。与 Barnett 等人 ([14](#bib.bib14)) 的研究相反，Tschandl 等人测量了两个特征向量之间的余弦相似度，以检索与图像查询最相似的图像。结果显示，通过
    CBIR 获得的诊断准确率与 softmax 分类器的性能相当，这使得 Tschandl 等人声称 CBIR 可以替代传统的 softmax 分类器，以提高临床工作流程中的诊断可解释性。
- en: A more recent approach was introduced by Barata and Santiago ([13](#bib.bib13)),
    where CBIR was applied to explain the decisions of a CNN model for skin cancer
    diagnosis. When compared to the work of Tschandl et al., Barata and Santiago implemented
    an augmented category-cross entropy loss function composed of three regularization
    losses, namely the triplet loss, the contrastive loss, and the distillation loss.
    These losses encourage the model to learn a more structured feature space. The
    experimental results on ISIC 2018 ([149](#bib.bib149)) dermoscopy dataset confirmed
    that the combination of different loss functions lead to more structured feature
    spaces, which improves the performance of the classification model. Lamy et al. ([69](#bib.bib69))
    proposed an explainable CBR system with a visual interface, combining quantitative
    and qualitative approaches. In contrast to the above-discussed works, it used
    numerical data and provided a user study in which clinicians validated their approach.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的方法由 Barata 和 Santiago 提出 ([13](#bib.bib13))，其中将 CBIR 应用于解释 CNN 模型在皮肤癌诊断中的决策。与
    Tschandl 等人的工作相比，Barata 和 Santiago 实现了一种扩展的类别交叉熵损失函数，由三种正则化损失组成，即三元组损失、对比损失和蒸馏损失。这些损失促使模型学习到更结构化的特征空间。在
    ISIC 2018 ([149](#bib.bib149)) 皮肤镜数据集上的实验结果证实，不同损失函数的组合导致了更结构化的特征空间，从而提升了分类模型的性能。Lamy
    等人 ([69](#bib.bib69)) 提出了一个具有可解释性的 CBR 系统，配有视觉界面，结合了定量和定性方法。与上述讨论的工作相比，该系统使用了数值数据，并提供了一个用户研究，其中临床医生验证了他们的方法。
- en: 'The recent work of Hu et al. ([53](#bib.bib53)) introduced the eXplainable
    Medical Image Retrieval (X-MIR) approach, which explored the use of similarity-based
    saliency maps to explain the retrieved images visually. Concretely, they adapted
    the saliency map generation process for the problem of image retrieval through
    the use of a similarity-based formulation. These similarity-based saliency methods
    take as input a retrieval image and a query image for producing a saliency map
    highlighting the most similar regions of the retrieval image to the query image.
    To evaluate the quality of the generated saliency maps, the authors adapted two
    causal metrics, namely deletion and insertion (refer to section [6.1](#S6.SS1
    "6.1\. Evaluating the Quality of Visual Explanations ‣ 6\. Evaluation Metrics
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")
    for a detailed description), to measure the decrease or increase in image similarity
    score as the retrieved image is gradually perturbed based on the important regions
    of its saliency map. The authors evaluated their approach on two medical datasets,
    namely COVIDx ([156](#bib.bib156)) and ISIC 2017\. They found that for both cases,
    the generated saliency maps focused on relevant regions when retrieved images
    were correct and observed the contrary when the retrieved images were incorrect.
    Finally, the authors pointed out for the importance of conducting user studies
    with clinicians to validate the utility of their approach. Silva et al. ([132](#bib.bib132))
    also explored the medical image retrieval with the addition of saliency maps to
    improve the class-consistency of top retrieved results while enhancing the interpretability
    of the whole system by accompanying the retrieval with visual explanations.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hu 等人（[53](#bib.bib53)）近期的工作介绍了 eXplainable Medical Image Retrieval (X-MIR)
    方法，该方法探讨了使用基于相似性的显著性图来视觉解释检索图像。具体来说，他们通过使用基于相似性的公式调整了显著性图生成过程以解决图像检索问题。这些基于相似性的显著性方法以检索图像和查询图像为输入，生成一个显著性图，突出显示检索图像中与查询图像最相似的区域。为了评估生成的显著性图的质量，作者采用了两种因果指标，即删除和插入（有关详细描述，请参见[6.1](#S6.SS1
    "6.1\. Evaluating the Quality of Visual Explanations ‣ 6\. Evaluation Metrics
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")），以测量基于显著性图的重要区域逐渐扰动检索图像时图像相似性得分的减少或增加。作者在两个医疗数据集（即
    COVIDx ([156](#bib.bib156)) 和 ISIC 2017）上评估了他们的方法。他们发现，在检索图像正确时生成的显著性图集中于相关区域，而在检索图像错误时则观察到相反的情况。最后，作者指出了与临床医生进行用户研究以验证其方法实用性的重要性。Silva
    等人（[132](#bib.bib132)）也探讨了在医学图像检索中加入显著性图，以提高顶级检索结果的类别一致性，同时通过视觉解释增强整个系统的可解释性。'
- en: 'In order to assess the effectiveness of using a CBIR system as an auxiliary
    tool for classifying skin lesions through dermatology images, a user-centered
    study was done by Sadeghi et al. ([119](#bib.bib119)). Sixteen non-expert users
    were invited to classify skin lesions images among four categories (Nevus, Seborrheic
    Keratosis, Basal Cell Carcinoma, and Malignant Melanoma) based on two conditions:
    using CBIR and without using CBIR. The results indicated that CBIR enabled users
    to make a significantly more accurate classification on a new skin lesion image.
    These findings suggest that CBIR can indeed assist clinicians understand model
    decisions as well as allow less experienced practitioners to improve their skills.
    Thus, CBIR-based systems can have a significant clinical application value as
    a decision-support tool to accelerate the diagnosis of pathologies.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估使用 CBIR 系统作为辅助工具通过皮肤病学图像分类皮肤病变的有效性，Sadeghi 等人进行了以用户为中心的研究（[119](#bib.bib119)）。邀请了十六位非专家用户对四个类别（痣、脂溢性角化病、基底细胞癌和恶性黑色素瘤）的皮肤病变图像进行分类，条件包括使用
    CBIR 和不使用 CBIR。结果显示，CBIR 使用户能够对新的皮肤病变图像进行显著更准确的分类。这些发现表明，CBIR 确实可以帮助临床医生理解模型决策，并使经验较少的从业人员提高技能。因此，基于
    CBIR 的系统作为决策支持工具在加速病理诊断方面具有显著的临床应用价值。
- en: 5.3.2\. Counterfactual Explanations
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 反事实解释
- en: 'Counterfactual explanations are based on the principle that “an action on the
    input data will cause an outcome” ([97](#bib.bib97)). The idea is to perturb the
    input data in a controlled way in order to reverse the final model prediction,
    being the modified input the counterfactual example, as illustrated in the diagram
    of the Figure [6](#S5.F6 "Figure 6 ‣ 5.3.1\. Case-Based Reasoning ‣ 5.3\. Explanation
    by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey")b. Furthermore, counterfactual
    explanations are deemed human-interpretable and post-hoc, meaning that they do
    not require access to model internals.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗解释基于“对输入数据的一个操作会导致一个结果”的原则 ([97](#bib.bib97))。其思想是以受控的方式扰动输入数据，以逆转最终模型预测，将修改后的输入作为对抗例子，如图
    [6](#S5.F6 "Figure 6 ‣ 5.3.1\. Case-Based Reasoning ‣ 5.3\. Explanation by Examples
    ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in
    Medical Image Classification: A Survey")b 中所示。此外，对抗解释被认为是可人为解释的和事后解释的，意味着它们不需要访问模型内部。'
- en: The work of Schutte et al. ([126](#bib.bib126)) constituted a new approach in
    the way of interpreting the predictions made by deep learning models by using
    generative models to produce a sequence of images depicting the evolution of a
    pathology. Through the sequence of images produced, a human can understand which
    biomarkers triggered the prediction made by the model. Concretely, the proposed
    method aims to identify the optimal direction in latent space to produce a series
    of synthetic images with minor modifications leading to different model predictions.
    By observing these modified synthetic versions of the original image, it is expectable
    that a human can perceive the features that caused the model prediction. Experimental
    results on two medical image datasets showed that the proposed approach allows
    visualizing where the most relevant features are localized and how they contributed
    to the model prediction by analyzing the generated images. Moreover, this generative
    approach may be helpful for the identification of new biomarkers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Schutte 等人 ([126](#bib.bib126)) 的工作采用了一种新方法，通过使用生成模型生成描述病理演变的图像序列来解释深度学习模型的预测。通过生成的图像序列，人们可以理解哪些生物标志物触发了模型的预测。具体而言，所提出的方法旨在识别潜在空间中的最佳方向，以生成一系列具有微小修改的合成图像，从而导致不同的模型预测。通过观察这些修改后的合成图像，人们可以感知导致模型预测的特征。对两个医学图像数据集的实验结果表明，所提出的方法可以可视化最相关特征的位置及其如何通过分析生成的图像贡献于模型预测。此外，这种生成方法可能有助于新生物标志物的识别。
- en: Kim et al. ([62](#bib.bib62)) proposed the Counterfactual Generative Network
    (CGN), which is able to generate counterfactual images to explain the predictions
    of a pneumonia classifier from chest X-ray images. To guide the CGN towards the
    generation of contrastive images from query image, the prediction of the classification
    network was manipulated to shift the original class to the negative class. The
    subtraction of the counterfactual image from the input image allows the generation
    of attribution maps evidencing the most relevant regions to the prediction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 等人 ([62](#bib.bib62)) 提出了对抗生成网络 (CGN)，该网络能够生成对抗图像来解释肺炎分类器从胸部 X 射线图像的预测。为了引导
    CGN 从查询图像生成对比图像，分类网络的预测被操控以将原始类别转换为负类。从输入图像中减去对抗图像可以生成归因图，证明了与预测最相关的区域。
- en: In the same fashion, Singla et al. ([138](#bib.bib138)) used a conditional Generative
    Adversarial Network (cGAN) to produce a set of counterfactual images with changed
    posterior probability to explain the class predictions of a chest X-ray classifier.
    Additionally, the context from semantic segmentation and object detection was
    incorporated into the loss function to preserve subtle information about the medical
    images in the generated counterfactual images. The validity of the generated counterfactual
    explanations was assessed through the use of three evaluation metrics, namely
    1) Fréchet Instance Distance score to evaluate the visual quality of the counterfactual
    images, 2) Counterfactual Validity score to quantify the class floppiness of the
    counterfactual images, and 3) Foreign Object Preservation score to assess the
    presence of unique properties of patients in the generated explanations. Furthermore,
    clinical measurements, namely, cardiothoracic ratio and costophrenic recess, were
    adopted to demonstrate the utility of the explanations in terms of the clinical
    context.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Singla 等人 ([138](#bib.bib138)) 使用条件生成对抗网络（cGAN）生成了一组具有改变后验概率的反事实图像，以解释胸部
    X 光分类器的类别预测。此外，将语义分割和目标检测的上下文纳入损失函数中，以保留生成反事实图像中的医学图像的细微信息。生成反事实解释的有效性通过三个评估指标来评估，即
    1) Fréchet 实例距离分数，用于评估反事实图像的视觉质量，2) 反事实有效性分数，用于量化反事实图像的类别模糊性，以及 3) 外来物体保留分数，用于评估生成解释中患者独特属性的存在。此外，还采用了临床测量指标，即心胸比和膈角，以展示解释在临床背景中的实用性。
- en: Given the recent advances in the scope of image synthesis, the use of generative
    diffusion probabilistic models ([50](#bib.bib50)) to produce counterfactual explanations
    is an interesting future research direction as it is under-explored in medical
    imaging. The benefit of using these models to generate counterfactual explanations
    is related to their ability to handle missing data and their robustness to distributional
    shifts ([58](#bib.bib58)). A prominent example is the work of Sanchez et al. ([122](#bib.bib122))
    that relied on conditional diffusion models for synthesizing healthy counterfactual
    examples of brain images, allowing to segment the lesion through the difference
    between the observed image and the healthy counterfactual.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于图像合成领域的最新进展，使用生成扩散概率模型 ([50](#bib.bib50)) 来生成反事实解释是一个有趣的未来研究方向，因为在医学成像中这一领域尚未得到充分探索。使用这些模型生成反事实解释的好处与它们处理缺失数据的能力和对分布变化的鲁棒性有关
    ([58](#bib.bib58))。一个显著的例子是 Sanchez 等人 ([122](#bib.bib122)) 的工作，他们依靠条件扩散模型合成健康的反事实脑图像示例，通过观察图像与健康反事实之间的差异来分割病变。
- en: '![Refer to caption](img/ff2629524027eba98bd3a0b82bba7f80.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ff2629524027eba98bd3a0b82bba7f80.png)'
- en: Figure 7\. Explanation by Prototypes. During the model training, a set of prototypes
    are learned to visually represent a certain class (represented with blue and orange
    colours in the figure). In the test phase, the features extracted from the test
    image are compared with a set of prototypes using a similarity metric, such as
    cosine similarity. Then, the final class prediction is based on the similarity
    scores computed between the prototypes and the different parts of the input image.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 原型解释。在模型训练过程中，会学习一组原型来可视化地表示某个类别（在图中用蓝色和橙色表示）。在测试阶段，通过相似度度量（如余弦相似度）将从测试图像中提取的特征与一组原型进行比较。然后，最终的类别预测基于原型与输入图像不同部分之间计算的相似度分数。
- en: 5.3.3\. Propotypes
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 原型
- en: 'While most research on interpretability is still oriented towards the use of
    post-hoc approaches, some authors have advocated the need for devising inherently
    interpretable models ([118](#bib.bib118)) to obtain explanation that are indeed
    interpretable by humans. The learning of prototypes during the training phase
    of the model is a common strategy in the development of inherently interpretable
    models. This idea was initially explored in ([22](#bib.bib22)), where the authors
    incorporate a prototype layer at the end of the network ([22](#bib.bib22); [14](#bib.bib14)),
    named ProtoPNet, for bird species recognition. The rationale behind this approach
    is that different parts of the image act as class-representative prototypes during
    training. When a new image needs to be evaluated in the testing phase, the network
    finds the most similar prototypes to the parts of the test image. The final class
    prediction is based on a score computed with the similarities between the prototypes.
    Figure [7](#S5.F7 "Figure 7 ‣ 5.3.2\. Counterfactual Explanations ‣ 5.3\. Explanation
    by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey") illustrates the general pipeline
    for deriving a class prediction from the similarity score between different parts
    of the input image and a set of learned prototypes.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然大多数关于可解释性的研究仍然倾向于使用事后方法，一些作者提倡需要设计本质上可解释的模型 ([118](#bib.bib118))，以获得人类确实能够解释的解释。模型训练阶段原型的学习是开发本质上可解释模型的一个常见策略。这一思想最初在
    ([22](#bib.bib22)) 中进行了探讨，作者们在网络末尾加入了一个名为ProtoPNet的原型层 ([22](#bib.bib22); [14](#bib.bib14))，用于鸟类物种识别。这种方法的基本原理是图像的不同部分在训练过程中充当类别代表原型。当新图像在测试阶段需要评估时，网络找到与测试图像部分最相似的原型。最终的类别预测基于通过原型之间的相似度计算的分数。图
    [7](#S5.F7 "Figure 7 ‣ 5.3.2\. Counterfactual Explanations ‣ 5.3\. Explanation
    by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey") 说明了从输入图像不同部分与一组学习的原型之间的相似度分数中推导类别预测的一般流程。'
- en: Based on ProtoPNet, Donnelly et al. ([33](#bib.bib33)) introduced the Deformable
    ProtoPNet. This prototypical case-based interpretable neural network provided
    spatially flexible deformable prototypes, i.e., prototypes that can change their
    relative position to detect semantically similar parts of an input image.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ProtoPNet，Donnelly等人 ([33](#bib.bib33)) 引入了可变形ProtoPNet。这种原型案例基础的可解释神经网络提供了空间上灵活的可变形原型，即可以改变相对位置以检测输入图像中的语义相似部分的原型。
- en: Despite the significance of ProtoPNet, Hoffmann et al. ([51](#bib.bib51)) investigated
    its shortcomings, and proved that ProtoPNet could be susceptible to adversarial
    and compression noise, and thus compromise the inner interpretability of the model.
    Although these limitations were not significant in the bird recognition problem,
    the picture changes in high-stake applications, such as the healthcare domain,
    where the lack of robustness can have severe implications.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ProtoPNet具有重要意义，Hoffmann等人 ([51](#bib.bib51)) 调查了其缺陷，并证明ProtoPNet可能容易受到对抗性和压缩噪声的影响，从而妨碍模型的内在可解释性。尽管这些限制在鸟类识别问题中并不显著，但在如医疗保健领域等高风险应用中，缺乏鲁棒性可能会带来严重后果。
- en: Regarding the applicability of these inherently interpretable networks to medical
    imaging, Kim et al. ([61](#bib.bib61)) proposed an interpretable diagnosis framework,
    dubbed XProtoNet, for chest radiography to learn disease representative features
    within a dynamic area, using an occurrence map. Contrary to ProtoPNet, in XProtoNet
    the prototypes are class-representative and completely dynamic in terms of area,
    which is particularly important for accommodating the high variability in size
    of discriminative regions of medical images. To produce an appropriate occurrence
    map, the authors introduced two regularization terms. The L1 loss forces the occurrence
    area to be small enough to avoid covering irrelevant regions, and the transformation
    loss approximates each occurrence map with a transformed version via an affine
    transformation that did not change the relative location of a disease pattern.
    The experiments on NIH Chest X-ray dataset ([159](#bib.bib159)) confirmed that
    the XProtoNet surpasses the state-of-the-art models in diagnosing chest diseases
    from X-ray images. Later, Singh et al. ([135](#bib.bib135)) introduced an interpretable
    deep learning model, named Generalized Prototypical Part Network (Gen-ProtoPNet),
    for detecting Covid-19 from X-ray images. Gen-ProtoPNet was inspired in the original
    ProtoPNet ([22](#bib.bib22)) and the NP-ProtoPNet ([136](#bib.bib136)). Unlike
    ProtoPNet and NP-ProtoPNet that use L2 distance to calculate the similarity between
    prototypes, Gen-ProtoPNet used a generalized version of the L2 distance, allowing
    the use of prototypical parts of any dimension, i.e., squared and rectangular
    spatial dimensions. Furthermore, the experiments on two Covid-19 chest X-ray datasets ([28](#bib.bib28);
    [157](#bib.bib157)) confirmed that using prototypical parts of spatial dimensions
    bigger than 1 x 1 improves performance of the model, specifically when using the
    VGG-16 model as the feature extractor.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些固有可解释网络在医学影像中的适用性，Kim 等人 ([61](#bib.bib61)) 提出了一个名为 XProtoNet 的可解释诊断框架，用于胸部X光检查，在动态区域内学习疾病代表特征，使用发生图。与
    ProtoPNet 相反，XProtoNet 中的原型在区域上是类别代表性的且完全动态的，这对于适应医学图像中判别区域大小的高变异性尤为重要。为了生成适当的发生图，作者引入了两个正则化项。L1
    损失迫使发生区域足够小，以避免覆盖无关区域，变换损失则通过仿射变换将每个发生图近似为变换版本，该变换没有改变疾病模式的相对位置。对 NIH 胸部 X 光数据集
    ([159](#bib.bib159)) 的实验确认 XProtoNet 在从 X 光图像中诊断胸部疾病方面超越了现有最先进模型。后来，Singh 等人 ([135](#bib.bib135))
    介绍了一个名为 Generalized Prototypical Part Network（Gen-ProtoPNet）的可解释深度学习模型，用于从 X 光图像中检测
    Covid-19。Gen-ProtoPNet 的灵感来源于原始的 ProtoPNet ([22](#bib.bib22)) 和 NP-ProtoPNet ([136](#bib.bib136))。与使用
    L2 距离计算原型之间相似性的 ProtoPNet 和 NP-ProtoPNet 不同，Gen-ProtoPNet 使用了 L2 距离的广义版本，允许使用任意维度的原型部分，即方形和矩形空间维度。此外，在两个
    Covid-19 胸部 X 光数据集 ([28](#bib.bib28); [157](#bib.bib157)) 上的实验确认，使用大于 1 x 1 的空间维度的原型部分可以提高模型性能，特别是在使用
    VGG-16 模型作为特征提取器时。
- en: 5.3.4\. Discussion
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4\. 讨论
- en: Relying on example-based strategies may be a more trustworthy option since the
    retrieved examples are plausible and tend to contain similar findings to the input
    query image. However, the performance of the example-based systems can be compromised
    if a significant number of examples per class is unavailable. This assumption
    is also valid in the case of prototype-based approaches, where performance depends
    directly on the diversity and amount of class-representative prototypes. Regarding
    the counterfactual explanations, it is desirable to discover credible causal structures
    to create ground-truth explanations to improve further the modelling of the interventions
    made over the images ([138](#bib.bib138)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于基于示例的策略可能是更值得信赖的选择，因为检索到的示例是可信的，并且通常包含与输入查询图像类似的发现。然而，如果每个类别的示例数量不足，则示例基于的系统性能可能会受到影响。这个假设在基于原型的方法中也是适用的，其中性能直接依赖于类别代表性原型的多样性和数量。关于反事实解释，发现可靠的因果结构以创建真实解释是可取的，以进一步改善对图像进行干预的建模
    ([138](#bib.bib138))。
- en: 5.4\. Explanation by Concepts
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 概念解释
- en: 'The rationale behind concept-based learning approaches is using human-specified
    concepts as an intermediate step to derive the final predictions. This idea was
    used in the works of Kumar et al. ([67](#bib.bib67)) and Lampert et al. ([68](#bib.bib68)),
    with specific applications in few-shot learning approaches. In ([68](#bib.bib68)),
    the proposed model first estimated a set of attributes which were subsequently
    used to infer the final predictions. This type of model architectures were dubbed
    Concept Bottleneck Models (CBM) in the work of Koh et al. ([64](#bib.bib64)).
    In simple terms, these models relied on an encoder-decoder paradigm, where the
    encoder is responsible for predicting the concepts given the raw input image,
    and the decoder leverages the predicted concepts by the encoder to make the final
    predictions. The encoder is typically a CNN model with a bottleneck layer inserted
    after the last convolutional layer, while the decoder can be a multi-layer perceptron
    to map the concepts to the final predictions. This pipeline is illustrated in
    Figure [8](#S5.F8 "Figure 8 ‣ 5.4\. Explanation by Concepts ‣ 5\. XAI Methods
    in Medical Diagnosis ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey"). The idea of CBMs can be applied to any model just by inserting the
    bottleneck layer after the final convolutional layer. However, the main disadvantage
    of these methods is that annotated concepts are required. Koh et al. provided
    a systematic study on different ways to learn CBMs. Among the considered setup
    models, they concluded that the joint training is the preferred approach, which
    minimizes the weighted sum considering the classification loss and concept loss.
    The authors also stated that it is possible to intervene in the concept predictions
    to change the final output, which raises the question of to what extent it is
    feasible to revisit, for example, 100 concepts and perceive which concept would
    be modified to make the correct prediction. Later, Yuksekgonul et al. ([173](#bib.bib173))
    introduced Post-hoc CBM (PCBM) to address some limitations of CBM, specifically
    the need for concept-level annotations. The authors claim that PCBM can convert
    any pre-trained model into a concept bottleneck model. When concept annotations
    are unavailable, PCBM can leverage concept examples from other datasets and train
    linear binary classifiers to distinguish between examples of a single concept
    and negative examples. Yuksekgonul et al. ([173](#bib.bib173)) identified that
    the problem with CBM is that they require concept-level annotations per image,
    which is expensive and difficult to obtain, particularly in the context of skin
    lesions. Concept Activation Vectors (CAVs) ([60](#bib.bib60)) can be adopted to
    mitigate this by automatically predicting the presence or absence of single concepts
    on unseen images.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '基于概念的学习方法的基本原理是使用人为指定的概念作为中间步骤来推导最终预测结果。这个想法被Kumar等人（[67](#bib.bib67)）和Lampert等人（[68](#bib.bib68)）的研究所采用，并在少样本学习方法中得到了具体应用。在([68](#bib.bib68))中，提出的模型首先估计了一组属性，然后使用这些属性来推断最终预测。这类模型架构在Koh等人的研究中被称为概念瓶颈模型（CBM）（[64](#bib.bib64)）。简单来说，这些模型依赖于编码器-解码器范式，其中编码器负责根据原始输入图像预测概念，解码器则利用编码器预测的概念来做出最终预测。编码器通常是一个CNN模型，最后卷积层后插入一个瓶颈层，而解码器可以是一个多层感知机，用于将概念映射到最终预测结果。这一流程在图[8](#S5.F8
    "Figure 8 ‣ 5.4\. Explanation by Concepts ‣ 5\. XAI Methods in Medical Diagnosis
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")中进行了说明。CBM的理念可以应用于任何模型，只需在最后的卷积层后插入瓶颈层。然而，这些方法的主要缺点是需要注释概念。Koh等人对不同学习CBM的方法进行了系统研究。他们得出的结论是，联合训练是首选方法，这可以最小化分类损失和概念损失的加权和。作者还指出，可以干预概念预测以改变最终输出，这引发了一个问题：例如，重新审视100个概念并判断哪些概念会被修改以做出正确预测的可行性。后来，Yuksekgonul等人（[173](#bib.bib173)）引入了事后CBM（PCBM），以解决CBM的一些局限性，特别是对概念级注释的需求。作者声称，PCBM可以将任何预训练模型转换为概念瓶颈模型。当没有概念注释时，PCBM可以利用其他数据集中的概念示例，并训练线性二分类器来区分单一概念的示例和负样本。Yuksekgonul等人（[173](#bib.bib173)）发现CBM的问题在于它们需要每张图像的概念级注释，这在皮肤病变的背景下尤其昂贵且难以获得。概念激活向量（CAVs）（[60](#bib.bib60)）可以被采用于通过自动预测未见图像上的单一概念的存在或缺失来缓解这个问题。'
- en: '![Refer to caption](img/4a354d17a324582e295710666dd176c7.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4a354d17a324582e295710666dd176c7.png)'
- en: Figure 8\. Explanation by Concepts. In the first phase, the concept layer is
    trained to predict the concepts associated with the input image. Then, given a
    test image, the model first predicts the concepts presented in the image which
    are subsequently processed by a fully connected layer to infer the final predictions.
    Simultaneously, it is possible to produce visualizations of the filters of the
    concept layer that highlight the relevant regions for each concept. Additionally,
    the contribution of each concept to the final prediction is obtained to perceive
    which concepts had more influence on the final decision.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 概念解释。在第一阶段，概念层被训练以预测与输入图像相关的概念。然后，给定测试图像，模型首先预测图像中呈现的概念，这些概念随后被全连接层处理以推断最终预测。同时，可以生成概念层过滤器的可视化图，突出显示每个概念的相关区域。此外，还可以获得每个概念对最终预测的贡献，以感知哪些概念对最终决策的影响更大。
- en: A different approach was introduced by Chen et al. ([24](#bib.bib24)) where
    they proposed the Concept Whitening (CW), a module that is inserted into a neural
    network, and that can replace the Batch Normalization layer so that each point
    in the latent space has an interpretation in terms of known concepts. In a similar
    line of research, the decision process was decomposed in a set of human-interpretable
    concepts, along with a visual interpretation of the spatial localization where
    these concepts are present in the image ([163](#bib.bib163); [106](#bib.bib106)).
    In particular, Patrício et al. ([106](#bib.bib106)) proposed an approach for enforcing
    the visual coherence of concept activations by using a hard attention mechanism
    to guide the activations of concept filters towards the locations where to which
    the concept is visually related to. This strategy has been shown to improve the
    visual explanation of concept-based approaches for skin lesion diagnosis.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人（[24](#bib.bib24)）提出了一种不同的方法，即概念白化（CW），这是一个插入到神经网络中的模块，可以替代批量归一化层，使得潜在空间中的每一点都能用已知概念来解释。在类似的研究方向上，决策过程被分解为一组人类可解释的概念，并且对这些概念在图像中存在的空间位置进行了可视化解释（[163](#bib.bib163);
    [106](#bib.bib106)）。特别是，Patrício等人（[106](#bib.bib106)）提出了一种通过使用硬注意机制来强制概念激活的视觉一致性的方法，以引导概念过滤器的激活到与概念视觉相关的位置。这一策略已被证明能改善基于概念的方法在皮肤病变诊断中的视觉解释。
- en: 'Ghorbani et al. ([41](#bib.bib41)) introduced ACE, which can automatically
    identify a set of high-level concepts in an unsupervised way. In a first phase,
    each image is segmented in multiple resolutions using the SLIC ([2](#bib.bib2))
    algorithm. Then, the segments are clustered by its similarity according to the
    euclidean distance, which is measured in the latent space. Each group of segments
    represents a different concept, labelled as pseudo-concepts. Lastly, to retain
    only the important concepts in each group, the TCAV ([60](#bib.bib60)) importance
    score was computed. The work of Fang et al. ([36](#bib.bib36)) built on the rationale
    of the ACE. The authors proposed the Visual Concept Mining (VCM) method to explain
    the decisions of an infectious keratitis classifier based on human-interpretable
    concepts. VCM encompasses two stages: (i) the proposed Potential Concept Generator
    module is responsible for identifying relevant concepts based on the segmentation
    of image patches according to the relevant regions highlighted on the produced
    saliency maps; (ii) the visual concept extractor module learns the similarity
    and diversity among the segmented image parts and groups them according to the
    DeepCluster ([21](#bib.bib21)) algorithm. The authors of VCM claimed that the
    concepts learned by their method were coherent with the medical annotations whilst
    being more diverse for the different classes, contrary to the ACE, which provides
    too broad concepts.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Ghorbani 等人 ([41](#bib.bib41)) 介绍了 ACE，该方法可以以无监督的方式自动识别一组高级概念。在第一阶段，使用 SLIC
    ([2](#bib.bib2)) 算法对每个图像进行多分辨率分割。然后，根据欧几里得距离对这些分割进行聚类，欧几里得距离在潜在空间中进行测量。每组分割代表一个不同的概念，标记为伪概念。最后，为了保留每组中的重要概念，计算了
    TCAV ([60](#bib.bib60)) 重要性得分。Fang 等人 ([36](#bib.bib36)) 的工作基于 ACE 的原理。作者提出了视觉概念挖掘（VCM）方法，以基于人类可解释的概念解释传染性角膜炎分类器的决策。VCM
    包括两个阶段：（i）所提议的潜在概念生成模块负责根据在生成的显著性图上突出显示的相关区域对图像块进行分割，识别相关概念；（ii）视觉概念提取模块学习分割图像部分之间的相似性和多样性，并根据
    DeepCluster ([21](#bib.bib21)) 算法对它们进行分组。VCM 的作者声称，他们的方法学习到的概念与医学注释一致，同时对不同类别更加多样，而
    ACE 提供的概念则过于宽泛。
- en: A disruptive approach was proposed by Sarkar et al. ([123](#bib.bib123)) that
    introduced an ante-hoc explainable model. A concept encoder on top of a backbone
    classification architecture is used for learning a set of human interpretable
    concepts providing thus an explanation for the classifier predictions. Additionally,
    the output of the concept encoder is passed to a decoder that reconstructs the
    input image, encouraging the model to capture the semantic features of the input
    image. Despite the method only reporting results for generic datasets, it would
    be interesting to extend this work for medical imaging datasets. Recently, following
    the philosophy of CBM, Yan et al. ([168](#bib.bib168)) proposed a method to improve
    the trustworthiness of skin cancer diagnosis by allowing doctors to intervene
    in the decisions of the trained models based on their knowledge and expertise.
    This human-in-the-loop framework allows for discovering and removing potential
    confounding behaviors of the model (e.g., artifacts or bias) within the dataset
    during the training phase. They concluded that modifying the output of the predicted
    concepts lead to a more accurate model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Sarkar 等人提出了一种颠覆性的方法 ([123](#bib.bib123))，该方法引入了一种 ante-hoc 可解释模型。在基础分类架构上方使用了概念编码器，用于学习一组人类可解释的概念，从而为分类器的预测提供解释。此外，概念编码器的输出被传递给解码器，解码器重建输入图像，鼓励模型捕捉输入图像的语义特征。尽管该方法仅报告了通用数据集的结果，但将此工作扩展到医学影像数据集是很有意义的。最近，遵循
    CBM 的理念，Yan 等人 ([168](#bib.bib168)) 提出了一个方法，通过允许医生根据他们的知识和专业知识干预训练模型的决策，从而提高皮肤癌诊断的可信度。这种人机协作框架允许在训练阶段发现和消除模型的潜在混淆行为（例如，伪影或偏差）。他们得出结论，修改预测概念的输出会导致模型更加准确。
- en: 5.4.1\. Discussion
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 讨论
- en: Although concept-based explanation methods are under-explored in medical imaging,
    they constitute a promising way of providing human-understandable explanations.
    The main advantage of these methods is that they are interpretable by design since
    the final predictions are derived from the learned concepts. However, the dependency
    on manual annotation of these concepts is the major limitation in concept-learning
    approaches. Recently, to overcome the annotation-dependency of the concepts, proposed
    methods built on unsupervised techniques to discover a set of pseudo-concepts
    related to the input image ([41](#bib.bib41); [36](#bib.bib36)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于概念的解释方法在医学成像中尚未得到充分探讨，但它们构成了一种有前景的提供人类可理解解释的方式。这些方法的主要优点在于它们在设计上是可解释的，因为最终的预测是基于学到的概念。然而，依赖于这些概念的手动注释是概念学习方法的主要限制。最近，为了克服对概念注释的依赖，提出的方法基于无监督技术来发现与输入图像相关的一组伪概念（[41](#bib.bib41);
    [36](#bib.bib36)）。
- en: 5.5\. Other Approaches
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 其他方法
- en: In contrast to the above-discussed approaches, some authors have investigated
    alternative strategies to confer interpretability to the models, including Bayesian
    Neural Networks (BNN) to quantify the uncertainty associated with the model prediction
    or using adversarial training to improve the quality of the generated explanations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述讨论的方法相反，一些作者研究了赋予模型可解释性的替代策略，包括使用贝叶斯神经网络（BNN）来量化模型预测相关的不确定性，或使用对抗训练来提高生成解释的质量。
- en: 5.5.1\. Bayesian Approaches
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1\. 贝叶斯方法
- en: Despite the success of CNN architectures, it is infeasible to quantify their
    uncertainty, given the deterministic nature associated with the internal parameters.
    Furthermore, CNNs are likely to overestimate the data when it is biased. In order
    to address these problems, Thiagarajan et al. ([145](#bib.bib145)) proposed the
    use of Bayesian CNNs (BCNN), which allow for the uncertainty estimation associated
    with the predictions. In particular, the uncertainty associated with the predictions
    of an Invasive Ductal Carcinoma (IDC) classifier on breast histopathology images
    was quantified. The examples characterized by a high value of uncertainty were
    projected into a lower-dimensional space using the t-SNE ([151](#bib.bib151))
    technique to facilitate data visualization and interpretation of the test data.
    Furthermore, the uncertainty allowed selecting the examples requiring human evaluation,
    which constitutes an interesting approach in the case of problems in the medical
    imaging domain. Similarly, Billah and Javed ([16](#bib.bib16)) relied on BCNNs
    to quantify the uncertainty associated to the predictions of a classifier model
    for the diagnosis of blood cancer. Recently, Gour and Jain ([43](#bib.bib43))
    proposed the UA-ConvNet, an uncertainty-aware CNN to detect COVID-19 from chest
    X-ray images and provide an estimation of the model uncertainty. For this, they
    used Monte Carlo dropout ([38](#bib.bib38)) to obtain a probability distribution
    of the model prediction.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN架构取得了成功，但由于内部参数的确定性特性，量化其不确定性是不切实际的。此外，当数据存在偏差时，CNN可能会过高估计数据。为了解决这些问题，Thiagarajan等人（[145](#bib.bib145)）提出使用贝叶斯CNN（BCNN），以便对与预测相关的不确定性进行估计。特别地，对乳腺组织病理图像中的侵袭性导管癌（IDC）分类器的预测相关的不确定性进行了量化。通过t-SNE（[151](#bib.bib151)）技术将具有高不确定性的示例投影到较低维度的空间中，以便于数据可视化和测试数据的解释。此外，不确定性允许选择需要人工评估的示例，这在医学成像领域的问题中是一个有趣的方法。同样，Billah和Javed（[16](#bib.bib16)）依赖BCNN量化用于血癌诊断的分类模型预测的不确定性。最近，Gour和Jain（[43](#bib.bib43)）提出了UA-ConvNet，一种不确定性感知CNN，用于从胸部X光图像中检测COVID-19，并提供模型不确定性的估计。为此，他们使用了Monte
    Carlo dropout（[38](#bib.bib38)）来获得模型预测的概率分布。
- en: 5.5.2\. Adversarial Training
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2\. 对抗训练
- en: In adversarial training, examples of the training set are augmented with adversarial
    perturbations at each training loop, allowing to increase the robustness of the
    model when provided with potential malicious examples ([11](#bib.bib11)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗训练中，训练集的示例在每次训练循环中都增加了对抗扰动，从而提高了模型在面对潜在恶意示例时的鲁棒性（[11](#bib.bib11)）。
- en: The first attempt on using adversarial training to improve interpretability
    in a medical imaging diagnosis task was made by Margeloiu et al. ([93](#bib.bib93)).
    They explored the use of adversarial training to improve the interpretability
    of CNNs, mainly when applied to diagnosing skin cancer. Specifically, the trained
    model was retrained from scratch using adversarial training with the Projected
    Gradient Descent (PGD) adversarial attack ([89](#bib.bib89)). The experiments
    on the dermatology dataset HAM10000 ([149](#bib.bib149)) showed that saliency
    maps of the robust model are significantly sharper and visually more coherent
    than those obtained from the standard trained model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Margeloiu 等人 ([93](#bib.bib93)) 首次尝试使用对抗训练来提高医学影像诊断任务中的解释性。他们探索了使用对抗训练来改善 CNNs
    的解释性，特别是在应用于皮肤癌诊断时。具体来说，训练模型使用 Projected Gradient Descent (PGD) 对抗攻击进行从头再训练 ([89](#bib.bib89))。在皮肤科数据集
    HAM10000 ([149](#bib.bib149)) 上的实验表明，鲁棒模型的显著性图比标准训练模型获得的显著性图显著更清晰且视觉上更一致。
- en: However, further research is needed since adversarial training is under-explored
    in medical imaging interpretability. Specifically, applying the above-referred
    findings to other datasets and network architectures becomes necessary to perceive
    the generalization capability of the methods. Furthermore, as stated by the authors
    in ([93](#bib.bib93)), the proposed method is not ready to be deployed in real-world
    scenarios due to the sensitivity of saliency methods to training noise, which
    can cause those methods to assign importance to artifacts available in the image
    (e.g., dark regions and irrelevant medical regions). Therefore, it is crucial
    to understand the limitations of adversarial training to improve interpretability
    in medical imaging diagnosis tasks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍需进一步研究，因为对抗训练在医学影像解释中的研究尚未深入。具体来说，需要将上述研究结果应用于其他数据集和网络架构，以感知方法的泛化能力。此外，正如作者在 ([93](#bib.bib93)) 中所述，由于显著性方法对训练噪声的敏感性，该方法尚未准备好在实际场景中部署，这可能导致这些方法将重要性分配给图像中的伪影（例如，黑暗区域和无关的医学区域）。因此，理解对抗训练的局限性对于改善医学影像诊断任务中的解释性至关重要。
- en: 5.5.3\. Discussion
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. 讨论
- en: The uncertainty estimation associated with a classifier’s predictions is helpful
    in the clinical workflow since clinicians can support their decisions based on
    the uncertainty value. Additionally, the use of adversarial training can be viewed
    as a method to improve the robustness of the model to adversarial attacks. As
    also demonstrated by Margeloiu et al. ([93](#bib.bib93)), the produced explanations
    by adversarially trained models seem to be more coherent and sharpening. Despite
    the under-exploitation of these strategies in medical imaging, these preliminary
    findings may encourage the emergence of methods adopting these alternative strategies
    as an additional layer to improve the models’ reliability and robustness.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类器预测相关的不确定性估计在临床工作流程中非常有用，因为临床医生可以根据不确定性值支持他们的决策。此外，对抗训练的使用可以被视为一种提高模型对对抗攻击鲁棒性的方法。正如
    Margeloiu 等人 ([93](#bib.bib93)) 也展示的那样，对抗训练模型生成的解释似乎更具连贯性和清晰度。尽管这些策略在医学影像中的利用尚不充分，但这些初步发现可能会鼓励采用这些替代策略作为改进模型可靠性和鲁棒性的附加层。
- en: 6\. Evaluation Metrics
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 评估指标
- en: 'Depending on the type of explanation modality (visual or textual), there are
    different ways to assess the quality of the generated explanations. We divide
    the evaluation metrics used in the literature into two categories: (i) evaluation
    metrics to assess the quality of visual explanations; and (ii) evaluation metrics
    for measuring the quality of textual explanations.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 根据解释方式的类型（视觉或文本），评估生成解释的质量有不同的方法。我们将文献中使用的评估指标分为两类：（i）评估视觉解释质量的指标；（ii）评估文本解释质量的指标。
- en: 6.1\. Evaluating the Quality of Visual Explanations
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 评估视觉解释的质量
- en: Evaluating the quality of model explanations remains an active area of research.
    A common approach for evaluating the model interpretability, specifically when
    applied to the medical domain, is to request an expert opinion from the clinicians
    and radiologists. However, this evaluation approach is time-consuming and depends
    on the level of experience of the clinicians  ([39](#bib.bib39); [150](#bib.bib150)).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型解释的质量仍然是一个活跃的研究领域。评估模型可解释性的一个常见方法，特别是在医学领域应用时，是请求临床医生和放射科医生的专家意见。然而，这种评估方法耗时且依赖于临床医生的经验水平
    ([39](#bib.bib39); [150](#bib.bib150))。
- en: Therefore, there has been attempts to propose evaluation metrics capable of
    objectively assessing the quality of the explanations. Samek et al. ([121](#bib.bib121))
    were precursors in contributing to the question of how to objectively evaluate
    the quality of heatmaps by introducing the area over the Most Relevant First (MoRF)
    perturbation curve (AOPC) measure. This measure is based on a region perturbation
    strategy that iteratively removes information from some regions of the input image
    according to its relevance to the class, allowing to perceive the performance
    decay of the model. The conducted experiments showed that a large AOPC value denotes
    high model sensitivity to the perturbations, indicating that the heatmap is actually
    informative.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经尝试提出能够客观评估解释质量的评价指标。Samek 等人 ([121](#bib.bib121)) 是在提出如何客观评估热图质量的问题方面的先驱，他们引入了最相关的第一（MoRF）扰动曲线下的区域（AOPC）测量。这一测量基于区域扰动策略，根据图像的类别相关性迭代地从输入图像的某些区域中移除信息，从而感知模型性能的下降。实验结果表明，较大的
    AOPC 值表示模型对扰动的敏感性较高，这表明热图实际上是信息丰富的。
- en: Inspired by the work in ([121](#bib.bib121)), Petsiuk et al. ([108](#bib.bib108))
    proposed two causal metrics, namely deletion and insertion, to evaluate the produced
    explanations for a black-box model. The deletion metric measures the degradation
    of the class probability, as important pixels of the image, derived from the saliency
    map, are removed. On the other hand, the insertion metric intends to measure the
    increase of the class probability, as pixels are inserted based on the generated
    saliency map.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 受 ([121](#bib.bib121)) 工作的启发，Petsiuk 等人 ([108](#bib.bib108)) 提出了两种因果指标，即删除和插入，用于评估黑箱模型生成的解释。删除指标测量类别概率的降级，因为从显著性图中得出的重要像素被移除。另一方面，插入指标旨在测量类别概率的增加，因为像素是基于生成的显著性图插入的。
- en: Later, Hooker et al. ([52](#bib.bib52)) argued that the modification-based metrics
    introduced by Petsiuk et al. ([108](#bib.bib108)) might not capture the actual
    reasoning behind the model’s degradation since this degradation could be due to
    artefacts introduced by the values used to replace the removed pixels. Thus, the
    authors proposed the RemOve And Retrain (ROAR) to evaluate interpretability methods
    by verifying how the accuracy of a retrained model degrades as important features
    are removed. The most important features are removed in certain regions of the
    image with a fixed uninformative value for each interpretability method. The main
    drawback of this metric is the required retraining of the model, which is computationally
    expensive.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，Hooker 等人 ([52](#bib.bib52)) 认为 Petsiuk 等人 ([108](#bib.bib108)) 提出的基于修改的指标可能无法捕捉模型降级背后的实际推理，因为这种降级可能是由于用来替换移除像素的值引入的伪影。因此，作者提出了
    RemOve And Retrain (ROAR) 方法，通过验证在移除重要特征时重新训练的模型的准确性如何下降来评估可解释性方法。最重要的特征在图像的某些区域中被移除，并为每种可解释性方法分配一个固定的无信息值。该指标的主要缺点是需要重新训练模型，这在计算上是昂贵的。
- en: Recently, Rio-Torto et al. ([115](#bib.bib115)) proposed the POMPOM (Percentage
    of Meaningful Pixels Outside the Mask) metric, which determines the number of
    meaningful pixels outside the region of interest in relation to the total number
    of pixels, to evaluate the quality of a given explanation. Similarly, Barnett
    et al. ([14](#bib.bib14)) introduced the Activation Precision evaluation metric,
    to quantify the proportion of relevant information from the “relevant region”
    used to classify the mass margin regarding the radiologist annotations. Despite
    the relevance of both metrics, they require manual annotations of the masks, which
    is time-consuming and may be difficult to obtain for some medical image datasets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Rio-Torto 等人 ([115](#bib.bib115)) 提出了 POMPOM（百分比的有意义像素超出掩模）度量标准，该标准确定了相对于总像素数量的区域外有意义像素的数量，以评估给定解释的质量。同样，Barnett
    等人 ([14](#bib.bib14)) 引入了激活精度评估度量，以量化用于分类质量边界的“相关区域”中的相关信息的比例，涉及放射科医生的注释。尽管这两种度量标准都很重要，但它们需要手动标注掩模，这既耗时又可能难以获取某些医学图像数据集。
- en: In spite of the valuable contribution of these proposed evaluation metrics,
    we believe that a new evaluation method for model interpretability can be developed
    with the aid of the Bayesian Neural Networks (BNN) characteristics. Considering
    that the weights of the BNN follow a probability distribution, we can sample different
    “models” from the posterior distribution and generate an arbitrary number of explanations
    for a given example ([20](#bib.bib20)). Then, using intersection or union operations
    over those generated explanations seems to be an interesting direction to estimate
    whether most of the explanations highlight the same Region Of Interest (ROI).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些提出的评估度量标准具有重要贡献，我们相信可以借助贝叶斯神经网络（BNN）的特性开发一种新的模型可解释性评估方法。考虑到 BNN 的权重遵循概率分布，我们可以从后验分布中抽样不同的“模型”，并为给定示例生成任意数量的解释
    ([20](#bib.bib20))。然后，利用这些生成的解释进行交集或并集操作似乎是估计大多数解释是否突出相同感兴趣区域（ROI）的一个有趣方向。
- en: 6.2\. Evaluating the Quality of Textual Explanations
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 评估文本解释的质量
- en: In this category, the metrics are used to measure the quality of the generated
    text, and are originated from generic NLP tasks. The most used metrics in the
    reviewed papers were BLEU ([104](#bib.bib104)), ROUGE-L ([76](#bib.bib76)), METEOR ([71](#bib.bib71))
    and CIDEr ([155](#bib.bib155)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一类别中，度量标准用于衡量生成文本的质量，起源于通用 NLP 任务。回顾的论文中最常用的度量标准有 BLEU ([104](#bib.bib104))、ROUGE-L
    ([76](#bib.bib76))、METEOR ([71](#bib.bib71)) 和 CIDEr ([155](#bib.bib155))。
- en: 'BLEU (Bilingual Evaluation Understudy) score is the most common evaluation
    metric in NLP. In simple terms, it compares n-gram matches between the generated
    sentence (also known as candidate sentence) and the ground-truth sentence (also
    known as reference sentence), expressed in modified precision²²2Takes into consideration
    the maximum frequency of each n-gram in the reference sentence (clipped count).
    The modified precision is then calculated by summing the clipped counts of the
    candidate sentence divided by the total number of candidate n-grams. for each
    n-gram. The BLEU metric has N variations (BLEU-N), typically N $\in\{1,2,3,4\}$,
    with respect to the considered n-grams. The value of BLEU score ranges from 0
    to 1, which means that the closer to 1, the better the translation. Formally,
    BLEU score is calculated according to the following formulation:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU（双语评价替代指标）分数是 NLP 中最常用的评估度量。简单来说，它比较生成句子（也称为候选句子）和真实句子（也称为参考句子）之间的 n-gram
    匹配，表示为修改精度²²2考虑每个 n-gram 在参考句子中的最大频率（截断计数）。然后通过将候选句子的截断计数总和除以候选 n-grams 的总数来计算修改精度。BLEU
    度量有 N 种变体（BLEU-N），通常 N $\in\{1,2,3,4\}$，根据所考虑的 n-grams。BLEU 分数的值范围从 0 到 1，这意味着接近
    1 的值表示翻译质量更好。正式地，BLEU 分数按照以下公式计算：
- en: '| (1) |  | $BLEU=BP.\exp{\sum_{n=1}^{N}w_{n}\log{p_{n}}},$ |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $BLEU=BP.\exp{\sum_{n=1}^{N}w_{n}\log{p_{n}}},$ |  |'
- en: 'where $p_{n}$ is the modified precision for $n$-gram, $w_{n}$ is a weight,
    ranging from $0$ and $1$, and $\sum_{n=1}^{N}w_{n}=1$, i.e., if $N=4$, $w_{n}=1/N$,
    and BP is the brevity penalty that penalizes short generated sentences, denoted
    as:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{n}$ 是 $n$-gram 的修改精度，$w_{n}$ 是一个权重，范围从 $0$ 到 $1$，且 $\sum_{n=1}^{N}w_{n}=1$，即，如果
    $N=4$，则 $w_{n}=1/N$，BP 是简洁惩罚，用于惩罚生成的短句，如下所示：
- en: '| (2) |  | $BP=\left\{\begin{matrix}1&amp;\textrm{if}\;\;c>r\\ \exp(1-\frac{1}{c})&amp;\textrm{if}\;\;c\leq
    r\end{matrix}\right.,$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $BP=\left\{\begin{matrix}1&amp;\textrm{if}\;\;c>r\\ \exp(1-\frac{1}{c})&amp;\textrm{if}\;\;c\leq
    r\end{matrix}\right.,$ |  |'
- en: where $c$ is the length of candidate translation, and $r$ is the reference corpus
    length.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$ 是候选翻译的长度，$r$ 是参考语料库的长度。
- en: 'The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric is actually
    a set of metrics. We focus on the ROUGE-L variant as it was the prevalent metric
    in the most of the reviewed methods. ROUGE-L measures the Longest Common Subsequence
    (LCS) between the generated sentence and the ground-truth sentence both in terms
    of precision and recall. This means that if both sentences share a long sub-sentence,
    the similarity between the two sentences is expected to be high. The final value
    of ROUGE-L is given in F1 score, as formally described bellow (Eq. [3](#S6.E3
    "In 6.2\. Evaluating the Quality of Textual Explanations ‣ 6\. Evaluation Metrics
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE（**面向召回的生成评估**）度量实际上是一组度量。我们关注 ROUGE-L 变体，因为它是大多数审查方法中普遍使用的度量。ROUGE-L 测量生成句子和真实句子之间的最长公共子序列（LCS），包括精度和召回率。这意味着如果两个句子共享一个长的子句，那么两个句子之间的相似度预计会很高。ROUGE-L
    的最终值以 F1 分数给出，如下所示（公式 [3](#S6.E3 "在 6.2\. 评估文本解释的质量 ‣ 6\. 评估指标 ‣ 医学图像分类中的可解释深度学习方法：一项调查")）：
- en: '| (3) |  | $ROUGE-L_{F1}=2\cdot\frac{Precision\cdot Recall}{Precision+Recall},$
    |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $ROUGE-L_{F1}=2\cdot\frac{Precision\cdot Recall}{Precision+Recall},$
    |  |'
- en: where $Precision=\frac{LCS(c,r)}{m}$ and $Recall=\frac{LCS(c,r)}{n}$, with $LCS(c,r)$
    denoting the Longest Common Subsequence between the candidate sentence (c) and
    the reference sentence (r), $m$ is the number of $n$-grams in the reference sentence,
    and $n$ the number of $n$-grams in the candidate sentence.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Precision=\frac{LCS(c,r)}{m}$ 和 $Recall=\frac{LCS(c,r)}{n}$，$LCS(c,r)$ 表示候选句子（c）和参考句子（r）之间的最长公共子序列，$m$
    是参考句子中的 $n$-grams 数量，$n$ 是候选句子中的 $n$-grams 数量。
- en: Differently from the two above-discussed metrics, METEOR (Metric for Evaluation
    of Translation with Explicit Ordering) gives attention to the position of the
    words in the sentence by including a chunk penalty that weights the final score.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述讨论的两种度量标准不同，METEOR（**具有明确排序的翻译评估度量**）通过包括一个加权最终得分的块惩罚来关注句子中单词的位置。
- en: '| (4) |  | $METEOR=F_{mean}\cdot(1-Penalty)$ |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $METEOR=F_{mean}\cdot(1-Penalty)$ |  |'
- en: where $Penalty=0.5\cdot(\frac{m}{n})$, where $m$ is the number of chunks and
    $n$ is the total number of unigram matches, and $F_{mean}=\frac{10PR}{R+9P}$.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Penalty=0.5\cdot(\frac{m}{n})$，其中 $m$ 是块的数量，$n$ 是单词匹配的总数，$F_{mean}=\frac{10PR}{R+9P}$。
- en: Alternatively, CIDEr (Consensus-Based Image Description Evaluation) is an evaluation
    metric that uses the Term Frequency Inverse Document Frequency (TF-IDF) ([116](#bib.bib116))
    for weighting each $n$-gram. The intuition behind CIDEr is that $n$-grams that
    frequently appear in the reference sentences are less likely to be informative,
    and hence they have a lower weight using the IDF term. CIDEr[n], $n=\{1,2,3,4\}$,
    is the average cosine similarity between the candidate sentence and the reference
    sentences, considering both precision and recall.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种度量标准，CIDEr（**基于共识的图像描述评估**），是一种使用**词频-逆文档频率**（TF-IDF） ([116](#bib.bib116))
    对每个 $n$-gram 进行加权的评估度量。CIDEr 的直觉是，频繁出现的 $n$-grams 在参考句子中不太可能具有信息性，因此它们使用 IDF 术语的权重较低。CIDEr[n]，$n=\{1,2,3,4\}$，是候选句子与参考句子之间的平均余弦相似度，同时考虑了精度和召回率。
- en: 'TF-IDF weighting $g_{k}(s_{ij})$ for each $n$-gram $w_{k}$ is computed as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 加权 $g_{k}(s_{ij})$ 对每个 $n$-gram $w_{k}$ 的计算如下：
- en: '| (5) |  | $g_{k}(s_{ij})=\frac{h_{k}(s_{ij})}{\sum_{w_{l}\in\omega}h_{l}(s_{ij})}\log(\frac{\left&#124;I\right&#124;}{\sum_{I_{p}\in
    I}}\min(1,\sum_{q}h_{k}(s_{pq}))),$ |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $g_{k}(s_{ij})=\frac{h_{k}(s_{ij})}{\sum_{w_{l}\in\omega}h_{l}(s_{ij})}\log(\frac{\left&#124;I\right&#124;}{\sum_{I_{p}\in
    I}}\min(1,\sum_{q}h_{k}(s_{pq}))),$ |  |'
- en: where $h_{k}(s_{ij})$ is the number of times an $n$-gram occurs in a reference
    sentence $s_{ij}$, and $h_{k}(c_{i})$ for the candidate sentence $c_{i}$, $\omega$
    is the vocabulary of all $n$-grams, and $I$ is the set of all images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{k}(s_{ij})$ 是 $n$-gram 在参考句子 $s_{ij}$ 中出现的次数，$h_{k}(c_{i})$ 是候选句子 $c_{i}$
    中的次数，$\omega$ 是所有 $n$-grams 的词汇表，$I$ 是所有图像的集合。
- en: '| (6) |  | $CIDEr_{n}(c_{i},S_{i})=\frac{1}{m}\sum_{j}\frac{g^{n}(c_{i})\cdot
    g^{n}(s_{ij})}{\&#124;g^{n}(c_{i})\&#124;\&#124;g^{n}(s_{ij})\&#124;}.$ |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $CIDEr_{n}(c_{i},S_{i})=\frac{1}{m}\sum_{j}\frac{g^{n}(c_{i})\cdot
    g^{n}(s_{ij})}{\&#124;g^{n}(c_{i})\&#124;\&#124;g^{n}(s_{ij})\&#124;}.$ |  |'
- en: 'The final CIDEr score is the weighted average of CIDEr[n]:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的CIDEr分数是CIDEr[n]的加权平均值：
- en: '| (7) |  | $CIDEr(c_{i},S_{i})=\sum_{n=1}^{N}w_{k}CIDEr_{n}(c_{i},S_{i}),$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $CIDEr(c_{i},S_{i})=\sum_{n=1}^{N}w_{k}CIDEr_{n}(c_{i},S_{i}),$
    |  |'
- en: where $w_{n}=1/N,N=4$.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{n}=1/N,N=4$。
- en: 7\. Performance Comparison
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 性能比较
- en: 'In the previous sections, we reviewed several works focused on providing explanations
    to the output of automated medical diagnosis. At the end of the review, an important
    question arises: “What is the best approach?”. Unfortunately, in many cases, there
    is no trivial answer, since most methods adopt different evaluation metrics making
    performance comparison with other competing methods unfeasible. To add up to the
    question, we compare the performance of some of the methods reviewed. In order
    to find common ground for a fair comparison of the methods, only those methods
    that considered the same dataset for evaluation purposes were selected. As presented
    in table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), IU Chest X-ray ([32](#bib.bib32))
    is the most used dataset among the reviewed methods. As such, we verified whether
    the methods that were evaluated on the IU Chest X-ray ([32](#bib.bib32)) used
    the same evaluation metrics. Additionally, and since MIMIC-CXR ([56](#bib.bib56))
    dataset provides an official training-test partition, we include some methods
    that report results on this dataset under the same evaluation metrics. In this
    way, a comparison of the performance between these methods was carried out. Table
    [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning Methods
    in Medical Image Classification: A Survey") conveys the results in terms of a
    set of NLP evaluation metrics (BLEU score, ROUGE, METEOR, and CIDEr) for each
    of the selected methods.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们回顾了几项工作，这些工作专注于提供自动医学诊断输出的解释。在回顾结束时，出现了一个重要的问题：“什么是最佳方法？”不幸的是，在许多情况下，没有简单的答案，因为大多数方法采用不同的评估指标，这使得与其他竞争方法进行性能比较变得不可行。为了解决这个问题，我们比较了一些已回顾方法的性能。为了找到公平比较方法的共同基础，我们仅选择了那些考虑了相同数据集进行评估的方法。如表[3](#S7.T3
    "Table 3 ‣ 7\. 性能比较 ‣ 医学图像分类中的可解释深度学习方法：综述")所示，IU胸部X光数据集 ([32](#bib.bib32)) 是回顾中使用最广泛的数据集。因此，我们验证了在IU胸部X光数据集
    ([32](#bib.bib32)) 上评估的方法是否使用了相同的评估指标。此外，由于MIMIC-CXR ([56](#bib.bib56)) 数据集提供了官方的训练-测试划分，我们包括了一些在该数据集上报告结果并使用相同评估指标的方法。通过这种方式，我们对这些方法之间的性能进行了比较。表[3](#S7.T3
    "Table 3 ‣ 7\. 性能比较 ‣ 医学图像分类中的可解释深度学习方法：综述") 以一组NLP评估指标（BLEU分数、ROUGE、METEOR和CIDEr）呈现了每种选择的方法的结果。
- en: Table 3\. Performance of the selected methods that generate textual explanations
    for interpreting the decision of a classifier. Spaces marked with a “-” mean no
    value is available for the respective metric. ¹ Results were taken from the work
    of Liu et al. ([83](#bib.bib83)). The methods evaluated on the IU Chest X-ray
    dataset are marked with a symbol ($\star$, $\dagger$, $\circ$, $\ddagger$), meaning
    that the methods with each symbol used the same training-validation-test split.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 选择的方法在解释分类器决策方面生成文本说明的性能。标记为“–”的空格表示相应指标没有值。¹ 结果摘自刘等人的工作 ([83](#bib.bib83))。在IU胸部X光数据集上评估的方法用符号
    ($\star$, $\dagger$, $\circ$, $\ddagger$) 标记，意味着每个符号标记的方法使用了相同的训练-验证-测试划分。
- en: '|  | Model | Year | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | METEOR |
    CIDEr |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 年份 | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | METEOR | CIDEr
    |'
- en: '|  | IU Chest X-Ray |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | IU胸部X光 |'
- en: '| Find. + Impress. | Jing et al. ([55](#bib.bib55))^† | 2017 | ${0.517}$ |
    ${0.386}$ | 0.306 | 0.247 | ${0.447}$ | 0.217 | $0.327$ |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 查找 + 印象 | Jing et al. ([55](#bib.bib55))^† | 2017 | ${0.517}$ | ${0.386}$
    | 0.306 | 0.247 | ${0.447}$ | 0.217 | $0.327$ |'
- en: '| Singh et al. ([137](#bib.bib137))^∘ | 2019 | $0.374$ | $0.224$ | $0.153$
    | $0.110$ | $0.308$ | $0.164$ | ${0.360}$ |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Singh et al. ([137](#bib.bib137))^∘ | 2019 | $0.374$ | $0.224$ | $0.153$
    | $0.110$ | $0.308$ | $0.164$ | ${0.360}$ |'
- en: '| HRNN ([170](#bib.bib170))^† | 2019 | $0.445$ | $0.292$ | $0.201$ | $0.154$
    | $0.344$ | $0.175$ | $0.342$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| HRNN ([170](#bib.bib170))^† | 2019 | $0.445$ | $0.292$ | $0.201$ | $0.154$
    | $0.344$ | $0.175$ | $0.342$ |'
- en: '| Selivanov et al. ([127](#bib.bib127))^† | 2023 | 0.520 | 0.390 | $0.296$
    | $0.235$ | 0.450 | $-$ | 0.701 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Selivanov et al. ([127](#bib.bib127))^† | 2023 | 0.520 | 0.390 | $0.296$
    | $0.235$ | 0.450 | $-$ | 0.701 |'
- en: '| Findings | HRGR-Agent ([74](#bib.bib74))^⋆ | 2018 | $0.438$ | $0.298$ | $0.208$
    | $0.151$ | $0.322$ | $-$ | $0.343$ |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 发现 | HRGR-Agent ([74](#bib.bib74))^⋆ | 2018 | $0.438$ | $0.298$ | $0.208$
    | $0.151$ | $0.322$ | $-$ | $0.343$ |'
- en: '| TieNet¹ ([160](#bib.bib160))^⋆ | 2018 | $0.330$ | $0.194$ | $0.124$ | $0.081$
    | $0.311$ | $-$ | $1.334$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| TieNet¹ ([160](#bib.bib160))^⋆ | 2018 | $0.330$ | $0.194$ | $0.124$ | $0.081$
    | $0.311$ | $-$ | $1.334$ |'
- en: '| Liu et al. ([83](#bib.bib83))^⋆ | 2019 | $0.369$ | $0.246$ | $0.171$ | $0.115$
    | $0.359$ | $-$ | 1.490 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. ([83](#bib.bib83))^⋆ | 2019 | $0.369$ | $0.246$ | $0.171$ | $0.115$
    | $0.359$ | $-$ | 1.490 |'
- en: '| R2Gen ([25](#bib.bib25))^⋆ | 2020 | ${0.470}$ | ${0.304}$ | ${0.219}$ | ${0.165}$
    | ${0.371}$ | ${0.187}$ | $-$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| R2Gen ([25](#bib.bib25))^⋆ | 2020 | ${0.470}$ | ${0.304}$ | ${0.219}$ | ${0.165}$
    | ${0.371}$ | ${0.187}$ | $-$ |'
- en: '| PPKED ([81](#bib.bib81))^⋆ | 2021 | $0.483$ | $0.315$ | $0.224$ | $0.168$
    | $0.376$ | $-$ | $0.351$ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| PPKED ([81](#bib.bib81))^⋆ | 2021 | $0.483$ | $0.315$ | $0.224$ | $0.168$
    | $0.376$ | $-$ | $0.351$ |'
- en: '| CA ([82](#bib.bib82))^⋆ | 2021 | $0.492$ | $0.314$ | $0.222$ | $0.169$ |
    $0.381$ | $0.193$ | $-$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| CA ([82](#bib.bib82))^⋆ | 2021 | $0.492$ | $0.314$ | $0.222$ | $0.169$ |
    $0.381$ | $0.193$ | $-$ |'
- en: '| ICT ([175](#bib.bib175))^⋆ | 2023 | ${0.503}$ | 0.341 | 0.246 | ${0.186}$
    | ${0.390}$ | 0.208 | $-$ |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ICT ([175](#bib.bib175))^⋆ | 2023 | ${0.503}$ | 0.341 | 0.246 | ${0.186}$
    | ${0.390}$ | 0.208 | $-$ |'
- en: '| METransformer ([161](#bib.bib161))^⋆ | 2023 | $0.483$ | $0.322$ | $0.228$
    | $0.172$ | $0.380$ | $0.192$ | $0.435$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| METransformer ([161](#bib.bib161))^⋆ | 2023 | $0.483$ | $0.322$ | $0.228$
    | $0.172$ | $0.380$ | $0.192$ | $0.435$ |'
- en: '| VLCI ([23](#bib.bib23))^⋆ | 2023 | 0.505 | $0.334$ | $0.245$ | 0.189 | $0.397$
    | $0.204$ | $0.456$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| VLCI ([23](#bib.bib23))^⋆ | 2023 | 0.505 | $0.334$ | $0.245$ | 0.189 | $0.397$
    | $0.204$ | $0.456$ |'
- en: '| Yang et al. ([169](#bib.bib169))^⋆ | 2023 | ${0.497}$ | $0.319$ | $0.230$
    | $0.174$ | 0.399 | $-$ | $0.407$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. ([169](#bib.bib169))^⋆ | 2023 | ${0.497}$ | $0.319$ | $0.230$
    | $0.174$ | 0.399 | $-$ | $0.407$ |'
- en: '|  | MIMIC-CXR |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | MIMIC-CXR |'
- en: '|  | TieNet¹ ([160](#bib.bib160))^‡ | 2018 | $0.332$ | $0.212$ | $0.142$ |
    $0.095$ | $0.296$ | $-$ | $1.004$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | TieNet¹ ([160](#bib.bib160))^‡ | 2018 | $0.332$ | $0.212$ | $0.142$ |
    $0.095$ | $0.296$ | $-$ | $1.004$ |'
- en: '|  | Liu et al. ([83](#bib.bib83))^‡ | 2019 | $0.352$ | ${0.223}$ | ${0.153}$
    | ${0.104}$ | ${0.307}$ | $-$ | ${1.153}$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | Liu et al. ([83](#bib.bib83))^‡ | 2019 | $0.352$ | ${0.223}$ | ${0.153}$
    | ${0.104}$ | ${0.307}$ | $-$ | ${1.153}$ |'
- en: '|  | R2Gen ([25](#bib.bib25)) | 2020 | ${0.353}$ | $0.218$ | $0.145$ | $0.103$
    | $0.277$ | $0.142$ | $-$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | R2Gen ([25](#bib.bib25)) | 2020 | ${0.353}$ | $0.218$ | $0.145$ | $0.103$
    | $0.277$ | $0.142$ | $-$ |'
- en: '|  | CA ([82](#bib.bib82)) | 2021 | $0.350$ | $0.219$ | $0.152$ | $0.109$ |
    $0.283$ | ${0.151}$ | $-$ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | CA ([82](#bib.bib82)) | 2021 | $0.350$ | $0.219$ | $0.152$ | $0.109$ |
    $0.283$ | ${0.151}$ | $-$ |'
- en: '|  | PPKED ([81](#bib.bib81)) | 2021 | $0.360$ | $0.224$ | $0.149$ | $0.106$
    | $0.284$ | $0.149$ | $-$ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | PPKED ([81](#bib.bib81)) | 2021 | $0.360$ | $0.224$ | $0.149$ | $0.106$
    | $0.284$ | $0.149$ | $-$ |'
- en: '|  | MSAT ([162](#bib.bib162)) | 2022 | $0.373$ | $0.235$ | $0.162$ | $0.120$
    | $0.282$ | $0.143$ | $0.299$ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | MSAT ([162](#bib.bib162)) | 2022 | $0.373$ | $0.235$ | $0.162$ | $0.120$
    | $0.282$ | $0.143$ | $0.299$ |'
- en: '|  | ICT ([175](#bib.bib175)) | 2023 | ${0.376}$ | ${0.233}$ | ${0.157}$ |
    ${0.113}$ | $0.276$ | $0.144$ | $-$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | ICT ([175](#bib.bib175)) | 2023 | ${0.376}$ | ${0.233}$ | ${0.157}$ |
    ${0.113}$ | $0.276$ | $0.144$ | $-$ |'
- en: '|  | METransformer ([161](#bib.bib161)) | 2023 | $0.386$ | ${0.250}$ | ${0.169}$
    | ${0.124}$ | $0.291$ | 0.152 | $0.362$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | METransformer ([161](#bib.bib161)) | 2023 | $0.386$ | ${0.250}$ | ${0.169}$
    | ${0.124}$ | $0.291$ | 0.152 | $0.362$ |'
- en: '|  | Yang et al. ([169](#bib.bib169)) | 2023 | $0.386$ | $0.237$ | $0.157$
    | $0.111$ | $0.274$ | $-$ | $0.111$ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | Yang et al. ([169](#bib.bib169)) | 2023 | $0.386$ | $0.237$ | $0.157$
    | $0.111$ | $0.274$ | $-$ | $0.111$ |'
- en: '|  | VLCI ([23](#bib.bib23)) | 2023 | $0.400$ | $0.245$ | $0.165$ | $0.119$
    | $0.280$ | $0.150$ | $0.190$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | VLCI ([23](#bib.bib23)) | 2023 | $0.400$ | $0.245$ | $0.165$ | $0.119$
    | $0.280$ | $0.150$ | $0.190$ |'
- en: '|  | Selivanov et al. ([127](#bib.bib127)) | 2023 | 0.725 | 0.626 | 0.505 |
    0.418 | 0.480 | $-$ | 1.989 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | Selivanov et al. ([127](#bib.bib127)) | 2023 | 0.725 | 0.626 | 0.505 |
    0.418 | 0.480 | $-$ | 1.989 |'
- en: 7.1\. Results Discussion
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 结果讨论
- en: 'It is worth noticing that all the presented results in Table [3](#S7.T3 "Table
    3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey") were taken from the original paper of each method,
    except for the TieNet ([160](#bib.bib160)) method, whose results were taken from
    the work of Liu et al. ([83](#bib.bib83)), since in the original paper the authors
    only provide results in ChestX-ray14 using BLEU, METEOR, and ROUGE-L.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，表格 [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey") 中呈现的所有结果均来自每种方法的原始论文，除了
    TieNet ([160](#bib.bib160)) 方法，其结果取自 Liu 等人的工作 ([83](#bib.bib83))，因为在原始论文中，作者仅提供了使用
    BLEU、METEOR 和 ROUGE-L 在 ChestX-ray14 中的结果。'
- en: 'Regarding the methods that considered the “findings+impression” section from
    the radiology report to generate the free-text report, and as evidenced by the
    results in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey"), the model proposed
    by Selivanov et al. ([127](#bib.bib127)) proved to be more accurate in terms of
    BLUE-1, BLEU-2 and ROUGE-L. The preprocessing and squeezing approaches used for
    clinical records jointly with the combination of two large language models (Show-Attend-Tell
    (SAT) ([166](#bib.bib166)) and Generative Pretrained Transformer (GPT-3) ([18](#bib.bib18)))
    can explain the performance improvement. Moreover, generated reports are accompanied
    by 2D heatmaps that localize each pathology on the input scans. Conversely, the
    method of Jing et al. ([55](#bib.bib55)) demonstrates superior performance in
    terms of BLEU-3, BLEU-4 and METEOR, which can be explained by the co-attention
    mechanism adopted by the authors. As the method of Singh et al. ([137](#bib.bib137))
    does not follow the same data partition, it is not strictly comparable to other
    methods.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '关于考虑“发现+印象”部分的生成自由文本报告的方法，正如表[3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")中结果所示，Selivanov
    等人 ([127](#bib.bib127)) 提出的模型在BLUE-1、BLEU-2和ROUGE-L方面更为准确。用于临床记录的预处理和压缩方法以及两个大型语言模型（Show-Attend-Tell
    (SAT) ([166](#bib.bib166)) 和 Generative Pretrained Transformer (GPT-3) ([18](#bib.bib18)))
    的组合可以解释性能的提升。此外，生成的报告还附带2D热图，用于定位输入扫描中的每个病变。相反，Jing 等人 ([55](#bib.bib55)) 的方法在BLEU-3、BLEU-4和METEOR指标上表现优越，这可以通过作者采用的共同注意机制来解释。由于Singh
    等人 ([137](#bib.bib137)) 的方法不遵循相同的数据划分，因此与其他方法不完全可比。'
- en: Among the selected methods that only consider the “findings” section, ICT ([175](#bib.bib175))
    demonstrates superior performance on BLEU-2, BLEU-3 and METEOR metrics. Their
    transformer-based model incorporates two modules responsible for capturing inter-intra
    features of medical reports as auxiliary information and subsequently calibrating
    the report generation process by integrating that information. This combination
    results in a performance boost in the report generation model and improves the
    quality of medical diagnosis. In contrast, with regard to the BLEU-1 score, VLCI ([23](#bib.bib23))
    exhibits a superior performance, comparable to ICT ([175](#bib.bib175)), which
    could be justified by the cross-modal causal intervention strategy employed by
    the authors to mitigate spurious correlations from visual and linguistic confounders.
    Furthermore, the model proposed by Liu et al. ([83](#bib.bib83)) adopted a fine-tuning
    procedure that uses reinforcement learning via CIDEr to ensure more coherent report
    generation, which could justify the performance in the CIDEr metric.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅考虑“发现”部分的选定方法中，ICT ([175](#bib.bib175)) 在BLEU-2、BLEU-3和METEOR指标上表现优越。他们的基于变换器的模型结合了两个模块，负责捕捉医疗报告的内部和外部特征作为辅助信息，并通过整合这些信息来校准报告生成过程。这种组合在报告生成模型中带来了性能提升，并改善了医疗诊断质量。相比之下，在BLEU-1分数方面，VLCI
    ([23](#bib.bib23)) 表现更佳，与ICT ([175](#bib.bib175)) 可媲美，这可以通过作者采用的跨模态因果干预策略来解释，以减少来自视觉和语言混淆因素的虚假相关性。此外，刘等人
    ([83](#bib.bib83)) 提出的模型采用了通过CIDEr进行强化学习的微调程序，以确保更连贯的报告生成，这也能解释在CIDEr指标上的表现。
- en: 'Regarding the methods that report results for the MIMIC-CXR dataset, it is
    worth noting that only TieNet ([160](#bib.bib160)) and the approach by Liu et
    al. [82] have followed a distinct data partition strategy. Consequently, these
    methods are included here primarily for reference, as their divergent data partitioning
    makes direct comparisons with other methods infeasible. On the contrary, the model
    proposed by Selivanov et al. ([127](#bib.bib127)) distinguishes itself from the
    others by exhibiting superior performance across all metrics except for METEOR
    (which was not reported by the authors). Their approach leveraged the capabilities
    of two large language models, SAT and GPT-3, trained on large text corpora. This
    fusion of language models significantly improved standard text generation scores,
    as shown in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '关于报告MIMIC-CXR数据集结果的方法，值得注意的是，只有TieNet（[160](#bib.bib160)）和刘等人[82]采用了不同的数据划分策略。因此，这些方法主要作为参考，因为它们不同的数据划分使得与其他方法的直接比较不可行。相反，Selivanov等人提出的模型（[127](#bib.bib127)）在所有指标上都表现出优越的性能，除了METEOR（作者未报告）。他们的方法利用了两个大型语言模型，SAT和GPT-3，这些模型在大型文本语料库上进行了训练。这种语言模型的融合显著提高了标准文本生成分数，如表格[3](#S7.T3
    "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey")所示。'
- en: 'Overall, it is noticeable in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")
    that the use of transformer-based models ([81](#bib.bib81); [82](#bib.bib82);
    [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)) with additional mechanisms
    to capture complex and relevant features proved to be effective in improving the
    performance of the generated reports, as observed in the results obtained in the
    MIMIC-CXR dataset.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '总体来说，在表格[3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey")中可以明显看出，使用基于变换器的模型（[81](#bib.bib81);
    [82](#bib.bib82); [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)）以及额外机制来捕捉复杂且相关的特征，确实有效地提高了生成报告的性能，这在MIMIC-CXR数据集的结果中得到了验证。'
- en: 8\. General Discussion
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 一般讨论
- en: Although XAI is a relatively recent research field, its constant growth is undeniable,
    with applications in many areas, particularly in the medical domain. However,
    despite the advances and the efforts made toward developing interpretable deep
    learning-based models for medical imaging, there are open issues that require
    more research and advances in this growing field. This section identifies open
    challenges in the literature and potential research paths to further improve the
    trustworthiness of provided explanations and foster the adoption of deep learning-based
    systems into clinical routine.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管XAI是一个相对较新的研究领域，其不断增长是不容否认的，特别是在医疗领域。然而，尽管在为医疗影像开发可解释的深度学习模型方面取得了进展并付出了努力，但仍然存在一些未解决的问题，需要更多的研究和进展来推动这个不断增长的领域。本节识别了文献中的开放挑战和潜在的研究路径，以进一步提高解释的可信度并促进深度学习系统在临床常规中的应用。
- en: Based on reviewed literature, it can be concluded that the go-to method for
    model interpretation in medical imaging is producing saliency maps, using classical
    techniques, such as Grad-CAM, Integrated Gradients, or LRP. However, as evidenced
    by some authors, saliency maps can be unreliable and fragile ([118](#bib.bib118);
    [3](#bib.bib3)), as they often highlight irrelevant regions in the images. In
    addition, it is frequent that very similar explanations are given for different
    classes, and often none of them are useful explanations ([118](#bib.bib118)).
    Thus, the development of inherently interpretable models has been a line of research
    with promising results in the medical imaging domain. Although these methods remain
    largely unexplored in medical imaging, future research will undoubtedly be devoted
    to develop inherently interpretable models. These models have the primary benefit
    of providing their own explanations, which contributes to their transparency and
    fidelity, increasing the chances of being adopted into the clinical routine.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据回顾的文献，可以得出结论，医学影像模型解释的首选方法是生成显著性图，使用经典技术，如Grad-CAM、集成梯度或LRP。然而，正如一些作者所证明的，显著性图可能不可靠且脆弱（[118](#bib.bib118);
    [3](#bib.bib3)），因为它们通常突出图像中不相关的区域。此外，给出的解释经常对于不同的类别非常相似，并且往往没有一个是有用的解释（[118](#bib.bib118)）。因此，开发固有可解释的模型已经成为医学影像领域的一个有前景的研究方向。尽管这些方法在医学影像中仍然基本未被探索，但未来的研究无疑将致力于开发固有可解释的模型。这些模型的主要好处在于提供自身的解释，这有助于提高其透明度和可信度，增加被纳入临床常规的可能性。
- en: On the other hand, different end-users could have different backgrounds and
    preferences at interpreting the explanation, which can generate some contradictory
    opinions. This fostered the use of textual explanations, which are preferred over
    visual explanations by some authors ([39](#bib.bib39)) since they are inherently
    understandable by humans ([152](#bib.bib152)). Since then, methods that generate
    textual descriptions for explaining a prediction and multimodal methods that combine
    visual and textual explanations have emerged. However, generating free-text reports
    is deemed a challenging task since the radiologist reports are technically structured,
    and the most used language models based on RNNs have some limitations in generating
    long texts, as stated by Pascanu et al. ([105](#bib.bib105)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，不同的最终用户可能在解释时具有不同的背景和偏好，这可能产生一些矛盾的意见。这促使了文本解释的使用，一些作者（[39](#bib.bib39)）偏爱文本解释，因为它们天生易于被人类理解（[152](#bib.bib152)）。自那时以来，生成文本描述以解释预测的技术以及结合视觉和文本解释的多模态方法已经出现。然而，生成自由文本报告被认为是一项具有挑战性的任务，因为放射科医师报告在技术上是结构化的，而基于RNN的最常用语言模型在生成长文本时存在一些局限性，正如Pascanu等人（[105](#bib.bib105)）所述。
- en: 'As an alternative to text-based explanations, the use of example-based explanations
    was proposed, since this explanation modality is directly linked to how humans
    try to explain something to the other humans. This way, some example-based approaches
    have emerged with promising results that were even comparable to the performance
    of standard classifiers. These example-based methods include CBR approaches, prototype-based
    and concept-based strategies. Recently, Schutte et al. ([126](#bib.bib126)) introduced
    a disruptive approach that strives to generate synthetic examples to explain a
    model decision, as discussed in section [5.5](#S5.SS5 "5.5\. Other Approaches
    ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in
    Medical Image Classification: A Survey"). The possible limitations of using example-based
    methods are related to the availability of a considerable amount of data covering
    all the classes in a balanced way, without forgetting the sensibility of these
    methods to adversarial attacks, even though this can be prevented by using adversarial
    training ([93](#bib.bib93)).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '作为对基于文本的解释的替代方案，提出了使用基于示例的解释方法，因为这种解释方式直接与人类如何向其他人解释事物相关。因此，一些基于示例的方法应运而生，取得了有前景的结果，甚至可以与标准分类器的表现相媲美。这些基于示例的方法包括CBR方法、基于原型的方法和基于概念的方法。最近，Schutte等人（[126](#bib.bib126)）提出了一种突破性的方法，致力于生成合成示例以解释模型决策，如第[5.5](#S5.SS5
    "5.5\. Other Approaches ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey")节中讨论的那样。使用基于示例的方法的可能局限性与覆盖所有类别的足够数据的可用性相关，同时也要注意这些方法对对抗攻击的敏感性，尽管这可以通过使用对抗训练（[93](#bib.bib93)）来防止。'
- en: Alternatively, other methods have emerged as candidates for explaining the decision
    of a model. The adoption of Bayesian Neural Networks to estimate or quantify the
    uncertainty regarding the model predictions might be an interesting option, although
    few works attempted to prove its effectiveness.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，其他方法也作为解释模型决策的候选方法出现。采用贝叶斯神经网络来估计或量化模型预测的不确定性可能是一个有趣的选项，尽管很少有工作尝试证明其有效性。
- en: Future research in medical image interpretability can also include the use of
    vision transformers (ViTs) ([34](#bib.bib34)). According to ([94](#bib.bib94)),
    vision transformers proved to be comparable with CNN in terms of performance (accuracy)
    in the medical classification tasks. Furthermore, ViTs have the benefit of providing
    a type of built-in saliency maps that are used to better understand the model’s
    decisions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 未来医学图像解释的研究还可以包括使用视觉变换器（ViTs） ([34](#bib.bib34))。根据 ([94](#bib.bib94))，视觉变换器在医学分类任务中的性能（准确性）与卷积神经网络（CNN）相当。此外，ViTs
    具有提供内建显著性图的好处，这些图用于更好地理解模型的决策。
- en: Regarding medical image datasets, existing publicly available datasets for medical
    image captioning are limited in number and there is need to generate more large
    size datasets. Moreover, most of the existing datasets has focused only on few
    anatomical parts of body, such as chest, while ignoring other important parts
    like breast and brain ([9](#bib.bib9)).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 关于医学图像数据集，现有的公开医学图像描述数据集数量有限，需要生成更多的大型数据集。此外，大多数现有数据集仅关注于少数解剖部位，例如胸部，而忽略了其他重要部位如乳房和大脑
    ([9](#bib.bib9))。
- en: 8.1\. Challenges and Future Research Trends
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 挑战和未来研究趋势
- en: Despite the rapid pace of advances in the medical imaging and deep learning,
    there are problems that remain without a definitive solution.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管医学影像和深度学习的进展迅速，但仍存在一些问题没有得到明确的解决。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Small datasets: The collection of medical data depends on multiple entities
    and background bureaucracies. Nevertheless, the main issue is related to the availability
    of the physicians in annotating a vast amount of data, that is time-consuming
    and costly. This is even more critical in the XAI field, where additional annotations
    are required (e.g., concepts, textual descriptions). For this reason, interpretability-compliant
    medical datasets have a lower representativeness of the classes, resulting in
    poor generalizability and applicability of the developed methods to real-world
    scenarios. To surpass these constraints, distinct data augmentation techniques
    have emerged as an alternative for collecting new data. Recently, Wickramanayake
    et al.  ([164](#bib.bib164)) proposed the BRACE framework to augment the dataset
    based on concept-based explanations from model decisions, which can help to discover
    the samples in the under-represented regions in the training set. Furthermore,
    Wickramanayake et al. introduced a utility function to select the images in the
    under-representation regions and concepts that caused the misclassification. The
    images with a high utility score are selected to incorporate the training set.
    On the other hand, the use of generative approaches to perform data augmentation
    in a controlled way might be an interesting research direction.'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小数据集：医学数据的收集依赖于多个实体和背景官僚体系。然而，主要问题与医生在标注大量数据时的可用性相关，这既耗时又昂贵。这在解释性人工智能（XAI）领域尤为关键，因为需要额外的标注（例如，概念、文本描述）。因此，符合解释性的医学数据集在类别代表性方面较低，导致开发的方法在实际场景中的泛化能力和适用性较差。为了超越这些限制，出现了不同的数据增强技术作为收集新数据的替代方案。最近，Wickramanayake
    等人 ([164](#bib.bib164)) 提出了 BRACE 框架，通过模型决策中的基于概念的解释来增强数据集，这有助于发现训练集中代表性不足的样本。此外，Wickramanayake
    等人引入了一种效用函数，以选择在代表性不足区域的图像和导致误分类的概念。具有高效用分数的图像被选择以纳入训练集。另一方面，使用生成性方法以受控的方式进行数据增强可能是一个有趣的研究方向。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Insufficient labelled data: Although most works rely on the supervised learning
    paradigm, it is often not the best choice when working in the medical domain,
    since the process of label annotation is time-consuming and costly for large-scale
    datasets, especially in domains such as digital pathology where the manual annotation
    is subject to inter- and intra-observer variability ([172](#bib.bib172)). Transfer
    learning was adopted in most works to address these issues, but this technique
    is not completely effective in the medical domain, since the original models were
    trained in images belonging to standard object detection datasets (e.g., ImageNet),
    which do not share the same patterns of medical imagery. This way, Self-Supervised
    Learning (SSL) has emerged to tackle these challenges, allowing the network to
    learn visual meaningful feature representations without the need of annotated
    data ([27](#bib.bib27)). Besides its effectiveness in dealing with scarce labelled
    data, it confers robustness to the model, rendering it more resistant to adversarial
    attacks. In medical imaging, the use of SSL seems to be a promising research direction
    due to the characteristics of medical datasets. Furthermore, contrastive learning
    approaches have achieved impressive results, due to the contrastive loss that
    encourages the network to learn high-level features that occur in images across
    multiple views, which are created through the use of geometric transformation
    such as random cropping, color distortion, gaussian blur. For a comprehensive
    overview of the state-of-the-art of SSL with a particular focus on medical domain
    we refer the reader to ([27](#bib.bib27)). The inherently interpretable models,
    specifically the concept bottleneck models, require the annotation of concepts
    for each class or image. The presented datasets in Table [2](#S4.T2 "Table 2 ‣
    4\. Datasets ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") show that the majority does not include this type of annotation, hampering
    the rapid employment of the concept bottleneck models. Furthermore, despite the
    significance of the existing methods that emerged to surpass these issues (CAV ([60](#bib.bib60))),
    annotations regarding clinical concepts remain necessary. This could be solved
    with closer cooperation between clinicians and the AI community. As discussed
    in Section [4.6](#S4.SS6 "4.6\. Discussion ‣ 4\. Datasets ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), appropriate annotations in
    medical imaging datasets are needed to ensure a quick development of interpretability
    methods for medical diagnosis.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qualitative assessment of the explanations: The automated evaluation of the
    explanations provided by interpretability methods remains an open challenge. As
    previously discussed in section [6.1](#S6.SS1 "6.1\. Evaluating the Quality of
    Visual Explanations ‣ 6\. Evaluation Metrics ‣ Explainable Deep Learning Methods
    in Medical Image Classification: A Survey"), the most adopted method for evaluating
    the explanations in the context of the medical domain is to resort to the clinician’s
    expertise. However, considering variability in experts opinions ([147](#bib.bib147)),
    this strategy is particularly biased and subjective. On the other hand, the existing
    strategies for objectively measuring the quality of visual explanations are still
    dependent on manual annotations of relevant regions ([115](#bib.bib115)), or iterative
    model retraining (ROAR ([52](#bib.bib52))). For these reasons, we believe that
    the design of objective metrics for assessing the quality of the model explanations
    will be one of the important research trends on the topic of XAI.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释的定性评估：自动评估解释方法提供的解释仍然是一个未解的挑战。如在第[6.1节](#S6.SS1 "6.1\. 评估视觉解释的质量 ‣ 6\. 评估指标
    ‣ 医学图像分类中的可解释深度学习方法：综述")中讨论的，在医学领域中，评估解释的最常用方法是求助于临床专家的专业知识。然而，考虑到专家意见的变异性 ([147](#bib.bib147))，这一策略特别具有偏见和主观性。另一方面，现有的客观测量视觉解释质量的策略仍然依赖于相关区域的手动标注 ([115](#bib.bib115))，或迭代的模型再训练（ROAR ([52](#bib.bib52)））。因此，我们认为，设计用于评估模型解释质量的客观指标将成为XAI主题的一个重要研究趋势。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Report generation in medical imaging: Text-based explanations are usually obtained
    using RNN-based approaches through the generation of words forming a sentence.
    Nevertheless, RNN-based approaches have some limitations in generating long text
    reports ([105](#bib.bib105)). Consequently, the use of Transformers for the automatic
    generation of radiology reports was adopted in an attempt to overcome the limitations
    of traditional RNNs, namely the problem of vanishing gradients. The self-attention
    mechanism of the Transformer architecture allows for the learning of contextual
    relationships between the words that constitute the sequence. In addition, Transformer-based
    networks can be trained faster than traditional RNNs as they allow for simultaneous
    processing of sequential data. With the rise of foundation models, such as Generative
    Pretrained Transformers ([127](#bib.bib127)), the limitations encountered in previous
    methods have been alleviated, specifically the coherency of the generated texts.
    On the other hand, using concept-based approaches as a transition bridge between
    free-text report generation and concept-based explanations may be an exciting
    future research direction. Instead of trying to generate free-text reports, which
    is challenging, having a set of concepts that are sufficient to describe the phenomenon
    depicted in the image can support clinicians in writing a complete report.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医学影像中的报告生成：文本解释通常通过生成形成句子的单词来使用基于RNN的方法获得。然而，基于RNN的方法在生成长文本报告方面存在一些限制 ([105](#bib.bib105))。因此，采用Transformers进行放射学报告的自动生成，旨在克服传统RNN的限制，即梯度消失问题。Transformer架构的自注意力机制允许学习构成序列的单词之间的上下文关系。此外，基于Transformer的网络可以比传统RNN更快地进行训练，因为它们允许同时处理顺序数据。随着基础模型（如生成预训练Transformer ([127](#bib.bib127)））的兴起，之前方法中遇到的限制，特别是生成文本的连贯性问题，得到了缓解。另一方面，使用基于概念的方法作为自由文本报告生成和基于概念的解释之间的过渡桥梁可能是一个令人兴奋的未来研究方向。与其尝试生成具有挑战性的自由文本报告，不如拥有一组足够描述图像中现象的概念，这可以支持临床医生撰写完整报告。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deployment in clinical practice: The implementation of XAI methods in clinical
    practice requires rigorous validation to ensure their safety, effectiveness, and
    reliability, which can be challenging due to the complex and dynamic nature of
    clinical environments. Additionally, the field of medical imaging is subject to
    rigorous regulations, and the development and deployment of XAI methods must comply
    with regulatory and legal requirements ([42](#bib.bib42)), such as FDA approvals ([15](#bib.bib15)),
    data privacy regulations, and liability concerns.'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 临床实践中的部署：在临床实践中实施XAI方法需要严格的验证，以确保其安全性、有效性和可靠性，由于临床环境的复杂性和动态性，这可能会很具挑战性。此外，医学影像领域受到了严格的监管，XAI方法的开发和部署必须遵守监管和法律要求 ([42](#bib.bib42))，如FDA批准 ([15](#bib.bib15))、数据隐私法规和责任问题。
- en: 9\. Conclusions
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: This paper reviewed the advances on explainable deep learning applied to medical
    imaging diagnosis. First we introduced a comparative analysis between the existing
    surveys on the topic, where the major conclusions and weaknesses of the each were
    highlighted. Then, the most prominent XAI methods were briefly described to provide
    the readers with fundamental concepts of the field, necessary to the discussion
    of the recent advances on the medical imaging domain. Additionally, several frameworks
    that implement XAI methods were presented and a brief discussion of the existing
    medical imaging datasets was drawn. After this, we comprehensively reviewed the
    works focused on explaining the decision process of deep learning applied to medical
    imaging. The works were grouped according to the explanation modality comprising
    explanations by feature attribution, explanations by text, explanation by examples
    and explanations by concepts. Contrary to other surveys on the topic, we focused
    this review on inherently interpretable models over post-hoc approaches, which
    has been recently considered a future research direction on deep learning interpretability.
    The discussion of the adopted evaluation metrics used in the literature was also
    carried out, where we described the existing metrics to assess the quality of
    visual explanations and the commonly NLP metrics to evaluate the quality of the
    generated textual explanations. Additionally, a comparison of the performance
    of a set of prominent XAI methods was performed based on the dataset used and
    the evaluation metrics adopted. Finally, the discussion and future outlook in
    XAI for medical diagnosis were addressed where we identified open challenges in
    the literature and potential research avenues to improve the trustworthiness of
    provided explanations and foment the adoption of deep learning-based systems into
    clinical routine. To conclude, we believe that this survey will be helpful to
    the XAI community, particularly to the medical imaging field, as an entry point
    to guide the research and the future advances in the topic of XAI.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了应用于医学影像诊断的可解释深度学习的进展。首先，我们介绍了现有相关调查的比较分析，重点突出每个调查的主要结论和弱点。接着，简要描述了最突出的XAI方法，以向读者提供该领域的基本概念，这对于讨论医学影像领域的最新进展是必要的。此外，介绍了几种实现XAI方法的框架，并简要讨论了现有的医学影像数据集。随后，我们全面回顾了针对医学影像的深度学习决策过程解释的研究。这些研究根据解释方式分为特征归因解释、文本解释、实例解释和概念解释。与其他相关调查不同，我们将本综述重点放在了固有可解释模型上，而不是后验方法，这在深度学习可解释性领域被认为是未来的研究方向。我们还讨论了文献中采用的评估指标，描述了现有的视觉解释质量评估指标和用于评估生成文本解释质量的常用自然语言处理（NLP）指标。此外，基于使用的数据集和采用的评估指标，对一组突出的XAI方法的性能进行了比较。最后，讨论了XAI在医学诊断中的未来展望，识别了文献中的开放挑战和改善解释可信度以及促进深度学习系统在临床常规中应用的潜在研究途径。总之，我们相信这项综述将对XAI社区特别是医学影像领域有所帮助，为该领域的研究和未来进展提供指导。
- en: Acknowledgements
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was funded by the Portuguese Foundation for Science and Technology
    (FCT) under the PhD grant “2022.11566.BD”, and supported by NOVA LINCS (UIDB/04516/2020)
    with the financial support of FCT.IP.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作由葡萄牙科学技术基金会（FCT）资助，资助编号为“2022.11566.BD”，并由NOVA LINCS（UIDB/04516/2020）在FCT.IP的财政支持下提供支持。
- en: References
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Achanta et al. (2012) Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
    Lucchi, Pascal Fua, and Sabine Süsstrunk. 2012. SLIC Superpixels Compared to State-of-the-Art
    Superpixel Methods. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    34, 11 (2012), 2274–2282.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achanta 等（2012）拉达克里希纳·阿昌塔、阿普·沙吉、凯文·史密斯、奥雷利安·卢奇、帕斯卡尔·弗亚和萨宾·苏斯特朗克。2012年。SLIC超像素与最先进的超像素方法的比较。*IEEE模式分析与机器智能学报*
    34, 11（2012），2274–2282。
- en: Adebayo et al. (2018) Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow,
    Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. *Advances in
    Neural Information Processing Systems* 31 (2018).
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adebayo 等（2018）朱利叶斯·阿德巴约、贾斯汀·吉尔默、迈克尔·穆利、伊恩·古德费洛、莫里茨·哈特和宾·金。2018年。显著性图的合理性检查。*神经信息处理系统进展*
    31（2018）。
- en: 'Akgül et al. (2011) Ceyhun Burak Akgül, Daniel L Rubin, Sandy Napel, Christopher F
    Beaulieu, Hayit Greenspan, and Burak Acar. 2011. Content-Based Image Retrieval
    in Radiology: Current Status and Future Directions. *Journal of Digital Imaging*
    24, 2 (2011), 208–222.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akgül 等（2011）切尤洪·布拉克·阿克古尔、丹尼尔·L·鲁宾、桑迪·纳佩尔、克里斯托弗·F·博利厄、哈伊特·格林斯潘和布拉克·阿卡尔。2011年。放射学中的基于内容的图像检索：现状与未来方向。*数字成像杂志*
    24, 2（2011），208–222。
- en: Alber et al. (2019) Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer,
    Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert
    Müller, Sven Dähne, and Pieter-Jan Kindermans. 2019. iNNvestigate Neural Networks!
    *Journal of Machine Learning Research* 20, 93 (2019), 1–8.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alber 等（2019）马克西米利安·阿尔伯、塞巴斯蒂安·拉普什金、菲利普·西格勒、米里亚姆·哈戈尔、克里斯托夫·T·舒特、格雷戈瓦·蒙塔冯、沃伊切赫·萨梅克、克劳斯-罗伯特·穆勒、斯文·达赫和彼得-简·金德曼斯。2019年。iNNvestigate
    神经网络！*机器学习研究杂志* 20, 93（2019），1–8。
- en: Allegretti et al. (2021) Stefano Allegretti, Federico Bolelli, Federico Pollastri,
    Sabrina Longhitano, Giovanni Pellacani, and Costantino Grana. 2021. Supporting
    Skin Lesion Diagnosis with Content-Based Image Retrieval. In *Proceedings of the
    IEEE International Conference on Pattern Recognition (ICPR)*. 8053–8060.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allegretti 等（2021）斯特凡诺·阿列格雷蒂、费德里科·博莱利、费德里科·波拉斯特里、萨布丽娜·隆吉塔诺、乔瓦尼·佩拉卡尼和科斯坦蒂诺·格拉纳。2021年。通过基于内容的图像检索支持皮肤病变诊断。在*IEEE国际模式识别会议（ICPR）*上。8053–8060。
- en: Ancona et al. (2018) Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus
    Gross. 2018. Towards better understanding of gradient-based attribution methods
    for Deep Neural Networks. In *International Conference on Learning Representations
    (ICLR)*.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancona 等（2018）马尔科·安科纳、恩尼亚·切奥里尼、曾吉兹·厄兹提雷利和马库斯·格罗斯。2018年。为了更好地理解基于梯度的深度神经网络归因方法。在*国际学习表征会议（ICLR）*上。
- en: 'Armato III et al. (2011) Samuel G Armato III, Geoffrey McLennan, Luc Bidaut,
    Michael F McNitt-Gray, Charles Meyer, Anthony P Reeves, Binsheng Zhao, Denise R
    Aberle, Claudia I Henschke, Eric A Hoffman, et al. 2011. The Lung Image Database
    Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference
    database of lung nodules on CT scans. *Medical Physics* 38, 2 (2011), 915–931.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Armato III 等（2011）塞缪尔·G·阿玛托 III、杰弗里·麦克伦南、卢克·比多、迈克尔·F·麦克尼特-格雷、查尔斯·迈耶、安东尼·P·里夫斯、宾生·赵、丹妮丝·R·阿贝尔、克劳迪娅·I·亨施克、埃里克·A·霍夫曼等。2011年。肺部影像数据库联盟（LIDC）和影像数据库资源计划（IDRI）：一个完成的CT扫描肺结节参考数据库。*医学物理*
    38, 2（2011），915–931。
- en: 'Ayesha et al. (2021) Hareem Ayesha, Sajid Iqbal, Mehreen Tariq, Muhammad Abrar,
    Muhammad Sanaullah, Ishaq Abbas, Amjad Rehman, Muhammad Farooq Khan Niazi, and
    Shafiq Hussain. 2021. Automatic medical image interpretation: State of the art
    and future directions. *Pattern Recognition* 114 (2021), 107856.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ayesha 等（2021）哈里姆·艾莎、萨吉德·伊克巴尔、梅赫林·塔里克、穆罕默德·阿布拉尔、穆罕默德·萨瑟乌拉、伊沙克·阿巴斯、阿姆贾德·雷赫曼、穆罕默德·法鲁克·汗·尼亚齐和沙菲克·侯赛因。2021年。自动医学图像解读：现状与未来方向。*模式识别*
    114（2021），107856。
- en: Bach et al. (2015) Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick
    Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations
    for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. *PLoS
    ONE* 10, 7 (2015), e0130140.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bach 等（2015）塞巴斯蒂安·巴赫、亚历山大·宾德、格雷戈瓦·蒙塔冯、弗雷德里克·克劳申、克劳斯-罗伯特·穆勒和沃伊切赫·萨梅克。2015年。逐层相关传播的像素级解释用于非线性分类器决策。*PLoS
    ONE* 10, 7（2015），e0130140。
- en: Bai et al. (2021) Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.
    Recent Advances in Adversarial Training for Adversarial Robustness. In *Proceedings
    of the International Joint Conference on Artificial Intelligence (IJCAI)*. 4312–4321.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2021）陶·白、林琦·罗、俊·赵、碧寒·温和钱·王。2021年。对抗训练的最新进展以提高对抗鲁棒性。在*国际联合人工智能会议（IJCAI）*上。4312–4321。
- en: Barata et al. (2019) Catarina Barata, Jorge S Marques, and M Emre Celebi. 2019.
    Deep Attention Model for the Hierarchical Diagnosis of Skin Lesions. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*.
    2757–2765.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barata 等（2019） Catarina Barata, Jorge S Marques, 和 M Emre Celebi。2019年。用于皮肤病变分级诊断的深度注意力模型。*《IEEE计算机视觉与模式识别会议工作坊论文集（CVPRW）》*。2757–2765。
- en: Barata and Santiago (2021) Catarina Barata and Carlos Santiago. 2021. Improving
    the Explainability of Skin Cancer Diagnosis Using CBIR. In *Proceedings of the
    International Conference on Medical Image Computing and Computer-Assisted Intervention
    (MICCAI)*. 550–559.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barata 和 Santiago（2021） Catarina Barata 和 Carlos Santiago。2021年。通过CBIR提高皮肤癌诊断的可解释性。*《医学图像计算与计算机辅助手术国际会议论文集（MICCAI）》*。550–559。
- en: 'Barnett et al. (2021) Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao,
    Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia Rudin. 2021. Interpretable
    Mammographic Image Classification using Case-Based Reasoning and Deep Learning.
    In *Workshop on Deep Learning, Case-Based Reasoning, and AutoML: Present and Future
    Synergies - IJCAI*.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barnett 等（2021） Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan
    Chen, Yinhao Ren, Joseph Y Lo, 和 Cynthia Rudin。2021年。使用基于案例的推理和深度学习的可解释乳腺影像分类。*《深度学习、基于案例的推理和自动化机器学习研讨会：现在与未来的协同》
    - IJCAI*。
- en: 'Benjamens et al. (2020) Stan Benjamens, Pranavsingh Dhunnoo, and Bertalan Meskó.
    2020. The state of artificial intelligence-based FDA-approved medical devices
    and algorithms: an online database. *NPJ Digital Medicine* 3, 1 (2020), 118.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benjamens 等（2020） Stan Benjamens, Pranavsingh Dhunnoo, 和 Bertalan Meskó。2020年。基于人工智能的FDA批准的医疗设备和算法的现状：一个在线数据库。*《NPJ数字医学》*
    3, 1（2020），118。
- en: Billah and Javed (2022) Mohammad Ehtasham Billah and Farrukh Javed. 2022. Bayesian
    Convolutional Neural Network-based Models for Diagnosis of Blood Cancer. *Applied
    Artificial Intelligence* (2022), 1–22.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Billah 和 Javed（2022） Mohammad Ehtasham Billah 和 Farrukh Javed。2022年。基于贝叶斯卷积神经网络的血癌诊断模型。*《应用人工智能》*（2022），1–22。
- en: Bolhasani et al. (2020) Hamidreza Bolhasani, Elham Amjadi, Maryam Tabatabaeian,
    and Somayyeh Jafarali Jassbi. 2020. A histopathological image dataset for grading
    breast invasive ductal carcinomas. *Informatics in Medicine Unlocked* 19 (2020),
    100341.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolhasani 等（2020） Hamidreza Bolhasani, Elham Amjadi, Maryam Tabatabaeian, 和
    Somayyeh Jafarali Jassbi。2020年。用于乳腺侵袭性导管癌分级的组织病理图像数据集。*《医学信息解锁》* 19（2020），100341。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in Neural
    Information Processing Systems (NeurIPS)* 33 (2020), 1877–1901.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020） Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等。2020年。语言模型是少样本学习者。*《神经信息处理系统进展（NeurIPS）》* 33（2020），1877–1901。
- en: 'Bustos et al. (2020) Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and
    Maria de la Iglesia-Vayá. 2020. PadChest: A large chest x-ray image dataset with
    multi-label annotated reports. *Medical Image Analysis* 66 (2020), 101797.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bustos 等（2020） Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, 和 Maria
    de la Iglesia-Vayá。2020年。PadChest：一个大型胸部X光图像数据集，具有多标签注释报告。*《医学图像分析》* 66（2020），101797。
- en: Bykov et al. (2021) Kirill Bykov, Marina M-C Höhne, Adelaida Creosteanu, Klaus-Robert
    Müller, Frederick Klauschen, Shinichi Nakajima, and Marius Kloft. 2021. Explaining
    Bayesian Neural Networks. *arXiv preprint arXiv:2108.10346* (2021).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bykov 等（2021） Kirill Bykov, Marina M-C Höhne, Adelaida Creosteanu, Klaus-Robert
    Müller, Frederick Klauschen, Shinichi Nakajima, 和 Marius Kloft。2021年。解释贝叶斯神经网络。*arXiv
    预印本 arXiv:2108.10346*（2021）。
- en: Caron et al. (2018) Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs
    Douze. 2018. Deep Clustering for Unsupervised Learning of Visual Features. In
    *Proceedings of the European Conference on Computer Vision (ECCV)*. 132–149.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caron 等（2018） Mathilde Caron, Piotr Bojanowski, Armand Joulin, 和 Matthijs Douze。2018年。用于无监督学习视觉特征的深度聚类。*《欧洲计算机视觉会议论文集（ECCV）》*。132–149。
- en: 'Chen et al. (2019) Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett,
    Jonathan Su, and Cynthia Rudin. 2019. This Looks Like That: Deep Learning for
    Interpretable Image Recognition. In *Proceedings of the International Conference
    of Neural Information Processing Systems (NIPS)*.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019） Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan
    Su, 和 Cynthia Rudin。2019年。这看起来像那样：用于可解释图像识别的深度学习。*《神经信息处理系统国际会议论文集（NIPS）》*。
- en: Chen et al. (2023) Weixing Chen, Yang Liu, Ce Wang, Guanbin Li, Jiarui Zhu,
    and Liang Lin. 2023. Visual-Linguistic Causal Intervention for Radiology Report
    Generation. *arXiv preprint arXiv:2303.09117* (2023).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023) Weixing Chen、Yang Liu、Ce Wang、Guanbin Li、Jiarui Zhu 和 Liang Lin。2023年。用于放射科报告生成的视觉-语言因果干预。*arXiv
    预印本 arXiv:2303.09117* (2023)。
- en: Chen et al. (2020a) Zhi Chen, Yijie Bei, and Cynthia Rudin. 2020a. Concept Whitening
    for Interpretable Image Recognition. *Nature Machine Intelligence* 2, 12 (2020),
    772–782.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020a) Zhi Chen、Yijie Bei 和 Cynthia Rudin。2020年。用于可解释图像识别的概念美白。*自然机器智能*
    2, 12 (2020), 772–782。
- en: Chen et al. (2020b) Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan.
    2020b. Generating Radiology Reports via Memory-driven Transformer. In *Proceedings
    of the Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    1439–1449.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020b) Zhihong Chen、Yan Song、Tsung-Hui Chang 和 Xiang Wan。2020年。通过记忆驱动的变换器生成放射科报告。在
    *自然语言处理实证方法会议（EMNLP）会议录* 中。1439–1449。
- en: 'Chittajallu et al. (2019) Deepak Roy Chittajallu, Bo Dong, Paul Tunison, Roddy
    Collins, Katerina Wells, James Fleshman, Ganesh Sankaranarayanan, Steven Schwaitzberg,
    Lora Cavuoto, and Andinet Enquobahrie. 2019. XAI-CBIR: Explainable AI System for
    Content based Retrieval of Video Frames from Minimally Invasive Surgery Videos.
    In *Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI)*.
    66–69.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chittajallu 等 (2019) Deepak Roy Chittajallu、Bo Dong、Paul Tunison、Roddy Collins、Katerina
    Wells、James Fleshman、Ganesh Sankaranarayanan、Steven Schwaitzberg、Lora Cavuoto
    和 Andinet Enquobahrie。2019年。XAI-CBIR：用于从微创手术视频中基于内容的视频帧检索的可解释人工智能系统。在 *IEEE生物医学影像国际研讨会（ISBI）会议录*
    中。66–69。
- en: 'Chowdhury et al. (2021) Alexander Chowdhury, Jacob Rosenthal, Jonathan Waring,
    and Renato Umeton. 2021. Applying Self-Supervised Learning to Medicine: Review
    of the State of the Art and Medical Implementations. *Informatics* 8, 3 (2021),
    59.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury 等 (2021) Alexander Chowdhury、Jacob Rosenthal、Jonathan Waring 和 Renato
    Umeton。2021年。将自监督学习应用于医学：前沿技术和医学应用的回顾。*信息学* 8, 3 (2021), 59。
- en: Cohen et al. (2020) Joseph Paul Cohen, Paul Morrison, and Lan Dao. 2020. COVID-19
    Image Data Collection. *arXiv 2003.11597* (2020). [https://github.com/ieee8023/covid-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset)
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等 (2020) Joseph Paul Cohen、Paul Morrison 和 Lan Dao。2020年。COVID-19 图像数据集收集。*arXiv
    2003.11597* (2020)。[https://github.com/ieee8023/covid-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset)
- en: 'Daneshjou et al. (2022) Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto
    Novoa, and James Y Zou. 2022. SkinCon: A skin disease dataset densely annotated
    by domain experts for fine-grained debugging and analysis. *Advances in Neural
    Information Processing Systems* 35 (2022), 18157–18167.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daneshjou 等 (2022) Roxana Daneshjou、Mert Yuksekgonul、Zhuo Ran Cai、Roberto Novoa
    和 James Y Zou。2022年。SkinCon：由领域专家密集注释的皮肤疾病数据集，用于细粒度调试和分析。*神经信息处理系统进展* 35 (2022),
    18157–18167。
- en: Darias et al. (2021) Jesus M Darias, Belén Dıaz-Agudo, and Juan A Recio-Garcia.
    2021. A Systematic Review on Model-agnostic XAI Libraries. (2021).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darias 等 (2021) Jesus M Darias、Belén Díaz-Agudo 和 Juan A Recio-Garcia。2021年。关于模型无关的
    XAI 库的系统评审。 (2021)。
- en: de Faria et al. (2019) Sergio de Faria, Jose Filipe, Pedro Pereira, Luis Tavora,
    Pedro Assuncao, Miguel Santos, Rui Fonseca-Pinto, Felicidade Santiago, Victoria
    Dominguez, and Martinha Henrique. 2019. Light Field Image Dataset of Skin Lesions.
    In *Proceedings of the International Conference of the IEEE Engineering in Medicine
    and Biology Society (EMBC)*. 3905–3908.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Faria 等 (2019) Sergio de Faria、Jose Filipe、Pedro Pereira、Luis Tavora、Pedro
    Assuncao、Miguel Santos、Rui Fonseca-Pinto、Felicidade Santiago、Victoria Dominguez
    和 Martinha Henrique。2019年。皮肤病变的光场图像数据集。在 *IEEE医学与生物学工程学会国际会议（EMBC）会议录* 中。3905–3908。
- en: Demner-Fushman et al. (2016) Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman,
    Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J
    McDonald. 2016. Preparing a collection of radiology examinations for distribution
    and retrieval. *Journal of the American Medical Informatics Association* 23, 2
    (2016), 304–310.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demner-Fushman 等 (2016) Dina Demner-Fushman、Marc D Kohli、Marc B Rosenman、Sonya
    E Shooshan、Laritza Rodriguez、Sameer Antani、George R Thoma 和 Clement J McDonald。2016年。准备一组放射学检查以供分发和检索。*美国医学信息学学会杂志*
    23, 2 (2016), 304–310。
- en: 'Donnelly et al. (2021) Jon Donnelly, Alina Jade Barnett, and Chaofan Chen.
    2021. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable
    Prototypes. *arXiv preprint arXiv:2111.15000* (2021).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donnelly 等 (2021) Jon Donnelly、Alina Jade Barnett 和 Chaofan Chen。2021年。可变形 ProtoPNet：使用可变形原型的可解释图像分类器。*arXiv
    预印本 arXiv:2111.15000* (2021)。
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In
    *Proceedings of the International Conference on Learning Representations (ICLR)*.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy等 (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
    Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
    Georg Heigold, Sylvain Gelly, Jakob Uszkoreit 和 Neil Houlsby. 2021. 一张图像胜过16x16个词：大规模图像识别中的变压器。载于*国际学习表征大会论文集
    (ICLR)*。
- en: Eitel and for the Alzheimer’s Disease Neuroimaging Initiative  (ADNI) Fabian
    Eitel and Kerstin Ritter for the Alzheimer’s Disease Neuroimaging Initiative (ADNI).
    2019. Testing the Robustness of Attribution Methods for Convolutional Neural Networks
    in MRI-Based Alzheimer’s Disease Classification. In *Interpretability of Machine
    Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision
    Support (IMIMIC)*. 3–11.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eitel和阿尔茨海默病神经影像学倡议 (ADNI) Fabian Eitel 和 Kerstin Ritter 为阿尔茨海默病神经影像学倡议 (ADNI)
    做的研究。2019. 测试卷积神经网络在基于MRI的阿尔茨海默病分类中的归因方法的鲁棒性。载于*医学影像计算和临床决策支持的机器智能可解释性 (IMIMIC)*.
    3–11。
- en: Fang et al. (2020) Zhengqing Fang, Kun Kuang, Yuxiao Lin, Fei Wu, and Yu-Feng
    Yao. 2020. Concept-based Explanation for Fine-grained Images and Its Application
    in Infectious Keratitis Classification. In *Proceedings of the ACM International
    Conference on Multimedia*. 700–708.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang等 (2020) Zhengqing Fang, Kun Kuang, Yuxiao Lin, Fei Wu 和 Yu-Feng Yao. 2020.
    基于概念的细粒度图像解释及其在传染性角膜炎分类中的应用。载于*ACM国际多媒体会议论文集*。700–708。
- en: Fong et al. (2019) Ruth Fong, Mandela Patrick, and Andrea Vedaldi. 2019. Understanding
    Deep Networks via Extremal Perturbations and Smooth Masks. In *Proceedings of
    the IEEE/CVF International Conference on Computer Vision (ICCV)*. 2950–2958.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fong等 (2019) Ruth Fong, Mandela Patrick 和 Andrea Vedaldi. 2019. 通过极端扰动和光滑掩膜理解深度网络。载于*IEEE/CVF国际计算机视觉大会论文集
    (ICCV)*. 2950–2958。
- en: 'Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
    a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In
    *Proceedings of the International Conference on Machine Learning (ICML)*. 1050–1059.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal和Ghahramani (2016) Yarin Gal和Zoubin Ghahramani. 2016. Dropout作为贝叶斯近似：在深度学习中表示模型不确定性。载于*国际机器学习大会论文集
    (ICML)*. 1050–1059。
- en: Gale et al. (2019) William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P
    Bradley, and Lyle J Palmer. 2019. Producing Radiologist-Quality Reports for Interpretable
    Deep Learning. In *Proceedings of the IEEE International Symposium on Biomedical
    Imaging (ISBI)*. 1275–1279.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale等 (2019) William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P Bradley
    和 Lyle J Palmer. 2019. 为可解释的深度学习生成放射科医生质量的报告。载于*IEEE国际生物医学影像学研讨会论文集 (ISBI)*. 1275–1279。
- en: Ghorbani et al. (2019a) Amirata Ghorbani, Abubakar Abid, and James Zou. 2019a.
    Interpretation of Neural Networks is Fragile. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. 3681–3688.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghorbani等 (2019a) Amirata Ghorbani, Abubakar Abid 和 James Zou. 2019a. 神经网络的解释是脆弱的。载于*AAAI人工智能会议论文集*，第33卷.
    3681–3688。
- en: Ghorbani et al. (2019b) Amirata Ghorbani, James Wexler, James Y Zou, and Been
    Kim. 2019b. Towards Automatic Concept-Based Explanations. In *Advances in Neural
    Information Processing Systems*, Vol. 32.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghorbani等 (2019b) Amirata Ghorbani, James Wexler, James Y Zou 和 Been Kim. 2019b.
    朝着自动化概念解释的方向前进。载于*神经信息处理系统进展*，第32卷。
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European Union
    Regulations on Algorithmic Decision-Making and a “Right to Explanation”. *AI Magazine*
    38, 3 (2017), 50–57.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman和Flaxman (2017) Bryce Goodman 和 Seth Flaxman. 2017. 欧盟关于算法决策的法规与“解释权”。*AI杂志*
    38, 3 (2017), 50–57。
- en: Gour and Jain (2022) Mahesh Gour and Sweta Jain. 2022. Uncertainty-aware convolutional
    neural network for COVID-19 X-ray images classification. *Computers in Biology
    and Medicine* 140 (2022), 105047.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gour和Jain (2022) Mahesh Gour 和 Sweta Jain. 2022. 面向不确定性的卷积神经网络用于COVID-19 X光图像分类。*计算机与生物医学*
    140 (2022), 105047。
- en: 'Graziani et al. (2020) Mara Graziani, Vincent Andrearczyk, Stéphane Marchand-Maillet,
    and Henning Müller. 2020. Concept attribution: Explaining CNN decisions to physicians.
    *Computers in Biology and Medicine* 123 (2020), 103865.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graziani等 (2020) Mara Graziani, Vincent Andrearczyk, Stéphane Marchand-Maillet
    和 Henning Müller. 2020. 概念归因：向医生解释CNN的决策。*计算机与生物医学* 123 (2020), 103865。
- en: Groh et al. (2021) Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel
    Han, Aerin Kim, Arash Koochek, and Omar Badri. 2021. Evaluating deep neural networks
    trained on clinical images in dermatology with the fitzpatrick 17k dataset. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 1820–1828.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groh等（2021）Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han,
    Aerin Kim, Arash Koochek 和 Omar Badri。2021年。使用Fitzpatrick 17k数据集评估在皮肤病学领域训练的深度神经网络。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*中，1820–1828页。
- en: Gulum et al. (2021) Mehmet A Gulum, Christopher M Trombley, and Mehmed Kantardzic.
    2021. A Review of Explainable Deep Learning Cancer Detection Models in Medical
    Imaging. *Applied Sciences* 11, 10 (2021), 4573.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulum等（2021）Mehmet A Gulum, Christopher M Trombley 和 Mehmed Kantardzic。2021年。医学影像中可解释深度学习癌症检测模型的综述。*应用科学*
    11, 10（2021），4573。
- en: Gunning and Aha (2019) David Gunning and David Aha. 2019. DARPA’s Explainable
    Artificial Intelligence (XAI) Program. *AI Magazine* 40, 2 (2019), 44–58.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunning和Aha（2019）David Gunning 和 David Aha。2019年。DARPA的可解释人工智能（XAI）计划。*AI杂志*
    40, 2（2019），44–58页。
- en: 'Gupta et al. (2021) Tarun Gupta, Libin Kutty, Ritu Gahir, Nnamdi Ukwu, Sayantan
    Polley, and Marcus Thiel. 2021. IRTEX: Image Retrieval with Textual Explanations.
    In *Proceedings of the IEEE International Conference on Human-Machine Systems
    (ICHMS)*. 1–4.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta等（2021）Tarun Gupta, Libin Kutty, Ritu Gahir, Nnamdi Ukwu, Sayantan Polley
    和 Marcus Thiel。2021年。IRTEX：带文本解释的图像检索。在*IEEE国际人机系统会议（ICHMS）论文集*中，1–4页。
- en: 'Hansell et al. (2008) David M. Hansell, Alexander A. Bankier, Heber MacMahon,
    Theresa C. McLoud, Nestor L. Müller, and Jacques Remy. 2008. Fleischner Society:
    Glossary of Terms for Thoracic Imaging. *Radiology* 246, 3 (2008), 697–722.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansell等（2008）David M. Hansell, Alexander A. Bankier, Heber MacMahon, Theresa
    C. McLoud, Nestor L. Müller 和 Jacques Remy。2008年。Fleischner Society：胸部影像学术语词汇表。*放射学*
    246, 3（2008），697–722页。
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    Diffusion Probabilistic Models. *Advances in Neural Information Processing Systems*
    33 (2020), 6840–6851.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho等（2020）Jonathan Ho, Ajay Jain 和 Pieter Abbeel。2020年。去噪扩散概率模型。*神经信息处理系统进展*
    33（2020），6840–6851页。
- en: Hoffmann et al. (2021) Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas
    Kohler. 2021. This Looks Like That… Does it? Shortcomings of Latent Space Prototype
    Interpretability in Deep Networks. In *ICML Workshop on Theoretic Foundation,
    Criticism, and Application Trend of Explainable AI*.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann等（2021）Adrian Hoffmann, Claudio Fanconi, Rahul Rade 和 Jonas Kohler。2021年。这看起来像那个……真的如此吗？深度网络中潜在空间原型解释性的不足。在*ICML解释性人工智能的理论基础、批评和应用趋势研讨会*中。
- en: Hooker et al. (2019) Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and
    Been Kim. 2019. A Benchmark for Interpretability Methods in Deep Neural Networks.
    In *Advances in Neural Information Processing Systems*, Vol. 32. 9737–9748.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker等（2019）Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans 和 Been Kim。2019年。深度神经网络中可解释性方法的基准。在*神经信息处理系统进展*中，第32卷，9737–9748页。
- en: 'Hu et al. (2022) Brian Hu, Bhavan Vasu, and Anthony Hoogs. 2022. X-MIR: EXplainable
    Medical Image Retrieval. In *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision (WACV)*. 440–450.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2022）Brian Hu, Bhavan Vasu 和 Anthony Hoogs。2022年。X-MIR：可解释的医学图像检索。在*IEEE/CVF冬季计算机视觉应用会议（WACV）论文集*中，440–450页。
- en: 'Irvin et al. (2019) Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana
    Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie
    Shpanskaya, et al. 2019. CheXpert: A Large Chest Radiograph Dataset with Uncertainty
    Labels and Expert Comparison. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. 590–597.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irvin等（2019）Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus,
    Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya等。2019年。CheXpert：一个具有不确定性标签和专家比较的大型胸部X光数据集。在*AAAI人工智能会议论文集*中，第33卷，590–597页。
- en: Jing et al. (2018) Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the Automatic
    Generation of Medical Imaging Reports. In *Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (ACL)*.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing等（2018）Baoyu Jing, Pengtao Xie 和 Eric Xing。2018年。关于医学影像报告的自动生成。在*第56届计算语言学协会年会（ACL）论文集*中。
- en: Johnson et al. (2019) Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
    Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven
    Horng. 2019. MIMIC-CXR, a de-identified publicly available database of chest radiographs
    with free-text reports. *Scientific Data* 6, 1 (2019), 1–8.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人（2019）Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel
    R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark 和 Steven Horng。2019。MIMIC-CXR，一个去标识化的公共胸部放射图像数据库，包含自由文本报告。*科学数据*
    6, 1 (2019), 1–8。
- en: Kawahara et al. (2019) Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano,
    and Ghassan Hamarneh. 2019. Seven-Point Checklist and Skin Lesion Classification
    Using Multitask Multimodal Neural Nets. *IEEE Journal of Biomedical and Health
    Informatics* 23, 2 (2019), 538–546.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kawahara 等人（2019）Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano 和 Ghassan
    Hamarneh。2019。七点检查表和使用多任务多模态神经网络的皮肤病变分类。*IEEE 生物医学与健康信息学期刊* 23, 2 (2019), 538–546。
- en: 'Kazerouni et al. (2022) Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein
    Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. 2022.
    Diffusion models for medical image analysis: A comprehensive survey. *arXiv preprint
    arXiv:2211.07804* (2022).'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazerouni 等人（2022）Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari,
    Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu 和 Dorit Merhof。2022。医学图像分析中的扩散模型：全面综述。*arXiv
    预印本 arXiv:2211.07804* (2022)。
- en: Keenan et al. (2020) Oisin J. F. Keenan, George Holland, Julian F. Maempel,
    John F. Keating, and Chloe E. H. Scott. 2020. Correlations between radiological
    classification systems and confirmed cartilage loss in severe knee osteoarthritis.
    *The Bone & Joint Journal* 102-B, 3 (2020), 301–309.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keenan 等人（2020）Oisin J. F. Keenan, George Holland, Julian F. Maempel, John F.
    Keating 和 Chloe E. H. Scott。2020。放射学分类系统与严重膝关节骨关节炎中的确认软骨损失之间的相关性。*骨与关节杂志* 102-B,
    3 (2020), 301–309。
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai,
    James Wexler, Fernanda B. Viégas, and Rory Sayres. 2018. Interpretability Beyond
    Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).
    In *Proceedings of the International Conference on Machine Learning (ICML)*. 2668–2677.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2018）Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James
    Wexler, Fernanda B. Viégas 和 Rory Sayres。2018。超越特征归因的可解释性：使用概念激活向量（TCAV）的定量测试。在
    *国际机器学习大会（ICML）* 论文集。2668–2677。
- en: 'Kim et al. (2021b) Eunji Kim, Siwon Kim, Minji Seo, and Sungroh Yoon. 2021b.
    XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 15719–15728.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2021b）Eunji Kim, Siwon Kim, Minji Seo 和 Sungroh Yoon。2021b。XProtoNet：使用全局和局部解释进行胸部放射影像诊断。在
    *IEEE/CVF 计算机视觉与模式识别大会（CVPR）* 论文集。15719–15728。
- en: Kim et al. (2021a) Junho Kim, Minsu Kim, and Yong Man Ro. 2021a. Interpretation
    of Lesional Detection via Counterfactual Generation. In *Proceedings of the IEEE
    International Conference on Image Processing (ICIP)*. 96–100.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2021a）Junho Kim, Minsu Kim 和 Yong Man Ro。2021a。通过反事实生成的病变检测解释。在 *IEEE
    国际图像处理大会（ICIP）* 论文集。96–100。
- en: 'Kindermans et al. (2018) Pieter-Jan Kindermans, Kristof T Schütt, Maximilian
    Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. 2018. Learning
    how to explain neural networks: PatternNet and PatternAttribution. In *International
    Conference on Learning Representations (ICML)*.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kindermans 等人（2018）Pieter-Jan Kindermans, Kristof T Schütt, Maximilian Alber,
    Klaus-Robert Müller, Dumitru Erhan, Been Kim 和 Sven Dähne。2018。学习如何解释神经网络：PatternNet
    和 PatternAttribution。在 *国际学习表示大会（ICML）*。
- en: Koh et al. (2020) Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann,
    Emma Pierson, Been Kim, and Percy Liang. 2020. Concept Bottleneck Models. In *Proceedings
    of the International Conference on Machine Learning (ICML)*. 5338–5348.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人（2020）Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma
    Pierson, Been Kim 和 Percy Liang。2020。概念瓶颈模型。在 *国际机器学习大会（ICML）* 论文集。5338–5348。
- en: 'Kokhlikyanet (2019) Kokhlikyanet. 2019. Pytorch Captum. [Online]. Accessed
    January, 21 2021\. Available: [https://github.com/pytorch/captum](https://github.com/pytorch/captum).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kokhlikyanet（2019）Kokhlikyanet。2019。Pytorch Captum。 [在线]。访问时间：2021年1月21日。可用：[https://github.com/pytorch/captum](https://github.com/pytorch/captum)。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. *Advances
    in Neural Information Processing Systems (NIPS)* 25 (2012).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人（2012）Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton。2012。使用深度卷积神经网络进行
    ImageNet 分类。*神经信息处理系统进展（NIPS）* 25 (2012)。
- en: Kumar et al. (2009) Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and
    Shree K. Nayar. 2009. Attribute and Simile Classifiers for Face Verification.
    In *Proceedings of the IEEE International Conference on Computer Vision (CVPR)*.
    365–372.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等（2009）Neeraj Kumar、Alexander C. Berg、Peter N. Belhumeur 和 Shree K. Nayar。2009年。面部验证的属性和相似性分类器。在*IEEE国际计算机视觉会议（CVPR）论文集*中。365–372。
- en: Lampert et al. (2009) Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling.
    2009. Learning to Detect Unseen Object Classes by Between-class Attribute Transfer.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 951–958.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lampert 等（2009）Christoph H Lampert、Hannes Nickisch 和 Stefan Harmeling。2009年。通过类间属性转移学习检测未见的物体类别。在*IEEE计算机视觉与模式识别会议（CVPR）论文集*中。951–958。
- en: 'Lamy et al. (2019) Jean-Baptiste Lamy, Boomadevi Sekar, Gilles Guezennec, Jacques
    Bouaud, and Brigitte Séroussi. 2019. Explainable artificial intelligence for breast
    cancer: A visual case-based reasoning approach. *Artificial Intelligence in Medicine*
    94 (2019), 42–53.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamy 等（2019）Jean-Baptiste Lamy、Boomadevi Sekar、Gilles Guezennec、Jacques Bouaud
    和 Brigitte Séroussi。2019年。用于乳腺癌的可解释人工智能：一种视觉案例推理方法。*医学中的人工智能* 94（2019），42–53。
- en: Lapuschkin et al. (2016) Sebastian Lapuschkin, Alexander Binder, Grégoire Montavon,
    Klaus-Robert Müller, and Wojciech Samek. 2016. The LRP Toolbox for Artificial
    Neural Networks. *Journal of Machine Learning Research* 17, 114 (2016), 1–5.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lapuschkin 等（2016）Sebastian Lapuschkin、Alexander Binder、Grégoire Montavon、Klaus-Robert
    Müller 和 Wojciech Samek。2016年。人工神经网络的LRP工具箱。*机器学习研究杂志* 17, 114（2016），1–5。
- en: 'Lavie and Agarwal (2007) Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automatic
    Metric for MT Evaluation with High Levels of Correlation with Human Judgments.
    In *Proceedings of the Workshop on Statistical Machine Translation*. 228–231.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lavie 和 Agarwal（2007）Alon Lavie 和 Abhaya Agarwal。2007年。METEOR：一种与人工判断高度相关的机器翻译评估自动指标。在*统计机器翻译研讨会论文集*中。228–231。
- en: Lee et al. (2019) Hyebin Lee, Seong Tae Kim, and Yong Man Ro. 2019. Generation
    of Multimodal Justification Using Visual Word Constraint Model for Explainable
    Computer-Aided Diagnosis. In *Interpretability of Machine Intelligence in Medical
    Image Computing and Multimodal Learning for Clinical Decision Support*. 21–29.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2019）Hyebin Lee、Seong Tae Kim 和 Yong Man Ro。2019年。基于视觉词约束模型的多模态解释生成，用于可解释的计算机辅助诊断。在*医学图像计算中的机器智能可解释性与临床决策支持的多模态学习*中。21–29。
- en: Lee et al. (2017) Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai
    Miyake, Mia Gorovoy, and Daniel L Rubin. 2017. A curated mammography data set
    for use in computer-aided detection and diagnosis research. *Scientific Data*
    4, 1 (2017), 1–9.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2017）Rebecca Sawyer Lee、Francisco Gimenez、Assaf Hoogi、Kanae Kawai Miyake、Mia
    Gorovoy 和 Daniel L Rubin。2017年。用于计算机辅助检测和诊断研究的策划乳腺X光数据集。*科学数据* 4, 1（2017），1–9。
- en: Li et al. (2018) Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. 2018.
    Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation.
    In *Proceedings of the International Conference on Neural Information Processing
    Systems (NIPS)*. 1537–1547.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018）Christy Y Li、Xiaodan Liang、Zhiting Hu 和 Eric P Xing。2018年。用于医疗图像报告生成的混合检索生成强化代理。在*国际神经信息处理系统会议（NIPS）论文集*中。1537–1547。
- en: Li et al. (2023) Mingjie Li, Rui Liu, Fuyu Wang, Xiaojun Chang, and Xiaodan
    Liang. 2023. Auxiliary signal-guided knowledge encoder-decoder for medical report
    generation. *World Wide Web* 26, 1 (2023), 253–270.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Mingjie Li、Rui Liu、Fuyu Wang、Xiaojun Chang 和 Xiaodan Liang。2023年。用于医疗报告生成的辅助信号引导的知识编码解码器。*世界宽网*
    26, 1（2023），253–270。
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. 74–81.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin。2004年。ROUGE：一种自动评估摘要的工具包。在*文本摘要扩展*中。74–81。
- en: Lin and Lee (2020) Tsung-Chieh Lin and Hsi-Chieh Lee. 2020. Covid-19 Chest Radiography
    Images Analysis Based on Integration of Image Preprocess, Guided Grad-CAM, Machine
    Learning and Risk Management. In *Proceedings of the International Conference
    on Medical and Health Informatics (ICMHI)*. 281–288.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 和 Lee（2020）Tsung-Chieh Lin 和 Hsi-Chieh Lee。2020年。基于图像预处理、引导Grad-CAM、机器学习和风险管理的Covid-19胸部X光图像分析。在*国际医疗和健康信息学会议（ICMHI）论文集*中。281–288。
- en: Lipton (2017) Zachary C Lipton. 2017. The Doctor Just Won’t Accept That! *arXiv
    preprint arXiv:1711.08037* (2017).
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton（2017）Zachary C Lipton。2017年。医生就是不接受这一点！*arXiv 预印本 arXiv:1711.08037*（2017）。
- en: 'Litjens et al. (2018) Geert Litjens, Peter Bandi, Babak Ehteshami Bejnordi,
    Oscar Geessink, Maschenka Balkenhol, et al. 2018. 1399 H&E-stained sentinel lymph
    node sections of breast cancer patients: the CAMELYON dataset. *GigaScience* 7,
    6 (2018).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等（2018）Geert Litjens、Peter Bandi、Babak Ehteshami Bejnordi、Oscar Geessink、Maschenka
    Balkenhol 等。2018年。《1399 例H&E染色乳腺癌患者的哨兵淋巴结切片：CAMELYON 数据集》。*GigaScience* 7, 6（2018）。
- en: Litjens et al. (2017) Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
    Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak,
    Bram Van Ginneken, and Clara I Sánchez. 2017. A Survey on Deep Learning in Medical
    Image Analysis. *Medical Image Analysis* 42 (2017), 60–88.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等（2017）Geert Litjens、Thijs Kooi、Babak Ehteshami Bejnordi、Arnaud Arindra
    Adiyoso Setio、Francesco Ciompi、Mohsen Ghafoorian、Jeroen Awm Van Der Laak、Bram
    Van Ginneken 和 Clara I Sánchez。2017年。《关于医学图像分析中深度学习的调查》。*医学图像分析* 42（2017），60–88。
- en: Liu et al. (2021a) Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou.
    2021a. Exploring and Distilling Posterior and Prior Knowledge for Radiology Report
    Generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*. 13753–13762.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021a）Fenglin Liu、Xian Wu、Shen Ge、Wei Fan 和 Yuexian Zou。2021a。《探索和提炼放射学报告生成的后验和先验知识》。发表于
    *IEEE/CVF计算机视觉与模式识别会议*。13753–13762。
- en: 'Liu et al. (2021b) Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang,
    and Xu Sun. 2021b. Contrastive Attention for Automatic Chest X-ray Report Generation.
    In *Proceedings of the Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*. 269–280.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021b）Fenglin Liu、Changchang Yin、Xian Wu、Shen Ge、Ping Zhang 和 Xu Sun。2021b。《对比注意力用于自动胸部X射线报告生成》。发表于
    *计算语言学协会发现：ACL-IJCNLP 2021*。269–280。
- en: Liu et al. (2019) Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie
    Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. 2019. Clinically Accurate
    Chest X-Ray Report Generation. In *Machine Learning for Healthcare Conference*.
    249–269.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019）Guanxiong Liu、Tzu-Ming Harry Hsu、Matthew McDermott、Willie Boag、Wei-Hung
    Weng、Peter Szolovits 和 Marzyeh Ghassemi。2019年。《临床准确的胸部X射线报告生成》。发表于 *医疗保健机器学习会议*。249–269。
- en: Lopatina et al. (2020) Alina Lopatina, Stefan Ropele, Renat Sibgatulin, Jürgen R
    Reichenbach, and Daniel Güllmar. 2020. Investigation of Deep-Learning-Driven Identification
    of Multiple Sclerosis Patients Based on Susceptibility-Weighted Images Using Relevance
    Analysis. *Frontiers in N euroscience* (2020), 1356.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lopatina 等（2020）Alina Lopatina、Stefan Ropele、Renat Sibgatulin、Jürgen R Reichenbach
    和 Daniel Güllmar。2020年。《基于易损性加权图像的多发性硬化症患者深度学习驱动识别的相关性分析》。*前沿神经科学*（2020），1356。
- en: 'Lucieri et al. (2022) Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Alexander
    Braun, Muhammad Imran Malik, Andreas Dengel, and Sheraz Ahmed. 2022. ExAID: A
    Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions.
    *Computer Methods and Programs in Biomedicine* (2022), 106620.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucieri 等（2022）Adriano Lucieri、Muhammad Naseer Bajwa、Stephan Alexander Braun、Muhammad
    Imran Malik、Andreas Dengel 和 Sheraz Ahmed。2022年。《ExAID：一种用于计算机辅助诊断皮肤病变的多模态解释框架》。*计算方法与生物医学程序*（2022），106620。
- en: Lucieri et al. (2020) Adriano Lucieri, Muhammad Naseer Bajwa, Andreas Dengel,
    and Sheraz Ahmed. 2020. Explaining AI-based Decision Support Systems using Concept
    Localization Maps. In *Proceedings of the International Conference on Neural Information
    Processing*. 185–193.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucieri 等（2020）Adriano Lucieri、Muhammad Naseer Bajwa、Andreas Dengel 和 Sheraz
    Ahmed。2020年。《使用概念定位图解释基于AI的决策支持系统》。发表于 *神经信息处理国际会议*。185–193。
- en: Lundberg et al. (2018) Scott M Lundberg, Gabriel G Erion, and Su-In Lee. 2018.
    Consistent Individualized Feature Attribution for Tree Ensembles. *arXiv preprint
    arXiv:1802.03888* (2018).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 等（2018）Scott M Lundberg、Gabriel G Erion 和 Su-In Lee。2018年。《树集成的个性化特征归因一致性》。*arXiv
    预印本 arXiv:1802.03888*（2018）。
- en: Lundberg and Lee (2017) Scott M Lundberg and Su-In Lee. 2017. A Unified Approach
    to Interpreting Model Predictions. In *Proceedings of the International Conference
    on Neural Information Processing Systems (NIPS)*. 4768–4777.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 和 Lee（2017）Scott M Lundberg 和 Su-In Lee。2017年。《统一的模型预测解释方法》。发表于 *神经信息处理系统国际会议（NIPS）*。4768–4777。
- en: Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
    Attacks. In *Proceedings of the International Conference on Learning Representations
    (ICLR)*.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madry 等（2018）Aleksander Madry、Aleksandar Makelov、Ludwig Schmidt、Dimitris Tsipras
    和 Adrian Vladu。2018年。《朝着对抗攻击具有抵抗力的深度学习模型迈进》。发表于 *国际学习表征会议（ICLR）*。
- en: Magesh et al. (2020) Pavan Rajkumar Magesh, Richard Delwin Myloth, and Rijo Jackson
    Tom. 2020. An Explainable Machine Learning Model for Early Detection of Parkinson’s
    Disease using LIME on DaTscan Imagery. *Computers in Biology and Medicine* 126
    (2020), 104041.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magesh 等人 (2020) Pavan Rajkumar Magesh、Richard Delwin Myloth 和 Rijo Jackson
    Tom。2020年。一种基于 LIME 的可解释机器学习模型用于帕金森病的早期检测，应用于 DaTscan 图像。*生物医学计算机* 126 (2020),
    104041。
- en: 'Majkowska et al. (2020) Anna Majkowska, Sid Mittal, David F Steiner, Joshua J
    Reicher, Scott Mayer McKinney, Gavin E Duggan, Krish Eswaran, Po-Hsuan Cameron Chen,
    Yun Liu, Sreenivasa Raju Kalidindi, et al. 2020. Chest Radiograph Interpretation
    with Deep Learning Models: Assessment with Radiologist-adjudicated Reference Standards
    and Population-adjusted Evaluation. *Radiology* 294, 2 (2020), 421–431.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majkowska 等人 (2020) Anna Majkowska、Sid Mittal、David F Steiner、Joshua J Reicher、Scott
    Mayer McKinney、Gavin E Duggan、Krish Eswaran、Po-Hsuan Cameron Chen、Yun Liu、Sreenivasa
    Raju Kalidindi 等人。2020年。使用深度学习模型进行胸部 X 射线图像解释：使用放射科医生裁定的参考标准和调整后的人群评估。*放射学* 294,
    2 (2020), 421–431。
- en: 'Malhi et al. (2019) Avleen Malhi, Timotheus Kampik, Husanbir Pannu, Manik Madhikermi,
    and Kary Främling. 2019. Explaining Machine Learning-Based Classifications of
    In-Vivo Gastral Images. In *Digital Image Computing: Techniques and Applications
    (DICTA)*. 1–7.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malhi 等人 (2019) Avleen Malhi、Timotheus Kampik、Husanbir Pannu、Manik Madhikermi
    和 Kary Främling。2019年。解释基于机器学习的体内胃部图像分类。在 *数字图像计算：技术与应用 (DICTA)* 中。1–7。
- en: Margeloiu et al. (2020) Andrei Margeloiu, Nikola Simidjievski, Mateja Jamnik,
    and Adrian Weller. 2020. Improving Interpretability in Medical Imaging Diagnosis
    using Adversarial Training. In *Medical Imaging Meets NeurIPS Workshop*.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Margeloiu 等人 (2020) Andrei Margeloiu、Nikola Simidjievski、Mateja Jamnik 和 Adrian
    Weller。2020年。利用对抗训练提高医学影像诊断的可解释性。在 *医学影像与 NeurIPS 研讨会* 中。
- en: 'Matsoukas et al. (2021) Christos Matsoukas, Johan Fredin Haslum, Magnus Söderberg,
    and Kevin Smith. 2021. Is it Time to Replace CNNs with Transformers for Medical
    Images?. In *ICCV 2021: Workshop on Computer Vision for Automated Medical Diagnosis
    (CVAMD)*.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matsoukas 等人 (2021) Christos Matsoukas、Johan Fredin Haslum、Magnus Söderberg
    和 Kevin Smith。2021年。是时候用 Transformers 替代 CNNs 来处理医学图像了吗？在 *ICCV 2021：自动医学诊断计算机视觉研讨会
    (CVAMD)* 中。
- en: Mendonça et al. (2013) Teresa Mendonça, Pedro Ferreira, Jorge Marques, André
    Marcal, and Jorge Rozeira. 2013. PH 2-A dermoscopic image database for research
    and benchmarking. In *Proceedings of the International Conference of the IEEE
    Engineering in Medicine and Biology Society (EMBS)*. 5437–5440.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendonça 等人 (2013) Teresa Mendonça、Pedro Ferreira、Jorge Marques、André Marcal
    和 Jorge Rozeira。2013年。PH 2-用于研究和基准测试的皮肤镜图像数据库。在 *IEEE 医学与生物工程学会 (EMBS) 国际会议论文集*
    中。5437–5440。
- en: Messina et al. (2022) Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia
    Besa, Sergio Uribe, Marcelo Andía, Cristian Tejos, Claudia Prieto, and Daniel
    Capurro. 2022. A survey on deep learning and explainability for automatic report
    generation from medical images. *ACM Computing Surveys (CSUR)* 54, 10s (2022),
    1–40.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Messina 等人 (2022) Pablo Messina、Pablo Pino、Denis Parra、Alvaro Soto、Cecilia Besa、Sergio
    Uribe、Marcelo Andía、Cristian Tejos、Claudia Prieto 和 Daniel Capurro。2022年。关于深度学习和可解释性在医学图像自动报告生成中的应用的综述。*ACM
    计算调查 (CSUR)* 54, 10s (2022), 1–40。
- en: 'Molnar (2022) Christoph Molnar. 2022. Interpretable Machine Learning: A Guide
    for Making Black Box Models Explainable. Available: [h](h)ttps://christophm.github.io/interpretable-ml-book.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molnar (2022) Christoph Molnar。2022年。可解释的机器学习：使黑箱模型可解释的指南。可用：[h](h)ttps://christophm.github.io/interpretable-ml-book。
- en: Montavon et al. (2017) Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder,
    Wojciech Samek, and Klaus-Robert Müller. 2017. Explaining NonLinear Classification
    Decisions with Deep Taylor Decomposition. *Pattern Recognition* 65 (2017), 211–222.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montavon 等人 (2017) Grégoire Montavon、Sebastian Lapuschkin、Alexander Binder、Wojciech
    Samek 和 Klaus-Robert Müller。2017年。使用深度泰勒分解解释非线性分类决策。*模式识别* 65 (2017), 211–222。
- en: 'Moore and Swartout (1988) Johanna D Moore and William R Swartout. 1988. Explanation
    in Expert Systems: A Survey. *University of Southern California* (1988).'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moore 和 Swartout (1988) Johanna D Moore 和 William R Swartout。1988年。专家系统中的解释：一项综述。*南加州大学*
    (1988)。
- en: 'Moreira et al. (2012) Inês C Moreira, Igor Amaral, Inês Domingues, António
    Cardoso, Maria Joao Cardoso, and Jaime S Cardoso. 2012. INbreast: Toward a Full-field
    Digital Mammographic Database. *Academic Radiology* 19, 2 (2012), 236–248.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreira 等人 (2012) Inês C Moreira、Igor Amaral、Inês Domingues、António Cardoso、Maria
    Joao Cardoso 和 Jaime S Cardoso。2012年。INbreast：迈向全幅数字化乳腺摄影数据库。*学术放射学* 19, 2 (2012),
    236–248。
- en: Nevitt et al. (2006) M Nevitt, D Felson, and Gayle Lester. 2006. The Osteoarthritis
    Initiative. *Protocol for the Cohort Study* 1 (2006).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nevitt 等 (2006) M Nevitt, D Felson 和 Gayle Lester. 2006. 骨关节炎倡议。*队列研究协议* 1 (2006)。
- en: 'Nguyen et al. (2020) Ha Nguyen, Khanh Lam, Linh Le, Hieu Pham, Dat Tran, et al.
    2020. VinDr-CXR: An Open Dataset of Chest X-rays with Radiologist’s Annotations.
    *arXiv preprint arXiv:2012.15029* (2020).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen 等 (2020) Ha Nguyen, Khanh Lam, Linh Le, Hieu Pham, Dat Tran 等. 2020.
    VinDr-CXR: 一个带有放射科医师注释的胸部 X 射线开放数据集。*arXiv 预印本 arXiv:2012.15029* (2020)。'
- en: 'Nguyen et al. (2021) Hieu T. Nguyen, Hieu H. Pham, Nghia T. Nguyen, Ha Q. Nguyen,
    Thang Q. Huynh, et al. 2021. VinDr-SpineXR: A Deep Learning Framework for Spinal
    Lesions Detection and Classification from Radiographs. In *Proceedings of the
    International Conference on Medical Image Computing and Computer Assisted Intervention
    (MICCAI)*. 291–301.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen 等 (2021) Hieu T. Nguyen, Hieu H. Pham, Nghia T. Nguyen, Ha Q. Nguyen,
    Thang Q. Huynh 等. 2021. VinDr-SpineXR: 用于从放射线图像中检测和分类脊柱病变的深度学习框架。在 *国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*。291–301。'
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In
    *Proceedings of the Annual Meeting of the Association for Computational Linguistics
    (ACL)*. 311–318.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等 (2002) Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing Zhu.
    2002. BLEU：一种自动评估机器翻译的方法。在 *计算语言学协会年会论文集（ACL）*。311–318。
- en: Pascanu et al. (2013) Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013.
    On the difficulty of training Recurrent Neural Networks. In *Proceedings of the
    International Conference on Machine Learning (ICML)*. 1310–1318.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu 等 (2013) Razvan Pascanu, Tomas Mikolov 和 Yoshua Bengio. 2013. 训练递归神经网络的难度。在
    *国际机器学习会议（ICML）论文集*。1310–1318。
- en: Patrício et al. (2023) Cristiano Patrício, João C. Neves, and Luis F. Teixeira.
    2023. Coherent Concept-based Explanations in Medical Image and Its Application
    to Skin Lesion Diagnosis. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW)*. 3799–3808.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrício 等 (2023) Cristiano Patrício, João C. Neves 和 Luis F. Teixeira. 2023.
    医学图像中的一致性概念解释及其在皮肤病变诊断中的应用。在 *IEEE/CVF 计算机视觉与模式识别会议研讨会（CVPRW）论文集*。3799–3808。
- en: 'Pelka et al. (2018) Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa,
    et al. 2018. Radiology Objects in COntext (ROCO): A Multimodal Image Dataset.
    In *Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation
    of Biomedical Data and Expert Label Synthesis*. 180–189.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pelka 等 (2018) Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa 等. 2018.
    胶囊放射学对象 (ROCO)：多模态图像数据集。在 *血管内成像与计算机辅助支架植入及生物医学数据的大规模注释与专家标签合成*。180–189。
- en: 'Petsiuk et al. (2018) Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. RISE:
    Randomized Input Sampling for Explanation of Black-box Models. In *British Machine
    Vision Conference (BMVC)*.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Petsiuk 等 (2018) Vitali Petsiuk, Abir Das 和 Kate Saenko. 2018. RISE: 随机输入采样用于解释黑箱模型。在
    *英国机器视觉会议（BMVC）*。'
- en: 'Pocevičiūtė et al. (2020) Milda Pocevičiūtė, Gabriel Eilertsen, and Claes Lundström.
    2020. Survey of XAI in Digital Pathology. In *Artificial Intelligence and Machine
    Learning for Digital Pathology: State-of-the-Art and Future Challenges*. 56–88.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pocevičiūtė 等 (2020) Milda Pocevičiūtė, Gabriel Eilertsen 和 Claes Lundström.
    2020. 数字病理学中的 XAI 调查。在 *人工智能与机器学习在数字病理学中的应用：前沿技术与未来挑战*。56–88。
- en: Punn and Agarwal (2021) Narinder Singh Punn and Sonali Agarwal. 2021. Automated
    diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned
    deep neural networks. *Applied Intelligence* 51, 5 (2021), 2689–2702.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Punn 和 Agarwal (2021) Narinder Singh Punn 和 Sonali Agarwal. 2021. 使用微调的深度神经网络自动诊断
    COVID-19，基于有限的后前位胸部 X 射线图像。*应用智能* 51, 5 (2021), 2689–2702。
- en: 'Rajpurkar et al. (2018) Pranav Rajpurkar, Jeremy Irvin, Robyn L Ball, Kaylie
    Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, et al. 2018.
    Deep learning for chest radiograph diagnosis: A retrospective comparison of the
    CheXNeXt algorithm to practicing radiologists. *PLoS medicine* 15, 11 (2018).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等 (2018) Pranav Rajpurkar, Jeremy Irvin, Robyn L Ball, Kaylie Zhu,
    Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul 等. 2018. 胸部 X
    射线诊断中的深度学习：CheXNeXt 算法与执业放射科医师的回顾性比较。*PLoS 医学* 15, 11 (2018)。
- en: Rehman et al. (2021) Aasia Rehman, Muheet Ahmed Butt, and Majid Zaman. 2021.
    A Survey of Medical Image Analysis Using Deep Learning Approaches. In *Proceedings
    of the IEEE International Conference on Computing Methodologies and Communication
    (ICCMC)*. 1334–1342.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rehman et al. (2021) Aasia Rehman, Muheet Ahmed Butt, 和 Majid Zaman. 2021. 使用深度学习方法的医学图像分析调查。载于*IEEE国际计算方法与通信会议（ICCMC）论文集*。1334–1342。
- en: 'Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier.
    In *Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*. 1135–1144.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, 和 Carlos Guestrin.
    2016. “我为什么应该信任你？”：解释任何分类器的预测。载于*ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1135–1144。
- en: 'Richter et al. (2020) Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco
    Ruiz, and Omer Deniz Akyildiz. 2020. VarGrad: A Low-Variance Gradient Estimator
    for Variational Inference. *Advances in Neural Information Processing Systems*
    33 (2020), 13481–13492.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richter et al. (2020) Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco
    Ruiz, 和 Omer Deniz Akyildiz. 2020. VarGrad：一种低方差梯度估计器用于变分推断。*神经信息处理系统进展* 33 (2020)，13481–13492。
- en: 'Rio-Torto et al. (2020) Isabel Rio-Torto, Kelwin Fernandes, and Luís Teixeira.
    2020. Understanding the decisions of CNNs: An in-model approach. *Pattern Recognition
    Letters* 133 (2020), 373–380.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rio-Torto et al. (2020) Isabel Rio-Torto, Kelwin Fernandes, 和 Luís Teixeira.
    2020. 理解CNN的决策：一种模型内的方法。*模式识别快报* 133 (2020)，373–380。
- en: 'Robertson (2004) Stephen Robertson. 2004. Understanding Inverse Document Frequency:
    on Theoretical Arguments for IDF. *Journal of Documentation* 60 (2004), 503–520.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson (2004) Stephen Robertson. 2004. 理解逆文档频率：关于IDF的理论论证。*文献学杂志* 60 (2004)，503–520。
- en: Rotemberg et al. (2021) Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein,
    Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza,
    Pascale Guitera, David Gutman, et al. 2021. A Patient-Centric Dataset of Images
    and Metadata for Identifying Melanomas Using Clinical Context. *Scientific Data*
    8, 1 (2021), 1–8.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rotemberg et al. (2021) Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein,
    Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza,
    Pascale Guitera, David Gutman, 等. 2021. 一种以患者为中心的图像和元数据数据集，用于在临床背景下识别黑色素瘤。*科学数据*
    8, 1 (2021)，1–8。
- en: Rudin (2019) Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning
    Models for High Stakes Decisions and Use Interpretable Models Instead. *Nature
    Machine Intelligence* 1, 5 (2019), 206–215.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudin (2019) Cynthia Rudin. 2019. 停止解释黑箱机器学习模型用于高风险决策，改用可解释的模型。*自然机器智能* 1, 5
    (2019)，206–215。
- en: Sadeghi et al. (2018) Mahya Sadeghi, Parmit K Chilana, and M Stella Atkins.
    2018. How Users Perceive Content-based Image Retrieval for Identifying Skin Images.
    In *Understanding and Interpreting Machine Learning in Medical Image Computing
    Applications*. 141–148.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sadeghi et al. (2018) Mahya Sadeghi, Parmit K Chilana, 和 M Stella Atkins. 2018.
    用户如何感知基于内容的图像检索用于识别皮肤图像。载于*理解与解释医学图像计算应用中的机器学习*。141–148。
- en: 'Salahuddin et al. (2022) Zohaib Salahuddin, Henry C. Woodruff, Avishek Chatterjee,
    and Philippe Lambin. 2022. Transparency of Deep Neural Networks for Medical Image
    Analysis: A Review of Interpretability Methods. *Computers in Biology and Medicine*
    140 (2022), 105111.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salahuddin et al. (2022) Zohaib Salahuddin, Henry C. Woodruff, Avishek Chatterjee,
    和 Philippe Lambin. 2022. 深度神经网络在医学图像分析中的透明度：解释方法的综述。*生物医学计算机* 140 (2022)，105111。
- en: Samek et al. (2016) Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian
    Lapuschkin, and Klaus-Robert Müller. 2016. Evaluating the Visualization of What
    a Deep Neural Network Has Learned. *IEEE Transactions on Neural Networks and Learning
    Systems* 28, 11 (2016), 2660–2673.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samek et al. (2016) Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian
    Lapuschkin, 和 Klaus-Robert Müller. 2016. 评估深度神经网络所学内容的可视化。*IEEE神经网络与学习系统学报* 28,
    11 (2016)，2660–2673。
- en: 'Sanchez et al. (2022) Pedro Sanchez, Antanas Kascenas, Xiao Liu, Alison Q O’Neil,
    and Sotirios A Tsaftaris. 2022. What is healthy? generative counterfactual diffusion
    for lesion localization. In *Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI
    2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings*.
    34–44.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez et al. (2022) Pedro Sanchez, Antanas Kascenas, Xiao Liu, Alison Q O’Neil,
    和 Sotirios A Tsaftaris. 2022. 什么是健康？生成性反事实扩散用于病变定位。载于*深度生成模型：第二届MICCAI研讨会，DGM4MICCAI
    2022，与MICCAI 2022同步举行，新加坡，2022年9月22日，会议论文集*。34–44。
- en: Sarkar et al. (2022) Anirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, and
    Vineeth N Balasubramanian. 2022. A Framework for Learning Ante-hoc Explainable
    Models via Concepts. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*. 10286–10295.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarkar 等（2022）**安尼尔班·萨尔卡尔**、**迪帕克·维贾凯尔提**、**安尼利亚·萨尔卡尔**和**维尼斯·N·巴拉苏布拉马尼安**。2022。通过概念学习前瞻性可解释模型的框架。见
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*，10286–10295。
- en: Sayres et al. (2019) Rory Sayres, Ankur Taly, Ehsan Rahimy, Katy Blumer, David
    Coz, Naama Hammel, Jonathan Krause, Arunachalam Narayanaswamy, Zahra Rastegar,
    Derek Wu, et al. 2019. Using a Deep Learning Algorithm and Integrated Gradients
    Explanation to Assist Grading for Diabetic Retinopathy. *Ophthalmology* 126, 4
    (2019), 552–564.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sayres 等（2019）**罗里·塞耶斯**、**安库尔·塔利**、**艾赫桑·拉希米**、**凯蒂·布卢默**、**大卫·科兹**、**纳阿玛·哈梅尔**、**乔纳森·克劳斯**、**阿鲁纳查拉姆·纳拉亚纳斯瓦米**、**扎赫拉·拉斯特加**、**德里克·吴**
    等。2019。使用深度学习算法和集成梯度解释来辅助糖尿病视网膜病变评分。 *眼科学* 126，4（2019），552–564。
- en: Schlegl et al. (2015) Thomas Schlegl, Sebastian M Waldstein, Wolf-Dieter Vogl,
    Ursula Schmidt-Erfurth, and Georg Langs. 2015. Predicting Semantic Descriptions
    from Medical Images with Convolutional Neural Networks. In *Proceedings of the
    International Conference on Information Processing in Medical Imaging (IPMI)*.
    437–448.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlegl 等（2015）**托马斯·施莱格尔**、**塞巴斯蒂安·M·瓦尔德斯坦**、**沃尔夫-迪特尔·福格尔**、**乌尔苏拉·施密特-厄尔弗斯**和**乔治·朗斯**。2015。利用卷积神经网络从医学图像中预测语义描述。见
    *国际医学影像信息处理会议（IPMI）论文集*，437–448。
- en: Schutte et al. (2021) Kathryn Schutte, Olivier Moindrot, Paul Hérent, Jean-Baptiste
    Schiratti, and Simon Jégou. 2021. Using StyleGAN for Visual Interpretability of
    Deep Learning Models on Medical Images. In *Medical Imaging Meets NeurIPS Workshop*.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schutte 等（2021）**凯瑟琳·舒特**、**奥利维耶·莫安德罗**、**保罗·赫伦特**、**让-巴普蒂斯特·希拉提**和**西蒙·杰戈**。2021。使用
    StyleGAN 对深度学习模型在医学图像上的视觉可解释性。见 *医学影像与 NeurIPS 研讨会*。
- en: Selivanov et al. (2023) Alexander Selivanov, Oleg Y Rogov, Daniil Chesakov,
    Artem Shelmanov, Irina Fedulova, and Dmitry V Dylov. 2023. Medical image captioning
    via generative pretrained transformers. *Scientific Reports* 13, 1 (2023), 4171.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selivanov 等（2023）**亚历山大·塞利瓦诺夫**、**奥列格·Y·罗戈夫**、**达尼尔·切萨科夫**、**阿尔忒弥斯·谢尔曼诺夫**、**伊琳娜·费杜洛娃**和**德米特里·V·迪洛夫**。2023。通过生成预训练变换器进行医学图像字幕生成。*科学报告*
    13，1（2023），4171。
- en: 'Selvaraju et al. (2017) Ramprasaath R Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-based Localization. In *Proceedings
    of the IEEE International Conference on Computer Vision (ICCV)*. 618–626.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selvaraju 等（2017）**拉姆普拉萨斯·R·塞尔瓦拉朱**、**迈克尔·科斯维尔**、**阿比谢克·达斯**、**拉马克里希纳·维丹塔姆**、**德维·帕里克**和**德鲁夫·巴特拉**。2017。Grad-CAM：通过基于梯度的定位从深度网络中获得视觉解释。见
    *IEEE 国际计算机视觉会议（ICCV）论文集*，618–626。
- en: 'Shamshad et al. (2023) Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris
    Khan, Munawar Hayat, Fahad Shahbaz Khan, and Huazhu Fu. 2023. Transformers in
    Medical Imaging: A Survey. *Medical Image Analysis* (2023), 102802.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamshad 等（2023）**法哈德·沙姆沙德**、**萨尔曼·汗**、**赛义德·瓦卡斯·扎米尔**、**穆罕默德·哈里斯·汗**、**穆纳瓦尔·哈亚特**、**法哈德·沙赫巴兹·汗**和**华祝·傅**。2023。医学影像中的变换器：一项调查。*医学影像分析*（2023），102802。
- en: Shapley (2016) L. S. Shapley. 2016. A Value for n-Person Games. In *Contributions
    to the Theory of Games (AM-28)*, Harold William Kuhn and Albert William Tucker
    (Eds.). Vol. II. 307–318.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley（2016）**L. S. Shapley**。2016。n 人游戏的价值。见 *博弈理论的贡献（AM-28）*，**哈罗德·威廉·库恩**和**阿尔伯特·威廉·塔克**（编）。第
    II 卷，307–318。
- en: Shrikumar et al. (2017) Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
    2017. Learning Important Features Through Propagating Activation Differences.
    In *Proceedings of the International Conference on Machine Learning (ICML)*, Vol. 70\.
    3145–3153.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrikumar 等（2017）**阿万提·施里库马尔**、**佩顿·格林赛德**和**安舒尔·昆达杰**。2017。通过传播激活差异学习重要特征。见
    *国际机器学习会议（ICML）论文集*，第 70 卷，3145–3153。
- en: Silva et al. (2020) Wilson Silva, Alexander Poellinger, Jaime S Cardoso, and
    Mauricio Reyes. 2020. Interpretability-guided Content-based Medical Image Retrieval.
    In *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*. 305–314.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva 等（2020）**威尔逊·席尔瓦**、**亚历山大·波林格**、**贾梅·S·卡多索**和**毛里西奥·雷耶斯**。2020。以可解释性为指导的基于内容的医学图像检索。见
    *国际医学图像计算与计算机辅助手术会议（MICCAI）论文集*，305–314。
- en: 'Simonyan et al. (2014) Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
    2014. Deep Inside Convolutional Networks: Visualising Image Classification Models
    and Saliency Maps. In *Workshop at International Conference on Learning Representations
    (ICLR)*.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 等（2014）**卡伦·西蒙尼扬**、**安德烈亚·维达尔迪**和**安德鲁·齐瑟曼**。2014。深度卷积网络内部：可视化图像分类模型和显著性图。见
    *国际学习表征会议（ICLR）研讨会*。
- en: Singh et al. (2020) Amitojdeep Singh, Sourya Sengupta, and Vasudevan Lakshminarayanan.
    2020. Explainable Deep Learning Models in Medical Image Analysis. *Journal of
    Imaging* 6, 6 (2020), 52.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Yow (2021a) Gurmail Singh and Kin-Choong Yow. 2021a. An Interpretable
    Deep Learning Model For Covid-19 Detection With Chest X-ray Images. *IEEE Access*
    9 (2021), 85198–85208.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh and Yow (2021b) Gurmail Singh and Kin-Choong Yow. 2021b. These do not
    Look Like Those: An Interpretable Deep Learning Model for Image Recognition. *IEEE
    Access* 9 (2021), 41482–41493.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2019) Sonit Singh, Sarvnaz Karimi, Kevin Ho-Shon, and Len Hamey.
    2019. From Chest X-Rays to Radiology Reports: A Multimodal Machine Learning Approach.
    In *Digital Image Computing: Techniques and Applications (DICTA)*. 1–8.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singla et al. (2023) Sumedha Singla, Motahhare Eslami, Brian Pollack, Stephen
    Wallace, and Kayhan Batmanghelich. 2023. Explaining the Black-box Smoothly - A
    Counterfactual Approach. *Medical Image Analysis* 84 (2023), 102721.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smilkov et al. (2017) Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas,
    and Martin Wattenberg. 2017. SmoothGrad: Removing Noise by Adding Noise. In *ICML
    Workshop on Visualization for Deep Learning*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Society (2019) Asia Pacific Tele-Ophthalmology Society. 2019. APTOS Diabetic
    Retinopathy Dataset. [Online]. Accessed November, 23 2021\. Available: [https://www.kaggle.com/c/aptos2019-blindness-detection/data](https://www.kaggle.com/c/aptos2019-blindness-detection/data).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spanhol et al. (2015) Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean,
    and Laurent Heutte. 2015. A Dataset for Breast Cancer Histopathological Image
    Classification. *IEEE Transactions on Biomedical Engineering* 63, 7 (2015), 1455–1462.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Springenberg et al. (2014) Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
    Brox, and Martin Riedmiller. 2014. Striving for Simplicity: The All Convolutional
    Net. *arXiv preprint arXiv:1412.6806* (2014).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Li Sun, Weipeng Wang, Jiyun Li, and Jingsheng Lin. 2019. Study
    on Medical Image Report Generation Based on Improved Encoding-Decoding Method.
    In *Intelligent Computing Theories and Application*. 686–696.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
    Axiomatic Attribution for Deep Networks. In *Proceedings of the International
    Conference on Machine Learning (ICML)*. 3319–3328.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thiagarajan et al. (2021) Ponkrshnan Thiagarajan, Pushkar Khairnar, and Susanta
    Ghosh. 2021. Explanation and Use of Uncertainty Obtained by Bayesian Neural Network
    Classifiers for Breast Histopathology Images. *IEEE Transactions on Medical Imaging*
    41, 4 (2021), 815–825.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjoa and Guan (2020) Erico Tjoa and Cuntai Guan. 2020. A Survey on Explainable
    Artificial Intelligence (XAI): Towards Medical XAI. *IEEE Transactions on Neural
    Networks and Learning Systems* 32, 11 (2020), 4793–4813.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tonekaboni et al. (2019) Sana Tonekaboni, Shalmali Joshi, Melissa D McCradden,
    and Anna Goldenberg. 2019. What Clinicians Want: Contextualizing Explainable Machine
    Learning for Clinical End Use. In *Proceedings of the Machine Learning for Healthcare
    Conference*. 359–380.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tschandl et al. (2019) Philipp Tschandl, Giuseppe Argenziano, Majid Razmara,
    and Jordan Yap. 2019. Diagnostic Accuracy of Content Based Dermatoscopic Image
    Retrieval with Deep Classification Features. *British Journal of Dermatology*
    181, 1 (2019), 155–165.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler.
    2018. The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images
    of Common Pigmented Skin Lesions. *Scientific Data* 5, 1 (2018), 1–9.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsiknakis et al. (2020) Nikos Tsiknakis, Eleftherios Trivizakis, Evangelia E
    Vassalou, Georgios Z Papadakis, Demetrios A Spandidos, Aristidis Tsatsakis, Jose
    Sánchez-García, Rafael López-González, Nikolaos Papanikolaou, Apostolos H Karantanas,
    et al. 2020. Interpretable artificial intelligence framework for COVID-19 screening
    on chest X-rays. *Experimental and Therapeutic Medicine* 20, 2 (2020), 727–735.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton.
    2008. Visualizing Data using t-SNE. *Journal of Machine Learning Research* 9,
    11 (2008).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Velden et al. (2022) Bas H.M. van der Velden, Hugo J. Kuijf, Kenneth G.A.
    Gilhuijs, and Max A. Viergever. 2022. Explainable Artificial Intelligence (XAI)
    in Deep Learning-based Medical Image Analysis. *Medical Image Analysis* 79 (2022),
    102470.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Lent et al. (2004) Michael Van Lent, William Fisher, and Michael Mancuso.
    2004. An Explainable Artificial Intelligence System for Small-unit Tactical Behavior.
    In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*.
    900–907.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    Is All You Need. In *Advances in Neural Information Processing Systems*. 5998–6008.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    2015. CIDEr: Consensus-based Image Description Evaluation. In *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 4566–4575.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Linda Wang, Zhong Qiu Lin, and Alexander Wong. 2020a. COVID-Net:
    a tailored deep convolutional neural network design for detection of COVID-19
    cases from chest X-ray images. *Scientific Reports* 10, 1 (2020), 1–12.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell
    Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William
    Merrill, et al. 2020b. CORD-19: The COVID-19 Open Research Dataset. *ArXiv* (2020).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Sutong Wang, Yunqiang Yin, Dujuan Wang, Yanzhang Wang, and
    Yaochu Jin. 2021. Interpretability-Based Multimodal Convolutional Neural Networks
    for Skin Lesion Diagnosis. *IEEE Transactions on Cybernetics* (2021).
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi
    Bagheri, and Ronald Summers. 2017. ChestX-ray8: Hospital-scale Chest X-ray Database
    and Benchmarks on Weakly-Supervised Classification and Localization of Common
    Thorax Diseases. In *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 2097–2106.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald
    Summers. 2018. TieNet: Text-Image Embedding Network for Common Thorax Disease
    Classification and Reporting in Chest X-rays. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 9049–9058.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. 2023.
    METransformer: Radiology Report Generation by Transformer with Multiple Learnable
    Expert Tokens. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 11558–11567.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Zhanyu Wang, Mingkang Tang, Lei Wang, Xiu Li, and Luping
    Zhou. 2022. A Medical Semantic-Assisted Transformer for Radiographic Report Generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    III*. 655–664.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wickramanayake et al. (2021a) Sandareka Wickramanayake, Wynne Hsu, and Mong Li
    Lee. 2021a. Comprehensible Convolutional Neural Networks via Guided Concept Learning.
    In *Proceedings of the IEEE International Joint Conference on Neural Networks
    (IJCNN)*. 1–8.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wickramanayake et al. (2021b) Sandareka Wickramanayake, Wynne Hsu, and Mong Li
    Lee. 2021b. Explanation-based Data Augmentation for Image Classification. In *Advances
    in Neural Information Processing Systems*, Vol. 34.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windisch et al. (2020) Paul Windisch, Pascal Weber, Christoph Fürweger, Felix
    Ehret, Markus Kufeld, Daniel Zwahlen, and Alexander Muacevic. 2020. Implementation
    of model explainability for a basic brain tumor detection using convolutional
    neural networks on MRI slices. *Neuroradiology* 62, 11 (2020), 1515–1518.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell:
    Neural image caption generation with visual attention. In *International Conference
    on Machine Learning (ICML)*. 2048–2057.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Yesheng Xu, Ming Kong, Wenjia Xie, Runping Duan, Zhengqing
    Fang, Yuxiao Lin, Qiang Zhu, Siliang Tang, Fei Wu, and Yu-Feng Yao. 2021. Deep
    Sequential Feature Learning in Clinical Image Classification of Infectious Keratitis.
    *Engineering* 7, 7 (2021), 1002–1010.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2023) Siyuan Yan, Zhen Yu, Xuelin Zhang, Dwarikanath Mahapatra,
    Shekhar S Chandra, Monika Janda, Peter Soyer, and Zongyuan Ge. 2023. Towards Trustable
    Skin Cancer Diagnosis via Rewriting Model’s Decision. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. 11568–11577.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Shuxin Yang, Xian Wu, Shen Ge, Zhuozhao Zheng, S Kevin Zhou,
    and Li Xiao. 2023. Radiology report generation with a learned knowledge base and
    multi-modal alignment. *Medical Image Analysis* (2023), 102798.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2019) Changchang Yin, Buyue Qian, Jishang Wei, Xiaoyu Li, Xianli
    Zhang, Yang Li, and Qinghua Zheng. 2019. Automatic Generation of Medical Imaging
    Diagnostic Report with Hierarchical Recurrent Neural Network. In *Proceedings
    of the IEEE International Conference on Data Mining (ICDM)*. 728–737.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2019) Kyle Young, Gareth Booth, Becks Simpson, Reuben Dutton,
    and Sally Shrapnel. 2019. Deep Neural Network or Dermatologist? In *Interpretability
    of Machine Intelligence in Medical Image Computing and Multimodal Learning for
    Clinical Decision Support*. 48–55.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Hang Yu, Laurence T Yang, Qingchen Zhang, David Armstrong,
    and M Jamal Deen. 2021. Convolutional Neural Networks for Medical Image Analysis.
    *Neurocomputing* 444 (2021), 92–110.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuksekgonul et al. (2022) Mert Yuksekgonul, Maggie Wang, and James Zou. 2022.
    Post-hoc concept bottleneck models. *arXiv preprint arXiv:2205.15480* (2022).
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler and Fergus (2014) Matthew D Zeiler and Rob Fergus. 2014. Visualizing
    and Understanding Convolutional Networks. In *Proceedings of the European Conference
    on Computer Vision (ECCV)*. 818–833.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Junsan Zhang, Xiuxuan Shen, Shaohua Wan, Sotirios K Goudos,
    Jie Wu, Ming Cheng, and Weishan Zhang. 2023. A Novel Deep Learning Model for Medical
    Report Generation by Inter-Intra Information Calibration. *IEEE Journal of Biomedical
    and Health Informatics* (2023).
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, and
    Lin Yang. 2017. MDNet: A Semantically and Visually Interpretable Medical Image
    Diagnosis Network. In *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 6428–6436.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. 2016. Learning Deep Features for Discriminative Localization.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 2921–2929.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1\. Intepretability Frameworks
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The increasing interest in the interpretability of machine learning fostered
    the development of frameworks implementing classical XAI methods. The LRP Toolbox ([70](#bib.bib70))
    was launched in 2016 and provides the implementation of the LRP ([10](#bib.bib10))
    algorithm for artificial neural networks supporting Matlab and Python. Additionally,
    the LRP toolbox released an extension to be compatible with the Caffe Deep Learning
    framework. DeepExplain ([7](#bib.bib7)) is a framework that implements perturbation
    and gradient-based attribution methods, including Saliency Maps ([133](#bib.bib133)),
    Gradient * Input ([131](#bib.bib131)), Integrated Gradients (IG) ([144](#bib.bib144)),
    DeepLIFT ([131](#bib.bib131)), $\epsilon$-LRP ([10](#bib.bib10)), and DeConvNet ([174](#bib.bib174)).
    It also supports Tensorflow as well as Keras with Tensorflow backend. Alternatively,
    iNNvestigate ([5](#bib.bib5)) is a more complete toolbox that provides implementations
    for SmoothGrad ([139](#bib.bib139)), DeConvNet ([174](#bib.bib174)), Guided-BackProp ([142](#bib.bib142)),
    PatternNet ([63](#bib.bib63)), Input * Gradients ([131](#bib.bib131)), DeepTaylor ([98](#bib.bib98)),
    PatternAttribution ([63](#bib.bib63)), LRP ([10](#bib.bib10)) and Integrated Gradients
    (IG) ([144](#bib.bib144)). It also supports Tensorflow and Keras.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to PyTorch frameworks, TorchRay ([37](#bib.bib37)) implements several
    visualization methods, namely Gradient ([133](#bib.bib133)), Guided-BackProp ([142](#bib.bib142)),
    Grad-CAM ([128](#bib.bib128)) and RISE ([108](#bib.bib108)). TorchRay is considered
    research-oriented, since it provides code for reproducing results that appear
    in several papers. Captum ([65](#bib.bib65)) is a PyTorch library that provides
    state-of-the-art algorithms for model interpretability and understanding. It contains
    general purpose implementations of Integrated Gradients ([144](#bib.bib144)),
    SmoothGrad ([139](#bib.bib139)), VarGrad ([114](#bib.bib114)) and others for PyTorch
    models. Table [4](#A1.T4 "Table 4 ‣ A.1\. Intepretability Frameworks ‣ Appendix
    A Appendix ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") summarizes the aforementioned frameworks alongside the supported XAI
    methods. Additionally, we refer the reader to the work of Darias et al. ([30](#bib.bib30))
    in which some other model-agnostic XAI libraries were approached.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Publicly available interpretability frameworks.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Year | Methods | Supported DL Libraries |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| LRP Toolbox ([70](#bib.bib70)) | 2016 | LRP | Caffe |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| DeepExplain ([7](#bib.bib7)) | 2017 | Saliency maps, Grad * Input, $\epsilon$-LRP,
    DeepLIFT, DeConvNet | Tensorflow, Keras |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| iNNvestigate ([5](#bib.bib5)) | 2019 | SmoothGrad, DeConvNet, Guided-BackProp,
    PatternNet, LRP Input * Gradients, DeepTaylor, PatternAttribution, IG | Tensorflow,
    Keras |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| TorchRay ([37](#bib.bib37)) | 2019 | Gradient, Guided-BackProp, Grad-CAM,
    RISE | PyTorch |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| Captum ([65](#bib.bib65)) | 2019 | SmoothGrad, DeConvNet, Guided-BackProp,
    PatternNet, LRP Input * Gradients, DeepLIFT, DeepTaylor, LIME, Kernel SHAP, IG
    | PyTorch |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: A.2\. Methods
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 5\. Summary of the XAI methods categorized by interpretability method
    employed, image modality and dataset. The spaces marked with a ”-” mean that explanation
    is only provided through text sentences.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Interpretability Method | Modality | Dataset |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([176](#bib.bib176)) | 2017 | Attention-based | Microscopic
    Images | BCIDR |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| Jing et al. ([55](#bib.bib55)) | 2017 | Attention-based | X-ray | IU Chest
    X-ray, PEIR Gross |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| Rajpurkar et al. ([111](#bib.bib111)) | 2018 | CAM | X-ray | ChestX-ray8
    |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([160](#bib.bib160)) | 2018 | Saliency maps | X-ray | IU Chest
    X-ray,ChestX-ray14 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| Gale et al. ([39](#bib.bib39)) | 2018 | SmoothGrad | X-ray | Pelvic X-ray
    |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([74](#bib.bib74)) | 2018 | Text-based | X-ray | IU Chest X-ray,
    CX-CHR (private) |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| Malhi et al. ([92](#bib.bib92)) | 2019 | LIME | Endoscopy | Red Lesion Endoscopy
    |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| Young et al. ([171](#bib.bib171)) | 2019 | KernelSHAP | Dermoscopy | HAM10000
    |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| Eitel et al. ([35](#bib.bib35)) | 2019 | Occlusion | MRI | ADNI Initiative
    |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| Sayres et al. ([124](#bib.bib124)) | 2019 | Integrated Gradients | Fundus
    Images | Private |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| Barata et al. ([12](#bib.bib12)) | 2019 | CAM | Dermoscopy | ISIC 2017, ISIC
    2018 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| Tschandl et al. ([148](#bib.bib148)) | 2019 | CBIR | Dermoscopy | EDRA, ISIC
    2017, Private Dataset |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| Sun et al. ([143](#bib.bib143)) | 2019 | Text-based | Mammography | Inbreast
    |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. ([72](#bib.bib72)) | 2019 | Saliency maps | Mammography | CBIS-DDSM
    |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| Lamy et al. ([69](#bib.bib69)) | 2019 | CBR | Mammography | BCW, Mammographic
    Mass, Breast Cancer |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([137](#bib.bib137)) | 2019 | Stacked LSTM | X-ray | IU Chest
    X-ray |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| Yin et al. ([170](#bib.bib170)) | 2019 | t-SNE | X-ray | IU Chest X-ray |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([83](#bib.bib83)) | 2019 | Attention maps | X-ray | IU Chest
    X-ray, MIMIC-CXR |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| Windish et al. ([165](#bib.bib165)) | 2020 | Grad-CAM | MRI | IXI, Gliobastoma
    |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| Magesh et al. ([90](#bib.bib90)) | 2020 | LIME | DaTscan | PPMI |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([77](#bib.bib77)) | 2020 | Guided Grad-CAM | X-ray | COVIDx |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| Lopatina et al. ([84](#bib.bib84)) | 2020 | DeepLIFT | MRI | Private |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| Graziani et al. ([44](#bib.bib44)) | 2020 | TCAV | Microscopic Images | Camelyon16,
    Camelyon17 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| Margeloiu et al. ([93](#bib.bib93)) | 2020 | Adversarial Training | Dermoscopy
    | HAM10000 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| Rio-Torto et al. ([115](#bib.bib115)) | 2020 | In-model | Microscopic Images
    | NIH-NCI Cervical Cancer |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. ([36](#bib.bib36)) | 2020 | Concept-based | Microscopic Images
    | Infectious Keratitis Dataset |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([25](#bib.bib25)) | 2020 | Multi-Head Attention | X-ray | IU
    Chest X-ray, MIMIC-CXR |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| Silva et al. ([132](#bib.bib132)) | 2020 | CBIR, Saliency Map | X-ray | CheXpert
    |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| Punn et al. ([110](#bib.bib110)) | 2021 | LIME | X-ray | COVID-19 Dataset
    |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([158](#bib.bib158)) | 2021 | SHAP | Dermoscopy | HAM10000 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| Billah and Javed ([16](#bib.bib16)) | 2021 | BCNN | Microscopy Images | ALL-IDB
    |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| Barata et al. ([13](#bib.bib13)) | 2021 | CBIR | Dermoscopy | ISIC 2018 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| Thiagarajan et al. ([145](#bib.bib145)) | 2021 | t-SNE | Microscopic Images
    | Breast Histopathology |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| Barnett et al. ([14](#bib.bib14)) | 2021 | Prototype Activation Map | Mammography
    | Mammography Private Dataset |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. ([62](#bib.bib62)) | 2021 | Counterfactual Examples | X-ray |
    Chest X-ray 14, VinDr-CXR |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| Schutte et al. ([126](#bib.bib126)) | 2021 | Grad-CAM | X-ray/Microscopy
    Images | Osteoarthritis X-ray, Camelyon16 |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. ([61](#bib.bib61)) | 2021 | Saliency maps | X-ray | NIH chest
    X-ray14 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([135](#bib.bib135)) | 2021 | Prototype Activation Maps | X-ray
    | CORD-19 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| Liu et al.  ([81](#bib.bib81)) | 2021 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([82](#bib.bib82)) | 2021 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| Lucieri et al. ([85](#bib.bib85)) | 2022 | TCAV | Dermoscopy | ISIC 2019,
    Derm7pt, PH2, SkinL2 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. ([53](#bib.bib53)) | 2022 | CBIR | X-ray/Dermoscopy | COVIDx, ISIC
    2019 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| Gour and Jain ([43](#bib.bib43)) | 2022 | Uncertainty-based | X-ray | COVID19CXr
    |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: '| Yuksekgonul et al. ([173](#bib.bib173)) | 2022 | CBM | Dermoscopy | HAM 10000,
    ISIC 2020 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([162](#bib.bib162)) | 2022 | Text-based | X-ray | MIMIC-CXR
    |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
- en: '| Singla et al. ([138](#bib.bib138)) | 2023 | Counterfactual Examples | X-ray
    | MIMIC-CXR |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
- en: '| Yan et al. ([168](#bib.bib168)) | 2023 | CBM | Dermoscopy | ISIC 2016-2020
    |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
- en: '| Selivanov et al. ([127](#bib.bib127)) | 2023 | Text-based | X-ray | IU Chest
    X-ray, MIMIC-CXR |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([175](#bib.bib175)) | 2023 | Text-based | X-ray | IU Chest
    X-ray, MIMIC-CXR, COV-CTR |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([161](#bib.bib161)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([23](#bib.bib23)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([169](#bib.bib169)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '| Patrício et al. ([106](#bib.bib106)) | 2023 | CBM | Dermoscopy | PH2, Derm7pt
    |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
