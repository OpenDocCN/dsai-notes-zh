- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:46:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2205.04766] Explainable Deep Learning Methods in Medical Image Classification:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.04766](https://ar5iv.labs.arxiv.org/html/2205.04766)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Explainable Deep Learning Methods in Medical Image Classification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cristiano Patrício [cristiano.patricio@ubi.pt](mailto:cristiano.patricio@ubi.pt)
    ,  João C. Neves [jcneves@di.ubi.pt](mailto:jcneves@di.ubi.pt) University of Beira
    Interior and NOVA LINCSCovilhãPortugal6201-001  and  Luís F. Teixeira [luisft@fe.up.pt](mailto:luisft@fe.up.pt)
    University of Porto and INESC TECPortoPortugal4200-465(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The remarkable success of deep learning has prompted interest in its application
    to medical imaging diagnosis. Even though state-of-the-art deep learning models
    have achieved human-level accuracy on the classification of different types of
    medical data, these models are hardly adopted in clinical workflows, mainly due
    to their lack of interpretability. The black-box-ness of deep learning models
    has raised the need for devising strategies to explain the decision process of
    these models, leading to the creation of the topic of eXplainable Artificial Intelligence
    (XAI). In this context, we provide a thorough survey of XAI applied to medical
    imaging diagnosis, including visual, textual, example-based and concept-based
    explanation methods. Moreover, this work reviews the existing medical imaging
    datasets and the existing metrics for evaluating the quality of the explanations.
    In addition, we include a performance comparison among a set of report generation-based
    methods. Finally, the major challenges in applying XAI to medical imaging and
    the future research directions on the topic are also discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explainable AI, Explainability, Interpretability, Deep Learning, Medical Image
    Analysis^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi: 10.1145/nnnnnnn.nnnnnnn^†^†ccs:
    Applied computing Health care information systems'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The progress made on the last decade in the field of artificial intelligence
    (AI) has supported a dramatic increase in the accuracy of most computer vision
    applications. Medical image analysis is one of the applications where the progress
    made assured human-level accuracy on the classification of different types of
    medical data (e.g., chest X-rays ([91](#bib.bib91)), corneal images ([167](#bib.bib167))).
    However, and in spite of these advances, automated medical imaging is seldom adopted
    in clinical practice. According to Zachary Lipton ([78](#bib.bib78)), the explanation
    to this apparent paradox is straightforward, doctors will never trust the decision
    of an algorithm without understanding its decision process. This fact has raised
    the need for producing strategies capable of explaining the decision process of
    AI algorithms, leading subsequently to the creation of a novel research topic
    named as eXplainable Artificial Intelligence (XAI). According to DARPA ([47](#bib.bib47)),
    XAI aims to “produce more explainable models, while maintaining a high level of
    learning performance (prediction accuracy); and enable human users to understand,
    appropriately, trust, and effectively manage the emerging generation of artificially
    intelligent partners”. In spite of its general applicability, XAI is particularly
    important in high-stake decisions, such as clinical workflow, where the consequences
    of a wrong decision could lead to human deaths. This is also evidenced by European
    Union’s General Data Protection Regulation (GDPR) law, which requires an explanation
    of the decision-making process of the algorithm, thus improving its transparency
    before it can be used for patientcare ([42](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, it is of utmost importance to invest in the research of novel
    strategies to improve the interpretability of deep learning methods before being
    possible to deploy them into clinical practice. During the last years, the research
    on this topic has focused primarily on devising methods for indirectly analysing
    the decision process of pre-built models. These methods operate either by analysing
    the impact of specific regions of the input images on the final prediction (perturbation-based
    methods ([113](#bib.bib113); [88](#bib.bib88)) and occlusion-based methods ([174](#bib.bib174)))
    or inspecting the network activations (saliency methods ([128](#bib.bib128); [177](#bib.bib177))).
    The fact that these methods can be applied to arbitrary network architectures
    without requiring an additional customization of the model has supported their
    popularity in the early days of XAI. However, it has been recently shown that
    post-hoc strategies suffer from several drawbacks regarding the significance of
    the explanations ([118](#bib.bib118); [3](#bib.bib3)). As a consequence, researchers
    have focused their attention in the design of models/architectures capable of
    explaining their decision process per se. Inherently interpretable models are
    believed to be particularly useful in medical imaging ([118](#bib.bib118)), justifying
    the recent growth in the number of medical imaging works focusing on this paradigm
    rather the traditional post-hoc strategies ([61](#bib.bib61); [163](#bib.bib163)).
    In spite of the recent popularity of inherently interpretable models, the existing
    surveys on the interpretability of deep learning applied to medical imaging have
    not comprehensively reviewed the progress done in this novel research trend. Also,
    the significant increase in the number of works focused on the interpretation
    of the decision process of deep learning applied to medical imaging justifies
    the need for an updated review over the most recent methods not covered by the
    last surveys on the topic. Moreover, the particular challenges of medical imaging
    analysis, including image complexity (anatomical structures, organs, and artifacts
    are often harder to identify compared to general images), data availability, and
    the misclassification risk, emphasize the need for a dedicated survey on interpretability
    applied to medical imaging.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these concerns, we comprehensively review the recent advances on
    explainable deep learning applied to medical diagnosis. In particular, this survey
    provides the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a review of the recent surveys on the topic of interpretable deep learning in
    medical imaging, including the major conclusions derived from each work, as well
    as a comparative analysis to our survey;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an exhaustive list of the datasets commonly used in the study of interpretability
    of deep learning approaches applied to medical imaging;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a comprehensive review of the state-of-the-art interpretable medical imaging
    approaches, covering both post-hoc and inherently interpretable models;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a complete description of the metrics commonly used for benchmarking interpretability
    methods either for visual or textual explanations;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a benchmark of interpretable medical imaging approaches regarding the quality
    of the textual explanations;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the future research directions on the topic of interpretable deep learning in
    medical imaging.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explaining the decisions of deep learning models has been an active area of
    research, with various methods and algorithms proposed in the literature in the
    last years. The rapid pace of development of XAI has raised the need for comprehensive
    overviews of the advances in the state-of-the-art, and in most cases, the analysis
    of specific domains, due to the vast number of works published in the last years.
    Accordingly, in this section, we provide a critical analysis of the existing surveys
    in deep learning applied to medical imaging (section 2.1), with a particular focus
    on explainable approaches (section 2.2), and we compare the surveys analyzed with
    our survey (section 2.3).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Deep Learning in Medical Image Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The advent of deep learning significantly changed the field of Computer Vision,
    where handcrafted feature extraction was replaced by end-to-end learning using
    Convolutional Neural Networks (CNNs). This new paradigm emerged in 2012 through
    the seminal work of Krizhevsky et al. ([66](#bib.bib66)), but it was not immediately
    incorporated by all the applications of Computer Vision. Litjens et al. ([80](#bib.bib80))
    were the first to review the advances on medical image analysis fostered by the
    advent of deep learning, where is clear that the use of CNNs in medical imaging
    research has only become the standard approach in 2017, being clearly preferred
    over traditional handcrafted feature extraction for most of the anatomical regions.
    Based on the works reviewed, the authors concluded that data preprocessing and
    data augmentation techniques were essential to obtain superior results, and that
    the combination of medical images with text data (medical reports) could improve
    the image classification accuracy ([125](#bib.bib125)). Despite the relevance
    of this survey at the time, the rapid advances in the field of deep learning occurring
    in the last 5 years have made this work outdated, since the major conclusions
    of the survey are currently common sense, and novel deep learning models are currently
    being used in the medical imaging domain.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, Rehman et al. ([112](#bib.bib112)) provided an updated overview
    of the advances on deep learning applied to medical image analysis. The survey
    was divided over different pattern recognition tasks (image classification, segmentation,
    image registration). Regarding the image classification task, the authors suggested
    using generative models to perform data augmentation to improve the results. The
    survey also gave future research directions to overcome the most common challenges
    identified by Litjens et al. ([80](#bib.bib80)). The use of techniques such as
    transfer learning and synthetic data generation were suggested to address these
    challenges while improving the generalization capability of the developed strategies.
    Nevertheless, the authors concluded that the non-availability of large-scale annotated
    datasets remains one of the major challenges in medical imaging, which is impacting
    the performance of the deep learning models due to data overfitting and bias issues.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Interpretable Deep Learning in Medical Imaging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The works of Litjens et al. ([80](#bib.bib80)) and Rehman et al. ([112](#bib.bib112))
    show that in the last years the use of deep learning has greatly improved the
    performance of medical imaging analysis algorithms, allowing also to create a
    myriad of approaches for the different image modalities and recognition tasks.
    Nevertheless, this contrasts with the adoption of these algorithms by clinicians
    who refuse to rely on decisions that they do not understand ([78](#bib.bib78)).
    In fact, as foresaw by Litjens et al. ([80](#bib.bib80)), the importance of designing
    interpretable models for medical imaging has been growing in the last years and
    is nowadays one of the major challenges in medical imaging. The following paragraphs
    describe the different surveys focused on reviewing the recent advances on interpretable
    deep learning applied to medical imaging. Additionally, the major conclusions
    derived from each work and a brief comparison to our survey are also outlined.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. General Reviews
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Tjoa and Guan ([146](#bib.bib146)) provide a general overview of machine learning
    and deep learning interpretability methods with an emphasis on its application
    to medical field. The authors consider two types of interpretability: (i) perceptive
    interpretability, where the saliency methods are included, and (ii) interpretability
    by mathematical structures, which include mathematical formulations that can analyze
    the patterns in data. Although a significant number of works per image modality
    are covered, the survey lacks a comparison between the reviewed works. Also, the
    survey of Tjoa and Guan is more suitable for technical oriented readers, which
    is corroborated by a poor intuitive categorization for the medical community.
    Most works were discussed based on their mathematical foundation instead of describing
    the rationale behind the proposed method. As major conclusions, Tjoa and Guan
    state that combining visual and textual explanations is the most promising modality
    for conveying the explanations of medical imaging analysis algorithms, and that
    these algorithms should always be considered as a complementary aid/support to
    clinicians, who should be responsible for the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: Singh et al. ([134](#bib.bib134)) propose a review of the works related to the
    explainability of deep learning models in the context of medical imaging. The
    methods are broadly divided in two major categories (attribution-based and non
    attribution-based) with respect to their capability of determining the contribution
    of an input feature to the target output. Both categories are reviewed by describing
    works applied to the different image modalities of medical data. Nevertheless,
    the survey focuses primarily on the attribution-based category, providing a superficial
    discussion of existing methods on the different categories of non-attribution
    methods, where inherently interpretable approaches are included. Based on the
    works reviewed, the authors conclude that leveraging patient record data and images
    can be an exciting research direction to push forward the performance of deep
    learning in medical imaging. When compared to our survey, the work of Singh et
    al. lacks a comprehensive analysis of inherently interpretable approaches, an
    analysis of the available medical imaging datasets, and the benchmarking of the
    most prominent reviewed methods.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Salahuddin et al. ([120](#bib.bib120)) review a set of interpretability
    methods which are grouped into nine different categories based on the type of
    explanations generated. They also discuss the problem of evaluating explanations
    and describe a set of evaluation strategies adopted to quantitatively and qualitatively
    measure the explanations’ quality. Similarly to the other surveys, the authors
    also emphasize the importance of involving clinicians in designing and validating
    interpretability models to ensure the utility of the generated explanations. For
    the future perspectives, Salahuddin et al. claim that case-based and concept-learning
    models are promising interpretability models for being inherently interpretable
    and achieving similar performance to black-box CNNs. Despite being one of the
    most complete surveys on the topic, it lacks the description of most relevant
    datasets of the field as well as the benchmarking of most prominent approaches
    reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Specific Image Modality Reviews
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In contrast to the above-mentioned works, the work of Pocevičiūtė et al. ([109](#bib.bib109))
    is focused on a particular image modality. The XAI techniques devised for digital
    pathology are reviewed with respect to three criteria: 1) what is going to be
    explained (e.g., model predictions, predictions uncertainty); 2) explanation modality;
    3) how the explanations are derived (e.g., perturbation-based strategies, interpretable
    network design). The authors point out the importance of developing a toolbox
    for objectively measuring the quality of explanations, as the lack of an evaluation
    framework remains an open problem in the XAI field. Additionally, the authors
    state that the use of counterfactual examples can enhance the interpretability
    of the methods. When compared to our survey, this work has disregarded the textual
    explanation modality, focusing solely on visual explanations, either by visual
    examples or saliency maps.'
  prefs: []
  type: TYPE_NORMAL
- en: Gulum et al. ([46](#bib.bib46)) produce a review of the visual explainability
    techniques applied to cancer detection from Magnetic Resonance Imaging (MRI) scans.
    Contrary to the work of Pocevičiūtė et al. ([109](#bib.bib109)), Gulum et al.
    discuss the strategies used for measuring the quality of explanations, but they
    only consider one metric to quantitatively evaluate the explanations. They also
    emphasize that there is a lack of studies that assess the explanation methods
    based on human evaluation. As future directions, Gulum et al. highlight the need
    for developing inherently interpretable approaches, as opposed to the traditional
    post-hoc strategy. Finally, the authors proposed the use of uncertainty estimation
    associated with model predictions to perceive how a model is confident in making
    a prediction. Despite its relevance, the work of Gulum et al. is specific to a
    particular image modality (MRI) and target disease (cancer).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Specific Explanation Modality Reviews
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While visual explanations are usually the primary option for explaining the
    model decisions, these strategies can be unreliable since they often highlight
    regions regardless of the class of interest ([118](#bib.bib118)). This has fostered
    the research on textual explanations, and in the particular case of medical imaging
    led researchers to devise approaches capable of producing different types of textual
    explanations: 1) textual concepts and 2) textual reports. The recent developments
    on this topic have been covered in the surveys of Messina et al. ([96](#bib.bib96))
    and Ayesha et al. ([9](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In ([96](#bib.bib96)), a thorough overview of the current state-of-the-art
    on automatic report generation from medical images is provided. The authors review
    40 papers with respect to four dimensions: datasets used, model design, explainability
    and evaluation metrics. Furthermore, a benchmark of most relevant approaches is
    provided with respect to the performance in terms of NLP metrics on IU Chest X-ray
    dataset. Based on the works analysed, the authors identify the following challenges
    and future research directions: 1) the validation of the obtained explanations
    by clinicians is impractical, being necessary to create automatic metrics positively
    correlated with the clinicians opinion; 2) most research has concentrated on chest
    X-rays, mainly due to the availability of public data; 3) supervised learning
    may not be the most adequate strategy for medical report generation learning,
    whereas reinforcement learning seems a more reasonable training paradigm to explore.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to the work of Messina et al., Ayesha et al. ([9](#bib.bib9)) presents
    a detailed survey of the existing automatic caption generation methods for medical
    images. The most used datasets and the evaluation metrics are also discussed.
    An extensive study is done around the most significant works under the various
    deep learning-based medical imaging caption generation methods, namely encoder-decoder
    based, retrieval-based, attention-based, concepts detection-based, and patient’s
    metadata-based. Additionally, a comparative analysis of the performance of the
    methods reviewed is provided. Finally, Ayesha et al. suggest some future research
    directions to deal with the main open issues in medical imaging. They point out
    the lack of large-scale annotated datasets as the major limitation in the medical
    imaging field, where the data is scarce and often mislabelled. Also, they claim
    that the lack of a suitable evaluation metric to assess the generated caption
    remains an open problem since the evaluation of the generated text is still based
    on standard NLP metrics, such as BLEU score, ROUGE, METEOR, and CIDEr. As a final
    remark, Ayesha et al. also indicate the importance of having a model capable of
    detecting multiple diseases simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Comparative analysis between the surveys on the topic of explainable
    deep learning applied to medical imaging. Our survey is the first to comprehensively
    review the advances on the topic regarding the different explanation modalities
    and the explanation processes. Also, it analyses the most relevant datasets on
    the the field, as well as their use for the development of explainable approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Year | Explanations | Model Type | Medical Imaging | Benchmarking
    Performance |'
  prefs: []
  type: TYPE_TB
- en: '| Visual | Textual | Post-hoc | In-model | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Pocevičiūtė et al. ([109](#bib.bib109)) | 2020 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Tjoa and Guan ([146](#bib.bib146)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([134](#bib.bib134)) | 2020 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Gulum et al. ([46](#bib.bib46)) | 2021 | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ayesha et al. ([9](#bib.bib9)) | 2021 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Salahuddin et al. ([120](#bib.bib120)) | 2022 | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Messina et al. ([96](#bib.bib96)) | 2022 | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| This survey | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 2.3\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the significant contributions of each reviewed survey, few of them
    have described the most important datasets for medical imaging. Moreover, most
    surveys focused on particular aspects of interpretability, such as visual or textual
    approaches, and few works have comprehensively reviewed inherently interpretable
    models devised for medical image analysis. Another problem was the lack of a performance
    comparison among the reviewed methods. Accordingly, this survey covers these limitations
    by providing a broader overview of the current state-of-the-art XAI applied to
    medical diagnosis, including uni and multimodal approaches, followed by the most
    important medical imaging datasets and a comparative analysis of the models performance
    using standard evaluation metrics. In addition, this survey explores a contemporary
    trend and an under-exploited category of inherently interpretable models, specifically
    concept-based learning approaches. As delineated in subsequent sections, these
    approaches are advantageous for medical diagnosis as they provide explanations
    in the context of high-level concepts that align with the knowledge of the physicians
    and promote the interaction between physicians and AI through model intervention.
    Table [1](#S2.T1 "Table 1 ‣ 2.2.3\. Specific Explanation Modality Reviews ‣ 2.2\.
    Interpretable Deep Learning in Medical Imaging ‣ 2\. Related Surveys ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey") summarizes the
    major differences between the reviewed surveys and our work (This survey).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87b8a354e32ab9681eb98e9f590606f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Proposed categorization of the XAI methods taxonomy. The proposed
    categorization was inspired by the various taxonomies presented in the reviewed
    papers.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Background in XAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a historical perspective, the problem of explaining expert systems has
    its origin in the mid-80s ([99](#bib.bib99)), but the term XAI was only introduced
    in 2004 by Van et al. ([153](#bib.bib153)). Nevertheless, XAI has only gained
    prominence when deep learning dominated AI, and the first sign of this interest
    was demonstrated by the launching, in 2015, of the Explainable AI program by DARPA,
    whose primary goal was producing more explainable models to increase their understanding
    and transparency, leading to greater trust by users. Later, the European Union
    (EU) ([42](#bib.bib42)) introduced legislation about the “right to algorithmic
    explanation” which provided citizens with the right to receive an explanation
    for algorithmic decisions obtained from personal data. Considering this, researchers
    shifted their efforts towards the creation of interpretable models rather than
    simply focusing on accuracy, leading to an exponential increase in the popularity
    and interest on XAI, whose number of works on the topic has rapidly increased
    in the last years.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section provides a general overview of the taxonomy of XAI methods, the
    description of seminal XAI methods, as well as the existing frameworks providing
    implementations of these methods (Table [4](#A1.T4 "Table 4 ‣ A.1\. Intepretability
    Frameworks ‣ Appendix A Appendix ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey") in appendix [A.1](#A1.SS1 "A.1\. Intepretability
    Frameworks ‣ Appendix A Appendix ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. XAI Methods Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the reviewed literature, XAI methods can be categorized according
    to three criteria: (i) Model-Agnostic versus Model-Specific; (ii) Global Interpretability
    versus Local Interpretability; and (iii) Post-hoc versus Intrinsic. Figure [1](#S2.F1
    "Figure 1 ‣ 2.3\. Discussion ‣ 2\. Related Surveys ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey") illustrates the general taxonomy
    of the XAI methods, and each category is detailed in the following paragraphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Model-Agnostic versus Model-Specific
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A distinguishing factor between interpretability approaches is their comprehensiveness
    regarding the models they can be applied to. Model-agnostic methods can be used
    to explain arbitrary models, not being limited to a specific model architecture.
    Conversely, model-specific methods are restricted to a specific model architecture,
    meaning that these methods require access to the model’s internal information.
  prefs: []
  type: TYPE_NORMAL
- en: Global Interpretability versus Local Interpretability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The type of explanations provided by XAI methods can be broadly divided into
    global and local whether the explanations provide insights about the model functioning
    for the general data distribution or for a specific data sample, respectively.
    Global interpretability methods explain which patterns in the data, i.e., class
    features, contributed the most to the model’s prediction. These explanations can
    reveal critical reasoning about what the model is learning. On the other hand,
    local interpretability methods seek to explain why a model performs a specific
    prediction for a single input.
  prefs: []
  type: TYPE_NORMAL
- en: Post-hoc versus Intrinsic
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This criterion distinguishes the methods with respect to whether the explanation
    mechanism lies in the internal architecture of the model (intrinsic) or if it
    is applied after the learning/development of the model (post-hoc). Post-hoc methods
    usually operate by perturbing parts of the data so that they can understand the
    contribution of different features in the model prediction, or by analytically
    determining the contribution of different features to the model prediction. On
    the other hand, intrinsic models, also known as in-model approaches or inherently
    interpretable models, are self-explainable since they are designed to produce
    human-understandable representations from the internal model features.
  prefs: []
  type: TYPE_NORMAL
- en: Explanation Modality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Explanation modality refers to the type of explanation provided by each interpretability
    method. Among the reviewed methods, the explanation can be provided in the form
    of saliency maps (Explanation by Feature Attribution), semantic descriptions (Explanation
    by Text), similar examples (Explanation by Examples), or using high-level concepts
    (Explanation by Concepts). In Chapter [5](#S5 "5\. XAI Methods in Medical Diagnosis
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey"),
    we used this categorization to discuss the reviewed methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Classical XAI Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first attempts to explain deep learning models relied on the post-hoc analysis
    of the models. In spite of the criticism that post-hoc approaches have been recently
    subjected to ([118](#bib.bib118)), they are still being used in many domains of
    medical imaging, and their understanding is important to explain the advances
    on the topic of interpretable deep learning. As such, the following sections briefly
    describe the most popular XAI algorithms according to the two major categories
    of post-hoc analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Perturbation-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The rationale behind perturbation-based methods is to perceive how a perturbation
    in the input affects the model’s prediction. Examples of perturbation-based methods
    are LIME ([113](#bib.bib113)) and SHAP ([88](#bib.bib88)).
  prefs: []
  type: TYPE_NORMAL
- en: LIME
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'LIME ([113](#bib.bib113)) stands for Local Interpretable Model-agnostic Explanations.
    As the name suggests, it can explain any black-box model, and according to the
    XAI taxonomy is a post-hoc, model-agnostic method providing local explanations.
    The intuition behind LIME is to approximate the complex model (black-box model)
    locally with an interpretable model, usually denoted as local surrogate model.
    Thus, an individual instance is explained locally using a simple interpretable
    model around the prediction, such as linear models or decision trees. Figure [2](#S3.F2
    "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based methods ‣ 3.2\. Classical XAI Methods
    ‣ 3\. Background in XAI ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey")a) provides an intuitive illustration of the overall functioning of
    LIME.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to approximate the model prediction locally, a new dataset consisting
    of perturbed samples conditioned on their proximity to the instance being explained
    is used to fit the interpretable model. The labels for those perturbed samples
    are obtained through the complex model. In the case of tabular data, the perturbed
    instances are sampled around the instance being explained, by randomly changing
    the feature values in order to obtain samples both in the vicinity and far away
    from the instance being explained. Analogously, when LIME is applied to the image
    classification problem, the image being explained is first segmented into superpixels,
    which are groups of pixels in the image sharing common characteristics, such as
    colour and intensity. Then, the perturbed versions of the original data are obtained
    by randomly masking out a subset of superpixels, resulting in an image with occluded
    patches. The new dataset used to fit the interpretable model consists of perturbed
    versions of the image being explained, and the superpixels with the highest positive
    coefficients in the interpretable model suggest they largely contributed to the
    prediction. Thus, they will be selected as part of the interpretable representation
    that is simply a binary vector indicating the presence or absence of those superpixels.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'SHAP ([88](#bib.bib88)) was inspired on the Shapley values from the cooperative
    game theory ([130](#bib.bib130)) and operates by determining the average contribution
    of a feature value to the model prediction using all combinations of the features
    powerset. As an example, given the task of predicting the risk of stroke based
    on age, gender and Body Mass Index (BMI), the SHAP explanations for a particular
    prediction are given in terms of the contribution of each feature. This contribution
    is determined from the change observed in model prediction when using the $2^{n}$
    combinations from the features powerset, where the missing features are replaced
    by random values. Figure [2](#S3.F2 "Figure 2 ‣ SHAP ‣ 3.2.1\. Perturbation-based
    methods ‣ 3.2\. Classical XAI Methods ‣ 3\. Background in XAI ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey")b) illustrates the
    above-described example. Similarly to LIME, SHAP is a local model-agnostic interpretation
    method that can be applied to both tabular and image data. In the case of tabular
    data, the explanation is given in the form of importance values to each feature.
    In the case of image data, it follows a similar procedure to the LIME, by calculating
    the Shapley values for all possible combinations between superpixels. Several
    variations of SHAP method were proposed to approximate Shapley values in a more
    efficient way, namely KernelSHAP, DeepSHAP and TreeSHAP ([87](#bib.bib87)).'
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture" height="213.38" overflow="visible"
    version="1.1" width="614.13"><g transform="translate(0,213.38) matrix(1 0 0 -1
    0 0) translate(160.02,0) translate(0,121.77)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -155.41 -84)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="168" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/a7b63c723119b71caca81b9091e4f485.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 150.5 -87)" fill="#000000" stroke="#000000"><foreignobject
    width="299" height="174" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/7e1cf444120dc676ca47dc03989c702d.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -113.7)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -113.7)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. (a) LIME. The black curved line represents a decision boundary learned
    by the complex black-box model. LIME explains a new test sample (dashed circle),
    by fitting an interpretable model (represented by a green dashed line) to the
    variations of the test sample (orange circles), which are generated by randomly
    perturbing the test sample features. The fitted model allows to perceive the contribution
    of each feature for classifying that specific test sample. (b) SHAP. The predicted
    risk of stroke of a classification model for a 13 years-old female person, a body
    mass index of 29.1 and an average glucose level of 76.55 was “No Stroke”. As evidenced
    by the bar plot, which provides the Shapley values for each feature, “age” was
    the feature with a higher impact on the prediction of “No Stroke”, followed by
    the BMI and average glucose level features.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Saliency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Saliency maps are one of the most popular techniques to explain the decisions
    of a model. Saliency methods produce visual explanation maps representing the
    importance of image pixels to the model classification.
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Mapping (CAM) ([177](#bib.bib177)) is a seminal saliency method
    which allowed the generation of a saliency map using a linear combination of the
    output of the last Global Average Pooling (GAP) layer of the network. Despite
    being a seminal contribution, CAM can only be applied to architectures following
    a specific pattern. To address this problem, Selvaraju et al. ([128](#bib.bib128))
    proposed the Gradient-weighted Class Activation Mapping (Grad-CAM) ([128](#bib.bib128))
    that uses the gradient information of the target class with respect to the input
    image to produce a class-discriminative localization map that acts as a visual
    explanation for the model’s prediction. Grad-CAM is, therefore, a generalization
    of CAM. Alternatively, SmoothGrad ([139](#bib.bib139)) is another gradient-based
    explanation method whose core idea is to attenuate the noise of the explanations
    provided by gradient-based techniques. The rationale behind SmoothGrad is to sample
    multiple images from the input image by adding noise to it. Then, the sensitivity
    maps are computed for each sampled image. The final map is the average of the
    sensitivity maps.
  prefs: []
  type: TYPE_NORMAL
- en: The Integrated Gradients (IG) ([144](#bib.bib144)) is an attribution method
    that relies on generating a set of images between the baseline and the original
    image using linear interpolation. These interpolated images are minor changes
    in the feature space between the baseline and input image and consistently increase
    with each interpolated image’s intensity. Calculating the gradients per feature
    (pixels) makes it possible to measure the correlation between changes to a feature
    and changes in the model’s predictions. The pixels with a high score are the ones
    that contributed the most to the prediction. The Layer-wise Relevance Propagation
    (LRP) ([10](#bib.bib10)) is an alternative solution to the use of gradients, where
    the decision function is decomposed into the relevance score of each neuron in
    the network. The output is propagated backwards through the model to determine
    the relevance score of the input, allowing to produce an importance heatmap of
    image pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of saliency methods is the reduced computational cost when
    compared to perturbation-based methods. However, different studies argue that
    the explanations provided by gradient-based methods can be ambiguous and unreliable,
    as well as sensitive to adversarial perturbations ([3](#bib.bib3); [40](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Among the reviewed literature, a set of 25 publicly available medical imaging
    datasets were considered based on the reviewed papers to provide a thorough overview
    of the existing medical imaging databases. Table [3](#S7.T3 "Table 3 ‣ 7\. Performance
    Comparison ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") presents the main characteristics of the selected datasets, grouped
    by image type.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Medical Imaging Datasets. The datasets marked with a “*” have reports
    written in Spanish. The datasets marked with a “**” have reports written in Portuguese.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Image Type | Year | No. Images | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| IU Chest X-Ray ([32](#bib.bib32)) | Chest X-ray | 2016 | 7,470 | Includes
    reports |'
  prefs: []
  type: TYPE_TB
- en: '| Chest X-Ray14 ([159](#bib.bib159)) | Chest X-ray | 2017 | 112,120 | Multiple
    labels |'
  prefs: []
  type: TYPE_TB
- en: '| CheXpert ([54](#bib.bib54)) | Chest X-ray | 2019 | 224,316 | Multiple labels
    |'
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-CXR ([56](#bib.bib56)) | Chest X-ray | 2019 | 377,110 | Includes reports
    |'
  prefs: []
  type: TYPE_TB
- en: '| PadChest^∗ ([19](#bib.bib19)) | Chest X-ray | 2020 | 160,868 | Includes reports
    |'
  prefs: []
  type: TYPE_TB
- en: '| VinDr-CXR ([102](#bib.bib102)) | Chest X-ray | 2020 | 18,000 | Multiple labels
    |'
  prefs: []
  type: TYPE_TB
- en: '| COVIDx ([156](#bib.bib156)) | Chest X-ray | 2020 | 13,975 | Multiclass |'
  prefs: []
  type: TYPE_TB
- en: '| Inbreast^(∗∗) ([100](#bib.bib100)) | Mammography X-ray | 2012 | 410 | Includes
    reports |'
  prefs: []
  type: TYPE_TB
- en: '| CBIS-DDSM ([73](#bib.bib73)) | Mammography X-ray | 2017 | 2,620 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| VinDr-SpineXR ([103](#bib.bib103)) | Spinal X-ray | 2021 | 10,469 | Multiple
    labels/BBox |'
  prefs: []
  type: TYPE_TB
- en: '| Knee Osteoarthirtis ([101](#bib.bib101)) | Knee X-ray | 2006 | 8,894 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| PH² ([95](#bib.bib95)) | Dermatoscopic Images | 2013 | 200 | Multiple labels/Lesion
    segment. |'
  prefs: []
  type: TYPE_TB
- en: '| HAM10000 ([149](#bib.bib149)) | Dermatoscopic Images | 2018 | 10,015 | Multiclass/Lesion
    segment. |'
  prefs: []
  type: TYPE_TB
- en: '| SKINL2 ([31](#bib.bib31)) | Dermatoscopic Images | 2019 | 376 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| Derm7pt ([57](#bib.bib57)) | Dermatoscopic Images | 2019 | 2,000 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| ISIC 2020 ([117](#bib.bib117)) | Dermatoscopic Images | 2020 | 33,126 | Multiple
    labels |'
  prefs: []
  type: TYPE_TB
- en: '| SkinCon ([29](#bib.bib29)) | Dermatoscopic Images | 2022 | 3,230 | Concept
    annotations |'
  prefs: []
  type: TYPE_TB
- en: '| BreakHis ([141](#bib.bib141)) | Microscopy Images | 2015 | 9,109 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| Camelyon17 ([79](#bib.bib79)) | Microscopy Images | 2018 | 1,000 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| Databiox ([17](#bib.bib17)) | Microscopy Images | 2020 | 922 | Multiclass
    |'
  prefs: []
  type: TYPE_TB
- en: '| BCIDR^((priv)) ([176](#bib.bib176)) | Microscopy Images | 2017 | 5,000 |
    Includes reports |'
  prefs: []
  type: TYPE_TB
- en: '| APTOS ([140](#bib.bib140)) | Retina Images | 2019 | 5,590 | Multiclass |'
  prefs: []
  type: TYPE_TB
- en: '| LIDC-IDRI ([8](#bib.bib8)) | CT scans | 2011 | 1,018 | Includes annotations
    |'
  prefs: []
  type: TYPE_TB
- en: '| COV-CTR ([75](#bib.bib75)) | CT scans | 2023 | 728 | Includes reports |'
  prefs: []
  type: TYPE_TB
- en: '| PEIR ([55](#bib.bib55)) | Photographs | 2017 | 33,648 | Includes reports
    |'
  prefs: []
  type: TYPE_TB
- en: '| ROCO ([107](#bib.bib107)) | Multimodal | 2018 | 81,825 | Includes reports/UMLS
    Concepts |'
  prefs: []
  type: TYPE_TB
- en: 4.1\. Chest X-ray
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With respect to X-ray imaging modality, IU Chest X-ray ([32](#bib.bib32)), Chest
    X-ray 14 ([159](#bib.bib159)), CheXpert ([54](#bib.bib54)), MIMIC-CXR ([56](#bib.bib56)),
    PadChest ([19](#bib.bib19)), COVIDx ([156](#bib.bib156)), and VinDr-CXR ([102](#bib.bib102))
    datasets pertain to the chest anatomical region. Moreover, IU Chest X-Ray, MIMIC-CXR,
    and PadChest datasets include free-text radiology reports. The reports are written
    in English, except for the PadChest dataset, whose reports are written in Spanish.
    MIMIC-CXR and CheXpert are the largest databases, composed by $377,110$ and $224,316$
    radiographs, respectively. CheXpert does not provide the raw free-text reports,
    but it provides an automated rule-based labeller for extracting keywords from
    medical reports conforming to the Fleischner Society’s recommended glossary ([49](#bib.bib49)).
    This tool was also used in the MIMIC-CXR dataset to extract the labels from the
    radiology reports. UI Chest X-Ray ([32](#bib.bib32)) comprises $7,470$ chest X-ray
    images jointly with $3,955$ free-text reports, being the most used dataset in
    the literature. The VineDr-CXR consists of $18,000$ chest X-ray images annotated
    with $22$ findings (local labels) and $6$ diagnosis (global labels). The local
    labels are inferred from the “findings” section in the radiology reports. In contrast,
    the global labels come from “impressions” section and indicate suspected diseases,
    such as “Pneumonia”, “Tuberculosis”, “Lung Tumor”, “Chronic obstructive pulmonary
    disease”, “Other diseases”, and “No Findings”. Additionally, each “finding” is
    annotated on the X-ray image with a bounding box. Finally, COVIDx ([156](#bib.bib156))
    dataset comprises 13,975 chest X-ray images across $13,870$ patient cases, distributed
    by “Normal”, “Pneumonia” and “COVID-19” cases.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the majority of the chest X-ray dataset’s labels were
    extracted using an automatic rule-based labeler, such as the CheXpert NLP tool ([54](#bib.bib54)).
    However, relying on these automated tools can pose several issues concerning the
    quality of the labels. Consequently, the authors of the VinDr-CXR ([102](#bib.bib102))
    dataset provided only radiologist-level annotations in both training and test
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Other X-ray modalities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the same segment of the X-ray imaging, and similarly to the VinDr-CXR ([102](#bib.bib102))
    dataset approach, VinDr-SpinalXR ([103](#bib.bib103)) is a recent dataset comprising
    $10,469$ spine X-ray images manually annotated by an experienced radiologist with
    bounding-boxes around abnormal findings in $13$ categories. The Knee Osteoarthritis ([101](#bib.bib101))
    dataset contains 8,894 knee X-rays for both knee joint detection and knee Kellgren
    and Lawrence grading ([59](#bib.bib59)), whose value ranges from 0 to 4, according
    to the level of severity. Regarding the mammography datasets, Inbreast ([100](#bib.bib100))
    consists of $410$ mammography X-rays along with 115 radiology reports written
    in Portuguese. Similarly, CBIS-DDSM ([73](#bib.bib73)) dataset is composed of
    $2,620$ mammography scans distributed into “Normal”, “Benign”, and “Malignant”
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Dermatoscopy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the scope of dermatology, the ISIC 2020 ([117](#bib.bib117)) dataset consists
    of $33,126$ skin lesion images of different categorizations (malignant, melanoma,
    keratosis, etc), and it is part of the International Skin Imaging Collaboration,
    which promotes annual challenges to enhance the diagnosis of malignant skin lesions
    in dermoscopy images. The HAM10000 ([149](#bib.bib149)) dataset consists of $10,015$
    dermatoscopic images distributed into diagnostic categories in the realm of pigmented
    lesions. The PH² ([95](#bib.bib95)) dataset comprises $200$ dermoscopic images
    of melanocytic lesions, including common nevi, atypical nevi, and melanoma. Moreover,
    the PH² database includes medical segmentation of the lesions, clinical and histological
    diagnosis, and the assessment of several dermoscopic criteria. The Derm7pt ([57](#bib.bib57))
    is another dermoscopic image dataset with over $2,000$ images annotated following
    the 7-point melanoma checklist criteria. Finally, the SKINL2 ([31](#bib.bib31))
    database consists of a total of $376$ light fields of skin lesions manually annotated
    under eight categories, based on the type of skin lesion and using the correspondent
    International Classification of Diseases (ICD) code. SkinCon ([29](#bib.bib29))
    includes 3,230 images from the Fitzpatrick 17k skin disease dataset ([45](#bib.bib45))
    annotated with 48 clinical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Microscopy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With regard to datasets composed of microscopy images, the BreakHis ([141](#bib.bib141))
    dataset comprises $9,109$ microscopic images of breast tumour tissue distributed
    in various magnifying factors (40x, 100x, 200x, and 400x) and categorized into
    “benign” and “malignant” tumours. The Camelyon 17 ([79](#bib.bib79)) dataset consists
    of $1,000$ annotated whole-slide images (WSI) of lymph nodes. It is part of a
    challenge whose primary goal is the classification of breast cancer metastases
    in WSI of histological lymph node sections. Similarly, Databiox  ([17](#bib.bib17))
    dataset comprises $922$ histopathological microscopy images with four levels of
    magnification (4x, 10x, 20x, and 40x) for the task of Invasive Ductal Carcinoma
    (IDC) grading using three grades of IDC.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For blindness detection purposes, the APTOS ([140](#bib.bib140)) dataset provides
    $5,590$ images of the retina taken using fundus photography. A clinician annotated
    each image according to the severity of diabetic retinopathy on a scale of 0 (Diabetic
    Retinopathy) to 4 (Proliferative Diabetic Retinopathy). In the scope of COVID-19,
    COVID-19 CT Reports (COV-CTR) ([75](#bib.bib75)) is a dataset composed of 728
    CT scans paired with Chinese and English reports.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding databases with multiple imaging modalities, the Pathology Education
    Informational Resource (PEIR) is a multidisciplinary public access image database
    intended for medical education. The PEIR database consists of $33,648$ images
    and the respective descriptions from different sub-classes of PEIR albums (PEIR
    Pathology, PEIR Radiology, and PEIR Slice). Similarly, the Radiology Objects in
    COntext (ROCO) ([107](#bib.bib107)) dataset is a multimodal image dataset consisting
    of $81,825$ radiology images divided into CT, MRI, X-ray Ultrasound, and Mammography.
    Each image is accompanied by the corresponding caption and the annotated Unified
    Medical Language System (UMLS) concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to Table [2](#S4.T2 "Table 2 ‣ 4\. Datasets ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), Chest X-ray is the most popular
    imaging modality regarding simultaneously the number of datasets and their scale.
    Large-scale datasets have the advantage that they can be used to train a model
    from scratch. However, the automatic labeling process of some examples could compromise
    the reliability of the model since some class labels are not human verified and
    can be mislabeled. On the other hand, as evidenced by the number of images per
    dataset, the scarcity of large-scale datasets concerning other imaging modalities
    is noticed, hindering the emergence of specialized task-specific models due to
    the limited training data. The majority of the medical imaging datasets have been
    gathered primarily for use in classification or segmentation tasks, where the
    labeling of class and mask annotations are sufficient for their intended purpose.
    Nevertheless, when these sets are utilized for interpretability purposes, their
    annotations may prove inadequate for a comprehensive qualitative evaluation. Nonetheless,
    some datasets, such as SkinCon and others, have been created explicitly with interpretability
    in mind and contain appropriate annotations to facilitate such evaluations. Accordingly,
    it is particularly important that researchers consider interpretability issues
    when building novel medical datasets, ensuring that appropriate annotations are
    included to further enable comprehensive and informative analyses of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. XAI Methods in Medical Diagnosis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As aforementioned, deep learning models must confer transparency and trustworthiness
    when deployed in real-world scenarios. Moreover, this requirement becomes particularly
    relevant in clinical practice, where a less informed decision can put patient’s
    lives at risk. Among the reviewed literature, several methods have been proposed
    to confer interpretability in the deep learning methods applied to medical diagnosis.
    The following sections summarize and categorize the most relevant works in the
    scope of explainable models applied to medical diagnosis. Furthermore, we give
    particular attention to the inherently interpretable neural networks and their
    applicability to medical imaging. We categorize the methods according to the explanation
    modality: (i) Explanation by Feature Attribution, (ii) Explanation by Text, (iii)
    Explanation by Examples, (iv) Explanation by Concepts, and (v) Other Approaches;
    inspired by the taxonomy presented in ([97](#bib.bib97)). The list of the reviewed
    methods categorized by the interpretability method employed, image modality, and
    the dataset is provided in Table [5](#A1.T5 "Table 5 ‣ A.2\. Methods ‣ Appendix
    A Appendix ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") in appendix [A.2](#A1.SS2 "A.2\. Methods ‣ Appendix A Appendix ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey")¹¹1An interactive
    version of the Table 5 is provided in this [link](https://github.com/CristianoPatricio/Explainable-Deep-Learning-Methods-in-Medical-Image-Classification-A-Survey)..'
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture" height="193.22" overflow="visible"
    version="1.1" width="611.13"><g transform="translate(0,193.22) matrix(1 0 0 -1
    0 0) translate(160.02,0) translate(0,100.11)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -155.41 -88.5)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="177" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/7190cb54a5e4ed377341cddf9967e7ff.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 153.5 -88)" fill="#000000" stroke="#000000"><foreignobject
    width="293" height="176" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/194d17549768fabd785cfcd596630683.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -92.04)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -92.04)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. (a) Perturbation-based methods. The input image is randomly perturbed
    by turning on and off certain pixels, resulting in an image with occluded parts
    (Perturbed Examples in the figure). Then, the perturbed image is fed to the classification
    model and the prediction confidence is exploited to determine the regions that
    contributed positively to the class prediction. Those regions will be considered
    to obtain the final explanation map (Explanation in the figure). (b) Saliency
    methods. The input image is fed to the classification model to obtain a class
    prediction. Then, the gradient is calculated for the score of the class concerning
    the feature maps of the last convolutional layer. After calculating the importance
    of the feature map regarding the predicted class, they are weighted with each
    of respective weight, followed by a ReLU operation to obtain the final saliency
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Explanation by Feature Attribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature attribution methods indicate how much each input feature contributed
    to the final model prediction. These methods can work on tabular data or image
    data by depicting feature importance scores in a bar chart or using a saliency
    map, respectively. Among the existing feature attribution approaches in the literature,
    we categorize them into (i) perturbation-based methods and (ii) saliency methods,
    emphasizing their application to medical image analysis. A schematic diagram illustrating
    the general pipeline of these methods is shown in Figure [3](#S5.F3 "Figure 3
    ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in
    Medical Image Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Perturbation-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As previously described in section [3.2.1](#S3.SS2.SSS1 "3.2.1\. Perturbation-based
    methods ‣ 3.2\. Classical XAI Methods ‣ 3\. Background in XAI ‣ Explainable Deep
    Learning Methods in Medical Image Classification: A Survey"), perturbation-based
    methods aim to perform a modification in the input data to perceive how it affects
    the model’s prediction. Popular examples of perturbation-based methods are LIME ([113](#bib.bib113))
    and SHAP ([88](#bib.bib88)). Regarding the use of perturbation methods to explain
    the prediction of medical diagnosis algorithms, Malhi et al. ([92](#bib.bib92))
    applied the LIME method to explain the decisions of a classifier to detect bleeding
    in gastral endoscopy images. Similarly, Punn et al. ([110](#bib.bib110)) applied
    the LIME technique to explain the predictions of various state-of-the-art deep
    learning models used to classify pulmonary diseases in chest X-ray images. Magesh
    et al. ([90](#bib.bib90)) also used LIME to justify the decisions of a Parkison’s
    disease classifier model. In the context of melanoma detection, Young et al. ([171](#bib.bib171))
    used Kernel SHAP ([88](#bib.bib88)) interpretability method to investigate its
    reliability in providing explanations for a melanoma classifier from dermoscopy
    images. They concluded that the interpretability strategy highlighted irrelevant
    features to the final prediction. The authors also conducted sanity checks on
    the interpretability methods, which confirmed that these methods often produce
    different explanations for models with similar performance. This can be explained
    by the fact that the model can learn some spurious correlations, causing interpretability
    methods to give exaggerated importance to those spurious regions highlighted on
    the produced saliency maps. In the same context, Wang et al. ([158](#bib.bib158))
    proposed a multimodal CNN for skin lesion diagnosis, which considered patient
    metadata and the skin lesion images. To analyze the contribution of each feature
    regarding the patient metadata, they adopted SHAP. Similarly, Eitel et al. ([35](#bib.bib35))
    relied on an occlusion-based interpretability method ([174](#bib.bib174)) and
    investigated its robustness for the task of Alzheimer’s disease classification.'
  prefs: []
  type: TYPE_NORMAL
- en: RISE was proposed by Petsiuk et al. ([108](#bib.bib108)), consisting also of
    a post-hoc model-agnostic method for explaining the predictions of a black-box
    model through the generation of a saliency map indicating the important pixels
    to the model’s prediction. The core idea is to probe the model with a set of perturbed
    images from the input image via random masking to perceive the model’s response
    as important regions of the image are randomly occluded. The final saliency map
    is generated as a linear combination of the generated masks weighted with the
    output probabilities predicted by the model. For evaluation, the authors proposed
    two novel metrics, namely deletion and insertion, based on the removal and insertion
    of important pixels in the image to perceive the increase or the decrease of the
    model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Saliency Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Saliency methods allow to produce a saliency map in which each pixel is assigned
    a value that represents its relevance to the prediction of a certain class. Popular
    techniques are CAM ([177](#bib.bib177)), Grad-CAM ([128](#bib.bib128)), DeepLIFT ([131](#bib.bib131))
    and Integrated Gradients ([144](#bib.bib144)).
  prefs: []
  type: TYPE_NORMAL
- en: Rajpurkar et al. ([111](#bib.bib111)) proposed the CheXNeXt model to detect
    pulmonary pathologies and used CAM to identify the locations on the chest radiograph
    that contributed most to the final model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of detecting COVID-19 from chest radiographs, Lin et al. ([77](#bib.bib77))
    used Grad-CAM and Guided Grad-CAM to investigate the regions that the model considered
    more discriminative. The produced heatmaps showed that when no preprocessing is
    used, the CNN tends to concentrate on non-lung areas (e.g., spine, heart, background),
    deemed irrelevant for the classification decision. However, when a masking process
    is used to highlight only the lung’s area, the produced heatmaps highlight only
    the relevant regions since the CNN attention is limited to the critical area for
    detecting pulmonary diseases (lung’s area). Following the same procedures, Lopatina
    et al. ([84](#bib.bib84)) used DeepLIFT attribution algorithm to investigate the
    decisions of a multiple sclerosis classification model, and Sayres et al. ([124](#bib.bib124))
    used Integrated Gradients to provide explanations for the task of predicting diabetic
    retinopathy from retinal fundus images.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the previous approaches, Rio-Torto et al. ([115](#bib.bib115))
    proposed an in-model joint architecture composed of an explainer and a classifier
    to produce visual explanations for the predicted class labels. The explainer consists
    of an encoder-decoder network based on U-Net, and the classifier is based on VGG-16\.
    Since the classifier is trained using the explanations provided by the explainer,
    the classifier focuses only on the relevant regions of the image containing the
    class. The qualitative assessment of the provided explanations was carried out
    by using state-of-the-art explainability methods provided by the Captum library ([65](#bib.bib65)).
    Additionally, a quantitative analysis was also provided in terms of accuracy,
    average precision, AUROC, AOPC ([121](#bib.bib121)) and the proposed POMPOM metric.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the simplicity of the feature attribution methods and their applicability
    to a wide range of approaches, these methods may often produce ambiguous explanations,
    difficulting their qualitative evaluation. Furthermore, preprocessing techniques
    were required for some methods to generate more plausible explanations ([77](#bib.bib77)).
    Thus, researchers began exploring other modalities, such as textual explanations,
    and it was discovered that textual explanations were indeed valid explanations
    and, in some cases, preferred over visual explanations since they are inherently
    understandable by humans  ([152](#bib.bib152)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Explanation by Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use of semantic descriptions became another way of explaining the model
    decisions since most of the clinicians prefer textual explanations compared to
    visual explanations solely, and the combination of textual and visual explanations
    over either alone  ([39](#bib.bib39)). In general, providing textual explanations
    can be built on three paradigms: (i) image captioning, (ii) image captioning with
    visual explanation, and (iii) concept attribution ([152](#bib.bib152)). Figure
    [4](#S5.F4 "Figure 4 ‣ 5.2\. Explanation by Text ‣ 5\. XAI Methods in Medical
    Diagnosis ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") depicts the general scheme adopted by most works to generate a textual
    description based on the visual features of the input image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46f0589ce7d87d94d63e6f50803ddf0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Explanation by textual descriptions. The typical architecture for
    obtaining a textual description from image data combines an image embedding model
    (e.g., CNN) for extracting the features from the input image and a language model
    (e.g., LSTM) for generating the word sentences. The attention module can be inserted
    between those two models to guide the language model to focus only on relevant
    regions of the input image to improve the generation of the word sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Image Captioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The task of generating a textual description for explaining a model decision
    can be viewed as an extension of the image captioning problem, commonly treated
    in Natural Language Processing (NLP). Indeed, the vast majority of works that
    aim to generate a textual description for a given input image follow the classical
    strategy of combining a CNN for extracting the visual features with an RNN (e.g.,
    LSTM) to generate the word sentences. Based on this paradigm, Sun et al. ([143](#bib.bib143))
    developed a joint framework to generate sequences of words to provide a textual
    explanation for the task of diagnosing malignant tumors from breast mammography.
    Similarly, Singh et al. ([137](#bib.bib137)) built on an encoder-decoder framework
    composed of a CNN and a stacked Long Short-Term Memory (LSTM) for automatically
    generating radiology reports from chest X-rays. Regarding image captioning with
    visual explanation, Zhang et al. ([176](#bib.bib176)) proposed a multimodal approach,
    dubbed MDNet, composed of an image embedding model and a language model, that
    can generate diagnostic reports, retrieve images by symptom descriptions, and
    visualize network attention. The MDNet model was evaluated on a dataset (BCIDR)
    containing histopathological images of bladder cancer. Furthermore, MDNet inspired
    several approaches. For example, Jing et al. ([55](#bib.bib55)) proposed a multi-task
    learning framework with a co-attention mechanism to guide the generation of text
    according to the localized regions containing abnormalities. They showed that
    a hierarchical LSTM model performs better in generating long text reports. Subsequently,
    Wang et al. ([160](#bib.bib160)) proposed TieNet, which makes use of attention
    modules to extract the most important information from chest X-ray images and
    also use their diagnostic reports in order to guide the model to produce more
    coherent reports. Similarly, Barata et al. ([12](#bib.bib12)) proposed a hierarchical
    classification model that uses attention modules, including channel and spatial
    attention, to identify relevant regions in the skin lesions and subsequently guide
    further the LSTM attending at different locations whilst conferring more transparency
    to the network. Lee et al. ([72](#bib.bib72)) also explained the decisions of
    a breast masses classifier using both visual and textual explanations, based on
    a CNN-RNN architecture. In the same fashion, Gale et al. ([39](#bib.bib39)) proposed
    a model-agnostic interpretable method based on an RNN to produce textual explanations
    for the decisions of deep learning classifiers. Furthermore, they developed a
    visual attention mechanism in charge of highlighting the relevant regions for
    classifying hip fractures in pelvic X-rays. Yin et al. ([170](#bib.bib170)) also
    used attention mechanisms to attend to the regions at sentence level. They proposed
    the Hierarchical Recurrent Neural Network (HRNN) model, composed of two-level
    LSTMs: a word RNN and a sentence RNN. The sentence RNN produces the topic vectors
    whereas the word RNN receives the output of the sentence RNN and infers the words
    that constitute the final report. Moreover, they introduced a matching mechanism
    to map the topic vectors and the sentences into a jointly semantic space that
    minimizes a contrastive loss. Similar to the work of Yin et al. ([170](#bib.bib170)),
    the model proposed by Liu et al. ([83](#bib.bib83)) generates topics from images
    and then completes sentences from these topics. When compared to  ([170](#bib.bib170)),
    this work allows the generation of more coherent report generation due to the
    use of a fine-tuning process that uses reinforcement learning via CIDEr.'
  prefs: []
  type: TYPE_NORMAL
- en: A more disruptive approach was introduced by Li et al. ([74](#bib.bib74)) that
    proposed the Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) consisting
    of a retrieval policy module and a generation module. The retrieval policy module
    is responsible for deciding whether sentences are obtained from a generation module
    or retrieved from the template database, which is composed of a set of template
    sentences. Moreover, the retrieval policy and generation modules are updated via
    reinforcement learning, guided by sentence-level and word-level rewards using
    the CIDEr.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the previous approaches, Chen et al. ([25](#bib.bib25)) exploited
    the Transformer ([154](#bib.bib154)) architecture, where they incorporated two
    memory modules into the decoder. These modules are responsible for memorizing
    textual patterns and assisting the decoder of the Transformer in generating radiology
    reports containing relevant information associated with chest X-ray images. The
    same tendency is reflected in recent works ([81](#bib.bib81); [82](#bib.bib82);
    [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)), which employ transformer-based
    models with additional custom modules to better capture the relevant features
    of input images, leading to an improved performance in the task of radiological
    report generation. Recently, Selivanov et al. ([127](#bib.bib127)) introduced
    a novel image captioning architecture that combines two language models, incorporating
    image-attention (SAT) ([166](#bib.bib166)) and text-attention (GPT-3) ([18](#bib.bib18)),
    resulting in an outstanding performance compared to the previous methods. For
    a more comprehensive analysis of the use of Transformers in medical imaging, we
    refer the reader to the survey of Shamshad et al. ([129](#bib.bib129)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Concept Attribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The idea behind concept-based attribution is learning human-defined concepts
    from the internal activations of a CNN. The use of concepts to provide global
    explanations was proposed by Kim et al.  ([60](#bib.bib60)) with the introduction
    of the Concept Activation Vectors (CAVs), which provide explanations in terms
    of human-understandable concepts that are typically related to parts of the image.
    Kim et al. also proposed Testing with CAVs (TCAV), which enable to quantify the
    importance of a user-defined concept to a classification result. Figure [5](#S5.F5
    "Figure 5 ‣ 5.2.2\. Concept Attribution ‣ 5.2\. Explanation by Text ‣ 5\. XAI
    Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in Medical Image
    Classification: A Survey") illustrates the typical pipeline of concept-based attribution
    methods. In the context of medical imaging, the term “microaneurysm” can be viewed
    as a concept, being possible to be identified by humans in fundus imaging, and
    which denote the presence of diabetic retinopathy  ([152](#bib.bib152)). In the
    same line of research, Graziani et al. ([44](#bib.bib44)) proposed a framework
    for concept-based attribution to generate explanations for CNN decisions of a
    breast histopathology classifier. They built on TCAV by incorporating Regression
    Concept Vectors (RCV), which provide continuous-values measures of a concept instead
    of solely indicating its presence or absence. This is particularly useful in the
    medical domain since a value indicating, for example, tumor size is more informative
    than a binary value indicating its presence or absence. Graziani et al. also concluded
    that the learning of concepts by an intermediate layer of a CNN could be improved
    by removing spatial dependencies of the convolutional layers and introducing L2
    norm regularization in the regression problem. Recently, Lucieri et al. ([85](#bib.bib85))
    introduced ExAID, a framework that provides multimodal concept-based explanations
    for the task of melanoma classification. Their framework relied on CAVs for concept
    identification and used the TCAV method to estimate the influence of a specific
    concept on the decision. The authors provided textual explanations in the form
    of template phrases by only replacing the identified concepts and their importance
    to the prediction in the phrase structure. In order to localize the regions of
    the learned concepts in the latent space of the trained classifier, the authors
    used the Concept Localization Maps ([86](#bib.bib86)) which uses perturbation-based
    concept localization to generate a saliency map highlighting the relevant regions
    with respect to the learned concepts, thus providing visual explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8e4f3c71e91e903a8e675b8d96f51fff.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Explanation by concept attribution. In the first phase, the human-defined
    concepts (Regular Dots and Globules, Atypical Pigment Network, Typical Pigment
    Network, Streaks, Irregular Dots and Globules, and Blue-Whitish Veil) are modeled
    as numeric features, following the CAV technique. Then, after the input image
    passes through a classifier model, the class prediction is globally explained
    based on the importance of each concept to the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In summary, except for the work of Chen et al. ([25](#bib.bib25)), all the explanation
    text-based approaches mentioned above rely on RNN architectures to generate text
    descriptions towards providing a more human-interpretable explanation for a model
    decision. However, as stated by Pascanu et al. ([105](#bib.bib105)), RNN-based
    approaches, such as LSTM, have some limitations in generating long text reports.
    On the contrary, concept-based attribution methods provide a more objective and
    human-understandable way of interpreting classification decisions. However, the
    main limitation of these methods is the need for manual annotations of the concept
    examples, which may be impractical for specific medical image modalities, increasing
    the need for involving clinicians in the annotation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Explanation by Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The type of methods that explain a model decision by selecting a set of similar
    examples are dubbed example-based explanation methods. Apart from explaining algorithm
    decisions, this strategy is also commonly used between clinicians to explain the
    rationale behind their decision process. Below, we categorize example-based explanation
    methods into the following categories: (i) Case-Based Reasoning, (ii) Counterfactual
    Explanations, and (iii) Propotypes.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1\. Case-Based Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Case-Based Reasoning (CBR) and Content-Based Image Retrieval (CBIR) are example-based
    explanation methods that aim to search a database for visually similar entries
    to a specific query image. The general scheme for implementing a CBR system using
    DNNs is depicted in Figure [6](#S5.F6 "Figure 6 ‣ 5.3.1\. Case-Based Reasoning
    ‣ 5.3\. Explanation by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey")a. Although the
    idea of using CBIR systems in clinical settings is not novel ([4](#bib.bib4)),
    there has been renewed interest in CBIR approaches as a way to provide explainability
    to deep learning methods for medical diagnosis ([26](#bib.bib26); [48](#bib.bib48);
    [6](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture" height="200.78" overflow="visible"
    version="1.1" width="586.6"><g transform="translate(0,200.78) matrix(1 0 0 -1
    0 0) translate(161.99,0) translate(0,109.96)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -157.37 -85.5)" fill="#000000"
    stroke="#000000"><foreignobject width="299" height="171" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/a13dfd21b302b7cbc02aa03fb0d29fd0.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 180 -83)" fill="#000000" stroke="#000000"><foreignobject
    width="240" height="166" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/239036c54a031057caa680f9967a0015.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 9.16 -101.88)" fill="#000000" stroke="#000000"><foreignobject width="17.68"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(a)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 272.78 -101.88)" fill="#000000" stroke="#000000"><foreignobject
    width="18.45" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(b)</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. (a) Explanation by Case-Based Reasoning. The feature vector corresponding
    to the input image is compared against the feature vectors of the images in the
    catalogue using a distance metric, such as L2 distance. Finally, the images are
    retrieved from the catalogue ranked by their similarity to the input image. (b)
    Explanation by Counterfactual Examples. To explain the prediction made by a classifier,
    the input image is modified in a controlled way, typically by using a generative
    model (e.g., GAN) to shift the original class, i.e., from normal to abnormal or
    vice-versa. Thus, the counterfactual example intends to explain the prediction
    by showing that the image was classified as ”abnormal” because it is not ”normal”,
    as perceived by the absence of tissue inflammation (white spots) in the generated
    counterfactual example.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Barnett et al. ([14](#bib.bib14)) introduced a novel interpretable
    AI algorithm (IAIA-BL) for classifying breast masses using CBR. The model provided
    both a prediction of malignancy and its explanation by using known medical features
    (mass margins). Given an image region to analyze, the algorithm compared that
    region with a set of previous similar cases (image patches) using euclidean distance.
    The similarity score was then used to provide the mass margin scores for each
    medical feature, and those scores were then used to predict the malignancy score
    (benign or malignant). The model was trained using a fine-annotation loss penalizing
    activations of medically irrelevant regions on the data. The authors also introduced
    an interpretable evaluation metric, namely Activation Precision, to quantify the
    proportion of relevant information from the “relevant region” used to classify
    the mass margin regarding the radiologist annotations. The experimental results
    showed that the IAIA-BL achieved comparable performance to black-box models.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach was presented by Tschandl et al. ([148](#bib.bib148)),
    in which they compared the predictions of the ResNet-50 softmax classifier with
    the diagnostic accuracy obtained by using CBIR. Contrary to Barnett et al. ([14](#bib.bib14)),
    Tschandl et al. measured the cosine similarity between two feature vectors to
    retrieve the most similar images to the image query. The results showed that the
    diagnostic accuracy obtained through CBIR is comparable to the performance of
    a softmax classifier leading Tschandl et al. to claim that CBIR can replace traditional
    softmax classifiers to improve diagnostic interpretability in a clinical workflow.
  prefs: []
  type: TYPE_NORMAL
- en: A more recent approach was introduced by Barata and Santiago ([13](#bib.bib13)),
    where CBIR was applied to explain the decisions of a CNN model for skin cancer
    diagnosis. When compared to the work of Tschandl et al., Barata and Santiago implemented
    an augmented category-cross entropy loss function composed of three regularization
    losses, namely the triplet loss, the contrastive loss, and the distillation loss.
    These losses encourage the model to learn a more structured feature space. The
    experimental results on ISIC 2018 ([149](#bib.bib149)) dermoscopy dataset confirmed
    that the combination of different loss functions lead to more structured feature
    spaces, which improves the performance of the classification model. Lamy et al. ([69](#bib.bib69))
    proposed an explainable CBR system with a visual interface, combining quantitative
    and qualitative approaches. In contrast to the above-discussed works, it used
    numerical data and provided a user study in which clinicians validated their approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recent work of Hu et al. ([53](#bib.bib53)) introduced the eXplainable
    Medical Image Retrieval (X-MIR) approach, which explored the use of similarity-based
    saliency maps to explain the retrieved images visually. Concretely, they adapted
    the saliency map generation process for the problem of image retrieval through
    the use of a similarity-based formulation. These similarity-based saliency methods
    take as input a retrieval image and a query image for producing a saliency map
    highlighting the most similar regions of the retrieval image to the query image.
    To evaluate the quality of the generated saliency maps, the authors adapted two
    causal metrics, namely deletion and insertion (refer to section [6.1](#S6.SS1
    "6.1\. Evaluating the Quality of Visual Explanations ‣ 6\. Evaluation Metrics
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")
    for a detailed description), to measure the decrease or increase in image similarity
    score as the retrieved image is gradually perturbed based on the important regions
    of its saliency map. The authors evaluated their approach on two medical datasets,
    namely COVIDx ([156](#bib.bib156)) and ISIC 2017\. They found that for both cases,
    the generated saliency maps focused on relevant regions when retrieved images
    were correct and observed the contrary when the retrieved images were incorrect.
    Finally, the authors pointed out for the importance of conducting user studies
    with clinicians to validate the utility of their approach. Silva et al. ([132](#bib.bib132))
    also explored the medical image retrieval with the addition of saliency maps to
    improve the class-consistency of top retrieved results while enhancing the interpretability
    of the whole system by accompanying the retrieval with visual explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to assess the effectiveness of using a CBIR system as an auxiliary
    tool for classifying skin lesions through dermatology images, a user-centered
    study was done by Sadeghi et al. ([119](#bib.bib119)). Sixteen non-expert users
    were invited to classify skin lesions images among four categories (Nevus, Seborrheic
    Keratosis, Basal Cell Carcinoma, and Malignant Melanoma) based on two conditions:
    using CBIR and without using CBIR. The results indicated that CBIR enabled users
    to make a significantly more accurate classification on a new skin lesion image.
    These findings suggest that CBIR can indeed assist clinicians understand model
    decisions as well as allow less experienced practitioners to improve their skills.
    Thus, CBIR-based systems can have a significant clinical application value as
    a decision-support tool to accelerate the diagnosis of pathologies.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. Counterfactual Explanations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Counterfactual explanations are based on the principle that “an action on the
    input data will cause an outcome” ([97](#bib.bib97)). The idea is to perturb the
    input data in a controlled way in order to reverse the final model prediction,
    being the modified input the counterfactual example, as illustrated in the diagram
    of the Figure [6](#S5.F6 "Figure 6 ‣ 5.3.1\. Case-Based Reasoning ‣ 5.3\. Explanation
    by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey")b. Furthermore, counterfactual
    explanations are deemed human-interpretable and post-hoc, meaning that they do
    not require access to model internals.'
  prefs: []
  type: TYPE_NORMAL
- en: The work of Schutte et al. ([126](#bib.bib126)) constituted a new approach in
    the way of interpreting the predictions made by deep learning models by using
    generative models to produce a sequence of images depicting the evolution of a
    pathology. Through the sequence of images produced, a human can understand which
    biomarkers triggered the prediction made by the model. Concretely, the proposed
    method aims to identify the optimal direction in latent space to produce a series
    of synthetic images with minor modifications leading to different model predictions.
    By observing these modified synthetic versions of the original image, it is expectable
    that a human can perceive the features that caused the model prediction. Experimental
    results on two medical image datasets showed that the proposed approach allows
    visualizing where the most relevant features are localized and how they contributed
    to the model prediction by analyzing the generated images. Moreover, this generative
    approach may be helpful for the identification of new biomarkers.
  prefs: []
  type: TYPE_NORMAL
- en: Kim et al. ([62](#bib.bib62)) proposed the Counterfactual Generative Network
    (CGN), which is able to generate counterfactual images to explain the predictions
    of a pneumonia classifier from chest X-ray images. To guide the CGN towards the
    generation of contrastive images from query image, the prediction of the classification
    network was manipulated to shift the original class to the negative class. The
    subtraction of the counterfactual image from the input image allows the generation
    of attribution maps evidencing the most relevant regions to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the same fashion, Singla et al. ([138](#bib.bib138)) used a conditional Generative
    Adversarial Network (cGAN) to produce a set of counterfactual images with changed
    posterior probability to explain the class predictions of a chest X-ray classifier.
    Additionally, the context from semantic segmentation and object detection was
    incorporated into the loss function to preserve subtle information about the medical
    images in the generated counterfactual images. The validity of the generated counterfactual
    explanations was assessed through the use of three evaluation metrics, namely
    1) Fréchet Instance Distance score to evaluate the visual quality of the counterfactual
    images, 2) Counterfactual Validity score to quantify the class floppiness of the
    counterfactual images, and 3) Foreign Object Preservation score to assess the
    presence of unique properties of patients in the generated explanations. Furthermore,
    clinical measurements, namely, cardiothoracic ratio and costophrenic recess, were
    adopted to demonstrate the utility of the explanations in terms of the clinical
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Given the recent advances in the scope of image synthesis, the use of generative
    diffusion probabilistic models ([50](#bib.bib50)) to produce counterfactual explanations
    is an interesting future research direction as it is under-explored in medical
    imaging. The benefit of using these models to generate counterfactual explanations
    is related to their ability to handle missing data and their robustness to distributional
    shifts ([58](#bib.bib58)). A prominent example is the work of Sanchez et al. ([122](#bib.bib122))
    that relied on conditional diffusion models for synthesizing healthy counterfactual
    examples of brain images, allowing to segment the lesion through the difference
    between the observed image and the healthy counterfactual.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff2629524027eba98bd3a0b82bba7f80.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Explanation by Prototypes. During the model training, a set of prototypes
    are learned to visually represent a certain class (represented with blue and orange
    colours in the figure). In the test phase, the features extracted from the test
    image are compared with a set of prototypes using a similarity metric, such as
    cosine similarity. Then, the final class prediction is based on the similarity
    scores computed between the prototypes and the different parts of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3\. Propotypes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While most research on interpretability is still oriented towards the use of
    post-hoc approaches, some authors have advocated the need for devising inherently
    interpretable models ([118](#bib.bib118)) to obtain explanation that are indeed
    interpretable by humans. The learning of prototypes during the training phase
    of the model is a common strategy in the development of inherently interpretable
    models. This idea was initially explored in ([22](#bib.bib22)), where the authors
    incorporate a prototype layer at the end of the network ([22](#bib.bib22); [14](#bib.bib14)),
    named ProtoPNet, for bird species recognition. The rationale behind this approach
    is that different parts of the image act as class-representative prototypes during
    training. When a new image needs to be evaluated in the testing phase, the network
    finds the most similar prototypes to the parts of the test image. The final class
    prediction is based on a score computed with the similarities between the prototypes.
    Figure [7](#S5.F7 "Figure 7 ‣ 5.3.2\. Counterfactual Explanations ‣ 5.3\. Explanation
    by Examples ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey") illustrates the general pipeline
    for deriving a class prediction from the similarity score between different parts
    of the input image and a set of learned prototypes.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on ProtoPNet, Donnelly et al. ([33](#bib.bib33)) introduced the Deformable
    ProtoPNet. This prototypical case-based interpretable neural network provided
    spatially flexible deformable prototypes, i.e., prototypes that can change their
    relative position to detect semantically similar parts of an input image.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the significance of ProtoPNet, Hoffmann et al. ([51](#bib.bib51)) investigated
    its shortcomings, and proved that ProtoPNet could be susceptible to adversarial
    and compression noise, and thus compromise the inner interpretability of the model.
    Although these limitations were not significant in the bird recognition problem,
    the picture changes in high-stake applications, such as the healthcare domain,
    where the lack of robustness can have severe implications.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the applicability of these inherently interpretable networks to medical
    imaging, Kim et al. ([61](#bib.bib61)) proposed an interpretable diagnosis framework,
    dubbed XProtoNet, for chest radiography to learn disease representative features
    within a dynamic area, using an occurrence map. Contrary to ProtoPNet, in XProtoNet
    the prototypes are class-representative and completely dynamic in terms of area,
    which is particularly important for accommodating the high variability in size
    of discriminative regions of medical images. To produce an appropriate occurrence
    map, the authors introduced two regularization terms. The L1 loss forces the occurrence
    area to be small enough to avoid covering irrelevant regions, and the transformation
    loss approximates each occurrence map with a transformed version via an affine
    transformation that did not change the relative location of a disease pattern.
    The experiments on NIH Chest X-ray dataset ([159](#bib.bib159)) confirmed that
    the XProtoNet surpasses the state-of-the-art models in diagnosing chest diseases
    from X-ray images. Later, Singh et al. ([135](#bib.bib135)) introduced an interpretable
    deep learning model, named Generalized Prototypical Part Network (Gen-ProtoPNet),
    for detecting Covid-19 from X-ray images. Gen-ProtoPNet was inspired in the original
    ProtoPNet ([22](#bib.bib22)) and the NP-ProtoPNet ([136](#bib.bib136)). Unlike
    ProtoPNet and NP-ProtoPNet that use L2 distance to calculate the similarity between
    prototypes, Gen-ProtoPNet used a generalized version of the L2 distance, allowing
    the use of prototypical parts of any dimension, i.e., squared and rectangular
    spatial dimensions. Furthermore, the experiments on two Covid-19 chest X-ray datasets ([28](#bib.bib28);
    [157](#bib.bib157)) confirmed that using prototypical parts of spatial dimensions
    bigger than 1 x 1 improves performance of the model, specifically when using the
    VGG-16 model as the feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Relying on example-based strategies may be a more trustworthy option since the
    retrieved examples are plausible and tend to contain similar findings to the input
    query image. However, the performance of the example-based systems can be compromised
    if a significant number of examples per class is unavailable. This assumption
    is also valid in the case of prototype-based approaches, where performance depends
    directly on the diversity and amount of class-representative prototypes. Regarding
    the counterfactual explanations, it is desirable to discover credible causal structures
    to create ground-truth explanations to improve further the modelling of the interventions
    made over the images ([138](#bib.bib138)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Explanation by Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rationale behind concept-based learning approaches is using human-specified
    concepts as an intermediate step to derive the final predictions. This idea was
    used in the works of Kumar et al. ([67](#bib.bib67)) and Lampert et al. ([68](#bib.bib68)),
    with specific applications in few-shot learning approaches. In ([68](#bib.bib68)),
    the proposed model first estimated a set of attributes which were subsequently
    used to infer the final predictions. This type of model architectures were dubbed
    Concept Bottleneck Models (CBM) in the work of Koh et al. ([64](#bib.bib64)).
    In simple terms, these models relied on an encoder-decoder paradigm, where the
    encoder is responsible for predicting the concepts given the raw input image,
    and the decoder leverages the predicted concepts by the encoder to make the final
    predictions. The encoder is typically a CNN model with a bottleneck layer inserted
    after the last convolutional layer, while the decoder can be a multi-layer perceptron
    to map the concepts to the final predictions. This pipeline is illustrated in
    Figure [8](#S5.F8 "Figure 8 ‣ 5.4\. Explanation by Concepts ‣ 5\. XAI Methods
    in Medical Diagnosis ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey"). The idea of CBMs can be applied to any model just by inserting the
    bottleneck layer after the final convolutional layer. However, the main disadvantage
    of these methods is that annotated concepts are required. Koh et al. provided
    a systematic study on different ways to learn CBMs. Among the considered setup
    models, they concluded that the joint training is the preferred approach, which
    minimizes the weighted sum considering the classification loss and concept loss.
    The authors also stated that it is possible to intervene in the concept predictions
    to change the final output, which raises the question of to what extent it is
    feasible to revisit, for example, 100 concepts and perceive which concept would
    be modified to make the correct prediction. Later, Yuksekgonul et al. ([173](#bib.bib173))
    introduced Post-hoc CBM (PCBM) to address some limitations of CBM, specifically
    the need for concept-level annotations. The authors claim that PCBM can convert
    any pre-trained model into a concept bottleneck model. When concept annotations
    are unavailable, PCBM can leverage concept examples from other datasets and train
    linear binary classifiers to distinguish between examples of a single concept
    and negative examples. Yuksekgonul et al. ([173](#bib.bib173)) identified that
    the problem with CBM is that they require concept-level annotations per image,
    which is expensive and difficult to obtain, particularly in the context of skin
    lesions. Concept Activation Vectors (CAVs) ([60](#bib.bib60)) can be adopted to
    mitigate this by automatically predicting the presence or absence of single concepts
    on unseen images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a354d17a324582e295710666dd176c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Explanation by Concepts. In the first phase, the concept layer is
    trained to predict the concepts associated with the input image. Then, given a
    test image, the model first predicts the concepts presented in the image which
    are subsequently processed by a fully connected layer to infer the final predictions.
    Simultaneously, it is possible to produce visualizations of the filters of the
    concept layer that highlight the relevant regions for each concept. Additionally,
    the contribution of each concept to the final prediction is obtained to perceive
    which concepts had more influence on the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach was introduced by Chen et al. ([24](#bib.bib24)) where
    they proposed the Concept Whitening (CW), a module that is inserted into a neural
    network, and that can replace the Batch Normalization layer so that each point
    in the latent space has an interpretation in terms of known concepts. In a similar
    line of research, the decision process was decomposed in a set of human-interpretable
    concepts, along with a visual interpretation of the spatial localization where
    these concepts are present in the image ([163](#bib.bib163); [106](#bib.bib106)).
    In particular, Patrício et al. ([106](#bib.bib106)) proposed an approach for enforcing
    the visual coherence of concept activations by using a hard attention mechanism
    to guide the activations of concept filters towards the locations where to which
    the concept is visually related to. This strategy has been shown to improve the
    visual explanation of concept-based approaches for skin lesion diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ghorbani et al. ([41](#bib.bib41)) introduced ACE, which can automatically
    identify a set of high-level concepts in an unsupervised way. In a first phase,
    each image is segmented in multiple resolutions using the SLIC ([2](#bib.bib2))
    algorithm. Then, the segments are clustered by its similarity according to the
    euclidean distance, which is measured in the latent space. Each group of segments
    represents a different concept, labelled as pseudo-concepts. Lastly, to retain
    only the important concepts in each group, the TCAV ([60](#bib.bib60)) importance
    score was computed. The work of Fang et al. ([36](#bib.bib36)) built on the rationale
    of the ACE. The authors proposed the Visual Concept Mining (VCM) method to explain
    the decisions of an infectious keratitis classifier based on human-interpretable
    concepts. VCM encompasses two stages: (i) the proposed Potential Concept Generator
    module is responsible for identifying relevant concepts based on the segmentation
    of image patches according to the relevant regions highlighted on the produced
    saliency maps; (ii) the visual concept extractor module learns the similarity
    and diversity among the segmented image parts and groups them according to the
    DeepCluster ([21](#bib.bib21)) algorithm. The authors of VCM claimed that the
    concepts learned by their method were coherent with the medical annotations whilst
    being more diverse for the different classes, contrary to the ACE, which provides
    too broad concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: A disruptive approach was proposed by Sarkar et al. ([123](#bib.bib123)) that
    introduced an ante-hoc explainable model. A concept encoder on top of a backbone
    classification architecture is used for learning a set of human interpretable
    concepts providing thus an explanation for the classifier predictions. Additionally,
    the output of the concept encoder is passed to a decoder that reconstructs the
    input image, encouraging the model to capture the semantic features of the input
    image. Despite the method only reporting results for generic datasets, it would
    be interesting to extend this work for medical imaging datasets. Recently, following
    the philosophy of CBM, Yan et al. ([168](#bib.bib168)) proposed a method to improve
    the trustworthiness of skin cancer diagnosis by allowing doctors to intervene
    in the decisions of the trained models based on their knowledge and expertise.
    This human-in-the-loop framework allows for discovering and removing potential
    confounding behaviors of the model (e.g., artifacts or bias) within the dataset
    during the training phase. They concluded that modifying the output of the predicted
    concepts lead to a more accurate model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although concept-based explanation methods are under-explored in medical imaging,
    they constitute a promising way of providing human-understandable explanations.
    The main advantage of these methods is that they are interpretable by design since
    the final predictions are derived from the learned concepts. However, the dependency
    on manual annotation of these concepts is the major limitation in concept-learning
    approaches. Recently, to overcome the annotation-dependency of the concepts, proposed
    methods built on unsupervised techniques to discover a set of pseudo-concepts
    related to the input image ([41](#bib.bib41); [36](#bib.bib36)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Other Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to the above-discussed approaches, some authors have investigated
    alternative strategies to confer interpretability to the models, including Bayesian
    Neural Networks (BNN) to quantify the uncertainty associated with the model prediction
    or using adversarial training to improve the quality of the generated explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1\. Bayesian Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the success of CNN architectures, it is infeasible to quantify their
    uncertainty, given the deterministic nature associated with the internal parameters.
    Furthermore, CNNs are likely to overestimate the data when it is biased. In order
    to address these problems, Thiagarajan et al. ([145](#bib.bib145)) proposed the
    use of Bayesian CNNs (BCNN), which allow for the uncertainty estimation associated
    with the predictions. In particular, the uncertainty associated with the predictions
    of an Invasive Ductal Carcinoma (IDC) classifier on breast histopathology images
    was quantified. The examples characterized by a high value of uncertainty were
    projected into a lower-dimensional space using the t-SNE ([151](#bib.bib151))
    technique to facilitate data visualization and interpretation of the test data.
    Furthermore, the uncertainty allowed selecting the examples requiring human evaluation,
    which constitutes an interesting approach in the case of problems in the medical
    imaging domain. Similarly, Billah and Javed ([16](#bib.bib16)) relied on BCNNs
    to quantify the uncertainty associated to the predictions of a classifier model
    for the diagnosis of blood cancer. Recently, Gour and Jain ([43](#bib.bib43))
    proposed the UA-ConvNet, an uncertainty-aware CNN to detect COVID-19 from chest
    X-ray images and provide an estimation of the model uncertainty. For this, they
    used Monte Carlo dropout ([38](#bib.bib38)) to obtain a probability distribution
    of the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2\. Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In adversarial training, examples of the training set are augmented with adversarial
    perturbations at each training loop, allowing to increase the robustness of the
    model when provided with potential malicious examples ([11](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: The first attempt on using adversarial training to improve interpretability
    in a medical imaging diagnosis task was made by Margeloiu et al. ([93](#bib.bib93)).
    They explored the use of adversarial training to improve the interpretability
    of CNNs, mainly when applied to diagnosing skin cancer. Specifically, the trained
    model was retrained from scratch using adversarial training with the Projected
    Gradient Descent (PGD) adversarial attack ([89](#bib.bib89)). The experiments
    on the dermatology dataset HAM10000 ([149](#bib.bib149)) showed that saliency
    maps of the robust model are significantly sharper and visually more coherent
    than those obtained from the standard trained model.
  prefs: []
  type: TYPE_NORMAL
- en: However, further research is needed since adversarial training is under-explored
    in medical imaging interpretability. Specifically, applying the above-referred
    findings to other datasets and network architectures becomes necessary to perceive
    the generalization capability of the methods. Furthermore, as stated by the authors
    in ([93](#bib.bib93)), the proposed method is not ready to be deployed in real-world
    scenarios due to the sensitivity of saliency methods to training noise, which
    can cause those methods to assign importance to artifacts available in the image
    (e.g., dark regions and irrelevant medical regions). Therefore, it is crucial
    to understand the limitations of adversarial training to improve interpretability
    in medical imaging diagnosis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The uncertainty estimation associated with a classifier’s predictions is helpful
    in the clinical workflow since clinicians can support their decisions based on
    the uncertainty value. Additionally, the use of adversarial training can be viewed
    as a method to improve the robustness of the model to adversarial attacks. As
    also demonstrated by Margeloiu et al. ([93](#bib.bib93)), the produced explanations
    by adversarially trained models seem to be more coherent and sharpening. Despite
    the under-exploitation of these strategies in medical imaging, these preliminary
    findings may encourage the emergence of methods adopting these alternative strategies
    as an additional layer to improve the models’ reliability and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the type of explanation modality (visual or textual), there are
    different ways to assess the quality of the generated explanations. We divide
    the evaluation metrics used in the literature into two categories: (i) evaluation
    metrics to assess the quality of visual explanations; and (ii) evaluation metrics
    for measuring the quality of textual explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Evaluating the Quality of Visual Explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluating the quality of model explanations remains an active area of research.
    A common approach for evaluating the model interpretability, specifically when
    applied to the medical domain, is to request an expert opinion from the clinicians
    and radiologists. However, this evaluation approach is time-consuming and depends
    on the level of experience of the clinicians  ([39](#bib.bib39); [150](#bib.bib150)).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, there has been attempts to propose evaluation metrics capable of
    objectively assessing the quality of the explanations. Samek et al. ([121](#bib.bib121))
    were precursors in contributing to the question of how to objectively evaluate
    the quality of heatmaps by introducing the area over the Most Relevant First (MoRF)
    perturbation curve (AOPC) measure. This measure is based on a region perturbation
    strategy that iteratively removes information from some regions of the input image
    according to its relevance to the class, allowing to perceive the performance
    decay of the model. The conducted experiments showed that a large AOPC value denotes
    high model sensitivity to the perturbations, indicating that the heatmap is actually
    informative.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the work in ([121](#bib.bib121)), Petsiuk et al. ([108](#bib.bib108))
    proposed two causal metrics, namely deletion and insertion, to evaluate the produced
    explanations for a black-box model. The deletion metric measures the degradation
    of the class probability, as important pixels of the image, derived from the saliency
    map, are removed. On the other hand, the insertion metric intends to measure the
    increase of the class probability, as pixels are inserted based on the generated
    saliency map.
  prefs: []
  type: TYPE_NORMAL
- en: Later, Hooker et al. ([52](#bib.bib52)) argued that the modification-based metrics
    introduced by Petsiuk et al. ([108](#bib.bib108)) might not capture the actual
    reasoning behind the model’s degradation since this degradation could be due to
    artefacts introduced by the values used to replace the removed pixels. Thus, the
    authors proposed the RemOve And Retrain (ROAR) to evaluate interpretability methods
    by verifying how the accuracy of a retrained model degrades as important features
    are removed. The most important features are removed in certain regions of the
    image with a fixed uninformative value for each interpretability method. The main
    drawback of this metric is the required retraining of the model, which is computationally
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Rio-Torto et al. ([115](#bib.bib115)) proposed the POMPOM (Percentage
    of Meaningful Pixels Outside the Mask) metric, which determines the number of
    meaningful pixels outside the region of interest in relation to the total number
    of pixels, to evaluate the quality of a given explanation. Similarly, Barnett
    et al. ([14](#bib.bib14)) introduced the Activation Precision evaluation metric,
    to quantify the proportion of relevant information from the “relevant region”
    used to classify the mass margin regarding the radiologist annotations. Despite
    the relevance of both metrics, they require manual annotations of the masks, which
    is time-consuming and may be difficult to obtain for some medical image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the valuable contribution of these proposed evaluation metrics,
    we believe that a new evaluation method for model interpretability can be developed
    with the aid of the Bayesian Neural Networks (BNN) characteristics. Considering
    that the weights of the BNN follow a probability distribution, we can sample different
    “models” from the posterior distribution and generate an arbitrary number of explanations
    for a given example ([20](#bib.bib20)). Then, using intersection or union operations
    over those generated explanations seems to be an interesting direction to estimate
    whether most of the explanations highlight the same Region Of Interest (ROI).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Evaluating the Quality of Textual Explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this category, the metrics are used to measure the quality of the generated
    text, and are originated from generic NLP tasks. The most used metrics in the
    reviewed papers were BLEU ([104](#bib.bib104)), ROUGE-L ([76](#bib.bib76)), METEOR ([71](#bib.bib71))
    and CIDEr ([155](#bib.bib155)).
  prefs: []
  type: TYPE_NORMAL
- en: 'BLEU (Bilingual Evaluation Understudy) score is the most common evaluation
    metric in NLP. In simple terms, it compares n-gram matches between the generated
    sentence (also known as candidate sentence) and the ground-truth sentence (also
    known as reference sentence), expressed in modified precision²²2Takes into consideration
    the maximum frequency of each n-gram in the reference sentence (clipped count).
    The modified precision is then calculated by summing the clipped counts of the
    candidate sentence divided by the total number of candidate n-grams. for each
    n-gram. The BLEU metric has N variations (BLEU-N), typically N $\in\{1,2,3,4\}$,
    with respect to the considered n-grams. The value of BLEU score ranges from 0
    to 1, which means that the closer to 1, the better the translation. Formally,
    BLEU score is calculated according to the following formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $BLEU=BP.\exp{\sum_{n=1}^{N}w_{n}\log{p_{n}}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $p_{n}$ is the modified precision for $n$-gram, $w_{n}$ is a weight,
    ranging from $0$ and $1$, and $\sum_{n=1}^{N}w_{n}=1$, i.e., if $N=4$, $w_{n}=1/N$,
    and BP is the brevity penalty that penalizes short generated sentences, denoted
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $BP=\left\{\begin{matrix}1&amp;\textrm{if}\;\;c>r\\ \exp(1-\frac{1}{c})&amp;\textrm{if}\;\;c\leq
    r\end{matrix}\right.,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $c$ is the length of candidate translation, and $r$ is the reference corpus
    length.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric is actually
    a set of metrics. We focus on the ROUGE-L variant as it was the prevalent metric
    in the most of the reviewed methods. ROUGE-L measures the Longest Common Subsequence
    (LCS) between the generated sentence and the ground-truth sentence both in terms
    of precision and recall. This means that if both sentences share a long sub-sentence,
    the similarity between the two sentences is expected to be high. The final value
    of ROUGE-L is given in F1 score, as formally described bellow (Eq. [3](#S6.E3
    "In 6.2\. Evaluating the Quality of Textual Explanations ‣ 6\. Evaluation Metrics
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $ROUGE-L_{F1}=2\cdot\frac{Precision\cdot Recall}{Precision+Recall},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Precision=\frac{LCS(c,r)}{m}$ and $Recall=\frac{LCS(c,r)}{n}$, with $LCS(c,r)$
    denoting the Longest Common Subsequence between the candidate sentence (c) and
    the reference sentence (r), $m$ is the number of $n$-grams in the reference sentence,
    and $n$ the number of $n$-grams in the candidate sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Differently from the two above-discussed metrics, METEOR (Metric for Evaluation
    of Translation with Explicit Ordering) gives attention to the position of the
    words in the sentence by including a chunk penalty that weights the final score.
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $METEOR=F_{mean}\cdot(1-Penalty)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $Penalty=0.5\cdot(\frac{m}{n})$, where $m$ is the number of chunks and
    $n$ is the total number of unigram matches, and $F_{mean}=\frac{10PR}{R+9P}$.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, CIDEr (Consensus-Based Image Description Evaluation) is an evaluation
    metric that uses the Term Frequency Inverse Document Frequency (TF-IDF) ([116](#bib.bib116))
    for weighting each $n$-gram. The intuition behind CIDEr is that $n$-grams that
    frequently appear in the reference sentences are less likely to be informative,
    and hence they have a lower weight using the IDF term. CIDEr[n], $n=\{1,2,3,4\}$,
    is the average cosine similarity between the candidate sentence and the reference
    sentences, considering both precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF-IDF weighting $g_{k}(s_{ij})$ for each $n$-gram $w_{k}$ is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $g_{k}(s_{ij})=\frac{h_{k}(s_{ij})}{\sum_{w_{l}\in\omega}h_{l}(s_{ij})}\log(\frac{\left&#124;I\right&#124;}{\sum_{I_{p}\in
    I}}\min(1,\sum_{q}h_{k}(s_{pq}))),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $h_{k}(s_{ij})$ is the number of times an $n$-gram occurs in a reference
    sentence $s_{ij}$, and $h_{k}(c_{i})$ for the candidate sentence $c_{i}$, $\omega$
    is the vocabulary of all $n$-grams, and $I$ is the set of all images.
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $CIDEr_{n}(c_{i},S_{i})=\frac{1}{m}\sum_{j}\frac{g^{n}(c_{i})\cdot
    g^{n}(s_{ij})}{\&#124;g^{n}(c_{i})\&#124;\&#124;g^{n}(s_{ij})\&#124;}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The final CIDEr score is the weighted average of CIDEr[n]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $CIDEr(c_{i},S_{i})=\sum_{n=1}^{N}w_{k}CIDEr_{n}(c_{i},S_{i}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $w_{n}=1/N,N=4$.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Performance Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, we reviewed several works focused on providing explanations
    to the output of automated medical diagnosis. At the end of the review, an important
    question arises: “What is the best approach?”. Unfortunately, in many cases, there
    is no trivial answer, since most methods adopt different evaluation metrics making
    performance comparison with other competing methods unfeasible. To add up to the
    question, we compare the performance of some of the methods reviewed. In order
    to find common ground for a fair comparison of the methods, only those methods
    that considered the same dataset for evaluation purposes were selected. As presented
    in table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), IU Chest X-ray ([32](#bib.bib32))
    is the most used dataset among the reviewed methods. As such, we verified whether
    the methods that were evaluated on the IU Chest X-ray ([32](#bib.bib32)) used
    the same evaluation metrics. Additionally, and since MIMIC-CXR ([56](#bib.bib56))
    dataset provides an official training-test partition, we include some methods
    that report results on this dataset under the same evaluation metrics. In this
    way, a comparison of the performance between these methods was carried out. Table
    [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning Methods
    in Medical Image Classification: A Survey") conveys the results in terms of a
    set of NLP evaluation metrics (BLEU score, ROUGE, METEOR, and CIDEr) for each
    of the selected methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Performance of the selected methods that generate textual explanations
    for interpreting the decision of a classifier. Spaces marked with a “-” mean no
    value is available for the respective metric. ¹ Results were taken from the work
    of Liu et al. ([83](#bib.bib83)). The methods evaluated on the IU Chest X-ray
    dataset are marked with a symbol ($\star$, $\dagger$, $\circ$, $\ddagger$), meaning
    that the methods with each symbol used the same training-validation-test split.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Model | Year | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | METEOR |
    CIDEr |'
  prefs: []
  type: TYPE_TB
- en: '|  | IU Chest X-Ray |'
  prefs: []
  type: TYPE_TB
- en: '| Find. + Impress. | Jing et al. ([55](#bib.bib55))^† | 2017 | ${0.517}$ |
    ${0.386}$ | 0.306 | 0.247 | ${0.447}$ | 0.217 | $0.327$ |'
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([137](#bib.bib137))^∘ | 2019 | $0.374$ | $0.224$ | $0.153$
    | $0.110$ | $0.308$ | $0.164$ | ${0.360}$ |'
  prefs: []
  type: TYPE_TB
- en: '| HRNN ([170](#bib.bib170))^† | 2019 | $0.445$ | $0.292$ | $0.201$ | $0.154$
    | $0.344$ | $0.175$ | $0.342$ |'
  prefs: []
  type: TYPE_TB
- en: '| Selivanov et al. ([127](#bib.bib127))^† | 2023 | 0.520 | 0.390 | $0.296$
    | $0.235$ | 0.450 | $-$ | 0.701 |'
  prefs: []
  type: TYPE_TB
- en: '| Findings | HRGR-Agent ([74](#bib.bib74))^⋆ | 2018 | $0.438$ | $0.298$ | $0.208$
    | $0.151$ | $0.322$ | $-$ | $0.343$ |'
  prefs: []
  type: TYPE_TB
- en: '| TieNet¹ ([160](#bib.bib160))^⋆ | 2018 | $0.330$ | $0.194$ | $0.124$ | $0.081$
    | $0.311$ | $-$ | $1.334$ |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([83](#bib.bib83))^⋆ | 2019 | $0.369$ | $0.246$ | $0.171$ | $0.115$
    | $0.359$ | $-$ | 1.490 |'
  prefs: []
  type: TYPE_TB
- en: '| R2Gen ([25](#bib.bib25))^⋆ | 2020 | ${0.470}$ | ${0.304}$ | ${0.219}$ | ${0.165}$
    | ${0.371}$ | ${0.187}$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| PPKED ([81](#bib.bib81))^⋆ | 2021 | $0.483$ | $0.315$ | $0.224$ | $0.168$
    | $0.376$ | $-$ | $0.351$ |'
  prefs: []
  type: TYPE_TB
- en: '| CA ([82](#bib.bib82))^⋆ | 2021 | $0.492$ | $0.314$ | $0.222$ | $0.169$ |
    $0.381$ | $0.193$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| ICT ([175](#bib.bib175))^⋆ | 2023 | ${0.503}$ | 0.341 | 0.246 | ${0.186}$
    | ${0.390}$ | 0.208 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| METransformer ([161](#bib.bib161))^⋆ | 2023 | $0.483$ | $0.322$ | $0.228$
    | $0.172$ | $0.380$ | $0.192$ | $0.435$ |'
  prefs: []
  type: TYPE_TB
- en: '| VLCI ([23](#bib.bib23))^⋆ | 2023 | 0.505 | $0.334$ | $0.245$ | 0.189 | $0.397$
    | $0.204$ | $0.456$ |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([169](#bib.bib169))^⋆ | 2023 | ${0.497}$ | $0.319$ | $0.230$
    | $0.174$ | 0.399 | $-$ | $0.407$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '|  | TieNet¹ ([160](#bib.bib160))^‡ | 2018 | $0.332$ | $0.212$ | $0.142$ |
    $0.095$ | $0.296$ | $-$ | $1.004$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Liu et al. ([83](#bib.bib83))^‡ | 2019 | $0.352$ | ${0.223}$ | ${0.153}$
    | ${0.104}$ | ${0.307}$ | $-$ | ${1.153}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | R2Gen ([25](#bib.bib25)) | 2020 | ${0.353}$ | $0.218$ | $0.145$ | $0.103$
    | $0.277$ | $0.142$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | CA ([82](#bib.bib82)) | 2021 | $0.350$ | $0.219$ | $0.152$ | $0.109$ |
    $0.283$ | ${0.151}$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | PPKED ([81](#bib.bib81)) | 2021 | $0.360$ | $0.224$ | $0.149$ | $0.106$
    | $0.284$ | $0.149$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSAT ([162](#bib.bib162)) | 2022 | $0.373$ | $0.235$ | $0.162$ | $0.120$
    | $0.282$ | $0.143$ | $0.299$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ICT ([175](#bib.bib175)) | 2023 | ${0.376}$ | ${0.233}$ | ${0.157}$ |
    ${0.113}$ | $0.276$ | $0.144$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | METransformer ([161](#bib.bib161)) | 2023 | $0.386$ | ${0.250}$ | ${0.169}$
    | ${0.124}$ | $0.291$ | 0.152 | $0.362$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Yang et al. ([169](#bib.bib169)) | 2023 | $0.386$ | $0.237$ | $0.157$
    | $0.111$ | $0.274$ | $-$ | $0.111$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | VLCI ([23](#bib.bib23)) | 2023 | $0.400$ | $0.245$ | $0.165$ | $0.119$
    | $0.280$ | $0.150$ | $0.190$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Selivanov et al. ([127](#bib.bib127)) | 2023 | 0.725 | 0.626 | 0.505 |
    0.418 | 0.480 | $-$ | 1.989 |'
  prefs: []
  type: TYPE_TB
- en: 7.1\. Results Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is worth noticing that all the presented results in Table [3](#S7.T3 "Table
    3 ‣ 7\. Performance Comparison ‣ Explainable Deep Learning Methods in Medical
    Image Classification: A Survey") were taken from the original paper of each method,
    except for the TieNet ([160](#bib.bib160)) method, whose results were taken from
    the work of Liu et al. ([83](#bib.bib83)), since in the original paper the authors
    only provide results in ChestX-ray14 using BLEU, METEOR, and ROUGE-L.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the methods that considered the “findings+impression” section from
    the radiology report to generate the free-text report, and as evidenced by the
    results in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey"), the model proposed
    by Selivanov et al. ([127](#bib.bib127)) proved to be more accurate in terms of
    BLUE-1, BLEU-2 and ROUGE-L. The preprocessing and squeezing approaches used for
    clinical records jointly with the combination of two large language models (Show-Attend-Tell
    (SAT) ([166](#bib.bib166)) and Generative Pretrained Transformer (GPT-3) ([18](#bib.bib18)))
    can explain the performance improvement. Moreover, generated reports are accompanied
    by 2D heatmaps that localize each pathology on the input scans. Conversely, the
    method of Jing et al. ([55](#bib.bib55)) demonstrates superior performance in
    terms of BLEU-3, BLEU-4 and METEOR, which can be explained by the co-attention
    mechanism adopted by the authors. As the method of Singh et al. ([137](#bib.bib137))
    does not follow the same data partition, it is not strictly comparable to other
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Among the selected methods that only consider the “findings” section, ICT ([175](#bib.bib175))
    demonstrates superior performance on BLEU-2, BLEU-3 and METEOR metrics. Their
    transformer-based model incorporates two modules responsible for capturing inter-intra
    features of medical reports as auxiliary information and subsequently calibrating
    the report generation process by integrating that information. This combination
    results in a performance boost in the report generation model and improves the
    quality of medical diagnosis. In contrast, with regard to the BLEU-1 score, VLCI ([23](#bib.bib23))
    exhibits a superior performance, comparable to ICT ([175](#bib.bib175)), which
    could be justified by the cross-modal causal intervention strategy employed by
    the authors to mitigate spurious correlations from visual and linguistic confounders.
    Furthermore, the model proposed by Liu et al. ([83](#bib.bib83)) adopted a fine-tuning
    procedure that uses reinforcement learning via CIDEr to ensure more coherent report
    generation, which could justify the performance in the CIDEr metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the methods that report results for the MIMIC-CXR dataset, it is
    worth noting that only TieNet ([160](#bib.bib160)) and the approach by Liu et
    al. [82] have followed a distinct data partition strategy. Consequently, these
    methods are included here primarily for reference, as their divergent data partitioning
    makes direct comparisons with other methods infeasible. On the contrary, the model
    proposed by Selivanov et al. ([127](#bib.bib127)) distinguishes itself from the
    others by exhibiting superior performance across all metrics except for METEOR
    (which was not reported by the authors). Their approach leveraged the capabilities
    of two large language models, SAT and GPT-3, trained on large text corpora. This
    fusion of language models significantly improved standard text generation scores,
    as shown in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison ‣ Explainable
    Deep Learning Methods in Medical Image Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, it is noticeable in Table [3](#S7.T3 "Table 3 ‣ 7\. Performance Comparison
    ‣ Explainable Deep Learning Methods in Medical Image Classification: A Survey")
    that the use of transformer-based models ([81](#bib.bib81); [82](#bib.bib82);
    [162](#bib.bib162); [161](#bib.bib161); [169](#bib.bib169)) with additional mechanisms
    to capture complex and relevant features proved to be effective in improving the
    performance of the generated reports, as observed in the results obtained in the
    MIMIC-CXR dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. General Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although XAI is a relatively recent research field, its constant growth is undeniable,
    with applications in many areas, particularly in the medical domain. However,
    despite the advances and the efforts made toward developing interpretable deep
    learning-based models for medical imaging, there are open issues that require
    more research and advances in this growing field. This section identifies open
    challenges in the literature and potential research paths to further improve the
    trustworthiness of provided explanations and foster the adoption of deep learning-based
    systems into clinical routine.
  prefs: []
  type: TYPE_NORMAL
- en: Based on reviewed literature, it can be concluded that the go-to method for
    model interpretation in medical imaging is producing saliency maps, using classical
    techniques, such as Grad-CAM, Integrated Gradients, or LRP. However, as evidenced
    by some authors, saliency maps can be unreliable and fragile ([118](#bib.bib118);
    [3](#bib.bib3)), as they often highlight irrelevant regions in the images. In
    addition, it is frequent that very similar explanations are given for different
    classes, and often none of them are useful explanations ([118](#bib.bib118)).
    Thus, the development of inherently interpretable models has been a line of research
    with promising results in the medical imaging domain. Although these methods remain
    largely unexplored in medical imaging, future research will undoubtedly be devoted
    to develop inherently interpretable models. These models have the primary benefit
    of providing their own explanations, which contributes to their transparency and
    fidelity, increasing the chances of being adopted into the clinical routine.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, different end-users could have different backgrounds and
    preferences at interpreting the explanation, which can generate some contradictory
    opinions. This fostered the use of textual explanations, which are preferred over
    visual explanations by some authors ([39](#bib.bib39)) since they are inherently
    understandable by humans ([152](#bib.bib152)). Since then, methods that generate
    textual descriptions for explaining a prediction and multimodal methods that combine
    visual and textual explanations have emerged. However, generating free-text reports
    is deemed a challenging task since the radiologist reports are technically structured,
    and the most used language models based on RNNs have some limitations in generating
    long texts, as stated by Pascanu et al. ([105](#bib.bib105)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to text-based explanations, the use of example-based explanations
    was proposed, since this explanation modality is directly linked to how humans
    try to explain something to the other humans. This way, some example-based approaches
    have emerged with promising results that were even comparable to the performance
    of standard classifiers. These example-based methods include CBR approaches, prototype-based
    and concept-based strategies. Recently, Schutte et al. ([126](#bib.bib126)) introduced
    a disruptive approach that strives to generate synthetic examples to explain a
    model decision, as discussed in section [5.5](#S5.SS5 "5.5\. Other Approaches
    ‣ 5\. XAI Methods in Medical Diagnosis ‣ Explainable Deep Learning Methods in
    Medical Image Classification: A Survey"). The possible limitations of using example-based
    methods are related to the availability of a considerable amount of data covering
    all the classes in a balanced way, without forgetting the sensibility of these
    methods to adversarial attacks, even though this can be prevented by using adversarial
    training ([93](#bib.bib93)).'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, other methods have emerged as candidates for explaining the decision
    of a model. The adoption of Bayesian Neural Networks to estimate or quantify the
    uncertainty regarding the model predictions might be an interesting option, although
    few works attempted to prove its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Future research in medical image interpretability can also include the use of
    vision transformers (ViTs) ([34](#bib.bib34)). According to ([94](#bib.bib94)),
    vision transformers proved to be comparable with CNN in terms of performance (accuracy)
    in the medical classification tasks. Furthermore, ViTs have the benefit of providing
    a type of built-in saliency maps that are used to better understand the model’s
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding medical image datasets, existing publicly available datasets for medical
    image captioning are limited in number and there is need to generate more large
    size datasets. Moreover, most of the existing datasets has focused only on few
    anatomical parts of body, such as chest, while ignoring other important parts
    like breast and brain ([9](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Challenges and Future Research Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the rapid pace of advances in the medical imaging and deep learning,
    there are problems that remain without a definitive solution.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Small datasets: The collection of medical data depends on multiple entities
    and background bureaucracies. Nevertheless, the main issue is related to the availability
    of the physicians in annotating a vast amount of data, that is time-consuming
    and costly. This is even more critical in the XAI field, where additional annotations
    are required (e.g., concepts, textual descriptions). For this reason, interpretability-compliant
    medical datasets have a lower representativeness of the classes, resulting in
    poor generalizability and applicability of the developed methods to real-world
    scenarios. To surpass these constraints, distinct data augmentation techniques
    have emerged as an alternative for collecting new data. Recently, Wickramanayake
    et al.  ([164](#bib.bib164)) proposed the BRACE framework to augment the dataset
    based on concept-based explanations from model decisions, which can help to discover
    the samples in the under-represented regions in the training set. Furthermore,
    Wickramanayake et al. introduced a utility function to select the images in the
    under-representation regions and concepts that caused the misclassification. The
    images with a high utility score are selected to incorporate the training set.
    On the other hand, the use of generative approaches to perform data augmentation
    in a controlled way might be an interesting research direction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Insufficient labelled data: Although most works rely on the supervised learning
    paradigm, it is often not the best choice when working in the medical domain,
    since the process of label annotation is time-consuming and costly for large-scale
    datasets, especially in domains such as digital pathology where the manual annotation
    is subject to inter- and intra-observer variability ([172](#bib.bib172)). Transfer
    learning was adopted in most works to address these issues, but this technique
    is not completely effective in the medical domain, since the original models were
    trained in images belonging to standard object detection datasets (e.g., ImageNet),
    which do not share the same patterns of medical imagery. This way, Self-Supervised
    Learning (SSL) has emerged to tackle these challenges, allowing the network to
    learn visual meaningful feature representations without the need of annotated
    data ([27](#bib.bib27)). Besides its effectiveness in dealing with scarce labelled
    data, it confers robustness to the model, rendering it more resistant to adversarial
    attacks. In medical imaging, the use of SSL seems to be a promising research direction
    due to the characteristics of medical datasets. Furthermore, contrastive learning
    approaches have achieved impressive results, due to the contrastive loss that
    encourages the network to learn high-level features that occur in images across
    multiple views, which are created through the use of geometric transformation
    such as random cropping, color distortion, gaussian blur. For a comprehensive
    overview of the state-of-the-art of SSL with a particular focus on medical domain
    we refer the reader to ([27](#bib.bib27)). The inherently interpretable models,
    specifically the concept bottleneck models, require the annotation of concepts
    for each class or image. The presented datasets in Table [2](#S4.T2 "Table 2 ‣
    4\. Datasets ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") show that the majority does not include this type of annotation, hampering
    the rapid employment of the concept bottleneck models. Furthermore, despite the
    significance of the existing methods that emerged to surpass these issues (CAV ([60](#bib.bib60))),
    annotations regarding clinical concepts remain necessary. This could be solved
    with closer cooperation between clinicians and the AI community. As discussed
    in Section [4.6](#S4.SS6 "4.6\. Discussion ‣ 4\. Datasets ‣ Explainable Deep Learning
    Methods in Medical Image Classification: A Survey"), appropriate annotations in
    medical imaging datasets are needed to ensure a quick development of interpretability
    methods for medical diagnosis.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qualitative assessment of the explanations: The automated evaluation of the
    explanations provided by interpretability methods remains an open challenge. As
    previously discussed in section [6.1](#S6.SS1 "6.1\. Evaluating the Quality of
    Visual Explanations ‣ 6\. Evaluation Metrics ‣ Explainable Deep Learning Methods
    in Medical Image Classification: A Survey"), the most adopted method for evaluating
    the explanations in the context of the medical domain is to resort to the clinician’s
    expertise. However, considering variability in experts opinions ([147](#bib.bib147)),
    this strategy is particularly biased and subjective. On the other hand, the existing
    strategies for objectively measuring the quality of visual explanations are still
    dependent on manual annotations of relevant regions ([115](#bib.bib115)), or iterative
    model retraining (ROAR ([52](#bib.bib52))). For these reasons, we believe that
    the design of objective metrics for assessing the quality of the model explanations
    will be one of the important research trends on the topic of XAI.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Report generation in medical imaging: Text-based explanations are usually obtained
    using RNN-based approaches through the generation of words forming a sentence.
    Nevertheless, RNN-based approaches have some limitations in generating long text
    reports ([105](#bib.bib105)). Consequently, the use of Transformers for the automatic
    generation of radiology reports was adopted in an attempt to overcome the limitations
    of traditional RNNs, namely the problem of vanishing gradients. The self-attention
    mechanism of the Transformer architecture allows for the learning of contextual
    relationships between the words that constitute the sequence. In addition, Transformer-based
    networks can be trained faster than traditional RNNs as they allow for simultaneous
    processing of sequential data. With the rise of foundation models, such as Generative
    Pretrained Transformers ([127](#bib.bib127)), the limitations encountered in previous
    methods have been alleviated, specifically the coherency of the generated texts.
    On the other hand, using concept-based approaches as a transition bridge between
    free-text report generation and concept-based explanations may be an exciting
    future research direction. Instead of trying to generate free-text reports, which
    is challenging, having a set of concepts that are sufficient to describe the phenomenon
    depicted in the image can support clinicians in writing a complete report.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment in clinical practice: The implementation of XAI methods in clinical
    practice requires rigorous validation to ensure their safety, effectiveness, and
    reliability, which can be challenging due to the complex and dynamic nature of
    clinical environments. Additionally, the field of medical imaging is subject to
    rigorous regulations, and the development and deployment of XAI methods must comply
    with regulatory and legal requirements ([42](#bib.bib42)), such as FDA approvals ([15](#bib.bib15)),
    data privacy regulations, and liability concerns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper reviewed the advances on explainable deep learning applied to medical
    imaging diagnosis. First we introduced a comparative analysis between the existing
    surveys on the topic, where the major conclusions and weaknesses of the each were
    highlighted. Then, the most prominent XAI methods were briefly described to provide
    the readers with fundamental concepts of the field, necessary to the discussion
    of the recent advances on the medical imaging domain. Additionally, several frameworks
    that implement XAI methods were presented and a brief discussion of the existing
    medical imaging datasets was drawn. After this, we comprehensively reviewed the
    works focused on explaining the decision process of deep learning applied to medical
    imaging. The works were grouped according to the explanation modality comprising
    explanations by feature attribution, explanations by text, explanation by examples
    and explanations by concepts. Contrary to other surveys on the topic, we focused
    this review on inherently interpretable models over post-hoc approaches, which
    has been recently considered a future research direction on deep learning interpretability.
    The discussion of the adopted evaluation metrics used in the literature was also
    carried out, where we described the existing metrics to assess the quality of
    visual explanations and the commonly NLP metrics to evaluate the quality of the
    generated textual explanations. Additionally, a comparison of the performance
    of a set of prominent XAI methods was performed based on the dataset used and
    the evaluation metrics adopted. Finally, the discussion and future outlook in
    XAI for medical diagnosis were addressed where we identified open challenges in
    the literature and potential research avenues to improve the trustworthiness of
    provided explanations and foment the adoption of deep learning-based systems into
    clinical routine. To conclude, we believe that this survey will be helpful to
    the XAI community, particularly to the medical imaging field, as an entry point
    to guide the research and the future advances in the topic of XAI.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was funded by the Portuguese Foundation for Science and Technology
    (FCT) under the PhD grant “2022.11566.BD”, and supported by NOVA LINCS (UIDB/04516/2020)
    with the financial support of FCT.IP.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achanta et al. (2012) Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
    Lucchi, Pascal Fua, and Sabine Süsstrunk. 2012. SLIC Superpixels Compared to State-of-the-Art
    Superpixel Methods. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    34, 11 (2012), 2274–2282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adebayo et al. (2018) Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow,
    Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. *Advances in
    Neural Information Processing Systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akgül et al. (2011) Ceyhun Burak Akgül, Daniel L Rubin, Sandy Napel, Christopher F
    Beaulieu, Hayit Greenspan, and Burak Acar. 2011. Content-Based Image Retrieval
    in Radiology: Current Status and Future Directions. *Journal of Digital Imaging*
    24, 2 (2011), 208–222.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alber et al. (2019) Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer,
    Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert
    Müller, Sven Dähne, and Pieter-Jan Kindermans. 2019. iNNvestigate Neural Networks!
    *Journal of Machine Learning Research* 20, 93 (2019), 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allegretti et al. (2021) Stefano Allegretti, Federico Bolelli, Federico Pollastri,
    Sabrina Longhitano, Giovanni Pellacani, and Costantino Grana. 2021. Supporting
    Skin Lesion Diagnosis with Content-Based Image Retrieval. In *Proceedings of the
    IEEE International Conference on Pattern Recognition (ICPR)*. 8053–8060.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancona et al. (2018) Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus
    Gross. 2018. Towards better understanding of gradient-based attribution methods
    for Deep Neural Networks. In *International Conference on Learning Representations
    (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Armato III et al. (2011) Samuel G Armato III, Geoffrey McLennan, Luc Bidaut,
    Michael F McNitt-Gray, Charles Meyer, Anthony P Reeves, Binsheng Zhao, Denise R
    Aberle, Claudia I Henschke, Eric A Hoffman, et al. 2011. The Lung Image Database
    Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference
    database of lung nodules on CT scans. *Medical Physics* 38, 2 (2011), 915–931.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ayesha et al. (2021) Hareem Ayesha, Sajid Iqbal, Mehreen Tariq, Muhammad Abrar,
    Muhammad Sanaullah, Ishaq Abbas, Amjad Rehman, Muhammad Farooq Khan Niazi, and
    Shafiq Hussain. 2021. Automatic medical image interpretation: State of the art
    and future directions. *Pattern Recognition* 114 (2021), 107856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bach et al. (2015) Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick
    Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations
    for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. *PLoS
    ONE* 10, 7 (2015), e0130140.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2021) Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.
    Recent Advances in Adversarial Training for Adversarial Robustness. In *Proceedings
    of the International Joint Conference on Artificial Intelligence (IJCAI)*. 4312–4321.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barata et al. (2019) Catarina Barata, Jorge S Marques, and M Emre Celebi. 2019.
    Deep Attention Model for the Hierarchical Diagnosis of Skin Lesions. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*.
    2757–2765.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barata and Santiago (2021) Catarina Barata and Carlos Santiago. 2021. Improving
    the Explainability of Skin Cancer Diagnosis Using CBIR. In *Proceedings of the
    International Conference on Medical Image Computing and Computer-Assisted Intervention
    (MICCAI)*. 550–559.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barnett et al. (2021) Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao,
    Chaofan Chen, Yinhao Ren, Joseph Y Lo, and Cynthia Rudin. 2021. Interpretable
    Mammographic Image Classification using Case-Based Reasoning and Deep Learning.
    In *Workshop on Deep Learning, Case-Based Reasoning, and AutoML: Present and Future
    Synergies - IJCAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benjamens et al. (2020) Stan Benjamens, Pranavsingh Dhunnoo, and Bertalan Meskó.
    2020. The state of artificial intelligence-based FDA-approved medical devices
    and algorithms: an online database. *NPJ Digital Medicine* 3, 1 (2020), 118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Billah and Javed (2022) Mohammad Ehtasham Billah and Farrukh Javed. 2022. Bayesian
    Convolutional Neural Network-based Models for Diagnosis of Blood Cancer. *Applied
    Artificial Intelligence* (2022), 1–22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolhasani et al. (2020) Hamidreza Bolhasani, Elham Amjadi, Maryam Tabatabaeian,
    and Somayyeh Jafarali Jassbi. 2020. A histopathological image dataset for grading
    breast invasive ductal carcinomas. *Informatics in Medicine Unlocked* 19 (2020),
    100341.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in Neural
    Information Processing Systems (NeurIPS)* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bustos et al. (2020) Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and
    Maria de la Iglesia-Vayá. 2020. PadChest: A large chest x-ray image dataset with
    multi-label annotated reports. *Medical Image Analysis* 66 (2020), 101797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bykov et al. (2021) Kirill Bykov, Marina M-C Höhne, Adelaida Creosteanu, Klaus-Robert
    Müller, Frederick Klauschen, Shinichi Nakajima, and Marius Kloft. 2021. Explaining
    Bayesian Neural Networks. *arXiv preprint arXiv:2108.10346* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caron et al. (2018) Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs
    Douze. 2018. Deep Clustering for Unsupervised Learning of Visual Features. In
    *Proceedings of the European Conference on Computer Vision (ECCV)*. 132–149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019) Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett,
    Jonathan Su, and Cynthia Rudin. 2019. This Looks Like That: Deep Learning for
    Interpretable Image Recognition. In *Proceedings of the International Conference
    of Neural Information Processing Systems (NIPS)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Weixing Chen, Yang Liu, Ce Wang, Guanbin Li, Jiarui Zhu,
    and Liang Lin. 2023. Visual-Linguistic Causal Intervention for Radiology Report
    Generation. *arXiv preprint arXiv:2303.09117* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Zhi Chen, Yijie Bei, and Cynthia Rudin. 2020a. Concept Whitening
    for Interpretable Image Recognition. *Nature Machine Intelligence* 2, 12 (2020),
    772–782.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan.
    2020b. Generating Radiology Reports via Memory-driven Transformer. In *Proceedings
    of the Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    1439–1449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chittajallu et al. (2019) Deepak Roy Chittajallu, Bo Dong, Paul Tunison, Roddy
    Collins, Katerina Wells, James Fleshman, Ganesh Sankaranarayanan, Steven Schwaitzberg,
    Lora Cavuoto, and Andinet Enquobahrie. 2019. XAI-CBIR: Explainable AI System for
    Content based Retrieval of Video Frames from Minimally Invasive Surgery Videos.
    In *Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI)*.
    66–69.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhury et al. (2021) Alexander Chowdhury, Jacob Rosenthal, Jonathan Waring,
    and Renato Umeton. 2021. Applying Self-Supervised Learning to Medicine: Review
    of the State of the Art and Medical Implementations. *Informatics* 8, 3 (2021),
    59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. (2020) Joseph Paul Cohen, Paul Morrison, and Lan Dao. 2020. COVID-19
    Image Data Collection. *arXiv 2003.11597* (2020). [https://github.com/ieee8023/covid-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daneshjou et al. (2022) Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto
    Novoa, and James Y Zou. 2022. SkinCon: A skin disease dataset densely annotated
    by domain experts for fine-grained debugging and analysis. *Advances in Neural
    Information Processing Systems* 35 (2022), 18157–18167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Darias et al. (2021) Jesus M Darias, Belén Dıaz-Agudo, and Juan A Recio-Garcia.
    2021. A Systematic Review on Model-agnostic XAI Libraries. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Faria et al. (2019) Sergio de Faria, Jose Filipe, Pedro Pereira, Luis Tavora,
    Pedro Assuncao, Miguel Santos, Rui Fonseca-Pinto, Felicidade Santiago, Victoria
    Dominguez, and Martinha Henrique. 2019. Light Field Image Dataset of Skin Lesions.
    In *Proceedings of the International Conference of the IEEE Engineering in Medicine
    and Biology Society (EMBC)*. 3905–3908.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demner-Fushman et al. (2016) Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman,
    Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J
    McDonald. 2016. Preparing a collection of radiology examinations for distribution
    and retrieval. *Journal of the American Medical Informatics Association* 23, 2
    (2016), 304–310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donnelly et al. (2021) Jon Donnelly, Alina Jade Barnett, and Chaofan Chen.
    2021. Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable
    Prototypes. *arXiv preprint arXiv:2111.15000* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In
    *Proceedings of the International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eitel and for the Alzheimer’s Disease Neuroimaging Initiative  (ADNI) Fabian
    Eitel and Kerstin Ritter for the Alzheimer’s Disease Neuroimaging Initiative (ADNI).
    2019. Testing the Robustness of Attribution Methods for Convolutional Neural Networks
    in MRI-Based Alzheimer’s Disease Classification. In *Interpretability of Machine
    Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision
    Support (IMIMIC)*. 3–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2020) Zhengqing Fang, Kun Kuang, Yuxiao Lin, Fei Wu, and Yu-Feng
    Yao. 2020. Concept-based Explanation for Fine-grained Images and Its Application
    in Infectious Keratitis Classification. In *Proceedings of the ACM International
    Conference on Multimedia*. 700–708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fong et al. (2019) Ruth Fong, Mandela Patrick, and Andrea Vedaldi. 2019. Understanding
    Deep Networks via Extremal Perturbations and Smooth Masks. In *Proceedings of
    the IEEE/CVF International Conference on Computer Vision (ICCV)*. 2950–2958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
    a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In
    *Proceedings of the International Conference on Machine Learning (ICML)*. 1050–1059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P
    Bradley, and Lyle J Palmer. 2019. Producing Radiologist-Quality Reports for Interpretable
    Deep Learning. In *Proceedings of the IEEE International Symposium on Biomedical
    Imaging (ISBI)*. 1275–1279.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghorbani et al. (2019a) Amirata Ghorbani, Abubakar Abid, and James Zou. 2019a.
    Interpretation of Neural Networks is Fragile. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. 3681–3688.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghorbani et al. (2019b) Amirata Ghorbani, James Wexler, James Y Zou, and Been
    Kim. 2019b. Towards Automatic Concept-Based Explanations. In *Advances in Neural
    Information Processing Systems*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European Union
    Regulations on Algorithmic Decision-Making and a “Right to Explanation”. *AI Magazine*
    38, 3 (2017), 50–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gour and Jain (2022) Mahesh Gour and Sweta Jain. 2022. Uncertainty-aware convolutional
    neural network for COVID-19 X-ray images classification. *Computers in Biology
    and Medicine* 140 (2022), 105047.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graziani et al. (2020) Mara Graziani, Vincent Andrearczyk, Stéphane Marchand-Maillet,
    and Henning Müller. 2020. Concept attribution: Explaining CNN decisions to physicians.
    *Computers in Biology and Medicine* 123 (2020), 103865.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groh et al. (2021) Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel
    Han, Aerin Kim, Arash Koochek, and Omar Badri. 2021. Evaluating deep neural networks
    trained on clinical images in dermatology with the fitzpatrick 17k dataset. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 1820–1828.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulum et al. (2021) Mehmet A Gulum, Christopher M Trombley, and Mehmed Kantardzic.
    2021. A Review of Explainable Deep Learning Cancer Detection Models in Medical
    Imaging. *Applied Sciences* 11, 10 (2021), 4573.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunning and Aha (2019) David Gunning and David Aha. 2019. DARPA’s Explainable
    Artificial Intelligence (XAI) Program. *AI Magazine* 40, 2 (2019), 44–58.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2021) Tarun Gupta, Libin Kutty, Ritu Gahir, Nnamdi Ukwu, Sayantan
    Polley, and Marcus Thiel. 2021. IRTEX: Image Retrieval with Textual Explanations.
    In *Proceedings of the IEEE International Conference on Human-Machine Systems
    (ICHMS)*. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hansell et al. (2008) David M. Hansell, Alexander A. Bankier, Heber MacMahon,
    Theresa C. McLoud, Nestor L. Müller, and Jacques Remy. 2008. Fleischner Society:
    Glossary of Terms for Thoracic Imaging. *Radiology* 246, 3 (2008), 697–722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    Diffusion Probabilistic Models. *Advances in Neural Information Processing Systems*
    33 (2020), 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. (2021) Adrian Hoffmann, Claudio Fanconi, Rahul Rade, and Jonas
    Kohler. 2021. This Looks Like That… Does it? Shortcomings of Latent Space Prototype
    Interpretability in Deep Networks. In *ICML Workshop on Theoretic Foundation,
    Criticism, and Application Trend of Explainable AI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hooker et al. (2019) Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and
    Been Kim. 2019. A Benchmark for Interpretability Methods in Deep Neural Networks.
    In *Advances in Neural Information Processing Systems*, Vol. 32. 9737–9748.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Brian Hu, Bhavan Vasu, and Anthony Hoogs. 2022. X-MIR: EXplainable
    Medical Image Retrieval. In *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision (WACV)*. 440–450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irvin et al. (2019) Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana
    Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie
    Shpanskaya, et al. 2019. CheXpert: A Large Chest Radiograph Dataset with Uncertainty
    Labels and Expert Comparison. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. 590–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jing et al. (2018) Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the Automatic
    Generation of Medical Imaging Reports. In *Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (ACL)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2019) Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
    Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven
    Horng. 2019. MIMIC-CXR, a de-identified publicly available database of chest radiographs
    with free-text reports. *Scientific Data* 6, 1 (2019), 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kawahara et al. (2019) Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano,
    and Ghassan Hamarneh. 2019. Seven-Point Checklist and Skin Lesion Classification
    Using Multitask Multimodal Neural Nets. *IEEE Journal of Biomedical and Health
    Informatics* 23, 2 (2019), 538–546.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazerouni et al. (2022) Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein
    Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. 2022.
    Diffusion models for medical image analysis: A comprehensive survey. *arXiv preprint
    arXiv:2211.07804* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keenan et al. (2020) Oisin J. F. Keenan, George Holland, Julian F. Maempel,
    John F. Keating, and Chloe E. H. Scott. 2020. Correlations between radiological
    classification systems and confirmed cartilage loss in severe knee osteoarthritis.
    *The Bone & Joint Journal* 102-B, 3 (2020), 301–309.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai,
    James Wexler, Fernanda B. Viégas, and Rory Sayres. 2018. Interpretability Beyond
    Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).
    In *Proceedings of the International Conference on Machine Learning (ICML)*. 2668–2677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021b) Eunji Kim, Siwon Kim, Minji Seo, and Sungroh Yoon. 2021b.
    XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 15719–15728.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021a) Junho Kim, Minsu Kim, and Yong Man Ro. 2021a. Interpretation
    of Lesional Detection via Counterfactual Generation. In *Proceedings of the IEEE
    International Conference on Image Processing (ICIP)*. 96–100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kindermans et al. (2018) Pieter-Jan Kindermans, Kristof T Schütt, Maximilian
    Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. 2018. Learning
    how to explain neural networks: PatternNet and PatternAttribution. In *International
    Conference on Learning Representations (ICML)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koh et al. (2020) Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann,
    Emma Pierson, Been Kim, and Percy Liang. 2020. Concept Bottleneck Models. In *Proceedings
    of the International Conference on Machine Learning (ICML)*. 5338–5348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kokhlikyanet (2019) Kokhlikyanet. 2019. Pytorch Captum. [Online]. Accessed
    January, 21 2021\. Available: [https://github.com/pytorch/captum](https://github.com/pytorch/captum).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. *Advances
    in Neural Information Processing Systems (NIPS)* 25 (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2009) Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and
    Shree K. Nayar. 2009. Attribute and Simile Classifiers for Face Verification.
    In *Proceedings of the IEEE International Conference on Computer Vision (CVPR)*.
    365–372.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lampert et al. (2009) Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling.
    2009. Learning to Detect Unseen Object Classes by Between-class Attribute Transfer.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 951–958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lamy et al. (2019) Jean-Baptiste Lamy, Boomadevi Sekar, Gilles Guezennec, Jacques
    Bouaud, and Brigitte Séroussi. 2019. Explainable artificial intelligence for breast
    cancer: A visual case-based reasoning approach. *Artificial Intelligence in Medicine*
    94 (2019), 42–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lapuschkin et al. (2016) Sebastian Lapuschkin, Alexander Binder, Grégoire Montavon,
    Klaus-Robert Müller, and Wojciech Samek. 2016. The LRP Toolbox for Artificial
    Neural Networks. *Journal of Machine Learning Research* 17, 114 (2016), 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lavie and Agarwal (2007) Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automatic
    Metric for MT Evaluation with High Levels of Correlation with Human Judgments.
    In *Proceedings of the Workshop on Statistical Machine Translation*. 228–231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2019) Hyebin Lee, Seong Tae Kim, and Yong Man Ro. 2019. Generation
    of Multimodal Justification Using Visual Word Constraint Model for Explainable
    Computer-Aided Diagnosis. In *Interpretability of Machine Intelligence in Medical
    Image Computing and Multimodal Learning for Clinical Decision Support*. 21–29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2017) Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai
    Miyake, Mia Gorovoy, and Daniel L Rubin. 2017. A curated mammography data set
    for use in computer-aided detection and diagnosis research. *Scientific Data*
    4, 1 (2017), 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018) Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. 2018.
    Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation.
    In *Proceedings of the International Conference on Neural Information Processing
    Systems (NIPS)*. 1537–1547.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Mingjie Li, Rui Liu, Fuyu Wang, Xiaojun Chang, and Xiaodan
    Liang. 2023. Auxiliary signal-guided knowledge encoder-decoder for medical report
    generation. *World Wide Web* 26, 1 (2023), 253–270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. 74–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Lee (2020) Tsung-Chieh Lin and Hsi-Chieh Lee. 2020. Covid-19 Chest Radiography
    Images Analysis Based on Integration of Image Preprocess, Guided Grad-CAM, Machine
    Learning and Risk Management. In *Proceedings of the International Conference
    on Medical and Health Informatics (ICMHI)*. 281–288.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lipton (2017) Zachary C Lipton. 2017. The Doctor Just Won’t Accept That! *arXiv
    preprint arXiv:1711.08037* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Litjens et al. (2018) Geert Litjens, Peter Bandi, Babak Ehteshami Bejnordi,
    Oscar Geessink, Maschenka Balkenhol, et al. 2018. 1399 H&E-stained sentinel lymph
    node sections of breast cancer patients: the CAMELYON dataset. *GigaScience* 7,
    6 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. (2017) Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
    Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak,
    Bram Van Ginneken, and Clara I Sánchez. 2017. A Survey on Deep Learning in Medical
    Image Analysis. *Medical Image Analysis* 42 (2017), 60–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou.
    2021a. Exploring and Distilling Posterior and Prior Knowledge for Radiology Report
    Generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*. 13753–13762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang,
    and Xu Sun. 2021b. Contrastive Attention for Automatic Chest X-ray Report Generation.
    In *Proceedings of the Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*. 269–280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie
    Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. 2019. Clinically Accurate
    Chest X-Ray Report Generation. In *Machine Learning for Healthcare Conference*.
    249–269.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lopatina et al. (2020) Alina Lopatina, Stefan Ropele, Renat Sibgatulin, Jürgen R
    Reichenbach, and Daniel Güllmar. 2020. Investigation of Deep-Learning-Driven Identification
    of Multiple Sclerosis Patients Based on Susceptibility-Weighted Images Using Relevance
    Analysis. *Frontiers in N euroscience* (2020), 1356.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lucieri et al. (2022) Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Alexander
    Braun, Muhammad Imran Malik, Andreas Dengel, and Sheraz Ahmed. 2022. ExAID: A
    Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions.
    *Computer Methods and Programs in Biomedicine* (2022), 106620.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lucieri et al. (2020) Adriano Lucieri, Muhammad Naseer Bajwa, Andreas Dengel,
    and Sheraz Ahmed. 2020. Explaining AI-based Decision Support Systems using Concept
    Localization Maps. In *Proceedings of the International Conference on Neural Information
    Processing*. 185–193.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg et al. (2018) Scott M Lundberg, Gabriel G Erion, and Su-In Lee. 2018.
    Consistent Individualized Feature Attribution for Tree Ensembles. *arXiv preprint
    arXiv:1802.03888* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg and Lee (2017) Scott M Lundberg and Su-In Lee. 2017. A Unified Approach
    to Interpreting Model Predictions. In *Proceedings of the International Conference
    on Neural Information Processing Systems (NIPS)*. 4768–4777.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
    Attacks. In *Proceedings of the International Conference on Learning Representations
    (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magesh et al. (2020) Pavan Rajkumar Magesh, Richard Delwin Myloth, and Rijo Jackson
    Tom. 2020. An Explainable Machine Learning Model for Early Detection of Parkinson’s
    Disease using LIME on DaTscan Imagery. *Computers in Biology and Medicine* 126
    (2020), 104041.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Majkowska et al. (2020) Anna Majkowska, Sid Mittal, David F Steiner, Joshua J
    Reicher, Scott Mayer McKinney, Gavin E Duggan, Krish Eswaran, Po-Hsuan Cameron Chen,
    Yun Liu, Sreenivasa Raju Kalidindi, et al. 2020. Chest Radiograph Interpretation
    with Deep Learning Models: Assessment with Radiologist-adjudicated Reference Standards
    and Population-adjusted Evaluation. *Radiology* 294, 2 (2020), 421–431.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malhi et al. (2019) Avleen Malhi, Timotheus Kampik, Husanbir Pannu, Manik Madhikermi,
    and Kary Främling. 2019. Explaining Machine Learning-Based Classifications of
    In-Vivo Gastral Images. In *Digital Image Computing: Techniques and Applications
    (DICTA)*. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Margeloiu et al. (2020) Andrei Margeloiu, Nikola Simidjievski, Mateja Jamnik,
    and Adrian Weller. 2020. Improving Interpretability in Medical Imaging Diagnosis
    using Adversarial Training. In *Medical Imaging Meets NeurIPS Workshop*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matsoukas et al. (2021) Christos Matsoukas, Johan Fredin Haslum, Magnus Söderberg,
    and Kevin Smith. 2021. Is it Time to Replace CNNs with Transformers for Medical
    Images?. In *ICCV 2021: Workshop on Computer Vision for Automated Medical Diagnosis
    (CVAMD)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mendonça et al. (2013) Teresa Mendonça, Pedro Ferreira, Jorge Marques, André
    Marcal, and Jorge Rozeira. 2013. PH 2-A dermoscopic image database for research
    and benchmarking. In *Proceedings of the International Conference of the IEEE
    Engineering in Medicine and Biology Society (EMBS)*. 5437–5440.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messina et al. (2022) Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia
    Besa, Sergio Uribe, Marcelo Andía, Cristian Tejos, Claudia Prieto, and Daniel
    Capurro. 2022. A survey on deep learning and explainability for automatic report
    generation from medical images. *ACM Computing Surveys (CSUR)* 54, 10s (2022),
    1–40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Molnar (2022) Christoph Molnar. 2022. Interpretable Machine Learning: A Guide
    for Making Black Box Models Explainable. Available: [h](h)ttps://christophm.github.io/interpretable-ml-book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montavon et al. (2017) Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder,
    Wojciech Samek, and Klaus-Robert Müller. 2017. Explaining NonLinear Classification
    Decisions with Deep Taylor Decomposition. *Pattern Recognition* 65 (2017), 211–222.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moore and Swartout (1988) Johanna D Moore and William R Swartout. 1988. Explanation
    in Expert Systems: A Survey. *University of Southern California* (1988).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreira et al. (2012) Inês C Moreira, Igor Amaral, Inês Domingues, António
    Cardoso, Maria Joao Cardoso, and Jaime S Cardoso. 2012. INbreast: Toward a Full-field
    Digital Mammographic Database. *Academic Radiology* 19, 2 (2012), 236–248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nevitt et al. (2006) M Nevitt, D Felson, and Gayle Lester. 2006. The Osteoarthritis
    Initiative. *Protocol for the Cohort Study* 1 (2006).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2020) Ha Nguyen, Khanh Lam, Linh Le, Hieu Pham, Dat Tran, et al.
    2020. VinDr-CXR: An Open Dataset of Chest X-rays with Radiologist’s Annotations.
    *arXiv preprint arXiv:2012.15029* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2021) Hieu T. Nguyen, Hieu H. Pham, Nghia T. Nguyen, Ha Q. Nguyen,
    Thang Q. Huynh, et al. 2021. VinDr-SpineXR: A Deep Learning Framework for Spinal
    Lesions Detection and Classification from Radiographs. In *Proceedings of the
    International Conference on Medical Image Computing and Computer Assisted Intervention
    (MICCAI)*. 291–301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In
    *Proceedings of the Annual Meeting of the Association for Computational Linguistics
    (ACL)*. 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pascanu et al. (2013) Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013.
    On the difficulty of training Recurrent Neural Networks. In *Proceedings of the
    International Conference on Machine Learning (ICML)*. 1310–1318.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patrício et al. (2023) Cristiano Patrício, João C. Neves, and Luis F. Teixeira.
    2023. Coherent Concept-based Explanations in Medical Image and Its Application
    to Skin Lesion Diagnosis. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW)*. 3799–3808.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pelka et al. (2018) Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa,
    et al. 2018. Radiology Objects in COntext (ROCO): A Multimodal Image Dataset.
    In *Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation
    of Biomedical Data and Expert Label Synthesis*. 180–189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petsiuk et al. (2018) Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. RISE:
    Randomized Input Sampling for Explanation of Black-box Models. In *British Machine
    Vision Conference (BMVC)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pocevičiūtė et al. (2020) Milda Pocevičiūtė, Gabriel Eilertsen, and Claes Lundström.
    2020. Survey of XAI in Digital Pathology. In *Artificial Intelligence and Machine
    Learning for Digital Pathology: State-of-the-Art and Future Challenges*. 56–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punn and Agarwal (2021) Narinder Singh Punn and Sonali Agarwal. 2021. Automated
    diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned
    deep neural networks. *Applied Intelligence* 51, 5 (2021), 2689–2702.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2018) Pranav Rajpurkar, Jeremy Irvin, Robyn L Ball, Kaylie
    Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, et al. 2018.
    Deep learning for chest radiograph diagnosis: A retrospective comparison of the
    CheXNeXt algorithm to practicing radiologists. *PLoS medicine* 15, 11 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rehman et al. (2021) Aasia Rehman, Muheet Ahmed Butt, and Majid Zaman. 2021.
    A Survey of Medical Image Analysis Using Deep Learning Approaches. In *Proceedings
    of the IEEE International Conference on Computing Methodologies and Communication
    (ICCMC)*. 1334–1342.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier.
    In *Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*. 1135–1144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richter et al. (2020) Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco
    Ruiz, and Omer Deniz Akyildiz. 2020. VarGrad: A Low-Variance Gradient Estimator
    for Variational Inference. *Advances in Neural Information Processing Systems*
    33 (2020), 13481–13492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rio-Torto et al. (2020) Isabel Rio-Torto, Kelwin Fernandes, and Luís Teixeira.
    2020. Understanding the decisions of CNNs: An in-model approach. *Pattern Recognition
    Letters* 133 (2020), 373–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robertson (2004) Stephen Robertson. 2004. Understanding Inverse Document Frequency:
    on Theoretical Arguments for IDF. *Journal of Documentation* 60 (2004), 503–520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotemberg et al. (2021) Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein,
    Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza,
    Pascale Guitera, David Gutman, et al. 2021. A Patient-Centric Dataset of Images
    and Metadata for Identifying Melanomas Using Clinical Context. *Scientific Data*
    8, 1 (2021), 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rudin (2019) Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning
    Models for High Stakes Decisions and Use Interpretable Models Instead. *Nature
    Machine Intelligence* 1, 5 (2019), 206–215.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadeghi et al. (2018) Mahya Sadeghi, Parmit K Chilana, and M Stella Atkins.
    2018. How Users Perceive Content-based Image Retrieval for Identifying Skin Images.
    In *Understanding and Interpreting Machine Learning in Medical Image Computing
    Applications*. 141–148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salahuddin et al. (2022) Zohaib Salahuddin, Henry C. Woodruff, Avishek Chatterjee,
    and Philippe Lambin. 2022. Transparency of Deep Neural Networks for Medical Image
    Analysis: A Review of Interpretability Methods. *Computers in Biology and Medicine*
    140 (2022), 105111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samek et al. (2016) Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian
    Lapuschkin, and Klaus-Robert Müller. 2016. Evaluating the Visualization of What
    a Deep Neural Network Has Learned. *IEEE Transactions on Neural Networks and Learning
    Systems* 28, 11 (2016), 2660–2673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanchez et al. (2022) Pedro Sanchez, Antanas Kascenas, Xiao Liu, Alison Q O’Neil,
    and Sotirios A Tsaftaris. 2022. What is healthy? generative counterfactual diffusion
    for lesion localization. In *Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI
    2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings*.
    34–44.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarkar et al. (2022) Anirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, and
    Vineeth N Balasubramanian. 2022. A Framework for Learning Ante-hoc Explainable
    Models via Concepts. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*. 10286–10295.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sayres et al. (2019) Rory Sayres, Ankur Taly, Ehsan Rahimy, Katy Blumer, David
    Coz, Naama Hammel, Jonathan Krause, Arunachalam Narayanaswamy, Zahra Rastegar,
    Derek Wu, et al. 2019. Using a Deep Learning Algorithm and Integrated Gradients
    Explanation to Assist Grading for Diabetic Retinopathy. *Ophthalmology* 126, 4
    (2019), 552–564.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schlegl et al. (2015) Thomas Schlegl, Sebastian M Waldstein, Wolf-Dieter Vogl,
    Ursula Schmidt-Erfurth, and Georg Langs. 2015. Predicting Semantic Descriptions
    from Medical Images with Convolutional Neural Networks. In *Proceedings of the
    International Conference on Information Processing in Medical Imaging (IPMI)*.
    437–448.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schutte et al. (2021) Kathryn Schutte, Olivier Moindrot, Paul Hérent, Jean-Baptiste
    Schiratti, and Simon Jégou. 2021. Using StyleGAN for Visual Interpretability of
    Deep Learning Models on Medical Images. In *Medical Imaging Meets NeurIPS Workshop*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selivanov et al. (2023) Alexander Selivanov, Oleg Y Rogov, Daniil Chesakov,
    Artem Shelmanov, Irina Fedulova, and Dmitry V Dylov. 2023. Medical image captioning
    via generative pretrained transformers. *Scientific Reports* 13, 1 (2023), 4171.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selvaraju et al. (2017) Ramprasaath R Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-based Localization. In *Proceedings
    of the IEEE International Conference on Computer Vision (ICCV)*. 618–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamshad et al. (2023) Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris
    Khan, Munawar Hayat, Fahad Shahbaz Khan, and Huazhu Fu. 2023. Transformers in
    Medical Imaging: A Survey. *Medical Image Analysis* (2023), 102802.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shapley (2016) L. S. Shapley. 2016. A Value for n-Person Games. In *Contributions
    to the Theory of Games (AM-28)*, Harold William Kuhn and Albert William Tucker
    (Eds.). Vol. II. 307–318.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrikumar et al. (2017) Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
    2017. Learning Important Features Through Propagating Activation Differences.
    In *Proceedings of the International Conference on Machine Learning (ICML)*, Vol. 70\.
    3145–3153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silva et al. (2020) Wilson Silva, Alexander Poellinger, Jaime S Cardoso, and
    Mauricio Reyes. 2020. Interpretability-guided Content-based Medical Image Retrieval.
    In *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*. 305–314.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan et al. (2014) Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
    2014. Deep Inside Convolutional Networks: Visualising Image Classification Models
    and Saliency Maps. In *Workshop at International Conference on Learning Representations
    (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2020) Amitojdeep Singh, Sourya Sengupta, and Vasudevan Lakshminarayanan.
    2020. Explainable Deep Learning Models in Medical Image Analysis. *Journal of
    Imaging* 6, 6 (2020), 52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Yow (2021a) Gurmail Singh and Kin-Choong Yow. 2021a. An Interpretable
    Deep Learning Model For Covid-19 Detection With Chest X-ray Images. *IEEE Access*
    9 (2021), 85198–85208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh and Yow (2021b) Gurmail Singh and Kin-Choong Yow. 2021b. These do not
    Look Like Those: An Interpretable Deep Learning Model for Image Recognition. *IEEE
    Access* 9 (2021), 41482–41493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2019) Sonit Singh, Sarvnaz Karimi, Kevin Ho-Shon, and Len Hamey.
    2019. From Chest X-Rays to Radiology Reports: A Multimodal Machine Learning Approach.
    In *Digital Image Computing: Techniques and Applications (DICTA)*. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singla et al. (2023) Sumedha Singla, Motahhare Eslami, Brian Pollack, Stephen
    Wallace, and Kayhan Batmanghelich. 2023. Explaining the Black-box Smoothly - A
    Counterfactual Approach. *Medical Image Analysis* 84 (2023), 102721.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smilkov et al. (2017) Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas,
    and Martin Wattenberg. 2017. SmoothGrad: Removing Noise by Adding Noise. In *ICML
    Workshop on Visualization for Deep Learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Society (2019) Asia Pacific Tele-Ophthalmology Society. 2019. APTOS Diabetic
    Retinopathy Dataset. [Online]. Accessed November, 23 2021\. Available: [https://www.kaggle.com/c/aptos2019-blindness-detection/data](https://www.kaggle.com/c/aptos2019-blindness-detection/data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spanhol et al. (2015) Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean,
    and Laurent Heutte. 2015. A Dataset for Breast Cancer Histopathological Image
    Classification. *IEEE Transactions on Biomedical Engineering* 63, 7 (2015), 1455–1462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Springenberg et al. (2014) Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
    Brox, and Martin Riedmiller. 2014. Striving for Simplicity: The All Convolutional
    Net. *arXiv preprint arXiv:1412.6806* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Li Sun, Weipeng Wang, Jiyun Li, and Jingsheng Lin. 2019. Study
    on Medical Image Report Generation Based on Improved Encoding-Decoding Method.
    In *Intelligent Computing Theories and Application*. 686–696.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
    Axiomatic Attribution for Deep Networks. In *Proceedings of the International
    Conference on Machine Learning (ICML)*. 3319–3328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thiagarajan et al. (2021) Ponkrshnan Thiagarajan, Pushkar Khairnar, and Susanta
    Ghosh. 2021. Explanation and Use of Uncertainty Obtained by Bayesian Neural Network
    Classifiers for Breast Histopathology Images. *IEEE Transactions on Medical Imaging*
    41, 4 (2021), 815–825.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjoa and Guan (2020) Erico Tjoa and Cuntai Guan. 2020. A Survey on Explainable
    Artificial Intelligence (XAI): Towards Medical XAI. *IEEE Transactions on Neural
    Networks and Learning Systems* 32, 11 (2020), 4793–4813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tonekaboni et al. (2019) Sana Tonekaboni, Shalmali Joshi, Melissa D McCradden,
    and Anna Goldenberg. 2019. What Clinicians Want: Contextualizing Explainable Machine
    Learning for Clinical End Use. In *Proceedings of the Machine Learning for Healthcare
    Conference*. 359–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tschandl et al. (2019) Philipp Tschandl, Giuseppe Argenziano, Majid Razmara,
    and Jordan Yap. 2019. Diagnostic Accuracy of Content Based Dermatoscopic Image
    Retrieval with Deep Classification Features. *British Journal of Dermatology*
    181, 1 (2019), 155–165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler.
    2018. The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images
    of Common Pigmented Skin Lesions. *Scientific Data* 5, 1 (2018), 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsiknakis et al. (2020) Nikos Tsiknakis, Eleftherios Trivizakis, Evangelia E
    Vassalou, Georgios Z Papadakis, Demetrios A Spandidos, Aristidis Tsatsakis, Jose
    Sánchez-García, Rafael López-González, Nikolaos Papanikolaou, Apostolos H Karantanas,
    et al. 2020. Interpretable artificial intelligence framework for COVID-19 screening
    on chest X-rays. *Experimental and Therapeutic Medicine* 20, 2 (2020), 727–735.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton.
    2008. Visualizing Data using t-SNE. *Journal of Machine Learning Research* 9,
    11 (2008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Velden et al. (2022) Bas H.M. van der Velden, Hugo J. Kuijf, Kenneth G.A.
    Gilhuijs, and Max A. Viergever. 2022. Explainable Artificial Intelligence (XAI)
    in Deep Learning-based Medical Image Analysis. *Medical Image Analysis* 79 (2022),
    102470.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Lent et al. (2004) Michael Van Lent, William Fisher, and Michael Mancuso.
    2004. An Explainable Artificial Intelligence System for Small-unit Tactical Behavior.
    In *Proceedings of the National Conference on Artificial Intelligence (AAAI)*.
    900–907.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    Is All You Need. In *Advances in Neural Information Processing Systems*. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    2015. CIDEr: Consensus-based Image Description Evaluation. In *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 4566–4575.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Linda Wang, Zhong Qiu Lin, and Alexander Wong. 2020a. COVID-Net:
    a tailored deep convolutional neural network design for detection of COVID-19
    cases from chest X-ray images. *Scientific Reports* 10, 1 (2020), 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell
    Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William
    Merrill, et al. 2020b. CORD-19: The COVID-19 Open Research Dataset. *ArXiv* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Sutong Wang, Yunqiang Yin, Dujuan Wang, Yanzhang Wang, and
    Yaochu Jin. 2021. Interpretability-Based Multimodal Convolutional Neural Networks
    for Skin Lesion Diagnosis. *IEEE Transactions on Cybernetics* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi
    Bagheri, and Ronald Summers. 2017. ChestX-ray8: Hospital-scale Chest X-ray Database
    and Benchmarks on Weakly-Supervised Classification and Localization of Common
    Thorax Diseases. In *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 2097–2106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald
    Summers. 2018. TieNet: Text-Image Embedding Network for Common Thorax Disease
    Classification and Reporting in Chest X-rays. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 9049–9058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. 2023.
    METransformer: Radiology Report Generation by Transformer with Multiple Learnable
    Expert Tokens. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 11558–11567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Zhanyu Wang, Mingkang Tang, Lei Wang, Xiu Li, and Luping
    Zhou. 2022. A Medical Semantic-Assisted Transformer for Radiographic Report Generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    III*. 655–664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wickramanayake et al. (2021a) Sandareka Wickramanayake, Wynne Hsu, and Mong Li
    Lee. 2021a. Comprehensible Convolutional Neural Networks via Guided Concept Learning.
    In *Proceedings of the IEEE International Joint Conference on Neural Networks
    (IJCNN)*. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wickramanayake et al. (2021b) Sandareka Wickramanayake, Wynne Hsu, and Mong Li
    Lee. 2021b. Explanation-based Data Augmentation for Image Classification. In *Advances
    in Neural Information Processing Systems*, Vol. 34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windisch et al. (2020) Paul Windisch, Pascal Weber, Christoph Fürweger, Felix
    Ehret, Markus Kufeld, Daniel Zwahlen, and Alexander Muacevic. 2020. Implementation
    of model explainability for a basic brain tumor detection using convolutional
    neural networks on MRI slices. *Neuroradiology* 62, 11 (2020), 1515–1518.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell:
    Neural image caption generation with visual attention. In *International Conference
    on Machine Learning (ICML)*. 2048–2057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Yesheng Xu, Ming Kong, Wenjia Xie, Runping Duan, Zhengqing
    Fang, Yuxiao Lin, Qiang Zhu, Siliang Tang, Fei Wu, and Yu-Feng Yao. 2021. Deep
    Sequential Feature Learning in Clinical Image Classification of Infectious Keratitis.
    *Engineering* 7, 7 (2021), 1002–1010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2023) Siyuan Yan, Zhen Yu, Xuelin Zhang, Dwarikanath Mahapatra,
    Shekhar S Chandra, Monika Janda, Peter Soyer, and Zongyuan Ge. 2023. Towards Trustable
    Skin Cancer Diagnosis via Rewriting Model’s Decision. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. 11568–11577.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Shuxin Yang, Xian Wu, Shen Ge, Zhuozhao Zheng, S Kevin Zhou,
    and Li Xiao. 2023. Radiology report generation with a learned knowledge base and
    multi-modal alignment. *Medical Image Analysis* (2023), 102798.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2019) Changchang Yin, Buyue Qian, Jishang Wei, Xiaoyu Li, Xianli
    Zhang, Yang Li, and Qinghua Zheng. 2019. Automatic Generation of Medical Imaging
    Diagnostic Report with Hierarchical Recurrent Neural Network. In *Proceedings
    of the IEEE International Conference on Data Mining (ICDM)*. 728–737.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2019) Kyle Young, Gareth Booth, Becks Simpson, Reuben Dutton,
    and Sally Shrapnel. 2019. Deep Neural Network or Dermatologist? In *Interpretability
    of Machine Intelligence in Medical Image Computing and Multimodal Learning for
    Clinical Decision Support*. 48–55.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Hang Yu, Laurence T Yang, Qingchen Zhang, David Armstrong,
    and M Jamal Deen. 2021. Convolutional Neural Networks for Medical Image Analysis.
    *Neurocomputing* 444 (2021), 92–110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuksekgonul et al. (2022) Mert Yuksekgonul, Maggie Wang, and James Zou. 2022.
    Post-hoc concept bottleneck models. *arXiv preprint arXiv:2205.15480* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler and Fergus (2014) Matthew D Zeiler and Rob Fergus. 2014. Visualizing
    and Understanding Convolutional Networks. In *Proceedings of the European Conference
    on Computer Vision (ECCV)*. 818–833.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Junsan Zhang, Xiuxuan Shen, Shaohua Wan, Sotirios K Goudos,
    Jie Wu, Ming Cheng, and Weishan Zhang. 2023. A Novel Deep Learning Model for Medical
    Report Generation by Inter-Intra Information Calibration. *IEEE Journal of Biomedical
    and Health Informatics* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, and
    Lin Yang. 2017. MDNet: A Semantically and Visually Interpretable Medical Image
    Diagnosis Network. In *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 6428–6436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. 2016. Learning Deep Features for Discriminative Localization.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 2921–2929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1\. Intepretability Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The increasing interest in the interpretability of machine learning fostered
    the development of frameworks implementing classical XAI methods. The LRP Toolbox ([70](#bib.bib70))
    was launched in 2016 and provides the implementation of the LRP ([10](#bib.bib10))
    algorithm for artificial neural networks supporting Matlab and Python. Additionally,
    the LRP toolbox released an extension to be compatible with the Caffe Deep Learning
    framework. DeepExplain ([7](#bib.bib7)) is a framework that implements perturbation
    and gradient-based attribution methods, including Saliency Maps ([133](#bib.bib133)),
    Gradient * Input ([131](#bib.bib131)), Integrated Gradients (IG) ([144](#bib.bib144)),
    DeepLIFT ([131](#bib.bib131)), $\epsilon$-LRP ([10](#bib.bib10)), and DeConvNet ([174](#bib.bib174)).
    It also supports Tensorflow as well as Keras with Tensorflow backend. Alternatively,
    iNNvestigate ([5](#bib.bib5)) is a more complete toolbox that provides implementations
    for SmoothGrad ([139](#bib.bib139)), DeConvNet ([174](#bib.bib174)), Guided-BackProp ([142](#bib.bib142)),
    PatternNet ([63](#bib.bib63)), Input * Gradients ([131](#bib.bib131)), DeepTaylor ([98](#bib.bib98)),
    PatternAttribution ([63](#bib.bib63)), LRP ([10](#bib.bib10)) and Integrated Gradients
    (IG) ([144](#bib.bib144)). It also supports Tensorflow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to PyTorch frameworks, TorchRay ([37](#bib.bib37)) implements several
    visualization methods, namely Gradient ([133](#bib.bib133)), Guided-BackProp ([142](#bib.bib142)),
    Grad-CAM ([128](#bib.bib128)) and RISE ([108](#bib.bib108)). TorchRay is considered
    research-oriented, since it provides code for reproducing results that appear
    in several papers. Captum ([65](#bib.bib65)) is a PyTorch library that provides
    state-of-the-art algorithms for model interpretability and understanding. It contains
    general purpose implementations of Integrated Gradients ([144](#bib.bib144)),
    SmoothGrad ([139](#bib.bib139)), VarGrad ([114](#bib.bib114)) and others for PyTorch
    models. Table [4](#A1.T4 "Table 4 ‣ A.1\. Intepretability Frameworks ‣ Appendix
    A Appendix ‣ Explainable Deep Learning Methods in Medical Image Classification:
    A Survey") summarizes the aforementioned frameworks alongside the supported XAI
    methods. Additionally, we refer the reader to the work of Darias et al. ([30](#bib.bib30))
    in which some other model-agnostic XAI libraries were approached.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Publicly available interpretability frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Year | Methods | Supported DL Libraries |'
  prefs: []
  type: TYPE_TB
- en: '| LRP Toolbox ([70](#bib.bib70)) | 2016 | LRP | Caffe |'
  prefs: []
  type: TYPE_TB
- en: '| DeepExplain ([7](#bib.bib7)) | 2017 | Saliency maps, Grad * Input, $\epsilon$-LRP,
    DeepLIFT, DeConvNet | Tensorflow, Keras |'
  prefs: []
  type: TYPE_TB
- en: '| iNNvestigate ([5](#bib.bib5)) | 2019 | SmoothGrad, DeConvNet, Guided-BackProp,
    PatternNet, LRP Input * Gradients, DeepTaylor, PatternAttribution, IG | Tensorflow,
    Keras |'
  prefs: []
  type: TYPE_TB
- en: '| TorchRay ([37](#bib.bib37)) | 2019 | Gradient, Guided-BackProp, Grad-CAM,
    RISE | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| Captum ([65](#bib.bib65)) | 2019 | SmoothGrad, DeConvNet, Guided-BackProp,
    PatternNet, LRP Input * Gradients, DeepLIFT, DeepTaylor, LIME, Kernel SHAP, IG
    | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: A.2\. Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 5\. Summary of the XAI methods categorized by interpretability method
    employed, image modality and dataset. The spaces marked with a ”-” mean that explanation
    is only provided through text sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Interpretability Method | Modality | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([176](#bib.bib176)) | 2017 | Attention-based | Microscopic
    Images | BCIDR |'
  prefs: []
  type: TYPE_TB
- en: '| Jing et al. ([55](#bib.bib55)) | 2017 | Attention-based | X-ray | IU Chest
    X-ray, PEIR Gross |'
  prefs: []
  type: TYPE_TB
- en: '| Rajpurkar et al. ([111](#bib.bib111)) | 2018 | CAM | X-ray | ChestX-ray8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([160](#bib.bib160)) | 2018 | Saliency maps | X-ray | IU Chest
    X-ray,ChestX-ray14 |'
  prefs: []
  type: TYPE_TB
- en: '| Gale et al. ([39](#bib.bib39)) | 2018 | SmoothGrad | X-ray | Pelvic X-ray
    |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([74](#bib.bib74)) | 2018 | Text-based | X-ray | IU Chest X-ray,
    CX-CHR (private) |'
  prefs: []
  type: TYPE_TB
- en: '| Malhi et al. ([92](#bib.bib92)) | 2019 | LIME | Endoscopy | Red Lesion Endoscopy
    |'
  prefs: []
  type: TYPE_TB
- en: '| Young et al. ([171](#bib.bib171)) | 2019 | KernelSHAP | Dermoscopy | HAM10000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Eitel et al. ([35](#bib.bib35)) | 2019 | Occlusion | MRI | ADNI Initiative
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sayres et al. ([124](#bib.bib124)) | 2019 | Integrated Gradients | Fundus
    Images | Private |'
  prefs: []
  type: TYPE_TB
- en: '| Barata et al. ([12](#bib.bib12)) | 2019 | CAM | Dermoscopy | ISIC 2017, ISIC
    2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Tschandl et al. ([148](#bib.bib148)) | 2019 | CBIR | Dermoscopy | EDRA, ISIC
    2017, Private Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et al. ([143](#bib.bib143)) | 2019 | Text-based | Mammography | Inbreast
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. ([72](#bib.bib72)) | 2019 | Saliency maps | Mammography | CBIS-DDSM
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lamy et al. ([69](#bib.bib69)) | 2019 | CBR | Mammography | BCW, Mammographic
    Mass, Breast Cancer |'
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([137](#bib.bib137)) | 2019 | Stacked LSTM | X-ray | IU Chest
    X-ray |'
  prefs: []
  type: TYPE_TB
- en: '| Yin et al. ([170](#bib.bib170)) | 2019 | t-SNE | X-ray | IU Chest X-ray |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([83](#bib.bib83)) | 2019 | Attention maps | X-ray | IU Chest
    X-ray, MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Windish et al. ([165](#bib.bib165)) | 2020 | Grad-CAM | MRI | IXI, Gliobastoma
    |'
  prefs: []
  type: TYPE_TB
- en: '| Magesh et al. ([90](#bib.bib90)) | 2020 | LIME | DaTscan | PPMI |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([77](#bib.bib77)) | 2020 | Guided Grad-CAM | X-ray | COVIDx |'
  prefs: []
  type: TYPE_TB
- en: '| Lopatina et al. ([84](#bib.bib84)) | 2020 | DeepLIFT | MRI | Private |'
  prefs: []
  type: TYPE_TB
- en: '| Graziani et al. ([44](#bib.bib44)) | 2020 | TCAV | Microscopic Images | Camelyon16,
    Camelyon17 |'
  prefs: []
  type: TYPE_TB
- en: '| Margeloiu et al. ([93](#bib.bib93)) | 2020 | Adversarial Training | Dermoscopy
    | HAM10000 |'
  prefs: []
  type: TYPE_TB
- en: '| Rio-Torto et al. ([115](#bib.bib115)) | 2020 | In-model | Microscopic Images
    | NIH-NCI Cervical Cancer |'
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. ([36](#bib.bib36)) | 2020 | Concept-based | Microscopic Images
    | Infectious Keratitis Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([25](#bib.bib25)) | 2020 | Multi-Head Attention | X-ray | IU
    Chest X-ray, MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Silva et al. ([132](#bib.bib132)) | 2020 | CBIR, Saliency Map | X-ray | CheXpert
    |'
  prefs: []
  type: TYPE_TB
- en: '| Punn et al. ([110](#bib.bib110)) | 2021 | LIME | X-ray | COVID-19 Dataset
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([158](#bib.bib158)) | 2021 | SHAP | Dermoscopy | HAM10000 |'
  prefs: []
  type: TYPE_TB
- en: '| Billah and Javed ([16](#bib.bib16)) | 2021 | BCNN | Microscopy Images | ALL-IDB
    |'
  prefs: []
  type: TYPE_TB
- en: '| Barata et al. ([13](#bib.bib13)) | 2021 | CBIR | Dermoscopy | ISIC 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Thiagarajan et al. ([145](#bib.bib145)) | 2021 | t-SNE | Microscopic Images
    | Breast Histopathology |'
  prefs: []
  type: TYPE_TB
- en: '| Barnett et al. ([14](#bib.bib14)) | 2021 | Prototype Activation Map | Mammography
    | Mammography Private Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. ([62](#bib.bib62)) | 2021 | Counterfactual Examples | X-ray |
    Chest X-ray 14, VinDr-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Schutte et al. ([126](#bib.bib126)) | 2021 | Grad-CAM | X-ray/Microscopy
    Images | Osteoarthritis X-ray, Camelyon16 |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. ([61](#bib.bib61)) | 2021 | Saliency maps | X-ray | NIH chest
    X-ray14 |'
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([135](#bib.bib135)) | 2021 | Prototype Activation Maps | X-ray
    | CORD-19 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al.  ([81](#bib.bib81)) | 2021 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([82](#bib.bib82)) | 2021 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Lucieri et al. ([85](#bib.bib85)) | 2022 | TCAV | Dermoscopy | ISIC 2019,
    Derm7pt, PH2, SkinL2 |'
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. ([53](#bib.bib53)) | 2022 | CBIR | X-ray/Dermoscopy | COVIDx, ISIC
    2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Gour and Jain ([43](#bib.bib43)) | 2022 | Uncertainty-based | X-ray | COVID19CXr
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yuksekgonul et al. ([173](#bib.bib173)) | 2022 | CBM | Dermoscopy | HAM 10000,
    ISIC 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([162](#bib.bib162)) | 2022 | Text-based | X-ray | MIMIC-CXR
    |'
  prefs: []
  type: TYPE_TB
- en: '| Singla et al. ([138](#bib.bib138)) | 2023 | Counterfactual Examples | X-ray
    | MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Yan et al. ([168](#bib.bib168)) | 2023 | CBM | Dermoscopy | ISIC 2016-2020
    |'
  prefs: []
  type: TYPE_TB
- en: '| Selivanov et al. ([127](#bib.bib127)) | 2023 | Text-based | X-ray | IU Chest
    X-ray, MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([175](#bib.bib175)) | 2023 | Text-based | X-ray | IU Chest
    X-ray, MIMIC-CXR, COV-CTR |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([161](#bib.bib161)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([23](#bib.bib23)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([169](#bib.bib169)) | 2023 | Text-based | X-ray | IU Chest X-ray,
    MIMIC-CXR |'
  prefs: []
  type: TYPE_TB
- en: '| Patrício et al. ([106](#bib.bib106)) | 2023 | CBM | Dermoscopy | PH2, Derm7pt
    |'
  prefs: []
  type: TYPE_TB
