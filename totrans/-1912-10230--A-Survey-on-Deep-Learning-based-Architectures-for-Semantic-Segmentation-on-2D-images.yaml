- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:03:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.10230] A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.10230] 关于基于深度学习的二维图像语义分割架构的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10230](https://ar5iv.labs.arxiv.org/html/1912.10230)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10230](https://ar5iv.labs.arxiv.org/html/1912.10230)
- en: A Survey on Deep Learning-based Architectures
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的架构调查
- en: for Semantic Segmentation on 2D images
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关于二维图像的语义分割
- en: Irem Ulku
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Irem Ulku
- en: Department of Computer Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机工程系
- en: Ankara University,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 安卡拉大学，
- en: Ankara, TURKEY
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 安卡拉，土耳其
- en: irem.ulku@ankara.edu.tr
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: irem.ulku@ankara.edu.tr
- en: ORCID-ID:0000-0003-4998-607X \AndErdem Akagündüz
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ORCID-ID:0000-0003-4998-607X \AndErdem Akagündüz
- en: Graduate School of Informatics
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 信息学研究生院
- en: Middle East Technical University,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 中东技术大学，
- en: Ankara, TURKEY
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 安卡拉，土耳其
- en: akaerdem@metu.edu.tr
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: akaerdem@metu.edu.tr
- en: ORCID-ID:0000-0002-0792-7306
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ORCID-ID:0000-0002-0792-7306
- en: '*corresponding author*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*通讯作者*'
- en: Abstract
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Semantic segmentation is the pixel-wise labelling of an image. Boosted by the
    extraordinary ability of convolutional neural networks (CNN) in creating semantic,
    high level and hierarchical image features; several deep learning-based 2D semantic
    segmentation approaches have been proposed within the last decade. In this survey,
    we mainly focus on the recent scientific developments in semantic segmentation,
    specifically on deep learning-based methods using 2D images. We started with an
    analysis of the public image sets and leaderboards for 2D semantic segmentation,
    with an overview of the techniques employed in performance evaluation. In examining
    the evolution of the field, we chronologically categorised the approaches into
    three main periods, namely pre-and early deep learning era, the fully convolutional
    era, and the post-FCN era. We technically analysed the solutions put forward in
    terms of solving the fundamental problems of the field, such as fine-grained localisation
    and scale invariance. Before drawing our conclusions, we present a table of methods
    from all mentioned eras, with a summary of each approach that explains their contribution
    to the field. We conclude the survey by discussing the current challenges of the
    field and to what extent they have been solved.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割是对图像进行逐像素标注。借助卷积神经网络（CNN）在生成语义、高级和层次化图像特征方面的非凡能力，过去十年中提出了几种基于深度学习的二维语义分割方法。在这项调查中，我们主要关注语义分割的最新科学进展，特别是基于深度学习的二维图像方法。我们从分析公共图像集和二维语义分割的排行榜开始，并概述了用于性能评估的技术。在审视该领域的发展时，我们将方法按时间顺序分为三个主要阶段，即深度学习早期阶段、完全卷积阶段和FCN后阶段。我们在技术上分析了提出的解决方案，涉及该领域的基本问题，如细粒度定位和尺度不变性。在得出结论之前，我们提供了所有提到的阶段的方法表，并总结了每种方法对该领域的贡献。我们通过讨论该领域当前的挑战及其解决程度来结束调查。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Semantic segmentation has recently become one of the fundamental problems, and
    accordingly, a hot topic for the fields of computer vision and machine learning.
    Assigning a separate class label to each pixel of an image is one of the important
    steps in building complex robotic systems such as driverless cars/drones, human-friendly
    robots, robot-assisted surgery, and intelligent military systems. Thus, it is
    no wonder that in addition to scientific institutions, industry-leading companies
    studying artificial intelligence are now summarily confronting this problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割最近已成为基础性问题之一，因此也是计算机视觉和机器学习领域的热门话题。为图像中的每一个像素分配一个单独的类别标签是构建复杂机器人系统（如无人驾驶汽车/无人机、人性化机器人、机器人辅助手术和智能军事系统）中的一个重要步骤。因此，除了科学机构外，研究人工智能的行业领先公司现在也在积极应对这个问题。
- en: The simplest problem definition for semantic segmentation is pixel-wise labelling.
    Because the problem is defined at the pixel level, finding only class labels that
    the scene includes is considered insufficient, but localising labels at the original
    image pixel resolution is also a fundamental goal. Depending on the context, class
    labels may change. For example, in a driverless car, the pixel labels may be *human,
    road* and *car* (Siam et al. [2017](#bib.bib150)) whereas for a medical system
    (Saha and Chakraborty [2018](#bib.bib141); Jiang et al. [2017](#bib.bib62)), they
    could be *cancer cells, muscle tissue, aorta wall* etc.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割最简单的问题定义是像素级标注。由于问题在像素级别上定义，因此仅找到场景中包含的类别标签被认为是不够的，但在原始图像像素分辨率下定位标签也是一个基本目标。根据上下文，类别标签可能会改变。例如，在无人驾驶汽车中，像素标签可能是*人、道路*和*汽车*（Siam等
    [2017](#bib.bib150)），而对于医学系统（Saha和Chakraborty [2018](#bib.bib141); Jiang等 [2017](#bib.bib62)），它们可能是*癌细胞、肌肉组织、主动脉壁*等。
- en: The recent increase in interest in this topic has been undeniably caused by
    the extraordinary success seen with convolutional neural networks (LeCun et al.
    [1989](#bib.bib82)) (CNN) that have been brought to semantic segmentation. Understanding
    a scene at the semantic level has long been one of the main topics of computer
    vision, but it is only now that we have seen actual solutions to the problem.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对这一主题的兴趣最近显著增加，毫无疑问是由于卷积神经网络（LeCun等 [1989](#bib.bib82)）(CNN)在语义分割中的卓越成功。理解场景的语义层面长期以来一直是计算机视觉的主要课题之一，但直到现在我们才看到问题的实际解决方案。
- en: In this paper, our primary motivation is to focus on the recent scientific developments
    in semantic segmentation, specifically on the evolution of deep learning-based
    methods using 2D images. The reason we narrowed down our survey to techniques
    that utilise only 2D visible imagery is that, in our opinion, the scale of the
    problem in the literature is so vast and widespread that it would be impractical
    to analyse and categorise all semantic segmentation modalities (such as 3D point
    clouds, hyper-spectral data, MRI, CT¹¹1We consider MRI and CT essentially as 3D
    volume data. Although individual MRI/CT slices are 2D, when doing semantic segmentation
    on these types of data, neighbourhood information in all three dimensions are
    utilised. For this reason, medical applications are excluded from this survey.
    etc.) found in journal articles to any degree of detail. In addition to analysing
    the techniques which make semantic segmentation possible and accurate, we also
    examine the most popular image sets created for this problem. Additionally, we
    review the performance measures used for evaluating the success of semantic segmentation.
    Most importantly, we propose a taxonomy of methods, together with a technical
    evolution of them, which we believe is novel in the sense that it provides insight
    to the existing deficiencies and suggests future directions for the field.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的主要动机是关注语义分割领域最近的科学进展，特别是基于深度学习方法在2D图像上的演变。我们将调查范围缩小到仅使用2D可见图像的技术，是因为在我们看来，文献中的问题规模如此庞大和广泛，以至于分析和分类所有语义分割模态（如3D点云、高光谱数据、MRI、CT¹¹1我们将MRI和CT视为3D体积数据。虽然单个MRI/CT切片是2D的，但在对这些类型的数据进行语义分割时，会利用所有三个维度的邻域信息。因此，医学应用被排除在此调查之外。等）是不切实际的。除了分析使语义分割成为可能和准确的技术外，我们还检查了为这个问题创建的最受欢迎的图像集。此外，我们还审查了用于评估语义分割成功的性能度量。最重要的是，我们提出了一种方法分类法，以及其技术演变，我们认为这在提供对现有缺陷的洞察并建议该领域未来方向的意义上是新的。
- en: 'The remainder of the paper is organised as follows: in the following subsection,
    we refer to other survey studies on the subject and underline our contribution.
    Section 2 presents information about the different image sets, the challenges,
    and how to measure the performance of semantic segmentation. Starting with Section 3,
    we chronologically scrutinise semantic segmentation methods under three main titles,
    hence in three separate sections. Section 3 covers the methods of pre- and early
    deep convolutional neural networks era. Section 4 provides details on the fully
    convolutional neural networks, which we consider a milestone for the semantic
    segmentation literature. Section 5 covers the state-of-the-art methods on the
    problem and provides details on both the architectural details and the success
    of these methods. Before finally concluding the paper in Section 7, Section 6
    provides a future scope and potential directions for the field.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下：在接下来的小节中，我们提到其他相关的调查研究，并强调我们的贡献。第2节介绍了不同图像集的信息、挑战以及如何衡量语义分割的性能。从第3节开始，我们按照三个主要标题对语义分割方法进行按时间顺序的详细审查，因此分为三个独立的部分。第3节涵盖了早期深度卷积神经网络时代的方法。第4节提供了关于全卷积神经网络的详细信息，我们认为这是语义分割文献的一个里程碑。第5节介绍了问题的最新方法，并提供了这些方法的架构细节及其成功情况。在第7节最终总结论文之前，第6节提供了未来的展望和领域的潜在方向。
- en: 1.1 Surveys on Semantic Segmentation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 语义分割的调查
- en: Very recently, driven by both academia and industry, the rapid increase of interest
    in semantic segmentation has inevitably led to a number of survey studies being
    published (Thoma [2016](#bib.bib156); Ahmad et al. [2017](#bib.bib1); Jiang et al.
    [2017](#bib.bib62); Siam et al. [2017](#bib.bib150); Garcia-Garcia et al. [2017](#bib.bib40);
    Saffar et al. [2018](#bib.bib140); Yu et al. [2018](#bib.bib191); Guo et al. [2018](#bib.bib45);
    Lateef and Ruichek [2019](#bib.bib80); Minaee et al. [2020](#bib.bib108)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于学术界和工业界的推动，对语义分割的兴趣迅速增加，导致大量调查研究的发表（Thoma [2016](#bib.bib156); Ahmad et
    al. [2017](#bib.bib1); Jiang et al. [2017](#bib.bib62); Siam et al. [2017](#bib.bib150);
    Garcia-Garcia et al. [2017](#bib.bib40); Saffar et al. [2018](#bib.bib140); Yu
    et al. [2018](#bib.bib191); Guo et al. [2018](#bib.bib45); Lateef and Ruichek
    [2019](#bib.bib80); Minaee et al. [2020](#bib.bib108)).
- en: Some of these surveys focus on a specific problem, such as comparing semantic
    segmentation approaches for horizon/skyline detection (Ahmad et al. [2017](#bib.bib1)),
    whilst others deal with relatively broader problems related to industrial challenges,
    such as semantic segmentation for driverless cars (Siam et al. [2017](#bib.bib150))
    or medical systems (Jiang et al. [2017](#bib.bib62)). These studies are useful
    if working on the same specific problem, but they lack an overarching vision that
    may ‘technically’ contribute to the future directions of the field.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些调查专注于特定问题，如比较用于地平线/天际线检测的语义分割方法（Ahmad et al. [2017](#bib.bib1)），而其他调查涉及与工业挑战相关的较广泛问题，如无人驾驶汽车（Siam
    et al. [2017](#bib.bib150)）或医疗系统（Jiang et al. [2017](#bib.bib62)）的语义分割。这些研究对于处理相同的特定问题是有用的，但缺乏可能“从技术上”贡献于该领域未来方向的总体视角。
- en: Another group (Thoma [2016](#bib.bib156); Saffar et al. [2018](#bib.bib140);
    Yu et al. [2018](#bib.bib191); Guo et al. [2018](#bib.bib45)) of survey studies
    on semantic segmentation have provided a general overview of the subject, but
    they lack the necessary depth of analysis regarding deep learning-based methods.
    Whilst semantic segmentation was studied for two decades prior to deep learning,
    actual contribution to the field has only been achieved very recently, particularly
    following a revolutionary paper on fully convolutional networks (FCN) (Shelhamer
    et al. [2017](#bib.bib143)) (which has also been thoroughly analysed in this paper).
    It could be said that most state-of-the-art studies are in fact extensions of
    that same (Shelhamer et al. [2017](#bib.bib143)) study. For this reason, without
    scrupulous analysis of FCNs and the direction of the subsequent papers, survey
    studies will lack the necessary academic rigour in examining semantic segmentation
    using deep learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组(Thoma [2016](#bib.bib156); Saffar 等 [2018](#bib.bib140); Yu 等 [2018](#bib.bib191);
    Guo 等 [2018](#bib.bib45))的语义分割综述研究提供了该主题的一般概述，但在深度学习方法的分析深度方面存在不足。尽管在深度学习之前，语义分割已经研究了二十年，但真正对该领域的贡献直到最近才取得，特别是在一篇关于全卷积网络(FCN)的革命性论文(Shelhamer
    等 [2017](#bib.bib143))之后(该论文也在本文中进行了详细分析)。可以说，大多数最先进的研究实际上是对那项(Shelhamer 等 [2017](#bib.bib143))研究的扩展。因此，如果不对FCNs和后续论文的方向进行详细分析，综述研究将缺乏必要的学术严谨性。
- en: There are recent reviews of deep semantic segmentation, for example by (Garcia-Garcia
    et al. [2017](#bib.bib40)) and (Minaee et al. [2020](#bib.bib108)), which provide
    a comprehensive survey on the subject. These survey studies cover almost all the
    popular semantic segmentation image sets and methods, and for all modalities such
    as 2D, RGB, 2.5D, RGB-D, and 3D data. Although they are inclusive in the sense
    that most related material on deep semantic segmentation is included, the categorisation
    of the methods is coarse, since the surveys attempt to cover almost everything
    umbrellaed under the topic of semantic segmentation literature.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 近期有关于深度语义分割的综述，例如(Garcia-Garcia 等 [2017](#bib.bib40)) 和(Minaee 等 [2020](#bib.bib108))，提供了该主题的全面调查。这些调查研究涵盖了几乎所有流行的语义分割图像集和方法，以及所有模态如2D、RGB、2.5D、RGB-D和3D数据。尽管它们在包含大部分相关深度语义分割材料方面具有包容性，但方法的分类较为粗略，因为这些调查试图覆盖几乎所有与语义分割文献相关的内容。
- en: A detailed categorisation of the subject was provided in (Lateef and Ruichek
    [2019](#bib.bib80)). Although this survey provides important details on the subcategories
    that cover almost all approaches in the field, discussions on how the proposed
    techniques are chronologically correlated are left out of their scope. Recent
    deep learning studies on semantic segmentation follow a number of fundamental
    directions and labour with tackling the varied corresponding issues. In this survey
    paper, we define and describe these new challenges, and present the chronological
    evolution of the techniques of all the studies within this proposed context. We
    believe our attempt to understand the evolution of semantic segmentation architectures
    is the main contribution of the paper. We provide a table of these related methods,
    and explain them briefly one after another in chronological order, with their
    metric performance and computational efficiency. This way, we believe that readers
    will better understand the evolution, current state-of-the-art, as well as the
    future directions seen for 2D semantic segmentation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Lateef 和 Ruichek [2019](#bib.bib80))中提供了对该主题的详细分类。尽管这项调查提供了涵盖该领域几乎所有方法的子类别的重要细节，但如何按时间顺序关联这些提出的技术的讨论超出了他们的范围。最近的深度学习研究在语义分割方面遵循了一些基本方向，并致力于解决各种相关问题。在这项调查论文中，我们定义并描述了这些新的挑战，并呈现了所有研究技术在该提出的背景下的时间演变。我们认为，理解语义分割架构的演变是本文的主要贡献。我们提供了这些相关方法的表格，并按时间顺序简要解释了它们，包括它们的度量性能和计算效率。通过这种方式，我们相信读者将更好地理解2D语义分割的演变、当前的最先进技术以及未来的方向。
- en: 2 Image Sets, Challenges and Performance Evaluation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 图像集、挑战与性能评估
- en: 2.1 Image Sets and Challenges
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 图像集和挑战
- en: The level of success for any machine-learning application is undoubtedly determined
    by the quality and the depth of the data being used for training. When it comes
    to deep learning, data is even more important since most systems are termed end-to-end;
    thus, even the features are determined *by* the data, not *for* the data. Therefore,
    data is no longer the object but becomes the actual subject in the case of deep
    learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习应用的成功水平无疑取决于用于训练的数据的质量和深度。对于深度学习来说，数据更为重要，因为大多数系统被称为端到端的；因此，即使特征也是由数据决定的，而不是为数据决定的。因此，数据不再是对象，而是在深度学习中成为实际的主题。
- en: In this section, we scrutinise the most popular large-scale 2D image sets that
    have been utilised for the semantic segmentation problem. The image sets were
    categorised into two main branches, namely general-purpose image sets, with generic
    class labels including almost every type of object or background, and also urban
    street image sets, which include class labels such as car and person, and are
    generally created for the training of driverless car systems. There are many other
    unresolved 2D semantic segmentation problem domains such as medical imaging, satellite
    imagery, or infrared imagery. However, urban street image is currently driving
    scientific development in the field because they attract more attention from the
    industry. Therefore, very large-scale image sets and challenges with crowded leaderboards
    exist, yet, only specifically for industrial users. Scientific interest for depth-based
    semantic segmentation is growing rapidly; however, as mentioned in the Introduction,
    we have excluded depth-based and 3D-based segmentation datasets from the current
    study in order to focus with sufficient detail on the novel categorisation of
    recent techniques pertinent to 2D semantic segmentation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细审视了用于语义分割问题的最受欢迎的大规模2D图像集。这些图像集被分为两个主要类别，即通用图像集，具有包括几乎所有类型的对象或背景的通用类别标签，以及城市街道图像集，包括诸如汽车和人的类别标签，通常用于无人驾驶汽车系统的训练。还有许多其他未解决的2D语义分割问题领域，如医学成像、卫星图像或红外成像。然而，城市街道图像目前正在推动这一领域的科学发展，因为它们吸引了更多来自工业界的关注。因此，存在非常大规模的图像集和拥挤的排行榜，但仅限于工业用户。基于深度的语义分割的科学兴趣正在迅速增长；然而，正如引言中提到的，我们在当前研究中排除了基于深度和3D的分割数据集，以便充分详细地关注于最近的2D语义分割技术的最新分类。
- en: 2.1.1 General Purpose Semantic Segmentation Image Sets
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 通用语义分割图像集
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PASCAL Visual Object Classes (VOC) (Everingham et al. [2010](#bib.bib32)):
    This image set includes image annotations not only for semantic segmentation,
    but for also classification, detection, action classification, and person layout
    tasks. The image set and annotations are regularly updated and the leaderboard
    of the challenge is public²²2[http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php](http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php)
    (with more than 140 submissions just for the segmentation challenge alone). It
    is the most popular among the semantic segmentation challenges and is still active
    following its initial release in 2005\. The PASCAL VOC semantic segmentation challenge
    image set includes 20 foreground object classes and one background class. The
    original data consisted of 1,464 images for the purposes of training, plus 1,449
    images for validation. The 1,456 test images are kept private for the challenge.
    The image set includes all types of indoor and outdoor images and is generic across
    all categories.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PASCAL视觉目标类别（VOC）（Everingham et al. [2010](#bib.bib32)）：此图像集不仅包括用于语义分割的图像注释，还包括分类、检测、动作分类和人物布局任务的注释。该图像集和注释会定期更新，挑战的排行榜是公开的²²2[http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php](http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php)（仅分割挑战就有超过140个提交）。这是语义分割挑战中最受欢迎的，并且自2005年首次发布以来仍在积极进行。PASCAL
    VOC语义分割挑战图像集包括20个前景对象类别和一个背景类别。原始数据包括用于训练的1,464张图像和用于验证的1,449张图像。1,456张测试图像对挑战保持私密。该图像集包括所有类型的室内和室外图像，并在所有类别中具有通用性。
- en: The PASCAL VOC image set has a number of extension image sets, the most popular
    among these are PASCAL Context (Mottaghi et al. [2014](#bib.bib111)) and PASCAL
    Parts (Chen et al. [2014](#bib.bib20)). The first (Mottaghi et al. [2014](#bib.bib111))
    is a set of additional annotations for PASCAL VOC 2010, which goes beyond the
    original PASCAL semantic segmentation task by providing annotations for the whole
    scene. The statistics section contains a full list of more than 400 labels (compared
    to the original 21 labels). The second (Chen et al. [2014](#bib.bib20)) is also
    a set of additional annotations for PASCAL VOC 2010\. It provides segmentation
    masks for each body part of the object, such as the separately labelled limbs
    and body of an animal. For these extensions, the training and validation set contains
    10,103 images, while the test set contains 9,637 images. There are other extensions
    to PASCAL VOC using other functional annotations such as the Semantic Parts (PASParts)
    (Wang et al. [2015](#bib.bib168)) image set and the Semantic Boundaries Dataset
    (SBD) (Hariharan et al. [2011](#bib.bib47)). For example, PASParts (Wang et al.
    [2015](#bib.bib168)) additionally provides ‘instance’ labels such as two instances
    of an object within an image are labelled separately, rather than using a single
    class label. However, unlike the former two additional extensions (Chen et al.
    [2014](#bib.bib20); Mottaghi et al. [2014](#bib.bib111)), these further extensions
    (Wang et al. [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47)) have proven
    less popular as their challenges have attracted much less attention in state-of-the-art
    semantic segmentation studies; thus, their leaderboards are less crowded. In Figure
    [1](#S2.F1 "Figure 1 ‣ 1st item ‣ 2.1.1 General Purpose Semantic Segmentation
    Image Sets ‣ 2.1 Image Sets and Challenges ‣ 2 Image Sets, Challenges and Performance
    Evaluation ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images"), a sample object, parts and instance segmentation are depicted.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PASCAL VOC 图像集有若干扩展图像集，其中最受欢迎的是 PASCAL Context (Mottaghi et al. [2014](#bib.bib111))
    和 PASCAL Parts (Chen et al. [2014](#bib.bib20))。第一个 (Mottaghi et al. [2014](#bib.bib111))
    是 PASCAL VOC 2010 的附加注释集，超越了原始的 PASCAL 语义分割任务，通过提供整个场景的注释来扩展。统计部分包含了超过 400 个标签的完整列表（相比原始的
    21 个标签）。第二个 (Chen et al. [2014](#bib.bib20)) 也是 PASCAL VOC 2010 的附加注释集。它为每个物体的身体部位提供了分割掩码，例如动物的各个肢体和身体的单独标签。对于这些扩展，训练集和验证集包含
    10,103 张图像，而测试集包含 9,637 张图像。还有其他对 PASCAL VOC 的扩展，使用了其他功能性注释，例如语义部件（PASParts）(Wang
    et al. [2015](#bib.bib168)) 图像集和语义边界数据集（SBD）(Hariharan et al. [2011](#bib.bib47))。例如，PASParts
    (Wang et al. [2015](#bib.bib168)) 额外提供了“实例”标签，例如图像中的两个实例被单独标记，而不是使用单一的类别标签。然而，与前两个附加扩展
    (Chen et al. [2014](#bib.bib20); Mottaghi et al. [2014](#bib.bib111)) 不同，这些进一步的扩展
    (Wang et al. [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47)) 的受欢迎程度较低，因为它们的挑战在最先进的语义分割研究中吸引了较少的关注，因此它们的排行榜相对较冷清。在图
    [1](#S2.F1 "Figure 1 ‣ 1st item ‣ 2.1.1 General Purpose Semantic Segmentation
    Image Sets ‣ 2.1 Image Sets and Challenges ‣ 2 Image Sets, Challenges and Performance
    Evaluation ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images") 中，展示了一个样本物体、部件和实例分割。
- en: '![Refer to caption](img/9c385325d885e6f0171be926c3d973ec.png)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明文字](img/9c385325d885e6f0171be926c3d973ec.png)'
- en: 'Figure 1: A sample image and its annotation for object, instance and parts
    segmentations separately, from left to right.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 1: 从左到右分别展示了物体、实例和部件分割的样本图像及其注释。'
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Common Objects in Context (COCO) (Lin et al. [2014](#bib.bib97)): With 200K
    labelled images, 1.5 million object instances, and 80 object categories, COCO
    is a very large scale object detection, semantic segmentation, and captioning
    image set, including almost every possible types of scene. COCO provides challenges
    not only at the instance-level and pixel-level (which they refer to as *stuff*)
    semantic segmentation, but also introduces a novel task, namely that of *panoptic*
    segmentation (Kirillov et al. [2018](#bib.bib71)), which aims at unifying instance-level
    and pixel-level segmentation tasks. Their leaderboards³³3[http://cocodataset.org](http://cocodataset.org)
    are relatively less crowded because of the scale of the data. On the other hand,
    for the same reason, their challenges are assessed only by the most ambitious
    scientific and industrial groups, and thus are considered as the state-of-the-art
    in their leaderboards. Due to its extensive volume, most studies partially use
    this image set to pre-train or fine-tune their model, before submitting to other
    challenges such as PASCAL VOC 2012.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Common Objects in Context (COCO) (Lin et al. [2014](#bib.bib97))：COCO 是一个具有
    200K 标记图像、150 万个物体实例和 80 个物体类别的大规模物体检测、语义分割和描述图像数据集，涵盖了几乎所有可能的场景类型。COCO 在实例级和像素级（他们称之为*stuff*）语义分割上提供了挑战，还引入了一项新任务，即*全景*分割
    (Kirillov et al. [2018](#bib.bib71))，旨在统一实例级和像素级分割任务。他们的排行榜³³3[http://cocodataset.org](http://cocodataset.org)由于数据规模相对较少，因此较为冷清。另一方面，由于同样的原因，其挑战仅由最雄心勃勃的科学和工业团体评估，因此被认为是他们排行榜上的*最先进*技术。由于其庞大的体量，大多数研究在提交到其他挑战（如
    PASCAL VOC 2012）之前，通常会部分使用此数据集来预训练或微调他们的模型。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ADE20K dataset (Zhou et al. [2019](#bib.bib210)): ADE20K contains more than
    20K scene-centric images with objects and object parts annotations. Similarly
    to PASCAL VOC, there is a public leaderboard⁴⁴4[http://sceneparsing.csail.mit.edu/](http://sceneparsing.csail.mit.edu/)
    and the benchmark is divided into 20K images for training, 2K images for validation,
    and another batch of held-out images for testing. The samples in the dataset have
    varying resolutions (average image size being 1.3M pixels), which can be up to
    2400$\times$1800 pixels. There are total of150 semantic categories included for
    evaluation.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ADE20K 数据集 (Zhou et al. [2019](#bib.bib210))：ADE20K 包含超过 20K 场景中心的图像，并带有物体和物体部件的标注。类似于
    PASCAL VOC，存在一个公开排行榜⁴⁴4[http://sceneparsing.csail.mit.edu/](http://sceneparsing.csail.mit.edu/)，基准被分为
    20K 图像用于训练、2K 图像用于验证，以及另一批用于测试的保留图像。数据集中样本的分辨率各异（平均图像大小为 1.3M 像素），最高可达 2400$\times$1800
    像素。评估包括总共 150 个语义类别。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Other General Purpose Semantic Segmentation Image Sets: Although less popular
    than either PASCAL VOC or COCO, there are also some other image sets in the same
    domain. Introduced by (Prest et al. [2012](#bib.bib128)), YouTube-Objects is a
    set of low-resolution (480$\times$360) video clips with more than 10k pixel-wise
    annotated frames.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他通用语义分割图像数据集：虽然不如 PASCAL VOC 或 COCO 流行，但在相同领域还有一些其他图像数据集。由 (Prest et al. [2012](#bib.bib128))
    介绍的 YouTube-Objects 是一个低分辨率 (480$\times$360) 视频片段的数据集，包含超过 10k 像素级标注的帧。
- en: Similarly, SIFT-flow (Tighe and Lazebnik [2010](#bib.bib158)) is another low-resolution
    (256$\times$256) semantic segmentation image set with 33 class labels for a total
    of 2,688 images. These and other relatively primitive image sets have been mostly
    abandoned in the semantic segmentation literature due to their limited resolution
    and low volume.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，SIFT-flow (Tighe and Lazebnik [2010](#bib.bib158)) 是另一个低分辨率 (256$\times$256)
    的语义分割图像数据集，具有 33 个类别标签，共 2,688 张图像。由于其有限的分辨率和较小的体量，这些以及其他相对原始的图像数据集在语义分割文献中大多已被淘汰。
- en: 2.1.2 Urban Street Semantic Segmentation Image Sets
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 城市街道语义分割图像数据集
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cityscapes (Cordts et al. [2016](#bib.bib27)): This is a largescale image set
    with a focus on the semantic understanding of urban street scenes. It contains
    annotations for high-resolution images from 50 different cities, taken at different
    hours of the day and from all seasons of the year, and also with varying backgrounds
    and scene layouts. The annotations are carried out at two quality levels: fine
    for 5,000 images and course for 20,000 images. There are 30 different class labels,
    some of which also have instance annotations (vehicles, people, riders etc.).
    Consequently, there are two challenges with separate public leaderboards⁵⁵5[https://www.cityscapes-dataset.com/benchmarks/](https://www.cityscapes-dataset.com/benchmarks/):
    one for pixel-level semantic segmentation, and a second for instance-level semantic
    segmentation. There are more than 100 entries to the challenge, making it the
    most popular regarding semantic segmentation of urban street scenes.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Cityscapes（Cordts et al. [2016](#bib.bib27)）：这是一个大规模的图像集，专注于城市街道场景的语义理解。它包含来自
    50 个不同城市的高分辨率图像的标注，这些图像在一天中的不同时间和一年中的所有季节拍摄，并且具有不同的背景和场景布局。标注分为两个质量等级：对 5,000
    张图像进行细致标注，对 20,000 张图像进行粗略标注。共有 30 个不同的类别标签，其中一些还具有实例标注（车辆、行人、骑车者等）。因此，存在两个具有独立公共排行榜的挑战⁵⁵5[https://www.cityscapes-dataset.com/benchmarks/](https://www.cityscapes-dataset.com/benchmarks/):
    一个是像素级语义分割的挑战，另一个是实例级语义分割的挑战。该挑战有超过 100 个参赛作品，使其在城市街道场景的语义分割方面最受欢迎。'
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Other Urban Street Semantic Segmentation Image Sets: There are a number of
    alternative image sets for urban street semantic segmentation, such as Cam-Vid
    (Brostow et al. [2009](#bib.bib10)), KITTI (Geiger et al. [2013](#bib.bib41)),
    SYNTHIA (Ros et al. [2016a](#bib.bib135)), and IDD (Varma et al. [2018](#bib.bib161)).
    These are generally overshadowed by the Cityscapes image set (Cordts et al. [2016](#bib.bib27))
    for several reasons. Principally, their scale is relatively low. Only the SYNTHIA
    image set (Ros et al. [2016a](#bib.bib135)) can be considered as largescale (with
    more than 13k annotated images); however, it is an artificially generated image
    set, and this is considered a major limitation for security-critical systems like
    driverless cars.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他城市街道语义分割图像集：有许多替代的城市街道语义分割图像集，例如 Cam-Vid（Brostow et al. [2009](#bib.bib10)）、KITTI（Geiger
    et al. [2013](#bib.bib41)）、SYNTHIA（Ros et al. [2016a](#bib.bib135)）和 IDD（Varma
    et al. [2018](#bib.bib161)）。由于几个原因，这些图像集通常被 Cityscapes 图像集（Cordts et al. [2016](#bib.bib27)）所掩盖。主要原因在于它们的规模相对较小。只有
    SYNTHIA 图像集（Ros et al. [2016a](#bib.bib135)）可以被认为是大规模的（包含超过 13k 的标注图像）；然而，它是一个人工生成的图像集，这被认为是像无人驾驶汽车这样对安全性要求极高的系统的一个主要限制。
- en: 2.1.3 Small-scale and Imbalanced Image Sets
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 小规模和不平衡的图像集
- en: In addition to the aforementioned large-scale image sets of different categories,
    there are several image sets with insufficient scale or strong imbalance such
    that, when applied to deep learning-based semantic segmentation models, high-level
    segmentation accuracies cannot be directly obtained. Most public challenges on
    semantic segmentation include sets of this nature such as the DSTL or RIT-18 (DSTLab.
    [2016](#bib.bib30); Kemker et al. [2018](#bib.bib68)), just to name a few. Because
    of the overwhelming numbers of these types of sets, we chose to include only the
    details of the large-scale sets that attract the utmost attention from the field.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述不同类别的大规模图像集，还有一些图像集规模不足或严重不平衡，因此，当应用于基于深度学习的语义分割模型时，无法直接获得高水平的分割精度。大多数关于语义分割的公共挑战包括这种性质的图像集，如
    DSTL 或 RIT-18（DSTLab. [2016](#bib.bib30)；Kemker et al. [2018](#bib.bib68)），仅举几个例子。由于这些图像集数量庞大，我们选择只包含那些在领域中引起极大关注的大规模图像集的详细信息。
- en: Nonetheless, being able to train a model that performs well on small-scale or
    imbalanced data is a correlated problem to ours. Besides conventional deep learning
    techniques such as transfer learning or data augmentation; the problem of insufficient
    or imbalanced data can be attacked by using specially designed deep learning architectures
    such as some optimized convolution layer types (Chen et al. ([2018](#bib.bib17));
    He et al. ([2015](#bib.bib49)), etc.) and others that we cover in this survey
    paper. What is more, there are recent studies that focus on the specific problem
    of utilizing insufficient sets for the problem of deep learning-based semantic
    segmentation (Xia et al. [2019](#bib.bib175)). Although we acknowledge this problem
    as fundamental for the semantic segmentation field, we leave the discussions on
    techniques to handle small-scale or imbalanced sets for semantic segmentation,
    beyond the scope of this survey paper.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，能够训练出在小规模或不平衡数据上表现良好的模型与我们的相关问题有关。除了传统的深度学习技术，如迁移学习或数据增强之外；对于不足或不平衡数据的问题，还可以通过使用专门设计的深度学习架构来解决，例如一些优化过的卷积层类型（Chen
    et al. ([2018](#bib.bib17)); He et al. ([2015](#bib.bib49))等）和我们在这篇综述论文中讨论的其他方法。此外，还有最近的研究专注于利用不足数据集解决基于深度学习的语义分割问题（Xia
    et al. [2019](#bib.bib175)）。虽然我们承认这一问题对语义分割领域至关重要，但讨论处理小规模或不平衡数据集的技术超出了这篇综述论文的范围。
- en: 2.2 Performance Evaluation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 性能评估
- en: 'There are two main criteria in evaluating the performance of semantics segmentation:
    accuracy, or in other words, the success of an algorithm; and computation complexity
    in terms of speed and memory requirements. In this section, we analyse these two
    criteria separately.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 评估语义分割性能的主要标准有两个：准确性，也就是算法的成功程度；以及计算复杂性，包括速度和内存要求。在本节中，我们将分别分析这两个标准。
- en: 2.2.1 Accuracy
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 准确性
- en: Measuring the performance of segmentation can be complicated, mainly because
    there are two distinct values to measure. The first is classification, which is
    simply determining the pixel-wise class labels; and the second is localisation,
    or finding the correct set of pixels that enclose the object. Different metrics
    can be found in the literature to measure one or both of these values. The following
    is a brief explanation of the principal measures most commonly used in evaluating
    semantic segmentation performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 测量分割性能可能比较复杂，主要因为需要测量两个不同的值。第一个是分类，即简单地确定逐像素的类别标签；第二个是定位，或者说是找到包围对象的正确像素集。文献中可以找到不同的度量标准来测量这一个或两个值。以下是对评估语义分割性能时最常用的主要度量指标的简要解释。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ROC-AUC*: ROC stands for the Receiver-Operator Characteristic curve, which
    summarises the trade-off between true positive rate and false-positive rate for
    a predictive model using different probability thresholds; whereas AUC stands
    for the area under this curve, which is 1 at maximum. This tool is useful in interpreting
    binary classification problems and is appropriate when observations are balanced
    between classes. However, since most semantic segmentation image sets (Everingham
    et al. [2010](#bib.bib32); Mottaghi et al. [2014](#bib.bib111); Chen et al. [2014](#bib.bib20);
    Wang et al. [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47); Lin et al.
    [2014](#bib.bib97); Cordts et al. [2016](#bib.bib27)) are not balanced between
    the classes, this metric is no longer used by the most popular challenges.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ROC-AUC*：ROC代表接收操作特征曲线，它总结了预测模型在不同概率阈值下的真正率和假正率之间的权衡；而AUC代表曲线下的面积，最大值为1。这个工具对于解释二分类问题非常有用，并且当观测在各类之间平衡时适用。然而，由于大多数语义分割图像集（Everingham
    et al. [2010](#bib.bib32); Mottaghi et al. [2014](#bib.bib111); Chen et al. [2014](#bib.bib20);
    Wang et al. [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47); Lin et al.
    [2014](#bib.bib97); Cordts et al. [2016](#bib.bib27)）在各类之间并不平衡，因此这个度量标准不再被最流行的挑战所使用。'
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Pixel Accuracy*: Also known as *global accuracy* (Badrinarayanan et al. [2015](#bib.bib6)),
    pixel accuracy (PA) is a very simple metric which calculates the ratio between
    the amount of properly classified pixels and their total number. Mean pixel accuracy
    (mPA), is a version of this metric which computes the ratio of correct pixels
    on a per-class basis. mPA is also referred to as *class average accuracy* (Badrinarayanan
    et al. [2015](#bib.bib6)).'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*像素准确率*：也称为 *全局准确率* (Badrinarayanan et al. [2015](#bib.bib6))，像素准确率（PA）是一个非常简单的指标，它计算了正确分类的像素数量与总像素数量之间的比率。均值像素准确率（mPA）是该指标的一个版本，它计算了每类的正确像素比率。mPA
    也被称为 *类别平均准确率* (Badrinarayanan et al. [2015](#bib.bib6))。'
- en: '|  | $PA=\frac{\sum_{j=1}^{k}{n_{jj}}}{\sum_{j=1}^{k}{t_{j}}},\qquad mPA=\frac{1}{k}\sum_{j=1}^{k}\frac{n_{jj}}{t_{j}}$
    |  | (1) |'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $PA=\frac{\sum_{j=1}^{k}{n_{jj}}}{\sum_{j=1}^{k}{t_{j}}},\qquad mPA=\frac{1}{k}\sum_{j=1}^{k}\frac{n_{jj}}{t_{j}}$
    |  | (1) |'
- en: where $n_{jj}$ is the total number of pixels both classified and labelled as
    class *j*. In other words, $n_{jj}$ corresponds to the total number of True Positives
    for class *j*. $t_{j}$ is the total number of pixels labelled as class *j*.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $n_{jj}$ 是同时被分类和标记为类别 *j* 的像素总数。换句话说，$n_{jj}$ 对应于类别 *j* 的真实阳性总数。$t_{j}$ 是标记为类别
    *j* 的像素总数。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Intersection over Union* (IoU): Also known as the Jaccard Index, IoU is a
    statistic used for comparing the similarity and diversity of sample sets. In semantics
    segmentation, it is the ratio of the intersection of the pixel-wise classification
    results with the ground truth, to their union.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*交并比* (IoU)：也称为 Jaccard 指数，IoU 是一种用于比较样本集相似性和多样性的统计量。在语义分割中，它是像素级分类结果与真实值的交集与它们的并集之间的比率。'
- en: '|  | $IoU=\frac{\sum_{j=1}^{k}{n_{jj}}}{\sum_{j=1}^{k}({n_{ij}+n_{ji}+n_{jj}})},\qquad
    i\neq j$ |  | (2) |'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $IoU=\frac{\sum_{j=1}^{k}{n_{jj}}}{\sum_{j=1}^{k}({n_{ij}+n_{ji}+n_{jj}})},\qquad
    i\neq j$ |  | (2) |'
- en: where, $n_{ij}$ is the number of pixels which are labelled as class *i*, but
    classified as class *j*. In other words, they are False Positives (false alarms)
    for class *j*. Similarly, ${n_{ji}}$, the total number of pixels labelled as class
    *j*, but classified as class *i* are the False Negatives (misses) for class *j*.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$n_{ij}$ 是标记为类别 *i* 但被分类为类别 *j* 的像素数量。换句话说，它们是类别 *j* 的假阳性（误报）。类似地，${n_{ji}}$，标记为类别
    *j* 但被分类为类别 *i* 的像素总数是类别 *j* 的假阴性（遗漏）。
- en: 'Two extended versions of IoU are also widely in use:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IoU 的两个扩展版本也被广泛使用：
- en: '$\circ$ *Mean Intersection over Union* (mIoU): mIoU is the class-averaged IoU,
    as in ([3](#S2.E3 "In 3rd item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣
    2 Image Sets, Challenges and Performance Evaluation ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images")).'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\circ$ *均值交并比* (mIoU)：mIoU 是类别平均 IoU，如在 ([3](#S2.E3 "第 3 项 ‣ 2.2.1 准确性 ‣ 2.2
    性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 深度学习架构在二维图像语义分割中的应用调研")) 中所述。
- en: '|  | $mIoU=\frac{1}{k}\sum_{j=1}^{k}\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}},\qquad
    i\neq j$ |  | (3) |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $mIoU=\frac{1}{k}\sum_{j=1}^{k}\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}},\qquad
    i\neq j$ |  | (3) |'
- en: '$\circ$ *Frequency-weighted IoU* (FwIoU): This is an improved version of MIoU
    that weighs each class importance depending on appearance frequency by using $t_{j}$
    (the total number of pixels labelled as class *j*, as also defined in ([1](#S2.E1
    "In 2nd item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images"))). The formula of FwIoU is given in ([4](#S2.E4
    "In 3rd item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images")):'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\circ$ *频率加权 IoU* (FwIoU)：这是 MIoU 的改进版本，通过使用 $t_{j}$（被标记为类别 *j* 的像素总数，如在 ([1](#S2.E1
    "第 2 项 ‣ 2.2.1 准确性 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 深度学习架构在二维图像语义分割中的应用调研")) 中所定义），对每个类别的重要性进行加权。FwIoU
    的公式在 ([4](#S2.E4 "第 3 项 ‣ 2.2.1 准确性 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 深度学习架构在二维图像语义分割中的应用调研"))
    中给出：
- en: '|  | $FwIoU=\frac{1}{\sum_{j=1}^{k}t_{j}}\sum_{j=1}^{k}{t_{j}\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}}},\qquad
    i\neq j$ |  | (4) |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $FwIoU=\frac{1}{\sum_{j=1}^{k}t_{j}}\sum_{j=1}^{k}{t_{j}\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}}},\qquad
    i\neq j$ |  | (4) |'
- en: IoU and its extensions, compute the ratio of true positives (hits) to the sum
    of false positives (false alarms), false negatives (misses) and true positives
    (hits). Thereby, the IoU measure is more informative when compared to pixel accuracy
    simply because it takes false alarms into consideration, whereas PA does not.
    However, since false alarms and misses are summed up in the denominator, the significance
    between them is not measured by this metric, which is considered its primary drawback.
    In addition, IoU only measures the number of pixels correctly labelled without
    considering how accurate the segmentation boundaries are.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IoU及其扩展，计算真实正例（命中数）与虚警（虚警数）、漏检（遗漏数）和真实正例（命中数）之和的比率。因此，相比于像素准确度，IoU度量更具信息量，因为它考虑了虚警，而PA则没有。然而，由于虚警和漏检在分母中被相加，这种度量未能衡量它们之间的显著性，这被认为是其主要缺陷。此外，IoU仅测量正确标记的像素数量，而不考虑分割边界的准确性。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Precision-Recall Curve (PRC)-based metrics*: Precision (ratio of hits over
    a summation of hits and false alarms) and recall (ratio of hits over a summation
    of hits and misses) are the two axes of the PRC used to depict the trade-off between
    precision and recall, under a varying threshold for the task of binary classification.
    PRC is very similar to ROC. However, PRC is more powerful in discriminating the
    effects between the false positives (alarms) and false negatives (misses). That
    is predominantly why PRC-based metrics are commonly used for evaluating the performance
    of semantic segmentation. The formula for Precision (also called Specificity)
    and Recall (also called Sensitivity) for a given class *j*, are provided in ([5](#S2.E5
    "In 4th item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images")):'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*基于精确率-召回率曲线（PRC）的度量*：精确率（命中数与命中数加上虚警数的比率）和召回率（命中数与命中数加上漏检数的比率）是PRC的两个轴，用于描述在二分类任务中，随着阈值变化精确率与召回率之间的权衡。PRC与ROC非常相似。然而，PRC在区分虚警（警报）和漏检（遗漏）效果方面更具优势。这主要是为什么基于PRC的度量常用于评估语义分割性能的原因。给定类别*j*的精确率（也称为特异性）和召回率（也称为敏感性）的公式，参见([5](#S2.E5
    "在第4项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 2D图像语义分割深度学习架构调查"))：'
- en: '|  | $Prec.=\frac{n{{}_{jj}}}{n_{ij}+n_{jj}},\quad Recall=\frac{n_{jj}}{n_{ji}+n_{jj}},i\neq
    j$ |  | (5) |'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Prec.=\frac{n{{}_{jj}}}{n_{ij}+n_{jj}},\quad Recall=\frac{n_{jj}}{n_{ji}+n_{jj}},i\neq
    j$ |  | (5) |'
- en: 'There are three main PRC-based metrics:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主要有三种基于PRC的度量：
- en: '$\circ$ *F*[score]: Also known as the ‘*dice coefficient*’, this measure is
    the harmonic mean of the precision and recall for a given threshold. It is a normalised
    measure of similarity, and ranges between 0 and 1 (Please see ([6](#S2.E6 "In
    4th item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images"))).'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\circ$ *F*[score]：也称为‘*dice系数*’，此度量是给定阈值下精确率和召回率的调和均值。它是一个归一化的相似度度量，范围在0到1之间（请参见
    ([6](#S2.E6 "在第4项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 2D图像语义分割深度学习架构调查"))）。
- en: '|  | $F_{score}=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |
    (6) |'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $F_{score}=2\times\frac{Precision\times Recall}{Precision+Recall}$ |  |
    (6) |'
- en: '$\circ$ *PRC-AuC*: This is similar to the ROC-AUC metric. It is simply the
    area under the PRC. This metric refers to information about the precision-recall
    trade-off for different thresholds, but not the *shape* of the PR curve.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\circ$ *PRC-AuC*：这类似于ROC-AUC度量。它只是PRC下的面积。此度量提供了不同阈值下的精确率-召回率权衡的信息，但不涉及PR曲线的*形状*。
- en: '$\circ$ *Average Precision* (AP): This metric is a single value that summarises
    both the shape and the AUC of PRC. In order to calculate AP, using the PRC, for
    uniformly sampled recall values (e.g., 0.0, 0.1, 0.2, …, 1.0), precision values
    are recorded. The average of these precision values is referred to as the average
    precision. This is the most commonly used single value metric for semantic segmentation.
    Similarly, mean average precision (mAP) is the mean of the AP values, calculated
    on a per-class basis.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\circ$ *平均精度*（AP）：该度量是一个总结PRC形状和AUC的单一值。为了计算AP，使用PRC，对均匀采样的召回值（例如，0.0、0.1、0.2、……、1.0）记录精确率值。这些精确率值的平均值被称为平均精度。这是语义分割中最常用的单值度量。类似地，平均平均精度（mAP）是按类别计算的AP值的平均值。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Hausdorff Distance* (HD): Hausdorff Distance is used incorporating the longest
    distance between classified and labelled pixels as an indicator of the largest
    segmentation error (Karimi and Salcudean [2019](#bib.bib66); Jadon [2020](#bib.bib61)),
    with the aim of tracking the performance of a semantic segmentation model. The
    unidirectional HDs as $hd(X,Y)$ and $hd(Y,X)$ are presented in ([7](#S2.E7 "In
    5th item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images")) and ([8](#S2.E8 "In 5th item ‣ 2.2.1 Accuracy
    ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges and Performance Evaluation
    ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D
    images")), respectively.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Hausdorff 距离*（HD）：Hausdorff 距离通过将分类和标记的像素之间的最大距离作为最大分割误差的指标来使用（Karimi 和 Salcudean
    [2019](#bib.bib66); Jadon [2020](#bib.bib61)），旨在跟踪语义分割模型的性能。单向 HDs 如 $hd(X,Y)$
    和 $hd(Y,X)$ 分别展示在 ([7](#S2.E7 "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查"))
    和 ([8](#S2.E8 "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查"))
    中。'
- en: '|  | $hd\left(X,Y\right)=\max_{{x\epsilon X}}\min_{{y\epsilon Y}}\left\&#124;x-y\right\&#124;_{2},$
    |  | (7) |'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $hd\left(X,Y\right)=\max_{{x\epsilon X}}\min_{{y\epsilon Y}}\left\|x-y\right\|_{2},$
    |  | (7) |'
- en: '|  | $hd\left(Y,X\right)=\max_{{y\epsilon Y}}\min_{{x\epsilon X}}\left\&#124;x-y\right\&#124;_{2}.$
    |  | (8) |'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $hd\left(Y,X\right)=\max_{{y\epsilon Y}}\min_{{x\epsilon X}}\left\|x-y\right\|_{2}.$
    |  | (8) |'
- en: where, $X$ and $Y$ are the pixel sets. The $x$ is the pixel in the segmented
    counter $X$ and $y$ is the pixel in the target counter $Y$ (Huang et al. [2020](#bib.bib58)).
    The bidirectional HD between these sets is shown in ([9](#S2.E9 "In 5th item ‣
    2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges and Performance
    Evaluation ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images")), where the Euclidean distance is employed for ([7](#S2.E7 "In
    5th item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges
    and Performance Evaluation ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images")), ([8](#S2.E8 "In 5th item ‣ 2.2.1 Accuracy
    ‣ 2.2 Performance Evaluation ‣ 2 Image Sets, Challenges and Performance Evaluation
    ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D
    images")) and ([9](#S2.E9 "In 5th item ‣ 2.2.1 Accuracy ‣ 2.2 Performance Evaluation
    ‣ 2 Image Sets, Challenges and Performance Evaluation ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images")).
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$X$ 和 $Y$ 是像素集合。$x$ 是分割计数器 $X$ 中的像素，而 $y$ 是目标计数器 $Y$ 中的像素（Huang et al. [2020](#bib.bib58)）。这些集合之间的双向
    HD 显示在 ([9](#S2.E9 "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查"))
    中，其中欧几里得距离被用于 ([7](#S2.E7 "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查"))、([8](#S2.E8
    "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查")) 和 ([9](#S2.E9
    "在第5项 ‣ 2.2.1 准确度 ‣ 2.2 性能评估 ‣ 2 图像集、挑战和性能评估 ‣ 关于基于深度学习的2D图像语义分割架构的调查"))。
- en: '|  | $HD\left(X,Y\right)=max\left(hd\left(X,Y\right),hd\left(Y,X\right)\right).$
    |  | (9) |'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $HD\left(X,Y\right)=\max\left(hd\left(X,Y\right),hd\left(Y,X\right)\right).$
    |  | (9) |'
- en: IoU and its variants, along with AP, are the most commonly used accuracy evaluation
    metrics in the most popular semantic segmentation challenges (Everingham et al.
    [2010](#bib.bib32); Mottaghi et al. [2014](#bib.bib111); Chen et al. [2014](#bib.bib20);
    Wang et al. [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47); Lin et al.
    [2014](#bib.bib97); Cordts et al. [2016](#bib.bib27)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: IoU 及其变体，加上 AP，是最流行的语义分割挑战中最常用的准确度评估指标（Everingham et al. [2010](#bib.bib32);
    Mottaghi et al. [2014](#bib.bib111); Chen et al. [2014](#bib.bib20); Wang et al.
    [2015](#bib.bib168); Hariharan et al. [2011](#bib.bib47); Lin et al. [2014](#bib.bib97);
    Cordts et al. [2016](#bib.bib27)）。
- en: 2.2.2 Computational Complexity
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 计算复杂度
- en: 'The burden of computation is evaluated using two main metrics: how fast the
    algorithm completes, and how much computational memory is demanded.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 计算负担通过两个主要指标进行评估：算法完成的速度以及所需的计算内存量。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Execution time*: This is measured as the whole processing time, starting from
    the instant a single image is introduced to the system/algorithm right through
    until the pixel-wise semantic segmentation results are obtained. The performance
    of this metric significantly depends on the hardware utilised. Thus, for an algorithm,
    any execution time metric should be accompanied by a thorough description of the
    hardware used. There are notations such as Big-O, which provide a complexity measure
    independent of the implementation domain. However, these notations are highly
    theoretical and are predominantly not preferred for extremely complex algorithms
    such as deep semantic segmentation as they are simple and largely inaccurate.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*执行时间*：这是测量整个处理时间的指标，从单个图像被引入系统/算法的瞬间开始，一直到获得逐像素语义分割结果为止。此指标的性能在很大程度上取决于所使用的硬件。因此，对于一个算法，任何执行时间指标都应附有对所使用硬件的详细描述。有诸如Big-O这样的符号，它提供了独立于实现领域的复杂性度量。然而，这些符号高度理论化，主要不适用于诸如深度语义分割等极其复杂的算法，因为它们简单且大多不准确。'
- en: For a deep learning-based algorithm, the offline (i.e., training) and online
    (i.e., testing) operation may last for considerably different time intervals.
    Technically, the execution time refers only to the online operation or, academically
    speaking, the test duration for a single image. Although this metric is extremely
    important for industrial applications, academic studies refrain from publishing
    exact execution times, and none of the aforementioned challenges was found to
    have provided this metric. A recent study, (Zhao et al. [2018](#bib.bib200)) provided
    a 2D histogram of Accuracy (MIoU%) vs frames-per-second, in which some of the
    state-of-the-art methods with open source codes (including their proposed structure,
    namely image cascade network – ICNet), were benchmarked using the Cityscapes (Cordts
    et al. [2016](#bib.bib27)) image set.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于基于深度学习的算法，离线（即训练）和在线（即测试）操作的时间间隔可能会有很大差异。从技术上讲，执行时间仅指在线操作或学术上讲的单张图像的测试持续时间。尽管这个指标对工业应用极其重要，但学术研究通常不会公布确切的执行时间，而且没有发现上述挑战中的任何一个提供了这个指标。一项最近的研究（Zhao等
    [2018](#bib.bib200)）提供了准确度（MIoU%）与每秒帧数的二维直方图，其中一些具有开源代码的最先进方法（包括他们提出的结构，即图像级联网络
    - ICNet）使用Cityscapes（Cordts等 [2016](#bib.bib27)）图像集进行基准测试。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Memory Usage*: Memory usage is specifically important when semantic segmentation
    is utilised in limited performance devices such as smartphones, digital cameras,
    or when the requirements of the system are extremely restrictive. The prime examples
    of these would be military systems or security-critical systems such as self-driving
    cars.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*内存使用量*：当语义分割用于性能有限的设备如智能手机、数码相机，或当系统要求极其严格时，内存使用量尤为重要。这些设备的主要例子包括军事系统或安全关键系统，如自动驾驶汽车。'
- en: The usage of memory for a complex algorithm like semantic segmentation may change
    drastically during operation. That is why a common metric for this purpose is
    *peak memory usage*, which is simply the maximum memory required for the entire
    segmentation operation for a single image. The metric may apply to computer (data)
    memory or GPU memory depending on the hardware design.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于像语义分割这样的复杂算法，内存的使用可能在操作过程中发生剧烈变化。这就是为什么一个常见的指标是*峰值内存使用量*，即单张图像的整个分割操作所需的最大内存。这个指标可以适用于计算机（数据）内存或GPU内存，具体取决于硬件设计。
- en: Although critical for industrial applications, this metric is not usually made
    available for any of the aforementioned challenges.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管对工业应用至关重要，但通常不会在上述挑战中提供这个指标。
- en: Computational efficiency is a very important aspect of any algorithm that is
    to be implemented on a real system. A comparative assessment of the speed and
    capacity of various semantic segmentation algorithms is a challenging task. Although
    most state-of-the-art algorithms are available with open-source codes, benchmarking
    all of them, with their optimal hyper-parameters, seems implausible. For this
    purpose, we provide an inductive way of comparing the computational efficiencies
    of methods in the following sections. In Table [1](#S5.T1 "Table 1 ‣ 5.3 Object
    Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images"), we categorise methods
    into mainly four levels of computational efficiency and discuss our categorisation
    related to the architectural design of a given method. This table also provides
    a chronological evolution of the semantic segmentation methods in the literature.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 计算效率是任何要在实际系统上实现的算法中非常重要的方面。对各种语义分割算法的速度和容量进行比较评估是一项具有挑战性的任务。尽管大多数最先进的算法都有开源代码，但对它们进行基准测试并使用其最佳超参数似乎不切实际。为此，我们提供了一种归纳的方式来比较以下部分中方法的计算效率。在表[1](#S5.T1
    "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey
    on Deep Learning-based Architectures for Semantic Segmentation on 2D images")中，我们将方法分为主要的四个计算效率级别，并讨论我们分类与给定方法的架构设计相关的内容。该表还提供了文献中语义分割方法的时间演变。
- en: 3 Before Fully Convolutional Networks
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 在全卷积网络之前
- en: As mentioned in the Introduction, the utilisation of FCNs is a breaking point
    for semantic segmentation literature. Efforts on semantic segmentation literature
    prior to FCNs (Shelhamer et al. [2017](#bib.bib143)) can be analysed in two separate
    branches, as pre-deep learning and early deep learning approaches. In this section,
    we briefly discuss both sets of approaches.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中所述，FCNs的使用是语义分割文献中的一个突破点。在FCNs之前的语义分割文献（Shelhamer et al. [2017](#bib.bib143)）可以分为两个不同的分支，分别是深度学习前和早期深度学习方法。在本节中，我们简要讨论这两组方法。
- en: 3.1 Pre-Deep Learning Approaches
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度学习前的方法
- en: The differentiating factor between conventional image segmentation and semantic
    segmentation is the utilisation of semantic features in the process. Conventional
    methods for image segmentation such as thresholding, clustering, and region growing,
    etc. (*please see* (Zaitoun and Aqel [2015](#bib.bib193)) *for a survey on conventional
    image segmentation techniques*) utilise handcrafted low-level features (i.e.,
    edges, blobs) to locate object boundaries in images. Thus, in situations where
    the semantic information of an image is necessary for pixel-wise segmentation,
    such as in similar objects occluding each other, these methods usually return
    a poor performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 传统图像分割和语义分割之间的区别在于在过程中利用了语义特征。传统的图像分割方法如阈值处理、聚类和区域生长等（*请参见* (Zaitoun and Aqel
    [2015](#bib.bib193)) *了解传统图像分割技术的综述*）使用手工制作的低级特征（即边缘、斑点）来定位图像中的物体边界。因此，在需要图像的语义信息进行逐像素分割的情况下，例如在相似物体遮挡彼此时，这些方法通常表现较差。
- en: Regarding semantic segmentation efforts prior to DCNNs becoming popular, a wide
    variety of approaches (He and Zemel [2009](#bib.bib52); Ulusoy and Bishop [2005](#bib.bib160);
    Ladický et al. [2009](#bib.bib78); Fröhlich et al. [2013](#bib.bib37); Montillo
    et al. [2011](#bib.bib110); Ravì et al. [2016](#bib.bib130); Vezhnevets et al.
    [2011](#bib.bib163); Shotton et al. [2008](#bib.bib148); Yao et al. [2012](#bib.bib186);
    Xiao and Quan [2009](#bib.bib177); Mičušĺík and Košecká [2009](#bib.bib109); Lempitsky
    et al. [2011](#bib.bib86); Krähenbühl and Koltun [2011](#bib.bib73)) utilised
    graphical models, such as Markov Random Fields (MRF), Conditional Random Fields
    (CRF) or forest-based (or sometimes referred to as ‘holistic’) methods, in order
    to find scene labels at the pixel level. The main idea was to find an inference
    by observing the dependencies between neighbouring pixels. In other words, these
    methods modelled the semantics of the image as a kind of prior information among
    adjacent pixels. Thanks to deep learning, today we know that image semantics require
    abstract exploitation of largescale data. Initially, graph-based approaches were
    thought to have this potential. The so-called ‘super-pixelisation’, which is usually
    the term applied in these studies, was a process of modelling abstract regions.
    However, a practical and feasible implementation for largescale data processing
    was never achieved for these methods, while it was accomplished for DCNNs, first
    by (Krizhevsky et al. [2012](#bib.bib74)) and then in many other studies.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在深度卷积神经网络（DCNNs）变得流行之前的语义分割努力，各种方法（He and Zemel [2009](#bib.bib52); Ulusoy
    and Bishop [2005](#bib.bib160); Ladický et al. [2009](#bib.bib78); Fröhlich et
    al. [2013](#bib.bib37); Montillo et al. [2011](#bib.bib110); Ravì et al. [2016](#bib.bib130);
    Vezhnevets et al. [2011](#bib.bib163); Shotton et al. [2008](#bib.bib148); Yao
    et al. [2012](#bib.bib186); Xiao and Quan [2009](#bib.bib177); Mičušĺík and Košecká
    [2009](#bib.bib109); Lempitsky et al. [2011](#bib.bib86); Krähenbühl and Koltun
    [2011](#bib.bib73)) 运用了图形模型，如马尔可夫随机场（MRF）、条件随机场（CRF）或基于森林（有时称为‘整体’）的方法，以在像素级别找到场景标签。主要思想是通过观察邻近像素之间的依赖关系来找到推断。换句话说，这些方法将图像的语义建模为相邻像素之间的先验信息。由于深度学习，如今我们知道图像语义需要对大规模数据的抽象利用。最初，基于图的
    approaches 被认为具有这种潜力。所谓的“超级像素化”，通常是这些研究中使用的术语，是建模抽象区域的过程。然而，这些方法从未实现大规模数据处理的实际可行实现，而这已由（Krizhevsky
    et al. [2012](#bib.bib74)）首次实现，并在许多其他研究中得到应用。
- en: Another group of studies, sometimes referred to as the ‘Layered models’ (Yang
    et al. [2012](#bib.bib185); Arbeláez et al. [2012](#bib.bib4); Ladický et al.
    [2010](#bib.bib77)), used a composition of pre-trained and separate object detectors
    so as to extract the semantic information from the image. Because the individual
    object detectors failed to classify regions properly, or because the methods were
    limited by the finite number of object classes provided by the ‘hand-selected’
    bank of detectors in general, their performance was seen as relatively low compared
    to today’s state-of-the-art methods.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组研究，有时称为“层次模型”（Yang et al. [2012](#bib.bib185); Arbeláez et al. [2012](#bib.bib4);
    Ladický et al. [2010](#bib.bib77)），使用了预训练的单独目标检测器的组合，以从图像中提取语义信息。由于单独的目标检测器无法正确分类区域，或因为这些方法受限于‘手动选择’检测器库中提供的有限目标类别数量，相较于今天最先进的方法，它们的性能被认为较低。
- en: Although the aforementioned methods of the pre-deep learning era are no longer
    preferred as segmentation methods, some of the graphical models, especially CRFs,
    are currently being utilised by the state-of-the-art methods as post-processing
    (refinement) layers, with the purpose of improving the semantic segmentation performance,
    the details of which are discussed in the following section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前深度学习时代提到的方法不再被优先作为分割方法，但一些图形模型，尤其是条件随机场（CRFs），目前被最先进的方法用作后处理（细化）层，目的是提高语义分割性能，具体细节将在以下部分讨论。
- en: '![Refer to caption](img/7ab09c7412fcf601c948f1a3da29cf8d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ab09c7412fcf601c948f1a3da29cf8d.png)'
- en: (a) Input Image
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入图像
- en: '![Refer to caption](img/f1716e7db046c8fc17e82b8fac78f293.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f1716e7db046c8fc17e82b8fac78f293.png)'
- en: (b) Segmented Image
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 分割图像
- en: '![Refer to caption](img/7bea5946e59185e47f41191db4baf49c.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7bea5946e59185e47f41191db4baf49c.png)'
- en: (c) Refined Result
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 细化结果
- en: 'Figure 2: Effect of using graphical model-based refinement on segmentation
    results.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用基于图形模型的细化对分割结果的影响。
- en: 3.1.1 Refinement Methods
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 细化方法
- en: Deep neural networks are powerful in extracting abstract local features. However,
    they lack the capability to utilise global context information, and accordingly
    cannot model interactions between adjacent pixel predictions (Teichmann and Cipolla
    [2018](#bib.bib155)). On the other hand, the popular segmentation methods of the
    pre-deep learning era, the graphical models, are highly suited to this sort of
    task. That is why they are currently being used as a refinement layer on many
    DCNN-based semantic segmentation architectures.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在提取抽象局部特征方面非常强大。然而，它们缺乏利用全局上下文信息的能力，因此无法建模相邻像素预测之间的交互（Teichmann 和 Cipolla
    [2018](#bib.bib155)）。另一方面，预深度学习时代流行的分割方法——图形模型——非常适合这种任务。这就是为什么它们目前被用作许多基于 DCNN
    的语义分割架构的细化层。
- en: As also mentioned in the previous section, the idea behind using graphical models
    for segmentation is finding an inference by observing the low-level relations
    between neighbouring pixels. In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Pre-Deep Learning
    Approaches ‣ 3 Before Fully Convolutional Networks ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images"), the effect of using a
    graphical model-based refinement on segmentation results can be seen. The classifier
    (see Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Pre-Deep Learning Approaches ‣ 3 Before
    Fully Convolutional Networks ‣ A Survey on Deep Learning-based Architectures for
    Semantic Segmentation on 2D images").b) cannot correctly segment pixels where
    different class labels are adjacent. In this example, a CRF-based refinement (Krähenbühl
    and Koltun [2011](#bib.bib73)) is applied to improve the pixel-wise segmentation
    results. CRF-based methods are widely used for the refinement of deep semantic
    segmentation methods, although some alternative graphical model-based refinement
    methods also exist in the literature (Liu et al. [2015](#bib.bib102); Zuo and
    Drummond [2017](#bib.bib213)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所提到的，使用图形模型进行分割的理念是通过观察相邻像素之间的低级关系来寻找推断。在图 [2](#S3.F2 "Figure 2 ‣ 3.1 Pre-Deep
    Learning Approaches ‣ 3 Before Fully Convolutional Networks ‣ A Survey on Deep
    Learning-based Architectures for Semantic Segmentation on 2D images") 中，可以看到使用基于图形模型的细化对分割结果的影响。分类器（见图
    [2](#S3.F2 "Figure 2 ‣ 3.1 Pre-Deep Learning Approaches ‣ 3 Before Fully Convolutional
    Networks ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images").b) 在不同类别标签相邻的像素处无法正确分割。在这个例子中，应用了基于 CRF 的细化（Krähenbühl 和 Koltun
    [2011](#bib.bib73)）以改善逐像素的分割结果。虽然也存在一些替代的基于图形模型的细化方法（Liu 等 [2015](#bib.bib102);
    Zuo 和 Drummond [2017](#bib.bib213)），但 CRF 基于的方法在深度语义分割方法的细化中被广泛使用。
- en: CRFs (Lafferty et al. [2001](#bib.bib79)) are a type of discriminative undirected
    probabilistic graphical model. They are used to encode known relationships between
    observations and to construct consistent interpretations. Their usage as a refinement
    layer comes from the fact that, unlike a discrete classifier, which does not consider
    the similarity of adjacent pixels, a CRF can utilise this information. The main
    advantage of CRFs over other graphical models (such as Hidden Markov Models) is
    their conditional nature and their ability to avoid the problem of label bias
    (Lafferty et al. [2001](#bib.bib79)). Even though a considerable number of methods
    (see Table [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images")) utilise CRFs for refinement, these models started to lose popularity
    in relatively recent approaches because they are notoriously slow and very difficult
    to optimise (Teichmann and Cipolla [2018](#bib.bib155)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: CRF（Lafferty 等 [2001](#bib.bib79)）是一种判别性无向概率图模型。它们用于编码观测之间已知的关系，并构建一致的解释。作为细化层的使用源于这样一个事实，即与不考虑相邻像素相似性的离散分类器不同，CRF
    可以利用这些信息。CRF 相对于其他图形模型（如隐马尔可夫模型）的主要优势在于其条件性质和避免标签偏差问题的能力（Lafferty 等 [2001](#bib.bib79)）。尽管有相当多的方法（见表
    [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches
    ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D
    images")）利用 CRF 进行细化，但这些模型在相对较新的方法中开始失去人气，因为它们 notoriously 缓慢且非常难以优化（Teichmann
    和 Cipolla [2018](#bib.bib155)）。
- en: 3.2 Early Deep Learning Approaches
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 早期深度学习方法
- en: Before FCNs first appeared in 2014⁶⁶6FCN (Shelhamer et al. [2017](#bib.bib143))
    ] was officially published in 2017\. However, the same group first shared the
    idea online as pre-printed literature in 2014 (Long et al. [2014](#bib.bib103)).,
    the initial few years of deep convolutional networks saw a growing interest in
    the idea of utilising the newly discovered deep features for semantic segmentation
    (Ning et al. [2005](#bib.bib112); Ganin and Lempitsky [2014](#bib.bib39); Ciresan
    et al. [2012](#bib.bib26); Farabet et al. [2013](#bib.bib35); Hariharan et al.
    [2014](#bib.bib46); Pinheiro and Collobert [2014](#bib.bib124)). The very first
    approaches, which were published prior to the proposal of a Rectified Linear Unit
    (ReLU) layer (Krizhevsky et al. [2012](#bib.bib74)), used activation functions
    such as *tanh* (Ning et al. [2005](#bib.bib112)) (or similar continuous functions),
    which can be computationally difficult to differentiate. Thus, training such systems
    were not considered to be computation-friendly, or even feasible for largescale
    data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FCNs 首次出现之前，6FCN（Shelhamer 等 [2017](#bib.bib143)）于 2017 年正式发布。然而，同一小组在 2014
    年首次在线分享了这一想法作为预印本文献（Long 等 [2014](#bib.bib103)）。在深度卷积网络的初期几年中，越来越多的研究关注利用新发现的深度特征进行语义分割（Ning
    等 [2005](#bib.bib112)；Ganin 和 Lempitsky [2014](#bib.bib39)；Ciresan 等 [2012](#bib.bib26)；Farabet
    等 [2013](#bib.bib35)；Hariharan 等 [2014](#bib.bib46)；Pinheiro 和 Collobert [2014](#bib.bib124)）。在提出
    Rectified Linear Unit (ReLU) 层（Krizhevsky 等 [2012](#bib.bib74)）之前，最初的方法使用了像 *tanh*（Ning
    等 [2005](#bib.bib112)）这样的激活函数（或类似的连续函数），这些函数在计算上可能很难区分。因此，训练这样的系统并不被认为是计算友好的，甚至对于大规模数据也不可行。
- en: However, the first mature approaches were just simple attempts to convert classification
    networks such Alex-Net and VGG to segmentation networks by fine-tuning the fully
    connected layers (Ning et al. [2005](#bib.bib112); Ganin and Lempitsky [2014](#bib.bib39);
    Ciresan et al. [2012](#bib.bib26)). They suffered from the overfitting and time-consuming
    nature of their fully connected layers in the training phase. Moreover, the CNNs
    used were not sufficiently deep so as to create abstract features, which would
    relate to the semantics of the image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最初成熟的方法只是简单地尝试通过微调全连接层将分类网络如 Alex-Net 和 VGG 转换为分割网络（Ning 等 [2005](#bib.bib112)；Ganin
    和 Lempitsky [2014](#bib.bib39)；Ciresan 等 [2012](#bib.bib26)）。它们在训练阶段遭遇了过拟合和全连接层耗时的问题。此外，所使用的
    CNN 不够深，无法创建与图像语义相关的抽象特征。
- en: There were a few early deep learning studies in which the researchers declined
    to use fully connected layers for their decision-making. However, they utilised
    different structures such as a recurrent architecture (Pinheiro and Collobert
    [2014](#bib.bib124)) or using labelling from a family of separately computed segmentations
    (Farabet et al. [2013](#bib.bib35)). By proposing alternative solutions to fully
    connected layers, these early studies showed the first traces of the necessity
    for a structure like the FCN, and unsurprisingly they were succeeded by (Shelhamer
    et al. [2017](#bib.bib143)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的深度学习研究中，一些研究者选择不使用全连接层进行决策。然而，他们利用了不同的结构，如递归架构（Pinheiro 和 Collobert [2014](#bib.bib124)）或利用从一组独立计算的分割中得到的标签（Farabet
    等 [2013](#bib.bib35)）。通过提出替代全连接层的解决方案，这些早期研究显示了像 FCN 这样的结构的必要性，毫无疑问，之后出现了（Shelhamer
    等 [2017](#bib.bib143)）。
- en: Since their segmentation results were deemed to be unsatisfactory, these studies
    generally utilised a refinement process, either as a post-processing layer(Ning
    et al. [2005](#bib.bib112); Ganin and Lempitsky [2014](#bib.bib39); Ciresan et al.
    [2012](#bib.bib26); Hariharan et al. [2014](#bib.bib46)) or as an alternative
    architecture to fully connected decision layers (Farabet et al. [2013](#bib.bib35);
    Pinheiro and Collobert [2014](#bib.bib124)). Refinement methods varied, such as
    Markov random fields (Ning et al. [2005](#bib.bib112)), nearest neighbour-based
    approach (Ganin and Lempitsky [2014](#bib.bib39)), the use of a calibration layer
    (Ciresan et al. [2012](#bib.bib26)), using super-pixels (Farabet et al. [2013](#bib.bib35);
    Hariharan et al. [2014](#bib.bib46)), or a recurrent network of plain CNNs (Pinheiro
    and Collobert [2014](#bib.bib124)). Refinement layers, as discussed in the previous
    section, are still being utilised by post-FCN methods, with the purpose of increasing
    the pixel-wise labelling performance around regions where class intersections
    occur.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其分割结果被认为不令人满意，这些研究通常采用了精细化过程，无论是作为后处理层 (Ning et al. [2005](#bib.bib112); Ganin
    and Lempitsky [2014](#bib.bib39); Ciresan et al. [2012](#bib.bib26); Hariharan
    et al. [2014](#bib.bib46)) 还是作为全连接决策层的替代架构 (Farabet et al. [2013](#bib.bib35);
    Pinheiro and Collobert [2014](#bib.bib124))。精细化方法有所不同，例如马尔可夫随机场 (Ning et al. [2005](#bib.bib112))、基于最近邻的方法
    (Ganin and Lempitsky [2014](#bib.bib39))、使用校准层 (Ciresan et al. [2012](#bib.bib26))、使用超像素
    (Farabet et al. [2013](#bib.bib35); Hariharan et al. [2014](#bib.bib46))，或是普通
    CNN 的递归网络 (Pinheiro and Collobert [2014](#bib.bib124))。如前面章节所述，精细化层仍在 FCN 之后的方法中被使用，目的是提高类交叉区域周围的像素级标注性能。
- en: 4 Fully Convolutional Networks for Semantic Segmentation
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 全卷积网络用于语义分割
- en: In (Shelhamer et al. [2017](#bib.bib143)), the idea of dismantling fully connected
    layers from deep CNNs (DCNN) was proposed, and to imply this idea, the proposed
    architecture was named as ‘Fully Convolutional Networks’ (see Figure 3). The main
    objective was to create semantic segmentation networks by adapting classification
    networks such as AlexNet (Krizhevsky et al. [2012](#bib.bib75)), VGG (Simonyan
    and Zisserman [2015](#bib.bib151)) , and GoogLeNet (Szegedy et al. [2015](#bib.bib154))
    into fully convolutional networks, and then transferring their learnt representations
    by fine-tuning. The most widely used architectures obtained from the study (Shelhamer
    et al. [2017](#bib.bib143)) are known as ‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’, which
    are all transfer-learnt using the VGG architecture (Simonyan and Zisserman [2015](#bib.bib151)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (Shelhamer et al. [2017](#bib.bib143)) 中，提出了从深度卷积神经网络（DCNN）中拆除全连接层的思想，并为实现这一思想，提出的架构被命名为“全卷积网络”（见图
    3）。其主要目标是通过将分类网络如 AlexNet (Krizhevsky et al. [2012](#bib.bib75))、VGG (Simonyan
    and Zisserman [2015](#bib.bib151)) 和 GoogLeNet (Szegedy et al. [2015](#bib.bib154))
    适配为全卷积网络来创建语义分割网络，并通过微调来转移其学习到的表示。从研究 (Shelhamer et al. [2017](#bib.bib143)) 中获得的最广泛使用的架构被称为“FCN-32s”、“FCN16s”和“FCN8s”，它们都是使用
    VGG 架构 (Simonyan and Zisserman [2015](#bib.bib151)) 进行迁移学习的。
- en: FCN architecture was considered revolutionary in many aspects. First of all,
    since FCNs did not include fully connected layers, inference per image was seen
    to be considerably faster. This was mainly because convolutional layers when compared
    to fully connected layers, had a marginal number of weights. Second, and maybe
    more significant, the structure allowed segmentation maps to be generated for
    images of any resolution. In order to achieve this, FCNs used deconvolutional
    layers that can upsample coarse deep convolutional layer outputs to dense pixels
    of any desired resolution. Finally, and most importantly, they proposed the skip
    architecture for DCNNs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: FCN 架构在许多方面被认为是具有革命性的。首先，由于 FCNs 不包含全连接层，每张图像的推理速度显著更快。这主要是因为与全连接层相比，卷积层的权重数量较少。其次，也许更重要的是，这种结构允许生成任何分辨率的图像的分割图。在实现这一点时，FCNs
    使用了反卷积层，可以将粗糙的深度卷积层输出上采样到任何所需分辨率的密集像素。最后，也是最重要的，他们提出了针对 DCNN 的跳跃结构。
- en: '![Refer to caption](img/e898e2f7f444422249a2dacdd13fdc99.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e898e2f7f444422249a2dacdd13fdc99.png)'
- en: 'Figure 3: Fully convolutional networks (FCNs) are trained end-to-end and are
    designed to make dense predictions for per-pixel tasks like semantic segmentation.
    FCNs consist of no fully connected layers .'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：全卷积网络（FCNs）经过端到端训练，旨在对像语义分割这样的每像素任务进行密集预测。FCNs 不包含任何全连接层。
- en: Skip architectures (or connections) provide links between nonadjacent layers
    in DCNNs. Simply by summing or concatenating outputs of unconnected layers, these
    connections enable information to flow, which would otherwise be lost because
    of an architectural choice such as max-pooling layers or dropouts. The most common
    practice is to use skip connections preceding a max-pooling layer, which downsamples
    layer output by choosing the maximum value in a specific region. Pooling layers
    helps the architecture create feature hierarchies, but also causes loss of localised
    information which could be valuable for semantic segmentation, especially at object
    borders. Skip connections preserve and forward this information to deeper layers
    by way of bypassing the pooling layers. Actually, the usage of skip connections
    in (Shelhamer et al. [2017](#bib.bib143)) was perceived as being considerably
    primitive. The ‘FCN-8s’ and ‘FCN-16s’ networks included these skip connections
    at different layers. Denser skip connections for the same architecture, namely
    ‘FCN-4s’ and ‘FCN-2s’, were also utilised for various applications (Zhong et al.
    [2016](#bib.bib208); Lee et al. [2017](#bib.bib83)). This idea eventually evolved
    into the encoder-decoder structures (Ronneberger et al. [2015](#bib.bib134); Badrinarayanan
    et al. [2015](#bib.bib6)) for semantic segmentation, which are presented in the
    following section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃架构（或连接）提供了DCNN中非相邻层之间的链接。通过简单地对未连接层的输出进行求和或连接，这些连接使信息得以流动，否则会因为建筑选择如最大池化层或丢弃而丢失。最常见的做法是使用跳跃连接，置于最大池化层之前，最大池化层通过选择特定区域中的最大值来降采样层输出。池化层有助于架构创建特征层次，但也会导致局部信息的丢失，这对语义分割尤其是在物体边界处可能非常有价值。跳跃连接通过绕过池化层来保留并传递这些信息到更深层次。实际上，在(Shelhamer
    et al. [2017](#bib.bib143))中使用跳跃连接被认为相当原始。‘FCN-8s’和‘FCN-16s’网络在不同层中包括了这些跳跃连接。对于相同架构，‘FCN-4s’和‘FCN-2s’也被用于各种应用（Zhong
    et al. [2016](#bib.bib208); Lee et al. [2017](#bib.bib83)）。这一理念最终演变为编码器-解码器结构（Ronneberger
    et al. [2015](#bib.bib134); Badrinarayanan et al. [2015](#bib.bib6)），用于语义分割，这将在接下来的部分中介绍。
- en: 5 Post-FCN Approaches
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种后FCN方法
- en: Almost all subsequent approaches on semantic segmentation followed the idea
    of FCNs; thus it would not be wrong to state that decision-making with fully-connected
    layers effectively ceased to exist⁷⁷7Many methods utilise fully connected layers
    such as RCNN (Girshick [2015](#bib.bib43)), which are discussed in the following
    sections. However, this and other similar methods that include fully connected
    layers have mostly been succeeded by fully convolutional versions for the sake
    of computational efficiency. following the appearance of FCNs to the issue of
    semantic segmentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有后续的语义分割方法都遵循了FCNs的理念；因此，完全连接层的决策方式可以说基本上不再存在⁷⁷7许多方法利用全连接层，例如RCNN（Girshick
    [2015](#bib.bib43)），这些将在接下来的章节中讨论。然而，这些包含全连接层的类似方法大多被完全卷积版本取代，以提高计算效率。随着FCNs的出现，这种语义分割的问题也随之而来。
- en: On the other hand, the idea of FCNs also created new opportunities to further
    improve deep semantic segmentation architectures. Generally speaking, the main
    drawbacks of FCNs can be summarised as inefficient loss of label localisation
    within the feature hierarchy, inability to process global context knowledge, and
    the lack of a mechanism for multiscale processing. Thus, most subsequent studies
    have been principally aimed at solving these issues through the proposal of various
    architectures or techniques. For the remainder of this paper, we analyse these
    issues under the title, ‘fine-grained localisation’. Consequently, before presenting
    a list of the post-FCN state-of-the-art methods, we focus on this categorisation
    of techniques and examine different approaches that aim at solving these main
    issues. In the following, we also discuss scale invariance in the semantic segmentation
    context and finish with object detection-based approaches, which are a new breed
    of solution that aim at resolving the semantic segmentation problem simultaneously
    with detecting object instances.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，FCNs的理念也创造了进一步改进深度语义分割架构的新机会。一般来说，FCNs的主要缺点可以总结为特征层次中标签定位的效率低下、无法处理全局上下文知识以及缺乏多尺度处理机制。因此，大多数后续研究主要通过提出各种架构或技术来解决这些问题。本文的其余部分我们将在“精细定位”这一标题下分析这些问题。因此，在列出后FCN的最先进方法之前，我们重点关注这些技术的分类，并检查旨在解决这些主要问题的不同方法。接下来，我们还讨论了语义分割中的尺度不变性，并以基于对象检测的方法结束，这是一种新兴的解决方案，旨在同时解决语义分割问题和检测对象实例。
- en: '![Refer to caption](img/2d99532fde393d50664806ff90fddbd0.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2d99532fde393d50664806ff90fddbd0.png)'
- en: (a) Encoder-Decoder Architecture.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 编码器-解码器架构。
- en: '![Refer to caption](img/6aed19b00f0ef05a2d64a490fb83d00b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6aed19b00f0ef05a2d64a490fb83d00b.png)'
- en: (b) Spatial-Pyramid Pooling Layer
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 空间金字塔池化层
- en: '![Refer to caption](img/c4eb88ff89ea2c4a9814fd9af49a26be.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c4eb88ff89ea2c4a9814fd9af49a26be.png)'
- en: (c) Regular vs. Dilated Convolutions
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 常规卷积与扩张卷积
- en: '![Refer to caption](img/00ec8a95c16c1e3960483202592113c0.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/00ec8a95c16c1e3960483202592113c0.png)'
- en: (d) DeepLabv3+ Architecture
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (d) DeepLabv3+ 架构
- en: '![Refer to caption](img/d5014e92def3b86f9f493b2ff56a5a8f.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d5014e92def3b86f9f493b2ff56a5a8f.png)'
- en: (e) DlinkNet Architecture
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (e) DlinkNet 架构
- en: 'Figure 4: Different architectures for fine-grained pixel-wise label localisation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于精细像素级标签定位的不同架构。
- en: 5.1 Techniques for Fine-grained Localisation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 精细定位技术
- en: Semantic segmentation is, by definition, a dense procedure; hence it requires
    fine-grained localisation of class labels at the pixel level. For example, in
    robotic surgery, pixel errors in semantic segmentation can lead to life or death
    situations. Hierarchical features created by pooling (i.e., max-pooling) layers
    can partially lose localisation. Moreover, due to their fully convolutional nature,
    FCNs do not inherently possess the ability to model global context information
    in an image, which is also very effective in the localisation of class labels.
    Thus, these two issues are intertwined in nature, and in the following, we discuss
    different approaches that aim at overcoming these problems and to providing finer
    localisation of class labels.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割本质上是一个密集过程，因此它需要在像素级别对类别标签进行精细的定位。例如，在机器人手术中，语义分割中的像素错误可能导致生死攸关的情况。通过池化（即最大池化）层创建的层次特征可能会部分丧失定位能力。此外，由于全卷积网络（FCNs）的完全卷积特性，它们本身并不具备建模图像中的全局上下文信息的能力，而全局上下文信息在类别标签的定位中也非常有效。因此，这两个问题本质上是相互交织的，在下文中，我们讨论旨在克服这些问题并提供更精细类别标签定位的不同方法。
- en: 5.1.1 Encoder-Decoder Architecture
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 编码器-解码器架构
- en: The so-called Encoder-Decoder (ED) architectures (also known as the U-nets,
    referring to the pioneering study of (Ronneberger et al. [2015](#bib.bib134)))
    are comprised of two parts. Encoder gradually reduces the spatial dimension with
    pooling layers, whilst decoder gradually recovers the object details and spatial
    dimension. Each feature map of the decoder part only directly receives the information
    from the feature map at the same level of the encoder part using skip connections,
    thus EDs can create abstract hierarchical features with fine localisation (see
    Figure [4](#S5.F4 "Figure 4 ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images").a). U-Net (Ronneberger
    et al. [2015](#bib.bib134)) and Seg-Net (Badrinarayanan et al. [2015](#bib.bib6))
    are very well-known examples. In this architecture, the strongly correlated semantic
    information, which is provided by the adjacent lower-resolution feature map of
    the encoder part, has to pass through additional intermediate layers in order
    to reach the same decoder layer. This usually results in a level of information
    decay. However, U-Net architectures have proven very useful for the segmentation
    of different applications, such as medical images (Ronneberger et al. [2015](#bib.bib134)),
    street view images (Badrinarayanan et al. [2015](#bib.bib6)), satellite images
    (Ulku et al. [2019](#bib.bib159)), just to name a few. Although earlier ED architectures
    were designed for object segmentation tasks only, there are also modified versions
    such as “TernausNetV2” (Iglovikov et al. [2018](#bib.bib60)), that provide instance
    segmentation capability with minor architectural changes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的编码器-解码器（ED）架构（也称为 U-net，指的是（Ronneberger et al. [2015](#bib.bib134)）的开创性研究）由两个部分组成。编码器通过池化层逐渐减少空间维度，而解码器则逐渐恢复物体细节和空间维度。解码器部分的每个特征图仅直接接收来自编码器部分相同层级特征图的信息，通过跳跃连接，因此
    EDs 可以创建具有精细定位的抽象层次特征（参见图 [4](#S5.F4 "图 4 ‣ 5 Post-FCN 方法 ‣ 关于 2D 图像语义分割的深度学习架构调查")。a）。U-Net（Ronneberger
    et al. [2015](#bib.bib134)）和 Seg-Net（Badrinarayanan et al. [2015](#bib.bib6)）是非常知名的例子。在这种架构中，来自编码器部分的相邻低分辨率特征图提供的强相关语义信息必须经过额外的中间层才能到达相同的解码器层。这通常会导致信息的衰减。然而，U-Net
    架构在不同应用的分割中已被证明非常有用，例如医学图像（Ronneberger et al. [2015](#bib.bib134)）、街景图像（Badrinarayanan
    et al. [2015](#bib.bib6)）、卫星图像（Ulku et al. [2019](#bib.bib159)），仅举几例。尽管早期的 ED
    架构仅设计用于物体分割任务，但也有修改版，如“TernausNetV2”（Iglovikov et al. [2018](#bib.bib60)），通过少量架构改动提供实例分割能力。
- en: 5.1.2 Spatial Pyramid Pooling
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 空间金字塔池化
- en: The idea of constructing a fixed-sized spatial pyramid was first proposed by
    (Lazebnik et al. [2006](#bib.bib81)), in order to prevent a Bag-of-Words system
    losing spatial relations among features. Later, the approach was adopted to CNNs
    by (He et al. [2015](#bib.bib49)), in that, regardless of the input size, a spatial
    pyramid representation of deep features could be created in a Spatial Pyramid
    Pooling Network (SPP-Net). The most important contribution of the SPP-Net was
    that it allowed inputs of different sizes to be fed into CNNs. Images of different
    sizes fed into convolutional layers inevitably create different-sized feature
    maps. However, if a pooling layer, just prior to a decision layer, has stride
    values proportional to the input size, the feature map created by that layer would
    be fixed (see Figure [4](#S5.F4 "Figure 4 ‣ 5 Post-FCN Approaches ‣ A Survey on
    Deep Learning-based Architectures for Semantic Segmentation on 2D images").b).
    By (Li et al. [2018](#bib.bib87)), a modified version, namely Pyramid Attention
    Network (PAN) was additionally proposed. The idea of PAN was combining an SPP
    layer with global pooling to learn a better feature representation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 构建固定大小空间金字塔的想法最早由（Lazebnik et al. [2006](#bib.bib81)）提出，以防止 Bag-of-Words 系统丧失特征之间的空间关系。后来，（He
    et al. [2015](#bib.bib49)）将这一方法应用于卷积神经网络（CNNs），即无论输入大小如何，都可以在空间金字塔池化网络（SPP-Net）中创建深度特征的空间金字塔表示。SPP-Net
    的最重要贡献是它允许不同大小的输入被输入到 CNNs 中。不同大小的图像输入到卷积层中不可避免地会创建不同大小的特征图。然而，如果在决策层之前的池化层的步幅值与输入大小成比例，则该层创建的特征图将是固定的（参见图
    [4](#S5.F4 "图 4 ‣ 5 Post-FCN 方法 ‣ 关于 2D 图像语义分割的深度学习架构调查")。b）。通过（Li et al. [2018](#bib.bib87)），还提出了一种修改版本，即金字塔注意网络（PAN）。PAN
    的想法是将 SPP 层与全局池化相结合，以学习更好的特征表示。
- en: There is a common misconception that SPP-Net structure carries an inherent scale-invariance
    property, which is incorrect. SPP-Net allows the efficient training of images
    at different scales (or resolutions) by allowing different input sizes to a CNN.
    However, the trained CNN with SPP is scale-invariant if, and only if, the training
    set includes images with different scales. This fact is also true for a CNN without
    SPP layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种常见的误解认为 SPP-Net 结构具有固有的尺度不变性，这并不正确。SPP-Net 通过允许不同输入尺寸的 CNN，有效地训练不同尺度（或分辨率）的图像。然而，只有当训练集包含不同尺度的图像时，带有
    SPP 的训练 CNN 才具有尺度不变性。这一点对于没有 SPP 层的 CNN 也是适用的。
- en: However, similar to the original idea proposed in (Lazebnik et al. [2006](#bib.bib81)),
    the SPP layer in a CNN constructs relations among the features of different hierarchies.
    Thus, it is quite similar to skip connections in ED structures, which also allow
    information flow between feature hierarchies.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 (Lazebnik et al. [2006](#bib.bib81)) 中提出的原始想法类似，CNN 中的 SPP 层构建了不同层级特征之间的关系。因此，它与
    ED 结构中的跳跃连接非常相似，后者也允许特征层级之间的信息流动。
- en: The most common utilisation of an SPP layer for semantic segmentation is proposed
    in (He et al. [2015](#bib.bib49)), such that the SPP layer is appended to the
    last convolutional layer and fed to the pixel-wise classifier.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的 SPP 层用于语义分割的利用方式在 (He et al. [2015](#bib.bib49)) 中提出，即将 SPP 层附加到最后一个卷积层，并传递给逐像素分类器。
- en: 5.1.3 Feature Concatenation
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 特征连接
- en: This idea is based on fusing features extracted from different sources. For
    example, in (Pinheiro et al. [2015](#bib.bib126)) the so-called ‘DeepMask’ network
    utilises skip connections in a feed-forward manner, so that an architecture partially
    similar to both SPP layer and ED is obtained. The same group extends this idea
    with a top-down refinement approach of the feed-forward module and propose the
    so-called ‘SharpMask’ network (Pinheiro et al. [2016](#bib.bib127)), which has
    proven to be more efficient and accurate in segmentation performance. Another
    approach from this category is the so-called ‘ParseNet’ (Liu et al. [2015](#bib.bib99)),
    which fuses CNN features with external global features from previous layers in
    order to provide context knowledge. Another approach by (Wang et al. [2020](#bib.bib166))
    is to fuse the “stage features” (i.e. deep encoder activations) with “refinement
    path features” (an idea similar to skip connections), using a convolutional (Feature
    Adaptive Fusion FAF) block. Although a novel idea in principle, feature fusion
    approaches (including SPP) create hybrid structures, therefore they are relatively
    difficult to train.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法基于融合来自不同来源的特征。例如，在 (Pinheiro et al. [2015](#bib.bib126)) 中，所谓的“DeepMask”网络以前馈方式利用跳跃连接，从而获得一个与
    SPP 层和 ED 部分相似的架构。同一团队通过前馈模块的自上而下的细化方法扩展了这个想法，提出了所谓的“SharpMask”网络 (Pinheiro et
    al. [2016](#bib.bib127))，其在分割性能上被证明更高效和准确。来自这一类别的另一种方法是所谓的“ParseNet” (Liu et al.
    [2015](#bib.bib99))，它通过将 CNN 特征与来自先前层的外部全局特征融合，以提供上下文知识。另一种方法由 (Wang et al. [2020](#bib.bib166))
    提出，即使用卷积（特征自适应融合 FAF）块将“阶段特征”（即深层编码器激活）与“细化路径特征”（类似于跳跃连接的想法）融合。尽管原则上是一个新颖的想法，但特征融合方法（包括
    SPP）创建了混合结构，因此它们相对难以训练。
- en: 5.1.4 Dilated Convolution
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 膨胀卷积
- en: 'The idea of dilated (atrous) convolutions is actually quite simple: with contiguous
    convolutional filters, an effective receptive field of units can only grow linearly
    with layers; whereas with dilated convolution, which has gaps in the filter (see
    Figure [4](#S5.F4 "Figure 4 ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images").c), the effective receptive
    field would grow much more quickly (Chen et al. [2018](#bib.bib17)). Thus, with
    no pooling or subsampling, a rectangular prism of convolutional layers is created.
    Dilated convolution is a very effective and powerful method for the detailed preservation
    of feature map resolutions. The negative aspect of the technique, compared to
    other techniques, concerns its higher demand for GPU storage and computation,
    since the feature map resolutions do not shrink within the feature hierarchy (He
    et al. [2016](#bib.bib50)).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积（atrous convolutions）的想法实际上很简单：使用连续的卷积滤波器时，有效接收场的单元只能随着层的增加线性增长；而使用膨胀卷积（其滤波器中有间隙，见图
    [4](#S5.F4 "图 4 ‣ 5 Post-FCN 方法 ‣ 基于深度学习的二维图像语义分割架构调查").c)），有效接收场会增长得更快 (Chen
    et al. [2018](#bib.bib17))。因此，无需池化或下采样，就会创建一个卷积层的矩形棱柱。膨胀卷积是一种非常有效且强大的方法，用于详细保留特征图分辨率。与其他技术相比，该技术的负面方面在于对
    GPU 存储和计算的更高需求，因为特征图分辨率在特征层次结构中不会缩小 (He et al. [2016](#bib.bib50))。
- en: 5.1.5 Conditional Random Fields
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.5 条件随机场
- en: As also discussed in Section 3.1.1, CNNs naturally lack mechanisms to specifically
    ‘focus’ on regions where class intersections occur. Around these regions, graphical
    models are used to find inference by observing low-level relations between neighbouring
    feature maps of CNN layers. Consequently, graphical models, mainly CRFs, are utilised
    as refinement layers in deep semantic segmentation architectures. As in (Rother
    et al. [2004](#bib.bib139)), CRFs connect low-level interactions with output from
    multiclass interactions, and in this way, global context knowledge is constructed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 3.1.1 节所讨论的那样，CNN 自然缺乏专门‘聚焦’于类别交集区域的机制。在这些区域周围，图形模型通过观察 CNN 层之间的低级关系来进行推断。因此，图形模型，主要是
    CRF，被用作深度语义分割架构中的细化层。如 (Rother et al. [2004](#bib.bib139)) 所述，CRF 将低级交互与多类交互的输出连接，从而构建了全局上下文知识。
- en: As a refinement layer, various methods exist that employ CRFs to DCNNs, such
    as the Convolutional CRFs (Teichmann and Cipolla [2018](#bib.bib155)), the Dense
    CRF (Krähenbühl and Koltun [2011](#bib.bib73)), and CRN-as-RNN (Zheng et al. [2015](#bib.bib205)).
    In general, CRFs help build context knowledge and thus a finer level of localisation
    in class labels.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为细化层，存在多种方法将 CRF 应用于 DCNN，例如卷积 CRF (Teichmann and Cipolla [2018](#bib.bib155))、稠密
    CRF (Krähenbühl and Koltun [2011](#bib.bib73)) 和 CRN-as-RNN (Zheng et al. [2015](#bib.bib205))。一般而言，CRF
    有助于构建上下文知识，从而在类别标签中实现更精细的定位。
- en: 5.1.6 Recurrent Approaches
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.6 循环方法
- en: The ability of Recurrent Neural Networks (RNNs) to handle sequential information
    can help improve segmentation accuracy. For example, (Pfeuffer et al. [2019](#bib.bib123))
    used Conv-LSTM layers to improve their semantic segmentation results in image
    sequences. However, there are also methods that use recurrent structures on still
    images. For example, the Graph LSTM network (Liang et al. [2016](#bib.bib93))
    is a generalization of LSTM from sequential data or multidimensional data to general
    graph-structured data for semantic segmentation on 2D still images. Graph-RNN
    (Shuai et al. [2016](#bib.bib149)) is another example of a similar approach in
    which an LSTM-based network is used to fuse a deep encoder output with the original
    image in order to obtain a finer pixel-level segmentation. Likewise, in (Lin et al.
    [2018](#bib.bib95)), the researchers utilised LSTM-chains in order to intertwine
    multiple scales, resulting in pixel-wise segmentation improvements. There are
    also hybrid approaches where CNNs and RNNs are fused. A good example of this is
    the so-called ReSeg model (Visin et al. [2016](#bib.bib164)), in which the input
    image is fed to a VGG-like CNN encoder, and is then processed afterwards by recurrent
    layers (namely the ReNet architecture) in order to better localise the pixel labels.
    Another similar approach is the DAG-RNN (Shuai et al. [2016](#bib.bib149)), which
    utilize a DAG-structured CNN+RNN network, and models long-range semantic dependencies
    among image units. To the best of our knowledge, no purely recurrent structures
    for semantic segmentation exist, mainly because semantic segmentation requires
    a preliminary CNN-based feature encoding scheme.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）处理序列信息的能力有助于提高分割精度。例如，(Pfeuffer et al. [2019](#bib.bib123)) 使用Conv-LSTM层来改善图像序列中的语义分割结果。然而，也有一些方法在静态图像上使用递归结构。例如，图卷积LSTM网络（Liang
    et al. [2016](#bib.bib93)）是将LSTM从序列数据或多维数据推广到一般图结构数据，用于二维静态图像的语义分割。Graph-RNN（Shuai
    et al. [2016](#bib.bib149)）是类似方法的另一个例子，其中使用LSTM基础的网络将深层编码器输出与原始图像融合，以获得更精细的像素级分割。同样，在(Lin
    et al. [2018](#bib.bib95))中，研究人员利用LSTM链来交织多个尺度，从而实现像素级分割改进。还有一些混合方法，将CNN和RNN结合在一起。一个很好的例子是所谓的ReSeg模型（Visin
    et al. [2016](#bib.bib164)），该模型将输入图像送入类似VGG的CNN编码器，然后通过递归层（即ReNet架构）进行处理，以更好地定位像素标签。另一个类似的方法是DAG-RNN（Shuai
    et al. [2016](#bib.bib149)），它利用DAG结构的CNN+RNN网络，建模图像单元之间的长程语义依赖。根据我们的了解，目前还不存在纯递归结构的语义分割方法，主要因为语义分割需要一个初步的基于CNN的特征编码方案。
- en: There is currently an increasing trend in one specific type of RNN, namely ‘recurrent
    attention modules’. In these modules, attention (Vaswani et al. [2017](#bib.bib162))
    is technically fused in the RNN, providing a focus on certain regions of the input
    when predicting a certain part of the output sequence. Consequently, they are
    also being utilised in semantic segmentation (Li et al. [2019](#bib.bib90); Zhao
    et al. [2018](#bib.bib202); Oktay et al. [2018](#bib.bib114)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，一种特定类型的递归神经网络（RNN）——即“递归注意力模块”——正呈现出日益增长的趋势。在这些模块中，注意力（Vaswani et al. [2017](#bib.bib162)）在RNN中技术性地融合，实现了在预测输出序列的某一部分时，关注输入的特定区域。因此，它们也被用于语义分割（Li
    et al. [2019](#bib.bib90); Zhao et al. [2018](#bib.bib202); Oktay et al. [2018](#bib.bib114)）。
- en: '![Refer to caption](img/bb1ea7a9b7f79430e42fe0bbe826eeae.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bb1ea7a9b7f79430e42fe0bbe826eeae.png)'
- en: (a) Regions with CNN features-based (Mask-RCNN) architecture
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于CNN特征的（Mask-RCNN）架构
- en: '![Refer to caption](img/c1602b0eb6252374ecad0aa89559086a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c1602b0eb6252374ecad0aa89559086a.png)'
- en: (b) Fully Convolutional Object Detector-based (YOLO)-based architecture
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于完全卷积对象检测器（YOLO）的架构
- en: 'Figure 5: Different architectures for object detection-based semantic segmentation
    methods'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于对象检测的语义分割方法的不同架构
- en: 5.2 Scale-Invariance
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 尺度不变性
- en: Scale Invariance is, by definition, the ability of a method to process a given
    input, independent of the relative scale (i.e. the scale of an object to its scene)
    or image resolution. Although it is extremely crucial for certain applications,
    this ability is usually overlooked or is confused with a method’s ability to include
    multiscale information. A method may use multiscale information to improve its
    pixel-wise segmentation ability, but can still be dependent on scale or resolution.
    That is why we find it necessary to discuss this issue under a different title
    and to provide information on the techniques that provide scale and/or resolution
    invariance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度不变性定义为方法处理给定输入的能力，无论相对尺度（即对象与其场景的尺度）或图像分辨率如何。尽管这对于某些应用极其重要，但这种能力通常被忽视或与方法包含多尺度信息的能力混淆。一个方法可能使用多尺度信息来改善其像素级分割能力，但仍可能依赖于尺度或分辨率。这就是为什么我们认为有必要在不同的标题下讨论这个问题，并提供有关提供尺度和/或分辨率不变性的技术的信息。
- en: In computer vision, any method can become scale invariant if trained with multiple
    scales of the training set. Some semantic segmentation methods such as (Farabet
    et al. [2013](#bib.bib35); Eigen and Fergus [2014](#bib.bib31); Pinheiro and Collobert
    [2014](#bib.bib124); Lin et al. [2016](#bib.bib96); Yu and Koltun [2015](#bib.bib190))
    utilise this strategy. However, these methods do not possess an inherent scale-invariance
    property, which is usually obtained by normalisation with a global scale factor
    (such as in SIFT by (Lowe [2004](#bib.bib104))). This approach is not usually
    preferred in the literature on semantic segmentation. The image sets that exist
    in semantic segmentation literature are extremely large in size. Thus, the methods
    are trained to memorise that training set, because in principle, overfitting a
    largescale training set is actually tantamount to solving the entire problem space.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，任何方法都可以通过使用多尺度的训练集来实现尺度不变性。一些语义分割方法如(Farabet et al. [2013](#bib.bib35);
    Eigen 和 Fergus [2014](#bib.bib31); Pinheiro 和 Collobert [2014](#bib.bib124); Lin
    et al. [2016](#bib.bib96); Yu 和 Koltun [2015](#bib.bib190)) 采用了这种策略。然而，这些方法并不具备固有的尺度不变性属性，这通常通过使用全局尺度因子进行归一化（如在SIFT中(Lowe
    [2004](#bib.bib104)))。这种方法在语义分割文献中通常不被优先考虑。存在于语义分割文献中的图像集极其庞大。因此，这些方法被训练来记忆这些训练集，因为原则上，过拟合一个大规模训练集实际上相当于解决整个问题空间。
- en: 5.3 Object Detection-based Methods
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于对象检测的方法
- en: There has been a recent growing trend in computer vision, which aims at specifically
    resolving the problem of object detection, that is, establishing a bounding box
    around all objects within an image. Given that the image may or may not contain
    any number of objects, the architectures utilised to tackle such a problem differ
    to the existing fully-connected/convolutional classification or segmentation models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，计算机视觉领域出现了一个日益增长的趋势，专门解决对象检测的问题，即在图像中的所有对象周围建立一个边界框。考虑到图像中可能包含或不包含任意数量的对象，用于处理这种问题的架构与现有的全连接/卷积分类或分割模型不同。
- en: '![Refer to caption](img/27c76e945c4f8da6464d2c40eec10245.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/27c76e945c4f8da6464d2c40eec10245.png)'
- en: (a) image
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 图像
- en: '![Refer to caption](img/c87c59642172e095519349471ee0036c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c87c59642172e095519349471ee0036c.png)'
- en: (b) reference
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 参考
- en: '![Refer to caption](img/bc9fc884b3a09df3123cd736bc5434a7.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc9fc884b3a09df3123cd736bc5434a7.png)'
- en: (c) FCN-32S
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (c) FCN-32S
- en: '![Refer to caption](img/eba7b34561b5680f27e8736f58e7037b.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eba7b34561b5680f27e8736f58e7037b.png)'
- en: (d) FCN-8S
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: (d) FCN-8S
- en: '![Refer to caption](img/f82fc5d19719873916352e6fbceaf239.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f82fc5d19719873916352e6fbceaf239.png)'
- en: (e) CMSA
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: (e) CMSA
- en: '![Refer to caption](img/724899ee55cec8d26f93a6b9e95cc480.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/724899ee55cec8d26f93a6b9e95cc480.png)'
- en: (f) DeepLabv1
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: (f) DeepLabv1
- en: '![Refer to caption](img/1eb07e79f0b7c2bd7404db2bd1923f1c.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1eb07e79f0b7c2bd7404db2bd1923f1c.png)'
- en: (g) CRF-as-RNN
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (g) CRF-as-RNN
- en: '![Refer to caption](img/42e2f2696458767611b0ee5a29df3d60.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42e2f2696458767611b0ee5a29df3d60.png)'
- en: (h) DeepLab.v2
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (h) DeepLab.v2
- en: '![Refer to caption](img/dc8056f4cda54dcd25d9a48af0c376c2.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc8056f4cda54dcd25d9a48af0c376c2.png)'
- en: (i) DeepLab.v2+CRF
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (i) DeepLab.v2+CRF
- en: '![Refer to caption](img/e65188054143c18107777e262d9b21d2.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e65188054143c18107777e262d9b21d2.png)'
- en: (j) PAN
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (j) PAN
- en: 'Figure 6: (a) A sample image from the PASCAL VOC validation set, (b) its semantic
    segmentation ground truth, and results obtained from different studies are depicted:
    c) FCN-32S (Shelhamer et al. [2017](#bib.bib143)), d) FCN-8S (Shelhamer et al.
    [2017](#bib.bib143)), e) CMSA (Eigen and Fergus [2014](#bib.bib31)), f) DeepLab-v1
    (Chen et al. [2014](#bib.bib16)), g) CRF-as-RNN (Zheng et al. [2015](#bib.bib205)),
    h) DeepLab-v2 (Chen et al. [2018](#bib.bib17)), i) DeepLab-v2 with CRF refinement
    (Chen et al. [2018](#bib.bib17)), j) PAN (Li et al. [2018](#bib.bib87)).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：(a) PASCAL VOC 验证集中的一张样本图像，(b) 其语义分割的真实标注，以及不同研究所得的结果展示：c) FCN-32S（Shelhamer
    等人 [2017](#bib.bib143)），d) FCN-8S（Shelhamer 等人 [2017](#bib.bib143)），e) CMSA（Eigen
    和 Fergus [2014](#bib.bib31)），f) DeepLab-v1（Chen 等人 [2014](#bib.bib16)），g) CRF-as-RNN（Zheng
    等人 [2015](#bib.bib205)），h) DeepLab-v2（Chen 等人 [2018](#bib.bib17)），i) DeepLab-v2
    结合 CRF 细化（Chen 等人 [2018](#bib.bib17)），j) PAN（Li 等人 [2018](#bib.bib87)）。
- en: The pioneering study that represents this idea is the renowned ‘Regions with
    CNN features’ (RCNN) network (Girshick et al. [2013](#bib.bib44)). Standard CNNs
    with fully convolutional and fully connected layers lack the ability to provide
    varying length output, which is a major flaw for an object detection algorithm
    that aims to detect an unknown number of objects within an image. The simplest
    way to resolve this problem is to take different regions of interest from the
    image, and then to employ a CNN in order to detect objects within each region
    separately. This region selection architecture is called the ‘Region Proposal
    Network’ (RPN) and is the fundamental structure used to construct the RCNN network
    (see Figure [5](#S5.F5 "Figure 5 ‣ 5.1.6 Recurrent Approaches ‣ 5.1 Techniques
    for Fine-grained Localisation ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images").a). Improved versions of
    RCNN, namely ‘Fast-RCNN’ (Girshick et al. [2013](#bib.bib44)) and ‘Faster-RCNN’
    (Ren et al. [2015](#bib.bib132)) were subsequently also proposed by the same research
    group. Because these networks allow for the separate detection of all objects
    within the image, the idea was easily implemented for instance segmentation, as
    the ‘Mask-RCNN’ (He et al. [2017](#bib.bib48)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 代表这一思想的开创性研究是著名的“具有 CNN 特征的区域”（RCNN）网络（Girshick 等人 [2013](#bib.bib44)）。标准的 CNN
    由于其完全卷积和完全连接的层结构，缺乏提供可变长度输出的能力，这对于旨在检测图像中未知数量对象的目标检测算法来说是一个主要缺陷。解决这一问题的最简单方法是从图像中提取不同的感兴趣区域，然后使用
    CNN 分别检测每个区域中的对象。这种区域选择结构被称为“区域提议网络”（RPN），是构建 RCNN 网络的基本结构（见图 [5](#S5.F5 "图 5
    ‣ 5.1.6 循环方法 ‣ 5.1 精细定位技术 ‣ 5 FCN 后的方法 ‣ 关于 2D 图像语义分割的深度学习架构综述").a)。RCNN 的改进版本，即“Fast-RCNN”（Girshick
    等人 [2013](#bib.bib44)）和“Faster-RCNN”（Ren 等人 [2015](#bib.bib132)），随后也由同一研究小组提出。由于这些网络允许图像中所有对象的独立检测，该思想也很容易被应用于实例分割，例如“Mask-RCNN”（He
    等人 [2017](#bib.bib48)）。
- en: The basic structure of RCNNs included the RPN, which is the combination of CNN
    layers and a fully connected structure in order to decide the object categories
    and bounding box positions. As discussed within the previous sections of this
    paper, due to their cumbersome structure, fully connected layers were largely
    abandoned with FCNs. RCNNs shared a similar fate when the ‘You-Only-Look-Once’
    (YOLO) by (Redmon et al. [2016](#bib.bib131)) and ‘Single Shot Detector’ (SSD)
    by (Liu et al. [2016](#bib.bib98)) were proposed. YOLO utilises a single convolutional
    network that predicts the bounding boxes and the class probabilities for these
    boxes. It consists of no fully connected layers, and consequently provides real-time
    performance. SSD proposed a similar idea, in which bounding boxes were predicted
    after multiple convolutional layers. Since each convolutional layer operates at
    a different scale, the architecture is able to detect objects of various scales.
    Whilst slower than YOLO, it is still considered to be faster then RCNNs. This
    new breed of object detection techniques was immediately applied to semantic segmentation.
    Similar to MaskRCNN, ‘Mask-YOLO’ (Sun [2019](#bib.bib152)) and ‘YOLACT’ (Bolya
    et al. [2019](#bib.bib9)) architectures were implementations of these object detectors
    to the problem of instance segmentation (see Figure [5](#S5.F5 "Figure 5 ‣ 5.1.6
    Recurrent Approaches ‣ 5.1 Techniques for Fine-grained Localisation ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images")b). Similar to YOLACT, some other methods also achieve fast, real-time
    instance segmentation such as; ESE-Seg (Xu et al. [2019](#bib.bib183)), SOLO(Wang
    et al. [2019](#bib.bib170)), SOLOv2(Wang et al. [2020](#bib.bib171)), DeepSnake
    (Peng et al. [2020](#bib.bib121)), and CenterPoly(Perreault et al. [2021](#bib.bib122)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: RCNN 的基本结构包括 RPN，它是 CNN 层和一个全连接结构的组合，用于确定对象类别和边界框位置。如本文前面部分所讨论，由于其繁琐的结构，全连接层在
    FCN 中被大幅度抛弃。RCNN 也遭遇了类似的命运，当‘你只看一次’（YOLO）（Redmon et al. [2016](#bib.bib131)）和‘单次检测器’（SSD）（Liu
    et al. [2016](#bib.bib98)）被提出时。YOLO 利用一个单一的卷积网络来预测这些框的边界框和类别概率。它不包含全连接层，因此提供了实时性能。SSD
    提出了类似的理念，在多个卷积层后预测边界框。由于每个卷积层在不同的尺度上操作，这种架构能够检测各种尺度的对象。尽管比 YOLO 慢，但仍被认为比 RCNN
    快。这种新型的目标检测技术立即被应用于语义分割。类似于 MaskRCNN，‘Mask-YOLO’（Sun [2019](#bib.bib152)）和‘YOLACT’（Bolya
    et al. [2019](#bib.bib9)）架构是这些目标检测器在实例分割问题上的实现（见图 [5](#S5.F5 "Figure 5 ‣ 5.1.6
    Recurrent Approaches ‣ 5.1 Techniques for Fine-grained Localisation ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images")b）。类似于 YOLACT，一些其他方法也实现了快速的实时实例分割，如；ESE-Seg（Xu et al. [2019](#bib.bib183)），SOLO（Wang
    et al. [2019](#bib.bib170)），SOLOv2（Wang et al. [2020](#bib.bib171)），DeepSnake（Peng
    et al. [2020](#bib.bib121)）和 CenterPoly（Perreault et al. [2021](#bib.bib122)）。
- en: Locating objects within an image prior to segmenting them at the pixel level
    is both intuitive and natural, due to the fact that it is effectively how the
    human brain supposedly accomplishes this task (Rosenholtz [2016](#bib.bib138)).
    In addition to these “two-stage (detection+segmentation) methods, there are some
    recent studies that aim at utilizing the segmentation task to be incorporated
    into one-stage bounding-box detectors and result in a simple yet efficient instance
    segmentation framework (Xu et al. [2019](#bib.bib184); R. Zhang et al. [2020](#bib.bib129);
    Lee and Park [2020](#bib.bib85); Xie et al. [2020](#bib.bib178)). However, the
    latest trend is to use global-area-based methods by generating intermediate FCN
    feature maps and then assembling these basis features to obtain final masks (Chen
    et al. [2020](#bib.bib15); Kim et al. [2021](#bib.bib70); Ke et al. [2021](#bib.bib67)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在对图像进行像素级分割之前定位对象是直观且自然的，因为这实际上是人类大脑完成这一任务的方式（Rosenholtz [2016](#bib.bib138)）。除了这些“两阶段（检测+分割）”方法外，还有一些最新的研究旨在将分割任务纳入到单阶段边界框检测器中，从而形成一种简单而高效的实例分割框架（Xu
    et al. [2019](#bib.bib184); R. Zhang et al. [2020](#bib.bib129); Lee and Park
    [2020](#bib.bib85); Xie et al. [2020](#bib.bib178)）。然而，最新的趋势是使用基于全局区域的方法，通过生成中间
    FCN 特征图，然后将这些基础特征组合以获得最终的掩膜（Chen et al. [2020](#bib.bib15); Kim et al. [2021](#bib.bib70);
    Ke et al. [2021](#bib.bib67)）。
- en: In recent years, a trend of alleviating the demand for pixel-wise labels is
    realized mainly by employing bounding boxes, and by expanding from semantic segmentation
    to instance segmentation applications. In both semantic segmentation and instance
    segmentation methods, the category of each pixel is recognized, and the only difference
    is that instance segmentation also differentiates object occurrences of the same
    category. Therefore, weakly-supervised instance segmentation (WSIS) methods are
    also utilized for instance segmentation. The supervision of WSIS methods can use
    different annotation types for training, which are usually in the form of either
    bounding boxes (Khoreva et al. [2017](#bib.bib69); Hsu et al. [2019](#bib.bib55);
    Arun et al. [2020](#bib.bib5); Tian et al. [2021](#bib.bib157); Lee et al. [2021](#bib.bib84);
    Cheng et al. [2021](#bib.bib21)) or image-level labels (Liu et al. [2020](#bib.bib100);
    Shen et al. [2021](#bib.bib145); Zhou et al. [2016](#bib.bib209); Shen et al.
    [2021](#bib.bib144)). Hence, employing object detection-based methods for semantic
    segmentation is an area significantly prone to further development in near future
    by the time this manuscript is prepared.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，通过使用边界框来减轻像素级标签的需求，并从语义分割扩展到实例分割应用已成为一种趋势。在语义分割和实例分割方法中，识别每个像素的类别，唯一的区别是实例分割还区分了同一类别的对象出现。因此，弱监督实例分割（WSIS）方法也被用于实例分割。WSIS方法的监督可以使用不同类型的标注进行训练，这些标注通常以边界框（Khoreva
    et al. [2017](#bib.bib69); Hsu et al. [2019](#bib.bib55); Arun et al. [2020](#bib.bib5);
    Tian et al. [2021](#bib.bib157); Lee et al. [2021](#bib.bib84); Cheng et al. [2021](#bib.bib21)）或图像级标签（Liu
    et al. [2020](#bib.bib100); Shen et al. [2021](#bib.bib145); Zhou et al. [2016](#bib.bib209);
    Shen et al. [2021](#bib.bib144)）的形式存在。因此，基于目标检测的方法在语义分割中的应用是一个在本手稿准备时具有显著发展潜力的领域。
- en: '| Method | Method Summary | Rankings | Eff. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 方法总结 | 排名 | 效率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MultiScale-Net. (Farabet et al. [2013](#bib.bib35)) | *Multiscale convolutional
    network fused parallel with a segmentation framework (either superpixel or CRF-based).
    Relatively lower computational efficiency due to a CRF block.* | 68.7% mPA @SIFTflow
    | $\star$ $\star$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| MultiScale-Net. (Farabet et al. [2013](#bib.bib35)) | *多尺度卷积网络与分割框架（超像素或基于CRF）并行融合。由于CRF块，相对较低的计算效率。*
    | 68.7% mPA @SIFTflow | $\star$ $\star$ |'
- en: '| Recurrent CNN (Pinheiro and Collobert [2014](#bib.bib124)) | *Recurrent architecture
    constructed by using different instances of a CNN, in which each network instance
    is fed with previous label predictions (obtained from the previous instance).
    Heavy computational load when multiple instances (3 in their best performing experiments)
    are fed.* | 77.7% mPA @SIFTflow | $\star$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 循环CNN (Pinheiro and Collobert [2014](#bib.bib124)) | *通过使用不同实例的CNN构建的递归架构，其中每个网络实例都输入之前的标签预测（从之前的实例获得）。当输入多个实例（在其最佳实验中为3个）时，计算负担较重。*
    | 77.7% mPA @SIFTflow | $\star$ |'
- en: '| FCN (Shelhamer et al. [2017](#bib.bib143))  | *Fully convolutional encoder
    structure (i.e., no fully connected layers) with skip connections that fuse multiscale
    activations at the final decision layer. Relative fast due to no fully connected
    layers or a refinement block.* | 85.2% mPA @SIFTflow 62.2% mIoU @PASCAL 2012'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '| FCN (Shelhamer et al. [2017](#bib.bib143))  | *全卷积编码结构（即没有全连接层），带有跳跃连接，在最终决策层融合多尺度激活。由于没有全连接层或细化块，相对较快。*
    | 85.2% mPA @SIFTflow 62.2% mIoU @PASCAL 2012'
- en: 65.3% mIoU @CitySca. (w/o course)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 65.3% mIoU @CitySca.（不含课程）
- en: 39.3% mIoU @ADE20K | $\star$ $\star$ $\star$ |
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 39.3% mIoU @ADE20K | $\star$ $\star$ $\star$ |
- en: '| DeepLab.v1 (Chen et al. [2014](#bib.bib16)) | *CNN with dilated convolutions,
    succeeded by a fully-connected (i.e. Dense) CRF.* Fast and optimized computation
    leads to near real-time performance. | 66.4% mIoU @PASCAL 2012 | $\star$ $\star$
    $\star$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| DeepLab.v1 (Chen et al. [2014](#bib.bib16)) | *带有膨胀卷积的CNN，后跟一个全连接（即Dense）CRF。快速且优化的计算导致接近实时的性能。*
    | 66.4% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
- en: '| CMSA (Eigen and Fergus [2014](#bib.bib31))  | *Layers of a pyramidal input
    are fed to separate FCNs for different scales in parallel. These multiscale FCNs
    are also connected in series to provide pixel-wise category, depth and normal
    output, simultaneously. Relatively lower computational efficiency due to progressive
    processing of sequence of different scales*. | 83.8% mPA @SIFTflow 62.6% mIoU
    @PASCAL 2012 | $\star$ $\star$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| CMSA（Eigen 和 Fergus [2014](#bib.bib31)） | *将金字塔输入的层馈送到并行的不同尺度的独立 FCNs。这些多尺度
    FCNs 还串联连接以同时提供像素级别的类别、深度和法线输出。由于逐步处理不同尺度的序列，计算效率相对较低。* | 83.8% mPA @SIFTflow
    62.6% mIoU @PASCAL 2012 | $\star$ $\star$ |'
- en: '| UNet (Ronneberger et al. [2015](#bib.bib134)) | *Encoder/decoder structure
    with skip connections that connect same levels of ED and final input-sized classification
    layer. Efficient computation load due to no fully connected layers or a refinement
    block.* | 72.7% mIoU @PASCAL 2012 (*tested by (Zhang et al. [2018](#bib.bib199)))*
    | $\star$ $\star$ $\star$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| UNet（Ronneberger 等人 [2015](#bib.bib134)） | *具有跳跃连接的编码器/解码器结构，连接相同级别的 ED 和最终输入大小的分类层。由于没有全连接层或精细化模块，计算负载高效。*
    | 72.7% mIoU @PASCAL 2012（*由 Zhang 等人 [2018](#bib.bib199) 测试*） | $\star$ $\star$
    $\star$ |'
- en: '| SegNet (Badrinarayanan et al. [2015](#bib.bib6))  | *Encoder/decoder structure
    (similar to UNet) with skip connections that transmit only pooling indices (unlike
    U-Net, for which skip connections concatenate same-level activations. Efficient
    computation load due to no fully connected layers or a refinement block).* | 59.9%
    mIoU @PASCAL 2012 79.2% mIoU @CitySca. (w/o course) | $\star$ $\star$ $\star$
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SegNet（Badrinarayanan 等人 [2015](#bib.bib6)） | *编码器/解码器结构（类似于 UNet）具有仅传递池化索引的跳跃连接（不同于
    U-Net，其中跳跃连接拼接相同级别的激活）。由于没有全连接层或精细化模块，计算负载高效。* | 59.9% mIoU @PASCAL 2012 79.2%
    mIoU @CitySca.（不含课程） | $\star$ $\star$ $\star$ |'
- en: '| DeconvNet (Noh et al. [2015](#bib.bib113))  | *Encoder/decoder structure
    (namely ‘the Conv./Deconv. Network’) without skip connections. The encoder (convolutional)
    part of the network is transferred from the VGG-VD-16L. Efficient computation
    load due to no fully connected layers or a refinement block.* (Simonyan and Zisserman
    [2015](#bib.bib151)). | 74.8% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DeconvNet（Noh 等人 [2015](#bib.bib113)） | *编码器/解码器结构（即‘卷积/反卷积网络’）没有跳跃连接。网络的编码器（卷积）部分转移自
    VGG-VD-16L。由于没有全连接层或精细化模块，计算负载高效。*（Simonyan 和 Zisserman [2015](#bib.bib151)）。
    | 74.8% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
- en: '| MSCG (Yu and Koltun [2015](#bib.bib190))  | *Multiscale context aggregation
    using only a rectangular prism of dilated convolutional layers, without pooling
    or subsampling layers, to perform pixel-wise labelling. Efficient computation
    load due to no fully connected layers or a refinement block.* | 67.6% mIoU @PASCAL
    2012 67.1% mIoU @CitySca. (w/o course) | $\star$ $\star$ $\star$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| MSCG（Yu 和 Koltun [2015](#bib.bib190)） | *使用仅一个稀疏卷积层的矩形棱柱进行多尺度上下文聚合，未使用池化或下采样层，进行像素级标注。由于没有全连接层或精细化模块，计算负载高效。*
    | 67.6% mIoU @PASCAL 2012 67.1% mIoU @CitySca.（不含课程） | $\star$ $\star$ $\star$
    |'
- en: '| CRF-as-RNN (Zheng et al. [2015](#bib.bib205))  | *Fully convolutional CNN
    (i.e., FCN) followed by a CRF-as-RNN layer, in which an iterative CRF algorithm
    is formulated as an RNN. Because of the RNN block, computational efficiency is
    limited.* | 65.2% mIoU @PASCAL 2012 62.5% mIoU @CitySca. (w/o course) | $\star$ $\star$
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| CRF-as-RNN（郑等人 [2015](#bib.bib205)） | *全卷积 CNN（即 FCN）后接 CRF-as-RNN 层，其中迭代
    CRF 算法被表述为 RNN。由于 RNN 模块，计算效率有限。* | 65.2% mIoU @PASCAL 2012 62.5% mIoU @CitySca.（不含课程）
    | $\star$ $\star$ |'
- en: '| FeatMap-Net. (Lin et al. [2016](#bib.bib96))  | *Layers of a pyramidal input
    fed to parallel multiscale feature maps (i.e., CNNS), and later fused in an upsample/concatenation
    (i.e. pyramid pooling) layer to provide the final feature map to a Dense CRF Layer.
    Well-planned but loaded architecture leads to moderate computational efficiency.*
    | 88.1% mPA @SIFTflow 75.3% mIoU @PASCAL 2012 | $\star$ $\star$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| FeatMap-Net（Lin 等人 [2016](#bib.bib96)） | *将金字塔输入的层馈送到并行的多尺度特征图（即 CNNS），然后在上采样/拼接（即金字塔池化）层中融合，以提供最终特征图给
    Dense CRF 层。结构设计良好但负载较重，计算效率中等。* | 88.1% mPA @SIFTflow 75.3% mIoU @PASCAL 2012
    | $\star$ $\star$ |'
- en: '| Graph LSTM (Liang et al. [2016](#bib.bib93))  | *Generalization of LSTM from
    sequential data to general graph-structured data for semantic segmentation on
    2D still images, mostly people/parts. Graph-LSTM processing considerably limits
    computation efficiency.* | 60.2% mIoU @PASCAL Person/Parts 2010 | $\star$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Graph LSTM (Liang et al. [2016](#bib.bib93))  | *LSTM从顺序数据到一般图结构数据的泛化，用于2D静态图像上的语义分割，主要是人/部件。Graph-LSTM处理显著限制计算效率。*
    | 60.2% mIoU @PASCAL Person/Parts 2010 | $\star$ |'
- en: '| DAG-RNN (Shuai et al. [2016](#bib.bib149))  | *DAG-structured CNN+RNN network
    that models long-range semantic dependencies among image units. Due to chain structured
    sequential processing of pixels with a recurrent model, the computational efficiency
    is considerably limited.* | 85.3% mPA @SIFTflow | $\star$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| DAG-RNN (Shuai et al. [2016](#bib.bib149))  | *DAG结构的CNN+RNN网络，用于建模图像单元之间的长程语义依赖。由于使用递归模型对像素进行链式结构的顺序处理，计算效率受到显著限制。*
    | 85.3% mPA @SIFTflow | $\star$ |'
- en: '| cont’d. |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| cont’d. |  |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DeepLab.v2 (Chen et al. [2018](#bib.bib17))  | *Improved version of DeepLab.v1,
    with additional ‘dilated (atrous) spatial pyramid pooling’ (ASPP) layer. Similar
    computational performance to DeepLab.v1.* | 79.7% mIoU @PASCAL 2012 70.4% mIoU
    @CitySca. (w/o course) | $\star$ $\star$ $\star$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| DeepLab.v2 (Chen et al. [2018](#bib.bib17))  | *DeepLab.v1的改进版本，增加了“膨胀（空洞）空间金字塔池化”（ASPP）层。计算性能与DeepLab.v1相似。*
    | 79.7% mIoU @PASCAL 2012 70.4% mIoU @CitySca.（无课程） | $\star$ $\star$ $\star$
    |'
- en: '| PSPNet (Zhao et al. [2017](#bib.bib201))  | *CNN followed by a pyramid pooling
    layer similar to (He et al. [2015](#bib.bib49)), but without a fully connected
    decision layer. Hence, computational performance closer to FCN (Shelhamer et al.
    [2017](#bib.bib143)).* | 85.5% mIoU @PASCAL 2012 81.2% mIoU @CitySca. (w. course)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '| PSPNet (Zhao et al. [2017](#bib.bib201))  | *CNN后接一个类似于（He et al. [2015](#bib.bib49)）的金字塔池化层，但没有完全连接的决策层。因此，计算性能更接近于FCN（Shelhamer
    et al. [2017](#bib.bib143)）。* | 85.5% mIoU @PASCAL 2012 81.2% mIoU @CitySca.（有课程）'
- en: 55.4% mIoU @ADE20K | $\star$ $\star$ $\star$ |
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 55.4% mIoU @ADE20K | $\star$ $\star$ $\star$ |
- en: '| DeepLab.v3 (Chen et al. [2017](#bib.bib18)) | *Improved version of DeepLab.v2,
    with optimisation of ASPP layer hyperparameters and without a Dense CRF layer,
    for faster operation.* | 85.7% mIoU @PASCAL 2012 81.3% mIoU @CitySca. (w. course)
    | $\star$ $\star$ $\star$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| DeepLab.v3 (Chen et al. [2017](#bib.bib18)) | *DeepLab.v2的改进版本，优化了ASPP层的超参数，并且没有Dense
    CRF层，以实现更快的操作。* | 85.7% mIoU @PASCAL 2012 81.3% mIoU @CitySca.（有课程） | $\star$
    $\star$ $\star$ |'
- en: '| DIS (Luo et al. [2017](#bib.bib106))  | *One network predicts labelmaps/tags,
    while another performs semantic segmentation using these predictions. Both networks
    use ResNet101 (He et al. [2016](#bib.bib50)) for preliminary feature extraction.
    They declare similar computational efficiency to DeepLabv2 (Chen et al. [2018](#bib.bib17))*
    | 41.7% mIoU @COCO 86.8% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| DIS (Luo et al. [2017](#bib.bib106))  | *一个网络预测标签图/标记，另一个网络使用这些预测进行语义分割。两个网络都使用ResNet101（He
    et al. [2016](#bib.bib50)）进行初步特征提取。它们的计算效率与DeepLabv2（Chen et al. [2018](#bib.bib17)）相似*
    | 41.7% mIoU @COCO 86.8% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
- en: '| Mask-RCNN (He et al. [2017](#bib.bib48))  | *Object Detector Fast-RCNN followed
    by ROI-pooling and Convolutional layers, applied to instance segmentation, with
    near real-time performance (see Figure [5](#S5.F5 "Figure 5 ‣ 5.1.6 Recurrent
    Approaches ‣ 5.1 Techniques for Fine-grained Localisation ‣ 5 Post-FCN Approaches
    ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D
    images").a).* | 37.1% mIoU @COCO *tested by* (Bolya et al. [2019](#bib.bib9))
    | $\star$ $\star$ $\star$ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Mask-RCNN (He et al. [2017](#bib.bib48))  | *对象检测器Fast-RCNN后接ROI池化和卷积层，应用于实例分割，具有近实时性能（参见图
    [5](#S5.F5 "Figure 5 ‣ 5.1.6 Recurrent Approaches ‣ 5.1 Techniques for Fine-grained
    Localisation ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based Architectures
    for Semantic Segmentation on 2D images").a）。* | 37.1% mIoU @COCO *经（Bolya et al.
    [2019](#bib.bib9)）测试* | $\star$ $\star$ $\star$ |'
- en: '| GCN (Peng et al. [2017](#bib.bib120))  | *Fed by an initial ResNet-based
    (He et al. [2016](#bib.bib50)) encoder, GCN uses large kernels to fuse high- and
    low-level features in a multiscale manner, followed by a convolutional Border
    Refinement (BR) module. Its fully convolutional architectue allows near real-time
    performance.* | 83.6% mIoU @PASCAL 2012 76.9% mIoU @CitySca. (w/o course) | $\star$
    $\star$ $\star$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| GCN (Peng et al. [2017](#bib.bib120))  | *由初始的基于ResNet（He et al. [2016](#bib.bib50)）编码器提供支持，GCN使用大卷积核以多尺度方式融合高层次和低层次特征，随后是卷积边界细化（BR）模块。其完全卷积的架构允许近实时性能。*
    | 83.6% mIoU @PASCAL 2012 76.9% mIoU @CitySca.（无课程） | $\star$ $\star$ $\star$
    |'
- en: '| SDN (Fu et al. [2017](#bib.bib38))  | *UNET architecture that consists of
    multiple shallow deconvolutional networks, called SDN units, stacked one by one
    to integrate contextual information and guarantee fine recovery of localised information.
    Computational efficiency similar to UNET-like architectures.* | 83.5% mIoU @PASCAL
    2012 | $\star$ $\star$ $\star$ |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| SDN (Fu et al. [2017](#bib.bib38))  | *UNET架构，由多个浅层反卷积网络（称为SDN单元）逐个堆叠，以整合上下文信息并保证局部信息的精细恢复。计算效率类似于UNET类架构。*
    | 83.5% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
- en: '| DFN (Yu et al. [2018](#bib.bib189))  | *Consists of two sub-networks: Smooth
    Net (SN) and Border Net (BN). SN utilises an attention module and handles global
    context, whereas BN employs a refinement block to handle borders. Limited computational
    efficiency due to an attention block*. | 86.2% mIoU @PASCAL 2012 80.3% mIoU @CitySca.
    (w.course) | $\star$ $\star$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| DFN (Yu et al. [2018](#bib.bib189))  | *由两个子网络组成：Smooth Net (SN) 和 Border
    Net (BN)。SN利用注意力模块处理全局上下文，而BN使用细化模块处理边界。由于一个注意力模块导致计算效率有限。* | 86.2% mIoU @PASCAL
    2012 80.3% mIoU @CitySca. (w.course) | $\star$ $\star$ |'
- en: '| MSCI (Lin et al. [2018](#bib.bib95))  | *Aggregates features from different
    scales via connections between Long Short-term Memory (LSTM) chains. Limited computational
    efficiency due to multiple RNN blocks (i.e. LSTMs).* | 88.0% mIoU @PASCAL 2012
    | $\star$ $\star$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| MSCI (Lin et al. [2018](#bib.bib95))  | *通过长短期记忆（LSTM）链之间的连接聚合来自不同尺度的特征。由于多个RNN块（即LSTM）导致计算效率有限。*
    | 88.0% mIoU @PASCAL 2012 | $\star$ $\star$ |'
- en: '| DeepLab.v3+ (Chen et al. [2018](#bib.bib19))  | *Improved version of DeepLab.v3,
    using special encoder-decoder structure with dilated convolutions (with no Dense
    CRF employed for faster operation).* | 87.3% mIoU @PASCAL 2012 82.1% mIoU @CitySca.
    (w. course) | $\star$ $\star$ $\star$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| DeepLab.v3+ (Chen et al. [2018](#bib.bib19))  | *DeepLab.v3的改进版，使用特殊的编码器-解码器结构，并采用扩张卷积（未使用Dense
    CRF以提高操作速度）。* | 87.3% mIoU @PASCAL 2012 82.1% mIoU @CitySca. (w. course) | $\star$
    $\star$ $\star$ |'
- en: '| HPN (Shi et al. [2018](#bib.bib146))  | *Followed by a convolutional ‘Appearance
    Feature Encoder’, a ‘Contextual Feature Encoder’ consisting of LSTMs generates
    super-pixel features fed to a Softmax-based classification layer. Limited computational
    efficiency due to multiple LSTMs.* | 85.8% mIoU @PASCAL 2012 92.3% mPA @SIFTflow
    | $\star$ $\star$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| HPN (Shi et al. [2018](#bib.bib146))  | *紧随其后的是一个卷积‘外观特征编码器’，由LSTM组成的‘上下文特征编码器’生成超像素特征，并输入到基于Softmax的分类层。由于多个LSTM导致计算效率有限。*
    | 85.8% mIoU @PASCAL 2012 92.3% mPA @SIFTflow | $\star$ $\star$ |'
- en: '| EncNet (Zhang et al. [2018](#bib.bib195))  | *Fully connected structure to
    extract context is fed by dense feature maps (obtained from ResNet (He et al.
    [2016](#bib.bib50))) and followed by a convolutional prediction layer. Fully connected
    layers within their “Context Encoding Module” limits computational performance.*
    | 85.9% mIoU @PASCAL 2012 55.7% mIoU @ADE20K | $\star$ $\star$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| EncNet (Zhang et al. [2018](#bib.bib195))  | *完全连接结构提取的上下文由密集特征图（从ResNet
    (He et al. [2016](#bib.bib50))获得）提供，并随后由一个卷积预测层处理。其“上下文编码模块”中的完全连接层限制了计算性能。* |
    85.9% mIoU @PASCAL 2012 55.7% mIoU @ADE20K | $\star$ $\star$ |'
- en: '| PSANet (Zhao et al. [2018](#bib.bib202)) | *A convolutional point-wise spatial
    attention (PSA) module is attached to o pretrained convolutional encoder, so that
    pixels are interconnected through a self-adaptively learnt attention map to provide
    global context. Additional PSA module limits computational efficieny compared
    to fully convolutional architectures (e.g. FCN).* | 85.7% mIoU @PASCAL 2012 81.4%
    mIoU @CitySca. (w. course) | $\star$ $\star$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| PSANet (Zhao et al. [2018](#bib.bib202)) | *附加到预训练卷积编码器上的卷积点位空间注意（PSA）模块，通过自适应学习的注意力图使像素互联，以提供全局上下文。额外的PSA模块相比于完全卷积架构（如FCN）限制了计算效率。*
    | 85.7% mIoU @PASCAL 2012 81.4% mIoU @CitySca. (w. course) | $\star$ $\star$ |'
- en: '| cont’d. |  |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| cont’d. |  |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PAN (Li et al. [2018](#bib.bib87)) | *SPP layer with global pooling architecture.
    Similar architecture and thus, computational efficiency with PSPNet (Zhao et al.
    [2017](#bib.bib201)).* | 84.0% mIoU @PASCAL 2012 (*taken from the paper, not listed
    in the leaderboard*) | $\star$ $\star$ $\star$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| PAN (Li et al. [2018](#bib.bib87)) | *带有全局池化架构的SPP层。与PSPNet (Zhao et al.
    [2017](#bib.bib201))具有类似的架构，因此计算效率相当。* | 84.0% mIoU @PASCAL 2012 (*取自论文，不在排行榜中列出*)
    | $\star$ $\star$ $\star$ |'
- en: '| ExFuse (Zhang et al. [2018](#bib.bib199))  | *Improved version of GCN (Peng
    et al. [2017](#bib.bib120)) for feature fusing which introduces more semantic
    information into low-level features and more spatial details into high-level features,
    by additional skip connections. Computational performance comparable to GCN.*
    | 87.9% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| EMANet152 (Li et al. [2019](#bib.bib90))  | *Novel attention module between
    two CNN structures converts input feature maps to output feature maps, thus providing
    global context. Computationally more efficient compared to other attention governing
    architectures (e.g. PSANet).* | 88.2% mIoU @PASCAL 2012 39.9% mIoU @COCO | $\star$
    $\star$ $\star$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| KSAC (Huang et al. [2019](#bib.bib59))  | *Allows branches of different receptive
    fields to share the same kernel to facilitate communication among branches and
    perform feature augmentation inside the network. The idea is similar to ASPP layer
    of DeepLabv3 (Chen et al. [2017](#bib.bib18)), hence similar computational performance.*
    | 88.1% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| CFNet (Zhang et al. [2019](#bib.bib196))  | *Using a distribution of co-occurrent
    features for a given target in an image, a fine-grained spatial invariant representation
    is learnt and the CFNet is constructed. Similar architecture to PSANet (Zhao et al.
    [2018](#bib.bib202)), hence similar (and limited) computational performance due
    to fully connected layers.* | 87.2% mIoU @PASCAL 2012 | $\star$ $\star$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| YOLACT (Bolya et al. [2019](#bib.bib9)) | *Object Detector YOLO followed
    by Class Probability and Convolutional layers, applied to instance segmentation
    (see Figure [5](#S5.F5 "Figure 5 ‣ 5.1.6 Recurrent Approaches ‣ 5.1 Techniques
    for Fine-grained Localisation ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based
    Architectures for Semantic Segmentation on 2D images").b), with real-time semantic
    segmentation performance*. | 72.3% mAP[50] @PASCAL SBD 31.2% mAP @COCO | $\star$
    $\star$ $\star$ $\star$ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| ESE-Seg (Xu et al. [2019](#bib.bib184)) | *ESE-Seg is an object detection-based
    approach that uses explicit shape encoding by explicitly decoding the multiple
    object shapes with tensor operations in real-time.* | 69.3% mAP[50] @PASCAL SBD
    21.6% mAP @COCO | $\star$ $\star$ $\star$ $\star$ |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| SOLO (Wang et al. [2019](#bib.bib170))  | *The central idea of SOLO framework
    is to reformulate the instance segmentation as two simultaneous problems: category
    prediction and instance mask generation, using a single convolutional backbone.
    The model can run in real-time with proper parameter tuning.* | 37.8% mAP @COCO
    | $\star$ $\star$ $\star$ |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| EfficientNet-L2 + NASFPN + Noisy Student (Zoph et al. [2020](#bib.bib211))
    | *The study aims at understaing the effect of pre- and self training and apply
    this to semantic segmentation problem. For their experiment, they utilize a neural
    architecture search (NAS) strategy (Ghiasi et al. [2019](#bib.bib42)) with EfficientNet-L2
    (Xie et al. [2020](#bib.bib180)) as the backbone architecture. The model is the
    leader of PASCAL VOC 2012 challenge by the time this manuscript was written.*
    | 90.5% mIoU @PASCAL 2012 | $\star$ $\star$ $\star$ |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| DCNAS (Zhang et al. [2020](#bib.bib197))  | *Neural Architecture Search applied
    to MobileNetV3 (Howard et al. [2019](#bib.bib54)), a densely connected search
    space for semantic segmentation. Although computational performance is not explicitly
    indicated, the resulting architecture possibly provides U-Net like computational
    efficiency for model inference.* | 86.9% mIoU @PASCAL 2012 (*taken from the paper,
    not listed in the leaderboard*) 83.6% mIoU @CitySca. (w. course) | $\star$ $\star$
    $\star$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| SOLOv2 (Wang et al. [2020](#bib.bib171))  | *Updated, real-time version of
    SOLO (Wang et al. [2019](#bib.bib170)), empowered by an efficient and holistic
    instance mask representation scheme, which dynamically segments each instance
    in the image, without resorting to bounding box detection.* | 37.1% mAP @COCO
    | $\star$ $\star$ $\star$ $\star$ |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| cont’d. |  |  |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Deep Snake (Peng et al. [2020](#bib.bib121))  | *Deep Snake is a fully convolutional
    architecture with a contour-based approach for real-time instance segmentation.*
    | 62.1% mAP[50] @PASCAL SBD 30.3% mAP @COCO | $\star$ $\star$ $\star$ $\star$
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| BlendMask (Chen et al. [2020](#bib.bib15))  | *Using both top-down and bottom-up
    instance segmentation approaches, BlendMask learns attention maps for each instance
    using a single convolution layer.* | 37.1% mAP @COCO | $\star$ $\star$ $\star$
    $\star$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| SwiftNetRN18-Pyr (Oršić and Šegvić [2021](#bib.bib116))  | *Based on shared
    pyramidal representation and fusion of heterogeneous features, SwiftNetRN18-Pry
    fuses hybrid representation within a ladder-style decoder. Provides beyond real-time
    performance with modest accuracy.* | 35.0% mIoU @ADE20K | $\star$ $\star$ $\star$
    $\star$ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| BOXInst (Tian et al. [2021](#bib.bib157))  | *Achieves mask-level instance
    segmentation with only bounding-box annotations for training. Core idea is to
    redesign the loss of learning masks in instance segmentation* | 61.4% mAP[50]
    @PASCAL SBD 31.6% mAP @COCO | $\star$ $\star$ $\star$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: 'Table 1: State-of-the-art semantic segmentation methods, showing the method
    name and reference, brief summary, problem type targeted, and refinement model
    (if any).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Evolution of Methods
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Table [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images"), we present several semantic segmentation methods, each with a
    brief summary, explaining the fundamental idea that represents the proposed solutions,
    their position in available leaderboards, and a categorical level of the method’s
    computational efficiency. The intention is for readers to gain a better evolutionary
    understanding of the methods and architectures in this field, and a clearer conception
    of how the field may subsequently progress in the future. Regarding the brief
    summaries of the listed methods, please refer to the categorisations provided
    earlier in this section.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S5.T1 "表 1 ‣ 5.3 基于目标检测的方法 ‣ 5 后FCN方法 ‣ 基于深度学习的二维图像语义分割架构调查")中，我们展示了几种语义分割方法，每种方法都有简要总结，解释了代表提出的解决方案的基本思想、它们在现有排行榜中的位置以及方法计算效率的分类水平。目的在于使读者对该领域的方法和架构有更好的演变理解，并对该领域未来可能的进展有更清晰的认识。关于所列方法的简要总结，请参阅本节前面提供的分类。
- en: Table [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images") includes 34 methods spanning an eight-year period, starting with
    early deep learning approaches through to the most recent state-of-the-art techniques.
    Most of the listed studies have been quite successful and have significantly high
    rankings in the previously mentioned leaderboards. Whilst there are many other
    methods, we believe this list to be a clear depiction of the advances in deep
    learning-based semantic segmentation approaches. In Figure [6](#S5.F6 "Figure
    6 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey on Deep
    Learning-based Architectures for Semantic Segmentation on 2D images"), a sample
    image from the PASCAL VOC validation set, its semantic segmentation ground truth
    and results obtained from some of the listed studies are depicted. Figure [6](#S5.F6
    "Figure 6 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey
    on Deep Learning-based Architectures for Semantic Segmentation on 2D images")
    clearly shows the gradually growing success of different methods starting with
    the pioneering FCN architectures to more advanced architectures such as DeepLab
    (Chen et al. [2014](#bib.bib16), [2018](#bib.bib17)) or CRF-as-RNN (Zheng et al.
    [2015](#bib.bib205)).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S5.T1 "表 1 ‣ 5.3 基于目标检测的方法 ‣ 5 后FCN方法 ‣ 基于深度学习的二维图像语义分割架构调查")包括了涵盖八年时间跨度的34种方法，从早期的深度学习方法到最新的最先进技术。大多数列出的研究非常成功，并在前述排行榜中排名很高。虽然还有许多其他方法，但我们认为这个列表清晰地描绘了深度学习语义分割方法的进展。在图[6](#S5.F6
    "图 6 ‣ 5.3 基于目标检测的方法 ‣ 5 后FCN方法 ‣ 基于深度学习的二维图像语义分割架构调查")中，展示了来自PASCAL VOC验证集的示例图像、其语义分割的真实情况以及从一些列出研究中获得的结果。图[6](#S5.F6
    "图 6 ‣ 5.3 基于目标检测的方法 ‣ 5 后FCN方法 ‣ 基于深度学习的二维图像语义分割架构调查")清楚地展示了不同方法逐渐增长的成功，从开创性的FCN架构到更先进的架构如DeepLab（Chen
    et al. [2014](#bib.bib16), [2018](#bib.bib17)）或CRF-as-RNN（Zheng et al. [2015](#bib.bib205)）。
- en: Judging by the picture it portrays, the deep evolution of the literature clearly
    reveals a number of important implications. First, graphical model-based refinement
    modules are being abandoned due to their slow nature. A good example of this trend
    would be the evolution of DeepLab from (Chen et al. [2014](#bib.bib16)) to (Chen
    et al. [2018](#bib.bib19)) (see Table [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based
    Methods ‣ 5 Post-FCN Approaches ‣ A Survey on Deep Learning-based Architectures
    for Semantic Segmentation on 2D images")). Notably, no significant study published
    in 2019 and 2020 employed a CRF-based or similar module to refine their segmentation
    results. Second, most studies published in the past two years show no significant
    leap in performance rates. For this reason, researchers have tended to focus on
    experimental solutions such as object detection-based or Neural Architecture Search
    (NAS)-based approaches. Some of these very recent group of studies (Zhang et al.
    [2020](#bib.bib197); Zoph et al. [2020](#bib.bib211)) focus on (NAS)-based techniques,
    instead of hand-crafted architectures. EfficientNet-NAS (Zoph et al. [2020](#bib.bib211))
    belongs to this category and is the leading study in PASCAL VOC 2012 semantic
    segmentation challenge at the time the paper was prepared. We believe that the
    field will witness an increasing interest in NAS-based methods in the near future.
    In general, considering all studies of the post-FCN era, the main challenge of
    the field still remains to be *efficiently* integrating (i.e. in real-time) global
    context to localisation information, which still does not appear to have an off-the-shelf
    solution, although there are some promising techniques, such as YOLACT (Bolya
    et al. [2019](#bib.bib9)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从图示中可以看出，文献的深度演变清楚地揭示了许多重要的启示。首先，基于图形模型的细化模块由于其缓慢的特性正在被弃用。一个很好的例子是DeepLab从（Chen
    et al. [2014](#bib.bib16)）到（Chen et al. [2018](#bib.bib19)）的演变（见表[1](#S5.T1 "Table
    1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey on Deep
    Learning-based Architectures for Semantic Segmentation on 2D images")）。值得注意的是，2019年和2020年没有显著的研究使用CRF或类似模块来细化他们的分割结果。其次，过去两年发表的大多数研究在性能提升方面没有显著进展。因此，研究人员倾向于关注基于对象检测或神经架构搜索（NAS）的方法。这些非常近期的研究组（Zhang
    et al. [2020](#bib.bib197)；Zoph et al. [2020](#bib.bib211)）专注于基于（NAS）的技术，而非手工设计的架构。EfficientNet-NAS（Zoph
    et al. [2020](#bib.bib211)）属于这一类别，并且是撰写本文时PASCAL VOC 2012语义分割挑战的领先研究。我们相信，未来该领域将对基于NAS的方法表现出越来越大的兴趣。总体而言，考虑到所有后FCN时代的研究，该领域的主要挑战仍然是*高效*地将全局上下文整合到定位信息中，这仍然没有现成的解决方案，尽管有一些有前景的技术，例如YOLACT（Bolya
    et al. [2019](#bib.bib9)）。
- en: In Table [1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN
    Approaches ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation
    on 2D images"), the right-most column represents a categorical level of computational
    efficiency. We use a four-level categorisation (one star to four stars) to indicate
    the computational efficiency of each listed method. For any assigned level of
    the computational efficiency of a method, we explain our reasoning in the table
    with solid arguments. For example, one of the four-star methods in Table [1](#S5.T1
    "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey
    on Deep Learning-based Architectures for Semantic Segmentation on 2D images")
    is “YOLACT” by (Bolya et al. [2019](#bib.bib9)), which claims to provide real-time
    performance (i.e. $>$30fps) on both PASCAL VOC 2012 and COCO image sets.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[1](#S5.T1 "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches
    ‣ A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D
    images")中，最右边的列表示计算效率的分类级别。我们使用四级分类（一星到四星）来表示每种列出方法的计算效率。对于每个分配的计算效率级别，我们在表中用确凿的论据解释我们的理由。例如，表格[1](#S5.T1
    "Table 1 ‣ 5.3 Object Detection-based Methods ‣ 5 Post-FCN Approaches ‣ A Survey
    on Deep Learning-based Architectures for Semantic Segmentation on 2D images")中的一个四星级方法是（Bolya
    et al. [2019](#bib.bib9)）的“YOLACT”，它声称在PASCAL VOC 2012和COCO图像集上提供实时性能（即$>$30fps）。
- en: 6 Future Scope and Potential Research Directions
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来的范围和潜在的研究方向
- en: Although tremendous successes have been achieved so far in the semantic segmentation
    field, there are still many open challenges in this field due to hard requirements
    time-consuming pixel-level annotations, lack of generalization ability to new
    domains and classes, and need for real-time performance with higher segmentation
    accuracies. In this section, we categorize possible future directions under different
    titles by providing examples of recent studies that represent that direction.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在语义分割领域已经取得了巨大成功，但由于对时间消耗巨大的像素级标注的严格要求、缺乏对新领域和类别的泛化能力，以及对更高分割精度的实时性能的需求，这个领域仍然面临许多未解挑战。在本节中，我们通过提供代表这些方向的近期研究的例子，将可能的未来方向分类。
- en: 6.1 Weakly-Supervised Semantic Segmentation (WSSS)
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 弱监督语义分割（WSSS）
- en: Over the last few years, there has been an increasing research effort directed
    towards the approaches that are alternative to pixel-level annotations such as;
    unsupervised, semi-supervised (He et al. [2021](#bib.bib51)) and weakly-supervised
    methods. Recent studies show that, WSSS methods usually perform better than the
    other schemes (Chan et al. [2021](#bib.bib13)) where annotations are in the form
    of image-level labels (Kolesnikov and Lampert [2016](#bib.bib72); Pathak et al.
    [2015](#bib.bib119); Pinheiro and Collobert [2015](#bib.bib125); Wang et al. [2020](#bib.bib173);
    Ahn and Kwak [2018](#bib.bib2); Li et al. [2021](#bib.bib91); Chang et al. [2020](#bib.bib14);
    Xu et al. [2021](#bib.bib182); Yao et al. [2021](#bib.bib187); Jiang et al. [2021](#bib.bib63)),
    video-level labels (Zhong et al. [2016](#bib.bib207)), scribbles (Lin et al. [2016](#bib.bib94)),
    points (Bearman et al. [2016](#bib.bib8)), and bounding boxes (Dai et al. [2015](#bib.bib28);
    Khoreva et al. [2017](#bib.bib69); Xu et al. [2015](#bib.bib181)). In case of
    image-level labels, class activation maps (CAMs) (Zhou et al. [2016](#bib.bib209))
    are used to localize the small discriminative regions which are not suitable particularly
    for the large-scale objects, but can be utilized as initial seeds (pseudo-masks)
    (Araslanov and Roth [2020](#bib.bib3); Fan et al. [2020](#bib.bib33); Sun et al.
    [2021](#bib.bib153); Kweon et al. [2021](#bib.bib76)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，越来越多的研究致力于像无监督、半监督（He et al. [2021](#bib.bib51)）和弱监督方法这样替代像素级标注的方法。近期的研究表明，WSSS方法通常表现优于其他方案（Chan
    et al. [2021](#bib.bib13)），这些方案的标注形式包括图像级标签（Kolesnikov and Lampert [2016](#bib.bib72);
    Pathak et al. [2015](#bib.bib119); Pinheiro and Collobert [2015](#bib.bib125);
    Wang et al. [2020](#bib.bib173); Ahn and Kwak [2018](#bib.bib2); Li et al. [2021](#bib.bib91);
    Chang et al. [2020](#bib.bib14); Xu et al. [2021](#bib.bib182); Yao et al. [2021](#bib.bib187);
    Jiang et al. [2021](#bib.bib63))、视频级标签（Zhong et al. [2016](#bib.bib207)）、涂鸦（Lin
    et al. [2016](#bib.bib94)）、点（Bearman et al. [2016](#bib.bib8)）和边界框（Dai et al.
    [2015](#bib.bib28); Khoreva et al. [2017](#bib.bib69); Xu et al. [2015](#bib.bib181)）。在图像级标签的情况下，类激活映射（CAMs）（Zhou
    et al. [2016](#bib.bib209)）被用来定位那些小的区分性区域，这些区域特别不适合大规模对象，但可以用作初步种子（伪掩模）（Araslanov
    and Roth [2020](#bib.bib3); Fan et al. [2020](#bib.bib33); Sun et al. [2021](#bib.bib153);
    Kweon et al. [2021](#bib.bib76)）。
- en: 6.2 Zero-/Few-Shot Learning
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 零样本/少样本学习
- en: Motivated by humans’ ability to recognize new concepts in a scene by using only
    a few visual samples, zero-shot and/or few-shot learning methods have been introduced.
    Few-shot semantic segmentation (FS3) methods (Wang et al. [2019](#bib.bib167);
    Xie et al. [2021](#bib.bib179)) has been proposed to recognize objects from unseen
    classes by utilizing few annotated examples; however, these methods are limited
    to handling a single unseen class only. Zero-shot semantic segmentation (ZS3)
    methods have been developed recently to generate visual features by exploiting word
    embedding vectors in the case of zero training samples (Bucher et al. [2019](#bib.bib11);
    Xian et al. [2019](#bib.bib176); Pastore et al. [2021](#bib.bib118); Lu et al.
    [2021](#bib.bib105)). However, the major drawback of ZS3 methods is their insufficient
    prediction ability to distinguish between the seen and the unseen classes even
    if both are included in a scene. This disadvantage is usually overcome by generalized
    ZS3 (GZS3), which recognizes both seen and unseen classes simultaneously. GZS3
    studies mainly rely on generative-based methods. Feature extractor training is
    realized without considering semantic features in GZS3 adopted with generative
    approaches so that the bias is introduced towards the seen classes. Therefore,
    GZS3 methods result in performance reduction on unseen classes (Pastore et al.
    [2021](#bib.bib118)). Much of the recent work on ZS3 has involved such as; exploiting joint
    embedding space to alleviate the seen bias problem (Baek et al. [2021](#bib.bib7)),
    analyzing different domain performances (Chan et al. [2021](#bib.bib13)), and
    incorporating spatial information (Cheng et al. [2021](#bib.bib22)).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类通过仅使用少量视觉样本来识别场景中新概念的能力启发，零-shot 和/或 few-shot 学习方法被引入。few-shot 语义分割（FS3）方法（Wang
    et al. [2019](#bib.bib167); Xie et al. [2021](#bib.bib179)）已被提出，通过利用少量标注示例来识别未见类别的对象；然而，这些方法仅限于处理单一未见类别。零-shot
    语义分割（ZS3）方法最近已被开发，通过利用词嵌入向量在没有训练样本的情况下生成视觉特征（Bucher et al. [2019](#bib.bib11);
    Xian et al. [2019](#bib.bib176); Pastore et al. [2021](#bib.bib118); Lu et al.
    [2021](#bib.bib105)）。然而，ZS3 方法的主要缺点是即使场景中包含了已见和未见类别，它们在区分这两者时的预测能力不足。这种缺点通常通过广义
    ZS3（GZS3）来克服，该方法同时识别已见和未见类别。GZS3 研究主要依赖于生成式方法。GZS3 中的特征提取器训练在采用生成式方法时未考虑语义特征，因此引入了对已见类别的偏差。因此，GZS3
    方法会导致对未见类别的性能降低（Pastore et al. [2021](#bib.bib118)）。最近关于 ZS3 的研究涉及；利用联合嵌入空间来缓解已见偏差问题（Baek
    et al. [2021](#bib.bib7)），分析不同领域性能（Chan et al. [2021](#bib.bib13)），以及结合空间信息（Cheng
    et al. [2021](#bib.bib22)）。
- en: 6.3 Domain Adaptation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 领域适应
- en: Recent studies also rely on the use of synthetic large-scale image sets such
    as GTA5 (Richter et al. [2016](#bib.bib133)) and SYNTHIA (Ros et al. [2016b](#bib.bib136))
    because of their capability to cope with laborious pixel-level annotations. Although
    these rich-labeled synthetic images have the advantage of reducing the labeling
    cost, they also bring about domain shift while training with unlabeled real images.
    Therefore, applying domain adaptation for aligning the synthetic and the real
    image sets is of much importance (Zhao et al. [2019](#bib.bib203); Kang et al.
    [2020](#bib.bib65); Wu et al. [2021](#bib.bib174); Wang et al. [2021](#bib.bib172);
    Shin et al. [2021](#bib.bib147); Fleuret et al. [2021](#bib.bib36)). Unsupervised
    domain adaptation (UDA) methods are widely employed in semantic segmentation (Cheng
    et al. [2021](#bib.bib23); Liu et al. [2021](#bib.bib101); Hong et al. [2018](#bib.bib53);
    Vu et al. [2019](#bib.bib165); Pan et al. [2020](#bib.bib117); Wang et al. [2021](#bib.bib169);
    Saporta et al. [2021](#bib.bib142); Zheng and Yang [2021](#bib.bib206)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究还依赖于使用合成的大规模图像集，如 GTA5（Richter et al. [2016](#bib.bib133)）和 SYNTHIA（Ros
    et al. [2016b](#bib.bib136)），因为它们能够处理繁琐的像素级标注。虽然这些富标注的合成图像具有减少标注成本的优势，但在与未标注的真实图像训练时，也会带来领域偏移。因此，为了对齐合成和真实图像集，应用领域适应是非常重要的（Zhao
    et al. [2019](#bib.bib203); Kang et al. [2020](#bib.bib65); Wu et al. [2021](#bib.bib174);
    Wang et al. [2021](#bib.bib172); Shin et al. [2021](#bib.bib147); Fleuret et al.
    [2021](#bib.bib36)）。无监督领域适应（UDA）方法在语义分割中被广泛应用（Cheng et al. [2021](#bib.bib23);
    Liu et al. [2021](#bib.bib101); Hong et al. [2018](#bib.bib53); Vu et al. [2019](#bib.bib165);
    Pan et al. [2020](#bib.bib117); Wang et al. [2021](#bib.bib169); Saporta et al.
    [2021](#bib.bib142); Zheng and Yang [2021](#bib.bib206)）。
- en: 6.4 Real-Time Processing
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 实时处理
- en: Adopting compact and shallow model architectures (Zhao et al. [2018](#bib.bib200);
    Orsic et al. [2019](#bib.bib115); Yu et al. [2018](#bib.bib188); Li et al. [2019](#bib.bib88);
    Fan et al. [2021](#bib.bib34)) and restricting the input to be low-resolution
    (Marin et al. [2019](#bib.bib107)) are brand new innovations proposed very recently
    to overcome the computational burden of large-scale semantic segmentation. To
    choose a real-time semantic segmentation strategy, all aspects of an application
    should be considered, as all of these strategies somehow correlate with decreasing
    the model’s discriminative ability and losing information of object boundaries
    or small objects to some extent. Some other strategies have also been proposed
    for the retrieval of rich contextual information in real-time applications including attention
    mechanisms (Ding et al. [2021](#bib.bib29); Hu et al. [2020](#bib.bib56)), depth-wise
    separable convolutions (Chollet [2017](#bib.bib25); Howard et al. [2019](#bib.bib54)),
    pyramid fusion (Rosas-Arias et al. [2021](#bib.bib137); Oršić and Šegvić [2021](#bib.bib116)),
    grouped convolutions (Zhang et al. [2018](#bib.bib198); Huang et al. [2018](#bib.bib57))
    and neural architecture search (Zoph et al. [2018](#bib.bib212)), pipeline parallelism
    (Chew et al. [2022](#bib.bib24)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 采用紧凑和浅层模型架构（Zhao et al. [2018](#bib.bib200); Orsic et al. [2019](#bib.bib115);
    Yu et al. [2018](#bib.bib188); Li et al. [2019](#bib.bib88); Fan et al. [2021](#bib.bib34)）和将输入限制为低分辨率（Marin
    et al. [2019](#bib.bib107)）是最近提出的全新创新，以克服大规模语义分割的计算负担。为了选择实时语义分割策略，应该考虑应用的各个方面，因为所有这些策略在某种程度上都与降低模型的区分能力和丧失对象边界或小对象信息有关。还提出了其他一些策略，用于在实时应用中检索丰富的上下文信息，包括注意力机制（Ding
    et al. [2021](#bib.bib29); Hu et al. [2020](#bib.bib56)）、深度可分离卷积（Chollet [2017](#bib.bib25);
    Howard et al. [2019](#bib.bib54)）、金字塔融合（Rosas-Arias et al. [2021](#bib.bib137);
    Oršić and Šegvić [2021](#bib.bib116)）、分组卷积（Zhang et al. [2018](#bib.bib198); Huang
    et al. [2018](#bib.bib57)）和神经架构搜索（Zoph et al. [2018](#bib.bib212)）、流水线并行（Chew
    et al. [2022](#bib.bib24)）。
- en: 6.5 Contextual Information
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 上下文信息
- en: Contextual information aggregation with the purpose of augmenting pixel representations
    in semantic segmentation architectures is another promising research direction
    in recent years. In this aspect, mining contextual information (Jin et al. [2021](#bib.bib64)),
    exploring context information on spatial and channel dimensions (Li et al. [2021](#bib.bib92)),
    focusing on object based contextual representations (Yuan et al. [2020](#bib.bib192))
    and capturing the global contextual information for fine-resolution remote sensing
    imagery (Li et al. [2021](#bib.bib89)) are some of the recent studies. Alternative
    methods of reducing dense pixel-level annotations in semantic segmentation have
    been described which are based on using pixel-wise contrastive loss (Chaitanya
    et al. [2020](#bib.bib12); Zhao et al. [2021](#bib.bib204); Zhang et al. [2021](#bib.bib194)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文信息聚合，旨在增强语义分割架构中的像素表示，是近年来另一个有前景的研究方向。在这方面，挖掘上下文信息（Jin et al. [2021](#bib.bib64)）、探索空间和通道维度上的上下文信息（Li
    et al. [2021](#bib.bib92)）、关注基于对象的上下文表示（Yuan et al. [2020](#bib.bib192)）以及捕捉用于高分辨率遥感图像的全局上下文信息（Li
    et al. [2021](#bib.bib89)）是一些近期的研究。减少语义分割中密集像素级注释的替代方法已被描述，这些方法基于使用像素级对比损失（Chaitanya
    et al. [2020](#bib.bib12); Zhao et al. [2021](#bib.bib204); Zhang et al. [2021](#bib.bib194)）。
- en: 7 Conclusions
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this survey, we aimed at reviewing the current developments in the literature
    regarding deep learning-based 2D image semantic segmentation. We commenced with
    an analysis of the public image sets and leaderboards for 2D semantic segmentation
    and then continued by providing an overview of the techniques for performance
    evaluation. Following this introduction, our focus shifted to the 10-year evolution
    seen in this field under three chronological titles, namely the pre- and early-
    deep learning era, the fully convolutional era, and the post-FCN era. After a
    technical analysis on the approaches of each period, we presented a table of methods
    spanning all three eras, with a brief summary of each technique that explicates
    their contribution to the field.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们旨在回顾文献中基于深度学习的2D图像语义分割的当前发展。我们首先分析了公共图像集和2D语义分割的排行榜，然后概述了性能评估的技术。在此介绍之后，我们的重点转向了该领域在三大时间段的10年演变，即前期和早期深度学习时代、全卷积时代以及后FCN时代。在对每个时期的方法进行技术分析后，我们呈现了一张涵盖所有三个时代的方法表，并对每种技术进行了简要总结，阐明了它们对该领域的贡献。
- en: In our review, we paid particular attention to the key technical challenges
    of the 2D semantic segmentation problem, the deep learning-based solutions that
    were proposed, and how these solutions evolved as they shaped the advancements
    in the field. To this end, we observed that the fine-grained localisation of pixel
    labels is clearly the definitive challenge to the overall problem. Although the
    title may imply a more ‘local’ interest, the research published in this field
    evidently shows that it is the global context that determines the actual performance
    of a method. Thus, it is eminently conceivable why the literature is rich with
    approaches that attempt to bridge local information with a more global context,
    such as graphical models, context aggregating networks, recurrent approaches,
    and attention-based modules. It is also clear that efforts to fulfil this local-global
    semantics gap at the pixel level will continue for the foreseeable future.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的综述中，我们特别关注了二维语义分割问题的关键技术挑战、提出的基于深度学习的解决方案以及这些解决方案如何随着领域的发展而演变。为此，我们观察到像素标签的细粒度定位显然是整体问题的决定性挑战。尽管标题可能暗示更“局部”的兴趣，但该领域发表的研究显然表明，决定方法实际性能的是全球上下文。因此，文献中充斥着试图将局部信息与更全球上下文桥接的方法，如图形模型、上下文聚合网络、递归方法和基于注意力的模块。这也清楚地表明，弥补像素级的局部-全球语义差距的努力将会在可预见的未来持续下去。
- en: Another important revelation from this review has been the profound effect seen
    from public challenges to the field. Academic and industrial groups alike are
    in a constant struggle to top these public leaderboards, which has an obvious
    effect of accelerating development in this field. Therefore, it would be prudent
    to promote or even contribute to creating similar public image sets and challenges
    affiliated to more specific subjects of the semantic segmentation problem, such
    as 2D medical images.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次综述中得到的另一个重要启示是公众挑战对这一领域的深远影响。学术界和工业界都在不断努力超越这些公众排行榜，这显然加速了该领域的发展。因此，推动或甚至参与创建类似的公共图像集和挑战，特别是针对语义分割问题中更具体主题的挑战，如二维医学图像，将是明智的选择。
- en: Considering the rapid and continuing development seen in this field, there is
    an irrefutable need for an update on the surveys regarding the semantic segmentation
    problem. However, we believe that the current survey may be considered as a milestone
    in measuring how much the field has progressed thus far, and where the future
    directions possibly lie.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到该领域的快速和持续发展，不可否认地需要对语义分割问题的调查进行更新。然而，我们相信，当前的调查可以被视为衡量该领域迄今为止进展程度的一个里程碑，并指示未来可能的发展方向。
- en: References
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmad et al. (2017) Ahmad, T., P. Campr, M. Cadik, and G. Bebis (2017, May).
    Comparison of semantic segmentation approaches for horizon/sky line detection.
    2017 International Joint Conference on Neural Networks (IJCNN).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等（2017）Ahmad, T., P. Campr, M. Cadik 和 G. Bebis（2017年5月）。地平线/天空线检测的语义分割方法比较。2017年国际神经网络联合会议（IJCNN）。
- en: Ahn and Kwak (2018) Ahn, J. and S. Kwak (2018). Learning pixel-level semantic
    affinity with image-level supervision for weakly supervised semantic segmentation.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp.  4981–4990.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 和 Kwak（2018）Ahn, J. 和 S. Kwak（2018）。通过图像级监督学习像素级语义关联，用于弱监督语义分割。发表于《IEEE计算机视觉与模式识别会议论文集》，第4981–4990页。
- en: Araslanov and Roth (2020) Araslanov, N. and S. Roth (2020). Single-stage semantic
    segmentation from image labels. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp.  4253–4262.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Araslanov 和 Roth（2020）Araslanov, N. 和 S. Roth（2020）。从图像标签中进行单阶段语义分割。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，第4253–4262页。
- en: Arbeláez et al. (2012) Arbeláez, P., B. Hariharan, C. Gu, S. Gupta, L. Bourdev,
    and J. Malik (2012). Semantic segmentation using regions and parts. In Computer
    Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp.  3378–3385\.
    IEEE.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbeláez 等（2012）Arbeláez, P., B. Hariharan, C. Gu, S. Gupta, L. Bourdev 和 J.
    Malik（2012）。使用区域和部件的语义分割。发表于《计算机视觉与模式识别（CVPR）》，2012年IEEE会议，第3378–3385页。IEEE。
- en: Arun et al. (2020) Arun, A., C. Jawahar, and M. P. Kumar (2020). Weakly supervised
    instance segmentation by learning annotation consistent instances. In European
    Conference on Computer Vision, pp.  254–270. Springer.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arun 等（2020）Arun, A., C. Jawahar 和 M. P. Kumar（2020）。通过学习一致注释实例进行弱监督实例分割。发表于《欧洲计算机视觉会议论文集》，第254–270页。Springer。
- en: 'Badrinarayanan et al. (2015) Badrinarayanan, V., A. Kendall, and R. Cipolla
    (2015). Segnet: A deep convolutional encoder-decoder architecture for image segmentation.
    CoRR abs/1511.00561.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baek et al. (2021) Baek, D., Y. Oh, and B. Ham (2021). Exploiting a joint embedding
    space for generalized zero-shot semantic segmentation. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp.  9536–9545.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bearman et al. (2016) Bearman, A., O. Russakovsky, V. Ferrari, and L. Fei-Fei
    (2016). What’s the point: Semantic segmentation with point supervision. In European
    conference on computer vision, pp.  549–565. Springer.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bolya et al. (2019) Bolya, D., C. Zhou, F. Xiao, and Y. J. Lee (2019). YOLACT:
    real-time instance segmentation. CoRR abs/1904.02689.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brostow et al. (2009) Brostow, G. J., J. Fauqueur, and R. Cipolla (2009). Semantic
    object classes in video: A high-definition ground truth database. Pattern Recognition
    Letters 30, 88–97.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucher et al. (2019) Bucher, M., T.-H. Vu, M. Cord, and P. Pérez (2019). Zero-shot
    semantic segmentation. Advances in Neural Information Processing Systems 32, 468–479.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaitanya et al. (2020) Chaitanya, K., E. Erdil, N. Karani, and E. Konukoglu
    (2020). Contrastive learning of global and local features for medical image segmentation
    with limited annotations. arXiv preprint arXiv:2006.10511.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chan et al. (2021) Chan, L., M. S. Hosseini, and K. N. Plataniotis (2021). A
    comprehensive analysis of weakly-supervised semantic segmentation in different
    image domains. International Journal of Computer Vision 129(2), 361–384.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2020) Chang, Y.-T., Q. Wang, W.-C. Hung, R. Piramuthu, Y.-H. Tsai,
    and M.-H. Yang (2020). Weakly-supervised semantic segmentation via sub-category
    exploration. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp.  8991–9000.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Chen, H., K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan
    (2020). Blendmask: Top-down meets bottom-up for instance segmentation. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp.  8573–8581.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2014) Chen, L., G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
    Yuille (2014). Semantic image segmentation with deep convolutional nets and fully
    connected crfs. CoRR abs/1412.7062.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Chen, L., G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
    Yuille (2018, April). Deeplab: Semantic image segmentation with deep convolutional
    nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern
    Analysis and Machine Intelligence 40(4), 834–848.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2017) Chen, L., G. Papandreou, F. Schroff, and H. Adam (2017).
    Rethinking atrous convolution for semantic image segmentation. CoRR, 2843–2851.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Chen, L., Y. Zhu, G. Papandreou, F. Schroff, and H. Adam
    (2018). Encoder-decoder with atrous separable convolution for semantic image segmentation.
    CoRR abs/1802.02611.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2014) Chen, X., R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and
    A. Yuille (2014). Detect what you can: Detecting and representing objects using
    holistic models and body parts. In IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2021) Cheng, B., A. Schwing, and A. Kirillov (2021). Per-pixel
    classification is not all you need for semantic segmentation. Advances in Neural
    Information Processing Systems 34.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2021) Cheng, J., S. Nandi, P. Natarajan, and W. Abd-Almageed
    (2021). Sign: Spatial-information incorporated generative network for generalized
    zero-shot semantic segmentation. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp.  9556–9566.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2021) Cheng, Y., F. Wei, J. Bao, D. Chen, F. Wen, and W. Zhang
    (2021). Dual path learning for domain adaptation of semantic segmentation. In
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    9082–9091.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chew et al. (2022) Chew, A. W. Z., A. Ji, and L. Zhang (2022). Large-scale 3d
    point-cloud semantic segmentation of urban and rural scenes using data volume
    decomposition coupled with pipeline parallelism. Automation in Construction 133,
    103995.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) Chollet, F. (2017). Xception: Deep learning with depthwise separable
    convolutions. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp.  1251–1258.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciresan et al. (2012) Ciresan, D., A. Giusti, L. M. Gambardella, and J. Schmidhuber
    (2012). Deep neural networks segment neuronal membranes in electron microscopy
    images. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.),
    Advances in Neural Information Processing Systems 25, pp. 2843–2851\. Curran Associates,
    Inc.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cordts et al. (2016) Cordts, M., M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
    R. Benenson, U. Franke, S. Roth, and B. Schiele (2016). The cityscapes dataset
    for semantic urban scene understanding. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp.  3213–3223.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2015) Dai, J., K. He, and J. Sun (2015). Boxsup: Exploiting bounding
    boxes to supervise convolutional networks for semantic segmentation. In Proceedings
    of the IEEE international conference on computer vision, pp.  1635–1643.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Ding, X., C. Shen, Z. Che, T. Zeng, and Y. Peng (2021).
    Scarf: A semantic constrained attention refinement network for semantic segmentation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    3002–3011.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSTLab. (2016) DSTLab. (2016).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigen and Fergus (2014) Eigen, D. and R. Fergus (2014). Predicting depth, surface
    normals and semantic labels with a common multi-scale convolutional architecture.
    CoRR abs/1411.4734.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) Everingham, M., L. Gool, C. K. Williams, J. Winn, and
    A. Zisserman (2010, June). The pascal visual object classes (voc) challenge. Int.
    J. Comput. Vision 88(2), 303–338.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Fan, J., Z. Zhang, C. Song, and T. Tan (2020). Learning integral
    objects with intra-class discriminator for weakly-supervised semantic segmentation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  4283–4292.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2021) Fan, M., S. Lai, J. Huang, X. Wei, Z. Chai, J. Luo, and X. Wei
    (2021). Rethinking bisenet for real-time semantic segmentation. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  9716–9725.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farabet et al. (2013) Farabet, C., C. Couprie, L. Najman, and Y. LeCun (2013,
    Aug). Learning hierarchical features for scene labeling. IEEE Transactions on
    Pattern Analysis and Machine Intelligence 35(8), 1915–1929.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleuret et al. (2021) Fleuret, F. et al. (2021). Uncertainty reduction for model
    adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp.  9613–9623.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fröhlich et al. (2013) Fröhlich, B., E. Rodner, and J. Denzler (2013). Semantic
    segmentation with millions of features: Integrating multiple cues in a combined
    random forest approach. In Computer Vision – ACCV 2012, Berlin, Heidelberg, pp. 218–231\.
    Springer Berlin Heidelberg.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2017) Fu, J., J. Liu, Y. Wang, and H. Lu (2017). Stacked deconvolutional
    network for semantic segmentation. CoRR abs/1708.04943.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganin and Lempitsky (2014) Ganin, Y. and V. S. Lempitsky (2014). N4-fields:
    Neural network nearest neighbor fields for image transforms. CoRR abs/1406.6558.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garcia-Garcia et al. (2017) Garcia-Garcia, A., S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
    and J. G. Rodríguez (2017). A review on deep learning techniques applied to semantic
    segmentation. CoRR abs/1704.06857.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger et al. (2013) Geiger, A., P. Lenz, C. Stiller, and R. Urtasun (2013,
    September). Vision meets robotics: The kitti dataset. Int. J. Rob. Res. 32(11),
    1231–1237.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghiasi et al. (2019) Ghiasi, G., T. Lin, and Q. V. Le (2019). Nas-fpn: Learning
    scalable feature pyramid architecture for object detection. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pp.  7029–7038.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick (2015) Girshick, R. B. (2015). Fast r-cnn. CoRR.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick et al. (2013) Girshick, R. B., J. Donahue, T. Darrell, and J. Malik
    (2013). Rich feature hierarchies for accurate object detection and semantic segmentation.
    CoRR abs/1311.2524.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2018) Guo, Y., Y. Liu, T. Georgiou, and M. S. Lew (2018, Jun). A
    review of semantic segmentation using deep neural networks. International Journal
    of Multimedia Information Retrieval 7(2), 87–93.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hariharan et al. (2014) Hariharan, B., P. Arbeláez, R. Girshick, and J. Malik
    (2014). Simultaneous detection and segmentation. In Computer Vision – ECCV 2014,
    Cham, pp.  297–312\. Springer International Publishing.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hariharan et al. (2011) Hariharan, B., P. Arbeláez, L. Bourdev, S. Maji, and
    J. Malik (2011, Nov). Semantic contours from inverse detectors. In 2011 International
    Conference on Computer Vision, pp. 991–998.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) He, K., G. Gkioxari, P. Dollár, and R. B. Girshick (2017).
    Mask r-cnn. 2017 IEEE International Conference on Computer Vision (ICCV), 2980–2988.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2015) He, K., X. Zhang, S. Ren, and J. Sun (2015, Sep.). Spatial
    pyramid pooling in deep convolutional networks for visual recognition. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 37(9), 1904–1916.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) He, K., X. Zhang, S. Ren, and J. Sun (2016, June). Deep residual
    learning for image recognition. In 2016 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pp.  770–778.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) He, R., J. Yang, and X. Qi (2021). Re-distributing biased
    pseudo labels for semi-supervised semantic segmentation: A baseline investigation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    6930–6940.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Zemel (2009) He, X. and R. S. Zemel (2009). Learning hybrid models for
    image annotation with partially labeled data. In Advances in Neural Information
    Processing Systems 21, pp. 625–632\. Curran Associates, Inc.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2018) Hong, W., Z. Wang, M. Yang, and J. Yuan (2018). Conditional
    generative adversarial network for structured domain adaptation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp.  1335–1344.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) Howard, A., M. Sandler, B. Chen, W. Wang, L. Chen, M. Tan,
    G. Chu, V. Vasudevan, Y. Zhu, R. Pang, H. Adam, and Q. Le (2019). Searching for
    mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
    pp.  1314–1324.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsu et al. (2019) Hsu, C.-C., K.-J. Hsu, C.-C. Tsai, Y.-Y. Lin, and Y.-Y. Chuang
    (2019). Weakly supervised instance segmentation using the bounding box tightness
    prior. Advances in Neural Information Processing Systems 32, 6586–6597.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020) Hu, P., F. Perazzi, F. C. Heilbron, O. Wang, Z. Lin, K. Saenko,
    and S. Sclaroff (2020). Real-time semantic segmentation with fast attention. IEEE
    Robotics and Automation Letters 6(1), 263–270.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang, G., S. Liu, L. Van der Maaten, and K. Q. Weinberger
    (2018). Condensenet: An efficient densenet using learned group convolutions. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp.  2752–2761.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020) Huang, G., J. Zhu, J. Li, Z. Wang, L. Cheng, L. Liu, H. Li,
    and J. Zhou (2020). Channel-attention u-net: Channel attention mechanism for semantic
    segmentation of esophagus and esophageal cancer. IEEE Access 8, 122798–122810.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Huang, Y., Q. Wang, W. Jia, and X. He (2019). See more than
    once – kernel-sharing atrous convolution for semantic segmentation.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iglovikov et al. (2018) Iglovikov, V., S. Seferbekov, A. Buslaev, and A. Shvets
    (2018). Ternausnetv2: Fully convolutional network for instance segmentation. In
    2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
    (CVPRW), pp.  228–2284.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jadon (2020) Jadon, S. (2020). A survey of loss functions for semantic segmentation.
    In 2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational
    Biology (CIBCB), pp.  1–7\. IEEE.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2017) Jiang, F., A. Grigorev, S. Rho, Z. Tian, Y. Fu, W. Jifara,
    A. Khan, and S. Liu (2017, 07). Medical image semantic segmentation based on deep
    learning. Neural Computing and Applications.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2021) Jiang, P.-T., L.-H. Han, Q. Hou, M.-M. Cheng, and Y. Wei
    (2021). Online attention accumulation for weakly supervised semantic segmentation.
    IEEE Transactions on Pattern Analysis and Machine Intelligence.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2021) Jin, Z., T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao
    (2021). Mining contextual information beyond image for semantic segmentation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    7231–7241.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2020) Kang, G., Y. Wei, Y. Yang, Y. Zhuang, and A. G. Hauptmann
    (2020). Pixel-level cycle association: A new perspective for domain adaptive semantic
    segmentation. arXiv preprint arXiv:2011.00147.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karimi and Salcudean (2019) Karimi, D. and S. E. Salcudean (2019). Reducing
    the hausdorff distance in medical image segmentation with convolutional neural
    networks. IEEE Transactions on medical imaging 39(2), 499–513.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ke et al. (2021) Ke, L., Y.-W. Tai, and C.-K. Tang (2021). Deep occlusion-aware
    instance segmentation with overlapping bilayers. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp.  4019–4028.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kemker et al. (2018) Kemker, R., C. Salvaggio, and C. Kanan (2018). Algorithms
    for semantic segmentation of multispectral remote sensing imagery using deep learning.
    ISPRS journal of photogrammetry and remote sensing 145, 60–77.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khoreva et al. (2017) Khoreva, A., R. Benenson, J. Hosang, M. Hein, and B. Schiele
    (2017). Simple does it: Weakly supervised instance and semantic segmentation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp.  876–885.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Kim, M., S. Woo, D. Kim, and I. S. Kweon (2021). The devil
    is in the boundary: Exploiting boundary representation for basis-based instance
    segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision, pp.  929–938.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. (2018) Kirillov, A., K. He, R. B. Girshick, C. Rother, and P. Dollar
    (2018). Panoptic segmentation. CoRR abs/1801.00868.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kolesnikov and Lampert (2016) Kolesnikov, A. and C. H. Lampert (2016). Seed,
    expand and constrain: Three principles for weakly-supervised image segmentation.
    In European conference on computer vision, pp.  695–711. Springer.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krähenbühl and Koltun (2011) Krähenbühl, P. and V. Koltun (2011). Efficient
    inference in fully connected crfs with gaussian edge potentials. In Advances in
    neural information processing systems, pp. 109–117.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012,
    01). Imagenet classification with deep convolutional neural networks. Neural Information
    Processing Systems 25.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012).
    Imagenet classification with deep convolutional neural networks. In Proceedings
    of the 25th International Conference on Neural Information Processing Systems
    - Volume 1, NIPS’12, USA, pp.  1097–1105. Curran Associates Inc.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kweon et al. (2021) Kweon, H., S.-H. Yoon, H. Kim, D. Park, and K.-J. Yoon
    (2021). Unlocking the potential of ordinary classifier: Class-specific adversarial
    erasing framework for weakly supervised semantic segmentation. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp.  6994–7003.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ladický et al. (2010) Ladický, Ľ., P. Sturgess, K. Alahari, C. Russell, and
    P. H. S. Torr (2010). What, where and how many? combining object detectors and
    crfs. In K. Daniilidis, P. Maragos, and N. Paragios (Eds.), Computer Vision –
    ECCV 2010, Berlin, Heidelberg, pp.  424–437\. Springer Berlin Heidelberg.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ladický et al. (2009) Ladický, L., C. Russell, P. Kohli, and P. H. S. Torr (2009,
    Sep.). Associative hierarchical crfs for object class image segmentation. In 2009
    IEEE 12th International Conference on Computer Vision, pp.  739–746.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lafferty et al. (2001) Lafferty, J., A. McCallum, and F. C. Pereira (2001).
    Conditional random fields: Probabilistic models for segmenting and labeling sequence
    data.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lateef and Ruichek (2019) Lateef, F. and Y. Ruichek (2019). Survey on semantic
    segmentation using deep learning techniques. Neurocomputing 338, 321 – 348.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazebnik et al. (2006) Lazebnik, S., C. Schmid, and J. Ponce (2006, June).
    Beyond bags of features: Spatial pyramid matching for recognizing natural scene
    categories. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern
    Recognition (CVPR06), Volume 2, pp.  2169–2178.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun, Y. et al. (1989). Generalization and network design
    strategies. Connectionism in perspective, 143–155.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2017) Lee, H., F. M. Troschel, S. Tajmir, G. Fuchs, J. Mario, F. J.
    Fintelmann, and S. Do (2017, Aug). Pixel-level deep segmentation: Artificial intelligence
    quantifies muscle on computed tomography for body morphometric analysis. Journal
    of Digital Imaging 30(4), 487–498.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2021) Lee, J., J. Yi, C. Shin, and S. Yoon (2021). Bbam: Bounding
    box attribution map for weakly supervised semantic and instance segmentation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  2643–2652.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Park (2020) Lee, Y. and J. Park (2020). Centermask: Real-time anchor-free
    instance segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), pp.  13903–13912.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lempitsky et al. (2011) Lempitsky, V., A. Vedaldi, and A. Zisserman (2011).
    Pylon model for semantic segmentation. In J. Shawe-Taylor, R. S. Zemel, P. L.
    Bartlett, F. Pereira, and K. Q. Weinberger (Eds.), Advances in Neural Information
    Processing Systems 24, pp.  1485–1493\. Curran Associates, Inc.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018) Li, H., P. Xiong, J. An, and L. Wang (2018). Pyramid attention
    network for semantic segmentation. In British Machine Vision Conference 2018,
    BMVC 2018, Newcastle, UK, September 3-6, 2018, pp.  285\. BMVA Press.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Li, H., P. Xiong, H. Fan, and J. Sun (2019). Dfanet: Deep
    feature aggregation for real-time semantic segmentation. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  9522–9531.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Li, R., S. Zheng, C. Zhang, C. Duan, L. Wang, and P. M. Atkinson
    (2021). Abcnet: Attentive bilateral contextual network for efficient semantic
    segmentation of fine-resolution remotely sensed imagery. ISPRS Journal of Photogrammetry
    and Remote Sensing 181, 84–98.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li, X., Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu (2019).
    Expectation-maximization attention networks for semantic segmentation.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Li, Y., Z. Kuang, L. Liu, Y. Chen, and W. Zhang (2021). Pseudo-mask
    matters in weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp.  6964–6973.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Li, Z., Y. Sun, L. Zhang, and J. Tang (2021). Ctnet: Context-based
    tandem network for semantic segmentation. IEEE Transactions on Pattern Analysis
    and Machine Intelligence.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2016) Liang, X., X. Shen, J. Feng, L. Lin, and S. Yan (2016).
    Semantic object parsing with graph lstm. In B. Leibe, J. Matas, N. Sebe, and M. Welling
    (Eds.), Computer Vision – ECCV 2016, pp.  125–143.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2016) Lin, D., J. Dai, J. Jia, K. He, and J. Sun (2016). Scribblesup:
    Scribble-supervised convolutional networks for semantic segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp.  3159–3167.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2018) Lin, D., Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang (2018,
    September). Multi-scale context intertwining for semantic segmentation. In The
    European Conference on Computer Vision (ECCV).
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2016) Lin, G., C. Shen, A. v. d. Hengel, and I. Reid (2016, June).
    Efficient piecewise training of deep structured models for semantic segmentation.
    In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 
    3194–3203.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Lin, T.-Y., M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
    P. Dollár, and C. L. Zitnick (2014). Microsoft coco: Common objects in context.
    In European conference on computer vision, pp.  740–755. Springer.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) Liu, W., D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu,
    and A. C. Berg (2016). SSD: single shot multibox detector. In Computer Vision
    - ECCV 2016 - 14th European Conference, pp.  21–37.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2015) Liu, W., A. Rabinovich, and A. C. Berg (2015). Parsenet:
    Looking wider to see better.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Liu, Y., Y.-H. Wu, P.-S. Wen, Y.-J. Shi, Y. Qiu, and M.-M.
    Cheng (2020). Leveraging instance-, image-and dataset-level information for weakly
    supervised instance segmentation. IEEE Transactions on Pattern Analysis and Machine
    Intelligence.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Liu, Y., W. Zhang, and J. Wang (2021). Source-free domain
    adaptation for semantic segmentation. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp.  1215–1224.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2015) Liu, Z., X. Li, P. Luo, C. Loy, and X. Tang (2015, Dec). Semantic
    image segmentation via deep parsing network. In 2015 IEEE International Conference
    on Computer Vision (ICCV), pp.  1377–1385.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2014) Long, J., E. Shelhamer, and T. Darrell (2014). Fully convolutional
    networks for semantic segmentation. CoRR abs/1411.4038.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe (2004) Lowe, D. G. (2004, Nov). Distinctive image features from scale-invariant
    keypoints. International Journal of Computer Vision 60(2), 91–110.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2021) Lu, H., L. Fang, M. Lin, and Z. Deng (2021). Feature enhanced
    projection network for zero-shot semantic segmentation. In 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp.  14011–14017\. IEEE.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2017) Luo, P., G. Wang, L. Lin, and X. Wang (2017, Oct). Deep dual
    learning for semantic image segmentation. In 2017 IEEE International Conference
    on Computer Vision (ICCV), pp.  2737–2745.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marin et al. (2019) Marin, D., Z. He, P. Vajda, P. Chatterjee, S. Tsai, F. Yang,
    and Y. Boykov (2019). Efficient segmentation: Learning downsampling near semantic
    boundaries. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp.  2131–2141.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minaee et al. (2020) Minaee, S., Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz,
    and D. Terzopoulos (2020). Image segmentation using deep learning: A survey.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mičušĺík and Košecká (2009) Mičušĺík, B. and J. Košecká (2009, Sep.). Semantic
    segmentation of street scenes by superpixel co-occurrence and 3d geometry. In
    2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops,
    pp.  625–632.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montillo et al. (2011) Montillo, A., J. Shotton, J. Winn, J. E. Iglesias, D. Metaxas,
    and A. Criminisi (2011). Entangled decision forests and their application for
    semantic segmentation of ct images. In Information Processing in Medical Imaging,
    Berlin, Heidelberg, pp.  184–196\. Springer Berlin Heidelberg.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mottaghi et al. (2014) Mottaghi, R., X. Chen, X. Liu, N.-G. Cho, S.-W. Lee,
    S. Fidler, R. Urtasun, and A. Yuille (2014). The role of context for object detection
    and semantic segmentation in the wild. In IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ning et al. (2005) Ning, F., D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and
    P. E. Barbano (2005, Sept). Toward automatic phenotyping of developing embryos
    from videos. IEEE Transactions on Image Processing 14(9), 1360–1371.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noh et al. (2015) Noh, H., S. Hong, and B. Han (2015). Learning deconvolution
    network for semantic segmentation. In Proceedings of the 2015 IEEE International
    Conference on Computer Vision (ICCV), ICCV ’15, Washington, DC, USA, pp.  1520–1528.
    IEEE Computer Society.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oktay et al. (2018) Oktay, O., J. Schlemper, L. L. Folgoc, M. C. H. Lee, M. P.
    Heinrich, K. Misawa, K. Mori, S. G. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker,
    and D. Rueckert (2018). Attention u-net: Learning where to look for the pancreas.
    CoRR abs/1804.03999.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orsic et al. (2019) Orsic, M., I. Kreso, P. Bevandic, and S. Segvic (2019).
    In defense of pre-trained imagenet architectures for real-time semantic segmentation
    of road-driving images. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp.  12607–12616.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oršić and Šegvić (2021) Oršić, M. and S. Šegvić (2021). Efficient semantic segmentation
    with pyramidal fusion. Pattern Recognition 110, 107611.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2020) Pan, F., I. Shin, F. Rameau, S. Lee, and I. S. Kweon (2020).
    Unsupervised intra-domain adaptation for semantic segmentation through self-supervision.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  3764–3773.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pastore et al. (2021) Pastore, G., F. Cermelli, Y. Xian, M. Mancini, Z. Akata,
    and B. Caputo (2021). A closer look at self-training for zero-label semantic segmentation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  2693–2702.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. (2015) Pathak, D., P. Krahenbuhl, and T. Darrell (2015). Constrained
    convolutional neural networks for weakly supervised segmentation. In Proceedings
    of the IEEE international conference on computer vision, pp.  1796–1804.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2017) Peng, C., X. Zhang, G. Yu, G. Luo, and J. Sun (2017, July).
    Large kernel matters — improve semantic segmentation by global convolutional network.
    In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 
    1743–1751.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2020) Peng, S., W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou (2020).
    Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp.  8533–8542.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perreault et al. (2021) Perreault, H., G.-A. Bilodeau, N. Saunier, and M. Héritier
    (2021). Centerpoly: real-time instance segmentation using bounding polygons. In
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    2982–2991.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pfeuffer et al. (2019) Pfeuffer, A., K. Schulz, and K. Dietmayer (2019). Semantic
    segmentation of video sequences with convolutional lstms. CoRR abs/1905.01058.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinheiro and Collobert (2014) Pinheiro, P. O. and R. Collobert (2014). Recurrent
    convolutional neural networks for scene labeling. In Proceedings of the 31st International
    Conference on International Conference on Machine Learning - Volume 32, ICML’14,
    pp. I–82–I–90\. JMLR.org.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinheiro and Collobert (2015) Pinheiro, P. O. and R. Collobert (2015). From
    image-level to pixel-level labeling with convolutional networks. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp.  1713–1721.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinheiro et al. (2015) Pinheiro, P. O., R. Collobert, and P. Dollar (2015).
    Learning to segment object candidates. In C. Cortes, N. D. Lawrence, D. D. Lee,
    M. Sugiyama, and R. Garnett (Eds.), Advances in Neural Information Processing
    Systems 28, pp. 1990–1998\. Curran Associates, Inc.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinheiro et al. (2016) Pinheiro, P. O., T.-Y. Lin, R. Collobert, and P. Dollár
    (2016). Learning to refine object segments. In B. Leibe, J. Matas, N. Sebe, and
    M. Welling (Eds.), Computer Vision – ECCV 2016, Cham, pp.  75–91\. Springer International
    Publishing.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prest et al. (2012) Prest, A., C. Leistner, J. Civera, C. Schmid, and V. Ferrari
    (2012, June). Learning object class detectors from weakly annotated video. In
    2012 IEEE Conference on Computer Vision and Pattern Recognition, pp.  3282–3289.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R. Zhang et al. (2020) R. Zhang, Z. Tian, C. Shen, M. You, and Y. Yan (2020).
    Mask encoding for single shot instance segmentation. In 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pp.  10223–10232.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravì et al. (2016) Ravì, D., M. Bober, G. Farinella, M. Guarnera, and S. Battiato
    (2016, April). Semantic segmentation of images exploiting dct based features and
    random forest. Pattern Recogn. 52(C), 260–273.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. (2016) Redmon, J., S. Divvala, R. Girshick, and A. Farhadi (2016,
    jun). You only look once: Unified, real-time object detection. In 2016 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, pp. 
    779–788\. IEEE Computer Society.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., K. He, R. Girshick, and J. Sun (2015). Faster r-cnn:
    Towards real-time object detection with region proposal networks. In C. Cortes,
    N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Eds.), Advances in Neural
    Information Processing Systems 28, pp. 91–99.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richter et al. (2016) Richter, S. R., V. Vineet, S. Roth, and V. Koltun (2016).
    Playing for data: Ground truth from computer games. In European conference on
    computer vision, pp.  102–118. Springer.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger, O., P. Fischer, and T. Brox (2015).
    U-net: Convolutional networks for biomedical image segmentation. In Medical Image
    Computing and Computer-Assisted Intervention – MICCAI 2015, Cham, pp.  234–241\.
    Springer International Publishing.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ros et al. (2016a) Ros, G., L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez
    (2016a). The SYNTHIA Dataset: A large collection of synthetic images for semantic
    segmentation of urban scenes.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ros et al. (2016b) Ros, G., L. Sellart, J. Materzynska, D. Vazquez, and A. M.
    Lopez (2016b). The synthia dataset: A large collection of synthetic images for
    semantic segmentation of urban scenes. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp.  3234–3243.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosas-Arias et al. (2021) Rosas-Arias, L., G. Benitez-Garcia, J. Portillo-Portillo,
    G. Sánchez-Pérez, and K. Yanai (2021). Fast and accurate real-time semantic segmentation
    with dilated asymmetric convolutions. In 2020 25th International Conference on
    Pattern Recognition (ICPR), pp.  2264–2271\. IEEE.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenholtz (2016) Rosenholtz, R. (2016). Capabilities and limitations of peripheral
    vision. Annual Review of Vision Science 2(1), 437–457.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rother et al. (2004) Rother, C., V. Kolmogorov, and A. Blake (2004). Grabcut:
    Interactive foreground extraction using iterated graph cuts. In ACM transactions
    on graphics (TOG), Volume 23, pp. 309–314\. ACM.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saffar et al. (2018) Saffar, M. H., M. Fayyaz, M. Sabokrou, and M. Fathy (2018).
    Semantic video segmentation: A review on recent approaches.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha and Chakraborty (2018) Saha, M. and C. Chakraborty (2018, May). Her2net:
    A deep framework for semantic segmentation and classification of cell membranes
    and nuclei in breast cancer evaluation. IEEE Transactions on Image Processing 27(5),
    2189–2200.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saporta et al. (2021) Saporta, A., T.-H. Vu, M. Cord, and P. Pérez (2021, October).
    Multi-target adversarial frameworks for domain adaptation in semantic segmentation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),
    pp.  9072–9081.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shelhamer et al. (2017) Shelhamer, E., J. Long, and T. Darrell (2017, April).
    Fully convolutional networks for semantic segmentation. IEEE Trans. Pattern Anal.
    Mach. Intell. 39(4), 640–651.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Shen, Y., L. Cao, Z. Chen, F. Lian, B. Zhang, C. Su, Y. Wu,
    F. Huang, and R. Ji (2021). Toward joint thing-and-stuff mining for weakly supervised
    panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp.  16694–16705.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Shen, Y., L. Cao, Z. Chen, B. Zhang, C. Su, Y. Wu, F. Huang,
    and R. Ji (2021). Parallel detection-and-segmentation learning for weakly supervised
    instance segmentation. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp.  8198–8208.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2018) Shi, H., H. Li, F. Meng, Q. Wu, L. Xu, and K. N. Ngan (2018,
    Oct). Hierarchical parsing net: Semantic scene parsing from global scene to objects.
    IEEE Transactions on Multimedia 20(10), 2670–2682.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2021) Shin, I., D.-J. Kim, J. W. Cho, S. Woo, K. Park, and I. S.
    Kweon (2021). Labor: Labeling only if required for domain adaptive semantic segmentation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 
    8588–8598.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shotton et al. (2008) Shotton, J., M. Johnson, and R. Cipolla (2008, June).
    Semantic texton forests for image categorization and segmentation. In 2008 IEEE
    Conference on Computer Vision and Pattern Recognition, pp.  1–8.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuai et al. (2016) Shuai, B., Z. Zuo, B. Wang, and G. Wang (2016, June). Dag-recurrent
    neural networks for scene labeling. In 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), pp.  3620–3629.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siam et al. (2017) Siam, M., S. Elkerdawy, M. Jägersand, and S. Yogamani (2017).
    Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges.
    In 20th IEEE International Conference on Intelligent Transportation Systems, ITSC
    2017, Yokohama, Japan, October 16-19, 2017, pp.  1–8.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) Simonyan, K. and A. Zisserman (2015). Very deep
    convolutional networks for large-scale image recognition. In Proc. of Workshop
    at Int. Conf. on Learning Representations (ICLR) Workshops.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun (2019) Sun, J. (2019). Mask-yolo: Efficient instance-level segmentation
    network based on yolo-v2. GitHub Repository.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2021) Sun, K., H. Shi, Z. Zhang, and Y. Huang (2021). Ecs-net:
    Improving weakly supervised semantic segmentation by using connections between
    class activation maps. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp.  7283–7292.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Szegedy, C., W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich (2015). Going deeper with convolutions.
    In Computer Vision and Pattern Recognition (CVPR).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teichmann and Cipolla (2018) Teichmann, M. T. T. and R. Cipolla (2018). Convolutional
    crfs for semantic segmentation. CoRR abs/1805.04777.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoma (2016) Thoma, M. (2016). A survey of semantic segmentation. arXiv preprint
    arXiv:1602.06541.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2021) Tian, Z., C. Shen, X. Wang, and H. Chen (2021). Boxinst:
    High-performance instance segmentation with box annotations. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  5443–5452.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tighe and Lazebnik (2010) Tighe, J. and S. Lazebnik (2010). Superparsing: Scalable
    nonparametric image parsing with superpixels. In European Conference on Computer
    Vision – ECCV 2010, pp. 352–365.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ulku et al. (2019) Ulku, I., P. Barmpoutis, T. Stathaki, and E. Akagündüz (2019).
    Comparison of single channel indices for u-net-based segmentation of vegetation
    in satellite images. In 12th International Conference on Machine Vision (ICMV19),
    SPIE Proceedings.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ulusoy and Bishop (2005) Ulusoy, I. and C. M. Bishop (2005). Generative versus
    discriminative methods for object recognition. In 2005 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR 2005), 20-26 June 2005, San Diego,
    CA, USA, pp.  258–265.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Varma et al. (2018) Varma, G., A. Subramanian, A. M. Namboodiri, M. Chandraker,
    and C. V. Jawahar (2018). IDD: A dataset for exploring problems of autonomous
    navigation in unconstrained environments. CoRR abs/1811.10200.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin (2017). Attention is all you need. CoRR abs/1706.03762.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vezhnevets et al. (2011) Vezhnevets, A., V. Ferrari, and J. M. Buhmann (2011,
    Nov). Weakly supervised semantic segmentation with a multi-image model. In 2011
    International Conference on Computer Vision, pp. 643–650.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visin et al. (2016) Visin, F., A. Romero, K. Cho, M. Matteucci, M. Ciccone,
    K. Kastner, Y. Bengio, and A. Courville (2016, June). Reseg: A recurrent neural
    network-based model for semantic segmentation. In 2016 IEEE Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW), pp.  426–433.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2019) Vu, T.-H., H. Jain, M. Bucher, M. Cord, and P. Pérez (2019).
    Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  2517–2526.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wang, D., G. Hu, and C. Lyu (2020, May). Frnet: an end-to-end
    feature refinement neural network for medical image segmentation. The Visual Computer.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, K., J. H. Liew, Y. Zou, D. Zhou, and J. Feng (2019).
    Panet: Few-shot image semantic segmentation with prototype alignment. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp.  9197–9206.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2015) Wang, P., X. Shen, Z. Lin, S. Cohen, B. Price, and A. Yuille
    (2015, Dec). Joint object and part segmentation using deep learned potentials.
    In 2015 IEEE International Conference on Computer Vision (ICCV), pp.  1573–1581.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Wang, Q., D. Dai, L. Hoyer, O. Fink, and L. Van Gool (2021).
    Domain adaptive semantic segmentation with self-supervised depth estimation. arXiv
    preprint arXiv:2104.13613.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, X., T. Kong, C. Shen, Y. Jiang, and L. Li (2019).
    Solo: Segmenting objects by locations.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wang, X., R. Zhang, T. Kong, L. Li, and C. Shen (2020).
    Solov2: Dynamic and fast instance segmentation. arXiv preprint arXiv:2003.10152.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Wang, Y., J. Peng, and Z. Zhang (2021). Uncertainty-aware
    pseudo label refinery for domain adaptive semantic segmentation. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp.  9092–9101.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Wang, Y., J. Zhang, M. Kan, S. Shan, and X. Chen (2020).
    Self-supervised equivariant attention mechanism for weakly supervised semantic
    segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp.  12275–12284.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) Wu, X., Z. Wu, H. Guo, L. Ju, and S. Wang (2021). Dannet:
    A one-stage domain adaptation network for unsupervised nighttime semantic segmentation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp.  15769–15778.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2019) Xia, X., Q. Lu, and X. Gu (2019, jun). Exploring an easy
    way for imbalanced data sets in semantic image segmentation. Journal of Physics:
    Conference Series 1213(2), 022003.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xian et al. (2019) Xian, Y., S. Choudhury, Y. He, B. Schiele, and Z. Akata (2019).
    Semantic projection network for zero-and few-label semantic segmentation. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  8256–8265.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao and Quan (2009) Xiao, J. and L. Quan (2009, Sep.). Multiple view semantic
    segmentation for street view images. pp.  686–693.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Xie, E., P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen,
    and P. Luo (2020). Polarmask: Single shot instance segmentation with polar representation.
    In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    pp.  12190–12199.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Xie, G.-S., J. Liu, H. Xiong, and L. Shao (2021). Scale-aware
    graph neural network for few-shot semantic segmentation. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  5475–5484.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020) Xie, Q., M. T. Luong, E. Hovy, and Q. V. Le (2020). Self-training
    with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pp.  10684–10695.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2015) Xu, J., A. G. Schwing, and R. Urtasun (2015). Learning to segment
    under various forms of weak supervision. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp.  3781–3790.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Xu, L., W. Ouyang, M. Bennamoun, F. Boussaid, F. Sohel, and
    D. Xu (2021). Leveraging auxiliary tasks with affinity learning for weakly supervised
    semantic segmentation. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp.  6984–6993.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Xu, W., H. Wang, F. Qi, and C. Lu (2019). Explicit shape encoding
    for real-time instance segmentation. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pp.  5168–5177.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Xu, W., H. Wang, F. Qi, and C. Lu (2019). Explicit shape encoding
    for real-time instance segmentation. In 2019 IEEE/CVF International Conference
    on Computer Vision (ICCV), pp.  5167–5176.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2012) Yang, Y., S. Hallman, D. Ramanan, and C. C. Fowlkes (2012).
    Layered object models for image segmentation. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 34(9), 1731–1743.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2012) Yao, J., S. Fidler, and R. Urtasun (2012, June). Describing
    the scene as a whole: Joint object detection, scene classification and semantic
    segmentation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition,
    pp.  702–709.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2021) Yao, Y., T. Chen, G.-S. Xie, C. Zhang, F. Shen, Q. Wu, Z. Tang,
    and J. Zhang (2021). Non-salient region object mining for weakly supervised semantic
    segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp.  2623–2632.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Yu, C., J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang (2018).
    Bisenet: Bilateral segmentation network for real-time semantic segmentation. In
    Proceedings of the European conference on computer vision (ECCV), pp.  325–341.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2018) Yu, C., J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang (2018,
    June). Learning a discriminative feature network for semantic segmentation. In
    2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  1857–1866.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu and Koltun (2015) Yu, F. and V. Koltun (2015). Multi-scale context aggregation
    by dilated convolutions. CoRR abs/1511.07122.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Yu, H., Y. Zhengeng, L. Tan, Y. Wang, W. Sun, M. Sun, and
    Y. Tang (2018, 05). Methods and datasets on semantic segmentation: A review. Neurocomputing 304.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2020) Yuan, Y., X. Chen, and J. Wang (2020). Object-contextual
    representations for semantic segmentation. In Computer Vision–ECCV 2020: 16th
    European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16,
    pp.  173–190. Springer.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaitoun and Aqel (2015) Zaitoun, N. M. and M. J. Aqel (2015). Survey on image
    segmentation techniques. Procedia Computer Science 65, 797 – 806. International
    Conference on Communications, management, and Information technology (ICCMIT’2015).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Zhang, F., P. Torr, R. Ranftl, and S. Richter (2021). Looking
    beyond single images for contrastive semantic segmentation learning. Advances
    in Neural Information Processing Systems 34.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Zhang, H., K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi,
    and A. Agrawal (2018, June). Context encoding for semantic segmentation. In The
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Zhang, H., H. Zhang, C. Wang, and J. Xie (2019, June). Co-occurrent
    features in semantic segmentation. In The IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR).
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Zhang, X., H. Xu, H. Mo, J. Tan, C. Yang, and W. Ren (2020).
    Dcnas: Densely connected neural architecture search for semantic image segmentation.
    CoRR.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, X., X. Zhou, M. Lin, and J. Sun (2018). Shufflenet:
    An extremely efficient convolutional neural network for mobile devices. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp.  6848–6856.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, Z., X. Zhang, C. Peng, X. Xue, and J. Sun (2018).
    Exfuse: Enhancing feature fusion for semantic segmentation. Lecture Notes in Computer
    Science, 273–288.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao, H., X. Qi, X. Shen, J. Shi, and J. Jia (2018). ICNet
    for real-time semantic segmentation on high-resolution images. In ECCV.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Zhao, H., J. Shi, X. Qi, X. Wang, and J. Jia (2017, July).
    Pyramid scene parsing network. In 2017 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pp.  6230–6239.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2018) Zhao, H., Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and
    J. Jia (2018). Psanet: Point-wise spatial attention network for scene parsing.
    In V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss (Eds.), Computer Vision
    – ECCV 2018, Cham, pp.  270–286\. Springer International Publishing.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Zhao, S., B. Li, X. Yue, Y. Gu, P. Xu, R. Hu, H. Chai, and
    K. Keutzer (2019). Multi-source domain adaptation for semantic segmentation. arXiv
    preprint arXiv:1910.12181.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021) Zhao, X., R. Vemulapalli, P. A. Mansfield, B. Gong, B. Green,
    L. Shapira, and Y. Wu (2021). Contrastive learning for label efficient semantic
    segmentation. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp.  10623–10633.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2015) Zheng, S., S. Jayasumana, B. Romera-Paredes, V. Vineet,
    Z. Su, D. Du, C. Huang, and P. H. S. Torr (2015, Dec). Conditional random fields
    as recurrent neural networks. In 2015 IEEE International Conference on Computer
    Vision (ICCV), pp.  1529–1537.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng and Yang (2021) Zheng, Z. and Y. Yang (2021). Rectifying pseudo label
    learning via uncertainty estimation for domain adaptive semantic segmentation.
    International Journal of Computer Vision 129(4), 1106–1120.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2016) Zhong, G., Y.-H. Tsai, and M.-H. Yang (2016). Weakly-supervised
    video scene co-parsing. In Asian Conference on Computer Vision, pp.  20–36\. Springer.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2016) Zhong, Z., J. Li, W. Cui, and H. Jiang (2016, July). Fully
    convolutional networks for building and road extraction: Preliminary results.
    In 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 
    1591–1594.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Zhou, B., A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba
    (2016). Learning deep features for discriminative localization. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp.  2921–2929.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Zhou, B., H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso,
    and A. Torralba (2019). Semantic understanding of scenes through the ade20k dataset.
    International Journal of Computer Vision 127(3), 302–321.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zoph et al. (2020) Zoph, B., G. Ghiasi, T. Lin, Y. Cui, H. Liu, E. D. Cubuk,
    and Q. Le (2020). Rethinking pre-training and self-training. In H. Larochelle,
    M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph et al. (2018) Zoph, B., V. Vasudevan, J. Shlens, and Q. V. Le (2018). Learning
    transferable architectures for scalable image recognition. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp.  8697–8710.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zuo and Drummond (2017) Zuo, Y. and T. Drummond (2017, 13–15 Nov). Fast residual
    forests: Rapid ensemble learning for semantic segmentation. In S. Levine, V. Vanhoucke,
    and K. Goldberg (Eds.), Proceedings of the 1st Annual Conference on Robot Learning,
    Volume 78 of Proceedings of Machine Learning Research, pp.  27–36\. PMLR.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
