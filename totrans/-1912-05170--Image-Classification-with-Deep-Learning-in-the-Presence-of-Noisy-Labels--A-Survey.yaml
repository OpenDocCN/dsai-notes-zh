- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:03:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.05170] Image Classification with Deep Learning in the Presence of Noisy
    Labels: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.05170] 带噪声标签的深度学习图像分类：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.05170](https://ar5iv.labs.arxiv.org/html/1912.05170)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.05170](https://ar5iv.labs.arxiv.org/html/1912.05170)
- en: 'Image Classification with Deep Learning in the Presence of Noisy Labels: A
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带噪声标签的深度学习图像分类：综述
- en: Görkem Algan¹¹1ASELSAN, Ankara²²2Middle East Technical University, Electrical-Electronics
    Engineering, Ankara [e162565@metu.edu.tr](mailto:e162565@metu.edu.tr) Ilkay Ulusoy³³3Middle
    East Technical University, Electrical-Electronics Engineering, Ankara [ilkay@metu.edu.tr](mailto:ilkay@metu.edu.tr)
    ASELSAN, Balikhisar mah. Cankiri bulvari 7.km No:89, 06750, Ankara, Turkey METU,
    Universiteler mah. Dumlupinar mah.Electrical-Electronics Engineering Department
    A-410, 06800, Ankara, Turkey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Görkem Algan¹¹1ASELSAN, Ankara²²2中东技术大学，电气电子工程系，安卡拉 [e162565@metu.edu.tr](mailto:e162565@metu.edu.tr)
    Ilkay Ulusoy³³3中东技术大学，电气电子工程系，安卡拉 [ilkay@metu.edu.tr](mailto:ilkay@metu.edu.tr)
    ASELSAN，Balikhisar mah. Cankiri bulvari 7.km No:89, 06750, 安卡拉，土耳其 METU，Universiteler
    mah. Dumlupinar mah.电气电子工程系 A-410, 06800, 安卡拉，土耳其
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Image classification systems recently made a giant leap with the advancement
    of deep neural networks. However, these systems require an excessive amount of
    labeled data to be adequately trained. Gathering a correctly annotated dataset
    is not always feasible due to several factors, such as the expensiveness of the
    labeling process or difficulty of correctly classifying data, even for the experts.
    Because of these practical challenges, label noise is a common problem in real-world
    datasets, and numerous methods to train deep neural networks with label noise
    are proposed in the literature. Although deep neural networks are known to be
    relatively robust to label noise, their tendency to overfit data makes them vulnerable
    to memorizing even random noise. Therefore, it is crucial to consider the existence
    of label noise and develop counter algorithms to fade away its adverse effects
    to train deep neural networks efficiently. Even though an extensive survey of
    machine learning techniques under label noise exists, the literature lacks a comprehensive
    survey of methodologies centered explicitly around deep learning in the presence
    of noisy labels. This paper aims to present these algorithms while categorizing
    them into one of the two subgroups: noise model based and noise model free methods.
    Algorithms in the first group aim to estimate the noise structure and use this
    information to avoid the adverse effects of noisy labels. Differently, methods
    in the second group try to come up with inherently noise robust algorithms by
    using approaches like robust losses, regularizers or other learning paradigms.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类系统最近随着深度神经网络的进步取得了巨大的飞跃。然而，这些系统需要大量标注的数据才能进行充分训练。由于标注过程的高成本或即使是专家也难以正确分类数据等因素，收集正确注释的数据集并不总是可行的。由于这些实际挑战，标签噪声在现实世界的数据集中是一个常见问题，文献中提出了许多用于处理标签噪声的深度神经网络训练方法。尽管深度神经网络被认为对标签噪声具有相对的鲁棒性，但它们对数据的过拟合倾向使其容易记住即使是随机噪声。因此，考虑标签噪声的存在并开发对抗算法以消除其不利影响，以有效训练深度神经网络是至关重要的。尽管关于标签噪声下的机器学习技术已有广泛的调查，但文献中缺乏专门围绕噪声标签下深度学习方法的全面调查。本文旨在介绍这些算法，并将其归为两大类：基于噪声模型的方法和无噪声模型的方法。第一组算法旨在估计噪声结构，并利用这些信息来避免噪声标签的不利影响。与此不同，第二组方法试图通过使用稳健的损失函数、正则化器或其他学习范式来设计固有的噪声鲁棒算法。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'deep learning, label noise, classification with noise, noise robust, noise
    tolerant^†^†journal: Journal of Pattern Recognition'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、标签噪声、带噪声的分类、噪声鲁棒、噪声容忍^†^†期刊：模式识别杂志
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancement in deep learning has led to great improvements on many different
    domains, such as image classification [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    object detection [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], semantic segmentation
    [[7](#bib.bib7), [8](#bib.bib8)] and others. Despite their impressive ability
    for representation learning [[9](#bib.bib9), [10](#bib.bib10)], it is shown that
    these powerful models can overfit to even complete random noise [[11](#bib.bib11)].
    Various works are devoted to explain this phenomenon [[12](#bib.bib12), [13](#bib.bib13)],
    yet regularizing deep neural networks (DNNs) while avoiding overfitting stays
    to be an important challenge. It gets even more crucial when there exists noise
    in data. Therefore, various methods are proposed in the literature to train deep
    neural networks effectively in the presence of noise.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在深度学习领域的进展导致了许多不同领域的巨大改进，例如图像分类[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]、目标检测[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]、语义分割[[7](#bib.bib7), [8](#bib.bib8)]等。尽管这些强大的模型在表示学习[[9](#bib.bib9),
    [10](#bib.bib10)]方面表现出色，但研究表明，这些模型可能会对完全随机噪声[[11](#bib.bib11)]产生过拟合。虽然有很多工作致力于解释这一现象[[12](#bib.bib12),
    [13](#bib.bib13)]，但在避免过拟合的同时对深度神经网络（DNNs）进行正则化仍然是一个重要的挑战。特别是在数据中存在噪声时，这一点变得更加重要。因此，文献中提出了各种方法来有效地训练深度神经网络以应对噪声。
- en: 'There are two kinds of noise in the literature: feature noise and label noise
    [[14](#bib.bib14)]. Feature noise corresponds to the corruption in observed data
    features, while label noise means the change of label from its actual class. Even
    though both noise types may cause a significant decrease in the performance [[15](#bib.bib15),
    [16](#bib.bib16)], label noise is considered to be more harmful [[14](#bib.bib14),
    [17](#bib.bib17)] and shown to deteriorate the performance of classification systems
    in a broad range of problems [[14](#bib.bib14), [18](#bib.bib18), [19](#bib.bib19)].
    This is due to several factors; the label is unique for each data while features
    are multiple, and the importance of each feature varies while the label always
    has a significant impact [[15](#bib.bib15)]. This work focuses on label noise;
    therefore, noise and label noise is used synonymously throughout the article.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中存在两种类型的噪声：特征噪声和标签噪声[[14](#bib.bib14)]。特征噪声指的是观测数据特征中的损坏，而标签噪声则是指标签从其实际类别的变化。尽管这两种噪声类型都可能导致性能显著下降[[15](#bib.bib15),
    [16](#bib.bib16)]，但标签噪声被认为更具危害[[14](#bib.bib14), [17](#bib.bib17)]，并且被证明会在广泛的问题范围内恶化分类系统的性能[[14](#bib.bib14),
    [18](#bib.bib18), [19](#bib.bib19)]。这是由于几个因素；标签对于每个数据都是唯一的，而特征是多样的，每个特征的重要性也不同，而标签始终有显著影响[[15](#bib.bib15)]。这项工作集中在标签噪声上，因此，噪声和标签噪声在整个文章中是同义使用的。
- en: The necessity of an excessive amount of labeled data for supervised learning
    is a significant drawback since it requires an expensive dataset collection and
    labeling process. To overcome this issue, cheaper alternatives have emerged. For
    example, an almost unlimited amount of data can be collected from the web via
    search engines or social media. Similarly, the labeling process can be crowdsourced
    with the help of systems like Amazon Mechanical Turk⁴⁴4http://www.mturk.com, Crowdflower⁵⁵5http://crowdflower.com,
    which decrease the cost of labeling notably. Another widely used approach is to
    label data with automated systems. However, all these approaches led to a common
    problem; label noise. Besides these methods, label noise can occur even in the
    case of expert annotators. Labelers may lack the necessary experience, or data
    can be too complex to be correctly classified, even for the experts. Moreover,
    label noise can also be introduced to data for adversarial poisoning purposes
    [[20](#bib.bib20), [21](#bib.bib21)]. Being a natural outcome of dataset collection
    and labeling process makes label noise robust algorithms an essential topic for
    the development of efficient computer vision systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习而言，大量标注数据的必要性是一个显著的缺点，因为这需要昂贵的数据集收集和标注过程。为了解决这个问题，出现了更便宜的替代方案。例如，几乎可以从网络上通过搜索引擎或社交媒体收集到无限量的数据。类似地，标注过程可以借助像**Amazon
    Mechanical Turk**⁴⁴[http://www.mturk.com](http://www.mturk.com)和**Crowdflower**⁵⁵[http://crowdflower.com](http://crowdflower.com)这样的系统进行众包，这大大降低了标注成本。另一种广泛使用的方法是使用自动化系统进行标注。然而，所有这些方法都带来了一个共同的问题：标注噪声。除了这些方法之外，即使是专家注释者也可能出现标注噪声。标注者可能缺乏必要的经验，或者数据过于复杂，即使对专家来说也难以正确分类。此外，标注噪声还可能由于对抗性毒化目的而引入数据中[[20](#bib.bib20),
    [21](#bib.bib21)]。作为数据集收集和标注过程的自然结果，使得标注噪声鲁棒算法成为开发高效计算机视觉系统的一个重要课题。
- en: Supervised learning with label noise is an old phenomenon with three decades
    of history [[22](#bib.bib22)]. An extensive survey about relatively old machine
    learning techniques under label noise is available [[15](#bib.bib15), [16](#bib.bib16)].
    However, no work is proposed to provide a comprehensive survey on classification
    methods centered around deep learning in the presence of label noise. This work
    focuses explicitly on filling this absence. Even though deep networks are considered
    to be relatively robust to label noise [[9](#bib.bib9), [10](#bib.bib10)], they
    have an immense capacity to overfit data [[11](#bib.bib11)]. Therefore, preventing
    DNNs to overfit noisy data is very important, especially for fail-safe applications,
    such as automated medical diagnosis systems. Considering the significant success
    of deep learning over its alternatives, it is a topic of interest, and many works
    are presented in the literature. Throughout the paper, these methods are briefly
    explained and grouped to provide the reader with a clear overview of the literature.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 带有标注噪声的监督学习是一个已有三十年历史的老现象[[22](#bib.bib22)]。关于标注噪声下的相对较旧的机器学习技术已有广泛的调查[[15](#bib.bib15),
    [16](#bib.bib16)]。然而，尚未有研究提供针对标注噪声存在的深度学习分类方法的全面调查。本研究明确地集中在填补这一空白。尽管深度网络被认为对标注噪声相对鲁棒[[9](#bib.bib9),
    [10](#bib.bib10)]，但它们具有过度拟合数据的巨大能力[[11](#bib.bib11)]。因此，防止**DNNs**对噪声数据的过度拟合是非常重要的，特别是对于自动化医疗诊断系统等可靠性要求高的应用。考虑到深度学习相对于其替代方案的显著成功，这是一个值得关注的话题，文献中也有许多相关研究。本文简要地解释和归纳了这些方法，以便为读者提供清晰的文献概述。
- en: 'This paper is organized as follows. Section [2](#S2 "2 Preliminaries ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    explains several concepts that are used throughout the paper. Proposed solutions
    in literature are categorized into two major groups, and these methods are discussed
    in [section 3](#S3 "3 Noise Model Based Methods ‣ Image Classification with Deep
    Learning in the Presence of Noisy Labels: A Survey") - [section 4](#S4 "4 Noise
    Model Free Methods ‣ Image Classification with Deep Learning in the Presence of
    Noisy Labels: A Survey"). In [section 5](#S5 "5 Experiments ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") widely used experimental
    setups are presented, and leaderboard on a benchmarking dataset is provided. Finally,
    [section 6](#S6 "6 Conclusion ‣ Image Classification with Deep Learning in the
    Presence of Noisy Labels: A Survey") concludes the paper.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '本文组织结构如下。第[2](#S2 "2 Preliminaries ‣ Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey")节解释了文中使用的几个概念。文献中提出的解决方案被分为两个主要组，这些方法在[第3节](#S3
    "3 Noise Model Based Methods ‣ Image Classification with Deep Learning in the
    Presence of Noisy Labels: A Survey") - [第4节](#S4 "4 Noise Model Free Methods ‣
    Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")中讨论。在[第5节](#S5
    "5 Experiments ‣ Image Classification with Deep Learning in the Presence of Noisy
    Labels: A Survey")中，介绍了广泛使用的实验设置，并提供了基准数据集上的排行榜。最后，[第6节](#S6 "6 Conclusion ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")对论文进行了总结。'
- en: 2 Preliminaries
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识
- en: This section introduces necessary concepts for a better understanding of the
    paper. Firstly, the problem statement for supervised learning in the presence
    of noisy labels is given. Secondly, types of label noises are presented. Finally,
    sources of label noise are discussed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了理解本文所需的必要概念。首先，给出了在标签噪声存在下的监督学习问题陈述。其次，介绍了标签噪声的类型。最后，讨论了标签噪声的来源。
- en: 2.1 Problem Statement
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题陈述
- en: Classical supervised learning consists of an input dataset $\mathcal{S}=\{(x_{1},y_{1}),...,(x_{N},y_{N})\}\in(X,Y)^{N}$
    drawn according to an unknown distribution $\mathcal{D}$, over $(X,Y)$. The learning
    objective is to find the best mapping function $f:X\rightarrow Y$ among family
    of functions $\mathcal{F}$, where each function is parametrized by $\theta$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的监督学习由输入数据集$\mathcal{S}=\{(x_{1},y_{1}),...,(x_{N},y_{N})\}\in(X,Y)^{N}$组成，这些数据是根据一个未知分布$\mathcal{D}$从$(X,Y)$中抽取的。学习目标是在函数族$\mathcal{F}$中找到最佳映射函数$f:X\rightarrow
    Y$，其中每个函数都由$\theta$参数化。
- en: One way of evaluating the performance of a classifier is the so called loss
    function, denoted as $l:\mathcal{R}\times Y\rightarrow\mathcal{R^{+}}$. Given
    an example $(x_{i},y_{i})\in(X,Y)$, $l(f_{\theta}(x_{i}),y_{i})$ evaluates how
    good is the classifier prediction. Then, for any classifier $f$, expected risk
    is defined as follow, where E denotes the expectation over distribution $\mathcal{D}$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 评估分类器性能的一种方法是所谓的损失函数，表示为$l:\mathcal{R}\times Y\rightarrow\mathcal{R^{+}}$。给定一个示例$(x_{i},y_{i})\in(X,Y)$，$l(f_{\theta}(x_{i}),y_{i})$评估分类器预测的准确性。然后，对于任何分类器$f$，期望风险定义如下，其中E表示对分布$\mathcal{D}$的期望。
- en: '|  | $R_{l,\mathcal{D}}(f_{\theta})=E_{\mathcal{D}}[l(f_{\theta}(x),y)]$ |  |
    (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{l,\mathcal{D}}(f_{\theta})=E_{\mathcal{D}}[l(f_{\theta}(x),y)]$ |  |
    (1) |'
- en: Since it is not generally feasible to have complete knowledge over distribution
    $\mathcal{D}$, as an approximation, the empirical risk is used.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通常无法对分布$\mathcal{D}$有完全了解，因此作为一种近似，使用了经验风险。
- en: '|  | $\hat{R}_{l,\mathcal{D}}(f_{\theta})=\dfrac{1}{N}\sum_{i=1}^{N}l(f_{\theta}(x_{i}),y_{i})$
    |  | (2) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{R}_{l,\mathcal{D}}(f_{\theta})=\dfrac{1}{N}\sum_{i=1}^{N}l(f_{\theta}(x_{i}),y_{i})$
    |  | (2) |'
- en: Various methods of learning a classifier may be seen as minimizing the empirical
    risk subjected to network parameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 各种学习分类器的方法可以视为在网络参数的约束下最小化经验风险。
- en: '|  | $\theta^{\star}=\underset{\theta}{\operatorname*{arg\,min}}\hat{R}_{l,\mathcal{D}}(f_{\theta})$
    |  | (3) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{\star}=\underset{\theta}{\operatorname*{arg\,min}}\hat{R}_{l,\mathcal{D}}(f_{\theta})$
    |  | (3) |'
- en: In the presence of the label noise, dataset turns into $\mathcal{S}_{n}=\{(x_{1},\tilde{y}_{1}),...,(x_{N},\tilde{y}_{N})\}\in(X,Y)^{N}$
    drawn according to a noisy distribution $\mathcal{D}_{n}$, over $(X,Y)$. Then,
    the risk minimization results in as follows.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在标签噪声存在的情况下，数据集变为$\mathcal{S}_{n}=\{(x_{1},\tilde{y}_{1}),...,(x_{N},\tilde{y}_{N})\}\in(X,Y)^{N}$，这些数据是根据噪声分布$\mathcal{D}_{n}$从$(X,Y)$中抽取的。然后，风险最小化的结果如下。
- en: '|  | $\theta_{n}^{\star}=\underset{\theta}{\operatorname*{arg\,min}}\hat{R}_{l,\mathcal{D}_{n}}(f_{\theta})$
    |  | (4) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{n}^{\star}=\underset{\theta}{\operatorname*{arg\,min}}\hat{R}_{l,\mathcal{D}_{n}}(f_{\theta})$
    |  | (4) |'
- en: As a result, obtained parameters by minimizing over $\mathcal{D}_{n}$ are different
    from desired optimal classifier parameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，通过在$\mathcal{D}_{n}$上最小化得到的参数与期望的最佳分类器参数不同。
- en: $\theta^{\star}\neq\theta_{n}^{\star}$
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta^{\star}\neq\theta_{n}^{\star}$
- en: Classical supervised learning aims to find the best estimator parameters $\theta^{\star}$
    for given distribution $\mathcal{D}$ while iterating over $\mathcal{D}$. However,
    in noisy label setup, the task is still finding $\theta^{\star}$ while working
    on distribution $\mathcal{D}_{n}$. Therefore, classical risk minimization is insufficient
    in the presence of label noise since it would result in $\theta_{n}^{\star}$.
    As a result, variations of classical risk minimization methods are proposed in
    the literature, and they will be further evaluated in the upcoming sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的监督学习旨在为给定的分布$\mathcal{D}$找到最佳估计参数$\theta^{\star}$，并在$\mathcal{D}$上迭代。然而，在有噪声标签的设置中，任务仍然是找到$\theta^{\star}$，同时处理分布$\mathcal{D}_{n}$。因此，经典风险最小化在标签噪声存在的情况下是不够的，因为这会导致$\theta_{n}^{\star}$。因此，文献中提出了经典风险最小化方法的变体，接下来的部分将进一步评估这些方法。
- en: 2.2 Label Noise Models
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 标签噪声模型
- en: 'A detailed taxonomy of label noise is provided in [[15](#bib.bib15)]. In this
    work, we follow the same taxonomy with a little abuse of notation. Label noise
    can be affected by three factors: data features, the true label of data, and the
    labeler characteristics. According to the dependence of these factors, label noise
    can be categorized into three subclasses.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声的详细分类见[[15](#bib.bib15)]。在这项工作中，我们采用了相同的分类方法，虽然稍有符号上的滥用。标签噪声受三种因素影响：数据特征、数据的真实标签以及标注者的特征。根据这些因素的依赖关系，标签噪声可以分为三类。
- en: 'Random noise is totally random and depends on neither instance features nor
    its true class. With a given probability $p_{e}$ label is changed from its true
    class. Y-dependent noise is independent of image features but depends on its class;
    $p_{e}=p(e|y)$. That means data from a particular class are more likely to be
    mislabeled. For example, in a handwritten digit recognition task, ”3” and ”8”
    are much more likely to be confused with each other rather than ”3” and ”5”. XY-dependent
    noise depends on both image features and its class; $p_{e}=p(e|x,y)$. As in the
    y-dependent case, objects from a particular class may be more likely to be mislabeled.
    Moreover, the chance of mislabeling may change according to data features. If
    an instance has similar features to another instance from another class, it is
    more likely to be mislabeled. Generating xy-dependent synthetic noise is harder
    than the previous two models; therefore, some works tried to provide a generic
    framework by either checking the complexity of data [[23](#bib.bib23)] or their
    position in feature space [[24](#bib.bib24)]. All these types of noises are illustrated
    in [Figure 1](#S2.F1 "Figure 1 ‣ 2.2 Label Noise Models ‣ 2 Preliminaries ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '随机噪声完全随机，不依赖于实例特征或其真实类别。以概率$p_{e}$，标签会从其真实类别中改变。Y-依赖噪声与图像特征无关，但依赖于其类别；$p_{e}=p(e|y)$。这意味着来自特定类别的数据更容易被错误标记。例如，在手写数字识别任务中，“3”和“8”更容易混淆，而不是“3”和“5”。XY-依赖噪声则同时依赖于图像特征和其类别；$p_{e}=p(e|x,y)$。与y-依赖的情况一样，来自特定类别的对象可能更容易被错误标记。此外，错误标记的概率可能根据数据特征而变化。如果一个实例具有类似于另一类别实例的特征，它更可能被错误标记。生成xy-依赖的合成噪声比前两种模型更难，因此一些研究尝试通过检查数据的复杂性[[23](#bib.bib23)]或它们在特征空间中的位置[[24](#bib.bib24)]来提供一个通用框架。所有这些噪声类型在[图1](#S2.F1
    "Figure 1 ‣ 2.2 Label Noise Models ‣ 2 Preliminaries ‣ Image Classification with
    Deep Learning in the Presence of Noisy Labels: A Survey")中有说明。'
- en: The case of multi-labeled data, in which each instance has multiple labels given
    by different annotators, is not considered here. In that scenario, works show
    that modeling each labeler’s characteristics and using this information during
    training significantly boosts the performance [[25](#bib.bib25)]. However, various
    characteristics of different labelers can be explained with given noise models.
    For example, in a crowd-sourced dataset, some labelers can be total spammers who
    label with a random selection [[26](#bib.bib26)]; therefore, they can be modeled
    as random noise. On the other hand, labelers with better accuracies than random
    selection can be modeled by y-dependent or xy-dependent noise. As a result, the
    labeler’s characteristic is not introduced as an extra ingredient in these definitions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在多标签数据的情况下，每个实例由不同的标注者提供多个标签，此情形在这里未考虑。在这种情况下，研究表明，建模每个标注者的特征并在训练过程中使用这些信息显著提升了性能[[25](#bib.bib25)]。然而，不同标注者的各种特征可以通过给定的噪声模型进行解释。例如，在众包数据集中，一些标注者可能是完全的垃圾标注者，他们的标注是随机选择的[[26](#bib.bib26)]；因此，他们可以被建模为随机噪声。另一方面，比随机选择更准确的标注者可以通过
    y 依赖或 xy 依赖噪声进行建模。因此，标注者的特征不会在这些定义中作为额外的成分引入。
- en: '![Refer to caption](img/ebe7c6e1b9bc43ac6ffe004d6b3c2f28.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ebe7c6e1b9bc43ac6ffe004d6b3c2f28.png)'
- en: 'Figure 1: T-SNE plot of data distribution of MNIST dataset in feature space
    for 25% noise ratio. a) clean data b) random noise c) y-dependent noise which
    is still randomly distributed in feature domain d) xy-dependent noise in locally
    concentrated form e) xy-dependent noise that is concentrated on decision boundaries'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MNIST 数据集在特征空间中的 T-SNE 数据分布图，噪声比率为 25%。a) 干净的数据 b) 随机噪声 c) y 依赖噪声，仍然在特征域中随机分布
    d) xy 依赖噪声，局部集中 e) xy 依赖噪声，集中在决策边界上
- en: 2.3 Sources of Label Noise
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 标签噪声的来源
- en: As mentioned, label noise is a natural outcome of dataset collection process
    and can occur in various domains, such as medical imaging [[27](#bib.bib27), [28](#bib.bib28),
    [25](#bib.bib25)], semantic segmentation [[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31)], crowd-sourcing [[32](#bib.bib32)], social network tagging [[33](#bib.bib33),
    [34](#bib.bib34)], financial analysis [[35](#bib.bib35)] and many more. This work
    focuses on various solutions to such problems, but it may be helpful to investigate
    the causes of label noise to understand the phenomenon better.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标签噪声是数据集收集过程的自然结果，并且可以发生在各种领域，例如医学成像[[27](#bib.bib27), [28](#bib.bib28),
    [25](#bib.bib25)]，语义分割[[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]，众包[[32](#bib.bib32)]，社交网络标签[[33](#bib.bib33),
    [34](#bib.bib34)]，金融分析[[35](#bib.bib35)]等。这项工作关注于这些问题的各种解决方案，但研究标签噪声的原因可能有助于更好地理解这一现象。
- en: Firstly, with the availability of the immense amount of data on the web and
    social media, it is a great interest of computer vision community to make use
    of that [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)]. Nevertheless, labels of these data are coming
    from messy user tags or automated systems used by search engines. These processes
    of obtaining datasets are well known to result in noisy labels.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，随着网络和社交媒体上大量数据的可用，计算机视觉社区对利用这些数据非常感兴趣[[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]。然而，这些数据的标签来自混乱的用户标签或搜索引擎使用的自动化系统。这些获取数据集的过程已知会导致噪声标签。
- en: Secondly, the dataset can be labeled by multiple experts resulting in a multi-labeled
    dataset. Each labeler has a varying level of expertise, and their opinions may
    commonly conflict with each other, which results in noisy label problem [[26](#bib.bib26)].
    There are several reasons to get data labeled by more than one expert. Opinions
    of multiple labelers can be used to double-check each other’s predictions for
    challenging datasets, or crowd-sourcing platforms can be used to decrease the
    cost of labeling for big data. Despite its cheapness, labels obtained from non-experts
    are commonly noisy with a differentiating rate of error. Some labelers even can
    be a total spammer who labels with random selection [[26](#bib.bib26)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，数据集可能由多个专家标注， resulting in a multi-labeled dataset。每个标注者的专业水平不同，他们的意见可能常常冲突，导致标签噪声问题[[26](#bib.bib26)]。有几个原因需要由多个专家标注数据。多个标注者的意见可以用来相互核对挑战性数据集的预测，或者可以使用众包平台来降低大数据标注的成本。尽管其便宜，来自非专家的标签通常带有噪声，并且错误率各不相同。一些标注者甚至可能是完全的垃圾标注者，他们的标注是随机选择的[[26](#bib.bib26)]。
- en: Thirdly, data can be too complicated for even the experts in the field, e.g.,
    medical imaging. For example, to collect gold standard validation data for retinal
    images, annotations are gathered from 6-8 different experts [[42](#bib.bib42),
    [43](#bib.bib43)]. This complexity can be due to the subjectiveness of the task
    for human experts or the lack of annotator experience. Considering the fields
    where the accurate diagnosis is of crucial importance, overcoming this noise is
    of great interest.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，数据可能过于复杂，即便是领域内的专家也难以处理，例如医学影像。举例来说，为了收集视网膜图像的黄金标准验证数据，需要从6-8位不同的专家那里收集注释[[42](#bib.bib42),
    [43](#bib.bib43)]。这种复杂性可能源于任务的主观性或注释者经验的不足。在准确诊断至关重要的领域，克服这种噪声具有重要意义。
- en: Lastly, label noise can intentionally be injected in purpose of regularizing
    [[44](#bib.bib44)] or data poisoning [[20](#bib.bib20), [21](#bib.bib21)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标签噪声可以故意注入以实现正则化[[44](#bib.bib44)]或数据中毒[[20](#bib.bib20), [21](#bib.bib21)]。
- en: 2.4 Methodologies
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 方法论
- en: There are many possible ways to group proposed methods in the literature. For
    example, one possible way to distinguish algorithms is according to their need
    for a noise-free subset of data or not. Alternatively, they can be divided according
    to the noise type they are dealing with or label type such as singly-labeled or
    multi-labeled. However, these are not handy to understand the main approaches
    behind the proposed algorithms; therefore, different sectioning is proposed as
    noise model based and noise model free methods.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出的方法可以通过多种方式进行分组。例如，可以根据算法是否需要噪声-free 数据子集来区分算法。或者，也可以根据它们处理的噪声类型或标签类型（如单标签或多标签）进行划分。然而，这些分类方式并不便于理解提出算法背后的主要方法；因此，建议采用基于噪声模型的方法和无噪声模型的方法进行不同的分区。
- en: Noise model based methods aim to model the noise structure so that this information
    can be used during training to come through noisy labels. In general, approaches
    in this category aim to extract noise-free information contained within the dataset
    by either neglecting or de-emphasizing information coming from noisy samples.
    Furthermore, some methods attempt to reform the dataset by correcting noisy labels
    to increase the quality of the dataset for the classifier. The performance of
    these methods is heavily dependent on the accurate estimate of the underlying
    noise. The advantage of noise model based methods is the decoupling of classification
    and label noise estimation, which helps them to work with the classification algorithm
    at hand. Another good side is in the case of prior knowledge about the noise structure,
    noise model based methods can easily be head-started with this extra information
    inserted to the system.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于噪声模型的方法旨在建模噪声结构，以便在训练过程中利用这些信息来处理噪声标签。通常，这类方法旨在通过忽略或减少来自噪声样本的信息，提取数据集中包含的噪声-free
    信息。此外，一些方法试图通过纠正噪声标签来改造数据集，以提高分类器的数据集质量。这些方法的性能严重依赖于对基础噪声的准确估计。基于噪声模型的方法的优点是分类与标签噪声估计的解耦，这有助于它们与现有的分类算法一起工作。另一个好处是，如果对噪声结构有先验知识，基于噪声模型的方法可以通过将额外信息插入系统来轻松启动。
- en: Differently, noise model free methods aim to develop inherently noise robust
    strategies without explicit modeling of the noise structure. These approaches
    assume that the classifier is not too sensitive to the noise, and performance
    degradation results from overfitting. Therefore, the main focus is given to overfit
    avoidance by regularizing the network training procedure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相对地，无噪声模型的方法旨在开发固有的噪声鲁棒策略，而无需显式建模噪声结构。这些方法假设分类器对噪声的敏感性不强，性能下降来自过拟合。因此，主要关注点在于通过正则化网络训练过程来避免过拟合。
- en: 'Both of the mentioned approaches are discussed and further categorized in [section 3](#S3
    "3 Noise Model Based Methods ‣ Image Classification with Deep Learning in the
    Presence of Noisy Labels: A Survey") and [section 4](#S4 "4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey").
    [Table 1](#S2.T1 "Table 1 ‣ 2.4 Methodologies ‣ 2 Preliminaries ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") presents all the
    mentioned methods to provide a clear picture as a whole. It should be noted that
    most of the time there are no sharp boundaries among the algorithms, and they
    may belong to more than one category. However, for the sake of integrity, they
    are placed in the subclass of most resemblance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提到的两种方法在 [第3节](#S3 "3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述") 和 [第4节](#S4 "4 噪声模型自由方法
    ‣ 在有噪声标签情况下的深度学习图像分类：综述") 中进行了讨论和进一步分类。 [表1](#S2.T1 "表1 ‣ 2.4 方法论 ‣ 2 初步研究 ‣ 在有噪声标签情况下的深度学习图像分类：综述")
    展示了所有提到的方法，以提供一个整体的清晰图景。应注意，大多数时候，算法之间没有明确的界限，它们可能属于多个类别。然而，为了完整性，它们被放置在最相似的子类中。
- en: '| [Noise Model Based Methods](#S3 "In Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey") |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [噪声模型基础方法](#S3 "在有噪声标签情况下的深度学习图像分类：综述") |'
- en: '&#124; 1\. [Noisy Channel](#S3.SS1 "In 3 Noise Model Based Methods ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. [噪声通道](#S3.SS1 "在 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述") &#124;'
- en: '&#124; a.[Explicit calculation](#S3.SS1.SSS1 "In 3.1 Noisy Channel ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey"): predictions on noisy data [[45](#bib.bib45)], predictions
    on clean data [[46](#bib.bib46)] &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a.[显式计算](#S3.SS1.SSS1 "在 3.1 噪声通道 ‣ 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述"):
    对噪声数据的预测 [[45](#bib.bib45)], 对干净数据的预测 [[46](#bib.bib46)] &#124;'
- en: '&#124; easy data [[47](#bib.bib47)] &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简单数据 [[47](#bib.bib47)] &#124;'
- en: '&#124; b.[Iterative calculation](#S3.SS1.SSS2 "In 3.1 Noisy Channel ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey"): EM [[48](#bib.bib48), [49](#bib.bib49), [27](#bib.bib27)],
    fully connected layer [[50](#bib.bib50)], anchor point estimate [[51](#bib.bib51)]
    &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; b.[迭代计算](#S3.SS1.SSS2 "在 3.1 噪声通道 ‣ 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述"):
    EM [[48](#bib.bib48), [49](#bib.bib49), [27](#bib.bib27)], 全连接层 [[50](#bib.bib50)],
    锚点估计 [[51](#bib.bib51)] &#124;'
- en: '&#124; Drichlet-distribution [[52](#bib.bib52)] &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Drichlet-分布 [[52](#bib.bib52)] &#124;'
- en: '&#124; c.[Complex noisy channel](#S3.SS1.SSS3 "In 3.1 Noisy Channel ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey"): noise type estimation [[53](#bib.bib53)], relevance
    estimation [[54](#bib.bib54)] &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; c.[复杂噪声通道](#S3.SS1.SSS3 "在 3.1 噪声通道 ‣ 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述"):
    噪声类型估计 [[53](#bib.bib53)], 相关性估计 [[54](#bib.bib54)] &#124;'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2\. [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based Methods ‣
    Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. [标签噪声清理](#S3.SS2 "在 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述") &#124;'
- en: '&#124; a.[Using data with clean labels](#S3.SS2.SSS1 "In 3.2 Label Noise Cleaning
    ‣ 3 Noise Model Based Methods ‣ Image Classification with Deep Learning in the
    Presence of Noisy Labels: A Survey"): train on clean set [[55](#bib.bib55)], ensemble
    [[56](#bib.bib56)], graph-based [[57](#bib.bib57)] &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a.[使用干净标签的数据](#S3.SS2.SSS1 "在 3.2 标签噪声清理 ‣ 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述"):
    在干净的数据集上训练 [[55](#bib.bib55)], 集成 [[56](#bib.bib56)], 基于图的 [[57](#bib.bib57)]
    &#124;'
- en: '&#124; b.[Using data with both clean and noisy labels](#S3.SS2.SSS2 "In 3.2
    Label Noise Cleaning ‣ 3 Noise Model Based Methods ‣ Image Classification with
    Deep Learning in the Presence of Noisy Labels: A Survey"): iteratively correct
    [[58](#bib.bib58)], correct for fine-tune [[59](#bib.bib59)] &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; b.[使用同时包含干净和噪声标签的数据](#S3.SS2.SSS2 "在 3.2 标签噪声清理 ‣ 3 噪声模型基础方法 ‣ 在有噪声标签情况下的深度学习图像分类：综述"):
    迭代修正 [[58](#bib.bib58)], 用于微调的修正 [[59](#bib.bib59)] &#124;'
- en: '&#124; c.[Using data with just noisy labels](#S3.SS2.SSS3 "In 3.2 Label Noise
    Cleaning ‣ 3 Noise Model Based Methods ‣ Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey"): calculate posterior [[60](#bib.bib60)],
    posterior with compatibility [[61](#bib.bib61)] &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; c.[使用仅带噪声标签的数据](#S3.SS2.SSS3 "在3.2标签噪声清理 ‣ 3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述"):
    计算后验 [[60](#bib.bib60)], 兼容的后验 [[61](#bib.bib61)] &#124;'
- en: '&#124; consistency with model [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)],
    ensemble [[65](#bib.bib65)], prototypes [[66](#bib.bib66)], quality embedding
    [[67](#bib.bib67)] &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与模型的一致性 [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)], 集成 [[65](#bib.bib65)],
    原型 [[66](#bib.bib66)], 质量嵌入 [[67](#bib.bib67)] &#124;'
- en: '&#124; partial labels [[68](#bib.bib68)] &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部分标签 [[68](#bib.bib68)] &#124;'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3\. [Dataset Pruning](#S3.SS3 "In 3 Noise Model Based Methods ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3\. [数据集剪枝](#S3.SS3 "在3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述") &#124;'
- en: '&#124; a.Data pruning network prediction based [[69](#bib.bib69)], ensemble
    of filters [[70](#bib.bib70), [71](#bib.bib71)], according to noise rate [[72](#bib.bib72)]
    &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a. 基于数据剪枝网络预测 [[69](#bib.bib69)], 过滤器集成 [[70](#bib.bib70), [71](#bib.bib71)],
    根据噪声率 [[72](#bib.bib72)] &#124;'
- en: '&#124; transfer learning [[73](#bib.bib73)], cyclic state [[74](#bib.bib74)],
    K-means [[75](#bib.bib75)] &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转移学习 [[73](#bib.bib73)], 循环状态 [[74](#bib.bib74)], K-means [[75](#bib.bib75)]
    &#124;'
- en: '&#124; b.Label pruning semi-supervised learning [[76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79)], relabeling [[80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)] &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; b. 标签剪枝半监督学习 [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)], 重新标注 [[80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82)]
    &#124;'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 4\. [Sample Choosing](#S3.SS4 "In 3 Noise Model Based Methods ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4\. [样本选择](#S3.SS4 "在3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述") &#124;'
- en: '&#124; a.[Curriculum Learning](#S3.SS4.SSS1 "In 3.4 Sample Choosing ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey"): Screening loss [[83](#bib.bib83)], teacher-student
    [[84](#bib.bib84)] &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a.[课程学习](#S3.SS4.SSS1 "在3.4样本选择 ‣ 3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述"): 筛选损失
    [[83](#bib.bib83)], 教师-学生 [[84](#bib.bib84)] &#124;'
- en: '&#124; selecting uncertain samples [[85](#bib.bib85)], curriculum loss [[86](#bib.bib86)],
    data complexity [[87](#bib.bib87)] &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择不确定样本 [[85](#bib.bib85)], 课程损失 [[86](#bib.bib86)], 数据复杂性 [[87](#bib.bib87)]
    &#124;'
- en: '&#124; consistency with model [[88](#bib.bib88)] &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与模型的一致性 [[88](#bib.bib88)] &#124;'
- en: '&#124; b.[Multiple Classifiers](#S3.SS4.SSS2 "In 3.4 Sample Choosing ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey"): Consistency of networks [[89](#bib.bib89)], co-teaching
    [[90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)] &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; b.[多个分类器](#S3.SS4.SSS2 "在3.4样本选择 ‣ 3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述"): 网络一致性
    [[89](#bib.bib89)], 共同教学 [[90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93)] &#124;'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5\. [Sample Importance Weighting](#S3.SS5 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5\. [样本重要性加权](#S3.SS5 "在3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述") &#124;'
- en: '&#124; Meta task [[94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96)], siamese
    network [[97](#bib.bib97)], pLOF [[28](#bib.bib28)], abstention [[98](#bib.bib98)]
    &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 元任务 [[94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96)], 孪生网络 [[97](#bib.bib97)],
    pLOF [[28](#bib.bib28)], 避免 [[98](#bib.bib98)] &#124;'
- en: '&#124; estimate noise rate [[99](#bib.bib99), [100](#bib.bib100)], similarity
    loss [[101](#bib.bib101)], transfer learning [[102](#bib.bib102)], $\theta$-distribution
    [[103](#bib.bib103)] &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计噪声率 [[99](#bib.bib99), [100](#bib.bib100)], 相似度损失 [[101](#bib.bib101)],
    转移学习 [[102](#bib.bib102)], $\theta$-分布 [[103](#bib.bib103)] &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 6\. [Labeler Quality Assessment](#S3.SS6 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 6\. [标注者质量评估](#S3.SS6 "在3噪声模型方法 ‣ 在噪声标签下的深度学习图像分类：综述") &#124;'
- en: '&#124; EM [[104](#bib.bib104), [105](#bib.bib105), [26](#bib.bib26)], trace
    regularizer [[106](#bib.bib106)], crowd-layer [[107](#bib.bib107)], image difficulty
    estimate [[108](#bib.bib108)] &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EM [[104](#bib.bib104)，[105](#bib.bib105)，[26](#bib.bib26)]，迹正则化 [[106](#bib.bib106)]，众包层
    [[107](#bib.bib107)]，图像难度估计 [[108](#bib.bib108)] &#124;'
- en: '&#124; consistency with network [[109](#bib.bib109)], omitting probability
    variable [[110](#bib.bib110)] &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与网络的一致性 [[109](#bib.bib109)]，省略概率变量 [[110](#bib.bib110)] &#124;'
- en: '&#124; softmax layer per labeler [[25](#bib.bib25)] &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个标注者的 softmax 层 [[25](#bib.bib25)] &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [Noise Model Free Methods](#S4 "In Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey") |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [噪声模型无关方法](#S4 "在存在噪声标签的深度学习图像分类中的调查") |'
- en: '&#124; 1\. [Robust Losses](#S4.SS1 "In 4 Noise Model Free Methods ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. [稳健损失](#S4.SS1 "在 4 噪声模型无关方法 ‣ 在存在噪声标签的深度学习图像分类中的调查") &#124;'
- en: '&#124; Non-convex loss functions [[111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)],
    0-1 loss surrogate [[114](#bib.bib114)], MAE [[115](#bib.bib115)], IMEA [[116](#bib.bib116)]
    &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非凸损失函数 [[111](#bib.bib111)，[112](#bib.bib112)，[113](#bib.bib113)]，0-1
    损失替代 [[114](#bib.bib114)]，MAE [[115](#bib.bib115)]，IMEA [[116](#bib.bib116)] &#124;'
- en: '&#124; Generalized cross-entropy [[117](#bib.bib117)], symmetric loss [[118](#bib.bib118)],
    unbiased estimator [[119](#bib.bib119)], &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广义交叉熵 [[117](#bib.bib117)]，对称损失 [[118](#bib.bib118)]，无偏估计器 [[119](#bib.bib119)]，&#124;'
- en: '&#124; modified cross-entropy for omission [[120](#bib.bib120)], information
    theoric loss [[121](#bib.bib121)] &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 针对省略的修改交叉熵 [[120](#bib.bib120)]，信息理论损失 [[121](#bib.bib121)] &#124;'
- en: '&#124; linear-odd losses [[122](#bib.bib122)], classification calibrated losses
    [[123](#bib.bib123)], SGD with robust losses [[124](#bib.bib124)] &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 线性奇数损失 [[122](#bib.bib122)]，分类校准损失 [[123](#bib.bib123)]，带有稳健损失的 SGD
    [[124](#bib.bib124)] &#124;'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2\. [Meta Learning](#S4.SS2 "In 4 Noise Model Free Methods ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. [元学习](#S4.SS2 "在 4 噪声模型无关方法 ‣ 在存在噪声标签的深度学习图像分类中的调查") &#124;'
- en: '&#124; Choosing best methods [[125](#bib.bib125)], pumpout [[126](#bib.bib126)],
    noise tolerant parameter initialization [[127](#bib.bib127)], &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择最佳方法 [[125](#bib.bib125)]，泵出 [[126](#bib.bib126)]，噪声容忍参数初始化 [[127](#bib.bib127)]，&#124;'
- en: '&#124; knowledge distillation [[128](#bib.bib128), [129](#bib.bib129)], gradient
    magnitude adjustment [[130](#bib.bib130), [131](#bib.bib131)], meta soft labels
    [[132](#bib.bib132)] &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识蒸馏 [[128](#bib.bib128)，[129](#bib.bib129)]，梯度幅度调整 [[130](#bib.bib130)，[131](#bib.bib131)]，元
    soft 标签 [[132](#bib.bib132)] &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3\. [Regularizers](#S4.SS3 "In 4 Noise Model Free Methods ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3\. [正则化器](#S4.SS3 "在 4 噪声模型无关方法 ‣ 在存在噪声标签的深度学习图像分类中的调查") &#124;'
- en: '&#124; Dropout [[133](#bib.bib133)], adversarial training [[134](#bib.bib134)],
    mixup [[135](#bib.bib135)], label smoothing [[136](#bib.bib136), [137](#bib.bib137)]
    &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dropout [[133](#bib.bib133)]，对抗训练 [[134](#bib.bib134)]，mixup [[135](#bib.bib135)]，标签平滑
    [[136](#bib.bib136)，[137](#bib.bib137)] &#124;'
- en: '&#124; pre-training [[138](#bib.bib138)], dropout on final layer [[139](#bib.bib139)],
    checking dimensionality &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练 [[138](#bib.bib138)]，最后一层的 dropout [[139](#bib.bib139)]，检查维度 &#124;'
- en: '&#124; [[140](#bib.bib140)], auxiliary image regularizer [[141](#bib.bib141)]
    &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[140](#bib.bib140)]，辅助图像正则化器 [[141](#bib.bib141)] &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 4\. [Ensemble Methods](#S4.SS4 "In 4 Noise Model Free Methods ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4\. [集成方法](#S4.SS4 "在 4 噪声模型无关方法 ‣ 在存在噪声标签的深度学习图像分类中的调查") &#124;'
- en: '&#124; LogitBoost&BrownBoost [[142](#bib.bib142)], noise detection based AdaBoost
    [[143](#bib.bib143)], rBoost [[144](#bib.bib144)] &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LogitBoost&BrownBoost [[142](#bib.bib142)]，基于噪声检测的 AdaBoost [[143](#bib.bib143)]，rBoost
    [[144](#bib.bib144)] &#124;'
- en: '&#124; RBoost1&RBoost2 [[145](#bib.bib145)], robust multi-class AdaBoost [[146](#bib.bib146)]
    &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RBoost1&RBoost2 [[145](#bib.bib145)]，稳健的多类 AdaBoost [[146](#bib.bib146)]
    &#124;'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5\. [Others](#S4.SS5 "In 4 Noise Model Free Methods ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey") &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5\. [其他](#S4.SS5 "在 4 噪声模型无关方法 ‣ 在存在噪声标签的深度学习图像分类中的调查") &#124;'
- en: '&#124; Complementary labels [[147](#bib.bib147), [148](#bib.bib148)], autoencoder
    reconstruction error [[149](#bib.bib149)] &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互补标签 [[147](#bib.bib147)，[148](#bib.bib148)]，自动编码器重构误差 [[149](#bib.bib149)]
    &#124;'
- en: '&#124; minimum covariance determinant [[150](#bib.bib150)], less noisy data
    [[151](#bib.bib151)], &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最小协方差行列式 [[150](#bib.bib150)], 噪声较少的数据 [[151](#bib.bib151)], &#124;'
- en: '&#124; data quality [[152](#bib.bib152)], prototype learning [[153](#bib.bib153),
    [154](#bib.bib154)], multiple instance learning [[155](#bib.bib155), [156](#bib.bib156)]
    &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据质量 [[152](#bib.bib152)], 原型学习 [[153](#bib.bib153), [154](#bib.bib154)],
    多实例学习 [[155](#bib.bib155), [156](#bib.bib156)] &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: Existing methods to deal with label noise in the literature'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：文献中处理标签噪声的现有方法
- en: 3 Noise Model Based Methods
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于噪声模型的方法
- en: In the presence of noisy labels, the learning objective is to find the best
    estimator for hidden distribution $\mathcal{D}$, while iterating over distribution
    $\mathcal{D}_{n}$. If the mapping function $M:\mathcal{D}\rightarrow\mathcal{D}_{n}$
    is known, it can be used to reverse the effect of noisy samples. Algorithms under
    this section simultaneously try to find underlying noise structure and train the
    base classifier with estimated noise parameters. They need a better estimate of
    $M$ to train better classifiers and better classifiers to estimate $M$ accurately.
    Therefore, they usually suffer from a chicken-egg problem. Approaches belonging
    to this category are explained in the following subsections.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在噪声标签的情况下，学习目标是找到隐藏分布 $\mathcal{D}$ 的最佳估计，同时迭代分布 $\mathcal{D}_{n}$。如果映射函数
    $M:\mathcal{D}\rightarrow\mathcal{D}_{n}$ 是已知的，它可以用来逆转噪声样本的影响。本节中的算法同时尝试找到潜在的噪声结构，并用估计的噪声参数训练基础分类器。它们需要对
    $M$ 的更好估计来训练更好的分类器，同时更好的分类器也能准确地估计 $M$。因此，它们通常会面临鸡蛋与鸡的问题。属于这一类别的方法将在以下小节中解释。
- en: 3.1 Noisy Channel
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 噪声通道
- en: 'The general setup for the noisy channel is illustrated in [Figure 2](#S3.F2
    "Figure 2 ‣ 3.1 Noisy Channel ‣ 3 Noise Model Based Methods ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey"). Methods belonging
    to this category minimize the following risk'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '噪声通道的一般设置在 [图 2](#S3.F2 "Figure 2 ‣ 3.1 Noisy Channel ‣ 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") 中进行了说明。属于这一类别的方法最小化以下风险'
- en: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}l(Q(f_{\theta}(x_{i})),\tilde{y_{i}})$
    |  | (5) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}l(Q(f_{\theta}(x_{i})),\tilde{y_{i}})$
    |  | (5) |'
- en: where $Q(f_{\theta}(x_{i}))=p(\tilde{y_{i}}|f_{\theta}(x_{i}))$ is the mapping
    from network predictions to given noisy labels. If $Q$ adapts the noise structure
    $p(\tilde{y}|y)$, then network will be forced to learn true mapping $p(y|x)$.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q(f_{\theta}(x_{i}))=p(\tilde{y_{i}}|f_{\theta}(x_{i}))$ 是网络预测到给定噪声标签的映射。如果
    $Q$ 适应噪声结构 $p(\tilde{y}|y)$，那么网络将被迫学习真实的映射 $p(y|x)$。
- en: $Q$ can be formulated with a noise transition matrix $T$ so that $Q(f_{\theta}(x_{i}))=Tf_{\theta}(x_{i})$
    where each element of the matrix represents the transition probability of given
    true label to noisy label, $T_{ij}=p(\tilde{y}=j|y=i)$. Since $T$ is composed
    of probabilities, weights coming from a single node should sum to one $\sum_{j}T_{ij}=1$.
    This procedure of correcting predictions to match given label distribution is
    also called loss-correction [[45](#bib.bib45)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: $Q$ 可以用噪声转移矩阵 $T$ 进行公式化，使得 $Q(f_{\theta}(x_{i}))=Tf_{\theta}(x_{i})$，其中矩阵的每个元素表示给定真实标签到噪声标签的转移概率，$T_{ij}=p(\tilde{y}=j|y=i)$。由于
    $T$ 由概率组成，因此来自单个节点的权重应和为 1，即 $\sum_{j}T_{ij}=1$。这种将预测结果校正以匹配给定标签分布的过程也称为损失校正 [[45](#bib.bib45)]。
- en: A common problem in noisy channel estimation is scalability. As the number of
    classes increases, the size of the noise transition matrix increases exponentially,
    making it intractable to calculate. This can be partially avoided by allowing
    connections only among the most probable nodes [[49](#bib.bib49)], or predefined
    nodes [[157](#bib.bib157)]. These restrictions are determined by human experts,
    which allows additional noise information to be inserted into the training procedure.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在噪声通道估计中，一个常见的问题是可扩展性。随着类别数量的增加，噪声转移矩阵的大小呈指数级增长，使得计算变得不可行。通过仅允许在最可能的节点 [[49](#bib.bib49)]
    或预定义的节点 [[157](#bib.bib157)] 之间建立连接，可以部分避免这一问题。这些限制由人工专家确定，允许将额外的噪声信息插入训练过程。
- en: The noisy channel is used only in the training phase. In the evaluation phase,
    the noisy channel is removed to get noise-free predictions of the base classifier.
    In these kinds of approaches, performance heavily depends on the accurate estimation
    of noisy channel parameters; therefore, works mainly focus on estimating $Q$.
    Various ways of formulating the noisy channel are explained below.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声通道仅在训练阶段使用。在评估阶段，噪声通道被移除，以获得基础分类器的无噪声预测。在这些方法中，性能在很大程度上依赖于噪声通道参数的准确估计；因此，工作主要集中在估计$Q$。以下解释了噪声通道的各种表述方式。
- en: '![Refer to caption](img/522ab95490dbe7751fc5348b4b4f6a47.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/522ab95490dbe7751fc5348b4b4f6a47.png)'
- en: 'Figure 2: Noise can be modeled as a noisy channel on top of base classifier.
    Noisy channel adapts the characteristic of the noise so that base classifier is
    fed with noise-free gradients during traning.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：噪声可以被建模为基础分类器上的噪声通道。噪声通道适应噪声的特性，以便在训练过程中基础分类器接收无噪声的梯度。
- en: 3.1.1 Explicit calculation
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 显式计算
- en: 'Noise transition matrix is calculated explicitly, and then the base classifier
    is trained using this matrix. Assuming dataset is balanced in terms of clean representative
    samples and noisy samples, so that there exists samples for each class with $p(y=\tilde{y}_{i}|x_{i})=1$,
    [[45](#bib.bib45)] constructs $T$ just based on noisy class probability estimates
    of a pre-trained model, so-called confusion matrix. A similar approach is followed
    in [[46](#bib.bib46)]; however, the noise transition matrix is calculated from
    the network’s confusion matrix on the clean subset of data. Two datasets are gathered
    in [[47](#bib.bib47)], namely: easy data and hard data. The classifier is first
    trained on the easy data to extract similarity relationships among classes. Afterward,
    the calculated similarity matrix is used as the noise transition matrix. Another
    method proposed in [[50](#bib.bib50)] calculates the confusion matrix on both
    noisy data and clean data. Then, the difference between these two confusion matrices
    gives $T$.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声转移矩阵是显式计算的，然后使用这个矩阵训练基础分类器。假设数据集在干净的代表性样本和带噪声样本方面是平衡的，以便每个类别都有样本满足$p(y=\tilde{y}_{i}|x_{i})=1$，[[45](#bib.bib45)]
    仅基于预训练模型的带噪声类别概率估计来构建$T$，即所谓的混淆矩阵。[[46](#bib.bib46)] 中采用了类似的方法；然而，噪声转移矩阵是从网络在干净数据子集上的混淆矩阵计算的。在[[47](#bib.bib47)]中收集了两个数据集，即：易数据和难数据。分类器首先在易数据上进行训练，以提取类别之间的相似性关系。之后，计算得到的相似性矩阵被用作噪声转移矩阵。在[[50](#bib.bib50)]中提出的另一种方法是在带噪声数据和干净数据上计算混淆矩阵。然后，这两个混淆矩阵之间的差异给出$T$。
- en: 3.1.2 Iterative calculation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 迭代计算
- en: Noise transition matrix is estimated incrementally during the training of the
    base classifier. In [[48](#bib.bib48), [49](#bib.bib49)] expectation-maximization
    (EM) [[158](#bib.bib158)] is used to iteratively train network to match given
    distribution and estimate noise transition matrix given the model prediction.
    The same approach is used on medical data with noisy labels in [[27](#bib.bib27)].
    [[50](#bib.bib50)] adds a linear fully connected layer as a last layer of the
    base classifier, which is trained to adapt noise behavior. To avoid this additional
    layer to converge the identity matrix and base classifier overfitting the noise,
    the weight decay regularizer is applied to this layer. [[51](#bib.bib51)] suggests
    using class probability estimates on anchor points (data points that belong to
    a specific class almost surely) to construct the noise transition matrix. In the
    absence of a noise-free subset of data, anchor points are extracted from data
    points with high noisy class posterior probabilities. Then, the matrix is updated
    iteratively to minimize loss during training. Instead of using softmax probabilities,
    [[52](#bib.bib52)] models noise transition matrix in Bayesian form by projecting
    it into a Dirichlet-distributed space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础分类器的训练过程中，噪声转移矩阵是逐步估计的。在[[48](#bib.bib48)、[49](#bib.bib49)]中，期望最大化（EM）[[158](#bib.bib158)]
    被用来迭代地训练网络以匹配给定的分布，并根据模型预测估计噪声转移矩阵。相同的方法在[[27](#bib.bib27)]的带噪声标签的医学数据上使用。[[50](#bib.bib50)]
    在基础分类器的最后一层添加了一个线性全连接层，该层经过训练以适应噪声行为。为了避免这个额外的层收敛到单位矩阵和基础分类器过度拟合噪声，对该层应用了权重衰减正则化。[[51](#bib.bib51)]
    建议在锚点（几乎肯定属于特定类别的数据点）上使用类别概率估计来构建噪声转移矩阵。在没有无噪声数据子集的情况下，从具有高噪声类别后验概率的数据点中提取锚点。然后，矩阵被迭代更新以最小化训练过程中的损失。[[52](#bib.bib52)]
    通过将噪声转移矩阵投影到Dirichlet分布空间中来以贝叶斯形式建模噪声转移矩阵，而不是使用softmax概率。
- en: 3.1.3 Complex noisy channel
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 复杂的噪声通道
- en: 'Different then simple confusion matrix, some works formalize the noisy channel
    as a more complex function. This enables noisy channel parameters to be calculated
    not just by using network outputs but additional information about the content
    of data. For example, three types of label noises are defined in [[53](#bib.bib53)],
    namely: no noise, random noise, structured noise. An additional convolutional
    neural network (CNN) is used to interpret the noise type of each sample. Finally,
    the noisy layer aims to match predicted labels to noisy labels with the help of
    predicted noise type. Another work in [[54](#bib.bib54)] proposes training an
    extra network as a relevance estimator, which attains the label’s relevance to
    the given instance. Predicted labels are mapped to noisy labels with the consideration
    of relevance. If relevance is low, in case of noise, the classifier can still
    make predictions of true class and doesn’t get penalized much for it.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的混淆矩阵不同，一些工作将噪声通道形式化为更复杂的函数。这使得噪声通道参数不仅通过使用网络输出，还可以通过关于数据内容的额外信息来计算。例如，[[53](#bib.bib53)]中定义了三种类型的标签噪声：无噪声、随机噪声、结构化噪声。使用额外的卷积神经网络（CNN）来解释每个样本的噪声类型。最后，噪声层旨在通过预测的噪声类型来将预测标签与噪声标签匹配。另一个在[[54](#bib.bib54)]中的工作建议训练一个额外的网络作为相关性估计器，以获得标签与给定实例的相关性。预测标签在考虑相关性的情况下被映射到噪声标签。如果相关性低，在噪声情况下，分类器仍然可以对真实类别进行预测，并且不会因此受到太大惩罚。
- en: 3.2 Label Noise Cleaning
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 标签噪声清理
- en: An obvious solution to noisy labels is to identify and correct suspicious labels
    to their corresponding true classes. Cleaning the whole dataset manually can be
    costly; therefore, some works propose to pick only suspicious samples to be sent
    to a human annotator to reduce the cost [[41](#bib.bib41)]. However, this is still
    not a scalable approach. As a result, various algorithms are proposed in the literature.
    Including the label correction algorithm, the empirical risk takes the following
    form
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 解决标签噪声的一个明显方法是识别和纠正可疑标签，使其对应于真实类别。手动清理整个数据集可能代价高昂；因此，一些工作建议仅挑选可疑样本交给人工标注者，以减少成本[[41](#bib.bib41)]。然而，这仍然不是一种可扩展的方法。因此，文献中提出了各种算法。包括标签修正算法，经验风险的形式如下：
- en: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}l(f_{\theta}(x_{i}),G(\tilde{y_{i}},x_{i}))$
    |  | (6) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}l(f_{\theta}(x_{i}),G(\tilde{y_{i}},x_{i}))$
    |  | (6) |'
- en: 'where $G(\tilde{y_{i}},x_{i})=p(y_{i}|\tilde{y_{i}},x_{i})$ represents the
    label cleaning algorithm. Label cleaning algorithms rely on a feature extractor
    to map data to the feature domain to investigate noisiness. While some works use
    a pre-trained network as the feature extractor, others use the base classifier
    as it gets more and more accurate during training. This approach results in an
    iterative framework: as the classifier gets better, the label cleaning is more
    accurate, and as the label quality gets better, the classifier gets better. From
    this point of view, label cleaning can be viewed as a dynamically evolving component
    of the system instead of preprocessing of the data. Such methods usually tackle
    the difficulty of distinguishing informative hard samples from those with noisy
    labels [[15](#bib.bib15)]. As a result, they can end up removing too many samples
    or changing labels in a delusional way. Approaches for label cleaning can be separated
    according to their need for clean data or not.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(\tilde{y_{i}},x_{i})=p(y_{i}|\tilde{y_{i}},x_{i})$ 代表标签清理算法。标签清理算法依赖于特征提取器，将数据映射到特征域以调查噪声。虽然一些工作使用预训练网络作为特征提取器，但其他工作使用基本分类器，因为它在训练过程中变得越来越准确。这种方法产生了一个迭代框架：随着分类器的改进，标签清理变得更准确，而随着标签质量的提高，分类器变得更好。从这个角度来看，标签清理可以被视为系统的动态演变组成部分，而不是数据的预处理。这些方法通常解决了区分信息性难样本和具有噪声标签的样本的困难[[15](#bib.bib15)]。因此，它们可能会删除过多的样本或以错觉的方式更改标签。标签清理的方法可以根据其对干净数据的需求进行区分。
- en: 3.2.1 Using data with clean labels
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 使用干净标签的数据
- en: In the existence of a clean subset of data, the aim is to fuse noise-free label
    structure to noisy labels for correction. If the clean subset is large enough
    to train a network, one obvious way is to relabel noisy labels by predictions
    of the network trained on clean data. For relabeling, [[55](#bib.bib55)] uses
    alpha blending of given noisy labels and predicted labels. An ensemble of networks
    trained with different subsets of the dataset is used in [[56](#bib.bib56)]. If
    they all agree on the label, it is changed to the predicted label; otherwise,
    it is set to a random label. Instead of keeping the noisy label, setting it randomly
    helps break the noise structure and makes noise more uniformly distributed in
    label space. In [[57](#bib.bib57)] a graph-based approach is used, where a conditional
    random field extracts relation among noisy labels and clean labels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在干净数据子集的情况下，目标是将无噪声标签结构与有噪声的标签融合以进行修正。如果干净子集足够大以训练网络，一种显而易见的方法是通过在干净数据上训练的网络的预测来重新标记有噪声的标签。对于重新标记，[［55］(#bib.bib55)]
    使用了给定的有噪声标签和预测标签的 alpha blending。[[56](#bib.bib56)] 中使用了一个由不同数据集子集训练的网络集成。如果它们都对标签达成一致，则将标签更改为预测标签；否则，将其设置为随机标签。与其保留有噪声标签，不如随机设置标签，这有助于打破噪声结构，并使噪声在标签空间中更均匀分布。在
    [[57](#bib.bib57)] 中使用了基于图的方法，其中条件随机场提取了有噪声标签和干净标签之间的关系。
- en: 3.2.2 Using data with both clean and noisy labels
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 使用同时具有干净和有噪声标签的数据
- en: 'Some works rely on a subset of data, for which both clean and noisy labels
    are provided. Then label noise structure is extracted from these conflicting labels
    and used to correct noisy data. In [[58](#bib.bib58)], the label cleaning network
    gets two inputs: extracted features of instances by the base classifier and corresponding
    noisy labels. Label cleaning network and base classifier are trained jointly so
    that label cleaning network learns to correct labels on the clean subset of data
    and provides corrected labels for base classifier on noisy data. Same approach
    is decoupled in [[59](#bib.bib59)] in teacher-student manner. First, the student
    is trained on noisy data. Then features are extracted from the clean data via
    the student model, and the teacher learns the structure of noise depending on
    these extracted features. Afterward, the teacher predicts soft labels for noisy
    data, and the student is again trained on these soft labels for fine-tuning.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究依赖于一个数据子集，其中同时提供了干净和有噪声的标签。然后，从这些冲突的标签中提取标签噪声结构，并用于修正有噪声的数据。在 [[58](#bib.bib58)]
    中，标签清理网络接收两个输入：由基础分类器提取的实例特征和相应的有噪声标签。标签清理网络和基础分类器共同训练，使得标签清理网络学会在干净子集数据上修正标签，并为基础分类器提供有噪声数据的修正标签。同样的方法在
    [[59](#bib.bib59)] 中以教师-学生方式解耦。首先，学生在有噪声数据上训练。然后，通过学生模型从干净数据中提取特征，教师根据这些提取的特征学习噪声的结构。之后，教师为有噪声数据预测软标签，学生再次在这些软标签上进行训练以进行微调。
- en: 3.2.3 Using data with just noisy labels
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 使用仅有噪声标签的数据
- en: Noise-free data is not always available, so the primary approach in this situation
    is to estimate cleaner posterior label distribution incrementally. However, there
    is a possible undesired solution to this approach so that all labels are attained
    to a single class and base network predicting constant class, which would result
    in delusional top training accuracy. Therefore, additional regularizers are commonly
    used to make label posterior distribution even. A joint optimization framework
    for both training base classifier and propagating noisy labels to cleaner labels
    is presented in [[60](#bib.bib60)]. Using expectation-maximization, both classifier
    parameters and label posterior distribution is estimated to minimize the loss.
    A similar approach is used in [[61](#bib.bib61)] with additional compatibility
    loss conditioned on label posterior. Considering noisy labels are in the minority,
    this term assures posterior label distribution does not diverge too much from
    the given noisy label distribution so that majority of the clean label contribution
    is not lost. [[62](#bib.bib62), [63](#bib.bib63)] deploy a confidence policy where
    labels are determined by either network output or given noisy labels, depending
    on the confidence of the model’s prediction. Arguing that, in the case of noisy
    labels, the model first learns correctly labeled data and then overfits to noisy
    data, [[64](#bib.bib64)] aims to extract the probability of a sample being noisy
    or not from its loss value. To achieve this, the loss of each instance is fitted
    by a beta mixture model, which models the label noise in an unsupervised manner.
    [[65](#bib.bib65)] proposes a two-level approach. In the first stage, with any
    chosen inference algorithm, the ground truth labels are determined, and data is
    divided into two subsets as noisy and clean. In the second stage, an ensemble
    of weak classifiers is trained on clean data to predict true labels of noisy data.
    Afterward, these two subsets of data are merged to create the final enhanced dataset.
    [[66](#bib.bib66)] constructs prototypes that can represent deep feature distribution
    of the corresponding class for each class. Then corrected labels are found by
    checking similarity among data samples and prototypes. [[67](#bib.bib67)] introduces
    a new parameter, namely quality embedding, which represents the trustworthiness
    of data. Depending on two latent variables, true class probability and quality
    embedding, an additional network tries to extract each instance’s true class.
    In a multi-labeled dataset, where each instance has multiple labels representing
    its content, some labels may be partially missing resulting in partial labels.
    In the case of partial labels, [[68](#bib.bib68)] uses one network to find and
    estimate easy missing labels and another network to be trained on this corrected
    data. [[159](#bib.bib159)] formulates video anomaly detection as a classification
    with label noise problem and trains a graph convolutional label noise cleaning
    network depending on features and temporal consistency of video snippets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声数据并不总是可用，因此在这种情况下，主要的方法是逐步估计更干净的后验标签分布。然而，这种方法可能会出现一个不希望出现的解决方案，即所有标签都被归为一个单一类别，基础网络预测一个常量类别，这将导致虚假的高训练准确率。因此，通常会使用额外的正则化器来使标签后验分布更加均匀。在[[60](#bib.bib60)]中提出了一个联合优化框架，用于同时训练基础分类器和将噪声标签传播到更干净标签。通过期望最大化，估计分类器参数和标签后验分布，以最小化损失。在[[61](#bib.bib61)]中使用了类似的方法，但增加了基于标签后验的兼容性损失。考虑到噪声标签占少数，这一术语确保后验标签分布不会过多偏离给定的噪声标签分布，从而不会丢失大多数干净标签的贡献。[[62](#bib.bib62),
    [63](#bib.bib63)] 部署了一种信心策略，其中标签由网络输出或给定的噪声标签决定，具体取决于模型预测的置信度。[[64](#bib.bib64)]
    认为，在噪声标签的情况下，模型首先学习正确标记的数据，然后再对噪声数据进行过拟合，旨在从损失值中提取样本是否为噪声的概率。为此，每个实例的损失由一个β混合模型拟合，该模型以无监督方式建模标签噪声。[[65](#bib.bib65)]
    提出了一个两级方法。在第一阶段，使用任何选择的推断算法来确定真实标签，并将数据分为噪声和干净两个子集。在第二阶段，训练一个弱分类器的集成模型，以预测噪声数据的真实标签。然后，这两个数据子集被合并以创建最终的增强数据集。[[66](#bib.bib66)]
    构建了可以表示每个类别的深度特征分布的原型。然后，通过检查数据样本与原型之间的相似性来找到修正后的标签。[[67](#bib.bib67)] 引入了一个新的参数，即质量嵌入，代表数据的可信度。根据两个潜在变量：真实类别概率和质量嵌入，额外的网络尝试提取每个实例的真实类别。在多标签数据集中，每个实例都有多个标签来表示其内容，有些标签可能部分缺失，导致部分标签。在部分标签的情况下，[[68](#bib.bib68)]
    使用一个网络来查找和估计容易缺失的标签，另一个网络则在这些修正后的数据上进行训练。[[159](#bib.bib159)] 将视频异常检测表述为具有标签噪声的问题，并根据视频片段的特征和时间一致性训练了一个图卷积标签噪声清理网络。
- en: 3.3 Dataset Pruning
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据集修剪
- en: Instead of correcting noisy labels to their true classes, an alternative approach
    is to remove them. While this would result in loss of information, preventing
    the harmful impact of noise may result in better performance. In these methods,
    there is a risk of removing too many samples. Therefore, it is crucial to remove
    as few samples as possible to prevent unnecessary data loss.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 代替将噪声标签更正为其真实类别的做法，可以选择删除这些标签。虽然这会导致信息丢失，但防止噪声的有害影响可能会导致更好的性能。在这些方法中，存在删除过多样本的风险。因此，至关重要的是尽可能少地删除样本，以防止不必要的数据丢失。
- en: There are two alternative ways for data pruning. The first option is to remove
    noisy samples completely and train the classifier on the pruned dataset. The second
    option is to remove just labels of noisy data and transform the dataset into two
    subsets as; labeled and unlabeled data. Then semi-supervised learning algorithms
    can be employed on the resultant dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据修剪有两种替代方法。第一种选项是完全删除噪声样本，并在修剪后的数据集上训练分类器。第二种选项是仅删除噪声数据的标签，并将数据集转换为两个子集：标记数据和未标记数据。然后可以在结果数据集上应用半监督学习算法。
- en: 3.3.1 Removing Data
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 删除数据
- en: 'The most straightforward approach is to remove instances misclassified by the
    base network [[69](#bib.bib69)]. [[70](#bib.bib70)] uses an ensemble of filtering
    methods, where each of them assigns a noisiness level for each sample. Then, these
    predictions are combined, and data with the highest noisiness level predictions
    are removed. [[71](#bib.bib71)] extends this work with label correction. If the
    majority of noise filters predict the same label for the noisy instance, it’s
    label is corrected to the predicted label. Otherwise, it is removed from the dataset.
    In [[72](#bib.bib72)], with the help of a probabilistic classifier, training data
    is divided into two subsets: confidently clean and noisy. Noise rates are estimated
    according to the sizes of these subsets. Finally, relying on the base network’s
    output confidence in data instances, the number of most unconfident samples is
    removed according to the estimated noise rate. In [[73](#bib.bib73)], transfer
    learning is used so that network trained on a clean dataset from a similar domain
    is fine-tuned on the noisy dataset for relabeling. Afterward, the network is again
    trained on relabeled data to re-sample the dataset to construct a final clean
    dataset. In [[74](#bib.bib74)], the learning rate is adjusted cyclicly to change
    network status between underfitting and overfitting. Since, while underfitted,
    noisy samples cause high loss, samples with large noise during this cyclic process
    are removed. [[75](#bib.bib75)] first train network on noisy data and extract
    feature vectors by using this model. Afterward, data is clustered with the K-means
    algorithm running on extracted features, and outliers are removed. [[160](#bib.bib160)]
    provides a comparison of performances of various noise-filtering methods for crowd-sourced
    datasets.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法是删除由基础网络误分类的实例[[69](#bib.bib69)]。[[70](#bib.bib70)] 使用了过滤方法的集成，每种方法为每个样本分配噪声等级。然后，这些预测结果被组合，噪声等级最高的数据被删除。[[71](#bib.bib71)]
    扩展了这项工作，并进行了标签修正。如果大多数噪声过滤器对噪声实例预测相同的标签，则将其标签更正为预测标签。否则，将其从数据集中删除。在[[72](#bib.bib72)]中，通过概率分类器的帮助，将训练数据划分为两个子集：自信的干净数据和噪声数据。根据这些子集的大小估计噪声率。最后，根据基础网络对数据实例的输出置信度，按估计的噪声率删除最不自信的样本。在[[73](#bib.bib73)]中，使用迁移学习，使在类似领域的干净数据集上训练的网络在噪声数据集上进行微调以重新标记。之后，网络在重新标记的数据上再次训练，以重新采样数据集以构建最终的干净数据集。在[[74](#bib.bib74)]中，学习率循环调整以改变网络状态在欠拟合和过拟合之间切换。由于在欠拟合时，噪声样本会导致高损失，因此在此循环过程中大噪声样本被删除。[[75](#bib.bib75)]
    首先在噪声数据上训练网络，并使用该模型提取特征向量。随后，使用 K-means 算法对提取的特征进行聚类，并删除离群值。[[160](#bib.bib160)]
    对各种噪声过滤方法在众包数据集上的性能进行了比较。
- en: 3.3.2 Removing Labels
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 删除标签
- en: The simplest option is to employ straightforward semi-supervised training on
    labeled and unlabeled data [[76](#bib.bib76)]. Alternatively, label removing can
    be done iteratively in each epoch to update the dataset for better utilization
    of semi-supervised learning dynamically. [[77](#bib.bib77)] uses consistency among
    the given label and moving average of model predictions to evaluate if the provided
    label is noisy or not. Then the model is trained on clean samples on the next
    iteration. This procedure continues until convergence to the best estimator. The
    same approach is used in [[78](#bib.bib78)] with a little tweak. Instead of comparing
    with given labels, the moving average of predictions is compared with predicted
    labels in the current epoch. To avoid the data selection biased caused by one
    model, [[79](#bib.bib79)] uses two models to select an unlabeled set for each
    other. Afterward, each network is trained in a semi-supervised learning manner
    on the dataset chosen by its peer network. Another approach in this class is to
    train a network on labeled and unlabeled data and then use it to relabel noisy
    data [[82](#bib.bib82), [80](#bib.bib80)]. Assuming that correctly labeled data
    account for the majority, [[81](#bib.bib81)] proposes splitting datasets into
    labeled and unlabeled subgroups randomly. Then, labels are propagated to unlabeled
    data using a similarity index among instances. This procedure is repeated to produce
    multiple labels per instance, and then the final label is set with majority voting.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的选择是对标记和未标记的数据进行直接的半监督训练[[76](#bib.bib76)]。另一种方法是通过每个时期迭代地去除标签，以动态地更新数据集，从而更好地利用半监督学习。[[77](#bib.bib77)]
    使用给定标签和模型预测的移动平均之间的一致性来评估提供的标签是否存在噪声。然后，模型在下一个迭代中在干净的样本上进行训练。这个过程会持续进行，直到收敛到最佳估计器。[[78](#bib.bib78)]
    中使用了相同的方法，但做了一些小的调整。不是与给定标签进行比较，而是将预测的移动平均与当前时期的预测标签进行比较。为了避免由于一个模型导致的数据选择偏倚，[[79](#bib.bib79)]
    使用两个模型相互选择未标记的集合。之后，每个网络在其对等网络选择的数据集上以半监督学习的方式进行训练。这个类别中的另一种方法是对标记和未标记的数据进行训练，然后用它来重新标记噪声数据[[82](#bib.bib82),
    [80](#bib.bib80)]。假设正确标记的数据占大多数，[[81](#bib.bib81)] 提出了将数据集随机划分为标记和未标记的子组。然后，利用实例之间的相似性指数将标签传播到未标记的数据。这个过程会重复进行，为每个实例生成多个标签，最后通过多数投票来确定最终标签。
- en: 3.4 Sample Choosing
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 样本选择
- en: A widely used approach to overcome label noise is to manipulate the input stream
    to the classifier. Guiding the network with choosing the right instances to feed
    can help the classifier finding its way easier in the presence of noisy labels.
    It can be formulated as follows
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一种广泛使用的克服标签噪声的方法是操作输入流到分类器中。通过选择正确的实例来引导网络可以帮助分类器在存在噪声标签的情况下更容易找到其路径。它可以被表述为如下：
- en: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}V(x_{i},y_{i})l(f_{\theta}(x_{i}),\tilde{y_{i}}))$
    |  | (7) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}V(x_{i},y_{i})l(f_{\theta}(x_{i}),\tilde{y_{i}}))$
    |  | (7) |'
- en: 'where $V(x_{i},y_{i})\in\{0,1\}$ is a binary operator that decides to whether
    use the given data $(x_{i},y_{i})$ or not. If $V(x_{i},y_{i})=1$ for all data,
    then it turns out to be classical risk minimization ([Equation 7](#S3.E7 "7 ‣
    3.4 Sample Choosing ‣ 3 Noise Model Based Methods ‣ Image Classification with
    Deep Learning in the Presence of Noisy Labels: A Survey")). If $V$ happens to
    be a static function, which means choosing the same samples during whole training
    according to a predefined rule, then it turns out to be dataset pruning, as explained
    in [subsection 3.3](#S3.SS3 "3.3 Dataset Pruning ‣ 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey").
    Differently, sample choosing methods continuously monitor the base classifier
    and select samples to be trained on for the next training iteration. The task
    can be seen as drawing a path through data that would mimic the noise-free distribution
    of $\mathcal{D}$. Since these methods operate outside of the existing system,
    they are easier to attach to the existing algorithm at hand by just manipulating
    the input stream. However, it is vital to keep the balance so that system does
    not ignore unnecessarily large quantities of data. Additionally, these methods
    prioritize low loss samples, which results in a slow learning rate since hard
    informative samples are considered only in the later stages of training. Two major
    approaches under this group are discussed in the following subsections.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V(x_{i},y_{i})\in\{0,1\}$ 是一个二元运算符，用于决定是否使用给定的数据 $(x_{i},y_{i})$。如果所有数据的
    $V(x_{i},y_{i})=1$，那么这将变成经典的风险最小化（[公式 7](#S3.E7 "7 ‣ 3.4 样本选择 ‣ 3 基于噪声模型的方法 ‣
    在有噪声标签的情况下使用深度学习进行图像分类：一项调查")）。如果 $V$ 恰好是一个静态函数，即按照预定义规则在整个训练过程中选择相同的样本，那么这将变成数据集修剪，如
    [小节 3.3](#S3.SS3 "3.3 数据集修剪 ‣ 3 基于噪声模型的方法 ‣ 在有噪声标签的情况下使用深度学习进行图像分类：一项调查") 中所解释的那样。不同的是，样本选择方法持续监控基础分类器，并选择样本用于下一次训练迭代。这个任务可以被视为绘制一条路径，模拟
    $\mathcal{D}$ 的无噪声分布。由于这些方法在现有系统之外操作，它们更容易通过仅仅操控输入流来附加到现有算法上。然而，保持平衡至关重要，以防系统忽略不必要的大量数据。此外，这些方法优先考虑低损失样本，这会导致学习速率较慢，因为困难的有信息样本只在训练的后期被考虑。以下小节讨论了该组的两种主要方法。
- en: 3.4.1 Curriculum Learning
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 课程学习
- en: Curriculum learning (CL) [[161](#bib.bib161)], inspired from human cognition,
    proposes to start from easy samples and go through harder samples to guide training.
    This learning strategy is also called self-paced learning [[162](#bib.bib162),
    [163](#bib.bib163)] when prior to sample hardness is not known and inferred from
    the loss of the current model on that sample. In the noisy label framework, clean
    labeled data can be accepted as an easy task, while noisily labeled data is the
    harder task. Therefore, the idea of CL can be transferred to label noise setup
    as starting from confidently clean instances and go through noisier samples as
    the classifier gets better. Various screening loss functions are proposed in [[83](#bib.bib83)]
    to sort instances according to their noisiness level. The teacher-student approach
    is implemented in [[84](#bib.bib84)], where the teacher’s task is to choose confidently
    clean samples for the student. Instead of using a predefined curriculum, the teacher
    constantly updates its curriculum depending on the student’s outputs. Arguing
    that CL slows down the learning speed, since it focuses on easy samples, [[85](#bib.bib85)]
    suggests choosing uncertain samples that are mispredicted sometimes and correctly
    on others during training. These samples are assumed to be probably not noisy
    since noisy samples should be mispredicted all the time. Arguing that it is hard
    to optimize 0-1 loss, curriculum loss that chooses samples with low loss values
    for loss calculation, is proposed as an upper bound for 0-1 loss in [[86](#bib.bib86)].
    In [[87](#bib.bib87)], data is split into subgroups according to their complexities.
    Since less complex data groups are expected to have more clean labels, training
    will start from less complex data and go through more complex instances as the
    network gets better. Next samples to be trained on can be chosen by checking the
    consistency of the label with the network prediction. In [[88](#bib.bib88)], if
    both label and model prediction of the given sample is consistent, it is used
    in the training set. Otherwise, the model has a right to disagree. Iteratively
    this provides better training data and a better model. However, there is a risk
    of the model being too skeptical and choosing labels in a delusional way; therefore,
    consistency balance should be established.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习（CL）[[161](#bib.bib161)]，受人类认知的启发，提出从简单样本开始，逐步处理更难样本以指导训练。当对样本的难度未知并从当前模型在该样本上的损失推断时，这种学习策略也称为自适应学习[[162](#bib.bib162),
    [163](#bib.bib163)]。在噪声标签框架中，干净标记的数据可以被视为简单任务，而噪声标记的数据则是更难的任务。因此，CL的理念可以转移到标签噪声设置中，从可靠的干净实例开始，随着分类器的提高，逐步处理更嘈杂的样本。[[83](#bib.bib83)]中提出了各种筛选损失函数，用于根据噪声水平对实例进行排序。[[84](#bib.bib84)]中实现了师生方法，其中教师的任务是选择可靠的干净样本供学生使用。教师不断根据学生的输出更新其课程，而不是使用预定义的课程。由于CL在关注简单样本时会减慢学习速度，[[85](#bib.bib85)]建议选择那些有时被错误预测、有时被正确预测的不确定样本。这些样本被认为可能不是噪声样本，因为噪声样本应该总是被错误预测。[[86](#bib.bib86)]中提出了课程损失作为0-1损失的上界，选择低损失值的样本进行损失计算，因为优化0-1损失很困难。在[[87](#bib.bib87)]中，数据根据复杂性分成子组。由于较少复杂的数据组预计有更多的干净标签，因此训练将从较少复杂的数据开始，并随着网络的改进，处理更复杂的实例。可以通过检查标签与网络预测的一致性来选择下一个训练样本。在[[88](#bib.bib88)]中，如果给定样本的标签与模型预测一致，则将其用于训练集。否则，模型有权不同意。这种迭代过程提供了更好的训练数据和更好的模型。然而，模型有可能过于怀疑，以一种妄想的方式选择标签；因此，需要建立一致性平衡。
- en: 3.4.2 Multiple Classifiers
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 多分类器
- en: Some works use multiple classifiers to help each other to choose the next batch
    of data to train on. This is different from the teacher-student approach since
    none of the networks supervise the other but rather help each other out. Multiple
    models can provide robustness since networks can correct each other’s mistakes
    due to their differences in learned representations. For this setup to work, the
    initialization of the classifiers is essential. They are most likely to be initialized
    with a different subset of the data. If they have the same weight initializations,
    there is no update since they agree to disagree with labels. In [[89](#bib.bib89)]
    label is assumed to be noisy if both networks disagree with the given label, and
    update on model weights happens only when the prediction of two networks conflicts.
    The paradigm of co-teaching is introduced in [[90](#bib.bib90)], where two networks
    select the next batch of data for each other. The next batch is chosen as the
    data batch, which has small loss values according to the pair network. It is claimed
    that using one network accumulates the noise-related error, whereas two networks
    filter noise error more successfully. The idea of co-teaching is further improved
    by iterating over data where two networks disagree to prevent two networks converging
    each other with the increasing number of epochs [[91](#bib.bib91), [92](#bib.bib92)].
    Another work using co-teaching first trains two networks on a selected subset
    for a given number of epochs and then moves to the full dataset [[93](#bib.bib93)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作使用多个分类器互相帮助选择下一批数据进行训练。这与师生方法不同，因为这些网络互不监督，而是互相帮助。多个模型可以提供鲁棒性，因为网络可以通过它们在学习表示上的差异来纠正彼此的错误。为了使这种设置有效，分类器的初始化至关重要。它们很可能会用不同的数据子集初始化。如果它们具有相同的权重初始化，则不会有更新，因为它们对标签的分歧没有改变。在
    [[89](#bib.bib89)] 中，如果两个网络对给定标签的意见不一致，则假定标签有噪声，模型权重的更新仅发生在两个网络的预测冲突时。[[90](#bib.bib90)]
    中介绍了共同教学的范式，其中两个网络为对方选择下一批数据。下一批数据是根据对网络对的损失值较小的数据批次选择的。声称使用一个网络会累积噪声相关的误差，而两个网络则更成功地过滤噪声误差。共同教学的思想通过对两个网络不一致的数据进行迭代进一步改进，以防止两个网络在训练周期数增加时相互收敛
    [[91](#bib.bib91), [92](#bib.bib92)]。另一项使用共同教学的工作首先在选定的子集上训练两个网络若干个周期，然后转到完整数据集
    [[93](#bib.bib93)]。
- en: 3.5 Sample Importance Weighting
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 样本重要性加权
- en: Similar to sample choosing, training can be made more effective by assigning
    weights to instances according to their estimated noisiness level. This has an
    effect of emphasizing cleaner instances for a better update on model weights.
    Following empirical risk is minimized by these algorithms.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于样本选择，通过根据估计的噪声水平为实例分配权重可以使训练更加有效。这可以强调更干净的实例，从而更好地更新模型权重。这些算法通过最小化经验风险来实现。
- en: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}\beta(x_{i},y_{i})l(f_{\theta}(x_{i}),\tilde{y_{i}}))$
    |  | (8) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{R}_{l,\mathcal{D}}(f)=\dfrac{1}{N}\sum_{i=1}^{N}\beta(x_{i},y_{i})l(f_{\theta}(x_{i}),\tilde{y_{i}}))$
    |  | (8) |'
- en: 'where $\beta(x_{i},y_{i})$ determines the instance dependent weight. If $\beta$
    would be binary, then formulation is the same with sample choosing, as explained
    in [subsection 3.4](#S3.SS4 "3.4 Sample Choosing ‣ 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey").
    Differently, here $\beta$ is not binary and has a different value for each instance.
    Like in sample choosing algorithms, $\beta$ is a dynamic function, which means
    weights for instances keep changing during the training. Therefore, it is commonly
    a challenge to prevent $\beta$ changing too rapidly and sharply, such that it
    disrupts the stabilized training loop. Moreover, these methods commonly suffer
    from accumulated errors. As a result, they can easily get biased towards a certain
    subset of data. There are various methods proposed to obtain optimal $\beta$ to
    fade away the negative effects of noise.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '$\beta(x_{i},y_{i})$ 决定了实例依赖的权重。如果 $\beta$ 是二进制的，则公式与样本选择相同，如 [子节 3.4](#S3.SS4
    "3.4 Sample Choosing ‣ 3 Noise Model Based Methods ‣ Image Classification with
    Deep Learning in the Presence of Noisy Labels: A Survey") 中所解释的那样。不同的是，这里 $\beta$
    不是二进制的，并且每个实例具有不同的值。与样本选择算法一样，$\beta$ 是一个动态函数，这意味着实例的权重在训练过程中不断变化。因此，防止 $\beta$
    变化过快或过于剧烈以免干扰稳定的训练循环常常是一个挑战。此外，这些方法通常会受到累积误差的困扰。因此，它们很容易偏向于某一数据子集。有多种方法被提出以获取最佳的
    $\beta$，以减少噪声的负面影响。'
- en: The simplest approach would be, in case of availability of both clean and noisy
    data, weighting clean data more [[50](#bib.bib50)]. However, this utilizes information
    poorly; moreover, clean data is not always available. Works of [[94](#bib.bib94)]
    and [[95](#bib.bib95)], uses meta-learning paradigm to determine the weighting
    factor. In each iteration, gradient descent step on the given mini-batch for weighting
    factor is performed so that it minimizes the loss on clean validation data. A
    similar method is adopted in [[96](#bib.bib96)], but instead of implicit calculation
    of the weighting factor, multi layer perceptron (MLP) is used to estimate the
    weighting function. Open-set noisy labels, where data samples associated with
    noisy labels might belong to a true class that is not present in the training
    data, are considered in [[97](#bib.bib97)]. Siamese network is trained to detect
    noisy labels by learning discriminative features to apart clean and noisy data.
    Noisy samples are iteratively detected and pulled from clean samples. Then, each
    iteration weighting factor is recalculated for noisy samples, and the base classifier
    is trained on the whole dataset. [[28](#bib.bib28)] also iteratively separates
    noisy samples and clean samples. On top of that, not to miss valuable information
    from clean hard samples, noisy data are weighted according to their noisiness
    level, estimated by pLOF [[164](#bib.bib164)]. [[98](#bib.bib98)] introduces abstention,
    which gives option to abstain samples, depending on their loss value, with an
    abstention penalty. Therefore, the network learns to abstain from confusing samples,
    and with the abstention penalty, the tendency to abstain can be adjusted. In [[99](#bib.bib99)],
    weighting factor is conditioned on distribution of training data, $\beta(X,Y)=P_{\mathcal{D}}(X,Y)/P_{\mathcal{D}_{n}}(X,\tilde{Y})$.
    The same methodology is extended to the multi-class case in [[100](#bib.bib100)].
    In [[101](#bib.bib101)], the weighting factor is determined by checking instance
    similarity to its representative class prototype in the feature domain. [[102](#bib.bib102)]
    formulates the problem as transfer learning where the source domain is noisy data,
    and the target domain is a clean subset of data. Then weighting in the source
    domain is arranged in a way to minimize target domain loss. [[103](#bib.bib103)]
    uses $\theta$ values of samples in $\theta$-distribution to calculate their probability
    of being clean and use this information to weight clean samples more in training.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是，如果同时存在干净数据和噪声数据，则对干净数据进行更多的加权[[50](#bib.bib50)]。然而，这种方法利用信息不充分；此外，干净数据并不总是可用。[[94](#bib.bib94)]和[[95](#bib.bib95)]的研究使用了元学习范式来确定加权因子。在每次迭代中，针对给定的小批量数据进行梯度下降步骤，以便最小化在干净验证数据上的损失。[[96](#bib.bib96)]采用了类似的方法，但不是隐式计算加权因子，而是使用多层感知器（MLP）来估计加权函数。[[97](#bib.bib97)]考虑了开放集噪声标签，其中与噪声标签关联的数据样本可能属于训练数据中不存在的真实类别。Siamese网络通过学习区分特征来检测噪声标签，将干净数据与噪声数据区分开来。噪声样本被迭代检测并从干净样本中剔除。然后，在每次迭代中，为噪声样本重新计算加权因子，并在整个数据集上训练基础分类器。[[28](#bib.bib28)]也迭代地分离噪声样本和干净样本。此外，为了不遗漏干净难样本中的有价值信息，噪声数据根据其噪声水平进行加权，该水平由pLOF[[164](#bib.bib164)]估计。[[98](#bib.bib98)]引入了弃权机制，根据样本的损失值提供弃权选项，并附有弃权罚款。因此，网络学会从混淆样本中弃权，并且可以通过弃权罚款调整弃权倾向。在[[99](#bib.bib99)]中，加权因子依赖于训练数据的分布，$\beta(X,Y)=P_{\mathcal{D}}(X,Y)/P_{\mathcal{D}_{n}}(X,\tilde{Y})$。相同的方法在[[100](#bib.bib100)]中扩展到多分类情况。在[[101](#bib.bib101)]中，通过检查实例在特征域中与其代表性类别原型的相似性来确定加权因子。[[102](#bib.bib102)]将问题公式化为迁移学习，其中源领域是噪声数据，目标领域是干净数据的子集。然后，在源领域中安排加权以最小化目标领域的损失。[[103](#bib.bib103)]使用$\theta$分布中的样本$\theta$值来计算其为干净样本的概率，并利用该信息在训练中给予干净样本更多的加权。
- en: '![Refer to caption](img/1720a64efae6593584c7047d0e3d203c.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1720a64efae6593584c7047d0e3d203c.png)'
- en: 'Figure 3: Illustration of different types of algorithms. Starting from left;
    1) representation of samples from a single class in the 2d-space. Green samples
    represent the clean samples, and red ones represent noisy samples. 2) label noise
    cleaning algorithms aim to correct the labels of noisy data 3) dataset pruning
    methods aim to eliminate noisy data (or just their labels) 4) sample importance
    weighting algorithms aim to up-weight clean samples and down-weight noisy samples
    (which is illustrated by size)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：不同类型算法的示意图。从左到右；1）表示单一类别样本在二维空间中的分布。绿色样本代表干净样本，红色样本代表噪声样本。2）标签噪声清理算法旨在修正噪声数据的标签。3）数据集剪枝方法旨在消除噪声数据（或仅其标签）。4）样本重要性加权算法旨在增加干净样本的权重，减少噪声样本的权重（通过大小进行说明）。
- en: 3.6 Labeler Quality Assessment
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 标注员质量评估
- en: 'As explained in [subsection 2.3](#S2.SS3 "2.3 Sources of Label Noise ‣ 2 Preliminaries
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"),
    there can be several reasons for the dataset to be labeled by multiple annotators.
    Each labeler may have a different level of expertise, and their labels may occasionally
    contradict each other. This is a typical case in crowd-sourced data [[165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167)] or datasets that require a high level
    of expertise such as medical imaging [[19](#bib.bib19)]. Therefore, modeling and
    using labeler characteristics can significantly increase performance [[25](#bib.bib25)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[2.3节](#S2.SS3 "2.3 标签噪声来源 ‣ 2 初步 ‣ 有噪声标签的深度学习图像分类：综述")中所解释，数据集可能由多个标注员进行标注。每个标注员可能拥有不同的专业水平，他们的标签有时可能相互矛盾。这是众包数据[[165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167)]或需要高水平专业知识的数据集（如医学影像[[19](#bib.bib19)]）中的典型情况。因此，建模和利用标注员特征可以显著提高性能[[25](#bib.bib25)]。
- en: 'In this setup, there are two unknowns; noisy labeler characteristics and ground
    truth labels. One can estimate both with expectation-maximization [[104](#bib.bib104),
    [105](#bib.bib105), [26](#bib.bib26)]. If noise is assumed to be y-dependent,
    the labeler characteristic can be modeled with a noise transition matrix, just
    like in [subsection 3.1](#S3.SS1 "3.1 Noisy Channel ‣ 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey").
    [[106](#bib.bib106)] adds a regularizer to the loss function, which is the sum
    of traces of annotator confusion matrices, to force sparsity on each labeler’s
    estimated confusion matrix. A similar approach is implemented in [[107](#bib.bib107)],
    where a crowd-layer is added to the end of the network. In [[108](#bib.bib108)],
    xy-dependent noise is also considered by taking image complexities into account.
    Human annotators and computer vision systems are used mutually in [[109](#bib.bib109)],
    where consistency among predictions of these two components is used to evaluate
    labelers’ reliability. [[110](#bib.bib110)] deals with the noise when the labeler
    omits a tag in the image. Therefore, instead of the noise transition matrix for
    labelers, the omitting probability variable is used, which is estimated together
    with the true class using the expectation-maximization algorithm. Separate softmax
    layers are trained for each annotator in [[25](#bib.bib25)] and an additional
    network to predict the true class of data depending on the outputs of labeler
    specific networks and features of data. This setup enables to model each labeler
    and their overall noise structure in separate networks.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，有两个未知量：噪声标注员特征和真实标签。可以通过期望最大化方法估计这两者[[104](#bib.bib104), [105](#bib.bib105),
    [26](#bib.bib26)]。如果噪声假设为y依赖的，可以用噪声转移矩阵来建模标注员特征，就像在[subsection 3.1](#S3.SS1 "3.1
    噪声通道 ‣ 3 噪声模型方法 ‣ 有噪声标签的深度学习图像分类：综述")中那样。[[106](#bib.bib106)] 在损失函数中添加了一个正则化项，该正则化项是标注员混淆矩阵迹的和，以迫使每个标注员的估计混淆矩阵稀疏化。[[107](#bib.bib107)]
    实现了一种类似的方法，在网络的末端添加了一个众包层。[[108](#bib.bib108)] 通过考虑图像复杂度来处理xy依赖噪声。在[[109](#bib.bib109)]中，人类标注员和计算机视觉系统相互配合，其中这些两个组件的预测一致性用于评估标注员的可靠性。[[110](#bib.bib110)]
    处理了标注员在图像中遗漏标签时的噪声。因此，与其使用标注员的噪声转移矩阵，不如使用遗漏概率变量，该变量与真实类别一起通过期望最大化算法进行估计。在[[25](#bib.bib25)]中，为每个标注员训练了单独的softmax层，并且增加了一个额外的网络，以预测数据的真实类别，依据标注员特定网络的输出和数据特征。这种设置能够在不同的网络中建模每个标注员及其整体噪声结构。
- en: 3.7 Discussion
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 讨论
- en: 'A visual illustration of some of the methods is presented in [Figure 3](#S3.F3
    "Figure 3 ‣ 3.5 Sample Importance Weighting ‣ 3 Noise Model Based Methods ‣ Image
    Classification with Deep Learning in the Presence of Noisy Labels: A Survey").
    Noise model based methods are heavily dependent on the accurate estimate of the
    noise structure. This brings a dilemma. For a better noise model, one needs better
    estimators, and for better estimators, it is necessary to have a better estimate
    of underlying noise. Therefore, many approaches can be seen as an expectation-maximization
    of both noise estimation and classification. However, it is essential to prevent
    the system diverging from reality. Therefore regularizing noise estimates and
    not letting it getting delusional is crucial. To achieve this, the literature
    commonly makes assumptions about the underlying noise structure, which damages
    their applicability to different setups. On the other hand, this lets any prior
    information about the noise be inserted into the system for a head-start. It is
    also useful to handle domain-specific noise. One another advantage of these algorithms
    is they decouple noise estimation and classification tasks. Therefore, they are
    easier to implement on an existing classification algorithm at hand.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '一些方法的视觉示意图展示在[图 3](#S3.F3 "Figure 3 ‣ 3.5 Sample Importance Weighting ‣ 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey")中。噪声模型方法在很大程度上依赖于噪声结构的准确估计。这带来了一个困境。为了获得更好的噪声模型，需要更好的估计器，而要获得更好的估计器，则需要对基础噪声有更好的估计。因此，许多方法可以看作是噪声估计和分类的期望最大化。然而，防止系统偏离现实是至关重要的。因此，规范化噪声估计，防止其变得不切实际是关键。为了实现这一点，文献中通常对基础噪声结构做出假设，这会损害其在不同设置下的适用性。另一方面，这允许将任何关于噪声的先验信息插入系统以获得先机。这对于处理特定领域的噪声也很有用。这些算法的另一个优点是它们将噪声估计和分类任务解耦。因此，它们更容易在现有分类算法上实现。'
- en: 4 Noise Model Free Methods
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 无噪声模型的方法
- en: These methods aim to achieve label noise robustness without explicitly modeling
    it, but rather designing robustness in the proposed algorithm. Noisy data is treated
    as an anomaly; therefore, these methods are similar to overfit avoidance. They
    commonly rely on the classifier’s internal noise tolerance and aim to boost performance
    by regularizing undesired memorization of noisy data. Various methodologies are
    presented in the following subsections.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法旨在在不明确建模标签噪声的情况下实现标签噪声鲁棒性，而是通过设计算法中的鲁棒性来实现。噪声数据被视为异常；因此，这些方法类似于过拟合避免。它们通常依赖于分类器的内部噪声容忍度，并旨在通过规范化对噪声数据的非期望记忆来提升性能。以下小节中介绍了各种方法论。
- en: 4.1 Robust Losses
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 鲁棒损失
- en: A loss function is said to be noise robust if the classifier learned with noisy
    and noise-free data achieves the same classification accuracy [[111](#bib.bib111)].
    Algorithms under this section aim to design loss function so that the existence
    of the noise would not decrease the performance. However, it is shown that noise
    can badly affect the performance even for the robust loss functions [[15](#bib.bib15)].
    Moreover, these methods treat both noisy and clean data in the same way, which
    prevents the utilization of any prior information over data distribution.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器在有噪声和无噪声的数据上学习到的分类准确率相同，则称损失函数具有噪声鲁棒性[[111](#bib.bib111)]。本节下的算法旨在设计损失函数，使得噪声的存在不会降低性能。然而，已显示即使对于鲁棒损失函数，噪声也会严重影响性能[[15](#bib.bib15)]。此外，这些方法对有噪声和干净的数据采取相同的处理方式，这阻碍了对数据分布的任何先验信息的利用。
- en: In [[111](#bib.bib111)], it is shown that certain non-convex loss functions,
    such as 0-1 loss, has noise tolerance much more than commonly used convex losses.
    Extending this work [[112](#bib.bib112), [113](#bib.bib113)] derives sufficient
    conditions for a loss function to be noise tolerant for uniform noise. Their work
    shows that, if the given loss function satisfies $\sum_{k}l(f_{\theta}(x),k)=C,\forall
    x\in X$ where $C$ is a constant value, then loss function is tolerant to uniform
    noise. In this content, they empirically show that none of the standard convex
    loss functions has noise robustness while 0-1 loss has, up to a certain noise
    ratio. However, 0-1 loss is non-convex and non-differentiable; therefore, surrogate
    loss of 0-1 loss is proposed in [[114](#bib.bib114)], which is still noise sensitive.
    Widely used categorical cross entropy (CCE) loss is compared with mean absolute
    value of error (MAE) in the work of [[115](#bib.bib115)], where it is shown empirically
    that mean absolute value of error is more noise tolerant. [[116](#bib.bib116)]
    shows that the robustness of MEA is due to its weighting scheme. While CCE is
    sensitive to abnormal samples and produces bigger gradients in magnitude, MAE
    treats all data points equally, which would result in an underfitting of data.
    Therefore, Improved mean absolute value of error (IMAE), which is an improved
    version of MAE, is proposed in [[116](#bib.bib116)], where gradients are scaled
    with a hyper-parameter to adjusts weighting variance of MAE. [[117](#bib.bib117)]
    also argues that MAE provides a much lower learning rate than CCE; therefore,
    a new loss function is suggested, which combines the robustness of MAE and implicit
    weighting of CCE. With a tuning parameter, the loss function characteristic can
    be adjusted in a line from MAE to CCE. Loss functions are commonly not symmetric,
    meaning that $l(f_{\theta}(x_{i}),y_{i})\neq l(y_{i},f_{\theta}(x_{i}))$. Inspired
    from the idea of symmetric KL-divergence, [[118](#bib.bib118)] proposes symmetric
    cross entropy loss $l_{SCE}(f_{\theta}(x_{i}),y_{i})=l(f_{\theta}(x_{i}),y_{i})+l(y_{i},f_{\theta}(x_{i}))$
    to battle noisy labels.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[111](#bib.bib111)]中，展示了某些非凸损失函数（如0-1损失）的噪声容忍度远高于常用的凸损失函数。扩展这项工作[[112](#bib.bib112),
    [113](#bib.bib113)]推导出了损失函数对均匀噪声具有噪声容忍的充分条件。他们的工作表明，如果给定的损失函数满足$\sum_{k}l(f_{\theta}(x),k)=C,\forall
    x\in X$，其中$C$为常数值，则该损失函数对均匀噪声具有容忍度。在这方面，他们通过实验证明了标准的凸损失函数都不具有噪声鲁棒性，而0-1损失具有，直到某个噪声比率。然而，0-1损失是非凸的且不可微分的；因此，[[114](#bib.bib114)]中提出了0-1损失的替代损失，但它仍然对噪声敏感。在[[115](#bib.bib115)]的工作中，将广泛使用的分类交叉熵（CCE）损失与均值绝对误差（MAE）进行比较，实验证明均值绝对误差对噪声更具容忍性。[[116](#bib.bib116)]显示，MAE的鲁棒性归因于其加权方案。尽管CCE对异常样本敏感，并且梯度幅度更大，MAE对所有数据点一视同仁，这可能导致数据的欠拟合。因此，[[116](#bib.bib116)]中提出了改进的均值绝对误差（IMAE），这是MAE的改进版本，其中梯度通过超参数进行缩放，以调整MAE的加权方差。[[117](#bib.bib117)]还指出，MAE提供的学习率远低于CCE；因此，建议使用一种新的损失函数，它结合了MAE的鲁棒性和CCE的隐式加权。通过一个调节参数，可以在MAE和CCE之间调整损失函数的特性。损失函数通常是不对称的，这意味着$l(f_{\theta}(x_{i}),y_{i})\neq
    l(y_{i},f_{\theta}(x_{i}))$。受到对称KL散度的启发，[[118](#bib.bib118)]提出了对称交叉熵损失$l_{SCE}(f_{\theta}(x_{i}),y_{i})=l(f_{\theta}(x_{i}),y_{i})+l(y_{i},f_{\theta}(x_{i}))$以对抗噪声标签。
- en: Given that noise prior is known, [[119](#bib.bib119)] provides two surrogate
    loss functions using the prior information about label noise, namely, an unbiased
    and a weighted estimator of the loss function. [[120](#bib.bib120)] considers
    asymmetric omission noise for the binary classification case, where the task is
    to find road pixels from a satellite map image. Omission noise makes the network
    less confident about its predictions, so they modified cross-entropy loss to penalize
    the network less for producing wrong but confident predictions since these labels
    are more likely to be noisy. Instead of using distance-based loss, [[121](#bib.bib121)]
    proposes to use information-theoretic loss, in which determinant based mutual
    information [[168](#bib.bib168)] between given labels and predictions are evaluated
    for loss calculation. Weakly supervised learning with noisy labels are considered
    in [[122](#bib.bib122)], and necessary conditions for loss to be noise tolerant
    are drawn. [[123](#bib.bib123)] shows that classification-calibrated loss functions
    are asymptotically robust to symmetric label noise. Stochastic gradient descent
    with robust losses is analyzed in general [[124](#bib.bib124)] and shown to be
    more robust to label noise than its counterparts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在噪声先验已知的情况下，[[119](#bib.bib119)] 提供了两种利用先验信息的代理损失函数，即损失函数的无偏估计器和加权估计器。[[120](#bib.bib120)]
    针对二分类情况考虑了不对称的遗漏噪声，其中任务是从卫星地图图像中找到道路像素。遗漏噪声使得网络对其预测的信心降低，因此他们修改了交叉熵损失函数，以减少对产生错误但自信的预测的惩罚，因为这些标签更可能是噪声。[[121](#bib.bib121)]
    提出了使用信息论损失代替基于距离的损失，其中计算给定标签和预测之间的行列式互信息[[168](#bib.bib168)]用于损失计算。[[122](#bib.bib122)]
    讨论了带有噪声标签的弱监督学习，并提出了损失容忍噪声的必要条件。[[123](#bib.bib123)] 证明了分类校准损失函数在对称标签噪声下渐近鲁棒。对于具有鲁棒损失的随机梯度下降，一般来说[[124](#bib.bib124)]
    被分析并显示其在处理标签噪声时比传统方法更为鲁棒。
- en: 4.2 Meta Learning
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 元学习
- en: With the recent advancements of deep neural networks, the necessity of hand-designed
    features for computer vision systems are mostly eliminated. Instead, these features
    are learned autonomously via machine learning techniques. Even though these algorithms
    can learn complex functions on their own, there remain many hand-designed parameters
    such as network architecture, loss function, optimizer algorithm, and so on. Meta-learning
    aims to eliminate these necessities by learning not just the required complex
    function for the task but also learning the learning itself [[169](#bib.bib169),
    [170](#bib.bib170)]. Algorithms belonging to this category usually implement an
    additional learning loop for the meta objective, optimizing the base learning
    procedure. In general, the biggest drawback of these methods is their computational
    cost. Since they require nested loops of gradient computations for each training
    loop, they are several times slower than the conventional training process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度神经网络的最新进展，计算机视觉系统对手工设计特征的需求已大大减少。相反，这些特征通过机器学习技术自主学习。尽管这些算法可以自行学习复杂的函数，但仍然存在许多手工设计的参数，如网络架构、损失函数、优化算法等。元学习旨在通过不仅学习任务所需的复杂函数，还学习学习本身[[169](#bib.bib169),
    [170](#bib.bib170)]来消除这些需求。属于这一类别的算法通常会为元目标实现一个额外的学习循环，以优化基础学习过程。一般而言，这些方法最大的缺点是其计算成本。由于它们需要为每个训练循环计算嵌套的梯度循环，因此比传统训练过程慢几倍。
- en: Designing a task beyond classical supervised learning in a meta-learning fashion
    has been used to deal with label noise as well. A meta task is defined as predicting
    the most suitable method, among the family of methods, for a given noisy dataset
    in [[125](#bib.bib125)]. Pumpout [[126](#bib.bib126)] presents a meta objective
    as recovering the damage done by noisy samples by erasing their effect on model
    via scaled gradient ascent. As a meta-learning paradigm, model-agnostic-meta-learning
    (MAML) [[170](#bib.bib170)] seeks optimal weight initialization that can easily
    be fine-tuned for the desired objective. A similar mentality is used in [[127](#bib.bib127)]
    for noisy labels, which aims to find noise-tolerant model parameters that are
    less prone to noise under teacher-student training framework [[171](#bib.bib171),
    [172](#bib.bib172)]. Multiple student networks are fed with data corrupted by
    synthetic noise. A meta objective is defined to maximize consistency with teacher
    outputs obtained from raw data without synthetic noise. Therefore, student networks
    are forced to find most noise robust weight initialization such that weight update
    will still be consistent after training an epoch on synthetically corrupted data.
    Then, final classifier weights are set as an exponential moving average of student
    networks.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以元学习方式设计超越经典监督学习的任务也被用来处理标签噪声。在[[125](#bib.bib125)]中，元任务被定义为预测在给定噪声数据集的情况下，方法家族中最合适的方法。Pumpout
    [[126](#bib.bib126)]提出了一个元目标，即通过缩放梯度上升来恢复噪声样本造成的损害，从而抹去它们对模型的影响。作为一种元学习范式，模型无关元学习（MAML）[[170](#bib.bib170)]旨在寻求可以轻松调整以实现所需目标的最佳权重初始化。在[[127](#bib.bib127)]中，对于噪声标签采用了类似的思路，旨在寻找噪声容忍的模型参数，这些参数在师生训练框架[[171](#bib.bib171),
    [172](#bib.bib172)]下对噪声的敏感性较低。多个学生网络接收被合成噪声污染的数据。定义了一个元目标以最大化与从原始数据中获得的教师输出的一致性，而原始数据不含合成噪声。因此，学生网络被迫找到最具噪声鲁棒性的权重初始化，使得在对合成污染数据训练一个时期后，权重更新仍然保持一致。然后，最终分类器权重被设置为学生网络的指数移动平均值。
- en: Alternatively, in the case of available clean data, a meta objective can be
    defined to utilize this information. The approach used in [[128](#bib.bib128)]
    is to train a teacher network in a clean dataset and transfer its knowledge to
    the student network to guide the training process in the presence of mislabeled
    data. They used distillation technique proposed in [[173](#bib.bib173)] for controlled
    transfer of knowledge from teacher to student. A similar methodology of using
    distillation and label correction in the human pose estimation task is implemented
    in [[129](#bib.bib129)]. In [[130](#bib.bib130), [131](#bib.bib131)], the target
    network is trained on excessive noisy data, and the confidence network is trained
    on a clean subset. Inspiring from [[169](#bib.bib169)], the confidence network’s
    task is to control the magnitude of gradient updates to the target network so
    that noisy labels are not resulting in updating gradients. [[132](#bib.bib132)]
    uses clean data to produce soft labels for noisy data, for which the classifier
    trained on it would give the best performance on the clean data. As a result,
    it seeks optimal label distribution to provide the most noise robust learning
    for the base classifier.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在有干净数据的情况下，可以定义一个元目标来利用这些信息。[[128](#bib.bib128)]中使用的方法是在干净的数据集上训练一个教师网络，并将其知识转移到学生网络，以指导在存在标签错误数据的情况下的训练过程。他们使用了[[173](#bib.bib173)]中提出的蒸馏技术来控制从教师到学生的知识转移。在[[129](#bib.bib129)]中，实现了在人类姿态估计任务中使用蒸馏和标签修正的类似方法。在[[130](#bib.bib130),
    [131](#bib.bib131)]中，目标网络在大量噪声数据上训练，而信心网络在一个干净的子集上训练。受到[[169](#bib.bib169)]的启发，信心网络的任务是控制对目标网络的梯度更新幅度，以确保噪声标签不会导致梯度更新。[[132](#bib.bib132)]使用干净数据为噪声数据生成软标签，训练出的分类器在干净数据上表现最佳。因此，它寻求最佳标签分布，以提供对基础分类器最具噪声鲁棒性的学习。
- en: 4.3 Regularizers
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 正则化器
- en: 'Regularizers are well known to prevent DNNs from overfitting noisy labels.
    From this perspective, these methods treat performance degradation due to noisy
    data as overfitting to noise. Even though this assumption is mostly valid in random
    noise, it may not be the case for more complex noises. Some widely used techniques
    are weight decay, dropout [[133](#bib.bib133)], adversarial training [[134](#bib.bib134)],
    mixup [[135](#bib.bib135)], label smoothing [[136](#bib.bib136), [137](#bib.bib137)].
    [[138](#bib.bib138)] shows that pre-training has a regularization effect in the
    presence of noisy labels. In [[139](#bib.bib139)] an additional softmax layer
    is added, and dropout regularization is applied to this layer, arguing that it
    provides more robust training and prevents memorizing noise due to randomness
    of dropout [[133](#bib.bib133)]. [[140](#bib.bib140)] proposes a complexity measure
    to understand if the network starts to overfit. It is shown that learning consists
    of two steps: 1) dimensionality compression, which models low-dimensional subspaces
    that closely match the underlying data distribution, 2) dimensionality expansion,
    which steadily increases subspace dimensionality to overfit the data. The key
    is to stop before the second step. Local intrinsic dimensionality [[174](#bib.bib174)]
    is used to measure the complexity of the trained model and stop before it starts
    to overfit. [[141](#bib.bib141)] takes a pre-trained network on a different domain
    and fine-tunes it for the noisy labeled dataset. Groups of image features are
    formed, and group sparsity regularization is imposed so that model is forced to
    choose relative features and up-weights the reliable images.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化方法众所周知能够防止深度神经网络（DNN）在噪声标签上过拟合。从这个角度来看，这些方法将由于噪声数据引起的性能下降视为对噪声的过拟合。尽管这一假设在随机噪声中大多是有效的，但对于更复杂的噪声情况可能并非如此。一些广泛使用的技术包括权重衰减、丢弃法
    [[133](#bib.bib133)]、对抗训练 [[134](#bib.bib134)]、混合训练 [[135](#bib.bib135)] 和标签平滑
    [[136](#bib.bib136), [137](#bib.bib137)]。[[138](#bib.bib138)] 显示了在存在噪声标签的情况下，预训练具有正则化效果。在
    [[139](#bib.bib139)] 中添加了一个额外的 softmax 层，并对该层应用了丢弃法正则化，认为这提供了更稳健的训练，并防止了由于丢弃法的随机性而记住噪声
    [[133](#bib.bib133)]。[[140](#bib.bib140)] 提出了一个复杂性度量来理解网络是否开始过拟合。研究表明，学习包含两个步骤：1）维度压缩，即建模与底层数据分布紧密匹配的低维子空间，2）维度扩展，即稳步增加子空间维度以过拟合数据。关键是要在第二步之前停止。局部内在维度
    [[174](#bib.bib174)] 用于测量训练模型的复杂性，并在开始过拟合之前停止。[[141](#bib.bib141)] 采用在不同领域预训练的网络，并对带有噪声标签的数据集进行微调。形成图像特征组，并施加组稀疏正则化，以便模型被迫选择相关特征并增加可靠图像的权重。
- en: 4.4 Ensemble Methods
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 集成方法
- en: It is well known that bagging is more robust to label noise than boosting [[175](#bib.bib175)].
    Boosting algorithms like AdaBoost puts too much weight on noisy samples, resulting
    in overfitting the noise. However, the degree of label noise robustness changes
    for the chosen boosting algorithm. For example, it is shown that BrownBoost and
    LogitBoost are more robust than AdaBoost [[142](#bib.bib142)]. Therefore, noise-robust
    alternatives of AdaBoost is proposed in literature, such as noise detection based
    AdaBoost [[143](#bib.bib143)], rBoost [[144](#bib.bib144)], RBoost1&RBoost2 [[145](#bib.bib145)]
    and robust multi-class AdaBoost [[146](#bib.bib146)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，袋装法比提升法对标签噪声更具鲁棒性 [[175](#bib.bib175)]。像 AdaBoost 这样的提升算法对噪声样本赋予了过多的权重，导致过拟合噪声。然而，对于所选择的提升算法，标签噪声鲁棒性的程度有所不同。例如，研究表明
    BrownBoost 和 LogitBoost 比 AdaBoost 更具鲁棒性 [[142](#bib.bib142)]。因此，文献中提出了 AdaBoost
    的噪声鲁棒替代方法，例如基于噪声检测的 AdaBoost [[143](#bib.bib143)]、rBoost [[144](#bib.bib144)]、RBoost1&RBoost2
    [[145](#bib.bib145)] 和鲁棒多类 AdaBoost [[146](#bib.bib146)]。
- en: 4.5 Others
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 其他
- en: Complementary labels define classes that observations do not belong to. For
    example, in the case of ten classes, there is one true class and nine complimentary
    classes for each instance. Since annotators are less likely to mislabel, some
    works propose to work in complementary label space [[147](#bib.bib147), [148](#bib.bib148)].
    [[149](#bib.bib149)] uses reconstruction error of autoencoder to discriminate
    noisy data from clean data, arguing that noisy data tend to have bigger reconstruction
    error. In [[150](#bib.bib150)], the base model is trained with noisy data. An
    additional generative classifier is trained on top of the feature space generated
    by the base model. By estimating its parameters with minimum covariance determinant,
    noise-robust decision boundaries are aimed to be found. In [[151](#bib.bib151)],
    a special setup is considered where dataset consists of noisy and less-noisy data
    for binary classification task. [[152](#bib.bib152)] aims to extract the quality
    of data instances. Assuming that the training dataset is generated from a mixture
    of the target distribution and other unknown distributions, it estimates the quality
    of data samples by checking the consistency between generated and target distributions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 互补标签定义了观测数据不属于的类别。例如，在十个类别的情况下，每个实例有一个真实类别和九个互补类别。由于注释者较少会标记错误，一些研究建议在互补标签空间中进行工作
    [[147](#bib.bib147), [148](#bib.bib148)]。[[149](#bib.bib149)] 使用自编码器的重建误差来区分噪声数据与干净数据，认为噪声数据通常具有较大的重建误差。在
    [[150](#bib.bib150)] 中，基模型在噪声数据上进行训练。一个额外的生成分类器在基模型生成的特征空间上进行训练。通过用最小协方差行列式估计其参数，旨在找到噪声鲁棒的决策边界。在
    [[151](#bib.bib151)] 中，考虑了一个特殊设置，其中数据集由噪声数据和较少噪声数据组成，用于二分类任务。[[152](#bib.bib152)]
    旨在提取数据实例的质量。假设训练数据集是由目标分布和其他未知分布的混合生成的，它通过检查生成分布和目标分布之间的一致性来估计数据样本的质量。
- en: Prototype learning aims to construct prototypes that can represent features
    of a class in order to learn clean representations. Some works in the literature
    [[153](#bib.bib153), [154](#bib.bib154)] propose to create clean representative
    prototypes for noisy data, so that base classifier can be trained on them instead
    of noisy labels.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 原型学习旨在构建可以代表类别特征的原型，以便学习干净的表示。一些文献中的研究 [[153](#bib.bib153), [154](#bib.bib154)]
    提议为噪声数据创建干净的代表性原型，以便可以在这些原型上训练基分类器，而不是噪声标签。
- en: In multiple-instance learning, data are grouped in clusters, called bags, and
    each bag is labeled as positive if there is at least one positive instance in
    it and negative otherwise. The network is fed with a group of data and produces
    a single prediction for each bag by learning the inner discriminative representation
    of data. Since the group of images is used and one prediction is made, the existence
    of noisy labels along with true labels in a bag has less impact on learning. In
    [[155](#bib.bib155)], authors propose to effectively choose training samples from
    each bag by minimizing the total bag level loss. Extra model is trained in [[156](#bib.bib156)]
    as an attention model, which determines parts of the images to be focused on.
    The aim is to focus on a few regions on correctly labeled images and not focus
    on any region for mislabeled images.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在多实例学习中，数据被分组为簇，称为袋，每个袋如果包含至少一个正实例则标记为正，否则标记为负。网络接收一组数据，并通过学习数据的内部区分表示为每个袋产生一个预测。由于使用了一组图像并做出一个预测，袋中噪声标签与真实标签的存在对学习的影响较小。在
    [[155](#bib.bib155)] 中，作者建议通过最小化总袋级损失来有效选择每个袋的训练样本。额外的模型在 [[156](#bib.bib156)]
    中作为注意力模型进行训练，该模型决定需要关注的图像部分。其目的是在正确标记的图像上集中注意力于少数区域，而对错误标记的图像则不关注任何区域。
- en: 4.6 Discussion
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 讨论
- en: Overall, methods belonging to this category treat noisy data as an anomaly.
    Therefore, they are in a similar line with overfit avoidance and anomaly detection.
    Even though this assumption may be quite valid for random noise, it loses its
    validity in the case of more complicated and structured noises. Since noise modeling
    is not decoupled from classification task explicitly, proposed methods are, in
    the general sense, embedded into the existing algorithm. This prevents their quick
    deployment to the existing system at hand. Moreover, algorithms belonging to meta-learning
    and ensemble methods can be computationally costly since they require multiple
    iterations of training loops.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这类方法将噪声数据视为异常。因此，它们与过拟合避免和异常检测的思路相似。尽管这种假设对于随机噪声可能相当有效，但在面对更复杂和结构化的噪声时，它的有效性会降低。由于噪声建模并未与分类任务明确分离，提出的方法一般都嵌入到现有算法中。这阻碍了它们在现有系统中的快速部署。此外，元学习和集成方法所涉及的算法可能计算成本较高，因为它们需要多次训练循环。
- en: 5 Experiments
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'This section discusses how the proposed algorithms from the literature conduct
    experiments to test their robustness against label noise. In general, for quick
    implementation and testing, most of the works start by testing on toy datasets
    (MNIST[[176](#bib.bib176)], MNIST-Fashion[[177](#bib.bib177)], CIFAR10&CIFAR100[[178](#bib.bib178)])
    with synthetic label noise. However, as explained in [subsection 2.2](#S2.SS2
    "2.2 Label Noise Models ‣ 2 Preliminaries ‣ Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey"), there are various types of artificial
    noises. Moreover, each work experiment with a different model architecture and
    hyper-parameter set. Therefore, results on these datasets are not appropriate
    for a fair comparison of the algorithms. They are instead used as a proof of concept
    for the proposed algorithm.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '本节讨论了文献中提出的算法如何进行实验以测试其对标签噪声的鲁棒性。一般来说，为了快速实现和测试，大多数研究开始时会在合成标签噪声的玩具数据集（MNIST[[176](#bib.bib176)]，MNIST-Fashion[[177](#bib.bib177)]，CIFAR10&CIFAR100[[178](#bib.bib178)]）上进行测试。然而，如在[subsection
    2.2](#S2.SS2 "2.2 Label Noise Models ‣ 2 Preliminaries ‣ Image Classification
    with Deep Learning in the Presence of Noisy Labels: A Survey")中所述，存在各种类型的人工噪声。此外，每项研究使用不同的模型架构和超参数设置进行实验。因此，这些数据集上的结果不适合公平地比较算法。它们更多地作为提出算法的概念验证。'
- en: Some works from the literature use two alternative datasets. The first one is
    the Food101N dataset [[101](#bib.bib101)] containing 310k images of food recipes
    belonging to 101 different classes. However, its noise ratio is pretty small (around
    20%), making it inadequate to evaluate noise robust algorithms’ performance. The
    second option is the WebVision dataset [[179](#bib.bib179)] containing 2.4 million
    images crawled from Flickr website and Google Images search. This is a big dataset,
    which requires a lot of computational power to run algorithms. Some works conduct
    tests on this dataset by using data only from the first 50 classes, aiming to
    make it computationally feasible. But still, WebVision fails to provide a benchmarking
    dataset for the evaluation of noise robust algorithms.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文献中的研究使用了两个替代数据集。第一个是Food101N数据集[[101](#bib.bib101)]，包含310k张属于101个不同类别的食谱图像。然而，它的噪声比例相当小（约20%），使得它不适合用于评估噪声鲁棒算法的性能。第二个选项是WebVision数据集[[179](#bib.bib179)]，包含从Flickr网站和Google
    Images搜索中抓取的240万张图像。这是一个大数据集，需要大量的计算资源来运行算法。一些研究通过仅使用前50个类别的数据来对该数据集进行测试，以使其在计算上可行。但即便如此，WebVision仍未能提供一个用于评估噪声鲁棒算法的基准数据集。
- en: To fill the absence of a benchmarking dataset, [[53](#bib.bib53)] collects a
    large amount of images from the web with labels interpreted from the surrounding
    user tags. As a result, it has real-world noisy labels with an estimated noise
    ratio of around 40%. Dataset consists of one million images belonging to 14 different
    classes. 50K, 14K and 10K additional images with verified clean labels for train,
    validation and test purposes. We observed a high majority of the methods do not
    use additional 50K clean data for training. Furthermore, the literature seems
    to have a consensus on the experimental setup. All methods use the same model
    architecture of ResNet50 [[2](#bib.bib2)] with pre-trained parameters on Imagenet
    [[180](#bib.bib180)] and stochastic gradient descent optimizer. Considering the
    identical experimental setup and real-world noisiness of the dataset, the Clothing1M
    dataset is widely accepted as a benchmarking dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补基准数据集的空白，[[53](#bib.bib53)] 从网络上收集了大量图像，这些图像的标签来自周围用户标签的解释。因此，它包含了现实世界中的噪声标签，估计噪声比率约为40%。数据集包括一百万张属于14个不同类别的图像。还包含50K、14K和10K额外的已验证干净标签图像，用于训练、验证和测试目的。我们观察到，大多数方法并未使用额外的50K干净数据进行训练。此外，文献似乎对实验设置达成了一致。所有方法都使用相同的ResNet50
    [[2](#bib.bib2)] 模型架构，预训练参数来自Imagenet [[180](#bib.bib180)]，并使用随机梯度下降优化器。考虑到实验设置的一致性和数据集的现实噪声，Clothing1M数据集被广泛接受为基准数据集。
- en: 'We listed (to the best of our knowledge) all of the results presented on this
    dataset in [Table 2](#S5.T2 "Table 2 ‣ 5 Experiments ‣ Image Classification with
    Deep Learning in the Presence of Noisy Labels: A Survey"). We collected results
    only from the works trained on 1M noisy training data without additional 50K clean
    data for a fair evaluation. We sorted algorithms according to their test accuracy.
    Nevertheless, it should be noted that each method has its pros and cons, such
    as computational cost, memory requirements, etc.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了（根据我们所知）所有在该数据集上呈现的结果，见[表2](#S5.T2 "表2 ‣ 5个实验 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")。我们仅收集了在1M噪声训练数据上训练且没有额外50K干净数据的工作的结果，以进行公平评估。我们按测试准确率对算法进行了排序。然而，应该注意的是，每种方法都有其优缺点，如计算成本、内存需求等。
- en: '| Method | Category | Accuracy |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 准确率 |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[132](#bib.bib132)] | [Meta Learning](#S4.SS2 "In 4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 76.02 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | [元学习](#S4.SS2 "在4种噪声模型无关方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 76.02 |'
- en: '| [[79](#bib.bib79)] | [Dataset Pruning](#S3.SS3 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 74.76 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| [[79](#bib.bib79)] | [数据集剪枝](#S3.SS3 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 74.76 |'
- en: '| [[101](#bib.bib101)] | [Sample Importance Weighting](#S3.SS5 "In 3 Noise
    Model Based Methods ‣ Image Classification with Deep Learning in the Presence
    of Noisy Labels: A Survey") | 74.69 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | [样本重要性加权](#S3.SS5 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 74.69 |'
- en: '| [[66](#bib.bib66)] | [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") | 74.45 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | [标签噪声清理](#S3.SS2 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 74.45 |'
- en: '| [[51](#bib.bib51)] | [Noisy Channel](#S3.SS1 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 74.18 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | [噪声通道](#S3.SS1 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 74.18 |'
- en: '| [[75](#bib.bib75)] | [Dataset Pruning](#S3.SS3 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 73.77 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | [数据集剪枝](#S3.SS3 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 73.77 |'
- en: '| [[96](#bib.bib96)] | [Sample Importance Weighting](#S3.SS5 "In 3 Noise Model
    Based Methods ‣ Image Classification with Deep Learning in the Presence of Noisy
    Labels: A Survey") | 73.72 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | [样本重要性加权](#S3.SS5 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 73.72 |'
- en: '| [[61](#bib.bib61)] | [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") | 73.49 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | [标签噪声清理](#S3.SS2 "在3种噪声模型方法中 ‣ 在噪声标签存在的情况下使用深度学习进行图像分类：综述")
    | 73.49 |'
- en: '| [[127](#bib.bib127)] | [Meta Learning](#S4.SS2 "In 4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 73.47 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [[127](#bib.bib127)] | [元学习](#S4.SS2 "在 4 噪声模型无关方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 73.47 |'
- en: '| [[116](#bib.bib116)] | [Robust Losses](#S4.SS1 "In 4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 73.20 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | [鲁棒损失](#S4.SS1 "在 4 噪声模型无关方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 73.20 |'
- en: '| [[52](#bib.bib52)] | [Noisy Channel](#S3.SS1 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 73.07 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] | [噪声通道](#S3.SS1 "在 3 噪声模型基础方法 ‣ 噪声标签下的深度学习图像分类：综述") |
    73.07 |'
- en: '| [[153](#bib.bib153)] | [Others](#S4.SS5 "In 4 Noise Model Free Methods ‣
    Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 72.50 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [[153](#bib.bib153)] | [其他](#S4.SS5 "在 4 噪声模型无关方法 ‣ 噪声标签下的深度学习图像分类：综述") |
    72.50 |'
- en: '| [[121](#bib.bib121)] | [Robust Losses](#S4.SS1 "In 4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 72.46 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | [鲁棒损失](#S4.SS1 "在 4 噪声模型无关方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 72.46 |'
- en: '| [[60](#bib.bib60)] | [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") | 72.23 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | [标签噪声清理](#S3.SS2 "在 3 噪声模型基础方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 72.23 |'
- en: '| [[63](#bib.bib63)] | [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") | 71.74 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bib63)] | [标签噪声清理](#S3.SS2 "在 3 噪声模型基础方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 71.74 |'
- en: '| [[157](#bib.bib157)] | [Noisy Channel](#S3.SS1 "In 3 Noise Model Based Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 71.10 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | [噪声通道](#S3.SS1 "在 3 噪声模型基础方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 71.10 |'
- en: '| [[118](#bib.bib118)] | [Robust Losses](#S4.SS1 "In 4 Noise Model Free Methods
    ‣ Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey")
    | 71.02 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | [鲁棒损失](#S4.SS1 "在 4 噪声模型无关方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 71.02 |'
- en: '| [[64](#bib.bib64)] | [Label Noise Cleaning](#S3.SS2 "In 3 Noise Model Based
    Methods ‣ Image Classification with Deep Learning in the Presence of Noisy Labels:
    A Survey") | 71.00 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | [标签噪声清理](#S3.SS2 "在 3 噪声模型基础方法 ‣ 噪声标签下的深度学习图像分类：综述")
    | 71.00 |'
- en: 'Table 2: Leaderboard for algorithms tested on the Clothing1M dataset. All results
    are taken from the corresponding paper. For fair evaluation only the works which
    did not used additional 50k clean training data are presented.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 Clothing1M 数据集上测试的算法排行榜。所有结果均取自相关论文。为确保公平评价，仅展示未使用额外 50k 清洁训练数据的工作。
- en: 6 Conclusion
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Throughout this paper, it is shown that label noise is an important obstacle
    to deal with in order to achieve desirable performance from real-world datasets.
    Despite its importance for supervised learning in practical applications, it is
    also an important step to collect datasets from the web [[181](#bib.bib181), [182](#bib.bib182)],
    design networks that can learn from unlimited web data with no human supervision
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]. Furthermore,
    beside image classification, there are more fields where dealing with mislabeled
    instances is important, such as generative networks [[183](#bib.bib183), [184](#bib.bib184)],
    semantic segmentation [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)],
    sound classification [[185](#bib.bib185)] and more. All these factors make dealing
    with label noise an important step through self-sustained learning systems.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了标签噪声是实现从现实世界数据集中获得理想性能的一个重要障碍。尽管它在实际应用中的监督学习中非常重要，但收集来自网络的数据集[[181](#bib.bib181),
    [182](#bib.bib182)]、设计能够从无限网络数据中学习而无需人工监督的网络[[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40)]也是一个重要步骤。此外，除了图像分类外，还有更多领域需要处理标签错误的实例，如生成网络[[183](#bib.bib183),
    [184](#bib.bib184)]、语义分割[[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]、声音分类[[185](#bib.bib185)]等。这些因素使得处理标签噪声成为自我维持学习系统中的一个重要步骤。
- en: Different approaches to come through noisy label phenomenon are proposed in
    the literature. All methods have their advantages and disadvantages, so one can
    choose the most appropriate algorithm for the use case. However, in order to draw
    a generic line, we make the following suggestions. If the noise structure is domain-specific
    and there is prior information or assumption about its structure, noise model
    based methods are more appropriate. Among these models, one can choose the best-suited
    method according to need. For example, if noise can be represented as a noise
    transition matrix, noisy channel or labeler quality assessment for multi labeler
    case can be chosen. If the purpose is to purify the dataset as a preprocessing
    stage, then dataset pruning or label noise cleansing methods can be employed.
    Sample choosing or sample importance weighting algorithms are handy if instances
    can be ranked according to their informativeness on training. Unlike noise model-based
    algorithms, noise model free methods do not depend on any prior information about
    the noise structure. Therefore, they are easier to implement if noise is assumed
    to be random, and performance degradation is due to overfitting since they do
    not require the hassle of implementing an external algorithm for noise structure
    estimation. If there is no clean subset of data, robust losses or regularizers
    are appropriate options since they treat all samples the same. Meta-learning techniques
    can be used in the presence of a clean subset of data since they can easily be
    adapted to utilize this subset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了多种应对噪声标签现象的方法。所有方法都有其优缺点，因此可以根据具体情况选择最合适的算法。然而，为了画出一个通用的线，我们提出以下建议。如果噪声结构是领域特定的，并且对其结构有先验信息或假设，则基于噪声模型的方法更为合适。在这些模型中，可以根据需要选择最适合的方法。例如，如果噪声可以表示为噪声转移矩阵，则可以选择噪声通道或多标注者情况下的标注者质量评估。如果目的是将数据集作为预处理阶段进行净化，则可以采用数据集修剪或标签噪声清除方法。如果实例可以根据其对训练的有用性进行排序，则样本选择或样本重要性加权算法很方便。与基于噪声模型的算法不同，基于噪声模型自由的方法不依赖于噪声结构的任何先验信息。因此，如果噪声被假定为随机的，它们更容易实现，性能下降通常是由于过拟合，因为它们不需要实现一个外部算法来估计噪声结构。如果没有干净的数据子集，则鲁棒损失或正则化器是适当的选项，因为它们对所有样本一视同仁。元学习技术可以在存在干净数据子集的情况下使用，因为它们可以很容易地适应利用这个子集。
- en: 'Even though an extensive amount of research is conducted for machine learning
    techniques [[15](#bib.bib15)], deep learning in the presence of noisy labels is
    certainly an understudied problem. Considering its dramatic effect on DNNs [[11](#bib.bib11)],
    there still are many open research topics in the field. For example, truly understanding
    the impact of label noise on deep networks can be a fruitful future research topic.
    [[186](#bib.bib186)] shows that each layer of CNN learns to extract different
    features from the data. Moreover, learned representations form a hierarchical
    pattern, where each layer learns more complex features from the previous layer.
    A fully connected layer uses features from the last layer to interpret the corresponding
    label on the final layer. Understanding which parts of the network is highly affected
    by label noise may help analyze the adverse effect of the label noise on neural
    networks. For example, if initial layers are affected, one can conclude that learned
    primitive features are corrupted, so the rest of the network cannot be adequately
    trained. On the other hand, if final convolution layers are affected, it can be
    said that the network can not form the hierarchical feature pattern throughout
    the convolutional layers. Alternatively, if the convolutional layers are not affected
    but the fully connected layer is the cause of the problem, it can be concluded
    that feature representation learning is not corrupted, but the network cannot
    correctly interpret meanings from the extracted features. Moreover, it can be
    interesting to investigate the cause of the problem for different types of label
    noise models presented in [subsection 2.2](#S2.SS2 "2.2 Label Noise Models ‣ 2
    Preliminaries ‣ Image Classification with Deep Learning in the Presence of Noisy
    Labels: A Survey"). If for different noise models, different parts of the network
    are affected, one can analyze the true nature of the label noise in the dataset
    by checking corruption in the neural network layers.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管在机器学习技术上进行了大量研究[[15](#bib.bib15)]，但在噪声标签情况下的深度学习仍然是一个未充分研究的问题。考虑到其对深度神经网络的剧烈影响[[11](#bib.bib11)]，该领域仍有许多开放的研究课题。例如，真正理解标签噪声对深度网络的影响可能是一个有前景的未来研究主题。[186](#bib.bib186)显示，每一层的CNN都学习从数据中提取不同的特征。此外，学习到的表示形成了一个层级模式，每一层从前一层学习到更复杂的特征。全连接层利用上一层的特征来解释最终层上的对应标签。理解网络的哪些部分受到标签噪声的高度影响可能有助于分析标签噪声对神经网络的不利影响。例如，如果初始层受到影响，可以得出学习到的原始特征被破坏的结论，因此网络的其余部分无法得到充分训练。另一方面，如果最终卷积层受到影响，可以说网络无法在卷积层之间形成层级特征模式。或者，如果卷积层未受影响，但全连接层是问题的根源，可以得出特征表示学习没有被破坏的结论，但网络无法正确解读从提取特征中获得的含义。此外，调查不同类型标签噪声模型造成问题的原因可能会很有趣，如[subsection 2.2](#S2.SS2
    "2.2 Label Noise Models ‣ 2 Preliminaries ‣ Image Classification with Deep Learning
    in the Presence of Noisy Labels: A Survey")所述。如果不同的噪声模型影响网络的不同部分，可以通过检查神经网络层的腐败情况来分析数据集中标签噪声的真实性质。'
- en: Alternatively, the question of how to train in the existence of both attribute
    and label noise is an understudied problem with significant potential for practical
    applications [[55](#bib.bib55)]. [[118](#bib.bib118)] shows noisy labels degrades
    the learning, especially for challenging samples. So, instead of overfitting to
    noisy samples, underfitting to challenging samples may be the reason for the performance
    degradation, which is an open question to be answered in the future. Another possible
    research direction may be on the effort of breaking the structure of the noise
    to make it uniformly distributed in the feature domain [[81](#bib.bib81)]. This
    approach would be handy where labelers have a particular bias.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如何在属性和标签噪声共存的情况下进行训练是一个未被充分研究的问题，具有重要的实际应用潜力[[55](#bib.bib55)]。[118](#bib.bib118)显示，噪声标签会降低学习效果，特别是对具有挑战性的样本。因此，性能下降的原因可能是对噪声样本的过拟合，而不是对挑战样本的欠拟合，这是一个未来需要回答的开放性问题。另一个可能的研究方向可能是努力打破噪声的结构，使其在特征领域中均匀分布[[81](#bib.bib81)]。这种方法在标签者具有特定偏见的情况下特别有用。
- en: A widely used approach for quick testing of proposed algorithms is to create
    noisy datasets by adding synthetic label noise to benchmarking toy datasets [[176](#bib.bib176),
    [177](#bib.bib177), [178](#bib.bib178), [187](#bib.bib187), [188](#bib.bib188)].
    However, this prevents fair comparison and evaluation of algorithms since each
    work adds its own noise type. Some large datasets with noisy labels are proposed
    in literature [[9](#bib.bib9), [101](#bib.bib101), [179](#bib.bib179), [189](#bib.bib189)].
    These datasets are collected from the web, and labels are attained from noisy
    user tags. Even though these datasets provide a useful domain for benchmarking
    proposed solutions, their noise rates are mostly unknown and they are biased in
    terms of data distribution for classes. Moreover, one can not adjust the noise
    rate for testing under extreme or moderate conditions. From this perspective,
    we believe literature lacks a noisy dataset where a major part of it has both
    noisy and verified labels; thus, the noise rate can be adjusted as desired.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对提议算法进行快速测试的一个广泛使用的方法是通过向基准玩具数据集中添加合成标签噪声来创建嘈杂数据集[[176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [187](#bib.bib187), [188](#bib.bib188)]。然而，这种方法阻碍了算法的公平比较和评估，因为每项工作都添加了自己的噪声类型。文献中提出了一些具有嘈杂标签的大型数据集[[9](#bib.bib9),
    [101](#bib.bib101), [179](#bib.bib179), [189](#bib.bib189)]。这些数据集来自网络，标签是通过嘈杂的用户标签获得的。尽管这些数据集为基准测试提供了有用的领域，但它们的噪声率大多未知，并且在类别数据分布方面存在偏差。此外，无法在极端或中等条件下调整噪声率。从这个角度来看，我们认为文献中缺乏一个嘈杂数据集，其中大部分数据既有嘈杂标签也有已验证标签，从而可以根据需要调整噪声率。
- en: Minimal attention is given to the learning from a noisy labeled dataset when
    there is a small amount of data. This can be a fruitful research direction considering
    its potential in fields where harvesting dataset is costly. For example, in medical
    imaging, collecting a cleanly annotated large dataset is not feasible most of
    the time [[55](#bib.bib55)], due to its cost or privacy. Effectively learning
    from a small amount of noisy data with no ground truth can significantly improve
    autonomous medical diagnosis systems. Even though some pioneer researches are
    available [[25](#bib.bib25), [27](#bib.bib27), [28](#bib.bib28)], there is still
    much more to be explored.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量较少时，对来自嘈杂标注数据集的学习关注较少。考虑到在数据收集成本高的领域中，这可能是一个有前途的研究方向。例如，在医学影像中，由于成本或隐私问题，大规模清晰标注的数据集在大多数情况下不可行[[55](#bib.bib55)]。有效地从少量无真值的嘈杂数据中学习，可以显著提高自主医学诊断系统的性能。尽管已有一些先驱性研究[[25](#bib.bib25),
    [27](#bib.bib27), [28](#bib.bib28)]，但仍有许多需要探索的领域。
- en: The ability to effectively learn from noisily labeled data brings up big opportunities
    for practical applications of machine learning algorithms. The bottleneck of data
    collection can easily be resolved with the massive amount of data collected from
    the web. Labels for this data can be assigned with simple algorithms (such as
    interpreting from the surrounding text [[53](#bib.bib53)]). By effectively dealing
    with noisy labels, deep learning algorithms can be fed with massive datasets.
    Moreover, there are research opportunities for alternative usage of semi-supervised
    learning algorithms along with noisily labeled data. Common usage of semi-supervised
    learning methods for noisily labeled data is to remove the labels of noisy data
    and then train with conventional semi-supervised learning methods. Alternatively,
    algorithms can be developed to use three types of data; cleanly labeled data,
    noisily labeled data, and unlabeled data. With the help of these algorithms, a
    massive amount of unlabeled and noisy data can be effectively used under the supervision
    of a small cleanly annotated data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从嘈杂标注数据中有效学习的能力为机器学习算法的实际应用带来了巨大机遇。数据收集的瓶颈可以通过从网络收集的大量数据轻松解决。这些数据的标签可以通过简单的算法（例如从周围文本中解释[[53](#bib.bib53)]）来分配。通过有效处理嘈杂标签，深度学习算法可以处理大量数据集。此外，还存在研究机会，可以探索半监督学习算法在嘈杂标注数据上的替代用法。对嘈杂标注数据常见的半监督学习方法是去除嘈杂数据的标签，然后使用传统的半监督学习方法进行训练。另一种方法是开发能够使用三种数据类型的算法：清晰标注数据、嘈杂标注数据和未标注数据。借助这些算法，大量未标注和嘈杂的数据可以在少量清晰标注数据的监督下有效利用。
- en: Besides classification, knowledge of learning from noisily labeled data can
    be used in alternative fields by transforming the task. For example, in a multi-labeled
    dataset, where each instance belongs to multiple classes, not all classes are
    equally relevant. One can assume labels with a small resemblance to the data sample
    as noisy and employ algorithms designed for learning from noisy labels [[190](#bib.bib190)].
    Similarly, [[191](#bib.bib191)] divides an untrimmed video into smaller parts
    and aims to find the video snippet most relevant to the video tag. Irrelevant
    video parts are assumed to be noisily labeled. Even though the original dataset
    does not have noisy labels, it can be transformed to use algorithms from the field.
    As a result, learning from noisy labels has many potentials in various areas besides
    straight image classification.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分类，利用带有噪声标签的数据进行学习的知识还可以通过转换任务用于其他领域。例如，在多标签数据集中，每个实例属于多个类别，并非所有类别都同等相关。可以将与数据样本相似度小的标签视为噪声，并采用为处理噪声标签而设计的算法[[190](#bib.bib190)]。类似地，[[191](#bib.bib191)]将未裁剪的视频分成较小的部分，并旨在找到与视频标签最相关的视频片段。与视频标签无关的视频部分被视为噪声标签。即使原始数据集没有噪声标签，也可以通过转换来使用该领域的算法。因此，学习噪声标签在除直接图像分类之外的各种领域中具有很大的潜力。
- en: References
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: '[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
    deep convolutional neural networks, in: Advances in neural information processing
    systems, 2012, pp. 1097–1105.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
    deep convolutional neural networks, in: Advances in neural information processing
    systems, 2012, pp. 1097–1105.'
- en: '[2] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
- en: '[3] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
- en: '[4] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies
    for accurate object detection and semantic segmentation, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies
    for accurate object detection and semantic segmentation, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.'
- en: '[5] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, in: Advances in neural information processing
    systems, 2015, pp. 91–99.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, in: Advances in neural information processing
    systems, 2015, pp. 91–99.'
- en: '[6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg,
    Ssd: Single shot multibox detector, in: European conference on computer vision,
    Springer, 2016, pp. 21–37.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg,
    Ssd: Single shot multibox detector, in: European conference on computer vision,
    Springer, 2016, pp. 21–37.'
- en: '[7] G. Lin, C. Shen, A. Van Den Hengel, I. Reid, Efficient piecewise training
    of deep structured models for semantic segmentation, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2016, pp. 3194–3203.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Lin, C. Shen, A. Van Den Hengel, I. Reid, Efficient piecewise training
    of deep structured models for semantic segmentation, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2016, pp. 3194–3203.'
- en: '[8] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic
    segmentation, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2015, pp. 3431–3440.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic
    segmentation, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2015, pp. 3431–3440.'
- en: '[9] D. Rolnick, A. Veit, S. Belongie, N. Shavit, Deep learning is robust to
    massive label noise, arXiv preprint arXiv:1705.10694 (2017).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Rolnick, A. Veit, S. Belongie, N. Shavit, Deep learning is robust to
    massive label noise, arXiv preprint arXiv:1705.10694 (2017).'
- en: '[10] A. Drory, O. Ratzon, S. Avidan, R. Giryes, The resistance to label noise
    in k-nn and cnn depends on its concentration, arXiv preprint arXiv:1803.11410
    (2018).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Drory, O. Ratzon, S. Avidan, R. Giryes, The resistance to label noise
    in k-nn and cnn depends on its concentration, arXiv preprint arXiv:1803.11410
    (2018).'
- en: '[11] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep
    learning requires rethinking generalization, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep
    learning requires rethinking generalization, in: International Conference on Learning
    Representations, 2017.'
- en: '[12] D. Krueger, N. Ballas, S. Jastrzebski, D. Arpit, M. S. Kanwal, T. Maharaj,
    E. Bengio, A. Fischer, A. Courville, Deep nets don’t learn via memorization (2017).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Krueger, N. Ballas, S. Jastrzebski, D. Arpit, M. S. Kanwal, T. Maharaj,
    E. Bengio, A. Fischer, A. Courville, 深度网络不是通过记忆化学习的（2017）。'
- en: '[13] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, et al., A closer look at memorization
    in deep networks, in: Proceedings of the 34th International Conference on Machine
    Learning-Volume 70, JMLR. org, 2017, pp. 233–242.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, 等，深入了解深度网络中的记忆化，载于：第34届国际机器学习大会论文集-第70卷，JMLR.org，2017年，第233–242页。'
- en: '[14] X. Zhu, X. Wu, Class noise vs. attribute noise: A quantitative study,
    Artificial intelligence review 22 (3) (2004) 177–210.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Zhu, X. Wu, 类别噪声与属性噪声：定量研究，人工智能评论 22 (3) (2004) 177–210。'
- en: '[15] B. Frénay, M. Verleysen, Classification in the presence of label noise:
    a survey, IEEE transactions on neural networks and learning systems 25 (5) (2014)
    845–869.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Frénay, M. Verleysen, 标签噪声存在下的分类：综述，IEEE神经网络与学习系统交易 25 (5) (2014) 845–869。'
- en: '[16] B. Frénay, A. Kabán, et al., A comprehensive introduction to label noise,
    in: ESANN, 2014.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] B. Frénay, A. Kabán, 等，标签噪声的全面介绍，载于：ESANN，2014年。'
- en: '[17] R. Hataya, H. Nakayama, Investigating cnns’ learning representation under
    label noise (2018).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Hataya, H. Nakayama, 调查CNN在标签噪声下的学习表示（2018）。'
- en: '[18] D. F. Nettleton, A. Orriols-Puig, A. Fornells, A study of the effect of
    different types of noise on the precision of supervised learning techniques, Artificial
    intelligence review 33 (4) (2010) 275–306.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. F. Nettleton, A. Orriols-Puig, A. Fornells, 不同类型噪声对监督学习技术精度的影响研究，人工智能评论
    33 (4) (2010) 275–306。'
- en: '[19] M. Pechenizkiy, A. Tsymbal, S. Puuronen, O. Pechenizkiy, Class noise and
    supervised learning in medical domains: The effect of feature extraction, in:
    19th IEEE Symposium on Computer-Based Medical Systems (CBMS’06), IEEE, 2006, pp.
    708–713.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Pechenizkiy, A. Tsymbal, S. Puuronen, O. Pechenizkiy, 医疗领域中的类别噪声与监督学习：特征提取的影响，载于：第十九届IEEE计算机医学系统研讨会（CBMS’06），IEEE，2006年，第708–713页。'
- en: '[20] B. Li, Y. Wang, A. Singh, Y. Vorobeychik, Data poisoning attacks on factorization-based
    collaborative filtering, in: Advances in Neural Information Processing Systems,
    2016, pp. 1893–1901.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Li, Y. Wang, A. Singh, Y. Vorobeychik, 基于分解的协同过滤中的数据中毒攻击，载于：神经信息处理系统进展，2016年，第1893–1901页。'
- en: '[21] J. Steinhardt, P. W. Koh, P. Liang, Certified defenses for data poisoning
    attacks, in: Advances in Neural Information Processing Systems, Vol. 2017-Decem,
    2017, pp. 3518–3530.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Steinhardt, P. W. Koh, P. Liang, 数据中毒攻击的认证防御，载于：神经信息处理系统进展，2017年12月卷，2017年，第3518–3530页。'
- en: '[22] D. Angluin, P. Laird, Learning from noisy examples, Machine Learning 2 (4)
    (1988) 343–370.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. Angluin, P. Laird, 从噪声示例中学习，机器学习 2 (4) (1988) 343–370。'
- en: '[23] L. P. Garcia, J. Lehmann, A. C. de Carvalho, A. C. Lorena, New label noise
    injection methods for the evaluation of noise filters, Knowledge-Based Systems
    163 (2019) 693–704.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] L. P. Garcia, J. Lehmann, A. C. de Carvalho, A. C. Lorena, 新的标签噪声注入方法用于噪声过滤器的评估，知识基础系统
    163 (2019) 693–704。'
- en: '[24] G. Algan, İ. Ulusoy, Label noise types and their effects on deep learning,
    arXiv preprint arXiv:2003.10471 (2020).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] G. Algan, İ. Ulusoy, 标签噪声类型及其对深度学习的影响，arXiv预印本 arXiv:2003.10471 (2020)。'
- en: '[25] M. Y. Guan, V. Gulshan, A. M. Dai, G. E. Hinton, Who said what: Modeling
    individual labelers improves classification, in: Thirty-Second AAAI Conference
    on Artificial Intelligence, 2018.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Y. Guan, V. Gulshan, A. M. Dai, G. E. Hinton, 谁说了什么：建模个体标注者提升分类准确性，载于：第三十二届人工智能AAAI会议，2018年。'
- en: '[26] A. Khetan, Z. C. Lipton, A. Anandkumar, Learning from noisy singly-labeled
    data, arXiv preprint arXiv:1712.04577 (2017).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Khetan, Z. C. Lipton, A. Anandkumar, 从噪声单标签数据中学习，arXiv预印本 arXiv:1712.04577
    (2017)。'
- en: '[27] Y. Dgani, H. Greenspan, J. Goldberger, Training a neural network based
    on unreliable human annotation of medical images, in: 2018 IEEE 15th International
    Symposium on Biomedical Imaging (ISBI 2018), IEEE, 2018, pp. 39–42.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Dgani, H. Greenspan, J. Goldberger, 基于不可靠的人类注释训练神经网络用于医学图像，载于：2018
    IEEE第十五届生物医学成像国际研讨会（ISBI 2018），IEEE，2018年，第39–42页。'
- en: '[28] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, Robust learning at noisy
    labeled medical images: Applied to skin lesion classification, in: 2019 IEEE 16th
    International Symposium on Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1280–1283.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, 在带噪声标记的医学图像上进行鲁棒学习：应用于皮肤病变分类，载于：2019
    IEEE第十六届生物医学成像国际研讨会（ISBI 2019），IEEE，2019年，第1280–1283页。'
- en: '[29] Z. Lu, Z. Fu, T. Xiang, P. Han, L. Wang, X. Gao, Learning from weak and
    noisy labels for semantic segmentation, IEEE transactions on pattern analysis
    and machine intelligence 39 (3) (2016) 486–500.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Lu, Z. Fu, T. Xiang, P. Han, L. Wang, X. Gao, 从弱且噪声标签中学习用于语义分割, IEEE模式分析与机器智能杂志39
    (3) (2016) 486–500。'
- en: '[30] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, B. Catanzaro,
    Improving semantic segmentation via video propagation and label relaxation, in:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2019, pp. 8856–8865.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, B. Catanzaro,
    通过视频传播和标签松弛改进语义分割, 见：IEEE计算机视觉与模式识别会议论文集, 2019, 页8856–8865。'
- en: '[31] D. Acuna, A. Kar, S. Fidler, Devil is in the edges: Learning semantic
    boundaries from noisy annotations, in: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019, pp. 11075–11083.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Acuna, A. Kar, S. Fidler, 恶魔藏在边缘：从噪声标注中学习语义边界, 见：IEEE计算机视觉与模式识别会议论文集,
    2019, 页11075–11083。'
- en: '[32] P. Welinder, S. Branson, P. Perona, S. J. Belongie, The multidimensional
    wisdom of crowds, in: Advances in neural information processing systems, 2010,
    pp. 2424–2432.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] P. Welinder, S. Branson, P. Perona, S. J. Belongie, 群体的多维智慧, 见：神经信息处理系统进展,
    2010, 页2424–2432。'
- en: '[33] Y. Cha, J. Cho, Social-network analysis using topic models, in: Proceedings
    of the 35th international ACM SIGIR conference on Research and development in
    information retrieval, ACM, 2012, pp. 565–574.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Cha, J. Cho, 使用主题模型进行社交网络分析, 见：第35届国际ACM SIGIR信息检索研究与发展会议论文集, ACM,
    2012, 页565–574。'
- en: '[34] Y. Wang, Y. Rao, X. Zhan, H. Chen, M. Luo, J. Yin, Sentiment and emotion
    classification over noisy labels, Knowledge-Based Systems 111 (2016) 207–216.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Wang, Y. Rao, X. Zhan, H. Chen, M. Luo, J. Yin, 噪声标签上的情感与情绪分类, 知识驱动系统111
    (2016) 207–216。'
- en: '[35] Y. Aït-Sahalia, J. Fan, D. Xiu, High-frequency covariance estimates with
    noisy and asynchronous financial data, Journal of the American Statistical Association
    105 (492) (2010) 1504–1517.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Aït-Sahalia, J. Fan, D. Xiu, 使用噪声和异步金融数据的高频协方差估计, 美国统计协会杂志105 (492)
    (2010) 1504–1517。'
- en: '[36] F. Schroff, A. Criminisi, A. Zisserman, Harvesting image databases from
    the web, IEEE transactions on pattern analysis and machine intelligence 33 (4)
    (2010) 754–766.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] F. Schroff, A. Criminisi, A. Zisserman, 从网络中获取图像数据库, IEEE模式分析与机器智能杂志33
    (4) (2010) 754–766。'
- en: '[37] R. Fergus, L. Fei-Fei, P. Perona, A. Zisserman, Learning object categories
    from internet image searches, Proceedings of the IEEE 98 (8) (2010) 1453–1466.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] R. Fergus, L. Fei-Fei, P. Perona, A. Zisserman, 从互联网图像搜索中学习物体类别, IEEE学报98
    (8) (2010) 1453–1466。'
- en: '[38] X. Chen, A. Shrivastava, A. Gupta, NEIL: Extracting visual knowledge from
    web data, in: Proceedings of the IEEE International Conference on Computer Vision,
    2013, pp. 1409–1416. [doi:10.1109/ICCV.2013.178](https://doi.org/10.1109/ICCV.2013.178).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. Chen, A. Shrivastava, A. Gupta, NEIL：从网络数据中提取视觉知识, 见：IEEE国际计算机视觉会议论文集,
    2013, 页1409–1416。[doi:10.1109/ICCV.2013.178](https://doi.org/10.1109/ICCV.2013.178)。'
- en: '[39] S. K. Divvala, A. Farhadi, C. Guestrin, Learning everything about anything:
    Webly-supervised visual concept learning, in: Proceedings of the IEEE Computer
    Society Conference on Computer Vision and Pattern Recognition, 2014, pp. 3270–3277.
    [doi:10.1109/CVPR.2014.412](https://doi.org/10.1109/CVPR.2014.412).'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. K. Divvala, A. Farhadi, C. Guestrin, 学习关于任何事物的一切：Webly监督的视觉概念学习, 见：IEEE计算机视觉与模式识别会议论文集,
    2014, 页3270–3277。[doi:10.1109/CVPR.2014.412](https://doi.org/10.1109/CVPR.2014.412)。'
- en: '[40] A. Joulin, L. van der Maaten, A. Jabri, N. Vasilache, Learning visual
    features from large weakly supervised data, in: European Conference on Computer
    Vision, Springer, 2016, pp. 67–84.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Joulin, L. van der Maaten, A. Jabri, N. Vasilache, 从大量弱监督数据中学习视觉特征,
    见：欧洲计算机视觉会议, Springer, 2016, 页67–84。'
- en: '[41] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin,
    L. Fei-Fei, The unreasonable effectiveness of noisy data for fine-grained recognition,
    in: European Conference on Computer Vision, Springer, 2016, pp. 301–320.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin,
    L. Fei-Fei, 噪声数据在细粒度识别中的不合理有效性, 见：欧洲计算机视觉会议, Springer, 2016, 页301–320。'
- en: '[42] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell,
    H. Askham, X. Glorot, B. O’Donoghue, D. Visentin, et al., Clinically applicable
    deep learning for diagnosis and referral in retinal disease, Nature medicine 24 (9)
    (2018) 1342.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S.
    Blackwell, H. Askham, X. Glorot, B. O’Donoghue, D. Visentin, 等, 临床适用的深度学习用于视网膜疾病的诊断和转诊,
    自然医学24 (9) (2018) 1342。'
- en: '[43] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros, et al., Development and validation
    of a deep learning algorithm for detection of diabetic retinopathy in retinal
    fundus photographs, Jama 316 (22) (2016) 2402–2410.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
    S. Venugopalan, K. Widner, T. Madams, J. Cuadros 等，《用于检测糖尿病视网膜病变的深度学习算法的开发与验证》，《Jama》316
    (22) (2016) 2402–2410。'
- en: '[44] L. Xie, J. Wang, Z. Wei, M. Wang, Q. Tian, Disturblabel: Regularizing
    cnn on the loss layer, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2016, pp. 4753–4762.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. Xie, J. Wang, Z. Wei, M. Wang, Q. Tian，《Disturblabel：对损失层进行正则化的卷积神经网络》，发表于：IEEE计算机视觉与模式识别会议论文集，2016年，第4753–4762页。'
- en: '[45] G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, L. Qu, Making deep neural
    networks robust to label noise: A loss correction approach, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1944–1952.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, L. Qu，《使深度神经网络对标签噪声具有鲁棒性：一种损失修正方法》，发表于：IEEE计算机视觉与模式识别会议论文集，2017年，第1944–1952页。'
- en: '[46] D. Hendrycks, M. Mazeika, D. Wilson, K. Gimpel, Using trusted data to
    train deep networks on labels corrupted by severe noise, in: Advances in neural
    information processing systems, 2018, pp. 10456–10465.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] D. Hendrycks, M. Mazeika, D. Wilson, K. Gimpel，《利用可信数据训练深度网络以处理严重噪声的标签》，发表于：神经信息处理系统进展，2018年，第10456–10465页。'
- en: '[47] X. Chen, A. Gupta, Webly supervised learning of convolutional networks,
    in: Proceedings of the IEEE International Conference on Computer Vision, 2015,
    pp. 1431–1439.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] X. Chen, A. Gupta，《卷积网络的网络监督学习》，发表于：IEEE国际计算机视觉会议论文集，2015年，第1431–1439页。'
- en: '[48] A. J. Bekker, J. Goldberger, Training deep neural-networks based on unreliable
    labels, in: 2016 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), IEEE, 2016, pp. 2682–2686.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. J. Bekker, J. Goldberger，《基于不可靠标签的深度神经网络训练》，发表于：2016年IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2016年，第2682–2686页。'
- en: '[49] J. Goldberger, E. Ben-Reuven, Training deep neural-networks using a noise
    adaptation layer (2016).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Goldberger, E. Ben-Reuven，《使用噪声适应层训练深度神经网络》（2016年）。'
- en: '[50] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, R. Fergus, Training convolutional
    networks with noisy labels, in: International Conference on Learning Representations,
    2015.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, R. Fergus，《使用噪声标签训练卷积网络》，发表于：国际学习表征会议，2015年。'
- en: '[51] X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, M. Sugiyama, Are anchor
    points really indispensable in label-noise learning?, in: Advances in Neural Information
    Processing Systems, 2019, pp. 6835–6846.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, M. Sugiyama，《在标签噪声学习中，锚点是否真的不可或缺？》，发表于：神经信息处理系统进展，2019年，第6835–6846页。'
- en: '[52] J. Yao, H. Wu, Y. Zhang, I. W. Tsang, J. Sun, Safeguarded Dynamic Label
    Regression for Noisy Supervision, Proceedings of the AAAI Conference on Artificial
    Intelligence 33 (2019) 9103–9110. [arXiv:1903.02152](http://arxiv.org/abs/1903.02152),
    [doi:10.1609/aaai.v33i01.33019103](https://doi.org/10.1609/aaai.v33i01.33019103).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Yao, H. Wu, Y. Zhang, I. W. Tsang, J. Sun，《为噪声监督保护的动态标签回归》，《人工智能AAAI会议论文集》33
    (2019) 9103–9110。[arXiv:1903.02152](http://arxiv.org/abs/1903.02152)，[doi:10.1609/aaai.v33i01.33019103](https://doi.org/10.1609/aaai.v33i01.33019103)。'
- en: '[53] T. Xiao, T. Xia, Y. Yang, C. Huang, X. Wang, Learning from massive noisy
    labeled data for image classification, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2015, pp. 2691–2699.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] T. Xiao, T. Xia, Y. Yang, C. Huang, X. Wang，《从大量噪声标记数据中学习进行图像分类》，发表于：IEEE计算机视觉与模式识别会议论文集，2015年，第2691–2699页。'
- en: '[54] I. Misra, C. Lawrence Zitnick, M. Mitchell, R. Girshick, Seeing through
    the human reporting bias: Visual classifiers from noisy human-centric labels,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2016, pp. 2930–2939.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] I. Misra, C. Lawrence Zitnick, M. Mitchell, R. Girshick，《看穿人类报告偏差：来自噪声人类中心标签的视觉分类器》，发表于：IEEE计算机视觉与模式识别会议论文集，2016年，第2930–2939页。'
- en: '[55] L. Jaehwan, Y. Donggeun, K. Hyo-Eun, Photometric transformer networks
    and label adjustment for breast density prediction, in: Proceedings of the IEEE
    International Conference on Computer Vision Workshops, 2019, pp. 0–0.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Jaehwan, Y. Donggeun, K. Hyo-Eun，《用于乳腺密度预测的光度变换网络和标签调整》，发表于：IEEE国际计算机视觉会议研讨会论文集，2019年，第0–0页。'
- en: '[56] B. Yuan, J. Chen, W. Zhang, H. S. Tai, S. McMains, Iterative cross learning
    on noisy labels, in: Proceedings - 2018 IEEE Winter Conference on Applications
    of Computer Vision, WACV 2018, Vol. 2018-Janua, 2018, pp. 757–765. [doi:10.1109/WACV.2018.00088](https://doi.org/10.1109/WACV.2018.00088).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] B. Yuan, J. Chen, W. Zhang, H. S. Tai, S. McMains, 在嘈杂标签上进行迭代交叉学习，载于：2018年IEEE冬季计算机视觉应用会议论文集，WACV
    2018，第757–765页。[doi:10.1109/WACV.2018.00088](https://doi.org/10.1109/WACV.2018.00088)。'
- en: '[57] A. Vahdat, Toward robustness against label noise in training deep discriminative
    neural networks, in: Advances in Neural Information Processing Systems, 2017,
    pp. 5596–5605.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Vahdat, 朝着训练深度判别神经网络对标签噪声的鲁棒性，载于：神经信息处理系统进展，2017年，第5596–5605页。'
- en: '[58] A. Veit, N. Alldrin, G. Chechik, I. Krasin, A. Gupta, S. Belongie, Learning
    from noisy large-scale datasets with minimal supervision, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 839–847.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Veit, N. Alldrin, G. Chechik, I. Krasin, A. Gupta, S. Belongie, 从嘈杂的大规模数据集中学习与最小监督，载于：IEEE计算机视觉与模式识别会议论文集，2017年，第839–847页。'
- en: '[59] M. Dehghani, A. Mehrjou, S. Gouws, J. Kamps, B. Schölkopf, Fidelity-weighted
    learning, in: International Conference on Learning Representations, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Dehghani, A. Mehrjou, S. Gouws, J. Kamps, B. Schölkopf, 忠实度加权学习，载于：国际学习表征会议，2018年。'
- en: '[60] D. Tanaka, D. Ikami, T. Yamasaki, K. Aizawa, Joint optimization framework
    for learning with noisy labels, in: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2018, pp. 5552–5560.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] D. Tanaka, D. Ikami, T. Yamasaki, K. Aizawa, 噪声标签学习的联合优化框架，载于：IEEE计算机视觉与模式识别会议论文集，2018年，第5552–5560页。'
- en: '[61] K. Yi, J. Wu, Probabilistic end-to-end noise correction for learning with
    noisy labels, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 7017–7025.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] K. Yi, J. Wu, 用于嘈杂标签学习的概率端到端噪声校正，载于：IEEE计算机视觉与模式识别会议论文集，2019年，第7017–7025页。'
- en: '[62] X. Liu, S. Li, M. Kan, S. Shan, X. Chen, Self-error-correcting convolutional
    neural network for learning with noisy labels, in: 2017 12th IEEE International
    Conference on Automatic Face & Gesture Recognition (FG 2017), IEEE, 2017, pp.
    111–117.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. Liu, S. Li, M. Kan, S. Shan, X. Chen, 自我错误纠正卷积神经网络用于嘈杂标签学习，载于：2017年第12届IEEE国际自动人脸与姿势识别会议（FG
    2017），IEEE，2017年，第111–117页。'
- en: '[63] S. Zheng, P. Wu, A. Goswami, M. Goswami, D. Metaxas, C. Chen, Error-bounded
    correction of noisy labels, in: International Conference on Machine Learning,
    2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Zheng, P. Wu, A. Goswami, M. Goswami, D. Metaxas, C. Chen, 错误界限的噪声标签修正，载于：国际机器学习会议，2020年。'
- en: '[64] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, K. McGuinness, Unsupervised
    label noise modeling and loss correction, in: International Conference on Machine
    Learning, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, K. McGuinness, 无监督标签噪声建模与损失修正，载于：国际机器学习会议，2019年。'
- en: '[65] J. Zhang, V. S. Sheng, T. Li, X. Wu, Improving crowdsourced label quality
    using noise correction, IEEE transactions on neural networks and learning systems
    29 (5) (2017) 1675–1688.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Zhang, V. S. Sheng, T. Li, X. Wu, 使用噪声校正改进众包标签质量，IEEE神经网络与学习系统汇刊 29
    (5) (2017) 1675–1688。'
- en: '[66] J. Han, P. Luo, X. Wang, Deep self-learning from noisy labels, in: Proceedings
    of the IEEE International Conference on Computer Vision, 2019, pp. 5138–5147.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Han, P. Luo, X. Wang, 从嘈杂标签中深度自学习，载于：IEEE国际计算机视觉会议论文集，2019年，第5138–5147页。'
- en: '[67] J. Yao, J. Wang, I. W. Tsang, Y. Zhang, J. Sun, C. Zhang, R. Zhang, Deep
    learning from noisy image labels with quality embedding, IEEE Transactions on
    Image Processing 28 (4) (2018) 1909–1922.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Yao, J. Wang, I. W. Tsang, Y. Zhang, J. Sun, C. Zhang, R. Zhang, 从嘈杂图像标签中学习的深度学习与质量嵌入，IEEE图像处理汇刊
    28 (4) (2018) 1909–1922。'
- en: '[68] T. Durand, N. Mehrasa, G. Mori, Learning a deep convnet for multi-label
    classification with partial labels, in: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, 2019, pp. 647–657.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. Durand, N. Mehrasa, G. Mori, 为多标签分类学习深度卷积网络与部分标签，载于：IEEE计算机视觉与模式识别会议论文集，2019年，第647–657页。'
- en: '[69] S. J. Delany, N. Segata, B. Mac Namee, Profiling instances in noise reduction,
    Knowledge-Based Systems 31 (2012) 28–40.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. J. Delany, N. Segata, B. Mac Namee, 噪声减少中的实例分析，知识基础系统 31 (2012) 28–40。'
- en: '[70] L. P. Garcia, J. A. Sáez, J. Luengo, A. C. Lorena, A. C. de Carvalho,
    F. Herrera, Using the one-vs-one decomposition to improve the performance of class
    noise filters via an aggregation strategy in multi-class classification problems,
    Knowledge-Based Systems 90 (2015) 153–164.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. P. Garcia, J. A. Sáez, J. Luengo, A. C. Lorena, A. C. de Carvalho,
    F. Herrera, 使用一对一分解通过聚合策略改进类别噪声过滤器在多类分类问题中的性能，《知识基系统》90 (2015) 153–164。'
- en: '[71] J. Luengo, S.-O. Shim, S. Alshomrani, A. Altalhi, F. Herrera, Cnc-nos:
    Class noise cleaning by ensemble filtering and noise scoring, Knowledge-Based
    Systems 140 (2018) 27–49.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Luengo, S.-O. Shim, S. Alshomrani, A. Altalhi, F. Herrera, Cnc-nos:
    通过集成过滤和噪声评分进行类别噪声清理，《知识基系统》140 (2018) 27–49。'
- en: '[72] C. G. Northcutt, T. Wu, I. L. Chuang, Learning with confident examples:
    Rank pruning for robust classification with noisy labels, in: Uncertainty in Artificial
    Intelligence - Proceedings of the 33rd Conference, UAI 2017, 2017. [arXiv:1705.01936](http://arxiv.org/abs/1705.01936).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. G. Northcutt, T. Wu, I. L. Chuang, 使用自信示例进行学习：带噪声标签的鲁棒分类的排名剪枝，见：人工智能不确定性——第33届会议论文集，UAI
    2017，2017。 [arXiv:1705.01936](http://arxiv.org/abs/1705.01936)。'
- en: '[73] X. Wu, R. He, Z. Sun, T. Tan, A light CNN for deep face representation
    with noisy labels, IEEE Transactions on Information Forensics and Security 13 (11)
    (2018) 2884–2896. [arXiv:1511.02683](http://arxiv.org/abs/1511.02683), [doi:10.1109/TIFS.2018.2833032](https://doi.org/10.1109/TIFS.2018.2833032).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] X. Wu, R. He, Z. Sun, T. Tan, 一种用于深度人脸表示的轻量级CNN，具有噪声标签，《IEEE信息取证与安全学报》13
    (11) (2018) 2884–2896。 [arXiv:1511.02683](http://arxiv.org/abs/1511.02683)，[doi:10.1109/TIFS.2018.2833032](https://doi.org/10.1109/TIFS.2018.2833032)。'
- en: '[74] J. Huang, L. Qu, R. Jia, B. Zhao, O2u-net: A simple noisy label detection
    approach for deep neural networks, in: Proceedings of the IEEE International Conference
    on Computer Vision, 2019, pp. 3326–3334.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Huang, L. Qu, R. Jia, B. Zhao, O2u-net: 一种简单的深度神经网络噪声标签检测方法，见：IEEE国际计算机视觉会议论文集，2019，pp.
    3326–3334。'
- en: '[75] K. Sharma, P. Donmez, E. Luo, Y. Liu, I. Z. Yalniz, Noiserank: Unsupervised
    label noise reduction with dependence models, 2020.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Sharma, P. Donmez, E. Luo, Y. Liu, I. Z. Yalniz, Noiserank: 通过依赖模型进行无监督标签噪声减少，2020。'
- en: '[76] Y. Ding, L. Wang, D. Fan, B. Gong, A semi-supervised two-stage approach
    to learning from noisy labels, in: 2018 IEEE Winter Conference on Applications
    of Computer Vision (WACV), IEEE, 2018, pp. 1215–1224.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Ding, L. Wang, D. Fan, B. Gong, 一种半监督的两阶段方法来处理噪声标签，见：2018 IEEE 冬季计算机视觉应用会议（WACV），IEEE，2018，pp.
    1215–1224。'
- en: '[77] D. T. Nguyen, T.-P.-N. Ngo, Z. Lou, M. Klar, L. Beggel, T. Brox, Robust
    Learning Under Label Noise With Iterative Noise-Filtering (2019). [arXiv:1906.00216](http://arxiv.org/abs/1906.00216).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] D. T. Nguyen, T.-P.-N. Ngo, Z. Lou, M. Klar, L. Beggel, T. Brox, 在标签噪声下的稳健学习与迭代噪声过滤（2019）。
    [arXiv:1906.00216](http://arxiv.org/abs/1906.00216)。'
- en: '[78] D. T. Nguyen, C. K. Mummadi, T. P. N. Ngo, T. H. P. Nguyen, L. Beggel,
    T. Brox, SELF: Learning to Filter Noisy Labels with Self-Ensembling (oct 2019).
    [arXiv:1910.01842](http://arxiv.org/abs/1910.01842).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] D. T. Nguyen, C. K. Mummadi, T. P. N. Ngo, T. H. P. Nguyen, L. Beggel,
    T. Brox, SELF: 通过自我集成学习过滤噪声标签（2019年10月）。 [arXiv:1910.01842](http://arxiv.org/abs/1910.01842)。'
- en: '[79] J. Li, R. Socher, S. C. Hoi, Dividemix: Learning with noisy labels as
    semi-supervised learning, in: International Conference on Learning Representations,
    2020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] J. Li, R. Socher, S. C. Hoi, Dividemix: 将带噪声标签的学习视作半监督学习，见：国际学习表征会议，2020。'
- en: '[80] Y. Yan, Z. Xu, I. W. Tsang, G. Long, Y. Yang, Robust semi-supervised learning
    through label aggregation, in: 30th AAAI Conference on Artificial Intelligence,
    AAAI 2016, 2016, pp. 2244–2250.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Yan, Z. Xu, I. W. Tsang, G. Long, Y. Yang, 通过标签聚合实现稳健的半监督学习，见：第30届AAAI人工智能会议，AAAI
    2016，2016，pp. 2244–2250。'
- en: '[81] J. Jiang, J. Ma, Z. Wang, C. Chen, X. Liu, Hyperspectral image classification
    in the presence of noisy labels, IEEE Transactions on Geoscience and Remote Sensing
    57 (2) (2019) 851–865. [arXiv:1809.04212](http://arxiv.org/abs/1809.04212), [doi:10.1109/TGRS.2018.2861992](https://doi.org/10.1109/TGRS.2018.2861992).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Jiang, J. Ma, Z. Wang, C. Chen, X. Liu, 在噪声标签存在下的高光谱图像分类，《IEEE地球科学与遥感学报》57
    (2) (2019) 851–865。 [arXiv:1809.04212](http://arxiv.org/abs/1809.04212)，[doi:10.1109/TGRS.2018.2861992](https://doi.org/10.1109/TGRS.2018.2861992)。'
- en: '[82] J. Sahota, D. Shanmugam, J. Ramanan, S. Eghbali, M. Brubaker, An energy-based
    framework for arbitrary label noise correction (2018).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Sahota, D. Shanmugam, J. Ramanan, S. Eghbali, M. Brubaker, 一种用于任意标签噪声校正的基于能量的框架（2018）。'
- en: '[83] B. Han, I. W. Tsang, L. Chen, P. Y. Celina, S.-F. Fung, Progressive stochastic
    learning for noisy labels, IEEE transactions on neural networks and learning systems (99)
    (2018) 1–13.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] B. Han, I. W. Tsang, L. Chen, P. Y. Celina, S.-F. Fung, 面向噪声标签的渐进随机学习,
    IEEE神经网络与学习系统汇刊 (99) (2018) 1–13。'
- en: '[84] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, L. Fei-Fei, Mentornet: Learning
    data-driven curriculum for very deep neural networks on corrupted labels, in:
    International Conference on Machine Learning, 2018, pp. 2304–2313.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, L. Fei-Fei, Mentornet: 在污染标签上为非常深的神经网络学习数据驱动课程,
    载于：国际机器学习会议，2018年，页码2304–2313。'
- en: '[85] H.-S. Chang, E. Learned-Miller, A. McCallum, Active bias: Training more
    accurate neural networks by emphasizing high variance samples, in: Advances in
    Neural Information Processing Systems, 2017, pp. 1002–1012.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] H.-S. Chang, E. Learned-Miller, A. McCallum, 主动偏差：通过强调高方差样本训练更准确的神经网络,
    载于：神经信息处理系统进展，2017年，页码1002–1012。'
- en: '[86] Y. Lyu, I. W. Tsang, Curriculum Loss: Robust Learning and Generalization
    against Label Corruption (may 2019). [arXiv:1905.10045](http://arxiv.org/abs/1905.10045).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Lyu, I. W. Tsang, Curriculum Loss: 针对标签污染的鲁棒学习与泛化（2019年5月）。[arXiv:1905.10045](http://arxiv.org/abs/1905.10045)。'
- en: '[87] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang,
    Curriculumnet: Weakly supervised learning from large-scale web images, in: Proceedings
    of the European Conference on Computer Vision (ECCV), 2018, pp. 135–150.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang,
    Curriculumnet: 从大规模网络图像中进行弱监督学习, 载于：欧洲计算机视觉会议（ECCV）论文集，2018年，页码135–150。'
- en: '[88] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, A. Rabinovich, Training
    deep neural networks on noisy labels with bootstrapping, arXiv preprint arXiv:1412.6596
    (2014).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, A. Rabinovich, 在噪声标签下使用自助法训练深度神经网络,
    arXiv预印本 arXiv:1412.6596 (2014)。'
- en: '[89] E. Malach, S. Shalev-Shwartz, Decoupling” when to update” from” how to
    update”, in: Advances in Neural Information Processing Systems, 2017, pp. 960–970.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] E. Malach, S. Shalev-Shwartz, “解耦”何时更新与“如何更新”, 载于：神经信息处理系统进展，2017年，页码960–970。'
- en: '[90] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama, Co-teaching:
    Robust training of deep neural networks with extremely noisy labels, in: Advances
    in Neural Information Processing Systems, 2018, pp. 8527–8537.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama, Co-teaching:
    在极度噪声标签下的深度神经网络的鲁棒训练, 载于：神经信息处理系统进展，2018年，页码8527–8537。'
- en: '[91] X. Yu, B. Han, J. Yao, G. Niu, I. W. Tsang, M. Sugiyama, How does Disagreement
    Help Generalization against Label Corruption?, in: International Conference on
    Machine Learning, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] X. Yu, B. Han, J. Yao, G. Niu, I. W. Tsang, M. Sugiyama, 不一致性如何帮助对抗标签污染的泛化？,
    载于：国际机器学习会议，2019年。'
- en: '[92] X. Wang, S. Wang, J. Wang, H. Shi, T. Mei, Co-mining: Deep face recognition
    with noisy labels, in: Proceedings of the IEEE International Conference on Computer
    Vision, 2019, pp. 9358–9367.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] X. Wang, S. Wang, J. Wang, H. Shi, T. Mei, Co-mining: 深度人脸识别中的噪声标签, 载于：IEEE国际计算机视觉会议论文集，2019年，页码9358–9367。'
- en: '[93] P. Chen, B. B. Liao, G. Chen, S. Zhang, Understanding and utilizing deep
    neural networks trained with noisy labels, in: International Conference on Machine
    Learning, 2019, pp. 1062–1070.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] P. Chen, B. B. Liao, G. Chen, S. Zhang, 理解和利用训练有噪声标签的深度神经网络, 载于：国际机器学习会议，2019年，页码1062–1070。'
- en: '[94] M. Ren, W. Zeng, B. Yang, R. Urtasun, Learning to reweight examples for
    robust deep learning, International Conference on Machine Learning (2018).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Ren, W. Zeng, B. Yang, R. Urtasun, 学习为鲁棒深度学习重新加权示例, 国际机器学习会议（2018年）。'
- en: '[95] S. Jenni, P. Favaro, Deep bilevel learning, in: Proceedings of the European
    Conference on Computer Vision (ECCV), 2018, pp. 618–633.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Jenni, P. Favaro, 深度双层学习, 载于：欧洲计算机视觉会议（ECCV）论文集，2018年，页码618–633。'
- en: '[96] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, D. Meng, Meta-weight-net:
    Learning an explicit mapping for sample weighting, in: Advances in Neural Information
    Processing Systems, 2019, pp. 1917–1928.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, D. Meng, Meta-weight-net:
    学习样本加权的显式映射, 载于：神经信息处理系统进展，2019年，页码1917–1928。'
- en: '[97] Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, S.-T. Xia, Iterative
    learning with open-set noisy labels, in: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, 2018, pp. 8688–8696.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, S.-T. Xia, 在开放集噪声标签下的迭代学习,
    载于：IEEE计算机视觉与模式识别会议论文集，2018年，页码8688–8696。'
- en: '[98] S. Thulasidasan, T. Bhattacharya, J. Bilmes, G. Chennupati, J. Mohd-Yusof,
    Combating Label Noise in Deep Learning Using Abstention, in: International Conference
    on Machine Learning, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] S. Thulasidasan, T. Bhattacharya, J. Bilmes, G. Chennupati, J. Mohd-Yusof,
    使用弃权来对抗深度学习中的标签噪声，见：国际机器学习会议，2019年。'
- en: '[99] T. Liu, D. Tao, Classification with noisy labels by importance reweighting,
    IEEE Transactions on pattern analysis and machine intelligence 38 (3) (2015) 447–461.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. Liu, D. Tao, 通过重要性重新加权处理噪声标签的分类，IEEE模式分析与机器智能汇刊 38 (3) (2015) 447–461。'
- en: '[100] R. Wang, T. Liu, D. Tao, Multiclass learning with partially corrupted
    labels, IEEE transactions on neural networks and learning systems 29 (6) (2018)
    2568–2580.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] R. Wang, T. Liu, D. Tao, 带有部分污染标签的多类学习，IEEE神经网络与学习系统汇刊 29 (6) (2018)
    2568–2580。'
- en: '[101] K.-H. Lee, X. He, L. Zhang, L. Yang, Cleannet: Transfer learning for
    scalable image classifier training with label noise, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2018, pp. 5447–5456.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] K.-H. Lee, X. He, L. Zhang, L. Yang, Cleannet：用于可扩展图像分类器训练的迁移学习与标签噪声，见：IEEE计算机视觉与模式识别会议论文集，2018年，第5447–5456页。'
- en: '[102] O. Litany, D. Freedman, Soseleto: A unified approach to transfer learning
    and training with noisy labels, in: International Conference on Learning Representations
    workshop on Learning from Limited Labeled Data, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] O. Litany, D. Freedman, Soseleto：一种统一的迁移学习和带噪声标签训练的方法，见：有限标记数据学习国际会议研讨会，2018年。'
- en: '[103] W. Hu, Y. Huang, F. Zhang, R. Li, Noise-tolerant paradigm for training
    face recognition cnns, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2019, pp. 11887–11896.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] W. Hu, Y. Huang, F. Zhang, R. Li, 面部识别CNN的噪声容忍训练范式，见：IEEE计算机视觉与模式识别会议论文集，2019年，第11887–11896页。'
- en: '[104] V. C. Raykar, S. Yu, L. H. Zhao, A. Jerebko, C. Florin, G. H. Valadez,
    L. Bogoni, L. Moy, Supervised learning from multiple experts: whom to trust when
    everyone lies a bit, in: Proceedings of the 26th Annual international conference
    on machine learning, ACM, 2009, pp. 889–896.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] V. C. Raykar, S. Yu, L. H. Zhao, A. Jerebko, C. Florin, G. H. Valadez,
    L. Bogoni, L. Moy, 来自多个专家的监督学习：当每个人都有点撒谎时，应该相信谁，见：第26届国际机器学习年会论文集，ACM，2009年，第889–896页。'
- en: '[105] Y. Yan, R. Rosales, G. Fung, R. Subramanian, J. Dy, Learning from multiple
    annotators with varying expertise, Machine Learning 95 (3) (2014) 291–327. [doi:10.1007/s10994-013-5412-1](https://doi.org/10.1007/s10994-013-5412-1).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Yan, R. Rosales, G. Fung, R. Subramanian, J. Dy, 从多位具有不同专业的标注者处学习，《机器学习》95
    (3) (2014) 291–327。[doi:10.1007/s10994-013-5412-1](https://doi.org/10.1007/s10994-013-5412-1)。'
- en: '[106] R. Tanno, A. Saeedi, S. Sankaranarayanan, D. C. Alexander, N. Silberman,
    Learning from noisy labels by regularized estimation of annotator confusion, in:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2019, pp. 11244–11253.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] R. Tanno, A. Saeedi, S. Sankaranarayanan, D. C. Alexander, N. Silberman,
    通过正则化标注者混淆的估计从噪声标签中学习，见：IEEE计算机视觉与模式识别会议论文集，2019年，第11244–11253页。'
- en: '[107] F. Rodrigues, F. C. Pereira, Deep learning from crowds, in: 32nd AAAI
    Conference on Artificial Intelligence, AAAI 2018, 2018, pp. 1611–1618.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] F. Rodrigues, F. C. Pereira, 从众包中进行深度学习，见：第32届AAAI人工智能会议，AAAI 2018，2018年，第1611–1618页。'
- en: '[108] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, P. L. Ruvolo, Whose
    vote should count more: Optimal integration of labels from labelers of unknown
    expertise, in: Advances in neural information processing systems, 2009, pp. 2035–2043.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, P. L. Ruvolo, 谁的投票应该更重要：来自未知专业标签者的标签的最佳整合，见：神经信息处理系统进展，2009年，第2035–2043页。'
- en: '[109] S. Branson, G. Van Horn, P. Perona, Lean crowdsourcing: Combining humans
    and machines in an online system, in: Proceedings - 30th IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2017, Vol. 2017-Janua, 2017, pp. 6109–6118.
    [doi:10.1109/CVPR.2017.647](https://doi.org/10.1109/CVPR.2017.647).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Branson, G. Van Horn, P. Perona, 精简众包：在在线系统中结合人类与机器，见：第30届IEEE计算机视觉与模式识别会议论文集，CVPR
    2017，2017年，第6109–6118页。[doi:10.1109/CVPR.2017.647](https://doi.org/10.1109/CVPR.2017.647)。'
- en: '[110] H. Izadinia, B. C. Russell, A. Farhadi, M. D. Hoffman, A. Hertzmann,
    Deep classifiers from image tags in the wild, in: Proceedings of the 2015 Workshop
    on Community-Organized Multimodal Mining: Opportunities for Novel Solutions, ACM,
    2015, pp. 13–18.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. Izadinia, B. C. Russell, A. Farhadi, M. D. Hoffman, A. Hertzmann,
    从现实世界的图像标签中进行深度分类，见：2015年社区组织的多模态挖掘研讨会：新解决方案的机会，ACM，2015年，第13–18页。'
- en: '[111] N. Manwani, P. Sastry, Noise tolerance under risk minimization, IEEE
    transactions on cybernetics 43 (3) (2013) 1146–1151.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] N. Manwani, P. Sastry, 风险最小化下的噪声容忍，IEEE网络系统学报 43 (3) (2013) 1146–1151。'
- en: '[112] A. Ghosh, N. Manwani, P. Sastry, Making risk minimization tolerant to
    label noise, Neurocomputing 160 (2015) 93–107.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. Ghosh, N. Manwani, P. Sastry, 使风险最小化对标签噪声具有容忍性，Neurocomputing 160
    (2015) 93–107。'
- en: '[113] N. Charoenphakdee, J. Lee, M. Sugiyama, On symmetric losses for learning
    from corrupted labels, in: International Conference on Machine Learning, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] N. Charoenphakdee, J. Lee, M. Sugiyama, 关于从损坏标签中学习的对称损失，载于：国际机器学习会议，2019年。'
- en: '[114] P. L. Bartlett, M. I. Jordan, J. D. McAuliffe, Convexity, classification,
    and risk bounds, Journal of the American Statistical Association 101 (473) (2006)
    138–156.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. L. Bartlett, M. I. Jordan, J. D. McAuliffe, 凸性、分类及风险界限，《美国统计学会期刊》101
    (473) (2006) 138–156。'
- en: '[115] A. Ghosh, H. Kumar, P. Sastry, Robust loss functions under label noise
    for deep neural networks, in: Thirty-First AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Ghosh, H. Kumar, P. Sastry, 在标签噪声下深度神经网络的鲁棒损失函数，载于：第31届AAAI人工智能大会，2017年。'
- en: '[116] X. Wang, E. Kodirov, Y. Hua, N. M. Robertson, Improved Mean Absolute
    Error for Learning Meaningful Patterns from Abnormal Training Data, Tech. rep.
    (2019). [arXiv:1903.12141v5](http://arxiv.org/abs/1903.12141v5).'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] X. Wang, E. Kodirov, Y. Hua, N. M. Robertson, 改进的平均绝对误差，用于从异常训练数据中学习有意义的模式，技术报告（2019）。[arXiv:1903.12141v5](http://arxiv.org/abs/1903.12141v5)。'
- en: '[117] Z. Zhang, M. Sabuncu, Generalized cross entropy loss for training deep
    neural networks with noisy labels, in: Advances in neural information processing
    systems, 2018, pp. 8778–8788.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Z. Zhang, M. Sabuncu, 用于训练带噪声标签的深度神经网络的广义交叉熵损失，载于：神经信息处理系统进展，2018年，第8778–8788页。'
- en: '[118] Y. Wang, X. Ma, Z. Chen, Y. Luo, J. Yi, J. Bailey, Symmetric cross entropy
    for robust learning with noisy labels, in: Proceedings of the IEEE International
    Conference on Computer Vision, 2019, pp. 322–330.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Y. Wang, X. Ma, Z. Chen, Y. Luo, J. Yi, J. Bailey, 用于鲁棒学习的对称交叉熵，载于：IEEE国际计算机视觉会议论文集，2019年，第322–330页。'
- en: '[119] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, A. Tewari, Learning with
    noisy labels, in: Advances in neural information processing systems, 2013, pp.
    1196–1204.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, A. Tewari, 带噪声标签的学习，载于：神经信息处理系统进展，2013年，第1196–1204页。'
- en: '[120] V. Mnih, G. E. Hinton, Learning to label aerial images from noisy data,
    in: Proceedings of the 29th International conference on machine learning (ICML-12),
    2012, pp. 567–574.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] V. Mnih, G. E. Hinton, 从噪声数据中学习标记航空图像，载于：第29届国际机器学习会议（ICML-12）论文集，2012年，第567–574页。'
- en: '[121] Y. Xu, P. Cao, Y. Kong, Y. Wang, L_dmi: A novel information-theoretic
    loss function for training deep nets robust to label noise, in: Advances in Neural
    Information Processing Systems, 2019, pp. 6222–6233.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Xu, P. Cao, Y. Kong, Y. Wang, L_dmi：一种新颖的信息论损失函数，用于训练对标签噪声鲁棒的深度网络，载于：神经信息处理系统进展，2019年，第6222–6233页。'
- en: '[122] G. Patrini, F. Nielsen, R. Nock, M. Carioni, Loss factorization, weakly
    supervised learning and label noise robustness, in: International conference on
    machine learning, 2016, pp. 708–717.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] G. Patrini, F. Nielsen, R. Nock, M. Carioni, 损失因子分解、弱监督学习及标签噪声鲁棒性，载于：国际机器学习会议，2016年，第708–717页。'
- en: '[123] B. Van Rooyen, A. Menon, R. C. Williamson, Learning with symmetric label
    noise: The importance of being unhinged, in: Advances in Neural Information Processing
    Systems, 2015, pp. 10–18.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] B. Van Rooyen, A. Menon, R. C. Williamson, 带对称标签噪声的学习：保持冷静的重要性，载于：神经信息处理系统进展，2015年，第10–18页。'
- en: '[124] B. Han, I. W. Tsang, L. Chen, On the convergence of a family of robust
    losses for stochastic gradient descent, in: Joint European conference on machine
    learning and knowledge discovery in databases, Springer, 2016, pp. 665–680.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] B. Han, I. W. Tsang, L. Chen, 一类鲁棒损失函数在随机梯度下降中的收敛性，载于：欧洲机器学习与数据库知识发现联合会议，Springer，2016年，第665–680页。'
- en: '[125] L. P. Garcia, A. C. de Carvalho, A. C. Lorena, Noise detection in the
    meta-learning level, Neurocomputing 176 (2016) 14–25.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. P. Garcia, A. C. de Carvalho, A. C. Lorena, 元学习层级中的噪声检测，Neurocomputing
    176 (2016) 14–25。'
- en: '[126] B. Han, G. Niu, J. Yao, X. Yu, M. Xu, I. Tsang, M. Sugiyama, Pumpout:
    A meta approach for robustly training deep neural networks with noisy labels,
    arXiv preprint arXiv:1809.11008 (2018).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] B. Han, G. Niu, J. Yao, X. Yu, M. Xu, I. Tsang, M. Sugiyama, Pumpout：一种针对噪声标签鲁棒训练深度神经网络的元方法，arXiv预印本
    arXiv:1809.11008 (2018)。'
- en: '[127] J. Li, Y. Wong, Q. Zhao, M. S. Kankanhalli, Learning to learn from noisy
    labeled data, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 5051–5059.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Li, Y. Wong, Q. Zhao, M. S. Kankanhalli, 从噪声标记数据中学习，IEEE 计算机视觉与模式识别大会论文集，2019，页
    5051–5059。'
- en: '[128] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, L.-J. Li, Learning from noisy
    labels with distillation, in: Proceedings of the IEEE International Conference
    on Computer Vision, 2017, pp. 1910–1918.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, L.-J. Li, 通过蒸馏从噪声标签中学习，《IEEE
    国际计算机视觉会议论文集》，2017，页 1910–1918。'
- en: '[129] N. Kato, T. Li, K. Nishino, Y. Uchida, Improving Multi-Person Pose Estimation
    using Label Correction (nov 2018). [arXiv:1811.03331](http://arxiv.org/abs/1811.03331).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] N. Kato, T. Li, K. Nishino, Y. Uchida, 使用标签修正改进多人体姿态估计 (2018年11月)。 [arXiv:1811.03331](http://arxiv.org/abs/1811.03331)。'
- en: '[130] M. Dehghani, A. Severyn, S. Rothe, J. Kamps, Learning to Learn from Weak
    Supervision by Full Supervision, arxiv.org (2017). [arXiv:1711.11383](http://arxiv.org/abs/1711.11383).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Dehghani, A. Severyn, S. Rothe, J. Kamps, 通过完全监督学习从弱监督中学习，arxiv.org
    (2017)。 [arXiv:1711.11383](http://arxiv.org/abs/1711.11383)。'
- en: '[131] M. Dehghani, A. Severyn, S. Rothe, J. Kamps, Avoiding your teacher’s
    mistakes: Training neural networks with controlled weak supervision, arXiv preprint
    arXiv:1711.00313 (2017).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Dehghani, A. Severyn, S. Rothe, J. Kamps, 避免老师的错误：使用受控弱监督训练神经网络，arXiv
    预印本 arXiv:1711.00313 (2017)。'
- en: '[132] G. Algan, I. Ulusoy, Meta soft label generation for noisy labels, in:
    International Conferance on Pattern Recognition, ICPR, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] G. Algan, I. Ulusoy, 噪声标签的元软标签生成，国际模式识别大会 ICPR，2020。'
- en: '[133] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
    Dropout: a simple way to prevent neural networks from overfitting, The journal
    of machine learning research 15 (1) (2014) 1929–1958.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
    Dropout：防止神经网络过拟合的简单方法，《机器学习研究杂志》15 (1) (2014) 1929–1958。'
- en: '[134] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial
    examples, arXiv preprint arXiv:1412.6572 (2014).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] I. J. Goodfellow, J. Shlens, C. Szegedy, 解释和利用对抗样本，arXiv 预印本 arXiv:1412.6572
    (2014)。'
- en: '[135] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond Empirical
    Risk Minimization (oct 2017). [arXiv:1710.09412](http://arxiv.org/abs/1710.09412).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup：超越经验风险最小化 (2017年10月)。
    [arXiv:1710.09412](http://arxiv.org/abs/1710.09412)。'
- en: '[136] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, G. Hinton, Regularizing
    neural networks by penalizing confident output distributions, arXiv preprint arXiv:1701.06548
    (2017).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, G. Hinton, 通过惩罚自信输出分布来正则化神经网络，arXiv
    预印本 arXiv:1701.06548 (2017)。'
- en: '[137] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the
    inception architecture for computer vision, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2016, pp. 2818–2826.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, 重新思考计算机视觉的 inception
    架构，IEEE 计算机视觉与模式识别会议论文集，2016，页 2818–2826。'
- en: '[138] D. Hendrycks, K. Lee, M. Mazeika, Using Pre-Training Can Improve Model
    Robustness and Uncertainty (jan 2019). [arXiv:1901.09960](http://arxiv.org/abs/1901.09960).'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] D. Hendrycks, K. Lee, M. Mazeika, 使用预训练可以提高模型的鲁棒性和不确定性 (2019年1月)。 [arXiv:1901.09960](http://arxiv.org/abs/1901.09960)。'
- en: '[139] I. Jindal, M. Nokleby, X. Chen, Learning deep networks from noisy labels
    with dropout regularization, in: 2016 IEEE 16th International Conference on Data
    Mining (ICDM), IEEE, 2016, pp. 967–972.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] I. Jindal, M. Nokleby, X. Chen, 使用 dropout 正则化从噪声标签中学习深度网络，2016 IEEE
    第16届数据挖掘国际会议 (ICDM)，IEEE，2016，页 967–972。'
- en: '[140] X. Ma, Y. Wang, M. E. Houle, S. Zhou, S. M. Erfani, S.-T. Xia, S. Wijewickrema,
    J. Bailey, Dimensionality-driven learning with noisy labels, in: International
    Conference on Learning Representations, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. Ma, Y. Wang, M. E. Houle, S. Zhou, S. M. Erfani, S.-T. Xia, S. Wijewickrema,
    J. Bailey, 基于维度驱动的噪声标签学习，国际学习表示会议，2018。'
- en: '[141] S. Azadi, J. Feng, S. Jegelka, T. Darrell, Auxiliary image regularization
    for deep cnns with noisy labels, in: International Conference on Learning Representations,
    2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] S. Azadi, J. Feng, S. Jegelka, T. Darrell, 具有噪声标签的深度 CNN 的辅助图像正则化，国际学习表示会议，2016。'
- en: '[142] X. Sun, H. Zhou, An empirical comparison of two boosting algorithms on
    real data sets with artificial class noise, in: Communications in Computer and
    Information Science, Vol. 201 CCIS, 2011, pp. 23–30. [doi:10.1007/978-3-642-22418-8_4](https://doi.org/10.1007/978-3-642-22418-8_4).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Sun, H. Zhou, 两种提升算法在人工类别噪声的真实数据集上的实证比较，见：计算机与信息科学通信，第201 CCIS卷，2011，第23–30页。
    [doi:10.1007/978-3-642-22418-8_4](https://doi.org/10.1007/978-3-642-22418-8_4)。'
- en: '[143] J. Cao, S. Kwong, R. Wang, A noise-detection based adaboost algorithm
    for mislabeled data, Pattern Recognition 45 (12) (2012) 4451–4465.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Cao, S. Kwong, R. Wang, 基于噪声检测的AdaBoost算法用于错误标记数据，《模式识别》45 (12) (2012)
    4451–4465。'
- en: '[144] J. Bootkrajang, A. Kabán, Boosting in the presence of label noise, in:
    Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI
    2013, 2013, pp. 82–91. [arXiv:1309.6818](http://arxiv.org/abs/1309.6818).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Bootkrajang, A. Kabán, 在标签噪声存在下的提升，见：人工智能不确定性 - 第29届会议论文集，UAI 2013，2013，第82–91页。
    [arXiv:1309.6818](http://arxiv.org/abs/1309.6818)。'
- en: '[145] Q. Miao, Y. Cao, G. Xia, M. Gong, J. Liu, J. Song, Rboost: label noise-robust
    boosting algorithm based on a nonconvex loss function and the numerically stable
    base learners, IEEE transactions on neural networks and learning systems 27 (11)
    (2015) 2216–2228.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Q. Miao, Y. Cao, G. Xia, M. Gong, J. Liu, J. Song, Rboost：基于非凸损失函数和数值稳定基学习器的标签噪声鲁棒提升算法，《IEEE神经网络与学习系统交易》27
    (11) (2015) 2216–2228。'
- en: '[146] B. Sun, S. Chen, J. Wang, H. Chen, A robust multi-class adaboost algorithm
    for mislabeled noisy data, Knowledge-Based Systems 102 (2016) 87–102.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] B. Sun, S. Chen, J. Wang, H. Chen, 一种针对错误标记噪声数据的鲁棒多类AdaBoost算法，《知识基础系统》102
    (2016) 87–102。'
- en: '[147] X. Yu, T. Liu, M. Gong, D. Tao, Learning with biased complementary labels,
    in: Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp.
    68–83.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Yu, T. Liu, M. Gong, D. Tao, 通过有偏互补标签进行学习，见：欧洲计算机视觉会议（ECCV）论文集，2018，第68–83页。'
- en: '[148] Y. Kim, J. Yim, J. Yun, J. Kim, Nlnl: Negative learning for noisy labels,
    in: Proceedings of the IEEE International Conference on Computer Vision, 2019,
    pp. 101–110.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Kim, J. Yim, J. Yun, J. Kim, Nlnl：用于噪声标签的负学习，见：IEEE国际计算机视觉会议论文集，2019，第101–110页。'
- en: '[149] Y. Xia, X. Cao, F. Wen, G. Hua, J. Sun, Learning discriminative reconstructions
    for unsupervised outlier removal, in: Proceedings of the IEEE International Conference
    on Computer Vision, Vol. 2015 Inter, 2015, pp. 1511–1519. [doi:10.1109/ICCV.2015.177](https://doi.org/10.1109/ICCV.2015.177).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Xia, X. Cao, F. Wen, G. Hua, J. Sun, 学习用于无监督异常值去除的区分重建，见：IEEE国际计算机视觉会议论文集，第2015届，2015，第1511–1519页。
    [doi:10.1109/ICCV.2015.177](https://doi.org/10.1109/ICCV.2015.177)。'
- en: '[150] K. Lee, S. Yun, K. Lee, H. Lee, B. Li, J. Shin, Robust determinantal
    generative classifier for noisy labels and adversarial attacks (2018).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. Lee, S. Yun, K. Lee, H. Lee, B. Li, J. Shin, 用于噪声标签和对抗攻击的鲁棒确定性生成分类器（2018）。'
- en: '[151] Y. Duan, O. Wu, Learning with Auxiliary Less-Noisy Labels, IEEE Transactions
    on Neural Networks and Learning Systems 28 (7) (2017) 1716–1721. [doi:10.1109/TNNLS.2016.2546956](https://doi.org/10.1109/TNNLS.2016.2546956).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Y. Duan, O. Wu, 使用辅助少噪声标签进行学习，《IEEE神经网络与学习系统交易》28 (7) (2017) 1716–1721。
    [doi:10.1109/TNNLS.2016.2546956](https://doi.org/10.1109/TNNLS.2016.2546956)。'
- en: '[152] S. Choi, S. Hong, S. Lim, ChoiceNet: Robust Learning by Revealing Output
    Correlations (may 2018). [arXiv:1805.06431](http://arxiv.org/abs/1805.06431).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] S. Choi, S. Hong, S. Lim, ChoiceNet：通过揭示输出相关性进行鲁棒学习（2018年5月）。 [arXiv:1805.06431](http://arxiv.org/abs/1805.06431)。'
- en: '[153] W. Zhang, Y. Wang, Y. Qiao, Metacleaner: Learning to hallucinate clean
    representations for noisy-labeled visual recognition, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 7373–7382.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] W. Zhang, Y. Wang, Y. Qiao, Metacleaner：学习为带噪标签的视觉识别生成干净表示，见：IEEE计算机视觉与模式识别会议论文集，2019，第7373–7382页。'
- en: '[154] P. H. Seo, G. Kim, B. Han, Combinatorial inference against label noise,
    in: Advances in Neural Information Processing Systems, 2019, pp. 1171–1181.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. H. Seo, G. Kim, B. Han, 针对标签噪声的组合推断，见：神经信息处理系统进展，2019，第1171–1181页。'
- en: '[155] L. Niu, W. Li, D. Xu, Visual recognition by learning from web data: A
    weakly supervised domain generalization approach, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2015, pp. 2774–2783.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] L. Niu, W. Li, D. Xu, 通过从网络数据中学习进行视觉识别：一种弱监督领域泛化方法，见：IEEE计算机视觉与模式识别会议论文集，2015，第2774–2783页。'
- en: '[156] B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid, Attend in groups: a weakly-supervised
    deep learning framework for learning from web data, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, 2017, pp. 1878–1887.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] B. Zhuang, L. Liu, Y. Li, C. Shen, I. Reid, 分组关注：一种弱监督深度学习框架，用于从网页数据中学习，载于《IEEE计算机视觉与模式识别会议论文集》，2017年，页码1878–1887。'
- en: '[157] B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, M. Sugiyama, Masking:
    A new perspective of noisy supervision, in: Advances in Neural Information Processing
    Systems, 2018, pp. 5836–5846.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, M. Sugiyama, Masking：一种新的噪声监督视角，载于《神经信息处理系统进展》，2018年，页码5836–5846。'
- en: '[158] A. P. Dawid, A. M. Skene, Maximum likelihood estimation of observer error-rates
    using the em algorithm, Journal of the Royal Statistical Society: Series C (Applied
    Statistics) 28 (1) (1979) 20–28.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. P. Dawid, A. M. Skene, 使用EM算法的观察者错误率最大似然估计，《皇家统计学会期刊：C系列（应用统计）》28（1）（1979年）20–28。'
- en: '[159] J.-X. Zhong, N. Li, W. Kong, S. Liu, T. H. Li, G. Li, Graph convolutional
    label noise cleaner: Train a plug-and-play action classifier for anomaly detection,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2019, pp. 1237–1246.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] J.-X. Zhong, N. Li, W. Kong, S. Liu, T. H. Li, G. Li, 图卷积标签噪声清理器：训练一个即插即用的动作分类器用于异常检测，载于《IEEE计算机视觉与模式识别会议论文集》，2019年，页码1237–1246。'
- en: '[160] C. Li, V. S. Sheng, L. Jiang, H. Li, Noise filtering to improve data
    and model quality for crowdsourcing, Knowledge-Based Systems 107 (2016) 96–103.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] C. Li, V. S. Sheng, L. Jiang, H. Li, 噪声过滤以提高众包数据和模型质量，《知识基础系统》107（2016年）96–103。'
- en: '[161] Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning,
    in: Proceedings of the 26th annual international conference on machine learning,
    ACM, 2009, pp. 41–48.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Y. Bengio, J. Louradour, R. Collobert, J. Weston, 课程学习，载于《第26届国际机器学习年会论文集》，ACM，2009年，页码41–48。'
- en: '[162] M. P. Kumar, B. Packer, D. Koller, Self-paced learning for latent variable
    models, in: Advances in Neural Information Processing Systems, 2010, pp. 1189–1197.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. P. Kumar, B. Packer, D. Koller, 潜变量模型的自适应学习，载于《神经信息处理系统进展》，2010年，页码1189–1197。'
- en: '[163] L. Jiang, D. Meng, S.-I. Yu, Z. Lan, S. Shan, A. Hauptmann, Self-paced
    learning with diversity, in: Advances in Neural Information Processing Systems,
    2014, pp. 2078–2086.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] L. Jiang, D. Meng, S.-I. Yu, Z. Lan, S. Shan, A. Hauptmann, 自适应学习与多样性，载于《神经信息处理系统进展》，2014年，页码2078–2086。'
- en: '[164] H.-P. Kriegel, P. Kröger, E. Schubert, A. Zimek, Loop: local outlier
    probabilities, in: Proceedings of the 18th ACM conference on Information and knowledge
    management, ACM, 2009, pp. 1649–1652.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] H.-P. Kriegel, P. Kröger, E. Schubert, A. Zimek, Loop：局部异常值概率，载于《第18届ACM信息与知识管理大会论文集》，ACM，2009年，页码1649–1652。'
- en: '[165] J. Vuurens, A. P. de Vries, C. Eickhoff, How much spam can you take?
    an analysis of crowdsourcing results to increase accuracy, in: Proc. ACM SIGIR
    Workshop on Crowdsourcing for Information Retrieval (CIR’11), 2011, pp. 21–26.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] J. Vuurens, A. P. de Vries, C. Eickhoff, 你能处理多少垃圾邮件？通过分析众包结果来提高准确性，载于《ACM
    SIGIR关于信息检索的众包研讨会论文集（CIR’11）》，2011年，页码21–26。'
- en: '[166] P. Wais, S. Lingamneni, D. Cook, J. Fennell, B. Goldenberg, D. Lubarov,
    D. Marin, H. Simons, Towards building a high-quality workforce with mechanical
    turk, Proceedings of computational social science and the wisdom of crowds (NIPS)
    (2010) 1–5.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] P. Wais, S. Lingamneni, D. Cook, J. Fennell, B. Goldenberg, D. Lubarov,
    D. Marin, H. Simons, 迈向建立高质量劳动力的目标：机械土耳其工人，载于计算社会科学与群体智慧的会议论文集（NIPS）（2010年）1–5。'
- en: '[167] P. G. Ipeirotis, F. Provost, J. Wang, Quality management on amazon mechanical
    turk, in: Proceedings of the ACM SIGKDD workshop on human computation, ACM, 2010,
    pp. 64–67.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] P. G. Ipeirotis, F. Provost, J. Wang, 亚马逊机械土耳其工人的质量管理，载于《ACM SIGKDD人类计算研讨会论文集》，ACM，2010年，页码64–67。'
- en: '[168] Y. Kong, Dominantly truthful multi-task peer prediction with a constant
    number of tasks, in: Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
    Discrete Algorithms, SIAM, 2020, pp. 2398–2411.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Kong, 具有固定任务数量的主导真实多任务同行预测，载于《第十四届ACM-SIAM离散算法年会论文集》，SIAM，2020年，页码2398–2411。'
- en: '[169] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul,
    B. Shillingford, N. De Freitas, Learning to learn by gradient descent by gradient
    descent, in: Advances in Neural Information Processing Systems, 2016, pp. 3981–3989.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul,
    B. Shillingford, N. De Freitas, 通过梯度下降学习学习，载于《神经信息处理系统进展》，2016年，页码3981–3989。'
- en: '[170] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast
    adaptation of deep networks, in: Proceedings of the 34th International Conference
    on Machine Learning-Volume 70, JMLR. org, 2017, pp. 1126–1135.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C. Finn, P. Abbeel, S. Levine, 模型无关的元学习用于深度网络的快速适应, 见：第 34 届国际机器学习会议论文集-第
    70 卷, JMLR.org, 2017, 页 1126–1135。'
- en: '[171] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, T. Raiko, Semi-supervised
    learning with ladder networks, in: Advances in neural information processing systems,
    2015, pp. 3546–3554.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, T. Raiko, 梯形网络的半监督学习,
    见：神经信息处理系统进展, 2015, 页 3546–3554。'
- en: '[172] A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results, in: Advances
    in neural information processing systems, 2017, pp. 1195–1204.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Tarvainen, H. Valpola, 平均教师是更好的榜样：加权平均一致性目标改进半监督深度学习结果, 见：神经信息处理系统进展,
    2017, 页 1195–1204。'
- en: '[173] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural
    network, arXiv preprint arXiv:1503.02531 (2015).'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] G. Hinton, O. Vinyals, J. Dean, 从神经网络中提取知识, arXiv 预印本 arXiv:1503.02531
    (2015)。'
- en: '[174] M. E. Houle, Dimensionality, discriminability, density and distance distributions,
    in: 2013 IEEE 13th International Conference on Data Mining Workshops, IEEE, 2013,
    pp. 468–473.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. E. Houle, 维度、可分辨性、密度与距离分布, 见：2013 IEEE 第 13 届数据挖掘国际会议研讨会, IEEE, 2013,
    页 468–473。'
- en: '[175] T. G. Dietterich, An experimental comparison of three methods for constructing
    ensembles of decision trees: Bagging, boosting, and randomization, Machine learning
    40 (2) (2000) 139–157.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] T. G. Dietterich, 三种决策树集成构建方法的实验比较：袋装、提升和随机化, 机器学习 40 (2) (2000) 139–157。'
- en: '[176] Y. LeCun, The mnist database of handwritten digits, http://yann. lecun.
    com/exdb/mnist/ (1998).'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Y. LeCun, 手写数字的 MNIST 数据库, http://yann.lecun.com/exdb/mnist/ (1998)。'
- en: '[177] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist: a novel image dataset
    for benchmarking machine learning algorithms, arXiv preprint arXiv:1708.07747
    (2017).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist: 用于基准测试机器学习算法的新颖图像数据集,
    arXiv 预印本 arXiv:1708.07747 (2017)。'
- en: '[178] A. Torralba, R. Fergus, W. T. Freeman, 80 million tiny images: A large
    data set for nonparametric object and scene recognition, IEEE transactions on
    pattern analysis and machine intelligence 30 (11) (2008) 1958–1970.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] A. Torralba, R. Fergus, W. T. Freeman, 8000 万张小图像：用于非参数对象和场景识别的大型数据集,
    IEEE 模式分析与机器智能学报 30 (11) (2008) 1958–1970。'
- en: '[179] W. Li, L. Wang, W. Li, E. Agustsson, L. Van Gool, Webvision database:
    Visual learning and understanding from web data, arXiv preprint arXiv:1708.02862
    (2017).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] W. Li, L. Wang, W. Li, E. Agustsson, L. Van Gool, Webvision 数据库：从网络数据中进行视觉学习和理解,
    arXiv 预印本 arXiv:1708.02862 (2017)。'
- en: '[180] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
    large-scale hierarchical image database, in: 2009 IEEE conference on computer
    vision and pattern recognition, Ieee, 2009, pp. 248–255.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: 一个大规模分层图像数据库,
    见：2009 IEEE 计算机视觉与模式识别会议, IEEE, 2009, 页 248–255。'
- en: '[181] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, J. Xiao, Lsun: Construction
    of a large-scale image dataset using deep learning with humans in the loop, arXiv
    preprint arXiv:1506.03365 (2015).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, Lsun: 使用深度学习和人工干预构建的大规模图像数据集,
    arXiv 预印本 arXiv:1506.03365 (2015)。'
- en: '[182] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, A. Torralba, Places: A 10
    million image database for scene recognition, IEEE transactions on pattern analysis
    and machine intelligence 40 (6) (2017) 1452–1464.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, A. Torralba, Places: 用于场景识别的
    1000 万图像数据库, IEEE 模式分析与机器智能学报 40 (6) (2017) 1452–1464。'
- en: '[183] T. Kaneko, Y. Ushiku, T. Harada, Label-noise robust generative adversarial
    networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, 2019, pp. 2467–2476.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] T. Kaneko, Y. Ushiku, T. Harada, 标签噪声鲁棒生成对抗网络, 见：IEEE 计算机视觉与模式识别会议论文集,
    2019, 页 2467–2476。'
- en: '[184] K. K. Thekumparampil, A. Khetan, Z. Lin, S. Oh, Robustness of conditional
    GANs to noisy labels, in: Advances in Neural Information Processing Systems, Vol.
    2018-Decem, 2018, pp. 10271–10282. [arXiv:1811.03205](http://arxiv.org/abs/1811.03205).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] K. K. Thekumparampil, A. Khetan, Z. Lin, S. Oh, 条件 GAN 对噪声标签的鲁棒性, 见：神经信息处理系统进展,
    第 2018 年 12 月卷, 2018, 页 10271–10282。 [arXiv:1811.03205](http://arxiv.org/abs/1811.03205)。'
- en: '[185] E. Fonseca, M. Plakal, D. P. Ellis, F. Font, X. Favory, X. Serra, Learning
    Sound Event Classifiers from Web Audio with Noisy Labels, in: ICASSP, IEEE International
    Conference on Acoustics, Speech and Signal Processing - Proceedings, Vol. 2019-May,
    2019, pp. 21–25. [arXiv:1901.01189](http://arxiv.org/abs/1901.01189), [doi:10.1109/ICASSP.2019.8683158](https://doi.org/10.1109/ICASSP.2019.8683158).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] E. Fonseca, M. Plakal, D. P. Ellis, F. Font, X. Favory, X. Serra, 《从网络音频中学习带噪标签的声音事件分类器》，载于：ICASSP，IEEE国际声学、语音与信号处理会议论文集，2019年5月卷，第21–25页。[arXiv:1901.01189](http://arxiv.org/abs/1901.01189)，[doi:10.1109/ICASSP.2019.8683158](https://doi.org/10.1109/ICASSP.2019.8683158)。'
- en: '[186] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional
    networks, in: European conference on computer vision, Springer, 2014, pp. 818–833.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. D. Zeiler, R. Fergus, 《卷积网络的可视化与理解》，载于：欧洲计算机视觉会议，Springer，2014年，第818–833页。'
- en: '[187] A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features
    from tiny images, Tech. rep., Citeseer (2009).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] A. Krizhevsky, G. Hinton等，《从微小图像中学习多个特征层》，技术报告，Citeseer（2009年）。'
- en: '[188] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Ng, Reading digits
    in natural images with unsupervised feature learning, NIPS (01 2011).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Ng, 《通过无监督特征学习阅读自然图像中的数字》，NIPS（2011年1月）。'
- en: '[189] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, L.-J. Li, The new data and new challenges in multimedia research, arXiv
    preprint arXiv:1503.01817 1 (8) (2015).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, L.-J. Li, 《多媒体研究中的新数据与新挑战》，arXiv预印本 arXiv:1503.01817 1 (8) (2015年)。'
- en: '[190] Y. Lin, Y. Li, C. Wang, J. Chen, Attribute reduction for multi-label
    learning with fuzzy rough set, Knowledge-Based Systems 152 (2018) 51–61.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Y. Lin, Y. Li, C. Wang, J. Chen, 《基于模糊粗糙集的多标签学习属性简化》，知识库系统 152 (2018)
    51–61。'
- en: '[191] H. Fan, X. Chang, D. Cheng, Y. Yang, D. Xu, A. G. Hauptmann, Complex
    event detection by identifying reliable shots from untrimmed videos, in: Proceedings
    of the IEEE International Conference on Computer Vision, 2017, pp. 736–744.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] H. Fan, X. Chang, D. Cheng, Y. Yang, D. Xu, A. G. Hauptmann, 《通过识别未剪辑视频中的可靠镜头进行复杂事件检测》，载于：IEEE国际计算机视觉会议论文集，2017年，第736–744页。'
