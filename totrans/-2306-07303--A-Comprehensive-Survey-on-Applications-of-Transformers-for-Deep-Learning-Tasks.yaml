- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:39:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:10
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2306.07303] A Comprehensive Survey on Applications of Transformers for Deep
    Learning Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2306.07303] 深度学习任务中的变换器应用的综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.07303](https://ar5iv.labs.arxiv.org/html/2306.07303)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.07303](https://ar5iv.labs.arxiv.org/html/2306.07303)
- en: A Comprehensive Survey on Applications of
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习任务中的变换器应用的综合调查
- en: Transformers for Deep Learning Tasks
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习任务中的变换器
- en: Saidul Islam¹, Hanae Elmekki¹, Ahmed Elsebai¹, Jamal Bentahar^(1,2,∗), Najat
    Drawel ¹, Gaith Rjoub^(3,1), Witold Pedrycz^(4,5,6,7)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Saidul Islam¹, Hanae Elmekki¹, Ahmed Elsebai¹, Jamal Bentahar^(1,2,∗), Najat
    Drawel ¹, Gaith Rjoub^(3,1), Witold Pedrycz^(4,5,6,7)
- en: ¹Concordia Institute for Information Systems Engineering, Concordia University,
    Montreal, Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹康考迪亚大学信息系统工程研究所，加拿大蒙特利尔
- en: ²Department of Electrical Engineering and Computer Science, Khalifa University,
    Abu Dhabi, UAE
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²哈利法大学电气工程与计算机科学系，阿布扎比，阿联酋
- en: ³King Hussein School of Computing Sciences, Princess Sumaya University for Technology,
    Jordan
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³约旦公主苏玛亚科技大学计算科学系
- en: ⁴Department of Electrical and Computer Engineering, University of Alberta, Edmonton,
    Canada
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴阿尔伯塔大学电气与计算机工程系，加拿大埃德蒙顿
- en: ⁵Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵波兰科学院系统研究所，波兰华沙
- en: ⁶Department of Computer Engineering, Istinye University, Sariyer/Istanbul, Turkiye
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶伊斯廷耶大学计算机工程系，土耳其伊斯坦布尔
- en: ⁷Department of Electrical and Computer Engineering, King Abdulaziz University,
    Jeddah, Saudi Arabia
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷阿卜杜拉齐兹国王大学电气与计算机工程系，沙特阿拉伯吉达
- en: '^∗Corresponding Author’s Email: jamal.bentahar@concordia.ca'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者电子邮件：jamal.bentahar@concordia.ca
- en: 'Contributing Authors’ Emails: saidul.islam@concordia.ca; hanae.elmekki@mail.concordia.ca;'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献作者电子邮件：saidul.islam@concordia.ca; hanae.elmekki@mail.concordia.ca;
- en: ahmed.elsebai@outlook.com; n_drawe@encs.concordia.ca; g.rjoub@psut.edu.jo; wpedrycz@ualberta.ca
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ahmed.elsebai@outlook.com; n_drawe@encs.concordia.ca; g.rjoub@psut.edu.jo; wpedrycz@ualberta.ca
- en: The authors contributed equally to this work
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者对本工作贡献均等
- en: Abstract
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Transformer is a deep neural network that employs a self-attention mechanism
    to comprehend the contextual relationships within sequential data. Unlike conventional
    neural networks or updated versions of Recurrent Neural Networks (RNNs) such as
    Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies
    between input sequence elements and enable parallel processing. As a result, transformer-based
    models have attracted substantial interest among researchers in the field of artificial
    intelligence. This can be attributed to their immense potential and remarkable
    achievements, not only in Natural Language Processing (NLP) tasks but also in
    a wide range of domains, including computer vision, audio and speech processing,
    healthcare, and the Internet of Things (IoT). Although several survey papers have
    been published highlighting the transformer’s contributions in specific fields,
    architectural differences, or performance evaluations, there is still a significant
    absence of a comprehensive survey paper encompassing its major applications across
    various domains. Therefore, we undertook the task of filling this gap by conducting
    an extensive survey of proposed transformer models from 2017 to 2022\. Our survey
    encompasses the identification of the top five application domains for transformer-based
    models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing,
    and Signal Processing. We analyze the impact of highly influential transformer-based
    models in these domains and subsequently classify them based on their respective
    tasks using a proposed taxonomy. Our aim is to shed light on the existing potential
    and future possibilities of transformers for enthusiastic researchers, thus contributing
    to the broader understanding of this groundbreaking technology.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是一种深度神经网络，它采用自注意力机制来理解序列数据中的上下文关系。与传统神经网络或更新版的递归神经网络（RNN），如长短期记忆（LSTM）不同，transformer
    模型在处理输入序列元素之间的长期依赖关系方面表现出色，并且能够进行并行处理。因此，基于 transformer 的模型在人工智能领域引起了研究人员的广泛关注。这可以归因于它们在自然语言处理（NLP）任务以及计算机视觉、音频和语音处理、医疗保健和物联网（IoT）等多个领域中的巨大潜力和卓越成就。尽管已有若干综述论文重点介绍了
    transformer 在特定领域的贡献、架构差异或性能评估，但仍然缺乏一份全面的综述论文，涵盖其在各个领域的主要应用。因此，我们承担了填补这一空白的任务，通过对
    2017 年至 2022 年提出的 transformer 模型进行广泛调查。我们的调查包括识别基于 transformer 的模型的五大应用领域，即：NLP、计算机视觉、多模态、音频和语音处理以及信号处理。我们分析了这些领域中影响力巨大的基于
    transformer 的模型，并随后根据它们的任务使用提出的分类法对其进行分类。我们的目标是揭示 transformer 的现有潜力和未来可能性，为热心的研究人员提供启示，从而促进对这一突破性技术的更广泛理解。
- en: 'Keywords: Self-attention; Transformer; Deep learning, Recurrent networks; Long
    short-term memory-LSTM; Multi-modality.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：自注意力；Transformer；深度学习；递归网络；长短期记忆-LSTM；多模态。
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Neural Networks (DNNs) have emerged as the predominant infrastructure and
    state-of-the-art solution for the majority of learning-based machine intelligence
    tasks in the field of artificial intelligence. Although various types of DNNs
    are utilized for specific tasks, the multilayer perceptron (MLP) represents the
    classic form of neural network which is characterized by multiple linear layers
    and nonlinear activation functions (Murtagh, [1990](#bib.bib114)). For instance,
    in computer vision, convolutional neural networks incorporate convolutional layers
    to process images, while recurrent neural networks employ recurrent cells to process
    sequential data, particularly in Natural Language Processing (NLP) (O’Shea & Nash,
    [2015](#bib.bib120), Mikolov et al., [2010](#bib.bib111)). Despite the wide use
    of recurrent neural networks, they exhibit certain limitations. One of the major
    issues with conventional networks is that they have short-term dependencies associated
    with exploding and vanishing gradients. In contrast, to achieve good results in
    NLP, long-term dependencies must be captured. Additionally, recurrent neural networks
    are slow to train due to their sequential data processing and computational approach
    (Giles et al., [1995](#bib.bib41)). To address these issues, the long-short-term
    memory (LSTM) version of recurrent networks was developed, which improves the
    gradient descent problem of recurrent neural networks and increases the memory
    range of NLP tasks (Hochreiter & Schmidhuber, [1997](#bib.bib61)). However, LSTMs
    still struggle with the problem of sequential processing, which hinders the extraction
    of the actual meaning of the context. To tackle this challenge, bidirectional
    LSTMs were introduced, which process natural language from both directions, i.e.,
    left to right and right to left, and then concatenate the outcomes to obtain the
    context’s actual meaning. Nevertheless, this technique still results in a slight
    loss of the true meaning of the context (Graves & Schmidhuber, [2005](#bib.bib43),
    Li et al., [2020b](#bib.bib91)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）已经成为人工智能领域中大多数基于学习的机器智能任务的主要基础设施和最先进的解决方案。虽然各种类型的DNN被用于特定任务，但多层感知器（MLP）代表了经典的神经网络形式，其特点是多个线性层和非线性激活函数（Murtagh，[1990](#bib.bib114)）。例如，在计算机视觉中，卷积神经网络通过卷积层处理图像，而递归神经网络则采用递归单元处理序列数据，特别是在自然语言处理（NLP）中（O’Shea
    & Nash，[2015](#bib.bib120)，Mikolov et al., [2010](#bib.bib111)）。尽管递归神经网络被广泛使用，但它们仍然存在一些局限性。传统网络的主要问题之一是与梯度爆炸和消失相关的短期依赖。相比之下，要在NLP中取得良好的结果，必须捕捉长期依赖。此外，由于其序列数据处理和计算方法，递归神经网络的训练速度较慢（Giles
    et al., [1995](#bib.bib41)）。为了解决这些问题，开发了递归网络的长短期记忆（LSTM）版本，它改善了递归神经网络的梯度下降问题，并增加了NLP任务的记忆范围（Hochreiter
    & Schmidhuber，[1997](#bib.bib61)）。然而，LSTM仍然面临序列处理的问题，这阻碍了实际语境含义的提取。为应对这一挑战，引入了双向LSTM，它从两个方向处理自然语言，即从左到右和从右到左，然后将结果连接起来以获得语境的实际含义。然而，这种技术仍然会导致语境真实含义的轻微丧失（Graves
    & Schmidhuber，[2005](#bib.bib43)，Li et al., [2020b](#bib.bib91)）。
- en: Transformers are a type of deep neural network (DNNs) that offer a solution
    to the limitations of sequence-to-sequence (seq-2-seq) architectures, including
    short-term dependency of sequence inputs and the sequential processing of input,
    which hinders parallel training of networks. Transformers leverage the multi-head
    self-attention mechanism to extract features, and they exhibit great potential
    for application in NLP. Unlike traditional recurrence methods, transformers utilize
    attention to learn from an entire segment of a sequence, using encoding and decoding
    blocks. One key advantage of transformers over LSTM and recurrent neural networks
    is their ability to capture the true meaning of the context, owing to their attention
    mechanism. Moreover, transformers are faster since they can work in parallel,
    unlike recurrent networks, and can be calculated using Graphic Processing Units
    (GPUs), allowing for faster computation of tasks with large inputs (Niu et al.,
    [2021](#bib.bib118), Vaswani et al., [2017](#bib.bib172), Zheng et al., [2020](#bib.bib208)).
    The advantages of the transformer model have inspired deep learning researchers
    to explore its potential for various tasks in different fields of application
    (Ren et al., [2023](#bib.bib137)), leading to numerous research papers and the
    development of transformer-based models for a range of tasks in the field of artificial
    intelligence (Yeh et al., [2019](#bib.bib195), Wang et al., [2019](#bib.bib182),
    Reza et al., [2022](#bib.bib139)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是一种深度神经网络（DNNs），它们提供了解决序列到序列（seq-2-seq）架构局限性的方法，包括序列输入的短期依赖和输入的顺序处理，这阻碍了网络的并行训练。变压器利用多头自注意力机制提取特征，并展现出在自然语言处理（NLP）中的巨大应用潜力。与传统的递归方法不同，变压器使用注意力机制从序列的整个段落中学习，采用编码和解码模块。与LSTM和递归神经网络相比，变压器的一个关键优势是其能够捕捉上下文的真实含义，这要归功于其注意力机制。此外，变压器由于可以并行工作，计算速度更快，不像递归网络那样，并且可以使用图形处理单元（GPUs）进行计算，允许更快地处理大输入任务（Niu
    et al., [2021](#bib.bib118)，Vaswani et al., [2017](#bib.bib172)，Zheng et al.,
    [2020](#bib.bib208)）。变压器模型的优势激励了深度学习研究者探索其在不同应用领域中各种任务的潜力（Ren et al., [2023](#bib.bib137)），从而产生了大量研究论文以及针对人工智能领域各种任务的变压器模型的发展（Yeh
    et al., [2019](#bib.bib195)，Wang et al., [2019](#bib.bib182)，Reza et al., [2022](#bib.bib139)）。
- en: In the research community, the importance of survey papers in providing a productive
    analysis, comparison, and contribution of progressive topics is widely recognized.
    Numerous survey papers on the topic of transformers can be found in the literature.
    Most of them are addressing specific fields of application (Khan et al., [2022](#bib.bib79),
    Wang et al., [2020a](#bib.bib175), Shamshad et al., [2023](#bib.bib149)), compare
    the performance of different model(Tay et al., [2023](#bib.bib169), Fournier et al.,
    [2021](#bib.bib38), Selva et al., [2023](#bib.bib148)), or conduct architecture-based
    analysis (Lin et al., [2022](#bib.bib95)). Nevertheless, a well-defined structure
    that comprehensively focuses on the top application fields and systematically
    analyzes the contribution of transformer-based models in the execution of various
    deep learning tasks within those fields is still widely needed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究界，调研论文在提供生产性分析、比较和进步主题贡献方面的重要性得到了广泛认可。文献中可以找到大量关于变压器的调研论文。它们中的大多数都涉及特定的应用领域（Khan
    et al., [2022](#bib.bib79)，Wang et al., [2020a](#bib.bib175)，Shamshad et al.,
    [2023](#bib.bib149)），比较不同模型的性能（Tay et al., [2023](#bib.bib169)，Fournier et al.,
    [2021](#bib.bib38)，Selva et al., [2023](#bib.bib148)），或进行基于架构的分析（Lin et al., [2022](#bib.bib95)）。然而，仍然需要一个明确定义的结构，全面关注顶级应用领域，并系统地分析变压器模型在这些领域中执行各种深度学习任务的贡献。
- en: Indeed, conducting a survey on transformer applications would serve as a valuable
    reference source for enthusiastic deep-learning researchers seeking to gain a
    better understanding of the contributions of transformer models in diverse fields.
    Such a survey would enable the identification and discussion of potential models,
    their characteristics, and working methodology, thus promoting the refinement
    of existing transformer models and the discovery of novel transformer models or
    applications. To address the absence of such a survey, this paper presents a comprehensive
    analysis of all transformer-based models, and identifies the top five application
    fields, namely NLP, Computer Vision, Multi-Modality, Audio & Speech, and Signal
    Processing), and proposes a taxonomy of transformer models, with significant models
    being classified and analyzed based on their task execution within these fields.
    Furthermore, the top-performing and significant models are analyzed within the
    application fields, and based on this analysis, we discuss the future prospects
    and challenges of transformer models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，对变换器应用进行调查将为热衷于深度学习的研究人员提供宝贵的参考资料，帮助他们更好地理解变换器模型在各个领域的贡献。这种调查将有助于识别和讨论潜在模型、它们的特性和工作方法，从而促进现有变换器模型的完善以及新型变换器模型或应用的发现。为了解决缺乏此类调查的问题，本文提供了对所有基于变换器的模型的综合分析，并确定了五个主要应用领域，即NLP、计算机视觉、多模态、音频与语音以及信号处理，并提出了变换器模型的分类法，将重要模型根据其在这些领域内的任务执行进行分类和分析。此外，本文还分析了在这些应用领域内表现优异和重要的模型，并基于这些分析讨论了变换器模型的未来前景和挑战。
- en: 1.1 Contributions and Motivations
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 贡献与动机
- en: 'Although several survey articles on the topic of transformers already exist
    in the literature, our motivations for conducting this survey stem from two essential
    observations. First, most of these studies have focused on transformer architecture,
    model efficiency, and specific artificial intelligence fields, such as NLP, computer
    vision, multi-modality, audio & speech, and signal processing. They have often
    neglected other crucial aspects, such as the transformer-based model’s execution
    in deep learning tasks across multiple application domains. We aim in this survey
    to cover all major fields of application and present significant models for different
    task executions. The second motivation is the absence of a comprehensive and methodical
    analysis encompassing various prevalent application domains, and their corresponding
    utilization of transformer-based models, in relation to diverse deep learning
    tasks within distinct fields of application. We propose a high-level classification
    framework for transformer models, which is based on their most prominent fields
    of application. The prominent models are categorized and evaluated based on their
    task performance within the respective fields. In this survey, we highlight the
    application domains of transformers that have received comparatively greater or
    lesser attention from researchers. To the best of our knowledge, this is the first
    review paper that presents a high-level classification scheme for the transformer-based
    models and provides a collection of criteria that aim to achieve two objectives:
    (1) assessing the effectiveness of transformer models in various applications;
    and (2) assisting researchers interested in exploring and extending the capabilities
    of transformer-based models to new domains. Moreover, the paper provides valuable
    insights into potential future applications and highlights unresolved challenges
    within this field.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文献中已经存在若干关于变压器主题的综述文章，但我们进行这次综述的动机源于两个基本观察。首先，这些研究大多集中在变压器架构、模型效率以及特定的人工智能领域，如NLP、计算机视觉、多模态、音频与语音以及信号处理。他们常常忽视了其他关键方面，比如基于变压器的模型在多个应用领域的深度学习任务中的执行。我们在本次综述中旨在涵盖所有主要的应用领域，并展示不同任务执行的重要模型。第二个动机是缺乏一个全面而系统的分析，涵盖各种流行应用领域及其相应的变压器模型利用情况，与不同应用领域内的深度学习任务相关联。我们提出了一个基于变压器模型最突出应用领域的高级分类框架。这些显著模型根据其在各自领域内的任务表现进行分类和评估。在这次综述中，我们突出变压器在研究者关注度较高或较低的应用领域。根据我们所知，这是第一篇提出变压器模型高级分类方案并提供一系列旨在实现两个目标的标准的综述论文：（1）评估变压器模型在各种应用中的有效性；（2）帮助有意探索和扩展变压器模型在新领域中能力的研究者。此外，本文还提供了对未来潜在应用的宝贵见解，并突出了该领域内尚未解决的挑战。
- en: The remainder of the paper is organized as follows. Preliminary concepts important
    for the rest of the paper are explained in Section [2](#S2 "2 Preliminaries ‣
    A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks").
    A detailed description of the systematic methodology used to search for relevant
    research articles is provided in Section [3](#S3 "3 Research Methodology ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks"). Section [4](#S4
    "4 Related Work ‣ A Comprehensive Survey on Applications of Transformers for Deep
    Learning Tasks") presents related review papers and discusses similarities and
    differences with the current survey paper, which helps us identify the unique
    characteristics and the value added of our survey. Section [5](#S5 "5 TRANSFORMER
    APPLICATIONS ‣ A Comprehensive Survey on Applications of Transformers for Deep
    Learning Tasks") identifies the the transformer models proposed so far across
    different fields of application. A Classification of the selected scientific articles
    on section [6](#S6 "6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS
    ‣ A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks").
    Section [7](#S7 "7 Future Prospects and Challenges ‣ A Comprehensive Survey on
    Applications of Transformers for Deep Learning Tasks") outlines potential directions
    for future work. Finally, Section [8](#S8 "8 Conclusion ‣ A Comprehensive Survey
    on Applications of Transformers for Deep Learning Tasks") concludes the paper
    and summarizes the key findings and contributions of the study.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。对于论文其余部分重要的初步概念在[2](#S2 "2 Preliminaries ‣ A Comprehensive Survey
    on Applications of Transformers for Deep Learning Tasks")节中进行了说明。用于检索相关研究文章的系统方法在[3](#S3
    "3 Research Methodology ‣ A Comprehensive Survey on Applications of Transformers
    for Deep Learning Tasks")节中进行了详细描述。[4](#S4 "4 Related Work ‣ A Comprehensive Survey
    on Applications of Transformers for Deep Learning Tasks")节展示了相关的综述论文，并讨论了与当前综述论文的相似之处和不同之处，帮助我们识别我们综述的独特特征和附加价值。[5](#S5
    "5 TRANSFORMER APPLICATIONS ‣ A Comprehensive Survey on Applications of Transformers
    for Deep Learning Tasks")节识别了迄今为止在不同应用领域提出的Transformer模型。[6](#S6 "6 APPLICATION-BASED
    CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive Survey on Applications
    of Transformers for Deep Learning Tasks")节对所选科学文章进行了分类。[7](#S7 "7 Future Prospects
    and Challenges ‣ A Comprehensive Survey on Applications of Transformers for Deep
    Learning Tasks")节概述了未来工作的潜在方向。最后，[8](#S8 "8 Conclusion ‣ A Comprehensive Survey
    on Applications of Transformers for Deep Learning Tasks")节总结了论文的关键发现和贡献。
- en: 2 Preliminaries
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步知识
- en: Before delving into the literature of transformers, let us describe some concepts
    that will be used throughout this article.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在*深入*探讨Transformer文献之前，让我们描述一些将在本文中使用的概念。
- en: 2.1 Transformer Architecture
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 Transformer架构
- en: The transformer model was first proposed in 2017 for a machine translation task,
    and since then, numerous models have been developed based on the inspiration of
    the original transformer model to address a variety of tasks across different
    fields. While some models have utilized the vanilla transformer architecture as
    is, others have leveraged only the encoder or decoder module of the transformer
    model. As a result, the task and performance of transformer-based models can vary
    depending on the specific architecture employed. Nonetheless, a key and widely
    used component of transformer models is self-attention, which is essential to
    their functionality. All transformer-based models employ the self-attention mechanism
    and multi-head attention, which typically forms the primary learning layer of
    the architecture. Given the significance of self-attention, the role of the attention
    mechanism is crucial in transformer models (Vaswani et al., [2017](#bib.bib172))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型首次提出是在2017年用于机器翻译任务，此后，许多基于原始Transformer模型灵感开发的模型应运而生，以解决不同领域的各种任务。虽然一些模型采用了原始的Transformer架构，但也有一些模型仅利用了Transformer模型的编码器或解码器模块。因此，基于Transformer的模型的任务和性能会因所采用的具体架构而有所不同。然而，Transformer模型的一个关键且广泛使用的组件是自注意力机制，它对其功能至关重要。所有基于Transformer的模型都使用自注意力机制和多头注意力，这通常构成了架构的主要学习层。鉴于自注意力机制的重要性，注意力机制在Transformer模型中的作用至关重要（Vaswani等，[2017](#bib.bib172)）。
- en: 2.1.1 Attention Mechanism
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 注意力机制
- en: The attention mechanism has garnered significant recognition since its introduction
    in the 1990s, owing to its ability to concentrate on critical pieces of information.
    In image processing, certain regions of images were found to be more pertinent
    than others. Consequently, the attention mechanism was introduced as a novel approach
    in computer vision tasks, aiming to emphasize important parts based on their contextual
    relevance in the application. This technique yielded significant outcomes when
    implemented in computer vision, thereby promoting its widespread adoption in various
    other fields such as language processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年代引入以来，注意力机制因其能够集中关注关键信息而获得了广泛认可。在图像处理领域，发现图像的某些区域比其他区域更为相关。因此，注意力机制被引入计算机视觉任务中，旨在根据应用中的上下文相关性强调重要部分。这项技术在计算机视觉中的应用取得了显著成果，从而推动了它在语言处理等其他领域的广泛采用。
- en: 'In 2017, a novel attention-based neural network, named “Transformer”, was introduced
    to address the limitations of other neural networks (such as A recurrent neural
    network (RNN)) in encoding long-range dependencies in sequences, particularly
    in language translation tasks (Vaswani et al., [2017](#bib.bib172)). The incorporation
    of a self-attention mechanism in the transformer model improved the performance
    of the attention mechanism by better capturing local features and reducing the
    reliance on external information. In the original transformer architecture, the
    attention technique is implemented through the “Scaled Dot Product Attention”,
    which is based on three primary parameter matrices: Query (Q), Key (K), and Value
    (V). Each of these matrices carries an encoded representation of each input in
    the sequence (Vaswani et al., [2017](#bib.bib172)). The SoftMax function is applied
    to obtain the final output of the attention process, which is a probability score
    computed from the combination of the weights of the three matrices (see Figure
    [1](#S2.F1 "Figure 1 ‣ 2.1.1 Attention Mechanism ‣ 2.1 Transformer Architecture
    ‣ 2 Preliminaries ‣ A Comprehensive Survey on Applications of Transformers for
    Deep Learning Tasks")). Mathematically, the scaled dot product attention function
    is computed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，介绍了一种名为“Transformer”的新型基于注意力的神经网络，以解决其他神经网络（如递归神经网络（RNN））在编码序列中的长程依赖关系，特别是在语言翻译任务中的局限性（Vaswani等，[2017](#bib.bib172)）。Transformer模型中自注意力机制的引入，通过更好地捕捉局部特征并减少对外部信息的依赖，提升了注意力机制的性能。在原始Transformer架构中，注意力技术通过“缩放点积注意力”实现，该方法基于三个主要参数矩阵：查询（Q）、键（K）和值（V）。每个矩阵携带序列中每个输入的编码表示（Vaswani等，[2017](#bib.bib172)）。应用SoftMax函数以获得注意力过程的最终输出，即从三个矩阵的权重组合中计算得出的概率得分（见图[1](#S2.F1
    "图1 ‣ 2.1.1 注意力机制 ‣ 2.1 Transformer架构 ‣ 2 前言 ‣ 关于深度学习任务中Transformer应用的综合调查")）。在数学上，缩放点积注意力函数的计算方式如下：
- en: '|  | $Attention(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{dk}}\right)V$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $Attention(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{dk}}\right)V$ |  |'
- en: The matrices $Q$ and $K$ represent the Query and Key vectors respectively, both
    having a dimension of $dk$, while the matrix $V$ represents the values vectors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵$Q$和$K$分别代表查询向量和键向量，维度均为$dk$，而矩阵$V$则代表值向量。
- en: '![Refer to caption](img/343750999960716671197f7ba7b2ea2d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/343750999960716671197f7ba7b2ea2d.png)'
- en: 'Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al.,
    [2017](#bib.bib172))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：多头注意力与缩放点积注意力（Vaswani等，[2017](#bib.bib172)）
- en: 2.1.2 Multi-head attention
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 多头注意力
- en: The application of the scaled dot-product attention function in parallel within
    the multi-head Attention module is essential for extracting the maximum dependencies
    among different segments in the input sequence. Each head denoted by $k$ performs
    the attention mechanism based on its own learnable weights $W^{kQ}$, $W^{kK}$,
    and $W^{kv}$. The attention outputs calculated by each head are subsequently concatenated
    and linearly transformed into a single matrix with the expected dimension (Vaswani
    et al., [2017](#bib.bib172)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在多头注意力模块中并行应用缩放点积注意力函数对于提取输入序列中不同片段之间的最大依赖关系至关重要。每个头用$k$表示，基于其自身可学习的权重$W^{kQ}$、$W^{kK}$和$W^{kv}$执行注意力机制。每个头计算出的注意力输出随后被连接在一起，并线性转换为期望维度的单一矩阵（Vaswani等，[2017](#bib.bib172)）。
- en: '|  | $\displaystyle headk=Attention(QW^{kQ},KW^{kK},VW^{kV})$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle headk=Attention(QW^{kQ},KW^{kK},VW^{kV})$ |  |'
- en: '|  | $\displaystyle MultiHead(Q,K,V)=Concat(head1,head2,....headH)W^{0}$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MultiHead(Q,K,V)=Concat(head1,head2,....headH)W^{0}$ |  |'
- en: The utilization of multi-head attention facilitates the neural network in learning
    and capturing diverse characteristics of the input sequential data. Consequently,
    this enhances the representation of the input contexts, as it merges information
    from distinct features of the attention mechanism within a specific range, which
    could be either short or long. This approach allows the attention mechanism to
    jointly function, which results in better network performance (Vaswani et al.,
    [2017](#bib.bib172)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力的使用帮助神经网络学习和捕捉输入序列数据的多样特征。因此，这增强了输入上下文的表示，因为它融合了来自注意力机制不同特征的信息，这些信息可以是短期的也可以是长期的。这种方法允许注意力机制共同作用，从而提高网络性能（Vaswani
    等，[2017](#bib.bib172)）。
- en: 2.2 Architecture of the Transformer Model
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 Transformer 模型的架构
- en: The transformer model was primarily developed based on the attention mechanism
    (Vaswani et al., [2017](#bib.bib172)), with the aim of processing sequential data.
    Its outstanding performance, especially in achieving state-of-the-art benchmarks
    for NLP translation models, has led to the widespread use of transformers. As
    depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Architecture of the Transformer
    Model ‣ 2 Preliminaries ‣ A Comprehensive Survey on Applications of Transformers
    for Deep Learning Tasks"), the overall architecture of the transformer model for
    sentence translation tasks involves the use of attention mechanisms. However,
    for different applications, the transformer architecture may be subject to variation,
    depending on specific requirements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型主要基于注意力机制（Vaswani 等，[2017](#bib.bib172)）开发，旨在处理序列数据。其卓越的表现，特别是在
    NLP 翻译模型中达到最先进的基准，导致了 Transformer 的广泛使用。如图 [2](#S2.F2 "Figure 2 ‣ 2.2 Architecture
    of the Transformer Model ‣ 2 Preliminaries ‣ A Comprehensive Survey on Applications
    of Transformers for Deep Learning Tasks") 所示，Transformer 模型的整体架构用于句子翻译任务中涉及使用注意力机制。然而，对于不同的应用，Transformer
    架构可能会根据具体要求有所变化。
- en: '![Refer to caption](img/64c3762c24af2d9c1d6b45a747468c6e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64c3762c24af2d9c1d6b45a747468c6e.png)'
- en: 'Figure 2: Transformer architecture (Vaswani et al., [2017](#bib.bib172))'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: Transformer 架构 (Vaswani 等，[2017](#bib.bib172))'
- en: The initial transformer architecture was developed based on the auto-regressive
    sequence transduction model, comprising two primary modules, namely Encoder and
    Decoder. These modules are executed multiple times, as required by the task at
    hand. Each module comprises several layers that integrate the attention mechanism.
    Particularly, the attention mechanism is executed in parallel multiple times within
    the transformer architecture, which explains the presence of multiple “Attention
    Heads” (Vaswani et al., [2017](#bib.bib172)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的 Transformer 架构基于自回归序列转导模型开发，包括两个主要模块，即编码器和解码器。这些模块根据任务需要被执行多次。每个模块包含几个层，这些层整合了注意力机制。特别地，注意力机制在
    Transformer 架构中并行执行多次，这解释了多个“注意力头”的存在（Vaswani 等，[2017](#bib.bib172)）。
- en: 2.2.1 Encoder module
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 编码器模块
- en: The stacked module within the transformer architecture comprises two fundamental
    layers, namely the Feed-Forward Layer and Multi-Head Attention Layer. In addition,
    it incorporates Residual connections around both layers, as well as two Add and
    Norm layers, which play a pivotal role (Vaswani et al., [2017](#bib.bib172)).
    In the case of text translation, the Encoder module receives an embedding input
    that is generated based on the input’s meaning and position information via the
    Embedding and Position Encoding layers. From the embedding input, three parameter
    matrices are created, namely the Query ($Q$), Key ($K$), and Value ($V$) matrices,
    along with positional information, which are passed through the “Multi-Head Attention”
    layer. Following this step, the Feed-Forward layer addresses the issue of rank
    collapse that can arise during the computation process. Additionally, a normalization
    layer is applied to each step, which reduces the dependencies between layers by
    normalizing the weights used in gradient computation within each layer. To address
    the issue of vanishing gradients, the Residual Connection is applied to every
    output of both the attention and feed-forward layers, as illustrated in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.2 Architecture of the Transformer Model ‣ 2 Preliminaries
    ‣ A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks").
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器架构中的堆叠模块包含两个基本层，即前馈层和多头注意力层。此外，它还在两个层之间加入了残差连接，以及两个加法归一化层，这些层起着关键作用（Vaswani
    et al., [2017](#bib.bib172)）。在文本翻译的情况下，编码器模块接收一个嵌入输入，该输入是通过嵌入层和位置编码层基于输入的含义和位置信息生成的。从嵌入输入中，创建了三个参数矩阵，即查询矩阵（$Q$）、键矩阵（$K$）和值矩阵（$V$），以及位置编码信息，这些都通过“多头注意力”层处理。接下来，前馈层解决了计算过程中可能出现的秩崩溃问题。此外，每一步都应用了归一化层，这通过在每层中归一化用于梯度计算的权重来减少层之间的依赖。为了应对梯度消失问题，残差连接被应用于注意力层和前馈层的每个输出，如图[2](#S2.F2
    "Figure 2 ‣ 2.2 Architecture of the Transformer Model ‣ 2 Preliminaries ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks")所示。
- en: 2.2.2 Decoder module
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 解码器模块
- en: 'The Decoder module in the transformer architecture is similar to the Encoder
    module, with the inclusion of additional layers such as Masked Multi-Head Attention.
    In addition to the Feed-Forward, Multi-Head Attention, Residual connection, and
    Add and Norm layers, the Decoder also contains Masked Multi-Head Attention layers.
    These layers use the scaled dot product and Mask Operations to exclude future
    predictions and consider only previous outputs. The Attention mechanism is applied
    twice in the Decoder: one for computing attention between elements of the targeted
    output and another for finding attention between the encoding inputs and targeted
    output. Each attention vector is then passed through the feed-forward unit to
    make the output more comprehensible to the layers. The generated decoding result
    is then caught by Linear and SoftMax layers at the top of the Decoder to compute
    the final output of the transformer architecture. This process is repeated multiple
    times until the last token of a sentence is found (Vaswani et al., [2017](#bib.bib172)),
    as illustrated in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Architecture of the Transformer
    Model ‣ 2 Preliminaries ‣ A Comprehensive Survey on Applications of Transformers
    for Deep Learning Tasks").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器架构中的解码器模块类似于编码器模块，但增加了额外的层，如掩码多头注意力（**Masked Multi-Head Attention**）。除了前馈层、多头注意力层、残差连接层和加法归一化层之外，解码器还包含掩码多头注意力层。这些层使用缩放点积和掩码操作来排除未来的预测，只考虑之前的输出。注意力机制在解码器中应用两次：一次用于计算目标输出元素之间的注意力，另一次用于找到编码输入和目标输出之间的注意力。每个注意力向量随后通过前馈单元，以使输出对各层更加易于理解。生成的解码结果随后被解码器顶部的线性（Linear）和SoftMax层捕获，以计算转换器架构的最终输出。此过程重复多次，直到找到句子的最后一个标记（Vaswani
    et al., [2017](#bib.bib172)），如图[2](#S2.F2 "Figure 2 ‣ 2.2 Architecture of the
    Transformer Model ‣ 2 Preliminaries ‣ A Comprehensive Survey on Applications of
    Transformers for Deep Learning Tasks")所示。
- en: 3 Research Methodology
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 研究方法论
- en: 'In this survey, we collect and analyze the most recent surveys on transformers
    that have been published in refereed journals and conferences with the aim of
    studying their contributions and limitations. To gather the relevant papers, we
    employed a two-fold strategy: (1) searching using several established search engines
    and selected papers based on the keywords “survey”, “review”, “Transformer”, “attention”,
    “self-attention”, “artificial intelligence”, and “deep learning; and (2) evaluating
    the selected papers and eliminated those that were deemed irrelevant for our study.
    A detailed organization of our survey is depicted in Figure [3](#S3.F3 "Figure
    3 ‣ 3 Research Methodology ‣ A Comprehensive Survey on Applications of Transformers
    for Deep Learning Tasks").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们收集并分析了最近在审稿期刊和会议上发布的有关变换器的调查文献，旨在研究它们的贡献和局限性。为了收集相关论文，我们采用了两步策略：(1)
    使用多个已知的搜索引擎进行搜索，并根据关键词“调查”、“综述”、“变换器”、“注意力”、“自注意力”、“人工智能”和“深度学习”选择论文；(2) 评估选定的论文，并剔除那些被认为与我们研究不相关的论文。我们调查的详细组织结构如图
    [3](#S3.F3 "图 3 ‣ 3 研究方法 ‣ 深度学习任务中变换器应用的综合调查")所示。
- en: '![Refer to caption](img/8ee88cf5ec3d1e02a62ab696b505321d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8ee88cf5ec3d1e02a62ab696b505321d.png)'
- en: 'Figure 3: Methodology of the survey'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：调查方法
- en: 'Indeed, by means of a comprehensive examination of survey papers and expert
    discussions on deep learning, we have identified the top five domains of application
    for transformer-based models, these are: (i) NLP, (ii) computer vision, (iii)
    multi-modality, (iv) audio/speech, and (v) signal processing. Subsequently, we
    performed a systematic search for journal and conference papers that presented
    transformer-based models in each of the aforementioned fields of application,
    utilizing the keywords presented in Table LABEL:tab:_methodology_table. Our search
    yielded a substantial number of papers for each field, which we thoroughly reviewed
    and evaluated. We selected papers that proposed novel transformer-based or transformer-inspired
    models for deep learning tasks, while disregarding others. Through our examination
    of this extensive collection of models, we have identified prevalent deep-learning
    tasks associated with each field of application.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，通过对调查论文和深度学习专家讨论的全面审查，我们确定了变换器模型的前五个应用领域，它们是：(i) 自然语言处理 (NLP)，(ii) 计算机视觉，(iii)
    多模态，(iv) 音频/语音，和 (v) 信号处理。随后，我们对在上述各个应用领域中呈现变换器模型的期刊和会议论文进行了系统搜索，使用了表 LABEL:tab:_methodology_table
    中呈现的关键词。我们的搜索在每个领域中产生了大量论文，我们对这些论文进行了彻底的审查和评估。我们选择了提出新颖的基于变换器或变换器启发的深度学习任务模型的论文，同时忽略了其他论文。通过对这批模型的广泛审查，我们确定了与每个应用领域相关的流行深度学习任务。
- en: 'As we have examined more than 600 transformer models during this process, it
    has become exceedingly difficult to classify such a large number of models and
    conduct thorough analyses of each task within every field of application. Therefore,
    we have opted to perform a more comprehensive analysis of a number of transformer
    models for each task within every field of application. These models were selected
    based on specific criteria and an in-depth analysis was carried out accordingly.
    The selected models are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在此过程中我们审查了超过 600 个变换器模型，因此对如此大量的模型进行分类和深入分析每个应用领域中的每个任务变得极其困难。因此，我们选择对每个应用领域中的多个变换器模型进行更全面的分析。这些模型是根据特定标准选择的，并进行了相应的深入分析。所选模型如下：
- en: '1.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The transformer-based models that have been proposed to execute a deep learning
    task for the first time and opened up new path for research in the field of transformer
    applications.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首次提出基于变换器的模型来执行深度学习任务，并为变换器应用领域的研究开辟了新路径。
- en: '2.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The models that have proposed alternative or novel approaches to implementing
    the transformer’s attention mechanism, as compared to the vanilla architecture,
    such as introducing a new attention mechanism or enhancing the position encoding
    module.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与传统架构相比，提出了替代或新颖的方法来实现变换器的注意力机制的模型，例如引入新的注意力机制或增强位置编码模块。
- en: '3.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The transformer models have had a significant impact in the field, with higher
    citation rates, and have been widely accepted by the scientific community. Models
    that have also contributed to breakthroughs in the advancement of transformer
    applications.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变换器模型在该领域产生了重大影响，引用率较高，已被科学界广泛接受。这些模型也为变换器应用的突破做出了贡献。
- en: '4.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The models and their variants have been proposed for the purpose of applying
    the transformer technology to real-world applications, with the aim of achieving
    superior performance results in comparison to other deep learning methods.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些模型及其变体被提出用于将变换器技术应用于实际世界应用，旨在实现比其他深度学习方法更优的性能结果。
- en: '5.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: The transformer-based models generated a significant buzz within the theoretical
    and applied artificial intelligence community.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于变换器的模型在理论和应用人工智能领域产生了重大轰动。
- en: 'Table 1: Transformer models’ field of application, used keywords for paper
    search, popular deep learning tasks, number of relevant papers by search, and
    number of selected models for taxonomy and further discussion.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：变换器模型的应用领域、用于论文搜索的关键词、流行深度学习任务、相关论文数量以及分类法和进一步讨论中选择的模型数量。
- en: '| Fields of Application | Keywords for Paper Search | Tasks Of Application
    | Number of papers |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 应用领域 | 论文搜索关键词 | 应用任务 | 论文数量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '&#124; Relevant models &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关模型 &#124;'
- en: '&#124; using keywords &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用关键词 &#124;'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Selected models &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择的模型 &#124;'
- en: '&#124; for Taxonomy &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类法 &#124;'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural Language &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然语言 &#124;'
- en: '&#124; Processing &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理 &#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; “Natural Language Processing”, &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自然语言处理”, &#124;'
- en: '&#124; “NLP”,“Text”,“Text Processing”, &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自然语言处理”,“文本”,“文本处理”, &#124;'
- en: '&#124; “Transformer”, “Attention”, &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “变换器”, “注意力”, &#124;'
- en: '&#124; “Self-attention”, “multi-head &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自注意力”, “多头 &#124;'
- en: '&#124; attention”, “Language model”. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力”, “语言模型”。 &#124;'
- en: '| Language Translation | 257 | 25 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 语言翻译 | 257 | 25 |'
- en: '|  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Text Classification & Segmentation &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本分类与分割 &#124;'
- en: '|  |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  | Question Answering |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 问答 |  |  |'
- en: '|  |  | Text Summarization |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 文本摘要 |  |  |'
- en: '|  |  | Text Generation |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 文本生成 |  |  |'
- en: '|  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Natural Language Reasoning &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然语言推理 &#124;'
- en: '|  |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Automated Symbolic &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动符号化 &#124;'
- en: '&#124; Reasoning &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '|  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| Computer Vision |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 计算机视觉 |'
- en: '&#124; “Transformer”,“Attention”, &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “变换器”,“注意力”, &#124;'
- en: '&#124; “Self-attention”,“Image”, &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自注意力”,“图像”, &#124;'
- en: '&#124; “Natural image”,“medical &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自然图像”,“医学 &#124;'
- en: '&#124; image”,“Biomedical”, &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像”,“生物医学”, &#124;'
- en: '&#124; “health”,“Image processing”, &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “健康”,“图像处理”, &#124;'
- en: '&#124; “Computer vision”,“Vision”. &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “计算机视觉”,“视觉”。 &#124;'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Natural Image &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '&#124; Processing &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理 &#124;'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Classification &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '| 197 | 27 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 197 | 27 |'
- en: '|  |  |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '&#124; Recognition & &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别与 &#124;'
- en: '&#124; Object Detection &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测 &#124;'
- en: '|  |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '&#124; Image &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |  | Image Generation |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 图像生成 |  |  |'
- en: '|  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Medical Image &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学图像 &#124;'
- en: '&#124; Processing &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理 &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|  |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '&#124; Image &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Classification &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '|  |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '&#124; Image &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Translation &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 翻译 &#124;'
- en: '|  |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| Multi-modal |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 多模态 |'
- en: '&#124; “Transformer”,“Attention”, &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “变换器”,“注意力”, &#124;'
- en: '&#124; “Self-attention”,“multi-head &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自注意力”,“多头 &#124;'
- en: '&#124; attention”,“multimodal”, &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力”,“多模态”, &#124;'
- en: '&#124; “multi-modality”,“text-image”, &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “多模态”,“文本-图像”, &#124;'
- en: '&#124; “image-text”,“ video-audio- &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “图像-文本”,“视频-音频- &#124;'
- en: '&#124; text, “text-audio”,“audio-text”, &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本, “文本-音频”,“音频-文本”, &#124;'
- en: '&#124; “vision-language”, &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “视觉-语言”, &#124;'
- en: '&#124; “language-vision”. &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “语言-视觉”。 &#124;'
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classification & &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类与 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| 94 | 20 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 94 | 20 |'
- en: '|  |  | Visual Question Answering |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 可视化问答 |  |  |'
- en: '|  |  | Visual Captioning |  |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 视觉描述 |  |  |'
- en: '|  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Visual Common-sense &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉常识 &#124;'
- en: '&#124; Reasoning &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '|  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Text/Image/Video/Speech &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本/图像/视频/语音 &#124;'
- en: '&#124; Generation &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 &#124;'
- en: '|  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  | Cloud Task Computing |  |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 云任务计算 |  |  |'
- en: '| Audio & Speech |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 音频与语音 |'
- en: '&#124; “Transformer”,“Attention”, &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “Transformer”，“注意力机制”，&#124;'
- en: '&#124; “Self-attention”,“multi-head &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自注意力机制”，“多头 &#124;'
- en: '&#124; attention”,“audio”,“Speech”, &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制”，“音频”，“语音”，&#124;'
- en: '&#124; “audio processing”,“speech &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “音频处理”，“语音 &#124;'
- en: '&#124; processing”, &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理”，&#124;'
- en: '| Audio & Speech Recognition | 70 | 16 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 音频与语音识别 | 70 | 16 |'
- en: '|  |  | Audio & Speech Separation |  |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 音频与语音分离 |  |  |'
- en: '|  |  | Audio & Speech Classification |  |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 音频与语音分类 |  |  |'
- en: '| Signal Processing |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 信号处理 |'
- en: '&#124; “Transformer”, “Attention”, &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “Transformer”，“注意力机制”，&#124;'
- en: '&#124; “Self-attention”, “multi-head &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “自注意力机制”，“多头 &#124;'
- en: '&#124; attention”, “signal”, “signal &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意力机制”，“信号”，“信号 &#124;'
- en: '&#124; processing” , “wireless”, &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理”，“无线”，&#124;'
- en: '&#124; “wireless signal”, “wireless &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “无线信号”，“无线 &#124;'
- en: '&#124; network”, “biosignal”, “medical &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络”，“生物信号”，“医学 &#124;'
- en: '&#124; signal”. &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信号”。&#124;'
- en: '|'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Wireless network Signal &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无线网络信号 &#124;'
- en: '&#124; processing &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 处理 &#124;'
- en: '| 23 | 11 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 11 |'
- en: '|  |  | Medical Signal Processing |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 医学信号处理 |  |  |'
- en: '|  |  |  |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |'
- en: In the field of application, we have classified the selected models based on
    their task execution and developed a taxonomy of transformer applications. Our
    analysis involved a comprehensive examination of the models, including their structures,
    characteristics, operational methods, and datasets, among others. Based on this
    investigation, we provide an in-depth discussion of the potential future applications
    of transformers. To conduct this research, we consulted various prominent research
    repositories, such as “AAAI”, “ACM”, “ACL”, “CVPR”, “ICML”, “ICLR”, “ICCV”, “NeurlIPS”,
    “LREC”, “IEEE”, “PMLR”, ”National Library of medicine”,“SCOPUS”, “MDPI”, “ScienceDirect”,
    and “Cornell University-arxiv library”. Table LABEL:tab:_methodology_table depicts
    the category of the selected models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用领域，我们基于模型的任务执行情况对所选模型进行了分类，并开发了变换器应用的分类法。我们的分析涉及了对模型的全面检查，包括它们的结构、特征、操作方法和数据集等。基于这项调查，我们深入讨论了变换器的潜在未来应用。为了进行这项研究，我们查阅了各种著名的研究资源库，如“AAAI”、“ACM”、“ACL”、“CVPR”、“ICML”、“ICLR”、“ICCV”、“NeurlIPS”、“LREC”、“IEEE”、“PMLR”、“国家医学图书馆”、“SCOPUS”、“MDPI”、“ScienceDirect”和“康奈尔大学arxiv图书馆”。表LABEL:tab:_methodology_table展示了所选模型的类别。
- en: 4 Related Work
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Transformers have been subjected to a number of surveys in recent years due
    to their effectiveness and a broad range of applications. We recorded more than
    $50$ survey papers about transformers in various digital libraries and examined
    them. After carefully considering these surveys, we selected $17$ significant
    survey papers for further in-depth analysis of their works. During this process,
    we considered the surveys published in reputed conferences and journals with high
    number of citations, whereas we discarded the papers that have not been published
    yet. We extensively analyzed this set of $17$ papers, delving into their content
    and investigating their respective fields of work and applications. We prioritized
    examining the resemblances and disparities between the existing surveys and our
    own paper. Our investigation revealed that numerous surveys primarily concentrated
    on the architecture and efficiency of transformers, while others solely focused
    on applications in NLP and computer vision. However, only a few explored the utilization
    of transformers in multi-modal combinations involving text and image data. These
    findings, along with supporting details, are presented in Table LABEL:tab:related_works.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其有效性和广泛的应用范围，近年来变压器受到了许多调查的关注。我们在各种数字图书馆记录了50多篇关于变压器的调查论文，并对其进行了审查。在仔细考虑了这些调查后，我们选择了17篇重要的调查论文，对其工作进行了进一步深入分析。在此过程中，我们考虑了发表在知名会议和期刊上且具有高引用量的调查，而丢弃了尚未发表的论文。我们对这组17篇论文进行了广泛的分析，深入研究了它们的内容，并调查了它们各自的工作和应用领域。我们优先考虑研究现有调查与我们自己论文之间的相似之处和差异。我们的调查结果表明，许多调查主要集中在变压器的架构和效率上，而其他的则专注于在自然语言处理和计算机视觉中的应用。然而，只有少数研究了变压器在涉及文本和图像数据的多模组合中的利用。这些发现以及支持细节被呈现在表
    LABEL:tab:related_works 中。
- en: 'Several review papers centered their attention on conducting architecture and
    performance-based analyses of transformers. Among them, the survey paper titled
    “A Survey of Transformers” stands out as it offers a comprehensive examination
    of different X-formers and introduces a taxonomy based on architecture, pre-training,
    and application (Lin et al., [2022](#bib.bib95)). Another survey paper on transformers
    was entitled “Efficient Transformers: A survey” to compare the computational power
    and memory efficiency of X-formers (Tay et al., [2023](#bib.bib169)). Moreover,
    another paper focused on light and fast transformers while it explored different
    efficient alternatives to the standard transformers (Fournier et al., [2021](#bib.bib38)).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有几篇综述性论文将注意力集中在变压器的架构和性能分析上。其中，名为“变压器调查”的调查论文以其对不同X形变压器的全面考察以及基于架构、预训练和应用的分类法而脱颖而出（Lin等人，[2022](#bib.bib95)）。另一篇名为“高效变压器：一项调查”的调查论文旨在比较X形变压器的计算能力和内存效率（Tay等人，[2023](#bib.bib169)）。此外，另一篇论文着眼于轻量快速的变压器，并探讨了标准变压器的不同高效替代品（Fournier等人，[2021](#bib.bib38)）。
- en: 'Within the field of NLP, there exists a survey paper titled “Visualizing Transformers
    for NLP: A Brief Survey” (Brasoveanu & Andonie, [2020](#bib.bib11)). This particular
    study centers its attention on exploring the different aspects of transformers
    that can be effectively examined and understood through the application of visual
    analytics techniques. On a related note, another survey paper delves into the
    realm of pre-trained transformer-based models for NLP (Subramanyam et al., [2021b](#bib.bib161)).
    This study extensively discusses pretraining methods and tasks employed in these
    models. Moreover, it introduces a taxonomy that effectively categorizes the wide
    range of transformer-based Pre-Trained Language Models (T-PTLMs) available in
    the literature.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，存在一篇名为“用于自然语言处理的变压器可视化：一项简要调查”的调查论文（Brasoveanu & Andonie，[2020](#bib.bib11)）。这项研究特别关注于通过视觉分析技术有效审视和理解变压器的不同方面。另一篇调查论文深入探讨了用于自然语言处理的预训练基于变压器的模型（Subramanyam等人，[2021b](#bib.bib161)）。该研究广泛讨论了这些模型中采用的预训练方法和任务。此外，它介绍了一个有效分类了文献中广泛范围的基于变压器的预训练语言模型（T-PTLMs）。
- en: 'Moreover, the paper entitled “Survey on Automatic Text Summarization and Transformer
    Models Applicability” focused on using transformers for text summarization tasks
    and proposed a transformer model that solves the issue of the long sequence input
    (Wang et al., [2020a](#bib.bib175)). On the other hand, another survey worked
    on applying a bidirectional transformer encoder (BERT) in multi-layer as a word-embedding
    tool (Kaliyar, [2020](#bib.bib76)). Furthermore, the application of transformers
    to detect different levels of emotions from text-based data has been explored
    in (Acheampong et al., [2021](#bib.bib1)) under the tile “Transformer models for
    text-based emotion detection: a review of BERT-based approaches”. Another paper
    explored the use of the transformer language model in different information systems
    (Gruetzemacher & Paradice, [2022](#bib.bib44)). It focused on using transformers
    as text miners to extract useful data from large organizations’ data.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，题为“自动文本摘要与变换器模型适用性的调查”的论文专注于使用变换器进行文本摘要任务，并提出了一种解决长序列输入问题的变换器模型（Wang et al.,
    [2020a](#bib.bib175)）。另一方面，另一篇调查研究了将双向变换器编码器（BERT）应用于多层作为词嵌入工具（Kaliyar, [2020](#bib.bib76)）。此外，变换器在文本数据中检测不同情感水平的应用在（Acheampong
    et al., [2021](#bib.bib1)）中进行了探讨，标题为“基于变换器模型的文本情感检测：基于BERT的 approaches 综述”。另一篇论文探讨了变换器语言模型在不同信息系统中的使用（Gruetzemacher
    & Paradice, [2022](#bib.bib44)）。该论文重点研究了将变换器作为文本挖掘工具，从大型组织的数据中提取有用数据。
- en: 'Due to huge improvements in image processing tasks and amazing applications
    on computer vision with the help of transformer models in recent years, these
    models gained popularity among computer vision researchers. For instance, “Transformers
    in Vision: A Survey” provided a comprehensive overview of the existing transformer
    models in the field of computer vision and classified the models based on popular
    recognition tasks (Khan et al., [2022](#bib.bib79)). A meticulous survey was undertaken
    to comprehensively analyze the merits and drawbacks of the leading “Vision Transformers”.
    This study placed significant emphasis on scrutinizing the training and testing
    datasets associated with these top-performing models, offering valuable insights
    into their performance and suitability for various applications (Han et al., [2023](#bib.bib53)).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于近年来图像处理任务的巨大进步以及在计算机视觉中的惊人应用，变换器模型在计算机视觉研究人员中获得了广泛的关注。例如，“视觉中的变换器：调查”提供了计算机视觉领域现有变换器模型的全面概述，并根据流行的识别任务对模型进行了分类（Khan
    et al., [2022](#bib.bib79)）。一项细致的调查分析了领先的“视觉变换器”的优缺点。本研究特别强调了对这些顶级模型相关训练和测试数据集的审查，提供了有关其性能和适用于各种应用的宝贵见解（Han
    et al., [2023](#bib.bib53)）。
- en: Another survey paper compared transformer models developed for image and video
    data based on their performance in classification tasks (Selva et al., [2023](#bib.bib148)).
    Recent advances in computer vision and multi-modality have been emphasized in
    another survey paper (Xu et al., [2022](#bib.bib188)) comparing the performance
    of different transformer models and providing some information regarding their
    pre-training. Furthermore, an existing survey describes in detail several transformer
    models that have been developed for medical images; however, it does not provide
    information regarding medical signals (Li et al., [2023](#bib.bib89)). Another
    paper gives an overview of transformer models developed in the medical field;
    however, it only concerns medical images, not medical signals (Shamshad et al.,
    [2023](#bib.bib149)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一篇调查论文比较了针对图像和视频数据开发的变换器模型，并基于其在分类任务中的表现进行评估（Selva et al., [2023](#bib.bib148)）。另一篇调查论文强调了计算机视觉和多模态的最新进展（Xu
    et al., [2022](#bib.bib188)），比较了不同变换器模型的性能，并提供了有关其预训练的一些信息。此外，一篇现有的调查详细描述了为医学图像开发的几种变换器模型；然而，它未提供有关医学信号的信息（Li
    et al., [2023](#bib.bib89)）。另一篇论文概述了医学领域开发的变换器模型；然而，它仅涉及医学图像，而非医学信号（Shamshad et
    al., [2023](#bib.bib149)）。
- en: Multi-modality is getting very popular in deep learning tasks that helped to
    decipher several surveys on transformer focusing multi-modal domain. A paper worked
    on categorizing transformer vision-language models based on tasks and summarizing
    their co-responding advantages and disadvantages. Moreover, this survey paper
    covered video-language pre-trained models and categorized the models into single-stream
    and multi-stream structures, and the performance of the models is also compared
    in this survey (Ruan & Jin, [2022](#bib.bib144)). Another survey, “Perspectives
    and Prospects on Transformer Architecture for Cross-Modal Tasks with Language
    and Vision”, explored transformers in multi-modal visual-linguistic tasks (Shin
    et al., [2022](#bib.bib156)). Other than NLP, computer vision and multi-modality,
    transformers are getting significant attention from researchers to apply to other
    fields such as time series and reasoning tasks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态在深度学习任务中越来越受欢迎，这帮助解读了几篇关于集中在多模态领域的变压器的调查论文。一篇论文致力于基于任务对变压器视觉语言模型进行分类，并总结了它们的相应优缺点。此外，这篇调查论文涵盖了视频语言预训练模型，并将模型分为单流和多流结构，同时在此调查中还比较了模型的性能（Ruan
    & Jin，[2022](#bib.bib144)）。另一篇调查，“变压器架构在语言和视觉交叉模态任务中的视角和前景”，探索了多模态视觉语言任务中的变压器（Shin
    et al.，[2022](#bib.bib156)）。除了自然语言处理、计算机视觉和多模态，变压器还受到研究人员的重大关注，应用于时间序列和推理任务等其他领域。
- en: 'Table 2: Comparative summary between our survey and the existing surveys'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们调查与现有调查的比较总结
- en: '| Approach | Fields of Application | Similarities | Differences |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 应用领域 | 相似之处 | 差异 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Q Fournier et al. (Fournier et al., [2021](#bib.bib38)) | Performance /Architecture
    | • A classification of the transformers is suggested, and this classification
    is based on attention mechanism modification or architecture modification | •
    This pare surveyed the different alternatives of the standard transformers that
    are more efficient in terms of time and memory complexities, and these alternatives
    are categorized by either modifying the attention mechanism or the network architecture.
    Their classification is based on the change in architecture or change in attention
    mechanism, while our classification is driven by application areas. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Q Fournier et al. (Fournier et al.，[2021](#bib.bib38)) | 性能/架构 | • 提出了变压器的分类，该分类基于注意力机制的修改或架构修改
    | • 该论文调查了比标准变压器更高效的不同替代方案，这些替代方案通过修改注意力机制或网络架构进行分类。他们的分类基于架构的变化或注意力机制的变化，而我们的分类则由应用领域驱动。
    |'
- en: '| T. Lin et al. (Lin et al., [2022](#bib.bib95)) | Performance /Architecture
    | • Proposed taxonomy of X-formers covering several fields | • This existing survey
    compared X-formers from architectural modification, pre-training, and a very small
    range of application perspectives, while our survey deeply focuses on popular
    tasks under each field of application. • The wireless/medical signal processing
    and cloud computing tasks application were missing in this exciting survey, while
    our survey covers these tasks and applications. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| T. Lin et al. (Lin et al.，[2022](#bib.bib95)) | 性能/架构 | • 提出了涵盖多个领域的X-formers分类法
    | • 该现有调查从架构修改、预训练和非常小的应用视角比较了X-formers，而我们的调查深入关注每个应用领域下的热门任务。• 该引人注目的调查中缺少无线/医疗信号处理和云计算任务的应用，而我们的调查涵盖了这些任务和应用。'
- en: '| Y. Tay et al. (Tay et al., [2023](#bib.bib169)) | Performance /Architecture
    | • Proposed a taxonomy considering the primary use case of transformer models
    in language and vision domains . | • This existing survey compared the computational
    power and memory efficiency of transformer models, whereas our survey focuses
    on deep learning tasks and applications. • This exciting survey focused on language
    and vision domain only, while we cover other top five fields of transformer applications:
    NLP, computer vision, multi-modality, audio/speech, and signal processing. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Y. Tay et al. (Tay et al.，[2023](#bib.bib169)) | 性能/架构 | • 提出了一个考虑变压器模型在语言和视觉领域主要使用情况的分类法。
    | • 这篇现有的调查比较了变压器模型的计算能力和内存效率，而我们的调查关注深度学习任务和应用。• 这篇引人注目的调查仅关注语言和视觉领域，而我们涵盖了变压器应用的其他五个重要领域：自然语言处理、计算机视觉、多模态、音频/语音和信号处理。
    |'
- en: '| A. M. P. Braşoveanu et al. (Brasoveanu & Andonie, [2020](#bib.bib11)) | Natural
    language Processing-NLP | • Explain transformer architecture and explain its features.
    | • Our survey describes the transformer model and the significant models’ working
    processing for a range of tasks. However, this existing paper focused on visualization
    techniques used to explain the most recent transformer architectures and explored
    two large tool classes to explain the inner workings of Transformers. • we covered
    five fields of transformer applications: NLP, computer vision, multi-modality,
    audio/speech, and signal processing and this exciting survey focused on the models
    for NLP only. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| A. M. P. Braşoveanu 等人（Brasoveanu & Andonie, [2020](#bib.bib11)） | 自然语言处理-NLP
    | • 解释 transformer 架构并说明其特征。 | • 我们的调查描述了 transformer 模型及其在各种任务中的重要模型工作处理。然而，现有的论文专注于用于解释最新
    transformer 架构的可视化技术，并探索了两大类工具来解释 Transformers 的内部工作原理。• 我们涵盖了 transformer 应用的五个领域：NLP、计算机视觉、多模态、音频/语音和信号处理，本次激动人心的调查仅专注于
    NLP 模型。 |'
- en: '| W Guan et al. (Wang et al., [2020a](#bib.bib175)) | Natural language Processing-NLP
    | • Survey an application area of transformers, which is text summarization, which
    is one of the application areas covered in our survey | • The authors propose
    a transformer-based summarizer that solves the issues of standard transformers
    that cannot take a long text as an input. They survey different use cases of applying
    transformers to different text summarization tasks and they only cover text summarization.
    no proposed transformers have been built in our survey. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| W Guan 等人（Wang et al., [2020a](#bib.bib175)） | 自然语言处理-NLP | • 调查 transformers
    的一个应用领域，即文本摘要，这也是我们调查中涵盖的应用领域之一 | • 作者提出了一种基于 transformer 的摘要生成器，解决了标准 transformers
    无法处理长文本输入的问题。他们调查了将 transformers 应用于不同文本摘要任务的不同用例，但仅涵盖了文本摘要。我们调查中没有建立提出的 transformers。'
- en: '| R Kumar (Kaliyar, [2020](#bib.bib76)) | Natural language Processing-NLP |
    • Discussion of different NLP downstream tasks that BERT performs. BERT is covered
    in our survey as well as the different NLP tasks | • Survey different techniques
    on using BERT as a word-embedder against traditional word-embedding techniques.
    Their survey is only focused as using transformers as a tool for embedding text
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| R Kumar（Kaliyar, [2020](#bib.bib76)） | 自然语言处理-NLP | • 讨论 BERT 执行的不同 NLP 下游任务。我们的调查中也涵盖了
    BERT 以及不同的 NLP 任务 | • 调查使用 BERT 作为词嵌入工具与传统词嵌入技术的不同技术。他们的调查仅专注于将 transformers 作为嵌入文本的工具
    |'
- en: '| F Acheampong et al. (Acheampong et al., [2021](#bib.bib1)) | Natural language
    Processing-NLP | • Survey different transformer architectures that accomplish
    the emotion detection task. We do the same, the application of different transformers
    to the same type if task | • Survey the application of transformer architecture
    to a single application area but in too much detail, which is emotion detection
    from text-based data, a form of sentiment analysis but the goal is to extract
    fine-grained emotion from the data. The task of sentiment analysis is covered
    in our survey, but we didn’t cover especially the task of detecting emotions on
    different levels and not just as a binary classification task as usually done
    in sentiment analysis |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| F Acheampong 等人（Acheampong et al., [2021](#bib.bib1)） | 自然语言处理-NLP | • 调查完成情感检测任务的不同
    transformer 架构。我们也做了相同的工作，即将不同的 transformers 应用于相同类型的任务 | • 调查 transformer 架构在单一应用领域中的应用，但细节过多，即从基于文本的数据中进行情感检测，这是一种情感分析形式，但目标是从数据中提取细致的情感。情感分析任务在我们的调查中有所涵盖，但我们没有特别涵盖不同层次的情感检测任务，而不仅仅是如情感分析中通常所做的二分类任务
    |'
- en: '| R Gruetzemacher et al. (Gruetzemacher & Paradice, [2022](#bib.bib44)) | Natural
    language Processing-NLP | • Survey the progress of transformers in the text-mining
    application area. We do cover in our survey the progress of transformers on a
    wide variety of tasks | • Tackle the different transformers on how they can be
    used as text miners for organizations that have huge amounts of unstructured data
    against traditional NLP text-mining techniques |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| R Gruetzemacher 等人（Gruetzemacher & Paradice, [2022](#bib.bib44)） | 自然语言处理-NLP
    | • 调查 transformers 在文本挖掘应用领域的进展。我们的调查涵盖了 transformers 在各种任务上的进展 | • 探讨不同的 transformers
    如何用作文本挖掘器，处理拥有大量非结构化数据的组织，较传统的 NLP 文本挖掘技术 |'
- en: '| J. Selva et al. (Selva et al., [2023](#bib.bib148)) | Computer Vision | •
    This paper is an overview of transformers developed for modeling images and video
    data | • This survey focuses solely on image and video data. Models are compared
    based on their performance in video classification, it does not cover any other
    applications. The paper proposes a taxonomy of various transformer models based
    on their recurrence properties, memory capacities, and architectural design |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| J. Selva 等（Selva et al., [2023](#bib.bib148)） | 计算机视觉 | • 本文概述了用于建模图像和视频数据的变换器
    | • 本调查仅关注图像和视频数据。模型根据其在视频分类中的表现进行比较，不涉及其他应用。本文提出了基于变换器的不同模型的分类法，依据其递归特性、内存容量和架构设计
    |'
- en: '| K. S. Kalyan et al. (Subramanyam et al., [2021a](#bib.bib160)) | Natural
    language Processing-Medical | • This paper provides an overview of the developed
    transformer-based BPLMs for a wide range of NLP tasks, including Natural language
    inference, Entity extraction, Relation extraction, Semantic textual similarity,
    Text classification, Question answering, and Text summarization | • This survey
    addresses only transformer-based biomedical pre-trained language models, which
    restricts its scope to the specific field of biomedical natural language processing.
    The taxonomy does not distinguish models based on the type of application they
    are used for, but rather based on the dataset of pre-training, the embedding type,
    and other criteria such as the targeted language |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| K. S. Kalyan 等（Subramanyam et al., [2021a](#bib.bib160)） | 自然语言处理-医疗 | •
    本文概述了用于各种NLP任务的变换器基础BPLMs，包括自然语言推断、实体提取、关系提取、语义文本相似性、文本分类、问答和文本摘要 | • 本调查仅涉及基于变换器的生物医学预训练语言模型，将其范围限制在生物医学自然语言处理的特定领域。分类法不根据模型的应用类型进行区分，而是根据预训练的数据集、嵌入类型以及其他标准，如目标语言
    |'
- en: '| K. Han et al. (Han et al., [2023](#bib.bib53)) | Computer Vision | • Categorized
    vision transformer models based on different tasks | • This existing paper analyzed
    transformer models’ advantages and disadvantages, and efficient transformer methods
    for the backbone network, while our survey categorizes transformer models based
    on tasks and summarize downstream tasks and commonly used dataset. • While our
    survey paper classified computer vision tasks into two segments: natural image
    processing & medical image processing and then focused on popular computer vision
    like visual question answering, classification, segmentation, question answering,
    and so on, then this existing paper focused on high/mid-level vision, low-level
    vision, and video processing computer vision tasks. • This survey focused on computer
    vision tasks only, while we covered other four fields of applications-NLP, Multi-modal,
    Audio/speech, and signal processing besides computer vision |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| K. Han 等（Han et al., [2023](#bib.bib53)） | 计算机视觉 | • 根据不同任务对视觉变换器模型进行分类 |
    • 该现有文献分析了变换器模型的优缺点，以及用于骨干网络的高效变换器方法，而我们的调查根据任务对变换器模型进行分类，并总结了下游任务和常用数据集。 • 虽然我们的调查论文将计算机视觉任务分为两个部分：自然图像处理和医疗图像处理，然后重点关注流行的计算机视觉任务，如视觉问答、分类、分割、问答等，而该现有文献重点关注高/中级视觉、低级视觉和视频处理计算机视觉任务。
    • 本调查仅关注计算机视觉任务，而我们则覆盖了包括NLP、多模态、音频/语音和信号处理在内的其他四个应用领域，以及计算机视觉。'
- en: '| Y. Xu et al. (Xu et al., [2022](#bib.bib188)) | Computer Vision | • The survey
    covers the fields of computer vision and multimodal in a similar fashion to our
    survey | • This survey focuses primarily on recent advancements in computer vision
    by comparing the performance of different transformer models. Specifically, this
    study discusses four areas of research: advances in the design of the ViT models
    for image classification, high-level vision tasks (such as object detection and
    semantic segmentation), low-level vision tasks (such as super-resolution, and
    image generation), and multimodal learning (such as visual question answering
    (VQA), image captioning) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Y. Xu 等（Xu et al., [2022](#bib.bib188)） | 计算机视觉 | • 该调查类似于我们的调查，涵盖了计算机视觉和多模态领域
    | • 本调查主要关注计算机视觉领域的最新进展，通过比较不同变换器模型的表现。具体而言，本研究讨论了四个研究领域：用于图像分类的ViT模型设计的进展、高级视觉任务（如目标检测和语义分割）、低级视觉任务（如超分辨率和图像生成），以及多模态学习（如视觉问答（VQA）、图像描述）
    |'
- en: '| J Li et al. (Li et al., [2023](#bib.bib89)) | Computer Vision | • Comparative
    analysis of transformer models is presented in this paper for several tasks involved
    in medical vision. Several criteria are considered when comparing papers, including
    the type of dataset, the type of input data, and the architecture of the model
    | • This paper describes in detail several transformer models that have been developed
    for medical images; however, it does not provide information regarding medical
    signals |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| J Li 等（Li 等, [2023](#bib.bib89)） | 计算机视觉 | • 本文对涉及医学视觉的几个任务中的变换器模型进行了比较分析。在比较论文时考虑了几个标准，包括数据集类型、输入数据类型和模型架构
    | • 本文详细描述了为医学图像开发的几种变换器模型；然而，它没有提供有关医学信号的信息 |'
- en: '| F Shamshad et al. (Shamshad et al., [2023](#bib.bib149)) | Computer Vision-medical
    | • A review of a number of transformer models with a focus on some tasks related
    to medical images and different image modalities, and a description of the datasets
    used for these tasks | • This paper compares deep learning models starting with
    CNNs and moving up to vision transformers. In this paper, medical image modalities
    and several medical computer vision tasks are discussed to compare papers through
    the specification of datasets used and also provide an overview of models’ performance.
    In this paper, the comparison is based solely on medical images; medical signals
    are not considered |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| F Shamshad 等（Shamshad 等, [2023](#bib.bib149)） | 医学计算机视觉 | • 回顾了若干变换器模型，重点关注一些与医学图像和不同图像模态相关的任务，并描述了这些任务使用的数据集
    | • 本文比较了从 CNNs 到视觉变换器的深度学习模型。本文讨论了医学图像模态和若干医学计算机视觉任务，通过数据集的具体描述比较论文，并提供了模型性能的概述。本文的比较仅基于医学图像，不考虑医学信号
    |'
- en: '| Salman Khan et al. (Khan et al., [2022](#bib.bib79)) | Computer Vision |
    • A overview of existing transformer computer vision models and classified the
    models based on popular tasks | • While this existing survey paper compared the
    popular techniques in terms of architectural design and experimental value, while
    our survey worked based on popular tasks and applications. • In the computer vision
    section, we put a special focus on Medical image tasks besides natural image processing.
    • This survey focused on computer vision tasks only, while we covered other four
    fields of applications, namely NLP, Multi-modal, Audio/speech, and signal processing
    besides computer vision |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Salman Khan 等（Khan 等, [2022](#bib.bib79)） | 计算机视觉 | • 对现有变换器计算机视觉模型进行了概述，并根据流行任务对模型进行了分类
    | • 尽管该现有调查论文在建筑设计和实验价值方面比较了流行技术，但我们的调查是基于流行任务和应用进行的 | • 在计算机视觉部分，我们特别关注医学图像任务，而不仅仅是自然图像处理
    | • 该调查仅关注计算机视觉任务，而我们涵盖了除计算机视觉外的其他四个应用领域，即NLP、多模态、音频/语音和信号处理 |'
- en: '| L. Ruan et al. (Ruan & Jin, [2022](#bib.bib144)) | Multi-modal(NLP-CV) |
    • Categorize transformer vision-language models based on tasks and summarize downstream
    tasks and commonly used video dataset | • This existing survey focused on multi-modal(NLP-CV)
    tasks only, while we covered other four fields of applications-NLP, Computer vision,
    Audio/speech, and signal processing besides multi-modal |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| L. Ruan 等（Ruan & Jin, [2022](#bib.bib144)） | 多模态（NLP-CV） | • 基于任务对变换器视觉语言模型进行分类，并总结下游任务和常用的视频数据集
    | • 该现有调查仅关注多模态（NLP-CV）任务，而我们则涵盖了除多模态外的其他四个应用领域——NLP、计算机视觉、音频/语音和信号处理 |'
- en: '| A Shin et al. (Shin et al., [2022](#bib.bib156)) | Multi-modal (Performance
    /Architecture) | • They survey transformers for multi-modal tasks, which we do
    also include in our different application tasks | • Cover only one application
    area in detail, which is multimodal visual-linguistic tasks |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| A Shin 等（Shin 等, [2022](#bib.bib156)） | 多模态（性能/架构） | • 他们调查了用于多模态任务的变换器，这也是我们在不同应用任务中涵盖的内容
    | • 详细讨论了一个应用领域，即多模态视觉语言任务 |'
- en: '|  |  |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: 'After having a thorough search and analysis of these survey papers, we realized
    that still, a survey on transformers is missing which focused on a most common
    field of application together and discussed the contribution of transformer-based
    models in the execution of different deep learning tasks in regarding fields of
    application. In this paper, first, we surveyed all transformer-based models out
    there based on our best possible search, identified top-$5$ fields of application
    and the proportion of transformers models’ contribution in the progression of
    top fields of application: Natural Language Processing (NLP), Computer Vision
    (CV), Multi-modal, Audio and Speech, and Signal processing. Moreover, we proposed
    a taxonomy of transformer models based on these top five fields of application
    whereas top performed, and significant models are being classified and analyzed
    based on their task’s execution under the regarding fields. Through this survey,
    different aspects of Transformer-based models’ tasks and applications become more
    explicit in different fields, and it also depicted the fields of transformer applications
    that got higher and less attention by the researchers so far. Based on this analysis,
    we discussed future prospects and possibilities of transformers application in
    different fields of application. One of the objectives of this survey is to make
    a combined source of reference for a better understanding of the contribution
    of transformer models in different fields and the characteristics and execution
    methods of the models which kept significant contributions to improving the performance
    of the different tasks in their fields. Besides, this paper would be a resource
    to perceive future possibilities and scope of transformer-based models’ application
    for enthusiastic researchers who wants to extend and work for the new application
    of transformers.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这些调查论文进行彻底的搜索和分析后，我们意识到，仍然缺乏一项专注于**变压器**在最常见应用领域的调查，并讨论了基于变压器的模型在不同深度学习任务中的贡献。在本文中，我们首先基于我们最好的搜索，对所有现有的基于变压器的模型进行了调查，确定了前$5$个应用领域以及变压器模型在这些领域的贡献比例：自然语言处理（NLP）、计算机视觉（CV）、多模态、音频与语音，以及信号处理。此外，我们根据这五个主要应用领域提出了一个变压器模型的分类法，其中对表现优异和重要的模型进行了分类和分析，基于它们在相关领域任务的执行情况。通过这项调查，变压器模型在不同领域的任务和应用变得更加明确，同时也描述了变压器应用领域中获得更多或较少关注的情况。基于这项分析，我们讨论了未来变压器在不同应用领域的前景和可能性。这项调查的一个目标是为更好地理解变压器模型在不同领域的贡献以及那些对提升各领域任务性能有显著贡献的模型的特征和执行方法，提供一个综合的参考来源。此外，本文将成为有志研究者了解变压器模型未来应用可能性和范围的资源，他们希望扩展并研究变压器的新应用。
- en: 5 TRANSFORMER APPLICATIONS
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个变压器应用
- en: Since 2017, the transformer model has emerged as a highly attractive research
    model in the field of deep learning. Originally developed for processing long-range
    textual sentences. However, its scope has expanded to a variety of applications
    beyond NLP tasks. In fact, after a series of successes in NLP, researchers turned
    their attention to computer vision, exploring the potential of transformer models’
    global attention capability, while Convolutional Neural Networks (CNNs) were adept
    at tracking local features. The transformer model has also been tested and applied
    in various other fields and for various tasks. To gain a deeper understanding
    of transformer applications, we conducted a comprehensive search of various research
    libraries and reviewed the transformer models available from $2017$ to the present
    day. Our search yielded approximately more than $650$ transformer-based models
    that are being applied in various fields.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年以来，变压器模型已成为深度学习领域中一个极具吸引力的研究模型。最初是为处理长距离文本句子而开发的。然而，它的应用范围已扩展到超越NLP任务的各种应用。实际上，在NLP领域取得一系列成功后，研究人员将注意力转向计算机视觉，探索变压器模型的全局注意力能力的潜力，而卷积神经网络（CNNs）则擅长于跟踪局部特征。变压器模型也在其他各种领域和任务中进行了测试和应用。为了更深入地了解变压器的应用，我们对各种研究库进行了全面搜索，并审查了自$2017$年至今的变压器模型。我们的搜索结果显示，约有$650$多个基于变压器的模型正在各种领域中应用。
- en: We identified the major fields in which transformer models are being used, including
    NLP, CV, Multi-modal applications, Audio and Speech processing, and Signal Processing.
    Our analysis provides an overview of the transformer models available in each
    field, their applications, and their impact on their respective industries.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别了变压器模型被使用的主要领域，包括自然语言处理、计算机视觉、多模态应用、音频和语音处理以及信号处理。我们的分析提供了每个领域中可用变压器模型的概述、它们的应用以及对各自行业的影响。
- en: '![Refer to caption](img/6f3da053fcd98340a2accfabd6a7d849.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f3da053fcd98340a2accfabd6a7d849.png)'
- en: 'Figure 4: Proportion of transformer application in Top-5 fields'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：变压器应用在前五大领域中的比例
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5 TRANSFORMER APPLICATIONS ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks") shows the percentage
    breakdown of the transformer models proposed so far across different application
    fields. Our analysis revealed approximately $250$ transformer-based models for
    NLP, representing around $40\%$ of the total transformer models collected. Moreover,
    we accounted for approximately $200$ models for computer vision. Due to the different
    processing of natural and medical images, and the extensive growth of both fields,
    we segmented computer vision into two categories: (i) Natural Image Processing
    and (ii) Medical Image Processing. As per this categorization, Natural Image Processing
    accounted for $20\%$ of transformer-based models, medical image processing accounted
    for $11\%$, and combinedly they accounted for $31\%$ of transformer-based models.
    Additionally, our analysis identified approximately $90$ transformer models for
    multi-modal applications, representing $15\%$ of the total, and around $70$ models
    for audio and speech processing, representing $11\%$ of total transformer models.
    Finally, only $4\%$ of transformer models were recorded for signal processing.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S5.F4 "图 4 ‣ 5 变压器应用 ‣ 深度学习任务变压器应用综合调查") 显示了迄今为止在不同应用领域中提出的变压器模型的百分比细分。我们的分析揭示了约$250$个用于自然语言处理的变压器模型，占总收集的变压器模型的约$40\%$。此外，我们还统计了约$200$个计算机视觉模型。由于自然和医学图像的处理方式不同以及这两个领域的广泛增长，我们将计算机视觉分为两个类别：（i）自然图像处理和（ii）医学图像处理。根据这一分类，自然图像处理占变压器模型的$20\%$，医学图像处理占$11\%$，二者合计占变压器模型的$31\%$。此外，我们的分析识别出约$90$个用于多模态应用的变压器模型，占总数的$15\%$，以及约$70$个用于音频和语音处理的模型，占总变压器模型的$11\%$。最后，仅有$4\%$的变压器模型记录在信号处理领域。
- en: Our analysis provides a clear understanding of the proportion of attention received
    by transformer applications in each field, facilitating the identification of
    further research areas and tasks where transformer models can be implemented.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析清晰地展示了各领域对变压器应用的关注比例，方便识别进一步的研究领域和可以实施变压器模型的任务。
- en: 6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于应用的变压器分类法
- en: 'As a result of conducting a thorough comprehensive analysis of all selected
    articles following the methodology explained in Section [3](#S3 "3 Research Methodology
    ‣ A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"),
    we noticed that the existing categorizations did not fully capture the wide range
    of transformer-based models and their diverse applications across different fields.
    Hence, in this study, we aimed to propose a more comprehensive taxonomy of transformers
    that would reflect their practical applications. To achieve this, we carefully
    reviewed a large number of transformer models and classified them based on their
    tasks within their respective fields of application. Our analysis identified several
    highly impactful and significant transformer-based models that have been successfully
    applied in a variety of fields. We then organized these models into five different
    application areas: Natural Language Processing (NLP), Computer Vision, Multi-modality,
    Audio and Speech, and Signal Processing. The proposed taxonomy in Figure [5](#S6.F5
    "Figure 5 ‣ 6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks") provides a more
    nuanced and comprehensive framework for understanding the diverse applications
    of transformers. We believe that this taxonomy would be beneficial for researchers
    and practitioners working on transformer-based models, as it would help them to
    identify the most relevant models and techniques for their specific applications.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对所有选定文章进行全面分析，根据第[3](#S3 "3 研究方法 ‣ 变换器在深度学习任务中的应用全面调查")节中解释的方法，我们注意到现有的分类没有完全涵盖基于变换器的模型及其在不同领域的多样化应用。因此，本研究旨在提出一个更全面的变换器分类法，以反映其实际应用。为实现这一目标，我们仔细审查了大量变换器模型，并根据其在各自应用领域中的任务进行分类。我们的分析识别出了一些在多个领域成功应用的具有重大影响和意义的基于变换器的模型。然后，我们将这些模型组织到五个不同的应用领域中：自然语言处理（NLP）、计算机视觉、多模态、音频与语音以及信号处理。图[5](#S6.F5
    "图 5 ‣ 基于应用的变换器分类法 ‣ 变换器在深度学习任务中的应用全面调查")中提出的分类法提供了一个更细致和全面的框架，用于理解变换器的多样化应用。我们相信这一分类法将对研究变换器模型的研究人员和从业者有所帮助，因为它将帮助他们识别出最相关的模型和技术。
- en: '![Refer to caption](img/524a265256315887ce2ac9b6ef9eb727.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/524a265256315887ce2ac9b6ef9eb727.png)'
- en: 'Figure 5: Application-based taxonomy of transformer models'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于应用的变换器模型分类法
- en: 6.1 Natural Language Processing (NLP)
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 自然语言处理（NLP）
- en: Transformers have become a vital tool in NLP, and various NLP tasks have largely
    benefited from these models. Our proposed taxonomy focuses on NLP and organizes
    transformer models into seven popular NLP tasks, including Translation, Summarization,
    Classification and Segmentation, Question Answering, Text Generation, Natural
    Language Reasoning, and Automated Symbolic Reasoning. To ensure a comprehensive
    analysis, we only considered transformer models that have significantly impacted
    the NLP field and improved its performance. Our analysis included an in-depth
    discussion of each NLP task, along with essential information about each model
    presented in Table LABEL:tab:language_Translation. We also highlighted the significance
    and working methods of each model. This taxonomy provides a valuable framework
    for understanding the different transformer models used in NLP and their practical
    applications. It can help researchers and practitioners select the most appropriate
    transformer model for their specific NLP task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器已成为自然语言处理中的重要工具，许多NLP任务都从这些模型中获益匪浅。我们提出的分类法专注于NLP，将变换器模型组织为七种流行的NLP任务，包括翻译、摘要、分类与分割、问答、文本生成、自然语言推理和自动符号推理。为了确保全面分析，我们仅考虑了那些对NLP领域产生重大影响并提高其性能的变换器模型。我们的分析包括对每个NLP任务的深入讨论，以及表格LABEL:tab:language_Translation中展示的每个模型的关键信息。我们还突出说明了每个模型的意义和工作方法。这种分类法提供了一个有价值的框架，用于理解NLP中不同的变换器模型及其实际应用。它可以帮助研究人员和从业者选择最适合其特定NLP任务的变换器模型。
- en: 6.1.1 Language Translation
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 语言翻译
- en: Language translation is a fundamental task in NLP, aimed at converting input
    text from one language to another. Its primary objective is to produce an output
    text that accurately reflects the meaning of the source text in the desired language.
    For example, given an English sentence as input text, the task aims to produce
    its equivalent in French or any other desired language. The original transformer
    model was developed explicitly for the purpose of language translation, highlighting
    the significance of this task in the NLP field. Table LABEL:tab:language_Translation
    identifies the transformer-based models that have demonstrated significant performance
    in the Language Translation task. These models play a vital role in facilitating
    effective communication and collaboration across different languages, enabling
    more efficient information exchange and knowledge sharing. Overall, the language
    translation task represents a crucial area of research in NLP, with significant
    implications for diverse applications, including business, science, education,
    and social interactions. The transformer-based models presented in the table offer
    promising solutions for advancing the state-of-the-art in this field, paving the
    way for new and innovative approaches to language translation (Chowdhary & Chowdhary,
    [2020](#bib.bib25), Monroe, [2017](#bib.bib113), Hirschberg & Manning, [2015](#bib.bib59)).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 语言翻译是自然语言处理中的一个基本任务，旨在将输入文本从一种语言转换为另一种语言。其主要目标是生成一个准确反映源文本含义的目标语言输出文本。例如，给定一个英语句子作为输入文本，该任务旨在生成其法语或其他所需语言的等效句子。原始Transformer模型专门为语言翻译目的而开发，突显了这一任务在自然语言处理领域的重要性。表LABEL:tab:language_Translation列出了在语言翻译任务中表现显著的基于Transformer的模型。这些模型在促进跨语言的有效沟通与合作中发挥了至关重要的作用，能够更高效地进行信息交流和知识共享。总体而言，语言翻译任务代表了自然语言处理领域的一个关键研究方向，对商业、科学、教育和社会互动等多个应用领域具有重要影响。表中呈现的基于Transformer的模型为推动该领域的最先进技术提供了有希望的解决方案，为语言翻译的新颖和创新方法铺平了道路（Chowdhary
    & Chowdhary，[2020](#bib.bib25)，Monroe，[2017](#bib.bib113)，Hirschberg & Manning，[2015](#bib.bib59)）。
- en: 'Table 3: Transformer models for NLP - language translation task'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：用于自然语言处理的Transformer模型 - 语言翻译任务
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Transformer模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调，训练，测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Transformer-2017 (Vaswani et al., [2017](#bib.bib172)) | Language Translation
    | 2017 | Encoder & Decoder | No | NA | WMT 2014 English-German,WMT 2014 English-French
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Transformer-2017 (Vaswani 等，[2017](#bib.bib172)) | 语言翻译 | 2017 | 编码器 & 解码器
    | 否 | NA | WMT 2014 英语-德语，WMT 2014 英语-法语 |'
- en: '| XLM (Conneau & Lample, [2019](#bib.bib30)) | Translation and Classification
    for multiple language | 2019 | Encoder & Decoder | Yes | WMT’16, WMT’14 English-French,
    WMT’16 (English-German, English-Romanian, Romanian-English) | Wikipedia of 16
    XNLI languages(English, French, Spanish, Russian, Arabic, Chinese, Hindi, German,
    Greek, Bulgarian, Turkish, Vietnamese, Thai, Urdu, Swahili, Japanese) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| XLM (Conneau & Lample，[2019](#bib.bib30)) | 多语言翻译与分类 | 2019 | 编码器 & 解码器 |
    是 | WMT’16，WMT’14 英语-法语，WMT’16（英语-德语，英语-罗马尼亚语，罗马尼亚语-英语） | 16种XNLI语言的维基百科（英语，法语，西班牙语，俄语，阿拉伯语，中文，印地语，德语，希腊语，保加利亚语，土耳其语，越南语，泰语，乌尔都语，斯瓦希里语，日语）
    |'
- en: '| BART (Lewis et al., [2020](#bib.bib87)) | Language Translation, Sentence
    Reconstruction, Comprehension, text Generation | 2019 | Encoder & Decoder | Yes
    | Corrupting documents, 1M steps on a combination of books and Wikipedia data,
    news, stories, and web text (Training) | SQuAD, MNLI, ELI5, XSum, ConvAI2, CNN/DM,
    CNN/DailyMail, WMT16 Romanian-English, augmented with back-translation data from
    Sennrich et al. (2016). |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| BART (Lewis 等，[2020](#bib.bib87)) | 语言翻译，句子重构，理解，文本生成 | 2019 | 编码器 & 解码器
    | 是 | 损坏文档，1M步结合书籍和维基百科数据、新闻、故事和网络文本（训练） | SQuAD，MNLI，ELI5，XSum，ConvAI2，CNN/DM，CNN/DailyMail，WMT16
    罗马尼亚语-英语，增补了Sennrich 等（2016）的回译数据。 |'
- en: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | Language understanding
    task- Translation, question answering, Classification, and so on. | 2021 | Encoder
    & Decoder | Yes | C4(Colossal Clean Crawled Corpus) | GLUE and SuperGLUE benchmarks,
    CNNDM, BBC XSum, and SQuAD data sets, ARC Reasoning Challenge,3 closed-book question
    answering data sets (Natural Questions, Web Questions, and Trivia QA), Winogrande
    Schema Challenge, Adversarial NLI Benchmark |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Switch Transformer (Fedus 等, [2021](#bib.bib37)) | 语言理解任务 - 翻译、问答、分类等。 |
    2021 | 编码器与解码器 | 是 | C4（巨型清洁抓取语料库） | GLUE 和 SuperGLUE 基准测试，CNNDM，BBC XSum 和 SQuAD
    数据集，ARC 推理挑战，3 个封闭书籍问答数据集（自然问题、网页问题和Trivia QA），Winogrande 语料库挑战，对抗性 NLI 基准测试 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformer: In 2017, Vaswani et al. (Vaswani et al., [2017](#bib.bib172))
    introduced the first transformer model, which has since revolutionized the field
    of NLP. The transformer model was designed specifically for language translation
    and is known as the Vanilla transformer model. Unlike its predecessors, the transformer
    model incorporates both an encoder and a decoder module, employing multi-head
    attention and masked-multi-head attention mechanisms. The encoder module is responsible
    for analyzing the contextual information of the input language, while the decoder
    module generates the output in the target language, using the output of the encoder
    and masked multi-head attention. The transformer model’s success is largely attributed
    to its ability to perform parallel computations, which allows it to process words
    simultaneously with positional information. This feature makes it highly efficient
    in processing large volumes of text and enables it to handle long-range dependencies,
    which are crucial in language translation.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer：2017 年，Vaswani 等（Vaswani 等, [2017](#bib.bib172)）首次介绍了 Transformer
    模型，这一模型自此彻底改变了 NLP 领域。Transformer 模型专门为语言翻译设计，被称为**Vanilla Transformer 模型**。与其前身不同，Transformer
    模型结合了编码器和解码器模块，采用了**多头注意力**和**掩蔽多头注意力**机制。编码器模块负责分析输入语言的上下文信息，而解码器模块则利用编码器的输出和掩蔽多头注意力生成目标语言的输出。Transformer
    模型的成功在很大程度上归功于其进行并行计算的能力，这使得它可以同时处理带有位置编码的信息。这一特性使其在处理大量文本时非常高效，并能够处理在语言翻译中至关重要的长距离依赖。
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'XLM: It is a cross-lingual language pretraining model developed to support
    multiple languages. The model is built using two methods: a supervised method
    and an unsupervised method. The unsupervised method utilizes Masked Language Modeling
    (MLM) and Casual Language Modeling (CLM) techniques and has shown remarkable effectiveness
    in translation tasks. On the other hand, the supervised method has further improved
    the translation tasks (Conneau & Lample, [2019](#bib.bib30)). This combination
    of supervised and unsupervised learning has made the XLM model a powerful tool
    for cross-lingual applications, making it possible to perform natural language
    processing tasks in multiple languages. The effectiveness of the XLM model in
    translation tasks has made it a popular choice among researchers in the field
    of natural language processing.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XLM：这是一种跨语言预训练模型，旨在支持多种语言。该模型使用两种方法构建：监督方法和无监督方法。无监督方法利用了**掩蔽语言建模**（MLM）和**因果语言建模**（CLM）技术，在翻译任务中表现出显著的有效性。另一方面，监督方法进一步改善了翻译任务（Conneau
    & Lample, [2019](#bib.bib30)）。这种监督与无监督学习的结合，使 XLM 模型成为一个强大的跨语言应用工具，使得在多种语言中执行自然语言处理任务成为可能。XLM
    模型在翻译任务中的有效性使其成为自然语言处理领域研究者的热门选择。
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BART: BART (Bidirectional and Auto-Regressive Transformers) is an advanced
    pre-trained model primarily aimed at cleaning up the corrupt text. It features
    two pre-training stages: the first stage corrupts the text with noise, while the
    second stage focuses on recovering the original text from the noisy version. BART
    employs a transformer translation model that integrates both the encoder and decoder
    modules, allowing it to perform various tasks such as text generation, translation,
    and comprehension with impressive accuracy (Lewis et al., [2020](#bib.bib87)).
    Its bi-directional approach enables it to learn from the past and future tokens,
    while its auto-regressive properties make it suitable for generating output tokens
    sequentially. These features make BART an incredibly versatile model for various
    natural language processing tasks.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BART：BART（双向自回归变换器）是一个先进的预训练模型，主要用于清理损坏的文本。它具有两个预训练阶段：第一个阶段用噪声破坏文本，第二个阶段专注于从噪声版本中恢复原始文本。BART
    使用一个集成了编码器和解码器模块的变换器翻译模型，使其能够以令人印象深刻的准确度执行文本生成、翻译和理解等各种任务（Lewis et al., [2020](#bib.bib87)）。它的双向方法使其能够从过去和未来的标记中学习，而其自回归特性使其适合顺序生成输出标记。这些特性使
    BART 成为一个极其通用的自然语言处理任务模型。
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Switch Transformer: The Switch transformer model is a recent development in
    the field of NLP that has gained attention for its ability to perform various
    tasks with high accuracy. It incorporates two key components: a permutation-based
    routing mechanism and a gating mechanism. The permutation-based routing mechanism
    allows the model to learn a routing strategy that selects which parts of the input
    sequence to attend to. This enables the model to handle variable-length inputs,
    as it can dynamically determine which parts of the sequence to attend to for each
    input. The gating mechanism allows the model to perform both Classification and
    Segmentation tasks. The gating mechanism is designed to learn how to combine information
    from different parts of the input sequence in order to make predictions. This
    allows the model to perform Classification tasks by predicting a label for the
    entire input sequence, or Segmentation tasks by predicting labels for each part
    of the input sequence. The Switch transformer is a highly versatile model that
    can effectively perform both Classification and Segmentation tasks (Bogatinovski
    et al., [2022](#bib.bib10)). Its detailed description can be found in the dedicated
    Classification & Segmentation section below (Fedus et al., [2021](#bib.bib37)).'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Switch Transformer：Switch transformer 模型是 NLP 领域的最新发展，以其高精度执行各种任务而受到关注。它包含两个关键组件：基于排列的路由机制和门控机制。基于排列的路由机制允许模型学习一种路由策略，以选择输入序列中的哪些部分进行关注。这使得模型能够处理可变长度的输入，因为它可以动态确定每个输入的序列中需要关注的部分。门控机制使模型能够同时执行分类和分割任务。门控机制旨在学习如何结合输入序列不同部分的信息以进行预测。这使得模型能够通过预测整个输入序列的标签来执行分类任务，或通过预测输入序列每个部分的标签来执行分割任务。Switch
    transformer 是一个高度通用的模型，能够有效地执行分类和分割任务（Bogatinovski et al., [2022](#bib.bib10)）。其详细描述可以在下面的分类与分割部分找到（Fedus
    et al., [2021](#bib.bib37)）。
- en: 6.1.2 Classification & Segmentation
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 分类与分割
- en: Text classification and segmentation are fundamental tasks in natural language
    processing (NLP) that enable the automatic organization and analysis of large
    volumes of textual data. Text classification involves assigning tags or labels
    to text based on its contents, such as sentiment, topic, or intent, among others.
    This process helps to categorize textual documents from different sources and
    can be useful in a variety of applications, such as recommendation systems, information
    retrieval, and content filtering. On the other hand, text segmentation involves
    dividing the text into meaningful units, such as sentences, words, or topics,
    to facilitate further analysis or processing. This task is crucial for various
    NLP applications, including language understanding, summarization, and question
    answering, among others (Chowdhary & Chowdhary, [2020](#bib.bib25), Kuhn, [2014](#bib.bib82),
    Hu et al., [2016](#bib.bib64)).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类和分割是自然语言处理（NLP）中的基本任务，能够实现对大量文本数据的自动组织和分析。文本分类涉及根据文本内容（如情感、主题或意图等）分配标签或标记。这一过程有助于对来自不同来源的文本文件进行分类，并在推荐系统、信息检索和内容过滤等多种应用中发挥作用。另一方面，文本分割涉及将文本划分为有意义的单元，如句子、单词或主题，以便进一步分析或处理。这项任务对于语言理解、摘要和问答等各种NLP应用至关重要（Chowdhary
    & Chowdhary，[2020](#bib.bib25)，Kuhn，[2014](#bib.bib82)，Hu等，[2016](#bib.bib64)）。
- en: Transformer-based models have been shown to achieve state-of-the-art performance
    in text classification and segmentation tasks. These models are characterized
    by their ability to capture long-range dependencies and contextual information
    in text, making them well-suited for complex NLP tasks. Table LABEL:tab:classification_&_segmentation
    highlights some of the most prominent transformer-based models that have demonstrated
    significant performance in text classification and segmentation tasks.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的模型已被证明在文本分类和分割任务中取得了**最先进**的表现。这些模型的特点是能够捕捉文本中的长程依赖关系和上下文信息，使其非常适合复杂的自然语言处理任务。表格LABEL:tab:classification_&_segmentation突出了在文本分类和分割任务中表现突出的几种Transformer模型。
- en: 'Table 4: Transformer models for NLP - language classification & segmentation
    tasks'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：用于NLP的Transformer模型 - 语言分类与分割任务
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成的任务 | 年份 | 架构（编码器/ 解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT & variants (Radford et al., [2018](#bib.bib132); [2019](#bib.bib133),
    Brown et al., [2020](#bib.bib12)) | Text classification, Question answering, textual
    entailment, semantic similarity | 2018 | Decoder | Yes | Book corpus | SNLI, MNLI,
    QNLI, SciTail, RTE, RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| GPT及其变体（Radford等，[2018](#bib.bib132); [2019](#bib.bib133)，Brown等，[2020](#bib.bib12)）
    | 文本分类、问答、文本蕴涵、语义相似性 | 2018 | 解码器 | 是 | 书籍语料库 | SNLI, MNLI, QNLI, SciTail, RTE,
    RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
- en: '| XLM (Conneau & Lample, [2019](#bib.bib30)) | Translation and Classification
    for multiple language | 2019 | Encoder & Decoder | Yes | WMT’16, WMT’14 English-French,
    WMT’16 (English-German, English-Romanian, Romanian-English) | Wikipedia of 16
    XNLI languages(English, French, Spanish, Russian, Arabic, Chinese, Hindi, German,
    Greek, Bulgarian, Turkish, Vietnamese, Thai, Urdu, Swahili, Japanese) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| XLM（Conneau & Lample，[2019](#bib.bib30)） | 多语言翻译和分类 | 2019 | 编码器 & 解码器 |
    是 | WMT’16, WMT’14 英语-法语，WMT’16（英语-德语，英语-罗马尼亚语，罗马尼亚语-英语） | 16种XNLI语言的维基百科（英语、法语、西班牙语、俄语、阿拉伯语、中文、印地语、德语、希腊语、保加利亚语、土耳其语、越南语、泰语、乌尔都语、斯瓦希里语、日语）
    |'
- en: '| T5 (Raffel et al., [2020](#bib.bib134)) | Text summarization, Question answering,
    text classification | 2020 | Encoder & Decoder | Yes | C4 (Colossal Clean Crawled
    Corpus) | GLUE and SuperGLUE benchmarks, CNN/Daily Mail abstractive summarization,
    SQuAD question answering, WMT English to German, French, and Romanian translation
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| T5（Raffel等，[2020](#bib.bib134)） | 文本摘要、问答、文本分类 | 2020 | 编码器 & 解码器 | 是 | C4（大规模清洗爬取语料库）
    | GLUE和SuperGLUE基准，CNN/每日邮件摘要，SQuAD问答，WMT英语到德语、法语和罗马尼亚语翻译 |'
- en: '| Charformer (Tay et al., [2022](#bib.bib170)) | Classification task, toxicity
    detection, and so on. | 2022 | Encoder & Decoder | Yes | The same datasets used
    in T5 model-C4(Colossal Clean Crawled Corpus) | GLUE IMDb, AGNews,(Maas et al.,
    2011), (Zhang et al., 2015), Civil Comments, Wikipedia Comments, TyDiQA-GoldP,
    XQuAD, MLQA, XNLI, PAWS-X.. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Charformer (Tay et al., [2022](#bib.bib170)) | 分类任务、毒性检测等。 | 2022 | 编码器 &
    解码器 | 是 | 使用与 T5 模型相同的数据集-C4（庞大的清理抓取语料库） | GLUE IMDb、AGNews、（Maas et al., 2011）、（Zhang
    et al., 2015）、Civil Comments、Wikipedia Comments、TyDiQA-GoldP、XQuAD、MLQA、XNLI、PAWS-X..
    |'
- en: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | Language understanding
    task- Translation, question answering, Classification, and so on. | 2021 | Encoder
    & Decoder | Yes | C4(Colossal Clean Crawled Corpus) | GLUE and SuperGLUE benchmarks,
    CNNDM, BBC XSum and SQuAD data sets, ARC Reasoning Challenge,3 closed-book question
    answering data sets (Natural Questions, Web Questions, and Trivia QA), Winogrande
    Schema Challenge, Adversarial NLI Benchmark |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | 语言理解任务- 翻译、问答、分类等。
    | 2021 | 编码器 & 解码器 | 是 | C4（庞大的清理抓取语料库） | GLUE 和 SuperGLUE 基准，CNNDM、BBC XSum 和
    SQuAD 数据集，ARC 推理挑战，3 个封闭书籍问答数据集（Natural Questions、Web Questions 和 Trivia QA），Winogrande
    Schema Challenge，对抗性 NLI 基准 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Charformer: It is a transformer-based model that introduces Gradient-based
    subword tokenization (GBST), a lightweight approach to learning latent subwords
    directly from characters at the byte level. The model has both English and multi-lingual
    variants and has demonstrated outstanding performance on language understanding
    tasks, such as the classification of long text documents (Tay et al., [2022](#bib.bib170)).'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Charformer：这是一种基于变换器的模型，引入了基于梯度的子词分词（GBST），这是一种从字节级字符直接学习潜在子词的轻量级方法。该模型既有英语版本也有多语言版本，在语言理解任务上表现出色，例如长文本文档的分类（Tay
    et al., [2022](#bib.bib170)）。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Switch Transformer: The use of pre-trained models such as BERT and GPT, trained
    on large datasets, has gained popularity in the field of natural language processing.
    However, there are concerns about the economic and environmental costs of training
    such models. To address these concerns, the Switch transformer was introduced,
    which offers a larger model size without a significant increase in computational
    cost. The Switch transformer replaces the feed-forward neural network (FFN) with
    a switch layer that contains multiple FFNs, resulting in a model with trillions
    of parameters. Despite the increase in model size, the computational cost of the
    Switch transformer remains comparable to that of other models. In fact, the Switch
    transformer has been evaluated on 11 different tasks and has shown significant
    improvement in tasks such as translation, question-answering, classification,
    and summarization (Fedus et al., [2021](#bib.bib37)).'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Switch Transformer：使用预训练模型如 BERT 和 GPT，在大数据集上进行训练，在自然语言处理领域变得越来越受欢迎。然而，对于训练这些模型的经济和环境成本存在担忧。为了解决这些问题，Switch
    Transformer 被引入，它提供了更大的模型规模而不显著增加计算成本。Switch Transformer 用一个包含多个 FFN 的开关层替代了前馈神经网络（FFN），从而得到一个参数量达到万亿级的模型。尽管模型规模增加，Switch
    Transformer 的计算成本仍与其他模型相当。事实上，Switch Transformer 在 11 个不同任务上进行了评估，并在翻译、问答、分类和摘要等任务上显示出显著的改进（Fedus
    et al., [2021](#bib.bib37)）。
- en: 'GPT & Variants, XLM, T5: These models are versatile and capable of performing
    a range of NLP tasks, including but not limited to classification, segmentation,
    question answering, and language translation. Section [6.1.3](#S6.SS1.SSS3 "6.1.3
    Question Answering ‣ 6.1 Natural Language Processing (NLP) ‣ 6 APPLICATION-BASED
    CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive Survey on Applications
    of Transformers for Deep Learning Tasks") will provide detailed description.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT 及其变体、XLM、T5：这些模型功能多样，能够执行一系列自然语言处理（NLP）任务，包括但不限于分类、分割、问答和语言翻译。第[6.1.3](#S6.SS1.SSS3
    "6.1.3 Question Answering ‣ 6.1 Natural Language Processing (NLP) ‣ 6 APPLICATION-BASED
    CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive Survey on Applications
    of Transformers for Deep Learning Tasks")节将提供详细描述。
- en: 6.1.3 Question Answering
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 问答系统
- en: Question Answering is a classical NLP task. It involves matching a text query
    to the most relevant answer in the form of text, based on the relevance of the
    text to the query. This task is challenging, as finding a concise and accurate
    answer to a given query can be difficult (Chowdhary & Chowdhary, [2020](#bib.bib25),
    Hirschman & Gaizauskas, [2001](#bib.bib60)). Recent research has focused on this
    task, leading to the development of several transformer-based models that leverage
    deep learning techniques to improve the accuracy and efficiency of this task.
    A detailed overview of these models is provided in Table LABEL:tab:Question_Answering.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 问答是一个经典的NLP任务。它涉及根据文本与查询的相关性，将文本查询匹配到最相关的答案。这项任务具有挑战性，因为找到一个简洁而准确的答案可能很困难（Chowdhary
    & Chowdhary, [2020](#bib.bib25), Hirschman & Gaizauskas, [2001](#bib.bib60)）。近期的研究集中于此任务，导致了多个基于Transformer的模型的开发，这些模型利用深度学习技术来提高任务的准确性和效率。有关这些模型的详细概述见表LABEL:tab:Question_Answering。
- en: 'Table 5: Transformer models for NLP - question answering task'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：用于NLP的Transformer模型 - 问答任务
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成的任务 | 年份 | 架构（编码器/ 解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BERT (Devlin et al., [2019](#bib.bib33)) | Question answering, Sentence Prediction,
    language understanding | 2018 | Encoder | Yes | Book Corpus, English Wikipedia
    | SQuAD v1.1, SQuAD v2.0, SWAG, QNLI, MNLI |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| BERT (Devlin et al., [2019](#bib.bib33)) | 问答、句子预测、语言理解 | 2018 | 编码器 | 是
    | 书籍语料库，英文维基百科 | SQuAD v1.1, SQuAD v2.0, SWAG, QNLI, MNLI |'
- en: '| ELECTRA (Clark et al., [2020a](#bib.bib26)) | Language understanding tasks-
    Question answering and so on | 2020 | Encoder | Yes | Wikipedia, BooksCorpus,
    ClueWeb, CommonCrawl, Gigaword | SQuAD 1.1, SQuAD 2.0, GLUE |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| ELECTRA (Clark et al., [2020a](#bib.bib26)) | 语言理解任务- 问答等 | 2020 | 编码器 |
    是 | Wikipedia, BooksCorpus, ClueWeb, CommonCrawl, Gigaword | SQuAD 1.1, SQuAD
    2.0, GLUE |'
- en: '| GPT & variants (Radford et al., [2018](#bib.bib132); [2019](#bib.bib133),
    Brown et al., [2020](#bib.bib12)) | Text classification, Question answering, textual
    entailment, semantic similarity | 2018 | Decoder | Yes | Book corpus | SNLI, MNLI,
    QNLI, SciTail, RTE, RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| GPT及变体 (Radford et al., [2018](#bib.bib132); [2019](#bib.bib133), Brown et al.,
    [2020](#bib.bib12)) | 文本分类、问答、文本蕴含、语义相似性 | 2018 | 解码器 | 是 | 书籍语料库 | SNLI, MNLI,
    QNLI, SciTail, RTE, RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
- en: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | Language understanding
    task- Translation, question answering, Classification, and so on. | 2021 | Encoder
    & Decoder | Yes | C4(Colossal Clean Crawled Corpus) | GLUE and SuperGLUE benchmarks,
    CNNDM, BBC XSum, and SQuAD data sets, ARC Reasoning Challenge,3 closed-book question
    answering data sets (Natural Questions, Web Questions, and Trivia QA), Winogrande
    Schema Challenge, Adversarial NLI Benchmark |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | 语言理解任务- 翻译、问答、分类等。
    | 2021 | 编码器 & 解码器 | 是 | C4(Colossal Clean Crawled Corpus) | GLUE 和 SuperGLUE
    基准测试，CNNDM，BBC XSum 和 SQuAD 数据集，ARC 推理挑战，3 个封闭式问答数据集（自然问题，网络问题和 Trivia QA），Winogrande
    方案挑战，敌对 NLI 基准测试 |'
- en: '| T5 (Raffel et al., [2020](#bib.bib134)) | Text summarization, Question answering,
    text classification | 2020 | Encoder & Decoder | Yes | C4 (Colossal Clean Crawled
    Corpus) | GLUE and SuperGLUE benchmarks, CNN/Daily Mail abstractive summarization,
    SQuAD question answering, WMT English to German, French, and Romanian translation
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| T5 (Raffel et al., [2020](#bib.bib134)) | 文本摘要、问答、文本分类 | 2020 | 编码器 & 解码器
    | 是 | C4 (Colossal Clean Crawled Corpus) | GLUE 和 SuperGLUE 基准测试，CNN/Daily Mail
    抽象摘要，SQuAD 问答，WMT 英语到德语、法语和罗马尼亚语翻译 |'
- en: '| InstructGPT (Ouyang et al., [2022](#bib.bib121)) | Text Generation, Question
    Answering, summarization, and so on. | 2022 | Decoder | Yes | Based on the pre-training
    model GPT-3 | SFT dataset, RM dataset, PPO dataset, a dataset of prompts and completions
    Winogender, CrowS-Pairs, Real Toxicity Prompts, TruthfulQA, DROP, QuAC, SquadV2,
    Hellaswag, SST, RTE and WSC, WMT 15 Fr ! En, CNN/Daily Mail Summarization, Reddit
    TLDR Summarization datasets. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT (Ouyang et al., [2022](#bib.bib121)) | 文本生成、问答、摘要等。 | 2022 |
    解码器 | 是 | 基于预训练模型GPT-3 | SFT 数据集，RM 数据集，PPO 数据集，提示和完成数据集 Winogender，CrowS-Pairs，真实毒性提示，TruthfulQA，DROP，QuAC，SquadV2，Hellaswag，SST，RTE
    和 WSC，WMT 15 法语 ! 英语，CNN/Daily Mail 摘要，Reddit TLDR 摘要数据集。 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BERT & BERT variants: BERT is an acronym that stands for Bidirectional Encoder
    Representations of transformers. It was introduced by the Google AI team and is
    embedded within the encoder module of the transformer. BERT employs a bidirectional
    approach, allowing it to pre-train a transformer on unannotated text by considering
    the context of each word. As a result, BERT has achieved remarkable performance
    on various natural language processing (NLP) tasks (Devlin et al., [2019](#bib.bib33)).'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERT & BERT 变体：BERT 是变换器双向编码表示的缩写。它由 Google AI 团队提出，并嵌入在变换器的编码器模块中。BERT 采用双向方法，使其能够通过考虑每个词的上下文来对未标注的文本进行预训练。因此，BERT
    在各种自然语言处理（NLP）任务中取得了显著的性能（Devlin et al., [2019](#bib.bib33)）。
- en: A variety of BERT-based models have been developed with different characteristics.
    For instance, some are optimized for fast computation, while others produce superior
    results with a small number of parameters. Some are also tailored to specific
    tasks, such as RoBERTa, which is designed for masked language modeling and next
    sentence prediction (Liu et al., [2019](#bib.bib100)). FlueBERT is another model
    that can be used for tasks such as text classification, paraphrasing, natural
    language inference, parsing, and word sense disambiguation (Le et al., [2020](#bib.bib85)).
    Additionally, DistilBERT is suitable for question answering and other specific
    tasks. These models have significantly improved pre-trained transformer models
    (Sanh et al., [2019](#bib.bib146)).
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 各种基于 BERT 的模型已经开发出来，具有不同的特点。例如，有些模型针对快速计算进行了优化，而其他模型则在参数较少的情况下产生了更优的结果。有些模型还针对特定任务进行了调整，例如
    RoBERTa，旨在用于掩码语言建模和下一句预测（Liu et al., [2019](#bib.bib100)）。FlueBERT 是另一个可以用于文本分类、改写、自然语言推理、句法分析和词义消歧等任务的模型（Le
    et al., [2020](#bib.bib85)）。此外，DistilBERT 适用于问答和其他特定任务。这些模型显著改进了预训练的变换器模型（Sanh
    et al., [2019](#bib.bib146)）。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPT & GPT variants: Generative Pre-Trained Transformer (GPT) models are built
    exclusively on the decoder block of transformers, which significantly improves
    the progress of transformers in natural language processing. GPT adopts a semi-supervised
    approach to language comprehension, which involves unsupervised pre-training and
    supervised fine-tuning methods (Radford et al., [2018](#bib.bib132)). In 2019,
    following the success of the GPT model, a massively pre-trained transformer-based
    model called GPT-2 with 1.5 billion parameters was introduced, which significantly
    improved the pre-trained version of transformers (Radford et al., [2019](#bib.bib133)).
    Subsequently, in 2020, the largest pre-trained version of GPT with 175 billion
    parameters, called GPT-3, was released. This model is 10 times larger than the
    previous non-sparse language model. One of the most notable achievements of GPT-3
    is that it exhibits strong performance across a range of tasks without the need
    for gradient updates or fine-tuning, which is a requirement for pre-training models
    like BERT (Brown et al., [2020](#bib.bib12)).'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT & GPT 变体：生成式预训练变换器（GPT）模型完全基于变换器的解码器块，这大大提高了变换器在自然语言处理中的进展。GPT 采用了半监督的语言理解方法，涉及无监督的预训练和监督的微调方法（Radford
    et al., [2018](#bib.bib132)）。2019年，在 GPT 模型取得成功后，推出了一个名为 GPT-2 的大规模预训练变换器模型，具有
    15 亿个参数，显著改进了变换器的预训练版本（Radford et al., [2019](#bib.bib133)）。随后，在 2020 年，发布了最大预训练版本的
    GPT，具有 1750 亿个参数，称为 GPT-3。该模型比之前的非稀疏语言模型大 10 倍。GPT-3 最显著的成就之一是它在不需要梯度更新或微调的情况下，在各种任务中表现出强大的性能，这对像
    BERT 这样的预训练模型来说是必须的（Brown et al., [2020](#bib.bib12)）。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Electra: An acronym for “Efficiently Learning an Encoder that Classifies Token
    Replacements Accurately”, utilizes a distinct pre-training method compared to
    other pre-trained models. Electra deploys a ”Masked Language Modeling” approach
    that masks certain words and trains the model to predict them. Additionally, Electra
    incorporates a ”Discriminator” network that aids in comprehending language without
    the need to memorize the training data. This unique approach enables Electra to
    generate superior text and surpass the performance of BERT (Clark et al., [2020a](#bib.bib26)).'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Electra：是“Efficiently Learning an Encoder that Classifies Token Replacements
    Accurately”（高效学习一个准确分类词替换的编码器）的缩写，采用了一种与其他预训练模型不同的预训练方法。Electra 使用“掩码语言建模”方法，掩盖某些词并训练模型预测这些词。此外，Electra
    还引入了一个“鉴别器”网络，帮助理解语言，而不需要记住训练数据。这种独特的方法使 Electra 能够生成更优质的文本，超越了 BERT 的表现（Clark
    et al., [2020a](#bib.bib26)）。
- en: 'InstructGPT, T5 and Switch Transformer: While the InstructGPT model can generate
    text apart from question-answering tasks, the T5 is significant in test summarization
    tasks and Switch transformer models can perform classification and segmentation
    tasks as well. More descriptions of these models have been provided in Sections
    [6.1.1](#S6.SS1.SSS1 "6.1.1 Language Translation ‣ 6.1 Natural Language Processing
    (NLP) ‣ 6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks") and [6.1.2](#S6.SS1.SSS2
    "6.1.2 Classification & Segmentation ‣ 6.1 Natural Language Processing (NLP) ‣
    6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS ‣ A Comprehensive
    Survey on Applications of Transformers for Deep Learning Tasks").'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: InstructGPT、T5 和 Switch Transformer：虽然 InstructGPT 模型可以生成文本，除了问答任务外，T5 在测试摘要任务中具有重要意义，而
    Switch Transformer 模型也能执行分类和分割任务。更多关于这些模型的描述请参见 [6.1.1](#S6.SS1.SSS1 "6.1.1 语言翻译
    ‣ 6.1 自然语言处理（NLP） ‣ 6 TRANSFORMERS 的应用分类 ‣ 深度学习任务中的Transformer应用综合调查") 和 [6.1.2](#S6.SS1.SSS2
    "6.1.2 分类与分割 ‣ 6.1 自然语言处理（NLP） ‣ 6 TRANSFORMERS 的应用分类 ‣ 深度学习任务中的Transformer应用综合调查")。
- en: 6.1.4 Text Summarization
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 文本摘要
- en: Text summarization is a natural language processing task that involves breaking
    down lengthy texts into shorter versions while retaining essential and valuable
    information and preserving the meaning of the text. Text summarization is particularly
    useful in comprehending lengthy textual documents, and it also helps to reduce
    computational resources and time (Chowdhary & Chowdhary, [2020](#bib.bib25), Tas
    & Kiyani, [2007](#bib.bib168)). transformer-based models have shown exceptional
    performance in text summarization tasks. The transformer-based models in text
    summarization are listed in Table LABEL:tab:Summarization.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要是一项自然语言处理任务，涉及将冗长的文本分解成较短的版本，同时保留重要且有价值的信息，并保持文本的含义。文本摘要在理解冗长的文本文件时特别有用，它还帮助减少计算资源和时间（Chowdhary
    & Chowdhary, [2020](#bib.bib25)，Tas & Kiyani, [2007](#bib.bib168)）。基于transformer的模型在文本摘要任务中表现出色。文本摘要中的基于transformer的模型列在表
    LABEL:tab:Summarization 中。
- en: 'Table 6: Transformer models for NLP - text summarization task'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：NLP中的Transformer模型 - 文本摘要任务
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Transformer模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调，训练，测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT & variants (Radford et al., [2018](#bib.bib132); [2019](#bib.bib133),
    Brown et al., [2020](#bib.bib12)) | Text classification, Question answering, textual
    entailment, semantic similarity | 2018 | Decoder | Yes | Book corpus | SNLI, MNLI,
    QNLI, SciTail, RTE, RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| GPT及变体（Radford et al., [2018](#bib.bib132); [2019](#bib.bib133)，Brown et
    al., [2020](#bib.bib12)) | 文本分类、问答、文本蕴涵、语义相似性 | 2018 | 解码器 | 是 | 书籍语料库 | SNLI,
    MNLI, QNLI, SciTail, RTE, RACE, CNN, SQuaD, MRPC, QQP, STS-B, SST2 & CoLA |'
- en: '| PEGASUS (Zhang et al., [2020a](#bib.bib203)) | Text summarization | 2020
    | Encoder & Decoder | Yes | C4, HugeNews | XSum, CNN/DailyMail, NEWSROOM, Multi-News,
    Gigaword, arXiv, PubMed, BIGPATENT, WikiHow, Reddit TIFU, AESLC,BillSum |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| PEGASUS (Zhang et al., [2020a](#bib.bib203)) | 文本摘要 | 2020 | 编码器 & 解码器 |
    是 | C4, HugeNews | XSum, CNN/DailyMail, NEWSROOM, Multi-News, Gigaword, arXiv,
    PubMed, BIGPATENT, WikiHow, Reddit TIFU, AESLC, BillSum |'
- en: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | Language understanding
    task- Translation, question answering, Classification, and so on. | 2021 | Encoder
    & Decoder | Yes | C4(Colossal Clean Crawled Corpus) | GLUE and SuperGLUE benchmarks,
    CNNDM, BBC XSum, and SQuAD data sets, ARC Reasoning Challenge,3 closed-book question
    answering data sets (Natural Questions, Web Questions, and Trivia QA), Winogrande
    Schema Challenge, Adversarial NLI Benchmark |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Switch Transformer (Fedus et al., [2021](#bib.bib37)) | 语言理解任务 - 翻译、问答、分类等。
    | 2021 | 编码器 & 解码器 | 是 | C4（Colossal Clean Crawled Corpus） | GLUE和SuperGLUE基准测试，CNNDM，BBC
    XSum和SQuAD数据集，ARC推理挑战，3个闭卷问答数据集（Natural Questions、Web Questions和Trivia QA），Winogrande
    Schema挑战，Adversarial NLI基准 |'
- en: '| T5 (Raffel et al., [2020](#bib.bib134)) | Text summarization, Question answering,
    text classification | 2020 | Encoder & Decoder | Yes | C4 (Colossal Clean Crawled
    Corpus) | GLUE and SuperGLUE benchmarks, CNN/Daily Mail abstractive summarization,
    SQuAD question answering, WMT English to German, French, and Romanian translation
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| T5 (Raffel 等，[2020](#bib.bib134)) | 文本摘要、问答、文本分类 | 2020 | 编码器 & 解码器 | 是 |
    C4（Colossal Clean Crawled Corpus） | GLUE 和 SuperGLUE 基准，CNN/Daily Mail 抽象摘要，SQuAD
    问答，WMT 英语到德语、法语和罗马尼亚语翻译 |'
- en: '| InstructGPT (Ouyang et al., [2022](#bib.bib121)) | Text Generation, Question
    Answering, summarization, and so on. | 2022 | Decoder | Yes | Based on the pre-training
    model GPT-3 | SFT dataset, RM dataset, PPO dataset, a dataset of prompts and completions
    Winogender, CrowS-Pairs, Real Toxicity Prompts, TruthfulQA, DROP, QuAC, SquadV2,
    Hellaswag, SST, RTE, and WSC, WMT 15 Fr ! En, CNN/Daily Mail Summarization, Reddit
    TLDR Summarization datasets. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT (Ouyang 等，[2022](#bib.bib121)) | 文本生成、问答、摘要等 | 2022 | 解码器 | 是
    | 基于预训练模型 GPT-3 | SFT 数据集、RM 数据集、PPO 数据集、提示和完成数据集 Winogender、CrowS-Pairs、真实毒性提示、TruthfulQA、DROP、QuAC、SquadV2、Hellaswag、SST、RTE
    和 WSC、WMT 15 法语！ 英语、CNN/Daily Mail 摘要、Reddit TLDR 摘要数据集。 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PEGASUS: It is an exemplary model for generative text summarization that employs
    both the encoder and decoder modules of the transformer. While models based on
    masked language modeling only mask a small portion of text, PEGASUS masks entire
    multiple sentences, selecting the masked sentences based on their significance
    and importance, and generating them as the output. The model has exhibited significant
    performance on unknown summarization datasets (Zhang et al., [2020a](#bib.bib203)).'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PEGASUS：这是一个示范性的生成文本摘要模型，采用 Transformer 的编码器和解码器模块。虽然基于掩蔽语言建模的模型仅掩蔽少量文本，但 PEGASUS
    会掩蔽整个多个句子，根据它们的重要性选择掩蔽的句子，并生成作为输出。该模型在未知摘要数据集上表现出显著的性能（Zhang 等，[2020a](#bib.bib203)）。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'T5: The T5 transformer model, which stands for Text-To-Text Transfer Transformer,
    introduced a dataset named “Colossal Clean Crawled Corpus (C4)” that improved
    the performance in various downstream NLP tasks. T5 is a multi-task model that
    can be trained to perform a range of NLP tasks using the same set of parameters.
    Following pre-training, the model can be fine-tuned for different tasks and achieves
    comparable performance to several task-specific models (Raffel et al., [2020](#bib.bib134)).'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: T5：T5 Transformer 模型，代表文本到文本的迁移 Transformer，推出了一个名为“Colossal Clean Crawled Corpus
    (C4)”的数据集，提升了各种下游 NLP 任务的表现。T5 是一个多任务模型，可以通过相同的参数集训练以执行一系列 NLP 任务。经过预训练后，该模型可以针对不同任务进行微调，并取得与多个任务特定模型相当的性能（Raffel
    等，[2020](#bib.bib134)）。
- en: 'GPT & variants, InstructGPT and Switch Transformer: These models have been
    discussed in earlier sections. Moreover, apart from text summarization, certain
    models such as GPT and its variants can perform question-answering tasks, InstructGPT
    can generate text, and Charformer models are capable of classification and segmentation
    tasks as well.'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT 及其变体、InstructGPT 和 Switch Transformer：这些模型在早期部分中已讨论过。此外，除了文本摘要之外，某些模型如 GPT
    及其变体可以执行问答任务，InstructGPT 可以生成文本，Charformer 模型也能够进行分类和分割任务。
- en: 6.1.5 Text Generation
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.5 文本生成
- en: The task of text generation has gained immense popularity in the field of NLP
    due to its usefulness in generating long-form documentation, among other applications.
    Text generation models attempt to derive meaning from trained text data and create
    a connection between the text that has been previously outputted. These models
    typically operate on the basis of this connection (Chowdhary & Chowdhary, [2020](#bib.bib25),
    Reiter & Dale, [1997](#bib.bib136)). The use of transformer-based models has led
    to significant advancements in the task of text generation. Please refer to Table
    LABEL:tab:Text_Generation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成任务在 NLP 领域获得了极大的关注，因为它在生成长篇文档等应用中非常有用。文本生成模型试图从训练文本数据中推导意义，并创建与之前输出的文本之间的联系。这些模型通常基于这种联系进行操作（Chowdhary
    & Chowdhary，[2020](#bib.bib25)，Reiter & Dale，[1997](#bib.bib136)）。基于 Transformer
    的模型在文本生成任务中取得了显著的进展。请参见表 LABEL:tab:Text_Generation。
- en: 'Table 7: Transformer models for NLP - text generation task'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：用于 NLP 的 Transformer 模型 - 文本生成任务
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CTRL (Keskar et al., [2019](#bib.bib77)) | Text Generation | 2019 | Encoder
    & Decoder | Yes | Project Gutenberg, subreddits, News Data, Amazon Review, open
    WebText, WMT Translation date, question-answer pairs, MRQA | Multilingual Wikipedia
    and Open WebText. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CTRL (Keskar et al., [2019](#bib.bib77)) | 文本生成 | 2019 | 编码器 & 解码器 | 是 |
    Project Gutenberg, 子版块, 新闻数据, 亚马逊评论, 开放的 WebText, WMT 翻译数据, 问答对, MRQA | 多语言维基百科和开放的
    WebText。 |'
- en: '| BART (Lewis et al., [2020](#bib.bib87)) | Language Translation, Sentence
    Reconstruction, Comprehension, text Generation | 2019 | Encoder & Decoder | Yes
    | Corrupting documents, 1M steps on a combination of books and Wikipedia data,
    news, stories, and web text (Training) | SQuAD, MNLI, ELI5, XSum, ConvAI2, CNN/DM,
    CNN/DailyMail, WMT16 Romanian-English, augmented with back-translation data from
    Sennrich et al. (2016). |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| BART (Lewis et al., [2020](#bib.bib87)) | 语言翻译、句子重构、理解、文本生成 | 2019 | 编码器
    & 解码器 | 是 | 破损文档，基于书籍和维基百科数据的 1M 步骤，新闻、故事和网络文本（训练） | SQuAD, MNLI, ELI5, XSum,
    ConvAI2, CNN/DM, CNN/DailyMail, WMT16 罗马尼亚语-英语，增加了来自 Sennrich 等 (2016) 的回译数据。
    |'
- en: '| ProphetNET (Qi et al., [2020](#bib.bib129)) | Text Prediction | 2020 | Encoder
    & Decoder | Yes | Bookcorpus, English Wikipedia news, books, stories, and web
    text | CNN/dailymail, Giga-word Corpus, SQuAD dataset. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ProphetNET (Qi et al., [2020](#bib.bib129)) | 文本预测 | 2020 | 编码器 & 解码器 | 是
    | Bookcorpus, 英文维基百科新闻、书籍、故事和网络文本 | CNN/dailymail, Giga-word 语料库, SQuAD 数据集。 |'
- en: '| InstructGPT (Ouyang et al., [2022](#bib.bib121)) | Text Generation, Question
    Answering, summarization and so on. | 2022 | Decoder | Yes | Based on the pre-training
    model GPT-3 | SFT dataset, RM dataset, PPO dataset, dataset of prompts and completions
    Winogender, CrowS-Pairs, Real Toxicity Prompts , TruthfulQA , DROP , QuAC , SquadV2
    , Hellaswag , SST , RTE and WSC, WMT 15 Fr ! En, CNN/Daily Mail Summarization,
    Reddit TLDR Summarization datasets. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT (Ouyang et al., [2022](#bib.bib121)) | 文本生成、问答、摘要等 | 2022 | 解码器
    | 是 | 基于预训练模型 GPT-3 | SFT 数据集, RM 数据集, PPO 数据集, 提示和完成数据集 Winogender, CrowS-Pairs,
    真实毒性提示, TruthfulQA, DROP, QuAC, SquadV2, Hellaswag, SST, RTE 和 WSC, WMT 15 法语!
    英语, CNN/Daily Mail 摘要, Reddit TLDR 摘要数据集。'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CTRL: The acronym CTRL denotes the Conditional transformer Language model,
    which excels in generating realistic text resembling human language, contingent
    on a given condition. In addition, CTRL can produce text in multiple languages.
    This model is large-scale, boasting 1.63 billion parameters, and can be fine-tuned
    for various generative tasks, such as question answering and text summarization
    (Keskar et al., [2019](#bib.bib77)).'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CTRL: 缩写 CTRL 代表条件转换器语言模型，它在生成类似人类语言的真实文本方面表现出色，具体取决于给定的条件。此外，CTRL 可以生成多种语言的文本。该模型规模庞大，拥有
    16.3 亿个参数，可以针对各种生成任务进行微调，如问答和文本摘要 (Keskar et al., [2019](#bib.bib77))。'
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ProphetNET: ProphetNET is a sequence-to-sequence model that utilizes future
    n-gram prediction to facilitate text generation by predicting n-grams ahead. The
    model adheres to the transformer architecture, comprising encoder and decoder
    modules. It distinguishes itself by employing an n-stream self-attention mechanism.
    ProphetNET demonstrates remarkable performance in summarization and is also competent
    in question generation tasks (Qi et al., [2020](#bib.bib129)).'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ProphetNET: ProphetNET 是一个序列到序列模型，利用未来 n-gram 预测来促进文本生成，通过预测前面的 n-gram 来实现。该模型遵循转换器架构，包括编码器和解码器模块。它通过使用
    n-stream 自注意机制而有所不同。ProphetNET 在摘要生成方面表现出色，并且在问题生成任务中也很有能力 (Qi et al., [2020](#bib.bib129))。'
- en: •
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'InstructGPT: It was proposed as a solution to the problem of language generative
    models failing to produce realistic and truthful results. It achieves this by
    incorporating human feedback during fine-tuning and reinforcement learning from
    the feedback. The GPT-3 model was fine-tuned for this purpose. As a result, InstructGPT
    can generate more realistic and natural output that is useful in real-life applications.
    ChatGPT, which follows a similar methodology as InstructGPT, has gained significant
    attention in the field of NLP at the end of 2022 (Ouyang et al., [2022](#bib.bib121)).'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'InstructGPT: 这是为了解决语言生成模型无法产生现实且真实结果的问题而提出的解决方案。它通过在微调过程中结合人工反馈和从反馈中进行强化学习来实现这一点。GPT-3
    模型为此进行了微调。因此，InstructGPT 能够生成更加现实和自然的输出，这在实际应用中非常有用。ChatGPT 采用了与 InstructGPT 类似的方法，并在
    2022 年底（Ouyang 等， [2022](#bib.bib121)）在 NLP 领域引起了广泛关注。'
- en: 'BART: The BART model’s description is mentioned above in the language translation
    section. This model can execute the language translation task as well.'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BART: BART 模型的描述已在语言翻译部分提到。该模型也可以执行语言翻译任务。'
- en: 6.1.6 Natural Language Reasoning
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.6 自然语言推理
- en: The pursuit of natural language reasoning is a field of study that is distinct
    from that of question-answering. Question-answering focuses on finding the answer
    to a specific query within a given text passage. On the other hand, natural language
    reasoning involves the application of deductive reasoning to derive a conclusion
    from the given premises and rules that are represented in natural language. Neural
    network architectures aim to learn how to utilize these premises and rules to
    infer new conclusions. Previously, a similar task was traditionally tackled by
    systems equipped with the knowledge represented in a formal format and rules to
    be applied for the derivation of new knowledge. However, the use of formal representation
    has posed a significant challenge to this line of research (Mark A Musen, [1988](#bib.bib109)).
    With the advent of transformers and their remarkable performance in numerous NLP
    tasks, it is now possible to circumvent formal representation and allow transformers
    to engage in reasoning directly using natural language. Table LABEL:tab:reasoning
    highlights some of the significant transformer models for natural language reasoning
    tasks.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推理的追求是一个与问答不同的研究领域。问答关注的是在给定的文本段落中找到特定查询的答案。另一方面，自然语言推理涉及将演绎推理应用于从给定的前提和用自然语言表示的规则中得出结论。神经网络架构旨在学习如何利用这些前提和规则来推断新的结论。以前，类似的任务通常由配备了以正式格式表示的知识和应用规则的系统来处理。然而，正式表示的使用对这一研究领域构成了重大挑战（Mark
    A Musen，[1988](#bib.bib109)）。随着变换器的出现及其在众多 NLP 任务中的卓越表现，现在可以绕过正式表示，让变换器直接使用自然语言进行推理。表
    LABEL:tab:reasoning 突出了一些重要的用于自然语言推理任务的变换器模型。
- en: 'Table 8: Transformer models for natural language reasoning'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 自然语言推理的变换器模型'
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| (Clark et al., 2020) (Clark et al., [2020b](#bib.bib27)) | Binary Classification
    | 2020 | RoBERTa (Encoder) | Yes | RACE | RuleTaker |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| (Clark et al., 2020) (Clark et al., [2020b](#bib.bib27)) | 二分类 | 2020 | RoBERTa（编码器）
    | 是 | RACE | RuleTaker |'
- en: '| (Richardson et al., 2022) (Richardson & Sabharwal, [2022](#bib.bib140)) |
    Binary Classification | 2022 | RoBERTa Large (Encoder) | Yes | RACE | Hard-RuleTaker
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| (Richardson et al., 2022) (Richardson & Sabharwal, [2022](#bib.bib140)) |
    二分类 | 2022 | RoBERTa Large（编码器） | 是 | RACE | Hard-RuleTaker |'
- en: '| (Saha et al., 2020) (Saha et al., [2020](#bib.bib145)) | Binary Classification,
    Sequence Generation | 2020 | PRover [RoBERTa-based](Encoder) | No | NA | RuleTaker
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| (Saha et al., 2020) (Saha et al., [2020](#bib.bib145)) | 二分类，序列生成 | 2020
    | PRover [基于 RoBERTa](编码器) | 否 | NA | RuleTaker |'
- en: '| (Sinha et al., 2019) (Sinha et al., [2019](#bib.bib157)) | Sequence Generation
    | 2019 | BERT (Encoder) | No | NA | CLUTRR |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| (Sinha et al., 2019) (Sinha et al., [2019](#bib.bib157)) | 序列生成 | 2019 |
    BERT（编码器） | 否 | NA | CLUTRR |'
- en: '| (Picco et al., 2021) (Picco et al., [2021](#bib.bib126)) | Binary Classification
    | 2021 | BERT-Based (Encoder) | Yes | RACE | RuleTaker |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| (Picco et al., 2021) (Picco et al., [2021](#bib.bib126)) | 二分类 | 2021 | 基于
    BERT（编码器） | 是 | RACE | RuleTaker |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RoBERTa: In a 2020 study by Clark et al. ([2020b](#bib.bib27)), a binary classification
    task was assigned to the transformer, which aimed to determine whether a given
    statement can be inferred from a provided set of premises and rules represented
    in natural language. The architecture utilized for the transformer was RoBERTa-large,
    which was pre-trained on a dataset of high school exam questions that required
    reasoning skills. This pre-training enabled the transformer to achieve a high
    accuracy of $98\%$ on the test dataset. The dataset contained theories that were
    randomly sampled and constructed using sets of names and attributes. The task
    required the transformer to classify whether the given statement (Statement) followed
    from the provided premises and rules (Context) (Clark et al., [2020b](#bib.bib27))'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RoBERTa：在Clark等人（[2020b](#bib.bib27)）的2020年研究中，给变压器分配了一个二分类任务，旨在确定一个给定的陈述是否可以从一组用自然语言表示的前提和规则中推断出来。用于变压器的架构是RoBERTa-large，经过在需要推理技能的高中考试问题数据集上的预训练。这种预训练使得变压器在测试数据集上达到了$98\%$的高准确率。数据集包含了随机抽样的理论，这些理论是通过一组名称和属性构建的。任务要求变压器将给定的陈述（Statement）与提供的前提和规则（Context）进行分类，以判断陈述是否跟随于前提和规则（Clark等人，[2020b](#bib.bib27)）。
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RoBERTa-Large: In the work by Richardson & Sabharwal ([2022](#bib.bib140)),
    the authors aimed to address a limitation of the dataset construction approach
    presented in the work by Clark et al. ([2020b](#bib.bib27)). They highlighted
    that the uniform random sampling of theories, as done in (Clark et al., [2020b](#bib.bib27)),
    does not always result in challenging instances. To overcome this limitation,
    they proposed a novel methodology for creating more challenging algorithmic reasoning
    datasets. The key idea of their methodology is to sample hard instances from ordinary
    SAT propositional formulas and translate them into natural language using a predefined
    set of English rule languages. By following this approach, they were able to construct
    a more challenging dataset that is consequential for training robust models and
    for reliable evaluation. To demonstrate the effectiveness of their approach, the
    authors conducted experiments where they tested the models trained using the dataset
    from (Clark et al., [2020b](#bib.bib27)) on their newly constructed dataset. The
    results showed that the models achieved an accuracy of 57.7% and 59.6% for T5
    and RoBERTa, respectively. These findings highlight that models trained on easy
    datasets may not be capable of solving challenging instances of the problem.'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RoBERTa-Large：在Richardson & Sabharwal（[2022](#bib.bib140)）的工作中，作者旨在解决Clark等人（[2020b](#bib.bib27)）提出的数据集构建方法的局限性。他们指出，像（Clark等人，[2020b](#bib.bib27)）中那样的均匀随机抽样理论并不总能产生具有挑战性的实例。为了克服这一局限性，他们提出了一种新颖的方法来创建更具挑战性的算法推理数据集。他们的方法的关键思想是从普通SAT命题公式中抽样困难实例，并使用预定义的英语规则语言将其翻译成自然语言。通过这种方法，他们能够构建一个更具挑战性的数据集，这对训练强健的模型和可靠的评估至关重要。为了验证他们的方法的有效性，作者进行了实验，将使用（Clark等人，[2020b](#bib.bib27)）数据集训练的模型在他们新构建的数据集上进行测试。结果表明，模型在T5和RoBERTa上分别达到了57.7%和59.6%的准确率。这些发现表明，训练于简单数据集的模型可能无法解决问题的具有挑战性的实例。
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PRover: In a related study, Saha et al. ([2020](#bib.bib145)) proposed a model
    called PRover, which is an interpretable joint transformer capable of generating
    a corresponding proof with an accuracy of $87\%$. The task addressed by PRover
    is the same as that in the study by Clark et al. ([2020b](#bib.bib27)) and Richardson
    & Sabharwal ([2022](#bib.bib140)), where the aim is to determine whether a given
    conclusion follows from the provided premises and rules. The proof generated by
    PRover is represented as a directed graph, where the nodes represent statements
    and rules, and the edges indicate which new statements follow from applying rules
    on the previous statements. Overall, the proposed approach by Saha et al. ([2020](#bib.bib145))
    provides a promising direction towards achieving interpretable and accurate reasoning
    models.'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PRover：在相关研究中，Saha等人（[2020](#bib.bib145)）提出了一种名为PRover的模型，它是一种可解释的联合变压器，能够生成相应的证明，准确率为$87\%$。PRover处理的任务与Clark等人（[2020b](#bib.bib27)）和Richardson
    & Sabharwal（[2022](#bib.bib140)）的研究相同，旨在确定给定的结论是否可以从提供的前提和规则中推断出来。PRover生成的证明表示为有向图，其中节点表示陈述和规则，边表示应用规则在先前陈述上得出的新陈述。总体而言，Saha等人（[2020](#bib.bib145)）提出的方法为实现可解释和准确的推理模型提供了一个有前途的方向。
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BERT-based: In (Picco et al., [2021](#bib.bib126)), a BERT-based architecture
    called “neural unifier” was proposed to improve the generalization performance
    of the model on the RuleTaker dataset. The authors aimed to mimic some elements
    of the backward-chaining reasoning procedure to enhance the model’s ability to
    handle queries that require multiple steps to answer, even when trained on shallow
    queries only. The neural unifier consists of two standard BERT transformers, namely
    the fact-checking unit and the unification unit. The fact-checking unit is trained
    to classify whether a query of depth 0, represented by the embedding vector q-0,
    follows from a given knowledge base represented by the embedding vector C. The
    unification unit takes as input the embedding vector q-n of a depth-n query and
    the embedding vector of the knowledge base, vector C, and tries to predict an
    embedding vector q0, thereby performing backward-chaining.'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于BERT：在（Picco et al., [2021](#bib.bib126)）中，提出了一种名为“神经统一器”的基于BERT的架构，以提高模型在RuleTaker数据集上的泛化性能。作者旨在模拟一些反向推理程序的元素，以增强模型处理需要多步骤回答的查询的能力，即使仅在浅层查询上进行训练。神经统一器由两个标准BERT变换器组成，即事实检查单元和统一单元。事实检查单元经过训练，用于分类深度为0的查询（由嵌入向量q-0表示）是否从给定的知识库（由嵌入向量C表示）中推导出来。统一单元输入深度为n的查询的嵌入向量q-n和知识库的嵌入向量C，并尝试预测嵌入向量q0，从而执行反向推理。
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BERT: Sinha et al. ([2019](#bib.bib157)) introduced a dataset named CLUTRR,
    which differs from the previously discussed studies in that the rules are not
    given in the input to be used to infer conclusions. Instead, the BERT transformer
    model is tasked with both extracting relationships between entities and inferring
    the rules governing these relationships. For instance, given a knowledge base
    consisting of statements such as “Alice is Bob’s mother” and “Jim is Alice’s father”,
    the network can infer that “Jim is Bob’s grandfather”.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERT：Sinha et al. ([2019](#bib.bib157)) 引入了一个名为CLUTRR的数据集，这与之前讨论的研究不同，因为规则未在输入中给出用于推断结论。相反，BERT变换器模型负责提取实体之间的关系并推断这些关系的规则。例如，给定一个包含“爱丽丝是鲍勃的母亲”和“吉姆是爱丽丝的父亲”的知识库，网络可以推断出“吉姆是鲍勃的祖父”。
- en: 6.1.7 Automated symbolic reasoning
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.7 自动符号推理
- en: Automated symbolic reasoning is a subfield of computer science that deals with
    solving logical problems such as SAT solving and automated theorem proving. These
    problems are traditionally addressed using search techniques with heuristics.
    However, recent studies have explored the use of learning-based techniques to
    improve the efficiency and effectiveness of these methods. One approach is to
    learn the selection of efficient heuristics used by traditional algorithms. Alternatively,
    an end-to-end learning-based solution can be employed for the problem. Both approaches
    have shown promising results and offer the potential for further advancements
    in automated symbolic reasoning (Kurin et al., [2020](#bib.bib83), Selsam et al.,
    [2019](#bib.bib147)). In this regard, a number of transformer based models have
    shown significant performance in automated symbolic reasoning tasks. For those
    models, please look at Table LABEL:tab:naturalReasoning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 自动符号推理是计算机科学的一个子领域，涉及解决逻辑问题，如SAT求解和自动定理证明。这些问题传统上使用启发式搜索技术来解决。然而，最近的研究探索了使用基于学习的技术来提高这些方法的效率和效果。一种方法是学习传统算法使用的高效启发式选择。或者，可以使用端到端的基于学习的解决方案来处理问题。这两种方法都显示出有希望的结果，并提供了进一步推进自动符号推理的潜力（Kurin
    et al., [2020](#bib.bib83)，Selsam et al., [2019](#bib.bib147)）。在这方面，许多基于变换器的模型在自动符号推理任务中显示出显著的性能。有关这些模型的信息，请参见表
    LABEL:tab:naturalReasoning。
- en: 'Table 9: Transformer models for automated symbolic reasoning'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：用于自动符号推理的变换器模型
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 变换器模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| (Shi et al., 2022) (Shi et al., [2022b](#bib.bib154)) | Binary Classification
    | 2022 | SATFormer (Encoder / Decoder) | No | NA | Synthetic |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| (Shi et al., 2022) (Shi et al., [2022b](#bib.bib154)) | 二分类 | 2022 | SATFormer（编码器/解码器）
    | 否 | NA | 合成 |'
- en: '| (Shi et al., 2021) (Shi et al., [2021](#bib.bib153)) | Binary Classification
    | 2021 | TRSAT (Encoder / Decoder) | No | NA | Synthetic, SATLIB |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| (Shi et al., 2021) (Shi et al., [2021](#bib.bib153)) | 二分类 | 2021 | TRSAT（编码器
    / 解码器） | 否 | NA | 合成, SATLIB |'
- en: '| (Hahn et al., 2021) (Hahn et al., [2021](#bib.bib50)) | Sequence Generation
    | 2021 | Transformer (Encoder / Decoder) | No | NA | Synthetic |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| (Hahn et al., 2021) (Hahn et al., [2021](#bib.bib50)) | 序列生成 | 2021 | 变换器（编码器
    / 解码器） | 否 | NA | 合成 |'
- en: '| (Polu et al, 2020) (Polu & Sutskever, [2020](#bib.bib128)) | Sequence Generation
    | 2020 | GPT-f (Decoder) | Yes | CommonCrawl, Github, arXiv, WebMath | set.mm
    |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| (Polu et al, 2020) (Polu & Sutskever, [2020](#bib.bib128)) | 序列生成 | 2020
    | GPT-f（解码器） | 是 | CommonCrawl, Github, arXiv, WebMath | set.mm |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SATformer: The SAT-solving problem for boolean formulas was addressed by Shi
    et al. in 2022 (Shi et al., [2021](#bib.bib153)) through the introduction of SATformer,
    a hierarchical transformer architecture that offers an end-to-end learning-based
    solution for solving the problem. Traditionally, in the context of SAT-solving,
    a boolean formula is transformed into its conjunctive normal form (CNF), which
    serves as an input for the SAT solver. The CNF formula is a conjunction of boolean
    variables and their negations, known as literals, organized into clauses where
    each clause is a disjunction of these literals. For example, a CNF formula utilizing
    boolean variables would be represented as (A OR B) AND (NOT A OR C), where each
    clause (A OR B) and (NOT A OR C) is made up of literals.'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SATformer：布尔公式的SAT求解问题在2022年由Shi et al.解决（Shi et al., [2021](#bib.bib153)），通过引入SATformer，这是一种分层变换器架构，提供了一种端到端学习的解决方案来解决该问题。在传统的SAT求解上下文中，布尔公式被转换为其合取范式（CNF），作为SAT求解器的输入。CNF公式是布尔变量及其否定的合取，称为文字，组织成子句，每个子句是这些文字的析取。例如，利用布尔变量的CNF公式可以表示为(A
    OR B) AND (NOT A OR C)，其中每个子句(A OR B)和(NOT A OR C)由文字组成。
- en: The authors employ a graph neural network (GNN) to obtain the embeddings of
    the clauses in the CNF formula. SATformer then operates on these clause embeddings
    to capture the interdependencies among clauses, with the self-attention weight
    being trained to be high when groups of clauses that could potentially lead to
    an unsatisfiable formula are attended together, and low otherwise. Through this
    approach, SATformer efficiently learns the correlations between clauses, resulting
    in improved SAT prediction capabilities (Shi et al., [2022b](#bib.bib154)).
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者们使用图神经网络（GNN）来获取CNF公式中子句的嵌入表示。SATformer随后在这些子句嵌入上操作，以捕捉子句之间的相互依赖关系，当可能导致不满足的子句组一起被关注时，自注意力权重被训练得较高，否则较低。通过这种方法，SATformer有效地学习了子句之间的相关性，从而提高了SAT预测能力（Shi
    et al., [2022b](#bib.bib154)）。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TRSAT: Another research endeavor conducted by Shi et al. in 2021 investigated
    a variant of the boolean SAT problem known as MaxSAT and introduced a transformer
    model named TRSAT, which serves as an end-to-end learning-based SAT solver (Shi
    et al., [2021](#bib.bib153)). A comparable problem to the boolean SAT is the satisfiability
    of a linear temporal formula (Pnueli, [1977](#bib.bib127)), where a satisfying
    symbolic trace to the formula is sought after given a linear temporal formula.'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TRSAT：Shi et al.于2021年进行的另一项研究探讨了一种被称为MaxSAT的布尔SAT问题的变体，并引入了一种名为TRSAT的变换器模型，该模型作为一种端到端学习的SAT求解器（Shi
    et al., [2021](#bib.bib153)）。与布尔SAT问题类似的是线性时间公式的可满足性（Pnueli, [1977](#bib.bib127)），在给定线性时间公式的情况下，寻找一个满足该公式的符号轨迹。
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformer: In a study conducted by Hahn et al. ([2021](#bib.bib50)), the
    authors addressed the boolean SAT problem and the temporal satisfiability problem,
    both of which are more complex than binary classification tasks that were tackled
    in previous studies. In these problems, the task is to generate a satisfying sequence
    assignment for a given formula, rather than simply classifying whether the formula
    is satisfied or not. The authors constructed their datasets by using classical
    solvers to generate linear temporal formulas with their corresponding satisfying
    symbolic traces, and boolean formulas with their corresponding satisfying partial
    assignments. The authors employed a standard transformer architecture to solve
    the sequence-to-sequence task. The Transformer was able to generate satisfying
    traces, some of which were not observed during training, demonstrating its capability
    to solve the problem and not merely mimic the behavior of the classical solvers
    used in the dataset generation process.'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变压器：在 Hahn 等人 ([2021](#bib.bib50)) 进行的一项研究中，作者解决了布尔 SAT 问题和时间可满足性问题，这些问题比之前研究中的二分类任务要复杂。在这些问题中，任务是为给定的公式生成一个满足的序列分配，而不仅仅是分类公式是否被满足。作者通过使用经典求解器生成线性时间公式及其相应的满足符号轨迹，以及布尔公式及其相应的满足部分分配，来构建数据集。作者采用了标准的变压器架构来解决序列到序列的任务。变压器能够生成满足的轨迹，其中一些在训练过程中未曾观察到，这表明它具有解决问题的能力，而不仅仅是模仿用于数据集生成过程中的经典求解器的行为。
- en: •
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPT-f: In their work, Polu & Sutskever ([2020](#bib.bib128)) presented GPT-F,
    an automated prover and proof assistant that utilizes a decoder-only transformers
    architecture similar to GPT-2 and GPT-3\. GPT-F was trained on a dataset called
    set.mm, which contains approximately 38,000 proofs. The largest model investigated
    by the authors consists of 36 layers and 774 million trainable parameters. This
    deep learning network has generated novel proofs that have been accepted and incorporated
    into mathematical proof libraries and communities.'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-f：在他们的研究中，Polu 和 Sutskever ([2020](#bib.bib128)) 提出了 GPT-F，这是一种自动化证明器和证明助手，利用类似于
    GPT-2 和 GPT-3 的仅解码器变压器架构。GPT-F 在一个名为 set.mm 的数据集上进行了训练，该数据集包含约 38,000 个证明。作者研究的最大模型由
    36 层和 7.74 亿个可训练参数组成。这个深度学习网络生成了新的证明，这些证明已被接受并纳入数学证明库和社区。
- en: 6.2 Computer Vision
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 计算机视觉
- en: 'Motivated by the success of transformers in natural language processing, researchers
    have explored the application of the transformer concept in computer vision tasks.
    Traditionally, convolutional neural networks (CNNs) have been considered the fundamental
    component for processing visual data. However, different types of images require
    different processing techniques, with natural images and medical images being
    two prime examples. Furthermore, research in computer vision for natural images
    and medical images is vast and distinct. As a result, transformer models for computer
    vision can be broadly classified into two categories: (i) those designed for natural
    image processing, and (ii) those designed for medical image processing.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 受变压器在自然语言处理中的成功激励，研究人员探索了变压器概念在计算机视觉任务中的应用。传统上，卷积神经网络（CNNs）被认为是处理视觉数据的基础组件。然而，不同类型的图像需要不同的处理技术，自然图像和医学图像是两个主要例子。此外，针对自然图像和医学图像的计算机视觉研究广泛且各异。因此，计算机视觉中的变压器模型可以大致分为两类：（i）专为自然图像处理设计的模型，以及（ii）专为医学图像处理设计的模型。
- en: 6.2.1 Natural Image processing
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 自然图像处理
- en: 'In the domain of computer vision, natural image processing is a primary focus
    as compared to medical image processing, owing to the greater availability of
    natural image data. Furthermore, computer vision with natural images has wide-ranging
    applications in various domains. Among the numerous tasks associated with computer
    vision and natural images, we have identified four of the most common and popular
    tasks: (i) classification and segmentation, (ii) recognition and feature extraction,
    (iii) mask modeling prediction, and (iv) image generation. In this context, we
    have provided a comprehensive discussion of each of these computer vision tasks
    with natural images. Additionally, we have presented a table that provides crucial
    information about each transformer-based model and have highlighted their working
    methods and significance.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，相较于医学图像处理，自然图像处理是主要关注点，因为自然图像数据的可用性更高。此外，计算机视觉与自然图像在各种领域中有广泛的应用。在与计算机视觉和自然图像相关的众多任务中，我们已经确定了四个最常见且流行的任务：（i）分类和分割，（ii）识别和特征提取，（iii）掩膜建模预测，以及（iv）图像生成。在这种背景下，我们提供了对每个自然图像计算机视觉任务的全面讨论。此外，我们还展示了一张表格，提供了有关每个基于变换器的模型的重要信息，并强调了它们的工作方法和重要性。
- en: 6.2.2 Image Classification
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 图像分类
- en: Image classification is a crucial and popular task in the field of computer
    vision, which aims to analyze and categorize images based on their features, type,
    genre, or objects. This task is considered as a primary stage for many other image
    processing tasks. For example, if we have a set of images of different animals,
    we can classify them into different animal categories such as cat, dog, horse,
    etc., based on their characteristics and features (Szummer & Picard, [1998](#bib.bib166),
    Lu & Weng, [2007](#bib.bib105)). Due to its significance, many transformer-based
    models have been developed to address image classification tasks. Table LABEL:tab:image_classification
    highlights some of the significant transformer models for image classification
    tasks and discusses their important features and working methodologies.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是计算机视觉领域中的一个关键且流行的任务，其目的是根据图像的特征、类型、类别或物体对图像进行分析和分类。这个任务被视为许多其他图像处理任务的基础阶段。例如，如果我们有一组不同动物的图像，我们可以根据它们的特征和特性将它们分类为不同的动物类别，如猫、狗、马等（Szummer
    & Picard，[1998](#bib.bib166)，Lu & Weng，[2007](#bib.bib105)）。由于其重要性，许多基于变换器的模型已经被开发出来以解决图像分类任务。表格
    LABEL:tab:image_classification 突出了图像分类任务中的一些重要变换器模型，并讨论了它们的重要特性和工作方法。
- en: 'Table 10: Transformer models for natural image processing - image classification'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 自然图像处理的变换器模型 - 图像分类'
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 变换器模型 | 已完成任务 | 年份 | 结构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调，训练，测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| VIT (Dosovitskiy et al., [2021](#bib.bib36)) | Image classification, image
    recognition | 2021 | Encoder | Yes | JFT-300M, ILSVRC-2012 ImageNet, ImageNet-21k
    | ImageNet-RL, CIFAR-10/100, Oxford Flowers-102, Oxford-IIIT Pets, VTAB |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| VIT (Dosovitskiy et al., [2021](#bib.bib36)) | 图像分类，图像识别 | 2021 | 编码器 | 是
    | JFT-300M, ILSVRC-2012 ImageNet, ImageNet-21k | ImageNet-RL, CIFAR-10/100, Oxford
    Flowers-102, Oxford-IIIT Pets, VTAB |'
- en: '| ViT Variants (d’Ascoli et al., [2021](#bib.bib31), Ahmed et al., [2021](#bib.bib2),
    Touvron et al., [2021](#bib.bib171), Arnab et al., [2021](#bib.bib5)) | Image
    classification | 2020-2021 | Encoder | Yes | ConViT: ImageNet (Based on DeiT)
     SiT: STL10, CUB200, CIFAR10, CIFAR100, ImageNet-1K, Pascal VOC, MS-COCO, Visual-Genome
     DEIT: ImageNet.  ViViT: ImageNet, JFT | ConViT: ImageNet,CIFAR100  SiT: CIFAR-10,CIFAR-100
    , STL-10, CUB200, ImageNet-1K, Pascal VOC, MS-COCO, Visual-Genome.  DEIT: ImageNet,
    iNaturalist 2018, iNaturalist 2019, Flowers-102, Stanford Cars, CIFAR-100, CIFAR-10\.
    ViViT: Larger JFT, Kinetics, Epic Kitchens-100, Moments in Time, SSv2. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| ViT 变体（d’Ascoli et al., [2021](#bib.bib31)，Ahmed et al., [2021](#bib.bib2)，Touvron
    et al., [2021](#bib.bib171)，Arnab et al., [2021](#bib.bib5)） | 图像分类 | 2020-2021
    | 编码器 | 是 | ConViT: ImageNet（基于DeiT） SiT: STL10, CUB200, CIFAR10, CIFAR100, ImageNet-1K,
    Pascal VOC, MS-COCO, Visual-Genome DEIT: ImageNet. ViViT: ImageNet, JFT | ConViT:
    ImageNet,CIFAR100 SiT: CIFAR-10,CIFAR-100 , STL-10, CUB200, ImageNet-1K, Pascal
    VOC, MS-COCO, Visual-Genome. DEIT: ImageNet, iNaturalist 2018, iNaturalist 2019,
    Flowers-102, Stanford Cars, CIFAR-100, CIFAR-10\. ViViT: Larger JFT, Kinetics,
    Epic Kitchens-100, Moments in Time, SSv2. |'
- en: '| BEIT (Bao et al., [2022](#bib.bib9)) | Image classification & segmentation
    | 2021 | Encoder | Yes | ImageNet-1K, ImageNet-22k | ILSVRC-2012 ImageNet, ADE20K,
    CIFAR-100 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| BEIT (Bao et al., [2022](#bib.bib9)) | 图像分类与分割 | 2021 | 编码器 | 是 | ImageNet-1K,
    ImageNet-22k | ILSVRC-2012 ImageNet, ADE20K, CIFAR-100 |'
- en: '| IBOT (Zhou et al., [2021b](#bib.bib210)) | Image classification, segmentation,
    object detection & recognition | 2022 | Encoder | Yes | ImageNet-1K, ViT-L/16,
    ImageNet-22K | COCO, ADE20K |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| IBOT (Zhou et al., [2021b](#bib.bib210)) | 图像分类、分割、物体检测与识别 | 2022 | 编码器 |
    是 | ImageNet-1K, ViT-L/16, ImageNet-22K | COCO, ADE20K |'
- en: '| Conformer (Peng et al., [2021](#bib.bib125)) | Image recognition & object
    detection, Classification | 2021 | Encoder | Not mentioned in paper | N/A | LibriSpeech
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Conformer (Peng et al., [2021](#bib.bib125)) | 图像识别与物体检测、分类 | 2021 | 编码器
    | 论文中未提及 | 不适用 | LibriSpeech |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ViT Variants: There are several ViT-based models that have been developed for
    specific tasks. For instance, ConViT is an improved version of ViT that combines
    CNN and transformer by adding an inductive bias to ViT, resulting in better accuracy
    for image classification tasks (d’Ascoli et al., [2021](#bib.bib31)). Self-supervised
    Vision transformer (SiT) allows for the use of the architecture as an autoencoder
    and seamlessly works with multiple self-supervised tasks (Ahmed et al., [2021](#bib.bib2)).
    Data Efficient Image Transformer (DeiT) is a type of vision transformer designed
    for image classification tasks that require less data to be trained (Touvron et al.,
    [2021](#bib.bib171)). There are numerous ViT variants available with certain improvements
    or designed for specific tasks. For example, Video Vision Transformer (ViViT)
    is a ViT-based model that classifies videos using both encoder and decoder modules
    of the transformer, whereas most ViT and ViT-variant models use only the encoder
    module (Arnab et al., [2021](#bib.bib5)).'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ViT变体：有几种基于ViT的模型已经被开发用于特定任务。例如，ConViT是ViT的改进版，它通过为ViT添加归纳偏置，结合了CNN和transformer，从而在图像分类任务中取得了更好的准确性（d’Ascoli
    et al., [2021](#bib.bib31)）。自监督视觉transformer（SiT）允许将该架构作为自编码器使用，并无缝地处理多种自监督任务（Ahmed
    et al., [2021](#bib.bib2)）。数据高效图像transformer（DeiT）是一种用于图像分类任务的视觉transformer，它需要较少的数据进行训练（Touvron
    et al., [2021](#bib.bib171)）。有许多ViT变体具有一定的改进或专门设计用于特定任务。例如，视频视觉transformer（ViViT）是一个基于ViT的模型，它使用transformer的编码器和解码器模块来分类视频，而大多数ViT及其变体模型仅使用编码器模块（Arnab
    et al., [2021](#bib.bib5)）。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BEIT: Bidirectional Encoder Representation from Image Transformers (BEIT) (Bao
    et al., [2022](#bib.bib9)) is a transformer-based model that draws inspiration
    from BERT and introduces a new pre-training task called Masked Image Modeling
    (MIM) for vision Transformers. In MIM, a portion of the image is randomly masked,
    and the corrupted image is passed through the architecture, which then recovers
    the original image tokens. BEIT has shown competitive performance on image classification
    and segmentation tasks, demonstrating its effectiveness for a variety of computer
    vision applications.'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BEIT：图像transformers的双向编码器表示（BEIT）（Bao et al., [2022](#bib.bib9)）是一个基于transformer的模型，它从BERT中获得灵感，并为视觉transformers引入了一种新的预训练任务，称为掩码图像建模（MIM）。在MIM中，图像的一部分会被随机遮盖，损坏的图像会通过架构，然后恢复原始图像标记。BEIT在图像分类和分割任务中表现出竞争力，展示了其在各种计算机视觉应用中的有效性。
- en: •
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Conformer: In the field of computer vision, Conformer (Peng et al., [2021](#bib.bib125))
    is a model that works similarly to the CMT model. While CNN is responsible for
    capturing the local features of the image, Transformer works for the global context
    and long-range dependencies of images. However, the Conformer model proposes a
    new method called cross-attention, which combines both local and global features
    to focus on various parts of the image based on the task. The model has shown
    promising results for classification and object detection/recognition tasks.'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Conformer：在计算机视觉领域，Conformer（Peng et al., [2021](#bib.bib125)）是一个与CMT模型类似的模型。虽然CNN负责捕捉图像的局部特征，但Transformer则处理图像的全局上下文和长距离依赖。然而，Conformer模型提出了一种称为交叉注意力的新方法，它结合了局部和全局特征，根据任务聚焦于图像的不同部分。该模型在分类和物体检测/识别任务中表现出了良好的前景。
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'IBOT: IBOT represents Image BERT Pre-training with Online Tokennizer which
    is a self-supervised model. This model studied masked image modeling using an
    online tokenizer and it learns to distill features using a tokenizer. This online
    tokenizer helps this model to improve the feature representation capability. Besides,
    the image classification task, this model shows significant performance in object
    detection and segmentation tasks.'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'IBOT: IBOT 代表图像 BERT 预训练与在线标记器，它是一个自监督模型。该模型研究了使用在线标记器进行掩码图像建模，并通过标记器学习提取特征。这个在线标记器有助于提高模型的特征表示能力。除了图像分类任务之外，该模型在目标检测和分割任务中表现出显著的性能。'
- en: ViT The ViT model, which has been discussed in detail in the Recognition and
    Object Detection section, is also capable of performing recognition and object
    detection tasks.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ViT ViT 模型在识别和目标检测部分已详细讨论，该模型也能够执行识别和目标检测任务。
- en: 6.2.3 Image Recognition & Object Detection
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 图像识别与目标检测
- en: Image recognition & Object detection is often considered as nearly similar and
    related task in computer vision. It is the capability of detecting or recognizing
    any object, person, or feature in an image or video. An image or video contains
    a number of objects & features; by extracting the features from the image, a model
    tries to capture the features of an object through training. By understanding
    these useful features, a model can recognize the specific object from the other
    available object in the image or video (Zhao et al., [2019](#bib.bib206), Jiao
    et al., [2019](#bib.bib73), Hénaff, [2020](#bib.bib58), Chen et al., [2019a](#bib.bib16)).
    Here we highlight and discuss the significant transformer models for image/object
    recognition tasks (see Table LABEL:tab:image_recognition).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别与目标检测在计算机视觉中通常被视为非常相似且相关的任务。它是检测或识别图像或视频中的任何对象、人物或特征的能力。图像或视频包含多个对象和特征；通过从图像中提取特征，模型尝试通过训练捕捉对象的特征。通过理解这些有用的特征，模型可以从图像或视频中的其他对象中识别特定对象（Zhao
    et al., [2019](#bib.bib206), Jiao et al., [2019](#bib.bib73), Hénaff, [2020](#bib.bib58),
    Chen et al., [2019a](#bib.bib16)）。在这里，我们重点讨论图像/对象识别任务的重要 Transformer 模型（参见表 LABEL:tab:image_recognition）。
- en: 'Table 11: Transformer models for natural image processing - image recognition
    & object detection'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 自然图像处理的 Transformer 模型 - 图像识别与目标检测'
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| VIT (Dosovitskiy et al., [2021](#bib.bib36)) | Image classification, image
    recognition | 2021 | Encoder | Yes | JFT-300M, ILSVRC-2012 ImageNet, ImageNet-21k
    | ImageNet-RL, CIFAR-10/100, Oxford Flowers-102, Oxford-IIIT Pets, VTAB |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| VIT (Dosovitskiy et al., [2021](#bib.bib36)) | 图像分类、图像识别 | 2021 | 编码器 | 是
    | JFT-300M, ILSVRC-2012 ImageNet, ImageNet-21k | ImageNet-RL, CIFAR-10/100, Oxford
    Flowers-102, Oxford-IIIT Pets, VTAB |'
- en: '| Conformer (Peng et al., [2021](#bib.bib125)) | Image recognition & object
    detection, classification | 2021 | Encoder | Not mentioned in paper | N/A | LibriSpeech
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Conformer (Peng et al., [2021](#bib.bib125)) | 图像识别与目标检测、分类 | 2021 | 编码器
    | 论文中未提及 | 不适用 | LibriSpeech |'
- en: '| LoFTR (Sun et al., [2021a](#bib.bib163)) | Image feature matching & visual
    localization | 2021 | Encoder & Decoder | No | NA | MegaDepth, ScanNet HPatches,
    ScanNet, MegaDepth, VisLoc benchmark (the Aachen-Day-Night, InLoc) |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR (Sun et al., [2021a](#bib.bib163)) | 图像特征匹配与视觉定位 | 2021 | 编码器与解码器 |
    否 | 无 | MegaDepth, ScanNet HPatches, ScanNet, MegaDepth, VisLoc benchmark（Aachen-Day-Night,
    InLoc） |'
- en: '| CMT (Guo et al., [2022a](#bib.bib48)) | Image recognition, detection & segmentation
    | 2022 | Encoder | No | NA | ImageNet, CIFAR10, CIFAR100, Flowers, Standford cars,
    Oxford-IIIT pets, COCO val2017 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| CMT (Guo et al., [2022a](#bib.bib48)) | 图像识别、检测与分割 | 2022 | 编码器 | 否 | 无 |
    ImageNet, CIFAR10, CIFAR100, Flowers, Standford cars, Oxford-IIIT pets, COCO val2017
    |'
- en: '| Transformer in Transformer-TNT (Han et al., [2021](#bib.bib54)) | Image recognition
    | 2021 | Encoder & Decoder | Yes | ImageNet ILSVRC 2012 | COCO2017, ADE20K, Oxford
    102 Flowers, Oxford-IIIT Pets, iNaturalist 2019, CIFAR-10, CIFAR-100 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Transformer in Transformer-TNT (Han et al., [2021](#bib.bib54)) | 图像识别 |
    2021 | 编码器与解码器 | 是 | ImageNet ILSVRC 2012 | COCO2017, ADE20K, Oxford 102 Flowers,
    Oxford-IIIT Pets, iNaturalist 2019, CIFAR-10, CIFAR-100 |'
- en: '| SWIN (Liu et al., [2021](#bib.bib102)) | Object detection and segmentation
    | 2021 | Encoder | Yes | ImageNet-22k | ImageNet-1k, COCO 2017, ADE20K |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| SWIN (Liu et al., [2021](#bib.bib102)) | 目标检测与分割 | 2021 | 编码器 | 是 | ImageNet-22k
    | ImageNet-1k, COCO 2017, ADE20K |'
- en: '| DETR (Carion et al., [2020](#bib.bib13)) | Object detection & prediction
    | 2020 | Encoder & Decoder | Yes | ImageNet pretrained backbone ResNet-50 | COCO
    2017, panoptic segmentation datasets |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| DETR (Carion et al., [2020](#bib.bib13)) | 物体检测与预测 | 2020 | 编码器与解码器 | 是 |
    ImageNet 预训练骨干 ResNet-50 | COCO 2017, 全景分割数据集 |'
- en: '| HOTR (Kim et al., [2021a](#bib.bib80)) | Human-object interaction detection
    | 2021 | Encoder & Decoder | Yes | MS-COCO | V-COCO HICO-DET |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| HOTR (Kim et al., [2021a](#bib.bib80)) | 人体与物体交互检测 | 2021 | 编码器与解码器 | 是 |
    MS-COCO | V-COCO HICO-DET |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ViT: The ViT (Vision Transformer) is one of the earliest transformer-based
    models that has been applied to computer vision. ViT views an image as a sequence
    of patches and processes it using only the encoder module of the Transformer.
    ViT performs very well for classification tasks and can also be applied to image
    recognition tasks. It demonstrates that a transformer-based model can serve as
    an alternative to convolutional neural networks (Dosovitskiy et al., [2021](#bib.bib36)).'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ViT: ViT（视觉变换器）是最早应用于计算机视觉的变换器基础模型之一。ViT将图像视为一系列补丁，并仅使用变换器的编码器模块进行处理。ViT在分类任务中表现非常出色，也可以应用于图像识别任务。它展示了变换器基础模型可以作为卷积神经网络的替代方案（Dosovitskiy
    et al., [2021](#bib.bib36)）。'
- en: •
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TNT: Transformer in Transformers (TNT) is a transformer-based computer vision
    model that uses a transformer model inside another transformer model to capture
    features inside local patches of an image (Han et al., [2021](#bib.bib54)). The
    image is divided into local patches, which are further divided into smaller patches
    to capture more detailed information through attention mechanisms. TNT shows promising
    results in visual recognition tasks and offers an alternative to convolutional
    neural networks for computer vision tasks.'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TNT: 变换器中的变换器（TNT）是一种基于变换器的计算机视觉模型，它在另一个变换器模型内部使用变换器模型，以捕捉图像中局部补丁的特征（Han et
    al., [2021](#bib.bib54)）。图像被划分为局部补丁，这些补丁进一步被划分为更小的补丁，通过注意力机制捕捉更详细的信息。TNT在视觉识别任务中展示了有希望的结果，并为计算机视觉任务提供了卷积神经网络的替代方案。'
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LoFTR: LoFTR, which stands for Local Feature Matching with Transformer, is
    a computer vision model that is capable of learning feature representations directly
    from raw images, as opposed to relying on hand-crafted feature detectors for feature
    matching. This model employs both the encoder and decoder modules of the transformer.
    The encoder takes features from the image, while the decoder works to create a
    feature map. By leveraging the transformer’s ability to capture global context
    and long-range dependencies, LoFTR can achieve high performance in visual recognition
    tasks.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LoFTR: LoFTR（局部特征匹配与变换器）是一种计算机视觉模型，能够直接从原始图像中学习特征表示，而不是依赖手工设计的特征检测器进行特征匹配。该模型使用了变换器的编码器和解码器模块。编码器从图像中提取特征，而解码器则创建特征图。通过利用变换器捕捉全局上下文和长距离依赖的能力，LoFTR在视觉识别任务中可以实现高性能。'
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DETR: The Detection Transformer (DETR) represents a new approach to object
    detection or recognition, which performs the object detection task as a direct
    set of prediction problems (Carion et al., [2020](#bib.bib13)). In contrast, other
    models accomplish this task in two stages. DETR uses an encoder to generate object
    queries, a self-attention mechanism to capture the relationship between the queries
    and objects in the image, and creates an object detection scheme. This model has
    been shown to be effective for object detection and recognition tasks and represents
    a significant advancement in the field.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DETR: 检测变换器（DETR）代表了一种新的物体检测或识别方法，它将物体检测任务作为一组直接的预测问题来处理（Carion et al., [2020](#bib.bib13)）。与此不同，其他模型将此任务分为两个阶段。DETR使用编码器生成物体查询，自注意力机制捕捉查询与图像中物体之间的关系，并创建物体检测方案。该模型已被证明在物体检测和识别任务中有效，代表了该领域的重要进展。'
- en: •
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'HOTR: The HOTR model, which stands for Human-Object Interaction Transformer,
    is a Transformer-based model designed for predicting Human-Object Interaction.
    It is the first Transformer-based Human-Object Interaction (HOI) detection prediction
    model that employs both the encoder and decoder modules of the Transformer. Unlike
    conventional hand-crafted post-processing schemes, HOTR uses a prediction set
    to extract the semantic relationship of the image, making it one of the fastest
    human-object interaction detection models available (Kim et al., [2021a](#bib.bib80)).'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HOTR：HOTR 模型，全称为人-对象互动 Transformer，是一种基于 Transformer 的模型，专为预测人-对象互动设计。它是第一个使用
    Transformer 的编码器和解码器模块的 Transformer 基础的人-对象互动（HOI）检测预测模型。与传统的手工后处理方案不同，HOTR 使用预测集来提取图像的语义关系，使其成为目前最快的人-对象互动检测模型之一（Kim
    et al., [2021a](#bib.bib80)）。
- en: CMT, Conformer & SWIN Transformer The CMT and SWIN Transformer model have already
    been described in the Image Segmentation and Image Classification sections, respectively.
    Both of these models are also capable of performing the task of Image Segmentation.
    Additionally, the Conformer model was described in the Image Classification section.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CMT、Conformer 和 SWIN Transformer CMT 和 SWIN Transformer 模型已分别在图像分割和图像分类部分描述过。这些模型也能够执行图像分割任务。此外，Conformer
    模型已在图像分类部分描述。
- en: 6.2.4 Image Segmentation
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4 图像分割
- en: 'Segmentation is the process of partitioning an image based on objects and creating
    boundaries between them, requiring pixel-level information extraction. There are
    two popular types of image segmentation tasks in computer vision: (i) Semantic
    Segmentation, which aims to identify and color similar objects belonging to the
    same class among all other objects in an image, and (ii) Instance Segmentation,
    which aims to detect instances of objects and their boundaries (Minaee et al.,
    [2022](#bib.bib112), Haralick & Shapiro, [1985](#bib.bib55)). In this section,
    we will discuss some Transformer-based models that have shown exceptional performance
    in image segmentation tasks (refer to Table LABEL:tab:Image_segmentation for more
    details).'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是基于对象对图像进行划分并创建对象之间边界的过程，需要进行像素级信息提取。计算机视觉中有两种流行的图像分割任务：（i）语义分割，旨在识别并为属于同一类别的相似对象上色，以及（ii）实例分割，旨在检测对象实例及其边界（Minaee
    et al., [2022](#bib.bib112)，Haralick & Shapiro，[1985](#bib.bib55)）。在这一部分，我们将讨论一些在图像分割任务中表现出色的基于
    Transformer 的模型（详细信息请参见表 LABEL:tab:Image_segmentation）。
- en: 'Table 12: Transformer models for natural image processing - image segmentation'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：用于自然图像处理的 Transformer 模型 - 图像分割
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| SWIN (Liu et al., [2021](#bib.bib102)) | Object detection and segmentation
    | 2021 | Encoder | Yes | ImageNet-22k | ImageNet-1k, COCO 2017, ADE20K |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| SWIN (Liu et al., [2021](#bib.bib102)) | 对象检测和分割 | 2021 | 编码器 | 是 | ImageNet-22k
    | ImageNet-1k，COCO 2017，ADE20K |'
- en: '| CMT (Guo et al., [2022a](#bib.bib48)) | Image recognition, detection & segmentation
    | 2022 | Encoder | No | NA | ImageNet, CIFAR10, CIFAR100, Flowers, Standford cars,
    Oxford-IIIT pets, COCO val2017 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| CMT (Guo et al., [2022a](#bib.bib48)) | 图像识别、检测与分割 | 2022 | 编码器 | 否 | NA
    | ImageNet、CIFAR10、CIFAR100、Flowers、Standford cars、Oxford-IIIT pets、COCO val2017
    |'
- en: '| SETR (Zheng et al., [2021](#bib.bib207)) | Image segmentation | 2020 | Encoder
    & Decoder | Yes | ImageNet-1k, pre-trained weights provided by ViT or DeiT | ADE20K,
    Pascal Context, CityScapes |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| SETR (Zheng et al., [2021](#bib.bib207)) | 图像分割 | 2020 | 编码器 & 解码器 | 是 |
    ImageNet-1k，由 ViT 或 DeiT 提供的预训练权重 | ADE20K，Pascal Context，CityScapes |'
- en: '| IBOT (Zhou et al., [2021b](#bib.bib210)) | Image classification, segmentation,
    object detection & recognition | 2022 | Encoder | Yes | ImageNet-1K, ViT-L/16
    ImageNet-22K | COCO, ADE20K |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| IBOT (Zhou et al., [2021b](#bib.bib210)) | 图像分类、分割、对象检测与识别 | 2022 | 编码器 |
    是 | ImageNet-1K，ViT-L/16 ImageNet-22K | COCO，ADE20K |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SWIN Transformer: The SWIN Transformer (Liu et al., [2021](#bib.bib102)), short
    for Scaled WINdowed Transformer, is a transformer-based model that is capable
    of handling large images by dividing them into small patches, or windows, and
    processing them through its architecture. By using shifted windows, the model
    requires a smaller number of parameters and less computational power, making it
    useful for real-life image applications. SWIN Transformer can perform image classification,
    segmentation, and object detection tasks with exceptional accuracy and efficiency
    (Zidan et al., [2023](#bib.bib213), Yang & Yang, [2023](#bib.bib193)).'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SWIN 变换器：SWIN 变换器（Liu 等，[2021](#bib.bib102)），即缩放窗口变换器，是一个能够处理大型图像的基于变换器的模型，通过将图像划分为小补丁或窗口，并通过其架构进行处理。通过使用位移窗口，该模型需要较少的参数和计算能力，使其适用于实际图像应用。SWIN
    变换器能够以卓越的准确性和效率执行图像分类、分割和对象检测任务（Zidan 等，[2023](#bib.bib213)，Yang & Yang，[2023](#bib.bib193)）。
- en: •
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CMT: CNNs Meet Transformer is a model that combines both Convolutional Neural
    Networks (CNN) and Vision Transformer (ViT). CNNs are better suited to capturing
    local features, while Transformers excel at capturing global context. CMT takes
    advantage of the strengths of both these models and performs well in image classification
    tasks as well as object detection and recognition tasks. The integration of CNN
    and Transformer allows CMT to handle both spatial and sequential data effectively,
    making it a powerful tool for computer vision tasks (Guo et al., [2022a](#bib.bib48)).'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CMT：CNNs Meet Transformer 是一个结合了卷积神经网络（CNN）和视觉变换器（ViT）的模型。CNN 更适合捕捉局部特征，而变换器则擅长捕捉全局上下文。CMT
    充分利用了这两种模型的优势，在图像分类任务以及对象检测和识别任务中表现良好。CNN 和变换器的结合使 CMT 能够有效处理空间和序列数据，使其成为计算机视觉任务中的强大工具（Guo
    等，[2022a](#bib.bib48)）。
- en: •
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SETR: SETR stands for SEgmentation TRansformer, which is a transformer-based
    model used for image segmentation tasks. It uses sequence-to-sequence prediction
    methods and removes the dependency of fully convolutional network with vanilla
    Transformer architecture. Before feeding the image into the Transformer architecture,
    it divides the image into a sequence of patches and the flattened pixel of each
    patch. There are three variants of SETR models available with different model
    sizes and performance levels (Zheng et al., [2021](#bib.bib207)).'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SETR：SETR 代表 SEgmentation TRansformer，是一个用于图像分割任务的基于变换器的模型。它使用序列到序列的预测方法，并去除了对传统卷积网络的依赖。它在将图像输入变换器架构之前，将图像划分为一系列补丁及每个补丁的展平像素。SETR
    模型有三种变体，具有不同的模型大小和性能水平（Zheng 等，[2021](#bib.bib207)）。
- en: IBOT The IBOT model, described above in the Image Classification section, is
    also capable of performing the Image Classification task.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IBOT：上述图像分类部分中描述的 IBOT 模型也能够执行图像分类任务。
- en: 6.2.5 Image Generation
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5 图像生成
- en: Image generation is a challenging task in computer vision, and transformer-based
    models have shown promising results in this area due to their parallel computational
    capability. This task involves generating new images using existing image pixels
    as input. It can be used for object reconstruction and data augmentation (van den
    Oord et al., [2016](#bib.bib119), Liu et al., [2017](#bib.bib99)). While several
    text-to-image generation models exist, we focus on image generation models that
    use image pixels without any other type of data. In Table LABEL:tab:image_generation,
    we discuss some transformer-based models that have demonstrated exceptional performance
    in image generation tasks.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成是计算机视觉中的一个具有挑战性的任务，基于变换器的模型由于其并行计算能力在这一领域显示出良好的前景。该任务涉及使用现有图像像素作为输入生成新图像。它可以用于对象重建和数据增强（van
    den Oord 等，[2016](#bib.bib119)，Liu 等，[2017](#bib.bib99)）。尽管存在多个文本到图像生成模型，但我们专注于那些仅使用图像像素而不依赖其他类型数据的图像生成模型。在表
    LABEL:tab:image_generation 中，我们讨论了一些在图像生成任务中表现出色的变换器模型。
- en: 'Table 13: Transformer models for natural image processing - image generation'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 13：自然图像处理的变换器模型 - 图像生成
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 变换器模型 | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Image Transformer (Parmar et al., [2018](#bib.bib124)) | Image Generation
    | 2018 | Encoder & Decoder | Not mentioned | N/A | ImageNet, CIFAR-10, CelebA
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Image Transformer (Parmar et al., [2018](#bib.bib124)) | 图像生成 | 2018 | 编码器
    & 解码器 | 未提及 | N/A | ImageNet, CIFAR-10, CelebA |'
- en: '| I-GPT (Chen et al., [2020b](#bib.bib20)) | Image Generation | 2020 | Decoder
    | Yes | BooksCorpus dataset, 1B Word Benchmark | SNLI, MultiNLI, Question NLI,
    RTE, SciTail, RACE, Story Cloze, MSR Paraphrase Corpus, Quora Question Pairs,
    STS Benchmark, Stanford Sentiment Treebank-2, CoLA |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| I-GPT (Chen et al., [2020b](#bib.bib20)) | 图像生成 | 2020 | 解码器 | 是 | BooksCorpus
    数据集, 1B Word Benchmark | SNLI, MultiNLI, Question NLI, RTE, SciTail, RACE, Story
    Cloze, MSR Paraphrase Corpus, Quora Question Pairs, STS Benchmark, Stanford Sentiment
    Treebank-2, CoLA |'
- en: '| VideoGPT (Yan et al., [2021](#bib.bib192)) | Video Generation | 2021 | Decoder
    | No | NA | BAIR RobotNet, Moving MNIST, ViZDoom, UCF-101, Tumblr GIF |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| VideoGPT (Yan et al., [2021](#bib.bib192)) | 视频生成 | 2021 | 解码器 | 否 | NA |
    BAIR RobotNet, Moving MNIST, ViZDoom, UCF-101, Tumblr GIF |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image Transformer: Image Transformer is an autoregressive sequence generative
    model that uses the self-attention mechanism for image generation. This model
    generates new pixels and increases the size of the image by utilizing the attention
    mechanism on local pixels. It uses both the encoder and decoder module of the
    transformer, but does not use masking in the encoder. The encoder layer is used
    less than the decoder for better performance on image generation. Image Transformer
    is a remarkable model in the field of image generation (Parmar et al., [2018](#bib.bib124)).'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Image Transformer：Image Transformer 是一种自回归序列生成模型，使用自注意力机制进行图像生成。该模型通过在局部像素上利用注意力机制来生成新像素并增加图像大小。它使用了
    Transformer 的编码器和解码器模块，但在编码器中不使用遮罩。为了在图像生成方面获得更好的性能，编码器层的使用频率低于解码器。Image Transformer
    是图像生成领域的一个卓越模型（Parmar et al., [2018](#bib.bib124)）。
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'I-GPT: I-GPT or Image GPT is an image generative model that utilizes the GPT-2
    model for training to auto-regressively predict pixels by learning image representation,
    without using the 2D image. BERT motifs can also be used during pre-training.
    I-GPT has four variants based on the number of parameters: IGPT-S (76M parameters),
    IGPT-M (455M parameters), IGPT-L (1.4B parameters), and IGPT-XL (6.8M parameters),
    where models with higher parameters have more validation losses (Chen et al.,
    [2020b](#bib.bib20)).'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: I-GPT：I-GPT 或 Image GPT 是一种图像生成模型，利用 GPT-2 模型进行训练，通过学习图像表示来自回归地预测像素，而不使用 2D
    图像。在预训练期间，也可以使用 BERT 模式。I-GPT 基于参数数量有四种变体：IGPT-S（76M 参数）、IGPT-M（455M 参数）、IGPT-L（1.4B
    参数）和 IGPT-XL（6.8M 参数），其中参数更多的模型具有更高的验证损失（Chen et al., [2020b](#bib.bib20)）。
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VideoGPT: VideoGPT is a generative model that combines two classes of architecture:
    likelihood-based models and VAE (Vector Quantized Variational Autoencoder). The
    aim of this combination is to create a model that is easy to maintain and use,
    as well as resource-efficient, while also being able to encode spatio-temporal
    correlations in video frames. VideoGPT has shown remarkable results compared to
    other models, particularly in tests conducted on the “BAIR Robot Pushing” dataset
    (Yan et al., [2021](#bib.bib192)).'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VideoGPT：VideoGPT 是一种生成模型，结合了两类架构：基于似然的模型和 VAE（向量量化变分自编码器）。这种组合的目的是创建一个易于维护和使用、资源高效的模型，同时能够对视频帧中的时空相关性进行编码。与其他模型相比，VideoGPT
    在“BAIR Robot Pushing”数据集上的测试中表现出了显著的结果（Yan et al., [2021](#bib.bib192)）。
- en: 6.3 Medical Image processing
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 医学图像处理
- en: The diagnosis of pathologies based on medical images is often criticized as
    complicated, time-consuming, error-prone, and subjective (López-Linares et al.,
    [2020](#bib.bib104)). To overcome these challenges, alternative solutions such
    as deep learning approaches have been explored. Deep learning has made great progress
    in many other applications, such as Natural Language Processing and Computer Vision.
    Although transformers have been successfully applied in various domains, their
    application to medical images is still relatively new. Other deep learning approaches
    such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),
    and Generative Adversarial Networks (GAN) are commonly used. This survey aims
    to provide a comprehensive overview of the various transformer models developed
    for processing medical images.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 基于医学图像的病理诊断常常被批评为复杂、耗时、易出错且主观（López-Linares et al., [2020](#bib.bib104)）。为了克服这些挑战，已经探索了如深度学习方法等替代解决方案。深度学习在许多其他应用中取得了巨大进展，例如自然语言处理和计算机视觉。尽管变压器模型在多个领域取得了成功，但它们在医学图像中的应用仍然相对较新。其他深度学习方法，如卷积神经网络（CNN）、递归神经网络（RNN）和生成对抗网络（GAN），也被广泛使用。本调查旨在提供对用于处理医学图像的各种变压器模型的全面概述。
- en: 6.3.1 Medical Image Segmentation
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 医学图像分割
- en: Image segmentation refers to the task of grouping parts of the image that belong
    to the same category. In general, encoder-decoder architectures are commonly used
    for image segmentation (López-Linares et al., [2020](#bib.bib104)). In some cases,
    image segmentation is performed upstream of the classification task to improve
    the accuracy of the classification results (Wang et al., [2022c](#bib.bib180)).
    The most frequently used loss functions in image segmentation are Pixel-wise cross-entropy
    loss and Dice Loss (López-Linares et al., [2020](#bib.bib104)). Common applications
    of medical image segmentation include detecting lesions, identifying cancer as
    benign or malignant, and predicting disease risk. This paper presents a comprehensive
    overview of relevant models used in medical image segmentation. Table LABEL:tab:segmentation
    provides a summary of these models.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是指将图像中属于同一类别的部分进行分组的任务。通常，编码器-解码器架构常用于图像分割（López-Linares et al., [2020](#bib.bib104)）。在某些情况下，图像分割是在分类任务之前进行的，以提高分类结果的准确性（Wang
    et al., [2022c](#bib.bib180)）。在图像分割中最常用的损失函数是像素级交叉熵损失和骰子损失（López-Linares et al.,
    [2020](#bib.bib104)）。医学图像分割的常见应用包括检测病变、判断癌症是良性还是恶性以及预测疾病风险。本文提供了用于医学图像分割的相关模型的全面概述。表
    LABEL:tab:segmentation 对这些模型进行了总结。
- en: 'Table 14: Transformer models for medical image segmentation'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 医学图像分割的变压器模型'
- en: '| Transformer Name | Field of application | Year | Fully Transformer Architecture
    | Image type | Transformer Task | Dataset |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 变压器名称 | 应用领域 | 年份 | 完全变压器架构 | 图像类型 | 变压器任务 | 数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FTN (He et al., [2022](#bib.bib57)) | Skin lesion | 2022 | YES | 2D | Image
    segmentation / classification | ISIC 2018 dataset |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| FTN (He et al., [2022](#bib.bib57)) | 皮肤病变 | 2022 | YES | 2D | 图像分割 / 分类
    | ISIC 2018 数据集 |'
- en: '| RAT-Net (Zhu et al., [2022](#bib.bib212)) | Oncology (breast cancer) | 2022
    | NO | 3D ultrasound | Image segmentation | a dataset of 256 subjects(330 Automatic
    Breast Ultrasound images for each patient) |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| RAT-Net (Zhu et al., [2022](#bib.bib212)) | 肿瘤学（乳腺癌） | 2022 | NO | 3D 超声
    | 图像分割 | 256 名受试者的数据集（每位患者 330 张自动乳腺超声图像） |'
- en: '| nnFormer (Zhou et al., [2021a](#bib.bib209)) | Brain tumor multi-organ cardiac
    diagnosis | 2022 | YES | 3D | Image segmentation | Medical Segmentation Decathlon
    (MSD), Synapse multiorgan segmentation, Automatic Cardiac Diagnosis Challenge
    (ACDC) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| nnFormer (Zhou et al., [2021a](#bib.bib209)) | 脑肿瘤多脏器心脏诊断 | 2022 | YES |
    3D | 图像分割 | 医学分割十项全能（MSD），Synapse 多脏器分割，自动心脏诊断挑战（ACDC） |'
- en: '| TransConver (Liang et al., [2022](#bib.bib94)) | Brain tumor | 2022 | NO
    | 2D/3D | Image Segmentation | MICCAI BraTS2019, MICCAI BraTS2018 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| TransConver (Liang et al., [2022](#bib.bib94)) | 脑肿瘤 | 2022 | NO | 2D/3D
    | 图像分割 | MICCAI BraTS2019, MICCAI BraTS2018 |'
- en: '| SwinBTS (Jiang et al., [2022b](#bib.bib72)) | Brain tumor | 2022 | NO | 3D
    | Image Segmentation | BraTS 2019, BraTS 2020, BraTS 2021 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| SwinBTS (Jiang et al., [2022b](#bib.bib72)) | 脑肿瘤 | 2022 | NO | 3D | 图像分割
    | BraTS 2019, BraTS 2020, BraTS 2021 |'
- en: '| MTPA_Unet (Jiang et al., [2022a](#bib.bib71)) | Retinal vessel | 2022 | NO
    | 2D | Image segmentation | DRIVE, CHASE DB1, and STARE Datasets |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| MTPA_Unet (Jiang et al., [2022a](#bib.bib71)) | 视网膜血管 | 2022 | NO | 2D |
    图像分割 | DRIVE, CHASE DB1 和 STARE 数据集 |'
- en: '| Dilated Transformer (Shen et al., [2022b](#bib.bib151)) | Oncology (Breast
    Cancer) | 2022 | NO | 2D ultrasound | Image segmentation | 2 small breast ultrasound
    image datasets |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 扩张变换器 (Shen et al., [2022b](#bib.bib151)) | 肿瘤学（乳腺癌） | 2022 | 否 | 2D 超声波
    | 图像分割 | 2 个小型乳腺超声图像数据集 |'
- en: '| TFNet (Wang et al., [2021b](#bib.bib179)) | Oncology (Breast lesion) | 2022
    | NO | 2D ultrasound | Image Segmentation | BUSI Dataset DDTI Dataset |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| TFNet (Wang et al., [2021b](#bib.bib179)) | 肿瘤学（乳腺病变） | 2022 | 否 | 2D 超声波
    | 图像分割 | BUSI 数据集 DDTI 数据集 |'
- en: '| Chest L-Transformer (Gu et al., [2022](#bib.bib45)) | Chest radiograph /
    Thoracic diseases | 2022 | NO | 2D | Image Classification / Segmentation | SIIM-ACR
    Pneumothorax Segmentation dataset contains 12,047 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 胸部 L-Transformer (Gu et al., [2022](#bib.bib45)) | 胸部 X 光 / 胸部疾病 | 2022 |
    否 | 2D | 图像分类 / 分割 | SIIM-ACR 气胸分割数据集包含 12,047 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FTN: FTN is a transformer-based architecture developed specifically for segmenting
    and classifying 2D images of skin lesions. It comprises of 5 layers, where each
    layer has a tokenization module known as SWT ”Sliding Window Tokenization” and
    a transformer module. The model is segregated into encoders and decoders for the
    segmentation task, while only an encoder is needed for classification tasks. To
    improve computational efficiency and storage optimization, MSPA ”Multi-head Spatial
    Pyramid Attention” is utilized in the ”transformer” module instead of the traditional
    multi-head attention (MHA). In comparison to CNN, FTN has demonstrated superior
    performance on 10,025 images extracted from the publicly available ISIC 2018 dataset
    (He et al., [2022](#bib.bib57)).'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FTN：FTN 是一种基于变换器的架构，专门用于对皮肤病变的 2D 图像进行分割和分类。它由 5 层组成，每层都有一个称为 SWT ”Sliding Window
    Tokenization”的标记化模块和一个变换器模块。该模型被分为用于分割任务的编码器和解码器，而分类任务仅需要一个编码器。为了提高计算效率和存储优化，MSPA
    ”Multi-head Spatial Pyramid Attention” 在 ”transformer” 模块中取代了传统的多头注意力（MHA）。与 CNN
    相比，FTN 在从公开的 ISIC 2018 数据集中提取的 10,025 张图像上展示了更优的性能（He et al., [2022](#bib.bib57)）。
- en: •
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RAT-Net: The primary objective of RAT-Net (Region Aware Transformer Network)
    is to replace the laborious and time-consuming manual task of detecting lesion
    contours in 3D ABUS (Automatic Breast Ultrasound) images. Compared to other state-of-the-art
    models proposed for medical image segmentation, RAT-Net has shown excellent performance.
    It is based on the SegFormer Transformer model, which is used to encode input
    images and determine the regions that are more relevant for lesion segmentation
    (Zhu et al., [2022](#bib.bib212)).'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RAT-Net：RAT-Net（Region Aware Transformer Network）的主要目标是取代在 3D ABUS（自动乳腺超声）图像中检测病变轮廓的繁琐且耗时的手动任务。与其他针对医学图像分割提出的最先进模型相比，RAT-Net
    展现了卓越的性能。它基于 SegFormer 变换器模型，用于编码输入图像并确定对病变分割更相关的区域（Zhu et al., [2022](#bib.bib212)）。
- en: •
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'nnFormer: is a model that uses Transformer architecture for segmentation of
    3D medical images. The experiments were performed on 484 brain tumor images, 30
    multi-organ scans, and 100 cardiac diagnosis images. Instead of using the conventional
    attention mechanism, nnFormer introduced LV-MSA “Volume-based Multi-head Self-attention”
    and GV-MSA ”Global Volume-based Multi-head Self-attention” to reduce the computational
    complexity. Additionally, nnFormer employs multiple convolution layers with small
    kernels in the encoder instead of large convolution kernels as in other visual
    transformers (Zhou et al., [2021a](#bib.bib209)).'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: nnFormer：是一种利用变换器架构进行 3D 医学图像分割的模型。实验在 484 张脑肿瘤图像、30 张多脏器扫描图像和 100 张心脏诊断图像上进行。nnFormer
    引入了 LV-MSA “Volume-based Multi-head Self-attention”和 GV-MSA ”Global Volume-based
    Multi-head Self-attention” 来减少计算复杂性，而不是使用传统的注意力机制。此外，nnFormer 在编码器中采用了多个小卷积层，而不是像其他视觉变换器中使用的大卷积核（Zhou
    et al., [2021a](#bib.bib209)）。
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TransConver: It combines CNN and SWIN transformers in parallel to extract global
    and local features simultaneously is proposed. The transformer block employs a
    cross-attention mechanism to merge semantically different global and local features.
    The network is designed to process both 2D and 3D brain tumor images and is trained
    on 335 cases from the training dataset of MICCAI BraTS2019\. It is evaluated on
    66 cases from MICCAI BraTS2018 and 125 cases from MICCAI BraTS2019 (Liang et al.,
    [2022](#bib.bib94)).'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TransConver：该模型将CNN和SWIN Transformer并行结合，以同时提取全局和局部特征。Transformer块采用交叉注意力机制来合并语义上不同的全局和局部特征。网络设计用于处理2D和3D脑肿瘤图像，并在MICCAI
    BraTS2019的335个训练数据集案例上进行训练。在MICCAI BraTS2018的66个案例和MICCAI BraTS2019的125个案例上进行了评估（Liang
    et al., [2022](#bib.bib94)）。
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SwinBTS: is a recently developed model that addresses the segmentation of 3D
    medical images by combining the Swin Transformer with CNN. It adopts an encoder-decoder
    architecture that applies the Swin Transformer to both the encoder and decoder.
    In addition, SwinBTS incorporates an advanced feature extraction module called
    ETrans (Enhanced Transformer) that follows the transformer approach and leverages
    convolution techniques (Jiang et al., [2022b](#bib.bib72)).'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SwinBTS：这是一个最近开发的模型，通过将Swin Transformer与CNN结合来处理3D医学图像的分割问题。它采用了编码器-解码器架构，将Swin
    Transformer应用于编码器和解码器。此外，SwinBTS还包含一个名为ETrans（增强型Transformer）的先进特征提取模块，该模块遵循Transformer方法并利用卷积技术（Jiang
    et al., [2022b](#bib.bib72)）。
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MTPA_Unet: (Multi-scale Transformer-Position Attention Unet) is a model that
    has been evaluated on several publicly recognized retinal datasets to enhance
    the performance of retinal image segmentation tasks. This model combines CNN and
    transformer architectures sequentially to accurately capture local and global
    image information. To capture long-term dependencies between pixels as well as
    contextual information about each pixel location, this model employs TPA (Transformer
    Position Attention), which is a combination of MSA (Multi-headed Self-Attention)
    and ”Position Attention Module”. Additionally, to optimize the model’s extraction
    ability, feature map inputs of different resolutions are implemented due to the
    detailed information contained in retinal images (Jiang et al., [2022a](#bib.bib71)).'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MTPA_Unet：（多尺度Transformer-位置注意力Unet）是一个在多个公认的视网膜数据集上评估的模型，旨在提升视网膜图像分割任务的性能。该模型将CNN和Transformer架构顺序结合，以准确捕捉图像的局部和全局信息。为了捕捉像素之间的长期依赖关系以及每个像素位置的上下文信息，该模型使用TPA（Transformer位置注意力），它结合了MSA（多头自注意力）和“位置注意力模块”。此外，为了优化模型的提取能力，由于视网膜图像中包含的详细信息，该模型实现了不同分辨率的特征图输入（Jiang
    et al., [2022a](#bib.bib71)）。
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TFNet: TFNet, aims to segment 2D ultrasound images of breast lesions by combining
    CNN with a transformer architecture. To address the challenge of lesions with
    different scales and variable intensities, CNN is employed as a backbone to extract
    features from the images, resulting in 3 high-level features containing semantic
    information and 1 low-level feature. These high-level features are fused through
    a Transformer Fuse Module (TFM), while the low-level features are fused via skip
    connection. The transformer module includes two main parts: Vanilla Multi-Head
    Self-Attention to capture the long-range dependency between sequences and MultiHead
    Channel-Attention (MCA) to detect dependencies between channels. To enhance the
    model’s performance, novel loss functions are introduced, resulting in superior
    segmentation performance compared to other models. This approach is evaluated
    on a range of ultrasound image datasets, demonstrating excellent segmentation
    results (Wang et al., [2021b](#bib.bib179)).'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TFNet：TFNet旨在通过将CNN与Transformer架构结合来分割乳腺病变的2D超声图像。为了解决不同尺度和可变强度的病变问题，CNN被用作提取图像特征的主干，从而得到3个包含语义信息的高级特征和1个低级特征。这些高级特征通过Transformer融合模块（TFM）进行融合，而低级特征则通过跳跃连接融合。Transformer模块包括两个主要部分：Vanilla多头自注意力以捕捉序列之间的长程依赖关系，以及多头通道注意力（MCA）以检测通道之间的依赖关系。为了提升模型性能，引入了新的损失函数，与其他模型相比，表现出卓越的分割性能。该方法在多个超声图像数据集上进行了评估，展示了优异的分割结果（Wang
    et al., [2021b](#bib.bib179)）。
- en: •
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dilated Transformer: The (DT) model has been developed for the segmentation
    of 2D ultrasound images from small datasets of breast cancer using transformer
    architecture. Standard transformer models require large pre-training datasets
    to generate high-quality segmentation results, but DT overcomes this challenge
    by implementing the ”Residual Axial Attention” mechanism for segmenting images
    from small breast ultrasound datasets. This approach applies attention to a single
    axis, namely the height axis and the width axis, instead of the whole feature
    map, which saves time and enhances computation efficiency (Shen et al., [2022b](#bib.bib151)).'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dilated Transformer：该（DT）模型已为小型乳腺癌数据集的 2D 超声图像分割而开发，采用 Transformer 架构。标准 Transformer
    模型需要大量预训练数据集以生成高质量的分割结果，但 DT 通过实施“Residual Axial Attention”机制来解决这一挑战，从而可以对来自小型乳腺超声数据集的图像进行分割。这种方法对单一轴线应用注意力，即高度轴和宽度轴，而不是整个特征图，从而节省时间并提高计算效率（Shen
    et al., [2022b](#bib.bib151)）。
- en: •
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Chest L-Transformer: This model is developed for the segmentation and classification
    of chest radiograph images. It uses a combination of CNN and transformer architecture,
    where the CNN is used as a backbone to extract local features from the 2D images
    and the transformer block is applied to detect the location of lesions using attention
    mechanisms. By using transformers in chest radiograph images, the model focuses
    more on areas where the disease may be more likely to occur, as opposed to treating
    them similarly using CNN alone (Gu et al., [2022](#bib.bib45)).'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chest L-Transformer：该模型用于胸部X光图像的分割和分类。它结合了 CNN 和 Transformer 架构，其中 CNN 用作骨干提取
    2D 图像的局部特征，而 Transformer 模块则利用注意力机制检测病变的位置。通过在胸部X光图像中使用 Transformers，模型更关注疾病可能发生的区域，而不是像仅使用
    CNN 时那样对所有区域进行相同处理（Gu et al., [2022](#bib.bib45)）。
- en: 6.3.2 Medical Image Classification
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 医学图像分类
- en: 'Image classification refers to the process of recognizing, extracting, and
    selecting different types of features from an image for classification using labels
    (Wang et al., [2020b](#bib.bib181)). Features in an image can be categorized into
    three types: low-level features, mid-level features, and high-level features (Wang
    et al., [2020b](#bib.bib181)). Deep learning networks are designed to extract
    high-level features. Common applications of medical image classification include
    the detection of lesions, the identification of cancers as benign or malignant,
    and the prediction of disease risk (Khan & Lee, [2023](#bib.bib78), Jungiewicz
    et al., [2023](#bib.bib75)). Table LABEL:tab:classification, provides an overview
    of several relevant examples of Transformers used in medical image classification.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是指识别、提取和选择图像中的不同类型特征，并使用标签进行分类的过程（Wang et al., [2020b](#bib.bib181)）。图像中的特征可以分为三种类型：低级特征、中级特征和高级特征（Wang
    et al., [2020b](#bib.bib181)）。深度学习网络被设计用来提取高级特征。医学图像分类的常见应用包括病变检测、癌症的良性或恶性鉴别以及疾病风险预测（Khan
    & Lee, [2023](#bib.bib78), Jungiewicz et al., [2023](#bib.bib75)）。表 LABEL:tab:classification
    提供了几种在医学图像分类中使用的 Transformer 的相关示例。
- en: 'Table 15: Transformer models for medical image classification'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：用于医学图像分类的 Transformer 模型
- en: '| Transformer Name | Field of application | Year | Fully Transformer Architecture
    | Image type | Transformer Task | Dataset |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 名称 | 应用领域 | 年份 | 是否完全 Transformer 架构 | 图像类型 | Transformer 任务
    | 数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CCT-based Model (Islam et al., [2022](#bib.bib67)) | Malaria Disease | 2022
    | NO | 2D images | Image Classification | National Library of Medicine malaria
    dataset |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 基于 CCT 的模型 (Islam et al., [2022](#bib.bib67)) | 疟疾 | 2022 | 否 | 2D 图像 | 图像分类
    | 国家医学图书馆疟疾数据集 |'
- en: '| Chest L-Transformer (Gu et al., [2022](#bib.bib45)) | Chest radiograph /
    Thoracic diseases | 2022 | NO | 2D | Image Classification / Segmentation | SIIM-ACR
    Pneumothorax Segmentation dataset contains 12,047 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Chest L-Transformer (Gu et al., [2022](#bib.bib45)) | 胸部X光 / 胸部疾病 | 2022
    | 否 | 2D | 图像分类 / 分割 | SIIM-ACR 气胸分割数据集包含 12,047 张 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CCT-based Model (Islam et al., [2022](#bib.bib67)): The model presented in
    this work is designed for classifying red blood cell (RBC) images as containing
    malaria parasites or not, by using Compact Convolutional Transformers (CCTs).
    The model input consists of image patches generated through convolutional operations
    and preprocessed by reshaping them to a fixed size. Unlike other vision transformer
    models, this model performs classification using sequence pooling instead of class
    tokens.'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于CCT的模型（Islam 等，[2022](#bib.bib67)）：本研究中提出的模型旨在通过使用紧凑卷积变换器（CCTs）将红细胞（RBC）图像分类为含有疟疾寄生虫或不含。该模型输入由通过卷积操作生成的图像块组成，并通过将其调整为固定大小进行预处理。与其他视觉变换器模型不同，该模型使用序列池化进行分类，而不是类标记。
- en: Compared to other deep learning models such as CNN, this model shows good performance
    in classifying RBC images. This satisfactory result was achieved by implementing
    a transformer architecture, using GRAD-CAM techniques to validate the learning
    process, and fine-tuning hyperparameters.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他深度学习模型如CNN相比，该模型在分类RBC图像方面表现良好。这个令人满意的结果是通过实施变换器架构、使用GRAD-CAM技术验证学习过程并微调超参数实现的。
- en: •
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Chest L-Transformer (Gu et al., [2022](#bib.bib45)): is a model designed for
    segmenting and classifying chest radiograph images (Gu et al., [2022](#bib.bib45)).
    The model utilizes a CNN backbone to extract local features from 2D images and
    a transformer block to apply attention mechanisms for detecting lesion locations.
    By incorporating transformers into the chest radiograph image analysis, the model
    is better able to attend to areas where disease may be more likely to occur, as
    opposed to traditional CNNs which treat all areas similarly.'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 胸部L-Transformer（Gu 等，[2022](#bib.bib45)）：这是一个设计用于分割和分类胸部X光图像的模型（Gu 等，[2022](#bib.bib45)）。该模型利用CNN主干从2D图像中提取局部特征，并使用变换器块应用注意机制以检测病变位置。通过将变换器纳入胸部X光图像分析中，该模型能够更好地关注疾病可能发生的区域，而传统的CNN则对所有区域一视同仁。
- en: 6.3.3 Medical Image Translation
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 医学图像翻译
- en: The field of research that involves altering the context (or domain) of an image
    without changing its original content is gaining traction. One example of this
    involves applying cartoon-style effects to images to change their appearance (Pang
    et al., [2022](#bib.bib122)). Image-to-image translation is a promising technique
    that can be utilized to synthesize medical images from non-corrupted sources with
    less cost and time, and it is also helpful for preparing medical images for registration
    or segmentation. Some of the most popular deep learning models developed for this
    area include “Pix2Pix” and “cyclic-consistency generative adversarial network”
    (GAN) (Yan et al., [2022b](#bib.bib191)). Table LABEL:tab:translation provides
    an overview of some relevant examples of ”Transformers” designed for medical image-to-image
    translation.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及在不改变原始内容的情况下改变图像上下文（或领域）的研究领域正在受到关注。其中一个例子是将卡通风格效果应用于图像以改变其外观（Pang 等，[2022](#bib.bib122)）。图像到图像的翻译是一种有前途的技术，可用于从未损坏的源中合成医学图像，降低成本和时间，同时也有助于为医学图像的配准或分割做准备。在这个领域开发的一些最受欢迎的深度学习模型包括“Pix2Pix”和“循环一致性生成对抗网络”（GAN）（Yan
    等，[2022b](#bib.bib191)）。表 LABEL:tab:translation 提供了一些针对医学图像到图像翻译设计的“Transformers”的相关示例概述。
- en: 'Table 16: Transformer models for medical image translation'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 表16：用于医学图像翻译的变换器模型
- en: '| Transformer Name | Field of application | Year | Fully Transformer Architecture
    | Image type | Transformer Task | Dataset |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 变换器名称 | 应用领域 | 年份 | 完全变换器架构 | 图像类型 | 变换器任务 | 数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MMTrans (Yan et al., [2022b](#bib.bib191)) | Magnetic resonance imaging (MRI)
    | 2022 | NO | 2D | Medical image-to-image translation | BraTs2018, fastMRI, The
    clinical brain MRI dataset |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| MMTrans（Yan 等，[2022b](#bib.bib191)） | 磁共振成像（MRI） | 2022 | 否 | 2D | 医学图像到图像翻译
    | BraTs2018、fastMRI、临床脑MRI数据集 |'
- en: '| TransCBCT (Chen et al., [2022c](#bib.bib23)) | Oncology (prostate Cancer)
    | 2022 | NO | 2D | Image Translation | 91 patients with prostate cancer |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| TransCBCT（Chen 等，[2022c](#bib.bib23)） | 肿瘤学（前列腺癌） | 2022 | 否 | 2D | 图像翻译
    | 91名前列腺癌患者 |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MMTrans (Yan et al., [2022b](#bib.bib191)): The MMtrans (Multi-Modal Medical
    Image Translation) model is proposed based on the GAN architecture and Swin transformer
    structure for performing medical image-to-image translation on Magnetic Resonance
    Imaging (MRI). Unlike other image-to-image translation frameworks, MMtrans utilizes
    the transformer to model long global dependencies to ensure accurate translation
    results. Moreover, MMtrans does not require images to be paired and pixel-aligned
    since it employs SWIN as a registration module adapted for paired and unpaired
    images, which makes it different from other architectures like Pix2Pix. The remaining
    modules of GAN use SwinIR as a generator module, and CNN as a discriminator module.'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MMTrans（Yan et al., [2022b](#bib.bib191)）：MMtrans（多模态医学图像翻译）模型基于GAN架构和Swin变换器结构，旨在对磁共振成像（MRI）进行医学图像到图像的翻译。与其他图像到图像翻译框架不同，MMtrans利用变换器建模长距离全局依赖，以确保准确的翻译结果。此外，MMtrans不需要将图像配对和像素对齐，因为它采用SWIN作为适用于配对和非配对图像的注册模块，这使其不同于像Pix2Pix这样的其他架构。GAN的其余模块使用SwinIR作为生成模块，CNN作为判别模块。
- en: •
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TransCBCT (Chen et al., [2022c](#bib.bib23)): A new architecture called TransCBT
    is proposed for the purpose of performing accurate radiotherapy by improving the
    quality of 2D images, specifically cone-beam computed tomography (CBCT), and generating
    synthetic 2D images (sCT) without damaging their structures. TransCBT integrates
    pure-transformer modeling and convolution approaches to facilitate the extraction
    of global information and enhances performance by introducing the multi-head self-attention
    method (SW-MSA). Another model that can improve the quality of CT images reconstructed
    via sinograms is the CCTR (Shi et al., [2022a](#bib.bib152)). In comparison to
    TransCBCT, CCTR experiments utilized a lung image database with 1010 patients,
    rather than the 91 patients used in TransCBCT.'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TransCBCT（Chen et al., [2022c](#bib.bib23)）：提出了一种名为TransCBT的新架构，旨在通过提高2D图像（特别是圆锥束计算机断层扫描（CBCT））的质量并生成合成2D图像（sCT）来实现准确的放射治疗，而不破坏其结构。TransCBT整合了纯变换器建模和卷积方法，以便提取全局信息，并通过引入多头自注意力方法（SW-MSA）来提高性能。另一个可以改善通过正弦图重建的CT图像质量的模型是CCTR（Shi
    et al., [2022a](#bib.bib152)）。与TransCBCT相比，CCTR实验使用了1010名患者的肺部图像数据库，而TransCBCT使用了91名患者的数据。
- en: 6.4 Multi-Modality
  id: totrans-504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 多模态
- en: The transformer has demonstrated its potential in multi-modality, which stems
    from the human ability to perceive and process information from various senses
    such as vision, hearing, and language. Multi-modality machine learning models
    are capable of processing and combining different types of data simultaneously.
    Natural language, vision, and speech are among the most common types of data handled
    by multi-modal models. Several popular tasks in multi-modality include visual
    question answering, classification and segmentation, visual captioning, commonsense
    reasoning, and text/image/video/speech generation. In this section, we present
    a selection of transformer-based multi-modal models for each of these tasks providing
    an overview of their key features and working methods.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在多模态中的潜力得到了展示，这源于人类从视觉、听觉和语言等各种感官中感知和处理信息的能力。多模态机器学习模型能够同时处理和结合不同类型的数据。自然语言、视觉和语音是多模态模型处理的最常见数据类型之一。在多模态中，一些流行的任务包括视觉问答、分类和分割、视觉描述、常识推理以及文本/图像/视频/语音生成。本节将介绍每个任务的一些基于变换器的多模态模型，提供其关键特征和工作方法的概述。
- en: 6.4.1 Visual Question Answering
  id: totrans-506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 视觉问答
- en: Visual question answering is a popular task that can be accomplished using multi-modal
    models. It involves combining NLP and computer vision to answer questions about
    an image or video. The goal is to understand the features of both textual and
    visual information and provide the correct answer. Typically, the models take
    an image or video and text as input and deliver text as output answers (Antol
    et al., [2015](#bib.bib4), Shih et al., [2016](#bib.bib155)). In this context,
    we have identified and discussed the significant transformer models for visual
    question-answering tasks in Table LABEL:tab:visual_question_answering.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答是一个热门任务，可以通过多模态模型来完成。它涉及将自然语言处理和计算机视觉结合起来，以回答关于图像或视频的问题。目标是理解文本和视觉信息的特征，并提供正确的答案。通常，这些模型将图像或视频和文本作为输入，并输出文本答案（Antol
    et al., [2015](#bib.bib4), Shih et al., [2016](#bib.bib155)）。在这种背景下，我们在表格LABEL:tab:visual_question_answering中确定并讨论了重要的变换器模型用于视觉问答任务。
- en: 'Table 17: Transformer models for multi-modality - visual question answering
    task'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '表17: 多模态 - 视觉问答任务的 Transformer 模型'
- en: '| Transformer Models | Processed Data type (i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 处理的数据类型 (输入/输出) | 完成的任务 | 年份 | 架构 (编码器/解码器) | 预训练 (是/否)
    | 预训练数据集 | 数据集 (微调, 训练, 测试) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BERT-Verients (Huang et al., [2020](#bib.bib66), Tan & Bansal, [2019](#bib.bib167),
    Lu et al., [2019](#bib.bib106), Su et al., [2020](#bib.bib158), Chen et al., [2020c](#bib.bib24))
    | Text and Image | Question Answering, Common sense reasoning | 2019-2020 | Encoder
    | Yes | Pixel-BERT: MS-COCO, Visual Genome  LX-MERT: MS COCO,Visual Genome,VQA
    v2.0,GQA,VG-QA  ViLBERT: Visual Genome, COCO  VL-BERT: Conceptual Captions, BooksCorpus,
    English Wikipedia Uniter: COCO, VG, CC, SBU | Pixel-BERT: VQA 2.0 NLVR2, Flickr30K
    MS-COCO  LX-MERT: VQA,GQA,NLVR  ViLBERT: Conceptual Captions, Flickr30k  VL-BERT:
    VCR dataset, RefCOCO Uniter: COCO, Flickr30K, VG, CC, SBU |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| BERT-Verients (Huang et al., [2020](#bib.bib66), Tan & Bansal, [2019](#bib.bib167),
    Lu et al., [2019](#bib.bib106), Su et al., [2020](#bib.bib158), Chen et al., [2020c](#bib.bib24))
    | 文本和图像 | 问答，常识推理 | 2019-2020 | 编码器 | 是 | Pixel-BERT: MS-COCO, Visual Genome LX-MERT:
    MS COCO, Visual Genome, VQA v2.0, GQA, VG-QA ViLBERT: Visual Genome, COCO VL-BERT:
    Conceptual Captions, BooksCorpus, English Wikipedia Uniter: COCO, VG, CC, SBU
    | Pixel-BERT: VQA 2.0 NLVR2, Flickr30K MS-COCO LX-MERT: VQA, GQA, NLVR ViLBERT:
    Conceptual Captions, Flickr30k VL-BERT: VCR 数据集, RefCOCO Uniter: COCO, Flickr30K,
    VG, CC, SBU |'
- en: '| VIOLET (Fu et al., [2021](#bib.bib39)) | Video and Text | Video Question
    Answering, Text-to-video retrieval, Visual-Text Matching | 2022 | Encoder | Yes
    | Conceptual Captions-3M, WebVid-2.5M, YT-Temporal-180M | MSRVTT, DiDeMo, YouCook2,
    LSMDC, TGIF-Action, TGITransition, TGIF-Frame, MSRVTT-MC, MSRVTT-QA, MSVD-QA,
    LSMDC-MC, LSMDC-FiB |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| VIOLET (Fu et al., [2021](#bib.bib39)) | 视频和文本 | 视频问答，文本到视频检索，视觉-文本匹配 | 2022
    | 编码器 | 是 | Conceptual Captions-3M, WebVid-2.5M, YT-Temporal-180M | MSRVTT, DiDeMo,
    YouCook2, LSMDC, TGIF-Action, TGITransition, TGIF-Frame, MSRVTT-MC, MSRVTT-QA,
    MSVD-QA, LSMDC-MC, LSMDC-FiB |'
- en: '| GIT (Wang et al., [2022a](#bib.bib176)) | Image and Text | Image Classification,
    Image/video captioning, Question answering | 2022 | Encoder & Decoder | Yes |
    combination of COCO, SBU, CC3M, VG, GITL, ALT200M and CC12M | Karpathy split-COCO,
    Flickr30K, no caps, TextCaps, VizWiz-Captions, CUTE, TextOCR |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| GIT (Wang et al., [2022a](#bib.bib176)) | 图像和文本 | 图像分类，图像/视频描述，问答 | 2022
    | 编码器 & 解码器 | 是 | COCO、SBU、CC3M、VG、GITL、ALT200M 和 CC12M 的组合 | Karpathy 分割-COCO,
    Flickr30K, 无描述, TextCaps, VizWiz-Captions, CUTE, TextOCR |'
- en: '| SIMVLM (Wang et al., [2022d](#bib.bib183)) | Image and Text | Visual Question
    answering, image captioning | 2022 | Encoder & Decoder | Yes | ALIGN & Colossal
    Clean Crawled Corpus (C4) datasets | SNLI-VE, SNLI, MNLI, Multi30k, 10% ALIGN
    , CC-3M |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| SIMVLM (Wang et al., [2022d](#bib.bib183)) | 图像和文本 | 视觉问答，图像描述 | 2022 | 编码器
    & 解码器 | 是 | ALIGN & Colossal Clean Crawled Corpus (C4) 数据集 | SNLI-VE, SNLI, MNLI,
    Multi30k, 10% ALIGN, CC-3M |'
- en: '| BLIP (Li et al., [2022](#bib.bib90)) | Image, Video and Text | Question Answering,
    Image Captioning, image-text retrieval | 2022 | Encoder & Decoder | Yes | Bootstrapped
    dataset- COCO, VG, SBU, CC3M, CC12M, LAION | COCO, Flickr30K, NoCaps, MSRVTT |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| BLIP (Li et al., [2022](#bib.bib90)) | 图像、视频和文本 | 问答，图像描述，图像-文本检索 | 2022
    | 编码器 & 解码器 | 是 | Bootstrap 数据集- COCO, VG, SBU, CC3M, CC12M, LAION | COCO, Flickr30K,
    NoCaps, MSRVTT |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: •
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BERT-Variants: Following the successful application of BERT-based models in
    NLP and computer vision tasks, several BERT-based models have demonstrated significant
    improvements in multi-modal tasks, particularly in question answering and commonsense
    reasoning. Currently, there are two distinct types of BERT-based models available
    in the literature: (i) Single-Stream Models and (ii) Two-Stream Models.'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERT 变体：继 BERT 基础模型在 NLP 和计算机视觉任务中的成功应用之后，几种基于 BERT 的模型在多模态任务中，特别是在问答和常识推理方面，展示了显著的改进。目前，文献中有两种不同类型的
    BERT 基础模型：（i）单流模型和（ii）双流模型。
- en: Single-Stream Models, such as VL-BERT, Uniter, etc., encode both modalities
    (text and image) within the same module. In contrast, Two-Stream Models, such
    as VilBERT, LXMERT, etc., process text and image through separate modules. Both
    types of models have been shown to yield promising results in various multi-modal
    tasks.
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单流模型，如 VL-BERT、Uniter 等，在同一模块中编码文本和图像两种模态。相比之下，双流模型，如 VilBERT、LXMERT 等，通过不同的模块处理文本和图像。这两种类型的模型在各种多模态任务中均显示出有前景的结果。
- en: •
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ViLBERT: ViLBERT is a two-stream model that is trained on text-image pairs
    and then passed both of the modules through co-attention, which helps to detect
    the important features of both text and images (Lu et al., [2019](#bib.bib106)).
    VLBERT, on the other hand, is a single-stream model that is pre-trained and takes
    both the image and text embedding features as input, making this model simple
    yet powerful (Su et al., [2020](#bib.bib158)). Uniter represents Universal Image-Text
    Representation, which is a large-scale pre-trained model completed through masking
    (Chen et al., [2020c](#bib.bib24)). Pixel-BERT is built using a combination of
    convolutional neural network (CNN) to extract image pixels and an encoder to extract
    text tokens, while the BERT-based transformer works as the cross-modality module.
    To capture all the spatial information from the image, Pixel-BERT takes the whole
    image as input, whereas other models extract image features from the regions (Huang
    et al., [2020](#bib.bib66)). Finally, LXMERT stands for Learning Cross-Modality
    Encoder Representations from Transformers. It processes the image and text through
    two different modules and is built with three encoders. This pre-trained model
    follows masked modeling and cross-modality for pre-training, which captures better
    relationships between text and images (Tan & Bansal, [2019](#bib.bib167)).'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ViLBERT：ViLBERT 是一种双流模型，经过文本-图像对的训练，并通过共同注意力机制处理这两个模块，这有助于检测文本和图像的关键特征（Lu et
    al., [2019](#bib.bib106)）。另一方面，VLBERT 是一种单流模型，经过预训练，并将图像和文本嵌入特征作为输入，使该模型既简单又强大（Su
    et al., [2020](#bib.bib158)）。Uniter 代表通用图像-文本表示，这是一个通过遮蔽完成的大规模预训练模型（Chen et al.,
    [2020c](#bib.bib24)）。Pixel-BERT 结合了卷积神经网络（CNN）来提取图像像素和编码器来提取文本标记，而基于 BERT 的变压器则作为跨模态模块。为了捕获图像中的所有空间信息，Pixel-BERT
    以整个图像作为输入，而其他模型则从区域中提取图像特征（Huang et al., [2020](#bib.bib66)）。最后，LXMERT 代表从变压器中学习跨模态编码表示。它通过两个不同的模块处理图像和文本，并由三个编码器构建。该预训练模型遵循遮蔽建模和跨模态预训练，从而捕获文本和图像之间更好的关系（Tan
    & Bansal, [2019](#bib.bib167)）。
- en: •
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VIOLET: VIOLET is a neural network based on the transformer architecture that
    is designed to associate video with text. It consists of three modules: (1) Video
    Swin Transformer (VT), (2) Language Embedder (LE), and (3) Cross-modal Transformer
    (CT). VIOLET differs from other models in that it processes the temporal-spatial
    information from the video, rather than just extracting static images. This model
    has been evaluated on 12 datasets and has shown outstanding performance in many
    downstream tasks, such as Text-To-Video Retrieval and Video Question Answering.
    Furthermore, the authors propose a new pre-training task called ”Masked Visual
    Token Modeling,” which is used to pre-train VIOLET. This approach is combined
    with two other pre-training approaches, Masked Language Modeling and Visual-Text
    Matching, to achieve state-of-the-art performance on various benchmarks (Fu et al.,
    [2021](#bib.bib39)).'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VIOLET：VIOLET 是一个基于 Transformer 架构的神经网络，旨在将视频与文本关联起来。它由三个模块组成：（1）视频 Swin Transformer（VT），（2）语言嵌入器（LE），和（3）跨模态
    Transformer（CT）。VIOLET 的不同之处在于它处理视频中的时序空间信息，而不仅仅是提取静态图像。该模型在 12 个数据集上进行了评估，并在许多下游任务中表现出色，例如文本到视频检索和视频问答。此外，作者提出了一种新的预训练任务，称为“掩码视觉标记建模”，用于预训练
    VIOLET。这种方法与另外两种预训练方法——掩码语言建模和视觉文本匹配结合，以在各种基准测试中实现最先进的性能（Fu et al., [2021](#bib.bib39)）。
- en: •
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GIT: The Generative Image-to-Text Transformer, or GIT, is a multi-modal model
    designed to generate textual descriptions from visual images. This model employs
    both the encoder and decoder modules of the transformer architecture, using the
    image for encoding and decoding the text. To train the model, a large dataset
    of images paired with textual descriptions is used, allowing GIT to generate textual
    descriptions for previously unseen images. This approach has shown promising results
    in generating high-quality textual descriptions of images (Wang et al., [2022a](#bib.bib176)).'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GIT：生成图像到文本 Transformer（GIT）是一个多模态模型，旨在从视觉图像生成文本描述。该模型使用 Transformer 架构的编码器和解码器模块，利用图像进行编码和解码文本。为了训练该模型，使用了一个包含图像和文本描述配对的大型数据集，使
    GIT 能够为以前未见过的图像生成文本描述。这种方法在生成高质量图像文本描述方面显示出有希望的结果（Wang et al., [2022a](#bib.bib176)）。
- en: 'SimVLM & BLIP: Both SimVLM and BLIP are models that can perform the task of
    visual captioning, which involves generating textual descriptions of visual images.
    The highlights of these models can be found in that section.'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SimVLM 和 BLIP：SimVLM 和 BLIP 都是能够执行视觉字幕生成任务的模型，即生成视觉图像的文本描述。这些模型的亮点可以在该部分找到。
- en: 6.4.2 Classification & Segmentation
  id: totrans-527
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 分类与分割
- en: Multi-modal classification and segmentation are often considered related tasks
    that involve classifying or segmenting data based on multiple modalities, such
    as text, image/video, and speech. As segmentation often helps to classify the
    image, text, or speech. In multi-modal classification, the task is to classify
    data based on its similarity and features using multiple modalities. This can
    involve taking text, image/video, or speech as input and using all of these modalities
    to classify the data more accurately. Similarly, in multi-modal segmentation,
    the task is to segment data based on its features and use multiple modalities
    to achieve a more accurate segmentation. Both of these tasks require a deep understanding
    of the different forms of data and how they can be used together to achieve better
    classification or segmentation performance (Liu et al., [2022c](#bib.bib101),
    Mahesh & Renjit, [2020](#bib.bib108), Wu et al., [2016](#bib.bib184), Menze et al.,
    [2015](#bib.bib110)). In recent years, transformer models have shown promising
    results in multi-modal classification and segmentation tasks. In Table LABEL:tab:multi-modal_classification_&_segmentation,
    we highlight some of the significant transformer models that have been developed
    for these tasks.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态分类和分割通常被视为相关任务，它们涉及基于多种模态（如文本、图像/视频和语音）对数据进行分类或分割。由于分割通常有助于分类图像、文本或语音，因此在多模态分类中，任务是基于其相似性和特征使用多种模态对数据进行分类。这可能涉及将文本、图像/视频或语音作为输入，并利用所有这些模态来更准确地分类数据。同样，在多模态分割中，任务是基于特征对数据进行分割，并使用多种模态实现更准确的分割。这两项任务都需要对不同数据形式有深刻的理解，并知道如何将它们结合起来以实现更好的分类或分割性能
    (Liu et al., [2022c](#bib.bib101), Mahesh & Renjit, [2020](#bib.bib108), Wu et
    al., [2016](#bib.bib184), Menze et al., [2015](#bib.bib110))。近年来，Transformer 模型在多模态分类和分割任务中显示出了令人鼓舞的结果。在表
    LABEL:tab:multi-modal_classification_&_segmentation 中，我们突出了为这些任务开发的一些重要 Transformer
    模型。
- en: 'Table 18: Multi-modal Transformer models - classification & segmentation tasks'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '表18: 多模态 Transformer 模型 - 分类与分割任务'
- en: '| Transformer Models | Processed Data type (i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 处理数据类型（输入/输出） | 完成任务 | 年份 | 结构（编码器/解码器） | 预训练（是/否） | 预训练数据集
    | 数据集（微调、训练、测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| CLIP (Radford et al., [2021](#bib.bib130)) | Image and Text | image Classification
    | 2021 | Encoder | Yes | Pre-training dataset from internet for CLIP | ImageNet,
    ImageNet V2, ImageNet Rendition, ObjectNet, ImageNet Sketch, ImageNet Adversarial
    30 datasets |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| CLIP (Radford et al., [2021](#bib.bib130)) | 图像和文本 | 图像分类 | 2021 | 编码器 |
    是 | CLIP 的互联网预训练数据集 | ImageNet, ImageNet V2, ImageNet Rendition, ObjectNet, ImageNet
    Sketch, ImageNet Adversarial 30 数据集 |'
- en: '| VATT (Akbari et al., [2021](#bib.bib3)) | Video, Audio and Text | Audio event
    classification, Image classification, Video action recognition, Text-To-Video
    retrieval | 2021 | Encoder | Yes | AudioSet, HowTo100M | UCF10,HMDB5, Kinetics-400,
    Kinetics-600,Moments in Time, ESC50, AudioSet, YouCook2, MSR-VTT, ImageNet |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| VATT (Akbari et al., [2021](#bib.bib3)) | 视频、音频和文本 | 音频事件分类、图像分类、视频动作识别、文本到视频检索
    | 2021 | 编码器 | 是 | AudioSet, HowTo100M | UCF10, HMDB5, Kinetics-400, Kinetics-600,
    Moments in Time, ESC50, AudioSet, YouCook2, MSR-VTT, ImageNet |'
- en: '| Unicoder-VL (Li et al., [2020a](#bib.bib88)) | Image and Text | Object Classification,
    Visual-linguistic Matching, visual commonsense reasoning, image-text retrieval
    | 2020 | Encoder | Yes | Conceptual Captions-3M, SBU Captions | MSCOCO, Flickr30K
    |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| Unicoder-VL (Li et al., [2020a](#bib.bib88)) | 图像和文本 | 对象分类、视觉语言匹配、视觉常识推理、图像文本检索
    | 2020 | 编码器 | 是 | Conceptual Captions-3M, SBU Captions | MSCOCO, Flickr30K |'
- en: '| ViLT (Kim et al., [2021b](#bib.bib81)) | Image and Text | Visual Question
    Answering, Image text matching, Natural Language for Visual Reasoning | 2021 |
    Encoder | Yes | MS-COCO,Visual Genome, SBU Captions, Google Conceptual Captions
    | VQA 2.0, NLVR2, MSCOCO, Flickr30K |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| ViLT (Kim et al., [2021b](#bib.bib81)) | 图像和文本 | 视觉问答、图像文本匹配、用于视觉推理的自然语言
    | 2021 | 编码器 | 是 | MS-COCO, Visual Genome, SBU Captions, Google Conceptual Captions
    | VQA 2.0, NLVR2, MSCOCO, Flickr30K |'
- en: '| MBT (Nagrani et al., [2021](#bib.bib115)) | Audio and Visual | Audio-visual
    classification | 2022 | Encoder | Yes | VGGSoun, Kinetics400 and AS-500K, VGGSound
    | Audioset-mini and VGGSound, Moments In Time, Kinetics |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| MBT (Nagrani et al., [2021](#bib.bib115)) | 音频和视觉 | 音频视觉分类 | 2022 | 编码器 |
    是 | VGGSoun, Kinetics400 和 AS-500K, VGGSound | Audioset-mini 和 VGGSound, Moments
    In Time, Kinetics |'
- en: '| ALIGN (Jia et al., [2021](#bib.bib70)) | Image and Text | Visual Classification
    | 2021 | Encoder | Yes | ALIGN training data, 0% randomly sampled ALIGN training
    data, and CC-3M | Conceptual Captions-CC, Flickr30K, MSCOCO, ILSVRC-2012 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| ALIGN (Jia et al., [2021](#bib.bib70)) | 图像和文本 | 视觉分类 | 2021 | 编码器 | 是 |
    ALIGN 训练数据、0% 随机采样的 ALIGN 训练数据和 CC-3M | Conceptual Captions-CC、Flickr30K、MSCOCO、ILSVRC-2012
    |'
- en: '| Florence (Yuan et al., [2021](#bib.bib199)) | Image and Text | Classification,
    image caption, visual action recognition, Text-visual & visual-text retrieval
    | 2021 | Encoder | Yes | FLD-900M, ImageNet (Swin transformer & CLIP) | Web-scale
    by data curation, UniCL, ImageNet, COCO, Kinetics-600, Flickr30k, MSCOCO, SR-VTT
    |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| Florence (Yuan et al., [2021](#bib.bib199)) | 图像和文本 | 分类、图像字幕、视觉动作识别、文本-视觉和视觉-文本检索
    | 2021 | 编码器 | 是 | FLD-900M、ImageNet（Swin transformer & CLIP） | 通过数据策展的 Web 规模、UniCL、ImageNet、COCO、Kinetics-600、Flickr30k、MSCOCO、SR-VTT
    |'
- en: '| GIT (Wang et al., [2022a](#bib.bib176)) | Image and Text | Image Classification,
    Image/video captioning, Question answering | 2022 | Encoder & Decoder | Yes |
    combination of COCO, SBU, CC3M, VG, GITL, ALT200M and CC12M | Karpathy split-COCO,
    Flickr30K, no caps, TextCaps, VizWiz-Captions, CUTE, TextOCR |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| GIT (Wang et al., [2022a](#bib.bib176)) | 图像和文本 | 图像分类、图像/视频字幕、问答 | 2022
    | 编码器 & 解码器 | 是 | COCO、SBU、CC3M、VG、GITL、ALT200M 和 CC12M 的组合 | Karpathy 分割-COCO、Flickr30K、无字幕、TextCaps、VizWiz-Captions、CUTE、TextOCR
    |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: •
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CLIP: CLIP (Constructive Language-Image Pre-Training) is a multi-modal model
    that is trained in a supervised way with text and image data. This model simultaneously
    trains both the text and image through an encoder and predicts proper batches
    of text and image. CLIP is capable of understanding the relationship between text
    and images, and can generate images based on input text as well as generate text
    based on input images. CLIP has been shown to perform well on several benchmark
    datasets and is considered a state-of-the-art model for multi-modal tasks involving
    text and images (Radford et al., [2021](#bib.bib130)).'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CLIP: CLIP（构造性语言-图像预训练）是一个多模态模型，通过文本和图像数据进行监督训练。该模型通过编码器同时训练文本和图像，并预测适当的文本和图像批次。CLIP
    能够理解文本和图像之间的关系，并可以根据输入文本生成图像，也可以根据输入图像生成文本。CLIP 在多个基准数据集上表现良好，被认为是涉及文本和图像的多模态任务的最先进模型（Radford
    et al., [2021](#bib.bib130)）。'
- en: •
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VATT: which stands for Video, Audio, Text Transformer, is a multi-modal model
    based on the traditional transformer architecture without convolution layers.
    It is inspired by BERT and ViT and is pre-trained on two datasets using the Drop
    Token approach to optimize the training process. VATT is evaluated on 10 datasets
    containing videos, audio data, and text speech for 4 downstream tasks: Video action
    recognition, audio event classification, image classification, and text-to-video
    retrieval (Akbari et al., [2021](#bib.bib3)).'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VATT: 视频、音频、文本变换器（VATT）是一个基于传统变换器架构的多模态模型，没有卷积层。它受到 BERT 和 ViT 的启发，并使用 Drop
    Token 方法在两个数据集上进行预训练，以优化训练过程。VATT 在 10 个包含视频、音频数据和文本语音的数据集上进行评估，涉及 4 个下游任务：视频动作识别、音频事件分类、图像分类和文本到视频检索（Akbari
    et al., [2021](#bib.bib3)）。'
- en: •
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MBT: Multimodal Bottleneck Transformer (MBT) is a transformer-based model designed
    for processing both audio and visual data. The MBT model utilizes a bottleneck
    structure to focus on essential information for processing through the transformer
    architecture, thereby reducing the amount of data processed and minimizing the
    risk of overfitting. The bottleneck structure reduces the size of the model, training
    time, and computational cost. MBT has shown promising results in various multimodal
    tasks, such as audio-visual speech recognition and audio-visual event detection
    (Nagrani et al., [2021](#bib.bib115)).'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MBT: 多模态瓶颈变换器（MBT）是一个基于变换器的模型，旨在处理音频和视觉数据。MBT 模型利用瓶颈结构来专注于通过变换器架构处理的关键数据，从而减少处理的数据量并最小化过拟合的风险。瓶颈结构减少了模型的大小、训练时间和计算成本。MBT
    在各种多模态任务中显示出令人鼓舞的结果，例如音频-视觉语音识别和音频-视觉事件检测（Nagrani et al., [2021](#bib.bib115)）。'
- en: •
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ALIGN: it stands for Large-scale ImaGe and Noise-text embedding. This model
    is a large-scale model that uses vision-language representational learning with
    noisy text annotations. ALIGN is a pre-trained model which uses a dual-encoder
    and is trained on huge-sized noisy image-text pair datasets. The dataset scale
    is able to adjust for noise, eliminating the need for pre-processing. ALIGN uses
    the contrastive loss to train the model, considering both image-to-text and text-to-image
    classification losses (Jia et al., [2021](#bib.bib70)).'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ALIGN：它代表大规模图像和噪声文本嵌入。该模型是一个大规模模型，使用视觉-语言表示学习和噪声文本注释。ALIGN 是一个预训练模型，使用双编码器，并在大规模噪声图像-文本对数据集上进行训练。数据集规模能够调整噪声，消除预处理的需要。ALIGN
    使用对比损失来训练模型，考虑图像到文本和文本到图像分类损失（Jia等，[2021](#bib.bib70)）。
- en: •
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Florence: Florence is a visual-language representation model that is capable
    of handling multiple tasks. It is an encoder-based pre-trained model trained on
    web-scale image-text data, and it can handle high-resolution images. This model
    shows strong performance on classification tasks, as well as other tasks like
    object/action detection and question answering (Yuan et al., [2021](#bib.bib199)).'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Florence：Florence 是一种视觉-语言表示模型，能够处理多种任务。它是基于编码器的预训练模型，训练于网络规模的图像-文本数据上，并且能够处理高分辨率图像。该模型在分类任务以及对象/动作检测和问答等其他任务上表现出强大的性能（Yuan等，[2021](#bib.bib199)）。
- en: 'Unicoder-VL, GIT & ViLT: Unicoder-VL and ViLT models have been described in
    the Visual Commonsense Reasoning section. Both models can perform the Commonsense
    Reasoning task in addition to other tasks. However, the characteristics of GIT
    model can be found on the visual question-answering section.'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Unicoder-VL、GIT 和 ViLT：Unicoder-VL 和 ViLT 模型已在视觉常识推理部分描述。这两个模型除了可以执行常识推理任务外，还可以执行其他任务。然而，GIT
    模型的特点可以在视觉问答部分找到。
- en: 6.4.3 Visual Captioning
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3 视觉描述
- en: Visual captioning is a multi-modal task that involves both computer vision and
    NLP. The task aims to generate a textual description of an image, which requires
    a deep understanding of the relationship between image features and text. The
    visual captioning process usually involves several steps, starting with image
    processing, followed by encoding the features into vectors that can be used by
    the NLP model. These encoded vectors are then decoded into text, typically through
    generative NLP models. Although it is a complex process, visual captioning has
    a wide range of applications (Yu et al., [2020](#bib.bib197), Hossain et al.,
    [2019](#bib.bib62)). In this section, we discuss significant transformer models
    for visual captioning tasks (see Table LABEL:tab:visual_captioning).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉描述是一项多模态任务，涉及计算机视觉和自然语言处理（NLP）。该任务旨在生成图像的文本描述，这需要深刻理解图像特征与文本之间的关系。视觉描述过程通常包括多个步骤，从图像处理开始，接着将特征编码为可以被NLP模型使用的向量。这些编码的向量随后通过生成式NLP模型解码成文本。尽管这是一个复杂的过程，视觉描述具有广泛的应用（Yu等，[2020](#bib.bib197)，Hossain等，[2019](#bib.bib62)）。在本节中，我们讨论了视觉描述任务中的重要变换器模型（见表
    LABEL:tab:visual_captioning）。
- en: 'Table 19: Multi-modal Transformer models - visual captioning task'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：多模态变换器模型 - 视觉描述任务
- en: '| Transformer Models | Processed Data type (i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 变换器模型 | 处理的数据类型（输入/输出） | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BLIP (Li et al., [2022](#bib.bib90)) | Image, Video and Text | Image Captioning,
    Question Answering, image-text retrieval | 2022 | Encoder & Decoder | Yes | Bootstrapped
    dataset- COCO, VG, SBU, CC3M, CC12M, LAION | COCO, Flickr30K, NoCaps, MSRVTT |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| BLIP (Li et al., [2022](#bib.bib90)) | 图像、视频和文本 | 图像描述、问答、图像-文本检索 | 2022
    | 编码器 & 解码器 | 是 | 自举数据集 - COCO, VG, SBU, CC3M, CC12M, LAION | COCO, Flickr30K,
    NoCaps, MSRVTT |'
- en: '| SIMVLM (Wang et al., [2022d](#bib.bib183)) | Image and Text | Image captioning,
    Visual Question answering | 2022 | Encoder & Decoder | Yes | ALIGN & Colossal
    Clean Crawled Corpus (C4) datasets | SNLI-VE, SNLI, MNLI, Multi30k, 10% ALIGN
    , CC-3M |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| SIMVLM (Wang et al., [2022d](#bib.bib183)) | 图像和文本 | 图像描述、视觉问答 | 2022 | 编码器
    & 解码器 | 是 | ALIGN & Colossal Clean Crawled Corpus (C4) 数据集 | SNLI-VE, SNLI, MNLI,
    Multi30k, 10% ALIGN , CC-3M |'
- en: '| Florence (Yuan et al., [2021](#bib.bib199)) | Image and Text | Classification,
    image caption, visual action recognition, Text-visual & visual-text retrieval
    | 2021 | Encoder | Yes | FLD-900M, ImageNet (Swin transformer & CLIP) | Web-scale
    by data curation, UniCL, ImageNet, COCO, Kinetics-600, Flickr30k, MSCOCO, SR-VTT
    |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| Florence (Yuan et al., [2021](#bib.bib199)) | 图像和文本 | 分类、图像描述、视觉动作识别、文本-视觉
    & 视觉-文本检索 | 2021 | 编码器 | 是 | FLD-900M、ImageNet（Swin transformer & CLIP） | 通过数据策划的大规模网络数据、UniCL、ImageNet、COCO、Kinetics-600、Flickr30k、MSCOCO、SR-VTT
    |'
- en: '| GIT (Wang et al., [2022a](#bib.bib176)) | Image and Text | Image Classification,
    Image/video captioning, Question answering | 2022 | Encoder & Decoder | Yes |
    combination of COCO, SBU, CC3M, VG, GITL, ALT200M and CC12M | Karpathy split-COCO,
    Flickr30K, no caps, TextCaps, VizWiz-Captions, CUTE, TextOCR |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| GIT (Wang et al., [2022a](#bib.bib176)) | 图像和文本 | 图像分类、图像/视频描述、问答 | 2022
    | 编码器 & 解码器 | 是 | COCO、SBU、CC3M、VG、GITL、ALT200M 和 CC12M 的组合 | Karpathy split-COCO、Flickr30K、无标签、TextCaps、VizWiz-Captions、CUTE、TextOCR
    |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: •
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BLIP: Bootstrapping Language-Image Pre-training (BLIP) is a pre-trained model
    designed to enhance performance on various tasks through fine-tuning for specific
    tasks. This model utilizes a VLP (Vision and Language Pre-training) framework
    with an encoder-decoder module of the Transformer architecture, which uses noisy
    data with captions and is trained to remove noisy captions. BLIP is capable of
    performing a range of downstream tasks, including image captioning, question answering,
    image-text retrieval, and more (Li et al., [2022](#bib.bib90)).'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BLIP：Bootstrapping Language-Image Pre-training (BLIP) 是一个预训练模型，旨在通过针对特定任务的微调来提升多种任务的性能。该模型利用了一个
    VLP（视觉和语言预训练）框架，配备了 Transformer 架构的编码器-解码器模块，使用带有标签的噪声数据，并训练去除噪声标签。BLIP 能够执行一系列下游任务，包括图像描述、问答、图像-文本检索等（Li
    et al., [2022](#bib.bib90)）。
- en: •
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SimVLM: short for SIMple Visual Language Model, SimVLM is a pre-trained model
    that uses weak supervision methods for training. This approach provides the model
    with greater flexibility and scalability. Instead of using pixel patch projection,
    this model uses the full image as patches and is trained with a language model.
    As a result of these methods, SimVLM is capable of performing various tasks, with
    question answering being one of its significant strengths (Wang et al., [2022d](#bib.bib183)).'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SimVLM：SIMple Visual Language Model 的简称，SimVLM 是一个预训练模型，使用弱监督方法进行训练。这种方法使模型具有更大的灵活性和可扩展性。该模型使用整个图像作为补丁进行训练，而不是使用像素补丁投影，结合了语言模型。这些方法使
    SimVLM 能够执行各种任务，其中问答是其重要的优势之一（Wang et al., [2022d](#bib.bib183)）。
- en: •
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Florence: Florence is a visual-language representation model that can perform
    multiple tasks. It is an encoder-based pre-trained model trained on web-scale
    image-text data, which enables it to handle high-resolution images. In addition
    to tasks such as object/action detection and question answering, Florence also
    shows strong performance in classification tasks (Yuan et al., [2021](#bib.bib199)).'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Florence：Florence 是一个视觉-语言表示模型，能够执行多种任务。它是一个基于编码器的预训练模型，训练于大规模的图像-文本数据上，使其能够处理高分辨率图像。除了对象/动作检测和问答任务，Florence
    在分类任务中也表现出强大的性能（Yuan et al., [2021](#bib.bib199)）。
- en: 'GIT: The description of the model has already been provided in the Question
    Answering section above. These models can also perform the Question Answering
    task with high performance.'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GIT：模型的描述已在上面的问答部分提供。这些模型也能够高性能地执行问答任务。
- en: 6.4.4 Visual Commonsense Reasoning
  id: totrans-569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.4 视觉常识推理
- en: Visual commonsense reasoning is a challenging task that requires a model with
    a deep understanding of visualization and different images or videos containing
    objects and scenes, inspired by how humans see and visualize things. These models
    capture information from different sub-tasks like object recognition and feature
    extraction. This information is then transformed into a vector to be used for
    reasoning. The reasoning module understands the relationship between the objects
    in the image and the output of the inferencing step provides a prediction about
    the interaction and relationship between the objects. Visual commonsense reasoning
    helps to improve the performance of various tasks like classification, image captioning,
    and other deep understanding-related tasks (Zellers et al., [2019](#bib.bib201),
    Xing et al., [2021](#bib.bib187)). In this section, we highlight and discuss significant
    transformer models for visual commonsense reasoning tasks that are summarized
    in Table LABEL:tab:visual_commonsense.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉常识推理是一项具有挑战性的任务，需要模型对视觉化及包含对象和场景的不同图像或视频有深刻的理解，灵感来源于人类如何观察和视觉化事物。这些模型从不同的子任务如对象识别和特征提取中捕捉信息。这些信息随后被转换成向量用于推理。推理模块理解图像中对象之间的关系，推理步骤的输出提供了有关对象之间互动和关系的预测。视觉常识推理有助于提高各种任务的性能，如分类、图像描述和其他深度理解相关任务（Zellers
    et al., [2019](#bib.bib201), Xing et al., [2021](#bib.bib187)）。在本节中，我们强调并讨论了用于视觉常识推理任务的重要
    Transformer 模型，这些模型在表格 LABEL:tab:visual_commonsense 中进行了总结。
- en: 'Table 20: Multi-modal Transformer models - visual commonsense reasoning task'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 表 20：多模态 Transformer 模型 - 视觉常识推理任务
- en: '| Transformer Models | Processed Data type(i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 处理的数据类型（输入/输出） | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集
    | 数据集（微调、训练、测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BERT-Verients (Huang et al., [2020](#bib.bib66), Tan & Bansal, [2019](#bib.bib167),
    Lu et al., [2019](#bib.bib106), Su et al., [2020](#bib.bib158), Chen et al., [2020c](#bib.bib24))
    | Text and Image | Question Answering, Common sense reasoning | 2019-2020 | Encoder
    | Yes | Pixel-BERT: MS-COCO, Visual Genome  LX-MERT: MS COCO,Visual Genome,VQA
    v2.0,GQA,VG-QA  ViLBERT: Visual Genome, COCO  VL-BERT: Conceptual Captions, BooksCorpus,
    English Wikipedia Uniter: COCO, VG, CC, SBU | Pixel-BERT: VQA 2.0 NLVR2, Flickr30K
    MS-COCO  LX-MERT: VQA,GQA,NLVR  ViLBERT: Conceptual Captions, Flickr30k  VL-BERT:
    VCR dataset, RefCOCO Uniter: COCO, Flickr30K, VG, CC, SBU |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| BERT-Verients (Huang et al., [2020](#bib.bib66), Tan & Bansal, [2019](#bib.bib167),
    Lu et al., [2019](#bib.bib106), Su et al., [2020](#bib.bib158), Chen et al., [2020c](#bib.bib24))
    | 文本和图像 | 问答，常识推理 | 2019-2020 | 编码器 | 是 | Pixel-BERT: MS-COCO, Visual Genome  LX-MERT:
    MS COCO, Visual Genome, VQA v2.0, GQA, VG-QA  ViLBERT: Visual Genome, COCO  VL-BERT:
    Conceptual Captions, BooksCorpus, English Wikipedia Uniter: COCO, VG, CC, SBU
    | Pixel-BERT: VQA 2.0 NLVR2, Flickr30K MS-COCO  LX-MERT: VQA, GQA, NLVR  ViLBERT:
    Conceptual Captions, Flickr30k  VL-BERT: VCR 数据集, RefCOCO Uniter: COCO, Flickr30K,
    VG, CC, SBU |'
- en: '| ViLT (Kim et al., [2021b](#bib.bib81)) | Image and Text | Visual Question
    Answering, Image text matching, Natural Language for Visual Reasoning | 2021 |
    Encoder | Yes | MS-COCO,Visual Genome, SBU Captions, Google Conceptual Captions
    | VQA 2.0, NLVR2, MSCOCO, Flickr30K |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| ViLT (Kim et al., [2021b](#bib.bib81)) | 图像和文本 | 视觉问答，图像文本匹配，视觉推理的自然语言 |
    2021 | 编码器 | 是 | MS-COCO, Visual Genome, SBU Captions, Google Conceptual Captions
    | VQA 2.0, NLVR2, MSCOCO, Flickr30K |'
- en: '| Unicode-VL (Li et al., [2020a](#bib.bib88)) | Image and Text | Object Classification,
    Visual-linguistic Matching, visual commonsense reasoning, image-text retrieval
    | 2020 | Encoder | Yes | Conceptual Captions-3M, SBU Captions | MSCOCO, Flickr30K
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| Unicode-VL (Li et al., [2020a](#bib.bib88)) | 图像和文本 | 对象分类、视觉语言匹配、视觉常识推理、图像文本检索
    | 2020 | 编码器 | 是 | Conceptual Captions-3M, SBU Captions | MSCOCO, Flickr30K |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: •
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unicoder-VL: Unicoder-VL is a large-scale pre-trained encoder-based model that
    utilizes cross-modeling to build a strong understanding of the relationship between
    image and language. The model employs a masking scheme for pre-training on a large
    corpus of data. These methods enhance the model’s performance on visual commonsense
    reasoning tasks in addition to visual classification tasks (Li et al., [2020a](#bib.bib88))'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Unicoder-VL: Unicoder-VL 是一个大规模预训练的基于编码器的模型，利用跨模型建构对图像和语言之间关系的深刻理解。该模型采用掩蔽方案在大数据集上进行预训练。这些方法增强了模型在视觉常识推理任务中的表现，以及视觉分类任务中的表现（Li
    et al., [2020a](#bib.bib88)）。'
- en: •
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ViLT (Vision-and-Language Transformer): is a multi-modal architecture based
    on the ViT (Vision Transformer) model, utilizing a free-convolution approach.
    Unlike other VLP (Vision-and-Language Pre-training) models, ViLT performs data
    augmentation during the execution of downstream tasks of classification and retrievals,
    which improves the model’s performance. Inspired by Pixel-BERT, ViLT takes the
    entire image as input instead of just using selected regions. By omitting convolutional
    visual embedders, ViLT reduces the model size and achieves remarkable performance
    compared to other VLP models (Kim et al., [2021b](#bib.bib81)).'
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ViLT（视觉与语言变换器）：是一种基于 ViT（视觉变换器）模型的多模态架构，利用自由卷积方法。与其他 VLP（视觉与语言预训练）模型不同，ViLT
    在执行分类和检索的下游任务期间进行数据增强，从而提高了模型的性能。受 Pixel-BERT 启发，ViLT 将整个图像作为输入，而不是仅使用选择的区域。通过省略卷积视觉嵌入器，ViLT
    减少了模型的大小，并且与其他 VLP 模型相比，表现出色（Kim et al., [2021b](#bib.bib81)）。
- en: 'BERT-Variants: The BERT-Variants models have been previously described in the
    Classification & segmentation section. It should be noted that these models are
    also capable of performing the Classification & segmentation task.'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERT 变体：BERT 变体模型在分类与分割部分已经描述过。需要注意的是，这些模型也能够执行分类与分割任务。
- en: 6.4.5 Image/Video/Speech Generation
  id: totrans-583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.5 图像/视频/语音生成
- en: Multi-modal generation tasks have gained a lot of attention in the field of
    artificial intelligence. These tasks involve generating images, text, or speech
    from inputs of different modalities of input. In recent times, several generative
    models have demonstrated outstanding performance, making this field of research
    even more attractive (Suzuki & Matsuo, [2022](#bib.bib165)). In this section,
    we discuss some significant transformer models that have been used for multi-modal
    generation tasks. These models are summarized in Table LABEL:tab:_multi-modal_generation.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态生成任务在人工智能领域引起了广泛关注。这些任务涉及从不同输入模态生成图像、文本或语音。近年来，一些生成模型展示了卓越的性能，使得这一研究领域更加引人注目（Suzuki
    & Matsuo, [2022](#bib.bib165)）。在本节中，我们讨论了一些用于多模态生成任务的重要变换器模型。这些模型总结在表 LABEL:tab:_multi-modal_generation
    中。
- en: 'Table 21: Multi-modal Transformer models - image/video/speech generation task'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 表 21：多模态变换器模型 - 图像/视频/语音生成任务
- en: '| Transformer Models | Processed Data type(i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 处理数据类型（输入/输出） | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集
    | 数据集（微调、训练、测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| DALL-E (Ramesh et al., [2021](#bib.bib135)) | Image and Text | Image Generation
    from text | 2021 | Encoder & Decoder | Yes | Not disclosed | Conceptual Captions
    (MS-COCO extension), Wikipedia (text-image pairs), and YFCC100M(filtered subset)
    |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E (Ramesh et al., [2021](#bib.bib135)) | 图像和文本 | 从文本生成图像 | 2021 | 编码器
    & 解码器 | 是 | 未披露 | 概念性字幕（MS-COCO 扩展）、维基百科（文本-图像对）、以及 YFCC100M（筛选子集） |'
- en: '| GLIDE (Nichol et al., [2022](#bib.bib117)) | Image and Text | Image generation
    & edit from text | 2021 | Encoder & Decoder | No | NA | MS-COCO, ViT-B CLIP (noised),
    CLIP and DALL-E filtered datasets |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| GLIDE (Nichol et al., [2022](#bib.bib117)) | 图像和文本 | 从文本生成与编辑图像 | 2021 |
    编码器 & 解码器 | 否 | NA | MS-COCO、ViT-B CLIP（噪声）、CLIP 和 DALL-E 筛选数据集 |'
- en: '| Chimera (Li & Hoefler, [2021](#bib.bib93)) | Audio and Text | Text generation
    from speech | 2021 | Encoder & Decoder | Yes | ALIGN & MT-Dataset | MuST-C, Augmented
    LibriSpeech Dataset (En-Fr), Machine Translation Datasets (WMT 2016, WMT 2014,
    OPUS100, OpenSubtitles) |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| Chimera (Li & Hoefler, [2021](#bib.bib93)) | 音频和文本 | 从语音生成文本 | 2021 | 编码器
    & 解码器 | 是 | ALIGN & MT 数据集 | MuST-C、增强版 LibriSpeech 数据集（英法）、机器翻译数据集（WMT 2016、WMT
    2014、OPUS100、OpenSubtitles） |'
- en: '| CogView (Ding et al., [2021](#bib.bib34)) | Image and Text | Classification,
    Image generation from text | 2021 | Decoder | Yes | VQ-VAE | MS COCO, Wudao Corpora-extension
    dataset. |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| CogView (Ding et al., [2021](#bib.bib34)) | 图像和文本 | 从文本生成图像 | 2021 | 解码器
    | 是 | VQ-VAE | MS COCO, Wudao Corpora-extension 数据集。 |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: •
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DALL-E: DALL-E (Ramesh et al., [2021](#bib.bib135)) is a popular transformer-based
    model for generating images from text. It is trained on a large and diverse dataset
    of both text and images, utilizing 12 billion parameters from the GPT-3 architecture.
    To reduce memory consumption, DALL-E compresses images without compromising their
    visual quality. An updated version of DALL-E, known as DALL-E 2, has been introduced
    with a higher number of parameters (175 billion) which allows for the generation
    of higher resolution images. Additionally, DALL-E 2 is capable of generating a
    wider range of images.'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DALL-E：DALL-E（Ramesh等人，[2021](#bib.bib135)）是一种流行的基于Transformer的模型，用于从文本中生成图像。它在大量的文本和图像数据集上进行训练，利用了GPT-3架构的120亿个参数。为了减少内存消耗，DALL-E在不影响视觉质量的情况下压缩图像。DALL-E的更新版本称为DALL-E
    2，具有更高数量的参数（1750亿个），可以生成更高分辨率的图像。此外，DALL-E 2能够生成更广泛的图像。
- en: •
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Chimera: The Chimera end-to-end architecture is designed for Speech-And-Text-Translation
    (ST). This architecture draws inspiration from Text-Machine-Translation and proposes
    a new module called the Shared Semantic Projection Module based on attention mechanisms.
    The objective of this module is to reduce latency and errors during the speech
    translation process by removing dissimilarities between speech and language modalities.'
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chimera：Chimera端到端架构专为语音文本翻译（ST）而设计。该架构借鉴了文本机器翻译的思想，并提出了一种称为共享语义投影模块的新模块，基于注意机制。该模块的目标是通过消除语音和语言模态之间的差异，减少语音翻译过程中的延迟和错误。
- en: •
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CogView: CogView (Ding et al., [2021](#bib.bib34)) is an image generation model
    that generates images based on input text descriptions, making it a challenging
    task that requires a deep understanding of the contextual relationship between
    text and image. It utilizes a transformer-GPT-based architecture to encode the
    text into a vector and decode it into an image. This model outperforms DALL-E
    in some cases, which also generates images from text descriptions, but uses text-image
    pairs for training the model.'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CogView：CogView（Ding等人，[2021](#bib.bib34)）是一种基于输入文本描述生成图像的图像生成模型，这是一项需要深入理解文本和图像之间上下文关系的挑战性任务。它利用基于transformer-GPT的架构将文本编码成向量，并将其解码成图像。在某些情况下，此模型的性能超过了DALL-E，后者也可以从文本描述生成图像，但使用文本-图像对来训练模型。
- en: •
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GLIDE: short for Guided Language to Image Diffusion for Generation and Editing,
    GLIDE is a diffusion model that is distinct from conventional models in that it
    can both generate and edit images. Unlike other models, diffusion models are sequentially
    injected with random noise and trained to remove that noise to construct the original
    data. GLIDE takes textual information as input and generates an output image conditioned
    on that information. In some instances, the images generated by GLIDE are more
    impressive than those generated by DALL-E (Nichol et al., [2022](#bib.bib117)).'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GLIDE：全称为Guided Language to Image Diffusion for Generation and Editing，GLIDE是一种与传统模型不同的扩散模型，既可以生成图像，也可以编辑图像。与其他模型不同，扩散模型是顺序注入随机噪声并通过训练去除该噪声以构造原始数据。GLIDE将文本信息作为输入，并生成与该信息相关的输出图像。在某些情况下，GLIDE生成的图像比DALL-E生成的图像更令人印象深刻（Nichol等人，[2022](#bib.bib117)）。
- en: 6.4.6 Cloud computing
  id: totrans-601
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.6 云计算
- en: Cloud computing is a crucial element of modern technology, particularly with
    regard to the Internet of Things (IoT). It encompasses a wide variety of cloud-based
    tasks, including server computing, task scheduling, storage, networking, and more.
    In wireless networks, cloud computing aims to improve scalability, flexibility,
    and adaptability, thereby providing seamless connectivity. To achieve this, data
    or information is retrieved from the network for computation, with various types
    of data being processed, including text, images, speech, and digits. Due to this
    multi-modal approach, cloud computing is classified in this category (Li et al.,
    [2017](#bib.bib92), Jauro et al., [2020](#bib.bib69), Yu et al., [2017](#bib.bib198)).
    In this article, we focus on the significant transformer models used in cloud
    computing tasks, which are presented in Table LABEL:tab:_cloud_computing.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算是现代技术的一个关键组成部分，特别是在物联网（IoT）方面。它涵盖了各种基于云的任务，包括服务器计算、任务调度、存储、网络等。在无线网络中，云计算旨在提高可扩展性、灵活性和适应性，从而提供无缝连接。为实现这一目标，需要从网络中检索数据或信息进行计算，包括文本、图像、语音和数字等各种类型的数据。由于这种多模态的方法，云计算被归类于这一类别（Li
    et al., [2017](#bib.bib92)，Jauro et al., [2020](#bib.bib69)，Yu et al., [2017](#bib.bib198)）。在本文中，我们重点介绍了用于云计算任务的重要transformer模型，详见表
    LABEL:tab:_cloud_computing。
- en: 'Table 22: Multi-modal Transformer models - cloud computing task'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 表 22：多模态 Transformer 模型 - 云计算任务
- en: '| Transformer Models | Processed Data type(i/o) | Task Accomplished | Year
    | Architecture (Encoder/ Decoder) | Pre-trained (Yes/NO) | Pre-training Dataset
    | Dataset (Fine-tuning, Training, Testing) |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 处理的数据类型（输入/输出） | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集
    | 数据集（微调，训练，测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| VMD & R-Transformer (Zhou et al., [2020](#bib.bib211)) | workload sequence
    | cloud workload forecasting | 2020 | Encoder & Decoder | No | NA | Google cluster
    trace, Alibaba cluster trace |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| VMD & R-Transformer (Zhou et al., [2020](#bib.bib211)) | 工作负载序列 | 云工作负载预测
    | 2020 | 编码器 & 解码器 | 否 | 不适用 | Google 集群追踪，阿里巴巴集群追踪 |'
- en: '| ACT4JS (Xu & Zhao, [2022](#bib.bib189)) | Cloud jobs/task | Cloud computing
    resource job scheduling. | 2022 | Encoder | No | NA | Alibaba Cluster-V2018 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| ACT4JS (Xu & Zhao, [2022](#bib.bib189)) | 云任务/作业 | 云计算资源作业调度 | 2022 | 编码器
    | 否 | 不适用 | 阿里巴巴集群-V2018 |'
- en: '| TEDGE-Catching (Hajiakhondi-Meybodi et al., [2022](#bib.bib51)) | Sequential
    request pattern of the contents (Ex: video, image, websites, etc) | Predict the
    content popularity in proactive caching schemes. | 2021 | Encoder | No | NA |
    MovieLens |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| TEDGE-Catching (Hajiakhondi-Meybodi et al., [2022](#bib.bib51)) | 内容的顺序请求模式（如：视频、图像、网站等）
    | 在主动缓存方案中预测内容的受欢迎程度 | 2021 | 编码器 | 否 | 不适用 | MovieLens |'
- en: '| SACCT (Wang et al., [2022b](#bib.bib178)) | Network status (Bandwidth, storage,
    and etc) | Optimize network based on network status. | 2021 | Encoder | No | NA
    | Not mentioned clearly |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| SACCT (Wang et al., [2022b](#bib.bib178)) | 网络状态（带宽、存储等） | 根据网络状态优化网络 | 2021
    | 编码器 | 否 | 不适用 | 未明确提及 |'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: •
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VMD & R-TRANSFORMER: Workload forecasting is a critical task for the cloud,
    and previous research has focused on using recurrent neural networks (RNNs) for
    this purpose. However, due to the highly complex and dynamic nature of workloads,
    RNN-based models struggle to provide accurate forecasts because of the problem
    of vanishing gradients. In this context, the proposed Variational Mode Decomposition-VMD
    and R-Transformer model offers a more accurate solution by capturing long-term
    dependencies using multi-head attention and local non-linear relationships of
    workload sequences with local techniques (Zhou et al., [2020](#bib.bib211)). Therefore,
    this model is capable of executing the workload forecasting task with greater
    precision than existing RNN-based models.'
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VMD & R-TRANSFORMER：工作负载预测是云计算中的一个关键任务，之前的研究主要集中在使用递归神经网络（RNNs）来实现这一目标。然而，由于工作负载的高度复杂性和动态特性，基于RNN的模型难以提供准确的预测，因为梯度消失问题。在这种情况下，提出的变分模态分解-VMD和R-Transformer模型通过使用多头注意力机制和局部非线性关系捕捉长期依赖关系，从而提供了更准确的解决方案（Zhou
    et al., [2020](#bib.bib211)）。因此，该模型能够比现有的基于RNN的模型更精确地执行工作负载预测任务。
- en: •
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TEDGE-Caching: TEDGE is an acronym for Transformer-based Edge Caching, which
    is a critical component of the 6G wireless network as it provides a high-bandwidth,
    low-latency connection. Edge caching stores multimedia content to deliver it to
    users with low latency. To achieve this, it is essential to proactively predict
    popular content. However, conventional models are limited by long-term dependencies,
    computational complexity, and the inability to compute in parallel. In this context,
    the Transformer-based Edge (TEDGE) caching framework incorporates a vision transformer
    (ViT) to overcome these limitations, without requiring data pre-processing or
    additional contextual information to predict popular content at the Mobile Edge.
    This is the first model to apply a transformer-based approach to execute this
    task, resulting in superior performance (Hajiakhondi-Meybodi et al., [2022](#bib.bib51)).'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TEDGE-Caching：TEDGE 是基于变压器的边缘缓存的缩写，它是 6G 无线网络中的一个关键组件，因为它提供了高带宽、低延迟的连接。边缘缓存存储多媒体内容，以低延迟将其传递给用户。为实现这一点，主动预测热门内容至关重要。然而，传统模型受限于长期依赖、计算复杂性和无法并行计算。在这种情况下，基于变压器的边缘（TEDGE）缓存框架结合了视觉变压器（ViT），克服了这些限制，无需数据预处理或额外的上下文信息即可预测移动边缘的热门内容。这是第一个应用基于变压器的方法执行此任务的模型，结果表现优越（Hajiakhondi-Meybodi
    et al., [2022](#bib.bib51)）。
- en: •
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ACT4JS: The Actor-Critic Transformer for Job Scheduling (ACT4JS) is a transformer-based
    model designed to allocate cloud computing resources to different tasks in cloud
    computing. The model consists of an Actor and Critic network, where the Actor-network
    selects the best action to take at each step, and the Critic network evaluates
    the action taken by the Actor-network and provides feedback to improve future
    steps. This approach allows for a better understanding of the complex relationship
    between cloud jobs and enables prediction or scheduling of jobs based on different
    features such as job priority, network conditions, resource availability, and
    more (Xu & Zhao, [2022](#bib.bib189)).'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ACT4JS：作业调度的演员-评论家变压器（ACT4JS）是一个基于变压器的模型，旨在将云计算资源分配给云计算中的不同任务。该模型由演员网络和评论家网络组成，其中演员网络选择每一步的最佳行动，而评论家网络评估演员网络采取的行动并提供反馈，以改善未来的步骤。这种方法使得更好地理解云作业之间的复杂关系成为可能，并能够基于不同的特征（如作业优先级、网络条件、资源可用性等）预测或调度作业（Xu
    & Zhao, [2022](#bib.bib189)）。
- en: •
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SACCT: SACCT refers to the Soft Actor-Critic framework with Communication Transformer,
    which combines the transformer, reinforcement learning, and convex optimization
    techniques. This model introduces the Communication Transformer (CT), which works
    with reinforcement learning to adapt to different challenges, such as bandwidth
    limitations, storage constraints, and more, in the wireless edge network during
    live streaming. Adapting to changing network conditions is critical during live
    streaming, and SACCT provides a system that adjusts resources to improve the quality
    of service based on user demand and network conditions. The SACCT model’s ability
    to adapt to changing conditions and optimize resources makes it an important contribution
    to the field of wireless edge network technology (Wang et al., [2022b](#bib.bib178)).'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SACCT：SACCT 指的是带有通信变压器的软演员-评论家框架，它结合了变压器、强化学习和凸优化技术。该模型引入了通信变压器（CT），它与强化学习一起工作，以适应无线边缘网络中直播期间的带宽限制、存储约束等各种挑战。适应变化的网络条件在直播过程中至关重要，而
    SACCT 提供了一个系统，可以根据用户需求和网络条件调整资源，以改善服务质量。SACCT 模型在适应变化的条件和优化资源方面的能力，使其成为无线边缘网络技术领域的重要贡献（Wang
    et al., [2022b](#bib.bib178)）。
- en: 6.5 Audio & Speech
  id: totrans-619
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 音频与语音
- en: Audio and speech processing is one of the most essential tasks in the field
    of deep learning. Along with NLP, speech processing has also gained attention
    from researchers, leading to the application of deep neural network methods. As
    transformers have achieved great success in the field of NLP, researchers have
    also had significant success when applying transformer-based models to speech
    processing.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音处理是深度学习领域中最重要的任务之一。与自然语言处理（NLP）一起，语音处理也获得了研究人员的关注，促使深度神经网络方法的应用。由于变压器在
    NLP 领域取得了巨大成功，研究人员在将基于变压器的模型应用于语音处理时也取得了显著成功。
- en: 6.5.1 Speech Recognition
  id: totrans-621
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1 语音识别
- en: Speech Recognition is one of the most popular tasks in the field of artificial
    intelligence. It is the ability of a model to identify human speech and convert
    it into a textual or written format. This process is also known as speech-to-text,
    automatic speech recognition, or computer-assisted transcription. Speech recognition
    technology has advanced significantly in the last few years. It involves two types
    of models, namely the acoustic model and the language model. Several features
    contribute to the effectiveness of speech recognition models, including language
    weighting, speaker labeling, acoustics training, and profanity filtering. Despite
    significant advancements in speech recognition technology, there is still room
    for further improvement (Yu & Deng, [2016](#bib.bib196), Nassif et al., [2019](#bib.bib116),
    Deng et al., [2013](#bib.bib32)). One promising development in this area is the
    use of transformer-based models, which have shown significant improvement in various
    stages of the speech recognition task. The majority of transformer-based audio/speech
    processing models have focused on speech recognition tasks, and among these, several
    models have made exceptional contributions, exhibited high levels of accuracy,
    introduced new effective ideas, or created buzz in the AI field. In Table LABEL:tab:speech_recognition,
    we highlight the most significant transformer models for speech recognition tasks.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别是人工智能领域最受欢迎的任务之一。它是指模型识别人的语音并将其转换为文本或书面格式的能力。这个过程也被称为语音转文本、自动语音识别或计算机辅助转录。近年来，语音识别技术有了显著进步。它涉及两种类型的模型，即声学模型和语言模型。多个特性影响语音识别模型的有效性，包括语言权重、说话者标记、声学训练和粗口过滤。尽管语音识别技术取得了显著进展，但仍有进一步改进的空间（Yu
    & Deng, [2016](#bib.bib196)，Nassif et al., [2019](#bib.bib116)，Deng et al., [2013](#bib.bib32)）。在这一领域的一个有前景的发展是使用基于变换器的模型，这些模型在语音识别任务的各个阶段显示出了显著的改进。大多数基于变换器的音频/语音处理模型都专注于语音识别任务，其中一些模型做出了卓越的贡献，展现了高水平的准确性，引入了新的有效理念，或在人工智能领域引起了轰动。在表格
    LABEL:tab:speech_recognition 中，我们突出了在语音识别任务中最重要的变换器模型。
- en: 'Table 23: Transformer models for audio & speech recognition task'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '表 23: 音频与语音识别任务的变换器模型'
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 变换器模型 | 任务完成 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试） |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Conformer (Peng et al., [2021](#bib.bib125)) | Speech Recognition | 2020
    | Encoder & Decoder | No | NA | Librispeech, test/testother. |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| Conformer (Peng et al., [2021](#bib.bib125)) | 语音识别 | 2020 | 编码器 & 解码器 |
    否 | 无 | Librispeech，测试/其他测试 |'
- en: '| Speech Transformer (Dong et al., [2018](#bib.bib35)) | Speech Recognition
    | 2018 | Encoder & Decoder | No | NA | Wall StreetJournal (WSJ), NVIDIA K80 G-PU
    |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| Speech Transformer (Dong et al., [2018](#bib.bib35)) | 语音识别 | 2018 | 编码器
    & 解码器 | 否 | 无 | 华尔街日报（WSJ），NVIDIA K80 G-PU |'
- en: '| VQ-Wav2vec (Baevski et al., [2020a](#bib.bib7)) | Speech Recognition | 2020
    | Encoder | Yes | Librispeech | TIMIT, WSJ-Wall StreetJournal |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| VQ-Wav2vec (Baevski et al., [2020a](#bib.bib7)) | 语音识别 | 2020 | 编码器 | 是 |
    Librispeech | TIMIT，WSJ-华尔街日报 |'
- en: '| Wav2vec 2.0 (Baevski et al., [2020b](#bib.bib8)) | Speech Recognition | 2020
    | Encoder | Yes | Librispeech, LibriVox | Librispeech, LibriVox, TIMIT |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| Wav2vec 2.0 (Baevski et al., [2020b](#bib.bib8)) | 语音识别 | 2020 | 编码器 | 是
    | Librispeech, LibriVox | Librispeech, LibriVox, TIMIT |'
- en: '| HuBERT (Hsu et al., [2021](#bib.bib63)) | Speech Recognition | 2021 | Encoder
    | Yes | Librispeech, Libri-light | Librispeech(train-clean), Libri-light(train-clean)
    |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| HuBERT (Hsu et al., [2021](#bib.bib63)) | 语音识别 | 2021 | 编码器 | 是 | Librispeech,
    Libri-light | Librispeech（训练清洁），Libri-light（训练清洁） |'
- en: '| BigSSL (Zhang et al., [2022](#bib.bib205)) | Speech Recognition | 2022 |
    Encoder & Decoder | Yes | wav2vec 2.0, YT-U, Libri-Light | YouTube, English(US)
    Voice search(VS), SpeechStew, LibriSpeech, CHiME6, Telephony |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| BigSSL (Zhang et al., [2022](#bib.bib205)) | 语音识别 | 2022 | 编码器 & 解码器 | 是
    | wav2vec 2.0, YT-U, Libri-Light | YouTube, 英语（美国），语音搜索（VS），SpeechStew, LibriSpeech,
    CHiME6, 电话 |'
- en: '| Whisper (Radford et al., [2022](#bib.bib131)) | Speech recognition, Translation,
    Language Identification | 2022 | Encoder & Decoder | Yes | Not Mentioned | VoxLingua107,
    LibriSpeeech, CoVoST2, Fleurs, Kincaid46 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| Whisper (Radford et al., [2022](#bib.bib131)) | 语音识别、翻译、语言识别 | 2022 | 编码器
    & 解码器 | 是 | 未提及 | VoxLingua107, LibriSpeech, CoVoST2, Fleurs, Kincaid46 |'
- en: '| Transformer Transducer (Zhang et al., [2020b](#bib.bib204)) | Speech recognition
    | 2020 | Encoder | No | NA | LibriSpeeech |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| Transformer Transducer (Zhang et al., [2020b](#bib.bib204)) | 语音识别 | 2020
    | 编码器 | 否 | NA | LibriSpeeech |'
- en: '| XLSR-Wav2Vec2 (Conneau et al., [2021](#bib.bib29)) | Speech recognition |
    2020 | Encoder | Yes | 53 languages datasets | CommonVoice, BABEL,Multilingual
    LibriSpeech(MLS) |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| XLSR-Wav2Vec2 (Conneau et al., [2021](#bib.bib29)) | 语音识别 | 2020 | 编码器 |
    是 | 53 语言数据集 | CommonVoice, BABEL, Multilingual LibriSpeech(MLS) |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Conformer: The Conformer architecture is a model that combines the advantages
    of both the Transformer and convolutional neural network (CNN) for automatic speech
    recognition tasks. While the Transformer is proficient at capturing global features,
    and CNN excels at capturing local features, the Conformer architecture leverages
    the strengths of both to achieve superior performance. Recent studies have shown
    that the Conformer architecture outperforms both CNN and Transformer models individually,
    thereby setting a new state-of-the-art in automatic speech recognition performance
    (Gulati et al., [2020](#bib.bib47)).'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Conformer：Conformer 架构是一个结合了 Transformer 和卷积神经网络（CNN）优点的自动语音识别模型。虽然 Transformer
    擅长捕捉全局特征，而 CNN 擅长捕捉局部特征，Conformer 架构结合了这两者的优势，取得了卓越的性能。最近的研究表明，Conformer 架构超越了单独的
    CNN 和 Transformer 模型，从而在自动语音识别性能上设立了新的最先进水平（Gulati et al., [2020](#bib.bib47)）。
- en: •
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Speech Transformer: It is a speech recognition model published in 2018 which
    is one of the earliest Transformer inspired speech models. It eliminates the conventional
    Recurrent Neural Network (RNN)-based approach in speech processing and instead
    applies an attention mechanism. The introduction of the Transformer model into
    speech recognition has led to a number of benefits. For instance, the training
    time and memory usage become lower, allowing for better scalability. This is especially
    helpful for tasks that require a long-term dependency due to the elimination of
    recurrence sequence-to-sequence processing (Dong et al., [2018](#bib.bib35)).'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Speech Transformer：这是一个在 2018 年发布的语音识别模型，是最早的受 Transformer 启发的语音模型之一。它摒弃了传统的基于递归神经网络（RNN）的语音处理方法，而是应用了注意力机制。引入
    Transformer 模型到语音识别中带来了许多好处。例如，训练时间和内存使用降低了，从而提高了可扩展性。这对于需要长期依赖的任务特别有帮助，因为消除了递归序列到序列的处理（Dong
    et al., [2018](#bib.bib35)）。
- en: •
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Wav2vec 2.0: Wav2Vec 2.0 is a self-supervised model which uses discretization
    algorithms to capture the vocabulary from raw speech representation. The learned
    vocabulary is passed through an architecture consisting of multi-layer convolutional
    feature encoder. This encoder has multiple convolution layers, layer normalization
    and an activation function, with the audio representations being masked during
    the training process. Wav2Vec 2.0 offers the advantage of performing well on speech
    recognition tasks with a small amount of supervised data (Baevski et al., [2020b](#bib.bib8)).'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Wav2Vec 2.0：Wav2Vec 2.0 是一种自监督模型，利用离散化算法从原始语音表示中捕获词汇。学习到的词汇通过一个包含多层卷积特征编码器的架构。该编码器具有多个卷积层、层归一化和激活函数，同时在训练过程中对音频表示进行遮蔽。Wav2Vec
    2.0 的优势在于在少量监督数据的情况下仍能在语音识别任务中表现良好（Baevski et al., [2020b](#bib.bib8)）。
- en: •
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VQ-Wav2vec: VQ-Wav2Vec is a Transformer based model that enables Vector Quantization-VQ
    tasks to be executed in a self-supervised way. It is built on the Wav2Vec model,
    which is an effective way of compressing continuous signals into discrete symbols.
    Unlike other models, VQ-Wav2Vec trains without the need of unlabeled data. Instead,
    corrupted speech is used, and the model learns by predicting the missing parts
    of the speech. This process of training has been proven to be highly effective,
    and the model is capable of achieving a higher accuracy when compared to other
    models (Baevski et al., [2020a](#bib.bib7)).'
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VQ-Wav2Vec：VQ-Wav2Vec 是一种基于 Transformer 的模型，能够以自监督的方式执行向量量化（VQ）任务。它建立在 Wav2Vec
    模型的基础上，这是将连续信号压缩成离散符号的一种有效方法。与其他模型不同，VQ-Wav2Vec 训练时不需要无标注数据。相反，它使用损坏的语音，模型通过预测语音的缺失部分来学习。这一训练过程被证明非常有效，模型在与其他模型比较时能够实现更高的准确率（Baevski
    et al., [2020a](#bib.bib7)）。
- en: •
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BigSSL: BigSSL is a large-scale semi-supervised Transformer-based model designed
    specifically for speech recognition tasks. Obtaining labeled speech data is a
    challenging and time-consuming process, while the availability of unlabeled data
    is considerably vast. In this context, the BigSSL model proposes a novel approach
    to enhance the performance of speech recognition tasks. By leveraging a smaller
    portion of labeled data in conjunction with a substantial amount of unlabeled
    data, this model achieves improved performance. Furthermore, the utilization of
    a larger quantity of unlabeled data helps alleviate the overfitting issue, thereby
    further enhancing the overall performance of the BigSSL model (Zhang et al., [2022](#bib.bib205)).'
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BigSSL: BigSSL 是一种大规模的半监督 Transformer 模型，专门设计用于语音识别任务。获取标注语音数据是一个具有挑战性且耗时的过程，而未标注数据的可用性却非常广泛。在这种背景下，BigSSL
    模型提出了一种新颖的方法来提升语音识别任务的性能。通过结合少量的标注数据与大量的未标注数据，该模型实现了性能的提升。此外，利用更多的未标注数据有助于缓解过拟合问题，从而进一步增强
    BigSSL 模型的整体性能（Zhang et al., [2022](#bib.bib205)）。'
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'HuBERT: HuBERT, short for Hidden-Unit BERT, is a self-supervised speech representation
    model. Its approach involves offline clustering for feature representation, with
    the loss calculation restricted to the masked regions. This emphasis allows the
    model to effectively learn a combination of acoustic and language models over
    the input data. HuBERT consists of a convolutional waveform encoder, a projection
    layer, a BERT encoder, and a code embedding layer. The CNN component generates
    feature representations, which are then subjected to random masking. These masked
    representations are subsequently passed through the BERT encoder, yielding another
    set of feature representations. HuBERT’s functioning resembles that of a mask
    language model and has demonstrated notable performance in speech representation
    tasks, particularly speech recognition (Hsu et al., [2021](#bib.bib63)).'
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'HuBERT: HuBERT，即隐藏单元 BERT 的缩写，是一种自监督的语音表示模型。它的方法包括对特征表示进行离线聚类，并将损失计算限制在被遮蔽的区域。此重点使模型能够有效地学习输入数据上的声学和语言模型的组合。HuBERT
    包含一个卷积波形编码器、一个投影层、一个 BERT 编码器和一个代码嵌入层。CNN 组件生成特征表示，然后对这些表示进行随机遮蔽。这些被遮蔽的表示随后通过
    BERT 编码器，生成另一组特征表示。HuBERT 的功能类似于掩蔽语言模型，并在语音表示任务中展示了显著的性能，特别是在语音识别方面（Hsu et al.,
    [2021](#bib.bib63)）。'
- en: •
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformer Transducer: It is a speech recognition model that capitalizes on
    the strengths of both the self-attention mechanism of the Transformer and the
    recurrent neural network (RNN). This model is constructed by integrating the encoder
    module of the transformer with the RNN-T loss function. The encoder module is
    responsible for extracting speech representations, while the RNN-T component utilizes
    this information to make real-time predictions of the transcript, facilitating
    a swift response—an essential requirement for speech recognition tasks (Transformer-Transducer
    (Zhang et al., [2020b](#bib.bib204)).'
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Transformer Transducer: 这是一个语音识别模型，充分利用了 Transformer 的自注意力机制和递归神经网络（RNN）的优点。该模型通过将
    Transformer 的编码器模块与 RNN-T 损失函数集成而构建。编码器模块负责提取语音表示，而 RNN-T 组件利用这些信息实时预测转录内容，提供快速响应——这是语音识别任务中的一个重要需求（Transformer-Transducer
    (Zhang et al., [2020b](#bib.bib204)））。'
- en: •
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Whisper: It is a noteworthy speech recognition model that emerged in late 2022,
    specifically designed to address the challenging task of recognizing speech with
    low volume. The uniqueness of this model lies in its dedicated efforts to improve
    low-volume speech recognition. Whisper adopts a training approach that incorporates
    a lower level of speech data and leverages weak supervision methods, enabling
    training on a larger corpus of data. This strategic approach has proven instrumental
    in enhancing the performance of the Whisper model, enabling it to effectively
    capture and comprehend low-level speech phenomena (Radford et al., [2022](#bib.bib131)).'
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Whisper: 这是一个值得注意的语音识别模型，出现于 2022 年末，专门设计用于应对低音量语音识别的挑战。该模型的独特性在于其致力于改善低音量语音的识别。Whisper
    采用了一种包含较低级别语音数据的训练方法，并利用弱监督方法，使训练可以在更大语料库上进行。这种策略性的方法被证明对提升 Whisper 模型的性能至关重要，使其能够有效捕捉和理解低级别的语音现象（Radford
    et al., [2022](#bib.bib131)）。'
- en: •
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'XLSR-Wav2Vec2: This model demonstrates the capability to recognize speech across
    multiple languages, eliminating the need for extensive labeled data in each language
    for training. By learning the relationships and shared characteristics among different
    languages, this model surpasses the requirement of training on specific language-labeled
    speech data. Consequently, the XLSR-Wav2Vec2 model offers an efficient solution
    for multiple-language speech recognition, requiring significantly less data for
    training while adhering to the architectural principles of Wav2Vec2 (Conneau et al.,
    [2021](#bib.bib29)).'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XLSR-Wav2Vec2：该模型展示了识别多语言语音的能力，消除了每种语言训练中需要大量标注数据的需求。通过学习不同语言之间的关系和共同特征，该模型超越了在特定语言标注语音数据上训练的要求。因此，XLSR-Wav2Vec2模型提供了一个高效的多语言语音识别解决方案，所需的训练数据显著减少，同时遵循了Wav2Vec2的架构原则（Conneau
    et al., [2021](#bib.bib29)）。
- en: 6.5.2 Speech Separation
  id: totrans-654
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2 语音分离
- en: 'It poses a considerable challenge within the field of audio signal processing.
    It involves the task of separating the desired speech signal, which may include
    various sources such as different speakers or human voices, from additional sounds
    such as background noise or interfering sources. In the domain of speech separation,
    three commonly employed methods are followed: (i) Blind source separation, (ii)
    Beamforming, and (iii) Single-channel speech separation. The significance of speech
    separation has grown with the increasing popularity of automatic speech recognition
    (ASR) systems. It is often employed as a preprocessing step for speech recognition
    tasks. The accurate distinction between the desired speech signal and unwanted
    noise is crucial to ensure precise speech recognition results. Failure to properly
    segregate the desired speech from interfering noise can lead to erroneous speech
    recognition outcomes (Wang & Chen, [2018](#bib.bib174), Huang et al., [2014](#bib.bib65)).
    In this context, we present several Transformer-based models that have showcased
    noteworthy advancements in audio and speech separation tasks. The details of these
    models are presented in Table LABEL:tab:speech_separation.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 这在音频信号处理领域提出了相当大的挑战。它涉及将所需的语音信号（可能包括不同的说话者或人声等各种来源）从背景噪声或干扰源等额外声音中分离出来的任务。在语音分离领域，通常采用三种方法：（i）盲源分离，（ii）波束形成，以及（iii）单通道语音分离。随着自动语音识别（ASR）系统的普及，语音分离的重要性不断增长。它通常作为语音识别任务的预处理步骤。准确区分所需的语音信号和不需要的噪声对于确保精确的语音识别结果至关重要。如果无法正确将所需语音从干扰噪声中分离出来，可能会导致语音识别结果错误（Wang
    & Chen, [2018](#bib.bib174)，Huang et al., [2014](#bib.bib65)）。在这种背景下，我们展示了几种基于Transformer的模型，这些模型在音频和语音分离任务中展示了显著的进展。这些模型的详细信息见表LABEL:tab:speech_separation。
- en: 'Table 24: Transformer models for audio & speech separation task'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 表24：用于音频和语音分离任务的Transformer模型
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| Transformer模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| DPTNeT (Chen et al., [2020a](#bib.bib17)) | Speech Separation | 2020 | Encoder
    & Decoder | No | NA | WSJ0-2mix, LS-2mix |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| DPTNeT (Chen et al., [2020a](#bib.bib17)) | 语音分离 | 2020 | 编码器 & 解码器 | 否 |
    NA | WSJ0-2mix, LS-2mix |'
- en: '| Sepformer (Subakan et al., [2021](#bib.bib159)) | Speech Separation | 2021
    | Encoder & Decoder | No | NA | WSJ0-2mix, WSJ0-3mix |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| Sepformer (Subakan et al., [2021](#bib.bib159)) | 语音分离 | 2021 | 编码器 & 解码器
    | 否 | NA | WSJ0-2mix, WSJ0-3mix |'
- en: '| WavLM (Chen et al., [2022a](#bib.bib21)) | Speech separation, speech denoising,
    speech prediction, Speaker Verification, Speech recognition | 2022 | Encoder |
    Yes | GigaSpeech, VoxPopuli | VoxCeleb1, VoxCeleb2, Switchboard-2 CALLHOME, LIBRICSS,
    LibriSpeech |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| WavLM (Chen et al., [2022a](#bib.bib21)) | 语音分离、语音去噪、语音预测、说话人验证、语音识别 | 2022
    | 编码器 | 是 | GigaSpeech, VoxPopuli | VoxCeleb1, VoxCeleb2, Switchboard-2 CALLHOME,
    LIBRICSS, LibriSpeech |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sepformer: The sepformer model was published in a paper titled “Attention is
    all you need in speech separation” which uses an attention mechanism to separate
    speeches that are overlapped. This model does not contain any kind of recurrence
    scheme and it follows the self-attention mechanisms. Sepformer uses a binary mask
    prediction scheme for training while this masking network captures both short
    and long-term dependencies and provide higher accuracy in performance (Subakan
    et al., [2021](#bib.bib159)).'
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sepformer：Sepformer 模型在题为“注意力就是你所需要的语音分离” 的论文中发布，该模型使用注意力机制来分离重叠的语音。该模型不包含任何递归方案，而是采用自注意力机制。Sepformer
    使用二进制掩码预测方案进行训练，同时这个掩码网络捕捉了短期和长期的依赖关系，并提供了更高的准确性（Subakan 等， [2021](#bib.bib159)）。
- en: •
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DPTNeT: DPTNeT stands for Dual-Path Transformer Network for monaural speech
    separation tasks. This model trained directly minimizes the error between estimated
    and target value which is called end-to-end processing. This model uses dual-path
    architecture, replacing positional encoding with the RNN in the transformer architecture
    which helps to capture complex features of the signal and improves the performance
    of the speech separation from the overlapped speech (Chen et al., [2020a](#bib.bib17)).'
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DPTNeT：DPTNeT 代表双路径 Transformer 网络，用于单耳语音分离任务。该模型直接通过最小化估计值与目标值之间的误差来训练，这称为端到端处理。该模型采用双路径架构，将位置编码替换为
    Transformer 架构中的 RNN，这有助于捕捉信号的复杂特征，提高了从重叠语音中分离语音的性能（Chen 等， [2020a](#bib.bib17)）。
- en: •
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'WavLM: WavLM is a large-scale pre-trained model that can execute a range of
    tasks for speech. WavLM follows BERT inspired speech processing model-HuBERT,
    whereas, with the help of mask speech prediction, the model predicts the actual
    speech by removing the noise from the corrupted speech. By this way, this model
    is trained for a variety of tasks besides automatic speech recognition-ASR task
    (Chen et al., [2022a](#bib.bib21)).'
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WavLM：WavLM 是一个大规模的预训练模型，可以执行各种语音任务。WavLM 遵循了 BERT 启发的语音处理模型 HuBERT，通过掩码语音预测，模型通过从受损语音中去除噪声来预测实际语音。通过这种方式，该模型被训练用于除自动语音识别-ASR
    任务之外的各种任务（Chen 等， [2022a](#bib.bib21)）。
- en: 6.5.3 Speech Classification
  id: totrans-669
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.3 语音分类
- en: The speech classification task refers to the ability to categorize input speech
    or audio into distinct categories based on various features, including speaker,
    words, phrases, language, and more. There exist several speech classifiers, such
    as voice activity detection (binary/multi-class), speech detection (multi-class),
    language identification, speech enhancement, and speaker identification. Speech
    classification plays a crucial role in identifying important speech signals, enabling
    the extraction of relevant information from large speech datasets (Livezey et al.,
    [2019](#bib.bib103), Gu et al., [2017](#bib.bib46)). In this context, we present
    a compilation of transformer-based models, which have demonstrated superior accuracy
    in speech classification tasks compared to conventional models. The details of
    these models are depicted in Table LABEL:tab:speech_classification.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 语音分类任务指的是根据各种特征（包括说话者、单词、短语、语言等）将输入的语音或音频分类为不同类别的能力。存在多种语音分类器，如语音活动检测（二元/多类）、语音检测（多类）、语言识别、语音增强和说话者识别。语音分类在识别重要语音信号中发挥了关键作用，使得从大量语音数据集中提取相关信息成为可能（Livezey
    等， [2019](#bib.bib103), Gu 等， [2017](#bib.bib46)）。在这种背景下，我们呈现了一些基于 Transformer
    的模型，这些模型在语音分类任务中的准确性优于传统模型。这些模型的详细信息如表 LABEL:tab:speech_classification 所示。
- en: 'Table 25: Transformer models for audio & speech classification task'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 表 25：用于音频和语音分类任务的 Transformer 模型
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模型 | 完成的任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| AST (Gong et al., [2021](#bib.bib42)) | Audio Classification | 2021 | Encoder
    | Yes | ImageNeT | AudioSet, ESC-50, Speech Commands |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| AST (Gong 等， [2021](#bib.bib42)) | 音频分类 | 2021 | 编码器 | 是 | ImageNeT | AudioSet,
    ESC-50, 语音命令 |'
- en: '| Mockingjay (Liu et al., [2020](#bib.bib96)) | Speech Classification & recognition
    | 2020 | Encoder | Yes | LibriSpeech | LibriSpeech test clean, MOSEI |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| Mockingjay (Liu 等， [2020](#bib.bib96)) | 语音分类与识别 | 2020 | 编码器 | 是 | LibriSpeech
    | LibriSpeech 测试干净集, MOSEI |'
- en: '| XLS-R (Babu et al., [2022](#bib.bib6)) | Speech Classification, Speech Translation,
    Speech Recognition | 2021 | Encoder & Decoder | Yes | VoxPopuli (VP-400K), Multilingual
    Librispeech (MLS), CommonVoice, VoxLingua107, BABEL | VoxPopuli, Multilingual
    Librispeech (MLS), CommonVoice, BABEL |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| XLS-R (Babu et al., [2022](#bib.bib6)) | 语音分类、语音翻译、语音识别 | 2021 | 编码器 & 解码器
    | 是 | VoxPopuli (VP-400K), 多语言 Librispeech (MLS), CommonVoice, VoxLingua107, BABEL
    | VoxPopuli, 多语言 Librispeech (MLS), CommonVoice, BABEL |'
- en: '| UniSpeech-SAT (Chen et al., [2022b](#bib.bib22)) | Speech Classification
    & Recognition | 2022 | Encoder | Yes | LibriVox, Librispeech GigaSpeech, VoxPopuli
    | SUPERB |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| UniSpeech-SAT (Chen et al., [2022b](#bib.bib22)) | 语音分类与识别 | 2022 | 编码器 |
    是 | LibriVox, Librispeech GigaSpeech, VoxPopuli | SUPERB |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AST: AST - Audio Spectrogram Transformer is a transformer-based model which
    is applied to an audio spectrogram. AST is the first audio classification model
    where the convolution was not used and it is capable of capturing long-range frames
    context. It used a transformer encoder to capture the features in the audio spectrogram,
    a linear projection layer and sigmoid activation function to capture the audio
    spectrogram representation for audio classification. As the attention mechanism
    is renowned for capturing global features so it shows significant performance
    in audio/speech classification tasks (Gong et al., [2021](#bib.bib42)).'
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'AST: AST - 音频谱图 Transformer 是一种基于 Transformer 的模型，应用于音频谱图。AST 是第一个未使用卷积的音频分类模型，能够捕捉长期帧的上下文。它使用
    Transformer 编码器捕捉音频谱图中的特征，线性投影层和 sigmoid 激活函数来捕捉音频谱图表示以进行音频分类。由于注意力机制擅长捕捉全局特征，因此在音频/语音分类任务中表现显著（Gong
    et al., [2021](#bib.bib42)）。'
- en: •
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mockingjay: Mockingjay is an unsupervised speech representation model that
    uses multiple layers of bidirectional Transformer pre-trained encoders. It uses
    both past and future features for speech representation rather than only past
    information, which helps it to gather more information about the speech context.
    Mockingjay also can improve the performance of the supervised learning tasks as
    well where the amount of labeled data is low. Capturing more information helped
    to improve several speech representational tasks like speech classification and
    recognition (Liu et al., [2020](#bib.bib96)).'
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Mockingjay: Mockingjay 是一个无监督的语音表示模型，使用了多个层次的双向 Transformer 预训练编码器。它同时使用过去和未来的特征进行语音表示，而不仅仅是过去的信息，这有助于获取更多关于语音上下文的信息。Mockingjay
    还可以提升监督学习任务的表现，尤其是在标注数据量较少的情况下。捕捉更多信息有助于改善几个语音表示任务，如语音分类和识别（Liu et al., [2020](#bib.bib96)）。'
- en: •
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'XLS-R: XLS-R is a transformer-based self-supervised large-scale speech representation
    model which is trained with a large amount of data. It is built on the wav2vec
    discretization algorithm, whereas it uses Wav2Vec 2.0 model that is pretrained
    with multiple languages. The architecture contains multiple convolution encoders
    to map raw speech and the output from this stage is transferred to the transformer
    model(encoder module) as input which provides better audio representation finally.
    A large amount of training data is crucial for this model where a range of public
    speech is used and it performed well for multiple downstream multilingual speech
    tasks (Babu et al., [2022](#bib.bib6)).'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'XLS-R: XLS-R 是一个基于 Transformer 的自监督大规模语音表示模型，使用大量数据进行训练。它建立在 wav2vec 离散化算法上，使用了预训练有多种语言的
    Wav2Vec 2.0 模型。其架构包含多个卷积编码器，将原始语音映射到 Transformer 模型（编码器模块）作为输入，这样可以最终提供更好的音频表示。大量的训练数据对该模型至关重要，其中使用了多种公共语音数据，并且在多个下游多语言语音任务中表现良好（Babu
    et al., [2022](#bib.bib6)）。'
- en: •
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'UniSpeech: UniSpeech is a semi-supervised unified pre-trained model for speech
    representation. This model follows the Wav2Vec 2.0 architecture where it contains
    convolutional feature encoders that converts the raw audio to a higher-dimensional
    representation and this output is fed into the Transformer. This model is capable
    of learning to multitask while a quantizer is used in its architecture which helps
    to capture specific speech recognition information (Chen et al., [2022b](#bib.bib22)).'
  id: totrans-686
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'UniSpeech: UniSpeech 是一个半监督的统一预训练语音表示模型。该模型遵循 Wav2Vec 2.0 架构，包含卷积特征编码器，将原始音频转换为更高维度的表示，输出被送入
    Transformer。该模型能够进行多任务学习，同时其架构中使用了量化器，帮助捕捉特定的语音识别信息（Chen et al., [2022b](#bib.bib22)）。'
- en: 6.6 Signal Processing
  id: totrans-687
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 信号处理
- en: 'With the growing recognition of the usability of Transformer-based models across
    various sectors, researchers have started exploring their application in signal
    processing. This recent development of utilizing Transformer-based models in signal
    processing represents a novel approach that outperforms conventional methods in
    terms of performance. Signal processing involves the manipulation and analysis
    of various types of data, including signal status, information, frequency, amplitude,
    and more. While audio and speech are considered forms of signals, we have segregated
    the audio and speech sections to highlight the specific applications of Transformer-based
    models in those domains. Within the signal processing domain, we have focused
    on two distinct areas: wireless network signal processing and medical signal processing.
    These two fields exhibit distinct processing methods and functionalities due to
    their inherent differences. Here, we delve into both of these tasks and provide
    an overview of significant Transformer-based models that have demonstrated effectiveness
    in these specific domains.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对Transformer模型在各个领域可用性的认识不断提高，研究人员已经开始探索其在信号处理中的应用。这一利用Transformer模型进行信号处理的最新发展代表了一种新颖的方法，在性能上优于传统方法。信号处理涉及各种类型数据的操作和分析，包括信号状态、信息、频率、幅度等。虽然音频和语音被视为信号的一种形式，但我们将音频和语音部分分开，以突出Transformer模型在这些领域的具体应用。在信号处理领域，我们重点关注两个不同的领域：无线网络信号处理和医学信号处理。由于这两个领域的固有差异，它们表现出不同的处理方法和功能。在这里，我们深入探讨这两个任务，并概述了在这些特定领域中表现出有效性的显著Transformer模型。
- en: 6.6.1 Wireless Network & Signal Processing
  id: totrans-689
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1 无线网络与信号处理
- en: In the current era of the 21st century, wireless network communication has emerged
    as a prominent technology. However, the application of transformers in wireless
    network signal processing has not received substantial attention thus far. Consequently,
    the number of Transformer-inspired models developed for this field remains limited.
    Wireless network signal processing encompasses various tasks, including signal
    denoising, signal interface detection, wireless signal channel estimation, interface
    identification, signal classification, and more. Deep neural networks offer great
    potential for tackling these tasks effectively, and Transformer-based models have
    introduced significant advancements in this domain (Sun et al., [2017](#bib.bib162),
    Clerckx et al., [2021](#bib.bib28), Zhang et al., [2019](#bib.bib202), Chen et al.,
    [2019b](#bib.bib19)). In this section, we present several models that have made
    notable contributions to the enhancements in wireless communication networks and
    signal processing. The details of these models are provided in Table LABEL:tab:wireless_network.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 在21世纪的现今时代，无线网络通信已成为一种重要技术。然而，Transformer在无线网络信号处理中的应用迄今为止尚未受到充分关注。因此，针对这一领域开发的Transformer灵感模型数量仍然有限。无线网络信号处理涵盖了诸如信号去噪、信号接口检测、无线信号通道估计、接口识别、信号分类等多种任务。深度神经网络在有效处理这些任务方面具有巨大的潜力，而基于Transformer的模型在这一领域带来了重要的进展（Sun等，[2017](#bib.bib162)，Clerckx等，[2021](#bib.bib28)，Zhang等，[2019](#bib.bib202)，Chen等，[2019b](#bib.bib19)）。在本节中，我们介绍了几个在无线通信网络和信号处理方面做出显著贡献的模型。这些模型的详细信息见表LABEL:tab:wireless_network。
- en: 'Table 26: Transformer models for wireless network & signal processing'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 表26：用于无线网络和信号处理的Transformer模型
- en: '| Transformer Models | Task Accomplished | Year | Architecture (Encoder/ Decoder)
    | Pre-trained (Yes/NO) | Pre-training Dataset | Dataset (Fine-tuning, Training,
    Testing) |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| Transformer模型 | 完成任务 | 年份 | 架构（编码器/解码器） | 预训练（是/否） | 预训练数据集 | 数据集（微调、训练、测试）
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| SigT (Ren et al., [2022](#bib.bib138)) | Signal detection, channel estimation,
    interference suppression, and data decoding in MIMO-OFDM | 2022 | Encoder | No
    | NA | Peng Cheng laboratory(PCL), local area data |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| SigT (Ren等，[2022](#bib.bib138)) | 在MIMO-OFDM中进行信号检测、通道估计、干扰抑制和数据解码 | 2022
    | 编码器 | 否 | NA | 彭程实验室（PCL），局部数据 |'
- en: '| TSDN (Liu et al., [2022b](#bib.bib98)) | Remove Interference and nose from
    wireless signal | 2022 | Encoder & Decoder | No | NA | Wall NLoS, Foil NLOS, UWB
    dataset |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| TSDN (Liu等，[2022b](#bib.bib98)) | 移除无线信号中的干扰和噪声 | 2022 | 编码器 & 解码器 | 否 |
    NA | Wall NLoS, Foil NLOS, UWB数据集 |'
- en: '| ACNNT (Wang et al., [2021a](#bib.bib177)) | Wireless interface identification
    | 2021 | Encoder | No | NA | ST, BPSK, AM, NAM, SFM, LFM, 4FSK, 2FSK signal dataset
    |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| ACNNT (Wang et al., [2021a](#bib.bib177)) | 无线接口识别 | 2021 | 编码器 | 否 | 不适用
    | ST, BPSK, AM, NAM, SFM, LFM, 4FSK, 2FSK 信号数据集 |'
- en: '| MCformer (Hamidi-Rad & Jain, [2021](#bib.bib52)) | Automatic modulation classification
    complex raw radio signals | 2021 | Encoder | No | NA | RadioML2016.10b |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| MCformer (Hamidi-Rad & Jain, [2021](#bib.bib52)) | 自动调制分类复杂原始无线电信号 | 2021
    | 编码器 | 否 | 不适用 | RadioML2016.10b |'
- en: '| Quan-Transformer (Xie et al., [2022](#bib.bib186)) | Compress & recover channel
    state information | 2022 | Encoder & Decoder | No | NA | CsiNet, CLNet and CRNet
    |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| Quan-Transformer (Xie et al., [2022](#bib.bib186)) | 压缩与恢复信道状态信息 | 2022 |
    编码器与解码器 | 否 | 不适用 | CsiNet, CLNet 和 CRNet |'
- en: '|  |  |  |  |  |  |  |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |'
- en: •
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SigT: SigT is a wireless communication network signal receiver designed with
    a transformer architecture, capable of handling Multiple-input Multiple-output
    (MIMO-OFDM) signals. Leveraging the transformer’s encoder module, this innovative
    framework enables parallel data processing and performs essential tasks such as
    signal detection, channel estimation, and data decoding. Unlike traditional receivers
    that rely on distinct modules for each task, SigT seamlessly integrates these
    functions, providing a unified solution (Ren et al., [2022](#bib.bib138)).'
  id: totrans-701
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SigT：SigT 是一种无线通信网络信号接收器，采用变压器架构设计，能够处理多输入多输出（MIMO-OFDM）信号。利用变压器的编码器模块，这一创新框架支持并行数据处理，并执行信号检测、信道估计和数据解码等关键任务。与传统的接收器依赖于不同模块执行各项任务不同，SigT
    无缝集成了这些功能，提供了统一的解决方案（Ren et al., [2022](#bib.bib138)）。
- en: •
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TSDN: TSDN is an abbreviation for Transformer-based Signal Denoising Network.
    It refers to a signal denoising model based on transformers that aims to estimate
    the Angle-of-Arrival (AoA) of signals transmitted by users within a wireless communication
    network. This transformer-based model significantly enhances the accuracy of AoA
    estimation, especially in challenging non-line-of-sight (NLoS) environments where
    conventional methods often fall short in delivering the desired precision (Liu
    et al., [2022b](#bib.bib98)).'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TSDN：TSDN 是基于变压器的信号去噪网络的缩写。它指的是一种基于变压器的信号去噪模型，旨在估计无线通信网络中用户发送信号的到达角（AoA）。这一基于变压器的模型显著提高了AoA估计的准确性，特别是在传统方法往往无法提供所需精度的挑战性非视距（NLoS）环境中（Liu
    et al., [2022b](#bib.bib98)）。
- en: •
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ACNNT: The Augmented Convolution Neural Network with Transformer (ACNNT) is
    an architectural framework specifically designed for identifying interference
    within wireless networks. This model combines the power of Convolutional Neural
    Networks (CNN) and transformer architectures. The multiple CNN layers in ACNNT
    extract localized features from the input signal, while the transformer component
    captures global relationships between various elements of the input sequence.
    By exploiting the strengths of both CNN and Transformer, this model has demonstrated
    superior accuracy in the identification of wireless interference compared to conventional
    approaches (Wang et al., [2021a](#bib.bib177)).'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ACNNT：增强卷积神经网络与变压器（ACNNT）是一种专门设计用于识别无线网络干扰的架构框架。该模型结合了卷积神经网络（CNN）和变压器架构的强大功能。ACNNT
    中的多个 CNN 层从输入信号中提取局部特征，而变压器组件则捕捉输入序列中各元素之间的全局关系。通过利用 CNN 和变压器的优势，该模型在识别无线干扰方面表现出比传统方法更高的准确性（Wang
    et al., [2021a](#bib.bib177)）。
- en: •
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MCformer: MCformer, short for Modulation Classification Transformer, refers
    to a model architecture based on transformers that performs feature extraction
    from input signals and subsequently classifies them based on modulation. This
    architectural design combines Convolutional Neural Network (CNN) and self-attention
    layers, enabling the processing of intricate features within the signal and achieving
    superior accuracy in comparison to conventional approaches. The introduction of
    this model has brought about noteworthy advancements in a wireless network and
    communication signals, particularly in the realm of Automatic Modulation Classification,
    thereby enhancing system security and performance (Hamidi-Rad & Jain, [2021](#bib.bib52)).'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MCformer：MCformer，即调制分类 Transformer，指的是一种基于 Transformers 的模型架构，该架构从输入信号中提取特征，并基于调制进行分类。该架构设计结合了卷积神经网络（CNN）和自注意力层，能够处理信号中的复杂特征，并相较于传统方法实现更高的准确性。该模型的引入在无线网络和通信信号领域带来了显著进展，特别是在自动调制分类领域，从而提高了系统的安全性和性能（Hamidi-Rad
    & Jain，[2021](#bib.bib52)）。
- en: •
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quan-Transformer: It refers to a Transformer-based model specifically designed
    to perform quantization in wireless communication systems. Quantization is the
    vital process of converting a continuous signal into a discrete signal, and it
    plays a crucial role in network channel feedback processing. Channel feedback
    processing is essential for estimating channel state information, which in turn
    aids in adjusting signal transmission parameters. This feedback mechanism holds
    particular significance in the context of Reconfigurable Intelligent Surface (RIS)-aided
    wireless networks, a critical component of the 6th-generation communication system
    (Xie et al., [2022](#bib.bib186)).'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Quan-Transformer：指的是一种基于 Transformer 的模型，专门设计用于在无线通信系统中执行量化。量化是将连续信号转换为离散信号的关键过程，它在网络信道反馈处理中的作用至关重要。信道反馈处理对于估计信道状态信息至关重要，而这反过来又有助于调整信号传输参数。在可重构智能表面（RIS）辅助的无线网络中，这种反馈机制尤为重要，这是第六代通信系统的一个关键组成部分（Xie
    等，[2022](#bib.bib186)）。
- en: 6.6.2 Medical Signal processing
  id: totrans-710
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.2 医学信号处理
- en: 'The rise of healthcare data has resulted in the rapid growth of deep learning
    applications, enabling the automatic detection of pathologies, enhanced medical
    diagnosis, and improved healthcare services. These data can be categorized into
    three distinct forms: relational data ( symptoms, examinations, and laboratory
    tests), medical images, and biomedical signals (consisting of raw electronic and
    sound signals). While the application of deep learning models, particularly transformers,
    in the context of medical images has gained considerable attention and yielded
    promising results, the application of transformers to biomedical signals is still
    in its early stages. The majority of relevant studies have been published between
    the years 2021 and 2022, with a particular focus on the task of signal classification.
    We have summarized our findings regarding the application of transformers to biomedical
    signals in Table [27](#S6.T27 "Table 27 ‣ 6.6.2 Medical Signal processing ‣ 6.6
    Signal Processing ‣ 6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS
    ‣ A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks").'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗数据的增长导致了深度学习应用的快速发展，实现了病理的自动检测、医学诊断的增强以及医疗服务的改善。这些数据可以分为三种不同的形式：关系数据（症状、检查和实验室测试）、医学图像以及生物医学信号（包括原始电子信号和声音信号）。尽管深度学习模型，特别是
    Transformers，在医学图像中的应用已获得了相当多的关注并取得了良好成果，但 Transformers 在生物医学信号中的应用仍处于初期阶段。相关研究大多数发表于
    2021 年至 2022 年之间，特别集中在信号分类任务上。我们在表格 [27](#S6.T27 "Table 27 ‣ 6.6.2 Medical Signal
    processing ‣ 6.6 Signal Processing ‣ 6 APPLICATION-BASED CLASSIFICATION TAXONOMY
    OF TRANSFORMERS ‣ A Comprehensive Survey on Applications of Transformers for Deep
    Learning Tasks") 中总结了关于 Transformers 在生物医学信号应用的发现。
- en: •
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Epilepsy disease case:'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 癫痫病案例：
- en: Epilepsy is a serious debilitating condition for those it affects. Typically,
    its symptoms are detected through the use of electrical signals, such as electroencephalograms
    (EEGs) and magnetoencephalograms (MEGs). With the rise of deep learning, it is
    now possible to detect and predict epilepsy cases using its architecture. Utilizing
    transformers as their underlying framework, the following models have been developed
    to analyze electric signals for predicting and classifying epilepsy.
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 癫痫是一种严重的致残性疾病，影响着那些它所涉及的人。通常，其症状通过电信号（如脑电图EEG和磁脑电图MEG）来检测。随着深度学习的发展，现在可以利用其架构来检测和预测癫痫病例。利用变压器作为其基础框架，以下模型被开发用于分析电信号，以预测和分类癫痫。
- en: –
  id: totrans-715
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Three-tower transformer network (Yan et al., [2022a](#bib.bib190)): The purpose
    of this model is to predict epileptic seizures from EEG signals. A transformer-based
    model is used to perform binary classification of EEG signals based on three EEG
    features: time, frequency, and channel. This model processes EEG signals as a
    whole using a model that is based on the classic transformer, which contains three
    encoders: a time encoder, a frequency encoder, and a channel encoder. Compared
    to other models, such as CNN, the model shows better performance results in predicting
    epilepsy attacks.'
  id: totrans-716
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三塔变压器网络（Yan et al., [2022a](#bib.bib190)）：该模型的目的是从脑电图（EEG）信号中预测癫痫发作。该模型基于变压器，通过三个EEG特征（时间、频率和通道）对EEG信号进行二分类。该模型使用基于经典变压器的模型处理整体EEG信号，该模型包含三个编码器：时间编码器、频率编码器和通道编码器。与其他模型（如CNN）相比，该模型在预测癫痫发作的性能表现更佳。
- en: –
  id: totrans-717
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'TransHFO (Guo et al., [2022b](#bib.bib49)): (Transformer-based HFO) is a deep
    learning model based on BERT for classifying High-Frequency Oscillation (HFO)
    from normal control (NC). A transformer is used to detect the presence of HFO
    with high accuracy in one-dimensional magnetoencephalography (MEG) data in order
    to identify epileptic areas more precisely. Signal classification is performed
    under k-fold cross-validation and through various algorithms, including logistic
    regression, SMO, and the ResDen model. Due to the small dataset available, the
    authors propose to use the data augmentation technique “ADASYN”. Nevertheless,
    even with the addition of data, the dataset remains small, which makes the shallow
    transformer more efficient than the deep transformer with more layers.'
  id: totrans-718
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: TransHFO（Guo et al., [2022b](#bib.bib49)）：（基于变压器的HFO）是一个基于BERT的深度学习模型，用于从正常对照（NC）中分类高频振荡（HFO）。变压器用于在一维磁脑电图（MEG）数据中高精度地检测HFO的存在，以更精确地识别癫痫区域。信号分类通过k折交叉验证和各种算法（包括逻辑回归、SMO和ResDen模型）进行。由于数据集较小，作者建议使用数据增强技术“ADASYN”。尽管增加了数据，但数据集仍然较小，这使得浅层变压器比具有更多层的深层变压器更高效。
- en: Transformer Name Field of application Year Fully Transformer Architecture Signal
    type Transformer Task Dataset Three-tower transformer network (Yan et al., [2022a](#bib.bib190))
    Epilepsy 2022 YES EEG Classification of EEG signals CHB-MIT datasets TransHFO
    (Guo et al., [2022b](#bib.bib49)) Epilepsy 2022 YES MEG Classification of MEG
    signals 20 clinical patients TCN and Transformer-based model (Casal et al., [2022](#bib.bib14))
    Sleep pathologies 2022 No (using TCN which is based on CNN) Cardiac signals (
    Heart Rate) Classification of sleep stages Sleep Heart Health Study dataset Constrained
    transformer network (Che et al., [2021](#bib.bib15)) Heart disease 2021 No (Using
    CNN) ECG Classification of ECG signals 6877 patients CRT-NET (Liu et al., [2022a](#bib.bib97))
    Heart Disease 2022 No (Using CNN and Bi-directional GRU) ECG Classification and
    recognition of ECG signals MIT-BIH, CPSC arrhythmia clinical private data CAT
    (Yang et al., [2022](#bib.bib194)) Atrial Fibrillation 2022 No (Using MLP) ECG
    Classification of ECG signals Shaoxing database (more than 10000 patients)
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器名称 应用领域 年份 完全变压器架构 信号类型 变压器任务 数据集 三塔变压器网络（Yan et al., [2022a](#bib.bib190)）
    癫痫 2022 是 EEG EEG信号分类 CHB-MIT 数据集 TransHFO（Guo et al., [2022b](#bib.bib49)） 癫痫
    2022 是 MEG MEG信号分类 20名临床患者 TCN和基于变压器的模型（Casal et al., [2022](#bib.bib14)） 睡眠病理
    2022 否（使用基于CNN的TCN） 心脏信号（心率） 睡眠阶段分类 睡眠心脏健康研究数据集 约束变压器网络（Che et al., [2021](#bib.bib15)）
    心脏病 2021 否（使用CNN） ECG ECG信号分类 6877名患者 CRT-NET（Liu et al., [2022a](#bib.bib97)）
    心脏病 2022 否（使用CNN和双向GRU） ECG ECG信号分类与识别 MIT-BIH, CPSC 心律失常临床私有数据 CAT（Yang et al.,
    [2022](#bib.bib194)） 心房颤动 2022 否（使用MLP） ECG ECG信号分类 绍兴数据库（超过10000名患者）
- en: 'Table 27: Transformer models for medical signal processing'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 表 27：用于医学信号处理的变换器模型
- en: •
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cardiac diseases cases: heart diseases are among the areas in which researchers
    are interested in applying transformers. Using ECG signals, a transformer model
    can detect long-range dependencies and identify heart disease types based on their
    characteristics.'
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 心脏疾病案例：心脏病是研究人员有兴趣应用变换器的领域之一。使用心电图（ECG）信号，变换器模型可以检测长期依赖关系，并根据其特征识别心脏疾病类型。
- en: –
  id: totrans-723
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Constrained transformer network (Che et al., [2021](#bib.bib15)): Using CNN
    and transformer architecture, this model classifies heart arrhythmia disease based
    on temporal information in ECG (electrocardiogram) signals. There are also other
    models that use transformer encoders for classifying heart diseases (such as atrial
    fibrillation) using ECG signals, such as CRT-NET (Liu et al., [2022a](#bib.bib97))
    and CAT (Yang et al., [2022](#bib.bib194)) “Component-Aware Transformer”.'
  id: totrans-724
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 约束变换器网络（Che 等人，[2021](#bib.bib15)）：利用CNN和变换器架构，该模型基于ECG（心电图）信号中的时间信息对心律失常进行分类。还有其他模型使用变换器编码器来分类心脏疾病（如房颤），例如CRT-NET（Liu
    等人，[2022a](#bib.bib97)）和CAT（Yang 等人，[2022](#bib.bib194)）“组件感知变换器”。
- en: A major advantage of CAT’s model (Yang et al., [2022](#bib.bib194)) is the use
    of a large database containing data from over ten thousand patients for experiments.
    In contrast, the strength of CRT-NET (Liu et al., [2022a](#bib.bib97)) is the
    ability to extract different ECG features like waveforms, morphological characteristics,
    and time domain data, in order to identify many cardiovascular diseases.
  id: totrans-725
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: CAT模型（Yang 等人，[2022](#bib.bib194)）的主要优势之一是使用包含超过一万名患者数据的大型数据库进行实验。相比之下，CRT-NET（Liu
    等人，[2022a](#bib.bib97)）的优势在于能够提取不同的ECG特征，如波形、形态特征和时间域数据，以识别多种心血管疾病。
- en: –
  id: totrans-726
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'TCN and Transformer-based model (Casal et al., [2022](#bib.bib14)): An automatic
    sleep stage classification system based on 1-dimensional cardiac signals (Heart
    Rate). The classification is conducted in two steps: extracting features from
    signals using TCN ”Temporal Convolution Network”[], and modeling signal sequence
    dependencies using the standard Transformer architecture consisting of two stacks
    of encoders and a simplified decoder module. Based on a dataset of 5000 different
    participants, this study demonstrated that this new model outperforms other networks,
    such as CNN and RNN, which consume more memory and reduce process efficiency.'
  id: totrans-727
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: TCN和基于变换器的模型（Casal 等人，[2022](#bib.bib14)）：一个基于1维心脏信号（心率）的自动睡眠阶段分类系统。分类分为两个步骤：使用TCN“时间卷积网络”[]
    从信号中提取特征，以及使用包含两个编码器堆叠和简化解码器模块的标准变换器架构对信号序列依赖关系进行建模。基于5000名不同参与者的数据集，该研究表明，这一新模型优于其他网络，如CNN和RNN，这些网络消耗更多内存并降低处理效率。
- en: 7 Future Prospects and Challenges
  id: totrans-728
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来前景与挑战
- en: One of the primary objectives of this survey is to identify and highlight potential
    research directions for transformer applications, with the goal of expanding their
    range of applications beyond the currently popular fields of NLP and computer
    vision. Despite considerable research attention in these areas, there are still
    a number of areas that remain relatively unexplored with the potential for significant
    improvements in the future. In order to expand the application areas of Transformers,
    we have identified several potential directions for future research. These directions
    include but are not limited to the exploration of transformer-based models for
    speech recognition, recommendation systems, and natural language generation. In
    addition, further exploration of transformer-based approaches for multimodal tasks,
    such as combining audio and visual inputs, would be an interesting direction for
    future research. By pursuing these research directions, we hope to continue the
    advancement of transformer-based models and their utility in a broader range of
    applications.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的主要目标之一是识别和突出变换器应用的潜在研究方向，旨在将其应用范围扩展到目前流行的NLP和计算机视觉领域之外。尽管这些领域的研究得到了相当多的关注，但仍有许多领域相对未被探索，未来有显著改进的潜力。为了扩展变换器的应用领域，我们已经确定了几个未来研究的潜在方向。这些方向包括但不限于探索基于变换器的模型在语音识别、推荐系统和自然语言生成中的应用。此外，进一步探索基于变换器的多模态任务方法，如结合音频和视觉输入，将是未来研究的一个有趣方向。通过追求这些研究方向，我们希望继续推动基于变换器的模型及其在更广泛应用中的实用性。
- en: 7.1 Transformers in Wireless Network and Cloud Computing
  id: totrans-730
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 无线网络和云计算中的变压器
- en: While the majority of transformer applications have been in NLP and Computer
    Vision, there is an exciting potential for transformers in the wireless communication
    and cloud computing domains. Although there have been relatively fewer studies
    in this area, the ones that exist have demonstrated the enormous potential of
    transformers for improving various aspects of wireless signal communication and
    cloud workload computing. In this section, we discuss some of the transformer
    models that have been developed for wireless signal communication and cloud computing.
    These models have shown promising results in areas such as wireless interference
    recognition, wireless signal communication mitigation, and cloud workload forecasting.
    Moving forward, there are several potential directions for future research in
    both the wireless network and cloud domains. Some of the possible areas of focus
    for wireless communication include improving network security, enhancing the efficiency
    of wireless communication, and developing more accurate interference recognition
    models. On the other hand, for cloud computing, future work could focus on improving
    resource allocation and workload management, optimizing cloud performance, and
    enhancing data privacy and security.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数变压器应用集中在自然语言处理（NLP）和计算机视觉领域，但变压器在无线通信和云计算领域也具有令人兴奋的潜力。尽管在这方面的研究相对较少，但现有的研究已经展示了变压器在改善无线信号通信和云工作负载计算各个方面的巨大潜力。在本节中，我们讨论了一些为无线信号通信和云计算开发的变压器模型。这些模型在无线干扰识别、无线信号通信缓解和云工作负载预测等领域表现出了良好的结果。展望未来，无线网络和云领域都有若干潜在的研究方向。无线通信的可能关注点包括提高网络安全性、增强无线通信效率以及开发更精确的干扰识别模型。另一方面，对于云计算，未来的工作可以集中在改善资源分配和工作负载管理、优化云性能以及增强数据隐私和安全性上。
- en: 'Scope of future work for the wireless Signal communication:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 无线信号通信未来工作的范围：
- en: •
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Detection of Wireless Interference: The global attention capabilities of Transformers
    present an exciting avenue for future research in the field of wireless signal
    communication. By leveraging the power of Transformers, researchers can explore
    and develop more widely used applications for detecting wireless interference
    in communication systems. This can involve experimenting with various successive
    Transformer models to reduce complexity and enhance the efficiency of wireless
    interference recognition. This research can lead to improved communication systems
    that are more resilient to interference and provide better overall performance.'
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无线干扰检测：变压器的全球注意力能力为无线信号通信领域的未来研究提供了令人兴奋的途径。通过利用变压器的强大功能，研究人员可以探索和开发更多广泛应用于检测通信系统中无线干扰的应用。这可能涉及实验各种连续的变压器模型，以降低复杂性并提高无线干扰识别的效率。这项研究可以导致更为抗干扰且整体性能更好的通信系统。
- en: •
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhancing 5G & 6G Networks: As 5G and 6G networks gain popularity, there is
    significant potential for Transformers to contribute to this field. Advanced networking
    architectures, such as Reconfigurable Intelligent Surfaces (RIS) and Multiple-Output
    and Orthogonal Frequency-Division Multiplexing (MIMO-OFDM), play a crucial role
    in these networks. Transformers have shown promise in improving performance in
    these areas. Additionally, signal state feedback, which is essential for adjusting
    and updating networks based on signal state changes, can benefit from the parallel
    computational capability of Transformers. The ability of Transformers to handle
    multiple tasks simultaneously, including signal detection, channel estimation,
    and data decoding, makes them an effective alternative to conventional methods.'
  id: totrans-736
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强5G和6G网络：随着5G和6G网络的普及，变压器在这一领域具有显著的潜力。先进的网络架构，如可重构智能表面（RIS）和多输出正交频分复用（MIMO-OFDM），在这些网络中发挥了关键作用。变压器在提升这些领域的性能方面表现出了前景。此外，信号状态反馈对于根据信号状态变化调整和更新网络至关重要，变压器的并行计算能力可以带来好处。变压器同时处理多个任务的能力，包括信号检测、信道估计和数据解码，使其成为传统方法的有效替代方案。
- en: •
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integration of Transformers with Advanced Communication Technologies: Transformers
    can be integrated with other advanced communication technologies to further improve
    wireless signal communication. For example, combining Transformers with technologies
    like Massive MIMO, millimeter-wave communication, and cognitive radio can enhance
    the performance, capacity, and spectrum efficiency of wireless networks. Future
    research can focus on exploring these synergies and developing innovative solutions
    that leverage the unique capabilities of Transformers in conjunction with other
    cutting-edge communication technologies.'
  id: totrans-738
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与先进通信技术的Transformers整合：Transformers可以与其他先进通信技术集成，以进一步改善无线信号通信。例如，将Transformers与大规模MIMO、毫米波通信和认知无线电等技术结合，可以提高无线网络的性能、容量和频谱效率。未来的研究可以集中于探索这些协同效应，并开发创新的解决方案，将Transformers的独特能力与其他前沿通信技术结合起来。
- en: 'Future possibilities for the Cloud:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的未来可能性：
- en: •
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Advancements in Cloud Computing: With the increasing application of the Internet
    of Things (IoT), the cloud plays a crucial role in supporting and managing IoT
    devices. Transformers offer exciting possibilities for advancing cloud capabilities
    in various tasks, such as early attack and anomaly detection. By leveraging different
    Transformer approaches, the cloud can learn and adapt to its behavior, bringing
    more stability and security. Additionally, Transformers can be applied to cloud
    computing tasks like task scheduling and memory allocation. The multi-head attention
    and long-range attention features of the Transformers model make it well-suited
    for optimizing resource allocation and improving overall performance in cloud
    environments.'
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 云计算的进展：随着物联网（IoT）应用的增加，云在支持和管理IoT设备方面发挥着关键作用。Transformers为提高云在各种任务中的能力提供了令人兴奋的可能性，例如早期攻击和异常检测。通过利用不同的Transformer方法，云可以学习并适应其行为，从而带来更多的稳定性和安全性。此外，Transformers还可以应用于云计算任务，如任务调度和内存分配。Transformers模型的多头注意力和长程注意力特性使其非常适合优化资源分配和提高云环境中的整体性能。
- en: •
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformation in Mobile Edge Computing (MEC) and Mobile Edge Caching (MEC):
    In the context of advanced 6G networking systems, Mobile Edge Computing (MEC)
    and Mobile Edge Caching (MEC) play vital roles in reducing communication latency.
    Transformers have demonstrated significant potential in enhancing MEC and MEC
    through their parallel computational capabilities. Transformers can be applied
    to predict popular content, improve content management, optimize resource allocation,
    and enhance data transmission in MEC systems. By leveraging Transformers, the
    mobile cloud can respond and process user requests faster, resulting in reduced
    network response times and faster data transmission.'
  id: totrans-743
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 移动边缘计算（MEC）和移动边缘缓存（MEC）的变革：在先进的6G网络系统背景下，移动边缘计算（MEC）和移动边缘缓存（MEC）在减少通信延迟方面发挥着重要作用。Transformers在增强MEC和MEC方面展示了显著的潜力，得益于其并行计算能力。Transformers可以应用于预测热门内容、改进内容管理、优化资源分配以及提高MEC系统中的数据传输。通过利用Transformers，移动云可以更快地响应和处理用户请求，从而减少网络响应时间和加快数据传输速度。
- en: •
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intelligent Resource Management in the Cloud: Transformers offer opportunities
    for intelligent resource management in cloud environments. By applying Transformers
    to tasks like workload prediction, resource allocation, and load balancing, cloud
    systems can optimize resource utilization and enhance performance. Transformers’
    ability to capture long-range dependencies and handle complex patterns makes them
    well-suited for efficiently managing cloud resources and improving overall system
    efficiency.'
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 云中的智能资源管理：Transformers为云环境中的智能资源管理提供了机会。通过将Transformers应用于工作负载预测、资源分配和负载均衡等任务，云系统可以优化资源利用和提升性能。Transformers捕捉长程依赖关系和处理复杂模式的能力使其非常适合高效管理云资源并提高整体系统效率。
- en: •
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Security and Privacy in the Cloud: Transformers can contribute to enhancing
    security and privacy in the cloud by enabling advanced threat detection, anomaly
    detection, and data privacy protection mechanisms. Transformers can analyze large
    volumes of data, identify patterns, and detect potential security breaches or
    anomalies in real-time. Additionally, Transformers can be utilized for data anonymization
    and privacy-preserving computations, ensuring that sensitive information remains
    protected in cloud-based systems.'
  id: totrans-747
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 云中的安全与隐私：变换器可以通过启用先进的威胁检测、异常检测和数据隐私保护机制来提升云中的安全性和隐私。变换器可以分析大量数据，识别模式，并实时检测潜在的安全漏洞或异常。此外，变换器还可以用于数据匿名化和隐私保护计算，确保敏感信息在基于云的系统中得到保护。
- en: 7.2 Medical Image & Signal Processing
  id: totrans-748
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 医学图像与信号处理
- en: 'Two types of medical data are discussed in this paper: images and signals.
    According to our literature review, segmentation, and classification are the most
    transformer-based medical applications, followed by image translation (Yan et al.,
    [2022b](#bib.bib191), Chen et al., [2022c](#bib.bib23)). In the context of medical
    images, we commonly see the reuse of existing transformers, such as BERT, ViT,
    and SWIN, regardless of how the original model was modified. Further, we observe
    that various types of medical images are used to conduct transformer-based medical
    applications, such as 2D images (He et al., [2022](#bib.bib57), Gu et al., [2022](#bib.bib45)),
    3D images (Jiang et al., [2022b](#bib.bib72), Liang et al., [2022](#bib.bib94),
    Zhou et al., [2021a](#bib.bib209), Zhu et al., [2022](#bib.bib212)) and multi-mode
    images (Sun et al., [2021b](#bib.bib164)).'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了两种类型的医学数据：图像和信号。根据我们的文献综述，分割和分类是基于变换器的医学应用中最常见的，其次是图像翻译（Yan等，[2022b](#bib.bib191)，Chen等，[2022c](#bib.bib23)）。在医学图像的背景下，我们通常看到现有的变换器被重复使用，如BERT、ViT和SWIN，无论原始模型如何修改。此外，我们观察到各种类型的医学图像被用来进行基于变换器的医学应用，如2D图像（He等，[2022](#bib.bib57)，Gu等，[2022](#bib.bib45)），3D图像（Jiang等，[2022b](#bib.bib72)，Liang等，[2022](#bib.bib94)，Zhou等，[2021a](#bib.bib209)，Zhu等，[2022](#bib.bib212)）和多模态图像（Sun等，[2021b](#bib.bib164)）。
- en: 'The selected research papers for this survey span the years 2021 to 2022, indicating
    that the use of transformer architecture in the medical field is still in its
    nascent stages. Despite its early adoption, there has been a remarkable influx
    of excellent publications exploring transformer applications in the analysis of
    medical images within this relatively short period. However, several challenges
    persist in applying transformers to medical images that need to be addressed and
    overcome:'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查选择的研究论文涵盖了2021至2022年，表明变换器架构在医学领域的应用仍处于初期阶段。尽管刚刚开始采用，但在这一相对短暂的时间内，已经有大量优秀的出版物探索了变换器在医学图像分析中的应用。然而，将变换器应用于医学图像仍面临几个挑战，需要解决和克服：
- en: •
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited focus on 3D images: There is a scarcity of studies that specifically
    address the application of transformers to 3D medical images. Most research has
    been concentrated on 2D images, indicating the need for further exploration and
    development in this area.'
  id: totrans-752
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对3D图像的关注有限：专门研究变换器在3D医学图像应用的研究较少。大多数研究集中于2D图像，表明该领域还需要进一步的探索和发展。
- en: •
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Small and private medical image databases: Medical image databases are often
    small and privately owned due to legal and ethical concerns regarding patient
    data privacy (López-Linares et al., [2020](#bib.bib104)). This limits the availability
    of large-scale datasets necessary for training transformer models effectively.'
  id: totrans-754
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小型和私有的医学图像数据库：由于对患者数据隐私的法律和伦理关注，医学图像数据库通常较小且由私人拥有（López-Linares等，[2020](#bib.bib104)）。这限制了有效训练变换器模型所需的大规模数据集的可用性。
- en: •
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computational complexity in high-resolution imaging: Transformer-based architectures
    encounter computational challenges when dealing with high-resolution medical images.
    The self-attention mechanism, which is integral to transformers, becomes computationally
    demanding for large images. However, some models, like DI-UNET (Wu et al., [2022](#bib.bib185)),
    have introduced enhanced self-attention mechanisms to handle higher resolution
    images effectively.'
  id: totrans-756
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高分辨率成像中的计算复杂性：基于变换器的架构在处理高分辨率医学图像时遇到计算挑战。自注意力机制是变换器的核心，对于大图像来说计算要求很高。然而，一些模型，如DI-UNET（Wu等，[2022](#bib.bib185)），引入了增强的自注意力机制，以有效处理更高分辨率的图像。
- en: •
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited number of fully developed transformer-based models: The development
    of transformer-based models for processing medical images is still relatively
    nascent. Due to the computational complexity and parameter requirements of transformers,
    existing architectures often combine deep learning techniques like CNNs and GANs
    with transformers (Ma et al., [2022](#bib.bib107)). Knowledge distillation techniques
    may offer a viable solution for training transformer models with limited computational
    and storage resources (Leng et al., [2022](#bib.bib86)).'
  id: totrans-758
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全开发的基于变换器的模型数量有限：用于处理医学图像的变换器模型的开发仍相对初步。由于变换器的计算复杂性和参数需求，现有架构通常将深度学习技术如CNN和GAN与变换器结合使用（Ma
    et al., [2022](#bib.bib107)）。知识蒸馏技术可能为训练变换器模型提供了一个切实可行的解决方案，尤其是在计算和存储资源有限的情况下（Leng
    et al., [2022](#bib.bib86)）。
- en: 'Moreover, the application of transformers to bio-signals is relatively limited
    compared to medical images. There are two main challenges that transformers face
    in the domain of biomedical signals:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与医学图像相比，变换器在生物信号中的应用相对有限。在生物医学信号领域，变换器面临两个主要挑战：
- en: •
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Small bio-signal databases: Bio-signal databases often have limited sizes,
    which poses challenges for training and validating transformer models effectively.
    For instance, in a study mentioned by (Guo et al., [2022b](#bib.bib49)), only
    20 patients were included, which is considered insufficient to establish the effectiveness
    of a model. To mitigate the limitations of small databases, some studies have
    proposed the use of virtual sample generation techniques like ADASYN (He et al.,
    [2008](#bib.bib56)) to augment the dataset.'
  id: totrans-761
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小型生物信号数据库：生物信号数据库通常规模有限，这对有效训练和验证变换器模型提出了挑战。例如，在(Guo et al., [2022b](#bib.bib49))提到的一项研究中，仅包含20名患者，这被认为不足以证明模型的有效性。为了缓解小型数据库的限制，一些研究提出了使用如ADASYN
    (He et al., [2008](#bib.bib56))的虚拟样本生成技术来扩充数据集。
- en: •
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited availability of transformer-based models: Currently, there is a scarcity
    of models that are exclusively based on transformers for processing biomedical
    signals. The application of transformers in this context is still relatively unexplored,
    and more research is needed to develop dedicated transformer architectures for
    bio-signal analysis and processing.'
  id: totrans-763
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于变换器的模型可用性有限：目前，专门基于变换器的生物医学信号处理模型仍较为稀缺。在这一背景下，变换器的应用仍较为未开发，需进一步研究以开发专门的变换器架构用于生物信号分析和处理。
- en: 7.3 Reinforcement Learning
  id: totrans-764
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 强化学习
- en: 'The integration of transformers with deep reinforcement learning (RL) methods
    has emerged as a promising approach for enhancing sequential decision-making processes.
    Within this domain, two main research categories can be identified: architecture
    enhancement and trajectory optimization.'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 将变换器与深度强化学习(RL)方法结合已成为提升序列决策过程的有前景的方法。在这一领域，可以识别出两个主要研究类别：架构增强和轨迹优化。
- en: In the ”architecture enhancement” category, transformers are applied to RL problems
    based on traditional RL paradigms. This involves leveraging the capabilities of
    transformers to improve the representation and processing of RL states and actions.
    On the other hand, the ”trajectory optimization” approach treats RL problems as
    sequence modeling tasks. It involves training a joint state-action model over
    entire trajectories, utilizing transformers to learn policies from static datasets,
    and leveraging the transformers’ ability to model long sequences.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 在“架构增强”类别中，变换器应用于基于传统强化学习(RL)范式的RL问题。这涉及利用变换器的能力来改善RL状态和动作的表示和处理。另一方面，“轨迹优化”方法将RL问题视为序列建模任务。这涉及训练整个轨迹上的联合状态-动作模型，利用变换器从静态数据集中学习策略，并利用变换器对长序列建模的能力。
- en: Deep RL heavily relies on interactions with the environment to collect data
    dynamically Rjoub et al. ([2019](#bib.bib142); [2021](#bib.bib141)). However,
    in certain scenarios such as expensive environments like robotic applications
    or autonomous vehicles, collecting sufficient training data through real-time
    interaction may be challenging. To address this, offline RL techniques have been
    developed, which leverage deep networks to learn optimal policies from static
    datasets without direct environment interaction. In deep RL settings, transformers
    are often used to replace traditional components like convolutional neural networks
    (CNNs) or long short-term memory (LSTM) networks Rjoub et al. ([2022](#bib.bib143)),
    providing memory-awareness and improved modeling capabilities to the agent network.
    However, standard transformer structures applied directly to decision-making tasks
    may suffer from stability issues. To overcome this limitation, researchers have
    proposed modified transformer architectures, such as GtrX (Parisotto et al., [2020](#bib.bib123)),
    as an alternative solution.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习严重依赖与环境的互动以动态收集数据（Rjoub et al., [2019](#bib.bib142); [2021](#bib.bib141)）。然而，在某些场景下，如昂贵的环境（如机器人应用或自主驾驶车辆），通过实时互动收集足够的训练数据可能具有挑战性。为了解决这个问题，已经开发了离线强化学习技术，它利用深度网络从静态数据集中学习最佳策略，而无需直接与环境互动。在深度强化学习环境中，变换器通常用于替代传统组件，如卷积神经网络（CNNs）或长短期记忆（LSTM）网络（Rjoub
    et al., [2022](#bib.bib143)），为代理网络提供记忆意识和改进的建模能力。然而，直接应用于决策任务的标准变换器结构可能会面临稳定性问题。为克服这一限制，研究人员提出了改进的变换器架构，如GtrX（Parisotto
    et al., [2020](#bib.bib123)），作为一种替代解决方案。
- en: In summary, approaches like Decision Transformer and Trajectory Transformer
    have addressed RL problems as sequence modeling tasks, harnessing the power of
    transformer architectures to model sequential trajectories (Chen et al., [2021](#bib.bib18),
    Janner et al., [2021](#bib.bib68)). While these methods show promise in RL tasks,
    there is still significant room for improvement. Treating RL as sequence modeling
    simplifies certain limitations of traditional RL algorithms but may also overlook
    their advantages. Therefore, an interesting direction for further exploration
    is the integration of traditional RL algorithms with sequence modeling using transformers,
    combining the strengths of both approaches.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，像**决策变换器**和**轨迹变换器**这样的研究方法将强化学习（RL）问题视为序列建模任务，利用变换器架构的力量来建模序列轨迹（Chen et
    al., [2021](#bib.bib18), Janner et al., [2021](#bib.bib68)）。虽然这些方法在强化学习任务中展现了潜力，但仍有很大的改进空间。将强化学习视为序列建模可以简化传统强化学习算法的某些局限性，但也可能忽略它们的优势。因此，进一步探索的一个有趣方向是将传统强化学习算法与变换器序列建模相结合，融合两种方法的优点。
- en: 7.4 Other Prospects
  id: totrans-769
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 其他前景
- en: The successful application of transformers in the field of NLP has sparked interest
    and exploration in various other domains. Researchers have been inspired to apply
    transformer models to diverse areas, leading to promising developments. For instance,
    the transformer model BERT has been utilized to model proteins, which, similar
    to natural language, can be considered as sequential data (Vig et al., [2021](#bib.bib173)).
    Additionally, the transformer model GPT-2 has been employed to automatically fix
    JavaScript software bugs and generate patches without human intervention (Lajkó
    et al., [2022](#bib.bib84)).
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在自然语言处理（NLP）领域的成功应用激发了在其他各种领域的兴趣和探索。研究人员受到启发，将变换器模型应用于不同的领域，取得了令人鼓舞的进展。例如，变换器模型BERT已被用于建模蛋白质，这与自然语言类似，可以视为序列数据（Vig
    et al., [2021](#bib.bib173)）。此外，变换器模型GPT-2被用于自动修复JavaScript软件漏洞并生成补丁，无需人工干预（Lajkó
    et al., [2022](#bib.bib84)）。
- en: Beyond its impact in traditional machine learning and deep learning domains,
    transformers have found applications in industrial studies as well. They have
    demonstrated impressive performance in various tasks, ranging from predicting
    the state-of-charge of lithium batteries (Shen et al., [2022a](#bib.bib150)) to
    classifying vibration signals in mechanical structures (Jin & Chen, [2021](#bib.bib74)).
    Notably, transformers have showcased superior capabilities compared to Graph Neural
    Networks (GNNs) in constructing meta-paths from different types of edges in heterogeneous
    graphs (Yun et al., [2019](#bib.bib200)). This highlights the potential of transformers
    in handling complex and diverse data structures.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在传统机器学习和深度学习领域的影响，变压器还在工业研究中找到了应用。它们在各种任务中表现出了令人印象深刻的性能，从预测锂电池的充电状态（Shen 等人，[2022a](#bib.bib150)）到分类机械结构中的振动信号（Jin
    & Chen，[2021](#bib.bib74)）。值得注意的是，变压器在从异构图中不同类型的边构建元路径时，展示了比图神经网络（GNNs）更优越的能力（Yun
    等人，[2019](#bib.bib200)）。这突显了变压器在处理复杂和多样数据结构中的潜力。
- en: Another intriguing future application of transformers lies in the field of ”Generative
    Art,” where intelligent systems are leveraged for automated artistic creation,
    including images, music, and poetry. While image generation is a well-explored
    application area for transformers, often focused on natural or medical images,
    the domain of artistic image generation remains relatively unexplored. However,
    there have been some initial models based on transformers, such as ”AffectGAN,”
    which generates images based on semantic text and emotional expressions using
    transformer models (Galanos et al., [2021](#bib.bib40)). The exploration of transformers
    in generative art has significant untapped potential for further advancements
    and creative outputs.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的另一个令人兴奋的未来应用领域是“生成艺术”，在这个领域中，智能系统被用来自动进行艺术创作，包括图像、音乐和诗歌。虽然图像生成是变压器的一个广泛研究的应用领域，通常集中在自然或医学图像上，但艺术图像生成领域仍然相对未被充分探索。然而，已经有一些基于变压器的初步模型，如“AffectGAN”，它通过使用变压器模型生成基于语义文本和情感表达的图像（Galanos
    等人，[2021](#bib.bib40)）。在生成艺术中探索变压器具有巨大的未开发潜力，未来有望带来进一步的进展和创意输出。
- en: Overall, the application of transformers extends beyond NLP and showcases immense
    potential in various domains, ranging from scientific research to industrial applications
    and artistic creativity. Continued exploration and innovation in these areas will
    further expand the possibilities and impact of transformers in the future.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，变压器的应用超越了自然语言处理（NLP），在各个领域展示了巨大的潜力，从科学研究到工业应用，再到艺术创作。继续在这些领域进行探索和创新将进一步拓展变压器的可能性和未来的影响。
- en: 8 Conclusion
  id: totrans-774
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'The transformer, as a deep neural network, has demonstrated superior performance
    compared to traditional recurrence-based models in processing sequential data.
    Its ability to capture long-term dependencies and leverage parallel computation
    has made it a dominant force in various fields such as NLP, computer vision, and
    more. In this survey, we conducted a comprehensive overview of transformer models’
    applications in different deep learning tasks and proposed a new taxonomy based
    on the top five fields and respective tasks: NLP, Computer Vision, Multi-Modality,
    Audio & Speech, and Signal Processing. By examining the advancements in each field,
    we provided insights into the current research focus and progress of transformer
    models. This survey serves as a valuable reference for researchers seeking a deeper
    understanding of transformer applications and aims to inspire further exploration
    of transformers across various tasks. Additionally, we plan to extend our investigation
    to emerging fields like wireless networks, cloud computing, reinforcement learning,
    and others, to uncover new possibilities for transformer utilization. The rapid
    expansion of transformer applications in diverse domains showcases its versatility
    and potential for continued growth. With ongoing advancements and novel use cases,
    transformers are poised to shape the future of deep learning and contribute to
    advancements in fields beyond the traditional realms of NLP and computer vision.'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器作为一种深度神经网络，在处理序列数据方面表现出了优于传统递归模型的卓越性能。它捕捉长期依赖关系和利用并行计算的能力，使其在自然语言处理、计算机视觉等多个领域中成为主导力量。在本次综述中，我们对变换器模型在不同深度学习任务中的应用进行了全面的概述，并基于五个主要领域及相关任务提出了一种新的分类法：自然语言处理、计算机视觉、多模态、音频与语音，以及信号处理。通过考察每个领域的进展，我们提供了对变换器模型当前研究重点和进展的见解。这次综述为研究人员提供了关于变换器应用的宝贵参考，并旨在激发对变换器在各种任务中进一步探索的兴趣。此外，我们计划将调查扩展到无线网络、云计算、强化学习等新兴领域，以发现变换器利用的新可能性。变换器在各个领域的快速扩展展示了其多样性和持续增长的潜力。随着持续的进展和新颖的应用案例，变换器有望塑造深度学习的未来，并推动自然语言处理和计算机视觉之外领域的进步。
- en: References
  id: totrans-776
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Acheampong et al. (2021) Acheampong, F. A., Nunoo-Mensah, H., & Chen, W. (2021).
    Transformer models for text-based emotion detection: a review of bert-based approaches.
    Artif. Intell. Rev., 54, 5789–5829.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acheampong 等（2021）Acheampong, F. A., Nunoo-Mensah, H., & Chen, W.（2021）。基于文本的情感检测的变换器模型：基于
    BERT 方法的综述。人工智能评论，54，5789–5829。
- en: 'Ahmed et al. (2021) Ahmed, S. A. A., Awais, M., & Kittler, J. (2021). Sit:
    Self-supervised vision transformer. CoRR, abs/2104.03602. URL: [https://arxiv.org/abs/2104.03602](https://arxiv.org/abs/2104.03602).
    [arXiv:2104.03602](http://arxiv.org/abs/2104.03602).'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed 等（2021）Ahmed, S. A. A., Awais, M., & Kittler, J.（2021）。Sit：自监督视觉变换器。CoRR,
    abs/2104.03602。网址：[https://arxiv.org/abs/2104.03602](https://arxiv.org/abs/2104.03602)。
    [arXiv:2104.03602](http://arxiv.org/abs/2104.03602)。
- en: 'Akbari et al. (2021) Akbari, H., Yuan, L., Qian, R., Chuang, W., Chang, S.,
    Cui, Y., & Gong, B. (2021). VATT: transformers for multimodal self-supervised
    learning from raw video, audio and text. In M. Ranzato, A. Beygelzimer, Y. N.
    Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing
    Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS,
    December 6-14, 2021, virtual (pp. 24206--24221).'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akbari 等（2021）Akbari, H., Yuan, L., Qian, R., Chuang, W., Chang, S., Cui, Y.,
    & Gong, B.（2021）。VATT：用于多模态自监督学习的变换器，从原始视频、音频和文本中学习。发表于 M. Ranzato, A. Beygelzimer,
    Y. N. Dauphin, P. Liang, & J. W. Vaughan（编辑），神经信息处理系统进展 34：神经信息处理系统年会，NeurIPS，2021年12月6-14日，虚拟（第24206--24221页）。
- en: 'Antol et al. (2015) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,
    Zitnick, C. L., & Parikh, D. (2015). VQA: visual question answering. In IEEE International
    Conference on Computer Vision, ICCV, Santiago, Chile, December 7-13 (pp. 2425--2433).
    IEEE Computer Society.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antol 等（2015）Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick,
    C. L., & Parikh, D.（2015）。VQA：视觉问答。发表于 IEEE 国际计算机视觉会议，ICCV，智利圣地亚哥，12月7-13日（第2425--2433页）。IEEE计算机学会。
- en: 'Arnab et al. (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M.,
    & Schmid, C. (2021). Vivit: A video vision transformer. In 2021 IEEE/CVF International
    Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17,
    2021 (pp. 6816--6826). IEEE.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arnab 等（2021）Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., & Schmid,
    C.（2021）。Vivit：视频视觉变换器。发表于 2021 IEEE/CVF 国际计算机视觉会议，ICCV 2021，加拿大蒙特利尔，2021年10月10-17日（第6816--6826页）。IEEE。
- en: 'Babu et al. (2022) Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal,
    N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A.,
    & Auli, M. (2022). XLS-R: self-supervised cross-lingual speech representation
    learning at scale. In H. Ko, & J. H. L. Hansen (Eds.), Interspeech, 23rd Annual
    Conference of the International Speech Communication Association, Incheon, Korea,
    18-22 September (pp. 2278--2282). ISCA.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babu等人（2022）Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N.,
    Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., & Auli,
    M. (2022). XLS-R：大规模自监督跨语言语音表示学习。收录于H. Ko, & J. H. L. Hansen（编），国际语音通信协会第23届年度会议，Interspeech，韩国仁川，9月18-22日（第2278-2282页）。ISCA。
- en: 'Baevski et al. (2020a) Baevski, A., Schneider, S., & Auli, M. (2020a). vq-wav2vec:
    Self-supervised learning of discrete speech representations. In 8th International
    Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30.
    OpenReview.net.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baevski等人（2020a）Baevski, A., Schneider, S., & Auli, M. (2020a). vq-wav2vec：离散语音表示的自监督学习。收录于第8届国际学习表征会议，ICLR，埃塞俄比亚亚的斯亚贝巴，4月26-30日。OpenReview.net。
- en: 'Baevski et al. (2020b) Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020b).
    wav2vec 2.0: A framework for self-supervised learning of speech representations.
    In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems, NeurIPS, December 6-12, virtual.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baevski等人（2020b）Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020b). wav2vec
    2.0：自监督学习语音表示的框架。收录于H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin（编），神经信息处理系统33：年度神经信息处理系统会议，NeurIPS，12月6-12日，虚拟会议。
- en: 'Bao et al. (2022) Bao, H., Dong, L., Piao, S., & Wei, F. (2022). Beit: BERT
    pre-training of image transformers. In The Tenth International Conference on Learning
    Representations, ICLR Virtual Event, April 25-29. OpenReview.net.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bao等人（2022）Bao, H., Dong, L., Piao, S., & Wei, F. (2022). Beit: BERT预训练的图像变换器。收录于第十届国际学习表征会议，ICLR虚拟活动，4月25-29日。OpenReview.net。'
- en: Bogatinovski et al. (2022) Bogatinovski, J., Todorovski, L., Dzeroski, S., &
    Kocev, D. (2022). Comprehensive comparative study of multi-label classification
    methods. Expert Syst. Appl., 203, 117215.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogatinovski等人（2022）Bogatinovski, J., Todorovski, L., Dzeroski, S., & Kocev,
    D. (2022). 多标签分类方法的全面比较研究。专家系统应用，203，117215。
- en: 'Brasoveanu & Andonie (2020) Brasoveanu, A. M. P., & Andonie, R. (2020). Visualizing
    transformers for NLP: A brief survey. In 24th International Conference on Information
    Visualisation, IV 2020, Melbourne, Australia, September 7-11, 2020 (pp. 270--279).
    IEEE.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brasoveanu & Andonie（2020）Brasoveanu, A. M. P., & Andonie, R. (2020). NLP中的变换器可视化：简要调查。收录于第24届国际信息可视化会议，IV
    2020，澳大利亚墨尔本，2020年9月7-11日（第270-279页）。IEEE。
- en: 'Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., & Amodei, D. (2020). Language models are few-shot learners. In H. Larochelle,
    M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS, December 6-12, virtual.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.
    (2020). 语言模型是少样本学习者。收录于H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H.
    Lin（编），神经信息处理系统33：2020年度神经信息处理系统会议，NeurIPS，12月6-12日，虚拟会议。
- en: Carion et al. (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
    A., & Zagoruyko, S. (2020). End-to-end object detection with transformers. In
    A. Vedaldi, H. Bischof, T. Brox, & J. Frahm (Eds.), Computer Vision - ECCV - 16th
    European Conference, Glasgow, UK, August 23-28, Proceedings, Part I (pp. 213--229).
    Springer volume 12346 of Lecture Notes in Computer Science.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carion等人（2020）Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A.,
    & Zagoruyko, S. (2020). 基于变换器的端到端目标检测。收录于A. Vedaldi, H. Bischof, T. Brox, & J.
    Frahm（编），计算机视觉 - ECCV - 第16届欧洲会议，英国格拉斯哥，8月23-28日，论文集，第I部分（第213-229页）。Springer计算机科学讲义系列第12346卷。
- en: Casal et al. (2022) Casal, R., Persia, L. E. D., & Schlotthauer, G. (2022).
    Temporal convolutional networks and transformers for classifying the sleep stage
    in awake or asleep using pulse oximetry signals. J. Comput. Sci., 59, 101544.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casal等（2022）Casal, R., Persia, L. E. D., & Schlotthauer, G. (2022). 时序卷积网络和变换器用于通过脉搏血氧信号分类清醒或睡眠中的睡眠阶段。计算机科学杂志，59,
    101544。
- en: Che et al. (2021) Che, C., Zhang, P., Zhu, M., Qu, Y., & Jin, B. (2021). Constrained
    transformer network for ECG signal processing and arrhythmia classification. BMC
    Medical Informatics Decis. Mak., 21, 184.
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Che等（2021）Che, C., Zhang, P., Zhu, M., Qu, Y., & Jin, B. (2021). 约束变换器网络用于ECG信号处理和心律失常分类。BMC医学信息学与决策制定，21,
    184。
- en: 'Chen et al. (2019a) Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., & Su,
    J. (2019a). This looks like that: Deep learning for interpretable image recognition.
    In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, &
    R. Garnett (Eds.), Advances in Neural Information Processing Systems 32: Annual
    Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
    8-14, 2019, Vancouver, BC, Canada (pp. 8928--8939).'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2019a）Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., & Su, J. (2019a).
    这看起来像那样：用于可解释图像识别的深度学习。在H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc,
    E. B. Fox, & R. Garnett（编者），神经信息处理系统32：神经信息处理系统2019年会，NeurIPS 2019，2019年12月8-14日，温哥华，加拿大（第8928--8939页）。
- en: 'Chen et al. (2020a) Chen, J., Mao, Q., & Liu, D. (2020a). Dual-path transformer
    network: Direct context-aware modeling for end-to-end monaural speech separation.
    In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech, 21st Annual Conference of
    the International Speech Communication Association, Virtual Event, Shanghai, China,
    25-29 October (pp. 2642--2646). ISCA.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020a）Chen, J., Mao, Q., & Liu, D. (2020a). 双路径变换器网络：直接上下文感知建模用于端到端单耳语音分离。在H.
    Meng, B. Xu, & T. F. Zheng（编者），国际语音通信协会第21届年会，虚拟活动，上海，中国，10月25-29日（第2642--2646页）。ISCA。
- en: 'Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision transformer: Reinforcement
    learning via sequence modeling. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin,
    P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems
    34: Annual Conference on Neural Information Processing Systems, NeurIPS, December
    6-14, virtual (pp. 15084--15097).'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M.,
    Abbeel, P., Srinivas, A., & Mordatch, I. (2021). 决策变换器：通过序列建模进行强化学习。在M. Ranzato,
    A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan（编者），神经信息处理系统34：神经信息处理系统年会，NeurIPS，12月6-14日，虚拟（第15084--15097页）。
- en: 'Chen et al. (2019b) Chen, M., Challita, U., Saad, W., Yin, C., & Debbah, M.
    (2019b). Artificial neural networks-based machine learning for wireless networks:
    A tutorial. IEEE Commun. Surv. Tutorials, 21, 3039--3071.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2019b）Chen, M., Challita, U., Saad, W., Yin, C., & Debbah, M. (2019b).
    基于人工神经网络的无线网络机器学习：教程。IEEE通信调查与教程，21, 3039--3071。
- en: Chen et al. (2020b) Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,
    D., & Sutskever, I. (2020b). Generative pretraining from pixels. In Proceedings
    of the 37th International Conference on Machine Learning, ICML, 13-18 July, Virtual
    Event (pp. 1691--1703). PMLR volume 119 of Proceedings of Machine Learning Research.
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020b）Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever,
    I. (2020b). 从像素生成预训练。在第37届国际机器学习会议论文集，ICML，7月13-18日，虚拟活动（第1691--1703页）。PMLR第119卷，机器学习研究论文集。
- en: 'Chen et al. (2022a) Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z.,
    Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y.,
    Qian, Y., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022a). Wavlm: Large-scale self-supervised
    pre-training for full stack speech processing. IEEE J. Sel. Top. Signal Process.,
    16, 1505--1518.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等（2022a）Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J.,
    Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian,
    Y., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022a). Wavlm: 大规模自监督预训练用于全栈语音处理。IEEE选择性信号处理杂志，16,
    1505--1518。'
- en: 'Chen et al. (2022b) Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S.,
    Wu, J., Qian, Y., Wei, F., Li, J., & Yu, X. (2022b). Unispeech-sat: Universal
    speech representation learning with speaker aware pre-training. In IEEE International
    Conference on Acoustics, Speech and Signal Processing, ICASSP, Virtual and Singapore,
    23-27 May (pp. 6152--6156). IEEE.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2022b）Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J.,
    Qian, Y., Wei, F., Li, J., & Yu, X. (2022b). Unispeech-sat：带有说话者感知预训练的通用语音表示学习。在IEEE国际声学、语音和信号处理会议，ICASSP，虚拟和新加坡，5月23-27日（第6152--6156页）。IEEE。
- en: Chen et al. (2022c) Chen, X., Liu, Y., Yang, B., Zhu, J., Yuan, S., Xie, X.,
    Liu, Y., Dai, J., & Men, K. (2022c). A more effective ct synthesizer using transformers
    for cone-beam ct-guided adaptive radiotherapy. Frontiers in Oncology, 12.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen, X., Liu, Y., Yang, B., Zhu, J., Yuan, S., Xie, X., Liu, Y., Dai, J., &
    Men, K. (2022c). 使用变换器的更有效的CT合成器用于锥束CT引导的自适应放疗. 《前沿肿瘤学》，12。
- en: 'Chen et al. (2020c) Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan,
    Z., Cheng, Y., & Liu, J. (2020c). UNITER: universal image-text representation
    learning. In Computer Vision - ECCV - 16th European Conference, Glasgow, UK, August
    23-28 (pp. 104--120). Springer volume 12375 of Lecture Notes in Computer Science.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., & Liu,
    J. (2020c). UNITER: 通用图像-文本表示学习. 在计算机视觉 - ECCV - 第16届欧洲会议, 格拉斯哥, 英国, 8月23-28日
    (第104--120页). Springer，第12375卷，《计算机科学讲义》。'
- en: Chowdhary & Chowdhary (2020) Chowdhary, K., & Chowdhary, K. (2020). Natural
    language processing. Fundamentals of artificial intelligence, (pp. 603--649).
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhary, K., & Chowdhary, K. (2020). 自然语言处理. 人工智能基础, (第603--649页)。
- en: 'Clark et al. (2020a) Clark, K., Luong, M., Le, Q. V., & Manning, C. D. (2020a).
    ELECTRA: pre-training text encoders as discriminators rather than generators.
    In 8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30. OpenReview.net.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark, K., Luong, M., Le, Q. V., & Manning, C. D. (2020a). ELECTRA: 预训练文本编码器作为鉴别器而非生成器.
    在第8届国际学习表征会议, ICLR 2020, 亚的斯亚贝巴, 埃塞俄比亚, 4月26-30日. OpenReview.net。'
- en: Clark et al. (2020b) Clark, P., Tafjord, O., & Richardson, K. (2020b). Transformers
    as soft reasoners over language. In C. Bessiere (Ed.), Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence, IJCAI (pp. 3882--3890).
    ijcai.org.
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark, P., Tafjord, O., & Richardson, K. (2020b). 变换器作为语言上的软推理器. 在 C. Bessiere
    (编辑), 第二十九届国际人工智能联合会议论文集, IJCAI (第3882--3890页). ijcai.org。
- en: 'Clerckx et al. (2021) Clerckx, B., Huang, K., Varshney, L. R., Ulukus, S.,
    & Alouini, M. (2021). Wireless power transfer for future networks: Signal processing,
    machine learning, computing, and sensing. IEEE J. Sel. Top. Signal Process., 15,
    1060--1094.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clerckx, B., Huang, K., Varshney, L. R., Ulukus, S., & Alouini, M. (2021).
    面向未来网络的无线电力传输: 信号处理、机器学习、计算与传感. IEEE 选择性信号处理期刊, 15, 1060--1094。'
- en: Conneau et al. (2021) Conneau, A., Baevski, A., Collobert, R., Mohamed, A.,
    & Auli, M. (2021). Unsupervised cross-lingual representation learning for speech
    recognition. In H. Hermansky, H. Cernocký, L. Burget, L. Lamel, O. Scharenborg,
    & P. Motlícek (Eds.), Interspeech, 22nd Annual Conference of the International
    Speech Communication Association, Brno, Czechia, 30 August - 3 September (pp.
    2426--2430). ISCA.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2021). 无监督跨语言表示学习用于语音识别.
    在 H. Hermansky, H. Cernocký, L. Burget, L. Lamel, O. Scharenborg, & P. Motlícek
    (编辑), Interspeech, 第22届国际语音通信协会年会, 布尔诺, 捷克, 8月30日 - 9月3日 (第2426--2430页). ISCA。
- en: 'Conneau & Lample (2019) Conneau, A., & Lample, G. (2019). Cross-lingual language
    model pretraining. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc,
    E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, Vancouver, BC, Canada (pp. 7057--7067).'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conneau, A., & Lample, G. (2019). 跨语言语言模型预训练. 在 H. M. Wallach, H. Larochelle,
    A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, & R. Garnett (编辑), 神经信息处理系统32: 2019年神经信息处理系统年会,
    NeurIPS 2019, 12月8-14日, 温哥华, BC, 加拿大 (第7057--7067页)。'
- en: 'd’Ascoli et al. (2021) d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,
    Biroli, G., & Sagun, L. (2021). Convit: Improving vision transformers with soft
    convolutional inductive biases. In M. Meila, & T. Zhang (Eds.), Proceedings of
    the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual
    Event (pp. 2286--2296). PMLR volume 139 of Proceedings of Machine Learning Research.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'd’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun,
    L. (2021). Convit: 通过软卷积归纳偏置改进视觉变换器. 在 M. Meila, & T. Zhang (编辑), 第38届国际机器学习会议论文集,
    ICML, 7月18-24日, 虚拟活动 (第2286--2296页). PMLR，第139卷，《机器学习研究论文集》。'
- en: 'Deng et al. (2013) Deng, L., Hinton, G. E., & Kingsbury, B. (2013). New types
    of deep neural network learning for speech recognition and related applications:
    an overview. In IEEE International Conference on Acoustics, Speech and Signal
    Processing, ICASSP, Vancouver, BC, Canada, May 26-31 (pp. 8599--8603). IEEE.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2013) Deng, L., Hinton, G. E., & Kingsbury, B. (2013). 语音识别及相关应用的新型深度神经网络学习方法：概述。载于
    IEEE 国际声学、语音与信号处理会议（ICASSP），加拿大不列颠哥伦比亚省温哥华，2013年5月26-31日（第8599--8603页）。IEEE。
- en: 'Devlin et al. (2019) Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    In J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, Volume
    1 (Long and Short Papers) (pp. 4171--4186). Association for Computational Linguistics.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019).
    BERT：用于语言理解的深度双向变换器的预训练。载于 J. Burstein, C. Doran, & T. Solorio（编），《2019年北美计算语言学协会：人类语言技术会议（NAACL-HLT
    2019）会议论文集》，美国明尼苏达州明尼阿波利斯，2019年6月2-7日，第1卷（长篇和短篇论文）（第4171--4186页）。计算语言学协会。
- en: 'Ding et al. (2021) Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,
    D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). Cogview: Mastering
    text-to-image generation via transformers. In M. Ranzato, A. Beygelzimer, Y. N.
    Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing
    Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS,
    December 6-14, virtual (pp. 19822--19835).'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2021) Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,
    D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). Cogview: 通过变换器掌握文本到图像生成。载于
    M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan（编），《神经信息处理系统进展
    34：神经信息处理系统年会（NeurIPS）》，2021年12月6-14日，虚拟会议（第19822--19835页）。'
- en: 'Dong et al. (2018) Dong, L., Xu, S., & Xu, B. (2018). Speech-transformer: A
    no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE
    International Conference on Acoustics, Speech and Signal Processing, ICASSP, Calgary,
    AB, Canada, April 15-20 (pp. 5884--5888). IEEE.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2018) Dong, L., Xu, S., & Xu, B. (2018). Speech-transformer: 一种无递归的序列到序列模型用于语音识别。载于
    2018 IEEE 国际声学、语音与信号处理会议（ICASSP），加拿大阿尔伯塔省卡尔加里，2018年4月15-20日（第5884--5888页）。IEEE。'
- en: 'Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., & Houlsby, N. (2021). An image is worth 16x16 words: Transformers
    for image recognition at scale. In 9th International Conference on Learning Representations,
    ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., & Houlsby, N. (2021). 一张图片胜过16x16个词：用于大规模图像识别的变换器。载于第9届国际学习表征会议（ICLR），奥地利虚拟会议，2021年5月3-7日。OpenReview.net。
- en: 'Fedus et al. (2021) Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. CoRR,
    abs/2101.03961. URL: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961).
    [arXiv:2101.03961](http://arxiv.org/abs/2101.03961).'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fedus et al. (2021) Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. CoRR,
    abs/2101.03961. URL: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961).
    [arXiv:2101.03961](http://arxiv.org/abs/2101.03961).'
- en: 'Fournier et al. (2021) Fournier, Q., Caron, G. M., & Aloise, D. (2021). A practical
    survey on faster and lighter transformers. CoRR, abs/2103.14636. URL: [https://arxiv.org/abs/2103.14636](https://arxiv.org/abs/2103.14636).
    [arXiv:2103.14636](http://arxiv.org/abs/2103.14636).'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fournier et al. (2021) Fournier, Q., Caron, G. M., & Aloise, D. (2021). 关于更快和更轻量化的变换器的实用调查。CoRR,
    abs/2103.14636. URL: [https://arxiv.org/abs/2103.14636](https://arxiv.org/abs/2103.14636).
    [arXiv:2103.14636](http://arxiv.org/abs/2103.14636).'
- en: 'Fu et al. (2021) Fu, T., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., &
    Liu, Z. (2021). VIOLET : End-to-end video-language transformers with masked visual-token
    modeling. CoRR, abs/2111.12681. URL: [https://arxiv.org/abs/2111.12681](https://arxiv.org/abs/2111.12681).
    [arXiv:2111.12681](http://arxiv.org/abs/2111.12681).'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2021) Fu, T., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., &
    Liu, Z. (2021). VIOLET：端到端的视频语言变换器，采用掩蔽视觉标记建模。CoRR, abs/2111.12681. URL: [https://arxiv.org/abs/2111.12681](https://arxiv.org/abs/2111.12681).
    [arXiv:2111.12681](http://arxiv.org/abs/2111.12681).'
- en: 'Galanos et al. (2021) Galanos, T., Liapis, A., & Yannakakis, G. N. (2021).
    Affectgan: Affect-based generative art driven by semantics. In 9th International
    Conference on Affective Computing and Intelligent Interaction, ACII - Workshops
    and Demos, Nara, Japan, September 28 - Oct. 1 (pp. 1--7). IEEE.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Galanos 等 (2021) Galanos, T., Liapis, A., & Yannakakis, G. N. (2021). Affectgan:
    基于情感的生成艺术驱动的语义。见第 9 届国际情感计算与智能互动大会，ACII - 工作坊和演示, 奈良, 日本, 9 月 28 日 - 10 月 1 日
    (第 1--7 页)。IEEE.'
- en: 'Giles et al. (1995) Giles, C. L., Chen, D., Sun, G., Chen, H., Lee, Y., & Goudreau,
    M. W. (1995). Constructive learning of recurrent neural networks: limitations
    of recurrent cascade correlation and a simple solution. IEEE Trans. Neural Networks,
    6, 829--836.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Giles 等 (1995) Giles, C. L., Chen, D., Sun, G., Chen, H., Lee, Y., & Goudreau,
    M. W. (1995). 递归神经网络的建设性学习: 递归级联相关的局限性和一个简单的解决方案。IEEE Trans. Neural Networks,
    6, 829--836.'
- en: 'Gong et al. (2021) Gong, Y., Chung, Y., & Glass, J. R. (2021). AST: audio spectrogram
    transformer. In H. Hermansky, H. Cernocký, L. Burget, L. Lamel, O. Scharenborg,
    & P. Motlícek (Eds.), Interspeech, 22nd Annual Conference of the International
    Speech Communication Association, Brno, Czechia, 30 August - 3 September (pp.
    571--575). ISCA.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gong 等 (2021) Gong, Y., Chung, Y., & Glass, J. R. (2021). AST: 音频频谱变换器。见 H.
    Hermansky, H. Cernocký, L. Burget, L. Lamel, O. Scharenborg, & P. Motlícek (编辑),
    Interspeech, 国际语音通信协会第 22 届年会，布尔诺, Czechia, 8 月 30 日 - 9 月 3 日 (第 571--575 页)。ISCA.'
- en: Graves & Schmidhuber (2005) Graves, A., & Schmidhuber, J. (2005). Framewise
    phoneme classification with bidirectional LSTM and other neural network architectures.
    Neural Networks, 18, 602--610.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 和 Schmidhuber (2005) Graves, A., & Schmidhuber, J. (2005). 使用双向 LSTM
    和其他神经网络架构的帧级音素分类。Neural Networks, 18, 602--610.
- en: 'Gruetzemacher & Paradice (2022) Gruetzemacher, R., & Paradice, D. B. (2022).
    Deep transfer learning & beyond: Transformer language models in information systems
    research. ACM Comput. Surv., 54, 204:1--204:35.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gruetzemacher 和 Paradice (2022) Gruetzemacher, R., & Paradice, D. B. (2022).
    深度迁移学习及其他: 信息系统研究中的变换器语言模型。ACM Comput. Surv., 54, 204:1--204:35.'
- en: 'Gu et al. (2022) Gu, H., Wang, H., Qin, P., & Wang, J. (2022). Chest l-transformer:
    local features with position attention for weakly supervised chest radiograph
    segmentation and classification. Frontiers in Medicine, (p. 1619).'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等 (2022) Gu, H., Wang, H., Qin, P., & Wang, J. (2022). 胸部 l-transformer:
    具有位置注意力的局部特征用于弱监督胸部 X 射线图像分割和分类。Frontiers in Medicine, (第 1619 页).'
- en: Gu et al. (2017) Gu, Y., Li, X., Chen, S., Zhang, J., & Marsic, I. (2017). Speech
    intention classification with multimodal deep learning. In M. Mouhoub, & P. Langlais
    (Eds.), Advances in Artificial Intelligence - 30th Canadian Conference on Artificial
    Intelligence, Canadian AI, Edmonton, AB, Canada, May 16-19, Proceedings (pp. 260--271).
    volume 10233 of Lecture Notes in Computer Science.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 (2017) Gu, Y., Li, X., Chen, S., Zhang, J., & Marsic, I. (2017). 多模态深度学习的语音意图分类。见
    M. Mouhoub, & P. Langlais (编辑), 人工智能进展 - 第 30 届加拿大人工智能大会, Canadian AI, 埃德蒙顿, AB,
    加拿大, 5 月 16-19 日, 论文集 (第 260--271 页)。计算机科学讲义笔记第 10233 卷.
- en: 'Gulati et al. (2020) Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y.,
    Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer: Convolution-augmented
    transformer for speech recognition. In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech,
    21st Annual Conference of the International Speech Communication Association,
    Virtual Event, Shanghai, China, 25-29 October (pp. 5036--5040). ISCA.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gulati 等 (2020) Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J.,
    Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer: 卷积增强变换器用于语音识别。见
    H. Meng, B. Xu, & T. F. Zheng (编辑), Interspeech, 国际语音通信协会第 21 届年会, 虚拟活动, 上海, 中国,
    10 月 25-29 (第 5036--5040 页)。ISCA.'
- en: 'Guo et al. (2022a) Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y.,
    & Xu, C. (2022a). CMT: convolutional neural networks meet vision transformers.
    In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, New Orleans,
    LA, USA, June 18-24 (pp. 12165--12175). IEEE.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 (2022a) Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu,
    C. (2022a). CMT: 卷积神经网络遇上视觉变换器。见 IEEE/CVF 计算机视觉与模式识别大会，CVPR, 新奥尔良, LA, USA, 6
    月 18-24 (第 12165--12175 页)。IEEE.'
- en: Guo et al. (2022b) Guo, J., Xiao, N., Li, H., He, L., Li, Q., Wu, T., He, X.,
    Chen, P., Chen, D., Xiang, J. et al. (2022b). Transformer-based high-frequency
    oscillation signal detection on magnetoencephalography from epileptic patients.
    Frontiers in Molecular Biosciences, 9.
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2022b) Guo, J., Xiao, N., Li, H., He, L., Li, Q., Wu, T., He, X., Chen,
    P., Chen, D., Xiang, J. 等 (2022b). 基于变换器的高频振荡信号检测在癫痫患者的磁脑电图中。Frontiers in Molecular
    Biosciences, 9.
- en: Hahn et al. (2021) Hahn, C., Schmitt, F., Kreber, J. U., Rabe, M. N., & Finkbeiner,
    B. (2021). Teaching temporal logics to neural networks. In 9th International Conference
    on Learning Representations, ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahn 等 (2021) Hahn, C., Schmitt, F., Kreber, J. U., Rabe, M. N., & Finkbeiner,
    B. (2021). 教授神经网络时间逻辑。在第9届国际学习表征会议，ICLR，虚拟活动，奥地利，5月3-7日。OpenReview.net。
- en: 'Hajiakhondi-Meybodi et al. (2022) Hajiakhondi-Meybodi, Z., Mohammadi, A., Rahimian,
    E., Heidarian, S., Abouei, J., & Plataniotis, K. N. (2022). Tedge-caching: Transformer-based
    edge caching towards 6g networks. In IEEE International Conference on Communications,
    ICC Seoul, Korea, May 16-20 (pp. 613--618). IEEE.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hajiakhondi-Meybodi 等 (2022) Hajiakhondi-Meybodi, Z., Mohammadi, A., Rahimian,
    E., Heidarian, S., Abouei, J., & Plataniotis, K. N. (2022). Tedge-caching: 面向
    6G 网络的基于 Transformer 的边缘缓存。在 IEEE 国际通信会议，ICC 首尔，韩国，5月16-20日（第613--618页）。IEEE。'
- en: 'Hamidi-Rad & Jain (2021) Hamidi-Rad, S., & Jain, S. (2021). Mcformer: A transformer
    based deep neural network for automatic modulation classification. In IEEE Global
    Communications Conference, GLOBECOM, Madrid, Spain, December 7-11 (pp. 1--6).
    IEEE.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hamidi-Rad 和 Jain (2021) Hamidi-Rad, S., & Jain, S. (2021). Mcformer: 基于 Transformer
    的深度神经网络用于自动调制分类。在 IEEE 全球通信会议，GLOBECOM，西班牙马德里，12月7-11日（第1--6页）。IEEE。'
- en: Han et al. (2023) Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang,
    Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y., & Tao, D. (2023). A survey
    on vision transformer. IEEE Trans. Pattern Anal. Mach. Intell., 45, 87--110.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2023) Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang,
    Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y., & Tao, D. (2023). 视觉 Transformer
    调查。IEEE Trans. Pattern Anal. Mach. Intell., 45, 87--110。
- en: 'Han et al. (2021) Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., & Wang, Y. (2021).
    Transformer in transformer. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang,
    & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34:
    Annual Conference on Neural Information Processing Systems, NeurIPS, December
    6-14, virtual (pp. 15908--15919).'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2021) Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., & Wang, Y. (2021).
    Transformer in transformer。在 M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang,
    & J. W. Vaughan (编), 《神经信息处理系统进展》第34卷：神经信息处理系统年会，NeurIPS，12月6-14日，虚拟会议（第15908--15919页）。
- en: Haralick & Shapiro (1985) Haralick, R. M., & Shapiro, L. G. (1985). Image segmentation
    techniques. Comput. Vis. Graph. Image Process., 29, 100--132.
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haralick 和 Shapiro (1985) Haralick, R. M., & Shapiro, L. G. (1985). 图像分割技术。Comput.
    Vis. Graph. Image Process., 29, 100--132。
- en: 'He et al. (2008) He, H., Bai, Y., Garcia, E. A., & Li, S. (2008). ADASYN: adaptive
    synthetic sampling approach for imbalanced learning. In Proceedings of the International
    Joint Conference on Neural Networks, IJCNN 2008, part of the IEEE World Congress
    on Computational Intelligence, WCCI, Hong Kong, China, June 1-6 (pp. 1322--1328).
    IEEE.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 (2008) He, H., Bai, Y., Garcia, E. A., & Li, S. (2008). ADASYN: 用于不平衡学习的自适应合成采样方法。在国际联合神经网络会议，IJCNN
    2008，IEEE 计算智能世界大会，WCCI，香港，中国，6月1-6日（第1322--1328页）。IEEE。'
- en: He et al. (2022) He, X., Tan, E., Bi, H., Zhang, X., Zhao, S., & Lei, B. (2022).
    Fully transformer network for skin lesion analysis. Medical Image Anal., 77, 102357.
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2022) He, X., Tan, E., Bi, H., Zhang, X., Zhao, S., & Lei, B. (2022).
    用于皮肤病变分析的全 Transformer 网络。Medical Image Anal., 77, 102357。
- en: Hénaff (2020) Hénaff, O. J. (2020). Data-efficient image recognition with contrastive
    predictive coding. In Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event (pp. 4182--4192). PMLR volume
    119 of Proceedings of Machine Learning Research.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hénaff (2020) Hénaff, O. J. (2020). 使用对比预测编码进行数据高效的图像识别。在第37届国际机器学习会议，ICML 2020，2020年7月13-18日，虚拟活动（第4182--4192页）。PMLR
    119卷，机器学习研究会议录。
- en: Hirschberg & Manning (2015) Hirschberg, J., & Manning, C. D. (2015). Advances
    in natural language processing. Science, 349, 261--266.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirschberg 和 Manning (2015) Hirschberg, J., & Manning, C. D. (2015). 自然语言处理的进展。Science,
    349, 261--266。
- en: 'Hirschman & Gaizauskas (2001) Hirschman, L., & Gaizauskas, R. J. (2001). Natural
    language question answering: the view from here. Nat. Lang. Eng., 7, 275--300.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirschman 和 Gaizauskas (2001) Hirschman, L., & Gaizauskas, R. J. (2001). 自然语言问答：我们的视角。Nat.
    Lang. Eng., 7, 275--300。
- en: Hochreiter & Schmidhuber (1997) Hochreiter, S., & Schmidhuber, J. (1997). Long
    short-term memory. Neural Comput., 9, 1735--1780.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Hochreiter, S., & Schmidhuber, J. (1997). 长短期记忆。Neural
    Comput., 9, 1735--1780。
- en: Hossain et al. (2019) Hossain, M. Z., Sohel, F., Shiratuddin, M. F., & Laga,
    H. (2019). A comprehensive survey of deep learning for image captioning. ACM Comput.
    Surv., 51, 118:1--118:36.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hossain 等 (2019) Hossain, M. Z., Sohel, F., Shiratuddin, M. F., & Laga, H. (2019).
    深度学习在图像描述中的综合调查。ACM Comput. Surv., 51, 118:1--118:36。
- en: 'Hsu et al. (2021) Hsu, W., Bolte, B., Tsai, Y. H., Lakhotia, K., Salakhutdinov,
    R., & Mohamed, A. (2021). Hubert: Self-supervised speech representation learning
    by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process.,
    29, 3451--3460.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsu 等 (2021) Hsu, W., Bolte, B., Tsai, Y. H., Lakhotia, K., Salakhutdinov,
    R., & Mohamed, A. (2021). Hubert: 通过掩码预测隐藏单元的自监督语音表示学习。IEEE ACM 音频语音语言处理汇刊, 29,
    3451-3460。'
- en: Hu et al. (2016) Hu, R., Rohrbach, M., & Darrell, T. (2016). Segmentation from
    natural language expressions. In B. Leibe, J. Matas, N. Sebe, & M. Welling (Eds.),
    Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,
    October 11-14, 2016, Proceedings, Part I (pp. 108--124). Springer volume 9905
    of Lecture Notes in Computer Science.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2016) Hu, R., Rohrbach, M., & Darrell, T. (2016). 从自然语言表达中进行分割。载于 B. Leibe,
    J. Matas, N. Sebe, & M. Welling (编), 计算机视觉 - ECCV 2016 - 第14届欧洲会议, 荷兰阿姆斯特丹, 2016年10月11-14日,
    论文集第一部分 (第108-124页). Springer第9905卷，计算机科学讲义笔记。
- en: Huang et al. (2014) Huang, P., Kim, M., Hasegawa-Johnson, M., & Smaragdis, P.
    (2014). Deep learning for monaural speech separation. In IEEE International Conference
    on Acoustics, Speech and Signal Processing, ICASSP, Florence, Italy, May 4-9 (pp.
    1562--1566). IEEE.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2014) Huang, P., Kim, M., Hasegawa-Johnson, M., & Smaragdis, P. (2014).
    单声道语音分离的深度学习。载于 IEEE 国际声学、语音与信号处理会议 (ICASSP), 意大利佛罗伦萨, 2014年5月4-9日 (第1562-1566页).
    IEEE。
- en: 'Huang et al. (2020) Huang, Z., Zeng, Z., Liu, B., Fu, D., & Fu, J. (2020).
    Pixel-bert: Aligning image pixels with text by deep multi-modal transformers.
    CoRR, abs/2004.00849. URL: [https://arxiv.org/abs/2004.00849](https://arxiv.org/abs/2004.00849).
    [arXiv:2004.00849](http://arxiv.org/abs/2004.00849).'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等 (2020) Huang, Z., Zeng, Z., Liu, B., Fu, D., & Fu, J. (2020). Pixel-bert:
    通过深度多模态变换器将图像像素与文本对齐。CoRR, abs/2004.00849. 网址: [https://arxiv.org/abs/2004.00849](https://arxiv.org/abs/2004.00849).
    [arXiv:2004.00849](http://arxiv.org/abs/2004.00849)。'
- en: Islam et al. (2022) Islam, M. R., Nahiduzzaman, M., Goni, M. O. F., Sayeed,
    A., Anower, M. S., Ahsan, M., & Haider, J. (2022). Explainable transformer-based
    deep learning model for the detection of malaria parasites from blood cell images.
    Sensors, 22, 4358.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam 等 (2022) Islam, M. R., Nahiduzzaman, M., Goni, M. O. F., Sayeed, A., Anower,
    M. S., Ahsan, M., & Haider, J. (2022). 基于变换器的可解释深度学习模型用于从血细胞图像中检测疟疾寄生虫。传感器, 22,
    4358。
- en: 'Janner et al. (2021) Janner, M., Li, Q., & Levine, S. (2021). Offline reinforcement
    learning as one big sequence modeling problem. In M. Ranzato, A. Beygelzimer,
    Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems,
    NeurIPS, December 6-14, virtual (pp. 1273--1286).'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Janner 等 (2021) Janner, M., Li, Q., & Levine, S. (2021). 离线强化学习作为一个大序列建模问题。载于
    M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (编), 神经信息处理系统进展34:
    年度神经信息处理系统会议 (NeurIPS), 2021年12月6-14日, 虚拟 (第1273-1286页)。'
- en: 'Jauro et al. (2020) Jauro, F., Chiroma, H., Gital, A. Y., Almutairi, M., Abdulhamid,
    S. M., & Abawajy, J. H. (2020). Deep learning architectures in emerging cloud
    computing architectures: Recent development, challenges and next research trend.
    Appl. Soft Comput., 96, 106582.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jauro 等 (2020) Jauro, F., Chiroma, H., Gital, A. Y., Almutairi, M., Abdulhamid,
    S. M., & Abawajy, J. H. (2020). 新兴云计算架构中的深度学习架构: 最近发展、挑战与下一步研究趋势。应用软计算, 96, 106582。'
- en: Jia et al. (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H.,
    Le, Q. V., Sung, Y., Li, Z., & Duerig, T. (2021). Scaling up visual and vision-language
    representation learning with noisy text supervision. In M. Meila, & T. Zhang (Eds.),
    Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24
    July, Virtual Event (pp. 4904--4916). PMLR volume 139 of Proceedings of Machine
    Learning Research.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le,
    Q. V., Sung, Y., Li, Z., & Duerig, T. (2021). 在噪声文本监督下扩展视觉和视觉语言表示学习。载于 M. Meila,
    & T. Zhang (编), 第38届国际机器学习会议(ICML)论文集, 2021年7月18-24日, 虚拟会议 (第4904-4916页). PMLR第139卷，机器学习研究论文集。
- en: 'Jiang et al. (2022a) Jiang, Y., Liang, J., Cheng, T., Lin, X., Zhang, Y., &
    Dong, J. (2022a). Mtpa_unet: Multi-scale transformer-position attention retinal
    vessel segmentation network joint transformer and CNN. Sensors, 22, 4592.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 (2022a) Jiang, Y., Liang, J., Cheng, T., Lin, X., Zhang, Y., & Dong,
    J. (2022a). Mtpa_unet: 多尺度变换器-位置注意力视网膜血管分割网络联合变换器和CNN。传感器, 22, 4592。'
- en: 'Jiang et al. (2022b) Jiang, Y., Zhang, Y., Lin, X., Dong, J., Cheng, T., &
    Liang, J. (2022b). Swinbts: A method for 3d multimodal brain tumor segmentation
    using swin transformer. Brain Sciences, 12, 797.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 (2022b) Jiang, Y., Zhang, Y., Lin, X., Dong, J., Cheng, T., & Liang,
    J. (2022b). Swinbts: 一种使用Swin变换器进行3D多模态脑肿瘤分割的方法。脑科学, 12, 797。'
- en: Jiao et al. (2019) Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z.,
    & Qu, R. (2019). A survey of deep learning-based object detection. IEEE Access,
    7, 128837--128868.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao等（2019）Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z., & Qu, R.（2019）。基于深度学习的目标检测调查。IEEE
    Access，7，128837--128868。
- en: Jin & Chen (2021) Jin, C., & Chen, X. (2021). An end-to-end framework combining
    time-frequency expert knowledge and modified transformer networks for vibration
    signal classification. Expert Syst. Appl., 171, 114570.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin & Chen（2021）Jin, C., & Chen, X.（2021）。结合时间-频率专家知识和修改的变换器网络的端到端框架用于振动信号分类。专家系统应用，171，114570。
- en: Jungiewicz et al. (2023) Jungiewicz, M., Jastrzébski, P., Wawryka, P., Przystalski,
    K., Sabatowski, K., & Bartuś, S. (2023). Vision transformer in stenosis detection
    of coronary arteries. Expert Syst. Appl., 228, 120234.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jungiewicz等（2023）Jungiewicz, M., Jastrzébski, P., Wawryka, P., Przystalski,
    K., Sabatowski, K., & Bartuś, S.（2023）。在冠状动脉狭窄检测中的视觉变换器。专家系统应用，228，120234。
- en: 'Kaliyar (2020) Kaliyar, R. K. (2020). A multi-layer bidirectional transformer
    encoder for pre-trained word embedding: A survey of bert. 2020 10th International
    Conference on Cloud Computing, Data Science & Engineering, .'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliyar（2020）Kaliyar, R. K.（2020）。用于预训练词嵌入的多层双向变换器编码器：BERT的调查。2020年第10届云计算、数据科学与工程国际会议。
- en: 'Keskar et al. (2019) Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C.,
    & Socher, R. (2019). CTRL: A conditional transformer language model for controllable
    generation. CoRR, abs/1909.05858. URL: [http://arxiv.org/abs/1909.05858](http://arxiv.org/abs/1909.05858).
    [arXiv:1909.05858](http://arxiv.org/abs/1909.05858).'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar等（2019）Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., & Socher,
    R.（2019）。CTRL：一种用于可控生成的条件变换器语言模型。CoRR，abs/1909.05858。网址：[http://arxiv.org/abs/1909.05858](http://arxiv.org/abs/1909.05858)。
    [arXiv:1909.05858](http://arxiv.org/abs/1909.05858)。
- en: 'Khan & Lee (2023) Khan, A., & Lee, B. (2023). DeepGene transformer: Transformer
    for the gene expression-based classification of cancer subtypes. Expert Syst.
    Appl., 226, 120047.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan & Lee（2023）Khan, A., & Lee, B.（2023）。DeepGene变换器：用于基于基因表达的癌症亚型分类的变换器。专家系统应用，226，120047。
- en: 'Khan et al. (2022) Khan, S. H., Naseer, M., Hayat, M., Zamir, S. W., Khan,
    F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM Comput. Surv.,
    54, 200:1--200:41.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan等（2022）Khan, S. H., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., &
    Shah, M.（2022）。视觉中的变换器：一项调查。ACM计算机调查，54，200:1--200:41。
- en: 'Kim et al. (2021a) Kim, B., Lee, J., Kang, J., Kim, E., & Kim, H. J. (2021a).
    HOTR: end-to-end human-object interaction detection with transformers. In IEEE
    Conference on Computer Vision and Pattern Recognition, CVPR, virtual, June 19-25
    (pp. 74--83). Computer Vision Foundation / IEEE.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2021a）Kim, B., Lee, J., Kang, J., Kim, E., & Kim, H. J.（2021a）。HOTR：基于变换器的端到端人机交互检测。在IEEE计算机视觉与模式识别会议，CVPR，虚拟，6月19-25日（第74-83页）。计算机视觉基金会/IEEE。
- en: 'Kim et al. (2021b) Kim, W., Son, B., & Kim, I. (2021b). Vilt: Vision-and-language
    transformer without convolution or region supervision. In M. Meila, & T. Zhang
    (Eds.), Proceedings of the 38th International Conference on Machine Learning,
    ICML, 18-24 July, Virtual Event (pp. 5583--5594). PMLR volume 139 of Proceedings
    of Machine Learning Research.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2021b）Kim, W., Son, B., & Kim, I.（2021b）。Vilt：无卷积或区域监督的视觉-语言转换器。在M. Meila
    & T. Zhang（编），第38届国际机器学习大会会议论文集，ICML，7月18-24日，虚拟会议（第5583-5594页）。PMLR，第139卷，机器学习研究会议论文集。
- en: Kuhn (2014) Kuhn, T. (2014). A survey and classification of controlled natural
    languages. Comput. Linguistics, 40, 121--170.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn（2014）Kuhn, T.（2014）。受控自然语言的调查与分类。计算语言学，40，121--170。
- en: 'Kurin et al. (2020) Kurin, V., Godil, S., Whiteson, S., & Catanzaro, B. (2020).
    Can q-learning with graph networks learn a generalizable branching heuristic for
    a SAT solver? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.),
    Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems, NeurIPS, December 6-12, virtual.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurin等（2020）Kurin, V., Godil, S., Whiteson, S., & Catanzaro, B.（2020）。图网络中的Q学习能否学习出对SAT求解器具有广泛适用性的分支启发式？在H.
    Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin（编），神经信息处理系统第33届年会：NeurIPS，12月6-12日，虚拟。
- en: Lajkó et al. (2022) Lajkó, M., Csuvik, V., & Vidács, L. (2022). Towards javascript
    program repair with generative pre-trained transformer (GPT-2). In 3rd IEEE/ACM
    International Workshop on Automated Program Repair, APR@ICSE, Pittsburgh, PA,
    USA, May 19 (pp. 61--68). IEEE.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lajkó等（2022）Lajkó, M., Csuvik, V., & Vidács, L.（2022）。基于生成预训练变换器（GPT-2）的JavaScript程序修复。第3届IEEE/ACM自动化程序修复国际研讨会，APR@ICSE，美国宾夕法尼亚州
    Pittsburgh，5月19日（第61-68页）。IEEE。
- en: 'Le et al. (2020) Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux,
    B., Allauzen, A., Crabbé, B., Besacier, L., & Schwab, D. (2020). Flaubert: Unsupervised
    language model pre-training for french. In Proceedings of The 12th Language Resources
    and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020 (pp.
    2479--2490). European Language Resources Association.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le et al. (2020) Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux,
    B., Allauzen, A., Crabbé, B., Besacier, L., & Schwab, D. (2020). Flaubert：法语的无监督语言模型预训练。在第12届语言资源与评估会议，LREC
    2020, Marseille, France, 2020年5月11-16日（第2479-2490页）。欧洲语言资源协会。
- en: Leng et al. (2022) Leng, B., Leng, M., Ge, M., & Dong, W. (2022). Knowledge
    distillation-based deep learning classification network for peripheral blood leukocytes.
    Biomed. Signal Process. Control., 75, 103590.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al. (2022) Leng, B., Leng, M., Ge, M., & Dong, W. (2022). 基于知识蒸馏的深度学习分类网络用于外周血白细胞。生物医学信号处理与控制，75,
    103590。
- en: 'Lewis et al. (2020) Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed,
    A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2020). BART: denoising sequence-to-sequence
    pre-training for natural language generation, translation, and comprehension.
    In D. Jurafsky, J. Chai, N. Schluter, & J. R. Tetreault (Eds.), Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020 (pp. 7871--7880). Association for Computational
    Linguistics.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed,
    A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2020). BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练。在D.
    Jurafsky, J. Chai, N. Schluter, & J. R. Tetreault（编），第58届计算语言学协会年会会议录，ACL 2020,
    在线, 2020年7月5-10日（第7871-7880页）。计算语言学协会。
- en: 'Li et al. (2020a) Li, G., Duan, N., Fang, Y., Gong, M., & Jiang, D. (2020a).
    Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training.
    In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, New York,
    NY, USA, February 7-12 (pp. 11336--11344). AAAI Press.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020a) Li, G., Duan, N., Fang, Y., Gong, M., & Jiang, D. (2020a).
    Unicoder-vl：通过跨模态预训练的视觉和语言通用编码器。在第34届AAAI人工智能大会，AAAI, New York, NY, USA, 2月7-12日（第11336-11344页）。AAAI出版社。
- en: Li et al. (2023) Li, J., Chen, J., Tang, Y., Wang, C., Landman, B. A., & Zhou,
    S. K. (2023). Transforming medical imaging with transformers? a comparative review
    of key properties, current progresses, and future perspectives. Medical image
    analysis, (p. 102762).
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Li, J., Chen, J., Tang, Y., Wang, C., Landman, B. A., & Zhou,
    S. K. (2023). 用变换器改变医学成像？关键属性、当前进展和未来展望的比较评审。医学图像分析，（第102762页）。
- en: 'Li et al. (2022) Li, J., Li, D., Xiong, C., & Hoi, S. C. H. (2022). BLIP: bootstrapping
    language-image pre-training for unified vision-language understanding and generation.
    In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, & S. Sabato (Eds.),
    International Conference on Machine Learning, ICML, 17-23 July, Baltimore, Maryland,
    USA (pp. 12888--12900). PMLR volume 162 of Proceedings of Machine Learning Research.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Li, J., Li, D., Xiong, C., & Hoi, S. C. H. (2022). BLIP：用于统一视觉-语言理解和生成的语言-图像预训练。K.
    Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, & S. Sabato（编），国际机器学习会议，ICML,
    7月17-23日, Baltimore, Maryland, USA（第12888-12900页）。PMLR第162卷，机器学习研究会议录。
- en: Li et al. (2020b) Li, P., Fu, T., & Ma, W. (2020b). Why attention? analyze bilstm
    deficiency and its remedies in the case of NER. In The Thirty-Fourth AAAI Conference
    on Artificial Intelligence, New York, NY, USA, February 7-12 (pp. 8236--8244).
    AAAI Press.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020b) Li, P., Fu, T., & Ma, W. (2020b). 为什么关注？分析bilstm的不足及其在NER中的补救措施。在第34届AAAI人工智能大会,
    New York, NY, USA, 2月7-12日（第8236-8244页）。AAAI出版社。
- en: Li et al. (2017) Li, P., Li, J., Huang, Z., Li, T., Gao, C., Yiu, S., & Chen,
    K. (2017). Multi-key privacy-preserving deep learning in cloud computing. Future
    Gener. Comput. Syst., 74, 76--85.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017) Li, P., Li, J., Huang, Z., Li, T., Gao, C., Yiu, S., & Chen,
    K. (2017). 云计算中的多密钥隐私保护深度学习。未来一代计算系统，74, 76-85。
- en: 'Li & Hoefler (2021) Li, S., & Hoefler, T. (2021). Chimera: efficiently training
    large-scale neural networks with bidirectional pipelines. In B. R. de Supinski,
    M. W. Hall, & T. Gamblin (Eds.), International Conference for High Performance
    Computing, Networking, Storage and Analysis, SC, St. Louis, Missouri, USA, November
    14-19 (p. 27). ACM.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Hoefler (2021) Li, S., & Hoefler, T. (2021). Chimera：利用双向管道高效训练大规模神经网络。B.
    R. de Supinski, M. W. Hall, & T. Gamblin（编），国际高性能计算、网络、存储和分析会议，SC, St. Louis,
    Missouri, USA, 11月14-19（第27页）。ACM。
- en: 'Liang et al. (2022) Liang, J., Yang, C., Zeng, M., & Wang, X. (2022). Transconver:
    transformer and convolution parallel network for developing automatic brain tumor
    segmentation in mri images. Quantitative Imaging in Medicine and Surgery, 12,
    2397.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2022）Liang, J., Yang, C., Zeng, M., & Wang, X.（2022）。Transconver：用于自动脑肿瘤分割的变换器和卷积并行网络。医学与外科定量影像，12，2397。
- en: Lin et al. (2022) Lin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of
    transformers. AI Open, 3, 111--132.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2022）Lin, T., Wang, Y., Liu, X., & Qiu, X.（2022）。变换器综述。AI Open，3，111--132。
- en: 'Liu et al. (2020) Liu, A. T., Yang, S., Chi, P., Hsu, P., & Lee, H. (2020).
    Mockingjay: Unsupervised speech representation learning with deep bidirectional
    transformer encoders. In IEEE International Conference on Acoustics, Speech and
    Signal Processing, ICASSP, Barcelona, Spain, May 4-8 (pp. 6419--6423). IEEE.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2020）Liu, A. T., Yang, S., Chi, P., Hsu, P., & Lee, H.（2020）。Mockingjay：使用深度双向变换器编码器的无监督语音表示学习。在IEEE国际声学、语音和信号处理会议ICASSP，西班牙巴塞罗那，2020年5月4日至8日（第6419--6423页）。IEEE。
- en: 'Liu et al. (2022a) Liu, J., Li, Z., Fan, X., Hu, X., Yan, J., Li, B., Xia,
    Q., Zhu, J., & Wu, Y. (2022a). Crt-net: A generalized and scalable framework for
    the computer-aided diagnosis of electrocardiogram signals. Appl. Soft Comput.,
    128, 109481.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022a）Liu, J., Li, Z., Fan, X., Hu, X., Yan, J., Li, B., Xia, Q., Zhu,
    J., & Wu, Y.（2022a）。Crt-net：一种用于计算机辅助心电图信号诊断的通用和可扩展框架。应用软计算，128，109481。
- en: Liu et al. (2022b) Liu, J., Wang, T., Li, Y., Li, C., Wang, Y., & Shen, Y. (2022b).
    A transformer-based signal denoising network for aoa estimation in nlos environments.
    IEEE Commun. Lett., 26, 2336--2339.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022b）Liu, J., Wang, T., Li, Y., Li, C., Wang, Y., & Shen, Y.（2022b）。一种基于变换器的信号去噪网络，用于NLOS环境中的AOA估计。IEEE通信快报，26，2336--2339。
- en: 'Liu et al. (2017) Liu, M., Breuel, T. M., & Kautz, J. (2017). Unsupervised
    image-to-image translation networks. In I. Guyon, U. von Luxburg, S. Bengio, H. M.
    Wallach, R. Fergus, S. V. N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural
    Information Processing Systems 30: Annual Conference on Neural Information Processing
    Systems 2017, December 4-9, Long Beach, CA, USA (pp. 700--708).'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2017）Liu, M., Breuel, T. M., & Kautz, J.（2017）。无监督图像到图像翻译网络。在 I. Guyon、U.
    von Luxburg、S. Bengio、H. M. Wallach、R. Fergus、S. V. N. Vishwanathan 和 R. Garnett（编辑），神经信息处理系统进展30：2017年神经信息处理系统年会，2017年12月4日至9日，美国加州长滩（第700--708页）。
- en: 'Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: A robustly
    optimized BERT pretraining approach. CoRR, abs/1907.11692. URL: [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692).
    [arXiv:1907.11692](http://arxiv.org/abs/1907.11692).'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2019）Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
    O., Lewis, M., Zettlemoyer, L., & Stoyanov, V.（2019）。RoBERTa：一种稳健优化的BERT预训练方法。CoRR,
    abs/1907.11692。网址：[http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)。[arXiv:1907.11692](http://arxiv.org/abs/1907.11692)。
- en: 'Liu et al. (2022c) Liu, Y., Qiao, L., Yin, D., Jiang, Z., Jiang, X., Jiang,
    D., & Ren, B. (2022c). OS-MSL: one stage multimodal sequential link framework
    for scene segmentation and classification. In J. Magalhães, A. D. Bimbo, S. Satoh,
    N. Sebe, X. Alameda-Pineda, Q. Jin, V. Oria, & L. Toni (Eds.), MM: The 30th ACM
    International Conference on Multimedia, Lisboa, Portugal, October 10 - 14 (pp.
    6269--6277). ACM.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022c）Liu, Y., Qiao, L., Yin, D., Jiang, Z., Jiang, X., Jiang, D., &
    Ren, B.（2022c）。OS-MSL：一种用于场景分割和分类的一阶段多模态序列链接框架。在 J. Magalhães、A. D. Bimbo、S. Satoh、N.
    Sebe、X. Alameda-Pineda、Q. Jin、V. Oria 和 L. Toni（编辑），MM：第30届ACM国际多媒体会议，葡萄牙里斯本，2024年10月10日至14日（第6269--6277页）。ACM。
- en: 'Liu et al. (2021) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using
    shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision
    ICCV, Montreal, QC, Canada, October 10-17 (pp. 9992--10002). IEEE.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021）Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S.,
    & Guo, B.（2021）。Swin变换器：使用移动窗口的分层视觉变换器。在2021年IEEE/CVF国际计算机视觉会议ICCV，加拿大蒙特利尔，2021年10月10日至17日（第9992--10002页）。IEEE。
- en: 'Livezey et al. (2019) Livezey, J. A., Bouchard, K. E., & Chang, E. F. (2019).
    Deep learning as a tool for neural data analysis: Speech classification and cross-frequency
    coupling in human sensorimotor cortex. PLoS Comput. Biol., 15.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Livezey 等人（2019）Livezey, J. A., Bouchard, K. E., & Chang, E. F.（2019）。深度学习作为神经数据分析的工具：人类感觉运动皮层中的语音分类和跨频率耦合。PLoS计算生物学，15。
- en: 'López-Linares et al. (2020) López-Linares, K., García Ocaña, M. I., Lete Urzelai,
    N., González Ballester, M. Á., & Macía Oliver, I. (2020). Medical image segmentation
    using deep learning. Deep Learning in Healthcare: Paradigms and Applications,
    (pp. 17--31).'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'López-Linares 等人 (2020) López-Linares, K., García Ocaña, M. I., Lete Urzelai,
    N., González Ballester, M. Á., & Macía Oliver, I. (2020). 使用深度学习进行医学图像分割. 《医疗保健中的深度学习:
    范式与应用》, (第17-31页).'
- en: Lu & Weng (2007) Lu, D., & Weng, Q. (2007). A survey of image classification
    methods and techniques for improving classification performance. International
    journal of Remote sensing, 28, 823--870.
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu & Weng (2007) Lu, D., & Weng, Q. (2007). 图像分类方法及提高分类性能的技术综述. 国际遥感杂志, 28,
    823-870.
- en: 'Lu et al. (2019) Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert:
    Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B.
    Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 32:
    Annual Conference on Neural Information Processing Systems, NeurIPS, December
    8-14, Vancouver, BC, Canada (pp. 13--23).'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人 (2019) Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: 预训练任务无关的视觉语言表示，用于视觉和语言任务.
    见 H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, & R.
    Garnett (编), 神经信息处理系统进展 32: 神经信息处理系统年会, NeurIPS, 12月8-14日, 加拿大不列颠哥伦比亚省温哥华 (第13-23页).'
- en: Ma et al. (2022) Ma, M., Xu, Y., Song, L., & Liu, G. (2022). Symmetric transformer-based
    network for unsupervised image registration. Knowl. Based Syst., 257, 109959.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人 (2022) Ma, M., Xu, Y., Song, L., & Liu, G. (2022). 基于对称变换器网络的无监督图像配准.
    知识基础系统, 257, 109959.
- en: Mahesh & Renjit (2020) Mahesh, K. M., & Renjit, J. A. (2020). Deepjoint segmentation
    for the classification of severity-levels of glioma tumour using multimodal MRI
    images. IET Image Process., 14, 2541--2552.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahesh & Renjit (2020) Mahesh, K. M., & Renjit, J. A. (2020). 利用多模态MRI图像进行胶质瘤严重程度分类的深度联合分割.
    IET 图像处理, 14, 2541-2552.
- en: 'Mark A Musen (1988) Mark A Musen, J. V. d. L. (1988). Of brittleness and bottlenecks:
    Challenges in the creation of pattern-recognition and expert-system models. In
    Machine Intelligence and Pattern Recognition,, 7, 335–352.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mark A Musen (1988) Mark A Musen, J. V. d. L. (1988). 脆弱性与瓶颈: 创建模式识别和专家系统模型的挑战.
    见《机器智能与模式识别》, 7, 335-352.'
- en: Menze et al. (2015) Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J.,
    Farahani, K., Kirby, J. S., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi,
    L., Gerstner, E. R., Weber, M., Arbel, T., Avants, B. B., Ayache, N., Buendia,
    P., Collins, D. L., Cordier, N., Corso, J. J., Criminisi, A., Das, T., Delingette,
    H., Demiralp, Ç., Durst, C. R., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia,
    E., Glocker, B., Golland, P., Guo, X., Hamamci, A., Iftekharuddin, K. M., Jena,
    R., John, N. M., Konukoglu, E., Lashkari, D., Mariz, J. A., Meier, R., Pereira,
    S., Precup, D., Price, S. J., Raviv, T. R., Reza, S. M. S., Ryan, M. T., Sarikaya,
    D., Schwartz, L. H., Shin, H., Shotton, J., Silva, C. A., Sousa, N. J., Subbanna,
    N. K., Székely, G., Taylor, T. J., Thomas, O. M., Tustison, N. J., Ünal, G. B.,
    Vasseur, F., Wintermark, M., Ye, D. H., Zhao, L., Zhao, B., Zikic, D., Prastawa,
    M., Reyes, M., & Leemput, K. V. (2015). The multimodal brain tumor image segmentation
    benchmark (BRATS). IEEE Trans. Medical Imaging, 34, 1993--2024.
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menze 等人 (2015) Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani,
    K., Kirby, J. S., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi, L., Gerstner,
    E. R., Weber, M., Arbel, T., Avants, B. B., Ayache, N., Buendia, P., Collins,
    D. L., Cordier, N., Corso, J. J., Criminisi, A., Das, T., Delingette, H., Demiralp,
    Ç., Durst, C. R., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker,
    B., Golland, P., Guo, X., Hamamci, A., Iftekharuddin, K. M., Jena, R., John, N.
    M., Konukoglu, E., Lashkari, D., Mariz, J. A., Meier, R., Pereira, S., Precup,
    D., Price, S. J., Raviv, T. R., Reza, S. M. S., Ryan, M. T., Sarikaya, D., Schwartz,
    L. H., Shin, H., Shotton, J., Silva, C. A., Sousa, N. J., Subbanna, N. K., Székely,
    G., Taylor, T. J., Thomas, O. M., Tustison, N. J., Ünal, G. B., Vasseur, F., Wintermark,
    M., Ye, D. H., Zhao, L., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., & Leemput,
    K. V. (2015). 多模态脑肿瘤图像分割基准 (BRATS). IEEE 医学影像学期刊, 34, 1993-2024.
- en: Mikolov et al. (2010) Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., &
    Khudanpur, S. (2010). Recurrent neural network based language model. In T. Kobayashi,
    K. Hirose, & S. Nakamura (Eds.), INTERSPEECH 2010, 11th Annual Conference of the
    International Speech Communication Association, Makuhari, Chiba, Japan, September
    26-30, 2010 (pp. 1045--1048). ISCA.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等人 (2010) Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., & Khudanpur,
    S. (2010). 基于递归神经网络的语言模型。见 T. Kobayashi, K. Hirose, & S. Nakamura (编), INTERSPEECH
    2010, 第11届国际语音通信协会年会, 幕张, 千叶, 日本, 2010年9月26-30日 (第1045-1048页). ISCA.
- en: 'Minaee et al. (2022) Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz,
    N., & Terzopoulos, D. (2022). Image segmentation using deep learning: A survey.
    IEEE Trans. Pattern Anal. Mach. Intell., 44, 3523--3542.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等（2022）Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N.,
    & Terzopoulos, D.（2022）。基于深度学习的图像分割：综述。IEEE 计算机学会《模式分析与机器智能》杂志，44，3523--3542。
- en: Monroe (2017) Monroe, D. (2017). Deep learning takes on translation. Commun.
    ACM, 60, 12--14.
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monroe（2017）Monroe, D.（2017）。深度学习在翻译领域的应用。ACM 通讯，60，12--14。
- en: Murtagh (1990) Murtagh, F. (1990). Multilayer perceptrons for classification
    and regression. Neurocomputing, 2, 183--197.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murtagh（1990）Murtagh, F.（1990）。用于分类和回归的多层感知器。神经计算，2，183--197。
- en: 'Nagrani et al. (2021) Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid,
    C., & Sun, C. (2021). Attention bottlenecks for multimodal fusion. In M. Ranzato,
    A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural
    Information Processing Systems 34: Annual Conference on Neural Information Processing
    Systems, NeurIPS, December 6-14, virtual (pp. 14200--14213).'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagrani 等（2021）Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., & Sun,
    C.（2021）。多模态融合中的注意力瓶颈。见 M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, &
    J. W. Vaughan（编辑），《神经信息处理系统进展 34：神经信息处理系统年会，NeurIPS，12月6-14日，虚拟》（第14200--14213页）。
- en: 'Nassif et al. (2019) Nassif, A. B., Shahin, I., Attili, I. B., Azzeh, M., &
    Shaalan, K. (2019). Speech recognition using deep neural networks: A systematic
    review. IEEE Access, 7, 19143--19165.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nassif 等（2019）Nassif, A. B., Shahin, I., Attili, I. B., Azzeh, M., & Shaalan,
    K.（2019）。基于深度神经网络的语音识别：系统性综述。IEEE Access, 7，19143--19165。
- en: 'Nichol et al. (2022) Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
    P., McGrew, B., Sutskever, I., & Chen, M. (2022). GLIDE: towards photorealistic
    image generation and editing with text-guided diffusion models. In K. Chaudhuri,
    S. Jegelka, L. Song, C. Szepesvári, G. Niu, & S. Sabato (Eds.), International
    Conference on Machine Learning, ICML, 17-23 July, Baltimore, Maryland, USA (pp.
    16784--16804). PMLR volume 162 of Proceedings of Machine Learning Research.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 等（2022）Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P.,
    McGrew, B., Sutskever, I., & Chen, M.（2022）。GLIDE：基于文本引导的扩散模型的逼真图像生成与编辑。见 K. Chaudhuri,
    S. Jegelka, L. Song, C. Szepesvári, G. Niu, & S. Sabato（编辑），《国际机器学习会议，ICML，7月17-23日，马里兰州巴尔的摩，美国》（第16784--16804页）。PMLR卷162，《机器学习研究会议论文集》。
- en: Niu et al. (2021) Niu, Z., Zhong, G., & Yu, H. (2021). A review on the attention
    mechanism of deep learning. Neurocomputing, 452, 48--62.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等（2021）Niu, Z., Zhong, G., & Yu, H.（2021）。深度学习的注意力机制综述。神经计算，452，48--62。
- en: 'van den Oord et al. (2016) van den Oord, A., Kalchbrenner, N., Espeholt, L.,
    Kavukcuoglu, K., Vinyals, O., & Graves, A. (2016). Conditional image generation
    with pixelcnn decoders. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, &
    R. Garnett (Eds.), Advances in Neural Information Processing Systems 29: Annual
    Conference on Neural Information Processing Systems, December 5-10, Barcelona,
    Spain (pp. 4790--4798).'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Oord 等（2016）van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu,
    K., Vinyals, O., & Graves, A.（2016）。使用 PixelCNN 解码器的条件图像生成。见 D. D. Lee, M. Sugiyama,
    U. von Luxburg, I. Guyon, & R. Garnett（编辑），《神经信息处理系统进展 29：神经信息处理系统年会，12月5-10日，西班牙巴萨罗那》（第4790--4798页）。
- en: 'O’Shea & Nash (2015) O’Shea, K., & Nash, R. (2015). An introduction to convolutional
    neural networks. CoRR, abs/1511.08458. URL: [http://arxiv.org/abs/1511.08458](http://arxiv.org/abs/1511.08458).
    [arXiv:1511.08458](http://arxiv.org/abs/1511.08458).'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Shea & Nash（2015）O’Shea, K., & Nash, R.（2015）。卷积神经网络简介。CoRR, abs/1511.08458。网址：[http://arxiv.org/abs/1511.08458](http://arxiv.org/abs/1511.08458)。[arXiv:1511.08458](http://arxiv.org/abs/1511.08458)。
- en: 'Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., & Lowe, R. (2022). Training language models to follow instructions
    with human feedback. CoRR, abs/2203.02155. URL: [https://doi.org/10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155).
    doi:[10.48550/arXiv.2203.02155](http://dx.doi.org/10.48550/arXiv.2203.02155).
    [arXiv:2203.02155](http://arxiv.org/abs/2203.02155).'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L.,
    Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,
    J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., & Lowe, R.（2022）。训练语言模型以遵循带有人类反馈的指令。CoRR, abs/2203.02155。网址：[https://doi.org/10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155)。doi：[10.48550/arXiv.2203.02155](http://dx.doi.org/10.48550/arXiv.2203.02155)。[arXiv:2203.02155](http://arxiv.org/abs/2203.02155)。
- en: 'Pang et al. (2022) Pang, Y., Lin, J., Qin, T., & Chen, Z. (2022). Image-to-image
    translation: Methods and applications. IEEE Trans. Multim., 24, 3859--3881.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等人 (2022) Pang, Y., Lin, J., Qin, T., & Chen, Z. (2022). 图像到图像转换：方法与应用。IEEE
    多模态通讯，24，3859-3881。
- en: Parisotto et al. (2020) Parisotto, E., Song, H. F., Rae, J. W., Pascanu, R.,
    Gülçehre, Ç., Jayakumar, S. M., Jaderberg, M., Kaufman, R. L., Clark, A., Noury,
    S., Botvinick, M. M., Heess, N., & Hadsell, R. (2020). Stabilizing transformers
    for reinforcement learning. In Proceedings of the 37th International Conference
    on Machine Learning, ICML, 13-18 July, Virtual Event (pp. 7487--7498). PMLR volume
    119 of Proceedings of Machine Learning Research.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 等人 (2020) Parisotto, E., Song, H. F., Rae, J. W., Pascanu, R., Gülçehre,
    Ç., Jayakumar, S. M., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., Botvinick,
    M. M., Heess, N., & Hadsell, R. (2020). 稳定化变换器用于强化学习。载于第37届国际机器学习大会（ICML）论文集，2020年7月13-18日，虚拟会议
    (第7487-7498页)。PMLR，第119卷，机器学习研究会议论文集。
- en: Parmar et al. (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
    N., Ku, A., & Tran, D. (2018). Image transformer. In J. G. Dy, & A. Krause (Eds.),
    Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholmsmässan,
    Stockholm, Sweden, July 10-15 (pp. 4052--4061). PMLR volume 80 of Proceedings
    of Machine Learning Research.
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parmar 等人 (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
    N., Ku, A., & Tran, D. (2018). 图像变换器。载于 J. G. Dy, & A. Krause (编), 第35届国际机器学习会议（ICML）论文集，瑞典斯德哥尔摩，Stockholmsmässan，2018年7月10-15日
    (第4052-4061页)。PMLR，第80卷，机器学习研究会议论文集。
- en: 'Peng et al. (2021) Peng, Z., Huang, W., Gu, S., Xie, L., Wang, Y., Jiao, J.,
    & Ye, Q. (2021). Conformer: Local features coupling global representations for
    visual recognition. In 2021 IEEE/CVF International Conference on Computer Vision,
    ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 (pp. 357--366). IEEE.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2021) Peng, Z., Huang, W., Gu, S., Xie, L., Wang, Y., Jiao, J., & Ye,
    Q. (2021). Conformer：局部特征耦合全局表示用于视觉识别。载于2021 IEEE/CVF国际计算机视觉会议（ICCV 2021），2021年10月10-17日，加拿大蒙特利尔
    (第357-366页)。IEEE。
- en: 'Picco et al. (2021) Picco, G., Hoang, T. L., Sbodio, M. L., & López, V. (2021).
    Neural unification for logic reasoning over natural language. In M. Moens, X. Huang,
    L. Specia, & S. W. Yih (Eds.), Findings of the Association for Computational Linguistics:
    EMNLP, Virtual Event / Punta Cana, Dominican Republic, 16-20 November (pp. 3939--3950).
    Association for Computational Linguistics.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Picco 等人 (2021) Picco, G., Hoang, T. L., Sbodio, M. L., & López, V. (2021).
    自然语言逻辑推理的神经统一。载于 M. Moens, X. Huang, L. Specia, & S. W. Yih (编), 计算语言学协会发现：EMNLP，虚拟会议
    / 多米尼加共和国蓬塔卡纳，2021年11月16-20日 (第3939-3950页)。计算语言学协会。
- en: Pnueli (1977) Pnueli, A. (1977). The temporal logic of programs. In 18th Annual
    Symposium on Foundations of Computer Science, Providence, Rhode Island, USA, 31
    October - 1 November (pp. 46--57). IEEE Computer Society.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pnueli (1977) Pnueli, A. (1977). 程序的时序逻辑。载于第18届计算机科学基础年会，罗德岛州普罗维登斯，美国，1977年10月31日-11月1日
    (第46-57页)。IEEE计算机协会。
- en: 'Polu & Sutskever (2020) Polu, S., & Sutskever, I. (2020). Generative language
    modeling for automated theorem proving. CoRR, abs/2009.03393. URL: [https://arxiv.org/abs/2009.03393](https://arxiv.org/abs/2009.03393).
    [arXiv:2009.03393](http://arxiv.org/abs/2009.03393).'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polu & Sutskever (2020) Polu, S., & Sutskever, I. (2020). 自动定理证明的生成语言建模。CoRR,
    abs/2009.03393。网址：[https://arxiv.org/abs/2009.03393](https://arxiv.org/abs/2009.03393)。[arXiv:2009.03393](http://arxiv.org/abs/2009.03393)。
- en: 'Qi et al. (2020) Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang,
    R., & Zhou, M. (2020). Prophetnet: Predicting future n-gram for sequence-to-sequence
    pre-training. In T. Cohn, Y. He, & Y. Liu (Eds.), Findings of the Association
    for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November (pp. 2401--2410).
    Association for Computational Linguistics volume EMNLP 2020 of Findings of ACL.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等人 (2020) Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang,
    R., & Zhou, M. (2020). Prophetnet：预测序列到序列预训练的未来n-gram。载于 T. Cohn, Y. He, & Y.
    Liu (编), 计算语言学协会发现：EMNLP 2020，在线会议，2020年11月16-20日 (第2401-2410页)。计算语言学协会，第2020年EMNLP卷，ACL发现。
- en: Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G.,
    & Sutskever, I. (2021). Learning transferable visual models from natural language
    supervision. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International
    Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 8748--8763).
    PMLR volume 139 of Proceedings of Machine Learning Research.
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2021）Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,
    S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever,
    I. (2021). 从自然语言监督中学习可迁移的视觉模型。载于M. Meila, & T. Zhang（编辑），第38届国际机器学习大会论文集，ICML,
    2021年7月18-24日，虚拟会议（第8748-8763页）。PMLR第139卷机器学习研究论文集。
- en: 'Radford et al. (2022) Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey,
    C., & Sutskever, I. (2022). Robust speech recognition via large-scale weak supervision.
    CoRR, abs/2212.04356. URL: [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356).
    doi:[10.48550/arXiv.2212.04356](http://dx.doi.org/10.48550/arXiv.2212.04356).
    [arXiv:2212.04356](http://arxiv.org/abs/2212.04356).'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford等人（2022）Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C.,
    & Sutskever, I. (2022). 通过大规模弱监督实现鲁棒的语音识别。CoRR, abs/2212.04356. 网址: [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356).
    doi:[10.48550/arXiv.2212.04356](http://dx.doi.org/10.48550/arXiv.2212.04356).
    [arXiv:2212.04356](http://arxiv.org/abs/2212.04356)。'
- en: Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., & Sutskever,
    I. (2018). Improving language understanding with unsupervised learning. Technical
    Report OpenAI.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2018）Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018).
    通过无监督学习提升语言理解能力。技术报告OpenAI。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I. et al. (2019). Language models are unsupervised multitask learners.
    OpenAI blog, 1, 9.
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I. 等. (2019). 语言模型是无监督的多任务学习者。OpenAI博客, 1, 9。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of
    transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.,
    21, 140:1--140:67.
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等人（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., & Liu, P. J. (2020). 通过统一的文本到文本Transformer探索迁移学习的极限。J. Mach.
    Learn. Res., 21, 140:1-140:67。
- en: Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford,
    A., Chen, M., & Sutskever, I. (2021). Zero-shot text-to-image generation. In M. Meila,
    & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine
    Learning, ICML, 18-24 July, Virtual Event (pp. 8821--8831). PMLR volume 139 of
    Proceedings of Machine Learning Research.
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh等人（2021）Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford,
    A., Chen, M., & Sutskever, I. (2021). 零样本文本到图像生成。载于M. Meila, & T. Zhang（编辑），第38届国际机器学习大会论文集，ICML,
    2021年7月18-24日，虚拟会议（第8821-8831页）。PMLR第139卷机器学习研究论文集。
- en: Reiter & Dale (1997) Reiter, E., & Dale, R. (1997). Building applied natural
    language generation systems. Nat. Lang. Eng., 3, 57--87.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter & Dale（1997）Reiter, E., & Dale, R. (1997). 构建应用自然语言生成系统。Nat. Lang. Eng.,
    3, 57-87。
- en: Ren et al. (2023) Ren, Q., Li, Y., & Liu, Y. (2023). Transformer-enhanced periodic
    temporal convolution network for long short-term traffic flow forecasting. Expert
    Syst. Appl., 227, 120203.
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等人（2023）Ren, Q., Li, Y., & Liu, Y. (2023). 基于Transformer增强的周期性时间卷积网络用于长短期交通流预测。Expert
    Syst. Appl., 227, 120203。
- en: 'Ren et al. (2022) Ren, Z., Cheng, N., Sun, R., Wang, X., Lu, N., & Xu, W. (2022).
    Sigt: An efficient end-to-end MIMO-OFDM receiver framework based on transformer.
    In 5th International Conference on Communications, Signal Processing, and their
    Applications, ICCSPA, Cairo, Egypt, December 27-29 (pp. 1--6). IEEE.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren等人（2022）Ren, Z., Cheng, N., Sun, R., Wang, X., Lu, N., & Xu, W. (2022).
    Sigt: 基于Transformer的高效端到端MIMO-OFDM接收框架。载于第5届国际通信、信号处理及其应用大会，ICCSPA, 开罗，埃及，2022年12月27-29日（第1-6页）。IEEE。'
- en: Reza et al. (2022) Reza, S., Ferreira, M. C., Machado, J. J. M., & Tavares,
    J. M. R. S. (2022). A multi-head attention-based transformer model for traffic
    flow forecasting with a comparative analysis to recurrent neural networks. Expert
    Syst. Appl., 202, 117275.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reza等人（2022）Reza, S., Ferreira, M. C., Machado, J. J. M., & Tavares, J. M. R.
    S. (2022). 基于多头注意力的Transformer模型用于交通流预测，并与递归神经网络进行比较分析。Expert Syst. Appl., 202,
    117275。
- en: Richardson & Sabharwal (2022) Richardson, K., & Sabharwal, A. (2022). Pushing
    the limits of rule reasoning in transformers through natural language satisfiability.
    In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI, Virtual Event,
    February 22 - March 1 (pp. 11209--11219). AAAI Press.
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richardson & Sabharwal（2022）Richardson, K., & Sabharwal, A. (2022). 通过自然语言可满足性推动变换器中规则推理的极限。在第36届AAAI人工智能会议，AAAI，虚拟会议，2022年2月22日
    - 3月1日（第11209--11219页）。AAAI出版社。
- en: 'Rjoub et al. (2021) Rjoub, G., Bentahar, J., Abdel Wahab, O., & Saleh Bataineh,
    A. (2021). Deep and reinforcement learning for automated task scheduling in large-scale
    cloud computing systems. Concurrency and Computation: Practice and Experience,
    33, e5919.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rjoub 等（2021）Rjoub, G., Bentahar, J., Abdel Wahab, O., & Saleh Bataineh, A.
    (2021). 大规模云计算系统中的深度和强化学习用于自动任务调度。并发与计算：实践与经验, 33, e5919。
- en: 'Rjoub et al. (2019) Rjoub, G., Bentahar, J., Wahab, O. A., & Bataineh, A. (2019).
    Deep smart scheduling: A deep learning approach for automated big data scheduling
    over the cloud. In 2019 7th International Conference on Future Internet of Things
    and Cloud (FiCloud) (pp. 189--196). IEEE.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rjoub 等（2019）Rjoub, G., Bentahar, J., Wahab, O. A., & Bataineh, A. (2019). 深度智能调度：一种用于自动大数据调度的深度学习方法。在2019年第七届未来物联网与云计算国际会议（FiCloud）（第189--196页）。IEEE。
- en: Rjoub et al. (2022) Rjoub, G., Wahab, O. A., Bentahar, J., & Bataineh, A. (2022).
    Trust-driven reinforcement selection strategy for federated learning on IoT devices.
    Computing, (pp. 1--23).
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rjoub 等（2022）Rjoub, G., Wahab, O. A., Bentahar, J., & Bataineh, A. (2022). 针对物联网设备的信任驱动强化选择策略。计算，(第1--23页)。
- en: 'Ruan & Jin (2022) Ruan, L., & Jin, Q. (2022). Survey: Transformer based video-language
    pre-training. AI Open, 3, 1--13.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan & Jin（2022）Ruan, L., & Jin, Q. (2022). 调查：基于变换器的视频语言预训练。AI Open, 3, 1--13。
- en: 'Saha et al. (2020) Saha, S., Ghosh, S., Srivastava, S., & Bansal, M. (2020).
    Prover: Proof generation for interpretable reasoning over rules. In B. Webber,
    T. Cohn, Y. He, & Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020
    (pp. 122--136). Association for Computational Linguistics.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saha 等（2020）Saha, S., Ghosh, S., Srivastava, S., & Bansal, M. (2020). Prover:
    生成可解释推理的证明。见 B. Webber, T. Cohn, Y. He, & Y. Liu（编），2020年自然语言处理经验方法会议论文集，EMNLP
    2020，在线，2020年11月16-20日（第122--136页）。计算语言学协会。'
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). Distilbert,
    a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108.
    URL: [http://arxiv.org/abs/1910.01108](http://arxiv.org/abs/1910.01108). [arXiv:1910.01108](http://arxiv.org/abs/1910.01108).'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sanh 等（2019）Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). Distilbert,
    BERT 的蒸馏版本：更小、更快、更便宜、更轻便。CoRR, abs/1910.01108. URL: [http://arxiv.org/abs/1910.01108](http://arxiv.org/abs/1910.01108).
    [arXiv:1910.01108](http://arxiv.org/abs/1910.01108).'
- en: Selsam et al. (2019) Selsam, D., Lamm, M., Bünz, B., Liang, P., de Moura, L.,
    & Dill, D. L. (2019). Learning a SAT solver from single-bit supervision. In 7th
    International Conference on Learning Representations, ICLR, New Orleans, LA, USA,
    May 6-9. OpenReview.net.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selsam 等（2019）Selsam, D., Lamm, M., Bünz, B., Liang, P., de Moura, L., & Dill,
    D. L. (2019). 从单比特监督中学习SAT求解器。在第七届国际学习表示会议，ICLR，美国路易斯安那州新奥尔良，2023年5月6-9日。OpenReview.net。
- en: 'Selva et al. (2023) Selva, J., Johansen, A. S., Escalera, S., Nasrollahi, K.,
    Moeslund, T. B., & Clapés, A. (2023). Video transformers: A survey. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, .'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selva 等（2023）Selva, J., Johansen, A. S., Escalera, S., Nasrollahi, K., Moeslund,
    T. B., & Clapés, A. (2023). 视频变换器：一项调查。IEEE模式分析与机器智能汇刊, .
- en: 'Shamshad et al. (2023) Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H., Hayat,
    M., Khan, F. S., & Fu, H. (2023). Transformers in medical imaging: A survey. Medical
    Image Analysis, (p. 102802).'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamshad 等（2023）Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H., Hayat, M.,
    Khan, F. S., & Fu, H. (2023). 医学成像中的变换器：一项调查。医学图像分析, (第102802页)。
- en: Shen et al. (2022a) Shen, H., Zhou, X., Wang, Z., & Wang, J. (2022a). State
    of charge estimation for lithium-ion battery using transformer with immersion
    and invariance adaptive observer. Journal of Energy Storage, 45, 103768.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2022a）Shen, H., Zhou, X., Wang, Z., & Wang, J. (2022a). 使用带浸入和不变性自适应观察者的变换器对锂离子电池进行充电状态估计。能源存储期刊,
    45, 103768。
- en: 'Shen et al. (2022b) Shen, X., Wang, L., Zhao, Y., Liu, R., Qian, W., & Ma,
    H. (2022b). Dilated transformer: residual axial attention for breast ultrasound
    image segmentation. Quantitative Imaging in Medicine and Surgery, 12, 4512.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2022b）Shen, X., Wang, L., Zhao, Y., Liu, R., Qian, W., & Ma, H. (2022b).
    膨胀变换器：用于乳腺超声图像分割的残差轴向注意力。医学与手术定量成像, 12, 4512。
- en: Shi et al. (2022a) Shi, C., Xiao, Y., & Chen, Z. (2022a). Dual-domain sparse-view
    ct reconstruction with transformers. Physica Medica, 101, 1--7.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等（2022a）Shi, C., Xiao, Y., & Chen, Z.（2022a）。基于变换器的双域稀疏视图CT重建。《医学物理学》，101，1-7。
- en: 'Shi et al. (2021) Shi, F., Lee, C., Bashar, M. K., Shukla, N., Zhu, S., & Narayanan,
    V. (2021). Transformer-based machine learning for fast SAT solvers and logic synthesis.
    CoRR, abs/2107.07116. URL: [https://arxiv.org/abs/2107.07116](https://arxiv.org/abs/2107.07116).
    [arXiv:2107.07116](http://arxiv.org/abs/2107.07116).'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等（2021）Shi, F., Lee, C., Bashar, M. K., Shukla, N., Zhu, S., & Narayanan,
    V.（2021）。基于变换器的机器学习用于快速SAT求解器和逻辑综合。CoRR，abs/2107.07116。网址：[https://arxiv.org/abs/2107.07116](https://arxiv.org/abs/2107.07116)。
    [arXiv:2107.07116](http://arxiv.org/abs/2107.07116)。
- en: 'Shi et al. (2022b) Shi, Z., Li, M., Khan, S., Zhen, H., Yuan, M., & Xu, Q.
    (2022b). Satformer: Transformers for SAT solving. CoRR, abs/2209.00953. URL: [https://doi.org/10.48550/arXiv.2209.00953](https://doi.org/10.48550/arXiv.2209.00953).
    doi:[10.48550/arXiv.2209.00953](http://dx.doi.org/10.48550/arXiv.2209.00953).
    [arXiv:2209.00953](http://arxiv.org/abs/2209.00953).'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等（2022b）Shi, Z., Li, M., Khan, S., Zhen, H., Yuan, M., & Xu, Q.（2022b）。Satformer：用于SAT求解的变换器。CoRR，abs/2209.00953。网址：[https://doi.org/10.48550/arXiv.2209.00953](https://doi.org/10.48550/arXiv.2209.00953)。
    doi：[10.48550/arXiv.2209.00953](http://dx.doi.org/10.48550/arXiv.2209.00953)。
    [arXiv:2209.00953](http://arxiv.org/abs/2209.00953)。
- en: 'Shih et al. (2016) Shih, K. J., Singh, S., & Hoiem, D. (2016). Where to look:
    Focus regions for visual question answering. In 2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR, Las Vegas, NV, USA, June 27-30 (pp. 4613--4621).
    IEEE Computer Society.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shih等（2016）Shih, K. J., Singh, S., & Hoiem, D.（2016）。在哪里查看：视觉问答的焦点区域。见于2016年IEEE计算机视觉与模式识别大会，CVPR，拉斯维加斯，内华达州，美国，6月27-30日（第4613-4621页）。IEEE计算机学会。
- en: Shin et al. (2022) Shin, A., Ishii, M., & Narihira, T. (2022). Perspectives
    and prospects on transformer architecture for cross-modal tasks with language
    and vision. Int. J. Comput. Vis., 130, 435--454.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin等（2022）Shin, A., Ishii, M., & Narihira, T.（2022）。关于变换器架构在语言和视觉交叉模态任务中的前景和展望。《计算机视觉国际杂志》，130，435-454。
- en: 'Sinha et al. (2019) Sinha, K., Sodhani, S., Dong, J., Pineau, J., & Hamilton,
    W. L. (2019). CLUTRR: A diagnostic benchmark for inductive reasoning from text.
    In K. Inui, J. Jiang, V. Ng, & X. Wan (Eds.), Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China,
    November 3-7 (pp. 4505--4514). Association for Computational Linguistics.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha等（2019）Sinha, K., Sodhani, S., Dong, J., Pineau, J., & Hamilton, W. L.（2019）。CLUTRR：用于从文本中进行归纳推理的诊断基准。见于K.
    Inui, J. Jiang, V. Ng, & X. Wan（编辑），2019年自然语言处理经验方法会议和第9届国际联合自然语言处理会议的论文集，EMNLP-IJCNLP，香港，中国，11月3-7日（第4505-4514页）。计算语言学协会。
- en: 'Su et al. (2020) Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., & Dai,
    J. (2020). VL-BERT: pre-training of generic visual-linguistic representations.
    In 8th International Conference on Learning Representations, ICLR, Addis Ababa,
    Ethiopia, April 26-30. OpenReview.net.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su等（2020）Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., & Dai, J.（2020）。VL-BERT：通用视觉语言表示的预训练。见于第8届国际学习表示大会，ICLR，亚的斯亚贝巴，埃塞俄比亚，4月26-30日。OpenReview.net。
- en: Subakan et al. (2021) Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., &
    Zhong, J. (2021). Attention is all you need in speech separation. In IEEE International
    Conference on Acoustics, Speech and Signal Processing, ICASSP, Toronto, ON, Canada,
    June 6-11 (pp. 21--25). IEEE.
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subakan等（2021）Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong,
    J.（2021）。语音分离中所需的一切。见于IEEE国际声学、语音和信号处理会议，ICASSP，多伦多，安大略省，加拿大，6月6-11日（第21-25页）。IEEE。
- en: 'Subramanyam et al. (2021a) Subramanyam, K., Rajasekharan, A., & Sangeetha,
    S. (2021a). Ammu: A survey of transformer-based biomedical pretrained language
    models. arXiv e-prints, (pp. arXiv--2105).'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramanyam等（2021a）Subramanyam, K., Rajasekharan, A., & Sangeetha, S.（2021a）。AMMU：基于变换器的生物医学预训练语言模型综述。arXiv
    e-prints，（第arXiv-2105页）。
- en: 'Subramanyam et al. (2021b) Subramanyam, K., Rajasekharan, A., & Sangeetha,
    S. (2021b). AMMUS : A survey of transformer-based pretrained models in natural
    language processing. CoRR, abs/2108.05542. URL: [https://arxiv.org/abs/2108.05542](https://arxiv.org/abs/2108.05542).
    [arXiv:2108.05542](http://arxiv.org/abs/2108.05542).'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramanyam等（2021b）Subramanyam, K., Rajasekharan, A., & Sangeetha, S.（2021b）。AMMUS：自然语言处理中的基于变换器的预训练模型综述。CoRR，abs/2108.05542。网址：[https://arxiv.org/abs/2108.05542](https://arxiv.org/abs/2108.05542)。
    [arXiv:2108.05542](http://arxiv.org/abs/2108.05542)。
- en: 'Sun et al. (2017) Sun, H., Chen, X., Shi, Q., Hong, M., Fu, X., & Sidiropoulos,
    N. D. (2017). Learning to optimize: Training deep neural networks for wireless
    resource management. In 18th IEEE International Workshop on Signal Processing
    Advances in Wireless Communications, SPAWC, Sapporo, Japan, July 3-6 (pp. 1--6).
    IEEE.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2017) Sun, H., Chen, X., Shi, Q., Hong, M., Fu, X., & Sidiropoulos, N.
    D. (2017)。学习优化：为无线资源管理训练深度神经网络。在第 18 届 IEEE 无线通信信号处理进展国际研讨会 (SPAWC)，日本札幌，2017年7月3-6日
    (第 1--6 页)。IEEE。
- en: 'Sun et al. (2021a) Sun, J., Shen, Z., Wang, Y., Bao, H., & Zhou, X. (2021a).
    Loftr: Detector-free local feature matching with transformers. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR virtual, June 19-25 (pp. 8922--8931).
    Computer Vision Foundation / IEEE.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2021a) Sun, J., Shen, Z., Wang, Y., Bao, H., & Zhou, X. (2021a)。Loftr:
    无检测器的局部特征匹配与 Transformers。在 IEEE 计算机视觉与模式识别会议 (CVPR) 虚拟会议，2021年6月19-25日 (第 8922--8931
    页)。计算机视觉基金会 / IEEE。'
- en: 'Sun et al. (2021b) Sun, Q., Fang, N., Liu, Z., Zhao, L., Wen, Y., Lin, H. et al.
    (2021b). Hybridctrm: Bridging cnn and transformer for multimodal brain image segmentation.
    Journal of Healthcare Engineering, 2021.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2021b) Sun, Q., Fang, N., Liu, Z., Zhao, L., Wen, Y., Lin, H. 等 (2021b)。Hybridctrm:
    结合 CNN 和 Transformer 进行多模态脑部图像分割。《医疗工程期刊》，2021。'
- en: Suzuki & Matsuo (2022) Suzuki, M., & Matsuo, Y. (2022). A survey of multimodal
    deep generative models. Adv. Robotics, 36, 261--278.
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzuki & Matsuo (2022) Suzuki, M., & Matsuo, Y. (2022)。多模态深度生成模型的综述。《先进机器人学》，36，261--278。
- en: Szummer & Picard (1998) Szummer, M., & Picard, R. W. (1998). Indoor-outdoor
    image classification. In 1998 International Workshop on Content-Based Access of
    Image and Video Databases, CAIVD 1998, Bombay, India, January 3, 1998 (pp. 42--51).
    IEEE Computer Society.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szummer & Picard (1998) Szummer, M., & Picard, R. W. (1998)。室内-室外图像分类。在 1998
    年国际图像和视频数据库内容检索研讨会 (CAIVD 1998)，印度孟买，1998年1月3日 (第 42--51 页)。IEEE 计算机学会。
- en: 'Tan & Bansal (2019) Tan, H., & Bansal, M. (2019). LXMERT: learning cross-modality
    encoder representations from transformers. In K. Inui, J. Jiang, V. Ng, & X. Wan
    (Eds.), Proceedings of the Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing,
    EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp. 5099--5110). Association for
    Computational Linguistics.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan & Bansal (2019) Tan, H., & Bansal, M. (2019)。LXMERT: 从 Transformers 学习跨模态编码器表示。在
    K. Inui, J. Jiang, V. Ng, & X. Wan (编辑)，自然语言处理经验方法会议与第 9 届国际自然语言处理联合会议 (EMNLP-IJCNLP)
    论文集，中国香港，2019年11月3-7日 (第 5099--5110 页)。计算语言学协会。'
- en: Tas & Kiyani (2007) Tas, O., & Kiyani, F. (2007). A survey automatic text summarization.
    PressAcademia Procedia, 5, 205--213.
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tas & Kiyani (2007) Tas, O., & Kiyani, F. (2007)。自动文本摘要的综述。《PressAcademia Procedia》，5，205--213。
- en: 'Tay et al. (2023) Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2023). Efficient
    transformers: A survey. ACM Comput. Surv., 55, 109:1--109:28.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tay 等 (2023) Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2023)。高效的 Transformers:
    一项综述。《ACM计算机调查》，55，109:1--109:28。'
- en: 'Tay et al. (2022) Tay, Y., Tran, V. Q., Ruder, S., Gupta, J. P., Chung, H. W.,
    Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. (2022). Charformer:
    Fast character transformers via gradient-based subword tokenization. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29. OpenReview.net.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tay 等 (2022) Tay, Y., Tran, V. Q., Ruder, S., Gupta, J. P., Chung, H. W., Bahri,
    D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. (2022)。Charformer: 通过基于梯度的子词标记化实现快速字符
    Transformers。在第十届国际学习表征会议 (ICLR 2022)，虚拟活动，2022年4月25-29日。OpenReview.net。'
- en: Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
    A., & Jégou, H. (2021). Training data-efficient image transformers & distillation
    through attention. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International
    Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 10347--10357).
    PMLR volume 139 of Proceedings of Machine Learning Research.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
    A., & Jégou, H. (2021)。通过注意力训练数据高效的图像 Transformer 及其蒸馏。在 M. Meila 和 T. Zhang (编辑)，第
    38 届国际机器学习大会(ICML) 论文集，2021年7月18-24日，虚拟活动 (第 10347--10357 页)。PMLR 第 139 卷，机器学习研究论文集。
- en: 'Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all
    you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N.
    Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems
    30: Annual Conference on Neural Information Processing Systems, December 4-9,
    Long Beach, CA, USA (pp. 5998--6008).'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). 注意力即你所需要的一切。在I. Guyon,
    U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, &
    R. Garnett (编辑)，神经信息处理系统第30届年会：神经信息处理系统年会，12月4-9日，长滩，加利福尼亚，美国（第5998--6008页）。
- en: 'Vig et al. (2021) Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher,
    R., & Rajani, N. F. (2021). Bertology meets biology: Interpreting attention in
    protein language models. In 9th International Conference on Learning Representations,
    ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig et al. (2021) Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R.,
    & Rajani, N. F. (2021). Bertology遇见生物学：解读蛋白质语言模型中的注意力。在第9届国际学习表示会议，ICLR，虚拟会议，奥地利，5月3-7日。OpenReview.net。
- en: 'Wang & Chen (2018) Wang, D., & Chen, J. (2018). Supervised speech separation
    based on deep learning: An overview. IEEE ACM Trans. Audio Speech Lang. Process.,
    26, 1702--1726.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Chen (2018) Wang, D., & Chen, J. (2018). 基于深度学习的监督语音分离：概述。IEEE ACM Trans.
    Audio Speech Lang. Process., 26, 1702--1726。
- en: 'Wang et al. (2020a) Wang, G., Smetannikov, I., & Man, T. (2020a). Survey on
    automatic text summarization and transformer models applicability. In CCRIS: International
    Conference on Control, Robotics and Intelligent System, Xiamen, China, October
    27-29 (pp. 176--184). ACM.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Wang, G., Smetannikov, I., & Man, T. (2020a). 自动文本摘要和变换器模型适用性的调查。在CCRIS：控制、机器人及智能系统国际会议，厦门，中国，10月27-29日（第176--184页）。ACM。
- en: 'Wang et al. (2022a) Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu,
    Z., Liu, C., & Wang, L. (2022a). GIT: A generative image-to-text transformer for
    vision and language. CoRR, abs/2205.14100. URL: [https://doi.org/10.48550/arXiv.2205.14100](https://doi.org/10.48550/arXiv.2205.14100).
    doi:[10.48550/arXiv.2205.14100](http://dx.doi.org/10.48550/arXiv.2205.14100).
    [arXiv:2205.14100](http://arxiv.org/abs/2205.14100).'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu,
    Z., Liu, C., & Wang, L. (2022a). GIT：一种用于视觉和语言的生成图像到文本变换器。CoRR, abs/2205.14100。网址：[https://doi.org/10.48550/arXiv.2205.14100](https://doi.org/10.48550/arXiv.2205.14100)。doi:[10.48550/arXiv.2205.14100](http://dx.doi.org/10.48550/arXiv.2205.14100)。[arXiv:2205.14100](http://arxiv.org/abs/2205.14100)。
- en: Wang et al. (2021a) Wang, P., Cheng, Y., & Dong, B. (2021a). Augmented convolutional
    neural networks with transformer for wireless interference identification. In
    IEEE Global Communications Conference, GLOBECOM, Madrid, Spain, December 7-11
    (pp. 1--6). IEEE.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Wang, P., Cheng, Y., & Dong, B. (2021a). 增强型卷积神经网络与变换器用于无线干扰识别。在IEEE全球通信会议，GLOBECOM，马德里，西班牙，12月7-11日（第1--6页）。IEEE。
- en: Wang et al. (2022b) Wang, S., Bi, S., & Zhang, Y.-J. A. (2022b). Deep reinforcement
    learning with communication transformer for adaptive live streaming in wireless
    edge networks. IEEE Journal on Selected Areas in Communications, 40, 308--322.
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Wang, S., Bi, S., & Zhang, Y.-J. A. (2022b). 使用通信变换器的深度强化学习用于无线边缘网络中的自适应直播。IEEE
    Journal on Selected Areas in Communications, 40, 308--322。
- en: 'Wang et al. (2021b) Wang, T., Lai, Z., & Kong, H. (2021b). Tfnet: Transformer
    fusion network for ultrasound image segmentation. In C. Wallraven, Q. Liu, & H. Nagahara
    (Eds.), Pattern Recognition - 6th Asian Conference, ACPR, Jeju Island, South Korea,
    November 9-12, Revised Selected Papers, Part I (pp. 314--325). Springer volume
    13188 of Lecture Notes in Computer Science.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) Wang, T., Lai, Z., & Kong, H. (2021b). Tfnet：用于超声图像分割的变换器融合网络。在C.
    Wallraven, Q. Liu, & H. Nagahara (编辑)，模式识别 - 第6届亚洲会议，ACPR，济州岛，韩国，11月9-12日，修订精选论文，第I部分（第314--325页）。Springer，计算机科学讲义笔记第13188卷。
- en: 'Wang et al. (2022c) Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y.,
    Zhang, H., Wang, J., Chen, M., Jiang, H. et al. (2022c). O-net: a novel framework
    with deep fusion of cnn and transformer for simultaneous segmentation and classification.
    Frontiers in Neuroscience, 16.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022c) Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y.,
    Zhang, H., Wang, J., Chen, M., Jiang, H. 等 (2022c). O-net：一种通过CNN和变换器的深度融合进行同步分割和分类的新型框架。Frontiers
    in Neuroscience, 16。
- en: 'Wang et al. (2020b) Wang, W., Liang, D., Chen, Q., Iwamoto, Y., Han, X.-H.,
    Zhang, Q., Hu, H., Lin, L., & Chen, Y.-W. (2020b). Medical image classification
    using deep learning. Deep learning in healthcare: paradigms and applications,
    (pp. 33--51).'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020b）Wang, W., Liang, D., Chen, Q., Iwamoto, Y., Han, X.-H., Zhang,
    Q., Hu, H., Lin, L., & Chen, Y.-W.（2020b）。使用深度学习进行医学图像分类。《深度学习在医疗保健中的应用：范式与应用》，（第33-51页）。
- en: 'Wang et al. (2019) Wang, Z., Ma, Y., Liu, Z., & Tang, J. (2019). R-transformer:
    Recurrent neural network enhanced transformer. CoRR, abs/1907.05572. URL: [http://arxiv.org/abs/1907.05572](http://arxiv.org/abs/1907.05572).
    [arXiv:1907.05572](http://arxiv.org/abs/1907.05572).'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019）Wang, Z., Ma, Y., Liu, Z., & Tang, J.（2019）。R-transformer：递归神经网络增强的Transformer。CoRR，abs/1907.05572。网址：[http://arxiv.org/abs/1907.05572](http://arxiv.org/abs/1907.05572)。[arXiv:1907.05572](http://arxiv.org/abs/1907.05572)。
- en: 'Wang et al. (2022d) Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao,
    Y. (2022d). Simvlm: Simple visual language model pretraining with weak supervision.
    In The Tenth International Conference on Learning Representations, ICLR, Virtual
    Event, April 25-29. OpenReview.net.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022d）Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y.（2022d）。Simvlm：一种简单的视觉语言模型，通过弱监督进行预训练。发表于第十届国际学习表征会议，ICLR，虚拟会议，4月25-29日。OpenReview.net。
- en: Wu et al. (2016) Wu, D., Pigou, L., Kindermans, P., Le, N. D., Shao, L., Dambre,
    J., & Odobez, J. (2016). Deep dynamic neural networks for multimodal gesture segmentation
    and recognition. IEEE Trans. Pattern Anal. Mach. Intell., 38, 1583--1597.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2016）Wu, D., Pigou, L., Kindermans, P., Le, N. D., Shao, L., Dambre, J.,
    & Odobez, J.（2016）。用于多模态手势分割和识别的深度动态神经网络。《IEEE模式分析与机器智能汇刊》，38，1583-1597。
- en: 'Wu et al. (2022) Wu, Y., Wang, G., Wang, Z., Wang, H., & Li, Y. (2022). Di-unet:
    Dimensional interaction self-attention for medical image segmentation. Biomed.
    Signal Process. Control., 78, 103896.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2022）Wu, Y., Wang, G., Wang, Z., Wang, H., & Li, Y.（2022）。Di-unet：用于医学图像分割的维度交互自注意力。《生物医学信号处理与控制》，78，103896。
- en: Xie et al. (2022) Xie, W., Zou, J., Xiao, J., Li, M., & Peng, X. (2022). Quan-transformer
    based channel feedback for ris-aided wireless communication systems. IEEE Commun.
    Lett., 26, 2631--2635.
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人（2022）Xie, W., Zou, J., Xiao, J., Li, M., & Peng, X.（2022）。基于Quan-Transformer的通道反馈用于RIS辅助无线通信系统。《IEEE通信快报》，26，2631-2635。
- en: 'Xing et al. (2021) Xing, Y., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y., & Wattenhofer,
    R. (2021). KM-BART: knowledge enhanced multimodal BART for visual commonsense
    generation. In C. Zong, F. Xia, W. Li, & R. Navigli (Eds.), Proceedings of the
    59th Annual Meeting of the Association for Computational Linguistics and the 11th
    International Joint Conference on Natural Language Processing, ACL/IJCNLP, (Volume
    1: Long Papers), Virtual Event, August 1-6 (pp. 525--535). Association for Computational
    Linguistics.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等人（2021）Xing, Y., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y., & Wattenhofer,
    R.（2021）。KM-BART：用于视觉常识生成的知识增强多模态BART。发表于C. Zong, F. Xia, W. Li, & R. Navigli（编辑），第59届计算语言学协会年会暨第11届国际自然语言处理联合会议的论文集，ACL/IJCNLP，（第1卷：长篇论文），虚拟会议，8月1-6日（第525-535页）。计算语言学协会。
- en: 'Xu et al. (2022) Xu, Y., Wei, H., Lin, M., Deng, Y., Sheng, K., Zhang, M.,
    Tang, F., Dong, W., Huang, F., & Xu, C. (2022). Transformers in computational
    visual media: A survey. Computational Visual Media, 8, 33--62.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2022）Xu, Y., Wei, H., Lin, M., Deng, Y., Sheng, K., Zhang, M., Tang, F.,
    Dong, W., Huang, F., & Xu, C.（2022）。计算视觉媒体中的Transformers：一项调查。《计算视觉媒体》，8，33-62。
- en: Xu & Zhao (2022) Xu, Y., & Zhao, J. (2022). Actor-critic with transformer for
    cloud computing resource three stage job scheduling. In 7th International Conference
    on Cloud Computing and Big Data Analytics (ICCCBDA), Chengdu, China, 22-24 April
    (pp. 33--37).
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu & Zhao（2022）Xu, Y., & Zhao, J.（2022）。基于Transformer的演员-评论家方法用于云计算资源的三阶段作业调度。发表于第七届国际云计算与大数据分析会议（ICCCBDA），中国成都，4月22-24日（第33-37页）。
- en: Yan et al. (2022a) Yan, J., Li, J., Xu, H., Yu, Y., & Xu, T. (2022a). Seizure
    prediction based on transformer using scalp electroencephalogram. Applied Sciences,
    12, 4158.
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人（2022a）Yan, J., Li, J., Xu, H., Yu, Y., & Xu, T.（2022a）。基于Transformer的头皮脑电图癫痫预测。《应用科学》，12，4158。
- en: Yan et al. (2022b) Yan, S., Wang, C., Chen, W., & Lyu, J. (2022b). Swin transformer-based
    GAN for multi-modal medical image translation. Frontiers in Oncology, 12.
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人（2022b）Yan, S., Wang, C., Chen, W., & Lyu, J.（2022b）。基于Swin Transformer的GAN用于多模态医学图像翻译。《肿瘤学前沿》，12。
- en: 'Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., & Srinivas, A. (2021). Videogpt:
    Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157. URL: [https://arxiv.org/abs/2104.10157](https://arxiv.org/abs/2104.10157).
    [arXiv:2104.10157](http://arxiv.org/abs/2104.10157).'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., & Srinivas, A. (2021). Videogpt:
    使用 VQ-VAE 和变换器的视频生成。CoRR, abs/2104.10157. 网址: [https://arxiv.org/abs/2104.10157](https://arxiv.org/abs/2104.10157).
    [arXiv:2104.10157](http://arxiv.org/abs/2104.10157)。'
- en: 'Yang & Yang (2023) Yang, H., & Yang, D. (2023). Cswin-pnet: A cnn-swin transformer
    combined pyramid network for breast lesion segmentation in ultrasound images.
    Expert Syst. Appl., Volume 213, Part B, 119024.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang & Yang (2023) Yang, H., & Yang, D. (2023). Cswin-pnet: 一种结合 CNN 和 swin
    变换器的金字塔网络，用于乳腺病变的超声图像分割。Expert Syst. Appl., 第213卷，第B部分，119024。'
- en: Yang et al. (2022) Yang, M., Lee, D., & Park, S. (2022). Automated diagnosis
    of atrial fibrillation using ECG component-aware transformer. Comput. Biol. Medicine,
    150, 106115.
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2022) Yang, M., Lee, D., & Park, S. (2022). 基于 ECG 组件感知变换器的房颤自动诊断。Comput.
    Biol. Medicine, 150, 106115。
- en: 'Yeh et al. (2019) Yeh, C., Mahadeokar, J., Kalgaonkar, K., Wang, Y., Le, D.,
    Jain, M., Schubert, K., Fuegen, C., & Seltzer, M. L. (2019). Transformer-transducer:
    End-to-end speech recognition with self-attention. CoRR, abs/1910.12977. URL:
    [http://arxiv.org/abs/1910.12977](http://arxiv.org/abs/1910.12977). [arXiv:1910.12977](http://arxiv.org/abs/1910.12977).'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yeh et al. (2019) Yeh, C., Mahadeokar, J., Kalgaonkar, K., Wang, Y., Le, D.,
    Jain, M., Schubert, K., Fuegen, C., & Seltzer, M. L. (2019). Transformer-transducer:
    使用自注意力的端到端语音识别。CoRR, abs/1910.12977. 网址: [http://arxiv.org/abs/1910.12977](http://arxiv.org/abs/1910.12977).
    [arXiv:1910.12977](http://arxiv.org/abs/1910.12977)。'
- en: Yu & Deng (2016) Yu, D., & Deng, L. (2016). Automatic speech recognition volume 1.
    Springer.
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu & Deng (2016) Yu, D., & Deng, L. (2016). 《自动语音识别 第一卷》。Springer。
- en: Yu et al. (2020) Yu, J., Li, J., Yu, Z., & Huang, Q. (2020). Multimodal transformer
    with multi-view visual representation for image captioning. IEEE Trans. Circuits
    Syst. Video Technol., 30, 4467--4480.
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2020) Yu, J., Li, J., Yu, Z., & Huang, Q. (2020). 带多视角视觉表示的多模态变换器用于图像描述。IEEE
    Trans. Circuits Syst. Video Technol., 30, 4467-4480。
- en: 'Yu et al. (2017) Yu, S., Wang, X., & Langar, R. (2017). Computation offloading
    for mobile edge computing: A deep learning approach. In 28th IEEE Annual International
    Symposium on Personal, Indoor, and Mobile Radio Communications, PIMRC, Montreal,
    QC, Canada, October 8-13 (pp. 1--6). IEEE.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2017) Yu, S., Wang, X., & Langar, R. (2017). 移动边缘计算的计算卸载：一种深度学习方法。在第28届IEEE个人、室内和移动无线通信年会，PIMRC,
    Montreal, QC, Canada, 10月8-13日（第1-6页）。IEEE。
- en: 'Yuan et al. (2021) Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao,
    J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi,
    Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., & Zhang,
    P. (2021). Florence: A new foundation model for computer vision. CoRR, abs/2111.11432.
    URL: [https://arxiv.org/abs/2111.11432](https://arxiv.org/abs/2111.11432). [arXiv:2111.11432](http://arxiv.org/abs/2111.11432).'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2021) Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao,
    J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi,
    Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., & Zhang,
    P. (2021). Florence: 一种用于计算机视觉的新基础模型。CoRR, abs/2111.11432. 网址: [https://arxiv.org/abs/2111.11432](https://arxiv.org/abs/2111.11432).
    [arXiv:2111.11432](http://arxiv.org/abs/2111.11432)。'
- en: 'Yun et al. (2019) Yun, S., Jeong, M., Kim, R., Kang, J., & Kim, H. J. (2019).
    Graph transformer networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc,
    E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems, NeurIPS, December
    8-14, Vancouver, BC, Canada (pp. 11960--11970).'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yun et al. (2019) Yun, S., Jeong, M., Kim, R., Kang, J., & Kim, H. J. (2019).
    图变换器网络。在 H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox,
    & R. Garnett (编), 《神经信息处理系统进展 32: 神经信息处理系统年会》，NeurIPS, 12月8-14日, Vancouver, BC,
    Canada（第11960-11970页）。'
- en: 'Zellers et al. (2019) Zellers, R., Bisk, Y., Farhadi, A., & Choi, Y. (2019).
    From recognition to cognition: Visual commonsense reasoning. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR, Long Beach, CA, USA, June 16-20
    (pp. 6720--6731). Computer Vision Foundation / IEEE.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) Zellers, R., Bisk, Y., Farhadi, A., & Choi, Y. (2019).
    从识别到认知：视觉常识推理。在IEEE计算机视觉与模式识别会议，CVPR, Long Beach, CA, USA, 6月16-20日（第6720-6731页）。计算机视觉基金会
    / IEEE。
- en: 'Zhang et al. (2019) Zhang, C., Patras, P., & Haddadi, H. (2019). Deep learning
    in mobile and wireless networking: A survey. IEEE Commun. Surv. Tutorials, 21,
    2224--2287.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019) Zhang, C., Patras, P., & Haddadi, H. (2019). 移动和无线网络中的深度学习：综述。IEEE
    Commun. Surv. Tutorials, 21, 2224-2287。
- en: 'Zhang et al. (2020a) Zhang, J., Zhao, Y., Saleh, M., & Liu, P. J. (2020a).
    PEGASUS: pre-training with extracted gap-sentences for abstractive summarization.
    In Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July, Virtual Event (pp. 11328--11339). PMLR volume 119 of Proceedings
    of Machine Learning Research.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2020a）Zhang，J.，Zhao，Y.，Saleh，M.，& Liu，P.J.（2020a）。 PEGASUS：用于抽象摘要的提取间隙句子的预训练。在第37届国际机器学习大会ICML
    2020的论文集，7月13-18日，虚拟活动（pp. 11328-11339）。PMLR机器学习研究论文集的第119卷。
- en: 'Zhang et al. (2020b) Zhang, Q., Lu, H., Sak, H., Tripathi, A., McDermott, E.,
    Koo, S., & Kumar, S. (2020b). Transformer transducer: A streamable speech recognition
    model with transformer encoders and RNN-T loss. In IEEE International Conference
    on Acoustics, Speech and Signal Processing, ICASSP, Barcelona, Spain, May 4-8
    (pp. 7829--7833). IEEE.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2020b）Zhang，Q.，Lu，H.，Sak，H.，Tripathi，A.，McDermott，E.，Koo，S.，& Kumar，S.（2020b）。
    转换器传导器：具有转换器编码器和RNN-T损失的可流式语音识别模型。在IEEE国际会议上，ICASSP，巴塞罗那，西班牙，5月4-8日（pp. 7829-7833）。IEEE。
- en: 'Zhang et al. (2022) Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor,
    J., Jansen, A., Xu, Y., Huang, Y., Wang, S., Zhou, Z., Li, B., Ma, M., Chan, W.,
    Yu, J., Wang, Y., Cao, L., Sim, K. C., Ramabhadran, B., Sainath, T. N., Beaufays,
    F., Chen, Z., Le, Q. V., Chiu, C., Pang, R., & Wu, Y. (2022). Bigssl: Exploring
    the frontier of large-scale semi-supervised learning for automatic speech recognition.
    IEEE J. Sel. Top. Signal Process., 16, 1519--1532.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2022）Zhang，Y.，Park，D.S.，Han，W.，Qin，J.，Gulati，A.，Shor，J.，Jansen，A.，Xu，Y.，Huang，Y.，Wang，S.，Zhou，Z.，Li，B.，Ma，M.，Chan，W.，Yu，J.，Wang，Y.，Cao，L.，Sim，K.C.，Ramabhadran，B.，Sainath，T.N.，Beaufays，F.，Chen，Z.，Le，Q.V.，Chiu，C.，Pang，R.，&
    Wu，Y.（2022）。 Bigssl：探索大规模半监督学习对自动语音识别的前沿。IEEE J. Sel. Top. Signal Process.，16，1519-1532。
- en: 'Zhao et al. (2019) Zhao, Z., Zheng, P., Xu, S., & Wu, X. (2019). Object detection
    with deep learning: A review. IEEE Trans. Neural Networks Learn. Syst., 30, 3212--3232.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2019）Zhao，Z.，Zheng，P.，Xu，S.，& Wu，X.（2019）。 利用深度学习进行目标检测：一项回顾。IEEE Trans.
    Neural Networks Learn. Syst.，30，3212-3232。
- en: Zheng et al. (2021) Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
    Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., & Zhang, L. (2021). Rethinking semantic
    segmentation from a sequence-to-sequence perspective with transformers. In IEEE
    Conference on Computer Vision and Pattern Recognition, CVPR, virtual, June 19-25
    (pp. 6881--6890). Computer Vision Foundation / IEEE.
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2021）Zheng，S.，Lu，J.，Zhao，H.，Zhu，X.，Luo，Z.，Wang，Y.，Fu，Y.，Feng，J.，Xiang，T.，Torr，P.H.S.，&
    Zhang，L.（2021）。 从transformer的序列到序列的观点重新思考语义分割。在计算机视觉和模式识别IEEE会议上，CVPR，虚拟，6月19-25日（pp.
    6881-6890）。计算机视觉基金会/ IEEE。
- en: Zheng et al. (2020) Zheng, Y., Li, X., Xie, F., & Lu, L. (2020). Improving end-to-end
    speech synthesis with local recurrent neural network enhanced transformer. In
    2020 IEEE International Conference on Acoustics, Speech and Signal Processing,
    ICASSP 2020, Barcelona, Spain, May 4-8, 2020 (pp. 6734--6738). IEEE.
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2020）Zheng，Y.，Li，X.，Xie，F.，& Lu，L.（2020）。 用本地循环神经网络增强transformer改进端到端语音合成。在2020年IEEE国际会议上，ICASSP
    2020，西班牙巴塞罗那，2020年5月4-8日（pp. 6734-6738）。IEEE。
- en: 'Zhou et al. (2021a) Zhou, H., Guo, J., Zhang, Y., Yu, L., Wang, L., & Yu, Y.
    (2021a). nnformer: Interleaved transformer for volumetric segmentation. CoRR,
    abs/2109.03201. URL: [https://arxiv.org/abs/2109.03201](https://arxiv.org/abs/2109.03201).
    [arXiv:2109.03201](http://arxiv.org/abs/2109.03201).'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2021a）Zhou，H.，Guo，J.，Zhang，Y.，Yu，L.，Wang，L.，& Yu，Y.（2021a）。 nnformer：用于体积分割的交替变压器。CoRR，abs/2109.03201。网址：[https://arxiv.org/abs/2109.03201](https://arxiv.org/abs/2109.03201)。[arXiv:2109.03201](http://arxiv.org/abs/2109.03201)。
- en: 'Zhou et al. (2021b) Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille,
    A. L., & Kong, T. (2021b). iBOT: Image BERT pre-training with online tokenizer.
    CoRR, abs/2111.07832. URL: [https://arxiv.org/abs/2111.07832](https://arxiv.org/abs/2111.07832).
    [arXiv:2111.07832](http://arxiv.org/abs/2111.07832).'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2021b）Zhou，J.，Wei，C.，Wang，H.，Shen，W.，Xie，C.，Yuille，A.L.，& Kong，T.（2021b）。
    iBOT：使用在线分词器进行图像BERT预训练。CoRR，abs/2111.07832。网址：[https://arxiv.org/abs/2111.07832](https://arxiv.org/abs/2111.07832)。[arXiv:2111.07832](http://arxiv.org/abs/2111.07832)。
- en: Zhou et al. (2020) Zhou, S., Li, J., Zhang, K., Wen, M., & Guan, Q. (2020).
    An accurate ensemble forecasting approach for highly dynamic cloud workload with
    VMD and r-transformer. IEEE Access, 8, 115992--116003.
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2020）Zhou，S.，Li，J.，Zhang，K.，Wen，M.，& Guan，Q.（2020）。 一种用VMD和r-transformer进行高度动态云工作负载精确集成预测方法。IEEE
    Access，8，115992-116003。
- en: Zhu et al. (2022) Zhu, X., Hu, H., Wang, H., Yao, J., Li, W., Ou, D., & Xu,
    D. (2022). Region aware transformer for automatic breast ultrasound tumor segmentation.
    In E. Konukoglu, B. H. Menze, A. Venkataraman, C. F. Baumgartner, Q. Dou, & S. Albarqouni
    (Eds.), International Conference on Medical Imaging with Deep Learning, MIDL,
    6-8 July, Zurich, Switzerland (pp. 1523--1537). PMLR volume 172 of Proceedings
    of Machine Learning Research.
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2022) Zhu, X., Hu, H., Wang, H., Yao, J., Li, W., Ou, D., & Xu, D. (2022).
    区域感知 transformer 用于自动乳腺超声肿瘤分割. 在 E. Konukoglu, B. H. Menze, A. Venkataraman, C.
    F. Baumgartner, Q. Dou, & S. Albarqouni (编), 国际医学影像深度学习会议，MIDL, 6-8 七月，苏黎世，瑞士
    (第 1523--1537 页). PMLR 第 172 卷 机器学习研究会议录。
- en: 'Zidan et al. (2023) Zidan, U., Gaber, M. M., & Abdelsamea, M. M. (2023). Swincup:
    Cascaded swin transformer for histopathological structures segmentation in colorectal
    cancer. Expert Syst. Appl., 216, 119452.'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zidan 等人 (2023) Zidan, U., Gaber, M. M., & Abdelsamea, M. M. (2023). Swincup:
    用于结直肠癌组织学结构分割的级联 swin transformer. Expert Syst. Appl., 216, 119452.'
