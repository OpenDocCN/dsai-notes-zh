- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2212.10535] A Survey of Deep Learning for Mathematical Reasoning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2212.10535] 数学推理的深度学习调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2212.10535](https://ar5iv.labs.arxiv.org/html/2212.10535)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2212.10535](https://ar5iv.labs.arxiv.org/html/2212.10535)
- en: A Survey of Deep Learning for Mathematical Reasoning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学推理的深度学习调研
- en: Pan Lu¹, Liang Qiu¹, Wenhao Yu², Sean Welleck^(3∗), Kai-Wei Chang^(1∗)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 潘璐¹、邱亮¹、余文浩²、肖恩·韦勒克^(3∗)、常凯维^(1∗)
- en: ¹UCLA, ²University of Notre Dame, ³University of Washington
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹加州大学洛杉矶分校，²圣母大学，³华盛顿大学
- en: '[https://github.com/lupantech/dl4math](https://github.com/lupantech/dl4math)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/lupantech/dl4math](https://github.com/lupantech/dl4math)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Mathematical reasoning is a fundamental aspect of human intelligence and is
    applicable in various fields, including science, engineering, finance, and everyday
    life. The development of artificial intelligence (AI) systems capable of solving
    math problems and proving theorems in language has garnered significant interest
    in the fields of machine learning and natural language processing. For example,
    mathematics serves as a testbed for aspects of reasoning that are challenging
    for powerful deep learning models, driving new algorithmic and modeling advances.
    On the other hand, recent advances in large-scale neural language models have
    opened up new benchmarks and opportunities to use deep learning for mathematical
    reasoning. In this survey paper, we review the key tasks, datasets, and methods
    at the intersection of mathematical reasoning and deep learning over the past
    decade. We also evaluate existing benchmarks and methods, and discuss future research
    directions in this domain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数学推理是人类智能的一个基本方面，适用于科学、工程、金融和日常生活等多个领域。能够解决数学问题和用语言证明定理的人工智能（AI）系统的发展在机器学习和自然语言处理领域引起了广泛关注。例如，数学作为测试强大深度学习模型在推理方面的挑战的试验场，推动了新的算法和建模进展。另一方面，最近在大规模神经语言模型方面的进展为数学推理的深度学习开辟了新的基准和机会。在这篇调研论文中，我们回顾了过去十年数学推理和深度学习交汇处的关键任务、数据集和方法。我们还评估了现有的基准和方法，并讨论了这一领域未来的研究方向。
- en: '^($*$)^($*$)footnotetext: denotes co-senior authors.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注：表示共同高级作者。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: “The study of mathematics, like the Nile, begins in minuteness but ends in magnificence.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “数学的研究，就像尼罗河一样，开始时微小，但最终却壮丽宏伟。”
- en: — Charles Caleb Colton, English writer
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: — 查尔斯·卡勒布·科尔顿，英国作家
- en: 'Mathematical reasoning is a key aspect of human intelligence that enables us
    to comprehend and make decisions based on numerical data and language. It is applicable
    in various fields, including science, engineering, finance, and everyday life,
    and encompasses a range of abilities, from basic skills such as pattern recognition
    and numerical operations to more advanced skills like problem-solving, logical
    reasoning, and abstract thinking. The development of artificial intelligence (AI)
    systems capable of solving math problems and proving theorems in language has
    been a long-standing focus of research in the fields of machine learning and natural
    language processing (NLP), dating back to the 1960s Feigenbaum et al. ([1963](#bib.bib35));
    Bobrow ([1964](#bib.bib14)). In recent years, there has been a surge of interest
    in this area: for instance, the number of papers has grown from approximately
    10 in 2018 to 66 in 2022 (see [Figure 3](#A1.F3 "Figure 3 ‣ Appendix A Mathematical
    Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning") in
    the Appendix).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数学推理是人类智能的一个关键方面，使我们能够理解和基于数值数据和语言做出决策。它适用于科学、工程、金融和日常生活等多个领域，涵盖了从基本技能如模式识别和数值运算到更高级的技能如问题解决、逻辑推理和抽象思维等各种能力。能够解决数学问题和用语言证明定理的人工智能（AI）系统的发展，一直是机器学习和自然语言处理（NLP）领域的研究重点，这一研究可以追溯到1960年代的Feigenbaum等人（[1963](#bib.bib35)）；Bobrow（[1964](#bib.bib14)）。近年来，这一领域的兴趣激增：例如，相关论文数量从2018年的大约10篇增长到2022年的66篇（参见附录中的[图3](#A1.F3
    "Figure 3 ‣ Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep Learning
    for Mathematical Reasoning)")。
- en: As deep learning continues to revolutionize NLP tasks such as question answering
    and machine translation Sutskever et al. ([2014](#bib.bib168)); Devlin et al.
    ([2019](#bib.bib33)), it has also made significant strides in the field of mathematical
    reasoning Wang et al. ([2017](#bib.bib183)); Yang and Deng ([2019](#bib.bib203));
    Geva et al. ([2020](#bib.bib46)); Wei et al. ([2022](#bib.bib184)). However, despite
    the impressive capabilities of these models, there is still a lack of a clear
    taxonomy of the different types of mathematical reasoning tasks and the specific
    capabilities required of deep learning models to solve them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习不断革新自然语言处理任务，如问答系统和机器翻译 Sutskever 等 ([2014](#bib.bib168)); Devlin 等 ([2019](#bib.bib33))，它在数学推理领域也取得了显著进展 Wang
    等 ([2017](#bib.bib183)); Yang 和 Deng ([2019](#bib.bib203)); Geva 等 ([2020](#bib.bib46));
    Wei 等 ([2022](#bib.bib184))。然而，尽管这些模型具有令人印象深刻的能力，但仍缺乏对不同类型数学推理任务的明确分类，以及深度学习模型解决这些任务所需的具体能力。
- en: Previous literature has been limited to the discussion of specific aspects,
    such as solving math word problems Bhattacharya ([2017](#bib.bib13)); Zhang et al.
    ([2019](#bib.bib209)); Ughade and Kumbhar ([2019](#bib.bib173)), representing
    numbers representation Thawani et al. ([2021](#bib.bib172)), or solving informal
    problems Meadows and Freitas ([2022](#bib.bib119)). Additionally, with the recent
    advancements in large language models like GPT-3 Brown et al. ([2020](#bib.bib15)),
    there is a growing need to understand the capabilities and limitations of these
    models in the context of mathematical reasoning. This is where a comprehensive
    survey of this rapidly advancing domain becomes crucial, as it can provide an
    overview of the current state and limitations of the field, and indicate further
    research areas.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的文献讨论仅限于特定方面，例如解决数学语言问题 Bhattacharya ([2017](#bib.bib13)); Zhang 等 ([2019](#bib.bib209));
    Ughade 和 Kumbhar ([2019](#bib.bib173))，表示数字的表示 Thawani 等 ([2021](#bib.bib172))，或解决非正式问题 Meadows
    和 Freitas ([2022](#bib.bib119))。此外，随着 GPT-3 Brown 等 ([2020](#bib.bib15)) 等大型语言模型的最新进展，理解这些模型在数学推理中的能力和局限性变得越来越重要。因此，对这一快速发展的领域进行全面的调研至关重要，因为它可以提供该领域当前状态和局限性的概述，并指示进一步的研究方向。
- en: '{forest}'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hidden-black, rounded corners,
    align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner
    xsep=2pt, inner ysep=3pt, line width=0.8pt, ver/.style=rotate=90, child anchor=north,
    parent anchor=south, anchor=center, , where level=1text width=6.2em,font=,, where
    level=2text width=10.5em,font=,, where level=3text width=13.5em,font=,, where
    level=4text width=12em,font=,, [ Deep Learning for Mathematical Reasoning, ver
    [ Tasks and
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hidden-black, rounded corners,
    align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner
    xsep=2pt, inner ysep=3pt, line width=0.8pt, ver/.style=rotate=90, child anchor=north,
    parent anchor=south, anchor=center, , where level=1text width=6.2em,font=,, where
    level=2text width=10.5em,font=,, where level=3text width=13.5em,font=,, where
    level=4text width=12em,font=,, [ 深度学习在数学推理中的应用, ver [ 任务和
- en: Datasets (§[2](#S2 "2 Mathematical Reasoning Tasks ‣ A Survey of Deep Learning
    for Mathematical Reasoning")) [ Math Word Problem
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 (§[2](#S2 "2 数学推理任务 ‣ 深度学习在数学推理中的应用调研")) [ 数学语言问题
- en: Solving (§[A.1](#A1.SS1 "A.1 Math Word Problem Solving ‣ Appendix A Mathematical
    Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning")) [
    Textual [ E.g., MathQA Amini et al. ([2019](#bib.bib4)), SVAMP Patel et al. ([2021](#bib.bib134))
    , leaf, text width=32em ] ] [ Multimodal [ E.g., IconQA Lu et al. ([2021b](#bib.bib116)),
    TabMWP Lu et al. ([2022b](#bib.bib115)) , leaf, text width=32em ] ] ] [ Theorem
    Proving (§[A.2](#A1.SS2 "A.2 Theorem Proving ‣ Appendix A Mathematical Reasoning
    Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning")) [ Formal [
    E.g., CoqGym Yang and Deng ([2019](#bib.bib203)) , leaf, text width=32em ] ] [
    Informal [ E.g., NaturalProofs Welleck et al. ([2021](#bib.bib185)) , leaf, text
    width=32em ] ] [ Formal + Informal [ E.g., miniF2F+informal Jiang et al. ([2022a](#bib.bib66))
    , leaf, text width=32em ] ] ] [ Geometry Problem
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 解题 (§[A.1](#A1.SS1 "A.1 数学文字题解决 ‣ 附录 A 数学推理数据集 ‣ 数学推理深度学习概述")) [ 文本 [ 例如，MathQA
    Amini 等人 ([2019](#bib.bib4))，SVAMP Patel 等人 ([2021](#bib.bib134)) ，leaf，文本宽度=32em
    ] ] [ 多模态 [ 例如，IconQA Lu 等人 ([2021b](#bib.bib116))，TabMWP Lu 等人 ([2022b](#bib.bib115))
    ，leaf，文本宽度=32em ] ] ] [ 定理证明 (§[A.2](#A1.SS2 "A.2 定理证明 ‣ 附录 A 数学推理数据集 ‣ 数学推理深度学习概述"))
    [ 形式 [ 例如，CoqGym Yang 和 Deng ([2019](#bib.bib203)) ，leaf，文本宽度=32em ] ] [ 非形式 [
    例如，NaturalProofs Welleck 等人 ([2021](#bib.bib185)) ，leaf，文本宽度=32em ] ] [ 形式 + 非形式
    [ 例如，miniF2F+informal Jiang 等人 ([2022a](#bib.bib66)) ，leaf，文本宽度=32em ] ] ] [ 几何问题
- en: Solving (§[A.3](#A1.SS3 "A.3 Geometry Problem Solving ‣ Appendix A Mathematical
    Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning")) [
    Without Annotations [ E.g., GEOS Seo et al. ([2015](#bib.bib162)), GEOS++ Sachan
    et al. ([2017](#bib.bib157)) , leaf, text width=32em ] ] [ With Annotations [
    E.g., Geometry3K Lu et al. ([2021a](#bib.bib112)), UniGeo Chen et al. ([2022a](#bib.bib19))
    , leaf, text width=32em ] ] ] [ Math Question
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 解题 (§[A.3](#A1.SS3 "A.3 几何问题解决 ‣ 附录 A 数学推理数据集 ‣ 数学推理深度学习概述")) [ 无注释 [ 例如，GEOS
    Seo 等人 ([2015](#bib.bib162))，GEOS++ Sachan 等人 ([2017](#bib.bib157)) ，leaf，文本宽度=32em
    ] ] [ 有注释 [ 例如，Geometry3K Lu 等人 ([2021a](#bib.bib112))，UniGeo Chen 等人 ([2022a](#bib.bib19))
    ，leaf，文本宽度=32em ] ] ] [ 数学问题
- en: Answering (§[A.4](#A1.SS4 "A.4 Math Question Answering ‣ Appendix A Mathematical
    Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning")) [
    Single Benchmark [ E.g., DROP Dua et al. ([2019](#bib.bib34)), Mathematics Saxton
    et al. ([2020](#bib.bib159)) , leaf, text width=32em ] ] [ Unified Benchmark [
    E.g., Lila Mishra et al. ([2022a](#bib.bib125)), TheoremQA Chen et al. ([2023](#bib.bib23))
    , leaf, text width=32em ] ] ] [ Other Quantitative
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回答 (§[A.4](#A1.SS4 "A.4 数学问题回答 ‣ 附录 A 数学推理数据集 ‣ 数学推理深度学习概述")) [ 单一基准 [ 例如，DROP
    Dua 等人 ([2019](#bib.bib34))，Mathematics Saxton 等人 ([2020](#bib.bib159)) ，leaf，文本宽度=32em
    ] ] [ 统一基准 [ 例如，Lila Mishra 等人 ([2022a](#bib.bib125))，TheoremQA Chen 等人 ([2023](#bib.bib23))
    ，leaf，文本宽度=32em ] ] ] [ 其他定量
- en: Problems (§[A.5](#A1.SS5 "A.5 Other Quantitative Problems ‣ Appendix A Mathematical
    Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning")) [
    Diagram [ E.g., FigureQA Kahou et al. ([2018](#bib.bib72)), DVQA Kafle et al.
    ([2018](#bib.bib71)) , leaf, text width=32em ] ] [ Finance, [ E.g., ConvFinQA Chen
    et al. ([2022c](#bib.bib25)) , leaf, text width=32em ] ] [ Science [ E.g., ScienceQA Lu
    et al. ([2022a](#bib.bib113)) , leaf, text width=32em ] ] [ Programming [ E.g., P3 Schuster
    et al. ([2021](#bib.bib160)) , leaf, text width=32em ] ] ] ] [ Deep Learning
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 (§[A.5](#A1.SS5 "A.5 其他定量问题 ‣ 附录 A 数学推理数据集 ‣ 数学推理深度学习概述")) [ 图示 [ 例如，FigureQA
    Kahou 等人 ([2018](#bib.bib72))，DVQA Kafle 等人 ([2018](#bib.bib71)) ，leaf，文本宽度=32em
    ] ] [ 财务 [ 例如，ConvFinQA Chen 等人 ([2022c](#bib.bib25)) ，leaf，文本宽度=32em ] ] [ 科学
    [ 例如，ScienceQA Lu 等人 ([2022a](#bib.bib113)) ，leaf，文本宽度=32em ] ] [ 编程 [ 例如，P3 Schuster
    等人 ([2021](#bib.bib160)) ，leaf，文本宽度=32em ] ] ] ] [ 深度学习
- en: Methods [ Neural Networks (§[3](#S3 "3 Neural Networks for Mathematical Reasoning
    ‣ A Survey of Deep Learning for Mathematical Reasoning")) [ Seq2Seq-based (§[3.1](#S3.SS1
    "3.1 Seq2Seq-based Networks for Math ‣ 3 Neural Networks for Mathematical Reasoning
    ‣ A Survey of Deep Learning for Mathematical Reasoning")) [ E.g., DNS Wang et al.
    ([2017](#bib.bib183)), AnsRat Ling et al. ([2017](#bib.bib105)) , leaf, text width=32em
    ] ] [ Graph-based (§[3.2](#S3.SS2 "3.2 Graph-based Networks for Math ‣ 3 Neural
    Networks for Mathematical Reasoning ‣ A Survey of Deep Learning for Mathematical
    Reasoning")) [ E.g., GTS Xie and Sun ([2019](#bib.bib201)), Graph2Tree Li et al.
    ([2020b](#bib.bib96)) , leaf, text width=32em ] ] [ Attention-based (§[3.3](#S3.SS3
    "3.3 Attention-based Networks for Math ‣ 3 Neural Networks for Mathematical Reasoning
    ‣ A Survey of Deep Learning for Mathematical Reasoning")) [ E.g., Math-EN Wang
    et al. ([2018a](#bib.bib179)), GROUP-ATT Li et al. ([2019](#bib.bib93)) , leaf,
    text width=32em ] ] [ Other (§[3.4](#S3.SS4 "3.4 Other Neural Networks for Math
    ‣ 3 Neural Networks for Mathematical Reasoning ‣ A Survey of Deep Learning for
    Mathematical Reasoning")) [ E.g., CNNTP Loos et al. ([2017](#bib.bib111)), MathDQN Wang
    et al. ([2018b](#bib.bib180)) , leaf, text width=32em ] ] ] [ Pre-trained Language
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 [ 神经网络 (§[3](#S3 "3 神经网络在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 基于Seq2Seq (§[3.1](#S3.SS1
    "3.1 基于Seq2Seq的网络 ‣ 3 神经网络在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，DNS Wang等人 ([2017](#bib.bib183)),
    AnsRat Ling等人 ([2017](#bib.bib105)) , leaf, text width=32em ] ] [ 基于图的 (§[3.2](#S3.SS2
    "3.2 基于图的网络 ‣ 3 神经网络在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，GTS Xie和Sun ([2019](#bib.bib201)),
    Graph2Tree Li等人 ([2020b](#bib.bib96)) , leaf, text width=32em ] ] [ 基于注意力的 (§[3.3](#S3.SS3
    "3.3 基于注意力的网络 ‣ 3 神经网络在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，Math-EN Wang等人 ([2018a](#bib.bib179)),
    GROUP-ATT Li等人 ([2019](#bib.bib93)) , leaf, text width=32em ] ] [ 其他 (§[3.4](#S3.SS4
    "3.4 其他数学网络 ‣ 3 神经网络在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，CNNTP Loos等人 ([2017](#bib.bib111)),
    MathDQN Wang等人 ([2018b](#bib.bib180)) , leaf, text width=32em ] ] ] [ 预训练语言
- en: Models (§[4](#S4 "4 Pre-trained Language Models for Mathematical Reasoning ‣
    A Survey of Deep Learning for Mathematical Reasoning")) [ Self-Supervised Learning
    (§[4.1](#S4.SS1 "4.1 Self-Supervised Learning for Math ‣ 4 Pre-trained Language
    Models for Mathematical Reasoning ‣ A Survey of Deep Learning for Mathematical
    Reasoning")) [ E.g., GenBERT Geva et al. ([2020](#bib.bib46)), Minerva Lewkowycz
    et al. ([2022](#bib.bib92)) , leaf, text width=32em ] ] [ Task-specific Fine-tuning
    (§[4.2](#S4.SS2 "4.2 Task-specific Fine-tuning for Math ‣ 4 Pre-trained Language
    Models for Mathematical Reasoning ‣ A Survey of Deep Learning for Mathematical
    Reasoning")) [ E.g., Scratchpad Nye et al. ([2021](#bib.bib132)), Bhaskara Mishra
    et al. ([2022a](#bib.bib125)) , leaf, text width=32em ] ] ] [ In-context Learning
    (§[5](#S5 "5 In-context Learning for Mathematical Reasoning ‣ A Survey of Deep
    Learning for Mathematical Reasoning")) [ Example Selection (§[5.1](#S5.SS1 "5.1
    In-context Example Selection ‣ 5 In-context Learning for Mathematical Reasoning
    ‣ A Survey of Deep Learning for Mathematical Reasoning")) [ E.g., Few-shot-CoT Wei
    et al. ([2022](#bib.bib184)), PromptPG Lu et al. ([2022b](#bib.bib115)) , leaf,
    text width=32em ] ] [ High-quality Chains (§[5.2](#S5.SS2 "5.2 High-quality Reasoning
    Chains ‣ 5 In-context Learning for Mathematical Reasoning ‣ A Survey of Deep Learning
    for Mathematical Reasoning")) [ E.g., Self-Consistency Wang et al. ([2023](#bib.bib182)),
    Least-to-most Zhou et al. ([2023](#bib.bib224)) , leaf, text width=32em ] ] ]
    ] ]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 (§[4](#S4 "4 预训练语言模型在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 自监督学习 (§[4.1](#S4.SS1
    "4.1 自监督学习 ‣ 4 预训练语言模型在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，GenBERT Geva等人 ([2020](#bib.bib46)),
    Minerva Lewkowycz等人 ([2022](#bib.bib92)) , leaf, text width=32em ] ] [ 任务特定微调
    (§[4.2](#S4.SS2 "4.2 任务特定微调 ‣ 4 预训练语言模型在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，Scratchpad
    Nye等人 ([2021](#bib.bib132)), Bhaskara Mishra等人 ([2022a](#bib.bib125)) , leaf,
    text width=32em ] ] ] [ 上下文学习 (§[5](#S5 "5 上下文学习在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [
    示例选择 (§[5.1](#S5.SS1 "5.1 上下文示例选择 ‣ 5 上下文学习在数学推理中的应用 ‣ 深度学习在数学推理中的调查")) [ 例如，Few-shot-CoT
    Wei等人 ([2022](#bib.bib184)), PromptPG Lu等人 ([2022b](#bib.bib115)) , leaf, text
    width=32em ] ] [ 高质量链 (§[5.2](#S5.SS2 "5.2 高质量推理链 ‣ 5 上下文学习在数学推理中的应用 ‣ 深度学习在数学推理中的调查"))
    [ 例如，Self-Consistency Wang等人 ([2023](#bib.bib182)), Least-to-most Zhou等人 ([2023](#bib.bib224))
    , leaf, text width=32em ] ] ] ]
- en: 'Figure 1: Taxonomy of deep learning for mathematical reasoning. The associated
    tasks are elaborated in §[2](#S2 "2 Mathematical Reasoning Tasks ‣ A Survey of
    Deep Learning for Mathematical Reasoning"), with a comprehensive dataset list
    found in §[A](#A1 "Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep
    Learning for Mathematical Reasoning"). Deep learning methods are further discussed
    in §[3](#S3 "3 Neural Networks for Mathematical Reasoning ‣ A Survey of Deep Learning
    for Mathematical Reasoning"), §[4](#S4 "4 Pre-trained Language Models for Mathematical
    Reasoning ‣ A Survey of Deep Learning for Mathematical Reasoning"), and §[5](#S5
    "5 In-context Learning for Mathematical Reasoning ‣ A Survey of Deep Learning
    for Mathematical Reasoning").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：数学推理深度学习的分类法。相关任务详述于§[2](#S2 "2 数学推理任务 ‣ 深度学习在数学推理中的调查")，综合数据集列表见于§[A](#A1
    "附录 A 数学推理数据集 ‣ 深度学习在数学推理中的调查")。深度学习方法在§[3](#S3 "3 数学推理的神经网络 ‣ 深度学习在数学推理中的调查")，§[4](#S4
    "4 预训练语言模型在数学推理中的应用 ‣ 深度学习在数学推理中的调查")和§[5](#S5 "5 上下文学习在数学推理中的应用 ‣ 深度学习在数学推理中的调查")中进一步讨论。
- en: In this paper, we survey over 180 papers from the NLP and AI communities in
    the field of deep learning for mathematical reasoning. We study various types
    of mathematical reasoning problems, such as math word problems, theorem proving,
    geometry problem solving, math question answering, and other quantitative problems
    (§[2](#S2 "2 Mathematical Reasoning Tasks ‣ A Survey of Deep Learning for Mathematical
    Reasoning"), §[A](#A1 "Appendix A Mathematical Reasoning Datasets ‣ A Survey of
    Deep Learning for Mathematical Reasoning")). Additionally, we explore different
    deep learning architectures for mathematical reasoning, including neural networks
    (§[3](#S3 "3 Neural Networks for Mathematical Reasoning ‣ A Survey of Deep Learning
    for Mathematical Reasoning")), pre-trained language models (§[4](#S4 "4 Pre-trained
    Language Models for Mathematical Reasoning ‣ A Survey of Deep Learning for Mathematical
    Reasoning")), and recent in-context learning for large language models (§[5](#S5
    "5 In-context Learning for Mathematical Reasoning ‣ A Survey of Deep Learning
    for Mathematical Reasoning")).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们调查了来自NLP和AI领域的180多篇关于数学推理深度学习的论文。我们研究了各种类型的数学推理问题，例如数学词汇题、定理证明、几何问题解决、数学问答以及其他定量问题（§[2](#S2
    "2 数学推理任务 ‣ 深度学习在数学推理中的调查")，§[A](#A1 "附录 A 数学推理数据集 ‣ 深度学习在数学推理中的调查")）。此外，我们探讨了用于数学推理的不同深度学习架构，包括神经网络（§[3](#S3
    "3 数学推理的神经网络 ‣ 深度学习在数学推理中的调查")），预训练语言模型（§[4](#S4 "4 预训练语言模型在数学推理中的应用 ‣ 深度学习在数学推理中的调查")）以及最近的大型语言模型的上下文学习（§[5](#S5
    "5 上下文学习在数学推理中的应用 ‣ 深度学习在数学推理中的调查")）。
- en: We also analyze existing benchmarks and find that there is less focus on multi-modal
    and low-resource settings (§[6.1](#S6.SS1 "6.1 Analysis of Benchmarks ‣ 6 Discussion
    and Findings ‣ A Survey of Deep Learning for Mathematical Reasoning")). Our evidence-based
    studies suggest that current numeracy representations are insufficient and deep
    learning methods are inconsistent for mathematical reasoning (§[6.2](#S6.SS2 "6.2
    Analysis of Deep Learning Methods ‣ 6 Discussion and Findings ‣ A Survey of Deep
    Learning for Mathematical Reasoning")). Following this, we suggest future research
    directions related to generalization and robustness, trustworthy reasoning, learning
    from feedback, and multi-modal mathematical reasoning (§[7](#S7 "7 Future Work
    ‣ A Survey of Deep Learning for Mathematical Reasoning")).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了现有的基准测试，发现对多模态和低资源环境的关注较少（§[6.1](#S6.SS1 "6.1 分析基准 ‣ 6 讨论与发现 ‣ 深度学习在数学推理中的调查")）。我们的基于证据的研究表明，当前的数字表示不够充分，深度学习方法在数学推理中也不一致（§[6.2](#S6.SS2
    "6.2 深度学习方法分析 ‣ 6 讨论与发现 ‣ 深度学习在数学推理中的调查")）。因此，我们建议未来的研究方向包括泛化和鲁棒性、可信推理、从反馈中学习以及多模态数学推理（§[7](#S7
    "7 未来工作 ‣ 深度学习在数学推理中的调查")）。
- en: 2 Mathematical Reasoning Tasks
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数学推理任务
- en: In this section, we briefly introduce different tasks for mathematical reasoning.
    A detailed summary and discussion of commonly used datasets can be found in [Table 7](#A1.T7
    "Table 7 ‣ A.5 Other Quantitative Problems ‣ Appendix A Mathematical Reasoning
    Datasets ‣ A Survey of Deep Learning for Mathematical Reasoning") and Appendix
    [A](#A1 "Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep Learning
    for Mathematical Reasoning").
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了不同的数学推理任务。常用数据集的详细总结和讨论可在[表 7](#A1.T7 "Table 7 ‣ A.5 Other Quantitative
    Problems ‣ Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep Learning
    for Mathematical Reasoning")和附录[A](#A1 "Appendix A Mathematical Reasoning Datasets
    ‣ A Survey of Deep Learning for Mathematical Reasoning")中找到。
- en: Math Word Problem Solving. Developing algorithms to automatically solve math
    word problems (MWPs) has been of interest to NLP researchers for decades Feigenbaum
    et al. ([1963](#bib.bib35)); Bobrow ([1964](#bib.bib14)). An example of a MWP
    is shown in [Table 1](#S2.T1 "Table 1 ‣ 2 Mathematical Reasoning Tasks ‣ A Survey
    of Deep Learning for Mathematical Reasoning"). A question involves four basic
    arithmetic operations with single or multiple operation steps. The challenge posed
    by MWPs lies in the need for language comprehension, semantic parsing, and the
    application of multiple mathematical reasoning skills.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数学应用题求解。开发自动解决数学应用题（MWPs）的算法已引起 NLP 研究人员的关注数十年 Feigenbaum 等人 ([1963](#bib.bib35));
    Bobrow ([1964](#bib.bib14))。如[表 1](#S2.T1 "Table 1 ‣ 2 Mathematical Reasoning
    Tasks ‣ A Survey of Deep Learning for Mathematical Reasoning")所示，一个应用题涉及四种基本的算术运算，可能有单步或多步操作。应用题的挑战在于需要语言理解、语义解析以及多种数学推理技能的应用。
- en: '| Question: Bod has 2 apples and David has 5 apples. How many apples do they
    have in total? |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 问题：Bod 有 2 个苹果，David 有 5 个苹果。他们总共有多少个苹果？ |'
- en: '| Rationale: $x=2+5$ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 理由：$x=2+5$ |'
- en: '| Solution: $7$ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 解决方案：$7$ |'
- en: 'Table 1: A typical math word problem.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：典型的数学应用题。
- en: Theorem Proving. Automating theorem proving is a long-standing challenge in
    AI Newell et al. ([1957](#bib.bib130)); Feigenbaum et al. ([1963](#bib.bib35)).
    The problem is to demonstrate the truth of a mathematical claim (a theorem) through
    a sequence of logical arguments (a proof). Theorem proving tests various skills,
    such as choosing effective multi-step strategies, using background knowledge,
    and performing symbolic manipulations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 定理证明。自动化定理证明是 AI 中一个长期存在的挑战 Newell 等人 ([1957](#bib.bib130)); Feigenbaum 等人 ([1963](#bib.bib35))。问题在于通过一系列逻辑论证（证明）来证明一个数学声明（定理）的真实性。定理证明考验各种技能，例如选择有效的多步骤策略、使用背景知识和进行符号操作。
- en: Geometry Problem Solving. Automated geometry problem solving (GPS) is also a
    long-standing mathematical reasoning task Gelernter et al. ([1960](#bib.bib45));
    Wen-Tsun ([1986](#bib.bib189)). As shown in [Figure 2](#S2.F2 "Figure 2 ‣ 2 Mathematical
    Reasoning Tasks ‣ A Survey of Deep Learning for Mathematical Reasoning"), a geometry
    problem consists of a textual description and a diagram. The multimodal inputs
    describe the entities, attributes, and relationships of geometric elements, and
    the goal is to find the numeric solution to an unknown variable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 几何问题求解。自动化几何问题求解（GPS）也是一个长期存在的数学推理任务 Gelernter 等人 ([1960](#bib.bib45)); Wen-Tsun
    ([1986](#bib.bib189))。如[图 2](#S2.F2 "Figure 2 ‣ 2 Mathematical Reasoning Tasks
    ‣ A Survey of Deep Learning for Mathematical Reasoning")所示，几何问题包括文本描述和图表。多模态输入描述几何元素的实体、属性和关系，目标是找到未知变量的数值解。
- en: '![Refer to caption](img/cddafec82f375b87429e154f1b320e98.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cddafec82f375b87429e154f1b320e98.png)'
- en: 'Figure 2: An example of geometry problems.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：几何问题的示例。
- en: Math Question Answering. There is a wide range of question answering (QA) benchmarks
    that center around mathematical reasoning, which we refer to as math question
    answering (MathQA). For example, DROP Dua et al. ([2019](#bib.bib34)) is a MathQA
    dataset that requires discrete reasoning to answer questions such as “Which kicker
    kicked the most field goals?” over the content of paragraphs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数学问题回答。存在广泛的数学推理相关问题回答（QA）基准，我们称之为数学问题回答（MathQA）。例如，DROP Dua 等人 ([2019](#bib.bib34))
    是一个 MathQA 数据集，要求进行离散推理以回答诸如“哪位踢球员踢进了最多的进球？”这样的问题。
- en: 3 Neural Networks for Mathematical Reasoning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数学推理的神经网络
- en: Neural networks have become a popular tool in the field of mathematical reasoning,
    mirroring their success in NLP. In recent years, a number of different neural
    network architectures have been proposed for mathematical reasoning tasks, including
    Seq2Seq-based networks, graph-based networks, and attention-based networks. These
    methods are outlined in more detail in [Table 8](#A1.T8 "Table 8 ‣ A.5 Other Quantitative
    Problems ‣ Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep Learning
    for Mathematical Reasoning") in the Appendix.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已成为数学推理领域中的一种流行工具，与其在自然语言处理中的成功相呼应。近年来，提出了多种不同的神经网络架构用于数学推理任务，包括基于 Seq2Seq
    的网络、基于图的网络和基于注意力的网络。这些方法在附录中的[表 8](#A1.T8 "Table 8 ‣ A.5 Other Quantitative Problems
    ‣ Appendix A Mathematical Reasoning Datasets ‣ A Survey of Deep Learning for Mathematical
    Reasoning")中有更详细的介绍。
- en: 3.1 Seq2Seq-based Networks for Math
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于 Seq2Seq 的数学网络
- en: 'Sequence-to-sequence (Seq2Seq) Sutskever et al. ([2014](#bib.bib168)) neural
    networks have been successfully applied to mathematical reasoning tasks, such
    as math word problem solving Wang et al. ([2017](#bib.bib183)), theorem proving
     Yang and Deng ([2019](#bib.bib203)), geometry problem solving Robaidek et al.
    ([2018](#bib.bib151)), and math question answering Tafjord et al. ([2019](#bib.bib169)).
    A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical
    reasoning as a sequence generation task. The basic idea behind this approach is
    to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g.
    an equation, program, and proof). Common encoders and decoders include Long Short
    Term Memory network (LSTM) Hochreiter and Schmidhuber ([1997](#bib.bib58)), Gated
    Recurrent Unit (GRU) Cho et al. ([2014](#bib.bib28)), and their bidirectional
    variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage
    of Seq2Seq models over previous statistical learning approaches Ling et al. ([2017](#bib.bib105));
    Wang et al. ([2018a](#bib.bib179)); Huang et al. ([2018](#bib.bib63)); Wang et al.
    ([2019](#bib.bib181)); Li et al. ([2019](#bib.bib93)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列（Seq2Seq）Sutskever 等人 ([2014](#bib.bib168)) 的神经网络已成功应用于数学推理任务，例如数学文字题解决
    Wang 等人 ([2017](#bib.bib183))、定理证明 Yang 和 Deng ([2019](#bib.bib203))、几何问题解决 Robaidek
    等人 ([2018](#bib.bib151)) 和数学问答 Tafjord 等人 ([2019](#bib.bib169))。Seq2Seq 模型使用编码器-解码器架构，通常将数学推理形式化为序列生成任务。这种方法的基本思想是将输入序列（例如数学问题）映射到输出序列（例如方程、程序和证明）。常见的编码器和解码器包括长短期记忆网络（LSTM）Hochreiter
    和 Schmidhuber ([1997](#bib.bib58))、门控递归单元（GRU）Cho 等人 ([2014](#bib.bib28)) 及其双向变体：BiLSTM
    和 BiGRU。大量工作已显示 Seq2Seq 模型相较于之前的统计学习方法具有性能优势 Ling 等人 ([2017](#bib.bib105))；Wang
    等人 ([2018a](#bib.bib179))；Huang 等人 ([2018](#bib.bib63))；Wang 等人 ([2019](#bib.bib181))；Li
    等人 ([2019](#bib.bib93))。
- en: 3.2 Graph-based Networks for Math
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于图的数学网络
- en: Seq2Seq approaches show their advantages of generating mathematical expressions
    without relying on hand-crafted features. It is noteworthy that mathematical expressions
    can be represented as tree-based structures, such as abstract syntax trees (ASTs)
    and graph-based structures, which capture the structural information in the expressions.
    However, Seq2Seq methods do not explicitly this important information. To address
    this limitation, graph-based neural networks have been developed to explicitly
    model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly
    model the tree structure when encoding the output sequences Xie and Sun ([2019](#bib.bib201));
    Wu et al. ([2020](#bib.bib192)); Zaporojets et al. ([2021](#bib.bib208)); Qin
    et al. ([2021](#bib.bib139)). For example,  Liu et al. ([2019a](#bib.bib109))
    devise a Seq2Tree model to better use information from an equation’s AST. Seq2DAG
    Cao et al. ([2021](#bib.bib17)), instead, applies a sequence-to-graph (Seq2Graph)
    framework when generating the equations since the graph decoder is able to extract
    complex relationships among multiple variables. The graph-based information can
    also be embedded when encoding the input mathematical sequences Zhang et al. ([2020b](#bib.bib211));
    Shen and Jin ([2020](#bib.bib164)); Li et al. ([2020b](#bib.bib96)); Wu et al.
    ([2021a](#bib.bib193)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 方法展示了其在生成数学表达式时不依赖手工特征的优势。值得注意的是，数学表达式可以表示为基于树的结构，例如抽象语法树（ASTs）和基于图的结构，这些结构捕捉了表达式中的结构信息。然而，Seq2Seq
    方法并未明确建模这些重要信息。为了克服这一局限性，已经开发了基于图的神经网络以显式建模表达式中的结构。序列到树（Seq2Tree）模型在编码输出序列时明确建模树结构
    Xie 和 Sun ([2019](#bib.bib201)); Wu 等 ([2020](#bib.bib192)); Zaporojets 等 ([2021](#bib.bib208));
    Qin 等 ([2021](#bib.bib139))。例如，Liu 等 ([2019a](#bib.bib109)) 设计了一个 Seq2Tree 模型，以更好地利用方程的
    AST 信息。Seq2DAG Cao 等 ([2021](#bib.bib17))，则在生成方程时应用了序列到图（Seq2Graph）框架，因为图解码器能够提取多个变量之间的复杂关系。图形信息也可以在编码输入数学序列时嵌入
    Zhang 等 ([2020b](#bib.bib211)); Shen 和 Jin ([2020](#bib.bib164)); Li 等 ([2020b](#bib.bib96));
    Wu 等 ([2021a](#bib.bib193))。
- en: 3.3 Attention-based Networks for Math
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于注意力的数学网络
- en: The attention mechanism has been successfully applied to NLP Bahdanau et al.
    ([2015](#bib.bib9)) and vision problems Xu et al. ([2015](#bib.bib202)); Woo et al.
    ([2018](#bib.bib191)), taking into account the hidden vectors of the inputs during
    the decoding processing. Recently, researchers have been exploring its usefulness
    in mathematical reasoning tasks, as it can be used to identify the most important
    relationships between mathematical concepts. For instance, MATH-EN Wang et al.
    ([2018a](#bib.bib179)) is a math word problem solver which benefits from long-distance
    dependency information learned by self-attention. Attention-based methods have
    also been applied to other mathematical reasoning tasks such as geometry problems
    solving Robaidek et al. ([2018](#bib.bib151)); Chen et al. ([2021a](#bib.bib20))
    and theorem proving Yang and Deng ([2019](#bib.bib203)). Various attention mechanisms
    have been studied to extract better representations, such as Group-ATT Li et al.
    ([2019](#bib.bib93)) which uses different multi-head attention to extract various
    types of MWP features, and graph attention which is applied to extract knowledge-aware
    information in Wu et al. ([2020](#bib.bib192)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制已经成功应用于自然语言处理 Bahdanau 等 ([2015](#bib.bib9)) 和视觉问题 Xu 等 ([2015](#bib.bib202));
    Woo 等 ([2018](#bib.bib191))，在解码过程中考虑了输入的隐藏向量。最近，研究人员一直在探索其在数学推理任务中的有用性，因为它可以用来识别数学概念之间最重要的关系。例如，MATH-EN
    Wang 等 ([2018a](#bib.bib179)) 是一个数学词问题求解器，它受益于通过自注意力学习的长距离依赖信息。基于注意力的方法也被应用于其他数学推理任务，例如几何问题求解
    Robaidek 等 ([2018](#bib.bib151)); Chen 等 ([2021a](#bib.bib20)) 和定理证明 Yang 和 Deng
    ([2019](#bib.bib203))。各种注意力机制已经被研究以提取更好的表示，例如 Group-ATT Li 等 ([2019](#bib.bib93))，它使用不同的多头注意力来提取各种类型的
    MWP 特征，以及应用于 Wu 等 ([2020](#bib.bib192)) 的图注意力，用于提取知识感知信息。
- en: 3.4 Other Neural Networks for Math
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 其他用于数学的神经网络
- en: Deep learning approaches to mathematical reasoning tasks can also make use of
    other neural networks, such as convolutional neural networks (CNN) and multimodal
    networks. Some work encodes the input text using a convolutional neural network
    architecture, giving the model the ability to capture long-term relationships
    between symbols in the input Gehring et al. ([2017](#bib.bib44)); Wang et al.
    ([2018a](#bib.bib179), [a](#bib.bib179)); Robaidek et al. ([2018](#bib.bib151));
    Alemi et al. ([2016](#bib.bib1)); Loos et al. ([2017](#bib.bib111)). For example,
    the first application of deep neural networks for theorem proving is proposed
    in Alemi et al. ([2016](#bib.bib1)), which relies on convolutional networks for
    premise selection.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法在数学推理任务中也可以利用其他神经网络，如卷积神经网络（CNN）和多模态网络。一些研究通过卷积神经网络架构对输入文本进行编码，使模型能够捕捉输入中符号之间的长期关系
    Gehring et al. ([2017](#bib.bib44)); Wang et al. ([2018a](#bib.bib179), [a](#bib.bib179));
    Robaidek et al. ([2018](#bib.bib151)); Alemi et al. ([2016](#bib.bib1)); Loos
    et al. ([2017](#bib.bib111))。例如，Alemi et al. ([2016](#bib.bib1)) 提出的第一个深度神经网络应用于定理证明，依赖于卷积网络进行前提选择。
- en: Multimodal mathematical reasoning tasks, such as geometry problem solving and
    diagram-based mathematical reasoning, are formalized as visual question answer
    (VQA) problems Kafle et al. ([2018](#bib.bib71)); Chen et al. ([2021a](#bib.bib20));
    Lu et al. ([2021b](#bib.bib116)). In this domain, visual inputs are encoded using
    ResNet He et al. ([2016](#bib.bib53)) or Faster-RCNN Ren et al. ([2015](#bib.bib149)),
    while textual representations are obtained via GRU or LTSM. Subsequently, the
    joint representation is learned using multimodal fusion models, such as BAN Kim
    et al. ([2018](#bib.bib79)), FiLM Perez et al. ([2018](#bib.bib136)), and DAFA
    Gao et al. ([2019](#bib.bib42)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数学推理任务，如几何问题解决和基于图表的数学推理，被形式化为视觉问答（VQA）问题 Kafle et al. ([2018](#bib.bib71));
    Chen et al. ([2021a](#bib.bib20)); Lu et al. ([2021b](#bib.bib116))。在这一领域，视觉输入使用
    ResNet He et al. ([2016](#bib.bib53)) 或 Faster-RCNN Ren et al. ([2015](#bib.bib149))
    编码，而文本表示则通过 GRU 或 LTSM 获得。随后，使用多模态融合模型学习联合表示，如 BAN Kim et al. ([2018](#bib.bib79)),
    FiLM Perez et al. ([2018](#bib.bib136)) 和 DAFA Gao et al. ([2019](#bib.bib42))。
- en: Other deep neural network structures can also be used in mathematical reasoning.
    A Graph Neural Network (GNN) is employed for geometry problem parsing in Zhang
    et al. ([2022](#bib.bib212)), taking advantage of its success in spatial reasoning.
    WaveNet has been applied to theorem proving Loos et al. ([2017](#bib.bib111));
    Bansal et al. ([2019](#bib.bib10)), due to its ability to address longitudinal
    time-series data. Furthermore, Transformers are found to outperform GRU in generating
    mathematical equations in DDT Meng and Rumshisky ([2019](#bib.bib121)). Finally,
    MathDQN Wang et al. ([2018b](#bib.bib180)) is the first work to explore reinforcement
    learning for math word problem solving, taking advantage of its strong search
    capabilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其他深度神经网络结构也可以用于数学推理。Zhang et al. ([2022](#bib.bib212)) 使用图神经网络（GNN）进行几何问题解析，利用其在空间推理中的成功。WaveNet
    被应用于定理证明 Loos et al. ([2017](#bib.bib111)); Bansal et al. ([2019](#bib.bib10))，由于其处理纵向时间序列数据的能力。此外，Transformers
    在生成数学方程式方面优于 GRU DDT Meng 和 Rumshisky ([2019](#bib.bib121))。最后，MathDQN Wang et
    al. ([2018b](#bib.bib180)) 是首个探索强化学习用于数学文字问题解决的工作，利用其强大的搜索能力。
- en: 4 Pre-trained Language Models for Mathematical Reasoning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种用于数学推理的预训练语言模型
- en: '| Paper | Backbone | Size | Corpus | Pre-training task |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 基础架构 | 大小 | 语料库 | 预训练任务 |'
- en: '| GPT-f Polu and Sutskever ([2020](#bib.bib138)) | Transformer ([2017](#bib.bib177))
    | 774M | Math | Causal language modeling |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| GPT-f Polu 和 Sutskever ([2020](#bib.bib138)) | Transformer ([2017](#bib.bib177))
    | 774M | 数学 | 因果语言建模 |'
- en: '| LISA Jiang et al. ([2021](#bib.bib67)) | Transformer ([2017](#bib.bib177))
    | 163M | Math | Causal language modeling |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| LISA Jiang et al. ([2021](#bib.bib67)) | Transformer ([2017](#bib.bib177))
    | 163M | 数学 | 因果语言建模 |'
- en: '| MATH-PLM Hendrycks et al. ([2021b](#bib.bib55)) | GPT-2 ([2020](#bib.bib144))
    | 1.5B | Math | Causal language modeling |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| MATH-PLM Hendrycks et al. ([2021b](#bib.bib55)) | GPT-2 ([2020](#bib.bib144))
    | 1.5B | 数学 | 因果语言建模 |'
- en: '| MWP-BERT Liang et al. ([2022b](#bib.bib102)) | RoBERTa ([2019b](#bib.bib110))
    | 123M | Math | 8 numeracy augmented tasks |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| MWP-BERT Liang et al. ([2022b](#bib.bib102)) | RoBERTa ([2019b](#bib.bib110))
    | 123M | 数学 | 8 个算术增强任务 |'
- en: '| TaPEx Liu et al. ([2022b](#bib.bib107)) | BART ([2020](#bib.bib91)) | 406M
    | SQL | Query result generation |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| TaPEx Liu et al. ([2022b](#bib.bib107)) | BART ([2020](#bib.bib91)) | 406M
    | SQL | 查询结果生成 |'
- en: '| HTPS Lample et al. ([2022](#bib.bib87)) | Transformer ([2017](#bib.bib177))
    | 600M | Math | Masked Seq2Seq modeling |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| HTPS Lample 等 ([2022](#bib.bib87)) | Transformer ([2017](#bib.bib177)) |
    600M | 数学 | 掩码 Seq2Seq 建模 |'
- en: '| Thor Jiang et al. ([2022b](#bib.bib68)) | Transformer ([2017](#bib.bib177))
    | 700M | Github, arXiv | Causal language modeling |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Thor Jiang 等 ([2022b](#bib.bib68)) | Transformer ([2017](#bib.bib177)) |
    700M | Github, arXiv | 因果语言建模 |'
- en: '| PACT Han et al. ([2022](#bib.bib51)) | Transformer ([2017](#bib.bib177))
    | 837M | Math | Masked/Causal language modeling |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| PACT Han 等 ([2022](#bib.bib51)) | Transformer ([2017](#bib.bib177)) | 837M
    | 数学 | 掩码/因果语言建模 |'
- en: '| Minerva Lewkowycz et al. ([2022](#bib.bib92)) | PaLM ([2022](#bib.bib30))
    | 540B | Science & Math | Causal language modeling |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Minerva Lewkowycz 等 ([2022](#bib.bib92)) | PaLM ([2022](#bib.bib30)) | 540B
    | 科学与数学 | 因果语言建模 |'
- en: '| GenBERT Geva et al. ([2020](#bib.bib46)) | BERT ([2019](#bib.bib33)) | 110M
    | Number, Text | Masked/Causal language modeling |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GenBERT Geva 等 ([2020](#bib.bib46)) | BERT ([2019](#bib.bib33)) | 110M |
    数字, 文本 | 掩码/因果语言建模 |'
- en: '| NF-NSM Feng et al. ([2021](#bib.bib36)) | RoBERTa ([2019b](#bib.bib110))
    | 110M | Number | Number prediction |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| NF-NSM Feng 等 ([2021](#bib.bib36)) | RoBERTa ([2019b](#bib.bib110)) | 110M
    | 数字 | 数字预测 |'
- en: '| LIME Wu et al. ([2021d](#bib.bib200)) | Transformer ([2017](#bib.bib177))
    | 11B | Math | Causal language modeling |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LIME Wu 等 ([2021d](#bib.bib200)) | Transformer ([2017](#bib.bib177)) | 11B
    | 数学 | 因果语言建模 |'
- en: '| Set Wu et al. ([2022c](#bib.bib199)) | T5 ([2020](#bib.bib146)) | 60M | Math
    | Unique token generation |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Set Wu 等 ([2022c](#bib.bib199)) | T5 ([2020](#bib.bib146)) | 60M | 数学 | 独特的标记生成
    |'
- en: 'Table 2: Comparison of pre-training language models for mathematical reasoning.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 数学推理的预训练语言模型比较。'
- en: Pre-trained language models Devlin et al. ([2019](#bib.bib33)); Radford et al.
    ([2020](#bib.bib144)); Brown et al. ([2020](#bib.bib15)) have demonstrated remarkable
    performance gains on a wide range of NLP tasks. By pre-training on a large corpus
    of text, the models learn valuable world knowledge Guu et al. ([2020](#bib.bib50)),
    which could be applied to downstream tasks. Similar ideas can be applied to math-related
    problems, and previous work has shown the promising performance of pre-trained
    language models in answering math word problems Kim et al. ([2020](#bib.bib78)),
    assisting with theorem proving Wu et al. ([2022b](#bib.bib198)), as well as solving
    other mathematical tasks Charton ([2022](#bib.bib18)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型 Devlin 等 ([2019](#bib.bib33)); Radford 等 ([2020](#bib.bib144)); Brown
    等 ([2020](#bib.bib15)) 在广泛的 NLP 任务中展示了显著的性能提升。通过在大规模文本语料库上进行预训练，这些模型学习了宝贵的世界知识
    Guu 等 ([2020](#bib.bib50))，这些知识可以应用于下游任务。类似的思想也可以应用于与数学相关的问题，之前的研究已展示了预训练语言模型在回答数学应用题
    Kim 等 ([2020](#bib.bib78))、辅助定理证明 Wu 等 ([2022b](#bib.bib198)) 以及解决其他数学任务 Charton
    ([2022](#bib.bib18)) 中的良好表现。
- en: However, though large language models excel in modeling natural language, there
    are several challenges to using them for mathematical reasoning. First, pre-trained
    language models are not specifically trained on mathematical data. This likely
    contributes to them being less proficient in math-related tasks compared to natural
    language tasks. There is also less mathematical or scientific data available for
    large-scale pre-training compared to text data. Second, the size of pre-trained
    models continues to grow, making it expensive to train the entire model from scratch
    for specific downstream tasks. Additionally, downstream tasks may deal with different
    input formats or modalities, such as structured tables Zhao et al. ([2022](#bib.bib220))
    or diagrams Lu et al. ([2021b](#bib.bib116)). To address these challenges, researchers
    have to adjust pre-trained models by finetuning them on downstream tasks or adapting
    the neural architectures.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管大型语言模型在建模自然语言方面表现出色，但在数学推理中使用它们仍面临一些挑战。首先，预训练语言模型没有专门针对数学数据进行训练，这可能导致它们在数学相关任务中表现不如自然语言任务。此外，与文本数据相比，用于大规模预训练的数学或科学数据较少。其次，预训练模型的规模不断扩大，使得从头开始训练整个模型以应对特定下游任务变得昂贵。此外，下游任务可能涉及不同的输入格式或模态，例如结构化表格
    Zhao 等 ([2022](#bib.bib220)) 或图表 Lu 等 ([2021b](#bib.bib116))。为了解决这些挑战，研究人员必须通过在下游任务上微调预训练模型或调整神经网络结构来适应这些挑战。
- en: 4.1 Self-Supervised Learning for Math
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 自监督学习在数学中的应用
- en: Self-supervised learning is a machine learning approach in which an algorithm
    learns to perform a task without being explicitly provided with labeled training
    data. [Table 2](#S4.T2 "Table 2 ‣ 4 Pre-trained Language Models for Mathematical
    Reasoning ‣ A Survey of Deep Learning for Mathematical Reasoning") provides a
    list of language models pre-trained with self-supervised tasks for mathematical
    reasoning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是一种机器学习方法，其中算法在没有明确提供标记训练数据的情况下学习执行任务。[表 2](#S4.T2 "表 2 ‣ 4 预训练语言模型用于数学推理
    ‣ 深度学习在数学推理中的调查") 提供了一些用于数学推理的自监督任务预训练语言模型的列表。
- en: 'Model scale. There is a clear trend that pre-trained language models have become
    increasingly larger in the past few years Devlin et al. ([2019](#bib.bib33));
    Lewis et al. ([2020](#bib.bib91)); Raffel et al. ([2020](#bib.bib146)); Radford
    et al. ([2020](#bib.bib144)); Brown et al. ([2020](#bib.bib15)). A recent study Liang
    et al. ([2022a](#bib.bib100)) shows that model scale within a model family reliably
    predicts model accuracy. The study also mentions an interesting thresholding effect:
    “all models that win head-to-head model comparisons for accuracy at a rate well
    above chance are at least 50B parameters”. A similar size-growing trend can be
    observed in the field of mathematical reasoning with pre-trained language models.
    For example, MWP-BERT Liang et al. ([2022b](#bib.bib102)) uses a backbone of BERT
    (110M) Devlin et al. ([2019](#bib.bib33)) and RoBERTa (123M) Liu et al. ([2019b](#bib.bib110))
    for Math Word Problems. Most recently, Minerva Lewkowycz et al. ([2022](#bib.bib92)),
    which is based on the PaLM Chowdhery et al. ([2022](#bib.bib30)) pre-trained language
    model, has a size up to 540B parameters.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规模。近年来，预训练语言模型的规模明显增大了，Devlin 等人（[2019](#bib.bib33)）；Lewis 等人（[2020](#bib.bib91)）；Raffel
    等人（[2020](#bib.bib146)）；Radford 等人（[2020](#bib.bib144)）；Brown 等人（[2020](#bib.bib15)）。Liang
    等人（[2022a](#bib.bib100)）的最新研究表明，同一模型家族内的模型规模可以可靠地预测模型准确性。该研究还提到了一个有趣的阈值效应：“所有在准确性上赢得头对头模型比较的模型，其参数至少为
    50B”。在数学推理领域中，也可以观察到类似的规模增长趋势。例如，MWP-BERT Liang 等人（[2022b](#bib.bib102)）使用了 BERT（110M）
    Devlin 等人（[2019](#bib.bib33)）和 RoBERTa（123M） Liu 等人（[2019b](#bib.bib110)）作为数学词汇问题的骨干。最近，基于
    PaLM Chowdhery 等人（[2022](#bib.bib30)）预训练语言模型的 Minerva Lewkowycz 等人（[2022](#bib.bib92)），其规模已达
    540B 参数。
- en: Pre-training corpus. There are generally two types of pre-training corpus for
    mathematical language models. (i) Curated datasets from openly accessible sources.
    For example, Hendrycks et al. ([2021b](#bib.bib55)) present the first large-scale
    mathematics pre-training dataset with step-by-step solutions in natural language
    and LaTeX, called the Auxiliary Mathematics Problems and Solutions (AMPS). AMPS
    consists of Khan Academy and Mathematica data. Minerva Lewkowycz et al. ([2022](#bib.bib92))
    collects a high-quality dataset containing scientific and mathematical data, which
    contains 38.5B tokens from webpages filtered for mathematical content and from
    papers submitted to the arXiv preprint server. Thor Jiang et al. ([2022b](#bib.bib68))
    pre-trains a language model on the GitHub + arXiv subsets of The Pile Gao et al.
    ([2020](#bib.bib40)). (ii) Synthetic datasets based on templates or interaction
    with engines. Recent work Wu et al. ([2021d](#bib.bib200)); Krishna et al. ([2021](#bib.bib84));
    Ri and Tsuruoka ([2022](#bib.bib150)); Anderson and Farrell ([2022](#bib.bib5));
    Wu et al. ([2022c](#bib.bib199)) shows that pre-training on data that is fully
    synthetically generated—synthetic pre-training can actually provide substantial
    gains. Representative work includes TaPEX Liu et al. ([2022b](#bib.bib107)), which
    obtains a pre-training corpus by automatically synthesizing executable SQL queries
    and their execution outputs. LISA Jiang et al. ([2021](#bib.bib67)) extracts lemmas
    and theorems by interacting with the Isabelle standard library and the Archive
    of Formal Proofs. GenBERT Geva et al. ([2020](#bib.bib46)) generates numerical
    and textual pre-training datasets based on manually crafted and extracted templates.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语料库。数学语言模型的预训练语料库通常有两种类型。（i）来自开放访问源的精心策划的数据集。例如，Hendrycks 等 ([2021b](#bib.bib55))
    提出了首个大规模的数学预训练数据集，提供了自然语言和LaTeX中的逐步解决方案，称为辅助数学问题与解决方案（AMPS）。AMPS包含了Khan Academy和Mathematica的数据。Minerva Lewkowycz
    等 ([2022](#bib.bib92)) 收集了一个高质量的数据集，包含科学和数学数据，其中包括从网页中筛选的38.5B个标记以及从arXiv预印本服务器提交的论文。Thor Jiang
    等 ([2022b](#bib.bib68)) 在GitHub + arXiv子集上对The Pile Gao 等 ([2020](#bib.bib40))进行了语言模型预训练。（ii）基于模板或与引擎交互的合成数据集。近期的工作 Wu
    等 ([2021d](#bib.bib200))；Krishna 等 ([2021](#bib.bib84))；Ri 和 Tsuruoka ([2022](#bib.bib150))；Anderson
    和 Farrell ([2022](#bib.bib5))；Wu 等 ([2022c](#bib.bib199)) 表明，完全合成生成的数据进行预训练——合成预训练实际上可以提供显著的提升。代表性工作包括
    TaPEX Liu 等 ([2022b](#bib.bib107))，通过自动合成可执行的SQL查询及其执行结果来获得预训练语料库。LISA Jiang 等
    ([2021](#bib.bib67)) 通过与Isabelle标准库和正式证明档案库互动，提取了引理和定理。GenBERT Geva 等 ([2020](#bib.bib46))
    基于手工制作和提取的模板生成数值和文本预训练数据集。
- en: 'Pre-training tasks. General pre-training language models have two typical self-supervised
    learning tasks: (i) Masked Language Modeling (MLM), where it randomly masks a
    portion of words in each sequence to predict the outcome; (ii) Causal Language
    Modeling (CLM), where the model is trained to predict the next token in a sequence
    of tokens. Following the same paradigm, researchers pre-train language models
    with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks Polu
    and Sutskever ([2020](#bib.bib138)); Hendrycks et al. ([2021b](#bib.bib55)); Han
    et al. ([2022](#bib.bib51)); Jiang et al. ([2022b](#bib.bib68)).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练任务。一般的预训练语言模型有两种典型的自监督学习任务：（i）掩码语言建模（MLM），在每个序列中随机掩盖一部分单词以预测结果；（ii）因果语言建模（CLM），模型训练以预测序列中下一个标记。按照相同的范式，研究人员在数学或科学语料库上使用MLM和CLM任务对语言模型进行预训练，以进行下游任务 Polu
    和 Sutskever ([2020](#bib.bib138))；Hendrycks 等 ([2021b](#bib.bib55))；Han 等 ([2022](#bib.bib51))；Jiang
    等 ([2022b](#bib.bib68))。
- en: 'There is also recent work that designs customized tasks to inject mathematical
    reasoning capabilities into language models. For instance, Liang et al. ([2022b](#bib.bib102))
    pre-train language models with a suite of 8 numeracy-augmented tasks with consideration
    of reasoning logic and numerical properties. LIME Wu et al. ([2021d](#bib.bib200))
    proposes synthetic pre-training tasks to learn three reasoning primitives: deduction,
    induction, and abduction before learning more complex reasoning skills, which
    also be regarded as a form of curriculum learning.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些近期的工作设计了定制任务，以将数学推理能力注入语言模型。例如，Liang 等 ([2022b](#bib.bib102)) 通过考虑推理逻辑和数值属性，使用一套包含8个数值增强任务的工具对语言模型进行预训练。LIME Wu
    等 ([2021d](#bib.bib200)) 提出了合成预训练任务，以学习三种推理原语：演绎推理、归纳推理和溯因推理，然后再学习更复杂的推理技能，这也可以视为一种课程学习形式。
- en: '| Paper | Backbone | Task |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 基础模型 | 任务 |'
- en: '| EPT ([2020](#bib.bib78)) | ALBERT ([2019](#bib.bib89)) | MWP |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| EPT ([2020](#bib.bib78)) | ALBERT ([2019](#bib.bib89)) | MWP |'
- en: '| Generate & Rank ([2021](#bib.bib163)) | BART ([2020](#bib.bib91)) | MWP |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Generate & Rank ([2021](#bib.bib163)) | BART ([2020](#bib.bib91)) | MWP |'
- en: '| RPKHS ([2021b](#bib.bib206)) | RoBERTa ([2019b](#bib.bib110)) | MWP |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| RPKHS ([2021b](#bib.bib206)) | RoBERTa ([2019b](#bib.bib110)) | MWP |'
- en: '| PatchTRM ([2021b](#bib.bib116)) | ResNet+BERT ([2019](#bib.bib33)) | MWP
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| PatchTRM ([2021b](#bib.bib116)) | ResNet+BERT ([2019](#bib.bib33)) | MWP
    |'
- en: '| GSM8K-PLM ([2021](#bib.bib32)) | GPT-3 ([2020](#bib.bib15)) | MWP |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K-PLM ([2021](#bib.bib32)) | GPT-3 ([2020](#bib.bib15)) | MWP |'
- en: '| BERT-TD+CL ([2022b](#bib.bib99)) | BERT ([2019](#bib.bib33)) | MWP |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| BERT-TD+CL ([2022b](#bib.bib99)) | BERT ([2019](#bib.bib33)) | MWP |'
- en: '| DeductReasoner ([2022](#bib.bib69)) | RoBERTa ([2019b](#bib.bib110)) | MWP
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| DeductReasoner ([2022](#bib.bib69)) | RoBERTa ([2019b](#bib.bib110)) | MWP
    |'
- en: '| Self-Sampling ([2023](#bib.bib131)) | GPT-Neo ([2020](#bib.bib40)) | MWP
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Self-Sampling ([2023](#bib.bib131)) | GPT-Neo ([2020](#bib.bib40)) | MWP
    |'
- en: '| Bhaskara ([2022a](#bib.bib125)) | GPT-Neo ([2020](#bib.bib40)) | MWP |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Bhaskara ([2022a](#bib.bib125)) | GPT-Neo ([2020](#bib.bib40)) | MWP |'
- en: '| miniF2F-PLM ([2022](#bib.bib222)) | GPT-f ([2020](#bib.bib138)) | TP |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| miniF2F-PLM ([2022](#bib.bib222)) | GPT-f ([2020](#bib.bib138)) | TP |'
- en: '| NaturalProver ([2022a](#bib.bib186)) | GPT-3 ([2020](#bib.bib15)) | TP |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| NaturalProver ([2022a](#bib.bib186)) | GPT-3 ([2020](#bib.bib15)) | TP |'
- en: '| Inter-GPS ([2021a](#bib.bib112)) | BART ([2020](#bib.bib91)) | GPS |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Inter-GPS ([2021a](#bib.bib112)) | BART ([2020](#bib.bib91)) | GPS |'
- en: '| UniGeo ([2022a](#bib.bib19)) | VL-T5 ([2021](#bib.bib27)) | GPS |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| UniGeo ([2022a](#bib.bib19)) | VL-T5 ([2021](#bib.bib27)) | GPS |'
- en: '| DPE-NGS ([2022](#bib.bib16)) | RoBERTa ([2019b](#bib.bib110)) | GPS |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DPE-NGS ([2022](#bib.bib16)) | RoBERTa ([2019b](#bib.bib110)) | GPS |'
- en: '| Aristo ([2020](#bib.bib31)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Aristo ([2020](#bib.bib31)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
- en: '| FinQANet ([2021c](#bib.bib24)) | RoBERTa ([2019b](#bib.bib110)) | MathQA
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| FinQANet ([2021c](#bib.bib24)) | RoBERTa ([2019b](#bib.bib110)) | MathQA
    |'
- en: '| TAGOP ([2021](#bib.bib225)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| TAGOP ([2021](#bib.bib225)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
- en: '| MT2Net ([2022](#bib.bib220)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MT2Net ([2022](#bib.bib220)) | RoBERTa ([2019b](#bib.bib110)) | MathQA |'
- en: '| Scratchpad ([2021](#bib.bib132)) | Transformer ([2017](#bib.bib177)) | Mixed
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Scratchpad ([2021](#bib.bib132)) | Transformer ([2017](#bib.bib177)) | 混合
    |'
- en: '| LAMT ([2022](#bib.bib18)) | Transformer ([2017](#bib.bib177)) | Mixed |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LAMT ([2022](#bib.bib18)) | Transformer ([2017](#bib.bib177)) | 混合 |'
- en: 'Table 3: Finetuned pre-trained language models for downstream mathematical
    reasoning tasks.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 针对下游数学推理任务的微调预训练语言模型。'
- en: 4.2 Task-specific Fine-tuning for Math
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 针对数学任务的特定微调
- en: Task-specific fine-tuning is a technique to improve the performance of a pre-trained
    language model on a specific task. This is also a common practice when there is
    not enough data for training the large models from scratch. As shown in [Table 3](#S4.T3
    "Table 3 ‣ 4.1 Self-Supervised Learning for Math ‣ 4 Pre-trained Language Models
    for Mathematical Reasoning ‣ A Survey of Deep Learning for Mathematical Reasoning"),
    existing work fine-tunes pre-trained language models on a variety of downstream
    tasks, such as math word problems Kim et al. ([2020](#bib.bib78)); Shen et al.
    ([2021](#bib.bib163)), MathQA Zhao et al. ([2022](#bib.bib220)), geometry problem
    solving Lu et al. ([2021a](#bib.bib112)), linear algebra Charton ([2022](#bib.bib18)),
    and theorem proving Welleck et al. ([2022a](#bib.bib186)). Apart from fine-tuning
    the model parameters, some work also uses pre-trained language models as encoders
    and ensembles them with other modules for downstream tasks Lu et al. ([2021b](#bib.bib116)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 特定任务的微调是一种提高预训练语言模型在特定任务上表现的技术。这也是在数据不足以从头训练大型模型时的常见做法。如在[表3](#S4.T3 "表 3 ‣
    4.1 自监督学习用于数学 ‣ 4 预训练语言模型用于数学推理 ‣ 深度学习在数学推理中的综述")中所示，现有的工作将预训练语言模型微调于各种下游任务，如数学词汇问题
    Kim et al. ([2020](#bib.bib78)); Shen et al. ([2021](#bib.bib163)), MathQA Zhao
    et al. ([2022](#bib.bib220)), 几何问题求解 Lu et al. ([2021a](#bib.bib112)), 线性代数 Charton
    ([2022](#bib.bib18)), 和定理证明 Welleck et al. ([2022a](#bib.bib186))。除了微调模型参数，一些工作还将预训练语言模型作为编码器，并与其他模块集成用于下游任务 Lu
    et al. ([2021b](#bib.bib116))。
- en: 5 In-context Learning for Mathematical Reasoning
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 上下文学习用于数学推理
- en: '| Models | Engine | ICL | Rationale | Rationale | Post method |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 引擎 | ICL | 理由 | 理由 | 后处理方法 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| (best performed) | source | type | source |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| （最佳表现） | 来源 | 类型 | 来源 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Few-shot-CoT Wei et al. ([2022](#bib.bib184)) | PaLM (540B) | Random | Language
    | Hand-crafted | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Few-shot-CoT Wei et al. ([2022](#bib.bib184)) | PaLM (540B) | 随机 | 语言 | 手工制作
    | - |'
- en: '| Self-Consistency-CoT Wang et al. ([2023](#bib.bib182)) | Codex (175B) | Random
    | Language | Hand-crafted | Self-consistency |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Self-Consistency-CoT Wang et al. ([2023](#bib.bib182)) | Codex (175B) | 随机
    | 语言 | 手工设计 | 自我一致性 |'
- en: '| Least-to-most CoT Zhou et al. ([2023](#bib.bib224)) | Codex (175B) | Random
    | Language | Hand-crafted | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Least-to-most CoT Zhou et al. ([2023](#bib.bib224)) | Codex (175B) | 随机 |
    语言 | 手工设计 | - |'
- en: '| PromptPG-CoT Lu et al. ([2022b](#bib.bib115)) | GPT-3 (175B) | RL | Language
    | Hand-crafted | - |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| PromptPG-CoT Lu et al. ([2022b](#bib.bib115)) | GPT-3 (175B) | 强化学习 | 语言
    | 手工设计 | - |'
- en: '| Retrieval-CoT Zhang et al. ([2023](#bib.bib218)) | GPT-3 (175B) | Retrival
    | Language | Auto-generated | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Retrieval-CoT Zhang et al. ([2023](#bib.bib218)) | GPT-3 (175B) | 检索 | 语言
    | 自动生成 | - |'
- en: '| Auto-CoT Zhang et al. ([2023](#bib.bib218)) | Codex (175B) | Clustering |
    Language | Auto-generated | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Auto-CoT Zhang et al. ([2023](#bib.bib218)) | Codex (175B) | 聚类 | 语言 | 自动生成
    | - |'
- en: '| Complexity-CoT Fu et al. ([2023](#bib.bib39)) | GPT-3 (175B) | Complexity
    | Language | Hand-crafted | Self-consistency |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Complexity-CoT Fu et al. ([2023](#bib.bib39)) | GPT-3 (175B) | 复杂性 | 语言 |
    手工设计 | 自我一致性 |'
- en: '| Few-shot-PoT Chen et al. ([2022b](#bib.bib22)) | GPT-3 (175B) | Random |
    Code | Hand-crafted | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Few-shot-PoT Chen et al. ([2022b](#bib.bib22)) | GPT-3 (175B) | 随机 | 代码 |
    手工设计 | - |'
- en: 'Table 4: In-context learning with large language models for mathematical reasoning.
    For GPT-3, all papers use the $\mathrm{text}$-$\mathrm{davinci}$-$\mathrm{002}$
    version; for Codex, all papers use the $\mathrm{code}$-$\mathrm{davinci}$-$\mathrm{002}$.
    RL is short for reinforcement learning.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：用于数学推理的大型语言模型的上下文学习。对于 GPT-3，所有论文使用 $\mathrm{text}$-$\mathrm{davinci}$-$\mathrm{002}$
    版本；对于 Codex，所有论文使用 $\mathrm{code}$-$\mathrm{davinci}$-$\mathrm{002}$。RL 是强化学习的缩写。
- en: Large language models (LLMs), such as GPT-3 (Brown et al., [2020](#bib.bib15)),
    have recently revolutionized the field of natural language processing (NLP), especially
    on account of their powerful few-shot in-context learning capabilities Brown et al.
    ([2020](#bib.bib15)). In-context Learning (ICL) enables LLMs to perform target
    tasks by providing some task examples as conditions at inference time, without
    updating model parameters Radford et al. ([2020](#bib.bib144)); Brown et al. ([2020](#bib.bib15)).
    ICL allows users to quickly build models for new use cases without worrying about
    fine-tuning and storing a large amount of new parameters for each task, so it
    is widely used in few-shot settings nowadays Min et al. ([2022](#bib.bib123)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），例如 GPT-3（Brown et al., [2020](#bib.bib15)），最近在自然语言处理（NLP）领域引发了革命，尤其是由于其强大的少量示例上下文学习能力（Brown
    et al., [2020](#bib.bib15)）。上下文学习（ICL）使 LLMs 能够通过在推理时提供一些任务示例作为条件来执行目标任务，而无需更新模型参数（Radford
    et al., [2020](#bib.bib144)）；Brown et al., [2020](#bib.bib15)）。ICL 使用户能够快速构建适用于新用例的模型，而不必担心为每个任务调整和存储大量新参数，因此在少量示例设置中得到了广泛应用（Min
    et al., [2022](#bib.bib123)）。
- en: 'An in-context example typically contains an input-output pair with some prompt
    words, e.g., Please select the largest number from the list. Input: [2, 4, 1,
    5, 8]. Output: 8, and few-shot works by giving multiple examples, and then a final
    input example, where the model is expected to predict the output. However, such
    standard few-shot promptings, in which the LLM is given in-context examples of
    input–output pairs in front of test-time examples, have not yet proved sufficient
    to achieve high performance on challenging tasks such as mathematical reasoning Rae
    et al. ([2021](#bib.bib145)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个上下文示例通常包含一个带有一些提示词的输入输出对，例如，请从列表中选择最大的数字。输入：[2, 4, 1, 5, 8]。输出：8，少量示例的工作原理是提供多个示例，然后是最终的输入示例，模型预计会预测输出。然而，这种标准的少量示例提示，在测试时提供
    LLM 输入输出对的上下文示例，还未能证明足以在如数学推理等具有挑战性的任务上取得高性能（Rae et al., [2021](#bib.bib145)）。
- en: Chain-of-thought prompting (CoT) Wei et al. ([2022](#bib.bib184)) leverages
    intermediate natural language rationales as prompts to enable LLMs to first generate
    reasoning chains and then predict an answer for an input question. For example,
    a CoT prompt for solving the math word problem could be
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链提示（CoT）Wei et al. ([2022](#bib.bib184)) 利用中间自然语言推理作为提示，使 LLMs 首先生成推理链，然后为输入问题预测答案。例如，解决数学文字题的
    CoT 提示可能是
- en: 'Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
    can has 3 tennis balls. Then, how many tennis balls does Roger have now?'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：罗杰有 5 个网球。他又买了 2 罐网球。每罐有 3 个网球。那么，罗杰现在有多少个网球？
- en: 'Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis
    balls. 5 + 6 = 11. The answer is 11.'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：罗杰开始时有 5 个球。2 罐每罐 3 个网球共 6 个网球。5 + 6 = 11。答案是 11。
- en: Apart from Kojima et al. ([2022](#bib.bib81)) showing that LLMs are decent zero-shot
    reasoners when given the “Let’s think step by step!” prompt, most of the recent
    work has focused on how to improve chain-of-thought reasoning under the few-shot
    setting. This work is mainly divided into two parts, (i) selecting better in-context
    examples and (ii) creating better reasoning chains.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Kojima et al. ([2022](#bib.bib81)) 显示 LLM 在给定“让我们一步一步思考！”提示时表现出色的零样本推理外，大多数近期工作集中在如何在少样本设置下改进思维链推理。这项工作主要分为两个部分，（i）选择更好的上下文示例和（ii）创建更好的推理链。
- en: 5.1 In-context Example Selection
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 上下文示例选择
- en: Early chain-of-thought work randomly or heuristically selects in-context examples.
    However, recent studies have shown that this type of few-shot learning can be
    highly unstable across different selections of in-context examples Rubin et al.
    ([2022](#bib.bib156)); Liu et al. ([2022a](#bib.bib106)). Therefore, which in-context
    reasoning examples make the most effective prompts is still an unknown problem
    in the literature. To address the limitation, recent work has investigated various
    methods to optimize the in-context examples selection process Rubin et al. ([2022](#bib.bib156));
    Zhang et al. ([2023](#bib.bib218)); Lu et al. ([2022b](#bib.bib115)); Yu et al.
    ([2023](#bib.bib207)); Fu et al. ([2023](#bib.bib39)). For example, Rubin et al.
    ([2022](#bib.bib156)) attempt to address this issue by retrieving semantically
    similar examples. In addition, Fu et al. ([2023](#bib.bib39)) propose complexity-based
    prompting, which chooses examples with complex reasoning chains, i.e., chains
    with more reasoning steps, as the prompt. PromptPG Lu et al. ([2022b](#bib.bib115))
    learns to select optimal in-context examples via reinforcement learning (RL) from
    a candidate pool.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的思维链工作随机或启发式地选择上下文示例。然而，近期研究表明，这种少样本学习在不同的上下文示例选择中可能非常不稳定 Rubin et al. ([2022](#bib.bib156));
    Liu et al. ([2022a](#bib.bib106))。因此，哪些上下文推理示例最有效仍然是文献中未知的问题。为了解决这个限制，近期的工作调查了各种方法来优化上下文示例选择过程
    Rubin et al. ([2022](#bib.bib156)); Zhang et al. ([2023](#bib.bib218)); Lu et
    al. ([2022b](#bib.bib115)); Yu et al. ([2023](#bib.bib207)); Fu et al. ([2023](#bib.bib39))。例如，Rubin
    et al. ([2022](#bib.bib156)) 试图通过检索语义相似的示例来解决这个问题。此外，Fu et al. ([2023](#bib.bib39))
    提出了基于复杂度的提示，选择具有复杂推理链的示例，即具有更多推理步骤的链作为提示。PromptPG Lu et al. ([2022b](#bib.bib115))
    通过强化学习（RL）从候选池中学习选择最佳的上下文示例。
- en: 5.2 High-quality Reasoning Chains
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 高质量推理链
- en: Early chain of thought work (e.g., Wei et al. ([2022](#bib.bib184))) mainly
    relies on a single human-annotated reasoning chain as a prompt. However, manually
    creating reasoning chains has two disadvantages. First, as tasks become more complex,
    current models may not be sufficient to learn to perform all necessary reasoning
    steps and cannot easily generalize to different tasks. Second, a single decoding
    process is vulnerable to incorrect inference steps, leading to an incorrect prediction
    as the final answer. To address this limitation, recent studies mainly focus on
    two aspects, (i) hand-crafting more complex demonstrations, which we refer to
    as process-based approaches Zhou et al. ([2023](#bib.bib224)); Chen et al. ([2022b](#bib.bib22)),
    (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches Wang
    et al. ([2023](#bib.bib182)); Li et al. ([2022a](#bib.bib98)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的思维链工作（例如，Wei et al. ([2022](#bib.bib184))) 主要依赖单一的人工注释推理链作为提示。然而，手动创建推理链有两个缺点。首先，随着任务的复杂化，当前模型可能不足以学习执行所有必要的推理步骤，并且难以轻松泛化到不同的任务。其次，单一的解码过程易受到错误推理步骤的影响，导致最终预测结果错误。为了解决这一局限性，近期研究主要集中在两个方面，（i）手工设计更复杂的示例，我们称之为基于过程的方法
    Zhou et al. ([2023](#bib.bib224)); Chen et al. ([2022b](#bib.bib22))，（ii）利用类似集成的方法，我们称之为基于结果的方法
    Wang et al. ([2023](#bib.bib182)); Li et al. ([2022a](#bib.bib98))。
- en: 'Process-based approaches aim to improve the chain-of-thought reasoning quality,
    especially for complex reasoning tasks. In least-to-most prompting Zhou et al.
    ([2023](#bib.bib224)), the problem-solving process is implemented through two-stage
    prompting: (i) reducing a complex problem into a list of sub-problems; (ii) solving
    these sub-problems sequentially, so that solving a given sub-problem is facilitated
    by the answers to previously solved sub-problems. Similarly, Khot et al. ([2022](#bib.bib77))
    leverage diverse decomposition structures and use different prompts to answer
    each sub-question. Apart from these multi-step reasoning methods, Chen et al.
    ([2022b](#bib.bib22)); Gao et al. ([2022](#bib.bib41)) propose program-of-thoughts
    (PoT), an alternative solution that uses large language models to express the
    reasoning process as a program. The computation is then relegated to an external
    computer, which executes the generated programs to derive the answer. A more recent
    work, Chameleon Lu et al. ([2023](#bib.bib114)), integrates different tools to
    enhance the abilities of LLMs for compositional reasoning.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过程的方法旨在提高链式思维推理的质量，特别是对于复杂的推理任务。在从少到多的提示中 Zhou et al. ([2023](#bib.bib224))，问题解决过程通过两阶段提示实现：(i)
    将复杂问题简化为子问题列表；(ii) 顺序解决这些子问题，从而通过之前解决的子问题的答案来促进解决给定的子问题。类似地，Khot et al. ([2022](#bib.bib77))
    利用多样的分解结构，并使用不同的提示来回答每个子问题。除了这些多步骤推理方法，Chen et al. ([2022b](#bib.bib22)); Gao
    et al. ([2022](#bib.bib41)) 提出了思维程序（PoT），这是一种替代解决方案，使用大型语言模型将推理过程表达为程序。计算则被交给外部计算机，后者执行生成的程序以得出答案。最近的工作
    Chameleon Lu et al. ([2023](#bib.bib114)) 集成了不同的工具，以增强LLMs在组合推理方面的能力。
- en: Outcome-based approaches acknowledge the potential incorrectness of an individual
    reasoning path, and instead use multiple reasoning paths Wang et al. ([2023](#bib.bib182));
    Li et al. ([2022a](#bib.bib98)). Self-consistency Wang et al. ([2023](#bib.bib182))
    generates a set of reasoning paths by sampling from the language model, and marginalizes
    out the reasoning paths by choosing the most common answer. In addition to using
    sampling with a single prompt to produce multiple reasoning paths, Li et al. ([2022a](#bib.bib98))
    propose to introduce diverse prompts through “self-teaching”, as a complementary
    solution to produce a higher degree of diversity.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于结果的方法承认单个推理路径可能存在错误，而是使用多个推理路径 Wang et al. ([2023](#bib.bib182)); Li et al.
    ([2022a](#bib.bib98))。自我一致性 Wang et al. ([2023](#bib.bib182)) 通过从语言模型中抽样生成一组推理路径，并通过选择最常见的答案来边际化推理路径。除了使用单个提示生成多个推理路径外，Li
    et al. ([2022a](#bib.bib98)) 提出了通过“自我教学”引入多样化的提示，作为产生更高程度多样性的补充解决方案。
- en: 6 Discussion and Findings
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与发现
- en: 6.1 Analysis of Benchmarks
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基准分析
- en: The multi-modal setting is underexplored but is gaining increasing attention.
    Most existing benchmarks for mathematical reasoning have targeted the textual-only
    modality. However, visual elements can provide a rich source of quantitative information,
    making multi-modal datasets beneficial for reasoning over quantitative relations
    in natural images Lu et al. ([2022a](#bib.bib113)), abstract diagrams Lu et al.
    ([2021b](#bib.bib116)), figures Kahou et al. ([2018](#bib.bib72)), and charts
    Kafle et al. ([2018](#bib.bib71)). Tables, which are commonly found in daily documents
    and contain hierarchically structured information, have also been the focus of
    tasks that require quantitative reasoning over textual and tabular context Chen
    et al. ([2021c](#bib.bib24)); Zhu et al. ([2021](#bib.bib225)); Zhao et al. ([2022](#bib.bib220));
    Lu et al. ([2022b](#bib.bib115)). In addition, recent datasets have been developed
    for mathematical reasoning grounded on conversations Sun et al. ([2019](#bib.bib167));
    Zhang et al. ([2021](#bib.bib213)); Chen et al. ([2022c](#bib.bib25)), as well
    as reports Chen et al. ([2022c](#bib.bib25)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态设置尚未充分探索，但正获得越来越多的关注。现有的大多数数学推理基准只针对文本模式。然而，视觉元素可以提供丰富的定量信息，使得多模态数据集在自然图像
    Lu et al. ([2022a](#bib.bib113))、抽象图表 Lu et al. ([2021b](#bib.bib116))、图形 Kahou
    et al. ([2018](#bib.bib72)) 和图表 Kafle et al. ([2018](#bib.bib71)) 上的定量关系推理中具有重要价值。表格作为日常文档中常见的元素，包含层级结构的信息，也被用于需要在文本和表格上下文中进行定量推理的任务
    Chen et al. ([2021c](#bib.bib24)); Zhu et al. ([2021](#bib.bib225)); Zhao et al.
    ([2022](#bib.bib220)); Lu et al. ([2022b](#bib.bib115))。此外，近期还开发了基于对话的数学推理数据集
    Sun et al. ([2019](#bib.bib167)); Zhang et al. ([2021](#bib.bib213)); Chen et al.
    ([2022c](#bib.bib25))，以及报告 Chen et al. ([2022c](#bib.bib25))。
- en: Pioneering work is emerging in the exploration of low-resource settings. Despite
    the creation of various datasets, mathematical reasoning in low-resource settings
    remains largely under-explored. Pioneering research has developed mathematical
    reasoning benchmarks for financial Chen et al. ([2021c](#bib.bib24)); Zhu et al.
    ([2021](#bib.bib225)); Zhao et al. ([2022](#bib.bib220)) and scientific domains
    Lu et al. ([2022a](#bib.bib113)). Additionally, there have been attempts to build
    non-English datasets for Chinese Wang et al. ([2017](#bib.bib183)); Qin et al.
    ([2020](#bib.bib140)); Yu et al. ([2021a](#bib.bib205)) and Arabic Alghamdi et al.
    ([2022](#bib.bib2)) for mathematical reasoning.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在低资源环境下的探索中，开创性的工作正在出现。尽管创建了各种数据集，但低资源环境中的数学推理仍然在很大程度上未被充分探索。开创性研究为金融领域 Chen
    et al. ([2021c](#bib.bib24)); Zhu et al. ([2021](#bib.bib225)); Zhao et al. ([2022](#bib.bib220))
    和科学领域 Lu et al. ([2022a](#bib.bib113)) 开发了数学推理基准。此外，还有尝试为中文 Wang et al. ([2017](#bib.bib183));
    Qin et al. ([2020](#bib.bib140)); Yu et al. ([2021a](#bib.bib205)) 和阿拉伯语 Alghamdi
    et al. ([2022](#bib.bib2)) 构建非英语数据集用于数学推理。
- en: Diverse rationale annotations have been widely explored. Complex reasoning usually
    involves multiple steps to arrive at the final answer. To bridge this gap, datasets
    annotated with intermediate rationales such as logic forms Tafjord et al. ([2019](#bib.bib169));
    Lu et al. ([2021a](#bib.bib112)), programs Amini et al. ([2019](#bib.bib4)); Chen
    et al. ([2021c](#bib.bib24), [a](#bib.bib20)); Cao and Xiao ([2022](#bib.bib16));
    Chen et al. ([2022a](#bib.bib19)), and reasoning graphs Zhang et al. ([2021](#bib.bib213))
    have been proposed to train models for complex reasoning tasks. Python programs
    are used as reasoning annotations in Austin et al. ([2021](#bib.bib8)); Mishra
    et al. ([2022a](#bib.bib125)) due to their enhanced accessibility and readability.
    To imitate the reasoning process of a human, a more recent trend is to annotate
    solutions in natural language Ling et al. ([2017](#bib.bib105)); Cobbe et al.
    ([2021](#bib.bib32)); Lu et al. ([2022b](#bib.bib115)); Hendrycks et al. ([2021b](#bib.bib55));
    Lu et al. ([2022a](#bib.bib113)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的理由注释已被广泛探索。复杂的推理通常涉及多个步骤以得出最终答案。为了弥补这一差距，已经提出了标注有中间理由的数据集，如逻辑形式 Tafjord
    et al. ([2019](#bib.bib169)); Lu et al. ([2021a](#bib.bib112)), 程序 Amini et al.
    ([2019](#bib.bib4)); Chen et al. ([2021c](#bib.bib24), [a](#bib.bib20)); Cao and
    Xiao ([2022](#bib.bib16)); Chen et al. ([2022a](#bib.bib19)), 和推理图 Zhang et al.
    ([2021](#bib.bib213))，以训练模型进行复杂的推理任务。Python 程序被用作推理注释 Austin et al. ([2021](#bib.bib8));
    Mishra et al. ([2022a](#bib.bib125))，由于其增强的可访问性和可读性。为了模仿人类的推理过程，更近期的趋势是用自然语言注释解决方案
    Ling et al. ([2017](#bib.bib105)); Cobbe et al. ([2021](#bib.bib32)); Lu et al.
    ([2022b](#bib.bib115)); Hendrycks et al. ([2021b](#bib.bib55)); Lu et al. ([2022a](#bib.bib113)).
- en: 6.2 Analysis of Deep Learning Methods
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 深度学习方法分析
- en: '|  | T5 | UnifiedQA | GPT-3 | GPT-3 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | T5 | UnifiedQA | GPT-3 | GPT-3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | (Large) | (Large) | (davinci-002) | (davinci-003) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | (Large) | (Large) | (davinci-002) | (davinci-003) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 3 balls + 5 balls = | ✗ | 5 balls | 8 balls | 8 balls |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 3 balls + 5 balls = | ✗ | 5 balls | 8 balls | 8 balls |'
- en: '| 23 balls + 145 balls = | ✗ | ✗ | 58 balls | 168 balls |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 23 balls + 145 balls = | ✗ | ✗ | 58 balls | 168 balls |'
- en: '| 23 balls + 1,855 balls = | ✗ | ✗ | 2,878 balls | 2,988 balls |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 23 balls + 1,855 balls = | ✗ | ✗ | 2,878 balls | 2,988 balls |'
- en: 'Table 5: Language models struggle with large numbers.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 语言模型在处理大数字时表现挣扎。'
- en: Is the current representation of numeracy sufficient? The standard practice
    for deep learning techniques is to treat numbers in the same way as words. Early
    neural network methods create a vocabulary that maps input words and numbers to
    token IDs, resulting in less frequent numbers being collapsed into an “UNK” token.
    Recent language models use subword tokenization techniques Wu et al. ([2016](#bib.bib196));
    Sennrich et al. ([2016](#bib.bib161)) to split numbers into atomic tokens. Recent
    studies have shown that these tokenization approaches are suboptimal Wallace et al.
    ([2019](#bib.bib178)); Lin et al. ([2020](#bib.bib103)); Zhang et al. ([2020d](#bib.bib215));
    Thawani et al. ([2022](#bib.bib171)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的数值表示是否足够？深度学习技术的标准做法是将数字处理与词语相同。早期的神经网络方法创建了一个词汇表，将输入的词语和数字映射到标记ID上，从而将出现频率较低的数字合并为“UNK”标记。最近的语言模型使用了子词标记化技术
    Wu et al. ([2016](#bib.bib196)); Sennrich et al. ([2016](#bib.bib161)) 将数字拆分为原子标记。最近的研究表明，这些标记化方法是次优的
    Wallace et al. ([2019](#bib.bib178)); Lin et al. ([2020](#bib.bib103)); Zhang
    et al. ([2020d](#bib.bib215)); Thawani et al. ([2022](#bib.bib171)).
- en: 'Two numbers on the same or close number line could have surface forms with
    no shared common tokens. For example, a number like $1598$ is tokenized as “$15$”
    and “$98$” in GPT-3, while another format like $1,598$ is split as three different
    tokens: “$1$”, “,”, and “$598$”. This lack of consistent representation can make
    it difficult for deep learning models to effectively process numbers, especially
    when compared to pure text. The insufficient representations of numbers can lead
    to out-of-distribution (OOD) problems. [Table 5](#S6.T5 "Table 5 ‣ 6.2 Analysis
    of Deep Learning Methods ‣ 6 Discussion and Findings ‣ A Survey of Deep Learning
    for Mathematical Reasoning") provides examples of where language models tend to
    struggle with large numbers. Although increasing model scales could help, even
    the state-of-the-art large language model GPT-3 performs poorly when reasoning
    over large numbers. Some recent work suggests that using scientific notation Zhang
    et al. ([2020d](#bib.bib215)) and digit-level decomposition Geva et al. ([2020](#bib.bib46))
    may be helpful in improving numeracy representation, but this remains an open
    problem.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一条或接近的数字线上，两个数字的表面形式可能没有共同的标记。例如，数字 $1598$ 在 GPT-3 中被分词为 “$15$” 和 “$98$”，而另一种格式如
    $1,598$ 被拆分为三个不同的标记：“$1$”、“,” 和 “$598$”。这种不一致的表示方式使得深度学习模型在处理数字时可能变得困难，特别是与纯文本相比。数字表示的不充分可能导致分布外（OOD）问题。
    [表 5](#S6.T5 "Table 5 ‣ 6.2 Analysis of Deep Learning Methods ‣ 6 Discussion and
    Findings ‣ A Survey of Deep Learning for Mathematical Reasoning") 提供了语言模型在处理大数字时容易出现问题的示例。虽然增加模型规模可能有帮助，但即使是最先进的大型语言模型
    GPT-3 在处理大数字时表现也不佳。一些近期的研究表明，使用科学记数法 Zhang 等人 ([2020d](#bib.bib215)) 和数字级分解 Geva
    等人 ([2020](#bib.bib46)) 可能有助于改善数字表示，但这仍然是一个未解的问题。
- en: '| Problems | GPT-3 (text-davinci-002) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | GPT-3 (text-davinci-002) |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| John had 8 balls and he gave 3 to Mary. How many balls does John have now?
    | John has 5 balls. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| John 有 8 个球，他给了 Mary 3 个。John 现在有多少个球？ | John 现在有 5 个球。 |'
- en: '| John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls
    does Mary have now? | Mary has 5 balls. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| John 有 3 个苹果。John 有 8 个球，他给了 Mary 3 个。Mary 现在有多少个球？ | Mary 现在有 5 个球。 |'
- en: '| John had 8 balls and he gave 3 to Mary. Who has more balls now? | John has
    more balls. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| John 有 8 个球，他给了 Mary 3 个。谁现在有更多的球？ | John 有更多的球。 |'
- en: '| John had 8 balls and he gave 3 to Mary. Does John have more balls now? |
    No, John has 5 balls now. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| John 有 8 个球，他给了 Mary 3 个。John 现在有更多的球吗？ | 不，John 现在有 5 个球。 |'
- en: '| John had 8 balls and he gave 4 to Mary. Does John have more balls now? |
    No, John has 4 balls now. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| John 有 8 个球，他给了 Mary 4 个。John 现在有更多的球吗？ | 不，John 现在有 4 个球。 |'
- en: '| John had 8 balls and he gave 4 to Mary. Who has more balls now? | John has
    more balls. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| John 有 8 个球，他给了 Mary 4 个。谁现在有更多的球？ | John 有更多的球。 |'
- en: 'Table 6: Examples where large language models are not consistent for mathematical
    reasoning.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：大型语言模型在数学推理方面不一致的示例。
- en: Are deep learning methods consistent for mathematical reasoning? Recent developments
    in deep learning have led to impressive results on various mathematical reasoning
    tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM
    benchmark Hendrycks et al. ([2021a](#bib.bib54)), which assesses multitask reasoning
    ability in the fields of science, technology, engineering, and mathematics (STEM)
    at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves
    a high accuracy of 93.0% on the MultiArith task. However, the question remains
    as to whether these methods are sufficiently advanced to tackle more complex problems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法在数学推理方面是否一致？近期深度学习的发展在各种数学推理任务中取得了令人瞩目的成果。Zero-shot-CoT Minerva 540B 在
    MMLU-STEM 基准测试 Hendrycks 等人 ([2021a](#bib.bib54)) 中得分为 75.0%，该测试评估了科学、技术、工程和数学（STEM）领域的多任务推理能力，包括高中和大学水平。类似地，few-shot-CoT
    GPT-3 175B 在 MultiArith 任务中取得了 93.0% 的高准确率。然而，问题依然存在，这些方法是否足够先进以解决更复杂的问题。
- en: There is strong evidence that deep learning methods for mathematical reasoning
    are not robust and susceptible to adversarial attacks Lin et al. ([2020](#bib.bib103));
    Patel et al. ([2021](#bib.bib134)); Mishra et al. ([2022b](#bib.bib126), [a](#bib.bib125));
    Welleck et al. ([2022b](#bib.bib188)). The SVAMP Patel et al. ([2021](#bib.bib134))
    dataset is a collection of one-unknown arithmetic word problems up to grade 4,
    with slight word variations from previous datasets. It is surprising that current
    state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree
    achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%,
    which is just above an “F” grade. [Table 6](#S6.T6 "Table 6 ‣ 6.2 Analysis of
    Deep Learning Methods ‣ 6 Discussion and Findings ‣ A Survey of Deep Learning
    for Mathematical Reasoning") also shows the inconsistent performance of the zero-shot
    GPT-3 model in scenarios with slightly different descriptions, while human performance
    remains unchanged. This indicates a lack of consistency in the mathematical reasoning
    ability of SOTA large language models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有强有力的证据表明，深度学习方法在数学推理方面并不鲁棒，容易受到对抗性攻击的影响Lin 等人 ([2020](#bib.bib103))；Patel 等人
    ([2021](#bib.bib134))；Mishra 等人 ([2022b](#bib.bib126), [a](#bib.bib125))；Welleck
    等人 ([2022b](#bib.bib188))。SVAMP Patel 等人 ([2021](#bib.bib134)) 数据集是一个包含1-未知算术词题的数据集，涵盖到4年级，具有与以前的数据集稍微不同的词汇变体。令人惊讶的是，目前最先进（SOTA）的方法在该数据集上的表现很差，其中Graph2Tree仅达到43.8%的准确率，而zero-shot-CoT
    GPT-3 (175B)仅达到63.7%，略高于“F”级。 [表 6](#S6.T6 "Table 6 ‣ 6.2 Analysis of Deep Learning
    Methods ‣ 6 Discussion and Findings ‣ A Survey of Deep Learning for Mathematical
    Reasoning") 还显示了zero-shot GPT-3模型在描述略有不同的场景中的不一致表现，而人类的表现保持不变。这表明，最先进的大型语言模型在数学推理能力上的一致性不足。
- en: 7 Future Work
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来工作
- en: 7.1 Generalization and Robustness
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 泛化和鲁棒性
- en: Despite impressive progress, neural models commonly display generalization and
    robustness failures on reasoning tasks. For example, above we discussed difficulties
    in generalizing to larger numbers ([Table 5](#S6.T5 "Table 5 ‣ 6.2 Analysis of
    Deep Learning Methods ‣ 6 Discussion and Findings ‣ A Survey of Deep Learning
    for Mathematical Reasoning")) or remaining robust to nearby problems ([Table 6](#S6.T6
    "Table 6 ‣ 6.2 Analysis of Deep Learning Methods ‣ 6 Discussion and Findings ‣
    A Survey of Deep Learning for Mathematical Reasoning")), while others identify
    failures in generalizing to longer problems than those observed in training (e.g.,
    Anil et al. ([2022](#bib.bib7))). One direction is to explore new inference-time Jung
    et al. ([2022](#bib.bib70)); Mitchell et al. ([2022](#bib.bib127)) or fine-tuning Anil
    et al. ([2022](#bib.bib7)) strategies.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了令人印象深刻的进展，神经模型在推理任务上常常表现出泛化和鲁棒性不足的问题。例如，上面我们讨论了在对更大数字进行泛化时的困难（[表 5](#S6.T5
    "Table 5 ‣ 6.2 Analysis of Deep Learning Methods ‣ 6 Discussion and Findings ‣
    A Survey of Deep Learning for Mathematical Reasoning")）或在面对类似问题时保持鲁棒性的问题（[表 6](#S6.T6
    "Table 6 ‣ 6.2 Analysis of Deep Learning Methods ‣ 6 Discussion and Findings ‣
    A Survey of Deep Learning for Mathematical Reasoning")），而其他研究发现了模型在泛化到比训练时观察到的更长问题上的失败（例如，Anil
    等人 ([2022](#bib.bib7))）。一个方向是探索新的推理时间Jung 等人 ([2022](#bib.bib70))；Mitchell 等人
    ([2022](#bib.bib127)) 或微调Anil 等人 ([2022](#bib.bib7)) 策略。
- en: Another aspect of generalization relates to the role of memorization. For example,
    is the ability to produce a complex solution dependent on seeing many similar
    solutions during training, or even on memorizing the solution? Term frequency
    in the pretraining corpus is known to impact accuracy in simple arithmetic tasks Razeghi
    et al. ([2022](#bib.bib148)) or factual question answering Kandpal et al. ([2022](#bib.bib75)).
    On the other hand, Lewkowycz et al. ([2022](#bib.bib92)) did not find evidence
    of memorization in complex outputs, yet their training set and model are not available
    for inspection. Gaining a full understanding of these factors for complex problems
    and outputs (e.g., multi-step solutions or proofs) requires more analysis, as
    well as accessible datasets and models.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的另一个方面涉及记忆的作用。例如，产生复杂解决方案的能力是否依赖于在训练过程中看到许多类似的解决方案，甚至是记忆解决方案？已知预训练语料库中的术语频率会影响简单算术任务的准确性Razeghi
    等人 ([2022](#bib.bib148)) 或事实问答Kandpal 等人 ([2022](#bib.bib75))。另一方面，Lewkowycz 等人
    ([2022](#bib.bib92)) 没有发现复杂输出中记忆的证据，但他们的训练集和模型不可供检查。要全面了解这些因素对复杂问题和输出（例如，多步解决方案或证明）的影响，需要更多的分析，以及可访问的数据集和模型。
- en: 7.2 Trustworthy Reasoning
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 可信赖的推理
- en: 'Recent advances in language models have demonstrated their powerful capabilities
    for mathematical reasoning. However, due to the potential for generating ungrounded
    answers Nakano et al. ([2021](#bib.bib129)), users can’t always trust the predicted
    outcomes or have to verify then with extra efforts. Even with recent prompting
    strategies that provide rationales before making predictions Wei et al. ([2022](#bib.bib184)),
    language models can still hallucinate statements, produce flawed reasoning, and
    output wrong answers. Consequently, novel approaches that enable more reliable
    reasoning are needed urgently. Some potential directions for this include: (i)
    using language models to provide evidence, such as theorems, to support the reasoning
    process; (ii) incorporating a mechanism that makes a judgment when the model is
    unsure of the answer; and (iii) using a model itself or another module to detect
    and locate mistakes in a model’s reasoning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的语言模型进展展示了它们在数学推理方面的强大能力。然而，由于可能生成没有根据的答案，Nakano 等人（[2021](#bib.bib129)）指出，用户无法始终信任预测结果，或需要付出额外努力进行验证。即使是最近的提示策略，在做出预测之前提供理由，Wei
    等人（[2022](#bib.bib184)）的研究也发现，语言模型仍然可能产生虚假陈述、产生有缺陷的推理，并输出错误的答案。因此，迫切需要能够实现更可靠推理的新方法。一些潜在的方向包括：（i）使用语言模型提供证据，如定理，以支持推理过程；（ii）加入机制，当模型对答案不确定时做出判断；以及（iii）使用模型自身或其他模块检测和定位模型推理中的错误。
- en: 7.3 Learning from Feedback
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 从反馈中学习
- en: Another important direction to further improve language models for mathematical
    reasoning is to let the model learn from feedback. Such a process makes the continual
    improvement of models’ output quality and safety possible. An example is using
    reinforcement learning from human feedback (RLHF) Ouyang et al. ([2022](#bib.bib133))
    to align language models with instructions. The idea is to let humans rank the
    generated outputs of language models and use the learned reward function to finetune
    the language model with policy gradient Ouyang et al. ([2022](#bib.bib133)); Glaese
    et al. ([2022](#bib.bib48)); Qiu et al. ([2022a](#bib.bib141)). In the context
    of mathematical reasoning, feedback does not necessarily come from humans directly.
    The outcome of a theorem-proof engine Jiang et al. ([2021](#bib.bib67)); Wu et al.
    ([2021d](#bib.bib200), [2022c](#bib.bib199)) or the execution result of model-generated
    scripts can also be used as the reward source Polu and Sutskever ([2020](#bib.bib138)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个进一步改善语言模型在数学推理方面的重要方向是让模型从反馈中学习。这一过程使得模型输出质量和安全性得以持续改进。例如，使用来自人类反馈的强化学习（RLHF）Ouyang
    等人（[2022](#bib.bib133)）将语言模型与指令对齐。其思想是让人类对语言模型生成的输出进行排序，并使用学到的奖励函数通过策略梯度对语言模型进行微调Ouyang
    等人（[2022](#bib.bib133)）；Glaese 等人（[2022](#bib.bib48)）；Qiu 等人（[2022a](#bib.bib141)）。在数学推理的背景下，反馈不一定直接来自人类。定理证明引擎的结果Jiang
    等人（[2021](#bib.bib67)）；Wu 等人（[2021d](#bib.bib200), [2022c](#bib.bib199)）或模型生成脚本的执行结果也可以作为奖励来源Polu
    和 Sutskever（[2020](#bib.bib138)）。
- en: 7.4 Multi-modal Mathematical Reasoning
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 多模态数学推理
- en: In recent years, there has been growing interest in multi-modal mathematical
    reasoning, which involves using multiple sources of information, such as text,
    tables, natural images, and diagrams  Kahou et al. ([2018](#bib.bib72)); Kafle
    et al. ([2018](#bib.bib71)); Lu et al. ([2021b](#bib.bib116), [2022b](#bib.bib115)).
    However, currently available datasets in this domain tend to be small Zhao et al.
    ([2022](#bib.bib220)), generated from templates Kahou et al. ([2018](#bib.bib72)),
    or focus on specific topics Lu et al. ([2021a](#bib.bib112)); Chen et al. ([2022a](#bib.bib19)).
    One line of current research involves applying VQA-based frameworks to analyze
    figures and plots, but this approach can result in significant semantic gaps due
    to the fact that most VQA models are trained on natural images. One potential
    direction for future work is to enhance the ability of multi-modal mathematical
    reasoning systems to tackle more complex and realistic problems. This may involve
    creating unified models for interpreting and integrating different modalities,
    as well as developing better evaluation benchmarks to assess the performance of
    these systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，对多模态数学推理的兴趣日益增长，这涉及使用多种信息来源，如文本、表格、自然图像和图示。Kahou 等人 ([2018](#bib.bib72))；Kafle
    等人 ([2018](#bib.bib71))；Lu 等人 ([2021b](#bib.bib116), [2022b](#bib.bib115))。然而，目前在这一领域可用的数据集往往较小，Zhao
    等人 ([2022](#bib.bib220))，生成于模板中，Kahou 等人 ([2018](#bib.bib72))，或专注于特定主题，Lu 等人 ([2021a](#bib.bib112))；Chen
    等人 ([2022a](#bib.bib19))。当前研究的一个方向是应用基于VQA的框架来分析图形和图表，但这种方法可能导致显著的语义差距，因为大多数VQA模型都是在自然图像上训练的。未来工作的一个潜在方向是增强多模态数学推理系统解决更复杂和现实问题的能力。这可能涉及创建统一的模型来解释和整合不同的模态，以及开发更好的评估基准来评估这些系统的性能。
- en: 8 Conclusion
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we present a comprehensive survey of deep learning for mathematical
    reasoning. We review the various tasks, datasets, and deep learning approaches.
    We also identify several gaps in the existing datasets and methods. Finally, we
    outline directions for future research and highlight the potential for further
    exploration in this field. Our goal with this paper is to provide a comprehensive
    and useful resource for readers interested in the development of deep learning
    for mathematical reasoning. To aid in this effort, we have created a reading list
    that will be continually updated in a GitHub repository at [https://github.com/lupantech/dl4math](https://github.com/lupantech/dl4math).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了深度学习在数学推理领域的综合调查。我们回顾了各种任务、数据集和深度学习方法。我们还识别了现有数据集和方法中的几个空白。最后，我们概述了未来研究的方向，并强调了进一步探索这一领域的潜力。我们希望通过本文为对深度学习在数学推理中的发展感兴趣的读者提供一个全面而有用的资源。为了支持这一努力，我们创建了一个阅读列表，并将在
    [https://github.com/lupantech/dl4math](https://github.com/lupantech/dl4math) 上持续更新。
- en: Limitations
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: One limitation of our survey work is that it is focused on the intersection
    of mathematical reasoning and deep learning over the past decade, which may not
    encompass the entire field and its history. Additionally, our evaluation of existing
    benchmarks and methods is based on a curated set of papers and may not fully represent
    the state of the art in the field. Furthermore, due to the fast-paced nature of
    the field, our survey may not reflect the latest developments and advancements
    which may have come out close to or after the survey was conducted. Despite these
    limitations, our survey still provides a valuable overview of the current state
    and key trends in the field of mathematical reasoning and deep learning, and can
    serve as a valuable resource for researchers and practitioners working in this
    field.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查工作的一个限制是，它专注于过去十年数学推理与深度学习的交集，可能未涵盖整个领域及其历史。此外，我们对现有基准和方法的评估是基于一组精心挑选的论文，可能未能完全代表该领域的最新技术。此外，由于该领域的快速发展，我们的调查可能未能反映出在调查进行时或接近调查之后出现的最新发展和进展。尽管存在这些限制，我们的调查仍然提供了对数学推理和深度学习领域当前状态和关键趋势的有价值概述，并且可以为从事该领域的研究人员和实践者提供宝贵的资源。
- en: Broader Impact
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: Our survey paper on the intersection of mathematical reasoning and deep learning
    has the potential to significantly impact the field of artificial intelligence.
    By providing a comprehensive overview of the key tasks, datasets, and methods
    that have been developed in the past decade, we give researchers and practitioners
    a clear understanding of the current state-of-the-art and help them make informed
    decisions about their own research. Additionally, by evaluating existing benchmarks
    and methods and discussing future research directions, we aim to identify gaps
    in the current state of the art and guide future research and development efforts
    towards more advanced and effective mathematical reasoning systems. Overall, our
    survey has the potential to contribute to the advancement of mathematical reasoning
    and deep learning, and have a profound impact on machine learning and natural
    language processing.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查论文关于数学推理与深度学习的交集，具有显著影响人工智能领域的潜力。通过提供过去十年中开发的关键任务、数据集和方法的全面概述，我们为研究人员和从业者提供了当前最先进技术的清晰理解，并帮助他们做出有关自己研究的明智决策。此外，通过评估现有的基准和方法并讨论未来的研究方向，我们旨在识别当前技术状态中的不足，并指导未来的研究和开发工作，朝着更先进和有效的数学推理系统发展。总体而言，我们的调查有可能推动数学推理和深度学习的发展，并对机器学习和自然语言处理产生深远的影响。
- en: References
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alemi et al. (2016) Alexander A. Alemi, François Chollet, Niklas Een, Geoffrey
    Irving, Christian Szegedy, and Josef Urban. 2016. [Deepmath - deep sequence models
    for premise selection](https://arxiv.org/abs/1606.04442). *Advances in neural
    information processing systems (NeurIPS)*, 29.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alemi 等人 (2016) Alexander A. Alemi, François Chollet, Niklas Een, Geoffrey Irving,
    Christian Szegedy, 和 Josef Urban. 2016. [Deepmath - 用于前提选择的深度序列模型](https://arxiv.org/abs/1606.04442)。*神经信息处理系统进展
    (NeurIPS)*，29。
- en: 'Alghamdi et al. (2022) Reem Alghamdi, Zhenwen Liang, and Xiangliang Zhang.
    2022. [Armath: a dataset for solving arabic math word problems](https://aclanthology.org/2022.lrec-1.37/).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference
    (LREC)*, pages 351–362.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alghamdi 等人 (2022) Reem Alghamdi, Zhenwen Liang, 和 Xiangliang Zhang. 2022.
    [Armath: 解决阿拉伯数学问题的数据集](https://aclanthology.org/2022.lrec-1.37/)。在 *第十三届语言资源与评估会议
    (LREC)*，第 351–362 页。'
- en: Alvin et al. (2017) Chris Alvin, Sumit Gulwani, Rupak Majumdar, and Supratik
    Mukhopadhyay. 2017. [Synthesis of solutions for shaded area geometry problems](https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15416/14902).
    In *The Thirtieth International Flairs Conference*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alvin 等人 (2017) Chris Alvin, Sumit Gulwani, Rupak Majumdar, 和 Supratik Mukhopadhyay.
    2017. [阴影区域几何问题的解法综合](https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15416/14902)。在
    *第三十届国际 Flairs 会议*。
- en: 'Amini et al. (2019) Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. 2019. [Mathqa: Towards interpretable math
    word problem solving with operation-based formalisms](https://aclanthology.org/N19-1245/).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*, pages
    2357–2367.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amini 等人 (2019) Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, 和 Hannaneh Hajishirzi. 2019. [Mathqa: 通过基于操作的形式主义实现可解释的数学问题解决](https://aclanthology.org/N19-1245/)。在
    *2019 年北美计算语言学协会：人类语言技术会议 (NAACL-HLT)*，第 2357–2367 页。'
- en: Anderson and Farrell (2022) Connor Anderson and Ryan Farrell. 2022. [Improving
    fractal pre-training](https://arxiv.org/abs/2110.03091). In *Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 1300–1309.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 和 Farrell (2022) Connor Anderson 和 Ryan Farrell. 2022. [改进分形预训练](https://arxiv.org/abs/2110.03091)。在
    *IEEE/CVF 计算机视觉应用冬季会议*，第 1300–1309 页。
- en: Anderson et al. (2018) Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney,
    Mark Johnson, Stephen Gould, and Lei Zhang. 2018. [Bottom-up and top-down attention
    for image captioning and visual question answering](https://arxiv.org/abs/1707.07998).
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*, pages 6077–6086.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等人 (2018) Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney,
    Mark Johnson, Stephen Gould, 和 Lei Zhang. 2018. [自下而上与自上而下的注意力机制用于图像描述和视觉问答](https://arxiv.org/abs/1707.07998)。在
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，第 6077–6086 页。
- en: Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz,
    Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer,
    and Behnam Neyshabur. 2022. [Exploring length generalization in large language
    models](https://openreview.net/forum?id=zSkYVeX7bC4). In *Advances in Neural Information
    Processing Systems (NeurIPS)*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz,
    Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer,
    和 Behnam Neyshabur. 2022. [探索大型语言模型中的长度泛化](https://openreview.net/forum?id=zSkYVeX7bC4).
    在 *神经信息处理系统进展 (NeurIPS)*。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. [Program synthesis with large language models](https://arxiv.org/abs/2108.07732).
    *arXiv preprint arXiv:2108.07732*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, 等. 2021. [使用大语言模型进行程序合成](https://arxiv.org/abs/2108.07732). *arXiv 预印本 arXiv:2108.07732*。
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, 和 Yoshua Bengio. 2015.
    [通过共同学习对齐和翻译的神经机器翻译](https://arxiv.org/abs/1409.0473). 在 *国际学习表征会议 (ICLR)*。
- en: 'Bansal et al. (2019) Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy,
    and Stewart Wilcox. 2019. [Holist: An environment for machine learning of higher
    order logic theorem proving](https://arxiv.org/abs/1904.03241). In *International
    Conference on Machine Learning (ICML)*, pages 454–463\. PMLR.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bansal et al. (2019) Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy,
    和 Stewart Wilcox. 2019. [Holist: 一个用于机器学习高阶逻辑定理证明的环境](https://arxiv.org/abs/1904.03241).
    在 *国际机器学习会议 (ICML)*，页码 454–463. PMLR。'
- en: Barras et al. (1999) Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël
    Courant, Yann Coscoy, David Delahaye, Daniel de Rauglaudre, Jean-Christophe Filliâtre,
    Eduardo Giménez, Hugo Herbelin, et al. 1999. [The coq proof assistant reference
    manual](https://flint.cs.yale.edu/cs430/coq/pdf/Reference-Manual.pdf). *INRIA,
    version*, 6(11).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barras et al. (1999) Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël
    Courant, Yann Coscoy, David Delahaye, Daniel de Rauglaudre, Jean-Christophe Filliâtre,
    Eduardo Giménez, Hugo Herbelin, 等. 1999. [Coq 证明助手参考手册](https://flint.cs.yale.edu/cs430/coq/pdf/Reference-Manual.pdf).
    *INRIA, 版本*，6(11)。
- en: Berg-Kirkpatrick and Spokoyny (2020) Taylor Berg-Kirkpatrick and Daniel Spokoyny.
    2020. [An empirical investigation of contextualized number prediction](https://doi.org/10.18653/v1/2020.emnlp-main.385).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 4754–4764.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berg-Kirkpatrick and Spokoyny (2020) Taylor Berg-Kirkpatrick 和 Daniel Spokoyny.
    2020. [上下文化数字预测的实证研究](https://doi.org/10.18653/v1/2020.emnlp-main.385). 在 *2020
    年自然语言处理实证方法会议 (EMNLP)* 论文集，页码 4754–4764。
- en: Bhattacharya (2017) Arindam Bhattacharya. 2017. [A survey of question answering
    for math and science problem](https://arxiv.org/abs/1705.04530). *arXiv preprint
    arXiv:1705.04530*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhattacharya (2017) Arindam Bhattacharya. 2017. [数学和科学问题的问答调查](https://arxiv.org/abs/1705.04530).
    *arXiv 预印本 arXiv:1705.04530*。
- en: Bobrow (1964) Daniel G Bobrow. 1964. [Natural language input for a computer
    problem solving system](http://dspace.mit.edu/handle/1721.1/5922). *AI Technical
    Reports*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bobrow (1964) Daniel G Bobrow. 1964. [计算机问题解决系统的自然语言输入](http://dspace.mit.edu/handle/1721.1/5922).
    *AI 技术报告*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
    *Advances in Neural Information Processing Systems (NeurIPS)*, 33:1877–1901.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. [语言模型是少量学习者](https://arxiv.org/abs/2005.14165). *神经信息处理系统进展
    (NeurIPS)*，33:1877–1901。
- en: Cao and Xiao (2022) Jie Cao and Jing Xiao. 2022. [An augmented benchmark dataset
    for geometric question answering through dual parallel text encoding](https://aclanthology.org/2022.coling-1.130/).
    In *Proceedings of the 29th International Conference on Computational Linguistics
    (COLING)*, pages 1511–1520.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao and Xiao (2022) Jie Cao 和 Jing Xiao. 2022. [通过双重平行文本编码的几何问题回答增强基准数据集](https://aclanthology.org/2022.coling-1.130/).
    在 *第29届计算语言学国际会议 (COLING)*，页码 1511–1520。
- en: Cao et al. (2021) Yixuan Cao, Feng Hong, Hongwei Li, and Ping Luo. 2021. [A
    bottom-up dag structure extraction model for math word problems](https://ojs.aaai.org/index.php/AAAI/article/view/16075).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, pages
    39–46.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2021) Yixuan Cao, Feng Hong, Hongwei Li, 和 Ping Luo. 2021. [一种用于数学词问题的自下而上DAG结构提取模型](https://ojs.aaai.org/index.php/AAAI/article/view/16075).
    载于*AAAI人工智能会议论文集*，第39–46页。
- en: Charton (2022) François Charton. 2022. [Linear algebra with transformers](https://openreview.net/forum?id=Hp4g7FAXXG).
    *Transactions on Machine Learning Research*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charton (2022) François Charton. 2022. [变压器线性代数](https://openreview.net/forum?id=Hp4g7FAXXG).
    *机器学习研究期刊*。
- en: 'Chen et al. (2022a) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu
    Chen, and Xiaodan Liang. 2022a. [Unigeo: Unifying geometry logical reasoning via
    reformulating mathematical expression](https://lupantech.github.io/papers/emnlp22_unigeo.pdf).
    In *The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2022a) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu
    Chen, 和 Xiaodan Liang. 2022a. [Unigeo: 通过重构数学表达统一几何逻辑推理](https://lupantech.github.io/papers/emnlp22_unigeo.pdf).
    载于*2022年自然语言处理经验方法会议（EMNLP）*。'
- en: 'Chen et al. (2021a) Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,
    Lingbo Liu, Eric Xing, and Liang Lin. 2021a. [Geoqa: A geometric question answering
    benchmark towards multimodal numerical reasoning](https://aclanthology.org/2021.findings-acl.46.pdf).
    In *Findings of the Association for Computational Linguistics (ACL)*, pages 513–523.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2021a) Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,
    Lingbo Liu, Eric Xing, 和 Liang Lin. 2021a. [Geoqa: 一种面向多模态数值推理的几何问题回答基准](https://aclanthology.org/2021.findings-acl.46.pdf).
    载于*计算语言学协会发现论文集（ACL）*，第513–523页。'
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021b. [Evaluating large language models trained on code](https://arxiv.org/abs/2107.03374).
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, 等人. 2021b. [评估在代码上训练的大型语言模型](https://arxiv.org/abs/2107.03374).
    *arXiv 预印本 arXiv:2107.03374*。
- en: 'Chen et al. (2022b) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    2022b. [Program of thoughts prompting: Disentangling computation from reasoning
    for numerical reasoning tasks](https://arxiv.org/abs/2211.12588). *arXiv preprint
    arXiv:2211.12588*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2022b) Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W Cohen.
    2022b. [思维提示程序: 为数值推理任务解开计算与推理的关系](https://arxiv.org/abs/2211.12588). *arXiv 预印本
    arXiv:2211.12588*。'
- en: 'Chen et al. (2023) Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu
    Xu, Tony Xia, Xinyi Wang, and Pan Lu. 2023. Theoremqa: A theorem-driven question
    answering dataset. *arXiv preprint arXiv:2305.12524*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023) Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu
    Xu, Tony Xia, Xinyi Wang, 和 Pan Lu. 2023. Theoremqa: 一个基于定理的问答数据集。*arXiv 预印本 arXiv:2305.12524*。'
- en: 'Chen et al. (2021c) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana
    Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge,
    et al. 2021c. [Finqa: A dataset of numerical reasoning over financial data](https://arxiv.org/abs/2109.00122).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 3697–3711.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2021c) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana
    Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge,
    等人. 2021c. [Finqa: 一个关于金融数据的数值推理数据集](https://arxiv.org/abs/2109.00122). 载于*2021年自然语言处理经验方法会议（EMNLP）论文集*，第3697–3711页。'
- en: 'Chen et al. (2022c) Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena
    Shah, and William Yang Wang. 2022c. [Convfinqa: Exploring the chain of numerical
    reasoning in conversational finance question answering](https://arxiv.org/abs/2210.03849).
    *arXiv preprint arXiv:2210.03849*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2022c) Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena
    Shah, 和 William Yang Wang. 2022c. [Convfinqa: 探索对话金融问答中的数值推理链](https://arxiv.org/abs/2210.03849).
    *arXiv 预印本 arXiv:2210.03849*。'
- en: 'Chiang and Chen (2019) Ting-Rui Chiang and Yun-Nung Chen. 2019. [Semantically-aligned
    equation generation for solving and reasoning math word problems](https://aclanthology.org/N19-1272/).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*, pages
    2656–2668.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang and Chen (2019) Ting-Rui Chiang 和 Yun-Nung Chen. 2019. [语义对齐方程生成以解决和推理数学词问题](https://aclanthology.org/N19-1272/).
    载于*2019年北美计算语言学协会: 人类语言技术会议（NAACL-HLT）论文集*，第2656–2668页。'
- en: Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. [Unifying
    vision-and-language tasks via text generation](https://proceedings.mlr.press/v139/cho21a.html).
    In *Proceedings of the 38th International Conference on Machine Learning (ICML)*,
    pages 1931–1942.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等 (2021) Jaemin Cho, Jie Lei, Hao Tan, 和 Mohit Bansal. 2021. [通过文本生成统一视觉与语言任务](https://proceedings.mlr.press/v139/cho21a.html).
    见 *第38届国际机器学习会议（ICML）论文集*，第1931–1942页。
- en: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares Holger Schwenk, and Yoshua Bengio. 2014. [Learning phrase
    representations using rnn encoder–decoder for statistical machine translation](https://aclanthology.org/D14-1179/).
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 1724–1734.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等 (2014) Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, 和 Yoshua Bengio. 2014. [使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译](https://aclanthology.org/D14-1179/).
    见 *2014年自然语言处理经验方法会议（EMNLP）论文集*，第1724–1734页。
- en: Chou et al. (1996) Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong Zhang. 1996.
    [Automated generation of readable proofs with geometric invariants](https://link.springer.com/article/10.1007/BF00283133).
    *Journal of Automated Reasoning*, 17(3):325–347.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chou 等 (1996) Shang-Ching Chou, Xiao-Shan Gao, 和 Jing-Zhong Zhang. 1996. [利用几何不变生成可读证明](https://link.springer.com/article/10.1007/BF00283133).
    *自动推理杂志*，17(3):325–347。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. [Palm: Scaling language modeling with pathways](https://arxiv.org/abs/2204.02311).
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等 (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等. 2022. [Palm：通过路径扩展语言建模](https://arxiv.org/abs/2204.02311).
    *arXiv 预印本 arXiv:2204.02311*。
- en: 'Clark et al. (2020) Peter Clark, Oren Etzioni, Tushar Khot, Daniel Khashabi,
    Bhavana Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord,
    Niket Tandon, et al. 2020. [From ‘f’to ‘a’on the ny regents science exams: An
    overview of the aristo project](https://arxiv.org/abs/1909.01958). *AI Magazine*,
    41(4):39–53.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2020) Peter Clark, Oren Etzioni, Tushar Khot, Daniel Khashabi, Bhavana
    Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord,
    Niket Tandon 等. 2020. [从‘f’到‘a’在纽约州科学考试中的表现：Aristo 项目概述](https://arxiv.org/abs/1909.01958).
    *AI 杂志*，41(4):39–53。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton,
    Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers
    to solve math word problems](https://arxiv.org/abs/2110.14168). *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等 (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton,
    Reiichiro Nakano, Christopher Hesse, 和 John Schulman. 2021. [训练验证器以解决数学文字题](https://arxiv.org/abs/2110.14168).
    *arXiv 预印本 arXiv:2110.14168*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT)*, pages 4171–4186.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. [BERT：用于语言理解的深度双向变换器预训练](https://doi.org/10.18653/v1/N19-1423). 见 *2019年北美计算语言学协会：人类语言技术会议（NAACL-HLT）论文集*，第4171–4186页。
- en: 'Dua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky,
    Sameer Singh, and Matt Gardner. 2019. [Drop: A reading comprehension benchmark
    requiring discrete reasoning over paragraphs](https://aclanthology.org/N19-1246/).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*, pages
    2368–2378.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua 等 (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer
    Singh, 和 Matt Gardner. 2019. [Drop：需要对段落进行离散推理的阅读理解基准](https://aclanthology.org/N19-1246/).
    见 *2019年北美计算语言学协会：人类语言技术会议（NAACL-HLT）论文集*，第2368–2378页。
- en: Feigenbaum et al. (1963) Edward A Feigenbaum et al. 1963. [*Computers and thought*](https://mitpress.mit.edu/9780262691338/computers-and-thought/).
    McGraw-Hill.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feigenbaum 等 (1963) Edward A Feigenbaum 等. 1963. [*计算机与思想*](https://mitpress.mit.edu/9780262691338/computers-and-thought/).
    McGraw-Hill.
- en: Feng et al. (2021) Yu Feng, Jing Zhang, Xiaokang Zhang, Lemao Liu, Cuiping Li,
    and Hong Chen. 2021. [Injecting numerical reasoning skills into knowledge base
    question answering models](https://arxiv.org/abs/2112.06109). *arXiv preprint
    arXiv:2112.06109*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2021) 冯宇、张静、张晓康、刘乐茂、李翠平和陈洪。2021年。[将数字推理技能注入知识库问答模型](https://arxiv.org/abs/2112.06109)。*arXiv预印本
    arXiv:2112.06109*。
- en: 'Ferreira and Freitas (2020a) Deborah Ferreira and André Freitas. 2020a. [Natural
    language premise selection: Finding supporting statements for mathematical text](https://aclanthology.org/2020.lrec-1.266).
    In *Proceedings of the Twelfth Language Resources and Evaluation Conference*,
    pages 2175–2182.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira and Freitas (2020a) 黛博拉·费雷拉和安德烈·弗雷塔斯。2020a年。[自然语言前提选择：寻找数学文本的支持性陈述](https://aclanthology.org/2020.lrec-1.266)。在*第十二届语言资源与评估会议论文集*，第2175–2182页。
- en: Ferreira and Freitas (2020b) Deborah Ferreira and André Freitas. 2020b. [Premise
    selection in natural language mathematical texts](https://doi.org/10.18653/v1/2020.acl-main.657).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 7365–7374.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira and Freitas (2020b) 黛博拉·费雷拉和安德烈·弗雷塔斯。2020b年。[自然语言数学文本中的前提选择](https://doi.org/10.18653/v1/2020.acl-main.657)。在*第58届计算语言学协会（ACL）年会论文集*，第7365–7374页。
- en: Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2023. [Complexity-based prompting for multi-step reasoning](https://arxiv.org/abs/2210.00720).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) 傅尧、彭浩、阿希什·萨布哈瓦尔、彼得·克拉克和图沙尔·科特。2023年。[基于复杂性的多步推理提示](https://arxiv.org/abs/2210.00720)。在*国际学习表示会议（ICLR）*。
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.
    2020. [The pile: An 800gb dataset of diverse text for language modeling](https://arxiv.org/abs/2101.00027).
    *arXiv preprint arXiv:2101.00027*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2020) 高乐、斯特拉·比德曼、西德·布莱克、劳伦斯·戈尔丁、特拉维斯·霍普、查尔斯·福斯特、贾森·方、霍勒斯·赫、阿尼什·蒂特、诺亚·纳贝希马等。2020年。[The
    pile: 一个800GB的多样化文本数据集用于语言建模](https://arxiv.org/abs/2101.00027)。*arXiv预印本 arXiv:2101.00027*。'
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2022. [Pal: Program-aided language
    models](https://arxiv.org/abs/2211.10435). *arXiv preprint arXiv:2211.10435*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2022) 高路宇、阿曼·马丹、舒延·周、乌里·阿隆、彭飞·刘、易铭·杨、杰米·卡伦和格雷厄姆·纽比。2022年。[Pal:
    程序辅助语言模型](https://arxiv.org/abs/2211.10435)。*arXiv预印本 arXiv:2211.10435*。'
- en: Gao et al. (2019) Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi,
    Xiaogang Wang, and Hongsheng Li. 2019. [Dynamic fusion with intra-and inter-modality
    attention flow for visual question answering](https://arxiv.org/abs/1812.05252).
    In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages
    6639–6648.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2019) 高鹏、郑凯·姜、郝轩·游、潘璐、史蒂文·CH·霍伊、肖岗·王和洪生·李。2019年。[动态融合的视觉问答中的内部和跨模态注意流](https://arxiv.org/abs/1812.05252)。在*IEEE计算机视觉与模式识别会议（CVPR）*，第6639–6648页。
- en: 'Gauthier et al. (2021) Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana
    Kumar, and Michael Norrish. 2021. [TacticToe: Learning to Prove with Tactics](https://doi.org/10.1007/s10817-020-09580-x).
    *Journal of Automated Reasoning*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gauthier et al. (2021) 提博·戈蒂耶、切扎里·卡利瑟克、约瑟夫·厄本、拉马纳·库马尔和迈克尔·诺里什。2021年。[TacticToe:
    用战术进行证明学习](https://doi.org/10.1007/s10817-020-09580-x)。*自动推理期刊*。'
- en: Gehring et al. (2017) Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
    and Yann N Dauphin. 2017. [Convolutional sequence to sequence learning](https://arxiv.org/abs/1705.03122).
    In *International conference on machine learning (ICML)*, pages 1243–1252\. PMLR.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring et al. (2017) 乔纳斯·盖尔林、迈克尔·奥利、大卫·格兰吉尔、德尼斯·亚拉茨和扬·N·道芬。2017年。[卷积序列到序列学习](https://arxiv.org/abs/1705.03122)。在*国际机器学习会议（ICML）*，第1243–1252页。PMLR。
- en: Gelernter et al. (1960) Herbert Gelernter, James R Hansen, and Donald W Loveland.
    1960. [Empirical explorations of the geometry theorem machine](https://dl.acm.org/doi/10.1145/1460361.1460381).
    In *Papers presented at the May 3-5, 1960, western joint IRE-AIEE-ACM computer
    conference*, pages 143–149.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelernter et al. (1960) 赫伯特·盖伦特、詹姆斯·R·汉森和唐纳德·W·洛夫兰德。1960年。[几何定理机的实证探索](https://dl.acm.org/doi/10.1145/1460361.1460381)。在*1960年5月3-5日的西部联合IRE-AIEE-ACM计算机会议论文集*，第143–149页。
- en: Geva et al. (2020) Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. [Injecting
    numerical reasoning skills into language models](https://arxiv.org/abs/2004.04487).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 946–958.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gimpel et al. (2010) Kevin Gimpel, Dipanjan Das, and Noah A Smith. 2010. [Distributed
    asynchronous online learning for natural language processing](https://aclanthology.org/W10-2925/).
    In *Proceedings of the Fourteenth Conference on Computational Natural Language
    Learning*, pages 213–222.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glaese et al. (2022) Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides,
    Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe
    Thacker, et al. 2022. [Improving alignment of dialogue agents via targeted human
    judgements](https://arxiv.org/abs/2209.14375). *arXiv preprint arXiv:2209.14375*.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grabowski et al. (2015) Adam Grabowski, Artur Korniłowicz, and Adam Naumowicz.
    2015. [Four decades of mizar](https://dl.acm.org/doi/abs/10.1007/s10817-015-9345-1).
    *Journal of Automated Reasoning*, 55(3):191–198.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. [Retrieval augmented language model pre-training](https://arxiv.org/abs/2002.08909).
    In *International Conference on Machine Learning (ICML)*, pages 3929–3938\. PMLR.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers,
    and Stanislas Polu. 2022. [Proof artifact co-training for theorem proving with
    language models](https://arxiv.org/abs/2102.06203). In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2022) Yihan Hao, Mingliang Zhang, Fei Yin, and Linlin Huang. 2022.
    [Pgdp5k: A diagram parsing dataset for plane geometry problems](https://arxiv.org/abs/2205.09947).
    In *26th International Conference on Pattern Recognition (ICPR)*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385).
    In *Proceedings of the IEEE conference on computer vision and pattern recognition
    (CVPR)*, pages 770–778.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. [Measuring massive multitask
    language understanding](https://arxiv.org/abs/2009.03300). In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
    Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. [Measuring
    mathematical problem solving with the math dataset](https://arxiv.org/abs/2103.03874).
    In *35th Conference on Neural Information Processing Systems (NeurIPS) Track on
    Datasets and Benchmarks*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic,
    Rishabh Krishnan, and Dawn Song. 2020. [Pretrained transformers improve out-of-distribution
    robustness](https://arxiv.org/abs/2004.06100). In *Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics (ACL)*, pages 2744–2751.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2020) Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic,
    Rishabh Krishnan, 和 Dawn Song. 2020. [预训练的变换器提高了对分布外样本的鲁棒性](https://arxiv.org/abs/2004.06100)。在
    *第58届计算语言学协会年会（ACL）会议论文集* 中，第2744–2751页。
- en: 'Herzig et al. (2020) Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller,
    Francesco Piccinno, and Julian Eisenschlos. 2020. [Tapas: Weakly supervised table
    parsing via pre-training](https://arxiv.org/abs/2004.02349). In *Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*,
    pages 4320–4333.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herzig et al. (2020) Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller,
    Francesco Piccinno, 和 Julian Eisenschlos. 2020. [Tapas：通过预训练的弱监督表格解析](https://arxiv.org/abs/2004.02349)。在
    *第58届计算语言学协会年会（ACL）会议论文集* 中，第4320–4333页。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    [Long short-term memory](https://ieeexplore.ieee.org/abstract/document/6795963).
    *Neural computation*, 9(8):1735–1780.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    [长短期记忆](https://ieeexplore.ieee.org/abstract/document/6795963)。*神经计算*，9(8):1735–1780。
- en: 'Hong et al. (2021a) Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, and Song-Chun
    Zhu. 2021a. [Learning by fixing: Solving math word problems with weak supervision](https://arxiv.org/abs/2012.10582).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, pages
    4959–4967.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. (2021a) Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, 和 Song-Chun
    Zhu. 2021a. [通过修复学习：使用弱监督解决数学文字问题](https://arxiv.org/abs/2012.10582)。在 *AAAI人工智能会议论文集*
    中，第4959–4967页。
- en: 'Hong et al. (2021b) Yining Hong, Qing Li, Ran Gong, Daniel Ciao, Siyuan Huang,
    and Song-Chun Zhu. 2021b. [Smart: A situation model for algebra story problems
    via attributed grammar](https://arxiv.org/abs/2012.14011). In *AAAI*, pages 13009–13017.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. (2021b) Yining Hong, Qing Li, Ran Gong, Daniel Ciao, Siyuan Huang,
    和 Song-Chun Zhu. 2021b. [Smart：通过属性语法为代数故事问题构建情境模型](https://arxiv.org/abs/2012.14011)。在
    *AAAI* 中，第13009–13017页。
- en: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    and Nate Kushman. 2014. [Learning to solve arithmetic word problems with verb
    categorization](https://aclanthology.org/D14-1058). In *Proceedings of the 2014
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    和 Nate Kushman. 2014. [通过动词分类学习解决算术文字问题](https://aclanthology.org/D14-1058)。在
    *2014年自然语言处理实证方法会议（EMNLP）* 中。
- en: 'Huang et al. (2019) Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever.
    2019. [Gamepad: A learning environment for theorem proving](https://arxiv.org/abs/1806.00608).
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2019) Daniel Huang, Prafulla Dhariwal, Dawn Song, 和 Ilya Sutskever.
    2019. [Gamepad：用于定理证明的学习环境](https://arxiv.org/abs/1806.00608)。在 *国际学习表征会议（ICLR）*
    中。
- en: Huang et al. (2018) Danqing Huang, Jing Liu, Chin-Yew Lin, and Jian Yin. 2018.
    [Neural math word problem solver with reinforcement learning](https://aclanthology.org/C18-1018/).
    In *Proceedings of the 27th International Conference on Computational Linguistics
    (COLING)*, pages 213–223.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2018) Danqing Huang, Jing Liu, Chin-Yew Lin, 和 Jian Yin. 2018.
    [带有强化学习的神经数学文字问题求解器](https://aclanthology.org/C18-1018/)。在 *第27届国际计算语言学会议（COLING）*
    中，第213–223页。
- en: Huang et al. (2017) Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian Yin.
    2017. [Learning fine-grained expressions to solve math word problems](https://aclanthology.org/D17-1084/).
    In *Proceedings of Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 805–814.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) Danqing Huang, Shuming Shi, Chin-Yew Lin, 和 Jian Yin. 2017.
    [学习精细化表达以解决数学文字问题](https://aclanthology.org/D17-1084/)。在 *自然语言处理实证方法会议（EMNLP）*
    中，第805–814页。
- en: Huang et al. (2016) Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and
    Wei-Ying Ma. 2016. [How well do computers solve math word problems? large-scale
    dataset construction and evaluation](https://aclanthology.org/P16-1084/). In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*,
    pages 887–896.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2016) Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, 和 Wei-Ying
    Ma. 2016. [计算机解决数学文字问题的能力如何？大规模数据集的构建与评估](https://aclanthology.org/P16-1084/)。在
    *第54届计算语言学协会年会（ACL）会议论文集* 中，第887–896页。
- en: 'Jiang et al. (2022a) Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li,
    Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample.
    2022a. [Draft, sketch, and prove: Guiding formal theorem provers with informal
    proofs](https://arxiv.org/abs/2210.12283). In *Submitted to The Eleventh International
    Conference on Learning Representations*.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2022a) Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li,
    Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, 和 Guillaume Lample.
    2022a. [草稿、素描和证明：用非正式证明引导形式定理证明器](https://arxiv.org/abs/2210.12283)。在 *提交至第十一届国际学习表征会议*。
- en: 'Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and
    Yuhuai Wu. 2021. [Lisa: Language models of isabelle proofs](http://aitp-conference.org/2021/abstract/paper_17.pdf).
    In *6th Conference on Artificial Intelligence and Theorem Proving (AITP)*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, 和 Yuhuai
    Wu. 2021. [Lisa: Isabelle 证明的语言模型](http://aitp-conference.org/2021/abstract/paper_17.pdf)。在
    *第六届人工智能与定理证明会议（AITP）* 上。'
- en: 'Jiang et al. (2022b) Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad
    Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. 2022b.
    Thor: Wielding hammers to integrate language models and automated theorem provers.
    *Advances in Neural Information Processing Systems (NeurIPS)*, 35:8360–8373.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2022b) Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad
    Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, 和 Mateja Jamnik. 2022b.
    Thor: 使用锤子将语言模型与自动定理证明器整合。*神经信息处理系统进展（NeurIPS）*，35:8360–8373。'
- en: 'Jie et al. (2022) Zhanming Jie, Jierui Li, and Wei Lu. 2022. [Learning to reason
    deductively: Math word problem solving as complex relation extraction](https://arxiv.org/abs/2203.10316).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 5944–5955.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jie et al. (2022) Zhanming Jie, Jierui Li, 和 Wei Lu. 2022. [学习演绎推理：将数学问题解决视为复杂关系提取](https://arxiv.org/abs/2203.10316)。在
    *第60届计算语言学协会年会（ACL）* 论文集中，5944–5955页。
- en: 'Jung et al. (2022) Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra
    Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. [Maieutic prompting: Logically
    consistent reasoning with recursive explanations](https://aclanthology.org/2022.emnlp-main.82).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 1266–1279.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jung et al. (2022) Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra
    Bhagavatula, Ronan Le Bras, 和 Yejin Choi. 2022. [Maieutic prompting: 具有递归解释的逻辑一致推理](https://aclanthology.org/2022.emnlp-main.82)。在
    *2022年自然语言处理经验方法会议（EMNLP）* 论文集中，1266–1279页。'
- en: 'Kafle et al. (2018) Kushal Kafle, Brian Price, Scott Cohen, and Christopher
    Kanan. 2018. [Dvqa: Understanding data visualizations via question answering](https://arxiv.org/abs/1801.08163).
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, pages 5648–5656.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kafle et al. (2018) Kushal Kafle, Brian Price, Scott Cohen, 和 Christopher Kanan.
    2018. [Dvqa: 通过问答理解数据可视化](https://arxiv.org/abs/1801.08163)。在 *IEEE计算机视觉与模式识别会议（CVPR）*
    论文集中，5648–5656页。'
- en: 'Kahou et al. (2018) Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson,
    Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2018. [Figureqa: An annotated figure
    dataset for visual reasoning](https://arxiv.org/abs/1710.07300). In *International
    Conference on Learning Representations (ICLR)*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kahou et al. (2018) Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson,
    Ákos Kádár, Adam Trischler, 和 Yoshua Bengio. 2018. [Figureqa: 一个用于视觉推理的注释图像数据集](https://arxiv.org/abs/1710.07300)。在
    *国际学习表征会议（ICLR）* 上。'
- en: 'Kaliszyk et al. (2017) Cezary Kaliszyk, François Chollet, and Christian Szegedy.
    2017. [Holstep: A machine learning dataset for higher-order logic theorem proving](https://arxiv.org/abs/1703.00426).
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaliszyk et al. (2017) Cezary Kaliszyk, François Chollet, 和 Christian Szegedy.
    2017. [Holstep: 高阶逻辑定理证明的机器学习数据集](https://arxiv.org/abs/1703.00426)。在 *国际学习表征会议（ICLR）*
    上。'
- en: 'Kalyan et al. (2021) Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish
    Sabharwal, and Peter Clark. 2021. [How much coffee was consumed during emnlp 2019?
    fermi problems: A new reasoning challenge for ai](https://arxiv.org/abs/2110.14207).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 7318–7328.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalyan et al. (2021) Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish
    Sabharwal, 和 Peter Clark. 2021. [在EMNLP 2019期间消耗了多少咖啡？费米问题：AI的新推理挑战](https://arxiv.org/abs/2110.14207)。在
    *2021年自然语言处理经验方法会议（EMNLP）* 论文集中，7318–7328页。
- en: Kandpal et al. (2022) Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, and
    Colin Raffel. 2022. [Large language models struggle to learn long-tail knowledge](https://arxiv.org/abs/2211.08411).
    *ArXiv*, abs/2211.08411.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandpal et al. (2022) Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, 和
    Colin Raffel. 2022. [大型语言模型在学习长尾知识方面挣扎](https://arxiv.org/abs/2211.08411)。*ArXiv*，abs/2211.08411。
- en: 'Khashabi et al. (2020) Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal,
    Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. [Unifiedqa: Crossing
    format boundaries with a single qa system](https://arxiv.org/abs/2005.00700).
    In *Findings of the Association for Computational Linguistics (EMNLP)*, pages
    1896–1907.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khashabi et al. (2020) Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal,
    Oyvind Tafjord, Peter Clark, 和 Hannaneh Hajishirzi. 2020. [Unifiedqa：通过单一qa系统跨越格式边界](https://arxiv.org/abs/2005.00700)。在
    *计算语言学协会会议记录（EMNLP）*，第1896–1907页。
- en: 'Khot et al. (2022) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, and Ashish Sabharwal. 2022. [Decomposed prompting: A
    modular approach for solving complex tasks](https://arxiv.org/abs/2210.02406).
    *arXiv preprint arXiv:2210.02406*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khot et al. (2022) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, 和 Ashish Sabharwal. 2022. [分解提示：解决复杂任务的模块化方法](https://arxiv.org/abs/2210.02406)。*arXiv
    预印本 arXiv:2210.02406*。
- en: 'Kim et al. (2020) Bugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gahgene Gweon.
    2020. [Point to the expression: Solving algebraic word problems using the expression-pointer
    transformer model](https://aclanthology.org/2020.emnlp-main.308/). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 3768–3779.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2020) Bugeun Kim, Kyung Seo Ki, Donggeon Lee, 和 Gahgene Gweon. 2020.
    [指向表达式：使用表达式指针转换器模型解决代数词问题](https://aclanthology.org/2020.emnlp-main.308/)。在 *2020年自然语言处理经验方法会议（EMNLP）论文集*，第3768–3779页。
- en: Kim et al. (2018) Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. [Bilinear
    attention networks](https://arxiv.org/abs/1805.07932). In *Advances in Neural
    Information Processing Systems (NeurIPS)*, pages 1571–1581.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2018) Jin-Hwa Kim, Jaehyun Jun, 和 Byoung-Tak Zhang. 2018. [双线性注意力网络](https://arxiv.org/abs/1805.07932)。在
    *神经信息处理系统进展（NeurIPS）*，第1571–1581页。
- en: 'Kim et al. (2021) Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. [Vilt: Vision-and-language
    transformer without convolution or region supervision](https://arxiv.org/abs/2102.03334).
    In *Proceedings of the 38th International Conference on Machine Learning (ICML)*,
    pages 5583–5594.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2021) Wonjae Kim, Bokyung Son, 和 Ildoo Kim. 2021. [Vilt：没有卷积或区域监督的视觉与语言转换器](https://arxiv.org/abs/2102.03334)。在
    *第38届国际机器学习大会（ICML）论文集*，第5583–5594页。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://arxiv.org/abs/2205.11916).
    In *36th Conference on Neural Information Processing Systems (NeurIPS)*.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, 和 Yusuke Iwasawa. 2022. [大型语言模型是零-shot推理者](https://arxiv.org/abs/2205.11916)。在
    *第36届神经信息处理系统会议（NeurIPS）*。
- en: 'Koncel-K. et al. (2016) Rik Koncel-K., Subhro Roy, Aida Amini, Nate Kushman,
    and Hannaneh Hajishirzi. 2016. [Mawps: A math word problem repository](https://aclanthology.org/N16-1136/).
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL)*, pages 1152–1157.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-K. et al. (2016) Rik Koncel-K., Subhro Roy, Aida Amini, Nate Kushman,
    和 Hannaneh Hajishirzi. 2016. [Mawps：一个数学词问题库](https://aclanthology.org/N16-1136/)。在
    *2016年北美计算语言学协会：人类语言技术会议（NAACL）论文集*，第1152–1157页。
- en: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. [Parsing algebraic
    word problems into equations](https://aclanthology.org/Q15-1042/). *Transactions
    of the Association for Computational Linguistics (TACL)*, 3:585–597.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, 和 Siena Dumas Ang. 2015. [将代数词问题解析为方程](https://aclanthology.org/Q15-1042/)。*计算语言学协会会刊（TACL）*，3:585–597。
- en: 'Krishna et al. (2021) Kundan Krishna, Jeffrey Bigham, and Zachary C Lipton.
    2021. [Does pretraining for summarization require knowledge transfer?](https://aclanthology.org/2021.findings-emnlp.273)
    In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages
    3178–3189.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna et al. (2021) Kundan Krishna, Jeffrey Bigham, 和 Zachary C Lipton. 2021.
    [预训练总结是否需要知识迁移？](https://aclanthology.org/2021.findings-emnlp.273) 在 *计算语言学协会会议记录：EMNLP
    2021*，第3178–3189页。
- en: Kushman et al. (2014) Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina
    Barzilay. 2014. [Learning to automatically solve algebra word problems](https://aclanthology.org/P14-1026/).
    In *Proceedings of the 52nd Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 271–281.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kushman et al. (2014) Nate Kushman, Yoav Artzi, Luke Zettlemoyer, 和 Regina Barzilay.
    2014. [学习自动解决代数问题](https://aclanthology.org/P14-1026/)。发表于 *第52届计算语言学协会年会论文集（ACL）*，第271–281页。
- en: Lample and Charton (2020) Guillaume Lample and François Charton. 2020. [Deep
    learning for symbolic mathematics](https://arxiv.org/abs/1912.01412). In *International
    Conference on Learning Representations (ICLR)*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample and Charton (2020) Guillaume Lample 和 François Charton. 2020. [符号数学的深度学习](https://arxiv.org/abs/1912.01412)。发表于
    *国际学习表征会议（ICLR）*。
- en: Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux,
    Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.
    2022. Hypertree proof search for neural theorem proving. *Advances in Neural Information
    Processing Systems (NeurIPS)*, 35:26337–26349.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux,
    Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, 和 Xavier Martinet.
    2022. Hypertree证明搜索用于神经定理证明。*神经信息处理系统进展（NeurIPS）*，35:26337–26349。
- en: 'Lan et al. (2022) Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian
    Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2022. [Mwptoolkit: an open-source
    framework for deep learning-based math word problem solvers](https://arxiv.org/abs/2109.00799).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, pages
    13188–13190.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2022) Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian
    Dai, Yan Wang, Dongxiang Zhang, 和 Ee-Peng Lim. 2022. [Mwptoolkit: 一个开源的深度学习数学问题求解框架](https://arxiv.org/abs/2109.00799)。发表于
    *AAAI人工智能会议论文集（AAAI）*，第13188–13190页。'
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. [Albert: A lite bert for self-supervised
    learning of language representations](https://arxiv.org/abs/1909.11942). *arXiv
    preprint arXiv:1909.11942*.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, 和 Radu Soricut. 2019. [Albert: 一种用于自监督语言表示学习的轻量级BERT](https://arxiv.org/abs/1909.11942)。*arXiv预印本
    arXiv:1909.11942*。'
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/document/726791).
    *Proceedings of the IEEE*, 86(11):2278–2324.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner.
    1998. [基于梯度的学习应用于文档识别](https://ieeexplore.ieee.org/document/726791)。发表于 *IEEE会议论文集*，86(11):2278–2324。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    [BART: Denoising sequence-to-sequence pre-training for natural language generation,
    translation, and comprehension](https://arxiv.org/abs/1910.13461). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*,
    pages 7871–7880.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, 和 Luke Zettlemoyer. 2020. [BART:
    用于自然语言生成、翻译和理解的去噪序列到序列预训练](https://arxiv.org/abs/1910.13461)。发表于 *第58届计算语言学协会年会论文集（ACL）*，第7871–7880页。'
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Johan Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil,
    Imanol Schlag, Theo Gutman-Solo, et al. 2022. [Solving quantitative reasoning
    problems with language models](https://arxiv.org/abs/2206.14858). In *Advances
    in Neural Information Processing Systems (NeurIPS)*.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Johan Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil,
    Imanol Schlag, Theo Gutman-Solo, 等. 2022. [使用语言模型解决定量推理问题](https://arxiv.org/abs/2206.14858)。发表于
    *神经信息处理系统进展（NeurIPS）*。
- en: Li et al. (2019) Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai,
    and Dongxiang Zhang. 2019. [Modeling intra-relation in math word problems with
    different functional multi-head attentions](https://aclanthology.org/P19-1619/).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 6162–6167.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai,
    和 Dongxiang Zhang. 2019. [使用不同功能的多头注意力模型化数学问题中的内部关系](https://aclanthology.org/P19-1619/)。发表于
    *第57届计算语言学协会年会论文集（ACL）*，第6162–6167页。
- en: Li et al. (2017) Jiwei Li, Alexander H Miller, Sumit Chopra, Marc’Aurelio Ranzato,
    and Jason Weston. 2017. [Dialogue learning with human-in-the-loop](https://arxiv.org/abs/1611.09823).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2017）Jiwei Li、Alexander H Miller、Sumit Chopra、Marc’Aurelio Ranzato 和 Jason
    Weston。2017年。[与人类环节的对话学习](https://arxiv.org/abs/1611.09823)。在 *国际学习表征会议（ICLR）*
    上。
- en: Li et al. (2020a) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2020a. [What does bert with vision look at?](https://aclanthology.org/2020.acl-main.469/)
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 5265–5275.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020a）Liunian Harold Li、Mark Yatskar、Da Yin、Cho-Jui Hsieh 和 Kai-Wei Chang。2020a。[BERT
    与视觉的结合是什么样的？](https://aclanthology.org/2020.acl-main.469/) 在 *计算语言学协会第58届年会（ACL）*
    上，页面 5265–5275。
- en: Li et al. (2020b) Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan
    Xu, and Sheng Zhong. 2020b. [Graph-to-tree neural networks for learning structured
    input-output translation with applications to semantic parsing and math word problem](https://arxiv.org/abs/2004.13781).
    In *Findings of the Association for Computational Linguistics (EMNLP)*, pages
    2841–2852.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020b）Shucheng Li、Lingfei Wu、Shiwei Feng、Fangli Xu、Fengyuan Xu 和 Sheng
    Zhong。2020b。[图到树神经网络：用于学习结构化输入输出翻译的应用，涉及语义解析和数学问题](https://arxiv.org/abs/2004.13781)。在
    *计算语言学协会成果（EMNLP）* 上，页面 2841–2852。
- en: 'Li et al. (2021) Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. 2021.
    [Isarstep: a benchmark for high-level mathematical reasoning](https://arxiv.org/abs/2006.09265).
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2021）Wenda Li、Lei Yu、Yuhuai Wu 和 Lawrence C Paulson。2021年。[Isarstep:
    高层次数学推理的基准](https://arxiv.org/abs/2006.09265)。在 *国际学习表征会议（ICLR）* 上。'
- en: Li et al. (2022a) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang
    Lou, and Weizhu Chen. 2022a. [On the advance of making language models better
    reasoners](https://arxiv.org/abs/2206.02336). *arXiv preprint arXiv:2206.02336*.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022a）Yifei Li、Zeqi Lin、Shizhuo Zhang、Qiang Fu、Bei Chen、Jian-Guang Lou
    和 Weizhu Chen。2022a。[提升语言模型推理能力的进展](https://arxiv.org/abs/2206.02336)。*arXiv 预印本
    arXiv:2206.02336*。
- en: 'Li et al. (2022b) Zhongli Li, Wenxuan Zhang, Chao Yan, Qingyu Zhou, Chao Li,
    Hongzhi Liu, and Yunbo Cao. 2022b. [Seeking patterns, not just memorizing procedures:
    Contrastive learning for solving math word problems](https://arxiv.org/abs/2110.08464).
    In *Findings of the Association for Computational Linguistics (ACL)*, pages 2486–2496.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022b）Zhongli Li、Wenxuan Zhang、Chao Yan、Qingyu Zhou、Chao Li、Hongzhi Liu
    和 Yunbo Cao。2022b。[寻找模式，而不仅仅是记忆过程：用于解决数学问题的对比学习](https://arxiv.org/abs/2110.08464)。在
    *计算语言学协会成果（ACL）* 上，页面 2486–2496。
- en: Liang et al. (2022a) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022a. [Holistic evaluation of language models](https://arxiv.org/abs/2211.09110).
    *arXiv preprint arXiv:2211.09110*.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2022a）Percy Liang、Rishi Bommasani、Tony Lee、Dimitris Tsipras、Dilara
    Soylu、Michihiro Yasunaga、Yian Zhang、Deepak Narayanan、Yuhuai Wu、Ananya Kumar 等。2022a。[语言模型的全面评估](https://arxiv.org/abs/2211.09110)。*arXiv
    预印本 arXiv:2211.09110*。
- en: 'Liang and Klein (2009) Percy Liang and Dan Klein. 2009. [Online em for unsupervised
    models](https://aclanthology.org/N09-1069/). In *Proceedings of human language
    technologies: The 2009 annual conference of the North American chapter of the
    association for computational linguistics (NAACL)*, pages 611–619.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Klein（2009）Percy Liang 和 Dan Klein。2009年。[无监督模型的在线 EM](https://aclanthology.org/N09-1069/)。在
    *人类语言技术会议：2009年北美计算语言学协会年会（NAACL）* 上，页面 611–619。
- en: 'Liang et al. (2022b) Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi
    Lan, Jie Shao, and Xiangliang Zhang. 2022b. [Mwp-bert: Numeracy-augmented pre-training
    for math word problem solving](https://arxiv.org/abs/2107.13435). In *Findings
    of the Association for Computational Linguistics (NAACL)*, pages 997–1009.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2022b）Zhenwen Liang、Jipeng Zhang、Lei Wang、Wei Qin、Yunshi Lan、Jie Shao
    和 Xiangliang Zhang。2022b。[Mwp-bert：增强数理能力的数学问题解决预训练](https://arxiv.org/abs/2107.13435)。在
    *计算语言学协会成果（NAACL）* 上，页面 997–1009。
- en: 'Lin et al. (2020) Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren.
    2020. [Birds have four legs?! numersense: Probing numerical commonsense knowledge
    of pre-trained language models](https://arxiv.org/abs/2005.00683). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 6862–6868.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2020）Bill Yuchen Lin、Seyeon Lee、Rahul Khanna 和 Xiang Ren。2020年。[鸟有四条腿？！numersense：探测预训练语言模型的数字常识知识](https://arxiv.org/abs/2005.00683)。在
    *2020年自然语言处理经验方法会议（EMNLP）* 上，页面 6862–6868。
- en: 'Lin et al. (2021) Xin Lin, Zhenya Huang, Hongke Zhao, Enhong Chen, Qi Liu,
    Hao Wang, and Shijin Wang. 2021. [Hms: A hierarchical solver with dependency-enhanced
    understanding for math word problem](https://ojs.aaai.org/index.php/AAAI/article/view/16547).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, pages
    4232–4240.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    2017. [Program induction by rationale generation: Learning to solve and explain
    algebraic word problems](https://arxiv.org/abs/1705.04146). In *Proceedings of
    the 55th Annual Meeting of the Association for Computational Linguistics (ACL)*,
    pages 158–167.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan,
    Lawrence Carin, and Weizhu Chen. 2022a. [What makes good in-context examples for
    gpt-3?](https://arxiv.org/abs/2101.06804) In *Proceedings of Deep Learning Inside
    Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for
    Deep Learning Architectures*, pages 100–114.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin,
    Weizhu Chen, and Jian-Guang Lou. 2022b. [TAPEX: Table pre-training via learning
    a neural SQL executor](https://openreview.net/forum?id=O50443AsCP). In *International
    Conference on Learning Representations*.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, Daisuke Kawahara,
    and Sadao Kurohashi. 2020. [Reverse operation based data augmentation for solving
    math word problems](https://arxiv.org/abs/2010.01556). *IEEE Transactions on Audio,
    Speech and Language Processing*.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Qianying Liu, Wenyv Guan, Sujian Li, and Daisuke Kawahara.
    2019a. [Tree-structured decoding for solving math word problems](https://aclanthology.org/D19-1241/).
    In *Proceedings of the 2019 conference on empirical methods in natural language
    processing and the 9th international joint conference on natural language processing
    (EMNLP-IJCNLP)*, pages 2370–2379.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    [Roberta: A robustly optimized bert pretraining approach](https://aclanthology.org/N19-1423).
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HLT)*.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loos et al. (2017) Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary
    Kaliszyk. 2017. [Deep network guided proof search](https://arxiv.org/abs/1701.06972).
    *arXiv preprint arXiv:1701.06972*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021a) Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang,
    Xiaodan Liang, and Song-Chun Zhu. 2021a. [Inter-gps: Interpretable geometry problem
    solving with formal language and symbolic reasoning](https://aclanthology.org/2021.acl-long.528/).
    In *The 59th Annual Meeting of the Association for Computational Linguistics (ACL)*.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022a) Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang,
    Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022a. [Learn to
    explain: Multimodal reasoning via thought chains for science question answering](https://arxiv.org/abs/2209.09513).
    In *The 36th Conference on Neural Information Processing Systems (NeurIPS)*.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2022a）潘璐、斯瓦鲁普·米什拉、托尼·夏、梁秋、凯-魏·张、宋春柱、欧文德·塔福德、彼得·克拉克和阿什温·卡利扬。2022a. [**学会解释：通过思维链进行多模态推理以回答科学问题**](https://arxiv.org/abs/2209.09513)。见于
    *第36届神经信息处理系统会议（NeurIPS）*。
- en: 'Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play
    compositional reasoning with large language models. *arXiv preprint arXiv:2304.09842*.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2023）潘璐、鲍林·彭、郝程、米歇尔·加利、凯-魏·张、英年·吴、宋春柱和简锋·高。2023. **变色龙：具有大型语言模型的即插即用组合推理**。*arXiv
    预印本 arXiv:2304.09842*。
- en: Lu et al. (2022b) Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun
    Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022b. [Dynamic prompt
    learning via policy gradient for semi-structured mathematical reasoning](https://arxiv.org/abs/2209.14610).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2022b）潘璐、梁秋、凯-魏·张、英年·吴、宋春柱、坦梅·拉杰普罗希特、彼得·克拉克和阿什温·卡利扬。2022b. [**通过策略梯度进行动态提示学习以实现半结构化数学推理**](https://arxiv.org/abs/2209.14610)。见于
    *国际学习表征会议（ICLR）*。
- en: 'Lu et al. (2021b) Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei
    Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021b. [Iconqa: A new benchmark
    for abstract diagram understanding and visual language reasoning](https://arxiv.org/abs/2110.13214).
    In *The 35th Conference on Neural Information Processing Systems (NeurIPS) Track
    on Datasets and Benchmarks*.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等（2021b）潘璐、梁秋、贾奇·陈、托尼·夏、易舟·赵、魏章、周宇、肖丹·梁和宋春柱。2021b. [**Iconqa: 一个用于抽象图示理解和视觉语言推理的新基准**](https://arxiv.org/abs/2110.13214)。见于
    *第35届神经信息处理系统会议（NeurIPS）数据集和基准会议*。'
- en: 'Lu et al. (2022c) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2022c. [Fantastically ordered prompts and where to find them:
    Overcoming few-shot prompt order sensitivity](https://arxiv.org/abs/2104.08786).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 8086–8098.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2022c）姚璐、马克斯·巴托洛、阿拉斯泰尔·穆尔、塞巴斯蒂安·里德尔和庞图斯·斯特内托普。2022c. [**奇妙排序的提示及其来源：克服少样本提示顺序敏感性**](https://arxiv.org/abs/2104.08786)。见于
    *第60届计算语言学协会年会（ACL）论文集*，页码 8086–8098。
- en: mathlib Community (2020) The mathlib Community. 2020. [The lean mathematical
    library](https://doi.org/10.1145/3372885.3373824). In *CPP 2020 - Proceedings
    of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs,
    co-located with POPL 2020*.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mathlib 社区（2020）mathlib 社区。2020. [**精简数学库**](https://doi.org/10.1145/3372885.3373824)。见于
    *CPP 2020 - 第9届ACM SIGPLAN国际认证程序和证明会议论文集，与POPL 2020共同举办*。
- en: Meadows and Freitas (2022) Jordan Meadows and Andre Freitas. 2022. [A survey
    in mathematical language processing](https://arxiv.org/abs/2205.15231). *arXiv
    preprint arXiv:2205.15231*.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meadows 和 Freitas（2022）乔丹·梅多斯和安德烈·弗雷塔斯。2022. [**数学语言处理综述**](https://arxiv.org/abs/2205.15231)。*arXiv
    预印本 arXiv:2205.15231*。
- en: 'Megill and Wheeler (2019) Norman D. Megill and David A. Wheeler. 2019. [*Metamath:
    A Computer Language for Mathematical Proofs*](https://us.metamath.org/downloads/metamath.pdf).
    Lulu Press, Morrisville, North Carolina. http://us.metamath.org/downloads/metamath.pdf.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Megill 和 Wheeler（2019）诺曼·D·梅吉尔和大卫·A·惠勒。2019. [**Metamath: 一个用于数学证明的计算机语言**](https://us.metamath.org/downloads/metamath.pdf)。Lulu
    Press，北卡罗来纳州莫里斯维尔。 http://us.metamath.org/downloads/metamath.pdf。'
- en: Meng and Rumshisky (2019) Yuanliang Meng and Anna Rumshisky. 2019. [Solving
    math word problems with double-decoder transformer](https://arxiv.org/abs/1908.10924).
    *arXiv preprint arXiv:1908.10924*.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 和 Rumshisky（2019）袁亮·孟和安娜·鲁姆什基。2019. [**通过双解码器变换器解决数学词题**](https://arxiv.org/abs/1908.10924)。*arXiv
    预印本 arXiv:1908.10924*。
- en: Miao et al. (2020) Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. [A
    diverse corpus for evaluating and developing english math word problem solvers](https://arxiv.org/abs/2106.15772).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 975–984.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等（2020）沈云·苗、赵超春和苏克义。2020. [**用于评估和开发英语数学词题求解器的多样化语料库**](https://arxiv.org/abs/2106.15772)。见于
    *第58届计算语言学协会年会（ACL）论文集*，页码 975–984。
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. [Rethinking the role of demonstrations:
    What makes in-context learning work?](https://arxiv.org/abs/2202.12837) *Proceedings
    of Empirical Methods in Natural Language Processing (EMNLP)*.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min等（2022）Sewon Min、Xinxi Lyu、Ari Holtzman、Mikel Artetxe、Mike Lewis、Hannaneh
    Hajishirzi和Luke Zettlemoyer。2022年。[重新思考演示的作用：是什么使得上下文学习有效？](https://arxiv.org/abs/2202.12837)
    *自然语言处理经验方法会议（EMNLP）论文集*。
- en: 'Minaee et al. (2021) Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes
    Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2021. [Deep learning based text classification:
    a comprehensive review](https://arxiv.org/abs/2004.03705). *ACM Computing Surveys
    (CSUR)*, 54(3):1–40.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee等（2021）Shervin Minaee、Nal Kalchbrenner、Erik Cambria、Narjes Nikzad、Meysam
    Chenaghlu和Jianfeng Gao。2021年。[基于深度学习的文本分类：全面回顾](https://arxiv.org/abs/2004.03705)。*ACM计算调查（CSUR）*，54(3):1–40。
- en: 'Mishra et al. (2022a) Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang,
    Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal,
    Peter Clark, and Ashwin Kalyan. 2022a. [Lila: A unified benchmark for mathematical
    reasoning](https://arxiv.org/abs/2210.17517). In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mishra等（2022a）Swaroop Mishra、Matthew Finlayson、Pan Lu、Leonard Tang、Sean Welleck、Chitta
    Baral、Tanmay Rajpurohit、Oyvind Tafjord、Ashish Sabharwal、Peter Clark和Ashwin Kalyan。2022a。[Lila:
    数学推理的统一基准](https://arxiv.org/abs/2210.17517)。收录于*2022年自然语言处理经验方法会议（EMNLP）论文集*。'
- en: 'Mishra et al. (2022b) Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep
    Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022b. [Numglue: A suite
    of fundamental yet challenging mathematical reasoning tasks](https://aclanthology.org/2022.acl-long.246/).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 3505–3523.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mishra等（2022b）Swaroop Mishra、Arindam Mitra、Neeraj Varshney、Bhavdeep Sachdeva、Peter
    Clark、Chitta Baral和Ashwin Kalyan。2022b。[Numglue: 一套基础而具有挑战性的数学推理任务](https://aclanthology.org/2022.acl-long.246/)。收录于*第60届计算语言学协会年会（ACL）论文集*，页码3505–3523。'
- en: Mitchell et al. (2022) Eric Mitchell, Joseph J. Noh, Siyan Li, William S. Armstrong,
    Ananth Agarwal, Patrick Liu, Chelsea Finn, and Christopher D. Manning. 2022. [Enhancing
    self-consistency and performance of pretrained language models with nli](https://ericmitchell.ai/concord.pdf).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*. Association for Computational Linguistics.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell等（2022）Eric Mitchell、Joseph J. Noh、Siyan Li、William S. Armstrong、Ananth
    Agarwal、Patrick Liu、Chelsea Finn和Christopher D. Manning。2022年。[通过NLI增强预训练语言模型的一致性和性能](https://ericmitchell.ai/concord.pdf)。收录于*2022年自然语言处理经验方法会议（EMNLP）论文集*。计算语言学协会。
- en: Moura et al. (2015) Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris van
    Doorn, and Jakob von Raumer. 2015. [The lean theorem prover (system description)](https://link.springer.com/chapter/10.1007/978-3-319-21401-6_26).
    In *International Conference on Automated Deduction*, pages 378–388\. Springer.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moura等（2015）Leonardo de Moura、Soonho Kong、Jeremy Avigad、Floris van Doorn和Jakob
    von Raumer。2015年。[精简定理证明器（系统描述）](https://link.springer.com/chapter/10.1007/978-3-319-21401-6_26)。收录于*国际自动推理会议*，页码378–388。Springer。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. [Webgpt: Browser-assisted question-answering with
    human feedback](https://arxiv.org/abs/2112.09332). *arXiv preprint arXiv:2112.09332*.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano等（2021）Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina
    Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders等。2021年。[Webgpt:
    浏览器辅助的问答系统与人工反馈](https://arxiv.org/abs/2112.09332)。*arXiv预印本 arXiv:2112.09332*。'
- en: 'Newell et al. (1957) Allen Newell, John Clifford Shaw, and Herbert A Simon.
    1957. [Empirical explorations of the logic theory machine: A case study in heuristic](https://doi.org/10.1145/1455567.1455605).
    In *Proceedings of the Western Joint Computer Conference, IRE-AIEE-ACM 1957*.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell等（1957）Allen Newell、John Clifford Shaw和Herbert A Simon。1957年。[逻辑理论机的经验探索：启发式的案例研究](https://doi.org/10.1145/1455567.1455605)。收录于*1957年西部联合计算机会议论文集，IRE-AIEE-ACM
    1957*。
- en: Ni et al. (2023) Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov,
    Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023. [Learning from self-sampled
    correct and partially-correct programs](https://arxiv.org/abs/2205.14318). In
    *International Conference on Learning Representations (ICLR)*.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. 2021. [Show your work: Scratchpads for intermediate
    computation with language models](https://arxiv.org/abs/2112.00114). *arXiv preprint
    arXiv:2112.00114*.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. [Training language models to follow instructions with human
    feedback](https://arxiv.org/abs/2203.02155). In *Advances in Neural Information
    Processing Systems (NeurIPS)*.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.
    [Are nlp models really able to solve simple math word problems?](https://arxiv.org/abs/2103.07191)
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HIT)*, pages
    2080–2094.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paulson (1994) Lawrence C. Paulson. 1994. [*Isabelle - A Generic Theorem Prover
    (with a contribution by T. Nipkow)*](https://doi.org/10.1007/BFb0030541), volume
    828 of *Lecture Notes in Computer Science*. Springer.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perez et al. (2018) Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin,
    and Aaron Courville. 2018. [Film: Visual reasoning with a general conditioning
    layer](https://arxiv.org/abs/1709.07871). In *Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI)*.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polu et al. (2023) Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys,
    Igor Babuschkin, and Ilya Sutskever. 2023. [Formal mathematics statement curriculum
    learning](https://arxiv.org/abs/2202.01344). In *International Conference on Learning
    Representations (ICLR)*, volume abs/2202.01344.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polu and Sutskever (2020) Stanislas Polu and Ilya Sutskever. 2020. [Generative
    language modeling for automated theorem proving](https://arxiv.org/abs/2009.03393).
    *arXiv preprint arXiv:2009.03393*.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2021) Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, and
    Liang Lin. 2021. [Neural-symbolic solver for math word problems with auxiliary
    tasks](https://arxiv.org/abs/2107.01431). In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (ACL)*, pages 5870–5881.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2020) Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, and Liang
    Lin. 2020. [Semantically-aligned universal tree-structured solver for math word
    problems](https://arxiv.org/abs/2010.06823). In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 3780–3789.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 (2020) Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang 和 Liang Lin.
    2020. [语义对齐的通用树状求解器用于数学文字问题](https://arxiv.org/abs/2010.06823)。发表于 *2020年自然语言处理经验方法会议
    (EMNLP)*，第3780–3789页。
- en: 'Qiu et al. (2022a) Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng,
    Jianfeng Gao, and Song-Chun Zhu. 2022a. [Valuenet: A new dataset for human value
    driven dialogue system](https://arxiv.org/abs/2112.06346). In *Proceedings of
    the AAAI Conference on Artificial Intelligence (AAAI)*, pages 2468–2484.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人 (2022a) Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng
    Gao 和 Song-Chun Zhu. 2022a. [Valuenet: 一个用于人类价值驱动对话系统的新数据集](https://arxiv.org/abs/2112.06346)。发表于
    *AAAI人工智能会议论文集 (AAAI)*，第2468–2484页。'
- en: Qiu et al. (2022b) Liang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan Shi, Zhou
    Yu, and Song-chun Zhu. 2022b. [Towards socially intelligent agents with mental
    state transition and human value](https://arxiv.org/abs/2103.07011). In *Proceedings
    of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue*,
    pages 146–158.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等人 (2022b) Liang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan Shi, Zhou
    Yu 和 Song-chun Zhu. 2022b. [朝着具备心理状态转换和人类价值的社会智能体迈进](https://arxiv.org/abs/2103.07011)。发表于
    *第23届话语与对话特别兴趣组年会*，第146–158页。
- en: 'Qiu et al. (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. 2020. [Pre-trained models for natural language processing:
    A survey](https://arxiv.org/abs/2003.08271). *Science China Technological Sciences*,
    63(10):1872–1897.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人 (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai 和 Xuanjing
    Huang. 2020. [自然语言处理的预训练模型: 综述](https://arxiv.org/abs/2003.08271)。*科学中国技术科学*，63(10):1872–1897。'
- en: Radford et al. (2020) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2020. [Language models are unsupervised multitask
    learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    *OpenAI Blog*.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2020) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等人. 2020. [语言模型是无监督的多任务学习者](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。*OpenAI
    博客*。
- en: 'Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, et al. 2021. [Scaling language models: Methods, analysis & insights from
    training gopher](https://arxiv.org/abs/2112.11446). *arXiv preprint arXiv:2112.11446*.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rae 等人 (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan
    Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young 等人. 2021. [扩展语言模型: 方法、分析与训练 Gopher 的见解](https://arxiv.org/abs/2112.11446)。*arXiv
    预印本 arXiv:2112.11446*。'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/abs/1910.10683).
    *Journal of Machine Learning Research (JMLR)*, 21:1–67.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu. 2020. [通过统一的文本到文本转换器探索迁移学习的极限](https://arxiv.org/abs/1910.10683)。*机器学习研究杂志
    (JMLR)*，21:1–67。
- en: 'Ravichander et al. (2019) Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose,
    and Eduard Hovy. 2019. [Equate: A benchmark evaluation framework for quantitative
    reasoning in natural language inference](https://arxiv.org/abs/1901.03735). In
    *Proceedings of the 23rd Conference on Computational Natural Language Learning
    (CoNLL)*, pages 349–361.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ravichander 等人 (2019) Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose 和
    Eduard Hovy. 2019. [Equate: 一个用于自然语言推理的定量推理基准评估框架](https://arxiv.org/abs/1901.03735)。发表于
    *第23届计算自然语言学习会议 (CoNLL)*，第349–361页。'
- en: 'Razeghi et al. (2022) Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and
    Sameer Singh. 2022. [Impact of pretraining term frequencies on few-shot numerical
    reasoning](https://aclanthology.org/2022.findings-emnlp.59). In *Findings of the
    Association for Computational Linguistics: EMNLP 2022*, pages 840–854.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Razeghi 等人 (2022) Yasaman Razeghi, Robert L Logan IV, Matt Gardner 和 Sameer
    Singh. 2022. [预训练术语频率对少样本数字推理的影响](https://aclanthology.org/2022.findings-emnlp.59)。发表于
    *计算语言学协会会议发现: EMNLP 2022*，第840–854页。'
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    [Faster r-cnn: Towards real-time object detection with region proposal networks](https://arxiv.org/abs/1506.01497).
    *Advances in neural information processing systems (NeurIPS)*, 28.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ri and Tsuruoka (2022) Ryokan Ri and Yoshimasa Tsuruoka. 2022. [Pretraining
    with artificial language: Studying transferable knowledge in language models](https://arxiv.org/abs/2203.10326).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 7302–7315.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robaidek et al. (2018) Benjamin Robaidek, Rik Koncel-Kedziorski, and Hannaneh
    Hajishirzi. 2018. [Data-driven methods for solving algebra word problems](https://arxiv.org/abs/1804.10718).
    *arXiv preprint arXiv:1804.10718*.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2015) Subhro Roy and Dan Roth. 2015. [Solving general arithmetic
    word problems](https://arxiv.org/abs/1608.01413). In *Proceedings of the 2015
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages
    1743–1752.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2017) Subhro Roy and Dan Roth. 2017. [Unit dependency graph and
    its application to arithmetic word problem solving](https://arxiv.org/abs/1612.00969).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2018) Subhro Roy and Dan Roth. 2018. [Mapping to declarative knowledge
    for word problem solving](https://arxiv.org/abs/1712.09391). *Transactions of
    the Association for Computational Linguistics (TACL)*, 6:159–172.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (2015) Subhro Roy, Tim Vieira, and Dan Roth. 2015. [Reasoning about
    quantities in natural language](https://aclanthology.org/Q15-1001/). *Transactions
    of the Association for Computational Linguistics (TACL)*, 3:1–13.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    [Learning to retrieve prompts for in-context learning](https://arxiv.org/abs/2112.08633).
    *North American Chapter of the Association for Computational Linguistics (NAACL)*.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sachan et al. (2017) Mrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017. [From
    textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks
    to solve geometry problems](https://aclanthology.org/D17-1081/). In *Proceedings
    of Empirical Methods in Natural Language Processing (EMNLP)*, pages 773–784.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sachan and Xing (2017) Mrinmaya Sachan and Eric Xing. 2017. [Learning to solve
    geometry problems from natural language demonstrations in textbooks](https://aclanthology.org/S17-1029/).
    In *Proceedings of the 6th Joint Conference on Lexical and Computational Semantics*,
    pages 251–261.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxton et al. (2020) David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet
    Kohli. 2020. [Analysing mathematical reasoning abilities of neural models](https://arxiv.org/abs/1904.01557).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuster et al. (2021) Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman
    Kalai. 2021. [Programming puzzles](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3988c7f88ebcb58c6ce932b957b6f332-Abstract-round1.html).
    In *Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)
    Datasets and Benchmarks Track*.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    [Neural machine translation of rare words with subword units](https://arxiv.org/abs/1508.07909).
    In *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (ACL)*, pages 1715–1725.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seo et al. (2015) Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni,
    and Clint Malcolm. 2015. [Solving geometry problems: Combining text and diagram
    interpretation](https://aclanthology.org/D15-1171/). In *Proceedings of Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 1466–1476.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2021) Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang,
    Ming Zhang, and Qun Liu. 2021. [Generate & rank: A multi-task framework for math
    word problems](https://arxiv.org/abs/2109.03034). In *Findings of the Association
    for Computational Linguistics (EMNLP)*, pages 2269–2279.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen and Jin (2020) Yibin Shen and Cheqing Jin. 2020. [Solving math word problems
    with multi-encoders and multi-decoders](https://aclanthology.org/2020.coling-main.262/).
    In *Proceedings of the 28th International Conference on Computational Linguistics
    (COLING)*, pages 2924–2934.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2015) Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and
    Yong Rui. 2015. [Automatically solving number word problems by semantic parsing
    and reasoning](https://aclanthology.org/D15-1135/). In *Proceedings of the 2015
    conference on empirical methods in natural language processing (EMNLP)*, pages
    1132–1142.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    2019. [Mass: Masked sequence to sequence pre-training for language generation](https://arxiv.org/abs/1905.02450).
    In *36th International Conference on Machine Learning (ICML)*.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and
    Claire Cardie. 2019. [Dream: A challenge data set and models for dialogue-based
    reading comprehension](https://arxiv.org/abs/1902.00164). *Transactions of the
    Association for Computational Linguistics (TACL)*, 7:217–231.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    [Sequence to sequence learning with neural networks](https://arxiv.org/abs/1409.3215).
    *Advances in neural information processing systems (NeurIPS)*, 27.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tafjord et al. (2019) Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih,
    and Ashish Sabharwal. 2019. [Quarel: A dataset and models for answering questions
    about qualitative relationships](https://arxiv.org/abs/1811.08048). In *Proceedings
    of the AAAI Conference on Artificial Intelligence (AAAI)*, pages 7063–7071.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D Manning.
    2015. [Improved semantic representations from tree-structured long short-term
    memory networks](https://arxiv.org/abs/1503.00075). In *Proceedings of the 53rd
    Annual Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing (ACL)*, pages 1556–1566.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thawani et al. (2022) Avijit Thawani, Jay Pujara, and Ashwin Kalyan. 2022. [Estimating
    numbers without regression](https://mathai2022.github.io/papers/11.pdf). In *36th
    Conference on Neural Information Processing Systems (NeurIPS 2022) Workshop on
    MATH-AI*.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thawani et al. (2021) Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip
    Ilievski. 2021. [Representing numbers in nlp: a survey and a vision](https://aclanthology.org/2021.naacl-main.53).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (NAACL-HIT)*, pages
    644–656.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ughade and Kumbhar (2019) Shounaak Ughade and Satish Kumbhar. 2019. [Survey
    on mathematical word problem solving using natural language processing](https://ieeexplore.ieee.org/document/8741437).
    In *2019 1st International Conference on Innovations in Information and Communication
    Technology (ICIICT)*, pages 1–5\. IEEE.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay and Chang (2015) Shyam Upadhyay and Ming-Wei Chang. 2015. [Draw: A
    challenging and diverse algebra word problem set](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tech_rep.pdf).
    Technical report, Citeseer.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay and Chang (2017) Shyam Upadhyay and Ming-Wei Chang. 2017. [Annotating
    derivations: A new evaluation strategy and dataset for algebra word problems](https://aclanthology.org/E17-1047/).
    In *Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics (ACL)*, pages 494–504.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Urban (2006) Josef Urban. 2006. [Mptp 0.2: Design, implementation, and initial
    experiments](https://link.springer.com/article/10.1007/s10817-006-9032-3). *Journal
    of Automated Reasoning*, 37(1):21–43.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://arxiv.org/abs/1706.03762). In *Advances in Neural Information
    Processing Systems (NeurIPS)*, pages 5998–6008.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wallace et al. (2019) Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and
    Matt Gardner. 2019. [Do nlp models know numbers? probing numeracy in embeddings](https://arxiv.org/abs/1909.07940).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 5307–5315.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, and Xiaojiang
    Liu. 2018a. [Translating a math word problem to a expression tree](https://arxiv.org/abs/1811.05632).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 1064–1069.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan Song, Long
    Guo, and Heng Tao Shen. 2018b. [Mathdqn: Solving arithmetic word problems via
    deep reinforcement learning](https://ojs.aaai.org/index.php/AAAI/article/view/11981).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli
    Gao, Bing Tian Dai, and Heng Tao Shen. 2019. [Template-based math word problem
    solvers with recursive neural networks](https://ojs.aaai.org/index.php/AAAI/article/view/4697).
    In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, pages
    7144–7151.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    and Denny Zhou. 2023. [Self-consistency improves chain of thought reasoning in
    language models](https://arxiv.org/abs/2203.11171). In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. [Deep neural
    solver for math word problems](https://aclanthology.org/D17-1088/). In *Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 845–854.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. [Chain of thought prompting elicits reasoning in
    large language models](https://arxiv.org/abs/2201.11903). *Advances in Neural
    Information Processing Systems (NeurIPS)*.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2021) Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi,
    Yejin Choi, and Kyunghyun Cho. 2021. [Naturalproofs: Mathematical theorem proving
    in natural language](https://arxiv.org/abs/2104.01112). In *Thirty-fifth Conference
    on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track*.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2022a) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,
    and Yejin Choi. 2022a. [Naturalprover: Grounded mathematical proof generation
    with language models](https://arxiv.org/abs/2205.12910). In *Advances in Neural
    Information Processing Systems (NeurIPS)*.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2023) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. 2023. [Generating sequences by learning
    to self-correct](https://arxiv.org/abs/2211.00053). In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2022b) Sean Welleck, Peter West, Jize Cao, and Yejin Choi.
    2022b. [Symbolic brittleness in sequence models: on systematic generalization
    in symbolic mathematics](https://arxiv.org/pdf/2109.13986.pdf). In *AAAI*.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen-Tsun (1986) Wu Wen-Tsun. 1986. [Basic principles of mechanical theorem proving
    in elementary geometries](https://link.springer.com/article/10.1007/BF02328447).
    *Journal of automated Reasoning*, 2(3):221–252.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whalen (2016) Daniel Whalen. 2016. [Holophrasm: a neural automated theorem
    prover for higher-order logic](https://arxiv.org/abs/1608.02644). *arXiv preprint
    arXiv:1608.02644*.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Woo et al. (2018) Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon.
    2018. [Cbam: Convolutional block attention module](https://arxiv.org/abs/1807.06521).
    In *Proceedings of the European conference on computer vision (ECCV)*, pages 3–19.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Qinzhuo Wu, Qi Zhang, Jinlan Fu, and Xuan-Jing Huang. 2020.
    [A knowledge-aware sequence-to-tree network for math word problem solving](https://aclanthology.org/2020.emnlp-main.579/).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 7137–7146.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021a) Qinzhuo Wu, Qi Zhang, and Zhongyu Wei. 2021a. [An edge-enhanced
    hierarchical graph-to-tree network for math word problem solving](https://aclanthology.org/2021.findings-emnlp.127/).
    In *Findings of the Association for Computational Linguistics (EMNLP)*, pages
    1473–1482.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021b) Qinzhuo Wu, Qi Zhang, Zhongyu Wei, and Xuan-Jing Huang. 2021b.
    [Math word problem solving with explicit numerical values](https://aclanthology.org/2021.acl-long.455/).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (ACL)*, pages 5859–5869.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022a) Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong
    Ma, and Liang He. 2022a. [A survey of human-in-the-loop for machine learning](https://arxiv.org/abs/2108.00941).
    *Future Generation Computer Systems*.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
    2016. [Google’s neural machine translation system: Bridging the gap between human
    and machine translation](https://arxiv.org/abs/1609.08144). *arXiv preprint arXiv:1609.08144*.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021c) Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse.
    2021c. [Int: An inequality benchmark for evaluating generalization in theorem
    proving](https://arxiv.org/abs/2007.02924). In *International Conference on Learning
    Representations (ICLR)*.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022b) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe,
    Charles E Staats, Mateja Jamnik, and Christian Szegedy. 2022b. [Autoformalization
    with large language models](https://openreview.net/forum?id=IUikebJ1Bf0). In *Advances
    in Neural Information Processing Systems (NeurIPS)*.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022c) Yuhuai Wu, Felix Li, and Percy Liang. 2022c. [Insights into
    pre-training via simpler synthetic tasks](https://arxiv.org/abs/2206.10139). *arXiv
    preprint arXiv:2206.10139*.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021d) Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse,
    and Christian Szegedy. 2021d. [Lime: Learning inductive bias for primitives of
    mathematical reasoning](https://arxiv.org/abs/2101.06223). In *International Conference
    on Machine Learning (ICML)*, pages 11251–11262\. PMLR.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Sun (2019) Zhipeng Xie and Shichao Sun. 2019. [A goal-driven tree-structured
    neural model for math word problems](https://www.ijcai.org/proceedings/2019/736).
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    5299–5305.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. [Show, attend and tell:
    Neural image caption generation with visual attention](https://arxiv.org/abs/1502.03044).
    In *International conference on machine learning (ICML)*, pages 2048–2057\. PMLR.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Deng (2019) Kaiyu Yang and Jia Deng. 2019. [Learning to prove theorems
    via interacting with proof assistants](https://arxiv.org/abs/1905.09381). In *International
    Conference on Machine Learning (ICML)*, pages 6984–6994\. PMLR.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2008) Zheng Ye, Shang-Ching Chou, and Xiao-Shan Gao. 2008. [An introduction
    to java geometry expert](https://link.springer.com/chapter/10.1007/978-3-642-21046-4_10).
    In *International workshop on automated deduction in geometry*, pages 189–195\.
    Springer.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021a) Wei Yu, Mengzhu Wang, Xiaodong Wang, Xun Zhou, Yongfu Zha,
    Yongjian Zhang, Shuyu Miao, and Jingdong Liu. 2021a. [Geore: A relation extraction
    dataset for chinese geometry problems](https://mathai4ed.github.io/papers/papers/paper_6.pdf).
    In *35th Conference on Neural Information Processing Systems (NeurIPS) Workshop
    on Math AI for Education (MATHAI4ED)*.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021b) Weijiang Yu, Yingpeng Wen, Fudan Zheng, and Nong Xiao. 2021b.
    [Improving math word problems with pre-trained knowledge and hierarchical reasoning](https://aclanthology.org/2021.emnlp-main.272/).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 3384–3394.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju,
    Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. [Generate rather
    than retrieve: Large language models are strong context generators](https://arxiv.org/abs/2209.10063).
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaporojets et al. (2021) Klim Zaporojets, Giannis Bekoulis, Johannes Deleu,
    Thomas Demeester, and Chris Develder. 2021. [Solving arithmetic word problems
    by scoring equations with recursive neural networks](https://arxiv.org/abs/2009.05639).
    *Expert Systems with Applications*, 174:114704.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai,
    and Heng Tao Shen. 2019. [The gap of semantic parsing: A survey on automatic math
    word problem solvers](https://arxiv.org/abs/1808.07290). *IEEE transactions on
    pattern analysis and machine intelligence*, 42(9):2287–2305.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Jipeng Zhang, Roy Ka-Wei Lee, Ee-Peng Lim, Wei Qin, Lei
    Wang, Jie Shao, and Qianru Sun. 2020a. [Teacher-student networks with multiple
    decoders for solving math word problem](https://www.ijcai.org/proceedings/2020/555).
    In *International Joint Conference on Artificial Intelligence (IJCAI)*.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang,
    Jie Shao, and Ee-Peng Lim. 2020b. [Graph-to-tree learning for solving math word
    problems](https://aclanthology.org/2020.acl-main.362/). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics (ACL)*, pages
    3928–3937.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Ming-Liang Zhang, Fei Yin, Yi-Han Hao, and Cheng-Lin Liu.
    2022. [Learning to understand plane geometry diagram](https://mathai2022.github.io/papers/6.pdf).
    In *36th Conference on Neural Information Processing Systems (NeurIPS) Workshop
    on MATH-AI*.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang
    Wang, Jing Jiang, and Ee-Peng Lim. 2021. [Noahqa: Numerical reasoning with interpretable
    graph question answering dataset](NOAHQA:%20Numerical%20Reasonin). In *Findings
    of the Association for Computational Linguistics (EMNLP)*, pages 4147–4161.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020c) Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu.
    2020c. [Machine number sense: A dataset of visual arithmetic problems for abstract
    and relational reasoning](https://arxiv.org/abs/2004.12193). In *Proceedings of
    the AAAI Conference on Artificial Intelligence (AAAI)*, pages 1332–1340.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020d) Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar,
    and Dan Roth. 2020d. [Do language embeddings capture scales?](https://aclanthology.org/2020.findings-emnlp.439/)
    In *Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP*, pages 292–299.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020e) Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar,
    and Dan Roth. 2020e. [Do language embeddings capture scales?](https://doi.org/10.18653/v1/2020.blackboxnlp-1.27)
    In *Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting
    Neural Networks for NLP*, pages 292–299.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020f) Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris
    Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020f. [Dialogpt:
    Large-scale generative pre-training for conversational response generation](https://arxiv.org/abs/1911.00536).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    [Automatic chain of thought prompting in large language models](https://arxiv.org/abs/2210.03493).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming
    Liu. 2020. [Ape210k: A large-scale and template-rich dataset of math word problems](https://arxiv.org/abs/2009.11506).
    *arXiv preprint arXiv:2009.11506*.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022) Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022.
    [Multihiertt: Numerical reasoning over multi hierarchical tabular and textual
    data](https://aclanthology.org/2022.acl-long.454/). In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (ACL)*, pages
    6588–6600.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. [Calibrate before use: Improving few-shot performance of language
    models](https://arxiv.org/abs/2102.09690). In *International Conference on Machine
    Learning (ICML)*, pages 12697–12706\. PMLR.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2022) Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2022.
    [Minif2f: a cross-system benchmark for formal olympiad-level mathematics](https://arxiv.org/abs/2109.00110).
    In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019.
    ["Going on a vacation" takes longer than "Going for a walk": A Study of Temporal
    Commonsense Understanding](https://cogcomp.seas.upenn.edu/papers/ZKNR19.pdf).
    In *Proc. of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP)*.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2023.
    [Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/abs/2205.10625).
    In *International Conference on Learning Representations (ICLR)*.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo
    Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. [Tat-qa: A question answering
    benchmark on a hybrid of tabular and textual content in finance](https://arxiv.org/abs/2105.07624).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (ACL-JCNLP)*, pages 3277–3287.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Mathematical Reasoning Datasets
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f72337d9f2db59c1513b961841baee8.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Estimated counts of annually published papers on deep learning for
    mathematical reasoning. This field has been experiencing rapid growth since 2018.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will examine the various datasets currently available for
    the study of mathematical reasoning using deep learning methods. A summary of
    the commonly used datasets in this field can be found in [Table 7](#A1.T7 "Table
    7 ‣ A.5 Other Quantitative Problems ‣ Appendix A Mathematical Reasoning Datasets
    ‣ A Survey of Deep Learning for Mathematical Reasoning").
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Math Word Problem Solving
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developing algorithms to solve math word problems (MWPs) automatically has been
    an interest of NLP researchers for decades Feigenbaum et al. ([1963](#bib.bib35));
    Bobrow ([1964](#bib.bib14)). A math word problem (also termed an algebraic or
    arithmetic word problem) describes a brief narrative that involves characters,
    entities, and quantities. The mathematical relationship of an MWP can be modeled
    with a set of equations whose solution reveals the final answer to the question.
    A typical example is shown in [Table 1](#S2.T1 "Table 1 ‣ 2 Mathematical Reasoning
    Tasks ‣ A Survey of Deep Learning for Mathematical Reasoning"). A question involves
    the four basic arithmetic operations of addition, subtraction, multiplication,
    and division with single or multiple operation steps. The challenge of MWPs for
    NLP systems lies in the need for language comprehension, semantic parsing, and
    multiple mathematical reasoning skills.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Existing MWP datasets cover grade school problems, which are crawled from online
    learning websites Koncel-Kedziorski et al. ([2015](#bib.bib83)), collected from
    textbooks, or manually annotated by human workers Patel et al. ([2021](#bib.bib134)).
    Early math word problem datasets are relatively small or limited to a small number
    of operation steps Hosseini et al. ([2014](#bib.bib61)); Kushman et al. ([2014](#bib.bib85));
    Roy et al. ([2015](#bib.bib155)). Some recently curated datasets aim to increase
    problem diversity and difficulty levels. For example, Ape210K Zhao et al. ([2020](#bib.bib219))
    consists of 210k elementary math word problems, which is the largest publicly
    available. The problems in GSM8K Cobbe et al. ([2021](#bib.bib32)) can involve
    up to 8 steps to solve. SVAMP Patel et al. ([2021](#bib.bib134)) is a benchmark
    that tests the robustness of deep learning models to math word problems with simple
    variations. More recently built datasets involve modalities beyond text. For example,
    IconQA Lu et al. ([2021b](#bib.bib116)) provides an abstract diagram as a visual
    context, while TabMWP Lu et al. ([2022b](#bib.bib115)) provides a tabular context
    for each problem.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Most MWP datasets provide annotated equations as a rationale for the solution
    (e.g., [Table 1](#S2.T1 "Table 1 ‣ 2 Mathematical Reasoning Tasks ‣ A Survey of
    Deep Learning for Mathematical Reasoning")). To improve the performance and interpretability
    of the learned solvers, MathQA Tafjord et al. ([2019](#bib.bib169)) is annotated
    with precise operation programs, and MathQA-Python Austin et al. ([2021](#bib.bib8))
    is provided with specific Python programs instead. Another line of datasets annotates
    the problems with multi-step natural language solutions that are regarded as more
    human-readable Ling et al. ([2017](#bib.bib105)); Cobbe et al. ([2021](#bib.bib32));
    Lu et al. ([2022b](#bib.bib115)). Lila Mishra et al. ([2022a](#bib.bib125)) annotates
    many of the previously mentioned MWP datasets with Python program rationales.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Theorem Proving
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been increased interest in using language models for theorem
    proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever
    ([2020](#bib.bib138)); Han et al. ([2022](#bib.bib51)); Polu et al. ([2023](#bib.bib137));
    Jiang et al. ([2022b](#bib.bib68), [a](#bib.bib66)); Lample et al. ([2022](#bib.bib87))).
    Example ITPs include Lean Moura et al. ([2015](#bib.bib128)), Isabelle Paulson
    ([1994](#bib.bib135)), Coq Barras et al. ([1999](#bib.bib11)), and Metamath Megill
    and Wheeler ([2019](#bib.bib120)). To prove a theorem in an ITP, the theorem is
    stated in the ITP’s programming language, then simplified by generating “proof
    steps” until it is reduced to known facts. The result is a sequence of steps that
    constitutes a verified proof.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Data sources for neural theorem proving in ITPs include interactive learning
    environments that interface with ITPs, and datasets derived from proofs in ITP
    libraries. For example, CoqGym Yang and Deng ([2019](#bib.bib203)) provides an
    interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle,
    PISA Jiang et al. ([2021](#bib.bib67)) enables interaction and provides a dataset
    of 183k proofs mined from the Isabelle standard library and Archive of Formal
    Proofs. For Lean, LeanStep Han et al. ([2022](#bib.bib51)) provides a dataset
    of proof-steps from Lean’s mathematical library along with auxiliary tasks, while
    Lean-Gym Polu et al. ([2023](#bib.bib137)) provides an interactive REPL. The miniF2F
    Zheng et al. ([2022](#bib.bib222)) benchmark aims to provide a shared benchmark
    across ITPs, consisting of 488 problem statements sourced from mathematical competitions.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Other resources provide proxy environments or tasks. For example, INT Wu et al.
    ([2021c](#bib.bib197)) provide a synthetic proving environment to measure six
    different types of generalization. [Li et al.](#bib.bib97) construct IsarStep
    using the Isabelle Archive of Formal Proofs, and propose a task of filling in
    a missing intermediate proposition. Early applications of deep learning for formal
    theorem proving focus on selecting relevant premises Alemi et al. ([2016](#bib.bib1)).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Informal theorem proving presents an alternative medium for theorem proving,
    in which statements and proofs are written in the mixture of natural language
    and symbols used in “standard” mathematics (e.g., in LaTeX), and are checked for
    correctness by humans. Early work focuses on selecting relevant premises Ferreira
    and Freitas ([2020b](#bib.bib38), [a](#bib.bib37)). Welleck et al. ([2021](#bib.bib185))
    develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems,
    definitions, and proofs, and provide a benchmark for premise selection via retrieval
    and generation tasks. Welleck et al. ([2022a](#bib.bib186)) adapt NaturalProofs
    for full proof generation, and provide a human evaluation protocol and proxy automatic
    metrics.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: An emerging area of research aims to combine elements of informal and formal
    theorem proving. For example, Wu et al. ([2022b](#bib.bib198)) explore translating
    informal statements into formal statements, while Jiang et al. ([2022a](#bib.bib66))
    release a new version of the miniF2F benchmark augmented with informal statements
    and proofs, which we refer to as miniF2F+informal. Jiang et al. ([2022a](#bib.bib66))
    explore translating provided (or generated) informal proofs into formal proofs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Geometry Problem Solving
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automated geometry problem solving (GPS) is also a long-standing AI task in
    mathematical reasoning research Gelernter et al. ([1960](#bib.bib45)); Wen-Tsun
    ([1986](#bib.bib189)); Chou et al. ([1996](#bib.bib29)); Ye et al. ([2008](#bib.bib204))
    and has attracted much attention in recent years. Different from a math word problem,
    a geometry problem consists of a textual description in natural language and a
    geometric diagram. As shown in [Figure 2](#S2.F2 "Figure 2 ‣ 2 Mathematical Reasoning
    Tasks ‣ A Survey of Deep Learning for Mathematical Reasoning"), the multimodal
    inputs describe the entities, attributes, and relationships of geometric elements,
    and the goal is to find the numeric solution to an unknown variable. GPS is a
    challenging task for deep learning methods due to the complex skills it requires.
    It involves the ability to parse multimodal information, perform symbolic abstraction,
    utilize theorem knowledge, and conduct quantitative reasoning.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Some early datasets are proposed to facilitate research in this domain Seo et al.
    ([2015](#bib.bib162)); Alvin et al. ([2017](#bib.bib3)); Sachan et al. ([2017](#bib.bib157));
    Sachan and Xing ([2017](#bib.bib158)). However, these datasets are relatively
    small or not publicly available, which limits the development of deep learning
    methods. In response to this limitation, [Lu et al.](#bib.bib112) create the Geometry3K
    dataset, which consists of 3,002 multi-choice geometry problems with unified logic
    form annotations for the multimodal inputs. More recently, larger-scale datasets
    such as GeoQA Chen et al. ([2021a](#bib.bib20)), GeoQA+ Cao and Xiao ([2022](#bib.bib16)),
    and UniGeo Chen et al. ([2022a](#bib.bib19)) have been introduced and are annotated
    with programs that can be learned by neural solvers and executed to obtain the
    final answers.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Math Question Answering
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numerical reasoning is a core ability within human intelligence and plays an
    important role in many NLP tasks. Aside from theorem proving and grade-level math
    word problem solving, there is a wide range of question answering (QA) benchmarks
    that center around mathematical reasoning. In this work, we refer to these tasks
    as math question answering (MathQA). A large number of datasets have been presented
    recently. For example, QuaRel Tafjord et al. ([2019](#bib.bib169)) is a dataset
    of diverse story questions that involve 19 different types of quantities. McTaco
    Zhou et al. ([2019](#bib.bib223)) studies temporal commonsense problems, while
    Fermi Kalyan et al. ([2021](#bib.bib74)) studies Fermi problems whose answers
    can only be approximately estimated.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies have shown that state-of-the-art mathematical reasoning systems
    might suffer from brittleness in reasoning, in that the models rely on spurious
    signals and plug-and-chug calculations in the specific dataset to achieve “satisfactory”
    performance Hendrycks et al. ([2021b](#bib.bib55)); Mishra et al. ([2022b](#bib.bib126)).
    To address this issue, new benchmarks are proposed from various aspects. The Mathematics
    dataset Saxton et al. ([2020](#bib.bib159)) consists of many different types of
    mathematics problems, covering arithmetic, algebra, probability, and calculus.
    The dataset allows for measuring the algebraic generalization ability of a model.
    Similarly, MATH Hendrycks et al. ([2021b](#bib.bib55)) consists of challenging
    competition mathematics to measure the problem-solving ability of models in complex
    scenarios.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Some work incorporates tabular contexts in the question inputs. For example,
    FinQA Chen et al. ([2021c](#bib.bib24)), TAT-QA Zhu et al. ([2021](#bib.bib225)),
    and MultiHiertt Zhao et al. ([2022](#bib.bib220)) collect questions that require
    both table understanding and numeric reasoning to answer. Others, instead, present
    large-scale unified benchmarks for mathematical reasoning Mishra et al. ([2022b](#bib.bib126),
    [a](#bib.bib125)); Chen et al. ([2023](#bib.bib23)). NumGLUE Mishra et al. ([2022b](#bib.bib126))
    is a multi-task benchmark with the goal of evaluating the performance of models
    on eight different tasks. Mishra et al. [2022a](#bib.bib125) push this direction
    further and presents Lila, which consists of 23 mathematical reasoning tasks,
    spanning a wide range of mathematics topics, linguistic complexity, question formats,
    and background knowledge requirements.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Other Quantitative Problems
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numbers are an integral part of our daily lives, and we humans reason with numbers
    in a variety of tasks, such as understanding news, reports, elections, and markets.
    This has led many in the community to question whether AI systems can effectively
    perform quantitative reasoning in everyday scenarios. To this end, various benchmarks
    have been developed to evaluate the capabilities of AI systems in this area.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Diagrams, such as figures, charts, and plots, are essential media that convey
    large amounts of information in a concise way. FigureQA Kahou et al. ([2018](#bib.bib72)),
    DVQA Kafle et al. ([2018](#bib.bib71)), MNS Zhang et al. ([2020c](#bib.bib214)),
    PGDP5K Hao et al. ([2022](#bib.bib52)), and GeoRE Yu et al. ([2021a](#bib.bib205)),
    are released to investigate models’ abilities to reason about quantitative relationships
    among entities grounded in diagrams. NumerSense Lin et al. ([2020](#bib.bib103)),
    instead, examines whether and to what extent existing pre-trained language models
    can induce numerical commonsense knowledge. EQUATE Ravichander et al. ([2019](#bib.bib147))
    formalizes aspects of quantitative reasoning in a natural language inference framework.
    Quantitative reasoning can appear frequently in specific domains like finance,
    science, and programming. For instance, the ConvFinQA Chen et al. ([2022c](#bib.bib25))
    targets numerical reasoning over financial reports in a conversational question
    answering format. ScienceQA Lu et al. ([2022a](#bib.bib113)) involves numerical
    reasoning in scientific domains, while P3 Schuster et al. ([2021](#bib.bib160))
    studies the function inference ability of deep learning models to find a valid
    input which makes the given program return True.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Task | Size | Input | Output | Rationale | Domain |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| Verb395 ([2014](#bib.bib61)) | MWP | 395 | Question | Number | Equation |
    Math |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| Alg514 ([2014](#bib.bib85)) | MWP | 514 | Question | Number | Equation |
    Math |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| IL ([2015](#bib.bib155)) | MWP | - | Question | Number | Equation | Math
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| SingleEQ ([2015](#bib.bib83)) | MWP | 508 | Question | Number | Equation
    | Math |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| DRAW ([2015](#bib.bib174)) | MWP | 1,000 | Question | Number | Equation |
    Math |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Dolphin1878 ([2015](#bib.bib165)) | MWP | 1,878 | Question | Number | Equation
    | Math |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Dolphin18K ([2016](#bib.bib65)) | MWP | 18,460 | Question | Number | Equation
    | Math |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| MAWPS ([2016](#bib.bib82)) | MWP | 3,320 | Question | Number | Equation |
    Math |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| AllArith ([2017](#bib.bib153)) | MWP | 831 | Question | Number | Equation
    | Math |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| DRAW-1K ([2017](#bib.bib175)) | MWP | 1,000 | Question | Number | Equation
    | Math |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Math23K ([2017](#bib.bib183)) | MWP | 23,162 | Question | Number | Equation
    | Math |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| AQuA ([2017](#bib.bib105)) | MWP | 100,000 | Question | Option | Natural
    language | Math |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| Aggregate ([2018](#bib.bib154)) | MWP | 1,492 | Question | Number | Equation
    | Math |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| MathQA ([2019](#bib.bib4)) | MWP | 37,297 | Question | Number | Program |
    Math |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| ASDiv ([2020](#bib.bib122)) | MWP | 2,305 | Question | Number | Equation
    | Math |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| HMWP ([2020](#bib.bib140)) | MWP | 5,470 | Question | Number | Equation |
    Math |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| Ape210K ([2020](#bib.bib219)) | MWP | 210,488 | Question | Number | Equation
    | Math |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| SVAMP ([2021](#bib.bib134)) | MWP | 1,000 | Question | Number | Equation
    | Math |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| GSM8K ([2021](#bib.bib32)) | MWP | 8,792 | Question | Number | Natural language
    | Math |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| IconQA ([2021b](#bib.bib116)) | MWP | 107,439 | Figure+Question | Option+Text
    span | ✗ | Math |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| MathQA-Python ([2021](#bib.bib8)) | MWP | 23,914 | Question | Number | Python
    program | Math |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| ArMATH ([2022](#bib.bib2)) | MWP | 6,000 | Question | Number | Equation |
    Math |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| TabMWP ([2022b](#bib.bib115)) | MWP | 38,431 | Table+Question | Option+Number
    | Natural language | Math |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| MML ([2015](#bib.bib49)) | TP | 57,882 | Statement | Proof steps | ✗ | Math
    |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| HolStep ([2017](#bib.bib73)) | TP | 2,209,076 | Statement | Proof steps |
    ✗ | Math |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| Gamepad ([2019](#bib.bib62)) | TP | - | Statement | Proof steps | ✗ | Math
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| CoqGym ([2019](#bib.bib203)) | TP | 71,000 | Statement | Proof steps | ✗
    | Math |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| HOList ([2019](#bib.bib10)) | TP | 29,462 | Statement | Proof steps | ✗ |
    Math |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| IsarStep ([2021](#bib.bib97)) | TP | 860,000 | Statement | Proof steps |
    ✗ | Math |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| PISA ([2021](#bib.bib67)) | TP | 183,000 | Statement | Proof steps | ✗ |
    Math |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| INT ([2021c](#bib.bib197)) | TP | - | Statement | Proof steps | ✗ | Math
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| NaturalProofs ([2021](#bib.bib185)) | TP | 32,000 | Statement | Proof steps
    | ✗ | Math |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| NaturalProofs-Gen ([2022a](#bib.bib186)) | TP | 14,500 | Statement | Proof
    steps | ✗ | Math |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| miniF2F ([2022](#bib.bib222)) | TP | 488 | Statement | Proof steps | ✗ |
    Math |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| miniF2F+informal ([2022a](#bib.bib66)) | TP | 488 | Statement | Proof steps
    | ✗ | Math |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| LeanStep ([2022](#bib.bib51)) | TP | 21,606,000 | Statement | Proof steps
    | ✗ | Math |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| GEOS ([2015](#bib.bib162)) | GPS | 186 | Figure+Question | Option | ✗ | Geometry
    |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| GeoShader ([2017](#bib.bib3)) | GPS | 102 | Figure+Question | Number | ✗
    | Geometry |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| GEOS++ ([2017](#bib.bib157)) | GPS | 1,406 | Figure+Question | Number | ✗
    | Geometry |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| GEOS-OS ([2017](#bib.bib158)) | GPS | 2,235 | Figure+Question | Option |
    Demonstration | Geometry |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| Geometry3K ([2021a](#bib.bib112)) | GPS | 3,002 | Figure+Question | Option
    | Logical form | Geometry |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| GeoQA ([2021a](#bib.bib20)) | GPS | 4,998 | Figure+Question | Option | Program
    | Geometry |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| GeoQA+ ([2022](#bib.bib16)) | GPS | 12,054 | Figure+Question | Option | Program
    | Geometry |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| UniGeo ([2022a](#bib.bib19)) | GPS/TP | 14,541 | Figure+Question | Option
    | Program | Geometry |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| Quarel ([2019](#bib.bib169)) | MathQA | 2,771 | Question | Option | Logical
    form | Math |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| McTaco ([2019](#bib.bib223)) | MathQA | 13,225 | Text+Question | Option |
    ✗ | Time |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| DROP ([2019](#bib.bib34)) | MathQA | 96,567 | Passage+Question | Number+Text
    span | ✗ | Math |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| Mathematics ([2020](#bib.bib159)) | MathQA | 2,010,000 | Question | Free-form
    | Number | Math |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| FinQA ([2021c](#bib.bib24)) | MathQA | 8,281 | Text+Table+Q | Number | Program
    | Finance |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| Fermi ([2021](#bib.bib74)) | MathQA | 11,000 | Question | Number | Program+Fact
    | Math |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| MATH ([2021b](#bib.bib55)) | MathQA | 12,500 | Question | Number | Natural
    language | Math |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '| TAT-QA ([2021](#bib.bib225)) | MathQA | 16,552 | Text+Table+Q | Number+Text
    span | ✗ | Finance |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| AMPS ([2021b](#bib.bib55)) | MathQA | 5,000,000 | Question | - | LaTeX |
    Math |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| MultiHiertt ([2022](#bib.bib220)) | MathQA | 10,440 | Text+Table+Q | Number+Text
    span | Expression | Finance |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| NumGLUE ([2022b](#bib.bib126)) | MathQA | 101,835 | Text+Question | Number+Text
    span | ✗ | Math |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| Lila ([2022a](#bib.bib125)) | MathQA | 134,000 | Text+Question | Free-form
    | Python program | Math |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| FigureQA ([2018](#bib.bib72)) | VQA | 1,000,000+ | Figure+Question | Binary
    | ✗ | Math |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| DVQA ([2018](#bib.bib71)) | VQA | 3,487,194 | Figure+Question | Text span
    | Number+Text span | Math |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| DREAM ([2019](#bib.bib167)) | ConvQA | 10,197 | Dialog+Question | Option
    | ✗ | Math |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| EQUATE ([2019](#bib.bib147)) | NLI | - | Premise+Hypothesis | Binary | ✗
    | Math |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| NumerSense ([2020](#bib.bib103)) | Filling | 13,600 | Masked question | Word
    | ✗ | Math |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| MNS ([2020c](#bib.bib214)) | IQ Test | - | Figure | Number | ✗ | Math |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| P3 ([2021](#bib.bib160)) | Puzzle | 397 | Text | Program | ✗ | Math |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| NOAHQA ([2021](#bib.bib213)) | ConvQA | 21,347 | Dialog+Question | Text span
    | Reasoning graph | Math |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| ConvFinQA ([2022c](#bib.bib25)) | ConvQA | 3,892 | Report+Dialog+Q | Number
    | Expression | Math |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| PGDP5K ([2022](#bib.bib52)) | Parsing | 5,000 | Figure+Question | Number
    | ✗ | Geometry |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '| GeoRE ([2022a](#bib.bib19)) | Parsing | 12,901 | Figure+Question | Number
    | ✗ | Geometry |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
- en: '| ScienceQA ([2022a](#bib.bib113)) | VQA | 21,208 | Context+Question | Option
    | Natural language | Science |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: 'Table 7: A summarization of mathematical reasoning datasets.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Task | Problem | Network | Encod | Decod | ATT | Description |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| DNS Wang et al. ([2017](#bib.bib183)) | MWP | Generation | Seq2Seq | GRU
    | LSTM | ✗ | The first deep MWP solver |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| AnsRat Ling et al. ([2017](#bib.bib105)) | MWP | Generation | Seq2Seq | LSTM
    | LSTM | ✗ | Trained with staged back-propagation |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| Math-EN Wang et al. ([2018a](#bib.bib179)) | MWP | Generation | Seq2Seq |
    BiLSTM | LSTM | ✔ | A standard Seq2Seq model with attention |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| CASS Huang et al. ([2018](#bib.bib63)) | MWP | Generation | Seq2Seq | BiGRU
    | BiGRU | ✔ | Copy and alignment with RL |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| S-Aligned Chiang and Chen ([2019](#bib.bib26)) | MWP | Generation | Seq2Seq
    | BiLSTM | LSTM | ✔ | Operating symbols |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| T-RNN Wang et al. ([2019](#bib.bib181)) | MWP | Generation | Seq2Seq | BiLSTM
    | BiLSTM | ✔ | Predicting a tree-structure math template |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| GROUP-ATT Li et al. ([2019](#bib.bib93)) | MWP | Generation | Seq2Seq | BiLSTM
    | LSTM | ✔ | Group attention |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| SMART Hong et al. ([2021b](#bib.bib60)) | MWP | Generation | Seq2Seq | -
    | - | ✗ | Explicitly incorporating values |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| SelfAtt Robaidek et al. ([2018](#bib.bib151)) | GPS | Classification | Seq2Seq
    | BiLSTM | - | ✔ | Multi-hop self-attention |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| QuaSP+ Tafjord et al. ([2019](#bib.bib169)) | MathQA | Generation | Seq2Seq
    | BiLSTM | LSTM | ✗ | Adopting attributed grammar |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| AST-Dec Liu et al. ([2019a](#bib.bib109)) | MWP | Generation | Seq2Tree |
    BiLSTM | Tree | ✔ | Using prefix order decoding |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| GTS Xie and Sun ([2019](#bib.bib201)) | MWP | Generation | Seq2Tree | BiGRU
    | Tree | ✔ | A goal-driven tree-structured approach |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| KA-S2T Wu et al. ([2020](#bib.bib192)) | MWP | Generation | Seq2Tree | BiLSTM
    | Tree | ✔ | A knowledge-aware method |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| TSN-MD Zhang et al. ([2020a](#bib.bib210)) | MWP | Generation | Seq2Tree
    | BiGRU | Tree | ✔ | A teacher-student network |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| T-LSTM Zaporojets et al. ([2021](#bib.bib208)) | MWP | Generation | Seq2Tree
    | BiLSTM | Tree | ✗ | A child-sum tree-LSTM model |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '| NT-LSTM Zaporojets et al. ([2021](#bib.bib208)) | MWP | Generation | Seq2Tree
    | BiLSTM | Tree | ✗ | An N-ary tree-LSTM model |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| NS-Solver Qin et al. ([2021](#bib.bib139)) | MWP | Generation | Seq2Tree
    | BiGRU | Tree | ✔ | A neural-symbolic solver with programs |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| NumS2T Wu et al. ([2021b](#bib.bib194)) | MWP | Generation | Seq2Tree | BiLSTM
    | Tree | ✔ | Explicitly incorporating values |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| HMS Lin et al. ([2021](#bib.bib104)) | MWP | Generation | Seq2Tree | GRU
    | Tree | ✔ | A word-clause-problem encoder |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| LBF Hong et al. ([2021a](#bib.bib59)) | MWP | Generation | Seq2Tree | BiGRU
    | Tree | ✔ | A learning-by-fixing (LBF) framework |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| Seq2DAG Cao et al. ([2021](#bib.bib17)) | MWP | Generation | Seq2Graph |
    GRU | Graph | ✗ | A direct acyclic graph (DAG) structure |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| Graph2Tree Zhang et al. ([2020b](#bib.bib211)) | MWP | Generation | Graph2Tree
    | Graph | Tree | ✗ | Generating better solution expressions |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| Multi-E/D Shen and Jin ([2020](#bib.bib164)) | MWP | Generation | Graph2Tree
    | Graph | Tree | ✔ | A graph encoder and a tree-bad decoder |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| Graph2Tree Li et al. ([2020b](#bib.bib96)) | MWP | Generation | Graph2Tree
    | Graph | Tree | ✔ | A graph-to-tree neural network |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| EEH-G2T Wu et al. ([2021a](#bib.bib193)) | MWP | Generation | Graph2Tree
    | Graph | Tree | ✗ | A hierarchical graph-to-tree model |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| ASTactic Yang and Deng ([2019](#bib.bib203)) | TP | Generation | Tree2Seq
    | TreeLSTM | GRU | ✔ | Generating tactics as programs |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| MathDQN Wang et al. ([2018b](#bib.bib180)) | MWP | Search | DQN | - | - |
    ✗ | RL with a deep Q-network (DQN) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| DDT Meng and Rumshisky ([2019](#bib.bib121)) | MWP | Generation | Transformer
    | Trm | Trm | ✔ | A Transformer-based model |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| DeepMath Alemi et al. ([2016](#bib.bib1)) | TP | Classification | CNN | CNN
    | - | ✗ | The first deep large scale theorem prover |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| Holophrasm Whalen ([2016](#bib.bib190)) | TP | Classification | BiGRU | BiGRU
    | - | ✗ | A neural prover for higher-order logic |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| CNNTP Loos et al. ([2017](#bib.bib111)) | TP | Classification | CNN | CNN
    | - | ✗ | A CNN-based theorem prover |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| WaveNetTP Loos et al. ([2017](#bib.bib111)) | TP | Classification | WaveNet
    | WaveNet | - | ✗ | A WaveNet-based theorem prover |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| DeepHOL Bansal et al. ([2019](#bib.bib10)) | TP | Generation | WaveNet |
    WaveNet | - | ✗ | A neural theorem prover with RL |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| NGS Chen et al. ([2021a](#bib.bib20)) | GPS | Generation | VQA | LSTM* |
    LSTM | ✔ | The first deep geometry solver |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '| PGDPNet Zhang et al. ([2022](#bib.bib212)) | Parsing | Generation | GNN |
    - | - | ✗ | A neural diagram parser with GNN |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
- en: 'Table 8: A summarization of deep neural network models for mathematical reasoning.
    Encod: encoder, Decod: decoder, ATT: Attention. LSTM*: ResNet + LSTM, Trm: Transformer'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
