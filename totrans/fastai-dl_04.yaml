- en: 'Deep Learning 2: Part 1 Lesson 4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Lesson 4](http://forums.fast.ai/t/wiki-lesson-4/9402/1)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Articles by students:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Improving the way we work with learning rate](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Cyclical Learning Rate technique](http://teleported.in/posts/cyclic-learning-rate/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Stochastic Gradient Descent with Restarts (SGDR)](/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning using differential learning rates](https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Computers To See Better Than Humans](/@ArjunRajkumar/getting-computers-to-see-better-than-humans-346d96634f73)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout [04:59]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`precompute=True` : Pre-compute the activations that come out of the last convolutional
    layer. Remember, activation is a number that is calculated based on some weights/parameter
    that makes up kernels/filters, and they get applied to the previous layer’s activations
    or inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`learn` — This will display the layers we added at the end. These are the layers
    we train when `precompute=True`'
  prefs: []
  type: TYPE_NORMAL
- en: '(0), (4): `BatchNorm` will be covered in the last lesson'
  prefs: []
  type: TYPE_NORMAL
- en: '(1), (5): `Dropout`'
  prefs: []
  type: TYPE_NORMAL
- en: (2):`Linear` layer simply means a matrix multiply. This is a matrix which has
    1024 rows and 512 columns, so it will take in 1024 activations and spit out 512
    activations.
  prefs: []
  type: TYPE_NORMAL
- en: (3):`ReLU` — just replace negatives with zero
  prefs: []
  type: TYPE_NORMAL
- en: '(6): `Linear` — the second linear layer that takes those 512 activations from
    the previous linear layer and put them through a new matrix multiply 512 by 120
    and outputs 120 activations'
  prefs: []
  type: TYPE_NORMAL
- en: '(7): `Softmax` — The activation function that returns numbers that adds up
    to 1 and each of them is between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: For minor numerical precision reasons, it turns out to be better to tahe the
    log of the softmax than softmax directly [[15:03](https://youtu.be/gbceqO8PpBg?t=15m3s)].
    That is why when we get predictions out of our models, we have to do `np.exp(log_preds)`.
  prefs: []
  type: TYPE_NORMAL
- en: What is `Dropout` and what is `p`? [[08:17](https://youtu.be/gbceqO8PpBg?t=8m17s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we applied dropout with `p=0.5` to `Conv2` layer, it would look like the
    above. We go through, pick an activation, and delete it with 50% chance. So `p=0.5`
    is the probability of deleting that cell. Output does not actually change by very
    much, just a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: Randomly throwing away half of the activations in a layer has an interesting
    effect. An important thing to note is for each mini-batch, we throw away a different
    random half of activations in that layer. It forces it to not overfit. In other
    words, when a particular activation that learned just that exact dog or exact
    cat gets dropped out, the model has to try and find a representation that continues
    to work even as random half of the activations get thrown away every time.
  prefs: []
  type: TYPE_NORMAL
- en: This has been absolutely critical in making modern deep learning work and just
    about solve the problem of generalization. Geoffrey Hinton and his colleagues
    came up with this idea loosely inspired by the way the brain works.
  prefs: []
  type: TYPE_NORMAL
- en: '`p=0.01` will throw away 1% of the activations. It will not change things up
    very much at all, and will not prevent overfitting (not generalized).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p=0.99` will throw away 99% of the activations. Not going to overfit and great
    for generalization, but will kill your accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the first layer is `0.25` and second layer is `0.5`[17:54]. If you
    find it is overfitting, start bumping it up — try setting all to `0.5`, still
    overfitting, try `0.7` etc. If you are under-fitting, you can try making it lower
    but is unlikely you would need to make it much lower.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet34 has less parameters so it does not overfit as much, but for bigger
    architecture like ResNet50, you often need to increase dropout.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have you wondered why the validation losses better than the training losses
    particularly early in the training? [[12:32](https://youtu.be/gbceqO8PpBg?t=12m32s)]
    This is because we turn off dropout when we run inference (i.e. making prediction)
    on the validation set. We want to be using the best model we can.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Do you have to do anything to accommodate for the fact that you
    are throwing away activations? [[13:26](https://youtu.be/gbceqO8PpBg?t=13m26s)]
    We do not, but PyTorch does two things when you say `p=0.5`. It throws away half
    of the activations, and it doubles all the activations that are already there
    so that average activation does not change.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Fast.ai, you can pass in `ps` which is the `p` value for all of the added
    layers. It will not change the dropout in the pre-trained network since it should
    have been already trained with some appropriate level of dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can remove dropout by setting `ps=0.` but even after a couple epochs, we
    start to massively overfit (training loss ≪ validation loss):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When `ps=0.` , dropout layers are not even added to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed, it has been adding two `Linear` layers [[16:19](https://youtu.be/gbceqO8PpBg?t=16m19s)].
    We do not have to do that. There is `xtra_fc` parameter you can set. Note: you
    do need at least one which takes the output of the convolutional layer (4096 in
    this example) and turns it into the number of classes (120 dog breeds):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Question**: Is there a particular way in which you can determine if it is
    overfitted? [[19:53](https://youtu.be/gbceqO8PpBg?t=19m53s)]. Yes, you can see
    the training loss is much lower than the validation loss. You cannot tell if it
    is *too* overfitted. Zero overfitting is not generally optimal. The only thing
    you are trying to do is to get the validation loss low, so you need to play around
    with a few things and see what makes the validation loss low. You will get a feel
    for it overtime for your particular problem what too much overfitting looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Why does average activation matter? [[21:15](https://youtu.be/gbceqO8PpBg?t=21m15s)]
    If we just deleted a half of activations, the next activation who takes them as
    input will also get halved, and everything after that. For example, fluffy ears
    are fluffy if this is greater than 0.6, and now it is only fluffy if it is greater
    than 0.3 — which is changing the meaning. The goal here is delete activations
    without changing the meanings.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Can we have different level of dropout by layer? [[22:41](https://youtu.be/gbceqO8PpBg?t=22m41s)]
    Yes, that is why it is called `ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There is no rule of thumb for when earlier or later layers should have different
    amounts of dropout yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If in doubt, use the same dropout for every fully connected layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often people only put dropout on the very last linear layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question**: Why monitor loss and not accuracy? [[23:53](https://youtu.be/gbceqO8PpBg?t=23m53s)]
    Loss is the only thing that we can see for both the validation set and the training
    set. As we learn later, the loss is the thing that we are actually optimizing
    so it is easier to monitor and understand what that means.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Do we need to adjust the learning rate after adding dropouts?[[24:33](https://youtu.be/gbceqO8PpBg?t=24m33s)]
    It does not seem to impact the learning rate enough to notice. In theory, it might
    but not enough to affect us.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured and Time Series Data [[25:03](https://youtu.be/gbceqO8PpBg?t=25m3s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)
    / [Kaggle](https://www.kaggle.com/c/rossmann-store-sales)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical — It has a number of “levels” e.g. StoreType, Assortment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous — It has a number where differences or ratios of that numbers have
    some kind of meanings e.g. CompetitionDistance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Numbers like `Year` , `Month`, although we could treat them as continuous, we
    do not have to. If we decide to make `Year` a categorical variable, we are telling
    our neural net that for every different “level”of `Year` (2000, 2001, 2002), you
    can treat it totally differently; where-else if we say it is continuous, it has
    to come up with some kind of smooth function to fit them. So often things that
    actually are continuous but do not have many distinct levels (e.g. `Year`, `DayOfWeek`),
    it often works better to treat them as categorical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing categorical vs. continuous variable is a modeling decision you get
    to make. In summary, if it is categorical in the data, it has to be categorical.
    If it is continuous in the data, you get to pick whether to make it continuous
    or categorical in the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, floating point numbers are hard to make categorical as there are
    many levels (we call number of levels “**Cardinality**” — e.g. the cardinality
    of the day of week variable is 7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question**: Do you ever *bin* continuous variables?[[31:02](https://youtu.be/gbceqO8PpBg?t=31m2s)]
    Jeremy does not bin variables but one thing we could do with, say max temperature,
    is to group into 0–10, 10–20, 20–30, and call that categorical. Interestingly,
    a paper just came out last week in which a group of researchers found that sometimes
    binning can be helpful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: If you are using year as a category, what happens when a model
    encounters a year it has never seen before? [[31:47](https://youtu.be/gbceqO8PpBg?t=31m47s)]
    We will get there, but the short answer is that it will be treated as an unknown
    category. Pandas has a special category called unknown and if it sees a category
    it has not seen before, it gets treated as unknown.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Loop through `cat_vars` and turn applicable data frame columns into categorical
    columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop through `contin_vars` and set them as `float32` (32 bit floating point)
    because that is what PyTorch expects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with a small sample [[34:29](https://youtu.be/gbceqO8PpBg?t=34m29s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here is what our data looks like. Even though we set some of the columns as
    “category” (e.g. ‘StoreType’, ‘Year’), Pandas still display as string in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`proc_df` (process data frame) — A function in Fast.ai that does a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Pulls out the dependent variable, puts it into a separate variable, and deletes
    it from the original data frame. In other words, `df` do not have `Sales` column,
    and `y` only contains `Sales` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`do_scale` : Neural nets really like to have the input data to all be somewhere
    around zero with a standard deviation of somewhere around 1\. So we take our data,
    subtract the mean, and divide by the standard deviation to make that happen. It
    returns a special object which keeps track of what mean and standard deviation
    it used for that normalization so you can do the same to the test set later (`mapper`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It also handles missing values — for categorical variable, it becomes ID: 0
    and other categories become 1, 2, 3, and so on. For continuous variable, it replaces
    the missing value with the median and create a new boolean column that says whether
    it was missing or not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After processing, year 2014 for example becomes 2 since categorical variables
    have been replaced with contiguous integers starting at zero. The reason for that
    is, we are going to put them into a matrix later, and we would not want the matrix
    to be 2014 rows long when it could just be two rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a data frame which does not contain the dependent variable and
    where everything is a number. That is where we need to get to to do deep learning.
    Check out Machine Learning course on further details. Another thing that is covered
    in Machine Learning course is validation sets. In this case, we need to predict
    the next two weeks of sales therefore we should create a validation set which
    is the last two weeks of our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[How (and why) to create a good validation set](http://www.fast.ai/2017/11/13/validation-sets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get straight to the deep learning action [[39:48](https://youtu.be/gbceqO8PpBg?t=39m48s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For any Kaggle competitions, it is important that you have a strong understanding
    of your metric — how you are going to be judged. In [this competition](https://www.kaggle.com/c/rossmann-store-sales#evaluation),
    we are going to be judged on Root Mean Square Percentage Error (RMSPE).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When you take the log of the data, getting the root mean squared error will
    actually get you the root mean square percentage error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As per usual, we will start by creating model data object which has a validation
    set, training set, and optional test set built into it. From that, we will get
    a learner, we will then optionally call `lr_find`, then call `learn.fit` and so
    forth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference here is we are not using `ImageClassifierData.from_csv` or `.from_paths`,
    we need a different kind of model data called `ColumnarModelData` and we call
    `from_data_frame`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PATH` : Specifies where to store model files etc'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val_idx` : A list of the indexes of the rows that we want to put in the validation
    set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`df` : data frame that contains independent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yl` : We took the dependent variable `y` returned by `proc_df` and took the
    log of that (i.e. `np.log(y)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat_flds` : which columns to be treated as categorical. Remember, by this
    time, everything is a number, so unless we specify, it will treat them all as
    continuous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have a standard model data object which we are familiar with and contains
    `train_dl`, `val_dl` , `train_ds` , `val_ds` , etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are asking it to create a learner that is suitable for our model data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.04` : how much dropout to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[1000,500]` : how many activations to have in each layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[0.001,0.01]` : how many dropout to use at later layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key New Concept: Embeddings [[45:39](https://youtu.be/gbceqO8PpBg?t=45m39s)]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s forget about categorical variables for a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, you never want to put ReLU in the last layer because softmax needs
    negatives to create low probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple view of fully connected neural net [**[**49:13**](https://youtu.be/gbceqO8PpBg?t=49m13s)**]:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For regression problems (not classification), you can even skip the softmax
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables [[50:49](https://youtu.be/gbceqO8PpBg?t=50m49s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We create a new matrix of 7 rows and as many columns as we choose (4, for example)
    and fill it with floating numbers. To add “Sunday” to our rank 1 tensor with continuous
    variables, we do a look up to this matrix, which will return 4 floating numbers,
    and we use them as “Sunday”.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, these numbers are random. But we can put them through a neural net
    and update them in a way that reduces the loss. In other words, this matrix is
    just another bunch of weights in our neural net. And matrices of this type are
    called “**embedding matrices**”. An embedding matrix is something where we start
    out with an integer between zero and maximum number of levels of that category.
    We index into the matrix to find a particular row, and we append it to all of
    our continuous variables, and everything after that is just the same as before
    (linear → ReLU → etc).
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What do those 4 numbers represent?[[55:12](https://youtu.be/gbceqO8PpBg?t=55m12s)]
    We will learn more about that when we look at collaborative filtering, but for
    now, they are just parameters that we are learning that happen to end up giving
    us a good loss. We will discover later that these particular parameters often
    are human interpretable and quite interesting but that a side effect.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Do you have good heuristics for the dimensionality of the embedding
    matrix? [[55:57](https://youtu.be/gbceqO8PpBg?t=55m57s)] I sure do! Let’s take
    a look.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here is a list of every categorical variable and its cardinality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if there were no missing values in the original data, you should still
    set aside one for unknown just in case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rule of thumb for determining the embedding size is the cardinality size
    divided by 2, but no bigger than 50.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then pass the embedding size to the learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Question**: Is there a way to initialize embedding matrices besides random?
    [[58:14](https://youtu.be/gbceqO8PpBg?t=58m14s)] We will probably talk about pre-trained
    more later in the course, but the basic idea is if somebody else at Rossmann had
    already trained a neural network to predict cheese sales, you may as well start
    with their embedding matrix of stores to predict liquor sales. This is what happens,
    for example, at Pinterest and Instacart. Instacart uses this technique for routing
    their shoppers, and Pinterest uses it for deciding what to display on a webpage.
    They have embedding matrices of products/stores that get shared in the organization
    so people do not have to train new ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What is the advantage of using embedding matrices over one-hot-encoding?
    [[59:23](https://youtu.be/gbceqO8PpBg?t=59m23s)] For the day of week example above,
    instead of the 4 numbers, we could have easily passed 7 numbers (e.g. [0, 1, 0,
    0, 0, 0, 0] for Sunday). That also is a list of floats and that would totally
    work — and that is how, generally speaking, categorical variables have been used
    in statistics for many years (called “dummy variables”). The problem is, the concept
    of Sunday could only ever be associated with a single floating-point number. So
    it gets this kind of linear behavior — it says Sunday is more or less of a single
    thing. With embeddings, Sunday is a concept in four dimensional space. What we
    tend to find happen is that these embedding vectors tend to get these rich semantic
    concepts. For example, if it turns out that weekends have a different behavior,
    you tend to see that Saturday and Sunday will have some particular number higher.'
  prefs: []
  type: TYPE_NORMAL
- en: By having higher dimensionality vector rather than just a single number, it
    gives the deep learning network a chance to learn these rich representations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The idea of an embedding is what is called a “distributed representation” —
    the most fundamental concept of neural networks. This is the idea that a concept
    in neural network has a high dimensional representation which can be hard to interpret.
    These numbers in this vector does not even have to have just one meaning. It could
    mean one thing if this is low and that one is high, and something else if that
    one is high and that one is low because it is going through this rich nonlinear
    function. It is this rich representation that allows it to learn such interesting
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Are embeddings suitable for certain types of variables? [[01:02:45](https://youtu.be/gbceqO8PpBg?t=1h2m45s)]
    Embedding is suitable for any categorical variables. The only thing it cannot
    work well for would be something with too high cardinality. If you had 600,000
    rows and a variable had 600,000 levels, that is just not a useful categorical
    variable. But in general, the third winner in this competition really decided
    that everything that was not too high cardinality, they put them all as categorical.
    The good rule of thumb is if you can make a categorical variable, you may as well
    because that way it can learn this rich distributed representation; where else
    if you leave it as continuous, the most it can do is to try and find a single
    functional form that fits it well.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix algebra behind the scene [[01:04:47](https://youtu.be/gbceqO8PpBg?t=1h4m47s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking up an embedding with an index is identical to doing a matrix product
    between a one-hot encoded vector and the embedding matrix. But doing so is terribly
    inefficient, so modern libraries implement this as taking an integer and doing
    a look up into an array.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Could you touch on using dates and times as categorical and how
    that affects seasonality? [[01:06:59](https://youtu.be/gbceqO8PpBg?t=1h6m59s)]
    There is a Fast.ai function called `add_datepart` which takes a data frame and
    a column name. It optionally removes the column from the data frame and replaces
    it with lots of column representing all of the useful information about that date
    such as day of week, day of month, month of year, etc (basically everything Pandas
    gives us).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So for example, day of week now becomes eight rows by four columns embedding
    matrix. Conceptually this allows our model to create some interesting time series
    models. If there is something that has a seven day period cycle that goes up on
    Mondays and down on Wednesdays but only for daily and only in Berlin, it can totally
    do that — it has all the information it needs. This is a fantastic way to deal
    with time series. You just need to make sure that the cycle indicator in your
    time series exists as a column. If you did not have a column called day of week,
    it would be very difficult for the neural network to learn to do mod seven and
    look up in an embedding matrix. It is not impossible but really hard. If you are
    predicting sales of beverages in San Francisco, you probably want a list of when
    the ball game is on at AT&T park because that is going to to impact how many people
    are drinking beer in SoMa. So you need to make sure that the basic indicators
    or periodicity is in your data, and as long as they are there, neural net is going
    to learn to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Learner [[01:10:13](https://youtu.be/gbceqO8PpBg?t=1h10m13s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`emb_szs` : embedding size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`len(df.columns)-len(cat_vars)` : number of continuous variables in the data
    frame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.04` : embedding matrix has its own dropout and this is the dropout rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` : how many outputs we want to create (output of the last linear layer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[1000, 500]` : number of activations in the first linear layer, and the second
    linear layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[0.001, 0.01]` : dropout in the first linear layer, and the second linear
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_range` : we will not worry about that for now'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`metrics` : this is a custom metric which specifies a function to be called
    at the end of every epoch and prints out a result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: By using all of the training data, we achieved a RMSPE around 0.09711\. There
    is a big difference between public leader board and private leader board, but
    we are certainly in the top end of this competition.
  prefs: []
  type: TYPE_NORMAL
- en: So this is a technique for dealing with time series and structured data. Interestingly,
    compared to the group that used this technique ([Entity Embeddings of Categorical
    Variables](https://arxiv.org/abs/1604.06737)), the second place winner did way
    more feature engineering. The winners of this competition were actually subject
    matter experts in logistics sales forecasting so they had their own code to create
    lots and lots of features. Folks at Pinterest who build a very similar model for
    recommendations also said that when they switched from gradient boosting machines
    to deep learning, they did way less feature engineering and it was much simpler
    model which requires less maintenance. So this is one of the big benefits of using
    this approach to deep learning — you can get state of the art results but with
    a lot less work.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Are we using any time series in any of these? [[01:15:01](https://youtu.be/gbceqO8PpBg?t=1h15m1s)]
    Indirectly, yes. As we just saw, we have a day of week, month of year, etc in
    our columns and most of them are being treated as categories, so we are building
    a distributed representation of January, Sunday, and so on. We are not using any
    classic time series techniques, all we are doing is true fully connected layers
    in a neural net. The embedding matrix is able to deal with things like day of
    week periodicity in a much richer way than than any standard time series techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question** regarding the difference between image models and this model [[01:15:59](https://youtu.be/gbceqO8PpBg?t=1h15m59s)]:
    There is a difference in a way we are calling `get_learner`. In imaging we just
    did `Learner.trained` and pass the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For these kinds of models, in fact for a lot of the models, the model we build
    depends on the data. In this case, we need to know what embedding matrices we
    have. So in this case, the data objects creates the learner (upside down to what
    we have seen before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Summary of steps** (if you want to use this for your own dataset) [[01:17:56](https://youtu.be/gbceqO8PpBg?t=1h17m56s)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**. List categorical variable names, and list continuous variable names,
    and put them in a Pandas data frame'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**. Create a list of which row indexes you want in your validation
    set'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**. Call this exact line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4**. Create a list of how big you want each embedding matrix to be'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5**. Call `get_learner` — you can use these exact parameters to start
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6**. Call `m.fit`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: How to use data augmentation for this type of data, and how does
    dropout work? [[01:18:59](https://youtu.be/gbceqO8PpBg?t=1h18m59s)] No idea. Jeremy
    thinks it has to be domain-specific, but he has never seen any paper or anybody
    in industry doing data augmentation with structured data and deep learning. He
    thinks it can be done but has not seen it done. What dropout is doing is exactly
    the same as before.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What is the downside? Almost no one is using this. Why not? [[01:20:41](https://youtu.be/gbceqO8PpBg?t=1h20m41s)]
    Basically the answer is as we discussed before, no one in academia almost is working
    on this because it is not something that people publish on. As a result, there
    have not been really great examples people could look at and say “oh here is a
    technique that works well so let’s have our company implement it”. But perhaps
    equally importantly, until now with this Fast.ai library, there has not been any
    way to do it conveniently. If you wanted to implement one of these models, you
    had to write all the custom code yourself. There are a lot of big commercial and
    scientific opportunity to use this and solve problems that previously haven’t
    been solved very well.'
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing [[01:23:37](https://youtu.be/gbceqO8PpBg?t=1h23m37s)]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most up-and-coming area of deep learning and it is two or three years behind
    computer vision. The state of software and some of the concepts is much less mature
    than it is for computer vision. One of the things you find in NLP is there are
    particular problems you can solve and they have particular names. There is a particular
    kind of problem in NLP called “language modeling” and it has a very specific definition
    — it means build a model where given a few words of a sentence, can you predict
    what the next word is going to be.
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling [[01:25:48](https://youtu.be/gbceqO8PpBg?t=1h25m48s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have 18 months worth of papers from arXiv (arXiv.org) and this is an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`<cat>` — category of the paper. CSNI is Computer Science and Networking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<summ>` — abstract of the paper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are what the output of a trained language model looks like. We did simple
    little tests in which you pass some priming text and see what the model thinks
    should come next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It learned by reading arXiv papers that somebody who is writing about computer
    networking would talk like this. Remember, it started out not knowing English
    at all. It started out with an embedding matrix for every word in English that
    was random. By reading lots of arXiv papers, it learned what kind of words followed
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we tried specifying a category to be computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: It not only learned how to write English pretty well, but also after you say
    something like “convolutional neural network” you should then use parenthesis
    to specify an acronym “(CNN)”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: A language model can be incredibly deep and subtle, so we are going to try and
    build that — not because we care about this at all, but because we are trying
    to create a pre-trained model which is used to do some other tasks. For example,
    given an IMDB movie review, we will figure out whether they are positive or negative.
    It is a lot like cats vs. dogs — a classification problem. So we would really
    like to use a pre-trained network which at least knows how to read English. So
    we will train a model that predicts a next word of a sentence (i.e. language model),
    and just like in computer vision, stick some new layers on the end and ask it
    to predict whether something is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: IMDB [[1:31:11](https://youtu.be/gbceqO8PpBg?t=1h31m11s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: What we are going to do is to train a language model, making that the pre-trained
    model for a classification model. In other words, we are trying to leverage exactly
    what we learned in our computer vision which is how to do fine-tuning to create
    powerful classification models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: why would doing directly what you want to do not work? [[01:31:34](https://youtu.be/gbceqO8PpBg?t=1h31m34s)]
    It just turns out it doesn’t empirically. There are several reasons. First of
    all, we know fine-tuning a pre-trained network is really powerful. So if we can
    get it to learn some related tasks first, then we can use all that information
    to try and help it on the second task. The other is IMDB movie reviews are up
    to a thousands words long. So after reading a thousands words knowing nothing
    about how English is structured or concept of a word or punctuation, all you get
    is a 1 or a 0 (positive or negative). Trying to learn the entire structure of
    English and then how it expresses positive and negative sentiments from a single
    number is just too much to expect.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Is this similar to Char-RNN by Karpathy? [[01:33:09](https://youtu.be/gbceqO8PpBg?t=1h33m9s)]
    This is somewhat similar to Char-RNN which predicts the next letter given a number
    of previous letters. Language model generally work at a word level (but they do
    not have to), and we will focus on word level modeling in this course.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: To what extent are these generated words/sentences actual copies
    of what it found in the training set? [[01:33:44](https://youtu.be/gbceqO8PpBg?t=1h33m44s)]
    Words are definitely words it has seen before because it is not a character level
    so it can only give us the word it has seen before. Sentences, there are rigorous
    ways of doing it but the easiest would be by looking at examples like above, you
    get a sense of it. Most importantly, when we train the language model, we will
    have a validation set so that we are trying to predict the next word of something
    that has never seen before. There are tricks to using language models to generate
    text like [beam search](http://forums.fast.ai/t/tricks-for-using-language-models-to-generate-text/8127/2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use cases of text classification:'
  prefs: []
  type: TYPE_NORMAL
- en: For hedge fund, identify things in articles or Twitter that caused massive market
    drops in the past.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify customer service queries which tend to be associated with people who
    cancel their contracts in the next month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organize documents into whether they are part of legal discovery or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`torchtext` — PyTorch’s NLP library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data [[01:37:05](https://youtu.be/gbceqO8PpBg?t=1h37m5s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IMDB [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We do not have separate test and validation in this case. Just like in vision,
    the training directory has bunch of files in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will check how many words are in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Before we can do anything with text, we have to turn it into a list of tokens.
    Token is basically like a word. Eventually we will turn them into a list of numbers,
    but the first step is to turn it into a list of words — this is called “tokenization”
    in NLP. A good tokenizer will do a good job of recognizing pieces in your sentence.
    Each separated piece of punctuation will be separated, and each part of multi-part
    word will be separated as appropriate. Spacy does a lot of NLP stuff, and it has
    the best tokenizer Jeremy knows. So Fast.ai library is designed to work well with
    the Spacey tokenizer as with torchtext.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a field [[01:41:01](https://youtu.be/gbceqO8PpBg?t=1h41m1s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A field is a definition of how to pre-process some text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`lower=True` — lowercase the text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenize=spacy_tok` — tokenize with `spacy_tok`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we create the usual Fast.ai model data object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`PATH` : as per usual where the data is, where to save models, etc'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TEXT` : torchtext’s Field definition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**FILES` : list of all of the files we have: training, validation, and test
    (to keep things simple, we do not have a separate validation and test set, so
    both points to validation folder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs` : batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bptt` : Back Prop Through Time. It means how long a sentence we will stick
    on the GPU at once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_freq=10` : In a moment, we are going to be replacing words with integers
    (a unique index for every word). If there are any words that occur less than 10
    times, just call it unknown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After building our `ModelData` object, it automatically fills the `TEXT` object
    with a very important attribute: `TEXT.vocab`. This is a *vocabulary*, which stores
    which unique words (or *tokens*) have been seen in the text, and how each word
    will be mapped to a unique integer id.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`itos` is sorted by frequency except for the first two special ones. Using
    `vocab`, torchtext will turn words into integer IDs for us :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Question**: Is it common to do any stemming or lemma-tizing? [[01:45:47](https://youtu.be/gbceqO8PpBg?t=1h45m47s)]
    Not really, no. Generally tokenization is what we want. To keep it as general
    as possible, we want to know what is coming next so whether it is future tense
    or past tense or plural or singular, we don’t really know which things are going
    to be interesting and which are not, so it seems that it is generally best to
    leave it alone as much as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: When dealing with natural language, isn’t context important?
    Why are we tokenizing and looking at individual word? [[01:46:38](https://youtu.be/gbceqO8PpBg?t=1h46m38s)]
    No, we are not looking at individual word — they are still in order. Just because
    we replaced I with a number 12, they are still in that order. There is a different
    way of dealing with natural language called “bag of words” and they do throw away
    the order and context. In the Machine Learning course, we will be learning about
    working with bag of words representations but my belief is that they are no longer
    useful or in the verge of becoming no longer useful. We are starting to learn
    how to use deep learning to use context properly.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size and BPTT [[01:47:40](https://youtu.be/gbceqO8PpBg?t=1h47m40s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What happens in a language model is even though we have lots of movie reviews,
    they all get concatenated together into one big block of text. So we predict the
    next word in this huge long thing which is all of the IMDB movie reviews concatenated
    together.
  prefs: []
  type: TYPE_NORMAL
- en: We split up the concatenated reviews into batches. In this case, we will split
    it to 64 sections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then move each section underneath the previous one, and transpose it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We end up with a matrix which is 1 million by 64.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then grab a little chunk at time and those chunk lengths are **approximately**
    equal to BPTT. Here, we grab a little 70 long section and that is the first thing
    we chuck into our GPU (i.e. the batch).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We grab our first training batch by wrapping data loader with `iter` then calling
    `next`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We got back a 75 by 64 tensor (approximately 70 rows but not exactly)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A neat trick torchtext does is to randomly change the `bptt` number every time
    so each epoch it is getting slightly different bits of text — similar to shuffling
    images in computer vision. We cannot randomly shuffle the words because they need
    to be in the right order, so instead, we randomly move their breakpoints a little
    bit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value is also 75 by 64 but for minor technical reasons it is flattened
    out into a single vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question**: Why not split by a sentence? [[01:53:40](https://youtu.be/gbceqO8PpBg?t=1h53m40s)]
    Not really. Remember, we are using columns. So each of our column is of length
    about 1 million, so although it is true that those columns are not always exactly
    finishing on a full stop, they are so darn long we do not care. Each column contains
    multiple sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: Pertaining to this question, Jeremy found what is in this language model matrix
    a little mind-bending for quite a while, so do not worry if it takes a while and
    you have to ask a thousands questions.
  prefs: []
  type: TYPE_NORMAL
- en: Create a model [[01:55:46](https://youtu.be/gbceqO8PpBg?t=1h55m46s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a model data object that can fee d us batches, we can create
    a model. First, we are going to create an embedding matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the: # batches; # unique tokens in the vocab; length of the dataset;
    # of words'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is our embedding matrix looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a high cardinality categorical variable and furthermore, it is the only
    variable — this is typical in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding size is 200 which is much bigger than our previous embedding vectors.
    Not surprising because a word has a lot more nuance to it than the concept of
    Sunday. **Generally, an embedding size for a word will be somewhere between 50
    and 600.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Researchers have found that large amounts of *momentum* (which we’ll learn
    about later) don’t work well with these kinds of *RNN* models, so we create a
    version of the *Adam* optimizer with less momentum than its default of `0.9`.
    Any time you are doing NLP, you should probably include this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Fast.ai uses a variant of the state of the art [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182)
    developed by Stephen Merity. A key feature of this model is that it provides excellent
    regularization through [Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout).
    There is no simple way known (yet!) to find the best values of the dropout parameters
    below — you just have to experiment…
  prefs: []
  type: TYPE_NORMAL
- en: However, the other parameters (`alpha`, `beta`, and `clip`) shouldn't generally
    need tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the last lecture, we will learn what the architecture is and what all these
    dropouts are. For now, just know it is the same as per usual, if you try to build
    an NLP model and you are under-fitting, then decrease all these dropouts, if overfitting,
    then increase all these dropouts in roughly this ratio. Since this is such a recent
    paper so there is not a lot of guidance but these ratios worked well — it is what
    Stephen has been using as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is another kind of way we can avoid overfitting that we will talk about
    in the last class. For now, `learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)`
    works reliably so all of your NLP models probably want this particular line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learner.clip=0.3` : when you look at your gradients and you multiply them
    by the learning rate to decide how much to update your weights by, this will not
    allow them be more than 0.3\. This is a cool little trick to prevent us from taking
    too big of a step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details do not matter too much right now, so you can use them as they are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question**: There are word embedding out there such as Word2vec or GloVe.
    How are they different from this? And why not initialize the weights with those
    initially? [[02:02:29](https://youtu.be/gbceqO8PpBg?t=2h2m29s)] People have pre-trained
    these embedding matrices before to do various other tasks. They are not called
    pre-trained models; they are just a pre-trained embedding matrix and you can download
    them. There is no reason we could not download them. I found that building a whole
    pre-trained model in this way did not seem to benefit much if at all from using
    pre-trained word vectors; where else using a whole pre-trained language model
    made a much bigger difference. Maybe we can combine both to make them a little
    better still.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** What is the architecture of the model? [[02:03:55](https://youtu.be/gbceqO8PpBg?t=2h3m55s)]
    We will be learning about the model architecture in the last lesson but for now,
    it is a recurrent neural network using something called LSTM (Long Short Term
    Memory).'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting [[02:04:24](https://youtu.be/gbceqO8PpBg?t=2h4m24s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the sentiment analysis section, we'll just need half of the language model
    - the *encoder*, so we save that part.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Language modeling accuracy is generally measured using the metric *perplexity*,
    which is simply `exp()` of the loss function we used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Testing [[02:04:53](https://youtu.be/gbceqO8PpBg?t=2h4m53s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can play around with our language model a bit to check it seems to be working
    OK. First, let’s create a short bit of text to ‘prime’ a set of predictions. We’ll
    use our torchtext field to numericalize it so we can feed it to our language model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We haven’t yet added methods to make it easy to test a language model, so we’ll
    need to manually go through the steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what the top 10 predictions were for the next word after our short
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: …and let’s see if our model can generate a bit more text all by itself!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Sentiment [[02:05:09](https://youtu.be/gbceqO8PpBg?t=2h5m9s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So we had pre-trained a language model and now we want to fine-tune it to do
    sentiment classification.
  prefs: []
  type: TYPE_NORMAL
- en: To use a pre-trained model, we will need to the saved vocab from the language
    model, since we need to ensure the same words map to the same IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`sequential=False` tells torchtext that a text field should be tokenized (in
    this case, we just want to store the ''positive'' or ''negative'' single label).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This time, we need to not treat the whole thing as one big piece of text but
    every review is separate because each one has a different sentiment attached to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '`splits` is a torchtext method that creates train, test, and validation sets.
    The IMDB dataset is built into torchtext, so we can take advantage of that. Take
    a look at `lang_model-arxiv.ipynb` to see how to define your own fastai/torchtext
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: fastai can create a `ModelData` object directly from torchtext `splits`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now you can go ahead and call `get_model` that gets us our learner. Then we
    can load into it the pre-trained language model (`load_encoder`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Because we’re fine-tuning a pretrained model, we’ll use differential learning
    rates, and also increase the max gradient for clipping, to allow the SGDR to work
    better.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We make sure all except the last layer is frozen. Then we train a bit, unfreeze
    it, train it a bit. The nice thing is once you have got a pre-trained language
    model, it actually trains really fast.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'A recent paper from Bradbury et al, [Learned in translation: contextualized
    word vectors](https://einstein.ai/research/learned-in-translation-contextualized-word-vectors),
    has a handy summary of the latest academic research in solving this IMDB sentiment
    analysis problem. Many of the latest algorithms shown are tuned for this specific
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: As you see, we just got a new state of the art result in sentiment analysis,
    decreasing the error from 5.9% to 5.5%! You should be able to get similarly world-class
    results on other NLP classification problems using the same basic steps.
  prefs: []
  type: TYPE_NORMAL
- en: There are many opportunities to further improve this, although we won’t be able
    to get to them until part 2 of this course.
  prefs: []
  type: TYPE_NORMAL
- en: For example we could start training language models that look at lots of medical
    journals and then we could make a downloadable medical language model that then
    anybody could use to fine-tune on a prostate cancer subset of medical literature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could also combine this with pre-trained word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could have pre-trained a Wikipedia corpus language model and then fine-tuned
    it into an IMDB language model, and then fine-tune that into an IMDB sentiment
    analysis model and we would have gotten something better than this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a really fantastic researcher called Sebastian Ruder who is the only
    NLP researcher who has been really writing a lot about pre-training, fine-tuning,
    and transfer learning in NLP. Jeremy was asking him why this is not happening
    more, and his view was it is because there is not a software to make it easy.
    Hopefully Fast.ai will change that.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Filtering Introduction [[02:11:38](https://youtu.be/gbceqO8PpBg?t=2h11m38s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Data available from [http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: It contains ratings by users. Our goal will be for some user-movie combination
    we have not seen before, we have to predict a rating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: To make it more interesting, we will also actually download a list of movies
    so that we can interpret what is actually in these embedding matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This is what we are creating — this kind of cross tab of users by movies.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to look ahead and you will find that most of the steps are familiar
    to you already.
  prefs: []
  type: TYPE_NORMAL
