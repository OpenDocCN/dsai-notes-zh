- en: 'Deep Learning 2: Part 1 Lesson 4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第1部分第4课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai课程*](http://www.fast.ai/)*中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: '[Lesson 4](http://forums.fast.ai/t/wiki-lesson-4/9402/1)'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[第4课](http://forums.fast.ai/t/wiki-lesson-4/9402/1)'
- en: 'Articles by students:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学生的文章：
- en: '[Improving the way we work with learning rate](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[改进我们使用学习率的方式](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)'
- en: '[The Cyclical Learning Rate technique](http://teleported.in/posts/cyclic-learning-rate/)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[循环学习率技术](http://teleported.in/posts/cyclic-learning-rate/)'
- en: '[Exploring Stochastic Gradient Descent with Restarts (SGDR)](/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索带有重启的随机梯度下降（SGDR）](/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)'
- en: '[Transfer Learning using differential learning rates](https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用不同学习率的迁移学习](https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00)'
- en: '[Getting Computers To See Better Than Humans](/@ArjunRajkumar/getting-computers-to-see-better-than-humans-346d96634f73)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让计算机看得比人类更好](/@ArjunRajkumar/getting-computers-to-see-better-than-humans-346d96634f73)'
- en: Dropout [04:59]
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout [04:59]
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`precompute=True` : Pre-compute the activations that come out of the last convolutional
    layer. Remember, activation is a number that is calculated based on some weights/parameter
    that makes up kernels/filters, and they get applied to the previous layer’s activations
    or inputs.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precompute=True`：预先计算出最后一个卷积层的激活。请记住，激活是根据一些权重/参数计算出来的数字，这些权重/参数构成了卷积核/滤波器，并且它们被应用于前一层的激活或输入。'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`learn` — This will display the layers we added at the end. These are the layers
    we train when `precompute=True`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn` — 这将显示我们在末尾添加的层。这些是我们在`precompute=True`时训练的层'
- en: '(0), (4): `BatchNorm` will be covered in the last lesson'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '(0), (4): `BatchNorm`将在最后一课中讨论'
- en: '(1), (5): `Dropout`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '(1), (5): `Dropout`'
- en: (2):`Linear` layer simply means a matrix multiply. This is a matrix which has
    1024 rows and 512 columns, so it will take in 1024 activations and spit out 512
    activations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (2):`Linear`层简单地意味着矩阵相乘。这是一个具有1024行和512列的矩阵，因此它将接收1024个激活并输出512个激活。
- en: (3):`ReLU` — just replace negatives with zero
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (3):`ReLU` — 只是用零替换负数
- en: '(6): `Linear` — the second linear layer that takes those 512 activations from
    the previous linear layer and put them through a new matrix multiply 512 by 120
    and outputs 120 activations'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '(6): `Linear` — 第二个线性层，将前一个线性层的512个激活通过一个新的矩阵相乘512乘以120，并输出120个激活'
- en: '(7): `Softmax` — The activation function that returns numbers that adds up
    to 1 and each of them is between 0 and 1:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '(7): `Softmax` — 返回总和为1的数字的激活函数，每个数字都在0到1之间：'
- en: For minor numerical precision reasons, it turns out to be better to tahe the
    log of the softmax than softmax directly [[15:03](https://youtu.be/gbceqO8PpBg?t=15m3s)].
    That is why when we get predictions out of our models, we have to do `np.exp(log_preds)`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 出于微小的数值精度原因，事实证明取softmax的对数比直接取softmax要好[[15:03](https://youtu.be/gbceqO8PpBg?t=15m3s)]。这就是为什么当我们从模型中获取预测时，我们必须执行`np.exp(log_preds)`。
- en: What is `Dropout` and what is `p`? [[08:17](https://youtu.be/gbceqO8PpBg?t=8m17s)]
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是`Dropout`和什么是`p`？[[08:17](https://youtu.be/gbceqO8PpBg?t=8m17s)]
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we applied dropout with `p=0.5` to `Conv2` layer, it would look like the
    above. We go through, pick an activation, and delete it with 50% chance. So `p=0.5`
    is the probability of deleting that cell. Output does not actually change by very
    much, just a little bit.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对`Conv2`层应用了`p=0.5`的dropout，它看起来像上面那样。我们遍历，选择一个激活，并以50%的概率删除它。因此，`p=0.5`是删除该单元的概率。输出实际上并没有太大变化，只是有点。
- en: Randomly throwing away half of the activations in a layer has an interesting
    effect. An important thing to note is for each mini-batch, we throw away a different
    random half of activations in that layer. It forces it to not overfit. In other
    words, when a particular activation that learned just that exact dog or exact
    cat gets dropped out, the model has to try and find a representation that continues
    to work even as random half of the activations get thrown away every time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个层中随机丢弃一半的激活具有有趣的效果。需要注意的一点是，对于每个小批量，我们在该层中随机丢弃不同的一半激活。这迫使它不会过拟合。换句话说，当一个特定的激活被删除时，模型必须尝试找到一个表示，即使在每次随机丢弃一半激活时，它仍然有效。
- en: This has been absolutely critical in making modern deep learning work and just
    about solve the problem of generalization. Geoffrey Hinton and his colleagues
    came up with this idea loosely inspired by the way the brain works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于使现代深度学习工作并几乎解决泛化问题至关重要。Geoffrey Hinton和他的同事们提出了这个想法，受到了大脑工作方式的启发。
- en: '`p=0.01` will throw away 1% of the activations. It will not change things up
    very much at all, and will not prevent overfitting (not generalized).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p=0.01`将丢弃1%的激活。这几乎不会改变事情，也不会防止过拟合（不具有泛化性）。'
- en: '`p=0.99` will throw away 99% of the activations. Not going to overfit and great
    for generalization, but will kill your accuracy.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p=0.99`将丢弃99%的激活。不会过拟合，对泛化很好，但会降低准确性。'
- en: By default, the first layer is `0.25` and second layer is `0.5`[17:54]. If you
    find it is overfitting, start bumping it up — try setting all to `0.5`, still
    overfitting, try `0.7` etc. If you are under-fitting, you can try making it lower
    but is unlikely you would need to make it much lower.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，第一层为`0.25`，第二层为`0.5`。如果发现过拟合，开始逐步增加-尝试将所有设置为`0.5`，仍然过拟合，尝试`0.7`等。如果欠拟合，可以尝试降低，但不太可能需要降低太多。
- en: ResNet34 has less parameters so it does not overfit as much, but for bigger
    architecture like ResNet50, you often need to increase dropout.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet34的参数较少，因此不会过度拟合，但对于更大的架构如ResNet50，通常需要增加辍学率。
- en: Have you wondered why the validation losses better than the training losses
    particularly early in the training? [[12:32](https://youtu.be/gbceqO8PpBg?t=12m32s)]
    This is because we turn off dropout when we run inference (i.e. making prediction)
    on the validation set. We want to be using the best model we can.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否想知道为什么验证损失在训练早期特别好？这是因为我们在对验证集进行推断（即进行预测）时关闭了辍学。我们希望使用我们能够使用的最佳模型。
- en: '**Question**: Do you have to do anything to accommodate for the fact that you
    are throwing away activations? [[13:26](https://youtu.be/gbceqO8PpBg?t=13m26s)]
    We do not, but PyTorch does two things when you say `p=0.5`. It throws away half
    of the activations, and it doubles all the activations that are already there
    so that average activation does not change.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您是否需要做任何事情来适应丢弃激活的事实？我们不需要，但当您说`p=0.5`时，PyTorch会执行两件事。它会丢弃一半的激活，并将所有已经存在的激活加倍，以使平均激活不变。
- en: 'In Fast.ai, you can pass in `ps` which is the `p` value for all of the added
    layers. It will not change the dropout in the pre-trained network since it should
    have been already trained with some appropriate level of dropout:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fast.ai中，您可以传递`ps`，这是所有添加层的`p`值。它不会改变预训练网络中的辍学，因为它应该已经使用了适当水平的辍学进行训练：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can remove dropout by setting `ps=0.` but even after a couple epochs, we
    start to massively overfit (training loss ≪ validation loss):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过设置`ps=0.`来删除辍学，但即使经过几个时期，我们开始严重过拟合（训练损失≪验证损失）：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When `ps=0.` , dropout layers are not even added to the model:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当`ps=0.`时，辍学层甚至不会添加到模型中：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You may have noticed, it has been adding two `Linear` layers [[16:19](https://youtu.be/gbceqO8PpBg?t=16m19s)].
    We do not have to do that. There is `xtra_fc` parameter you can set. Note: you
    do need at least one which takes the output of the convolutional layer (4096 in
    this example) and turns it into the number of classes (120 dog breeds):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，它已经添加了两个`Linear`层。我们不必这样做。您可以设置`xtra_fc`参数。注意：您至少需要一个，它接受卷积层的输出（在此示例中为4096）并将其转换为类别数（120种狗品种）：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Question**: Is there a particular way in which you can determine if it is
    overfitted? [[19:53](https://youtu.be/gbceqO8PpBg?t=19m53s)]. Yes, you can see
    the training loss is much lower than the validation loss. You cannot tell if it
    is *too* overfitted. Zero overfitting is not generally optimal. The only thing
    you are trying to do is to get the validation loss low, so you need to play around
    with a few things and see what makes the validation loss low. You will get a feel
    for it overtime for your particular problem what too much overfitting looks like.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：有没有一种特定的方法可以确定是否过拟合？是的，您可以看到训练损失远低于验证损失。您无法确定是否过度拟合。零过拟合通常不是最佳选择。您唯一要做的就是使验证损失降低，因此您需要尝试一些方法，看看什么可以使验证损失降低。随着时间的推移，您会对您的特定问题过度拟合的程度有所了解。
- en: '**Question**: Why does average activation matter? [[21:15](https://youtu.be/gbceqO8PpBg?t=21m15s)]
    If we just deleted a half of activations, the next activation who takes them as
    input will also get halved, and everything after that. For example, fluffy ears
    are fluffy if this is greater than 0.6, and now it is only fluffy if it is greater
    than 0.3 — which is changing the meaning. The goal here is delete activations
    without changing the meanings.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么平均激活很重要？如果我们只删除了一半的激活，那么接下来以它们作为输入的激活也会减半，以及之后的所有激活。例如，如果这大于0.6，则毛茸茸的耳朵是毛茸茸的，现在只有大于0.3才是毛茸茸的-这改变了含义。这里的目标是删除激活而不改变含义。
- en: '**Question**: Can we have different level of dropout by layer? [[22:41](https://youtu.be/gbceqO8PpBg?t=22m41s)]
    Yes, that is why it is called `ps`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我们可以通过层设置不同的辍学率吗？是的，这就是为什么称为`ps`：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There is no rule of thumb for when earlier or later layers should have different
    amounts of dropout yet.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前还没有关于早期或后期层应该具有不同辍学率的经验法则。
- en: If in doubt, use the same dropout for every fully connected layer.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有疑问，对每个全连接层使用相同的辍学率。
- en: Often people only put dropout on the very last linear layer.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常人们只在最后一个线性层上放置辍学。
- en: '**Question**: Why monitor loss and not accuracy? [[23:53](https://youtu.be/gbceqO8PpBg?t=23m53s)]
    Loss is the only thing that we can see for both the validation set and the training
    set. As we learn later, the loss is the thing that we are actually optimizing
    so it is easier to monitor and understand what that means.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么监控损失而不是准确率？损失是我们可以看到验证集和训练集的唯一东西。正如我们后来了解的那样，损失是我们实际优化的东西，因此更容易监控和理解其含义。
- en: '**Question**: Do we need to adjust the learning rate after adding dropouts?[[24:33](https://youtu.be/gbceqO8PpBg?t=24m33s)]
    It does not seem to impact the learning rate enough to notice. In theory, it might
    but not enough to affect us.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在添加辍学率后，我们需要调整学习率吗？似乎不会对学习率产生足够的影响以引起注意。理论上可能会，但不足以影响我们。
- en: Structured and Time Series Data [[25:03](https://youtu.be/gbceqO8PpBg?t=25m3s)]
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化和时间序列数据
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)
    / [Kaggle](https://www.kaggle.com/c/rossmann-store-sales)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)
    / [Kaggle](https://www.kaggle.com/c/rossmann-store-sales)'
- en: 'There are two types of columns:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的列：
- en: Categorical — It has a number of “levels” e.g. StoreType, Assortment
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类——它有一定数量的“级别”，例如StoreType、Assortment
- en: Continuous — It has a number where differences or ratios of that numbers have
    some kind of meanings e.g. CompetitionDistance
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续型——它有一个数字，该数字的差异或比率具有某种含义，例如CompetitionDistance
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Numbers like `Year` , `Month`, although we could treat them as continuous, we
    do not have to. If we decide to make `Year` a categorical variable, we are telling
    our neural net that for every different “level”of `Year` (2000, 2001, 2002), you
    can treat it totally differently; where-else if we say it is continuous, it has
    to come up with some kind of smooth function to fit them. So often things that
    actually are continuous but do not have many distinct levels (e.g. `Year`, `DayOfWeek`),
    it often works better to treat them as categorical.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像`Year`，`Month`这样的数字，尽管我们可以将它们视为连续的，但我们不必这样做。如果我们决定将`Year`作为分类变量，我们告诉我们的神经网络，对于每个不同的`Year`“级别”（2000、2001、2002），你可以完全不同地对待它；而如果我们说它是连续的，它必须提出某种平滑函数来拟合它们。因此，通常实际上是连续的但没有许多不同级别的事物（例如`Year`，`DayOfWeek`），通常最好将它们视为分类变量。
- en: Choosing categorical vs. continuous variable is a modeling decision you get
    to make. In summary, if it is categorical in the data, it has to be categorical.
    If it is continuous in the data, you get to pick whether to make it continuous
    or categorical in the model.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择分类变量还是连续变量是您要做的建模决策。总之，如果数据中是分类的，那么它必须是分类的。如果数据中是连续的，您可以选择在模型中将其视为连续或分类。
- en: Generally, floating point numbers are hard to make categorical as there are
    many levels (we call number of levels “**Cardinality**” — e.g. the cardinality
    of the day of week variable is 7).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，浮点数很难转换为分类变量，因为有许多级别（我们称级别数为“**基数**”——例如，星期几变量的基数为7）。
- en: '**Question**: Do you ever *bin* continuous variables?[[31:02](https://youtu.be/gbceqO8PpBg?t=31m2s)]
    Jeremy does not bin variables but one thing we could do with, say max temperature,
    is to group into 0–10, 10–20, 20–30, and call that categorical. Interestingly,
    a paper just came out last week in which a group of researchers found that sometimes
    binning can be helpful.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您是否对连续变量进行*分箱*？Jeremy不对变量进行分箱，但我们可以对最高温度等进行分组，例如0-10，10-20，20-30，并将其视为分类变量。有趣的是，上周刚刚发表了一篇论文，其中一组研究人员发现有时分箱可能有所帮助。'
- en: '**Question**: If you are using year as a category, what happens when a model
    encounters a year it has never seen before? [[31:47](https://youtu.be/gbceqO8PpBg?t=31m47s)]
    We will get there, but the short answer is that it will be treated as an unknown
    category. Pandas has a special category called unknown and if it sees a category
    it has not seen before, it gets treated as unknown.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果将年份用作类别，当模型遇到以前从未见过的年份时会发生什么？我们会解决这个问题，但简短的答案是它将被视为未知类别。Pandas有一个特殊的未知类别，如果它看到以前未见过的类别，它将被视为未知。'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Loop through `cat_vars` and turn applicable data frame columns into categorical
    columns.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环遍历`cat_vars`并将适用的数据框列转换为分类列。
- en: Loop through `contin_vars` and set them as `float32` (32 bit floating point)
    because that is what PyTorch expects.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环遍历`contin_vars`并将它们设置为`float32`（32位浮点数），因为这是PyTorch所期望的。
- en: Start with a small sample [[34:29](https://youtu.be/gbceqO8PpBg?t=34m29s)]
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从一个小样本开始
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here is what our data looks like. Even though we set some of the columns as
    “category” (e.g. ‘StoreType’, ‘Year’), Pandas still display as string in the notebook.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数据样子。尽管我们将一些列设置为“category”（例如‘StoreType’，‘Year’），但Pandas在笔记本中仍然显示为字符串。
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`proc_df` (process data frame) — A function in Fast.ai that does a few things:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`proc_df`（处理数据框）——Fast.ai中的一个函数，执行以下几项操作：'
- en: Pulls out the dependent variable, puts it into a separate variable, and deletes
    it from the original data frame. In other words, `df` do not have `Sales` column,
    and `y` only contains `Sales` column.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将因变量提取出来，放入一个单独的变量中，并从原始数据框中删除它。换句话说，`df`没有`Sales`列，而`y`只包含`Sales`列。
- en: '`do_scale` : Neural nets really like to have the input data to all be somewhere
    around zero with a standard deviation of somewhere around 1\. So we take our data,
    subtract the mean, and divide by the standard deviation to make that happen. It
    returns a special object which keeps track of what mean and standard deviation
    it used for that normalization so you can do the same to the test set later (`mapper`).'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`do_scale`：神经网络非常喜欢输入数据的均值大约为零，标准差大约为1。因此，我们取出数据，减去均值，除以标准差以实现这一点。它返回一个特殊对象，用于跟踪用于归一化的均值和标准差，以便稍后对测试集执行相同操作（`mapper`）。'
- en: 'It also handles missing values — for categorical variable, it becomes ID: 0
    and other categories become 1, 2, 3, and so on. For continuous variable, it replaces
    the missing value with the median and create a new boolean column that says whether
    it was missing or not.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它还处理缺失值——对于分类变量，它变为ID：0，其他类别变为1、2、3等。对于连续变量，它用中位数替换缺失值，并创建一个新的布尔列，指示是否缺失。
- en: After processing, year 2014 for example becomes 2 since categorical variables
    have been replaced with contiguous integers starting at zero. The reason for that
    is, we are going to put them into a matrix later, and we would not want the matrix
    to be 2014 rows long when it could just be two rows.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后，例如2014年变成2，因为分类变量已被替换为从零开始的连续整数。原因是，我们稍后要将它们放入矩阵中，我们不希望矩阵在可以只有两行时却有2014行。
- en: 'Now we have a data frame which does not contain the dependent variable and
    where everything is a number. That is where we need to get to to do deep learning.
    Check out Machine Learning course on further details. Another thing that is covered
    in Machine Learning course is validation sets. In this case, we need to predict
    the next two weeks of sales therefore we should create a validation set which
    is the last two weeks of our training set:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个不包含因变量且所有内容都是数字的数据框。这就是我们需要进行深度学习的地方。查看机器学习课程以获取更多详细信息。机器学习课程中涵盖的另一件事是验证集。在这种情况下，我们需要预测接下来两周的销售额，因此我们应该创建一个验证集，即我们训练集的最后两周：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[How (and why) to create a good validation set](http://www.fast.ai/2017/11/13/validation-sets/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何（以及为什么）创建一个好的验证集](http://www.fast.ai/2017/11/13/validation-sets/)'
- en: Let’s get straight to the deep learning action [[39:48](https://youtu.be/gbceqO8PpBg?t=39m48s)]
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们直接进入深度学习行动[[39:48](https://youtu.be/gbceqO8PpBg?t=39m48s)]
- en: For any Kaggle competitions, it is important that you have a strong understanding
    of your metric — how you are going to be judged. In [this competition](https://www.kaggle.com/c/rossmann-store-sales#evaluation),
    we are going to be judged on Root Mean Square Percentage Error (RMSPE).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何Kaggle竞赛，重要的是您要对您的指标有一个很好的理解 - 您将如何被评判。在[这个比赛](https://www.kaggle.com/c/rossmann-store-sales#evaluation)中，我们将根据均方根百分比误差（RMSPE）进行评判。
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When you take the log of the data, getting the root mean squared error will
    actually get you the root mean square percentage error.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您对数据取对数时，得到均方根误差将实际上得到均方根百分比误差。
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As per usual, we will start by creating model data object which has a validation
    set, training set, and optional test set built into it. From that, we will get
    a learner, we will then optionally call `lr_find`, then call `learn.fit` and so
    forth.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像往常一样，我们将首先创建一个模型数据对象，其中包含验证集、训练集和可选的测试集。从中，我们将得到一个学习器，然后我们可以选择调用`lr_find`，然后调用`learn.fit`等等。
- en: The difference here is we are not using `ImageClassifierData.from_csv` or `.from_paths`,
    we need a different kind of model data called `ColumnarModelData` and we call
    `from_data_frame`.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '这里的区别是我们不是使用`ImageClassifierData.from_csv`或`.from_paths`，我们需要一种称为`ColumnarModelData`的不同类型的模型数据，并调用`from_data_frame`。 '
- en: '`PATH` : Specifies where to store model files etc'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PATH`：指定存储模型文件等的位置'
- en: '`val_idx` : A list of the indexes of the rows that we want to put in the validation
    set'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val_idx`：我们要放入验证集的行的索引列表'
- en: '`df` : data frame that contains independent variable'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df`：包含自变量的数据框'
- en: '`yl` : We took the dependent variable `y` returned by `proc_df` and took the
    log of that (i.e. `np.log(y)`)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yl`：我们取`proc_df`返回的因变量`y`，并取其对数（即`np.log(y)`）'
- en: '`cat_flds` : which columns to be treated as categorical. Remember, by this
    time, everything is a number, so unless we specify, it will treat them all as
    continuous.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cat_flds`：要作为分类变量处理的列。请记住，到这个时候，一切都是数字，所以除非我们指定，否则它会将它们全部视为连续的。'
- en: Now we have a standard model data object which we are familiar with and contains
    `train_dl`, `val_dl` , `train_ds` , `val_ds` , etc.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个标准模型数据对象，我们熟悉并包含`train_dl`，`val_dl`，`train_ds`，`val_ds`等。
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we are asking it to create a learner that is suitable for our model data.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们要求它创建一个适合我们模型数据的学习器。
- en: '`0.04` : how much dropout to use'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0.04`：要使用多少dropout'
- en: '`[1000,500]` : how many activations to have in each layer'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[1000,500]`：每个层中要有多少激活'
- en: '`[0.001,0.01]` : how many dropout to use at later layers'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[0.001,0.01]`：在后续层中要使用多少dropout'
- en: 'Key New Concept: Embeddings [[45:39](https://youtu.be/gbceqO8PpBg?t=45m39s)]'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键新概念：嵌入[[45:39](https://youtu.be/gbceqO8PpBg?t=45m39s)]
- en: 'Let’s forget about categorical variables for a moment:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时忘记分类变量：
- en: Remember, you never want to put ReLU in the last layer because softmax needs
    negatives to create low probabilities.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您永远不要在最后一层中放置ReLU，因为softmax需要负数来创建低概率。
- en: '**Simple view of fully connected neural net [**[**49:13**](https://youtu.be/gbceqO8PpBg?t=49m13s)**]:**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**全连接神经网络的简单视图[**[**49:13**](https://youtu.be/gbceqO8PpBg?t=49m13s)**]**：'
- en: For regression problems (not classification), you can even skip the softmax
    layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题（而非分类），您甚至可以跳过softmax层。
- en: Categorical variables [[50:49](https://youtu.be/gbceqO8PpBg?t=50m49s)]
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类变量[[50:49](https://youtu.be/gbceqO8PpBg?t=50m49s)]
- en: We create a new matrix of 7 rows and as many columns as we choose (4, for example)
    and fill it with floating numbers. To add “Sunday” to our rank 1 tensor with continuous
    variables, we do a look up to this matrix, which will return 4 floating numbers,
    and we use them as “Sunday”.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个新的矩阵，有7行，以及我们选择的列数（例如4），并用浮点数填充它。要将“Sunday”添加到我们的连续变量的秩1张量中，我们查找这个矩阵，它将返回4个浮点数，我们将它们用作“Sunday”。
- en: Initially, these numbers are random. But we can put them through a neural net
    and update them in a way that reduces the loss. In other words, this matrix is
    just another bunch of weights in our neural net. And matrices of this type are
    called “**embedding matrices**”. An embedding matrix is something where we start
    out with an integer between zero and maximum number of levels of that category.
    We index into the matrix to find a particular row, and we append it to all of
    our continuous variables, and everything after that is just the same as before
    (linear → ReLU → etc).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些数字是随机的。但是我们可以将它们通过神经网络，并以减少损失的方式更新它们。换句话说，这个矩阵只是我们神经网络中的另一组权重。这种类型的矩阵被称为“**嵌入矩阵**”。嵌入矩阵是一种我们从该类别的零到最大级别数之间开始的整数。我们索引到矩阵中找到特定行，并将其附加到所有连续变量，之后的一切就和以前一样（线性→ReLU→等等）。
- en: '**Question**: What do those 4 numbers represent?[[55:12](https://youtu.be/gbceqO8PpBg?t=55m12s)]
    We will learn more about that when we look at collaborative filtering, but for
    now, they are just parameters that we are learning that happen to end up giving
    us a good loss. We will discover later that these particular parameters often
    are human interpretable and quite interesting but that a side effect.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：这四个数字代表什么？我们将在查看协同过滤时了解更多，但目前，它们只是我们正在学习的参数，最终给我们带来了良好的损失。我们将在后面发现，这些特定的参数通常是人类可解释的，而且相当有趣，但这是一个副作用。'
- en: '**Question**: Do you have good heuristics for the dimensionality of the embedding
    matrix? [[55:57](https://youtu.be/gbceqO8PpBg?t=55m57s)] I sure do! Let’s take
    a look.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于嵌入矩阵的维度有好的启发吗？我有！让我们看一看。'
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here is a list of every categorical variable and its cardinality.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里是每个分类变量及其基数的列表。
- en: Even if there were no missing values in the original data, you should still
    set aside one for unknown just in case.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使原始数据中没有缺失值，你仍然应该留出一个未知值。
- en: The rule of thumb for determining the embedding size is the cardinality size
    divided by 2, but no bigger than 50.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定嵌入大小的经验法则是基数大小除以2，但不超过50。
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then pass the embedding size to the learner:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将嵌入大小传递给学习器：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Question**: Is there a way to initialize embedding matrices besides random?
    [[58:14](https://youtu.be/gbceqO8PpBg?t=58m14s)] We will probably talk about pre-trained
    more later in the course, but the basic idea is if somebody else at Rossmann had
    already trained a neural network to predict cheese sales, you may as well start
    with their embedding matrix of stores to predict liquor sales. This is what happens,
    for example, at Pinterest and Instacart. Instacart uses this technique for routing
    their shoppers, and Pinterest uses it for deciding what to display on a webpage.
    They have embedding matrices of products/stores that get shared in the organization
    so people do not have to train new ones.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：除了随机初始化，是否有初始化嵌入矩阵的方法？我们可能会在课程后面谈论更多关于预训练的内容，但基本思想是，如果Rossmann的其他人已经训练了一个神经网络来预测奶酪销售，你可能会从他们的店铺嵌入矩阵开始，以预测酒类销售。例如，Pinterest和Instacart就是这样做的。Instacart使用这种技术来为他们的购物者规划路线，Pinterest使用它来决定在网页上显示什么。他们有产品/店铺的嵌入矩阵在组织中共享，这样人们就不必训练新的了。'
- en: '**Question**: What is the advantage of using embedding matrices over one-hot-encoding?
    [[59:23](https://youtu.be/gbceqO8PpBg?t=59m23s)] For the day of week example above,
    instead of the 4 numbers, we could have easily passed 7 numbers (e.g. [0, 1, 0,
    0, 0, 0, 0] for Sunday). That also is a list of floats and that would totally
    work — and that is how, generally speaking, categorical variables have been used
    in statistics for many years (called “dummy variables”). The problem is, the concept
    of Sunday could only ever be associated with a single floating-point number. So
    it gets this kind of linear behavior — it says Sunday is more or less of a single
    thing. With embeddings, Sunday is a concept in four dimensional space. What we
    tend to find happen is that these embedding vectors tend to get these rich semantic
    concepts. For example, if it turns out that weekends have a different behavior,
    you tend to see that Saturday and Sunday will have some particular number higher.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：使用嵌入矩阵相比独热编码有什么优势？对于上面的星期几示例，我们可以轻松地传递7个数字（例如，星期日为[0, 1, 0, 0, 0, 0,
    0]）。那也是一组浮点数，完全可行——这通常是统计学中多年来使用分类变量的方式（称为“虚拟变量”）。问题是，星期日这个概念只能与一个单一的浮点数相关联。因此它具有这种线性行为——它说星期日更多或更少是一个单一的事物。通过嵌入，星期日是一个四维空间中的概念。我们通常发现的情况是，这些嵌入向量往往具有丰富的语义概念。例如，如果周末有不同的行为，你会发现周六和周日的某个特定数字更高。'
- en: By having higher dimensionality vector rather than just a single number, it
    gives the deep learning network a chance to learn these rich representations.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过使用高维向量而不仅仅是一个单一数字，深度学习网络有机会学习这些丰富的表示。
- en: The idea of an embedding is what is called a “distributed representation” —
    the most fundamental concept of neural networks. This is the idea that a concept
    in neural network has a high dimensional representation which can be hard to interpret.
    These numbers in this vector does not even have to have just one meaning. It could
    mean one thing if this is low and that one is high, and something else if that
    one is high and that one is low because it is going through this rich nonlinear
    function. It is this rich representation that allows it to learn such interesting
    relationships.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的概念被称为“分布式表示”——神经网络的最基本概念。这是神经网络中的一个概念，它具有一个高维表示，很难解释。这个向量中的数字甚至不必只有一个含义。如果这个数字低，那个数字高，它可能表示一件事，如果那个数字高，那个数字低，它可能表示另一件事，因为它经过了这个丰富的非线性函数。正是这种丰富的表示使得它能够学习这样有趣的关系。
- en: '**Question**: Are embeddings suitable for certain types of variables? [[01:02:45](https://youtu.be/gbceqO8PpBg?t=1h2m45s)]
    Embedding is suitable for any categorical variables. The only thing it cannot
    work well for would be something with too high cardinality. If you had 600,000
    rows and a variable had 600,000 levels, that is just not a useful categorical
    variable. But in general, the third winner in this competition really decided
    that everything that was not too high cardinality, they put them all as categorical.
    The good rule of thumb is if you can make a categorical variable, you may as well
    because that way it can learn this rich distributed representation; where else
    if you leave it as continuous, the most it can do is to try and find a single
    functional form that fits it well.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：嵌入适用于某些类型的变量吗？[[01:02:45](https://youtu.be/gbceqO8PpBg?t=1h2m45s)] 嵌入适用于任何分类变量。唯一不能很好工作的是具有太高基数的变量。如果您有60万行数据，一个变量有60万个水平，那就不是一个有用的分类变量。但总的来说，在这个比赛中的第三名真的决定将所有不太高基数的变量都作为分类变量。一个很好的经验法则是，如果您可以将一个变量变成分类变量，最好这样做，因为这样它可以学习到丰富的分布式表示；否则，如果您将其保留为连续变量，它最多只能尝试找到一个适合它的单一函数形式。'
- en: Matrix algebra behind the scene [[01:04:47](https://youtu.be/gbceqO8PpBg?t=1h4m47s)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幕后的矩阵代数[[01:04:47](https://youtu.be/gbceqO8PpBg?t=1h4m47s)]
- en: Looking up an embedding with an index is identical to doing a matrix product
    between a one-hot encoded vector and the embedding matrix. But doing so is terribly
    inefficient, so modern libraries implement this as taking an integer and doing
    a look up into an array.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过索引查找嵌入与对一个独热编码向量和嵌入矩阵进行矩阵乘积是相同的。但这样做效率非常低，所以现代库将其实现为取一个整数并查找数组中的值。
- en: '**Question**: Could you touch on using dates and times as categorical and how
    that affects seasonality? [[01:06:59](https://youtu.be/gbceqO8PpBg?t=1h6m59s)]
    There is a Fast.ai function called `add_datepart` which takes a data frame and
    a column name. It optionally removes the column from the data frame and replaces
    it with lots of column representing all of the useful information about that date
    such as day of week, day of month, month of year, etc (basically everything Pandas
    gives us).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您能谈谈如何将日期和时间作为分类变量以及这如何影响季节性吗？[[01:06:59](https://youtu.be/gbceqO8PpBg?t=1h6m59s)]
    有一个Fast.ai函数叫做`add_datepart`，它接受一个数据框和一个列名。它可以选择从数据框中删除该列，并用许多列代替，表示有关该日期的所有有用信息，如星期几、月份、年份等（基本上是Pandas给我们的所有信息）。'
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So for example, day of week now becomes eight rows by four columns embedding
    matrix. Conceptually this allows our model to create some interesting time series
    models. If there is something that has a seven day period cycle that goes up on
    Mondays and down on Wednesdays but only for daily and only in Berlin, it can totally
    do that — it has all the information it needs. This is a fantastic way to deal
    with time series. You just need to make sure that the cycle indicator in your
    time series exists as a column. If you did not have a column called day of week,
    it would be very difficult for the neural network to learn to do mod seven and
    look up in an embedding matrix. It is not impossible but really hard. If you are
    predicting sales of beverages in San Francisco, you probably want a list of when
    the ball game is on at AT&T park because that is going to to impact how many people
    are drinking beer in SoMa. So you need to make sure that the basic indicators
    or periodicity is in your data, and as long as they are there, neural net is going
    to learn to use them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在星期几变成了一个八行四列的嵌入矩阵。从概念上讲，这使我们的模型能够创建一些有趣的时间序列模型。如果有一些东西有一个七天周期循环，星期一上升，星期三下降，但仅限于每天且仅在柏林，它完全可以做到
    - 它拥有所有需要的信息。这是处理时间序列的一种奇妙方式。您只需要确保时间序列中的周期指示器存在为一列。如果没有一个名为星期几的列，神经网络学习进行模七和查找嵌入矩阵将非常困难。这并非不可能，但确实很难。如果您正在预测旧金山饮料的销售情况，您可能想要一个AT&T球场的球赛时间表，因为这将影响到SoMa地区喝啤酒的人数。因此，您需要确保基本指标或周期性存在于您的数据中，只要它们存在，神经网络就会学会使用它们。
- en: Learner [[01:10:13](https://youtu.be/gbceqO8PpBg?t=1h10m13s)]
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习者[[01:10:13](https://youtu.be/gbceqO8PpBg?t=1h10m13s)]
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`emb_szs` : embedding size'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_szs`：嵌入大小'
- en: '`len(df.columns)-len(cat_vars)` : number of continuous variables in the data
    frame'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len(df.columns)-len(cat_vars)`：数据框中连续变量的数量'
- en: '`0.04` : embedding matrix has its own dropout and this is the dropout rate'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0.04`：嵌入矩阵有自己的丢失率，这是丢失率'
- en: '`1` : how many outputs we want to create (output of the last linear layer)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`：我们想要创建多少输出（最后一个线性层的输出）'
- en: '`[1000, 500]` : number of activations in the first linear layer, and the second
    linear layer'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[1000, 500]`：第一个线性层和第二个线性层中的激活数量'
- en: '`[0.001, 0.01]` : dropout in the first linear layer, and the second linear
    layer'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[0.001, 0.01]`：第一个线性层和第二个线性层中的丢失率'
- en: '`y_range` : we will not worry about that for now'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_range`：现在我们不会担心这个'
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`metrics` : this is a custom metric which specifies a function to be called
    at the end of every epoch and prints out a result'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`：这是一个自定义指标，指定在每个时期结束时调用的函数并打印结果'
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By using all of the training data, we achieved a RMSPE around 0.09711\. There
    is a big difference between public leader board and private leader board, but
    we are certainly in the top end of this competition.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用所有的训练数据，我们实现了大约0.09711的RMSPE。公共排行榜和私人排行榜之间存在很大差异，但我们肯定处于比赛的前端。
- en: So this is a technique for dealing with time series and structured data. Interestingly,
    compared to the group that used this technique ([Entity Embeddings of Categorical
    Variables](https://arxiv.org/abs/1604.06737)), the second place winner did way
    more feature engineering. The winners of this competition were actually subject
    matter experts in logistics sales forecasting so they had their own code to create
    lots and lots of features. Folks at Pinterest who build a very similar model for
    recommendations also said that when they switched from gradient boosting machines
    to deep learning, they did way less feature engineering and it was much simpler
    model which requires less maintenance. So this is one of the big benefits of using
    this approach to deep learning — you can get state of the art results but with
    a lot less work.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是一种处理时间序列和结构化数据的技术。有趣的是，与使用这种技术的组相比（[分类变量的实体嵌入](https://arxiv.org/abs/1604.06737)），第二名的获胜者进行了更多的特征工程。这个比赛的获胜者实际上是物流销售预测方面的专家，因此他们有自己的代码来创建大量的特征。Pinterest的人们为推荐构建了一个非常相似的模型，他们也说当他们从梯度提升机转向深度学习时，他们做了更少的特征工程，这是一个更简单的模型，需要更少的维护。因此，使用这种深度学习方法的一个重要好处是，您可以获得最先进的结果，但工作量要少得多。
- en: '**Question**: Are we using any time series in any of these? [[01:15:01](https://youtu.be/gbceqO8PpBg?t=1h15m1s)]
    Indirectly, yes. As we just saw, we have a day of week, month of year, etc in
    our columns and most of them are being treated as categories, so we are building
    a distributed representation of January, Sunday, and so on. We are not using any
    classic time series techniques, all we are doing is true fully connected layers
    in a neural net. The embedding matrix is able to deal with things like day of
    week periodicity in a much richer way than than any standard time series techniques.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们在这些中使用任何时间序列吗？间接地，是的。正如我们刚才看到的，我们的列中有一周的天数，一年的月份等，大多数被视为类别，因此我们正在构建一种分布式表示，例如一月，星期日等。我们没有使用任何经典的时间序列技术，我们所做的只是在神经网络中进行真正的全连接层。嵌入矩阵能够以比任何标准时间序列技术更丰富的方式处理一周中的周期性。'
- en: '**Question** regarding the difference between image models and this model [[01:15:59](https://youtu.be/gbceqO8PpBg?t=1h15m59s)]:
    There is a difference in a way we are calling `get_learner`. In imaging we just
    did `Learner.trained` and pass the data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图像模型和这个模型之间的区别的问题：在调用`get_learner`的方式上有所不同。在图像处理中，我们只是做了`Learner.trained`并传递数据：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For these kinds of models, in fact for a lot of the models, the model we build
    depends on the data. In this case, we need to know what embedding matrices we
    have. So in this case, the data objects creates the learner (upside down to what
    we have seen before):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些类型的模型，实际上对于很多模型，我们构建的模型取决于数据。在这种情况下，我们需要知道我们有哪些嵌入矩阵。因此，在这种情况下，数据对象创建了学习者（与我们之前看到的相反）：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Summary of steps** (if you want to use this for your own dataset) [[01:17:56](https://youtu.be/gbceqO8PpBg?t=1h17m56s)]:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤总结**（如果你想为自己的数据集使用这个）：'
- en: '**Step 1**. List categorical variable names, and list continuous variable names,
    and put them in a Pandas data frame'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1**。列出分类变量名称和连续变量名称，并将它们放入Pandas数据框中'
- en: '**Step 2**. Create a list of which row indexes you want in your validation
    set'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2**。创建一个列表，其中包含您想要在验证集中的行索引'
- en: '**Step 3**. Call this exact line of code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**。调用这行代码：'
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 4**. Create a list of how big you want each embedding matrix to be'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4**。创建一个您想要每个嵌入矩阵有多大的列表'
- en: '**Step 5**. Call `get_learner` — you can use these exact parameters to start
    with:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤5**。调用`get_learner` — 您可以使用这些确切的参数开始：'
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Step 6**. Call `m.fit`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6**。调用`m.fit`'
- en: '**Question**: How to use data augmentation for this type of data, and how does
    dropout work? [[01:18:59](https://youtu.be/gbceqO8PpBg?t=1h18m59s)] No idea. Jeremy
    thinks it has to be domain-specific, but he has never seen any paper or anybody
    in industry doing data augmentation with structured data and deep learning. He
    thinks it can be done but has not seen it done. What dropout is doing is exactly
    the same as before.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如何为这种类型的数据使用数据增强，以及辍学是如何工作的？没有头绪。Jeremy认为这必须是特定于领域的，但他从未见过任何论文或任何行业人士在结构化数据和深度学习中使用数据增强。他认为这是可以做到的，但还没有看到有人这样做。辍学所做的事情与以前完全相同。'
- en: '**Question**: What is the downside? Almost no one is using this. Why not? [[01:20:41](https://youtu.be/gbceqO8PpBg?t=1h20m41s)]
    Basically the answer is as we discussed before, no one in academia almost is working
    on this because it is not something that people publish on. As a result, there
    have not been really great examples people could look at and say “oh here is a
    technique that works well so let’s have our company implement it”. But perhaps
    equally importantly, until now with this Fast.ai library, there has not been any
    way to do it conveniently. If you wanted to implement one of these models, you
    had to write all the custom code yourself. There are a lot of big commercial and
    scientific opportunity to use this and solve problems that previously haven’t
    been solved very well.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：有什么缺点？几乎没有人在使用这个。为什么？基本上答案就像我们之前讨论的那样，几乎没有人在学术界从事这方面的工作，因为这不是人们发表论文的内容。因此，人们没有真正出色的例子可以参考，说“哦，这是一个很好的技术，让我们让我们的公司实施它”。但也许同样重要的是，直到现在有了这个Fast.ai库，没有任何方便的方法来做到这一点。如果你想要实现其中一个模型，你必须自己编写所有的自定义代码。有很多商业和科学机会可以利用这一点，并解决以前没有很好解决的问题。'
- en: Natural Language Processing [[01:23:37](https://youtu.be/gbceqO8PpBg?t=1h23m37s)]
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: The most up-and-coming area of deep learning and it is two or three years behind
    computer vision. The state of software and some of the concepts is much less mature
    than it is for computer vision. One of the things you find in NLP is there are
    particular problems you can solve and they have particular names. There is a particular
    kind of problem in NLP called “language modeling” and it has a very specific definition
    — it means build a model where given a few words of a sentence, can you predict
    what the next word is going to be.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中最具潜力的领域是自然语言处理，它比计算机视觉落后了两三年。软件状态和一些概念的成熟程度远不及计算机视觉。在自然语言处理中，你会发现有一些特定的问题可以解决，它们有特定的名称。在自然语言处理中有一种特定的问题叫做“语言建模”，它有一个非常具体的定义——构建一个模型，在给定一个句子的几个单词后，你能否预测下一个单词是什么。
- en: Language Modeling [[01:25:48](https://youtu.be/gbceqO8PpBg?t=1h25m48s)]
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb)'
- en: 'Here we have 18 months worth of papers from arXiv (arXiv.org) and this is an
    example:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有来自arXiv（arXiv.org）的18个月的论文，这是一个例子：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`<cat>` — category of the paper. CSNI is Computer Science and Networking'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<cat>` — 论文的类别。CSNI是计算机科学和网络'
- en: '`<summ>` — abstract of the paper'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<summ>` — 论文的摘要'
- en: 'Here are what the output of a trained language model looks like. We did simple
    little tests in which you pass some priming text and see what the model thinks
    should come next:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练语言模型的输出看起来像什么。我们进行了一些简单的测试，输入一些初始文本，看模型认为接下来应该是什么：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It learned by reading arXiv papers that somebody who is writing about computer
    networking would talk like this. Remember, it started out not knowing English
    at all. It started out with an embedding matrix for every word in English that
    was random. By reading lots of arXiv papers, it learned what kind of words followed
    others.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过阅读arXiv论文学到，写关于计算机网络的人会这样说话。记住，它最初完全不懂英语。它最初为英语中的每个单词都有一个随机的嵌入矩阵。通过阅读大量的arXiv论文，它学会了哪些单词跟随其他单词。
- en: 'Here we tried specifying a category to be computer vision:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们尝试指定一个类别为计算机视觉：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It not only learned how to write English pretty well, but also after you say
    something like “convolutional neural network” you should then use parenthesis
    to specify an acronym “(CNN)”.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅学会了写英语，而且在你说完“卷积神经网络”之后，你应该使用括号来指定一个缩写“(CNN)”。
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: A language model can be incredibly deep and subtle, so we are going to try and
    build that — not because we care about this at all, but because we are trying
    to create a pre-trained model which is used to do some other tasks. For example,
    given an IMDB movie review, we will figure out whether they are positive or negative.
    It is a lot like cats vs. dogs — a classification problem. So we would really
    like to use a pre-trained network which at least knows how to read English. So
    we will train a model that predicts a next word of a sentence (i.e. language model),
    and just like in computer vision, stick some new layers on the end and ask it
    to predict whether something is positive or negative.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型可以非常深奥，因此我们将尝试构建它——不是因为我们真的在乎这个，而是因为我们试图创建一个用于执行其他任务的预训练模型。例如，给定一个IMDB电影评论，我们将确定它们是积极的还是消极的。这很像猫和狗——一个分类问题。因此，我们真的希望使用一个至少知道如何阅读英语的预训练网络。因此，我们将训练一个模型来预测句子的下一个单词（即语言模型），就像在计算机视觉中一样，在最后添加一些新层，并要求它预测某物是积极的还是消极的。
- en: IMDB [[1:31:11](https://youtu.be/gbceqO8PpBg?t=1h31m11s)]
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IMDB
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb)'
- en: What we are going to do is to train a language model, making that the pre-trained
    model for a classification model. In other words, we are trying to leverage exactly
    what we learned in our computer vision which is how to do fine-tuning to create
    powerful classification models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的是训练一个语言模型，使其成为分类模型的预训练模型。换句话说，我们试图利用我们在计算机视觉中学到的微调技巧来创建强大的分类模型。
- en: '**Question**: why would doing directly what you want to do not work? [[01:31:34](https://youtu.be/gbceqO8PpBg?t=1h31m34s)]
    It just turns out it doesn’t empirically. There are several reasons. First of
    all, we know fine-tuning a pre-trained network is really powerful. So if we can
    get it to learn some related tasks first, then we can use all that information
    to try and help it on the second task. The other is IMDB movie reviews are up
    to a thousands words long. So after reading a thousands words knowing nothing
    about how English is structured or concept of a word or punctuation, all you get
    is a 1 or a 0 (positive or negative). Trying to learn the entire structure of
    English and then how it expresses positive and negative sentiments from a single
    number is just too much to expect.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么直接做你想做的事情不起作用？事实证明它并不起作用。有几个原因。首先，我们知道微调预训练网络非常强大。因此，如果我们可以让它先学习一些相关任务，然后我们可以利用所有这些信息来尝试帮助它完成第二个任务。另一个原因是IMDB电影评论长达数千字。因此，在阅读数千字后，不了解英语结构或单词或标点符号的情况下，你只会得到一个1或0（积极或消极）。试图学习整个英语结构，然后从一个数字中了解它如何表达积极和消极情绪，这是期望太高了。'
- en: '**Question**: Is this similar to Char-RNN by Karpathy? [[01:33:09](https://youtu.be/gbceqO8PpBg?t=1h33m9s)]
    This is somewhat similar to Char-RNN which predicts the next letter given a number
    of previous letters. Language model generally work at a word level (but they do
    not have to), and we will focus on word level modeling in this course.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：这与Karpathy的Char-RNN相似吗？这与Char-RNN有些相似，它可以根据前几个字母预测下一个字母。语言模型通常在单词级别上工作（但不一定），我们将在本课程中专注于单词级别的建模。'
- en: '**Question**: To what extent are these generated words/sentences actual copies
    of what it found in the training set? [[01:33:44](https://youtu.be/gbceqO8PpBg?t=1h33m44s)]
    Words are definitely words it has seen before because it is not a character level
    so it can only give us the word it has seen before. Sentences, there are rigorous
    ways of doing it but the easiest would be by looking at examples like above, you
    get a sense of it. Most importantly, when we train the language model, we will
    have a validation set so that we are trying to predict the next word of something
    that has never seen before. There are tricks to using language models to generate
    text like [beam search](http://forums.fast.ai/t/tricks-for-using-language-models-to-generate-text/8127/2).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：这些生成的单词/句子在多大程度上是实际复制了在训练集中找到的内容？单词肯定是之前见过的单词，因为它不是字符级别的，所以它只能给我们之前见过的单词。句子，有严格的方法来做，但最简单的方法是看上面的例子，你会对此有所了解。最重要的是，当我们训练语言模型时，我们将有一个验证集，以便我们尝试预测以前从未见过的东西的下一个单词。有一些技巧可以使用语言模型来生成文本，比如beam
    search。'
- en: 'Use cases of text classification:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的用例：
- en: For hedge fund, identify things in articles or Twitter that caused massive market
    drops in the past.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于对冲基金，识别过去导致市场大幅下跌的文章或Twitter中的内容。
- en: Identify customer service queries which tend to be associated with people who
    cancel their contracts in the next month
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别客户服务查询，这些查询往往与下个月取消合同的人相关联
- en: Organize documents into whether they are part of legal discovery or not.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档组织成是否属于法律发现的一部分。
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`torchtext` — PyTorch’s NLP library'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchtext` — PyTorch的NLP库'
- en: Data [[01:37:05](https://youtu.be/gbceqO8PpBg?t=1h37m5s)]
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: IMDB [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: IMDB大型电影评论数据集
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We do not have separate test and validation in this case. Just like in vision,
    the training directory has bunch of files in it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们没有单独的测试和验证。就像在视觉中一样，训练目录中有一堆文件：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we will check how many words are in the dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将检查数据集中有多少单词：
- en: '[PRE34]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Before we can do anything with text, we have to turn it into a list of tokens.
    Token is basically like a word. Eventually we will turn them into a list of numbers,
    but the first step is to turn it into a list of words — this is called “tokenization”
    in NLP. A good tokenizer will do a good job of recognizing pieces in your sentence.
    Each separated piece of punctuation will be separated, and each part of multi-part
    word will be separated as appropriate. Spacy does a lot of NLP stuff, and it has
    the best tokenizer Jeremy knows. So Fast.ai library is designed to work well with
    the Spacey tokenizer as with torchtext.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以对文本进行任何操作之前，我们必须将其转换为标记列表。标记基本上就像一个单词。最终我们将把它们转换成一系列数字，但第一步是将其转换成一系列单词——这在NLP中称为“标记化”。一个好的标记器将很好地识别句子中的片段。每个分隔的标点符号将被分开，每个多部分单词的部分将被适当地分开。Spacy做了很多NLP工作，Jeremy知道它有最好的标记器。因此，Fast.ai库被设计为与Spacey标记器以及torchtext一起很好地工作。
- en: Creating a field [[01:41:01](https://youtu.be/gbceqO8PpBg?t=1h41m1s)]
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个字段
- en: A field is a definition of how to pre-process some text.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 字段是如何预处理一些文本的定义。
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`lower=True` — lowercase the text'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower=True` — 将文本转换为小写'
- en: '`tokenize=spacy_tok` — tokenize with `spacy_tok`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize=spacy_tok` — 使用`spacy_tok`进行标记化'
- en: 'Now we create the usual Fast.ai model data object:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建通常的Fast.ai模型数据对象：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`PATH` : as per usual where the data is, where to save models, etc'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PATH`：通常是数据所在的位置，保存模型等'
- en: '`TEXT` : torchtext’s Field definition'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TEXT`：torchtext的Field定义'
- en: '`**FILES` : list of all of the files we have: training, validation, and test
    (to keep things simple, we do not have a separate validation and test set, so
    both points to validation folder)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**FILES`：我们拥有的所有文件的列表：训练、验证和测试（为了保持简单，我们没有单独的验证和测试集，所以两者都指向验证文件夹）'
- en: '`bs` : batch size'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs`：批量大小'
- en: '`bptt` : Back Prop Through Time. It means how long a sentence we will stick
    on the GPU at once'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bptt`：通过时间反向传播。这意味着我们一次将多长的句子放在GPU上'
- en: '`min_freq=10` : In a moment, we are going to be replacing words with integers
    (a unique index for every word). If there are any words that occur less than 10
    times, just call it unknown.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_freq=10`：一会儿，我们将用整数（每个单词的唯一索引）替换单词。如果有任何出现次数少于10次的单词，就称之为未知。'
- en: 'After building our `ModelData` object, it automatically fills the `TEXT` object
    with a very important attribute: `TEXT.vocab`. This is a *vocabulary*, which stores
    which unique words (or *tokens*) have been seen in the text, and how each word
    will be mapped to a unique integer id.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了我们的`ModelData`对象之后，它会自动填充`TEXT`对象的一个非常重要的属性：`TEXT.vocab`。这是一个*词汇表*，它存储了文本中看到的哪些唯一单词（或*标记*），以及每个单词将被映射到一个唯一的整数ID。
- en: '[PRE37]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`itos` is sorted by frequency except for the first two special ones. Using
    `vocab`, torchtext will turn words into integer IDs for us :'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`itos`按频率排序，除了前两个特殊的。使用`vocab`，torchtext将为我们将单词转换为整数ID：'
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Question**: Is it common to do any stemming or lemma-tizing? [[01:45:47](https://youtu.be/gbceqO8PpBg?t=1h45m47s)]
    Not really, no. Generally tokenization is what we want. To keep it as general
    as possible, we want to know what is coming next so whether it is future tense
    or past tense or plural or singular, we don’t really know which things are going
    to be interesting and which are not, so it seems that it is generally best to
    leave it alone as much as possible.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：通常会进行任何词干处理或词形还原吗？不是很常见。一般来说，我们只需要进行分词。为了尽可能通用，我们想知道接下来会发生什么，所以无论是将来时还是过去时，还是复数还是单数，我们并不真的知道哪些事情会有趣，哪些不会，所以似乎最好尽可能保持不变。'
- en: '**Question**: When dealing with natural language, isn’t context important?
    Why are we tokenizing and looking at individual word? [[01:46:38](https://youtu.be/gbceqO8PpBg?t=1h46m38s)]
    No, we are not looking at individual word — they are still in order. Just because
    we replaced I with a number 12, they are still in that order. There is a different
    way of dealing with natural language called “bag of words” and they do throw away
    the order and context. In the Machine Learning course, we will be learning about
    working with bag of words representations but my belief is that they are no longer
    useful or in the verge of becoming no longer useful. We are starting to learn
    how to use deep learning to use context properly.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：处理自然语言时，上下文不重要吗？为什么我们要对单个词进行标记化和查看？[[01:46:38](https://youtu.be/gbceqO8PpBg?t=1h46m38s)]
    不，我们不是在查看单个词 - 它们仍然是有序的。只是因为我们用数字12替换了I，它们仍然是按照那个顺序的。处理自然语言的另一种方法叫做“词袋”，它们会丢弃顺序和上下文。在机器学习课程中，我们将学习如何使用词袋表示，但我认为它们已经不再有用或即将不再有用。我们开始学习如何正确使用深度学习来使用上下文。'
- en: Batch size and BPTT [[01:47:40](https://youtu.be/gbceqO8PpBg?t=1h47m40s)]
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批次大小和BPTT [[01:47:40](https://youtu.be/gbceqO8PpBg?t=1h47m40s)]
- en: What happens in a language model is even though we have lots of movie reviews,
    they all get concatenated together into one big block of text. So we predict the
    next word in this huge long thing which is all of the IMDB movie reviews concatenated
    together.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型中发生的情况是，尽管我们有很多电影评论，它们都被连接在一起成为一个大文本块。因此，我们预测这个巨大的长文本中的下一个单词，其中包含所有IMDB电影评论的连接。
- en: We split up the concatenated reviews into batches. In this case, we will split
    it to 64 sections
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将连接的评论分成批次。在这种情况下，我们将其分成64个部分。
- en: We then move each section underneath the previous one, and transpose it.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将每个部分移动到前一个部分的下方，并对其进行转置。
- en: We end up with a matrix which is 1 million by 64.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最终得到一个大小为100万乘以64的矩阵。
- en: We then grab a little chunk at time and those chunk lengths are **approximately**
    equal to BPTT. Here, we grab a little 70 long section and that is the first thing
    we chuck into our GPU (i.e. the batch).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们每次抓取一小块，这些块的长度**大致**等于BPTT。在这里，我们抓取一个大约70个字符长的部分，这是我们放入GPU（即批次）的第一件事。
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We grab our first training batch by wrapping data loader with `iter` then calling
    `next`.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过将数据加载器包装在`iter`中，然后调用`next`来获取我们的第一个训练批次。
- en: We got back a 75 by 64 tensor (approximately 70 rows but not exactly)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们得到了一个75乘以64的张量（大约70行，但不完全）
- en: A neat trick torchtext does is to randomly change the `bptt` number every time
    so each epoch it is getting slightly different bits of text — similar to shuffling
    images in computer vision. We cannot randomly shuffle the words because they need
    to be in the right order, so instead, we randomly move their breakpoints a little
    bit.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torchtext做的一个巧妙的技巧是每次随机更改`bptt`数字，因此每个时期它都会获取略有不同的文本片段 - 类似于在计算机视觉中对图像进行洗牌。我们不能随机洗牌单词，因为它们需要按正确的顺序排列，所以我们随机移动它们的断点一点点。
- en: The target value is also 75 by 64 but for minor technical reasons it is flattened
    out into a single vector.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标值也是75乘以64，但出于一些技术原因，它被展平为一个单一向量。
- en: '**Question**: Why not split by a sentence? [[01:53:40](https://youtu.be/gbceqO8PpBg?t=1h53m40s)]
    Not really. Remember, we are using columns. So each of our column is of length
    about 1 million, so although it is true that those columns are not always exactly
    finishing on a full stop, they are so darn long we do not care. Each column contains
    multiple sentences.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么不按句子分割？[[01:53:40](https://youtu.be/gbceqO8PpBg?t=1h53m40s)] 不完全是。请记住，我们使用的是列。因此，我们的每一列长度约为100万，因此尽管这些列并不总是完全以句号结束，但它们非常长，我们不在乎。每列包含多个句子。'
- en: Pertaining to this question, Jeremy found what is in this language model matrix
    a little mind-bending for quite a while, so do not worry if it takes a while and
    you have to ask a thousands questions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个问题，Jeremy发现了在这个语言模型矩阵中的内容有一段时间让人有点费解，所以如果需要一段时间并且需要问一千个问题，不要担心。
- en: Create a model [[01:55:46](https://youtu.be/gbceqO8PpBg?t=1h55m46s)]
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个模型 [[01:55:46](https://youtu.be/gbceqO8PpBg?t=1h55m46s)]
- en: Now that we have a model data object that can fee d us batches, we can create
    a model. First, we are going to create an embedding matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个可以提供批次的模型数据对象，我们可以创建一个模型。首先，我们将创建一个嵌入矩阵。
- en: 'Here are the: # batches; # unique tokens in the vocab; length of the dataset;
    # of words'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是：#批次；词汇表中的唯一标记数；数据集的长度；单词数
- en: '[PRE40]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This is our embedding matrix looks like:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的嵌入矩阵的样子：
- en: It is a high cardinality categorical variable and furthermore, it is the only
    variable — this is typical in NLP
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个高基数分类变量，而且，这是唯一的变量 - 这在自然语言处理中很典型
- en: The embedding size is 200 which is much bigger than our previous embedding vectors.
    Not surprising because a word has a lot more nuance to it than the concept of
    Sunday. **Generally, an embedding size for a word will be somewhere between 50
    and 600.**
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入大小为200，比我们以前的嵌入向量要大得多。这并不奇怪，因为一个词比“星期天”的概念要复杂得多。**一般来说，一个词的嵌入大小会在50到600之间。**
- en: '[PRE41]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Researchers have found that large amounts of *momentum* (which we’ll learn
    about later) don’t work well with these kinds of *RNN* models, so we create a
    version of the *Adam* optimizer with less momentum than its default of `0.9`.
    Any time you are doing NLP, you should probably include this line:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现大量的*动量*（我们稍后会了解）与这些*循环神经网络*模型不太兼容，因此我们创建了一个*Adam*优化器的版本，其动量小于默认值`0.9`。每当你在做自然语言处理时，你应该包括这一行：
- en: '[PRE42]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Fast.ai uses a variant of the state of the art [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182)
    developed by Stephen Merity. A key feature of this model is that it provides excellent
    regularization through [Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout).
    There is no simple way known (yet!) to find the best values of the dropout parameters
    below — you just have to experiment…
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Fast.ai使用了由Stephen Merity开发的最先进的[AWD LSTM语言模型](https://arxiv.org/abs/1708.02182)的变体。这个模型的一个关键特征是通过[Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout)提供了出色的正则化。目前还没有简单的方法来找到下面的dropout参数的最佳值
    - 您只需要进行实验...
- en: However, the other parameters (`alpha`, `beta`, and `clip`) shouldn't generally
    need tuning.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他参数（`alpha`，`beta`和`clip`）通常不需要调整。
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the last lecture, we will learn what the architecture is and what all these
    dropouts are. For now, just know it is the same as per usual, if you try to build
    an NLP model and you are under-fitting, then decrease all these dropouts, if overfitting,
    then increase all these dropouts in roughly this ratio. Since this is such a recent
    paper so there is not a lot of guidance but these ratios worked well — it is what
    Stephen has been using as well.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一堂课中，我们将学习架构是什么以及所有这些dropout是什么。现在，只需知道它与通常情况下相同，如果您尝试构建一个NLP模型并且欠拟合，那么减少所有这些dropout，如果过拟合，那么以大致这个比例增加所有这些dropout。由于这是一篇最近的论文，所以没有太多的指导，但这些比例效果很好
    - 这也是Stephen一直在使用的。
- en: There is another kind of way we can avoid overfitting that we will talk about
    in the last class. For now, `learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)`
    works reliably so all of your NLP models probably want this particular line.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有另一种我们可以避免过拟合的方法，我们将在最后一堂课上讨论。目前，`learner.reg_fn = partial(seq2seq_reg, alpha=2,
    beta=1)`可以可靠地工作，因此您所有的NLP模型可能都需要这一行。
- en: '`learner.clip=0.3` : when you look at your gradients and you multiply them
    by the learning rate to decide how much to update your weights by, this will not
    allow them be more than 0.3\. This is a cool little trick to prevent us from taking
    too big of a step.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learner.clip=0.3`：当您查看梯度并将其乘以学习率以决定更新权重的量时，这将不允许它们超过0.3。这是一个很酷的小技巧，可以防止我们迈出太大的一步。'
- en: Details do not matter too much right now, so you can use them as they are.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细节现在并不太重要，所以您可以按原样使用它们。
- en: '**Question**: There are word embedding out there such as Word2vec or GloVe.
    How are they different from this? And why not initialize the weights with those
    initially? [[02:02:29](https://youtu.be/gbceqO8PpBg?t=2h2m29s)] People have pre-trained
    these embedding matrices before to do various other tasks. They are not called
    pre-trained models; they are just a pre-trained embedding matrix and you can download
    them. There is no reason we could not download them. I found that building a whole
    pre-trained model in this way did not seem to benefit much if at all from using
    pre-trained word vectors; where else using a whole pre-trained language model
    made a much bigger difference. Maybe we can combine both to make them a little
    better still.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：有一些词嵌入，如Word2vec或GloVe。它们与这个有什么不同？为什么不最初使用它们来初始化权重？人们以前已经对这些嵌入矩阵进行了预训练，以执行各种其他任务。它们不被称为预训练模型；它们只是一个预训练的嵌入矩阵，您可以下载它们。我们完全可以下载它们。我发现以这种方式构建一个完整的预训练模型似乎并没有从使用预训练词向量中受益，而使用一个完整的预训练语言模型则产生了更大的差异。也许我们可以将两者结合起来使它们变得更好。'
- en: '**Question:** What is the architecture of the model? [[02:03:55](https://youtu.be/gbceqO8PpBg?t=2h3m55s)]
    We will be learning about the model architecture in the last lesson but for now,
    it is a recurrent neural network using something called LSTM (Long Short Term
    Memory).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：模型的架构是什么？我们将在最后一课中学习有关模型架构的知识，但现在，它是使用一种称为LSTM（长短期记忆）的递归神经网络。'
- en: Fitting [[02:04:24](https://youtu.be/gbceqO8PpBg?t=2h4m24s)]
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the sentiment analysis section, we'll just need half of the language model
    - the *encoder*, so we save that part.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在情感分析部分，我们只需要语言模型的一半 - *编码器*，所以我们保存了那部分。
- en: '[PRE45]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Language modeling accuracy is generally measured using the metric *perplexity*,
    which is simply `exp()` of the loss function we used.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模的准确性通常使用指标*困惑度*来衡量，这只是我们使用的损失函数的`exp()`。
- en: '[PRE46]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Testing [[02:04:53](https://youtu.be/gbceqO8PpBg?t=2h4m53s)]
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: We can play around with our language model a bit to check it seems to be working
    OK. First, let’s create a short bit of text to ‘prime’ a set of predictions. We’ll
    use our torchtext field to numericalize it so we can feed it to our language model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以稍微玩弄一下我们的语言模型，以确保它运行正常。首先，让我们创建一小段文本来“引导”一组预测。我们将使用我们的torchtext字段对其进行数值化，以便将其馈送给我们的语言模型。
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We haven’t yet added methods to make it easy to test a language model, so we’ll
    need to manually go through the steps.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有添加使测试语言模型变得容易的方法，因此我们需要手动执行这些步骤。
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s see what the top 10 predictions were for the next word after our short
    text:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在我们短文本之后的下一个单词的前10个预测是什么：
- en: '[PRE49]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: …and let’s see if our model can generate a bit more text all by itself!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '...让我们看看我们的模型是否可以自己生成更多文本！'
- en: '[PRE50]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Sentiment [[02:05:09](https://youtu.be/gbceqO8PpBg?t=2h5m9s)]
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感
- en: So we had pre-trained a language model and now we want to fine-tune it to do
    sentiment classification.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们之前已经预训练了一个语言模型，现在我们想要微调它以进行情感分类。
- en: To use a pre-trained model, we will need to the saved vocab from the language
    model, since we need to ensure the same words map to the same IDs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用预训练模型，我们将需要语言模型的保存的词汇表，因为我们需要确保相同的单词映射到相同的ID。
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`sequential=False` tells torchtext that a text field should be tokenized (in
    this case, we just want to store the ''positive'' or ''negative'' single label).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`sequential=False`告诉torchtext文本字段应该被标记化（在这种情况下，我们只想存储“正面”或“负面”单个标签）。'
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This time, we need to not treat the whole thing as one big piece of text but
    every review is separate because each one has a different sentiment attached to
    it.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们需要将每个评论视为单独的而不是作为一个大段的文本，因为每个评论都有不同的情感附着。
- en: '`splits` is a torchtext method that creates train, test, and validation sets.
    The IMDB dataset is built into torchtext, so we can take advantage of that. Take
    a look at `lang_model-arxiv.ipynb` to see how to define your own fastai/torchtext
    datasets.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`splits`是torchtext的一个方法，用于创建训练、测试和验证集。IMDB数据集内置在torchtext中，因此我们可以利用它。查看`lang_model-arxiv.ipynb`，了解如何定义自己的fastai/torchtext数据集。'
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: fastai can create a `ModelData` object directly from torchtext `splits`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: fastai可以直接从torchtext的`splits`创建一个`ModelData`对象。
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now you can go ahead and call `get_model` that gets us our learner. Then we
    can load into it the pre-trained language model (`load_encoder`).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以继续调用`get_model`来获取我们的学习者。然后我们可以加载预训练的语言模型（`load_encoder`）。
- en: '[PRE55]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Because we’re fine-tuning a pretrained model, we’ll use differential learning
    rates, and also increase the max gradient for clipping, to allow the SGDR to work
    better.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在微调一个预训练模型，我们将使用不同的学习率，并增加用于剪切的最大梯度，以使SGDR更好地工作。
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We make sure all except the last layer is frozen. Then we train a bit, unfreeze
    it, train it a bit. The nice thing is once you have got a pre-trained language
    model, it actually trains really fast.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保除了最后一层外，所有层都被冻结。然后我们进行一些训练，解冻它，再进行一些训练。好处是一旦你有了一个预训练的语言模型，它实际上训练速度非常快。
- en: '[PRE57]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'A recent paper from Bradbury et al, [Learned in translation: contextualized
    word vectors](https://einstein.ai/research/learned-in-translation-contextualized-word-vectors),
    has a handy summary of the latest academic research in solving this IMDB sentiment
    analysis problem. Many of the latest algorithms shown are tuned for this specific
    problem.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Bradbury等人最近发表的一篇论文，[学习中的翻译：上下文化的词向量](https://einstein.ai/research/learned-in-translation-contextualized-word-vectors)，对解决IMDB情感分析问题的最新学术研究进行了方便的总结。许多最新的算法都是针对这个特定问题进行调整的。
- en: As you see, we just got a new state of the art result in sentiment analysis,
    decreasing the error from 5.9% to 5.5%! You should be able to get similarly world-class
    results on other NLP classification problems using the same basic steps.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们在情感分析方面取得了最新的技术成果，将错误率从5.9%降低到5.5%！你应该能够使用相同的基本步骤在其他NLP分类问题上获得同样世界级的结果。
- en: There are many opportunities to further improve this, although we won’t be able
    to get to them until part 2 of this course.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机会进一步改进这一点，尽管我们在本课程的第二部分之前不会能够做到这一点。
- en: For example we could start training language models that look at lots of medical
    journals and then we could make a downloadable medical language model that then
    anybody could use to fine-tune on a prostate cancer subset of medical literature.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，我们可以开始训练查看许多医学期刊的语言模型，然后我们可以制作一个可下载的医学语言模型，然后任何人都可以用它来在医学文献的前列腺癌子集上进行微调。
- en: We could also combine this with pre-trained word vectors
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以将其与预训练的词向量结合使用
- en: We could have pre-trained a Wikipedia corpus language model and then fine-tuned
    it into an IMDB language model, and then fine-tune that into an IMDB sentiment
    analysis model and we would have gotten something better than this.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们本可以预先训练一个维基百科语料库语言模型，然后将其微调为一个IMDB语言模型，然后再将其微调为一个IMDB情感分析模型，我们会得到比这个更好的东西。
- en: There is a really fantastic researcher called Sebastian Ruder who is the only
    NLP researcher who has been really writing a lot about pre-training, fine-tuning,
    and transfer learning in NLP. Jeremy was asking him why this is not happening
    more, and his view was it is because there is not a software to make it easy.
    Hopefully Fast.ai will change that.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为Sebastian Ruder的非常出色的研究人员，他是唯一一个真正大量撰写关于NLP中预训练、微调和迁移学习的研究人员。Jeremy问他为什么这种情况没有更多发生，他的观点是因为没有软件使其变得容易。希望Fast.ai会改变这一点。
- en: Collaborative Filtering Introduction [[02:11:38](https://youtu.be/gbceqO8PpBg?t=2h11m38s)]
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协同过滤介绍 [[02:11:38](https://youtu.be/gbceqO8PpBg?t=2h11m38s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)'
- en: Data available from [http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可从[http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip)获取
- en: '[PRE58]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The dataset looks like this:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集看起来像这样：
- en: It contains ratings by users. Our goal will be for some user-movie combination
    we have not seen before, we have to predict a rating.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含用户的评分。我们的目标是对于我们以前没有见过的某个用户-电影组合，我们必须预测一个评分。
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: To make it more interesting, we will also actually download a list of movies
    so that we can interpret what is actually in these embedding matrices.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更有趣，我们还将实际下载一份电影列表，以便我们可以解释这些嵌入矩阵中实际包含的内容。
- en: '[PRE60]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This is what we are creating — this kind of cross tab of users by movies.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们正在创建的——用户和电影的这种交叉表。
- en: Feel free to look ahead and you will find that most of the steps are familiar
    to you already.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 随意提前查看，你会发现大部分步骤对你来说已经很熟悉了。
