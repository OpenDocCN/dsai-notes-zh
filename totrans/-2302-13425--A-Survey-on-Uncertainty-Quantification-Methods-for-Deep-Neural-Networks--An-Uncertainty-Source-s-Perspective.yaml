- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:41:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:41:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.13425] A Survey on Uncertainty Quantification Methods for Deep Neural
    Networks: An Uncertainty Source’s Perspective'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.13425] 深度神经网络的不确定性量化方法综述：从不确定性来源的视角'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.13425](https://ar5iv.labs.arxiv.org/html/2302.13425)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.13425](https://ar5iv.labs.arxiv.org/html/2302.13425)
- en: 'A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络的不确定性量化方法综述：从不确定性来源的视角
- en: Wenchong He [whe2@ufl.edu](mailto:whe2@ufl.edu)  and  Zhe Jiang [zhe.jiang@ufl.edu](mailto:zhe.jiang@ufl.edu)
    The University of FloridaGainesvilleFLUSA32611
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 温崇赫 [whe2@ufl.edu](mailto:whe2@ufl.edu) 和 赵哲 [zhe.jiang@ufl.edu](mailto:zhe.jiang@ufl.edu)
    佛罗里达大学盖恩斯维尔 FL 美国 32611
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Deep neural networks (DNNs) have achieved tremendous success in making accurate
    predictions for computer vision, natural language processing, as well as science
    and engineering domains. However, it is also well-recognized that DNNs sometimes
    make unexpected, incorrect, but overconfident predictions. This can cause serious
    consequences in high-stake applications, such as autonomous driving, medical diagnosis,
    and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence
    of DNN predictions beyond prediction accuracy. In recent years, many UQ methods
    have been developed for DNNs. It is of great practical value to systematically
    categorize these UQ methods and compare their advantages and disadvantages. However,
    existing surveys mostly focus on categorizing UQ methodologies from a neural network
    architecture’s perspective or a Bayesian perspective and ignore the source of
    uncertainty that each methodology can incorporate, making it difficult to select
    an appropriate UQ method in practice. To fill the gap, this paper presents a systematic
    taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data
    uncertainty versus model uncertainty). We summarize the advantages and disadvantages
    of methods in each category. We show how our taxonomy of UQ methodologies can
    potentially help guide the choice of UQ method in different machine learning problems
    (e.g., active learning, robustness, and reinforcement learning). We also identify
    current research gaps and propose several future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）在计算机视觉、自然语言处理以及科学和工程领域取得了巨大成功。然而，也众所周知，DNNs 有时会做出意外的、不正确但过于自信的预测。这在高风险应用中，如自动驾驶、医疗诊断和灾难响应，可能会导致严重后果。不确定性量化（UQ）的目标是估计
    DNN 预测的信心，超越预测准确性。近年来，许多 UQ 方法已被开发用于 DNNs。系统地对这些 UQ 方法进行分类并比较其优缺点具有重要的实际价值。然而，现有的综述大多侧重于从神经网络架构或贝叶斯视角对
    UQ 方法进行分类，忽略了每种方法可以包含的不确定性来源，使得在实际应用中选择适当的 UQ 方法变得困难。为填补这一空白，本文基于不确定性来源（数据不确定性与模型不确定性）提出了
    DNNs UQ 方法的系统分类。我们总结了每个类别方法的优缺点。我们展示了我们的方法学分类如何在不同的机器学习问题（例如主动学习、鲁棒性和强化学习）中指导选择
    UQ 方法。我们还识别了当前的研究空白并提出了若干未来研究方向。
- en: 'Deep neural networks, uncertainty quantification, data uncertainty, model uncertainty,
    trustworthy AI.^†^†ccs: Computing methodologies Uncertainty quantification^†^†ccs:
    Computing methodologies Machine learning approaches^†^†ccs: Computing methodologies Knowledge
    representation and reasoning^†^†ccs: Applied computing Physical sciences and engineering^†^†ccs:
    Information systems Data mining'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络、不确定性量化、数据不确定性、模型不确定性、可信赖的 AI。^†^†ccs: 计算方法学 不确定性量化^†^†ccs: 计算方法学 机器学习方法^†^†ccs:
    计算方法学 知识表示与推理^†^†ccs: 应用计算 物理科学与工程^†^†ccs: 信息系统 数据挖掘'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Deep neural network (DNN) models have achieved remarkable success in computer
    vision, natural language processing, as well as science and engineering domains
    (Pandey et al., [2022](#bib.bib104); LeCun et al., [2015](#bib.bib74); Deng et al.,
    [2009](#bib.bib29)). Existing popular DNN models can be largely regarded as a
    deterministic function, which maps the input features to a target prediction through
    hierarchical representation learning (Bengio et al., [2013](#bib.bib10)). While
    these DNN models often achieve strong performance in overall prediction accuracy,
    it is also well-recognized that they sometimes make unexpected, incorrect, but
    overconfident predictions, especially in a complex real-world environment (Reichstein
    et al., [2019](#bib.bib113)). This can cause serious consequences in high-stake
    applications, such as autonomous driving (Choi et al., [2019](#bib.bib22)), medical
    diagnosis (Begoli et al., [2019](#bib.bib8)), and disaster response (Alam et al.,
    [2017](#bib.bib4)). In this regard, a DNN model should be aware of what it does
    not know in order to avoid overconfident predictions. For example, in the medical
    domain, when DNN-based automatic diagnosis systems encounter uncertain cases,
    a patient should be referred to a medical expert for more in-depth analysis to
    avoid fatal mistakes. In an autonomous vehicle, if a DNN model knows in what scenarios
    (e.g., bad weather) it tends to make mistakes in estimating road conditions, it
    can warn the driver to take over and avoid potential crashes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）模型在计算机视觉、自然语言处理以及科学和工程领域取得了显著成功（Pandey et al.，[2022](#bib.bib104)；LeCun
    et al.，[2015](#bib.bib74)；Deng et al.，[2009](#bib.bib29)）。现有的流行 DNN 模型在很大程度上可以被视为一种确定性函数，通过层次表示学习（Bengio
    et al.，[2013](#bib.bib10)）将输入特征映射到目标预测。虽然这些 DNN 模型通常在整体预测准确性上表现强劲，但也普遍认识到它们有时会做出意外的、错误的，但过于自信的预测，尤其是在复杂的现实环境中（Reichstein
    et al.，[2019](#bib.bib113)）。这在高风险应用中可能造成严重后果，例如自动驾驶（Choi et al.，[2019](#bib.bib22)）、医疗诊断（Begoli
    et al.，[2019](#bib.bib8)）和灾难响应（Alam et al.，[2017](#bib.bib4)）。在这方面，DNN 模型应当意识到其未知的内容，以避免过度自信的预测。例如，在医疗领域，当基于
    DNN 的自动诊断系统遇到不确定的情况时，应该将患者转诊给医疗专家进行更深入的分析，以避免致命的错误。在自动驾驶车辆中，如果 DNN 模型知道在什么场景（例如恶劣天气）下它倾向于在估计路况时出错，它可以警告司机接管控制，避免潜在的碰撞。
- en: ”Knowing what a DNN model does not know” comes down to placing appropriate uncertainty
    scores in its predictions, also called uncertainty quantification (UQ). Uncertainty
    in DNNs may come from different types of sources, including data uncertainty and
    model uncertainty (Yarin, [2016](#bib.bib150)). Data uncertainty (also aleatory
    uncertainty) is an inherent property of the data, which originates from the randomness
    and stochasticity of the data (e.g., sensor noises) or conflicting evidence between
    the training labels (e.g., class overlap). Data uncertainty is often considered
    irreducible because we cannot reduce it by adding more training samples. Model
    uncertainty (also epistemic uncertainty), on the other hand, comes from the lack
    of evidence or knowledge during model training or prediction for a new test sample,
    e.g., limited training samples, sub-optimal DNN model architecture or parameter
    learning, and out-of-distribution (OOD) samples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “了解 DNN 模型不知道什么”归结于在其预测中放置适当的不确定性分数，也称为不确定性量化（UQ）。DNN 中的不确定性可能来自不同类型的来源，包括数据不确定性和模型不确定性（Yarin，[2016](#bib.bib150)）。数据不确定性（也称为偶然不确定性）是数据的固有特性，源于数据的随机性和随机波动（例如，传感器噪声）或训练标签之间的冲突证据（例如，类别重叠）。数据不确定性通常被认为是不可减少的，因为我们无法通过添加更多的训练样本来减少它。另一方面，模型不确定性（也称为认识不确定性）来自于模型训练或对新测试样本预测时缺乏证据或知识，例如，有限的训练样本、次优的
    DNN 模型架构或参数学习，以及分布外（OOD）样本。
- en: 'In recent years, researchers have developed a growing number of UQ methods
    for DNN models. However, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective"), existing surveys of UQ methods for DNNs are
    either from a neural network architecture’s perspective or a Bayesian perspective,
    ignoring the type of uncertainty sources. Specifically, (Gawlikowski et al., [2021](#bib.bib41))
    categorizes existing UQ methods based on their types of DNN model architectures,
    including Bayesian neural networks, ensemble models, and single model architecture,
    but it does not discuss the connection between the DNN model architectures and
    the type of uncertainty sources they incorporate. Other surveys only focus on
    the Bayesian perspective. For example, (Mena et al., [2021](#bib.bib89)) gives
    a comprehensive review of Bayesian neural networks for UQ but ignores existing
    methods from the frequentist perspective (e.g., prediction interval, ensemble
    methods). (Abdar et al., [2021](#bib.bib2)) covers the ensemble and other frequentist
    methods, but it does not compare their advantages and disadvantages. To the best
    of our knowledge, existing surveys on UQ methods often ignore the types of uncertainty
    sources they incorporate. This perspective is important for selecting the appropriate
    UQ methods for different applications, where one type of uncertainty may dominate
    others.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，研究人员为深度神经网络（DNN）模型开发了越来越多的不确定性量化（UQ）方法。然而，如图[1](#S1.F1 "Figure 1 ‣ 1\.
    Introduction ‣ A Survey on Uncertainty Quantification Methods for Deep Neural
    Networks: An Uncertainty Source’s Perspective")所示，现有的DNN UQ方法的调查要么是从神经网络架构的角度，要么是从贝叶斯视角出发，忽略了不确定性来源的类型。具体而言，（Gawlikowski等，[2021](#bib.bib41)）根据DNN模型架构的类型对现有UQ方法进行了分类，包括贝叶斯神经网络、集成模型和单一模型架构，但没有讨论DNN模型架构与其所包含的不确定性来源类型之间的关系。其他调查仅关注贝叶斯视角。例如，（Mena等，[2021](#bib.bib89)）全面回顾了贝叶斯神经网络用于UQ的情况，但忽视了来自频率主义视角的现有方法（例如，预测区间、集成方法）。（Abdar等，[2021](#bib.bib2)）涵盖了集成和其他频率主义方法，但没有比较它们的优缺点。据我们所知，现有的UQ方法调查常常忽略了它们所包含的不确定性来源类型。这一视角对于选择适合不同应用的UQ方法至关重要，因为某一类型的不确定性可能会主导其他类型。'
- en: '![Refer to caption](img/6279ad970bcfb8109505028ce56dbabc.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6279ad970bcfb8109505028ce56dbabc.png)'
- en: Figure 1\. Existing survey on UQ for DNN models
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 现有的DNN模型UQ调查
- en: To fill the gap, we provide the first survey of UQ methods for DNNs from an
    uncertainty source’s perspective. Specifically, we create a systematic taxonomy
    for DNN uncertainty quantification methodologies based on the type of uncertainty
    sources that they incorporate. We summarize the characteristics of different methods
    in their technical approaches and compare the advantages and disadvantages when
    addressing different types of uncertainty sources. We also connect the taxonomy
    to several major deep learning topics where UQ methods are critical, including
    OOD detection, active learning, and deep reinforcement learning. Lastly, we identify
    the research gaps and point out several future research directions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，我们提供了首个从不确定性来源视角出发的DNN UQ方法调查。具体而言，我们基于所包含的不确定性来源类型，为DNN不确定性量化方法创建了系统的分类法。我们总结了不同方法在技术方法上的特点，并在处理不同类型的不确定性来源时比较了它们的优缺点。我们还将这一分类法与UQ方法在一些主要深度学习主题中的重要性相连接，包括OOD检测、主动学习和深度强化学习。最后，我们识别了研究空白，并指出了若干未来的研究方向。
- en: 2\. Problem definition
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 问题定义
- en: 'Given the training dataset $\mathcal{D}_{tr}=\{\boldsymbol{x}_{i},{y}_{i}\}_{i=1}^{n}$
    with input features $\mathbf{X}=\{\boldsymbol{x}_{i}\}_{i=1}^{n}$, $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$
    and the target label $\mathbf{Y}=\{{y}_{i}\}_{i=1}^{n}$, the learning problem
    is to find a parameterized neural network model $y=f(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})$
    that fits we on the training data (and potentially generalizes well on the unseen
    test data). The model is learned by minimizing the empirical risk, defined as
    the average loss over the training data: (Murphy, [2012](#bib.bib93)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练数据集$\mathcal{D}_{tr}=\{\boldsymbol{x}_{i},{y}_{i}\}_{i=1}^{n}$，其中输入特征为$\mathbf{X}=\{\boldsymbol{x}_{i}\}_{i=1}^{n}$，$\boldsymbol{x}_{i}\in\mathbb{R}^{d}$，目标标签为$\mathbf{Y}=\{{y}_{i}\}_{i=1}^{n}$，学习问题是找到一个参数化的神经网络模型$y=f(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})$，使其能够在训练数据上拟合（并且在未见的测试数据上潜在地表现良好）。通过最小化经验风险来学习该模型，经验风险定义为训练数据上的平均损失：（Murphy，[2012](#bib.bib93)）：
- en: '| (1) |  | $\operatorname*{arg\,min}_{f}R(\mathcal{D}_{tr},f),\ \text{where}\
    R(\mathcal{D}_{tr},f)=\frac{1}{n}\sum_{i=1}^{n}{l}(y_{i},f(\boldsymbol{x}_{i}))$
    |  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\operatorname*{arg\,min}_{f}R(\mathcal{D}_{tr},f),\ \text{where}\
    R(\mathcal{D}_{tr},f)=\frac{1}{n}\sum_{i=1}^{n}{l}(y_{i},f(\boldsymbol{x}_{i}))$
    |  |'
- en: 'In addition to making an accurate prediction, the goal of UQ is to also obtain
    a parametric uncertainty framework $g$ that outputs an uncertainty estimate $u=g(\boldsymbol{x};\boldsymbol{\phi})\in\mathbb{R^{+}}$
    reflecting the confidence of the model prediction. To this end, we formulate the
    problem as an optimization problem that minimizes a new loss function, which is
    a combination of the empirical risk and the expected calibration error (ECE) (Nixon
    et al., [2019](#bib.bib99)):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了做出准确的预测外，UQ 的目标还包括获得一个参数不确定性框架 $g$，该框架输出一个反映模型预测的不确定性估计 $u=g(\boldsymbol{x};\boldsymbol{\phi})\in\mathbb{R^{+}}$，反映了模型预测的置信度。为此，我们将问题表述为一个优化问题，最小化一个新的损失函数，该函数是经验风险和预期校准误差（ECE）（Nixon等，[2019](#bib.bib99)）的组合。
- en: '| (2) |  | $\small\begin{split}\operatorname*{arg\,min}_{f,g}\hat{R}(\mathcal{D},f,g),\
    \text{where}\ \hat{R}(\mathcal{D},f,g)=R(\mathcal{D}_{tr},f)+\text{ECE}(\mathcal{D}_{tr},f,g)\end{split}$
    |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\small\begin{split}\operatorname*{arg\,min}_{f,g}\hat{R}(\mathcal{D},f,g),\
    \text{where}\ \hat{R}(\mathcal{D},f,g)=R(\mathcal{D}_{tr},f)+\text{ECE}(\mathcal{D}_{tr},f,g)\end{split}$
    |  |'
- en: ECE measures the consistency between the prediction error and the uncertainty
    of the prediction. Specifically, the uncertainty interval is grouped into fixed
    bins, and the average of the difference between the uncertainty and error in each
    bin is compared. ECE encourages higher uncertainty on larger error predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ECE 用于衡量预测误差与预测不确定性之间的一致性。具体而言，将不确定性区间分组到固定的区间中，并比较每个区间中的不确定性与误差之间的差值的平均值。ECE
    鼓励在误差较大的预测中有更高的不确定性。
- en: Given a sample $\boldsymbol{x}$, the goal is to predict both the target and
    the associated uncertainty $(y,u)=(f_{\boldsymbol{\theta}}(\boldsymbol{x}),g_{\boldsymbol{\phi}}(\boldsymbol{x}))$.
    The prediction and uncertainty estimation are formulated as two separate models,
    $f$ and $g$, to distinguish the two concepts. However, some approaches may represent
    both in a single, coherent framework.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定样本 $\boldsymbol{x}$，目标是预测目标和相关的不确定性 $(y,u)=(f_{\boldsymbol{\theta}}(\boldsymbol{x}),g_{\boldsymbol{\phi}}(\boldsymbol{x}))$。预测和不确定性估计被规划为两个独立的模型，$f$
    和 $g$，以区分两个概念。然而，一些方法可能将两者都表示为一个单一的连贯框架。
- en: 3\. Types of uncertainty source
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 不确定性来源的类型
- en: 'In this section, we provide some background of two major types of uncertainty:
    data uncertainty and model uncertainty. Specifically, we discuss the potential
    sources of each type, and their representation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了两种主要类型不确定性的背景：数据不确定性和模型不确定性。具体而言，我们讨论了每种类型的潜在来源及其表征方式。
- en: 3.1\. Data uncertainty
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 数据不确定性
- en: 3.1.1\. Source of data uncertainty
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 数据不确定性的来源
- en: '![Refer to caption](img/66b52927772789a750103c5b6b63707b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/66b52927772789a750103c5b6b63707b.png)'
- en: (a) Classes distribution with clean separable boundary (low uncertainty)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 具有清晰可分边界的类分布（低不确定性）
- en: '![Refer to caption](img/f5d84cb129d5b2b33356244fe9fcbc38.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f5d84cb129d5b2b33356244fe9fcbc38.png)'
- en: (b) Entropy of example a
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 例子 a 的熵
- en: '![Refer to caption](img/180aff08f122ddb72c4c70fb42d807c0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/180aff08f122ddb72c4c70fb42d807c0.png)'
- en: (c) Classes distribution with ambiguous boundary (high uncertainty)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 具有模糊边界的类分布（高不确定性）
- en: '![Refer to caption](img/3d53c3b293c8f720e2ccf47bff6438ce.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3d53c3b293c8f720e2ccf47bff6438ce.png)'
- en: (d) Entropy of example c
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 例子 c 的熵
- en: Figure 2\. Data uncertainty visualization examples (Different colors represent
    samples in different classes)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 数据不确定性可视化示例（不同颜色代表不同类别的样本）
- en: The data uncertainty (aka. aleatoric uncertainty) arises from the data’s inherent
    randomness, noise, or the feature distribution overlap in different classes, which
    is irreducible even given more training data (Yarin, [2016](#bib.bib150)). Randomness
    or noise in data can occur in the data acquisition process. The uncertainty in
    the data acquisition process may be due to instrument errors, inappropriate frequency
    of data acquisition, and data transmission errors (Hariri et al., [2019](#bib.bib48)).
    For example, certain data acquisition devices cannot work properly in a complex
    environment (e.g., poor weather conditions). On the other hand, factors such as
    inappropriate sampling methods, data storage, data representation methods and
    interpolation techniques also affect uncertainty during data processing  (Yarin,
    [2016](#bib.bib150)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的不确定性（即 aleatoric 不确定性）源于数据的固有随机性、噪声或不同类别中的特征分布重叠，即使提供更多训练数据也无法减少（Yarin，[2016](#bib.bib150)）。数据中的随机性或噪声可能发生在数据采集过程中。数据采集过程中的不确定性可能由于仪器误差、不适当的数据采集频率以及数据传输错误（Hariri
    等人，[2019](#bib.bib48)）而产生。例如，某些数据采集设备在复杂环境中（如恶劣的天气条件）可能无法正常工作。另一方面，不适当的采样方法、数据存储、数据表示方法和插值技术等因素也会影响数据处理过程中的不确定性（Yarin，[2016](#bib.bib150)）。
- en: For example, for spatiotemporal data, collected from various space and airborne
    platforms (e.g., CubeSat, UAVs), the data uncertainty may result not only from
    the sensor errors associated with the data acquisition devices but also from the
    fact that data is acquired in a digital format (which is discrete in nature) (Cheng
    et al., [2014](#bib.bib21)) even though the underlying phenomenon is continuous.
    The uncertainty of the representation of an object’s movement is also affected
    by the frequency with which location samples are taken, i.e., the sample rate
    (Pfoser and Jensen, [1999](#bib.bib107)). Uncertainty in the data can also accumulate
    from multiple sources and may be propagated into the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于从各种空间和空中平台（如 CubeSat、无人机）收集的时空数据，数据的不确定性不仅可能来源于与数据采集设备相关的传感器误差，还可能源于数据以数字格式（本质上是离散的）进行采集（Cheng
    等人，[2014](#bib.bib21)），尽管其基础现象是连续的。对象运动的表示的不确定性还受到位置样本采集频率的影响，即样本率（Pfoser 和 Jensen，[1999](#bib.bib107)）。数据中的不确定性还可以从多个来源积累，并可能传播到模型中。
- en: 3.1.2\. Data uncertainty representation
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 数据不确定性表示
- en: 'Consider a training dataset $\mathcal{D}_{tr}$ drawn from the distribution
    ${p_{tr}(\boldsymbol{x},y)}$ over the $d$ dimensional input features $\boldsymbol{x}\in\mathbb{R}^{d}$
    and target class labels $y\in\{\omega_{1},...,\omega_{k}\}$ for classification
    problem with $k$ classes or $y\in\mathbb{R}^{k}$ for regression model. In the
    context of a discriminative classification task, the uncertainty of the class
    variable $y$ given a specific input instance $\boldsymbol{x}$ is defined as the
    entropy of the true condition class distribution $\mathcal{H}[p_{tr}(y|\boldsymbol{x})]$
    as Eq. [3](#S3.E3 "In 3.1.2\. Data uncertainty representation ‣ 3.1\. Data uncertainty
    ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") shows. The conditional
    entropy describes the randomness of the class distribution due to the overlap
    of feature values among samples in different classes.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑一个训练数据集 $\mathcal{D}_{tr}$，从 $d$ 维输入特征 $\boldsymbol{x}\in\mathbb{R}^{d}$
    和目标类别标签 $y\in\{\omega_{1},...,\omega_{k}\}$（用于 $k$ 类分类问题）或 $y\in\mathbb{R}^{k}$（用于回归模型）的分布
    ${p_{tr}(\boldsymbol{x},y)}$ 中抽取。在区分分类任务的背景下，给定特定输入实例 $\boldsymbol{x}$ 的类别变量 $y$
    的不确定性定义为真实条件类别分布的熵 $\mathcal{H}[p_{tr}(y|\boldsymbol{x})]$，如 Eq. [3](#S3.E3 "In
    3.1.2\. Data uncertainty representation ‣ 3.1\. Data uncertainty ‣ 3\. Types of
    uncertainty source ‣ A Survey on Uncertainty Quantification Methods for Deep Neural
    Networks: An Uncertainty Source’s Perspective") 所示。条件熵描述了由于不同类别样本间特征值重叠导致的类别分布的随机性。'
- en: '| (3) |  | $\mathcal{H}[p_{tr}(y&#124;\boldsymbol{x})]=-\sum_{i=1}^{k}p_{tr}(y=\omega_{i}&#124;\boldsymbol{x})\log(p_{tr}(y=\omega_{i}&#124;\boldsymbol{x}))$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathcal{H}[p_{tr}(y\mid\boldsymbol{x})]=-\sum_{i=1}^{k}p_{tr}(y=\omega_{i}\mid\boldsymbol{x})\log(p_{tr}(y=\omega_{i}\mid\boldsymbol{x}))$
    |  |'
- en: 'For classification problems, data uncertainty arises from the natural complexity
    of the data and the structure of the class boundary on the feature space. Take
    a toy distribution as an example in Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source
    of data uncertainty ‣ 3.1\. Data uncertainty ‣ 3\. Types of uncertainty source
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") which consists of two normally distributed
    clusters (in two colors). Each cluster represents a separate class. A dataset
    that has more clear class boundary has a lower level of data uncertainty as Figure [2](#S3.F2
    "Figure 2 ‣ 3.1.1\. Source of data uncertainty ‣ 3.1\. Data uncertainty ‣ 3\.
    Types of uncertainty source ‣ A Survey on Uncertainty Quantification Methods for
    Deep Neural Networks: An Uncertainty Source’s Perspective") (a) shows. The entropy
    is low for most samples except for those near the class boundary. Conversely,
    if the feature space has larger overlapping between classes, then there is high
    data uncertainty as Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source of data uncertainty
    ‣ 3.1\. Data uncertainty ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective")
    (c) shows. The entropy of the boundary samples is higher than the examples in
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source of data uncertainty ‣ 3.1\. Data
    uncertainty ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") (a).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，数据的不确定性来源于数据的自然复杂性以及特征空间中类边界的结构。以图 [2](#S3.F2 "图 2 ‣ 3.1.1\. 数据不确定性的来源
    ‣ 3.1\. 数据不确定性 ‣ 3\. 不确定性来源的类型 ‣ 深度神经网络不确定性量化方法的调查：一种不确定性来源的视角") 为例，该图由两个正态分布的簇（用两种颜色表示）组成。每个簇代表一个独立的类别。一个类边界更清晰的数据集具有较低的数据不确定性，如图 [2](#S3.F2
    "图 2 ‣ 3.1.1\. 数据不确定性的来源 ‣ 3.1\. 数据不确定性 ‣ 3\. 不确定性来源的类型 ‣ 深度神经网络不确定性量化方法的调查：一种不确定性来源的视角")（a）所示。除了接近类边界的样本外，大多数样本的熵较低。相反，如果特征空间中类之间的重叠较大，则数据不确定性较高，如图 [2](#S3.F2
    "图 2 ‣ 3.1.1\. 数据不确定性的来源 ‣ 3.1\. 数据不确定性 ‣ 3\. 不确定性来源的类型 ‣ 深度神经网络不确定性量化方法的调查：一种不确定性来源的视角")（c）所示。边界样本的熵高于图 [2](#S3.F2
    "图 2 ‣ 3.1.1\. 数据不确定性的来源 ‣ 3.1\. 数据不确定性 ‣ 3\. 不确定性来源的类型 ‣ 深度神经网络不确定性量化方法的调查：一种不确定性来源的视角")（a）中的示例。
- en: 'Unlike classification problems, for the regression problem data uncertainty
    arises from the inherent noise (variability) in the data generation or collection
    process:$y=f(\boldsymbol{x})+\epsilon(\boldsymbol{x})$, where $\epsilon(\boldsymbol{x})$
    is the observation or measurement noise in the dataset. There are two classes
    of noise: homeostatic noise and heteroscedastic noise (Kendall and Gal, [2017](#bib.bib67)).
    Homeostatic noise assumes a constant observation noise over all the inputs $\boldsymbol{x}$.
    Heteroscedastic noise, on the other hand, models the observation noise as a function
    of input $\epsilon(\boldsymbol{x})\sim p(\epsilon|\boldsymbol{x})$ (e.g., heteroscedastic
    Gaussian noise). The heteroscedastic noise model is useful in the case where the
    noise level vary for different samples.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类问题不同，对于回归问题，数据的不确定性来源于数据生成或收集过程中的固有噪声（变异性）：`$y=f(\boldsymbol{x})+\epsilon(\boldsymbol{x})$`，其中`$\epsilon(\boldsymbol{x})$`是数据集中的观测或测量噪声。噪声分为两类：稳态噪声和异方差噪声（Kendall
    和 Gal，[2017](#bib.bib67)）。稳态噪声假设所有输入`$\boldsymbol{x}$`的观测噪声是恒定的。另一方面，异方差噪声将观测噪声建模为输入的函数`$\epsilon(\boldsymbol{x})\sim
    p(\epsilon|\boldsymbol{x})$`（例如，异方差高斯噪声）。异方差噪声模型在噪声水平因样本不同而变化的情况下非常有用。
- en: 3.2\. Model uncertainty
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 模型不确定性
- en: 3.2.1\. Source of model uncertainty
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 模型不确定性的来源
- en: 'The model uncertainty (aka aleatoric uncertainty) represents the uncertainty
    in the models’ predictions related to the imperfect model training process (e.g.,
    due to a lack of knowledge on model choices based on the current input data).
    It is reducible given more training data. Model uncertainty can be divided into
    three types: uncertainty in model architecture; uncertainty in the model parameters;
    uncertainty due to a mismatch between training and test dataset distribution.
    For the first type, model architecture uncertainty arises due to the lack of understanding
    of which type of model architecture is most suitable for the data. For example,
    for deep learning models, there exists uncertainty regarding how many neural network
    layers, and how many neurons in each layer are optimum for the given dataset.
    A too complex model can cause an overfitting issue (a model performs well on the
    training dataset but cannot generalize well on a test dataset). Uncertainty in
    model parameters is due to the unknown optimal parameters for the selected model
    architecture, which may arise from an improper training strategy, lack of training
    instances, or local optimal issue for non-convex problems. There is no guarantee
    that the converged weight values correspond to the global minimum of the loss
    function. The last type of model uncertainty is caused by data distribution drift,
    which means the test sample distribution is different from the training sample
    distribution. The issue is not uncommon for real-world deployment of deep learning
    models, also known as out-of-distribution(OOD) data (Schwaiger et al., [2020](#bib.bib121)),
    as real-world test cases can be very complex.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不确定性（又称为随机不确定性）代表了与模型训练过程不完善相关的模型预测的不确定性（例如，由于对基于当前输入数据的模型选择缺乏知识）。给定更多的训练数据，这种不确定性是可以减少的。模型不确定性可以分为三种类型：模型架构的不确定性；模型参数的不确定性；由于训练数据集和测试数据集分布不匹配导致的不确定性。对于第一种类型，模型架构不确定性源于对哪种模型架构最适合数据的理解不足。例如，对于深度学习模型，存在关于神经网络层数和每层神经元数量对给定数据集的最佳配置的不确定性。一个过于复杂的模型可能会导致过拟合问题（模型在训练数据集上表现良好，但在测试数据集上无法很好地推广）。模型参数的不确定性是由于对所选模型架构的最佳参数未知，这可能源于不适当的训练策略、训练实例不足，或非凸问题的局部最优问题。没有保证收敛的权重值对应于损失函数的全局最小值。最后一种模型不确定性是由数据分布漂移引起的，这意味着测试样本分布与训练样本分布不同。这个问题在深度学习模型的实际部署中并不罕见，也称为分布外（OOD）数据（Schwaiger
    et al., [2020](#bib.bib121)），因为现实世界的测试案例可能非常复杂。
- en: 3.2.2\. Model uncertainty representation
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 模型不确定性表示
- en: 'Compared with data uncertainty, model uncertainty is more difficult to estimate
    since the source of model uncertainty can arise from three different aspects:
    model parameters, model architectures, and dataset distribution shift. In general,
    different methods are adopted to represent the model uncertainty from each type.
    First, uncertainty from model parameters can be estimated with a Bayesian neural
    network (BNN) (Jospin et al., [2022](#bib.bib63)). BNN assumes a prior over the
    model parameters and aims to infer the posterior distribution of model parameters
    to reflect the parameters uncertainty. Second, uncertainty arising from the model
    architectures are estimated with ensembles of neural network (deep ensembles).
    The intuition is to construct an ensemble of neural network architectures and
    each model is trained separately. The predictions of the ensemble on an input
    form a distribution on the target variable. Thus the variance of the target variable
    predictions can be an estimation of the prediction uncertainty. The third type
    is the uncertainty from dataset distribution shift, which is caused by the mismatch
    between the training dataset distribution and that of the test samples. The further
    away a new test sample is from the training samples, the greater model uncertainty
    there is.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据不确定性相比，模型不确定性更难以估计，因为模型不确定性的来源可以从三个不同方面产生：模型参数、模型架构和数据集分布的变化。通常，对于每种类型的模型不确定性，采用不同的方法进行表示。首先，模型参数的不确定性可以通过贝叶斯神经网络（BNN）来估计（Jospin
    et al., [2022](#bib.bib63)）。BNN假设模型参数具有先验分布，并旨在推断模型参数的后验分布，以反映参数的不确定性。其次，来自模型架构的不确定性通过神经网络的集成（深度集成）来估计。直觉是构建一个神经网络架构的集成，每个模型单独训练。集成对输入的预测形成目标变量的分布。因此，目标变量预测的方差可以作为预测不确定性的估计。第三种类型是数据集分布变化带来的不确定性，这由训练数据集分布和测试样本的分布不匹配引起。新测试样本与训练样本的距离越远，模型的不确定性越大。
- en: 'In summary, we highlight two primary types of uncertainty and discuss their
    potential sources and representations. As Fig. [3](#S3.F3 "Figure 3 ‣ 3.2.2\.
    Model uncertainty representation ‣ 3.2\. Model uncertainty ‣ 3\. Types of uncertainty
    source ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective") shows, data uncertainty arises from its
    inherent property of the input data features and labels, and model uncertainty
    stems from the misspecification of model architectures, parameters, and the dataset
    distribution shit. Depending on the nature of the application, the predominant
    type of uncertainty may vary, and specific methods may be necessary to address
    them. These uncertainties underscore the importance of robust analysis and interpretation
    of data and model outputs in various fields.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们强调了两种主要的不确定性类型，并讨论了它们的潜在来源和表示方式。如图[3](#S3.F3 "Figure 3 ‣ 3.2.2\. Model
    uncertainty representation ‣ 3.2\. Model uncertainty ‣ 3\. Types of uncertainty
    source ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective")所示，数据不确定性源自输入数据特征和标签的固有属性，而模型不确定性则源于模型架构、参数的误设以及数据集分布的变化。根据应用的性质，主要的不确定性类型可能会有所不同，可能需要特定的方法来应对这些不确定性。这些不确定性突显了在各个领域中对数据和模型输出进行稳健分析和解释的重要性。'
- en: '![Refer to caption](img/a2d4a1b80494f0d232331d28d9cc04d6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a2d4a1b80494f0d232331d28d9cc04d6.png)'
- en: Figure 3\. Two types of uncertainty source
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 两种不确定性来源
- en: 4\. Application domains
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 应用领域
- en: In this section, we discuss several application domains of uncertainty quantification
    for deep learning models. For each application domain, we discuss the motivation
    for developing uncertainty-aware models and the nature of the data, the source
    of uncertainty, and the challenges associated with uncertainty quantification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了深度学习模型不确定性量化的若干应用领域。对于每个应用领域，我们讨论了开发具备不确定性意识模型的动机、数据的性质、不确定性的来源以及与不确定性量化相关的挑战。
- en: 'Medical diagnosis: Deep neural network models have achieved tremendous success
    in a wide variety of medical applications, including medical imaging, clinical
    diagnosis support, and treatment planning (Oh et al., [2020](#bib.bib101); Van Tulder
    and De Bruijne, [2016](#bib.bib137); Cireşan et al., [2013](#bib.bib23)). However,
    one critical concern for deploying deep learning systems in the medical domain
    is trustworthy of the neural network predictions, since deep learning models tend
    to be over-confident even for a wrong prediction (Loftus et al., [2022](#bib.bib82)).
    The issue is important for medical applications because a wrong decision can be
    life-threatening. In this regard, a trustworthy deep neural network model should
    be able to convey uncertainty in its predictions. Both data uncertainty and model
    uncertainty exist in medical domains. Data uncertainty comes from noisy measurements
    from medical devices, ambiguous class labels (e.g., non-consensus tumor boundary
    annotations on imagery between different radiologists), and registration errors
    between medical imagery taken at different times or from different devices (Gong
    et al., [2022](#bib.bib43)). Model uncertainty also exists because patient subjects
    in the test cases may not be well-represented in the training set. There exist
    several challenges to developing UQ methods in the medical domain. First, there
    are multiple sources of data with diverse noise or uncertainty. For example, in
    MRI imagery, the measurement error is an important contributing factor while in
    clinical notes, semantic uncertainty (ambiguity) is more important. It is important
    to consider the unique uncertainty source for a particular problem. Second, it
    is important to enhance the interpretability of a model’s uncertainty quantification.
    Currently, interpretability of deep learning in medical domains is still an open
    research area. Many works exist for uncertainty-aware deep learning models in
    medical domains. These works can be generally divided into those related to medical
    imaging and those for non-medical imaging applications (Loftus et al., [2022](#bib.bib82)).
    In medical imaging, numerous deep imagery segmentation or classification methods
    have been studied for MRI, ultrasound, and cohere tomography (CT) imagery (Kohl
    et al., [2018](#bib.bib70); Edupuganti et al., [2020](#bib.bib34)). These studies
    often focus on data uncertainty due to the ambiguous object label boundary of
    the MRI images (Natekar et al., [2020](#bib.bib95); Qin et al., [2021](#bib.bib111)),
    and the registration uncertainty (Chen et al., [2021a](#bib.bib19)) due to the
    registration error between different frames or sensors. The second category, non-medical
    imaging applications, is mostly related to clinical diagnosis support and treatment
    planning from Electronic Health Records (EHR). EHR contains temporal health records
    of patients (e.g. medications, lab orders, clinical notes) and global contextual
    features (e.g. gender, age, ethnicity, body index). Recurrent neural networks
    and graph neural networks have been applied to EHR data to predict patient diseases
    and possible treatment (e.g., personalized healthcare) (Zhang et al., [2020](#bib.bib153)).
    The presence of significant variability in patient-specific predictions and optimal
    decisions (Dusenberry et al., [2020](#bib.bib33)) requires a model to capture
    prediction uncertainty in clinical decision systems. This allows the model to
    refer highly uncertain cases to clinicians for further diagnosis. The uncertainty-aware
    automatic clinical systems flow chart is shown in Fig. [4](#S4.F4 "Figure 4 ‣
    4\. Application domains ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '医学诊断：深度神经网络模型在各种医学应用中取得了巨大成功，包括医学影像、临床诊断支持和治疗规划（Oh et al., [2020](#bib.bib101);
    Van Tulder 和 De Bruijne, [2016](#bib.bib137); Cireşan et al., [2013](#bib.bib23)）。然而，在医学领域部署深度学习系统时，一个关键问题是神经网络预测的可信度，因为深度学习模型往往过于自信，即使预测错误（Loftus
    et al., [2022](#bib.bib82)）。这一问题在医学应用中尤为重要，因为错误的决定可能危及生命。在这方面，一个值得信赖的深度神经网络模型应该能够传达其预测的不确定性。医学领域存在数据不确定性和模型不确定性。数据不确定性来自于医学设备的噪声测量、模糊的类别标签（例如，不同放射科医生之间影像中的肿瘤边界标注的不一致），以及不同时间或不同设备拍摄的医学影像之间的配准误差（Gong
    et al., [2022](#bib.bib43)）。模型不确定性也存在，因为测试病例中的患者可能在训练集中没有得到很好的代表。医学领域中开发不确定性量化方法存在若干挑战。首先，数据的多个来源具有不同的噪声或不确定性。例如，在MRI影像中，测量误差是一个重要的因素，而在临床记录中，语义不确定性（模糊性）则更为重要。需要考虑特定问题的独特不确定性来源。其次，提高模型不确定性量化的可解释性也很重要。目前，深度学习在医学领域的可解释性仍然是一个开放的研究领域。许多针对医学领域不确定性感知深度学习模型的研究存在。这些研究大致可以分为与医学影像相关的和与非医学影像应用相关的（Loftus
    et al., [2022](#bib.bib82)）。在医学影像领域，已经研究了大量的深度影像分割或分类方法用于MRI、超声和计算机断层扫描（CT）影像（Kohl
    et al., [2018](#bib.bib70); Edupuganti et al., [2020](#bib.bib34)）。这些研究通常关注于由于MRI图像中物体标签边界模糊导致的数据不确定性（Natekar
    et al., [2020](#bib.bib95); Qin et al., [2021](#bib.bib111)），以及由于不同帧或传感器之间的配准误差导致的配准不确定性（Chen
    et al., [2021a](#bib.bib19)）。第二类，非医学影像应用，主要涉及从电子健康记录（EHR）中提供的临床诊断支持和治疗规划。EHR包含患者的时间健康记录（例如药物、实验室订单、临床记录）和全局背景特征（例如性别、年龄、种族、身体指数）。递归神经网络和图神经网络已被应用于EHR数据，以预测患者疾病和可能的治疗（例如个性化医疗）（Zhang
    et al., [2020](#bib.bib153)）。患者特定预测和最佳决策的显著变异性（Dusenberry et al., [2020](#bib.bib33)）要求模型在临床决策系统中捕捉预测的不确定性。这使得模型能够将高度不确定的病例转交给临床医生进行进一步诊断。图
    [4](#S4.F4 "Figure 4 ‣ 4\. Application domains ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") 显示了不确定性感知自动临床系统的流程图。'
- en: '![Refer to caption](img/0b875003e046e4e9086b5b638c66bf25.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0b875003e046e4e9086b5b638c66bf25.png)'
- en: Figure 4\. Uncertainty-guided clinical diagnosis
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 不确定性引导的临床诊断
- en: 'Geoscience: With the advance of GPS and remote sensing technologies, a rapidly
    growing volume of spatiotemporal data is collected from spaceborne, airborne,
    seaborne and terrestrial platforms (Shekhar et al., [2015](#bib.bib124); NASA Goddard,
    [2019](#bib.bib94); Florida and Wildlife, [2021](#bib.bib37)). For example, small
    satellites (called CubeSats) collect high-resolution Earth imagery that covers
    the entire globe every day. The combination of emerging spatiotemporal big data,
    increased computational power (GPUs), and the recent advances in deep learning
    technologies provide exciting new opportunities for advancing our knowledge about
    the Earth system (Reichstein et al., [2019](#bib.bib113)). For example, deep learning
    has been used to predict river flow and temperature (Jia et al., [2021](#bib.bib56))
    and predict hurricane tracks (Kim et al., [2019](#bib.bib68)). Uncertainty quantification
    for deep learning in geoscience is important because many problems are related
    to high-stake decision-making (e.g., evacuation planning based on hurricane tracking).
    Thus, in hurricane tracking, scientists often provide not only the most likely
    point of landfall but also provide a “cone of uncertainty” across other likely
    points of impact and future trajectories of the storm.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 地球科学：随着 GPS 和遥感技术的进步，来自空间、空中、海上和陆地平台的时空数据量迅速增长（Shekhar et al., [2015](#bib.bib124);
    NASA Goddard, [2019](#bib.bib94); Florida and Wildlife, [2021](#bib.bib37)）。例如，小型卫星（称为
    CubeSats）每天都会收集覆盖全球的高分辨率地球影像。新兴的时空大数据、计算能力的提升（如 GPU）以及深度学习技术的最新进展为我们深入了解地球系统提供了令人兴奋的新机会（Reichstein
    et al., [2019](#bib.bib113)）。例如，深度学习已被用于预测河流流量和温度（Jia et al., [2021](#bib.bib56)），以及预测飓风路径（Kim
    et al., [2019](#bib.bib68)）。由于许多问题涉及高风险决策（如基于飓风追踪的疏散计划），地球科学中深度学习的不确定性量化非常重要。因此，在飓风追踪中，科学家们通常不仅提供最可能的登陆点，还提供一个“不确定性锥”来表示其他可能的影响点和未来的风暴轨迹。
- en: UQ for geoscience problems faces several challenges due to the unique characteristics
    of spatiotemporal data. First, spatiotemporal data have various spatial, temporal,
    and spectral resolutions and diverse sources of noise and errors (e.g., noise
    and atmospheric effects in remote sensing signals (Licata and Mehta, [2022](#bib.bib79)),
    GPS errors). Second, spatial registration error and uncertainty may exist when
    co-registering different layers of geospatial data into the same spatial reference
    system (He et al., [2022](#bib.bib49)). This can cause location uncertainty of
    Earth image pixels or pixel labels. Third, spatiotemporal data are heterogeneous,
    i.e., the data distribution often varies across different regions or time periods
    (Jiang et al., [2019](#bib.bib60)). Thus, a deep learning model trained in one
    region (or time) may not generalize well to another region (or time) due to the
    distribution shift. An uncertainty-aware model can potentially alleviate the heterogeneity
    issues by providing a confidence measure when applying the pre-trained model to
    a new test location. This issue is particularly important when spatial observation
    samples are sparsely distributed in the continuous space, causing uncertainty
    when inferring the observations at other locations in continuous space (Hengl
    et al., [2017](#bib.bib52)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 地球科学问题中的不确定性量化面临若干挑战，这些挑战源于时空数据的独特特性。首先，时空数据具有各种空间、时间和光谱分辨率，并且噪声和误差的来源多种多样（例如，遥感信号中的噪声和大气效应（Licata
    and Mehta, [2022](#bib.bib79)），GPS 误差）。其次，当将不同层次的地理空间数据对齐到同一空间参考系统时，可能会存在空间配准误差和不确定性（He
    et al., [2022](#bib.bib49)）。这可能导致地球影像像素或像素标签的位置信息不确定。第三，时空数据是异质的，即数据分布在不同区域或时间段内往往存在差异（Jiang
    et al., [2019](#bib.bib60)）。因此，在一个区域（或时间）训练的深度学习模型可能无法很好地泛化到另一个区域（或时间），因为数据分布发生了变化。一个关注不确定性的模型可以通过在将预训练模型应用到新的测试位置时提供置信度测量，从而潜在地缓解异质性问题。当空间观察样本在连续空间中稀疏分布时，这一问题尤为重要，因为这会导致在推断其他位置的观察数据时的不确定性（Hengl
    et al., [2017](#bib.bib52)）。
- en: 'Transportation: Deep learning technologies that are applied to transportation
    data from ground sensors and video cameras on road networks provide unique opportunities
    to monitor traffic conditions, analyze traffic patterns and improve decision-making.
    For instance, temporal graph neural networks are used to predict traffic flows
    (Zhao et al., [2019](#bib.bib154)). Deep neural network models are also used to
    extract insights into traffic dynamics and identify potential risks, such as congestion
    or accidents (Mo and Fu, [2022](#bib.bib91)). Autonomous driving is another application,
    which uses lidar sensors and optical cameras to detect road lanes and other vehicles
    or pedestrians. However, transportation data are unique due to the temporal dynamics,
    the sensitivity to external factors, and the existence of noise and uncertainty
    (e.g., omission, sparse sensor coverage, errors or inherent bias). For example,
    highly crowded events can disrupt normal traffic flows on road networks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 交通：应用于来自地面传感器和道路网络视频摄像头的交通数据的深度学习技术提供了独特的机会来监控交通状况、分析交通模式并改进决策。例如，时间图神经网络用于预测交通流量（Zhao
    et al., [2019](#bib.bib154)）。深度神经网络模型还用于提取交通动态的见解并识别潜在风险，如拥堵或事故（Mo and Fu, [2022](#bib.bib91)）。自动驾驶是另一种应用，它使用激光雷达传感器和光学摄像头来检测道路车道和其他车辆或行人。然而，交通数据由于时间动态、对外部因素的敏感性以及噪声和不确定性的存在（例如遗漏、稀疏的传感器覆盖、错误或固有偏差）而具有独特性。例如，高度拥挤的事件可以扰乱道路网络上的正常交通流。
- en: Existing works on trajectory uncertainty consider the data uncertainty due to
    the sparse or insufficient training data (Zhou et al., [2022](#bib.bib155)), and
    erroneous or missing measurements due to signal loss (Markos et al., [2021](#bib.bib87)).
    Other works consider external factors like weather impacts (Pang et al., [2021](#bib.bib105);
    Zhu et al., [2022](#bib.bib156); Lempert et al., [2022](#bib.bib76); Wang et al.,
    [2019a](#bib.bib142)) into uncertainty quantification. Short-term traffic status
    forecasting (such as volume, travel speeds, and occupancy) requires the incorporation
    of uncertainty due to the stochastic environment and model training (Wang et al.,
    [2014](#bib.bib141)). For long-term traffic modeling, existing works focus on
    uncertainty from the exogenous factors of traffic flow (e.g., rainstorms and snowstorms
    that could potentially contribute to the prediction uncertainty) (Li et al., [2022](#bib.bib77)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的轨迹不确定性研究考虑了由于稀疏或不足的训练数据导致的数据不确定性（Zhou et al., [2022](#bib.bib155)），以及由于信号丢失导致的错误或缺失测量（Markos
    et al., [2021](#bib.bib87)）。其他研究将天气影响等外部因素（Pang et al., [2021](#bib.bib105); Zhu
    et al., [2022](#bib.bib156); Lempert et al., [2022](#bib.bib76); Wang et al.,
    [2019a](#bib.bib142)）纳入不确定性量化。短期交通状态预测（如流量、旅行速度和占用率）需要考虑由随机环境和模型训练引起的不确定性（Wang
    et al., [2014](#bib.bib141)）。对于长期交通建模，现有研究集中在交通流量的外生因素（例如可能影响预测不确定性的暴雨和雪暴）（Li
    et al., [2022](#bib.bib77)）。
- en: 'Biochemistry engineering: Traditional biochemistry discoveries are primarily
    based on experiments, which are expensive and time-consuming (Bower and Bolouri,
    [2001](#bib.bib13)). With the rapid advancement of computational science, researchers
    have turned to simulation and data-driven approaches to aid scientific discovery
    (Brunton and Kutz, [2022](#bib.bib15)). Simulation methods create mathematical
    models based on fundamental principles, such as kinetics of microbial growth and
    product synthesis, fluid dynamics in bioreactors, and design of new proteins and
    enzymes (Mowbray et al., [2021](#bib.bib92)). However, these models can be computationally
    intensive and require an in-depth understanding of the biochemical system. In
    contrast, deep learning-based models use neural networks and a vast amount of
    biochemistry data to directly extract knowledge and learn complex relationships.
    However, the common data-driven DNN models are unaware of domain knowledge and
    face challenges in representing the complexities of biochemical systems, such
    as the heterogeneity of microbial populations and genetic-environmental variations.
    This often results in unreliable outputs that require human experts to verify,
    adding additional labor to the process (Hie et al., [2020](#bib.bib53)). Several
    factors contribute to the uncertainty of data-driven biochemistry engineering.
    For example, researchers often have only a partial understanding of the complex
    biological mechanism and the macromolecules released from the cells and it is
    difficult to quantify the heterogeneity in the microbial population, due to genetic
    and environmental variations. For different tasks, these factors may play a different
    role in the UQ for neural network model. In protein engineering, the protein function
    prediction (based on protein sequences, protein structures, and protein-protein
    interactions) is challenging because of the large amount of diversity of protein
    folds and the lack of a complete understanding of the protein structure (Bradford
    et al., [2018](#bib.bib14)). For bioreactor engineering, the prediction of bioreactor
    system performance is affected by the intrinsic interactions between bioreactor
    operating factors (e.g., pH, temperature, and substrate concentration). An iterative
    method has been used to enable the propagation of model uncertainty for multi-step
    ahead predictions (Mowbray et al., [2021](#bib.bib92)).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 生物化学工程：传统的生物化学发现主要基于实验，这些实验既昂贵又耗时（Bower 和 Bolouri，[2001](#bib.bib13)）。随着计算科学的迅速发展，研究人员转向模拟和数据驱动的方法来辅助科学发现（Brunton
    和 Kutz，[2022](#bib.bib15)）。模拟方法基于基本原理创建数学模型，例如微生物生长和产品合成的动力学、生物反应器中的流体动力学，以及新蛋白质和酶的设计（Mowbray
    等，[2021](#bib.bib92)）。然而，这些模型可能计算密集且需要对生物化学系统有深入的理解。相比之下，基于深度学习的模型利用神经网络和大量的生物化学数据直接提取知识并学习复杂的关系。然而，常见的数据驱动的深度神经网络（DNN）模型对领域知识不甚了解，面临着表示生物化学系统复杂性的挑战，例如微生物群体的异质性和遗传环境变化。这通常会导致不可靠的输出，需要人工专家进行验证，增加了额外的劳动（Hie
    等，[2020](#bib.bib53)）。几个因素导致了数据驱动的生物化学工程的不确定性。例如，研究人员通常仅对复杂的生物机制和细胞释放的高分子有部分了解，而且由于遗传和环境变化，微生物群体的异质性难以量化。对于不同的任务，这些因素可能在神经网络模型的UQ（不确定性量化）中发挥不同的作用。在蛋白质工程中，蛋白质功能预测（基于蛋白质序列、蛋白质结构和蛋白质-蛋白质相互作用）具有挑战性，因为蛋白质折叠的多样性很大且对蛋白质结构的理解不完整（Bradford
    等，[2018](#bib.bib14)）。在生物反应器工程中，生物反应器系统性能的预测受到生物反应器操作因素（例如，pH、温度和底物浓度）之间的内在相互作用的影响。迭代方法已被用来使模型不确定性的传播能够进行多步预测（Mowbray
    等，[2021](#bib.bib92)）。
- en: 5\. A Taxonomy of DNN UQ Methodology
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 深度神经网络不确定性量化方法的分类
- en: 'In this section, we provide a new taxonomy of existing DNN UQ methods based
    on the type of uncertainty sources each method can capture. As mentioned before,
    two main sources of uncertainty exist in the DNN predictions and for different
    domain applications, different sources may play a major role in affecting the
    model prediction. Thus it is vital to understand which type of uncertainty source
    each method can address. In general, we categorize existing works into three branches
    based on the uncertainty sources as Fig. [5](#S5.F5 "Figure 5 ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") shows: model uncertainty,
    data uncertainty, and the combination of the two. We briefly introduce each category
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了一种基于每种方法能够捕捉的不确定性来源类型的新分类方法。如前所述，DNN 预测中存在两种主要的不确定性来源，对于不同的领域应用，这些来源可能在影响模型预测中发挥主要作用。因此，了解每种方法可以解决哪种类型的不确定性来源是至关重要的。一般而言，我们将现有工作根据不确定性来源分为三类，如图
    [5](#S5.F5 "图 5 ‣ 5\. DNN UQ 方法学分类 ‣ 深度神经网络不确定性量化方法：从不确定性来源的视角") 所示：模型不确定性、数据不确定性以及两者的结合。我们简要介绍每一类别如下：
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model uncertainty: In this category, the approaches consider the model uncertainty
    resulting from the parameters (BNN), architectures (ensemble methods), or sample
    density (Deep Gaussian process).'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型不确定性：在这一类别中，方法考虑了由参数（BNN）、架构（集成方法）或样本密度（深度高斯过程）引起的模型不确定性。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data uncertainty: The approaches in this category aim to account for the uncertainty
    from the inherent randomness or noise in the data. The general idea is to construct
    a distribution over the prediction. Those approaches are further split into deep
    discriminative models and deep generative models.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据不确定性：这一类别的方法旨在考虑数据中固有的随机性或噪声带来的不确定性。总体思路是对预测构建一个分布。这些方法进一步分为深度判别模型和深度生成模型。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The combination of model and data uncertainty: Many approaches aim to capture
    the total uncertainty in one framework, which is the combination of model and
    data uncertainty. The straightforward approach is to combine the approaches in
    data and model uncertainty and form a coherent framework, but introduces more
    computation and storage demand. Another framework called evidential deep learning
    overcomes the computational challenge with a single neural network model.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型和数据不确定性的结合：许多方法旨在在一个框架中捕捉总体不确定性，即模型和数据不确定性的结合。直接的方法是将数据和模型不确定性的方法结合起来，形成一个连贯的框架，但这会引入更多的计算和存储需求。另一种称为证据深度学习的框架通过单个神经网络模型克服了计算挑战。
- en: In the following, the main intuitions and approaches of the three types are
    presented and their main advantages and disadvantages are discussed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，介绍这三种类型的主要直观和方法，并讨论它们的主要优缺点。
- en: '![Refer to caption](img/7fc528ad4d3041cdbff8e29da293adf0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7fc528ad4d3041cdbff8e29da293adf0.png)'
- en: Figure 5\. A taxonomy for existing literature on UQ for DNN
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 现有 DNN UQ 文献的分类
- en: 5.1\. Model Uncertainty
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 模型不确定性
- en: 'This subsection reviews the existing framework for DNN model uncertainty. We
    categorize the approaches into three subcategories: Bayesian neural network, ensemble
    models, and sample density aware models. We will introduce each subcategory in
    detail.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节回顾了现有的 DNN 模型不确定性框架。我们将方法分为三类：贝叶斯神经网络、集成模型和样本密度感知模型。我们将详细介绍每一子类。
- en: 5.1.1\. Bayesian Neural Networks
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 贝叶斯神经网络
- en: From a frequentist point of view, there exists a single set of parameters $\boldsymbol{\boldsymbol{\theta}}*$
    that fit the DNN model best, where $\boldsymbol{\theta}*=\operatorname*{arg\,min}_{\boldsymbol{\theta}}\mathcal{L}(\mathbf{Y},f(\mathbf{X},\boldsymbol{\theta}))$
    and $\mathcal{L}$ is the loss function. However, the point estimation of DNN parameters
    tends to be overfitting and overconfident (Thulasidasan et al., [2019](#bib.bib132)).
    On the other hand, the Bayesian neural network (BNN) imposes a prior on the neural
    network parameters $p(\boldsymbol{\theta})$ and aims to learn the posterior distribution
    of these parameters $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从频率主义的角度来看，存在一组参数$\boldsymbol{\boldsymbol{\theta}}*$能够最适合DNN模型，其中$\boldsymbol{\theta}*=\operatorname*{arg\,min}_{\boldsymbol{\theta}}\mathcal{L}(\mathbf{Y},f(\mathbf{X},\boldsymbol{\theta}))$，$\mathcal{L}$是损失函数。然而，DNN参数的点估计往往会出现过拟合和过度自信的情况（Thulasidasan
    et al., [2019](#bib.bib132)）。另一方面，贝叶斯神经网络（BNN）对神经网络参数$p(\boldsymbol{\theta})$施加了先验，并旨在学习这些参数的后验分布$p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$。
- en: '| (4) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})=\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{Y}&#124;\mathbf{X})}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})=\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{Y}&#124;\mathbf{X})}$
    |  |'
- en: This term is the posterior distribution of model parameters conditioned on the
    training dataset. This distribution reflects what extent our model can capture
    the training data pattern. Assuming a Gaussian distribution for the model parameters,
    the larger the variance of the distribution is, the larger uncertainty there exists
    in the model. Such uncertainty can be caused by a small amount of training data
    or inappropriate configuration of neural network architecture.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项是条件于训练数据集的模型参数的后验分布。这个分布反映了我们的模型能够捕捉训练数据模式的程度。假设模型参数服从高斯分布，分布的方差越大，模型中的不确定性也越大。这种不确定性可能由少量的训练数据或不合适的神经网络结构配置引起。
- en: 'For inference on the new samples $\boldsymbol{x}*$, we can marginalize out
    the model parameters by:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新样本$\boldsymbol{x}*$的推断，我们可以通过以下方式边际化模型参数：
- en: '| (5) |  | $p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}$
    |  |'
- en: The uncertainty on the test samples will be reflected by the prediction distribution
    variance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 测试样本的不确定性将通过预测分布的方差反映出来。
- en: However, the approach is analytically intractable and does not have a closed-form
    solution and an approximation must be made for prediction in BNN. To estimate
    the posterior of the neural network parameters, various approaches have been proposed
    to approximate the parameter posterior in a simpler form to solve it in a tractable
    way. Some approximation methods define a parameterized class of distributions,
    $\mathcal{Q}$, from which they select an approximation $q_{\phi}(\boldsymbol{\theta})$for
    the posterior. For example, $\mathcal{Q}$ can be the set of all factorized Gaussian
    distributions, and $\phi$ is the parameters of mean and diagonal variance. The
    distribution $q_{\phi}(\boldsymbol{\theta})\in\mathcal{Q}$ is selected according
    to some optimization criteria to approximate the posterior. Two popular approaches
    for optimization are variational inference (Blei et al., [2017](#bib.bib12)) and
    Laplace approximation (Friston et al., [2007](#bib.bib38)). Instead of approximating
    the posterior in an analytical way, another approach aims to solve this problem
    by Monto Carlo sampling called Markov Chain Monto Carlo sampling. In the following,
    we will review existing methodologies in the three subcategories in detail.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法在解析上是不可处理的，没有闭式解，因此必须对贝叶斯神经网络（BNN）的预测进行近似。为了估计神经网络参数的后验分布，已经提出了各种方法来近似参数后验分布的简化形式，以便以可处理的方式求解。一些近似方法定义了一个参数化的分布类$\mathcal{Q}$，从中选择一个近似$
    q_{\phi}(\boldsymbol{\theta})$来表示后验分布。例如，$\mathcal{Q}$可以是所有因子化的高斯分布的集合，$\phi$是均值和对角方差的参数。根据某些优化标准选择分布$q_{\phi}(\boldsymbol{\theta})\in\mathcal{Q}$来近似后验分布。两种常见的优化方法是变分推断（Blei
    et al., [2017](#bib.bib12)）和拉普拉斯近似（Friston et al., [2007](#bib.bib38)）。除了用解析方法近似后验分布外，另一种方法是通过蒙特卡洛采样来解决这个问题，这种方法称为马尔科夫链蒙特卡洛采样。接下来，我们将详细回顾这三种子类别中的现有方法。
- en: 'Variational Inference (VI): In  (Hinton and Van Camp, [1993](#bib.bib54); Graves,
    [2011](#bib.bib45)), the authors propose to find a variational approximation to
    the Bayesian posterior distribution on the weights by maximizing the evidence
    lower bound (ELBO) of the log marginal likelihood. The intractable posterior $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$
    is approximated with a parametric distribution $q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断 (VI)：在(Hinton 和 Van Camp, [1993](#bib.bib54); Graves, [2011](#bib.bib45))中，作者提出通过最大化对数边际似然的证据下界
    (ELBO) 来找到对贝叶斯后验分布的变分近似。难以处理的后验$p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$用一个参数分布$q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$来近似。
- en: '| (6) |  | $\footnotesize\begin{split}\log p(\mathbf{Y}&#124;\mathbf{X})&amp;\geq\mathbb{E}_{\boldsymbol{\theta}\sim
    q_{\phi}(\boldsymbol{\theta})}\log\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{q_{\phi}(\boldsymbol{\theta})}\\
    &amp;=\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})-\mathcal{KL}(q_{\phi}(\boldsymbol{\theta})&#124;&#124;p(\boldsymbol{\theta}))\end{split}$
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\footnotesize\begin{split}\log p(\mathbf{Y}&#124;\mathbf{X})&\geq\mathbb{E}_{\boldsymbol{\theta}\sim
    q_{\phi}(\boldsymbol{\theta})}\log\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{q_{\phi}(\boldsymbol{\theta})}\\
    & =\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})-\mathcal{KL}(q_{\phi}(\boldsymbol{\theta})&#124;&#124;p(\boldsymbol{\theta}))\end{split}$
    |  |'
- en: 'where the first term $\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log
    p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta}))$ is log-likelihood of the training
    data on the neural network model. This term increases as the model accuracy increases.
    The second term is the Kullback-Leibler (KL) divergence between the posterior
    estimation and the prior of the model parameters, which controls the complexity
    of the neural network model. Maximizing Eq. [6](#S5.E6 "In 5.1.1\. Bayesian Neural
    Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective") corresponds to finding a tradeoff between the prediction
    accuracy and complexity of the model. Thus the posterior inference problem becomes
    an optimization problem on the parameter $\boldsymbol{\phi}$.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个项$\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log
    p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta}))$是神经网络模型在训练数据上的对数似然。这个项随着模型准确率的提高而增加。第二项是模型参数后验估计与先验的Kullback-Leibler
    (KL) 散度，它控制神经网络模型的复杂性。最大化方程[6](#S5.E6 "在 5.1.1\. 贝叶斯神经网络 ‣ 5.1\. 模型不确定性 ‣ 5\.
    深度神经网络不确定性量化方法的分类 ‣ 深度神经网络不确定性量化方法：不确定性来源的视角")相当于在预测准确性和模型复杂性之间找到一个权衡。因此，后验推断问题变成了对参数$\boldsymbol{\phi}$的优化问题。
- en: One challenge of variational inference lies in the choice of the parameterized
    class of distribution $q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$. The original
    method relies on the use of Gaussian approximating distribution with a diagonal
    covariance matrix (mean-field variational inference) (Hinton and Van Camp, [1993](#bib.bib54);
    Posch et al., [2019](#bib.bib109)). This method leads to a straightforward lower
    bound for optimization, but the approximation capability is limiting. To capture
    the posterior correlations between parameters, the second category of approaches
    extends the diagonal covariance to general covariance matrix while still leading
    to a tractable algorithm by maximizing the above ELBO (Posch and Pilz, [2020](#bib.bib108)).
    However, the full covariance matrix not only increases the number of trainable
    parameters but also introduce substantial memory and computational cost for DNN.
    To reduce the computation, the third category of approaches aim to simplify the
    covariance matrix structure with certain assumption. Some approaches assume independence
    among layers, resulting in block-diagonal structure covariance matrix (Sun et al.,
    [2017](#bib.bib129); Zhang et al., [2018](#bib.bib152)). Others find that for
    a variety of deep BNN trained using Gaussian variational inference, the posterior
    consistently exhibits strong low-rank structure after convergence (Swiatkowski
    et al., [2020](#bib.bib131)), they propose to decompose the dense covariance matrix
    into a low-rank factorization to simplify the computation. Additionally, (Mishkin
    et al., [2018](#bib.bib90)) assumes the covariance matrix taking the form of “diagonal
    plus low-rank” structure for a more flexible and faster approximation. Another
    line of work introduces sparse uncertainty structure via a hierarchical posterior
    or employing normalizing flow with low-dimensional auxiliary variables (Ritter
    et al., [2021](#bib.bib117); Louizos and Welling, [2017](#bib.bib83)) to reduce
    the computation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断的一个挑战在于选择参数化分布类 $q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$。原始方法依赖于使用对角协方差矩阵的高斯近似分布（均值场变分推断）（Hinton
    和 Van Camp，[1993](#bib.bib54)；Posch 等，[2019](#bib.bib109)）。该方法为优化提供了一个简单的下界，但近似能力有限。为了捕捉参数之间的后验相关性，第二类方法将对角协方差扩展到一般协方差矩阵，同时通过最大化上述
    ELBO 仍然得到一个可处理的算法（Posch 和 Pilz，[2020](#bib.bib108)）。然而，全协方差矩阵不仅增加了可训练参数的数量，还为深度神经网络带来了巨大的内存和计算开销。为了减少计算量，第三类方法旨在通过某些假设简化协方差矩阵结构。一些方法假设层之间的独立性，导致块对角结构协方差矩阵（Sun
    等，[2017](#bib.bib129)；Zhang 等，[2018](#bib.bib152)）。其他研究发现，对于使用高斯变分推断训练的各种深度贝叶斯神经网络，后验在收敛后始终表现出强烈的低秩结构（Swiatkowski
    等，[2020](#bib.bib131)），他们提出将密集协方差矩阵分解为低秩因式以简化计算。此外，（Mishkin 等，[2018](#bib.bib90)）假设协方差矩阵具有“对角加低秩”结构，以实现更灵活和更快的近似。另一方向的工作通过层次后验引入稀疏不确定性结构，或采用低维辅助变量的归一化流（Ritter
    等，[2021](#bib.bib117)；Louizos 和 Welling，[2017](#bib.bib83)）来减少计算。
- en: The disadvantage of variational Gaussian approximation for DNN parameters is
    that to capture the full correlations among model latent weights, it requires
    large amount of variational parameters to be optimized, which scales quadratically
    with the number of latent weights in the model. Existing works aim to simplify
    the covariance structure with certain assumption while still capturing the correlation
    between neural network parameters to optimize the computational speed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 变分高斯近似在深度神经网络参数中的缺点是，为了捕捉模型潜在权重之间的全部相关性，它需要优化大量的变分参数，这与模型中的潜在权重数量呈平方关系。现有工作旨在通过某些假设简化协方差结构，同时仍能捕捉神经网络参数之间的相关性，以优化计算速度。
- en: 'Laplace approximation: The idea behind the Laplace approximation is to obtain
    an approximate posterior around the ’maximum a posterior’ (MAP) estimator of neural
    network weights with a Gaussian distribution, based on the second derivative of
    the neural network likelihood functions (MacKay, [1992](#bib.bib84)). The method
    can be applied post-hoc to a pre-trained neural network model.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯近似：拉普拉斯近似的理念是通过神经网络权重的“最大后验”（MAP）估计器，获得围绕高斯分布的近似后验，该估计器基于神经网络似然函数的二阶导数（MacKay，[1992](#bib.bib84)）。该方法可以在预训练的神经网络模型后应用。
- en: '| (7) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})\approx p(\boldsymbol{\hat{\theta}}&#124;\mathbf{X},\mathbf{Y})\exp(-\frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{\hat{\theta}})^{T}\mathbf{H}(\boldsymbol{\theta}-\boldsymbol{\hat{\theta}}))$
    |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $p(\boldsymbol{\theta} \mid \mathbf{X}, \mathbf{Y}) \approx p(\boldsymbol{\hat{\theta}}
    \mid \mathbf{X}, \mathbf{Y}) \exp\left(-\frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\hat{\theta}})^{T}
    \mathbf{H} (\boldsymbol{\theta} - \boldsymbol{\hat{\theta}})\right)$ |  |'
- en: 'Thus, the posterior is then approximated as a Gaussian:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，后验分布被近似为高斯分布：
- en: '| (8) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})=\mathcal{N}(\boldsymbol{\hat{\theta}},\mathbf{H}^{-1})$
    |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $p(\boldsymbol{\theta} \mid \mathbf{X}, \mathbf{Y}) = \mathcal{N}(\boldsymbol{\hat{\theta}},
    \mathbf{H}^{-1})$ |  |'
- en: 'Where $\boldsymbol{\hat{\theta}}$ is the MAP estimate and $\mathbf{H}$ is the
    Hessian matrix. It is the second derivative of the neural network likelihood function
    regarding the model weights, i.e., $\mathbf{H}_{ij}=-\frac{\partial^{2}}{\partial\boldsymbol{\theta}_{i}\partial\boldsymbol{\theta}_{j}}\log
    p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta})$. Fig.  [6](#S5.F6 "Figure 6 ‣ 5.1.1\.
    Bayesian Neural Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ
    Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective")(a) illustrates the intuition of Laplace
    approximation, where the blue density function is the true posterior distribution
    and the orange Gaussian distribution is the Laplace approximation. This method
    approximates the posterior locally can simplify the computation of posterior,
    but the downside is that for the it cannot capture the multi-modal distribution
    with more than one mode since it is a local estimation around the MAP mode.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\hat{\theta}}$ 是最大后验（MAP）估计，$\mathbf{H}$ 是海森矩阵。它是关于模型权重的神经网络似然函数的二阶导数，即
    $\mathbf{H}_{ij} = -\frac{\partial^{2}}{\partial \boldsymbol{\theta}_{i} \partial
    \boldsymbol{\theta}_{j}} \log p(\mathbf{Y} \mid \mathbf{X}, \boldsymbol{\theta})$。图
    [6](#S5.F6 "图 6 ‣ 5.1.1 贝叶斯神经网络 ‣ 5.1 模型不确定性 ‣ 5 深度神经网络不确定性量化方法的分类 ‣ 关于深度神经网络不确定性量化方法的调查：不确定性源的视角")
    (a) 说明了拉普拉斯近似的直觉，其中蓝色密度函数是实际的后验分布，橙色高斯分布是拉普拉斯近似。这种方法可以在局部近似后验分布，从而简化后验分布的计算，但缺点是它无法捕捉到具有多个模态的多模态分布，因为它是围绕
    MAP 模态的局部估计。
- en: '![Refer to caption](img/561031e9476361e62ea00111097a6c82.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/561031e9476361e62ea00111097a6c82.png)'
- en: Figure 6\. Laplace approximation
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 拉普拉斯近似
- en: Moreover, for DNN, it is still infeasible to compute the invert the Hessian
    matrix for all parameters, which is typically in the order of several million.
    Previous work on using Laplace approximation for neural network uncertainty quantification
    mainly aims to leverage the techniques of Hessian approximation to simplify the
    computation. One simple solution is to ignore the covariance between weights and
    only extract the diagonal of the Hessian matrix. Such methods Inspired by the
    Kronecker factored approximations of the curvature of a neural network, the approach
    (Ritter et al., [2018b](#bib.bib116)) factorizes the Hessian matrix into the Kronecker
    product (matrix operation that results in a block matrix) of two smaller matrices.
    The method brings down the inversion cost of the Hessian matrix and can be scaled
    to deep convolutional networks and applied to Bayesian online learning efficiently
    (Ritter et al., [2018a](#bib.bib115)). However, the method introduces an additional
    assumption that each layer of the neural network is independent (ignoring covariance
    between layers), which might lead to an overestimation of the variance (uncertainty)
    in certain directions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于深度神经网络（DNN），计算所有参数的海森矩阵的逆仍然不可行，这通常是数百万的级别。以前关于使用拉普拉斯近似进行神经网络不确定性量化的工作主要旨在利用海森矩阵近似技术来简化计算。一种简单的解决方案是忽略权重之间的协方差，只提取海森矩阵的对角线。这样的办法受到神经网络曲率的克罗内克因子近似的启发，该方法（Ritter
    等人，[2018b](#bib.bib116)）将海森矩阵分解为两个较小矩阵的克罗内克积（矩阵运算，结果是一个块矩阵）。该方法降低了海森矩阵的求逆成本，并且可以扩展到深度卷积网络，并高效地应用于贝叶斯在线学习（Ritter
    等人，[2018a](#bib.bib115)）。然而，该方法引入了一个额外的假设，即神经网络的每一层是独立的（忽略了层间的协方差），这可能会导致在某些方向上对方差（不确定性）的高估。
- en: To make the Laplace approximation allow for uncertainty quantification for contemporary
    DNN, another challenge is to calibrate the predictive uncertainty. One standard
    practice is to tune the prior precision of the Gaussian prior on model weights
    (Ritter et al., [2018b](#bib.bib116)). This has a regularizing effect both on
    the approximation to the true Hessian, as well as the Laplace approximation itself,
    which may be placing probability mass in low probability areas of the true posterior.
    However, these parameters require optimization w.r.t. the prediction uncertainty
    performance on a validation dataset, which may not generalize well on new test
    data. To overcome this disadvantage, another approach introduces a more flexible
    framework to tune the uncertainty of Laplace-approximated BNNs by adding some
    additional ’hidden units’ to the hidden layer of MLP-trained network (Kristiadi
    et al., [2021](#bib.bib72)). The framework is trained with an uncertainty-aware
    objective to improve the uncertainty calibration of Laplace approximations. However,
    the limitation of this method is it can only be applied to MLP-trained networks,
    and cannot generalize to other models, e.g., convolutional neural networks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使拉普拉斯近似能够对现代深度神经网络进行不确定性量化的另一个挑战是校准预测不确定性。一种标准做法是调整高斯先验在模型权重上的先验精度（Ritter 等，[2018b](#bib.bib116)）。这对真实Hessian的近似以及拉普拉斯近似本身都有正则化效果，可能将概率质量放在真实后验的低概率区域。然而，这些参数需要根据验证数据集上的预测不确定性性能进行优化，可能在新的测试数据上无法很好地推广。为了克服这一缺点，另一种方法引入了一个更灵活的框架，通过向MLP训练的网络的隐藏层添加一些额外的“隐藏单元”来调整拉普拉斯近似的BNNs的不确定性（Kristiadi
    等，[2021](#bib.bib72)）。该框架以不确定性感知的目标进行训练，以改善拉普拉斯近似的不确定性校准。然而，这种方法的限制在于它只能应用于MLP训练的网络，无法推广到其他模型，例如卷积神经网络。
- en: To sum up, the previous methods employed optimization-based schemes like variational
    inference and Laplace approximations of the posterior. In doing so, strong assumptions
    and restrictions on the form of the posterior are enforced. The common practice
    is to approximate the posterior with a Gaussian distribution. The restrictions
    placed are often credited with inaccuracies induced in predictions and uncertainty
    quantification performance. The difference between two approximations is that
    Laplace approximation is a local approximation around the MAP estimation, and
    it can be applied to a pre-trained neural network and obtain uncertainty quantification
    without influencing the performance of the neural network. On the other hand,
    the variational inference is global optimization used during training and may
    influence the model prediction performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，以前的方法采用了基于优化的方案，如变分推断和后验的拉普拉斯近似。在这样做的过程中，对后验的形式施加了强假设和限制。常见的做法是用高斯分布来近似后验。施加的限制通常被认为会导致预测和不确定性量化性能中的不准确性。两种近似之间的区别在于，拉普拉斯近似是在MAP估计附近的局部近似，并且可以应用于预训练的神经网络，以获得不确定性量化，而不会影响神经网络的性能。另一方面，变分推断是用于训练中的全局优化，可能会影响模型的预测性能。
- en: 'Markov Chain Monto Carlo approximation: Markov Chain Monto Carlo (MCMC) is
    a general method for sampling from an intractable distribution. MCMC constructs
    an ergodic Markov chain whose stationary distribution is posterior $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$.
    Then we can sample from the stationary distribution. The inference step of BNN
    (Eq. [5](#S5.E5 "In 5.1.1\. Bayesian Neural Networks ‣ 5.1\. Model Uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective")) can be
    approximated as the following equation:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '马尔可夫链蒙特卡洛近似：马尔可夫链蒙特卡洛（MCMC）是一种从难以处理的分布中进行采样的通用方法。MCMC构造一个遍历性的马尔可夫链，其平稳分布是后验分布
    $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$。然后我们可以从平稳分布中进行采样。贝叶斯神经网络（BNN）的推断步骤（公式
    [5](#S5.E5 "In 5.1.1\. Bayesian Neural Networks ‣ 5.1\. Model Uncertainty ‣ 5\.
    A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective")）可以近似为以下方程：'
- en: '| (9) |  | $\footnotesize p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}\approx\frac{1}{N}\sum_{i=1}^{N}p(y*&#124;x*,\boldsymbol{\theta}_{i})$
    |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\footnotesize p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}\approx\frac{1}{N}\sum_{i=1}^{N}p(y*&#124;x*,\boldsymbol{\theta}_{i})$
    |  |'
- en: where $\boldsymbol{\theta}_{i}\sim p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$
    is a sampled from MCMC. This process generates samples by a Markov chain over
    a state space, where each sample only depends on the state of the previous sample.
    The dependency is described with a proposal distribution $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})$
    that specifies the probability density of transitioning to a new sample $\boldsymbol{\theta}^{\prime}$
    from a given sample $\boldsymbol{\theta}$. Some acceptance criteria are based
    on the relative density (energy) of two successive samples evaluated at the posterior
    to determine whether accept the new sample or the previous sample. The vanilla
    implementation is through the Metropolis-Hastings algorithm (Murphy, [2012](#bib.bib93))
    with Gaussian proposal distribution $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})\sim\mathcal{N}(\boldsymbol{\theta},\Sigma)$.
    Specifically, in each iteration, the algorithm constructs a Markov chain over
    the state space of $\boldsymbol{\theta}$ with the proposal density distribution.
    The proposal samples is stochastically accepted with the acceptance probability
    $\alpha(\boldsymbol{\theta}^{\prime},\boldsymbol{\theta})=\frac{T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})p(\boldsymbol{\theta}^{\prime}|x)}{T(\boldsymbol{\theta}|\boldsymbol{\theta}^{\prime})p(\boldsymbol{\theta}|\boldsymbol{x})}$
    via a random variable $\mu$ drawn uniformly from the interval $[0,1]$ ($\mu\sim\text{Unif}(0,1)$).
    If we reject the proposal sample, we retain the previous sample $\boldsymbol{\theta}$.
    This strategy ensures the stationary distribution of the samples converges to
    the true posterior after a sufficient number of iterations. However, the isotropic
    Gaussian proposal distribution shows random walk behavior and can cause slow exploration
    of the sample space and a high rejection rate. Thus it takes a longer time to
    converge to the stationary distribution. The problem is more severe because of
    the high dimensional parameter space of modern DNN and hinders the application
    of MCMC on neural network parameter sampling (Neal, [2012](#bib.bib96)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\theta}_{i}\sim p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$
    是从 MCMC 中采样得到的。这个过程通过一个状态空间上的马尔可夫链生成样本，每个样本仅依赖于前一个样本的状态。这个依赖关系用提议分布 $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})$
    来描述，该分布指定了从给定样本 $\boldsymbol{\theta}$ 转移到新样本 $\boldsymbol{\theta}^{\prime}$ 的概率密度。一些接受标准基于在后验分布下评估的两个连续样本的相对密度（能量），以决定是接受新样本还是保留之前的样本。最简单的实现是通过
    Metropolis-Hastings 算法（Murphy, [2012](#bib.bib93)）来完成的，该算法使用高斯提议分布 $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})\sim\mathcal{N}(\boldsymbol{\theta},\Sigma)$。具体而言，在每次迭代中，该算法在
    $\boldsymbol{\theta}$ 的状态空间上构建马尔可夫链，并使用提议密度分布。提议样本通过接受概率 $\alpha(\boldsymbol{\theta}^{\prime},\boldsymbol{\theta})=\frac{T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})p(\boldsymbol{\theta}^{\prime}|x)}{T(\boldsymbol{\theta}|\boldsymbol{\theta}^{\prime})p(\boldsymbol{\theta}|\boldsymbol{x})}$
    被随机接受，随机变量 $\mu$ 从区间 $[0,1]$ 中均匀抽取 ($\mu\sim\text{Unif}(0,1)$)。如果我们拒绝提议样本，我们保留之前的样本
    $\boldsymbol{\theta}$。这个策略确保样本的平稳分布在经过足够多的迭代后收敛到真实后验分布。然而，各向同性高斯提议分布表现出随机游走行为，可能导致样本空间探索缓慢和高拒绝率。因此，需要更长的时间才能收敛到平稳分布。由于现代深度神经网络的高维参数空间，这个问题更为严重，并阻碍了
    MCMC 在神经网络参数采样中的应用（Neal, [2012](#bib.bib96)）。
- en: The recent development on MCMC for modern DNN mainly focuses on how to sample
    more efficiently and reduce convergence iterations. For example, One direction
    aims to combine the MCMC sampling with variational inference (Salimans et al.,
    [2015](#bib.bib120); Wolf et al., [2016](#bib.bib147)) to take advantage of both
    methods. Since variational inference approximates the posterior by formulating
    an optimization problem w.r.t. the variational posterior is selected from a fixed
    family of distributions. This approach could be fast by ensuring some constraints
    on the variational posterior format, but they may approximate the true posterior
    poorly even with very low ELBO loss. On the other hand, MCMC does not have any
    constraints on the approximated posterior shape, and can potentially approximate
    the exact posterior arbitrarily well with a sufficient number of iterations. Markov
    Chain Variational Inference (MCVI) bridge the accuracy and speed gap between MCMC
    and VI by interpreting the iterative Markov chain $q(\boldsymbol{\theta}|x)=q(\boldsymbol{\theta}_{0})\prod_{t=1}^{T}q(\boldsymbol{\theta}_{t}|\boldsymbol{\theta}_{t-1},x)$
    as a variational approximation in the expanded space with $\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$.
    Thus, instead of constructing a sequential Markov Chain to sample $\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$,
    they use another auxiliary variational inference distribution $r(\boldsymbol{\theta}_{0},...,\boldsymbol{\theta}_{T-1})$
    to approximate the true distribution with some flexible parametric form. By optimizing
    the lower bound over the parameters of the variational distribution with a neural
    network, we can obtain the Markov Chain samples.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 DNN 的 MCMC 近期发展主要集中在如何更高效地采样和减少收敛迭代次数。例如，一种方向是将 MCMC 采样与变分推断相结合（Salimans
    等，[2015](#bib.bib120); Wolf 等，[2016](#bib.bib147)），以发挥两者的优势。由于变分推断通过将后验的优化问题公式化来近似后验，而变分后验从固定的分布家族中选择。这种方法通过确保对变分后验格式的一些约束可以加快速度，但即使在非常低的
    ELBO 损失下，它们可能对真实后验的近似效果较差。另一方面，MCMC 对近似后验的形状没有任何约束，并且可以在足够多的迭代下任意好地近似准确后验。Markov
    Chain Variational Inference (MCVI) 通过将迭代的 Markov 链 $q(\boldsymbol{\theta}|x)=q(\boldsymbol{\theta}_{0})\prod_{t=1}^{T}q(\boldsymbol{\theta}_{t}|\boldsymbol{\theta}_{t-1},x)$
    解释为扩展空间中的变分近似，从而弥合 MCMC 和 VI 之间的准确度和速度差距。在这个扩展空间中，$\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$。因此，它们不是构建一个顺序的
    Markov 链来采样 $\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$，而是使用另一个辅助变分推断分布
    $r(\boldsymbol{\theta}_{0},...,\boldsymbol{\theta}_{T-1})$ 来以一些灵活的参数形式近似真实分布。通过用神经网络优化变分分布参数的下界，我们可以获得
    Markov 链样本。
- en: The advantage of MCMC methods is that the samples it gives are guaranteed to
    converge to the exact posterior after a sufficient number of iterations. This
    property allows us the control the trade-off between sampling accuracy and computation.
    However, the downside of this method is that we do not know how many iterations
    are enough for convergence and it may take an excessive amount of time and computing
    resources.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MCMC 方法的优势在于它提供的样本在经过足够多的迭代后可以保证收敛到准确的后验。这一特性使我们能够控制采样准确度与计算之间的权衡。然而，这种方法的缺点是我们不知道多少次迭代才足够收敛，并且可能需要过多的时间和计算资源。
- en: '![Refer to caption](img/567cfb01ea268199ae7f7c2666f6e1b9.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/567cfb01ea268199ae7f7c2666f6e1b9.png)'
- en: (a) MC dropout
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MC dropout
- en: Figure 7\. Illustration of model uncertainty
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 模型不确定性的示意图
- en: 'Monto-Carlo (MC) dropout: MC dropout approach (Gal and Ghahramani, [2016](#bib.bib39))
    is currently the most popular method for DNN uncertainty quantification in many
    domains due to its simplicity and ease of implementation. This approach demonstrates
    that the optimization of a neural network with a dropout layer can be equivalent
    to approximating a BNN with variational inference on the parametric Bernoulli
    distribution (Gal and Ghahramani, [2016](#bib.bib39)). Uncertainty estimation
    can be obtained by computing the variance on multiple stochastic forward predictions
    with different dropout masks (switching-off some neurons’ activation) as Fig. [7](#S5.F7
    "Figure 7 ‣ 5.1.1\. Bayesian Neural Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A
    Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") shows. The average
    predictions with various weights dropout can be interpreted as doing the approximate
    integration over the model’s weights (as Eq.[5](#S5.E5 "In 5.1.1\. Bayesian Neural
    Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective")) whose variational distribution follows the Bernoulli distribution.
    The advantages of MC dropout are: First, it requires little modification to existing
    DNN architecture design, which allows for straightforward implementation in practice.
    Second, it mitigates the problem of representing uncertainty by sacrificing prediction
    accuracy since the methods only influence inference step. However, though there
    is theoretical justification for the probabilistic interpretation of MC dropout
    from variational approximation perspective, in many uncertainty benchmark dataset,
    MC dropout tends to be less calibrated than other baseline UQ methods (Guo et al.,
    [2017](#bib.bib47)).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Monto-Carlo (MC) dropout：MC dropout方法（Gal 和 Ghahramani, [2016](#bib.bib39)）由于其简单性和易于实现，当前在许多领域中是DNN不确定性量化的最流行方法。该方法表明，通过带有dropout层的神经网络的优化，可以等同于通过对参数Bernoulli分布进行变分推断来近似BNN（Gal
    和 Ghahramani, [2016](#bib.bib39)）。不确定性估计可以通过计算不同dropout掩码（关闭一些神经元激活）的多个随机前向预测的方差来获得，如图[7](#S5.F7
    "图7 ‣ 5.1.1. 贝叶斯神经网络 ‣ 5.1. 模型不确定性 ‣ 5. DNN UQ方法学分类 ‣ 深度神经网络不确定性量化方法的调查：不确定性来源的视角")所示。具有各种权重dropout的平均预测可以解释为对模型权重进行近似积分（如等式[5](#S5.E5
    "在5.1.1. 贝叶斯神经网络 ‣ 5.1. 模型不确定性 ‣ 5. DNN UQ方法学分类 ‣ 深度神经网络不确定性量化方法的调查：不确定性来源的视角")）其变分分布遵循Bernoulli分布。MC
    dropout的优点是：首先，它对现有DNN架构设计几乎没有修改要求，这使得在实践中易于实现。其次，它通过牺牲预测准确性来缓解表示不确定性的问题，因为这些方法仅影响推断步骤。然而，尽管从变分近似的角度对MC
    dropout的概率解释有理论依据，但在许多不确定性基准数据集中，MC dropout往往比其他基线UQ方法校准度差（Guo 等人，[2017](#bib.bib47)）。
- en: In summary, we summarize several types of approximation on BNN, which aim to
    reduce the computation and memory burden of BNN and make it scalable to modern
    deep neural networks. Those approximation methods can capture the model uncertainty
    associated with parameters to some extent. However, the drawback is that the process
    requires approximation, which may lead to inaccurate uncertainty estimation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们总结了几种关于BNN的近似方法，这些方法旨在减少BNN的计算和内存负担，并使其能够扩展到现代深度神经网络。这些近似方法在一定程度上可以捕捉与参数相关的模型不确定性。然而，缺点是该过程需要近似，这可能导致不准确的不确定性估计。
- en: 5.1.2\. Ensemble models
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2. 集成模型
- en: 'Ensemble models combine multiple neural network models in the prediction process.
    The ensemble predictions form an output distribution. The prediction variability
    of the ensemble models can be a measure of model uncertainty (e.g., a larger variance
    implies larger uncertainty). To capture the model uncertainty arising from different
    aspects, several strategies for constructing ensembles can be adopted, which are
    summarized into three major categories: the first kind is through bootstrapping
    (Lakshminarayanan et al., [2017](#bib.bib73)). This is a strategy for random sampling
    from the original dataset with replacement. An ensemble of neural network models
    is constructed and each model is trained on different bootstrapped samples. After
    training, the inference is done through the aggregation of the ensembles, and
    uncertainty is obtained from the prediction variance (regression) or average entropy
    (classification). The second strategy is to construct different neural network
    architectures (number of layers, hidden neurons, type of activation functions.)
    (Mallick et al., [2022](#bib.bib86)). This strategy can account for the uncertainty
    from model misspecification. Other strategies include different initialization
    of parameters along with a random shuffle of the datasets. This is better than
    the bootstrap strategy since more samples are utilized for each model. The third
    type is hyperensemble (Wenzel et al., [2020](#bib.bib143)). This approach constructs
    ensembles with different hyper-parameters, such as learning rate, optimization
    strategy, and training strategy.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型在预测过程中结合了多个神经网络模型。集成预测形成一个输出分布。集成模型的预测变异性可以作为模型不确定性的衡量标准（例如，较大的方差意味着较大的不确定性）。为了捕捉来自不同方面的模型不确定性，可以采用几种构建集成的策略，这些策略总结为三大类：第一类是通过自助法（Lakshminarayanan
    et al., [2017](#bib.bib73)）。这是一种从原始数据集中随机抽样并替换的策略。构建一个神经网络模型的集成，每个模型在不同的自助样本上进行训练。训练完成后，通过集成的聚合进行推理，并从预测方差（回归）或平均熵（分类）中获得不确定性。第二种策略是构建不同的神经网络架构（层数、隐藏神经元、激活函数类型）（Mallick
    et al., [2022](#bib.bib86)）。这种策略可以考虑到来自模型误指定的不确定性。其他策略包括参数的不同初始化以及数据集的随机洗牌。这比自助法策略更好，因为每个模型利用了更多的样本。第三种类型是超集成（Wenzel
    et al., [2020](#bib.bib143)）。这种方法构建具有不同超参数的集成，如学习率、优化策略和训练策略。
- en: 'Though the ensemble model is simple and scalable to the large dataset for modern
    neural networks, this method has several limitations: first, there is much computational
    overhead, since it requires training multiple independent networks and needs to
    keep all these networks in memory during inference. Second, model diversity is
    a necessary requirement for ensuring the diversity of the ensemble models to provide
    accurate uncertainty estimation. Otherwise, the ensemble model can collapse to
    the same local minima.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管集成模型简单且可以扩展到现代神经网络的大型数据集，但这种方法有几个局限性：首先，计算开销较大，因为它需要训练多个独立的网络，并且在推理过程中需要保持所有这些网络在内存中。其次，模型多样性是确保集成模型多样性以提供准确的不确定性估计的必要要求。否则，集成模型可能会收敛到相同的局部最小值。
- en: 5.1.3\. Sample density-aware neural network
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 样本密度感知神经网络
- en: Because of the theoretical soundness of the BNN model and the simplicity of
    deep ensemble models, they are the popular methods for modern DNN uncertainty
    quantification. However, the computational challenge with BNN and ensemble models
    make it infeasible for real-world applications. In addition, the approaches can
    address the model uncertainty associated with the parameter, model architecture,
    and training process stochasticity, but cannot generalize to model uncertainty
    coming from low sample density, which means, the samples lie far away from the
    support of the training sets may show over-confident results. In this regard,
    many approaches have been motivated to develop sample density-aware neural networks
    to capture the model uncertainty due to low training sample density.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BNN模型的理论可靠性和深度集成模型的简单性，它们是现代DNN不确定性量化的流行方法。然而，BNN和集成模型的计算挑战使其在实际应用中不可行。此外，这些方法可以解决与参数、模型架构和训练过程随机性相关的模型不确定性，但无法推广到来自低样本密度的模型不确定性，即样本远离训练集支持区域时可能会显示过度自信的结果。在这方面，许多方法被激发出来以开发样本密度感知神经网络，以捕捉由于低训练样本密度导致的模型不确定性。
- en: 'Gaussian Process Hybrid Neural Network:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程混合神经网络：
- en: 'Summary on Gaussian process: Gaussian process (GP) is a variant of stochastic
    process, where any finite collection of random variables follow a multivariate
    Gaussian distribution (Williams and Rasmussen, [2006](#bib.bib144)). Given a set
    of points $\boldsymbol{x}_{1},...\boldsymbol{x}_{n}$, a GP defines a prior over
    functions $y_{i}=f(\boldsymbol{x}_{i})$, and assumes the $p(y_{1},...,y_{n})$
    follow the Gaussian distribution $\mathcal{N}(\boldsymbol{\mu}(\boldsymbol{x}),\boldsymbol{\Sigma}(\boldsymbol{x}))$,
    where$\boldsymbol{\mu}(\boldsymbol{x})$ is the mean function, and $\boldsymbol{\Sigma}(\boldsymbol{x})$
    is the covariance function, given with $\boldsymbol{\Sigma}_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$.
    $\kappa$ is a positive definite kernel function (radial basis function), that
    measures the similarity between any pairs of input samples. The kernel function
    also plays a role in controlling the smoothness of the GPs. For GP inference,
    given the new sample $\boldsymbol{x_{*}}$, the joint distribution between the
    new sample prediction $y_{*}$ and training samples target variable $y$ has the
    following form'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关于高斯过程的总结：高斯过程（GP）是随机过程的一种变体，其中任何有限的随机变量集合都遵循多元高斯分布（Williams 和 Rasmussen，[2006](#bib.bib144)）。给定一组点
    $\boldsymbol{x}_{1},...\boldsymbol{x}_{n}$，GP 定义了函数 $y_{i}=f(\boldsymbol{x}_{i})$
    的先验，并假设 $p(y_{1},...,y_{n})$ 遵循高斯分布 $\mathcal{N}(\boldsymbol{\mu}(\boldsymbol{x}),\boldsymbol{\Sigma}(\boldsymbol{x}))$，其中
    $\boldsymbol{\mu}(\boldsymbol{x})$ 是均值函数，$\boldsymbol{\Sigma}(\boldsymbol{x})$
    是协方差函数，给定 $\boldsymbol{\Sigma}_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$。$\kappa$
    是一个正定核函数（径向基函数），用于衡量任何输入样本对之间的相似性。核函数还在控制 GP 的平滑度方面发挥作用。对于 GP 推断，给定新的样本 $\boldsymbol{x_{*}}$，新样本预测
    $y_{*}$ 和训练样本目标变量 $y$ 之间的联合分布具有以下形式
- en: '| (10) |  | <math   alttext="\begin{pmatrix}\mathbf{y}\\ y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '| (10) |  | <math   alttext="\begin{pmatrix}\mathbf{y}\\ y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\'
- en: \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&amp;\mathbf{K}_{\boldsymbol{x}}\\
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&amp;\mathbf{K}_{\boldsymbol{x}}\\
- en: \mathbf{K}_{\boldsymbol{x}}^{T}&amp;K_{*}\end{pmatrix}\Bigg{)}" display="block"><semantics
    ><mrow  ><mrow ><mo  >(</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mi  >𝐲</mi></mtd></mtr><mtr ><mtd ><msub  ><mi >y</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo
    >)</mo></mrow><mo >=</mo><mrow  ><mi >𝒩</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="260%" minsize="260%" >(</mo><mrow ><mo  >(</mo><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd ><mi  >𝝁</mi></mtd></mtr><mtr ><mtd ><msub  ><mi
    >μ</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo >)</mo></mrow><mo >,</mo><mrow  ><mo
    >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><msub  ><mi >𝐊</mi><mi >n</mi></msub></mtd><mtd ><msub ><mi  >𝐊</mi><mi
    >𝒙</mi></msub></mtd></mtr><mtr ><mtd ><msubsup  ><mi >𝐊</mi><mi >𝒙</mi><mi >T</mi></msubsup></mtd><mtd
    ><msub ><mi  >K</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo >)</mo></mrow><mo
    maxsize="260%" minsize="260%"  >)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix  ><matrixrow
    ><ci >𝐲</ci></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci></apply></matrixrow></matrix></apply><apply ><ci >𝒩</ci><interval closure="open"
    ><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><ci  >𝝁</ci></matrixrow><matrixrow
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜇</ci></apply></matrixrow></matrix></apply><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix  ><matrixrow ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐊</ci><ci >𝑛</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐊</ci><ci >𝒙</ci></apply></matrixrow><matrixrow
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐊</ci><ci >𝒙</ci></apply><ci >𝑇</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐾</ci></apply></matrixrow></matrix></apply></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}\mathbf{y}\\ y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\
    \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&\mathbf{K}_{\boldsymbol{x}}\\
    \mathbf{K}_{\boldsymbol{x}}^{T}&K_{*}\end{pmatrix}\Bigg{)}</annotation></semantics></math>
    |  |
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{K}_{\boldsymbol{x}}^{T}&amp;K_{*}\end{pmatrix}\Bigg{)}" display="block"><semantics
    ><mrow  ><mrow ><mo  >(</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mi  >𝐲</mi></mtd></mtr><mtr ><mtd ><msub  ><mi >y</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo
    >)</mo></mrow><mo >=</mo><mrow  ><mi >𝒩</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="260%" minsize="260%" >(</mo><mrow ><mo  >(</mo><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd ><mi  >𝝁</mi></mtd></mtr><mtr ><mtd ><msub  ><mi
    >μ</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo >)</mo></mrow><mo >,</mo><mrow  ><mo
    >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><msub  ><mi >𝐊</mi><mi >n</mi></msub></mtd><mtd ><msub ><mi  >𝐊</mi><mi
    >𝒙</mi></msub></mtd></mtr><mtr ><mtd ><msubsup  ><mi >𝐊</mi><mi >𝒙</mi><mi >T</mi></msubsup></mtd><mtd
    ><msub ><mi  >K</mi><mo >∗</mo></msub></mtd></mtr></mtable><mo >)</mo></mrow><mo
    maxsize="260%" minsize="260%"  >)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix  ><matrixrow
    ><ci >𝐲</ci></matrixrow><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci></apply></matrixrow></matrix></apply><apply ><ci >𝒩</ci><interval closure="open"
    ><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><ci  >𝝁</ci></matrixrow><matrixrow
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜇</ci></apply></matrixrow></matrix></apply><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix  ><matrixrow ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐊</ci><ci >𝑛</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐊</ci><ci >𝒙</ci></apply></matrixrow><matrixrow
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐊</ci><ci >𝒙</ci></apply><ci >𝑇</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐾</ci></apply></matrixrow></matrix></apply></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}\mathbf{y}\\ y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\
    \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&\mathbf{K}_{\boldsymbol{x}}\\
    \mathbf{K}_{\boldsymbol{x}}^{T}&K_{*}\end{pmatrix}\Bigg{)}</annotation></semantics></math>
    |  |
- en: where $\mathbf{K}_{n}$ is the covariance matrix between $n$ training samples
    and $\mathbf{K}_{\boldsymbol{x}}$ is the covariance vector between the test sample
    and training samples. $K_{*}$ is the prior variance of the test sample.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{K}_{n}$ 是 $n$ 个训练样本之间的协方差矩阵，$\mathbf{K}_{\boldsymbol{x}}$ 是测试样本与训练样本之间的协方差向量。
    $K_{*}$ 是测试样本的先验方差。
- en: '![Refer to caption](img/21a02173846eda93835a7293138bc94c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/21a02173846eda93835a7293138bc94c.png)'
- en: 'Figure 8\. Gaussian Process inference example: green lines are the prediction
    sample distribution'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 高斯过程推断示例：绿色线条是预测样本分布
- en: 'Then the prediction on new test input sample $\boldsymbol{x}_{*}$ is through
    computing the posterior distribution conditioned on the training data $\mathcal{D}_{\text{tr}}$,
    given by Eq. [11](#S5.E11 "In 5.1.3\. Sample density-aware neural network ‣ 5.1\.
    Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective").
    The mean term of the predictive distribution can be interpreted as a weighted
    average of the training set label. The weight is dependent on the input samples’
    similarity measured by kernel function in feature space. Besides the point estimation
    from the mean function, the prediction uncertainty is indicated with the variance
    of the inferred distribution. GP inference gives lower uncertainty if the test
    samples are around the region where training samples are abundant (higher sample
    density as in the middle part of Fig [8](#S5.F8 "Figure 8 ‣ 5.1.3\. Sample density-aware
    neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective")), otherwise resulting in higher uncertainty
    (boundary part of Fig [8](#S5.F8 "Figure 8 ‣ 5.1.3\. Sample density-aware neural
    network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective")). This property is important for the out-of-distribution
    (OOD) data since OOD data lies far away from the training samples in the feature
    space, which could be detected with GP efficiently.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，对新的测试输入样本 $\boldsymbol{x}_{*}$ 的预测是通过计算条件在训练数据 $\mathcal{D}_{\text{tr}}$
    下的后验分布来实现的，如 Eq. [11](#S5.E11 "In 5.1.3\. Sample density-aware neural network
    ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on
    Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty Source’s
    Perspective") 所示。预测分布的均值项可以被解释为训练集标签的加权平均。权重取决于输入样本在特征空间中通过核函数测量的相似性。除了均值函数的点估计外，预测不确定性由推断分布的方差表示。如果测试样本位于训练样本丰富的区域（如图 [8](#S5.F8
    "Figure 8 ‣ 5.1.3\. Sample density-aware neural network ‣ 5.1\. Model Uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") 中的中间部分），GP
    推断会给出较低的不确定性，否则会导致较高的不确定性（图 [8](#S5.F8 "Figure 8 ‣ 5.1.3\. Sample density-aware
    neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") 的边界部分）。这一特性对分布外（OOD）数据非常重要，因为 OOD 数据在特征空间中远离训练样本，这可以通过
    GP 高效检测出来。'
- en: '| (11) |  | $p(y_{*}&#124;x_{*},\mathcal{D}_{\text{train}},\boldsymbol{\theta})=\mathcal{N}(y&#124;\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}^{-1}\boldsymbol{y},K_{x*}-\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}\mathbf{K}_{\boldsymbol{x}}),$
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $p(y_{*}&#124;x_{*},\mathcal{D}_{\text{train}},\boldsymbol{\theta})=\mathcal{N}(y&#124;\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}^{-1}\boldsymbol{y},K_{x*}-\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}\mathbf{K}_{\boldsymbol{x}}),$
    |  |'
- en: 'Sparse Gaussian process: Though possessing the intriguing property of inherent
    uncertainty estimation, GP is prohibitive for large datasets because the inversion
    of covariance matrix requires $\mathcal{O}(n^{3})$ complexity ($n$ is the total
    number of training samples), which takes high computation and storage cost for
    training and inference on large datasets. In this regard, many methods (Snelson
    and Ghahramani, [2005](#bib.bib126); Titsias, [2009](#bib.bib133))attempt to make
    a sparse approximation to the full GP to bring down the computation to $\mathcal{O}(m^{2}n)$
    ($m$ is the number of inducing valriables and $m\ll n$. The inducing variables
    can be anywhere in the input domain, not constrained to be a subset of the training
    data, which is denoted as input-output pairs $\{\hat{\boldsymbol{x}}_{i},\hat{y}_{i}\}_{i=1}^{m}$.
    Then the inversion of the original covariance matrix $\mathbf{K}_{n}$ can be replaced
    with a low-rank approximation from the inducing variables, which only requires
    the inversion of $m\times m$ matrix $\mathbf{K}_{m}$. Then the question becomes
    how to select $m$ best-inducing variables to be representative of the training
    dataset. Common approaches assume that the best representative inducing variables
    are those that maximize the likelihood (ML) of the training data (Snelson and
    Ghahramani, [2005](#bib.bib126)). Then the location of inducing variables and
    hyper-parameters of GP are optimized simultaneously through ML. The training data
    likelihood can be obtained by marginalizing out the inducing variables on the
    joint distribution of training data and inducing variables. Another approach variation
    sparse GP formulates a variation lower bound of the exact likelihood by treating
    the locations of inducing variables as variational parameters (Titsias, [2009](#bib.bib133))
    for optimization. To further reduce the computation for more scalable inference
    on a large dataset, the paper (Wilson and Nickisch, [2015](#bib.bib145)) proposes
    a kernel interpolation method for scalable structured GP method by exploiting
    the structure of the covariance by imposing grid constraint on the inducing variables.
    In this way, the kernel matrix $\mathbf{K}_{m}$ admits the Kronecker structure
    and is much easier for computing the inversion.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏高斯过程：尽管具有固有不确定性估计的有趣特性，但GP对于大数据集来说是不可行的，因为协方差矩阵的求逆需要$\mathcal{O}(n^{3})$复杂度（$n$是训练样本的总数），这在大数据集上的训练和推断需要高计算和存储成本。在这方面，许多方法（Snelson和Ghahramani，[2005](#bib.bib126)；Titsias，[2009](#bib.bib133)）试图对完整的GP进行稀疏近似，以将计算降低到$\mathcal{O}(m^{2}n)$（$m$是诱导变量的数量且$m\ll
    n$）。诱导变量可以在输入域中的任何位置，而不必限制为训练数据的子集，这表示为输入-输出对$\{\hat{\boldsymbol{x}}_{i},\hat{y}_{i}\}_{i=1}^{m}$。然后，可以用诱导变量的低秩近似替代原始协方差矩阵$\mathbf{K}_{n}$的求逆，这只需要对$m\times
    m$矩阵$\mathbf{K}_{m}$进行求逆。接下来，问题变成了如何选择$m$个最佳诱导变量以代表训练数据集。常见的方法假设最佳的代表性诱导变量是那些最大化训练数据的似然（ML）的变量（Snelson和Ghahramani，[2005](#bib.bib126)）。然后，诱导变量的位置和GP的超参数通过ML同时优化。训练数据的似然可以通过在训练数据和诱导变量的联合分布上对诱导变量进行边际化来获得。另一种变体稀疏GP通过将诱导变量的位置视为变分参数（Titsias，[2009](#bib.bib133)）来优化精确似然的变分下界。为了进一步减少计算以实现对大数据集的更大规模推断，论文（Wilson和Nickisch，[2015](#bib.bib145)）提出了一种通过对诱导变量施加网格约束来利用协方差结构的可扩展结构化GP方法的核插值方法。通过这种方式，核矩阵$\mathbf{K}_{m}$承认Kronecker结构，计算求逆变得更为简单。
- en: 'Deep Gaussian process: Besides the computational challenge, another limitation
    of GP is the joint Gaussian distribution assumption on the target variables limits
    the model’s capability in capturing diverse relationships among instances in large
    datasets. Additionally, GP relies heavily on the kernel function to compute the
    similarity between samples by transforming input features into the high-dimensional
    manifold. However, for high-dimension structured data, it is challenging to construct
    appropriate kernel functions to extract hierarchical features for computing similarity
    between samples. To address these limitations, two lines of researches area have
    been proposed: deep kernel learning and Deep Gaussian process.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 深度高斯过程：除了计算挑战，GP的另一个限制是目标变量上的联合高斯分布假设限制了模型在捕捉大数据集中实例之间多样化关系的能力。此外，GP在计算样本间相似性时严重依赖核函数，通过将输入特征转换为高维流形。然而，对于高维结构化数据，构造适当的核函数以提取层次特征以计算样本间相似性具有挑战性。为了解决这些限制，提出了两种研究方向：深度核学习和深度高斯过程。
- en: 'Deep kernel learning (Wilson et al., [2016](#bib.bib146)) aims to combine the
    structured feature learning capability of DNN with the GP to learn more flexible
    representations. The motivation is that DNN can automatically discover meaningful
    representations from high-dimensional data, which could alleviate the fixed kernel
    limitations of GP and improve its expressiveness. Specifically, the deep kernel
    learning transforms the kernel $\mathbf{K}_{\boldsymbol{\theta}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    to $\mathbf{K}_{\boldsymbol{\theta}}(g(\boldsymbol{x}_{i};\boldsymbol{w}),g(\boldsymbol{x}_{j},\boldsymbol{w}))$,
    where $g(\cdot;\boldsymbol{w})$ is the neural network parameterized with $\boldsymbol{w}$
    and $\mathbf{K}_{\boldsymbol{\theta}}$ is the base kernel function (e.g., radial
    basis function) of GP. The deep learning transformation can capture the non-linear
    and hierarchical structure in high-dimensional data. The GP with the base kernel
    is applied on the final layer of DNN and make inference based on the learned latent
    features as Fig. [9](#S5.F9 "Figure 9 ‣ 5.1.3\. Sample density-aware neural network
    ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on
    Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty Source’s
    Perspective")(a) shows. The idea has been successfully applied to spatio-temporal
    crop yield prediction where GP plays a role in accounting for the spatio-temporal
    auto-correlation between samples (You et al., [2017](#bib.bib151)), which cannot
    be reflected from the DNN features.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度核学习（Wilson 等，[2016](#bib.bib146)）旨在将深度神经网络的结构特征学习能力与高斯过程结合起来，以学习更灵活的表示。其动机是深度神经网络可以从高维数据中自动发现有意义的表示，这可以缓解高斯过程固定核的限制，提高其表达能力。具体来说，深度核学习将核
    $\mathbf{K}_{\boldsymbol{\theta}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$ 转换为
    $\mathbf{K}_{\boldsymbol{\theta}}(g(\boldsymbol{x}_{i};\boldsymbol{w}),g(\boldsymbol{x}_{j};\boldsymbol{w}))$，其中
    $g(\cdot;\boldsymbol{w})$ 是以 $\boldsymbol{w}$ 参数化的神经网络，$\mathbf{K}_{\boldsymbol{\theta}}$
    是高斯过程的基础核函数（例如，径向基函数）。深度学习转换可以捕捉高维数据中的非线性和层次结构。高斯过程与基础核函数应用于深度神经网络的最终层，并基于学习到的潜在特征进行推断，如图
    [9](#S5.F9 "图 9 ‣ 5.1.3\. 样本密度感知神经网络 ‣ 5.1\. 模型不确定性 ‣ 5\. 深度神经网络不确定性量化方法分类 ‣ 对深度神经网络不确定性量化方法的调查：不确定性来源的视角")(a)
    所示。这个理念已成功应用于时空作物产量预测，其中高斯过程在考虑样本间时空自相关方面发挥了作用（You 等，[2017](#bib.bib151)），这种自相关不能通过深度神经网络特征反映出来。
- en: '![Refer to caption](img/83edde01462f3353ec34798fb17d585f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83edde01462f3353ec34798fb17d585f.png)'
- en: (a) Deep kernel learning
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 深度核学习
- en: '![Refer to caption](img/57e92ec4383f8c1c1b2a4f9815a1ab24.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57e92ec4383f8c1c1b2a4f9815a1ab24.png)'
- en: (b) Deep Gaussian process
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 深度高斯过程
- en: Figure 9\. Illustration of model uncertainty
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 模型不确定性的示意图
- en: 'The other category of deep GP is compositional Gaussian process (Damianou,
    [2015](#bib.bib25)) and focuses on function composition inspired by the architecture
    of deep neural networks. In this model, each layer is a GP model, whose inputs
    are governed by the output of another GP as Fig. [9](#S5.F9 "Figure 9 ‣ 5.1.3\.
    Sample density-aware neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective")(b) shows. The recursive
    composition of GPs results in a more complex distribution over the target variables
    prediction. which overcomes the joint Gaussian distribution limitation of vanilla
    GP. The forward propagation and joint probability distribution can be written
    as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类深度高斯过程是组合高斯过程（Damianou，[2015](#bib.bib25)），它专注于受深度神经网络架构启发的函数组合。在这个模型中，每一层都是一个高斯过程模型，其输入由另一个高斯过程的输出决定，如图
    [9](#S5.F9 "图 9 ‣ 5.1.3\. 样本密度感知神经网络 ‣ 5.1\. 模型不确定性 ‣ 5\. 深度神经网络不确定性量化方法分类 ‣ 对深度神经网络不确定性量化方法的调查：不确定性来源的视角")(b)
    所示。高斯过程的递归组合导致了对目标变量预测的更复杂分布，这克服了原始高斯过程的联合高斯分布限制。前向传播和联合概率分布可以写成：
- en: '| (12) |  | $\begin{split}&amp;\boldsymbol{y}=\boldsymbol{f}_{L}(\boldsymbol{f}_{L-1}(...\boldsymbol{f}_{1}(\boldsymbol{x})))\\
    &amp;p(y,\boldsymbol{f}_{L},...\boldsymbol{f}_{1}&#124;\boldsymbol{x})\sim p(\boldsymbol{y&#124;\boldsymbol{f}_{L}})\prod_{i=2}^{L}p(\boldsymbol{f}_{i}&#124;\boldsymbol{f}_{i-1})p(\boldsymbol{f}_{1}&#124;\boldsymbol{x})\end{split}$
    |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\begin{split}&amp;\boldsymbol{y}=\boldsymbol{f}_{L}(\boldsymbol{f}_{L-1}(...\boldsymbol{f}_{1}(\boldsymbol{x})))\\
    &amp;p(y,\boldsymbol{f}_{L},...\boldsymbol{f}_{1}|\boldsymbol{x})\sim p(\boldsymbol{y}|\boldsymbol{f}_{L})\prod_{i=2}^{L}p(\boldsymbol{f}_{i}|\boldsymbol{f}_{i-1})p(\boldsymbol{f}_{1}|\boldsymbol{x})\end{split}$
    |  |'
- en: 'where each function $\boldsymbol{f}_{i}(\cdot)$ is a Gaussian process model.
    The intermediate distribution follows the Gaussian distribution, but the final
    distribution will capture a more complex distribution over the target variable
    $\boldsymbol{y}$. The composition also allows the uncertainty to propagate from
    input through each intermediate layer. However, the challenge associated with
    the compositional Gaussian process is: to maximize the data likelihood over $p(\boldsymbol{y}|\boldsymbol{x})$,
    the direct marginalization of hidden variables $\boldsymbol{f}_{i}$ are intractable.
    To overcome this challenge, variational inference by introducing inducing points
    on each hidden layer and optimizing over the variational distribution $q(\boldsymbol{f}_{i})$.
    Then the marginal likelihood lower bound can be obtained by forwarding the variational
    propagation at each layer (Ustyuzhaninov et al., [2020](#bib.bib134)). Moreover,
    the framework also allows for incorporating partial or uncertainty observations
    into the model by placing a prior over the input variables $\boldsymbol{x}\sim\mathcal{N}(\mu_{\boldsymbol{x}},\Sigma_{\boldsymbol{x}})$
    and propagating uncertainty layer by layer (Damianou et al., [2016](#bib.bib26)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 每个函数 $\boldsymbol{f}_{i}(\cdot)$ 是一个高斯过程模型。中间分布遵循高斯分布，但最终分布将捕捉目标变量 $\boldsymbol{y}$
    的更复杂的分布。组合也允许不确定性从输入通过每个中间层传播。然而，组合高斯过程面临的挑战是：为了最大化 $p(\boldsymbol{y}|\boldsymbol{x})$
    的数据似然，隐藏变量 $\boldsymbol{f}_{i}$ 的直接边际化是不可处理的。为克服这一挑战，通过在每个隐藏层引入诱导点并优化变分分布 $q(\boldsymbol{f}_{i})$
    进行变分推断。然后，可以通过在每一层前向变分传播来获得边际似然下界 (Ustyuzhaninov 等，[2020](#bib.bib134))。此外，该框架还允许通过在输入变量
    $\boldsymbol{x}\sim\mathcal{N}(\mu_{\boldsymbol{x}},\Sigma_{\boldsymbol{x}})$
    上放置先验，并逐层传播不确定性，将部分或不确定观察值纳入模型 (Damianou 等，[2016](#bib.bib26))。
- en: Table 1\. Model uncertainty methods comparison
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 模型不确定性方法比较
- en: '| Model | Approach | Pros | Cons |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BNN: Pros: Capture parameter uncertainty Cons: High computational costs |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| BNN: 优点: 捕捉参数不确定性 缺点: 高计算成本 |'
- en: '&#124; Variational &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变分 &#124;'
- en: '&#124; inference &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推断中捕捉不确定性 &#124;'
- en: '&#124; (Posch et al., [2019](#bib.bib109); Louizos and Welling, [2017](#bib.bib83))
    &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Posch 等，[2019](#bib.bib109); Louizos 和 Welling，[2017](#bib.bib83))
    &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; More flexible in the &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 &#124;'
- en: '&#124; variational distribution format &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变分分布格式 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need approximation on &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要对 &#124;'
- en: '&#124; the covariance format to &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协方差格式进行 &#124;'
- en: '&#124; trade off the computation. &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 权衡计算。 &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Laplace &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拉普拉斯 &#124;'
- en: '&#124; approximation &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 近似 &#124;'
- en: '&#124; (Ritter et al., [2018b](#bib.bib116), [a](#bib.bib115); Kristiadi et al.,
    [2021](#bib.bib72)) &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Ritter 等，[2018b](#bib.bib116)，[a](#bib.bib115); Kristiadi 等，[2021](#bib.bib72))
    &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Do not impact the &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不影响 &#124;'
- en: '&#124; neural network performance. &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经网络性能进行近似。 &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need approximation in &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要进行近似的 &#124;'
- en: '&#124; the covariance format and &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协方差格式和 &#124;'
- en: '&#124; cannot capture the &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无法捕捉 &#124;'
- en: '&#124; multi-modality posterior &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态后验 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MCMC &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MCMC &#124;'
- en: '&#124; approximation &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 近似 &#124;'
- en: '&#124; (Neal, [2012](#bib.bib96); Salimans et al., [2015](#bib.bib120); Wolf
    et al., [2016](#bib.bib147)) &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Neal，[2012](#bib.bib96); Salimans 等，[2015](#bib.bib120); Wolf 等，[2016](#bib.bib147))
    &#124;'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Converge to exact posterior. &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 收敛到精确后验。 &#124;'
- en: '&#124; No strict assumption on the &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对 &#124;'
- en: '&#124; distribution form. &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布形式。 &#124;'
- en: '| Hard to converge |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 难以收敛 |'
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MC dropout &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MC dropout &#124;'
- en: '&#124; (Gal and Ghahramani, [2016](#bib.bib39); Guo et al., [2017](#bib.bib47))
    &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Gal 和 Ghahramani，[2016](#bib.bib39); Guo 等，[2017](#bib.bib47)) &#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Simple and compatible to &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 简单且与 &#124;'
- en: '&#124; modern neural network. &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现代神经网络中更具灵活性。 &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lack rigorous &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缺乏严格的 &#124;'
- en: '&#124; theoretical analysis &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理论分析 &#124;'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ensemble Pros: Capture model uncertainty from multiple perspectives. Cons:
    High computational and storage cost. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 集成 优点: 从多个角度捕捉模型不确定性。 缺点: 高计算和存储成本。 |'
- en: '&#124; Network &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '&#124; ensemble (Mallick et al., [2022](#bib.bib86)) &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集成 (Mallick 等，[2022](#bib.bib86)) &#124;'
- en: '|'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture uncertainty from &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 &#124;'
- en: '&#124; the misspecification of &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误规范的 &#124;'
- en: '&#124; model architecture. &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型架构. &#124;'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need to design various &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要设计各种 &#124;'
- en: '&#124; network architecture &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络架构 &#124;'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Bootstrap &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引导 &#124;'
- en: '&#124; ensemble (Lakshminarayanan et al., [2017](#bib.bib73)) &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集成 (Lakshminarayanan 等人, [2017](#bib.bib73)) &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture uncertainty from &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉来自 &#124;'
- en: '&#124; low training dataset. &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低训练数据集. &#124;'
- en: '| Use less dataset for training. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 使用较少的数据集进行训练。 |'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hyper- &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超- &#124;'
- en: '&#124; ensemble (Wenzel et al., [2020](#bib.bib143)) &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集成 (Wenzel 等人, [2020](#bib.bib143)) &#124;'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture uncertainty from &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉来自 &#124;'
- en: '&#124; training hyperparameters. &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练超参数. &#124;'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard to choose suitable &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 难以选择合适的 &#124;'
- en: '&#124; hyperparameters for UQ. &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不确定性量化的超参数. &#124;'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sample-density aware model Pros: Capture dataset shift uncertainty. Cons:
    Hard to ensure certain properties |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 样本密度感知模型 优点：捕捉数据集转移的不确定性。 缺点：难以确保某些特性 |'
- en: '&#124; Deep &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '&#124; Gaussian process &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高斯过程 &#124;'
- en: '&#124; (Wilson et al., [2016](#bib.bib146); You et al., [2017](#bib.bib151);
    Damianou, [2015](#bib.bib25)) &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Wilson 等人, [2016](#bib.bib146); You 等人, [2017](#bib.bib151); Damianou,
    [2015](#bib.bib25)) &#124;'
- en: '|'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Leverage the capability &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用能力 &#124;'
- en: '&#124; of Gaussian process and &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高斯过程和 &#124;'
- en: '&#124; DNN model. &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DNN 模型. &#124;'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The computation complexity &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算复杂性 &#124;'
- en: '&#124; for inference is high. &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理的成本较高. &#124;'
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Distance-aware &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 距离感知 &#124;'
- en: '&#124; DNN &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DNN &#124;'
- en: '&#124; (Liu et al., [2020](#bib.bib80); Van Amersfoort et al., [2020](#bib.bib136);
    van Amersfoort et al., [2021](#bib.bib135)) &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Liu 等人, [2020](#bib.bib80); Van Amersfoort 等人, [2020](#bib.bib136);
    van Amersfoort 等人, [2021](#bib.bib135)) &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Ensure the hidden &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 确保隐藏的 &#124;'
- en: '&#124; feature distance reflect &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征距离反映 &#124;'
- en: '&#124; sample distance in &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 样本距离在 &#124;'
- en: '&#124; the feature space. &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征空间. &#124;'
- en: '|'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard to ensure the &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 难以确保 &#124;'
- en: '&#124; bi-Lipschit constraints. &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; bi-Lipschitz 约束. &#124;'
- en: '|'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Distance-aware neural network: Though the modern neural networks have the capability
    of extracting representative features from large datasets, they are not aware
    of the extent of the distinction of new test samples to the training datasets.
    Thus, to characterize the uncertainty resulting from sample feature density, many
    approaches aim to take the distance awareness between samples into the design
    process of neural network motivated by Gaussian process (Liu et al., [2020](#bib.bib80)).
    Assume the input data manifold is equipped with a metric $||\cdot||_{\mathcal{X}}$,
    which can quantify the distance between samples in the feature space. The intuition
    of a distance-aware neural network is to leverage the feature extraction capability
    of DNN to learn a hidden representation $h(\boldsymbol{x})$ that reflects a meaningful
    distance in the data manifold $||\boldsymbol{x}-\boldsymbol{x^{\prime}}||_{\mathcal{X}}$.
    However, one significant issue with the unconstrainted DNN model is the feature
    collapse, which means DNN feature extraction can map in-distribution data (training
    samples) and out-of-distribution data (lies further to the training data) to similar
    latent representations. Thus the Gaussian process based on the DNN extracted feature
    can be over-confident for those samples that lie further away from training samples.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 距离感知神经网络：尽管现代神经网络具有从大规模数据集中提取代表性特征的能力，但它们并不了解新测试样本与训练数据集之间的区别程度。因此，为了描述由样本特征密度引起的不确定性，许多方法旨在将样本之间的距离感知纳入基于高斯过程的神经网络设计过程中
    (Liu 等人, [2020](#bib.bib80))。假设输入数据流形配备了一个度量 $||\cdot||_{\mathcal{X}}$，可以量化特征空间中样本之间的距离。距离感知神经网络的直觉是利用深度神经网络
    (DNN) 的特征提取能力来学习一个隐藏表示 $h(\boldsymbol{x})$，它反映了数据流形中的有意义距离 $||\boldsymbol{x}-\boldsymbol{x^{\prime}}||_{\mathcal{X}}$。然而，一个显著的问题是无约束的
    DNN 模型中的特征崩溃，这意味着 DNN 特征提取可以将分布内的数据（训练样本）和分布外的数据（距离训练数据更远）映射到类似的潜在表示。因此，基于 DNN
    提取特征的高斯过程可能对那些距离训练样本较远的样本过于自信。
- en: '![Refer to caption](img/5f4fb6bf69cd7b5b906ec56774620b7d.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5f4fb6bf69cd7b5b906ec56774620b7d.png)'
- en: Figure 10\. Illustration on distance-aware neural network feature learning (adapted
    from (Liu et al., [2020](#bib.bib80)))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10. 距离感知神经网络特征学习示意图（改编自 (Liu 等人, [2020](#bib.bib80)））
- en: 'To avoid the feature collapse problem, several constraints have been proposed
    on the models: sensitivity and smoothness (Van Amersfoort et al., [2020](#bib.bib136);
    van Amersfoort et al., [2021](#bib.bib135)). Sensitivity means a small change
    in the input should result in small changes in the feature representation, which
    can ensure distinct samples are mapped to different latent features. Smoothness
    means small changes in the input cannot lead to a dramatic transformation in the
    output. In general, these two constraints can be ensured by the bi-Lipschitz constraints
    (Liu et al., [2022](#bib.bib81)), which means the relative changes in the hidden
    feature representation $h_{\boldsymbol{\theta}}(\boldsymbol{x})$ is bounded by
    the changes in input space as Eq. [13](#S5.E13 "In 5.1.3\. Sample density-aware
    neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") shows.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '为了避免特征崩溃问题，已经对模型提出了几种约束条件：敏感性和平滑性（Van Amersfoort 等，[2020](#bib.bib136)；van
    Amersfoort 等，[2021](#bib.bib135)）。敏感性意味着输入的微小变化应该导致特征表示的微小变化，这可以确保不同样本被映射到不同的潜在特征。平滑性意味着输入的微小变化不能导致输出的剧烈变化。通常，这两个约束可以通过
    bi-Lipschitz 约束来确保（Liu 等，[2022](#bib.bib81)），这意味着隐藏特征表示 $h_{\boldsymbol{\theta}}(\boldsymbol{x})$
    的相对变化受限于输入空间的变化，如公式 [13](#S5.E13 "In 5.1.3\. Sample density-aware neural network
    ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on
    Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty Source’s
    Perspective") 所示。'
- en: '| (13) |  | $L_{1}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}<&#124;h_{\boldsymbol{\theta}}(\boldsymbol{x})-h_{\boldsymbol{\theta}}(\boldsymbol{x}^{\prime})&#124;_{\mathcal{H}}<L_{2}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $L_{1}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}<&#124;h_{\boldsymbol{\theta}}(\boldsymbol{x})-h_{\boldsymbol{\theta}}(\boldsymbol{x}^{\prime})&#124;_{\mathcal{H}}<L_{2}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}$
    |  |'
- en: 'To enforce bi-Lipschitz constraints to DNN, two approaches have been proposed:
    spectral normalization and gradient penalty. Spectral normalization (Liu et al.,
    [2020](#bib.bib80)) claims that the bi-Lipschitz constants $L$ can be ensured
    to be less than one by normalizing the weights matrix in each layer with the spectral
    norm. This method is fast and effective for practice implementation. The other
    approach is called gradient penalty (Van Amersfoort et al., [2020](#bib.bib136)),
    which introduces another loss penalty: the square gradient at each input sample
    $\nabla_{\boldsymbol{x}}^{2}h_{\boldsymbol{\theta}}(\boldsymbol{x})$. This will
    add a soft constraint to the neural networks to constrain the Lipschitz coefficients.
    Compared to spectral normalization, gradient penalty is a soft constraint and
    takes more computation to implement.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制执行 DNN 的 bi-Lipschitz 约束，提出了两种方法：光谱归一化和梯度惩罚。光谱归一化（Liu 等，[2020](#bib.bib80)）声称，通过对每一层的权重矩阵进行光谱范数归一化，可以确保
    bi-Lipschitz 常数 $L$ 小于一。这种方法快速且在实际中有效。另一种方法叫做梯度惩罚（Van Amersfoort 等，[2020](#bib.bib136)），它引入了另一个损失惩罚：每个输入样本的平方梯度
    $\nabla_{\boldsymbol{x}}^{2}h_{\boldsymbol{\theta}}(\boldsymbol{x})$。这将对神经网络添加一个软约束，以限制
    Lipschitz 系数。与光谱归一化相比，梯度惩罚是软约束，需要更多的计算来实现。
- en: 'In summary, we summarize and compare existing works on uncertainty quantification
    arising from model uncertainty. The advantages and disadvantages of each method
    are concluded in Table [1](#S5.T1 "Table 1 ‣ 5.1.3\. Sample density-aware neural
    network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective"). BNN model can capture model uncertainty arising from parameter
    estimation, but usually have high computational costs. The ensemble models can
    capture uncertainty from multiple perspectives, such as model architecture misspecification,
    low training dataset, and hyperparameters. The method also take high computational
    cost. On the other hand, the sample-density aware model can capture dataset shift
    uncertainty, but it’s often hard to learn the distance-aware feature space and
    need to add constraints to the neural network model.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们总结并比较了现有的模型不确定性量化工作的成果。每种方法的优缺点都在表格[1](#S5.T1 "Table 1 ‣ 5.1.3\. Sample
    density-aware neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN
    UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep Neural
    Networks: An Uncertainty Source’s Perspective")中得出结论。BNN模型能够捕捉由参数估计引起的模型不确定性，但通常具有较高的计算成本。集成模型可以从多个角度捕捉不确定性，例如模型架构错误、训练数据集不足和超参数问题。该方法也具有高计算成本。另一方面，样本密度感知模型能够捕捉数据集偏移的不确定性，但通常很难学习到距离感知特征空间，并且需要对神经网络模型添加约束。'
- en: 5.2\. Data Uncertainty
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 数据不确定性
- en: In this section, we discuss the existing methodologies that quantify the data
    uncertainty for DNN models. Generally speaking, data uncertainty is represented
    by the distribution $p(y|\boldsymbol{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$
    is the neural network parameters. To learn this distribution, we categorize the
    approach into deep discriminative models and deep generative models. Deep discriminative
    models are further divided into parametric and non-parametric models based on
    the distribution format. Deep generative models are divided into VAE-based and
    GAN-based models according to the generative frameworks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了量化DNN模型数据不确定性的现有方法。一般来说，数据不确定性由分布$p(y|\boldsymbol{x},\boldsymbol{\theta})$表示，其中$\boldsymbol{\theta}$是神经网络参数。为了学习这个分布，我们将方法分为深度判别模型和深度生成模型。深度判别模型根据分布格式进一步划分为参数模型和非参数模型。深度生成模型根据生成框架分为基于VAE的模型和基于GAN的模型。
- en: 5.2.1\. Deep discriminative model
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 深度判别模型
- en: To quantify the data uncertainty, a discriminative model directly outputs a
    predictive distribution with a neural network. Specifically, the distribution
    can be represented by a parametric or non-parametric model. The parametric model
    assumes the output has an explicit parameterized family of probability distributions
    whose parameters (e.g., mean and variance for Gaussian distribution) are predicted
    with the neural network while the non-parametric model does not have any assumption
    on the underlying distributions. We will discuss existing works for each category
    in detail.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化数据不确定性，判别模型直接使用神经网络输出预测分布。具体来说，该分布可以由参数化或非参数化模型表示。参数模型假设输出具有一个明确参数化的概率分布族，其参数（例如，Gaussian分布的均值和方差）通过神经网络进行预测，而非参数模型则对基础分布没有任何假设。我们将详细讨论每个类别的现有工作。
- en: 'Parametric model: The standard approach for quantifying data uncertainty is
    directly learning a parametric model for $p(y|\boldsymbol{x},\boldsymbol{\theta})$.
    From a frequentist view, there exist a single set of optimum parameters $\boldsymbol{\theta}*$.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数模型：量化数据不确定性的标准方法是直接学习一个参数化模型来表示$p(y|\boldsymbol{x},\boldsymbol{\theta})$。从频率主义的角度来看，存在一组单一的最优参数$\boldsymbol{\theta}*$。
- en: 'For the classification problem, $p(y|\boldsymbol{x},\boldsymbol{\theta})$ is
    a parameterized categorical distribution on the $k$ class, and the distribution
    parameter $\boldsymbol{\pi}=(\pi_{1},...,\pi_{K})$ are predicted from neural network
    output as Eq. [14](#S5.E14 "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data
    Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") shows.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '对于分类问题，$p(y|\boldsymbol{x},\boldsymbol{\theta})$是对$k$类的参数化类别分布，分布参数$\boldsymbol{\pi}=(\pi_{1},...,\pi_{K})$从神经网络输出中预测，如等式[14](#S5.E14
    "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective")所示。'
- en: '| (14) |  | $\footnotesize p(y&#124;\boldsymbol{x},\boldsymbol{\theta})=\text{Categorical}(y;\boldsymbol{\boldsymbol{\pi}})\
    ,\ \boldsymbol{\pi}=f(\boldsymbol{x};\boldsymbol{\theta}),\ \sum_{c=1}^{K}\pi_{c}=1,\
    \pi_{c}>0$ |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\footnotesize p(y\mid\boldsymbol{x},\boldsymbol{\theta})=\text{Categorical}(y;\boldsymbol{\boldsymbol{\pi}})\
    ,\ \boldsymbol{\pi}=f(\boldsymbol{x};\boldsymbol{\theta}),\ \sum_{c=1}^{K}\pi_{c}=1,\
    \pi_{c}>0$ |  |'
- en: In order to obtain the categorical distribution parameters, a straightforward
    method directly utilizes the softmax probability output as $\pi_{i}=\frac{\exp(h_{i}(x;\boldsymbol{\theta}))}{\sum_{c=1}^{k}\exp(h_{c}(x;\boldsymbol{\theta}))}$
    as the predicted to indicate the uncertainty, but these methods tend to be over-confident
    because the softmax operation squeezes the prediction probability toward extreme
    values (zero or one) for the vast majority range of $h_{i}$ (Hendrycks and Gimpel,
    [2016](#bib.bib51)). Following work (Guo et al., [2017](#bib.bib47)) calibrate
    the softmax uncertainty with temperature scaling, which simply add one more hyper-parameter
    $T$ to the softmax calculation as $p=\frac{\exp(h_{i}(x)/T)}{\sum_{c=1}^{k}\exp(h_{c}(x)/T)}$
    to overcome the overconfident outputs. This approach is straightforward to implement,
    but is still faced with the potential to be over-confident due to the lack of
    any constraints, and require post-hoc calibration on the temperature parameter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得类别分布参数，一种直接的方法是将 softmax 概率输出作为 $\pi_{i}=\frac{\exp(h_{i}(x;\boldsymbol{\theta}))}{\sum_{c=1}^{k}\exp(h_{c}(x;\boldsymbol{\theta}))}$
    作为预测值来表示不确定性，但这些方法往往过于自信，因为 softmax 操作将预测概率压缩到极端值（零或一），对于大多数 $h_{i}$ 的范围（Hendrycks
    和 Gimpel，[2016](#bib.bib51)）。后续工作（Guo 等，[2017](#bib.bib47)）通过温度缩放校准了 softmax 不确定性，该方法在
    softmax 计算中简单地添加了一个超参数 $T$，计算公式为 $p=\frac{\exp(h_{i}(x)/T)}{\sum_{c=1}^{k}\exp(h_{c}(x)/T)}$
    以克服过度自信的输出。这种方法实现起来比较简单，但由于缺乏约束，仍然可能面临过度自信的问题，并且需要在温度参数上进行事后校准。
- en: 'For regression problems, data uncertainty is assumed to come from the inherent
    noise (or measurement error, human labeling error) in the training data. In general,
    the training data is modeled as independent additive Gaussian noise with sample-dependent
    variance $\sigma(\boldsymbol{x})$, which indicates the target variable $y_{i}=f_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})+\epsilon(\boldsymbol{x}_{i})$.
    $\epsilon(\boldsymbol{x}_{i})$ is the independent heterogeneous Gaussian noise,
    which represents the uncertainty for each sample. In this way, the output will
    be a parameterized continuous Gaussian distribution, as Eq. [15](#S5.E15 "In 5.2.1\.
    Deep discriminative model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy of DNN UQ
    Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective") shows. Both the mean and variance are predicted
    from the neural network (Kendall and Gal, [2017](#bib.bib67)). The mean represents
    the prediction of the model and the variance represents the uncertainty of the
    sample prediction. To optimize the neural network parameters $\boldsymbol{\theta}$,
    maximize likelihood is performed on the mean and variance jointly as Eq. [15](#S5.E15
    "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") shows. This is also known
    as heteroscedastic regression, which assumes the observational noise level varies
    with different samples. This is suitable for the case where some samples have
    higher noise (uncertainty), while others have lower. Besides Gaussian distribution,
    the neural network can be parameterized with many other kinds of distributions,
    such as mixture Gaussian distribution (Guillaumes, [2017](#bib.bib46)), which
    is implemented with mixture density network (MDN) (Bishop, [1994](#bib.bib11)),
    assuming multiple modes for the prediction. MDN has the advantage that can account
    for the uncertainty from multiple prediction modes but consumes more computation.
    It’s important to choose a suitable parameterized distribution, which depends
    on the nature of the problem.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '对于回归问题，数据不确定性假设来源于训练数据中的固有噪声（或测量误差、人为标注误差）。一般来说，训练数据被建模为独立的加性高斯噪声，其样本相关的方差为$\sigma(\boldsymbol{x})$，这表明目标变量为$y_{i}=f_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})+\epsilon(\boldsymbol{x}_{i})$。$\epsilon(\boldsymbol{x}_{i})$是独立的异质高斯噪声，代表每个样本的不确定性。这样，输出将是一个参数化的连续高斯分布，如
    Eq. [15](#S5.E15 "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data Uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") 所示。均值和方差由神经网络预测（Kendall
    和 Gal，[2017](#bib.bib67)）。均值表示模型的预测，方差表示样本预测的不确定性。为了优化神经网络参数 $\boldsymbol{\theta}$，对均值和方差进行联合最大似然估计，如
    Eq. [15](#S5.E15 "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data Uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") 所示。这也称为异方差回归，它假设观察噪声水平随不同样本而变化。这适用于一些样本具有较高噪声（不确定性），而其他样本则较低的情况。除了高斯分布，神经网络还可以参数化为许多其他种类的分布，例如混合高斯分布（Guillaumes，[2017](#bib.bib46)），该分布通过混合密度网络（MDN）（Bishop，[1994](#bib.bib11)）实现，假设预测具有多个模式。MDN
    的优势在于能够考虑来自多个预测模式的不确定性，但消耗更多计算资源。选择合适的参数化分布很重要，这取决于问题的性质。'
- en: '| (15) |  | $\footnotesize\begin{split}&amp;p(y&#124;\boldsymbol{x},\boldsymbol{\theta})=\mathcal{N}(f_{\boldsymbol{\theta}}(\boldsymbol{x}),\sigma_{\boldsymbol{\theta}}(\boldsymbol{x})),\\
    &amp;\mathcal{L}_{\text{NN}}(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})}^{2}&#124;&#124;y_{i}-f_{\boldsymbol{\theta}}(\boldsymbol{x_{i}})&#124;&#124;^{2}+\frac{1}{2}\log\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})^{2}\end{split}$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\footnotesize\begin{split}&amp;p(y&#124;\boldsymbol{x},\boldsymbol{\theta})=\mathcal{N}(f_{\boldsymbol{\theta}}(\boldsymbol{x}),\sigma_{\boldsymbol{\theta}}(\boldsymbol{x})),\\
    &amp;\mathcal{L}_{\text{NN}}(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})}^{2}&#124;&#124;y_{i}-f_{\boldsymbol{\theta}}(\boldsymbol{x_{i}})&#124;&#124;^{2}+\frac{1}{2}\log\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})^{2}\end{split}$
    |  |'
- en: The advantage of prediction distribution is that it is simple to add the approach
    to existing neural network architecture and requires little modification to the
    training and inference process. However, the explicit parameterization form requires
    choosing the appropriate distribution to accurately capture the underlying uncertainty,
    which can be hard without any prior information on the data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分布的优点在于它很容易将这种方法添加到现有的神经网络架构中，并且对训练和推断过程的修改很少。然而，显式参数化形式需要选择合适的分布来准确捕捉潜在的不确定性，这在没有数据的先验信息的情况下可能很困难。
- en: 'Non-parametric model: Another widely popular approach to indicate the data
    uncertainty is through prediction interval (PI) (Pearce et al., [2018](#bib.bib106);
    Wu et al., [2021](#bib.bib148)). For regression problems, the prediction intervals
    output a lower and upper bound $[y_{l},y_{u}]$, with which we expect the ground
    truth $y$ falls into the interval with a prescribed confidence level, $1-\alpha$,
    meaning that $p(y\in[y_{l},y_{u}])>1-\alpha$. This approach does not require explicit
    distribution over the prediction variable and is more flexible. Traditional prediction
    intervals are constructed in two steps, First step is to learn the point estimation
    of the target variable, which is obtained through the minimization of the error-based
    loss function (i.e., mean square loss), and then estimate the prediction variance
    around the local optimum prediction. The strategy is trying to minimize the prediction
    error, but not optimize the prediction interval quality. One recent paper explicitly
    constructs a lower and upper bound estimation (LUBE) to improve the PI characteristics,
    i.e., the width and coverage probability. The basic intuition is that the PI should
    cover the ground-truth with a certain pre-defined probability (confidence level),
    but should be as narrow as possible. The approach improves the quality of the
    constructed PI, but the new cost function is non-differentiable and requires Simulated
    Annealing (SA) sampling to obtain the optimal NN parameters.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型：另一种广泛使用的方法是通过预测区间（PI）（Pearce et al., [2018](#bib.bib106); Wu et al., [2021](#bib.bib148)）来表示数据的不确定性。对于回归问题，预测区间输出一个下界和上界
    $[y_{l},y_{u}]$，我们期望真实值 $y$ 以规定的置信水平 $1-\alpha$ 落在该区间内，即 $p(y\in[y_{l},y_{u}])>1-\alpha$。这种方法不需要对预测变量进行显式分布，并且更加灵活。传统的预测区间通过两个步骤来构建：第一步是学习目标变量的点估计，通过最小化基于误差的损失函数（即均方误差）获得，然后估计围绕局部最优预测的预测方差。这个策略试图最小化预测误差，但不是优化预测区间的质量。最近一篇论文明确构造了一个下界和上界估计（LUBE）来改进
    PI 特性，即宽度和覆盖概率。基本的直觉是 PI 应该以某个预定义的概率（置信水平）覆盖真实值，但应尽可能窄。这种方法提高了构建的 PI 的质量，但新的成本函数是不可微分的，需要模拟退火（SA）采样来获得最佳的神经网络参数。
- en: '![Refer to caption](img/5760f50591dd23b39280aba770eb6d70.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5760f50591dd23b39280aba770eb6d70.png)'
- en: (a) Prediction distribution example
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预测分布示例
- en: '![Refer to caption](img/bfb653e0e0e6e898a11e8e31bea5978c.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bfb653e0e0e6e898a11e8e31bea5978c.png)'
- en: (b) Prediction interval example
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 预测区间示例
- en: Figure 11\. Neural network architecture for parametric and non-parametric model
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 参数化和非参数模型的神经网络架构
- en: 'To overcome the non-differentiable limitation of LUBE, another approach propose
    a coverage width-based loss function (Pearce et al., [2018](#bib.bib106)) with
    a similar goal as LUBE, shown in the Eq.[16](#S5.E16 "In 5.2.1\. Deep discriminative
    model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective"). The mean prediction interval width (MPIW) is equal to
    $|y_{u}-y_{l}|$, and prediction interval coverage width (PICP) indicates the average
    probability that the PI covers the ground truth. The total loss encourages the
    PI to be as narrow as possible while having a higher coverage probability than
    the prescribed confidence level $\alpha$.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 LUBE 的不可微分限制，另一种方法提出了基于覆盖宽度的损失函数（Pearce et al., [2018](#bib.bib106)），目标与
    LUBE 相似，如公式 [16](#S5.E16 "在 5.2.1\. 深度判别模型 ‣ 5.2\. 数据不确定性 ‣ 5\. DNN UQ 方法的分类 ‣
    深度神经网络不确定性量化方法的调查：不确定性源的视角") 所示。平均预测区间宽度（MPIW）等于 $|y_{u}-y_{l}|$，预测区间覆盖宽度（PICP）表示
    PI 覆盖真实值的平均概率。总损失鼓励 PI 尽可能窄，同时具有比规定的置信水平 $\alpha$ 更高的覆盖概率。
- en: '| (16) |  | $\text{Loss}=\text{MPIW}+\lambda*\max(0,(1-\alpha)-\text{PICP})^{2}$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\text{损失}=\text{MPIW}+\lambda*\max(0,(1-\alpha)-\text{PICP})^{2}$
    |  |'
- en: 'Recent approaches put forward the method of prediction interval learning as
    an empirical constrained optimization problem. There are two different views of
    this optimization problem: primal and dual perspectives. The primal perspective
    views the optimization objective as minimizing the PI intervals under the constraints
    that the PI attains a coverage probability larger than the confidence level (Chen
    et al., [2021b](#bib.bib18)), which is expressed as following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来的方法提出了将预测区间学习作为一种经验约束优化问题的方案。对此优化问题有两种不同的观点：原始视角和对偶视角。原始视角将优化目标视为在保证PI区间覆盖概率大于置信水平的约束下，最小化PI区间（Chen
    et al., [2021b](#bib.bib18)），表达式如下：
- en: '| (17) |  | $\footnotesize\min_{L,U\in\mathcal{H},L<U}\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}(U(\boldsymbol{x})-L(\boldsymbol{x}))\
    s.t.\ p_{\pi}(y\in[L(\boldsymbol{x}),U(\boldsymbol{x})])>1-\alpha$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\footnotesize\min_{L,U\in\mathcal{H},L<U}\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}(U(\boldsymbol{x})-L(\boldsymbol{x}))\
    \text{使得}\ p_{\pi}(y\in[L(\boldsymbol{x}),U(\boldsymbol{x})])>1-\alpha$ |  |'
- en: 'where $\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}$ denotes the expectation
    with respect to the marginal distribution of input samples $\boldsymbol{x}$, and
    $p_{\pi}$ denote the probability of input and output pair distribution. To enforce
    the optimality and feasibility of the optimization problem, the tradeoff is developed
    through the studying of two characteristics of this approach: Lipschitz continuous
    model class (Virmaux and Scaman, [2018](#bib.bib138)) and Vapnik–Chervonenkis
    (VC)-subgraph class (Chen et al., [2021b](#bib.bib18)). On the other hand, the
    dual perspective constructs the learning objective as maximizing the PI coverage
    probability under the fixed global budget constraints (average PI width) in a
    batch setting (Rosenfeld et al., [2018](#bib.bib118)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}$ 表示相对于输入样本 $\boldsymbol{x}$
    的边际分布的期望，$p_{\pi}$ 表示输入和输出对分布的概率。为了确保优化问题的最优性和可行性，通过研究该方法的两个特征：Lipschitz连续模型类（Virmaux和Scaman，[2018](#bib.bib138)）和Vapnik–Chervonenkis
    (VC)-子图类（Chen et al., [2021b](#bib.bib18)），开发了权衡。另一方面，对偶视角将学习目标构建为在批量设置中最大化PI覆盖概率，同时固定全局预算约束（平均PI宽度）（Rosenfeld
    et al., [2018](#bib.bib118)）。
- en: '| (18) |  | $\footnotesize\min_{f\in\mathcal{F}}\mathbb{E}_{(\boldsymbol{x},y)\sim\pi(\boldsymbol{x},y)}L(y,U(\boldsymbol{x}),L(\boldsymbol{x}))\
    s.t.\ \sum_{i}(U(\boldsymbol{x}_{i})-L(\boldsymbol{x})_{i})<B$ |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\footnotesize\min_{f\in\mathcal{F}}\mathbb{E}_{(\boldsymbol{x},y)\sim\pi(\boldsymbol{x},y)}L(y,U(\boldsymbol{x}),L(\boldsymbol{x}))\
    \text{使得}\ \sum_{i}(U(\boldsymbol{x}_{i})-L(\boldsymbol{x})_{i})<B$ |  |'
- en: They presented a discriminative learning framework that optimizes the expected
    error rate under a budget constraint on the interval sizes. In this way, the approach
    avoids single-point loss and can provide a statistical guarantee of the generalization
    capability on the whole population. Compared with the primal setup, the dual perspective
    in a batch learning can construct the prediction interval of a group of test points
    at the same time and avoid the computational overhead in the primal problem.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了一个判别学习框架，该框架在区间大小的预算约束下优化期望错误率。通过这种方式，该方法避免了单点损失，并能够提供整个数据集的泛化能力的统计保证。与原始设置相比，对偶视角的批量学习可以同时构建一组测试点的预测区间，并避免了原始问题中的计算开销。
- en: 5.2.2\. Deep Generative Model
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 深度生成模型
- en: The deep generative model (DGM) is a family of probabilistic models that aim
    to learn the complicated, high-dimensional data distribution $p_{\text{data}}(\boldsymbol{x})$
    with DNN. DGMs are capable of learning the intractable data distribution in the
    high-dimensional feature space $\mathcal{X}\in\mathbb{R}^{n}$ from a large number
    of independent and identical observed samples $\{\boldsymbol{x}_{i}\}_{i=1}^{m}$.
    Specifically, they learn a probabilistic mapping from some latent variables $\boldsymbol{z}\in\mathbb{R}^{d}$
    which following tractable distribution to the data distribution, e.g., $\mathcal{N}(\mathbf{0},\mathbf{I})$
    to the data distribution $p_{\text{data}}(\boldsymbol{x})$. Mathematically, the
    generative model can be defined as a mapping function $g_{\boldsymbol{\theta}}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}$,
    where $d$ and $n$ are the dimensions of latent variable and original data, respectively.
    The ability of the deep generative model to learn an intractable distribution
    makes it useful for uncertainty quantification.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 深度生成模型（DGM）是一类概率模型，旨在利用 DNN 学习复杂的高维数据分布 $p_{\text{data}}(\boldsymbol{x})$。DGM
    能够从大量独立且相同的观察样本 $\{\boldsymbol{x}_{i}\}_{i=1}^{m}$ 中学习高维特征空间 $\mathcal{X}\in\mathbb{R}^{n}$
    中难以处理的数据分布。具体而言，它们学习从一些潜变量 $\boldsymbol{z}\in\mathbb{R}^{d}$（遵循可处理分布，如 $\mathcal{N}(\mathbf{0},\mathbf{I})$）到数据分布
    $p_{\text{data}}(\boldsymbol{x})$ 的概率映射。数学上，生成模型可以定义为一个映射函数 $g_{\boldsymbol{\theta}}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}$，其中
    $d$ 和 $n$ 分别是潜变量和原始数据的维度。深度生成模型学习难以处理的分布的能力使其在不确定性量化中非常有用。
- en: The basic idea is to employ DGM to learn the predictive distribution $p(y|\boldsymbol{x})$
    given the supervised training data pairs $\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{m}$.
    It should be noted that if we aim to learn the predictive distribution instead
    of the data distribution in feature space, the conditional deep generative model
    (cDGM) (Sohn et al., [2015](#bib.bib127)) should be employed. Generally speaking,
    cDGM-based uncertainty quantification models learn a conditional density over
    the prediction $y$, given the input feature $\boldsymbol{x}$. This amounts to
    learn a model $g_{\boldsymbol{\theta}}(\boldsymbol{z},\cdot):\mathbb{X}\rightarrow\mathbb{Y}$
    such that the generative model $g(\boldsymbol{z},\boldsymbol{x})$ with $\boldsymbol{z}\sim
    p(\boldsymbol{z})$ is approximately distributed as the true unknown distribution
    $p_{\text{true}}(y|\boldsymbol{x})$. The variability of the prediction distribution
    $p(y|\boldsymbol{x})$ is encoded into the latent variable $\boldsymbol{z}$ and
    the generative model. During inference, for any $\boldsymbol{x}\in\mathbb{X}$,
    we can generate $m$ samples with $y_{i}=g_{\boldsymbol{\theta}}(\boldsymbol{z}_{i},\boldsymbol{x})$
    and $\boldsymbol{z}_{i}\sim p(\boldsymbol{z})$ and from the samples $\{y_{i}\}_{i=1}^{m}$
    the variability we can quantify the prediction uncertainty.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是利用 DGM 学习给定监督训练数据对 $\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{m}$ 的预测分布 $p(y|\boldsymbol{x})$。需要注意的是，如果我们旨在学习预测分布而不是特征空间中的数据分布，应使用条件深度生成模型（cDGM）（Sohn
    等，[2015](#bib.bib127)）。一般而言，基于 cDGM 的不确定性量化模型学习在输入特征 $\boldsymbol{x}$ 给定下的预测 $y$
    的条件密度。这相当于学习一个模型 $g_{\boldsymbol{\theta}}(\boldsymbol{z},\cdot):\mathbb{X}\rightarrow\mathbb{Y}$，使得生成模型
    $g(\boldsymbol{z},\boldsymbol{x})$ 在 $\boldsymbol{z}\sim p(\boldsymbol{z})$ 下近似分布为真实的未知分布
    $p_{\text{true}}(y|\boldsymbol{x})$。预测分布 $p(y|\boldsymbol{x})$ 的变异性被编码到潜变量 $\boldsymbol{z}$
    和生成模型中。在推断过程中，对于任何 $\boldsymbol{x}\in\mathbb{X}$，我们可以生成 $m$ 个样本，具有 $y_{i}=g_{\boldsymbol{\theta}}(\boldsymbol{z}_{i},\boldsymbol{x})$
    和 $\boldsymbol{z}_{i}\sim p(\boldsymbol{z})$，并从样本 $\{y_{i}\}_{i=1}^{m}$ 中量化预测不确定性。
- en: 'In the following subsection, we consider two types of deep generative models:
    variational auto-encoder (VAE) (Kingma and Welling, [2013](#bib.bib69)) and generative
    adversarial network (GAN) (Goodfellow et al., [2020](#bib.bib44)). VAE belongs
    to the likelihood-based generative model and is trained via maximizing evidence
    lower bound (ELBO) of the likelihood function. The GAN model is an implicit generative
    model trained with a two-player zero-game approach. We will consider how the two
    frameworks could be utilized for prediction uncertainty estimation.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们考虑两种类型的深度生成模型：变分自编码器（VAE）（Kingma 和 Welling，[2013](#bib.bib69)）和生成对抗网络（GAN）（Goodfellow
    等，[2020](#bib.bib44)）。VAE 属于基于似然的生成模型，通过最大化似然函数的证据下界（ELBO）进行训练。GAN 模型是一种隐式生成模型，通过双玩家零和游戏方法进行训练。我们将探讨这两种框架如何用于预测不确定性估计。
- en: 'VAE-based model: VAE model consists of two modules: an encoder and a decoder.
    The encoder network $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})$ aims to embed the
    high dimensional structural output $\boldsymbol{x}$ into a low-dimensional code
    $\boldsymbol{z}$, that capture the inherent ambiguity or noise of the input data.
    The decoder $p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z})$ aim to reconstruct
    the input feature. VAE model has been popular for structured output uncertainty,
    especially for the tasks on image data, because of their capability to model the
    global and local structure dependency in regular grid images. Specifically, two
    kinds of frameworks based on the VAE model have been proposed to account for the
    data uncertainty coming from input noise and target output noise: The first category
    aims to capture the noise that lies in the input samples. The basic idea is to
    embed each sample as a Gaussian distribution instead of a deterministic embedding
    in the low dimensional latent space, where the mean represents the feature embedding
    and variance represents the uncertainty of the embedding (Chang et al., [2020](#bib.bib16)).
    The method takes into the different noise levels inherent in the dataset, which
    is ubiquitous in many kinds of real-world datasets, i.e., face image recognition
    (Chang et al., [2020](#bib.bib16)), medical image reconstruction (Edupuganti et al.,
    [2020](#bib.bib34)). The probabilistic embedding framework takes advantage of
    the VAE model architecture to estimate the embedding and uncertainty simultaneously.
    The second category aims to capture the noise that lies in the target outputs,
    which also means the ground-truth is imperfect, ambiguous, or corrupted. This
    scenario is common in the medical domain (Lee et al., [2020](#bib.bib75)), where
    the objects in the image are ambiguous and the experts may not reach a consensus
    on the class of the objects (large uncertainty). Thus for segmentation or classification
    tasks, the model should be aware of the prediction uncertainty. To capture the
    prediction uncertainty in the target outputs, the conditional VAE (cVAE) (Sohn
    et al., [2015](#bib.bib127))framework is adopted. Specifically, cVAE formulates
    the prediction distribution as an integration over the latent embedding $\boldsymbol{z}$,'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于VAE的模型：VAE模型由两个模块组成：编码器和解码器。编码器网络 $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})$
    旨在将高维结构输出 $\boldsymbol{x}$ 嵌入到低维代码 $\boldsymbol{z}$ 中，以捕捉输入数据的固有模糊性或噪声。解码器 $p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z})$
    旨在重建输入特征。VAE模型在结构化输出不确定性方面非常受欢迎，尤其是在图像数据任务中，因为它们能够建模规则网格图像中的全局和局部结构依赖性。具体而言，已经提出了两种基于VAE模型的框架来考虑输入噪声和目标输出噪声带来的数据不确定性：第一类旨在捕捉输入样本中的噪声。基本思想是将每个样本嵌入为高斯分布，而不是低维潜在空间中的确定性嵌入，其中均值代表特征嵌入，方差代表嵌入的不确定性（Chang
    et al., [2020](#bib.bib16)）。该方法考虑了数据集中固有的不同噪声水平，这在许多现实世界的数据集中是普遍存在的，即面部图像识别（Chang
    et al., [2020](#bib.bib16)）、医学图像重建（Edupuganti et al., [2020](#bib.bib34)）。概率嵌入框架利用VAE模型架构同时估计嵌入和不确定性。第二类旨在捕捉目标输出中的噪声，这也意味着真实值是不完美的、模糊的或被破坏的。这种情况在医学领域很常见（Lee
    et al., [2020](#bib.bib75)），其中图像中的物体模糊，专家可能无法就物体的类别达成共识（大不确定性）。因此，对于分割或分类任务，模型应注意预测不确定性。为了捕捉目标输出中的预测不确定性，采用了条件VAE（cVAE）（Sohn
    et al., [2015](#bib.bib127)）框架。具体而言，cVAE将预测分布表示为对潜在嵌入 $\boldsymbol{z}$ 的积分，
- en: '| (19) |  | $\footnotesize p(y&#124;\boldsymbol{x})=\int p(y&#124;\boldsymbol{x},\boldsymbol{z})p(z&#124;\boldsymbol{x})d\boldsymbol{z}\approx\sum_{j=1}^{n}p(y&#124;\boldsymbol{x},\boldsymbol{z}_{j}),\
    \text{where}\ z_{j}\sim p(z)$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\footnotesize p(y&#124;\boldsymbol{x})=\int p(y&#124;\boldsymbol{x},\boldsymbol{z})p(z&#124;\boldsymbol{x})d\boldsymbol{z}\approx\sum_{j=1}^{n}p(y&#124;\boldsymbol{x},\boldsymbol{z}_{j}),\
    \text{where}\ z_{j}\sim p(z)$ |  |'
- en: The cVAE model is trained by maximizing the evidence lower bound of the likelihood.
    Then during inference, multiple latent feature $\boldsymbol{z}_{j}$ can be drawn
    from the prior distribution and the integration over latent $\boldsymbol{z}$ can
    be approximated with the sampling distribution (Prokudin et al., [2018](#bib.bib110)).
    Probabilistic U-Net model (Kohl et al., [2018](#bib.bib70)) combines the architecture
    of cVAE and U-Net model by treating the U-Net model as the encoder to produce
    a probabilistic segmentation map. The U-Net model can capture multi-scale feature
    representations in the low-dimensional latent space to encode the potential variability
    in the segmentation map.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: cVAE模型通过最大化似然的证据下界来训练。然后在推断过程中，可以从先验分布中抽取多个潜在特征$\boldsymbol{z}_{j}$，并且通过采样分布来近似潜在$\boldsymbol{z}$的积分（Prokudin
    et al., [2018](#bib.bib110)）。概率U-Net模型（Kohl et al., [2018](#bib.bib70)）通过将U-Net模型视为编码器来生成概率分割图，从而结合了cVAE和U-Net模型的架构。U-Net模型能够捕捉低维潜在空间中的多尺度特征表示，以编码分割图中的潜在变异性。
- en: In summary, the VAE-based framework can take into consideration the data uncertainty
    coming from the input noise or the target output noise and can adapt the current
    state-of-the-art neural network architecture into its framework, making it more
    flexible for many kinds of applications. The key success lies in modeling the
    joint probability of all samples (pixels) in the image. The approach is suitable
    for structured uncertainty quantification (e.g., image grid structure, graph structure,
    et al.) through learning the implicit joint distribution on the structure.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于VAE的框架可以考虑来自输入噪声或目标输出噪声的数据不确定性，并且可以将当前最先进的神经网络架构适配到其框架中，使其对多种应用更具灵活性。成功的关键在于建模图像中所有样本（像素）的联合概率。该方法适用于结构化不确定性量化（例如，图像网格结构、图结构等），通过学习结构上的隐式联合分布。
- en: '![Refer to caption](img/f0ff7b33bc502c9e1dc10e715e2060cb.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f0ff7b33bc502c9e1dc10e715e2060cb.png)'
- en: (a) Input uncertainty
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 输入不确定性
- en: '![Refer to caption](img/817193aca9449ba3416ce53f5e746791.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/817193aca9449ba3416ce53f5e746791.png)'
- en: (b) Target prediction uncertainty
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 目标预测不确定性
- en: Figure 12\. VAE framework for uncertainty quantification
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12. 用于不确定性量化的VAE框架
- en: 'GAN-based generative model: GAN is a type of generative model trained with
    a two-player zero-game. It consists of a generator and a discriminator. In conditional
    GAN (cGAN), the generator takes the input $x$ and random noise $\boldsymbol{z}$
    as input, and generates the target variables $y$: $\mathcal{G}:(\boldsymbol{x},\boldsymbol{z})\rightarrow
    y$. The discriminator is trained to distinguish the generated samples and ground-truth
    samples. The idea has been adopted in many domains. For example, in transportation
    domain, they adpot GAN-based approach for the prediction of traffic vollume (Mo
    and Fu, [2022](#bib.bib91)). the flow model is integrated into GAN to enable likelihood
    estimation and better uncertainty quantification. Another approach (Oberdiek et al.,
    [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40)) further extends the method
    by utilizing the Wasserstein GAN (Arjovsky et al., [2017](#bib.bib5)) based on
    gradient penalty to improve model convergence. The key advantage of deep generative
    modeling for uncertainty quantification is that they directly parameterize a deep
    neural network to represent the prediction distribution, without the need to have
    an explicit distribution format. Moreover, it can integrate a physics-informed
    neural network for better uncertainty estimation of physical science (Daw et al.,
    [2021](#bib.bib27)). However, the disadvantage is that deep generative models
    are much harder to train, especially for GAN-based models. The convergence of
    the model learning is not guaranteed.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GAN的生成模型：GAN是一种通过双人零和游戏训练的生成模型。它由一个生成器和一个判别器组成。在条件GAN（cGAN）中，生成器以输入 $x$ 和随机噪声
    $\boldsymbol{z}$ 为输入，生成目标变量 $y$：$\mathcal{G}:(\boldsymbol{x},\boldsymbol{z})\rightarrow
    y$。判别器的训练目标是区分生成的样本和真实样本。这一思想已被应用于许多领域。例如，在交通领域，他们采用基于GAN的方法预测交通流量（Mo 和 Fu，[2022](#bib.bib91)）。流模型被集成到GAN中，以实现似然估计和更好的不确定性量化。另一种方法（Oberdiek
    等，[2022](#bib.bib100)；Gao 和 Ng，[2022](#bib.bib40)）进一步扩展了该方法，利用基于梯度惩罚的Wasserstein
    GAN（Arjovsky 等，[2017](#bib.bib5)）以改进模型收敛。深度生成建模用于不确定性量化的关键优点在于它们直接参数化深度神经网络以表示预测分布，而不需要显式的分布格式。此外，它可以集成物理信息神经网络，以更好地估计物理科学中的不确定性（Daw
    等，[2021](#bib.bib27)）。然而，缺点是深度生成模型的训练难度更大，特别是对于基于GAN的模型。模型学习的收敛性无法保证。
- en: Table 2\. Data uncertainty methods comparison
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 数据不确定性方法比较
- en: '| Model | Approach | Pros | Cons |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Deep Discriminative model Pros: compatible to existing network architecture.
    Cons: Hard for structured uncertainty |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 深度判别模型 优点：兼容现有网络架构。 缺点：对结构化不确定性较难处理 |'
- en: '&#124; Parametric: predictive &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 参数化：预测&#124;'
- en: '&#124; distribution (Kendall and Gal, [2017](#bib.bib67); Guillaumes, [2017](#bib.bib46))
    &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布（Kendall 和 Gal，[2017](#bib.bib67)；Guillaumes，[2017](#bib.bib46)）&#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Can leverage &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可利用&#124;'
- en: '&#124; existing DNN &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现有的DNN&#124;'
- en: '&#124; training framework. &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练框架。&#124;'
- en: '|'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need to choose &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要选择&#124;'
- en: '&#124; appropriate parametric &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适当的参数化&#124;'
- en: '&#124; distribution. &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布。&#124;'
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Non-parametric: &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非参数化：&#124;'
- en: '&#124; predictive interval &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预测区间&#124;'
- en: '&#124; (Pearce et al., [2018](#bib.bib106); Wu et al., [2021](#bib.bib148);
    Rosenfeld et al., [2018](#bib.bib118); Chen et al., [2021b](#bib.bib18)) &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （Pearce 等，[2018](#bib.bib106)；Wu 等，[2021](#bib.bib148)；Rosenfeld 等，[2018](#bib.bib118)；Chen
    等，[2021b](#bib.bib18)）&#124;'
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; No need to &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无需&#124;'
- en: '&#124; choose specific &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择特定的&#124;'
- en: '&#124; distribution format. &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布格式。&#124;'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need to design new &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要设计新的&#124;'
- en: '&#124; loss/training strategy. &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 损失/训练策略。&#124;'
- en: '|'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deep Generative model Pros: Capture structured uncertainty. Cons: Require
    modification to existing architecture and hard to train |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 深度生成模型 优点：捕捉结构化不确定性。 缺点：需要对现有架构进行修改且训练困难 |'
- en: '&#124; VAE-based model &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于VAE的模型&#124;'
- en: '&#124; (Chang et al., [2020](#bib.bib16); Edupuganti et al., [2020](#bib.bib34);
    Prokudin et al., [2018](#bib.bib110); Kohl et al., [2018](#bib.bib70)) &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （Chang 等，[2020](#bib.bib16)；Edupuganti 等，[2020](#bib.bib34)；Prokudin
    等，[2018](#bib.bib110)；Kohl 等，[2018](#bib.bib70)）&#124;'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture the &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉&#124;'
- en: '&#124; structured data &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结构化数据&#124;'
- en: '&#124; uncertainty from &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自&#124;'
- en: '&#124; the input noise &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入噪声&#124;'
- en: '&#124; and ambiguous label. &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和模糊标签。&#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Need to modify neural &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要修改神经&#124;'
- en: '&#124; network architecture &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络结构 &#124;'
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GAN-based model &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于GAN的模型 &#124;'
- en: '&#124; (Oberdiek et al., [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40);
    Mo and Fu, [2022](#bib.bib91)) &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Oberdiek 等，[2022](#bib.bib100)；Gao 和 Ng，[2022](#bib.bib40)；Mo 和 Fu，[2022](#bib.bib91))
    &#124;'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture the structure &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕获结构 &#124;'
- en: '&#124; data uncertainty. &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据不确定性。&#124;'
- en: '|'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GAN model is hard &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN模型难以 &#124;'
- en: '&#124; to train. &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 进行训练。&#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.3\. Model and data uncertainty
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 模型和数据不确定性
- en: Besides considering the data and model uncertainty separately, many frameworks
    attempt to jointly consider the two kinds of uncertainty for more accurate quantification.
    In this part, we will review existing that aims to quantify two types of uncertainty
    simultaneously.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分别考虑数据和模型的不确定性外，许多框架试图将这两种不确定性联合考虑，以实现更准确的量化。在这一部分，我们将回顾旨在同时量化这两种不确定性的现有方法。
- en: 5.3.1\. Approaches combining data and model uncertainty
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 结合数据和模型不确定性的方法
- en: A straightforward way to consider both data and model uncertainty is to select
    one of the approaches in each category and combine them in a single framework.
    Below we will introduce some major ways to combine the approach of data and model
    uncertainty and their potential drawbacks.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接考虑数据和模型不确定性的方法是选择每个类别中的一种方法，并将它们结合在一个框架中。下面我们将介绍一些主要的结合数据和模型不确定性的方法及其潜在的缺点。
- en: 'Combine BNN model with prediction distribution: The method aim to capture both
    data and model uncertainty in a single framework (Kendall and Gal, [2017](#bib.bib67))
    by combining the BNN and prediction distribution. The model uncertainty is captured
    with the BNN approximation approach. Specifically, MC drop-out is adopted due
    to its simplicity for implementation. For each dropout forward pass, one sample
    of the weight is drawn from the weight distribution approximation $\mathbf{W}_{t}\sim\text{Bernoulli}(p)$,
    where $p$ is the dropout rate, then one forward prediction can be made with the
    weight by $y_{t}=p(y|\boldsymbol{x},\mathbf{W}_{t})$. To obtain the data uncertainty,
    the output is formulated as a parameterized Gaussian distribution instead of point
    estimation $[y_{t},\sigma_{t}^{2}]=p(y|\boldsymbol{x};\mathbf{W}_{t})$, where
    $y_{t}$ is the target variable mean prediction and $\sigma_{t}^{2}$ is the prediction
    variance for a single forward prediction. With multiple dropout forward passes,
    we have a set of $T$ prediction samples $\{y_{t},\sigma^{2}_{t}\}_{t=1}^{T}$.
    The predictive uncertainty in the combined model can be approximated with the
    law of total variance expressed as $\text{Var}(y)$ in Eq. [20](#S5.E20 "In 5.3.1\.
    Approaches combining data and model uncertainty ‣ 5.3\. Model and data uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"). The intuition
    of this equation is that the total uncertainty comes from two parts, the last
    $\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$ represents the average data uncertainty,
    and the first part $\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}$
    represent the disagreement of $T$ MC-dropout models, which capture the model uncertainty.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 结合BNN模型与预测分布：该方法旨在通过将BNN和预测分布结合在一个框架中来捕捉数据和模型的不确定性（Kendall和Gal，[2017](#bib.bib67)）。模型不确定性通过BNN近似方法来捕获。具体而言，由于其实现简单，采用了MC
    dropout。对于每个dropout前向传播，从权重分布近似 $\mathbf{W}_{t}\sim\text{Bernoulli}(p)$ 中抽取一个权重样本，其中
    $p$ 是dropout率，然后用该权重进行前向预测 $y_{t}=p(y|\boldsymbol{x},\mathbf{W}_{t})$。为了获得数据不确定性，输出被表述为参数化的高斯分布，而不是点估计
    $[y_{t},\sigma_{t}^{2}]=p(y|\boldsymbol{x};\mathbf{W}_{t})$，其中 $y_{t}$ 是目标变量的均值预测，$\sigma_{t}^{2}$
    是单次前向预测的方差。通过多次dropout前向传播，我们得到了 $T$ 个预测样本 $\{y_{t},\sigma^{2}_{t}\}_{t=1}^{T}$。结合模型中的预测不确定性可以通过总方差定律来近似，表示为
    $\text{Var}(y)$，如公式 [20](#S5.E20 "在5.3.1\. 结合数据和模型不确定性的方法 ‣ 5.3\. 模型和数据不确定性 ‣
    5\. DNN UQ方法的分类 ‣ 关于深度神经网络不确定性量化方法的调查：从不确定性源的角度")。这个方程的直观理解是，总不确定性来自两个部分，最后的 $\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$
    代表平均数据不确定性，第一部分 $\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}$
    代表 $T$ 个MC-dropout模型之间的分歧，捕获了模型不确定性。
- en: '| (20) |  | $\text{Var}(y)\approx\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}+\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\text{Var}(y)\approx\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}+\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$
    |  |'
- en: 'Combine ensemble model with prediction distribution: This approach (Lakshminarayanan
    et al., [2017](#bib.bib73)) combines the ensemble method with prediction distribution.
    The deep ensemble method constructs an ensemble of DNN models $\mathcal{M}=\{\mathcal{M}_{i}\}_{i=1}^{K}$,
    where each model $\mathcal{M}_{i}$ can be set with different parameters, or architectures
    choices, et, al. The model uncertainty is expressed as the variance or ”disagreement”
    of the ensemble. In this way, the output of each model is modified as parameterized
    distribution to capture the data uncertainty. Similar to MC-dropout, we have an
    ensemble of prediction distribution $\{p(y|\boldsymbol{x},\mathcal{M}_{i})\}_{i=1}^{K}$.
    In this part, we take the classification problem as an example, where the prediction
    distribution is parameterized categorical distribution. The total uncertainty
    is captured with the entropy of average prediction distribution $\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y|\boldsymbol{x},\mathcal{M}_{i}))$,
    and the data uncertainty is the average entropy of each model, expressed as $\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y|\boldsymbol{x},\mathcal{M}_{i}))$.
    The model uncertainty can be expressed with the mutual information between the
    prediction and the ensemble model $y,\mathcal{M}$ as expressed in Eq. [21](#S5.E21
    "In 5.3.1\. Approaches combining data and model uncertainty ‣ 5.3\. Model and
    data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective").'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 将集成模型与预测分布结合：这种方法（Lakshminarayanan 等，[2017](#bib.bib73)）将集成方法与预测分布相结合。深度集成方法构建了一个
    DNN 模型的集成 $\mathcal{M}=\{\mathcal{M}_{i}\}_{i=1}^{K}$，其中每个模型 $\mathcal{M}_{i}$
    可以设置不同的参数或架构选择等。模型不确定性表示为集成的方差或“分歧”。这样，每个模型的输出被修改为参数化分布以捕获数据不确定性。类似于 MC-dropout，我们有一个预测分布的集成
    $\{p(y\mid\boldsymbol{x},\mathcal{M}_{i})\}_{i=1}^{K}$。在这一部分，我们以分类问题为例，其中预测分布是参数化的类别分布。总的不确定性通过平均预测分布的熵
    $\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y\mid\boldsymbol{x},\mathcal{M}_{i}))$
    来捕获，而数据不确定性是每个模型的平均熵，表示为 $\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y\mid\boldsymbol{x},\mathcal{M}_{i}))$。模型不确定性可以用预测与集成模型之间的互信息来表达，即
    Eq. [21](#S5.E21 "在 5.3.1\. 结合数据与模型不确定性的方法 ‣ 5.3\. 模型与数据不确定性 ‣ 5\. 深度神经网络不确定性量化方法分类
    ‣ 深度神经网络不确定性量化方法的综述：不确定性来源的视角")。
- en: '| (21) |  | $\text{MI}(y,\mathcal{M})=\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y&#124;\boldsymbol{x},\mathcal{M}_{i}))-\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y&#124;\boldsymbol{x},\mathcal{M}_{i}))$
    |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\text{MI}(y,\mathcal{M})=\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y\mid\boldsymbol{x},\mathcal{M}_{i}))-\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y\mid\boldsymbol{x},\mathcal{M}_{i}))$
    |  |'
- en: 'Combine ensemble model with prediction interval: Since the prediction interval
    constructed in some approaches accounts only for the data noise variance, not
    the model uncertainty. To improve the total uncertainty estimation, ensemble methods
    are adopted to combine with prediction interval to account for the model uncertainty
    that comes from model architectures misspecification, parameter initialization,
    et.al. (Pearce et al., [2018](#bib.bib106)). Specifically, Given an ensemble of
    models trained with different model specifications or sub-sampling of training
    datasets, where the model prediction intervals are denoted as $[y^{ij}_{l},y^{ij}_{u}]$
    for sample $i=\{1,...,n\}$ and model $j=\{1,...,m\}$, the model uncertainty can
    be captured by the variance of the lower bound $\sigma_{l}^{(i)^{2}}$ and upper
    bound variance $\sigma_{u}^{(i)^{2}}$. For example, the lower bound uncertainty
    is:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 将集成模型与预测区间结合：由于某些方法构建的预测区间仅考虑了数据噪声方差，而未考虑模型不确定性。为了提高总的不确定性估计，采用了集成方法与预测区间相结合，以考虑来自模型结构误设、参数初始化等的模型不确定性（Pearce
    等，[2018](#bib.bib106)）。具体来说，给定一个使用不同模型规格或训练数据集的子采样训练的模型集成，其中模型预测区间记作 $[y^{ij}_{l},y^{ij}_{u}]$，对于样本
    $i=\{1,...,n\}$ 和模型 $j=\{1,...,m\}$，模型不确定性可以通过下限方差 $\sigma_{l}^{(i)^{2}}$ 和上限方差
    $\sigma_{u}^{(i)^{2}}$ 来捕获。例如，下限不确定性为：
- en: '| (22) |  | $\begin{split}&amp;\sigma_{l}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{l}^{(ij)}-\hat{y}_{l}^{(i)})^{2},\
    \text{where}\ \hat{y}_{l}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{l}^{(ij)}\\ &amp;\sigma_{u}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{u}^{(ij)}-\hat{y}_{u}^{(i)})^{2},\
    \text{where}\ \hat{y}_{u}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{u}^{(ij)}\end{split}$
    |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\begin{split}&amp;\sigma_{l}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{l}^{(ij)}-\hat{y}_{l}^{(i)})^{2},\
    \text{其中}\ \hat{y}_{l}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{l}^{(ij)}\\ &amp;\sigma_{u}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{u}^{(ij)}-\hat{y}_{u}^{(i)})^{2},\
    \text{其中}\ \hat{y}_{u}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{u}^{(ij)}\end{split}$
    |  |'
- en: 'Then the new prediction interval $[\tilde{y}_{l},\tilde{y}_{u}]$ with $95\%$
    confidence level can be constructed as:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，新的预测区间 $[\tilde{y}_{l},\tilde{y}_{u}]$ 在 $95\%$ 置信水平下可以构造为：
- en: '| (23) |  | $\begin{split}&amp;\tilde{y}_{l}=y_{l}-1.96\sigma_{l}^{(i)^{2}},\
    \text{and}\ \tilde{y}_{u}=y_{u}+1.96\sigma_{u}^{(i)^{2}}\end{split}$ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\begin{split}&amp;\tilde{y}_{l}=y_{l}-1.96\sigma_{l}^{(i)^{2}},\
    \text{和}\ \tilde{y}_{u}=y_{u}+1.96\sigma_{u}^{(i)^{2}}\end{split}$ |  |'
- en: The constructed interval can reflect both the data uncertainty and model uncertainty,
    but the limitation is that the model uncertainty is simply constructed from the
    variance of the lower and upper bound of the ensemble, which lacks theoretical
    justification because of the independent consideration of two boundaries. To overcome
    this limitation, one recent approach proposes a split normal aggregation method
    to aggregate the prediction interval ensembles into final intervals (Salem et al.,
    [2020](#bib.bib119)). Specifically, the method fits a split normal distribution
    (two pieces of normal distribution) over each prediction interval, and then the
    final prediction will become a mixture of split normal distribution. The PI can
    be derived from the $1-\alpha$ quantile of the cumulative distribution.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 构造的区间可以反映数据不确定性和模型不确定性，但其局限性在于模型不确定性仅仅是从集成的上下界的方差中构造的，这因为独立考虑两个边界而缺乏理论依据。为了克服这一局限性，最近一种方法提出了分裂正态聚合方法，将预测区间集成到最终区间中（Salem
    等，[2020](#bib.bib119)）。具体而言，该方法将分裂正态分布（两个正态分布）拟合到每个预测区间上，然后最终预测将成为分裂正态分布的混合体。预测区间可以从累计分布的
    $1-\alpha$ 分位数中得出。
- en: 'In conclusion, to capture both the data and model uncertainty, existing literature
    can combine the methodologies in the two categories. There are several limitations
    to the combination approaches: first, the BNN or ensemble models require multiple
    forward passes for the prediction, which introduces computation overhead and extra
    storage. Efficiency is a concern. Second, the simple combination of data and model
    uncertainty lack a theoretical guarantee, which requires post-hoc calibration
    on the model.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，为了同时捕捉数据和模型的不确定性，现有文献可以结合这两类方法。然而，这种组合方法存在几个限制：首先，BNN或集成模型需要多次前向传递来进行预测，这引入了计算开销和额外的存储需求，效率成为一个问题。其次，数据和模型不确定性的简单组合缺乏理论保证，这需要对模型进行事后校准。
- en: 5.3.2\. Evidential deep learning
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 证据深度学习
- en: '![Refer to caption](img/20c1d1330c1d8b23344cafb93baba9bf.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/20c1d1330c1d8b23344cafb93baba9bf.png)'
- en: Figure 13\. Evidential deep learning architecture
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 证据深度学习架构
- en: To overcome the computational challenge for the combination approaches, evidential
    deep learning was proposed to use one single deterministic model to capture both
    the data and model uncertainty without multiple forward passes of the neural network
    (Malinin and Gales, [2018](#bib.bib85); Charpentier et al., [2020](#bib.bib17);
    Sensoy et al., [2018](#bib.bib122); Bao et al., [2021](#bib.bib7)). The intuition
    of evidential deep learning is to predict class-wise evidence instead of directly
    predicting class probabilities. In the following we review the methodologies,
    the advantages and disadvantages of those methods.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服组合方法的计算挑战，提出了证据深度学习，它使用一个确定性模型来捕捉数据和模型的不确定性，而无需对神经网络进行多次前向传递（Malinin 和 Gales，[2018](#bib.bib85)；Charpentier
    等，[2020](#bib.bib17)；Sensoy 等，[2018](#bib.bib122)；Bao 等，[2021](#bib.bib7)）。证据深度学习的直觉是预测类别证据，而不是直接预测类别概率。接下来，我们回顾这些方法的技术、优点和缺点。
- en: 'As discussed in the aforementioned sections, for classification problems, existing
    deep learning-based models explicitly or implicitly predict class probabilities
    (categorical distribution parameters) with softmax-layer parameterized by DNNs
    to quantify prediction uncertainty. However, the softmax prediction uncertainty
    tends to be over-confident (Hendrycks and Gimpel, [2016](#bib.bib51)). Evidential
    deep learning is developed to overcome the limitation by introducing the evidence
    theory (JSANG, [2018](#bib.bib64)) to neural network frameworks. The motivation
    of evidential deep learning is to construct predictions based on evidence and
    predict the parameters of Dirichlet density. For example, considering the 3-class
    classification problem, a vanilla neural network directly predicts the categorical
    distribution on the class $\pi={\pi_{1},\pi_{2},\pi_{3}}$ with $\sum_{i}\pi_{i}=1$.
    However, this approach can only represent a point estimation of prediction distribution.
    On the other hand, evidential deep learning aims to predict the evidence for each
    class $\boldsymbol{\alpha}=\{\alpha_{1},\alpha_{2},\alpha_{3}\}$ with the constraints
    $\boldsymbol{\alpha}>0$, which can be considered as the parameters of Dirichlet
    distribution (Sensoy et al., [2018](#bib.bib122)). The framework is shown in Fig [13](#S5.F13
    "Figure 13 ‣ 5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"), where
    the output is the evidence $\boldsymbol{\alpha}$ for each class, and the prediction
    distribution is sampled from the Dirichlet distribution. The expected prediction
    distribution for each class is $p_{i}=\frac{\alpha_{i}}{\sum_{c=1}^{3}\alpha_{c}}$,
    whose entropy reflects the data uncertainty. On the other hand, the model uncertainty
    is reflected with the total evidence $\sum_{i}\alpha_{i}$, which means the more
    evidence we collect, the more confident the model is.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '如前述章节所讨论，对于分类问题，现有的基于深度学习的模型显式或隐式地通过 DNN 参数化的 softmax 层来预测类别概率（类别分布参数），以量化预测不确定性。然而，softmax
    预测的不确定性往往过于自信（Hendrycks 和 Gimpel，[2016](#bib.bib51)）。证据深度学习的提出是为了克服这一限制，通过将证据理论（JSANG，[2018](#bib.bib64)）引入神经网络框架。证据深度学习的动机是基于证据构建预测，并预测
    Dirichlet 密度的参数。例如，考虑到三类分类问题，一个普通神经网络直接预测类别 $\pi={\pi_{1},\pi_{2},\pi_{3}}$ 上的类别分布，其中
    $\sum_{i}\pi_{i}=1$。然而，这种方法只能表示预测分布的点估计。另一方面，证据深度学习旨在预测每个类别的证据 $\boldsymbol{\alpha}=\{\alpha_{1},\alpha_{2},\alpha_{3}\}$，其中有约束条件
    $\boldsymbol{\alpha}>0$，这可以视为 Dirichlet 分布的参数（Sensoy 等，[2018](#bib.bib122)）。该框架如图 [13](#S5.F13
    "Figure 13 ‣ 5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") 所示，其中输出是每个类别的证据
    $\boldsymbol{\alpha}$，预测分布是从 Dirichlet 分布中采样的。每个类别的期望预测分布为 $p_{i}=\frac{\alpha_{i}}{\sum_{c=1}^{3}\alpha_{c}}$，其熵反映了数据的不确定性。另一方面，模型的不确定性通过总证据
    $\sum_{i}\alpha_{i}$ 反映，这意味着我们收集的证据越多，模型的信心也越强。'
- en: 'Mathematically, evidential deep learning aims to learn the prior distribution
    of categorical distribution parameters, which is represented by the Dirichlet
    distribution. The Dirichlet distribution is parameterized by its concentration
    parameters $\boldsymbol{\alpha}$ (evidence) where $\alpha_{0}$ is the sum of all
    $\alpha_{i}$ and is referred to as precision of Dirichlet distribution. A higher
    value of $\alpha_{0}$ will lead to sharper distribution and lower model uncertainty.
    As shown in Eq.[24](#S5.E24 "In 5.3.2\. Evidential deep learning ‣ 5.3\. Model
    and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"),
    the $\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$ defines a probability density
    function over the k-dimensional random variable $\boldsymbol{\mu}=[{\mu_{1}},...,{\mu}_{k}]$,
    where $k$ is the number of classes, $\boldsymbol{\mu}$ belongs to the standard
    $k-1$ simplex (${\mu_{1}}+...+{\mu}_{k}=1$ and ${\mu}_{i}\in[0,1]$ for all $i\in{1,...,k}$),
    and can be regarded as the categorical distribution parameters. The relationship
    between Dirichlet distribution and uncertainty quantification can be illustrated
    using a 2-simplex, as shown in Fig [14](#S5.F14 "Figure 14 ‣ 5.3.2\. Evidential
    deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective"). The random variable $\boldsymbol{\mu}=[{\mu}_{1},{\mu}_{2},{\mu}_{3}]$
    is represented by its Barycentric coordinates in Fig [14](#S5.F14 "Figure 14 ‣
    5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") (a) on the 2-simplex. The
    Barycentric coordinate is a coordinate system where point are located inside a
    simplex, and the value in each coordinate can be interpreted as the fraction of
    masses placed at the corresponding vertices of the simplex. Fig [14](#S5.F14 "Figure
    14 ‣ 5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\.
    A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") (b) shows a scenario
    where the evidence parameters are equal, resulting in three classes being indistinguishable
    and implying high data uncertainty (high entropy for the sampled $\boldsymbol{\mu}$).
    As the total evidence $\sum_{i}\alpha_{i}$ increases, the density becomes more
    concentrated, which means the model uncertainty decreases, while the data uncertainty
    remains fixed. Fig [14](#S5.F14 "Figure 14 ‣ 5.3.2\. Evidential deep learning
    ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective") (c) shows a scenario with fixed model uncertainty, as the
    sum of evidence parameters remains constant. When the evidence becomes imbalanced,
    the density becomes more concentrated toward one class, thus decreasing the data
    uncertainty.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，证据深度学习旨在学习类别分布参数的先验分布，这种分布由**Dirichlet分布**表示。Dirichlet分布由其集中参数$\boldsymbol{\alpha}$（证据）参数化，其中$\alpha_{0}$是所有$\alpha_{i}$的总和，被称为Dirichlet分布的精度。$\alpha_{0}$的值越高，会导致分布越尖锐，模型的不确定性越低。如
    Eq.[24](#S5.E24 "在5.3.2. 证据深度学习 ‣ 5.3. 模型和数据不确定性 ‣ 5. DNN UQ方法的分类 ‣ 深度神经网络不确定性量化方法调查：不确定性来源的视角")中所示，$\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$定义了k维随机变量$\boldsymbol{\mu}=[{\mu_{1}},...,{\mu}_{k}]$上的概率密度函数，其中$k$是类别的数量，$\boldsymbol{\mu}$属于标准的$k-1$单纯形（${\mu_{1}}+...+{\mu}_{k}=1$且${\mu}_{i}\in[0,1]$对于所有$i\in{1,...,k}$），可以视为类别分布参数。Dirichlet分布与不确定性量化之间的关系可以通过2-单纯形来说明，如图[14](#S5.F14
    "图14 ‣ 5.3.2. 证据深度学习 ‣ 5.3. 模型和数据不确定性 ‣ 5. DNN UQ方法的分类 ‣ 深度神经网络不确定性量化方法调查：不确定性来源的视角")所示。随机变量$\boldsymbol{\mu}=[{\mu}_{1},{\mu}_{2},{\mu}_{3}]$在图[14](#S5.F14
    "图14 ‣ 5.3.2. 证据深度学习 ‣ 5.3. 模型和数据不确定性 ‣ 5. DNN UQ方法的分类 ‣ 深度神经网络不确定性量化方法调查：不确定性来源的视角")（a）中以其重心坐标表示在2-单纯形上。重心坐标是一种坐标系统，其中点位于单纯形内部，每个坐标中的值可以解释为放置在单纯形相应顶点上的质量的分数。图[14](#S5.F14
    "图14 ‣ 5.3.2. 证据深度学习 ‣ 5.3. 模型和数据不确定性 ‣ 5. DNN UQ方法的分类 ‣ 深度神经网络不确定性量化方法调查：不确定性来源的视角")（b）展示了证据参数相等的情况，导致三个类别无法区分，意味着数据的不确定性很高（采样的$\boldsymbol{\mu}$的熵很高）。随着总证据$\sum_{i}\alpha_{i}$的增加，密度变得更加集中，这意味着模型不确定性减少，而数据不确定性保持不变。图[14](#S5.F14
    "图14 ‣ 5.3.2. 证据深度学习 ‣ 5.3. 模型和数据不确定性 ‣ 5. DNN UQ方法的分类 ‣ 深度神经网络不确定性量化方法调查：不确定性来源的视角")（c）展示了模型不确定性固定的情况，因为证据参数的总和保持不变。当证据变得不平衡时，密度会更集中到一个类别，从而减少数据的不确定性。
- en: '| (24) |  | $\text{Dir}(\boldsymbol{\mu}&#124;\boldsymbol{\alpha})=\frac{\mathrm{T}(\alpha_{0})}{\prod_{c=1}^{K}\mathrm{T}(\alpha_{c})}\prod_{c=1}^{K}{\mu}_{c}^{\alpha_{c}-1},\alpha_{c}>0,\
    \alpha_{0}=\sum_{c=1}^{K}\alpha_{c}$ |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\text{Dir}(\boldsymbol{\mu}&#124;\boldsymbol{\alpha})=\frac{\mathrm{T}(\alpha_{0})}{\prod_{c=1}^{K}\mathrm{T}(\alpha_{c})}\prod_{c=1}^{K}{\mu}_{c}^{\alpha_{c}-1},\alpha_{c}>0,\
    \alpha_{0}=\sum_{c=1}^{K}\alpha_{c}$ |  |'
- en: '![Refer to caption](img/25f7a3ac4d8679e4eaf7eb8a85783f91.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25f7a3ac4d8679e4eaf7eb8a85783f91.png)'
- en: (a) Barycentric coordinate
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 重心坐标
- en: '![Refer to caption](img/7c0867023307c09f77ebdfc9f5560062.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c0867023307c09f77ebdfc9f5560062.png)'
- en: (b) Keep data uncertainty fixed , and model uncertainty decrease from left to
    right.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 保持数据不确定性不变，模型不确定性从左到右减少。
- en: '![Refer to caption](img/7c618847f3bedb89e5bed6684b27a2d3.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c618847f3bedb89e5bed6684b27a2d3.png)'
- en: (c) Keep the model uncertainty fixed, and the data uncertainty decrease from
    left to right (the entropy of the categoric distribution decrease).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 保持模型不确定性不变，数据不确定性从左到右减少（类别分布的熵减少）。
- en: Figure 14\. Dirichlet distribution density visualization
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. Dirichlet 分布密度可视化
- en: Due to the intriguing property of Dirichlet distribution, evidential deep learning
    directly predict the parameters of Dirichlet density. For example, the Dirichlet
    prior network (DPN) (Malinin and Gales, [2018](#bib.bib85)) learn the concentration
    parameter $\boldsymbol{\alpha}$ for the Dirichlet distribution $\boldsymbol{\alpha}=\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta})$.
    Then the categorical distribution parameters can be drawn from the Dirichlet distribution
    as $p(\boldsymbol{\mu}|\boldsymbol{x},\boldsymbol{\theta})=\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$.
    The prediction of class probability is the average over possible $\boldsymbol{\mu}$
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dirichlet 分布的独特性质，证据深度学习直接预测 Dirichlet 密度的参数。例如，Dirichlet 先验网络（DPN）（Malinin
    和 Gales，[2018](#bib.bib85)）学习 Dirichlet 分布的浓度参数 $\boldsymbol{\alpha}$，其中 $\boldsymbol{\alpha}=\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta})$。然后，可以从
    Dirichlet 分布中绘制类别分布参数为 $p(\boldsymbol{\mu}|\boldsymbol{x},\boldsymbol{\theta})=\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$。类别概率的预测是对可能的
    $\boldsymbol{\mu}$ 的平均值。
- en: '| (25) |  | $p(w_{c}&#124;\boldsymbol{x},\boldsymbol{\theta})=\int p(w_{c}&#124;\boldsymbol{\mu})p(\boldsymbol{\mu}&#124;\boldsymbol{x},\boldsymbol{\theta})=\frac{\alpha_{c}}{\alpha_{0}}$
    |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $p(w_{c}&#124;\boldsymbol{x},\boldsymbol{\theta})=\int p(w_{c}&#124;\boldsymbol{\mu})p(\boldsymbol{\mu}&#124;\boldsymbol{x},\boldsymbol{\theta})=\frac{\alpha_{c}}{\alpha_{0}}$
    |  |'
- en: To measure the uncertainty from the Dirichlet distribution, the total uncertainty
    is the entropy of the average prediction distribution, and data uncertainty is
    the average of the entropy in each realization of $\boldsymbol{\mu}$. Several
    approaches have extended the Prior network to a regression model and a posterior
    network has also been proposed for more reliable uncertainty estimation.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量 Dirichlet 分布中的不确定性，总不确定性是平均预测分布的熵，而数据不确定性是每个 $\boldsymbol{\mu}$ 实现中的熵的平均值。一些方法已将先验网络扩展为回归模型，并且还提出了后验网络以实现更可靠的不确定性估计。
- en: The advantage of evidential deep learning is that the approach only requires
    a single forward pass during inference, and is much more computationally efficient.
    The approach also explicitly distinguishes data and model uncertainty in a principled
    way. The disadvantage is that the training stage is more complicated and is not
    guaranteed to obtain the same or similar prediction accuracy with vanilla network
    and cannot leverage existing progress in DNN. Furthermore, the training stage
    requires OOD samples to learn well representations and increase amount of the
    training dataset.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 证据深度学习的优点是该方法在推理过程中只需要一次前向传播，计算效率高得多。该方法还以原理性方式明确区分数据和模型不确定性。缺点是训练阶段更复杂，并且无法保证获得与普通网络相同或类似的预测精度，且不能利用
    DNN 中的现有进展。此外，训练阶段需要 OOD 样本以良好地学习表示，并增加训练数据集的数量。
- en: Table 3\. Simultaneous Model and data uncertainty methods comparison
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 同时模型和数据不确定性方法比较
- en: '| Model | Approach | Pros | Cons |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 优点 | 缺点 |'
- en: '| Combination of existing approaches Pros: Easy to implement. Cons: High computation
    and storage cost. |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 现有方法的组合 优点：易于实现。 缺点：计算和存储成本高。 |'
- en: '&#124; Combine BNN &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合 BNN &#124;'
- en: '&#124; with prediction &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带预测 &#124;'
- en: '&#124; distribution (Kendall and Gal, [2017](#bib.bib67)) &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布 (Kendall 和 Gal, [2017](#bib.bib67)) &#124;'
- en: '|'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture uncertainty &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉不确定性 &#124;'
- en: '&#124; from both data noise &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自数据噪声 &#124;'
- en: '&#124; and model parameters. &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和模型参数。 &#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Require multiple &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要多个 &#124;'
- en: '&#124; forward pass &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 前向传播 &#124;'
- en: '&#124; during inference. &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在推断过程中。 &#124;'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Combine ensemble &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合集成 &#124;'
- en: '&#124; with prediction &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与预测 &#124;'
- en: '&#124; distribution (Lakshminarayanan et al., [2017](#bib.bib73)) &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分布（Lakshminarayanan 等，[2017](#bib.bib73)）&#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture uncertainty &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉不确定性 &#124;'
- en: '&#124; from both data noise &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自数据噪声 &#124;'
- en: '&#124; and mode architectures. &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和模式架构。 &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Require more &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要更多 &#124;'
- en: '&#124; computation and &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算和 &#124;'
- en: '&#124; storage requirement. &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 存储需求。 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Combine ensemble &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合集成 &#124;'
- en: '&#124; with prediction &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与预测 &#124;'
- en: '&#124; interval (Pearce et al., [2018](#bib.bib106); Salem et al., [2020](#bib.bib119))
    &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区间（Pearce 等，[2018](#bib.bib106)；Salem 等，[2020](#bib.bib119)）&#124;'
- en: '|'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capture both data and &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉数据和 &#124;'
- en: '&#124; model uncertainty and &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型不确定性和 &#124;'
- en: '&#124; do not need explicit &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不需要显式的 &#124;'
- en: '&#124; parametric distribution &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 参数分布 &#124;'
- en: '&#124; form. &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 形式。 &#124;'
- en: '|'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Require modification &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要修改 &#124;'
- en: '&#124; on existing DNN &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在现有 DNN 上 &#124;'
- en: '&#124; training process. &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练过程。 &#124;'
- en: '|'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Evidential deep &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 证据深度 &#124;'
- en: '&#124; learning &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Evidential deep &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 证据深度 &#124;'
- en: '&#124; learning (Malinin and Gales, [2018](#bib.bib85); Charpentier et al.,
    [2020](#bib.bib17); Sensoy et al., [2018](#bib.bib122); Bao et al., [2021](#bib.bib7))
    &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习（Malinin 和 Gales，[2018](#bib.bib85)；Charpentier 等，[2020](#bib.bib17)；Sensoy
    等，[2018](#bib.bib122)；Bao 等，[2021](#bib.bib7)）&#124;'
- en: '| Computational efficient. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 计算效率。 |'
- en: '&#124; Cannot ensure &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无法保证 &#124;'
- en: '&#124; similar accuracy &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似的准确性 &#124;'
- en: '&#124; compared with &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与 &#124;'
- en: '&#124; vanilla DNN models. &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始的深度神经网络（DNN）模型。 &#124;'
- en: '&#124; Only capture OOD &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅捕捉 OOD &#124;'
- en: '&#124; uncertainty &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不确定性 &#124;'
- en: '|'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 6\. Uncertainty estimation in various machine learning problems
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 各种机器学习问题中的不确定性估计
- en: In this section, we discuss several major ML problems where UQ can play a critical
    role. For each problem, we discuss the following four perspectives, including
    the nature of the data on the application, the source of uncertainty, the challenges
    associated with uncertainty quantification, and the existing literature for the
    specific problems.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了几个主要的机器学习问题，其中不确定性量化（UQ）可以发挥关键作用。对于每个问题，我们讨论以下四个方面，包括应用中的数据性质、不确定性的来源、不确定性量化相关的挑战以及针对具体问题的现有文献。
- en: 6.1\. Out-of-distribution detection
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 分布外检测
- en: It is a fundamental assumption for a deep neural network that the test data
    distribution is similar to the training data distribution $p_{\text{train}}(x)\approx
    p_{\text{test}}(x)$. However, in real-world tasks, DNN can encounter many out-of-distribution
    (OOD) data, which come from some different distributions compared with the training
    data. DNN performance may drop significantly and give unreliable predictions in
    such situations. The model should be aware of such situations to avoid over-confident
    predictions on OOD samples.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度神经网络而言，一个基本假设是测试数据分布与训练数据分布 $p_{\text{train}}(x)\approx p_{\text{test}}(x)$
    相似。然而，在实际任务中，DNN 可能会遇到许多分布外（OOD）数据，这些数据来自与训练数据不同的分布。在这种情况下，DNN 的性能可能会显著下降，并且给出不可靠的预测。模型应该能够意识到这种情况，以避免对
    OOD 样本做出过度自信的预测。
- en: Given a training data distribution $p(x)$, the OOD data are those samples that
    are either unlikely under the training data distribution or outside the support
    of $p(x)$. Accurate detection of OOD samples is of paramount importance for the
    safety-critical application, e.g., autonomous driving (Shafaei et al., [2018](#bib.bib123)),
    medical image analysis (Ghoshal et al., [2021](#bib.bib42)) Since OOD samples
    lie further away from the training samples, the trained model may not generalize
    well and produces diverse and unstable predictions for those samples. The model
    parameters or architecture are not suitable for OOD samples. Thus OOD samples
    are expected to have higher model uncertainty. The primary uncertainty for OOD
    data is concerned with model uncertainty, because the model trained with in-distribution
    may not generalize well to other domains. It is important to make the model aware
    of the sample density when quantifying the uncertainty.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练数据分布 $p(x)$，OOD数据是指那些在训练数据分布下不太可能出现或在 $p(x)$ 的支持范围之外的样本。准确检测OOD样本对于安全关键应用（例如自动驾驶（Shafaei等，[2018](#bib.bib123)），医学图像分析（Ghoshal等，[2021](#bib.bib42)）至关重要。由于OOD样本远离训练样本，训练好的模型可能无法很好地泛化，并且对这些样本产生多样且不稳定的预测。模型参数或架构可能不适用于OOD样本。因此，OOD样本预计会有更高的模型不确定性。OOD数据的主要不确定性与模型不确定性有关，因为训练过的模型可能无法很好地泛化到其他领域。在量化不确定性时，使模型意识到样本密度是很重要的。
- en: 'Existing approaches: From the above analysis we are clear that model uncertainty
    is more important for OOD detection. Thus existing approaches leveraging the model
    uncertainty framework are much more popular. For example, drop-out-based BNN approaches
    have been applied to OOD detection and improved the performance through the use
    of the randomized embeddings induced by the intermediate layers of a dropout BNN
    (Nguyen et al., [2022a](#bib.bib97)); Deep ensembles are simple but perform well
    on OOD detection (Lakshminarayanan et al., [2017](#bib.bib73); Ovadia et al.,
    [2019](#bib.bib103)). Recent advances attempt to develop distance-aware DNN for
    more accurate OOD detection by imposing constraints on the feature extracting
    process (Liu et al., [2020](#bib.bib80); van Amersfoort et al., [2021](#bib.bib135)).
    Recent developments in evidential deep learning framework have also demonstrated
    its capability on OOD detection in many benchmark datasets because of the explicit
    distinction between two types of uncertainty in the framework (Malinin and Gales,
    [2018](#bib.bib85); Charpentier et al., [2020](#bib.bib17); Sensoy et al., [2018](#bib.bib122);
    Bao et al., [2021](#bib.bib7)).'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法：从上述分析可以看出，模型不确定性对于OOD检测更加重要。因此，现有的方法中，利用模型不确定性框架的方法更加流行。例如，基于drop-out的BNN方法已经应用于OOD检测，并通过使用dropout
    BNN中间层引发的随机化嵌入提高了性能（Nguyen等，[2022a](#bib.bib97)）；深度集成方法简单但在OOD检测中表现良好（Lakshminarayanan等，[2017](#bib.bib73)；Ovadia等，[2019](#bib.bib103)）。最近的进展试图通过对特征提取过程施加约束，开发距离感知DNN，以实现更准确的OOD检测（Liu等，[2020](#bib.bib80)；van
    Amersfoort等，[2021](#bib.bib135)）。近期在证据深度学习框架中的发展也展示了其在许多基准数据集上进行OOD检测的能力，因为该框架明确区分了两种类型的不确定性（Malinin和Gales，[2018](#bib.bib85)；Charpentier等，[2020](#bib.bib17)；Sensoy等，[2018](#bib.bib122)；Bao等，[2021](#bib.bib7)）。
- en: 6.2\. Active Learning
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 主动学习
- en: For the deep learning model, obtaining labeled data can be laborious, and time-consuming.
    Active learning (Ren et al., [2021](#bib.bib114)) aims to solve the data labeling
    issue by learning from a small amount of data and choosing by the model what data
    it requires the user to label and retrain the model iteratively. The goal is to
    reduce the number of labeled examples needed for learning as much as possible,
    which means requiring a strategy to choose which sample is more worth labeling
    for the model. Utilizing predictive uncertainty has been a popular strategy and
    those instances whose prediction is maximally uncertain are most useful.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型，获得标记数据可能是费力且耗时的。主动学习（Ren等，[2021](#bib.bib114)）旨在通过从少量数据中学习，并由模型选择需要用户标记的数据，迭代地重新训练模型，来解决数据标记问题。其目标是尽可能减少学习所需的标记示例数量，这意味着需要一种策略来选择哪个样本更值得为模型标记。利用预测不确定性是一种流行的策略，那些预测最大不确定的实例是最有用的。
- en: The key goal in active learning is to choose observations $\boldsymbol{x}$ for
    which obtaining labels $y$ would improve the performance of the learning. As discussed
    in the background, adding samples with large data uncertainty cannot improve the
    trained model because its inherent randomness is irreducible while more samples
    with model uncertainty can improve the model performance. In this regard, model
    uncertainty is more important for active learning (Nguyen et al., [2022b](#bib.bib98)).
    The critical challenge for active learning is to distinguish between the data
    and model uncertainty and utilize model uncertainty for selecting new samples.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的关键目标是选择那些获取标签$y$能够提升学习性能的观察样本$\boldsymbol{x}$。如背景中所讨论的，添加数据不确定性大的样本无法改善训练模型，因为其固有的随机性是不可减少的，而更多的模型不确定性样本可以提升模型性能。在这方面，模型不确定性对主动学习更为重要（Nguyen
    et al., [2022b](#bib.bib98)）。主动学习的关键挑战在于区分数据不确定性和模型不确定性，并利用模型不确定性来选择新的样本。
- en: 'Existing approaches: Similar to OOD detection, the approaches for model uncertainty
    detection can be adapted to an active learning framework by considering uncertainty
    coming from the parameter, model architecture, and sample density sparsity. For
    example, the BNN framework considers the samples that decrease the entropy of
    $p(\boldsymbol{\theta}|\mathcal{D}_{\text{tr}},\{\boldsymbol{x},y\})$ most will
    be most useful (Depeweg, [2019](#bib.bib31)). The deep ensemble and MC-dropout
    approach can also be a straightforward way for quantifying the model uncertainty
    in active learning (Hein et al., [2022](#bib.bib50)). Recent approaches propose
    margin-based uncertainty sampling scheme and provide convergence analysis (Raj
    and Bach, [2022](#bib.bib112))'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法：类似于OOD检测，模型不确定性检测的方法可以通过考虑来自参数、模型架构和样本密度稀疏的分歧来适应主动学习框架。例如，BNN框架考虑那些能最大程度减少$p(\boldsymbol{\theta}|\mathcal{D}_{\text{tr}},\{\boldsymbol{x},y\})$熵的样本最为有用（Depeweg,
    [2019](#bib.bib31)）。深度集成和MC-dropout方法也可以是量化主动学习中模型不确定性的直接方式（Hein et al., [2022](#bib.bib50)）。最近的方法提出了基于边际的不确定性采样方案，并提供了收敛分析（Raj
    and Bach, [2022](#bib.bib112)）。
- en: 6.3\. Deep Reinforcement Learning
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 深度强化学习
- en: The purpose of deep reinforcement learning (DRL) is to train an agent interacting
    with environment to maximize its total rewards (Sutton and Barto, [2018](#bib.bib130)).
    DRL can be regarded as learning via Markov Decision Process, defined by the tuple
    $\{S,A,R,P\}$, where $S$ is the set of states (environment condition), $A$ is
    the set of actions (agent), $R$ is the function mapping state-action pairs to
    rewards, and $P$ is the transition probability (the probability of next state
    after performing actions on current state). The goal of DRL is to learn the policy
    $\pi$ (a function mapping given state to an action) that maximize the sum of discounted
    future rewards $J(\pi)=\mathbb{E}[\sum_{i}\gamma^{i}R(s_{i},a_{i})]$, where $\gamma$
    is the discount factor on the future reward (meaning rewards in more future are
    less important) (Sutton and Barto, [2018](#bib.bib130)). In Deep Q-learning, the
    DNN models are utilized to learn the value functions (expected rewards for a state-action
    pair) for each action given the input state.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）的目的是训练一个与环境互动的代理，以最大化其总奖励（Sutton and Barto, [2018](#bib.bib130)）。DRL可以被看作是通过马尔可夫决策过程进行学习，该过程由元组$\{S,A,R,P\}$定义，其中$S$是状态集（环境条件），$A$是动作集（代理），$R$是将状态-动作对映射到奖励的函数，$P$是转移概率（在当前状态下执行动作后的下一状态的概率）。DRL的目标是学习策略$\pi$（将给定状态映射到动作的函数），以最大化折扣未来奖励的总和$J(\pi)=\mathbb{E}[\sum_{i}\gamma^{i}R(s_{i},a_{i})]$，其中$\gamma$是对未来奖励的折扣因子（即未来的奖励重要性较低）（Sutton
    and Barto, [2018](#bib.bib130)）。在深度Q学习中，DNN模型被用来学习每个给定状态下的动作的价值函数（期望奖励）。
- en: 'Due to the complex agent and environment conditions and limited training states,
    there also exist two types of uncertainty sources: data and model uncertainty.
    Data uncertainty can arise from the intrinsic randomness in the interactions between
    the agent and environment, which causes randomness in the reward $R$, the transition
    functions $P$ and the randomness in next state value distribution. To characterize
    the data uncertainty arising from those sources, the distributional RL (Bellemare
    et al., [2017](#bib.bib9)) take a probabilistic perspective on learning the rewards
    functions instead of approximating the expectation of the value. Thus the approach
    can implement risk-aware behavior on the agent. Similar approach is proposed to
    quantify the data uncertainty in DRL aiming for curiosity-based learning in face
    of unpredictable transitions (Mavor-Parker et al., [2022](#bib.bib88)). The following
    work (Dabney et al., [2018](#bib.bib24)) extend the parametric distribution to
    non-parametric prediction interval methods to quantify the data uncertainty and
    avoid the explicit parametric format. The approach corresponds to the literature
    we discuss in section 5.1\. On the other hand, given the limited training state
    space, there exists high model uncertainty. The DNN model may not learn the optimum
    policy function and miss the unexplored state spaces, which potentially give higher
    rewards. This means DRL model face a trade-off between exploitation and exploration.
    Exploration means utilizing what the model has learned and choose the best policy
    to maximize future rewards. Exploration refers to choose the unexplored state
    to learn more possible high state-action pairs. The challenge of effective exploration
    is connected to model uncertainty. The higher model uncertainty means the model
    are not learnt well on the given state and require more exploration on that sample.
    For example, deep ensemble Q-network (Chen et al., [2021c](#bib.bib20)) is proposed
    to inject model uncertainty into Q learning for more efficient exploration sampling.
    To reduce the computational overhead, Dropout Q-functions (Hiraoka et al., [2021](#bib.bib55))
    method is proposed to use MC-dropout for model uncertainty quantification. The
    following work (Osband et al., [2018](#bib.bib102)) demonstrates the previous
    ensemble and dropout methods may produce poor approximation to the model uncertainty
    in case where state density does not correlate with the true uncertainty. To overcome
    the shortcoming, they suggest adding a random prior to the ensemble DQNs.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代理和环境条件的复杂性以及训练状态的有限性，还存在两种不确定性来源：数据不确定性和模型不确定性。数据不确定性可能源于代理与环境之间交互的内在随机性，这导致奖励$R$、转移函数$P$以及下一状态值分布的随机性。为了表征这些来源的数据显示的不确定性，分布式强化学习（Bellemare
    et al., [2017](#bib.bib9)）从概率的角度学习奖励函数，而不是仅仅近似值的期望。因此，该方法可以实现对代理的风险感知行为。类似的方法被提出用于量化DRL中的数据不确定性，旨在应对不可预测的转移中的好奇心驱动学习（Mavor-Parker
    et al., [2022](#bib.bib88)）。以下工作（Dabney et al., [2018](#bib.bib24)）将参数化分布扩展到非参数预测区间方法，以量化数据不确定性并避免显式的参数化格式。这种方法对应于我们在第5.1节讨论的文献。另一方面，鉴于有限的训练状态空间，存在较高的模型不确定性。DNN模型可能无法学习到最佳政策函数，并且可能遗漏一些尚未探索的状态空间，这些状态空间可能带来更高的奖励。这意味着DRL模型面临利用和探索之间的权衡。利用指的是使用模型已经学到的知识，并选择最佳策略以最大化未来奖励。探索指的是选择未探索的状态以学习更多可能的高状态-动作对。有效探索的挑战与模型不确定性相关。模型不确定性越高，模型在给定状态上的学习效果越差，需要对该样本进行更多的探索。例如，深度集成Q网络（Chen
    et al., [2021c](#bib.bib20)）被提出以将模型不确定性注入Q学习，从而实现更高效的探索采样。为了减少计算开销，提出了使用MC-dropout进行模型不确定性量化的Dropout
    Q-functions方法（Hiraoka et al., [2021](#bib.bib55)）。以下工作（Osband et al., [2018](#bib.bib102)）展示了之前的集成和dropout方法在状态密度与真实不确定性不相关的情况下可能产生对模型不确定性的较差近似。为了克服这一缺点，他们建议在集成DQN中加入随机先验。
- en: In summary, both data and model uncertainty are crucial aspects of DRL for more
    efficient policy learning and exploration. Many works inspired from DNN uncertainty
    quantification have been proposed to improve the performance of DRL.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据和模型不确定性是强化学习（DRL）中提高政策学习和探索效率的重要方面。许多从深度神经网络（DNN）不确定性量化中获得灵感的研究已被提出，以提高DRL的性能。
- en: 7\. Future direction
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来方向
- en: We identify some research gaps and list several future directions that current
    literature has not explored or has little coverage from both the methodologies
    and application perspectives.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出一些研究空白，并列出当前文献尚未探索或覆盖较少的未来方向，包括方法论和应用视角。
- en: 7.1\. Combine UQ with DNN explainability
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 将不确定性量化与深度神经网络可解释性结合
- en: The explanation for DNN model predictions has been increasingly crucial because
    it provides tools for understanding and diagnosing the model’s prediction. Recently,
    many explainability methods, termed explainers (Vu and Thai, [2020](#bib.bib139)),
    have been introduced in the category of local feature attribution methods. That
    is, methods that return a real-valued score for each feature of a given data sample,
    representing the feature’s relative importance for the sample prediction. These
    explanations are local in that each data sample may have a different explanation.
    Using local feature attribution methods, therefore, helps users better understand
    nonlinear and complex black-box models. Both uncertainty quantification and explanation
    are important for a robust, trustworthy AI model. Current methodologies consider
    two directions separately, we consider it could enable a more trustworthy AI system
    if combining them. Though many methodologies have been proposed for more precise
    uncertainty quantification, very few techiniques attempt to explain why the uncertainty
    exists in the predictions.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 对深度神经网络（DNN）模型预测的解释变得越来越重要，因为它提供了理解和诊断模型预测的工具。最近，许多可解释性方法，即解释器（Vu 和 Thai，[2020](#bib.bib139)），在局部特征归因方法类别中被提出。这些方法会返回每个数据样本特征的实值评分，表示该特征对样本预测的相对重要性。这些解释是局部的，因为每个数据样本可能有不同的解释。因此，使用局部特征归因方法可以帮助用户更好地理解非线性和复杂的黑箱模型。不确定性量化和解释对于构建一个可靠和值得信赖的人工智能模型都很重要。目前的方法论分别考虑了这两个方向，我们认为如果将它们结合起来，可能会使人工智能系统更可信。虽然已经提出了许多方法来更精确地量化不确定性，但很少有技术尝试解释为什么预测中存在不确定性。
- en: 'There are two possible directions to combine the power of explanations and
    uncertainty quantification: First, existing explanation methods could be potentially
    improved after considering the prediction uncertainty since those uncertain samples’
    explanations may not be trustworthy and can be omitted. Second, from another perspective,
    after obtaining the uncertainty quantification, we can leverage the existing post-hoc
    explanation methods for understanding the reason that the model is uncertain.
    For example, it is intriguging to ask the question why the prediction is uncertainty
    and which or which set of input features are uncertain, or due to which layer
    of the model is imperfect.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 结合解释和不确定性量化的可能方向有两个：首先，现有的解释方法在考虑预测不确定性后可能会有所改进，因为这些不确定样本的解释可能不可信并且可以被忽略。其次，从另一个角度来看，在获得不确定性量化后，我们可以利用现有的后验解释方法来理解模型不确定的原因。例如，令人感兴趣的问题是为什么预测存在不确定性，哪些输入特征或哪些特征集存在不确定性，或由于模型的哪一层存在缺陷。
- en: 7.2\. Structured Uncertainty Quantification
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 结构化不确定性量化
- en: Structured prediction concerns learning on structured objects, where the samples
    are correlated with each other, violating the i.i.d assumption in most machine
    learning problems. Many methodologies have been developed for structured learning
    (BakIr et al., [2007](#bib.bib6)), such as conditional random field, hidden Markov
    model, and graph neural networks. The intuition is to leverage the probabilistic
    graphic model among variables or samples to explicitly utilize the dependency
    structure for prediction. Not only the target variable predictions are correlated,
    but the prediction uncertainty could also be related to the structured dependency
    inherent in the model. However, most methodologies ignore the uncertainty structure,
    which is of paramount importance for calibrated UQ in many tasks. In the following,
    we briefly introduce the uncertainty of three kinds of structured data, image,
    graph, and spatio-temporal data.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化预测涉及对结构化对象进行学习，其中样本彼此相关，违反了大多数机器学习问题中的独立同分布（i.i.d）假设。已经开发了许多用于结构化学习的方法（BakIr
    等，[2007](#bib.bib6)），如条件随机场、隐马尔可夫模型和图神经网络。其直观理解是利用变量或样本之间的概率图模型，显式地利用依赖结构进行预测。不仅目标变量的预测相关，而且预测的不确定性也可能与模型固有的结构化依赖有关。然而，大多数方法忽略了不确定性结构，这对于许多任务中的校准不确定性量化至关重要。接下来，我们简要介绍三种结构化数据的不确定性，即图像、图和时空数据。
- en: 7.2.1\. Imaging and inverse problem
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 成像与逆问题
- en: The goal of the imaging process is to reconstruct an unknown image from measurements,
    which is an inverse problem commonly used in medical imaging (e.g., magnetic resonance
    imaging and X-ray computed tomography) and geophysical inversion (e.g., seismic
    inversion) (Fessler, [2010](#bib.bib36); Edupuganti et al., [2020](#bib.bib34);
    Sun et al., [2021](#bib.bib128)). However, this process is challenging due to
    the limited and noisy information used to determine the original image, leading
    to structured uncertainty and correlations between nearby pixels in the reconstructed
    image (Kendall and Gal, [2017](#bib.bib67)). To overcome this issue, current research
    in uncertainty quantification of inverse problems employs conditional deep generative
    models, such as cVAE, cGAN, and conditional normalizing flow models (Dorta et al.,
    [2018](#bib.bib32); Adler and Öktem, [2018](#bib.bib3); Denker et al., [2020](#bib.bib30)).
    These methods utilize a low-dimensional latent space for image generation but
    may overlook unique data characteristics, such as structural constraints from
    domain physics in certain types of image data, such as remote sensing images,
    MRI images, or geological subsurface images (Jiang and Shekhar, [2017](#bib.bib61);
    Shih et al., [2022](#bib.bib125); Sun et al., [2021](#bib.bib128)). The use of
    physics-informed models may improve uncertainty quantification in these cases.
    It’s promising to incorporate the physics constraints for quantifying the uncertainty
    associated with the imaging process.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 成像过程的目标是从测量中重建未知图像，这是一种在医学成像（例如磁共振成像和X射线计算机断层扫描）和地球物理反演（例如地震反演）中常用的逆问题（Fessler,
    [2010](#bib.bib36); Edupuganti et al., [2020](#bib.bib34); Sun et al., [2021](#bib.bib128)）。然而，由于用于确定原始图像的有限且噪声较大的信息，这一过程具有挑战性，导致重建图像中相邻像素之间存在结构性不确定性和相关性（Kendall
    and Gal, [2017](#bib.bib67)）。为了克服这个问题，当前在逆问题不确定性量化方面的研究采用了条件深度生成模型，如cVAE、cGAN和条件正则化流模型（Dorta
    et al., [2018](#bib.bib32); Adler and Öktem, [2018](#bib.bib3); Denker et al.,
    [2020](#bib.bib30)）。这些方法利用低维潜在空间进行图像生成，但可能忽略了独特的数据特征，如某些类型图像数据中的领域物理结构约束，例如遥感图像、MRI图像或地质地下图像（Jiang
    and Shekhar, [2017](#bib.bib61); Shih et al., [2022](#bib.bib125); Sun et al.,
    [2021](#bib.bib128)）。使用物理信息模型可能改善这些情况下的不确定性量化。将物理约束纳入不确定性量化过程是非常有前景的。
- en: 7.2.2\. Spatiotemporal data
  id: totrans-442
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 时空数据
- en: Spatiotemporal data are measurements over space and time  (Shekhar et al., [2015](#bib.bib124);
    Jiang, [2018](#bib.bib57); Jiang et al., [2022b](#bib.bib62)). They are special
    due to the violation of the common assumption that samples follow an identical
    and independent distribution. Uncertainty quantification of spatiotemporal deep
    learning poses several unique challenges. First, the analysis of spatiotemporal
    data requires co-registration of different maps (e.g., points, lines, polygons,
    geo-raster) into the same spatial coordinate system. The process is subject to
    registration uncertainty due to GPS errors or annotation mistakes in map generation 
    (Jiang et al., [2022a](#bib.bib59), [2021](#bib.bib58)). For example, the vector
    annotation labels of a river can be misaligned with corresponding river pixels
    in high-resolution Earth imagery (e.g., due to annotation mistakes or interpreting
    low-resolution background imagery). Such registration uncertainty causes troubles
    when training deep neural networks. Quantifying registration uncertainty for vector
    annotations or between vector labels to raster features is non-trivial as it involves
    modeling a continuous line (or polygon)  (He et al., [2022](#bib.bib49)). Second,
    implicit dependency structures exist in continuous space and time (e.g., spatial
    and temporal autocorrelation, temporal dynamics). Thus, the uncertainty quantification
    process should be aware of the dynamic dependency structure among predictions
    at different locations and times. Third, spatiotemporal data are non-stationary,
    i.e., sample distribution varies across space and time. Because of this, a deep
    neural network trained from samples in one region may not generalize well to test
    samples in a new region. Addressing this issue requires characterizing out-of-distribution
    samples due to spatiotemporal non-stationarity (e.g., spatiotemporal outliers) 
    (Shekhar et al., [2015](#bib.bib124); Jiang, [2018](#bib.bib57)). In addition,
    even within the same region where training samples are collected, the model prediction
    at a new test location may exhibit a different level of uncertainty based on the
    nearby density of training samples. For example, in air quality prediction, real-world
    sensors are often non-uniformly distributed. A sub-area with fewer observations
    nearby tends to be more uncertain. Traditionally, the Gaussian process has been
    used to quantify such uncertainty. However, for deep neural network models, new
    techniques are needed that consider sample density both in the non-spatial feature
    space and in the geographic space. To address the above challenges, novel UQ methods
    need to be developed.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 时空数据是对空间和时间的测量（Shekhar 等，[2015](#bib.bib124)；Jiang，[2018](#bib.bib57)；Jiang
    等，[2022b](#bib.bib62)）。它们之所以特殊，是因为它们违反了样本遵循相同且独立分布的常见假设。时空深度学习的不确定性量化面临几个独特的挑战。首先，时空数据的分析需要将不同的地图（例如，点、线、多边形、地理光栅）配准到相同的空间坐标系统中。由于
    GPS 错误或地图生成中的注释错误，整个过程会受到配准不确定性的影响（Jiang 等，[2022a](#bib.bib59)，[2021](#bib.bib58)）。例如，河流的矢量注释标签可能与高分辨率地球影像中的相应河流像素不对齐（例如，由于注释错误或解释低分辨率背景影像）。这种配准不确定性在训练深度神经网络时会造成问题。量化矢量注释的配准不确定性或矢量标签与光栅特征之间的配准不确定性并非易事，因为这涉及到建模一个连续的线（或多边形）（He
    等，[2022](#bib.bib49)）。其次，连续空间和时间中存在隐式依赖结构（例如，空间和时间自相关、时间动态）。因此，不确定性量化过程应考虑不同位置和时间之间预测的动态依赖结构。第三，时空数据是非平稳的，即样本分布在空间和时间上会发生变化。因此，基于一个区域中的样本训练的深度神经网络可能无法很好地泛化到新区域的测试样本。解决这个问题需要对由于时空非平稳性造成的分布外样本进行表征（例如，时空异常值）（Shekhar
    等，[2015](#bib.bib124)；Jiang，[2018](#bib.bib57)）。此外，即使在收集训练样本的同一区域，新测试位置的模型预测也可能会根据附近训练样本的密度表现出不同的误差水平。例如，在空气质量预测中，现实世界中的传感器通常分布不均。观察数量较少的子区域往往更不确定。传统上，高斯过程用于量化这种不确定性。然而，对于深度神经网络模型，需要考虑非空间特征空间和地理空间中的样本密度的新技术。为了应对上述挑战，需要开发新型的不确定性量化方法。
- en: 7.2.3\. Graph data
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3. 图数据
- en: Graph data is a general type of structured data with nodes and edge connections.
    Graph neural networks (GNNs) have been widely used for graph applications related
    to node classification and edge (link) prediction. However, UQ for GNN models
    has been less explored. Some work utilizes existing UQ techniques for GNN models 
    (Feng et al., [2021](#bib.bib35)) without considering their unique characteristics.
    First, predictions on a graph are structured, so the UQ module needs to consider
    such structural dependency. Second, many GNN models assume a fixed graph topology
    from training and test instances (e.g., spectral-based methods  (Defferrard et al.,
    [2016](#bib.bib28))). Uncertainty exists in GNN predictions due to the shift of
    graph topology from training graphs and test graphs. Similarly, uncertainty exists
    when the graph is perturbed by removing nodes and edges. Finally, many real-world
    graph problems are spatiotemporal at the same time (e.g., traffic flow prediction
    on road networks). Thus, challenges related to UQ for spatiotemporal deep learning
    also apply to graphs.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据是一种具有节点和边连接的一般结构化数据类型。图神经网络（GNNs）已被广泛用于与节点分类和边（链接）预测相关的图应用。然而，GNN 模型的不确定性量化（UQ）研究较少。一些工作利用现有的
    UQ 技术应用于 GNN 模型（Feng et al., [2021](#bib.bib35)），但没有考虑其独特特征。首先，图上的预测是结构化的，因此 UQ
    模块需要考虑这种结构依赖性。其次，许多 GNN 模型假设训练和测试实例的图拓扑是固定的（例如，基于谱的方法（Defferrard et al., [2016](#bib.bib28)））。由于图拓扑从训练图到测试图的变化，GNN
    预测中存在不确定性。同样，当图通过移除节点和边而被扰动时，也存在不确定性。最后，许多现实世界的图问题同时具有时空特征（例如，道路网络上的交通流预测）。因此，与时空深度学习相关的不确定性量化挑战也适用于图。
- en: 7.3\. UQ for physics-aware DNN models
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 物理感知的 DNN 模型的不确定性量化
- en: It’s well-known that many scientific applications can be described with some
    kind of a physics principle, e.g., the conservation law for diffusion process
    in the form of a partial differential equation (PDE). Given the dramatic capability
    of DL in extracting complex patterns in CV, and NLP, many scientific domains aim
    to leverage the massive amount of observed data to advance scientific discoveries.
    However, deep learning models do not necessarily obey the fundamental physics
    principle of the system that drives the real-world phenomenon (Kashinath et al.,
    [2021](#bib.bib66)), because of the black-box nature of neural networks. To address
    the challenge, physics-informed neural networks (PINN) (Krishnapriyan et al.,
    [2021](#bib.bib71); Yang and Perdikaris, [2019](#bib.bib149); Li et al., [2020](#bib.bib78);
    Karniadakis et al., [2021](#bib.bib65)) attempt to incorporate physical principles
    and domain knowledge from theoretical understanding into deep learning models
    to build physically consistent predictive models. For example, considering a spatio-temporal
    observation in a physical system $\mathcal{D}\times\mathcal{T}$ driven by a PDE
    in the form
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，许多科学应用可以用某种物理原理来描述，例如，扩散过程的守恒定律以偏微分方程（PDE）的形式表示。鉴于深度学习在计算机视觉（CV）和自然语言处理（NLP）中提取复杂模式的惊人能力，许多科学领域旨在利用大量观察数据来推动科学发现。然而，由于神经网络的黑箱特性，深度学习模型不一定遵循驱动现实世界现象的系统的基本物理原理（Kashinath
    et al., [2021](#bib.bib66)）。为了解决这一挑战，物理信息神经网络（PINN）（Krishnapriyan et al., [2021](#bib.bib71);
    Yang and Perdikaris, [2019](#bib.bib149); Li et al., [2020](#bib.bib78); Karniadakis
    et al., [2021](#bib.bib65)）尝试将物理原理和理论理解中的领域知识融入深度学习模型中，以建立物理一致的预测模型。例如，考虑一个由 PDE
    驱动的物理系统中的时空观察 $\mathcal{D}\times\mathcal{T}$ 形式
- en: '| (26) |  | $u_{t}(x,t)+\mathcal{N}_{x}u(x,t)=0,x\in\mathcal{D},t\in\mathcal{T}$
    |  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $u_{t}(x,t)+\mathcal{N}_{x}u(x,t)=0,x\in\mathcal{D},t\in\mathcal{T}$
    |  |'
- en: 'where $u(x,t)$ is the interesting physical variables to be modeled, which can
    be parameterized with a neural network, i.e., $u(x,t)=f_{\boldsymbol{\theta}}(x,t)$,
    $x$ represents the spatial coordinate, $t$ is the time coordinate and $\mathcal{N}_{x}$
    is the nonlinear differential operator. The intuition of the PINN model is to
    build a neural network that explicitly encodes Eq. [26](#S7.E26 "In 7.3\. UQ for
    physics-aware DNN models ‣ 7\. Future direction ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") as residual
    loss, i.e., $r_{\boldsymbol{\theta}}(x,t)=\partial_{t}{f_{\boldsymbol{\theta}}(x,t)}-\mathcal{N}_{x}f_{\boldsymbol{\theta}}(x,t)$
    to control the optimization space of the neural network (Karniadakis et al., [2021](#bib.bib65)).
    Thus, the physical constraints can be incorporated into the model training process
    as a soft penalty.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $u(x,t)$ 是待建模的有趣物理变量，可以用神经网络进行参数化，即 $u(x,t)=f_{\boldsymbol{\theta}}(x,t)$，$x$
    代表空间坐标，$t$ 是时间坐标，而 $\mathcal{N}_{x}$ 是非线性微分算子。PINN 模型的直观概念是构建一个神经网络，该网络明确地将方程
    [26](#S7.E26 "In 7.3\. UQ for physics-aware DNN models ‣ 7\. Future direction
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") 编码为残差损失，即 $r_{\boldsymbol{\theta}}(x,t)=\partial_{t}{f_{\boldsymbol{\theta}}(x,t)}-\mathcal{N}_{x}f_{\boldsymbol{\theta}}(x,t)$，以控制神经网络的优化空间
    (Karniadakis et al., [2021](#bib.bib65))。因此，物理约束可以作为软惩罚被纳入模型训练过程中。'
- en: For many physical systems, the underlying physical principle generating the
    samples may be non-deterministic or unknown. The system is inherently chaotic
    and stochastic. For example, in weather forecasting and climate prediction problem,
    the state of the system is sensitive to the initial conditions, in which a small
    perturbation in the nonlinear system can result in a large difference in the later
    state (a.k.a. butterfly effect) (Kashinath et al., [2021](#bib.bib66)). Even with
    a deterministic initial condition, the unknown physics equation (e.g., PDE) that
    drives the system can be stochastic, which is described by a stochastic differential
    equation. Uncertainty quantification is of vital importance to improve the prediction’s
    reliability, especially under distribution shifts. To summarize, the uncertainty
    of physical system modeling can come from the following source. First, the initial
    and boundary condition of the physical system is non-deterministic, and the system
    may be chaotic, i.e., in weather forecasting, a small perturbation can introduce
    a large deviation in future prediction (Wang et al., [2019b](#bib.bib140)). Second,
    the inherent physical principle may not be perfectly known or the parameter of
    the governing equation may be stochastic, i.e., in an imperfect physical system,
    the conservation law of heat may be violated in a non-closed system (Yang and
    Perdikaris, [2019](#bib.bib149)). Lastly, the physical system can be inhomogeneous
    and data distribution is non-stationary (Daw et al., [2021](#bib.bib27)).
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多物理系统，生成样本的基本物理原理可能是非确定性的或未知的。系统本质上是混沌和随机的。例如，在天气预报和气候预测问题中，系统的状态对初始条件非常敏感，在非线性系统中，初始的小扰动可以导致后续状态的巨大差异（即蝴蝶效应）
    (Kashinath et al., [2021](#bib.bib66))。即使在确定性的初始条件下，驱动系统的未知物理方程（例如 PDE）也可能是随机的，这由随机微分方程描述。为了提高预测的可靠性，特别是在分布变化下，量化不确定性至关重要。总结来说，物理系统建模的不确定性可能来源于以下几个方面。首先，物理系统的初始和边界条件是非确定性的，系统可能是混沌的，即在天气预报中，小的扰动可能会在未来预测中引入大的偏差
    (Wang et al., [2019b](#bib.bib140))。其次，固有的物理原理可能不是完全已知的，或支配方程的参数可能是随机的，即在一个不完美的物理系统中，热量的守恒定律可能在一个非封闭系统中被违反
    (Yang and Perdikaris, [2019](#bib.bib149))。最后，物理系统可能是不均匀的，数据分布是非平稳的 (Daw et al.,
    [2021](#bib.bib27))。
- en: These cases indicate that for the stochastic physical system, there exists inherent
    data uncertainty. Thus, a natural way to model distributions and incorporate stochasticity
    and uncertainty into the neural network is through probabilistic models. Because
    of the unique characteristic of physical data, there exist several challenges
    for quantifying the uncertainty in PINN. First, the system requires considering
    the physical principle and its uncertainty simultaneously. The incorporation of
    physical constraints can significantly reduce the data and model uncertainty.
    Second, there are various sources of uncertainty, which may arise from the inherent
    physical system randomness, the measurement error, and the limited knowledge of
    the physical governing equation.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这些案例表明，对于随机物理系统，存在固有的数据不确定性。因此，将分布建模并将随机性和不确定性纳入神经网络的自然方式是通过概率模型。由于物理数据的独特特性，量化PINN中的不确定性面临几个挑战。首先，系统需要同时考虑物理原理及其不确定性。引入物理约束可以显著减少数据和模型的不确定性。其次，不确定性来源多种多样，可能来自固有的物理系统随机性、测量误差以及对物理控制方程的有限知识。
- en: One direction of uncertainty quantification for PINN is to build a probabilistic
    neural network $p_{\boldsymbol{\theta}}(\boldsymbol{u}|\boldsymbol{x},t,\boldsymbol{z})$
    for propagating uncertainty originating from various sources, where $\boldsymbol{u}(\boldsymbol{x},t)$
    represents the spatio-temporal random field with the explanatory spatial variable
    $\boldsymbol{x}$ and temporal variable $t$. $\boldsymbol{z}$ is a latent variable
    that follows a prior distribution $p(\boldsymbol{z})$ and aim to encode the variability
    of the high-dimensional observation $\boldsymbol{u}$ into low-dimensional embedding.
    many approaches based on the deep generative model for uncertainty quantification
    have been proposed to leverage their capability of probabilistic modeling for
    structured outputs (Oberdiek et al., [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40);
    Daw et al., [2021](#bib.bib27)).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 对于物理信息神经网络（PINN），不确定性量化的一个方向是建立一个概率神经网络 $p_{\boldsymbol{\theta}}(\boldsymbol{u}|\boldsymbol{x},t,\boldsymbol{z})$，以传播来自各种来源的不确定性，其中
    $\boldsymbol{u}(\boldsymbol{x},t)$ 代表具有解释性空间变量 $\boldsymbol{x}$ 和时间变量 $t$ 的时空随机场。$\boldsymbol{z}$
    是一个遵循先验分布 $p(\boldsymbol{z})$ 的潜变量，旨在将高维观察 $\boldsymbol{u}$ 的变异性编码到低维嵌入中。许多基于深度生成模型的不确定性量化方法已被提出，以利用其对结构化输出的概率建模能力（Oberdiek
    等， [2022](#bib.bib100)；Gao 和 Ng，[2022](#bib.bib40)；Daw 等，[2021](#bib.bib27)）。
- en: 8\. Conclusion
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: 'This paper presents a systematic survey on uncertainty quantification for DNNs
    based on their types of uncertainty sources. We categorize existing literature
    into three groups: model uncertainty, data uncertainty, and the combination of
    the two. In addition, we analyze the advantages and disadvantages of each approach
    corresponding to the subtype of uncertainty source it addresses. The uncertainty
    source and unique challenge of various applications in different domains and ML
    problems are also summarized. We list several future research directions.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 本文针对基于不确定性来源类型的深度神经网络（DNN）不确定性量化进行了系统的调查。我们将现有文献分为三类：模型不确定性、数据不确定性以及两者的结合。此外，我们还分析了每种方法对应的不确定性源子类型的优缺点。不同领域和机器学习问题中各种应用的不确定性来源和独特挑战也进行了总结。我们列出了几个未来的研究方向。
- en: Acknowledgements.
  id: totrans-455
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This material is based upon work supported by the National Science Foundation
    (NSF) under Grant No. IIS-2147908, IIS-2207072, CNS-1951974, OAC-2152085, and
    the National Oceanic and Atmospheric Administration grant NA19NES4320002 (CISESS)
    at the University of Maryland.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 本材料基于国家科学基金会（NSF）资助的工作，资助号为 IIS-2147908、IIS-2207072、CNS-1951974、OAC-2152085，以及马里兰大学国家海洋和大气管理局资助的NA19NES4320002（CISESS）。
- en: References
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Abdar et al. (2021) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan,
    Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
    Acharya, et al. 2021. A review of uncertainty quantification in deep learning:
    Techniques, applications and challenges. *Information Fusion* 76 (2021), 243–297.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdar等（2021）Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan,
    Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
    Acharya等。2021年。深度学习中的不确定性量化综述：技术、应用与挑战。*信息融合* 76 (2021), 243–297。
- en: Adler and Öktem (2018) Jonas Adler and Ozan Öktem. 2018. Deep bayesian inversion.
    *arXiv preprint arXiv:1811.05910* (2018).
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adler 和 Öktem（2018）Jonas Adler 和 Ozan Öktem。2018年。深度贝叶斯反演。*arXiv 预印本 arXiv:1811.05910*（2018）。
- en: 'Alam et al. (2017) Firoj Alam, Muhammad Imran, and Ferda Ofli. 2017. Image4act:
    Online social media image processing for disaster response. In *Proceedings of
    the 2017 IEEE/ACM international conference on advances in social networks analysis
    and mining 2017*. 601–604.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alam et al. (2017) Firoj Alam, Muhammad Imran, 和 Ferda Ofli. 2017. Image4act：灾难响应的在线社交媒体图像处理。见
    *2017年IEEE/ACM国际社交网络分析与挖掘会议论文集*。601–604.
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017.
    Wasserstein generative adversarial networks. In *International conference on machine
    learning*. PMLR, 214–223.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, 和 Léon Bottou. 2017.
    Wasserstein生成对抗网络。见 *国际机器学习会议*。PMLR, 214–223.
- en: BakIr et al. (2007) Gökhan BakIr, Thomas Hofmann, Alexander J Smola, Bernhard
    Schölkopf, and Ben Taskar. 2007. *Predicting structured data*. MIT press.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BakIr et al. (2007) Gökhan BakIr, Thomas Hofmann, Alexander J Smola, Bernhard
    Schölkopf, 和 Ben Taskar. 2007. *结构化数据预测*。麻省理工学院出版社。
- en: Bao et al. (2021) Wentao Bao, Qi Yu, and Yu Kong. 2021. Evidential deep learning
    for open set action recognition. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 13349–13358.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao et al. (2021) Wentao Bao, Qi Yu, 和 Yu Kong. 2021. 开放集动作识别的证据深度学习。见 *IEEE/CVF国际计算机视觉会议论文集*。13349–13358.
- en: Begoli et al. (2019) Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov.
    2019. The need for uncertainty quantification in machine-assisted medical decision
    making. *Nature Machine Intelligence* 1, 1 (2019), 20–23.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Begoli et al. (2019) Edmon Begoli, Tanmoy Bhattacharya, 和 Dimitri Kusnezov.
    2019. 机器辅助医疗决策中对不确定性量化的需求。*自然机器智能* 1, 1 (2019), 20–23.
- en: Bellemare et al. (2017) Marc G Bellemare, Will Dabney, and Rémi Munos. 2017.
    A distributional perspective on reinforcement learning. In *International Conference
    on Machine Learning*. PMLR, 449–458.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare et al. (2017) Marc G Bellemare, Will Dabney, 和 Rémi Munos. 2017. 强化学习的分布视角。见
    *国际机器学习会议*。PMLR, 449–458.
- en: 'Bengio et al. (2013) Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013.
    Representation learning: A review and new perspectives. *IEEE transactions on
    pattern analysis and machine intelligence* 35, 8 (2013), 1798–1828.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2013) Yoshua Bengio, Aaron Courville, 和 Pascal Vincent. 2013.
    表示学习：综述与新视角。*IEEE模式分析与机器智能学报* 35, 8 (2013), 1798–1828.
- en: Bishop (1994) Christopher M Bishop. 1994. Mixture density networks. (1994).
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bishop (1994) Christopher M Bishop. 1994. 混合密度网络。 (1994).
- en: 'Blei et al. (2017) David M Blei, Alp Kucukelbir, and Jon D McAuliffe. 2017.
    Variational inference: A review for statisticians. *Journal of the American statistical
    Association* 112, 518 (2017), 859–877.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei et al. (2017) David M Blei, Alp Kucukelbir, 和 Jon D McAuliffe. 2017. 变分推断：统计学家的综述。*美国统计协会杂志*
    112, 518 (2017), 859–877.
- en: Bower and Bolouri (2001) James M Bower and Hamid Bolouri. 2001. *Computational
    modeling of genetic and biochemical networks*. MIT press.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bower and Bolouri (2001) James M Bower 和 Hamid Bolouri. 2001. *遗传和生化网络的计算建模*。麻省理工学院出版社。
- en: Bradford et al. (2018) Eric Bradford, Artur M Schweidtmann, Dongda Zhang, Keju
    Jing, and Ehecatl Antonio del Rio-Chanona. 2018. Dynamic modeling and optimization
    of sustainable algal production with uncertainty using multivariate Gaussian processes.
    *Computers & Chemical Engineering* 118 (2018), 143–158.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradford et al. (2018) Eric Bradford, Artur M Schweidtmann, Dongda Zhang, Keju
    Jing, 和 Ehecatl Antonio del Rio-Chanona. 2018. 使用多元高斯过程对可持续藻类生产进行动态建模与优化。*计算机与化学工程*
    118 (2018), 143–158.
- en: 'Brunton and Kutz (2022) Steven L Brunton and J Nathan Kutz. 2022. *Data-driven
    science and engineering: Machine learning, dynamical systems, and control*. Cambridge
    University Press.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunton and Kutz (2022) Steven L Brunton 和 J Nathan Kutz. 2022. *数据驱动的科学与工程：机器学习、动态系统与控制*。剑桥大学出版社。
- en: Chang et al. (2020) Jie Chang, Zhonghao Lan, Changmao Cheng, and Yichen Wei.
    2020. Data uncertainty learning in face recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 5710–5719.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2020) Jie Chang, Zhonghao Lan, Changmao Cheng, 和 Yichen Wei. 2020.
    面部识别中的数据不确定性学习。见 *IEEE/CVF计算机视觉与模式识别会议论文集*。5710–5719.
- en: 'Charpentier et al. (2020) Bertrand Charpentier, Daniel Zügner, and Stephan
    Günnemann. 2020. Posterior network: Uncertainty estimation without ood samples
    via density-based pseudo-counts. *Advances in Neural Information Processing Systems*
    33 (2020), 1356–1367.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charpentier et al. (2020) Bertrand Charpentier, Daniel Zügner, 和 Stephan Günnemann.
    2020. 后验网络：通过基于密度的伪计数进行无OOD样本的不确定性估计。*神经信息处理系统进展* 33 (2020), 1356–1367.
- en: 'Chen et al. (2021b) Haoxian Chen, Ziyi Huang, Henry Lam, Huajie Qian, and Haofeng
    Zhang. 2021b. Learning prediction intervals for regression: Generalization and
    calibration. In *International Conference on Artificial Intelligence and Statistics*.
    PMLR, 820–828.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021b）Haoxian Chen、Ziyi Huang、Henry Lam、Huajie Qian 和 Haofeng Zhang。2021b。回归预测区间的学习：泛化与校准。发表于
    *国际人工智能与统计会议*。PMLR，第 820–828 页。
- en: Chen et al. (2021a) Xiang Chen, Andres Diaz-Pinto, Nishant Ravikumar, and Alejandro F
    Frangi. 2021a. Deep learning in medical image registration. *Progress in Biomedical
    Engineering* 3, 1 (2021), 012003.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021a）Xiang Chen、Andres Diaz-Pinto、Nishant Ravikumar 和 Alejandro F Frangi。2021a。医学图像配准中的深度学习。*生物医学工程进展*
    3，第 1 期（2021），012003。
- en: 'Chen et al. (2021c) Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. 2021c.
    Randomized ensembled double q-learning: Learning fast without a model. *arXiv
    preprint arXiv:2101.05982* (2021).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021c）Xinyue Chen、Che Wang、Zijian Zhou 和 Keith Ross。2021c。随机集成双 Q 学习：在没有模型的情况下快速学习。*arXiv
    预印本 arXiv:2101.05982*（2021）。
- en: Cheng et al. (2014) Reynold Cheng, Tobias Emrich, Hans-Peter Kriegel, Nikos
    Mamoulis, Matthias Renz, Goce Trajcevski, and Andreas Züfle. 2014. Managing uncertainty
    in spatial and spatio-temporal data. In *2014 IEEE 30th International Conference
    on Data Engineering*. IEEE, 1302–1305.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2014）Reynold Cheng、Tobias Emrich、Hans-Peter Kriegel、Nikos Mamoulis、Matthias
    Renz、Goce Trajcevski 和 Andreas Züfle。2014。空间和时空数据中的不确定性管理。发表于 *2014 IEEE 第30届数据工程国际会议*。IEEE，第
    1302–1305 页。
- en: 'Choi et al. (2019) Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee.
    2019. Gaussian yolov3: An accurate and fast object detector using localization
    uncertainty for autonomous driving. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 502–511.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2019）Jiwoong Choi、Dayoung Chun、Hyun Kim 和 Hyuk-Jae Lee。2019。高斯 yolov3：一种利用定位不确定性进行自主驾驶的准确而快速的目标检测器。发表于
    *IEEE/CVF 国际计算机视觉会议论文集*。第 502–511 页。
- en: Cireşan et al. (2013) Dan C Cireşan, Alessandro Giusti, Luca M Gambardella,
    and Jürgen Schmidhuber. 2013. Mitosis detection in breast cancer histology images
    with deep neural networks. In *International conference on medical image computing
    and computer-assisted intervention*. Springer, 411–418.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cireşan 等（2013）Dan C Cireşan、Alessandro Giusti、Luca M Gambardella 和 Jürgen Schmidhuber。2013。使用深度神经网络在乳腺癌组织图像中检测有丝分裂。发表于
    *医学图像计算与计算机辅助手术国际会议*。Springer，第 411–418 页。
- en: Dabney et al. (2018) Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos.
    2018. Distributional reinforcement learning with quantile regression. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dabney 等（2018）Will Dabney、Mark Rowland、Marc Bellemare 和 Rémi Munos。2018。使用分位回归的分布式强化学习。发表于
    *AAAI 人工智能会议论文集*，第 32 卷。
- en: Damianou (2015) Andreas Damianou. 2015. *Deep Gaussian processes and variational
    propagation of uncertainty*. Ph. D. Dissertation. University of Sheffield.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damianou（2015）Andreas Damianou。2015。*深度高斯过程与不确定性的变分传播*。博士论文。谢菲尔德大学。
- en: Damianou et al. (2016) Andreas C Damianou, Michalis K Titsias, and Neil Lawrence.
    2016. Variational inference for latent variables and uncertain inputs in Gaussian
    processes. (2016).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damianou 等（2016）Andreas C Damianou、Michalis K Titsias 和 Neil Lawrence。2016。高斯过程中的潜变量和不确定输入的变分推断。（2016）。
- en: 'Daw et al. (2021) Arka Daw, M Maruf, and Anuj Karpatne. 2021. PID-GAN: A GAN
    Framework based on a Physics-informed Discriminator for Uncertainty Quantification
    with Physics. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*. 237–247.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daw 等（2021）Arka Daw、M Maruf 和 Anuj Karpatne。2021。PID-GAN：基于物理信息判别器的 GAN 框架，用于物理学中的不确定性量化。发表于
    *第27届 ACM SIGKDD 知识发现与数据挖掘会议论文集*。第 237–247 页。
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    *Advances in neural information processing systems* 29 (2016).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defferrard 等（2016）Michaël Defferrard、Xavier Bresson 和 Pierre Vandergheynst。2016。带有快速局部谱滤波的图卷积神经网络。*神经信息处理系统进展*
    29（2016）。
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In *2009
    IEEE conference on computer vision and pattern recognition*. Ieee, 248–255.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2009）Jia Deng、Wei Dong、Richard Socher、Li-Jia Li、Kai Li 和 Li Fei-Fei。2009。Imagenet：大规模层次图像数据库。发表于
    *2009 IEEE 计算机视觉与模式识别会议*。IEEE，第 248–255 页。
- en: Denker et al. (2020) Alexander Denker, Maximilian Schmidt, Johannes Leuschner,
    Peter Maass, and Jens Behrmann. 2020. Conditional normalizing flows for low-dose
    computed tomography image reconstruction. *arXiv preprint arXiv:2006.06270* (2020).
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denker et al. (2020) Alexander Denker, Maximilian Schmidt, Johannes Leuschner,
    Peter Maass, 和 Jens Behrmann. 2020. *低剂量计算机断层扫描图像重建的条件归一化流*。*arXiv预印本 arXiv:2006.06270*
    (2020)。
- en: Depeweg (2019) Stefan Depeweg. 2019. *Modeling epistemic and aleatoric uncertainty
    with bayesian neural networks and latent variables*. Ph. D. Dissertation. Technische
    Universität München.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Depeweg (2019) Stefan Depeweg. 2019. *利用贝叶斯神经网络和潜变量建模认知和随机不确定性*。博士论文。慕尼黑工业大学。
- en: Dorta et al. (2018) Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell,
    and Ivor Simpson. 2018. Structured uncertainty prediction networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 5477–5485.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorta et al. (2018) Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell,
    和 Ivor Simpson. 2018. *结构化不确定性预测网络*。在 *IEEE计算机视觉与模式识别会议论文集* 中，5477–5485。
- en: Dusenberry et al. (2020) Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas
    Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, and Andrew M Dai. 2020.
    Analyzing the role of model uncertainty for electronic health records. In *Proceedings
    of the ACM Conference on Health, Inference, and Learning*. 204–213.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dusenberry et al. (2020) Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas
    Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, 和 Andrew M Dai. 2020. *分析模型不确定性在电子健康记录中的作用*。在
    *ACM健康、推断与学习会议论文集* 中，204–213。
- en: Edupuganti et al. (2020) Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala,
    and John Pauly. 2020. Uncertainty quantification in deep MRI reconstruction. *IEEE
    Transactions on Medical Imaging* 40, 1 (2020), 239–250.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edupuganti et al. (2020) Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala,
    和 John Pauly. 2020. *深度MRI重建中的不确定性量化*。*IEEE医学成像学报* 40, 1 (2020), 239–250。
- en: 'Feng et al. (2021) Boyuan Feng, Yuke Wang, and Yufei Ding. 2021. Uag: Uncertainty-aware
    attention graph neural network for defending adversarial attacks. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 35\. 7404–7412.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2021) Boyuan Feng, Yuke Wang, 和 Yufei Ding. 2021. Uag: 不确定性感知注意图神经网络用于防御对抗攻击。
    在 *AAAI人工智能会议论文集* 中，第35卷。7404–7412。'
- en: Fessler (2010) Jeffrey A Fessler. 2010. Model-based image reconstruction for
    MRI. *IEEE signal processing magazine* 27, 4 (2010), 81–89.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fessler (2010) Jeffrey A Fessler. 2010. *基于模型的MRI图像重建*。*IEEE信号处理杂志* 27, 4 (2010),
    81–89。
- en: Florida and Wildlife (2021) Fish Florida and Conservation Commission Wildlife.
    2021. Florida Fish and Wildlife Conservation Commission Data. (2021). [10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florida and Wildlife (2021) Fish Florida 和 Conservation Commission Wildlife.
    2021. *佛罗里达州鱼类和野生动物保护委员会数据*。 (2021)。 [10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
- en: Friston et al. (2007) Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto,
    John Ashburner, and Will Penny. 2007. Variational free energy and the Laplace
    approximation. *Neuroimage* 34, 1 (2007), 220–234.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friston et al. (2007) Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto,
    John Ashburner, 和 Will Penny. 2007. *变分自由能和拉普拉斯近似*。*Neuroimage* 34, 1 (2007),
    220–234。
- en: 'Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
    a bayesian approximation: Representing model uncertainty in deep learning. In
    *international conference on machine learning*. PMLR, 1050–1059.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal and Ghahramani (2016) Yarin Gal 和 Zoubin Ghahramani. 2016. *Dropout作为贝叶斯近似：在深度学习中表示模型不确定性*。在
    *国际机器学习会议* 中。PMLR, 1050–1059。
- en: Gao and Ng (2022) Yihang Gao and Michael K Ng. 2022. Wasserstein generative
    adversarial uncertainty quantification in physics-informed neural networks. *J.
    Comput. Phys.* 463 (2022), 111270.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao and Ng (2022) Yihang Gao 和 Michael K Ng. 2022. *物理信息神经网络中的Wasserstein生成对抗不确定性量化*。*J.
    Comput. Phys.* 463 (2022), 111270。
- en: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher, et al. 2021. A survey of uncertainty in deep
    neural networks. *arXiv preprint arXiv:2107.03342* (2021).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher 等。2021. *深度神经网络中的不确定性调查*。*arXiv预印本 arXiv:2107.03342*
    (2021)。
- en: Ghoshal et al. (2021) Biraja Ghoshal, Allan Tucker, Bal Sanghera, and Wai Lup Wong.
    2021. Estimating uncertainty in deep learning for reporting confidence to clinicians
    in medical image segmentation and diseases detection. *Computational Intelligence*
    37, 2 (2021), 701–734.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghoshal et al. (2021) Biraja Ghoshal, Allan Tucker, Bal Sanghera, 和 Wai Lup
    Wong. 2021. 在医学图像分割和疾病检测中为临床医生报告置信度的深度学习不确定性估计. *计算智能* 37, 2 (2021), 701–734。
- en: Gong et al. (2022) Xuan Gong, Luckyson Khaidem, Wentao Zhu, Baochang Zhang,
    and David Doermann. 2022. Uncertainty learning towards unsupervised deformable
    medical image registration. In *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*. 2484–2493.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2022) Xuan Gong, Luckyson Khaidem, Wentao Zhu, Baochang Zhang,
    和 David Doermann. 2022. 面向无监督可变形医学图像配准的不确定性学习. 在 *IEEE/CVF冬季计算机视觉应用大会论文集* 中。2484–2493。
- en: Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.
    Generative adversarial networks. *Commun. ACM* 63, 11 (2020), 139–144.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 2020.
    生成对抗网络. *Commun. ACM* 63, 11 (2020), 139–144。
- en: Graves (2011) Alex Graves. 2011. Practical variational inference for neural
    networks. *Advances in neural information processing systems* 24 (2011).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves (2011) Alex Graves. 2011. 神经网络的实用变分推断. *神经信息处理系统进展* 24 (2011)。
- en: Guillaumes (2017) Axel Brando Guillaumes. 2017. *Mixture density networks for
    distribution and uncertainty estimation*. Ph. D. Dissertation. Universitat Politècnica
    de Catalunya. Facultat d’Informàtica de Barcelona.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guillaumes (2017) Axel Brando Guillaumes. 2017. *用于分布和不确定性估计的混合密度网络*. 博士学位论文。加泰罗尼亚理工大学。巴塞罗那计算机学院。
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
    2017. On calibration of modern neural networks. In *International conference on
    machine learning*. PMLR, 1321–1330.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, 和 Kilian Q Weinberger. 2017.
    现代神经网络的校准问题. 在 *国际机器学习会议* 中。PMLR, 1321–1330。
- en: 'Hariri et al. (2019) Reihaneh H Hariri, Erik M Fredericks, and Kate M Bowers.
    2019. Uncertainty in big data analytics: survey, opportunities, and challenges.
    *Journal of Big Data* 6, 1 (2019), 1–16.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hariri et al. (2019) Reihaneh H Hariri, Erik M Fredericks, 和 Kate M Bowers.
    2019. 大数据分析中的不确定性: 调查、机会和挑战. *大数据杂志* 6, 1 (2019), 1–16。'
- en: He et al. (2022) Wenchong He, Zhe Jiang, Marcus Kriby, Yiqun Xie, Xiaowei Jia,
    Da Yan, and Yang Zhou. 2022. Quantifying and Reducing Registration Uncertainty
    of Spatial Vector Labels on Earth Imagery. In *Proceedings of the 28th ACM SIGKDD
    Conference on Knowledge Discovery and Data Mining*. 554–564.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022) Wenchong He, Zhe Jiang, Marcus Kriby, Yiqun Xie, Xiaowei Jia,
    Da Yan, 和 Yang Zhou. 2022. 量化和减少地球影像上空间向量标签的配准不确定性. 在 *第28届ACM SIGKDD知识发现与数据挖掘会议论文集*
    中。554–564。
- en: Hein et al. (2022) Alice Hein, Stefan Röhrl, Thea Grobel, Manuel Lengl, Nawal
    Hafez, Martin Knopp, Christian Klenk, Dominik Heim, Oliver Hayden, and Klaus Diepold.
    2022. A Comparison of Uncertainty Quantification Methods for Active Learning in
    Image Classification. In *2022 International Joint Conference on Neural Networks
    (IJCNN)*. IEEE, 1–8.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hein et al. (2022) Alice Hein, Stefan Röhrl, Thea Grobel, Manuel Lengl, Nawal
    Hafez, Martin Knopp, Christian Klenk, Dominik Heim, Oliver Hayden, 和 Klaus Diepold.
    2022. 图像分类中的主动学习不确定性量化方法比较. 在 *2022国际联合神经网络大会（IJCNN）* 中。IEEE, 1–8。
- en: Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. A baseline
    for detecting misclassified and out-of-distribution examples in neural networks.
    *arXiv preprint arXiv:1610.02136* (2016).
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks and Gimpel (2016) Dan Hendrycks 和 Kevin Gimpel. 2016. 检测神经网络中错误分类和分布外样本的基线.
    *arXiv预印本arXiv:1610.02136* (2016)。
- en: 'Hengl et al. (2017) Tomislav Hengl, Jorge Mendes de Jesus, Gerard BM Heuvelink,
    Maria Ruiperez Gonzalez, Milan Kilibarda, Aleksandar Blagotić, Wei Shangguan,
    Marvin N Wright, Xiaoyuan Geng, Bernhard Bauer-Marschallinger, et al. 2017. SoilGrids250m:
    Global gridded soil information based on machine learning. *PLoS one* 12, 2 (2017),
    e0169748.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hengl et al. (2017) Tomislav Hengl, Jorge Mendes de Jesus, Gerard BM Heuvelink,
    Maria Ruiperez Gonzalez, Milan Kilibarda, Aleksandar Blagotić, Wei Shangguan,
    Marvin N Wright, Xiaoyuan Geng, Bernhard Bauer-Marschallinger, 等. 2017. SoilGrids250m:
    基于机器学习的全球网格土壤信息. *PLoS one* 12, 2 (2017), e0169748。'
- en: Hie et al. (2020) Brian Hie, Bryan D Bryson, and Bonnie Berger. 2020. Leveraging
    uncertainty in machine learning accelerates biological discovery and design. *Cell
    systems* 11, 5 (2020), 461–477.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hie et al. (2020) Brian Hie, Bryan D Bryson, 和 Bonnie Berger. 2020. 利用机器学习中的不确定性加速生物发现和设计.
    *细胞系统* 11, 5 (2020), 461–477。
- en: Hinton and Van Camp (1993) Geoffrey E Hinton and Drew Van Camp. 1993. Keeping
    the neural networks simple by minimizing the description length of the weights.
    In *Proceedings of the sixth annual conference on Computational learning theory*.
    5–13.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 和 Van Camp (1993) Geoffrey E Hinton 和 Drew Van Camp. 1993. 通过最小化权重描述长度来保持神经网络的简单性。收录于
    *第六届年度计算学习理论会议论文集*。5–13。
- en: Hiraoka et al. (2021) Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi
    Onishi, and Yoshimasa Tsuruoka. 2021. Dropout Q-Functions for Doubly Efficient
    Reinforcement Learning. *arXiv preprint arXiv:2110.02034* (2021).
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hiraoka 等 (2021) Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi
    Onishi 和 Yoshimasa Tsuruoka. 2021. 用于双重高效强化学习的 Dropout Q-函数。*arXiv 预印本 arXiv:2110.02034*（2021）。
- en: Jia et al. (2021) Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling,
    Samantha Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach,
    Jordan Read, et al. 2021. Physics-guided recurrent graph model for predicting
    flow and temperature in river networks. In *Proceedings of the 2021 SIAM International
    Conference on Data Mining (SDM)*. SIAM, 612–620.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2021) Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha
    Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach, Jordan
    Read 等. 2021. 基于物理指导的递归图模型用于预测河流网络中的流量和温度。收录于 *2021 年 SIAM 国际数据挖掘会议 (SDM) 论文集*。SIAM,
    612–620。
- en: Jiang (2018) Zhe Jiang. 2018. A survey on spatial prediction methods. *IEEE
    Transactions on Knowledge and Data Engineering* 31, 9 (2018), 1645–1664.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang (2018) Zhe Jiang. 2018. 空间预测方法综述。*IEEE 知识与数据工程学报* 31, 9 (2018), 1645–1664。
- en: Jiang et al. (2021) Zhe Jiang, Wenchong He, Marcus Kirby, Sultan Asiri, and
    Da Yan. 2021. Weakly Supervised Spatial Deep Learning based on Imperfect Vector
    Labels with Registration Errors. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*. 767–775.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2021) Zhe Jiang, Wenchong He, Marcus Kirby, Sultan Asiri 和 Da Yan.
    2021. 基于不完美向量标签与配准误差的弱监督空间深度学习。收录于 *第 27 届 ACM SIGKDD 知识发现与数据挖掘会议论文集*。767–775。
- en: Jiang et al. (2022a) Zhe Jiang, Wenchong He, Marcus Stephen Kirby, Arpan Man
    Sainju, Shaowen Wang, Lawrence V Stanislawski, Ethan J Shavers, and E Lynn Usery.
    2022a. Weakly Supervised Spatial Deep Learning for Earth Image Segmentation Based
    on Imperfect Polyline Labels. *ACM Transactions on Intelligent Systems and Technology
    (TIST)* 13, 2 (2022), 1–20.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2022a) Zhe Jiang, Wenchong He, Marcus Stephen Kirby, Arpan Man Sainju,
    Shaowen Wang, Lawrence V Stanislawski, Ethan J Shavers 和 E Lynn Usery. 2022a.
    基于不完美折线标签的弱监督空间深度学习用于地球图像分割。*ACM 智能系统与技术学报 (TIST)* 13, 2 (2022), 1–20。
- en: Jiang et al. (2019) Zhe Jiang, Arpan Man Sainju, Yan Li, Shashi Shekhar, and
    Joseph Knight. 2019. Spatial ensemble learning for heterogeneous geographic data
    with class ambiguity. *ACM Transactions on Intelligent Systems and Technology
    (TIST)* 10, 4 (2019), 1–25.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2019) Zhe Jiang, Arpan Man Sainju, Yan Li, Shashi Shekhar 和 Joseph
    Knight. 2019. 处理类模糊的异质地理数据的空间集成学习。*ACM 智能系统与技术学报 (TIST)* 10, 4 (2019), 1–25。
- en: 'Jiang and Shekhar (2017) Zhe Jiang and Shashi Shekhar. 2017. Spatial big data
    science. *Schweiz: Springer International Publishing AG* (2017).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 和 Shekhar (2017) Zhe Jiang 和 Shashi Shekhar. 2017. 《空间大数据科学》。*瑞士：Springer
    International Publishing AG*（2017）。
- en: 'Jiang et al. (2022b) Zhe Jiang, Liang Zhao, Xun Zhou, Robert N Stewart, Junbo
    Zhang, Shashi Shekhar, and Jieping Ye. 2022b. DeepSpatial’22: The 3rd International
    Workshop on Deep Learning for Spatiotemporal Data, Applications, and Systems.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*. 4878–4879.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 (2022b) Zhe Jiang, Liang Zhao, Xun Zhou, Robert N Stewart, Junbo Zhang,
    Shashi Shekhar 和 Jieping Ye. 2022b. DeepSpatial’22: 第三届国际深度学习空间时间数据、应用和系统研讨会。收录于
    *第 28 届 ACM SIGKDD 知识发现与数据挖掘会议论文集*。4878–4879。'
- en: Jospin et al. (2022) Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray
    Buntine, and Mohammed Bennamoun. 2022. Hands-on Bayesian neural networks—A tutorial
    for deep learning users. *IEEE Computational Intelligence Magazine* 17, 2 (2022),
    29–48.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jospin 等 (2022) Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine
    和 Mohammed Bennamoun. 2022. 实用贝叶斯神经网络——深度学习用户教程。*IEEE 计算智能杂志* 17, 2 (2022), 29–48。
- en: 'JSANG (2018) AUDUN. JSANG. 2018. *Subjective Logic: A formalism for reasoning
    under uncertainty*. Springer.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSANG (2018) AUDUN. JSANG. 2018. *主观逻辑：不确定性推理的形式化方法*。Springer。
- en: Karniadakis et al. (2021) George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu,
    Paris Perdikaris, Sifan Wang, and Liu Yang. 2021. Physics-informed machine learning.
    *Nature Reviews Physics* 3, 6 (2021), 422–440.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karniadakis 等 (2021) George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris
    Perdikaris, Sifan Wang 和 Liu Yang. 2021. 物理信息机器学习。*自然评论物理* 3, 6 (2021), 422–440。
- en: 'Kashinath et al. (2021) K Kashinath, M Mustafa, A Albert, JL Wu, C Jiang, S
    Esmaeilzadeh, K Azizzadenesheli, R Wang, A Chattopadhyay, A Singh, et al. 2021.
    Physics-informed machine learning: case studies for weather and climate modelling.
    *Philosophical Transactions of the Royal Society A* 379, 2194 (2021), 20200093.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kashinath et al. (2021) K·卡辛纳特、M·穆斯塔法、A·阿尔伯特、JL·吴、C·姜、S·埃斯梅伊尔扎德赫、K·阿齐兹代内什赫利、R·王、A·查托帕迪亚、A·辛格等。2021年。物理信息机器学习：天气和气候建模的案例研究。*皇家学会哲学会刊A*
    379, 2194 (2021), 20200093。
- en: Kendall and Gal (2017) Alex Kendall and Yarin Gal. 2017. What uncertainties
    do we need in bayesian deep learning for computer vision? *Advances in neural
    information processing systems* 30 (2017).
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall and Gal (2017) 亚历克斯·肯德尔和亚林·加尔。2017年。计算机视觉中的贝叶斯深度学习需要哪些不确定性？*神经信息处理系统进展*
    30 (2017)。
- en: 'Kim et al. (2019) Sookyung Kim, Hyojin Kim, Joonseok Lee, Sangwoong Yoon, Samira Ebrahimi
    Kahou, Karthik Kashinath, and Mr Prabhat. 2019. Deep-hurricane-tracker: Tracking
    and forecasting extreme climate events. In *2019 IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. IEEE, 1761–1769.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2019) 金素京、金孝真、李俊锡、尹相雄、萨米拉·伊布拉希米·卡侯、卡尔提克·卡辛纳特、普拉巴特。2019年。深度飓风追踪器：极端气候事件的跟踪与预测。发表于*2019
    IEEE冬季计算机视觉应用会议 (WACV)*。IEEE, 1761–1769。
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling (2013) 迪德里克·P·金马和马克斯·威林。2013年。自动编码变分贝叶斯。*arXiv预印本 arXiv:1312.6114*
    (2013)。
- en: Kohl et al. (2018) Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey
    De Fauw, Joseph R Ledsam, Klaus Maier-Hein, SM Eslami, Danilo Jimenez Rezende,
    and Olaf Ronneberger. 2018. A probabilistic u-net for segmentation of ambiguous
    images. *Advances in neural information processing systems* 31 (2018).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohl et al. (2018) 西蒙·科尔、贝尔纳迪诺·罗梅拉-帕雷德斯、克莱门斯·迈耶、杰弗里·德·福、约瑟夫·R·莱萨姆、克劳斯·迈尔-海因、SM·埃斯拉米、达尼洛·吉门内斯·雷泽德、奥拉夫·罗内伯格。2018年。一种用于模糊图像分割的概率u-net。*神经信息处理系统进展*
    31 (2018)。
- en: Krishnapriyan et al. (2021) Aditi Krishnapriyan, Amir Gholami, Shandian Zhe,
    Robert Kirby, and Michael W Mahoney. 2021. Characterizing possible failure modes
    in physics-informed neural networks. *Advances in Neural Information Processing
    Systems* 34 (2021), 26548–26560.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnapriyan et al. (2021) 阿迪提·克里什纳普里扬、阿米尔·戈拉米、山点·哲、罗伯特·柯比、迈克尔·W·马霍尼。2021年。物理信息神经网络中的可能失败模式特征化。*神经信息处理系统进展*
    34 (2021), 26548–26560。
- en: Kristiadi et al. (2021) Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
    2021. Learnable uncertainty under laplace approximations. In *Uncertainty in Artificial
    Intelligence*. PMLR, 344–353.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kristiadi et al. (2021) 阿古斯丁努斯·克里斯提亚迪、马蒂亚斯·海因、菲利普·亨尼。2021年。拉普拉斯近似下的可学习不确定性。发表于*人工智能中的不确定性*。PMLR,
    344–353。
- en: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and
    Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation
    using deep ensembles. *Advances in neural information processing systems* 30 (2017).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakshminarayanan et al. (2017) 巴拉吉·拉克什米纳拉扬、亚历山大·普里策尔、查尔斯·布伦德尔。2017年。使用深度集成的简单且可扩展的预测不确定性估计。*神经信息处理系统进展*
    30 (2017)。
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    learning. *nature* 521, 7553 (2015), 436–444.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (2015) 杨·勒昆、约书亚·本吉奥、杰弗里·辛顿。2015年。深度学习。*自然* 521, 7553 (2015), 436–444。
- en: Lee et al. (2020) Hong Joo Lee, Jung Uk Kim, Sangmin Lee, Hak Gu Kim, and Yong Man
    Ro. 2020. Structure boundary preserving segmentation for medical image with ambiguous
    boundary. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*. 4817–4826.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2020) 李洪柱、金正勋、李尚敏、金学久、罗永满。2020年。用于医学图像的结构边界保留分割。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。4817–4826。
- en: 'Lempert et al. (2022) Robert J Lempert, Steven W Popper, Carlos Calvo Hernandez,
    et al. 2022. *Transportation Planning for Uncertain Times: A Practical Guide to
    Decision Making Under Deep Uncertainty for MPOs*. Technical Report. United States.
    Federal Highway Administration.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lempert et al. (2022) 罗伯特·J·伦珀特、史蒂文·W·波普、卡洛斯·卡尔沃·埃尔南德斯等。2022年。*不确定时代的交通规划：面向MPOs的深度不确定性下的决策实用指南*。技术报告。美国。联邦公路管理局。
- en: Li et al. (2022) Yiqun Li, Songjian Chai, Guibin Wang, Xian Zhang, and Jing
    Qiu. 2022. Quantifying the Uncertainty in Long-Term Traffic Prediction Based on
    PI-ConvLSTM Network. *IEEE Transactions on Intelligent Transportation Systems*
    23, 11 (2022), 20429–20441.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) 李一群、柴松建、王贵宾、张贤、邱静。2022年。基于PI-ConvLSTM网络的长期交通预测不确定性量化。*IEEE智能交通系统汇刊*
    23, 11 (2022), 20429–20441。
- en: Li et al. (2020) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede
    Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2020. Fourier
    neural operator for parametric partial differential equations. *arXiv preprint
    arXiv:2010.08895* (2020).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu,
    Kaushik Bhattacharya, Andrew Stuart, 和 Anima Anandkumar。2020。用于参数偏微分方程的傅里叶神经算子。*arXiv
    预印本 arXiv:2010.08895*（2020）。
- en: 'Licata and Mehta (2022) Richard J Licata and Piyush M Mehta. 2022. Uncertainty
    quantification techniques for data-driven space weather modeling: thermospheric
    density application. *Scientific Reports* 12, 1 (2022), 7256.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Licata 和 Mehta（2022）Richard J Licata 和 Piyush M Mehta。2022。数据驱动空间天气建模的不确定性量化技术：热层密度应用。*科学报告*
    12, 1（2022），7256。
- en: Liu et al. (2020) Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss,
    and Balaji Lakshminarayanan. 2020. Simple and principled uncertainty estimation
    with deterministic deep learning via distance awareness. *Advances in Neural Information
    Processing Systems* 33 (2020), 7498–7512.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss,
    和 Balaji Lakshminarayanan。2020。通过距离感知进行简单而有原则的确定性深度学习不确定性估计。*神经信息处理系统进展* 33（2020），7498–7512。
- en: Liu et al. (2022) Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen,
    Ghassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran, and Balaji Lakshminarayanan.
    2022. A simple approach to improve single-model deep uncertainty via distance-awareness.
    *Journal of Machine Learning Research* 23 (2022), 1–63.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen
    Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran, 和 Balaji Lakshminarayanan。2022。通过距离感知改进单模型深度不确定性的简单方法。*机器学习研究杂志*
    23（2022），1–63。
- en: 'Loftus et al. (2022) Tyler J Loftus, Benjamin Shickel, Matthew M Ruppert, Jeremy A
    Balch, Tezcan Ozrazgat-Baslanti, Patrick J Tighe, Philip A Efron, William R Hogan,
    Parisa Rashidi, Gilbert R Upchurch Jr, et al. 2022. Uncertainty-aware deep learning
    in healthcare: a scoping review. *PLOS digital health* 1, 8 (2022), e0000085.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loftus 等（2022）Tyler J Loftus, Benjamin Shickel, Matthew M Ruppert, Jeremy A
    Balch, Tezcan Ozrazgat-Baslanti, Patrick J Tighe, Philip A Efron, William R Hogan,
    Parisa Rashidi, Gilbert R Upchurch Jr, 等。2022。医疗保健中的不确定性感知深度学习：范围评估。*PLOS 数字健康*
    1, 8（2022），e0000085。
- en: Louizos and Welling (2017) Christos Louizos and Max Welling. 2017. Multiplicative
    normalizing flows for variational bayesian neural networks. In *International
    Conference on Machine Learning*. PMLR, 2218–2227.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louizos 和 Welling（2017）Christos Louizos 和 Max Welling。2017。用于变分贝叶斯神经网络的乘法归一化流。在*国际机器学习会议*。PMLR，2218–2227。
- en: MacKay (1992) David JC MacKay. 1992. A practical Bayesian framework for backpropagation
    networks. *Neural computation* 4, 3 (1992), 448–472.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacKay（1992）David JC MacKay。1992。用于反向传播网络的实用贝叶斯框架。*神经计算* 4, 3（1992），448–472。
- en: Malinin and Gales (2018) Andrey Malinin and Mark Gales. 2018. Predictive uncertainty
    estimation via prior networks. *Advances in neural information processing systems*
    31 (2018).
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin 和 Gales（2018）Andrey Malinin 和 Mark Gales。2018。通过先验网络进行预测性不确定性估计。*神经信息处理系统进展*
    31（2018）。
- en: Mallick et al. (2022) Tanwi Mallick, Prasanna Balaprakash, and Jane Macfarlane.
    2022. Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph Neural
    Networks for Traffic Forecasting. *arXiv preprint arXiv:2204.01618* (2022).
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallick 等（2022）Tanwi Mallick, Prasanna Balaprakash, 和 Jane Macfarlane。2022。基于深度集成的不确定性量化用于交通预测中的时空图神经网络。*arXiv
    预印本 arXiv:2204.01618*（2022）。
- en: Markos et al. (2021) Christos Markos, JQ James, and Richard Yi Da Xu. 2021.
    Capturing uncertainty in unsupervised GPS trajectory segmentation using Bayesian
    deep learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 35\. 390–398.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markos 等（2021）Christos Markos, JQ James, 和 Richard Yi Da Xu。2021。使用贝叶斯深度学习捕捉无监督
    GPS 轨迹分割中的不确定性。在*AAAI 人工智能会议论文集*，第 35 卷。390–398。
- en: Mavor-Parker et al. (2022) Augustine Mavor-Parker, Kimberly Young, Caswell Barry,
    and Lewis Griffin. 2022. How to stay curious while avoiding noisy tvs using aleatoric
    uncertainty estimation. In *International Conference on Machine Learning*. PMLR,
    15220–15240.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mavor-Parker 等（2022）Augustine Mavor-Parker, Kimberly Young, Caswell Barry, 和
    Lewis Griffin。2022。在避免嘈杂电视的同时保持好奇心，使用随机不确定性估计。在*国际机器学习会议*。PMLR，15220–15240。
- en: Mena et al. (2021) José Mena, Oriol Pujol, and Jordi Vitrià. 2021. A survey
    on uncertainty estimation in deep learning classification systems from a bayesian
    perspective. *ACM Computing Surveys (CSUR)* 54, 9 (2021), 1–35.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mena 等（2021）José Mena, Oriol Pujol, 和 Jordi Vitrià。2021。从贝叶斯视角对深度学习分类系统中的不确定性估计进行调查。*ACM
    计算调查（CSUR）* 54, 9（2021），1–35。
- en: 'Mishkin et al. (2018) Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark
    Schmidt, and Mohammad Emtiyaz Khan. 2018. Slang: Fast structured covariance approximations
    for bayesian deep learning with natural gradient. *Advances in Neural Information
    Processing Systems* 31 (2018).'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishkin 等（2018）Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt
    和 Mohammad Emtiyaz Khan。2018年。Slang：用于贝叶斯深度学习的自然梯度下的快速结构化协方差近似。*神经信息处理系统进展* 31（2018）。
- en: 'Mo and Fu (2022) Zhaobin Mo and Yongjie Fu. 2022. TrafficFlowGAN: Physics-informed
    Flow based Generative Adversarial Network for Uncertainty Quantification. In *European
    Conference on Machine Learning and Data Mining (ECML PKDD)*.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo 和 Fu（2022）Zhaobin Mo 和 Yongjie Fu。2022年。TrafficFlowGAN：基于物理的流量生成对抗网络用于不确定性量化。在
    *欧洲机器学习与数据挖掘会议（ECML PKDD）*。
- en: 'Mowbray et al. (2021) Max Mowbray, Thomas Savage, Chufan Wu, Ziqi Song, Bovinille Anye
    Cho, Ehecatl A Del Rio-Chanona, and Dongda Zhang. 2021. Machine learning for biochemical
    engineering: A review. *Biochemical Engineering Journal* 172 (2021), 108054.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mowbray 等（2021）Max Mowbray, Thomas Savage, Chufan Wu, Ziqi Song, Bovinille Anye
    Cho, Ehecatl A Del Rio-Chanona 和 Dongda Zhang。2021年。生物化学工程中的机器学习：综述。*生物化学工程期刊*
    172（2021），108054。
- en: 'Murphy (2012) Kevin P Murphy. 2012. *Machine learning: a probabilistic perspective*.
    MIT press.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy（2012）Kevin P Murphy。2012年。*机器学习：概率视角*。MIT出版社。
- en: NASA Goddard (2019) Space Flight Center NASA Goddard. 2019. Sea-viewing Wide
    Field-of-view Sensor (SeaWiFS) Ocean Color Data. (2019). [https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NASA 戈达德（2019）空间飞行中心 NASA 戈达德。2019年。海洋观测宽视场传感器（SeaWiFS）海洋颜色数据。（2019）。[https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
- en: 'Natekar et al. (2020) Parth Natekar, Avinash Kori, and Ganapathy Krishnamurthi.
    2020. Demystifying brain tumor segmentation networks: interpretability and uncertainty
    analysis. *Frontiers in computational neuroscience* 14 (2020), 6.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Natekar 等（2020）Parth Natekar, Avinash Kori 和 Ganapathy Krishnamurthi。2020年。解密脑肿瘤分割网络：可解释性和不确定性分析。*计算神经科学前沿*
    14（2020），6。
- en: Neal (2012) Radford M Neal. 2012. *Bayesian learning for neural networks*. Vol. 118.
    Springer Science & Business Media.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neal（2012）Radford M Neal。2012年。*神经网络的贝叶斯学习*。第118卷。Springer Science & Business
    Media。
- en: Nguyen et al. (2022a) Andre T Nguyen, Fred Lu, Gary Lopez Munoz, Edward Raff,
    Charles Nicholas, and James Holt. 2022a. Out of Distribution Data Detection Using
    Dropout Bayesian Neural Networks. *arXiv preprint arXiv:2202.08985* (2022).
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2022a）Andre T Nguyen, Fred Lu, Gary Lopez Munoz, Edward Raff, Charles
    Nicholas 和 James Holt。2022a。使用 dropout 贝叶斯神经网络进行分布外数据检测。*arXiv 预印本 arXiv:2202.08985*（2022）。
- en: Nguyen et al. (2022b) Vu-Linh Nguyen, Mohammad Hossein Shaker, and Eyke Hüllermeier.
    2022b. How to measure uncertainty in uncertainty sampling for active learning.
    *Machine Learning* 111, 1 (2022), 89–122.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2022b）Vu-Linh Nguyen, Mohammad Hossein Shaker 和 Eyke Hüllermeier。2022b。如何测量主动学习中的不确定性抽样中的不确定性。*机器学习*
    111, 1（2022），89–122。
- en: Nixon et al. (2019) Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen
    Jerfel, and Dustin Tran. 2019. Measuring Calibration in Deep Learning.. In *CVPR
    workshops*, Vol. 2.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nixon 等（2019）Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel
    和 Dustin Tran。2019年。深度学习中的测量校准。见于 *CVPR 研讨会*，第2卷。
- en: 'Oberdiek et al. (2022) Philipp Oberdiek, Gernot A Fink, and Matthias Rottmann.
    2022. UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers
    trained via Conditional GANs. *arXiv preprint arXiv:2201.13279* (2022).'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oberdiek 等（2022）Philipp Oberdiek, Gernot A Fink 和 Matthias Rottmann。2022年。UQGAN：通过条件
    GAN 训练的深度分类器的不确定性量化统一模型。*arXiv 预印本 arXiv:2201.13279*（2022）。
- en: Oh et al. (2020) Shu Lih Oh, Yuki Hagiwara, U Raghavendra, Rajamanickam Yuvaraj,
    N Arunkumar, M Murugappan, and U Rajendra Acharya. 2020. A deep learning approach
    for Parkinson’s disease diagnosis from EEG signals. *Neural Computing and Applications*
    32, 15 (2020), 10927–10933.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等（2020）Shu Lih Oh, Yuki Hagiwara, U Raghavendra, Rajamanickam Yuvaraj, N
    Arunkumar, M Murugappan 和 U Rajendra Acharya。2020年。一种基于深度学习的帕金森病诊断方法，基于 EEG 信号。*神经计算与应用*
    32, 15（2020），10927–10933。
- en: Osband et al. (2018) Ian Osband, John Aslanides, and Albin Cassirer. 2018. Randomized
    prior functions for deep reinforcement learning. *Advances in Neural Information
    Processing Systems* 31 (2018).
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 等（2018）Ian Osband, John Aslanides 和 Albin Cassirer。2018年。深度强化学习的随机先验函数。*神经信息处理系统进展*
    31（2018）。
- en: Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David
    Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper
    Snoek. 2019. Can you trust your model’s uncertainty? evaluating predictive uncertainty
    under dataset shift. *Advances in neural information processing systems* 32 (2019).
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ovadia 等人（2019）Yaniv Ovadia、Emily Fertig、Jie Ren、Zachary Nado、David Sculley、Sebastian
    Nowozin、Joshua Dillon、Balaji Lakshminarayanan 和 Jasper Snoek。2019。你能相信模型的不确定性吗？在数据集变化下评估预测不确定性。*神经信息处理系统进展*
    32 (2019)。
- en: Pandey et al. (2022) Mohit Pandey, Michael Fernandez, Francesco Gentile, Olexandr
    Isayev, Alexander Tropsha, Abraham C Stern, and Artem Cherkasov. 2022. The transformational
    role of GPU computing and deep learning in drug discovery. *Nature Machine Intelligence*
    4, 3 (2022), 211–221.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandey 等人（2022）Mohit Pandey、Michael Fernandez、Francesco Gentile、Olexandr Isayev、Alexander
    Tropsha、Abraham C Stern 和 Artem Cherkasov。2022。GPU 计算和深度学习在药物发现中的变革性作用。*自然机器智能*
    4, 3 (2022)，211–221。
- en: 'Pang et al. (2021) Yutian Pang, Xinyu Zhao, Hao Yan, and Yongming Liu. 2021.
    Data-driven trajectory prediction with weather uncertainties: A Bayesian deep
    learning approach. *Transportation Research Part C: Emerging Technologies* 130
    (2021), 103326.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等人（2021）Yutian Pang、Xinyu Zhao、Hao Yan 和 Yongming Liu。2021。基于数据的轨迹预测与天气不确定性：一种贝叶斯深度学习方法。*运输研究
    C 部分：新兴技术* 130 (2021)，103326。
- en: 'Pearce et al. (2018) Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy
    Neely. 2018. High-quality prediction intervals for deep learning: A distribution-free,
    ensembled approach. In *International conference on machine learning*. PMLR, 4075–4084.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce 等人（2018）Tim Pearce、Alexandra Brintrup、Mohamed Zaki 和 Andy Neely。2018。深度学习的高质量预测区间：一种无分布、集成的方法。在
    *国际机器学习大会*。PMLR，4075–4084。
- en: Pfoser and Jensen (1999) Dieter Pfoser and Christian S Jensen. 1999. Capturing
    the uncertainty of moving-object representations. In *International Symposium
    on Spatial Databases*. Springer, 111–131.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfoser 和 Jensen（1999）Dieter Pfoser 和 Christian S Jensen。1999。捕捉移动对象表示的不确定性。在
    *空间数据库国际研讨会*。Springer，111–131。
- en: Posch and Pilz (2020) Konstantin Posch and Juergen Pilz. 2020. Correlated parameters
    to accurately measure uncertainty in deep neural networks. *IEEE Transactions
    on Neural Networks and Learning Systems* 32, 3 (2020), 1037–1051.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Posch 和 Pilz（2020）Konstantin Posch 和 Juergen Pilz。2020。相关参数以准确测量深度神经网络中的不确定性。*IEEE
    神经网络与学习系统汇刊* 32, 3 (2020)，1037–1051。
- en: Posch et al. (2019) Konstantin Posch, Jan Steinbrener, and Jürgen Pilz. 2019.
    Variational inference to measure model uncertainty in deep neural networks. *arXiv
    preprint arXiv:1902.10189* (2019).
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Posch 等人（2019）Konstantin Posch、Jan Steinbrener 和 Jürgen Pilz。2019。变分推断用于测量深度神经网络中的模型不确定性。*arXiv
    预印本 arXiv:1902.10189* (2019)。
- en: 'Prokudin et al. (2018) Sergey Prokudin, Peter Gehler, and Sebastian Nowozin.
    2018. Deep directional statistics: Pose estimation with uncertainty quantification.
    In *Proceedings of the European conference on computer vision (ECCV)*. 534–551.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prokudin 等人（2018）Sergey Prokudin、Peter Gehler 和 Sebastian Nowozin。2018。深度方向统计：带有不确定性量化的姿态估计。在
    *欧洲计算机视觉会议（ECCV）论文集*。534–551。
- en: Qin et al. (2021) Yu Qin, Zhiwen Liu, Chenghao Liu, Yuxing Li, Xiangzhu Zeng,
    and Chuyang Ye. 2021. Super-Resolved q-Space deep learning with uncertainty quantification.
    *Medical Image Analysis* 67 (2021), 101885.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2021）Yu Qin、Zhiwen Liu、Chenghao Liu、Yuxing Li、Xiangzhu Zeng 和 Chuyang
    Ye。2021。超分辨率 q-空间深度学习与不确定性量化。*医学图像分析* 67 (2021)，101885。
- en: Raj and Bach (2022) Anant Raj and Francis Bach. 2022. Convergence of uncertainty
    sampling for active learning. In *International Conference on Machine Learning*.
    PMLR, 18310–18331.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raj 和 Bach（2022）Anant Raj 和 Francis Bach。2022。不确定性采样在主动学习中的收敛性。在 *国际机器学习大会*。PMLR，18310–18331。
- en: Reichstein et al. (2019) Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens,
    Martin Jung, Joachim Denzler, and Nuno Carvalhais. 2019. Deep learning and process
    understanding for data-driven Earth system science. *Nature* 566, 7743 (2019),
    195–204.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reichstein 等人（2019）Markus Reichstein、Gustau Camps-Valls、Bjorn Stevens、Martin
    Jung、Joachim Denzler 和 Nuno Carvalhais。2019。深度学习和过程理解用于数据驱动的地球系统科学。*自然* 566, 7743
    (2019)，195–204。
- en: Ren et al. (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui
    Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active
    learning. *ACM computing surveys (CSUR)* 54, 9 (2021), 1–40.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2021）Pengzhen Ren、Yun Xiao、Xiaojun Chang、Po-Yao Huang、Zhihui Li、Brij
    B Gupta、Xiaojiang Chen 和 Xin Wang。2021。深度主动学习的调查。*ACM 计算机调查* 54, 9 (2021)，1–40。
- en: Ritter et al. (2018a) Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018a.
    Online structured laplace approximations for overcoming catastrophic forgetting.
    In *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*. 3742–3752.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter 等（2018a）Hippolyt Ritter, Aleksandar Botev, 和 David Barber. 2018a. 在线结构化拉普拉斯近似以克服灾难性遗忘。发表于
    *第32届国际神经信息处理系统大会论文集*。3742–3752。
- en: Ritter et al. (2018b) Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018b.
    A scalable laplace approximation for neural networks. In *6th International Conference
    on Learning Representations, ICLR 2018-Conference Track Proceedings*, Vol. 6\.
    International Conference on Representation Learning.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter 等（2018b）Hippolyt Ritter, Aleksandar Botev, 和 David Barber. 2018b. 用于神经网络的可扩展拉普拉斯近似。发表于
    *第6届国际学习表征会议，ICLR 2018-会议轨道论文集*，第6卷。国际表征学习会议。
- en: Ritter et al. (2021) Hippolyt Ritter, Martin Kukla, Cheng Zhang, and Yingzhen
    Li. 2021. Sparse Uncertainty Representation in Deep Learning with Inducing Weights.
    *Advances in Neural Information Processing Systems* 34 (2021), 6515–6528.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter 等（2021）Hippolyt Ritter, Martin Kukla, Cheng Zhang, 和 Yingzhen Li. 2021.
    深度学习中的稀疏不确定性表示与诱导权重。*神经信息处理系统进展* 34（2021），6515–6528。
- en: Rosenfeld et al. (2018) Nir Rosenfeld, Yishay Mansour, and Elad Yom-Tov. 2018.
    Discriminative learning of prediction intervals. In *International Conference
    on Artificial Intelligence and Statistics*. PMLR, 347–355.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenfeld 等（2018）Nir Rosenfeld, Yishay Mansour, 和 Elad Yom-Tov. 2018. 预测区间的辨别学习。发表于
    *国际人工智能与统计会议*。PMLR，347–355。
- en: 'Salem et al. (2020) Tárik S Salem, Helge Langseth, and Heri Ramampiaro. 2020.
    Prediction intervals: Split normal mixture from quality-driven deep ensembles.
    In *Conference on Uncertainty in Artificial Intelligence*. PMLR, 1179–1187.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salem 等（2020）Tárik S Salem, Helge Langseth, 和 Heri Ramampiaro. 2020. 预测区间：从质量驱动的深度集成中分裂的正态混合模型。发表于
    *人工智能不确定性会议*。PMLR，1179–1187。
- en: 'Salimans et al. (2015) Tim Salimans, Diederik Kingma, and Max Welling. 2015.
    Markov chain monte carlo and variational inference: Bridging the gap. In *International
    conference on machine learning*. PMLR, 1218–1226.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等（2015）Tim Salimans, Diederik Kingma, 和 Max Welling. 2015. 马尔可夫链蒙特卡洛与变分推断：弥合差距。发表于
    *国际机器学习会议*。PMLR，1218–1226。
- en: Schwaiger et al. (2020) Adrian Schwaiger, Poulami Sinhamahapatra, Jens Gansloser,
    and Karsten Roscher. 2020. Is uncertainty quantification in deep learning sufficient
    for out-of-distribution detection?. In *Aisafety@ ijcai*.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwaiger 等（2020）Adrian Schwaiger, Poulami Sinhamahapatra, Jens Gansloser, 和
    Karsten Roscher. 2020. 深度学习中的不确定性量化是否足够用于分布外检测？发表于 *Aisafety@ ijcai*。
- en: Sensoy et al. (2018) Murat Sensoy, Lance Kaplan, and Melih Kandemir. 2018. Evidential
    deep learning to quantify classification uncertainty. *Advances in neural information
    processing systems* 31 (2018).
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy 等（2018）Murat Sensoy, Lance Kaplan, 和 Melih Kandemir. 2018. 证据深度学习用于量化分类不确定性。*神经信息处理系统进展*
    31（2018）。
- en: 'Shafaei et al. (2018) Sina Shafaei, Stefan Kugele, Mohd Hafeez Osman, and Alois
    Knoll. 2018. Uncertainty in machine learning: A safety perspective on autonomous
    driving. In *Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops,
    ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018,
    Proceedings 37*. Springer, 458–464.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shafaei 等（2018）Sina Shafaei, Stefan Kugele, Mohd Hafeez Osman, 和 Alois Knoll.
    2018. 机器学习中的不确定性：自动驾驶的安全视角。发表于 *计算机安全、可靠性和安全性：SAFECOMP 2018 研讨会，ASSURE，DECSoS，SASSUR，STRIVE，和
    WAISE，瑞典 Västerås，2018年9月18日，会议录第37期*。Springer，458–464。
- en: 'Shekhar et al. (2015) Shashi Shekhar, Zhe Jiang, Reem Y Ali, Emre Eftelioglu,
    Xun Tang, Venkata MV Gunturi, and Xun Zhou. 2015. Spatiotemporal data mining:
    A computational perspective. *ISPRS International Journal of Geo-Information*
    4, 4 (2015), 2306–2338.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shekhar 等（2015）Shashi Shekhar, Zhe Jiang, Reem Y Ali, Emre Eftelioglu, Xun Tang,
    Venkata MV Gunturi, 和 Xun Zhou. 2015. 时空数据挖掘：一种计算视角。*ISPRS 国际地理信息杂志* 4, 4（2015），2306–2338。
- en: Shih et al. (2022) Shu-Fu Shih, Sevgi Gokce Kafali, Kara L Calkins, and Holden H
    Wu. 2022. Uncertainty-aware physics-driven deep learning network for free-breathing
    liver fat and R2* quantification using self-gated stack-of-radial MRI. *Magnetic
    Resonance in Medicine* (2022).
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shih 等（2022）Shu-Fu Shih, Sevgi Gokce Kafali, Kara L Calkins, 和 Holden H Wu.
    2022. 面向不确定性的物理驱动深度学习网络，用于使用自激励的径向堆叠 MRI 进行自由呼吸的肝脏脂肪和 R2* 量化。*医学磁共振*（2022）。
- en: Snelson and Ghahramani (2005) Edward Snelson and Zoubin Ghahramani. 2005. Sparse
    Gaussian processes using pseudo-inputs. *Advances in neural information processing
    systems* 18 (2005).
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snelson 和 Ghahramani（2005）Edward Snelson 和 Zoubin Ghahramani. 2005. 使用伪输入的稀疏高斯过程。*神经信息处理系统进展*
    18（2005）。
- en: Sohn et al. (2015) Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning
    structured output representation using deep conditional generative models. *Advances
    in neural information processing systems* 28 (2015).
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn et al. (2015) Kihyuk Sohn, Honglak Lee, 和 Xinchen Yan. 2015. 使用深度条件生成模型学习结构化输出表示。*神经信息处理系统进展*
    28 (2015)。
- en: Sun et al. (2021) Jian Sun, Kristopher A Innanen, and Chao Huang. 2021. Physics-guided
    deep learning for seismic inversion with hybrid training and uncertainty analysis.
    *Geophysics* 86, 3 (2021), R303–R317.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2021) Jian Sun, Kristopher A Innanen, 和 Chao Huang. 2021. 物理指导的深度学习用于地震反演，具有混合训练和不确定性分析。*地球物理学*
    86, 3 (2021), R303–R317。
- en: Sun et al. (2017) Shengyang Sun, Changyou Chen, and Lawrence Carin. 2017. Learning
    structured weight uncertainty in bayesian neural networks. In *Artificial Intelligence
    and Statistics*. PMLR, 1283–1292.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2017) Shengyang Sun, Changyou Chen, 和 Lawrence Carin. 2017. 在贝叶斯神经网络中学习结构化权重不确定性。发表于
    *人工智能与统计*。PMLR, 1283–1292。
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    learning: An introduction*. MIT press.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton and Barto (2018) Richard S Sutton 和 Andrew G Barto. 2018. *强化学习：导论*。MIT出版社。
- en: 'Swiatkowski et al. (2020) Jakub Swiatkowski, Kevin Roth, Bastiaan Veeling,
    Linh Tran, Joshua Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe
    Jenatton, and Sebastian Nowozin. 2020. The k-tied normal distribution: A compact
    parameterization of Gaussian mean field posteriors in Bayesian neural networks.
    In *International Conference on Machine Learning*. PMLR, 9289–9299.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swiatkowski et al. (2020) Jakub Swiatkowski, Kevin Roth, Bastiaan Veeling, Linh
    Tran, Joshua Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe Jenatton,
    和 Sebastian Nowozin. 2020. k-绑正常分布：贝叶斯神经网络中高斯均值场后验的紧凑参数化。发表于 *国际机器学习会议*。PMLR,
    9289–9299。
- en: 'Thulasidasan et al. (2019) Sunil Thulasidasan, Gopinath Chennupati, Jeff A
    Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved
    calibration and predictive uncertainty for deep neural networks. *Advances in
    Neural Information Processing Systems* 32 (2019).'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thulasidasan et al. (2019) Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes,
    Tanmoy Bhattacharya, 和 Sarah Michalak. 2019. 关于mixup训练：改善深度神经网络的校准和预测不确定性。*神经信息处理系统进展*
    32 (2019)。
- en: Titsias (2009) Michalis Titsias. 2009. Variational learning of inducing variables
    in sparse Gaussian processes. In *Artificial intelligence and statistics*. PMLR,
    567–574.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Titsias (2009) Michalis Titsias. 2009. 稀疏高斯过程中的诱导变量变分学习。发表于 *人工智能与统计*。PMLR,
    567–574。
- en: Ustyuzhaninov et al. (2020) Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser,
    Erik Bodin, Neill Campbell, and Carl Henrik Ek. 2020. Compositional uncertainty
    in deep Gaussian processes. In *Conference on Uncertainty in Artificial Intelligence*.
    PMLR, 480–489.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ustyuzhaninov et al. (2020) Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser,
    Erik Bodin, Neill Campbell, 和 Carl Henrik Ek. 2020. 深度高斯过程中的组成不确定性。发表于 *人工智能不确定性会议*。PMLR,
    480–489。
- en: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, and Yarin Gal. 2021. On Feature Collapse and Deep Kernel Learning for
    Single Forward Pass Uncertainty. *arXiv preprint arXiv:2102.11409* (2021).
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, 和 Yarin Gal. 2021. 关于单次前向传播不确定性的特征崩溃与深度核学习。*arXiv 预印本 arXiv:2102.11409*
    (2021)。
- en: Van Amersfoort et al. (2020) Joost Van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. 2020. Uncertainty estimation using a single deep deterministic
    neural network. In *International conference on machine learning*. PMLR, 9690–9700.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Amersfoort et al. (2020) Joost Van Amersfoort, Lewis Smith, Yee Whye Teh,
    和 Yarin Gal. 2020. 使用单个深度确定性神经网络进行不确定性估计。发表于 *国际机器学习会议*。PMLR, 9690–9700。
- en: Van Tulder and De Bruijne (2016) Gijs Van Tulder and Marleen De Bruijne. 2016.
    Combining generative and discriminative representation learning for lung CT analysis
    with convolutional restricted Boltzmann machines. *IEEE transactions on medical
    imaging* 35, 5 (2016), 1262–1272.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Tulder and De Bruijne (2016) Gijs Van Tulder 和 Marleen De Bruijne. 2016.
    结合生成与判别表示学习用于肺部CT分析的卷积限制玻尔兹曼机。*IEEE医学成像汇刊* 35, 5 (2016), 1262–1272。
- en: 'Virmaux and Scaman (2018) Aladin Virmaux and Kevin Scaman. 2018. Lipschitz
    regularity of deep neural networks: analysis and efficient estimation. *Advances
    in Neural Information Processing Systems* 31 (2018).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virmaux and Scaman (2018) Aladin Virmaux 和 Kevin Scaman. 2018. 深度神经网络的Lipschitz正则性：分析与高效估计。*神经信息处理系统进展*
    31 (2018)。
- en: 'Vu and Thai (2020) Minh Vu and My T Thai. 2020. Pgm-explainer: Probabilistic
    graphical model explanations for graph neural networks. *Advances in neural information
    processing systems* 33 (2020), 12225–12235.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vu 和 Thai（2020）敏·Vu 和 My T·Thai。2020。PGM-explainer：图神经网络的概率图模型解释。*神经信息处理系统进展*
    33（2020），12225–12235。
- en: 'Wang et al. (2019b) Bin Wang, Jie Lu, Zheng Yan, Huaishao Luo, Tianrui Li,
    Yu Zheng, and Guangquan Zhang. 2019b. Deep uncertainty quantification: A machine
    learning approach for weather forecasting. In *Proceedings of the 25th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 2087–2095.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019b）汪斌、陆杰、严征、罗怀韶、李天瑞、郑宇和张广泉。2019b。深度不确定性量化：一种用于天气预测的机器学习方法。在 *第25届
    ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。2087–2095。
- en: 'Wang et al. (2014) Jian Wang, Wei Deng, and Yuntao Guo. 2014. New Bayesian
    combination method for short-term traffic flow forecasting. *Transportation Research
    Part C: Emerging Technologies* 43 (2014), 79–94.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2014）简·王、魏·邓和云涛·郭。2014。用于短期交通流量预测的新贝叶斯组合方法。*运输研究C部分：新兴技术* 43（2014），79–94。
- en: 'Wang et al. (2019a) Pengyue Wang, Yan Li, Shashi Shekhar, and William F Northrop.
    2019a. Uncertainty estimation with distributional reinforcement learning for applications
    in intelligent transportation systems: A case study. In *2019 IEEE Intelligent
    Transportation Systems Conference (ITSC)*. IEEE, 3822–3827.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019a）彭跃王、李燕、沙希·谢卡和威廉·F·诺斯罗普。2019a。利用分布式强化学习进行不确定性估计在智能交通系统中的应用：一个案例研究。在
    *2019 IEEE 智能交通系统大会（ITSC）*。IEEE，3822–3827。
- en: Wenzel et al. (2020) Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe
    Jenatton. 2020. Hyperparameter ensembles for robustness and uncertainty quantification.
    *Advances in Neural Information Processing Systems* 33 (2020), 6514–6527.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wenzel 等（2020）弗洛里安·温策尔、贾斯帕·斯诺克、达斯汀·特兰和罗多尔夫·杰纳通。2020。用于鲁棒性和不确定性量化的超参数集合。*神经信息处理系统进展*
    33（2020），6514–6527。
- en: Williams and Rasmussen (2006) Christopher KI Williams and Carl Edward Rasmussen.
    2006. *Gaussian processes for machine learning*. Vol. 2. MIT press Cambridge,
    MA.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams 和 Rasmussen（2006）克里斯托弗·KI·威廉姆斯和卡尔·爱德华·拉斯穆森。2006。*机器学习中的高斯过程*。第2卷。麻省理工学院出版社，剑桥，MA。
- en: Wilson and Nickisch (2015) Andrew Wilson and Hannes Nickisch. 2015. Kernel interpolation
    for scalable structured Gaussian processes (KISS-GP). In *International conference
    on machine learning*. PMLR, 1775–1784.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson 和 Nickisch（2015）安德鲁·威尔逊和汉内斯·尼基施。2015。可扩展结构高斯过程的核插值（KISS-GP）。在 *国际机器学习大会*。PMLR，1775–1784。
- en: Wilson et al. (2016) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov,
    and Eric P Xing. 2016. Deep kernel learning. In *Artificial intelligence and statistics*.
    PMLR, 370–378.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson 等（2016）安德鲁·戈登·威尔逊、朱婷·胡、鲁斯兰·萨拉胡丁诺夫和埃里克·P·邢。2016。深度核学习。在 *人工智能与统计学*。PMLR，370–378。
- en: Wolf et al. (2016) Christopher Wolf, Maximilian Karl, and Patrick van der Smagt.
    2016. Variational inference with hamiltonian monte carlo. *arXiv preprint arXiv:1609.08203*
    (2016).
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等（2016）克里斯托弗·沃尔夫、马克西米利安·卡尔和帕特里克·范·德·斯马赫特。2016。使用哈密顿蒙特卡洛的变分推断。*arXiv 预印本
    arXiv:1609.08203*（2016）。
- en: Wu et al. (2021) Dongxia Wu, Liyao Gao, Matteo Chinazzi, Xinyue Xiong, Alessandro
    Vespignani, Yi-An Ma, and Rose Yu. 2021. Quantifying Uncertainty in Deep Spatiotemporal
    Forecasting. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*. 1841–1851.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2021）吴冬霞、高力尧、马泰奥·奇纳齐、熊鑫月、亚历山德罗·维斯皮尼亚尼、马义安和玫瑰·余。2021。深度时空预测中的不确定性量化。在 *第27届
    ACM SIGKDD 知识发现与数据挖掘会议论文集*。1841–1851。
- en: Yang and Perdikaris (2019) Yibo Yang and Paris Perdikaris. 2019. Adversarial
    uncertainty quantification in physics-informed neural networks. *J. Comput. Phys.*
    394 (2019), 136–152.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 和 Perdikaris（2019）杨亿博和帕里斯·佩尔迪卡里斯。2019。物理信息神经网络中的对抗性不确定性量化。*计算物理学杂志* 394（2019），136–152。
- en: Yarin (2016) Gal Yarin. 2016. Uncertainty in deep learning. *University of Cambridge,
    Cambridge* (2016).
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarin（2016）盖尔·雅林。2016。深度学习中的不确定性。*剑桥大学，剑桥*（2016）。
- en: You et al. (2017) Jiaxuan You, Xiaocheng Li, Melvin Low, David Lobell, and Stefano
    Ermon. 2017. Deep gaussian process for crop yield prediction based on remote sensing
    data. In *Thirty-First AAAI conference on artificial intelligence*.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等（2017）贾轩·游、肖程·李、梅尔文·洛、戴维·洛贝尔和斯特凡诺·埃尔蒙。2017。基于遥感数据的作物产量预测深度高斯过程。在 *第三十一届
    AAAI 人工智能大会*。
- en: Zhang et al. (2018) Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger
    Grosse. 2018. Noisy natural gradient as variational inference. In *International
    Conference on Machine Learning*. PMLR, 5852–5861.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) Guodong Zhang, Shengyang Sun, David Duvenaud, 和 Roger Grosse.
    2018. Noisy natural gradient as variational inference. 见于*国际机器学习会议*。PMLR, 5852–5861。
- en: 'Zhang et al. (2020) Xianli Zhang, Buyue Qian, Shilei Cao, Yang Li, Hang Chen,
    Yefeng Zheng, and Ian Davidson. 2020. INPREM: An interpretable and trustworthy
    predictive model for healthcare. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 450–460.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020) Xianli Zhang, Buyue Qian, Shilei Cao, Yang Li, Hang Chen,
    Yefeng Zheng, 和 Ian Davidson. 2020. INPREM: An interpretable and trustworthy predictive
    model for healthcare. 见于*第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。450–460。'
- en: 'Zhao et al. (2019) Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao
    Lin, Min Deng, and Haifeng Li. 2019. T-gcn: A temporal graph convolutional network
    for traffic prediction. *IEEE Transactions on Intelligent Transportation Systems*
    21, 9 (2019), 3848–3858.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2019) Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao
    Lin, Min Deng, 和 Haifeng Li. 2019. T-gcn: A temporal graph convolutional network
    for traffic prediction. *IEEE智能交通系统汇刊* 21, 9 (2019), 3848–3858。'
- en: Zhou et al. (2022) Weitao Zhou, Zhong Cao, Yunkang Xu, Nanshan Deng, Xiaoyu
    Liu, Kun Jiang, and Diange Yang. 2022. Long-Tail Prediction Uncertainty Aware
    Trajectory Planning for Self-driving Vehicles. In *2022 IEEE 25th International
    Conference on Intelligent Transportation Systems (ITSC)*. IEEE, 1275–1282.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022) Weitao Zhou, Zhong Cao, Yunkang Xu, Nanshan Deng, Xiaoyu
    Liu, Kun Jiang, 和 Diange Yang. 2022. Long-Tail Prediction Uncertainty Aware Trajectory
    Planning for Self-driving Vehicles. 见于*2022 IEEE第25届国际智能交通系统会议（ITSC）*。IEEE, 1275–1282。
- en: 'Zhu et al. (2022) Yuanshao Zhu, Yongchao Ye, Yi Liu, and JQ James. 2022. Cross-area
    travel time uncertainty estimation from trajectory data: a federated learning
    approach. *IEEE Transactions on Intelligent Transportation Systems* 23, 12 (2022),
    24966–24978.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2022) Yuanshao Zhu, Yongchao Ye, Yi Liu, 和 JQ James. 2022. Cross-area
    travel time uncertainty estimation from trajectory data: a federated learning
    approach. *IEEE智能交通系统汇刊* 23, 12 (2022), 24966–24978。'
