- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:41:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2302.13425] A Survey on Uncertainty Quantification Methods for Deep Neural
    Networks: An Uncertainty Source’s Perspective'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.13425](https://ar5iv.labs.arxiv.org/html/2302.13425)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wenchong He [whe2@ufl.edu](mailto:whe2@ufl.edu)  and  Zhe Jiang [zhe.jiang@ufl.edu](mailto:zhe.jiang@ufl.edu)
    The University of FloridaGainesvilleFLUSA32611
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep neural networks (DNNs) have achieved tremendous success in making accurate
    predictions for computer vision, natural language processing, as well as science
    and engineering domains. However, it is also well-recognized that DNNs sometimes
    make unexpected, incorrect, but overconfident predictions. This can cause serious
    consequences in high-stake applications, such as autonomous driving, medical diagnosis,
    and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence
    of DNN predictions beyond prediction accuracy. In recent years, many UQ methods
    have been developed for DNNs. It is of great practical value to systematically
    categorize these UQ methods and compare their advantages and disadvantages. However,
    existing surveys mostly focus on categorizing UQ methodologies from a neural network
    architecture’s perspective or a Bayesian perspective and ignore the source of
    uncertainty that each methodology can incorporate, making it difficult to select
    an appropriate UQ method in practice. To fill the gap, this paper presents a systematic
    taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data
    uncertainty versus model uncertainty). We summarize the advantages and disadvantages
    of methods in each category. We show how our taxonomy of UQ methodologies can
    potentially help guide the choice of UQ method in different machine learning problems
    (e.g., active learning, robustness, and reinforcement learning). We also identify
    current research gaps and propose several future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep neural networks, uncertainty quantification, data uncertainty, model uncertainty,
    trustworthy AI.^†^†ccs: Computing methodologies Uncertainty quantification^†^†ccs:
    Computing methodologies Machine learning approaches^†^†ccs: Computing methodologies Knowledge
    representation and reasoning^†^†ccs: Applied computing Physical sciences and engineering^†^†ccs:
    Information systems Data mining'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep neural network (DNN) models have achieved remarkable success in computer
    vision, natural language processing, as well as science and engineering domains
    (Pandey et al., [2022](#bib.bib104); LeCun et al., [2015](#bib.bib74); Deng et al.,
    [2009](#bib.bib29)). Existing popular DNN models can be largely regarded as a
    deterministic function, which maps the input features to a target prediction through
    hierarchical representation learning (Bengio et al., [2013](#bib.bib10)). While
    these DNN models often achieve strong performance in overall prediction accuracy,
    it is also well-recognized that they sometimes make unexpected, incorrect, but
    overconfident predictions, especially in a complex real-world environment (Reichstein
    et al., [2019](#bib.bib113)). This can cause serious consequences in high-stake
    applications, such as autonomous driving (Choi et al., [2019](#bib.bib22)), medical
    diagnosis (Begoli et al., [2019](#bib.bib8)), and disaster response (Alam et al.,
    [2017](#bib.bib4)). In this regard, a DNN model should be aware of what it does
    not know in order to avoid overconfident predictions. For example, in the medical
    domain, when DNN-based automatic diagnosis systems encounter uncertain cases,
    a patient should be referred to a medical expert for more in-depth analysis to
    avoid fatal mistakes. In an autonomous vehicle, if a DNN model knows in what scenarios
    (e.g., bad weather) it tends to make mistakes in estimating road conditions, it
    can warn the driver to take over and avoid potential crashes.
  prefs: []
  type: TYPE_NORMAL
- en: ”Knowing what a DNN model does not know” comes down to placing appropriate uncertainty
    scores in its predictions, also called uncertainty quantification (UQ). Uncertainty
    in DNNs may come from different types of sources, including data uncertainty and
    model uncertainty (Yarin, [2016](#bib.bib150)). Data uncertainty (also aleatory
    uncertainty) is an inherent property of the data, which originates from the randomness
    and stochasticity of the data (e.g., sensor noises) or conflicting evidence between
    the training labels (e.g., class overlap). Data uncertainty is often considered
    irreducible because we cannot reduce it by adding more training samples. Model
    uncertainty (also epistemic uncertainty), on the other hand, comes from the lack
    of evidence or knowledge during model training or prediction for a new test sample,
    e.g., limited training samples, sub-optimal DNN model architecture or parameter
    learning, and out-of-distribution (OOD) samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, researchers have developed a growing number of UQ methods
    for DNN models. However, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective"), existing surveys of UQ methods for DNNs are
    either from a neural network architecture’s perspective or a Bayesian perspective,
    ignoring the type of uncertainty sources. Specifically, (Gawlikowski et al., [2021](#bib.bib41))
    categorizes existing UQ methods based on their types of DNN model architectures,
    including Bayesian neural networks, ensemble models, and single model architecture,
    but it does not discuss the connection between the DNN model architectures and
    the type of uncertainty sources they incorporate. Other surveys only focus on
    the Bayesian perspective. For example, (Mena et al., [2021](#bib.bib89)) gives
    a comprehensive review of Bayesian neural networks for UQ but ignores existing
    methods from the frequentist perspective (e.g., prediction interval, ensemble
    methods). (Abdar et al., [2021](#bib.bib2)) covers the ensemble and other frequentist
    methods, but it does not compare their advantages and disadvantages. To the best
    of our knowledge, existing surveys on UQ methods often ignore the types of uncertainty
    sources they incorporate. This perspective is important for selecting the appropriate
    UQ methods for different applications, where one type of uncertainty may dominate
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6279ad970bcfb8109505028ce56dbabc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Existing survey on UQ for DNN models
  prefs: []
  type: TYPE_NORMAL
- en: To fill the gap, we provide the first survey of UQ methods for DNNs from an
    uncertainty source’s perspective. Specifically, we create a systematic taxonomy
    for DNN uncertainty quantification methodologies based on the type of uncertainty
    sources that they incorporate. We summarize the characteristics of different methods
    in their technical approaches and compare the advantages and disadvantages when
    addressing different types of uncertainty sources. We also connect the taxonomy
    to several major deep learning topics where UQ methods are critical, including
    OOD detection, active learning, and deep reinforcement learning. Lastly, we identify
    the research gaps and point out several future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Problem definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the training dataset $\mathcal{D}_{tr}=\{\boldsymbol{x}_{i},{y}_{i}\}_{i=1}^{n}$
    with input features $\mathbf{X}=\{\boldsymbol{x}_{i}\}_{i=1}^{n}$, $\boldsymbol{x}_{i}\in\mathbb{R}^{d}$
    and the target label $\mathbf{Y}=\{{y}_{i}\}_{i=1}^{n}$, the learning problem
    is to find a parameterized neural network model $y=f(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})$
    that fits we on the training data (and potentially generalizes well on the unseen
    test data). The model is learned by minimizing the empirical risk, defined as
    the average loss over the training data: (Murphy, [2012](#bib.bib93)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\operatorname*{arg\,min}_{f}R(\mathcal{D}_{tr},f),\ \text{where}\
    R(\mathcal{D}_{tr},f)=\frac{1}{n}\sum_{i=1}^{n}{l}(y_{i},f(\boldsymbol{x}_{i}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In addition to making an accurate prediction, the goal of UQ is to also obtain
    a parametric uncertainty framework $g$ that outputs an uncertainty estimate $u=g(\boldsymbol{x};\boldsymbol{\phi})\in\mathbb{R^{+}}$
    reflecting the confidence of the model prediction. To this end, we formulate the
    problem as an optimization problem that minimizes a new loss function, which is
    a combination of the empirical risk and the expected calibration error (ECE) (Nixon
    et al., [2019](#bib.bib99)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\small\begin{split}\operatorname*{arg\,min}_{f,g}\hat{R}(\mathcal{D},f,g),\
    \text{where}\ \hat{R}(\mathcal{D},f,g)=R(\mathcal{D}_{tr},f)+\text{ECE}(\mathcal{D}_{tr},f,g)\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ECE measures the consistency between the prediction error and the uncertainty
    of the prediction. Specifically, the uncertainty interval is grouped into fixed
    bins, and the average of the difference between the uncertainty and error in each
    bin is compared. ECE encourages higher uncertainty on larger error predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Given a sample $\boldsymbol{x}$, the goal is to predict both the target and
    the associated uncertainty $(y,u)=(f_{\boldsymbol{\theta}}(\boldsymbol{x}),g_{\boldsymbol{\phi}}(\boldsymbol{x}))$.
    The prediction and uncertainty estimation are formulated as two separate models,
    $f$ and $g$, to distinguish the two concepts. However, some approaches may represent
    both in a single, coherent framework.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Types of uncertainty source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide some background of two major types of uncertainty:
    data uncertainty and model uncertainty. Specifically, we discuss the potential
    sources of each type, and their representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Data uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1\. Source of data uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66b52927772789a750103c5b6b63707b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Classes distribution with clean separable boundary (low uncertainty)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5d84cb129d5b2b33356244fe9fcbc38.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Entropy of example a
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/180aff08f122ddb72c4c70fb42d807c0.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Classes distribution with ambiguous boundary (high uncertainty)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d53c3b293c8f720e2ccf47bff6438ce.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Entropy of example c
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. Data uncertainty visualization examples (Different colors represent
    samples in different classes)
  prefs: []
  type: TYPE_NORMAL
- en: The data uncertainty (aka. aleatoric uncertainty) arises from the data’s inherent
    randomness, noise, or the feature distribution overlap in different classes, which
    is irreducible even given more training data (Yarin, [2016](#bib.bib150)). Randomness
    or noise in data can occur in the data acquisition process. The uncertainty in
    the data acquisition process may be due to instrument errors, inappropriate frequency
    of data acquisition, and data transmission errors (Hariri et al., [2019](#bib.bib48)).
    For example, certain data acquisition devices cannot work properly in a complex
    environment (e.g., poor weather conditions). On the other hand, factors such as
    inappropriate sampling methods, data storage, data representation methods and
    interpolation techniques also affect uncertainty during data processing  (Yarin,
    [2016](#bib.bib150)).
  prefs: []
  type: TYPE_NORMAL
- en: For example, for spatiotemporal data, collected from various space and airborne
    platforms (e.g., CubeSat, UAVs), the data uncertainty may result not only from
    the sensor errors associated with the data acquisition devices but also from the
    fact that data is acquired in a digital format (which is discrete in nature) (Cheng
    et al., [2014](#bib.bib21)) even though the underlying phenomenon is continuous.
    The uncertainty of the representation of an object’s movement is also affected
    by the frequency with which location samples are taken, i.e., the sample rate
    (Pfoser and Jensen, [1999](#bib.bib107)). Uncertainty in the data can also accumulate
    from multiple sources and may be propagated into the model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Data uncertainty representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a training dataset $\mathcal{D}_{tr}$ drawn from the distribution
    ${p_{tr}(\boldsymbol{x},y)}$ over the $d$ dimensional input features $\boldsymbol{x}\in\mathbb{R}^{d}$
    and target class labels $y\in\{\omega_{1},...,\omega_{k}\}$ for classification
    problem with $k$ classes or $y\in\mathbb{R}^{k}$ for regression model. In the
    context of a discriminative classification task, the uncertainty of the class
    variable $y$ given a specific input instance $\boldsymbol{x}$ is defined as the
    entropy of the true condition class distribution $\mathcal{H}[p_{tr}(y|\boldsymbol{x})]$
    as Eq. [3](#S3.E3 "In 3.1.2\. Data uncertainty representation ‣ 3.1\. Data uncertainty
    ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") shows. The conditional
    entropy describes the randomness of the class distribution due to the overlap
    of feature values among samples in different classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\mathcal{H}[p_{tr}(y&#124;\boldsymbol{x})]=-\sum_{i=1}^{k}p_{tr}(y=\omega_{i}&#124;\boldsymbol{x})\log(p_{tr}(y=\omega_{i}&#124;\boldsymbol{x}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'For classification problems, data uncertainty arises from the natural complexity
    of the data and the structure of the class boundary on the feature space. Take
    a toy distribution as an example in Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source
    of data uncertainty ‣ 3.1\. Data uncertainty ‣ 3\. Types of uncertainty source
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") which consists of two normally distributed
    clusters (in two colors). Each cluster represents a separate class. A dataset
    that has more clear class boundary has a lower level of data uncertainty as Figure [2](#S3.F2
    "Figure 2 ‣ 3.1.1\. Source of data uncertainty ‣ 3.1\. Data uncertainty ‣ 3\.
    Types of uncertainty source ‣ A Survey on Uncertainty Quantification Methods for
    Deep Neural Networks: An Uncertainty Source’s Perspective") (a) shows. The entropy
    is low for most samples except for those near the class boundary. Conversely,
    if the feature space has larger overlapping between classes, then there is high
    data uncertainty as Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source of data uncertainty
    ‣ 3.1\. Data uncertainty ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective")
    (c) shows. The entropy of the boundary samples is higher than the examples in
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\. Source of data uncertainty ‣ 3.1\. Data
    uncertainty ‣ 3\. Types of uncertainty source ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") (a).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike classification problems, for the regression problem data uncertainty
    arises from the inherent noise (variability) in the data generation or collection
    process:$y=f(\boldsymbol{x})+\epsilon(\boldsymbol{x})$, where $\epsilon(\boldsymbol{x})$
    is the observation or measurement noise in the dataset. There are two classes
    of noise: homeostatic noise and heteroscedastic noise (Kendall and Gal, [2017](#bib.bib67)).
    Homeostatic noise assumes a constant observation noise over all the inputs $\boldsymbol{x}$.
    Heteroscedastic noise, on the other hand, models the observation noise as a function
    of input $\epsilon(\boldsymbol{x})\sim p(\epsilon|\boldsymbol{x})$ (e.g., heteroscedastic
    Gaussian noise). The heteroscedastic noise model is useful in the case where the
    noise level vary for different samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Model uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1\. Source of model uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The model uncertainty (aka aleatoric uncertainty) represents the uncertainty
    in the models’ predictions related to the imperfect model training process (e.g.,
    due to a lack of knowledge on model choices based on the current input data).
    It is reducible given more training data. Model uncertainty can be divided into
    three types: uncertainty in model architecture; uncertainty in the model parameters;
    uncertainty due to a mismatch between training and test dataset distribution.
    For the first type, model architecture uncertainty arises due to the lack of understanding
    of which type of model architecture is most suitable for the data. For example,
    for deep learning models, there exists uncertainty regarding how many neural network
    layers, and how many neurons in each layer are optimum for the given dataset.
    A too complex model can cause an overfitting issue (a model performs well on the
    training dataset but cannot generalize well on a test dataset). Uncertainty in
    model parameters is due to the unknown optimal parameters for the selected model
    architecture, which may arise from an improper training strategy, lack of training
    instances, or local optimal issue for non-convex problems. There is no guarantee
    that the converged weight values correspond to the global minimum of the loss
    function. The last type of model uncertainty is caused by data distribution drift,
    which means the test sample distribution is different from the training sample
    distribution. The issue is not uncommon for real-world deployment of deep learning
    models, also known as out-of-distribution(OOD) data (Schwaiger et al., [2020](#bib.bib121)),
    as real-world test cases can be very complex.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Model uncertainty representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compared with data uncertainty, model uncertainty is more difficult to estimate
    since the source of model uncertainty can arise from three different aspects:
    model parameters, model architectures, and dataset distribution shift. In general,
    different methods are adopted to represent the model uncertainty from each type.
    First, uncertainty from model parameters can be estimated with a Bayesian neural
    network (BNN) (Jospin et al., [2022](#bib.bib63)). BNN assumes a prior over the
    model parameters and aims to infer the posterior distribution of model parameters
    to reflect the parameters uncertainty. Second, uncertainty arising from the model
    architectures are estimated with ensembles of neural network (deep ensembles).
    The intuition is to construct an ensemble of neural network architectures and
    each model is trained separately. The predictions of the ensemble on an input
    form a distribution on the target variable. Thus the variance of the target variable
    predictions can be an estimation of the prediction uncertainty. The third type
    is the uncertainty from dataset distribution shift, which is caused by the mismatch
    between the training dataset distribution and that of the test samples. The further
    away a new test sample is from the training samples, the greater model uncertainty
    there is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we highlight two primary types of uncertainty and discuss their
    potential sources and representations. As Fig. [3](#S3.F3 "Figure 3 ‣ 3.2.2\.
    Model uncertainty representation ‣ 3.2\. Model uncertainty ‣ 3\. Types of uncertainty
    source ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective") shows, data uncertainty arises from its
    inherent property of the input data features and labels, and model uncertainty
    stems from the misspecification of model architectures, parameters, and the dataset
    distribution shit. Depending on the nature of the application, the predominant
    type of uncertainty may vary, and specific methods may be necessary to address
    them. These uncertainties underscore the importance of robust analysis and interpretation
    of data and model outputs in various fields.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2d4a1b80494f0d232331d28d9cc04d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Two types of uncertainty source
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Application domains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss several application domains of uncertainty quantification
    for deep learning models. For each application domain, we discuss the motivation
    for developing uncertainty-aware models and the nature of the data, the source
    of uncertainty, and the challenges associated with uncertainty quantification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Medical diagnosis: Deep neural network models have achieved tremendous success
    in a wide variety of medical applications, including medical imaging, clinical
    diagnosis support, and treatment planning (Oh et al., [2020](#bib.bib101); Van Tulder
    and De Bruijne, [2016](#bib.bib137); Cireşan et al., [2013](#bib.bib23)). However,
    one critical concern for deploying deep learning systems in the medical domain
    is trustworthy of the neural network predictions, since deep learning models tend
    to be over-confident even for a wrong prediction (Loftus et al., [2022](#bib.bib82)).
    The issue is important for medical applications because a wrong decision can be
    life-threatening. In this regard, a trustworthy deep neural network model should
    be able to convey uncertainty in its predictions. Both data uncertainty and model
    uncertainty exist in medical domains. Data uncertainty comes from noisy measurements
    from medical devices, ambiguous class labels (e.g., non-consensus tumor boundary
    annotations on imagery between different radiologists), and registration errors
    between medical imagery taken at different times or from different devices (Gong
    et al., [2022](#bib.bib43)). Model uncertainty also exists because patient subjects
    in the test cases may not be well-represented in the training set. There exist
    several challenges to developing UQ methods in the medical domain. First, there
    are multiple sources of data with diverse noise or uncertainty. For example, in
    MRI imagery, the measurement error is an important contributing factor while in
    clinical notes, semantic uncertainty (ambiguity) is more important. It is important
    to consider the unique uncertainty source for a particular problem. Second, it
    is important to enhance the interpretability of a model’s uncertainty quantification.
    Currently, interpretability of deep learning in medical domains is still an open
    research area. Many works exist for uncertainty-aware deep learning models in
    medical domains. These works can be generally divided into those related to medical
    imaging and those for non-medical imaging applications (Loftus et al., [2022](#bib.bib82)).
    In medical imaging, numerous deep imagery segmentation or classification methods
    have been studied for MRI, ultrasound, and cohere tomography (CT) imagery (Kohl
    et al., [2018](#bib.bib70); Edupuganti et al., [2020](#bib.bib34)). These studies
    often focus on data uncertainty due to the ambiguous object label boundary of
    the MRI images (Natekar et al., [2020](#bib.bib95); Qin et al., [2021](#bib.bib111)),
    and the registration uncertainty (Chen et al., [2021a](#bib.bib19)) due to the
    registration error between different frames or sensors. The second category, non-medical
    imaging applications, is mostly related to clinical diagnosis support and treatment
    planning from Electronic Health Records (EHR). EHR contains temporal health records
    of patients (e.g. medications, lab orders, clinical notes) and global contextual
    features (e.g. gender, age, ethnicity, body index). Recurrent neural networks
    and graph neural networks have been applied to EHR data to predict patient diseases
    and possible treatment (e.g., personalized healthcare) (Zhang et al., [2020](#bib.bib153)).
    The presence of significant variability in patient-specific predictions and optimal
    decisions (Dusenberry et al., [2020](#bib.bib33)) requires a model to capture
    prediction uncertainty in clinical decision systems. This allows the model to
    refer highly uncertain cases to clinicians for further diagnosis. The uncertainty-aware
    automatic clinical systems flow chart is shown in Fig. [4](#S4.F4 "Figure 4 ‣
    4\. Application domains ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b875003e046e4e9086b5b638c66bf25.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Uncertainty-guided clinical diagnosis
  prefs: []
  type: TYPE_NORMAL
- en: 'Geoscience: With the advance of GPS and remote sensing technologies, a rapidly
    growing volume of spatiotemporal data is collected from spaceborne, airborne,
    seaborne and terrestrial platforms (Shekhar et al., [2015](#bib.bib124); NASA Goddard,
    [2019](#bib.bib94); Florida and Wildlife, [2021](#bib.bib37)). For example, small
    satellites (called CubeSats) collect high-resolution Earth imagery that covers
    the entire globe every day. The combination of emerging spatiotemporal big data,
    increased computational power (GPUs), and the recent advances in deep learning
    technologies provide exciting new opportunities for advancing our knowledge about
    the Earth system (Reichstein et al., [2019](#bib.bib113)). For example, deep learning
    has been used to predict river flow and temperature (Jia et al., [2021](#bib.bib56))
    and predict hurricane tracks (Kim et al., [2019](#bib.bib68)). Uncertainty quantification
    for deep learning in geoscience is important because many problems are related
    to high-stake decision-making (e.g., evacuation planning based on hurricane tracking).
    Thus, in hurricane tracking, scientists often provide not only the most likely
    point of landfall but also provide a “cone of uncertainty” across other likely
    points of impact and future trajectories of the storm.'
  prefs: []
  type: TYPE_NORMAL
- en: UQ for geoscience problems faces several challenges due to the unique characteristics
    of spatiotemporal data. First, spatiotemporal data have various spatial, temporal,
    and spectral resolutions and diverse sources of noise and errors (e.g., noise
    and atmospheric effects in remote sensing signals (Licata and Mehta, [2022](#bib.bib79)),
    GPS errors). Second, spatial registration error and uncertainty may exist when
    co-registering different layers of geospatial data into the same spatial reference
    system (He et al., [2022](#bib.bib49)). This can cause location uncertainty of
    Earth image pixels or pixel labels. Third, spatiotemporal data are heterogeneous,
    i.e., the data distribution often varies across different regions or time periods
    (Jiang et al., [2019](#bib.bib60)). Thus, a deep learning model trained in one
    region (or time) may not generalize well to another region (or time) due to the
    distribution shift. An uncertainty-aware model can potentially alleviate the heterogeneity
    issues by providing a confidence measure when applying the pre-trained model to
    a new test location. This issue is particularly important when spatial observation
    samples are sparsely distributed in the continuous space, causing uncertainty
    when inferring the observations at other locations in continuous space (Hengl
    et al., [2017](#bib.bib52)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Transportation: Deep learning technologies that are applied to transportation
    data from ground sensors and video cameras on road networks provide unique opportunities
    to monitor traffic conditions, analyze traffic patterns and improve decision-making.
    For instance, temporal graph neural networks are used to predict traffic flows
    (Zhao et al., [2019](#bib.bib154)). Deep neural network models are also used to
    extract insights into traffic dynamics and identify potential risks, such as congestion
    or accidents (Mo and Fu, [2022](#bib.bib91)). Autonomous driving is another application,
    which uses lidar sensors and optical cameras to detect road lanes and other vehicles
    or pedestrians. However, transportation data are unique due to the temporal dynamics,
    the sensitivity to external factors, and the existence of noise and uncertainty
    (e.g., omission, sparse sensor coverage, errors or inherent bias). For example,
    highly crowded events can disrupt normal traffic flows on road networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Existing works on trajectory uncertainty consider the data uncertainty due to
    the sparse or insufficient training data (Zhou et al., [2022](#bib.bib155)), and
    erroneous or missing measurements due to signal loss (Markos et al., [2021](#bib.bib87)).
    Other works consider external factors like weather impacts (Pang et al., [2021](#bib.bib105);
    Zhu et al., [2022](#bib.bib156); Lempert et al., [2022](#bib.bib76); Wang et al.,
    [2019a](#bib.bib142)) into uncertainty quantification. Short-term traffic status
    forecasting (such as volume, travel speeds, and occupancy) requires the incorporation
    of uncertainty due to the stochastic environment and model training (Wang et al.,
    [2014](#bib.bib141)). For long-term traffic modeling, existing works focus on
    uncertainty from the exogenous factors of traffic flow (e.g., rainstorms and snowstorms
    that could potentially contribute to the prediction uncertainty) (Li et al., [2022](#bib.bib77)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Biochemistry engineering: Traditional biochemistry discoveries are primarily
    based on experiments, which are expensive and time-consuming (Bower and Bolouri,
    [2001](#bib.bib13)). With the rapid advancement of computational science, researchers
    have turned to simulation and data-driven approaches to aid scientific discovery
    (Brunton and Kutz, [2022](#bib.bib15)). Simulation methods create mathematical
    models based on fundamental principles, such as kinetics of microbial growth and
    product synthesis, fluid dynamics in bioreactors, and design of new proteins and
    enzymes (Mowbray et al., [2021](#bib.bib92)). However, these models can be computationally
    intensive and require an in-depth understanding of the biochemical system. In
    contrast, deep learning-based models use neural networks and a vast amount of
    biochemistry data to directly extract knowledge and learn complex relationships.
    However, the common data-driven DNN models are unaware of domain knowledge and
    face challenges in representing the complexities of biochemical systems, such
    as the heterogeneity of microbial populations and genetic-environmental variations.
    This often results in unreliable outputs that require human experts to verify,
    adding additional labor to the process (Hie et al., [2020](#bib.bib53)). Several
    factors contribute to the uncertainty of data-driven biochemistry engineering.
    For example, researchers often have only a partial understanding of the complex
    biological mechanism and the macromolecules released from the cells and it is
    difficult to quantify the heterogeneity in the microbial population, due to genetic
    and environmental variations. For different tasks, these factors may play a different
    role in the UQ for neural network model. In protein engineering, the protein function
    prediction (based on protein sequences, protein structures, and protein-protein
    interactions) is challenging because of the large amount of diversity of protein
    folds and the lack of a complete understanding of the protein structure (Bradford
    et al., [2018](#bib.bib14)). For bioreactor engineering, the prediction of bioreactor
    system performance is affected by the intrinsic interactions between bioreactor
    operating factors (e.g., pH, temperature, and substrate concentration). An iterative
    method has been used to enable the propagation of model uncertainty for multi-step
    ahead predictions (Mowbray et al., [2021](#bib.bib92)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. A Taxonomy of DNN UQ Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide a new taxonomy of existing DNN UQ methods based
    on the type of uncertainty sources each method can capture. As mentioned before,
    two main sources of uncertainty exist in the DNN predictions and for different
    domain applications, different sources may play a major role in affecting the
    model prediction. Thus it is vital to understand which type of uncertainty source
    each method can address. In general, we categorize existing works into three branches
    based on the uncertainty sources as Fig. [5](#S5.F5 "Figure 5 ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") shows: model uncertainty,
    data uncertainty, and the combination of the two. We briefly introduce each category
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model uncertainty: In this category, the approaches consider the model uncertainty
    resulting from the parameters (BNN), architectures (ensemble methods), or sample
    density (Deep Gaussian process).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data uncertainty: The approaches in this category aim to account for the uncertainty
    from the inherent randomness or noise in the data. The general idea is to construct
    a distribution over the prediction. Those approaches are further split into deep
    discriminative models and deep generative models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The combination of model and data uncertainty: Many approaches aim to capture
    the total uncertainty in one framework, which is the combination of model and
    data uncertainty. The straightforward approach is to combine the approaches in
    data and model uncertainty and form a coherent framework, but introduces more
    computation and storage demand. Another framework called evidential deep learning
    overcomes the computational challenge with a single neural network model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following, the main intuitions and approaches of the three types are
    presented and their main advantages and disadvantages are discussed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7fc528ad4d3041cdbff8e29da293adf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. A taxonomy for existing literature on UQ for DNN
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Model Uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection reviews the existing framework for DNN model uncertainty. We
    categorize the approaches into three subcategories: Bayesian neural network, ensemble
    models, and sample density aware models. We will introduce each subcategory in
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Bayesian Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From a frequentist point of view, there exists a single set of parameters $\boldsymbol{\boldsymbol{\theta}}*$
    that fit the DNN model best, where $\boldsymbol{\theta}*=\operatorname*{arg\,min}_{\boldsymbol{\theta}}\mathcal{L}(\mathbf{Y},f(\mathbf{X},\boldsymbol{\theta}))$
    and $\mathcal{L}$ is the loss function. However, the point estimation of DNN parameters
    tends to be overfitting and overconfident (Thulasidasan et al., [2019](#bib.bib132)).
    On the other hand, the Bayesian neural network (BNN) imposes a prior on the neural
    network parameters $p(\boldsymbol{\theta})$ and aims to learn the posterior distribution
    of these parameters $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$.
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})=\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{Y}&#124;\mathbf{X})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This term is the posterior distribution of model parameters conditioned on the
    training dataset. This distribution reflects what extent our model can capture
    the training data pattern. Assuming a Gaussian distribution for the model parameters,
    the larger the variance of the distribution is, the larger uncertainty there exists
    in the model. Such uncertainty can be caused by a small amount of training data
    or inappropriate configuration of neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'For inference on the new samples $\boldsymbol{x}*$, we can marginalize out
    the model parameters by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The uncertainty on the test samples will be reflected by the prediction distribution
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: However, the approach is analytically intractable and does not have a closed-form
    solution and an approximation must be made for prediction in BNN. To estimate
    the posterior of the neural network parameters, various approaches have been proposed
    to approximate the parameter posterior in a simpler form to solve it in a tractable
    way. Some approximation methods define a parameterized class of distributions,
    $\mathcal{Q}$, from which they select an approximation $q_{\phi}(\boldsymbol{\theta})$for
    the posterior. For example, $\mathcal{Q}$ can be the set of all factorized Gaussian
    distributions, and $\phi$ is the parameters of mean and diagonal variance. The
    distribution $q_{\phi}(\boldsymbol{\theta})\in\mathcal{Q}$ is selected according
    to some optimization criteria to approximate the posterior. Two popular approaches
    for optimization are variational inference (Blei et al., [2017](#bib.bib12)) and
    Laplace approximation (Friston et al., [2007](#bib.bib38)). Instead of approximating
    the posterior in an analytical way, another approach aims to solve this problem
    by Monto Carlo sampling called Markov Chain Monto Carlo sampling. In the following,
    we will review existing methodologies in the three subcategories in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variational Inference (VI): In  (Hinton and Van Camp, [1993](#bib.bib54); Graves,
    [2011](#bib.bib45)), the authors propose to find a variational approximation to
    the Bayesian posterior distribution on the weights by maximizing the evidence
    lower bound (ELBO) of the log marginal likelihood. The intractable posterior $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$
    is approximated with a parametric distribution $q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\footnotesize\begin{split}\log p(\mathbf{Y}&#124;\mathbf{X})&amp;\geq\mathbb{E}_{\boldsymbol{\theta}\sim
    q_{\phi}(\boldsymbol{\theta})}\log\frac{p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})p(\boldsymbol{\theta})}{q_{\phi}(\boldsymbol{\theta})}\\
    &amp;=\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log p(\mathbf{Y}&#124;\mathbf{X},\boldsymbol{\theta})-\mathcal{KL}(q_{\phi}(\boldsymbol{\theta})&#124;&#124;p(\boldsymbol{\theta}))\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the first term $\mathbb{E}_{\boldsymbol{\theta}\sim q_{\phi}(\boldsymbol{\theta})}\log
    p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta}))$ is log-likelihood of the training
    data on the neural network model. This term increases as the model accuracy increases.
    The second term is the Kullback-Leibler (KL) divergence between the posterior
    estimation and the prior of the model parameters, which controls the complexity
    of the neural network model. Maximizing Eq. [6](#S5.E6 "In 5.1.1\. Bayesian Neural
    Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective") corresponds to finding a tradeoff between the prediction
    accuracy and complexity of the model. Thus the posterior inference problem becomes
    an optimization problem on the parameter $\boldsymbol{\phi}$.'
  prefs: []
  type: TYPE_NORMAL
- en: One challenge of variational inference lies in the choice of the parameterized
    class of distribution $q_{\boldsymbol{\phi}}(\boldsymbol{\theta})$. The original
    method relies on the use of Gaussian approximating distribution with a diagonal
    covariance matrix (mean-field variational inference) (Hinton and Van Camp, [1993](#bib.bib54);
    Posch et al., [2019](#bib.bib109)). This method leads to a straightforward lower
    bound for optimization, but the approximation capability is limiting. To capture
    the posterior correlations between parameters, the second category of approaches
    extends the diagonal covariance to general covariance matrix while still leading
    to a tractable algorithm by maximizing the above ELBO (Posch and Pilz, [2020](#bib.bib108)).
    However, the full covariance matrix not only increases the number of trainable
    parameters but also introduce substantial memory and computational cost for DNN.
    To reduce the computation, the third category of approaches aim to simplify the
    covariance matrix structure with certain assumption. Some approaches assume independence
    among layers, resulting in block-diagonal structure covariance matrix (Sun et al.,
    [2017](#bib.bib129); Zhang et al., [2018](#bib.bib152)). Others find that for
    a variety of deep BNN trained using Gaussian variational inference, the posterior
    consistently exhibits strong low-rank structure after convergence (Swiatkowski
    et al., [2020](#bib.bib131)), they propose to decompose the dense covariance matrix
    into a low-rank factorization to simplify the computation. Additionally, (Mishkin
    et al., [2018](#bib.bib90)) assumes the covariance matrix taking the form of “diagonal
    plus low-rank” structure for a more flexible and faster approximation. Another
    line of work introduces sparse uncertainty structure via a hierarchical posterior
    or employing normalizing flow with low-dimensional auxiliary variables (Ritter
    et al., [2021](#bib.bib117); Louizos and Welling, [2017](#bib.bib83)) to reduce
    the computation.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of variational Gaussian approximation for DNN parameters is
    that to capture the full correlations among model latent weights, it requires
    large amount of variational parameters to be optimized, which scales quadratically
    with the number of latent weights in the model. Existing works aim to simplify
    the covariance structure with certain assumption while still capturing the correlation
    between neural network parameters to optimize the computational speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Laplace approximation: The idea behind the Laplace approximation is to obtain
    an approximate posterior around the ’maximum a posterior’ (MAP) estimator of neural
    network weights with a Gaussian distribution, based on the second derivative of
    the neural network likelihood functions (MacKay, [1992](#bib.bib84)). The method
    can be applied post-hoc to a pre-trained neural network model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})\approx p(\boldsymbol{\hat{\theta}}&#124;\mathbf{X},\mathbf{Y})\exp(-\frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{\hat{\theta}})^{T}\mathbf{H}(\boldsymbol{\theta}-\boldsymbol{\hat{\theta}}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, the posterior is then approximated as a Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})=\mathcal{N}(\boldsymbol{\hat{\theta}},\mathbf{H}^{-1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Where $\boldsymbol{\hat{\theta}}$ is the MAP estimate and $\mathbf{H}$ is the
    Hessian matrix. It is the second derivative of the neural network likelihood function
    regarding the model weights, i.e., $\mathbf{H}_{ij}=-\frac{\partial^{2}}{\partial\boldsymbol{\theta}_{i}\partial\boldsymbol{\theta}_{j}}\log
    p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta})$. Fig.  [6](#S5.F6 "Figure 6 ‣ 5.1.1\.
    Bayesian Neural Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ
    Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective")(a) illustrates the intuition of Laplace
    approximation, where the blue density function is the true posterior distribution
    and the orange Gaussian distribution is the Laplace approximation. This method
    approximates the posterior locally can simplify the computation of posterior,
    but the downside is that for the it cannot capture the multi-modal distribution
    with more than one mode since it is a local estimation around the MAP mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/561031e9476361e62ea00111097a6c82.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Laplace approximation
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, for DNN, it is still infeasible to compute the invert the Hessian
    matrix for all parameters, which is typically in the order of several million.
    Previous work on using Laplace approximation for neural network uncertainty quantification
    mainly aims to leverage the techniques of Hessian approximation to simplify the
    computation. One simple solution is to ignore the covariance between weights and
    only extract the diagonal of the Hessian matrix. Such methods Inspired by the
    Kronecker factored approximations of the curvature of a neural network, the approach
    (Ritter et al., [2018b](#bib.bib116)) factorizes the Hessian matrix into the Kronecker
    product (matrix operation that results in a block matrix) of two smaller matrices.
    The method brings down the inversion cost of the Hessian matrix and can be scaled
    to deep convolutional networks and applied to Bayesian online learning efficiently
    (Ritter et al., [2018a](#bib.bib115)). However, the method introduces an additional
    assumption that each layer of the neural network is independent (ignoring covariance
    between layers), which might lead to an overestimation of the variance (uncertainty)
    in certain directions.
  prefs: []
  type: TYPE_NORMAL
- en: To make the Laplace approximation allow for uncertainty quantification for contemporary
    DNN, another challenge is to calibrate the predictive uncertainty. One standard
    practice is to tune the prior precision of the Gaussian prior on model weights
    (Ritter et al., [2018b](#bib.bib116)). This has a regularizing effect both on
    the approximation to the true Hessian, as well as the Laplace approximation itself,
    which may be placing probability mass in low probability areas of the true posterior.
    However, these parameters require optimization w.r.t. the prediction uncertainty
    performance on a validation dataset, which may not generalize well on new test
    data. To overcome this disadvantage, another approach introduces a more flexible
    framework to tune the uncertainty of Laplace-approximated BNNs by adding some
    additional ’hidden units’ to the hidden layer of MLP-trained network (Kristiadi
    et al., [2021](#bib.bib72)). The framework is trained with an uncertainty-aware
    objective to improve the uncertainty calibration of Laplace approximations. However,
    the limitation of this method is it can only be applied to MLP-trained networks,
    and cannot generalize to other models, e.g., convolutional neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, the previous methods employed optimization-based schemes like variational
    inference and Laplace approximations of the posterior. In doing so, strong assumptions
    and restrictions on the form of the posterior are enforced. The common practice
    is to approximate the posterior with a Gaussian distribution. The restrictions
    placed are often credited with inaccuracies induced in predictions and uncertainty
    quantification performance. The difference between two approximations is that
    Laplace approximation is a local approximation around the MAP estimation, and
    it can be applied to a pre-trained neural network and obtain uncertainty quantification
    without influencing the performance of the neural network. On the other hand,
    the variational inference is global optimization used during training and may
    influence the model prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Markov Chain Monto Carlo approximation: Markov Chain Monto Carlo (MCMC) is
    a general method for sampling from an intractable distribution. MCMC constructs
    an ergodic Markov chain whose stationary distribution is posterior $p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$.
    Then we can sample from the stationary distribution. The inference step of BNN
    (Eq. [5](#S5.E5 "In 5.1.1\. Bayesian Neural Networks ‣ 5.1\. Model Uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective")) can be
    approximated as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\footnotesize p(y*&#124;x*,\mathbf{X},\mathbf{Y})=\int p(y*&#124;\boldsymbol{x}*,\boldsymbol{\theta})p(\boldsymbol{\theta}&#124;\mathbf{X},\mathbf{Y})d\boldsymbol{\theta}\approx\frac{1}{N}\sum_{i=1}^{N}p(y*&#124;x*,\boldsymbol{\theta}_{i})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\theta}_{i}\sim p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y})$
    is a sampled from MCMC. This process generates samples by a Markov chain over
    a state space, where each sample only depends on the state of the previous sample.
    The dependency is described with a proposal distribution $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})$
    that specifies the probability density of transitioning to a new sample $\boldsymbol{\theta}^{\prime}$
    from a given sample $\boldsymbol{\theta}$. Some acceptance criteria are based
    on the relative density (energy) of two successive samples evaluated at the posterior
    to determine whether accept the new sample or the previous sample. The vanilla
    implementation is through the Metropolis-Hastings algorithm (Murphy, [2012](#bib.bib93))
    with Gaussian proposal distribution $T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})\sim\mathcal{N}(\boldsymbol{\theta},\Sigma)$.
    Specifically, in each iteration, the algorithm constructs a Markov chain over
    the state space of $\boldsymbol{\theta}$ with the proposal density distribution.
    The proposal samples is stochastically accepted with the acceptance probability
    $\alpha(\boldsymbol{\theta}^{\prime},\boldsymbol{\theta})=\frac{T(\boldsymbol{\theta}^{\prime}|\boldsymbol{\theta})p(\boldsymbol{\theta}^{\prime}|x)}{T(\boldsymbol{\theta}|\boldsymbol{\theta}^{\prime})p(\boldsymbol{\theta}|\boldsymbol{x})}$
    via a random variable $\mu$ drawn uniformly from the interval $[0,1]$ ($\mu\sim\text{Unif}(0,1)$).
    If we reject the proposal sample, we retain the previous sample $\boldsymbol{\theta}$.
    This strategy ensures the stationary distribution of the samples converges to
    the true posterior after a sufficient number of iterations. However, the isotropic
    Gaussian proposal distribution shows random walk behavior and can cause slow exploration
    of the sample space and a high rejection rate. Thus it takes a longer time to
    converge to the stationary distribution. The problem is more severe because of
    the high dimensional parameter space of modern DNN and hinders the application
    of MCMC on neural network parameter sampling (Neal, [2012](#bib.bib96)).
  prefs: []
  type: TYPE_NORMAL
- en: The recent development on MCMC for modern DNN mainly focuses on how to sample
    more efficiently and reduce convergence iterations. For example, One direction
    aims to combine the MCMC sampling with variational inference (Salimans et al.,
    [2015](#bib.bib120); Wolf et al., [2016](#bib.bib147)) to take advantage of both
    methods. Since variational inference approximates the posterior by formulating
    an optimization problem w.r.t. the variational posterior is selected from a fixed
    family of distributions. This approach could be fast by ensuring some constraints
    on the variational posterior format, but they may approximate the true posterior
    poorly even with very low ELBO loss. On the other hand, MCMC does not have any
    constraints on the approximated posterior shape, and can potentially approximate
    the exact posterior arbitrarily well with a sufficient number of iterations. Markov
    Chain Variational Inference (MCVI) bridge the accuracy and speed gap between MCMC
    and VI by interpreting the iterative Markov chain $q(\boldsymbol{\theta}|x)=q(\boldsymbol{\theta}_{0})\prod_{t=1}^{T}q(\boldsymbol{\theta}_{t}|\boldsymbol{\theta}_{t-1},x)$
    as a variational approximation in the expanded space with $\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$.
    Thus, instead of constructing a sequential Markov Chain to sample $\boldsymbol{\theta}_{0},...\boldsymbol{\theta}_{T-1}$,
    they use another auxiliary variational inference distribution $r(\boldsymbol{\theta}_{0},...,\boldsymbol{\theta}_{T-1})$
    to approximate the true distribution with some flexible parametric form. By optimizing
    the lower bound over the parameters of the variational distribution with a neural
    network, we can obtain the Markov Chain samples.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of MCMC methods is that the samples it gives are guaranteed to
    converge to the exact posterior after a sufficient number of iterations. This
    property allows us the control the trade-off between sampling accuracy and computation.
    However, the downside of this method is that we do not know how many iterations
    are enough for convergence and it may take an excessive amount of time and computing
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/567cfb01ea268199ae7f7c2666f6e1b9.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) MC dropout
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7\. Illustration of model uncertainty
  prefs: []
  type: TYPE_NORMAL
- en: 'Monto-Carlo (MC) dropout: MC dropout approach (Gal and Ghahramani, [2016](#bib.bib39))
    is currently the most popular method for DNN uncertainty quantification in many
    domains due to its simplicity and ease of implementation. This approach demonstrates
    that the optimization of a neural network with a dropout layer can be equivalent
    to approximating a BNN with variational inference on the parametric Bernoulli
    distribution (Gal and Ghahramani, [2016](#bib.bib39)). Uncertainty estimation
    can be obtained by computing the variance on multiple stochastic forward predictions
    with different dropout masks (switching-off some neurons’ activation) as Fig. [7](#S5.F7
    "Figure 7 ‣ 5.1.1\. Bayesian Neural Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A
    Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") shows. The average
    predictions with various weights dropout can be interpreted as doing the approximate
    integration over the model’s weights (as Eq.[5](#S5.E5 "In 5.1.1\. Bayesian Neural
    Networks ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective")) whose variational distribution follows the Bernoulli distribution.
    The advantages of MC dropout are: First, it requires little modification to existing
    DNN architecture design, which allows for straightforward implementation in practice.
    Second, it mitigates the problem of representing uncertainty by sacrificing prediction
    accuracy since the methods only influence inference step. However, though there
    is theoretical justification for the probabilistic interpretation of MC dropout
    from variational approximation perspective, in many uncertainty benchmark dataset,
    MC dropout tends to be less calibrated than other baseline UQ methods (Guo et al.,
    [2017](#bib.bib47)).'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we summarize several types of approximation on BNN, which aim to
    reduce the computation and memory burden of BNN and make it scalable to modern
    deep neural networks. Those approximation methods can capture the model uncertainty
    associated with parameters to some extent. However, the drawback is that the process
    requires approximation, which may lead to inaccurate uncertainty estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Ensemble models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Ensemble models combine multiple neural network models in the prediction process.
    The ensemble predictions form an output distribution. The prediction variability
    of the ensemble models can be a measure of model uncertainty (e.g., a larger variance
    implies larger uncertainty). To capture the model uncertainty arising from different
    aspects, several strategies for constructing ensembles can be adopted, which are
    summarized into three major categories: the first kind is through bootstrapping
    (Lakshminarayanan et al., [2017](#bib.bib73)). This is a strategy for random sampling
    from the original dataset with replacement. An ensemble of neural network models
    is constructed and each model is trained on different bootstrapped samples. After
    training, the inference is done through the aggregation of the ensembles, and
    uncertainty is obtained from the prediction variance (regression) or average entropy
    (classification). The second strategy is to construct different neural network
    architectures (number of layers, hidden neurons, type of activation functions.)
    (Mallick et al., [2022](#bib.bib86)). This strategy can account for the uncertainty
    from model misspecification. Other strategies include different initialization
    of parameters along with a random shuffle of the datasets. This is better than
    the bootstrap strategy since more samples are utilized for each model. The third
    type is hyperensemble (Wenzel et al., [2020](#bib.bib143)). This approach constructs
    ensembles with different hyper-parameters, such as learning rate, optimization
    strategy, and training strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the ensemble model is simple and scalable to the large dataset for modern
    neural networks, this method has several limitations: first, there is much computational
    overhead, since it requires training multiple independent networks and needs to
    keep all these networks in memory during inference. Second, model diversity is
    a necessary requirement for ensuring the diversity of the ensemble models to provide
    accurate uncertainty estimation. Otherwise, the ensemble model can collapse to
    the same local minima.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Sample density-aware neural network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because of the theoretical soundness of the BNN model and the simplicity of
    deep ensemble models, they are the popular methods for modern DNN uncertainty
    quantification. However, the computational challenge with BNN and ensemble models
    make it infeasible for real-world applications. In addition, the approaches can
    address the model uncertainty associated with the parameter, model architecture,
    and training process stochasticity, but cannot generalize to model uncertainty
    coming from low sample density, which means, the samples lie far away from the
    support of the training sets may show over-confident results. In this regard,
    many approaches have been motivated to develop sample density-aware neural networks
    to capture the model uncertainty due to low training sample density.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaussian Process Hybrid Neural Network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary on Gaussian process: Gaussian process (GP) is a variant of stochastic
    process, where any finite collection of random variables follow a multivariate
    Gaussian distribution (Williams and Rasmussen, [2006](#bib.bib144)). Given a set
    of points $\boldsymbol{x}_{1},...\boldsymbol{x}_{n}$, a GP defines a prior over
    functions $y_{i}=f(\boldsymbol{x}_{i})$, and assumes the $p(y_{1},...,y_{n})$
    follow the Gaussian distribution $\mathcal{N}(\boldsymbol{\mu}(\boldsymbol{x}),\boldsymbol{\Sigma}(\boldsymbol{x}))$,
    where$\boldsymbol{\mu}(\boldsymbol{x})$ is the mean function, and $\boldsymbol{\Sigma}(\boldsymbol{x})$
    is the covariance function, given with $\boldsymbol{\Sigma}_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$.
    $\kappa$ is a positive definite kernel function (radial basis function), that
    measures the similarity between any pairs of input samples. The kernel function
    also plays a role in controlling the smoothness of the GPs. For GP inference,
    given the new sample $\boldsymbol{x_{*}}$, the joint distribution between the
    new sample prediction $y_{*}$ and training samples target variable $y$ has the
    following form'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | <math   alttext="\begin{pmatrix}\mathbf{y}\\
    y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&amp;\mathbf{K}_{\boldsymbol{x}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{K}_{\boldsymbol{x}}^{T}&amp;K_{*}\end{pmatrix}\Bigg{)}" display="block"><semantics
    ><mrow  ><mrow 
    ><mo  >(</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    ><mi  >𝐲</mi></mtd></mtr><mtr
     ><mtd 
    ><msub  ><mi
     >y</mi><mo
     >∗</mo></msub></mtd></mtr></mtable><mo
     >)</mo></mrow><mo 
    >=</mo><mrow  ><mi
      >𝒩</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="260%" minsize="260%"
     >(</mo><mrow 
    ><mo  >(</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    ><mi  >𝝁</mi></mtd></mtr><mtr
     ><mtd 
    ><msub  ><mi
     >μ</mi><mo
     >∗</mo></msub></mtd></mtr></mtable><mo
     >)</mo></mrow><mo 
    >,</mo><mrow  ><mo
     >(</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    ><msub  ><mi
     >𝐊</mi><mi
     >n</mi></msub></mtd><mtd
     ><msub 
    ><mi  >𝐊</mi><mi
     >𝒙</mi></msub></mtd></mtr><mtr
     ><mtd 
    ><msubsup  ><mi
     >𝐊</mi><mi
     >𝒙</mi><mi
     >T</mi></msubsup></mtd><mtd
     ><msub 
    ><mi  >K</mi><mo
     >∗</mo></msub></mtd></mtr></mtable><mo
     >)</mo></mrow><mo maxsize="260%"
    minsize="260%"  >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><csymbol cd="latexml" 
    >matrix</csymbol><matrix  ><matrixrow
     ><ci 
    >𝐲</ci></matrixrow><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑦</ci></apply></matrixrow></matrix></apply><apply
     ><ci 
    >𝒩</ci><interval closure="open" 
    ><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix
     ><matrixrow 
    ><ci  >𝝁</ci></matrixrow><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝜇</ci></apply></matrixrow></matrix></apply><apply
     ><csymbol cd="latexml" 
    >matrix</csymbol><matrix  ><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐊</ci><ci 
    >𝑛</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐊</ci><ci 
    >𝒙</ci></apply></matrixrow><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐊</ci><ci
     >𝒙</ci></apply><ci
     >𝑇</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐾</ci></apply></matrixrow></matrix></apply></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}\mathbf{y}\\ y_{*}\end{pmatrix}=\mathcal{N}\Bigg{(}\begin{pmatrix}\boldsymbol{\mu}\\
    \mu_{*}\end{pmatrix},\begin{pmatrix}\mathbf{K}_{n}&\mathbf{K}_{\boldsymbol{x}}\\
    \mathbf{K}_{\boldsymbol{x}}^{T}&K_{*}\end{pmatrix}\Bigg{)}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathbf{K}_{n}$ is the covariance matrix between $n$ training samples
    and $\mathbf{K}_{\boldsymbol{x}}$ is the covariance vector between the test sample
    and training samples. $K_{*}$ is the prior variance of the test sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21a02173846eda93835a7293138bc94c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8\. Gaussian Process inference example: green lines are the prediction
    sample distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the prediction on new test input sample $\boldsymbol{x}_{*}$ is through
    computing the posterior distribution conditioned on the training data $\mathcal{D}_{\text{tr}}$,
    given by Eq. [11](#S5.E11 "In 5.1.3\. Sample density-aware neural network ‣ 5.1\.
    Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective").
    The mean term of the predictive distribution can be interpreted as a weighted
    average of the training set label. The weight is dependent on the input samples’
    similarity measured by kernel function in feature space. Besides the point estimation
    from the mean function, the prediction uncertainty is indicated with the variance
    of the inferred distribution. GP inference gives lower uncertainty if the test
    samples are around the region where training samples are abundant (higher sample
    density as in the middle part of Fig [8](#S5.F8 "Figure 8 ‣ 5.1.3\. Sample density-aware
    neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective")), otherwise resulting in higher uncertainty
    (boundary part of Fig [8](#S5.F8 "Figure 8 ‣ 5.1.3\. Sample density-aware neural
    network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective")). This property is important for the out-of-distribution
    (OOD) data since OOD data lies far away from the training samples in the feature
    space, which could be detected with GP efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $p(y_{*}&#124;x_{*},\mathcal{D}_{\text{train}},\boldsymbol{\theta})=\mathcal{N}(y&#124;\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}^{-1}\boldsymbol{y},K_{x*}-\mathbf{K}_{\boldsymbol{x}}^{T}\mathbf{K}_{n}\mathbf{K}_{\boldsymbol{x}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Sparse Gaussian process: Though possessing the intriguing property of inherent
    uncertainty estimation, GP is prohibitive for large datasets because the inversion
    of covariance matrix requires $\mathcal{O}(n^{3})$ complexity ($n$ is the total
    number of training samples), which takes high computation and storage cost for
    training and inference on large datasets. In this regard, many methods (Snelson
    and Ghahramani, [2005](#bib.bib126); Titsias, [2009](#bib.bib133))attempt to make
    a sparse approximation to the full GP to bring down the computation to $\mathcal{O}(m^{2}n)$
    ($m$ is the number of inducing valriables and $m\ll n$. The inducing variables
    can be anywhere in the input domain, not constrained to be a subset of the training
    data, which is denoted as input-output pairs $\{\hat{\boldsymbol{x}}_{i},\hat{y}_{i}\}_{i=1}^{m}$.
    Then the inversion of the original covariance matrix $\mathbf{K}_{n}$ can be replaced
    with a low-rank approximation from the inducing variables, which only requires
    the inversion of $m\times m$ matrix $\mathbf{K}_{m}$. Then the question becomes
    how to select $m$ best-inducing variables to be representative of the training
    dataset. Common approaches assume that the best representative inducing variables
    are those that maximize the likelihood (ML) of the training data (Snelson and
    Ghahramani, [2005](#bib.bib126)). Then the location of inducing variables and
    hyper-parameters of GP are optimized simultaneously through ML. The training data
    likelihood can be obtained by marginalizing out the inducing variables on the
    joint distribution of training data and inducing variables. Another approach variation
    sparse GP formulates a variation lower bound of the exact likelihood by treating
    the locations of inducing variables as variational parameters (Titsias, [2009](#bib.bib133))
    for optimization. To further reduce the computation for more scalable inference
    on a large dataset, the paper (Wilson and Nickisch, [2015](#bib.bib145)) proposes
    a kernel interpolation method for scalable structured GP method by exploiting
    the structure of the covariance by imposing grid constraint on the inducing variables.
    In this way, the kernel matrix $\mathbf{K}_{m}$ admits the Kronecker structure
    and is much easier for computing the inversion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Gaussian process: Besides the computational challenge, another limitation
    of GP is the joint Gaussian distribution assumption on the target variables limits
    the model’s capability in capturing diverse relationships among instances in large
    datasets. Additionally, GP relies heavily on the kernel function to compute the
    similarity between samples by transforming input features into the high-dimensional
    manifold. However, for high-dimension structured data, it is challenging to construct
    appropriate kernel functions to extract hierarchical features for computing similarity
    between samples. To address these limitations, two lines of researches area have
    been proposed: deep kernel learning and Deep Gaussian process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep kernel learning (Wilson et al., [2016](#bib.bib146)) aims to combine the
    structured feature learning capability of DNN with the GP to learn more flexible
    representations. The motivation is that DNN can automatically discover meaningful
    representations from high-dimensional data, which could alleviate the fixed kernel
    limitations of GP and improve its expressiveness. Specifically, the deep kernel
    learning transforms the kernel $\mathbf{K}_{\boldsymbol{\theta}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$
    to $\mathbf{K}_{\boldsymbol{\theta}}(g(\boldsymbol{x}_{i};\boldsymbol{w}),g(\boldsymbol{x}_{j},\boldsymbol{w}))$,
    where $g(\cdot;\boldsymbol{w})$ is the neural network parameterized with $\boldsymbol{w}$
    and $\mathbf{K}_{\boldsymbol{\theta}}$ is the base kernel function (e.g., radial
    basis function) of GP. The deep learning transformation can capture the non-linear
    and hierarchical structure in high-dimensional data. The GP with the base kernel
    is applied on the final layer of DNN and make inference based on the learned latent
    features as Fig. [9](#S5.F9 "Figure 9 ‣ 5.1.3\. Sample density-aware neural network
    ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on
    Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty Source’s
    Perspective")(a) shows. The idea has been successfully applied to spatio-temporal
    crop yield prediction where GP plays a role in accounting for the spatio-temporal
    auto-correlation between samples (You et al., [2017](#bib.bib151)), which cannot
    be reflected from the DNN features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83edde01462f3353ec34798fb17d585f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Deep kernel learning
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57e92ec4383f8c1c1b2a4f9815a1ab24.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Deep Gaussian process
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Illustration of model uncertainty
  prefs: []
  type: TYPE_NORMAL
- en: 'The other category of deep GP is compositional Gaussian process (Damianou,
    [2015](#bib.bib25)) and focuses on function composition inspired by the architecture
    of deep neural networks. In this model, each layer is a GP model, whose inputs
    are governed by the output of another GP as Fig. [9](#S5.F9 "Figure 9 ‣ 5.1.3\.
    Sample density-aware neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective")(b) shows. The recursive
    composition of GPs results in a more complex distribution over the target variables
    prediction. which overcomes the joint Gaussian distribution limitation of vanilla
    GP. The forward propagation and joint probability distribution can be written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\begin{split}&amp;\boldsymbol{y}=\boldsymbol{f}_{L}(\boldsymbol{f}_{L-1}(...\boldsymbol{f}_{1}(\boldsymbol{x})))\\
    &amp;p(y,\boldsymbol{f}_{L},...\boldsymbol{f}_{1}&#124;\boldsymbol{x})\sim p(\boldsymbol{y&#124;\boldsymbol{f}_{L}})\prod_{i=2}^{L}p(\boldsymbol{f}_{i}&#124;\boldsymbol{f}_{i-1})p(\boldsymbol{f}_{1}&#124;\boldsymbol{x})\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where each function $\boldsymbol{f}_{i}(\cdot)$ is a Gaussian process model.
    The intermediate distribution follows the Gaussian distribution, but the final
    distribution will capture a more complex distribution over the target variable
    $\boldsymbol{y}$. The composition also allows the uncertainty to propagate from
    input through each intermediate layer. However, the challenge associated with
    the compositional Gaussian process is: to maximize the data likelihood over $p(\boldsymbol{y}|\boldsymbol{x})$,
    the direct marginalization of hidden variables $\boldsymbol{f}_{i}$ are intractable.
    To overcome this challenge, variational inference by introducing inducing points
    on each hidden layer and optimizing over the variational distribution $q(\boldsymbol{f}_{i})$.
    Then the marginal likelihood lower bound can be obtained by forwarding the variational
    propagation at each layer (Ustyuzhaninov et al., [2020](#bib.bib134)). Moreover,
    the framework also allows for incorporating partial or uncertainty observations
    into the model by placing a prior over the input variables $\boldsymbol{x}\sim\mathcal{N}(\mu_{\boldsymbol{x}},\Sigma_{\boldsymbol{x}})$
    and propagating uncertainty layer by layer (Damianou et al., [2016](#bib.bib26)).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Model uncertainty methods comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Approach | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BNN: Pros: Capture parameter uncertainty Cons: High computational costs |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Variational &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Posch et al., [2019](#bib.bib109); Louizos and Welling, [2017](#bib.bib83))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; More flexible in the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; variational distribution format &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need approximation on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the covariance format to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; trade off the computation. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Laplace &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; approximation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Ritter et al., [2018b](#bib.bib116), [a](#bib.bib115); Kristiadi et al.,
    [2021](#bib.bib72)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Do not impact the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; neural network performance. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need approximation in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the covariance format and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cannot capture the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-modality posterior &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MCMC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; approximation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Neal, [2012](#bib.bib96); Salimans et al., [2015](#bib.bib120); Wolf
    et al., [2016](#bib.bib147)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Converge to exact posterior. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; No strict assumption on the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution form. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hard to converge |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MC dropout &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Gal and Ghahramani, [2016](#bib.bib39); Guo et al., [2017](#bib.bib47))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Simple and compatible to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modern neural network. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lack rigorous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; theoretical analysis &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ensemble Pros: Capture model uncertainty from multiple perspectives. Cons:
    High computational and storage cost. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ensemble (Mallick et al., [2022](#bib.bib86)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture uncertainty from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the misspecification of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model architecture. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need to design various &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network architecture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Bootstrap &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ensemble (Lakshminarayanan et al., [2017](#bib.bib73)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture uncertainty from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; low training dataset. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Use less dataset for training. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ensemble (Wenzel et al., [2020](#bib.bib143)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture uncertainty from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; training hyperparameters. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard to choose suitable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hyperparameters for UQ. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample-density aware model Pros: Capture dataset shift uncertainty. Cons:
    Hard to ensure certain properties |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaussian process &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Wilson et al., [2016](#bib.bib146); You et al., [2017](#bib.bib151);
    Damianou, [2015](#bib.bib25)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Leverage the capability &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of Gaussian process and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DNN model. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The computation complexity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for inference is high. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Distance-aware &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Liu et al., [2020](#bib.bib80); Van Amersfoort et al., [2020](#bib.bib136);
    van Amersfoort et al., [2021](#bib.bib135)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ensure the hidden &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; feature distance reflect &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sample distance in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the feature space. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hard to ensure the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bi-Lipschit constraints. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance-aware neural network: Though the modern neural networks have the capability
    of extracting representative features from large datasets, they are not aware
    of the extent of the distinction of new test samples to the training datasets.
    Thus, to characterize the uncertainty resulting from sample feature density, many
    approaches aim to take the distance awareness between samples into the design
    process of neural network motivated by Gaussian process (Liu et al., [2020](#bib.bib80)).
    Assume the input data manifold is equipped with a metric $||\cdot||_{\mathcal{X}}$,
    which can quantify the distance between samples in the feature space. The intuition
    of a distance-aware neural network is to leverage the feature extraction capability
    of DNN to learn a hidden representation $h(\boldsymbol{x})$ that reflects a meaningful
    distance in the data manifold $||\boldsymbol{x}-\boldsymbol{x^{\prime}}||_{\mathcal{X}}$.
    However, one significant issue with the unconstrainted DNN model is the feature
    collapse, which means DNN feature extraction can map in-distribution data (training
    samples) and out-of-distribution data (lies further to the training data) to similar
    latent representations. Thus the Gaussian process based on the DNN extracted feature
    can be over-confident for those samples that lie further away from training samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f4fb6bf69cd7b5b906ec56774620b7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Illustration on distance-aware neural network feature learning (adapted
    from (Liu et al., [2020](#bib.bib80)))
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the feature collapse problem, several constraints have been proposed
    on the models: sensitivity and smoothness (Van Amersfoort et al., [2020](#bib.bib136);
    van Amersfoort et al., [2021](#bib.bib135)). Sensitivity means a small change
    in the input should result in small changes in the feature representation, which
    can ensure distinct samples are mapped to different latent features. Smoothness
    means small changes in the input cannot lead to a dramatic transformation in the
    output. In general, these two constraints can be ensured by the bi-Lipschitz constraints
    (Liu et al., [2022](#bib.bib81)), which means the relative changes in the hidden
    feature representation $h_{\boldsymbol{\theta}}(\boldsymbol{x})$ is bounded by
    the changes in input space as Eq. [13](#S5.E13 "In 5.1.3\. Sample density-aware
    neural network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective") shows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $L_{1}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}<&#124;h_{\boldsymbol{\theta}}(\boldsymbol{x})-h_{\boldsymbol{\theta}}(\boldsymbol{x}^{\prime})&#124;_{\mathcal{H}}<L_{2}*&#124;\boldsymbol{x}-\boldsymbol{x}^{\prime}&#124;_{\mathcal{X}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'To enforce bi-Lipschitz constraints to DNN, two approaches have been proposed:
    spectral normalization and gradient penalty. Spectral normalization (Liu et al.,
    [2020](#bib.bib80)) claims that the bi-Lipschitz constants $L$ can be ensured
    to be less than one by normalizing the weights matrix in each layer with the spectral
    norm. This method is fast and effective for practice implementation. The other
    approach is called gradient penalty (Van Amersfoort et al., [2020](#bib.bib136)),
    which introduces another loss penalty: the square gradient at each input sample
    $\nabla_{\boldsymbol{x}}^{2}h_{\boldsymbol{\theta}}(\boldsymbol{x})$. This will
    add a soft constraint to the neural networks to constrain the Lipschitz coefficients.
    Compared to spectral normalization, gradient penalty is a soft constraint and
    takes more computation to implement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we summarize and compare existing works on uncertainty quantification
    arising from model uncertainty. The advantages and disadvantages of each method
    are concluded in Table [1](#S5.T1 "Table 1 ‣ 5.1.3\. Sample density-aware neural
    network ‣ 5.1\. Model Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective"). BNN model can capture model uncertainty arising from parameter
    estimation, but usually have high computational costs. The ensemble models can
    capture uncertainty from multiple perspectives, such as model architecture misspecification,
    low training dataset, and hyperparameters. The method also take high computational
    cost. On the other hand, the sample-density aware model can capture dataset shift
    uncertainty, but it’s often hard to learn the distance-aware feature space and
    need to add constraints to the neural network model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Data Uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we discuss the existing methodologies that quantify the data
    uncertainty for DNN models. Generally speaking, data uncertainty is represented
    by the distribution $p(y|\boldsymbol{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$
    is the neural network parameters. To learn this distribution, we categorize the
    approach into deep discriminative models and deep generative models. Deep discriminative
    models are further divided into parametric and non-parametric models based on
    the distribution format. Deep generative models are divided into VAE-based and
    GAN-based models according to the generative frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Deep discriminative model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To quantify the data uncertainty, a discriminative model directly outputs a
    predictive distribution with a neural network. Specifically, the distribution
    can be represented by a parametric or non-parametric model. The parametric model
    assumes the output has an explicit parameterized family of probability distributions
    whose parameters (e.g., mean and variance for Gaussian distribution) are predicted
    with the neural network while the non-parametric model does not have any assumption
    on the underlying distributions. We will discuss existing works for each category
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parametric model: The standard approach for quantifying data uncertainty is
    directly learning a parametric model for $p(y|\boldsymbol{x},\boldsymbol{\theta})$.
    From a frequentist view, there exist a single set of optimum parameters $\boldsymbol{\theta}*$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the classification problem, $p(y|\boldsymbol{x},\boldsymbol{\theta})$ is
    a parameterized categorical distribution on the $k$ class, and the distribution
    parameter $\boldsymbol{\pi}=(\pi_{1},...,\pi_{K})$ are predicted from neural network
    output as Eq. [14](#S5.E14 "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data
    Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") shows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $\footnotesize p(y&#124;\boldsymbol{x},\boldsymbol{\theta})=\text{Categorical}(y;\boldsymbol{\boldsymbol{\pi}})\
    ,\ \boldsymbol{\pi}=f(\boldsymbol{x};\boldsymbol{\theta}),\ \sum_{c=1}^{K}\pi_{c}=1,\
    \pi_{c}>0$ |  |'
  prefs: []
  type: TYPE_TB
- en: In order to obtain the categorical distribution parameters, a straightforward
    method directly utilizes the softmax probability output as $\pi_{i}=\frac{\exp(h_{i}(x;\boldsymbol{\theta}))}{\sum_{c=1}^{k}\exp(h_{c}(x;\boldsymbol{\theta}))}$
    as the predicted to indicate the uncertainty, but these methods tend to be over-confident
    because the softmax operation squeezes the prediction probability toward extreme
    values (zero or one) for the vast majority range of $h_{i}$ (Hendrycks and Gimpel,
    [2016](#bib.bib51)). Following work (Guo et al., [2017](#bib.bib47)) calibrate
    the softmax uncertainty with temperature scaling, which simply add one more hyper-parameter
    $T$ to the softmax calculation as $p=\frac{\exp(h_{i}(x)/T)}{\sum_{c=1}^{k}\exp(h_{c}(x)/T)}$
    to overcome the overconfident outputs. This approach is straightforward to implement,
    but is still faced with the potential to be over-confident due to the lack of
    any constraints, and require post-hoc calibration on the temperature parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression problems, data uncertainty is assumed to come from the inherent
    noise (or measurement error, human labeling error) in the training data. In general,
    the training data is modeled as independent additive Gaussian noise with sample-dependent
    variance $\sigma(\boldsymbol{x})$, which indicates the target variable $y_{i}=f_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})+\epsilon(\boldsymbol{x}_{i})$.
    $\epsilon(\boldsymbol{x}_{i})$ is the independent heterogeneous Gaussian noise,
    which represents the uncertainty for each sample. In this way, the output will
    be a parameterized continuous Gaussian distribution, as Eq. [15](#S5.E15 "In 5.2.1\.
    Deep discriminative model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy of DNN UQ
    Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks:
    An Uncertainty Source’s Perspective") shows. Both the mean and variance are predicted
    from the neural network (Kendall and Gal, [2017](#bib.bib67)). The mean represents
    the prediction of the model and the variance represents the uncertainty of the
    sample prediction. To optimize the neural network parameters $\boldsymbol{\theta}$,
    maximize likelihood is performed on the mean and variance jointly as Eq. [15](#S5.E15
    "In 5.2.1\. Deep discriminative model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") shows. This is also known
    as heteroscedastic regression, which assumes the observational noise level varies
    with different samples. This is suitable for the case where some samples have
    higher noise (uncertainty), while others have lower. Besides Gaussian distribution,
    the neural network can be parameterized with many other kinds of distributions,
    such as mixture Gaussian distribution (Guillaumes, [2017](#bib.bib46)), which
    is implemented with mixture density network (MDN) (Bishop, [1994](#bib.bib11)),
    assuming multiple modes for the prediction. MDN has the advantage that can account
    for the uncertainty from multiple prediction modes but consumes more computation.
    It’s important to choose a suitable parameterized distribution, which depends
    on the nature of the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $\footnotesize\begin{split}&amp;p(y&#124;\boldsymbol{x},\boldsymbol{\theta})=\mathcal{N}(f_{\boldsymbol{\theta}}(\boldsymbol{x}),\sigma_{\boldsymbol{\theta}}(\boldsymbol{x})),\\
    &amp;\mathcal{L}_{\text{NN}}(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})}^{2}&#124;&#124;y_{i}-f_{\boldsymbol{\theta}}(\boldsymbol{x_{i}})&#124;&#124;^{2}+\frac{1}{2}\log\sigma_{\boldsymbol{\theta}}(\boldsymbol{x}_{i})^{2}\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The advantage of prediction distribution is that it is simple to add the approach
    to existing neural network architecture and requires little modification to the
    training and inference process. However, the explicit parameterization form requires
    choosing the appropriate distribution to accurately capture the underlying uncertainty,
    which can be hard without any prior information on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-parametric model: Another widely popular approach to indicate the data
    uncertainty is through prediction interval (PI) (Pearce et al., [2018](#bib.bib106);
    Wu et al., [2021](#bib.bib148)). For regression problems, the prediction intervals
    output a lower and upper bound $[y_{l},y_{u}]$, with which we expect the ground
    truth $y$ falls into the interval with a prescribed confidence level, $1-\alpha$,
    meaning that $p(y\in[y_{l},y_{u}])>1-\alpha$. This approach does not require explicit
    distribution over the prediction variable and is more flexible. Traditional prediction
    intervals are constructed in two steps, First step is to learn the point estimation
    of the target variable, which is obtained through the minimization of the error-based
    loss function (i.e., mean square loss), and then estimate the prediction variance
    around the local optimum prediction. The strategy is trying to minimize the prediction
    error, but not optimize the prediction interval quality. One recent paper explicitly
    constructs a lower and upper bound estimation (LUBE) to improve the PI characteristics,
    i.e., the width and coverage probability. The basic intuition is that the PI should
    cover the ground-truth with a certain pre-defined probability (confidence level),
    but should be as narrow as possible. The approach improves the quality of the
    constructed PI, but the new cost function is non-differentiable and requires Simulated
    Annealing (SA) sampling to obtain the optimal NN parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5760f50591dd23b39280aba770eb6d70.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Prediction distribution example
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfb653e0e0e6e898a11e8e31bea5978c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Prediction interval example
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Neural network architecture for parametric and non-parametric model
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the non-differentiable limitation of LUBE, another approach propose
    a coverage width-based loss function (Pearce et al., [2018](#bib.bib106)) with
    a similar goal as LUBE, shown in the Eq.[16](#S5.E16 "In 5.2.1\. Deep discriminative
    model ‣ 5.2\. Data Uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey
    on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective"). The mean prediction interval width (MPIW) is equal to
    $|y_{u}-y_{l}|$, and prediction interval coverage width (PICP) indicates the average
    probability that the PI covers the ground truth. The total loss encourages the
    PI to be as narrow as possible while having a higher coverage probability than
    the prescribed confidence level $\alpha$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\text{Loss}=\text{MPIW}+\lambda*\max(0,(1-\alpha)-\text{PICP})^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Recent approaches put forward the method of prediction interval learning as
    an empirical constrained optimization problem. There are two different views of
    this optimization problem: primal and dual perspectives. The primal perspective
    views the optimization objective as minimizing the PI intervals under the constraints
    that the PI attains a coverage probability larger than the confidence level (Chen
    et al., [2021b](#bib.bib18)), which is expressed as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\footnotesize\min_{L,U\in\mathcal{H},L<U}\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}(U(\boldsymbol{x})-L(\boldsymbol{x}))\
    s.t.\ p_{\pi}(y\in[L(\boldsymbol{x}),U(\boldsymbol{x})])>1-\alpha$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbb{E}_{\boldsymbol{x}\sim\pi(\boldsymbol{x})}$ denotes the expectation
    with respect to the marginal distribution of input samples $\boldsymbol{x}$, and
    $p_{\pi}$ denote the probability of input and output pair distribution. To enforce
    the optimality and feasibility of the optimization problem, the tradeoff is developed
    through the studying of two characteristics of this approach: Lipschitz continuous
    model class (Virmaux and Scaman, [2018](#bib.bib138)) and Vapnik–Chervonenkis
    (VC)-subgraph class (Chen et al., [2021b](#bib.bib18)). On the other hand, the
    dual perspective constructs the learning objective as maximizing the PI coverage
    probability under the fixed global budget constraints (average PI width) in a
    batch setting (Rosenfeld et al., [2018](#bib.bib118)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\footnotesize\min_{f\in\mathcal{F}}\mathbb{E}_{(\boldsymbol{x},y)\sim\pi(\boldsymbol{x},y)}L(y,U(\boldsymbol{x}),L(\boldsymbol{x}))\
    s.t.\ \sum_{i}(U(\boldsymbol{x}_{i})-L(\boldsymbol{x})_{i})<B$ |  |'
  prefs: []
  type: TYPE_TB
- en: They presented a discriminative learning framework that optimizes the expected
    error rate under a budget constraint on the interval sizes. In this way, the approach
    avoids single-point loss and can provide a statistical guarantee of the generalization
    capability on the whole population. Compared with the primal setup, the dual perspective
    in a batch learning can construct the prediction interval of a group of test points
    at the same time and avoid the computational overhead in the primal problem.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Deep Generative Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The deep generative model (DGM) is a family of probabilistic models that aim
    to learn the complicated, high-dimensional data distribution $p_{\text{data}}(\boldsymbol{x})$
    with DNN. DGMs are capable of learning the intractable data distribution in the
    high-dimensional feature space $\mathcal{X}\in\mathbb{R}^{n}$ from a large number
    of independent and identical observed samples $\{\boldsymbol{x}_{i}\}_{i=1}^{m}$.
    Specifically, they learn a probabilistic mapping from some latent variables $\boldsymbol{z}\in\mathbb{R}^{d}$
    which following tractable distribution to the data distribution, e.g., $\mathcal{N}(\mathbf{0},\mathbf{I})$
    to the data distribution $p_{\text{data}}(\boldsymbol{x})$. Mathematically, the
    generative model can be defined as a mapping function $g_{\boldsymbol{\theta}}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}$,
    where $d$ and $n$ are the dimensions of latent variable and original data, respectively.
    The ability of the deep generative model to learn an intractable distribution
    makes it useful for uncertainty quantification.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is to employ DGM to learn the predictive distribution $p(y|\boldsymbol{x})$
    given the supervised training data pairs $\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{m}$.
    It should be noted that if we aim to learn the predictive distribution instead
    of the data distribution in feature space, the conditional deep generative model
    (cDGM) (Sohn et al., [2015](#bib.bib127)) should be employed. Generally speaking,
    cDGM-based uncertainty quantification models learn a conditional density over
    the prediction $y$, given the input feature $\boldsymbol{x}$. This amounts to
    learn a model $g_{\boldsymbol{\theta}}(\boldsymbol{z},\cdot):\mathbb{X}\rightarrow\mathbb{Y}$
    such that the generative model $g(\boldsymbol{z},\boldsymbol{x})$ with $\boldsymbol{z}\sim
    p(\boldsymbol{z})$ is approximately distributed as the true unknown distribution
    $p_{\text{true}}(y|\boldsymbol{x})$. The variability of the prediction distribution
    $p(y|\boldsymbol{x})$ is encoded into the latent variable $\boldsymbol{z}$ and
    the generative model. During inference, for any $\boldsymbol{x}\in\mathbb{X}$,
    we can generate $m$ samples with $y_{i}=g_{\boldsymbol{\theta}}(\boldsymbol{z}_{i},\boldsymbol{x})$
    and $\boldsymbol{z}_{i}\sim p(\boldsymbol{z})$ and from the samples $\{y_{i}\}_{i=1}^{m}$
    the variability we can quantify the prediction uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsection, we consider two types of deep generative models:
    variational auto-encoder (VAE) (Kingma and Welling, [2013](#bib.bib69)) and generative
    adversarial network (GAN) (Goodfellow et al., [2020](#bib.bib44)). VAE belongs
    to the likelihood-based generative model and is trained via maximizing evidence
    lower bound (ELBO) of the likelihood function. The GAN model is an implicit generative
    model trained with a two-player zero-game approach. We will consider how the two
    frameworks could be utilized for prediction uncertainty estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VAE-based model: VAE model consists of two modules: an encoder and a decoder.
    The encoder network $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})$ aims to embed the
    high dimensional structural output $\boldsymbol{x}$ into a low-dimensional code
    $\boldsymbol{z}$, that capture the inherent ambiguity or noise of the input data.
    The decoder $p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z})$ aim to reconstruct
    the input feature. VAE model has been popular for structured output uncertainty,
    especially for the tasks on image data, because of their capability to model the
    global and local structure dependency in regular grid images. Specifically, two
    kinds of frameworks based on the VAE model have been proposed to account for the
    data uncertainty coming from input noise and target output noise: The first category
    aims to capture the noise that lies in the input samples. The basic idea is to
    embed each sample as a Gaussian distribution instead of a deterministic embedding
    in the low dimensional latent space, where the mean represents the feature embedding
    and variance represents the uncertainty of the embedding (Chang et al., [2020](#bib.bib16)).
    The method takes into the different noise levels inherent in the dataset, which
    is ubiquitous in many kinds of real-world datasets, i.e., face image recognition
    (Chang et al., [2020](#bib.bib16)), medical image reconstruction (Edupuganti et al.,
    [2020](#bib.bib34)). The probabilistic embedding framework takes advantage of
    the VAE model architecture to estimate the embedding and uncertainty simultaneously.
    The second category aims to capture the noise that lies in the target outputs,
    which also means the ground-truth is imperfect, ambiguous, or corrupted. This
    scenario is common in the medical domain (Lee et al., [2020](#bib.bib75)), where
    the objects in the image are ambiguous and the experts may not reach a consensus
    on the class of the objects (large uncertainty). Thus for segmentation or classification
    tasks, the model should be aware of the prediction uncertainty. To capture the
    prediction uncertainty in the target outputs, the conditional VAE (cVAE) (Sohn
    et al., [2015](#bib.bib127))framework is adopted. Specifically, cVAE formulates
    the prediction distribution as an integration over the latent embedding $\boldsymbol{z}$,'
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $\footnotesize p(y&#124;\boldsymbol{x})=\int p(y&#124;\boldsymbol{x},\boldsymbol{z})p(z&#124;\boldsymbol{x})d\boldsymbol{z}\approx\sum_{j=1}^{n}p(y&#124;\boldsymbol{x},\boldsymbol{z}_{j}),\
    \text{where}\ z_{j}\sim p(z)$ |  |'
  prefs: []
  type: TYPE_TB
- en: The cVAE model is trained by maximizing the evidence lower bound of the likelihood.
    Then during inference, multiple latent feature $\boldsymbol{z}_{j}$ can be drawn
    from the prior distribution and the integration over latent $\boldsymbol{z}$ can
    be approximated with the sampling distribution (Prokudin et al., [2018](#bib.bib110)).
    Probabilistic U-Net model (Kohl et al., [2018](#bib.bib70)) combines the architecture
    of cVAE and U-Net model by treating the U-Net model as the encoder to produce
    a probabilistic segmentation map. The U-Net model can capture multi-scale feature
    representations in the low-dimensional latent space to encode the potential variability
    in the segmentation map.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the VAE-based framework can take into consideration the data uncertainty
    coming from the input noise or the target output noise and can adapt the current
    state-of-the-art neural network architecture into its framework, making it more
    flexible for many kinds of applications. The key success lies in modeling the
    joint probability of all samples (pixels) in the image. The approach is suitable
    for structured uncertainty quantification (e.g., image grid structure, graph structure,
    et al.) through learning the implicit joint distribution on the structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0ff7b33bc502c9e1dc10e715e2060cb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Input uncertainty
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/817193aca9449ba3416ce53f5e746791.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Target prediction uncertainty
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12\. VAE framework for uncertainty quantification
  prefs: []
  type: TYPE_NORMAL
- en: 'GAN-based generative model: GAN is a type of generative model trained with
    a two-player zero-game. It consists of a generator and a discriminator. In conditional
    GAN (cGAN), the generator takes the input $x$ and random noise $\boldsymbol{z}$
    as input, and generates the target variables $y$: $\mathcal{G}:(\boldsymbol{x},\boldsymbol{z})\rightarrow
    y$. The discriminator is trained to distinguish the generated samples and ground-truth
    samples. The idea has been adopted in many domains. For example, in transportation
    domain, they adpot GAN-based approach for the prediction of traffic vollume (Mo
    and Fu, [2022](#bib.bib91)). the flow model is integrated into GAN to enable likelihood
    estimation and better uncertainty quantification. Another approach (Oberdiek et al.,
    [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40)) further extends the method
    by utilizing the Wasserstein GAN (Arjovsky et al., [2017](#bib.bib5)) based on
    gradient penalty to improve model convergence. The key advantage of deep generative
    modeling for uncertainty quantification is that they directly parameterize a deep
    neural network to represent the prediction distribution, without the need to have
    an explicit distribution format. Moreover, it can integrate a physics-informed
    neural network for better uncertainty estimation of physical science (Daw et al.,
    [2021](#bib.bib27)). However, the disadvantage is that deep generative models
    are much harder to train, especially for GAN-based models. The convergence of
    the model learning is not guaranteed.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Data uncertainty methods comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Approach | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Discriminative model Pros: compatible to existing network architecture.
    Cons: Hard for structured uncertainty |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Parametric: predictive &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution (Kendall and Gal, [2017](#bib.bib67); Guillaumes, [2017](#bib.bib46))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Can leverage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; existing DNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; training framework. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need to choose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; appropriate parametric &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Non-parametric: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; predictive interval &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Pearce et al., [2018](#bib.bib106); Wu et al., [2021](#bib.bib148);
    Rosenfeld et al., [2018](#bib.bib118); Chen et al., [2021b](#bib.bib18)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; No need to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; choose specific &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution format. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need to design new &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; loss/training strategy. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deep Generative model Pros: Capture structured uncertainty. Cons: Require
    modification to existing architecture and hard to train |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VAE-based model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Chang et al., [2020](#bib.bib16); Edupuganti et al., [2020](#bib.bib34);
    Prokudin et al., [2018](#bib.bib110); Kohl et al., [2018](#bib.bib70)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; structured data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; uncertainty from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the input noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and ambiguous label. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Need to modify neural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network architecture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN-based model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Oberdiek et al., [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40);
    Mo and Fu, [2022](#bib.bib91)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture the structure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; data uncertainty. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN model is hard &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to train. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Model and data uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides considering the data and model uncertainty separately, many frameworks
    attempt to jointly consider the two kinds of uncertainty for more accurate quantification.
    In this part, we will review existing that aims to quantify two types of uncertainty
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1\. Approaches combining data and model uncertainty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A straightforward way to consider both data and model uncertainty is to select
    one of the approaches in each category and combine them in a single framework.
    Below we will introduce some major ways to combine the approach of data and model
    uncertainty and their potential drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combine BNN model with prediction distribution: The method aim to capture both
    data and model uncertainty in a single framework (Kendall and Gal, [2017](#bib.bib67))
    by combining the BNN and prediction distribution. The model uncertainty is captured
    with the BNN approximation approach. Specifically, MC drop-out is adopted due
    to its simplicity for implementation. For each dropout forward pass, one sample
    of the weight is drawn from the weight distribution approximation $\mathbf{W}_{t}\sim\text{Bernoulli}(p)$,
    where $p$ is the dropout rate, then one forward prediction can be made with the
    weight by $y_{t}=p(y|\boldsymbol{x},\mathbf{W}_{t})$. To obtain the data uncertainty,
    the output is formulated as a parameterized Gaussian distribution instead of point
    estimation $[y_{t},\sigma_{t}^{2}]=p(y|\boldsymbol{x};\mathbf{W}_{t})$, where
    $y_{t}$ is the target variable mean prediction and $\sigma_{t}^{2}$ is the prediction
    variance for a single forward prediction. With multiple dropout forward passes,
    we have a set of $T$ prediction samples $\{y_{t},\sigma^{2}_{t}\}_{t=1}^{T}$.
    The predictive uncertainty in the combined model can be approximated with the
    law of total variance expressed as $\text{Var}(y)$ in Eq. [20](#S5.E20 "In 5.3.1\.
    Approaches combining data and model uncertainty ‣ 5.3\. Model and data uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"). The intuition
    of this equation is that the total uncertainty comes from two parts, the last
    $\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$ represents the average data uncertainty,
    and the first part $\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}$
    represent the disagreement of $T$ MC-dropout models, which capture the model uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $\text{Var}(y)\approx\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}-(\frac{1}{T}\sum_{t=1}^{T}y_{t})^{2}+\frac{1}{T}\sum_{t=1}^{T}\sigma_{t}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Combine ensemble model with prediction distribution: This approach (Lakshminarayanan
    et al., [2017](#bib.bib73)) combines the ensemble method with prediction distribution.
    The deep ensemble method constructs an ensemble of DNN models $\mathcal{M}=\{\mathcal{M}_{i}\}_{i=1}^{K}$,
    where each model $\mathcal{M}_{i}$ can be set with different parameters, or architectures
    choices, et, al. The model uncertainty is expressed as the variance or ”disagreement”
    of the ensemble. In this way, the output of each model is modified as parameterized
    distribution to capture the data uncertainty. Similar to MC-dropout, we have an
    ensemble of prediction distribution $\{p(y|\boldsymbol{x},\mathcal{M}_{i})\}_{i=1}^{K}$.
    In this part, we take the classification problem as an example, where the prediction
    distribution is parameterized categorical distribution. The total uncertainty
    is captured with the entropy of average prediction distribution $\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y|\boldsymbol{x},\mathcal{M}_{i}))$,
    and the data uncertainty is the average entropy of each model, expressed as $\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y|\boldsymbol{x},\mathcal{M}_{i}))$.
    The model uncertainty can be expressed with the mutual information between the
    prediction and the ensemble model $y,\mathcal{M}$ as expressed in Eq. [21](#S5.E21
    "In 5.3.1\. Approaches combining data and model uncertainty ‣ 5.3\. Model and
    data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective").'
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $\text{MI}(y,\mathcal{M})=\mathcal{H}(\mathbb{E}_{p(\mathcal{M}_{i})}p(y&#124;\boldsymbol{x},\mathcal{M}_{i}))-\mathbb{E}_{p(\mathcal{M}_{i})}\mathcal{H}(p(y&#124;\boldsymbol{x},\mathcal{M}_{i}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Combine ensemble model with prediction interval: Since the prediction interval
    constructed in some approaches accounts only for the data noise variance, not
    the model uncertainty. To improve the total uncertainty estimation, ensemble methods
    are adopted to combine with prediction interval to account for the model uncertainty
    that comes from model architectures misspecification, parameter initialization,
    et.al. (Pearce et al., [2018](#bib.bib106)). Specifically, Given an ensemble of
    models trained with different model specifications or sub-sampling of training
    datasets, where the model prediction intervals are denoted as $[y^{ij}_{l},y^{ij}_{u}]$
    for sample $i=\{1,...,n\}$ and model $j=\{1,...,m\}$, the model uncertainty can
    be captured by the variance of the lower bound $\sigma_{l}^{(i)^{2}}$ and upper
    bound variance $\sigma_{u}^{(i)^{2}}$. For example, the lower bound uncertainty
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\begin{split}&amp;\sigma_{l}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{l}^{(ij)}-\hat{y}_{l}^{(i)})^{2},\
    \text{where}\ \hat{y}_{l}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{l}^{(ij)}\\ &amp;\sigma_{u}^{(i)^{2}}=\frac{1}{m-1}\sum_{j=1}^{m}(y_{u}^{(ij)}-\hat{y}_{u}^{(i)})^{2},\
    \text{where}\ \hat{y}_{u}^{(i)}=\frac{1}{m}\sum_{j=1}^{m}y_{u}^{(ij)}\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then the new prediction interval $[\tilde{y}_{l},\tilde{y}_{u}]$ with $95\%$
    confidence level can be constructed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\begin{split}&amp;\tilde{y}_{l}=y_{l}-1.96\sigma_{l}^{(i)^{2}},\
    \text{and}\ \tilde{y}_{u}=y_{u}+1.96\sigma_{u}^{(i)^{2}}\end{split}$ |  |'
  prefs: []
  type: TYPE_TB
- en: The constructed interval can reflect both the data uncertainty and model uncertainty,
    but the limitation is that the model uncertainty is simply constructed from the
    variance of the lower and upper bound of the ensemble, which lacks theoretical
    justification because of the independent consideration of two boundaries. To overcome
    this limitation, one recent approach proposes a split normal aggregation method
    to aggregate the prediction interval ensembles into final intervals (Salem et al.,
    [2020](#bib.bib119)). Specifically, the method fits a split normal distribution
    (two pieces of normal distribution) over each prediction interval, and then the
    final prediction will become a mixture of split normal distribution. The PI can
    be derived from the $1-\alpha$ quantile of the cumulative distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, to capture both the data and model uncertainty, existing literature
    can combine the methodologies in the two categories. There are several limitations
    to the combination approaches: first, the BNN or ensemble models require multiple
    forward passes for the prediction, which introduces computation overhead and extra
    storage. Efficiency is a concern. Second, the simple combination of data and model
    uncertainty lack a theoretical guarantee, which requires post-hoc calibration
    on the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. Evidential deep learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20c1d1330c1d8b23344cafb93baba9bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Evidential deep learning architecture
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the computational challenge for the combination approaches, evidential
    deep learning was proposed to use one single deterministic model to capture both
    the data and model uncertainty without multiple forward passes of the neural network
    (Malinin and Gales, [2018](#bib.bib85); Charpentier et al., [2020](#bib.bib17);
    Sensoy et al., [2018](#bib.bib122); Bao et al., [2021](#bib.bib7)). The intuition
    of evidential deep learning is to predict class-wise evidence instead of directly
    predicting class probabilities. In the following we review the methodologies,
    the advantages and disadvantages of those methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in the aforementioned sections, for classification problems, existing
    deep learning-based models explicitly or implicitly predict class probabilities
    (categorical distribution parameters) with softmax-layer parameterized by DNNs
    to quantify prediction uncertainty. However, the softmax prediction uncertainty
    tends to be over-confident (Hendrycks and Gimpel, [2016](#bib.bib51)). Evidential
    deep learning is developed to overcome the limitation by introducing the evidence
    theory (JSANG, [2018](#bib.bib64)) to neural network frameworks. The motivation
    of evidential deep learning is to construct predictions based on evidence and
    predict the parameters of Dirichlet density. For example, considering the 3-class
    classification problem, a vanilla neural network directly predicts the categorical
    distribution on the class $\pi={\pi_{1},\pi_{2},\pi_{3}}$ with $\sum_{i}\pi_{i}=1$.
    However, this approach can only represent a point estimation of prediction distribution.
    On the other hand, evidential deep learning aims to predict the evidence for each
    class $\boldsymbol{\alpha}=\{\alpha_{1},\alpha_{2},\alpha_{3}\}$ with the constraints
    $\boldsymbol{\alpha}>0$, which can be considered as the parameters of Dirichlet
    distribution (Sensoy et al., [2018](#bib.bib122)). The framework is shown in Fig [13](#S5.F13
    "Figure 13 ‣ 5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty
    ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"), where
    the output is the evidence $\boldsymbol{\alpha}$ for each class, and the prediction
    distribution is sampled from the Dirichlet distribution. The expected prediction
    distribution for each class is $p_{i}=\frac{\alpha_{i}}{\sum_{c=1}^{3}\alpha_{c}}$,
    whose entropy reflects the data uncertainty. On the other hand, the model uncertainty
    is reflected with the total evidence $\sum_{i}\alpha_{i}$, which means the more
    evidence we collect, the more confident the model is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, evidential deep learning aims to learn the prior distribution
    of categorical distribution parameters, which is represented by the Dirichlet
    distribution. The Dirichlet distribution is parameterized by its concentration
    parameters $\boldsymbol{\alpha}$ (evidence) where $\alpha_{0}$ is the sum of all
    $\alpha_{i}$ and is referred to as precision of Dirichlet distribution. A higher
    value of $\alpha_{0}$ will lead to sharper distribution and lower model uncertainty.
    As shown in Eq.[24](#S5.E24 "In 5.3.2\. Evidential deep learning ‣ 5.3\. Model
    and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty
    Quantification Methods for Deep Neural Networks: An Uncertainty Source’s Perspective"),
    the $\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$ defines a probability density
    function over the k-dimensional random variable $\boldsymbol{\mu}=[{\mu_{1}},...,{\mu}_{k}]$,
    where $k$ is the number of classes, $\boldsymbol{\mu}$ belongs to the standard
    $k-1$ simplex (${\mu_{1}}+...+{\mu}_{k}=1$ and ${\mu}_{i}\in[0,1]$ for all $i\in{1,...,k}$),
    and can be regarded as the categorical distribution parameters. The relationship
    between Dirichlet distribution and uncertainty quantification can be illustrated
    using a 2-simplex, as shown in Fig [14](#S5.F14 "Figure 14 ‣ 5.3.2\. Evidential
    deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology
    ‣ A Survey on Uncertainty Quantification Methods for Deep Neural Networks: An
    Uncertainty Source’s Perspective"). The random variable $\boldsymbol{\mu}=[{\mu}_{1},{\mu}_{2},{\mu}_{3}]$
    is represented by its Barycentric coordinates in Fig [14](#S5.F14 "Figure 14 ‣
    5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy
    of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods for Deep
    Neural Networks: An Uncertainty Source’s Perspective") (a) on the 2-simplex. The
    Barycentric coordinate is a coordinate system where point are located inside a
    simplex, and the value in each coordinate can be interpreted as the fraction of
    masses placed at the corresponding vertices of the simplex. Fig [14](#S5.F14 "Figure
    14 ‣ 5.3.2\. Evidential deep learning ‣ 5.3\. Model and data uncertainty ‣ 5\.
    A Taxonomy of DNN UQ Methodology ‣ A Survey on Uncertainty Quantification Methods
    for Deep Neural Networks: An Uncertainty Source’s Perspective") (b) shows a scenario
    where the evidence parameters are equal, resulting in three classes being indistinguishable
    and implying high data uncertainty (high entropy for the sampled $\boldsymbol{\mu}$).
    As the total evidence $\sum_{i}\alpha_{i}$ increases, the density becomes more
    concentrated, which means the model uncertainty decreases, while the data uncertainty
    remains fixed. Fig [14](#S5.F14 "Figure 14 ‣ 5.3.2\. Evidential deep learning
    ‣ 5.3\. Model and data uncertainty ‣ 5\. A Taxonomy of DNN UQ Methodology ‣ A
    Survey on Uncertainty Quantification Methods for Deep Neural Networks: An Uncertainty
    Source’s Perspective") (c) shows a scenario with fixed model uncertainty, as the
    sum of evidence parameters remains constant. When the evidence becomes imbalanced,
    the density becomes more concentrated toward one class, thus decreasing the data
    uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $\text{Dir}(\boldsymbol{\mu}&#124;\boldsymbol{\alpha})=\frac{\mathrm{T}(\alpha_{0})}{\prod_{c=1}^{K}\mathrm{T}(\alpha_{c})}\prod_{c=1}^{K}{\mu}_{c}^{\alpha_{c}-1},\alpha_{c}>0,\
    \alpha_{0}=\sum_{c=1}^{K}\alpha_{c}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/25f7a3ac4d8679e4eaf7eb8a85783f91.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Barycentric coordinate
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c0867023307c09f77ebdfc9f5560062.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Keep data uncertainty fixed , and model uncertainty decrease from left to
    right.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c618847f3bedb89e5bed6684b27a2d3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Keep the model uncertainty fixed, and the data uncertainty decrease from
    left to right (the entropy of the categoric distribution decrease).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14\. Dirichlet distribution density visualization
  prefs: []
  type: TYPE_NORMAL
- en: Due to the intriguing property of Dirichlet distribution, evidential deep learning
    directly predict the parameters of Dirichlet density. For example, the Dirichlet
    prior network (DPN) (Malinin and Gales, [2018](#bib.bib85)) learn the concentration
    parameter $\boldsymbol{\alpha}$ for the Dirichlet distribution $\boldsymbol{\alpha}=\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta})$.
    Then the categorical distribution parameters can be drawn from the Dirichlet distribution
    as $p(\boldsymbol{\mu}|\boldsymbol{x},\boldsymbol{\theta})=\text{Dir}(\boldsymbol{\mu}|\boldsymbol{\alpha})$.
    The prediction of class probability is the average over possible $\boldsymbol{\mu}$
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $p(w_{c}&#124;\boldsymbol{x},\boldsymbol{\theta})=\int p(w_{c}&#124;\boldsymbol{\mu})p(\boldsymbol{\mu}&#124;\boldsymbol{x},\boldsymbol{\theta})=\frac{\alpha_{c}}{\alpha_{0}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To measure the uncertainty from the Dirichlet distribution, the total uncertainty
    is the entropy of the average prediction distribution, and data uncertainty is
    the average of the entropy in each realization of $\boldsymbol{\mu}$. Several
    approaches have extended the Prior network to a regression model and a posterior
    network has also been proposed for more reliable uncertainty estimation.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of evidential deep learning is that the approach only requires
    a single forward pass during inference, and is much more computationally efficient.
    The approach also explicitly distinguishes data and model uncertainty in a principled
    way. The disadvantage is that the training stage is more complicated and is not
    guaranteed to obtain the same or similar prediction accuracy with vanilla network
    and cannot leverage existing progress in DNN. Furthermore, the training stage
    requires OOD samples to learn well representations and increase amount of the
    training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Simultaneous Model and data uncertainty methods comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Approach | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| Combination of existing approaches Pros: Easy to implement. Cons: High computation
    and storage cost. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combine BNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution (Kendall and Gal, [2017](#bib.bib67)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture uncertainty &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from both data noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and model parameters. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Require multiple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; forward pass &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; during inference. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Combine ensemble &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution (Lakshminarayanan et al., [2017](#bib.bib73)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture uncertainty &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from both data noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and mode architectures. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Require more &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; computation and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; storage requirement. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Combine ensemble &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; interval (Pearce et al., [2018](#bib.bib106); Salem et al., [2020](#bib.bib119))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capture both data and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model uncertainty and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; do not need explicit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; parametric distribution &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; form. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Require modification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on existing DNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; training process. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Evidential deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Evidential deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learning (Malinin and Gales, [2018](#bib.bib85); Charpentier et al.,
    [2020](#bib.bib17); Sensoy et al., [2018](#bib.bib122); Bao et al., [2021](#bib.bib7))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Computational efficient. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Cannot ensure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; similar accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; compared with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; vanilla DNN models. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only capture OOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; uncertainty &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Uncertainty estimation in various machine learning problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss several major ML problems where UQ can play a critical
    role. For each problem, we discuss the following four perspectives, including
    the nature of the data on the application, the source of uncertainty, the challenges
    associated with uncertainty quantification, and the existing literature for the
    specific problems.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Out-of-distribution detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a fundamental assumption for a deep neural network that the test data
    distribution is similar to the training data distribution $p_{\text{train}}(x)\approx
    p_{\text{test}}(x)$. However, in real-world tasks, DNN can encounter many out-of-distribution
    (OOD) data, which come from some different distributions compared with the training
    data. DNN performance may drop significantly and give unreliable predictions in
    such situations. The model should be aware of such situations to avoid over-confident
    predictions on OOD samples.
  prefs: []
  type: TYPE_NORMAL
- en: Given a training data distribution $p(x)$, the OOD data are those samples that
    are either unlikely under the training data distribution or outside the support
    of $p(x)$. Accurate detection of OOD samples is of paramount importance for the
    safety-critical application, e.g., autonomous driving (Shafaei et al., [2018](#bib.bib123)),
    medical image analysis (Ghoshal et al., [2021](#bib.bib42)) Since OOD samples
    lie further away from the training samples, the trained model may not generalize
    well and produces diverse and unstable predictions for those samples. The model
    parameters or architecture are not suitable for OOD samples. Thus OOD samples
    are expected to have higher model uncertainty. The primary uncertainty for OOD
    data is concerned with model uncertainty, because the model trained with in-distribution
    may not generalize well to other domains. It is important to make the model aware
    of the sample density when quantifying the uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing approaches: From the above analysis we are clear that model uncertainty
    is more important for OOD detection. Thus existing approaches leveraging the model
    uncertainty framework are much more popular. For example, drop-out-based BNN approaches
    have been applied to OOD detection and improved the performance through the use
    of the randomized embeddings induced by the intermediate layers of a dropout BNN
    (Nguyen et al., [2022a](#bib.bib97)); Deep ensembles are simple but perform well
    on OOD detection (Lakshminarayanan et al., [2017](#bib.bib73); Ovadia et al.,
    [2019](#bib.bib103)). Recent advances attempt to develop distance-aware DNN for
    more accurate OOD detection by imposing constraints on the feature extracting
    process (Liu et al., [2020](#bib.bib80); van Amersfoort et al., [2021](#bib.bib135)).
    Recent developments in evidential deep learning framework have also demonstrated
    its capability on OOD detection in many benchmark datasets because of the explicit
    distinction between two types of uncertainty in the framework (Malinin and Gales,
    [2018](#bib.bib85); Charpentier et al., [2020](#bib.bib17); Sensoy et al., [2018](#bib.bib122);
    Bao et al., [2021](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Active Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the deep learning model, obtaining labeled data can be laborious, and time-consuming.
    Active learning (Ren et al., [2021](#bib.bib114)) aims to solve the data labeling
    issue by learning from a small amount of data and choosing by the model what data
    it requires the user to label and retrain the model iteratively. The goal is to
    reduce the number of labeled examples needed for learning as much as possible,
    which means requiring a strategy to choose which sample is more worth labeling
    for the model. Utilizing predictive uncertainty has been a popular strategy and
    those instances whose prediction is maximally uncertain are most useful.
  prefs: []
  type: TYPE_NORMAL
- en: The key goal in active learning is to choose observations $\boldsymbol{x}$ for
    which obtaining labels $y$ would improve the performance of the learning. As discussed
    in the background, adding samples with large data uncertainty cannot improve the
    trained model because its inherent randomness is irreducible while more samples
    with model uncertainty can improve the model performance. In this regard, model
    uncertainty is more important for active learning (Nguyen et al., [2022b](#bib.bib98)).
    The critical challenge for active learning is to distinguish between the data
    and model uncertainty and utilize model uncertainty for selecting new samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing approaches: Similar to OOD detection, the approaches for model uncertainty
    detection can be adapted to an active learning framework by considering uncertainty
    coming from the parameter, model architecture, and sample density sparsity. For
    example, the BNN framework considers the samples that decrease the entropy of
    $p(\boldsymbol{\theta}|\mathcal{D}_{\text{tr}},\{\boldsymbol{x},y\})$ most will
    be most useful (Depeweg, [2019](#bib.bib31)). The deep ensemble and MC-dropout
    approach can also be a straightforward way for quantifying the model uncertainty
    in active learning (Hein et al., [2022](#bib.bib50)). Recent approaches propose
    margin-based uncertainty sampling scheme and provide convergence analysis (Raj
    and Bach, [2022](#bib.bib112))'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Deep Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of deep reinforcement learning (DRL) is to train an agent interacting
    with environment to maximize its total rewards (Sutton and Barto, [2018](#bib.bib130)).
    DRL can be regarded as learning via Markov Decision Process, defined by the tuple
    $\{S,A,R,P\}$, where $S$ is the set of states (environment condition), $A$ is
    the set of actions (agent), $R$ is the function mapping state-action pairs to
    rewards, and $P$ is the transition probability (the probability of next state
    after performing actions on current state). The goal of DRL is to learn the policy
    $\pi$ (a function mapping given state to an action) that maximize the sum of discounted
    future rewards $J(\pi)=\mathbb{E}[\sum_{i}\gamma^{i}R(s_{i},a_{i})]$, where $\gamma$
    is the discount factor on the future reward (meaning rewards in more future are
    less important) (Sutton and Barto, [2018](#bib.bib130)). In Deep Q-learning, the
    DNN models are utilized to learn the value functions (expected rewards for a state-action
    pair) for each action given the input state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the complex agent and environment conditions and limited training states,
    there also exist two types of uncertainty sources: data and model uncertainty.
    Data uncertainty can arise from the intrinsic randomness in the interactions between
    the agent and environment, which causes randomness in the reward $R$, the transition
    functions $P$ and the randomness in next state value distribution. To characterize
    the data uncertainty arising from those sources, the distributional RL (Bellemare
    et al., [2017](#bib.bib9)) take a probabilistic perspective on learning the rewards
    functions instead of approximating the expectation of the value. Thus the approach
    can implement risk-aware behavior on the agent. Similar approach is proposed to
    quantify the data uncertainty in DRL aiming for curiosity-based learning in face
    of unpredictable transitions (Mavor-Parker et al., [2022](#bib.bib88)). The following
    work (Dabney et al., [2018](#bib.bib24)) extend the parametric distribution to
    non-parametric prediction interval methods to quantify the data uncertainty and
    avoid the explicit parametric format. The approach corresponds to the literature
    we discuss in section 5.1\. On the other hand, given the limited training state
    space, there exists high model uncertainty. The DNN model may not learn the optimum
    policy function and miss the unexplored state spaces, which potentially give higher
    rewards. This means DRL model face a trade-off between exploitation and exploration.
    Exploration means utilizing what the model has learned and choose the best policy
    to maximize future rewards. Exploration refers to choose the unexplored state
    to learn more possible high state-action pairs. The challenge of effective exploration
    is connected to model uncertainty. The higher model uncertainty means the model
    are not learnt well on the given state and require more exploration on that sample.
    For example, deep ensemble Q-network (Chen et al., [2021c](#bib.bib20)) is proposed
    to inject model uncertainty into Q learning for more efficient exploration sampling.
    To reduce the computational overhead, Dropout Q-functions (Hiraoka et al., [2021](#bib.bib55))
    method is proposed to use MC-dropout for model uncertainty quantification. The
    following work (Osband et al., [2018](#bib.bib102)) demonstrates the previous
    ensemble and dropout methods may produce poor approximation to the model uncertainty
    in case where state density does not correlate with the true uncertainty. To overcome
    the shortcoming, they suggest adding a random prior to the ensemble DQNs.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, both data and model uncertainty are crucial aspects of DRL for more
    efficient policy learning and exploration. Many works inspired from DNN uncertainty
    quantification have been proposed to improve the performance of DRL.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Future direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We identify some research gaps and list several future directions that current
    literature has not explored or has little coverage from both the methodologies
    and application perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Combine UQ with DNN explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The explanation for DNN model predictions has been increasingly crucial because
    it provides tools for understanding and diagnosing the model’s prediction. Recently,
    many explainability methods, termed explainers (Vu and Thai, [2020](#bib.bib139)),
    have been introduced in the category of local feature attribution methods. That
    is, methods that return a real-valued score for each feature of a given data sample,
    representing the feature’s relative importance for the sample prediction. These
    explanations are local in that each data sample may have a different explanation.
    Using local feature attribution methods, therefore, helps users better understand
    nonlinear and complex black-box models. Both uncertainty quantification and explanation
    are important for a robust, trustworthy AI model. Current methodologies consider
    two directions separately, we consider it could enable a more trustworthy AI system
    if combining them. Though many methodologies have been proposed for more precise
    uncertainty quantification, very few techiniques attempt to explain why the uncertainty
    exists in the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible directions to combine the power of explanations and
    uncertainty quantification: First, existing explanation methods could be potentially
    improved after considering the prediction uncertainty since those uncertain samples’
    explanations may not be trustworthy and can be omitted. Second, from another perspective,
    after obtaining the uncertainty quantification, we can leverage the existing post-hoc
    explanation methods for understanding the reason that the model is uncertain.
    For example, it is intriguging to ask the question why the prediction is uncertainty
    and which or which set of input features are uncertain, or due to which layer
    of the model is imperfect.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Structured Uncertainty Quantification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structured prediction concerns learning on structured objects, where the samples
    are correlated with each other, violating the i.i.d assumption in most machine
    learning problems. Many methodologies have been developed for structured learning
    (BakIr et al., [2007](#bib.bib6)), such as conditional random field, hidden Markov
    model, and graph neural networks. The intuition is to leverage the probabilistic
    graphic model among variables or samples to explicitly utilize the dependency
    structure for prediction. Not only the target variable predictions are correlated,
    but the prediction uncertainty could also be related to the structured dependency
    inherent in the model. However, most methodologies ignore the uncertainty structure,
    which is of paramount importance for calibrated UQ in many tasks. In the following,
    we briefly introduce the uncertainty of three kinds of structured data, image,
    graph, and spatio-temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Imaging and inverse problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of the imaging process is to reconstruct an unknown image from measurements,
    which is an inverse problem commonly used in medical imaging (e.g., magnetic resonance
    imaging and X-ray computed tomography) and geophysical inversion (e.g., seismic
    inversion) (Fessler, [2010](#bib.bib36); Edupuganti et al., [2020](#bib.bib34);
    Sun et al., [2021](#bib.bib128)). However, this process is challenging due to
    the limited and noisy information used to determine the original image, leading
    to structured uncertainty and correlations between nearby pixels in the reconstructed
    image (Kendall and Gal, [2017](#bib.bib67)). To overcome this issue, current research
    in uncertainty quantification of inverse problems employs conditional deep generative
    models, such as cVAE, cGAN, and conditional normalizing flow models (Dorta et al.,
    [2018](#bib.bib32); Adler and Öktem, [2018](#bib.bib3); Denker et al., [2020](#bib.bib30)).
    These methods utilize a low-dimensional latent space for image generation but
    may overlook unique data characteristics, such as structural constraints from
    domain physics in certain types of image data, such as remote sensing images,
    MRI images, or geological subsurface images (Jiang and Shekhar, [2017](#bib.bib61);
    Shih et al., [2022](#bib.bib125); Sun et al., [2021](#bib.bib128)). The use of
    physics-informed models may improve uncertainty quantification in these cases.
    It’s promising to incorporate the physics constraints for quantifying the uncertainty
    associated with the imaging process.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2\. Spatiotemporal data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spatiotemporal data are measurements over space and time  (Shekhar et al., [2015](#bib.bib124);
    Jiang, [2018](#bib.bib57); Jiang et al., [2022b](#bib.bib62)). They are special
    due to the violation of the common assumption that samples follow an identical
    and independent distribution. Uncertainty quantification of spatiotemporal deep
    learning poses several unique challenges. First, the analysis of spatiotemporal
    data requires co-registration of different maps (e.g., points, lines, polygons,
    geo-raster) into the same spatial coordinate system. The process is subject to
    registration uncertainty due to GPS errors or annotation mistakes in map generation 
    (Jiang et al., [2022a](#bib.bib59), [2021](#bib.bib58)). For example, the vector
    annotation labels of a river can be misaligned with corresponding river pixels
    in high-resolution Earth imagery (e.g., due to annotation mistakes or interpreting
    low-resolution background imagery). Such registration uncertainty causes troubles
    when training deep neural networks. Quantifying registration uncertainty for vector
    annotations or between vector labels to raster features is non-trivial as it involves
    modeling a continuous line (or polygon)  (He et al., [2022](#bib.bib49)). Second,
    implicit dependency structures exist in continuous space and time (e.g., spatial
    and temporal autocorrelation, temporal dynamics). Thus, the uncertainty quantification
    process should be aware of the dynamic dependency structure among predictions
    at different locations and times. Third, spatiotemporal data are non-stationary,
    i.e., sample distribution varies across space and time. Because of this, a deep
    neural network trained from samples in one region may not generalize well to test
    samples in a new region. Addressing this issue requires characterizing out-of-distribution
    samples due to spatiotemporal non-stationarity (e.g., spatiotemporal outliers) 
    (Shekhar et al., [2015](#bib.bib124); Jiang, [2018](#bib.bib57)). In addition,
    even within the same region where training samples are collected, the model prediction
    at a new test location may exhibit a different level of uncertainty based on the
    nearby density of training samples. For example, in air quality prediction, real-world
    sensors are often non-uniformly distributed. A sub-area with fewer observations
    nearby tends to be more uncertain. Traditionally, the Gaussian process has been
    used to quantify such uncertainty. However, for deep neural network models, new
    techniques are needed that consider sample density both in the non-spatial feature
    space and in the geographic space. To address the above challenges, novel UQ methods
    need to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3\. Graph data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph data is a general type of structured data with nodes and edge connections.
    Graph neural networks (GNNs) have been widely used for graph applications related
    to node classification and edge (link) prediction. However, UQ for GNN models
    has been less explored. Some work utilizes existing UQ techniques for GNN models 
    (Feng et al., [2021](#bib.bib35)) without considering their unique characteristics.
    First, predictions on a graph are structured, so the UQ module needs to consider
    such structural dependency. Second, many GNN models assume a fixed graph topology
    from training and test instances (e.g., spectral-based methods  (Defferrard et al.,
    [2016](#bib.bib28))). Uncertainty exists in GNN predictions due to the shift of
    graph topology from training graphs and test graphs. Similarly, uncertainty exists
    when the graph is perturbed by removing nodes and edges. Finally, many real-world
    graph problems are spatiotemporal at the same time (e.g., traffic flow prediction
    on road networks). Thus, challenges related to UQ for spatiotemporal deep learning
    also apply to graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. UQ for physics-aware DNN models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s well-known that many scientific applications can be described with some
    kind of a physics principle, e.g., the conservation law for diffusion process
    in the form of a partial differential equation (PDE). Given the dramatic capability
    of DL in extracting complex patterns in CV, and NLP, many scientific domains aim
    to leverage the massive amount of observed data to advance scientific discoveries.
    However, deep learning models do not necessarily obey the fundamental physics
    principle of the system that drives the real-world phenomenon (Kashinath et al.,
    [2021](#bib.bib66)), because of the black-box nature of neural networks. To address
    the challenge, physics-informed neural networks (PINN) (Krishnapriyan et al.,
    [2021](#bib.bib71); Yang and Perdikaris, [2019](#bib.bib149); Li et al., [2020](#bib.bib78);
    Karniadakis et al., [2021](#bib.bib65)) attempt to incorporate physical principles
    and domain knowledge from theoretical understanding into deep learning models
    to build physically consistent predictive models. For example, considering a spatio-temporal
    observation in a physical system $\mathcal{D}\times\mathcal{T}$ driven by a PDE
    in the form
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $u_{t}(x,t)+\mathcal{N}_{x}u(x,t)=0,x\in\mathcal{D},t\in\mathcal{T}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $u(x,t)$ is the interesting physical variables to be modeled, which can
    be parameterized with a neural network, i.e., $u(x,t)=f_{\boldsymbol{\theta}}(x,t)$,
    $x$ represents the spatial coordinate, $t$ is the time coordinate and $\mathcal{N}_{x}$
    is the nonlinear differential operator. The intuition of the PINN model is to
    build a neural network that explicitly encodes Eq. [26](#S7.E26 "In 7.3\. UQ for
    physics-aware DNN models ‣ 7\. Future direction ‣ A Survey on Uncertainty Quantification
    Methods for Deep Neural Networks: An Uncertainty Source’s Perspective") as residual
    loss, i.e., $r_{\boldsymbol{\theta}}(x,t)=\partial_{t}{f_{\boldsymbol{\theta}}(x,t)}-\mathcal{N}_{x}f_{\boldsymbol{\theta}}(x,t)$
    to control the optimization space of the neural network (Karniadakis et al., [2021](#bib.bib65)).
    Thus, the physical constraints can be incorporated into the model training process
    as a soft penalty.'
  prefs: []
  type: TYPE_NORMAL
- en: For many physical systems, the underlying physical principle generating the
    samples may be non-deterministic or unknown. The system is inherently chaotic
    and stochastic. For example, in weather forecasting and climate prediction problem,
    the state of the system is sensitive to the initial conditions, in which a small
    perturbation in the nonlinear system can result in a large difference in the later
    state (a.k.a. butterfly effect) (Kashinath et al., [2021](#bib.bib66)). Even with
    a deterministic initial condition, the unknown physics equation (e.g., PDE) that
    drives the system can be stochastic, which is described by a stochastic differential
    equation. Uncertainty quantification is of vital importance to improve the prediction’s
    reliability, especially under distribution shifts. To summarize, the uncertainty
    of physical system modeling can come from the following source. First, the initial
    and boundary condition of the physical system is non-deterministic, and the system
    may be chaotic, i.e., in weather forecasting, a small perturbation can introduce
    a large deviation in future prediction (Wang et al., [2019b](#bib.bib140)). Second,
    the inherent physical principle may not be perfectly known or the parameter of
    the governing equation may be stochastic, i.e., in an imperfect physical system,
    the conservation law of heat may be violated in a non-closed system (Yang and
    Perdikaris, [2019](#bib.bib149)). Lastly, the physical system can be inhomogeneous
    and data distribution is non-stationary (Daw et al., [2021](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: These cases indicate that for the stochastic physical system, there exists inherent
    data uncertainty. Thus, a natural way to model distributions and incorporate stochasticity
    and uncertainty into the neural network is through probabilistic models. Because
    of the unique characteristic of physical data, there exist several challenges
    for quantifying the uncertainty in PINN. First, the system requires considering
    the physical principle and its uncertainty simultaneously. The incorporation of
    physical constraints can significantly reduce the data and model uncertainty.
    Second, there are various sources of uncertainty, which may arise from the inherent
    physical system randomness, the measurement error, and the limited knowledge of
    the physical governing equation.
  prefs: []
  type: TYPE_NORMAL
- en: One direction of uncertainty quantification for PINN is to build a probabilistic
    neural network $p_{\boldsymbol{\theta}}(\boldsymbol{u}|\boldsymbol{x},t,\boldsymbol{z})$
    for propagating uncertainty originating from various sources, where $\boldsymbol{u}(\boldsymbol{x},t)$
    represents the spatio-temporal random field with the explanatory spatial variable
    $\boldsymbol{x}$ and temporal variable $t$. $\boldsymbol{z}$ is a latent variable
    that follows a prior distribution $p(\boldsymbol{z})$ and aim to encode the variability
    of the high-dimensional observation $\boldsymbol{u}$ into low-dimensional embedding.
    many approaches based on the deep generative model for uncertainty quantification
    have been proposed to leverage their capability of probabilistic modeling for
    structured outputs (Oberdiek et al., [2022](#bib.bib100); Gao and Ng, [2022](#bib.bib40);
    Daw et al., [2021](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper presents a systematic survey on uncertainty quantification for DNNs
    based on their types of uncertainty sources. We categorize existing literature
    into three groups: model uncertainty, data uncertainty, and the combination of
    the two. In addition, we analyze the advantages and disadvantages of each approach
    corresponding to the subtype of uncertainty source it addresses. The uncertainty
    source and unique challenge of various applications in different domains and ML
    problems are also summarized. We list several future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This material is based upon work supported by the National Science Foundation
    (NSF) under Grant No. IIS-2147908, IIS-2207072, CNS-1951974, OAC-2152085, and
    the National Oceanic and Atmospheric Administration grant NA19NES4320002 (CISESS)
    at the University of Maryland.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdar et al. (2021) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan,
    Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
    Acharya, et al. 2021. A review of uncertainty quantification in deep learning:
    Techniques, applications and challenges. *Information Fusion* 76 (2021), 243–297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adler and Öktem (2018) Jonas Adler and Ozan Öktem. 2018. Deep bayesian inversion.
    *arXiv preprint arXiv:1811.05910* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alam et al. (2017) Firoj Alam, Muhammad Imran, and Ferda Ofli. 2017. Image4act:
    Online social media image processing for disaster response. In *Proceedings of
    the 2017 IEEE/ACM international conference on advances in social networks analysis
    and mining 2017*. 601–604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017.
    Wasserstein generative adversarial networks. In *International conference on machine
    learning*. PMLR, 214–223.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BakIr et al. (2007) Gökhan BakIr, Thomas Hofmann, Alexander J Smola, Bernhard
    Schölkopf, and Ben Taskar. 2007. *Predicting structured data*. MIT press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2021) Wentao Bao, Qi Yu, and Yu Kong. 2021. Evidential deep learning
    for open set action recognition. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 13349–13358.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Begoli et al. (2019) Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov.
    2019. The need for uncertainty quantification in machine-assisted medical decision
    making. *Nature Machine Intelligence* 1, 1 (2019), 20–23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellemare et al. (2017) Marc G Bellemare, Will Dabney, and Rémi Munos. 2017.
    A distributional perspective on reinforcement learning. In *International Conference
    on Machine Learning*. PMLR, 449–458.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (2013) Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013.
    Representation learning: A review and new perspectives. *IEEE transactions on
    pattern analysis and machine intelligence* 35, 8 (2013), 1798–1828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bishop (1994) Christopher M Bishop. 1994. Mixture density networks. (1994).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blei et al. (2017) David M Blei, Alp Kucukelbir, and Jon D McAuliffe. 2017.
    Variational inference: A review for statisticians. *Journal of the American statistical
    Association* 112, 518 (2017), 859–877.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bower and Bolouri (2001) James M Bower and Hamid Bolouri. 2001. *Computational
    modeling of genetic and biochemical networks*. MIT press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradford et al. (2018) Eric Bradford, Artur M Schweidtmann, Dongda Zhang, Keju
    Jing, and Ehecatl Antonio del Rio-Chanona. 2018. Dynamic modeling and optimization
    of sustainable algal production with uncertainty using multivariate Gaussian processes.
    *Computers & Chemical Engineering* 118 (2018), 143–158.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brunton and Kutz (2022) Steven L Brunton and J Nathan Kutz. 2022. *Data-driven
    science and engineering: Machine learning, dynamical systems, and control*. Cambridge
    University Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2020) Jie Chang, Zhonghao Lan, Changmao Cheng, and Yichen Wei.
    2020. Data uncertainty learning in face recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 5710–5719.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charpentier et al. (2020) Bertrand Charpentier, Daniel Zügner, and Stephan
    Günnemann. 2020. Posterior network: Uncertainty estimation without ood samples
    via density-based pseudo-counts. *Advances in Neural Information Processing Systems*
    33 (2020), 1356–1367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021b) Haoxian Chen, Ziyi Huang, Henry Lam, Huajie Qian, and Haofeng
    Zhang. 2021b. Learning prediction intervals for regression: Generalization and
    calibration. In *International Conference on Artificial Intelligence and Statistics*.
    PMLR, 820–828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) Xiang Chen, Andres Diaz-Pinto, Nishant Ravikumar, and Alejandro F
    Frangi. 2021a. Deep learning in medical image registration. *Progress in Biomedical
    Engineering* 3, 1 (2021), 012003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021c) Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. 2021c.
    Randomized ensembled double q-learning: Learning fast without a model. *arXiv
    preprint arXiv:2101.05982* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2014) Reynold Cheng, Tobias Emrich, Hans-Peter Kriegel, Nikos
    Mamoulis, Matthias Renz, Goce Trajcevski, and Andreas Züfle. 2014. Managing uncertainty
    in spatial and spatio-temporal data. In *2014 IEEE 30th International Conference
    on Data Engineering*. IEEE, 1302–1305.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2019) Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae Lee.
    2019. Gaussian yolov3: An accurate and fast object detector using localization
    uncertainty for autonomous driving. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 502–511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cireşan et al. (2013) Dan C Cireşan, Alessandro Giusti, Luca M Gambardella,
    and Jürgen Schmidhuber. 2013. Mitosis detection in breast cancer histology images
    with deep neural networks. In *International conference on medical image computing
    and computer-assisted intervention*. Springer, 411–418.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dabney et al. (2018) Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos.
    2018. Distributional reinforcement learning with quantile regression. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damianou (2015) Andreas Damianou. 2015. *Deep Gaussian processes and variational
    propagation of uncertainty*. Ph. D. Dissertation. University of Sheffield.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damianou et al. (2016) Andreas C Damianou, Michalis K Titsias, and Neil Lawrence.
    2016. Variational inference for latent variables and uncertain inputs in Gaussian
    processes. (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daw et al. (2021) Arka Daw, M Maruf, and Anuj Karpatne. 2021. PID-GAN: A GAN
    Framework based on a Physics-informed Discriminator for Uncertainty Quantification
    with Physics. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*. 237–247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    *Advances in neural information processing systems* 29 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In *2009
    IEEE conference on computer vision and pattern recognition*. Ieee, 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denker et al. (2020) Alexander Denker, Maximilian Schmidt, Johannes Leuschner,
    Peter Maass, and Jens Behrmann. 2020. Conditional normalizing flows for low-dose
    computed tomography image reconstruction. *arXiv preprint arXiv:2006.06270* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depeweg (2019) Stefan Depeweg. 2019. *Modeling epistemic and aleatoric uncertainty
    with bayesian neural networks and latent variables*. Ph. D. Dissertation. Technische
    Universität München.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dorta et al. (2018) Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell,
    and Ivor Simpson. 2018. Structured uncertainty prediction networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 5477–5485.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dusenberry et al. (2020) Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas
    Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, and Andrew M Dai. 2020.
    Analyzing the role of model uncertainty for electronic health records. In *Proceedings
    of the ACM Conference on Health, Inference, and Learning*. 204–213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edupuganti et al. (2020) Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala,
    and John Pauly. 2020. Uncertainty quantification in deep MRI reconstruction. *IEEE
    Transactions on Medical Imaging* 40, 1 (2020), 239–250.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2021) Boyuan Feng, Yuke Wang, and Yufei Ding. 2021. Uag: Uncertainty-aware
    attention graph neural network for defending adversarial attacks. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 35\. 7404–7412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fessler (2010) Jeffrey A Fessler. 2010. Model-based image reconstruction for
    MRI. *IEEE signal processing magazine* 27, 4 (2010), 81–89.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florida and Wildlife (2021) Fish Florida and Conservation Commission Wildlife.
    2021. Florida Fish and Wildlife Conservation Commission Data. (2021). [10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Friston et al. (2007) Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto,
    John Ashburner, and Will Penny. 2007. Variational free energy and the Laplace
    approximation. *Neuroimage* 34, 1 (2007), 220–234.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
    a bayesian approximation: Representing model uncertainty in deep learning. In
    *international conference on machine learning*. PMLR, 1050–1059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Ng (2022) Yihang Gao and Michael K Ng. 2022. Wasserstein generative
    adversarial uncertainty quantification in physics-informed neural networks. *J.
    Comput. Phys.* 463 (2022), 111270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher, et al. 2021. A survey of uncertainty in deep
    neural networks. *arXiv preprint arXiv:2107.03342* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghoshal et al. (2021) Biraja Ghoshal, Allan Tucker, Bal Sanghera, and Wai Lup Wong.
    2021. Estimating uncertainty in deep learning for reporting confidence to clinicians
    in medical image segmentation and diseases detection. *Computational Intelligence*
    37, 2 (2021), 701–734.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2022) Xuan Gong, Luckyson Khaidem, Wentao Zhu, Baochang Zhang,
    and David Doermann. 2022. Uncertainty learning towards unsupervised deformable
    medical image registration. In *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*. 2484–2493.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.
    Generative adversarial networks. *Commun. ACM* 63, 11 (2020), 139–144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2011) Alex Graves. 2011. Practical variational inference for neural
    networks. *Advances in neural information processing systems* 24 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guillaumes (2017) Axel Brando Guillaumes. 2017. *Mixture density networks for
    distribution and uncertainty estimation*. Ph. D. Dissertation. Universitat Politècnica
    de Catalunya. Facultat d’Informàtica de Barcelona.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
    2017. On calibration of modern neural networks. In *International conference on
    machine learning*. PMLR, 1321–1330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hariri et al. (2019) Reihaneh H Hariri, Erik M Fredericks, and Kate M Bowers.
    2019. Uncertainty in big data analytics: survey, opportunities, and challenges.
    *Journal of Big Data* 6, 1 (2019), 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2022) Wenchong He, Zhe Jiang, Marcus Kriby, Yiqun Xie, Xiaowei Jia,
    Da Yan, and Yang Zhou. 2022. Quantifying and Reducing Registration Uncertainty
    of Spatial Vector Labels on Earth Imagery. In *Proceedings of the 28th ACM SIGKDD
    Conference on Knowledge Discovery and Data Mining*. 554–564.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hein et al. (2022) Alice Hein, Stefan Röhrl, Thea Grobel, Manuel Lengl, Nawal
    Hafez, Martin Knopp, Christian Klenk, Dominik Heim, Oliver Hayden, and Klaus Diepold.
    2022. A Comparison of Uncertainty Quantification Methods for Active Learning in
    Image Classification. In *2022 International Joint Conference on Neural Networks
    (IJCNN)*. IEEE, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. A baseline
    for detecting misclassified and out-of-distribution examples in neural networks.
    *arXiv preprint arXiv:1610.02136* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hengl et al. (2017) Tomislav Hengl, Jorge Mendes de Jesus, Gerard BM Heuvelink,
    Maria Ruiperez Gonzalez, Milan Kilibarda, Aleksandar Blagotić, Wei Shangguan,
    Marvin N Wright, Xiaoyuan Geng, Bernhard Bauer-Marschallinger, et al. 2017. SoilGrids250m:
    Global gridded soil information based on machine learning. *PLoS one* 12, 2 (2017),
    e0169748.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hie et al. (2020) Brian Hie, Bryan D Bryson, and Bonnie Berger. 2020. Leveraging
    uncertainty in machine learning accelerates biological discovery and design. *Cell
    systems* 11, 5 (2020), 461–477.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton and Van Camp (1993) Geoffrey E Hinton and Drew Van Camp. 1993. Keeping
    the neural networks simple by minimizing the description length of the weights.
    In *Proceedings of the sixth annual conference on Computational learning theory*.
    5–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiraoka et al. (2021) Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi
    Onishi, and Yoshimasa Tsuruoka. 2021. Dropout Q-Functions for Doubly Efficient
    Reinforcement Learning. *arXiv preprint arXiv:2110.02034* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2021) Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling,
    Samantha Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach,
    Jordan Read, et al. 2021. Physics-guided recurrent graph model for predicting
    flow and temperature in river networks. In *Proceedings of the 2021 SIAM International
    Conference on Data Mining (SDM)*. SIAM, 612–620.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang (2018) Zhe Jiang. 2018. A survey on spatial prediction methods. *IEEE
    Transactions on Knowledge and Data Engineering* 31, 9 (2018), 1645–1664.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2021) Zhe Jiang, Wenchong He, Marcus Kirby, Sultan Asiri, and
    Da Yan. 2021. Weakly Supervised Spatial Deep Learning based on Imperfect Vector
    Labels with Registration Errors. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*. 767–775.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2022a) Zhe Jiang, Wenchong He, Marcus Stephen Kirby, Arpan Man
    Sainju, Shaowen Wang, Lawrence V Stanislawski, Ethan J Shavers, and E Lynn Usery.
    2022a. Weakly Supervised Spatial Deep Learning for Earth Image Segmentation Based
    on Imperfect Polyline Labels. *ACM Transactions on Intelligent Systems and Technology
    (TIST)* 13, 2 (2022), 1–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2019) Zhe Jiang, Arpan Man Sainju, Yan Li, Shashi Shekhar, and
    Joseph Knight. 2019. Spatial ensemble learning for heterogeneous geographic data
    with class ambiguity. *ACM Transactions on Intelligent Systems and Technology
    (TIST)* 10, 4 (2019), 1–25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang and Shekhar (2017) Zhe Jiang and Shashi Shekhar. 2017. Spatial big data
    science. *Schweiz: Springer International Publishing AG* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022b) Zhe Jiang, Liang Zhao, Xun Zhou, Robert N Stewart, Junbo
    Zhang, Shashi Shekhar, and Jieping Ye. 2022b. DeepSpatial’22: The 3rd International
    Workshop on Deep Learning for Spatiotemporal Data, Applications, and Systems.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*. 4878–4879.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jospin et al. (2022) Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray
    Buntine, and Mohammed Bennamoun. 2022. Hands-on Bayesian neural networks—A tutorial
    for deep learning users. *IEEE Computational Intelligence Magazine* 17, 2 (2022),
    29–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JSANG (2018) AUDUN. JSANG. 2018. *Subjective Logic: A formalism for reasoning
    under uncertainty*. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karniadakis et al. (2021) George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu,
    Paris Perdikaris, Sifan Wang, and Liu Yang. 2021. Physics-informed machine learning.
    *Nature Reviews Physics* 3, 6 (2021), 422–440.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kashinath et al. (2021) K Kashinath, M Mustafa, A Albert, JL Wu, C Jiang, S
    Esmaeilzadeh, K Azizzadenesheli, R Wang, A Chattopadhyay, A Singh, et al. 2021.
    Physics-informed machine learning: case studies for weather and climate modelling.
    *Philosophical Transactions of the Royal Society A* 379, 2194 (2021), 20200093.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall and Gal (2017) Alex Kendall and Yarin Gal. 2017. What uncertainties
    do we need in bayesian deep learning for computer vision? *Advances in neural
    information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2019) Sookyung Kim, Hyojin Kim, Joonseok Lee, Sangwoong Yoon, Samira Ebrahimi
    Kahou, Karthik Kashinath, and Mr Prabhat. 2019. Deep-hurricane-tracker: Tracking
    and forecasting extreme climate events. In *2019 IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. IEEE, 1761–1769.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohl et al. (2018) Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey
    De Fauw, Joseph R Ledsam, Klaus Maier-Hein, SM Eslami, Danilo Jimenez Rezende,
    and Olaf Ronneberger. 2018. A probabilistic u-net for segmentation of ambiguous
    images. *Advances in neural information processing systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishnapriyan et al. (2021) Aditi Krishnapriyan, Amir Gholami, Shandian Zhe,
    Robert Kirby, and Michael W Mahoney. 2021. Characterizing possible failure modes
    in physics-informed neural networks. *Advances in Neural Information Processing
    Systems* 34 (2021), 26548–26560.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kristiadi et al. (2021) Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
    2021. Learnable uncertainty under laplace approximations. In *Uncertainty in Artificial
    Intelligence*. PMLR, 344–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and
    Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation
    using deep ensembles. *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    learning. *nature* 521, 7553 (2015), 436–444.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020) Hong Joo Lee, Jung Uk Kim, Sangmin Lee, Hak Gu Kim, and Yong Man
    Ro. 2020. Structure boundary preserving segmentation for medical image with ambiguous
    boundary. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*. 4817–4826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lempert et al. (2022) Robert J Lempert, Steven W Popper, Carlos Calvo Hernandez,
    et al. 2022. *Transportation Planning for Uncertain Times: A Practical Guide to
    Decision Making Under Deep Uncertainty for MPOs*. Technical Report. United States.
    Federal Highway Administration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Yiqun Li, Songjian Chai, Guibin Wang, Xian Zhang, and Jing
    Qiu. 2022. Quantifying the Uncertainty in Long-Term Traffic Prediction Based on
    PI-ConvLSTM Network. *IEEE Transactions on Intelligent Transportation Systems*
    23, 11 (2022), 20429–20441.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede
    Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2020. Fourier
    neural operator for parametric partial differential equations. *arXiv preprint
    arXiv:2010.08895* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Licata and Mehta (2022) Richard J Licata and Piyush M Mehta. 2022. Uncertainty
    quantification techniques for data-driven space weather modeling: thermospheric
    density application. *Scientific Reports* 12, 1 (2022), 7256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss,
    and Balaji Lakshminarayanan. 2020. Simple and principled uncertainty estimation
    with deterministic deep learning via distance awareness. *Advances in Neural Information
    Processing Systems* 33 (2020), 7498–7512.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen,
    Ghassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran, and Balaji Lakshminarayanan.
    2022. A simple approach to improve single-model deep uncertainty via distance-awareness.
    *Journal of Machine Learning Research* 23 (2022), 1–63.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loftus et al. (2022) Tyler J Loftus, Benjamin Shickel, Matthew M Ruppert, Jeremy A
    Balch, Tezcan Ozrazgat-Baslanti, Patrick J Tighe, Philip A Efron, William R Hogan,
    Parisa Rashidi, Gilbert R Upchurch Jr, et al. 2022. Uncertainty-aware deep learning
    in healthcare: a scoping review. *PLOS digital health* 1, 8 (2022), e0000085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Louizos and Welling (2017) Christos Louizos and Max Welling. 2017. Multiplicative
    normalizing flows for variational bayesian neural networks. In *International
    Conference on Machine Learning*. PMLR, 2218–2227.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacKay (1992) David JC MacKay. 1992. A practical Bayesian framework for backpropagation
    networks. *Neural computation* 4, 3 (1992), 448–472.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin and Gales (2018) Andrey Malinin and Mark Gales. 2018. Predictive uncertainty
    estimation via prior networks. *Advances in neural information processing systems*
    31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mallick et al. (2022) Tanwi Mallick, Prasanna Balaprakash, and Jane Macfarlane.
    2022. Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph Neural
    Networks for Traffic Forecasting. *arXiv preprint arXiv:2204.01618* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markos et al. (2021) Christos Markos, JQ James, and Richard Yi Da Xu. 2021.
    Capturing uncertainty in unsupervised GPS trajectory segmentation using Bayesian
    deep learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 35\. 390–398.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mavor-Parker et al. (2022) Augustine Mavor-Parker, Kimberly Young, Caswell Barry,
    and Lewis Griffin. 2022. How to stay curious while avoiding noisy tvs using aleatoric
    uncertainty estimation. In *International Conference on Machine Learning*. PMLR,
    15220–15240.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mena et al. (2021) José Mena, Oriol Pujol, and Jordi Vitrià. 2021. A survey
    on uncertainty estimation in deep learning classification systems from a bayesian
    perspective. *ACM Computing Surveys (CSUR)* 54, 9 (2021), 1–35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishkin et al. (2018) Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark
    Schmidt, and Mohammad Emtiyaz Khan. 2018. Slang: Fast structured covariance approximations
    for bayesian deep learning with natural gradient. *Advances in Neural Information
    Processing Systems* 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo and Fu (2022) Zhaobin Mo and Yongjie Fu. 2022. TrafficFlowGAN: Physics-informed
    Flow based Generative Adversarial Network for Uncertainty Quantification. In *European
    Conference on Machine Learning and Data Mining (ECML PKDD)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mowbray et al. (2021) Max Mowbray, Thomas Savage, Chufan Wu, Ziqi Song, Bovinille Anye
    Cho, Ehecatl A Del Rio-Chanona, and Dongda Zhang. 2021. Machine learning for biochemical
    engineering: A review. *Biochemical Engineering Journal* 172 (2021), 108054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murphy (2012) Kevin P Murphy. 2012. *Machine learning: a probabilistic perspective*.
    MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NASA Goddard (2019) Space Flight Center NASA Goddard. 2019. Sea-viewing Wide
    Field-of-view Sensor (SeaWiFS) Ocean Color Data. (2019). [https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018](https://doi.org/10.5067/ORBVIEW-2/SEAWIFS/L2/OC/2018)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natekar et al. (2020) Parth Natekar, Avinash Kori, and Ganapathy Krishnamurthi.
    2020. Demystifying brain tumor segmentation networks: interpretability and uncertainty
    analysis. *Frontiers in computational neuroscience* 14 (2020), 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neal (2012) Radford M Neal. 2012. *Bayesian learning for neural networks*. Vol. 118.
    Springer Science & Business Media.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2022a) Andre T Nguyen, Fred Lu, Gary Lopez Munoz, Edward Raff,
    Charles Nicholas, and James Holt. 2022a. Out of Distribution Data Detection Using
    Dropout Bayesian Neural Networks. *arXiv preprint arXiv:2202.08985* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2022b) Vu-Linh Nguyen, Mohammad Hossein Shaker, and Eyke Hüllermeier.
    2022b. How to measure uncertainty in uncertainty sampling for active learning.
    *Machine Learning* 111, 1 (2022), 89–122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nixon et al. (2019) Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen
    Jerfel, and Dustin Tran. 2019. Measuring Calibration in Deep Learning.. In *CVPR
    workshops*, Vol. 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oberdiek et al. (2022) Philipp Oberdiek, Gernot A Fink, and Matthias Rottmann.
    2022. UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers
    trained via Conditional GANs. *arXiv preprint arXiv:2201.13279* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh et al. (2020) Shu Lih Oh, Yuki Hagiwara, U Raghavendra, Rajamanickam Yuvaraj,
    N Arunkumar, M Murugappan, and U Rajendra Acharya. 2020. A deep learning approach
    for Parkinson’s disease diagnosis from EEG signals. *Neural Computing and Applications*
    32, 15 (2020), 10927–10933.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osband et al. (2018) Ian Osband, John Aslanides, and Albin Cassirer. 2018. Randomized
    prior functions for deep reinforcement learning. *Advances in Neural Information
    Processing Systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David
    Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper
    Snoek. 2019. Can you trust your model’s uncertainty? evaluating predictive uncertainty
    under dataset shift. *Advances in neural information processing systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandey et al. (2022) Mohit Pandey, Michael Fernandez, Francesco Gentile, Olexandr
    Isayev, Alexander Tropsha, Abraham C Stern, and Artem Cherkasov. 2022. The transformational
    role of GPU computing and deep learning in drug discovery. *Nature Machine Intelligence*
    4, 3 (2022), 211–221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. (2021) Yutian Pang, Xinyu Zhao, Hao Yan, and Yongming Liu. 2021.
    Data-driven trajectory prediction with weather uncertainties: A Bayesian deep
    learning approach. *Transportation Research Part C: Emerging Technologies* 130
    (2021), 103326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pearce et al. (2018) Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy
    Neely. 2018. High-quality prediction intervals for deep learning: A distribution-free,
    ensembled approach. In *International conference on machine learning*. PMLR, 4075–4084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pfoser and Jensen (1999) Dieter Pfoser and Christian S Jensen. 1999. Capturing
    the uncertainty of moving-object representations. In *International Symposium
    on Spatial Databases*. Springer, 111–131.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posch and Pilz (2020) Konstantin Posch and Juergen Pilz. 2020. Correlated parameters
    to accurately measure uncertainty in deep neural networks. *IEEE Transactions
    on Neural Networks and Learning Systems* 32, 3 (2020), 1037–1051.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posch et al. (2019) Konstantin Posch, Jan Steinbrener, and Jürgen Pilz. 2019.
    Variational inference to measure model uncertainty in deep neural networks. *arXiv
    preprint arXiv:1902.10189* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prokudin et al. (2018) Sergey Prokudin, Peter Gehler, and Sebastian Nowozin.
    2018. Deep directional statistics: Pose estimation with uncertainty quantification.
    In *Proceedings of the European conference on computer vision (ECCV)*. 534–551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2021) Yu Qin, Zhiwen Liu, Chenghao Liu, Yuxing Li, Xiangzhu Zeng,
    and Chuyang Ye. 2021. Super-Resolved q-Space deep learning with uncertainty quantification.
    *Medical Image Analysis* 67 (2021), 101885.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raj and Bach (2022) Anant Raj and Francis Bach. 2022. Convergence of uncertainty
    sampling for active learning. In *International Conference on Machine Learning*.
    PMLR, 18310–18331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reichstein et al. (2019) Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens,
    Martin Jung, Joachim Denzler, and Nuno Carvalhais. 2019. Deep learning and process
    understanding for data-driven Earth system science. *Nature* 566, 7743 (2019),
    195–204.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2021) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui
    Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active
    learning. *ACM computing surveys (CSUR)* 54, 9 (2021), 1–40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ritter et al. (2018a) Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018a.
    Online structured laplace approximations for overcoming catastrophic forgetting.
    In *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*. 3742–3752.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ritter et al. (2018b) Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018b.
    A scalable laplace approximation for neural networks. In *6th International Conference
    on Learning Representations, ICLR 2018-Conference Track Proceedings*, Vol. 6\.
    International Conference on Representation Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ritter et al. (2021) Hippolyt Ritter, Martin Kukla, Cheng Zhang, and Yingzhen
    Li. 2021. Sparse Uncertainty Representation in Deep Learning with Inducing Weights.
    *Advances in Neural Information Processing Systems* 34 (2021), 6515–6528.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenfeld et al. (2018) Nir Rosenfeld, Yishay Mansour, and Elad Yom-Tov. 2018.
    Discriminative learning of prediction intervals. In *International Conference
    on Artificial Intelligence and Statistics*. PMLR, 347–355.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salem et al. (2020) Tárik S Salem, Helge Langseth, and Heri Ramampiaro. 2020.
    Prediction intervals: Split normal mixture from quality-driven deep ensembles.
    In *Conference on Uncertainty in Artificial Intelligence*. PMLR, 1179–1187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salimans et al. (2015) Tim Salimans, Diederik Kingma, and Max Welling. 2015.
    Markov chain monte carlo and variational inference: Bridging the gap. In *International
    conference on machine learning*. PMLR, 1218–1226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwaiger et al. (2020) Adrian Schwaiger, Poulami Sinhamahapatra, Jens Gansloser,
    and Karsten Roscher. 2020. Is uncertainty quantification in deep learning sufficient
    for out-of-distribution detection?. In *Aisafety@ ijcai*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensoy et al. (2018) Murat Sensoy, Lance Kaplan, and Melih Kandemir. 2018. Evidential
    deep learning to quantify classification uncertainty. *Advances in neural information
    processing systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shafaei et al. (2018) Sina Shafaei, Stefan Kugele, Mohd Hafeez Osman, and Alois
    Knoll. 2018. Uncertainty in machine learning: A safety perspective on autonomous
    driving. In *Computer Safety, Reliability, and Security: SAFECOMP 2018 Workshops,
    ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Västerås, Sweden, September 18, 2018,
    Proceedings 37*. Springer, 458–464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shekhar et al. (2015) Shashi Shekhar, Zhe Jiang, Reem Y Ali, Emre Eftelioglu,
    Xun Tang, Venkata MV Gunturi, and Xun Zhou. 2015. Spatiotemporal data mining:
    A computational perspective. *ISPRS International Journal of Geo-Information*
    4, 4 (2015), 2306–2338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shih et al. (2022) Shu-Fu Shih, Sevgi Gokce Kafali, Kara L Calkins, and Holden H
    Wu. 2022. Uncertainty-aware physics-driven deep learning network for free-breathing
    liver fat and R2* quantification using self-gated stack-of-radial MRI. *Magnetic
    Resonance in Medicine* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snelson and Ghahramani (2005) Edward Snelson and Zoubin Ghahramani. 2005. Sparse
    Gaussian processes using pseudo-inputs. *Advances in neural information processing
    systems* 18 (2005).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. (2015) Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning
    structured output representation using deep conditional generative models. *Advances
    in neural information processing systems* 28 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Jian Sun, Kristopher A Innanen, and Chao Huang. 2021. Physics-guided
    deep learning for seismic inversion with hybrid training and uncertainty analysis.
    *Geophysics* 86, 3 (2021), R303–R317.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2017) Shengyang Sun, Changyou Chen, and Lawrence Carin. 2017. Learning
    structured weight uncertainty in bayesian neural networks. In *Artificial Intelligence
    and Statistics*. PMLR, 1283–1292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    learning: An introduction*. MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swiatkowski et al. (2020) Jakub Swiatkowski, Kevin Roth, Bastiaan Veeling,
    Linh Tran, Joshua Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe
    Jenatton, and Sebastian Nowozin. 2020. The k-tied normal distribution: A compact
    parameterization of Gaussian mean field posteriors in Bayesian neural networks.
    In *International Conference on Machine Learning*. PMLR, 9289–9299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thulasidasan et al. (2019) Sunil Thulasidasan, Gopinath Chennupati, Jeff A
    Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved
    calibration and predictive uncertainty for deep neural networks. *Advances in
    Neural Information Processing Systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Titsias (2009) Michalis Titsias. 2009. Variational learning of inducing variables
    in sparse Gaussian processes. In *Artificial intelligence and statistics*. PMLR,
    567–574.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ustyuzhaninov et al. (2020) Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser,
    Erik Bodin, Neill Campbell, and Carl Henrik Ek. 2020. Compositional uncertainty
    in deep Gaussian processes. In *Conference on Uncertainty in Artificial Intelligence*.
    PMLR, 480–489.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, and Yarin Gal. 2021. On Feature Collapse and Deep Kernel Learning for
    Single Forward Pass Uncertainty. *arXiv preprint arXiv:2102.11409* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Amersfoort et al. (2020) Joost Van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. 2020. Uncertainty estimation using a single deep deterministic
    neural network. In *International conference on machine learning*. PMLR, 9690–9700.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Tulder and De Bruijne (2016) Gijs Van Tulder and Marleen De Bruijne. 2016.
    Combining generative and discriminative representation learning for lung CT analysis
    with convolutional restricted Boltzmann machines. *IEEE transactions on medical
    imaging* 35, 5 (2016), 1262–1272.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virmaux and Scaman (2018) Aladin Virmaux and Kevin Scaman. 2018. Lipschitz
    regularity of deep neural networks: analysis and efficient estimation. *Advances
    in Neural Information Processing Systems* 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu and Thai (2020) Minh Vu and My T Thai. 2020. Pgm-explainer: Probabilistic
    graphical model explanations for graph neural networks. *Advances in neural information
    processing systems* 33 (2020), 12225–12235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Bin Wang, Jie Lu, Zheng Yan, Huaishao Luo, Tianrui Li,
    Yu Zheng, and Guangquan Zhang. 2019b. Deep uncertainty quantification: A machine
    learning approach for weather forecasting. In *Proceedings of the 25th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 2087–2095.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2014) Jian Wang, Wei Deng, and Yuntao Guo. 2014. New Bayesian
    combination method for short-term traffic flow forecasting. *Transportation Research
    Part C: Emerging Technologies* 43 (2014), 79–94.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Pengyue Wang, Yan Li, Shashi Shekhar, and William F Northrop.
    2019a. Uncertainty estimation with distributional reinforcement learning for applications
    in intelligent transportation systems: A case study. In *2019 IEEE Intelligent
    Transportation Systems Conference (ITSC)*. IEEE, 3822–3827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wenzel et al. (2020) Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe
    Jenatton. 2020. Hyperparameter ensembles for robustness and uncertainty quantification.
    *Advances in Neural Information Processing Systems* 33 (2020), 6514–6527.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Rasmussen (2006) Christopher KI Williams and Carl Edward Rasmussen.
    2006. *Gaussian processes for machine learning*. Vol. 2. MIT press Cambridge,
    MA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson and Nickisch (2015) Andrew Wilson and Hannes Nickisch. 2015. Kernel interpolation
    for scalable structured Gaussian processes (KISS-GP). In *International conference
    on machine learning*. PMLR, 1775–1784.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson et al. (2016) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov,
    and Eric P Xing. 2016. Deep kernel learning. In *Artificial intelligence and statistics*.
    PMLR, 370–378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wolf et al. (2016) Christopher Wolf, Maximilian Karl, and Patrick van der Smagt.
    2016. Variational inference with hamiltonian monte carlo. *arXiv preprint arXiv:1609.08203*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Dongxia Wu, Liyao Gao, Matteo Chinazzi, Xinyue Xiong, Alessandro
    Vespignani, Yi-An Ma, and Rose Yu. 2021. Quantifying Uncertainty in Deep Spatiotemporal
    Forecasting. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*. 1841–1851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Perdikaris (2019) Yibo Yang and Paris Perdikaris. 2019. Adversarial
    uncertainty quantification in physics-informed neural networks. *J. Comput. Phys.*
    394 (2019), 136–152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yarin (2016) Gal Yarin. 2016. Uncertainty in deep learning. *University of Cambridge,
    Cambridge* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2017) Jiaxuan You, Xiaocheng Li, Melvin Low, David Lobell, and Stefano
    Ermon. 2017. Deep gaussian process for crop yield prediction based on remote sensing
    data. In *Thirty-First AAAI conference on artificial intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger
    Grosse. 2018. Noisy natural gradient as variational inference. In *International
    Conference on Machine Learning*. PMLR, 5852–5861.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Xianli Zhang, Buyue Qian, Shilei Cao, Yang Li, Hang Chen,
    Yefeng Zheng, and Ian Davidson. 2020. INPREM: An interpretable and trustworthy
    predictive model for healthcare. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 450–460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019) Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao
    Lin, Min Deng, and Haifeng Li. 2019. T-gcn: A temporal graph convolutional network
    for traffic prediction. *IEEE Transactions on Intelligent Transportation Systems*
    21, 9 (2019), 3848–3858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Weitao Zhou, Zhong Cao, Yunkang Xu, Nanshan Deng, Xiaoyu
    Liu, Kun Jiang, and Diange Yang. 2022. Long-Tail Prediction Uncertainty Aware
    Trajectory Planning for Self-driving Vehicles. In *2022 IEEE 25th International
    Conference on Intelligent Transportation Systems (ITSC)*. IEEE, 1275–1282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2022) Yuanshao Zhu, Yongchao Ye, Yi Liu, and JQ James. 2022. Cross-area
    travel time uncertainty estimation from trajectory data: a federated learning
    approach. *IEEE Transactions on Intelligent Transportation Systems* 23, 12 (2022),
    24966–24978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
